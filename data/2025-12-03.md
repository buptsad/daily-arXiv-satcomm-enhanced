<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 44]
- [eess.SP](#eess.SP) [Total: 8]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.IT](#cs.IT) [Total: 12]
- [eess.SY](#eess.SY) [Total: 13]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [PIBNet: a Physics-Inspired Boundary Network for Multiple Scattering Simulations](https://arxiv.org/abs/2512.02049)
*Rémi Marsal,Stéphanie Chaillat*

Main category: cs.LG

TL;DR: PIBNet provides a learning-based framework to approximate boundary traces in the boundary element method (BEM) for multiple scattering, using physics-inspired graphs and a multiscale graph neural network to improve efficiency and generalization to more obstacles.


<details>
  <summary>Details</summary>
Motivation: The boundary element method reduces discretization to boundaries, but the critical bottleneck is computing the solution trace on boundaries. A data-driven surrogate that efficiently captures long-range interactions among obstacles can accelerate BEM for large-scale scattering problems.

Method: PIBNet introduces a physics-inspired graph-based model to represent obstacles and their long-range interactions, and a novel multiscale graph neural network architecture to simulate multiple scattering. The network is trained and evaluated on a benchmark comprising diverse multiple-scattering datasets.

Result: The approach outperforms existing state-of-the-art learning-based methods on the considered tasks and shows superior generalization to settings with more obstacles.

Conclusion: PIBNet is an effective, scalable learning-based surrogate for the boundary trace in BEM, enabling efficient and accurate boundary-based simulations of multiple scattering with better generalization.

Abstract: The boundary element method (BEM) provides an efficient numerical framework for solving multiple scattering problems in unbounded homogeneous domains, since it reduces the discretization to the domain boundaries, thereby condensing the computational complexity. The procedure first consists in determining the solution trace on the boundaries of the domain by solving a boundary integral equation, after which the volumetric solution can be recovered at low computational cost with a boundary integral representation. As the first step of the BEM represents the main computational bottleneck, we introduce PIBNet, a learning-based approach designed to approximate the solution trace. The method leverages a physics-inspired graph-based strategy to model obstacles and their long-range interactions efficiently. Then, we introduce a novel multiscale graph neural network architecture for simulating the multiple scattering. To train and evaluate our network, we present a benchmark consisting of several datasets of different types of multiple scattering problems. The results indicate that our approach not only surpasses existing state-of-the-art learning-based methods on the considered tasks but also exhibits superior generalization to settings with an increased number of obstacles. github.com/ENSTA-U2IS-AI/pibnet

</details>


### [2] [Contextual Gating within the Transformer Stack: Synergistic Feature Modulation for Enhanced Lyrical Classification and Calibration](https://arxiv.org/abs/2512.02053)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: 在自注意力中引入辅助结构特征的SFL Transformer，通过Contextual Gating在BERT编码器栈中间层实现深层特征融合，显著提升歌词分类的准确性与概率校准。


<details>
  <summary>Details</summary>
Motivation: 解决仅在输出层融合结构信息导致的表示受限问题，提出在Transformer的深层中间阶段进行结构与语义信息的协同融合，以提升判别能力和概率估计的可信度。

Method: 提出SFL Transformer，采用Contextual Gating（一个中间SFL）来调制BERT编码器栈中的隐藏状态序列Hseq，利用低维结构线索Fstruct；对从UMAP降维的歌词嵌入得到的二分类任务进行评估。

Result: 在二分类任务上达到准确率0.9910、Macro F1 0.9910，显著超越前一代SFL模型（准确率0.9894），并保持低ECE（0.0081）和对数损失0.0489。数据来自基于UMAP降维的歌词嵌入。

Conclusion: 中间阶段注入辅助上下文是结构信息与语义信息的最有效协同融合路径，带来更强的判别能力与高保真概率估计。

Abstract: This study introduces a significant architectural advancement in feature fusion for lyrical content classification by integrating auxiliary structural features directly into the self-attention mechanism of a pre-trained Transformer. I propose the SFL Transformer, a novel deep learning model that utilizes a Contextual Gating mechanism (an Intermediate SFL) to modulate the sequence of hidden states within the BERT encoder stack, rather than fusing features at the final output layer. This approach modulates the deep, contextualized semantic features (Hseq) using low-dimensional structural cues (Fstruct). The model is applied to a challenging binary classification task derived from UMAP-reduced lyrical embeddings. The SFL Transformer achieved an Accuracy of 0.9910 and a Macro F1 score of 0.9910, significantly improving the state-of-the-art established by the previously published SFL model (Accuracy 0.9894). Crucially, this Contextual Gating strategy maintained exceptional reliability, with a low Expected Calibration Error (ECE = 0.0081) and Log Loss (0.0489). This work validates the hypothesis that injecting auxiliary context mid-stack is the most effective means of synergistically combining structural and semantic information, creating a model with both superior discriminative power and high-fidelity probability estimates.

</details>


### [3] [Opening the Black Box: An Explainable, Few-shot AI4E Framework Informed by Physics and Expert Knowledge for Materials Engineering](https://arxiv.org/abs/2512.02057)
*Haoxiang Zhang,Ruihao Yuan,Lihui Zhang,Yushi Luo,Qiang Zhang,Pan Ding,Xiaodong Ren,Weijie Xing,Niu Gao,Jishan Chen,Chubo Zhang*

Main category: cs.LG

TL;DR: 以物理和专家知识驱动的少样本可解释AI4E框架，针对航天合金裂纹倾向的预测，利用32个样本进行数据增强和符号回归发现可解释的本构方程，达到88%准确率，可用于工艺优化与虚拟数据生成。


<details>
  <summary>Details</summary>
Motivation: 工业AI在工程领域的推广受限于数据稀缺和模型不可解释性，尤其在安全敏感的领域。需要一个嵌入工程领域知识、可解释且对数据友好的方法。

Method: 三阶段数据增强：不同变量的噪声注入、强物理约束与变量关系保持；层级优化：符号回归探索结构，差分进化优化参数，后以混合全局局部优化精细化参数；通过一个嵌入物理知识的多目标架构实现可解释的本构方程。

Result: 得到一个可解释的本构方程，对热裂倾向预测的准确率为88%；方程揭示热、几何、冶金机理耦合对开裂的影响，提供定量预测与物理洞见，同时可用于工艺优化和高保真虚拟数据生成，提升其他数据驱动模型的性能。

Conclusion: 该方法为在数据受限、但可获取领域知识的高风险工业应用中落地可信AI提供普适蓝本，推动工程知识直接嵌入AI架构以实现可靠采用。

Abstract: The industrial adoption of Artificial Intelligence for Engineering (AI4E) faces two fundamental bottlenecks: scarce high-quality data and the lack of interpretability in black-box models-particularly critical in safety-sensitive sectors like aerospace. We present an explainable, few-shot AI4E framework that is systematically informed by physics and expert knowledge throughout its architecture. Starting from only 32 experimental samples in an aerial K439B superalloy castings repair welding case, we first augment physically plausible synthetic data through a three-stage protocol: differentiated noise injection calibrated to process variabilities, enforcement of hard physical constraints, and preservation of inter-parameter relationships. We then employ a nested optimization strategy for constitutive model discovery, where symbolic regression explores equation structures while differential evolution optimizes parameters, followed by intensive parameter refinement using hybrid global-local optimization. The resulting interpretable constitutive equation achieves 88% accuracy in predicting hot-cracking tendency. This equation not only provides quantitative predictions but also delivers explicit physical insight, revealing how thermal, geometric, and metallurgical mechanisms couple to drive cracking-thereby advancing engineers' cognitive understanding of the process. Furthermore, the constitutive equation serves as a multi-functional tool for process optimization and high-fidelity virtual data generation, enabling accuracy improvements in other data-driven models. Our approach provides a general blueprint for developing trustworthy AI systems that embed engineering domain knowledge directly into their architecture, enabling reliable adoption in high-stakes industrial applications where data is limited but physical understanding is available.

</details>


### [4] [Ada-MoGE: Adaptive Mixture of Gaussian Expert Model for Time Series Forecasting](https://arxiv.org/abs/2512.02061)
*Zhenliang Ni,Xiaowen Ma,Zhenkai Wu,Shuai Xiao,Han Shu,Xinghao Chen*

Main category: cs.LG

TL;DR: 提出 Ada-MoGE：自适应高斯混合专家以匹配输入数据的频谱分布，并使用高斯带通滤波平滑频域特征，从而在六个公开基准上实现 state-of-the-art，且模型参数仅0.2M。


<details>
  <summary>Details</summary>
Motivation: 传统 MoE 使用固定数量专家，难以随时间序列谱分布的动态变化进行自适应，导致信息被遗漏或噪声被引入。希望通过自适应专家数量和平滑的频域分解提升预测性能。

Method: 将谱强度与频率响应结合，用来动态确定专家数量；引入高斯带通滤波，平滑地分解频域特征，减少直接带截造成的噪声。

Result: 在六个公开基准数据集上达到最先进性能，参数量约 0.2M。

Conclusion: 自适应的专家数量和光滑的频域特征分解提升了多变量时间序列预测的鲁棒性和准确性，证明了 Ada-MoGE 的有效性。

Abstract: Multivariate time series forecasts are widely used, such as industrial, transportation and financial forecasts. However, the dominant frequencies in time series may shift with the evolving spectral distribution of the data. Traditional Mixture of Experts (MoE) models, which employ a fixed number of experts, struggle to adapt to these changes, resulting in frequency coverage imbalance issue. Specifically, too few experts can lead to the overlooking of critical information, while too many can introduce noise. To this end, we propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model. Ada-MoGE integrates spectral intensity and frequency response to adaptively determine the number of experts, ensuring alignment with the input data's frequency distribution. This approach prevents both information loss due to an insufficient number of experts and noise contamination from an excess of experts. Additionally, to prevent noise introduction from direct band truncation, we employ Gaussian band-pass filtering to smoothly decompose the frequency domain features, further optimizing the feature representation. The experimental results show that our model achieves state-of-the-art performance on six public benchmarks with only 0.2 million parameters.

</details>


### [5] [DPWMixer: Dual-Path Wavelet Mixer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2512.02070)
*Li Qianyang,Zhang Xingjun,Wang Shaoxun,Wei Jia*

Main category: cs.LG

TL;DR: 提出一个高效的双路径框架DPWMixer，用Lossless Haar小波金字塔替代池化，分离趋势与局部波动；通过全局线性映射与基于区域的MLP-Mixer实现宏观与微观动态，并通过自适应多尺度融合在八个基准数据集上获得SOTA提升。


<details>
  <summary>Details</summary>
Motivation: 当前长序列时间序列预测面临：Transformer的二次复杂度与在数据稀疏场景下易过拟合；线性模型难以表达复杂非线性局部动态；多尺度框架的平均池化造成信息丢失与频谱混叠。需要在保持高效的同时保留高频信息与局部波动。

Method: 提出Lossless Haar小波金字塔，以正交分解明确分离趋势与局部波动且不丢失信息；设计Dual-Path Trend Mixer，结合全局线性映射（宏观趋势锚定）与基于局部补丁的MLP-Mixer（微观动态演化）；提出自适应多尺度融合模块，对来自不同尺度的预测进行通道稳定性权重融合。

Result: 在八个公开基准数据集上，DPWMixer在与SOTA基线的比较中表现出一致的改进，且具有较高的计算效率。代码公开。

Conclusion: DPWMixer通过无损分解和双路径设计实现既高效又具强表达力的长时序预测，实验充分验证了其在多尺度融合中的有效性与鲁棒性。

Abstract: Long-term time series forecasting (LTSF) is a critical task in computational intelligence. While Transformer-based models effectively capture long-range dependencies, they often suffer from quadratic complexity and overfitting due to data sparsity. Conversely, efficient linear models struggle to depict complex non-linear local dynamics. Furthermore, existing multi-scale frameworks typically rely on average pooling, which acts as a non-ideal low-pass filter, leading to spectral aliasing and the irreversible loss of high-frequency transients. In response, this paper proposes DPWMixer, a computationally efficient Dual-Path architecture. The framework is built upon a Lossless Haar Wavelet Pyramid that replaces traditional pooling, utilizing orthogonal decomposition to explicitly disentangle trends and local fluctuations without information loss. To process these components, we design a Dual-Path Trend Mixer that integrates a global linear mapping for macro-trend anchoring and a flexible patch-based MLP-Mixer for micro-dynamic evolution. Finally, An adaptive multi-scale fusion module then integrates predictions from diverse scales, weighted by channel stationarity to optimize synthesis. Extensive experiments on eight public benchmarks demonstrate that our method achieves a consistent improvement over state-of-the-art baselines. The code is available at https://github.com/hit636/DPWMixer.

</details>


### [6] [HTG-GCL: Leveraging Hierarchical Topological Granularity from Cellular Complexes for Graph Contrastive Learning](https://arxiv.org/abs/2512.02073)
*Qirui Ji,Bin Qin,Yifan Jin,Yunze Zhao,Chuxiong Sun,Changwen Zheng,Jianwen Cao,Jiangmeng Li*

Main category: cs.LG

TL;DR: HTG-GCL 构建基于多尺度的环状细胞复合物，以捕获图结构对比学习中的层次拓扑信息，并引入基于不确定性的粒度权重机制实现粒度自适应的对比，提升图表征质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于结构增强的图对比学习在识别与任务相关的拓扑结构方面常常受限，难以适应不同下游任务所需的粗到细的拓扑粒度变化，因此需要一个能够产生多尺度拓扑视图并对粒度进行加权的框架。

Method: 提出 HTG-GCL，通过对同一图的转换生成多尺度的环状细胞复合物，体现拓扑粒度的概念，进而产生多样化的拓扑视图；引入多粒度解耦对比，以及基于不确定性估计的粒度特定加权机制，以抑制潜在的误导性粒度。

Result: 在多项基准数据集上的系统实验表明，该方法有效捕获层次拓扑信息，提升图表示的质量与任务性能，优于对比学习中的常规结构增强方法。

Conclusion: HTG-GCL 成功利用分层拓扑信息来提升图表征能力；不确定性驱动的粒度加权提高了对有用粒度的关注，显示出良好的鲁棒性和泛化潜力。

Abstract: Graph contrastive learning (GCL) aims to learn discriminative semantic invariance by contrasting different views of the same graph that share critical topological patterns. However, existing GCL approaches with structural augmentations often struggle to identify task-relevant topological structures, let alone adapt to the varying coarse-to-fine topological granularities required across different downstream tasks. To remedy this issue, we introduce Hierarchical Topological Granularity Graph Contrastive Learning (HTG-GCL), a novel framework that leverages transformations of the same graph to generate multi-scale ring-based cellular complexes, embodying the concept of topological granularity, thereby generating diverse topological views. Recognizing that a certain granularity may contain misleading semantics, we propose a multi-granularity decoupled contrast and apply a granularity-specific weighting mechanism based on uncertainty estimation. Comprehensive experiments on various benchmarks demonstrate the effectiveness of HTG-GCL, highlighting its superior performance in capturing meaningful graph representations through hierarchical topological information.

</details>


### [7] [Cross-View Topology-Aware Graph Representation Learning](https://arxiv.org/abs/2512.02130)
*Ahmet Sami Korkmaz,Selim Coskunuzer,Md Joshem Uddin*

Main category: cs.LG

TL;DR: 提出GraphTCL：一个双视角对比学习框架，将基于GNN的结构嵌入与由持久性同调得到的拓扑嵌入进行对齐，通过跨视角对比损失提升图表示并在TU和OGB分子图数据集上实现优异分类性能。


<details>
  <summary>Details</summary>
Motivation: GNNs擅长捕捉局部结构模式，但往往忽略全局拓扑特征，这些特征对稳健表示学习至关重要。将结构信息与拓扑信息结合并通过对比学习进行对齐，能够获得更具鲁棒性和区分性的图表示。

Method: 提出GraphTCL的双视角框架：1) 使用GNN提取图的结构嵌入；2) 通过持久性同调提取拓扑嵌入；3) 通过跨视角对比损失进行对齐，促进两个视角的一致性，从而提升图分类性能。

Result: 在基准数据集（TU数据集和OGB分子图）上，GraphTCL的性能稳定优于最先进的基线方法，证明了拓扑感知对比学习在图表示学习中的有效性。

Conclusion: 将拓扑特征纳入对比学习并实现跨视角对齐，可以提升图表示的质量与鲁棒性，GraphTCL验证了拓扑感知对比学习在图表示学习中的重要性。

Abstract: Graph classification has gained significant attention due to its applications in chemistry, social networks, and bioinformatics. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they often overlook global topological features that are critical for robust representation learning. In this work, we propose GraphTCL, a dual-view contrastive learning framework that integrates structural embeddings from GNNs with topological embeddings derived from persistent homology. By aligning these complementary views through a cross-view contrastive loss, our method enhances representation quality and improves classification performance. Extensive experiments on benchmark datasets, including TU and OGB molecular graphs, demonstrate that GraphTCL consistently outperforms state-of-the-art baselines. This study highlights the importance of topology-aware contrastive learning for advancing graph representation methods.

</details>


### [8] [CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation Models](https://arxiv.org/abs/2512.02180)
*Yuxuan Shu,Peter H. Charlton,Fahim Kawsar,Jussi Hernesniemi,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: 提出临床指南对比学习CLEF：通过利用临床风险评分对负样本进行自适应加权，结合缺失元数据处理，在MIMIC-IV数据上对12导ECG进行预训练，并在7个 Held-out 数据集上评估，优于自监督基线；在单导ECG分析上具有竞争力，代码可获取。


<details>
  <summary>Details</summary>
Motivation: 现有自监督预训练未充分利用临床元数据中的领域知识，难以将ECG嵌入与临床差异对齐；需要一种能整合临床风险信息、并对缺失元数据鲁棒的对比学习方法。

Method: 提出临床引导的对比学习（CLEF），使用已确立的临床风险分数自适应加权负样本，确保ECG嵌入的相似性与临床差异对齐，且具备处理缺失元数据的机制；在MIMIC-IV的161K患者的12导ECG数据上进行预训练，得到包括单导ECG的三种规模模型；在18个分类和回归任务、7个Hold-out数据集上评估，与5个基线与3个自监督算法比较。

Result: 在用12导ECG数据预训练并在Lead-I上测试时，CLEF优于自监督基线：中等规模的CLEF分类任务的平均AUROC提升≥2.6%，回归MAE下降≥3.2%；相较于现有自监督算法，CLEF平均AUROC提升≥1.8%。仅在Lead-I数据上预训练用于分类时，CLEF与监督学习的ECGFounder相当。

Conclusion: CLEF提升了单导ECG分析的准确性和可扩展性，推动远程健康监测的发展，证明将临床元数据整合进自监督学习可带来更优的表征；代码与预训练模型可在GitHub获取。

Abstract: The electrocardiogram (ECG) is a key diagnostic tool in cardiovascular health. Single-lead ECG recording is integrated into both clinical-grade and consumer wearables. While self-supervised pretraining of foundation models on unlabeled ECGs improves diagnostic performance, existing approaches do not incorporate domain knowledge from clinical metadata. We introduce a novel contrastive learning approach that utilizes an established clinical risk score to adaptively weight negative pairs: clinically-guided contrastive learning. It aligns the similarities of ECG embeddings with clinically meaningful differences between subjects, with an explicit mechanism to handle missing metadata. On 12-lead ECGs from 161K patients in the MIMIC-IV dataset, we pretrain single-lead ECG foundation models at three scales, collectively called CLEF, using only routinely collected metadata without requiring per-sample ECG annotations. We evaluate CLEF on 18 clinical classification and regression tasks across 7 held-out datasets, and benchmark against 5 foundation model baselines and 3 self-supervised algorithms. When pretrained on 12-lead ECG data and tested on lead-I data, CLEF outperforms self-supervised foundation model baselines: the medium-sized CLEF achieves average AUROC improvements of at least 2.6% in classification and average reductions in MAEs of at least 3.2% in regression. Comparing with existing self-supervised learning algorithms, CLEF improves the average AUROC by at least 1.8%. Moreover, when pretrained only on lead-I data for classification tasks, CLEF performs comparably to the state-of-the-art ECGFounder, which was trained in a supervised manner. Overall, CLEF enables more accurate and scalable single-lead ECG analysis, advancing remote health monitoring. Code and pretrained CLEF models are available at: github.com/Nokia-Bell-Labs/ecg-foundation-model.

</details>


### [9] [Enforcing Orderedness to Improve Feature Consistency](https://arxiv.org/abs/2512.02194)
*Sophie L. Wang,Alex Quach,Nithin Parsan,John J. Yang*

Main category: cs.LG

TL;DR: OSAE introduces an explicit ordering of latent features and deterministic use of all dimensions to improve identifiability and reproducibility of sparse autoencoders, with theoretical guarantees and improved consistency over Matryoshka baselines on large models.


<details>
  <summary>Details</summary>
Motivation: Sparse autoencoders offer interpretability but feature representations vary with seeds and hyperparameters due to permutation non-identifiability and sampling in nested methods. A stable, ordered, fully-utilized latent representation is desirable for reproducibility and interpretability.

Method: Propose Ordered Sparse Autoencoders (OSAE) by extending Matryoshka SAEs with (1) a strict ordering constraint on latent features and (2) deterministic usage of every feature dimension, removing sampling-based approximations. Provide theoretical analysis showing resolution of permutation non-identifiability in sparse dictionary learning where solutions are unique up to symmetries. Empirically evaluate on Gemma2-2B and Pythia-70M against Matryoshka baselines.

Result: Theoretical: OSAEs resolve permutation non-identifiability in settings with unique solutions (up to natural symmetries). Empirical: on Gemma2-2B and Pythia-70M, OSAEs improve consistency vs Matryoshka baselines.

Conclusion: OSAE offers more stable, interpretable representations by enforcing latent-feature ordering and deterministic utilization, reducing seed/hyperparameter-induced variability; demonstrated effectiveness on large-scale datasets.

Abstract: Sparse autoencoders (SAEs) have been widely used for interpretability of neural networks, but their learned features often vary across seeds and hyperparameter settings. We introduce Ordered Sparse Autoencoders (OSAE), which extend Matryoshka SAEs by (1) establishing a strict ordering of latent features and (2) deterministically using every feature dimension, avoiding the sampling-based approximations of prior nested SAE methods. Theoretically, we show that OSAEs resolve permutation non-identifiability in settings of sparse dictionary learning where solutions are unique (up to natural symmetries). Empirically on Gemma2-2B and Pythia-70M, we show that OSAEs can help improve consistency compared to Matryoshka baselines.

</details>


### [10] [WhAM: Towards A Translative Model of Sperm Whale Vocalization](https://arxiv.org/abs/2512.02206)
*Orr Paradise,Pranav Muralikrishnan,Liangyuan Chen,Hugo Flores García,Bryan Pardo,Roee Diamant,David F. Gruber,Shane Gero,Shafi Goldwasser*

Main category: cs.LG

TL;DR: WhAM, a transformer-based generator for sperm whale codas, fine-tuned from VampNet on 10k codas, producing high-fidelity synthetic codas; evaluated with Fréchet Audio Distance and expert perceptual studies; representations good for rhythm, social unit, and vowel classification; code released.


<details>
  <summary>Details</summary>
Motivation: Address the lack of generative models for sperm whale codas and explore whether transfer learning from musical audio can yield high-quality bioacoustic generation and useful representations for downstream tasks.

Method: Fine-tune a masked acoustic token model (VampNet) on ~10k coda recordings; employ iterative masked token prediction to generate codas; utilize transformer-based generation; evaluate with Fréchet Audio Distance and perceptual tests by expert marine biologists; assess learned representations on rhythm, social unit, and vowel classification.

Result: WhAM can generate high-fidelity synthetic codas that preserve key acoustic features of source recordings; learned representations achieve strong performance on downstream tasks (rhythm, social unit, vowel classification); code available at GitHub.

Conclusion: Demonstrates feasibility of transformer-based generation for marine mammal codas, with potential applications in data augmentation and exploratory bioacoustics; encourages further work on generalization, ecological validity, and broader evaluation.

Abstract: Sperm whales communicate in short sequences of clicks known as codas. We present WhAM (Whale Acoustics Model), the first transformer-based model capable of generating synthetic sperm whale codas from any audio prompt. WhAM is built by finetuning VampNet, a masked acoustic token model pretrained on musical audio, using 10k coda recordings collected over the past two decades. Through iterative masked token prediction, WhAM generates high-fidelity synthetic codas that preserve key acoustic features of the source recordings. We evaluate WhAM's synthetic codas using Fréchet Audio Distance and through perceptual studies with expert marine biologists. On downstream classification tasks including rhythm, social unit, and vowel classification, WhAM's learned representations achieve strong performance, despite being trained for generation rather than classification. Our code is available at https://github.com/Project-CETI/wham

</details>


### [11] [Decentralized Fairness Aware Multi Task Federated Learning for VR Network](https://arxiv.org/abs/2512.02513)
*Krishnendu S. Tharakan,Carlo Fischione*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms.

</details>


### [12] [InstructLR: A Scalable Approach to Create Instruction Dataset for Under-Resourced Languages](https://arxiv.org/abs/2512.02213)
*Mamadou K. Keita,Sebastien Diarra,Christopher Homan,Seydou Diallo*

Main category: cs.LG

TL;DR: 提出 InstructLR 框架，通过联合 LLM 生成与双层质量筛选来为低资源语言（LRL）生成高质量指令数据集，产出三项 50k 规模的跨域基准数据集（ZarmaInstruct-50k、BambaraInstruct-50k、FulfuldeInstruct-50k）。


<details>
  <summary>Details</summary>
Motivation: LRL 的高质量指令数据集匮乏，难以支撑现代大语言模型的指令式能力。自动翻译和合成数据往往缺乏流畅性和正字一致性，迫切需要一个可扩展、质量可控的数据生成与筛选框架。以 MMLU 等基准启发，在多域场景下构建稳定的指令数据。

Method: 通过 LLM 进行文本生成，结合两层质量筛选：第一层是基于检索增强生成（RAG）的 n-shot 提示进行自动化筛选；第二层为人工干预的验证环节，确保数据的可用性和语言规范性。由此生成 ZarmaInstruct-50k、BambaraInstruct-50k、FulfuldeInstruct-50k 等多域指令基准。

Result: 实现了面向若干低资源语言的高质量指令数据集生成框架，并产出三项 50k 规模的多域指令基准，为 LR LLM 支持提供数据支撑与评估基准。

Conclusion: InstructLR 提供了一种可扩展的低资源语言指令数据生成与质量控制的框架，结合自动化与人工校验，降低了高质量数据的获取门槛，预计提升 LR LLM 在指令遵循与多域任务上的表现。

Abstract: Effective text generation and chat interfaces for low-resource languages (LRLs) remain a challenge for state-of-the-art large language models (LLMs) to support. This is mainly due to the difficulty of curating high-quality instruction datasets for LRLs, a limitation prevalent in the languages spoken across the African continent and other regions. Current approaches, such as automated translation and synthetic data generation, frequently yield outputs that lack fluency or even orthographic consistency. In this paper, we introduce InstructLR, a novel framework designed to generate high-quality instruction datasets for LRLs. Our approach integrates LLM-driven text generation with a dual-layer quality filtering mechanism: an automated filtering layer based on retrieval-augmented-generation (RAG)-based n-shot prompting, and a human-in-the-loop validation layer. Drawing inspiration from benchmarks such as MMLU in task definition, InstructLR has facilitated the creation of three multi-domain instruction benchmarks: ZarmaInstruct-50k, BambaraInstruct-50k, and FulfuldeInstruct-50k.

</details>


### [13] [Adversarial Jamming for Autoencoder Distribution Matching](https://arxiv.org/abs/2512.02740)
*Waleed El-Geresy,Deniz Gündüz*

Main category: cs.LG

TL;DR: 提出将对抗性无线干扰作为正则化手段，通过使潜在空间分布接近对角高斯分布来实现分布匹配，与标准变分自编码器和WAE的结果相当，且可推广至其他潜在分布。


<details>
  <summary>Details</summary>
Motivation: 解决潜在空间分布匹配问题：希望通过对抗性干扰来引导聚合后验分布接近指定的对角高斯分布，并探究在编码-解码器-干扰器的对抗性设置下的理论最优性与可应用性。

Method: 在自编码器训练中引入对抗性干扰器，该干扰器输出对角高斯噪声以干扰编码后传输的高斯源的恢复；以最小化均方误差失真为目标，同时通过干扰器的对抗性信号作为辅助目标，推动聚集潜在后验接近对角高斯分布；该推理基于相关极值理论，作为实现潜在分布匹配的新手段。

Result: 通过该方法实现的潜在分布匹配与标准VAE和WAE在性能上相当；且该方法具有泛化到其他潜在分布的潜力。

Conclusion: 证明将对抗性噪声作为潜在分布匹配的正则化手段是可行的，并为在潜在空间中实现灵活的分布匹配提供了新方向，具有广泛的应用前景。

Abstract: We propose the use of adversarial wireless jamming to regularise the latent space of an autoencoder to match a diagonal Gaussian distribution. We consider the minimisation of a mean squared error distortion, where a jammer attempts to disrupt the recovery of a Gaussian source encoded and transmitted over the adversarial channel. A straightforward consequence of existing theoretical results is the fact that the saddle point of a minimax game - involving such an encoder, its corresponding decoder, and an adversarial jammer - consists of diagonal Gaussian noise output by the jammer. We use this result as inspiration for a novel approach to distribution matching in the latent space, utilising jamming as an auxiliary objective to encourage the aggregated latent posterior to match a diagonal Gaussian distribution. Using this new technique, we achieve distribution matching comparable to standard variational autoencoders and to Wasserstein autoencoders. This approach can also be generalised to other latent distributions.

</details>


### [14] [The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning Models](https://arxiv.org/abs/2512.02265)
*Joshua Wolff Anderson,Shyam Visweswaran*

Main category: cs.LG

TL;DR: 公平性改进可能改变基于Shapley的特征排名，影响跨子组的可解释性；需要在准确性、公平性和可解释性之间进行联动评估。


<details>
  <summary>Details</summary>
Motivation: 探究通过偏差缓解提高公平性时，是否会改变Shapley基于的特征重要性排序，以及这种变化在不同种族子群和模型类型中的差异。通过三个数据集（儿科泌尿道感染风险、直接抗凝药物出血风险、再犯风险）与多种模型进行比较。

Method: 对三个数据集在应用公平性约束前后，评估Shapley基于的特征排名的变化；比较不同族裔子组之间的排名变化；在多种模型类别上测试可解释性稳定性；量化排名变化的程度。

Result: 研究发现，在提升跨种族群体的公平性时，特征重要性排名可能显著改变，且变化在不同群体间呈现差异，方向甚至可能相反；不同模型的排名稳定性存在差异。

Conclusion: 应在模型评估中综合考虑准确性、公平性和可解释性，而不仅仅单独看待其中一个维度；在应用公平性约束后，应持续关注解释的一致性和可靠性，推动建立更稳健的评估框架。

Abstract: Trustworthy machine learning in healthcare requires strong predictive performance, fairness, and explanations. While it is known that improving fairness can affect predictive performance, little is known about how fairness improvements influence explainability, an essential ingredient for clinical trust. Clinicians may hesitate to rely on a model whose explanations shift after fairness constraints are applied. In this study, we examine how enhancing fairness through bias mitigation techniques reshapes Shapley-based feature rankings. We quantify changes in feature importance rankings after applying fairness constraints across three datasets: pediatric urinary tract infection risk, direct anticoagulant bleeding risk, and recidivism risk. We also evaluate multiple model classes on the stability of Shapley-based rankings. We find that increasing model fairness across racial subgroups can significantly alter feature importance rankings, sometimes in different ways across groups. These results highlight the need to jointly consider accuracy, fairness, and explainability in model assessment rather than in isolation.

</details>


### [15] [Limitations of Membership Queries in Testable Learning](https://arxiv.org/abs/2512.02279)
*Jane Lange,Mingda Qiao*

Main category: cs.LG

TL;DR: 在 testable learning 模型下，成员查询（MQ）无法普遍降低时间复杂度，不能超越基于样本的最优 PAC 学习；通过从样本证伪到 TL-Q 的一般化归约，建立了 TL-Q 的下界；并定义了“统计型” MQ 算法，表明 TL-Q 若存在，则可导出高效的 SQ 证伪与学习，但结合 SQ 维度下界，某些 MQ 学习器不可实现可测试性。


<details>
  <summary>Details</summary>
Motivation: 探究成员查询在可测试学习中的价值，特别是在分布特定设置下是否能提升时间复杂度，以及它与证伪、统计查询等概念之间的关系。

Method: 给出从样本基础的布尔概念类证伪到 TL-Q 的通用归约；在此基础上定义统计型 MQ 算法的集合，证明 TL-Q 推导出高效的 SQ 证伪与学习；利用已知的 SQ 维度下界得出不可实现的可测试性结论。

Result: 相对于给定概念类与分布族，没有 m-样本的 TL-Q 算法能在时间上多项式超越最佳的 m-样本 PAC 学习；对统计型 MQ 算法，TL-Q 意味着存在高效的 SQ 证伪与学习算法；结合 SQ 维度下界，意味着这些高效的 MQ 学习器不能被转化为可测试学习。

Conclusion: MQ 在可测试学习中的潜力受限，尤其在与证伪和 SQ 维度下界相结合时；这类工作表明要实现可测试性，需超越现有的统计型 MQ 框架或找到新的突破性方法。

Abstract: Membership queries (MQ) often yield speedups for learning tasks, particularly in the distribution-specific setting. We show that in the \emph{testable learning} model of Rubinfeld and Vasilyan [RV23], membership queries cannot decrease the time complexity of testable learning algorithms beyond the complexity of sample-only distribution-specific learning. In the testable learning model, the learner must output a hypothesis whenever the data distribution satisfies a desired property, and if it outputs a hypothesis, the hypothesis must be near-optimal.
  We give a general reduction from sample-based \emph{refutation} of boolean concept classes, as presented in [Vadhan17, KL18], to testable learning with queries (TL-Q). This yields lower bounds for TL-Q via the reduction from learning to refutation given in [KL18]. The result is that, relative to a concept class and a distribution family, no $m$-sample TL-Q algorithm can be super-polynomially more time-efficient than the best $m$-sample PAC learner.
  Finally, we define a class of ``statistical'' MQ algorithms that encompasses many known distribution-specific MQ learners, such as those based on influence estimation or subcube-conditional statistical queries. We show that TL-Q algorithms in this class imply efficient statistical-query refutation and learning algorithms. Thus, combined with known SQ dimension lower bounds, our results imply that these efficient membership query learners cannot be made testable.

</details>


### [16] [Training Dynamics of Learning 3D-Rotational Equivariance](https://arxiv.org/abs/2512.02303)
*Max W. Shen,Ewa Nowara,Michael Maser,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 3D旋转等变性学习可以快速实现，使用一个基于损失的等变误差度量，在1k-10k步内将持出损失中的非等变部分降至≤2%，非等变模型在某些情况下能以GPU时效率超越等变模型，但存在“效率差距”；论文提出了衡量等变误差的原理性指标，并讨论其与梯度和参数的关系。


<details>
  <summary>Details</summary>
Motivation: 定量评估模型对对称性（尤其是3D旋转）学习的速度和效果，提出用于衡量等变误差的原则性度量，并在高维分子任务上探索损失景观与学习动态。

Method: 推导用于凸损失的等变误差百分比度量；在流式匹配、力场预测、去噪体素等3D旋转等变任务上进行广泛实验；从理论和实验角度研究等变误差、梯度和模型参数之间的关系。

Result: 在1k-10k步内，模型将等变误差降至持出损失的≤2%，对模型大小和数据集规模具有鲁棒性；非等变模型在整个训练过程中的损失 penalty 小，可能在单位GPU小时内取得比等变模型更低的测试损失，但若降低“效率差距”，则等变方法具优势；还给出相对等变误差、学习梯度和参数之间关系的实验与理论分析。

Conclusion: 提供一个用于评估相对等变度量的框架及其对训练动力学的洞见；揭示对称性学习相对主任务的易学性，强调在设计模型时考虑等变效率差距与损失景观特征的重要性。

Abstract: While data augmentation is widely used to train symmetry-agnostic models, it remains unclear how quickly and effectively they learn to respect symmetries. We investigate this by deriving a principled measure of equivariance error that, for convex losses, calculates the percent of total loss attributable to imperfections in learned symmetry. We focus our empirical investigation to 3D-rotation equivariance on high-dimensional molecular tasks (flow matching, force field prediction, denoising voxels) and find that models reduce equivariance error quickly to $\leq$2\% held-out loss within 1k-10k training steps, a result robust to model and dataset size. This happens because learning 3D-rotational equivariance is an easier learning task, with a smoother and better-conditioned loss landscape, than the main prediction task. For 3D rotations, the loss penalty for non-equivariant models is small throughout training, so they may achieve lower test loss than equivariant models per GPU-hour unless the equivariant ``efficiency gap'' is narrowed. We also experimentally and theoretically investigate the relationships between relative equivariance error, learning gradients, and model parameters.

</details>


### [17] [Unlocking the Power of Boltzmann Machines by Parallelizable Sampler and Efficient Temperature Estimation](https://arxiv.org/abs/2512.02323)
*Kentaro Kubo,Hayato Goto*

Main category: cs.LG

TL;DR: 提出可并行的 Langevin SB 采样（LSB）用于 Boltzmann 机，与条件期望匹配（CEM）一起实现 SAL 框架，提升对一般耦合 Boltzmann 机的高效学习能力，超越 RBM。


<details>
  <summary>Details</summary>
Motivation: 解决传统 Boltzmann 机训练成本高、仅 RBM 可高效训练、需要 MCMC 采样且难以并行化的问题；需要可并行的高表达能力模型；需要稳定的逆温度估计以提升学习质量。

Method: 引入基于模拟分叉的 Langevin 采样，LSB 使并行化采样保持与 MCMC 相当的准确性，且适用于一般耦合的 Boltzmann 机；提出条件期望匹配（CEM）在学习过程中估计逆温度；将两者结合形成 sampler-adaptive learning (SAL)。

Result: LSB 能在并行环境中获得与 MCMC 相当的精度；CEM 提供高效的逆温度估计；SAL 实现对比 RBM 更强表达力的能量基生成建模，提升学习效率。

Conclusion: SAL 开辟能量基生成建模的新路径，超越 RBM，扩展可并行的高表达能力 Boltzmann 机的应用。

Abstract: Boltzmann machines (BMs) are powerful energy-based generative models, but their heavy training cost has largely confined practical use to Restricted BMs (RBMs) trained with an efficient learning method called contrastive divergence. More accurate learning typically requires Markov chain Monte Carlo (MCMC) Boltzmann sampling, but it is time-consuming due to the difficulty of parallelization for more expressive models. To address this limitation, we first propose a new Boltzmann sampler inspired by a quantum-inspired combinatorial optimization called simulated bifurcation (SB). This SB-inspired approach, which we name Langevin SB (LSB), enables parallelized sampling while maintaining accuracy comparable to MCMC. Furthermore, this is applicable not only to RBMs but also to BMs with general couplings. However, LSB cannot control the inverse temperature of the output Boltzmann distribution, which hinders learning and degrades performance. To overcome this limitation, we also developed an efficient method for estimating the inverse temperature during the learning process, which we call conditional expectation matching (CEM). By combining LSB and CEM, we establish an efficient learning framework for BMs with greater expressive power than RBMs. We refer to this framework as sampler-adaptive learning (SAL). SAL opens new avenues for energy-based generative modeling beyond RBMs.

</details>


### [18] [Retrieval-Augmented Memory for Online Learning](https://arxiv.org/abs/2512.02333)
*Wenzhang Du*

Main category: cs.LG

TL;DR: RAM-OL is a simple online retrieval-augmented learning method for non-stationary streams; it maintains a small buffer of past examples and uses nearest neighbors in the hidden space to augment updates. Two replay variants are studied: naive replay and gated replay with time/window constraints, similarity filters, and gradient reweighting. Theoretical analysis under a bounded drift model suggests retrieval can reduce adaptation cost and improve regret when patterns recur. Empirically, RAM-OL (simple online MLP) improves prequential accuracy on real streams (electricity pricing/load, airline delay) by up to ~7 percentage points and reduces seed variance; gated replay is robust on noisy data and can match purely online baselines. The work argues retrieval-augmented memory is practical for online learning under concept drift.


<details>
  <summary>Details</summary>
Motivation: To improve online classification under concept drift by leveraging a small, non-parametric memory that stores past examples and can be retrieved to assist updates, thereby reducing adaptation cost and stabilizing performance when patterns recur.

Method: Maintain a small buffer of past examples; at each time step, retrieve a few nearest neighbors of the current input in the hidden representation space and update the model on both the current example and the retrieved neighbors. Compare a naive replay variant with a gated replay variant that constrains neighbors using a time window, similarity thresholds, and gradient reweighting to balance fast reuse vs robustness to outdated regimes.

Result: Theoretical: under a bounded drift model, retrieval can reduce adaptation cost and improve regret constants when patterns recur. Empirical: on three real streams (electricity pricing, electricity load, airline delay) RAM-OL improves prequential accuracy by up to about 7 percentage points and reduces variance across seeds; on a noisy airline stream, the gated variant closely matches the purely online baseline.

Conclusion: Retrieval-augmented memory is a practical and robust tool for online learning under concept drift, with RAM-OL offering a simple extension to SGD that leverages past data for better adaptation.

Abstract: Retrieval-augmented models couple parametric predictors with non-parametric memories, but their use in streaming supervised learning with concept drift is not well understood. We study online classification in non-stationary environments and propose Retrieval-Augmented Memory for Online Learning (RAM-OL), a simple extension of stochastic gradient descent that maintains a small buffer of past examples. At each time step, RAM-OL retrieves a few nearest neighbours of the current input in the hidden representation space and updates the model jointly on the current example and the retrieved neighbours. We compare a naive replay variant with a gated replay variant that constrains neighbours using a time window, similarity thresholds, and gradient reweighting, in order to balance fast reuse of relevant past data against robustness to outdated regimes. From a theoretical perspective, we interpret RAM-OL under a bounded drift model and discuss how retrieval can reduce adaptation cost and improve regret constants when patterns recur over time. Empirically, we instantiate RAM-OL on a simple online multilayer perceptron and evaluate it on three real-world data streams derived from electricity pricing, electricity load, and airline delay data. On strongly and periodically drifting streams, RAM-OL improves prequential accuracy by up to about seven percentage points and greatly reduces variance across random seeds, while on a noisy airline stream the gated variant closely matches the purely online baseline. These results show that retrieval-augmented memory is a practical and robust tool for online learning under concept drift.

</details>


### [19] [SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification](https://arxiv.org/abs/2512.02337)
*Zhendong Tan,Xingjun Zhang,Chaoyi Hu,Junjie Peng,Kun Xia*

Main category: cs.LG

TL;DR: SpecPV 通过自我 speculate 解码和部分键值状态的快速验证，在长上下文生成中减少验证成本，周期性执行全量验证以纠正累计误差，从而在多模型和基准上实现最高约6x的解码加速，且精度下降很小。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度增加，后验验证成为长上下文生成的瓶颈；需要降低验证成本以提升推理速度。

Method: 提出 SpecPV：在 draft-verify 架构下，使用轻量化草案模型产生候选令牌并进行快速验证，采用部分 KV 状态实现快速验证，同时定期进行全量验证以消除累计的误差。

Result: 在多项长上下文基准和模型上取得加速效果，包括 LLaMA-3.1-8B-Instruct 和 Qwen3 系列，解码速度最高提升约 6 倍，伴随少量性能下降。

Conclusion: SpecPV 展示了自我 speculate 验证在降低长上下文解码成本方面的潜力，通过周期性全量验证来控制累积误差，使速度提升与精度降幅保持在可接受范围内。

Abstract: Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation.

</details>


### [20] [FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data](https://arxiv.org/abs/2512.02350)
*Nan Qiao,Sheng Yue,Ju Ren,Yaoxue Zhang*

Main category: cs.LG

TL;DR: 提出 FOVA：一种基于投票的离线联邦强化学习框架，通过投票机制在本地策略评估中识别高收益行动，减轻来自不同客户端的低质量数据的负面影响；在此基础上结合优势加权回归（AWR）构建一致的本地与全局训练目标，显著提升稳定性与效率。理论上证明学习策略对行为策略具有严格改进，实验在基准数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 在离线联邦强化学习中，混合质量的数据导致性能显著下降，亟需在跨客户端的数据异质性条件下实现鲁棒的策略学习与聚合。

Method: 在本地策略评估阶段引入投票机制以识别高回报动作，缓解低质量行为的影响；基于优势加权回归（AWR）设计一致的本地/全局训练目标，提升学习稳定性与效率。

Result: 理论分析显示 FOVA 学习得到的策略对行为策略具有严格的改进性；实验结果在广泛使用的基准上显著优于现有基线方法。

Conclusion: FOVA 有效缓解了数据质量不一致带来的负面影响，提升离线 FRL 的稳定性与效率，并提供理论保障与实证证据。

Abstract: Offline Federated Reinforcement Learning (FRL), a marriage of federated learning and offline reinforcement learning, has attracted increasing interest recently. Albeit with some advancement, we find that the performance of most existing offline FRL methods drops dramatically when provided with mixed-quality data, that is, the logging behaviors (offline data) are collected by policies with varying qualities across clients. To overcome this limitation, this paper introduces a new vote-based offline FRL framework, named FOVA. It exploits a \emph{vote mechanism} to identify high-return actions during local policy evaluation, alleviating the negative effect of low-quality behaviors from diverse local learning policies. Besides, building on advantage-weighted regression (AWR), we construct consistent local and global training objectives, significantly enhancing the efficiency and stability of FOVA. Further, we conduct an extensive theoretical analysis and rigorously show that the policy learned by FOVA enjoys strict policy improvement over the behavioral policy. Extensive experiments corroborate the significant performance gains of our proposed algorithm over existing baselines on widely used benchmarks.

</details>


### [21] [Reinforcement Learning in POMDP's via Direct Gradient Ascent](https://arxiv.org/abs/2512.02383)
*Jonathan Baxter,Peter L. Bartlett*

Main category: cs.LG

TL;DR: 提出 GPOMDP，一种用于受控部分可观测马尔可夫决策过程（POMDP）的平均回报梯度估计的 REINFORCE 风格算法。该算法只需单条样本轨迹、只引入一个β参数且无需状态信息；给出收敛性证明，并展示如何将梯度估计用于共轭梯度法以寻找局部最优解。


<details>
  <summary>Details</summary>
Motivation: 直接在部分可观测的环境中优化策略性能，避免对系统状态的完全知识或多次重采样的需求；通过引入单一的调参 β 实现偏差-方差权衡，并以无模型、直接从数据中估计梯度的方式提升学习效率。

Method: 提出 GPOMDP 算法：在带参数的随机策略下，通过对轨迹进行采样，利用一个衰减因子 β(∈[0,1)) 来控制梯度估计的偏差与方差；不需要对底层状态的知识，仅依赖观测与奖励；给出收敛性证明，并给出将该梯度估计用于共轭梯度方法以求解平均回报的局部最优解的流程。

Result: 核心优势包括：仅需要单条样本轨迹、仅用一个 β、无需状态信息；证明 GPOMDP 的收敛性；并示意如何将梯度估计用于共轭梯度以寻找平均回报的局部极值。

Conclusion: GPOMDP 提供了一种简单且样本高效的平均回报梯度估计方法，适用于部分可观测情景下的策略优化；β 作为偏差-方差权衡参数实现灵活控制，且在不需状态信息的前提下实现模型无关学习，并具备理论收敛性与可用于优化的实际通道。

Abstract: This paper discusses theoretical and experimental aspects of gradient-based approaches to the direct optimization of policy performance in controlled POMDPs. We introduce GPOMDP, a REINFORCE-like algorithm for estimating an approximation to the gradient of the average reward as a function of the parameters of a stochastic policy. The algorithm's chief advantages are that it requires only a single sample path of the underlying Markov chain, it uses only one free parameter $β\in [0,1)$, which has a natural interpretation in terms of bias-variance trade-off, and it requires no knowledge of the underlying state. We prove convergence of GPOMDP and show how the gradient estimates produced by GPOMDP can be used in a conjugate-gradient procedure to find local optima of the average reward.

</details>


### [22] [Risk-Sensitive Q-Learning in Continuous Time with Application to Dynamic Portfolio Selection](https://arxiv.org/abs/2512.02386)
*Chuhan Xie*

Main category: cs.LG

TL;DR: 在连续时间风险敏感强化学习中，若目标函数为优化确定性等价物（OCE），最优策略在增强环境下对状态-动作的马尔可夫性成立；提出基于鞅表征的 CT-RS-q 风险敏感 Q 学习算法；通过动态投资组合仿真验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 将风险敏感性引入连续时间强化学习，解决对非线性累积奖励函数的最优策略寻优，特别是在由可控的随机微分方程描述的环境中。

Method: 理论上证明当目标函数为 OCE 时，最优策略可表示为对增强环境的马尔可夫策略；提出 CT-RS-q 算法，基于新的 martingale 表征来实现风险敏感的 Q 学习。

Result: 在仿真实验的动态投资组合问题中，CT-RS-q 展现出有效性，支持所提出理论和算法设计。

Conclusion: 本工作提供了将 OCE 融入连续时间风险敏感强化学习的理论框架与可实现算法，并在金融场景展示出潜在应用价值。

Abstract: This paper studies the problem of risk-sensitive reinforcement learning (RSRL) in continuous time, where the environment is characterized by a controllable stochastic differential equation (SDE) and the objective is a potentially nonlinear functional of cumulative rewards. We prove that when the functional is an optimized certainty equivalent (OCE), the optimal policy is Markovian with respect to an augmented environment. We also propose \textit{CT-RS-q}, a risk-sensitive q-learning algorithm based on a novel martingale characterization approach. Finally, we run a simulation study on a dynamic portfolio selection problem and illustrate the effectiveness of our algorithm.

</details>


### [23] [ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity](https://arxiv.org/abs/2512.02403)
*Hongxiang Liu,Zhifang Deng,Tong Pu,Shengli Lu*

Main category: cs.LG

TL;DR: 提出 ESACT，端到端稀疏加速器，通过局部相似性 SPLS 预测注意力稀疏性，在不显著损失精度的情况下显著降低 Transformer 的计算量，且能提高能效。


<details>
  <summary>Details</summary>
Motivation: Transformer 的高计算成本阻碍高效硬件部署。现有加速器大多只利用注意力的行内稀疏，忽略行间稀疏，且对全局相似度的估计代价高，通常仅对一个或两个组件进行稀疏化，导致受限的加速收益。本工作通过对注意力分布和计算流程的分析，发现局部相似性可以实现端到端的稀疏加速，开箱即用地降低开销。

Method: 提出端到端稀疏加速器 ESACT，核心是 SPLS（Sparsity Prediction with Local Similarity）机制，利用 HLog 量化在 QK 生成前预测局部注意力稀疏性，从而在所有 Transformer 组件上实现高效稀疏化。为便于硬件实现，提出三项体系结构创新。

Result: 在 26 个基准上，SPLS 将总体计算量降低 52.03%，精度损失控制在 1% 以内。ESACT 的端到端能效达到 3.29 TOPS/W，在注意力层面相对 SOTA 加速器 SpAtten 与 Sanger的能效提升分别为 2.95x 与 2.26x。

Conclusion: ESA CT 实现了端到端的 Transformer 稀疏化加速，覆盖 QKV 生成、注意力计算与 FFN 三大组件，且通过 SPLS 在局部层面实现高效预测，结合硬件创新获得显著的计算和能效收益，展示了利用局部相似性进行全链路稀疏加速的潜力。

Abstract: Transformers, composed of QKV generation, attention computation, and FFNs,
  have become the dominant model across various domains due to their outstanding performance.
  However, their high computational cost hinders efficient hardware deployment.
  Sparsity offers a promising solution,
  yet most existing accelerators exploit only intra-row sparsity in attention,
  while few consider inter-row sparsity.
  Approaches leveraging inter-row sparsity often rely on costly global similarity estimation,
  which diminishes the acceleration benefits of sparsity,
  and typically apply sparsity to only one or two transformer components.
  Through careful analysis of the attention distribution and computation flow,
  we observe that local similarity allows end-to-end sparse acceleration with lower computational overhead.
  Motivated by this observation, we propose ESACT,
  an end-to-end sparse accelerator for compute-intensive Transformers.
  ESACT centers on the Sparsity Prediction with Local Similarity (SPLS) mechanism,
  which leverages HLog quantization to accurately predict local attention sparsity prior to QK generation,
  achieving efficient sparsity across all transformer components.
  To support efficient hardware realization, we introduce three architectural innovations.
  Experimental results on 26 benchmarks demonstrate that
  SPLS reduces total computation by 52.03% with less than 1% accuracy loss.
  ESACT achieves an end-to-end energy efficiency of 3.29 TOPS/W,
  and improves attention-level energy efficiency by 2.95x and 2.26x over
  SOTA attention accelerators SpAtten and Sanger, respectively.

</details>


### [24] [Dynamic Configuration of On-Street Parking Spaces using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2512.02406)
*Oshada Jayasinghe,Farhana Choudhury,Egemen Tanin,Shanika Karunasekera*

Main category: cs.LG

TL;DR: A scalable two-layer multi-agent reinforcement learning framework for dynamic on-street parking configuration using lane-level and block-level agents, leveraging Deep Q-learning with LSTM and Graph Attention Networks to model spatio-temporal dependencies; validated on SUMO with synthetic and Melbourne data, achieving up to 47% reduction in travel time loss with negligible walking distance increase.


<details>
  <summary>Details</summary>
Motivation: Urban traffic congestion worsens as on-street parking consumes road width. Leveraging vehicle-to-infrastructure connectivity, dynamically configuring parking spaces can alleviate congestion. The goal is a scalable, data-driven optimization framework that can adapt to large road networks.

Method: A two-layer multi-agent reinforcement learning framework where lane-level agents decide parking configurations per lane and block-level agents coordinate across blocks. The lane-level agents use a novel Deep Q-learning architecture that combines long short-term memory networks and graph attention networks to capture spatio-temporal correlations. Block-level agents regulate lane-level actions to ensure sufficient parking around blocks. Evaluation is conducted in SUMO on synthetic and Melbourne real-world data.

Result: Experiments show the proposed framework can reduce average travel time loss by up to 47%, with only a negligible increase in walking distance for parking.

Conclusion: Hierarchical, data-driven RL for dynamic on-street parking is effective and scalable, significantly mitigating travel time loss while preserving parking accessibility, and is suitable for large urban networks.

Abstract: With increased travelling needs more than ever, traffic congestion has become a major concern in most urban areas. Allocating spaces for on-street parking, further hinders traffic flow, by limiting the effective road width available for driving. With the advancement of vehicle-to-infrastructure connectivity technologies, we explore how the impact of on-street parking on traffic congestion could be minimized, by dynamically configuring on-street parking spaces. Towards that end, we formulate dynamic on-street parking space configuration as an optimization problem, and we follow a data driven approach, considering the nature of our problem. Our proposed solution comprises a two-layer multi agent reinforcement learning based framework, which is inherently scalable to large road networks. The lane level agents are responsible for deciding the optimal parking space configuration for each lane, and we introduce a novel Deep Q-learning architecture which effectively utilizes long short term memory networks and graph attention networks to capture the spatio-temporal correlations evident in the given problem. The block level agents control the actions of the lane level agents and maintain a sufficient level of parking around the block. We conduct a set of comprehensive experiments using SUMO, on both synthetic data as well as real-world data from the city of Melbourne. Our experiments show that the proposed framework could reduce the average travel time loss of vehicles significantly, reaching upto 47%, with a negligible increase in the walking distance for parking.

</details>


### [25] [Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data Filtering](https://arxiv.org/abs/2512.02435)
*Zhongjian Qiao,Rui Yang,Jiafei Lyu,Chenjia Bai,Xiu Li,Zhuoran Yang,Siyang Gao,Shuang Qiu*

Main category: cs.LG

TL;DR: 提出 DVDF 的跨域离线强化学习方法，通过对源域数据进行动态对齐和价值对齐的筛选，避免仅凭动态对齐导致的次优，并在极低数据量目标域场景下也能显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 跨域离线 RL 面临源域和目标域动力学错配，单纯合并数据往往性能下降。现有方法多关注动力学对齐，忽略在源域中筛选高价值样本。需要同时考虑价值信息以提升策略学习效果；理论上给出在仅用源域训练并在目标域评估的子最优界，以及引出对两类对齐的必要性。

Method: 提出 DVDF：动态与价值对齐的数据筛选方法，在源域数据中筛选出既与目标域动力学对齐、又具有较高回报价值的样本，进行共享与混合。设计覆盖多种动力学偏移设置（运动学、形态学等）的实验，且在目标域数据极少（仅 5,000 条转移）时评估其鲁棒性。

Result: 实验证明 DVDF 在多任务/数据集上持续超越强基线，显著提升跨域离线 RL 的性能。

Conclusion: 同时考虑动力学对齐和价值对齐是跨域离线强化学习的关键，DVDF 的筛选策略能在不同动力学偏移和极低数据条件下实现稳健提升。

Abstract: Cross-Domain Offline Reinforcement Learning aims to train an agent deployed in the target environment, leveraging both a limited target domain dataset and a source domain dataset with (possibly) sufficient data coverage. Due to the underlying dynamics misalignment between the source and target domain, simply merging the data from two datasets may incur inferior performance. Recent advances address this issue by selectively sharing source domain samples that exhibit dynamics alignment with the target domain. However, these approaches focus solely on dynamics alignment and overlook \textit{value alignment}, i.e., selecting high-quality, high-value samples from the source domain. In this paper, we first demonstrate that both dynamics alignment and value alignment are essential for policy learning, by examining the limitations of the current theoretical framework for cross-domain RL and establishing a concrete sub-optimality gap of a policy trained on the source domain and evaluated on the target domain. Motivated by the theoretical insights, we propose to selectively share those source domain samples with both high dynamics and value alignment and present our \textbf{\underline{D}}ynamics- and \textbf{\underline{V}}alue-aligned \textbf{\underline{D}}ata \textbf{\underline{F}}iltering (DVDF) method. We design a range of dynamics shift settings, including kinematic and morphology shifts, and evaluate DVDF on various tasks and datasets, as well as in challenging extremely low-data settings where the target domain dataset contains only 5,000 transitions. Extensive experiments demonstrate that DVDF consistently outperforms prior strong baselines and delivers exceptional performance across multiple tasks and datasets.

</details>


### [26] [TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links](https://arxiv.org/abs/2512.02465)
*Xingwang Li,Mengyun Chen,Jiamou Liu,Sijie Wang,Shuanggen Jin,Jafet C. M. Andersson,Jonas Olsson,Remco,van de Beek,Hai Victor Habi,Congzheng Han*

Main category: cs.LG

TL;DR: 提出了一种名为 TabGRU 的混合深度学习架构，将 Transformer 与 BiGRU 结合，用于 CML 数据的城市降雨监测。通过可学习的位置嵌入和注意力池化提升特征提取与泛化，在哥德堡数据集上取得优于基线和物理模型的表现，鲁棒性较好，能缓解峰值降雨时的过度估计。


<details>
  <summary>Details</summary>
Motivation: 在全球城市化加速和极端天气增多的背景下，需要高分辨率的城市降雨监测。商业微波链路（CML）虽有潜力，但基于物理模型的降雨提取在实际噪声和非线性衰减下表现有限，因此需要一个能同时捕捉长时依赖与局部序列特征的强大模型。

Method: 提出 TabGRU，融合 Transformer 与双向门控循环单元（BiGRU），通过可学习的位置嵌入和注意力汇聚机制提升动态特征提取与泛化。基于 Gothenburg 公共基准数据集（2015 年 6–9 月），在 12 条子链路、来自 Torp 与 Barl 两个雨量计的测试区间（8 月 22–31 日，约 10 次降雨事件）进行评估。

Result: 相较于深度学习基线，TabGRU 表现稳定，R^2 在 Torp 为 0.91，在 Barl 为 0.96；比物理模型（PL）在峰值降雨时对过估计问题有显著缓解，总体保持更高准确性。

Conclusion: TabGRU 能有效克服传统方法的局限性，在 tested 条件下为基于 CML 的城市降雨监测提供鲁棒、准确的解决方案。

Abstract: In the face of accelerating global urbanization and the increasing frequency of extreme weather events, highresolution urban rainfall monitoring is crucial for building resilient smart cities. Commercial Microwave Links (CMLs) are an emerging data source with great potential for this task.While traditional rainfall retrieval from CMLs relies on physicsbased models, these often struggle with real-world complexities like signal noise and nonlinear attenuation. To address these limitations, this paper proposes a novel hybrid deep learning architecture based on the Transformer and a Bidirectional Gated Recurrent Unit (BiGRU), which we name TabGRU. This design synergistically captures both long-term dependencies and local sequential features in the CML signal data. The model is further enhanced by a learnable positional embedding and an attention pooling mechanism to improve its dynamic feature extraction and generalization capabilities. The model was validated on a public benchmark dataset from Gothenburg, Sweden (June-September 2015). The evaluation used 12 sub-links from two rain gauges (Torp and Barl) over a test period (August 22-31) covering approximately 10 distinct rainfall events. The proposed TabGRU model demonstrated consistent advantages, outperforming deep learning baselines and achieving high coefficients of determination (R2) at both the Torp site (0.91) and the Barl site (0.96). Furthermore, compared to the physics-based approach, TabGRU maintained higher accuracy and was particularly effective in mitigating the significant overestimation problem observed in the PL model during peak rainfall events. This evaluation confirms that the TabGRU model can effectively overcome the limitations of traditional methods, providing a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.

</details>


### [27] [Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts](https://arxiv.org/abs/2512.02486)
*Zhongjian Qiao,Rui Yang,Jiafei Lyu,Xiu Li,Zhongxiang Dai,Zhuoran Yang,Siyang Gao,Shuang Qiu*

Main category: cs.LG

TL;DR: 提出一种在跨域离线强化学习中提升训练时和测试时对动力学偏移鲁棒性的算法DROCO，核心是鲁棒跨域Bellman（RCB）运算符，并结合动态值惩罚与Huber损失以缓解值函数估计偏差。


<details>
  <summary>Details</summary>
Motivation: 解决单域离线RL中数据覆盖不足导致的性能瓶颈，以及跨域离线RL在训练阶段引入动力学偏移时的鲁棒性不足问题，并延伸到测试阶段对动力学扰动的鲁棒性。

Method: 提出鲁棒跨域Bellman（RCB）运算符，使训练时对未知或分布外的动力学转移保持保守性以确保鲁棒性，同时提高对测试时动力学扰动的鲁棒性。为避免RCB引起的值估计偏差，结合动态值惩罚与Huber损失，形成实际可用的DROCO框架。

Result: 在多种动力学偏移场景下，DROCO相对于强基线表现更好，对动力学扰动具有更强的鲁棒性。

Conclusion: DROCO能够双重鲁棒化跨域离线强化学习，提升在动态变化环境中的稳定性和性能，具备较强的应用潜力。

Abstract: Single-domain offline reinforcement learning (RL) often suffers from limited data coverage, while cross-domain offline RL handles this issue by leveraging additional data from other domains with dynamics shifts. However, existing studies primarily focus on train-time robustness (handling dynamics shifts from training data), neglecting the test-time robustness against dynamics perturbations when deployed in practical scenarios. In this paper, we investigate dual (both train-time and test-time) robustness against dynamics shifts in cross-domain offline RL. We first empirically show that the policy trained with cross-domain offline RL exhibits fragility under dynamics perturbations during evaluation, particularly when target domain data is limited. To address this, we introduce a novel robust cross-domain Bellman (RCB) operator, which enhances test-time robustness against dynamics perturbations while staying conservative to the out-of-distribution dynamics transitions, thus guaranteeing the train-time robustness. To further counteract potential value overestimation or underestimation caused by the RCB operator, we introduce two techniques, the dynamic value penalty and the Huber loss, into our framework, resulting in the practical \textbf{D}ual-\textbf{RO}bust \textbf{C}ross-domain \textbf{O}ffline RL (DROCO) algorithm. Extensive empirical results across various dynamics shift scenarios show that DROCO outperforms strong baselines and exhibits enhanced robustness to dynamics perturbations.

</details>


### [28] [Hybrid(Penalized Regression and MLP) Models for Outcome Prediction in HDLSS Health Data](https://arxiv.org/abs/2512.02489)
*Mithra D K*

Main category: cs.LG

TL;DR: 混合模型对NHANES糖尿病预测有改进：基线模型与以XGBoost编码器为特征的MLP头相结合，提升AUC与平衡准确率，且提供代码以便复现。


<details>
  <summary>Details</summary>
Motivation: 在NHANES健康调查数据上应用成熟的机器学习技术来预测糖尿病状态，并系统比较基线模型与混合模型的性能，强调可复现性。

Method: 比较基线模型：逻辑回归、随机森林、XGBoost；提出混合方法：用XGBoost作为特征编码器，再接一个轻量级的多层感知机(MLP)头。对经过处理的NHANES子集进行实验评估，以AUC和平衡准确率衡量性能，并发布可复现的代码与脚本。

Result: 混合模型在处理后的NHANES子集上，相比基线模型获得更高的AUC和更好的平衡准确率。

Conclusion: 在糖尿病预测任务中，采用编码器+MLP头的混合方法可带来性能提升，研究者同时公开代码以促进结果复现。

Abstract: I present an application of established machine learning techniques to NHANES health survey data for predicting diabetes status. I compare baseline models (logistic regression, random forest, XGBoost) with a hybrid approach that uses an XGBoost feature encoder and a lightweight multilayer perceptron (MLP) head. Experiments show the hybrid model attains improved AUC and balanced accuracy compared to baselines on the processed NHANES subset. I release code and reproducible scripts to encourage replication.

</details>


### [29] [A Fully First-Order Layer for Differentiable Optimization](https://arxiv.org/abs/2512.02494)
*Zihao Zhao,Kai-Chia Mo,Shing-Hei Ho,Brandon Amos,Kai Wang*

Main category: cs.LG

TL;DR: 提出一种仅使用一阶信息的差分优化层梯度计算方法：将优化问题改写为双层结构，利用活跃集拉格朗日超梯度子程序避免海森矩阵计算，理论上给出非渐近的时间复杂度保证，并给出开源实现。


<details>
  <summary>Details</summary>
Motivation: 传统的隐式求导在计算梯度时需要求解海森矩阵相关的线性方程组，计算量和内存开销高。通过将差分优化问题转化为双层优化并采用一阶信息，可显著降低成本并获得理论保证。

Method: 将差分优化视为双层优化，提出一个活跃集拉格朗日超梯度哈希oracle，避免海森矩阵计算，提供有限时间、非渐近的近似保证。以仅使用一阶信息在 (1) 时间内近似超梯度，并得到对约束双层优化的总体复杂度为 (δ^{-1}ε^{-3})，与非光滑非凸优化的最佳已知率一致。公开实现库，基于现有求解器改造即可使用。

Result: 可在仅使用一阶信息的条件下，近似超梯度的时间复杂度为 (1)，总体复杂度为 (δ^{-1}ε^{-3})，与现有非光滑非凸优化的最佳速率相匹配。还提供了一个易于移植的开源Python库。

Conclusion: 为差分优化层提供更高效的梯度计算方案，避免海森矩阵带来的计算负担，并且具备明确的理论收敛/近似保证与可复现的实现。

Abstract: Differentiable optimization layers enable learning systems to make decisions by solving embedded optimization problems. However, computing gradients via implicit differentiation requires solving a linear system with Hessian terms, which is both compute- and memory-intensive. To address this challenge, we propose a novel algorithm that computes the gradient using only first-order information. The key insight is to rewrite the differentiable optimization as a bilevel optimization problem and leverage recent advances in bilevel methods. Specifically, we introduce an active-set Lagrangian hypergradient oracle that avoids Hessian evaluations and provides finite-time, non-asymptotic approximation guarantees. We show that an approximate hypergradient can be computed using only first-order information in $\tilde{\oo}(1)$ time, leading to an overall complexity of $\tilde{\oo}(δ^{-1}ε^{-3})$ for constrained bilevel optimization, which matches the best known rate for non-smooth non-convex optimization. Furthermore, we release an open-source Python library that can be easily adapted from existing solvers. Our code is available here: https://github.com/guaguakai/FFOLayer.

</details>


### [30] [Water Quality Estimation Through Machine Learning Multivariate Analysis](https://arxiv.org/abs/2512.02508)
*Marco Cardia,Stefano Chessa,Alessio Micheli,Antonella Giuliana Luminare,Francesca Gambineri*

Main category: cs.LG

TL;DR: UV-Vis spectroscopy combined with machine learning and SHAP enables rapid, interpretable water quality assessment for the agrifood sector.


<details>
  <summary>Details</summary>
Motivation: 水质对于农产品领域至关重要，水在灌溉、畜禽养殖和加工中广泛使用。随着数字化推进，需快速、可解释的水质评估以确保安全和合规。

Method: 将紫外-可见光谱数据与机器学习模型结合，并使用 SHAP 解释模型对不同波长吸收度的贡献，从而实现可解释的水质参数预测。

Result: 所提出的方法展示出在快速、准确且可解释地评估关键水质参数方面的潜力，但摘要未给出具体数值结果。

Conclusion: 该框架有助于保障水安全与法规合规，并能推动农食供应链中更快速的决策。

Abstract: The quality of water is key for the quality of agrifood sector. Water is used in agriculture for fertigation, for animal husbandry, and in the agrifood processing industry. In the context of the progressive digitalization of this sector, the automatic assessment of the quality of water is thus becoming an important asset. In this work, we present the integration of Ultraviolet-Visible (UV-Vis) spectroscopy with Machine Learning in the context of water quality assessment aiming at ensuring water safety and the compliance of water regulation. Furthermore, we emphasize the importance of model interpretability by employing SHapley Additive exPlanations (SHAP) to understand the contribution of absorbance at different wavelengths to the predictions. Our approach demonstrates the potential for rapid, accurate, and interpretable assessment of key water quality parameters.

</details>


### [31] [CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning](https://arxiv.org/abs/2512.02551)
*Songqiao Su,Xiaofei Sun,Xiaoya Li,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.LG

TL;DR: A system CUDA-L2 uses大语言模型(LLMs)与强化学习(RL)结合，自动优化HGEMM CUDA内核在1000种配置上的性能，通过RL奖励信号的CUDA执行速度实现自动化调优，显著超越主流基线。


<details>
  <summary>Details</summary>
Motivation: 解决HGEMM等高性能矩阵乘法在庞大配置空间中的优化难题；现有cuBLAS/ cuBLASLt等库的算法选择和实现未必能在特定输入、布局与精度下达到最优；利用LLM与RL在大规模配置搜索中引导优化，提升推理/训练中的实际性能。

Method: 将大语言模型与强化学习结合，构建CUDA-L2系统；在1000种配置中系统地探索，以CUDA执行速度作为强化学习奖励信号；离线模式下连续执行内核并评估性能；服务器模式模拟真实在线推理场景。与基线比较包括torch.matmul、cuBLAS、cuBLASLt的启发式及AutoTuning等策略。

Result: 离线模式：相比torch.matmul提升+22.0%；相对cuBLAS提升+19.2%（最优布局NN和TN）；相对cuBLASLt-heuristic提升+16.8%；相对cuBLASLt-AutoTuning提升+11.4%。服务器模式：相对torch.matmul提升+28.7%；相对cuBLAS提升+26.0%；相对cuBLASLt-heuristic提升+22.4%；相对cuBLASLt-AutoTuning提升+15.9%。

Conclusion: LLM-guided RL能够系统性地在大尺寸的配置空间中优化性能关键的HGEMM内核，获得显著速度提升，证明了LLM/ RL在高性能计算优化中的实际可行性，并给出可复用的代码与实现。

Abstract: In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\it cuBLAS}, {\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\% over {\it torch.matmul} on average; +19.2\% over {\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\% over {\it cuBLASLt-heuristic}, which queries {\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\% over the most competitive {\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\%, +26.0\%, +22.4\%, and +15.9\% for {\it torch.matmul}, {\it cuBLAS}, {\it cuBLASLt-heuristic}, and {\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2

</details>


### [32] [GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies](https://arxiv.org/abs/2512.02581)
*Chubin Zhang,Zhenglin Wan,Feng Chen,Xingrui Yu,Ivor Tsang,Bo An*

Main category: cs.LG

TL;DR: GoRL通过将优化与生成分离，学习一个可 tractable 的潜在策略并利用条件生成解码器合成动作，从而在在线强化学习中实现稳定且表达力强的策略，超越高斯策略与近期生成策略基线， HopperStand 达到 >870 的归一化回报（>3x 最强基线）


<details>
  <summary>Details</summary>
Motivation: 在 RL 中，稳定/简单的策略易于优化但难以表示多模态行动分布；而基于扩散/流量匹配的生成策略虽具表达力，但在线学习中因难以处理的似然性和深层采样链带来的梯度噪声而不稳定。需要一种在优化与生成之间解耦的结构原则。

Method: 提出 GoRL 框架：通过两时间尺度更新，优化一个可 tractable 的潜在策略，同时使用条件生成解码器来合成动作，使得潜在策略学习稳定而解码器逐步提升表达力；无需可 tractable 的行动似然。

Result: 在多任务连续控制上，GoRL 在多项基线上取得更好表现；在 HopperStand 任务中归一化回报超过 870，是最强基线的三倍以上。

Conclusion: 将优化与生成分离提供一条实用路径，兼具稳定性和高度表达力，适用于需要复杂行动分布的在线强化学习场景。

Abstract: Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.

</details>


### [33] [Modeling and Inverse Identification of Interfacial Heat Conduction in Finite Layer and Semi-Infinite Substrate Systems via a Physics-Guided Neural Framework](https://arxiv.org/abs/2512.02618)
*Wenhao Sha,Tienchong Chang*

Main category: cs.LG

TL;DR: HeatTransFormer 以物理引导的 Transformer 框架解决芯片-基板界面的扩散问题，结合物理信息采样、拉普拉斯激活和无掩膜注意力，实现前后向建模与三种待识别热参数的稳健逆推。


<details>
  <summary>Details</summary>
Motivation: 芯片层与半无限基板间的强热耦合导致界面处的高梯度和对界面的敏感性；传统数值求解需大量网格，PINN 在界面区域易失稳或失去物理一致性，需更稳健、统一的前/后向建模方法。

Method: 提出 HeatTransFormer：在时空域引入物理信息采样、使用仿真扩散解的拉普拉斯激活函数、以及支持双向时空耦合的无掩膜注意力。该框架可解决界面处的梯度问题，保持物理一致性，并在 PINN 失效区间表现稳定。与物理约束的逆推策略结合，能够仅基于外部测量识别三个未知热参数。

Result: 在有限层与半无限基底配置下，HeatTransFormer 产生连贯的界面温度场，具备对界面的良好描述能力；前向求解实现对界面的高梯度区域的稳定重现，逆推阶段可稳健识别三种未知热参数。

Conclusion: 物理引导的 Transformer 架构为界面支配型热传导问题提供了一个统一的前向与逆向建模框架，可应用于接口强耦合的热系统。

Abstract: Heat transfer in semiconductor devices is dominated by chip and substrate assemblies, where heat generated within a finite chip layer dissipates into a semi-infinite substrate with much higher thermophysical properties. This mismatch produces steep interfacial temperature gradients, making the transient thermal response highly sensitive to the interface. Conventional numerical solvers require excessive discretization to resolve these dynamics, while physics-informed neural networks (PINNs) often exhibit unstable convergence and loss of physical consistency near the material interface. To address these challenges, we introduce HeatTransFormer, a physics-guided Transformer architecture for interface-dominated diffusion problems. The framework integrates physically informed spatiotemporal sampling, a Laplace-based activation emulating analytical diffusion solutions, and a mask-free attention mechanism supporting bidirectional spatiotemporal coupling. These components enable the model to resolve steep gradients, maintain physical consistency, and remain stable where PINNs typically fail. HeatTransFormer produces coherent temperature fields across the interface when applied to a finite layer and semi-infinite substrate configuration. Coupled with a physics-constrained inverse strategy, it further enables reliable identification of three unknown thermal properties simultaneously using only external measurements. Overall, this work demonstrates that physics-guided Transformer architectures provide a unified framework for forward and inverse modeling in interface-dominated thermal systems.

</details>


### [34] [Adaptive Weighted LSSVM for Multi-View Classification](https://arxiv.org/abs/2512.02653)
*Farnaz Faramarzi Lighvan,Mehrdad Asadi,Lynn Houthuys*

Main category: cs.LG

TL;DR: 提出自适应加权的 LS-SVM（AW-LSSVM）用于多视图学习，通过迭代全局耦合促使各视图关注其他视图的难样本，从而实现互补学习并提升核方法的多视图性能，同时保持原始特征独立以利隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有基于核的多视图方法多采用简单融合或缺乏显式的跨视图协作/共正则化，导致全局层面的协作受限。需引入机制以强化视图之间的互补与对困难样本的关注。

Method: 提出 AW-LSSVM，为多视图学习设计自适应加权的 LS-SVM，并通过迭代的全局耦合机制，使每个视图在每轮迭代中关注其他视图的难样本，从而实现互补学习；在保持原始特征独立的同时实现跨视图协作，具备隐私友好特性。

Result: 在多数公开数据集上，该方法优于现有核基多视图方法，证明了跨视图全局耦合与互补学习的有效性，同时由于维持原始特征独立，适合作为隐私保护场景的基础。

Conclusion: 通过全局耦合和自适应加权，AW-LSSVM 实现了跨视图的互补学习与协作提升，提升了核方法的多视图性能并保持潜在的隐私友好性。

Abstract: Multi-view learning integrates diverse representations of the same instances to improve performance. Most existing kernel-based multi-view learning methods use fusion techniques without enforcing an explicit collaboration type across views or co-regularization which limits global collaboration. We propose AW-LSSVM, an adaptive weighted LS-SVM that promotes complementary learning by an iterative global coupling to make each view focus on hard samples of others from previous iterations. Experiments demonstrate that AW-LSSVM outperforms existing kernel-based multi-view methods on most datasets, while keeping raw features isolated, making it also suitable for privacy-preserving scenarios.

</details>


### [35] [Distill, Forget, Repeat: A Framework for Continual Unlearning in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.02657)
*Naveen George,Naoki Murata,Yuhta Takida,Konda Reddy Mopuri,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出一个基于教师-学生蒸馏的持续性未学习框架，在连续删除请求下实现目标化且稳定的未学习，适用于大型生成模型，且显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 随着视觉生成模型在大规模网页数据上训练，数据隐私与版权法规（如 GDPR 的“被遗忘权”）对模型提出持续的数据删除需求。现有机器忘记（MU）方法多为一次性，无法应对连续的删除请求，易陷入稳定性崩溃、对保留概念的干扰和生成质量下降等问题。需发展能在连续删除情境下保持模型整体性与可用性的持续未学习（CUL）方法。

Method: 把每一次未学习步骤重新表述为一个多目标的教师-学生蒸馏过程，结合持续学习的原则，通过生成蒸馏实现对目标概念的遗忘同时最大程度保留保留概念及图像质量，且在一个10步的连续基准上进行评估。

Result: 在一个10步的连续基准上，提出的方法能够以较高保真度忘记目标概念，同时对保留概念和整体图像质量干扰显著降低，显著优于基线方法。

Conclusion: 给出一种可落地的持续未学习框架，为大型生成模型在现实部署中满足持续数据删除请求提供可行且有效的解决路径。

Abstract: The recent rapid growth of visual generative models trained on vast web-scale datasets has created significant tension with data privacy regulations and copyright laws, such as GDPR's ``Right to be Forgotten.'' This necessitates machine unlearning (MU) to remove specific concepts without the prohibitive cost of retraining. However, existing MU techniques are fundamentally ill-equipped for real-world scenarios where deletion requests arrive sequentially, a setting known as continual unlearning (CUL). Naively applying one-shot methods in a continual setting triggers a stability crisis, leading to a cascade of degradation characterized by retention collapse, compounding collateral damage to related concepts, and a sharp decline in generative quality. To address this critical challenge, we introduce a novel generative distillation based continual unlearning framework that ensures targeted and stable unlearning under sequences of deletion requests. By reframing each unlearning step as a multi-objective, teacher-student distillation process, the framework leverages principles from continual learning to maintain model integrity. Experiments on a 10-step sequential benchmark demonstrate that our method unlearns forget concepts with better fidelity and achieves this without significant interference to the performance on retain concepts or the overall image quality, substantially outperforming baselines. This framework provides a viable pathway for the responsible deployment and maintenance of large-scale generative models, enabling industries to comply with ongoing data removal requests in a practical and effective manner.

</details>


### [36] [Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents](https://arxiv.org/abs/2512.02667)
*Haozhuo Zheng,Cheng Wang,Yang Liu*

Main category: cs.LG

TL;DR: 提出 Graph VQ-Transformer (GVT)，通过图VQ-VAE 将分子图压缩为高保真离散潜在序列，并以自回归 Transformer 在该离散序列上进行生成，从而实现高效且高质量的分子生成，在 ZINC250k、MOSES、GuacaMol 等基准上达到竞争性甚至领先表现。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型计算成本高、 autoregressive 模型易出现误差传播的问题；通过将分子图映射为离散潜在序列，将分子生成转化为结构良好的序列建模问题，从而利用大规模序列模型的优势，并为与大语言模型的协同奠定基础。

Method: 提出 Graph VQ-VAE：结合 Graph Transformer、canonical Reverse Cuthill-McKee (RCM) 节点排序与 Rotary Positional Embeddings (RoPE)，在图上得到高保真离散潜在编码。随后训练自回归 Transformer 在这些离散潜在序列上进行分子生成，将图生成问题转化为序列建模。还通过与主流基准的对比验证效率与质量。

Result: 在 ZINC250k、MOSES、GuacaMol 等基准上达到 state-of-the-art 或高度竞争的表现，且在分布相似性度量如 FCD 与 KL Divergence 上明显优于领先扩散模型。

Conclusion: GVT 提供了一种将复杂分子图映射到高保真离散序列的框架，结合图-序列模型与离散潜在空间，成为扩散模型的有力替代，并为未来在离散潜在空间的分子设计与与大语言模型的融合奠定新基线。

Abstract: The de novo generation of molecules with desirable properties is a critical challenge, where diffusion models are computationally intensive and autoregressive models struggle with error propagation. In this work, we introduce the Graph VQ-Transformer (GVT), a two-stage generative framework that achieves both high accuracy and efficiency. The core of our approach is a novel Graph Vector Quantized Variational Autoencoder (VQ-VAE) that compresses molecular graphs into high-fidelity discrete latent sequences. By synergistically combining a Graph Transformer with canonical Reverse Cuthill-McKee (RCM) node ordering and Rotary Positional Embeddings (RoPE), our VQ-VAE achieves near-perfect reconstruction rates. An autoregressive Transformer is then trained on these discrete latents, effectively converting graph generation into a well-structured sequence modeling problem. Crucially, this mapping of complex graphs to high-fidelity discrete sequences bridges molecular design with the powerful paradigm of large-scale sequence modeling, unlocking potential synergies with Large Language Models (LLMs). Extensive experiments show that GVT achieves state-of-the-art or highly competitive performance across major benchmarks like ZINC250k, MOSES, and GuacaMol, and notably outperforms leading diffusion models on key distribution similarity metrics such as FCD and KL Divergence. With its superior performance, efficiency, and architectural novelty, GVT not only presents a compelling alternative to diffusion models but also establishes a strong new baseline for the field, paving the way for future research in discrete latent-space molecular generation.

</details>


### [37] [Conformal Correction for Efficiency May be at Odds with Entropy](https://arxiv.org/abs/2512.02704)
*Senrong Xu,Tianyu Wang,Zenan Li,Yuan Yao,Taolue Chen,Feng Xu,Xiaoxing Ma*

Main category: cs.LG

TL;DR: 提出了一种基于熵约束的 conformal correction，以在 CP 的效率和预测熵之间实现 Pareto 最优。实验结果显示在多种数据集上提高 CP 效率，可达 34.4% 的提升。


<details>
  <summary>Details</summary>
Motivation: 在黑盒模型的 conformal prediction 框架中需提升效率，同时保持覆盖性和统计可信性；现有方法在效率与预测熵之间存在显著权衡。

Method: 引入熵约束的 conformal correction 模块，设定熵阈值以约束预测分布的熵，从而在保持覆盖率的前提下提升效率，并提供理论分析与大规模实验验证。

Result: 理论分析揭示了 CP 效率与预测熵之间的权衡关系；在计算机视觉和图数据集上实验表明方法可显著提升效率，且在给定熵阈值下最高提升可达约 34.4%。

Conclusion: 提出的熵约束 conformal correction 提供了更优的 Pareto-optimal 性能，能够在不同任务和数据集上提升 CP 效率并控制预测熵。

Abstract: Conformal prediction (CP) provides a comprehensive framework to produce statistically rigorous uncertainty sets for black-box machine learning models. To further improve the efficiency of CP, conformal correction is proposed to fine-tune or wrap the base model with an extra module using a conformal-aware inefficiency loss. In this work, we empirically and theoretically identify a trade-off between the CP efficiency and the entropy of model prediction. We then propose an entropy-constrained conformal correction method, exploring a better Pareto optimum between efficiency and entropy. Extensive experimental results on both computer vision and graph datasets demonstrate the efficacy of the proposed method. For instance, it can significantly improve the efficiency of state-of-the-art CP methods by up to 34.4%, given an entropy threshold.

</details>


### [38] [From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity](https://arxiv.org/abs/2512.02826)
*Haoming Liu,Jinnuo Liu,Yanhao Li,Liuyang Bai,Yunkai Ji,Yuanhe Guo,Shenji Wan,Hongyi Wen*

Main category: cs.LG

TL;DR: 通过对流基础扩散模型的边际速度场进行解析，揭示FM目标的两阶段训练信号，解释泛化与记忆的分布，以及对训练技巧的有效性提供理论解释与设计原则。


<details>
  <summary>Details</summary>
Motivation: 理解流式（flow-based）扩散模型在训练过程中的 memorization（记忆化）与 generalization（泛化）行为，以及 Flow Matching (FM) 目标的几何/物理含义，从而为模型架构和训练算法改进提供理论基石。

Method: 推导 FM 目标的边际速度场（存在闭式表达），并计算出 oracle FM 目标；据此分析模型在训练中呈现的两阶段信号：早期由数据模态混合驱动的导航阶段，晚期由最近样本驱动的细化阶段；结合推导结果解释实践技巧（如时间步移位、无分类器引导、潜在空间设计等）的作用。

Result: 证实边际速度场导出的两阶段训练目标导致模型先在全局数据模态上实现泛化以形成全局布局，随后逐步记忆细粒度细节；给出对常用训练技巧有效性的统一解释，并提出基于此理解的未来架构与训练算法改进原则。

Conclusion: 通过对FM目标及其边际速度场的深入分析，揭示流基扩散模型训练动力学的本质并提供可操作的设计原则，帮助指导未来的模型开发与优化。

Abstract: Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.

</details>


### [39] [A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models](https://arxiv.org/abs/2512.02833)
*Ihab Ahmed,Denis Krompaß,Cheng Feng,Volker Tresp*

Main category: cs.LG

TL;DR: 在时间序列基础模型中，输入归一化是提高泛化能力的关键手段，REVIN在四种不同架构的TSFM上表现最佳，显著提升零-shot鲁棒性并在不需数据级预处理的情况下实现高性价比。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据存在跨领域和通道的尺度变化及非平稳性，这些因素在TSFM上对泛化性能的影响大于模型架构的影响，因此需要系统评估并选择有效的归一化方法。

Method: 在四种架构差异显著的TSFM上进行系统评估，比较不同归一化策略（包括REV IN等），以零-shot和在-domain场景下的MASE表现，以及对训练损失尺度敏感性和模型类型的影响。

Result: 证明REVIN是最具效率的归一化方法，零-shot MASE相较未归一化基线下降约89%，相较其他归一化方法下降约44%；同时在不进行数据集级预处理的情况下达到与最佳域内相同的0.84 MASE，显示最高的准确度-效率权衡。

Conclusion: REV IN的效用受模型设计（目标函数、概率/点预报/LLM等）和损失尺度敏感性影响，需结合具体架构选择使用策略。

Abstract: We investigate input normalization methods for Time-Series Foundation Models (TSFMs). While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical. Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity. Through systematic evaluation across four architecturally diverse TSFMs, we empirically establish REVIN as the most efficient approach, reducing zero-shot MASE by 89\% relative to an un-normalized baseline and by 44\% versus other normalization methods, while matching the best in-domain accuracy (0.84 MASE) without any dataset-level preprocessing -- yielding the highest accuracy-efficiency trade-off. Yet its effect utilization depends on architectural design choices and optimization objective, particularly with respect to training loss scale sensitivity and model type (probabilistic, point-forecast, or LLM-based models).

</details>


### [40] [GraphMatch: Fusing Language and Graph Representations in a Dynamic Two-Sided Work Marketplace](https://arxiv.org/abs/2512.02849)
*Mikołaj Sacha,Hammad Jafri,Mattie Terzolo,Ayan Sinha,Andrew Rabinovich*

Main category: cs.LG

TL;DR: GraphMatch integrates pretrained language models with graph neural networks to recommend matches in a dynamic, text-rich two-sided marketplace, achieving higher accuracy and lower latency than language-only or graph-only baselines.


<details>
  <summary>Details</summary>
Motivation: Two-sided marketplaces with evolving content and interaction graphs require aligning textual semantics with graph structures; standalone models struggle to capture both fine-grained language meaning and time-sensitive graph dynamics.

Method: Fuse powerful text encoders (LMs) with graph neural networks (GNNs); employ adversarial negative sampling and point-in-time subgraph training to learn representations that reflect evolving text semantics and time-sensitive graph topology; optimize for low-latency inference; large-scale evaluation on real data.

Result: GraphMatch outperforms language-only and graph-only baselines on matching tasks at large scale while maintaining runtime efficiency.

Conclusion: Unifying language and graph representations yields an effective, practical solution for text-rich, dynamic two-sided recommendations, bridging pretrained LMs and large-scale graphs for real-time marketplaces.

Abstract: Recommending matches in a text-rich, dynamic two-sided marketplace presents unique challenges due to evolving content and interaction graphs. We introduce GraphMatch, a new large-scale recommendation framework that fuses pre-trained language models with graph neural networks to overcome these challenges. Unlike prior approaches centered on standalone models, GraphMatch is a comprehensive recipe built on powerful text encoders and GNNs working in tandem. It employs adversarial negative sampling alongside point-in-time subgraph training to learn representations that capture both the fine-grained semantics of evolving text and the time-sensitive structure of the graph. We evaluated extensively on interaction data from Upwork, a leading labor marketplace, at large scale, and discuss our approach towards low-latency inference suitable for real-time use. In our experiments, GraphMatch outperforms language-only and graph-only baselines on matching tasks while being efficient at runtime. These results demonstrate that unifying language and graph representations yields a highly effective solution to text-rich, dynamic two-sided recommendations, bridging the gap between powerful pretrained LMs and large-scale graphs in practice.

</details>


### [41] [Adaptive Decentralized Federated Learning for Robust Optimization](https://arxiv.org/abs/2512.02852)
*Shuyuan Wu,Feifei Wang,Yuan Gao,Hansheng Wang*

Main category: cs.LG

TL;DR: 提出自适应去中心化联邦学习（aDFL）来鲁棒估计，依据客户端的可疑程度自适应调整学习率，减少异常客户端对全局模型的影响，且无需先验信息，给出收敛性分析和丰富的实验结果。


<details>
  <summary>Details</summary>
Motivation: 在去中心化联邦学习（DFL）中，异常客户端（噪声/中毒数据）会显著扰动学习过程，现有方法往往依赖大量正常邻居或事先已知的可靠客户端，这限制了实际应用。

Method: 对每个客户端自适应分配学习率：对可疑客户端降低学习率，对正常客户端提高学习率，使鲁棒性以完全自适应的方式实现。该机制无需先验知识，理论给出收敛性分析并具备oracle性质。

Result: 理论结果显示收敛且具备oracle属性；大量数值实验表明aDFL在鲁棒性和精度方面优于基线方法。

Conclusion: aDFL提供一种无先验假设、对异常客户端鲁棒的自适应去中心化联邦学习框架，兼具理论保障与实验证据。

Abstract: In decentralized federated learning (DFL), the presence of abnormal clients, often caused by noisy or poisoned data, can significantly disrupt the learning process and degrade the overall robustness of the model. Previous methods on this issue often require a sufficiently large number of normal neighboring clients or prior knowledge of reliable clients, which reduces the practical applicability of DFL. To address these limitations, we develop here a novel adaptive DFL (aDFL) approach for robust estimation. The key idea is to adaptively adjust the learning rates of clients. By assigning smaller rates to suspicious clients and larger rates to normal clients, aDFL mitigates the negative impact of abnormal clients on the global model in a fully adaptive way. Our theory does not put any stringent conditions on neighboring nodes and requires no prior knowledge. A rigorous convergence analysis is provided to guarantee the oracle property of aDFL. Extensive numerical experiments demonstrate the superior performance of the aDFL method.

</details>


### [42] [Assessing the performance of correlation-based multi-fidelity neural emulators](https://arxiv.org/abs/2512.02868)
*Cristian J. Villatoro,Gianluca Geraci,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Outer loop tasks such as optimization, uncertainty quantification or inference can easily become intractable when the underlying high-fidelity model is computationally expensive. Similarly, data-driven architectures typically require large datasets to perform predictive tasks with sufficient accuracy. A possible approach to mitigate these challenges is the development of multi-fidelity emulators, leveraging potentially biased, inexpensive low-fidelity information while correcting and refining predictions using scarce, accurate high-fidelity data. This study investigates the performance of multi-fidelity neural emulators, neural networks designed to learn the input-to-output mapping by integrating limited high-fidelity data with abundant low-fidelity model solutions. We investigate the performance of such emulators for low and high-dimensional functions, with oscillatory character, in the presence of discontinuities, for collections of models with equal and dissimilar parametrization, and for a possibly large number of potentially corrupted low-fidelity sources. In doing so, we consider a large number of architectural, hyperparameter, and dataset configurations including networks with a different amount of spectral bias (Multi-Layered Perceptron, Siren and Kolmogorov Arnold Network), various mechanisms for coordinate encoding, exact or learnable low-fidelity information, and for varying training dataset size. We further analyze the added value of the multi-fidelity approach by conducting equivalent single-fidelity tests for each case, quantifying the performance gains achieved through fusing multiple sources of information.

</details>


### [43] [OptPO: Optimal Rollout Allocation for Test-time Policy Optimization](https://arxiv.org/abs/2512.02882)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [44] [Fast Gaussian Process Approximations for Autocorrelated Data](https://arxiv.org/abs/2512.02925)
*Ahmadreza Chokhachian,Matthias Katzfuss,Yu Ding*

Main category: cs.LG

TL;DR: 通过将自相关数据分块来加速高斯过程回归；在保留预测性能的前提下，改造现有的快速GP近似方法以适应分块数据，从而显著提升 autocorrelated 数据的计算速度。


<details>
  <summary>Details</summary>
Motivation: 自相关数据在使用标准独立同分布噪声假设的快速GP近似时会导致时序过拟合，降低对新样本的泛化能力。因此需要对快速近似方法进行修改，使之能处理自相关性以提高效率与预测稳定性。

Method: 将原始相关数据分割为相对独立的块，在这些块内进行解相关化处理后，使用或改造现有的快速GP近似算法以适用于分块数据结构，从而实现对自相关数据的高效GP回归。

Result: 在多种应用数据集上进行的数值实验表明，所提出的方法能显著加速高斯过程回归的计算，同时保持或提升预测性能。

Conclusion: 所提出的分块自相关数据的快速GP方法为在自相关数据场景中的GP回归提供了一种兼顾计算效率与预测效果的有效途径。

Abstract: This paper is concerned with the problem of how to speed up computation for Gaussian process models trained on autocorrelated data. The Gaussian process model is a powerful tool commonly used in nonlinear regression applications. Standard regression modeling assumes random samples and an independently, identically distributed noise. Various fast approximations that speed up Gaussian process regression work under this standard setting. But for autocorrelated data, failing to account for autocorrelation leads to a phenomenon known as temporal overfitting that deteriorates model performance on new test instances. To handle autocorrelated data, existing fast Gaussian process approximations have to be modified; one such approach is to segment the originally correlated data points into blocks in which the blocked data are de-correlated. This work explains how to make some of the existing Gaussian process approximations work with blocked data. Numerical experiments across diverse application datasets demonstrate that the proposed approaches can remarkably accelerate computation for Gaussian process regression on autocorrelated data without compromising model prediction performance.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [45] [DySTAN: Joint Modeling of Sedentary Activity and Social Context from Smartphone Sensors](https://arxiv.org/abs/2512.02025)
*Aditya Sneh,Nilesh Kumar Sahu,Snehil Gupta,Haroon R. Lone*

Main category: eess.SP

TL;DR: 双标签多任务学习框架DySTAN：从手机传感数据同时识别 sedentary context 与 social context，通过跨任务注意力与共享表示提升识别准确性，优于单任务和其他多任务基线。


<details>
  <summary>Details</summary>
Motivation: 在安坐场景下，微小的惯性模式难以分辨不同活动；社会上下文对理解用户行为至关重要，但在移动传感研究中常被忽视。因此需要联合建模和跨任务协同以提高鲁棒性与准确性。

Method: LogMe应用被动收集手机传感器数据（加速度计、陀螺仪、磁力计、旋转向量），并以每小时自报告捕捉 sedentary activity 与 social context 两个维度。基于此双标签数据集，提出DySTAN（Dynamic Cross-Stitch with Task Attention Network），通过共享表示与跨任务注意力实现对两个任务的联合分类。模型包含任务专用分支、跨任务交互与注意力机制，并以CNN-BiLSTM-GRU为基线进行对比。

Result: 在 sedentary activity 的宏F1分数上，DySTAN 相比单任务的CNN-BiLSTM-GRU提升21.8%，相较最强多任务基线Sluice Network提升8.2%。表明同时建模多维上下文对提高移动场景识别的准确性和鲁棒性具有重要作用。

Conclusion: 通过对共现的上下文维度进行联合建模，并利用跨任务注意力与共享表示，DySTAN显著提升了在移动传感环境下的上下文识别性能，验证了多任务协同和任务间信息共享的重要性。

Abstract: Accurately recognizing human context from smartphone sensor data remains a significant challenge, especially in sedentary settings where activities such as studying, attending lectures, relaxing, and eating exhibit highly similar inertial patterns. Furthermore, social context plays a critical role in understanding user behavior, yet is often overlooked in mobile sensing research. To address these gaps, we introduce LogMe, a mobile sensing application that passively collects smartphone sensor data (accelerometer, gyroscope, magnetometer, and rotation vector) and prompts users for hourly self-reports capturing both sedentary activity and social context. Using this dual-label dataset, we propose DySTAN (Dynamic Cross-Stitch with Task Attention Network), a multi-task learning framework that jointly classifies both context dimensions from shared sensor inputs. It integrates task-specific layers with cross-task attention to model subtle distinctions effectively. DySTAN improves sedentary activity macro F1 scores by 21.8% over a single-task CNN-BiLSTM-GRU (CBG) model and by 8.2% over the strongest multi-task baseline, Sluice Network (SN). These results demonstrate the importance of modeling multiple, co-occurring context dimensions to improve the accuracy and robustness of mobile context recognition.

</details>


### [46] [Towards Sustainable Precision: Machine Learning for Laser Micromachining Optimization](https://arxiv.org/abs/2512.02026)
*Luis Correas-Naranjo,Miguel Camacho-Sánchez,Laëtitia Launet,Milena Zuric,Valery Naranjo*

Main category: eess.SP

TL;DR: 提出一种针对超短脉冲激光微加工的轻量化机器学习框架，用于跨多种预处理技术的表面质量评估与实时监控，兼顾泛化与计算成本优化。


<details>
  <summary>Details</summary>
Motivation: 实现可持续制造中对高精度表面处理的实时缺陷检测，需降低数据维度和模型规模以满足实时性和资源限制。

Method: 设计一个跨预处理技术的机器学习框架，通过降维、特征提取与轻量化模型来降低计算开销，并优化监控系统以实现实时分析。

Result: 实验表明该模型在跨预处理技术场景下的泛化性能优于先前工作，并显著降低训练过程的计算资源需求。

Conclusion: 该框架为实现可持续制造中的高效实时监控提供基线，促进在不同预处理技术下的表面质量评估的普适性与经济性。

Abstract: In the pursuit of sustainable manufacturing, ultra-short pulse laser micromachining stands out as a promising solution while also offering high-precision and qualitative laser processing. However, unlocking the full potential of ultra-short pulse lasers requires an optimized monitoring system capable of early detection of defective workpieces, regardless of the preprocessing technique employed. While advances in machine learning can help predict process quality features, the complexity of monitoring data necessitates reducing both model size and data dimensionality to enable real-time analysis. To address these challenges, this paper introduces a machine learning framework designed to enhance surface quality assessment across diverse preprocessing techniques. To facilitate real-time laser processing monitoring, our solution aims to optimize the computational requirements of the machine learning model. Experimental results show that the proposed model not only outperforms the generalizability achieved by previous works across diverse preprocessing techniques but also significantly reduces the computational requirements for training. Through these advancements, we aim to establish the baseline for a more sustainable manufacturing process.

</details>


### [47] [Hardware Distortion Aware Precoding for ISAC Systems](https://arxiv.org/abs/2512.02153)
*Murat Babek Salman,Emil Björnson,Özlem Tugfe Demir*

Main category: eess.SP

TL;DR: 硬件失真会降低ISAC在复杂环境中的感知性能；提出面向失真和杂波的前向处理与功率分配策略，在接近通信最优预编码的同时提升感知鲁棒性，并通过数值结果验证相对于不考虑失真的设计具有显著提升。


<details>
  <summary>Details</summary>
Motivation: ISAC在杂波环境中受到硬件失真影响，然而在考虑失真下的感知性能评估与鲁棒设计尚缺乏系统研究；需要开发能同时抑制失真与杂波的鲁棒方案。

Method: 推导在硬件失真条件下的感知信噪比SCNR，分析多天线发送失真的各向同性特性对杂波抑制的影响；提出失真与杂波感知的前向处理策略，尽量保持与面向通信优化的预编码相近，同时提升感知鲁棒性；并给出一种低复杂度的基于功率分配的替代方案。

Result: 数值结果表明，所提出的方法能有效缓解硬件失真和杂波带来的限制，相对于忽略失真的设计具有显著性能提升。

Conclusion: 通过利用失真和杂波信息来设计鲁棒的前向处理和功率分配，ISAC系统的感知性能在存在硬件失真的场景下可以得到显著提升。

Abstract: The impact of hardware impairments on the spectral efficiency of communication systems is well studied, but their effect on sensing performance remains unexplored. In this paper, we analyze the influence of hardware impairments on integrated sensing and communication (ISAC) systems in cluttered environments. We derive the sensing signal-to-clutter-plus-noise ratio (SCNR) and show that hardware distortions significantly degrade sensing performance by enhancing clutter-induced noise, which masks target echoes. The isotropic nature of transmit distortion due to multiple stream transmission further complicates clutter suppression. To address this, we propose a distortion- and clutter-aware precoding strategy that minimizes the deviation from the communication-optimized precoder while improving sensing robustness. We also propose an alternative power allocation-based approach that reduces computational complexity. Numerical results confirm the effectiveness of the proposed approaches in overcoming hardware- and clutter-induced limitations, demonstrating significant performance gains over distortion-unaware designs.

</details>


### [48] [Wavenumber-Division Multiplexing in Holographic MIMO with NLoS Channels](https://arxiv.org/abs/2512.02245)
*Ashutosh Prajapati,Prathapasinghe Dharmawansa,Marco Di Renzo,Italo Atzeni*

Main category: eess.SP

TL;DR: 将波数分复用（WDM）扩展到带有NLoS传播的全息MIMO通道，给出角域表示及其功谱特征；在各向同性散射情况下给出闭式解并恢复Jakes模型，並通过数值分析评估自由度与遍历容量。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的NLoS传播使得线性LOS假设不足，需在全息MIMO框架下建立WDM的角域等效描述，以揭示信道的谱特性与容量潜力。

Method: 将WDM应用于NLoS通道，推导角域表示并定义功谱因子和功谱密度；在各向同性散射条件下给出闭式解并与Jakes模型对齐；通过数值仿真评估自由度和遍历容量，比较各向同性与非各向同性散射。

Result: 得到NLoS下的角域表示及其功谱特征；在各向同性散射下的闭式解成功恢复Jakes模型；实现了自由度与遍历容量的数值评估，揭示散射特性对性能的影响。

Conclusion: WDM可用于NLoS全息MIMO的角域分析，且在各向同性条件下与经典模型等价；该分析框架提供对NLoS通道谱特性与容量的定量洞察。

Abstract: Wavenumber-division multiplexing (WDM) was introduced as a counterpart of orthogonal frequency-division multiplexing in the spatial-frequency domain for line-of-sight holographic multiple-input multiple-output (MIMO) systems. In this paper, we extend WDM to holographic MIMO channels with non-line-of-sight (NLoS) propagation. We show that applying WDM to the NLoS channel yields the corresponding angular-domain representation, which we characterize through the power spectral factor and power spectral density. We further obtain a closed-form characterization for the case of isotropic scattering, recovering Jakes' isotropic model. The analysis is complemented by numerical results evaluating the degrees of freedom and ergodic capacity under both isotropic and non-isotropic scattering.

</details>


### [49] [Zero-Forcing MU-MIMO Precoding under Power Amplifier Non-Linearities](https://arxiv.org/abs/2512.02573)
*Juan Vidal Alegría,Ashkan Sheikhi,Ove Edfors*

Main category: eess.SP

TL;DR: 提出非线性感知的 ZF 预编码（NLA-ZF），在两用户、BS 具偶数天线且每根天线连接一个三阶记忆无关非线性 PA 的 MU-MIMO 系统中，通过初始迭代解实现对干扰的近乎零抵消，允许一定的容忍误差。


<details>
  <summary>Details</summary>
Motivation: 功放非线性会破坏线性预编码的干扰抑制；虽然可以通过数字前置失真（DPD）线性化，但成本和功耗较高。为降低成本同时实现对非线性干扰的抵消，研究在已知非线性响应的前提下的非线性感知线性预编码。

Method: 提出基于两用户场景、偶数天线、记忆无关三阶非线性功放的初始迭代解，设计非线性感知的 ZF 预编码（NLA-ZF），以在非线性存在下实现近似完美干扰取消，并给出收敛性和容忍度方面的设定。

Result: 在存在显著残留干扰的场景中，NLA-ZF 能带来性能提升，体现对非线性影响的有效抵消。

Conclusion: 通过利用对功放非线性响应的知识，可以在降低对 DPD 依赖的前提下实现实用的干扰抵消，尤其适用于两用户、偶数天线、三阶记忆无关非线性的场景。

Abstract: In multi-user multiple-input multiple-output (MU-MIMO) systems, the non-linear behavior of the power amplifiers (PAs) may cause degradation of the linear precoding schemes dealing with interference between user equipments (UEs), e.g., the zero-forcing (ZF) precoder. One way to minimize this effect is to use digital-pre-distortion (DPD) modules to linearize the PAs. However, using perfect DPD modules is costly and it may incur significant power consumption. As an alternative, we consider the problem of characterizing non-linearity-aware ZF (NLA-ZF) precoding schemes, hereby defined as linear precoders that achieve perfect interference cancellation in the presence of PA non-linearity by exploiting knowledge of this non-linear response. We provide initial iterative solutions that allow achieving NLA-ZF (up to adjustable tolerance) in a two-UE downlink MU-MIMO scenario where the base station (BS) has an even number of antennas, and each antenna is connected to a PA exhibiting third-order memory-less non-linear behavior. The proposed approach allows for performance gains in scenarios with significant residual interference.

</details>


### [50] [Channel Knowledge Map Construction via Physics-Inspired Diffusion Model Without Prior Observations](https://arxiv.org/abs/2512.02757)
*Yunzhe Zhu,Xuewen Liao,Zhenzhen Gao,Yong Zeng*

Main category: eess.SP

TL;DR: 提出一种将物理约束融入扩散模型的CKM构建方法，通过三项物理约束描述大尺度衰落的空间分布，并在扩散模型框架下实现CKM的高精度生成，达到优于现有方法的构建准确性。


<details>
  <summary>Details</summary>
Motivation: 当前CKM多将任务视为图像超分辨率或生成问题，采用通用计算机视觉模型，未能捕捉无线传播的物理特性，导致CKM的物理一致性不足。需要在保持高保真度的同时，确保CKM与物理传播规律的一致性，提升环境感知能力。

Method: 提出三项与大尺度衰落相关的物理约束项来刻画其空间分布，通过将这些约束与最先进的扩散模型结合，构建一个物理启发的CKM扩散生成框架。推导带有物理约束项的扩散模型损失函数，并设计CKM的训练与生成流程。

Result: 实验结果显示该方法在CKM构建精度方面优于现有方法，且所提出的框架具有统一性和强潜力，能够生成多样、准确且物理一致的CKM。

Conclusion: 物理启发的CKM扩散模型为大尺度衰落CKM的高精度构建提供了更强的一致性与泛化能力，形成了一个可扩展的生成框架，适用于未来6G环境感知需求。

Abstract: The ability to construct Channel Knowledge Map (CKM) with high precision is essential for environment awareness in 6G wireless systems. However, most existing CKM construction methods formulate the task as an image super-resolution or generation problem, thereby employing models originally developed for computer vision. As a result, the generated CKMs often fail to capture the underlying physical characteristics of wireless propagation. In this paper, we focus on the construction of CKM for large-scale fading scenarios and design three physics-based constraint terms to characterize the spatial distribution patterns of large-scale fading. By integrating these physical constraints with a state-of-the-art diffusion model that possesses superior generative capability, a physics-inspired diffusion model for CKM construction is proposed. Following this motivation, we derive the loss function of the diffusion model augmented with physics-based constraint terms and further design the training and generation framework for the proposed physics-inspired CKM generation diffusion model. Extensive experiments show that our approach outperforms all existing methods in terms of construction accuracy. Moreover, the proposed model provides a unified and effective framework with strong potential for generating diverse, accurate, and physically consistent CKM.

</details>


### [51] [Effects of disease duration and antipsychotics on brain age in schizophrenia](https://arxiv.org/abs/2512.02765)
*Alejandro Roig-Herrero,Luis M. San-José-Revuelta,Rafael Navarro-González,Rodrigo de Luis-García,Vicente Molina*

Main category: eess.SP

TL;DR: 多模态脑龄分析表明，精神分裂症患者的脑年龄加速并非仅由抗精神病药物驱动，且横截面证据不足以揭示时间动态；需要纵向研究以明确其时间演变。


<details>
  <summary>Details</summary>
Motivation: 探究脑年龄差（Brain Age Gap, BAG）的进展性及药物暴露的潜在作用，明确精神病与药物暴露对脑结构老化的贡献。

Method: 对第一发作精神病患者与健康对照组比较BAG，控制年龄、性别、BMI、颅内总体积；采用两种模型：基于FastSurfer提取的协调化体积特征的Transformer风格模型，以及先前训练的深度学习模型；药物因素方面，比较已用抗精神病药的双相抑郁人群与未用药人群，使用Mann-Whitney U检验。

Result: 两种模型都显示：药物暴露与BAG之间没有显著增大；整体结论为，脑年龄加速不太可能仅由抗精神病药物引起；需要纵向研究以明确时间动态。

Conclusion: 横截面分析提示药物暴露不是唯一解释脑龄加速的因素，建议开展纵向随访研究，以揭示精神分裂症相关脑老化的时间过程及药物对其的潜在作用。

Abstract: Accelerated brain aging has been consistently reported in patients with schizophrenia. Over the past decade, these findings have been replicated using the Brain Age paradigm, which applies machine learning techniques to estimate brain age from neuroimaging data. This approach yields a single index, the Brain Age Gap, defined as the difference between predicted and chronological age. Nevertheless, both the progressive nature of this phenomenon and the potential role of antipsychotic medication remain unclear. To investigate its progression, we compared the Brain Age Gap between individuals experiencing a first episode of psychosis and healthy controls using ANCOVA, adjusting for age, sex, body mass index, and estimated total intracranial volume. To enhance the robustness of our findings, we employed two distinct models: a transformer-inspired model based on harmonized volumetric brain features extracted with FastSurfer, and a previously trained deep learning model. To assess the potential effect of medication, we further compared bipolar patients who received antipsychotic treatment with those who did not. Mann-Whitney U test consistently showed that medicated bipolar patients did not exhibit a significantly larger Brain Age Gap. Both models converge on the conclusion that accelerated brain aging is unlikely to be explained by antipsychotic medication alone. Longitudinal studies are therefore required to clarify the temporal dynamics of brain aging in schizophrenia.

</details>


### [52] [Diffusion-Prior Split Gibbs Sampling for Synthetic Aperture Radar Imaging under Incomplete Measurements](https://arxiv.org/abs/2512.02768)
*Hefei Gao,Tianyao Huang,Letian Guo,Jie He,Yonina C. Eldar*

Main category: eess.SP

TL;DR: A diffusion-driven split Gibbs sampling framework for SAR reconstruction that fuses measurement fidelity with learned diffusion priors, achieving state-of-the-art PSNR gains and sidelobe suppression on simulated and Sentinel-1A data.


<details>
  <summary>Details</summary>
Motivation: SAR imaging is hampered by noise, undersampling, and complex scattering; conventional methods struggle with artifacts and loss of detail. Although diffusion models offer strong high-order priors, prior diffusion-based SAR methods rely on oversimplified likelihoods, leading to degraded reconstructions. A rigorous posterior-guided approach is needed.

Method: Propose a diffusion-driven split Gibbs sampling framework. Alternate likelihood- and prior-driven updates via proximal sampling to progressively approximate the true posterior, integrating measurement fidelity with learned diffusion priors and leveraging diffusion models for priors.

Result: On simulated data: >7 dB average PSNR gain; significant sidelobe suppression (MPLSR +2.96 dB, MISLR +11.5 dB) vs. best baseline. On real Sentinel-1A data: average PSNR gain ~1.6 dB with reduced artifacts and preserved ridges, edges, and fine textures.

Conclusion: The framework is robust and generalizable for high-fidelity SAR imaging across diverse sensing scenarios, illustrating the potential of combining diffusion priors with proximal sampling in a split Gibbs scheme to improve reconstruction quality.

Abstract: Synthetic aperture radar (SAR) imaging plays a critical role in all-weather, day-and-night remote sensing, yet reconstruction is often challenged by noise, undersampling, and complex scattering scenarios. Conventional methods, including matched filtering and sparsity-based compressed sensing, are limited in capturing intricate scene structures and frequently suffer from artifacts, elevated sidelobes, and loss of fine details. Recent diffusion models have demonstrated superior capability in representing high-order priors; however, existing diffusion-based SAR methods still yield degraded reconstructions due to oversimplified likelihood approximations in guided sampling. In this work, we propose a diffusion-driven split Gibbs sampling framework for SAR reconstruction, rigorously integrating measurement fidelity with learned diffusion priors. By alternately performing likelihood- and prior-driven updates via proximal sampling, this method ensures progressive convergence toward the true posterior while fully leveraging the expressive power of diffusion priors. Extensive experiments on simulated and Sentinel-1A datasets demonstrate substantial performance improvements: over 7 dB average PSNR gain in simulations, along with significant sidelobe suppression (MPLSR +2.96 dB, MISLR +11.5 dB) with respect to the best baseline result. On real-world Sentinel-1A data, the method achieves an average PSNR gain of 1.6 dB while effectively reducing artifacts and preserving scene details, including ridges, edges, and fine textures. These results underscore the potential of the adapted framework as a robust and generalizable solution for high-fidelity SAR imaging across diverse sensing scenarios.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [53] [ProtO-RU: An O-RAN Split-7.2 Radio Unit using SDRs](https://arxiv.org/abs/2512.02398)
*Zhiyu Zhou,Xin Zhe Khooi,Satis Kumar Permal,Mun Choon Chan*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present ProtO-RU, the first open source, software-defined O-RAN Split-7.2 Radio Unit built using SDRs and commodity CPUs. Unlike proprietary hardware-based commercial O-RUs, ProtO-RU is built on the open-source srsRAN software stack, and it is fully programmable. We demonstrate that ProtO-RU integrates with the srsRAN and OpenAirInterface5G CU/DU stacks, supports both TDD and FDD duplexing modes, and interoperates with commercial 5G UEs. Our evaluation shows that ProtO-RU remains stable under sustained load with multiple UEs and delivers throughput comparable to Split-8 and commercial O-RUs. ProtO-RU opens up new opportunities for RU-level innovations and lowers the barrier of entry for end-to-end O-RAN research.

</details>


### [54] [Wi-Fi Rate Adaptation for Moving Equipment in Industrial Environments](https://arxiv.org/abs/2512.02455)
*Pietro Chiavassa,Stefano Scanzio,Gianluca Cena*

Main category: cs.NI

TL;DR: 本文对 Minstrel 发送速率自适应在静态与移动场景下的性能进行评估，聚焦工业场景中的时延和丢包率，作为基于集中式数字孪生的改进速率自适应的初步基线。


<details>
  <summary>Details</summary>
Motivation: 工业环境对传输时延有严格的边界要求，且 802.11 的速率自适应设计并未统一标准，Minstrel 作为 Linux 的默认实现，其在静态与移动条件下的性能尚待系统性评估，因此需要建立基线以指导未来基于数字孪生的集中化优化。

Method: 在 Linux 内核下对 Minstrel 进行实验性评估，设计静态与移动场景，采集时延、包丢失率等指标；通过不同距离/移动速度等条件，对比不同场景下的速率选择与传输性能，作为初步基线。

Result: 结果表明移动场景下的时延增大、丢包率上升，静态场景下性能相对稳定并且更低的时延与丢包；速率自适应在高移动性与信道变化剧烈时波动明显，提示仅凭本地分散控制的速率自适应难以满足严格工业实时性要求。

Conclusion: 提供对 Minstrel 的基线评估，支持以集中式数字孪生为基础的改进方向，未来工作将探索DT驱动的集中化速率协同、实时反馈与多智能体协同等以提升可预测性与鲁棒性。

Abstract: Wi-Fi is currently considered one of the most promising solutions for interconnecting mobile equipment (e.g., autonomous mobile robots and active exoskeletons) in industrial environments. However, relability requirements imposed by the industrial context, such as ensuring bounded transmission latency, are a major challenge for over-the-air communication. One of the aspects of Wi-Fi technology that greatly affects the probability of a packet reaching its destination is the selection of the appropriate transmission rate. Rate adaptation algorithms are in charge of this operation, but their design and implementation are not regulated by the IEEE 802.11 standard. One of the most popular solutions, available as open source, is Minstrel, which is the default choice for the Linux Kernel. In this paper, Minstrel performance is evaluated for both static and mobility scenarios. Our analysis focuses on metrics of interest for industrial contexts, i.e., latency and packet loss ratio, and serves as a preliminary evaluation for the future development of enhanced rate adaptation algorithms based on centralized digital twins.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [55] [Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas](https://arxiv.org/abs/2512.02062)
*Issa Oe,Keiichiro Yamamura,Hiroki Ishikura,Ryo Hamahira,Katsuki Fujisawa*

Main category: cs.CR

TL;DR: 提出基于超像素的黑盒对抗攻击，结合多样化搜索（versatile search）和超像素扰动区域，显著提升对抗成功率（相比基于矩形的方法平均提升约2.1%），并给出代码实现。


<details>
  <summary>Details</summary>
Motivation: 在安全关键任务中，模型易受对抗扰动影响，现有黑盒攻击多使用简单矩形作为扰动区域，难以高效发现高质量扰动。因此需要更强的黑盒攻击来评估与提升防御能力；同时希望扰动区域在颜色方差与紧凑性之间取得平衡。

Method: 引入使用超像素作为扰动区域，并提出一种新的搜索策略（versatile search），将两者结合形成 Superpixel Attack，通过逐区域修改扰动来实现对模型的攻击。

Result: 相较于现有攻击，Superpixel Attack 的平均对抗成功率提升约2.1%。尽管大多数模型对对抗攻击具有鲁棒性，该改进对黑盒攻击仍具显著意义。代码公开可用。

Conclusion: 实验结果表明，超像素在局部扰动的色彩一致性和区域紧凑性方面更利于构造有效的对抗样本；该方法为评估与提升防御提供新的工具与思路，具有潜在应用价值。

Abstract: Deep learning models are used in safety-critical tasks such as automated driving and face recognition. However, small perturbations in the model input can significantly change the predictions. Adversarial attacks are used to identify small perturbations that can lead to misclassifications. More powerful black-box adversarial attacks are required to develop more effective defenses. A promising approach to black-box adversarial attacks is to repeat the process of extracting a specific image area and changing the perturbations added to it. Existing attacks adopt simple rectangles as the areas where perturbations are changed in a single iteration. We propose applying superpixels instead, which achieve a good balance between color variance and compactness. We also propose a new search method, versatile search, and a novel attack method, Superpixel Attack, which applies superpixels and performs versatile search. Superpixel Attack improves attack success rates by an average of 2.10% compared with existing attacks. Most models used in this study are robust against adversarial attacks, and this improvement is significant for black-box adversarial attacks. The code is avilable at https://github.com/oe1307/SuperpixelAttack.git.

</details>


### [56] [Large Language Model based Smart Contract Auditing with LLMBugScanner](https://arxiv.org/abs/2512.02069)
*Yining Yuan,Yifei Wang,Yichang Xu,Zachary Yahn,Sihao Hu,Ling Liu*

Main category: cs.CR

TL;DR: LLMBugScanner 利用微调和集成推理，在跨多个大型语言模型的基础上对智能合约漏洞进行检测，结合领域知识适配和共识式冲突解决，提高鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现阶段不同预训练模型在推理能力上存在差异，单个模型很难对所有漏洞类型和合约结构表现稳定；即使微调后，模型性能仍有限，亟需更稳健的框架。

Method: 通过领域知识适配，对多个LLM进行补充数据集微调，采用参数高效的微调以降低成本；通过集成推理，结合多模型的互补优势并使用基于共识的冲突解决策略，产出更可靠的漏洞评估。

Result: 在多种流行LLM上进行广泛实验，LLMBugScanner 相较于单一的预训练或微调模型，显示出一致的准确性提升和更强的泛化能力。

Conclusion: 提出一个 principled、成本效益高且可扩展的智能合约审计框架，为现实场景中的漏洞检测提供鲁棒解决方案。

Abstract: This paper presents LLMBugScanner, a large language model (LLM) based framework for smart contract vulnerability detection using fine-tuning and ensemble learning. Smart contract auditing presents several challenges for LLMs: different pretrained models exhibit varying reasoning abilities, and no single model performs consistently well across all vulnerability types or contract structures. These limitations persist even after fine-tuning individual LLMs.
  To address these challenges, LLMBugScanner combines domain knowledge adaptation with ensemble reasoning to improve robustness and generalization. Through domain knowledge adaptation, we fine-tune LLMs on complementary datasets to capture both general code semantics and instruction-guided vulnerability reasoning, using parameter-efficient tuning to reduce computational cost. Through ensemble reasoning, we leverage the complementary strengths of multiple LLMs and apply a consensus-based conflict resolution strategy to produce more reliable vulnerability assessments.
  We conduct extensive experiments across multiple popular LLMs and compare LLMBugScanner with both pretrained and fine-tuned individual models. Results show that LLMBugScanner achieves consistent accuracy improvements and stronger generalization, demonstrating that it provides a principled, cost-effective, and extensible framework for smart contract auditing.

</details>


### [57] [Deterministic Random Bit Generators Based on Ascon for Embedded Systems](https://arxiv.org/abs/2512.02082)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 提出了三种以Ascon为驱动的DRBG构造（哈希、HMAC、CTR），并在树莓派上实现与评测，显示在嵌入式环境中更高的计算效率和更低的内存占用，优于现有DRBG实现。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的嵌入式系统中，DRBG的性能和安全性至关重要。NIST在2025年确立了Ascon轻量级加密标准，为嵌入式环境提供了新基础。本研究拟改进DRBG标准并提出基于Ascon的新构造。

Method: 提出并实现三种Ascon驱动的DRBG：Ascon-Driven Hash-Based DRBG、Ascon-Driven HMAC DRBG、以及Ascon-Driven Counter-mode DRBG。将实现部署在树莓派平台上，并与现有DRBG实现进行对比评估。

Result: 实验结果表明，所提方法在计算效率和内存使用方面优于现有DRBG实现，适合在嵌入式系统中部署。

Conclusion: 所提出的Ascon驱动DRBG方案具有良好的嵌入式部署性，符合NIST Ascon标准的发展方向，具有实际应用潜力。

Abstract: As the Deterministic Random Bit Generator (DRBG) serves as a fundamental component in random number generation and cryptographic applications, its performance and security are particularly critical in resource-constrained embedded systems, where memory capacity and computational efficiency are limited. Establishing a high-performance and secure DRBG is therefore an important issue for embedded system design. Furthermore, the National Institute of Standards and Technology (NIST) established the Ascon lightweight cryptographic standard in August 2025, which is suitable for use in resource-limited embedded environments. Therefore, this study revises the DRBG standard and proposes three Ascon-driven constructions: the Ascon-Driven Hash-Based DRBG, the Ascon-Driven keyed-Hash Message Authentication Code (HMAC) DRBG, and the Ascon-Driven Counter-mode (CTR) DRBG. In the experiments, these methods are implemented on a Raspberry Pi platform. The experimental results demonstrate that the proposed approaches achieve higher computational efficiency and lower memory usage compared with existing DRBG implementations, making them suitable for deployment in embedded systems.

</details>


### [58] [A survey about Hidden Subgroup Problem from a mathematical and cryptographic perspective](https://arxiv.org/abs/2512.02087)
*Simone Dutto,Pietro Mercuri,Nadir Murru,Lorenzo Romano*

Main category: cs.CR

TL;DR: 该论文综述隐藏子群问题（HSP）的研究现状，系统梳理阿贝尔情形下的高效量子解法及非阿贝尔情形的部分进展，重点讨论Dihedral、对称群及半直积群等与最短向量问题、图同构问题、码等价问题的联系，归纳傅里叶采样与黑箱群方法等核心技术，并从密码学视角分析相关概念与开放问题。


<details>
  <summary>Details</summary>
Motivation: HSP是量子算法与公钥密码学安全性研究中的核心问题之一。阿贝尔HSP已有高效解法，而非阿贝尔HSP尚无通用高效算法。该论文旨在系统梳理现有方法、关键技术及在重要群上的结果与局限，为未来研究指明方向。

Method: 以系统文献综述为基础，解析HSP的抽象框架及可实现的技术工具（如傅里叶采样、黑箱方法），并对具体群（Dihedral、对称群、半直积群）的结果、复杂度及与相关问题的等价/归约关系进行比较与归纳。

Result: 结论性地指出：在阿贝尔情形存在高效量子解法；非阿贝尔情形没有通用高效算法。对Dihedral群存在子指数时间的量子算法（Kuperberg及改进版本），将HSP与最短向量问题相关联；对称群S_n的HSP与图同构问题相关但尚无高效解法；半直积群的研究揭示在某些情形可将HSP与码等价等问题联系起来，但总体仍然困难且多为局部结果。

Conclusion: 总体而言，非阿贝尔HSP仍是开放且高度活跃的研究领域，未来需要发展新的算法工具、深入分析群的结构性特征，并评估对密码学及相关计算问题的影响。

Abstract: We provide a survey on the Hidden Subgroup Problem (HSP), which plays an important role in studying the security of public-key cryptosystems. We first review the abelian case, where Kitaev's algorithm yields an efficient quantum solution to the HSP, recalling how classical problems (such as order finding, integer factorization, and discrete logarithm) can be formulated as abelian HSP instances. We then examine the current state of the art for non-abelian HSP, where no general efficient quantum solution is known, focusing on some relevant groups including dihedral group (connected to the shortest vector problem), symmetric groups (connected to the graph isomorphism problem), and semidirect product constructions (connected, in a special case, to the code equivalence problem). We also describe the main techniques for addressing the HSP in non-abelian cases, namely Fourier sampling and the black-box approach. Throughout the paper, we highlight the mathematical notions required and exploited in this context, providing a cryptography-oriented perspective.

</details>


### [59] [Factor(T,U): Factored Cognition Strengthens Monitoring of Untrusted AI](https://arxiv.org/abs/2512.02157)
*Aaron Sandoval,Cody Rushing*

Main category: cs.CR

TL;DR: 提出基于分解认知的控制协议：可信模型将任务分解，未信任模型对各子任务独立求解，再重组形成完整解；以提升对复杂任务中的监控安全性。


<details>
  <summary>Details</summary>
Motivation: 解决现有弱监控在复杂任务中难以检测或抑制不安全行为的问题，通过分解-并行求解-再组装的认知结构来降低对上下文的依赖并暴露潜在漏洞以便控制。

Method: 在 APPS 编程环境中实现基于分解认知的控制协议：可信模型分解任务，未信任模型对子任务独立求解，结果再组合；并对抗性攻击（GPT-4.1 Nano）下的后门测试进行红队评估。

Result: 实验显示：将分解认知加入受信监控可将安全性从41%提升到63%；提升原因在于监控性能提高；分解认知并未提高强大LLMs在APPS中编写后门的难度；在APPS场景下总体实用性较低，但在更复杂任务上具有潜在价值。

Conclusion: 分解认知为提升监控安全提供一种有效方向，但在当前APPS环境下效用有限，需要在更复杂任务中进一步验证与扩展。

Abstract: The field of AI Control seeks to develop robust control protocols, deployment safeguards for untrusted AI which may be intentionally subversive. However, existing protocols that rely on weaker monitors to detect unsafe behavior often fail on complex tasks beyond the monitor's comprehension. We develop control protocols based on factored cognition, in which a trusted model decomposes a task, an untrusted model solves each resultant child task in isolation, and the results are reassembled into a full solution. These protocols may improve safety by several means, such as by simplifying the context for monitors, or by obscuring vulnerabilities in the environment. We implement our protocols in the APPS coding setting and red team them against backdoor attempts from an adversarial GPT-4.1 Nano. We find that: (i) Adding factored cognition to a trusted monitoring protocol can boost safety from 41% to 63%; (ii) Safety improves because monitor performance improves; (iii) Factored cognition makes it no harder for capable LLMs to write backdoors in APPS. While our protocols show low usefulness in APPS, they hold promise for more complex tasks.

</details>


### [60] [PhishSnap: Image-Based Phishing Detection Using Perceptual Hashing](https://arxiv.org/abs/2512.02243)
*Md Abdul Ahad Minhaz,Zannatul Zahan Meem,Md. Shohrab Hossain*

Main category: cs.CR

TL;DR: PhishSnap 基于端侧感知哈希的视觉钓鱼检测系统，隐私保护、在设备上推理。通过截图哈希与模板比对实现防钓鱼，2024 年数据集约 1 万条 URL，表现约 0.79 的准确率、0.76 的精确率、0.78 的召回率。


<details>
  <summary>Details</summary>
Motivation: 应对现有基于 URL/HTML 的检测在对抗视觉欺骗和仿冒中的局限性，需求隐私友好且能在端设备上运行的钓鱼检测方案。

Method: 作为浏览器扩展实现：捕获网页截图，计算视觉哈希（pHash），与合法模板进行相似性比较；在本地完成推理，确保用户隐私；数据集按 70/20/10 的划分用于训练/验证/测试。

Result: 在 2024 数据集上实现约 0.79 的准确率、0.76 的精确率、0.78 的召回率；因安全 takedowns 部分钓鱼页面不可用，导致数据集多样性下降。

Conclusion: 视觉相似性证实仍是有效的反钓鱼信号；端设备推理带来隐私与低延迟优势；数据集局限可能影响泛化性，未来需提升数据多样性与对抗性评估。

Abstract: Phishing remains one of the most prevalent online threats, exploiting human trust to harvest sensitive credentials. Existing URL- and HTML-based detection systems struggle against obfuscation and visual deception. This paper presents \textbf{PhishSnap}, a privacy-preserving, on-device phishing detection system leveraging perceptual hashing (pHash). Implemented as a browser extension, PhishSnap captures webpage screenshots, computes visual hashes, and compares them against legitimate templates to identify visually similar phishing attempts. A \textbf{2024 dataset of 10,000 URLs} (70\%/20\%/10\% train/validation/test) was collected from PhishTank and Netcraft. Due to security takedowns, a subset of phishing pages was unavailable, reducing dataset diversity. The system achieved \textbf{0.79 accuracy}, \textbf{0.76 precision}, and \textbf{0.78 recall}, showing that visual similarity remains a viable anti-phishing measure. The entire inference process occurs locally, ensuring user privacy and minimal latency.

</details>


### [61] [CVE Breadcrumbs: Tracking Vulnerabilities Through Versioned Apache Libraries](https://arxiv.org/abs/2512.02259)
*Derek Garcia,Briana Lee,Ibrahim Matar,David Rickards,Andrew Zilnicki*

Main category: cs.CR

TL;DR: 对 Apache 生态系统的安全漏洞进行历史性数据分析，揭示最具重复性的 CWEs、漏洞的发现与修复延迟，以及从披露到修复的时间线。


<details>
  <summary>Details</summary>
Motivation: 在线开源生态中，Apache 组件广泛使用但易受漏洞影响，需理解漏洞生命周期以改进安全实践。

Method: 组建覆盖 24,285 个 Apache 库、1,285 个 CVEs、157 个 CWEs 的数据集，结合 CVE 与 CWE 分类，分析复发的漏洞类型、披露和修复时序，给出时间线。

Result: 研究发现最持久和重复的 CWEs 以及 CVE 的持续时间、从引入到披露的延迟、披露后到修复的时间等，并给出跨库的漏洞生命周期时间线。提供一个可复用的数据集，涵盖大量 Apache 库、CVE 与 CWE，并提出面向开发者的安全改进建议。

Conclusion: 基于大规模数据分析，可为改进安全编码实践、漏洞监控和修复策略提供证据；数据集可用于未来研究与工业界的漏洞治理优化。

Abstract: The Apache Software Foundation (ASF) ecosystem underpins a vast portion of modern software infrastructure, powering widely used components such as Log4j, Tomcat, and Struts. However, the ubiquity of these libraries has made them prime targets for high-impact security vulnerabilities, as illustrated by incidents like Log4Shell. Despite their widespread adoption, Apache projects are not immune to recurring and severe security weaknesses. We conduct a historical analysis of the Apache ecosystem to follow the "breadcrumb trail of vulnerabilities" by compiling a comprehensive dataset of Common Vulnerabilities and Exposures (CVEs) and Common Weakness Enumerations (CWEs). We examine trends in exploit recurrence, disclosure timelines, and remediation practices. Our analysis is guided by four key research questions: (1) What are the most persistent and repeated CWEs in Apache libraries? (2) How long do CVEs persist before being addressed? (3) What is the delay between CVE introduction and official disclosure? and (4) How long after disclosure are CVEs remediated? We present a detailed timeline of vulnerability lifecycle stages across Apache libraries and offer insights to improve secure coding practices, vulnerability monitoring, and remediation strategies. Our contributions include a curated dataset covering 24,285 Apache libraries, 1,285 CVEs, and 157 CWEs, along with empirical findings and developer-focused recommendations.

</details>


### [62] [Quantum Vanguard: Server Optimized Privacy Fortified Federated Intelligence for Future Vehicles](https://arxiv.org/abs/2512.02301)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work presents vQFL (vehicular Quantum Federated Learning), a new framework that leverages quantum machine learning techniques to tackle key privacy and security issues in autonomous vehicular networks. Furthermore, we propose a server-side adapted fine-tuning method, ft-VQFL,to achieve enhanced and more resilient performance. By integrating quantum federated learning with differential privacy and quantum key distribution (QKD), our quantum vanguard approach creates a multi-layered defense against both classical and quantum threats while preserving model utility. Extensive experimentation with industry-standard datasets (KITTI, Waymo, and nuScenes) demonstrates that vQFL maintains accuracy comparable to standard QFL while significantly improving privacy guaranties and communication security. Our implementation using various quantum models (VQC, QCNN, and SamplerQNN) reveals minimal performance overhead despite the added security measures. This work establishes a crucial foundation for quantum-resistant autonomous vehicle systems that can operate securely in the post-quantum era while efficiently processing the massive data volumes (20-40TB/day per vehicle) generated by modern autonomous fleets. The modular design of the framework allows for seamless integration with existing vehicular networks, positioning vQFL as an essential component for future intelligent transportation infrastructure.

</details>


### [63] [COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers](https://arxiv.org/abs/2512.02318)
*Junyu Wang,Changjia Zhu,Yuanbo Zhou,Lingyao Li,Xu He,Junjie Xiong*

Main category: cs.CR

TL;DR: 7种主流MLLM在18类真实世界验证码任务中能够以接近人类的成本和延迟自动破解识别导向、低交互的验证码；而对需要高精度定位、多步空间推理或跨帧一致性的任务则较难。提示工程和少量示例可提升或影响破解效果；这对验证码安全性构成挑战，需为平台运营者提供相应的防御性指南与强化任务设计。


<details>
  <summary>Details</summary>
Motivation: 系统评估多模态大语言模型在视觉CAPTCHA上的安全威胁，揭示可被低成本自动化攻击利用的攻击面；通过跨模型、跨任务的量化评估与分析，理解模型在不同类型验证码上的成功与失败原因，并据此提出防御性设计原则与任务强化建议。

Method: 对7种领先的商业和开源MLLM进行评测，覆盖18种真实世界的验证码任务类型；量化单-shot准确率、有限重试下的成功率、端到端延迟和每次求解成本；研究任务特定提示工程与少量示例对破解效果的影响；通过对模型推理轨迹的分析，探究成功/失败的机制，并据此提出防御性指南；并提供Code Availability链接以便复现。

Result: MLLMs能在识别导向和低交互的验证码任务上以接近人类的成本和延迟稳定破解；但对于需要细粒度定位、跨步骤的空间推理、跨帧一致性的任务，当前模型仍显著困难；提示工程与少量示例对效能有显著影响；通过分析推理轨迹可解释模型在不同题型上的成功/失败原因，帮助制定更有针对性的验证码设计与防御策略。

Conclusion: 对平台运营者部署验证码作为 Abuse-Mitigation管线的影响深远，需据此选取更具鲁棒性且对MLLM攻击更具抗性的验证码任务，并提出相应的强化与对抗策略以提升验证码的安全性。

Abstract: This paper studies how multimodal large language models (MLLMs) undermine the security guarantees of visual CAPTCHA. We identify the attack surface where an adversary can cheaply automate CAPTCHA solving using off-the-shelf models. We evaluate 7 leading commercial and open-source MLLMs across 18 real-world CAPTCHA task types, measuring single-shot accuracy, success under limited retries, end-to-end latency, and per-solve cost. We further analyze the impact of task-specific prompt engineering and few-shot demonstrations on solver effectiveness. We reveal that MLLMs can reliably solve recognition-oriented and low-interaction CAPTCHA tasks at human-like cost and latency, whereas tasks requiring fine-grained localization, multi-step spatial reasoning, or cross-frame consistency remain significantly harder for current models. By examining the reasoning traces of such MLLMs, we investigate the underlying mechanisms of why models succeed/fail on specific CAPTCHA puzzles and use these insights to derive defense-oriented guidelines for selecting and strengthening CAPTCHA tasks. We conclude by discussing implications for platform operators deploying CAPTCHA as part of their abuse-mitigation pipeline.Code Availability (https://anonymous.4open.science/r/Captcha-465E/).

</details>


### [64] [AtomGraph: Tackling Atomicity Violation in Smart Contracts using Multimodal GCNs](https://arxiv.org/abs/2512.02399)
*Xiaoqi Li,Zongwei Li,Wenkai Li,Zeng Zhang,Lei Xie*

Main category: cs.CR

TL;DR: AtomGraph is an automated framework using Graph Convolutional Networks to detect atomicity violations in smart contracts by combining CFG structural features and opcode semantics via adaptive fusion, achieving high accuracy and F1.


<details>
  <summary>Details</summary>
Motivation: Atomicity violations in smart contracts pose security risks; existing tools lack precision; adopt multimodal learning to improve detection.

Method: Graph Convolutional Networks with two modalities: CFG structural topological features and opcode sequence semantics; collaborative learning; adaptive weighted fusion; graph-level detection.

Result: Achieves 96.88% accuracy and 96.97% F1; outperforms existing tools; fusion via AtomGraph improves F1 by 6.4% over concatenation.

Conclusion: AtomGraph demonstrates strong efficacy for smart contract security detection, showing potential for improving atomicity-violation detection through multimodal graph learning.

Abstract: Smart contracts are a core component of blockchain technology and are widely deployed across various scenarios. However, atomicity violations have become a potential security risk. Existing analysis tools often lack the precision required to detect these issues effectively. To address this challenge, we introduce AtomGraph, an automated framework designed for detecting atomicity violations. This framework leverages Graph Convolutional Networks (GCN) to identify atomicity violations through multimodal feature learning and fusion. Specifically, driven by a collaborative learning mechanism, the model simultaneously learns from two heterogeneous modalities: extracting structural topological features from the contract's Control Flow Graph (CFG) and uncovering deep semantics from its opcode sequence. We designed an adaptive weighted fusion mechanism to dynamically adjust the weights of features from each modality to achieve optimal feature fusion. Finally, GCN detects graph-level atomicity violation on the contract. Comprehensive experimental evaluations demonstrate that AtomGraph achieves 96.88% accuracy and 96.97% F1 score, outperforming existing tools. Furthermore, compared to the concatenation fusion model, AtomGraph improves the F1 score by 6.4%, proving its potential in smart contract security detection.

</details>


### [65] [Leveraging Large Language Models to Bridge On-chain and Off-chain Transparency in Stablecoins](https://arxiv.org/abs/2512.02418)
*Yuexin Xiang,Yuchen Lei,SM Mahir Shazeed Rish,Yuanzhe Zhang,Qin Wang,Tsz Hon Yuen,Jiangshan Yu*

Main category: cs.CR

TL;DR: 提出一个基于大语言模型的框架，将稳定币的链上发行数据与离线披露文本对齐，以提升跨模态透明度和自动审计能力。


<details>
  <summary>Details</summary>
Motivation: 当前稳定币透明性分散在链上可核验数据和离线文本披露之间，缺乏统一的跨模态对齐。

Method: 提出集成框架，通过文档解析和语义对齐提取发行方的关键金融指标并映射到链上指标；在多链发行记录和披露文档上构建模型上下文协议（MCP），实现对量化市场数据和定性披露文本的统一访问与检索；演示跨模态数据分析能力，量化披露与观测流通之间的差异，并评估对价格动态的影响。

Result: 发现披露数据与可验证数据之间存在系统性差距，LLM辅助分析提升跨模态透明度，支持自动化、数据驱动的审计在DeFi中的应用。

Conclusion: 该工作展示了LLM在多模态、跨链稳定币信息整合中的潜力，为跨源透明度与合规性分析提供新路径。

Abstract: Stablecoins such as USDT and USDC aspire to peg stability by coupling issuance controls with reserve attestations. In practice, however, the transparency is split across two worlds: verifiable on-chain traces and off-chain disclosures locked in unstructured text that are unconnected. We introduce a large language model (LLM)-based automated framework that bridges these two dimensions by aligning on-chain issuance data with off-chain disclosure statements. First, we propose an integrative framework using LLMs to capture and analyze on- and off-chain data through document parsing and semantic alignment, extracting key financial indicators from issuer attestations and mapping them to corresponding on-chain metrics. Second, we integrate multi-chain issuance records and disclosure documents within a model context protocol (MCP) framework that standardizes LLMs access to both quantitative market data and qualitative disclosure narratives. This framework enables unified retrieval and contextual alignment across heterogeneous stablecoin information sources and facilitates consistent analysis. Third, we demonstrate the capability of LLMs to operate across heterogeneous data modalities in blockchain analytics, quantifying discrepancies between reported and observed circulation and examining their implications for cross-chain transparency and price dynamics. Our findings reveal systematic gaps between disclosed and verifiable data, showing that LLM-assisted analysis enhances cross-modal transparency and supports automated, data-driven auditing in decentralized finance (DeFi).

</details>


### [66] [Equilibrium SAT based PQC: New aegis against quantum computing](https://arxiv.org/abs/2512.02598)
*Keum-Bae Cho*

Main category: cs.CR

TL;DR: 提出了一种新型公钥密码体制：通过从多重集合中随机抽取子集并计数其元素来生成密文；使用可满足性问题对该概念进行数学 refine；声称在量子抗性条件下具有较高速度、无需大整数且可扩展为单公钥和多私钥的体系。


<details>
  <summary>Details</summary>
Motivation: 在互联网和量子计算快速发展背景下，现有公钥算法的计算复杂度和对大整数运算的依赖成为瓶颈，需要更高效且对资源友好的量子抗性方案。

Method: 核心思想是通过计数从多重集合中随机抽取的子集元件数来产生密文，并以可满足性问题（SAT）为工具对该概念进行数学 Refinement；提出在此基础上形成一个包含单一公钥和多私钥的公钥体系框架。

Result: 声称该算法显著快于现有公钥算法、无需大整数、可在任何设备上实现，且可扩展为具有量子抗性的公钥系统，且可用单公钥与多私钥组合实现。

Conclusion: 该工作提出的思路具有潜在实际应用价值，特别是在资源受限环境和需要量子抗性的场景；但仍需进行严格的安全性分析、实现验证及对可能的攻击路径进行评估。

Abstract: Public-key cryptography algorithms have evolved towards increasing computational complexity to hide desired messages, which is accelerating with the development of the Internet and quantum computing. This paper introduces a novel public-key cryptography algorithm that generates ciphertexts by counting the number of elements in randomly extracted subsets from a multiset. After explaining the novel cryptographic concept, the process of mathematically refining it using satisfiability problems is described. The advantages of the proposed algorithm are: first, it is significantly faster than other public-key algorithms; second, it does not require big numbers, making it executable on any devices; and third, it can be easily extended into a public-key cryptosystem using a single public key and multiple private keys while maintaining quantum resistance.

</details>


### [67] [Semigroup action based on skew polynomial evaluation with applications to Cryptography](https://arxiv.org/abs/2512.02603)
*Daniel Camazón-Portela,Juan Antonio López-Ramos*

Main category: cs.CR

TL;DR: 本文提出在有限域 F_q 上的伪多项式环 F_q[X; σ, δ] 的一个作用，基于多项式估值和左向偏斜积，构造集合 T(X) 以控制非交换性，并由此设计在 Canetti-Krawczyk 模型下安全的公钥交换协议。


<details>
  <summary>Details</summary>
Motivation: 研究偏斜多项式环的非交换性对密码学协议的潜在作用，提供一种在 CK 模型下具可行性的新公钥交换方案。

Method: 定义基于多项式估值和左向偏斜积的作用；构造子集 T(X) 来控制非交换性并分析其性质；基于该结构设计公钥交换协议并讨论安全性要点。

Result: 提出了一个可操作的框架与集合 T(X)，用于把控非交换性，并据此构建一个在 CK 模型下的公钥交换协议，给出设计思路与可行性分析。

Conclusion: 该工作提供了一个新颖的利用偏斜多项式环非交换性的密码学框架，为基于非对称代数的公钥交换提供新的方向，未来需进行严格的安全证明与参数设计。

Abstract: Through this work we introduce an action of the skew polynomial ring $\mathbb{F}_{q}\left[X; σ, δ\right]$ over $\mathbb{F}_{q}$ based on its polynomial valuation and the concept of left skew product of functions. This lead us to explore the construction of a certain subset $\mathcal{T}(X)\subset\mathbb{F}_{q}\left[X; σ, δ\right]$ that allow us to control the non-commutativity of this ring, and exploit this fact in order to build a public key exchange protocol that is secure in Canetti and Krawczyk model.

</details>


### [68] [CryptoQA: A Large-scale Question-answering Dataset for AI-assisted Cryptography](https://arxiv.org/abs/2512.02625)
*Mayar Elfares,Pascal Reisert,Tilman Dietz,Manpa Barman,Ahmed Zaki,Ralf Küsters,Andreas Bulling*

Main category: cs.CR

TL;DR: CryptoQA：首个用于密码学的大规模问答数据集，揭示大模型在密码学推理与精确数学方面的不足，并显示通过专门数据可提升该领域性能。


<details>
  <summary>Details</summary>
Motivation: 密码学研究对深层推理和严格数学推理的需求高，但现有数据对评估与训练LLMs在此领域的能力不足。缺乏针对密码学任务的专业数据集妨碍模型的开发与对比。

Method: 构建CryptoQA：从精选学术资源中抽取并生成超过200万对问答，附带上下文元数据以测试并训练LLMs在密码学任务中的能力；对15种前沿LLMs进行全面基准评估，考察事实准确性、数学推理、连贯性、引用、向后推理以及对对抗样本的鲁棒性；提供专家评审与黄金基线，并展示通过CryptoQA对LLMs进行微调以提升在密码学任务上的表现。

Result: 模型在需要严谨形式化推理和精确数学知识的任务上表现显著不足；通过在CryptoQA上的微调，LLMs在密码学任务上有可观提升。数据集还具备训练新模型的潜力，且可用于评估模型的推理与鲁棒性等维度。

Conclusion: CryptoQA证明了为密码学专门设计的LLM助手的迫切性；该数据集不仅是评估基准，也是训练资源，能够推动密码学研究与开发中的AI辅助工具的发展。

Abstract: Large language models (LLMs) excel at many general-purpose natural language processing tasks. However, their ability to perform deep reasoning and mathematical analysis, particularly for complex tasks as required in cryptography, remains poorly understood, largely due to the lack of suitable data for evaluation and training. To address this gap, we present CryptoQA, the first large-scale question-answering (QA) dataset specifically designed for cryptography. CryptoQA contains over two million QA pairs drawn from curated academic sources, along with contextual metadata that can be used to test the cryptographic capabilities of LLMs and to train new LLMs on cryptographic tasks. We benchmark 15 state-of-the-art LLMs on CryptoQA, evaluating their factual accuracy, mathematical reasoning, consistency, referencing, backward reasoning, and robustness to adversarial samples. In addition to quantitative metrics, we provide expert reviews that qualitatively assess model outputs and establish a gold-standard baseline. Our results reveal significant performance deficits of LLMs, particularly on tasks that require formal reasoning and precise mathematical knowledge. This shows the urgent need for LLM assistants tailored to cryptography research and development. We demonstrate that, by using CryptoQA, LLMs can be fine-tuned to exhibit better performance on cryptographic tasks.

</details>


### [69] [Decryption thorough polynomial ambiguity: noise-enhanced high-memory convolutional codes for post-quantum cryptography](https://arxiv.org/abs/2512.02822)
*Meir Ariel*

Main category: cs.CR

TL;DR: 提出一种新型后量子公钥密码体制，利用有向图解码的噪声增强高记忆卷积码，生成伪随机生成矩阵以隐藏代数结构并抵抗结构性攻击。通过在解密过程引入强噪声，合法方在多项式时间内解码，而攻击者则需指数时间。声称安全裕度超越 Classic McEliece 超过 2^200，并支持线性时间解密、统一比特开销与长消息的可扩展性，且可通过并行解码器硬件/软件实现。


<details>
  <summary>Details</summary>
Motivation: 应对量子计算时代的公钥密码学需求，提升对抗结构性攻击的安全裕度，提供对长期、可扩展消息的线性时间解密能力，以及在硬件实现中的高效并行性。

Method: 通过有向图构造生成矩阵以获得随机性并隐藏代数结构，在解密阶段故意注入噪声，使得对方的多项式除法产生的噪声干扰导致高复杂度攻击；合法接收者能在多项式时间内解码，攻击者则需要指数时间；利用并行的有向图解码器阵列在多项式歧义性中识别正确明文，达到线性时间、按位一致成本和对长消息的可扩展性。

Result: 声称在对比 Classic McEliece 的安全裕度方面超出2^200的因子，且具有线性时间解密、统一成本、以及可扩展到很长消息长度的能力，理论上支持并行化部署。

Conclusion: 该方案被描绘为鲁棒、可扩展且具量子抗性公钥密码学的有力候选者，设计灵活性较高，现实部署可通过并行解码结构实现。

Abstract: We present a novel approach to post-quantum cryptography that employs directed-graph decryption of noise-enhanced high-memory convolutional codes. The proposed construction generates random-like generator matrices that effectively conceal algebraic structure and resist known structural attacks. Security is further reinforced by the deliberate injection of strong noise during decryption, arising from polynomial division: while legitimate recipients retain polynomial-time decoding, adversaries face exponential-time complexity. As a result, the scheme achieves cryptanalytic security margins surpassing those of Classic McEliece by factors exceeding 2^(200). Beyond its enhanced security, the method offers greater design flexibility, supporting arbitrary plaintext lengths with linear-time decryption and uniform per-bit computational cost, enabling seamless scalability to long messages. Practical deployment is facilitated by parallel arrays of directed-graph decoders, which identify the correct plaintext through polynomial ambiguity while allowing efficient hardware and software implementations. Altogether, the scheme represents a compelling candidate for robust, scalable, and quantum-resistant public-key cryptography.

</details>


### [70] [Belobog: Move Language Fuzzing Framework For Real-World Smart Contracts](https://arxiv.org/abs/2512.02918)
*Wanxu Xia,Ziqiao Kong,Zhengwei Li,Yi Lu,Pan Li,Liqun Yang,Yang Liu,Xiapu Luo,Shaohua Li*

Main category: cs.CR

TL;DR: 一种面向 Move 的类型感知模糊测试框架 Belobog，通过类型图驱动交易生成，结合 concolic 执行实现高效漏洞检测与攻击复现。


<details>
  <summary>Details</summary>
Motivation: Move 的强类型系统对传统模糊测试的有效性构成挑战，需要一个能够产出类型正确的交易以测试合约安全性的框架；同时需要能够发现类型系统外的漏洞，并能复现真实攻击。

Method: 基于 Move 的类型系统构建类型图；在类型图的轨迹上生成或变异交易；设计并实现结合具体执行和符号执行的 concolic 执行器；在 109个真实 Move 合约项目上评估，展示检测效果和对 Cetus、Nemo 的攻击复现。

Result: 在实验中 Belobog 能发现 100% 的关键漏洞和 79% 的一般漏洞（均经人工审计确认），并能在无需先验知识的情况下成功复现 Cetus 与 Nemo 的完整利用。

Conclusion: 面向 Move 的类型感知模糊测试框架有效提升了 Move 合约的安全验证能力，证明了通过类型域驱动的生成和 concolic 执行在发现和复现实际攻击中的价值，适用于 Move 生态的安全审计。

Abstract: Move is a research-oriented programming language design for secure and verifiable smart contract development and has been widely used in managing billions of digital assets in blockchains, such as Sui and Aptos. Move features a strong static type system and explicit resource semantics to enforce safety properties such as the prevention of data races, invalid asset transfers, and entry vulnerabilities. However, smart contracts written in Move may still contain certain vulnerabilities that are beyond the reach of its type system. It is thus essential to validate Move smart contracts. Unfortunately, due to its strong type system, existing smart contract fuzzers are ineffective in producing syntactically or semantically valid transactions to test Move smart contracts. This paper introduces the first fuzzing framework, Belobog, for Move smart contracts. Belobog is type-aware and ensures that all generated and mutated transactions are well-typed. More specifically, for a target Move smart contract, Belobog first constructs a type graph based on Move's type system, and then generates or mutates a transaction based on the graph trace derived from the type graph. In order to overcome the complex checks in Move smart contracts, we further design and implement a concolic executor in Belobog. We evaluated Belobog on 109 real-world Move smart contract projects. The experimental results show that Belobog is able to detect 100\% critical and 79\% major vulnerabilities manually audited by human experts. We further selected two recent notorious incidents in Move smart contracts, i.e., Cetus and Nemo. Belobog successfully reproduced full exploits for both of them, without any prior knowledge.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [71] [Weight distributions of simplex codes over finite chain rings and their Gray map images](https://arxiv.org/abs/2512.02149)
*Cristina Fernández-Córdoba,Sergi Sánchez-Aragón,Mercè Villanueva*

Main category: cs.IT

TL;DR: 提出了有限链环 R 上线性 simplex 码及其 α、β 型的构造，并给出其基本参数（最小汉明距离、完整权重分布）以及与 Griesmer 型界的最优性分析。


<details>
  <summary>Details</summary>
Motivation: 扩展对环码的研究，将线性码通过广义 Gray 映射映射到 F_q 上的码，提出新的 simplex 码族，揭示其参数与分布特性，并考察其在 Griesmer 型界下的最优性。

Method: 给出在有限链环 R 上的线性 simplex 码构造，区分 α、β 型；利用广义 Gray 映射将线性码与 F_q 上的码相关联；推导并计算最小汉明距离、完整权重分布，并研究与 Griesmer 型界的关系。

Result: 给出所构造码族的基本参数（包括最小距离和完整权重分布）；给出是否达到或接近 Griesmer 型界的结论，及对 α、β 型的区分性结果。

Conclusion: 该工作扩展了环上的 simplex 码族，给出明确的参数与分布特征，并对其在 Griesmer 型界的最优性进行了分析，为后续对不同类型和分布的研究提供基础。

Abstract: A linear code of length $n$ over a finite chain ring $R$ with residue field $\F_q$ is a $R$-submodule of $R^n$. A $R$-linear code is a code over $\F_q$ (not necessarily linear) which is the generalized Gray map image of a linear code over $R$. These codes can be seen as a generalization of the linear codes over $\Z_{p^s}$ with $p$ prime and $s \geq 1$. In this paper, we present the construction of linear simplex codes over $R$ and their corresponding $R$-linear simplex codes of type $α$ and $β$. Moreover, we show the fundamental parameters of these codes, including their minimum Hamming distance, as well as their complete weight distributions. We also study whether these simplex codes are optimal with respect to the Griesmer-type bound.

</details>


### [72] [Low-Power Double RIS-Assisted Mobile LEO Satellite Communications](https://arxiv.org/abs/2512.02255)
*Kunnathully Sadanandan Sanila,Rickard Nilsson,Emad Ibrahim,Neelakandan Rajamohan*

Main category: cs.IT

TL;DR: 提出一种双RISLEO卫星通信架构，通过在卫星天线近场和地面近端各设一个小型RIS，满足RIS间远场条件，结合双级波束成形，在能量受限场景中显著提升能效；仿真表明上行功耗可降低约40 dB，RIS面积约0.0625 m^2。


<details>
  <summary>Details</summary>
Motivation: 在能源受限的LEO卫星通信场景中，通过被动式再配置表面（RIS）实现高效信号强化与功耗降低，提升系统能源效率和性能。

Method: 提出双RIS架构（卫星端近场RIS与地面端RIS），确保两RIS之间满足远场；建立双RIS链路的近场与远场耦合路径损耗模型；采用双级波束成形以最大化接收信号功率并最小化总功耗；通过仿真验证性能与能效提升。

Result: 仿真结果显示在上行链路中功耗可降低约40 dB，使用面积约为0.0625 m^2的RIS即可实现显著能效提升，适用于能量受限的LEO卫星通信应用。

Conclusion: 双RIS与双级波束成形的组合能显著提升LEO卫星链路的能效与信号性能，为能量受限场景提供可行的低功耗解决方案。

Abstract: We propose a low-power mobile low earth orbit (LEO) satellite communication architecture, employing double reconfigurable intelligent surfaces (RIS) to enhance energy efficiency and signal performance. With a distance between RISs that satisfies the far-field requirement, this architecture positions one small RIS each in the near-field of the satellite's antenna and the user on the ground. Moreover, we develop a path loss model for the double-RIS communication link, considering the near-field and far-field effects. Further, with the help of dual-stage beamforming, the proposed system maximizes the signal power and minimizes power consumption. Simulation results show that the proposed architecture can reduce the power consumption with 40 dB in the uplink, with a small $0.25^2$ $\text{m}^2$ RIS near the user, to communicate in energy-constrained LEO satellite communication circumstances.

</details>


### [73] [Entropies associated with orbits of finite groups](https://arxiv.org/abs/2512.02257)
*Ryan Leal,Jingtong Sun,Juan Pablo Vigneaux*

Main category: cs.IT

TL;DR: 通过对有限群及其 Lie 型对称性的共轭分区（parabolic 子群作用产生的旗稳定集合）之轨道计数的渐近分析，将信息熵与群论中的计数联系起来；在 A_n 系（对称群和 GL_n(F_q)）之外，拓展到 B_n, C_n, D_n 及其他例外型，揭示不同系列对应的熵泛函（含香农熵、Tsallis 熵及新型熵）并给出可能的乘法规则的渐近等效。


<details>
  <summary>Details</summary>
Motivation: 将信息理论中的乘法法则与群作用下的轨道计数联系起来，揭示旗稳定性与测度之间的深层结构。通过把对称群、线性群及其他 Lie-type 群的轨道数量的渐近增长映射到熵量度，统一信息论与代数结构的视角，拓展对不同群系列的熵概念。

Method: 分析有限反射群及其 Lie 型群在旗集合上的 parabolic 子群稳定性，计算商空间的轨道基数；利用 q-多项式、q-位势等工具得到轨道数量的渐近对数，并将其与香农熵、Tsallis 熵及新型熵泛函对应起来；按 Dynkin 图的类型对 A_n, B_n, C_n, D_n 系及少数异常型逐一处理，展示不同系列的关联熵性质。

Result: 得到轨道计数的渐近表达与信息熵之间的对应：在 A_n 系（对称群与 GL_n(F_q)）中与香农/定常多项式相关的熵关系得到明确；在其他系列中出现与之不同的 q-惟熵型关系，指出存在新的信息度量或熵泛函；并以辛群为例说明 Lie 型群下的普遍性与局部性。

Conclusion: 经典信息理论公式可视为对称群商空间轨道的计数结果的一种实现。将分析扩展到其他有限反射群和 Lie 型群不仅保持了与熵的联系，还揭示了新的熵泛函，提示在代数结构与信息量度之间可建立更广泛的统一框架，为后续在群论、量子信息与统计物理中的应用提供新的方向。

Abstract: For certain groups, parabolic subgroups appear as stabilizers of flags of sets or vector spaces. Quotients by these parabolic subgroups represent orbits of flags, and their cardinalities asymptotically reveal entropies (as rates of exponential or superexponential growth). The multiplicative "chain rules" that involve these cardinalities induce, asymptotically, additive analogues for entropies. Many traditional formulas in information theory correspond to quotients of symmetric groups, which are a particular kind of reflection group; in this case, the cardinalities of orbits are given by multinomial coefficients and are asymptotically related to Shannon entropy. One can treat similarly quotients of the general linear groups over a finite field; in this case, the cardinalities of orbits are given by $q$-multinomials and are asymptotically related to the Tsallis 2-entropy. In this contribution, we consider other finite reflection groups as well as the symplectic group as an example of a classical group over a finite field (groups of Lie type). In both cases, the groups are classified by Dynkin diagrams into infinite series of similar groups $A_n$, $B_n$, $C_n$, $D_n$ and a finite number of exceptional ones. The $A_n$ series consists of the symmetric groups (reflection case) and general linear groups (Lie case). Some of the other series, studied here from an information-theoretic perspective for the first time, are linked to new entropic functionals.

</details>


### [74] [New Constructions of Non-GRS MDS Codes, Recovery and Determination Algorithms for GRS Codes](https://arxiv.org/abs/2512.02325)
*Guodong Wang,Hongwei Liu,Jinquan Luo*

Main category: cs.IT

TL;DR: 提出一类新的非GRS MDS码构造，长度达到理论上界；给出利用Cauchy矩阵的充要条件、与twisted GRS码的 inequivalence、与现有非GRS MDS码族的关系及具体构造，并给出两种高效判定/恢复参数的算法，复杂度比Sidelnikov-Shestakov攻击更优。


<details>
  <summary>Details</summary>
Motivation: 扩展非GRS MDS码的构造空间与识别能力，提升可构造的码长界限，明确不同非GRS MDS码族之间的关系，并提出高效的参数恢复与判定算法以对抗潜在的攻击。

Method: 利用Cauchy矩阵结构导出必要充分条件以判定码是否为MDS且是否非GRS；分析与twisted GRS码的等价性与差异；梳理并构造与现有非GRS MDS码相关的关系网，给出Explicit构造；设计两种算法用于判断给定生成矩阵是否对应GRS码以及从中恢复α向量与v向量，复杂度为O(nk+n)与对比攻击的复杂度。

Result: 给出长度可达 (q+3)/2（奇特性）或 (q+4)/2（偶特性）的非GRS MDS码的构造和相应充要条件；证明与twisted GRS码不等价；建立与其他非GRS MDS码族之间的关系并给出具体构造；提出两种高效算法，复杂度为O(nk+n)，相较Sidelnikov-Shestakov攻击（高达O(qk^2n+qk^3)）更高效。

Conclusion: 本研究扩大了非GRS MDS码的构造与识别工具箱，提供了理论与算法基础，提升了对GRS码及其非GRS替代的理解与应用潜力，并为后续对比与攻击研究提供可操作的优化路径。

Abstract: In this paper, we propose a new method for constructing a class of non-GRS MDS codes. The lengths of these codes can reach up to $\frac{q+3}{2}$ (for finite fields of odd characteristic) and $\frac{q+4}{2}$ (for even characteristic), respectively. Owing to their special structure, we can use the Cauchy matrix method to obtain the necessary and sufficient conditions for these codes to be MDS codes and non-GRS MDS codes. Additionally, the inequivalence between these codes and twisted GRS codes is analyzed. Furthermore, we analyze the relationships among several existing classes of codes used for constructing non-GRS MDS codes, propose explicit constructions, and discuss the lengths of non-GRS MDS codes based on these constructions. Finally, we design two efficient algorithms to address two main problems in GRS code research, i.e., determining whether an unknown code $C$ is a GRS code from its generator matrix $G$, and recovering the key vectors $\bmα$ and $\bm{v}$ such that $C = \GRS_{n,k}(\bmα, \bm{v})$ if $C$ is indeed a GRS code. A computational complexity comparison of the proposed algorithms ($O(nk+n)$) with that of the Sidelnikov-Shestakov attack (exceeding $O(qk^2n+qk^3)$) shows that our methods offer superior computational efficiency.

</details>


### [75] [Age of Information for Constrained Scheduling with Imperfect Feedback](https://arxiv.org/abs/2512.02332)
*Yuqing Zhu,Yuan-Hsun Lo,Yan Lin,Yijin Zhang*

Main category: cs.IT

TL;DR: 提出在下行系统中考虑不完美反馈与传输速率受限的AoI最优化调度：在零反馈下给出AoI闭式下界并设计可达该下界的策略；在Bernoulli流量下建立带阈值的DPP策略并给出性能保证；扩展至一般不完美反馈且保持相同在线复杂度，数值结果验证理论与AoI优势。


<details>
  <summary>Details</summary>
Motivation: 现实系统中反馈往往不可靠且带宽有限，这对信息新鲜度（AoI）优化带来挑战。研究在有限反馈与速率约束下的调度策略，提升AoI表现并给出理论保证。

Method: 1) 零反馈、generate-at-will流量：推导AoI的闭式下界，提出通过速率分割与模算术相结合的策略以实现下界；2) 零反馈、Bernoulli流量：基于Lyapunov优化的DPP策略，具阈值结构并给出闭式性能保证；3) 将DPP策略扩展到一般不完美反馈，仍保持同等在线计算复杂度。

Result: 得到AoI下界；设计的策略在多种情形接近或达到下界；DPP策略有明确的性能保证；扩展至一般不完美反馈且不增加在线复杂度；数值结果验证理论分析并显示优势。

Conclusion: 给出一个在反馈受限和带宽受限的现实系统中提升AoI的综合调度框架，兼具理论保障与可实施性，并通过数值验证了相对于现有策略的AoI改善。

Abstract: This paper considers a downlink system where an access point sends the monitored status of multiple sources to multiple users. By jointly accounting for imperfect feedback and constrained transmission rate, which are key limited factors in practical systems, we aim to design scheduling algorithms to optimize the age of information (AoI) over the infinite time horizon. For zero feedback under the generate-at-will traffic, we derive a closed-form lower bound of achievable AoI, which, to the best of our knowledge, reflects the impact of zero feedback for the first time, and propose a policy that achieves this bound in many cases by jointly applying rate splitting and modular arithmetic. For zero feedback under the Bernoulli traffic, we develop a drift-plus-penalty (DPP) policy with a threshold structure based on the theory of Lyapunov optimization and provide a closed-form performance guarantee. Furthermore, we extend the design of this DPP policy to support general imperfect feedback without increasing the online computational complexity. Numerical results verify our theoretical analysis and the AoI advantage of the proposed policies over state-of-the-art policies.

</details>


### [76] [A Cyclic Shift Embedded Pilot based Channel Estimation for Multi-User MIMO-OTFS systems with fractional delay and Doppler](https://arxiv.org/abs/2512.02353)
*Ruizhe Wang,Hong Ren,Cunhua Pan,Ruisong Weng,Jiangzhou Wang*

Main category: cs.IT

TL;DR: 在多用户 MIMO-OTFS 系统中，提出基于多维分解的信道估计与 CSEP 结构的低开销 Pilot 方案，通过 AoA 的子空间估计和空间投影实现路径去干扰，辅以离散化前沿的 CS 信道估计，且通过循环移位的 ZC 序列实现 Pilot 复用，显著降低 Pilot 开销并在 BER/计算量之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: OTFS 在高多普勒/高移动场景下的信道估计困难，传统嵌入式 Pilot 需为每个用户分配独立 Pilot，导致 Pilot 开销随用户数线性增加，亟需降低多用户场景的开销与复杂度。

Method: 提出多维分解基信道估计：先用子空间方法估计 AOAs，并据此构建空间投影矩阵实现对传播路径子空间的解耦，随后对剩余的分数时延与多普勒进行 CS 的 off-grid 信道估计；同时提出 Cyclic Shift Embedded Pilot (CSEP) 结构，通过 Zadoff-Chu 序列的循环移位正交性实现多用户的 Pilot 复用以降低开销；并给出基于 CSEP 的改进信道估计算法。

Result: 仿真结果显示信道估计性能优于对比方法，CSEP 结构与估计算法在计算复杂度、估计精度和 BER 性能之间达到较好的折衷，Pilot 开销至少节省约 30%。

Conclusion: 该方法为多用户 OTFS 系统提供一个高效、低开销的信道估计与 Pilot 设计方案，适用于高移动性场景。

Abstract: Orthogonal time frequency space (OTFS) modulation has been proposed to meet the demand for reliable communication in high-mobility scenarios for future wireless networks. However, in multi-user OTFS systems, conventional embedded pilot schemes require independent pilot allocation for each user, leading to linearly increasing pilot overhead. To address these issues, in this paper, we investigate the uplink channel estimation and pilot design for multi-user multiple-input multiple-output (MIMO)-OTFS systems. We propose a multi-dimensional decomposition-based channel estimation algorithm. Specifically, the proposed algorithm first estimates the angles of arrivals (AoAs) via subspace decomposition-based method. A spatial projection matrix, constructed from the estimated AOAs, decouples the received signal by propagation path subspace, effectively mitigating inter-path interference. The remaining fractional delay and Doppler can be obtained by a compressed sensing (CS)-based off-grid channel estimation method. Furthermore, to reduce the pilot overhead in multi-user OTFS systems, this paper proposes a novel cyclic shift embedded pilot (CSEP) structure, which can reuse users through cyclic shift-orthogonality of Zadoff-Chu (ZC) sequences. Compared with conventional embedded pilot structures, the CSEP structure can save over 30\% of pilot overhead. Finally, an imporved channel estimation method based on the CSEP structure is proposed. Simulation results demonstrate that it achieves superior performance in channel estimation. Moreover, the proposed CSEP structure and channel estimation algorithm achieve a favorable balance between computational complexity, estimation accuracy, and bit error rate (BER) performance.

</details>


### [77] [Boltzmann-Shannon Index: A Geometric-Aware Measure of Clustering Balance](https://arxiv.org/abs/2512.02397)
*Emanuele Bossi,C. Tyler Diggans,Abd AlRahman R. AlMomani*

Main category: cs.IT

TL;DR: 提出并评估 Boltzmann-Shannon Index (BSI) 作为一个结合频率分布与几何分布的归一化聚类度量，在连续数据的分区中能反映群集的人口与几何尺度的交互，并在资源分配等场景提供优化友好目标和正则项。


<details>
  <summary>Details</summary>
Motivation: 常规聚类/分区指标在考虑数据的几何结构与分布信息方面往往互相独立或失效。在连续数据的聚类和资源分配问题中，需要一个能同时考虑密度与几何扩展的量化指标，以便在决策中兼顾人口权重与空间分布。

Method: 引入 Boltzmann-Shannon Index，与几何粗粒化和信息理论结合，归一化后将每个簇的人口与其有效几何尺度结合成一个联合概率分布并计算对应的熵/信息量，形成对分区的评价。通过对高斯混合、Iris 数据集和高不平衡资源分配场景的实验来展示其行为和特性，并给出在资源分配中的梯度友好目标与正则项。

Result: BSI 在给出传统指标信号不足时仍能提供一致且直观的评估；可以灵敏检测密度-几何不一致；在资源分配问题中既可以作为优化目标、也可以作为平滑的正则项，易于嵌入到策略制定和算法治理框架中。

Conclusion: BSI 将信息理论与几何直观结合，提供一个稳健、可解释的度量，适用于需要同时考虑群体权重与扩展性的聚类评估与治理优化场景。

Abstract: We introduce the Boltzmann-Shannon Index (BSI), a normalized measure for clustered continuous data that captures the interaction between frequency-based and geometry-based probability distributions. Building on ideas from geometric coarse-graining and information theory, the BSI quantifies how well a partition reflects both the population of each cluster and its effective geometric extent. We illustrate its behavior on synthetic Gaussian mixtures, the Iris benchmark, and a high-imbalance resource-allocation scenario, showing that the index provides a coherent assessment even when traditional metrics give incomplete or misleading signals. Moreover, in resource-allocation settings, we demonstrate that BSI not only detects severe density-geometry inconsistency with high sensitivity, but also offers a smooth, optimization-ready objective that naturally favors allocations balancing demographic weight with each group's effective spread in the outcome space, while providing a smooth, gradient-friendly regularizer that can be easily embedded in modern policy-making and algorithmic governance optimization frameworks.

</details>


### [78] [Optimal Handover Strategies in LEO Satellite Networks](https://arxiv.org/abs/2512.02449)
*Brendon McBain,Yi Hong,Emanuele Viterbo*

Main category: cs.IT

TL;DR: A general analytical framework for ergodic capacity of LEO satellite networks under arbitrary handover strategies, introducing a persistent channel via an i.i.d. renewal process, with closed-form bounds and a practical handover rule close to optimal.


<details>
  <summary>Details</summary>
Motivation: Limitations of existing analyses rely on restrictive assumptions (e.g., short serving times) and lack tractable handover evaluation for realistic LEO networks; a general framework is needed for accurate capacity characterization under uncoordinated handovers with known satellite ephemeris/fading.

Method: Model the link with shadowed-Rician fading; define the persistent satellite channel governed by an i.i.d. renewal process; apply renewal theory to derive persistent capacity and relate it to non-persistent capacity; derive bounds; formulate a nonlinear fractional program for handover optimization; solve via a variant of Dinkelbach's algorithm; show that a simpler serving-capacity-maximizing strategy closely tracks the optimum.

Result: Derivation of closed-form upper and lower bounds on persistent capacity; an explicit handover decision rule from the fractional programming; validation that a simpler serving-capacity-maximizing strategy closely approximates the optimal policy.

Conclusion: The framework enables accurate, tractable analysis of ergodic capacity under arbitrary handovers in LEO constellations, offering practical rules and bounds to guide the design of high-throughput satellite systems.

Abstract: Existing theoretical analyses of satellite mega-constellations often rely on restrictive assumptions, such as short serving times, or lack tractability when evaluating realistic handover strategies. Motivated by these limitations, this paper develops a general analytical framework for accurately characterising the ergodic capacity of low Earth orbit (LEO) satellite networks under arbitrary handover strategies. Specifically, we model the transmission link as shadowed-Rician fading and introduce the persistent satellite channel, wherein the channel process is governed by an i.i.d. renewal process under mild assumptions of uncoordinated handover decisions and knowledge of satellite ephemeris and fading parameters. Within this framework, we derive the ergodic capacity (persistent capacity) of the persistent satellite channel using renewal theory and establish its relation to the non-persistent capacity studied in prior work. To address computational challenges, we present closed-form upper and lower bounds on persistent capacity. The optimal handover problem is formulated as a non-linear fractional program, obtaining an explicit decision rule via a variant of Dinkelbach's algorithm. We further demonstrate that a simpler handover strategy maximising serving capacity closely approximates the optimal strategy, providing practical insights for designing high-throughput LEO satellite communication systems.

</details>


### [79] [Quantum Optimization in Wireless Communication Systems: Principles and Applications](https://arxiv.org/abs/2512.02468)
*Ioannis Krikidis,Valentin Gilbert*

Main category: cs.IT

TL;DR: 本文概述量子优化在无线通信系统设计中的应用框架，比较量子退火与门基QAOA的特性、优点与局限，并以二进式相位控制的被动RIS波束成形为案例，给出来自量子硬件的实验结果。


<details>
  <summary>Details</summary>
Motivation: 解决无线通信设计中的高维非线性优化和计算复杂性问题，探索量子优化在实际系统中的潜力与边界。

Method: 系统综述两大量子优化模型：量子退火与门基QAOA，比较其核心特征、性能、局限及差异；在被动RIS波束成形的二进相位分辨率案例中结合真实量子硬件实验数据进行分析。

Result: 给出两种量子优化模型的对比分析，并在RIS波束成形案例中展示实验可行性与潜在性能优势，同时指出实现与噪声鲁棒性等挑战。

Conclusion: 量子优化在无线系统设计中具有潜在显著的影响力，但需在硬件实现、算法适配及系统级集成方面进一步攻关，以促进实际应用。

Abstract: Quantum optimization is poised to play a transformative role in the design of next-generation wireless communication systems by addressing key computational and technological challenges. This paper provides an overview of the principles of adiabatic quantum computing, the foundation of quantum optimization, and explores its two primary computational models: quantum annealing and the gate-based quantum approximate optimization algorithm. By highlighting their core features, performance benefits, limitations, and distinctions, we position these methods as promising tools for advancing wireless communication system design. As a case study, we examine the design of passive reconfigurable intelligent surface beamforming with binary phase-shift resolution, supported by experimental results obtained from real-world quantum hardware.

</details>


### [80] [Digit-Indexed q-ary SEC-DED Codes with Near-Hamming Overhead](https://arxiv.org/abs/2512.02747)
*Jiaxu Hu,Kenneth J. Roche*

Main category: cs.IT

TL;DR: 提出一种基于基数展开的q进制单错误更正双错检（SEC-DED）线性码，采用坐标索引的基底位数来构造检验矩阵，在长度n=p^r时仅需要r+1个校验位，且解码以常量时间、单遍通过 syndromes 直接定位和幅度恢复，结构接近哈明码但开销更小。


<details>
  <summary>Details</summary>
Motivation: 动机在于在实现层面追求简单性、阵列友好性和可实现的解码性能，同时保持可观的错误纠正能力。通过将检验条件绑定到坐标索引的基数字，获取透明、明确的 syndrome 到错误定位的映射，从而实现逐位、低开销的解码流程。

Method: 核心构造利用坐标索引的 base-p 数字来定义 q 进制的校验关系，在 n=p^r 时仅需 r+1 个线性检验，形成近哈明(overhead near-Hamming)的结构。提出 Code A1 去除冗余 trit 提高信息速率并支持变长编码；Code A2 引入两组组和检验，以及对索引子集的三元 XOR 线性独立性条件，以获得三值距离为 4 的 SEC-TED 变体。进一步推广至 n-wise XOR 的线性独立集合，从而获得距离 d=n+1 的码，尤其能恢复三进制 Golay 码（n=5）。

Result: 构造出实现简单、阵列友好的 q 进制 SEC-DED 码族，检验都是按位数字和全局和实现，syndrome 到错误定位的映射明确且单遍解码可行；Code A1/Code A2 提高信息率和改进错误检测能力；通过 n-wise XOR 框架可推广到高距离，并与经典码（如 Golay）存在结构性联系。与传统的 q 进制 Hamming 及 SPC/乘积码基线相比，给出 parity 开销、解码工作量和两错行为的对比。

Conclusion: 该工作并非追求最优性，而是强调实现简易性与阵列友好性：检验为逐位数字和全局和，syndrome 到定位的映射显式，SEC-DED 的升级可模块化实现；同时提供了一个可扩展的框架，能将距离提升到 d=n+1，并与经典高效码有结构上的联系。

Abstract: We present a simple $q$-ary family of single-error-correcting, double-error-detecting (SEC--DED) linear codes whose parity checks are tied directly to the base-$p$ ($q=p$ prime) digits of the coordinate index. For blocklength $n=p^r$ the construction uses only $r+1$ parity checks -- \emph{near-Hamming} overhead -- and admits an index-based decoder that runs in a single pass with constant-time location and magnitude recovery from the syndromes. Based on the prototype, we develop two extensions: Code A1, which removes specific redundant trits to achieve higher information rate and support variable-length encoding; and Code A2, which incorporates two group-sum checks together with a 3-wise XOR linear independence condition on index subsets, yielding a ternary distance-4 (SEC--TED) variant. Furthermore, we demonstrate how the framework generalizes via $n$-wise XOR linearly independent sets to construct codes with distance $d = n + 1$, notably recovering the ternary Golay code for $n = 5$ -- showing both structural generality and a serendipitous link to optimal classical codes.
  Our contribution is not optimality but \emph{implementational simplicity} and an \emph{array-friendly} structure: the checks are digitwise and global sums, the mapping from syndromes to error location is explicit, and the SEC--TED upgrade is modular. We position the scheme against classical $q$-ary Hamming and SPC/product-code baselines and provide a small comparison of parity overhead, decoding work, and two-error behavior.

</details>


### [81] [Structural Properties of Entropic Vectors and Stability of the Ingleton Inequality](https://arxiv.org/abs/2512.02767)
*Rostislav Matveev,Andrei Romashchenko*

Main category: cs.IT

TL;DR: 在近似独立条件下研究 Ingleton 不等式的受限版本，给出稳定性界并通过结构性引理在不显式使用无限非Shannon型不等式的情况下揭示互信息的作用。


<details>
  <summary>Details</summary>
Motivation: Ingleton 不等式并非对所有熵分布都成立，但在某些严格独立性约束下成立。本工作拟在“较小但非零”的条件互信息情形下探究其稳定性，理解近似独立性对不等式的影响及界限。

Method: 提出一个结构性引理，显式地呈现两个变量之间部分互信息的结构，隐含地捕捉大量非Shannon型不等式的效应但不直接依赖于它们；在此框架中推导出当若干条件互信息很小时，Ingleton 不等式仍成立且带有可控误差；并将结果与 Matúš (2007) 和 Dougherty–Freiling–Zeger (2011) 的无穷族不等式联系起来。

Result: 给出在某些条件互信息小的情形下，Ingleton 不等式保有稳定性（含误差项）的界；部分结论统一了先前可由无穷族不等式推导出的结果，同时也给出若干新的界。

Conclusion: 通过结构性引理实现对近似独立性下 Ingleton 条件的透明证明与稳定性分析，避免直接诉诸大量无穷族不等式；这不仅统一了相关结果，也给出新界，为熵不等式的近似成立提供新工具与视角。

Abstract: We study constrained versions of the Ingleton inequality in the entropic setting and quantify its stability under small violations of conditional independence. Although the classical Ingleton inequality fails for general entropy profiles, it is known to hold under certain exact independence constraints. We focus on the regime where selected conditional mutual information terms are small (but not zero), and the inequality continues to hold up to controlled error terms. A central technical tool is a structural lemma that materializes part of the mutual information between two random variables, implicitly capturing the effect of infinitely many non-Shannon--type inequalities. This leads to conceptually transparent proofs without explicitly invoking such infinite families. Some of our bounds recover, in a unified way, what can also be deduced from the infinite families of inequalities of Matúš (2007) and of Dougherty--Freiling--Zeger (2011), while others appear to be new.

</details>


### [82] [Pseudocodewords of quantum, quasi-cyclic, and spatially-coupled LDPC codes: a fundamental cone perspective](https://arxiv.org/abs/2512.02941)
*Wittawat Kositwattanarerk,Gretchen L. Matthews,Emily McMillon,Tunchanok Yutitumsatit*

Main category: cs.IT

TL;DR: 本论文从LP解码视角系统分析伪码字在不同LDPC结构中的形成与特征，以及它们对解码性能的影响。


<details>
  <summary>Details</summary>
Motivation: 伪码字是阻碍LDPC近似容量性能实现的关键因素，且其形态受校验矩阵和解码算法影响；LP解码与图覆盖解码有紧密联系，分析伪码字有助于理解和改进解码策略，尤其针对量子稳定码、准循环LDPC与时空耦合LDPC等结构。

Method: 通过研究LP解码框架中的伪码字，结合图覆盖解码的等价描述，分析来自量子稳定码、准循环LDPC和时空耦合LDPC的伪码字的底层结构特征及其依赖关系。

Result: 揭示了伪码字结构在不同码型中的共性与差异，指出伪码字对LP解码及相关解码器性能的影响机制，并为设计更鲁棒的LDPC码与解码算法提供结构性线索。

Conclusion: 在LP解码及图覆盖解码框架下理解伪码字的结构有助于统一分析并指导未来的码设计，尤其在量子稳定码、准循环LDPC与时空耦合LDPC的应用场景中。

Abstract: While low-density parity-check (LDPC) codes are near capacity-achieving when paired with iterative decoders, these decoders may not output a codeword due to the existence of pseudocodewords. Thus, pseudocodewords have been studied to give insight into the performance of modern decoders including iterative and linear programming decoders. These pseudocodewords are found to be dependent on the parity-check matrix of the code and the particular decoding algorithm used. In this paper, we consider LP decoding, which has been linked to graph cover decoding, providing functions which capture these pseudocodewords. In particular, we analyze the underlying structure of pseudocodewords from quantum stabilizer codes that arise from LP decoding, quasi-cyclic LDPC codes, and spatially-coupled LDPC codes.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [83] [Edge-Native, Behavior-Adaptive Drone System for Wildlife Monitoring](https://arxiv.org/abs/2512.02285)
*Jenna Kline,Rugved Katole,Tanya Berger-Wolf,Christopher Stewart*

Main category: eess.SY

TL;DR: Edge-native, behavior-adaptive drone monitoring for wildlife that automates vigilance monitoring to reduce animal stress and improve data usability; real-time inference at 23.8ms per frame; significant reductions in adverse behaviors compared to manual piloting.


<details>
  <summary>Details</summary>
Motivation: Human operators face attentional bottlenecks when monitoring large animal groups; conventional piloting can trigger stress and fleeing, reducing data validity. The paper aims to balance close monitoring with animal welfare by automating vigilance detection and providing context-specific alerts.

Method: An edge-native pipeline using YOLOv11m for detection and YOLO-Behavior for behavior classification; aggregates vigilance into a real-time group stress metric; operator-tunable alert thresholds from 30fps video frames; SLO-based performance target (33ms per frame) with GPU-accelerated inference achieving 23.8ms; retrospective mission analysis and simulation of operator interventions.

Result: Achieved 23.8ms per-frame inference on GPU hardware; retrospective seven-mission analysis shows manual piloting yields 14s average adverse behavior and 71.9% usable frames; actionable alerts could arrive 51s before flee in 57% of missions; simulating 5s operator intervention yields 82.8% usable frames with 1s adverse behavior, a 93% reduction vs manual piloting.

Conclusion: The system demonstrates that automated vigilance monitoring and context-aware alerting can substantially improve data usability and animal welfare in wildlife drone surveys, enabling proactive interventions and substantial reductions in adverse behavior compared to manual piloting.

Abstract: Wildlife monitoring with drones must balance competing demands: approaching close enough to capture behaviorally-relevant video while avoiding stress responses that compromise animal welfare and data validity. Human operators face a fundamental attentional bottleneck: they cannot simultaneously control drone operations and monitor vigilance states across entire animal groups. By the time elevated vigilance becomes obvious, an adverse flee response by the animals may be unavoidable. To solve this challenge, we present an edge-native, behavior-adaptive drone system for wildlife monitoring. This configurable decision-support system augments operator expertise with automated group-level vigilance monitoring. Our system continuously tracks individual behaviors using YOLOv11m detection and YOLO-Behavior classification, aggregates vigilance states into a real-time group stress metric, and provides graduated alerts (alert vigilance to flee response) with operator-tunable thresholds for context-specific calibration. We derive service-level objectives (SLOs) from video frame rates and behavioral dynamics: to monitor 30fps video streams in real-time, our system must complete detection and classification within 33ms per frame. Our edge-native pipeline achieves 23.8ms total inference on GPU-accelerated hardware, meeting this constraint with a substantial margin. Retrospective analysis of seven wildlife monitoring missions demonstrates detection capability and quantifies the cost of reactive control: manual piloting results in 14 seconds average adverse behavior duration with 71.9% usable frames. Our analysis reveals operators could have received actionable alerts 51s before animals fled in 57% of missions. Simulating 5-second operator intervention yields a projected performance of 82.8% usable frames with 1-second adverse behavior duration,a 93% reduction compared to manual piloting.

</details>


### [84] [On Frequency-Weighted Extended Balanced Truncation](https://arxiv.org/abs/2512.02298)
*Sribalaji C. Anand,Henrik Sandberg*

Main category: eess.SY

TL;DR: 提出适用于离散与连续时间线性时不变系统的频率加权扩展平衡截断，离散时间系统的Lyapunov不等式及其扩展形式存在分块对角解，给出递归算法及其A priori误差界，理论结果推广到连续时间并通过数值实例验证。


<details>
  <summary>Details</summary>
Motivation: 在需要同时关注特定频率带内性能的场景中，对线性时不变系统进行高效的模型简化，保证在频率区间内的保真度，同时兼顾离散与连续时间两类系统的可用性。

Method: 1) 证明离散时间系统的频率加权扩展平衡截断的Lyapunov不等式具有分块对角解；2) 提出用于扩展平衡截断的递归算法，并给出a priori误差界；3) 将理论推广至连续时间系统；4) 通过数值例子验证理论。

Result: 建立了频率加权扩展平衡截断的理论框架：离散时间系统存在分块对角的Lyapunov解、递归的扩展平衡截断算法及其误差界；拓展到连续时间并通过数值验证。

Conclusion: 为频率加权且带扩展平衡截断的模型化简提供了一致的理论与可实施算法，适用于离散与连续时间的LTI系统，并给出明确的误差界与数值验证。

Abstract: This paper addresses the problem of frequency-weighted extended balanced truncation for discrete and continuous-time linear time-invariant plants. We show that the frequency-weighted discrete-time plant admits block-diagonal solutions to both the Lyapunov inequality and its extended form. A recursive algorithm for extended balanced truncation is proposed, together with corresponding a-priori error bounds. Theoretical results are extended to continuous-time systems and validated through numerical examples.

</details>


### [85] [On the Convergence of Density-Based Predictive Control for Multi-Agent Non-Uniform Area Coverage](https://arxiv.org/abs/2512.02367)
*Sungjun Seo,Kooktae Lee*

Main category: eess.SY

TL;DR: 提出了一种基于密度的预测控制（DPC）的多智能体覆盖控制方法，结合最优传输理论实现对非均匀参考分布的高效覆盖；在Wasserstein距离下分析收敛性，给出无约束的解析最优控制律及有约束场景的数值方法；仿真表明DPC能使轨迹接近非均匀参考分布，优于现有覆盖方法。


<details>
  <summary>Details</summary>
Motivation: 在大规模场景中（如搜救、环境监测），存在区域优先级差异，传统的均匀覆盖无法高效分配资源。通过利用目标分布（参考分布）实现对高优先区域的更多覆盖，从而提升覆盖效率与效果。

Method: 提出密度基预测控制（DPC），以预构建的参考分布为目标，分配多智能体的覆盖努力；分析基于Wasserstein距离的收敛性；在无约束下推导解析最优控制律；针对有约束情形提出数值求解方法。仿真采用一阶动力学和线性化四旋翼模型。

Result: DPC的轨迹能够较好地贴合非均匀参考分布，覆盖效果优于现有方法；在通过仿真验证的场景中，方法对高优先区域的覆盖更充分。

Conclusion: DPC能有效实现非均匀区域覆盖，且在收敛性分析和无约束解析解的基础上，提供了适用于约束情形的数值解法，适合大规模搜索与监测等应用，具扩展潜力。

Abstract: This paper presents Density-based Predictive Control (DPC), a novel multi-agent control strategy for efficient non-uniform area coverage, grounded in optimal transport theory. In large-scale scenarios such as search and rescue or environmental monitoring, traditional uniform coverage fails to account for varying regional priorities. DPC leverages a pre-constructed reference distribution to allocate agents' coverage efforts, spending more time in high-priority or densely sampled regions. We analyze convergence conditions using the Wasserstein distance, derive an analytic optimal control law for unconstrained cases, and propose a numerical method for constrained scenarios. Simulations on first-order dynamics and linearized quadrotor models demonstrate that DPC achieves trajectories closely matching the non-uniform reference distribution, outperforming existing coverage methods.

</details>


### [86] [Necessary and Sufficient Conditions for PID Design of MIMO Nonlinear Systems](https://arxiv.org/abs/2512.02452)
*Tianyou Xiang,Cheng Zhao*

Main category: eess.SY

TL;DR: 为具有未知输入增益和非线性不确定性的多输入、二阶系统，给出在Jacobian边界下的三维PID增益区域，既充分又必要，并在额外结构假设下达到必要充足的完备性；区域为闭式，依赖Jacobian边界与输入增益下界。


<details>
  <summary>Details</summary>
Motivation: 弥补对非线性不确定MIMO二阶系统的PID设计理论不足，提供显式的增益区间以保证全局稳定和跟踪，对所有满足Jacobian边界的不确定性成立。

Method: 通过对系统非线性不确定性在状态变量的雅可比界限进行建模，推导出三维PID增益的充分区，进一步推导必要区；在额外结构性假设下，充分区与必要区重合，给出完备的必要充足表征；所有区域均为闭式表达，且仅依赖于Jacobian界和输入增益下界。

Result: 给出稳定性和跟踪的闭式的PID增益区域，包含三维区域，且在额外结构假设下达到必要充足的一致性；区域仅依赖边界量，较一般调参方法更具可验证性。

Conclusion: 提供一个严格的、显式的PID设计框架，用于具有不确定性和未知输入增益的MIMO二阶系统，弥合理论和实操之间的差距；在具备额外结构信息时，给出全局稳定的必要充足条件的精确刻画。

Abstract: As is well known, classical PID control is ubiquitous in industrial processes, yet a rigorous and explicit design theory for nonlinear uncertain MIMO second-order systems remains underdeveloped. In this paper we consider a class of such systems with both uncertain dynamics and an unknown but strictly positive input gain, where the nonlinear uncertainty is characterized by bounds on the Jacobian with respect to the state variables. We explicitly construct a three-dimensional region for the PID gains that is sufficient to guarantee global stability and asymptotic tracking of constant references for all nonlinearities satisfying these Jacobian bounds. We then derive a corresponding necessary region, thereby revealing the inherent conservatism required to cope with worst-case uncertainties. Moreover, under additional structural assumptions on the nonlinearities, these sufficient and necessary regions coincide, yielding a precise necessary-and-sufficient characterization of all globally stabilizing PID gains. All these regions are given in closed form and depend only on the prescribed Jacobian bounds and the known lower bound of the input gain, in contrast to many qualitative tuning methods in the literature.

</details>


### [87] [Reduced-order Smith predictor for state feedback control with guaranteed stability](https://arxiv.org/abs/2512.02579)
*Jesus-Pablo Toledo-Zucco,Frédéric Gouaisbaut,Gaetan Chapput*

Main category: eess.SY

TL;DR: 提出一种将Smith Predictor用于状态反馈控制的实现方法，采用动态控制器近似需要积分项的控制律，通过Lyapunov与LMIs给出稳定性条件，并在三个典型示例中验证优势。


<details>
  <summary>Details</summary>
Motivation: 将通过偏微分方程和反步控制得到且包含需要离散实现的积分项的控制律，转化为可实现的动态控制器，同时保持系统的稳定性与性能。

Method: 将控制律分解为状态反馈项和一个动态项，后者渐进逼近待估的积分项；构建一个Lyapunov泛函，并将稳定性条件转化为LMIs，给出充要条件；通过三组文献中的示例进行验证。

Result: 给出闭环稳定性的充要条件（以LMIs形式），并通过三例示例显示所提方法在实现性与性能方面的优点。

Conclusion: 所提动态实现的Smith Predictor控制策略为PDE-Backstepping推导的控制律提供一个可实现且稳定的设计框架，且在三种典型示例中得到验证，具有推广潜力，但需在不同系统参数下进一步评估。

Abstract: This article deals with the implementation of the Smith Predictor for state feedback control in state space representation. The desired control law, obtained using partial differential equations and backstepping control, contains an integral term that has to be approximated for implementation. In this article, we propose a new way to implement this control law using a dynamic controller. The control law is composed of a state feedback term and a dynamic term that approaches the integral term that has to be estimated for implementation. Using a Lyapunov functional, we provide sufficient conditions, in terms of a linear matrix inequality, to guarantee that the closed-loop system is stable when the proposed control law is applied. We use three examples, taken from the literature, to show the benefits of the proposed approach.

</details>


### [88] [Modal Analysis of Core Inertial Dynamics: Re-evaluating Grid-Forming Control Design Principles](https://arxiv.org/abs/2512.02662)
*Gerardo Medrano,Santiago Cóbreces*

Main category: eess.SY

TL;DR: 通过模态分析揭示 GC-SG 与基于 Droop 的 GFM 逆变器之间的相互作用，挑战“高惯性/高阻尼” 的传统做法，提出低 droop 与低虚拟惯性的组合可实现更强阻尼和更小频率波动。


<details>
  <summary>Details</summary>
Motivation: 理解发电机组调速器–汽轮机动力学与 droop 型 GFM 逆变器的基本耦合与权衡，以评估在含大量 IBR 的现代电网中对稳定性设计的最优策略。

Method: 对 GC-SG（包含汽轮机-调速器模型）、基于 Droop 的 GFM 逆变器及其耦合进行模态分析，考察摆动模式、汽轮机–调速器模式及其耦合效应。

Result: 核心发现包括：1）以仿真 GC-SG 行为的高惯性/高阻尼策略在简单场景下可能不是最优；2）GC-SG 的阻尼需要较大 droop，然而这会增大全稳态频偏并依赖二次调节；3）GFM 逆变器中，降低 droop 可同时降低稳态频偏并增强阻尼，且虚拟惯性可自由设定；4）当两台 GC-SG 耦合时，会出现阻尼较差的机电摆振模式；5）用 GFM 替换其中之一（保持等效的 droop 和惯性）即可显著改善摆振与汽轮机–调速器模式的阻尼；6）进一步显著降低 GFM 虚拟惯性可获得额外阻尼增益；7）综合而言，当前行业趋势可能压缩 IBR 的潜在收益，最佳稳定性应通过低 droop 与低虚拟惯性实现，需重新评估 GFM 控制设计与网格代码要求。

Conclusion: 需要重新评估 GFM 控制设计原则及网格规约，主张低 droop 与低虚拟惯性以实现更紧凑的频率波动边界和更强的机电阻尼，从而充分释放 IBR 的稳定性潜力。

Abstract: This paper employs modal analysis to study the core inertial dynamics of governor-controlled synchronous generators (GC-SG), droop-based grid-forming (GFM) converters, and their most fundamental interactions. The results indicate that even in the simplest cases, the prevailing industry paradigm of emulating legacy GC-SG behaviour in GFM converters (high inertia to slow down the system and large droop to increase damping) could be a suboptimal policy. It is shown that GC-SGs exhibit a fundamental trade-off: adequate damping of the turbine-governor mode requires large droop constants, inevitably increasing steady-state frequency deviation and dependence on secondary regulation. In contrast, droop-based GFM converters invert this relationship: decreasing the droop constant simultaneously reduces steady-state frequency deviations and increases damping, while allowing virtual inertia to be freely chosen. When two GC-SGs are coupled, the poorly damped electromechanical swing mode emerges. Results show that replacing one GC-SG with a GFM converter of equivalent droop and inertia already significantly improves damping of both swing and turbine-governor modes. Counter-intuitively, further and remarkable damping gains are achieved by substantially lowering the GFM virtual inertia constant. These findings suggest that current industry trends may be constraining the potential benefits of Inverter Based Resources (IBRs). Optimal stability and performance are instead obtained with low droop and low virtual inertia, yielding tightly bounded frequency variations and strongly-damped electromechanical modes. The results indicate a need to re-evaluate GFM control design principles and emerging grid-code requirements.

</details>


### [89] [Off-grid solar energy storage system with lithium iron phosphate (LFP) batteries in high mountains: a case report of Tianchi Lodge in Taiwan](https://arxiv.org/abs/2512.02679)
*Hsien-Ching Chung*

Main category: eess.SY

TL;DR: 本研究通过对台湾天池山庄的离网太阳能储能系统进行案例分析，揭示其能源架构、运行历史及可持续性评估。


<details>
  <summary>Details</summary>
Motivation: 解决高海拔山舍能源供给问题，探讨在偏远地区实施可再生能源的可行性与挑战；以天池山庄为实证

Method: 基于系统架构描述、关键设备（光伏阵列、锂铁磷酸盐储能电池、逆变/控制系统）、容量、控制策略、运维记录等，梳理自2020年投入运行以来的历史状态与性能数据。

Result: 自2020年投入运行，系统实现离网供电，降低对化石燃料依赖；LFP电池组展现良好循环寿命与安全性；提供对能量自给能力、碳减排和运维成本的初步评估；报告中还讨论了系统稳定性、维护需求及潜在改进方向。

Conclusion: 在高海拔环境下，基于LFP的离网太阳能储能系统在山舍应用中具备可行性与可持续性，但需考虑气候、维护、备件供应等挑战；本案例有助于为山区 hut 供能设计提供经验教训。

Abstract: Mountain huts are buildings located at high altitude, providing shelter and a place for hikers. Energy supply on mountain huts remains an open issue. Using renewable energies could be an appropriate solution. Tianchi Lodge, a famous mountain hut in Taiwan, has operated an off-grid solar energy storage system with lithium iron phosphate (LFP) batteries since 2020. In this case report, the energy architecture, detailed descriptions, and historical status of the system are provided.

</details>


### [90] [Gain-Scheduling Data-Enabled Predictive Control for Nonlinear Systems with Linearized Operating Regions](https://arxiv.org/abs/2512.02797)
*Sebastian Zieglmeier,Mathias Hudoba de Badyn,Narada D. Warakagoda,Thomas R. Krogstad,Paal Engelstad*

Main category: eess.SY

TL;DR: 提出一种基于多局部线性数据表示的增益调度数据驱动预测控制（GS-DeePC），用区域Hankel矩阵替代全局矩阵，通过组合区域实现平滑切换并降低计算复杂度，在非线性DC电机实验中显著优于标准DeePC。


<details>
  <summary>Details</summary>
Motivation: 现有的DeePC依赖单一全局Hankel矩阵，难以覆盖非线性系统的广范围行为，且切换区域容易引发抖振并增加计算成本。通过对工作域进行区域化、采用局部数据表示并合并形成复合区域，可以提升建模灵活性、切换鲁棒性和实时性。

Method: 将工作范围按可测调节变量分割成多个局部区域，为每个区域构建局部可持续激励数据得到的Hankel矩阵；引入复合区域将相邻数据集合并以平滑区域切换，避免区域间抖振；保持DeePC原有问题结构，仅使用短且局部信息量的数据以降低计算负担。

Result: 在具有不平衡转子的一阶非线性直流电机等复杂非线性系统的实验中，所提出的GS-DeePC相较标准DeePC表现出显著的控制性能提升。

Conclusion: 通过区域化的局部线性数据表示与复合区域的切换策略，GS-DeePC在保持DeePC框架的同时实现更好的鲁棒性和更低的计算成本，适用于广域非线性系统的预测控制。

Abstract: This paper presents a Gain-Scheduled Data-Enabled Predictive Control (GS-DeePC) framework for nonlinear systems based on multiple locally linear data representations. Instead of relying on a single global Hankel matrix, the operating range of a measurable scheduling variable is partitioned into regions, and regional Hankel matrices are constructed from persistently exciting data. To ensure smooth transitions between linearization regions and suppress region-induced chattering, composite regions are introduced, merging neighboring data sets and enabling a robust switching mechanism. The proposed method maintains the original DeePC problem structure and can achieve reduced computational complexity by requiring only short, locally informative data sequences. Extensive experiments on a nonlinear DC-motor with an unbalanced disc demonstrate the significantly improved control performance compared to standard DeePC.

</details>


### [91] [Tempering the Bayes Filter towards Improved Model-Based Estimation](https://arxiv.org/abs/2512.02823)
*Menno van Zutphen,Domagoj Herceg,Giannis Delimpaltadakis,Duarte J. Antunes*

Main category: eess.SY

TL;DR: 提出了 tempered Bayes 过滤器，用于在模型不完备的情形下提高预测精度，结合似然 tempering、全后验 tempering 以及一种新模式，保持递归实现且计算复杂度不高于原 Bayes 滤波。


<details>
  <summary>Details</summary>
Motivation: 在学习部分可观测随机系统时，模型不完满带来估计误差与预测不确定性，如何在偏差-方差权衡及信息熵层面通过温和化来提升推断性能成为关键问题。

Method: 提出 tempered Bayes 滤波器，包含三种模态：似然温和化、全后验温和化以及一种新引入的模态，形成可递归实现的滤波算法。给出关于后验分布熵的分析，指出温和化对先验与似然的平衡影响，以及全后验温和化在最终信念分布中的熵控制作用。给出一个介于 Bayes 与 MAP 之间的温和化区间，并将其作为特例收敛到 Bayes、MAP。对线性高斯情形做专门化，得到 temper Kalman 滤波器，并解释参数对均值与协方差传播的影响。实证结果表明在预测对数损失（负对数似然）下普遍优于基线 Bayes 滤波器。

Result: 理论分析给出温和化滤波在改进预测性能方面的条件；线性高斯情况下解释参数对状态估计与协方差更新的影响；实验结果证实相较于 Bayes 滤波器基线的持续改进。

Conclusion: 温和化的 Bayes 滤波框架提供了一个在既保留递归实现又提升预测准确性的折中，且可在不同 tempering 下横跨 Bayes 与 MAP 的谱系，经线性高斯专门化可得到 tempered Kalman 滤波器。

Abstract: Model-based filtering is often carried out while subject to an imperfect model, as learning partially-observable stochastic systems remains a challenge. Recent work on Bayesian inference found that tempering the likelihood or full posterior of an imperfect model can improve predictive accuracy, as measured by expected negative log likelihood. In this paper, we develop the tempered Bayes filter, improving estimation performance through both of the aforementioned, and one newly introduced, modalities. The result admits a recursive implementation with a computational complexity no higher than that of the original Bayes filter. Our analysis reveals that -- besides the well-known fact in the field of Bayesian inference that likelihood tempering affects the balance between prior and likelihood -- full-posterior tempering tunes the level of entropy in the final belief distribution. We further find that a region of the tempering space can be understood as interpolating between the Bayes- and MAP filters, recovering these as special cases. Analytical results further establish conditions under which a tempered Bayes filter achieves improved predictive performance. Specializing the results to the linear Gaussian case, we obtain the tempered Kalman filter. In this context, we interpret how the parameters affect the Kalman state estimate and covariance propagation. Empirical results confirm that our method consistently improves predictive accuracy over the Bayes filter baseline.

</details>


### [92] [PAC-Bayesian Optimal Control with Stability and Generalization Guarantees](https://arxiv.org/abs/2512.02858)
*Mahrokh Ghoddousi Boroujeni,Clara Lucía Galimberti,Andreas Krause,Giancarlo Ferrari-Trecate*

Main category: eess.SY

TL;DR: 引入基于PAC-Bayes的泛化框架来为随机非线性最优控制（SNOC）提供严格的泛化界限，并在此基础上提出一个以先验知识为约束、兼顾经验性能与不确定性鲁棒性的控制器设计方法，辅以可计算的松弛和近似推断，以及神经控制器的闭环稳定性保证。通过仿真展示在小样本情形下的鲁棒性提升，尤其在合作机器人中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 在SNOC中，由于对随机扰动的期望通常不可直接计算，通常用有限扰动样本的经验代价来近似。但有限数据易导致过拟合，从而在未见扰动下的部署代价与经验代价之间出现显著差距。需要一个可以对未见扰动提供严格泛化保证的框架。

Method: 建立基于PAC-Bayes的SNOC泛化界限；在此基础上提出控制器设计目标，兼顾经验代价与先验信息的权衡；推导计算友好的界限松弛和近似推断方法；采用表达性强的神经控制器参数化，确保闭环稳定性。

Result: 给出SNOC的PAC-Bayes泛化界限及其可操作的设计框架，提供界限的松弛与近似推断以实现可计算性；在仿真实验中验证框架的有效性，展示如何把先验知识融入设计并提高在合作机器人等场景中的鲁棒性。

Conclusion: 该PAC-Bayes框架为SNOC提供了严格的未见扰动情形下的性能保证，且通过先验信息和神经控制器的结合实现更可靠的控制器合成，同时保持计算可行性与闭环稳定性。

Abstract: Stochastic Nonlinear Optimal Control (SNOC) seeks to minimize a cost function that accounts for random disturbances acting on a nonlinear dynamical system. Since the expectation over all disturbances is generally intractable, a common surrogate is the empirical cost, obtained by averaging over a finite dataset of sampled noise realizations. This substitution, however, introduces the challenge of guaranteeing performance under unseen disturbances. The issue is particularly severe when the dataset is limited, as the trained controllers may overfit, leading to substantial gaps between their empirical cost and the deployment cost. In this work, we develop a PAC-Bayesian framework that establishes rigorous generalization bounds for SNOC. Building on these bounds, we propose a principled controller design method that balances empirical performance and prior knowledge. To ensure tractability, we derive computationally efficient relaxations of the bounds and employ approximate inference methods. Our framework further leverages expressive neural controller parameterizations, guaranteeing closed-loop stability. Through simulated examples, we highlight how prior knowledge can be incorporated into control design and how more reliable controllers can be synthesized for cooperative robotics.

</details>


### [93] [AC/DC Frequency-Dependent Power Flow Jacobian: Quantifying Grid Support and Stability Implications](https://arxiv.org/abs/2512.02872)
*Dongyeong Lee,Eros Avdiaj,Jef Beerten*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This letter proposes an AC/DC frequency-dependent power flow Jacobian analysis to identify the system support capabilities. In addition, the analyses reveal that system support capabilities do not necessarily enhance the system stability margin, suggesting that technical requirements of narrow-frequency-band and AC-side focused specifications may not lead to the expected performance of GFM.

</details>


### [94] [Statistical-Symbolic Verification of Perception-Based Autonomous Systems using State-Dependent Conformal Prediction](https://arxiv.org/abs/2512.02893)
*Yuang Geng,Thomas Waite,Trevor Turnquist,Radoslav Ivanov,Ivan Ruchkin*

Main category: eess.SY

TL;DR: 提出状态自适应的 conformal prediction、基于遗传算法的状态分区，以及分支合并可达性分析，以降低感知误差带来的保守性并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 神经感知组件导致的不可控误差使直接的可达性分析不可行；现有的时间序列 conformal prediction 往往过于保守，难以与符号可达性结合且扩展性差。需要更紧凑、可证明、可扩展的误差界来支撑安全验证。

Method: 1) 引入状态依赖的 conformal prediction，利用系统状态的变化来构造更紧的感知误差界；2) 通过遗传算法对状态空间进行分区以优化 CP 边界的紧度；3) 将误差边界用于可达性分析，提出分支合并算法，在保持精度的同时降低分支数量，实现可扩展的高置信度验证。

Result: 在两个互补的案例研究中，与现有方法相比实现了更低的保守性和更紧的界限，并提升了可扩展性。

Conclusion: 状态依赖性是降低可达性分析保守性的关键；基于状态分区的 CP 与分支合并的可达性框架可提供可证明、可扩展的安全验证。

Abstract: Reachability analysis has been a prominent way to provide safety guarantees for neurally controlled autonomous systems, but its direct application to neural perception components is infeasible due to imperfect or intractable perception models. Typically, this issue has been bypassed by complementing reachability with statistical analysis of perception error, say with conformal prediction (CP). However, existing CP methods for time-series data often provide conservative bounds. The corresponding error accumulation over time has made it challenging to combine statistical bounds with symbolic reachability in a way that is provable, scalable, and minimally conservative. To reduce conservatism and improve scalability, our key insight is that perception error varies significantly with the system's dynamical state. This article proposes state-dependent conformal prediction, which exploits that dependency in constructing tight high-confidence bounds on perception error. Based on this idea, we provide an approach to partition the state space, using a genetic algorithm, so as to optimize the tightness of conformal bounds. Finally, since using these bounds in reachability analysis leads to additional uncertainty and branching in the resulting hybrid system, we propose a branch-merging reachability algorithm that trades off uncertainty for scalability so as to enable scalable and tight verification. The evaluation of our verification methodology on two complementary case studies demonstrates reduced conservatism compared to the state of the art.

</details>


### [95] [GNSS Array-Based Multipath Detection Employing UKF on Manifolds](https://arxiv.org/abs/2512.02994)
*Abdelgabar Ahmed,Tarig Ballal,Xing Liu,Mohanad Ahmed,Tareq Y. Al-Naffouri*

Main category: eess.SY

TL;DR: 提出一种在流形上的UKF GNSS/IMU 融合方法，通过姿态信息筛选多径卫星并使用RANSAC降低卫星组合数量，在KITTI数据集上验证，显著提升在强多径场景中的定位与姿态精度。


<details>
  <summary>Details</summary>
Motivation: 在城市环境中多径干扰是GNSS 精度的主要瓶颈之一，需要通过检测并剔除多径卫星来提升定位与姿态推算的鲁棒性，且需在实时动态场景中保持稳定估计。

Method: 在流形上对GNSS 与IMU 进行UKF 融合以实现持续姿态跟踪；利用卫星组合信息来识别被多径污染的卫星并剔除；为降低组合数量带来的计算挑战，采用RANSAC 以选取具有较好鲁棒性的卫星组合进行检测；观测数据在KITTI 场景下基于真实位置信息和星历进行GNSS 模拟。

Result: 方法能有效识别受多径干扰的卫星，在大部分可见卫星被强多径污染的场景中实现定位精度的显著提升，并提升姿态估计的稳定性与精度。

Conclusion: 通过将姿态信息与GNSS/IMU 融合在流形上的UKF 以及RANSAC 的多卫星组合筛选，提供了一种高效鲁棒的高精度定位与姿态估计方案，尤其在多径污染严重的场景表现突出。

Abstract: Global Navigation Satellite Systems (GNSS) applications are often hindered by various sources of error, with multipath interference being one of the most challenging, particularly in urban environments. In this work, we build on previous research by implementing a GNSS array-based multipath detection algorithm, incorporating real-time attitude estimation for dynamic scenarios. The method fuses GNSS and IMU data using an Unscented Kalman Filter (UKF) on a manifold, enabling continuous attitude tracking. The proposed approach utilizes attitude information from satellite combinations to identify and exclude multipath-affected satellites, improving the accuracy of both positioning and attitude determination. To address computational challenges associated with evaluating large numbers of satellite combinations, we propose the use of the Random Sample Consensus (RANSAC) algorithm, which reduces the number of combinations assessed while maintaining high detection performance. Performance evaluations are conducted using trajectories and IMU readings from the KITTI dataset. GNSS observations are simulated based on ground truth positions and satellite ephemeris. The results demonstrate the effectiveness of the proposed approach in detecting satellites affected by multipath interference. Significant improvements in positioning accuracy are observed, particularly in scenarios where a large portion of the visible satellites are contaminated by severe multipath.

</details>
