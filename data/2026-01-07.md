<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 7]
- [cs.LG](#cs.LG) [Total: 43]
- [eess.SP](#eess.SP) [Total: 6]
- [eess.SY](#eess.SY) [Total: 10]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.CR](#cs.CR) [Total: 13]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Breaking Rank - A Novel Unscented Kalman Filter for Parameter Estimations of a Lumped-Parameter Cardiovascular Model](https://arxiv.org/abs/2601.02390)
*Alex Thornton,Ian Halliday,Harry Saxton,Xu Xu*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We make modifications to the unscented Kalman filter (UKF) which bestow almost complete practical identifiability upon a lumped-parameter cardiovascular model with 10 parameters and 4 output observables - a highly non-linear, stiff problem of clinical significance. The modifications overcome the challenging problems of rank deficiency when applying the UKF to parameter estimation. Rank deficiency usually means only a small subset of parameters can be estimated. Traditionally, pragmatic compromises are made, such as selecting an optimal subset of parameters for estimation and fixing non-influential parameters. Kalman filters are typically used for dynamical state tracking, to facilitate the control u at every time step. However, for the purpose of parameter estimation, this constraint no longer applies. Our modification has transformed the utility of UKF for the parameter estimation purpose, including minimally influential parameters, with excellent robustness (i.e., under severe noise corruption, challenging patho-physiology, and no prior knowledge of parameter distributions). The modified UKF algorithm is robust in recovering almost all parameters to over 98% accuracy, over 90% of the time, with a challenging target data set of 50, 10-parameter samples. We compare this to the original implementation of the UKF algorithm for parameter estimation and demonstrate a significant improvement.

</details>


### [2] [Weights on finite fields and failures of the MacWilliams identities](https://arxiv.org/abs/2601.02608)
*Jay A. Wood*

Main category: cs.IT

TL;DR: 除了 Hamming 权重外，许多权重对于线性码的对偶性不满足 “同权重列举 → 双码同权重列举”。


<details>
  <summary>Details</summary>
Motivation: 探究不同权重定义对线性码对偶结构的影响，检验 MacWilliams 对偶性是否普遍适用于所有权重，或仅限于 Hamming 权重。

Method: 在 1960 年代 MacWilliams 的经典结果基础上，作者研究了广泛的有限域权重定义，分析各自权重枚举函数的对偶性关系，并构造具体线性码实例，以说明双码权重枚举的差异。

Result: 证明存在大量权重系统，其中相同权重列举的线性码其对偶码的权重列举不相同，并给出了示例与理论依据。

Conclusion: 这篇论文指出，对某些特定加权方案（除汉明权重外），即便两个线性码具有相同的权重列举，它们的双码的权重列举也可能不同，从而与 MacWilliams 定理所预示的 Hamming 权重性质形成对比。

Abstract: In the 1960s, MacWilliams proved that the Hamming weight enumerator of a linear code over a finite field completely determines, and is determined by, the Hamming weight enumerator of its dual code. In particular, if two linear codes have the same Hamming weight enumerator, then their dual codes have the same Hamming weight enumerator.
  In contrast, there is a wide class of weights on finite fields whose weight enumerators have the opposite behavior: there exist two linear codes having the same weight enumerator, but their dual codes have different weight enumerators.

</details>


### [3] [State-Dependent Fading Gaussian Channel with Common Reconstruction Constraints](https://arxiv.org/abs/2601.02802)
*Viswanathan Ramachandran*

Main category: cs.IT

TL;DR: 在已知状态干扰的衰落高斯通信模型中，本文得到完整的最优率失真区域，并通过数值示例验证了结果。


<details>
  <summary>Details</summary>
Motivation: 探究在既要解码消息又要共同重构通道状态的无线通信场景，尤其是带有高斯状态干扰的衰落模型中，双方如何协同操作以达到最优性能。

Method: 通过信息论分析，构造最优率失真平衡区域，并利用数值优化验证其可行性。

Result: 给出了该衰落高斯模型下完整的最优率失真区域；数值实验进一步验证了率失真与功率失真之间的折衷。

Conclusion: 本文实现了在已知状态的发射端与冲击已知的接收端共同解码消息与状态重构的最优理论极限，为类似通信系统的设计提供了基准。

Abstract: The task of jointly communicating a message and reconstructing a common estimate of the channel state is examined for a fading Gaussian model with additive state interference. The state is an independent and identically distributed Gaussian sequence known noncausally at the transmitter, and the instantaneous fading coefficient is perfectly known at both the transmitter and the receiver. The receiver is required to decode the transmitted message and, in addition, reconstruct the state under a common reconstruction constraint ensuring that its estimate coincides with that at the transmitter. A complete characterization of the optimal rate distortion tradeoff region for this setting is the main result of our work. The analytical results are also validated through numerical examples illustrating the rate distortion and power distortion tradeoffs.

</details>


### [4] [Context-aware Privacy Bounds for Linear Queries](https://arxiv.org/abs/2601.02855)
*Heng Zhao,Sara Saeidian,Tobias J. Oechtering*

Main category: cs.IT

TL;DR: 对Laplace机制的PML分析表明，引入记录类别分布假设能显著降低噪声量，提供更紧的隐私保护界限，优于传统DP。


<details>
  <summary>Details</summary>
Motivation: 差分隐私在无上下文条件下对噪声需求过大，缺少对真实数据生成分布的利用。

Method: 提出将线性查询的Laplace机制通过点逐点最大泄露度（PML）重新分析，假设任一记录属于特定类别的先验概率有下界，随后推导出与先验相关的泄露界限，并与标准DP界限做严格比较。

Result: 通过数值评估验证，利用先验知识可将所需噪声尺度显著降低，同时仍满足隐私保证。

Conclusion: 该研究证明，在引入先验分布假设后，基于点逐点最大泄露度衡量的线性查询的隐私保护可实现更紧的噪声尺度上界，明显优于传统的差分隐私标准，且当先验概率下界趋近于零时退化为差分隐私的界限。

Abstract: Linear queries, as the basis of broad analysis tasks, are often released through privacy mechanisms based on differential privacy (DP), the most popular framework for privacy protection. However, DP adopts a context-free definition that operates independently of the data-generating distribution. In this paper, we revisit the privacy analysis of the Laplace mechanism through the lens of pointwise maximal leakage (PML). We demonstrate that the distribution-agnostic definition of the DP framework often mandates excessive noise. To address this, we incorporate an assumption about the prior distribution by lower-bounding the probability of any single record belonging to any specific class. With this assumption, we derive a tight, context-aware leakage bound for general linear queries, and prove that our derived bound is strictly tighter than the standard DP guarantee and converges to the DP guarantee as this probability lower bound approaches zero. Numerical evaluations demonstrate that by exploiting this prior knowledge, the required noise scale can be reduced while maintaining privacy guarantees.

</details>


### [5] [Dualities for finite abelian groups and applications to coding theory](https://arxiv.org/abs/2601.03126)
*Jay A. Wood*

Main category: cs.IT

TL;DR: 本文通过选取适当的自同构，定义了有限阿贝尔群上加法码的对偶码，继续并扩展了Delsarte与Dougherty等人的研究，系统分析了自同构与对偶码的性质。


<details>
  <summary>Details</summary>
Motivation: 在有限阿贝尔群上构造加法码的对偶码并研究其性质

Method: 通过选取有限阿贝尔群A与其角色群之间的等价自同构，定义加法码的对偶码，继续Delsarte（1973）以及Dougherty等人的研究方法

Result: 对等价自同构与对偶码性质进行系统的分析与研究

Conclusion: 成功建立并阐述了有限阿贝尔群与其角色群之间的对偶关系，并对加法码的对偶性质进行了深入探讨

Abstract: The choice of an isomorphism, a duality, between a finite abelian group $A$ and its character group allows one to define dual codes of additive codes over $A$. Properties of dualities and dual codes are studied, continuing work of Delsarte from 1973 and more recent work of Dougherty and his collaborators.

</details>


### [6] [On the Euclidean duals of the cyclic codes generated by cyclotomic polynomials](https://arxiv.org/abs/2601.03165)
*Anuj Kumar Bhagat,Ritumoni Sarma*

Main category: cs.IT

TL;DR: 本文证明了先前猜想的结论：欧氏双码的最小距离等于$n$中不同质因数个数的二次幂。


<details>
  <summary>Details</summary>
Motivation: 验证此前提出的$2^{\omega(n)}$公式的猜想，为循环码双码理论提供新的距离估计

Method: 利用$n$次循环多项式生成的循环码的结构性质与欧氏内积特征，推导距离表达式

Result: 证明对于任意正整数$n$且与$q$互素，$\mathcal{C}_{n}^{\perp}$的最小距离确实为$2^{\omega(n)}$

Conclusion: 确定欧氏双码$\mathcal{C}_{n}^{\perp}$的最小距离为$2^{\omega(n)}$

Abstract: In this article, we determine the minimum distance of the Euclidean dual of the cyclic code $\mathcal{C}_n$ generated by the $n$th cyclotomic polynomial $Q_n(x)$ over $\mathbb{F}_q$, for every positive integer $n$ co-prime to $q$. In particular, we prove that the minimum distance of $\mathcal{C}_{n}^{\perp}$ is a function of $n$, namely $2^{ω(n)}$. This was precisely the conjecture posed by us in \cite{BHAGAT2025}.

</details>


### [7] [On the Capacity Region of Individual Key Rates in Vector Linear Secure Aggregation](https://arxiv.org/abs/2601.03241)
*Lei Hu,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文利用秩递增条件，构造了新的率区域，表明并非所有用户需要键，降低了综合键长度需求，并在最小持钥用户数上实现了最优


<details>
  <summary>Details</summary>
Motivation: 研究向量线性安全聚合问题中各用户键长度的最小需求

Method: 构造新的可实现率区域，使用二进制率分配与秩递增条件

Result: 得到多面体率区域，证明并非所有用户必需持钥，且在最小持钥用户数量上实现最优

Conclusion: 为Yuan‑Sun提出的开放问题给出最小键长度的可行解，并证明其在最小持钥用户数上的最优性

Abstract: We provide new insights into an open problem recently posed by Yuan-Sun [ISIT 2025], concerning the minimum individual key rate required in the vector linear secure aggregation problem. Consider a distributed system with $K$ users, where each user $k\in [K]$ holds a data stream $W_k$ and an individual key $Z_k$. A server aims to compute a linear function $\mathbf{F}[W_1;\ldots;W_K]$ without learning any information about another linear function $\mathbf{G}[W_1;\ldots;W_K]$, where $[W_1;\ldots;W_K]$ denotes the row stack of $W_1,\ldots,W_K$. The open problem is to determine the minimum required length of $Z_k$, denoted as $R_k$, $k\in [K]$. In this paper, we characterize a new achievable region for the rate tuple $(R_1,\ldots,R_K)$. The region is polyhedral, with vertices characterized by a binary rate assignment $(R_1,\ldots,R_K) = (\mathbf{1}(1 \in \mathcal{I}),\ldots,\mathbf{1}(K\in \mathcal{I}))$, where $\mathcal{I}\subseteq [K]$ satisfies the \textit{rank-increment condition}: $\mathrm{rank}\left(\bigl[\mathbf{F}_{\mathcal{I}};\mathbf{G}_{\mathcal{I}}\bigr]\right) =\mathrm{rank}\bigl(\mathbf{F}_{\mathcal{I}}\bigr)+N$. Here, $\mathbf{F}_\mathcal{I}$ and $\mathbf{G}_\mathcal{I}$ are the submatrices formed by the columns indexed by $\mathcal{I}$. Our results uncover the novel fact that it is not necessary for every user to hold a key, thereby strictly enlarging the best-known achievable region in the literature. Furthermore, we provide a converse analysis to demonstrate its optimality when minimizing the number of users that hold keys.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Physical Transformer](https://arxiv.org/abs/2601.02433)
*Tao Xu,Zhixin Hu,Li Luo,Momiao Xiong*

Main category: cs.LG

TL;DR: 本文构建了一种将传统Transformer与物理哈密顿动力学相结合的“物理变压器”，在多个层面引入物理约束，并在实验中表现出更优稳定性和准确性，暗示数字与物理推理的融合潜力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模AI在虚拟空间取得进展，却缺乏物理解释和与现实世界的交互；需要将数字推理与物理动力学统一，以提升可解释性和实际适用性。

Method: 在微观层面将注意力头和前馈块视为相互作用自旋，用有效哈密顿量与非哈密顿浴耦合；在中观层面建立学习的神经微分流形（NDM），在哈密顿流、希尔伯特-雅可比-贝尔曼最优控制下演化；在宏观层面维护生成语义工作空间与二维信息相位图；通过在流形上控制信息流来解决推理任务，并使用辛层保持几何及能量不变。

Result: 在数值积分与动力学系统的玩具实验中，物理变压器在稳定性与长时域准确性上优于传统基准，证明遵循几何与哈密顿结构的益处。

Conclusion: 提出一种“物理变压器”，通过在微观、中观和宏观层面引入哈密顿动力学和几何表示，将数字AI与物理世界对接；在物理约束下实现更稳定、长期准确的推理；开辟统一数字与物理推理的方向。

Abstract: Digital AI systems spanning large language models, vision models, and generative architectures that operate primarily in symbolic, linguistic, or pixel domains. They have achieved striking progress, but almost all of this progress lives in virtual spaces. These systems transform embeddings and tokens, yet do not themselves touch the world and rarely admit a physical interpretation. In this work we propose a physical transformer that couples modern transformer style computation with geometric representation and physical dynamics. At the micro level, attention heads, and feed-forward blocks are modeled as interacting spins governed by effective Hamiltonians plus non-Hamiltonian bath terms. At the meso level, their aggregated state evolves on a learned Neural Differential Manifold (NDM) under Hamiltonian flows and Hamilton, Jacobi, Bellman (HJB) optimal control, discretized by symplectic layers that approximately preserve geometric and energetic invariants. At the macro level, the model maintains a generative semantic workspace and a two-dimensional information-phase portrait that tracks uncertainty and information gain over a reasoning trajectory. Within this hierarchy, reasoning tasks are formulated as controlled information flows on the manifold, with solutions corresponding to low cost trajectories that satisfy geometric, energetic, and workspace-consistency constraints. On simple toy problems involving numerical integration and dynamical systems, the physical transformer outperforms naive baselines in stability and long-horizon accuracy, highlighting the benefits of respecting underlying geometric and Hamiltonian structure. More broadly, the framework suggests a path toward physical AI that unify digital reasoning with physically grounded manifolds, opening a route to more interpretable and potentially unified models of reasoning, control, and interaction with the real world.

</details>


### [9] [mHC-GNN: Manifold-Constrained Hyper-Connections for Graph Neural Networks](https://arxiv.org/abs/2601.02451)
*Subhankar Mishra*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Neural Networks (GNNs) suffer from over-smoothing in deep architectures and expressiveness bounded by the 1-Weisfeiler-Leman (1-WL) test. We adapt Manifold-Constrained Hyper-Connections (\mhc)~\citep{xie2025mhc}, recently proposed for Transformers, to graph neural networks. Our method, mHC-GNN, expands node representations across $n$ parallel streams and constrains stream-mixing matrices to the Birkhoff polytope via Sinkhorn-Knopp normalization. We prove that mHC-GNN exhibits exponentially slower over-smoothing (rate $(1-γ)^{L/n}$ vs.\ $(1-γ)^L$) and can distinguish graphs beyond 1-WL. Experiments on 10 datasets with 4 GNN architectures show consistent improvements. Depth experiments from 2 to 128 layers reveal that standard GNNs collapse to near-random performance beyond 16 layers, while mHC-GNN maintains over 74\% accuracy even at 128 layers, with improvements exceeding 50 percentage points at extreme depths. Ablations confirm that the manifold constraint is essential: removing it causes up to 82\% performance degradation. Code is available at \href{https://github.com/smlab-niser/mhc-gnn}{https://github.com/smlab-niser/mhc-gnn}

</details>


### [10] [Polynomial Convergence of Riemannian Diffusion Models](https://arxiv.org/abs/2601.02499)
*Xingyu Xu,Ziyi Zhang,Yorie Nakahira,Guannan Qu,Yuejie Chi*

Main category: cs.LG

TL;DR: 本文改进 Riemannian 扩散模型理论：只要分数估计 $L_{2}$ 精度，并在轻度曲率下，足以用多项式阶步长保证总变差误差小，无需光滑或正性假设，方法基于热核与参数展开。


<details>
  <summary>Details</summary>
Motivation: 已有文献多在欧氏空间研究扩散模型，且需使用指数级小步长、光滑正分布等严格假设。实际应用中数据往往位于欧氏子流形，且分布可能非光滑或非正。为降低计算成本并适应更广泛场景，需要在更弱假设下证明扩散模型的采样误差可控。

Method: 本研究基于 Li‑Yau 对热核对数梯度的估计以及 Minakshisundaram‑Pleijel 的参数化展开，对 Riemannian 空间中的受扰热方程进行系统分析，并结合 $L_{2}$-精度的分数估计，利用热行进技术推导采样误差上界。

Result: 证明：在满足 $L_{2}$-分数估计、仅有轻度曲率约束的 Riemannian 流形上，采用多项式阶步长即可保证采样误差（总变差距离）小；该结果不需要数据分布光滑或正性。理论框架涉及 Li‑Yau 成分和 Minakshisundaram‑Pleijel 参数化展开。

Conclusion: 在 Riemannian 扩散模型中，使用 $L_2$-精确的分数估计，仅需多项式阶的步长即可在总变差距离下得到小的采样误差，无需对数据分布的光滑性或正性做假设；该结论在仅具备轻度曲率假设的基础上成立。

Abstract: Diffusion models have demonstrated remarkable empirical success in the recent years and are considered one of the state-of-the-art generative models in modern AI. These models consist of a forward process, which gradually diffuses the data distribution to a noise distribution spanning the whole space, and a backward process, which inverts this transformation to recover the data distribution from noise. Most of the existing literature assumes that the underlying space is Euclidean. However, in many practical applications, the data are constrained to lie on a submanifold of Euclidean space. Addressing this setting, De Bortoli et al. (2022) introduced Riemannian diffusion models and proved that using an exponentially small step size yields a small sampling error in the Wasserstein distance, provided the data distribution is smooth and strictly positive, and the score estimate is $L_\infty$-accurate. In this paper, we greatly strengthen this theory by establishing that, under $L_2$-accurate score estimate, a {\em polynomially small stepsize} suffices to guarantee small sampling error in the total variation distance, without requiring smoothness or positivity of the data distribution. Our analysis only requires mild and standard curvature assumptions on the underlying manifold. The main ingredients in our analysis are Li-Yau estimate for the log-gradient of heat kernel, and Minakshisundaram-Pleijel parametrix expansion of the perturbed heat equation. Our approach opens the door to a sharper analysis of diffusion models on non-Euclidean spaces.

</details>


### [11] [GEM-Style Constraints for PEFT with Dual Gradient Projection in LoRA](https://arxiv.org/abs/2601.02500)
*Brian Tekmen,Jason Yin,Qianqian Tong*

Main category: cs.LG

TL;DR: I‑GEM 在 LoRA 子空间中近似 GEM，仅用适配器参数约束，极大降低投影时间并维持性能，是 LLM 规模持续学习的实用方案。


<details>
  <summary>Details</summary>
Motivation: LLM完全微调成本高，需采用参数高效适配器的持续学习方法。

Method: 在 LoRA 子空间中重现 GEM，引入 I‑GEM 双投影梯度近似；仅在适配器参数内约束无干扰。

Result: 在 GPT‑2(355M)/LoRA r=8 上 3 任务 AG News 域漂移实验，I‑GEM 与 GEM 准确率相近，优于 A‑GEM，且投影时间下降约 10⁽³⁾ 倍。

Conclusion: GEM 约束可在 LoRA 子空间实现，从而在 LLM 规模下实现可行的持续学习。

Abstract: Full fine-tuning of Large Language Models (LLMs) is computationally costly, motivating Continual Learning (CL) approaches that utilize parameter-efficient adapters. We revisit Gradient Episodic Memory (GEM) within the Low-Rank Adapter (LoRA) subspace and introduce I-GEM: a fixed-budget, GPU-resident dual projected-gradient approximation to GEM's quadratic projection. By constraining non-interference solely within the adapter parameters, I-GEM preserves GEM-like stability with orders-of-magnitude lower mean projection overhead. On a 3-task AG News split with induced domain drift, using GPT-2 (355M) and LoRA ($r=8$), I-GEM matches GEM's average accuracy (within $\sim\!0.04$ pts) and outperforms A-GEM by $\sim\!1.4$ pts. Crucially, it reduces projection time vs.\ GEM by a factor of $\sim\!10^3$. These results suggest that applying GEM constraints in the LoRA subspace is a practical pathway for continual learning at the LLM scale.

</details>


### [12] [hdlib 2.0: Extending Machine Learning Capabilities of Vector-Symbolic Architectures](https://arxiv.org/abs/2601.02509)
*Fabio Cumbo,Kabir Dhillon,Daniel Blankenberg*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Following the initial publication of hdlib, a Python library for designing Vector-Symbolic Architectures (VSA), we introduce a major extension that significantly enhances its machine learning capabilities. VSA, also known as Hyperdimensional Computing, is a computing paradigm that represents and processes information using high-dimensional vectors. While the first version of hdlib established a robust foundation for creating and manipulating these vectors, this update addresses the growing need for more advanced, data-driven modeling within the VSA framework. Here, we present four extensions: significant enhancements to the existing supervised classification model also enabling feature selection, and a new regression model for predicting continuous variables, a clustering model for unsupervised learning, and a graph-based learning model. Furthermore, we propose the first implementation ever of Quantum Hyperdimensional Computing with quantum-powered arithmetic operations and a new Quantum Machine Learning model for supervised learning. hdlib remains open-source and available on GitHub at https://github.com/cumbof/hdlib under the MIT license, and distributed through the Python Package Index (pip install hdlib) and Conda (conda install -c conda-forge hdlib). Documentation and examples of these new features are available on the official Wiki at https://github.com/cumbof/hdlib/wiki.

</details>


### [13] [CutisAI: Deep Learning Framework for Automated Dermatology and Cancer Screening](https://arxiv.org/abs/2601.02562)
*Rohit Kaushik,Eva Kaushik*

Main category: cs.LG

TL;DR: CBDC将理论与实践相结合，提供在皮肤诊断中可置信、可解释的深度学习模型，实验验证其准确性和不确定性校准效果。


<details>
  <summary>Details</summary>
Motivation: 随着皮肤影像与移动诊断工具的快速发展，单靠准确率已不足以进入临床，需要得到可靠且可校准的不确定性评估以保障安全可部署。

Method: 融合统计学习理论、拓扑数据分析（TDA）和贝叶斯合约推断，构建CBDC框架；对CNN嵌入层证明拓扑稳定性，并应用有限聚合式内证方法给出不确定性覆盖界。

Result: 在HAM10000、PH2及ISIC 2020数据集上，CBDC不仅实现了高分类准确率，同时输出与临床一致、可解释的校准预测，验证了理论与实践的兼容性。

Conclusion: CBDC提供了严谨的理论保障（可泛化界、拓扑稳定定理和有限内证覆盖保证），实现了可靠、可校准的不确定性估计，为临床部署的深度皮肤学诊断奠定了基础。

Abstract: The rapid growth of dermatological imaging and mobile diagnostic tools calls for systems that not only demonstrate empirical performance but also provide strong theoretical guarantees. Deep learning models have shown high predictive accuracy; however, they are often criticized for lacking well, calibrated uncertainty estimates without which these models are hardly deployable in a clinical setting. To this end, we present the Conformal Bayesian Dermatological Classifier (CBDC), a well, founded framework that combines Statistical Learning Theory, Topological Data Analysis (TDA), and Bayesian Conformal Inference. CBDC offers distribution, dependent generalization bounds that reflect dermatological variability, proves a topological stability theorem that guarantees the invariance of convolutional neural network embeddings under photometric and morphological perturbations and provides finite conformal coverage guarantees for trustworthy uncertainty quantification.
  Through exhaustive experiments on the HAM10000, PH2, and ISIC 2020 datasets, we show that CBDC not only attains classification accuracy but also generates calibrated predictions that are interpretable from a clinical perspective. This research constitutes a theoretical and practical leap for deep dermatological diagnostics, thereby opening the machine learning theory clinical applicability interface.

</details>


### [14] [LendNova: Towards Automated Credit Risk Assessment with Language Models](https://arxiv.org/abs/2601.02573)
*Kiarash Shamsi,Danijel Novokmet,Joshua Peters,Mao Lin Liu,Paul K Edwards,Vahab Khoshdel*

Main category: cs.LG

TL;DR: LendNova利用语言模型直接从原始信用文本中学习风险表示，形成自动化、成本低、可扩展的信用风险评估管道，在真实数据上展示出良好的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统信用风险模型成本高且无法充分利用包含大量行话的原始文本信息，需要一种更低成本、更高可扩展性的自动化评估方法。

Method: 采用先进的NLP技术和语言模型对信用局原始文本进行特征学习和风险信号提取，自动化替代传统手工预处理和特征工程，构建完整的风险评估流水线。

Result: 在真实金融数据上的实验表明，LendNova实现了与传统模型相当或更优的准确率，显著减少了预处理步骤，验证了语言模型在信用风险领域的可行性。

Conclusion: LendNova通过直接处理原始信用文件文本，利用语言模型实现了端到端的信用风险评估，降低了人工特征工程成本，提升了可扩展性，并在真实数据上表现出良好的准确性和效率。

Abstract: Credit risk assessment is essential in the financial sector, but has traditionally depended on costly feature-based models that often fail to utilize all available information in raw credit records. This paper introduces LendNova, the first practical automated end-to-end pipeline for credit risk assessment, designed to utilize all available information in raw credit records by leveraging advanced NLP techniques and language models. LendNova transforms risk modeling by operating directly on raw, jargon-heavy credit bureau text using a language model that learns task-relevant representations without manual feature engineering. By automatically capturing patterns and risk signals embedded in the text, it replaces manual preprocessing steps, reducing costs and improving scalability. Evaluation on real-world data further demonstrates its strong potential in accurate and efficient risk assessment. LendNova establishes a baseline for intelligent credit risk agents, demonstrating the feasibility of language models in this domain. It lays the groundwork for future research toward foundation systems that enable more accurate, adaptable, and automated financial decision-making.

</details>


### [15] [Threat Detection in Social Media Networks Using Machine Learning Based Network Analysis](https://arxiv.org/abs/2601.02581)
*Aditi Sanjay Agrawal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The accelerated development of social media websites has posed intricate security issues in cyberspace, where these sites have increasingly become victims of criminal activities including attempts to intrude into them, abnormal traffic patterns, and organized attacks. The conventional rule-based security systems are not always scalable and dynamic to meet such a threat. This paper introduces a threat detection framework based on machine learning that can be used to classify malicious behavior in the social media network environment based on the nature of network traffic. Exploiting a rich network traffic dataset, the massive preprocessing and exploratory data analysis is conducted to overcome the problem of data imbalance, feature inconsistency, and noise. A model of artificial neural network (ANN) is then created to acquire intricate, non-linear tendencies of malicious actions. The proposed model is tested on conventional performance metrics, such as accuracy, accuracy, recall, F1-score, and ROC-AUC, and shows good detection and high levels of strength. The findings suggest that neural network-based solutions have the potential to be used effectively to identify the latent threat dynamics within the context of a large-scale social media network and that they can be employed to complement the existing intrusion detection system and better to conduct proactive cybersecurity operations.

</details>


### [16] [Chronicals: A High-Performance Framework for LLM Fine-Tuning with 3.51x Speedup over Unsloth](https://arxiv.org/abs/2601.02609)
*Arjun S. Nair*

Main category: cs.LG

TL;DR: 通过四种优化，Chronicals将显存需求降至A100-40GB可用范围，并在同类框架上实现3.5-4.1倍速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有大模型微调受显存限制，速度低下，需要更高效的训练框架。

Method: 融合Triton内核、Cut Cross-Entropy、LoRA+差异学习率以及序列打包等四项技术。

Result: 对Qwen2.5-0.5B模型，Chronicals在A100-40GB显存上实现41,184 tokens/second，比Unsloth快3.51倍；LoRA Rank32时为11,699 tokens/second，速率提升4.10倍。

Conclusion: 该研究展示了Chronicals框架在大语言模型微调中的显著性能提升，显著降低显存占用并提升吞吐量。

Abstract: Large language model fine-tuning is bottlenecked by memory: a 7B parameter model requires 84GB--14GB for weights, 14GB for gradients, and 56GB for FP32 optimizer states--exceeding even A100-40GB capacity. We present Chronicals, an open-source training framework achieving 3.51x speedup over Unsloth through four synergistic optimizations: (1) fused Triton kernels eliminating 75% of memory traffic via RMSNorm (7x), SwiGLU (5x), and QK-RoPE (2.3x) fusion; (2) Cut Cross-Entropy reducing logit memory from 5GB to 135MB through online softmax computation; (3) LoRA+ with theoretically-derived 16x differential learning rates between adapter matrices; and (4) Best-Fit Decreasing sequence packing recovering 60-75% of compute wasted on padding.
  On Qwen2.5-0.5B with A100-40GB, Chronicals achieves 41,184 tokens/second for full fine-tuning versus Unsloth's 11,736 tokens/second (3.51x). For LoRA at rank 32, we reach 11,699 tokens/second versus Unsloth MAX's 2,857 tokens/second (4.10x). Critically, we discovered that Unsloth's reported 46,000 tokens/second benchmark exhibited zero gradient norms--the model was not training.
  We provide complete mathematical foundations: online softmax correctness proofs, FlashAttention IO complexity bounds O(N^2 d^2 M^{-1}), LoRA+ learning rate derivations from gradient magnitude analysis, and bin-packing approximation guarantees. All implementations, benchmarks, and proofs are available at https://github.com/Ajwebdevs/Chronicals with pip installation via https://pypi.org/project/chronicals/.

</details>


### [17] [Prioritized Replay for RL Post-training](https://arxiv.org/abs/2601.02648)
*Mehdi Fatemi*

Main category: cs.LG

TL;DR: 提出一种基于经验成功率的RL后训练优先级框架，无需预设难度层级，能自动聚焦于中等难度任务，通过堆式采样和周期重测实现高效部署，且在GRPO训练中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习倾向于以易任务开始，导致对中等难度问题关注不足；同时经验观察表明具备中等成功率的rollouts在GRPO等方法下能提供更强的学习信号。

Method: 基于经验成功率计算模型驱动的优先级分数，采用堆式优先采样与周期性重测机制实现自适应调度；不需要预设难度阶梯、辅助预测器或外部标签。

Result: 实验展示了该优先级框架在GRPO后训练中的可行性与可扩展性，显著降低了无效梯度信息，提高了缓解饥饿与遗忘、提升整体训练效果。

Conclusion: 该框架为RL后训练大型语言模型提供了问题层次的优先级排序，能够自动聚焦于学习信号强的中等难度问题，从而提升训练效率并避免对无效问题的浪费。

Abstract: We introduce a problem-level prioritization framework for RL post-training of large language models. Building on insights from prioritized replay in deep RL, as well as prior observations that rollouts with intermediate success rates tend to produce stronger learning signals under methods such as GRPO, our approach selects problems according to a simple, model-driven priority score derived from empirical success statistics. In contrast to conventional curriculum strategies that emphasize easier tasks early in training, the resulting schedule naturally focuses training on problems that are neither consistently solved nor consistently failed, while deprioritizing those that contribute little gradient information. The method yields a continuously adapting and automatic prioritization process that requires no predefined difficulty tiers, auxiliary predictors, or external labels. We further introduce lightweight mechanisms for practical deployment, including heap-based prioritized sampling and periodic retesting of solved and unsolved problems to mitigate starvation and forgetting. Overall, the approach offers a principled and scalable alternative to manually designed curricula while aligning data selection directly with the dynamics of GRPO-based post-training.

</details>


### [18] [MAFS: Multi-head Attention Feature Selection for High-Dimensional Data via Deep Fusion of Filter Methods](https://arxiv.org/abs/2601.02668)
*Xiaoyan Sun,Qingyu Meng,Yalu Wen*

Main category: cs.LG

TL;DR: MAFS集成统计先验、多头注意力与重排模块，实现可解释、稳定且高效的高维特征选择。


<details>
  <summary>Details</summary>
Motivation: 过滤方法可扩展但缺乏复杂关系建模，深度学习捕捉非线性但缺乏稳定性与可解释性，现有方法很少在超高维环境中兼顾统计解释与深度表示。

Method: 将过滤式先验用于稳定初始化，构建多头注意力捕获多层次非线性关系，随后通过重排模块整合各注意头输出，消除冲突并保留关键信号。

Result: 在模拟以及癌症基因表达和阿尔茨海默病数据集上，MAFS在覆盖率与稳定性上均优于传统方案，展示了更强的鲁棒性。

Conclusion: MAFS在高维生物医学数据上显著提升特征选择的覆盖率、稳定性和可解释性，优于现有过滤式及深度学习方法。

Abstract: Feature selection is essential for high-dimensional biomedical data, enabling stronger predictive performance, reduced computational cost, and improved interpretability in precision medicine applications. Existing approaches face notable challenges. Filter methods are highly scalable but cannot capture complex relationships or eliminate redundancy. Deep learning-based approaches can model nonlinear patterns but often lack stability, interpretability, and efficiency at scale. Single-head attention improves interpretability but is limited in capturing multi-level dependencies and remains sensitive to initialization, reducing reproducibility. Most existing methods rarely combine statistical interpretability with the representational power of deep learning, particularly in ultra-high-dimensional settings. Here, we introduce MAFS (Multi-head Attention-based Feature Selection), a hybrid framework that integrates statistical priors with deep learning capabilities. MAFS begins with filter-based priors for stable initialization and guide learning. It then uses multi-head attention to examine features from multiple perspectives in parallel, capturing complex nonlinear relationships and interactions. Finally, a reordering module consolidates outputs across attention heads, resolving conflicts and minimizing information loss to generate robust and consistent feature rankings. This design combines statistical guidance with deep modeling capacity, yielding interpretable importance scores while maximizing retention of informative signals. Across simulated and real-world datasets, including cancer gene expression and Alzheimer's disease data, MAFS consistently achieves superior coverage and stability compared with existing filter-based and deep learning-based alternatives, offering a scalable, interpretable, and robust solution for feature selection in high-dimensional biomedical data.

</details>


### [19] [Uni-FinLLM: A Unified Multimodal Large Language Model with Modular Task Heads for Micro-Level Stock Prediction and Macro-Level Systemic Risk Assessment](https://arxiv.org/abs/2601.02677)
*Gongao Zhang,Haijiang Zeng,Lu Jiang*

Main category: cs.LG

TL;DR: Uni‑FinLLM 通过整合多模态数据、共享 Transformer 主干与多任务学习，在股票、信用风险和宏观预警三大金融预测任务上均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 金融机构与监管机构需要整合异质数据的系统，以评估从个股波动到系统性风险的多层级风险。目前的做法往往将这些任务孤立处理，未能捕捉跨尺度依赖，导致风险评估不完整。

Method: 提出 Uni‑FinLLM：采用共享 Transformer 主干与模组化任务头，通过跨模态注意力和多任务优化，联合处理金融文本、数值时序、财报数据和视觉信息，学习统一表征实现微观、中观、宏观级预测。

Result: 在三项基准挑战中显著优于传统方法：股价方向准确率升至 67.4%（从 61.7%），信用风险准确率 84.1%（从 79.6%），宏观早期预警准确率 82.3%。

Conclusion: 统一多模态大型语言模型能够同时建模资产行为与系统性脆弱性，提供可扩展的金融决策支持引擎。

Abstract: Financial institutions and regulators require systems that integrate heterogeneous data to assess risks from stock fluctuations to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified multimodal large language model that uses a shared Transformer backbone and modular task heads to jointly process financial text, numerical time series, fundamentals, and visual data. Through cross-modal attention and multi-task optimization, it learns a coherent representation for micro-, meso-, and macro-level predictions. Evaluated on stock forecasting, credit-risk assessment, and systemic-risk detection, Uni-FinLLM significantly outperforms baselines. It raises stock directional accuracy to 67.4% (from 61.7%), credit-risk accuracy to 84.1% (from 79.6%), and macro early-warning accuracy to 82.3%. Results validate that a unified multimodal LLM can jointly model asset behavior and systemic vulnerabilities, offering a scalable decision-support engine for finance.

</details>


### [20] [CRoPE: Efficient Parametrization of Rotary Positional Embedding](https://arxiv.org/abs/2601.02728)
*Beicheng Lou,Zifei Xu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Rotary positional embedding has become the state-of-the-art approach to encode position information in transformer-based models. While it is often succinctly expressed in complex linear algebra, we note that the actual implementation of $Q/K/V$-projections is not equivalent to a complex linear transformation. We argue that complex linear transformation is a more natural parametrization and saves near 50\% parameters within the attention block. We show empirically that removing such redundancy has negligible impact on the model performance both in sample and out of sample. Our modification achieves more efficient parameter usage, as well as a cleaner interpretation of the representation space.

</details>


### [21] [Scalable Tree Ensemble Proximities in Python](https://arxiv.org/abs/2601.02735)
*Adrien Aumon,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: 利用叶节点碰撞的可分离加权近似，结合稀疏矩阵分解，实现了对树集成生成的相似度的高效、可扩展计算。


<details>
  <summary>Details</summary>
Motivation: 传统基于树集成的相似度计算方法在大样本规模下复杂度呈平方级，导致无法扩展。

Method: 通过定义Separable Weighted Leaf-Collision Proximities，并证明其可得到精确的稀疏矩阵分解，从而仅在叶节点碰撞上进行运算并使用稀疏线性代数实现实现高效计算。

Result: 在Python实现中，实验显示该方法相较传统技术在运行时间和内存占用上都有显著改进，可在普通CPU环境下处理数十万行的数据。

Conclusion: 该框架显著降低了用树集成方法计算相似度时的时间和内存消耗。

Abstract: Tree ensemble methods such as Random Forests naturally induce supervised similarity measures through their decision tree structure, but existing implementations of proximities derived from tree ensembles typically suffer from quadratic time or memory complexity, limiting their scalability. In this work, we introduce a general framework for efficient proximity computation by defining a family of Separable Weighted Leaf-Collision Proximities. We show that any proximity measure in this family admits an exact sparse matrix factorization, restricting computation to leaf-level collisions and avoiding explicit pairwise comparisons. This formulation enables low-memory, scalable proximity computation using sparse linear algebra in Python. Empirical benchmarks demonstrate substantial runtime and memory improvements over traditional approaches, allowing tree ensemble proximities to scale efficiently to datasets with hundreds of thousands of samples on standard CPU hardware.

</details>


### [22] [Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies](https://arxiv.org/abs/2601.02754)
*Mingming Zhang,Na Li,Zhuang Feiqing,Hongyang Zheng,Jiangbing Zhou,Wang Wuyin,Sheng-jie Sun,XiaoWei Chen,Junxiong Zhu,Lixin Zou,Chenliang Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the rapid development of e-commerce, auto-bidding has become a key asset in optimizing advertising performance under diverse advertiser environments. The current approaches focus on reinforcement learning (RL) and generative models. These efforts imitate offline historical behaviors by utilizing a complex structure with expensive hyperparameter tuning. The suboptimal trajectories further exacerbate the difficulty of policy learning.
  To address these challenges, we proposes QGA, a novel Q-value regularized Generative Auto-bidding method. In QGA, we propose to plug a Q-value regularization with double Q-learning strategy into the Decision Transformer backbone. This design enables joint optimization of policy imitation and action-value maximization, allowing the learned bidding policy to both leverage experience from the dataset and alleviate the adverse impact of the suboptimal trajectories. Furthermore, to safely explore the policy space beyond the data distribution, we propose a Q-value guided dual-exploration mechanism, in which the DT model is conditioned on multiple return-to-go targets and locally perturbed actions. This entire exploration process is dynamically guided by the aforementioned Q-value module, which provides principled evaluation for each candidate action. Experiments on public benchmarks and simulation environments demonstrate that QGA consistently achieves superior or highly competitive results compared to existing alternatives. Notably, in large-scale real-world A/B testing, QGA achieves a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.

</details>


### [23] [Electricity Price Forecasting: Bridging Linear Models, Neural Networks and Online Learning](https://arxiv.org/abs/2601.02856)
*Btissame El Mahtout,Florian Ziel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Precise day-ahead forecasts for electricity prices are crucial to ensure efficient portfolio management, support strategic decision-making for power plant operations, enable efficient battery storage optimization, and facilitate demand response planning. However, developing an accurate prediction model is highly challenging in an uncertain and volatile market environment. For instance, although linear models generally exhibit competitive performance in predicting electricity prices with minimal computational requirements, they fail to capture relevant nonlinear relationships. Nonlinear models, on the other hand, can improve forecasting accuracy with a surge in computational costs. We propose a novel multivariate neural network approach that combines linear and nonlinear feed-forward neural structures. Unlike previous hybrid models, our approach integrates online learning and forecast combination for efficient training and accuracy improvement. It also incorporates all relevant characteristics, particularly the fundamental relationships arising from wind and solar generation, electricity demand patterns, related energy fuel and carbon markets, in addition to autoregressive dynamics and calendar effects. Compared to the current state-of-the-art benchmark models, the proposed forecasting method significantly reduces computational cost while delivering superior forecasting accuracy (12-13% RMSE and 15-18% MAE reductions). Our results are derived from a six-year forecasting study conducted on major European electricity markets.

</details>


### [24] [Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction](https://arxiv.org/abs/2601.02884)
*Hana Yahia,Bruno Figliuzzi,Florent Di Meglio,Laurent Gerbaud,Stephane Menand,Mohamed Mahjoub*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper provides a comprehensive comparison of domain generalization techniques applied to time series data within a drilling context, focusing on the prediction of a continuous Stick-Slip Index (SSI), a critical metric for assessing torsional downhole vibrations at the drill bit. The study aims to develop a robust regression model that can generalize across domains by training on 60 second labeled sequences of 1 Hz surface drilling data to predict the SSI. The model is tested in wells that are different from those used during training. To fine-tune the model architecture, a grid search approach is employed to optimize key hyperparameters. A comparative analysis of the Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM) and baseline models is presented, along with an evaluation of the effectiveness of transfer learning (TL) in improving model performance. The ADG and IRM models achieve performance improvements of 10% and 8%, respectively, over the baseline model. Most importantly, severe events are detected 60% of the time, against 20% for the baseline model. Overall, the results indicate that both ADG and IRM models surpass the baseline, with the ADG model exhibiting a slight advantage over the IRM model. Additionally, applying TL to a pre-trained model further improves performance. Our findings demonstrate the potential of domain generalization approaches in drilling applications, with ADG emerging as the most effective approach.

</details>


### [25] [RPIQ: Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization for Visually Impaired Assistance](https://arxiv.org/abs/2601.02888)
*Xuanyu Wang,Haisen Su,Jingtao Zhang,Xiangxiang Wang,Yongbin Yu,Manping Fan,Bo Gong,Siqi Chen,Mingsheng Cao,Liyong Ren*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Visually impaired users face significant challenges in daily information access and real-time environmental perception, and there is an urgent need for intelligent assistive systems with accurate recognition capabilities. Although large-scale models provide effective solutions for perception and reasoning, their practical deployment on assistive devices is severely constrained by excessive memory consumption and high inference costs. Moreover, existing quantization strategies often ignore inter-block error accumulation, leading to degraded model stability. To address these challenges, this study proposes a novel quantization framework -- Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization(RPIQ), whose quantization process adopts a multi-collaborative closed-loop compensation scheme based on Single Instance Calibration and Gauss-Seidel Iterative Quantization. Experiments on various types of large-scale models, including language models such as OPT, Qwen, and LLaMA, as well as vision-language models such as CogVLM2, demonstrate that RPIQ can compress models to 4-bit representation while significantly reducing peak memory consumption (approximately 60%-75% reduction compared to original full-precision models). The method maintains performance highly close to full-precision models across multiple language and visual tasks, and exhibits excellent recognition and reasoning capabilities in key applications such as text understanding and visual question answering in complex scenarios. While verifying the effectiveness of RPIQ for deployment in real assistive systems, this study also advances the computational efficiency and reliability of large models, enabling them to provide visually impaired users with the required information accurately and rapidly.

</details>


### [26] [Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control](https://arxiv.org/abs/2601.02896)
*Harshvardhan Saini,Yiming Tang,Dianbo Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as "black boxes" with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt discovery. In specific, we propose two methods, RESGA and SAEGA, that both optimize randomly initialized prompts to achieve better aligned representation with an identified persona direction. We introduce fluent gradient ascent to control the fluency of discovered persona steering prompts. We demonstrate RESGA and SAEGA's effectiveness across Llama 3.1, Qwen 2.5, and Gemma 3 for steering three different personas,sycophancy, hallucination, and myopic reward. Crucially, on sycophancy, our automatically discovered prompts achieve significant improvement (49.90% compared with 79.24%). By grounding prompt discovery in mechanistically meaningful features, our method offers a new paradigm for controllable and interpretable behavior modification.

</details>


### [27] [Multi-Distribution Robust Conformal Prediction](https://arxiv.org/abs/2601.02998)
*Yuqi Yang,Ying Jin*

Main category: cs.LG

TL;DR: 提出一种基于 Max‑p 聚合的 conformal 预测集合，可在多源、非均质分布下统一保证覆盖率，并在效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在公平性、分布鲁棒性以及多源学习场景下，需要一个在任意来源分布下都能保证覆盖率的预测集合。

Method: 先使用 Max‑p 聚合提供有限样本多分布覆盖，然后在满足统一覆盖的条件下进行效率优化，并学习可泛化的合规性得分。

Result: 实验表明，该方法在保持最坏情况下覆盖率的同时，显著减小了集合尺寸，并可与标准单源方法相媲美。

Conclusion: 构造的 conformal 预测集合在多源、异质分布下保证统一的覆盖率，且在条件下可达到最优效率。

Abstract: In many fairness and distribution robustness problems, one has access to labeled data from multiple source distributions yet the test data may come from an arbitrary member or a mixture of them. We study the problem of constructing a conformal prediction set that is uniformly valid across multiple, heterogeneous distributions, in the sense that no matter which distribution the test point is from, the coverage of the prediction set is guaranteed to exceed a pre-specified level. We first propose a max-p aggregation scheme that delivers finite-sample, multi-distribution coverage given any conformity scores associated with each distribution. Upon studying several efficiency optimization programs subject to uniform coverage, we prove the optimality and tightness of our aggregation scheme, and propose a general algorithm to learn conformity scores that lead to efficient prediction sets after the aggregation under standard conditions. We discuss how our framework relates to group-wise distributionally robust optimization, sub-population shift, fairness, and multi-source learning. In synthetic and real-data experiments, our method delivers valid worst-case coverage across multiple distributions while greatly reducing the set size compared with naively applying max-p aggregation to single-source conformity scores, and can be comparable in size to single-source prediction sets with popular, standard conformity scores.

</details>


### [28] [In-Context Reinforcement Learning through Bayesian Fusion of Context and Value Prior](https://arxiv.org/abs/2601.03015)
*Anaïs Berkes,Vincent Taboga,Donna Vakalis,David Rolnick,Yoshua Bengio*

Main category: cs.LG

TL;DR: SPICE使用贝叶斯深度集成先验+UCB更新，理论与实验证明其在缺乏最佳数据时仍能在新环境中快速、优异地决策。


<details>
  <summary>Details</summary>
Motivation: 提升无参数更新的快速适应性，解决现有ICRL方法在训练分布外难做改进或需要近最优数据的局限

Method: 构造BP问Q-values的先验，使用深度集成学习得到先验；测试时通过贝叶斯更新利用上下文；结合UCB规则增强探索，弥补次优数据导致的先验不佳

Result: 理论证明在随机 bandit 与有限时 horizon MDP 下实现 regret-optimal；实验显示在 bandit 与控制基准上显著降低 regret，接近最优，且对分布移位保持鲁棒

Conclusion: SPICE 提供了理论与实践双重支持的 ICRL 方法，克服了训练分布依赖且对分布移位鲁棒性差的问题，推动 ICRL 在实际环境中的应用。

Abstract: In-context reinforcement learning (ICRL) promises fast adaptation to unseen environments without parameter updates, but current methods either cannot improve beyond the training distribution or require near-optimal data, limiting practical adoption. We introduce SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensemble and updates this prior at test-time using in-context information through Bayesian updates. To recover from poor priors resulting from training on sub-optimal data, our online inference follows an Upper-Confidence Bound rule that favours exploration and adaptation. We prove that SPICE achieves regret-optimal behaviour in both stochastic bandits and finite-horizon MDPs, even when pretrained only on suboptimal trajectories. We validate these findings empirically across bandit and control benchmarks. SPICE achieves near-optimal decisions on unseen tasks, substantially reduces regret compared to prior ICRL and meta-RL approaches while rapidly adapting to unseen tasks and remaining robust under distribution shift.

</details>


### [29] [Causal Manifold Fairness: Enforcing Geometric Invariance in Representation Learning](https://arxiv.org/abs/2601.03032)
*Vidhi Rathore*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fairness in machine learning is increasingly critical, yet standard approaches often treat data as static points in a high-dimensional space, ignoring the underlying generative structure. We posit that sensitive attributes (e.g., race, gender) do not merely shift data distributions but causally warp the geometry of the data manifold itself. To address this, we introduce Causal Manifold Fairness (CMF), a novel framework that bridges causal inference and geometric deep learning. CMF learns a latent representation where the local Riemannian geometry, defined by the metric tensor and curvature, remains invariant under counterfactual interventions on sensitive attributes. By enforcing constraints on the Jacobian and Hessian of the decoder, CMF ensures that the rules of the latent space (distances and shapes) are preserved across demographic groups. We validate CMF on synthetic Structural Causal Models (SCMs), demonstrating that it effectively disentangles sensitive geometric warping while preserving task utility, offering a rigorous quantification of the fairness-utility trade-off via geometric metrics.

</details>


### [30] [When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability](https://arxiv.org/abs/2601.03047)
*Raphael Ronge,Markus Maier,Frederick Eberhardt*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature extraction and steering capabilities, our investigation suggests that major caution is warranted regarding the generalizability of these claims. We find that feature steering exhibits substantial fragility, with sensitivity to layer selection, steering magnitude, and context. We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another. While SAE-based interpretability produces compelling demonstrations in selected cases, current methods often fall short of the systematic reliability required for safety-critical applications. This suggests a necessary shift in focus from prioritizing interpretability of internal representations toward reliable prediction and control of model output. Our work contributes to a more nuanced understanding of what mechanistic interpretability has achieved and highlights fundamental challenges for AI safety that remain unresolved.

</details>


### [31] [Joint Encoding of KV-Cache Blocks for Scalable LLM Serving](https://arxiv.org/abs/2601.03067)
*Joseph Kampeas,Emir Haleva*

Main category: cs.LG

TL;DR: 通过跨请求、跨段的 KV 缓存块联合压缩，提升 4.38 倍压缩率，推理吞吐提升 40%，兼容现有硬件。


<details>
  <summary>Details</summary>
Motivation: 传统 KV 缓存压缩方法受限于硬编码启发式、张量布局破坏或需要专用计算，难以在现有硬件上实现高并发推断。

Method: 联合编码 KV 缓存块，将跨请求、跨输入段相似的块融合成共享表示，保持原始缓存结构，分析其在泊松过程模型下的率-失真折衷。

Result: 在多种 LLM 与基准上实现高达 4.38 倍的 KV 缓存压缩，误差可忽略；在单机 vLLM 测试中，令标记吞吐量提升约 40%。

Conclusion: 联合编码显著缓解 KV 缓存内存瓶颈，提升高并发推理吞吐量，且不需要专用硬件，优于现有结构化与自适应压缩方案。

Abstract: Modern large language models (LLMs) drive interactive AI systems but are bottlenecked by the memory-heavy growth of key-value (KV) caches, which limits real-time throughput under concurrent loads. Existing KV-cache compression methods rely on rigid heuristics, disrupt tensor layouts, or require specialized compute, hindering scalability and deployment.
  We propose joint encoding of KV-cache blocks, which fuses similar blocks across requests and input chunks into shared representations while preserving standard cache structure. This alleviates the KV-cache memory bottleneck, supporting high-concurrency serving without specialized hardware. Theoretically, we analyze the rate-distortion tradeoff of fused cache blocks under a Poisson process model. Empirically, our method achieves up to 4.38 $\times$ KV-cache compression with negligible accuracy loss across diverse LLMs and benchmarks, outperforming recent structured and adaptive compression baselines. In real LLM serving, joint encoding improves the token throughput by $\sim$40\% on a single-machine vLLM benchmark, demonstrating substantial gains in inference throughput. Code is available at https://github.com/sef1/kv_fast_fusion  kv_joint_encoding.

</details>


### [32] [Real-Time Adaptive Anomaly Detection in Industrial IoT Environments](https://arxiv.org/abs/2601.03085)
*Mahsa Raeiszadeh,Amin Ebrahimzadeh,Roch H. Glitho,Johan Eker,Raquel A. F. Mini*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To ensure reliability and service availability, next-generation networks are expected to rely on automated anomaly detection systems powered by advanced machine learning methods with the capability of handling multi-dimensional data. Such multi-dimensional, heterogeneous data occurs mostly in today's industrial Internet of Things (IIoT), where real-time detection of anomalies is critical to prevent impending failures and resolve them in a timely manner. However, existing anomaly detection methods often fall short of effectively coping with the complexity and dynamism of multi-dimensional data streams in IIoT. In this paper, we propose an adaptive method for detecting anomalies in IIoT streaming data utilizing a multi-source prediction model and concept drift adaptation. The proposed anomaly detection algorithm merges a prediction model into a novel drift adaptation method resulting in accurate and efficient anomaly detection that exhibits improved scalability. Our trace-driven evaluations indicate that the proposed method outperforms the state-of-the-art anomaly detection methods by achieving up to an 89.71% accuracy (in terms of Area under the Curve (AUC)) while meeting the given efficiency and scalability requirements.

</details>


### [33] [Audit Me If You Can: Query-Efficient Active Fairness Auditing of Black-Box LLMs](https://arxiv.org/abs/2601.03087)
*David Hartmann,Lena Pohlmann,Lelia Hanslik,Noah Gießing,Bettina Berendt,Pieter Delobelle*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) exhibit systematic biases across demographic groups. Auditing is proposed as an accountability tool for black-box LLM applications, but suffers from resource-intensive query access. We conceptualise auditing as uncertainty estimation over a target fairness metric and introduce BAFA, the Bounded Active Fairness Auditor for query-efficient auditing of black-box LLMs. BAFA maintains a version space of surrogate models consistent with queried scores and computes uncertainty intervals for fairness metrics (e.g., $Δ$ AUC) via constrained empirical risk minimisation. Active query selection narrows these intervals to reduce estimation error. We evaluate BAFA on two standard fairness dataset case studies: \textsc{CivilComments} and \textsc{Bias-in-Bios}, comparing against stratified sampling, power sampling, and ablations. BAFA achieves target error thresholds with up to 40$\times$ fewer queries than stratified sampling (e.g., 144 vs 5,956 queries at $\varepsilon=0.02$ for \textsc{CivilComments}) for tight thresholds, demonstrates substantially better performance over time, and shows lower variance across runs. These results suggest that active sampling can reduce resources needed for independent fairness auditing with LLMs, supporting continuous model evaluations.

</details>


### [34] [ATLAS: Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning](https://arxiv.org/abs/2601.03093)
*Tuc Nguyen,Thai Le*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work on activation and latent steering has demonstrated that modifying internal representations can effectively guide large language models (LLMs) toward improved reasoning and efficiency without additional training. However, most existing approaches rely on fixed steering policies and static intervention strengths, which limit their robustness across problem instances and often result in over- or under-steering. We propose Adaptive Test-time Latent Steering, called (ATLAS), a task- specific framework that dynamically controls steering decisions at inference time using an external, lightweight latent verifier. Given intermediate hidden states, the verifier predicts the quality of ongoing reasoning and adaptively selects whether and how strongly to apply steering, enabling per-example and per-step adjustment with minimal overhead. To our knowledge, ATLAS is the first method to integrate learned latent verification into test-time steering for enhancing LLMs reasoning. Experiments on multiple mathematical reasoning benchmarks show that ATLAS consistently outperforms both vanilla decoding and fixed steering baselines, achieving higher accuracy while substantially reducing test-time token usage. These results demonstrate that verifier-guided latent adaptation provides an effective and scalable mechanism for controlling reasoning efficiency without sacrificing solution quality. All source code will be publicly available.

</details>


### [35] [From Muscle to Text with MyoText: sEMG to Text via Finger Classification and Transformer-Based Decoding](https://arxiv.org/abs/2601.03098)
*Meghna Roy Chowdhury,Shreyas Sen,Yi Ding*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Surface electromyography (sEMG) provides a direct neural interface for decoding muscle activity and offers a promising foundation for keyboard-free text input in wearable and mixed-reality systems. Previous sEMG-to-text studies mainly focused on recognizing letters directly from sEMG signals, forming an important first step toward translating muscle activity into text. Building on this foundation, we present MyoText, a hierarchical framework that decodes sEMG signals to text through physiologically grounded intermediate stages. MyoText first classifies finger activations from multichannel sEMG using a CNN-BiLSTM-Attention model, applies ergonomic typing priors to infer letters, and reconstructs full sentences with a fine-tuned T5 transformer. This modular design mirrors the natural hierarchy of typing, linking muscle intent to language output and reducing the search space for decoding. Evaluated on 30 users from the emg2qwerty dataset, MyoText outperforms baselines by achieving 85.4% finger-classification accuracy, 5.4% character error rate (CER), and 6.5% word error rate (WER). Beyond accuracy gains, this methodology establishes a principled pathway from neuromuscular signals to text, providing a blueprint for virtual and augmented-reality typing interfaces that operate entirely without physical keyboards. By integrating ergonomic structure with transformer-based linguistic reasoning, MyoText advances the feasibility of seamless, wearable neural input for future ubiquitous computing environments.

</details>


### [36] [Time-Aware Synthetic Control](https://arxiv.org/abs/2601.03099)
*Saeyoung Rho,Cyrus Illick,Samhitha Narasipura,Alberto Abadie,Daniel Hsu,Vishal Misra*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The synthetic control (SC) framework is widely used for observational causal inference with time-series panel data. SC has been successful in diverse applications, but existing methods typically treat the ordering of pre-intervention time indices interchangeable. This invariance means they may not fully take advantage of temporal structure when strong trends are present. We propose Time-Aware Synthetic Control (TASC), which employs a state-space model with a constant trend while preserving a low-rank structure of the signal. TASC uses the Kalman filter and Rauch-Tung-Striebel smoother: it first fits a generative time-series model with expectation-maximization and then performs counterfactual inference. We evaluate TASC on both simulated and real-world datasets, including policy evaluation and sports prediction. Our results suggest that TASC offers advantages in settings with strong temporal trends and high levels of observation noise.

</details>


### [37] [One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling](https://arxiv.org/abs/2601.03111)
*Yiyuan Li,Zhen Huang,Yanan Wu,Weixun Wang,Xuefeng Li,Yijia Luo,Wenbo Su,Bo Zheng,Pengfei Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.

</details>


### [38] [PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback](https://arxiv.org/abs/2601.03149)
*Dehao Yuan,Tyler Farnan,Stefan Tesliuc,Doron L Bergman,Yulun Wu,Xiaoyu Liu,Minghui Liu,James Montgomery,Nam H Nguyen,C. Bayan Bruss,Furong Huang*

Main category: cs.LG

TL;DR: 受隐私限制，金融AI缺乏真实交易数据；现有合成方法在多样性或约束性上不足。PersonaLedger通过LLM+程序引擎闭环生成，既保持行为多样，又严格遵守金融规则，生成30M交易数据并提供基准。


<details>
  <summary>Details</summary>
Motivation: 现实金融交易数据因隐私监管难以获取，现有合成方法要么缺乏行为多样性，要么难以满足金融约束，迫切需要一种既能生成真实多样行为又能严格遵守约束的合成数据生成引擎。

Method: 采用大语言模型在丰富的用户画像条件下生成交易事件，并通过闭环程序引擎实时校正用户状态与金融约束，将引擎返回的“nextprompt”反馈给LLM，引导持续产生符合规则的后续交易。

Result: 构建了PersonaLedger，生成30,000名用户、3,000万笔交易的公共数据集，并提供两个评测基准（流动性分类与身份盗窃分割），展示了该生成方法在多样性和逻辑正确性方面的优势。

Conclusion: PersonaLedger提供了一种结合LLM与可配置程序引擎的框架，能够生成行为多样且符合金融规则的合成交易数据，并公开30万条交易数据集及其基准任务；该方法在保证隐私的前提下促进金融AI研究。

Abstract: Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware "nextprompt" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.

</details>


### [39] [Prompt-Counterfactual Explanations for Generative AI System Behavior](https://arxiv.org/abs/2601.03156)
*Sofie Goethals,Foster Provost,João Sedoc*

Main category: cs.LG

TL;DR: 本文改造反事实解释框架，提出提示反事实解释(PCE)算法，利用下游分类器捕捉生成式AI输出特征，并通过政治倾向、毒性、情绪三例演示其在提示工程与红队中的效能，构建生成式AI提示可解释性的基础。


<details>
  <summary>Details</summary>
Motivation: 解释生成式AI系统为何在不同输入提示下产生特定输出特征（如毒性、负面情绪、政治偏见），满足决策者对模型透明度和可监管性需求。

Method: 采用可解释AI中的反事实解释框架，针对非确定性的生成式AI改造后提出提示反事实解释（PCE）算法，并结合下游分类器识别输出关键特征。

Result: 通过三个案例（政治倾向、毒性、情绪）展示PCE能够快速生成提示反事实解释，帮助提示工程师抑制不良输出并提升红队测试效果，证明其可作为生成式AI提示解释的基础工具。

Conclusion: 提示焦点的可解释性为在高风险任务中使用生成式AI时的透明度和问责制奠定基础；PCE为未来监管合规提供必要技术支持。

Abstract: As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -the prompt- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.

</details>


### [40] [Rapid Augmentations for Time Series (RATS): A High-Performance Library for Time Series Augmentation](https://arxiv.org/abs/2601.03159)
*Wadie Skaf,Felix Kern,Aryamaan Basu Roy,Tejas Pradhan,Roman Kalkreuth,Holger Hoos*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time series augmentation is critical for training robust deep learning models, particularly in domains where labelled data is scarce and expensive to obtain. However, existing augmentation libraries for time series, mainly written in Python, suffer from performance bottlenecks, where running time grows exponentially as dataset sizes increase -- an aspect limiting their applicability in large-scale, production-grade systems. We introduce RATS (Rapid Augmentations for Time Series), a high-performance library for time series augmentation written in Rust with Python bindings (RATSpy). RATS implements multiple augmentation methods spanning basic transformations, frequency-domain operations and time warping techniques, all accessible through a unified pipeline interface with built-in parallelisation. Comprehensive benchmarking of RATSpy versus a commonly used library (tasug) on 143 datasets demonstrates that RATSpy achieves an average speedup of 74.5\% over tsaug (up to 94.8\% on large datasets), with up to 47.9\% less peak memory usage.

</details>


### [41] [On the Convergence Behavior of Preconditioned Gradient Descent Toward the Rich Learning Regime](https://arxiv.org/abs/2601.03162)
*Shuai Jiang,Alexey Voronin,Eric Cyr,Ben Southworth*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Spectral bias, the tendency of neural networks to learn low frequencies first, can be both a blessing and a curse. While it enhances the generalization capabilities by suppressing high-frequency noise, it can be a limitation in scientific tasks that require capturing fine-scale structures. The delayed generalization phenomenon known as grokking is another barrier to rapid training of neural networks. Grokking has been hypothesized to arise as learning transitions from the NTK to the feature-rich regime. This paper explores the impact of preconditioned gradient descent (PGD), such as Gauss-Newton, on spectral bias and grokking phenomena. We demonstrate through theoretical and empirical results how PGD can mitigate issues associated with spectral bias. Additionally, building on the rich learning regime grokking hypothesis, we study how PGD can be used to reduce delays associated with grokking. Our conjecture is that PGD, without the impediment of spectral bias, enables uniform exploration of the parameter space in the NTK regime. Our experimental results confirm this prediction, providing strong evidence that grokking represents a transitional behavior between the lazy regime characterized by the NTK and the rich regime. These findings deepen our understanding of the interplay between optimization dynamics, spectral bias, and the phases of neural network learning.

</details>


### [42] [Dynamic Hyperparameter Importance for Efficient Multi-Objective Optimization](https://arxiv.org/abs/2601.03166)
*Daphne Theodorakopoulos,Marcel Wever,Marius Lindauer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Choosing a suitable ML model is a complex task that can depend on several objectives, e.g., accuracy, model size, fairness, inference time, or energy consumption. In practice, this requires trading off multiple, often competing, objectives through multi-objective optimization (MOO). However, existing MOO methods typically treat all hyperparameters as equally important, overlooking that hyperparameter importance (HPI) can vary significantly depending on the trade-off between objectives. We propose a novel dynamic optimization approach that prioritizes the most influential hyperparameters based on varying objective trade-offs during the search process, which accelerates empirical convergence and leads to better solutions. Building on prior work on HPI for MOO post-analysis, we now integrate HPI, calculated with HyperSHAP, into the optimization. For this, we leverage the objective weightings naturally produced by the MOO algorithm ParEGO and adapt the configuration space by fixing the unimportant hyperparameters, allowing the search to focus on the important ones. Eventually, we validate our method with diverse tasks from PyMOO and YAHPO-Gym. Empirical results demonstrate improvements in convergence speed and Pareto front quality compared to baselines.

</details>


### [43] [Predicting Time Pressure of Powered Two-Wheeler Riders for Proactive Safety Interventions](https://arxiv.org/abs/2601.03173)
*Sumit S. Shevtekar,Chandresh K. Maurya,Gourab Sil,Subasish Das*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time pressure critically influences risky maneuvers and crash proneness among powered two-wheeler riders, yet its prediction remains underexplored in intelligent transportation systems. We present a large-scale dataset of 129,000+ labeled multivariate time-series sequences from 153 rides by 51 participants under No, Low, and High Time Pressure conditions. Each sequence captures 63 features spanning vehicle kinematics, control inputs, behavioral violations, and environmental context. Our empirical analysis shows High Time Pressure induces 48% higher speeds, 36.4% greater speed variability, 58% more risky turns at intersections, 36% more sudden braking, and 50% higher rear brake forces versus No Time Pressure. To benchmark this dataset, we propose MotoTimePressure, a deep learning model combining convolutional preprocessing, dual-stage temporal attention, and Squeeze-and-Excitation feature recalibration, achieving 91.53% accuracy and 98.93% ROC AUC, outperforming eight baselines. Since time pressure cannot be directly measured in real time, we demonstrate its utility in collision prediction and threshold determination. Using MTPS-predicted time pressure as features, improves Informer-based collision risk accuracy from 91.25% to 93.51%, approaching oracle performance (93.72%). Thresholded time pressure states capture rider cognitive stress and enable proactive ITS interventions, including adaptive alerts, haptic feedback, V2I signaling, and speed guidance, supporting safer two-wheeler mobility under the Safe System Approach.

</details>


### [44] [Decentralized Autoregressive Generation](https://arxiv.org/abs/2601.03184)
*Stepan Maschan,Haoxuan Qu,Jun Liu*

Main category: cs.LG

TL;DR: 该研究通过 Decentralized Discrete Flow Matching 目标，展示了在多模态语言模型上去中心化训练与中心化训练的等价性，验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 探究如何在保持性能的前提下，实现自动回归生成任务的去中心化训练，以提升可扩展性与资源利用效率。

Method: 引入 Decentralized Discrete Flow Matching 目标，将生成概率的速度线性组合为专家流，并在实验中对比 LLaVA 与 InternVL 2.5‑1B 两种架构在去中心化与中心化训练下的效果。

Result: 实验表明，使用固定 CLIP 视觉编码器并在指令调优阶段进行全参数微调的 LLaVA 与 InternVL 2.5‑1B，在去中心化与中心化训练设置下均能实现等价的表现。

Conclusion: 本文证明了在多模态语言模型中，采用 Decentralized Discrete Flow Matching 目标的去中心化训练与传统中心化训练在各类基准测试上表现等价。

Abstract: We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrat- ing the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and per- forms full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.

</details>


### [45] [Sparse Knowledge Distillation: A Mathematical Framework for Probability-Domain Temperature Scaling and Multi-Stage Compression](https://arxiv.org/abs/2601.03195)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We develop a unified theoretical framework for sparse knowledge distillation based on probability-domain softening operators. While the equivalence $p^{1/T} \propto \mathrm{softmax}(z/T)$ is well known, our contribution is an operator-level analytical framework built on this foundation rather than the equivalence itself.
  The framework comprises four core components: (i) operator-agnostic bias--variance decompositions that characterize when sparse students outperform dense teachers, (ii) a homotopy path formalization of multi-stage pruning in function space explaining why iterative compression succeeds where one-shot pruning fails, (iii) convergence guarantees establishing $O(1/n)$ rates for $n$-stage distillation with explicit parameter dependence, and (iv) equivalence class characterizations identifying distinct probability-domain operators that yield identical student models under capacity constraints.
  We introduce an axiomatic definition of probability-domain softening operators based on ranking preservation, continuity, entropy monotonicity, identity, and boundary behavior, and show that multiple non-equivalent operator families satisfy these axioms. All learning-theoretic guarantees are shown to hold uniformly across this operator class, independent of implementation details. These results provide theoretical grounding for black-box teacher distillation, partial-access settings such as top-$k$ truncation and text-only outputs, and privacy-preserving model compression.

</details>


### [46] [Empowering Reliable Visual-Centric Instruction Following in MLLMs](https://arxiv.org/abs/2601.03198)
*Weilei He,Feng Ju,Zhiyuan Fan,Rui Min,Minhao Cheng,Yi R. Fung*

Main category: cs.LG

TL;DR: 推出VC-IFEval基准，系统结合视觉约束对多模态模型进行评测；微调后模型表现大幅提升，实验揭示了不同模型的优势与限制。


<details>
  <summary>Details</summary>
Motivation: 现有的评测仅聚焦文本指令，无法捕捉视觉模态中的隐式约束，导致对多模态模型指令遵循能力的评估不完整。

Method: 构建系统化的VC-IFEval 数据集，将视觉依赖约束嵌入指令设计；在该数据集上微调多模态大模型；在代表性模型上进行大规模实验评估。

Result: 微调后的多模态模型在视觉指令遵循准确率和遵从度上取得显著提升；对多种代表性模型的评估提供了新洞见，展示了各模型的强项与弱点。

Conclusion: VC-IFEval为多模态大模型的指令遵循评估提供了细粒度、严格的基准，证明在视觉依赖约束下的评测与微调能够显著提升模型的视觉指令遵循准确性，并揭示当前模型的优势与局限。

Abstract: Evaluating the instruction-following (IF) capabilities of Multimodal Large Language Models (MLLMs) is essential for rigorously assessing how faithfully model outputs adhere to user-specified intentions. Nevertheless, existing benchmarks for evaluating MLLMs' instruction-following capability primarily focus on verbal instructions in the textual modality. These limitations hinder a thorough analysis of instruction-following capabilities, as they overlook the implicit constraints embedded in the semantically rich visual modality. To address this gap, we introduce VC-IFEval, a new benchmark accompanied by a systematically constructed dataset that evaluates MLLMs' instruction-following ability under multimodal settings. Our benchmark systematically incorporates vision-dependent constraints into instruction design, enabling a more rigorous and fine-grained assessment of how well MLLMs align their outputs with both visual input and textual instructions. Furthermore, by fine-tuning MLLMs on our dataset, we achieve substantial gains in visual instruction-following accuracy and adherence. Through extensive evaluation across representative MLLMs, we provide new insights into the strengths and limitations of current models.

</details>


### [47] [Counterfactual Fairness with Graph Uncertainty](https://arxiv.org/abs/2601.03203)
*Davi Valério,Chrysoula Zerva,Mariana Pinto,Ricardo Santos,André Carreiro*

Main category: cs.LG

TL;DR: CF-GU在不确定的因果图下进行偏差评估，利用自助采样及熵量化提供统计置信区间，在合成与真实数据集上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 在现实环境下，因果图往往不确定，单一因果图下的Counterfactual Fairness审核可能失真。

Method: CF-GU通过在领域知识约束下对因果发现算法进行自助重采样，生成一组可行有向无环图；用归一化香农熵量化图结构不确定性，并给出CF指标的置信区间。

Result: 实验表明不同领域知识假设能支持或驳斥CF审核；在COMPAS与Adult数据集上，即使仅提供最小领域知识约束，也能以高置信度定位已知偏差。

Conclusion: 将因果图不确定性纳入CF评估，可提高偏差检测的鲁棒性和可信度。

Abstract: Evaluating machine learning (ML) model bias is key to building trustworthy and robust ML systems. Counterfactual Fairness (CF) audits allow the measurement of bias of ML models with a causal framework, yet their conclusions rely on a single causal graph that is rarely known with certainty in real-world scenarios. We propose CF with Graph Uncertainty (CF-GU), a bias evaluation procedure that incorporates the uncertainty of specifying a causal graph into CF. CF-GU (i) bootstraps a Causal Discovery algorithm under domain knowledge constraints to produce a bag of plausible Directed Acyclic Graphs (DAGs), (ii) quantifies graph uncertainty with the normalized Shannon entropy, and (iii) provides confidence bounds on CF metrics. Experiments on synthetic data show how contrasting domain knowledge assumptions support or refute audits of CF, while experiments on real-world data (COMPAS and Adult datasets) pinpoint well-known biases with high confidence, even when supplied with minimal domain knowledge constraints.

</details>


### [48] [Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion](https://arxiv.org/abs/2601.03213)
*Mykola Vysotskyi,Zahar Kohut,Mariia Shpir,Taras Rumezhak,Volodymyr Karpiv*

Main category: cs.LG

TL;DR: 提出一种基于强化学习的扩散消忘框架，将去噪视为序列决策，利用CLIP奖励预测器提供步骤性奖励，使用时序信用估计更新反向扩散核，实验表明优于传统方法且保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型的无监督消忘方法使用监督权重编辑或全局惩罚，RL方法受限于稀疏终时奖励导致高方差与弱信用分配，需要更稳健、细粒度的学习策略。

Method: 通过将去噪视为一系列决策，训练基于CLIP的奖励预测器以在噪声里程碑上产生奖励，并利用含步骤信息的时序信用估计进行策略梯度更新，同时支持离线重用。

Result: 在多种概念上进行实验，所提出方法在保持图像质量和提示忠实度的前提下，与强基线相比，取得了更优或相当的遗忘效果；零件实验验证了步骤级评估器与噪声条件奖励对于稳定性和效果的重要性。

Conclusion: 本论文提出了一种通用的强化学习框架，用于在文本到图像扩散模型中实现机器消忘，能够有效移除目标概念同时保持图像质量与提示的忠实度。

Abstract: Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.

</details>


### [49] [From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence](https://arxiv.org/abs/2601.03220)
*Marc Finzi,Shikai Qiu,Yiding Jiang,Pavel Izmailov,J. Zico Kolter,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 提出 epiplexity 概念以评估受限观测者下的数据信息价值，并提供实用估计方法，证明其对数据选择与泛化的正面影响。


<details>
  <summary>Details</summary>
Motivation: 传统信息熵与 Kolmogorov 复杂度无法衡量有限计算资源下的“有用信息”，导致难以评估数据价值与支持现代机器学习实践。

Method: 通过识别信息理论中的三悖论，定义epiplexity为受限观测者可学习的结构信息，排除时间受限的熵；设计实用估计程序，并在多种数据源上验证其与下游性能的相关性和分布外泛化改进。

Result: 展示通过计算可以创造信息、数据顺序影响信息内容以及似然模型可生成比生成过程更复杂的程序；实证估计方法捕获数据差异、跟踪下游表现，并指出数据介入提升 OOD 泛化。

Conclusion: 提出“epiplexity”概念，阐明在受限观察者下可从数据中生成信息、数据顺序重要性及似然建模能产生更复杂程序，并提供估计方法；为数据选择、生成与转换提供理论依据。

Abstract: Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, we identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, we introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, we demonstrate how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. We also present practical procedures to estimate epiplexity which we show capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization. In contrast to principles of model selection, epiplexity provides a theoretical foundation for data selection, guiding how to select, generate, or transform data for learning systems.

</details>


### [50] [PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters](https://arxiv.org/abs/2601.03237)
*Javier Salazar Cavazos*

Main category: cs.LG

TL;DR: PET‑TURTLE在TURTLE基础上加入了适应不平衡的幂律代价函数并使用稀疏logit，提升了聚类精度，特别是在类别不平衡数据上效果明显。


<details>
  <summary>Details</summary>
Motivation: TURTLE假设簇平衡，在面对大多数实际数据不平衡时会产生不理想的超平面，导致聚类误差。研究者希望通过改进代价函数来适应不平衡分布。

Method: 在TURTLE的迭代更新机制中加入一个能处理类别不平衡的代价函数，并在标签阶段使用稀疏logit降低搜索空间；这一改动使得学习过程更稳定，最终获得更优的超平面划分。

Result: 在合成与真实数据集实验中，PET‑TURTLE相比原始TURTLE显著降低了误聚类率，避免了少数类过度预测，并提升了整体聚类准确度。

Conclusion: PET‑TURTLE通过引入幂律先验和稀疏logit，解决了TURTLE在数据不平衡时高误差的问题，并提升了整体聚类准度。

Abstract: Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperplane margin, in a similar fashion to support vector machines (SVMs). However, TURTLE assumes clusters are balanced; when data is imbalanced, it yields non-ideal hyperplanes that cause higher clustering error. We propose PET-TURTLE, which generalizes the cost function to handle imbalanced data distributions by a power law prior. Additionally, by introducing sparse logits in the labeling process, PET-TURTLE optimizes a simpler search space that in turn improves accuracy for balanced datasets. Experiments on synthetic and real data show that PET-TURTLE improves accuracy for imbalanced sources, prevents over-prediction of minority clusters, and enhances overall clustering.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [51] [Hydrodynamic Whispering: Enabling Near-Field Silent Communication via Artificial Lateral Line Arrays](https://arxiv.org/abs/2601.02394)
*Yuan-Jie Chen*

Main category: eess.SP

TL;DR: 通过利用近场水压波的短程低截获特性，实现了安全、可靠的短距离水下通信。


<details>
  <summary>Details</summary>
Motivation: 解决水下隐蔽群体协作的通信安全与噪声干扰问题。

Method: 基于潜流理论，将发射器建模为振荡偶极源，采用BPSK调制和24传感器形态学响应式ALL阵列，结合空间匹配场波束形成的时空联合处理。

Result: 模拟结果显示，在有效范围内得到约13.8 dB的阵列增益，误码率几乎为零。

Conclusion: 本研究表明利用近场静默压力波可以实现短距离水下自律群体低概率截获的通讯；系统汇聚信噪比可达13.8 dB，误码率基本为零。

Abstract: To address the imperative for covert underwater swarm coordination, this paper introduces "Hydrodynamic Whispering," a near-field silent communication paradigm utilizing Artificial Lateral Line (ALL) arrays. Grounded in potential flow theory, we model the transmitter as an oscillating dipole source. The resulting pressure field exhibits steep nearfield attenuation (scaling with 1/r^2, naturally delimiting a secure "communication bubble" with intrinsic Low Probability of Interception (LPI) properties. We propose a transceiver architecture featuring a Binary Phase Shift Keying (BPSK) modulation scheme adapted for mechanical actuator inertia, coupled with a bio-inspired 24-sensor conformal array. To mitigate low Signal-to-Noise Ratio (SNR) in turbulent environments,a Spatio-Temporal Joint Processing framework incorporating Spatial Matched-Field Beamforming is developed. Simulation results demonstrate that the system achieves an array gain of approximately 13.8 dB and maintains a near-zero Bit Error Rate (BER) within the effective range. This study validates the feasibility of utilizing localized hydrodynamic pressure fluctuations for reliable and secure short-range underwater networking.

</details>


### [52] [Transparent and Resilient Activity Recognition via Attention-Based Distributed Radar Sensing](https://arxiv.org/abs/2601.02874)
*Mina Shahbazifar,Zolfa Zeinalpour-Yazdi,Matthias Hollick,Arash Asadi,Vahid Jamali*

Main category: eess.SP

TL;DR: 端到端雷达框架：轻量CNN+自注意力+监督对比，降低模型70.8%，提高准确度，降低通信延迟并解释节点贡献。


<details>
  <summary>Details</summary>
Motivation: 使用分布式雷达传感器实现鲁棒的人类行为识别，但在节点数量扩大时，特征提取与透明数据融合面临挑战。

Method: 提出端到端框架：每个雷达节点采用轻量化二维CNN提取局部特征，随后通过自注意力融合块建模节点间关系并自适应融合信息。局部特征提取可将输入维度降低多达480倍；自注意力机制提供透明解释性；混合监督对比损失提升特征分离度，尤其对细粒度与不平衡类别。

Result: 在真实 UWB 雷达数据上实验表明，该方法将模型复杂度降低70.8%，同时在平均准确率上优于基线。

Conclusion: 该框架实现了透明、高效、低开销的分布式雷达感知，显著提升了人类活动识别性能。

Abstract: Distributed radar sensors enable robust human activity recognition. However, scaling the number of coordinated nodes introduces challenges in feature extraction from large datasets, and transparent data fusion. We propose an end-to-end framework that operates directly on raw radar data. Each radar node employs a lightweight 2D Convolutional Neural Network (CNN) to extract local features. A self-attention fusion block then models inter-node relationships and performs adaptive information fusion. Local feature extraction reduces the input dimensionality by up to 480x. This significantly lowers communication overhead and latency. The attention mechanism provides inherent interpretability by quantifying the contribution of each radar node. A hybrid supervised contrastive loss further improves feature separability, especially for fine-grained and imbalanced activity classes. Experiments on real-world distributed Ultra Wide Band (UWB) radar data demonstrate that the proposed method reduces model complexity by 70.8\%, while achieving higher average accuracy than baseline approaches. Overall, the framework enables transparent, efficient, and low-overhead distributed radar sensing.

</details>


### [53] [Study of Class-Incremental Radio Frequency Fingerprint Recognition Without Storing Exemplars](https://arxiv.org/abs/2601.03063)
*Rundong Jiang,Jun Hu,Yunqi Song,Zhiyuan Xie,Shiyou Xu*

Main category: eess.SP

TL;DR: 提出一种不需存储样本的增量学习管线，用 GMM 伪特征、随机屏蔽增强及多教师蒸馏，将 Radio Frequency Fingerprint 的学习压缩为单一推理 Adapter。实验表明，在大规模 ADS‑B 数据上实现更高准确率、较低遗忘，并极大降低存储成本。


<details>
  <summary>Details</summary>
Motivation: 在 IoT 与无人系统中，设备数量快速增长，身份鉴别至关重要。传统 RFF 方案需存储设备样本，受限于类数增长、存储及隐私问题，难以在新设备不断加入的场景下持续上线。

Method: 1）冻结预训练特征提取器，仅训练分类器与轻量级 Adapter；2）为每个新类拟合对角 Gaussian 混合模型（GMM），并采样伪特征以回顾旧类；3）采用时域随机屏蔽（random‑masking）增强提升少样本鲁棒性；4）多教师蒸馏压缩多阶段 Adapter 为单一推理 Adapter，兼顾精度与速度。

Result: 在自建 ADS‑B 数据集上：预训练 2,175 类，增量实验 669 类，多轮多步；相比基线，平均准确率更高、遗忘更低、存储需求显著减小，并且完全不保存原始信号。

Conclusion: 该无样本增量 RFF 框架在资源和隐私受限的环境中实现可靠、低存储的设备身份识别，为未来的 IoT 与无人系统部署提供实用解决方案。

Abstract: The rapid proliferation of wireless devices makes robust identity authentication essential. Radio Frequency Fingerprinting (RFF) exploits device-specific, hard-to-forge physical-layer impairments for identification, and is promising for IoT and unmanned systems. In practice, however, new devices continuously join deployed systems while per-class training data are limited. Conventional static training and naive replay of stored exemplars are impractical due to growing class cardinality, storage cost, and privacy concerns.
  We propose an exemplar-free class-incremental learning framework tailored to RFF recognition. Starting from a pretrained feature extractor, we freeze the backbone during incremental stages and train only a classifier together with lightweight Adapter modules that perform small task-specific feature adjustments. For each class we fit a diagonal Gaussian Mixture Model (GMM) to the backbone features and sample pseudo-features from these fitted distributions to rehearse past classes without storing raw signals. To improve robustness under few-shot conditions we introduce a time-domain random-masking augmentation and adopt a multi-teacher distillation scheme to compress stage-wise Adapters into a single inference Adapter, trading off accuracy and runtime efficiency.
  We evaluate the method on large, self-collected ADS-B datasets: the backbone is pretrained on 2,175 classes and incremental experiments are run on a disjoint set of 669 classes with multiple rounds and step sizes. Against several representative baselines, our approach consistently yields higher average accuracy and lower forgetting, while using substantially less storage and avoiding raw-data retention.
  The proposed pipeline is reproducible and provides a practical, low-storage solution for RFF deployment in resource- and privacy-constrained environments.

</details>


### [54] [A Conditional Variational Framework for Channel Prediction in High-Mobility 6G OTFS Networks](https://arxiv.org/abs/2601.03084)
*Mohsen Kazemian,Jürgen Jasperneite*

Main category: eess.SP

TL;DR: 本研究提出 CVAE4CP，对 OTFS 高速移动通道进行预测，利用条件变分自编码器结合系统与速度参数，显著降低 NMSE，优于传统学习基线。


<details>
  <summary>Details</summary>
Motivation: 在高速移动 OTFS 环境下，快速时间变化导致传统基于导频的通道估计失效，亟需可靠的通道预测方法以支持稳健检测与解码。

Method: 使用条件变分自编码器（CVAE）学习 OTFS 延迟-多普勒通道系数的条件分布，输入物理系统和移动参数作为条件信息，输出未来通道系数的低维潜在表示，实现前瞻性通道预测。

Result: 数值仿真显示，CVAE4CP 在高多普勒频率和较长预测周期下，均能持续取得比竞争学习基线更低的归一化均方误差，证实了其预测精度和稳定性。

Conclusion: CVAE4CP 方法在高速移动 OTFS 通道预测中表现出优越的 NMSE 性能，显著优于基线模型，验证了其鲁棒性和有效性。

Abstract: This paper proposes a machine learning (ML) based method for channel prediction in high mobility orthogonal time frequency space (OTFS) channels. In these scenarios, rapid variations caused by Doppler spread and time varying multipath propagation lead to fast channel decorrelation, making conventional pilot based channel estimation methods prone to outdated channel state information (CSI) and excessive overhead. Therefore, reliable channel prediction methods become essential to support robust detection and decoding in OTFS systems. In this paper, we propose conditional variational autoencoder for channel prediction (CVAE4CP) method, which learns the conditional distribution of OTFS delay Doppler channel coefficients given physical system and mobility parameters. By incorporating these parameters as conditioning information, the proposed method enables the prediction of future channel coefficients before their actual realization, while accounting for inherent channel uncertainty through a low dimensional latent representation. The proposed framework is evaluated through extensive simulations under high mobility conditions. Numerical results demonstrate that CVAE4CP consistently outperforms a competing learning based baseline in terms of normalized mean squared error (NMSE), particularly at high Doppler frequencies and extended prediction horizons. These results confirm the effectiveness and robustness of the proposed approach for channel prediction in rapidly time varying OTFS systems.

</details>


### [55] [Spectral-Efficient LoRa with Low Complexity Detection](https://arxiv.org/abs/2601.03148)
*Alireza Maleki,Ebrahim Bedeer,Robert Barton*

Main category: eess.SP

TL;DR: 新的SE-LoRa+SIC检测器大幅提升LoRa频谱效率，误码率仅略逊于传统LoRa。


<details>
  <summary>Details</summary>
Motivation: 提升LoRa在高扩展因子（SF）下的频谱利用率，同时保持误码率可接受。

Method: 推导联合ML检测，随后利用去调频后SE-LoRa信号的频域特性设计低复杂度SIC检测器，计算复杂度与传统LoRa相当。

Result: 在SF=7、9、11时，频谱效率提升约445.45%、1011.11%、1071.88%；误码率在RSF信道上距传统LoRa不足3 dB。

Conclusion: SE-LoRa结合低复杂度SIC检测器显著提升LoRa频谱效率，并保持接近传统LoRa的误码性能。

Abstract: In this paper, we propose a spectral-efficient LoRa (SE-LoRa) modulation scheme with a low complexity successive interference cancellation (SIC)-based detector. The proposed communication scheme significantly improves the spectral efficiency of LoRa modulation, while achieving an acceptable error performance compared to conventional LoRa modulation, especially in higher spreading factor (SF) settings. We derive the joint maximum likelihood (ML) detection rule for the SE-LoRa transmission scheme that turns out to be of high computational complexity. To overcome this issue, and by exploiting the frequency-domain characteristics of the dechirped SE-LoRa signal, we propose a low complexity SIC-based detector with a computation complexity at the order of conventional LoRa detection. By computer simulations, we show that the proposed SE-LoRa with low complexity SIC-based detector can improve the spectral efficiency of LoRa modulation up to $445.45\%$, $1011.11\%$, and $1071.88\%$ for SF values of $7$, $9$, and $11$, respectively, while maintaining the error performance within less than $3$ dB of conventional LoRa at symbol error rate (SER) of $10^{-3}$ in Rician channel conditions.

</details>


### [56] [Inter-Year Transfer of Altitude-Dependent Spectrum Activity Models Using Minimal Calibration](https://arxiv.org/abs/2601.03234)
*Amir Hossein Fahim Raouf,İsmail Güvenc*

Main category: eess.SP

TL;DR: 降链/共享频段模型跨年可复用；上行频段则受益有限。


<details>
  <summary>Details</summary>
Motivation: 探索不同年份之间高度依赖频谱活动模型的可转移性，降低频谱测量与建模的成本。

Method: 构造平均场干扰的物理信息化随机图模型，提取线性视距改变斜率、过渡高程、有效活跃常数三参数，并基于两个高度区的最小校准方法重新估计。

Result: 在2023-2025年多子6 GHz波段测量验证中，降链和CBRS频段保持了一致的高度几何结构，可通过只用两高度区的测量在新年份准确预测；上行频段则不具备此特性。

Conclusion: 本文证明物理信息化的平均场模型在降链和共享频段中跨年可转移性良好，但上行频段由于传播几何对干扰影响弱，模型适用性受限。

Abstract: This paper studies the transferability of altitude-dependent spectrum activity models and measurements across years. We introduce a physics-informed, mean-only stochastic-geometry model of aggregate interference to altitude-binned received power, yielding three interpretable parameters for a given band and campaign: 1) line-of-sight transition slope, 2) transition altitude, and 3) effective activity constant. Analysis of aerial spectrum measurements collected from 2023 to 2025 across multiple sub-6 GHz bands reveals that downlink (DL) and shared-access bands preserve a persistent geometry-driven altitude structure that is stable across years. In contrast, uplink (UL) bands exhibit weak altitude dependence with no identifiable transition, indicating that interference is dominated by activity dynamics rather than propagation geometry. To quantify the practical limits of model reuse, we evaluate a minimal-calibration method in which the transition altitude is fixed from a reference year and the remaining parameters are estimated from only two altitude bins in the target year. The results further indicate that the proposed approach provides accurate predictions for DL and CBRS bands, suggesting the feasibility of low-cost model transfer in stable environments, while highlighting the reduced applicability of mean-field models for UL scenarios.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [57] [AMC26: VSSEA robust position control](https://arxiv.org/abs/2601.02557)
*Emre Sariyildiz*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents robust position control strategies for the novel VSSEA. By employing a constructed state-space model, two control schemes are developed in a unified framework: a state-feedback controller and a sliding mode controller, both integrated with a second-order DOb. The proposed framework achieves high-performance motion control by precisely estimating and compensating for internal and external disturbances, while preserving the nominal dynamic response. Simulation results demonstrate that pole-placement-based controllers are highly sensitive to disturbances, whereas LQR-based controllers offer improved robustness at the expense of slower dynamics. By incorporating DOb, robustness is significantly enhanced without degrading time response, and the LQR controller can be tuned solely for performance optimization. Experimental results confirm that the proposed robust position controllers can be implemented in real world applications. These results highlight the effectiveness of the proposed approach and lay the foundation for future investigations on robust stability and performance under different stiffness settings.

</details>


### [58] [AMC26: High-performance DOb for robust position control](https://arxiv.org/abs/2601.02560)
*Emre Sariyildiz*

Main category: eess.SY

TL;DR: 提出HPDOb，引入一阶截断误差动力学，在离散时域通过分析合成实现，更精确估计扰动，验证实验证明其表现在运动控制系统中优于传统观测器。


<details>
  <summary>Details</summary>
Motivation: 传统DObs仅考虑零阶截断误差，限制了扰动估计精度，亟需更精确的观测方法以提升控制系统鲁棒性。

Method: 在离散时域下对HPDOb进行分析与合成，并引入一阶截断误差动力学，构建高阶误差模型与观测器增益。

Result: 仿真与实验验证显示HPDOb在稳定性、估计精度和抗扰性能上均优于传统DObs。

Conclusion: 本论文提出的高阶截断误差动力学加权扰动观测器（HPDOb）显著提升了运动控制系统中的扰动估计精度与鲁棒性，优于传统观测器。

Abstract: This paper presents a new HPDOb that significantly improves disturbance estimation accuracy and robustness in motion control systems, surpassing the capabilities of conventional DObs. The proposed observer is analysed and synthesised in the discrete-time domain, providing a realistic representation of their dynamic behaviour and enabling enhanced controller design for practical applications. The core contribution of the HPDOb is a novel synthesis method that incorporates higher-order truncation error dynamics into disturbance estimation. Unlike conventional DObs, which are limited to zero-order truncation error, the HPDOb achieves first-order truncation error, yielding markedly improved estimation accuracy and robustness against disturbances in motion control systems. Simulation and experiments verify the stability and performance of HPDOb.

</details>


### [59] [AI Social Responsibility as Reachability: Execution-Level Semantics for the Social Responsibility Stack](https://arxiv.org/abs/2601.02585)
*Otman Basir*

Main category: eess.SY

TL;DR: 本论文将社会责任定义为系统执行的可达性属性，使用 Petri 网实现并验证 SRS——一种基于结构约束的可执行责任架构。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统虽在闭环环境中运行，但社会不可接受的结果往往源于重复、并发与反馈导致的累积执行轨迹，而非单一错误；迫切需要将责任嵌入系统执行层面，形成结构化约束。

Method: 通过正式化责任为可达性属性，利用Petri网作为执行层形式化工具，将SRS中的价值承诺映射为禁止标记，安全保障映射为结构约束，审计对应于可达性监测，治理对应于执行结构的合法修改。

Result: 证明了将SRS与Petri网结合即可内化责任为结构不变，并能通过可达性分析实现权限限制、监控和治理，从而构建可执行的责任架构。

Conclusion: 责任应被视为系统执行的可达性属性，而非仅仅的目标水平错误；通过在Petri网中将SRS定义为结构不变性，可实现可执行且可验证的社会责任体系。

Abstract: Artificial intelligence systems are increasingly embedded as persistent, closed-loop components within cyber-physical, social, and institutional processes. Rather than producing isolated outputs, such systems operate continuously under feedback, adaptation, and scale, reshaping physical flows, human behavior, and institutional practice over time. In these settings, socially unacceptable outcomes rarely arise from singular faults or explicit policy violations. Instead, they emerge through cumulative execution trajectories enabled by repetition, concurrency, and feedback.
  This paper advances the formal foundation of the Social Responsibility Stack (SRS) by making its central requirement explicit: responsibility is fundamentally a reachability property of system execution. A system is responsible iff its execution semantics prevent entry into inadmissible global configurations, regardless of local performance gains or optimization objectives. Responsibility failures are therefore not objective-level errors, but execution-level failures of trajectory control.
  To operationalize this perspective, we introduce Petri nets as an execution-level formalism for responsible autonomous systems. We show how SRS value commitments correspond to forbidden markings, safeguards to structural constraints on transition firing, auditing to monitoring of reachability pressure, and governance to legitimate modification of execution structure. Embedding Petri-net reachability within the SRS architecture internalizes responsibility as a structural invariant rather than an external objective or post-hoc mechanism.
  These results establish the Social Responsibility Stack as an executable responsibility architecture and position reachability-based execution semantics as a necessary foundation for responsible autonomy in feedback-rich cyber-physical and socio-technical systems.

</details>


### [60] [Distributionally Robust Game for Proof-of-Work Blockchain Mining Under Resource Uncertainties](https://arxiv.org/abs/2601.02804)
*Xunqiang Lan,Xiao Tang,Ruonan Zhang,Bin Li,Qinghe Du,Dusit Niyato,Zhu Han*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Blockchain plays a crucial role in ensuring the security and integrity of decentralized systems, with the proof-of-work (PoW) mechanism being fundamental for achieving distributed consensus. As PoW blockchains see broader adoption, an increasingly diverse set of miners with varying computing capabilities participate in the network. In this paper, we consider the PoW blockchain mining, where the miners are associated with resource uncertainties. To characterize the uncertainty computing resources at different mining participants, we establish an ambiguous set representing uncertainty of resource distributions. Then, the networked mining is formulated as a non-cooperative game, where distributionally robust performance is calculated for each individual miner to tackle the resource uncertainties. We prove the existence of the equilibrium of the distributionally robust mining game. To derive the equilibrium, we propose the conditional value-at-risk (CVaR)-based reinterpretation of the best response of each miner. We then solve the individual strategy with alternating optimization, which facilitates the iteration among miners towards the game equilibrium. Furthermore, we consider the case that the ambiguity of resource distribution reduces to Gaussian distribution and the case that another uncertainties vanish, and then characterize the properties of the equilibrium therein along with a distributed algorithm to achieve the equilibrium. Simulation results show that the proposed approaches effectively converge to the equilibrium, and effectively tackle the uncertainties in blockchain mining to achieve a robust performance guarantee.

</details>


### [61] [Site-Specific and Frequency-Dependent Channel Characterization and MIMO Performance in FR3](https://arxiv.org/abs/2601.02903)
*Zhuangzhuang Cui,Rudranil Chattopadhyay,Emiel Vanspranghels,Sofie Pollin*

Main category: eess.SY

TL;DR: FR3频段介于亚6 GHz与mmWave之间，射线追踪显示其支持有效空间复用和高频谱效率，尤其在大尺度天线布局下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 推动下一代无线系统实现按需连接，驱动利用动态频谱以满足多样化通信需求。

Method: 采用基于Sionna框架的场景特定射线追踪仿真，在代表性频点（3.5、7、10、14、20、24、28 GHz）下对室内外环境进行单天线与多天线配置分析。

Result: FR3在空传播特性上呈现亚6 GHz与毫米波之间的中间表现，空间复用保持有效；大数组分析显示性能提升与天线尺度紧密相关，强调大尺寸/大孔径MIMO的实用性。

Conclusion: 该论文指出FR3频段在传播特性与MIMO性能方面介于亚6 GHz与毫米波之间，并在大规模天线架构下可实现显著空间复用与频谱效率提升。

Abstract: Next-generation wireless systems aim to enable on-demand connectivity through dynamic spectrum utilization. Motivated by this vision, this paper investigates the propagation characteristics and MIMO performance of the upper mid-band, spanning approximately 7-24 GHz and unofficially referred to as FR3. Using site-specific ray-tracing (RT) simulations based on the Sionna framework, we analyze indoor and outdoor environments at representative frequencies across FR1, FR3, and FR2, including 3.5, 7, 10, 14, 20, 24, and 28 GHz, under both single-antenna and multi-antenna configurations. The results show that FR3 exhibits intermediate propagation behavior between sub-6 GHz and millimeter-wave bands while sustaining effective spatial multiplexing and favorable spectral efficiency. Furthermore, large-array analysis indicates that performance gains in FR3 are closely tied to antenna scaling, highlighting the importance of large-size or large-aperture MIMO architectures for practical deployments.

</details>


### [62] [Finite Memory Belief Approximation for Optimal Control in Partially Observable Markov Decision Processes](https://arxiv.org/abs/2601.03132)
*Mintae Kim*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study finite memory belief approximation for partially observable (PO) stochastic optimal control (SOC) problems. While belief states are sufficient for SOC in partially observable Markov decision processes (POMDPs), they are generally infinite-dimensional and impractical. We interpret truncated input-output (IO) histories as inducing a belief approximation and develop a metric-based theory that directly relates information loss to control performance. Using the Wasserstein metric, we derive policy-conditional performance bounds that quantify value degradation induced by finite memory along typical closed-loop trajectories. Our analysis proceeds via a fixed-policy comparison: we evaluate two cost functionals under the same closed-loop execution and isolate the effect of replacing the true belief by its finite memory approximation inside the belief-level cost. For linear quadratic Gaussian (LQG) systems, we provide closed-form belief mismatch evaluation and empirically validate the predicted mechanism, demonstrating that belief mismatch decays approximately exponentially with memory length and that the induced performance mismatch scales accordingly. Together, these results provide a metric-aware characterization of what finite memory belief approximation can and cannot achieve in PO settings.

</details>


### [63] [Closed-Loop Transmission Power Control for Reliable and Low-Power BLE Communication in Dynamic IoT Settings](https://arxiv.org/abs/2601.03003)
*Ziyao Zhou,Hen-Wei Huang*

Main category: eess.SY

TL;DR: 提出混合 RSSI-吞吐量反馈控制（基于 PID），在动态环境中保持高吞吐量且能效的闭环 TXP 调节方案，实验验证效果优于单一策略。


<details>
  <summary>Details</summary>
Motivation: BLE 的 RSSI 与吞吐量易受环境变化影响，导致传输质量下降，需要一种既能快速响应又能保证吞吐量的能效控制方案。

Method: 闭环 TXP 控制框架，先分别考察 RSSI 和吞吐量两个控制策略，随后引入混合策略将两者信号进行融合并通过 PID 调整 TXP。

Result: 实验表明混合策略在快速变化的环境下仍能保持吞吐量接近目标水平，方差最小，优于单一 RSSI 或吞吐量控制。

Conclusion: 基于 PID 的混合 RSSI-吞吐量反馈控制能在动态环境中稳定维持目标吞吐量，响应速度快且波动小。

Abstract: Reliable and energy-efficient Bluetooth Low Energy (BLE) communication is crucial for Internet of Things (IoT) applications in dynamic environments. However, the Received Signal Strength Indicator (RSSI) and data throughput in BLE are highly susceptible to environmental variability, which degrades communication performance. In this work, we systematically analyze the interdependence among RSSI, throughput, transmission power (TXP), and the peripheral device system power consumption under diverse real-world conditions. We observe that adjusting the TXP effectively influences both RSSI and throughput. We propose a robust closed-loop TXP control framework based on Proportional-Integral-Derivative (PID) controllers. Two initial control strategies are investigated: an RSSI-based approach and a throughput-based approach, each exhibiting distinct advantages and limitations. The RSSI-based method provides rapid responsiveness to signal fluctuations but lacks direct correlation with data throughput, whereas the throughput-based method offers more accurate feedback on effective throughput at the cost of slower response. To address these limitations, a hybrid RSSI-throughput control strategy is developed, combining the responsiveness of RSSI feedback with the accuracy of throughput measurements. Experimental results demonstrate that the proposed hybrid approach maintains data throughput close to the target level with minimal variance, even under rapidly changing environmental conditions.

</details>


### [64] [From inconsistency to decision: explainable operation and maintenance of battery energy storage systems](https://arxiv.org/abs/2601.03007)
*Jingbo Qu,Yijie Wang,Yujie Fu,Putai Zhang,Weihan Li,Mian Li*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Battery Energy Storage Systems (BESSs) are increasingly critical to power-system stability, yet their operation and maintenance remain dominated by reactive, expert-dependent diagnostics. While cell-level inconsistencies provide early warning signals of degradation and safety risks, the lack of scalable and interpretable decision-support frameworks prevents these signals from being effectively translated into operational actions. Here we introduce an inconsistency-driven operation and maintenance paradigm for large-scale BESSs that systematically transforms routine monitoring data into explainable, decision-oriented guidance. The proposed framework integrates multi-dimensional inconsistency evaluation with large language model-based semantic reasoning to bridge the gap between quantitative diagnostics and practical maintenance decisions. Using eight months of field data from an in-service battery system comprising 3,564 cells, we demonstrate how electrical, thermal, and aging-related inconsistencies can be distilled into structured operational records and converted into actionable maintenance insights through a multi-agent framework. The proposed approach enables accurate and explainable responses to real-world operation and maintenance queries, reducing response time and operational cost by over 80% compared with conventional expert-driven practices. These results establish a scalable pathway for intelligent operation and maintenance of battery energy storage systems, with direct implications for reliability, safety, and cost-effective integration of energy storage into modern power systems.

</details>


### [65] [Time-Varying Kinematics Control for Magnetically-Actuated Satellite Swarm without Additional Actuator](https://arxiv.org/abs/2601.03143)
*Yuta Takahashi,Hiraku Sakamoto,Shin-ichiro Sakai*

Main category: eess.SY

TL;DR: 提出了一种利用角动量守恒的电磁控制器，在无需姿态执行器的条件下实现多卫星的形成飞行控制，并通过仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统燃料推进在微卫星群控中受限，需开发无燃料的可控技术；电磁作用具备零排放和高精度的优势。

Method: 基于全系统角动量守恒的非完整约束，设计了新型电磁力/力矩控制器，并在控制律中不依赖传统姿态执行器。

Result: 通过理论分析与仿真验证，所述控制器能在不使用额外姿态执行器的前提下，实现多卫星的控制和姿态同步，提升形成飞行的可控性。

Conclusion: 本文提出了在无额外姿态执行器条件下，利用全系统角动量守恒约束设计的电磁控制方案，实现了多卫星的可控形成飞行。

Abstract: Electromagnetic Formation Flight is a technology that uses electromagnetic forces and torques to control multiple satellites without conventional fuel-based propulsion. In this paper, the controllability of the system is discussed based on the conservation of the entire system's angular momentum, which constitutes a nonholonomic constraint. This paper designs a new controller for multiple satellites without an additional attitude actuator.

</details>


### [66] [Conditioning Aircraft Trajectory Prediction on Meteorological Data with a Physics-Informed Machine Learning Approach](https://arxiv.org/abs/2601.03152)
*Amy Hodgkin,Nick Pepper,Marc Thomas*

Main category: eess.SY

TL;DR: 运用机器学习估计推力与空速，结合BADA能量约束，提升20%轨迹预测性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测航空轨迹受到多来源不确定性的干扰，尤其是气象条件和操作者特定操作，需要利用概率化学习模型生成轨迹，但此类模型若产生不符合物理约束的轨迹，其可信度受限。

Method: 提出物理信息化方法：通过数据学习机动推力与空速，条件化基于物理的BADA模型，后者在生成轨迹时施加能量约束。挑选一组信息特征，用于构建推力和空速的概率模型。

Result: 在六项评估指标上，相较于忽略气象等上下文信息的基准概率模型，所提方法提升了20%的准确率。

Conclusion: 将数据驱动的推力/空速预测与物理约束相结合，可显著提高航空轨迹预测的可信度与精度。

Abstract: Accurate aircraft trajectory prediction (TP) in air traffic management systems is confounded by a number of epistemic uncertainties, dominated by uncertain meteorological conditions and operator specific procedures. Handling this uncertainty necessitates the use of probabilistic, machine learned models for generating trajectories. However, the trustworthiness of such models is limited if generated trajectories are not physically plausible. For this reason we propose a physics-informed approach in which aircraft thrust and airspeed are learned from data and are used to condition the existing Base of Aircraft Data (BADA) model, which is physics-based and enforces energy-based constraints on generated trajectories. A set of informative features are identified and used to condition a probabilistic model of aircraft thrust and airspeed, with the proposed scheme demonstrating a 20% improvement in skilfulness across a set of six metrics, compared against a baseline probabilistic model that ignores contextual information such as meteorological conditions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [67] [How to Discover Knowledge for FutureG: Contextual RAG and LLM Prompting for O-RAN](https://arxiv.org/abs/2601.02382)
*Nathan Conger,Nathan Scollar,Kemal Davaslioglu,Yalin E. Sagduyu,Sastry Kompella*

Main category: cs.NI

TL;DR: Contextual RAG 通过答案引导检索、增强上下文，无需微调即可提升 O‑RAN 与 5G/6G 问答准确率，并保持效率与可持续性。


<details>
  <summary>Details</summary>
Motivation: O‑RAN 规范和接口快速演化，手工浏览漫长文档成本高且易出错，阻碍系统设计与部署。

Method: 利用候选答案引导文档检索，并为 LLM 提供片段级上下文的 Contextual RAG 方法，无需 LLM 微调即可在快速演变的技术域中持续更新并保持性能。

Result: 在 ORANBenchmark‑13K 数据集上，三种 LLM（Llama3.2、Qwen2.5‑7B、Qwen3.0‑4B）在 Direct Q&A 与 CoT 提示下，Contextual RAG 一直优于标准 RAG 与基础提示，且运行时与 CO2 排放保持竞争力。

Conclusion: Contextual RAG 为 O-RAN 与 5G/6G 环境提供了可扩展、高效且可持续的领域问答方案，显著提升了准确率并保持了合理的运行时和碳排放水平。

Abstract: We present a retrieval-augmented question answering framework for 5G/6G networks, where the Open Radio Access Network (O-RAN) has become central to disaggregated, virtualized, and AI-driven wireless systems. While O-RAN enables multi-vendor interoperability and cloud-native deployments, its fast-changing specifications and interfaces pose major challenges for researchers and practitioners. Manual navigation of these complex documents is labor-intensive and error-prone, slowing system design, integration, and deployment. To address this challenge, we adopt Contextual Retrieval-Augmented Generation (Contextual RAG), a strategy in which candidate answer choices guide document retrieval and chunk-specific context to improve large language model (LLM) performance. This improvement over traditional RAG achieves more targeted and context-aware retrieval, which improves the relevance of documents passed to the LLM, particularly when the query alone lacks sufficient context for accurate grounding. Our framework is designed for dynamic domains where data evolves rapidly and models must be continuously updated or redeployed, all without requiring LLM fine-tuning. We evaluate this framework using the ORANBenchmark-13K dataset, and compare three LLMs, namely, Llama3.2, Qwen2.5-7B, and Qwen3.0-4B, across both Direct Question Answering (Direct Q&A) and Chain-of-Thought (CoT) prompting strategies. We show that Contextual RAG consistently improves accuracy over standard RAG and base prompting, while maintaining competitive runtime and CO2 emissions. These results highlight the potential of Contextual RAG to serve as a scalable and effective solution for domain-specific Q&A in ORAN and broader 5G/6G environments, enabling more accurate interpretation of evolving standards while preserving efficiency and sustainability.

</details>


### [68] [Regional Resource Management for Service Provisioning in LEO Satellite Networks: A Topology Feature-Based DRL Approach](https://arxiv.org/abs/2601.02387)
*Chenxi Bao,Di Zhou,Min Sheng,Yan Shi,Jiandong Li,Zhili Sun*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Satellite networks with wide coverage are considered natural extensions to terrestrial networks for their long-distance end-to-end (E2E) service provisioning. However, the inherent topology dynamics of low earth orbit satellite networks and the uncertain network scales bring an inevitable requirement that resource chains for E2E service provisioning must be efficiently re-planned. Therefore, achieving highly adaptive resource management is of great significance in practical deployment applications. This paper first designs a regional resource management (RRM) mode and further formulates the RRM problem that can provide a unified decision space independent of the network scale. Subsequently, leveraging the RRM mode and deep reinforcement learning framework, we develop a topology feature-based dynamic and adaptive resource management algorithm to combat the varying network scales. The proposed algorithm successfully takes into account the fixed output dimension of the neural network and the changing resource chains for E2E service provisioning. The matched design of the service orientation information and phased reward function effectively improves the service performance of the algorithm under the RRM mode. The numerical results demonstrate that the proposed algorithm with the best convergence performance and fastest convergence rate significantly improves service performance for varying network scales, with gains over compared algorithms of more than 2.7%, 11.9%, and 10.2%, respectively.

</details>


### [69] [Eco-WakeLoc: An Energy-Neutral and Cooperative UWB Real-Time Locating System](https://arxiv.org/abs/2601.03171)
*Silvano Cortesi,Lukas Schulthess,Davide Plozza,Christian Vogt,Michele Magno*

Main category: cs.NI

TL;DR: Eco‑WakeLoc 使用唤醒无线电与光能收集，仅激活锚点实现低功耗厘米级UWB定位，实验表明能耗极低、精度达43 cm，并可在一年内保持能量中性。


<details>
  <summary>Details</summary>
Motivation: 在GPS不可用的室内环境下，移动机器人需要实时且能耗低的定位系统，传统RTLS要么持续供电影响可扩展性，要么响应慢。

Method: Eco‑WakeLoc 采用低功耗唤醒无线电与光能收集，仅按需激活锚点，实现厘米级UWB定位；主动标签发起三角测距，待机标签利用同一消息做TDOA；通过 AIMD 能耗调度根据收集能量动态调整定位速率。

Result: 实验验证：主动标签单次定位耗能3.22 mJ、被动标签951 µJ、锚点353 µJ；在一个四足机器人上部署九个锚点，平均定位精度43 cm；一年模拟后每日平均2031次定位，电池余能超过7%。

Conclusion: Eco‑WakeLoc 通过能耗中性、协同定位与自适应调度，在不持续开启基础设施的前提下，能够大规模实现厘米级室内定位。

Abstract: Indoor localization systems face a fundamental trade-off between efficiency and responsiveness, which is especially important for emerging use cases such as mobile robots operating in GPS-denied environments. Traditional RTLS either require continuously powered infrastructure, limiting their scalability, or are limited by their responsiveness. This work presents Eco-WakeLoc, designed to achieve centimeter-level UWB localization while remaining energy-neutral by combining ultra-low power wake-up radios (WuRs) with solar energy harvesting. By activating anchor nodes only on demand, the proposed system eliminates constant energy consumption while achieving centimeter-level positioning accuracy. To reduce coordination overhead and improve scalability, Eco-WakeLoc employs cooperative localization where active tags initiate ranging exchanges (trilateration), while passive tags opportunistically reuse these messages for TDOA positioning. An additive-increase/multiplicative-decrease (AIMD)-based energy-aware scheduler adapts localization rates according to the harvested energy, thereby maximizing the overall performance of the sensor network while ensuring long-term energy neutrality. The measured energy consumption is only 3.22mJ per localization for active tags, 951uJ for passive tags, and 353uJ for anchors. Real-world deployment on a quadruped robot with nine anchors confirms the practical feasibility, achieving an average accuracy of 43cm in dynamic indoor environments. Year-long simulations show that tags achieve an average of 2031 localizations per day, retaining over 7% battery capacity after one year -- demonstrating that the RTLS achieves sustained energy-neutral operation. Eco-WakeLoc demonstrates that high-accuracy indoor localization can be achieved at scale without continuous infrastructure operation, combining energy neutrality, cooperative positioning, and adaptive scheduling.

</details>


### [70] [Probabilistic Time Slot Leasing in TDMA-Based IoT Networks for Enhanced Channel Utilization](https://arxiv.org/abs/2601.02930)
*Hicham Lakhlef,Mohamed Ali Zormati,Khaled Abid,Toufik Ahmed*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In large-scale resource-constrained wireless networks, such as those prevalent in the Internet of Things (IoT), efficient communication scheduling remains a critical challenge. Among the various approaches, Time Division Multiple Access (TDMA) protocols have been widely adopted for their structured and collision-free communication capabilities. Nevertheless, despite extensive research in this area, current solutions often exhibit suboptimal performance, particularly in dynamic environments where node activity levels fluctuate over time.
  This paper introduces a novel fully distributed TDMA-based scheduling protocol that intelligently maximizes the utilization of communication resources. The proposed approach adaptively reallocates underutilized time slots, originally assigned to temporarily inactive nodes, to those experiencing higher communication demands. This dynamic reallocation not only improves channel utilization but also reduces idle periods, thereby enhancing overall network efficiency. To further enhance performance, we incorporate a lightweight probabilistic mechanism that governs the temporal leasing of unused slots. This mechanism balances the trade-off between slot availability and transmission reliability, minimizing packet loss while preserving fairness and stability within the network.
  Simulations across a range of network scenarios demonstrate that our protocol significantly improves throughput, latency, and reliability in resource-constrained environments. These results highlight the protocol's potential as a robust and scalable solution for adaptive and energy-efficient scheduling in next-generation IoT networks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [APoW: Auditable Proof-of-Work Against Block Withholding Attacks](https://arxiv.org/abs/2601.02496)
*Sergio Demian Lerner*

Main category: cs.CR

TL;DR: APoW通过在Nonce空间记录可再扫描区域，使矿池内部可追踪验证工作量，从而降低区块扣留攻击风险，无需信任硬件或第三方，可实际在当前矿池实现。


<details>
  <summary>Details</summary>
Motivation: 解决矿池内部矿工偷窃区块和上报假工作问题，避免对可信硬件/第三方的依赖。

Method: 在Hashcash式随机数搜索基础上加入可再扫描区域记录，实现工作证明可追溯。

Result: 提出APoW方案可证实矿工已对指定nonce区域工作，支持去中心化矿池核查工作并可通过现有矿池储备实现审核付费。

Conclusion: APoW为区块链PoW提供了可审核的工作证明机制，提升矿池透明度并降低区块扣留攻击。

Abstract: We introduce APoW, a novel proof-of-work (PoW) construction inspired by Hashcash-style nonce searching, which enables the auditing of other miners' work through accountable re-scanning of the nonce space. The proposed scheme allows a miner to probabilistically attest to having searched specified regions of the nonce space in earlier mining rounds, while concurrently earning rewards for performing productive work for a new block or pool share. This capability enables miners belonging to a mining pools to audit another miner's claimed effort retroactively, thereby allowing the probabilistic detection of block withholding attacks (BWAs) without requiring trusted hardware or trusted third parties. As a consequence, the construction supports the design of decentralized mining pools in which work attribution is verifiable and withholding incentives are substantially reduced. The scheme preserves the fundamental properties of conventional PoW, including public verifiability and difficulty adjustment, while adding an orthogonal auditability layer tailored to pool-based mining. Finally, while a full deployment of APoW in Bitcoin would require a consensus rule change and minor modifications to mining ASICs, the construction remains practically useful even without consensus changes, for instance, as a pool-level auditing mechanism that enables verifiable pay-for-auditing using existing pool reserves.

</details>


### [72] [SWaRL: Safeguard Code Watermarking via Reinforcement Learning](https://arxiv.org/abs/2601.02602)
*Neusha Javidnia,Ruisi Zhang,Ashish Kundu,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: SWaRL采用强化学习+编译器反馈+LoRA技术，实现高准确度、功能完整、可迁移且对抗性强的代码LLM水印方案。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法要么依赖人工规则导致功能损坏，要么在推理时改动生成概率，容易引发编译错误，缺乏鲁棒性与可迁移性。

Method: 利用强化学习训练联合的编译器反馈与保密验证器作为奖励信号，同时在微调时使用LoRA使水印信息可迁移。

Result: 实验表明SWaRL在水印检测准确率上优于先前方法，并完全保持被水印代码的功能完整，同时在重构和对抗变形攻击下表现出强鲁棒性。

Conclusion: SWaRL能够在不破坏代码功能的前提下，实现更高效、稳健且可转移的水印注入与检测，且对重构与对抗攻击具备较强韧性。

Abstract: We present SWaRL, a robust and fidelity-preserving watermarking framework designed to protect the intellectual property of code LLM owners by embedding unique and verifiable signatures in the generated output. Existing approaches rely on manually crafted transformation rules to preserve watermarked code functionality or manipulate token-generation probabilities at inference time, which are prone to compilation errors. To address these challenges, SWaRL employs a reinforcement learning-based co-training framework that uses compiler feedback for functional correctness and a jointly trained confidential verifier as a reward signal to maintain watermark detectability. Furthermore, SWaRL employs low-rank adaptation (LoRA) during fine-tuning, allowing the learned watermark information to be transferable across model updates. Extensive experiments show that SWaRL achieves higher watermark detection accuracy compared to prior methods while fully maintaining watermarked code functionality. The LoRA-based signature embedding steers the base model to generate and solve code in a watermark-specific manner without significant computational overhead. Moreover, SWaRL exhibits strong resilience against refactoring and adversarial transformation attacks.

</details>


### [73] [LAsset: An LLM-assisted Security Asset Identification Framework for System-on-Chip (SoC) Verification](https://arxiv.org/abs/2601.02624)
*Md Ajoad Hasan,Dipayan Saha,Khan Thamid Hasan,Nashmin Alam,Azim Uddin,Sujan Kumar Saha,Mark Tehranipoor,Farimah Farahmandi*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The growing complexity of modern system-on-chip (SoC) and IP designs is making security assurance difficult day by day. One of the fundamental steps in the pre-silicon security verification of a hardware design is the identification of security assets, as it substantially influences downstream security verification tasks, such as threat modeling, security property generation, and vulnerability detection. Traditionally, assets are determined manually by security experts, requiring significant time and expertise. To address this challenge, we present LAsset, a novel automated framework that leverages large language models (LLMs) to identify security assets from both hardware design specifications and register-transfer level (RTL) descriptions. The framework performs structural and semantic analysis to identify intra-module primary and secondary assets and derives inter-module relationships to systematically characterize security dependencies at the design level. Experimental results show that the proposed framework achieves high classification accuracy, reaching up to 90% recall rate in SoC design, and 93% recall rate in IP designs. This automation in asset identification significantly reduces manual overhead and supports a scalable path forward for secure hardware development.

</details>


### [74] [Adversarial Contrastive Learning for LLM Quantization Attacks](https://arxiv.org/abs/2601.02680)
*Dinghong Song,Zhiwei Xu,Hai Wan,Xibin Zhao,Pengfei Su,Dong Li*

Main category: cs.CR

TL;DR: ACL是一种梯度对比学习量化攻击方法，利用三元组损失最大化善意与恶意回应概率差距，通过两阶段投影梯度下降分布式微调，实验显示其在过度拒绝、破解禁令和广告注入等任务中大幅提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 在对资源受限硬件部署大型语言模型时，量化是必要手段，但近期发现全精度模型在量化后会出现恶意行为，迫切需要揭示与对抗此类安全风险的攻击手段。

Method: 基于梯度的量化攻击——Adversarial Contrastive Learning（ACL）使用三元组对比损失，将善意与有害输出的概率差距最大化，并结合投影梯度下降的两阶段分布式微调策略，稳定高效地优化攻击目标。

Result: ACL在三类攻击任务中的成功率分别达到86.00%（过度拒绝）、97.69%（破解禁令）、92.40%（广告注入），在最大提升度上分别比现有方法高出44.67%、18.84%和50.80%，证明其显著优越性。

Conclusion: ACL展现了量化过程中存在的安全隐患，利用对比学习的梯度攻击在各种恶意任务上均显著提升攻击成功率，提示在部署LLM时需谨慎处理量化策略与安全防护。

Abstract: Model quantization is critical for deploying large language models (LLMs) on resource-constrained hardware, yet recent work has revealed severe security risks that benign LLMs in full precision may exhibit malicious behaviors after quantization. In this paper, we propose Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that achieves superior attack effectiveness by explicitly maximizing the gap between benign and harmful responses probabilities. ACL formulates the attack objective as a triplet-based contrastive loss, and integrates it with a projected gradient descent two-stage distributed fine-tuning strategy to ensure stable and efficient optimization. Extensive experiments demonstrate ACL's remarkable effectiveness, achieving attack success rates of 86.00% for over-refusal, 97.69% for jailbreak, and 92.40% for advertisement injection, substantially outperforming state-of-the-art methods by up to 44.67%, 18.84%, and 50.80%, respectively.

</details>


### [75] [Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System](https://arxiv.org/abs/2601.02720)
*Yuqiao Xu,Mina Namazi,Sahith Reddy Jalapally,Osama Zafar,Youngjin Yoo,Erman Ayday*

Main category: cs.CR

TL;DR: 本研究提出一种基于TEE的区块链LER，自动从结构化与非结构化学习材料中抽取隐私安全的技能凭证，实现可信学历验证与无偏职业匹配。


<details>
  <summary>Details</summary>
Motivation: 传统区块链学习与就业记录系统缺乏自动化技能凭证生成及对非结构化学习证据的处理，难以满足完整可信的教育与职业匹配需求。

Method: 采用隐私保护的AI驱动去中心化LER，利用可信执行环境TEE内进行自然语言处理，将正式记录与非正式证据转换为可验证的自签名技能凭证；所有验证与职位匹配亦在隔离区完成，保证凭证与私钥安全。

Result: 在样本学习者数据上进行评估，映射遵循经验证的课程到O*NET方法，重复实验稳定性<5%；通过形式化安全说明和证明，证明凭证不可伪造且敏感信息保持机密。

Conclusion: 该系统实现了安全的教育与就业凭证化、稳健的成绩单验证以及自动化、隐私保护的技能提取，为去中心化的凭证生态提供了完整闭环。

Abstract: Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.

</details>


### [76] [Quality Degradation Attack in Synthetic Data](https://arxiv.org/abs/2601.02947)
*Qinyi Liu,Dong Liu,Farhad Vadiee,Mohammad Khalil,Pedro P. Vergara Barrios*

Main category: cs.CR

TL;DR: 研究发现，攻击者可通过在真实数据上
做细微干预来降低合成数据的质量，提示需加\n强完整性与鲁棒性措施。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注由接收方对合成数据进行隐私攻击，\n但忽略了数据所有者或生成方本身可能
对真实数据或生成流程实施恶意干扰，从而破坏\n生成数据的质量与可用性。

Method: 构建威胁模型，针对拥有真实数据或可控生成流程的攻击者，\u0040\u003C正确边界\u003E \n并在实验中通过标签翻转、基于特征重要性的干预等方式对原始数据\n进行细微扰动，评估对生成数据质量的影响。

Result: 实验结果表明，哪\u003C小幅扰动\u003E 即可显著降低生成数据的预测性能，\n提升统计差异性，揭示SDG流程的脆弱点。

Conclusion: 本研究指出合成数据生成存在主动干扰脆弱性，强调
需要除隐私保护外还要加入完整性与鲁棒性校验，确保
生成数据的可靠性与可信度。

Abstract: Synthetic Data Generation (SDG) can be used to facilitate privacy-preserving data sharing. However, most existing research focuses on privacy attacks where the adversary is the recipient of the released synthetic data and attempts to infer sensitive information from it. This study investigates quality degradation attacks initiated by adversaries who possess access to the real dataset or control over the generation process, such as the data owner, the synthetic data provider, or potential intruders. We formalize a corresponding threat model and empirically evaluate the effectiveness of targeted manipulations of real data (e.g., label flipping and feature-importance-based interventions) on the quality of generated synthetic data. The results show that even small perturbations can substantially reduce downstream predictive performance and increase statistical divergence, exposing vulnerabilities within SDG pipelines. This study highlights the need to integrate integrity verification and robustness mechanisms, alongside privacy protection, to ensure the reliability and trustworthiness of synthetic data sharing frameworks.

</details>


### [77] [Exploring Blockchain Interoperability: Frameworks, Use Cases, and Future Challenges](https://arxiv.org/abs/2601.02949)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ellis Solaiman,Omer Rana,Rajiv Ranjan*

Main category: cs.CR

TL;DR: 文章讨论了区块链互操作性的重要性，评估了现有平台并通过案例展示其优势，同时指出了仍需解决的技术难题。


<details>
  <summary>Details</summary>
Motivation: 在缺乏可信第三方的情况下，区块链旨在构建信任体系；随着区块链应用扩增，单链信息共享受限，迫切需要跨链互操作。

Method: 通过对多种区块链平台的互操作性特性进行对比讨论，并结合案例场景阐释其应用价值。

Result: 评估了几种主流平台的互操作性能，展示了案例中提升效率与信任的具体效果，并提出了未来研究方向。

Conclusion: 本论文概述了区块链互操作性的必要性与现有解决方案，强调跨异构链互联的可行性，并指出仍需解决的关键技术挑战。

Abstract: Trust between entities in any scenario without a trusted third party is very difficult, and trust is exactly what blockchain aims to bring into the digital world with its basic features. Many applications are moving to blockchain adoption, enabling users to work in a trustworthy manner. The early generations of blockchain have a problem; they cannot share information with other blockchains. As more and more entities move their applications to the blockchain, they generate large volumes of data, and as applications have become more complex, sharing information between different blockchains has become a necessity. This has led to the research and development of interoperable solutions allowing blockchains to connect together. This paper discusses a few blockchain platforms that provide interoperable solutions, emphasising their ability to connect heterogeneous blockchains. It also discusses a case study scenario to illustrate the importance and benefits of using interoperable solutions. We also present a few topics that need to be solved in the realm of interoperability.

</details>


### [78] [Developing and Evaluating Lightweight Cryptographic Algorithms for Secure Embedded Systems in IoT Devices](https://arxiv.org/abs/2601.02981)
*Brahim Khalil Sedraoui,Abdelmadjid Benmachiche,Amina Makhlouf*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The high rate of development of Internet of Things (IoT) devices has brought to attention new challenges in the area of data security, especially within the resource-limited realm of RFID tags, sensors, and embedded systems. Traditional cryptographic implementations can be of inappropriate computational complexity and energy usage and hence are not suitable on these platforms. This paper examines the design, implementation, and testing of lightweight cryptographic algorithms that have been specifically designed to be used in secure embedded systems. A comparison of some of the state-of-the-art lightweight encryption algorithms, that is PRESENT, SPECK, and SIMON, focuses on the main performance indicators, i.e., throughput, use of memory, and energy utilization. The study presents novel lightweight algorithms that are founded upon the Feistel-network architecture and their safety under cryptanalytic attacks, e.g., differential and linear cryptanalysis. The proposed solutions are proven through hardware implementation on the FPGA platform. The results have shown that lightweight cryptography is an effective strategy that could be used to establish security and maintain performance in the IoT and other resource-limited settings.

</details>


### [79] [Selfish Mining in Multi-Attacker Scenarios: An Empirical Evaluation of Nakamoto, Fruitchain, and Strongchain](https://arxiv.org/abs/2601.02984)
*Martin Perešíni,Tomáš Hladký,Jakub Kubík,Ivan Homoliak*

Main category: cs.CR

TL;DR: 开发随机模拟框架，研究多攻击者自私挖矿，验证并发现阈值，公开源码。


<details>
  <summary>Details</summary>
Motivation: 深化对多攻击者自私挖矿的理解，并填补现有研究仅关注单一攻击者的空白。

Method: 构建PoW纳姆克托克基本模型，并设计Fruitchain与Strongchain等对抗自私挖矿的共识模型，在此框架下进行多攻击者随机模拟，并对阈值进行验证与发现。

Result: 验证已知阈值，发现2位及以上攻击者的新阈值，并公开源码供其他研究者使用。

Conclusion: 通过构建基于随机模拟的多攻击者自私挖矿分析框架，本文不仅验证了已发表阈值，还发现了两名及以上攻击者的新阈值，促进了对抗自私挖矿的共识协议研究与发展。

Abstract: The aim of this work is to enhance blockchain security by deepening the understanding of selfish mining attacks in various consensus protocols, especially the ones that have the potential to mitigate selfish mining. Previous research was mainly focused on a particular protocol with a single selfish miner, while only limited studies have been conducted on two or more attackers. To address this gap, we proposed a stochastic simulation framework that enables analysis of selfish mining with multiple attackers across various consensus protocols. We created the model of Proof-of-Work (PoW) Nakamoto consensus (serving as the baseline) as well as models of two additional consensus protocols designed to mitigate selfish mining: Fruitchain and Strongchain. Using our framework, thresholds reported in the literature were verified, and several novel thresholds were discovered for 2 and more attackers. We made the source code of our framework available, enabling researchers to evaluate any newly added protocol with one or more selfish miners and cross-compare it with already modeled protocols.

</details>


### [80] [JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification](https://arxiv.org/abs/2601.03005)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Zhaoye Li,Bin Ji,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: 论文指出jailbreak攻击利用未擦除参数形成动态路径，提出JPU通过对抗样本周期性修正这些路径，从而提升LLM的安全性并保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽已加强安全性，却仍易遭遇jailbreak攻击。当前的机器忘却方法对多样化jailbreak脆弱。该研究通过实证发现攻击实质是激活中间层未被清除参数，且这些参数可动态重组构成禁用输出，揭示存在动态jailbreak路径。

Method: 首次设计Jailbreak Path Unlearning (JPU)。通过动态挖掘在策略（on-policy）下的对抗样本暴露安全弱点，然后定位并纠正动态jailbreak路径，使模型回归安全锚点。

Result: 实验表明，JPU显著提升了对动态攻击的抵抗力，同时保持了模型的原始功能效能。

Conclusion: 对抗动态jailbreak的核心在于及时检测并修正动态路径，JPU实现了此目标，弥补了先前机器忘却缺陷。

Abstract: Despite extensive safety alignment, Large Language Models (LLMs) often fail against jailbreak attacks. While machine unlearning has emerged as a promising defense by erasing specific harmful parameters, current methods remain vulnerable to diverse jailbreaks. We first conduct an empirical study and discover that this failure mechanism is caused by jailbreaks primarily activating non-erased parameters in the intermediate layers. Further, by probing the underlying mechanism through which these circumvented parameters reassemble into the prohibited output, we verify the persistent existence of dynamic $\textbf{jailbreak paths}$ and show that the inability to rectify them constitutes the fundamental gap in existing unlearning defenses. To bridge this gap, we propose $\textbf{J}$ailbreak $\textbf{P}$ath $\textbf{U}$nlearning (JPU), which is the first to rectify dynamic jailbreak paths towards safety anchors by dynamically mining on-policy adversarial samples to expose vulnerabilities and identify jailbreak paths. Extensive experiments demonstrate that JPU significantly enhances jailbreak resistance against dynamic attacks while preserving the model's utility.

</details>


### [81] [LLMs, You Can Evaluate It! Design of Multi-perspective Report Evaluation for Security Operation Centers](https://arxiv.org/abs/2601.03013)
*Hiroyuki Okada,Tatsumi Oba,Naoto Yanai*

Main category: cs.CR

TL;DR: 本文提出MESSALA框架，结合CHECKLIST、granularization guideline和multi‑perspective evaluation，显著提升LLM在SOC分析报告评估上的表现。


<details>
  <summary>Details</summary>
Motivation: 理解资深分析师在评估SOC分析报告时的标准及反馈，旨在提升LLM生成报告的质量与可用性。

Method: 通过构建Analyst-wise checklist并引入granularization guideline与multi-perspective evaluation技术，设计了MESSALA框架，并在大量实验中与现有LLM方法比较。

Result: 实验表明，MESSALA在评估指标上与资深SOC分析师最为接近，并能提供改进报告的可操作性建议。

Conclusion: Mellisa框架在评估SOC分析报告时，能够产生与资深分析师评价最为接近的结果，从而证明该框架在实际应用中的有效性。

Abstract: Security operation centers (SOCs) often produce analysis reports on security incidents, and large language models (LLMs) will likely be used for this task in the near future. We postulate that a better understanding of how veteran analysts evaluate reports, including their feedback, can help produce analysis reports in SOCs. In this paper, we aim to leverage LLMs for analysis reports. To this end, we first construct a Analyst-wise checklist to reflect SOC practitioners' opinions for analysis report evaluation through literature review and user study with SOC practitioners. Next, we design a novel LLM-based conceptual framework, named MESSALA, by further introducing two new techniques, granularization guideline and multi-perspective evaluation. MESSALA can maximize report evaluation and provide feedback on veteran SOC practitioners' perceptions. When we conduct extensive experiments with MESSALA, the evaluation results by MESSALA are the closest to those of veteran SOC practitioners compared with the existing LLM-based methods. We then show two key insights. We also conduct qualitative analysis with MESSALA, and then identify that MESSALA can provide actionable items that are necessary for improving analysis reports.

</details>


### [82] [FlexProofs: A Vector Commitment with Flexible Linear Time for Computing All Proofs](https://arxiv.org/abs/2601.03031)
*Jing Liu,Liang Feng Zhang*

Main category: cs.CR

TL;DR: FlexProofs是一个高效可扩展的向量承诺方案，以O(N)时间生成所有开启证明并支持可调批处理参数；实验表明其比HydraProofs快6倍，兼容多线性多项式zkSNARK，推动可验证秘密共享与鲁棒聚合等应用。


<details>
  <summary>Details</summary>
Motivation: 现有向量承诺如HydraProofs已实现O(N)时间，但对批处理支持有限；缺少高效且兼容具备多线性多项式输入的zkSNARK的方案；提升证明生成速度与灵活性是实现可扩展可验证应用的关键需求。

Method: 首先提出了首个支持批开启的多指数功能承诺（FC）方案；然后基于此构造FlexProofs向量承诺，利用可调的批大小参数b来重构证明生成流程，使验证效率维持O(N)的同时降低常数。

Result: 在实验中，N=2^16且b=log^2 N时，FlexProofs比HydraProofs快约6倍；与相应zkSNARK结合后，可实现可验证秘密分发与鲁棒聚合的实用部署。

Conclusion: FlexProofs提供了一种新型向量承诺方案，能够以最优时间生成个体开启证明，并支持可调批处理参数，显著加快证明生成速度；同时与多线性多项式类zkSNARKs兼容，拓展了可验证秘密共享和鲁棒聚合等应用。

Abstract: In this paper, we introduce FlexProofs, a new vector commitment (VC) scheme that achieves two key properties: (1) the prover can generate all individual opening proofs for a vector of size $N$ in optimal time ${\cal O}(N)$, and there is a flexible batch size parameter $b$ that can be increased to further reduce the time to generate all proofs; and (2) the scheme is directly compatible with a family of zkSNARKs that encode their input as a multi-linear polynomial. As a critical building block, we propose the first functional commitment (FC) scheme for multi-exponentiations with batch opening. Compared with HydraProofs, the only existing VC scheme that computes all proofs in optimal time ${\cal O}(N)$ and is directly compatible with zkSNARKs, FlexProofs may speed up the process of generating all proofs, if the parameter $b$ is properly chosen. Our experiments show that for $N=2^{16}$ and $b=\log^2 N$, FlexProofs can be $6\times$ faster than HydraProofs. Moreover, when combined with suitable zkSNARKs, FlexProofs enable practical applications such as verifiable secret sharing and verifiable robust aggregation.

</details>


### [83] [SLIM: Stealthy Low-Coverage Black-Box Watermarking via Latent-Space Confusion Zones](https://arxiv.org/abs/2601.03242)
*Hengyu Wu,Yang Cao*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Training data is a critical and often proprietary asset in Large Language Model (LLM) development, motivating the use of data watermarking to embed model-transferable signals for usage verification. We identify low coverage as a vital yet largely overlooked requirement for practicality, as individual data owners typically contribute only a minute fraction of massive training corpora. Prior methods fail to maintain stealthiness, verification feasibility, or robustness when only one or a few sequences can be modified. To address these limitations, we introduce SLIM, a framework enabling per-user data provenance verification under strict black-box access. SLIM leverages intrinsic LLM properties to induce a Latent-Space Confusion Zone by training the model to map semantically similar prefixes to divergent continuations. This manifests as localized generation instability, which can be reliably detected via hypothesis testing. Experiments demonstrate that SLIM achieves ultra-low coverage capability, strong black-box verification performance, and great scalability while preserving both stealthiness and model utility, offering a robust solution for protecting training data in modern LLM pipelines.

</details>
