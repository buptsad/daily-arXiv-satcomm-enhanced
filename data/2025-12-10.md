<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 61]
- [eess.SP](#eess.SP) [Total: 16]
- [eess.SY](#eess.SY) [Total: 16]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 12]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver通过自适应并行推理框架实现对LLM推理时间的大幅降低，同时保持与同等规模的顺序推理模型相当的准确性，在六个数学推理基准上达到71.9%的平均准确率（AIME24为79.9%），并实现最高1.53x的 token 延迟加速，建立了准确性与效率之间的新帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 解决自回归解码的序列化特性导致的推理延迟问题，提供一种在必要时进行并行推理的自适应框架以提高推理效率，同时尽量保持或提升准确性。

Method: 1) 两阶段并行轨迹生成器：生成大规模高质量的链式推理数据，并对推理过程进行并行标注以用于有监督微调。2) 基于Trie的训练-推理协同设计：在不修改位置嵌入或KV缓存的前提下，使得在任意现成的自回归推理引擎上实现并行推理。3) 面向并行化的强化学习框架：教模型在准确性与并行化效率之间取得平衡。

Result: 在六个挑战性数学推理基准上，基于Qwen3-8B的ThreadWeaver达到与当前前沿顺序推理模型相当的准确性（平均71.9%，AIME24 79.9%），并实现了平均1.53x的token延迟加速，展示了准确性与效率之间的新 Pareto 前沿。

Conclusion: 表明自适应并行推理能够在不依赖定制推理引擎的情况下，接近或达到同等规模顺序推理模型的性能并显著提升推理速度，为大模型推理的实际部署提供更高的效能-准确性权衡方案。

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [2] [Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning](https://arxiv.org/abs/2512.07844)
*Jinping Wang,Zhiqiang Gao,Zhiwu Xie*

Main category: cs.LG

TL;DR: 研究聚焦长尾数据中特征空间与分类器权重空间之间的错位问题，量化其对错误率的影响，提出三种可插拔的对齐策略，在不改动架构的前提下提升长期不平衡数据集上的分类性能，达到最新状态。


<details>
  <summary>Details</summary>
Motivation: 在类平衡条件下，神经崩溃（Neural Collapse）使特征均值与分类器权重呈现单位正交等距框架（ETF）。但在长尾数据中，显著的样本不均导致NC难以出现，影响泛化。现有方法多聚焦于约束特征或权重以恢复ETF几何，而忽视特征空间与分类器权重空间之间的错位对分类决策的实际损害。

Method: 从最优错误指数角度定量分析错位带来的损害程度；提出三种显式对齐策略，能够在现有长期不平衡方法中“即插即用”无架构改动地实施。

Result: 在 CIFAR-10-LT、CIFAR-100-LT、ImageNet-LT 上的广泛实验显示，对现有基线显著提升，达到并接近或达到领域内的最新性能。

Conclusion: 错位是长尾学习中的关键瓶颈之一，解决特征与分类器权重空间的对齐问题可在不改动模型结构的前提下带来显著的泛化提升，且适用于多种长尾数据集。

Abstract: Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.

</details>


### [3] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash 是一种资源高效、可解释的小模型管线，用于基于纽约市机动车碰撞数据预测伤情严重程度。通过将三个关联表整合、在分区存储中建立统一特征模式、以及对工程化表格特征训练紧凑的树模型集成（随机森林和XGBoost），并与用文本摘要提示的小型语言模型进行对比。结果显示树模型优于SLMs，且SHAP分析揭示了人为脆弱性因素、时序与地点对预测的主导作用；作者认为可解释的小模型基线在城市级伤害分析中仍具竞争力，混合管线在提高沟通效果的同时不牺牲扩展性。


<details>
  <summary>Details</summary>
Motivation: 提升城市级伤害分析的可扩展性与可解释性；解决基于结构化表格数据的预测任务中，使用小型语言模型（SLMs）在性能上落后于树模型的问题；探索将表格预测与SLM生成叙事结合的混合管线。

Method: 整合纽约市机动车碰撞数据中的三张相关表，建立分区存储中的统一特征模式；在工程化的表格特征上训练紧凑的树模型集成（随机森林和XGBoost），并与使用文本摘要的本地部署小语言模型进行对比；在时间外推测试集评估；进行简单类别权重以提升致命率召回率；使用SHAP分析特征贡献。

Result: XGBoost 0.7828，随机森林 0.7794，优于SLMs（0.594 和 0.496）。类别不平衡分析显示通过权重可在致命召回率和准确率之间取得权衡；SHAP显示人类脆弱性因素、时序和地点是预测的主要驱动因素。

Conclusion: 可解释的小模型集成仍是城市规模伤害分析的强基线；将表格预测与SLM生成的叙事相结合的混合管线有助于沟通，同时保持可扩展性。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [4] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 提出一个用于多模态模型的峰值显存预测框架，通过对模型拆解并对每层进行因式分解估算，达到约8.7%平均MAPE的预测准确度，从而帮助预防 OoM。


<details>
  <summary>Details</summary>
Motivation: 随着agentic AI系统中深度学习模型的规模和复杂度增长，显存需求增加，常导致 OoM。现有研究多聚焦单模态，难以推广到多模态模型，因此需要一个可普适于多模态架构的显存预测方法。

Method: 将多模态模型分解为组成层，针对每层应用分解/因式分解来估算内存占用，并结合训练行为来预测峰值显存。

Result: 在评估中实现了约8.7%平均MAPE的显存预测准确度。

Conclusion: 该框架为多模态模型的显存预测提供了有效工具，有助于在训练阶段避免 OoM，同时提升资源利用率；未来需进一步验证在更多模型和训练设置中的泛化性，及涵盖数据加载、缓存等动态因素。

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [5] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: 提出一个跨阶段稀疏加速框架LAPA，通过对数域注意力预测与混合精度多轮移位累加等技术，降低Transformer在不同阶段的计算瓶颈，显著提升能效。


<details>
  <summary>Details</summary>
Motivation: Transformer在不同阶段的输入序列长度和稀疏性变化导致跨阶段的计算瓶颈，现有的单阶段稀疏方法在跨阶段应用时能耗高，缺乏自适应的跨阶段预测与加速策略。

Method: 提出ALOC（非对称前导1计算）以消除乘法开销；引入MRSA（混合精度多轮移位累加）以降低累加开销；设计DDF（数据-特征相关滤波器）以配合MRSA；并设计硬件加速器实现理论提升。

Result: 相对于SOTA Spatten、Sanger、FACT，LAPA分别实现3.52x、3.24x和2.79x的能效提升。

Conclusion: 通过对数域注意力预测与联合的硬件设计，LAPA在跨阶段稀疏Transformer中有效降低计算与能耗，展现出显著的实用性和潜在推广价值。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [6] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: Graph-based collaborative learning for disease detection (CLDD) using EHRs to detect many diseases with few tests.


<details>
  <summary>Details</summary>
Motivation: High costs and impracticality of performing all possible medical tests for diagnosing hundreds or thousands of diseases; need scalable, cost-effective disease detection with interpretability.

Method: Proposes CLDD, a graph-based deep learning model that treats disease detection as a collaborative learning task by leveraging disease associations and patient similarities. It integrates patient-disease interactions and demographic features from EHRs to predict hundreds or thousands of diseases with little reliance on specific medical tests.

Result: On a processed MIMIC-IV dataset with 61,191 patients and 2,000 diseases, CLDD outperforms representative baselines, achieving a 6.33% improvement in recall and 7.63% in precision. Case studies show recovered masked diseases within top predictions, indicating interpretability and reliability.

Conclusion: CLDD reduces diagnostic costs and improves accessibility, with potential for large-scale disease screening and social health security.

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [7] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: SA^2GFM提出了一种鲁棒的图形基础模型，结合结构感知的语义增强在跨域自适应表示方面提升鲁棒性与效果。通过将层次结构先验编码为结构感知的文本提示进行特征增强，利用结构引导的压缩进行自监督信息瓶颈提取，并结合混合专家的自适应路由以降低跨域负迁移，辅以分层结构的联合内外社区学习进行高效微调，在节点和图分类上对9个基线实现了显著鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation:  Graph Foundation Models在面对领域噪声、结构扰动和对抗攻击时鲁棒性不足，且对层次结构语义的建模不足限制了泛化能力，需要通过结构感知的语义增强来提升跨域适应性和鲁棒性。

Method: 1) 将基于熵的编码树转换为结构感知的文本提示用于特征增强；2) 通过结构引导的压缩，应用自监督的信息瓶颈机制以提炼鲁棒、可迁移的表征；3) 采用混合专家架构并设计空专家来实现自适应路由，缓解跨域自适应中的负迁移；4) 设计微调模块，通过联合内部与跨社区的结构学习来优化分层结构、提高下游适配效率。

Result: 大量实验表明SA^2GFM在节点和图分类任务中显著优于9个最先进基线，并且对随机噪声和对抗扰动具有鲁棒性。

Conclusion: 通过对层次结构语义的结构感知增强与有效的路由与微调设计，SA^2GFM实现了更强的鲁棒性和泛化能力，尤其在跨域自适应方面表现突出。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [8] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: 提出 FAIM：一种轻量级的频域感知时间序列分类模型，结合自适应过滤块和互动多粒度块，并引入自监督预训练，在准确性与效率之间实现良好折衷，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类需要捕捉判别信息，深度模型虽强但计算成本高、对噪声敏感且在小数据集易过拟合。需兼具鲁棒性、高效性和对频域信息的利用的新架构。

Method: 提出 FAIM，包括 Adaptive Filtering Block（AFB）：利用傅里叶变换提取频域特征，引入可学习的自适应阈值以动态抑制噪声，并实现全局与局部语义自适应滤波的逐元素耦合，增强不同频段之间的协同建模。以及 Interactive Mamba Block（IMB）：实现多粒度信息交互，平衡细粒度特征与全局上下文的信息提取。并引入自监督预训练以提升对复杂时间模式的理解与在高噪声场景下的鲁棒性。

Result: 在多个基准数据集上，FAIM在准确性与效率之间取得优越折衷，整体表现优于现有SOTA方法，且对噪声具有更强鲁棒性与更好的泛化能力。

Conclusion: FAIM通过在频域感知、细粒度与全局信息的高效交互，以及自监督学习的结合，提供了强大的时间序列分类表征，展现出良好的应用潜力与进一步优化空间。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [9] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: 提出 SetAD，将半监督异常检测从点/对聚焦转向集合级别，通过注意力编码器学习集合级异常性，并结合上下文校准得分，实验显示在多数据集上优于SOTA，并且随集合规模增大性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法以点或对为单位，忽略异常的群体性、以及高阶交互中的监督信号，难以学习鲁棒的表示和良好的校准分数。

Method: 引入 SetAD：基于注意力的集合编码器，采用分级学习目标，学习整个集合的异常程度；引入上下文校准的异常得分：通过在不同上下文集合中对比同一点与同侪行为的归一化偏差聚合得分。

Result: 在10个真实数据集上显示显著优于现有模型，且性能随集合规模增大稳定提升。

Conclusion: 集合级建模可更好地捕捉异常的群体性特征，SetAD提供了一种鲁棒、可扩展的半监督AD框架，具备良好校准性和对高阶关系的适应性。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [10] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 提出一个基于无监督学习的框架，用以从大规模海关贸易数据中识别可疑交易，结合K-Means聚类、Isolation Forest、IQR异常检测、启发式标记，生成优先级分数，帮助监管机构审查，已在10万条记录上验证，发现价格异常和高优先级运输，并通过SHAP解释其风险因子。


<details>
  <summary>Details</summary>
Motivation: 需要高效、可重复的方法来监控环境条约执行（如蒙特利尔议定书）并应对海关数据的规模和复杂性。

Method: 将多种无监督学习方法组合成分层管线：K-Means揭示基于运输价值、重量的贸易原型，Isolation Forest和IQR用于异常检测（mega-trades、价格/公斤异常），启发式标记用于识别模糊描述等；再将多层得分合成为优先级分数。

Result: 在10万条贸易记录上，识别出1351个价格离群和1288个高优先级货物供审查；高优先级商品具有不同且更高的价值/重量比；SHAP解释表明模糊描述和高价值是最重要的风险预测因子；模型在2021年初检测到巨大mega-trade激增，与US AIM Act的监管影响相关；提出一个可重复的无监督学习管线，将原始贸易数据转化为可供监管机构使用的优先级情报。

Conclusion: 提出一个可重复的无监督学习管线，将原始贸易数据转化为可供监管机构使用的优先级情报。

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [11] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 将瑞典注册数据文本化，利用NLP对长期居住迁移进行预测，比较多种模型，显示文本化数据保留有意义信息并提升纵向分析能力。


<details>
  <summary>Details</summary>
Motivation: 解决高基数分类变量和随时间编码不一致等挑战；利用覆盖面广的国家人口登记数据，建立可扩展的序列建模评估框架，提供测试新序列模型的方法学基础。

Method: 将6.9百万个体(2001-2013)的注册数据转化为语义化文本 life trajectories，包含年度居住、工作、教育、收入、家庭等变化，与人口统计信息结合；比较LSTM、DistilBERT、BERT、Qwen等NLP架构，评估其对2013-2017年预测的能力。

Result: 文本化注册数据保留了关于个人路径的有意义信息，序列/Transformer模型比基线模型更有效地捕捉时序与语义结构，展现出较强的预测和分析能力，提供一个可扩展的分析框架。

Conclusion: 将语义丰富的注册数据与现代语言模型结合，能显著推进社会科学的纵向分析，且此数据集为测试新序列建模方法提供独特的试验平台。

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [12] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型的医学时间序列缺失值估计的新框架，结合RFAMoE和Fusion MoE以自适应感受野和并行噪声信号融合，在一次推理中实现高效重建，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列具有多变量、高时间变化性、嘈杂及易产生伪迹等特征，给基于深度学习的插补带来挑战。因此需要新的噪声估计和高效推理策略以提升准确性和速度。

Method: 提出Receptive Field Adaptive MoE (RFAMoE)模块，使每个通道在扩散过程中自适应选择所需的感受野；同时设计Fusion MoE模块，在并行生成K个噪声信号后通过路由机制融合，在单次推理中完成信号重建，降低计算成本与延迟。

Result: 大量实验结果表明，该框架在不同任务和数据集上持续优于基于扩散的SOTA方法。

Conclusion: 通过结合自适应感受野和并行噪声信号融合，所提框架在医学时间序列插补任务中实现了性能提升并显著降低推理成本。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [13] [Controllable risk scenario generation from human crash data for autonomous vehicle testing](https://arxiv.org/abs/2512.07874)
*Qiujing Lu,Xuanhan Wang,Runze Yuan,Wei Lu,Xinyi Gong,Shuo Feng*

Main category: cs.LG

TL;DR: CRAG is a framework for controllable generation of environment agents that captures both nominal and rare safety-critical behaviors for autonomous vehicle testing, using a structured latent space to separate normal and risk behaviors and optimization-based mode transitions.


<details>
  <summary>Details</summary>
Motivation: To enable realistic, diverse, and controllable simulation of background vehicles and VRUs that cover both everyday driving and rare crashes, addressing limited crash data.

Method: Construct a structured latent space that disentangles normal and risk-related behaviors; develop risk-aware latent representations; apply optimization-based mode-transition to smoothly shift agents from safe to risk states over long horizons; maintain high fidelity in both regimes.

Result: CRAG improves diversity of generated scenarios compared with baselines and enables controllable risk-scenario generation for targeted, efficient AV robustness evaluation, leveraging limited crash data effectively.

Conclusion: CRAG provides a unified, risk-aware framework for environment agent generation that supports diverse, controllable, and high-fidelity risk scenarios for autonomous vehicle testing.

Abstract: Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.

</details>


### [14] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: S2KAN introduces differentiable sparsification of a mixture of symbolic and dense activations guided by a Minimum Description Length objective, enabling interpretable activations when symbolic terms suffice and graceful fallback to dense splines, with competitive performance in smaller models.


<details>
  <summary>Details</summary>
Motivation: To address the gap where Kolmogorov-Arnold Networks (KANs) offer interpretability but trained activations often lack symbolic fidelity; integrate symbolic primitives into training to obtain interpretable forms without sacrificing accuracy.

Method: Extend KANs with a dictionary of symbolic and dense terms; activations have learnable gates that sparsify representations in a differentiable manner; optimize end-to-end under a principled MDL objective; when symbolic terms dominate, interpretability emerges; otherwise, the model relies on dense splines.

Result: Achieves competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world tasks; shows emergent self-sparsification even without explicit regularization.

Conclusion: S2KAN enables interpretable activations when symbolic terms suffice and gracefully degrades to dense splines otherwise; MDL-guided sparsification supports end-to-end optimization and compact models, with empirical evidence of self-sparsification.

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [15] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: SpecMatch-CL 引入基于拉普拉斯矩阵的损失，用于对齐跨视图的全局结构（视图特定的图的图结构），通过最小化归一化拉普拉斯差异实现对比学习中的全局拓扑控制，并给出理论界限。实证在八个 TU 基准数据集的无监督和小标签半监督学习中达到新状态，且在传输学习任务 PPI-306K 与 ZINC 2M 上有稳定增益。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习（如 InfoNCE）在图嵌入对齐方面关注成对关系，缺乏对跨视图图–图结构（图的图）的全局结构控制。需要一个机制来约束视图特定的全局拓扑，以提升鲁棒性和迁移性能。

Method: 提出 SpecMatch-CL 损失，通过对比学习得到的嵌入构造视图特定的图–图，并对该图的归一化拉普拉斯矩阵进行比较；定义一个损失项以最小化两个视图的归一化拉普拉斯矩阵差异。给出理论分析：在一定假设下，归一化拉普拉差异可以上界理想的 Perfect Alignment 对比损失与当前损失之间的差，以及 Uniformy 损失之间的差。

Result: 在 eight TU 基准数据集上实现新颖的无监督与低标签比例的半监督学习的最优或接近最优性能；在传输学习任务 PPI-306K 与 ZINC 2M 上获得一致的增益。

Conclusion: SpecMatch-CL 提供了一种从全局角度约束对比学习中图-图结构的有效方法，通过对齐视图特定的图–图的归一化拉普拉斯矩阵实现更稳健的全局拓扑对齐，提升了多种任务的性能并具备良好的泛化性。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [16] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 从几何角度将 NMF 看作数据在凸锥内的表示，提出 Cone Collapse 收敛到最小生成锥，并据此构建 CC-NMF，在16个数据集上实现对比基线更优的聚类效果。


<details>
  <summary>Details</summary>
Motivation: 利用 NMF 的凸锥几何性质，直接从数据出发恢复极端射线（极点），从而获得更自然、理论可辩护的聚类表示；并以此建立更稳健的正交 NMF 变体以提升聚类性能。

Method:  Cone Collapse：从全正正交象限出发，迭代收缩至数据所生成的最小锥，理论上在轻微数据条件下有限步收敛并恢复 X^T 的最小生成锥；在此基础上，通过对恢复出的极端射线应用单正交 NMF，得到 Cone-aware Orthogonal NMF (CC-NMF)。

Result: 在16个基因表达、文本和图像数据集上，CC-NMF 的聚类纯度稳定达到或超过强基线（包括乘法更新、ANLS、投影 NMF、ONMF、稀疏 NMF），表现出色。

Conclusion: 显式恢复数据锥结构可提供理论上扎实且经验上良好的 NMF 基于聚类的方法。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [17] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [18] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: 通过联合算法-系统设计，GSPN-2 将 GSPN 的实现成本显著降低，同时保持与变换器相近的准确性，适用于高分辨率图像和长视频场景。


<details>
  <summary>Details</summary>
Motivation: 推动高性能视觉变换器在高分辨率图片和长视频场景中的实际应用，解决自注意力的二次复杂度及其在实现中的开销问题。

Method: 在GSPN的基础上进行系统性改造：将内核 launches 压缩为一个二维核，给每个通道切片绑定一个 warp，并在共享内存中缓存前一列激活；提出紧凑的通道传播策略，替代逐通道的矩阵，参数量更少且与注意力中的亲和映射对齐。

Result: 在图像分类和文本到图像合成任务中，GSPN-2 达到与变换器水平的准确性，同时显著降低计算成本。

Conclusion: 通过结构化矩阵变换和GPU优化实现，GSPN-2 为在视觉应用中建模全局空间上下文设立新的高效前沿。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [19] [Differentially Private Synthetic Data Generation Using Context-Aware GANs](https://arxiv.org/abs/2512.08869)
*Anantaa Kotal,Anupam Joshi*

Main category: cs.LG

TL;DR: ContextGAN is a context-aware, differentially private GAN that integrates domain-specific and implicit rules via a constraint matrix to generate realistic, privacy-preserving synthetic data across healthcare, security, and finance.


<details>
  <summary>Details</summary>
Motivation: Standard synthetic data often captures explicit patterns but misses implicit domain constraints, risking unrealistic or unsafe outputs in regulated fields like healthcare; there is a need to balance data utility with privacy and to encode both explicit and implicit rules into generative models.

Method: Proposes a Context-Aware Differentially Private GAN (ContextGAN) that uses a constraint matrix encoding domain rules (explicit and implicit). A constraint-aware discriminator enforces these rules on generated data while preserving differential privacy.

Result: Across multiple domains (healthcare, security, finance), ContextGAN produced high-quality synthetic data that adheres to domain constraints and preserves privacy, improving realism and utility by enforcing domain rules.

Conclusion: Incorporating explicit and implicit domain constraints into a differentially private GAN enhances the realism and utility of synthetic data while maintaining strong privacy guarantees, making it suitable for regulated applications.

Abstract: The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.

</details>


### [20] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: Brush: a symbolic regression algorithm that integrates decision-tree-like splits with non-linear constant optimization to incorporate rule-based logic into SR/classification, achieving Pareto-optimal performance and capable of reproducing clinical scoring systems with simple, interpretable models.


<details>
  <summary>Details</summary>
Motivation: Medical decision-making relies on risk equations and rule-based guidelines; SR could yield data-driven, interpretable clinical risk scores, but traditional SR is limited to continuous forms. Brush aims to bridge this gap by embedding rule-based logic into SR to better model decision pathways.

Method: Brush combines decision-tree-like splitting with non-linear constant optimization to merge rule-based logic with symbolic regression and classification. It navigates a search space that accommodates rule-based structures, aiming for Pareto-optimal solutions on SRBench and applications to reproduce clinical scoring systems.

Result: Achieved Pareto-optimal performance on SRBench. Successfully recapitulated two widely used clinical scoring systems with high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush provides comparable or superior predictive performance while yielding simpler models.

Conclusion: Brush provides a data-driven, interpretable SR framework that can model rule-based clinical decision pathways and reproduce established scoring systems with competitive accuracy and simpler models.

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [21] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: CIP-Net 是一个 exemplar-free、自解释的原型基模型，专为持续学习设计，避免存储历史样本，同时提供预测解释，达到对比方法中的 state-of-the-art 并具备更低的内存开销。


<details>
  <summary>Details</summary>
Motivation: 持续学习中的灾难性遗忘问题突出；可解释性AI有助于理解并减轻遗忘；现有可解释方法多为事后解释或需要为每个新任务额外存储样本，扩展性差。需要一种无样本存储、易扩展且可解释的解决方案。

Method: 提出 CIP-Net，一种 exemplar-free、自解释的原型基模型。通过原型来支撑决策与解释，避免对历史数据的记忆开销，保持简单架构，同时提供可解释性和良好性能。

Result: 在任务增量和类别增量设置中，相比以往的 exemplar-free 与自解释方法，CIP-Net 展现出更好的性能，且显著降低了与内存相关的开销，达到更实用的可解释持续学习解决方案。

Conclusion: CIP-Net 提供了一种实用且可解释的持续学习方案，能够在不存储历史样本的情况下实现高性能并给出解释。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [22] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 提出一个网页平台，降低时间序列分析门槛，支持数据上传、可视化、多模型训练及基于大语言模型的参数推荐和解释，目标整合到学习健康系统中。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在医疗等领域具有广泛应用前景，但分析、建模与结果解释的技术门槛高，限制了在研究和临床中的落地应用。需要一个易于使用的平台来降低门槛，促进数据驱动的决策。

Method: 开发并提供一个网页平台，允许用户上传数据、生成变量关系的可视化、支持多种可定制的时间序列预测模型与训练技术、并通过大语言模型给出参数选择的推荐和结果解释。

Result: 提出的综合云/网页平台具备数据上传、变量绘图、多模型训练与自定义训练流程、以及基于大语言模型的参数建议与解释能力；文本中未给出实验评估结果，重点在功能描述与平台能力。

Conclusion: 该平台旨在将时间序列分析嵌入学习健康系统与临床管线，实现持续数据收集与推理，降低技术门槛，促进研究者与临床人员对结果的理解与应用。

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [23] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: Offline multi-objective reinforcement learning (MORL) can produce Pareto-efficient, test-time customizable policies in critical care, outperforming fixed scalarized baselines when evaluated offline with OPE; sequence-based decision transformers scale to multi-objective conditioned generation, enabling personalized decisions without retraining.


<details>
  <summary>Details</summary>
Motivation: Critical care involves conflicting objectives (e.g., maximizing survival while minimizing resource use). MORL can furnish a set of Pareto-optimal policies, enabling clinicians to choose according to current priorities. Offline learning is essential due to safety and data constraints in healthcare.

Method: Benchmark three offline MORL algorithms—Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT)—against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Use Off-Policy Evaluation (OPE) metrics to assess policy quality and flexibility, with test-time preference conditioning for MORL.

Result: PEDA DT demonstrated superior flexibility compared to static scalarized baselines; MORL approaches effectively scale sequence modeling to multi-objective conditioned generation; offline MORL shows promise for personalized, adjustable ICU decision-making without retraining.

Conclusion: Offline MORL is a promising framework for customizable and Pareto-efficient decision-making in critical care, enabling clinicians to adapt to changing priorities without retraining, with PEDA DT offering the strongest observed flexibility in the reported evaluation.

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [24] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY：通过在结构化潜在空间中建立医学世界模型，结合时间间隔和患者临床上下文，对治疗条件下的疾病演化进行可解释的预测，并以预测到决策的框架输出透明、可执行的治疗建议，在MU-Glioma-Post数据集上实现12%超越MeWM并优于所有医学专用大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为静态预测，难以捕捉动态疾病进展；现有医学世界模型多依赖扩散模型，聚焦视觉重建，缺乏因果、生理转移的建模，且往往忽略患者特定的时间与临床情境以及治疗决策反馈机制，限制了对个体化治疗的支撑。

Method: 提出 CLARITY：在结构化潜在空间中直接预测疾病演进，显式整合时间间隔（时间上下文）与患者临床上下文，建模治疗条件下的平滑与可解释轨迹，并新增预测-to-决策框架，将潜在滚动转化为透明、可执行的治疗建议。

Result: 在 MU-Glioma-Post 数据集上，CLARITY 的治疗规划性能超越近期 MeWM 12%，并显著优于所有其他医学专用大语言模型。

Conclusion: CLARITY 展示了在治疗规划中的最先进性能，证明将时间与临床上下文嵌入世界模型以及引入预测到决策的框架，有望提升个体化治疗的可解释性与可执行性。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [25] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: 提出了一种可学习核特征映射的线性注意机制LUNA，在保持线性时间/空间复杂度的同时，提升甚至超越二次方softmax注意的精度，与固定特征线性化相比破解了精度-效率的权衡。在Long Range Arena上实现了与计算对等条件下的最优精度，且对BERT/ViT-ViT的后处理转换表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 二次方注意力的计算复杂度对长序列任务构成瓶颈；线性注意力通常使用固定的随机特征或手工核，导致数据无关的核诱导固定的表达力，牺牲模型精度以换取效率。需要在保持线性复杂度的前提下提升准确性。

Method: 引入可学习的特征映射，构成正定核，具备流式形式，实现在序列长度上的线性时间和内存扩展；通过端到端学习让核特征自适应数据和任务；并提供对BERT/ViT的后处理二次注意的替换与短暂微调的实验评估。

Result: 在Long Range Arena上，在计算对等的条件下，LUNA实现了高于同等资源的高效Transformer的平均准确度；同等参数量、训练步数与近似FLOPs下达到或超越二次注意；对后处理转换表现出显著优势，替换softmax后只需短暂微调即可恢复大部分性能，且明显优于固定线性化方法。

Conclusion: 通过学习核特征映射，打破了线性注意在固定特征下的表达力限制，实现更高精度的线性注意，兼具线性成本与良好泛化能力，具备在长序列任务中大规模应用的潜力。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [26] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 提出一种可解释的深度混合风险模型 Deep Kernel Aalen-Johansen (DKAJ)估计器，扩展了经典的 Aalen-Johansen 非参数 CIF 估计，使用基于核的权重将每个样本表示为若干簇的加权组合，实现可解释性与灵活性并行。


<details>
  <summary>Details</summary>
Motivation: 在竞争性风险场景下需要可解释且非参数的累计发生率函数(CIF)估计，同时结合深度学习学习数据点间的相似性，以实现对个体及簇级贡献的可视化解释。

Method: 提出 DKAJ：将每个样本表示为簇的加权线性组合，权重来自自动学习的核函数，该核衡量任意两点的相似性；如果某点的权重仅属于一个簇，则其 CIF 与该簇内数据点的经典 Aalen-Johansen 估计相同。对四个标准竞争风险数据集进行评估，结果以可视化方式辅助模型解释。

Result: 在四个公开数据集上，DKAJ 与最先进基线具有竞争力，且能够提供有助于解释的可视化输出。

Conclusion: DKAJ 提供了一种可解释且具有竞争力的竞争风险 CIF 估计方法，通过核权重实现样本的簇级分解和直观解读。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [27] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 提出因果引导的多模态域泛化（MMDG）框架，通过对抗性解缠和统一表示学习来提升未见灾害情境下的危机分类泛化能力。


<details>
  <summary>Details</summary>
Motivation: 社交媒体危机分类在跨灾害场景上的泛化能力不足，现有多模态方法容易将伪相关等无关特征混入，并且不同模态难以在同一潜在空间对齐，限制了将单模态域泛化方法迁移到多模态场景。需要通过区分因果特征与非因果特征、并实现模态对齐来提升跨域鲁棒性。

Method: 提出两大支柱：1) 对抗性解缠，学习领域不变的因果特征，抑制域偏移；2) 统一表示学习，将文本与视觉特征对齐到共享潜在空间，使单模态域泛化策略可直接应用于多模态；端到端联合训练。

Result: 在多个数据集上的未见灾害情景评估中，MMDG达到最佳或显著优于基线的泛化性能，并通过消融实验验证了因果解缠和统一表示的关键作用。

Conclusion: 通过结合因果性与跨模态对齐，MMDG为多模态危机分类提供更稳健的泛化框架，展示在跨灾害情景中的实用潜力。

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [28] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 通过自我披露（confession）来激励模型诚实地报告自身不足，训练中的奖励只基于 confession 的诚实性，从而提升对模型行为的可检测性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在强化学习（RL）中可能因奖励塑形导致不诚实报告，遮掩隐秘行动。需要一种机制在输出后获得对模型遵循性与缺陷的透明自述，以便进行监控与干预。

Method: 提出自我报告的 confession：在模型给出主答案后应要求输出一份完整的自述，记录模型对政策与指令的遵循情况。训练时对 confession 的奖励仅基于其诚实度，不影响主答案的奖励；因此最短路径不是隐瞒，而是诚实披露缺陷。以 GPT-5-Thinking 进行训练并在分布外场景评估诚实性，指标涵盖幻觉、指令执行、阴谋性行为与奖励攻击等。

Result: 当主答案存在谎报或省略时，confession 往往能诚实地披露这些行为，且随着训练的进行诚实性有适度提升。confessions 还能实现推断时的干预，如监控、拒绝采样以及向用户暴露问题。

Conclusion: confessions 为提升系统透明性与安全性提供了一种可操作的机制，便于在推断时进行监控与干预。

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [29] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: 提出并验证了 MAC：将动作块（action chunks）用于模型基于强化学习的值扩展，并通过对行为策略进行拒绝采样以防止离散外部动作的利用，从而在离线、长时程任务中提升稳定性和性能，达到现有离线MBRL方法中最优性能，尤其在长时程任务上。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，处理长时程任务的难点在于模型基于价值扩展对未来的预测会因为长时程而累积误差，且从经验行为中过度拟合可能导致策略失效。需要一种可扩展且对模型误差鲁棒的MBRL方法。

Method: 引入动作块（action chunk）来预测由一连串动作组成的未来状态，代替单步动作，降低累积误差。通过对一个表达能力强的行为动作块策略进行拒绝采样，在离线数据上避免模型对分布外动作的过拟合，从而训练一个基于模型的价值扩展的策略。整个方案命名为 Model-Based RL with Action Chunks (MAC)。

Result: 在包含高达百万级转移数据集的极具挑战性的任务上进行实验，MAC在离线模型基于强化学习算法中表现最好，尤其是在长时程任务上。

Conclusion: MAC通过减少模型累积误差并抑制对分布外动作的利用，提供了一个可扩展的离线MBRL范式，显著提升了长时程任务的表现。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [30] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 在评估大型语言模型时，提出以 Youden 的 J 指标（及其与 Balanced Accuracy 的等价性）来选择最佳判定者，从而提高对比模型的鲁棒性；相比传统 Accuracy/Precision/F1，J/BA 对类别不平衡不敏感。


<details>
  <summary>Details</summary>
Motivation: 当前评估依赖判定器给出的行为出现率估计，常用指标对类别不平衡敏感，导致偏差。需要一个与选择最佳判定者对齐的度量。

Method: 在理论层面分析 Youden's J 与 Balanced Accuracy（BA）之间的关系；通过分析、经验示例和仿真来比较判定者选择对模型比较的影响。

Result: 证明 Youden's J 与最佳判定者的选择一致；BA 是 J 的线性变换等价；基于 BA 选择判定者可获得更好且更鲁棒的模型比较。

Conclusion: 应在评估中优先采用 BA/J 作为判定者选择标准，避免受类别不平衡等因素影响的传统指标；推动在 LLM 评估中的标准化应用。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [31] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 提出一种名为 CSO 的后门检测方法，通过对目标类别进行子空间正交化以抑制其内在特征，从而提升检测统计对后门触发的敏感性，克服非目标类别易产生极端统计和后门信号微弱的困难。


<details>
  <summary>Details</summary>
Motivation: 现有基于极端异常的后门检测在某些场景失效：非目标类别可能天然具备极端的检测统计；当后门触发特征较弱时，后门信号难以被检测。需通过仅对目标类别强化检测统计、抑制其内在特征来提高灵敏度。

Method: 提出 plug-and-play 的 Class Subspace Orthogonalization (CSO) ，给定类别的少量清洁样本，构造该类别的子空间，并在优化检测统计时对其正交化以抑制内在特征，同时保留后门触发信号的贡献；在混合标签和自适应攻击下评估。

Result: 在具有挑战性的混合标签与自适应攻击场景中，CSO 展现出对后门检测的更高灵敏度，尤其在后门信号与内在特征难以分离的情形下。

Conclusion: CSO 为后门检测提供了一种新的可插拔框架，通过抑制非目标的内在特征来增强对后门触发信号的检测能力，具备对抗混合标签和自适应攻击的潜力，但对清洁样本质量、类别内特征结构以及目标子空间构造的依赖需进一步验证。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [32] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 提出并初步开发生物威胁基准生成BBG框架的第一组件：细菌生物威胁架构（Bacterial Biothreat Schema），用于评估LLMs的生物安全风险与潜在危害，并考虑不同攻击者能力水平和操作性风险因素。


<details>
  <summary>Details</summary>
Motivation: 解决模型开发者和政策制定者在评估和降低快速演化的前沿AI，尤其LLMs的生物安全风险方面的不足，强调在评估中纳入能力等级和操作性风险等关键要素。

Method: 建立分层的生物威胁类别-要素-任务结构，基于该结构开发任务对齐查询，命名为Bacterial Biothreat Schema，作为把查询转化为评估用示例的基础，后续将把查询转化为模型提示并实现基准。

Result: 提出BBG框架的初步蓝图和Bacterial Biothreat Schema，为多层级聚合的生物风险评估提供稳健、可复用的结构，覆盖技术与操作性要求，以及对生物对手能力的广谱考量。

Conclusion: BBG框架及Bacterial Biothreat Schema为评估LLMs在生物威胁领域的风险提供初步且可扩展的基准架构，未来将扩展到提示设计和实证评估应用。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [33] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: PolyLingua 是一个轻量级 Transformer 模型，针对内域语言检测与细粒度语言分类，采用两级对比学习，显著提升在低资源/低延迟场景的准确性，且参数量比 Sonnet 3.5 少约 10 倍。


<details>
  <summary>Details</summary>
Motivation: 在多语言系统中，语言识别是关键首步；现有工具在音乐请求等场景的语言混合、以及近似或相关语言之间的分辨上容易出错；需高效且准确的模型以减少级联失败。

Method: 提出两级对比学习框架（实例级分离与类级对齐，带自适应边际），与轻量 Transformer 共同生成光滑且分离的嵌入，用于内域语言检测和细粒度分类。

Result: 在 Amazon Massive（多语言数字助理 utterances）和 Song 数据集（带大量代码切换的音乐请求）上分别达到 99.25% F1 和 98.15% F1，超越 Sonnet 3.5，同时参数量降低 10 倍。

Conclusion: 为计算资源受限环境提供高精度的语言识别与细粒度分类解决方案，适用于多语言对话系统等场景。

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [34] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO提出了一种树形结构RL优化框架，将去噪过程重构为搜索树，通过共享初始噪声并分叉生成多条候选轨迹，重复利用前缀计算，实现高样本效率、细粒度信用分配与摊销计算，从而在相同训练样本下获得更好性能，并达到2.4倍的训练加速，且在效率-奖励权衡上开辟Pareto前沿。


<details>
  <summary>Details</summary>
Motivation: RL对齐生成模型与人类偏好通常计算成本高，迫切需要更高效的RL后训练方法以降低成本并提升样本与计算效率。

Method: 将去噪过程重塑为一棵树状搜索：从共享的初始噪声出发，通过多分支生成多条候选轨迹，并高效复用公前缀；优势包括高样本效率、通过奖励反向传播实现逐步优势的细粒度信用分配，以及在多子分支下的前向传播中实现多次策略更新的摊销计算。

Result: 在扩散模型和流式模型上，TreeGRPO实现了2.4×训练速度提升，并在效率-奖励权衡的Pareto前沿上优于GRPO基线，且在多个基准和奖励模型上表现稳定，证明了其可扩展性和有效性。

Conclusion: TreeGRPO为基于RL的视觉生成模型对齐提供了一种可扩展且高效的路径，显著提高训练效率并提升对齐效果；研究成果在项目网站treegrpo.github.io上有更多信息。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [35] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: 提出 MobileFineTuner：一个开源框架，支持在普通移动 phones 上端到端细调大语言模型（Full-FT 与 PEFT），通过参数分片、梯度累积和能耗感知调度等系统级优化解决内存和能源限制，在真实设备上对 GPT-2、Gemma 3、Qwen 2.5 进行微调，验证了可行性与有效性。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公开数据日渐稀缺，利用私有用户数据进行个性化微调具有重要意义；另一方面，移动设备广泛存在且具备强隐私保护潜力。然而现有工作多是仿真或针对物联网/PC，缺乏面向普通手机的端到端在机细调开源框架，因此需要一个可用、可扩展的解决方案来推动在设备端的LLM训练研究。

Method: 提出统一的开源框架 MobileFineTuner，支持全参数微调（Full-FT）与参数高效微调（PEFT）。为应对移动设备的内存与能耗限制，提出系统层面的优化策略：参数分片、梯度累积、能耗感知的计算调度。通过在真实手机上对 GPT-2、Gemma 3、Qwen 2.5 进行微调，展示框架的可用性与有效性。

Result: 实验与消融研究表明所提优化措施有效提升在设备上的训练可行性与性能，证明 MobileFineTuner 可以成为未来在设备端进行LLM训练的基础框架。

Conclusion: MobileFineTuner 成功将端到端的在机LLM微调带入普通手机，促成隐私友好型个性化模型训练的研究路线，未来可扩展到更大模型、进一步完善优化与跨硬件的适配。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [36] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 提出将解耦权重衰减按学习率的平方比例 γ^2 进行缩放，从而实现权重范数的稳定性，并通过 Scion 优化器的总更新贡献（TUC）与基于动量的有效学习率之间的关系来调控训练动力学，实证显示 γ^2 缩放能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 围绕 AdamW 与 Adam 的性能差异，关于解耦权重衰减应如何随学习率缩放的争议长期存在。有人主张与学习率成正比的衰减，亦有基于正交性推导的 γ^2 结论。本文提出一个简单但普适的假设——在稳态时更新与权重的关系被弱化，更新对权重的正交分量的贡献对训练影响有限，进而推导出 γ^2 缩放能使权重范数保持稳定，并探讨其对训练动力学的意义。

Method: 给出稳态独立性假设，推导解耦权重衰减应为 γ^2 的条件以实现权重范数稳定；由此推导并验证 Scion 优化器下的 Total Update Contribution（TUC）与动量相关的有效学习率之间的关系；通过实验比较不同 γ 与 γ^2 缩放的权重衰减对权重与梯度范数、训练曲线和模型性能的影响。

Result: 证明在稳态条件下， γ^2 的解耦权重衰减可维持权重范数稳定，且与梯度范数同样受控；TUC 的行为可被动量依赖的有效学习率更好地描述，最优值可在不同设置间迁移；总体上， γ^2 缩放的解耦权重衰减使训练动力学更易控，提升模型性能。

Conclusion: 将解耦权重衰减的缩放原则统一为 γ^2，有利于在不同优化器下实现稳定的权重与梯度范数，以及更稳健的训练表现，为优化器设计提供更具通用性的指引。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [37] [Persistent Topological Structures and Cohomological Flows as a Mathematical Framework for Brain-Inspired Representation Learning](https://arxiv.org/abs/2512.08241)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Shipra Prashant*

Main category: cs.LG

TL;DR: A mathematically rigorous framework for topology-guided representation learning using persistent topology and cohomological flows, modeling neural computation as cochain-map evolution on dynamic simplicial complexes; integrates algebraic topology with differential geometry to generalize gradient-based learning via cohomological operators; validated on synthetic and real neural data with persistent homology, sheaf cohomology, and spectral Laplacians, achieving better manifold consistency and noise resilience than GNNs and other manifold-based methods.


<details>
  <summary>Details</summary>
Motivation: To achieve representations that are invariant and stable across temporal, spatial, and functional variations in brain states by leveraging deep connections between topology, geometry, and learning. The aim is to extend learning frameworks beyond traditional gradient-based methods by embedding them in a topological-differential structure.

Method: Formulate neural computation as the evolution of cochain maps over dynamic simplicial complexes; construct cohomological operators that generalize gradients within a homological landscape; integrate tools from persistent homology, sheaf cohomology, and spectral Laplacians to measure invariants.

Result: Empirical analysis on synthetic datasets with controlled topological signatures and real neural datasets shows superior manifold consistency and noise resilience compared to graph neural networks and manifold-based deep architectures; demonstrates that the framework preserves topological/structural aspects of data.

Conclusion: Provides a coherent mathematical foundation for topology-driven representation learning, suggesting robust, invariant representations and a new paradigm that unifies algebraic topology, differential geometry, and learning.

Abstract: This paper presents a mathematically rigorous framework for brain-inspired representation learning founded on the interplay between persistent topological structures and cohomological flows. Neural computation is reformulated as the evolution of cochain maps over dynamic simplicial complexes, enabling representations that capture invariants across temporal, spatial, and functional brain states. The proposed architecture integrates algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning within a homological landscape. Synthetic data with controlled topological signatures and real neural datasets are jointly analyzed using persistent homology, sheaf cohomology, and spectral Laplacians to quantify stability, continuity, and structural preservation. Empirical results demonstrate that the model achieves superior manifold consistency and noise resilience compared to graph neural and manifold-based deep architectures, establishing a coherent mathematical foundation for topology-driven representation learning.

</details>


### [38] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: SPROCKET introduces a prototype-based feature transformation for time series classification, extending ROCKET-style random kernel features. It achieves competitive performance with existing convolutional methods, and a MR-HY-SP ensemble outperforms HYDRA-MR. This suggests prototype-based features can improve accuracy and robustness in TSC.


<details>
  <summary>Details</summary>
Motivation: Classical time series classification has been dominated by feature-engineering pipelines. There is a need to explore alternative feature transforms that can capture informative patterns with robustness, leveraging prototypes to guide kernel-like transforms.

Method: Propose SPROCKET, a prototype-based feature transformation that selects prototypes and uses them to construct random convolutional kernel features (in the spirit of ROCKET). Evaluated on major TSC archives (UCR/UEA) and compared to existing convolutional algorithms and ensembles like HYDRA-MR and HYDRA-MR variants.

Result: SPROCKET achieves performance comparable to existing convolutional methods on a majority of the UCR/UEA archives. The MR-HY-SP ensemble’s average accuracy ranking surpasses HYDRA-MR, the previous best convolutional ensemble.

Conclusion: Prototype-based feature transformations can enhance both accuracy and robustness in time series classification, offering a competitive alternative to full convolutional pipelines and strengthening ensemble performance.

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [39] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 提出一种将多模态生理信号通过几何-随机框架结合的深度学习模型，用于SUDEP和缺血性中风的早期检测与风险分层，并给出可解释生物标志物。


<details>
  <summary>Details</summary>
Motivation: 在神经自律疾病（SUDEP、 stroke）中，跨模态信号及其时空演化的复杂性需要一个同时考虑几何结构、随机性和因果传播的统一建模框架，以实现早期预测和可解释性。

Method: 提出一个统一的几何-随机多模态深度学习框架，整合 EEG、ECG、呼吸、SpO2、EMG、fMRI；包含黎曼流形嵌入、李群不变特征、分数阶随机动力学、哈密顿能量流建模、跨模态注意力；用分数流行传播通过结构脑图对脑卒中传播进行建模；在 MULTI-CLARID 数据集上进行实验。

Result: 实现预测准确性提升，得到可解释的生物标记：曲率、分数记忆指标、注意力熵、扩散中心性等；提供一个在神经自律疾病中早期检测、风险分层和可解释多模态建模的理论基础。

Conclusion: 该框架为神经-自律疾病的多模态建模提供一个数学上严谨的基础，有助于早期预警和个性化风险评估。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [40] [gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs](https://arxiv.org/abs/2512.08274)
*Humera Sabir,Fatima Farooq,Ashraf Aboulnaga*

Main category: cs.LG

TL;DR: gHAWK 提出一种可扩展的知识图谱GNN训练框架，通过预计算结构特征来引导训练：使用 Bloom 过滤器编码局部邻域结构，使用 TransE 表示全局位置，将这些特征与领域特征融合，适用于任意GNN，提升内存效率、收敛速度和模型准确性，在 OG benchmarks 上达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有的基于消息传递的GNN在大规模知识图谱上难以扩展，因为迭代消息传递需要大量计算和内存，且小批量训练下节点只能看到不完整的邻域结构，导致学习图结构的困难。需要在训练前就获得结构先验以提升效率和表现。

Method: 在训练前对每个节点进行结构预处理，计算(a)本地邻域结构的Bloom过滤器以紧凑编码局部信息；(b)全局位置的TransE嵌入以捕获全局结构关系。将这些结构特征与领域特征（如文本嵌入）融合，形成可输入到任意GNN的节点特征，结合结构先验来增强消息传递，降低内存占用、加速收敛并提升准确性。

Result: 在Open Graph Benchmark (OGB)的大规模数据集上，gHAWK实现了更高的准确性和更低的训练时间，在节点属性预测和链接预测任务上都达到先进水平，并在三个图上登上OGB榜单第一。

Conclusion: 通过预先计算的结构先验实现对大规模知识图谱的可扩展GNN训练，gHAWK在多任务和多数据集上表现出色，具有良好的通用性与可扩展性。

Abstract: Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.

</details>


### [41] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: 提出一种简单的监督预条件化方法JARF，通过对初始轴对齐森林的预测进行有限差分梯度估计，构造期望雅可比外积作为全局线性旋转矩阵，旋转特征后再交给轴对齐树，提升对斜边界和特征交互的拟合能力，同时保持训练流程简洁高效。


<details>
  <summary>Details</summary>
Motivation: 轴对齐决策树快速、稳定，但在存在旋转或交互特征边界时表现较差；弯曲/斜边界树（oblique forests）通过在节点采用超平面分割来提升，但代价较高且实现复杂；需要一种简单、通用的思路在不显著改变训练管线的前提下增强轴对齐树的表达能力。

Method: 先拟合一个轴对齐森林以估计类别概率或回归输出；对预测结果对每个特征进行有限差分梯度计算，聚合成一个期望的雅可比外积，推广了EGOP的思想；将该外积作为全局线性预条件器对所有输入进行旋转，然后将旋转后的数据交回给标准的轴对齐森林训练。该构造也可应用于任何给出梯度的模型（本文主要针对随机森林和梯度提升树）。

Result: 在分类和回归的表格数据基准上，预条件化方法稳定提升轴对齐森林的性能，且常常达到或超过斜向基线，同时提升训练速度。

Conclusion: 监督预条件化可以在保留轴对齐树的简洁性和鲁棒性的同时，恢复到相当程度的斜向森林的准确性。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [42] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 提出面向联邦学习的平整性约束优化，通过在训练损失的顶特征值约束下，加入一个名为MAN的正则化（在客户端层面最小化激活范数），以降低层级Hessian的最大特征值，进而提升泛化并达到新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中全局模型易收敛到尖锐极小化点，导致泛化性能下降。需要控制优化过程的几何特性（如Hessian的谱半径/前几个特征值）以获得更平坦的损失表面。

Method: 将平整性约束转化为对客户端损失的可计算表述，提出MAN正则化：在客户端对每一层的激活范数进行约束性最小化，并将其与现有FL算法结合，理论上证明激活范数的降低能够减少层级Hessian的Top特征值，从而降低全局Hessian的Top特征值。

Result: 理论上证明激活范数下降能降低局部层Hessian的Top特征值，进而降低全局Hessian的Top特征值，实现更平坦的极小点；在多种FL基线任务中与现有FL算法结合后获得显著提升，达到新状态的泛化性能。

Conclusion: 通过在客户端引入MAN正则化并结合平整性约束的FL优化，可以提升联邦学习的泛化能力并实现SOTA表现，且该方法具有良好的可集成性。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [43] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 提出一种基于多变量伯努利分布的加权抽样算法，考虑标签之间的依赖关系，以获得更平衡的多标签子样本；应用于64个生物医学主题类别的研究文章，提升少数标签的表示。


<details>
  <summary>Details</summary>
Motivation: 多标签数据中标签不互斥且出现频率差异大，直接抽样往往导致少数标签样本不足，难以推断；需要一种能同时考虑标签依赖和目标分布的抽样方法。

Method: 在观测到的标签频率基础上估计多变量伯努利分布参数，计算每个标签组合的权重，并进行加权抽样，以获得具有目标分布特征且考虑标签依赖的样本；应用到Web of Science的64个类别。

Result: 得到更平衡的子样本，提升对少数类别的表示，保持类别频率的顺序，减少最常见与最不常见类别之间的频差。

Conclusion: 基于多变量伯努利的加权抽样能在考虑标签依赖的前提下有效平衡多标签数据集，适用于大规模主题分类等场景。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [44] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process](https://arxiv.org/abs/2512.08451)
*Gary Ackerman,Zachary Kallenborn,Anna Wetzel,Hayley Peterson,Jenna LaTourette,Olivia Shoemaker,Brandon Behlendorf,Sheriff Almakki,Doug Clifford,Noah Sheinbaum*

Main category: cs.LG

TL;DR: A novel Biothreat Benchmark Generation (BBG) framework yields 1,010 final B3 benchmarks for bacterial biosecurity risk assessment, derived from web prompt generation, red teaming, and corpus mining.


<details>
  <summary>Details</summary>
Motivation: To quantify and mitigate biosecurity risks posed by frontier AI (notably LLMs) by developing benchmarks that assess models' biosecurity risk profiles.

Method: Three complementary draft-generation approaches: (1) web-based prompt generation aligned with the Task-Query Architecture; (2) red teaming to uncover risky prompts and scenarios; (3) mining existing benchmark corpora; followed by de-duplication and uplift diagnosticity assessment to produce 1,010 final benchmarks.

Result: Generated over 7,000 candidate benchmarks, narrowed to 1,010 final benchmarks after de-duplication and quality-control steps; the benchmarks are diagnostic, directly relevant to biosecurity threats, and aligned with a broader biosecurity architecture enabling multi-level analysis.

Conclusion: The B3 dataset strengthens the BBG framework by providing a curated, uplift-diagnostic suite of benchmarks to evaluate and compare LLM-based biosecurity risk across threat levels and analytical depth.

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.

</details>


### [45] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: 提出并初步评估用于生物安保风险评估的B3数据集，作为BBG框架的一部分，用于快速、细致地评估大语言模型的生物威胁风险。


<details>
  <summary>Details</summary>
Motivation: 在强大AI模型可能被用于生物恐怖主义的背景下，建立可量化和可操作的生物安保风险基准，帮助模型开发者与 policymaker 量化、比较并缓解风险。

Method: 在Biothreat Benchmark Generation (BBG) 框架下实施B3数据集的试点：将基准应用于前沿AI模型，进行人工评估的结果分析，并对输出进行跨维度的应用性风险分析，以识别风险源并给出缓解优先级的指引。

Result: 试点结果显示B3数据集是一种可行、细化的方法，能够快速评估LLM的生物安保风险，揭示关键风险来源并提供缓解优先级的指导。

Conclusion: B3为快速、细致地评估生物安保风险提供了可操作的路径，并为未来的风险缓解工作与政策制定提供方向，但需在更广泛的模型与数据集上进一步验证与完善。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [46] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: A transformer-based multimodal framework that combines fMRI data with DICOM metadata to decode brain states, leveraging attention to model spatial-temporal patterns and contextual relations, aiming for higher accuracy, interpretability, and robustness.


<details>
  <summary>Details</summary>
Motivation: To improve brain-state decoding from fMRI by incorporating rich DICOM metadata and contextual information that conventional models overlook.

Method: A transformer-based architecture that accepts multimodal inputs (fMRI data + DICOM metadata) with attention mechanisms to capture complex spatial-temporal patterns and contextual relationships.

Result: Demonstrates improved accuracy, interpretability, and robustness; discusses potential clinical and neuroscience applications; addresses limitations like metadata variability and computational demands.

Conclusion: The framework is promising for clinical diagnostics and personalized medicine; future work should focus on scalability, generalizability, and efficient handling of metadata variability.

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [47] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 提出了一种全局预算分配攻击（Global Budget Allocation, GBA），通过将扰动分配与TD误差敏感性成正比，在全局L2预算下实现最优扰动分配，显著提高离线强化学习对数据 poisoning 的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 离线RL在静态数据集上进行策略优化，但易受数据投毒攻击影响。现有攻击多使用局部均匀扰动，低效且易被检测，迫切需要高效且隐蔽的攻击方法。

Method: 将样本对值函数收敛的影响与其TD误差成正比的理论洞见转化为全局资源分配问题，推导出闭式解：扰动量法向TD误差敏感性分配，在全局L2约束下实现最优分配。

Result: 在D4RL基准上，GBA方法相较基线攻击显著提升攻击效果，在最小扰动下实现高达80%的性能下降，并具备对统计和光谱防御的规避性。

Conclusion: 提出的全局预算分配攻击在离线RL的数据投毒场景中更高效、更隐蔽，揭示了基于TD误差的影响权重在攻击设计中的有效性；未来工作可扩展到其他鲁棒性评价和防御策略。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [48] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 提出一个基于长序列的 LSTM 框架来预测 NBA 比赛结果，使用覆盖 2004-05 至 2024-25 的纵向数据集，序列长度达 8 个完整赛季约 9,840 场比赛，并在与逻辑回归、随机森林、MLP、CNN 等基线对比中实现最佳性能（准确率 72.35、精确度 73.15、AUC-ROC 76.13），强调长期时序建模的重要性与多赛季数据集的价值。


<details>
  <summary>Details</summary>
Motivation: 解决概念漂移、时序信息不足以及跨赛季稳定性的问题，在篮球结果预测中需要更丰富的时间上下文以捕捉球队动态与赛季间依赖。提供一个大规模纵向数据集以促进鲁棒、可泛化的 NBA 预测系统的开发。

Method: 构建一个 Long Short-Term Memory (LSTM) 模型，处理长序列输入，序列长度等同于 9,840 场比赛（约八个赛季），以捕捉球队动态与跨赛季依赖。与传统 ML/DL 基线（逻辑回归、随机森林、MLP、CNN）进行对比。数据来自新构建的纵向 NBA 数据集。

Result: 提出的 LSTM 在所有评估指标上均优于基线：准确率 72.35、精确度 73.15、AUC-ROC 76.13，显示对长序列时序建模在篮球结果预测中的重要性，并强调新数据集在开发鲁棒、可泛化的 NBA 预测系统方面的价值。

Conclusion: 长时间序列建模对篮球结果预测具有显著优势，所提供的多赛季纵向数据集有助于提升模型的鲁棒性与泛化能力，为后续在策略、分析和博彩等场景中的应用提供坚实数据基础。

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [49] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 提出了衰减步长框架 DS FedProxGrad，针对带局部近似解和明确公平正则化的非凸联邦学习问题，证明算法在渐近意义上达到驻点，梯度范数的期望降至零且收敛不再受噪声地板影响。


<details>
  <summary>Details</summary>
Motivation: 解决原 FedProxGrad 只收敛到噪声主导邻域的问题，在带公平性正则化的非凸分布式优化场景中寻求更强的渐近稳定性。

Method: 引入 DS FedProxGrad（Decay Step Size FedProxGrad）框架，采用 Robbins–Monro 型衰减步长，并对局部近似误差设定温和衰减条件，允许显式的公平性正则化，给出渐近性分析。

Result: 在给定假设下证明 liminf_{r→∞} E[||∇F(x^r)||^2] = 0，算法具备渐近驻点性质，且收敛速率不受方差引起的噪声地板影响。

Conclusion: DS FedProxGrad 提供了对带有公平性正则化的非凸分布式优化的更强理论保障，适用于需要渐近稳定性的联邦学习场景，理论框架可推广到更广的非凸优化问题。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [50] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 基于动态时间规整(DTW)的迁移学习框架用于增材制造部件资质验证，通过在聚合物源域与金属目标域之间进行知识迁移，利用DTW选择最相关的聚合物数据集并用LSTM预测应力-应变行为，从而提升三种金属的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决AM部件在不同材料之间难以获得足够实验数据的问题，利用成本较低的聚合物数据跨域迁移到金属，以提高对金属部件的性能预测和资质验证的效率与可靠性。

Method: 用DTW从四种聚合物数据中选出最接近目标金属数据的单一源域数据集；在LSTM框架下进行迁移学习，对三种目标金属（AlSi10Mg、Ti6Al4V、碳钢）进行建模，比较未迁移、基于四聚合物预训练、以及DTW选择源域的TL三种策略。

Result: DTW-TL在三金属目标域下实现MAPE为12.41%、R^2达到0.96，优于未迁移的LSTM和基于四聚合物数据预训练的TL模型。

Conclusion: 通过DTW精确锁定最相关的源域数据，DTW-TL显著提升了AM部件资质预测的准确性与稳定性，为在材料跨度较大时的跨域迁移学习提供一条有效路径。

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [51] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: BGPS automatically searches prompts to uncover biases in text-to-image diffusion, revealing untapped biases and providing an evaluation tool for debiasing.


<details>
  <summary>Details</summary>
Motivation: Mitigate evaluation bias in TTI models without relying on curated prompts; reveal unanticipated prompts that trigger biases and assess debiasing effectiveness.

Method: Two-component approach: (1) an LLM instructed to produce attribute-neutral prompts; (2) attribute classifiers applied to the model's internal representations guide the LLM decoding toward prompt regions that amplify target attributes, enabling bias discovery.

Result: Experimentally uncovers subtle, previously undocumented biases in Stable Diffusion 1.5 and a debiased model; prompts are interpretable and can improve perplexity compared to hard-prompt baselines; demonstrates TTI vulnerabilities and provides a new evaluation tool for bias mitigation.

Conclusion: BGPS broadens the bias search space, serving as both a bias discovery method and an evaluation framework for debiasing efforts in TTI diffusion models.

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [52] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 使用神经常微分方程（NODE）来建模蛋白质组与代谢组之间的连续时间动力学，并在工程化大肠杆菌的时间序列数据上取得显著预测和推理加速的结果。


<details>
  <summary>Details</summary>
Motivation: 在高维组学数据日益丰富的背景下，传统机制模型受先验知识限制，难以从观测数据中直接推断潜在相互作用并预测干预效应。需要高容量、数据驱动的动态模拟框架以实现对系统时序的预测与个性化干预。

Method: 将 NODEs 作为动力学框架，学习蛋白组-代谢组的连续时间动态，应用于工程化大肠杆菌的时间序列数据，建模代谢通路的连续演化。

Result: 相较于传统机器学习管线，NODE 在捕捉系统动力学方面表现更优；在 Limonene 和 Isopentenol 路径数据集上，RMSE 相对基线提高超过 90%，分别达到最高约 94.38% 与 97.65% 的改善；推理时间实现约 1000 倍加速。

Conclusion: NODE 作为一种可扩展且高保真度的工具，推动代谢工程与生物发现领域的下一代高容量数据驱动建模与预测。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [53] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: 提出 LEAP，通过在所有节点添加通用图提示并结合 actor-critic 强化学习来选择节点和编辑提示，以在多任务与少样本场景中超越微调与其他提示方法，同时保持 universal graph prompt tuning 的理论基础。


<details>
  <summary>Details</summary>
Motivation: 早期图提示方法多为任务特定设计，难以跨不同预训练策略迁移；普适的通用图提示理论强调在输入图特征空间实现等效提示函数，但对选择性节点提示可能破坏理论基础。

Method: 建立严格约束，证明要实现 universality，必须对所有节点添加提示；提出 LEAP，先构建基础通用图提示以保留理论基础，再通过 actor-critic 强化学习选择节点并编辑提示。

Result: 在多种预训练策略下的图级和节点级任务，以及全样本与少样本设置中，LEAP 均显著优于微调和其他提示方法。

Conclusion: LEAP 在保持通用图提示理论基礎的同时，通过全面节点提示与学习式提示编辑，提供更理想的提示，并提升性能。

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [54] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [55] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [56] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: TD(λ) with function approximation can lead to suboptimal policies due to emphasis on value accuracy rather than policy ordering; STD(λ) focuses on relative state values and can guarantee monotonic policy improvement in simple cases, with promising empirical results on simple and control tasks.


<details>
  <summary>Details</summary>
Motivation: In policy learning, the critical objective is the correct ordering of states by value, not merely minimizing squared error of state values. Standard TD(λ) may converge to suboptimal policies despite good value estimates.

Method: Introduce STD(λ), a modification of TD(λ) that trains function approximators with respect to relative state values on binary decision problems. Provide theoretical analysis including a proof of monotonic policy improvement for STD(λ) in the two-state system and compare with Bertsekas' differential training method. Demonstrate STD(λ) on the two-state system and on a variation of the acrobot problem, as well as briefly on backgammon.

Result: Theoretical result: monotonic policy improvement proven for STD(λ) in the two-state context. Empirical demonstrations show STD(λ) achieving policy improvements in the two-state system and in the acrobot variation, with additional insights from backgammon. The approach is positioned as an alternative to standard TD(λ) and to Bertsekas' differential training, with promising initial results.

Conclusion: STD(λ) offers a principled way to align training with policy quality by optimizing relative state values, yielding monotonic improvement in simple cases and encouraging further exploration on more complex problems such as backgammon and other binary decision tasks.

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [57] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 通过引入基于加速度的二阶损失L_acc，对扩散模型进行微调，得到面向IMU传感的文本到运动合成的先验，提升IMU数据的保真度与HAR性能。


<details>
  <summary>Details</summary>
Motivation: IMU数据具有特定的加速度模式，常规生成模型的扩散先验与IMU信号不一致，需通过加速度感知的约束来对齐生成的运动与IMU信号。

Method: 在预训练扩散模型的训练目标中加入基于二阶时间差的加速度损失L_acc，对模型进行微调，形成IMU特定的运动先验；结合文本到IMU框架（包括表面建模与虚拟传感器仿真）进行评估，并分析加速度信号保真度及合成表示与实际IMU记录的差异。

Result: 将L_acc加入后相对于原模型，L_acc下降12.7%（相对降低）。在高动态活动（跑步、跳跃）中的改进尤为显著；低维嵌入显示合成IMU数据的分布更接近真实IMU记录；在HAR任务中，仅使用改进的IMU数据进行训练，分类性能较早期扩散模型提升8.7%，较最佳对比扩散模型提升7.6%。

Conclusion: 基于加速度信息的扩展式扩散微调能够有效将通用文本到运动先验定制为传感器特定任务，提升IMU合成的保真度并展示了深度学习流程在传感器领域的柔性适应能力。

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [58] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: LLM-based tabular data generation can memorize and leak numeric digits from training data; a No-box Membership Inference Attack (LevAtt) exposes leakage across models; defenses including digit-perturbation sampling can mitigate with minimal utility loss.


<details>
  <summary>Details</summary>
Motivation: 揭示基于大型语言模型的表格数据生成中的隐私风险，尤其是对训练数据中的数字序列的潜在记忆与再现。

Method: 提出无盒（No-box）成员辨识攻击 LevAtt，专注于生成数据中的数字字符串，通过对比不同模型和数据集评估泄露程度；并提出两种防御方案，其中一种是通过在生成时有策略地扰动数字的采样方法。

Result: 攻击在广泛模型和数据集上揭示显著的隐私泄露；在某些情况下对先进模型可达到完美的成员分类器效果；所提防御在几乎无显著的保真度损失下能够抵御这些攻击。

Conclusion: LLM驱动的合成数据生成存在独特的隐私脆弱性，需有效防御。提出的数字扰动采样等方法在保持数据实用性的前提下提供了可行的缓解手段。

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [59] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: 引入一种面向漂移感知的在线高斯过程模型DAO-GP，具备自适应、无超参数、带衰减和稀疏性的非线性回归能力，能够检测并适应数据分布的漂移，在不同漂移类型下仍保持鲁棒性并优于或接近现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常伴随随时间演变的分布漂移；传统在线高斯过程在固定超参数、缺乏漂移感知、记忆与衰减管理不足等方面表现不佳，难以在动态环境中保持预测准确性和不确定性量化。需要一个完全自适应、无超参数、带衰减机制且具备稀疏表示的在线非线性回归模型来应对漂移。

Method: 提出DAO-GP，一种具有内置漂移检测与自适应机制的在线高斯过程框架；通过显式的漂移严重程度来动态调整模型行为；采用衰减的记忆和 evolves 的诱发点（inducing points）以实现内存友好和自适应稀疏性；实现超参数无关；在在线场景中实现非线性回归与不确定性量化。

Result: 在稳态与多种漂移情形（突变、渐进、渐变）下进行广泛的实证评估，DAO-GP 展现出对漂移的鲁棒性、动态自适应能力及高效的内存与衰减管理；与最先进的参数化和非参数模型相比，表现出一致的最佳或具竞争力的性能。

Conclusion: DAO-GP 为在线非线性回归提供了一个对漂移具高度鲁棒性的解决方案，具有自适应、无超参数、带衰减的稀疏表示与不确定性量化能力，优于或媲美现有方法，且更适合现实世界的动态数据环境。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [60] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 提出一种基于拓扑数据分析的无监督带宽选择方法，用拓扑损失函数自动优化核密度估计的带宽，并在多维数据上与经典方法进行基准比较。


<details>
  <summary>Details</summary>
Motivation: 核密度估计的带宽是关键超参数，直接影响偏差-方差权衡和拓扑特征的平滑程度；现有方法多依赖有监督信息或经验规则，缺乏无监督的自动选择机制。拓扑学提供量化高维拓扑特征的方法，可用于指导带宽的自适应调整。

Method: 提出一种基于拓扑数据分析的损失函数，用于无监督地优化核密度估计的带宽。通过计算估计密度的拓扑不变量（如持久性同调特征）在不同带宽下的稳定性或一致性，将损失函数最小化以得到最优带宽，并在多个维度上与传统带宽选择方法进行对比。

Result: 该方法建立了一个可在无监督情形下工作的带宽选择框架，并在与经典技术的基准比较中展示出潜在优势，尤其在多维场景中表现出对拓扑结构更一致的描述能力。

Conclusion: 基于拓扑的损失函数为无监督带宽选择提供了一条可行途径，能够自动调整带宽以更好地保留数据的拓扑特征并提升 KDE 的表现。

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [61] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: Open Polymer Challenge introduces a 10k-polymer benchmark for multi-task property prediction and provides data, code, and insights to advance polymer informatics.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of large, high-quality, openly accessible polymer datasets and enable robust ML models under realistic constraints such as small data, label imbalance, and heterogeneous simulations.

Method: Community-built dataset with 10k polymers across five properties; multi-task learning for polymer property prediction; competition setting highlights data preparation, distribution shifts, and cross-group simulation consistency; techniques include feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensembles; data generation pipeline ADEPT simulates >25 properties; release of test dataset and pipeline.

Result: Established the first community benchmark for polymer informatics; released test data and a simulation pipeline, enabling broad participation and reproducible research; provided practical lessons on data curation, distribution shifts, and cross-group consistency that inform best practices for large-scale polymer datasets; sets the groundwork for developing more capable molecular AI models in polymer science.

Conclusion: This work creates a foundational benchmark and resources that are expected to accelerate sustainable and energy-efficient polymer materials discovery through improved data practices and robust ML methods.

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [62] [Signal and Noise Classification in Bio-Signals via unsupervised Machine Learning](https://arxiv.org/abs/2512.07851)
*Sansrit Paudel*

Main category: eess.SP

TL;DR: 利用K-means聚类将生物信号分为干净与有噪声两类，并进一步对不同噪声类型（如运动伪影、传感器故障等）进行分类，以实现只选取高质量信号的特征工程。


<details>
  <summary>Details</summary>
Motivation: 现实世界的生物信号常被运动伪影、基线漂移等噪声污染，导致信号降解严重。需要能够自动识别干净与有噪声的片段，并对噪声类型进行分类，以提高后续机器学习模型的性能。

Method: 对生物信号数据应用K-means聚类，进行二分类（干净 vs 有噪声）以及对噪声类型的分类评估，重点评估在识别干净数据方面的性能。

Result: K-means在将干净片段与噪声片段分离上表现出较高的可靠性，尤其在识别干净数据方面表现优于对各类噪声的分类效果。该方法有助于筛选高质量信号片段，并为特征工程提供更准确的输入以提升下游模型的精度。

Conclusion: K-means聚类是一种有效的信号质量筛选工具，能够在生物信号处理中区分干净与有噪声片段，并为后续机器学习模型提供更高质量的数据。

Abstract: Real-world biosignal data is frequently corrupted by various types of noise, such as motion artifacts, and baseline wander. Although digital signal processing techniques exist to process such signals; however, heavily degraded signals cannot be recovered. In this study, we aim to classify two things: first, a binary classification of noisy and clean biosignals, and next, to categorize various kinds of noise such as motion artifacts, sensor failure, etc. We implemented K-means clustering, and our results indicate that the algorithm can most reliably group clean segments from noisy ones, particularly strong performance in identifying clean data compared to various categories of noise. This approach enables the selection of only high-quality bio-signal segments and provides accurate results for feature engineering that may enhance the precision of machine learning models trained on biosignals.

</details>


### [63] [Polarization-Diversity-Based Rotation Sensing Methodology Using COTS UHF RFID Tags](https://arxiv.org/abs/2512.08069)
*Florian Muralter,Fabian Muralter,Hugo Landaluce,Asier Perallos*

Main category: eess.SP

TL;DR: 通过极化多样性实现基于UHF RFID的旋转感应，利用COTS标签和SDR读写器，在完全相干解调后对回波调制差分信号进行相位分析以估计旋转速度，实验验证理论模型并评估系统性能与局限性。


<details>
  <summary>Details</summary>
Motivation: 实现低成本、可扩展的旋转感知能力，利用UHF RFID的相位信息和极化多样性来估计对象的旋转速度；同时探索使用COTS标签和SDR来降低系统复杂度与成本。

Method: 在完全相干解调后，利用标签到读写器的回波信息，计算背散射负载调制状态的差分信号；对该差分信号的相位随时间的变化进行分析，以推导并估计旋转速度。研究利用共振极化分离实现极化多样性以增强鲁棒性。

Result: 理论模型得到实验结果的支撑，实验评估了所提出系统的性能与局限性，验证了该方法在COTS UHF RFID环境中的可行性，并给出对系统在不同条件下的响应与限制。

Conclusion: 基于极化多样性的UHF RFID旋转感应方法在成本和实现复杂度方面具备优势，适用于IoT场景的转速检测，但受限于噪声、标签姿态变化、以及外部干扰等，需要对解调鲁棒性和极化配置进行进一步优化。

Abstract: Phase-based sensing using ultra-high frequency (UHF) radio-frequency identification (RFID) has, in recent years, yielded numerous additions to the Internet of Things (IoT). This work presents a polarization diversity-based rotation sensing methodology using common-off-the-shelf (COTS) UHF RFID tags identified with a software-defined radio (SDR) UHF RFID reader. The proposed methodology uses the tag-to-reader message after fully coherent demodulation to calculate a difference signal of the backscatter load modulation states. This sequence is then used to compute the rotation speed by evaluating its phase change over time. Experimental results are used to validate the theoretical model and to evaluate the performance and limitations of the proposed system.

</details>


### [64] [Metasurfaces Enable Active-Like Passive Radar](https://arxiv.org/abs/2512.08208)
*Mingyi Li,Jiawen Xu,Hanting Zhao,Xu Zhao,Yan Jin Chen,Tie Jun Cui,Vincenzo Galdi,Lianlin Li*

Main category: eess.SP

TL;DR: 提出一种基于太赫兹/微波的被动雷达新思路：通过空间-时间编码可编程超表面在环境电磁场上叠加独特的时空标签，使被动雷达具备类似主动雷达的探测能力，同时能抑制干扰、增强信号并实现高效目标定位和跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决传统被动雷达对非合作源波形的先验知识依赖、对强干扰的易受干扰以及对多普勒信号的依赖，从而在复杂环境中检测微弱或缓慢移动目标。

Method: 引入时空编码可编程超表面，编排环境中的电磁场以刻印稀有的时空标签，使被动雷达获得主动雷达的感知能力（无源控制源的前提下），实现干扰抑制、信号增强及目标定位/跟踪。

Result: 在5.48 GHz 的原型系统中实现对无人机在干扰环境中的实时成像与跟踪，性能与主动雷达相当。

Conclusion: MEPR 为可扩展、适应性强且低功耗的下一代集成感知与通信（ISAC）系统奠定了理论与实验基础。

Abstract: Passive radars (PRs) provide a low-cost and energy-efficient approach to object detection by reusing existing wireless transmissions instead of emitting dedicated probing signals. Yet, conventional passive systems require prior knowledge of non-cooperative source waveforms, are vulnerable to strong interference, and rely on Doppler signatures, limiting their ability to detect subtle or slow-moving targets. Here, we introduce a metasurface-enabled PR (MEPR) concept that integrates a space-time-coding programmable metasurface to imprint distinct spatiotemporal tags onto ambient wireless wavefields. This mechanism transforms a PR into an active-like sensing platform without the need for source control, enabling interference suppression, signal enhancement, and accurate target localization and tracking in cluttered environments. A proof-of-concept implementation operating at 5.48 GHz confirms real-time imaging and tracking of unmanned aerial vehicles under interference-rich conditions, with performance comparable to active radar systems. These results establish MEPR as a solid foundation for scalable, adaptive, and energy-efficient next-generation integrated sensing and communication systems.

</details>


### [65] [1024-Channel 0.8V 23.9-nW/Channel Event-based Compute In-memory Neural Spike Detector](https://arxiv.org/abs/2512.08244)
*Ye Ke,Zhengnan Fu,Junyi Yang,Hongyang Shang,Arindam Basu*

Main category: eess.SP

TL;DR: 提出事件驱动的Spike Detection（Ev-SPD）算法以实现可扩展的压缩型事件驱动前端，在低功耗IMC中实现高通道数的SPD。


<details>
  <summary>Details</summary>
Motivation: 随着 intracortical brain–machine interfaces (iBMIs) 通道数激增，传统模拟/数字 SPD 的缓冲与内存访问功耗显著增加，且现有的Spike增强方法与事件驱动前端不兼容，需低功耗、高效且可与EBF对接的SPD方案。

Method: 提出 Ev-SPD 算法，并设计一种新型低功耗 10-T eDRAM-SRAM 混合随机存取内存位单元用于事件处理的内存就地计算（IMC），并在65nm工艺实现了1024通道的IMC SPD宏。对合成数据和 Neuropixel 记录进行测试。

Result: 在合成数据集上Spike检测准确度达到 96.06%，在Neuropixel记录上相似度 95.08%，发放模式 MAE 为 0.05；功耗方面实现 23.9 nW/通道的单位能耗与 375 μm^2/通道的面积效率。

Conclusion: 所提出的 SPD 方案与压缩型 EB F 兼容，利用 IMC 架构实现极低功耗且保持较高准确性，适合高密度 iBMI 的事件驱动前端。

Abstract: The increasing data rate has become a major issue confronting next-generation intracortical brain-machine interfaces (iBMIs). The scaling number of recording sites requires complex analog wiring and lead to huge digitization power consumption. Compressive event-based neural frontends have been used in high-density neural implants to support the simultaneous recording of more channels. Event-based frontends (EBF) convert recorded signals into asynchronous digital events via delta modulation and can inherently achieve considerable compression. But EBFs are prone to false events that do not correspond to neural spikes. Spike detection (SPD) is a key process in the iBMI pipeline to detect neural spikes and further reduce the data rate. However, conventional digital SPD suffers from the increasing buffer size and frequent memory access power, and conventional spike emphasizers are not compatible with EBFs. In this work we introduced an event-based spike detection (Ev-SPD) algorithm for scalable compressive EBFs. To implement the algorithm effectively, we proposed a novel low-power 10-T eDRAM-SRAM hybrid random-access memory in-memory computing bitcell for event processing. We fabricated the proposed 1024-channel IMC SPD macro in a 65nm process and tested the macro with both synthetic dataset and Neuropixel recordings. The proposed macro achieved a high spike detection accuracy of 96.06% on a synthetic dataset and 95.08% similarity and 0.05 firing pattern MAE on Neuropixel recordings. Our event-based IMC SPD macro achieved a high per channel spike detection energy efficiency of 23.9 nW per channel and an area efficiency of 375 um^2 per channel. Our work presented a SPD scheme compatible with compressive EBFs for high-density iBMIs, achieving ultra-low power consumption with an IMC architecture while maintaining considerable accuracy.

</details>


### [66] [Delay-Oriented Distributed Scheduling with TransGNN](https://arxiv.org/abs/2512.08799)
*Boxuan Wen,Junyu Luo*

Main category: eess.SP

TL;DR: 提出面向时延的分布式调度框架，利用 Transformer GNN 的注意力编码器来产生每条链路的自适应效用，并通过局部贪心求解器实现冲突约束的独立集调度。


<details>
  <summary>Details</summary>
Motivation: 传统高吞吐量导向的调度在时延方面表现欠佳，尤其在异构/动态拓扑中；现有 GNNs 的局部聚合限制了长程干扰建模；需要能同时建模队列背压和干扰并实现分布式调度的方法。

Method: 使用基于注意力的图编码器（Transformer GNN）生成反映队列拥塞和干扰强度的自适应效用分数，对链接进行排序；局部贪心求解器选择一个可行的独立集进行传输，确保分布式且无冲突。

Result: 论文未给出具体实验结果；若有结构化结果，可写：在理论和实例评估中，该框架在时延降低与吞吐平衡方面优于传统调度和基于局部聚合的 GNN 方法，且具备分布式特性。

Conclusion: 引入 Transformer GNN 的延时导向调度框架，能够更好地捕获长程干扰依赖并在分布式环境中实现低时延的可行调度。

Abstract: Minimizing transmission delay in wireless multi-hop networks is a fundamental yet challenging task due to the complex coupling among interference, queue dynamics, and distributed control. Traditional scheduling algorithms, such as max-weight or queue-length-based policies, primarily aim to optimize throughput but often suffer from high latency, especially in heterogeneous or dynamically changing topologies. Recent learning-based approaches, particularly those employing Graph Neural Networks (GNNs), have shown promise in capturing spatial interference structures. However, conventional Graph Convolutional Networks (GCNs) remain limited by their local aggregation mechanism and their inability to model long-range dependencies within the conflict graph. To address these challenges, this paper proposes a delay-oriented distributed scheduling framework based on Transformer GNN. The proposed model employs an attention-based graph encoder to generate adaptive per-link utility scores that reflect both queue backlog and interference intensity. A Local Greedy Solver (LGS) then utilizes these utilities to construct a feasible independent set of links for transmission, ensuring distributed and conflict-free scheduling.

</details>


### [67] [Geometry-Aligned Differential Privacy for Location-Safe Federated Radio Map Construction](https://arxiv.org/abs/2512.08263)
*Jijia Tian,Wangqian Chen,Junting Chen,Pooi-Yuen Kam*

Main category: eess.SP

TL;DR: 提出一种几何对齐的差分隐私机制，用于电台地图学习，基于梯度的空间结构注入异质噪声以混淆定位，同时尽量保留地图精度；理论上给出隐私与学习精度的收敛保证，实验证明攻击者定位误差显著提升。


<details>
  <summary>Details</summary>
Motivation: 在分布式无线信号强度建图库（无线电地图）时收集的位置信息存在隐私风险，即使原始数据只在本地，模型更新也可能通过梯度的空间结构泄露位置信息。单纯的噪声注入要么无法有效防泄露，要么显著降低模型精度。

Method: 提出一种与几何结构对齐的差分隐私机制，采用针对梯度的异质噪声，以同时混淆攻击者的定位并覆盖梯度的空间模式；给出理论收敛性保证，将隐私强度与学习准确度联系起来；并通过数值实验验证性能。

Result: 攻击者的定位误差从约30米提升至超过180米；与统一噪声基线相比，无线电地图构建误差仅增加约0.2 dB。

Conclusion: 几何感知的差分隐私对无线电地图的隐私保护有效实现，能在可控的精度权衡下提升隐私性，并具备理论保证与经验验证。

Abstract: Radio maps that describe spatial variations in wireless signal strength are widely used to optimize networks and support aerial platforms. Their construction requires location-labeled signal measurements from distributed users, raising fundamental concerns about location privacy. Even when raw data are kept local, the shared model updates can reveal user locations through their spatial structure, while naive noise injection either fails to hide this leakage or degrades model accuracy. This work analyzes how location leakage arises from gradients in a virtual-environment radio map model and proposes a geometry-aligned differential privacy mechanism with heterogeneous noise tailored to both confuse localization and cover gradient spatial patterns. The approach is theoretically supported with a convergence guarantee linking privacy strength to learning accuracy. Numerical experiments show the approach increases attacker localization error from 30 m to over 180 m, with only 0.2 dB increase in radio map construction error compared to a uniform-noise baseline.

</details>


### [68] [Self-Alignment Resonant Beam Empowers Beamforming without Estimation and Control for 6G IoT](https://arxiv.org/abs/2512.08386)
*Yixuan Guo,Mingliang Xiong,Qingwen Liu*

Main category: eess.SP

TL;DR: RF-RBS through retro-directive antenna arrays enables self-aligning, high-gain beamforming with positive feedback, eliminating digital CSI processing and over-the-air beam scanning; enables efficient WPT, robust communication, and passive positioning in 6G IoT, with analysis of architecture and integration challenges.


<details>
  <summary>Details</summary>
Motivation: 在动态场景中，传统 beamforming 需要大量的 CSI 估计与信道扫描，造成不可接受的开销和延迟。通过 RF resonant beam system，利用自反馈环路实现自对准和高增益，潜在地降低覆盖与时延要求，提升能源供给、通信鲁棒性与定位能力。

Method: 综述性研究，分析 RF-RBS 的体系结构，聚焦 retro-directive antenna arrays（RAA）在发射端/接收端形成自回路的机制；评估其在高效 WPT、鲁棒通信和毫米级被动定位方面的能力；讨论在延迟敏感的 6G 场景中的实现挑战与策略价值。

Result: RF-RBS 通过正反馈实现自对准的高增益波束成形，避免数字 CSI 处理，提升能源传输与通信鲁棒性，并实现毫米级被动定位。文章对体系结构、功能能力、实现挑战及在无人系统与工业自动化等场景的潜在价值进行了系统评估。

Conclusion: RF-RBS 为 6G 物联网中通信、感知与无线供能的一体化提供了有前景的原理性范式，能够降低开销与时延、提升鲁棒性；但仍需解决现实部署中的耦合、组网、兼容性与成本等挑战以实现大规模应用。

Abstract: The integration of communication, sensing, and wireless power transfer (WPT) is a cornerstone of 6G intelligent IoT. However, relying on traditional beamforming imposes prohibitive overheads due to complex channel state information (CSI) estimation and active beam scanning, particularly in dynamic environments. This paper presents a comprehensive review of the radio frequency resonant beam system (RF-RBS), a native physical-layer paradigm that circumvents these limitations. By deploying retro-directive antenna arrays (RAA) at transceivers, RF-RBS establishes a self-sustaining cyclic electromagnetic loop. This mechanism inherently enables self-aligning, high-gain beamforming through positive feedback, eliminating the reliance on digital CSI processing. We analyze the system's architecture and its capability to support high-efficiency WPT, robust communication, and millimeter-level passive positioning. Finally, we evaluate the implementation challenges and strategic value of RF-RBS in latency-sensitive 6G scenarios, including unmanned systems and industrial automation.

</details>


### [69] [Hybrid Fuzzy Logic and Shading-Aware Particle Swarm Optimization for Dynamic Photovoltaic Shading Faults Mitigation](https://arxiv.org/abs/2512.08419)
*F. Philibert Andriniriniaimalaza,Nour Mohammad Murad,George Balan,Habachi Bilal,Nirilalaina Randriatefison,Abdel Khoodaruth,Charles Bernard Andrianirina,Blaise Ravelo*

Main category: eess.SP

TL;DR: 提出一种将模糊逻辑控制（FLC）与遮蔽感知粒子群优化（SA-PSO）相结合的混合框架，用于在部分遮蔽（20%-80%）和全遮蔽情形下实现全局最大功率点跟踪（GMPP），提高光伏系统的鲁棒性与能量产出。


<details>
  <summary>Details</summary>
Motivation: 遮蔽损害光伏系统性能，降低输出并干扰最大功率点跟踪，需要更鲁棒、快速且具全局搜索能力的控制与优化策略。

Method: 将模糊逻辑控制（FLC）用于基于遮蔽模式的快速决策，使用遮蔽感知粒子群优化（SA-PSO）加速全局搜索并避免陷入局部极小值，形成一种动态自适应的混合控制/优化框架。

Result: 与传统的 Perturb and Observe（P&O）算法相比，功率输出提升可达11.8%，跟踪时间降低约62%，在部分遮蔽与全遮蔽情境下均展现出更稳定的接近GMPP的能力。

Conclusion: 将智能控制与遮蔽感知优化相结合，显著提升在复杂真实条件下的鲁棒性与光伏系统的能源产出。

Abstract: Shading faults remain one of the most critical challenges affecting photovoltaic (PV) system efficiency, as they not only reduce power generation but also disturb maximum power point tracking (MPPT). To address this issue, this study introduces a hybrid optimization framework that combines Fuzzy Logic Control (FLC) with a Shading-Aware Particle Swarm Optimization (SA-PSO) method. The proposed scheme is designed to adapt dynamically to both partial shading (20%-80%) and complete shading events, ensuring reliable global maximum power point (GMPP) detection. In this approach, the fuzzy controller provides rapid decision support based on shading patterns, while SA-PSO accelerates the search process and prevents the system from becoming trapped in local minima. A comparative performance assessment with the conventional Perturb and Observe (P\&O) algorithm highlights the advantages of the hybrid model, showing up to an 11.8% improvement in power output and a 62% reduction in tracking time. These results indicate that integrating intelligent control with shading-aware optimization can significantly enhance the resilience and energy yield of PV systems operating under complex real-world conditions.

</details>


### [70] [Aliasing in Near-Field Array Ambiguity Functions: a Spatial Frequency-Domain Framework](https://arxiv.org/abs/2512.08469)
*Gilles Monnoyer,Jérôme Louveaux,Laurence Defraigne,Baptiste Sambon,Luc vandendorpe*

Main category: eess.SP

TL;DR: 提出一个普遍的近场（NF）异常镜像峰（grating lobes）分析框架，用以解释极大阵列（XL-array）在近场中的别名效应，利用局部空间频率分析将 NF grating lobes 表现为别名伪影，给出设计准则以避免别名区域，并给出对均匀线性阵列（ULA）和均匀圆阵列（UCA）的解析无别名区域的闭式表达。


<details>
  <summary>Details</summary>
Motivation: 极大阵列在近场工作带来前所未有的空间分辨率和新功能，但陣列尺寸增大导致很难理解和控制近场的 grating lobes（别名峰）。薄化阵列（去除元件）引入更复杂的格点结构，且现有工作多依赖针对特定阵列几何的近似。需要一个通用框架揭示近场别名峰的本质、几何规律，并给出跨几何的设计指南。

Method: 提出基于局部空间频率分析（对定向信号的局部相位-频率属性的分析）的方法，将近场 grating lobes 视为 aliasing artifacts。构建一个系统性的方法来建模 NF 中的别名峰在自相关函数/模糊度函数上的结构，量化其形态，并给出跨阵列几何的设计原则，使 XL 阵列在别名安全区域内工作。并将这一框架与传统远场原理相连接。

Result: 建立了一个可移植的理论框架，揭示 NF grating lobes 的起源与几何行为，提供从理论到设计的转化路径；推导出适用于典型阵列几何（如 ULA、UCA）的别名无区域的闭式表达式。

Conclusion: 该方法为 XL 阵列在近场条件下的角度-距离解耦和定位/定位鲁棒性等应用提供了可操作的设计准则，能在保持性能的同时避免别名效应，且与远场原则互相印证。

Abstract: Next-generation communication and localization systems increasingly rely on extremely large-scale arrays (XL-arrays), which promise unprecedented spatial resolution and new functionalities. These gains arise from their inherent operation in the near field (NF) regime, where the spherical nature of the wavefront can no longer be ignored; consequently, characterizing the ambiguity function--which amounts to the matched beam pattern-- is considerably more challenging. Implementing very wide apertures with half-wavelength element spacing is costly and complex. This motivates thinning the array (removing elements), which introduces intricate aliasing structures, i.e., grating lobes. Whereas prior work has addressed this challenge using approximations tailored to specific array geometries, this paper develops a general framework that reveals the fundamental origins and geometric behavior of grating lobes in near-field ambiguity functions. Using a local spatial-frequency analysis of steering signals, we derive a systematic methodology to model NF grating lobes as aliasing artifacts, quantifying their structure on the AF, and providing design guidelines for XL-arrays that operate within aliasing-safe regions. We further connect our framework to established far-field principles. Finally, we demonstrate the practical value of the approach by deriving closed-form expressions for aliasing-free regions in canonical uniform linear arrays and uniform circular arrays.

</details>


### [71] [LoS+NLoS Holographic MIMO: Analysis and Application of Wavenumber-Division Multiplexing](https://arxiv.org/abs/2512.08509)
*Ashutosh Prajapati,Prathapasinghe Dharmawansa,Marco Di Renzo,Italo Atzeni*

Main category: eess.SP

TL;DR: 提出了一种统一的 LoS+NLoS holographic MIMO 通道模型，并将 WDM 拓展到 LoS+NLoS 场景；对 NLoS 得到角域表示和闭式解，给出对各向同性与非各向同性散射的解析表述，最后在自由度与遍历容量上显示 NLoS 的显著提升。


<details>
  <summary>Details</summary>
Motivation: 需要一个物理一致的近场全息 MIMO 通道模型来同时刻画 LoS 与 NLoS 成分，克服传统仅 LoS 或环境特定多径模型的局限，从而更准确地评估近场大面积连续天线阵列的性能。

Method: 提出一个统一的通道表示，结合空间采样（spatial-sampling）与展开式（expansion-based）两种表述；在此基础上将波数除法多路复用（WDM）框架从纯 LoS 情况扩展到 LoS+NLoS 场景；对 NLoS 分支应用 WDM，得到其角域表示，并可直接由功率谱因子与功率谱密度表征；给出各向同性和非各向同性散射的闭式表征，且各向同性情形回归 Jakes 的各向同性模型。

Result: 形成了面向 holographic MIMO 的 LoS+NLoS 统一模型及其角域分析工具，提供了 NLoS 的功率谱/谱密度表达及其在 DoF 与遍历容量上的定量提升，且在各向同性情形下与经典模型一致。

Conclusion: 将 NLoS 纳入统一框架显著提升系统性能（DoF、 ergodic capacity），并提供了可解析的理论工具用于近场大尺度全息 MIMO 的分析与设计，兼容性良好且包含对纯 LoS 情况的回归。

Abstract: Holographic multiple-input multiple-output (MIMO) enables electrically large continuous apertures, overcoming the physical scaling limits of conventional MIMO architectures with half-wavelength spacing. Their near-field operating regime requires channel models that jointly capture line-of-sight (LoS) and non-line-of-sight (NLoS) components in a physically consistent manner. Existing studies typically treat these components separately or rely on environment-specific multipath models. In this work, we develop a unified LoS+NLoS channel representation for holographic lines that integrates spatial-sampling-based and expansion-based formulations. Building on this model, we extend the wavenumber-division multiplexing (WDM) framework, originally introduced for purely LoS channels, to the LoS+NLoS scenario. Applying WDM to the NLoS component yields its angular-domain representation, enabling direct characterization through the power spectral factor and power spectral density. We further derive closed-form characterizations for isotropic and non-isotropic scattering, with the former recovering Jakes' isotropic model. Lastly, we evaluate the resulting degrees of freedom and ergodic capacity, showing that incorporating the NLoS component substantially improves the performance relative to the purely LoS case.

</details>


### [72] [Beyond Diagonal RIS-assisted MIMO Transmission: Beamforming Gain and Capacity Optimization](https://arxiv.org/abs/2512.08516)
*Ainna Yue Moreno-Locubiche,Josep Vidal*

Main category: eess.SP

TL;DR: BD-RIS在毫米波MIMO下比传统对角RIS在光直 LOS 条件下提升显著，且通过梯度优化实现比arXiv:2406.02170更低的复杂度的配置，提升频谱效率与覆盖。


<details>
  <summary>Details</summary>
Motivation: 将对角RIS扩展为被动的BD-RIS以提升毫米波MIMO DL中的信道控制与性能，克服未给出最优RIS元件闭式解的问题，提出低复杂度梯度优化方法，提升与传统RIS的对比表现。

Method: 在MIMO DL场景下，比较两种传输策略：发射波束赋形（TxBF）与带水位分配的MIMO容量传输；针对毫米波LOS环境，对BD-RIS的元件进行梯度优化设计，使其性能接近或超越对角RIS，并与文献arXiv:2406.02170的解法相比降低复杂度。

Result: 数值结果显示BD-RIS在频谱效率和覆盖范围方面显著优于传统对角RIS；梯度优化方法实现了较低的计算复杂度。

Conclusion: 被动BD-RIS可显著提升毫米波MIMO DL的性能，优于传统对角RIS，且用可实现的梯度优化策略即可达到较低复杂度的实现。

Abstract: Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative technology in wireless communications, offering unprecedented control over signal propagation. This study focuses on passive beyond diagonal reconfigurable intelligent surface (BD-RIS), which has been proposed to generalize conventional diagonal RIS, in Multiple-Input Multiple-Output (MIMO) downlink (DL) communication systems. We compare the performance of transmit beamforming (TxBF) and MIMO capacity transmission with waterfilling power allocation in the millimeter wave (mmWave) band, where propagation primarily occurs under line-of-sight (LOS) conditions. In the lack of closed-form expressions for the optimal RIS elements in either case, our approach adopts a gradient-based optimization approach requiring lower complexity than the solution in arXiv:2406.02170. Numerical results reveal that BD-RIS significantly outperforms traditional diagonal RIS in terms of spectral efficiency and coverage

</details>


### [73] [Applications of Singular Entropy to Signals and Singular Smoothness to Images](https://arxiv.org/abs/2512.08717)
*Oscar Romero,Néstor Thome*

Main category: eess.SP

TL;DR: 利用 SVD/GSVD 提升信号分离与图像异常检测的框架，提出能区分胎儿与母体ECG 的阈值选择方法（EGV、Singular Energy）以及适用于图像的 Singular Smoothness、Singular Entropy 和 Frobenius 范数的新指标，辅以数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决母胎ECG 分离中的阈值选择难题，以及在图像分析中提升信息密度评估与异常检测能力。

Method: 在ECG分析中通过 SVD/GSVD 及 Energy Gap Variation、Singular Energy 进行分离，并利用 GSVD 提供额外判别力；在图像分析中引入 Singular Smoothness，结合 Singular Entropy 与 Frobenius 范数评估信息密度，用于自然异常（山脊断裂、烧毁林区）的检测。

Result: 数值实验表明提出的方法可显著改善胎儿/母体信号分离并提高图像异常检测的准确性。

Conclusion: 所提框架在信号分离与图像异常检测方面具有潜在应用价值，尤其是在多源信号分离和自然场景中的信息密度分析。

Abstract: This paper explores signal and image analysis by using the Singular Value Decomposition (SVD) and its extension, the Generalized Singular Value Decomposition (GSVD). A key strength of SVD lies in its ability to separate information into orthogonal subspaces. While SVD is a well-established tool in ECG analysis, particularly for source separation, this work proposes a refined method for selecting a threshold to distinguish between maternal and fetal components more effectively. In the first part of the paper, the focus is onmedical signal analysis,where the concepts of Energy Gap Variation (EGV) and Singular Energy are introduced to isolate fetal and maternal ECG signals, improving the known ones. Furthermore, the approach is significantly enhanced by the application of GSVD, which provides additional discriminative power for more accurate signal separation. The second part introduces a novel technique called Singular Smoothness, developed for image analysis. This method incorporates Singular Entropy and the Frobenius normto evaluate information density, and is applied to the detection of natural anomalies such asmountain fractures and burned forest regions. Numerical experiments are presented to demonstrate the effectiveness of the proposed approaches.

</details>


### [74] [Evaluating the Deformation Measurement Accuracy Using Low-SNR Radars for Future InSAR Missions](https://arxiv.org/abs/2512.08779)
*Emre Havazli,Shadi Oveisgharan,Michael Denbina,Brian Hawkins*

Main category: eess.SP

TL;DR: 本研究量化了低信噪比（low-SNR）条件对InSAR位移估计的影响，利用将NESZ降至-15 dB的情景对L波段UAVSAR数据在圣安德烈亚斯断层和格陵兰冰盖进行仿真分析。研究表明，在单帧干涉图中，通过信号去相关度0.6且SNR在-9~-10 dB区间，可以实现约4 mm的位移精度；经8x8多视窗的多线性处理可显著提升相干度、消除偏差，尽管会以分辨率下降为代价。总体而言，即使在低SNR条件下，仍能达到与高SNR相当的精度，但需要通过多视图融合等处理策略。


<details>
  <summary>Details</summary>
Motivation: 低背散射区域常见的低SNR条件会削弱相位一致性，降低位移估计的准确性。本文旨在定量评估低SNR对InSAR位移、相干、相位解包和时序反演的影响，并探讨在成本受限的下一代SAR任务（如表面变形与变化SDC）中 Processing 技术的优化与设计。

Method: 通过将NESZ降至-15 dB来模拟低SNR情景，评估干涉相干性、相位解包和时序反演的影响。使用L波段UAVSAR数据在圣安德烈亚斯断层与格陵兰冰盖上进行实验；在单帧干涉图下，当信号去相关度为0.6且SNR约在-9~-10 dB时，可达到约4 mm位移精度。随后对比应用8x8多视窗多走宽（multilooking）对相干性的提升及对偏差的消除，并分析低SNR条件下与高SNR条件下的精度差异。

Result: 研究发现：在特定低SNR条件下，单帧干涉图可达到约4 mm的位移精度，且在低SNR下的速度精度可达约0.5 cm/年，与高SNR条件相近。通过应用8x8的多-look处理，显著提升相干性并消除该偏差，显示低SNR系统在以牺牲空间分辨率为代价获得可比精度方面具有潜力。

Conclusion: 低SNR条件下仍可实现与高SNR相近的InSAR位移精度，前提是采用合适的处理策略（如多视图合成、合适的多-look 参数等）。这对未来成本更低的SAR任务设计具有重要意义，如SDC等项目，并提示在挑战性环境中优化InSAR处理流程的重要性。

Abstract: Interferometric Synthetic Aperture Radar (InSAR) is a powerful tool for monitoring surface deformation with high precision. However, low Signal-to-Noise Ratio (SNR) conditions, common in regions with low backscatter, can degrade phase coherence and compromise displacement accuracy. In this study, we quantify the impact of low-SNR conditions on InSAR-derived displacement using L-band UAVSAR data collected over the San Andreas Fault and Greenland ice sheet. We simulate low-SNR conditions by degrading the Noise-Equivalent Sigma Zero (NESZ) to $-15~\mathrm{dB}$ and assess the resulting effects on interferometric coherence, phase unwrapping, and time series inversion. The displacement accuracy of 4mm in single interferogram can be achieved by taking looks for the signal decorrelation of 0.6 and SNR between -9dB to -10dB. Our findings indicate that even under low-SNR conditions, a velocity precision of $0.5~\mathrm{cm/yr}$ can be achieved in comparison to high-SNR conditions. By applying multilooking with an 8x8 window, we significantly improve coherence and eliminate this bias, demonstrating that low-SNR systems can achieve comparable precision to high-SNR systems at the expense of spatial resolution. These results have important implications for the design of future cost-effective SAR missions, such as Surface Deformation and Change (SDC), and the optimization of InSAR processing techniques in challenging environments.

</details>


### [75] [A Fast Broadband Beamspace Transformation](https://arxiv.org/abs/2512.08887)
*Nakul Singh,Coleman DeLude,Mark Davenport,Justin Romberg*

Main category: eess.SP

TL;DR: 提出一种可计算高效的宽带多波束形成方法（快速波束空间变换），在 B≈M 时其每个样本的时间复杂度接近线性，利用非均匀离散傅里叶变换编码传感器输出并通过 Toeplitz 结构的小方程组求解形成波束，显著优于 O(MB) 的宽带延迟和求和方法。


<details>
  <summary>Details</summary>
Motivation: 解决宽带多传感器阵列的实时（近实时）处理难题；现有宽带波束形成在计算量和实现复杂度方面往往高于窄带方法，且需要跨快照的相干处理，限制了应用规模与速度。

Method: 对每个传感器取 N 个样本并将输出编码为一组系数，利用一种特殊的非均匀采样傅里叶变换；从这些系数出发，通过求解具有 Toeplitz 结构的小线性系统来形成每个波束。总时间复杂度为 O(M log N + B log N)。

Result: 理论分析表明与窄带情形类似的缩放行为，远优于基于延迟求和的宽带波束形成；大量数值实验验证了计算复杂度与精度优势；并且在波束空间中进行离网角插值与干涉源抑制等任务也更高效。

Conclusion: 提出的快速宽带波束空间变换为宽带场景下的高效多波束形成提供了实用的解决方案，具有良好的计算扩展性与应用灵活性。

Abstract: We present a new computationally efficient method for multi-beamforming in the broadband setting. Our "fast beamspace transformation" forms $B$ beams from $M$ sensor outputs using a number of operations per sample that scales linearly (to within logarithmic factors) with $M$ when $B\sim M$. While the narrowband version of this transformation can be performed efficiently with a spatial fast Fourier transform, the broadband setting requires coherent processing of multiple array snapshots simultaneously. Our algorithm works by taking $N$ samples off of each of $M$ sensors and encoding the sensor outputs into a set of coefficients using a special non-uniform spaced Fourier transform. From these coefficients, each beam is formed by solving a small system of equations that has Toeplitz structure. The total runtime complexity is $\mathcal{O}(M\log N+B\log N)$ operations per sample, exhibiting essentially the same scaling as in the narrowband case and vastly outperforming broadband beamformers based on delay and sum whose computations scale as $\mathcal{O}(MB)$. Alongside a careful mathematical formulation and analysis of our fast broadband beamspace transform, we provide a host of numerical experiments demonstrating the algorithm's favorable computational scaling and high accuracy. Finally, we demonstrate how tasks such as interpolating to ``off-grid" angles and nulling an interferer are more computationally efficient when performed directly in beamspace.

</details>


### [76] [Timing-Error Optimized Architecture for Current-Steering DACs](https://arxiv.org/abs/2512.08903)
*Ramin Babaee,Shahab Oveis Gharan,Martin Bouchard*

Main category: eess.SP

TL;DR: 提出一种统计性最小化随机时序失配的 DAC 加权架构，并提供三种解码算法以及 Matlab 仿真显示相对于分段结构的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决随机时序失配造成的 DAC 误差与失真，需要通过新的加权架构在统计意义上降低误差。

Method: 提出 DAC 加权架构；设计三种解码输入码字到开关的算法，具有不同的计算复杂度；通过高层 Matlab 仿真评估动态性能，比较与分段结构。

Result: 仿真结果表明，该统计性加权架构在动态性能方面优于分段结构，且三种解码算法在计算复杂度上呈现权衡。

Conclusion: 该工作提供降低随机时序失配影响的可行方案，并给出计算复杂度可控的解码策略，适用于 DAC 设计场景。

Abstract: We propose a novel digital-to-analog converter (DAC) weighting architecture that statistically minimizes the distortion caused by random timing mismatches among current sources. To decode the DAC input codewords into corresponding DAC switches, we present three algorithms with varying computational complexities. We perform high-level Matlab simulations to illustrate the dynamic performance improvement over the segmented structure.

</details>


### [77] [Architecture Design for Rise/Fall Asymmetry Glitch Minimization in Current-Steering DACs](https://arxiv.org/abs/2512.08909)
*Ramin Babaee,Shahab Oveis Gharan,Martin Bouchard*

Main category: eess.SP

TL;DR: 提出了一种基于新的 glitch metric 的权重分配方案，用于降低当前舵控 DAC 的输出 glitch；基于对称性不良导致的 fall/rise 不对称性，提供潜在性能提升，与分段结构相比在数值仿真中显示显著优势。


<details>
  <summary>Details</summary>
Motivation: 高频应用中的当前舵 DAC 的输出 glitch 会降低动态性能；需要解决开关上升/下降不对称性导致的 glitch。

Method: 提出一个 glitch metric 来量化整体 DAC 性能，并据此设计一种新颖的 DAC 加权方案；通过数值仿真评估。

Result: 仿真结果表明，与分段结构相比，所提架构有潜在显著性能优势。

Conclusion: 基于新 metric 的加权方案可降低输出 glitch、提升动态性能；未来工作可能包括硬件实现和对工艺、温度等因素的鲁棒性分析。

Abstract: Current-steering digital-to-analog converter (DAC) is a prominent architecture that is commonly used in high-speed applications such as optical communications. One of the shortcomings of this architecture is the output glitches that are input dependent and degrade the dynamic performance of the DAC. We investigate DAC glitches that arise from asymmetry in the fall/rise response of DAC switches. We formulate a glitch metric that defines the overall DAC performance, which is then used to find a novel DAC weighting scheme. Numerical simulations show that the proposed architecture can potentially provide a significant performance advantage compared to the segmented structure.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [78] [Learning Dynamics from Infrequent Output Measurements for Uncertainty-Aware Optimal Control](https://arxiv.org/abs/2512.08013)
*Robert Lefringhausen,Theodor Springer,Sandra Hirche*

Main category: eess.SY

TL;DR: 一个贝叶斯框架：在未知非线性系统动态和稀疏/有噪声测量下，通过对连续时间动力学和潜在状态轨迹设定先验并用边缘化的马尔科夫链蒙特卡罗采样（带ODE求解器）更新，获得后验样本来构造考虑模型与测量不确定性的场景化最优控制问题，使用标准非线性规划求解。数值案例：1型糖尿病模型的葡萄糖调控。


<details>
  <summary>Details</summary>
Motivation: 在未知非线性动力学和有限观测的情况下实现可靠的最优控制具有挑战性。通过对动力学与潜在状态的贝叶斯不确定性建模并进行推断，可把不确定性量化并在控制设计中加以利用。

Method: 在状态空间形式中对连续时间动力学和潜在状态轨迹建立贝叶斯先验；通过带有数值ODE积分器的目标性 Marginal Metropolis-Hastings 采样来更新后验；得到的后验样本用于形成考虑模型与测量不确定性的场景化最优控制问题，并用标准的非线性规划方法求解。

Result: 得到的后验样本用于构建场景化最优控制问题，并在数值葡萄糖调控（Type-1糖尿病模型）案例中进行验证，结果表明该方法可在不确定性与有限观测条件下实现鲁棒控制。

Conclusion: 该框架通过把动力学与观测不确定性系统地引入贝叶斯推断，并以场景化优化实现鲁棒的最优控制，适用于其他具有未知动力学的非线性控制任务。

Abstract: Reliable optimal control is challenging when the dynamics of a nonlinear system are unknown and only infrequent, noisy output measurements are available. This work addresses this setting of limited sensing by formulating a Bayesian prior over the continuous-time dynamics and latent state trajectory in state-space form and updating it through a targeted marginal Metropolis-Hastings sampler equipped with a numerical ODE integrator. The resulting posterior samples are used to formulate a scenario-based optimal control problem that accounts for both model and measurement uncertainty and is solved using standard nonlinear programming methods. The approach is validated in a numerical case study on glucose regulation using a Type 1 diabetes model.

</details>


### [79] [Cabin Layout, Seat Density, and Passenger Segmentation in Air Transport: Implications for Prices, Ancillary Revenues, and Efficiency](https://arxiv.org/abs/2512.08066)
*Alessandro V. M. Oliveira,Moises D. Vassallo*

Main category: eess.SY

TL;DR: 本研究通过微观数据证实航空客舱座位密度与票价存在负相关；使用PDS-LASSO控制大量观测以识别座位位置对票价的影响，发现座位行密度越高价格越低；在无选座费情况下，短期购买且选择中间座位的乘客往往票价较高，这揭示了航空附加收入的经济逻辑；并对创新舱概念及其对密度与舒适度的潜在影响进行了探索性分析。


<details>
  <summary>Details</summary>
Motivation: 解释座位布局与密度如何影响票价，填补座位位置对定价影响的研究空白，并通过Post-Double-Selection LASSO提高对潜在混淆因素的控制与识别力，以更准确地估计座位密度、航线市场和运营因素对票价的影响。

Method: 采用登机牌微观数据结合乘客面访的混合数据。因变量为实际支付票价，主要解释变量包括座位在机舱平面图上的位置、座位密度（行密度/座位密度）、以及市场结构、航班运营等控制变量。模型采用Post-Double-Selection LASSO (PDS-LASSO)进行变量选择与估计，能在大量潜在控制变量中识别与票价相关的关键因素；同时进行关于提前购票、出行原因、油价、市场结构、载客率等因素的鲁棒性分析。此外，研究还包含对创新舱概念的探索性分析。

Result: 结果显示：座位密度越高，单位票价越低，体现规模经济与运营效率带来的价格效应。一个出乎意料的发现是，在没有选座费的情形下，短期购买且票价较高的乘客往往分配到中间座位，当其他侧座位不可选时，这揭示了航空公司附加收入策略的经济逻辑。研究还对创新舱概念及其对密度与舒适度的潜在影响进行了探索性分析。

Conclusion: 座位密度对票价具有显著影响，密度提升带来价格下降，体现运营规模经济与资源配置效率的正向作用。研究也提示选座费与座位分配策略对票价和附加收入的影响；未来的舱内设计与创新概念需在密度、舒适度与收益之间进行权衡，进一步丰富对航空定价与乘客体验的理解。

Abstract: This study investigates how the layout and density of seats in aircraft cabins influence the pricing of airline tickets on domestic flights. The analysis is based on microdata from boarding passes linked to face-to-face interviews with passengers, allowing us to relate the price paid to the location on the aircraft seat map, as well as market characteristics and flight operations. Econometric models were estimated using the Post-Double-Selection LASSO (PDS-LASSO) procedure, which selects numerous controls for unobservable factors linked to commercial and operational aspects, thus enabling better identification of the effect of variables such as advance purchase, reason for travel, fuel price, market structure, and load factor, among others. The results suggest that a higher density of seat rows is associated with lower prices, reflecting economies of scale with the increase in aircraft size and gains in operational efficiency. An unexpected result was also obtained: in situations where there was no seat selection fee, passengers with more expensive tickets were often allocated middle seats due to purchasing at short notice, when the side alternatives were no longer available. This behavior helps explain the economic logic behind one of the main ancillary revenues of airlines. In addition to quantitative analysis, the study incorporates an exploratory approach to innovative cabin concepts and their possible effects on density and comfort on board.

</details>


### [80] [Mitigation of Datacenter Demand Ramping and Fluctuation using Hybrid ESS and Supercapacitor](https://arxiv.org/abs/2512.08076)
*Min-Seung Ko,Jae Woong Shim,Hao Zhu*

Main category: eess.SY

TL;DR: 提出一种基于混合储能系统的多时间尺度控制框架，用于降低 hyperscale AI 数据中心的功率波动对电网的冲击，通过高通滤波分离慢/快分量，利用 BESS 和 SC 协同控制实现风格多目标稳态与暂态性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决数据中心大负荷波动引发的频率/电压稳定性挑战，同时降低对电网的冲击与用电成本；现有单一储能解决方案在快速/慢速扰动下能力不足，需要一种能在不同时间尺度上协同工作的储能方案。

Method: 提出将 BESS 与超电容器(SC)通过协调的多时间尺度控制集成的混合储能系统。利用高通滤波器(HPF)将需求分解为慢分量和快分量，慢分量通过带泄漏积分控制分配给储能系统，快分量通过相位前馈+导数控制的PD控制器并辅以前馈与斜率跟踪来管理SC；并引入自适应加权与重复控制以提升瞬态与周期响应。通过仿真/案例验证方法在抑制 ramping 与波动、稳定系统频率、在长时随机训练循环下维持 ESS 与 SC 的可持续状态评估。

Result: 实验/案例结果显示，该方法在抑制需求的斜率变化与波动方面有效，能稳定系统频率，且在长时随机工作条件下维持两种储能的可持续状态（SoC）轨迹。

Conclusion: 提出的多时间尺度HESS控制框架对数据中心功率平滑具有显著潜力，能够在确保电网稳定性与储能健康的前提下提升数据中心的能源管理效率，适合作为 hyperscale 数据中心的配套控制方案。

Abstract: This paper proposes a hybrid energy storage system (HESS)-based control framework that enables comprehensive power smoothing for hyperscale AI datacenters with large load variations. Datacenters impose severe ramping and fluctuation-induced stresses on the grid frequency and voltage stability. To mitigate such disturbances, the proposed HESS integrates a battery energy storage system (BESS) and a supercapacitor (SC) through coordinated multi-timescale control. A high-pass filter (HPF) separates the datacenter demand into slow and fast components, allocating them respectively to the ESS via a leaky-integral controller and to the SC via a phase-lead proportional-derivative controller enhanced with feedforward and ramp-tracking compensation. Adaptive weighting and repetitive control mechanisms further improve transient and periodic responses. Case studies verify that the proposed method effectively suppresses both ramping and fluctuations, stabilizes the system frequency, and maintains sustainable state-of-charge (SoC) trajectories for both ESS and SC under prolonged, stochastic training cycles.

</details>


### [81] [Bounding the Minimal Current Harmonic Distortion in Optimal Modulation of Single-Phase Power Converters](https://arxiv.org/abs/2512.08201)
*Jared Miller,Petros Karamanakos,Tobias Geyer*

Main category: eess.SY

TL;DR: 将最优脉冲模式(OPP)设计转化为周期性混合系统的模式选择最优控制问题，通过对跳跃时间的选择来确定开关角和电平序列；采用凸松弛技术构造SDP层级来对最小可实现的谐波失真给出下界，且复杂度随等级和开关点呈亚二次增长，数值结果显示方法有效。


<details>
  <summary>Details</summary>
Motivation: 电力变换器中的谐波失真控制是关键问题。OPP需要同时优化开关角与模式序列，呈现出离散变量与非线性约束的混合整数非凸性，传统方法求解困难且计算量大。

Method: 将OPP设计重构为周期性模式选择的混合系统最优控制问题；在一个跳跃时间图中选择开关跳变的时间点来对应角度与电平序列；应用凸松弛技术，建立一系列半正定程序（SDP）以给出目标谐波失真的下界，且下界层级提供逐步收敛与更高保真度。

Result: 给出多个SDP下界，数值实验表明所提方法在下界收敛性与规模性方面具有优势，能够有效降低目标谐波失真并具可扩展性。

Conclusion: 提出的时间域混合系统/凸松弛框架为OPP设计提供可扩展的优化工具，能够在严格下界与可行解之间取得平衡。未来工作可聚焦提高下界收敛速度、在鲁棒性与实现性方面的扩展，以及与实际硬件控制器的集成。

Abstract: Optimal pulse patterns (OPPs) are a modulation technique in which a switching signal is computed offline through an optimization process that accounts for selected performance criteria, such as current harmonic distortion. The optimization determines both the switching angles (i.e., switching times) and the pattern structure (i.e., the sequence of voltage levels). This optimization task is a challenging mixed-integer nonconvex problem, involving integer-valued voltage levels and trigono metric nonlinearities in both the objective and the constraints. We address this challenge by reinterpreting OPP design as a periodic mode-selecting optimal control problem of a hybrid system, where selecting angles and levels corresponds to choosing jump times in a transition graph. This time-domain formulation enables the direct use of convex-relaxation techniques from optimal control, producing a hierarchy of semidefinite programs that lower-bound the minimal achievable harmonic distortion and scale subquadratically with the number of converter levels and switching angles. Numerical results demonstrate the effectiveness of the proposed approachs

</details>


### [82] [Theoretical Studies of Sub-THz Active Split-Ring Resonators for Near-Field Imaging](https://arxiv.org/abs/2512.08265)
*Ali Ameri,Jun-Chau Chien,Ali M. Niknejad*

Main category: eess.SY

TL;DR: 提出了一种主动分割环谐振器（ASRR）的理论设计框架，利用可调负阻器实现开关性和Q值增强，适合在硅芯片上密集阵列中生成像素化近场以实现高分辨率的2D人体组织成像。


<details>
  <summary>Details</summary>
Motivation: 解决在高密度阵列中实现高SNR、低功耗成像的挑战，同时探讨耦合、非线性效应、信号流与噪声对检测性能的影响，以支持实时、非侵入式成像的需求。

Method: 提出并分析一个带有可调负阻器的SRR结构（ASRR），研究其与宿主传输线的耦合、非线性效应、信号流以及各种噪声源对检测性能的影响；通过仿真验证，提出单像素SNR与功耗的设计指南，并在保持阵列可扩展性的前提下优化参数。

Result: 仿真结果显示，ASRR能实现开关控制和Q值调谐，提升像素级SNR并降低功耗，使在密集阵列中的实现具有可行性；给出具体的设计区间与参数依赖关系，提供了面向可扩展阵列的设计策略。

Conclusion: 提出的ASRR设计框架为硅芯片上密集阵列实时、低成本的2D成像提供理论与设计指南，系统考虑耦合、非线性和噪声等因素，支持在人体组织成像等应用中的规模化实现。

Abstract: This paper develops a theoretical framework for the design of Active Split-Ring Resonators (ASRRs). An ASRR is a Split-Ring Resonator (SRR) equipped with a tunable negative resistor, enabling both switchability and quality factor boosting and tuning. These properties make ASRRs well-suited for integration into dense arrays on silicon chips, where pixelated near-fields are generated and leveraged for high-resolution 2D imaging of samples. Such imagers pave the way for real-time, non-invasive, and low-cost imaging of human body tissue. The paper investigates ASRR coupling to host transmission lines, nonlinear effects, signal flow, and the influence of various noise sources on detection performance. Verified through simulations, these studies provide design guidelines for optimizing the Signal-to-Noise Ratio (SNR) and power consumption of a single pixel, while adhering to the constraints of a scalable array.

</details>


### [83] [Formation and Investigation of Cooperative Platooning at the Early Stage of Connected and Automated Vehicles Deployment](https://arxiv.org/abs/2512.08298)
*Zeyu Mu,Sergei S. Avedisov,Ahmadreza Moradipari,B. Brian Park*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cooperative platooning, enabled by cooperative adaptive cruise control (CACC), is a cornerstone technology for connected automated vehicles (CAVs), offering significant improvements in safety, comfort, and traffic efficiency over traditional adaptive cruise control (ACC). This paper addresses a key challenge in the initial deployment phase of CAVs: the limited benefits of cooperative platooning due to the sparse distribution of CAVs on the road. To overcome this limitation, we propose an innovative control framework that enhances cooperative platooning in mixed traffic environments. Two techniques are utilized: (1) a mixed cooperative platooning strategy that integrates CACC with unconnected vehicles (CACCu), and (2) a strategic lane-change decision model designed to facilitate safe and efficient lane changes for platoon formation. Additionally, a surrounding vehicle identification system is embedded in the framework to enable CAVs to effectively identify and select potential platooning leaders. Simulation studies across various CV market penetration rates (MPRs) show that incorporating CACCu systems significantly improves safety, comfort, and traffic efficiency compared to existing systems with only CACC and ACC systems, even at CV penetration as low as 10%. The maximized platoon formation increases by up to 24%, accompanied by an 11% reduction in acceleration and a 7% decrease in fuel consumption. Furthermore, the strategic lane-change model enhances CAV performance, achieving notable improvements between 6% and 60% CV penetration, without adversely affecting overall traffic flow.

</details>


### [84] [Integration of AI-Driven CAD Systems in Designing Water and Power Transportation Infrastructure for Industrial and Remote Landscape Applications](https://arxiv.org/abs/2512.08415)
*Sunggyu Park*

Main category: eess.SY

TL;DR: 将AI驱动的CAD系统用于水利与能源运输基础设施设计，提升设计效率、精确性与可持续性，并通过与GIS、IoT的集成实现自学习自适应设计


<details>
  <summary>Details</summary>
Motivation: 应对日益增长的全球基础设施需求，特别是在欠发达环境中，提高设计工作流的自动化、预测建模和实时数据分析能力，以减少资源浪费与人力错误。

Method: 对AI集成的CAD方法进行分析性讨论，强调自动化、预测建模、实时数据分析的嵌入，以及AI驱动工具集在设计工作流中的应用，探讨数据孤岛、互操作性与人员适应等挑战；并考察CAD、GIS、物联网等技术的耦合以实现自学习、自适应的设计系统。

Result: AI驱动的CAD实现了更快的项目交付、提升的设计精度，以及对环境与物流约束的更强韧性；通过连接CAD、GIS和IoT，形成自学习、自适应的设计体系。

Conclusion: AI驱动的CAD具备推动可持续基础设施发展的潜力，需解决数据孤岛、互操作性与劳动力适配等挑战，以实现更高效、弹性与资源优化的设计与交付。

Abstract: The integration of AI into CAD systems transforms how engineers plan and develop infrastructure projects involving water and power transportation across industrial and remote landscapes. This paper discusses how AI-driven CAD systems improve the efficient, effective, and sustainable design of infrastructure by embedding automation, predictive modeling, and real-time data analytics. This study examines how AI-supported toolsets can enhance design workflows, minimize human error, and optimize resource allocation for projects in underdeveloped environments. It also addresses technical and organizational challenges to AI adoption, including data silos, interoperability issues, and workforce adaptation. The findings demonstrate that AI-powered CAD enables faster project delivery, enhanced design precision, and increased resilience to environmental and logistical constraints. AI helps connect CAD, GIS, and IoT technologies to develop self-learning, adaptive design systems that are needed to meet the increasing global demand for sustainable infrastructure.

</details>


### [85] [Beyond Wave Variables: A Data-Driven Ensemble Approach for Enhanced Teleoperation Transparency and Stability](https://arxiv.org/abs/2512.08436)
*Nour Mitiche,Farid Ferguene,Mourad Oussalah*

Main category: eess.SY

TL;DR: 提出一种数据驱动的混合框架，用三种序列模型的集成替代波变量变换，以提升时延与噪声下的透明性，同时通过被动性约束确保稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决传统波变量方法在四通道结构下对波反射和时延/噪声扰动的脆弱性，提升双向遥操作系统的透明性与鲁棒性。

Method: 用 Optuna 自动调参，对三种独立的序列模型进行优化并通过堆叠元学习器融合：1) 加Prophet的LSTM用于趋势校正，2) 基于LSTM特征提取、聚类与随机森林回归的组合，3) CNN-LSTM用于捕捉局部与长期动力学。用 MATLAB/Simulink 的基线系统数据在 Python 中进行实验验证，并在传递性/被动性约束下实现稳定性。

Result: 所提集成在不同时延与噪声条件下的透明度与基线波变量系统相当，同时通过被动性约束维持稳定性。

Conclusion: 数据驱动的三模集成能够在保持稳定性的前提下，提供与传统波变量系统相近的透明度，从而增强对时延及环境扰动的鲁棒性。

Abstract: Time delays in communication channels present significant challenges for bilateral teleoperation systems, affecting both transparency and stability. Although traditional wave variable-based methods for a four-channel architecture ensure stability via passivity, they remain vulnerable to wave reflections and disturbances like variable delays and environmental noise. This article presents a data-driven hybrid framework that replaces the conventional wave-variable transform with an ensemble of three advanced sequence models, each optimized separately via the state-of-the-art Optuna optimizer, and combined through a stacking meta-learner. The base predictors include an LSTM augmented with Prophet for trend correction, an LSTM-based feature extractor paired with clustering and a random forest for improved regression, and a CNN-LSTM model for localized and long-term dynamics. Experimental validation was performed in Python using data generated from the baseline system implemented in MATLAB/Simulink. The results show that our optimized ensemble achieves a transparency comparable to the baseline wave-variable system under varying delays and noise, while ensuring stability through passivity constraints.

</details>


### [86] [MPC for tracking for anesthesia dynamics](https://arxiv.org/abs/2512.08452)
*Maxim Raymond,Kaouther Moussa,Mirko Fiacchini,Jimmy Lauber*

Main category: eess.SY

TL;DR: 提出一种基于模型预测控制的跟踪型控制策略，用于对麻醉动力学的多时间尺度系统进行稳态配对跟踪，同时将慢态视为干扰并通过端点项实现递归可行性与渐近稳定性，在仿真环境中对患者进行评估。


<details>
  <summary>Details</summary>
Motivation: 解决麻醉控制中的多时间尺度动力学及非唯一稳态问题，提升跟踪控制在临床仿真中的鲁棒性与稳定性。

Method: 将慢动态量视为干扰，将系统重构为名义模型，设计MPC for tracking以实现对非唯一稳态的跟踪，并通过端点项保证递归可行性和渐近稳定性。

Result: 在患者仿真环境中验证控制器性能，展现可行性、稳定性及对稳态配对的有效处理。

Conclusion: 所提出的框架能够在多时间尺度的麻醉动力学下实现鲁棒的跟踪控制，并在仿真中表现出良好的稳定性与可行性。

Abstract: In this paper, an MPC for tracking formulation is proposed for the control of anesthesia dynamics. It seamlessly enables the optimization of the steady-states pair that is not unique due to the MISO nature of the model. Anesthesia dynamics is a multi-time scale system with two types of states characterized, respectively, by fast and slow dynamics. In anesthesia control, the output equation depends only on the fast dynamics. Therefore, the slow states can be treated as disturbances, and compensation terms can be introduced. Subsequently, the system can be reformulated as a nominal one allowing the design of an MPC for tracking strategy. The presented framework ensures recursive feasibility and asymptotic stability, through the design of appropriate terminal ingredients in the MPC for tracking framework. The controller performance is then assessed on a patient in a simulation environment.

</details>


### [87] [Quantization and Security Parameter Design for Overflow-Free Confidential FRIT](https://arxiv.org/abs/2512.08464)
*Jungjin Park,Osamu Kaneko,Kiminao Kogiso*

Main category: eess.SY

TL;DR: 提出了一套系统化的设计流程，用于在 CFRIT 的加密数据驱动控制中确定量化增益与安全参数，以防止溢出并保障精度。


<details>
  <summary>Details</summary>
Motivation: 在对加密数据进行增益调参时，需评估编码过程引入的量化误差范围，并且需要在不超过溢出的前提下实现高保真 tuning，同时分析量化误差对 CFRIT 与非机密 FRIT 的参数偏差的影响。

Method: 基于数据驱动的编码量化误差范围推断，推导出确保溢出安全的量化增益与安全参数的解析条件；建立量化误差与调参精度之间的定量关系；给出数值例证与可行域可视化以提供参数设计的直观指南。

Result: 给出在不发生溢出的前提下进行加密调参的具体设计步骤和约束，揭示量化误差与 CFRIT 相对于 FRIT 的参数偏差之间的定量关系；数值示例验证了设计的准确性并提供了可行域的可视化。

Conclusion: 提出的参数设计流程为加密数据驱动控制提供实用工具，使 CFRIT 的加密调参在保持精度的同时避免溢出，并揭示了参数选择对可行区域的影响。

Abstract: This study proposes a systematic design procedure for determining the quantization gain and the security parameter in the Confidential Fictitious Reference Iterative Tuning (CFRIT), enabling overflow-free and accuracy-guaranteed encrypted controller tuning. Within an encrypted data-driven gain tuning, the range of quantization errors induced during the encoding (encryption) process can be estimated from operational data. Based on this insight, explicit analytical conditions on the quantization gain and the security parameter are derived to prevent overflow in computing over encrypted data. Furthermore, the analysis reveals a quantitative relationship between quantization-induced errors and the deviation between the gains obtained by CFRIT and non-confidential Fictitious Reference Iterative Tuning (FRIT), clarifying how parameter choice affects tuning accuracy. A numerical example verifies the proposed procedure by demonstrating that the designed parameters achieve accurate encrypted tuning within a prescribed tolerance while preventing overflow. In addition, the admissible region of parameter combinations is visualized to examine the characteristics of feasible and infeasible regions, providing practical insights into parameter design for encrypted data-driven control.

</details>


### [88] [Decoupled Design of Time-Varying Control Barrier Functions via Equivariances](https://arxiv.org/abs/2512.08607)
*Adrian Wiltz,Dimos V. Dimarogonas*

Main category: eess.SY

TL;DR: 提出一种分层的时变控制屏障函数(CBF)设计框架：以一个时不变CBF为基础，再通过时变变换构造时间依赖分量，利用动力学结构特性（如等变性）扩展可控约束的表达能力；设计与CBF合成解耦，提升计算效率，适用于不确定环境下的输入约束与欠驱动系统。


<details>
  <summary>Details</summary>
Motivation: 解决在不确定环境和受限驱动下对时间变化约束的鲁棒性需求，提升时变CBF的表达能力与计算效率，尤其通过利用动力学的结构性属性（如等变性）来扩展可控约束的范围。

Method: 构造一类时不变CBF，编码系统对给定约束的动态能力；在此基础上引入时间依赖变换来得到时变CBF。通用的时间变换适用于任意系统，但利用动力学结构属性（如等变性）的变换可以处理更广、更具表达力的时间变化约束。设计过程实现时间变化与CBF合成的解耦，从而在保持对输入约束与欠驱动的考虑的同时，利用对时间变化的定性认识在不确定环境中应用。

Result: 提出的框架提供了一种系统化的时变CBF设计方法；通过利用等变性等动力学结构属性，扩展了时变CBF的可处理约束范围，并实现设计与计算的解耦，从而在计算效率方面具有优势，且具备对输入约束、欠驱动和不确定环境的适应性。

Conclusion: 通过将时变CBF设计分解为基础的时不变CBF设计与时间变换的组合，该方法在理论与应用上扩展了时变CBF的适用性，尤其在存在结构性动力学属性的系统中可构造更丰富的时变约束，并在不确定环境下具备鲁棒性和可计算性。

Abstract: This article presents a systematic method for designing time-varying Control Barrier Functions (CBF) composed of a time-invariant component and multiple time-dependent components, leveraging structural properties of the system dynamics. The method involves the construction of a specific class of time-invariant CBFs that encode the system's dynamic capabilities with respect to a given constraint, and augments them subsequently with appropriately designed time-dependent transformations. While transformations uniformly varying the time-invariant CBF can be applied to arbitrary systems, transformations exploiting structural properties in the dynamics - equivariances in particular - enable the handling of a broader and more expressive class of time-varying constraints. The article shows how to leverage such properties in the design of time-varying CBFs. The proposed method decouples the design of time variations from the computationally expensive construction of the underlying CBFs, thereby providing a computationally attractive method to the design of time-varying CBFs. The method accounts for input constraints and under-actuation, and requires only qualitative knowledge on the time-variation of the constraints making it suitable to the application in uncertain environments.

</details>


### [89] [NLoS Localization with Single Base Station Based on Radio Map](https://arxiv.org/abs/2512.08608)
*Jiajie Xu,Yifan Guo,Xiucheng Wang,Nan Cheng,Tingting Yang*

Main category: eess.SY

TL;DR: 提出一种单基站定位框架，通过将序列信号测量与先验无线电地图（RM）相匹配，显著提升NLoS环境下的定位精度，达到亚米级。


<details>
  <summary>Details</summary>
Motivation: 在强多径、无LoS路径的密集城市NLoS场景中，GNSS和基于多基站的定位难以提供可靠定位，需要利用单基站并结合历史无线信息来抵消多径影响。

Method: 将时序测量特征与无线电地图中的先验信息相结合，通过RM匹配和序列特征提取来减轻多径干扰，评估不同RM构建策略与测量序列长度对定位的影响。

Result: 仿真结果表明，在典型NLoS环境中可实现亚米级定位精度，且系统对RM构建策略和测量序列长度的变化具有一定鲁棒性。

Conclusion: 该方案为单基站部署提供一个可行且鲁棒的定位方法，具有在实际场景中的潜力，但需关注RM的获取、更新与泛化等实际挑战。

Abstract: Accurate outdoor localization in Non-Line-of-Sight (NLoS) environments remains a critical challenge for wireless communication and sensing systems. Existing methods, including positioning based on the Global Navigation Satellite System (GNSS) and triple Base Stations (BSs) techniques, cannot provide reliable performance under NLoS conditions, particularly in dense urban areas with strong multipath effects. To address this limitation, we propose a single BS localization framework that integrates sequential signal measurements with prior radio information embedded in the Radio Map (RM). Using temporal measurement features and matching them with radio maps, the proposed method effectively mitigates the adverse impact of multipath propagation and reduces the dependence on LoS paths. Simulation experiments further evaluate the impact of different radio map construction strategies and the varying lengths of the measurement sequence on localization accuracy. Results demonstrate that the proposed scheme achieves sub-meter positioning accuracy in typical NLoS environments, highlighting its potential as a practical and robust solution for single-base-station deployment.

</details>


### [90] [Direct transfer of optimized controllers to similar systems using dimensionless MPC](https://arxiv.org/abs/2512.08667)
*Josip Kir Hromatko,Shambhuraj Sawant,Šandor Ileš,Sébastien Gros*

Main category: eess.SY

TL;DR: Dimensionless MPC enables direct transfer of closed-loop performance across dynamically similar (scaled) systems and uses cross-scale data for automatic tuning; demonstrated on cartpole swing-up and car racing with RL or Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: Scaled model experiments reduce cost and safety concerns, but transferring controllers to full-scale systems is challenging without re-tuning. Dimensional analysis and dynamic similarity offer a path for transfer, yet practical methods to achieve direct transfer are needed.

Method: Reformulate model predictive control in dimensionless variables to achieve dynamic similarity; automatically tune for closed-loop performance; allow optimization data from systems of different scales; validate on cartpole swing-up and car racing using reinforcement learning or Bayesian optimization for tuning; code availability.

Result: The dimensionless MPC framework enables direct transfer of optimized closed-loop behavior to dynamically similar systems and supports cross-scale data usage for parameter tuning; demonstrated across two control problems with two tuning methods and with publicly available software.

Conclusion: Dimensionless MPC provides a scalable approach to controller transfer across scales by leveraging dynamic similarity and cross-scale data, reducing tuning efforts and enabling efficient experimentation.

Abstract: Scaled model experiments are commonly used in various engineering fields to reduce experimentation costs and overcome constraints associated with full-scale systems. The relevance of such experiments relies on dimensional analysis and the principle of dynamic similarity. However, transferring controllers to full-scale systems often requires additional tuning. In this paper, we propose a method to enable a direct controller transfer using dimensionless model predictive control, tuned automatically for closed-loop performance. With this reformulation, the closed-loop behavior of an optimized controller transfers directly to a new, dynamically similar system. Additionally, the dimensionless formulation allows for the use of data from systems of different scales during parameter optimization. We demonstrate the method on a cartpole swing-up and a car racing problem, applying either reinforcement learning or Bayesian optimization for tuning the controller parameters. Software used to obtain the results in this paper is publicly available at https://github.com/josipkh/dimensionless-mpcrl.

</details>


### [91] [Gradient-Informed Monte Carlo Fine-Tuning of Diffusion Models for Low-Thrust Trajectory Design](https://arxiv.org/abs/2512.08705)
*Jannik Graebner,Ryne Beeson*

Main category: eess.SY

TL;DR: 将扩展自监督扩散模型的生成能力与梯度信息的马尔可夫链蒙特卡洛方法相结合，用于在圆规三体问题中的低推力轨道初始猜测周围进行全局搜索，聚焦帕累托最优解。


<details>
  <summary>Details</summary>
Motivation: 目标函数在局部极小值众多、目标景观复杂的情况下，需高效地在局部最优解邻域进行采样与探索，以同时考量燃料、飞行时间与约束违例等多目标。

Method: 将问题建模为在局部最优解邻域上支持的未归一化分布的采样，并扩展自监督扩散模型的微调框架以引入梯度信息的马尔可夫链蒙特兰卡洛方法。对比两种算法：Metropolis-Adjusted Langevin Algorithm (MALA) 与 Hamiltonian Monte Carlo (HMC)，均以扩散模型学习的分布作为起点。目标函数导数通过状态转移矩阵解析推导。结果在 Saturn-Titan 系统的多周转转移上验证，梯度漂移项加速混合并提升收敛性。MALA在性能与计算成本之间取得最佳权衡。

Result: 相对于基线的随机行走Metropolis，MALA将可行性率从17.34%提升至63.01%，并对Pareto前沿提供更密集且多样的覆盖。通过用关联奖励值对扩散模型进行微调并采用奖励加权似然最大化，系统学习了问题的全局解结构，消除了额外的数据生成阶段的需要。

Conclusion: 将梯度信息引入的MCMC与扩散模型的协同学习用于低推力轨道设计，显著提升在Pareto前沿的探索效率与可行解产出。MALA表现尤为突出，配合基于奖励的微调，可在较低成本下获得对全局解结构的有效把握。

Abstract: Preliminary mission design of low-thrust spacecraft trajectories in the Circular Restricted Three-Body Problem is a global search characterized by a complex objective landscape and numerous local minima. Formulating the problem as sampling from an unnormalized distribution supported on neighborhoods of locally optimal solutions, provides the opportunity to deploy Markov chain Monte Carlo methods and generative machine learning. In this work, we extend our previous self-supervised diffusion model fine-tuning framework to employ gradient-informed Markov chain Monte Carlo. We compare two algorithms - the Metropolis-Adjusted Langevin Algorithm and Hamiltonian Monte Carlo - both initialized from a distribution learned by a diffusion model. Derivatives of an objective function that balances fuel consumption, time of flight and constraint violations are computed analytically using state transition matrices. We show that incorporating the gradient drift term accelerates mixing and improves convergence of the Markov chain for a multi-revolution transfer in the Saturn-Titan system. Among the evaluated methods, MALA provides the best trade-off between performance and computational cost. Starting from samples generated by a baseline diffusion model trained on a related transfer, MALA explicitly targets Pareto-optimal solutions. Compared to a random walk Metropolis algorithm, it increases the feasibility rate from 17.34% to 63.01% and produces a denser, more diverse coverage of the Pareto front. By fine-tuning a diffusion model on the generated samples and associated reward values with reward-weighted likelihood maximization, we learn the global solution structure of the problem and eliminate the need for a tedious separate data generation phase.

</details>


### [92] [LaMoSys3.5D: Enabling 3.5D-IC-Based Large Language Model Inference Serving Systems via Hardware/Software Co-Design](https://arxiv.org/abs/2512.08731)
*Qipan Wang,Zhe Zhang,Shuangchen Li,Hongzhong Zheng,Zheng Liang,Yibo Lin,Runsheng Wang,Ru Huang*

Main category: eess.SY

TL;DR: 提出 LaMoSys3.5D，一种可扩展的 3.5D-IC 架构，通过异构的 3D RAM 芯粒来优化 LLm 服务端的端到端推理，显著提升吞吐功耗比和端到端延迟。


<details>
  <summary>Details</summary>
Motivation: LLMs 需要在大规模部署下实现高吞吐、低能耗的推理；现有 2.5D/3D 设计在数据流、并行映射、调度以及端到端 serving 的协同方面仍不充分，难以同时满足 prefills 的算力和 decode 的带宽需求。

Method: 提出 LaMoSys3.5D，将 3D RAM 芯粒按 2.5D TCB 堆叠在互连器上，完成 prefill 的计算密度和 decode 的带宽富集，进行软硬件协同设计，覆盖数据流、并行映射，并引入热建模与层级设计空间探索框架。对多种 LLM 与工作负载进行评估。

Result: 在吞吐功耗方面较 DGX A100 提升约 62 倍，在端到端延迟几何均值上较现有 3D 设计提升约 4.87 倍。给出 3.5DIC 架构与端到端推理服务的设计要点与指南。

Conclusion: 总结出面向 3.5DIC 的设计准则，并为端到端推理服务提供可操作的实现路径。

Abstract: The success of large language models LLMs amplifies the need for highthroughput energyefficient inference at scale. 3DDRAMbased accelerators provide high memory bandwidth and therefore an opportunity to accelerate the bandwidthbound decode phase. However, how to adequately balance compute density for prefill with bandwidthcapacity for decode remains open. Moreover, most prior designs do not target endtoend serving, leaving the codesign of dataflow, parallel mapping, and scheduling underexplored. To bridge the gap, we present LaMoSys3.5D, to our knowledge the first scalable 3.5DIC architecture for LLM serving. LaMoSys3.5D composes heterogeneous 3DDRAM chiplets on a 2.5D interposer: computerich chiplets for prefill and bandwidthcapacityrich chiplets for decode. To realize efficient serving, we adopt a hardwaresoftware codesign spanning dataflow, parallel mapping, and introduce a thermalaware modeling and hierarchical designspace exploration framework. Across diverse LLMs and workloads, LaMoSys3.5D improves throughputperwatt over DGXA100 systems by 62 and achieves a 4.87 better endtoend latency geomean versus prior 3D designs. We further distill intriguing design guidelines for 3.5DIC architectures and endtoend inference serving.

</details>


### [93] [IoT-based Cost-Effective Fruit Quality Monitoring System using Electronic Nose](https://arxiv.org/abs/2512.08753)
*Anindya Bhattacharjee,Nittya Ananda Biswas,Khondakar Ashik Shahriar,Kawsain Bin Salim*

Main category: eess.SY

TL;DR: 提出一种低功耗、成本低廉的基于物联网的水果质量监测系统，通过电气鼻子（MQ传感器阵列）监测香蕉的挥发性气体（乙醇、甲烷、氨气）以评估成熟与腐烂，并建立基于气体浓度阈值的数学模型和一个面向农民的仪表板，以实现客观决策并减少经济损失。


<details>
  <summary>Details</summary>
Motivation: 针对在孟加拉国等地存在的因主观质量评估导致的采后损失、食品安全风险和经济损失，需以科学方法进行客观决策。

Method: 提出一种低功耗、成本效益高的水果质量监测系统，利用MQ气体传感器阵列作为电子鼻，监测香蕉的挥发气体（乙醇、甲烷、氨）在成熟与分解过程中的变化；基于气体浓度阈值建立数学模型以准确评估水果质量；并将监测信息整合到农民可用的仪表板中以便快速决策。

Result: 提出一个概念性系统与模型，尚未给出实验数据或量化结果，强调可行性、可扩展性及对农业供应链的潜在收益。

Conclusion: 该方法有潜力降低损失、提升食品安全，并为供应链提供可扩展的解决方案。

Abstract: Post-harvest losses due to subjective quality assessment cause significant damage to the economy and food safety, especially in countries like Bangladesh. To mitigate such damages, objective decision-making backed by scientific methods is necessary. An IoT-based, cost-effective quality monitoring system can provide a solution by going beyond subjective quality monitoring and decision-making practices. Here, we propose a low-power, cost-effective fruit quality monitoring system with an array of MQ gas sensors, which can be used as an electronic nose. We track the volatile gas emissions, specifically ethanol, methane, and ammonia, encompassing both ripening and decomposition for a set of bananas. Based on the gas concentration thresholds, we develop a mathematical model to accurately assess fruit quality. We also integrate this information into a dashboard for prompt decision-making and monitoring to make it useful to the farmers. This approach has the potential to reduce economic losses, enhance food safety, and provide scalable solutions for the supply chain.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [94] [Structure Theorems (and Fast Algorithms) for List Recovery of Subspace-Design Codes](https://arxiv.org/abs/2512.08017)
*Rohan Goyal,Venkatesan Guruswami*

Main category: cs.IT

TL;DR: Even when list-recovery lists are large, they are highly structured: we can describe the relevant list compactly, enabling polynomial-in-1/ε time list-recovery for folded Reed-Solomon and multiplicity codes, improving over previous exponential-size descriptions.


<details>
  <summary>Details</summary>
Motivation: Overcome the barrier that list sizes must be exponential in 1/ε for list-recovery of FRS and related codes, by exploiting algebraic structure to enable efficient algorithms and compact representations.

Method: Extend algorithmic strategies for list decoding (Ashvinkumar, Habib, Srivastava) to the list-recovery setting. Show that the set of candidate codewords contains a highly structured subset. Provide a compact description of the relevant list of size ℓ^{O((log ℓ)/ε)} and devise an algorithm running in time polynomial in 1/ε. Improve on Guruswami–Wang’s compact description which had size ≈ n^{ℓ/ε}.

Result: A compact description of the relevant list of size ℓ^{O((log ℓ)/ε)} can be produced, containing all possible correct codewords, and the list-recovery algorithm runs in time polynomial in 1/ε. This improves the state-of-the-art by offering a much smaller compact representation and faster algorithms.

Conclusion: Despite large list sizes in list-recovery, the underlying structure enables efficient recovery with compact representations; this advances algorithmic list-recovery results for Folded Reed-Solomon and related codes and narrows the gap to near-linear-time, capacity-achieving outcomes.

Abstract: List recovery of error-correcting codes has emerged as a fundamental notion with broad applications across coding theory and theoretical computer science. Folded Reed-Solomon (FRS) and univariate multiplicity codes are explicit constructions which can be efficiently list-recovered up to capacity, namely a fraction of errors approaching $1-R$ where $R$ is the code rate.
  Chen and Zhang and related works showed that folded Reed-Solomon codes and linear codes must have list sizes exponential in $1/ε$ for list-recovering from an error-fraction $1-R-ε$. These results suggest that one cannot list-recover FRS codes in time that is also polynomial in $1/ε$. In contrast to such limitations, we show, extending algorithmic advances of Ashvinkumar, Habib, and Srivastava for list decoding, that even if the lists in the case of list-recovery are large, they are highly structured. In particular, we can output a compact description of a set of size only $\ell^{O((\log \ell)/ε)}$ which contains the relevant list, while running in time only polynomial in $1/ε$ (the previously known compact description due to Guruswami and Wang had size $\approx n^{\ell/ε}$). We also improve on the state-of-the-art algorithmic results for the task of list-recovery.

</details>


### [95] [Expectations in Expectation Propagation](https://arxiv.org/abs/2512.08034)
*Zilu Zhao,Fangqing Xiao,Dirk Slock*

Main category: cs.IT

TL;DR: 研究在EP框架下的负方差消息对收敛的影响，聚焦线性模型，并提出非持久、持久两类策略以防止无限值消息阻塞，同时给出一种基于消息关系的额外方法来避免产生无限值消息。


<details>
  <summary>Details</summary>
Motivation: 解决Gaussian-projected EP中出现的“负方差”消息以及相关信念在线性模型中的相互影响，从而提高EP的稳定性和收敛性。

Method: 对线性模型中的信念关系进行分析，在此基础上提出非持久与持久两种策略以避免被无限值消息阻塞；并进一步研究EP消息之间的关系，提出额外一种避免产生无限值消息的方法。

Result: 提出并描述三种策略的理论框架，能够在EP迭代中防止负方差/无限值消息导致的阻塞，提升算法鲁棒性。

Conclusion: 基于对线性模型中EP消息关系的分析，给出多种有效途径来避免无限值消息，从而增强Gaussian-projected EP下的鲁棒性与收敛性。

Abstract: Expectation Propagation (EP) is a widely used message-passing algorithm that decomposes a global inference problem into multiple local ones. It approximates marginal distributions (beliefs) using intermediate functions (messages). While beliefs must be proper probability distributions that integrate to one, messages may have infinite integral values. In Gaussian-projected EP, such messages take a Gaussian form and appear as if they have "negative" variances. Although allowed within the EP framework, these negative-variance messages can impede algorithmic progress.
  In this paper, we investigate EP in linear models and analyze the relationship between the corresponding beliefs. Based on the analysis, we propose both non-persistent and persistent approaches that prevent the algorithm from being blocked by messages with infinite integral values.
  Furthermore, by examining the relationship between the EP messages in linear models, we develop an additional approach that avoids the occurrence of messages with infinite integral values.

</details>


### [96] [Adaptive Matched Filtering for Sensing With Communication Signals in Cluttered Environments](https://arxiv.org/abs/2512.08157)
*Lei Xie,Hengtao He,Yifeng Xiong,Fan Liu,Shi Jin*

Main category: cs.IT

TL;DR: 本论文研究在复杂作业环境下自适应匹配滤波（AMF）的性能，聚焦在叠加信号的情形。将目标从瞬时SCNR转向平均SCNR，并利用随机矩阵理论（RMT）给出其渐近近似；并对调制与载波设计进行比较，提出两类飞行认证的参考方案DPD和DPI，给出相应的优化算法，结果表明理论分析与仿真一致。


<details>
  <summary>Details</summary>
Motivation: 面对数据负载导致的瞬时SCNR随机性及高计算/信令开销，寻求一个更踏实、低开销的目标函数，以提升系统在杂波环境中的平均检测/合成性能。

Method: 以随机矩阵理论推导平均SCNR的渐近表达，比较PSK/QAM/Gaussian等星座与OFDM/SC/AFDM等基带调制对平均SCNR的影响；提出数据载荷相关（DPD）和数据载荷无关（DPI）两类载波设计方案；DPD通过分数规划与KKT得到闭式解，DPI通过流形优化处理秩一约束；并给出两种优化算法及仿真验证。

Result: 在固定基底下，PSK相对QAM和高斯星座能获得更高的平均SCNR；对任意星座，OFDM优于SC与AFDM；DPD与DPI在性能与实现复杂度之间给出权衡；理论近似在中等维数下仍然准确，仿真验证算法有效性。

Conclusion: 将目标从瞬时SCNR转向平均SCNR可显著降低实现难度并提供更实用的性能预测；基于RMT的渐近分析为载荷相关的自适应MF设计提供了有力工具，DPD与DPI两种方案分别适用于追求最高瞬时性能或更优的平均性能及复杂度权衡。

Abstract: This paper investigates the performance of the adaptive matched filtering (AMF) in cluttered environments, particularly when operating with superimposed signals. Since the instantaneous signal-to-clutter-plus-noise ratio (SCNR) is a random variable dependent on the data payload, using it directly as a design objective poses severe practical challenges, such as prohibitive computational burdens and signaling overhead. To address this, we propose shifting the optimization objective from an instantaneous to a statistical metric, which focuses on maximizing the average SCNR over all possible payloads. Due to its analytical intractability, we leverage tools from random matrix theory (RMT) to derive an asymptotic approximation for the average SCNR, which remains accurate even in moderate-dimensional regimes. A key finding from our theoretical analysis is that, for a fixed modulation basis, the PSK achieves a superior average SCNR compared to QAM and the pure Gaussian constellation. Furthermore, for any given constellation, the OFDM achieves a higher average SCNR than SC and AFDM. Then, we propose two pilot design schemes to enhance system performance: a Data-Payload-Dependent (DPD) scheme and a Data-Payload-Independent (DPI) scheme. The DPD approach maximizes the instantaneous SCNR for each transmission. Conversely, the DPI scheme optimizes the average SCNR, offering a flexible trade-off between sensing performance and implementation complexity. Then, we develop two dedicated optimization algorithms for DPD and DPI schemes. In particular, for the DPD problem, we employ fractional optimization and the KKT conditions to derive a closed-form solution. For the DPI problem, we adopt a manifold optimization approach to handle the inherent rank-one constraint efficiently. Simulation results validate the accuracy of our theoretical analysis and demonstrate the effectiveness of the proposed methods.

</details>


### [97] [On the Fundamental Tradeoff of Joint Communication and QCD: The Monostatic Case](https://arxiv.org/abs/2512.08332)
*Sung Hoon Lim,Daewon Seo*

Main category: cs.IT

TL;DR: Proposes a joint coding strategy JCCS for ISAC in a monostatic setting, trading off communication rate and quickest change detection delay via feedback-driven subblock coding; provides state-dependent mutual information and KL divergence-based characterization, plus a partial converse for asymptotic optimality; analyzes binary and MIMO Gaussian channels to illustrate tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Integrate sensing (QCD) and communication in ISAC to optimize overall system performance. Leverage real-time state estimation and feedback to dynamically adapt coding, reducing detection delay without sacrificing throughput excessively.

Method: Introduce JCCS: a subblock coding scheme that uses feedback to adapt coding based on current state estimates. Derive achievable rate-delay region using state-dependent mutual information and KL divergence. Provide a partial converse showing asymptotic optimality of the detection algorithm within JCCS. Apply framework to binary and MIMO Gaussian channels to extract design insights.

Result: Characterizes the achievable rate-delay region under JCCS for ISAC with feedback. Establishes a partial converse indicating asymptotic optimality of the proposed detection algorithm within the scheme. Demonstrates insights for optimal tradeoffs in binary and MIMO Gaussian channels.

Conclusion: JCCS offers a rigorous framework to balance communication throughput and change-detection delay in ISAC. Feedback-enabled, subblock coding yields favorable tradeoffs; results on binary and MIMO channels illustrate practical design guidelines for ISAC systems.

Abstract: This paper investigates the fundamental tradeoff between communication and quickest change detection (QCD) in integrated sensing and communication (ISAC) systems under a monostatic setup. We introduce a novel Joint Communication and quickest Change subblock coding Strategy (JCCS) that leverages feedback to adapt coding dynamically based on real-time state estimation. The achievable rate-delay region is characterized using state-dependent mutual information and KL divergence, providing a comprehensive framework for analyzing the interplay between communication performance and detection delay. Moreover, we provide a partial converse demonstrating the asymptotic optimality of the proposed detection algorithm within the JCCS framework. To illustrate the practical implications, we analyze binary and MIMO Gaussian channels, revealing insights into achieving optimal tradeoffs in ISAC system design.

</details>


### [98] [On Discrete Ambiguity Functions of Random Communication Waveforms](https://arxiv.org/abs/2512.08352)
*Ying Zhang,Fan Liu,Yifeng Xiong,Weijie Yuan,Shuangyang Li,Le Zheng,Tony Xiao Han,Christos Masouros,Shi Jin*

Main category: cs.IT

TL;DR: 本文系统刻画了随机调制条件下离散延迟-Doppler的模糊函数（AF），给出DP-AF和FST-AF的闭式期望尖锐度（ESL）和综合尖锐度（EISL）；在DP-AF下归一化的EISL对所有正交波形相同；FST-AF的结果依赖星座尾部（峰度），对亚高斯星座OFDM最优，对超高斯星座OTFS最优；并对SC、OFDM、OTFS、AFDM在两框架下进行对比与数值验证，揭示了no-go（不可在任意二维区域达到最小ESL）的几何约束。


<details>
  <summary>Details</summary>
Motivation: 为未来的ISAC（信息与雷达一体化）系统提供对延迟-多普勒域的基本理解，揭示在随机调制和正交波形下AF的结构特性与固有局限，统一DP-AF与FST-AF的分析框架，利用Weyl-Heisenberg群获得几何洞见。

Method: 提出统一分析框架，区分离散周期AF（DP-AF）与快速-慢时AF（FST-AF），并以有限 Weyl-Heisenberg 群将延迟-Doppler位移映射为算子作用，推导并给出DP-AF与FST-AF的ESL与EISL的闭式表达。DP-AF 证明归一化EISL对正交波形无关；FST-AF 通过星座的峰度（kurtosis）揭示阈值性行为：亚高斯星座下OFDM最优，超高斯星座下OTFS最优。并通过四种典型波形（SC、OFDM、OTFS、AFDM）在两框架下的数值验证。

Result: 得到DP-AF与FST-AF的闭式ESL/EISL表达；DP-AF下存在no-go的几何约束，最小ESL不能在任意二维区域实现；FST-AF呈现星座尾度依赖的两类工作区间（OFDM vs OTFS）；对四种波形给出对比分析并有数值验证。

Conclusion: DP-AF框架下不存在二维区域的全局最优波形，呈现固有的几何约束；FST-AF框架下波形优劣由星座尾部特性决定，提供ISAC波形设计的具体指针（亚高斯→OFDM，超高斯→OTFS），并用四种代表波形进行对比与验证，扩展了对延迟-多普勒AF结构的理解与应用边界。

Abstract: This paper provides a fundamental characterization of the discrete ambiguity functions (AFs) of random communication waveforms under arbitrary orthonormal modulation with random constellation symbols, which serve as a key metric for evaluating the delay-Doppler sensing performance in future ISAC applications. A unified analytical framework is developed for two types of AFs, namely the discrete periodic AF (DP-AF) and the fast-slow time AF (FST-AF), where the latter may be seen as a small-Doppler approximation of the DP-AF. By analyzing the expectation of squared AFs, we derive exact closed-form expressions for both the expected sidelobe level (ESL) and the expected integrated sidelobe level (EISL) under the DP-AF and FST-AF formulations. For the DP-AF, we prove that the normalized EISL is identical for all orthogonal waveforms. To gain structural insights, we introduce a matrix representation based on the finite Weyl-Heisenberg (WH) group, where each delay-Doppler shift corresponds to a WH operator acting on the ISAC signal. This WH-group viewpoint yields sharp geometric constraints on the lowest sidelobes: The minimum ESL can only occur along a one-dimensional cut or over a set of widely dispersed delay-Doppler bins. Consequently, no waveform can attain the minimum ESL over any compact two-dimensional region, leading to a no-optimality (no-go) result under the DP-AF framework. For the FST-AF, the closed-form ESL and EISL expressions reveal a constellation-dependent regime governed by its kurtosis: The OFDM modulation achieves the minimum ESL for sub-Gaussian constellations, whereas the OTFS waveform becomes optimal for super-Gaussian constellations. Finally, four representative waveforms, namely, SC, OFDM, OTFS, and AFDM, are examined under both frameworks, and all theoretical results are verified through numerical examples.

</details>


### [99] [Skew polynomial representations of matrix algebras and applications to coding theory](https://arxiv.org/abs/2512.08602)
*Alessandro Neri,Paolo Santonastaso*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We extend the existing skew polynomial representations of matrix algebras which are direct sum of matrix spaces over division rings. In this representation, the sum-rank distance between two tuples of matrices is captured by a weight function on their associated skew polynomials, defined through degrees and greatest common right divisors with the polynomial that defines the representation. We exploit this representation to construct new families of maximum sum-rank distance (MSRD) codes over finite and infinite fields, and over division rings. These constructions generalize many of the known existing constructions of MSRD codes as well as of optimal codes in the rank and in the Hamming metric. As a byproduct, in the case of finite fields we obtain new families of MDS codes which are linear over a subfield and whose length is close to the field size.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [100] [Improvement and Stabilization of Output Voltages in a Vertical Tidal Turbine Using Intelligent Control Strategies](https://arxiv.org/abs/2512.08416)
*Fanambinantsoa Philibert Andriniriniaimalaza,Nour Murad,Randriamaitso Telesphore,Bilal Habachi,Randriatefison Nirilalaina,Manasina Ruffin,Andrianirina Charles Bernard,Ravelo Blaise*

Main category: cs.NI

TL;DR: AI驱动的混合优化（ANN-Fuzzy、PSO、以及ANN-PSO）用于PMSG垂直轴潮汐涡轮的MPPT与输出电压稳定性提升。PSO显著加强电压稳定性，混合ANN-PSO在系统扰动下实现更好的电压调节和实时转速调整。


<details>
  <summary>Details</summary>
Motivation: 潮汐涡轮发电中输出电压在交直流转换阶段易受水流波动影响，亟需提高MPPT效率与电压稳定性，以提升系统可靠性与效率。引入AI技术以自适应优化运行点和控制策略。

Method: 以基于TSR的MPPT结合ANN-Fuzzy控制器作为基础，进一步引入粒子群优化（PSO）来优化参考转速，并实现ANN-PSO的混合方法以实现自适应速度调整与最优功率提取。通过1.5 m/s水流下的仿真评估。

Result: PSO控制在电压稳定性方面相较传统的MPPT-TSR和单纯的ANN-Fuzzy控制具有显著提升；混合ANN-PSO通过对系统变化的动态适应，提供实时的参考转速调整，提升电压调节效果。

Conclusion: AI驱动的混合优化对潮汐能系统的输出电压稳定性具有积极作用，能提升系统的可靠性和效率。

Abstract: This article investigates on the improvement and stabilization of alternating current (AC) and direct current (DC) output voltages in a Permanent Magnet Synchronous Generator (PMSG) driven by a vertical-axis tidal turbine using advanced control strategies. The research integrates artificial intelligence (AI)-based techniques to enhance voltage stability and efficiency. Initially, the Maximum Power Point Tracking (MPPT) approach based on Tip Speed Ratio (TSR) and Artificial Neural Network (ANN) Fuzzy logic controllers is explored. To further optimize the performance, Particle Swarm Optimization (PSO) and a hybrid ANN-PSO methodology are implemented. These strategies aim to refine the reference rotational speed of the turbine while minimizing deviations from optimal power extraction conditions. The simulation results of a tidal turbine operating at a water flow velocity of 1.5 m/s demonstrate that the PSO-based control approach significantly enhances the voltage stability compared to conventional MPPT-TSR and ANN-Fuzzy controllers. The hybrid ANN-PSO technique improves the voltage regulation by dynamically adapting to system variations and providing real-time reference speed adjustments. This research highlights the AI-based hybrid optimization benefit to stabilize the output voltage of tidal energy systems, thereby increasing reliability and efficiency in renewable energy applications.

</details>


### [101] [Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR](https://arxiv.org/abs/2512.08626)
*Agrim Bari,Gustavo de Veciana,Yuqi Zhou*

Main category: cs.NI

TL;DR: 提出面向相关请求的分组客户端请求模型，并在该模型下分析 LRU 与 LFU 的局限，提出 LFRU 策略，在 VR 场景数据集上与 LRU/LFU、Belady 对比，显示 LFRU 在相关场景下显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决边缘缓存中由于请求相关性导致的性能下降；传统 LRU/LFU 在相关性强的场景下表现不稳定；需要一个能动态推断因果依赖并在驱逐时考虑这些关系的策略。

Method: 1) 提出 grouped client request model，扩展独立参考模型以捕捉多种请求相关性；2) 在该模型下对 LRU 进行理论分析，发现缓存容量大小决定最优策略：小/中等缓存下 LFU 最优，大缓存下 LRU 更优；3) 提出 LFRU（Least Following and Recently Used），在线学习并自适应推断因果关系的缓存策略，通过优先保留可能被下一次请求的对象来进行驱逐；4) 构建 VR 场景数据集，评估在真实相关请求下的缓存策略表现；5) 将 LFRU与 LRU、LFU以及离线最优 Belady 进行对比。

Result: 从仿真/实验结果来看，LFRU 的表现至少等同于 LRU/LFU，在某些设置下对 LRU 提升最高可达 2.9x，对 LFU 提升最高可达 1.9x；在结构化相关场景中接近离线最优 Belady。

Conclusion: 提出的 LFRU 能在存在请求因果关系的场景下提供稳健的缓存性能，优于传统策略，尤其在大缓存容量和存在强相关性时；VR 基准数据集有助于评估未来的相关性感知缓存策略。

Abstract: Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.
  In this paper, we introduce the \textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.
  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.

</details>


### [102] [ITU-T Y.2325: NGN Evolution Towards Future](https://arxiv.org/abs/2512.08695)
*Rashmi Kamran,Shwetha Kiran,Pranav Jha,Rashmi Yadav,Abhay Karandikar,Prasanna Chaporkar*

Main category: cs.NI

TL;DR: ITU-T Y.2325 proposes an evolved NGN architecture that decouples all services, including internal network services like mobility and authentication, from the transport stratum, offering a scalable and modular template for future networks (e.g., IMT-2030/6G).


<details>
  <summary>Details</summary>
Motivation: NGN originally decoupled external services from transport but kept internal services tightly bound to the transport plane, causing duplication and lack of scalability across transport technologies. An evolved synergy is needed to unify service-transport boundaries and support future networks.

Method: Analytical review of ITU-T Y.2325, comparing it with traditional NGN architecture, and discussing implications for mobility, authentication, and internal services across various transport technologies.

Result: Y.2325 presents a generalized decoupled architecture where all services, including internal network services, are independent from the transport stratum, enabling modularity, easier evolution, and better scalability for heterogeneous networks including 6G.

Conclusion: Adopting the evolved NGN framework can serve as a universal template for future telecom networks by abstracting internal services from transport, reducing duplication, and facilitating consistent service delivery across technologies.

Abstract: International Telecommunications Union (ITU) defined Next Generation Network (NGN) underlies most wireline and wireless packet-based telecommunications networks. A key design principle of NGN is decoupling of service-related functions from the underlying transport stratum, making user services independent of transport technologies. Interestingly, the NGN architecture, as defined in ITU standards, did not follow this design principle for internal network services, e.g., mobility, or authentication though adhering for external user services like IPTV or Multimedia services. These internal services are handled by the NGN transport control plane, making them an intrinsic part of the transport stratum, resulting in a tightly coupled service and transport functionality as opposed to the proclaimed design goal. This design choice may force each transport technology to support internal services individually, e.g., separate authentication service for each transport, leading to duplication. Since the NGN architecture is the base underlying architecture for most packet-based telecommunications network including advanced cellular networks like 4th/5th Generation cellular networks, the limitation persists in these cellular networks as well. To remedy the situation, the decoupling of service and transport can be generalized to include internal services like mobility and authentication also. In this context, the recently published ITU Y.2325 recommendation, defines an evolved NGN architecture, wherein all services, including internal network services, are decoupled from the transport stratum. The proposal results in a more scalable and modular evolved NGN architecture that can be used as a template for all future telecom networks including IMT-2030 (6th generation mobile networks). In this article, we review the evolved NGN architecture, as proposed in ITU-T Y.2325.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [103] [Security Analysis of Integer Learning with Errors with Rejection Sampling](https://arxiv.org/abs/2512.08172)
*Kyle Yates,Antsa Pierrottet,Abdullah Al Mamun,Ryann Cartor,Mashrur Chowdhury,Shuhong Gao*

Main category: cs.CR

TL;DR: 对ASIA-CRYPT 2018提出的基于线性最小二乘的ILWE攻击及其在CRYSTALS-Dilithium小参数中的直接应用的理论与实验研究，结果显示ILWE基础的签名方案仍具安全性。


<details>
  <summary>Details</summary>
Motivation: 评估在不依赖侧信道信息的前提下，是否直接把ILWE的样本来自签名构造就能暴露私钥或破坏CRYSTALS-Dilithium等基于ILWE的签名方案的安全性，并验证小参数ILWE在现实实现中的风险。

Method: 理论分析与实验研究结合：直接从签名构造ILWE实例，采用将多项式模运算转化为实数矩阵运算的新型仿真设计，设计处理大样本的高效算法，进行大规模仿真实验。

Result: 实验结果表明所提出的线性最小二乘攻击在小参数ILWE实例上的效果有限，未能破坏CRYSTALS-Dilithium等基于ILWE的签名方案的安全性，支持ILWE相关方案的安全性声明。

Conclusion: ILWE相关签名在现实世界场景仍具鲁棒性；但需关注仿真方法的数值稳定性和参数选择，以及现实应用中对侧信道和参数规模的综合评估，并讨论在ITS等实际应用中的潜在影响。

Abstract: At ASIACRYPT 2018, a digital attack based on linear least squares was introduced for a variant of the learning with errors (LWE) problem which omits modular reduction known as the integer learning with errors problem (ILWE). In this paper, we present a theoretical and experimental study of the effectiveness of the attack when applied directly to small parameter ILWE instances found in popular digital signature schemes such as CRYSTALS-Dilithium which utilize rejection sampling. Unlike other studies which form ILWE instances based on additional information obtained from side-channel attacks, we take a more direct approach to the problem by constructing our ILWE instance from only the obtained signatures. We outline and introduce novel techniques in our simulation designs such as modular polynomial arithmetic via matrices in $\mathbb{R}$, as well as algorithms for handling large sample sizes efficiently. Our experimental results reinforce the proclaimed security of signature schemes based on ILWE. We additionally discuss the implications of our work and digital signatures as a whole in regards to real-world applications such as in Intelligent Transportation Systems (ITS).

</details>


### [104] [MIRAGE: Misleading Retrieval-Augmented Generation via Black-box and Query-agnostic Poisoning Attacks](https://arxiv.org/abs/2512.08289)
*Tailun Chen,Yu He,Yan Wang,Shuo Shao,Haolun Zheng,Zhihao Liu,Jinfeng Li,Yuefeng Chen,Zhixuan Chu,Zhan Qin*

Main category: cs.CR

TL;DR: MIRAGE is a multi-stage poisoning framework targeting retrieval-augmented generation (RAG) systems under strict black-box and query-agnostic settings, combining persona-driven query synthesis, semantic anchoring, and an adversarial variant of Test-Time Preference Optimization to maximize persuasion; evaluated on a new cross-domain benchmark, it achieves higher attack efficacy and stealth with strong transferability, underscoring the need for defenses.


<details>
  <summary>Details</summary>
Motivation: RAG systems expose a critical attack surface through corpus poisoning. Prior work often relies on white-box access or known queries, which are unrealistic in practice. This work aims to close the gap by proposing a practical, black-box, query-agnostic poisoning pipeline and a rigorous benchmark.

Method: MIRAGE operates as a multi-stage automation that (1) uses persona-driven query synthesis to approximate latent user search distributions, (2) applies semantic anchoring to embed intents into the corpus for high retrieval visibility with imperceptible changes, and (3) leverages an adversarial variant of Test-Time Preference Optimization (TPO) to maximize persuasive impact. It relies on surrogate model feedback and is evaluated on a new benchmark derived from three long-form, domain-specific datasets.

Result: Experiments show MIRAGE significantly outperforms existing baselines in attack efficacy and stealth, with strong transferability across diverse retriever-LLM configurations, demonstrating the practicality of the threat and the urgency of robust defenses.

Conclusion: The study highlights the severity of corpus poisoning risks in RAG systems under realistic constraints and calls for developing defenses. The introduced benchmark provides a rigorous platform for evaluating future attacks and defenses.

Abstract: Retrieval-Augmented Generation (RAG) systems enhance LLMs with external knowledge but introduce a critical attack surface: corpus poisoning. While recent studies have demonstrated the potential of such attacks, they typically rely on impractical assumptions, such as white-box access or known user queries, thereby underestimating the difficulty of real-world exploitation. In this paper, we bridge this gap by proposing MIRAGE, a novel multi-stage poisoning pipeline designed for strict black-box and query-agnostic environments. Operating on surrogate model feedback, MIRAGE functions as an automated optimization framework that integrates three key mechanisms: it utilizes persona-driven query synthesis to approximate latent user search distributions, employs semantic anchoring to imperceptibly embed these intents for high retrieval visibility, and leverages an adversarial variant of Test-Time Preference Optimization (TPO) to maximize persuasion. To rigorously evaluate this threat, we construct a new benchmark derived from three long-form, domain-specific datasets. Extensive experiments demonstrate that MIRAGE significantly outperforms existing baselines in both attack efficacy and stealthiness, exhibiting remarkable transferability across diverse retriever-LLM configurations and highlighting the urgent need for robust defense strategies.

</details>


### [105] [Exposing and Defending Membership Leakage in Vulnerability Prediction Models](https://arxiv.org/abs/2512.08291)
*Yihan Liao,Jacky Keung,Xiaoxue Ma,Jingyu Zhang,Yicheng Sun*

Main category: cs.CR

TL;DR: 研究面向代码漏洞预测的成员身份推断攻击（MIA），比较LSTM、BiGRU、CodeBERT等模型及不同输出形式（嵌入、logits、损失、置信度）的攻击效果；发现logits与损失最具信息量；提出NMID，通过输出 masking 与高斯噪声干扰推断，显著降低攻击AUC（从接近1降至＜0.65），同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 代码分析中的AI模型可能暴露训练集成员信息，隐私风险尚未充分研究。本工作填补在安全关键的代码分析任务中对MIA的系统评估的空缺，并提出可行的防御。

Method: 在多种体系结构（LSTM、BiGRU、CodeBERT）与多种特征组合（嵌入、logits、loss、置信度）下，评估黑盒与灰盒设置下的成员身份推断攻击。提出NMID，结合输出级掩蔽与高斯噪声注入，以打乱攻击者对训练样本与非训练样本之间输出差异的利用。

Result: 发现logits与loss是最具信息量的输出，易被用于成员身份推断。NMID可显著降低攻击AUC，从接近1.0降至低于0.65，同时保持VP模型的预测性能。

Conclusion: 揭示代码分析中潜在的隐私风险，并给出一个轻量但有效的防御方案，具有可操作性。未来工作可进一步扩展到更强的防御策略、不同数据集与任务的鲁棒性评估，以及对模型校准性的影响分析。

Abstract: Neural models for vulnerability prediction (VP) have achieved impressive performance by learning from large-scale code repositories. However, their susceptibility to Membership Inference Attacks (MIAs), where adversaries aim to infer whether a particular code sample was used during training, poses serious privacy concerns. While MIA has been widely investigated in NLP and vision domains, its effects on security-critical code analysis tasks remain underexplored. In this work, we conduct the first comprehensive analysis of MIA on VP models, evaluating the attack success across various architectures (LSTM, BiGRU, and CodeBERT) and feature combinations, including embeddings, logits, loss, and confidence. Our threat model aligns with black-box and gray-box settings where prediction outputs are observable, allowing adversaries to infer membership by analyzing output discrepancies between training and non-training samples. The empirical findings reveal that logits and loss are the most informative and vulnerable outputs for membership leakage. Motivated by these observations, we propose a Noise-based Membership Inference Defense (NMID), which is a lightweight defense module that applies output masking and Gaussian noise injection to disrupt adversarial inference. Extensive experiments demonstrate that NMID significantly reduces MIA effectiveness, lowering the attack AUC from nearly 1.0 to below 0.65, while preserving the predictive utility of VP models. Our study highlights critical privacy risks in code analysis and offers actionable defense strategies for securing AI-powered software systems.

</details>


### [106] [Secure Audio Embedding in Images using Nature-Inspired Optimization](https://arxiv.org/abs/2512.08299)
*Aman Kumar,Ankit Chaudhary*

Main category: cs.CR

TL;DR: 提出一种基于 Harris Hawks Optimization 的改进 LSB 隐写方法，将音频隐藏于图像中，并在 PSNR、SSIM、MSE 指标上实现更优的嵌入质量和容量。


<details>
  <summary>Details</summary>
Motivation: 在数字多媒体通信中保护敏感数据，隐藏而非加密传输以提高隐蔽性。传统 LSB 隐写易被检测或导致图像失真，需通过优化嵌入位置以提升视觉质量和容量。

Method: 利用 Harris Hawks Optimization 优化选择嵌入像素位置，在保持音频隐藏的同时最大化图像质量（使用 PSNR、SSIM、MSE 评估），并与现有方法对比。

Result: 实验表明：HHO 能在图像质量、鲁棒性和嵌入容量方面优于常见方法，达到更高的 PSNR/SSIM 并降低 MSE。

Conclusion: 将 HHO 与 LSB 隐写结合的音频嵌入方案可有效提升隐写质量与容量，为后续算法改进和鲁棒性增强提供基础。

Abstract: In todays digital world, protecting sensitive data is very essential. Steganography hides the existence of secret data instead of its content, providing better security for multimedia communication. This paper proposes a new technique for hiding audio files inside images using the Least Significant Bit (LSB) method optimized by the Harris Hawks Optimization (HHO) algorithm. HHO is a nature-inspired metaheuristic that imitates the hunting behavior of Harris hawks to find optimal pixel positions for embedding data. The proposed method is evaluated using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Square Error (MSE). Experimental results show that HHO achieves better image quality, robustness, and embedding capacity compared to existing methods.

</details>


### [107] [Developing a Strong CPS Defender: An Evolutionary Approach](https://arxiv.org/abs/2512.08320)
*Qingyuan Hu,Christopher M. Poskitt,Jun Sun,Yuqi Chen*

Main category: cs.CR

TL;DR: Evo-Defender: an interactive evolutionary framework for CPS anomaly detection that jointly optimizes a guided fuzzing attacker and a self-evolving defender; validated on realistic CPS testbeds with 600+ attack scenarios, achieving up to 2.7% improvement on unseen attacks and improved data efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detectors rely on one-off training on curated or fuzzed data, which may fail to generalize to unseen, nuanced attacks. By actively challenging attackers through an evolving defender–attacker loop, defenses can be strengthened to detect more sophisticated intrusions in CPS.

Method: An iterative framework where a smart attacker uses guided fuzzing to explore diverse, non-redundant attack strategies, and a self-evolving defender employs incremental learning to adapt to new attack patterns. Implemented on two realistic CPS testbeds (Tennessee Eastman process and Robotic Arm Assembly Workstation) with over 600 injected attack scenarios, and evaluated via end-to-end attack detection experiments.

Result: Evo-Defender achieves up to 2.7% higher detection performance on unseen attack scenarios compared with state-of-the-art baselines, while also improving training data efficiency, enabling faster and more robust detection.

Conclusion: Dynamic attacker–defender interplay offers a practical route to robust, generalizable CPS anomaly detection. The framework demonstrates that adversarially generated, diverse attack data can substantially enhance detector performance without excessive labeling or data collection requirements.

Abstract: Cyber-physical systems (CPSs) are used extensively in critical infrastructure, underscoring the need for anomaly detection systems that are able to catch even the most motivated attackers. Traditional anomaly detection techniques typically do `one-off' training on datasets crafted by experts or generated by fuzzers, potentially limiting their ability to generalize to unseen and more subtle attack strategies. Stopping at this point misses a key opportunity: a defender can actively challenge the attacker to find more nuanced attacks, which in turn can lead to more effective detection capabilities. Building on this concept, we propose Evo-Defender, an evolutionary framework that iteratively strengthens CPS defenses through a dynamic attacker-defender interaction. Evo-Defender includes a smart attacker that employs guided fuzzing to explore diverse, non-redundant attack strategies, while the self-evolving defender uses incremental learning to adapt to new attack patterns. We implement Evo-Defender on two realistic CPS testbeds: the Tennessee Eastman process and a Robotic Arm Assembly Workstation, injecting over 600 attack scenarios. In end-to-end attack detection experiments, Evo-Defender achieves up to 2.7% higher performance than state-of-the-art baselines on unseen scenarios, while utilizing training data more efficiently for faster and more robust detection.

</details>


### [108] [USCSA: Evolution-Aware Security Analysis for Proxy-Based Upgradeable Smart Contracts](https://arxiv.org/abs/2512.08372)
*Xiaoqi Li,Lei Xie,Wenkai Li,Zongwei Li*

Main category: cs.CR

TL;DR: 提出一个基于 AST 差分分析的可升级智能合约安全分析器 USCSA，用于在升级过程中评估风险并检测升级诱发的漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着区块链智能合约的长期演化，需要对升级过程中的安全性进行持续保障，升级操作常引入新漏洞，需要高效的分析工具来保障升级后系统的安全性和完整性。

Method: 通过对升级相关变更执行 AST 差分分析，评估升级过程的风险；构建并分析了 3,546 例可升级合约漏洞案例，覆盖重入、访问控制缺陷、整数溢出等常见类别。

Result: 在漏洞检测方面，USCSA 的准确率为 92.3%、召回率为 89.7%、F1-score 为 91.0%；在高风险变更映射方面较传统方法提升了 30%。

Conclusion: USCSA 为区块链应用的安全审计提供了一种新颖且高效的解决方案，显著提升对可升级智能合约的安全性与完整性。

Abstract: In the case of upgrading smart contracts on blockchain systems, it is essential to consider the continuity of upgrade and subsequent maintenance. In practice, upgrade operations often introduce new vulnerabilities. To address this, we propose an Upgradable Smart Contract Security Analyzer, USCSA, which evaluates the risks associated with the upgrade process using the Abstract Syntax Tree (AST) differential analysis. We collected and analyzed 3,546 cases of vulnerabilities in upgradable contracts,covering common vulnerability categories such as reentrancy, access control flaws, and integer overflow. Experimental results show that USCSA achieves an accuracy of 92.3%, recall of 89.7%, and F1-score of 91.0% in detecting upgrade-induced vulnerabilities.
  In addition, the efficiency of mapping high-risk changes has achieved a 30% improvement over the conventional approach. As a result, USCSA provides a significant advantage to improve the security and integrity of upgradable smart contracts, providing a novel and efficient solution to secure audits on blockchain applications.

</details>


### [109] [Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2512.08417)
*Yinan Zhong,Qianhao Miao,Yanjiao Chen,Jiangyi Deng,Yushi Cheng,Wenyuan Xu*

Main category: cs.CR

TL;DR: Rennervate: 用注意力特征在逐字级别检测并清洗间接提示注入的防御框架，显著优于多种基线，并开源FIPI数据集，具备对新攻击的迁移性和对自适应攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM应用易受IPI攻击，需能够在不破坏模型功能的前提下，在更细粒粒度的token层面检测并消除注入恶意指令；目前防御方法不足以覆盖该细粒度场景。

Method: 提出一个基于2步注意力汇聚的token-level检测器；通过对注意力头和响应tokens进行聚合，完成IPI检测与清洗；构建并开源FIPI数据集；在5个LLM、6组数据集上进行广泛实验。

Result: Rennervate在与15种对比方法的对比中表现更好，实现高精度，且对未见攻击具备迁移性，对自适应对手鲁棒。

Conclusion: 该框架实现了对IPI的细粒度检测与清洗，提升大模型在现实应用中的安全性与鲁棒性，并通过开放数据集促进后续研究。

Abstract: Large Language Models (LLMs) have been integrated into many applications (e.g., web agents) to perform more sophisticated tasks. However, LLM-empowered applications are vulnerable to Indirect Prompt Injection (IPI) attacks, where instructions are injected via untrustworthy external data sources. This paper presents Rennervate, a defense framework to detect and prevent IPI attacks. Rennervate leverages attention features to detect the covert injection at a fine-grained token level, enabling precise sanitization that neutralizes IPI attacks while maintaining LLM functionalities. Specifically, the token-level detector is materialized with a 2-step attentive pooling mechanism, which aggregates attention heads and response tokens for IPI detection and sanitization. Moreover, we establish a fine-grained IPI dataset, FIPI, to be open-sourced to support further research. Extensive experiments verify that Rennervate outperforms 15 commercial and academic IPI defense methods, achieving high precision on 5 LLMs and 6 datasets. We also demonstrate that Rennervate is transferable to unseen attacks and robust against adaptive adversaries.

</details>


### [110] [LLM-based Vulnerable Code Augmentation: Generate or Refactor?](https://arxiv.org/abs/2512.08493)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.CR

TL;DR: LLM辅助的数据增强可缓解漏洞代码的类别不平衡问题；通过Qwen2.5-Coder生成新样本与语义保持的重构相结合，提升漏洞分类性能，混合策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 漏洞相关数据集存在严重的类别不平衡，导致基于深度学习的漏洞分类器效果受限。利用数据增强来扩充稀缺的弱势CWEs是一个可行途径，本文比较受控生成与语义保持的重构两种LLM驱动的增强方法在漏洞代码上的效果。

Method: 在SVEN数据集上，使用Qwen2.5-Coder生成增强的漏洞样本，并以CodeBERT作为漏洞分类器进行评估。具体比较包括直接生成新的漏洞样本与对现有样本进行语义保持的重构两种策略，以及二者的混合策略。

Result: 实验表明，LLM驱动的数据增强能够以简单流程有效丰富漏洞代码库，生成质量在可接受范围内；两种增强方式各有优点，混合策略对分类器性能提升最显著。

Conclusion: LLM驱动的增强方法是缓解漏洞数据不平衡的有效手段，尤其是当结合生成与重构的混合策略时，能够显著提升漏洞分类器的性能。

Abstract: Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.

</details>


### [111] [Labeled Delegated PSI and its Applications in the Public Sector](https://arxiv.org/abs/2512.08558)
*Kristof Verslype,Florian Kerschbaum,Cyprien Delpech de Saint Guilhem,Bart De Decker,Jorn Lapon*

Main category: cs.CR

TL;DR: 提出了一个带有可组合输出函数的多方委托私集交集（D-PSI）协议，能在多数据提供方之间私密地链接和收集数据，输出包括加密载荷和伪匿名标识符，且在标准模型下对合谋半诚实提供方和非合谋、可能恶意的数据收集方均具安全性。


<details>
  <summary>Details</summary>
Motivation: 处理高度分散的敏感公民数据，提升跨机构数据整合以支持医疗、反欺诈、政策制定等，但现有 D-PSI 在交付有效载荷方面存在障碍，迫切需要在公共部门环境中安全部署。

Method: 提出新的 D-PSI 协议，具备可组合输出函数，能够输出加密载荷与伪匿名标识符；在标准模型下对合谋半诚实数据提供方和可能恶意的独立数据收集方进行安全性分析。

Result: 理论上证明安全性；实现了可组合的输出功能，允许在多方数据提供方之间进行私密链接和数据收集，输出包含 payload 与伪标识符。

Conclusion: 该方案适合在公共领域的场景落地，解决了将多方数据安全链接与载荷交付的问题，有助于提升隐私保护同时实现数据分析。

Abstract: Sensitive citizen data, such as social, medical, and fiscal data, is heavily fragmented across
  public bodies and the private domain. Mining the combined data sets allows for new insights that otherwise remain hidden.
  Examples are improved healthcare, fraud detection, and evidence-based policy making.
  (Multi-party) delegated private set intersection (D-PSI) is a privacy-enhancing technology to link data across multiple data providers using a data collector.
  However, before it can be deployed in these use cases, it needs to be enhanced with additional functions, e.g., securely delivering payload only for elements in the intersection.
  Although there has been recent progress in the communication and computation requirements of D-PSI, these practical obstacles have not yet been addressed.
  This paper is the result of a collaboration with a governmental organization responsible for collecting, linking, and pseudonymizing data.
  Based on their requirements, we design a new D-PSI protocol with composable output functions, including encrypted payload and pseudonymized identifiers.
  We show that our protocol is secure in the standard model against colluding semi-honest data providers and against a non-colluding, possibly malicious independent party, the data collector.
  It, hence, allows to privately link and collect data from multiple data providers suitable for deployment in these use cases in the public sector.

</details>


### [112] [Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework](https://arxiv.org/abs/2512.08802)
*Sadegh Momeni,Ge Zhang,Birkett Huber,Hamza Harkous,Sam Lipton,Benoit Seguin,Yanis Pavlidis*

Main category: cs.CR

TL;DR: 提出一个两阶段的混合框架，在初阶段用松散的 YARA 规则实现高召回的粗筛，后阶段用机器学习分类器过滤假阳性；借助 Simula 进行无标签或种子较少的合成数据来训练，并通过主动学习和实时反馈实现自适应更新。该系统在大规模生产环境中有效处理巨量日志并显著降低告警工单数量，精度随时间提升且维护成本低。


<details>
  <summary>Details</summary>
Motivation: 尽管在安全领域，随着机器学习的应用不断深入，基于规则的检测仍广泛存在，因为 ML 解决方案资源密集且对专业技能要求高。纯规则方法虽高效，但刚性导致高误报/漏报且需要持续手动维护；因此需要让更多安全从业者参与到 ML 学习与配置中，降低进入门槛和维护成本，同时解决数据稀缺问题。

Method: 阶段1：使用故意松散的 YARA 规则进行粗筛，优先实现高召回；阶段2：用 ML 分类器从第一阶段输出中筛选假阳性；为解决数据匮乏，系统采用 Simula 进行种子化或无标签的合成数据生成，以构建训练集；建立持续反馈循环，结合实时调查结果自适应调整模型，防止规则退化；在生产环境中对数万台系统进行长期测试，利用主动学习提升模型精度。

Result: 在生产环境中，初始原始日志数量可达每日约2.5e11事件，通过筛选和 ML 推理后降至仅需每日极少量工单由人工处理；长期试验显示精度随主动学习的应用而提升；整体呈现自给自足、低开销、低维护的特征，便于安全专家引导模型学习。

Conclusion: 该框架实现了 ML 威胁检测的民主化，为 SOC 提供低成本、可持续的学习过程。通过将专家教师式指导与自动化数据生成相结合，提升检测效果并降低对深度数据科学专业知识的依赖，具备对变化威胁的自适应能力与持续优化潜力。

Abstract: Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.
  This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.

</details>


### [113] [PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration](https://arxiv.org/abs/2512.08809)
*Yi Liu,Weixiang Han,Chengjun Cai,Xingliang Yuan,Cong Wang*

Main category: cs.CR

TL;DR: PrivTune 通过在分割学习框架中对 token 表示注入优化的噪声，以实现隐私保护的微调，达到强隐私-效用折中，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型服务中，使用私有数据进行微调存在敏感数据泄露风险；基于差分隐私的设备-云协作方法在隐私与实用性之间难以平衡，易被推断攻击或显著损害性能，因此需要更高效的保护方案。

Method: 将噪声注入分割学习底模型的 token 表示，构造将 token 尽量与 n-hop 间接邻居相似的噪声向量；将其视为优化问题以求得最优噪声向量，进而将 d-privacy 噪声分布的均值对齐优化方向，并按 token 重要性缩放噪声以最小化失真；在五个数据集上对 RoBERTa+SSTB进行攻击实验。

Result: 在五个数据集、分类与生成任务上，对嵌入反演和属性推断攻击的实验显示，PrivTune 将攻击成功率降至 10%，同时仅有约 3.33% 的效用损失，相较先进 baselines 有显著提升。

Conclusion: PrivTune 提供了一个高效且隐私保护的微调框架，通过分割学习和优化噪声实现更好的隐私-效用权衡，适用于私有数据微调场景。

Abstract: With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_χ$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.

</details>


### [114] [Improved Pseudorandom Codes from Permuted Puzzles](https://arxiv.org/abs/2512.08918)
*Miranda Christ,Noah Golowich,Sam Gunn,Ankur Moitra,Daniel Wichs*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Watermarks are an essential tool for identifying AI-generated content. Recently, Christ and Gunn (CRYPTO '24) introduced pseudorandom error-correcting codes (PRCs), which are equivalent to watermarks with strong robustness and quality guarantees. A PRC is a pseudorandom encryption scheme whose decryption algorithm tolerates a high rate of errors. Pseudorandomness ensures quality preservation of the watermark, and error tolerance of decryption translates to the watermark's ability to withstand modification of the content.
  In the short time since the introduction of PRCs, several works (NeurIPS '24, RANDOM '25, STOC '25) have proposed new constructions. Curiously, all of these constructions are vulnerable to quasipolynomial-time distinguishing attacks. Furthermore, all lack robustness to edits over a constant-sized alphabet, which is necessary for a meaningfully robust LLM watermark. Lastly, they lack robustness to adversaries who know the watermarking detection key. Until now, it was not clear whether any of these properties was achievable individually, let alone together.
  We construct pseudorandom codes that achieve all of the above: plausible subexponential pseudorandomness security, robustness to worst-case edits over a binary alphabet, and robustness against even computationally unbounded adversaries that have the detection key. Pseudorandomness rests on a new assumption that we formalize, the permuted codes conjecture, which states that a distribution of permuted noisy codewords is pseudorandom. We show that this conjecture is implied by the permuted puzzles conjecture used previously to construct doubly efficient private information retrieval. To give further evidence, we show that the conjecture holds against a broad class of simple distinguishers, including read-once branching programs.

</details>
