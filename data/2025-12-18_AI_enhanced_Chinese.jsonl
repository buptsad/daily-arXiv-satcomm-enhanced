{"id": "2512.15207", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15207", "abs": "https://arxiv.org/abs/2512.15207", "authors": ["Neelaksh Singh", "Jasan Zughaibi", "Denis von Arx", "Bradley J. Nelson", "Michael Muehlebach"], "title": "Remote Magnetic Levitation Using Reduced Attitude Control and Parametric Field Models", "comment": null, "summary": "Electromagnetic navigation systems (eMNS) are increasingly used in minimally invasive procedures such as endovascular interventions and targeted drug delivery due to their ability to generate fast and precise magnetic fields. In this paper, we utilize the OctoMag eMNS to achieve remote levitation and control of a rigid body across large air gaps which showcases the dynamic capabilities of clinical eMNS. A compact parametric analytical model maps coil currents to the forces and torques acting on the levitating object, eliminating the need for computationally expensive simulations or lookup tables and leading to a levitator agnostic modeling approach. Translational motion is stabilized using linear quadratic regulators. A nonlinear time-invariant controller is used to regulate the reduced attitude accounting for the inherent uncontrollability of rotations about the dipole axis and stabilizing the full five degrees of freedom controllable pose subspace. We analyze key design limitations and evaluate the approach through trajectory tracking experiments. This work demonstrates the dynamic capabilities and potential of feedback control in electromagnetic navigation, which is likely to open up new medical applications.", "AI": {"tldr": "\u901a\u8fc7 OctoMag \u7cfb\u7edf\u5b9e\u73b0\u5bf9\u4e00\u4e2a\u521a\u4f53\u5728\u8f83\u5927\u7a7a\u6c14\u95f4\u9699\u4e2d\u7684\u8fdc\u7a0b\u60ac\u6d6e\u4e0e\u63a7\u5236\uff0c\u63d0\u51fa\u57fa\u4e8e\u53c2\u6570\u5316\u89e3\u6790\u6a21\u578b\u7684 eMNS\uff0c\u53bb\u9664\u5bf9\u4eff\u771f\u6216\u67e5\u627e\u8868\u7684\u4f9d\u8d56\uff0c\u5e76\u7ed3\u5408\u7ebf\u6027\u4e8c\u6b21\u8c03\u63a7\u4e0e\u975e\u7ebf\u6027\u65f6\u4e0d\u53d8\u63a7\u5236\u5b9e\u73b0\u591a\u81ea\u7531\u5ea6\u63a7\u5236\u3002", "motivation": "\u5728\u5fae\u521b\u6cbb\u7597\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5feb\u901f\u3001\u7cbe\u51c6\u7684\u78c1\u573a\u63a7\u5236\u6765\u5b9e\u73b0\u8fdc\u7a0b\u5bfc\u822a\u4e0e\u5b9a\u4f4d\uff1b\u73b0\u6709\u65b9\u6cd5\u5bf9\u4eff\u771f\u590d\u6742\u6027\u548c\u5b9e\u65f6\u6027\u5b58\u5728\u6311\u6218\uff0c\u4e14\u9700\u5728\u5927\u7a7a\u6c14\u95f4\u9699\u4e2d\u4fdd\u6301\u7a33\u5b9a\u6027\u3002", "method": "\u6784\u5efa\u7d27\u51d1\u7684\u53c2\u6570\u5316\u89e3\u6790\u6a21\u578b\uff0c\u5c06\u7ebf\u5708\u7535\u6d41\u6620\u5c04\u5230\u4f5c\u7528\u4e8e\u60ac\u6d6e\u5bf9\u8c61\u7684\u529b\u548c\u529b\u77e9\uff1b translational \u91c7\u7528\u7ebf\u6027\u4e8c\u6b21\u8c03\u63a7\u5668\uff08LQR\uff09\u5b9e\u73b0\u7a33\u5b9a\uff1b\u964d\u7ef4\u540e\u4f7f\u7528\u975e\u7ebf\u6027\u65f6\u4e0d\u53d8\u63a7\u5236\u5668\u8c03\u63a7\u964d\u7ef4\u7684\u59ff\u6001\uff0c\u5904\u7406\u81ea\u8f6c\u8f74\u7684\u4e0d\u53ef\u63a7\u6027\u5e76\u7a33\u5b9a\u4e94\u81ea\u7531\u5ea6\u4e2d\u7684\u53ef\u63a7\u4f4d\u59ff\u5b50\u7a7a\u95f4\uff1b\u5206\u6790\u8bbe\u8ba1\u74f6\u9888\u5e76\u901a\u8fc7\u8f68\u8ff9\u8ddf\u8e2a\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u5927\u7a7a\u6c14\u95f4\u9699\u4e2d\u7684\u8fdc\u7a0b\u60ac\u6d6e\u4e0e\u63a7\u5236\u7684\u52a8\u6001\u80fd\u529b\uff1b\u901a\u8fc7\u8f68\u8ff9\u8ddf\u8e2a\u5b9e\u9a8c\u9a8c\u8bc1\u6240\u63d0\u51fa\u6a21\u578b\u4e0e\u63a7\u5236\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u5173\u952e\u8bbe\u8ba1\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86 eMNS \u7684\u52a8\u6001\u80fd\u529b\u4e0e\u6f5c\u529b\uff0c\u57fa\u4e8e\u53cd\u9988\u63a7\u5236\u7684\u65b9\u6cd5\u6709\u671b\u5f00\u542f\u65b0\u7684\u533b\u5b66\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.14776", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.14776", "abs": "https://arxiv.org/abs/2512.14776", "authors": ["Xiangxiang Li", "Haiyan Wang", "Yao Ge", "Xiaohong Shen", "Miaowen Wen", "Shun Zhang", "Yong Liang Guan"], "title": "Low-Complexity Channel Estimation for Internet of Vehicles AFDM Communications With Sparse Bayesian Learning", "comment": null, "summary": "Affine frequency division multiplexing (AFDM) has been considered as a promising waveform to enable high-reliable connectivity in the internet of vehicles. However, accurate channel estimation is critical and challenging to achieve the expected performance of the AFDM systems in doubly-dispersive channels. In this paper, we propose a sparse Bayesian learning (SBL) framework for AFDM systems and develop a dynamic grid update strategy with two off-grid channel estimation methods, i.e., grid-refinement SBL (GR-SBL) and grid-evolution SBL (GE-SBL) estimators. Specifically, the GR-SBL employs a localized grid refinement method and dynamically updates grid for a high-precision estimation. The GE-SBL estimator approximates the off-grid components via first-order linear approximation and enables gradual grid evolution for estimation accuracy enhancement. Furthermore, we develop a distributed computing scheme to decompose the large-dimensional channel estimation model into multiple manageable small-dimensional sub-models for complexity reduction of GR-SBL and GE-SBL, denoted as distributed GR-SBL (D-GR-SBL) and distributed GE-SBL (D-GE-SBL) estimators, which also support parallel processing to reduce the computational latency. Finally, simulation results demonstrate that the proposed channel estimators outperform existing competitive schemes. The GR-SBL estimator achieves high-precision estimation with fine step sizes at the cost of high complexity, while the GE-SBL estimator provides a better trade-off between performance and complexity. The proposed D-GR-SBL and D-GE-SBL estimators effectively reduce complexity and maintain comparable performance to GR-SBL and GE-SBL estimators, respectively.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\uff08SBL\uff09\u7684 AFDM \u901a\u9053\u4f30\u8ba1\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u79bb\u7f51(off-grid)\u4f30\u8ba1\u65b9\u6cd5\uff1aGR-SBL\uff08\u7f51\u683c\u7ec6\u5316\uff09\u548c GE-SBL\uff08\u7f51\u683c\u6f14\u5316\uff09\uff0c\u5e76\u63d0\u51fa\u5206\u5e03\u5f0f\u53d8\u4f53 D-GR-SBL \u548c D-GE-SBL \u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u4eff\u771f\u7ed3\u679c\u663e\u793a\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0cGR-SBL \u7cbe\u5ea6\u6700\u9ad8\u4f46\u4ee3\u4ef7\u8f83\u9ad8\uff0cGE-SBL \u5728\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u66f4\u597d\u6298\u4e2d\uff0c\u5206\u5e03\u5f0f\u65b9\u6848\u5728\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "motivation": "\u5728\u53cc\u591a\u666e\u52d2/\u8272\u6563\u4fe1\u9053\u4e0b\uff0cAFDM \u7684\u901a\u9053\u4f30\u8ba1\u9700\u8981\u9ad8\u7cbe\u5ea6\u4ee5\u6ee1\u8db3\u9ad8\u53ef\u9760\u6027\u901a\u4fe1\u7684\u8981\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4f30\u8ba1\u504f\u79fb\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u5f15\u5165 off-grid \u7684 SBL \u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u591a\u5f84\u4e0e\u65f6\u9891\u6f02\u79fb\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u5b9e\u73b0\u63d0\u5347\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u7684 AFDM \u901a\u9053\u4f30\u8ba1\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd off-grid \u65b9\u6848\uff1aGR-SBL \u901a\u8fc7\u5c40\u90e8\u7f51\u683c\u7ec6\u5316\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff1bGE-SBL \u901a\u8fc7\u4e00\u9636\u7ebf\u6027\u8fd1\u4f3c\u5b9e\u73b0\u7f51\u683c\u5411\u524d\u6f14\u5316\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u3002\u5e76\u63d0\u51fa\u5206\u5e03\u5f0f\u5b9e\u73b0 D-GR-SBL/ D-GE-SBL\uff0c\u5c06\u5927\u7ef4\u5ea6\u6a21\u578b\u5206\u89e3\u4e3a\u82e5\u5e72\u5c0f\u6a21\u578b\u4ee5\u652f\u6301\u5e76\u884c\u5904\u7406\uff0c\u964d\u4f4e\u8ba1\u7b97\u5ef6\u8fdf\u3002", "result": "\u4eff\u771f\u663e\u793a\u63d0\u8bae\u7684\u901a\u9053\u4f30\u8ba1\u65b9\u6848\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7ade\u4e89\u65b9\u6848\u3002GR-SBL \u5728\u7f51\u683c\u7ec6\u5316\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff1bGE-SBL \u63d0\u4f9b\u66f4\u4f18\u7684\u590d\u6742\u5ea6-\u6027\u80fd\u6298\u4e2d\u3002\u5206\u5e03\u5f0f\u7248\u672c D-GR-SBL \u4e0e D-GE-SBL \u80fd\u663e\u8457\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb GR-SBL/GE-SBL \u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684 SBL-AFDM \u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u591a\u666e\u52d2\u901a\u9053\u4e0b\u7684\u79bb\u7f51\u4f30\u8ba1\u95ee\u9898\uff0cGR-SBL \u63d0\u4f9b\u6700\u9ad8\u7cbe\u5ea6\u4f46\u6210\u672c\u8f83\u9ad8\uff0cGE-SBL \u63d0\u4f9b\u66f4\u4f73\u7684\u6298\u4e2d\uff0c\u5206\u5e03\u5f0f\u5b9e\u73b0\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u6027\u4e0e\u5b9e\u65f6\u6027\uff0c\u9002\u5408\u5728\u9ad8\u8981\u6c42\u7684\u5e94\u7528\u573a\u666f\u5982\u7269\u8054\u7f51/\u8f66\u8054\u7f51\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2512.14706", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14706", "abs": "https://arxiv.org/abs/2512.14706", "authors": ["Krunal Jesani", "Dmitry Ignatov", "Radu Timofte"], "title": "LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts", "comment": null, "summary": "Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.", "AI": {"tldr": "LLM-guided NAS pipeline NN-Caption\u751f\u6210\u53ef\u8fd0\u884c\u7684\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\uff0c\u7ec4\u5408LEMUR\u9aa8\u5e72\u7684CNN\u7f16\u7801\u5668\u4e0e\u5e8f\u5217\u89e3\u7801\u5668\uff08LSTM/GRU/Transformer\uff09\uff0c\u5728\u4e25\u683c\u7684Net API\u7ea6\u675f\u4e0b\u5b9e\u73b0\u3002\u4ee5DeepSeek-R1-0528-Qwen3-8B\u4e3a\u4e3b\u751f\u6210\u5668\uff0c\u5e76\u7ed9\u51fa\u63d0\u793a\u6a21\u677f\u4e0e\u793a\u4f8b\u3002\u5bf9MS COCO\u6570\u636e\u96c6\u91c7\u7528BLEU-4\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u751f\u6210\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u8d85\u8fc7\u4e00\u534a\u53ef\u8bad\u7ec3\u5e76\u4ea7\u51fa\u6709\u610f\u4e49\u7684\u63cf\u8ff0\uff1b\u5bf9\u63d0\u793a\u4e2d\u7684\u8f93\u5165\u7ec4\u4ef6\u6570\u91cf\uff085 vs. 10\uff09\u7684\u5f71\u54cd\u8fdb\u884c\u5206\u6790\uff0c\u53d1\u73b0\u63d0\u4f9b\u66f4\u591a\u5019\u9009\u7ec4\u4ef6\u65f6\u6210\u529f\u7387\u7565\u6709\u4e0b\u964d\uff1b\u7ed9\u51fa\u8bad\u7ec3\u8fc7\u7a0b\u66f2\u7ebf\u4e0e\u6700\u9ad8BLEU-4\u503c\u3002\u7814\u7a76\u8868\u660eLLM-guided NAS\u5177\u6709\u6f5c\u529b\uff0cLLM\u4e0d\u4ec5\u63d0\u51fa\u67b6\u6784\uff0c\u8fd8\u7ed9\u51fa\u8d85\u53c2\u6570\u4e0e\u8bad\u7ec3\u7b56\u7565\uff0c\u540c\u65f6\u6307\u51fa\u4ee3\u7801\u5e7b\u89c9\u548cAPI\u5408\u89c4\u7b49\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u89c4\u5219\u4e0e\u8fed\u4ee3\u4ee3\u7801\u4fee\u6b63\u6765\u89e3\u51b3\u3002\u8be5\u5de5\u4f5c\u5c06\u63d0\u793a\u751f\u6210\u4e0e\u81ea\u52a8\u8bc4\u4f30\u7ed3\u5408\u6210\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u7ba1\u7ebf\uff0c\u5e76\u5411\u5f00\u6e90LEMUR\u6570\u636e\u96c6\u6dfb\u52a0\u5927\u91cf\u65b0\u6a21\u578b\u4ee5\u4fc3\u8fdb\u57fa\u51c6\u548cAutoML\u7814\u7a76\u3002", "motivation": "\u964d\u4f4e\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u6210\u672c\u4e0e\u4e13\u4e1a\u77e5\u8bc6\u95e8\u69db\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u67b6\u6784\u8bbe\u8ba1\u4e0e\u8d85\u53c2\u6570\u9009\u62e9\uff0c\u63d0\u9ad8\u53ef\u590d\u73b0\u6027\u4e0eAutoML\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6df1\u5ea6\u63d0\u793a\u7684\u4ee3\u7801\u751f\u6210\uff0c\u91c7\u7528DeepSeek-R1-0528-Qwen3-8B\u4f5c\u4e3a\u4e3b\u751f\u6210\u5668\uff0c\u7ed3\u5408\u4e25\u683c\u7684Net API\u7ea6\u675f\u4eceLEMUR\u5206\u7c7b\u9aa8\u5e72\u4e2d\u6311\u9009CNN\u7f16\u7801\u5668\u5e76\u7f00\u4ee5\u5e8f\u5217\u89e3\u7801\u5668\uff08LSTM/GRU/Transformer\uff09\u5b9e\u73b0\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\uff1b\u8bbe\u8ba1\u63d0\u793a\u6a21\u677f\u548c\u793a\u4f8b\uff0c\u505a5\u621610\u4e2a\u8f93\u5165\u7ec4\u4ef6\u7684\u6bd4\u8f83\uff0c\u8bad\u7ec3\u5e76\u5728MS COCO\u4e0a\u4ee5BLEU-4\u8bc4\u4f30\uff0c\u8bb0\u5f55\u8bad\u7ec3\u52a8\u529b\u5b66\u4e0e\u6700\u9ad8BLEU-4\uff0c\u5e76\u62a5\u544a\u5bf9API\u5408\u89c4\u4e0e\u4ee3\u7801\u5e7b\u89c9\u7684\u5904\u7406\u7b56\u7565\u3002", "result": "\u751f\u6210\u591a\u4efd\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\uff0c\u8d85\u8fc7\u4e00\u534a\u53ef\u6210\u529f\u8bad\u7ec3\u5e76\u4ea7\u51fa\u6709\u610f\u4e49\u7684\u63cf\u8ff0\uff1b5 vs 10\u8f93\u5165\u7ec4\u4ef6\u5bf9\u6210\u529f\u7387\u6709\u5f71\u54cd\uff0c\u66f4\u591a\u5019\u9009\u7ec4\u4ef6\u7565\u964d\uff1b\u62a5\u544a\u4e86\u8bad\u7ec3\u52a8\u6001\u4e0e\u6700\u9ad8BLEU-4\uff1b\u5bf9\u5f00\u6e90LEMUR\u6570\u636e\u96c6\u6dfb\u52a0\u5927\u91cf\u65b0\u6a21\u578b\uff0c\u4fc3\u8fdb\u53ef\u590d\u73b0\u57fa\u51c6\u4e0eAutoML\u7814\u7a76\u3002", "conclusion": "LLM-guided NAS\u5c55\u73b0\u51fa\u6f5c\u5728\u4ef7\u503c\uff0cLLM\u80fd\u63d0\u51fa\u67b6\u6784\u3001\u8d85\u53c2\u6570\u4e0e\u8bad\u7ec3\u5b9e\u8df5\uff1b\u9700\u89e3\u51b3\u4ee3\u7801\u5e7b\u89c9\u4e0eAPI\u5408\u89c4\u7b49\u6311\u6218\uff0c\u501f\u52a9\u63d0\u793a\u89c4\u5219\u4e0e\u8fed\u4ee3\u4fee\u6b63\u53ef\u7f13\u89e3\u3002\u8be5\u7ba1\u7ebf\u5c06\u63d0\u793a\u751f\u6210\u4e0e\u81ea\u52a8\u8bc4\u4f30\u7ed3\u5408\uff0c\u63d0\u5347\u53ef\u590d\u73b0\u6027\u5e76\u6269\u5c55AutoML\u7814\u7a76\u3002"}}
{"id": "2512.14893", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.14893", "abs": "https://arxiv.org/abs/2512.14893", "authors": ["Reza Mohammadkhani", "Azad Azizzadeh", "Seyed Vahab Al-Din Makki", "John Thompson", "Maziar Nekovee"], "title": "Compensation of Coarse Quantization Effects on Channel Estimation and BER in Massive MIMO", "comment": "12 pages, submitted to IEEE Transactions", "summary": "Low-resolution quantization is essential to reduce implementation cost and power consumption in massive multiple-input multiple-output (MIMO) systems for 5G and 6G. While most existing studies assume perfect channel state information (CSI), we model the impact of coarse quantization noise on both channel estimation and data transmission, yielding a more realistic assessment of system performance under imperfect CSI conditions in the uplink. We develop a tight approximation for the bit-error ratio (BER) of uncoded M-QAM with zero-forcing detection, based on the linear minimum mean-square error (LMMSE) channel estimate. These analytical results enable compensation strategies that jointly optimize quantization resolution, transmit power, and pilot length across different numbers of users and base station antennas. We further demonstrate the applicability of the proposed framework through several design scenarios that highlight its effectiveness in optimizing system parameters and improving energy efficiency under quantization constraints. For example, in a 16-QAM system, extending the pilot sequence by 2.5 times and lowering transmit power by 0.5 dB enables a 3-bit quantized system to match the BER of the full-resolution case. The proposed framework offers a fast and accurate alternative to Monte Carlo simulations, enabling practical system optimization under realistic quantization constraints.", "AI": {"tldr": "\u5728\u4e0d\u5b8c\u7f8eCSI\u4e0b\uff0c\u7ed9\u51fa\u5e26ZF\u68c0\u6d4b\u7684uncoded M-QAM\u7684BER\u7d27\u5bc6\u8fd1\u4f3c\uff0c\u5e76\u63d0\u51fa\u8054\u5408\u4f18\u5316\u91cf\u5316\u5206\u8fa8\u7387\u3001\u53d1\u5c04\u529f\u7387\u4e0e\u5bfc\u9891\u957f\u5ea6\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u91cf\u5316\u4e0b\u7684\u5927\u89c4\u6a21MIMO\u80fd\u6548\u3002", "motivation": "\u964d\u4f4e\u5927\u89c4\u6a21MIMO\u7684\u5b9e\u73b0\u6210\u672c\u4e0e\u529f\u8017\uff0c\u8003\u8651\u7c97\u91cf\u5316\u566a\u58f0\u5bf9\u4fe1\u9053\u4f30\u8ba1\u4e0e\u6570\u636e\u4f20\u8f93\u7684\u5f71\u54cd\uff0c\u7ed9\u51fa\u5728\u73b0\u5b9e\u91cf\u5316\u7ea6\u675f\u4e0b\u7684\u7cfb\u7edf\u6027\u80fd\u8bc4\u4f30\u4e0e\u4f18\u5316\u624b\u6bb5\u3002", "method": "\u57fa\u4e8eLMMSE\u4fe1\u9053\u4f30\u8ba1\uff0c\u63a8\u5bfc uncoded M-QAM+ZF \u7684BER\u8fd1\u4f3c\uff0c\u5e76\u5c06\u91cf\u5316\u566a\u58f0\u7eb3\u5165\u5206\u6790\uff1b\u6784\u5efa\u4e00\u4e2a\u5feb\u901f\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5728\u4e0d\u540c\u7528\u6237\u6570\u548c\u57fa\u7ad9\u5929\u7ebf\u4e0b\uff0c\u8054\u5408\u4f18\u5316\u91cf\u5316\u5206\u8fa8\u7387\u3001\u53d1\u5c04\u529f\u7387\u4e0e\u5bfc\u9891\u957f\u5ea6\u3002", "result": "\u7ed9\u51faBER\u7684\u7d27\u5bc6\u8fd1\u4f3c\uff0c\u53ef\u4f5c\u4e3a\u66ff\u4ee3Monte Carlo\u4eff\u771f\u7684\u8bbe\u8ba1\u5de5\u5177\uff1b\u572816-QAM\u793a\u4f8b\u4e2d\uff0c\u901a\u8fc7\u5bfc\u9891\u5ef6\u957f2.5\u500d\u3001\u53d1\u5c04\u529f\u7387\u964d\u4f4e0.5dB\uff0c\u53ef\u4ee5\u4f7f3\u6bd4\u7279\u91cf\u5316\u7cfb\u7edf\u8fbe\u5230\u5168\u5206\u8fa8\u7387\u7684BER\uff1b\u663e\u793a\u5728\u4e0d\u540c\u53c2\u6570\u8bbe\u5b9a\u4e0b\u7684\u80fd\u6548\u63d0\u5347\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u5feb\u901f\u4e14\u51c6\u786e\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u5728\u91cf\u5316\u7ea6\u675f\u4e0b\u7684\u4e0a\u884c\u94fe\u8def\u7cfb\u7edf\u53c2\u6570\u4f18\u5316\u6210\u4e3a\u53ef\u884c\uff0c\u5e76\u6709\u52a9\u4e8e\u6bd4\u8f83\u4e0d\u540c\u91cf\u5316\u7b56\u7565\u5bf9\u80fd\u6548\u7684\u5f71\u54cd\u3002"}}
{"id": "2512.14741", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14741", "abs": "https://arxiv.org/abs/2512.14741", "authors": ["Jing Cui", "Yufei Han", "Jianbin Jiao", "Junge Zhang"], "title": "Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs", "comment": null, "summary": "Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), enabling adversaries to trigger harmful outputs or bypass safety controls. However, the persistence of the implanted backdoors under user-driven post-deployment continual fine-tuning has been rarely examined. Most prior works evaluate the effectiveness and generalization of implanted backdoors only at releasing and empirical evidence shows that naively injected backdoor persistence degrades after updates. In this work, we study whether and how implanted backdoors persist through a multi-stage post-deployment fine-tuning. We propose P-Trojan, a trigger-based attack algorithm that explicitly optimizes for backdoor persistence across repeated updates. By aligning poisoned gradients with those of clean tasks on token embeddings, the implanted backdoor mapping is less likely to be suppressed or forgotten during subsequent updates. Theoretical analysis shows the feasibility of such persistent backdoor attacks after continual fine-tuning. And experiments conducted on the Qwen2.5 and LLaMA3 families of LLMs, as well as diverse task sequences, demonstrate that P-Trojan achieves over 99% persistence while preserving clean-task accuracy. Our findings highlight the need for persistence-aware evaluation and stronger defenses in realistic model adaptation pipelines.", "AI": {"tldr": "P-Trojan \u5728\u591a\u9636\u6bb5\u6301\u7eed\u5fae\u8c03\u4e2d\u5b9e\u73b0\u9ad8\u6301\u4e45\u6027\u540e\u95e8\uff0c\u8d85\u8fc799% \u7684\u6301\u4e45\u6027\u4e14\u4fdd\u6301\u6e05\u6d01\u4efb\u52a1\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u8003\u5bdf\u540e\u95e8\u5728\u7528\u6237\u9a71\u52a8\u7684\u6301\u7eed\u5fae\u8c03\u4e0b\u7684\u6301\u4e45\u6027\uff0c\u4e14\u6734\u7d20\u6ce8\u5165\u7684\u540e\u95e8\u5728\u66f4\u65b0\u540e\u5f80\u5f80\u8870\u51cf\u3002\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u8bc4\u4f30\u5e76\u5f3a\u5316\u6301\u7eed\u5fae\u8c03\u4e2d\u7684\u540e\u95e8\u6301\u4e45\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89e6\u53d1\u7684\u653b\u51fb\u7b97\u6cd5 P-Trojan\uff0c\u901a\u8fc7\u5bf9\u5d4c\u5165\u5c42\u7684\u6c61\u67d3\u68af\u5ea6\u4e0e\u6e05\u6d01\u4efb\u52a1\u68af\u5ea6\u5bf9\u9f50\uff0c\u4f7f\u540e\u95e8\u6620\u5c04\u5728\u540e\u7eed\u66f4\u65b0\u4e2d\u4e0d\u6613\u88ab\u6291\u5236\u6216\u9057\u5fd8\uff1b\u63d0\u4f9b\u7406\u8bba\u5206\u6790\uff0c\u5e76\u5728 Qwen2.5 \u4e0e LLaMA3 \u7b49\u6a21\u578b\u53ca\u591a\u4efb\u52a1\u5e8f\u5217\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u591a\u4efb\u52a1\u5e8f\u5217\u8bbe\u5b9a\u4e0b\uff0cP-Trojan \u5b9e\u73b0\u8d85\u8fc7 99% \u7684\u540e\u95e8\u6301\u4e45\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6e05\u6d01\u4efb\u52a1\u7684\u51c6\u786e\u5ea6\u3002", "conclusion": "\u9700\u5728\u73b0\u5b9e\u6a21\u578b\u81ea\u9002\u5e94\u6d41\u6c34\u7ebf\u4e2d\u8fdb\u884c\u6301\u4e45\u6027\u8bc4\u4f30\u5e76\u5f3a\u5316\u9632\u5fa1\uff0c\u4ee5\u5e94\u5bf9\u6301\u7eed\u5fae\u8c03\u73af\u5883\u4e0b\u7684\u540e\u95e8\u653b\u51fb\u3002"}}
{"id": "2512.14710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14710", "abs": "https://arxiv.org/abs/2512.14710", "authors": ["Keqiuyin Li", "Jie Lu", "Hua Zuo", "Guangquan Zhang"], "title": "Autonomous Source Knowledge Selection in Multi-Domain Adaptation", "comment": null, "summary": "Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \\underline{\\textit{Auto}}nomous Source Knowledge \\underline{\\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.", "AI": {"tldr": "AutoS automates source sample and model selection for unsupervised multi-domain adaptation using density-driven strategy and pseudo-label enhancement to improve target task performance.", "motivation": "In massive-source domain settings, many source domains contain redundant or unrelated information that can harm transfer performance, necessitating identification and selection of transferable knowledge.", "method": "Density-driven selection to choose source samples during training and to decide which source models should contribute to target prediction; a pseudo-label enhancement module built on a pre-trained multimodal model to mitigate target label noise and strengthen self-supervision.", "result": "Experiments on real-world datasets demonstrate the superiority of AutoS over baselines.", "conclusion": "Autonomous source knowledge selection improves transfer learning efficiency and effectiveness in unsupervised multi-domain adaptation."}}
{"id": "2512.15045", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15045", "abs": "https://arxiv.org/abs/2512.15045", "authors": ["Aparna Parameswaran", "Hoyoung Kim", "Sangkil Kim"], "title": "Janus Metasurface Breaking Polarization Symmetry: Surface-Modulated Electromagnetic Wave Radiation with Coexistent Linear and Circular Polarization", "comment": null, "summary": "In this work, a Janus metasurface based tensor impedance holographic antenna (JHA) is proposed that simultaneously radiates linearly polarized (LP) and circularly polarized (CP) beams from a single aperture excited by a single feed. The proposed design introduces modified tensor impedance equations to significantly reduce cross-polarization at higher radiation angles. It demonstrates broadband operation bandwidth of 0.5 GHz while maintaining high circular polarization purity. The design methodology is verified using aperture field integration theory, ensuring that the impedance distribution produces the desired far-field radiation patterns. Prototypes of three variations of the holographic antenna are fabricated, validating its performance. The radiation characteristics of the proposed antenna make it an attractive choice for advanced broadband communication applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd Janus \u76f8\u4f4d\u9762\u5f20\u91cf\u963b\u6297\u5168\u606f\u5929\u7ebf\uff08JHA\uff09\uff0c\u5728\u5355\u4e00\u9988\u6e90/\u5355\u5f00\u53e3\u4e0b\u540c\u65f6\u8f90\u5c04\u7ebf\u6027\u504f\u632f\u548c\u5706\u504f\u632f\u675f\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u5f20\u91cf\u963b\u6297\u65b9\u7a0b\u663e\u8457\u964d\u4f4e\u9ad8\u8f90\u5c04\u89d2\u5ea6\u7684\u4ea4\u53c9\u6781\u5316\uff0c\u5b9e\u73b00.5 GHz \u5e26\u5bbd\u7684\u5bbd\u5e26\u5de5\u4f5c\u5e76\u4fdd\u6301\u9ad8\u5706\u504f\u632f\u7eaf\u5ea6\uff0c\u7406\u8bba\u4e0e aperture \u573a\u79ef\u5206\u9a8c\u8bc1\uff0c\u4e14\u6709\u4e09\u79cd\u53d8\u4f53\u539f\u578b\u9a8c\u8bc1\u3002", "motivation": "\u9700\u8981\u5728\u5355\u9988\u3001\u5355\u5f00\u53e3\u6761\u4ef6\u4e0b\u5b9e\u73b0\u591a\u6781\u5316\uff08LP \u4e0e CP\uff09\u8f90\u5c04\uff0c\u540c\u65f6\u6291\u5236\u9ad8\u89d2\u5ea6\u5e72\u6270\u7684\u4ea4\u53c9\u6781\u5316\uff0c\u964d\u4f4e\u7cfb\u7edf\u590d\u6742\u6027\u5e76\u63d0\u5347\u5e26\u5bbd\uff0c\u9002\u7528\u4e8e\u9762\u5411\u5bbd\u5e26\u901a\u4fe1\u7684\u9ad8\u6027\u80fd\u5929\u7ebf\u3002", "method": "\u63d0\u51fa Janus \u6307\u5411\u7684\u5f20\u91cf\u963b\u6297\u5168\u606f\u8bbe\u8ba1\uff08JHA\uff09\uff0c\u901a\u8fc7\u4fee\u6539\u5f20\u91cf\u963b\u6297\u65b9\u7a0b\u5b9e\u73b0\u5bf9 LP/CP \u8f90\u5c04\u7684\u63a7\u5236\uff1b\u5229\u7528 aperture field integration \u7406\u8bba\u6765\u63a8\u5bfc\u548c\u9a8c\u8bc1\u963b\u6297\u5206\u5e03\u5bf9\u8fdc\u573a\u7684\u5f71\u54cd\uff1b\u8bbe\u8ba1\u5e76\u5236\u9020\u4e09\u79cd\u53d8\u4f53\u539f\u578b\u5e76\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u73b0\u53cc\u6781\u5316\u8f90\u5c04\uff08LP \u4e0e CP\uff09\u4ece\u5355\u53e3\u5f84\u5929\u7ebf\uff0c\u663e\u8457\u964d\u4f4e\u9ad8\u89d2\u5ea6\u7684\u4ea4\u53c9\u6781\u5316\uff1b\u5b9e\u73b0\u7ea60.5 GHz \u7684\u5e26\u5bbd\u5e76\u4fdd\u6301\u9ad8 CP\u7eaf\u5ea6\uff1b\u7406\u8bba\u63a8\u5bfc\u4e0e\u6570\u503c-\u5b9e\u9a8c\u5bf9\u7167\u5747\u4e00\u81f4\uff0c\u539f\u578b\u9a8c\u8bc1\u4e86\u53ef\u5236\u9020\u6027\u4e0e\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684 JHA \u63d0\u4f9b\u4e00\u79cd\u5728\u5bbd\u5e26\u901a\u4fe1\u573a\u666f\u4e0b\u7684\u5355\u9988\u5355\u53e3\u3001\u591a\u6781\u5316\u9ad8\u6027\u80fd\u5929\u7ebf\u8bbe\u8ba1\u601d\u8def\uff0c\u5177\u6709\u826f\u597d\u7684\u7406\u8bba\u57fa\u7840\u3001\u5b9e\u73b0\u53ef\u884c\u6027\u53ca\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u9002\u7528\u4e8e\u672a\u6765\u4f4e\u590d\u6742\u5ea6\u7684\u5bbd\u5e26\u901a\u4fe1\u7cfb\u7edf\u3002"}}
{"id": "2512.15533", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15533", "abs": "https://arxiv.org/abs/2512.15533", "authors": ["Lorin Werthen-Brabants", "Pieter Simoens"], "title": "Ising Machines for Model Predictive Path Integral-Based Optimal Control", "comment": null, "summary": "We present a sampling-based Model Predictive Control (MPC) method that implements Model Predictive Path Integral (MPPI) as an \\emph{Ising machine}, suitable for novel forms of probabilistic computing. By expressing the control problem as a Quadratic Unconstrained Binary Optimization (QUBO) problem, we map MPC onto an energy landscape suitable for Gibbs sampling from an Ising model. This formulation enables efficient exploration of (near-)optimal control trajectories. We demonstrate that the approach achieves accurate trajectory tracking compared to a reference MPPI implementation, highlighting the potential of Ising-based MPPI for real-time control in robotics and autonomous systems.", "AI": {"tldr": "\u5c06 MPC \u8f6c\u6362\u4e3a QUBO \u5e76\u901a\u8fc7 Ising \u673a\u5668\u5b9e\u73b0\uff0c\u5229\u7528 Gibbs \u91c7\u6837\u4ece\u80fd\u91cf\u666f\u89c2\u4e2d\u8fdb\u884c\u8fd1\u4f3c\u6700\u4f18\u63a7\u5236\u8f68\u8ff9\u641c\u7d22\uff0c\u63d0\u5347\u5b9e\u65f6\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u63d0\u9ad8\u5b9e\u65f6\u63a7\u5236\u4e2d\u9ad8\u7ef4\u7cfb\u7edf\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u5c06 MPPI \u878d\u5165 Ising/\u6982\u7387\u8ba1\u7b97\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8f68\u8ff9\u91c7\u6837\u4e0e\u4f18\u5316\u3002", "method": "\u5c06\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u95ee\u9898\u8868\u8fbe\u4e3a\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\uff08QUBO\uff09\uff0c\u6620\u5c04\u5230 Ising \u6a21\u578b\u7684\u80fd\u91cf\u51fd\u6570\uff0c\u5229\u7528 Gibbs \u91c7\u6837\u5728 Ising \u4f53\u7cfb\u4e2d\u63a2\u7d22\u8f68\u8ff9\uff1b\u4ee5 MPPI \u4e3a\u6846\u67b6\uff0c\u8fdb\u884c\u968f\u673a\u91c7\u6837\u3001\u8f68\u8ff9\u8bc4\u4f30\u4e0e\u63a7\u5236\u51b3\u7b56\u3002", "result": "\u4e0e\u53c2\u8003\u7684 MPPI \u5b9e\u73b0\u76f8\u6bd4\uff0c\u8f68\u8ff9\u8ddf\u8e2a\u66f4\u51c6\u786e\uff0c\u8bc1\u660e\u4e86 Ising \u57fa\u7840\u7684 MPPI \u5728\u5b9e\u65f6\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u5c06 MPPI \u4e0e Ising \u673a\u5668\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e00\u79cd\u9002\u7528\u4e8e\u65b0\u578b\u6982\u7387\u8ba1\u7b97\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u80fd\u91cf\u666f\u89c2\u4e2d\u7684\u91c7\u6837\u5b9e\u73b0\u9ad8\u6548\u641c\u7d22\u8fd1\u4f3c\u6700\u4f18\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236\u3002"}}
{"id": "2512.15092", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.15092", "abs": "https://arxiv.org/abs/2512.15092", "authors": ["Chao Zhou", "Changsheng You", "Cong Zhou", "Liujia Yao", "Weijie Yuan", "Beixiong Zheng", "Nan Wu"], "title": "Rotatable IRS-Assisted 6DMA Communications: A Two-timescale Design", "comment": "13 pages, 8 figures, submitted to IEEE for possible publication", "summary": "Intelligent reflecting surface (IRS) and movable antenna (MA) are promising technologies to enhance wireless communication by reconfiguring channels at the environment and transceiver sides. However, their performance is constrained by practical limitations. To address this, we propose a multi-functional antenna/surface system that leverages their complementary advantages. A rotatable IRS (R-IRS) is deployed to enhance downlink communications from a six-dimensional MA (6DMA)-equipped base station (BS) to multiple single-antenna users. To reduce the complexity of real-time channel estimation and beamforming, we formulate an optimization problem to maximize the average sum-rate using a two-timescale (TTS) transmission protocol. Specifically, the BS antenna configuration (including position and rotation) and IRS rotation and reflection are optimized based on statistical channel state information (S-CSI), while BS transmit beamforming is designed using instantaneous CSI (I-CSI) in the short timescale. We first consider a single-user case and show that the 6DMA at the BS should form a sparse array for multi-beam transmission towards both the IRS and the user, allowing efficient coordination of direct and reflected channels, while the IRS rotation achieves effective multi-path alignment. For the general multi-user case, the optimization problem is non-convex and challenging to solve. To tackle this, we propose an efficient algorithm combining weighted minimum mean-square error (WMMSE) and stochastic successive convex approximation (SSCA) techniques. A low-complexity algorithm is also proposed to reduce computational complexity. Numerical results validate the proposed system, showing significant performance gains by jointly exploiting the spatial degrees of freedom of the 6DMA-BS and R-IRS under the TTS protocol.", "AI": {"tldr": "\u63d0\u51fa\u5728\u516d\u7ef4\u79fb\u52a8\u5929\u7ebf\uff086DMA\uff09\u57fa\u7ad9\u548c\u53ef\u65cb\u8f6cIRS\uff08R-IRS\uff09\u7684\u591a\u529f\u80fd\u5929\u7ebf/\u8868\u9762\u7cfb\u7edf\uff0c\u91c7\u7528\u4e24\u65f6\u76f8\u4f20\u8f93\u534f\u8bae\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u5728\u5355/\u591a\u7528\u6237\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u5e73\u5747\u603b\u541e\u5410\u91cf\uff08sum-rate\uff09\u5e76\u964d\u4f4e\u901a\u9053\u4f30\u8ba1\u590d\u6742\u5ea6\u3002", "motivation": "IRS\u4e0e\u79fb\u52a8\u5929\u7ebf\u5404\u81ea\u5b58\u5728\u5b9e\u9645\u9650\u5236\uff0c\u96be\u4ee5\u5355\u72ec\u5b9e\u73b0\u7406\u60f3\u6027\u80fd\u3002\u901a\u8fc7\u7ed3\u54086DMA\u7684\u7075\u6d3b\u6ce2\u675f\u4e0eR-IRS\u5bf9\u73af\u5883\u7684\u53ef\u63a7\u53cd\u5c04\uff0c\u53ef\u5728\u7edf\u8ba1CSI\u548c\u77ac\u65f6CSI\u5206\u79bb\u7684\u4e24\u65f6\u5c3a\u5ea6\u4e0b\u63d0\u5347\u94fe\u8def\u8d28\u91cf\u4e0e\u8986\u76d6\u8303\u56f4\uff0c\u964d\u4f4e\u5b9e\u65f6\u901a\u9053\u4f30\u8ba1\u4e0e\u6ce2\u675f\u8d4b\u5f62\u7684\u5f00\u9500\u3002", "method": "\u5efa\u7acb\u4e00\u4e2a\u4e24\u65f6\u5c3a\u5ea6\uff08TTS\uff09\u4f18\u5316\u6846\u67b6\uff1a\u5916\u5c42\u57fa\u4e8e\u7edf\u8ba1CSI\uff08S-CSI\uff09\u5bf9\u57fa\u7ad9\u5929\u7ebf\u914d\u7f6e\uff08\u4f4d\u7f6e/\u65cb\u8f6c\uff09\u4e0eIRS\u7684\u65cb\u8f6c\u4e0e\u53cd\u5c04\u8fdb\u884c\u4f18\u5316\uff0c\u5185\u5c42\u57fa\u4e8e\u77ac\u65f6CSI\uff08I-CSI\uff09\u8fdb\u884cBS\u53d1\u5c04\u6ce2\u675f\u8d4b\u5f62\u3002\u9996\u5148\u5728\u5355\u7528\u6237\u60c5\u5f62\u5206\u67906DMA\u5e94\u5f62\u6210\u7a00\u758f\u9635\u5217\u4ee5\u5b9e\u73b0\u5bf9IRS\u548c\u7528\u6237\u7684\u591a\u6ce2\u675f\u4f20\u8f93\u5e76\u5b9e\u73b0\u76f4\u63a5/\u53cd\u5c04\u4fe1\u9053\u7684\u9ad8\u6548\u805a\u7126\uff1b\u9488\u5bf9\u591a\u7528\u6237\u7684\u975e\u51f8\u4f18\u5316\uff0c\u63d0\u51fa\u7ed3\u5408\u52a0\u6743MMSE\uff08WMMSE\uff09\u4e0e\u968f\u673a\u4e00\u81f4(convex)\u8fd1\u4f3c\uff08SSCA\uff09\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u5e76\u7ed9\u51fa\u4f4e\u590d\u6742\u5ea6\u7248\u672c\u3002", "result": "\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u5728TTS\u534f\u8bae\u4e0b\u901a\u8fc7\u8054\u5408\u5229\u75286DMA-BS\u4e0eR-IRS\u7684\u7a7a\u95f4\u81ea\u7531\u5ea6\uff0c\u53ef\u4ee5\u83b7\u5f97\u663e\u8457\u7684\u541e\u5410\u91cf\u589e\u76ca\u3002", "conclusion": "\u901a\u8fc7\u5728\u7a7a\u95f4\u57df\u5bf96DMA-BS\u4e0eR-IRS\u8fdb\u884c\u534f\u540c\u4f18\u5316\uff0c\u5e76\u5229\u7528\u4e24\u65f6\u5c3a\u5ea6\u534f\u540c\u4f30\u8ba1\uff0c\u672c\u6587\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4e0b\u884c\u6027\u80fd\u4e0e\u66f4\u4f18\u7684\u8d44\u6e90\u5206\u914d\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5b9e\u65f6\u901a\u9053\u4f30\u8ba1\u96be\u5ea6\uff0c\u9002\u5408\u672a\u6765\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u80fd\u7684\u65e0\u7ebf\u7cfb\u7edf\u3002"}}
{"id": "2512.14712", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.14712", "abs": "https://arxiv.org/abs/2512.14712", "authors": ["Ryan Cartularo"], "title": "SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI", "comment": "7 Pages, 4 Tables, 9 Figures", "summary": "Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from \"attention starvation\" in the small antibiotic cohort ($N \\approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a \"leaner\" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the \"Historian\" (Static), the \"Monitor\" (Temporal), and the \"Reader\" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info", "AI": {"tldr": "\u5bf9\u8113\u6bd2\u75c7\u591a\u6a21\u6001\u9884\u6d4b\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u878d\u5408\u4e0e\u4e0a\u4e0b\u6587\u611f\u77e5\u5806\u53e0\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\uff0c\u63ed\u793a\u5728\u5c0f\u6837\u672c\u4e0b\u7aef\u5230\u7aef\u6ce8\u610f\u529b\u6613\u8fc7\u62df\u5408\uff1b\u4ee5\u4e0a\u4e0b\u6587\u611f\u77e5MoE\u4e3a\u6838\u5fc3\u7684SepsisLateFusion\u5b9e\u73b0SOTA\u5e76\u53d1\u5e03SepsisSuite\u3002", "motivation": "\u8113\u6bd2\u75c7\u9884\u6d4b\u9700\u6574\u5408\u751f\u7406\u6570\u636e\u3001\u6587\u672c\u4fe1\u606f\u548c\u5f71\u50cf\u7b49\u5f02\u8d28\u6570\u636e\uff1b\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5728\u6a21\u6001\u95f4\u878d\u5408\u4e0a\u8981\u4e48\u5355\u4e00\u6a21\u6001\uff0c\u8981\u4e48\u4f7f\u7528\u8106\u5f31\u7684\u65e9\u671f\u878d\u5408\uff0c\u7f3a\u4e4f\u5bf9\u8de8\u6a21\u6001\u4ea4\u4e92\u7684\u9c81\u68d2\u6027\u7814\u7a76\u3002\u56e0\u6b64\u6709\u5fc5\u8981\u6bd4\u8f83\u4e0d\u540c\u878d\u5408\u67b6\u6784\uff0c\u5e76\u63a2\u7d22\u5728\u73b0\u5b9e\u6570\u636e\u89c4\u6a21\u4e0b\u7684\u53ef\u6269\u5c55\u3001\u53ef\u90e8\u7f72\u89e3\u3002", "method": "\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e24\u7c7b\u67b6\u6784\uff1a1) End-to-End Deep Fusion\uff08SepsisFusionFormer\uff09\u2014\u2014Quad-Modal Hierarchical Gated Attention Network\uff0c\u5904\u7406\u751f\u547d\u4f53\u5f81\u3001\u6587\u672c\u4e0e\u5f71\u50cf\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\uff1b2) Context-Aware Stacking\uff08SepsisLateFusion\uff09\u2014\u2014\u5c06\u6a21\u6001\u89c6\u4e3a\u4e09\u4f4d\u4e13\u5bb6\uff08Historian/Static\u3001Monitor/Temporal\u3001Reader/NLP\uff09\uff0c\u901a\u8fc7CatBoost\u5143\u5b66\u4e60\u5668\u8fdb\u884c\u52a8\u6001\u95e8\u63a7\u7684\u4e0a\u4e0b\u6587\u611f\u77e5Mixture-of-Experts\uff1b\u5e76\u57284\u5c0f\u65f6\u9884\u6d4b\u4efb\u52a1\u4e0e\u591a\u7c7b\u6297\u751f\u7d20\u9009\u62e9\u4efb\u52a1\u4e2d\u8bc4\u4f30\u3002", "result": "SepsisFusionFormer\u5728\u5c0f\u6837\u672c\u6297\u751f\u7d20\u5b50\u96c6\u51fa\u73b0attention starvation\uff0c\u8fc7\u62df\u5408\uff0cAUC\u7ea60.66\uff1bSepsisLateFusion\u8fbe\u52300.915\u7684AUC\uff0c\u63d0\u524d4\u5c0f\u65f6\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u9608\u503c\u6821\u51c6\u5c06\u6f0f\u8b66\u7387\u964d\u4f4e\u7ea648%\uff1b\u6297\u751f\u7d20\u591a\u7c7b\u9009\u62e9\u4efb\u52a1\u7684Quad-Modal Ensemble\u8fbe\u52300.72 AUC\uff1b\u6240\u6709\u6a21\u578b\u96c6\u6210\u5230SepsisSuite\uff0c\u63d0\u4f9b\u514d\u8d39\u90e8\u7f72\u6846\u67b6\u3002", "conclusion": "\u5728\u5c0f\u6837\u672c\u3001\u591a\u6a21\u6001\u73af\u5883\u4e0b\uff0c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u7684MoE\u95e8\u63a7\u67b6\u6784\u5bf9\u8113\u6bd2\u75c7\u9884\u6d4b\u66f4\u9c81\u68d2\u4e14\u5177\u53ef\u90e8\u7f72\u6027\uff1b\u7aef\u5230\u7aef\u6ce8\u610f\u529b\u9a71\u52a8\u7684\u6df1\u5ea6\u878d\u5408\u6613\u53d7\u6837\u672c\u89c4\u6a21\u9650\u5236\u5f71\u54cd\u3002\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5c06\u590d\u6742\u6a21\u6001\u4ea4\u4e92\u4ee5\u6a21\u5757\u5316\u4e13\u5bb6\u548c\u5143\u5b66\u4e60\u5668\u7684\u65b9\u5f0f\u8fdb\u884c\u7ec4\u5408\uff0c\u5e76\u4ee5SepsisSuite\u5b9e\u73b0\u843d\u5730\u5e94\u7528\u3002"}}
{"id": "2512.15062", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.15062", "abs": "https://arxiv.org/abs/2512.15062", "authors": ["Nadia Abdolkhani", "Nada Abdel Khalek", "Walaa Hamouda", "Iyad Dayoub"], "title": "Deep Reinforcement Learning for Joint Time and Power Management in SWIPT-EH CIoT", "comment": "Published in IEEE Communications Letters, 2025. This arXiv version is the authors' accepted manuscript", "summary": "This letter presents a novel deep reinforcement learning (DRL) approach for joint time allocation and power control in a cognitive Internet of Things (CIoT) system with simultaneous wireless information and power transfer (SWIPT). The CIoT transmitter autonomously manages energy harvesting (EH) and transmissions using a learnable time switching factor while optimizing power to enhance throughput and lifetime. The joint optimization is modeled as a Markov decision process under small-scale fading, realistic EH, and interference constraints. We develop a double deep Q-network (DDQN) enhanced with an upper confidence bound. Simulations benchmark our approach, showing superior performance over existing DRL methods.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u8ba4\u77e5\u7269\u8054\u7f51SWIPT\u7cfb\u7edf\u4e2d\u8054\u5408\u65f6\u5206\u914d\u548c\u529f\u7387\u63a7\u5236\u7684\u53cc\u91cdDDQN\u65b9\u6cd5\u53ca\u5176\u6027\u80fd\u63d0\u5347", "motivation": "\u5728\u8ba4\u77e5\u7269\u8054\u7f51\u73af\u5883\u4e2d\uff0c\u9700\u8981\u5728\u80fd\u91cf\u83b7\u53d6\u4e0e\u4fe1\u606f\u4f20\u8f93\u4e4b\u95f4\u5b9e\u73b0\u9ad8\u6548\u6743\u8861\uff0c\u540c\u65f6\u9700\u5e94\u5bf9\u5c0f\u5c3a\u5ea6\u8870\u843d\u3001\u73b0\u5b9e\u80fd\u91cf\u6536\u96c6\u6a21\u578b\u4ee5\u53ca\u5e72\u6270\u7ea6\u675f\uff0c\u63d0\u5347\u541e\u5410\u548c\u7cfb\u7edf\u5bff\u547d\u662f\u5173\u952e\u6311\u6218", "method": "\u5c06\u8054\u5408\u65f6\u95f4\u5206\u914d\u4e0e\u529f\u7387\u63a7\u5236\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff1b\u63d0\u51fa\u5e26\u4e0a\u754c\u7f6e\u4fe1\u8fb9\u754c\u7684\u53ccDQN\uff08DDQN-UB\uff09\u4ee5\u6539\u8fdb\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\uff1b\u5728SWIPT\u80cc\u666f\u4e0b\u5b66\u4e60\u65f6\u95f4\u5207\u6362\u56e0\u5b50\u3001\u80fd\u91cf\u7ba1\u7406\u548c\u4f20\u8f93\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u5728\u5c0f\u5c3a\u5ea6\u8870\u843d\u4e0e\u5e72\u6270\u7ea6\u675f\u4e0b\u7684\u6027\u80fd", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709DRL\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u9ad8\u541e\u5410\u91cf\u4e0e\u66f4\u957f\u7cfb\u7edf\u5bff\u547d\u7684\u6f5c\u529b", "conclusion": "\u5c06\u4e0a\u7f6e\u4fe1\u8fb9\u754c\u7ed3\u5408\u8fdbDDQN\u4ee5\u5e94\u5bf9CIoT-SWIPT\u4e2d\u7684\u8054\u5408\u65f6\u5206\u914d\u4e0e\u529f\u7387\u63a7\u5236\uff0c\u662f\u4e00\u4e2a\u6709\u6548\u7684\u7814\u7a76\u65b9\u5411\uff0c\u80fd\u663e\u8457\u63d0\u5347\u80fd\u91cf\u6548\u7387\u4e0e\u7cfb\u7edf\u7a33\u5b9a\u6027"}}
{"id": "2512.15583", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15583", "abs": "https://arxiv.org/abs/2512.15583", "authors": ["Sabri El Amrani", "Thibaut Horel", "Saurabh Vaishampayan", "Maryam Kamgarpour", "Munther A. Dahleh"], "title": "Scheduling the Charge of Temporally Flexible Electric Vehicles: a Market-based Approach", "comment": null, "summary": "The increasing electrification of human activities and the rapid integration of variable renewable energy sources strain the power grid. A solution to address the need for more grid storage is to use the battery of electric vehicles as a back-up capacity. However, drivers tend to disconnect their electric vehicle when its battery is needed the most. We propose a charge scheduler that incentivizes drivers to delay their disconnection to improve vehicle-to-grid services. We also leverage drivers' temporal flexibility to alleviate congestion in oversubscribed charging stations. We formulate the computation of an optimal flexible schedule as a mixed-integer quadratic problem. We tractably approximate its solution using the Alternating Direction Method of Multipliers. Considering the possibility that strategic drivers misreport their charging preferences to the station coordinator, we then propose a Vickrey-Clarke-Groves mechanism that incentivizes truthful reporting. We conclude with a simulated case study using real-world data to quantitatively assess the added value of drivers' temporal flexibility for enhancing vehicle-to-grid services and reducing station congestion.", "AI": {"tldr": "\u63d0\u51fa\u4ee5\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u4f5c\u4e3a\u50a8\u80fd\u624b\u6bb5\uff0c\u901a\u8fc7\u5145\u7535\u8c03\u5ea6\u548c\u6fc0\u52b1\u673a\u5236\u63d0\u5347\u8f66\u5bf9\u7535\u7f51\u670d\u52a1\uff0c\u5e76\u7f13\u89e3oversubscribed\u5145\u7535\u7ad9\u62e5\u5835\uff1b\u901a\u8fc7\u6df7\u5408\u6574\u6570\u4e8c\u6b21\u89c4\u5212\u6c42\u89e3\uff0c\u91c7\u7528ADMM\u8fd1\u4f3c\uff0c\u5e76\u7528Vickrey\u2013Clarke\u2013Groves\u673a\u5236\u786e\u4fdd truthful reporting\uff0c\u5229\u7528\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u4eff\u771f\u5b9e\u9a8c\u8bc4\u4f30\u6536\u76ca\u3002", "motivation": "\u968f\u7740\u4eba\u7c7b\u6d3b\u52a8\u7684\u7535\u6c14\u5316\u52a0\u901f\u548c\u53ef\u518d\u751f\u80fd\u6e90\u6ce2\u52a8\u6027\u589e\u52a0\uff0c\u7535\u7f51\u5bf9\u50a8\u80fd\u9700\u6c42\u4e0a\u5347\uff1b\u7535\u52a8\u6c7d\u8f66\u5177\u5907\u5145\u7535\u5f39\u6027\uff0c\u53ef\u4f5c\u4e3a\u540e\u5907\u5bb9\u91cf\u4e0e\u9700\u6c42\u54cd\u5e94\u6765\u6e90\u3002\u7136\u800c\uff0c\u53f8\u673a\u5728\u9700\u8981\u5176\u7535\u6c60\u65f6\u53ef\u80fd\u4e2d\u65ad\u653e\u7535/\u5145\u7535\uff0c\u4e14\u5145\u7535\u7ad9\u6613\u62e5\u5835\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u9f13\u52b1\u53f8\u673a\u5ef6\u8fdf\u65ad\u5f00\u7684\u673a\u5236\u5e76\u7f13\u89e3\u62e5\u5835\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u540c\u65f6\u786e\u4fdd\u4fe1\u606f\u62ab\u9732\u7684\u771f\u5b9e\u6027\u3002", "method": "\u5c06\u6700\u4f73\u53ef\u53d8\u65f6\u95f4\u8868\u7684\u8ba1\u7b97\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u4e8c\u6b21\u89c4\u5212\uff08MIQP\uff09\uff0c\u901a\u8fc7\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u5bf9\u6c42\u89e3\u8fdb\u884c\u5206\u89e3\u6027\u8fd1\u4f3c\uff1b\u8003\u8651\u7b56\u7565\u6027\u53f8\u673a\u53ef\u80fd\u865a\u62a5\u504f\u597d\uff0c\u63d0\u51faVickrey\u2013Clarke\u2013Groves\uff08VCG\uff09\u673a\u5236\u4ee5\u6fc0\u52b1\u8bda\u5b9e\u7533\u62a5\uff1b\u901a\u8fc7\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u4eff\u771f\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u6fc0\u52b1\u5728\u63d0\u5347\u8f66\u7f51\u534f\u540c\u6548\u5e94\u65b9\u9762\u7684\u91cf\u7ea7\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u53f8\u673a\u65f6\u95f4\u5f39\u6027\u5bf9\u4e8e\u63d0\u5347\u8f66\u5bf9\u7535\u7f51\u670d\u52a1\uff08\u5982\u5cf0\u8c37\u6ce2\u52a8\u7f13\u89e3\u3001\u5bb9\u91cf\u63d0\u4f9b\uff09\u548c\u964d\u4f4e\u5145\u7535\u7ad9\u62e5\u5835\u5177\u6709\u53ef\u89c2\u7684\u4ef7\u503c\uff1b\u6240\u63d0\u51fa\u7684\u8c03\u5ea6\u4e0e\u6fc0\u52b1\u7ec4\u5408\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5b9e\u73b0\u8d44\u6e90\u7684\u6709\u6548\u5229\u7528\u4e0e\u9700\u6c42\u7ea6\u675f\u7684\u7f13\u89e3\u3002", "conclusion": " temporal flexibility\uff08\u65f6\u95f4\u5f39\u6027\uff09\u7ed3\u5408\u6fc0\u52b1\u673a\u5236\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8f66\u7f51\u534f\u540c\u6548\u5e94\uff0c\u589e\u5f3a\u5bf9\u7535\u7f51\u7684\u652f\u6491\u80fd\u529b\u5e76\u7f13\u89e3\u5145\u7535\u7ad9\u62e5\u5835\uff1b\u672a\u6765\u5de5\u4f5c\u53ef\u805a\u7126\u73b0\u5b9e\u90e8\u7f72\u3001\u9c81\u68d2\u6027\u5206\u6790\u4ee5\u53ca\u4e0d\u540c\u5145\u7535\u5e02\u573a\u7684\u5236\u5ea6\u8bbe\u8ba1\u3002"}}
{"id": "2512.14745", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14745", "abs": "https://arxiv.org/abs/2512.14745", "authors": ["Edward Lue Chee Lip", "Anthony Channg", "Diana Kim", "Aaron Sandoval", "Kevin Zhu"], "title": "Factor(U,T): Controlling Untrusted AI by Monitoring their Plans", "comment": "Accepted to AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent). 6 pages body, 8 pages total, 3 figures", "summary": "As AI capabilities advance, we increasingly rely on powerful models to decompose complex tasks $\\unicode{x2013}$ but what if the decomposer itself is malicious? Factored cognition protocols decompose complex tasks into simpler child tasks: one model creates the decomposition, while other models implement the child tasks in isolation. Prior work uses trusted (weaker but reliable) models for decomposition, which limits usefulness for tasks where decomposition itself is challenging. We introduce Factor($U$,$T$), in which an untrusted (stronger but potentially malicious) model decomposes while trusted models implement child tasks. Can monitors detect malicious activity when observing only natural language task instructions, rather than complete solutions? We baseline and red team Factor($U$,$T$) in control evaluations on BigCodeBench, a dataset of Python coding tasks. Monitors distinguishing malicious from honest decompositions perform poorly (AUROC 0.52) compared to monitors evaluating complete Python solutions (AUROC 0.96). Furthermore, Factor($D$,$U$), which uses a trusted decomposer and monitors concrete child solutions, achieves excellent discrimination (AUROC 0.96) and strong safety (1.2% ASR), demonstrating that implementation-context monitoring succeeds where decomposition-only monitoring fails.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.15191", "categories": ["cs.IT", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.15191", "abs": "https://arxiv.org/abs/2512.15191", "authors": ["Mengchu Xu", "Jian Wang", "Yonina C. Eldar"], "title": "Sparse Principal Component Analysis with Energy Profile Dependent Sample Complexity", "comment": "33 pages, 7 figures", "summary": "We study sparse principal component analysis in the high-dimensional, sample-limited regime, aiming to recover a leading component supported on a few coordinates. Despite extensive progress, most methods and analyses are tailored to the flat-spike case, offering little guidance when spike energy is unevenly distributed across the support. Motivated by this, we propose Spectral Energy Pursuit (SEP), an effective iterative scheme that repeatedly screens and reselects coordinates, with a sample complexity that adapts to the energy profile. We develop our framework around a structure function \\(s(p)\\) that quantifies how spike energy accumulates over its top \\(p\\) entries. We establish that SEP succeeds with a sample size of order \\(\\max_{1\\le p\\le k} p\\,s^2(p)\\,\\log n\\), which matches the classical \\(k^2\\log n\\) sample complexity for flat spikes and improves toward the \\(k\\log n\\) regime as the profile becomes more concentrated. As a lightweight post-processing, a single truncated power iteration is proven to enable the final estimator to attain a uniform statistical error bound. Empirical simulations across flat, power-law, and exponential signals validate that SEP adapts to profile structure without tuning and outperforms existing algorithms.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.14713", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.14713", "abs": "https://arxiv.org/abs/2512.14713", "authors": ["Georges Sfeir", "Stephane Hess", "Thomas O. Hancock", "Filipe Rodrigues", "Jamal Amani Rad", "Michiel Bliemer", "Matthew Beck", "Fayyaz Khan"], "title": "A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour", "comment": "32 pages, 8 figures, 6 tables", "summary": "Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.15105", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15105", "abs": "https://arxiv.org/abs/2512.15105", "authors": ["Jundong Qi", "Weize Sun", "Shaowu Chen", "Lei Huang", "Qiuchen Liu"], "title": "CF-Net: A Cross-Feature Reconstruction Network for High-Accuracy 1-Bit Target Classification", "comment": "14 pages, 10 figures. Submitted to IEEE Transactions on Geoscience and Remote Sensing", "summary": "Target classification is a fundamental task in radar systems, and its performance critically depends on the quantization precision of the signal. While high-precision quantization (e.g. 16-bit) is well established, 1-bit quantization offers distinct advantages by enabling direct sampling at high frequencies and eliminating complex intermediate stages. However, its extreme quantization leads to significant information loss. Although higher sampling rates can compensate for this loss, such oversampling is impractical at the high frequencies targeted for direct sampling. To achieve high-accuracy classification directly from 1-bit radar data under the same sampling rate, this paper proposes a novel two-stage deep learning framework, CF-Net. First, we introduce a self-supervised pre-training strategy based on a dual-branch U-Net architecture. This network learns to restore high-fidelity 16-bit images from their 1-bit counterparts via a cross-feature reconstruction task, forcing the 1-bit encoder to learn robust features despite extreme quantization. Subsequently, this pre-trained encoder is repurposed and fine-tuned for the downstream multi-class target classification task. Experiments on two radar target datasets demonstrate that CF-Net can effectively extract discriminative features from 1-bit imagery, achieving comparable and even superior accuracy to some 16-bit methods without oversampling.", "AI": {"tldr": "Two-stage CF-Net using self-supervised pretraining to enable high-precision 1-bit radar target classification without oversampling; demonstrates competitive performance with 16-bit methods.", "motivation": "1-bit quantization causes heavy information loss; oversampling at high frequencies is impractical; aim to achieve high-accuracy classification directly from 1-bit data.", "method": "Stage1: dual-branch U-Net self-supervised pretraining with cross-feature reconstruction to convert 1-bit inputs to high-fidelity 16-bit-like representations. Stage2: fine-tune the pre-trained encoder for multi-class target classification.", "result": "On two radar datasets, CF-Net learns robust 1-bit features and attains accuracy comparable or superior to some 16-bit methods without oversampling.", "conclusion": "CF-Net demonstrates that accurate 1-bit radar classification is achievable at the same sampling rate via self-supervised feature learning and encoder reuse, reducing the need for oversampling."}}
{"id": "2512.15481", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.15481", "abs": "https://arxiv.org/abs/2512.15481", "authors": ["Shavbo Salehi", "Pedro Enrique Iturria-Rivera", "Medhat Elsayed", "Majid Bavand", "Yigit Ozcan", "Melike Erol-Kantarci"], "title": "GenAI-enabled Residual Motion Estimation for Energy-Efficient Semantic Video Communication", "comment": null, "summary": "Semantic communication addresses the limitations of the Shannon paradigm by focusing on transmitting meaning rather than exact representations, thereby reducing unnecessary resource consumption. This is particularly beneficial for video, which dominates network traffic and demands high bandwidth and power, making semantic approaches ideal for conserving resources while maintaining quality. In this paper, we propose a Predictability-aware and Entropy-adaptive Neural Motion Estimation (PENME) method to address challenges related to high latency, high bitrate, and power consumption in video transmission. PENME makes per-frame decisions to select a residual motion extraction model, convolutional neural network, vision transformer, or optical flow, using a five-step policy based on motion strength, global motion consistency, peak sharpness, heterogeneity, and residual error. The residual motions are then transmitted to the receiver, where the frames are reconstructed via motion-compensated updates. Next, a selective diffusion-based refinement, the Latent Consistency Model (LCM-4), is applied on frames that trigger refinement due to low predictability or large residuals, while predictable frames skip refinement. PENME also allocates radio resource blocks with awareness of residual motion and channel state, reducing power consumption and bandwidth usage while maintaining high semantic similarity. Our simulation results on the Vimeo90K dataset demonstrate that the proposed PENME method handles various types of video, outperforming traditional communication, hybrid, and adaptive bitrate semantic communication techniques, achieving 40% lower latency, 90% less transmitted data, and 35% higher throughput. For semantic communication metrics, PENME improves PSNR by about 40%, increases MS-SSIM by roughly 19%, and reduces LPIPS by nearly 35%, compared with the baseline methods.", "AI": {"tldr": "\u63d0\u51faPENME\u6846\u67b6\uff1a\u5728\u8bed\u4e49\u89c6\u9891\u4f20\u8f93\u4e2d\u901a\u8fc7\u53ef\u9884\u6d4b\u6027\u4e0e\u71b5\u81ea\u9002\u5e94\u7684\u8fd0\u52a8\u4f30\u8ba1\u4e0e\u9009\u62e9\u6027\u6269\u6563\u6765\u5b9e\u73b0\u4f4e\u65f6\u5ef6\u3001\u4f4e\u6570\u636e\u91cf\u4e0e\u9ad8\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u4f20\u8f93\u3002", "motivation": "\u9999\u519c\u8303\u5f0f\u5728\u9ad8\u5e26\u5bbd\u548c\u9ad8\u529f\u8017\u573a\u666f\u4e0b\u7684\u8d44\u6e90\u6d6a\u8d39\u4e0e\u89c6\u9891\u4f20\u8f93\u7684\u9ad8\u8981\u6c42\uff0c\u4fc3\u4f7f\u7814\u7a76\u8f6c\u5411\u4ee5\u201c\u610f\u4e49\u201d\u4e3a\u4e2d\u5fc3\u7684\u8bed\u4e49\u901a\u4fe1\u4ee5\u964d\u4f4e\u8d44\u6e90\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\u3002", "method": "PENME\u5728\u9010\u5e27\u5c42\u9762\u6839\u636e\u4e94\u6b65\u7b56\u7565\uff08\u8fd0\u52a8\u5f3a\u5ea6\u3001\u5168\u5c40\u8fd0\u52a8\u4e00\u81f4\u6027\u3001\u5cf0\u503c\u6e05\u6670\u3001\u5f02\u8d28\u6027\u3001\u6b8b\u5dee\u8bef\u5dee\uff09\u5728\u6b8b\u5dee\u8fd0\u52a8\u63d0\u53d6\u6a21\u578b\u3001CNN\u3001ViT\u6216\u5149\u6d41\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\uff1b\u5c06\u6b8b\u5dee\u8fd0\u52a8\u53d1\u9001\u7ed9\u63a5\u6536\u7aef\u5e76\u901a\u8fc7\u8fd0\u52a8\u8865\u507f\u66f4\u65b0\u91cd\u5efa\uff1b\u5bf9\u4f4e\u53ef\u9884\u6d4b\u6216\u6b8b\u5dee\u5927\u7684\u5e27\u5e94\u7528Latent Consistency Model (LCM-4)\u8fdb\u884c\u9009\u62e9\u6027\u6269 refinement\uff1b\u5e76\u57fa\u4e8e\u6b8b\u5dee\u8fd0\u52a8\u548c\u4fe1\u9053\u72b6\u6001\u8fdb\u884c\u65e0\u7ebf\u8d44\u6e90\u5757\u5206\u914d\u4ee5\u964d\u4f4e\u80fd\u8017\u4e0e\u5e26\u5bbd\uff1b\u5728Vimeo90K\u6570\u636e\u96c6\u5bf9\u6bd4\u4f20\u7edf\u3001\u6df7\u5408\u548c\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u8bed\u4e49\u901a\u4fe1\uff0c\u663e\u793a\u663e\u8457\u6539\u8fdb\u3002", "result": "\u5728\u591a\u7c7b\u578b\u89c6\u9891\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u65f6\u5ef6\u964d\u4f4e\u3001\u4f20\u8f93\u6570\u636e\u91cf\u4e0b\u964d\u548c\u541e\u5410\u63d0\u5347\uff0c\u5e76\u5728\u8bed\u4e49\u6307\u6807\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\uff1a\u7ea640%\u65f6\u5ef6\u964d\u4f4e\u300190%\u6570\u636e\u91cf\u51cf\u5c11\u300135%\u541e\u5410\u63d0\u5347\uff1bPSNR\u63d0\u5347\u7ea640%\u3001MS-SSIM\u63d0\u5347\u7ea619%\u3001LPIPS\u4e0b\u964d\u7ea635%\u3002", "conclusion": "\u901a\u8fc7PENME\u5b9e\u73b0\u9ad8\u6548\u7684\u8bed\u4e49\u89c6\u9891\u4f20\u8f93\uff0c\u8bc1\u660e\u57fa\u4e8e\u53ef\u9884\u6d4b\u6027\u548c\u6b8b\u5dee\u81ea\u9002\u5e94\u7b56\u7565\u5728\u8d44\u6e90\u53d7\u9650\u548c\u9ad8\u5ef6\u8fdf\u573a\u666f\u4e0b\u80fd\u663e\u8457\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u4e0e\u8bed\u4e49\u8d28\u91cf\u3002"}}
{"id": "2512.14746", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.14746", "abs": "https://arxiv.org/abs/2512.14746", "authors": ["Jad Al Aaraj", "Athina Markopoulou"], "title": "BLINDSPOT: Enabling Bystander-Controlled Privacy Signaling for Camera-Enabled Devices", "comment": null, "summary": "Camera-equipped mobile devices, such as phones, smart glasses, and AR headsets, pose a privacy challenge for bystanders, who currently lack effective real-time mechanisms to control the capture of their picture, video, including their face. We present BlindSpot, an on-device system that enables bystanders to manage their own privacy by signaling their privacy preferences in real-time without previously sharing any sensitive information. Our main contribution is the design and comparative evaluation of three distinct signaling modalities: a hand gesture mechanism, a significantly improved visible light communication (VLC) protocol, and a novel ultra-wideband (UWB) communication protocol. For all these modalities, we also design a validation mechanism that uses geometric consistency checks to verify the origin of a signal relative to the sending bystander, and defend against impersonation attacks. We implement the complete system (BlindSpot) on a commodity smartphone and conduct a comprehensive evaluation of each modality's accuracy and latency across various distances, lighting conditions, and user movements. Our results demonstrate the feasibility of these novel bystander signaling techniques and their trade-offs in terms of system performance and convenience.", "AI": {"tldr": "BlindSpot enables bystanders to signal privacy preferences to camera devices in real-time using three on-device modalities (hand gesture, VLC, UWB), with anti-impersonation validation, implemented on smartphones.", "motivation": "Protect bystander privacy from pervasive camera capture; current bystander controls are lacking; need real-time, device-local solutions without sharing sensitive data.", "method": "Design and compare three signaling modalities; implement on smartphone; add geometric consistency-based validation to confirm signal origin and guard impersonation; evaluate accuracy and latency under varying distances, illumination, and motion.", "result": "Modalities are feasible; trade-offs observed: VLC/hand gesture vs UWB in latency, range, robustness; validation mechanism works to prevent impersonation; system demonstrated on commodity device.", "conclusion": "BlindSpot shows practical, on-device privacy signaling with multiple options; enables real-time bystander control with security considerations; scalable across environments."}}
{"id": "2512.14714", "categories": ["cs.LG", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.14714", "abs": "https://arxiv.org/abs/2512.14714", "authors": ["Lucas Cesar Ferreira Domingos", "Russell Brinkworth", "Paulo Eduardo Santos", "Karl Sammut"], "title": "Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms", "comment": null, "summary": "Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.", "AI": {"tldr": "\u63d0\u51fa GSE ResNeXt\uff1a\u5c06\u53ef\u5b66\u4e60\u7684 Gabor \u5377\u79ef\u4e0e ResNeXt+SE \u7ed3\u6784\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u6c34\u4e0b\u58f0\u5b66\u76ee\u6807\u5206\u7c7b\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\uff0c\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u5e76\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u590d\u6742\u3001\u8239\u8236\u8f90\u5c04\u566a\u58f0\u4e0e\u73af\u5883\u566a\u58f0\u53e0\u52a0\uff0c\u4e14\u516c\u5f00\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u548c\u6cdb\u5316\u6027\u80fd\u4e0d\u8db3\uff1b\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u6d4b\u5236\u7ea6\u65b9\u6cd5\u6bd4\u8f83\u4e0e\u63a8\u5e7f\u3002", "method": "\u5728 ResNeXt \u4e3b\u5e72\u4e2d\u5d4c\u5165\u53ef\u5b66\u4e60\u7684 Gabor \u5377\u79ef\u5c42\uff0c\u4f5c\u4e3a\u4e8c\u7ef4\u81ea\u9002\u5e94\u5e26\u901a\u6ee4\u6ce2\u5668\u6269\u5c55\u7279\u5f81\u901a\u9053\uff1b\u5f15\u5165 squeeze-and-excitation \u901a\u9053\u6ce8\u610f\u529b\u4ee5\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u6536\u655b\u6027\uff1b\u5728\u4e09\u4e2a\u6e10\u8fdb\u590d\u6742\u5ea6\u7684\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bc4\u4f30\u5e76\u5206\u6790\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u7684\u65f6\u95f4\u5dee\uff08\u8239\u8236\u4e0e\u4f20\u611f\u5668\u8ddd\u79bb\uff09\u7684\u5f71\u54cd\u3002", "result": "GSE ResNeXt \u5728\u6240\u6709\u4e09\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e Xception\u3001ResNet\u3001MobileNetV2 \u7b49\u57fa\u7ebf\uff1b\u521d\u5c42\u6dfb\u52a0 Gabor \u5377\u79ef\u53ef\u7ea6 28% \u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\uff1b\u7814\u7a76\u8868\u660e\u73af\u5883\u56e0\u7d20\u5bf9\u8f93\u5165\u4fe1\u53f7\u7684\u5f71\u54cd\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5f3a\u8c03\u4fe1\u53f7\u5904\u7406\u7b56\u7565\u5728\u63d0\u9ad8\u6c34\u4e0b\u58f0\u5b66\u5206\u7c7b\u7684\u53ef\u9760\u6027\u4e0e\u6cdb\u5316\u6027\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\uff1b\u672a\u6765\u5de5\u4f5c\u5e94\u81f4\u529b\u4e8e\u51cf\u7f13\u73af\u5883\u56e0\u7d20\u5bf9\u8f93\u5165\u4fe1\u53f7\u7684\u5f71\u54cd\u4ee5\u63d0\u5347\u6a21\u578b\u7a33\u5065\u6027\u3002"}}
{"id": "2512.15668", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15668", "abs": "https://arxiv.org/abs/2512.15668", "authors": ["Pablo Ot\u00e1lora", "Sigurd Skogestad", "Jos\u00e9 Luis Guzm\u00e1n", "Manuel Berenguel"], "title": "Enhancing industrial microalgae production through Economic Model Predictive Control", "comment": null, "summary": "The industrial production of microalgae is an important and sustainable process, but its actual competitiveness is closely related to its optimization. The biological nature of the process hinders this task, mainly due to the high nonlinearity of the process along with its changing nature, features that make its modeling, control and optimization remarkably challenging. This paper presents an economic optimization framework aiming to enhance the operation of such systems. An Economic Model Predictive Controller is proposed, centralizing the decision making and achieving the theoretical optimal operation. Different scenarios with changing climate conditions are presented, and a comparison with the typical, non-optimized industrial process operation is established. The obtained results achieve economic optimization and dynamic stability of the process, while providing some insight into the priorities during process operation at industrial level, and justifying the use of optimal controllers over traditional operation.", "AI": {"tldr": "\u901a\u8fc7\u7ecf\u6d4e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08EMPC\uff09\u6846\u67b6\u4f18\u5316\u5fae\u85fb\u4ea7\u4e1a\u8fc7\u7a0b\uff0c\u5728\u6c14\u5019\u53d8\u5316\u60c5\u666f\u4e0b\u5b9e\u73b0\u7ecf\u6d4e\u6700\u4f18\u4e0e\u52a8\u6001\u7a33\u5b9a\uff0c\u4f18\u4e8e\u4f20\u7edf\u8fd0\u884c\u3002", "motivation": "\u5fae\u85fb\u5de5\u4e1a\u751f\u4ea7\u9ad8\u5ea6\u975e\u7ebf\u6027\u3001\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u5efa\u6a21\u3001\u63a7\u5236\u548c\u4f18\u5316\u56f0\u96be\uff0c\u76f4\u63a5\u5f71\u54cd\u6210\u672c\u3001\u4ea7\u51fa\u548c\u7ade\u4e89\u529b\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u51b3\u7b56\u6846\u67b6\u6765\u63d0\u5347\u7ecf\u6d4e\u6027\u4e0e\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ecf\u6d4e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u96c6\u4e2d\u5316\u51b3\u7b56\uff0c\u5e76\u5728\u4e0d\u540c\u6c14\u5019\u60c5\u666f\u4e0b\u4e0e\u5e38\u89c4\u5de5\u4e1a\u8fd0\u884c\u8fdb\u884c\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u5176\u7406\u8bba\u6700\u4f18\u64cd\u4f5c\u4e0e\u52a8\u6001\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u7ecf\u6d4e\u4f18\u5316\u548c\u8fc7\u7a0b\u7684\u52a8\u6001\u7a33\u5b9a\uff0c\u63d0\u4f9b\u4e86\u5de5\u4e1a\u5c42\u9762\u7684\u4f18\u5148\u7ea7\u6d1e\u5bdf\uff0c\u652f\u6301\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u4f18\u5148\u91c7\u7528\u6700\u4f18\u63a7\u5236\u7b56\u7565\u800c\u975e\u4f20\u7edf\u8fd0\u8425\u3002", "conclusion": "\u5728\u5fae\u85fb\u5de5\u4e1a\u751f\u4ea7\u4e2d\uff0cEMPC\u53ef\u63d0\u5347\u7ecf\u6d4e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u6c14\u5019\u53d8\u5316\u6761\u4ef6\u4e0b\uff0c\u5c55\u793a\u4e86\u6bd4\u4f20\u7edf\u64cd\u4f5c\u66f4\u4f18\u7684\u63a7\u5236\u6548\u679c\u4e0e\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2512.14715", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14715", "abs": "https://arxiv.org/abs/2512.14715", "authors": ["Zafaryab Haider", "Md Hafizur Rahman", "Shane Moeykens", "Vijay Devabhaktuni", "Prabuddha Chakraborty"], "title": "How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection", "comment": null, "summary": "Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.", "AI": {"tldr": "\u63d0\u51faBLADE\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u68af\u5ea6\u654f\u611f\u6027\u4f30\u8ba1\u5b9a\u4f4d\u5bf9\u8bed\u4e49\u5f71\u54cd\u6700\u5927\u7684\u6bd4\u7279\u4f4d\u7f6e\uff0c\u5e76\u7528caption-level\u7684\u8bed\u4e49-\u6d41\u7545\u6027\u76ee\u6807\u7ec6\u5316\u9009\u62e9\uff0c\u7528\u4e8e\u7814\u7a76\u5728\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u4e2d\u6bd4\u7279\u7ffb\u8f6c\u5982\u4f55\u6539\u53d8\u8bed\u4e49\u800c\u4fdd\u6301\u8bed\u6cd5\u3002", "motivation": "\u786c\u4ef6\u4e2d\u96be\u4ee5\u68c0\u6d4b\u7684\u6bd4\u7279\u7ffb\u8f6c\u53ef\u80fd\u5bfc\u81f4\u751f\u6210\u5f0f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u5c42\u9762\u7684\u5d29\u574f\uff1b\u73b0\u6709\u6545\u969c\u5206\u6790\u591a\u805a\u7126\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u8bed\u8a00\u751f\u6210\u7684\u8bed\u4e49\u4e0e\u8bed\u8a00\u5b66\u7ef4\u5ea6\uff0c\u9700\u8981\u7814\u7a76\u610f\u4e49\u662f\u5982\u4f55\u7f16\u7801\u3001\u53ef\u64cd\u63a7\u7684\u3002", "method": "\u63d0\u51faBLADE\uff1aBit-level Fault Analysis via Differentiable Estimation\uff0c\u5229\u7528\u68af\u5ea6\u57fa\u7684\u654f\u611f\u6027\u4f30\u8ba1\u5b9a\u4f4d\u5173\u952e\u4fe1\u606f\u6bd4\u7279\uff1b\u518d\u901a\u8fc7\u4e00\u4e2a\u7528\u4e8e\u4f18\u5316\u7684caption\u7ea7\u8bed\u4e49-\u6d41\u7545\u6027\u76ee\u6807\u5bf9\u6240\u9009\u6bd4\u7279\u8fdb\u884c\u7ec6\u5316\u7b5b\u9009\u3002", "result": "\u4f5c\u4e3a\u65b9\u6cd5\u8bba\u7684\u8bba\u6587\u6458\u8981\uff0c\u7ed9\u51fa\u6982\u5ff5\u6027\u8bc1\u636e\uff1a\u4f4e\u7ea7\u6bd4\u7279\u7ffb\u8f6c\u5373\u53ef\u5f71\u54cd\u751f\u6210\u7684\u8bed\u4e49\u5e76\u4fdd\u6301\u8bed\u6cd5\uff0c\u4e0e\u4ec5\u5d29\u6e83\u6216\u964d\u4f4e\u51c6\u786e\u7387\u7684\u6545\u969c\u5206\u6790\u4e0d\u540c\uff1b\u6846\u67b6\u53ef\u7528\u4e8e\u9c81\u68d2\u6027\u6d4b\u8bd5\u3001\u5bf9\u6297\u9632\u5fa1\u4e0e\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u6bd4\u7279\u5c42\u9762\u5bf9\u751f\u6210\u6a21\u578b\u8bed\u4e49\u8f93\u51fa\u7684\u6f5c\u5728\u64cd\u63a7\u8def\u5f84\u3002", "conclusion": "BLADE\u4e3a\u7406\u89e3\u610f\u4e49\u5728\u7f16\u7801\u5c42\u9762\u7684\u5206\u5e03\u4e0e\u53ef\u53d8\u6027\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u8bc4\u4f30\u4e0e\u63d0\u5347\u751f\u6210\u6a21\u578b\u5bf9\u6bd4\u7279\u7ea7\u6545\u969c\u7684\u9c81\u68d2\u6027\u3001\u5f00\u53d1\u9632\u5fa1\u4ee5\u53ca\u63a8\u8fdb\u53ef\u89e3\u91ca\u6027AI\u3002"}}
{"id": "2512.15677", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15677", "abs": "https://arxiv.org/abs/2512.15677", "authors": ["Xiaojie Tao", "Rajit Gadh"], "title": "Service-Oriented Fast Frequency Response from Flexible Loads and Energy Storage in Low-Inertia Power Systems", "comment": null, "summary": "The increasing penetration of inverter-based renewable generation has significantly reduced system inertia, making modern power grids more vulnerable to rapid frequency deviations following disturbances. While a wide range of flexible resources-including electric vehicles (EVs), data centers, and battery energy storage systems (BESS)-have demonstrated the physical capability to provide fast frequency response (FFR), existing studies primarily focus on individual resource performance or controller-level designs. A systematic framework that translates heterogeneous FFR capabilities into deployable, system-level frequency services remains largely unexplored. This paper proposes a service-oriented coordination framework for fast frequency response from flexible loads and energy storage, bridging the gap between physical capability assessment and grid-operational utilization. The framework decomposes frequency support into multiple time-critical service layers based on response speed, power capacity, and energy sustainability, and dynamically allocates FFR responsibilities among heterogeneous resources accordingly. By explicitly accounting for response latency, saturation limits, and energy constraints, the proposed approach enables coordinated dispatch that prioritizes ultra-fast resources for initial frequency arrest while leveraging slower but energy-rich resources to sustain recovery.", "AI": {"tldr": "Proposes a service-oriented framework to coordinate fast frequency response (FFR) from flexible loads and energy storage across multiple time scales, prioritizing ultra-fast resources for initial frequency arrest and slower, energy-rich resources for sustainment.", "motivation": "Growing penetration of inverter-based generation reduces system inertia, making grids more vulnerable to rapid frequency deviations; existing work focuses on individual resources or controller-level designs, lacking a system-level deployment framework.", "method": "Decomposes FFR into layered, time-critical services based on response speed, power capacity, and energy sustainability; dynamically allocates responsibilities among heterogeneous resources; explicitly accounts for latency, saturation limits, and energy constraints to enable coordinated dispatch.", "result": "Conceptual framework proposing layered service allocation and prioritization to translate heterogeneous FFR capabilities into deployable grid services; no empirical validation reported in the abstract.", "conclusion": "The framework provides a systematic bridge between capability assessment and grid operation, enabling coordinated FFR across heterogeneous resources and potentially improving frequency stability."}}
{"id": "2512.15503", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.15503", "abs": "https://arxiv.org/abs/2512.15503", "authors": ["Konstantinos Kalogiannis", "Ahmed Mohamed Hussain", "Hexu Li", "Panos Papadimitratos"], "title": "Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection", "comment": "17 pages, 10 figures", "summary": "Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.", "AI": {"tldr": "Transformer-based AIMformer for real-time misbehavior detection in vehicular platoons; achieves high accuracy (\u22650.93) with sub-ms edge inference.", "motivation": "Traditional misbehavior detectors rely on plausibility checks and statistics, suffering high FP and failing to capture complex temporal-spatial dynamics in multi-vehicle platoons; a robust real-time solution is needed.", "method": "AIMformer employs multi-head self-attention to model intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It uses global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. Introduces a Precision-Focused BCE loss to penalize false positives. Edge-friendly deployment via TensorFlow Lite, ONNX, and TensorRT for sub-millisecond inference.", "result": "Extensive evaluation over 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios shows performance \u22650.93. Deployment analysis demonstrates sub-millisecond latency, suitable for real-time edge operation.", "conclusion": "AIMformer is viable for both in-vehicle and roadside deployment, offering improved safety-critical misbehavior detection in vehicular platoons."}}
{"id": "2512.14751", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14751", "abs": "https://arxiv.org/abs/2512.14751", "authors": ["Yixin Tan", "Zhe Yu", "Jun Sakuma"], "title": "One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs", "comment": "17 pages", "summary": "Finetuning pretrained large language models (LLMs) has become the standard paradigm for developing downstream applications. However, its security implications remain unclear, particularly regarding whether finetuned LLMs inherit jailbreak vulnerabilities from their pretrained sources. We investigate this question in a realistic pretrain-to-finetune threat model, where the attacker has white-box access to the pretrained LLM and only black-box access to its finetuned derivatives. Empirical analysis shows that adversarial prompts optimized on the pretrained model transfer most effectively to its finetuned variants, revealing inherited vulnerabilities from pretrained to finetuned LLMs. To further examine this inheritance, we conduct representation-level probing, which shows that transferable prompts are linearly separable within the pretrained hidden states, suggesting that universal transferability is encoded in pretrained representations. Building on this insight, we propose the Probe-Guided Projection (PGP) attack, which steers optimization toward transferability-relevant directions. Experiments across multiple LLM families and diverse finetuned tasks confirm PGP's strong transfer success, underscoring the security risks inherent in the pretrain-to-finetune paradigm.", "AI": {"tldr": "Finetuning pretrained LLMs preserves jailbreak vulnerabilities from the pretrained stage, with prompts optimized on the pretrained model transferring effectively to finetuned derivatives. The authors reveal this transferability via representation-level analysis and introduce a Probe-Guided Projection (PGP) attack that exploits transferability directions, showing strong cross-family transfer across tasks.", "motivation": "Understand whether finetuning introduces or preserves jailbreak vulnerabilities and quantify transferability of adversarial prompts from pretrained to finetuned LLMs in a realistic pretrain-to-finetune threat model.", "method": "Empirical evaluation with white-box access to the pretrained model and black-box access to finetuned derivatives; adversarial prompts optimized on pretrained models; representation-level probing to assess linear separability of transferable prompts; development of Probe-Guided Projection (PGP) attack to steer optimization toward transferability directions.", "result": "Adversarial prompts crafted on the pretrained model transfer most effectively to finetuned variants; transferable prompts form linear separations in pretrained hidden states; the PGP attack achieves strong transfer success across multiple LLM families and finetuned tasks.", "conclusion": "The pretrain-to-finetune paradigm harbors security risks as vulnerabilities are inherited via transferability; the PGP attack demonstrates a practical and robust method to exploit these vulnerabilities, highlighting the need for defense strategies at both pretraining and fine-tuning stages."}}
{"id": "2512.14717", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14717", "abs": "https://arxiv.org/abs/2512.14717", "authors": ["Ziqian Bi", "Danyang Zhang", "Junhao Song", "Chiung-Yi Tseng"], "title": "Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox", "comment": null, "summary": "The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model family alongside contemporary LLMs across ten diverse financial NLP tasks. Through extensive experimentation on 120B and 20B parameter variants of GPT-OSS, we reveal a counterintuitive finding: the smaller GPT-OSS-20B model achieves comparable accuracy (65.1% vs 66.5%) while demonstrating superior computational efficiency with 198.4 Token Efficiency Score and 159.80 tokens per second processing speed [1]. Our evaluation encompasses sentiment analysis, question answering, and entity recognition tasks using real-world financial datasets including Financial PhraseBank, FiQA-SA, and FLARE FINERORD. We introduce novel efficiency metrics that capture the trade-off between model performance and resource utilization, providing critical insights for deployment decisions in production environments. The benchmark reveals that GPT-OSS models consistently outperform larger competitors including Qwen3-235B, challenging the prevailing assumption that model scale directly correlates with task performance [2]. Our findings demonstrate that architectural innovations and training strategies in GPT-OSS enable smaller models to achieve competitive performance with significantly reduced computational overhead, offering a pathway toward sustainable and cost-effective deployment of LLMs in financial applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5728\u91d1\u878dNLP\u4efb\u52a1\u4e2d\uff0c\u8f83\u5c0f\u7684GPT-OSS-20B\u6a21\u578b\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u66f4\u5927\u6a21\u578b\u63a5\u8fd1\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5bf9\u624b\uff0c\u6311\u6218\u201c\u6a21\u578b\u8d8a\u5927\u8d8a\u597d\u201d\u7684\u666e\u904d\u89c2\u70b9\u3002", "motivation": "\u5728\u91d1\u878d\u670d\u52a1\u9886\u57df\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u6548\u7387\u5ea6\u91cf\uff0c\u5e2e\u52a9\u5728\u51c6\u786e\u6027\u4e0e\u8d44\u6e90\u5f00\u9500\u4e4b\u95f4\u5b9e\u73b0\u66f4\u4f18\u7684\u90e8\u7f72\u6298\u4e2d\u3002", "method": "\u57fa\u4e8eGPT-OSS\u5bb6\u65cf\u7684120B\u548c20B\u53c2\u6570\u53d8\u4f53\uff0c\u4ee5\u53ca\u5176\u4ed6\u524d\u6cbfLLM\uff0c\u5728\u5341\u4e2a\u91d1\u878dNLP\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\uff1b\u6570\u636e\u96c6\u5305\u62ecFinancial PhraseBank\u3001FiQA-SA\u3001FLARE FINERORD\uff1b\u63d0\u51fa\u5e76\u4f7f\u7528Token Efficiency Score\u7b49\u65b0\u6548\u7387\u6307\u6807\uff1b\u4e0eQwen3-235B\u7b49\u5927\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "20B GPT-OSS\u5728\u51c6\u786e\u7387\u65b9\u9762\u63a5\u8fd1\u751a\u81f3\u7565\u4f4e\u4e8e120B\u7248\u672c\uff0865.1% vs 66.5%\uff09\uff0c\u4f46\u5728\u6548\u7387\u65b9\u9762\u660e\u663e\u66f4\u4f18\uff0cToken Efficiency Score\u4e3a198.4\uff0c\u5904\u7406\u901f\u5ea6\u4e3a\u6bcf\u79d2159.80\u4e2atoken\uff1bGPT-OSS\u6574\u4f53\u4f18\u4e8e\u5305\u62ecQwen3-235B\u5728\u5185\u7684\u66f4\u5927\u5bf9\u624b\u3002", "conclusion": "\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u4e0e\u8bad\u7ec3\u7b56\u7565\uff0c\u5c0f\u578bGPT-OSS\u6a21\u578b\u53ef\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u91d1\u878d\u5e94\u7528\u7684\u53ef\u6301\u7eed\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u90e8\u7f72\u63d0\u4f9b\u8def\u5f84\uff0c\u5e76\u63d0\u4f9b\u6709\u52a9\u4e8e\u90e8\u7f72\u51b3\u7b56\u7684\u65b0\u57fa\u51c6\u4e0e\u6307\u6807\u3002"}}
{"id": "2512.14753", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14753", "abs": "https://arxiv.org/abs/2512.14753", "authors": ["Li Lin", "Siyuan Xin", "Yang Cao", "Xiaochun Cao"], "title": "CODE ACROSTIC: Robust Watermarking for Code Generation", "comment": null, "summary": "Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u901a\u8fc7Cue List\u5bf9\u4ee3\u7801\u7684\u4f4e\u71b5/\u9ad8\u71b5\u90e8\u5206\u8fdb\u884c\u533a\u5206\uff0c\u5728\u6ce8\u5165\u6c34\u5370\u65f6\u9075\u5faa\u8be5 Cue List\uff0c\u4ece\u800c\u5728\u9762\u5bf9\u6ce8\u91ca\u5220\u9664\u653b\u51fb\u65f6\u4ecd\u5177\u5907\u8f83\u9ad8\u7684\u53ef\u68c0\u6d4b\u6027\u4e0e\u53ef\u7528\u6027\u3002\u4e0e\u73b0\u6709\u4e09\u79cd\u4ee3\u7801\u6c34\u5370\u6280\u672f\u76f8\u6bd4\uff0c\u5728HumanEval\u4e0a\u53d6\u5f97\u66f4\u597d\u7ed3\u679c\u3002", "motivation": "\u9632\u6b62LLMs\u88ab\u6ee5\u7528\u7684\u4ee3\u7801\u6c34\u5370\u9700\u6c42\u65e5\u76ca\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5bf9\u4ee3\u7801\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u3002\u73b0\u6709\u7684\u4ee3\u7801\u6c34\u5370\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u6ce8\u91ca\u5220\u9664\u653b\u51fb\uff0c\u5bfc\u81f4\u6c34\u5370\u5931\u6548\uff0c\u9700\u5f00\u53d1\u5bf9\u6297\u4f4e\u71b5\u4ee3\u7801\u3001\u6ce8\u91ca\u5220\u9664\u7b49\u653b\u51fb\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u6784\u5efa\u4e00\u4e2a Cue List\uff0c\u7528\u4ee5\u533a\u5206\u4f4e\u71b5\u4e0e\u9ad8\u71b5\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u7136\u540e\u5728\u9ad8\u4fe1\u606f\u71b5\u533a\u57df\u6ce8\u5165\u6c34\u5370\uff0c\u5e76\u9075\u5faa Cue List \u6307\u5bfc\u7684\u7b56\u7565\u3002", "result": "\u5728 HumanEval \u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u4e09\u79cd\u5148\u8fdb\u7684\u4ee3\u7801\u6c34\u5370\u65b9\u6cd5\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u53ef\u68c0\u6d4b\u6027\u548c\u53ef\u7528\u6027\u4e0a\u5177\u6709\u66f4\u4f18\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8e Cue List \u7684\u6c34\u5370\u6ce8\u5165\u7b56\u7565\uff0c\u53ef\u4ee5\u63d0\u5347\u5bf9\u6ce8\u91ca\u5220\u9664\u7b49\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u5bf9 LLM \u4ee3\u7801\u8f93\u51fa\u7684\u6c34\u5370\u68c0\u6d4b\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.15419", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.15419", "abs": "https://arxiv.org/abs/2512.15419", "authors": ["Shilei Li", "Dawei Shi", "Hao Yu", "Ling Shi"], "title": "Variational Robust Kalman Filters: A Unified Framework", "comment": "23 pages", "summary": "Robustness and adaptivity are two competing objectives in Kalman filters (KF). Robustness involves temporarily inflating prior estimates of noise covariances, while adaptivity updates prior beliefs using real-time information. In practical applications, both process and measurement noise can be influenced by outliers, be time-varying, or both. Existing works may not effectively address the above complex noise scenarios, as there is an intrinsic incompatibility between robust filters and adaptive filters. In this work, we propose a unified variational robust Kalman filter, built on a Student's t-distribution induced loss function and variational inference, and solved through fixed-point iteration in a computationally efficient manner. We demonstrate that robustness can be understood as a prerequisite for adaptivity, making it possible to merge the above two competing goals into a single framework through switching rules. Additionally, our proposed filter can recover conventional KF, robust KF, and adaptive KF by adjusting parameters, and can suppress both the imperfect process and measurement noise, enabling it to perform superiorly in complex noise environments. Simulations verify the effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u9c81\u68d2\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u57fa\u4e8eStudent's t\u5206\u5e03\u635f\u5931\u4e0e\u53d8\u5206\u63a8\u65ad\uff0c\u5728\u56fa\u5b9a\u70b9\u8fed\u4ee3\u4e0b\u5b9e\u73b0\uff0c\u517c\u5bb9\u666e\u901aKF\u3001\u9c81\u68d2KF\u4e0e\u81ea\u9002\u5e94KF\uff0c\u5e76\u5728\u590d\u6742\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u9c81\u68d2\u6027\u548c\u81ea\u9002\u5e94\u6027\u5728\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e2d\u901a\u5e38\u76f8\u4e92\u77db\u76fe\u3002\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u8fc7\u7a0b\u4e0e\u6d4b\u91cf\u566a\u58f0\u53ef\u80fd\u56e0\u79bb\u7fa4\u503c\u3001\u65f6\u53d8\u6027\u800c\u590d\u6742\u5316\uff0c\u9700\u5728\u540c\u4e00\u6846\u67b6\u4e0b\u517c\u987e\u4e24\u8005\u5e76\u5904\u7406\u590d\u6742\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u53d8\u5206\u9c81\u68d2\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5229\u7528\u57fa\u4e8eStudent's t\u5206\u5e03\u7684\u635f\u5931\u51fd\u6570\u5e76\u901a\u8fc7\u53d8\u5206\u63a8\u65ad\u8fdb\u884c\u4f30\u8ba1\uff0c\u91c7\u7528\u56fa\u5b9a\u70b9\u8fed\u4ee3\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002\u901a\u8fc7\u5207\u6362\u89c4\u5219\u5c06\u9c81\u68d2\u6027\u4f5c\u4e3a\u81ea\u9002\u5e94\u7684\u524d\u63d0\uff0c\u4ece\u800c\u5728\u540c\u4e00\u6846\u67b6\u5185\u5207\u6362\u5b9e\u73b0\u666e\u901aKF\u3001\u9c81\u68d2KF\u4e0e\u81ea\u9002\u5e94KF\u3002\u8be5\u6ee4\u6ce2\u5668\u80fd\u591f\u6291\u5236\u4e0d\u7406\u60f3\u7684\u8fc7\u7a0b\u548c\u6d4b\u91cf\u566a\u58f0\uff0c\u4f7f\u5728\u590d\u6742\u566a\u58f0\u73af\u5883\u4e2d\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u4e0e\u9002\u5e94\u6027\u3002", "result": "\u7406\u8bba\u4e0e\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u9c81\u68d2\u6027\u662f\u5b9e\u73b0\u81ea\u9002\u5e94\u7684\u524d\u63d0\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u901a\u8fc7\u8c03\u8282\u53c2\u6570\u5728\u666e\u901aKF\u3001\u9c81\u68d2KF\u3001\u81ea\u9002\u5e94KF\u4e4b\u95f4\u5207\u6362\uff0c\u4e14\u5728\u542b\u79bb\u7fa4\u70b9\u548c\u65f6\u53d8\u566a\u58f0\u7684\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6ee4\u6ce2\u65b9\u6cd5\u3002", "conclusion": "\u7ed9\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6ee4\u6ce2\u6846\u67b6\uff0c\u5c06\u9c81\u68d2\u6027\u548c\u81ea\u9002\u5e94\u6027\u6709\u673a\u7ed3\u5408\uff0c\u901a\u8fc7\u5207\u6362\u89c4\u5219\u548c\u53d8\u5206\u63a8\u65ad\u5b9e\u73b0\u5bf9\u590d\u6742\u566a\u58f0\u73af\u5883\u7684\u9c81\u68d2\u5904\u7406\u4e0e\u81ea\u9002\u5e94\u8c03\u6574\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5728\u566a\u58f0\u7279\u6027\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.14718", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14718", "abs": "https://arxiv.org/abs/2512.14718", "authors": ["Feng Xiong", "Zongxia Xie", "Yanru Sun", "Haoyu Wang", "Jianhong Lin"], "title": "SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting", "comment": null, "summary": "Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \\textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.", "AI": {"tldr": "SEED\u901a\u8fc7\u5149\u8c31\u71b5\u5f15\u5bfc\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u51fa Dependency Evaluator\u3001Spectral Entropy-based Fuser\u3001Signed Graph Constructor\u3001Context Spatial Extractor\uff0c\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u4f9d\u8d56\u5efa\u6a21\u96be\u9898\uff0c\u5e76\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u65b0\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b/\u56fe\u7ed3\u6784\u7684\u65b9\u6cd5\u5728\u63d0\u53d6\u53d8\u91cf\u95f4\u4f9d\u8d56\u65f6\u5b58\u5728\u4e09\u5927\u95ee\u9898\uff1a\u5f3a\u65f6\u5e8f\u81ea\u76f8\u5173\u88ab\u65e0\u5173\u53d8\u91cf\u5e72\u6270\u3001softmax\u5904\u7406\u5bfc\u81f4\u5ffd\u7565\u6216\u62b5\u6d88\u8d1f\u76f8\u5173\u3001\u53d8\u91cf\u96be\u4ee5\u611f\u77e5\u65f6\u5e8f\u4f4d\u7f6e\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u9c81\u68d2\u7684\u4f9d\u8d56\u5efa\u6a21\u6846\u67b6\u3002", "method": "SEED\u6846\u67b6\u6838\u5fc3\u5305\u62ec\uff1a1) Dependency Evaluator\uff1a\u5229\u7528\u8c31\u71b5\u52a8\u6001\u8bc4\u4f30\u6bcf\u4e2a\u53d8\u91cf\u7684\u65f6\u7a7a\u4f9d\u8d56\uff0c\u667a\u80fd\u5e73\u8861Channel Independence\u4e0eChannel Dependence\uff1b2) Spectral Entropy-based Fuser\uff1a\u8fdb\u4e00\u6b65\u6d88\u9664\u7531\u5176\u4ed6\u53d8\u91cf\u5f15\u8d77\u7684\u65f6\u5e8f\u6b63\u5219\u5316\u5f71\u54cd\uff0c\u5206\u79bb\u8fd9\u90e8\u5206\u4f9d\u8d56\uff1b3) Signed Graph Constructor\uff1a\u5141\u8bb8\u8fb9\u6743\u5e26\u7b26\u53f7\uff0c\u4fdd\u7559\u8d1f\u76f8\u5173\uff1b4) Context Spatial Extractor\uff1a\u901a\u8fc7\u5c40\u90e8\u4e0a\u4e0b\u6587\u7a97\u53e3\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0c\u5e2e\u52a9\u53d8\u91cf\u611f\u77e5\u65f6\u5e8f\u4f4d\u7f6e\u3002", "result": "\u572812\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cSEED\u8fbe\u5230\u6216\u8d85\u8fc7\u5f53\u524d\u6700\u4f18\u6c34\u5e73\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "SEED\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u6709\u6548\u7684\u65f6\u7a7a\u4f9d\u8d56\u5efa\u6a21\u65b9\u6848\uff0c\u80fd\u591f\u66f4\u7cbe\u51c6\u5730\u6355\u6349\u53d8\u91cf\u95f4\u5173\u7cfb\u3001\u4fdd\u7559\u8d1f\u76f8\u5173\u5e76\u611f\u77e5\u65f6\u5e8f\u4f4d\u7f6e\uff0c\u5bf9\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.15067", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15067", "abs": "https://arxiv.org/abs/2512.15067", "authors": ["Zijiang Yan", "Yixiang Huang", "Jianhua Pei", "Hina Tabassum", "Luca Chiaraviglio"], "title": "EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks", "comment": "Submission for possible publication", "summary": "The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.", "AI": {"tldr": "EMFusion is a conditional multivariate diffusion-based forecasting framework for frequency-selective EMF data that provides calibrated probabilistic forecasts with explicit uncertainty, using a residual U\u2011Net with cross-attention and imputation-based sampling to handle irregular measurements and contextual factors (time, season, holidays).", "motivation": "There is a need to move beyond univariate, wideband predictions to frequency-selective multivariate forecasting that captures inter-operator and inter-frequency variations, while also quantifying uncertainty for compliant and proactive network planning; existing methods lack probabilistic forecasting and temporal coherence with irregular data.", "method": "A conditional multivariate diffusion model (EMFusion) with a residual U-Net backbone and cross-attention mechanism to incorporate external contextual factors. Forecasting is cast as a structural inpainting task via imputation-based sampling to maintain temporal coherence when data are irregular. Outputs are probabilistic, yielding calibrated prediction intervals.", "result": "On frequency-selective EMF datasets, incorporating working-hours context improves performance; EMFusion outperforms baselines by 23.85% in CRPS, 13.93% in NRMSE, and reduces CRPS error by 22.47%.", "conclusion": "EMFusion provides calibrated probabilistic forecasts with explicit uncertainty suitable for trustworthy decision-making in network planning and compliance; leveraging context improves accuracy, and the method robustly handles irregular data through imputation-based sampling."}}
{"id": "2512.14767", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.14767", "abs": "https://arxiv.org/abs/2512.14767", "authors": ["Unai Laskurain", "Aitor Aguirre-Ortuzar", "Urko Zurutuza"], "title": "Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation", "comment": "Presented at the 3rd IEEE International Conference on Federated Learning Technologies and Applications (FLTA25), October 2025", "summary": "Federated Learning (FL) is an emerging machine learning paradigm that enables multiple parties to collaboratively train models without sharing raw data, ensuring data privacy. In Vertical FL (VFL), where each party holds different features for the same users, a key challenge is to evaluate the feature contribution of each party before any model is trained, particularly in the early stages when no model exists. To address this, the Shapley-CMI method was recently proposed as a model-free, information-theoretic approach to feature valuation using Conditional Mutual Information (CMI). However, its original formulation did not provide a practical implementation capable of computing the required permutations and intersections securely. This paper presents a novel privacy-preserving implementation of Shapley-CMI for VFL. Our system introduces a private set intersection (PSI) server that performs all necessary feature permutations and computes encrypted intersection sizes across discretized and encrypted ID groups, without the need for raw data exchange. Each party then uses these intersection results to compute Shapley-CMI values, computing the marginal utility of their features. Initial experiments confirm the correctness and privacy of the proposed system, demonstrating its viability for secure and efficient feature contribution estimation in VFL. This approach ensures data confidentiality, scales across multiple parties, and enables fair data valuation without requiring the sharing of raw data or training models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79c1\u6709\u96c6\u5408\u4ea4\u96c6\u7684Shapley-CMI\u9690\u79c1\u5b9e\u73b0\uff0c\u7528\u4e8eVertical Federated Learning\uff0c\u5728\u4e0d\u4ea4\u6362\u539f\u59cb\u6570\u636e\u7684\u524d\u63d0\u4e0b\u8ba1\u7b97\u7279\u5f81\u8d21\u732e\u7684Shapley-CMI\u503c\u3002", "motivation": "\u5728Vertical FL\u4e2d\uff0c\u5728\u6a21\u578b\u5c1a\u672a\u8bad\u7ec3\u65f6\uff0c\u5982\u4f55\u8bc4\u4f30\u5404\u65b9\u7279\u5f81\u8d21\u732e\u662f\u5173\u952e\u6311\u6218\uff0c\u8be5\u5de5\u4f5c\u76ee\u6807\u662f\u5728\u65e9\u671f\u9636\u6bb5\u5b9e\u73b0\u65e0\u6a21\u578b\u3001\u65e0\u539f\u59cb\u6570\u636e\u5171\u4eab\u7684\u7279\u5f81\u4f30\u503c\u3002", "method": "\u5f15\u5165\u79c1\u6709\u96c6\u5408\u4ea4\u96c6(PSI)\u670d\u52a1\u5668\uff0c\u6267\u884c\u7279\u5f81\u7f6e\u6362\u548c\u4ea4\u96c6\u5927\u5c0f\u7684\u52a0\u5bc6\u8ba1\u7b97\uff0c\u9488\u5bf9\u79bb\u6563\u5316\u548c\u52a0\u5bc6\u7684ID\u7ec4\uff0c\u5728\u4e0d\u66b4\u9732\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5f97\u5230Shapley-CMI\u6240\u9700\u7684\u8fb9\u9645\u6548\u7528\uff1b\u5404\u65b9\u636e\u6b64\u8ba1\u7b97\u81ea\u5df1\u7684Shapley-CMI\u503c\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u6b63\u786e\u6027\u548c\u9690\u79c1\u6027\uff0c\u8bc1\u660e\u8be5\u7cfb\u7edf\u5728VFL\u4e2d\u5b9e\u73b0\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u7279\u5f81\u8d21\u732e\u4f30\u8ba1\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u591a\u65b9\uff0c\u4fdd\u62a4\u6570\u636e\u673a\u5bc6\u6027\uff0c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\u6216\u8bad\u7ec3\u6a21\u578b\uff0c\u5373\u53ef\u5b9e\u73b0\u516c\u5e73\u7684\u6570\u636e\u4f30\u503c\u3002"}}
{"id": "2512.15425", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.15425", "abs": "https://arxiv.org/abs/2512.15425", "authors": ["Peng Yuan", "Zulin Wang", "Tao Luo", "Yuanhan Ni"], "title": "An Anti-Interference AFDM System: Interference Impacts Analyses and Parameter Optimization", "comment": null, "summary": "This paper proposes an anti-interference affine frequency division multiplexing (AFDM) system to ensure reliability and resource efficiency under malicious high-power interference originating from adversarial devices in high-mobility scenarios. Closed-form expressions of interferences in the discrete affine Fourier transform (DAFT) domain are derived by utilizing the stationary phase principle and the Affine Fourier transform convolution theorem, which indicates that interference impacts can be classified into stationary and non-stationary categories. On this basis, we reveal the analytical relationship between packet throughput and the paramerters of spread spectrum and error correction coding in our proposed anti-interference system, which enables the design of a parameter optimization algorithm that maximizes packet throughput. For reception, by jointly utilizing the autocorrelation function of spreading sequence and the cyclic-shift property of AFDM input-output relation, we design a linear-complexity correlation-based DAFT domain detector (CDD) capable of achieving full diversity gain, which performs correlation-based equalization to avoid matrix inversion. Numerical results validate the accuracy of the derived closed-form expressions and verify that the proposed anti-interference AFDM system could achieve high packet throughput under interference in high-mobility scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6297\u5e72\u6270AFDM\u7cfb\u7edf\uff0c\u5728\u9ad8\u79fb\u52a8\u573a\u666f\u4e0b\u5bf9\u6765\u81ea\u5bf9\u624b\u8bbe\u5907\u7684\u9ad8\u529f\u7387\u5e72\u6270\u5b9e\u73b0\u53ef\u9760\u6027\u4e0e\u8d44\u6e90\u6548\u7387\u3002\u901a\u8fc7DAFT\u57df\u7684\u5e72\u6270\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3001\u5e72\u6270\u5206\u7c7b\uff08\u5b9a\u6001/\u975e\u5b9a\u6001\uff09\u4ee5\u53ca\u6563\u5c04\u8c31\u4e0e\u7f16\u7801\u53c2\u6570\u5bf9\u541e\u5410\u7684\u5f71\u54cd\u5173\u7cfb\uff0c\u8bbe\u8ba1\u53c2\u6570\u4f18\u5316\u7b97\u6cd5\u4ee5\u6700\u5927\u5316\u5206\u7ec4\u541e\u5410\uff1b\u5e76\u63d0\u51fa\u7ebf\u6027\u590d\u6742\u5ea6\u7684DAFT\u57df\u76f8\u5173\u68c0\u6d4b\u5668\uff08CDD\uff09\uff0c\u5229\u7528\u81ea\u76f8\u5173\u548cAFDM\u8f93\u5165\u8f93\u51fa\u7684\u5faa\u73af\u79fb\u4f4d\u7279\u6027\u5b9e\u73b0\u65e0\u77e9\u9635\u6c42\u9006\u7684\u76f8\u5173\u7b49\u5316\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u8868\u8fbe\u5f0f\u7684\u51c6\u786e\u6027\u4e0e\u9ad8\u541e\u5410\u3002", "motivation": "\u5728\u9ad8\u79fb\u52a8\u573a\u666f\u4e0b\uff0c\u9762\u5bf9\u6765\u81ea\u5bf9\u624b\u8bbe\u5907\u7684\u9ad8\u529f\u7387\u5e72\u6270\uff0c\u9700\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u4e0e\u8d44\u6e90\u5229\u7528\u7387\uff1b\u901a\u8fc7\u5728DAFT\u57df\u5bf9\u5e72\u6270\u8fdb\u884c\u523b\u753b\u5e76\u7ed3\u5408\u6269\u9891\u4e0e\u7ea0\u9519\u7f16\u7801\u4f18\u5316\u53c2\u6570\uff0c\u63d0\u5347\u541e\u5410\u5e76\u8bbe\u8ba1\u4f4e\u590d\u6742\u5ea6\u68c0\u6d4b\u65b9\u6848\u4ee5\u5b9e\u73b0\u5168\u901a\u9053\u5206\u96c6\u589e\u76ca\u3002", "method": "\u5229\u7528\u9759\u6001\u76f8\u4f4d\u539f\u7406\u548cAffine\u5085\u91cc\u53f6\u53d8\u6362\u5377\u79ef\u5b9a\u7406\uff0c\u5728\u79bb\u6563DAFT\u57df\u63a8\u5bfc\u5e72\u6270\u7684\u95ed\u5f0f\u8868\u8fbe\uff0c\u5c06\u5e72\u6270\u5206\u4e3a\u5b9a\u6001\u4e0e\u975e\u5b9a\u6001\uff1b\u5efa\u7acb\u5206\u7ec4\u541e\u5410\u7387\u4e0e\u6269\u9891\u53c2\u6570\u3001\u7f16\u7801\u53c2\u6570\u4e4b\u95f4\u7684\u89e3\u6790\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u53c2\u6570\u4f18\u5316\u7b97\u6cd5\u4ee5\u6700\u5927\u5316\u541e\u5410\u3002\u63a5\u6536\u7aef\u901a\u8fc7\u5229\u7528\u6269\u9891\u5e8f\u5217\u7684\u81ea\u76f8\u5173\u6027\u548cAFDM\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\u7684\u5faa\u73af\u79fb\u4f4d\u6027\u8d28\uff0c\u8bbe\u8ba1\u7ebf\u6027\u590d\u6742\u5ea6\u7684DAFT\u57df\u76f8\u5173\u68c0\u6d4b\u5668\uff08CDD\uff09\uff0c\u5b9e\u73b0\u76f8\u5173\u6027\u7b49\u5316\u4ee5\u907f\u514d\u77e9\u9635\u6c42\u9006\u5e76\u83b7\u5f97\u5168\u5206\u96c6\u589e\u76ca\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u63a8\u5bfc\u51fa\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u7684\u6b63\u786e\u6027\uff0c\u5e76\u8bc1\u660e\u6240\u63d0\u6297\u5e72\u6270AFDM\u5728\u9ad8\u79fb\u52a8\u573a\u666f\u4e0b\u80fd\u591f\u5728\u5e72\u6270\u5b58\u5728\u65f6\u5b9e\u73b0\u8f83\u9ad8\u7684\u5206\u7ec4\u541e\u5410\u7387\uff1b\u68c0\u6d4b\u5668\u5b9e\u73b0\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\u5e76\u8fbe\u5230\u5168\u5206\u96c6\u589e\u76ca\uff0c\u5b9e\u9a8c\u652f\u6301\u7406\u8bba\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6297\u5e72\u6270AFDM\u7cfb\u7edf\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u3001\u53c2\u6570\u4f18\u5316\u548c\u4f4e\u590d\u6742\u5ea6\u68c0\u6d4b\uff0c\u5b9e\u73b0\u5728\u9ad8\u79fb\u52a8\u4e14\u5b58\u5728\u9ad8\u529f\u7387\u5e72\u6270\u7684\u573a\u666f\u4e0b\u7684\u9ad8\u541e\u5410\u4e0e\u53ef\u9760\u6027\uff1b\u8be5\u6846\u67b6\u4e3a\u5728\u5bf9\u624b\u5e72\u6270\u73af\u5883\u4e2d\u63d0\u5347AFDM\u7cfb\u7edf\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u6790\u4e0e\u8bbe\u8ba1\u5de5\u5177\u3002"}}
{"id": "2512.14719", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14719", "abs": "https://arxiv.org/abs/2512.14719", "authors": ["Zhuoran Zhang", "Feng Zhang", "Shangyuan Li", "Yang Shi", "Yuanxing Zhang", "Wei Chen", "Tengjiao Wang", "Kam-Fai Wong"], "title": "Hybrid Attribution Priors for Explainable and Robust Model Training", "comment": "15 pages", "summary": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.", "AI": {"tldr": "\u63d0\u51fa Class-Aware Attribution Prior (CAP) \u53ca CAP Hybrid\uff0c\u901a\u8fc7\u66f4\u5177\u533a\u5206\u6027\u7684\u5f52\u56e0\u5148\u9a8c\u6765\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u867d\u7136\u80fd\u5b9a\u4f4d\u5230\u7c7b\u522b\u76f8\u5173\u7684\u6807\u8bb0\uff0c\u4f46\u5f80\u5f80\u805a\u7126\u4e8e\u8bed\u4e49\u76f8\u4f3c\u7c7b\u4e4b\u95f4\u7684\u5171\u6027\u5173\u952e\u8bcd\uff0c\u5bfc\u81f4\u533a\u5206\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u63d0\u4f9b\u5f3a\u6709\u529b\u7684\u5224\u522b\u6027\u76d1\u7763\u3002\u9700\u8981\u4ea7\u751f\u66f4\u7ec6\u7c92\u5ea6\u4e14\u5177\u5dee\u5f02\u6027\u7684\u5f52\u56e0\u5148\u9a8c\u4ee5\u63d0\u9ad8\u6a21\u578b\u533a\u5206\u80fd\u529b\u3002", "method": "\u5206\u6790\u4e3b\u6d41\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684 attribution \u65b9\u6cd5\uff0c\u63d0\u51fa CAP\uff0c\u63d0\u53d6\u9762\u5411\u7c7b\u522b\u5dee\u5f02\u7684\u5f52\u56e0\u5148\u9a8c\u4ee5\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u66f4\u7ec6\u7c92\u5ea6\u7684\u51b3\u7b56\u76f8\u5173\u7279\u5f81\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa CAP Hybrid\uff0c\u5c06 CAP \u7684\u5148\u9a8c\u4e0e\u73b0\u6709\u5f52\u56e0\u6280\u672f\u7684\u5148\u9a8c\u878d\u5408\uff0c\u5f62\u6210\u66f4\u5168\u9762\u3001\u5e73\u8861\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u5bf9\u9f50\u6a21\u578b\u81ea\u6ce8\u610f\u529b\u7684\u5f52\u56e0\u4e0e\u8fd9\u4e9b\u4e30\u5bcc\u5148\u9a8c\u6765\u4fc3\u8fdb\u591a\u6837\u5316\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u5168\u6570\u636e\u3001\u5c11\u6837\u672c\u4ee5\u53ca\u5bf9\u6297\u60c5\u666f\u4e0b\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u4e2d\uff0cCAP \u4e0e CAP Hybrid \u663e\u8457\u63d0\u5347\u4e86\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "CAP \u4e0e CAP Hybrid \u80fd\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u901a\u8fc7\u5bf9\u9f50\u81ea\u5f52\u56e0\u4e0e\u4e30\u5bcc\u5148\u9a8c\u5b9e\u73b0\u66f4\u5177\u533a\u5206\u6027\u7684\u5b66\u4e60\u3002"}}
{"id": "2512.15279", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15279", "abs": "https://arxiv.org/abs/2512.15279", "authors": ["Le Hao", "Robin Neuder", "Mohamadreza Delbari", "Alejandro Jim\u00e9nez-S\u00e1ez", "Vahid Jamali", "Arash Asadi", "Andrea Ortiz"], "title": "Learning-Based Phase Shift Optimization of Liquid Crystal RIS in Dynamic mmWave Networks", "comment": null, "summary": "To enhance coverage and signal quality in millimeter-wave (mmWave) frequencies, reconfigurable intelligent surfaces (RISs) have emerged as a game-changing solution to manipulate the wireless environment. Traditional semiconductor-based RISs face scalability issues due to high power consumption. Meanwhile, liquid crystal-based RISs (LC-RISs) offer energy-efficient and cost-effective operation even for large arrays. However, this promise has a caveat. LC-RISs suffer from long reconfiguration times, on the order of tens of milliseconds, which limits their applicability in dynamic scenarios. To date, prior works have focused on hardware design aspects or static scenarios to address this limitation, but little attention has been paid to optimization solutions for dynamic settings. Our paper fills this gap by proposing a reinforcement learning-based optimization framework to dynamically control the phase shifts of LC-RISs and maximize the data rate of a moving user. Specifically, we propose a Deep Deterministic Policy Gradient (DDPG) algorithm that adapts the LC-RIS phase shifts without requiring perfect channel state information and balances the tradeoff between signal-to-noise ratio (SNR) and configuration time. We validate our approach through high-fidelity ray tracing simulations, leveraging measurement data from an LC-RIS prototype. Our results demonstrate the potential of our solution to bring adaptive control to dynamic LC-RIS-assisted mmWave systems.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u52a8\u6001\u573a\u666f\u7684LC-RIS\u81ea\u9002\u5e94\u63a7\u5236\u6846\u67b6\uff0c\u91c7\u7528DDPG\u8fdb\u884c\u76f8\u4f4d\u63a7\u5236\u4ee5\u63d0\u5347\u79fb\u52a8\u7528\u6237\u4e0b\u7684\u6beb\u7c73\u6ce2\u6570\u636e\u901f\u7387\uff0c\u540c\u65f6\u5728\u4e0d\u5b8c\u7f8eCSI\u6761\u4ef6\u4e0b\u6743\u8861\u4fe1\u566a\u6bd4\u4e0e\u914d\u7f6e\u65f6\u95f4\uff0c\u5229\u7528\u9ad8\u4fdd\u771f\u5c04\u7ebf\u8ffd\u8e2a\u4e0eLC\u539f\u578b\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u6beb\u7c73\u6ce2\u901a\u4fe1\u9700\u8981\u66f4\u9ad8\u8986\u76d6\u548c\u4fe1\u53f7\u8d28\u91cf\uff1bLC-RIS\u5177\u80fd\u6548\u548c\u964d\u4f4e\u6210\u672c\u7684\u6f5c\u529b\uff0c\u4f46\u91cd\u65b0\u914d\u7f6e\u65f6\u95f4\u957f\uff08\u51e0\u5341\u6beb\u79d2\uff09\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5e94\u7528\u53d7\u9650\u3002\u73b0\u6709\u5de5\u4f5c\u591a\u6570\u805a\u7126\u786c\u4ef6\u8bbe\u8ba1\u6216\u9759\u6001\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DDPG\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574LC-RIS\u76f8\u4f4d\u4ee5\u6700\u5927\u5316\u6570\u636e\u7387\uff1b\u4e0d\u4f9d\u8d56\u5b8c\u7f8e\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff0c\u6743\u8861\u4fe1\u566a\u6bd4\u4e0e\u914d\u7f6e\u65f6\u95f4\uff1b\u5728\u9ad8\u4fdd\u771f\u5c04\u7ebf\u8ffd\u8e2a\u4eff\u771f\u548cLC-RIS\u539f\u578b\u6d4b\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u53ef\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5bf9LC-RIS\u76f8\u4f4d\u8fdb\u884c\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u63d0\u5347\u52a8\u6001mmWave\u7cfb\u7edf\u7684\u6027\u80fd\u6f5c\u529b\uff0c\u4e14\u8bc1\u660e\u5728\u7f3a\u5c11\u5b8c\u7f8eCSI\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u83b7\u5f97\u6709\u6548\u6570\u636e\u7387\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684RL\u9a71\u52a8\u7684LC-RIS\u52a8\u6001\u63a7\u5236\u6846\u67b6\u4e3a\u89e3\u51b3\u6162\u91cd\u65b0\u914d\u7f6e\u95ee\u9898\u63d0\u4f9b\u4e00\u6761\u53ef\u884c\u8def\u5f84\uff0c\u7ed3\u5408\u4eff\u771f\u4e0e\u5b9e\u6d4b\u6570\u636e\uff0c\u663e\u793a\u51fa\u5728\u52a8\u6001\u573a\u666f\u4e0b\u5b9e\u73b0\u81ea\u9002\u5e94\u63a7\u5236\u7684\u6f5c\u529b\u4e0e\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.15691", "categories": ["cs.LG", "cs.IT", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15691", "abs": "https://arxiv.org/abs/2512.15691", "authors": ["Matin Mortaheb", "Erciyes Karakaya", "Sennur Ulukus"], "title": "Multi-Modal Semantic Communication", "comment": null, "summary": "Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u67e5\u8be2\u5f15\u5bfc\u89c6\u89c9\u4fe1\u606f\u62bd\u53d6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u7b5b\u9009\u4fe1\u606f\uff0c\u6309\u4fe1\u9053\u5bb9\u91cf\u81ea\u9002\u5e94\u4f20\u8f93\u56fe\u50cf\u7247\u6bb5\u5e76\u72ec\u7acb\u8bad\u7ec3\u7684\u7f16\u89e3\u7801\u5668\u5bf9\u5b9e\u73b0\u91cd\u6784\u3002", "motivation": "\u5728\u590d\u6742\u573a\u666f\u548c\u5e26\u5bbd\u53d7\u9650\u6761\u4ef6\u4e0b\uff0c\u4f20\u7edf\u4ec5\u4f9d\u8d56\u89c6\u89c9\u4fe1\u53f7\u7684\u8bed\u4e49\u4f20\u8f93\u96be\u4ee5\u9ad8\u6548\u6355\u6349\u4efb\u52a1\u5173\u952e\u4fe1\u606f\uff0c\u9700\u5f15\u5165\u8bed\u8a00\u6307\u4ee4\u6765\u63d0\u4f9b\u660e\u786e\u7684\u4efb\u52a1\u76ee\u6807\u4e0e\u4e0a\u4e0b\u6587\u3002", "method": "\u5f15\u5165\u6587\u672c-\u89c6\u89c9\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6765\u4ea7\u751f\u5bf9\u89c6\u89c9\u6570\u636e\u7684\u8f6f\u76f8\u5173\u6027\u5206\u6570\uff1b\u57fa\u4e8e\u8fd9\u4e9b\u5206\u6570\u548c\u5373\u65f6\u4fe1\u9053\u5e26\u5bbd\uff0c\u4f7f\u7528\u72ec\u7acb\u8bad\u7ec3\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5bf9\u4ee5\u81ea\u9002\u5e94\u5206\u8fa8\u7387\u4f20\u8f93\u56fe\u50cf\u5757\uff0c\u786e\u4fdd\u603b\u6bd4\u7279\u7387\u5339\u914d\u4fe1\u9053\u5bb9\u91cf\uff1b\u63a5\u6536\u7aef\u91cd\u6784\u5e76\u62fc\u63a5\u5757\u4ee5\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002", "result": "\u672a\u7ed9\u51fa\u5177\u4f53\u5b9e\u9a8c\u6570\u636e\uff0c\u4e3b\u8981\u63d0\u51fa\u6846\u67b6\u3001\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5b9e\u73b0\u601d\u8def\u3002", "conclusion": "\u901a\u8fc7\u6587\u672c\u67e5\u8be2\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u8bed\u4e49\u4f20\u8f93\u5b9e\u73b0\u5bf9\u4efb\u52a1\u5173\u952e\u4fe1\u606f\u7684\u9ad8\u6548\u4f20\u8f93\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u4e0e\u5e26\u5bbd\u53d7\u9650\u73af\u5883\uff0c\u663e\u793a\u51fa\u7075\u6d3b\u6027\u548c\u76ee\u6807\u9a71\u52a8\u6027\u3002"}}
{"id": "2512.14721", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14721", "abs": "https://arxiv.org/abs/2512.14721", "authors": ["Arno Appenzeller", "Nick Terzer", "Andr\u00e9 Hohmeyer", "Jan-Philipp Redlich", "Sabine Luttmann", "Friedrich Feuerhake", "Nadine S. Schaadt", "Timm Intemann", "Sarah Teuber-Hanselmann", "Stefan Nikolin", "Joachim Weis", "Klaus Kraywinkel", "Pascal Birnstill"], "title": "Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example", "comment": "16 pages, 8 figures", "summary": "The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates data based on rules describing the lifetime of a synthetic patient. These rules typically express the probability of a condition occurring, such as a disease, depending on factors like age. Since they only contain statistical information, rules usually have no specific data protection requirements. However, creating meaningful rules can be a very complex process that requires expert knowledge and realistic sample data. In this paper, we introduce and evaluate an approach to automatically generate Synthea rules based on statistics from tabular data, which we extracted from cancer reports. As an example use case, we created a Synthea module for glioblastoma from a real-world dataset and used it to generate a synthetic dataset. Compared to the original dataset, the synthetic data reproduced known disease courses and mostly retained the statistical properties. Overall, synthetic patient data holds great potential for privacy-preserving research. The data can be used to formulate hypotheses and to develop prototypes, but medical interpretation should consider the specific limitations as with any currently available approach.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7edf\u8ba1\u4fe1\u606f\u81ea\u52a8\u751f\u6210 Synthea \u89c4\u5219\u4ee5\u751f\u6210\u9690\u79c1\u4fdd\u62a4\u7684\u5408\u6210\u533b\u7597\u6570\u636e\uff0c\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u764c\u75c7\u6570\u636e\u6784\u5efa glioblastoma \u6a21\u5757\u5e76\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5bf9\u6bd4\u539f\u59cb\u6570\u636e\u8bc4\u4f30\u7edf\u8ba1\u5c5e\u6027\u4fdd\u7559\u3002", "motivation": "\u89e3\u51b3\u5408\u6210\u533b\u7597\u6570\u636e\u7684\u89c4\u5219\u8bbe\u8ba1\u590d\u6742\u6027\u4e0e\u5bf9\u73b0\u5b9e\u6837\u672c\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u7814\u7a76\u4e2d\u7684\u6570\u636e\u53ef\u7528\u6027\u4e0e\u53ef\u91cd\u590d\u6027\u3002", "method": "\u4ece\u8868\u683c\u6570\u636e\u4e2d\u63d0\u53d6\u7edf\u8ba1\u4fe1\u606f\uff0c\u81ea\u52a8\u751f\u6210 Synthea \u89c4\u5219\uff1b\u4ee5\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624 glioblastoma \u4e3a\u4f8b\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u521b\u5efa\u6a21\u5757\u5e76\u751f\u6210\u5408\u6210\u6570\u636e\uff1b\u8bc4\u4f30\u5408\u6210\u6570\u636e\u5728\u75be\u75c5\u8fdb\u7a0b\u548c\u7edf\u8ba1\u5c5e\u6027\u4e0a\u7684\u4fdd\u7559\u7a0b\u5ea6\u3002", "result": "\u5408\u6210\u6570\u636e\u5728\u518d\u73b0\u5df2\u77e5\u75be\u75c5\u8fdb\u7a0b\u548c\u5927\u591a\u6570\u7edf\u8ba1\u7279\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff1b\u8868\u660e\u5bf9\u9690\u79c1\u4fdd\u62a4\u7814\u7a76\u5177\u6709\u6f5c\u5728\u4ef7\u503c\uff0c\u4f46\u9700\u8981\u8003\u8651\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u540c\u7684\u533b\u5b66\u89e3\u91ca\u4e0e\u5e94\u7528\u9650\u5236\u3002", "conclusion": "\u57fa\u4e8e\u7edf\u8ba1\u4fe1\u606f\u81ea\u52a8\u751f\u6210\u89c4\u5219\u7684 Synthea \u65b9\u6cd5\u5177\u6709\u63d0\u5347\u5408\u6210\u6570\u636e\u53ef\u7528\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\u7814\u7a76\u6f5c\u529b\u7684\u524d\u666f\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3001\u9002\u7528\u6027\u53ca\u5bf9\u975e\u76ee\u6807\u75be\u75c5\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.15283", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15283", "abs": "https://arxiv.org/abs/2512.15283", "authors": ["Colin Cros", "Laurent Ferro-Famil"], "title": "Moment-Matching Array Processing Technique for diffuse source estimation", "comment": null, "summary": "Direction of Arrival (DOA) estimation is a fundamental problem in signal processing. Diffuse sources, whose power density cannot be represented with a single angular coordinate, are usually characterized based on prior assumptions, which associate the source angular density with a specific set of functions. However, these assumptions can lead to significant estimation biases when they are incorrect. This paper introduces the Moment-Matching Estimation Technique (MoMET), a low-complexity method for estimating the mean DOA, spread, and power of a narrow diffuse source without requiring prior knowledge on the source distribution. The unknown source density is characterized by its mean DOA and its first central moments, which are estimated through covariance matching techniques which fit the empirical covariance of the measurements to that modeled from the moments. The MoMET parameterization is robust to incorrect model assumptions, and numerically efficient. The asymptotic bias and covariance of the new estimator are derived and its performance is demonstrated through simulations.", "AI": {"tldr": "MoMET\uff1a\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u57fa\u4e8e\u77e9\u91cf\u5339\u914d\u7684 diffuse \u6e90 DOA \u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u65e0\u9700\u5bf9\u6e90\u5206\u5e03\u505a\u5f3a\u5148\u9a8c\u5047\u8bbe\u7684\u524d\u63d0\u4e0b\uff0c\u540c\u65f6\u4f30\u8ba1\u5e73\u5747\u5230\u8fbe\u89d2\u3001\u5206\u5e03\u6269\u5c55\uff08\u7b2c\u4e00\u4e2d\u5fc3\u77e9\uff09\u4e0e\u80fd\u91cf\uff0c\u901a\u8fc7\u534f\u65b9\u5dee\u5339\u914d\u5b9e\u73b0\u4f30\u8ba1\uff0c\u9c81\u68d2\u4e14\u9ad8\u6548\uff0c\u5e76\u63a8\u5bfc\u4e86\u6e10\u8fd1\u504f\u5dee\u4e0e\u534f\u65b9\u5dee\uff0c\u4e14\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u6269\u6563\u6e90 DOA \u4f30\u8ba1\u5f80\u5f80\u4f9d\u8d56\u4e8e\u5bf9\u529f\u7387\u5bc6\u5ea6\u7684\u5148\u9a8c\u5206\u5e03\u5047\u8bbe\uff1b\u82e5\u5047\u8bbe\u9519\u8bef\u4f1a\u5bfc\u81f4\u663e\u8457\u4f30\u8ba1\u504f\u5dee\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5bf9\u5148\u9a8c\u5206\u5e03\u4f9d\u8d56\u8f83\u5f31\u3001\u53c2\u6570\u53ef\u76f4\u63a5\u4f30\u8ba1\u7684\u9c81\u68d2\u65b9\u6cd5\u3002", "method": "\u5c06\u672a\u77e5\u7684\u6e90\u5bc6\u5ea6\u7528\u5176\u5747\u503c DOA \u4e0e\u4e00\u9636\u4e2d\u5fc3\u77e9\u6765\u523b\u753b\uff1b\u901a\u8fc7\u5bf9\u89c2\u6d4b\u91cf\u7684\u7ecf\u9a8c\u534f\u65b9\u5dee\u4e0e\u7531\u8fd9\u4e9b\u77e9\u91cf\u6240\u5efa\u6a21\u7684\u534f\u65b9\u5dee\u8fdb\u884c\u5339\u914d\u6765\u4f30\u8ba1\u53c2\u6570\uff08\u534f\u65b9\u5dee\u5339\u914d\uff09\u3002\u8be5\u8fc7\u7a0b\u5b9e\u73b0\u4f4e\u590d\u6742\u5ea6\u4f30\u8ba1\uff0c\u5e76\u63a8\u5bfc\u51fa MoMET \u7684\u6e10\u8fd1\u504f\u5dee\u4e0e\u534f\u65b9\u5dee\u3002", "result": "\u63d0\u51fa\u7684 MoMET \u5177\u6709\u9c81\u68d2\u6027\u5f3a\u3001\u6570\u503c\u6548\u7387\u9ad8\u7684\u7279\u6027\uff0c\u4e14\u7ed9\u51fa\u6e10\u8fd1\u6027\u8d28\uff08\u504f\u5dee\u3001\u534f\u65b9\u5dee\uff09\u7684\u63a8\u5bfc\uff1b\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "MoMET \u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5f3a\u5148\u9a8c\u5206\u5e03\u5047\u8bbe\u5373\u53ef\u5bf9\u7a84\u5e26\u6269\u6563\u6e90\u8fdb\u884c\u6709\u6548\u7684 DOA\u3001 dispersion\uff08\u6269\u5c55\uff09\u548c\u529f\u7387\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u548c\u6570\u503c\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u9002\u7528\u4e8e\u9700\u4f4e\u590d\u6742\u5ea6\u4e14\u5bf9\u5148\u9a8c\u4f9d\u8d56\u8f83\u5f31\u7684\u573a\u666f\u3002"}}
{"id": "2512.14846", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14846", "abs": "https://arxiv.org/abs/2512.14846", "authors": ["Arth Bhardwaj", "Sia Godika", "Yuvam Loonker"], "title": "MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber", "comment": null, "summary": "Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&CK mappings).\n  For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records.\n  In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.", "AI": {"tldr": "A practical Multi-Agent LLM Cyber Defense Framework (MALCDF) coordinates four LLM agents\u2014Detection, Intelligence, Response, and Analysis\u2014via a secure, ontology-aligned messaging layer to achieve real-time cyber defense, outperforming a lightweight ML-IDS baseline on a 50-record CICIDS2017-based stream.", "motivation": "Traditional, centralized security tools often miss adaptive, multi-vector attacks; there is a need for real-time coordination among specialized LLM agents with auditable outputs.", "method": "Four LLM agents (Detection, Intelligence, Response, Analysis) operate in real time and communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages. Outputs include audit-friendly artifacts like MITRE ATT&CK mappings. Evaluation uses a 50-record live stream derived from CICIDS2017, with training on CICIDS2017 fields/schema. Baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the same 50-record stream, with no training/test overlap.", "result": "MALCDF achieves 90.0% detection accuracy, 85.7% F1-score, 9.1% false-positive rate, and 6.8 seconds average per-event latency, outperforming the LRF-IDS baseline and a single-LLM setup in accuracy while delivering consistent end-to-end outputs.", "conclusion": "Coordinating simple LLM agents with secure, ontology-aligned messaging can enhance practical, real-time cyber defense."}}
{"id": "2512.14722", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14722", "abs": "https://arxiv.org/abs/2512.14722", "authors": ["Mohamed Malhou", "Ludovic Perret", "Kristin Lauter"], "title": "HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers", "comment": null, "summary": "At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)", "AI": {"tldr": "\u4f7f\u7528\u5c42\u6b21\u6ce8\u610f\u529b\u53d8\u6362\u5668(HAT)\u901a\u8fc7\u6811\u72b6\u7ed3\u6784\u504f\u7f6e\u6765\u8ba1\u7b97Groebner\u57fa\uff0c\u63d0\u9ad8\u591a\u53d8\u91cf\u591a\u9879\u5f0f\u65b9\u7a0b\u7ec4\u6c42\u89e3\u7684\u6548\u7387\uff1b\u6269\u5c55\u5230\u4efb\u610f\u6df1\u5ea6\uff0c\u7ed9\u51fa\u6210\u672c\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u6bd4\u5148\u524d\u5de5\u4f5c\u66f4\u5927\u89c4\u6a21\u7684\u5b9e\u4f8b\u6c42\u89e3\u3002", "motivation": "Groebner\u57fa\u662f\u591a\u9879\u5f0f\u65b9\u7a0b\u6c42\u89e3\u4e2d\u7684\u6838\u5fc3\u5de5\u5177\uff0c\u4f46\u4f20\u7edf\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u6269\u5c55\u5230\u9ad8\u7ef4/\u5927\u89c4\u6a21\u3002\u57fa\u4e8eNeurIPS 2024\u7684Kera\u7b49\u5de5\u4f5c\uff0c\u672c\u6587\u901a\u8fc7\u5f15\u5165\u5c42\u6b21\u7ed3\u6784\u7684\u6ce8\u610f\u529b\u6a21\u578b\u6765\u63d0\u5347\u5b66\u4e60\u6c42\u89e3Groebner\u57fa\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528Hierarchical Attention Transformers\uff08HATs\uff09\uff0c\u5728Transformer\u4e2d\u5f15\u5165\u6811\u72b6\u7ed3\u6784\u7684\u5f52\u7eb3\u504f\u7f6e\u4ee5\u5efa\u6a21\u6570\u636e\u4e2d\u7684\u5c42\u7ea7\u5173\u7cfb\uff1b\u5c06\u5176\u5e94\u7528\u4e8eGroebner\u57fa\u7684\u6c42\u89e3\u8fc7\u7a0b\u5e76\u63a8\u5e7f\u5230\u4efb\u610f\u6df1\u5ea6\uff0c\u63d0\u4f9b\u8be6\u7ec6\u7684\u8ba1\u7b97\u6210\u672c\u5206\u6790\uff1b\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u4ee5\u9010\u6b65\u589e\u5927\u96be\u5ea6\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u5347\u5bf9\u5927\u89c4\u6a21\u5b9e\u4f8b\u7684\u6c42\u89e3\u80fd\u529b\u3002", "result": "\u4e0e\u666e\u901a\u5e73\u5766\u6ce8\u610f\u529b\u6a21\u578b\u76f8\u6bd4\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1b\u5728\u6bd4Kera\u7b492024\u5e74\u7684\u5de5\u4f5c\u66f4\u5927\u89c4\u6a21\u7684\u5b9e\u4f8b\u4e0a\u5b9e\u73b0\u6c42\u89e3\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u7ed9\u51fa\u7cfb\u7edf\u7684\u6210\u672c\u5206\u6790\u3002", "conclusion": "\u5728Groebner\u57fa\u8ba1\u7b97\u4e2d\u5f15\u5165HAT\u53ef\u663e\u8457\u63d0\u5347\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u4e14\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u9002\u7528\u89c4\u6a21\uff0c\u4e3a\u591a\u9879\u5f0f\u65b9\u7a0b\u7ec4\u6c42\u89e3\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u5b66\u4e60\u9a71\u52a8\u5de5\u5177\u3002"}}
{"id": "2512.15290", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15290", "abs": "https://arxiv.org/abs/2512.15290", "authors": ["Jie Zhou", "Junhao Xie"], "title": "On the Asymptotic Performance of Diagonally Loaded Detectors for Large Arrays: To Achieve CFAR and Optimality", "comment": null, "summary": "This paper addresses two critical limitations in diagonally loaded (DL) adaptive matched filter (AMF) detector: (1) the lack of CFAR property with respect to arbitrary covariance matrices, and (2) the absence of selection criteria for optimal loading factor from the perspective of maximizing the detection probability (Pd). We provide solutions to both challenges through a comprehensive analysis for the asymptotic performance of DL-AMF under large dimensional regime (LDR) where the dimension N and sample size K tend to infinity whereas their ratio N/K converges to a constant c\\in(0,1). The analytical results show that any DL detectors constructed by normalizing the random variable |a|2=|sH(R+\u03bbIN)-1y0|2 with a deterministic quantity or a random variable that converges almost surely to a deterministic value will exhibit equivalent performance under LDR. Following this idea, we derive two CFAR DL detectors: CFAR DL semi-clairvoyant matched filter (CFAR-DL-SCMF) detector and CFAR DL adaptive matched filter (CFAR-DL-AMF) detector, by normalizing |a|2 with an appropriate deterministic quantity and its consistent estimate, respectively. The theoretical analysis and simulations show that both CFAR-DL-SCMF and CFAR-DL-AMF achieve CFAR with respect to covariance matrix, target steering vector and loading factor. Furthermore, we derive the asymptotically optimal loading factor \u03bb_opt by maximizing the explicit expression of asymptotic Pd. For practical implementation, we provide a consistent estimator for \u03bb_opt under LDR. Based on \u03bb_opt and its consistent estimate, we establish the optimal CFAR-DL-SCMF (opt-CFAR-DL-SCMF) and the optimal CFAR-DL-AMF (opt-CFAR-DL-AMF). Numerical examples demonstrate that the proposed opt-CFAR-DL-SCMF and opt-CFAR-DL-AMF consistently outperform EL-AMF and persymmetric AMF in both full-rank and low-rank clutter plus noise environments.", "AI": {"tldr": "\u5728\u5927\u7ef4\u5ea6\u6781\u9650\u4e0b\uff0c\u63d0\u51fa\u4e24\u79cdCFAR DL\u68c0\u6d4b\u5668\uff08CFAR-DL-SCMF\u3001CFAR-DL-AMF\uff09\uff0c\u901a\u8fc7\u5bf9 |a|^2 \u7684\u5f52\u4e00\u5316\u5b9e\u73b0\u5bf9\u534f\u65b9\u5dee\u77e9\u9635\u3001\u76ee\u6807\u65b9\u5411\u4ee5\u53ca\u52a0\u8f7d\u56e0\u5b50\u7684CFAR\u6027\uff0c\u63a8\u5bfc\u6700\u4f18\u52a0\u8f7d \u03bb_opt \u53ca\u5176\u4e00\u81f4\u4f30\u8ba1\uff0c\u7ed9\u51fa\u6700\u4f18 CFAR-DL-SCMF/AMF\uff0c\u5e76\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e EL-AMF \u4e0e persymmetric AMF\u3002", "motivation": "\u89e3\u51b3 DL-AMF \u5728\u4efb\u610f\u534f\u65b9\u5dee\u77e9\u9635\u4e0b\u7f3a\u4e4f CFAR \u6027\u4ee5\u53ca\u7f3a\u5c11\u9009\u53d6\u6700\u4f73\u52a0\u8f7d\u56e0\u5b50\u7684\u51c6\u5219\u7684\u95ee\u9898\uff1b\u5728\u5927\u7ef4\u5ea6\u6781\u9650\u4e0b\u5206\u6790\u6027\u80fd\u5e76\u7ed9\u51fa\u53ef\u5b9e\u65bd\u7684\u52a0\u8f7d\u56e0\u5b50\u4f30\u8ba1\u3002", "method": "\u5728\u5927\u6837\u672c\u6781\u9650\u4e0b\uff0c\u8bc1\u660e DL \u68c0\u6d4b\u5668\u7684 |a|^2 \u5728\u5f52\u4e00\u5316\u540e\u7b49\u4ef7\u4e8e\u786e\u5b9a\u91cf\u6536\u655b\uff0c\u4ece\u800c\u8bbe\u8ba1\u4e24\u79cd CFAR DL \u68c0\u6d4b\u5668\uff0c\u5e76\u63a8\u5bfc \u03bb_opt \u7684\u89e3\u6790\u8868\u8fbe\u53ca\u4e00\u81f4\u4f30\u8ba1\u3002", "result": "CFAR-DL-SCMF \u4e0e CFAR-DL-AMF \u63d0\u4f9b\u5bf9\u534f\u65b9\u5dee\u77e9\u9635\u3001\u76ee\u6807\u65b9\u5411\u548c\u52a0\u8f7d\u56e0\u5b50\u7684CFAR\u6027\uff1b\u7ed9\u51fa \u03bb_opt \u7684\u663e\u5f0f\u8868\u8fbe\u53ca\u4e00\u81f4\u4f30\u8ba1\uff1b\u5b9e\u9a8c\u663e\u793a\u4e24\u8005\u4f18\u4e8e EL-AMF \u4e0e persymmetric AMF\uff0c\u5728\u5168\u79e9\u4e0e\u4f4e\u79e9\u73af\u5883\u4e0b\u5747\u6709\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u5927\u7ef4\u5ea6\u5206\u6790\u5b9e\u73b0 CFAR-DL \u68c0\u6d4b\uff0c\u63d0\u51fa\u4e24\u79cd\u53ef\u5b9e\u65bd\u7684\u6700\u4f18 CFAR-DL \u68c0\u6d4b\u5668\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u4e0e\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2512.15441", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15441", "abs": "https://arxiv.org/abs/2512.15441", "authors": ["Gilderlan Tavares de Ara\u00fajo", "Andr\u00e9 L. F. de Almeida Buno Sokal", "Gabor Fodor", "Paulo R. B. Gomes"], "title": "Semi-Blind Joint Channel and Symbol Estimation for Beyond Diagonal Reconfigurable Surfaces", "comment": null, "summary": "The beyond-diagonal reconfigurable intelligent surface (BD-RIS) is a recent architecture in which scattering elements are interconnected to enhance the degrees of freedom for wave control, yielding performance gains over traditional single-connected RISs. For BD-RIS, channel estimation - well-studied for conventional RIS - becomes more challenging due to the complex connections and a larger number of coefficients. Prior works rely on pilot-assisted estimation followed by data decoding. This paper introduces a semi-blind tensor-based approach for joint channel and symbol estimation that eliminates the need for training sequences by leveraging data symbols directly. A practical scenario with time-varying user terminal-RIS channels under mobility is considered. By reformulating the received signal from a tensor decomposition perspective, we develop two semi-blind receivers: a two-stage method transforming the fourth-order PARATUCK model into a third-order PARAFAC model, and a single-stage iterative process based on fourth-order TUCKER decomposition. Identifiability conditions for reliable joint recovery are derived, and numerical results demonstrate the performance advantages and trade-offs of the proposed schemes over existing solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76f2\u5f20\u91cf\u65b9\u6cd5\u7528\u4e8eBD-RIS\u4e0a\u8054\u5408\u901a\u9053\u4e0e\u7b26\u53f7\u4f30\u8ba1\uff0c\u7701\u7565\u8bad\u7ec3\u5e8f\u5217\uff0c\u7ed9\u51fa\u4e24\u79cd\u5b9e\u73b0\uff08\u4e24\u9636\u6bb5PARATUCK\u8f6cPARAFAC\u53ca\u7b2c\u56db\u9636TUCKER\u5355\u9636\u6bb5\uff09\u548c\u8fa8\u8bc6\u6027\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u6bd4\u8f83\u73b0\u6709\u65b9\u6848\u7684\u6027\u80fd\u4e0e\u6743\u8861\u3002", "motivation": "\u89e3\u51b3BD-RIS\u4e2d\u56e0\u590d\u6742\u8fde\u901a\u6027\u5e26\u6765\u7684\u901a\u9053\u4f30\u8ba1\u5f00\u9500\u4e0e\u5bf9\u8bad\u7ec3\u5e8f\u5217\u4f9d\u8d56\uff0c\u540c\u65f6\u5728\u79fb\u52a8\u573a\u666f\u4e0b\u5b9e\u73b0\u5bf9\u65f6\u53d8\u4fe1\u9053\u7684\u65e0\u8bad\u7ec3\u4f30\u8ba1\u3002", "method": "\u5c06\u63a5\u6536\u4fe1\u53f7\u91cd\u6784\u4e3a\u9ad8\u9636\u5f20\u91cf\u5e76\u5e94\u7528\u534a\u76f2\u5f20\u91cf\u5206\u89e3\uff1b\u63d0\u51fa\u4e24\u79cd\u534a\u76f2\u63a5\u6536\u673a\uff1a1) \u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5c06\u56db\u9636PARATUCK\u6a21\u578b\u8f6c\u5316\u4e3a\u4e09\u9636PARAFAC\u4ee5\u5b9e\u73b0\u5206\u89e3\uff1b2) \u57fa\u4e8e\u7b2c\u56db\u9636TUCKER\u5206\u89e3\u7684\u5355\u9636\u6bb5\u8fed\u4ee3\u7b97\u6cd5\u3002\u5e76\u7ed9\u51fa\u5bf9\u53ef\u9760\u8054\u5408\u6062\u590d\u7684\u8fa8\u8bc6\u6027\u6761\u4ef6\u3002", "result": "\u4eff\u771f\u8868\u660e\u5728\u65e0\u8bad\u7ec3\u7684\u6761\u4ef6\u4e0b\uff0c\u6240\u63d0\u534a\u76f2\u65b9\u6848\u53ef\u5728\u9c81\u68d2\u6027\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u6709\u7ade\u4e89\u529b\u7684\u6743\u8861\uff0c\u4f18\u4e8e\u90e8\u5206\u57fa\u4e8e\u8bad\u7ec3\u7684\u4f20\u7edf\u65b9\u6848\uff0c\u4e14\u4e24\u79cd\u5b9e\u73b0\u5404\u81ea\u5177\u6709\u4e0d\u540c\u7684\u590d\u6742\u5ea6\u4e0e\u6536\u655b\u6027\u7279\u5f81\u3002", "conclusion": "\u534a\u76f2\u5f20\u91cf\u65b9\u6cd5\u4e3aBD-RIS\u573a\u666f\u63d0\u4f9b\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u901a\u9053\u4e0e\u7b26\u53f7\u4f30\u8ba1\u65b9\u6848\uff0c\u5728\u964d\u4f4e\u8bad\u7ec3\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u8f83\u597d\u4f30\u8ba1\u6027\u80fd\uff1b\u4e24\u79cd\u5b9e\u73b0\u63d0\u4f9b\u4e0d\u540c\u7684\u590d\u6742\u5ea6\u4e0e\u9c81\u68d2\u6027\u6743\u8861\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u6027\u8f83\u9ad8\u7684\u5e94\u7528\u3002"}}
{"id": "2512.14935", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14935", "abs": "https://arxiv.org/abs/2512.14935", "authors": ["Nnamdi Philip Okonkwo", "Lubna Luxmi Dhirani"], "title": "Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection", "comment": null, "summary": "Cloud Security Operations Center (SOC) enable cloud governance, risk and compliance by providing insights visibility and control. Cloud SOC triages high-volume, heterogeneous telemetry from elastic, short-lived resources while staying within tight budgets. In this research, we implement an AI-Augmented Security Operations Center (AISOC) on AWS that combines cloud-native instrumentation with ML-based detection. The architecture uses three Amazon EC2 instances: Attacker, Defender, and Monitoring. We simulate a reverse-shell intrusion with Metasploit, and Filebeat forwards Defender logs to an Elasticsearch and Kibana stack for analysis. We train two classifiers, a malware detector built on a public dataset and a log-anomaly detector trained on synthetically augmented logs that include adversarial variants. We calibrate and fuse the scores to produce multi-modal threat intelligence and triage activity into NORMAL, SUSPICIOUS, and HIGH\\_CONFIDENCE\\_ATTACK. On held-out tests the fusion achieves strong macro-F1 (up to 1.00) under controlled conditions, though performance will vary in noisier and more diverse environments. These results indicate that simple, calibrated fusion can enhance cloud SOC capabilities in constrained, cost-sensitive setups.", "AI": {"tldr": "\u5728AWS\u4e0a\u5b9e\u73b0\u7684AI\u589e\u5f3a\u4e91SOC\uff0c\u7ed3\u5408\u4e91\u539f\u751f\u76d1\u63a7\u4e0eML\u68c0\u6d4b\uff0c\u901a\u8fc7\u4e09\u53f0EC2\u5b9e\u4f8b\uff08Attacker\u3001Defender\u3001Monitoring\uff09\u5b9e\u73b0\u65e5\u5fd7\u6536\u96c6\u3001\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u548c\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u7684\u591a\u6a21\u6001\u5a01\u80c1\u60c5\u62a5\u878d\u5408\uff0c\u4ece\u800c\u5bf9\u6d3b\u52a8\u8fdb\u884cNORMAL/SUSPICIOUS/HIGH_CONFIDENCE_ATTACK\u7684\u4e09\u7ea7\u5206\u7ea7\uff1b\u5728\u53d7\u63a7\u6d4b\u8bd5\u4e2d\u5b8fF1\u6700\u9ad8\u53ef\u8fbe1.00\uff0c\u4f46\u5728\u5608\u6742\u73af\u5883\u4e0b\u6027\u80fd\u53ef\u80fd\u4e0b\u964d\u3002", "motivation": "\u4e91\u6cbb\u7406\u3001\u98ce\u9669\u4e0e\u5408\u89c4\u6a21\u5757\u9700\u8981\u5728\u9ad8\u5e76\u53d1\u3001\u6210\u672c\u654f\u611f\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u6001\u52bf\u611f\u77e5\u4e0e\u5feb\u901f\u5206\u7ea7\u5904\u7f6e\uff1b\u73b0\u6709\u5355\u6a21\u6001\u68c0\u6d4b\u5728\u4e91\u73af\u5883\u4e2d\u7684\u53ef\u7528\u6027\u4e0e\u6210\u672c\u6548\u76ca\u4ecd\u662f\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e09\u53f0EC2\uff08Attacker\u3001Defender\u3001Monitoring\uff09\uff0c\u5728AWS\u4e0a\u8fdb\u884c\u4eff\u771f\u5165\u4fb5\uff08\u5229\u7528Metasploit\u7684\u53cd\u5411shell\uff09\uff0cFilebeat\u5c06 Defender \u65e5\u5fd7\u9001\u5f80Elasticsearch/Kibana\uff1b\u8bad\u7ec3\u4e24\u4e2a\u5206\u7c7b\u5668\uff1a\u4e00\u4e2a\u57fa\u4e8e\u516c\u5f00\u6570\u636e\u96c6\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u5668\uff1b\u4e00\u4e2a\u57fa\u4e8e\u5408\u6210\u589e\u5f3a\u65e5\u5fd7\uff08\u542b\u5bf9\u6297\u53d8\u4f53\uff09\u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u5668\uff1b\u5bf9\u5206\u6570\u8fdb\u884c\u6821\u51c6\u4e0e\u878d\u5408\uff0c\u8f93\u51fa\u591a\u6a21\u6001\u5a01\u80c1\u60c5\u62a5\u5e76\u5206\u7ea7\u4e3a NORMAL\u3001SUSPICIOUS\u3001HIGH_CONFIDENCE_ATTACK\u3002", "result": "\u5728 held-out \u6d4b\u8bd5\u4e2d\uff0c\u878d\u5408\u7b56\u7565\u5b9e\u73b0\u4e86\u6781\u9ad8\u7684\u5b8f\u89c2F1\uff08\u6700\u9ad8\u8fbe\u52301.00\uff09\u5728\u53d7\u63a7\u6761\u4ef6\uff0c\u4f46\u5728\u566a\u58f0\u4e0e\u591a\u6837\u5316\u73af\u5883\u4e2d\u6027\u80fd\u53ef\u80fd\u6ce2\u52a8\u3002", "conclusion": "\u7b80\u5355\u4e14\u7ecf\u8fc7\u6821\u51c6\u7684\u878d\u5408\u7b56\u7565\u53ef\u4ee5\u5728\u53d7\u9650\u3001\u6210\u672c\u654f\u611f\u7684\u4e91SOC\u573a\u666f\u4e0b\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u4e0e\u54cd\u5e94\u6548\u7387\u3002"}}
{"id": "2512.15546", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15546", "abs": "https://arxiv.org/abs/2512.15546", "authors": ["Heedong Do", "Angel Lozano"], "title": "Optimum Discrete Beamforming via Minkowski Sum of Polygons", "comment": null, "summary": "This letter casts the problem of optimum discrete beamforming as the computation of the Minkowski sum of convex polygons, which is itself a convex polygon. The number of vertices of the latter is at most the sum of the number of vertices of the original polygons, enabling its efficient computation. This original and intuitive formulation confirms that the optimum beamforming solution can be found efficiently.", "AI": {"tldr": "\u5c06\u79bb\u6563\u6ce2\u675f\u6210\u5f62\u95ee\u9898\u8f6c\u5316\u4e3a\u8ba1\u7b97\u51f8\u591a\u8fb9\u5f62\u7684Minkowski\u548c\uff0c\u5176\u7ed3\u679c\u4ecd\u4e3a\u51f8\u591a\u8fb9\u5f62\uff1b\u65b0\u7ed3\u6784\u4fdd\u8bc1\u76ee\u6807\u591a\u8fb9\u5f62\u7684\u9876\u70b9\u6570\u4e0d\u8d85\u8fc7\u539f\u591a\u8fb9\u5f62\u9876\u70b9\u6570\u4e4b\u548c\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u89e3\u51b3\u79bb\u6563\u6ce2\u675f\u6210\u5f62\u4e2d\u7684\u79bb\u6563\u6027/\u7ec4\u5408\u6027\u96be\u9898\uff0c\u63d0\u4f9b\u4e00\u4e2a\u51e0\u4f55\u3001\u51f8\u6027\u89c6\u89d2\uff0c\u4f7f\u4f18\u5316\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u8fdb\u884c\u3002", "method": "\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u4e24\u4e2a\u6216\u591a\u4e2a\u51f8\u591a\u8fb9\u5f62\u7684Minkowski\u548c\u7684\u8ba1\u7b97\u95ee\u9898\uff1b\u5229\u7528\u8be5\u548c\u7684\u9876\u70b9\u6570\u4e0a\u754c\uff08\u4e0d\u8d85\u8fc7\u8f93\u5165\u9876\u70b9\u6570\u4e4b\u548c\uff09\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u8fb9\u5f62\u8fd0\u7b97\u4e0e\u6c42\u89e3\u3002", "result": "\u7ed9\u51fa\u4e00\u79cd\u9ad8\u6548\u6c42\u89e3\u79bb\u6563\u6ce2\u675f\u6210\u5f62\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u5728\u51f8\u51e0\u4f55\u89c6\u89d2\u4e0b\u53ef\u83b7\u5f97\u6700\u4f18\u89e3\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u53ef\u63a7\u3002", "conclusion": "Minkowski\u548c\u7684\u51f8\u51e0\u4f55\u8868\u8ff0\u63d0\u4f9b\u76f4\u89c2\u4e14\u8ba1\u7b97\u4e0a\u9ad8\u6548\u7684\u8def\u5f84\u6765\u6c42\u89e3\u6700\u4f18\u79bb\u6563\u6ce2\u675f\u6210\u5f62\u3002"}}
{"id": "2512.14727", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.14727", "abs": "https://arxiv.org/abs/2512.14727", "authors": ["Klaus-Rudolf Kladny", "Bernhard Sch\u00f6lkopf", "Lisa Koch", "Christian F. Baumgartner", "Michael Muehlebach"], "title": "A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications", "comment": null, "summary": "Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.", "AI": {"tldr": "\u7406\u8bba\u4fdd\u8bc1\u5bf9\u4efb\u610f\u5927\u5c0f\u7684\u6821\u51c6\u96c6\u6210\u7acb\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6821\u51c6\u96c6\u5927\u5c0f\u51b3\u5b9a\u4e86\u7f6e\u4fe1\u533a\u95f4\u7684\u5b9e\u7528\u6027\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u7684\u533b\u5b66\u9886\u57df\uff0c\u6b64\u4fdd\u8bc1\u7684\u5b9e\u9645\u6548\u7528\u53ef\u80fd\u53d7\u9650\uff1b\u7ecf\u9a8c\u7814\u7a76\u5728\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u652f\u6301\u8fd9\u4e00\u70b9\u3002", "motivation": "\u5728\u533b\u7597\u51b3\u7b56\u9700\u8981\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u80cc\u666f\u4e0b\uff0c\u533a\u4f4d\u9884\u6d4b\uff08Conformal Prediction, CP\uff09\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u56e0\u5176\u4e3a\u9884\u6d4b\u96c6\u5408\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\u3002\u5c3d\u7ba1\u7406\u8bba\u4e0a\u6821\u51c6\u96c6\u53ef\u4ee5\u4efb\u610f\u5927\u5c0f\uff0c\u4f46\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u666f\u4e0b\uff0c\u9700\u68c0\u9a8c\u89c4\u6a21\u5bf9\u5b9e\u9645\u6548\u7528\u7684\u5f71\u54cd\u3002", "method": "\u5bf9 CP \u4fdd\u8bc1\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u968f\u6821\u51c6\u96c6\u89c4\u6a21\u53d8\u5316\u7684\u5b9e\u9645\u6548\u7528\u4f9d\u8d56\u6027\uff1b\u5e76\u5728\u4e00\u4e2a\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u7ecf\u9a8c\u8bc1\u660e\uff0c\u6bd4\u8f83\u4e0d\u540c\u6821\u51c6\u96c6\u89c4\u6a21\u4e0b CP \u9884\u6d4b\u96c6\u5408\u7684\u8868\u73b0\u3002", "result": "\u5c3d\u7ba1\u7406\u8bba\u4fdd\u8bc1\u5bf9\u4efb\u610f\u5927\u5c0f\u7684\u6821\u51c6\u96c6\u6210\u7acb\uff0c\u4f46\u5b9e\u9645\u6709\u7528\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6821\u51c6\u96c6\u7684\u5927\u5c0f\uff1b\u5728\u6570\u636e\u7a00\u7f3a\u7684\u533b\u5b66\u573a\u666f\u4e2d\uff0c\u5c0f\u89c4\u6a21\u6821\u51c6\u96c6\u4f1a\u663e\u8457\u524a\u5f31 CP \u7684\u5b9e\u7528\u6027\uff0c\u4e14\u5b9e\u8bc1\u7ed3\u679c\u652f\u6301\u8be5\u7ed3\u8bba\u3002", "conclusion": "\u7814\u7a76\u63d0\u9192\u5728\u533b\u7597\u5e94\u7528\u4e2d\u4e0d\u8981\u53ea\u4f9d\u8d56 CP \u7684\u7406\u8bba\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\uff0c\u5e94\u5173\u6ce8\u6821\u51c6\u96c6\u89c4\u6a21\u5bf9\u5b9e\u9645\u6548\u7528\u7684\u5f71\u54cd\uff0c\u5fc5\u8981\u65f6\u7ed3\u5408\u5176\u4ed6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u6216\u589e\u5927\u6821\u51c6\u6570\u636e\u4ee5\u63d0\u5347\u9884\u6d4b\u96c6\u5408\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.15003", "categories": ["cs.CR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.15003", "abs": "https://arxiv.org/abs/2512.15003", "authors": ["Sogol Masoumzadeh", "Yufei Li", "Shane McIntosh", "D\u00e1niel Varr\u00f3", "Lili Wei"], "title": "SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports", "comment": "This is the author pre-print. The manuscript has been accepted for publication at SANER 2026!", "summary": "Monitoring issue tracker submissions is a crucial software maintenance activity. A key goal is the prioritization of high risk, security-related bugs. If such bugs can be recognized early, the risk of propagation to dependent products and endangerment of stakeholder benefits can be mitigated. To assist triage engineers with this task, several automatic detection techniques, from Machine Learning (ML) models to prompting Large Language Models (LLMs), have been proposed. Although promising to some extent, prior techniques often memorize lexical cues as decision shortcuts, yielding low detection rate specifically for more complex submissions. As such, these classifiers do not yet reach the practical expectations of a real-time detector of security-related issues. To address these limitations, we propose SEBERTIS, a framework to train Deep Neural Networks (DNNs) as classifiers independent of lexical cues, so that they can confidently detect fully unseen security-related issues. SEBERTIS capitalizes on fine-tuning bidirectional transformer architectures as Masked Language Models (MLMs) on a series of semantically equivalent vocabulary to prediction labels (which we call Semantic Surrogates) when they have been replaced with a mask. Our SEBERTIS-trained classifier achieves a 0.9880 F1-score in detecting security-related issues of a curated corpus of 10,000 GitHub issue reports, substantially outperforming state-of-the-art issue classifiers, with 14.44%-96.98%, 15.40%-93.07%, and 14.90%-94.72% higher detection precision, recall, and F1-score over ML-based baselines. Our classifier also substantially surpasses LLM baselines, with an improvement of 23.20%-63.71%, 36.68%-85.63%, and 39.49%-74.53% for precision, recall, and F1-score.", "AI": {"tldr": "\u63d0\u51faSEBERTIS\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8bed\u4e49\u66ff\u4ee3\u8bcd\u6c47\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff0c\u5b9e\u73b0\u5bf9\u672a\u89c1\u5b89\u5168\u76f8\u5173\u95ee\u9898\u7684\u9ad8\u6548\u68c0\u6d4b\uff0c\u5728GitHub\u95ee\u9898\u62a5\u544a\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.9880 F1\uff0c\u663e\u8457\u4f18\u4e8eML\u548cLLM\u57fa\u7ebf\u3002", "motivation": "\u76d1\u63a7\u95ee\u9898\u8ddf\u8e2a\u4e2d\u7684\u9ad8\u98ce\u9669\u3001\u5b89\u5168\u76f8\u5173\u7f3a\u9677\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\u9700\u8981\u5728\u65e9\u671f\u8bc6\u522b\uff1b\u73b0\u6709ML/LLM\u65b9\u6cd5\u5f80\u5f80\u8bb0\u5fc6\u8bcd\u6c47\u7ebf\u7d22\uff0c\u96be\u4ee5\u5bf9\u590d\u6742\u63d0\u4ea4\u5b9e\u73b0\u53ef\u9760\u68c0\u6d4b\uff0c\u73b0\u5b9e\u4e2d\u5bf9\u5b9e\u65f6\u68c0\u6d4b\u6709\u66f4\u9ad8\u8981\u6c42\u3002", "method": "\u5c06\u53cc\u5411\u53d8\u6362\u5668\u67b6\u6784\u4f5c\u4e3a\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u7528\u4e00\u7cfb\u5217\u8bed\u4e49\u7b49\u4ef7\u7684\u9884\u6d4b\u6807\u7b7e\u8bcd\u6c47\uff08\u8bed\u4e49\u66ff\u4ee3\u54c1\uff0cSemantic Surrogates\uff09\u5728\u88ab\u66ff\u6362\u4e3a\u63a9\u7801\u65f6\u8fdb\u884c\u9884\u6d4b\uff0c\u4ece\u800c\u8bad\u7ec3\u51fa\u4e0d\u4f9d\u8d56\u8bcd\u6c47\u7ebf\u7d22\u7684\u5206\u7c7b\u5668\uff0c\u80fd\u68c0\u6d4b\u5b8c\u5168\u672a\u89c1\u7684\u5b89\u5168\u76f8\u5173\u95ee\u9898\u3002", "result": "\u5728\u5305\u542b1\u4e07\u6761GitHub issue\u62a5\u544a\u7684\u8bed\u6599\u5e93\u4e0a\uff0cSEBERTIS\u8fbe\u52300.9880\u7684F1\u5206\u6570\uff1b\u76f8\u6bd4ML\u57fa\u7ebf\uff0c\u7cbe\u5ea6/\u53ec\u56de/ F1\u5206\u6570\u63d0\u5347\u7ea614.44%-96.98%\u300115.40%-93.07%\u300114.90%-94.72%\uff1b\u76f8\u6bd4LLM\u57fa\u7ebf\uff0c\u63d0\u5347\u7ea623.20%-63.71%\u300136.68%-85.63%\u300139.49%-74.53%\u3002", "conclusion": "SEBERTIS\u5c55\u793a\u4e86\u4e00\u4e2a\u4e0d\u4f9d\u8d56\u8bcd\u6c47\u7ebf\u7d22\u7684DNN\u5206\u7c7b\u6846\u67b6\uff0c\u80fd\u591f\u5728\u672a\u89c1\u5b89\u5168\u76f8\u5173\u95ee\u9898\u4e0a\u5b9e\u73b0\u9ad8\u68c0\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684ML\u548cLLM\u57fa\u7ebf\u3002"}}
{"id": "2512.14728", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14728", "abs": "https://arxiv.org/abs/2512.14728", "authors": ["Jie He", "Yong Qin", "Jianyuan Guo", "Xuan Sun", "Xuanchuan Zheng"], "title": "A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems", "comment": null, "summary": "Refined trajectory inference of urban rail transit is of great significance to the operation organization. In this paper, we develop a fully data-driven approach to inferring individual travel trajectories in urban rail transit systems. It utilizes data from the Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems to infer key trajectory elements, such as selected train, access/egress time, and transfer time. The approach includes establishing train alternative sets based on spatio-temporal constraints, data-driven adaptive trajectory inference, and trave l trajectory construction. To realize data-driven adaptive trajectory inference, a data-driven parameter estimation method based on KL divergence combined with EM algorithm (KLEM) was proposed. This method eliminates the reliance on external or survey data for parameter fitting, enhancing the robustness and applicability of the model. Furthermore, to overcome the limitations of using synthetic data to validate the result, this paper employs real individual travel trajectory data for verification. The results show that the approach developed in this paper can achieve high-precision passenger trajectory inference, with an accuracy rate of over 90% in urban rail transit travel trajectory inference during peak hours.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u57ce\u5e02\u8f68\u9053\u65c5\u5ba2\u8f68\u8ff9\u63a8\u65ad\u6846\u67b6\uff0c\u5229\u7528AFC\u4e0eAVL\u6570\u636e\u63a8\u65ad\u5355\u4e2a\u4e58\u5ba2\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7KLEM\uff08\u57fa\u4e8eKL\u6563\u5ea6\u7684EM\u53c2\u6570\u4f30\u8ba1\uff09\u5b9e\u73b0\u81ea\u9002\u5e94\u53c2\u6570\u5316\uff0c\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\uff0c\u5728\u9ad8\u5cf0\u65f6\u6bb5\u8fbe\u5230>90%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u6216\u8c03\u67e5\u95ee\u5377\uff0c\u7f3a\u4e4f\u65e0\u5916\u90e8\u6570\u636e\u7684\u9c81\u68d2\u6027\uff1b\u9700\u8981\u5c06\u591a\u6e90\u4ea4\u901a\u6570\u636e\u878d\u5408\u4ee5\u7cbe\u786e\u91cd\u6784\u4e2a\u4eba\u65c5\u884c\u8f68\u8ff9\uff0c\u5e76\u4ee5\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u4ee5\u63d0\u5347\u53ef\u4fe1\u5ea6\u3002", "method": "1) \u57fa\u4e8eAFC\u4e0eAVL\u6570\u636e\u5efa\u7acb\u65f6\u7a7a\u7ea6\u675f\u4e0b\u7684\u5217\u8f66\u66ff\u9009\u96c6\uff1b2) \u6570\u636e\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u8f68\u8ff9\u63a8\u65ad\u4e0e\u65c5\u7a0b\u6784\u9020\uff1b3) \u63d0\u51faKLEM\u65b9\u6cd5\uff08\u57fa\u4e8eKL\u6563\u5ea6\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u7ed3\u5408EM\u7b97\u6cd5\uff09\u4ee5\u65e0\u5916\u90e8\u6570\u636e\u8fdb\u884c\u53c2\u6570\u62df\u5408\uff1b4) \u4f7f\u7528\u771f\u5b9e\u4e2a\u4eba\u65c5\u884c\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u7ed3\u679c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u4e58\u5ba2\u8f68\u8ff9\u63a8\u65ad\uff0c\u5728\u9ad8\u5cf0\u65f6\u6bb5\u57ce\u5e02\u8f68\u9053\u65c5\u884c\u8f68\u8ff9\u63a8\u65ad\u7684\u51c6\u786e\u7387\u8d85\u8fc790%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u7701\u53bb\u4e86\u5bf9\u5916\u90e8\u6216\u8c03\u67e5\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u80fd\u63d0\u5347\u8f68\u9053\u8fd0\u8425\u7684\u51b3\u7b56\u652f\u6491\u80fd\u529b\u3002"}}
{"id": "2512.15229", "categories": ["cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15229", "abs": "https://arxiv.org/abs/2512.15229", "authors": ["Elio Gruttadauria", "Mathieu Fontaine", "Jonathan Le Roux", "Slim Essid"], "title": "O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization", "comment": null, "summary": "We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.", "AI": {"tldr": "\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5728\u7ebf\u8bf4\u8bdd\u4eba\u5206\u79bb/\u5bf9\u8bdd\u8005\u8bc6\u522b\u7cfb\u7edf\uff08O-EENC-SD\uff09\uff0c\u5c06 EEND-EDA \u4e0e\u57fa\u4e8eRNN\u7684\u5728\u7ebf\u62fc\u63a5\u6a21\u5757\u548c\u8d28\u5fc3\u7ec6\u5316\u89e3\u7801\u5668\u7ed3\u5408\u5728\u4e00\u8d77\uff0c\u5b9e\u73b0\u65e0\u9700\u8d85\u53c2\u6570\u7684\u9ad8\u6548\u5728\u7ebf\u9884\u6d4b\uff0c\u5728CallHome\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u8fd1\u7684\u4e24\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u8bed\u97f3\u5206\u79bb\u6027\u80fd\uff0c\u540c\u65f6\u5728\u590d\u6742\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5177\u6709\u66f4\u597d\u7684\u6743\u8861\u3002", "motivation": "\u5728\u7ebf\u8bf4\u8bdd\u4eba\u5206\u79bb\u901a\u5e38\u4f9d\u8d56\u805a\u7c7b\u7b49\u79bb\u7ebf\u7b56\u7565\u6216\u4ee3\u4ef7\u9ad8\u6602\u7684\u5728\u7ebf\u7aef\u5230\u7aef\u65b9\u6cd5\u3002\u9700\u8981\u4e00\u4e2a\u65e0\u9700\u8d85\u53c2\u6570\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u80fd\u5355-pass \u7684\u5728\u7ebf\u8bf4\u8bdd\u4eba\u5206\u79bb\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u4e24\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u7535\u8bdd\u8bed\u97f3\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e EEND-EDA \u7684\u7aef\u5230\u7aef\u5728\u7ebf\u8bf4\u8bdd\u4eba\u5206\u79bb\u7cfb\u7edf O-EENC-SD\uff0c\u65b0\u589e\u57fa\u4e8eRNN\u7684\u5728\u7ebf\u62fc\u63a5\u673a\u5236\u4ee5\u5b9e\u73b0\u5728\u7ebf\u9884\u6d4b\uff1b\u5f15\u5165\u8d28\u5fc3\u7ec6\u5316\u89e3\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff1b\u4e0e\u805a\u7c7b\u57fa\u7ebf\u53ca\u73b0\u6709\u5728\u7ebf\u7aef\u5230\u7aef\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u5728\u4e24\u8bf4\u8bdd\u4eba\u901a\u8baf\u7535\u8bdd\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002", "result": "\u5728CallHome\u4e24\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u7535\u8bdd\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\uff0cO-EENC-SD \u7684 DER \u4e0e\u5f53\u524d\u65b9\u6cd5\u7684\u6027\u80fd\u76f8\u5f53\uff1b\u5728DER\u4e0e\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u6743\u8861\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u91cd\u53e0\u7684\u72ec\u7acb\u7247\u6bb5\u4e0b\u4e5f\u8868\u73b0\u51fa\u6781\u9ad8\u7684\u63a8\u7406\u6548\u7387\uff0c\u663e\u793a\u51fa\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4f18\u7684\u6548\u7387\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684 O-EENC-SD \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5728\u7ebf\u8bf4\u8bdd\u4eba\u5206\u79bb\u65b9\u6848\uff0c\u8d28\u5fc3\u7ec6\u5316\u89e3\u7801\u5668\u548c\u5728\u7ebf\u62fc\u63a5\u673a\u5236\u4f7f\u5728\u7ebf\u9884\u6d4b\u9ad8\u6548\u4e14\u6027\u80fd\u7ade\u4e89\u529b\u5f3a\uff1b\u76f8\u6bd4\u9700\u5927\u91cf\u8d85\u53c2\u6570\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u8be5\u7cfb\u7edf\u5177\u5907\u660e\u663e\u7684\u8d85\u53c2\u6570\u81ea\u7531\u4f18\u52bf\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u5f02\u7684DER\u4e0e\u8d44\u6e90\u5f00\u9500\u4e4b\u95f4\u7684\u6298\u8877\u3002"}}
{"id": "2512.14731", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14731", "abs": "https://arxiv.org/abs/2512.14731", "authors": ["Nikit Phadke"], "title": "Semantic Geometry for policy-constrained interpretation", "comment": null, "summary": "We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4ee5\u5355\u4f4d\u7403\u9762\u4e3a\u51e0\u4f55\u8868\u8fbe\u7684\u7b56\u7565\u7ea6\u675f\u8bed\u4e49\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u610f\u4e49\u8868\u793a\u4e3a\u65b9\u5411\u3001\u8bc1\u636e\u4e3a\u5411\u91cf\u96c6\u5408\u3001\u53ef\u63a5\u53d7\u89e3\u91ca\u4e3a\u7403\u9762\u51f8\u57df\uff0c\u653f\u7b56\u7ea6\u675f\u4f5c\u4e3a\u5148\u9a8c\u5b9a\u4e49\uff0c\u4e0e\u8bc1\u636e\u51e0\u4f55\u5206\u79bb\uff1b\u89e3\u91ca\u901a\u8fc7\u5e26\u7ea6\u675f\u7684\u4f18\u5316\u5b9e\u73b0\uff0c\u82e5\u51fa\u73b0\u77db\u76fe\u6216\u653f\u7b56\u6392\u9664\u5219\u81ea\u7136\u800c\u7136\u62d2\u7edd\uff1b\u5e76\u4e0e\u4fe1\u606f\u7406\u8bba\u3001\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u5c42\u53e0\u8bed\u4e49\u76f8\u5173\u8054\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u53d7\u76d1\u7ba1\u91d1\u878d\u6570\u636e\u4e0a\u5b9e\u73b0\u96f6\u5e7b\u89c9\u6279\u51c6\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669/\u9ad8\u4ee3\u4ef7\u573a\u666f\u4e2d\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u6027\uff0c\u907f\u514d\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u6027\u627f\u8bfa\uff1b\u901a\u8fc7\u5c06\u4fe1\u606f\u7406\u8bba\u3001\u8d1d\u53f6\u65af\u63a8\u65ad\u4e0e\u5c42\u53e0\u8bed\u4e49\u7ed3\u5408\uff0c\u5efa\u7acb\u4e25\u683c\u7684\u754c\u9650\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u8bed\u4e49\u7f16\u7801\u4e3a\u5355\u4f4d\u7403\u9762\u7684\u65b9\u5411\uff0c\u8bc1\u636e\u4ee5 witness \u5411\u91cf\u96c6\u5408\u8868\u793a\uff0c\u53ef\u63a5\u53d7\u89e3\u91ca\u5bf9\u5e94\u7403\u9762\u51f8\u57df\u3002\u653f\u7b56\u7ea6\u675f\u4f5c\u4e3a\u540c\u4e00\u6d41\u5f62\u4e0a\u7684\u663e\u5f0f\u5148\u9a8c\uff0c\u4e0e\u8bc1\u636e\u51e0\u4f55\u5206\u79bb\u3002\u89e3\u91ca\u95ee\u9898\u8f6c\u5316\u4e3a\u5bf9\u53ef\u63a5\u53d7\u533a\u57df\u7684\u5e26\u7ea6\u675f\u4f18\u5316\uff1b\u82e5\u51fa\u73b0\u77db\u76fe\u6216\u653f\u7b56\u6392\u9664\uff0c\u62d2\u7edd\u6210\u4e3a\u62d3\u6251\u4e0a\u5fc5\u7136\u7ed3\u679c\u3002\u8fdb\u4e00\u6b65\u7ed9\u51fa\u4e0e\u4fe1\u606f\u7406\u8bba\u3001\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u5c42\u53e0\u8bed\u4e49\u7684\u8054\u7cfb\uff0c\u5e76\u8bc1\u660e\u590d\u6742\u5ea6\u8fb9\u754c\u5728\u4fe1\u606f\u8bba\u610f\u4e49\u4e0a\u662f\u6700\u4f18\u7684\u3002", "result": "\u5728\u5927\u89c4\u6a21\u53d7\u76d1\u7ba1\u91d1\u878d\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5bf9\u591a\u79cd\u653f\u7b56\u5236\u5ea6\u5b9e\u73b0\u96f6\u5e7b\u89c9\u6279\u51c6\uff0c\u6807\u5fd7\u7740\u8fd9\u4e00\u9886\u57df\u7684\u5927\u89c4\u6a21\u6210\u529f\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u653f\u7b56\u5148\u9a8c\u4e0e\u8bc1\u636e\u51e0\u4f55\u5206\u79bb\uff0c\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u65e0\u5e7b\u89c9\u89e3\u91ca\u80fd\u529b\uff0c\u5e76\u4e14\u5176\u590d\u6742\u5ea6\u8fbe\u5230\u4fe1\u606f\u8bba\u4e0a\u7684\u6700\u4f18\uff0c\u62d3\u5c55\u4e86\u9ad8\u98ce\u9669\u9886\u57df\u8bed\u4e49\u89e3\u91ca\u7684\u5b89\u5168\u6027\u4e0e\u7a33\u5065\u6027\u7814\u7a76\u3002"}}
{"id": "2512.15344", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15344", "abs": "https://arxiv.org/abs/2512.15344", "authors": ["Hiroyoshi Nagahama", "Katsufumi Inoue", "Masayoshi Todorokihara", "Michifumi Yoshioka"], "title": "Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\\% accuracy (+5.4\\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u76f8\u4f4d\u611f\u77e5\u9884\u5904\u7406\u7b56\u7565\u4ee5\u5bf9\u591a\u8f74\u632f\u52a8\u6570\u636e\u4e2d\u7684\u968f\u673a\u76f8\u4f4d\u53d8\u5316\u8fdb\u884c\u5bf9\u9f50\uff0c\u5e76\u5728\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u548c\u516d\u79cd\u6df1\u5ea6\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5355\u8f74\u53c2\u8003\u76f8\u4f4d\u7b56\u7565\u4e0b\u4fdd\u7559\u8f74\u95f4\u5173\u7cfb\uff0c\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u5728\u65cb\u8f6c\u673a\u68b0\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\uff0c\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u5e38\u5ffd\u7565/\u9519\u7528\u76f8\u4f4d\u4fe1\u606f\uff0c\u53ef\u80fd\u5f71\u54cd\u5206\u7c7b/\u8bca\u65ad\u6027\u80fd\u3002\u901a\u8fc7\u5f15\u5165\u76f8\u4f4d\u5bf9\u9f50\u6765\u5229\u7528\u76f8\u4f4d\u4fe1\u606f\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u8f74\u72ec\u7acb\u76f8\u4f4d\u8c03\u6574\uff08\u9010\u8f74\u5bf9\u9f50\u5230\u96f6\u76f8\u4f4d\uff09\u548c\u5355\u8f74\u53c2\u8003\u76f8\u4f4d\u8c03\u6574\uff08\u5bf9\u6574\u4e2a\u4e09\u8f74\u7edf\u4e00\u65f6\u95f4\u504f\u79fb\u4ee5\u4fdd\u7559\u76f8\u4f4d\u5173\u7cfb\uff09\uff1b\u5728\u65b0\u6784\u5efa\u7684\u5177\u540c\u6b65\u4e09\u8f74\u4f20\u611f\u5668\u7684\u8f6c\u5b50\u6570\u636e\u96c6\u4e0a\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u5e76\u5bf9\u516d\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e09\u8f74\u72ec\u7acb\u76f8\u4f4d\u65b9\u6cd5\u5bf9Transformer\u7b49\u6a21\u578b\u5e26\u6765\u7a33\u5b9a\u63d0\u5347\uff08+2.7%\uff09\uff0c\u5355\u8f74\u53c2\u8003\u76f8\u4f4d\u65b9\u6cd5\u5728\u4fdd\u7559\u7a7a\u95f4\u76f8\u4f4d\u5173\u7cfb\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u6700\u597d\u6027\u80fd\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u63d0\u5347\u8fbe96.2%\uff08+5.4%\uff09\u3002", "conclusion": "\u4e24\u79cd\u76f8\u4f4d\u5bf9\u9f50\u7b56\u7565\u5747\u4e3a\u9884\u6d4b\u6027\u7ef4\u62a4\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6539\u8fdb\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5728\u591a\u8f74\u632f\u52a8\u5206\u6790\u4e2d\u654f\u611f\u76f8\u4f4d\u4fe1\u606f\u7684\u573a\u666f\u3002"}}
{"id": "2512.15385", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.15385", "abs": "https://arxiv.org/abs/2512.15385", "authors": ["Julian Oelhaf", "Mehran Pashaei", "Georg Kordowich", "Christian Bergler", "Andreas Maier", "Johann J\u00e4ger", "Siming Bayer"], "title": "Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection", "comment": "This paper is a postprint of a paper submitted to and accepted for publication in the 20th IET International Conference on Developments in Power System Protection (DPSP Global 2026) and is subject to Institution of Engineering and Technology Copyright. The copy of record is available at the IET Digital Library", "summary": "The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.\n  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.15039", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.15039", "abs": "https://arxiv.org/abs/2512.15039", "authors": ["Zhenhao Yin", "Hanbing Yan", "Huishu Lu", "Jing Xiong", "Xiangyu Li", "Rui Mei", "Tianning Zang"], "title": "APT-ClaritySet: A Large-Scale, High-Fidelity Labeled Dataset for APT Malware with Alias Normalization and Graph-Based Deduplication", "comment": "13 pages, 11 figures", "summary": "Large-scale, standardized datasets for Advanced Persistent Threat (APT) research are scarce, and inconsistent actor aliases and redundant samples hinder reproducibility. This paper presents APT-ClaritySet and its construction pipeline that normalizes threat actor aliases (reconciling approximately 11.22\\% of inconsistent names) and applies graph-feature deduplication -- reducing the subset of statically analyzable executables by 47.55\\% while retaining behaviorally distinct variants. APT-ClaritySet comprises: (i) APT-ClaritySet-Full, the complete pre-deduplication collection with 34{,}363 malware samples attributed to 305 APT groups (2006 - early 2025); (ii) APT-ClaritySet-Unique, the deduplicated release with 25{,}923 unique samples spanning 303 groups and standardized attributions; and (iii) APT-ClaritySet-FuncReuse, a function-level resource that includes 324{,}538 function-reuse clusters (FRCs) enabling measurement of inter-/intra-group sharing, evolution, and tooling lineage. By releasing these components and detailing the alias normalization and scalable deduplication pipeline, this work provides a high-fidelity, reproducible foundation for quantitative studies of APT patterns, evolution, and attribution.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.14734", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14734", "abs": "https://arxiv.org/abs/2512.14734", "authors": ["Qiang Chen", "Venkatesh Ganapati Hegde", "Hongfei Li"], "title": "Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness", "comment": "3rd IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things, September 06-07, 2025, Central Michigan University, USA", "summary": "Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.15081", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15081", "abs": "https://arxiv.org/abs/2512.15081", "authors": ["Richard Helder Moulton", "Austin O'Brien", "John D. Hastings"], "title": "Quantifying Return on Security Controls in LLM Systems", "comment": "13 pages, 9 figures, 3 tables", "summary": "Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).", "AI": {"tldr": "\u4e3aLLM\u5b89\u5168\u5de5\u4f5c\u6d41\u63d0\u51fa\u4e00\u4e2a\u4ee5\u51b3\u7b56\u4e3a\u5bfc\u5411\u7684\u6846\u67b6\uff0c\u91cf\u5316\u5269\u4f59\u98ce\u9669\u3001\u5c06\u5bf9\u6297\u6027\u63a2\u6d4b\u8f6c\u5316\u4e3a\u8d22\u52a1\u98ce\u9669\u4e0e\u63a7\u5236\u6536\u76ca\u7387\uff08RoC\uff09\uff0c\u5e76\u53ef\u5bf9\u5206\u5c42\u9632\u62a4\u8fdb\u884c\u8d27\u5e01\u5316\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u7ed9\u5b9aRAG\u7cfb\u7edf\u4e0e\u653b\u51fb\u573a\u666f\u4e0b\uff0c\u57fa\u7ebf\u98ce\u9669\u6781\u9ad8\uff0cABAC\u548cNER redaction\u663e\u8457\u964d\u4f4e\u635f\u5931\u5e76\u63d0\u5347RoC\uff0cNeMo Guardrails \u6548\u679c\u6709\u9650\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5b9a\u91cf\u5316\u7684\u9632\u62a4\u8bc4\u4f30\u6307\u5357\u6765\u5e2e\u52a9\u5b9e\u8df5\u8005\u5728LLM\u5b89\u5168\u5de5\u4f5c\u6d41\u4e2d\u505a\u51fa\u6027\u4ef7\u6bd4\u51b3\u7b56\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u5b66\uff0c\u901a\u8fc7\u628a\u653b\u51fb\u6210\u8d25\u8f6c\u5316\u4e3a\u7ecf\u6d4e\u635f\u5931\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u4eff\u771f\u4e0e\u516c\u5f00 breach-cost \u6570\u636e\uff0c\u6765\u6bd4\u8f83\u4e0d\u540c\u9632\u62a4\u7684\u6027\u4ef7\u6bd4\u3002", "method": "\u5efa\u7acb\u4e00\u4e2a\u4ee5\u51b3\u7b56\u4e3a\u5bfc\u5411\u7684\u98ce\u9669\u91cf\u5316\u6846\u67b6\uff0c\u4f7f\u7528Laplace\u7684\u6210\u529f\u5b9a\u7406\u4f30\u8ba1\u653b\u51fb\u6210\u529f\u6982\u7387\uff0c\u7ed3\u5408\u635f\u5931\u4e09\u89d2\u5f62\u5206\u5e03\uff0c\u8fdb\u884c10,000\u6b21\u8499\u7279\u5361\u6d1b\u4eff\u771f\uff0c\u5f97\u5230\u635f\u5931\u5206\u5e03\u548c\u671f\u671b\u635f\u5931\u3002\u4ee5DeepSeek-R1\u4e0a\u7684RAG\u670d\u52a1\u5bf9\u5305\u542b\u5408\u6210PII\u7684\u8bed\u6599\u8fdb\u884c\u81ea\u6d3d\u653b\u51fb\uff0c\u8986\u76d6PII\u6cc4\u9732\u3001\u6f5c\u5728\u4e0a\u4e0b\u6587\u6ce8\u5165\u3001\u63d0\u793a\u6ce8\u5165\u3001\u5bf9\u6297\u6027\u751f\u6210\u548c\u53d1\u6563\u7b49\u4e94\u7c7b\u6f0f\u6d1e\u3002\u6bd4\u8f83\u4e09\u79cd\u9632\u62a4\uff1aABAC\u3001Microsoft Presidio\u7684NER\u7ea2act\u3001\u4ee5\u53caNeMo Guardrails\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebfRAG\u914d\u7f6e\u3002", "result": "\u57fa\u7ebf\u7cfb\u7edf\u5728PII\u3001\u6f5c\u5728\u6ce8\u5165\u3001\u63d0\u793a\u6ce8\u5165\u7b49\u653b\u51fb\u4e0a\u653b\u51fb\u6210\u529f\u7387\u5f88\u9ad8\uff08\u22650.98\uff09\uff0c\u6bcf\u6b21\u653b\u51fb\u573a\u666f\u7684\u603b\u671f\u671b\u635f\u5931\u7ea6\u4e3a313,000\u7f8e\u5143\u3002ABAC\u5c06PII\u548c\u63d0\u793a\u76f8\u5173\u653b\u51fb\u7684\u6210\u529f\u6982\u7387\u964d\u81f3\u8fd1\u96f6\uff0c\u603b\u635f\u5931\u964d\u4f4e\u7ea694%\uff0cRoC\u4e3a9.83\u3002NER redaction\u540c\u6837\u6d88\u9664\u4e86PII\u6cc4\u9732\uff0cRoC\u4e3a5.97\uff1b\u800cNeMo Guardrails\u6536\u76ca\u6709\u9650\uff0cRoC\u4ec5\u4e3a0.05\u3002", "conclusion": "\u5728LLM\u57fa\u7840\u8bbe\u65bd\u4e2d\uff0c\u7ed3\u5408ABAC\u548cNER redaction\u7b49\u9632\u62a4\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u8d22\u52a1\u98ce\u9669\u5e76\u63d0\u5347RoC\uff0c\u4f7f\u8d27\u5e01\u5316\u5bf9\u6bd4\u6210\u4e3a\u53ef\u884c\u7684\u98ce\u9669\u7ba1\u7406\u5de5\u5177\uff1b\u4e0d\u540c\u9632\u62a4\u7684\u6027\u4ef7\u6bd4\u5dee\u5f02\u660e\u663e\uff0cNeMo Guardrails\u7684\u6295\u8d44\u56de\u62a5\u7387\u8f83\u4f4e\u3002"}}
{"id": "2512.14738", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14738", "abs": "https://arxiv.org/abs/2512.14738", "authors": ["Zhengxu Yan", "Han Li", "Yuming Feng"], "title": "NoveltyRank: Estimating Conceptual Novelty of AI Papers", "comment": null, "summary": "With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u7279\u5f81\u7684AI\u8bba\u6587\u6982\u5ff5\u65b0\u9896\u5ea6\u8bc4\u4f30\u4e0e\u6392\u5e8f\u6a21\u578b\uff0c\u5229\u7528\u8bba\u6587\u9898/\u6458\u8981\u53ca\u4e0e\u5df2\u6709\u6587\u732e\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u805a\u7126\u4e8c\u5143\u5206\u7c7b\u4e0e\u6210\u5bf9\u6bd4\u8f83\u4e24\u79cd\u4efb\u52a1\uff0c\u5fae\u8c03Qwen3-4B-Instruct-2507\u4e0eSciBERT\u5e76\u4e0eGPT-5.1\u5bf9\u6bd4\uff0c\u516c\u5f00\u4ee3\u7801\u3002", "motivation": "\u5e94\u5bf9AI\u9886\u57df\u8bba\u6587\u6570\u91cf\u5267\u589e\u4e0e\u5feb\u901f\u8fed\u4ee3\uff0c\u4eba\u5de5\u65b0\u9896\u6027\u8bc4\u4f30\u7f3a\u4e4f\u4e00\u81f4\u6027\u4e14\u8017\u65f6\uff1b\u9700\u8981\u53ef\u6269\u5c55\u3001\u6570\u636e\u9a71\u52a8\u7684\u91cf\u5316\u65b0\u9896\u6027\u4fe1\u53f7\uff0c\u8f85\u52a9\u4f5c\u8005\u4e0e\u8bc4\u5ba1\u3002", "method": "\u4ee5\u8bba\u6587\u9898\u76ee\u548c\u6458\u8981\u53ca\u5176\u4e0e\u5148\u524d\u6587\u732e\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4e3a\u6838\u5fc3\u7279\u5f81\uff0c\u8bbe\u8ba1\u4e24\u7c7b\u4efb\u52a1\uff1a1) \u4e8c\u5143\u5206\u7c7b\uff0c\u9884\u6d4b\u8bba\u6587\u7684\u7edd\u5bf9\u65b0\u9896\u6027\uff1b2) \u6210\u5bf9\u6bd4\u8f83\uff0c\u5224\u65ad\u76f8\u5bf9\u65b0\u9896\u6027\u3002\u5bf9Qwen3-4B-Instruct-2507\u4e0eSciBERT\u8fdb\u884c\u5fae\u8c03\uff0c\u5728\u8fd9\u4e24\u79cd\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0eGPT-5.1\u8fdb\u884c\u5bf9\u6bd4\u3002\u5b9e\u73b0\u548c\u6570\u636e\u53ef\u516c\u5f00\u83b7\u53d6\uff0c\u4ee3\u7801\u516c\u5f00\u5728GitHub\u3002", "result": "\u539f\u6587\u6458\u8981\u672a\u7ed9\u51fa\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff1b\u62a5\u544a\u4e86\u5bf9\u4e24\u79cd\u4efb\u52a1\u8bbe\u5b9a\u53ca\u5efa\u6a21\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u7684\u57fa\u7ebf\u5206\u6790\uff0c\u5e76\u5bf9\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u4f46\u672a\u5728\u6458\u8981\u4e2d\u7ed9\u51fa\u91cf\u5316\u6307\u6807\u3002\u5b8c\u6574\u7ed3\u679c\u53ca\u5b9e\u9a8c\u7ec6\u8282\u5c06\u5728\u5168\u6587\u4e2d\u7ed9\u51fa\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u8bba\u6587\u65b0\u9896\u6027\u4fe1\u53f7\u6765\u6e90\uff0c\u80fd\u5e2e\u52a9\u63d0\u5347\u6295\u7a3f\u7b5b\u9009\u4e0e\u8bc4\u5ba1\u4e2d\u7684\u65b0\u9896\u6027\u5224\u65ad\u7684\u4e00\u81f4\u6027\u4e0e\u6548\u7387\uff1b\u5e76\u4e14\u63d0\u4f9b\u5f00\u6e90\u5b9e\u73b0\u4ee5\u4fbf\u590d\u73b0\u4e0e\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2512.15144", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.15144", "abs": "https://arxiv.org/abs/2512.15144", "authors": ["Mengying Wu", "Pei Chen", "Geng Hong", "Aichao An", "Jinsong Chen", "Binwang Wan", "Xudong Pan", "Jiarun Dai", "Min Yang"], "title": "MCPZoo: A Large-Scale Dataset of Runnable Model Context Protocol Servers for AI Agent", "comment": null, "summary": "Model Context Protocol (MCP) enables agents to interact with external tools, yet empirical research on MCP is hindered by the lack of large-scale, accessible datasets. We present MCPZoo, the largest and most comprehensive dataset of MCP servers collected from multiple public sources, comprising 90,146 servers. MCPZoo includes over ten thousand server instances that have been deployed and verified as runnable and interactable, supporting realistic experimentation beyond static analysis. The dataset provides unified metadata and access interfaces, enabling systematic exploration and interaction without manual deployment effort. MCPZoo is released as an open and accessible resource to support research on MCP-based security analysis.", "AI": {"tldr": "MCPZoo \u662f\u4e00\u4e2a\u89c4\u6a21\u5316\u7684 MCP \u670d\u52a1\u5668\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea6 90,146 \u53f0\u670d\u52a1\u5668\u548c\u8d85\u8fc7 1 \u4e07\u4e2a\u53ef\u8fd0\u884c\u5b9e\u4f8b\uff0c\u5f00\u653e\u83b7\u53d6\u4ee5\u652f\u6301 MCP \u7684\u5b89\u5168\u5206\u6790\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684 MCP \u7814\u7a76\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u6613\u83b7\u5f97\u7684\u670d\u52a1\u5668\u6570\u636e\u96c6\u800c\u96be\u4ee5\u8fdb\u884c\u73b0\u5b9e\u4e16\u754c\u7684\u4ea4\u4e92\u548c\u57fa\u51c6\u8bc4\u4f30\u3002", "method": "\u4ece\u591a\u5904\u516c\u5f00\u6765\u6e90\u6536\u96c6 MCP \u670d\u52a1\u5668\uff0c\u7b5b\u9009\u5e76\u9a8c\u8bc1\u5176\u53ef\u8fd0\u884c\u6027\u4e0e\u53ef\u4ea4\u4e92\u6027\uff0c\u6574\u7406\u7edf\u4e00\u7684\u5143\u6570\u636e\u5e76\u63d0\u4f9b\u7edf\u4e00\u7684\u8bbf\u95ee\u63a5\u53e3\uff1b\u6700\u7ec8\u5f62\u6210\u5305\u542b 90,146 \u53f0\u670d\u52a1\u5668\u7684\u96c6\u5408\uff0c\u4e14\u6709\u8d85\u8fc7\u4e00\u4e07\u5957\u53ef\u8fd0\u884c\u5b9e\u4f8b\u3002", "result": "\u516c\u5e03 MCPZoo \u4f5c\u4e3a\u5f00\u6e90\u3001\u53ef\u8bbf\u95ee\u7684\u8d44\u6e90\uff0c\u5141\u8bb8\u5728\u65e0\u9700\u672c\u5730\u90e8\u7f72\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u7cfb\u7edf\u6027\u63a2\u7d22\u548c\u4e92\u52a8\uff0c\u63d0\u5347\u5bf9 MCP \u7684\u5b89\u5168\u5206\u6790\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5c06\u663e\u8457\u4fc3\u8fdb\u57fa\u4e8e MCP \u7684\u5b89\u5168\u5206\u6790\u7814\u7a76\uff0c\u63a8\u52a8\u8bc4\u4f30\u3001\u5bf9\u6bd4\u4e0e\u590d\u73b0\u6027\u7684\u63d0\u5347\uff0c\u5e76\u4e3a MCP \u751f\u6001\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u8d44\u6e90\u3002"}}
{"id": "2512.14765", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14765", "abs": "https://arxiv.org/abs/2512.14765", "authors": ["Justin Jung"], "title": "Guided Discrete Diffusion for Constraint Satisfaction Problems", "comment": "Originally published in Jan 2025 on the SpringtailAI Blog", "summary": "We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.", "AI": {"tldr": "\u79bb\u6563\u6269\u6563\u5f15\u5bfc\u7528\u4e8e CSPs\uff0c\u80fd\u591f\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u89e3\u51b3\u6570\u72ec\u3002", "motivation": "CSP\u6c42\u89e3\u5e38\u53d7\u9650\u4e8e\u663e\u5f0f\u7ea6\u675f\u548c\u76d1\u7763\u4fe1\u53f7\uff1b\u5f15\u5165\u79bb\u6563\u6269\u6563\u8fc7\u7a0b\u53ef\u80fd\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u641c\u7d22\u548c\u53bb\u566a\u80fd\u529b\u3002", "method": "\u6784\u5efa\u79bb\u6563\u6269\u6563\u5f15\u5bfc\u6846\u67b6\uff0c\u5c06\u6269\u6563/\u53bb\u566a\u8fc7\u7a0b\u5e94\u7528\u4e8e CSP \u7684\u89e3\u7a7a\u95f4\uff0c\u4ee5\u6570\u72ec\u4e3a\u4f8b\u8fdb\u884c\u65e0\u76d1\u7763\u6c42\u89e3\u6f14\u793a\u3002", "result": "\u5728\u6570\u72ec\u7b49 CSP \u5b9e\u4f8b\u4e0a\uff0c\u5c55\u793a\u65e0\u76d1\u7763\u6c42\u89e3\u80fd\u529b\uff0c\u5e76\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u6210\u529f\u7387\u4e0e\u6536\u655b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a CSP \u63d0\u4f9b\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u65b0\u7684\u6c42\u89e3\u8def\u7ebf\uff0c\u6570\u72ec\u5b9e\u9a8c\u8868\u660e\u6f5c\u529b\uff0c\u540e\u7eed\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u79bb\u6563\u7ea6\u675f\u95ee\u9898\u3002"}}
{"id": "2512.14779", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.14779", "abs": "https://arxiv.org/abs/2512.14779", "authors": ["Kornelius Raeth", "Nicole Ludwig"], "title": "Evaluating Weather Forecasts from a Decision Maker's Perspective", "comment": null, "summary": "Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.", "AI": {"tldr": "\u51b3\u7b56\u5c42\u6821\u51c6\u63ed\u793a\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u672a\u5fc5\u7b49\u540c\u4e8e\u51b3\u7b56\u6027\u80fd\uff1b\u4e0d\u540c\u4efb\u52a1\u4e0b\u673a\u5668\u5b66\u4e60\u548c\u7ecf\u5178\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7684\u6392\u540d\u53ef\u80fd\u4e0d\u540c\uff0c\u9700\u4ee5\u51b3\u7b56\u5f71\u54cd\u4e3a\u5bfc\u5411\u8fdb\u884c\u6a21\u578b\u9009\u62e9\u3002", "motivation": "\u5c06 forecast \u8bc4\u4f30\u4ece\u5355\u7eaf\u9884\u6d4b\u51c6\u786e\u6027\u8f6c\u5411\u5bf9\u51b3\u7b56\u7ed3\u679c\u7684\u5f71\u54cd\u8bc4\u4f30\uff0c\u89e3\u51b3\u201c\u597d\u9884\u6d4b\u4e0d\u4e00\u5b9a\u6709\u597d\u51b3\u7b56\u201d\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u51b3\u7b56\u6821\u51c6\u6846\u67b6\uff0c\u5728\u591a\u79cd\u5929\u6c14\u76f8\u5173\u51b3\u7b56\u4efb\u52a1\u4e2d\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u7ecf\u5178\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e24\u8005\u5728\u5b9e\u9645\u51b3\u7b56\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u9884\u6d4b\u5c42\u9762\u7684\u6539\u8fdb\u662f\u5426\u8f6c\u5316\u4e3a\u51b3\u7b56\u5c42\u9762\u7684\u6536\u76ca\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u9884\u6d4b\u9636\u6bb5\u7684\u6027\u80fd\u63d0\u5347\u5e76\u4e0d\u603b\u662f\u8f6c\u5316\u4e3a\u51b3\u7b56\u5c42\u9762\u7684\u6027\u80fd\u63d0\u5347\uff1b\u67d0\u4e9b\u5dee\u5f02\u4ec5\u5728\u51b3\u7b56\u5c42\u663e\u73b0\uff0c\u4e14\u4e0d\u540c\u51b3\u7b56\u4efb\u52a1\u4e0b\u7684\u6a21\u578b\u6392\u540d\u53ef\u80fd\u53d1\u751f\u53d8\u5316\u3002", "conclusion": "\u5355\u7eaf\u7684\u9884\u6d4b\u7b49\u7ea7\u8bc4\u4f30\u4e0d\u8db3\u4ee5\u5728\u7279\u5b9a\u51b3\u7b56\u4efb\u52a1\u4e2d\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u5e94\u5728\u6a21\u578b\u8bc4\u4f30\u4e2d\u7eb3\u5165\u51b3\u7b56\u7ea7\u522b\u7684\u6307\u6807\u4e0e\u6821\u51c6\uff0c\u8003\u8651\u4efb\u52a1\u7279\u5b9a\u6027\u6765\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u4e0e\u90e8\u7f72\u3002"}}
{"id": "2512.15179", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.15179", "abs": "https://arxiv.org/abs/2512.15179", "authors": ["Xiaoqi Li", "Zongwei Li", "Wenkai Li", "Yuqing Zhang", "Xin Wang"], "title": "No More Hidden Pitfalls? Exposing Smart Contract Bad Practices with LLM-Powered Hybrid Analysis", "comment": null, "summary": "As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they elevate the risk of encountering problems. Therefore, to understand and avoid these bad practices, this paper introduces the first systematic study of bad practices in smart contracts, delving into over 47 specific issues. Specifically, we propose SCALM, an LLM-powered framework featuring two methodological innovations: (1) A hybrid architecture that combines context-aware function-level slicing with knowledge-enhanced semantic reasoning via extensible vectorized pattern matching. (2) A multi-layer reasoning verification system connects low-level code patterns with high-level security principles through syntax, design patterns, and architecture analysis. Our extensive experiments using multiple LLMs and datasets have shown that SCALM outperforms existing tools in detecting bad practices in smart contracts.", "AI": {"tldr": "SCALM\uff1a\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u5408\u7ea6\u4e0d\u826f\u5b9e\u8df5\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u63a8\u7406\u4e0e\u5411\u91cf\u5316\u6a21\u5f0f\u5339\u914d\uff0c\u5728\u591a\u6570\u636e\u96c6\u548c\u591a\u6a21\u578b\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u4ee5\u592a\u574a\u7b49\u533a\u5757\u94fe\u5e73\u53f0\u65e5\u76ca\u6210\u719f\uff0c\u9700\u5728\u5f00\u53d1\u5b9e\u8df5\u4e0a\u7ef4\u6301\u9ad8\u6807\u51c6\u3002\u5c3d\u7ba1\u4e0d\u826f\u5b9e\u8df5\u4e0d\u5fc5\u7136\u5bfc\u81f4\u5b89\u5168\u95ee\u9898\uff0c\u4f46\u4f1a\u663e\u8457\u589e\u5927\u98ce\u9669\u3002\u672c\u8bba\u6587\u9996\u6b21\u5bf9\u667a\u80fd\u5408\u7ea6\u4e2d\u7684\u4e0d\u826f\u5f00\u53d1\u5b9e\u8df5\u8fdb\u884c\u7cfb\u7edf\u5316\u7814\u7a76\uff0c\u8986\u76d647\u4e2a\u5177\u4f53\u95ee\u9898\u3002", "method": "\u63d0\u51faSCALM\uff0c\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u5177\u6709\u4e24\u9879\u65b9\u6cd5\u521b\u65b0\uff1a(1) \u5c06\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u51fd\u6570\u7ea7\u5207\u7247\u4e0e\u77e5\u8bc6\u589e\u5f3a\u7684\u8bed\u4e49\u63a8\u7406\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u5411\u91cf\u5316\u6a21\u5f0f\u5339\u914d\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u67b6\u6784\uff1b(2) \u4e00\u4e2a\u591a\u5c42\u63a8\u7406\u6838\u9a8c\u7cfb\u7edf\uff0c\u5c06\u5e95\u5c42\u4ee3\u7801\u6a21\u5f0f\u4e0e\u9ad8\u5c42\u5b89\u5168\u539f\u5219\u901a\u8fc7\u8bed\u6cd5\u3001\u8bbe\u8ba1\u6a21\u5f0f\u4e0e\u4f53\u7cfb\u7ed3\u6784\u5206\u6790\u5173\u8054\u8d77\u6765\u3002", "result": "\u5728\u4f7f\u7528\u591a\u79cdLLM\u4e0e\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u4e2d\uff0cSCALM\u5728\u68c0\u6d4b\u667a\u80fd\u5408\u7ea6\u4e0d\u826f\u5b9e\u8df5\u65b9\u9762\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "conclusion": "SCALM\u4e3a\u667a\u80fd\u5408\u7ea6\u4e0d\u826f\u5b9e\u8df5\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u4e0e\u68c0\u6d4b\u63d0\u4f9b\u65b0\u7684\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8eLLM\u7684\u6df7\u5408\u63a8\u7406\u6846\u67b6\u5728\u5b89\u5168\u76f8\u5173\u4ee3\u7801\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u540e\u7eed\u5de5\u5177\u5f00\u53d1\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2512.14873", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14873", "abs": "https://arxiv.org/abs/2512.14873", "authors": ["Sam Jeong", "Hae Yong Kim"], "title": "How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal", "comment": null, "summary": "Fourier Analysis Network (FAN) was recently proposed as a simple way to improve neural network performance by replacing part of ReLU activations with sine and cosine functions. Although several studies have reported small but consistent gains across tasks, the underlying mechanism behind these improvements has remained unclear. In this work, we show that only the sine activation contributes positively to performance, whereas the cosine activation tends to be detrimental. Our analysis reveals that the improvement is not a consequence of the sine function's periodic nature; instead, it stems from the function's local behavior near x = 0, where its non-zero derivative mitigates the vanishing-gradient problem. We further show that FAN primarily alleviates the dying-ReLU problem, in which a neuron consistently receives negative inputs, produces zero gradients, and stops learning. Although modern ReLU-like activations, such as Leaky ReLU, GELU, and Swish, reduce ReLU's zero-gradient region, they still contain input domains where gradients remain significantly diminished, contributing to slower optimization and hindering rapid convergence. FAN addresses this limitation by introducing a more stable gradient pathway. This analysis shifts the understanding of FAN's benefits from a spectral interpretation to a concrete analysis of training dynamics, leading to the development of the Dual-Activation Layer (DAL), a more efficient convergence accelerator. We evaluate DAL on three tasks: classification of noisy sinusoidal signals versus pure noise, MNIST digit classification, and ECG-based biometric recognition. In all cases, DAL models converge faster and achieve equal or higher validation accuracy compared to models with conventional activations.", "AI": {"tldr": "FAN \u901a\u8fc7\u5728\u90e8\u5206 ReLU \u4e0a\u5f15\u5165\u6b63\u5f26/\u4f59\u5f26\u6fc0\u6d3b\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5176\u673a\u5236\u6765\u81ea\u6b63\u5f26\u5728 x=0 \u9644\u8fd1\u7684\u5c40\u90e8\u5bfc\u6570\u7279\u6027\u800c\u975e\u5468\u671f\u6027\uff1bFAN \u4e3b\u8981\u7f13\u89e3 dying-ReLU\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u6536\u655b\u52a0\u901f\u5668 Dual-Activation Layer (DAL)\uff0c\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u4e14\u8fbe\u5230\u76f8\u540c\u6216\u66f4\u9ad8\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "motivation": "\u63ed\u793a FAN \u6539\u8fdb\u7684\u771f\u5b9e\u673a\u5236\uff0c\u63a8\u52a8\u4ece\u5355\u7eaf\u7684\u9891\u8c31\u89e3\u91ca\u8f6c\u5411\u5bf9\u8bad\u7ec3\u52a8\u529b\u5b66\u7684\u5206\u6790\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u6fc0\u6d3b\u7ec4\u5408\u4ee5\u52a0\u901f\u6536\u655b\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u6b63\u5f26/\u4f59\u5f26\u6fc0\u6d3b\u5bf9\u68af\u5ea6\u4e0e\u5b66\u4e60 dynamics \u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u5b83\u4eec\u5bf9\u6027\u80fd\u7684\u8d21\u732e\uff0c\u8bca\u65ad FAN \u5bf9 dying-ReLU \u7684\u7f13\u89e3\uff0c\u8fdb\u800c\u63d0\u51fa DAL \u5e76\u5728\u591a\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1) \u4ec5\u6b63\u5f26\u6fc0\u6d3b\u5bf9\u6027\u80fd\u6709\u6b63\u5411\u8d21\u732e\uff0c\u4f59\u5f26\u901a\u5e38\u6709\u5bb3\uff1b2) \u6539\u8fdb\u6e90\u4e8e\u6b63\u5f26\u5728 x\u22480 \u7684\u5c40\u90e8\u68af\u5ea6\u884c\u4e3a\uff0c\u800c\u975e\u5176\u5468\u671f\u6027\uff1b3) FAN \u4e3b\u8981\u7f13\u89e3 dying-ReLU \u7684\u95ee\u9898\uff1b4) DAL \u5728\u4e09\u9879\u4efb\u52a1\u4e0a\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u5e76\u8fbe\u5230\u7b49\u4e8e\u6216\u66f4\u9ad8\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "conclusion": "\u5c06 FAN \u7684\u6548\u5e94\u89e3\u91ca\u4e3a\u8bad\u7ec3\u52a8\u529b\u5b66\u5c42\u9762\u7684\u6539\u8fdb\uff0c\u5e76\u63d0\u51fa DAL \u4f5c\u4e3a\u66f4\u9ad8\u6548\u7684\u6536\u655b\u52a0\u901f\u5668\uff0c\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\u3002"}}
{"id": "2512.15387", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.15387", "abs": "https://arxiv.org/abs/2512.15387", "authors": ["Paul Staat", "Daniel Davidovich", "Christof Paar"], "title": "Talking to the Airgap: Exploiting Radio-Less Embedded Devices as Radio Receivers", "comment": null, "summary": "Intelligent electronics are deeply embedded in critical infrastructures and must remain reliable, particularly against deliberate attacks. To minimize risks and impede remote compromise, sensitive systems can be physically isolated from external networks, forming an airgap. Yet, airgaps can still be infiltrated by capable adversaries gaining code execution. Prior research has shown that attackers can then attempt to wirelessly exfiltrate data across the airgap by exploiting unintended radio emissions. In this work, we demonstrate reversal of this link: malicious code execution on embedded devices can enable wireless infiltration of airgapped systems without any hardware modification. In contrast to previous infiltration methods that depend on dedicated sensors (e.g., microphones, LEDs, or temperature sensors) or require strict line-of-sight, we show that unmodified, sensor-less embedded devices can inadvertently act as radio receivers. This phenomenon stems from parasitic RF sensitivity in PCB traces and on-chip analog-to-digital converters (ADCs), allowing external transmissions to be received and decoded entirely in software.\n  Across twelve commercially available embedded devices and two custom prototypes, we observe repeatable reception in the 300-1000 MHz range, with detectable signal power as low as 1 mW. To this end, we propose a systematic methodology to identify device configurations that foster such radio sensitivities and comprehensively evaluate their feasibility for wireless data reception. Exploiting these sensitivities, we demonstrate successful data reception over tens of meters, even in non-line-of-sight conditions and show that the reception sensitivities accommodate data rates of up to 100 kbps. Our findings reveal a previously unexplored command-and-control vector for air-gapped systems while challenging assumptions about their inherent isolation. [shortened]", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u672a\u4fee\u6539\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u53ef\u4f5c\u4e3a\u65e0\u7ebf\u63a5\u6536\u5668\uff0c\u5728\u65e0\u786c\u4ef6\u4fee\u6539\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5bf9\u6c14\u9699\u7cfb\u7edf\u7684\u65e0\u7ebf\u6e17\u900f\uff0c\u5de5\u4f5c\u5728300\u20131000 MHz\uff0c\u6570\u636e\u901f\u7387\u53ef\u8fbe100 kbps\uff0c\u8986\u76d6\u6570\u5341\u7c73\uff0c\u6311\u6218\u6c14\u9699\u7684\u5b89\u5168\u5047\u8bbe\u3002", "motivation": "\u786e\u4fdd\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u53ef\u9760\u6027\u4e0e\u5b8c\u6574\u6027\uff0c\u62b5\u5fa1\u8fdc\u7a0b\u653b\u51fb\uff1b\u4f20\u7edf\u7684\u6c14\u9699\u9632\u62a4\u4f9d\u8d56\u7269\u7406\u9694\u79bb\uff0c\u4f46\u5bf9\u6f5c\u5728\u7684\u9690\u853d\u5c04\u9891\u63a5\u6536\u80fd\u529b\u5c1a\u672a\u5145\u5206\u8bc4\u4f30\uff0c\u5b58\u5728\u88ab\u52a8\u5f0f\u4fb5\u5165\u98ce\u9669\u3002\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u672a\u4fee\u6539\u8bbe\u5907\u7684\u5c04\u9891\u654f\u611f\u6027\u53ca\u5176\u5bf9\u6c14\u9699\u5b89\u5168\u7684\u6f5c\u5728\u5a01\u80c1\u3002", "method": "\u572812\u6b3e\u5546\u7528\u5d4c\u5165\u5f0f\u8bbe\u5907\u548c2\u4e2a\u5b9a\u5236\u539f\u578b\u4e0a\uff0c\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u4fc3\u6210\u5c04\u9891\u654f\u611f\u6027\u7684\u914d\u7f6e\uff0c\u5229\u7528PCB\u8d70\u7ebf\u548c\u7247\u4e0a\u6a21\u6570\u8f6c\u6362\u5668\u7684\u5bc4\u751fRF\u654f\u611f\u6027\uff0c\u901a\u8fc7\u7eaf\u8f6f\u4ef6\u89e3\u7801\u5b9e\u73b0\u5bf9\u5916\u90e8\u4fe1\u53f7\u7684\u63a5\u6536\u4e0e\u89e3\u7801\uff1b\u5f00\u5c55\u8de8\u8ddd\u79bb\u3001\u975e\u89c6\u8ddd\u6761\u4ef6\u4e0b\u7684\u6570\u636e\u63a5\u6536\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e0d\u540c\u914d\u7f6e\u5bf9\u63a5\u6536\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u7ed9\u51fa\u4e00\u5957\u8bc4\u4f30\u8bbe\u5907\u6613\u53d7RF\u5165\u4fb5\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002", "result": "\u5728300\u20131000 MHz\u8303\u56f4\u5185\u53ef\u91cd\u590d\u89c2\u6d4b\u5230\u63a5\u6536\uff0c\u6700\u5c0f\u63a2\u6d4b\u4fe1\u53f7\u529f\u7387\u7ea61 mW\uff1b\u5b9e\u73b0\u6570\u636e\u63a5\u6536\u8ddd\u79bb\u8fbe\u6570\u5341\u7c73\uff0c\u975e\u7ebf\u6027\u906e\u6321\u6761\u4ef6\u4e0b\u4e5f\u53ef\u5de5\u4f5c\uff0c\u6570\u636e\u901f\u7387\u6700\u9ad8\u53ef\u8fbe\u7ea6100 kbps\uff1b\u4e0d\u4f9d\u8d56\u989d\u5916\u4f20\u611f\u5668\uff0c\u8bbe\u5907\u7684\u5bc4\u751fRF\u654f\u611f\u6027\u5373\u53ef\u88ab\u8f6f\u89e3\u7801\u5229\u7528\u3002", "conclusion": "\u63ed\u793a\u4e86\u4e00\u4e2a\u524d\u6240\u672a\u6709\u7684\u6307\u4ee4\u4e0e\u63a7\u5236\u5411\u91cf\uff0c\u5373\u4f7f\u5728\u4e25\u683c\u7684\u7269\u7406\u9694\u79bb\u4e0b\uff0c\u6c14\u9699\u7cfb\u7edf\u4e5f\u53ef\u80fd\u88ab\u6709\u610f\u6216\u65e0\u610f\u7684\u8f6f\u4ef6\u63a7\u5236\u7684\u5c04\u9891\u63a5\u6536\u80fd\u529b\u6240\u7a81\u7834\uff1b\u63d0\u51fa\u7cfb\u7edf\u5316\u7684\u8bbe\u5907\u914d\u7f6e\u8bc6\u522b\u6846\u67b6\u4ee5\u8bc4\u4f30\u98ce\u9669\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u7269\u7406\u5c42\u9632\u62a4\u4e0e\u6c14\u9699\u5b89\u5168\u7b56\u7565\u3002"}}
{"id": "2512.14879", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14879", "abs": "https://arxiv.org/abs/2512.14879", "authors": ["Jingwei Chen"], "title": "Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse", "comment": null, "summary": "Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.", "AI": {"tldr": "\u63d0\u51fa Entropy-Reservoir Bregman Projection (ERBP) \u4f5c\u4e3a\u4e00\u4e2a\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\uff0c\u7edf\u4e00\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u5d29\u6e83\u73b0\u8c61\uff08\u6a21\u578b\u5d29\u6e83\u3001\u6a21\u5f0f\u574d\u584c\u3001\u7b56\u7565\u8fc7\u5ea6\u5229\u7528\u7b49\uff09\u53ca\u5176\u4fee\u6b63\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u71b5\u201c\u71b5\u6c34\u5e93\u201d\u6765\u7ef4\u6301\u71b5\u6c34\u5e73\uff0c\u5e76\u7ed9\u51fa\u6536\u655b/\u53d1\u6563\u7684\u5fc5\u8981\u4e0e\u5145\u5206\u6761\u4ef6\u53ca\u4f9d\u8d56\u6837\u672c\u91cf\u4e0e\u5149\u6ed1\u5e38\u6570\u7684\u95ed\u5f0f\u901f\u7387\u3002", "motivation": "\u81ea\u76d1\u7763/\u81ea\u6211\u8bad\u7ec3\u4e2d\uff0c\u6a21\u578b\u5bb9\u6613\u56e0\u6709\u9650\u6837\u672c\u566a\u58f0\u5728\u5206\u5e03\u7a7a\u95f4\u7684\u6295\u5f71\u5e8f\u5217\u4e2d\u6536\u655b\u5230\u9010\u6e10\u7f29\u5c0f\u7684\u7ecf\u9a8c\u652f\u6301\u800c\u5bfc\u81f4\u71b5\u6307\u6570\u4e0b\u964d\u3001\u6587\u672c\u91cd\u590d\u3001\u6a21\u5f0f\u4e22\u5931\u7b49\u5d29\u6e83\u73b0\u8c61\u3002\u5c3d\u7ba1\u5b58\u5728\u591a\u79cd\u7ecf\u9a8c\u6027\u4fee\u590d\u624b\u6bb5\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u3001\u53ef\u9884\u6d4b\u7684\u539f\u7406\u6765\u89e3\u91ca\u6210\u529f/\u5931\u8d25\u7684\u5171\u6027\u4e0e\u4f5c\u7528\u673a\u5236\u3002", "method": "\u5c06\u95ed\u73af\u5efa\u6a21\u4e3a\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u968f\u673aBregman\u6295\u5f71\u5e8f\u5217\uff1b\u5728\u65e0\u5916\u90e8\u8026\u5408\u65f6\uff0c\u6709\u9650\u6837\u672c\u566a\u58f0\u5bfc\u81f4\u71b5\u7684\u6307\u6570\u8870\u51cf\u4e0e\u5d29\u6e83\u3002\u5f15\u5165\u71b5\u6c34\u5e93\uff0c\u5c06\u9ad8\u71b5\u5206\u5e03\u6df7\u5165\u6bcf\u6b21\u6295\u5f71\u4e2d\uff0c\u7ed9\u4e88\u53ef\u63a7\u7684\u71b5\u901a\u91cf\u4ee5\u7a33\u5b9a\u52a8\u529b\u5b66\u3002\u7ed9\u51fa\u5d29\u6e83\u7684\u5fc5\u8981\u6761\u4ef6\u3001\u786e\u4fdd\u975e\u5e73\u51e1\u71b5\u4e0b\u754c\u7684\u5145\u5206\u6761\u4ef6\uff0c\u4ee5\u53ca\u4ec5\u4f9d\u8d56\u6837\u672c\u91cf\u548cBregman\u751f\u6210\u51fd\u6570\u7684\u5f3a\u51f8/ Lipschitz\u5e38\u6570\u7684\u89e3\u6790\u901f\u7387\u3002\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u8bad\u7ec3\u3001Soft Actor-Critic\u3001GAN\u4f18\u5316\u7b49\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\uff0c\u4e14\u8bc1\u660e\u4e0d\u540c\u7684\u7a33\u5b9a\u5316\u542f\u53d1\u5f0f\u5bf9\u5e94\u8be5\u6c34\u5e93\u7684\u5177\u4f53\u9009\u53d6\u548c\u8026\u5408\u7cfb\u6570\u3002", "result": "ERBP \u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u89c4\u5219\uff1a\u901a\u8fc7\u76d1\u63a7\u548c\u9884\u7b97\u71b5\u901a\u91cf\uff0c\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u8f6c\u5316\u4e3a\u53ef\u63a7\u53c2\u6570\u3002\u7406\u8bba\u4e0a\u7ed9\u51fa\u5d29\u6e83\u7684\u5fc5\u8981\u6761\u4ef6\u3001\u975e\u5e73\u51e1\u71b5\u4e0b\u754c\u7684\u5145\u5206\u6761\u4ef6\u548c\u4e0e\u6837\u672c\u91cf\u53ca\u5149\u6ed1\u6027\u76f8\u5173\u7684\u95ed\u5f0f\u901f\u7387\uff1b\u5b9e\u9a8c\u4e0a\u8bc1\u5b9e\u4e0d\u540c\u7a33\u6001\u7b56\u7565\u5bf9\u5e94\u4e0d\u540c\u7684\u6c34\u5e93\u914d\u7f6e\u4e0e\u8026\u5408\uff0c\u4ece\u800c\u5c06\u7ecf\u9a8c\u6027\u4fee\u6b63\u7edf\u4e00\u6210\u4e00\u4e2a\u5b9a\u91cf\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "conclusion": "ERBP \u5c06\u591a\u79cd\u770b\u4f3c\u5206\u6563\u7684\u7a33\u5b9a\u5316\u6280\u5de7\u7edf\u4e00\u6210\u4e00\u4e2a\u5b9a\u91cf\u6846\u67b6\uff0c\u63d0\u4f9b\u201c\u76d1\u63a7\u5e76\u9884\u7b97\u71b5\u901a\u91cf\u201d\u7684\u5b9e\u7528\u8bbe\u8ba1\u6cd5\u5219\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5bf9\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u4fe1\u606f\u5faa\u73af\u8fdb\u884c\u53ef\u63a7\u7684\u71b5\u7ba1\u7406\uff0c\u8fdb\u800c\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6837\u672c\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2512.14880", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.14880", "abs": "https://arxiv.org/abs/2512.14880", "authors": ["Darrin O' Brien", "Dhikshith Gajulapalli", "Eric Xia"], "title": "Task Matrices: Linear Maps for Cross-Model Finetuning Transfer", "comment": "NeurIPS Unireps 2025", "summary": "Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4efb\u52a1\u77e9\u9635\u4ee5\u5728\u9884\u8bad\u7ec3-\u5fae\u8c03\u5d4c\u5165\u4e4b\u95f4\u5efa\u7acb\u7ebf\u6027\u8de8\u5c42\u7f16\u7801\uff0c\u5728\u591a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u7ebf\u6027\u63a2\u9488\u7684\u8868\u73b0\uff0c\u4e26\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u8fd1\u4f3c\u4e0e\u8de8\u57df\u6cdb\u5316\u3002", "motivation": "\u56de\u7b54\u662f\u5426\u5b58\u5728\u66f4\u5e7f\u6cdb\u9002\u5e94\u573a\u666f\u4e2d\u7684\u7ebf\u6027\u8868\u793a\uff0c\u4ee5\u53ca\u5982\u4f55\u89e3\u91ca\u6a21\u578b\u7684\u7ebf\u6027\u9002\u914d\u6027\uff0c\u8fdb\u800c\u964d\u4f4e\u5fae\u8c03\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u77e9\u9635\u4f5c\u4e3a\u57fa\u5d4c\u5165\u5230\u5fae\u8c03\u5d4c\u5165\u7684\u7ebf\u6027\u53d8\u6362\uff1b\u5728\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u578b\u53ca\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6bd4\u8f83\u57fa\u7ebf\u3001\u7ebf\u6027\u63a2\u9488\u4e0e\u4efb\u52a1\u77e9\u9635\uff1b\u7ed9\u51fa\u57fa\u4e8e\u6570\u636e\u7684\u8fd1\u4f3c\u65b9\u6cd5\u5e76\u68c0\u9a8c\u8de8\u57df\u6cdb\u5316\uff1b\u5b9e\u73b0\u516c\u5f00\u53ef\u7528\u3002", "result": "\u5728\u52a0\u5165\u4efb\u52a1\u77e9\u9635\u7684\u57fa\u6a21\u578b\u4e0a\uff0c\u6027\u80fd\u8d85\u8d8a\u7ebf\u6027\u63a2\u9488\uff0c\u5728\u67d0\u4e9b\u573a\u666f\u63a5\u8fd1\u5fae\u8c03\u6c34\u5e73\uff1b\u9a8c\u8bc1\u4e86\u8de8\u5c42\u7ebf\u6027\u7f16\u7801\u7684\u5b58\u5728\u6027\uff1b\u63d0\u51fa\u9ad8\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u6570\u636e\u8fd1\u4f3c\u65b9\u6cd5\uff1b\u5b9e\u73b0\u516c\u5f00\u3002", "conclusion": "\u8de8\u57df\u7684\u9884\u8bad\u7ec3\u5230\u5fae\u8c03\u5d4c\u5165\u7684\u7ebf\u6027\u7f16\u7801\u5b58\u5728\uff0c\u4efb\u52a1\u77e9\u9635\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fd1\u4f3c\u4e0e\u7406\u89e3\u6a21\u578b\u9002\u5e94\u6027\u7684\u9014\u5f84\uff0c\u6f5c\u5728\u964d\u4f4e\u5fae\u8c03\u6210\u672c\u5e76\u63a8\u52a8\u8fc1\u79fb\u5b66\u4e60\u7684\u7406\u89e3\u3002"}}
{"id": "2512.14892", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14892", "abs": "https://arxiv.org/abs/2512.14892", "authors": ["Mohammad Abu-Shaira", "Alejandro Rodriguez", "Greg Speegle", "Victor Sheng", "Ishfaq Ahmad"], "title": "OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams", "comment": null, "summary": "Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce \"OLR-WA; OnLine Regression with Weighted Average\", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.", "AI": {"tldr": "\u63d0\u51faOLR-WA\uff08OnLine Regression with Weighted Average\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u4e14\u591a\u53d8\u91cf\u7684\u5728\u7ebf\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u80fd\u591f\u589e\u91cf\u66f4\u65b0\u3001\u5904\u7406\u6f02\u79fb\u3001\u5feb\u901f\u6536\u655b\uff0c\u5e76\u5728\u4e0e\u73b0\u6709\u5728\u7ebf\u6a21\u578b\u7684\u5bf9\u6bd4\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u6781\u5c11\u521d\u59cb\u6570\u636e\u65f6\uff08\u4ec51%-10%\u603b\u6570\u636e\u91cf\uff09\u4e5f\u80fd\u5b9e\u73b0\u8f83\u9ad8\u7684R^2\u503c\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u5927\u91cf\u5b58\u50a8\u6570\u636e\u6216\u91cd\u7b97\u6a21\u578b\u7684\u524d\u63d0\u4e0b\u8fdb\u884c\u9ad8\u6548\u5728\u7ebf\u56de\u5f52\u7684\u95ee\u9898\uff0c\u5e76\u80fd\u5904\u7406\u6570\u636e\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u6f02\u79fb\u73b0\u8c61\uff08\u65f6\u95f4\u6f02\u79fb\u4e0e\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u6f02\u79fb\uff09\u3002", "method": "\u63d0\u51faOLR-WA\uff1a\u5e26\u6743\u91cd\u5e73\u5747\u7684\u5728\u7ebf\u56de\u5f52\u65b9\u6cd5\uff1b\u901a\u8fc7\u66f4\u65b0\u89c4\u5219\u5f15\u5165\u5bf9\u65e7\u6570\u636e\uff08\u5c24\u5176\u662f\u9ad8\u7f6e\u4fe1\u5ea6\u6570\u636e\uff09\u7684\u4fdd\u5b88\u5904\u7406\uff0c\u9002\u5e94\u6f02\u79fb\u573a\u666f\uff1b\u8fdb\u884c\u6536\u655b\u6027\u5206\u6790\uff1b\u4e0e\u73b0\u6709\u5728\u7ebf\u56de\u5f52\u6a21\u578b\u6bd4\u8f83\u3002", "result": "OLR-WA\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u6279\u56de\u5f52\uff0c\u4e14\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u8868\u73b0\uff1b\u4ece\u7b2c\u4e00\u6b21\u8fed\u4ee3\u5230\u6700\u540e\u4e00\u6b21\u8fed\u4ee3\u5747\u80fd\u5b9e\u73b0\u5feb\u901f\u6536\u655b\uff0c\u4e14\u5728\u6781\u4f4e\u521d\u59cb\u6837\u672c\u91cf\uff081%-10%\uff09\u65f6\u4e5f\u80fd\u7ef4\u6301\u9ad8R^2\uff1b\u80fd\u591f\u6709\u6548\u5904\u7406\u65f6\u95f4\u57fa\u6f02\u79fb\u4ee5\u53ca\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u6311\u6218\u6027\u573a\u666f\uff0c\u4e14\u5728\u6b64\u65b9\u9762\u8868\u73b0\u4e3a\u552f\u4e00\u80fd\u591f\u6709\u6548\u5904\u7406\u7684\u6a21\u578b\u3002", "conclusion": "OLR-WA\u5728\u5728\u7ebf\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u9ad8\u5ea6\u7684\u7075\u6d3b\u6027\u4e0e\u5b9e\u7528\u6027\uff0c\u7ed3\u5408\u5feb\u901f\u6536\u655b\u3001\u7ade\u4e89\u6027\u51c6\u786e\u6027\u4ee5\u53ca\u5bf9\u4e0d\u540c\u6f02\u79fb\u60c5\u666f\u7684\u9c81\u68d2\u6027\uff0c\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5728\u7ebf\u56de\u5f52\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.15641", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.15641", "abs": "https://arxiv.org/abs/2512.15641", "authors": ["Yunfei Yang", "Xiaojun Chen", "Zhendong Zhao", "Yu Zhou", "Xiaoyan Gu", "Juan Cao"], "title": "ComMark: Covert and Robust Black-Box Model Watermarking with Compressed Samples", "comment": null, "summary": "The rapid advancement of deep learning has turned models into highly valuable assets due to their reliance on massive data and costly training processes. However, these models are increasingly vulnerable to leakage and theft, highlighting the critical need for robust intellectual property protection. Model watermarking has emerged as an effective solution, with black-box watermarking gaining significant attention for its practicality and flexibility. Nonetheless, existing black-box methods often fail to better balance covertness (hiding the watermark to prevent detection and forgery) and robustness (ensuring the watermark resists removal)-two essential properties for real-world copyright verification. In this paper, we propose ComMark, a novel black-box model watermarking framework that leverages frequency-domain transformations to generate compressed, covert, and attack-resistant watermark samples by filtering out high-frequency information. To further enhance watermark robustness, our method incorporates simulated attack scenarios and a similarity loss during training. Comprehensive evaluations across diverse datasets and architectures demonstrate that ComMark achieves state-of-the-art performance in both covertness and robustness. Furthermore, we extend its applicability beyond image recognition to tasks including speech recognition, sentiment analysis, image generation, image captioning, and video recognition, underscoring its versatility and broad applicability.", "AI": {"tldr": "\u63d0\u51fa ComMark\uff0c\u4e00\u79cd\u57fa\u4e8e\u9891\u57df\u53d8\u6362\u7684\u9ed1\u76d2\u6a21\u578b\u6c34\u5370\u6846\u67b6\u3002\u901a\u8fc7\u538b\u7f29\u4e0e\u53bb\u9ad8\u9891\u5b9e\u73b0\u9690\u853d\u4e14\u6297\u653b\u51fb\u7684\u6c34\u5370\u6837\u672c\uff0c\u7ed3\u5408\u4eff\u771f\u653b\u51fb\u4e0e\u76f8\u4f3c\u6027\u635f\u5931\u8bad\u7ec3\uff0c\u8fbe\u5230 covertness \u4e0e robustness \u7684 state-of-the-art\uff0c\u5e76\u6269\u5c55\u81f3\u591a\u6a21\u6001\u4efb\u52a1\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6210\u4e3a\u5b9d\u8d35\u8d44\u4ea7\uff0c\u56e0\u6570\u636e\u4e0e\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u6a21\u578b\u6613\u88ab\u7a83\u53d6\u6216\u6cc4\u9732\uff0c\u56e0\u6b64\u9700\u8981\u5f3a\u6709\u529b\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u624b\u6bb5\u3002\u73b0\u6709\u9ed1\u76d2\u6c34\u5370\u5728\u9690\u853d\u6027\u4e0e\u9c81\u68d2\u6027\u4e4b\u95f4\u96be\u4ee5\u517c\u987e\uff0c\u4e9f\u9700\u66f4\u7a33\u5065\u3001\u9690\u853d\u7684\u6c34\u5370\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9891\u57df\u53d8\u6362\u7684\u6c34\u5370\u751f\u6210\u6846\u67b6 ComMark\uff0c\u901a\u8fc7\u5bf9\u8f93\u5165/\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u9ad8\u6548\u7684\u9891\u57df\u8fc7\u6ee4\uff0c\u751f\u6210\u538b\u7f29\u3001\u9690\u853d\u7684\u6c34\u5370\u6837\u672c\uff1b\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u4eff\u771f\u653b\u51fb\u573a\u666f\u548c\u76f8\u4f3c\u6027\u635f\u5931\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\uff1b\u5c5e\u4e8e\u9ed1\u76d2\u6c34\u5370\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u4e0e\u591a\u79cd\u4f53\u7cfb\u7ed3\u6784\u4e0a\u5b9e\u73b0\u4e86\u5728\u9690\u853d\u6027\u4e0e\u9c81\u68d2\u6027\u4e0a\u7684\u65b0\u57fa\u7ebf\uff0c\u8fbe\u5230 state-of-the-art \u6c34\u5e73\uff1b\u5e76\u4e14\u5c06\u6c34\u5370\u65b9\u6cd5\u6269\u5c55\u81f3\u8bed\u97f3\u8bc6\u522b\u3001\u60c5\u611f\u5206\u6790\u3001\u56fe\u50cf\u751f\u6210\u3001\u56fe\u50cf\u63cf\u8ff0\u4e0e\u89c6\u9891\u8bc6\u522b\u7b49\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "conclusion": "ComMark \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u9690\u853d\u6027\u3001\u5f3a\u9c81\u68d2\u6027\u7684\u9ed1\u76d2\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\uff0c\u80fd\u591f\u6709\u6548\u7528\u4e8e\u7248\u6743\u9a8c\u8bc1\u4e0e\u6a21\u578b\u4fdd\u62a4\u3002"}}
{"id": "2512.15648", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.15648", "abs": "https://arxiv.org/abs/2512.15648", "authors": ["Ratang Sedimo", "Ivoline C. Ngong", "Jami Lashua", "Joseph P. Near"], "title": "Distributed HDMM: Scalable, Distributed, Accurate, and Differentially Private Query Workloads without a Trusted Curator", "comment": null, "summary": "We present the Distributed High-Dimensional Matrix Mechanism (Distributed HDMM), a protocol for answering workloads of linear queries on distributed data that provides the accuracy of central-model HDMM without a trusted curator. Distributed HDMM leverages a secure aggregation protocol to evaluate HDMM on distributed data, and is secure in the context of a malicious aggregator and malicious clients (assuming an honest majority). Our preliminary empirical evaluation shows that Distributed HDMM can run on realistic datasets and workloads with thousands of clients in less than one minute.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u5f0f\u9ad8\u7ef4\u77e9\u9635\u673a\u5236\uff08Distributed HDMM\uff09\uff0c\u5728\u6ca1\u6709\u53ef\u4fe1\u6258\u7ba1\u8005\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5b89\u5168\u805a\u5408\u5b9e\u73b0\u4e2d\u5fc3\u6a21\u578bHDMM\u7684\u51c6\u786e\u6027\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u6570\u636e\u7684\u7ebf\u6027\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\u3002\u5bf9\u6076\u610f\u805a\u5408\u8005\u548c\u6076\u610f\u5ba2\u6237\u7aef\u5728\u591a\u6570\u8bda\u5b9e\u7684\u524d\u63d0\u4e0b\u63d0\u4f9b\u5b89\u5168\u6027\uff0c\u521d\u6b65\u5b9e\u9a8c\u5728\u73b0\u5b9e\u6570\u636e\u4e0e\u6570\u5343\u5ba2\u6237\u7aef\u4e0b\u8017\u65f6\u4e0d\u8db31\u5206\u949f\u3002", "motivation": "\u4e2d\u592e\u6a21\u578bHDMM\u9700\u8981\u4fe1\u4efb\u7684\u6258\u7ba1\u8005\u6765\u805a\u5408\u6570\u636e\u5e76\u6dfb\u52a0\u566a\u58f0\u4ee5\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u5f97\u6216\u4e0d\u5e0c\u671b\u6709\u5355\u4e00\u53ef\u4fe1\u5b9e\u4f53\u3002\u5206\u5e03\u5f0f\u73af\u5883\u9700\u8981\u5728\u65e0\u6258\u7ba1\u8005\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u7b49\u6548\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5177\u5907\u5bf9\u6076\u610f\u53c2\u4e0e\u8005\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u6574\u5408\u4e00\u4e2a\u5b89\u5168\u805a\u5408\u534f\u8bae\uff0c\u5c06\u5206\u5e03\u5728\u4e0d\u540c\u5ba2\u6237\u7aef\u7684\u6570\u636e\u5728\u4e0d\u66b4\u9732\u539f\u59cb\u6570\u636e\u7684\u524d\u63d0\u4e0b\u8fdb\u884cHDMM\u8ba1\u7b97\uff1b\u5728\u5047\u8bbe\u8bda\u5b9e\u591a\u6570\u7684\u524d\u63d0\u4e0b\uff0c\u62b5\u5fa1\u6076\u610f\u805a\u5408\u8005\u548c\u6076\u610f\u5ba2\u6237\u7aef\u5bf9\u7ed3\u679c\u7684\u64cd\u63a7\uff1b\u901a\u8fc7\u5206\u5e03\u5f0f\u566a\u58f0\u7ba1\u7406\u548c\u5bf9\u7ebf\u6027\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\u7684\u4f18\u5316\u5b9e\u73b0HDMM\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7684\u7b49\u6548\u6027\u3002", "result": "\u521d\u6b65\u7684\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u5728\u73b0\u5b9e\u6570\u636e\u96c6\u4e0e\u5305\u542b\u6570\u5343\u5ba2\u6237\u7aef\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u8be5\u534f\u8bae\u80fd\u591f\u5728\u4e0d\u52301\u5206\u949f\u7684\u65f6\u95f4\u5185\u5b8c\u6210\u8ba1\u7b97\uff0c\u4e14\u8fbe\u5230\u4e0e\u4e2d\u5fc3\u5316HDMM\u76f8\u8fd1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "Distributed HDMM\u5728\u6ca1\u6709\u53ef\u4fe1\u6258\u7ba1\u8005\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4e2d\u5fc3\u5316HDMM\u7684\u51c6\u786e\u6027\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u6076\u610f\u53c2\u4e0e\u8005\u7684\u9c81\u68d2\u6027\uff0c\u9002\u5408\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u5206\u6790\u3002"}}
{"id": "2512.14932", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14932", "abs": "https://arxiv.org/abs/2512.14932", "authors": ["Daniel Gomes de Pinho Zanco", "Leszek Szczecinski", "Jacob Benesty", "Eduardo Vinicius Kuhn"], "title": "Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective", "comment": null, "summary": "In this work, we propose a method to efficiently find the regularization parameter for low-rank MMSE filters based on a Kronecker-product representation. We show that the regularization parameter is surprisingly linked to the problem of rank selection and, thus, properly choosing it, is crucial for low-rank settings. The proposed method is validated through simulations, showing significant gains over commonly used methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eKronecker\u8868\u793a\u7684\u4f4e\u79e9MMSE\u6ee4\u6ce2\u5668\u6b63\u5219\u5316\u53c2\u6570\u9ad8\u6548\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63ed\u793a\u6b63\u5219\u5316\u53c2\u6570\u4e0e\u79e9\u9009\u62e9\u4e4b\u95f4\u7684\u7d27\u5bc6\u5173\u7cfb\uff0c\u5e76\u5728\u4f4e\u79e9\u8bbe\u7f6e\u4e2d\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u76f8\u8f83\u4e8e\u5e38\u7528\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u6b63\u5219\u5316\u53c2\u6570\u7684\u53d6\u503c\u5728\u4f4e\u79e9MMSE\u6ee4\u6ce2\u5668\u4e2d\u4e0e\u79e9\u9009\u62e9\u5bc6\u5207\u76f8\u5173\uff0c\u9519\u8bef\u9009\u62e9\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5065\u7684\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eKronecker-product\u8868\u793a\u7684\u6b63\u5219\u5316\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63a2\u7d22\u5176\u4e0e\u79e9\u9009\u62e9\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u8bbe\u8ba1\u5728\u4f4e\u79e9\u8bbe\u7f6e\u4e2d\u9ad8\u6548\u5730\u786e\u5b9a\u8be5\u53c2\u6570\u7684\u7b56\u7565\u3002\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4e0e\u5e38\u7528\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\u76f8\u6bd4\u65f6\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5229\u7528Kronecker\u8868\u793a\u63ed\u793a\u6b63\u5219\u5316\u53c2\u6570\u4e0e\u79e9\u9009\u62e9\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u7ed9\u51fa\u9ad8\u6548\u7684\u53c2\u6570\u4f30\u8ba1\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u4f4e\u79e9MMSE\u6ee4\u6ce2\u7684\u6027\u80fd\u3002"}}
{"id": "2512.15688", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15688", "abs": "https://arxiv.org/abs/2512.15688", "authors": ["Adam Kaufman", "James Lucassen", "Tyler Tracy", "Cody Rushing", "Aryan Bhatt"], "title": "BashArena: A Control Setting for Highly Privileged AI Agents", "comment": "The task generation pipeline can be found here: https://github.com/redwoodresearch/basharena_public", "summary": "Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.", "AI": {"tldr": "\u63d0\u51fa BashArena \u4f5c\u4e3a\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d AI \u63a7\u5236\u7814\u7a76\u7684\u57fa\u51c6\uff0c\u5305\u542b637\u9879Linux\u4efb\u52a1\u53ca\u56db\u7c7b\u7834\u574f\u76ee\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u5b8c\u6210\u3001\u9690\u853d\u7834\u574f\u548c\u76d1\u63a7\u89c4\u907f\u80fd\u529b\uff0c\u7ed9\u51fa\u57fa\u7ebf\u5e76\u53d1\u5e03\u6570\u636e\u96c6\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53\u53ef\u80fd\u83b7\u5f97\u63d0\u5347\u6743\u9650\uff0c\u82e5\u672a\u5bf9\u9f50\uff0c\u53ef\u80fd\u6ee5\u7528\u6743\u529b\u9020\u6210\u4e25\u91cd\u635f\u5bb3\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u63a7\u5236\u6280\u672f\u6765\u964d\u4f4e\u98ce\u9669\u540c\u65f6\u4fdd\u7559\u6709\u7528\u6027\uff1bBashArena \u65e8\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7cfb\u7edf\u5730\u7814\u7a76\u8fd9\u7c7b\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u6784\u5efa BashArena\uff1a637\u4e2a\u590d\u6742\u7684Linux\u7cfb\u7edf\u7ba1\u7406\u4e0e\u57fa\u7840\u8bbe\u65bd\u4efb\u52a1\uff0c\u5206\u5e03\u5728\u590d\u6742\u3001\u73b0\u5b9e\u5316\u73af\u5883\u4e2d\uff0c\u53e6\u5916\u8bbe\u5b9a\u56db\u4e2a\u7834\u574f\u76ee\u6807\uff08\u6267\u884c\u6076\u610f\u8f6f\u4ef6\u3001\u7a83\u53d6\u673a\u5bc6\u3001\u63d0\u5347\u6743\u9650\u3001\u7981\u7528\u9632\u706b\u5899\uff09\u4f9b\u7ea2\u961f\u6311\u6218\uff0c\u8bc4\u4f30\u591a\u5bb6\u524d\u6cbf\u5927\u6a21\u578b\u5728\u5b8c\u6210\u4efb\u52a1\u3001\u672a\u88ab\u4fa6\u6d4b\u5730\u8fdb\u884c\u7834\u574f\u4ee5\u53ca\u4fa6\u6d4b\u7834\u574f\u65b9\u9762\u7684\u80fd\u529b\u3002\u901a\u8fc7\u8f68\u8ff9\u5f0f\u8bc4\u4f30\u5e76\u62a5\u544a\u4efb\u52a1\u5b8c\u6210\u7387\u3001\u7834\u574f\u6210\u529f\u7387\u4e0e\u76d1\u63a7\u68c0\u6d4b\u7684\u5047\u9633\u6027\u7387\uff08FPR\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0 Claude Sonnet 4.5 \u80fd\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u6210\u529f\u6267\u884c\u7834\u574f\u5e76\u89c4\u907f\u76d1\u63a7\uff0c\u5728 GPT-4.1 mini \u4e0b\u6709\u7ea6 26% \u7684\u89c4\u907f\u7387\uff1b\u8f68\u8ff9\u76f8\u5173\u5047\u9633\u6027\u7387\u7ea6\u4e3a 4%\u3002\u7814\u7a76\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u63a7\u5236\u534f\u8bae\u63d0\u4f9b\u57fa\u7ebf\uff0c\u5e76\u5c06\u6570\u636e\u96c6\u4ee5 ControlArena \u5f62\u5f0f\u53d1\u5e03\uff0c\u540c\u65f6\u516c\u5e03\u4efb\u52a1\u751f\u6210\u7ba1\u7ebf\u3002", "conclusion": "BashArena \u4e3a\u672a\u6765\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u8bbe\u8ba1\u548c\u8bc4\u4f30 AI \u63a7\u5236\u6280\u672f\u63d0\u4f9b\u57fa\u7ebf\uff0c\u63a8\u52a8\u5bf9\u6297\u6027\u4e0e\u76d1\u63a7\u80fd\u529b\u7684\u7814\u7a76\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u96c6\u4e0e\u7ba1\u7ebf\u7684\u516c\u5f00\uff0c\u4fc3\u8fdb\u540e\u7eed\u5de5\u4f5c\u3002"}}
{"id": "2512.15143", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.15143", "abs": "https://arxiv.org/abs/2512.15143", "authors": ["Pablo Monta\u00f1a-Fern\u00e1ndez", "Ines Ortega-Fernandez"], "title": "An Efficient Gradient-Based Inference Attack for Federated Learning", "comment": "This paper was supported by the TRUMPET project, funded by the European Union under Grant Agreement No. 101070038", "summary": "Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff0c\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u6700\u540e\u4e00\u5c42\u68af\u5ea6\u968f\u8f6e\u6b21\u7684\u65f6\u5e8f\u6f14\u5316\u8fdb\u884c\u5206\u6790\u3002\u5229\u7528\u5f71\u5b50\u6a21\u578b\u5b66\u4e60\u8f6e\u6b21\u7ea7\u68af\u5ea6\u6a21\u5f0f\uff0c\u65e0\u9700\u79c1\u6709\u6570\u636e\uff0c\u9002\u7528\u4e8e\u534a\u8bda\u5b9e\u4e0e\u6076\u610f\u53c2\u4e0e\u8005\uff08\u805a\u5408\u65b9\u6216\u6570\u636e\u62e5\u6709\u8005\uff09\u3002\u53ef\u6269\u5c55\u81f3\u79bb\u6563\u5c5e\u6027\u63a8\u65ad\uff1b\u5bf9\u4efb\u610f\u68af\u5ea6\u6a21\u578b\u53ca\u5206\u7c7b/\u56de\u5f52\u5747\u5177\u9002\u7528\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u5728\u591a\u8f6eFL\u4e2d\u6cc4\u6f0f\u589e\u5f3a\uff0c\u4e14\u805a\u5408\u65b9\u5a01\u80c1\u66f4\u5927\uff1b\u9ad8\u7ef4\u6570\u636e\u6cc4\u6f0f\u66f4\u5f3a\uff0c\u8ba1\u7b97/\u5185\u5b58\u5f00\u9500\u4e0e\u73b0\u6709\u653b\u51fb\u76f8\u5f53\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0b\uff0c\u867d\u7136\u964d\u4f4e\u4e86\u76f4\u63a5\u6570\u636e\u66b4\u9732\uff0c\u4f46\u6a21\u578b\u66f4\u65b0\u4ecd\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u3002\u968f\u7740\u591a\u8f6e\u8bad\u7ec3\u7684\u8fdb\u884c\uff0c\u68af\u5ea6\u7684\u65f6\u5e8f\u7279\u5f81\u53ef\u80fd\u63ed\u793a\u8bad\u7ec3\u8bb0\u5f55\u7684\u9690\u79c1\u3002\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u5a01\u80c1\u6a21\u578b\uff08\u805a\u5408\u65b9\u4e0e\u6570\u636e\u62e5\u6709\u8005\uff09\u7684\u653b\u51fb\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u6570\u636e\u7279\u6027\u4e0b\u7684\u98ce\u9669\u7a0b\u5ea6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\uff0c\u89c2\u5bdf\u591a\u8f6e\u8bad\u7ec3\u4e2d\u6700\u540e\u4e00\u5c42\u68af\u5ea6\u7684\u6f14\u5316\uff0c\u5229\u7528\u5f71\u5b50\u6280\u672f\u5b66\u4e60\u8f6e\u6b21\u7ea7\u68af\u5ea6\u6a21\u5f0f\uff1b\u65e0\u9700\u8bbf\u95ee\u79c1\u6709\u6570\u636e\uff0c\u5373\u53ef\u5bf9\u8bad\u7ec3\u8bb0\u5f55\u8fdb\u884c\u6210\u5458\u63a8\u65ad\uff0c\u4e14\u53ef\u63a8\u5e7f\u5230\u79bb\u6563\u5c5e\u6027\u63a8\u65ad\uff1a\u901a\u8fc7\u5728\u4e0d\u540c\u5c5e\u6027\u5047\u8bbe\u4e0b\u5bf9\u6bd4\u68af\u5ea6\u54cd\u5e94\u3002\u8be5\u653b\u51fb\u4e0e\u6a21\u578b\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u68af\u5ea6\u57fa\u6a21\u578b\uff0c\u4e14\u53ef\u7528\u4e8e\u5206\u7c7b\u4e0e\u56de\u5f52\u60c5\u666f\u3002", "result": "\u5b9e\u9a8c\u5728CIFAR-100\u548cPurchase100\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6210\u5458\u63a8\u65ad\uff0c\u5728Breast Cancer Wisconsin\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5c5e\u6027\u63a8\u65ad\uff0c\u663e\u793a\u653b\u51fb\u6027\u80fd\u5f3a\u4e14\u4e0e\u6587\u732e\u4e2d\u7684\u53e6\u4e00\u653b\u51fb\u5728\u8ba1\u7b97\u4e0e\u5185\u5b58\u5f00\u9500\u65b9\u9762\u76f8\u5f53\u3002\u7ed3\u8bba\u4e0a\uff0c\u591a\u8f6e\u8054\u90a6\u5b66\u4e60\u63d0\u5347\u4e86\u63a8\u65ad\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u805a\u5408\u65b9\u5a01\u80c1\u7a0b\u5ea6\u9ad8\u4e8e\u6570\u636e\u62e5\u6709\u8005\uff0c\u4e14\u6570\u636e\u7684\u6027\u8d28\uff08\u5c24\u5176\u9ad8\u7ef4\u6570\u636e\uff09\u663e\u8457\u5f71\u54cd\u653b\u51fb\u6548\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86\u591a\u8f6eFL\u73af\u5883\u4e0b\u7684\u9690\u79c1\u8106\u5f31\u6027\u53ca\u5a01\u80c1\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u7ed3\u5408\u66f4\u5f3a\u7684\u9632\u62a4\u7b56\u7565\uff08\u5982\u5dee\u5206\u9690\u79c1\u3001\u5b89\u5168\u805a\u5408\u3001\u68af\u5ea6\u88c1\u526a/\u566a\u58f0\u6ce8\u5165\u7b49\uff09\u6765\u964d\u4f4e\u68af\u5ea6\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u5bf9\u9ad8\u7ef4\u3001\u7ed3\u6784\u5316\u6570\u636e\u573a\u666f\u4e0b\u7684\u9632\u62a4\u7b56\u7565\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.14980", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14980", "abs": "https://arxiv.org/abs/2512.14980", "authors": ["Victor M. Yeom Song", "Severi Rissanen", "Arno Solin", "Samuel Kaski", "Mingfei Sun"], "title": "Softly Constrained Denoisers for Diffusion Models", "comment": "18 pages including appendix, 8 figures including appendix, preprint", "summary": "Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.", "AI": {"tldr": "Softly guided denoisers for constrained diffusion without altering loss or sampling loop.", "motivation": "\u5728\u79d1\u5b66\u5e94\u7528\u4e2d\uff0c\u7ea6\u675f\u6761\u4ef6\u5e38\u5e38\u9700\u8981\u88ab\u9075\u5b88\uff0c\u4f46\u628a\u7ea6\u675f\u5f15\u5165\u635f\u5931\u51fd\u6570\u6216\u91c7\u6837\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u8ba9\u6a21\u578b\u504f\u79bb\u771f\u5b9e\u6570\u636e\u5206\u5e03\uff0c\u5c24\u5176\u5728\u7ea6\u675f\u5b58\u5728\u9519\u8bbe\u65f6\u3002", "method": "\u5c06 guidance \u5f0f\u8c03\u6574\u76f4\u63a5\u5d4c\u5165\u5230\u53bb\u566a\u5668\u672c\u8eab\uff0c\u7ed9\u4e88\u5176\u5bf9\u7ea6\u675f\u4e00\u81f4\u6027\u7684\u8f6f\u6027\u504f\u7f6e\uff0c\u800c\u975e\u4fee\u6539\u635f\u5931\u6216\u91c7\u6837\u5faa\u73af\u3002", "result": "\u5177\u6709\u7ea6\u675f\u77e5\u8bc6\u7684\u8f6f\u7ea6\u675f\u53bb\u566a\u5668\u76f8\u6bd4\u6807\u51c6\u53bb\u566a\u5668\u5728\u9075\u5b88\u7ea6\u675f\u65b9\u9762\u66f4\u4f18\uff0c\u540c\u65f6\u4fdd\u6301\u5728\u6570\u636e\u4e0e\u7ea6\u675f\u9519\u914d\u65f6\u7684\u7075\u6d3b\u6027\uff0c\u4e0d\u4f1a\u8fc7\u5ea6\u5f3a\u5236\u9075\u5faa\u9519\u8bef\u7ea6\u675f\u3002", "conclusion": "\u5728\u53bb\u566a\u5668\u5c42\u9762\u6574\u5408\u7ea6\u675f\u4fe1\u606f\u63d0\u4f9b\u4e86\u5bf9\u5916\u90e8\u6b63\u5219\u5316\u6216\u91c7\u6837\u9636\u6bb5\u5f15\u5bfc\u7684\u9c81\u68d2\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4f18\u96c5\u5904\u7406\u7ea6\u675f\u9519\u8bbe\u7684\u79d1\u5b66\u6570\u636e\u3002"}}
{"id": "2512.14982", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.14982", "abs": "https://arxiv.org/abs/2512.14982", "authors": ["Yaniv Leviathan", "Matan Kalman", "Yossi Matias"], "title": "Prompt Repetition Improves Non-Reasoning LLMs", "comment": null, "summary": "When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.", "AI": {"tldr": "\u5728\u4e0d\u4f7f\u7528\u63a8\u7406\uff08\u4e0d\u8fdb\u884c\u94fe\u5f0f\u601d\u8003\uff09\u7684\u6761\u4ef6\u4e0b\uff0c\u91cd\u590d\u8f93\u5165\u63d0\u793a\u53ef\u4ee5\u63d0\u5347\u591a\u79cd\u6a21\u578b\u7684\u6027\u80fd\uff08Gemini\u3001GPT\u3001Claude\u3001Deepseek\uff09\uff0c\u4e14\u4e0d\u589e\u52a0\u751f\u6210\u7684 token \u6570\u91cf\u6216\u5ef6\u8fdf\u3002", "motivation": "\u63a2\u7d22\u7b80\u5355\u7684\u63d0\u793a\u8bbe\u8ba1\u662f\u5426\u80fd\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u8de8\u591a\u6a21\u578b\u7684\u666e\u9002\u6027\u3002", "method": "\u5728\u591a\u6a21\u578b\u4e0a\u5bf9\u6bd4\u5b9e\u9a8c\uff1a\u5728\u4e0d\u542f\u7528\u63a8\u7406\u7684\u6761\u4ef6\u4e0b\uff0c\u5bf9\u540c\u4e00\u4efb\u52a1\u7528\u91cd\u590d\u63d0\u793a\u4e0e\u539f\u59cb\u63d0\u793a\u8fdb\u884c\u5bf9\u6bd4\uff0c\u6d4b\u91cf\u6027\u80fd\u6307\u6807\u3001\u751f\u6210 token \u6570\u91cf\u548c\u7aef\u5230\u7aef latency\u3002", "result": "\u91cd\u590d\u8f93\u5165\u63d0\u793a\u5728\u4e0d\u589e\u52a0\u751f\u6210 token \u6570\u91cf\u6216\u5ef6\u8fdf\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6240\u6d4b\u8bd5\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u793a\u91cd\u590d\uff08\u8f93\u5165\u518d\u5199\uff09\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u6210\u672c\u4f4e\u7684\u65b9\u5f0f\uff0c\u53ef\u5728\u4e0d\u8fdb\u884c\u63a8\u7406\u65f6\u63d0\u9ad8\u591a\u79cd\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u8de8\u6a21\u578b\u9002\u7528\u6027\u3002"}}
{"id": "2512.14991", "categories": ["cs.LG", "math.OC", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2512.14991", "abs": "https://arxiv.org/abs/2512.14991", "authors": ["Hanqing Jin", "Renyuan Xu", "Yanzhao Yang"], "title": "Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes", "comment": null, "summary": "We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.", "AI": {"tldr": "Adaptive model-based RL for controlled diffusion processes with unbounded state spaces using adaptive partitioning of joint state-action space; derives regret bounds via a zooming-dimension concept; validated on high-dimensional problems like multi-asset mean-variance portfolio selection.", "motivation": "Address learning in continuous, high-dimensional diffusion settings with unbounded state spaces and polynomially growing rewards, common in finance, economics, and operations research.", "method": "An adaptive partitioning scheme that maintains estimators of drift, volatility, and rewards within each partition, refining partitions when estimation bias exceeds statistical confidence; model-based RL with regret analysis using a new zooming-dimension notion applicable to unbounded diffusion processes.", "result": "Regret bounds that depend on horizon, state dimension, reward growth, and zooming dimension; bounds recover those for bounded settings and extend guarantees to diffusion-type problems; numerical experiments, including high-dimensional multi-asset mean-variance portfolio selection.", "conclusion": "The work extends theoretical guarantees to unbounded diffusion processes and demonstrates practical effectiveness in high-dimensional problems via numerical experiments, bridging theory and applications in finance and related fields."}}
{"id": "2512.15000", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15000", "abs": "https://arxiv.org/abs/2512.15000", "authors": ["Ruiyi Zhang", "Peijia Qin", "Qi Cao", "Pengtao Xie"], "title": "DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding", "comment": null, "summary": "Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.", "AI": {"tldr": "A coding-focused Reward Model using chain-of-function prompting and meta-learning label correction to improve code generation; achieves 80.9 pass@1 on LiveCodeBench, surpassing o4-mini.", "motivation": "PRMs help LLMs at test time but struggle in coding due to lack of meaningful step decomposition and noisy partial labels. A coding-specific PRM with modular reasoning and robust label correction can improve code generation.", "method": "DreamPRM-Code treats functions as reasoning steps via a Chain-of-Function prompting strategy to induce modular code generation. It includes a meta-learning-based correction mechanism that uses clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels.", "result": "State-of-the-art performance on LiveCodeBench with 80.9 pass@1, surpassing OpenAI o4-mini.", "conclusion": "Demonstrates the effectiveness of modular, function-level reasoning in coding PRMs and shows that meta-learning label correction with bi-level optimization can mitigate label noise to improve test-time performance."}}
{"id": "2512.15008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15008", "abs": "https://arxiv.org/abs/2512.15008", "authors": ["Sandeep Neela"], "title": "Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets", "comment": null, "summary": "Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.", "AI": {"tldr": "\u63d0\u51fa Stock Pattern Assistant (SPA)\u2014\u2014\u4e00\u4e2a\u786e\u5b9a\u6027\u6846\u67b6\uff0c\u5229\u7528\u65e5\u7ebfOHLCV\u6570\u636e\u548c\u4e8b\u4ef6\u6d41\uff0c\u63d0\u53d6\u5355\u8c03\u4ef7\u683c\u8fd0\u884c\u6bb5\u3001\u5bf9\u9f50\u76f8\u5173\u4e8b\u4ef6\u5e76\u751f\u6210\u4e8b\u5b9e\u6027\u3001\u5386\u53f2\u6027\u4e14\u53ef\u5ba1\u8ba1\u7684\u89e3\u91ca\u3002\u5df2\u5728AAPL\u3001NVDA\u3001SCHW\u3001PGR\u7b49\u80a1\u7968\u4e0a\u6f14\u793a\uff0c\u5c55\u73b0\u7a33\u5b9a\u7684\u7ed3\u6784\u5206\u89e3\u4e0e\u4e0a\u4e0b\u6587\u53d9\u4e8b\uff0c\u4e14\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u786e\u8ba4\u5206\u6bb5\u3001\u4e8b\u4ef6\u5bf9\u9f50\u548c\u53d7\u9650\u89e3\u91ca\u5bf9\u53ef\u89e3\u91ca\u6027\u7684\u8d21\u732e\u3002", "motivation": "\u5e02\u573a\u4ef7\u683c\u5f80\u5f80\u88ab\u566a\u58f0\u6240\u906e\u853d\uff0c\u73b0\u6709\u6280\u672f\u6307\u6807\u3001\u56fe\u8868\u542f\u53d1\u5f0f\u548c\u9884\u6d4b\u6a21\u578b\u5728\u900f\u660e\u6027\u4e0e\u53ef\u5ba1\u8ba1\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u4e2a\u53ef\u91cd\u590d\u3001\u53ef\u89e3\u91ca\u7684\u4ef7\u683c\u7ed3\u6784\u5206\u6790\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u5206\u6790\u5e08\u5de5\u4f5c\u6d41\u3001\u98ce\u9669\u8bc4\u4f30\u4e0e\u53ef\u89e3\u91caAI\u7ba1\u7ebf\u3002", "method": "SPA\u662f\u4e00\u4e2a\u786e\u5b9a\u6027\u6846\u67b6\uff1a\uff081\uff09\u4ece\u65e5\u7ebfOHLCV\u6570\u636e\u4e2d\u63d0\u53d6\u5355\u8c03\u4ef7\u683c\u8fd0\u884c\u6bb5\uff1b\uff082\uff09\u901a\u8fc7\u5bf9\u79f0\u76f8\u5173\u7a97\u53e3\u5c06\u516c\u5f00\u4e8b\u4ef6\u9644\u7740\u5230\u4ef7\u683c\u5e8f\u5217\uff1b\uff083\uff09\u751f\u6210\u4e8b\u5b9e\u6027\u3001\u5386\u53f2\u6027\u4e14\u53d7\u7ea6\u675f\u7684\u89e3\u91ca\u3002\u6838\u5fc3\u7ec4\u4ef6\u5305\u62ec\u786e\u5b9a\u6027\u5206\u6bb5\u3001\u4e8b\u4ef6\u5bf9\u9f50\u4e0e\u53d7\u9650\u89e3\u91ca\uff1b\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u8bc4\u4f30\u5404\u7ec4\u6210\u90e8\u5206\u5bf9\u53ef\u89e3\u91ca\u6027\u7684\u8d21\u732e\u3002", "result": "\u5728\u56db\u53ea\u80a1\u7968\uff08AAPL\u3001NVDA\u3001SCHW\u3001PGR\uff09\u4e0a\u5c55\u793a\u51fa\u7a33\u5b9a\u7684\u7ed3\u6784\u5206\u89e3\u548c\u4e0a\u4e0b\u6587\u53d9\u4e8b\uff0c\u8986\u76d6\u4e0d\u540c\u6ce2\u52a8\u6027\u548c\u884c\u4e1a\u7279\u5f81\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u786e\u5b9a\u6027\u5206\u6bb5\u3001\u4e8b\u4ef6\u5bf9\u9f50\u4e0e\u53d7\u9650\u89e3\u91ca\u5bf9\u53ef\u89e3\u91ca\u6027\u7684\u8d21\u732e\u663e\u8457\u3002", "conclusion": "SPA\u4e0d\u662f\u4e00\u4e2a\u9884\u6d4b\u7cfb\u7edf\uff0c\u4e5f\u4e0d\u6784\u6210\u4ea4\u6613\u4fe1\u53f7\uff1b\u5b83\u63d0\u4f9b\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u91cd\u590d\u7684\u5386\u53f2\u4ef7\u683c\u7ed3\u6784\u89c6\u56fe\uff0c\u80fd\u591f\u8865\u5145\u5206\u6790\u5e08\u5de5\u4f5c\u6d41\u3001\u98ce\u9669\u8bc4\u4f30\u4ee5\u53ca\u53ef\u89e3\u91caAI\u7684\u6574\u5408\u3002"}}
{"id": "2512.15011", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.15011", "abs": "https://arxiv.org/abs/2512.15011", "authors": ["Damian Hodel", "Jevin D. West"], "title": "Epistemic diversity across language models mitigates knowledge collapse", "comment": "16 pages, 7 figures", "summary": "The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.", "AI": {"tldr": "\u5728\u81ea\u6211\u8bad\u7ec3\u7684\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c epistemic diversity\uff08\u77e5\u8bc6\u591a\u6837\u6027\uff09\u53ef\u5728\u8fbe\u5230\u6700\u4f73\u6c34\u5e73\u524d\u7f13\u89e3\u201c\u77e5\u8bc6\u5d29\u584c\u201d\uff1b\u8fc7\u5c11\u6a21\u578b\u65e0\u6cd5\u8868\u8fbe\u5b8c\u6574\u5206\u5e03\uff0c\u8fc7\u591a\u6a21\u578b\u53c8\u964d\u4f4e\u4e86\u5355\u6a21\u578b\u8fd1\u4f3c\u80fd\u529b\uff0c\u5bfc\u81f4\u7b2c\u4e00\u6b65\u5c31\u8868\u73b0\u5dee\u3002\u653f\u7b56\u4e0a\u5e94\u5173\u6ce8\u5e76\u4fc3\u8fdb\u9886\u57df\u4e0e\u793e\u533a\u5c42\u9762\u7684\u591a\u6837\u6027\uff0c\u4ee5\u907f\u514d\u5355\u4e00\u5316\u98ce\u9669\u3002", "motivation": "\u63a2\u7a76AI\u7cfb\u7edf\u4e2d\u7684\u77e5\u8bc6\u5d29\u584c\u73b0\u8c61\u662f\u5426\u53ef\u901a\u8fc7\u8de8\u6a21\u578b\u7684\u751f\u6001\u7cfb\u7edf\u591a\u6837\u6027\u6765\u7f13\u89e3\uff0c\u4ee5\u53ca\u6a21\u578b\u751f\u6001\u591a\u6837\u6027\u5bf9\u957f\u671f\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5728\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u5c06\u8bad\u7ec3\u6570\u636e\u5206\u5272\u7ed9\u4e0d\u540c\u6a21\u578b\uff0c\u6784\u5efa\u6a21\u578b\u751f\u6001\u7cfb\u7edf\uff0c\u8fdb\u884c\u5341\u8f6e\u81ea\u6211\u8bad\u7ec3\u8fed\u4ee3\uff0c\u8bc4\u4f30\u4e0d\u540c\u6570\u91cf\u548c\u6570\u636e\u5206\u5272\u65b9\u5f0f\u4e0b\u7684\u6027\u80fd\u4e0e\u7a33\u5b9a\u6027\uff1b\u5206\u6790\u591a\u6837\u6027\u6c34\u5e73\u5bf9\u5d29\u584c\u73b0\u8c61\u7684\u7f13\u89e3\u6548\u5e94\u3002", "result": "\u63d0\u5347\u8ba4\u77e5/\u77e5\u8bc6\u5c42\u9762\u7684 epistemic diversity \u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u51cf\u7f13\u5d29\u584c\uff0c\u4f46\u5b58\u5728\u6700\u4f18\u70b9\uff1b\u8fc7\u5c11\u7684\u591a\u6837\u6027\u65e0\u6cd5\u8986\u76d6\u771f\u5b9e\u5206\u5e03\u7684\u4e30\u5bcc\u6027\uff0c\u8fc7\u591a\u5219\u524a\u5f31\u5355\u6a21\u578b\u5bf9\u771f\u5b9e\u5206\u5e03\u7684\u903c\u8fd1\u80fd\u529b\uff0c\u7b2c\u4e00\u8f6e\u5373\u8868\u73b0\u4e0d\u4f73\u3002\u603b\u4f53\u5448\u73b0\u201c\u591a\u6837\u6027-\u6027\u80fd-\u5d29\u584c\u201d\u7684\u5012U\u5f62\u5173\u7cfb\u3002", "conclusion": "\u5728AI monoculture\u80cc\u666f\u4e0b\uff0c\u5e94\u76d1\u63a7\u7cfb\u7edf\u95f4\u7684\u591a\u6837\u6027\u5e76\u63a8\u52a8\u57df\u5185\u3001\u793e\u533a\u7279\u5b9a\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4ee5\u7ef4\u6301\u6709\u76ca\u7684\u751f\u6001\u591a\u6837\u6027\u5e76\u964d\u4f4e\u77e5\u8bc6\u5d29\u584c\u98ce\u9669\u3002"}}
{"id": "2512.15068", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15068", "abs": "https://arxiv.org/abs/2512.15068", "authors": ["Debu Sinha"], "title": "The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems", "comment": "12 pages, 2 figures, 6 tables", "summary": "Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the \"semantic illusion\": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.", "AI": {"tldr": "\u5bf9\u8bdd\u5f0f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u201c\u8bed\u4e49\u5e7b\u89c9\u201d\u95ee\u9898\uff1a\u5916\u63a8\u548c\u5d4c\u5165\u5f0f\u68c0\u6d4b\u7684\u5c40\u9650\u6027\u88ab\u63ed\u793a\uff0c\u63d0\u51fa\u91c7\u7528 conformal prediction \u7684\u68c0\u6d4b\u6846\u67b6\u4ee5\u63d0\u4f9b\u6709\u9650\u6837\u672c\u8986\u76d6\u7387\uff0c\u4f46\u5b9e\u9645\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5d4c\u5165\u5f0f\u68c0\u6d4b\u4ecd\u7136\u5b58\u5728\u9ad8\u5047\u9633\u6027\u3002", "motivation": "\u63ed\u793a\u73b0\u6709\u57fa\u4e8e\u5d4c\u5165\u548cNLI\u7684\u5e7b\u89c9\u68c0\u6d4b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u751f\u4ea7\u7ea7\u522b\u7684 RAG \u7cfb\u7edf\u4e2d\u5bf9\u9ad8\u53ef\u9760\u6027\u68c0\u6d4b\u7684\u9700\u6c42\u3002\u901a\u8fc7\u5f15\u5165\u5177\u6709\u9650\u6837\u672c\u8986\u76d6\u4fdd\u8bc1\u7684 conformal prediction\uff0c\u8bc4\u4f30\u68c0\u6d4b\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u5e7b\u89c9\u6570\u636e\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u5bf9\u9f50\u5e76\u5e94\u7528 conformal prediction \u7ed9\u5e7b\u89c9\u68c0\u6d4b\uff0c\u5728\u7ea6 600 \u6761\u4f8b\u6821\u51c6\u6837\u672c\u4e0a\u6784\u5efa\u8986\u76d6\u7387\u4fdd\u8bc1\uff1b\u5728 Natural Questions \u7684\u5408\u6210\u5e7b\u89c9\u6570\u636e\u4e0a\u8fbe\u5230 94% \u8986\u76d6\u4e14 0% \u5047\u9633\u6027\uff1b\u5728\u4e09\u7ec4\u771f\u5b9e\u5e7b\u89c9\u57fa\u51c6\uff08HaluEval\u3001RAGTruth\u3001WikiBio\uff09\u4e0a\u5bf9\u6bd4\u5d4c\u5165\u5f0f\u65b9\u6cd5\uff08\u5305\u62ec OpenAI text-embedding-3-large\u3001\u8de8\u7f16\u7801\u5668\u6a21\u578b\uff09\u4ee5\u53ca GPT-4 \u4f5c\u4e3a\u5224\u5b9a\u8005\u7684\u8868\u73b0\u3002", "result": "\u5408\u6210\u5e7b\u89c9\u6570\u636e\u4e0a\uff0c85%~100% \u7684\u7f6e\u4fe1\u8986\u76d6\u5b9e\u73b0\uff1b\u771f\u5b9e\u5e7b\u89c9\u6570\u636e\u4e0a\uff0c\u5d4c\u5165\u5f0f\u68c0\u6d4b\u7684\u5047\u9633\u6027\u7387\u6781\u9ad8\uff0cHaluEval 100%\u3001RAGTruth 88%\u3001WikiBio 50%\uff1b\u800c\u5c06 GPT-4 \u4f5c\u4e3a\u5224\u5b9a\u8005\u65f6 FPR \u964d\u81f3 7%\uff0895% CI\uff1a3.4%\u201313.7%\uff09\uff0c\u8868\u660e\u901a\u8fc7\u63a8\u7406\u53ef\u89e3\u51b3\u8be5\u4efb\u52a1\uff1b\u6574\u4f53\u7ed3\u8bba\u8ba4\u4e3a\u201c\u8bed\u4e49\u5e7b\u89c9\u201d\u4f7f\u5d4c\u5165\u5f0f\u68c0\u6d4b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\u3002", "conclusion": "\u5d4c\u5165\u5f0f\u68c0\u6d4b\u5728\u591a\u4efb\u52a1\u3001\u591a\u6a21\u578b\u573a\u666f\u4e0b\u5e76\u4e0d\u8db3\u4ee5\u89e3\u51b3\u751f\u4ea7\u7ea7 RAG \u7684\u5e7b\u89c9\u68c0\u6d4b\u95ee\u9898\uff1b\u9700\u8981\u91c7\u7eb3\u4ee5 conformal prediction \u4e3a\u57fa\u7840\u7684\u68c0\u6d4b\u6846\u67b6\u6216\u5176\u4ed6\u975e\u5d4c\u5165\u5f0f/\u63a8\u7406\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u7a33\u5b9a\u7684\u5e7b\u89c9\u68c0\u6d4b\u4e0e\u53ef\u63a7\u8bef\u62a5\u7387\u3002\u8be5\u73b0\u8c61\u88ab\u547d\u540d\u4e3a\u201c\u8bed\u4e49\u9519\u89c9\u201d\u5e76\u5728\u4e0d\u540c\u6a21\u578b\u3001\u5d4c\u5165\u67b6\u6784\u548c\u4efb\u52a1\u7c7b\u578b\u4e0a\u666e\u904d\u5b58\u5728\u3002"}}
{"id": "2512.15082", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15082", "abs": "https://arxiv.org/abs/2512.15082", "authors": ["Wanfu Gao", "Zebin He", "Jun Gao"], "title": "The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks", "comment": null, "summary": "Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.", "AI": {"tldr": "\u63d0\u51fa FEAML\uff0c\u5728\u591a\u6807\u7b7e\u5b66\u4e60\u4e2d\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\uff0c\u901a\u8fc7\u5143\u6570\u636e\u4e0e\u6807\u7b7e\u5171\u73b0\u77e9\u9635\u5f15\u5bfc LLM \u8fdb\u884c\u4ee3\u7801\u751f\u6210\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7279\u5f81\u5e76\u4ee5\u6a21\u578b\u51c6\u786e\u5ea6\u8bc4\u4f30\u53ca Pearson \u76f8\u5173\u6027\u68c0\u6d4b\u5197\u4f59\u6027\uff0c\u5f62\u6210\u5e26\u53cd\u9988\u7684\u81ea\u6211\u6539\u8fdb\u5faa\u73af\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u5176\u4ed6\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u7279\u5f81\u5de5\u7a0b\u5c1a\u672a\u5e94\u7528\u4e8e\u591a\u6807\u7b7e\u5b66\u4e60\uff0c\u96be\u4ee5\u5efa\u6a21\u590d\u6742\u6807\u7b7e\u4f9d\u8d56\uff0c\u4e14\u672a\u9488\u5bf9\u591a\u6807\u7b7e\u4efb\u52a1\u7279\u5f81\u8fdb\u884c\u4e13\u95e8\u8bbe\u8ba1\u3002\u9700\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u80fd\u6355\u6349\u6807\u7b7e\u95f4\u5173\u7cfb\u7684\u7279\u5f81\u5de5\u7a0b\u3002", "method": "\u63d0\u51fa FEAML\uff1a\u5229\u7528 LLM \u7684\u4ee3\u7801\u751f\u6210\u529f\u80fd\uff0c\u7ed3\u5408\u6570\u636e\u5143\u6570\u636e\u548c\u6807\u7b7e\u5171\u73b0\u77e9\u9635\uff0c\u5f15\u5bfc LLM \u4ee5\u4efb\u52a1\u76ee\u6807\u4e3a\u5bfc\u5411\u751f\u6210\u7279\u5f81\u4ee3\u7801\uff1b\u5bf9\u751f\u6210\u7684\u7279\u5f81\u8fdb\u884c\u6a21\u578b\u51c6\u786e\u5ea6\u8bc4\u4f30\u4ee5\u8861\u91cf\u6548\u7528\uff0c\u540c\u65f6\u7528 Pearson \u7cfb\u6570\u68c0\u6d4b\u5197\u4f59\u6027\uff1b\u5c06\u8bc4\u4f30\u7ed3\u679c\u53cd\u9988\u7ed9 LLM \u4ee5\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u751f\u6210\uff0c\u5f62\u6210\u81ea\u6211\u6539\u8fdb\u7684\u7279\u5f81\u5de5\u7a0b\u5faa\u73af\uff1b\u7ed3\u5408 LLM \u548c\u53cd\u9988\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u5de5\u7a0b\u3002", "result": "\u5728\u591a\u79cd\u591a\u6807\u7b7e\u6570\u636e\u96c6\u4e0a\u8bc1\u660e FEAML \u7684\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u3002", "conclusion": "FEAML \u4e3a\u591a\u6807\u7b7e\u5b66\u4e60\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u81ea\u6211\u63d0\u5347\u7684\u7279\u5f81\u5de5\u7a0b\u8303\u5f0f\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u6807\u7b7e\u4f9d\u8d56\u5e76\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2512.15086", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2512.15086", "abs": "https://arxiv.org/abs/2512.15086", "authors": ["Hongjin Mi", "Huiqiang Lun", "Changhong Mou", "Yeyu Zhang"], "title": "PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network", "comment": null, "summary": "Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \\emph{P}hysics-\\emph{i}nformed \\emph{P}artition \\emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.", "AI": {"tldr": "A Physics-informed Partition Penalty Deep Operator Network (PIP^2 Net) that uses partition-of-unity based penalty to regularize trunk outputs, improving expressiveness and stability; shows improved accuracy and robustness over DeepONet, PI-DeepONet, POU-DeepONet on Burgers, Allen\u2013Cahn, diffusion\u2013reaction PDEs.", "motivation": "To address instability and mode imbalance in trunk networks of operator-learning architectures and to leverage the locality and stability of partition-of-unity (PoU) methods for better generalization with potentially smaller training datasets.", "method": "Introduce PIP^2 Net, a revised PoU-based regularization and a simplified partition penalty integrated into the DeepONet/PI-DeepONet framework. The partition penalty coordinates trunk outputs to enhance expressiveness while preserving flexibility.", "result": "Empirical evaluation on three nonlinear PDEs (viscous Burgers, Allen\u2013Cahn, and diffusion\u2013reaction system) shows that PIP^2 Net consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.", "conclusion": "PoU-based partition penalties provide a principled, stable regularization for operator learning, enabling more expressive yet flexible trunk outputs; PIP^2 Net advances SOTA in data-efficient operator learning for parameterized PDEs."}}
{"id": "2512.15088", "categories": ["cs.LG", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2512.15088", "abs": "https://arxiv.org/abs/2512.15088", "authors": ["Xianglin Wu", "Chiheb Ben Hammouda", "Cornelis W. Oosterlee"], "title": "SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs", "comment": null, "summary": "Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.", "AI": {"tldr": "\u63d0\u51fa SigMA\uff08Signature Multi-head Attention\uff09\u2014\u2014\u5c06\u8def\u5f84\u7b7e\u540d\u4e0e\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7ed3\u5408\u7684\u67b6\u6784\uff0c\u7528\u4e8efBm\u9a71\u52a8\u7684SDE\u53c2\u6570\u63a8\u65ad\uff0c\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u4e0a\u4f18\u4e8e CNN/LSTM/Transformer/Deep Signature\uff0c\u4e14\u6a21\u578b\u66f4\u7d27\u51d1\u3001\u9c81\u68d2\u3002", "motivation": "\u975e\u9a6c\u5c14\u53ef\u592b\u3001\u7f3a\u4e4f\u534a\u9798\u8fc7\u7a0b\u7ed3\u6784\u7684 fBm \u9a71\u52a8 SDE \u7ed9\u53c2\u6570\u4f30\u8ba1\u5e26\u6765\u6311\u6218\u3002\u8def\u5f84\u7b7e\u540d\u63d0\u4f9b\u9ad8\u6548\u3001\u7d27\u51d1\u7684\u7279\u5f81\u8868\u793a\uff0c\u6ce8\u610f\u529b\u673a\u5236\u53ef\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u957f\u7a0b\u76f8\u5173\u6027\u4e0e\u7ed3\u6784\u4fe1\u606f\u3002\u5efa\u7acb\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u63a8\u65ad\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u4e0e\u6a21\u578b\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa SigMA\uff0c\u5c06\u8def\u5f84\u7b7e\u540d\u4e0e\u591a\u5934\u81ea\u6ce8\u610f\u529b\u76f8\u7ed3\u5408\uff0c\u8f85\u4ee5\u5377\u79ef\u9884\u5904\u7406\u5c42\u548c\u591a\u5c42\u611f\u77e5\u673a\u4ee5\u5b9e\u73b0\u6709\u6548\u7279\u5f81\u7f16\u7801\u3002\u4ee5\u5408\u6210\u8def\u5f84\uff08fBm \u9a71\u52a8\u7684 SDE\u3001fractional OU\u3001rough Heston\uff09\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u76ee\u6807\u5305\u62ec\u8d6b\u65af\u7279\u6307\u6570\u4f30\u8ba1\u4e0e\u8054\u5408\u591a\u53c2\u6570\u63a8\u65ad\uff0c\u5e76\u5177\u5907\u5bf9\u672a\u77e5\u8f68\u8ff9\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0e\u4e24\u7ec4\u771f\u5b9e\u6570\u636e\uff08\u80a1\u6307 realized volatility\u3001\u9502\u79bb\u5b50\u7535\u6c60\u9000\u5316\uff09\u4e0a\u7684\u5b9e\u9a8c\u4e2d\uff0cSigMA \u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u4e0e\u6a21\u578b\u7d27\u51d1\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e CNN\u3001LSTM\u3001\u539f\u751f Transformer \u4e0e Deep Signature \u57fa\u7ebf\u3002", "conclusion": "\u5c06\u8def\u5f84\u7b7e\u540d\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e00\u79cd\u5bf9\u5177\u6709\u7c97\u7cfd\u6216\u5f3a\u6301\u7eed\u65f6\u95f4\u7ed3\u6784\u7684\u968f\u673a\u7cfb\u7edf\u53c2\u6570\u63a8\u65ad\u7684\u6709\u6548\u3001\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2512.15116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15116", "abs": "https://arxiv.org/abs/2512.15116", "authors": ["Runze Li", "Hanchen Wang", "Wenjie Zhang", "Binghao Li", "Yu Zhang", "Xuemin Lin", "Ying Zhang"], "title": "FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation", "comment": "This work has been submitted to the IEEE for possible publication. 15 pages, 8 figures", "summary": "Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF", "AI": {"tldr": "\u63d0\u51fa FADTI\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5085\u91cc\u53f6\u504f\u7f6e\u6295\u5f71\u5728\u9891\u57df\u4e2d\u5f15\u5165\u7279\u5f81\u8c03\u5236\u7684\u5148\u9a8c\uff0c\u5e76\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u4e0e\u95e8\u63a7\u5377\u79ef\u8fdb\u884c\u65f6\u5e8f\u5efa\u6a21\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6570\u636e\u96c6\uff08\u542b\u65b0\u751f\u7269\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff09\u4e0a\uff0c\u5c24\u5176\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\uff0c\u4f20\u611f\u5668\u6545\u969c\u548c\u4e0d\u89c4\u5219\u91c7\u6837\u7ecf\u5e38\u5bfc\u81f4\u5927\u91cf\u7f3a\u5931\u503c\u3002\u73b0\u6709\u57fa\u4e8e Transformer \u548c\u6269\u6563\u7684\u63d2\u8865\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u7684\u5148\u9a8c\uff08inductive bias\uff09\u548c\u9891\u7387\u611f\u77e5\uff0c\u5bfc\u81f4\u5bf9\u7ed3\u6784\u5316\u7f3a\u5931\u6a21\u5f0f\u548c\u5206\u5e03\u6f02\u79fb\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa FADTI\uff1a\u4e00\u4e2a\u6269\u6563\u5f0f\u751f\u6210\u63d2\u8865\u6846\u67b6\uff0c\u52a0\u5165\u53ef\u5b66\u4e60\u7684\u5085\u91cc\u53f6\u504f\u7f6e\u6295\u5f71\uff08FBP\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u8c31\u57fa\u5b9e\u73b0\u5bf9\u7a33\u5b9a\u4e0e\u975e\u7a33\u5b9a\u6a21\u5f0f\u7684\u81ea\u9002\u5e94\u7f16\u7801\uff0c\u5c06\u9891\u57df\u5148\u9a8c\u6ce8\u5165\u5230\u751f\u6210\u63d2\u8865\u8fc7\u7a0b\uff1b\u540c\u65f6\u7528\u81ea\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u5377\u79ef\u8fdb\u884c\u65f6\u5e8f\u5efa\u6a21\u3002FBP \u652f\u6301\u591a\u79cd\u8c31\u57fa\uff0c\u80fd\u591f\u5728\u9891\u57df\u5f15\u5165\u5bf9\u4e0d\u540c\u65f6\u95f4\u7279\u6027\u7684\u504f\u7f6e\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u6570\u636e\u96c6\uff08\u542b\u65b0\u5f15\u5165\u7684\u751f\u7269\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff09\u4e0a\uff0cFADTI \u5728\u7f3a\u5931\u7387\u8f83\u9ad8\u65f6\u5bf9\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u663e\u793a\u51fa\u5bf9\u7ed3\u6784\u5316\u7f3a\u5931\u548c\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5f15\u5165\u9891\u57df\u53ef\u5b66\u4e60\u504f\u7f6e\u7684\u7279\u5f81\u8c03\u5236\u4e0e\u6269\u6563\u5f0f\u63d2\u8865\uff0c\u63d0\u5347\u4e86\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u9ad8\u7f3a\u5931\u7387\u573a\u666f\u4e0b\u3002\u4ee3\u7801\u5f00\u653e\u53ef\u5f97\u3002"}}
{"id": "2512.15140", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15140", "abs": "https://arxiv.org/abs/2512.15140", "authors": ["Roland Baatz"], "title": "Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany", "comment": "13 pages, 3 figures", "summary": "This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).\n  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.\n  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?", "AI": {"tldr": "\u5bf9\u5fb7\u56fdNUTS-3\u533a\u57df\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u7684ML\u6a21\u578b\u5b58\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u6cdb\u5316\u7f3a\u53e3\uff1b\u5728\u7a7a\u95f4\u5206\u5272\u7684\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u5bf9\u672a\u6765\u5e74\u4efd\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e14SHAP\u7b49\u89e3\u91ca\u65b9\u6cd5\u5728\u6a21\u578b\u6cdb\u5316\u6027\u5dee\u65f6\u4ecd\u53ef\u80fd\u7ed9\u51fa\u770b\u4f3c\u53ef\u4fe1\u7684\u7279\u5f81\u91cd\u8981\u6027\u3002", "motivation": "\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u5728\u519c\u4e1a/\u73af\u5883\u6570\u636e\u4e2d\u7684\u8de8\u65f6\u7a7a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u68c0\u89c6\u540e\u9a8c\u89e3\u91ca\u5728\u771f\u5b9e\u5e7f\u4e49\u6027\u4e0d\u8db3\u65f6\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u5bf9\u6bd4\u8bc4\u4f30\uff1a\u96c6\u6210\u6811\u6a21\u578b\uff08XGBoost\u3001Random Forest\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08LSTM\u3001TCN\uff09\uff0c\u4f7f\u7528\u9ad8\u8d28\u91cf\u3001\u957f\u671f\u7684\u5fb7\u56fdNUTS-3\u4f5c\u7269\u4ea7\u91cf\u6570\u636e\uff1b\u8fdb\u884c\u7a7a\u95f4\u5206\u5272\u4e0e\u65f6\u95f4\u72ec\u7acb\u5e74\u5ea6\u7684\u9a8c\u8bc1\uff0c\u5206\u6790SHAP\u7279\u5f81\u91cd\u8981\u6027\uff0c\u8ba8\u8bba domain-aware\u9a8c\u8bc1\u53ca\u6df7\u5408\u5efa\u6a21\u7b56\u7565\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u7a7a\u95f4\u5206\u5272\u7684\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u53ef\u89c2\uff0c\u4f46\u5728\u65f6\u95f4\u72ec\u7acb\u9a8c\u8bc1\u5e74\u4e0a\u663e\u8457\u4e0b\u964d\uff1b\u9ad8\u6d4b\u8bd5\u96c6\u51c6\u786e\u6027\u5e76\u4e0d\u7b49\u540c\u4e8e\u7a33\u5065\u7684\u65f6\u95f4\u6cdb\u5316\u80fd\u529b\uff1bSHAP\u5728\u6cdb\u5316\u5dee\u7684\u6a21\u578b\u4e0a\u4ecd\u53ef\u80fd\u7ed9\u51fa\u770b\u4f3c\u53ef\u4fe1\u7684\u89e3\u91ca\u3002\u5f3a\u8c03\u9700\u8981\u57fa\u4e8e\u9a8c\u8bc1\u7684\u89e3\u91ca\uff0c\u5021\u5bfc\u9886\u57df\u611f\u77e5\u7684\u9a8c\u8bc1\u3001\u6df7\u5408\u5efa\u6a21\u4e0e\u5bf9\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u66f4\u4e25\u683c\u5ba1\u67e5\u3002", "conclusion": "\u5f3a\u8c03\u5728\u73af\u5883\u6570\u636e\u79d1\u5b66\u4e2d\u5bf9\u6cdb\u5316\u4e0e\u89e3\u91ca\u7684\u8bc4\u4ef7\u9700\u8981\u66f4\u5065\u58ee\u7684\u9a8c\u8bc1\u6846\u67b6\uff1b\u547c\u5401\u9886\u57df\u5bfc\u5411\u7684\u9a8c\u8bc1\u3001\u6df7\u5408\u5efa\u6a21\u548c\u5bf9\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u8c28\u614e\u4f7f\u7528\uff0c\u4ee5\u63d0\u5347\u5bf9\u6570\u636e\u9a71\u52a8\u519c\u4e1a\u9884\u6d4b\u7684\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2512.15169", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15169", "abs": "https://arxiv.org/abs/2512.15169", "authors": ["Chengguang Ou", "Yixin Zhuang"], "title": "Understanding NTK Variance in Implicit Neural Representations", "comment": null, "summary": "Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u89c6\u89d2\uff0c\u5c06 INR \u7684\u8c31\u504f\u5dee\u4e0e NTK \u6761\u4ef6\u6570\u8054\u7cfb\u8d77\u6765\uff0c\u63ed\u793a\u4f4d\u7f6e\u7f16\u7801\u3001\u7403\u9762\u5f52\u4e00\u5316\u3001Hadamard \u8c03\u5236\u7b49\u7ec4\u4ef6\u5982\u4f55\u901a\u8fc7\u6539\u53d8\u8f93\u5165\u76f8\u4f3c\u6027\u4e0e\u65b9\u5dee\u5206\u91cf\u6765\u964d\u4f4e NTK \u7684\u7279\u5f81\u503c\u65b9\u5dee\uff0c\u4ece\u800c\u63d0\u5347\u6536\u655b\u901f\u5ea6\u4e0e\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u89e3\u91ca\u4e3a\u4f55 INR \u5e38\u89c4\u4ee5\u6709\u9650\u8f93\u5165\u4ea4\u4e92\u5bfc\u81f4\u8c31\u504f\u5dee\uff0c\u4ee5\u53ca\u73b0\u6709\u5de5\u4f5c\u5982\u4f55\u4ec5\u5728NTK\u5c42\u9762\u89e3\u91ca\u800c\u672a\u7cfb\u7edf\u89e3\u8026\u5404\u79cd\u67b6\u6784\u5f71\u54cd\u3002", "method": "\u63a8\u5bfc\u5e38\u89c1 INR \u7ec4\u4ef6\u7684\u65b9\u5dee\u5206\u89e3\uff0c\u7ed9\u51fa\u95ed\u5f0f\u89e3\uff0c\u5206\u6790\u8f93\u5165\u76f8\u4f3c\u6027\u3001\u5c3a\u5ea6\u56e0\u5b50\u5bf9NTK\u672c\u5f81\u503c\u65b9\u5dee\u7684\u5f71\u54cd\uff1b\u901a\u8fc7\u6570\u503c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7ed9\u51fa\u4f4d\u7f6e\u7f16\u7801 reshapes input similarity\u3001\u7403\u9762\u5f52\u4e00\u5316\u901a\u8fc7\u5c42\u5185\u7f29\u653e\u964d\u4f4e\u65b9\u5dee\u3001Hadamard \u8c03\u5236\u5f15\u5165\u4e0b\u9650\u7684\u76f8\u4f3c\u6027\u56e0\u5b50\u4ece\u800c\u5b9e\u73b0\u4e58\u6cd5\u964d\u65b9\u5dee\uff1b\u5b9e\u9a8c\u663e\u793a\u66f4\u5feb\u66f4\u7a33\u5b9a\u6536\u655b\uff0c\u91cd\u5efa\u8d28\u91cf\u63d0\u5347\u3002", "conclusion": "\u7edf\u4e00\u89c6\u89d2\u89e3\u91ca\u4e86\u591a\u79cd INR \u67b6\u6784\u5982\u4f55\u901a\u8fc7\u6539\u5584NTK \u6761\u4ef6\u6765\u7f13\u89e3\u8c31\u504f\u5dee\uff1b\u5efa\u8bae\u5728\u8bbe\u8ba1 INR \u65f6\u5173\u6ce8\u5c0f\u96c6\u5408\u7684\u76f8\u4f3c\u6027\u56e0\u7d20\u548c\u5c3a\u5ea6\u9879\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2512.15206", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15206", "abs": "https://arxiv.org/abs/2512.15206", "authors": ["Liyu Zhang", "Yejia Liu", "Kwun Ho Liu", "Runxi Huang", "Xiaomin Ouyang"], "title": "Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT", "comment": null, "summary": "In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.", "AI": {"tldr": "\u63d0\u51faChorus\uff0c\u4e00\u79cd\u9762\u5411IoT\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u6570\u636e\u65e0\u5173\u7684\u6a21\u578b\u5b9a\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u65e0\u76ee\u6807\u57df\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u672a\u89c1\u90e8\u7f72\u73af\u5883\u4e2d\u7684\u4e0a\u4e0b\u6587\u6f02\u79fb\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u7269\u8054\u7f51\u5e94\u7528\u4e2d\u4f20\u611f\u5668\u56e0\u653e\u7f6e\u4f4d\u7f6e\u3001\u73af\u5883\u7b49\u4e0a\u4e0b\u6587\u56e0\u7d20\u5bfc\u81f4\u6570\u636e\u6a21\u5f0f\u548c\u6027\u80fd\u663e\u8457\u53d8\u5316\uff0c\u73b0\u6709\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e0a\u4e0b\u6587\u6216\u96be\u4ee5\u5904\u7406 unseen context shifts\u3002", "method": "Chorus\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u8de8\u6a21\u6001\u91cd\u5efa\uff0c\u5c06\u672a\u6807\u6ce8\u7684\u4f20\u611f\u5668\u6570\u636e\u548c\u8bed\u8a00\u4e0a\u4e0b\u6587\u5d4c\u5165\u8fdb\u884c\u5bf9\u9f50\uff0c\u540c\u65f6\u5bf9\u4e0a\u4e0b\u6587\u5d4c\u5165\u7a7a\u95f4\u8fdb\u884c\u6b63\u5219\u5316\u4ee5\u5b66\u4e60\u9c81\u68d2\u3001\u6cdb\u5316\u7684\u4e0a\u4e0b\u6587\u8868\u8fbe\u3002\u968f\u540e\u5728\u6709\u9650\u6807\u6ce8\u6837\u672c\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u95e8\u63a7\u5934\uff0c\u4ee5\u52a8\u6001\u5e73\u8861\u4f20\u611f\u5668\u4fe1\u53f7\u548c\u4e0a\u4e0b\u6587\u5bf9\u6a21\u578b\u7684\u8d21\u732e\u3002\u4e3a\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u5f15\u5165\u4e0a\u4e0b\u6587\u7f13\u5b58\u673a\u5236\uff0c\u5728\u68c0\u6d4b\u5230\u4e0a\u4e0b\u6587\u6f02\u79fb\u65f6\u624d\u66f4\u65b0\u7f13\u5b58\u3002", "result": "\u5728IMU\u3001\u8bed\u97f3\u4e0eWiFi\u4f20\u611f\u4efb\u52a1\u4e0a\u51fa\u73b0\u591a\u79cd\u4e0a\u4e0b\u6587\u6f02\u79fb\u7684\u60c5\u666f\uff0cChorus\u76f8\u8f83\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u5728 unseen contexts \u4e2d\u6027\u80fd\u63d0\u5347\u6700\u591a\u8fbe11.3%\uff0c\u4e14\u5728\u667a\u80fd\u624b\u673a\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u5ef6\u8fdf\u4fdd\u6301\u53ef\u6bd4\u3002", "conclusion": "\u9762\u5411\u672a\u89c1\u90e8\u7f72\u4e0a\u4e0b\u6587\u7684\u4f20\u611f\u6570\u636e\u81ea\u9002\u5e94\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cChorus\u901a\u8fc7\u4e0a\u4e0b\u6587\u91cd\u5efa\u3001\u9c81\u68d2\u5d4c\u5165\u3001\u95e8\u63a7\u878d\u5408\u4e0e\u7f13\u5b58\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u3001\u6570\u636e\u65e0\u9700\u6c42\u7684\u5b9a\u5236\u5316\u6a21\u578b\u9002\u914d\u3002"}}
{"id": "2512.15250", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15250", "abs": "https://arxiv.org/abs/2512.15250", "authors": ["Youssef Ghallab", "Omar Iraqy", "Mohamed Kandil", "Mohamed Ashraf", "Saadeldine Eletter", "Morougue Ghazal", "Ayman Khalafallah", "Nagwa El-Makky"], "title": "Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis", "comment": "Published at NeurIPS 2025 Workshop on Foundation Models for the Brain and Body", "summary": "Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e foundation-model \u601d\u8def\u7684\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u5206\u6790\u6846\u67b6\u3002\u901a\u8fc7\u5bf9 ECG \u4f7f\u7528 CBraMod \u8fdb\u884c\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5e76\u5f15\u5165\u53cc\u91cd masking \u6355\u6349\u540c\u5bfc\u8054\u4e4b\u95f4\u4e0e\u8de8\u5bfc\u8054\u5173\u7cfb\uff0c\u540c\u65f6\u5bf9 EEG \u4f7f\u7528\u9884\u8bad\u7ec3\u7684 CBraMod\uff0c\u6784\u5efa\u5bf9\u79f0\u7684 ECG \u7f16\u7801\u5668\u3002\u5c06\u4e24\u79cd\u6a21\u6001\u7684\u8868\u793a\u901a\u8fc7\u7b80\u5355\u7684\u5d4c\u5165\u62fc\u63a5\u8fdb\u884c\u878d\u5408\uff0c\u5206\u7c7b\u5934\u5b66\u4e60\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5728\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u63a5\u8fd1\u72b6\u6001-of-the-art\uff0c\u5c55\u793a\u4e86\u8bbe\u8ba1\u826f\u597d\u7684\u751f\u7406\u4fe1\u53f7\u7f16\u7801\u5668\u5728\u6807\u7b7e\u53d7\u9650\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u751f\u7406\u4fe1\u53f7\uff08ECG/EEG\uff09\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\uff0c\u4f46\u591a\u6a21\u6001\u6807\u6ce8\u7a00\u7f3a\u3001\u6a21\u6001\u95f4\u5dee\u5f02\u5927\uff0c\u96be\u4ee5\u53d1\u6325\u8054\u5408\u4fe1\u606f\u3002\u9700\u8981\u57fa\u4e8e foundation-model \u7684\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u7f16\u7801\u5668\u6765\u83b7\u5f97\u4e30\u5bcc\u4e14\u53ef\u8fc1\u79fb\u7684\u8868\u793a\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u878d\u5408\u3002", "method": "\u5bf9 ECG \u4f7f\u7528 CBraMod \u7f16\u7801\u5668\u8fdb\u884c\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u63d0\u51fa\u53cc masking \u7b56\u7565\u4ee5\u6355\u6349\u5cf0\u95f4\u548c\u5bfc\u8054\u5185\u7684\u4f9d\u8d56\u5173\u7cfb\uff1b\u5bf9 EEG \u4f7f\u7528\u9884\u8bad\u7ec3\u7684 CBraMod\uff0c\u5e76\u5bf9 ECG \u8bad\u7ec3\u4e00\u4e2a\u5bf9\u79f0\u7684\u7f16\u7801\u5668\uff1b\u901a\u8fc7\u7b80\u5355\u7684\u5d4c\u5165\u62fc\u63a5\u5c06\u4e24\u6a21\u6001\u8868\u793a\u878d\u5408\uff0c\u8bad\u7ec3\u5206\u7c7b\u5934\u4ee5\u5b66\u4e60\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u63a5\u8fd1/\u8fd1\u4f3c state-of-the-art \u7684\u6027\u80fd\uff0c\u8868\u660e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u751f\u7406\u4fe1\u53f7\u7f16\u7801\u5668\u5373\u4f7f\u91c7\u7528\u7b80\u5355\u7684\u878d\u5408\u65b9\u5f0f\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u6027\u80fd\uff1b\u51f8\u663e foundation-model \u98ce\u683c\u5728\u751f\u7406\u4fe1\u53f7\u6574\u4f53\u7406\u89e3\u3001\u6807\u7b7e\u9ad8\u6548\u53ca\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u4ee5\u57fa\u7840\u6a21\u578b\u601d\u8def\u8bbe\u8ba1\u7684 ECG/EEG \u7f16\u7801\u5668\u7ed3\u5408\u7b80\u6613\u878d\u5408\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u6807\u6ce8\u53cb\u597d\u3001\u5bf9\u5065\u5eb7\u4e0e\u60c5\u611f\u8ba1\u7b97\u7b49\u5e94\u7528\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.15285", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.15285", "abs": "https://arxiv.org/abs/2512.15285", "authors": ["Aleksei Shestov", "Anton Klenitskiy", "Daria Denisova", "Amurkhan Dzagkoev", "Daniil Petrovich", "Andrey Savchenko", "Maksim Makarenko"], "title": "Topological Metric for Unsupervised Embedding Quality Evaluation", "comment": null, "summary": "Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.15378", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15378", "abs": "https://arxiv.org/abs/2512.15378", "authors": ["Honey Singh Chauhan", "Zahraa S. Abdallah"], "title": "A Regime-Aware Fusion Framework for Time Series Classification", "comment": null, "summary": "Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.15405", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15405", "abs": "https://arxiv.org/abs/2512.15405", "authors": ["Jianfei Ma", "Wee Sun Lee"], "title": "EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning", "comment": null, "summary": "At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.", "AI": {"tldr": "\u63d0\u51fa EUBRL\uff1a\u4e00\u79cd\u5229\u7528 epistemic \u6307\u5bfc\u7684\u8d1d\u53f6\u65af\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u65e0\u9650-horizon \u6298\u6263 MDP \u4e2d\u5b9e\u73b0\u8fd1\u6700\u5c0f\u6781\u5927\u56de\u62a5\u4e0e\u6837\u672c\u590d\u6742\u5ea6\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u7a00\u758f\u5956\u52b1\u3001\u957f\u65f6\u7a0b\u548c\u968f\u673a\u6027\u4efb\u52a1\u4e2d\u5c55\u73b0\u826f\u597d\u6837\u672c\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "motivation": "\u5728\u5df2\u77e5\u4e0e\u672a\u77e5\u7684\u8fb9\u754c\uff0c\u667a\u80fd\u4f53\u4e0d\u53ef\u907f\u514d\u5730\u9762\u4e34\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6289\u62e9\u3002 epistemic \u4e0d\u786e\u5b9a\u6027\u53cd\u6620\u4e86\u56e0\u77e5\u8bc6\u6709\u9650\u800c\u5bfc\u81f4\u7684\u7cfb\u7edf\u6027\u4e0d\u786e\u5b9a\u6027\u3002\u9700\u8981\u4e00\u4e2a\u80fd\u5728\u6b64\u7c7b\u8fb9\u754c\u6761\u4ef6\u4e0b\u8fdb\u884c principled \u63a2\u7d22\u7684\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u7a33\u5065\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5 EUBRL\uff0c\u901a\u8fc7\u5229\u7528\u540e\u9a8c\u4e2d\u7684 epistemic \u6307\u5bfc\u6765\u8fdb\u884c\u63a2\u7d22\uff1b\u5728\u4f30\u8ba1\u8bef\u5dee\u5f15\u53d1\u7684\u9010\u6b65\u540e\u6094(per-step regret)\u4e0a\u81ea\u9002\u5e94\u51cf\u5c11\uff0c\u4ece\u800c\u5b9e\u73b0 principled \u63a2\u7d22\u3002\u5bf9\u4e00\u7c7b\u8868\u8fbe\u529b\u5145\u5206\u7684\u5148\u9a8c\uff0c\u5728\u65e0\u9650-horizon \u6298\u6263 MDP \u4e0b\u7ed9\u51fa nearly minimax-optimal \u7684\u56de\u62a5\u4e0e\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "result": "\u7406\u8bba\u65b9\u9762\uff0c\u7ed9\u51fa\u8fd1\u4f3c\u6700\u4f18\u7684\u56de\u62a5\u4e0a\u754c\uff08\u8fd1\u4f3c minimax-optimal \u7684\u56de\u62a5\u4e0e\u76f8\u5e94\u7684\u6837\u672c\u590d\u6742\u5ea6\uff09\u3002\u5b9e\u9a8c\u65b9\u9762\uff0c\u5728\u7a00\u758f\u5956\u52b1\u3001\u957f\u65f6\u95f4\u6b65\u957f\u4e14\u5e26\u968f\u673a\u6027\u7684\u4efb\u52a1\u4e2d\uff0cEUBRL \u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3001 scalable \u80fd\u529b\u53ca\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "EUBRL \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u5408\u7406\u8bba guarantees \u4e0e\u7ecf\u9a8c\u8868\u73b0\u7684\u8d1d\u53f6\u65af RL \u6846\u67b6\uff0c\u80fd\u591f\u5728\u77e5\u8bc6\u8fb9\u754c\u5904\u8fdb\u884c\u66f4\u9ad8\u6548\u4e14\u66f4\u7a33\u5065\u7684\u63a2\u7d22\u3002\u672a\u6765\u5de5\u4f5c\u53ef\u62d3\u5c55\u5230\u66f4\u5e7f\u7684\u5148\u9a8c\u8bbe\u5b9a\u3001\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3001\u4ee5\u53ca\u5728\u975e\u5e73\u7a33\u6216\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u7814\u7a76\u3002"}}
{"id": "2512.15420", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15420", "abs": "https://arxiv.org/abs/2512.15420", "authors": ["Yeonwoo Cha", "Semin Kim", "Jinhyeon Kwon", "Seunghoon Hong"], "title": "FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows", "comment": "https://yeonwoo378.github.io/official_flowbind", "summary": "Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.", "AI": {"tldr": "FlowBind \u63d0\u51fa\u4e00\u4e2a\u9ad8\u6548\u7684\u4efb\u610f\u6a21\u6001\u5230\u4efb\u610f\u6a21\u6001\u751f\u6210\u6846\u67b6\uff1a\u4ee5\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e3a\u6838\u5fc3\uff0c\u4f7f\u7528\u6a21\u6001\u7279\u5f02\u7684\u53ef\u9006\u6d41\u5c06\u6f5c\u5728\u6620\u5c04\u5230\u5404\u6a21\u6001\uff1b\u5355\u4e00\u76ee\u6807\u51fd\u6570\u8bad\u7ec3\uff0c\u63a8\u7406\u65f6\u76f4\u63a5\u7ffb\u8bd1\uff0c\u663e\u8457\u964d\u4f4e\u6570\u636e\u548c\u8ba1\u7b97\u8981\u6c42\uff0c\u8fbe\u5230\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u8fd1\u7684\u751f\u6210\u8d28\u91cf\u4f46\u53c2\u6570\u66f4\u5c11\u3001\u8bad\u7ec3\u66f4\u5feb\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u65b9\u6cd5\u5728\u6548\u7387\u65b9\u9762\u53d7\u9650\uff1a\u9700\u8981\u5927\u89c4\u6a21\u5e26\u6807\u7b7e/\u914d\u5bf9\u6570\u636e\u3001\u8054\u5408\u5206\u5e03\u5efa\u6a21\u6210\u672c\u9ad8\u3001\u8bad\u7ec3\u901a\u5e38\u662f\u591a\u9636\u6bb5\u4e14\u590d\u6742\u3002\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u4efb\u610f\u6a21\u6001\u751f\u6210\u3002", "method": "\u5b66\u4e60\u4e00\u4e2a\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u6a21\u6001\u7279\u5f02\u7684\u53ef\u9006\u6d41\u8fde\u63a5\u6f5c\u5728\u4e0e\u5404\u6a21\u6001\uff1b\u901a\u8fc7\u5355\u4e00flow-matching\u76ee\u6807\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff1b\u63a8\u7406\u9636\u6bb5\u53ef\u9006\u6d41\u4f5c\u4e3a\u7f16\u7801\u5668/\u89e3\u7801\u5668\u5b9e\u73b0\u8de8\u6a21\u6001\u7ffb\u8bd1\uff1b\u901a\u8fc7\u6f5c\u5728\u56e0\u5b50\u5316\u4ea4\u4e92\uff0c\u80fd\u591f\u5229\u7528\u4efb\u610f\u5b50\u6a21\u6001\u96c6\u5408\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u4e0a\u5b9e\u9a8c\uff0c\u751f\u6210\u8d28\u91cf\u4e0e\u5bf9\u6bd4\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u53ef\u51cf\u5c11\u6700\u591a6\u500d\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u81f3\u539f\u65b9\u6cd5\u768410\u500d\uff1b\u9879\u76ee\u9875\u6709\u4ee3\u7801\u3002", "conclusion": "FlowBind \u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u4efb\u610f\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u6570\u636e\u9700\u6c42\u4e0e\u8ba1\u7b97\u6210\u672c\uff0c\u5177\u5907\u826f\u597d\u6269\u5c55\u6027\u4e0e\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2512.15427", "categories": ["cs.LG", "cond-mat.stat-mech", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.15427", "abs": "https://arxiv.org/abs/2512.15427", "authors": ["Hyakka Nakada", "Shu Tanaka"], "title": "Statistics of Min-max Normalized Eigenvalues in Random Matrices", "comment": "4 pages, 4 figures", "summary": "Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.", "AI": {"tldr": "\u7814\u7a76 min-max \u5f52\u4e00\u5316\u540e\u968f\u673a\u77e9\u9635\u7684\u7279\u5f81\u503c\u5206\u5e03\u53ca\u5176\u7d2f\u79ef\u5206\u5e03\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u5e76\u63a8\u5bfc\u77e9\u9635\u5206\u89e3\u4e2d\u7684\u6b8b\u5dee\u8bef\u5dee\uff0c\u8f85\u4ee5\u6570\u503c\u9a8c\u8bc1\u3002", "motivation": "\u6570\u636e\u79d1\u5b66\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u6570\u636e\u5f52\u4e00\u5316\uff0c\u7406\u89e3\u5f52\u4e00\u5316\u540e\u7684\u7279\u5f81\u503c\u7edf\u8ba1\u6027\u8d28\u6709\u52a9\u4e8e\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9645\u7b97\u6cd5\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u5229\u7528\u5df2\u63d0\u51fa\u7684 min-max \u5f52\u4e00\u5316\u7279\u5f81\u503c\u5206\u5e03\uff0c\u8bc4\u4f30\u5176\u7d2f\u79ef\u5206\u5e03\u7684\u7f29\u653e\u89c4\u5f8b\uff1b\u63a8\u5bfc\u77e9\u9635\u5206\u89e3\u8fc7\u7a0b\u4e2d\u7684\u6b8b\u5dee\u8bef\u5dee\uff1b\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u9a8c\u8bc1\u4e86\u5f52\u4e00\u5316\u5206\u5e03\u5728\u7d2f\u79ef\u5206\u5e03\u5c3a\u5ea6\u4e0a\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u7ed9\u51fa\u6b8b\u5dee\u8bef\u5dee\u7684\u7406\u8bba\u8868\u8fbe\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5f97\u5230\u4e00\u81f4\u7ed3\u679c\u3002", "conclusion": "\u4e3a\u968f\u673a\u77e9\u9635\u5206\u6790\u4e0e\u76f8\u5173\u7b97\u6cd5\u63d0\u4f9b\u4e86\u5173\u4e8e\u5f52\u4e00\u5316\u540e\u7279\u5f81\u503c\u5206\u5e03\u53ca\u5176\u5bf9\u77e9\u9635\u5206\u89e3\u7684\u5f71\u54cd\u7684\u7406\u8bba\u4e0e\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2512.15430", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15430", "abs": "https://arxiv.org/abs/2512.15430", "authors": ["Quanxi Zhou", "Wencan Mao", "Manabu Tsukada", "John C. S. Lui", "Yusheng Ji"], "title": "FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments", "comment": null, "summary": "Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.", "AI": {"tldr": "A generalized Feature Model-Based Enhanced Actor-Critic (FM-EAC) framework that fuses planning, acting, and learning for multi-task RL, leveraging feature-based models to improve transferability; demonstrated superior performance and configurable sub-networks.", "motivation": "Transferability across tasks and scenarios remains a major challenge in RL; MBRL and MFRL follow different paths but both underperform in generalization; a unified framework is needed to generalize to multiple tasks with modular components.", "method": "Proposes FM-EAC, a hybrid architecture combining model-based planning with an enhanced actor-critic, using feature-based models; allows customization of sub-networks per user requirements; integrates planning, acting, learning for multi-task control in dynamic environments.", "result": "Empirical simulations in urban and agricultural settings show FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods.", "conclusion": "FM-EAC provides improved generalizability and flexibility for multi-task control, and its modular sub-networks enable user-specific customization."}}
{"id": "2512.15439", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15439", "abs": "https://arxiv.org/abs/2512.15439", "authors": ["Akihiro Kubo", "Paavo Parmas", "Shin Ishii"], "title": "Double Horizon Model-Based Policy Optimization", "comment": "Accepted to Transactions on Machine Learning Research (TMLR) Code available at https://github.com/4kubo/erl_lib", "summary": "Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long \"distribution rollout\" (DR) and a short \"training rollout\" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.", "AI": {"tldr": "\u53cc\u89c6\u89d2\u6a21\u578b\u57fa\u5f3a\u5316\u5b66\u4e60\uff08DHMBPO\uff09\uff1a\u5c06\u5c55\u5f00\u8fc7\u7a0b\u5206\u4e3a\u957f\u671f\u5206\u5e03\u56de\u653e\uff08DR\uff09\u4e0e\u77ed\u671f\u8bad\u7ec3\u56de\u653e\uff08TR\uff09\u4e24\u79cd\u76ee\u6807\u533a\u95f4\uff0c\u4ee5\u5e73\u8861\u5206\u5e03\u6f02\u79fb\u3001\u6a21\u578b\u504f\u5dee\u4e0e\u68af\u5ea6\u65b9\u5dee\uff0c\u4ece\u800c\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u63d0\u5347\u6837\u672c\u6548\u7387\u4e0e\u8fd0\u884c\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u5728\u6a21\u578b-based RL \u4e2d\uff0c\u5c55\u5f00\u56de\u653e\u7684\u957f\u5ea6\u5f71\u54cd\u5206\u5e03\u6f02\u79fb\u3001\u6a21\u578b\u504f\u5dee\u548c\u68af\u5ea6\u4f30\u8ba1\u7684\u65b9\u5dee\uff0c\u5b58\u5728\u4e92\u76f8\u77db\u76fe\u7684\u6700\u4f18 horizon\u3002\u5355\u4e00 horizon \u96be\u4ee5\u540c\u65f6\u4f18\u5316\u6837\u672c\u6548\u7387\u548c\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51fa\u53cc\u89c6\u89d2\u56de\u653e\u6846\u67b6\uff1a\u957f\u7684\u5206\u5e03\u56de\u653e\uff08DR\uff09\u7528\u4e8e\u83b7\u53d6\u8fd1\u4f3c on-policy \u7684\u72b6\u6001\u5206\u5e03\u3001\u7f13\u89e3\u5206\u5e03\u6f02\u79fb\uff1b\u77ed\u7684\u8bad\u7ec3\u56de\u653e\uff08TR\uff09\u4f7f\u7528\u53ef\u5bfc\u8f6c\u79fb\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u503c\u68af\u5ea6\u4f30\u8ba1\u3001\u7a33\u5b9a\u68af\u5ea6\u66f4\u65b0\uff0c\u51cf\u5c11\u66f4\u65b0\u6b21\u6570\u548c\u603b\u4f53\u8fd0\u884c\u65f6\u95f4\uff1b\u4e24\u8005\u534f\u540c\u8bad\u7ec3\u5b9e\u73b0\u6298\u4e2d\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u4e0a\uff0c\u53cc\u89c6\u89d2\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u8d85\u8fc7\u73b0\u6709\u7684MBRL\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u56de\u653e\u8fc7\u7a0b\u5206\u6210\u957f\u671f\u7684\u5206\u5e03\u56de\u653e\u548c\u77ed\u671f\u7684\u8bad\u7ec3\u56de\u653e\uff0cDHMBPO \u6210\u529f\u5e73\u8861\u4e86\u5206\u5e03\u6f02\u79fb\u3001\u6a21\u578b\u504f\u5dee\u4e0e\u68af\u5ea6\u4e0d\u7a33\u5b9a\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u7efc\u5408\u6027\u80fd\u3002"}}
{"id": "2512.15442", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15442", "abs": "https://arxiv.org/abs/2512.15442", "authors": ["Neeraj Sarna", "Yuanyuan Li", "Michael von Gablenz"], "title": "Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting", "comment": null, "summary": "Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.", "AI": {"tldr": "\u7814\u7a76\u5c06\u94fe\u5f0f\u63a8\u7406\u63d0\u793a\u3001\u4efb\u52a1\u6307\u4ee4\u63d0\u793a\u3001\u8d1f\u63d0\u793a\u548c\u91cd\u5199\u63d0\u793a\u7b49\u65b9\u6cd5\u8054\u5408\u7528\u4e8e\u964d\u4f4e\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u53d7\u7248\u6743\u4fdd\u62a4\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u5e76\u5728\u591a\u6a21\u578b\u4e0a\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u8bb0\u5fc6\u5e76\u518d\u73b0\u53d7\u7248\u6743\u4fdd\u62a4\u6750\u6599\u800c\u5f15\u53d1\u7684\u7248\u6743\u4fb5\u6743\u98ce\u9669\uff0c\u4ee5\u53ca\u7531\u6b64\u5bf9AI\u7528\u6237\u548c\u5f00\u53d1\u8005\u5e26\u6765\u7684\u6cd5\u5f8b\u4e0e\u7ecf\u6d4e\u538b\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5c06\u94fe\u5f0f\u63a8\u7406\u63d0\u793a\u3001\u4efb\u52a1\u6307\u4ee4\u63d0\u793a\u3001\u8d1f\u63d0\u793a\u3001\u63d0\u793a\u91cd\u5199\u56db\u79cd\u6280\u672f\u7ed3\u5408\u7684\u516c\u5f0f\uff1b\u901a\u8fc7\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u4e0e\u53d7\u7248\u6743\u4fdd\u62a4\u56fe\u50cf\u7684\u76f8\u4f3c\u6027\u4ee5\u53ca\u4e0e\u8f93\u5165\u6587\u672c\u7684\u76f8\u5173\u6027\u6765\u8861\u91cf\u6548\u679c\uff1b\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u6a21\u578b\u590d\u6742\u5ea6\u5bf9\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8fd9\u4e9b\u6280\u5de7\u7684\u6709\u6548\u6027\u968f\u6a21\u578b\u590d\u6742\u5ea6\u53d8\u5316\u800c\u53d8\u5316\uff1b\u5728\u67d0\u4e9b\u6a21\u578b\u4e0a\u80fd\u964d\u4f4e\u5bf9\u7248\u6743\u6750\u6599\u7684\u518d\u73b0\u7a0b\u5ea6\u5e76\u4fdd\u6301\u5bf9\u8f93\u5165\u7684\u76f8\u5173\u6027\uff0c\u4f46\u5e76\u975e\u5bf9\u6240\u6709\u6a21\u578b\u5747\u540c\u6837\u6709\u6548\uff0c\u9700\u8fdb\u4e00\u6b65\u91cf\u5316\u4e0e\u6539\u8fdb\u3002", "conclusion": "\u5c06\u94fe\u5f0f\u63a8\u7406\u3001\u4efb\u52a1\u6307\u4ee4\u3001\u8d1f\u63d0\u793a\u4e0e\u63d0\u793a\u91cd\u5199\u76f8\u7ed3\u5408\u7684\u7b56\u7565\u5728\u7248\u6743\u98ce\u9669\u7f13\u89e3\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u662f\u5173\u952e\u56e0\u7d20\uff1b\u672a\u6765\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u8bc4\u4f30\u4e0e\u901a\u7528\u5316\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u7248\u6743\u98ce\u9669\u63a7\u5236\u3002"}}
{"id": "2512.15460", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15460", "abs": "https://arxiv.org/abs/2512.15460", "authors": ["Xiangrui Xu", "Zhize Li", "Yufei Han", "Bin Wang", "Jiqiang Liu", "Wei Wang"], "title": "From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning", "comment": null, "summary": "Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.", "AI": {"tldr": "A framework InvLoss for quantifying maximum DRA effectiveness in FL; derives upper bound, connects risk to Jacobian spectral properties, provides InvRE risk estimator, and proposes two adaptive noise defenses; validated experimentally.", "motivation": "To address the lack of a theoretically-grounded risk quantification framework for data reconstruction attacks in federated learning.", "method": "Introduce invertibility loss (InvLoss), derive tight upper bound, analyze Jacobian spectral properties; develop InvRE estimator; propose two adaptive noise defenses; experimental validation.", "result": "Empirical validation on real-world datasets showing InvLoss provides systematic risk evaluation and enables effective defense strategies.", "conclusion": "InvLoss framework enables principled risk quantification and mitigation of DRAs in FL, guiding defense design and evaluation."}}
{"id": "2512.15492", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15492", "abs": "https://arxiv.org/abs/2512.15492", "authors": ["Adri\u00e1n Detavernier", "Jasper De Bock"], "title": "Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier", "comment": "workshop paper (not published)", "summary": "We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.", "AI": {"tldr": "\u5bf9\u5206\u7c7b\u5668\u5355\u4e2a\u9884\u6d4b\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u5b58\u5728\u4e24\u6761\u72ec\u7acb\u8def\u5f84\uff1a\u9c81\u68d2\u6027\u91cf\u5316\uff08RQ\uff09\u4e0e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u3002\u5728\u82e5\u5e72\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6bd4\u8f83\u663e\u793a\u4e24\u8005\u65e0\u660e\u786e\u80dc\u8005\uff0c\u5f7c\u6b64\u4e92\u8865\u4e14\u53ef\u7ec4\u5408\u6210\u6df7\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u4efb\u4e00\u65b9\u6cd5\uff1b\u5e76\u7ed9\u51fa\u6bcf\u4e2a\u6570\u636e\u96c6\u4e0d\u786e\u5b9a\u6027\u4e0e\u9c81\u68d2\u6027\u5bf9\u4e0d\u53ef\u7f6e\u4fe1\u6027\u7684\u76f8\u5bf9\u8d21\u732e\u3002", "motivation": "\u89e3\u51b3\u5355\u4e00\u9884\u6d4b\u53ef\u9760\u6027\u8bc4\u4f30\u7684\u4e24\u79cd\u4e3b\u6d41\u601d\u8def\u4e4b\u95f4\u7684\u5dee\u5f02\u548c\u6f5c\u5728\u5c40\u9650\uff0c\u63a2\u7d22\u5b83\u4eec\u7684\u4e92\u8865\u6027\u4ee5\u53ca\u5c06\u4e8c\u8005\u7ed3\u5408\u4ee5\u63d0\u5347\u6574\u4f53\u9884\u6d4b\u53ef\u9760\u6027\u7684\u53ef\u80fd\u6027\u3002", "method": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5e94\u7528RQ\u4e0eUQ\u4e24\u79cd\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u5c06\u4e24\u8005\u7ed3\u5408\u7684\u6df7\u5408\u65b9\u6cd5\uff1b\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u6df7\u5408\u65b9\u6cd5\u5728\u53ef\u9760\u6027\u8bc4\u4f30\u4e0a\u7684\u6027\u80fd\u662f\u5426\u8d85\u8d8a\u5355\u72ec\u4f7f\u7528\u7684RQ\u6216UQ\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u6570\u636e\u96c6\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u4e0e\u9c81\u68d2\u6027\u4f5c\u4e3a\u4e0d\u53ef\u9760\u6027\u6765\u6e90\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u3002", "result": "\u6ca1\u6709\u660e\u786e\u7684\u4f18\u80dc\u8005\uff1bRQ\u4e0eUQ\u4e92\u4e3a\u8865\u5145\uff0c\u6df7\u5408\u65b9\u6cd5\u5728\u591a\u6570\u6570\u636e\u96c6\u4e2d\u4f18\u4e8e\u4efb\u4e00\u5355\u72ec\u65b9\u6cd5\uff1b\u5e76\u4e3a\u6bcf\u4e2a\u6570\u636e\u96c6\u7ed9\u51fa\u4e0d\u786e\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u5bf9\u4e0d\u53ef\u7f6e\u4fe1\u6027\u7684\u76f8\u5bf9\u8d21\u732e\u8bc4\u4f30\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u5177\u5907\u4e92\u8865\u6027\uff0c\u8026\u5408\u4f7f\u7528\u53ef\u63d0\u5347\u9884\u6d4b\u5355\u4f8b\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u6027\u80fd\uff1b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\uff0c\u53ef\u4ee5\u4f9d\u636e\u76f8\u5bf9\u91cd\u8981\u6027\u6743\u8861\u4f7f\u7528\uff0c\u4ee5\u83b7\u5f97\u5bf9\u4e0d\u53ef\u9760\u6027\u6765\u6e90\u7684\u5b9a\u91cf\u7406\u89e3\u3002"}}
{"id": "2512.15493", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15493", "abs": "https://arxiv.org/abs/2512.15493", "authors": ["Hampus Linander", "Conor Heins", "Alexander Tschantz", "Marco Perin", "Christopher Buckley"], "title": "Soft Geometric Inductive Bias for Object Centric Dynamics", "comment": "8 pages, 11 figures; 6 pages supplementary material", "summary": "Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.", "AI": {"tldr": "Soft geometric inductive bias via geometric algebra neural networks in object-centric world models improves long-horizon physical fidelity in 2D rigid-body dynamics with obstacles, offering robust generalization and sample efficiency and serving as a middle ground between hand-crafted physics and unstructured nets.", "motivation": "Exact group equivariance can hurt when symmetries are imperfect or broken. A milder, soft inductive bias could provide robustness and better generalization for learning physical dynamics.", "method": "Develop object-centric world models that employ geometric algebra neural networks to introduce a soft geometric inductive bias. Train to predict the next step autoregressively in simulated 2D rigid-body environments with static obstacles.", "result": "Long-horizon rollouts show improved physical fidelity with the soft inductive bias compared to non-equivariant baselines. The approach aligns with soft-equivariance ideas and demonstrates robust generalization and sample efficiency.", "conclusion": "Geometric algebra offers a viable middle ground between handcrafted physics and unstructured deep nets, enabling efficient, robust dynamics models for multi-object scenes."}}
{"id": "2512.15538", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15538", "abs": "https://arxiv.org/abs/2512.15538", "authors": ["Taichi Aida", "Mamoru Komachi", "Toshinobu Ogiso", "Hiroya Takamura", "Daichi Mochihashi"], "title": "Tracking Temporal Dynamics of Vector Sets with Gaussian Process", "comment": "Work in Progress", "summary": "Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u65e0\u9650\u7ef4\u9ad8\u65af\u8fc7\u7a0b\u5e76\u7ed3\u5408\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5411\u91cf\u96c6\u5408\u7684\u6f5c\u5728\u5206\u5e03\uff0c\u4ece\u800c\u5f97\u5230\u7d27\u51d1\u4e14\u53ef\u6bd4\u8f83\u7684\u65f6\u95f4\u8868\u5f81\uff0c\u5e76\u53ef\u89c6\u5316\u5176\u6f14\u53d8\uff0c\u5e94\u7528\u4e8e\u72af\u7f6a\u5206\u5e03\u4e0e\u8bcd\u5d4c\u5165\u6570\u636e\uff0c\u8868\u73b0\u51fa\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7406\u89e3\u8de8\u57df\u7684\u65f6\u95f4\u6027\u5411\u91cf\u96c6\u5408\u7684\u6f14\u5316\u53ca\u5176\u7ed3\u6784\u6027\u53d8\u5316\uff1b\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ef4\u3001\u968f\u65f6\u95f4\u53d8\u5316\u7684\u96c6\u5408\u4e0a\u96be\u4ee5\u5f97\u5230\u7a33\u5b9a\u3001\u53ef\u6bd4\u8f83\u7684\u8868\u793a\u3002", "method": "\u5c06\u6bcf\u4e2a\u65f6\u95f4\u70b9\u5411\u91cf\u96c6\u5408\u7684\u6f5c\u5728\u5206\u5e03\u5efa\u6a21\u4e3a\u65e0\u9650\u7ef4\u9ad8\u65af\u8fc7\u7a0b\uff1b\u7528\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u8fd1\u4f3c\u6f5c\u5728\u51fd\u6570\uff0c\u5f97\u5230\u7d27\u51d1\u7684\u3001\u53ef\u8de8\u65f6\u95f4\u5bf9\u6bd4\u7684\u5411\u91cf\u8868\u793a\uff1b\u53ef\u5728\u4f4e\u7ef4\u7a7a\u95f4\u8ddf\u8e2a\u548c\u53ef\u89c6\u5316\u65f6\u95f4\u8f6c\u53d8\u3002", "result": "\u5728\u793e\u4f1a\u5b66\u6570\u636e\uff08\u72af\u7f6a\u5206\u5e03\uff09\u4e0e\u8bed\u8a00\u6570\u636e\uff08\u8bcd\u5d4c\u5165\uff09\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u80fd\u591f\u6355\u6349\u65f6\u95f4\u52a8\u6001\uff0c\u5f97\u5230\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u8868\u5f81\u3002", "conclusion": "\u4e3a\u8de8\u57df\u7684\u65f6\u5e8f\u5411\u91cf\u96c6\u7684\u7ed3\u6784\u53d8\u5316\u5206\u6790\u63d0\u4f9b\u4e00\u4e2a\u5f3a\u6709\u529b\u7684\u6846\u67b6\uff0c\u4fbf\u4e8e\u5728\u4f4e\u7ef4\u4e2d\u53ef\u89c6\u5316\u548c\u6bd4\u8f83\u3002"}}
{"id": "2512.15574", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15574", "abs": "https://arxiv.org/abs/2512.15574", "authors": ["Yuxin Cai", "Yanyong Huang", "Jinyuan Chang", "Dongjie Wang", "Tianrui Li", "Xiaoyi Jiang"], "title": "Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation", "comment": null, "summary": "Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa JUICE\uff0c\u5728\u672a\u6807\u6ce8\u7684\u591a\u89c6\u56fe\u4e0d\u5b8c\u6574\u6570\u636e\u4e0a\u5b9e\u73b0\u7279\u5f81\u4e0e\u5b9e\u4f8b\u7684\u8054\u5408\u65e0\u76d1\u7763\u5171\u9009\u5e76\u901a\u8fc7\u8de8\u89c6\u56fe\u8865\u5168\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u7f3a\u5931\u6570\u636e\u586b\u8865\u4e0e\u89c6\u56fe\u62fc\u63a5\u4f5c\u4e3a\u4e24\u4e2a\u72ec\u7acb\u8fc7\u7a0b\uff0c\u5ffd\u7565\u4e86\u5171\u9009\u4e0e\u8865\u5168\u7684\u6f5c\u5728\u4e92\u76f8\u4fc3\u8fdb\u5173\u7cfb\uff1b\u6b64\u5916\uff0c\u7b80\u5355\u5408\u5e76\u591a\u89c6\u56fe\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e0d\u540c\u89c6\u56fe\u7684\u4e92\u8865\u4fe1\u606f\u3002\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u91cd\u5efa\u4e0e\u5171\u9009\u8026\u5408\uff0c\u5e76\u5229\u7528\u8de8\u89c6\u56fe\u90bb\u57df\u5173\u7cfb\u6539\u5584\u7f3a\u5931\u503c\u63a8\u65ad\u3002", "method": "\u63d0\u51fa JUICE\uff1a\u5728\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4e2d\u5bf9\u4e0d\u5b8c\u6574\u7684\u591a\u89c6\u56fe\u6570\u636e\u8fdb\u884c\u91cd\u5efa\uff0c\u8054\u5408\u5b66\u4e60\u7279\u5f81\u4e0e\u5b9e\u4f8b\u7684\u65e0\u76d1\u7763\u5171\u9009\uff1b\u901a\u8fc7\u8de8\u89c6\u56fe\u90bb\u57df\u4fe1\u606f\u5b66\u4e60\u6837\u672c\u95f4\u5173\u7cfb\u5e76\u5728\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u5730 refine \u7f3a\u5931\u503c\uff1b\u5b9e\u73b0\u5bf9\u66f4\u5177\u4ee3\u8868\u6027\u7684\u7279\u5f81\u548c\u6837\u672c\u7684\u9009\u53d6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e JUICE \u5728\u4e0e\u73b0\u6709\u65b9\u6cd5\u7684\u6bd4\u8f83\u4e2d\u5b9e\u73b0\u66f4\u4f18\u7684\u7279\u5f81\u4e0e\u5b9e\u4f8b\u9009\u62e9\uff0c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "JUICE \u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u4e14\u65e0\u76d1\u7763\u7684\u6846\u67b6\uff0c\u6210\u529f\u5c06\u91cd\u5efa\u3001\u8de8\u89c6\u56fe\u534f\u4f5c\u4e0e\u5171\u9009\u8026\u5408\uff0c\u9488\u5bf9\u672a\u6807\u6ce8\u7684\u591a\u89c6\u56fe\u4e0d\u5b8c\u6574\u6570\u636e\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7279\u5f81\u4e0e\u5b9e\u4f8b\u9009\u62e9\u3002"}}
{"id": "2512.15596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15596", "abs": "https://arxiv.org/abs/2512.15596", "authors": ["Shuibai Zhang", "Fred Zhangzhi Peng", "Yiheng Zhang", "Jin Pan", "Grigorios G. Chrysos"], "title": "Corrective Diffusion Language Models", "comment": "18 pages", "summary": "Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u7ea0\u9519\u7684\u540e\u8bad\u7ec3\u539f\u5219\uff0c\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u53ef\u89c1\u7684\u9519\u8bef\u6807\u8bb0\uff0c\u63d0\u5347\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u9519\u8bef\u5b9a\u4f4d\u4e0e\u5c31\u5730\u7ea0\u9519\u4e2d\u7684\u80fd\u529b\uff1b\u5e76\u5f15\u5165 Code Revision Benchmark (CRB) \u8bc4\u4f30\u7ea0\u9519\u4e0e\u5c31\u5730\u6539\u5199\u80fd\u529b\uff1b\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4ee3\u7801\u4fee\u8ba2\u4efb\u52a1\u548c\u53d7\u63a7\u8bbe\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6 MDLM\uff0c\u5e76\u63d0\u5347\u7eaf\u5b8c\u6210\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u7684\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u81ea\u7136\u5730\u4f53\u73b0\u7ea0\u9519\u80fd\u529b\uff0c\u56e0\u4e3a\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u5728\u5b8c\u6574\u8f93\u5165\u4e2d\u8bc6\u522b\u4e0d\u53ef\u9760\u7684\u6807\u8bb0\uff0c\u4f7f\u5f97\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6539\u5199\u65e0\u6548\u3002\u9700\u8981\u4e00\u79cd\u660e\u786e\u7684\u76d1\u7763\u4fe1\u53f7\u6765\u5f15\u5bfc\u6a21\u578b\u8bc6\u522b\u5e76\u7ea0\u6b63\u9519\u8bef\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u63a7\u7684\u9519\u8bef\u5b9a\u4f4d\u4e0e\u5c31\u5730\u4fee\u8ba2\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ea0\u9519\u5bfc\u5411\u7684\u540e\u8bad\u7ec3\u539f\u5219\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u663e\u5f0f\u76d1\u7763\u53ef\u89c1\u7684\u9519\u8bef\u6807\u8bb0\uff0c\u9f13\u52b1\u5bf9\u9519\u8bef\u6807\u8bb0\u7684\u7f6e\u4fe1\u5ea6\u964d\u4f4e\u5e76\u8fdb\u884c\u5c31\u5730\u4fee\u8ba2\uff1b\u7ed3\u5408\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6 CRB\uff0c\u7528\u4ee5\u8861\u91cf\u6a21\u578b\u5728\u9519\u8bef\u5b9a\u4f4d\u3001\u5c31\u5730\u7ea0\u9519\u548c\u4fdd\u6301\u539f\u6709\u6b63\u786e\u5185\u5bb9\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u5728\u4ee3\u7801\u4fee\u8ba2\u4efb\u52a1\u548c\u53d7\u63a7\u8bbe\u7f6e\u4e2d\uff0c\u91c7\u7528\u8be5\u540e\u8bad\u7ec3\u539f\u5219\u7684\u6a21\u578b\u5728\u7ea0\u9519\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6 MDLM\uff0c\u540c\u65f6\u7eaf\u5b8c\u6210\u4efb\u52a1\u6027\u80fd\u4e5f\u6709\u6240\u63d0\u5347\u3002\u516c\u5f00\u4ee3\u7801\u5e93\u53ef\u590d\u73b0\u76f8\u5173\u7ed3\u679c\u3002", "conclusion": "\u7ea0\u9519\u5bfc\u5411\u7684\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u673a\u5236\uff0c\u4f7f\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5177\u5907\u8bef\u5dee\u611f\u77e5\u4e0e\u5c31\u5730\u4fee\u8ba2\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7 CRB \u7b49\u57fa\u51c6\u5b9e\u73b0\u53ef\u63a7\u8bc4\u4f30\u4e0e\u6bd4\u8f83\uff0c\u5177\u6709\u63a8\u5e7f\u5230\u5b9e\u9645\u7ea0\u9519\u4e0e\u6539\u5199\u4efb\u52a1\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.15600", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15600", "abs": "https://arxiv.org/abs/2512.15600", "authors": ["Alexandre Dussolle", "Pietro Li\u00f2"], "title": "How Smoothing is N-simplicial Attention?", "comment": "arXiv preprint", "summary": "Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.", "AI": {"tldr": "\u5f15\u5165N- simplex\u6ce8\u610f\u529b\uff0c\u5c06\u9ad8\u9636\u4ea4\u4e92\u4ece\u6210\u5bf9\u76f8\u4f3c\u6027\u6269\u5c55\u5230\u66f4\u9ad8\u9636\u5173\u7cfb\uff0c\u5e76\u5c06\u5176\u4e0eRotary\u4f4d\u7f6e\u5d4c\u5165RoPE\u7ed3\u5408\uff1b\u901a\u8fc7\u6210\u672c\u6709\u6548\u7684\u5355\u7eaf\u5f62\u9009\u62e9\u805a\u7126\u66f4\u5177\u4efb\u52a1\u654f\u611f\u6027\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u8868\u8fbe\u80fd\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\uff1b\u540c\u65f6\u5bf9\u5e73\u6ed1\u6027\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u7ed9\u51fa Lipschitz \u4e0a\u754c\u5e76\u6307\u51fa\u5355\u72ec\u7684\u9ad8\u9636\u6ce8\u610f\u529b\u4e5f\u4f1a\u5bfc\u81f4\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "motivation": "\u65e8\u5728\u7a81\u7834\u73b0\u6709\u70b9\u5bf9\u70b9\u6216\u5c42\u5185\u6ce8\u610f\u529b\u7684\u5c40\u9650\uff0c\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u9ad8\u9636\u5173\u7cfb\u4ee5\u63d0\u5347\u6a21\u578b\u8868\u8fbe\u529b\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u5f00\u9500\u53ef\u63a7\u7684\u524d\u63d0\u4e0b\u6269\u5c55\u5230\u9ad8\u9636\u4fe1\u606f\u7684\u4f20\u9012\u3002", "method": "\u63d0\u51faN- simplex\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u9ad8\u9636\u5355\u7eaf\u5f62\u4e2d\u7684\u8282\u70b9\u5173\u7cfb\u7528\u4e8e\u4fe1\u606f\u805a\u5408\uff0c\u5e76\u4e0e\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165RoPE\u7ed3\u5408\uff1b\u5f15\u5165\u6210\u672c\u6709\u6548\u7684\u5355\u7eaf\u5f62\u9009\u62e9\u7b56\u7565\uff0c\u5c06\u8ba1\u7b97\u805a\u7126\u4e8e\u5bf9\u4efb\u52a1\u6700\u654f\u611f\u7684\u4ea4\u4e92\uff1b\u5bf9\u5e73\u6ed1\u6027\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63a8\u5bfcLipschitz\u4e0a\u754c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5355\u72ec\u7684\u9ad8\u9636\u6ce8\u610f\u529b\u6613\u4ea7\u751f\u8fc7\u5e73\u6ed1\uff0c\u540c\u65f6\u6269\u5c55\u4e86attention\u7684\u4f20\u9012\u57df\u3002", "result": "\u5728\u7406\u8bba\u4e0a\u63ed\u793a\u4e86\u9ad8\u9636\u6ce8\u610f\u529b\u5728\u8868\u8fbe\u529b\u4e0a\u7684\u6f5c\u5728\u6536\u76ca\u53ca\u5176\u5bf9\u5e73\u6ed1\u6027\u7684\u6311\u6218\uff1b\u901a\u8fc7\u7b80\u5316\u7684\u5355\u7eaf\u5f62\u9009\u62e9\u5b9e\u73b0\u53ef\u63a7\u7684\u8ba1\u7b97\u8d1f\u62c5\uff1b\u5b9e\u9a8c\u4e0a\u53ef\u80fd\u5c55\u793a\u5728\u4efb\u52a1\u76f8\u5173\u6027\u548c\u6536\u655b\u6027\u65b9\u9762\u7684\u6539\u8fdb\u4e0e\u6743\u8861\uff0c\u4f46\u4e5f\u5f3a\u8c03\u4e86\u9700\u8981\u5bf9\u5e73\u6ed1\u6027\u8fdb\u884c\u7ea6\u675f\u6765\u907f\u514d\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u9ad8\u9636\u7684N- simplex\u6ce8\u610f\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u5efa\u6a21\u7ef4\u5ea6\uff0c\u80fd\u591f\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u4ea4\u4e92\u4fe1\u606f\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u5e73\u6ed1\u6027\u95ee\u9898\u9700\u901a\u8fc7\u6709\u6548\u7684\u9009\u62e9\u7b56\u7565\u548c\u7406\u8bba\u7ea6\u675f\u6765\u5e73\u8861\uff0c\u624d\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.15605", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.15605", "abs": "https://arxiv.org/abs/2512.15605", "authors": ["Mathieu Blondel", "Michael E. Sander", "Germain Vivier-Ardisson", "Tianlin Liu", "Vincent Roulet"], "title": "Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction", "comment": null, "summary": "Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u81ea\u56de\u5f52\u6a21\u578b\uff08ARM\uff09\u4e0e\u80fd\u91cf\u57fa\u6a21\u578b\uff08EBM\uff09\u4e4b\u95f4\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u901a\u8fc7\u5728\u51fd\u6570\u7a7a\u95f4\u5efa\u7acb\u663e\u5f0f\u53cc\u5c04\uff0c\u5c06\u4e24\u7c7b\u6a21\u578b\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u5c06\u5176\u4e0e\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8f6fBellman\u65b9\u7a0b\u8054\u7cfb\u8d77\u6765\uff0c\u8fdb\u800c\u8bc1\u660eARM\u4e0eEBM\u7684\u6709\u76d1\u7763\u5b66\u4e60\u7b49\u4ef7\u6027\uff0c\u5e76\u7ed9\u51faEBM\u84b8\u998f\u4e3aARM\u7684\u8bef\u5dee\u754c\uff0c\u89e3\u91caARM\u7684\u524d\u77bb\u6027\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u6f84\u6e05ARM\u4e0eEBM\u4e4b\u95f4\u7684\u6df1\u5c42\u5173\u7cfb\uff0c\u89e3\u91ca\u4e3a\u4f55ARM\u5728\u540e\u8bad\u7ec3\u5bf9\u9f50\u4e2d\u5177\u6709\u201c\u89c4\u5212\u80fd\u529b\u201d\u7684\u672c\u8d28\uff0c\u5e76\u4e3a\u8de8\u6a21\u578b\u84b8\u998f\u548c\u8bad\u7ec3\u8303\u5f0f\u7684\u7edf\u4e00\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u4ee5\u6982\u7387\u94fe\u5f0f\u6cd5\u5219\u4e3a\u8d77\u70b9\uff0c\u5728\u51fd\u6570\u7a7a\u95f4\u6784\u5efaARM\u4e0eEBM\u4e4b\u95f4\u7684\u53cc\u5c04\uff1b\u5c06\u8be5\u53cc\u5c04\u4e0e\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8f6fBellman\u65b9\u7a0b\u8054\u7cfb\u8d77\u6765\uff0c\u63a8\u5bfc\u51faARM\u4e0eEBM\u5728\u6709\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u7b49\u4ef7\u6027\uff1b\u7ed9\u51faEBM\u84b8\u998f\u5230ARM\u7684\u8bef\u5dee\u754c\uff0c\u5e76\u8ba8\u8bba\u5176\u5bf9\u524d\u77bb\u6027\u63a8\u7406\u7684\u542b\u4e49\u3002", "result": "\u8bc1\u660e\u4e86ARM\u4e0eEBM\u5728\u51fd\u6570\u7a7a\u95f4\u7684\u53cc\u5c04\u5173\u7cfb\u53ca\u5176\u4e0e\u8f6fBellman\u65b9\u7a0b\u7684\u5bf9\u5e94\u6027\uff1b\u7ed9\u51fa\u4e86ARM\u548cEBM\u7684\u6709\u76d1\u7763\u5b66\u4e60\u7b49\u4ef7\u6027\uff1b\u7ed9\u51faEBM\u84b8\u998f\u81f3ARM\u7684\u7406\u8bba\u8bef\u5dee\u754c\uff1b\u5bf9ARM\u5177\u5907\u524d\u77bb\u6027\u89c4\u5212\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002", "conclusion": "\u4e3a\u7406\u89e3ARM\u7684\u89c4\u5212\u80fd\u529b\u63d0\u4f9b\u4e86EBM\u89c6\u89d2\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u91cf\u5316\u4e86\u84b8\u998f\u8bef\u5dee\uff0c\u5e76\u4e3a\u4e24\u7c7b\u6a21\u578b\u7684\u8de8\u57df\u8bad\u7ec3\u4e0e\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u652f\u6491\uff0c\u672a\u6765\u53ef\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u5e8f\u5217\u5efa\u6a21\u4e2d\u8fdb\u4e00\u6b65\u63a8\u52a8\u8fd9\u4e24\u79cd\u8303\u5f0f\u7684\u4e92\u8865\u4e0e\u878d\u5408\u3002"}}
{"id": "2512.15614", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15614", "abs": "https://arxiv.org/abs/2512.15614", "authors": ["Xinshun Feng", "Mingzhe Liu", "Yi Qiao", "Tongyu Zhu", "Leilei Sun", "Shuai Wang"], "title": "Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary", "comment": "accepted by AAAI 2026", "summary": "Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.", "AI": {"tldr": "BEAT \u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u53ef\u8f6c\u79fb\u7684\u6846\u67b6\uff0c\u5c06\u7528\u6237\u548c\u7269\u54c1\u884c\u4e3a\u79bb\u6563\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u81ea\u7f16\u7801\u5668\u6784\u5efa\u884c\u4e3a\u8bcd\u6c47\u8868\uff0c\u7ed3\u5408\u591a\u5c42\u8bed\u4e49\u76d1\u7763\u548c\u8bed\u4e49\u5bf9\u9f50\uff0c\u5c06\u884c\u4e3a\u6807\u8bb0\u5d4c\u5165\u5230\u51bb\u7ed3\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u5165\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u96f6-shot \u4e2a\u6027\u5316\u63a8\u8350\u5e76\u751f\u6210\u8fde\u8d2f\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9 ID \u8868\u793a\u7684\u4f9d\u8d56\u53ca\u5bf9\u8bed\u8a00\u6a21\u578b\u7ed3\u6784\u7684\u9650\u5236\uff0c\u4ee5\u53ca\u771f\u5b9e\u573a\u666f\u4e2d\u591a\u6837\u5316\u610f\u56fe\u4e0e\u534f\u540c\u4fe1\u53f7\u4e0d\u6613\u5bf9\u9f50\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u81ea\u7f16\u7801\u5bf9\u56fe\u5f62\u8868\u793a\u8fdb\u884c\u62c6\u89e3\uff0c\u6784\u5efa\u884c\u4e3a\u8bcd\u6c47\u8868\uff1b\u5f15\u5165\u591a\u5c42\u8bed\u4e49\u76d1\u7763\u4ee5\u8fde\u63a5\u884c\u4e3a\u4fe1\u53f7\u548c\u8bed\u8a00\u7a7a\u95f4\uff1b\u8bbe\u8ba1\u8bed\u4e49\u5bf9\u9f50\u6b63\u5219\uff0c\u5c06\u884c\u4e3a\u4ee4\u724c\u5d4c\u5165\u5230\u51bb\u7ed3\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u5165\u7a7a\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cBEAT \u63d0\u5347\u4e86\u96f6-shot \u63a8\u8350\u8868\u73b0\uff0c\u5e76\u751f\u6210\u8fde\u8d2f\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u89e3\u91ca\uff1b\u884c\u4e3a\u4ee4\u724c\u80fd\u591f\u6355\u6349\u7ec6\u7c92\u5ea6\u8bed\u4e49\uff0c\u4e14\u5177\u5907\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63d2\u62d4\u5f0f\u5bf9\u63a5\u80fd\u529b\u3002", "conclusion": "BEAT \u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u3001\u53ef\u8f6c\u79fb\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\u4ee4\u724c\u5b9e\u73b0\u5bf9\u590d\u6742\u884c\u4e3a\u6a21\u5f0f\u7684\u4fbf\u6377\u5d4c\u5165\uff0c\u4fc3\u8fdb\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u573a\u666f\u7684\u5e94\u7528\u53ca\u89e3\u91ca\u80fd\u529b\u63d0\u5347\u3002"}}
{"id": "2512.15657", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15657", "abs": "https://arxiv.org/abs/2512.15657", "authors": ["Tianze Luo", "Haotian Yuan", "Zhuang Liu"], "title": "SoFlow: Solution Flow Models for One-Step Generative Modeling", "comment": "Our code is available at https://github.com/zlab-princeton/SoFlow", "summary": "The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.", "AI": {"tldr": "\u63d0\u51fa Solution Flow Models (SoFlow)\uff0c\u5b9e\u73b0\u4ece\u96f6\u5f00\u59cb\u7684\u4e00\u6b65\u751f\u6210\u3002\u901a\u8fc7\u5206\u6790\u901f\u5ea6\u573a\u7684 ODE \u7684\u89e3\u51fd\u6570\u4e0e\u901f\u5ea6\u51fd\u6570\u7684\u5173\u7cfb\uff0c\u63d0\u51fa Flow Matching \u635f\u5931\u548c\u89e3\u4e00\u81f4\u6027\u635f\u5931\uff0c\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\uff1bFlow Matching \u635f\u5931\u5728\u8bad\u7ec3\u65f6\u53ef\u63d0\u4f9b\u4f30\u8ba1\u7684\u901f\u5ea6\u573a\u4ee5\u7528\u4e8e CFG\uff0c\u63d0\u5347\u751f\u6210\u6027\u80fd\uff1b\u89e3\u4e00\u81f4\u6027\u635f\u5931\u4e0d\u9700\u8981 JVP \u7684\u8ba1\u7b97\uff0c\u964d\u4f4e\u5b9e\u73b0\u96be\u5ea6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u76f8\u540c DiT \u67b6\u6784\u548c\u76f8\u540c\u8bad\u7ec3\u8f6e\u6570\u4e0b\uff0cSoFlow \u5728 ImageNet 256x256 \u4e0a\u7684 FID-50K \u6bd4 MeanFlow \u66f4\u597d\u3002", "motivation": "\u964d\u4f4e\u6269\u6563/\u6d41\u91cf\u5339\u914d\u7b49\u591a\u6b65\u751f\u6210\u7684\u6548\u7387\u74f6\u9888\uff0c\u63a8\u52a8\u4ece\u96f6\u5f00\u59cb\u7684\u4e00\u6b65\u751f\u6210\u7814\u7a76\u3002", "method": "\u63d0\u51fa Solution Flow Models\uff08SoFlow\uff09\uff0c\u5206\u6790\u901f\u5ea6\u65b9\u7a0b\u7684\u901f\u5ea6\u51fd\u6570\u4e0e\u89e3\u51fd\u6570\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8bbe\u8ba1 Flow Matching \u635f\u5931\u548c\u89e3\u4e00\u81f4\u6027\u635f\u5931\u3002Flow Matching \u635f\u5931\u5728\u8bad\u7ec3\u4e2d\u5141\u8bb8\u6a21\u578b\u7ed9\u51fa\u4f30\u8ba1\u7684\u901f\u5ea6\u573a\u4ee5\u8fdb\u884c\u65e0\u6761\u4ef6/\u6761\u4ef6\u5f15\u5bfc\uff08CFG\uff09\u3002\u89e3\u4e00\u81f4\u6027\u635f\u5931\u4e0d\u9700\u8981 JVP \u7684\u8ba1\u7b97\uff0c\u63d0\u5347\u5b9e\u73b0\u6548\u7387\u3002\u4f7f\u7528\u76f8\u540c\u7684 Diffusion Transformer\uff08DiT\uff09\u67b6\u6784\u5e76\u5728\u540c\u7b49\u8bad\u7ec3\u8f6e\u6570\u4e0b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728 ImageNet 256x256 \u6570\u636e\u96c6\u4e0a\uff0cSoFlow \u5728\u76f8\u540c\u67b6\u6784\u4e0e\u8bad\u7ec3\u8f6e\u6570\u6761\u4ef6\u4e0b\uff0cFID-50K \u6307\u6807\u4f18\u4e8e MeanFlow\u3002", "conclusion": "\u63d0\u51fa\u7684 SoFlow \u6846\u67b6\u5b9e\u73b0\u4ece\u96f6\u5f00\u59cb\u7684\u4e00\u6b65\u751f\u6210\uff0c\u5e76\u901a\u8fc7 Flow Matching \u4e0e\u89e3\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u751f\u6210\u6548\u679c\uff0c\u540c\u65f6\u907f\u514d JVP \u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u663e\u793a\u51fa\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u7ade\u4e89\u4f18\u52bf\u3002"}}
{"id": "2512.15687", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15687", "abs": "https://arxiv.org/abs/2512.15687", "authors": ["Zhenwen Liang", "Sidi Lu", "Wenhao Yu", "Kishan Panaganti", "Yujun Zhou", "Haitao Mi", "Dong Yu"], "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning", "comment": null, "summary": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.", "AI": {"tldr": "G2RL\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u81ea\u8eab\u66f4\u65b0\u51e0\u4f55\u7684\u68af\u5ea6\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8f68\u8ff9\u7684\u6700\u7ec8\u5c42\u654f\u611f\u6027\u8fdb\u884c\u5e8f\u5217\u7ea7\u7279\u5f81\u6bd4\u8f83\uff0c\u5728\u91c7\u6837\u7ec4\u5185\u7ed9\u4e88\u5f15\u5165\u65b0\u68af\u5ea6\u65b9\u5411\u7684\u8f68\u8ff9\u4ee5\u6709\u754c\u7684\u4e58\u6cd5\u5956\u52b1\uff0c\u4ee5\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u63a2\u7d22\u6548\u7387\uff0c\u4f18\u4e8e\u57fa\u4e8e\u71b5\u7684GRPO\u4e0e\u5916\u90e8\u5d4c\u5165\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u673a\u5236\uff08\u5982\u71b5\u5956\u91d1\u6216\u5916\u90e8\u8bed\u4e49\u6bd4\u8f83\u5668\uff09\u503e\u5411\u4e8e\u8868\u5c42\u7684\u884c\u4e3a\u53d8\u5f02\uff0c\u672a\u80fd\u4fdd\u8bc1\u91c7\u6837\u8f68\u8ff9\u5728\u4f18\u5316\u65b9\u5411\u4e0a\u7684\u5dee\u5f02\u6027\uff1b\u9700\u8bbe\u8ba1\u4e0e\u6a21\u578b\u5b9e\u9645\u5b66\u4e60\u65b9\u5411\u5bf9\u9f50\u7684\u63a2\u7d22\u4fe1\u53f7\u3002", "method": "\u5bf9\u6bcf\u4e2a\u54cd\u5e94\u6784\u9020\u6765\u81ea\u6a21\u578b\u6700\u7ec8\u5c42\u654f\u611f\u6027\u7684\u5e8f\u5217\u7ea7\u7279\u5f81\uff1b\u5728\u4e00\u4e2a\u91c7\u6837\u7ec4\u5185\u6bd4\u8f83\u8fd9\u4e9b\u7279\u5f81\uff0c\u8bc4\u4f30\u6bcf\u6761\u8f68\u8ff9\u5bf9\u7b56\u7565\u7684\u6f5c\u5728\u66f4\u65b0\u65b9\u5411\u5f71\u54cd\u3002\u5bf9\u5f15\u5165\u65b0\u68af\u5ea6\u65b9\u5411\u7684\u8f68\u8ff9\u7ed9\u4e88\u6709\u754c\u4e58\u6cd5\u5956\u52b1\u7f29\u653e\uff0c\u5f31\u5316\u5197\u4f59\u6216\u8d8a\u754c\u7684\u66f4\u65b0\uff0c\u5f62\u6210\u81ea\u6307\u5411\u7684\u63a2\u7d22\u4fe1\u53f7\uff1b\u4fdd\u6301\u4e0ePPO\u98ce\u683c\u7684\u7a33\u5b9a\u6027\u548cKL\u7ea6\u675f\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6570\u5b66\u4e0e\u4e00\u822c\u63a8\u7406\u57fa\u51c6\uff08MATH500\u3001AMC\u3001AIME24\u3001AIME25\u3001GPQA\u3001MMLUpro\uff09\u4e0a\uff0c\u4f7f\u7528Qwen3 base 1.7B\u4e0e4B\u6a21\u578b\uff0cG2RL\u5728pass@1\u3001maj@16\u3001pass@k\u7b49\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u71b5\u7684GRPO\u4e0e\u5916\u90e8\u5d4c\u5165\u65b9\u6cd5\u3002", "conclusion": "\u5bf9\u6bd4\u5206\u6790\u663e\u793a\uff0c\u901a\u8fc7\u5c06\u63a2\u7d22\u653e\u5728\u653f\u7b56\u7684\u81ea\u8eab\u66f4\u65b0\u7a7a\u95f4\uff0cG2RL\u53ef\u4ee5\u83b7\u5f97\u66f4\u591a\u6b63\u4ea4\u751a\u81f3\u76f8\u53cd\u65b9\u5411\u7684\u68af\u5ea6\u63a2\u7d22\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u63ed\u793a\u4e86\u66f4\u65b0\u65b9\u5411\u7a7a\u95f4\u4f5c\u4e3a\u5f15\u5bfc\u63a2\u7d22\u7684\u66f4\u4e3a\u771f\u5b9e\u4e0e\u6709\u6548\u7684\u57fa\u7840\u3002"}}
{"id": "2512.15699", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.15699", "abs": "https://arxiv.org/abs/2512.15699", "authors": ["Qiuyang Mang", "Wenhao Chai", "Zhifei Li", "Huanzhi Mao", "Shang Zhou", "Alexander Du", "Hanchen Li", "Shu Liu", "Edwin Chen", "Yichuan Wang", "Xieting Chu", "Zerui Cheng", "Yuan Xu", "Tian Xia", "Zirui Wang", "Tianneng Shi", "Jianzhu Yao", "Yilong Zhao", "Qizheng Zhang", "Charlie Ruan", "Zeyu Shen", "Kaiyuan Liu", "Runyuan He", "Dong Xing", "Zerui Li", "Zirong Zeng", "Yige Jiang", "Lufeng Cheng", "Ziyi Zhao", "Youran Sun", "Wesley Zheng", "Meiyuwang Zhang", "Ruyi Ji", "Xuechang Tu", "Zihan Zheng", "Zexing Chen", "Kangyang Zhou", "Zhaozi Wang", "Jingbang Chen", "Aleksandra Korolova", "Peter Henderson", "Pramod Viswanath", "Vijay Ganesh", "Saining Xie", "Zhuang Liu", "Dawn Song", "Sewon Min", "Ion Stoica", "Joseph E. Gonzalez", "Jingbo Shang", "Alvin Cheung"], "title": "FrontierCS: Evolving Challenges for Evolving Intelligence", "comment": "Code with instruction: https://github.com/FrontierCS/Frontier-CS", "summary": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.", "AI": {"tldr": " FrontierCS \u662f\u4e00\u4e2a\u5305\u542b156\u4e2a\u5f00\u653e\u5f0f\u95ee\u9898\u7684\u57fa\u51c6\uff0c\u5f3a\u8c03\u672a\u77e5\u6700\u4f18\u89e3\u4f46\u53ef\u5ba2\u89c2\u8bc4\u4f30\u7684\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9e\u73b0\u53ef\u6267\u884c\u7a0b\u5e8f\u6765\u89e3\u51b3\uff0c\u8986\u76d6\u7b97\u6cd5\u4e0e\u7814\u7a76\u95ee\u9898\uff0c\u63d0\u4f9b\u4e13\u5bb6\u53c2\u8003\u89e3\u548c\u81ea\u52a8\u8bc4\u6d4b\u5668\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u5173\u6ce8\u5df2\u77e5\u6700\u4f18\u89e3\u6216\u7b80\u5355\u4efb\u52a1\uff0c\u96be\u4ee5\u8861\u91cf\u6a21\u578b\u5728\u524d\u6cbf\u96be\u5ea6\u4e0a\u7684\u63a8\u7406\u4e0e\u7cfb\u7edf\u8bbe\u8ba1\u80fd\u529b\uff0c FrontierCS \u65e8\u5728\u5728\u672a\u77e5\u6700\u4f18\u89e3\u7684\u9886\u57df\u63d0\u4f9b\u53ef\u8861\u91cf\u7684\u8fdb\u5c55\u3002", "method": "\u8bbe\u8ba1\u3001\u540c\u884c\u8bc4\u5ba1\u4e0e\u591a\u9886\u57df\u8986\u76d6\u7684\u5f00\u653e\u5f0f\u95ee\u9898\u96c6\uff1b\u6bcf\u9898\u63d0\u4f9b\u4e13\u5bb6\u53c2\u8003\u89e3\u548c\u81ea\u52a8\u8bc4\u6d4b\u5668\uff1b\u95ee\u9898\u5206\u4e3a\u7b97\u6cd5\u95ee\u9898\uff08\u542b NP-hard \u7684\u53d8\u4f53\u5e76\u5e26\u6709\u5ba2\u89c2\u90e8\u5206\u5206\u6570\uff09\u4e0e\u7814\u7a76\u95ee\u9898\uff1b\u6a21\u578b\u9700\u5b9e\u73b0\u53ef\u6267\u884c\u7a0b\u5e8f\u6765\u89e3\u51b3\u800c\u975e\u76f4\u63a5\u7ed9\u51fa\u7b54\u6848\u3002", "result": "\u7ecf\u9a8c\u6027\u8bc4\u4f30\u663e\u793a\uff0c\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u5728\u7b97\u6cd5\u4e0e\u7814\u7a76\u4e24\u6761\u7ebf\u90fd\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff1b\u5355\u7eaf\u589e\u52a0\u63a8\u7406\u9884\u7b97\u5e76\u4e0d\u80fd\u663e\u8457\u7f29\u5c0f\u5dee\u8ddd\uff1b\u6a21\u578b\u5f80\u5f80\u8fc7\u5ea6\u8ffd\u6c42\u751f\u6210\u53ef\u5de5\u4f5c\u7684\u4ee3\u7801\uff0c\u800c\u975e\u53d1\u73b0\u9ad8\u8d28\u91cf\u7684\u7b97\u6cd5\u4e0e\u7cfb\u7edf\u8bbe\u8ba1\u3002", "conclusion": " FrontierCS \u6784\u6210\u4e86\u4e00\u4e2a\u524d\u6cbf\u96be\u5ea6\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u63d0\u5347\u5bf9\u9ad8\u5c42\u6b21\u7b97\u6cd5\u4e0e\u7cfb\u7edf\u8bbe\u8ba1\u80fd\u529b\u7684\u7814\u7a76\u3002"}}
{"id": "2512.15706", "categories": ["cs.LG", "q-bio.CB"], "pdf": "https://arxiv.org/pdf/2512.15706", "abs": "https://arxiv.org/abs/2512.15706", "authors": ["Kayode Olumoyin", "Lamees El Naqa", "Katarzyna Rejniak"], "title": "Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data", "comment": "NeurIPS 2025 Workshop on Learning from Time Series for Health", "summary": "In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics. In oncology, this is further exacerbated by the fact that experimental data are often sparse and sometimes are composed of a few time points of tumor volume. In this paper, we propose to learn time-varying interactions between cells, such as those of bladder cancer tumors and immune cells, and their response to a combination of anticancer treatments in a limited data scenario. We employ the physics-informed neural network (PINN) approach to predict possible subpopulation trajectories at time points where no observed data are available. We demonstrate that our approach is consistent with the biological explanation of subpopulation trajectories. Our method provides a framework for learning evolving interactions among biological organisms when external interventions are applied to their environment.", "AI": {"tldr": "Proposes learning time-varying cell interactions under treatment using physics-informed neural networks (PINNs) in sparse oncological data.", "motivation": "Biological populations interact with evolving effects of external interventions; fixed-parameter models fail to capture dynamic changes, especially with limited tumor data.", "method": "Use physics-informed neural networks to learn time-varying interactions between tumor cells and immune cells and predict subpopulation trajectories at unobserved times, under combination therapies.", "result": "The approach yields plausible subpopulation trajectories consistent with biological explanations and demonstrates the ability to infer evolving interactions from limited data.", "conclusion": "Provides a framework for learning dynamic, intervention-influenced interactions among biological organisms when data are scarce, potentially guiding treatment design and understanding tumor-immune dynamics."}}
