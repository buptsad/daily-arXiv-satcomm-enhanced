<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 61]
- [cs.IT](#cs.IT) [Total: 9]
- [cs.CR](#cs.CR) [Total: 18]
- [eess.SY](#eess.SY) [Total: 15]
- [eess.SP](#eess.SP) [Total: 13]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Road Rules for Radio: Why Your Wi-Fi Got Better](https://arxiv.org/abs/2512.23901)
*Bradley Fang,Michael Roger*

Main category: cs.NI

TL;DR: 本文通过对WiFi七大关键领域的综述，系统梳理问题-解决方案及局限，提出以WiFi 8/UHR为核心的可靠性优先发展路线，并以路标比喻帮助读者理解。


<details>
  <summary>Details</summary>
Motivation: 当前WiFi发展迅速但信息碎片化，缺乏对总体演进的清晰理解。本研究旨在提供一个覆盖带宽、能耗、冲突、干扰、数据密集传输、设备数量及峰值吞吐/调制等方面的综合文献综述，并展望未发布技术。

Method: 通过文献综述的方法，逐项分析七个领域存在的问题、已提出的解决思路及现有方案的局限，并讨论未来技术（如802.11bn UHR、WiFi 8）在这些方面的作用。使用路/高速公路类比辅助理解。

Result: 读者将获得对WiFi演进的总体框架和关键技术演变的清晰认识，能够从问题-解决方案-局限的角度理解现有研究，并了解未来方向。

Conclusion: 本文不仅是文献综述，还通过易懂的类比和对现有工作扎根的技术叙述，帮助低门槛读者建立对WiFi的系统性理解；并强调以可靠性优先于数据速率的未来趋势。

Abstract: WiFi allows for the connection of devices and people around the globe. It has proven to be a monumental and revolutionary tool that keeps the world connected. However, recent WiFi advancements are numerous and at times confusing. WiFi has grown significantly over the years, yet few understand the scope and scale of WiFi progression as a whole. This paper tackles that problem, providing a broad literature review on the advancements of key WiFi features to date. This paper will center on seven key areas of focus: (1) bandwidth, (2) battery life, (3) traffic collisions, (4) interference, (5) data-intensive transmissions, (6) numerous devices, and (7) peak throughput/modulation. Each section will focus on WiFi's problems, how those problems were fixed, as well as the limitations of existing solutions. Moreover, the paper explains the role of new unreleased technologies in these seven areas. This includes exploring the upcoming WiFi 8 standard based on the IEEE 802.11bn "Ultra High Reliability" (UHR) specification and how it builds upon current specifications. Compared to previous specifications, WiFi 8 marks a stronger and more significant shift toward prioritizing reliability over pure data rates. Beyond a sole literature review, this paper uses a novel analogy. A road/highway analogy will be integrated throughout the paper to facilitate understanding of networking mechanisms. This paper is approachable and is written such that someone with very little WiFi knowledge should come away with a strong understanding of WiFi. As is typical of literature review papers, technical claims will be grounded in prior work.

</details>


### [2] [Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations](https://arxiv.org/abs/2512.24452)
*Yalin E. Sagduyu,Tugba Erpek,Aylin Yener,Sennur Ulukus*

Main category: cs.NI

TL;DR: 提出一个基于深度学习的语义通信框架，支持多接收端任务并通过对抗性 min-max 训练和附加扰动层实现端到端隐私保护，在现实无线信道上对抗自适应窃听者，同时保持任务与重构性能。


<details>
  <summary>Details</summary>
Motivation: 语义通信强调传递与任务相关的语义信息，而非简单的比特重建；在这种新范式下，学习的语义表征可能对未授权接收者泄露敏感信息，因此需要对隐私进行端到端的保护，尤其是在面对自适应窃听者时。

Method: 传输端采用学习得到的编码器，接收端训练用于语义推理和数据重构的解码器。建立一个迭代的极小-极大优化流程：窃听者被训练以提升语义推理能力，而发射端-接收端对共同任务性能进行优化并降低窃听者的成功率。同时引入一个辅助层，在传输的波形上叠加对抗性扰动以削弱窃听者的语义泄露。系统在瑞利衰落通道和加性高斯噪声下，用 MNIST 与 CIFAR-10 进行评估。

Result: 随着潜在维度增大，语义准确性与重构质量提升；极小-极大机制显著降低窃听者的推理性能，同时不损害合法接收端；扰动层即使在合法链仅针对自身任务训练时也能有效降低泄露。

Conclusion: 给出了一个可调的端到端隐私语义通信框架，适用于现实无线场景下对自适应攻击者的隐私保护需求，激发对带隐私保护的语义通信设计的探索与实现。

Abstract: Semantic communications conveys task-relevant meaning rather than focusing solely on message reconstruction, improving bandwidth efficiency and robustness for next-generation wireless systems. However, learned semantic representations can still leak sensitive information to unintended receivers (eavesdroppers). This paper presents a deep learning-based semantic communication framework that jointly supports multiple receiver tasks while explicitly limiting semantic leakage to an eavesdropper. The legitimate link employs a learned encoder at the transmitter, while the receiver trains decoders for semantic inference and data reconstruction. The security problem is formulated via an iterative min-max optimization in which an eavesdropper is trained to improve its semantic inference, while the legitimate transmitter-receiver pair is trained to preserve task performance while reducing the eavesdropper's success. We also introduce an auxiliary layer that superimposes a cooperative, adversarially crafted perturbation on the transmitted waveform to degrade semantic leakage to an eavesdropper. Performance is evaluated over Rayleigh fading channels with additive white Gaussian noise using MNIST and CIFAR-10 datasets. Semantic accuracy and reconstruction quality improve with increasing latent dimension, while the min-max mechanism reduces the eavesdropper's inference performance significantly without degrading the legitimate receiver. The perturbation layer is successful in reducing semantic leakage even when the legitimate link is trained only for its own task. This comprehensive framework motivates semantic communication designs with tunable, end-to-end privacy against adaptive adversaries in realistic wireless settings.

</details>


### [3] [Hierarchical Online Optimization Approach for IRS-enabled Low-altitude MEC in Vehicular Networks](https://arxiv.org/abs/2512.24659)
*Yixian Wang,Geng Sun,Zemin Sun,Jiacheng Wang,Changyuan Zhao,Daxin Tian,Dusit Niyato,Shiwen Mao*

Main category: cs.NI

TL;DR: 提出了一个基于IRS的低高空多接入边缘计算（MEC）架构，结合空地协作的MEC服务器与混合IRS，提出分层在线优化框架HOOA，通过Stackelberg博弈、 Followers的多对一匹配以及GDMTD3-KKT的DRL算法，在任务卸载、无人机轨迹、IRS相位配置和计算资源分配上实现近似最优，从而降低任务完成延迟和能耗，并显示出良好收敛性、鲁棒性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决高空气域链路阻塞与动态变化的问题：通过空地协同的MEC服务器和混合IRS（建筑内置与无人机携带）来提升机-地面通信质量与可连通性；针对由任务卸载、轨迹和计算资源等多目标优化引发的NP-hard问题，需要高效的求解框架以在实际动态环境中实现低延迟与低能耗的平衡。

Method: 提出分层在线优化方法HOOA。将MOOP重构为一个Stackelberg博弈：MEC服务器作为领导者决定系统级决策，车辆作为跟随者做个体决策。对跟随者层采用多对一匹配机制生成可行的离散决策；对领导者层使用生成扩散模型增强的双迟滞深度确定性策略梯度算法（GDMTD3），并结合KKT方法确定连续决策，以实现对卸载、轨迹、IRS相位和计算资源的联合优化。

Result: 仿真结果表明，所提HOOA相比最佳基准方法在平均任务完成延迟上降低约2.5%，相比最先进的DRL算法在平均能耗上降低约3.1%。此外，HOOA在动态环境中展现出更好的收敛稳定性、鲁棒性和可扩展性。

Conclusion: 所提出的HOOA框架在IRS-支持的空地协同MEC场景中有效解决了NP-hard的多目标优化问题，通过层次化的博弈与DRL相结合实现近优解，且在动态环境下具有良好稳定性与扩展性，适合实际部署。

Abstract: In this paper, we propose an intelligent reflecting surface (IRS)-enabled low-altitude multi-access edge computing (MEC) architecture, where an aerial MEC server cooperates with a terrestrial MEC server to provide computing services, while hybrid IRSs (i.e., building-installed and UAV-carried IRSs) are deployed to enhance the air-ground connectivity under blockage. Based on this architecture, we formulate a multi-objective optimization problem (MOOP) to minimize the task completion delay and energy consumption by jointly optimizing task offloading, UAV trajectory control, IRS phase-shift configuration, and computation resource allocation. The considered problem is NP-hard, and thus we propose a hierarchical online optimization approach (HOOA) to efficiently solve the problem. Specifically, we reformulate the MOOP as a Stackelberg game, where MEC servers collectively act as the leader to determine the system-level decisions, while the vehicles act as followers to make individual decisions. At the follower level, we present a many-to-one matching mechanism to generate feasible discrete decisions. At the leader level, we propose a generative diffusion model-enhanced twin delayed deep deterministic policy gradient (GDMTD3) algorithm integrated with a Karush-Kuhn-Tucker (KKT)-based method, which is a deep reinforcement learning (DRL)-based approach, to determine the continuous decisions. Simulation results demonstrate that the proposed HOOA achieves significant improvements, which reduces average task completion delay by 2.5% and average energy consumption by 3.1% compared with the best-performing benchmark approach and state-of-the-art DRL algorithm, respectively. Moreover, the proposed HOOA exhibits superior convergence stability while maintaining strong robustness and scalability in dynamic environments.

</details>


### [4] [Analyzing Communication Predictability in LLM Training](https://arxiv.org/abs/2512.24750)
*Wenxue Li,Xiangzhou Liu,Yuxuan Li,Yilun Jin,Zhenghang Ren,Xudong Liao,Han Tian,Bo Ren,Zhizhen Zhong,Guyue Liu,Ying Zhang,Kai Chen*

Main category: cs.NI

TL;DR: 提出系统化分析通信可预测性，建立用于估计通信开销的解析模型，并基于该模型开发 ConfigTuner，在大模型混合并行场景下提升吞吐量并降低搜索复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦在线分析以优化运行时，但缺乏对通信模式的系统化理解，导致对带宽利用和通信开销的预测能力不足，制约对分布式训练性能的调优。

Method: 1) 研究典型 LLM 的可预测流量模式及影响 GPU 利用率和有效带宽的因素；2) 构建解析模型来估计分布式训练中的通信开销；3) 基于该模型设计 ConfigTuner 进行配置优化并在 Megatron-LM、Alpa 等基准上对比验证。

Result: 通过实验数据验证解析模型对通信开销的准确性；ConfigTuner 在与 Megatron-LM 的对比中实现了最高 1.36x 的吞吐提升；与 Alpa 的对比显示在给出相同配置建议时显著降低了搜索复杂度。

Conclusion: 系统化理解通信可预测性并结合可解释模型进行配置优化，ConfigTuner 能在多框架下提升训练吞吐并降低调优成本，体现了预测性分析在分布式训练中的实用价值。

Abstract: Effective communication is essential in distributed training, with predictability being one of its most significant characteristics. However, existing studies primarily focus on exploiting predictability through online profiling for runtime optimization, without a systematic understanding of it. In this work, we aim to systematically formulate communication predictability in distributed training, particularly in Large Language Models (LLMs) that utilize hybrid parallelism. Our analysis focuses on both traffic patterns and communication overhead. Specifically, we investigate predictable traffic patterns in typical LLMs and evaluate how various factors influence GPU utilization and effective bandwidth (two critical variables affecting communication overhead). Furthermore, we develop an analytical formulation to estimate communication overhead in LLM training, which is validated with high accuracy against empirical data. Leveraging this formulation, we propose a configuration tuning tool, ConfigTuner, to optimize training performance. Compared to Megatron-LM, the training configurations optimized by ConfigTuner demonstrate up to a 1.36$\times$ increase in throughput. Compared to Alpa, ConfigTuner generates the same configuration suggestion while significantly reducing the search complexity.

</details>


### [5] [Sidelink Positioning: Standardization Advancements, Challenges and Opportunities](https://arxiv.org/abs/2512.24803)
*Yuan Gao,Guangjin Pan,Zhiyong Zhong,Zhengyu Jin,Yichen Hu,Yifei Jin,Shugong Xu*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the integration of cellular networks in vertical industries that demand precise location information, such as vehicle-to-everything (V2X), public safety, and Industrial Internet of Things (IIoT), positioning has become an imperative component for future wireless networks. By exploiting a wider spectrum, multiple antennas and flexible architectures, cellular positioning achieves ever-increasing positioning accuracy. Still, it faces fundamental performance degradation when the distance between user equipment (UE) and the base station (BS) is large or in non-line-of-sight (NLoS) scenarios. To this end, the 3rd generation partnership project (3GPP) Rel-18 proposes to standardize sidelink (SL) positioning, which provides unique opportunities to extend the positioning coverage via direct positioning signaling between UEs. Despite the standardization advancements, the capability of SL positioning is controversial, especially how much spectrum is required to achieve the positioning accuracy defined in 3GPP. To this end, this article summarizes the latest standardization advancements of 3GPP on SL positioning comprehensively, covering a) network architecture; b) positioning types; and c) performance requirements. The capability of SL positioning using various positioning methods under different imperfect factors is evaluated and discussed in-depth. Finally, according to the evolution of SL in 3GPP Rel-19, we discuss the possible research directions and challenges of SL positioning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [A Comprehensive Study of Deep Learning Model Fixing Approaches](https://arxiv.org/abs/2512.23745)
*Hanmo You,Zan Wang,Zishuo Dong,Luanqi Mo,Jianjun Zhao,Junjie Chen*

Main category: cs.LG

TL;DR: 对16类前沿DL模型修复方法在模型级、层级、神经元级的大规模对比研究。结果显示模型级方法在修复效果上最优，但不存在单一方法在不影响准确性和其他属性（鲁棒性、公平性、向后兼容性等）时兼顾所有目标，需要研究减小副作用。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习系统在各行业广泛应用，模型故障可能带来重大风险，因此需要系统性的修复方法评估与对比；同时需要考察修复对鲁棒性、公平性、向后兼容性等关键属性的影响。

Method: 对16种最先进的DL模型修复方法进行大规模实验评估，覆盖模型级、层级、神经元级三个类别；在统一的实验设置中使用多样的数据集、模型结构和应用领域，全面评估修复效果与对鲁棒性、公平性、向后兼容性的影响。

Result: 模型级修复方法在修复效果上明显优于其他类别；无单一方法在实现最佳修复的同时提升准确性并兼顾所有其他属性；不同方法存在权衡，某些副作用需治理。

Conclusion: 学术界应将研究重点放在缓解修复副作用上，探索在不损及其他关键属性的前提下提高修复效果的方向，并为行业提供更全面的评估与应用指南。

Abstract: Deep Learning (DL) has been widely adopted in diverse industrial domains, including autonomous driving, intelligent healthcare, and aided programming. Like traditional software, DL systems are also prone to faults, whose malfunctioning may expose users to significant risks. Consequently, numerous approaches have been proposed to address these issues. In this paper, we conduct a large-scale empirical study on 16 state-of-the-art DL model fixing approaches, spanning model-level, layer-level, and neuron-level categories, to comprehensively evaluate their performance. We assess not only their fixing effectiveness (their primary purpose) but also their impact on other critical properties, such as robustness, fairness, and backward compatibility. To ensure comprehensive and fair evaluation, we employ a diverse set of datasets, model architectures, and application domains within a uniform experimental setup for experimentation. We summarize several key findings with implications for both industry and academia. For example, model-level approaches demonstrate superior fixing effectiveness compared to others. No single approach can achieve the best fixing performance while improving accuracy and maintaining all other properties. Thus, academia should prioritize research on mitigating these side effects. These insights highlight promising directions for future exploration in this field.

</details>


### [7] [A Review of Diffusion-based Simulation-Based Inference: Foundations and Applications in Non-Ideal Data Scenarios](https://arxiv.org/abs/2512.23748)
*Haley Rosso,Talea Mayo*

Main category: cs.LG

TL;DR: Diffusion-based simulation-based inference (SBI) 能在缺乏显式似然的情况下利用模拟器样本来学习给定观测的数据后验分布。与正规化流相比，扩散模型提供更灵活的后验/似然估计，但需要迭代采样，存在计算成本权衡。该综述聚焦扩散SBI在非理想数据条件下的鲁棒性（错配、非结构化/无限维观测、缺失数据），并梳理基础、方法与应用，最后指出开放问题及在地球物理中不确定性量化的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 在复杂仿真问题中，参数推断常受限于难以构建显式似然的情况。扩散模型作为生成模型，为无似然推断提供灵活框架，尤其在需要鲁棒性和对不完美数据的处理时具有潜力。

Method: 系统回顾扩散模型的基础：前向扰动、反向时间SDE/ODE、概率流、去噪分数匹配；讨论条件分数如何实现无似然后验采样。比较扩散模型与正规化流在神经后验/似然估计中的优劣与权衡，分析迭代采样成本等 Trade-offs。汇集Schrödinger桥等理论、条件/序贯后验采样器、对非结构化数据的摊销式架构、以及推断时先验自适应等方法。

Result: 汇总了从理论到实践的框架和方法，强调在非理想数据情形下的鲁棒性，提供了用于不确定性量化的综合工具箱与路线图；并指出扩散SBI相较于传统流模型的优势与局限，如需要逐步/迭代采样的成本。

Conclusion: 讨论了尚待解决的开放问题，强调未来在应用于含不确定性地球物理模型时扩散SBI的潜力，需明确条件以获得可靠后验，并提出对抗错配、处理无限/高维观测、改进缺失数据的方向。

Abstract: For complex simulation problems, inferring parameters of scientific interest often precludes the use of classical likelihood-based techniques due to intractable likelihood functions. Simulation-based inference (SBI) methods forego the need for explicit likelihoods by directly utilizing samples from the simulator to learn posterior distributions over parameters $\mathbfθ$ given observed data $\mathbf{x}_{\text{o}}$. Recent work has brought attention to diffusion models -- a type of generative model rooted in score matching and reverse-time stochastic dynamics -- as a flexible framework SBI tasks. This article reviews diffusion-based SBI from first principles to applications in practice. We first recall the mathematical foundations of diffusion modeling (forward noising, reverse-time SDE/ODE, probability flow, and denoising score matching) and explain how conditional scores enable likelihood-free posterior sampling. We then examine where diffusion models address pain points of normalizing flows in neural posterior/likelihood estimation and where they introduce new trade-offs (e.g., iterative sampling costs). The key theme of this review is robustness of diffusion-based SBI in non-ideal conditions common to scientific data: misspecification (mismatch between simulated training data and reality), unstructured or infinite-dimensional observations, and missingness. We synthesize methods spanning foundations drawing from Schrodinger-bridge formulations, conditional and sequential posterior samplers, amortized architectures for unstructured data, and inference-time prior adaptation. Throughout, we adopt consistent notation and emphasize conditions and caveats required for accurate posteriors. The review closes with a discussion of open problems with an eye toward applications of uncertainty quantification for probabilistic geophysical models that may benefit from diffusion-based SBI.

</details>


### [8] [Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents](https://arxiv.org/abs/2512.23749)
*Amin Sadri,M Maruf Hossain*

Main category: cs.LG

TL;DR: CM^2是一种小型模型，通过学习文档结构实现一-shot分类，强调结构特征以实现近似人类的一次样本学习，同时具备绿色AI、可解释性和在CPU上的高效性。


<details>
  <summary>Details</summary>
Motivation: 人类在概念学习中常在只有一个示例的情况下学习新概念，而常规机器学习算法通常需要大量样本。大规模预训练和能源密集型计算带来可持续性和效率挑战。

Method: 提出Coordinate Matrix Machine (CM^2)，这是一个专门设计的小型模型，学习文档的结构坐标并利用这些信息进行分类。与依赖大量语义向量的深度学习模型不同，CM^2聚焦“重要结构特征”，能够在每个类别只有一个样本时对高度相似的文档进行分类。

Result: 该算法在小数据集上优于传统向量化方法和需要大量数据的深度模型，具备：高准确度、结构几何智能、绿色AI、CPU端推断、可解释性、低延迟、对不平衡数据鲁棒、经济性以及通用性与可扩展性等优势。

Conclusion: CM^2展示了通过学习文档的结构坐标实现人类级的一-shot学习在文档分类任务中的可行性，提供了一种高效、可解释、绿色且易扩展的AI解决方案。

Abstract: Human-level concept learning argues that humans typically learn new concepts from a single example, whereas machine learning algorithms typically require hundreds of samples to learn a single concept. Our brain subconsciously identifies important features and learns more effectively. \vspace*{6pt}
  Contribution: In this paper, we present the Coordinate Matrix Machine (CM$^2$). This purpose-built small model augments human intelligence by learning document structures and using this information to classify documents. While modern "Red AI" trends rely on massive pre-training and energy-intensive GPU infrastructure, CM$^2$ is designed as a Green AI solution. It achieves human-level concept learning by identifying only the structural "important features" a human would consider, allowing it to classify very similar documents using only one sample per class.
  Advantage: Our algorithm outperforms traditional vectorizers and complex deep learning models that require larger datasets and significant compute. By focusing on structural coordinates rather than exhaustive semantic vectors, CM$^2$ offers: 1. High accuracy with minimal data (one-shot learning) 2. Geometric and structural intelligence 3. Green AI and environmental sustainability 4. Optimized for CPU-only environments 5. Inherent explainability (glass-box model) 6. Faster computation and low latency 7. Robustness against unbalanced classes 8. Economic viability 9. Generic, expandable, and extendable

</details>


### [9] [Geometric Scaling of Bayesian Inference in LLMs](https://arxiv.org/abs/2512.23752)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: cs.LG

TL;DR: 生产级语言模型保留在小型风洞中观察到的用于贝叶斯推断的几何底层；最后一层表示沿一个主轴对齐，该轴与预测熵高度相关；域受限的提示能将该结构收缩为与合成设置中相同的低维流形；对熵对齐轴的靶向干预扰乱局部不确定性几何，但并未导致贝叶斯式行为的成比例下降。


<details>
  <summary>Details</summary>
Motivation: 判断在大规模生产级语言模型中，先前在小型风洞实验中观察到的用于贝叶斯推断的几何底层是否仍然存在，并理解模型如何表示与不确定性相关的信息，以及对该几何结构的操控对推理的影响。

Method: 在Pythia、Phi-2、Llama-3、Mistral等系列模型中分析最后一层的值表示，找出与预测熵高度相关的主轴；通过域受限提示验证该几何是否在不同模型中保持；在Pythia-410M的上下文学习阶段对熵对齐轴进行靶向干预，比较移除/扰动该轴与随机轴干预的效果；评估对贝叶斯式行为的影响。

Result: 最后一层表示呈现一个主轴，该轴的位置与预测熵显著相关；域受限提示使该几何在合成设置中观察到的低维流形在实际模型中也被观察到；对熵对齐轴的干预可破坏局部的不确定性几何，但与之等量级的对贝叶斯式行为的衰退并不显著，相较于随机轴干预，前者对不确定性几何更具选择性。总体而言，现代语言模型保留了使贝叶斯推断在风洞中实现的几何底层，并沿该底层组织其近似贝叶斯更新。

Conclusion: 现代大语言模型保留了促进贝叶斯推断的几何子层，近似贝叶斯更新沿该轴进行，表明该几何是对不确定性的特权性读出，而非单一计算瓶颈。

Abstract: Recent work has shown that small transformers trained in controlled "wind-tunnel'' settings can implement exact Bayesian inference, and that their training dynamics produce a geometric substrate -- low-dimensional value manifolds and progressively orthogonal keys -- that encodes posterior structure. We investigate whether this geometric signature persists in production-grade language models. Across Pythia, Phi-2, Llama-3, and Mistral families, we find that last-layer value representations organize along a single dominant axis whose position strongly correlates with predictive entropy, and that domain-restricted prompts collapse this structure into the same low-dimensional manifolds observed in synthetic settings.
  To probe the role of this geometry, we perform targeted interventions on the entropy-aligned axis of Pythia-410M during in-context learning. Removing or perturbing this axis selectively disrupts the local uncertainty geometry, whereas matched random-axis interventions leave it intact. However, these single-layer manipulations do not produce proportionally specific degradation in Bayesian-like behavior, indicating that the geometry is a privileged readout of uncertainty rather than a singular computational bottleneck. Taken together, our results show that modern language models preserve the geometric substrate that enables Bayesian inference in wind tunnels, and organize their approximate Bayesian updates along this substrate.

</details>


### [10] [Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation](https://arxiv.org/abs/2512.23753)
*Deep Shankar Pandey,Hyomin Choi,Qi Yu*

Main category: cs.LG

TL;DR: 提出一种泛化的 evidential deep learning 框架，通过学习证据并在非负约束下设计新型激活与正则化，解决低证据区域的梯度消失问题，在多任务上验证稳定性与性能提升。


<details>
  <summary>Details</summary>
Motivation: EDL 将不确定性以证据形式表达但受证据非负性与激活选择的影响，可能导致学习动态异常（学习冻结）。需要理论分析并提供稳定的一致证据更新策略。

Method: 理论分析不同 evidential 激活对梯度与学习动态的影响，提出一族通用的激活函数及相应 evidential 正则化项，以实现跨激活域的一致证据更新；并在 MNIST、CIFAR-10/100、Tiny-ImageNet、少样本任务及盲脸修复等任务上进行广泛实验验证。

Result: 给出学习冻结现象及其与激活的关系的理论界定，并通过实验证明提出的激活族与正则化提升证据更新的稳定性与分类性能，具备鲁棒性与跨场景有效性。

Conclusion: 通过设计通用的 evidential 激活与正则化策略，建立在不同激活域下也能实现一致证据更新的框架，提升 EDL 的稳定性、学习效率及泛化能力。

Abstract: Evidential deep learning (EDL) models, based on Subjective Logic, introduce a principled and computationally efficient way to make deterministic neural networks uncertainty-aware. The resulting evidential models can quantify fine-grained uncertainty using learned evidence. However, the Subjective-Logic framework constrains evidence to be non-negative, requiring specific activation functions whose geometric properties can induce activation-dependent learning-freeze behavior: a regime where gradients become extremely small for samples mapped into low-evidence regions. We theoretically characterize this behavior and analyze how different evidential activations influence learning dynamics. Building on this analysis, we design a general family of activation functions and corresponding evidential regularizers that provide an alternative pathway for consistent evidence updates across activation regimes. Extensive experiments on four benchmark classification problems (MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet), two few-shot classification problems, and blind face restoration problem empirically validate the developed theory and demonstrate the effectiveness of the proposed generalized regularized evidential models.

</details>


### [11] [HINTS: Extraction of Human Insights from Time-Series Without External Sources](https://arxiv.org/abs/2512.23755)
*Sheo Yon Jhin,Noseong Park*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human decision-making, emotions, and collective psychology are complex factors that shape the temporal dynamics observed in financial and economic systems. Many recent time series forecasting models leverage external sources (e.g., news and social media) to capture human factors, but these approaches incur high data dependency costs in terms of financial, computational, and practical implications. In this study, we propose HINTS, a self-supervised learning framework that extracts these latent factors endogenously from time series residuals without external data. HINTS leverages the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns. The extracted human factors are integrated into a state-of-the-art backbone model as an attention map. Experimental results using nine real-world and benchmark datasets demonstrate that HINTS consistently improves forecasting accuracy. Furthermore, multiple case studies and ablation studies validate the interpretability of HINTS, demonstrating strong semantic alignment between the extracted factors and real-world events, demonstrating the practical utility of HINTS.

</details>


### [12] [Neural Optimal Design of Experiment for Inverse Problems](https://arxiv.org/abs/2512.23763)
*John E. Darges,Babak Maboudi Afkham,Matthias Chung*

Main category: cs.LG

TL;DR: A learning-based framework NODE (Neural Optimal Design of Experiments) jointly optimizes reconstruction models and a fixed-budget, continuous design variable set (sensor locations, times, angles) in a single loop, enforcing sparsity by design and avoiding bilevel optimization and l1-tuning. It reduces computational complexity and outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional optimal experimental design often relies on bilevel optimization and indirect sparsity regularization, which are computationally expensive and may require manual sparsity tuning. There is a need for an end-to-end, learnable approach that inherently yields sparse experimental designs.

Method: Neural Optimal Design of Experiments (NODE) couples a neural reconstruction model with a fixed-budget set of continuous design variables and trains them end-to-end in a single optimization loop. Design variables are directly optimized to select informative measurement locations, times, or angles, thereby inducing sparsity without l1 regularization and avoiding dense candidate grids. This leads to reduced computational cost and a simpler pipeline.

Result: NODE outperforms baseline approaches across multiple tasks: on an analytically tractable exponential growth benchmark, on MNIST image sampling, and on a real-world sparse-view X-ray CT example, with improved reconstruction accuracy and task-specific performance.

Conclusion: End-to-end, sparsity-enforcing design through jointly optimized continuous design variables yields substantial gains in reconstruction quality and efficiency, making optimal experimental design more practical for inverse problems.

Abstract: We introduce Neural Optimal Design of Experiments, a learning-based framework for optimal experimental design in inverse problems that avoids classical bilevel optimization and indirect sparsity regularization. NODE jointly trains a neural reconstruction model and a fixed-budget set of continuous design variables representing sensor locations, sampling times, or measurement angles, within a single optimization loop. By optimizing measurement locations directly rather than weighting a dense grid of candidates, the proposed approach enforces sparsity by design, eliminates the need for l1 tuning, and substantially reduces computational complexity. We validate NODE on an analytically tractable exponential growth benchmark, on MNIST image sampling, and illustrate its effectiveness on a real world sparse view X ray CT example. In all cases, NODE outperforms baseline approaches, demonstrating improved reconstruction accuracy and task-specific performance.

</details>


### [13] [Exploring Cumulative Effects in Survival Data Using Deep Learning Networks](https://arxiv.org/abs/2512.23764)
*Kang-Chung Yang,Shinsheng Yuan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In epidemiological research, modeling the cumulative effects of time-dependent exposures on survival outcomes presents a challenge due to their intricate temporal dynamics. Conventional spline-based statistical methods, though effective, require repeated data transformation for each spline parameter tuning, with survival analysis computations relying on the entire dataset, posing difficulties for large datasets. Meanwhile, existing neural network-based survival analysis methods focus on accuracy but often overlook the interpretability of cumulative exposure patterns. To bridge this gap, we introduce CENNSurv, a novel deep learning approach that captures dynamic risk relationships from time-dependent data. Evaluated on two diverse real-world datasets, CENNSurv revealed a multi-year lagged association between chronic environmental exposure and a critical survival outcome, as well as a critical short-term behavioral shift prior to subscription lapse. This demonstrates CENNSurv's ability to model complex temporal patterns with improved scalability. CENNSurv provides researchers studying cumulative effects a practical tool with interpretable insights.

</details>


### [14] [A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit](https://arxiv.org/abs/2512.23766)
*Karim Salta,Michael Kirby,Chris Peterson*

Main category: cs.LG

TL;DR: 提出在 LBG 流水线中使用可训练的原型 SVBF（Schubert Variety of Best Fit）来替代子空间均值的子空间聚类算法。SVBF 尽量使其与每个簇成员在固定方向上相交，从而在 Grassmann/flag 流形上作为原型点，提高簇纯度，同时保留适用于下游分析的几何结构。


<details>
  <summary>Details</summary>
Motivation: 在子空间表示的数据中，簇的几何代表需要在 Grassmann/flag 流形上定义，主角角等距离度量常用于衡量相似性。传统的簇均值在某些簇内无法充分捕捉变异，限制聚类性能。引入可训练的 SVBF 原型，旨在更贴近簇的几何结构，同时保持可分析性。

Method: 用可训练的 SVBF 替代子空间均值；SVBF 定义为尽可能与簇成员在固定方向上相交的一子空间；将 SVBF 融入 Linde-Buzo-Grey（LBG）流水线，以实现子空间聚类/量化。

Result: 在合成数据、图像、光谱和视频动作数据集上，SVBF-LBG 显著提升簇的纯度（聚类质量），并保持可用于下游分析的数学结构（如 Grassmann/flag 流形上的距离计算）。

Conclusion: SVBF-LBG 提供一种在子空间聚类中兼顾性能与可分析性的框架，具有良好泛化潜力并可扩展到其他子空间学习任务。

Abstract: In many classification and clustering tasks, it is useful to compute a geometric representative for a dataset or a cluster, such as a mean or median. When datasets are represented by subspaces, these representatives become points on the Grassmann or flag manifold, with distances induced by their geometry, often via principal angles. We introduce a subspace clustering algorithm that replaces subspace means with a trainable prototype defined as a Schubert Variety of Best Fit (SVBF) - a subspace that comes as close as possible to intersecting each cluster member in at least one fixed direction. Integrated in the Linde-Buzo-Grey (LBG) pipeline, this SVBF-LBG scheme yields improved cluster purity on synthetic, image, spectral, and video action data, while retaining the mathematical structure required for downstream analysis.

</details>


### [15] [Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions](https://arxiv.org/abs/2512.23770)
*Ankit Kanwar,Dominik Wagner,Luke Ong*

Main category: cs.LG

TL;DR: 提出一种以安全约束为偏置的信赖区域策略优化算法 SB-TRPO，通过对成本和奖励的自然策略梯度进行凸组合来进行更新，确保每一步在成本上的最优减小量的固定分数，同时追求奖励提升；理论保证局部安全进展，实验在 Safety Gymnasium 任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的强化学习中，现有的拉格朗日/投影等方法往往无法在严格的硬约束下实现近零安全违规或在约束压力下显著降低奖励，亟需能在保持安全的同时提升任务表现的新方法。

Method: SB-TRPO 在信赖域更新中对成本与奖励的自然策略梯度进行凸组合，确保每一步达到固定比例的最优成本减小，并在理论上给出局部安全进展的保证；当梯度对齐时也能实现奖励提升。

Result: 在标准及更具挑战性的 Safety Gymnasium 任务上，SB-TRPO 能 consistently 在安全性与任务完成度之间取得最佳平衡，优于现有的最先进方法。

Conclusion: SB-TRPO 提供了一种具有理论保证的硬约束强化学习新框架，兼顾安全性与任务性能，实验结果显示其在安全关键领域的应用潜力显著优于现有方法。

Abstract: Reinforcement learning (RL) in safety-critical domains requires agents to maximise rewards while strictly adhering to safety constraints. Existing approaches, such as Lagrangian and projection-based methods, often either fail to ensure near-zero safety violations or sacrifice reward performance in the face of hard constraints. We propose Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a new trust-region algorithm for hard-constrained RL. SB-TRPO adaptively biases policy updates towards constraint satisfaction while still seeking reward improvement. Concretely, it performs trust-region updates using a convex combination of the natural policy gradients of cost and reward, ensuring a fixed fraction of optimal cost reduction at each step. We provide a theoretical guarantee of local progress towards safety, with reward improvement when gradients are suitably aligned. Experiments on standard and challenging Safety Gymnasium tasks show that SB-TRPO consistently achieves the best balance of safety and meaningful task completion compared to state-of-the-art methods.

</details>


### [16] [FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading](https://arxiv.org/abs/2512.23773)
*Molei Qin,Xinyu Cai,Yewen Li,Haochong Xia,Chuqiao Zong,Shuo Sun,Xinrun Wang,Bo An*

Main category: cs.LG

TL;DR: Three-stage Efficient and Risk-Aware Ensemble RL (FineFT) for crypto futures trading under high leverage. It uses selective ensemble updates, profitability-based filtering with VAEs to identify capability boundaries, and a conservative policy to adapt to new market states, achieving stable training and reduced risk with superior profitability against 12 baselines.


<details>
  <summary>Details</summary>
Motivation: Futures trading with high leverage amplifies reward fluctuations and training instability; existing RL methods lack self-awareness of capability boundaries to avoid large losses on unseen market states (e.g., black swan events).

Method: Stage I: ensemble Q-learners are selectively updated via ensemble TD errors to improve convergence. Stage II: filter Q-learners by profitability and train VAEs on market states to identify capability boundaries of learners. Stage III: select from the filtered ensemble and a conservative policy guided by trained VAEs to maintain profitability and mitigate risk under new market states.

Result: Extensive experiments in a high-frequency crypto futures environment with 5x leverage show FineFT outperforms 12 SOTA baselines across 6 financial metrics, reducing risk by >40% while achieving superior profitability. Visualization indicates specialization among agents; ablation confirms VAEs routing reduces maximum drawdown and selective update improves convergence and performance.

Conclusion: FineFT offers stable training and risk-aware profitability in high-leverage futures trading by combining ensemble RL with VAEs to delineate capability boundaries and guide conservative decision-making under novel market conditions.

Abstract: Futures are contracts obligating the exchange of an asset at a predetermined date and price, notable for their high leverage and liquidity and, therefore, thrive in the Crypto market. RL has been widely applied in various quantitative tasks. However, most methods focus on the spot and could not be directly applied to the futures market with high leverage because of 2 challenges. First, high leverage amplifies reward fluctuations, making training stochastic and difficult to converge. Second, prior works lacked self-awareness of capability boundaries, exposing them to the risk of significant loss when encountering new market state (e.g.,a black swan event like COVID-19). To tackle these challenges, we propose the Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading (FineFT), a novel three-stage ensemble RL framework with stable training and proper risk management. In stage I, ensemble Q learners are selectively updated by ensemble TD errors to improve convergence. In stage II, we filter the Q-learners based on their profitabilities and train VAEs on market states to identify the capability boundaries of the learners. In stage III, we choose from the filtered ensemble and a conservative policy, guided by trained VAEs, to maintain profitability and mitigate risk with new market states. Through extensive experiments on crypto futures in a high-frequency trading environment with high fidelity and 5x leverage, we demonstrate that FineFT outperforms 12 SOTA baselines in 6 financial metrics, reducing risk by more than 40% while achieving superior profitability compared to the runner-up. Visualization of the selective update mechanism shows that different agents specialize in distinct market dynamics, and ablation studies certify routing with VAEs reduces maximum drawdown effectively, and selective update improves convergence and performance.

</details>


### [17] [Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems](https://arxiv.org/abs/2512.23809)
*Samaresh Kumar Singh,Joyjit Roy,Martin So*

Main category: cs.LG

TL;DR: 提出 ZTA-FL：一个在工业物联网中的防御性联邦学习框架，结合 TPM 基于的身份认证、SHAP 加权聚合实现对非 IID 场景下的拜占庭检测以及在设备端的隐私对抗性训练，以提升鲁棒性和可解释性，降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现实世界的对关键基础设施的攻击凸显了现有联邦学习在拜占庭攻击和严格端到端认证方面的薄弱环节，尤其在工业物联网场景中对隐私、可追溯性与鲁棒性的高需求。

Method: 1) 基于 TPM 的密钥及态认证以实现极低虚假接受率（<1e-7）；2) 提出 SHAP 加权聚合算法，在非独立同分布（non-IID）条件下对拜占庭节点进行可解释检测，并给出理论保证；3) 设备端在本地进行对抗训练，保障隐私。

Result: 在 Edge-IIoTset、CIC-IDS2017、UNSW-NB15 三个 IDS 基准上评估，ZTA-FL 达到 97.8% 的检测准确率；在 30% 拜占庭攻击下仍获得 93.2% 的总体准确率（比 FLAME 提升 3.1%，p<0.01）；对对抗性攻击具有 89.3% 的鲁棒性并将通信开销降低 34%。给出理论分析、失败模式表征，并开源代码以便复现实验。

Conclusion: ZTA-FL 为工业物联网中的联邦学习提供了一种多层次防御机制：强身份认证、可解释且具备理论保证的拜占庭检测聚合，以及隐私保护的本地对抗训练，显著提升鲁棒性与可解释性，且在实际数据集上表现优越，并具备可复现实验性。

Abstract: Recent attacks on critical infrastructure, including the 2021 Oldsmar water treatment breach and 2023 Danish energy sector compromises, highlight urgent security gaps in Industrial IoT (IIoT) deployments. While Federated Learning (FL) enables privacy-preserving collaborative intrusion detection, existing frameworks remain vulnerable to Byzantine poisoning attacks and lack robust agent authentication. We propose Zero-Trust Agentic Federated Learning (ZTA-FL), a defense in depth framework combining: (1) TPM-based cryptographic attestation achieving less than 0.0000001 false acceptance rate, (2) a novel SHAP-weighted aggregation algorithm providing explainable Byzantine detection under non-IID conditions with theoretical guarantees, and (3) privacy-preserving on-device adversarial training. Comprehensive experiments across three IDS benchmarks (Edge-IIoTset, CIC-IDS2017, UNSW-NB15) demonstrate that ZTA-FL achieves 97.8 percent detection accuracy, 93.2 percent accuracy under 30 percent Byzantine attacks (outperforming FLAME by 3.1 percent, p less than 0.01), and 89.3 percent adversarial robustness while reducing communication overhead by 34 percent. We provide theoretical analysis, failure mode characterization, and release code for reproducibility.

</details>


### [18] [Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics](https://arxiv.org/abs/2512.24445)
*Akash Samanta,Sheldon Williamson*

Main category: cs.LG

TL;DR: Diagnostic-driven adaptive learning framework that decomposes error evolution into bias (drift), noise (stochastic variability), and alignment (recurrent excitation) to serve as a unified backbone across supervised optimization, actor-critic RL, and learned optimizers, enabling stable, interpretable online adaptation in nonstationary environments.


<details>
  <summary>Details</summary>
Motivation: In nonstationary and safety-critical settings, learning systems often become unstable or converge slowly. While many optimization/RL/meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal. A principled, online diagnostic of error evolution could provide a robust, architecture- and task-agnostic control signal for adaptation.

Method: Decompose error evolution into three components—bias (persistent drift), noise (stochastic variability), and alignment (repeated directional excitation leading to overshoot). Compute these diagnostics online from lightweight statistics of loss or temporal-difference (TD) error trajectories. Build a unifying control backbone from these diagnostics and instantiate three algorithms: (i) a stabilized supervised optimizer, (ii) a diagnostic-regulated actor-critic scheme, and (iii) a diagnostic-conditioned learned optimizer. Prove bounded effective updates and stability under standard smoothness assumptions. Use diagnostic illustrations in actor-critic settings to show modulation of adaptation by TD error structure.

Result: A unifying, interpretable framework for adaptive learning that treats error evolution as a first-class object and provides lightweight, online diagnostics that inform stable learning across different paradigms. Three concrete instantiations demonstrate practical applicability and stable updates with asserted theoretical guarantees.

Conclusion: Diagnostic decomposition of error evolution (bias, noise, alignment) offers a concise, architecture-agnostic toolkit for reliable learning in dynamic environments, enabling interpretable control of adaptation across supervised learning, RL, and learned optimization.

Abstract: Learning systems deployed in nonstationary and safety-critical environments often suffer from instability, slow convergence, or brittle adaptation when learning dynamics evolve over time. While modern optimization, reinforcement learning, and meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal itself. This paper proposes a diagnostic-driven adaptive learning framework that explicitly models error evolution through a principled decomposition into bias, capturing persistent drift; noise, capturing stochastic variability; and alignment, capturing repeated directional excitation leading to overshoot. These diagnostics are computed online from lightweight statistics of loss or temporal-difference error trajectories and are independent of model architecture or task domain. We show that the proposed bias-noise-alignment decomposition provides a unifying control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers. Building on this framework, we derive diagnostic-driven instantiations including a stabilized supervised optimizer, a diagnostic-regulated actor-critic scheme, and a diagnostic-conditioned learned optimizer. Under standard smoothness assumptions, we establish bounded effective updates and stability properties for all cases. Representative diagnostic illustrations in actor-critic learning highlight how the proposed signals modulate adaptation in response to temporal-difference error structure. Overall, this work elevates error evolution to a first-class object in adaptive learning and provides an interpretable, lightweight foundation for reliable learning in dynamic environments.

</details>


### [19] [Improved Bounds for Private and Robust Alignment](https://arxiv.org/abs/2512.23816)
*Wenqian Weng,Yi He,Xingyu Zhou*

Main category: cs.LG

TL;DR: 本论文在理论层面研究语言模型的私有化与鲁棒对齐，给出离线和在线设置下子最优性差的上界。结合隐私约束与对抗性污染，考察两种顺序耦合（隐私优先与污染优先）。对仅隐私场景，证明以MLE风格算法处理的对数损失近似最优；在联合隐私与污染设置中，发现现有的离线算法实际上提供比以往理论更强的 guarantees，且在污染-单独场景下得到改进界限。此外，首次给出私有且鲁棒在线对齐的结果。方法基于对对数损失和平方损失在隐私和污染下的统一收敛性的新证明。


<details>
  <summary>Details</summary>
Motivation: 在保护数据隐私的前提下实现语言模型的对齐，同时对抗数据污染和对抗性扰动，弥补现有理论在私有化和鲁棒性耦合方面的不足，并提供可扩展到更广泛学习理论与统计学的统一收敛性工具。

Method: 通过对对数损失和平方损失在隐私保护（如差分隐私等）和污染条件下的统一收敛性分析构建子最优性差上界；区分两种耦合顺序（隐私优先、污染优先），分别推导离线与在线场景的界限；比较MLE风格的对数损失最优性、离线算法在联合隐私与污染的强度下的表现，以及在线场景的首次私有鲁棒分析。

Result: 给出隐私独立场景下对数损失+MLE风格算法的近似最优率；在联合隐私与污染的设定下，离线算法提供比之前更强的保证，并提升污染-仅场景的界限；首次给出私有且鲁棒的在线对齐结果；通过新颖的统一收敛性证明，适用于更广泛的学习理论与统计学问题。

Conclusion: 为私有化与鲁棒对齐提供系统的理论框架，揭示隐私与污染之间的相互影响及其对学习边界的影响，扩展现有理论至在线场景并提供可广泛应用的收敛性工具。

Abstract: In this paper, we study the private and robust alignment of language models from a theoretical perspective by establishing upper bounds on the suboptimality gap in both offline and online settings. We consider preference labels subject to privacy constraints and/or adversarial corruption, and analyze two distinct interplays between them: privacy-first and corruption-first. For the privacy-only setting, we show that log loss with an MLE-style algorithm achieves near-optimal rates, in contrast to conventional wisdom. For the joint privacy-and-corruption setting, we first demonstrate that existing offline algorithms in fact provide stronger guarantees -- simultaneously in terms of corruption level and privacy parameters -- than previously known, which further yields improved bounds in the corruption-only regime. In addition, we also present the first set of results for private and robust online alignment. Our results are enabled by new uniform convergence guarantees for log loss and square loss under privacy and corruption, which we believe have broad applicability across learning theory and statistics.

</details>


### [20] [MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control](https://arxiv.org/abs/2512.24955)
*Yongwei Zhang,Yuanzhe Xing,Quan Quan,Zhikun She*

Main category: cs.LG

TL;DR: MSACL 将指数稳定性理论与最大熵强化学习结合，通过多步 Lyapunov 证据学习来实现无模型、 provable 稳定性的安全 RL 框架。通过引入 Exponential Stability Labels (ESL) 与 λ 加权聚合来平衡多步学习中的偏差-方差，使用稳定性导向的优势函数进行策略优化。实验在六个基准任务上显示在简单奖励下实现指数稳定、快速收敛，并对不确定性具有鲁棒性，且对未见轨迹具有泛化能力。多步步长 n=20 被验证为稳健默认值。该方法为将 Lyapunov 理论与离线/按需学习的 Actor-Critic 框架结合提供了可验证的安全学习基础，且将公开源码和基准环境。


<details>
  <summary>Details</summary>
Motivation: 在模型无关的强化学习中实现可证明的稳定性与安全性仍然是一个挑战。现有方法往往依赖复杂的奖励设计或无法提供明确的理论稳定性保证。通过将 Lyapunov 稳定性理论与最大熵 RL 结合，利用多步数据学习 Lyapunov 证据，可在无模型设置下实现带理论保障的学习控制。

Method: 提出 MSACL 框架，将指数稳定性与最大熵 RL 融合；通过多步 Lyapunov 证据学习从离线/离策略数据中学习 Lyapunov 证据，得到 Exponential Stability Labels (ESL)；引入 λ 加权聚合以平衡多步学习中的偏差与方差；以稳定性导向的优势函数引导策略优化，使策略实现 Lyapunov 指向的下降；在六个基准任务（含稳定化与非线性跟踪）上评估。

Result: 在六个基准任务上优于最先进的基于 Lyapunov 的 RL 算法；在简单奖励下实现指数稳定与快速收敛，具备对不确定性鲁棒性和对未见轨迹的泛化能力；敏感度分析显示多步时域 n=20 为稳健默认值。

Conclusion: 将 Lyapunov 理论与离策略 actor-critic 框架结合，提供可验证的安全学习控制基础；代码和基准环境将公开提供。

Abstract: Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $λ$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.

</details>


### [21] [Exploiting the Prior of Generative Time Series Imputation](https://arxiv.org/abs/2512.23832)
*YuYang Miao,Chang Li,Zehua Chen*

Main category: cs.LG

TL;DR: Bridge-TS 提出面向时序填补的 data-to-data 生成框架，利用专家先验和组合先验等来自预训练模型的信息，显著提升缺失值的插补精度，在 ETT、Exchange、Weather 数据集上以均方误差（MSE）和平均绝对误差（MAE）达到新记录。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式缺失值填补方法（如扩散模型、Schrödinger 桥）对先验信息要求较低，导致生成过程负担大且准确性受限。需要引入更有信息性的先验以引导生成，提升插补性能。

Method: 提出 Bridge-TS：数据到数据的生成过程。两种新颖先验设计：1) 专家先验：用预训练的 Transformer 模块作为专家，给出缺失值的确定性估计，并以此作为 ground-truth 的先验；2) 组合先验：利用多个预训练模型给出不同的估计结果，在数据到数据生成过程中进行组合，形成复合型先验以提高对目标的引导。整体流程通过将先验引导整合进生成过程以提高插补准确性。

Result: 在多组基准数据集（如 ETT、Exchange、Weather）上，Bridge-TS 实现了更低的 MSE 和 MAE，达到新的插补精度记录，证明改进先验设计对于生成式时序缺失值填补的有效性。

Conclusion: 通过引入信息更丰富的先验（专家先验与组合先验），Bridge-TS 显著提升时序缺失值插补的效果，验证了先验设计在数据到数据生成框架中的重要性。未来工作可能聚焦于不同先验来源的鲁棒性、计算开销及对更多数据集的泛化能力。

Abstract: Time series imputation, i.e., filling the missing values of a time recording, finds various applications in electricity, finance, and weather modelling. Previous methods have introduced generative models such as diffusion probabilistic models and Schrodinger bridge models to conditionally generate the missing values from Gaussian noise or directly from linear interpolation results. However, as their prior is not informative to the ground-truth target, their generation process inevitably suffer increased burden and limited imputation accuracy. In this work, we present Bridge-TS, building a data-to-data generation process for generative time series imputation and exploiting the design of prior with two novel designs. Firstly, we propose expert prior, leveraging a pretrained transformer-based module as an expert to fill the missing values with a deterministic estimation, and then taking the results as the prior of ground truth target. Secondly, we explore compositional priors, utilizing several pretrained models to provide different estimation results, and then combining them in the data-to-data generation process to achieve a compositional priors-to-target imputation process. Experiments conducted on several benchmark datasets such as ETT, Exchange, and Weather show that Bridge-TS reaches a new record of imputation accuracy in terms of mean square error and mean absolute error, demonstrating the superiority of improving prior for generative time series imputation.

</details>


### [22] [Trellis: Learning to Compress Key-Value Memory in Attention Models](https://arxiv.org/abs/2512.23852)
*Mahdi Karami,Ali Behrouz,Praneeth Kacham,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 提出了 Trellis，一种具有有界内存的 Transformer 架构，通过在测试时动态压缩键值记忆，替换原有的 KV 缓存为固定大小的记忆单元，使用两阶段递归压缩和在线梯度下降的忘记门更新机制来更新记忆。


<details>
  <summary>Details</summary>
Motivation: Transformer 的二次复杂度和不断增长的 KV 缓存限制了长上下文处理能力。因此需要一个在推理阶段能够以有限内存有效地记忆并压缩历史上下文的机制。

Method: 引入固定大小的记忆单元来替代标准 KV 缓存，设计两遍递归压缩过程以将新键值写入记忆；采用在线梯度下降与忘记门的机制，使记忆能够递归更新并在测试阶段保留重要的上下文信息。

Result: 在语言建模、常识推理、需要高 recalls 的任务以及时间序列等多种任务上，Trellis 显著优于强基线，且性能增益随序列长度增加而增大，展现出在长上下文场景中的潜力。

Conclusion: Trellis 成功实现了有界内存的 Transformer，并通过动态压缩记忆在测试阶段提升长上下文处理能力，适合对记忆容量敏感的长序列任务。

Abstract: Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications.

</details>


### [23] [CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts](https://arxiv.org/abs/2512.24564)
*Shunbo Jia,Caizhi Liao*

Main category: cs.LG

TL;DR: Propose CPR, a causal physiological representation learning method for ECG that uses a structural causal model to disentangle invariant pathological QRS-T morphology from non-causal artifacts, achieving robustness against smooth adversarial perturbations with real-time inference and competitive certified robustness.


<details>
  <summary>Details</summary>
Motivation: ECG DL models are fragile to adversarial perturbations, particularly SAP. Adversarial training is computationally expensive and certified defenses like randomized smoothing add latency. There is a need for robust, efficient, clinically interpretable methods that leverage physiological priors.

Method: Introduce CPR with a Physiological Structural Prior framed as a Structural Causal Model (SCM) to enforce a structural intervention that isolates invariant pathological morphology (P-QRS-T complex) from non-causal artifacts via causal disentanglement.

Result: On PTB-XL, CPR outperforms standard clinical preprocessing under SAP; F1=0.632 vs 0.541 for Median Smoothing (9.1% improvement). CPR matches the certified robustness of Randomized Smoothing while enabling single-pass inference.

Conclusion: CPR provides a superior balance among robustness, efficiency, and interpretability by integrating physiological structure into causal representation learning, enabling robust ECG diagnosis under perturbations without prohibitive computation.

Abstract: Deep learning models for Electrocardiogram (ECG) diagnosis have achieved remarkable accuracy but exhibit fragility against adversarial perturbations, particularly Smooth Adversarial Perturbations (SAP) that mimic biological morphology. Existing defenses face a critical dilemma: Adversarial Training (AT) provides robustness but incurs a prohibitive computational burden, while certified methods like Randomized Smoothing (RS) introduce significant inference latency, rendering them impractical for real-time clinical monitoring. We posit that this vulnerability stems from the models' reliance on non-robust spurious correlations rather than invariant pathological features. To address this, we propose Causal Physiological Representation Learning (CPR). Unlike standard denoising approaches that operate without semantic constraints, CPR incorporates a Physiological Structural Prior within a causal disentanglement framework. By modeling ECG generation via a Structural Causal Model (SCM), CPR enforces a structural intervention that strictly separates invariant pathological morphology (P-QRS-T complex) from non-causal artifacts. Empirical results on PTB-XL demonstrate that CPR significantly outperforms standard clinical preprocessing methods. Specifically, under SAP attacks, CPR achieves an F1 score of 0.632, surpassing Median Smoothing (0.541 F1) by 9.1%. Crucially, CPR matches the certified robustness of Randomized Smoothing while maintaining single-pass inference efficiency, offering a superior trade-off between robustness, efficiency, and clinical interpretability.

</details>


### [24] [Flow Matching Neural Processes](https://arxiv.org/abs/2512.23853)
*Hussen Abu Hamad,Dan Rosenbaum*

Main category: cs.LG

TL;DR: 提出一个基于流量匹配的神经过程（Neural Process, NP）模型，用于从数据学习随机过程，并以流式变换和ODE求解实现条件采样。相较于以往 NP 方法，实现更简单、可通过ODE步数在准确性与运行时间间进行权衡，且在合成一维高斯、二维图像及真实天气数据等基准上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升神经过程在条件分布建模、采样与推断方面的性能与可用性，同时提供一个简单且可控的实现框架，通过流匹配将生成建模与NP框架结合，利用ODE求解进行高效条件采样。

Method: 将流量匹配（flow matching）引入NP框架，学习从数据到条件分布的映射，提供对任意数据点的条件分布的 amortized 预测；通过ODE求解器进行条件采样，允许通过降低/增加步骤数来权衡准确性与运行时间，避免依赖额外的条件化方法。

Result: 在多类基准上优于现有的神经过程方法：包括合成的一维高斯过程数据、二维图像和真实气象数据的任务。预计具有更简单的实现和更灵活的采样能力。

Conclusion: 该模型扩展了NP的应用场景，提供一种简单、可控且具备良好性能的条件分布采样方法，适合作为NP领域的新基线或实用工具。

Abstract: Neural processes (NPs) are a class of models that learn stochastic processes directly from data and can be used for inference, sampling and conditional sampling. We introduce a new NP model based on flow matching, a generative modeling paradigm that has demonstrated strong performance on various data modalities. Following the NP training framework, the model provides amortized predictions of conditional distributions over any arbitrary points in the data. Compared to previous NP models, our model is simple to implement and can be used to sample from conditional distributions using an ODE solver, without requiring auxiliary conditioning methods. In addition, the model provides a controllable tradeoff between accuracy and running time via the number of steps in the ODE solver. We show that our model outperforms previous state-of-the-art neural process methods on various benchmarks including synthetic 1D Gaussian processes data, 2D images, and real-world weather data.

</details>


### [25] [Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding](https://arxiv.org/abs/2512.23858)
*Yue Guan,Changming Yu,Shihan Fang,Weiming Hu,Zaifeng Pan,Zheng Wang,Zihan Liu,Yangjie Zhou,Yufei Ding,Minyi Guo,Jingwen Leng*

Main category: cs.LG

TL;DR: Yggdrasil是一种协同设计的延迟最优猜测解码系统，通过等增树、延迟感知草拟目标和分阶段调度实现对未修改的LLM的高效推理，在多种硬件上对现有基线实现最多达到约3.98×的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的猜测解码虽能提升推理吞吐，但由于动态猜测与静态运行时假设之间的错配，导致性能受限，需要实现更好的静态计划与动态猜测之间的一致性，以及在不改动模型前提下的高效执行。

Method: 与LLM共同设计：提出等增树以兼容静态图；引入面向延迟的草拟选择优化目标；采用分阶段调度以降低开销；支持未修改的LLM，确保在现有框架内实现高效推理。

Result: 在多种硬件设置上，相较于最先进基线，Yggdrasil实现了最高约3.98×的加速。

Conclusion: Yggdrasil通过上下文感知的草拟与编译器友好执行，实现了延迟最优的猜测解码，兼容现有LLM并显著提升性能表现。

Abstract: Speculative decoding improves LLM inference by generating and verifying multiple tokens in parallel, but existing systems suffer from suboptimal performance due to a mismatch between dynamic speculation and static runtime assumptions. We present Yggdrasil, a co-designed system that enables latency-optimal speculative decoding through context-aware tree drafting and compiler-friendly execution. Yggdrasil introduces an equal-growth tree structure for static graph compatibility, a latency-aware optimization objective for draft selection, and stage-based scheduling to reduce overhead. Yggdrasil supports unmodified LLMs and achieves up to $3.98\times$ speedup over state-of-the-art baselines across multiple hardware setups.

</details>


### [26] [Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining](https://arxiv.org/abs/2512.23862)
*Ruizhe Huang,Kexuan Zhang,Yihao Fang,Baifeng Yu*

Main category: cs.LG

TL;DR: 用 Infini-attention 的小规模预训练提升小语言模型的长上下文检索能力，实证表明在 16k 上下文下可比基线高出约 31%，记忆压缩平衡因子对性能关键。


<details>
  <summary>Details</summary>
Motivation: 在数据和算力有限的情况下，使小型语言模型具备更强的长上下文推理能力，同时降低成本并提升低资源环境的可达性。

Method: 对 300M 参数的 LLaMA 模型进行 Infini-attention 的预训练，构建来自历史段落的压缩记忆以保持局部注意力，评估在不同上下文长度下的检索/推理性能，分析记忆压缩次数对检索准确性的影响。

Result: 训练稳定性良好；在长上下文检索方面优于基线；重复记忆压缩会降低检索准确性，但 Infini-attention 仍显著提升性能；在 16,384-token 上下文下，性能提升达到约 31%。

Conclusion: 在小型语言模型中，通过引入类似 Infini-attention 的记忆结构可以显著提升长上下文能力，记忆设计是提升 SLM 长上下文性能的关键。

Abstract: This study investigates small-scale pretraining for Small Language Models (SLMs) to enable efficient use of limited data and compute, improve accessibility in low-resource settings and reduce costs. To enhance long-context extrapolation in compact models, we focus on Infini-attention, which builds a compressed memory from past segments while preserving local attention. In our work, we conduct an empirical study using 300M-parameter LLaMA models pretrained with Infini-attention. The model demonstrates training stability and outperforms the baseline in long-context retrieval. We identify the balance factor as a key part of the model performance, and we found that retrieval accuracy drops with repeated memory compressions over long sequences. Even so, Infini-attention still effectively compensates for the SLM's limited parameters. Particularly, despite performance degradation at a 16,384-token context, the Infini-attention model achieves up to 31% higher accuracy than the baseline. Our findings suggest that achieving robust long-context capability in SLMs benefits from architectural memory like Infini-attention.

</details>


### [27] [Max-Entropy Reinforcement Learning with Flow Matching and A Case Study on LQR](https://arxiv.org/abs/2512.23870)
*Yuyang Zhang,Yang Hu,Bo Dai,Na Li*

Main category: cs.LG

TL;DR: 提出在 SAC 框架中使用流式模型参数化策略，并引入在线流匹配（ISFM）进行策略更新，结合瞬时可变换进行评估，给出理论分析关于采样分布对学习效率的影响，并在最大熵线性二次调控问题上验证可学习到最优行动分布。


<details>
  <summary>Details</summary>
Motivation: SAC 的能量基策略往往用简单的策略族近似，表达力和鲁棒性受限。引入流式模型可显著提高策略的表达能力；结合在线流匹配使得策略更新可仅依赖于用户指定的采样分布的样本，从而提升实用性与稳定性。

Method: 将策略参数化为流模型；利用瞬时变量变换来对策略进行高效评估；提出 online 的流匹配算法 ISFM，并通过重要性采样实现对目标分布的采样外部更新；对 ISFM 的理论性质进行分析，尤其是不同采样分布对学习效率的影响。

Result: 在一个关于最大熵线性二次调控（max-entropy LQR）的案例中验证，该算法能够学习到最优的行动分布。

Conclusion: 通过将流式策略与 ISFM 相结合，提升了 SAC 的表达力和在最大熵设置下的学习能力，理论分析与数值案例共同支持其有效性。

Abstract: Soft actor-critic (SAC) is a popular algorithm for max-entropy reinforcement learning. In practice, the energy-based policies in SAC are often approximated using simple policy classes for efficiency, sacrificing the expressiveness and robustness. In this paper, we propose a variant of the SAC algorithm that parameterizes the policy with flow-based models, leveraging their rich expressiveness. In the algorithm, we evaluate the flow-based policy utilizing the instantaneous change-of-variable technique and update the policy with an online variant of flow matching developed in this paper. This online variant, termed importance sampling flow matching (ISFM), enables policy update with only samples from a user-specified sampling distribution rather than the unknown target distribution. We develop a theoretical analysis of ISFM, characterizing how different choices of sampling distributions affect the learning efficiency. Finally, we conduct a case study of our algorithm on the max-entropy linear quadratic regulator problems, demonstrating that the proposed algorithm learns the optimal action distribution.

</details>


### [28] [DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks](https://arxiv.org/abs/2512.23948)
*Kacem Khaled,Felipe Gohring de Magalhães,Gabriela Nicolescu*

Main category: cs.LG

TL;DR: 提出 DivQAT，在量化感知训练中嵌入防抽取机制，提升量化 CNN 对模型抽取攻击的鲁棒性，同时保持精度，并可与其他防御联合增强效果。


<details>
  <summary>Details</summary>
Motivation: 模型抽取攻击对知识产权构成严重威胁，特别是在边缘设备上部署的量化模型；现有防御多为训练后注入噪声，设计阶段未整合，计算成本高且假设不现实。

Method: 在 Quantization Aware Training 框架下，修改量化过程，将模型抽取防御目标纳入训练，以训练阶段即获得鲁棒性，同时可与其他防御结合提升效果。

Result: 在基准视觉数据集上实验表明，DivQAT 能在不降低模型精度的前提下提升对模型抽取攻击的鲁棒性，并且与其他防御机制结合时优于传统 QAT。

Conclusion: 将防御目标嵌入量化训练之中，可显著提升对模型抽取的鲁棒性，并具有与其他防御协同增效的潜力。

Abstract: Convolutional Neural Networks (CNNs) and their quantized counterparts are vulnerable to extraction attacks, posing a significant threat of IP theft. Yet, the robustness of quantized models against these attacks is little studied compared to large models. Previous defenses propose to inject calculated noise into the prediction probabilities. However, these defenses are limited since they are not incorporated during the model design and are only added as an afterthought after training. Additionally, most defense techniques are computationally expensive and often have unrealistic assumptions about the victim model that are not feasible in edge device implementations and do not apply to quantized models. In this paper, we propose DivQAT, a novel algorithm to train quantized CNNs based on Quantization Aware Training (QAT) aiming to enhance their robustness against extraction attacks. To the best of our knowledge, our technique is the first to modify the quantization process to integrate a model extraction defense into the training process. Through empirical validation on benchmark vision datasets, we demonstrate the efficacy of our technique in defending against model extraction attacks without compromising model accuracy. Furthermore, combining our quantization technique with other defense mechanisms improves their effectiveness compared to traditional QAT.

</details>


### [29] [Interactive Machine Learning: From Theory to Scale](https://arxiv.org/abs/2512.23924)
*Yinglun Zhu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning has achieved remarkable success across a wide range of applications, yet many of its most effective methods rely on access to large amounts of labeled data or extensive online interaction. In practice, acquiring high-quality labels and making decisions through trial-and-error can be expensive, time-consuming, or risky, particularly in large-scale or high-stakes settings. This dissertation studies interactive machine learning, in which the learner actively influences how information is collected or which actions are taken, using past observations to guide future interactions. We develop new algorithmic principles and establish fundamental limits for interactive learning along three dimensions: active learning with noisy data and rich model classes, sequential decision making with large action spaces, and model selection under partial feedback. Our results include the first computationally efficient active learning algorithms achieving exponential label savings without low-noise assumptions; the first efficient, general-purpose contextual bandit algorithms whose guarantees are independent of the size of the action space; and the first tight characterizations of the fundamental cost of model selection in sequential decision making. Overall, this dissertation advances the theoretical foundations of interactive learning by developing algorithms that are statistically optimal and computationally efficient, while also providing principled guidance for deploying interactive learning methods in large-scale, real-world settings.

</details>


### [30] [Improved Balanced Classification with Theoretically Grounded Loss Functions](https://arxiv.org/abs/2512.23947)
*Corinna Cortes,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The balanced loss is a widely adopted objective for multi-class classification under class imbalance. By assigning equal importance to all classes, regardless of their frequency, it promotes fairness and ensures that minority classes are not overlooked. However, directly minimizing the balanced classification loss is typically intractable, which makes the design of effective surrogate losses a central question. This paper introduces and studies two advanced surrogate loss families: Generalized Logit-Adjusted (GLA) loss functions and Generalized Class-Aware weighted (GCA) losses. GLA losses generalize Logit-Adjusted losses, which shift logits based on class priors, to the broader general cross-entropy loss family. GCA loss functions extend the standard class-weighted losses, which scale losses inversely by class frequency, by incorporating class-dependent confidence margins and extending them to the general cross-entropy family. We present a comprehensive theoretical analysis of consistency for both loss families. We show that GLA losses are Bayes-consistent, but only $H$-consistent for complete (i.e., unbounded) hypothesis sets. Moreover, their $H$-consistency bounds depend inversely on the minimum class probability, scaling at least as $1/\mathsf p_{\min}$. In contrast, GCA losses are $H$-consistent for any hypothesis set that is bounded or complete, with $H$-consistency bounds that scale more favorably as $1/\sqrt{\mathsf p_{\min}}$, offering significantly stronger theoretical guarantees in imbalanced settings. We report the results of experiments demonstrating that, empirically, both the GCA losses with calibrated class-dependent confidence margins and GLA losses can greatly outperform straightforward class-weighted losses as well as the LA losses. GLA generally performs slightly better in common benchmarks, whereas GCA exhibits a slight edge in highly imbalanced settings.

</details>


### [31] [Physics-informed Graph Neural Networks for Operational Flood Modeling](https://arxiv.org/abs/2512.23964)
*Carlo Malapad Acosta,Herath Mudiyanselage Viraj Vidura Herath,Jia Yu Lim,Abhishek Saha,Sanka Rasnayaka,Lucy Marshall*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Flood models inform strategic disaster management by simulating the spatiotemporal hydrodynamics of flooding. While physics-based numerical flood models are accurate, their substantial computational cost limits their use in operational settings where rapid predictions are essential. Models designed with graph neural networks (GNNs) provide both speed and accuracy while having the ability to process unstructured spatial domains. Given its flexible input and architecture, GNNs can be leveraged alongside physics-informed techniques with ease, significantly improving interpretability. This study introduces a novel flood GNN architecture, DUALFloodGNN, which embeds physical constraints at both global and local scales through explicit loss terms. The model jointly predicts water volume at nodes and flow along edges through a shared message-passing framework. To improve performance for autoregressive inference, model training is conducted with a multi-step loss enhanced with dynamic curriculum learning. Compared with standard GNN architectures and state-of-the-art GNN flood models, DUALFloodGNN achieves substantial improvements in predicting multiple hydrologic variables while maintaining high computational efficiency. The model is open-sourced at https://github.com/acostacos/dual_flood_gnn.

</details>


### [32] [Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing](https://arxiv.org/abs/2512.23977)
*Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: DataFlow 提供一个基于有向无环图的点时幂等性框架，用于将批处理模型无缝迁移到流式生产，同时在时间序列数据上实现严格因果性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 传统数据科学工作流依赖有限数据集，批-生产迁移需大量重复实现，易引入因果性违背、批界遗留和实时失败不可重复性等问题，需要一个统一、可扩展的执行模型来解决。

Method: 通过在 DAG 中实现点时幂等性，输出仅依赖前置的固定长度上下文；自动跟踪知识时间以避免未来信息；支持跨时序与特征维度的切块（tiling），通过配置在不同频率和内存配置下运行；与 Python 数据生态无缝集成，提供 online learning 的 fit/predict、缓存、增量计算和基于 DAG 调度的并行化。

Result: 在金融交易、物联网、欺诈检测和实时分析等领域的多域场景中展示有效性，证明同一模型可在批处理与流式生产中保持一致，提升可重复性与实时性，并减少代码修改。

Conclusion: DataFlow 为在无限时间序列数据上的高性能机器学习系统的构建、测试与部署提供一个统一、可配置且具强因果性保障的框架，适用于需要低延迟和跨频率部署的生产化场景。

Abstract: We present DataFlow, a computational framework for building, testing, and deploying high-performance machine learning systems on unbounded time-series data. Traditional data science workflows assume finite datasets and require substantial reimplementation when moving from batch prototypes to streaming production systems. This gap introduces causality violations, batch boundary artifacts, and poor reproducibility of real-time failures.
  DataFlow resolves these issues through a unified execution model based on directed acyclic graphs (DAGs) with point-in-time idempotency: outputs at any time t depend only on a fixed-length context window preceding t. This guarantee ensures that models developed in batch mode execute identically in streaming production without code changes. The framework enforces strict causality by automatically tracking knowledge time across all transformations, eliminating future-peeking bugs.
  DataFlow supports flexible tiling across temporal and feature dimensions, allowing the same model to operate at different frequencies and memory profiles via configuration alone. It integrates natively with the Python data science stack and provides fit/predict semantics for online learning, caching and incremental computation, and automatic parallelization through DAG-based scheduling. We demonstrate its effectiveness across domains including financial trading, IoT, fraud detection, and real-time analytics.

</details>


### [33] [Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems](https://arxiv.org/abs/2512.23978)
*Tinglong Dai,David Simchi-Levi,Michelle Xiao Wu,Yao Xie*

Main category: cs.LG

TL;DR: 提出一个两支撑的“被保证的自主性”框架，结合基于流的确定性传输生成模型（ODE/最优传输、鲁棒优化、序决策控制）与对抗鲁棒性评估（在不确定性集合内的最坏扰动下测试决策规则），将运筹学角色从求解器转变为防护栏再到系统架构师，面向安全关键、高可靠性场景的研究议程。


<details>
  <summary>Details</summary>
Motivation: 生成性人工智能正从对话助手走向具备操作自治的系统，但自主性提升带来更强的结构、约束和尾部风险控制需求。若只依赖随机性生成模型，系统在分布偏移和高风险情境中易失稳，因此需要可验证性、鲁棒性和压力测试机制以保障可行性与安全性。

Method: 提出两大互补路径：1) 流式生成模型，将生成视为通过ODE描述的确定性传输，提升可审计性、约束感知生成，并连接最优传输、鲁棒优化及序列决策控制。2) 将运营安全以对抗鲁棒性为视角，评估决策规则在不确定性/模糊性集合中的最坏扰动，以把未建模风险纳入设计。

Result: 构建了一个概念框架，明确了在更高自治水平下运筹学（OR）的角色雏形，即从 solver（求解器）转向 guardrail（防护栏）再到 system architect（系统架构师），并提出在安全关键、高可靠性领域落实的研究议程、设计原则与职责分配。

Conclusion: 强调实现安全关键与高可靠性场景下的“assured autonomy”的必要性，要求将控制逻辑、激励机制、监控机制与安全边界等要素系统化地嵌入设计之中，并推动相关研究向以OR为核心的 guardrails 与系统架构能力演进。

Abstract: Generative artificial intelligence (GenAI) is shifting from conversational assistants toward agentic systems -- autonomous decision-making systems that sense, decide, and act within operational workflows. This shift creates an autonomy paradox: as GenAI systems are granted greater operational autonomy, they should, by design, embody more formal structure, more explicit constraints, and stronger tail-risk discipline. We argue stochastic generative models can be fragile in operational domains unless paired with mechanisms that provide verifiable feasibility, robustness to distribution shift, and stress testing under high-consequence scenarios. To address this challenge, we develop a conceptual framework for assured autonomy grounded in operations research (OR), built on two complementary approaches. First, flow-based generative models frame generation as deterministic transport characterized by an ordinary differential equation, enabling auditability, constraint-aware generation, and connections to optimal transport, robust optimization, and sequential decision control. Second, operational safety is formulated through an adversarial robustness lens: decision rules are evaluated against worst-case perturbations within uncertainty or ambiguity sets, making unmodeled risks part of the design. This framework clarifies how increasing autonomy shifts OR's role from solver to guardrail to system architect, with responsibility for control logic, incentive protocols, monitoring regimes, and safety boundaries. These elements define a research agenda for assured autonomy in safety-critical, reliability-sensitive operational domains.

</details>


### [34] [Information-Theoretic Quality Metric of Low-Dimensional Embeddings](https://arxiv.org/abs/2512.23981)
*Sebastián Gutiérrez-Bernal,Hector Medel Cobaxin,Abiel Galindo González*

Main category: cs.LG

TL;DR: ERPM是一个基于香农熵与稳定秩的局部信息保留度量，补充现有距离/几何指标，用于评估降维嵌入在信息保留方面的表现，并能揭示局部信息丢失区域。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标（如Stress、邻域排序误差、Local Procrustes）关注距离或几何变形，但未直接衡量高维信息在降维中被保留的程度。因此需要一个信息理论驱动的指标来评价嵌入的信息保留能力。

Method: 提出Entropy Rank Preservation Measure (ERPM)：基于邻域矩阵的特征值谱的Shannon熵以及稳定秩，计算局部信息的不确定性变化，并给出局部指标与全局摘要。通过与基于距离的MRRE和基于几何的Local Procrustes比较，使用金融时间序列和常见流形进行验证。

Result: 发现基于距离的指标与几何及谱相关指标的相关性很低；ERPM与Local Procrustes在平均水平上相关性较高，但在局部子区域存在显著差异，说明ERPM能揭示信息损失严重的邻域。

Conclusion: ERPM作为对现有指标的有力补充，提供更全面的嵌入评估，尤其在信息敏感应用中有助于识别潜在的信息丢失区域，如早期预警指标的构建。

Abstract: In this work we study the quality of low-dimensional embeddings from an explicitly information-theoretic perspective. We begin by noting that classical evaluation metrics such as stress, rank-based neighborhood criteria, or Local Procrustes quantify distortions in distances or in local geometries, but do not directly assess how much information is preserved when projecting high-dimensional data onto a lower-dimensional space. To address this limitation, we introduce the Entropy Rank Preservation Measure (ERPM), a local metric based on the Shannon entropy of the singular-value spectrum of neighborhood matrices and on the stable rank, which quantifies changes in uncertainty between the original representation and its reduced projection, providing neighborhood-level indicators and a global summary statistic. To validate the results of the metric, we compare its outcomes with the Mean Relative Rank Error (MRRE), which is distance-based, and with Local Procrustes, which is based on geometric properties, using a financial time series and a manifold commonly studied in the literature. We observe that distance-based criteria exhibit very low correlation with geometric and spectral measures, while ERPM and Local Procrustes show strong average correlation but display significant discrepancies in local regimes, leading to the conclusion that ERPM complements existing metrics by identifying neighborhoods with severe information loss, thereby enabling a more comprehensive assessment of embeddings, particularly in information-sensitive applications such as the construction of early-warning indicators.

</details>


### [35] [Tracing the Heart's Pathways: ECG Representation Learning from a Cardiac Conduction Perspective](https://arxiv.org/abs/2512.24002)
*Tan Pan,Yixuan Sun,Chen Jiang,Qiong Gao,Rui Sun,Xingmeng Zhang,Zhenqi Yang,Limei Han,Yixiu Liang,Yuan Cheng,Kaiyu Guo*

Main category: cs.LG

TL;DR: 提出 CLEAR-HUG 的两阶段自监督学习框架，通过在第一阶段的 Conduction-LEAd Reconstructor（CLEAR）对心跳进行稀疏注意力重构，以捕捉不同心跳之间的传导差异与共性；第二阶段的 Hierarchical lead-Unified Group head（HUG）按临床工作流实现分层、头部统一的疾病诊断。整体在六项任务上提升 6.84%，并强调将模式与ECG诊断指南对齐。


<details>
  <summary>Details</summary>
Motivation: 现有 ECG self-supervised learning (eSSL) 往往强调不同导联和心跳的一致性，而忽视心跳在传导过程中的细微差异，以及随阈值的生理特征。且诊断流程通常从单个心跳-单导联到多导联组合的分层逻辑未被充分嵌入模型训练与评估。需要一种能够同时捕捉个体心跳的特征差异、保持共性，并与临床诊断路径对齐的学习框架。

Method: 提出两阶段框架：第一阶段 CLEAR 通过对每个心跳作为独立实体，使用稀疏注意力机制对信号进行重构，捕捉特异性变异与共性，减少不同心跳之间的干扰。第二阶段 HUG 作为分层的 lead‑Unified 诊断头，模仿临床工作流，对特征进行分层聚合以实现疾病诊断。

Result: 在六项任务上获得约 6.84% 的性能提升，验证了 CLEAR‑HUG 在提升心传导表示、并使模式更符合专家诊断指南方面的有效性。

Conclusion: CLEAR-HUG 能在提高对心肌传导细节的敏感性和保持跨导联的一致性之间取得权衡，并将模型输出更好地对齐 ECG 的诊断流程与指南。

Abstract: The multi-lead electrocardiogram (ECG) stands as a cornerstone of cardiac diagnosis. Recent strides in electrocardiogram self-supervised learning (eSSL) have brightened prospects for enhancing representation learning without relying on high-quality annotations. Yet earlier eSSL methods suffer a key limitation: they focus on consistent patterns across leads and beats, overlooking the inherent differences in heartbeats rooted in cardiac conduction processes, while subtle but significant variations carry unique physiological signatures. Moreover, representation learning for ECG analysis should align with ECG diagnostic guidelines, which progress from individual heartbeats to single leads and ultimately to lead combinations. This sequential logic, however, is often neglected when applying pre-trained models to downstream tasks. To address these gaps, we propose CLEAR-HUG, a two-stage framework designed to capture subtle variations in cardiac conduction across leads while adhering to ECG diagnostic guidelines. In the first stage, we introduce an eSSL model termed Conduction-LEAd Reconstructor (CLEAR), which captures both specific variations and general commonalities across heartbeats. Treating each heartbeat as a distinct entity, CLEAR employs a simple yet effective sparse attention mechanism to reconstruct signals without interference from other heartbeats. In the second stage, we implement a Hierarchical lead-Unified Group head (HUG) for disease diagnosis, mirroring clinical workflow. Experimental results across six tasks show a 6.84% improvement, validating the effectiveness of CLEAR-HUG. This highlights its ability to enhance representations of cardiac conduction and align patterns with expert diagnostic guidelines.

</details>


### [36] [How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns](https://arxiv.org/abs/2512.24063)
*Haoyue Bai,Yiyou Sun,Wenjie Hu,Shi Qiu,Maggie Ziyu Huan,Peiyang Song,Robert Nowak,Dawn Song*

Main category: cs.LG

TL;DR: RL 调优的语言模型在推理技能上呈现更稳定的行为特征并抵抗崩溃，而 SFT 往往出现漂移与对表面模式的过拟合；提出将推理分解为原子技能的新基准并结合元探测框架，以更细粒度地理解训练后推理的演化。


<details>
  <summary>Details</summary>
Motivation: 现有研究对比中对一般化的影响多依赖粗粒度的准确率指标，尚缺乏对推理在不同训练阶段内部机制的系统理解。需要一个能将推理分解为原子技能的基准，并结合低层统计信号分析，以揭示 SFT 与 RL 下推理技能的出现、迁移与崩溃过程。

Method: 提出一个将推理分解为计算、事实检索、仿真、枚举、诊断等原子技能的基准；通过对这些技能的分离测量，获得对推理本质的更细粒度理解。结合分布偏差、参数统计等低层信号分析，形成面向数学、科学推理和非推理任务的多任务评估框架。开发元探测框架，在不同训练阶段跟踪模型行为。

Result: 发现 RL 调优的模型更能保持稳定的行为轮廓，推理技能不易崩溃；SFT 模型则更易发生漂移并过度拟合表层模式。基于基准的分析揭示推理能力的演化特征及任务间迁移/崩溃趋势。

Conclusion: 为 LLM 推理的本质提供新洞见，并提出在训练设计上促进广泛且稳健泛化的原则。

Abstract: Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.

</details>


### [37] [Multi-Scenario Highway Lane-Change Intention Prediction: A Temporal Physics-Informed Multi-Modal Framework](https://arxiv.org/abs/2512.24075)
*Jiazhao Shi,Ziyu Wang,Yichen Lin,Shoufeng Lu*

Main category: cs.LG

TL;DR: Proposes Temporal Physics-Informed AI (TPI-AI), a hybrid framework that fuses deep temporal embeddings with physics-inspired interaction cues for lane-change intention prediction, using Bi-LSTM embeddings plus physics features and LightGBM classifier, with imbalance-aware optimization; evaluated on highD and exiD with horizons 1–3 s, achieving strong macro-F1 and showing robustness across scenarios.


<details>
  <summary>Details</summary>
Motivation: Lane-change intention prediction is safety-critical for autonomous driving and ADAS but remains challenging due to noisy kinematics, severe class imbalance, and limited generalization across heterogeneous highway scenarios.

Method: Two-layer bidirectional LSTM encoder to learn temporal embeddings from multi-step trajectories; concatenate embeddings with physics-informed features (e.g., headway, TTC, safe-gap) and other interaction-aware cues; train a LightGBM classifier for three classes (No-LC, Left-LC, Right-LC); apply imbalance-aware optimization with resampling/weighting and fold-wise threshold calibration.

Result: On two drone-based datasets, highD (straight highways) and exiD (ramp-rich), TPI-AI achieves macro-F1 at horizons T=1,2,3 s of 0.9562/0.9124/0.8345 (highD) and 0.9247/0.8197/0.7605 (exiD), outperforming standalone LightGBM and Bi-LSTM baselines.

Conclusion: Integrating physics-informed interaction cues with learned temporal embeddings yields robust multi-scenario lane-change intention prediction and improves minority-class reliability, supporting better generalization across heterogeneous highway environments.

Abstract: Lane-change intention prediction is safety-critical for autonomous driving and ADAS, but remains difficult in naturalistic traffic due to noisy kinematics, severe class imbalance, and limited generalization across heterogeneous highway scenarios. We propose Temporal Physics-Informed AI (TPI-AI), a hybrid framework that fuses deep temporal representations with physics-inspired interaction cues. A two-layer bidirectional LSTM (Bi-LSTM) encoder learns compact embeddings from multi-step trajectory histories; we concatenate these embeddings with kinematics-, safety-, and interaction-aware features (e.g., headway, TTC, and safe-gap indicators) and train a LightGBM classifier for three-class intention recognition (No-LC, Left-LC, Right-LC). To improve minority-class reliability, we apply imbalance-aware optimization including resampling/weighting and fold-wise threshold calibration. Experiments on two large-scale drone-based datasets, highD (straight highways) and exiD (ramp-rich environments), use location-based splits and evaluate prediction horizons T = 1, 2, 3 s. TPI-AI outperforms standalone LightGBM and Bi-LSTM baselines, achieving macro-F1 of 0.9562, 0.9124, 0.8345 on highD and 0.9247, 0.8197, 0.7605 on exiD at T = 1, 2, 3 s, respectively. These results show that combining physics-informed interaction features with learned temporal embeddings yields robust multi-scenario lane-change intention prediction.

</details>


### [38] [Autoregressivity in the Latent Space of a GP-VAE Language Model: An Empirical Ablation Study](https://arxiv.org/abs/2512.24102)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: Ablation study shows latent autoregression in GP-VAE improves long-horizon stability and alignment with the Gaussian-process prior; removing it degrades latent structure; the work analyzes representational structure, not proposing a new architecture.


<details>
  <summary>Details</summary>
Motivation: Assess the role of latent autoregression in GP-VAE by comparing full autoregressive latent dynamics, a non-autoregressive latent-variables ablation, and a token-level autoregressive Transformer to understand how latent structure contributes to long-range coherence.

Method: Systematic comparison among three setups: (i) GP-VAE with autoregressive latent dynamics, (ii) GP-VAE with independent (non-autoregressive) latent variables, (iii) standard token-level autoregressive Transformer; evaluation on medium-scale corpora with short training contexts; analysis of latent trajectories and long-horizon behavior.

Result: Latent autoregression yields latent trajectories that are more compatible with the Gaussian-process prior and exhibit greater long-horizon stability; removing autoregression degrades latent structure and long-range behavior.

Conclusion: Latent autoregression acts as an effective mechanism to organize long-range structure and is complementary to token-level autoregression; the work provides an empirical analysis of representational structure rather than a proposal for a new architecture.

Abstract: This paper provides an ablation-based analysis of latent autoregression in GP-VAE models, building upon our previous work introducing the architecture. Language models typically rely on an autoregressive factorization over tokens. In contrast, our prior work proposed shifting sequential structure to the latent space through a causal Gaussian process, while using a non-autoregressive decoder. Here, we conduct a systematic ablation study of the role played by latent autoregression. We compare (i) a full GP-VAE model with autoregressive latent dynamics, (ii) a non-autoregressive ablation in which latent variables are independent, and (iii) a standard token-level autoregressive Transformer. Our results show that, within the considered regime (medium-scale corpora and short training contexts), latent autoregression induces latent trajectories that are significantly more compatible with the Gaussian-process prior and exhibit greater long-horizon stability. In contrast, removing autoregression leads to degraded latent structure and unstable long-range behavior. These findings highlight the role of latent autoregression as an effective mechanism for organizing long-range structure, while remaining complementary to token-level autoregressive modeling. They should be interpreted as an empirical analysis of representational structure rather than as a proposal for a new architecture.

</details>


### [39] [Enhancing LLM Planning Capabilities through Intrinsic Self-Critique](https://arxiv.org/abs/2512.24103)
*Bernd Bohnet,Pierre-Alexandre Kamienny,Hanie Sedghi,Dilan Gorur,Pranjal Awasthi,Aaron Parisi,Kevin Swersky,Rosanne Liu,Azade Nova,Noah Fiedel*

Main category: cs.LG

TL;DR: 提出通过让大语言模型自我批评来改进自己的答案，以提升规划任务的性能，在 Blocksworld、Logistics、Mini-grid 等数据集上达到极优表现；以 few-shot、many-shot 逐步扩展并通过迭代纠错显著提升；与外部验证器无关的内在自我提升方法，达到截至2024年10月的相关模型最佳水平。


<details>
  <summary>Details</summary>
Motivation: 尽管早期研究对自我批评提升的有效性持怀疑态度，但该工作展示了在规划任务中通过内在自我评估来显著提升性能的可行性，且无需外部 verifier。强调自我批评方法具有跨数据集的普适性和模型版本无关性。

Method: 用少量示例（few-shot）逐步扩展到多-shot，通过让模型对自己的回答进行自我批评、修正和迭代 refinement，在 Blocksworld、Logistics、Mini-grid 数据集上进行评估；不调用外部验证器，利用模型内部推理实现自我纠错。

Result: 在规划数据集上显著优于基线，Blocksworld、Logistics、Mini-grid 均取得新的最强性能，达到面向该类模型在2024年10月前后检查点的 state-of-the-art；迭代过程进一步提升效果。

Conclusion: 自我批评是一种可转移的、内在的改进机制，适用于不同模型版本，未来若结合更复杂的搜索策略和更强的模型，性能有望进一步提高。

Abstract: We demonstrate an approach for LLMs to critique their \emph{own} answers with the goal of enhancing their performance that leads to significant improvements over established planning benchmarks. Despite the findings of earlier research that has cast doubt on the effectiveness of LLMs leveraging self critique methods, we show significant performance gains on planning datasets in the Blocksworld domain through intrinsic self-critique, without external source such as a verifier. We also demonstrate similar improvements on Logistics and Mini-grid datasets, exceeding strong baseline accuracies. We employ a few-shot learning technique and progressively extend it to a many-shot approach as our base method and demonstrate that it is possible to gain substantial improvement on top of this already competitive approach by employing an iterative process for correction and refinement. We illustrate how self-critique can significantly boost planning performance. Our empirical results present new state-of-the-art on the class of models considered, namely LLM model checkpoints from October 2024. Our primary focus lies on the method itself, demonstrating intrinsic self-improvement capabilities that are applicable regardless of the specific model version, and we believe that applying our method to more complex search techniques and more capable models will lead to even better performance.

</details>


### [40] [Paired Seed Evaluation: Statistical Reliability for Learning-Based Simulators](https://arxiv.org/abs/2512.24145)
*Udit Sharma*

Main category: cs.LG

TL;DR: 通过在同一组随机种子下对比评估，paired seed evaluation 能显著降低方差，提升统计功效，并在固定预算下提高有效样本量。


<details>
  <summary>Details</summary>
Motivation: 传统独立评估未能利用不同系统间的共同随机性，导致评估结果方差大、统计功效低。

Method: 提出成对种子评估设计，使竞争系统在相同随机种子下进行评估，从而获得匹配的随机组件；在种子层面分析相关性并推导方差降低与样本效率的条件。

Result: 实证发现种子层面的相关性通常为正且显著，能带来数量级的效率提升；在存在正相关时是弱支配的；若相关性不显著则评估等效于独立评估且不损害有效性。

Conclusion: 成对种子评估是一种实用且鲁棒的评估设计，能在存在相关性时显著提升统计可靠性；在相关性不强时不会降低评估的有效性。

Abstract: Machine learning systems appear stochastic but are deterministically random, as seeded pseudorandom number generators produce identical realisations across executions. Learning-based simulators are widely used to compare algorithms, design choices, and interventions under such dynamics, yet evaluation outcomes often exhibit high variance due to random initialisation and learning stochasticity. We analyse the statistical structure of comparative evaluation in these settings and show that standard independent evaluation designs fail to exploit shared sources of randomness across alternatives. We formalise a paired seed evaluation design in which competing systems are evaluated under identical random seeds, inducing matched realisations of stochastic components and strict variance reduction whenever outcomes are positively correlated at the seed level. This yields tighter confidence intervals, higher statistical power, and effective sample size gains at fixed computational budgets. Empirically, seed-level correlations are typically large and positive, producing order-of-magnitude efficiency gains. Paired seed evaluation is weakly dominant in practice, improving statistical reliability when correlation is present and reducing to independent evaluation without loss of validity when it is not.

</details>


### [41] [Early Prediction of Sepsis using Heart Rate Signals and Genetic Optimized LSTM Algorithm](https://arxiv.org/abs/2512.24253)
*Alireza Rafiei,Farshid Hajati,Alireza Rezaee,Amirhossien Panahi,Shahadat Uddin*

Main category: cs.LG

TL;DR: 提出了四种新型机器学习算法，用于在可穿戴设备上通过心率数据预测脓毒症发作，利用遗传算法优化模型结构以兼顾性能、计算复杂度与内存需求，并通过迁移学习将预测窗口从1小时扩展到4小时，显示在非病房环境中的早期诊断潜力。


<details>
  <summary>Details</summary>
Motivation: 脓毒症在感染性疾病中导致高死亡率与医疗成本，需在非病房场景实现对发病的早期预测与干预；可穿戴设备提供持续心率监测，为早期识别提供数据来源，但需面向资源受限的边缘设备进行模型设计。

Method: 设计并评估四种新颖的机器学习算法以预测脓毒症发作，使用遗传算法对模型架构进行优化，兼顾预测性能、计算复杂度和内存占用。初始以1小时预测窗口进行训练，随后通过迁移学习将模型扩展至4小时预测。对模型在可穿戴设备上的实现可行性进行性能评估。

Result: 初步结果令人鼓舞，表明所提模型具有在可穿戴设备上实现的潜力，并可在非ICU/病房环境中实现早期脓毒症检测的可能性。

Conclusion: 可穿戴技术有望支持在医院外进行脓毒症的早期检测，未来需进一步优化边缘设备上的实现、提升时效性与鲁棒性。

Abstract: Sepsis, characterized by a dysregulated immune response to infection, results in significant mortality, morbidity, and healthcare costs. The timely prediction of sepsis progression is crucial for reducing adverse outcomes through early intervention. Despite the development of numerous models for Intensive Care Unit (ICU) patients, there remains a notable gap in approaches for the early detection of sepsis in non-ward settings. This research introduces and evaluates four novel machine learning algorithms designed for predicting the onset of sepsis on wearable devices by analyzing heart rate data. The architecture of these models was refined through a genetic algorithm, optimizing for performance, computational complexity, and memory requirements. Performance metrics were subsequently extracted for each model to evaluate their feasibility for implementation on wearable devices capable of accurate heart rate monitoring. The models were initially tailored for a prediction window of one hour, later extended to four hours through transfer learning. The encouraging outcomes of this study suggest the potential for wearable technology to facilitate early sepsis detection outside ICU and ward environments.

</details>


### [42] [Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction](https://arxiv.org/abs/2512.24324)
*Haojin Li,Anbang Zhang,Chen Sun,Chenyuan Feng,Kaiqian Qu,Tony Q. S. Quek,Haijun Zhang*

Main category: cs.LG

TL;DR: 提出 SaM2B 框架用于在低空无人机通信中进行语义感知的多模态波束预测，通过对模态重要性进行可靠性感知的动态权重更新实现自适应融合，并通过跨模态对比学习对齐表示以提高鲁棒性和泛化能力；在真实低空数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合多采用固定或经验权重，忽视在不同飞行场景中模态可靠性可能大幅变化，导致性能在模态降级时迅速退化且跨场景泛化不足。需引入动态、可靠性感知的权重分配，以及跨模态表示对齐以提升鲁棒性。

Method: SaM2B 通过利用环境视觉、飞行姿态和地理空间数据等轻量线索，在时间维度上进行可靠性感知的动态权重更新以实现多模态贡献的自适应分配；并通过跨模态对比学习，将与波束信息相关的“多源表示波束语义”对齐到一个共享语义空间，从而提升在模态噪声和分布转移下的判别力与鲁棒性。

Result: 在真实世界的低高度 UAV 数据集上，SaM2B 相较基线方法表现更令人满意，体现出在模态噪声和分布转移条件下的鲁棒性提升与预测效果改善。

Conclusion: 所提出的可靠性感知动态加权与跨模态对齐策略有效提升了低空场景中多模态波束预测的鲁棒性与泛化能力，具有较强的实际部署潜力。

Abstract: The low-altitude economy (LAE) is rapidly expanding driven by urban air mobility, logistics drones, and aerial sensing, while fast and accurate beam prediction in uncrewed aerial vehicles (UAVs) communications is crucial for achieving reliable connectivity. Current research is shifting from single-signal to multi-modal collaborative approaches. However, existing multi-modal methods mostly employ fixed or empirical weights, assuming equal reliability across modalities at any given moment. Indeed, the importance of different modalities fluctuates dramatically with UAV motion scenarios, and static weighting amplifies the negative impact of degraded modalities. Furthermore, modal mismatch and weak alignment further undermine cross-scenario generalization. To this end, we propose a reliability-aware dynamic weighting scheme applied to a semantic-aware multi-modal beam prediction framework, named SaM2B. Specifically, SaM2B leverages lightweight cues such as environmental visual, flight posture, and geospatial data to adaptively allocate contributions across modalities at different time points through reliability-aware dynamic weight updates. Moreover, by utilizing cross-modal contrastive learning, we align the "multi-source representation beam semantics" associated with specific beam information to a shared semantic space, thereby enhancing discriminative power and robustness under modal noise and distribution shifts. Experiments on real-world low-altitude UAV datasets show that SaM2B achieves more satisfactory results than baseline methods.

</details>


### [43] [Tubular Riemannian Laplace Approximations for Bayesian Neural Networks](https://arxiv.org/abs/2512.24381)
*Rodrigo Pereira David*

Main category: cs.LG

TL;DR: 提出 Tubular Riemannian Laplace (TRL) 的近似，通过沿低损失峡谷的几何管模型后验，利用 Fisher/Gauss-Newton 度量分离先验不确定性与数据不确定性。


<details>
  <summary>Details</summary>
Motivation: 普通的欧几里得 Laplace 近似难以应对深度模型的强各向异性、曲率损害的损失面和对称性；需要几何/曲率感知的近似来更准确地描述后验分布。

Method: 将后验建模为沿着低损失峡谷的概率管，利用 Fisher/Gauss-Newton 度量在切向维度上捕捉先验主导的不确定性，在法向维度上捕捉数据主导的不确定性。把 TRL 视为可扩展的重新参数化高斯近似，利用隐式曲率估计在高维参数空间中工作。

Result: 在 ResNet-18 (CIFAR-10/CIFAR-100) 上的实验显示，TRL 能提供优异的校准性，在经验对比中达到或超过 Deep Ensembles 的可靠性（ECE），且训练成本只有其1/5。

Conclusion: TRL 有效弥合单模型效率与 ensemble 可靠性之间的差距，提供了一种可扩展的几何拉普拉斯近似。

Abstract: Laplace approximations are among the simplest and most practical methods for approximate Bayesian inference in neural networks, yet their Euclidean formulation struggles with the highly anisotropic, curved loss surfaces and large symmetry groups that characterize modern deep models. Recent work has proposed Riemannian and geometric Gaussian approximations to adapt to this structure. Building on these ideas, we introduce the Tubular Riemannian Laplace (TRL) approximation. TRL explicitly models the posterior as a probabilistic tube that follows a low-loss valley induced by functional symmetries, using a Fisher/Gauss-Newton metric to separate prior-dominated tangential uncertainty from data-dominated transverse uncertainty. We interpret TRL as a scalable reparametrised Gaussian approximation that utilizes implicit curvature estimates to operate in high-dimensional parameter spaces. Our empirical evaluation on ResNet-18 (CIFAR-10 and CIFAR-100) demonstrates that TRL achieves excellent calibration, matching or exceeding the reliability of Deep Ensembles (in terms of ECE) while requiring only a fraction (1/5) of the training cost. TRL effectively bridges the gap between single-model efficiency and ensemble-grade reliability.

</details>


### [44] [Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning](https://arxiv.org/abs/2512.24404)
*Soham Pahari,M. Srinivas*

Main category: cs.LG

TL;DR: 提出了一个仅使用视觉信息进行规划和定位的视觉推理框架 ViReLoc，结合地理一致的视觉规划（Geo-Consistent Visual Planning），在跨视角对齐和强化学习目标驱动下实现跨视图检索与空间推理的提升，显示出无需GPS可实现的导航与定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态/文本驱动的推理在空间任务（如视觉导航和地理定位）中受限，缺乏对几何关系的鲁棒建模，需探索仅以视觉信号进行高层次推理与定位的可能性。

Method: 提出以视觉为唯一信息源的推理范式 Geo-Consistent Visual Planning，并实现一个名为 ViReLoc 的框架，用逐步的视觉推理进行路径规划与定位；通过强化学习目标进行优化；引入对比学习与自适应特征交互以对齐跨视角视图并减小视点差异。

Result: 在多样化的导航与定位场景中，实验显示在空间推理准确性与跨视图检索性能方面有持续提升，证实视觉推理可作为导航与定位的有力补充。

Conclusion: 视觉推理能够在无需实时GPS数据的情况下完成导航与定位任务，带来更安全的导航解决方案，同时强调该范式作为传统文本或传感器驱动方法的有效补充。

Abstract: Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.

</details>


### [45] [Efficient Inference for Inverse Reinforcement Learning and Dynamic Discrete Choice Models](https://arxiv.org/abs/2512.24407)
*Lars van der Laan,Aurelien Bibaut,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出一个半参数化的去偏IRL/DDC框架，利用最大熵IRL和Gumbel噪声DDC的奖励相关函数的有效推断。通过将对数行为策略视为伪奖励，实现对策略值差异和归一化后奖励的点识别，给出目标函数、路径可微性与有效影响函数，并构建自动去偏的ML估计量，在非参数 nuisance组件下实现√n一致性、渐近正态性和半参数有效性。


<details>
  <summary>Details</summary>
Motivation: 弥合灵活机器学习方法（IRL）与经典动态离散选择（DDC）在推断上的不足：前者缺乏有效性保障，后者对参数设定过于约束且需要重复动态规划，需提供在非参数奖励下的统计有效推断框架。

Method: 提出半参数化去偏估计框架，建立对数行为策略作为伪奖励的点识别与归一化后的奖励的可识别性。将目标定义为在已知/对照软最大策略下的策略值及对正规化奖励的函数等，证明它们是行为策略与转移核的光路径可微的平滑函数，推导出高效的影响函数。基于此设计自动去偏的机器学习估计量，允许非参数 nuisance组件并实现√n一致性、渐近正态性和半参数有效性。

Result: 将可靠的统计推断扩展到对非参数化奖励的IRL和ML工具，提供统一、可计算的推断框架，适用于广义的奖励依赖目标。

Conclusion: 该框架将经典DDC推断推广至非参数化奖励并结合现代ML工具，形成一个计算友好且具有统计有效性的IRL推断方法。

Abstract: Inverse reinforcement learning (IRL) and dynamic discrete choice (DDC) models explain sequential decision-making by recovering reward functions that rationalize observed behavior. Flexible IRL methods typically rely on machine learning but provide no guarantees for valid inference, while classical DDC approaches impose restrictive parametric specifications and often require repeated dynamic programming. We develop a semiparametric framework for debiased inverse reinforcement learning that yields statistically efficient inference for a broad class of reward-dependent functionals in maximum entropy IRL and Gumbel-shock DDC models. We show that the log-behavior policy acts as a pseudo-reward that point-identifies policy value differences and, under a simple normalization, the reward itself. We then formalize these targets, including policy values under known and counterfactual softmax policies and functionals of the normalized reward, as smooth functionals of the behavior policy and transition kernel, establish pathwise differentiability, and derive their efficient influence functions. Building on this characterization, we construct automatic debiased machine-learning estimators that allow flexible nonparametric estimation of nuisance components while achieving $\sqrt{n}$-consistency, asymptotic normality, and semiparametric efficiency. Our framework extends classical inference for DDC models to nonparametric rewards and modern machine-learning tools, providing a unified and computationally tractable approach to statistical inference in IRL.

</details>


### [46] [Sparse classification with positive-confidence data in high dimensions](https://arxiv.org/abs/2512.24443)
*The Tien Mai,Mai Anh Nguyen,Trung Nghia Nguyen*

Main category: cs.LG

TL;DR: 提出了适用于高维Pconf分类的稀疏正则化框架，采用L1、SCAD、MCP罚函数，给出理论误差界与近最小极大上界，并给出基于近端梯度的高效求解算法；仿真实验显示在弱监督下的预测与变量选择性能接近全监督方法。


<details>
  <summary>Details</summary>
Motivation: 在正样本带置信度的弱监督Pconf学习中，特征维度往往远大于样本量，亟需稀疏正则化来实现预测与变量选择；现有Pconf方法在高维场景下表现欠佳，且难以实现有效的特征恢复。

Method: 提出带稀疏正则化的Pconf分类估计量，包含凸的L1罚和非凸的SCAD、MCP罚以缓解估计偏差并提高特征恢复能力；在L1情形下，建立估计和预测误差界，并在条件Restricted Strong Convexity下实现近似最小极大稀疏恢复率；为求解合成目标函数，设计基于近端梯度的高效优化算法。

Result: 理论上，L1正则的Pconf估计量具有接近最小极大值的稀疏恢复速率；在RSC条件下给出估计误差与预测误差界；算法实现稳定且高效；大量仿真实验表明，所提方法在预测能力与变量选择准确性方面达到与完全监督方法相当的水平。

Conclusion: 本工作将弱监督Pconf学习与高维统计中的稀疏正则化结合，提供理论与算法工具以实现高维Pconf的有效特征选择与预测，填补该领域的空白并具有实际应用潜力。

Abstract: High-dimensional learning problems, where the number of features exceeds the sample size, often require sparse regularization for effective prediction and variable selection. While established for fully supervised data, these techniques remain underexplored in weak-supervision settings such as Positive-Confidence (Pconf) classification. Pconf learning utilizes only positive samples equipped with confidence scores, thereby avoiding the need for negative data. However, existing Pconf methods are ill-suited for high-dimensional regimes. This paper proposes a novel sparse-penalization framework for high-dimensional Pconf classification. We introduce estimators using convex (Lasso) and non-convex (SCAD, MCP) penalties to address shrinkage bias and improve feature recovery. Theoretically, we establish estimation and prediction error bounds for the L1-regularized Pconf estimator, proving it achieves near minimax-optimal sparse recovery rates under Restricted Strong Convexity condition. To solve the resulting composite objective, we develop an efficient proximal gradient algorithm. Extensive simulations demonstrate that our proposed methods achieve predictive performance and variable selection accuracy comparable to fully supervised approaches, effectively bridging the gap between weak supervision and high-dimensional statistics.

</details>


### [47] [Generative forecasting with joint probability models](https://arxiv.org/abs/2512.24446)
*Patrick Wyrod,Ashesh Chattopadhyay,Daniele Venturi*

Main category: cs.LG

TL;DR: 提出将时间序列预测视为联合生成问题，通过学习滞后状态的联合分布进行多步预测并边际化得到单步输出，并给出一个模型无关的训练/推理框架用于不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 解决混沌系统中对初始条件的强敏感性以及存在的多尺度过程，使得确定性/单步预测能力有限；通过学习联合分布来捕捉非线性时序依赖并生成更丰富的轨迹分布，同时保持吸引子几何。

Method: 将短时间窗口内的滞后状态联合分布建模，通过边际化获得多步预测；提出通用、模型无关的联合生成预测训练与推理框架，并利用 ensemble 方差、短期自相关、累计 Wasserstein 漂移等不需要 ground truth 的指标来评估预测鲁棒性与可靠性。

Result: 在 Lorenz-63 与 Kuramoto–Sivashinsky 等经典混沌系统上，联合生成模型在短期预测、保持吸引子几何以及长期统计行为方面显著优于传统的条件单步预测模型。

Conclusion: 联合生成框架可实现更好的短期预测技能、更好地保持动力学结构并提升对长程统计行为的准确性，同时提供了无需 ground truth 的不确定性评估手段。

Abstract: Chaotic dynamical systems exhibit strong sensitivity to initial conditions and often contain unresolved multiscale processes, making deterministic forecasting fundamentally limited. Generative models offer an appealing alternative by learning distributions over plausible system evolutions; yet, most existing approaches focus on next-step conditional prediction rather than the structure of the underlying dynamics. In this work, we reframe forecasting as a fully generative problem by learning the joint probability distribution of lagged system states over short temporal windows and obtaining forecasts through marginalization. This new perspective allows the model to capture nonlinear temporal dependencies, represent multistep trajectory segments, and produce next-step predictions consistent with the learned joint distribution. We also introduce a general, model-agnostic training and inference framework for joint generative forecasting and show how it enables assessment of forecast robustness and reliability using three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift), without access to ground truth. We evaluate the performance of the proposed method on two canonical chaotic dynamical systems, the Lorenz-63 system and the Kuramoto-Sivashinsky equation, and show that joint generative models yield improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behaviour than conventional conditional next-step models.

</details>


### [48] [HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors](https://arxiv.org/abs/2512.24478)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 提出 HOLOGRAPH 框架，将基于大语言模型（LLM）的先验因果知识引入因果发现，并用层凝聚（sheaf）理论形式化局部信念与全局因果结构的关系；全局截面对应一致的全局因果结构，层上同调揭示拓扑障碍；提出 Algebraic Latent Projection 以处理隐藏混淆，及信念流形的自然梯度优化；在合成与真实数据的实验中表现与现有方法相当，且揭示 Locality 公理在大图中失效的非局部耦合现象；代码公开。


<details>
  <summary>Details</summary>
Motivation: 受 identifiability 限制所驱动的观测数据因果发现依然困难。希望通过将 LLM 的先验知识整合到因果推断中，并给出严谨的数学框架，使得局部信念能够在全局层面一致地整合；同时处理隐藏混淆与高维变量情形。

Method: 将局部因果信念表示为变量子集上的 presheaf 的截面，使用层理论来刻画全局一致性：若存在全局截面，则形成一致的全局因果结构；拓扑阻塞对应层上同调的不为零。引入 Algebraic Latent Projection 处理隐藏变量，利用信念流形上的自然梯度实现优化。通过合成和真实数据的实验验证理论正确性与实用性。

Result: 在合成与真实基准数据上实现竞争性因果发现性能，适用于 50-100 个变量的情形。数值上满足 Identity、Transitivity、Gluting 等公理，但 Locality 公理在较大图中失败，揭示潜变量投影存在非局部耦合。提供了代码：https://github.com/hyunjun1121/holograph。

Conclusion: 为基于 LLM 的因果发现提供严格的数学基础，利用层理论揭示全局一致性与拓扑阻碍之间的关系，同时揭示潜变量投影中的非局部耦合现象。该工作将 LLM 作为先验知识源的因果发现带入更严谨的理论框架，并展示在中等规模变量集上的竞争力。

Abstract: Causal discovery from observational data remains fundamentally limited by identifiability constraints. Recent work has explored leveraging Large Language Models (LLMs) as sources of prior causal knowledge, but existing approaches rely on heuristic integration that lacks theoretical grounding. We introduce HOLOGRAPH, a framework that formalizes LLM-guided causal discovery through sheaf theory--representing local causal beliefs as sections of a presheaf over variable subsets. Our key insight is that coherent global causal structure corresponds to the existence of a global section, while topological obstructions manifest as non-vanishing sheaf cohomology. We propose the Algebraic Latent Projection to handle hidden confounders and Natural Gradient Descent on the belief manifold for principled optimization. Experiments on synthetic and real-world benchmarks demonstrate that HOLOGRAPH provides rigorous mathematical foundations while achieving competitive performance on causal discovery tasks with 50-100 variables. Our sheaf-theoretic analysis reveals that while Identity, Transitivity, and Gluing axioms are satisfied to numerical precision (<10^{-6}), the Locality axiom fails for larger graphs, suggesting fundamental non-local coupling in latent variable projections. Code is available at [https://github.com/hyunjun1121/holograph](https://github.com/hyunjun1121/holograph).

</details>


### [49] [More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization](https://arxiv.org/abs/2512.24545)
*Yuma Ichikawa,Yoshihiko Fujisawa,Yudai Fujimoto,Akira Sakai,Katsuki Fujisawa*

Main category: cs.LG

TL;DR: MDBF 在 DBF 的基础上引入秩级的多-envelope 架构，保持一个共享的 1 位符号基底并用秩 l 的 envelope 提升数量级表达能力，从而在极低位宽量化下提升 LLM 的性能。


<details>
  <summary>Details</summary>
Motivation: 极端低位宽量化中的 DBF 的缩放参数过于受限；在符号被消除后，所有秩分量具有相同的幅度谱，导致性能饱和，需在二元载体预算内提升幅度表达能力。

Method: 提出 Multi-envelope DBF (MDBF)：保留一对共享的 1 位符号基底，但将单一 envelope 替换为秩为 l 的 envelope，并在 envelope 分量之间共享符号矩阵以维持二元载体并提升幅度表达；给出闭式初始化和交替优化算法进行 MDBF 的训练。

Result: 在 LLaMA 与 Qwen 系列模型上，MDBF 相对于在匹配的比特/权重下的先前二进制格式，改进了困惑度（perplexity）和零样本（zero-shot）准确率，且保持了同样的易部署推理原语。

Conclusion: MDBF 在保持二进制载体与低开销推理的前提下，提升了幅度表达能力并提升了极低比特量化下的模型性能，具备对多模型家族的可迁移性。

Abstract: For extreme low-bit quantization of large language models (LLMs), Double Binary Factorization (DBF) is attractive as it enables efficient inference without sacrificing accuracy. However, the scaling parameters of DBF are too restrictive; after factoring out signs, all rank components share the same magnitude profile, resulting in performance saturation. We propose Multi-envelope DBF (MDBF), which retains a shared pair of 1-bit sign bases but replaces the single envelope with a rank-$l$ envelope. By sharing sign matrices among envelope components, MDBF effectively maintains a binary carrier and utilizes the limited memory budget for magnitude expressiveness. We also introduce a closed-form initialization and an alternating refinement method to optimize MDBF. Across the LLaMA and Qwen families, MDBF enhances perplexity and zero-shot accuracy over previous binary formats at matched bits per weight while preserving the same deployment-friendly inference primitive.

</details>


### [50] [Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space](https://arxiv.org/abs/2512.24617)
*Xingwei Qu,Shaowen Wang,Zihao Huang,Kai Hua,Fan Yin,Rui-Jie Zhu,Jundong Zhou,Qiyang Min,Zihao Wang,Yizhi Li,Tianyu Zhang,He Xing,Zheng Zhang,Yuxuan Song,Tianyu Zheng,Zhiyuan Zeng,Chenghua Lin,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: 提出 Dynamic Large Concept Models (DLCM)，通过学习语义边界在层级压缩的概念空间中进行推理，将计算从 token 层转移到压缩的概念表示，从而在固定 FLOPs 下实现更高效推理。首次给出 compression-aware scaling law 与 decoupled μP 参数化。实验证明，在 R=4 的设置下，约三分之一的推理计算被转移到高容量推理骨架，12 项零-shot 基准上实现 +2.69% 的平均提升。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对所有 token 采用统一计算，忽略语言信息密度的非均匀性，导致在局部可预测的片段上算力浪费，同时对关键语义转变的推理能力不足。

Method: 提出 DLCM 这一层级语言建模框架：学习 latent 表征中的语义边界，通过压缩到概念空间实现跨 token 的高效推理；无需预定义语言单位即可发现可变长度的概念。提出 compression-aware scaling law，区分 token-level 容量、概念级推理容量和压缩比，并实现固定 FLOPs 的可 principled compute allocation。为稳定训练，提出 decoupled μP 参数化以实现跨宽度及压缩 regime 的零-shot超参迁移。

Result: 在实际设置 R=4（平均每概念4个 token）下，DLCM 将约三分之一的推理计算转移至更高容量的推理骨架，在匹配推理 FLOPs 的条件下获得 +2.69% 的平均提升，覆盖 12 项零-shot 基准。

Conclusion: 层级压缩显著改变标定扩展性，compression-aware scaling law 与 decoupled μP 提供了在固定 FLOPs 下更高效且可迁移的参数化框架，提升了多任务零-shot 的表现。

Abstract: Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.

</details>


### [51] [Nested Learning: The Illusion of Deep Learning Architectures](https://arxiv.org/abs/2512.24695)
*Ali Behrouz,Meisam Razaviyayn,Peilin Zhong,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 提出嵌套学习（NL）范式，使用多级/并行优化问题来实现自我修改、连续学习与长上下文推理，并以 Expressive Optimizers、自我修改学习模块、连续记忆系统构建 Hope 模块，初步应用于语言建模与少样本泛化。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在持续学习、记忆与自我改进方面存在根本性挑战，缺乏一个可表达多层次优化与上下文流的统一框架，因此需要一个嵌套、可扩展的学习范式来推动自我提升与持续适应。

Method: 提出嵌套学习（NL）框架，将模型表示为一组嵌套、多层次或并行的优化问题，各自拥有独立的上下文流。基于此，提出：1) Expressive Optimizers，认为常见梯度优化器本质是压缩梯度信息的关联记忆模块，并给出更具记忆性和学习规则更强的优化器；2) Self-Modifying Learning Module，一个序列模型学习自我更新算法；3) Continuum Memory System，给出一种扩展传统记忆模型的连续记忆系统。结合上述，构建 Continual Learning 模块 Hope，并在语言建模、知识整合、少样本泛化、持续学习与长上下文推理等任务上给出初步结果。

Result: 提出了一个理论与方法的组合框架，给出初步结果表明 Hope 在语言建模、知识整合、少样本泛化、持续学习以及长上下文推理方面具备潜在优势，显示 NL 框架的可行性与潜力。

Conclusion: NL 提供一种更具表达力与层级性的学习哲学，鼓励设计具有更多层级与上下文流的优化与记忆机制，可能显著提升自我改进、持续学习与长上下文推理任务的性能。

Abstract: Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.

</details>


### [52] [Causal Discovery with Mixed Latent Confounding via Precision Decomposition](https://arxiv.org/abs/2512.24696)
*Amir Asiaee,Samhita Pal,James O'quinn,James P. Long*

Main category: cs.LG

TL;DR: 提出 DCL-DECOR，在混合潜在混杂的线性高斯系统中，通过将观测精度矩阵分解为结构成分与低秩成分来去混，再用相关噪声 DAG 学习恢复有向边，并通过 bow-freeness 简化与整合。实验显示在普遍性混杂存在时，相比直接在混杂数据上学习，能更准确地识别因果边。


<details>
  <summary>Details</summary>
Motivation: 在观测数据中的线性高斯系统受混合型潜在混杂影响时，现有的可解释性不强。全局潜在因子可能被错误解读为因果边，局部/无向结构模型则难以恢复方向信息。需要一个能分离普遍性混杂与局部因果结构的模块化方法以提高因果发现的可靠性。

Method: 提出 DCL-DECOR 流程：1) 将观测数据的精度矩阵分解为结构成分和低秩成分，以去除普遍性混杂并保留仅由因果图及局部混杂产生的局部依赖；2) 对去混后的表示应用相关噪声 DAG 学习器以恢复有向边，并对剩余结构化误差相关进行建模；3) 通过一个简单的整合步骤强制 bow-freeness 以获得一致解。并给出关于可识别性的定理与模块化保证，问题转化为若干子问题。

Result: 在合成数据上，随着普遍性混杂强度与维度的变化，DCL-DECOR 相比直接在混杂数据上应用相关噪声 DAG 学习，能够更稳定地恢复有向边，表现出一致的改进。

Conclusion: 方法提供了在混合潜在混杂下进行因果发现的可行框架，给出可识别性结果与模块化分解的理论基础，并通过合成实验验证了相较于直接方法的优越性。

Abstract: We study causal discovery from observational data in linear Gaussian systems affected by \emph{mixed latent confounding}, where some unobserved factors act broadly across many variables while others influence only small subsets. This setting is common in practice and poses a challenge for existing methods: differentiable and score-based DAG learners can misinterpret global latent effects as causal edges, while latent-variable graphical models recover only undirected structure.
  We propose \textsc{DCL-DECOR}, a modular, precision-led pipeline that separates these roles. The method first isolates pervasive latent effects by decomposing the observed precision matrix into a structured component and a low-rank component. The structured component corresponds to the conditional distribution after accounting for pervasive confounders and retains only local dependence induced by the causal graph and localized confounding. A correlated-noise DAG learner is then applied to this deconfounded representation to recover directed edges while modeling remaining structured error correlations, followed by a simple reconciliation step to enforce bow-freeness.
  We provide identifiability results that characterize the recoverable causal target under mixed confounding and show how the overall problem reduces to well-studied subproblems with modular guarantees. Synthetic experiments that vary the strength and dimensionality of pervasive confounding demonstrate consistent improvements in directed edge recovery over applying correlated-noise DAG learning directly to the confounded data.

</details>


### [53] [FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference](https://arxiv.org/abs/2512.24713)
*Fen-Yu Hsieh,Yun-Chang Teng,Ding-Yong Hong,Jan-Jan Wu*

Main category: cs.LG

TL;DR: 提出一种结合N:M结构化裁剪和4位量化的自动化框架，并在CPU、GPU和FPGA上实现硬件-软件协同的LLM推理加速，利用2:4稀疏+量化实现显著的存储与计算加速，在LLaMA-7B等模型上显示吞吐量提升和端到端延迟下降。


<details>
  <summary>Details</summary>
Motivation: 降低大型语言模型在资源受限环境中的计算与内存消耗与部署难度，提高可部署性与性价比。

Method: 提出一个统一的推理流水线，先进行N:M结构化裁剪（以2:4为典型比例）和4位量化，然后进行去量化与高效矩阵乘法实现；支持Dense、2:4稀疏张量核，并通过硬件-软件协同的FPGA加速器实现；在多平台（CPU、NVIDIA GPU的Dense与2:4 Sparse Tensor Cores、自定义FPGA）上部署。

Result: 在4096×4096矩阵上实现最多4×的权重存储压缩，矩阵乘法加速约1.71×，端到端延迟相比密集GPU基线下降约1.29×；对LLaMA-7B进行放大分析，结构化稀疏提升吞吐量/每个token 1.36×。显示N:M稀疏与量化的协同优势，并且FPGA加速器具备对更广稀疏模式的灵活支持。

Conclusion: N:M稀疏与量化的协同可以显著提升LLM推理在多平台上的可部署性与效率，FPGA加速器提供灵活的架构路径来支持超出固定2:4约束的更多稀疏模式。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of language processing tasks. However, this success comes at the cost of substantial computation and memory requirements, which significantly impedes their deployment in resource-constrained environments. To address this challenge, this work introduces an automation framework that leverages weight pruning and low-bit quantization, and presents a hardware-software co-design method that generates accelerators on the Field-Programmable Gate Array (FPGA) platform. In particular, we implement a unified pipeline that applies N:M structured pruning and 4-bit integer quantization to reduce the memory footprint, followed by optimized dequantization and matrix multiplication to enhance LLM inference on several hardware platforms, including CPUs, NVIDIA GPUs with Dense and 2:4 Sparse Tensor Cores, and a custom systolic-array-based FPGA accelerator. Utilizing 2:4 sparsity combined with quantization on $4096 \times 4096$ matrices, our approach achieves a reduction of up to $4\times$ in weight storage and a $1.71\times$ speedup in matrix multiplication, yielding a $1.29\times$ end-to-end latency reduction compared to dense GPU baselines. Scaling analysis on the LLaMA-7B model further shows that structured sparsity enhances the throughput per token by $1.36\times$. These results demonstrate the synergy of fine-grained N:M sparsity and quantization for enabling efficient and deployable LLM inference, while the proposed FPGA accelerator offers a flexible architectural path for supporting a broader class of sparsity patterns beyond the fixed 2:4 hardware constraints.

</details>


### [54] [From Trial to Deployment: A SEM Analysis of Traveler Adoptions to Fully Operational Autonomous Taxis](https://arxiv.org/abs/2512.24767)
*Yutong Cai,Hua Wang*

Main category: cs.LG

TL;DR: 在中国武汉基于真实运营的自动驾驶出租车调查中，使用结构方程模型发现六个潜在心理构念对采用行为的影响，成本敏感性和行为意向最强；模型拟合优良，结果可用于定价、政策与公共宣传。


<details>
  <summary>Details</summary>
Motivation: 弥补文献对实际用户行为研究的不足：以实际运营的自动驾驶出租车服务为背景，利用真实服务属性设计问卷，提供面向政策与市场设计的证据。

Method: 通过对百度阿波罗Robotaxi在武汉的实际用户进行问卷调查（有效样本336份），设计包含真实服务属性的调查；使用结构方程模型（SEM）识别并估计六个潜在心理构念及其对在十个情景下的选择频次（采用行为）之影响。

Result: 识别出六个潜在构念：信任与政策支持、成本敏感性、性能、行为意向、生活方式、教育；成本敏感性与行为意向是采用的最强正向预测变量；其他构念的作用较为细致；模型在多项拟合指标上表现良好。

Conclusion: 提供经验证据以支持在真实城市环境中扩展自动驾驶出租车的 policymaking、定价设计与公共推广策略。

Abstract: Autonomous taxi services represent a transformative advancement in urban mobility, offering safety, efficiency, and round-the-clock operations. While existing literature has explored user acceptance of autonomous taxis through stated preference experiments and hypothetical scenarios, few studies have investigated actual user behavior based on operational AV services. This study addresses that gap by leveraging survey data from Wuhan, China, where Baidu's Apollo Robotaxi service operates at scale. We design a realistic survey incorporating actual service attributes and collect 336 valid responses from actual users. Using Structural Equation Modeling, we identify six latent psychological constructs, namely Trust \& Policy Support, Cost Sensitivity, Performance, Behavioral Intention, Lifestyle, and Education. Their influences on adoption behavior, measured by the selection frequency of autonomous taxis in ten scenarios, are examined and interpreted. Results show that Cost Sensitivity and Behavioral Intention are the strongest positive predictors of adoption, while other latent constructs play more nuanced roles. The model demonstrates strong goodness-of-fit across multiple indices. Our findings offer empirical evidence to support policymaking, fare design, and public outreach strategies for scaling autonomous taxis deployments in real-world urban settings.

</details>


### [55] [Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks](https://arxiv.org/abs/2512.24793)
*Shota Suzuki,Satoshi Ono*

Main category: cs.LG

TL;DR: Self-supervised NAS for multimodal DNNs enabling architecture search and pretraining using unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on labeled data for multimodal NAS and manage architectural complexity.

Method: Apply SSL to both architecture search and model pretraining; design a search space and SSL objectives that leverage multiple modalities.

Result: Demonstrates that architectures for DNNs can be designed from unlabeled data; effectiveness across modalities implied.

Conclusion: SSL-assisted NAS is a viable path to multimodal architecture search without labeled data; potential for broader applicability.

Abstract: Neural architecture search (NAS), which automates the architectural design process of deep neural networks (DNN), has attracted increasing attention. Multimodal DNNs that necessitate feature fusion from multiple modalities benefit from NAS due to their structural complexity; however, constructing an architecture for multimodal DNNs through NAS requires a substantial amount of labeled training data. Thus, this paper proposes a self-supervised learning (SSL) method for architecture search of multimodal DNNs. The proposed method applies SSL comprehensively for both the architecture search and model pretraining processes. Experimental results demonstrated that the proposed method successfully designed architectures for DNNs from unlabeled training data.

</details>


### [56] [DTI-GP: Bayesian operations for drug-target interactions using deep kernel Gaussian processes](https://arxiv.org/abs/2512.24810)
*Bence Bolgár,András Millinghoffer,Péter Antal*

Main category: cs.LG

TL;DR: Proposes DTI-GP: a deep kernel learning Gaussian process with neural embeddings for compounds and targets, enabling Bayesian classification with rejection, top-K selection, and ranking via sampling from the predictive distribution; achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Precise probabilistic information for DTI predictions is critical to understand limitations and boost predictive performance; Gaussian processes provide a scalable Bayesian framework to integrate advanced DTI representations and enable Bayesian decision operations.

Method: A deep kernel learning-based GP architecture (DTI-GP) that combines a neural embedding module for chemical compounds and protein targets with a GP module. The workflow samples from the predictive distribution to estimate a Bayesian precedence matrix, which is used to perform fast and accurate selection and ranking operations.

Result: DTI-GP outperforms state-of-the-art solutions and enables (1) construction of a Bayesian accuracy-confidence enrichment score, (2) rejection schemes for improved enrichment, and (3) estimation and search for top-K selections and ranking with high expected utility.

Conclusion: The proposed DTI-GP framework provides principled probabilistic predictions for DTIs and supports uncertainty-aware decision-making in classification, selection, and ranking tasks.

Abstract: Precise probabilistic information about drug-target interaction (DTI) predictions is vital for understanding limitations and boosting predictive performance. Gaussian processes (GP) offer a scalable framework to integrate state-of-the-art DTI representations and Bayesian inference, enabling novel operations, such as Bayesian classification with rejection, top-$K$ selection, and ranking. We propose a deep kernel learning-based GP architecture (DTI-GP), which incorporates a combined neural embedding module for chemical compounds and protein targets, and a GP module. The workflow continues with sampling from the predictive distribution to estimate a Bayesian precedence matrix, which is used in fast and accurate selection and ranking operations. DTI-GP outperforms state-of-the-art solutions, and it allows (1) the construction of a Bayesian accuracy-confidence enrichment score, (2) rejection schemes for improved enrichment, and (3) estimation and search for top-$K$ selections and ranking with high expected utility.

</details>


### [57] [Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback](https://arxiv.org/abs/2512.24818)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文给出 NLHF 框架中 Optimistic Multiplicative Weights Update (OMWU) 的第一性收敛性证明。若存在全支撑的纳什均衡，OMWU 在烧入期后对原始对偶间隙实现最后迭代线性收敛；无需假设 NE 为唯一性；并揭示边际收敛行为（稀有动作概率指数级增长）的新特征，同时在表格与神经策略等实验中验证其在大语言模型应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在对齐大语言模型时，传统一对偏好建模（如 Bradley–Terry）依赖的可传递性假设往往无法真实反映人类偏好中的非传递性。NLHF 将对齐问题建模为两人零和博弈，目标是找到 Nash 均衡（NE）；然而现有算法常需正则化以求稳定，导致在计算对偶间隙时引入偏差。需要在不引入额外正则化偏差的前提下，给出原始博弈的收敛性理论保证。

Method: 提出并分析对 NLHF 的 Optimistic Multiplicative Weights Update（OMWU）算法的收敛性：在存在全支撑的 NE 时，证明 OMWU 能在烧入期后实现对原始博弈的最后迭代线性收敛，收敛率与原始 NE 的对偶间隙相关；且不需要 NE 的唯一性假设。通过分析揭示一个边际收敛的新现象：稀有动作的概率从极小值出发呈指数级增长，从而在实例相关常数上获得优于以往结果的依赖。

Result: 给出对原始 NLHF 博弈的无偏收敛性证明：OMWU 在存在全支撑的 NE 条件下，实现最后一轮的线性收敛（烧入期后），且提供实例依赖的收敛率；相比 Wei et al. (2020) 的结果，无需 NE 的唯一性假设；理论还揭示稀有行动的概率呈指数级增长的边际收敛行为。实验结果在表格和神经策略类任务中支持理论，显示该方法对 LLM 应用的潜力。

Conclusion: 该工作首次在 NLHF 框架下给出 OMWU 的收敛性保障，且无需 NE 唯一性假设，结合对偶间隙的实例依赖收敛率分析，提升了对非传递偏好建模的理论与实践信心，具有对 LLM 偏好对齐的潜在应用价值。

Abstract: Aligning large language models (LLMs) with human preferences has proven effective for enhancing model capabilities, yet standard preference modeling using the Bradley-Terry model assumes transitivity, overlooking the inherent complexity of human population preferences. Nash learning from human feedback (NLHF) addresses this by framing non-transitive preferences as a two-player zero-sum game, where alignment reduces to finding the Nash equilibrium (NE). However, existing algorithms typically rely on regularization, incurring unavoidable bias when computing the duality gap in the original game. In this work, we provide the first convergence guarantee for Optimistic Multiplicative Weights Update ($\mathtt{OMWU}$) in NLHF, showing that it achieves last-iterate linear convergence after a burn-in phase whenever an NE with full support exists, with an instance-dependent linear convergence rate to the original NE, measured by duality gaps. Compared to prior results in Wei et al. (2020), we do not require the assumption of NE uniqueness. Our analysis identifies a novel marginal convergence behavior, where the probability of rarely played actions grows exponentially from exponentially small values, enabling exponentially better dependence on instance-dependent constants than prior results. Experiments corroborate the theoretical strengths of $\mathtt{OMWU}$ in both tabular and neural policy classes, demonstrating its potential for LLM applications.

</details>


### [58] [AODDiff: Probabilistic Reconstruction of Aerosol Optical Depth via Diffusion-based Bayesian Inference](https://arxiv.org/abs/2512.24847)
*Linhao Fan,Hongqiang Fang,Jingyang Dai,Yong Jiang,Qixing Zhang*

Main category: cs.LG

TL;DR: 提出 AODDiff，一种基于扩散的贝叶斯推断的 AOD 场重建框架，利用学习得到的时空先验，在不完备数据条件下进行下采样与修复，并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据稀缺与未包含不确定性量化的问题，提升大气监测中 AOD 场的高质量重建，同时具备对多种任务的灵活适配能力而无需专门重训练。

Method: 通过腐蚀感知的训练策略仅用不完整数据学习时空 AOD 先验；采用解耦annealing后验采样策略，将异质观测作为约束融入生成过程；在重分析数据上进行大规模评估，覆盖下采样与修复两类重建任务。

Result: 在下采样与修复任务上显示出良好性能与鲁棒性，能保持高的空间频谱保真，同时提供多样本采样带来的不确定性量化。

Conclusion: AODDiff 为 AOD 场提供一个灵活的概率重建框架，降低对完整数据的依赖，且可在不重训的情况下适配多种任务，并通过多样本采样实现不确定性评估，已在再分析数据上得到验证。

Abstract: High-quality reconstruction of Aerosol Optical Depth (AOD) fields is critical for Atmosphere monitoring, yet current models remain constrained by the scarcity of complete training data and a lack of uncertainty quantification.To address these limitations, we propose AODDiff, a probabilistic reconstruction framework based on diffusion-based Bayesian inference. By leveraging the learned spatiotemporal probability distribution of the AOD field as a generative prior, this framework can be flexibly adapted to various reconstruction tasks without requiring task-specific retraining. We first introduce a corruption-aware training strategy to learns a spatiotemporal AOD prior solely from naturally incomplete data. Subsequently, we employ a decoupled annealing posterior sampling strategy that enables the more effective and integration of heterogeneous observations as constraints to guide the generation process. We validate the proposed framework through extensive experiments on Reanalysis data. Results across downscaling and inpainting tasks confirm the efficacy and robustness of AODDiff, specifically demonstrating its advantage in maintaining high spatial spectral fidelity. Furthermore, as a generative model, AODDiff inherently enables uncertainty quantification via multiple sampling, offering critical confidence metrics for downstream applications.

</details>


### [59] [Characterization of Transfer Using Multi-task Learning Curves](https://arxiv.org/abs/2512.24866)
*András Millinghoffer,Bence Bolgár,Péter Antal*

Main category: cs.LG

TL;DR: 通过在不同样本规模下建模多任务学习曲线来定量描述迁移效应，认为通过增加数据集样本而非仅仅通过梯度更新更能揭示迁移的本质；提出一种高效的近似多任务学习曲线的方法并与统计与计算方法比较，结果显示学习曲线更能捕捉多任务迁移及其在基础模型中的对任务间及上下文的迁移。


<details>
  <summary>Details</summary>
Motivation: 揭示迁移效应的本质不仅在训练迭代（梯度更新），更在于数据规模的变化；用多任务学习曲线来量化Inductive Transfer，提供对迁移现象的更普遍且可比较的表征。

Method: 提出用多任务学习曲线来近似Inductive性能随样本规模变化的定量模型；设计一种高效的近似方法，类似Task Affinity Grouping在训练中的应用；比较统计与计算两类迁移分析方法；在药物-靶标相互作用数据集上进行评估。

Result: 学习曲线能更好地捕捉多任务学习的迁移效应及其多任务扩展，能够区分对任务之间的迁移（pairwise）和上下文相关的迁移（contextual）；相较于以往方法，统计方法成本较低但功效有限，原有的计算密集方法虽然功效更强但成本高、适用性更广。

Conclusion: 利用学习曲线的方法更有效地刻画多任务迁移效应及其在基础模型中的对任务-上下文的迁移关系，提出的高效近似方法提供了实际可行的分析路径。

Abstract: Transfer effects manifest themselves both during training using a fixed data set and in inductive inference using accumulating data. We hypothesize that perturbing the data set by including more samples, instead of perturbing the model by gradient updates, provides a complementary and more fundamental characterization of transfer effects. To capture this phenomenon, we quantitatively model transfer effects using multi-task learning curves approximating the inductive performance over varying sample sizes. We describe an efficient method to approximate multi-task learning curves analogous to the Task Affinity Grouping method applied during training. We compare the statistical and computational approaches to transfer, which indicates considerably higher compute costs for the previous but better power and broader applicability. Evaluations are performed using a benchmark drug-target interaction data set. Our results show that learning curves can better capture the effects of multi-task learning and their multi-task extensions can delineate pairwise and contextual transfer effects in foundation models.

</details>


### [60] [Attribution-Guided Distillation of Matryoshka Sparse Autoencoders](https://arxiv.org/abs/2512.24975)
*Cristina P. Martin-Linares,Jonathan P. Ling*

Main category: cs.LG

TL;DR: 通过迭代蒸馏得到一个稳定的核心特征集，并跨训练循环迁移核心编码器权重，以训练新的 SAEs，从而提升 SAEBench 指标并证明跨稀疏水平的可重复特征迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏自编码器在训练中容易产生冗余且跨运行/稀疏度不同的特征，难以解释和迁移，需要得到一个稳定且可重用的核心特征集。

Method: 提出 Distilled Matryoshka Sparse Autoencoders (DMSAEs)。进行迭代蒸馏循环：先训练带有共享核心的 Matryoshka SAE；用梯度 X activation 衡量每个特征对下一最嵌套重构的损失贡献；选取能够解释固定分数 attribution 的最小子集；仅将核心编码器权重在循环间迁移，核心解码器和非核心潜变量重新初始化。应用于 Gemma-2-2B 第12层残差流激活，进行七个蒸馏循环（5亿个 token，宽度 65k），得到重复被选中的核心特征 197 个。

Result: 得到的 distilled core 为 197 个特征，重复被选中；使用该核心训练的 SAEs 在多项 SAEBench 指标上提升；证明了跨稀疏水平转移的稳定特征集的可行性。

Conclusion: 核心编码器权重的稳定性和跨循环迁移证实了一个小而稳定的特征子集在 SAEs 的解释性与可迁移性方面的重要作用；DMSAEs 提供了一种提升可复用性和效率的训练流程。

Abstract: Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: train a Matryoshka SAE with a shared core, use gradient X activation to measure each feature's contribution to next-token loss in the most nested reconstruction, and keep only the smallest subset that explains a fixed fraction of the attribution. Only the core encoder weight vectors are transferred across cycles; the core decoder and all non-core latents are reinitialized each time. On Gemma-2-2B layer 12 residual stream activations, seven cycles of distillation (500M tokens, 65k width) yielded a distilled core of 197 features that were repeatedly selected. Training using this distilled core improves several SAEBench metrics and demonstrates that consistent sets of latent features can be transferred across sparsity levels

</details>


### [61] [Efficiently Estimating Data Efficiency for Language Model Fine-tuning](https://arxiv.org/abs/2512.24991)
*Gyung Hyun Je,Colin Raffel*

Main category: cs.LG

TL;DR: 提出一个数据效率度量，并用低置信样本的梯度余弦相似度来预测数据效率，从而在有限标注下估计所需注释量，降低注释成本；在30个任务上实现约8.6%预测误差，代码公开。


<details>
  <summary>Details</summary>
Motivation: 需要在不逐步标注的情况下预测任务数据效率，以减少注释-再训练循环的成本。

Method: 定义数据效率度量，利用低置信样本的梯度余弦相似度作为预测信号，在少量标注样本下预测整体数据效率。

Result: 在多任务数据集中实现约8.6%的总预测误差，通常能消除数百个不必要的标注工作。

Conclusion: 该方法简单且可推广，且实现代码在GitHub公开。

Abstract: While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task's data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task's data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.

</details>


### [62] [Diffusion Language Models are Provably Optimal Parallel Samplers](https://arxiv.org/abs/2512.25014)
*Haozhe Jiang,Nika Haghtalab,Lijie Chen*

Main category: cs.LG

TL;DR: DLMs with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm with the optimal number of sequential steps; enabling remasking or revision further achieves optimal space complexity and yields an expressivity advantage over non-revision models.


<details>
  <summary>Details</summary>
Motivation: Address the computational bottlenecks of autoregressive generation by providing a rigorous, formal foundation for parallel sampling with diffusion language models; quantify how CoT and token-editing operations impact efficiency and expressivity.

Method: Develop a formal model of parallel sampling for diffusion language models, prove that DLMs with polynomial-length CoT can replicate arbitrary parallel samplers using the minimal sequential steps; extend with remasking/revision to optimize space; establish an expressivity gap showing revisions outperform non-revision DLMs.

Result: Theoretical guarantees: (1) DLMs with CoT achieve optimal sequential-step efficiency for distributions that admit such representations; (2) Without token-editing, intermediate footprint can be large; (3) Adding remasking or revision with CoT enables simulation of any parallel sampler with optimal space; (4) Revision strictly increases expressivity compared to non-revision variants.

Conclusion: Diffusion language models, especially with revision or remasking, can be the most efficient parallel samplers among language models, and revision should be enabled to maximize expressivity and efficiency.

Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. We prove that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. We further justify the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Our results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.

</details>


### [63] [ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning](https://arxiv.org/abs/2512.25023)
*Timo Kaufmann,Yannick Metz,Daniel Keim,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.

</details>


### [64] [Generative Classifiers Avoid Shortcut Solutions](https://arxiv.org/abs/2512.25034)
*Alexander C. Li,Ananya Kumar,Deepak Pathak*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.

</details>


### [65] [On the geometry and topology of representations: the manifolds of modular addition](https://arxiv.org/abs/2512.25060)
*Gabriela Moisescu-Pareja,Gavin McCracken,Harley Wiltzer,Vincent Létourneau,Colin Daniels,Doina Precup,Jonathan Love*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Clock and Pizza interpretations, associated with architectures differing in either uniform or learnable attention, were introduced to argue that different architectural designs can yield distinct circuits for modular addition. In this work, we show that this is not the case, and that both uniform attention and trainable attention architectures implement the same algorithm via topologically and geometrically equivalent representations. Our methodology goes beyond the interpretation of individual neurons and weights. Instead, we identify all of the neurons corresponding to each learned representation and then study the collective group of neurons as one entity. This method reveals that each learned representation is a manifold that we can study utilizing tools from topology. Based on this insight, we can statistically analyze the learned representations across hundreds of circuits to demonstrate the similarity between learned modular addition circuits that arise naturally from common deep learning paradigms.

</details>


### [66] [Scaling Open-Ended Reasoning to Predict the Future](https://arxiv.org/abs/2512.25070)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.LG

TL;DR: 通过自动化从新闻中合成问题，训练用于高风险前瞻预测的语言模型。OpenForecaster 8B在校准、准确性与一致性方面优于多数同类模型，且与更大模型接近，并对外开放源代码和数据。


<details>
  <summary>Details</summary>
Motivation: 高风险决策需在不确定性条件下进行预测，需可扩展的数据集、良好校准与可解释性。通过离线新闻数据和检索增强来避免未来信息泄漏，并提升学习样本的多样性与现实性。

Method: 从全球新闻中自动合成前瞻性问题，构建数据集OpenForesight；使用离线新闻语料进行数据生成与检索；在Qwen3思维模型上训练，应用检索增强和改进的强化学习奖励函数；在2025年5–8月进行 held-out 测试，最终开放源代码、模型和数据。

Result: OpenForecaster 8B达到与更大专有模型相当的性能；训练提升了准确性、校准和预测一致性；前瞻性训练的校准改进具有跨基准的泛化性。

Conclusion: 证明了前瞻性训练结合检索与RL能提升语言模型在不确定情境下的预测能力，并以开源资源推动相关研究的发展。

Abstract: High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [67] [Hierarchical Quasi-cyclic Codes from Reed-Solomon and Polynomial Evaluation Codes](https://arxiv.org/abs/2512.23872)
*Emily McMillon,Kathryn Haymaker*

Main category: cs.IT

TL;DR: First algebraic construction of hierarchical quasi-cyclic codes from Reed-Solomon codes, with levels and indices tied to field size; bounds on rank and distance; Tanner-graph girth 6; some codes meet best known binary minimum distance; novel algebraic bounds vs prior simulation-based work.


<details>
  <summary>Details</summary>
Motivation: Address the lack of algebraic constructions and systematic parameter bounds for hierarchical quasi-cyclic (HQC) codes, by leveraging Reed-Solomon codes and Kautz-Singleton superimposed-code ideas to build HQC families with field-size-determined structure.

Method: Construct HQC codes by applying a 1964 Kautz-Singleton superimposed-code framework to Reed-Solomon codes, establishing that the hierarchy depth and RS-derived indices are governed by the underlying field size; extend to certain polynomial evaluation codes; derive explicit code parameters and bounds on rank and distance; provide a small catalog of codes and analyze Tanner-graph properties.

Result: Obtained explicit HQC codes whose hierarchical levels correspond to the field size; RS-based construction yields Tanner graphs with girth 6 from k=2; a table of small codes shows some attain the best-known binary minimum distance with the HQC structure; new algebraic bounds on parameters complement existing literature.

Conclusion: Demonstrates the feasibility and benefits of algebraic HQC constructions, offering concrete codes, parameter bounds, and connections to related work, and highlighting the shift from simulation-based studies to algebraic analysis.

Abstract: We introduce the first example of algebraically constructed hierarchical quasi-cyclic codes. These codes are built from Reed-Solomon codes using a 1964 construction of superimposed codes by Kautz and Singleton. We show both the number of levels in the hierarchy and the index of these Reed-Solomon derived codes are determined by the field size. We show that this property also holds for certain additional classes of polynomial evaluation codes.
  We provide explicit code parameters and properties as well as some additional bounds on parameters such as rank and distance. In particular, starting with Reed-Solomon codes of dimension $k=2$ yields hierarchical quasi-cyclic codes with Tanner graphs of girth 6.
  We present a table of small code parameters and note that some of these codes meet the best known minimum distance for binary codes, with the additional hierarchical quasi-cyclic structure. We draw connections to similar constructions in the literature, but importantly, while existing literature on related codes is largely simulation-based, we present a novel algebraic approach to determining new bounds on parameters of these codes.

</details>


### [68] [Continuous Angular Power Spectrum Recovery From Channel Covariance via Chebyshev Polynomials](https://arxiv.org/abs/2512.24039)
*Shengsong Luo,Ruilin Wu,Chongbin Xu,Junjie Ma,Xiaojun Yuan,Xin Wang*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper proposes a Chebyshev polynomial expansion framework for the recovery of a continuous angular power spectrum (APS) from channel covariance. By exploiting the orthogonality of Chebyshev polynomials in a transformed domain, we derive an exact series representation of the covariance and reformulate the inherently ill-posed APS inversion as a finite-dimensional linear regression problem via truncation. The associated approximation error is directly controlled by the tail of the APS's Chebyshev series and decays rapidly with increasing angular smoothness. Building on this representation, we derive an exact semidefinite characterization of nonnegative APS and introduce a derivative-based regularizer that promotes smoothly varying APS profiles while preserving transitions of clusters. Simulation results show that the proposed Chebyshev-based framework yields accurate APS reconstruction, and enables reliable downlink (DL) covariance prediction from uplink (UL) measurements in a frequency division duplex (FDD) setting. These findings indicate that jointly exploiting smoothness and nonnegativity in a Chebyshev domain provides an effective tool for covariance-domain processing in multi-antenna systems.

</details>


### [69] [Random Multiplexing](https://arxiv.org/abs/2512.24087)
*Lei Liu,Yuhao Chi,Shunqi Huang,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 随机多路复用从物理信道解耦，提升高机动场景下的鲁棒性，结合 AMP-type 检测实现 replica MAP BER 最优性；提出 CD-MAMP 低复杂度检测器与最优功率分配、编码策略，具广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 在高移动性和复杂多径环境中，传统基于信道结构的OFDM/OTFS等方法对信道结构敏感，鲁棒性受限。需要一种与物理信道无关、可应用于任意范数有界且光谱收敛的信道矩阵的鲁棒检测框架，以实现统计衰落信道的等效输入各向同性性并保证检测性能。

Method: 提出随机多路复用，将输入映射到随机变换域，构造等效输入各向同性的通道矩阵，证明在唯一固定点假设下，线性系统的 replica MAP BER 能够被 AMP 型探测器达到最优。设计低复杂度的跨域记忆型 AMP（CD-MAMP）探测器，利用时域信道稀疏性及等效信道随机性。给出最优功率分配以最小化 replica MAP BER、最大化 replica 限制容量，并讨论 CD-MAMP 探测器的最优编码原理与 replica 限制容量最优性，以及随机多路复用在多种无线应用中的适用性。

Result: 理论上证明：在任意范数有界且光谱收敛的信道矩阵和信号配置下，AMP 型探测器可达到 replica MAP BER 的渐进最优；提出 CD-MAMP 实现低复杂度检测并对时域信道的稀疏性与随机性进行有效利用；推导出使 BER 最小化与容量最大化的最优功率分配策略；给出 CD-MAMP 的编码与容量最优性分析，以及随机多路复用的广义适用性。

Conclusion: 随机多路复用为无线传输提供了一种与信道结构无关、对高机动场景鲁棒的检测范式，结合 CD-MAMP 等低复杂度检测和最优资源分配，具有广泛的理论保障和应用潜力。

Abstract: As wireless communication applications evolve from traditional multipath environments to high-mobility scenarios like unmanned aerial vehicles, multiplexing techniques have advanced accordingly. Traditional single-carrier frequency-domain equalization (SC-FDE) and orthogonal frequency-division multiplexing (OFDM) have given way to emerging orthogonal time-frequency space (OTFS) and affine frequency-division multiplexing (AFDM). These approaches exploit specific channel structures to diagonalize or sparsify the effective channel, thereby enabling low-complexity detection. However, their reliance on these structures significantly limits their robustness in dynamic, real-world environments. To address these challenges, this paper studies a random multiplexing technique that is decoupled from the physical channels, enabling its application to arbitrary norm-bounded and spectrally convergent channel matrices. Random multiplexing achieves statistical fading-channel ergodicity for transmitted signals by constructing an equivalent input-isotropic channel matrix in the random transform domain. It guarantees the asymptotic replica MAP bit-error rate (BER) optimality of AMP-type detectors for linear systems with arbitrary norm-bounded, spectrally convergent channel matrices and signaling configurations, under the unique fixed point assumption. A low-complexity cross-domain memory AMP (CD-MAMP) detector is considered, leveraging the sparsity of the time-domain channel and the randomness of the equivalent channel. Optimal power allocations are derived to minimize the replica MAP BER and maximize the replica constrained capacity of random multiplexing systems. The optimal coding principle and replica constrained-capacity optimality of CD-MAMP detector are investigated for random multiplexing systems. Additionally, the versatility of random multiplexing in diverse wireless applications is explored.

</details>


### [70] [When Wires Can't Keep Up: Reconfigurable AI Data Centers Empowered by Terahertz Wireless Communications](https://arxiv.org/abs/2512.24110)
*Chong Han,Mingjie Zhu,Wenqi Zhao,Ziming Yu,Guolong Huang,Guangjian Wang,Wen Tong,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出在数据中心内使用 Terahertz 无线链路（THz-WDC）来解决 AI 工作负载的互联瓶颈，目标实现高带宽、低时延和高能效，覆盖1-100米短中距，支持单链路1 Tbps、聚合10 Tbps，亚50 ns 单跳时延，20米时<10 pJ/bit能耗，并给出实现路径与对比分析。


<details>
  <summary>Details</summary>
Motivation: AI 工作负载急剧增长带来对高带宽、低时延、可扩展的互连需求；传统铜缆与光纤面临的功耗、延迟、刚性等瓶颈制约数据中心扩展。

Method: 提出 THz-WDC 愿景，并定义性能/技术要求；探讨实现要素，如数字孪生编排、低复杂度波束操控、全硅 THz 收发与低复杂度模拟基带架构；结合未来模块化（量子/芯片拼块）架构的 interconnect 场景；提供对比分析的数值研究。

Result: 给出具体性能指标、系统对比分析结论，以及在某些距离-吞吐门槛上 THz 链路优于有线方案的域；尚处于设计/分析阶段，需进一步原型与验证。

Conclusion: 提出面向无线化、可重构、可持续的 AI 数据中心的路标，强调 THz 无线互连作为灵活的中到近距离互连解决方案的重要性。

Abstract: The explosive growth of artificial intelligence (AI) workloads in modern data centers demands a radical transformation of interconnect architectures. Traditional copper and optical wiring face fundamental challenges in latency, power consumption, and rigidity, constraining the scalability of distributed AI clusters. This article introduces a vision for Terahertz (THz) Wireless Data Center (THz-WDC) that combines ultra-broadband capacity, one-hop low-latency communication, and energy efficiency in the short-to-medium range (1-100m). Performance and technical requirements are first articulated, including up to 1 Tbps per link, aggregate throughput up to 10 Tbps via spatial multiplexing, sub-50 ns single-hop latency, and sub-10 pJ/bit energy efficiency over 20m. To achieve these ambitious goals, key enabling technologies are explored, including digital-twin-based orchestration, low-complexity beam manipulation technologies, all-silicon THz transceivers, and low-complexity analog baseband architectures. Moreover, as future data centers shift toward quantum and chiplet-based modular architectures, THz wireless links provide a flexible mechanism for interconnecting, testing, and reconfiguring these modules. Finally, numerical analysis is presented on the latency and power regimes of THz versus optical and copper interconnects, identifying the specific distance and throughput domains where THz links can surpass conventional wired solutions. The article concludes with a roadmap toward wireless-defined, reconfigurable, and sustainable AI data centers.

</details>


### [71] [Efficient Decoding of Twisted GRS Codes and Roth--Lempel Codes](https://arxiv.org/abs/2512.24217)
*Runtian Zhu,Lingfei Jin*

Main category: cs.IT

TL;DR: 本文研究非GRS单元MDS码中的解码问题，聚焦扭转广义里德-索鲁（TGRS）码与 Roth-Lempel 码，提出基于 Guruswami– Sudan 算法的列表解码与唯一解码算法。并扩展了 TGRS 的扭转数至 O(n^2)，给出 Roth-Lempel 的首次高效解码，同时列表解码在许多参数下超过传统的唯一解码半径。结合 AMD 码后，能够以高概率从解列表中恢复正确信息。


<details>
  <summary>Details</summary>
Motivation: 非GRS MDS 码在理论上与应用中比 GR S 码更少被理解，且 GR S 结构在某些密码学场景中可能成为不利因素。因此，研究 TGRS 和 Roth-Lempel 码的解码不仅具有理论兴趣，也具有实际意义，尤其是在高效解码和抗结构信息泄露的需求下。

Method: 基于 Guruswami–Sudan 泛用的算法框架，提出适用于 TGRS 码与 Roth-Lempel 码的列表解码和唯一解码算法；对 TGRS 扭转数进行扩展（最多可达 O(n^2)），并给出 Roth-Lempel 的高效解码策略；在列表解码框架中引入 AMD 码以提高从输出列表中恢复正确消息的概率。

Result: 在给定的参数条件下，解码复杂度近线性（与码长成正比的常数因子较低），显著优于先前的二次时间复杂度；TGRS 解码器实现了对固定速率的多扭转情形（直至 O(n^2) 扭转）的扩展；Roth-Lempel 码给出似乎是首个高效解码算法；列表解码在广泛参数下超越经典的唯一解码半径；结合 AMD 码后，能够以高概率从输出列表中恢复正确信息。

Conclusion: 该工作显著推进了非GRS MDS 码的解码研究，提供了高效解码工具并扩展了可处理的扭转规模，亦为未来在更广参数范围内应用与理论分析铺平了道路，同时将 AMD 集成到列表解码框架提升了结果的鲁棒性。

Abstract: MDS codes play a central role in practice due to their broad applications. To date, most known MDS codes are generalized Reed-Solomon (GRS) codes, leaving codes that are not equivalent to GRS codes comparatively less understood. Studying this non-GRS regime is therefore of intrinsic theoretical interest, and is also practically relevant since the strong algebraic structure of GRS codes can be undesirable in cryptographic settings. Among the known non-GRS codes, twisted generalized Reed-Solomon (TGRS) codes and Roth-Lempel codes are two representative families of non-GRS codes that have attracted significant attention. Though substantial work has been devoted to the construction and structural analysis of TGRS and Roth-Lempel codes, comparatively little attention has been paid to their decoding, and many problems remain open. In this paper, we propose list and unique decoding algorithms for TGRS codes and Roth-Lempel codes based on the Guruswami-Sudan algorithm. Under suitable parameter conditions, our algorithms achieve near-linear running time in the code length, improving upon the previously best-known quadratic-time complexity. Our TGRS decoder supports fixed-rate TGRS codes with up to O(n^2) twists, substantially extending prior work that only handled the single-twist case. For Roth-Lempel codes, we provide what appears to be the first efficient decoder. Moreover, our list decoders surpass the classical unique-decoding radius for a broad range of parameters. Finally, we incorporate algebraic manipulation detection (AMD) codes into the list-decoding framework, enabling recovery of the correct message from the output list with high probability.

</details>


### [72] [SC-LDPC Codes Over $\mathbb{F}_q$: Minimum Distance, Decoding Analysis and Threshold Saturation](https://arxiv.org/abs/2512.24232)
*Jiaxin Lyu,Guanghui He*

Main category: cs.IT

TL;DR: 研究了有限域上随机时序耦合LDPC码族（SC-LDPC）在两种耦合结构下的距离性能和阈值行为，给出统一的理论框架和结果。


<details>
  <summary>Details</summary>
Motivation: 扩展和统一分析在有限字段上耦合LDPC码的最小距离、停止集大小及迭代解码阈值，探讨不同边扩展规则下的性能差异，并证明在QMSCs条件下的阈值饱和现象具有普适性。

Method: 定义多组独立的、均匀随机的单项映射来构造随机 Tanner 图，比较标准耦合与改进耦合两类 Ensemble；建立对对称概率测度及其在有限维概率简单形上的分析框架，研究其降解关系、度量拓扑及线性泛函；在QMSCs上推导并证明在耦合参数增大时，BP 阈值收敛到一个只依赖于集合和信道族的普适阈值。

Result: 两类耦合集合在极限下均具有渐近良好的最小距离和最小停止集大小；改进耦合集合在距离性能上优于标准耦合集合；给出一个普适的阈值饱和结果，即耦合系统的BP阈值趋于一个由集合和信道族决定的固定阈值。

Conclusion: 提出了一个统一的分析框架来研究有限字段SC-LDPC码的距离、解码阈值和降解特性，并证明在QMSCs上实现的阈值饱和具有普适性，利于设计更优的耦合LDPC码族。

Abstract: We investigate random spatially coupled low-density parity-check (SC-LDPC) code ensembles over finite fields. Under different variable-node edge-spreading rules, the random Tanner graphs of several coupled ensembles are defined by multiple independent, uniformly random monomial maps. The two main coupled ensembles considered are referred to as the standard coupled ensemble and the improved coupled ensemble. We prove that both coupled ensembles exhibit asymptotically good minimum distance and minimum stopping set size. Theoretical and numerical results show that the improved coupled ensemble can achieve better distance performance than the standard coupled ensemble. We introduce the essential preliminaries and analytical tools needed to analyze the iterative decoding threshold of coupled ensembles over any finite field. We consider a class of memoryless channels with special symmetry, termed q-ary input memoryless symmetric channels (QMSCs), and show that, for these channels, the distribution of channel messages (in form of probability vectors) likewise exhibits this symmetry. Consequently, we define symmetric probability measures and their reference measures on a finite-dimensional probability simplex, analyze their foundational properties and those of their linear functionals, endow their respective spaces with metric topologies, and conduct an in-depth study of their degradation theory. Based on our analytical framework, we establish a universal threshold saturation result for both of the coupled ensembles over a q-ary finite field on QMSCs. Specifically, as the coupling parameters increase, the belief-propagation threshold of a coupled system saturates to a well-defined threshold that depends only on the underlying ensemble and the channel family.

</details>


### [73] [Infinite families of graphs and stable completion of arbitrary matrices, Part I](https://arxiv.org/abs/2512.24468)
*Augustin Cosse*

Main category: cs.IT

TL;DR: 给出确定性图构造，使低秩矩阵在一般取值下能唯一且稳定地完成；通过自回避游走模式刻画可完成性，并利用 SOS 层次设计无限族图，实现对任意固定秩的精确与稳定完成。


<details>
  <summary>Details</summary>
Motivation: 解决矩阵完成中的确定性保障问题；把可完成性与支撑图中的自回避路径模式联系起来，利用格子图子图的结构来获得对任意输入的鲁棒性；并探索在 Sum-of-Squares（SOS）层次框架下的实现。

Method: 提出确定性图的构造；在由双边邻接支撑生成的格子图子图中研究自回避游走的特定并联合模式与可完成性的关系；据此设计无限族图，使得在每个固定秩下通过 SOS 层次实现精确且稳定的完成。

Result: 给出可验证的模式条件，若子图包含这些自回避游走的模式，则可实现唯一完成；构造出可无限扩展的图族；证明对于任意固定秩，在 SOS 层次下可获得精确与稳定的低秩矩阵完成。

Conclusion: 将可完成性与格子图中的自回避游走模式绑定，提供了确定性图设计的原则，使低秩矩阵的唯一且稳定完成在广泛输入下成为可能；SOS 层次提供了实现框架，桥接组合结构与矩阵完成的理论。

Abstract: We study deterministic constructions of graphs for which the unique completion of low rank matrices is generically possible regardless of the values of the entries. We relate the completability to the presence of some patterns (particular unions of self-avoiding walks) in the subgraph of the lattice graph generated from the support of the bi-adjacency matrix. The construction makes it possible to design infinite families of graphs on which exact and stable completion is possible for every fixed rank matrix through the sum-of-squares hierarchy.

</details>


### [74] [Overflow-Avoiding Memory AMP](https://arxiv.org/abs/2407.03898)
*Shunqi Huang,Lei Liu,Brian M. Kurkoski*

Main category: cs.IT

TL;DR: 提出两种 GD-MAMP 的改进：OA-GD-MAMP 以避免溢出、CR-GD-MAMP 以降低每次迭代的矩阵-向量乘法数量。


<details>
  <summary>Details</summary>
Motivation: 解决 GD-MAMP 在高维高噪声线性系统中的数值溢出问题，并在保证收敛性的前提下提升计算效率。

Method: 1) 设计溢出避免的 GD-MAMP（OA-GD-MAMP），通过数值稳定性策略消除中间变量的溢出风险。2) 设计降低复杂度的 GD-MAMP（CR-GD-MAMP），将每次迭代的矩阵-向量乘法从 3 次降至 2 次。

Result: 实现 OA-GD-MAMP 以提升数值稳定性，并通过 CR-GD-MAMP 在几乎不影响收敛速度的前提下降低了每次迭代的计算量（从 3 次到 2 次矩阵-向量乘法，降低约 1/3 的运算量）。

Conclusion: 本文提出两条改进路径：溢出鲁棒性和计算效率的提升，分别对应 OA-GD-MAMP 和 CR-GD-MAMP，为 Memory AMP 框架下的 GD-MAMP 提供更稳定且高效的实现。

Abstract: Approximate Message Passing (AMP) type algorithms are widely used for signal recovery in high-dimensional noisy linear systems. Recently, a principle called Memory AMP (MAMP) was proposed. Leveraging this principle, the gradient descent MAMP (GD-MAMP) algorithm was designed, inheriting the strengths of AMP and OAMP/VAMP. In this paper, we first provide an overflow-avoiding GD-MAMP (OA-GD-MAMP) to address the overflow problem that arises from some intermediate variables exceeding the range of floating point numbers. Second, we develop a complexity-reduced GD-MAMP (CR-GD-MAMP) to reduce the number of matrix-vector products per iteration by 1/3 (from 3 to 2) with little to no impact on the convergence speed.

</details>


### [75] [Random Modulation: Achieving Asymptotic Replica Optimality over Arbitrary Norm-Bounded and Spectrally Convergent Channel Matrices](https://arxiv.org/abs/2508.08099)
*Lei Liu,Yuhao Chi,Shunqi Huang*

Main category: cs.IT

TL;DR: 提出一种与信道矩阵解耦的随机调制，可将任意范数受限且谱收敛的信道转化为等效稠密随机信道，并给出可用于线性系统的 AMP-type 检测器的 replica MAP BER 最优性；提出低复杂度的跨域记忆型 AMP(CD-MAMP)检测器及在未知信道状态信息情况下的最优功率分配策略，实验显示相比 5G-NR LDPC 编码的 OFDM/OTFS/AFDM，BER/BLER 提升约 2–3 dB。


<details>
  <summary>Details</summary>
Motivation: 解决一般信道矩阵条件下 AMP 型检测的性能受限与理论分析困难，需通过引入随机调制实现充分统计的信道衰落，并获得 replica 理论下的近似最优性；同时追求低复杂度、可实现的检测算法和功率分配策略，以提升实际通信系统性能。

Method: 通过随机调制构造等效的稠密随机信道矩阵，使信道衰落统计性增强；给出在状态演化有唯一固定点条件下，AMP 型检测器的 replica MAP BER 最优性证明；提出跨域记忆型 AMP (CD-MAMP) 检测器，利用时域信道的稀疏性和变换域的随机性；推导在有 CSI 条件下实现最优功率分配以最小化 replica MAP BER、最大化 replica constrained capacity 的策略。

Result: 理论上给出 replica MAP BER 的最优性条件；提出 CD-MAMP 检测器；数值结果显示在与 OFDM/OTFS/AFDM+5G-NR LDPC 的对比中，BER/BLER 提升约 2–3 dB，且在平均与优化功率分配下均成立。

Conclusion: 随机调制使得任意范数界限且谱收敛的信道也可形成等效稠密随机信道，从而实现对 AMP 型检测的理论与实际性能提升，所提出的检测器与功率分配策略对未来高效线性系统具有指导意义。

Abstract: This paper introduces a random modulation technique that is decoupled from the channel matrix, allowing it to be applied to arbitrary norm-bounded and spectrally convergent channel matrices. The proposed random modulation constructs an equivalent dense and random channel matrix, ensuring that the signals undergo sufficient statistical channel fading. It also guarantees the asymptotic replica maximum a posteriori (MAP) bit-error rate (BER) optimality of approximate message passing (AMP)-type detectors for linear systems with arbitrary norm-bounded and spectrally convergent channel matrices when their state evolution has a unique fixed point. Then, a low-complexity cross-domain memory approximate message passing (CD-MAMP) detector is proposed for random modulation, leveraging the sparsity of the time-domain channel and the randomness of the random transform-domain channel. Furthermore, the optimal power allocation schemes are derived to minimize the replica MAP BER and maximize the replica constrained capacity of random-modulated linear systems, assuming the availability of channel state information (CSI) at the transceiver. Numerical results show that the proposed random modulation can achieve BER and block-error rate (BLER) performance gains of up to 2 - 3 dB compared to existing OFDM/OTFS/AFDM with 5G-NR LDPC codes, under both average and optimized power allocation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [76] [Secure and Governed API Gateway Architectures for Multi-Cluster Cloud Environments](https://arxiv.org/abs/2512.23774)
*Vinoth Punniyamoorthy,Kabilan Kannan,Akshay Deshpande,Lokesh Butra,Akash Kumar Agarwal,Adithya Parthasarathy,Suhas Malempati,Bikesh Kumar*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: API gateways serve as critical enforcement points for security, governance, and traffic management in cloud-native systems. As organizations increasingly adopt multi-cluster and hybrid cloud deployments, maintaining consistent policy enforcement, predictable performance, and operational stability across heterogeneous gateway environments becomes challenging. Existing approaches typically manage security, governance, and performance as loosely coupled concerns, leading to configuration drift, delayed policy propagation, and unstable runtime behavior under dynamic workloads. This paper presents a governance-aware, intent-driven architecture for coordinated API gateway management in multi-cluster cloud environments. The proposed approach expresses security, governance, and performance objectives as high-level declarative intents, which are systematically translated into enforceable gateway configurations and continuously validated through policy verification and telemetry-driven feedback. By decoupling intent specification from enforcement while enabling bounded, policy-compliant adaptation, the architecture supports heterogeneous gateway implementations without compromising governance guarantees or service-level objectives. A prototype implementation across multiple Kubernetes clusters demonstrates the effectiveness of the proposed design. Experimental results show up to a 42% reduction in policy drift, a 31% improvement in configuration propagation time, and sustained p95 latency overhead below 6% under variable workloads, compared to manual and declarative baseline approaches. These results indicate that governance-aware, intent-driven gateway orchestration provides a scalable and reliable foundation for secure, consistent, and performance-predictable cloud-native platforms.

</details>


### [77] [SyncGait: Robust Long-Distance Authentication for Drone Delivery via Implicit Gait Behaviors](https://arxiv.org/abs/2512.23778)
*Zijian Ling,Man Zhou,Hongda Zhai,Yating Huang,Lingchen Zhao,Qi Li,Chao Shen,Qian Wang*

Main category: cs.CR

TL;DR: 用步态生物识别实现的无人机互认证：SyncGait通过用户行走时的臂摆实现远距离无硬件认证，准确率高且抗欺骗。


<details>
  <summary>Details</summary>
Motivation: 解决无人机配送中高价值载荷对安全距离和互认证的需求，同时弥补现有方法在认证距离和抗攻击能力上的不足。

Method: 利用用户走向无人机时的隐式步态特征（臂摆）进行双向认证，无需额外硬件或专门的认证动作，实现被动、隐性 mutual authentication。

Result: 在31名受试者、14组数据集上进行广泛实验；在大于18米的远距离下平均准确率为99.84%，并对多类欺骗攻击具有较强鲁棒性。

Conclusion: SyncGait是一种鲁棒、安全、易用的步态基础互认证方案，适合实际无人机配送场景。

Abstract: In recent years, drone delivery, which utilizes unmanned aerial vehicles (UAVs) for package delivery and pickup, has gradually emerged as a crucial method in logistics. Since delivery drones are expensive and may carry valuable packages, they must maintain a safe distance from individuals until user-drone mutual authentication is confirmed. Despite numerous authentication schemes being developed, existing solutions are limited in authentication distance and lack resilience against sophisticated attacks. To this end, we introduce SyncGait, an implicit gait-based mutual authentication system for drone delivery. SyncGait leverages the user's unique arm swing as he walks toward the drone to achieve mutual authentication without requiring additional hardware or specific authentication actions. We conducted extensive experiments on 14 datasets collected from 31 subjects. The results demonstrate that SyncGait achieves an average accuracy of 99.84\% at a long distance ($>18m$) and exhibits strong resilience against various spoofing attacks, making it a robust, secure, and user-friendly solution in real-world scenarios.

</details>


### [78] [Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark](https://arxiv.org/abs/2512.23779)
*Manu,Yi Guo,Jo Plested,Tim Lynar,Kanchana Thilakarathna,Nirhoshan Sivaroopan,Jack Yang,Wangli Yang*

Main category: cs.CR

TL;DR: 提出一个黑箱、基于提示的 DoS 风格攻击基准，评估两种攻击者（EOGen 和 RL-GOAL）对大型语言模型的过生成行为，并引入 Over-Generation Factor（OGF）等度量。结果表明 RL-GOAL 相对 EOGen 更具攻击性，能产生更长的续写。


<details>
  <summary>Details</summary>
Motivation: LLMs 可能被诱导产生过长的输出，造成质量下降、延迟与成本上升，甚至被用于 DoS 攻击。需要一个黑箱、只查询、且具可比性的提示攻击基准来评估多种攻击算法的效力。

Method: 在黑箱、查询只访问、已知分词器的设定下，提出两种提示攻击：EOGen（进化搜索前缀以抑制 EOS、诱导长续写）和 RL-GOAL（基于目标长度的条件化前缀生成）并定义 Over-Generation Factor（OGF）及对 stall/延迟的衡量。对 Phi-3 目标模型进行评估。

Result: EOGen 的平均 OGF 为 1.38 ± 1.15，Success@OGF≥2 为 24.5%。RL-GOAL 表现更强，跨模型平均 OGF 可达 2.81 ± 1.38。

Conclusion: 提出的基准与指标能够在黑箱、提示级别对抗中比较不同攻击策略的有效性；RL-GOAL 在这类攻击中更具攻击性，且方法学有助于后续防护研究。

Abstract: Large language models (LLMs) can be driven into over-generation, emitting thousands of tokens before producing an end-of-sequence (EOS) token. This degrades answer quality, inflates latency and cost, and can be weaponized as a denial-of-service (DoS) attack. Recent work has begun to study DoS-style prompt attacks, but typically focuses on a single attack algorithm or assumes white-box access, without an attack-side benchmark that compares prompt-based attackers in a black-box, query-only regime with a known tokenizer. We introduce such a benchmark and study two prompt-only attackers. The first is Evolutionary Over-Generation Prompt Search (EOGen), which searches the token space for prefixes that suppress EOS and induce long continuations. The second is a goal-conditioned reinforcement learning attacker (RL-GOAL) that trains a network to generate prefixes conditioned on a target length. To characterize behavior, we introduce Over-Generation Factor (OGF), the ratio of produced tokens to a model's context window, along with stall and latency summaries. Our evolutionary attacker achieves mean OGF = 1.38 +/- 1.15 and Success@OGF >= 2 of 24.5 percent on Phi-3. RL-GOAL is stronger: across victims it achieves higher mean OGF (up to 2.81 +/- 1.38).

</details>


### [79] [Application-Specific Power Side-Channel Attacks and Countermeasures: A Survey](https://arxiv.org/abs/2512.23785)
*Sahan Sanjaya,Aruna Jayasena,Prabhat Mishra*

Main category: cs.CR

TL;DR: 综合评述功率侧信道攻击及对策，覆盖多应用领域并进行比较


<details>
  <summary>Details</summary>
Motivation: 随着功率侧信道在密钥提取、模型反向、用户行为利用等领域的安全威胁日益突出，系统性总结与对比分析需求增加

Method: 对相关文献进行广泛综述，分类攻击类型、应用域、对比指标和缓解措施，强调应用特定因素对防护策略的影响

Result: 给出一个全面的功率侧信道攻击综述，涵盖攻击分类、领域差异、潜在风险以及现有缓解手段

Conclusion: 为研究者提供跨领域的攻击地图与对策框架，帮助在不同应用场景中设计更有效的防护措施

Abstract: Side-channel attacks try to extract secret information from a system by analyzing different side-channel signatures, such as power consumption, electromagnetic emanation, thermal dissipation, acoustics, time, etc. Power-based side-channel attack is one of the most prominent side-channel attacks in cybersecurity, which rely on data-dependent power variations in a system to extract sensitive information. While there are related surveys, they primarily focus on power side-channel attacks on cryptographic implementations. In recent years, power-side channel attacks have been explored in diverse application domains, including key extraction from cryptographic implementations, reverse engineering of machine learning models, user behavior data exploitation, and instruction-level disassembly. In this paper, we provide a comprehensive survey of power side-channel attacks and their countermeasures in different application domains. Specifically, this survey aims to classify recent power side-channel attacks and provide a comprehensive comparison based on application-specific considerations.

</details>


### [80] [Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense](https://arxiv.org/abs/2512.23849)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.CR

TL;DR: EDS 是一种检测无关的安全框架，通过可自适应的计算难题、诱饵信息熵、时间拉伸和带宽征税四大机制，将攻击成本放大并弱化攻击者收益，从而在资源受限的 IoT/边缘环境中实现经济性防御。该框架以 Stackelberg 博弈形式建模，给出最优参数的闭式解（定理1），并证明组合机制的成本放大大于单一机制的线性叠加（定理2），实现<12KB 内存占用，能在 ESP32 级微控制器部署。评估覆盖 20 设备的异构 IoT 测试台和 IoT-23 恶意软件，显示显著的攻击减缓和成本不对称性，并与 ML-IDS 结合时提升效果。


<details>
  <summary>Details</summary>
Motivation: 在 IoT/边缘环境中，面向检测的安全方法容易被具备加密、潜行与低速攻击策略的高度隐蔽攻击者规避，资源受限使得基于 ML 的入侵检测难以落地，因此需要一种检测独立、以经济性压制攻击的防御框架。

Method: 将四种机制组合成一个检测无关的防御体系，并将其建模为栈式博弈（Stackelberg game）。推导出参数的闭式均衡（定理1），并证明机制组合的成本放大效应为2.1x（定理2）。实现对 ESP32 器件的低资源实现（<12KB 内存）。在一个 20 设备、异构的 IoT 测试台及 IoT-23 恶意软件数据集上进行实证评估，比较单独机制与组合机制的效果，及其与 ML-IDS 的联合效果。

Result: 4 种机制的组合在多项指标上实现显著改进：攻击速度下降 32-560 倍、成本不对称性 85-520:1、攻击成功率下降 8-62%、延迟开销 <20ms、误报率接近 0%。在 IoT-23 恶意软件上实现 88% 独立缓解；与 ML-IDS 联合时达到 94% 缓解率，相较于仅使用 IDS 的 67% 提升约 27%。

Conclusion: EDS 提供一种检测无关、适用于资源受限环境的保护策略，有别于传统侦测优先模式。其核心在于通过经济性抑制实现对攻击者的成本放大，从而改变战斗的经济平衡。结果显示即使不依赖 IDS，EDS 也能效果显著；与 IDS 结合时的总体性能更佳，具有成为 IoT/边缘系统防护补充或替代的潜力。

Abstract: Detection-based security fails against sophisticated attackers using encryption, stealth, and low-rate techniques, particularly in IoT/edge environments where resource constraints preclude ML-based intrusion detection. We present Economic Denial Security (EDS), a detection-independent framework that makes attacks economically infeasible by exploiting a fundamental asymmetry: defenders control their environment while attackers cannot. EDS composes four mechanisms adaptive computational puzzles, decoy-driven interaction entropy, temporal stretching, and bandwidth taxation achieving provably superlinear cost amplification. We formalize EDS as a Stackelberg game, deriving closed-form equilibria for optimal parameter selection (Theorem 1) and proving that mechanism composition yields 2.1x greater costs than the sum of individual mechanisms (Theorem 2). EDS requires < 12KB memory, enabling deployment on ESP32 class microcontrollers. Evaluation on a 20-device heterogeneous IoT testbed across four attack scenarios (n = 30 trials, p < 0.001) demonstrates: 32-560x attack slowdown, 85-520:1 cost asymmetry, 8-62% attack success reduction, < 20ms latency overhead, and close to 0% false positives. Validation against IoT-23 malware (Mirai, Torii, Hajime) shows 88% standalone mitigation; combined with ML-IDS, EDS achieves 94% mitigation versus 67% for IDS alone a 27% improvement. EDS provides detection-independent protection suitable for resource-constrained environments where traditional approaches fail. The ability to detect and mitigate the malware samples tested was enhanced; however, the benefits provided by EDS were realized even without the inclusion of an IDS. Overall, the implementation of EDS serves to shift the economic balance in favor of the defender and provides a viable method to protect IoT and edge systems methodologies.

</details>


### [81] [RepetitionCurse: Measuring and Understanding Router Imbalance in Mixture-of-Experts LLMs under DoS Stress](https://arxiv.org/abs/2512.23995)
*Ruixuan Huang,Qingyue Wang,Hantao Huang,Yudong Gao,Dong Chen,Shuai Wang,Wei Wang*

Main category: cs.CR

TL;DR: MoE路由在推理阶段缺乏显式负载均衡约束，可能被对抗性输入利用，将大多数token路由到少数顶k专家，导致设备瓶颈和服务中断；提出RepetitionCurse作为低成本黑盒攻击方法，在模型无关的情况下通过重复token模式放大延迟，在Mixtral-8x7B上实现约3.06x的端到端推理延迟增加。


<details>
  <summary>Details</summary>
Motivation: 在使用专家并行来扩展大语言模型时，推理阶段的负载均衡成为关键瓶颈。若路由策略易受输入模式影响，可能被对手利用触发资源不均，造成服务层级协议(SLA)违反和可用性下降。该研究揭示MoE路由中的普遍弱点并评估其现实影响。

Method: 提出一个黑盒攻击框架RepetitionCurse，通过观察和利用MoE路由行为的一致性缺陷，使用简单的重复token模式构建对抗性提示，且不依赖模型内部参数、可跨模型泛化。该方法在广泛部署的MoE模型（如Mixtral-8x7B）上验证。

Result: 攻击显著放大端到端推理延迟，报告延迟提升约3.063倍，并显著降低服务可用性，显示了MoE路由的潜在可被利用的拒绝服务风险。

Conclusion: MoE路由缺乏显式的负载均衡约束容易成为攻击面。需要在模型架构和推理系统层面引入鲁棒的路由控制、负载均衡机制以及对抗性测试，以确保在真实部署中的稳定性和 SLA合规性。

Abstract: Mixture-of-Experts architectures have become the standard for scaling large language models due to their superior parameter efficiency. To accommodate the growing number of experts in practice, modern inference systems commonly adopt expert parallelism to distribute experts across devices. However, the absence of explicit load balancing constraints during inference allows adversarial inputs to trigger severe routing concentration. We demonstrate that out-of-distribution prompts can manipulate the routing strategy such that all tokens are consistently routed to the same set of top-$k$ experts, which creates computational bottlenecks on certain devices while forcing others to idle. This converts an efficiency mechanism into a denial-of-service attack vector, leading to violations of service-level agreements for time to first token. We propose RepetitionCurse, a low-cost black-box strategy to exploit this vulnerability. By identifying a universal flaw in MoE router behavior, RepetitionCurse constructs adversarial prompts using simple repetitive token patterns in a model-agnostic manner. On widely deployed MoE models like Mixtral-8x7B, our method increases end-to-end inference latency by 3.063x, degrading service availability significantly.

</details>


### [82] [Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?](https://arxiv.org/abs/2512.24044)
*Yuan Xin,Dingfan Chen,Linyi Yang,Michael Backes,Xiao Zhang*

Main category: cs.CR

TL;DR: 系统性评估表明，越狱攻击在全推理管线下能被至少一个安全过滤器检测到；此前对攻击可行性的高估可能源自忽略输入/输出过滤的评估；仍需在召回率与精确度之间取得更好平衡并提高检测的实用性。


<details>
  <summary>Details</summary>
Motivation: 弥补以往仅评估模型本身、忽略部署管线安全机制的不足，首次对越狱攻击在完整推理管线中的有效性进行系统评估。

Method: 在包含输入和输出过滤的完整推理管线中，对多种越狱攻击进行测试，评估各安全过滤器的检测能力并量化攻击在不同过滤条件下的生效情况。

Result: 几乎所有越狱技术都能被至少一个安全过滤器检测到；安全过滤对检测有效，但在召回率与精确度之间仍存在权衡，且单纯评估攻击成功率可能被高估。

Conclusion: 需要提升检测的准确性与可用性，优化过滤系统的召回/精确度平衡，并推动对LLM安全系统的全面评估与改进。

Abstract: As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, previous evaluations have focused solely on the models, neglecting the full deployment pipeline, which typically incorporates additional safety mechanisms like content moderation filters. To address this gap, we present the first systematic evaluation of jailbreak attacks targeting LLM safety alignment, assessing their success across the full inference pipeline, including both input and output filtering stages. Our findings yield two key insights: first, nearly all evaluated jailbreak techniques can be detected by at least one safety filter, suggesting that prior assessments may have overestimated the practical success of these attacks; second, while safety filters are effective in detection, there remains room to better balance recall and precision to further optimize protection and user experience. We highlight critical gaps and call for further refinement of detection accuracy and usability in LLM safety systems.

</details>


### [83] [Spatial Discretization for Fine-Grain Zone Checks with STARKs](https://arxiv.org/abs/2512.24238)
*Sungmin Lee,Kichang Lee,Gyeongmin Han,JeongGil Ko*

Main category: cs.CR

TL;DR: 距离感知网格编码在零知识 PiP 检查中对网格粗略化具有显著准确性提升，同时保持可接受的验证开销。


<details>
  <summary>Details</summary>
Motivation: 在隐私保护的地理空间场景中，点-in-多边形测试的私有化执行成本高，需要研究不同区域编码对准确性和证明成本的影响。

Method: 在固定 STARK 执行模型下，利用网格基查找表对区域进行编码，比较布尔网格与距离感知编码，使用距离到边界的度量和单元内插值进行推理，并在真实数据上评估准确性与验证开销。

Result: 距离感知编码在粗网格上显著提升准确性（最多提升约60个百分点），且验证开销适中，约为布尔网格的1.4倍，证明区域编码是提升零知识空间检查效率的关键杠杆。

Conclusion: 区域编码（特别是距离感知编码）是实现高效零知识空间检查的关键技术之一，在权衡精度与证明成本方面具有明显优势。

Abstract: Many location-based services rely on a point-in-polygon test (PiP), checking whether a point or a trajectory lies inside a geographic zone. Since geometric operations are expensive in zero-knowledge proofs, privately performing the PiP test is challenging. In this paper, we answer the research questions of how different ways of encoding zones affect accuracy and proof cost by exploiting gridbased lookup tables under a fixed STARK execution model. Beyond a Boolean grid-based baseline that marks cells as in- or outside, we explore a distance-aware encoding approach that stores how far each cell is from a zone boundary and uses interpolation to reason within a cell. Our experiments on real-world data demonstrate that the proposed distance-aware approach achieves higher accuracy on coarse grids (max. 60%p accuracy gain) with only a moderate verification overhead (approximately 1.4x), making zone encoding the key lever for efficient zero-knowledge spatial checks.

</details>


### [84] [How Would Oblivious Memory Boost Graph Analytics on Trusted Processors?](https://arxiv.org/abs/2512.24255)
*Jiping Yu,Xiaowei Zhu,Kun Chen,Guanyu Feng,Yunyi Chen,Xiaoyu Fan,Wenguang Chen*

Main category: cs.CR

TL;DR: 在可信处理器中集成 oblivious memory (OM) 以保护图分析的访问模式，同时通过存储结构与算法的协同设计，在原型系统中实现显著的性能提升（约 100×，相对于基线）。


<details>
  <summary>Details</summary>
Motivation: 解决数据无关/访问模式导致的信息泄露与数据分析性能之间的矛盾，探讨在处理器层面引入 OM 的可行性与效用。

Method: 通过存储结构与算法的协同设计，将 OM 集成到现有处理器上，构建原型系统；OM 的容量被设计为接近每核缓存规模，以实现可观的性能与安全性折中，针对图分析工作负载进行评估。

Result: 当 OM 尺度接近每核缓存时，原型系统相对基线实现约 100×的速度提升，且对现有处理器几乎无额外开销，提供了在可信处理器上部署 OM 的可行性证据。

Conclusion: 存储与算法的协同设计是将 OM 落地在可信处理器中的关键，可显著提升图分析等对访问模式敏感的工作负载的性能，同时未来需要扩展到更多应用场景并评估成本与可扩展性。

Abstract: Trusted processors provide a way to perform joint computations while preserving data privacy. To overcome the performance degradation caused by data-oblivious algorithms to prevent information leakage, we explore the benefits of oblivious memory (OM) integrated in processors, to which the accesses are unobservable by adversaries. We focus on graph analytics, an important application vulnerable to access-pattern attacks. With a co-design between storage structure and algorithms, our prototype system is 100x faster than baselines given an OM sized around the per-core cache which can be implemented on existing processors with negligible overhead. This gives insights into equipping trusted processors with OM.

</details>


### [85] [FedSecureFormer: A Fast, Federated and Secure Transformer Framework for Lightweight Intrusion Detection in Connected and Autonomous Vehicles](https://arxiv.org/abs/2512.24345)
*Devika S,Vishnu Hari,Pratik Narang,Tejasvi Alladi,F. Richard Yu*

Main category: cs.CR

TL;DR: Encoder-only transformer with few layers for federated intrusion detection in connected and autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: 在连网与自动驾驶车辆中实现轻量级、隐私保护的入侵检测系统，降低计算和通信成本。

Method: 设计一个少层的编码器型 Transformer；在联邦学习框架中分布式训练，适配 CAV 域的安全特征与数据分布偏差。

Result: 摘要未给出具体实验结果；可推断该模型具有较低资源消耗，潜在保持检测性能，需要通过实证数据在未来工作中证实。

Conclusion: 该工作表明编码器仅 Transformer 在 CAV 场景下的联邦学习中具有应用潜力，可能为边缘安全与物联网安全研究提供参考。

Abstract: This works presents an encoder-only transformer built with minimum layers for intrusion detection in the domain of Connected and Autonomous Vehicles using Federated Learning.

</details>


### [86] [SourceBroken: A large-scale analysis on the (un)reliability of SourceRank in the PyPI ecosystem](https://arxiv.org/abs/2512.24400)
*Biagio Montaruli,Serena Elisa Ponta,Luca Compagna,Davide Balzarotti*

Main category: cs.CR

TL;DR: 对 SourceRank 的可靠性进行评估，聚焦对抗规避攻击的能力；发现现实世界数据中包的分布重叠，且 URL 混淆攻击显著提升恶意包的分数，导致难以区分良性与恶意包。


<details>
  <summary>Details</summary>
Motivation: 弥补已有研究未系统分析 SourceRank 在规避攻击下的鲁棒性；评估其在 PyPI 生态中的实际可用性和安全性。

Method: 提出威胁模型，辨识对 18 项指标的潜在规避手段（包括 URL 混淆；指向合法仓库的 URL 可能与恶意包无关）、在 MalwareBench 与包含 122,398 个包的真实数据集中评估 SourceRank 的分布；分析历史数据与实时数据的差异；观察 URL 混淆的流行趋势及与其他规避技术的组合效应。

Result: 历史数据能较好区分良性与恶意包，真实数据存在显著重叠，主要因对下架事件未及时更新；因此 SourceRank 难以在真实情景中区分包类别或筛选良性包。URL 混淆攻击比例从 MalwareBench 的 4.2% 上升到真实数据的 7.0%，且常与其他规避技术并用，可显著提升恶意包的分数。

Conclusion: SourceRank 在现实环境下不可靠用于区分良性与恶意包，需改进评估机制，考虑对下架更新的时效性以及对 URL 混淆等规避手段的鲁棒性。

Abstract: SourceRank is a scoring system made of 18 metrics that assess the popularity and quality of open-source packages. Despite being used in several recent studies, none has thoroughly analyzed its reliability against evasion attacks aimed at inflating the score of malicious packages, thereby masquerading them as trustworthy. To fill this gap, we first propose a threat model that identifies potential evasion approaches for each metric, including the URL confusion technique, which can affect 5 out of the 18 metrics by leveraging a URL pointing to a legitimate repository potentially unrelated to the malicious package.
  Furthermore, we study the reliability of SourceRank in the PyPI ecosystem by analyzing the SourceRank distributions of benign and malicious packages in the state-of-the-art MalwareBench dataset, as well as in a real-world dataset of 122,398 packages. Our analysis reveals that, while historical data suggests a clear distinction between benign and malicious packages, the real-world distributions overlap significantly, mainly due to SourceRank's failure to timely reflect package removals. As a result, SourceRank cannot be reliably used to discriminate between benign and malicious packages in real-world scenarios, nor to select benign packages among those available on PyPI.
  Finally, our analysis reveals that URL confusion represents an emerging attack vector, with its prevalence increasing from 4.2% in MalwareBench to 7.0% in our real-world dataset. Moreover, this technique is often used alongside other evasion techniques and can significantly inflate the SourceRank metrics of malicious packages.

</details>


### [87] [GateChain: A Blockchain Based Application for Country Entry Exit Registry Management](https://arxiv.org/abs/2512.24416)
*Mohamad Akkad,Hüseyin Bodur*

Main category: cs.CR

TL;DR: GateChain 通过区块链记录出入境事件，提供分布式、不可篡改、可验证的账本，以提升数据完整性、可靠性和可审计性，并实现对授权机构的实时访问控制。


<details>
  <summary>Details</summary>
Motivation: 随着全球人员流动增加、安全与互操作性需求上升，传统集中式边境控制系统易受数据篡改且跨机构协作受限，亟需一个可信的、可验证的跨机构数据共享解决方案。

Method: 设计并实现 GateChain 的区块链架构与安全组件，记录出入境事件在分布式账本上，提供加密可验证性、实时访问控制，以及面向授权机构的验证机制，同时对系统的性能与安全性进行评估。

Result: 论文描述了 GateChain 的体系结构和安全组件，并对其性能与安全特性进行了评估，表明该系统在数据完整性、可靠性、透明性以及授权机构的实时访问控制方面具有潜在提升。

Conclusion: GateChain 能有效缓解集中式边境控制的易篡改与互操作性不足问题，提升记录的不可变性、可验证性与可审计性，并促进跨机构的高效协作与信任。

Abstract: Recording entry and exit records for a country, with properties such as confidentiality, integrity, and auditability, is increasingly important due to rising international mobility and security requirements. Traditional border control systems, which rely on centralised databases, are vulnerable to data manipulation and have limited interoperability between institutions. This study presents GateChain, a blockchain-based application that addresses these vulnerabilities. GateChain aims to enhance data integrity, reliability, and transparency by recording entry and exit events on a distributed, immutable, and cryptographically verifiable ledger. The application provides real-time access control and verification for authorised institutions. This paper describes the architecture and security components of GateChain and evaluates its performance and security features.

</details>


### [88] [Document Data Matching for Blockchain-Supported Real Estate](https://arxiv.org/abs/2512.24457)
*Henrique Lin,Tiago Dias,Miguel Correia*

Main category: cs.CR

TL;DR: 提出一个将OCR、NLP和可验证凭证（VC）相结合的框架，用于自动化房地产业务中的文档提取、验证与管理；原型包含OCR-NLP提取管线、凭证发行与管理后端，以及支持发行人/持有人/验证者的前端；实验表明在多类文档上的准确性具有竞争力，端到端流程显著缩短验证时间，同时保持可靠性；该框架潜在地可提升交易效率、增强各方信任并实现可扩展的数字化流程。


<details>
  <summary>Details</summary>
Motivation: 房地产交易中大量文档依赖人工处理，流程低效且易受欺诈影响；需要一个可扩展、可信且自动化的解决方案来提高验证速度、降低风险并提升透明度。

Method: 将异构文档标准化为可验证凭证（VC），通过一个OCR-NLP提取管线实现文档数据提取，并用自动数据匹配检测不一致性；利用区块链作为去中心化的信任层，增强链上可追溯性与数据完整性；原型包括：1) 以合成数据集训练的OCR-NLP提取管线，2) 用于凭证发行与管理的后端，3) 支持发行人/持有人/验证者交互的前端。

Result: 模型在多种文档类型上达到具有竞争力的准确性；端到端管线显著减少了验证时间，同时保持了可靠性。

Conclusion: 所提出的框架具有简化房地产交易、提升各方信任、实现可扩展且安全的数字化流程的潜力，体现了将OCR/NLP与VCs和区块链结合的实际应用前景。

Abstract: The real estate sector remains highly dependent on manual document handling and verification, making processes inefficient and prone to fraud. This work presents a system that integrates optical character recognition (OCR), natural language processing (NLP), and verifiable credentials (VCs) to automate document extraction, verification, and management. The approach standardizes heterogeneous document formats into VCs and applies automated data matching to detect inconsistencies, while the blockchain provides a decentralized trust layer that reinforces transparency and integrity. A prototype was developed that comprises (i) an OCR-NLP extraction pipeline trained on synthetic datasets, (ii) a backend for credential issuance and management, and (iii) a frontend supporting issuer, holder, and verifier interactions. Experimental results show that the models achieve competitive accuracy across multiple document types and that the end-to-end pipeline reduces verification time while preserving reliability. The proposed framework demonstrates the potential to streamline real estate transactions, strengthen stakeholder trust, and enable scalable, secure digital processes.

</details>


### [89] [Correctness of Extended RSA Public Key Cryptosystem](https://arxiv.org/abs/2512.24531)
*Dar-jen Chang,Suranjan Gautam*

Main category: cs.CR

TL;DR: 提出对 RSA 正确性形式化证明的替代方法，分析对模数 N 的取值扩展条件，给出哪些 N 值有效、哪些无效的明确条件，聚焦于 RSA-like 方案的正确性证明，不涉及安全性。


<details>
  <summary>Details</summary>
Motivation: 挑战现有对 RSA 正确性的常规证明，探究在更广泛的模数选择下仍能保证正确性的理论边界，以深化对 RSA 结构的形式化理解。

Method: 提出非传统的形式化证明框架，推导在 RSA-like 架构中对模数 N 的有效性条件，给出必要且充分的条件，并解释为何某些 N 不满足正确性。

Result: 给出对 N 的明确有效性条件，并解释为何其他取值会违背正确性要求，研究范围限定在数学正确性证明，排除安全性分析。

Conclusion: 在不考虑安全性的前提下，提出了关于 RSA-like 方案正确性的替代证明路径和对模数选取的更广泛理解，为后续形式化分析提供新视角。

Abstract: This paper proposes an alternative approach to formally establishing the correctness of the RSA public key cryptosystem. The methodology presented herein deviates slightly from conventional proofs found in existing literature. Specifically, this study explores the conditions under which the choice of the positive integer N, a fundamental component of RSA, can be extended beyond the standard selection criteria. We derive explicit conditions that determine when certain values of N are valid for the encryption scheme and explain why others may fail to satisfy the correctness requirements. The scope of this paper is limited to the mathematical proof of correctness for RSA-like schemes, deliberately omitting issues related to the cryptographic security of RSA.

</details>


### [90] [SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System](https://arxiv.org/abs/2512.24571)
*Md Hasan Saju,Austin Page,Akramul Azim,Jeff Gardiner,Farzaneh Abazari,Frank Eargle*

Main category: cs.CR

TL;DR: SynRAG 是一个跨 SIEM 的查询生成框架，能够从平台无关的规范自动生成各 SIEM 的特定查询，从单一高层规格实现跨平台威胁检测和事件调查，显著优于现有基模型。


<details>
  <summary>Details</summary>
Motivation: 大企业的 SIEM 生态多样（Qradar、SecOps、Splunk、Microsoft Sentinel、Elastic Stack 等），不同查询语言和架构造成分析负担和培训成本高，亟需统一的查询生成能力。

Method: 提出一个平台无关的高层规格，自动生成目标 SIEM 的专用查询。对比基于大语言模型的查询生成，使用 Qradar 与 SecOps 作为代表性 SIEM 进行评估，比较与 GPT、Llama、DeepSeek、Gemma、Claude 的输出效果。

Result: SynRAG 在跨 SIEM 的威胁检测与事件调查任务中生成的查询显著优于基线模型，且在 Qradar 与 SecOps 上表现更好。

Conclusion: 通过将查询生成从平台特定实现中解耦，降低分析师对多平台培训和手动翻译的依赖，促进在异构 SIEM 环境中的无缝威胁检测与调查。

Abstract: Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.

</details>


### [91] [SoK: Web3 RegTech for Cryptocurrency VASP AML/CFT Compliance](https://arxiv.org/abs/2512.24888)
*Qian'ang Mao,Jiaxin Wang,Ya Liu,Li Zhu,Jiaman Chen,Jiaqi Yan*

Main category: cs.CR

TL;DR: 提出并系统化Web3 RegTech的三大分类体系，基于对41家平台和28个学术原型的分析，揭示跨链、隐私与DeFi场景的合规挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 去中心化的Web3特性使传统监管技术难以适用，需要在分布式账本属性上构建新的合规工具和框架。

Method: 通过系统综述（2015–2025），分析41家商用平台与28个学术原型，提出三个税onomies: 1) 监管范式演进框架（10个维度），2) 合规协议五层核验分类，3) RegTech生命周期覆盖预防、实时、调查阶段。

Result: 证实Web3 RegTech具备交易图分析、实时风险评估、跨链分析、隐私保护的核验等能力，且在中心化系统中难以实现；同时揭示学术创新与产业部署之间的差距，以及跨链跟踪、DeFi分析、隐私协议监控、可扩展性等挑战。

Conclusion: 提出架构最佳实践，指明在保持去中心化、透明性与用户主权前提下的研究方向。

Abstract: The decentralized architecture of Web3 technologies creates fundamental challenges for Anti-Money Laundering and Counter-Financing of Terrorism compliance. Traditional regulatory technology solutions designed for centralized financial systems prove inadequate for blockchain's transparent yet pseudonymous networks. This systematization examines how blockchain-native RegTech solutions leverage distributed ledger properties to enable novel compliance capabilities.
  We develop three taxonomies organizing the Web3 RegTech domain: a regulatory paradigm evolution framework across ten dimensions, a compliance protocol taxonomy encompassing five verification layers, and a RegTech lifecycle framework spanning preventive, real-time, and investigative phases. Through analysis of 41 operational commercial platforms and 28 academic prototypes selected from systematic literature review (2015-2025), we demonstrate that Web3 RegTech enables transaction graph analysis, real-time risk assessment, cross-chain analytics, and privacy-preserving verification approaches that are difficult to achieve or less commonly deployed in traditional centralized systems.
  Our analysis reveals critical gaps between academic innovation and industry deployment, alongside persistent challenges in cross-chain tracking, DeFi interaction analysis, privacy protocol monitoring, and scalability. We synthesize architectural best practices and identify research directions addressing these gaps while respecting Web3's core principles of decentralization, transparency, and user sovereignty.

</details>


### [92] [MTSP-LDP: A Framework for Multi-Task Streaming Data Publication under Local Differential Privacy](https://arxiv.org/abs/2512.24899)
*Chang Liu,Junzhou Zhao*

Main category: cs.CR

TL;DR: 提出 MTSP-LDP 框架，通过在 w-event LDP 下进行多任务流式数据发布，动态预算分配、数据自适应二叉树、跨时间分组与平滑，以及预算自由的多任务处理，提升对复杂查询的隐私保护和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有 w-event 本地差分隐私在处理复杂查询和利用时间相关性方面的局限性，提升流式数据发布的实用性与估计准确性。

Method: 1) 使用最优隐私预算分配以分析时间相关性并在窗口内动态分配预算；2) 构建数据自适应的私有二叉树结构以支持复杂查询；3) 通过跨时间分组与平滑操作提升估计精度；4) 引入预算自由的多任务处理机制以支持多种流查询而不额外消耗隐私预算。

Result: 在真实世界数据集上的实验显示，MTSP-LDP 在多任务流式查询中显著优于现有方法，提供更高的效用。

Conclusion: MTSP-LDP 提供一种可扩展的多任务流数据发布框架，在遵循 w-event LDP 的前提下提升复杂查询的效用，具有较好的推广潜力和研究价值。

Abstract: The proliferation of streaming data analytics in data-driven applications raises critical privacy concerns, as directly collecting user data may compromise personal privacy. Although existing $w$-event local differential privacy (LDP) mechanisms provide formal guarantees without relying on trusted third parties, their practical deployment is hindered by two key limitations. First, these methods are designed primarily for publishing simple statistics at each timestamp, making them inherently unsuitable for complex queries. Second, they handle data at each timestamp independently, failing to capture temporal correlations and consequently degrading the overall utility. To address these issues, we propose MTSP-LDP, a novel framework for \textbf{M}ulti-\textbf{T}ask \textbf{S}treaming data \textbf{P}ublication under $w$-event LDP. MTSP-LDP adopts an \emph{Optimal Privacy Budget Allocation} algorithm to dynamically allocate privacy budgets by analyzing temporal correlations within each window. It then constructs a \emph{data-adaptive private binary tree structure} to support complex queries, which is further refined by cross-timestamp grouping and smoothing operations to enhance estimation accuracy. Furthermore, a unified \emph{Budget-Free Multi-Task Processing} mechanism is introduced to support a variety of streaming queries without consuming additional privacy budget. Extensive experiments on real-world datasets demonstrate that MTSP-LDP consistently achieves high utility across various streaming tasks, significantly outperforming existing methods.

</details>


### [93] [Towards Provably Secure Generative AI: Reliable Consensus Sampling](https://arxiv.org/abs/2512.24925)
*Yu Cui,Hang Fu,Sicheng Pan,Zhuoyu Sun,Yifei Liu,Yuhong Nie,Bo Ran,Baohan Huang,Xufeng Zhang,Haibin Zhang,Cong Zuo,Licheng Wang*

Main category: cs.CR

TL;DR: 提出 Reliable Consensus Sampling (RCS)，在避免 abstention 的前提下提升鲁棒性和效用，并提供理论风险可控性及动态安全改进的反馈算法，实验显示时延与 CS 相近但显著提升鲁棒性与效用。


<details>
  <summary>Details</summary>
Motivation: 现有生成式 AI 安全研究多以经验驱动的攻击与防御，容易产生未知攻击并频繁需要更新防护机制；因此需要可证明安全性与可控风险的机制来支撑长期安全性。

Method: 提出 Reliable Consensus Sampling (RCS)，通过将接受概率与潜在极端对手行为相关联以容忍攻击并提升鲁棒性，完全去除 abstention；并设计一个反馈算法来动态提升 RCS 的安全性，辅以理论保证风险维持在可控阈值。

Result: 理论上证明 RCS 可以维持可控的风险阈值；大量实验显示其在鲁棒性与实用性方面显著优于 CS，且延迟与 CS 相近。

Conclusion: 为可证明安全的生成式 AI 研究提供一种新的可控风险框架与实现路径，促进安全强鲁棒的应用发展。

Abstract: Existing research on generative AI security is primarily driven by mutually reinforcing attack and defense methodologies grounded in empirical experience. This dynamic frequently gives rise to previously unknown attacks that can circumvent current detection and prevention. This necessitates the continual updating of security mechanisms. Constructing generative AI with provable security and theoretically controllable risk is therefore necessary. Consensus Sampling (CS) is a promising algorithm toward provably secure AI. It controls risk by leveraging overlap in model output probabilities. However, we find that CS relies on frequent abstention to avoid unsafe outputs, which reduces utility. Moreover, CS becomes highly vulnerable when unsafe models are maliciously manipulated. To address these issues, we propose a new primitive called Reliable Consensus Sampling (RCS), that traces acceptance probability to tolerate extreme adversarial behaviors, improving robustness. RCS also eliminates the need for abstention entirely. We further develop a feedback algorithm to continuously and dynamically enhance the safety of RCS. We provide theoretical guarantees that RCS maintains a controllable risk threshold. Extensive experiments show that RCS significantly improves robustness and utility while maintaining latency comparable to CS. We hope this work contributes to the development of provably secure generative AI.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [94] [Economic and Technical Feasibility of V2G in Non-Road Mobile Machinery sector](https://arxiv.org/abs/2512.24101)
*Rößler Nicolas,Khan Irfan,Schade Thomas,Wellmann Christoph,Cao Xinyuan,Kopynske Milan,Xia Feihong,Savelsberg Rene,Andert Jakob*

Main category: eess.SY

TL;DR: 将车辆对电网（V2G）应用于非道路移动机械NRMM，结合贝叶斯优化（BO）优化能源基础设施和运行策略，评估经济性与技术可行性，提出以NRMM租赁服务为用例的潜在收入。


<details>
  <summary>Details</summary>
Motivation: 在NRMM高额待机容量情况下，利用其闲置时的电池容量参与电力市场，提供电网服务并创造额外收入。

Method: 引入将贝叶斯优化与运行策略优化结合的新方法，以优化能量基础设施并降低用电成本，同时增强与电网的互动。

Result: 论文聚焦于方法论，给出对电 NRMM 场景的潜在金融机会的初步分析；但受限于缺乏真实世界数据，尚未给出完整的实证结果。

Conclusion: 需要进一步扩展模型精度并验证发现，同时研究还需应对数据可得性、法规等挑战，未来工作包括更广泛的数据和现实世界验证。

Abstract: This paper investigates the economic and technical feasibility of integrating Vehicle-to-Grid (V2G) technology in the Non-Road Mobile Machinery (NRMM) sector. These often-idling assets, with their substantial battery capacities, present a unique opportunity to participate in energy markets, providing grid services and generating additional revenue. A novel methodology is introduced that integrates Bayesian Optimization (BO) to optimize the energy infrastructure together with an operating strategy optimization to reduce the electricity costs while enhancing grid interaction. While the focus lies on the methodology, the financial opportunities for the use-case of an electric NRMM rental service will be presented. However, the study is limited by the availability of real-world data on the usage of electric NRMM and does not address regulatory challenges of V2G. Further research is needed to extend the model accuracy and validate these findings.

</details>


### [95] [Hybrid Voltage and Current Control Method for Harmonic Mitigation of Single-Phase AC Loads in DC Microgrids](https://arxiv.org/abs/2512.24170)
*Mehdi Baharizadeh,Mohammad Sadegh Golsorkhi,Neda Keshavarzi,Thomas Ebel*

Main category: eess.SY

TL;DR: 提出一种混合电压与电流控制（HCM）方法用于直流微网中的 DER，实现直流电压控制与输出电流谐波控制的耦合抑制。通过内部电流环和外部控制层实现对直流分量和谐波分量的同时调节，且两者解耦，经频域分析验证无不良耦合，并通过硬件在环（HIL）测试验证时域响应。


<details>
  <summary>Details</summary>
Motivation: 直接连接直流微网的单相交流负载会导致其瞬时功率的振荡转化为直流侧谐波电流，增加损耗并引入直流母线电压谐波，需对来自单相负载的谐波进行有效抑制以提升系统效率和电压质量。

Method: 提出一个混合控制架构：内部电流环负责快速跟踪，外部控制层给内部环提供参考。外控层将直流电压控制与输出谐波电流控制结合，实现在同一 DER 的直流输出端对直流分量和谐波分量的耦合调节；通过频域分析证明直流电压环与谐波电流环解耦、无相互干扰；并通过硬件在环测试验证时域响应。

Result: 频域分析表明直流电压环与谐波电流环相互独立，不存在不希望的耦合。HIL 验证证实该方法在时域上具有良好响应，能够在保持直流电压稳态的同时抑制输出电流的谐波分量。

Conclusion: 所提出的 HCM 能同时实现直流电压的稳态控制与输出谐波电流的抑制，且两者解耦，适用于直流微网中需要同时处理直流稳态与单相负载谐波的场景，提升能效与电压质量。

Abstract: DC microgrids provide an efficient framework for the interconnection of DC distributed energy resources (DERs) and DC loads. To continue to supply legacy single-phase AC loads, DC/AC converters can be integrated in the DC microgrid. The oscillatory instantaneous power of the single-phase AC load translates into a harmonic current on the converter's DC side, which increases the losses and causes unwanted voltage harmonics in the DC microgrid. To mitigate this issue, this paper proposes a hybrid voltage and current control method (HCM) for DERs. This scheme consists of an inner current control loop and an outer control layer which determines the reference for the inner loop. The outer control layer combines the DC voltage control loop with an output harmonic current control loop. This hybrid structure enables simultaneous regulation of the DC components of the DER output voltage and control of the harmonic component of the DER output current in accordance with the local single-phase AC load's demand. Frequency-domain analysis of the proposed method is presented to demonstrate the DC voltage and harmonic current loops are decoupled and there is no unwanted interaction between them. Additionally, time-domain response of the proposed scheme is validated through hardware-in-the-loop test results.

</details>


### [96] [Now or Never: Continuous Surveillance AIoT System for Ephemeral Events in Intermittent Sensor Networks](https://arxiv.org/abs/2512.24179)
*Joonhee Lee,Kichang Lee,Jeonggil Ko*

Main category: eess.SY

TL;DR: 通过能量感知的弹性分布式计算，将任务动态下沉到能量较充足的邻居节点，实现对资源受限的无人值守AIoT节点的连续感知，提升野外监测的可用性和事件捕获率。


<details>
  <summary>Details</summary>
Motivation: 在广袤且难以覆盖的野外监测场景中，传感器常因能源受限而需要频繁休眠，导致对瞬时、“现在-就要”事件（如野生动植物声音、森林火情等）的检测丢失，形成可用性缺口。

Method: 提出一种能量感知的弹性分布式计算算法（Elastic Split Computing），通过在能量丰富的邻居节点之间动态分割并下沉计算任务，优先保障连续感知，减少能源缓冲带来的计算吞吐偏好。

Result: 初步结果显示，该算法实现了对额外2,496平方米区域的稳定监测，并每日额外捕捉大约103个关键事件。

Conclusion: 该算法为在资源受限的节点上构建鲁棒、故障自愈的监控系统奠定基础，提升野外AIoT监控的可用性和可靠性。

Abstract: Wilderness monitoring tasks, such as poaching surveillance and forest fire detection, require pervasive and high-accuracy sensing. While AIoT offers a promising path, covering vast, inaccessible regions necessitates the massive deployment of maintenance-free, battery-less nodes with limited computational resources. However, these constraints create a critical `Availability Gap.' Conventional intermittent operations prioritize computation throughput, forcing sensors to sleep during energy buffering. Consequently, systems miss ephemeral, `now-or-never' events (e.g., Vocalizations of natural monuments or Fire), which is fatal for detecting rare but high-stakes anomalies. To address this, we propose an Energy-aware Elastic Split Computing Algorithm that prioritizes continuous sensing by dynamically offloading tasks to energy-rich neighbors. Preliminary results demonstrate stable monitoring of an additional $2,496\;\text{m}^2$ and the capture of approximately 103 more critical events per day. Ultimately, this algorithm establishes a robust foundation for building resilient, fail-safe surveillance systems even on resource-constrained nodes.

</details>


### [97] [New Insights into Cascaded Geometric Flight Control: From Performance Guarantees to Practical Pitfalls](https://arxiv.org/abs/2512.24377)
*Brett T. Lopez*

Main category: eess.SY

TL;DR: 提出一种新的级联几何控制稳定性证明，应用于无人机跟踪时变位置轨迹，结合滑变量与基于四元数的滑模控制器，理论上证明位置轨迹具有指数收敛；同时揭示姿态误差对位置环的耦合、模型不确定性对闭环系统的影响，以及控制结构的实际陷阱。


<details>
  <summary>Details</summary>
Motivation: 在无人机等飞行平台上，对级联几何控制的稳定性进行严格证明，特别是面对时变目标轨迹、系统耦合以及不确定性时，需求明确的收敛性和鲁棒性界限，并披露实现中的潜在问题。

Method: 采用滑变量框架并结合最近提出的基于四元数的滑模控制器，通过构造李雅普诺夫函数或等效稳定性分析，推导出位置环在满足一定条件下的指数收敛，并分析姿态环误差对位置环的耦合效应、模型不确定性的影响，以及控制架构的实践性问题。

Result: 理论上证明了在给定条件下，位置轨迹跟踪呈指数收敛；揭示了姿态误差对位置环的耦合机制；给出模型不确定性对闭环系统的鲁棒性边界与敏感性分析；并指出实现中的实际陷阱，如滑模控制的抖动、噪声影响和参数选择对稳定性的关键性。

Conclusion: 该工作提供一个理论上严格且具洞察力的稳定性证明框架，强化级联几何控制在飞行器对时变轨迹追踪中的可行性，并明确了设计与实现中的关键注意事项和改进方向。

Abstract: We present a new stability proof for cascaded geometric control used by aerial vehicles tracking time-varying position trajectories. Our approach uses sliding variables and a recently proposed quaternion-based sliding controller to demonstrate that exponentially convergent position trajectory tracking is theoretically possible. Notably, our analysis reveals new aspects of the control strategy, including how tracking error in the attitude loop influences the position loop, how model uncertainties affect the closed-loop system, and the practical pitfalls of the control architecture.

</details>


### [98] [Bayesian Subspace Identification in the MIMO Case](https://arxiv.org/abs/2512.24435)
*Alexandre Rodrigues Mesquita*

Main category: eess.SY

TL;DR: 将贝叶斯子空间系统辨识扩展到MIMO，提出等变先验和后验分布，并使用DAISY数据集验证。


<details>
  <summary>Details</summary>
Motivation: 解决在MIMO情形下应用贝叶斯子空间辨识的扩展挑战，提供适用于多输入多输出系统的新型等变先验与后验分布。

Method: 推导并给出适用于MIMO的等变先验与后验分布，基于贝叶斯子空间辨识框架，结合DAISY数据集进行数值验证。

Result: 在DAISY数据集上的数值结果验证所提方法的有效性，显示与现有方法的对比优势或竞争力。

Conclusion: 将贝叶斯子空间辨识扩展到MIMO，提出新的先验与后验推断框架，实验结果支持方法的可行性与潜在优势。

Abstract: This report investigates the extension of the Bayesian Subspace System Identification method proposed in our previous work to the Multiple-Input Multiple-Output (MIMO) case. We derive new equivariant priors and posterior distributions specifically suited for the MIMO framework. Numerical results utilizing the DAISY dataset are reported to validate the approach.

</details>


### [99] [Multipliers for forced Lurye systems with slope-restricted nonlinearities](https://arxiv.org/abs/2512.24453)
*William Paul Heath,Sayar Das,Joaquin Carrasco*

Main category: eess.SY

TL;DR: 动态乘子可以在带斜率限制的Lurie系统中界定闭环功率增益并量化，但不能保证有限的增量增益；对周期激励，闭环响应可能出现次谐波或混沌。乘子需要对偶个性（奇性）有所约束，且其相位限制来自离散时间乘子。需要圆判据辅助才能在所有频率上使用。


<details>
  <summary>Details</summary>
Motivation: 提供一个框架来评估带斜率限制非线性的Lurie系统的稳定性与性能，特别是闭环功率增益的界定与噪声鲁棒性，并探索增稳理论在增量稳定性与偏置测量下的扩展。弥补仅有动态乘子保证稳定性但无增量增益界的不足，以及在实际应用中对非奇性非线性与偏置的处理。

Method: 回顾并改进一类动态乘子，利用经典工具推导乘子、分析其相位约束与适用性；在合适的偏置项周围测量功率，讨论非奇性非线性条件下的可实现性；结合圆判据的使用限制，讨论在不同频率下的适用性。

Result: 证明动态乘子能保证闭环功率增益有界且可量化；若对其它外激信号保持稳态，乘子可实现对噪声的低灵敏度；对周期激励，闭环可能出现次谐波或混沌；某些乘子族能确保唯一、吸引且保持周期的解；相位限制来自离散时间乘子的性质；在没有圆判据的情形下不能在所有频率下使用。

Conclusion: 动态乘子在处理带斜率限制非线性的Lurie系统时，适合用于得到功率增益界与噪声鲁棒性，但存在不可避免的相位/适用性限制及对增量稳定性的不足，需要结合圆判据等辅助条件并注意偏置处理与非奇性假设。

Abstract: Dynamic multipliers can be used to guarantee the stability of Lurye systems with slope-restricted nonlinearities, but give no guarantee that the closed-loop system has finite incremental gain. We show that multipliers guarantee the closed-loop power gain to be bounded and quantifiable. Power may be measured about an appropriate steady state bias term, provided the multiplier does not require the nonlinearity to be odd. Hence dynamic multipliers can be used to guarantee such Lurye systems have low sensitivity to noise, provided other exogenous signals have constant steady state. For periodic excitation, the closed-loop response can apparently have a subharmonic or chaotic response. We revisit a class of multipliers that can guarantee a unique, attractive and period-preserving solution. We show the multipliers can be derived using classical tools and reconsider assumptions required for their application. Their phase limitations are inherited from those of discrete-time multipliers. The multipliers cannot be used at all frequencies unless the circle criterion can also be applied; this is consistent with known results about dynamic multipliers and incremental stability.

</details>


### [100] [Design of Linear Residual Generators for Combined Fault Detection and Estimation in Nonlinear Systems](https://arxiv.org/abs/2512.24484)
*Sunjeev Venkateswaran,Costas Kravaris*

Main category: eess.SY

TL;DR: 设计一个面向非线性系统的线性残差发生器的系统性方法，通过扩展模型中的故障动态并实现对扰动的解耦，给出存在性必要充足条件及显式设计公式，并在化工反应器案例中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在非线性系统中实现故障检测与估计具有挑战性，需兼顾扰动解耦与可操性（显式的合成公式），因此通过扩展的线性外生系统整合故障动态来构造线性泛函观测器。

Method: 为包含故障动力学的扩展系统构造线性泛函观测器，设计目标是实现残差解耦扰动并用于故障检测与估计；给出存在性必要充分条件并据此推导显式的残差发生器设计公式。

Result: 给出了存在性条件（必要且充分），并提供显式的设计公式；通过化学反应器案例验证所提方法的有效性。

Conclusion: 该方法提供一个系统且可操作的框架，用于在非线性系统中实现故障检测与估计的线性残差生成，且在化工案例中得到有效验证。

Abstract: A systematic method for the design of linear residual generators for combined fault detection and estimation in nonlinear systems is developed. The proposed residual generator is a linear functional observer built for an extended system that incorporates the fault dynamics from a linear exo-system, and in addition possesses disturbance-decoupling properties. Necessary and sufficient conditions for the existence of such residual generators for nonlinear systems are derived. As long as these conditions are satisfied, we obtain explicit design formulas for the residual generator. The results are illustrated through a chemical reactor case study, which demonstrates the effectiveness of the proposed methodology.

</details>


### [101] [Energy-Aware Bayesian Control Barrier Functions for Physics-Informed Gaussian Process Dynamics](https://arxiv.org/abs/2512.24493)
*Chi Ho Leung,Philip E. Paré*

Main category: eess.SY

TL;DR: 提出一个基于贝叶斯CBF的EB-CBF框架，用于GP学习的哈密顿系统的能量安全性，在能量约束下给出高概率安全保证，且对原控制器的改动尽可能小。通过从哈密顿量和向量场后验构造保守的能量障碍函数并实现安全滤波器，在质量-弹簧系统的数值仿真中，在带噪声的GP动力学下实现高概率安全。


<details>
  <summary>Details</summary>
Motivation: 在连续时间动力系统中，动力学通过高斯过程学习后，如何在能量约束的自然表达下实现安全控制成为关键问题。GP哈密顿后验提供了利用结构信息的机会，但需要系统地将不确定性纳入安全约束以给出高概率保证。

Method: 提出一个贝叶斯CBF框架，并具体实现为能量感知的贝叶斯CBF（EB-CBFs）。该方法直接从哈密顿量和向量场的后验中构造保守的、能量相关的安全障碍函数，并据此设计安全过滤器，使得对原有控制器的改动仅在必要时发生，且能提供概率意义上的能量安全保证。通过数值仿真（质量-弹簧系统）验证在带噪声的GP学习动力学下的高概率安全性。

Result: 在模拟中，EB-CBFs在含有噪声的GP动力学下实现了高概率安全，且对原控制器的干扰较小；证明了利用能量结构来设计CBF能提升安全性与鲁棒性。

Conclusion: 该工作将能量约束自然融入贝叶斯CBF框架，得到能量感知的保守性和概率安全性保证，适用于机械/端口-哈密顿系统等通过能量表达安全约束的情形。未来可扩展到更复杂的多体系统、不确定性建模的细化，以及对实际控制器实现的实时性分析。

Abstract: We study safe control for dynamical systems whose continuous-time dynamics are learned with Gaussian processes (GPs), focusing on mechanical and port-Hamiltonian systems where safety is naturally expressed via energy constraints. The availability of a GP Hamiltonian posterior naturally raises the question of how to systematically exploit this structure to design an energy-aware control barrier function with high-probability safety guarantees. We address this problem by developing a Bayesian-CBF framework and instantiating it with energy-aware Bayesian-CBFs (EB-CBFs) that construct conservative energy-based barriers directly from the Hamiltonian and vector-field posteriors, yielding safety filters that minimally modify a nominal controller while providing probabilistic energy safety guarantees. Numerical simulations on a mass-spring system demonstrate that the proposed EB-CBFs achieve high-probability safety under noisy sampled GP-learned dynamics.

</details>


### [102] [Decentralized No-Regret Frequency-Time Scheduling for FMCW Radar Interference Avoidance](https://arxiv.org/abs/2512.24619)
*Yunian Pan,Jun Li,Lifan Xu,Shunqiao Sun,Quanyan Zhu*

Main category: eess.SY

TL;DR: 提出 Time-Frequency No-Regret Hopping（TF-NRH）框架，用以在密集汽车FMCW雷达中通过分布式时频博弈实现干扰规避。雷达在频段子带与 chirp 时间偏移上进行混合策略更新，基于遗憾最小化，收敛至近似协同行为，提升系统的鲁棒性与时频分辨能力。


<details>
  <summary>Details</summary>
Motivation: 汽车雷达密度增加导致互干扰显著，现有抑制手段在可扩展性、对侧信道依赖及时-频分辨损失方面存在局限。基于去中心化的 No-Regret 跳变，需统一的时频自适应框架来跨越时域与频谱资源的干扰规避。

Method: 将干扰规避建模为重复的反协调博弈，雷达通过遗憾最小化动态在频段子带和 chirp 时间偏移上更新混合策略。提出 Time-Frequency No-Regret Hopping 算法，证明外部遗憾与交换遗憾趋于0，经验博弈收敛至 ε-粗相关均衡或相关均衡。给出联合域遗憾界，揭示时间自适应如何正则化频率选择并提升对异步干扰的鲁棒性。

Result: 理论上给出联合时频域的遗憾界与收敛性分析；数值实验在多雷达场景中相较时间-频率随机跳变与集中 Nash 基准，显著提升 SINR、碰撞率与时-频分辨质量。

Conclusion: 提供一种可扩展且鲁棒的干扰规避框架，利用时频自适应提升密集雷达环境下的性能，未来工作可扩展至现实场景实测以及对更异构干扰的适应。

Abstract: Automotive FMCW radars are indispensable to modern ADAS and autonomous-driving systems, but their increasing density has intensified the risk of mutual interference. Existing mitigation techniques, including reactive receiver-side suppression, proactive waveform design, and cooperative scheduling, often face limitations in scalability, reliance on side-channel communication, or degradation of range-Doppler resolution. Building on our earlier work on decentralized Frequency-Domain No-Regret hopping, this paper introduces a unified time-frequency game-theoretic framework that enables radars to adapt across both spectral and temporal resources. We formulate the interference-avoidance problem as a repeated anti-coordination game, in which each radar autonomously updates a mixed strategy over frequency subbands and chirp-level time offsets using regret-minimization dynamics. We show that the proposed Time-Frequency No-Regret Hopping algorithm achieves vanishing external and swap regret, and that the induced empirical play converges to an $\varepsilon$-coarse correlated equilibrium or a correlated equilibrium. Theoretical analysis provides regret bounds in the joint domain, revealing how temporal adaptation implicitly regularizes frequency selection and enhances robustness against asynchronous interference. Numerical experiments with multi-radar scenarios demonstrate substantial improvements in SINR, collision rate, and range-Doppler quality compared with time-frequency random hopping and centralized Nash-based benchmarks.

</details>


### [103] [Taking Advantage of Rational Canonical Form for Faster Ring-LWE based Encrypted Controller with Recursive Multiplication](https://arxiv.org/abs/2512.24658)
*Donghyeon Song,Yeongjun Jang,Joowon Lee,Junsoo Kim*

Main category: eess.SY

TL;DR: 通过将状态矩阵转化为有理标准型并对输入输出进行打包，该论文提出在Ring-LWE加密下的递归乘法的高效实现，显著降低同态运算的时间与空间开销。


<details>
  <summary>Details</summary>
Motivation: 解决在同态加密下实现线性动态控制器时的高计算成本和内存需求，使得加密控制器更具实际可行性。

Method: 将状态矩阵转化为有理标准型，利用其稀疏与循环结构仅对非平凡列进行加密与计算；提出将输入输出矩阵打包成单一多项式的策略，减少同态运算。

Result: 仿真结果表明所提设计在 encrypted 控制器实现方面具有显著的加速效果。

Conclusion: 通过结构化矩阵变换和打包技术，提升了 Ring-LWE 基础的加密控制器的计算效率和可扩展性。

Abstract: This paper aims to provide an efficient implementation of encrypted linear dynamic controllers that perform recursive multiplications on a Ring-Learning With Errors (Ring-LWE) based cryptosystem. By adopting a system-theoretical approach, we significantly reduce both time and space complexities, particularly the number of homomorphic operations required for recursive multiplications. Rather than encrypting the entire state matrix of a given controller, the state matrix is transformed into its rational canonical form, whose sparse and circulant structure enables that encryption and computation are required only on its nontrivial columns. Furthermore, we propose a novel method to ``pack'' each of the input and the output matrices into a single polynomial, thereby reducing the number of homomorphic operations. Simulation results demonstrate that the proposed design enables a remarkably fast implementation of encrypted controllers.

</details>


### [104] [Waste-to-Energy-Coupled AI Data Centers: Cooling Efficiency and Grid Resilience](https://arxiv.org/abs/2512.24683)
*Qi He,Chunyu Qu*

Main category: eess.SY

TL;DR: 通过整合废物转能的热能输出与吸收式制冷，提出一种 Waste-to-Energy-AI 数据中心（WtE-AIDC）耦合框架，以在电网压力下提升能效和经济性，且以LCOC与ESG视角进行可行性评估。


<details>
  <summary>Details</summary>
Motivation: 在数据中心扩张背景下，电力与制冷能力的耦合制约日益凸显。现有模式将制冷视为电力负担，本文提出将冷却作为第一类能源服务的集成系统，以提升热-冷耦合的整体经济与能源绩效。

Method: 将耦合系统建模为具有透明边界的输入-输出“黑箱”，并构建与之对照的基线（以机械制冷由电网供电）。核心机制是能量等级匹配：低级别的废物焚烧热输出驱动吸收式制冷以提供冷却服务，从而替代基线冷却用电。通过三大一阶决定因素分析热经济性，并给出一个可计算的 Levelized Cost of Computing (LCOC) 原型，以及基于可衡量机制的 ESG 估值通道，同时给出城市走廊距离下的站点可行性条件。

Result:  thermoeconomic superiority 由三大一阶决定因素支配：(1) IT 热负荷的冷却覆盖程度，(2) 输送与辅助装置的寄生电力，(3) 距离引发的配送衰减；存在一个超过该走廊距离后净收益消失的“盈亏平衡区间”。通过比较静态分析研究对 IT 使用率、原料质量（废弃物低位热值与通量）、气候参数化及走廊距离的敏感性。将会计增益转化为可计算的 LCOC 和以可衡量机制为基础的 ESG 估值渠道，无需重新推导完整生命周期清单。框架提供在高网压环境下面向城市 AI 走廊的 WtE-AIDC 耦合可行性条件。

Conclusion: 该框架可为在受网电压力的城市 AI 走廊中部署 WtE 与数据中心的耦合提供可落地的可行性条件，并通过 LCOC 与 ESG 视角实现经济与环境收益的可量化表达。

Abstract: AI data-center expansion is increasingly constrained by the coupled availability of deliverable electricity and heat-rejection (cooling) capacity. We propose and evaluate an integrated Waste-to-Energy-AI Data Center configuration that treats cooling as a first-class energy service rather than an unavoidable electricity burden. The coupled system is modeled as an input-output 'black box' with transparent boundaries and a standalone benchmark in which mechanical chilling is powered by grid electricity. The central mechanism is energy-grade matching: low-grade WtE thermal output drives absorption cooling to deliver chilled service, thereby displacing baseline cooling electricity. We show that thermoeconomic superiority is governed by three first-order determinants, (i) cooling coverage of IT heat load, (ii) parasitic electricity for transport and auxiliaries, and (iii) distance-driven delivery decay, yielding a break-even corridor beyond which net benefits vanish. Comparative statics characterize sensitivity to IT utilization, feedstock quality (waste LHV and throughput), climate parameterization, and corridor distance. We translate these accounting gains into decision language through a computable prototype for Levelized Cost of Computing (LCOC) and an ESG valuation channel grounded in measurable mechanisms, without re-deriving full lifecycle inventories. The framework provides siting-ready feasibility conditions for WtE-AIDC coupling in urban AI corridors under grid stress.

</details>


### [105] [Average Consensus with Dynamic Quantization Framing and Finite-Time Termination over Limited-Bandwidth Directed Networks](https://arxiv.org/abs/2512.24700)
*Evagoras Makridis,Gabriele Oliva,Apostolos I. Rikos,Themistoklis Charalambous*

Main category: eess.SY

TL;DR: PP-ACDC is a deterministic distributed algorithm for exact average consensus over strongly connected digraphs with a fixed, low number of quantization bits. It combines Push-Pull (surplus) consensus with a dynamic quantization framing (zooming + midpoint shifting) to preserve the true average while refining precision. It provides rigorous convergence to exact average, a fully distributed finite-time termination, and ε-convergence detection, with simulations showing efficiency on resource-constrained directed networks.


<details>
  <summary>Details</summary>
Motivation: To enable exact average consensus in directed networks under strict communication constraints by using a fixed, small bit budget and maintaining exactness through a dynamic quantization strategy.

Method: Integrates Push-Pull (surplus) consensus dynamics with a dynamic quantization framing scheme that periodically tightens quantization (zooming) and shifts the quantization midpoint (midpoint shifting). The scheme ensures the global average is preserved and refined across iterations. A fully distributed termination mechanism is devised, and conditions for ε-convergence within finite iterations are proven.

Result: Proves asymptotic (exact) average consensus on any strongly connected digraph under proper quantization parameters. Establishes a distributed finite-time termination method and finite-iteration ε-convergence detection. Numerical simulations validate reliability, communication efficiency, and precision under tight bit budgets for large-scale, directed networks.

Conclusion: PP-ACDC offers a rigorous, resource-efficient solution for exact average consensus in directed graphs with limited communication bits, combining surplus-based consensus with adaptive quantization to achieve convergence and practical termination in distributed settings.

Abstract: This paper proposes a deterministic distributed algorithm, referred to as PP-ACDC, that achieves exact average consensus over possibly unbalanced directed graphs using only a fixed and a priori specified number of quantization bits. The method integrates Push-Pull (surplus) consensus dynamics with a dynamic quantization framing scheme combining zooming and midpoint shifting, enabling agents to preserve the true global average while progressively refining their quantization precision. We establish a rigorous convergence theory showing that PP-ACDC achieves asymptotic (exact) average consensus on any strongly connected digraph under appropriately chosen quantization parameters. Moreover, we develop a fully distributed and synchronized finite-time termination mechanism, and we provide a formal proof on the detection of $ε$-convergence to the average within a finite number of iterations. Numerical simulations corroborate the theoretical results and demonstrate that PP-ACDC achieves reliable, communication-efficient, and precise average consensus even under very tight bit budgets, underscoring its suitability for large-scale and resource-constrained multi-agent systems operating over directed networks.

</details>


### [106] [Trustworthy Equipment Monitoring via Cascaded Anomaly Detection and Thermal Localization](https://arxiv.org/abs/2512.24755)
*Sungwoo Kang*

Main category: eess.SY

TL;DR: 级联异常检测框架：在预测性维护中，传感器时间序列的检测优于多模态融合，提供可解释性以揭示模态偏置。


<details>
  <summary>Details</summary>
Motivation: 解决在预测性维护中，单模态传感器检测与多模态融合之间的性能对比，以及如何提供可解释性来支持维护决策。

Method: Stage1：基于LSTM的传感器编码器+时间注意力进行高准确度检测；Stage2：基于CNN的热成像编码器进行检测后定位；可解释性：整合SHAP、时空注意力与门控权重分析，揭示模态偏置。

Result: 传感器单模态检测比全融合多出8.3个百分点（93.08% vs 84.79% F1）。在78,397样本的真实 bearing 数据集上实现了先进精度；融合模型对热模态赋权65-87%，证实了“模态偏置”现象。

Conclusion: 分级式检测-定位框架在准确性与可解释性方面优于简单融合，避免盲目多模态融合，提供可执行的维护诊断。

Abstract: Predictive maintenance demands accurate anomaly detection and trustable explanations. Although multimodal fusion of sensor time-series and thermal imagery shows promise, we demonstrate that naive fusion strategies can paradoxically degrade performance. This paper introduces a Cascaded Anomaly Detection framework that decouples detection and localization. Stage 1 employs an LSTM-based sensor encoder with temporal attention for high-accuracy detection, while Stage 2 activates a CNN-based thermal encoder for post-detection fault localization. Our results reveal that sensor-only detection outperforms full fusion by 8.3 percentage points (93.08% vs. 84.79% F1-score), challenging the assumption that additional modalities invariably improve performance. We further contribute an explainability pipeline integrating SHAP, temporal/spatial attention, and gate weight analysis. This analysis uncovers a "modality bias" where fusion models assign 65-87% weight to the weaker thermal modality. Validated on a real-world bearing dataset (78,397 samples), our cascaded approach achieves state-of-the-art accuracy while providing actionable diagnostics for maintenance decision-making.

</details>


### [107] [Heterogeneous Multi-Agent Multi-Target Tracking using Cellular Sheaves](https://arxiv.org/abs/2512.24886)
*Tyler Hanks,Cristian F. Nino,Joana Bou Barcelo,Austin Copeland,Warren Dixon,James Fairbanks*

Main category: eess.SY

TL;DR: 使用细胞层理论处理异质多智能体追踪问题，提出将跟踪建模为细胞层上的谐扩问题，并通过层析拉普拉斯实现去中心化控制，给出Lyapunov稳定性分析并通过仿真验证收敛。


<details>
  <summary>Details</summary>
Motivation: 传统的图拉普拉斯方法难以直接处理状态维度不一致、非线性动力学及多目标/非合作设置中的异质性。需要一个能原生表达异质系统、非线性扰动和未知目标的通用框架。

Method: 引入细胞层作为异质多智能体系统的通用建模工具，将多目标跟踪问题转化为细胞层上的谐扩问题；基于细胞层拉普拉斯设计去中心化控制律，并给出Lyapunov分析以保证跟踪误差收敛；通过仿真验证方法有效性。

Result: 仿真结果支持在含有非线性动态和外部扰动的情况下，系统实现跟踪误差收敛。

Conclusion: 将协调型资源（如共识）框架扩展到非合作目标跟踪，利用细胞层天然处理异质性和不同维度的优势，提供了一个稳定的去中心化解决方案。

Abstract: Multi-agent target tracking in the presence of nonlinear dynamics and agent heterogeneity, where state-space dimensions may differ, is a challenging problem that traditional graph Laplacian methods cannot easily address. This work leverages the framework of cellular sheaves, a mathematical generalization of graph theory, to natively model such heterogeneous systems. While existing coordination sheaf frameworks focus on cooperative problems like consensus, this work extends them to the non-cooperative target-tracking problem. The tracking of multiple, unknown targets is formulated as a harmonic extension problem on a cellular sheaf, accommodating nonlinear dynamics and external disturbances for all agents. A decentralized control law is developed using the sheaf Laplacian, and a corresponding Lyapunov-based stability analysis is provided to guarantee tracking error convergence, with results validated by simulation.

</details>


### [108] [One-Shot Camera-Based Extrusion Optimization for High Speed Fused Filament Fabrication](https://arxiv.org/abs/2512.24905)
*Yufan Lin,Xavier Guidetti,Yannick Nagel,Efe C. Balta,John Lygeros*

Main category: eess.SY

TL;DR: 基于手机拍照的一-shot标定与模型驱动的最优控制，硬件最小化地提升FDM 3D打印的高速度成形质量，达到3600 mm/min时的表面质量可媲美1600 mm/min的传统打印。


<details>
  <summary>Details</summary>
Motivation: 解决消费级FDM打印机在高速打印时因挤出与运动不同步、尤其转角过度挤出而导致的质量下降。现有方法依赖专用硬件、复杂标定或固件修改，不易普及。

Method: 通过两种简单的印刷模式和手机拍摄的一-shot标定，识别挤出动态与转角行为，建立模型；设计带约束的最优控制以生成经过优化的G-code，使打印头运动与挤出流量同步，从而提升高速度打印的质量。

Result: 在3600 mm/min的扫描/线宽场景中，宽度跟踪误差显著降低，转角缺陷减少，表面粗糙度降低；实现速度提升一倍（3600 mm/min）时的表面质量接近传统1600 mm/min的水平。

Conclusion: 提供一种可广泛使用、硬件最小化的端到端优化框架，使标准FDM打印机在不需要额外硬件或固件修改的前提下实现更高速度与稳定质量。

Abstract: Off-the-shelf fused filament fabrication 3D printers are widely accessible and convenient, yet they exhibit quality loss at high speeds due to dynamic mis-synchronization between printhead motion and material extrusion systems, notably corner over-extrusion. Existing methods require specialized hardware, extensive calibration, or firmware modifications that are inaccessible to most users. This work presents a practical, end-to-end optimization framework that enhances high-speed printing using only standard 3D printers and a phone camera, without requiring additional complex setup. The method employs a one-shot calibration approach in which two simple printed patterns, captured by a phone camera, enable identification of extrusion dynamics and cornering behavior. The identified systems enable a model-based constrained optimal control strategy that generates optimized G-code, synchronizing motion and extrusion. Experiments show reduced width tracking error, mitigated corner defects, and lower surface roughness, achieving surface quality at 3600 mm/min comparable to conventional printing at 1600 mm/min, effectively doubling production speed while maintaining print quality. This accessible, hardware-minimal approach enables a wide range of fused filament fabrication users to achieve high-quality, high-speed additive manufacturing.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [109] [Beamforming for Massive MIMO Aerial Communications: A Robust and Scalable DRL Approach](https://arxiv.org/abs/2512.23902)
*Hesam Khoshkbari,Georges Kaddoum,Omid Abbasi,Bassant Selim,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a distributed beamforming framework for a constellation of airborne platform stations (APSs) in a massive Multiple-Input and Multiple-Output (MIMO) non-terrestrial network (NTN) that targets the downlink sum-rate maximization under imperfect local channel state information (CSI). We propose a novel entropy-based multi-agent deep reinforcement learning (DRL) approach where each non-terrestrial base station (NTBS) independently computes its beamforming vector using a Fourier Neural Operator (FNO) to capture long-range dependencies in the frequency domain. To ensure scalability and robustness, the proposed framework integrates transfer learning based on a conjugate prior mechanism and a low-rank decomposition (LRD) technique, thus enabling efficient support for large-scale user deployments and aerial layers. Our simulation results demonstrate the superiority of the proposed method over baseline schemes including WMMSE, ZF, MRT, CNN-based DRL, and the deep deterministic policy gradient (DDPG) method in terms of average sum rate, robustness to CSI imperfection, user mobility, and scalability across varying network sizes and user densities. Furthermore, we show that the proposed method achieves significant computational efficiency compared to CNN-based and WMMSE methods, while reducing communication overhead in comparison with shared-critic DRL approaches.

</details>


### [110] [A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe](https://arxiv.org/abs/2512.23906)
*Wendong Yao,Binhua Huang,Soumyabrata Dev*

Main category: eess.SP

TL;DR: 提出一个多模态补丁Transformer用于近步次时刻的位移NOWCAST，显著优于基线，能对EGMS时间序列的下一观测提供高精度预测。


<details>
  <summary>Details</summary>
Motivation: 近实时区域尺度地面变形监测对于城市规划、关键基础设施管理和自然灾害缓解越来越重要，但由于长期趋势、季节性循环、突变性事件（如地震步骤）叠加以及强烈的空间异质性，预测下一观测仍具挑战性。

Method: 提出一个多模态补丁式Transformer，用于单步、固定间隔的下一时刻位移nowcasting，输入为64x64网格、覆盖100kmx100km区域的EGMS时间序列。模型摄取最近的位移快照，以及(i) 静态运动学指标（均速度、加速度、季节幅值），这些指标在训练窗内以泄漏安全方式计算，(ii) 日序列的谐波编码。评估在爱尔兰东部的 tile E32N34，比较在相同多模态输入下的CNN-LSTM、CNN-LSTM+Attn、以及多模态STGCN等基线，指标包括RMSE和R^2。

Result: 在东部爱尔兰 tile E32N34 上，受控于多模态输入的多模态Transformer明显优于其他模型（在所有模型输入相同的情况下）。该模型实现 RMSE = 0.90 mm、R^2 = 0.97，且在阈值准确性方面达到最佳表现。与仅在位移输入下表现最强的STGCN相比，优势体现在多模态输入条件下的综合预测性能。

Conclusion: 该方法有效整合多模态信息，提升近实时预测精度，适用于区域尺度地面变形监测与风险评估的实际应用；未来可进一步探索更大尺度或更复杂地形下的鲁棒性，以及对其他地表过程的迁移能力。

Abstract: Near-real-time regional-scale monitoring of ground deformation is increasingly required to support urban planning, critical infrastructure management, and natural hazard mitigation. While Interferometric Synthetic Aperture Radar (InSAR) and continental-scale services such as the European Ground Motion Service (EGMS) provide dense observations of past motion, predicting the next observation remains challenging due to the superposition of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a multimodal patch-based Transformer for single-step, fixed-interval next-epoch nowcasting of displacement maps from EGMS time series (resampled to a 64x64 grid over 100 km x 100 km tiles). The model ingests recent displacement snapshots together with (i) static kinematic indicators (mean velocity, acceleration, seasonal amplitude) computed in a leakage-safe manner from the training window only, and (ii) harmonic day-of-year encodings. On the eastern Ireland tile (E32N34), the STGCN is strongest in the displacement-only setting, whereas the multimodal Transformer clearly outperforms CNN-LSTM, CNN-LSTM+Attn, and multimodal STGCN when all models receive the same multimodal inputs, achieving RMSE = 0.90 mm and $R^2$ = 0.97 on the test set with the best threshold accuracies.

</details>


### [111] [Movable Antenna Enhanced Multi-Region Beam Coverage: A Multi-Notch-Filter-Inspired Design](https://arxiv.org/abs/2512.24090)
*Dong Wang,Weidong Mei,Zhi Chen,Boyu Ning*

Main category: eess.SP

TL;DR: 通过移动天线实现广覆盖：将发射波束成形与天线位置联合优化，以极大化目标子区域的最小波束增益；基于多 notch 滤波设计的思想，给出连续幅相的 MNF-型波束向量与逐步更新结合 Gibbs 采样的选择步骤，显著优于定位置天线并接近 AO 但复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 移动天线通过位置自由度提供新的性能提升，但覆盖多个目标子区域的优化问题高度非凸，且传统方法计算复杂性高。需高效算法在给定区域内实现尽量均衡的增益分布。

Method: 将空间 MNF 的观念应用于波束形成，假设天线移动区域内具有连续的幅值和相位分布，构造空间 MNF-型发射波束向量。基于该连续分布，提出逐步更新算法在多区域覆盖中选择最优 MA 位置信息子集，并结合 Gibbs 采样以避免局部最优。

Result: 仿真结果显示所提算法在多区域覆盖下显著优于传统固定位置天线（FPA），并能与交替优化（AO）算法的性能相当，同时显著降低复杂度。

Conclusion: 以 MNF-启发的波束成形策略实现可观的跨区域覆盖性能提升，提供一种高效的 MA 联合位置与波束优化解法，兼具优良性能与低复杂度。

Abstract: Movable antenna (MA) has emerged as a promising technology to enhance wireless communication performance by exploiting the new degree of freedom (DoF) via antenna position optimization. In this letter, we investigate the MA-enhanced wide beam coverage over multiple subregions in the spatial domain. Specifically, we aim to maximize the minimum beam gain over the desired subregions by jointly optimizing the transmit beamforming and antenna position vector (APV). Although this problem is non-convex, we propose an efficient algorithm to solve it by leveraging the similarity between the considered multi-region coverage and classical multi-notch filter (MNF) design. In particular, we construct a spatial MNF-based transmit beamforming vector by assuming a continuous amplitude and phase-shift profile within the antenna movement region. Based on this continuous profile, we propose a sequential update algorithm to select an optimal subset of MA positions for multi-region coverage, jointly with a Gibbs sampling (GS) procedure to avoid undesired local optimum. Numerical results show that our proposed algorithm can significantly outperform conventional fixed position antennas (FPAs) and achieve a comparable performance to the alternating optimization (AO) algorithm with dramatically lower complexity.

</details>


### [112] [Discovering Optimal Robust Minimum Redundancy Arrays (RMRAs) through Exhaustive Search and Algebraic Formulation of a New Sub-Optimal RMRA](https://arxiv.org/abs/2512.24155)
*Ashish Patwari,Sanjeeva Reddy S,G Ramachandra Reddy*

Main category: eess.SP

TL;DR: 提出两项贡献：1) 对N>10的RMRA进行系统性优化，扩展至11–14的全最优解与15–20的近/次优解，并获得大量近/次优RMRA的目录；2) 通过模式挖掘和代数泛化，为新TFRSA推导出闭式表达式（CFEs），覆盖传感器位置、可用孔径和DOF，并验证CFEs的正确性。


<details>
  <summary>Details</summary>
Motivation: 现代稀疏阵列易受单传感器失效影响，需具冗余的鲁棒结构（如TFRSA与RMRA）来确保对差分阵列的孔洞自由性和DOFs的稳定获取。现有对RMRA的最优配置仅在N=6–10范围内已知，缺乏N>10的系统性方法和可闭式表达的近/次优解。

Method: 采用基于穷举的MATLAB优化来搜索RMRA，获得N=11–14的最优解，N=15–20给出近/次优解并建立大量有效解的目录；在此基础上，利用模式挖掘与代数泛化从穷举结果中提取TFRSA的CFEs，形成传感器位置、孔径和DOF的闭式表达，并在MATLAB中进行验证，确保CFEs对N≥8有效。

Result: 成功找到N=11–14的最优RMRA；N=15–20得到近/次优解并构建了大量有效解的目录；提出的TFRSA新族具有CFEs，覆盖传感器位置、孔径和DOF，且经MATLAB验证对N≥8有效。

Conclusion: 工作在两方面具有重要创新性：一是扩展了已知最优RMRA的目录，二是提出了一种符合CFEs的近/次优RMRA族，且新TFRSA的CFEs可闭式表示，便于快速设计与实现。

Abstract: Modern sparse arrays are maximally economic in that they retain just as many sensors required to provide a specific aperture while maintaining a hole-free difference coarray. As a result, these are susceptible to the failure of even a single sensor. Contrarily, two-fold redundant sparse arrays (TFRSAs) and robust minimum redundancy arrays (RMRAs) ensure robustness against single-sensor failures due to their inherent redundancy in their coarrays. At present, optimal RMRA configurations are known only for arrays with sensor counts N=6 to N=10. To this end, this paper proposes two objectives: (i) developing a systematic algorithm to discover optimal RMRAs for N>10, and (ii) obtaining a new family of near-/sub-optimal RMRA that can be completely specified using closed-form expressions (CFEs). We solve the combinatorial optimization problem of finding RMRAs using an exhaustive search technique implemented in MATLAB. Optimal RMRAs for N = 11 to 14 were successfully found and near/sub-optimal arrays for N = 15 to 20 were determined using the proposed technique. As a byproduct of the exhaustive search, a large catalogue of valid near- and sub-optimal RMRAs was also obtained. In the second stage, CFEs for a new TFRSA were obtained by applying pattern mining and algebraic generalizations to the arrays obtained through exhaustive search. The proposed family enjoys CFEs for sensor positions, available aperture, and achievable degrees of freedom (DOFs). The CFEs have been thoroughly validated using MATLAB and are found to be valid for $N\geq8$. Hence, it can be concluded that the novelty of this work is two-fold: extending the catalogue of known optimal RMRAs and formulating a sub-optimal RMRA that abides by CFEs.

</details>


### [113] [Low-complexity spectral shaping method for OFDM signals with dynamically adaptive emission mask](https://arxiv.org/abs/2512.24412)
*Javier Giménez,José A. Cortés,Luis Díez*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Orthogonal frequency division multiplexing (OFDM) signals with rectangular pulses exhibit low spectral confinement. Shaping their power spectral density (PSD) is imperative in the increasingly overcrowded spectrum to benefit from the cognitive radio (CR) paradigm. However, since the available spectrum is non-contiguous and its occupancy changes with time, the spectral shaping solution has to be dynamically adapted. This work proposes a framework that allows using a reduced set of preoptimized pulses to shape the spectrum of OFDM signals, irrespective of its spectral width and location, by means of simple transformations. The employed pulses combine active interference cancellation (AIC) and adaptive symbol transition (AST) terms in a transparent way to the receiver. They can be easily adapted online by the communication device to changes in the location or width of the transmission band, which contrasts with existing methods of the same type that require solving NP-hard optimization problems.

</details>


### [114] [The Wigner-Ville Transform as an Information Theoretic Tool in Radio-frequency Signal Analysis](https://arxiv.org/abs/2512.24488)
*Erik Lentz,Emily Ellwein,Bill Kay,Audun Myers,Cameron Mackenzie*

Main category: eess.SP

TL;DR: 将Wigner-Ville变换作为信息测度工具，结合Tsallis熵及相关泛函，对信号的信息量进行测量并用于检测/定位；在某些射频场景下相对于能量基方法具有显著灵敏度提升（>15 dB），且无需大量训练。


<details>
  <summary>Details</summary>
Motivation: 在嘈杂背景和信息密集信号的检测/定位中，利用信息理论量化信息体积，提升探测性能并降低资源需求。

Method: 将Wigner-Ville变换与Tsallis熵及相关信息泛函结合，构建基于信息的检测量；对比能量法，给出射频通信中的具体用例，显示显著灵敏度提升，且强调较低训练需求。

Result: 在某些情境下，Wigner-Ville信息测度对检测具有>15 dB的灵敏度优势；相较于能量基方法，在资源受限情境下检测效果提升可达到多数量级。

Conclusion: Wigner-Ville基信息测度具有潜在的广泛应用前景和显著的性能提升，值得进一步研究和应用推广。

Abstract: This paper presents novel interpretations to the field of classical signal processing of the Wigner-Ville transform as an information measurement tool. The transform's utility in detecting and localizing information-laden signals amidst noisy and cluttered backgrounds, and further providing measure of their information volumes, are detailed herein using Tsallis' entropy and information and related functionals. Example use cases in radio frequency communications are given, where Wigner-Ville-based detection measures can be seen to provide significant sensitivity advantage, for some shown contexts greater than 15~dB advantage, over energy-based measures and without extensive training routines. Such an advantage is particularly significant for applications which have limitations on observation resources including time/space integration pressures and transient and/or feeble signals, where Wigner-Ville-based methods would improve sensing effectiveness by multiple orders of magnitude. The potential for advancement of several such applications is discussed.

</details>


### [115] [A Uniform Pilot and Data Payload Optimization Framework for OTFS-Based ISAC](https://arxiv.org/abs/2512.24624)
*Borui Du,Yumeng Zhang,Christos Masouros,Bruno Clerckx*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The orthogonal time frequency space (OTFS) signal is considered a promising solution for high-mobility wireless environments. It manages Doppler effects by utilizing delay-Doppler (DD) domain processing. However, the relatively long OTFS frame duration could introduce considerable sensing or communication latency when radar and communication are performed separately. By operating in a dual-functional radar and communication (DFRC) mode, the OTFS system performs sensing and data transmission simultaneously, thereby reducing the resulting latency. Nevertheless, the optimal OTFS DFRC signal strategy remains insufficiently explored. This paper investigates the optimal signal design for OTFS DFRC systems, focusing on pilot symbol design and data symbol power allocation. Specifically, we derive a channel capacity lower bound metric for communication that considers channel estimation errors in OTFS. For sensing, we derive an integrated sidelobe level (ISL), accounting for the randomness of the data symbols alongside the deterministic pilot symbols. Leveraging the above metrics, we formulate an optimization problem that balances radar and communication performance, and then solve it using an alternating optimization framework. We validate the proposed signal through numerical analysis and Monte Carlo simulations. Our analysis shows that OTFS DFRC enforces a deterministic pilot signal that is characterized by a concentrated peak in the DD domain, which furnishes a common structure in the DD domain facilitating sensing and channel estimation, with data multiplexed in other DD grids, thereby unifying sensing and communication within a single OTFS signal. Compared with conventional OTFS signals, the proposed OTFS DFRC signal expands the achievable sensing-communication performance region, delivering at least a 9.45 dB ISL suppression for sensing and a 4.82 dB SINR ratio gain for communication.

</details>


### [116] [Beam-Squint-Aided Hierarchical Sensing for Integrated Sensing and Communications with Uniform Planar Arrays](https://arxiv.org/abs/2512.24727)
*Jaehong Jo,Jihun Park,Yo-Seb Jeon,H. Vincent Poor*

Main category: eess.SP

TL;DR: 提出了一种基于分层感知的宽带集成感知与通信（ISAC）框架，利用波束偏斜效应在UPA上实现高效的二维角度估计。通过两阶段搜索（先高程再方位）并协同配置时延单元与相位换挡，覆盖多个网格点并利用OFDM子载波进行稀疏信号恢复。提出改进的匹配追踪算法与针对分层感知架构的功率分配策略，实验结果显示在 sensing power 更低且性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在宽带ISAC场景下，UPA的二维角度估计与功率高效感知面临挑战。利用宽带中的波束偏斜效应可将角度信息分解为更易处理的多阶段搜索，同时降低感知功耗。

Method: 提出分层感知框架，基于波束偏斜效应在宽带OFDM系统中实现两维角度估计；在每一阶段，使用时延线和相位换挡联合配置以跨OFDM子载波覆盖多个网格点；将角度估计问题建模为稀疏信号恢复并开发适配该分层架构的改进匹配追踪算法；设计最小化总发射功率的功率分配策略，需同时满足 sensing 与通信性能。

Result: 数值结果显示该框架在感知性能方面优于传统 sensing 方法，且感知功耗显著降低。

Conclusion: 提出的分层感知框架可实现高效的二维角度估计与功率友好感知-通信耦合，在宽带UPA-ISAC场景具有潜在应用价值。

Abstract: In this paper, we propose a novel hierarchical sensing framework for wideband integrated sensing and communications with uniform planar arrays (UPAs). Leveraging the beam-squint effect inherent in wideband orthogonal frequency-division multiplexing (OFDM) systems, the proposed framework enables efficient two-dimensional angle estimation through a structured multi-stage sensing process. Specifically, the sensing procedure first searches over the elevation angle domain, followed by a dedicated search over the azimuth angle domain given the estimated elevation angles. In each stage, true-time-delay lines and phase shifters of the UPA are jointly configured to cover multiple grid points simultaneously across OFDM subcarriers. To enable accurate and efficient target localization, we formulate the angle estimation problem as a sparse signal recovery problem and develop a modified matching pursuit algorithm tailored to the hierarchical sensing architecture. Additionally, we design power allocation strategies that minimize total transmit power while meeting performance requirements for both sensing and communication. Numerical results demonstrate that the proposed framework achieves superior performance over conventional sensing methods with reduced sensing power.

</details>


### [117] [Digitalizing Over-the-Air Computation via The Novel Complement Coded Modulation](https://arxiv.org/abs/2512.24788)
*Zhixu Wang,Jiacheng Yao,Wei Xu,Wei Shi,Kaibin Huang*

Main category: eess.SP

TL;DR: 提出基于二进制补码的AirComp数字化编码方案，结合子载波传输、截断逆转预处理、闭式LMMSE检测和不等功率分配，在低SNR下优于现有数字AirComp方法。


<details>
  <summary>Details</summary>
Motivation: 解决模拟AirComp的固有限制，提供鲁棒的数字AirComp编码-解码框架，利用截断逆、统一符号分布和子载波权重来提高准确率，尤其在低SNR场景。

Method: 将量化离散值编码为二进制序列，采用二进制补码，跨多子载波传输；接收端构造一个函数映射从叠加的数字调制信号到计算输出；在信道衰落情形下采用截断逆；在统一符号分布下推导闭式LMMSE探测器并给出低复杂度的截断选取算法；并对各子载波进行不等功率分配以提升准确性。

Result: 数值结果表明该方案在低SNR下显著优于现有数字AirComp方法，接近无误差的理论极限，并展示了对现有方法的鲁棒性。

Conclusion: 通过二进制补码编码、截断逆、闭式LMMSE检测及不等功率分配，本方案实现了在多子载波AirComp中的高效、鲁棒数字化计算，且在低SNR环境具有显著的性能提升。

Abstract: To overcome inherent limitations of analog signals in over-the-air computation (AirComp), this letter proposes a two's complement-based coding scheme for the AirComp implementation with compatible digital modulations. Specifically, quantized discrete values are encoded into binary sequences using the two's complement and transmitted over multiple subcarriers. At the receiver, we design a decoder that constructs a functional mapping between the superimposed digital modulation signals and the target of computational results, theoretically ensuring asymptotic error free computation with the minimal codeword length. To further mitigate the adverse effects of channel fading, we adopt a truncated inversion strategy for pre-processing. Benefiting from the unified symbol distribution after the proposed encoding, we derive the optimal linear minimum mean squared error (LMMSE) detector in closed form and propose a low complexity algorithm seeking for the optimal truncation selection. Furthermore, the inherent importance differences among the coded outputs motivate an uneven power allocation strategy across subcarriers to improve computational accuracy. Numerical results validate the superiority of the proposed scheme over existing digital AirComp approaches, especially at low signal to-noise ratio (SNR) regimes.

</details>


### [118] [Efficient Joint Resource Allocation for Wireless Powered ISAC with Target Localization](https://arxiv.org/abs/2512.24815)
*Boyao Li,Qinwei He,Boao Zhang,Xiaopeng Yuan,Anke Schmeink*

Main category: eess.SP

TL;DR: 提出一种可联合优化WPT持续时间、ISAC传输时间分配与发射功率的无线供能ISAC系统，以在目标定位CRB约束下最大化最小吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在无线供能的ISAC系统中，需要在能量供给、通信吞吐量和 sensing 精度之间取得权衡。WPT信号不仅供能，还能辅助目标定位，因此需要一个能同时考虑能量、通信和感知精度的优化框架。

Method: 提出以时间分配和功率分配为决策变量的优化问题，并以CRB约束描述定位精度。为解决非凸性，采用变量替换与对数函数单调性的 reformulation，并基于逐步凸近似(SCA)设计高效迭代算法。

Result: 数值结果显示算法收敛，并相较基准方案取得显著吞吐量提升，证明了在无线供能ISAC中协同时间与功率优化对平衡感知精度与通信性能的重要性。

Conclusion: 在无线供能ISAC系统中，通过对WPT时长、ISAC传输时长和发射功率的协同优化，可在给定CRB约束下实现更优的最小吞吐量，同时兼顾感知精度。

Abstract: Wireless powered integrated sensing and communication (ISAC) faces a fundamental tradeoff between energy supply, communication throughput, and sensing accuracy. This paper investigates a wireless powered ISAC system with target localization requirements, where users harvest energy from wireless power transfer (WPT) and then conduct ISAC transmissions in a time-division manner. In addition to energy supply, the WPT signal also contributes to target sensing, and the localization accuracy is characterized by Cramér-Rao bound (CRB) constraints. Under this setting, we formulate a max-min throughput maximization problem by jointly allocating the WPT duration, ISAC transmission time allocation, and transmit power. Due to the nonconvexity of the resulting problem, a suitable reformulation is developed by exploiting variable substitutions and the monotonicity of logarithmic functions, based on which an efficient successive convex approximation (SCA)-based iterative algorithm is proposed. Simulation results demonstrate convergence and significant performance gains over benchmark schemes, highlighting the importance of coordinated time-power optimization in balancing sensing accuracy and communication performance in wireless powered ISAC systems.

</details>


### [119] [No Vision, No Wearables: 5G-based 2D Human Pose Recognition with Integrated Sensing and Communications](https://arxiv.org/abs/2512.24923)
*Haojin Li,Dongzhe Li,Anbang Zhang,Wenqi Zhang,Chen Sun,Haijun Zhang*

Main category: eess.SP

TL;DR: 提出一种基于5G ISAC 的无触控人体姿态识别（HPR）系统，利用上行SRS从信号中提取多域特征，通过编码器对齐潜在空间并融合低维特征以推断2D姿态，在典型室内环境下显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着室内无控制器交互需求增加，现有视觉与RF HPR方法在隐私、遮挡、设备依赖及分辨率等方面存在挑战，5G ISAC提供将通信与感知融合的机会以克服这些瓶颈。

Method: 设计一个基于5G ISAC的系统，通过上行SRS实现2D HPR。提取来自多个域的丰富特征，利用编码器实现统一对齐和潜在空间表征；再将低维特征融合输出人体姿态状态。

Result: 在典型室内环境中，所提方案显著优于当前主流基线解决方案的HPR性能。

Conclusion: 为通用人机交互提供稳定的5G ISAC HPR技术基础，展示了将5G感知能力应用于自然、无控制器的人机交互的可行性。

Abstract: With the increasing maturity of contactless human pose recognition (HPR) technology, indoor interactive applications have raised higher demands for natural, controller-free interaction methods. However, current mainstream HPR solutions relying on vision or radio-frequency (RF) (including WiFi, radar) still face various challenges in practical deployment, such as privacy concerns, susceptibility to occlusion, dedicated equipment and functions, and limited sensing resolution and range. 5G-based integrated sensing and communication (ISAC) technology, by merging communication and sensing functions, offers a new approach to address these challenges in contactless HPR. We propose a practical 5G-based ISAC system capable of inferring 2D HPR from uplink sounding reference signals (SRS). Specifically, rich features are extracted from multiple domains and employ an encoder to achieve unified alignment and representation in a latent space. Subsequently, low-dimensional features are fused to output the human pose state. Experimental results demonstrate that in typical indoor environments, our proposed 5G-based ISAC HPR system significantly outperforms current mainstream baseline solutions in HPR performance, providing a solid technical foundation for universal human-computer interaction.

</details>


### [120] [Fundamental Limits for Near-Field Sensing -- Part I: Narrow-Band Systems](https://arxiv.org/abs/2512.24958)
*Tong Wei,Kumar Vijay Mishra,Bhavani Shankar M. R.,Björn Ottersten*

Main category: eess.SP

TL;DR: 提出极大阵列近场联合参数估计的 CRB，并给出近场/远场的尺度分析、Fresnel 修正条件及设计指南。


<details>
  <summary>Details</summary>
Motivation: 6G 需在极大阵列和高频条件下实现高分辨率 sensing；近场波前为球面，传统远场模型失效，需理论极限以指导算法与波束设计。

Method: 建立统一窄带近场信号模型；基于 Slepian–Bangs 公式推导目标位置、速度、RCS 的闭式 CRB；在慢时采样下分析，给出远近场近似以揭示尺度依赖；解析自信息项与交叉项的作用；提供设计准则。

Result: 给出闭式 CRB 和近/远场近似的显式表达，并明确阵列孔径、目标距离、波长、相干积分长度对极限的影响；通过仿真验证理论正确性并与尺度规律吻合。

Conclusion: 建立近场感知的统一理论基础，为 ELAAs 下的波束设计与算法提供指引；Part II 将继续扩展理论细节与应用。

Abstract: Extremely large-scale antenna arrays (ELAAs) envisioned for 6G enable high-resolution sensing. However, the ELAAs worked in extremely high frequency will push operation into the near-field region, where spherical wavefronts invalidate classical far-field models and alter fundamental estimation limits. The purpose of this and the companion paper (Part II) is to develop the theory of fundamental limits for near-field sensing systems in detail. In this paper (Part I), we develop a unified narrow-band near-field signal model for joint parameter sensing of moving targets using the ELAAs. Leveraging the Slepian--Bangs formulation, we derive closed-form Cram'er--Rao bounds (CRBs) for joint estimation of target position, velocity, and radar cross-section (RCS) under the slow-time sampling model. To obtain interpretable insights, we further establish explicit far-field and near-field approximations that reveal how the bounds scale with array aperture, target range, carrier wavelength, and coherent integration length. The resulting expressions expose the roles of self-information terms and their cross terms, clarifying when Fresnel corrections become non-negligible and providing beamformer and algorithm design guidelines for near-field sensing with ELAAs. Simulation results validate the derived CRBs and their far-field and near-field approximations, demonstrating accurate agreement with the analytical scaling laws across representative array sizes and target ranges.

</details>


### [121] [Fundamental Limits for Near-Field Sensing -- Part II: Wide-Band Systems](https://arxiv.org/abs/2512.24962)
*Tong Wei,Kumar Vijay Mishra,Bhavani Shankar M. R.,Björn Ottersten*

Main category: eess.SP

TL;DR: 本论文在广带宽近场感知中推导并验证了基于OFDM的宽带近场CRBs，给出目标位置、速度及雷达散射截面（RCS）的精确FIM与CRB，以及可行的近场/远场近似和设计洞见。


<details>
  <summary>Details</summary>
Motivation: 在极大阵列（ELAAs）和宽带通信场景下，延迟、多普勒与空间效应在频域上耦合，迫切需要从远场到近场、从窄带到宽带的统一极限来指导系统设计。本研究延续Part I，建立宽带近场感知的理论极限。

Method: 建立精确的近场宽带信号模型，考虑频率相关传播、球面波几何和跨子载波的目标位置与运动参数耦合；采用Slepian–Bangs方法推导宽带Fisher信息矩阵及联合位置、速度、RCS的CRB，并给出子载波的信息聚合规律；推导可行的远场与近场近似以便设计分析。

Result: 给出宽带近场的FIM与CRB，以及跨子载波的信息聚合规律；提出有意义的远场/近场近似，揭示带宽、相干积分长度和阵列孔径在估计精度中的作用；通过数值仿真验证CRB及近似的有效性，与典型范围、带宽、阵列配置下的分析放大系数高度一致。

Conclusion: 为广带宽近场 sensing 提供基本极限，并给出在ELAA场景下的设计指引，明确何时需要考虑宽带与近场效应以及如何利用它们提升定位、测速和RCS估计的性能。

Abstract: Near-field sensing with extremely large-scale antenna arrays (ELAAs) in practical 6G systems is expected to operate over broad bandwidths, where delay, Doppler, and spatial effects become tightly coupled across frequency. The purpose of this and the companion paper (Part I) is to develop the unified Cram'er--Rao bounds (CRBs) for sensing systems spanning from far-field to near-field, and narrow-band to wide-band. This paper (Part II) derives fundamental estimation limits for a wide-band near-field sensing systems employing orthogonal frequency-division multiplexing signaling over a coherent processing interval. We establish an exact near-field wide-band signal model that captures frequency-dependent propagation, spherical-wave geometry, and the intrinsic coupling between target location and motion parameters across subcarriers and slow time. Similar as Part I using the Slepian--Bangs formulation, we derive the wide-band Fisher information matrix and the CRBs for joint estimation of target position, velocity, and radar cross-section, and we show how wide-band information aggregates across orthogonal subcarriers. We further develop tractable far-field and near-field approximations which provide design-level insights into the roles of bandwidth, coherent integration length, and array aperture, and clarify when wide-band effects. Simulation results validate the derived CRBs and its approximations, demonstrating close agreement with the analytical scaling laws across representative ranges, bandwidths, and array configurations.

</details>
