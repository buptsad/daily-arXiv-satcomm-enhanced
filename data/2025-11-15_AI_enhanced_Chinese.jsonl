{"id": "2511.10611", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10611", "abs": "https://arxiv.org/abs/2511.10611", "authors": ["Alagappan Ramanathan", "Eunju Kang", "Dongsu Han", "Sangeetha Abdu Jyothi"], "title": "Towards an Agentic Workflow for Internet Measurement Research", "comment": null, "summary": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.\n  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.", "AI": {"tldr": "ArachNet uses four specialized LLM agents to automatically generate expert-like Internet measurement workflows, enabling multi-framework integration and rapid diagnostics without deep domain expertise.", "motivation": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools and domain expertise; when network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling.", "method": "ArachNet employs four specialized agents that mirror expert workflow, from problem decomposition to solution implementation; the system autonomously generates measurement workflows and integrates multi-framework tooling; validation is conducted across progressively challenging Internet resilience scenarios.", "result": "Generated workflows match expert-level reasoning and produce outputs similar to specialist solutions; they handle complex multi-framework integration that traditionally requires days of manual coordination.", "conclusion": "ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process used by experts, enabling broader access to sophisticated measurement capabilities while maintaining research-grade analytical rigor."}}
{"id": "2511.10619", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10619", "abs": "https://arxiv.org/abs/2511.10619", "authors": ["Avrim Blum", "Marten Garicano", "Kavya Ravichandran", "Dravyansh Sharma"], "title": "Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem", "comment": "25 pages", "summary": "The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $\u03a9(k)$ and $\u03a9(\\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u7c7b\u53c2\u6570\u5316\u7684\u6539\u8fdb\u5f0f Bandits \u7b97\u6cd5\u65cf\uff0c\u5e76\u5229\u7528\u79bb\u7ebf\u6570\u636e\u5b66\u4e60\u8fd1\u4f18\u7b56\u7565\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\uff1b\u5728\u5956\u52b1\u66f2\u7ebf\u5177\u5907\u5f3a\u51f9\u6027\u6761\u4ef6\u65f6\uff0c\u7b2c\u4e00\u65cf\u53ef\u5b9e\u73b0\u5bf9 k \u7684\u6700\u4f18\u4f9d\u8d56\uff1b\u7b2c\u4e8c\u65cf\u5728\u826f\u597d\u5b9e\u4f8b\u4e0b\u4fdd\u8bc1\u6700\u4f18\u81c2\u8bc6\u522b\uff0c\u5728\u5dee\u52a3\u5b9e\u4f8b\u4e0b\u56de\u843d\u5230\u6700\u574f\u60c5\u5f62\uff0c\u540c\u65f6\u7ed9\u51fa\u6570\u636e\u81ea\u9002\u5e94\u7684\u4fdd\u8bc1\uff0c\u65e0\u9700\u663e\u5f0f\u9a8c\u8bc1\u5047\u8bbe\u3002", "motivation": "\u6539\u8fdb\u5e26\u4e0d\u786e\u5b9a\u6027\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\u7684\u7406\u8bba\u5efa\u6a21\u2014\u2014\u6539\u8fdb\u578b\u591a\u81c2\u8d4c\u535a\u673a\uff1b\u6bcf\u6b21\u62c9\u81c2\u7684\u5956\u52b1\u5355\u8c03\u589e\u5927\u3001\u8fb9\u9645\u6536\u76ca\u4e0b\u964d\uff1b\u5728\u9762\u5bf9\u79bb\u6563\u7684\u5f3a\u4e0b\u754c \u03a9(k) \u4e0e \u03a9(\u221ak) \u7684 Worst-Case \u4fdd\u8bc1\u65f6\uff0c\u63a2\u7d22\u66f4\u5177\u6570\u636e\u4f9d\u8d56\u6027\u7684\u7b97\u6cd5\u4ee5\u63d0\u5347\u5b9e\u9645\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u66f2\u7ebf\u5177\u6709\u51f9\u6027\u6027\u8d28\u65f6\u3002", "method": "\u63d0\u51fa\u4e24\u7c7b\u53c2\u6570\u5316\u7684\u7b97\u6cd5\u65cf\uff0c\u5e76\u57fa\u4e8e\u79bb\u7ebf\u6570\u636e\u5bf9\u5b66\u4e60\u51fa\u8fd1\u4f3c\u6700\u4f18\u7b97\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u8fdb\u884c\u7406\u8bba\u754c\u5b9a\u3002\u7b2c\u4e00\u65cf\u5305\u542b\u4e4b\u524d\u6700\u4f18\u7684\u968f\u673a\u7b97\u6cd5\uff1b\u5728\u5956\u52b1\u66f2\u7ebf\u7684\u51f9\u6027\u5f3a\u5ea6\u6ee1\u8db3\u989d\u5916\u6761\u4ef6\u65f6\uff0c\u9009\u53d6\u8be5\u65cf\u4e2d\u7684\u7b97\u6cd5\u53ef\u83b7\u5f97\u5173\u4e8e k \u7684\u6700\u4f18\u4f9d\u8d56\u3002\u7b2c\u4e8c\u65cf\u5728\u826f\u597d\u5b9e\u4f8b\u4e0a\u4fdd\u8bc1\u80fd\u6b63\u786e\u8bc6\u522b\u6700\u4f73\u81c2\uff0c\u5728\u4e0d\u826f\u5b9e\u4f8b\u4e0a\u9000\u5316\u56de\u6700\u574f\u60c5\u5f62\uff0c\u5e76\u4ece\u7edf\u8ba1\u5b66\u4e60\u89d2\u5ea6\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u4fdd\u8bc1\u3002", "result": "\u7b2c\u4e00\u65cf\u5728\u5f3a\u51f9\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5bf9 k \u7684\u6700\u4f18\u4f9d\u8d56\u7684\u66f4\u5f3a\u4fdd\u8bc1\uff1b\u7b2c\u4e8c\u65cf\u5b9e\u73b0\u5bf9\u826f\u597d\u5b9e\u4f8b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u5e76\u5bf9\u5dee\u52a3\u5b9e\u4f8b\u56de\u843d\u5230\u6700\u574f\u60c5\u5f62\uff0c\u540c\u65f6\u65e0\u9700\u663e\u5f0f\u9a8c\u8bc1\u5047\u8bbe\u5373\u53ef\u83b7\u5f97\u66f4\u5f3a\u7684\u6570\u636e\u4f9d\u8d56\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6539\u8fdb\u578b Bandits \u4e0e\u7edf\u8ba1\u5b66\u4e60\u89c6\u89d2\u7ed3\u5408\uff0c\u63d0\u51fa\u7684\u4e24\u65cf\u7b97\u6cd5\u63d0\u4f9b\u4e86\u57fa\u4e8e\u79bb\u7ebf\u6570\u636e\u7684\u66f4\u5f3a\u6570\u636e\u76f8\u5173\u4fdd\u8bc1\uff0c\u5b9e\u73b0\u5bf9\u5b9e\u4f8b\u7684\u81ea\u9002\u5e94\u5904\u7406\uff0c\u5f25\u5408\u4e86\u4e25\u683c\u6700\u574f\u60c5\u51b5\u754c\u4e0e\u66f4\u5b9e\u73b0\u6027\u7684\u6570\u636e\u9a71\u52a8\u8868\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
