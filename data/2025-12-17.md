<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 4]
- [eess.SY](#eess.SY) [Total: 15]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 58]
- [eess.SP](#eess.SP) [Total: 14]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Symbol Distributions in Semantic Communications: A Source-Channel Equilibrium Perspective](https://arxiv.org/abs/2512.14022)
*Hanju Yoo,Dongha Choi,Songkuk Kim,Chan-Byoung Chae,Robert W. Heath*

Main category: cs.IT

TL;DR: 提出一个信息理论上的权衡框架，将语义通信编码器在源编码的有效码长与通信中的互信息之间进行权衡，得到符号分布服从Student's t分布；在基于图像的语义系统上验证该分布与学习符号的形状参数有关，并受变长编码和数据集熵变异性的影响，同时通过正则化逼近目标先验（如高斯分布）来提升训练收敛性，支持该理论。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端语义编码器产出的符号通常为固定维度且呈重尾分布，背后原因尚不清楚。理解并可控地解释该分布，有助于改进编码、压缩与传输策略，并提高训练稳定性。

Method: 建立一个信息论优化框架，将两类目标：为源编码分配功率以最小化有效码长，以及最大化互信息以提升通信性能，进行权衡。推导得到符号分布为Student's t分布。基于图像的语义系统进行广泛实验，分析变量长度编码、数据集熵对分布形状参数的影响，并引入正则化项以引导符号分布到目标先验（如高斯分布）以提升训练收敛性。

Result: 理论上得到Student's t分布作为符号分布；实验表明学习得到的符号随分布形状参数变化，与是否使用变长编码和数据集熵相关；引入目标分布的正则化可提升训练收敛性，支持提出的因果关系。

Conclusion: 提出的统一信息论框架解释了语义编码符号的分布特征及其可控性，揭示源编码与通信之间的权衡，并给出通过目标先验引导来改进训练与系统设计的方向。

Abstract: Semantic communication systems often use an end-to-end neural network to map input data into continuous symbols. These symbols, which are essentially neural network features, usually have fixed dimensions and heavy-tailed distributions. However, due to the end-to-end training nature of the neural network encoder, the underlying reason for the symbol distribution remains underexplored. We propose a new explanation for the semantic symbol distribution: an inherent trade-off between source coding and communications. Specifically, the encoder balances two objectives: allocating power for minimum \emph{effective codelength} (for source coding) and maximizing mutual information (for communications). We formalize this trade-off via an information-theoretic optimization framework, which yields a Student's $t$-distribution as the resulting symbol distribution. Through extensive studies on image-based semantic systems, we find that our formulation models the learned symbols and predicts how the symbol distribution's shape parameter changes with respect to (i) the use of variable-length coding and (ii) the dataset's entropy variability. Furthermore, we demonstrate how introducing a regularizer that enforces a target symbol distribution, which guides the encoder towards a target prior (e.g., Gaussian), improves training convergence and supports our hypothesis.

</details>


### [2] [Complete weight enumerators and weight hierarchies for linear codes from quadratic forms](https://arxiv.org/abs/2512.14073)
*Xiumei Li,Xiaotong Sun,Min Sha*

Main category: cs.IT

TL;DR: 提出两类来自二次型的 F_q 线性码 C_Q 与 C_Q'，至多四非零权，给出完整重量枚举与权序，并证明多数为最小码、部分达到 Griesmer 界，还给出下降码 C_Q,N 与 C_Q,N' 的权层次。


<details>
  <summary>Details</summary>
Motivation: 扩展 Xie 等人的构造，获得少权线性码及其权序、权层次，为编码理论与应用（如秘密共享、分发等）提供新的可用码族。

Method: 基于二元二次型的构造，通过双变量构造将其映射到 F_q，利用指数和技巧推导出完整重量枚举及权层次；并对 descended codes C_Q,N、C_Q,N' 的权层次进行分析。

Result: 得到两类至多四非零权的线性码 C_Q、C_Q'；完整重量枚举与重量层次被完全确定；多数码为最小，部分码达到 Griesmer 上界；并给出下降码 C_Q,N、C_Q,N' 的权层次。

Conclusion: 本文提供了新的少权线性码族及其结构特征，拓展了可用码的集合，具有理论价值和潜在应用。

Abstract: In this paper, for an odd prime power $q$, we extend the construction of Xie et al. \cite{XOYM2023} to propose two classes of linear codes $\mathcal{C}_{Q}$ and $\mathcal{C}_{Q}'$ over the finite field $\mathbb{F}_{q}$ with at most four nonzero weights. These codes are derived from quadratic forms through a bivariate construction. We completely determine their complete weight enumerators and weight hierarchies by employing exponential sums. Most of these codes are minimal and some are optimal in the sense that they meet the Griesmer bound. Furthermore, we also establish the weight hierarchies of $\mathcal{C}_{Q,N}$ and $\mathcal{C}_{Q,N}'$, which are the descended codes of $\mathcal{C}_{Q}$ and $\mathcal{C}_{Q}'$.

</details>


### [3] [Agile Affine Frequency Division Multiplexing](https://arxiv.org/abs/2512.14424)
*Yewen Cao,Yulin Shao*

Main category: cs.IT

TL;DR: 提出 Agile-AFDM，通过将 AFDM 的 chirp 参数设为按传输块数据与信道实时信息可优化的变量，将波形从静态调整转向自适应设计，从而在功率效率、解调干扰和 sensing 精度等方面实现多目标优化。


<details>
  <summary>Details</summary>
Motivation: 6G 需求更高的灵活性与智能自适应波形；传统 AFDM 虽对双时变信道具鲁棒性，但其 chirp 参数固定且面向最坏情形，导致性能受限。需面向数据与信道信息的自适应波形方案。

Method: 将 chirp 参数定义为可对每个传输块优化的变量，结合实时信道与数据信息，提出高效的优化算法以实现最小化 PAPR、抑制 ICI 或降低 CRLB 的目标，从而实现对波形的自适应重配置。

Result: 通过全面的仿真，Agile-AFDM 在功率效率、通信可靠性（ICI 控制）和 sensing 精度（CRLB）等指标上显著优于传统 OFDM 与静态 AFDM，显示出在 6G 及其以上场景中的潜在优势。

Conclusion: 将静态、统一的波形转变为基于上下文的自适应信号设计器，是实现面向 6G 的敏捷波形设计的关键一步。

Abstract: The advancement to 6G calls for waveforms that transcend static robustness to achieve intelligent adaptability. Affine Frequency Division Multiplexing (AFDM), despite its strength in doubly-dispersive channels, has been confined by chirp parameters optimized for worst-case scenarios. This paper shatters this limitation with Agile-AFDM, a novel framework that endows AFDM with dynamic, data-aware intelligence. By redefining chirp parameters as optimizable variables for each transmission block based on real-time channel and data information, Agile-AFDM transforms into an adaptive platform. It can actively reconfigure its waveform to minimize peak-to-average power ratio (PAPR) for power efficiency, suppress inter-carrier interference (ICI) for communication reliability, or reduce Cramer-Rao bound (CRLB) for sensing accuracy. This paradigm shift from a static, one-size-fits-all waveform to a context-aware signal designer is made practical by efficient, tailored optimization algorithms. Comprehensive simulations demonstrate that this capability delivers significant performance gains across all metrics, surpassing conventional OFDM and static AFDM. Agile-AFDM, therefore, offers a crucial step forward in the design of agile waveforms for 6G and beyond.

</details>


### [4] [The Performance of Compression-Based Denoisers](https://arxiv.org/abs/2512.14539)
*Dan Song,Ayfer Özgür,Tsachy Weissman*

Main category: cs.IT

TL;DR: 扩展基于压缩的去噪到通用离散记忆信道，通过让失真度量匹配信道条件分布，给出精确的损失表征，并在均方误差和汉明损失等特殊情形中给出结果。


<details>
  <summary>Details</summary>
Motivation: 现有工作仅限于加性噪声通道，难以处理更一般的离散记忆信道。需要一种统一的框架来理解在观测通过记忆信道时的去噪性能，并与间接率失真等视角进行对比。

Method: 通过将损失压缩器的失真度量设计为匹配信道的条件分布，同时分析源-观测-去噪输出的经验联合分布的偏离，以满足马尔可夫性质，得到对该去噪器损失的精确表征。对 MSE 和汉明损失等特殊情形给出具体结果，并将该框架与间接率失真视角进行比较。

Result: 给出对该去噪器在一般离散记忆信道下的损失的精确表征；在特定情形（如 MSE、汉明损失）具有可分析的结果；并提供与间接率失真视角的对比。

Conclusion: 该方法把压缩式去噪推广到一般离散记忆信道，并提供对性能的精确刻画，验证了在特定损失函数下的可分析性，并与其他理论视角相联系。

Abstract: We consider a denoiser that reconstructs a stationary ergodic source by lossily compressing samples of the source observed through a memoryless noisy channel. Prior work on compression-based denoising has been limited to additive noise channels. We extend this framework to general discrete memoryless channels by deliberately choosing the distortion measure for the lossy compressor to match the channel conditional distribution. By bounding the deviation of the empirical joint distribution of the source, observation, and denoiser outputs from satisfying a Markov property, we give an exact characterization of the loss achieved by such a denoiser. Consequences of these results are explicitly demonstrated in special cases, including for MSE and Hamming loss. A comparison is made to an indirect rate-distortion perspective on the problem.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [5] [Delay Optimization in a Simple Offloading System: Extended Version](https://arxiv.org/abs/2512.13810)
*Darin Jeff,Eytan Modiano*

Main category: eess.SY

TL;DR: 提出了一个两阶段的任务卸载系统：本地服务器顺序处理后转入云端服务器，提供两种服务模式并对资源分配进行最优控制以最小化时延。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算场景中，如何在两种服务模式之间动态分配任务并分配资源，以同时提高吞吐量与降低时延，是一个关键问题。本文旨在构建一个可推导的最优策略框架，以在给定分配策略下给出资源分区的闭式时延表达，并揭示延迟最优的任务分配结构。

Method: - 建立系统的稳定性区域的表征；- 设计吞吐最大化的服务模式原则；- 给定任务分配策略时，推导资源分区的最优解并给出闭式时延表达；- 证明延迟最优的分配策略具有分离/分叉结构：低负载时优选单一模式，超过临界负载后需要跨两个模式进行分配；- 通过数值实验验证理论结果。

Result: 得到系统稳定性区域的界限、吞吐最大化的服务模式设计原则，以及给定分配策略的最优分区与闭式时延表达；证明了延迟最优分配的分离性（breakaway）结构，并在低负载与高负载两种情形下形成不同的最优分配策略；通过数值验证支持理论结论。

Conclusion: 在低负载时，单一服务模式即可实现最优延迟；在达到临界负载后，必须在两个模式之间进行分配以进一步降低时延。该结论为动态任务卸载策略提供了清晰的设计准则与实现指引，并通过数值实验验证其有效性。

Abstract: We consider a computation offloading system where jobs are processed sequentially at a local server followed by a higher-capacity cloud server. The system offers two service modes, differing in how the processing is split between the servers. Our goal is to design an optimal policy for assigning jobs to service modes and partitioning server resources in order to minimize delay. We begin by characterizing the system's stability region and establishing design principles for service modes that maximize throughput. For any given job assignment strategy, we derive the optimal resource partitioning and present a closed-form expression for the resulting delay. Moreover, we establish that the delay-optimal assignment policy exhibits a distinct breakaway structure: at low system loads, it is optimal to route all jobs through a single service mode, whereas beyond a critical load threshold, jobs must be assigned across both modes. We conclude by validating these theoretical insights through numerical evaluation.

</details>


### [6] [A Convex Obstacle Avoidance Formulation](https://arxiv.org/abs/2512.13836)
*Ricardo Tapia,Iman Soltani*

Main category: eess.SY

TL;DR: 提出一种通用的凸障碍避免框架，并将其与凸MPC结合，以实现高频、实时的碰撞避免。即使障碍物超出预测视野，仍能有效工作；在不可避免的非凸情形下，性能不劣于常见非凸方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在动态环境中需要可靠的碰撞避免。NMPC在时间关键场景下需要高频求解，但常采用线性化、缩短时域等方法以提升速度，往往以牺牲准确性和鲁棒性为代价。本研究提出一个能在凸优化框架内进行障碍避免的通用方法，以提升实时性和可靠性。

Method: 提出一种将逻辑引入凸优化的 novel approach，将障碍避让编码进入凸MPC的约束/目标中，从而得到一个凸优化问题。该框架使得即使障碍物不在预测 horizon 内，仍保持有效避让能力，并且在需要非凸处理的场景下，其性能可与或优于典型的非凸替代方法。

Result: 相较于非凸方法，计算效率显著提升；对超出预测视野的障碍也有效，从而允许更短的预测 horizon 以满足实时性要求；在不可避免的非凸情形中，方法的表现与常见非凸方案相当或更优。

Conclusion: 该工作将障碍避免以凸优化方式融入凸MPC，使在自动驾驶等高度非线性动力学下的实时性和鲁棒性显著提升，并扩展了凸优化在动力学系统中的应用范围。

Abstract: Autonomous driving requires reliable collision avoidance in dynamic environments. Nonlinear Model Predictive Controllers (NMPCs) are suitable for this task, but struggle in time-critical scenarios requiring high frequency. To meet this demand, optimization problems are often simplified via linearization, narrowing the horizon window, or reduced temporal nodes, each compromising accuracy or reliability. This work presents the first general convex obstacle avoidance formulation, enabled by a novel approach to integrating logic. This facilitates the incorporation of an obstacle avoidance formulation into convex MPC schemes, enabling a convex optimization framework with substantially improved computational efficiency relative to conventional nonconvex methods. A key property of the formulation is that obstacle avoidance remains effective even when obstacles lie outside the prediction horizon, allowing shorter horizons for real-time deployment. In scenarios where nonconvex formulations are unavoidable, the proposed method meets or exceeds the performance of representative nonconvex alternatives. The method is evaluated in autonomous vehicle applications, where system dynamics are highly nonlinear.

</details>


### [7] [Safe Online Control-Informed Learning](https://arxiv.org/abs/2512.13868)
*Tianyu Zhou,Zihao Liang,Zehui Lu,Shaoshuai Mou*

Main category: eess.SY

TL;DR: 提出一个安全在线控制-学习框架，整合最优控制、参数估计（EKF）与软性约束，在线学习并保证收敛与安全性，在cart-pole与机器人臂等系统上验证。


<details>
  <summary>Details</summary>
Motivation: 针对安全关键的自动系统，在不确定性下需要数据高效的自适应，同时严格满足系统约束与安全边界。

Method: 将最优控制、参数估计（扩展卡尔曼滤波 EKF）与软阈值约束（softplus barrier）融入一个在线学习过程，实时更新系统参数并实现约束满足。同时通过理论分析给出收敛性与安全性保证，并在典型系统上验证。

Result: 给出收敛性与安全性定理；在cart-pole和robot-arm等系统上展示框架的有效性与鲁棒性。

Conclusion: 该框架提供一个统一的安全在线学习方案，融合控制、估计与安全约束，具备理论保证并在实际系统中表现出数据高效的自适应能力。

Abstract: This paper proposes a Safe Online Control-Informed Learning framework for safety-critical autonomous systems. The framework unifies optimal control, parameter estimation, and safety constraints into an online learning process. It employs an extended Kalman filter to incrementally update system parameters in real time, enabling robust and data-efficient adaptation under uncertainty. A softplus barrier function enforces constraint satisfaction during learning and control while eliminating the dependence on high-quality initial guesses. Theoretical analysis establishes convergence and safety guarantees, and the framework's effectiveness is demonstrated on cart-pole and robot-arm systems.

</details>


### [8] [A Fair, Flexible, Zero-Waste Digital Electricity Market: A First-Principles Approach Combining Automatic Market Making, Holarchic Architectures and Shapley Theory](https://arxiv.org/abs/2512.13871)
*Shaun Sweeney,Robert Shorten,Mark O'Malley*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This thesis presents a fundamental rethink of electricity market design at the wholesale and balancing layers. Rather than treating markets as static spot clearing mechanisms, it reframes them as a continuously online, event driven dynamical control system: a two sided marketplace operating directly on grid physics.
  Existing energy only, capacity augmented, and zonal market designs are shown to admit no shock robust Nash equilibrium under realistic uncertainty, instead relying on price caps, uplift, and regulatory intervention to preserve solvency and security. In response, the thesis develops a holarchic Automatic Market Maker (AMM) in which prices are bounded, exogenous control signals derived from physical tightness rather than emergent equilibrium outcomes.
  The AMM generalises nodal and zonal pricing through nested scarcity layers, from node to cluster to zone to region to system, such that participant facing prices inherit from the tightest binding constraint. Nodal and zonal pricing therefore emerge as special cases of a unified scarcity propagation rule.
  Beyond pricing, the AMM functions as a scarcity aware control system and a digitally enforceable rulebook for fair access and proportional allocation under shortage. Fuel costs are recovered through pay as bid energy dispatch consistent with merit order, while non fuel operating and capital costs are allocated according to adequacy, flexibility, and locational contribution.
  Large scale simulations demonstrate bounded input bounded output stability, controllable procurement costs, zero structural waste, and improved distributional outcomes. The architecture is climate aligned and policy configurable, but requires a managed transition and new operational tools for system operators and market participants.

</details>


### [9] [Data-Driven Control via Conditional Mean Embeddings: Formal Guarantees via Uncertain MDP Abstraction](https://arxiv.org/abs/2512.13940)
*Ibon Gracia,Morteza Lahijanian*

Main category: eess.SY

TL;DR: 基于数据驱动的策略合成框架，利用条件均值嵌入和不确定马尔可夫决策过程对未知动力学的随机系统给出形式化性能保证，并在温度调控基准上验证。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，系统动力学未知且需要可证明的性能保证；传统学习控制往往缺乏形式保障。本工作通过将学习结果转化为带有界限的抽象模型，实现带鲁棒性的策略合成。

Method: 从轨迹数据中学习系统状态转移核作为条件均值嵌入（CME），再构建一个有限状态的不确定MDP（UMDP）抽象，其转移不确定性覆盖学习误差和离散化误差；随后通过鲁棒动态规划在该UMDP上生成具备形式化性能界限的策略。

Result: 在温度调控基准上对所提方法进行了经验验证，证明框架能够提供可证明的性能界限并实现有效的控制性能。

Conclusion: 该框架将数据驱动学习与形式化保障结合，提供未知随机系统的可验证策略合成方法，适用于要求安全性和可证明性的重要场景；未来工作可扩展至更复杂的系统与更紧的性能界限。

Abstract: Controlling stochastic systems with unknown dynamics and under complex specifications is specially challenging in safety-critical settings, where performance guarantees are essential. We propose a data-driven policy synthesis framework that yields formal performance guarantees for such systems using conditional mean embeddings (CMEs) and uncertain Markov decision processes (UMDPs). From trajectory data, we learn the system's transition kernel as a CME, then construct a finite-state UMDP abstraction whose transition uncertainties capture learning and discretization errors. Next, we generate a policy with formal performance bounds through robust dynamic programming. We demonstrate and empirically validate our method through a temperature regulation benchmark.

</details>


### [10] [Fast Frequency Response Potential of Data Centers through Workload Modulation and UPS Coordination](https://arxiv.org/abs/2512.14128)
*Xiaojie Tao,Rajit Gadh*

Main category: eess.SY

TL;DR: 数据中心可通过实时工作负载调节和UPS协同，在低惯性电网中提供快速无功/无功？快速频率响应，降低频率谷值并缩短恢复时间，同时保障服务质量。


<details>
  <summary>Details</summary>
Motivation: 高比例可再生能源削减系统慣性，需要更快的频率响应来维持系统稳定；数据中心作为具备可控负载与就地UPS的潜在资源，具有实现快速功率支援的潜力。

Method: 建立数据中心功率消耗、IT服务器、制冷系统与储能等的耦合动态模型；提出基于频率偏差信号的控制策略，实时调整服务器功率和UPS放电；将该模型与电网频率动力学耦合，并在改进的IEEE 39–母线系统上进行仿真案例分析。

Result: 所提出的策略能够有效降低频率谷值并缩短恢復时间，同时不降低服务质量，数据中心在低惯性电网中的并网运行具备可行性。

Conclusion: 数据中心可成为未来低惯性系统中的有前景的并网支援资源，需实现与电网的实时协同控制及充分的系统建模与验证。

Abstract: The rapid growth of renewable energy sources has significantly reduced system inertia and increased the need for fast frequency response (FFR) in modern power systems. Data centers, as large and flexible electrical consumers, hold great potential to contribute to frequency stabilization due to their controllable IT workloads and on-site uninterruptible power supply (UPS) systems. This paper investigates the feasibility of leveraging data centers for providing fast frequency response through real-time workload modulation and UPS coordination. A dynamic model combining data center power consumption and grid frequency dynamics is developed, capturing the interactions between IT servers, cooling systems, and energy storage. Control strategies based on frequency deviation are implemented to adjust server power and discharge UPS batteries during frequency events. Case studies on a modified IEEE 39-bus system demonstrate that the proposed strategy can effectively reduce frequency nadir and shorten recovery time without compromising service quality. The results highlight the promising role of data centers as grid-supporting resources in future low-inertia systems.

</details>


### [11] [Coordinated Fast Frequency Response from Electric Vehicles, Data Centers, and Battery Energy Storage Systems](https://arxiv.org/abs/2512.14136)
*Xiaojie Tao,Rajit Gadh*

Main category: eess.SY

TL;DR: 提出一种协调框架，将EV车队、数据中心UPS及工作负载调制和BESS聚合，形成分层控制的快速频率响应（FFR）。在IEEE 39-总线系统的案例中，协同资源显著提高频率谷值、降低RoCoF并加速频率恢复，表明多资源聚合对低惯性电网更有价值。


<details>
  <summary>Details</summary>
Motivation: 高比例可再生能源降低系统惯性，导致需要更快的频率响应。虽然电动汽车、数据中心和BESS各自能提供亚秒级功率支撑，但它们的联合潜力尚未被系统评估，因此需要一个能整合异质资源并保证稳定可靠的FFR框架。

Method: 建立EV车队、数据中心UPS与工作负载调制、BESS的动力学模型，明确响应时间、功率上限和运行约束。提出一个分层控制架构：上层协调器根据响应速度和剩余容量动态分配FFR至各资源；下层控制器执行实际功率响应。

Result: 在IEEE 39-母线测试系统的案例研究中，协同的EV-DC-BESS框架相比单资源FFR，可使频率谷值提高多达0.2 Hz，RoCoF下降，且频率恢复加速，尤其在低惯性情景下更显著。结果表明协同多资源能够显著提升电网稳定性。

Conclusion: 多资源聚合在可再生主导的电网中具有显著价值，可用于未来的频率调控市场，提升低惯性条件下的稳定性和响应速率。

Abstract: High renewable penetration has significantly reduced system inertia in modern power grids, increasing the need for fast frequency response (FFR) from distributed and non-traditional resources. While electric vehicles (EVs), data centers, and battery energy storage systems (BESS) have each demonstrated the capability to provide sub-second active power support, their combined frequency response potential has not been systematically evaluated. This paper proposes a coordinated control framework that aggregates these heterogeneous resources to provide fast, stable, and reliable FFR. Dynamic models for EV fleets, data center UPS and workload modulation, and BESS are developed, explicitly capturing their response times, power limits, and operational constraints. A hierarchical control architecture is introduced, where an upper-level coordinator dynamically allocates FFR among resources based on response speed and available capacity, and lower-level controllers implement the actual power response. Case studies based on the IEEE 39-bus test system demonstrate that the coordinated EV-DC-BESS framework improves frequency nadir by up to 0.2 Hz, reduces RoCoF, and accelerates frequency recovery compared with single-resource FFR. Results confirm that synergistic coordination significantly enhances grid stability, especially in low-inertia scenarios. This work highlights the value of multi-resource aggregation for future frequency regulation markets in renewable-dominated grids.

</details>


### [12] [KalMRACO: Unifying Kalman Filter and Model Reference Adaptive Control for Robust Control and Estimation of Uncertain Systems](https://arxiv.org/abs/2512.14175)
*Lauritz Rismark Fosso,Christian Holden,Sveinung Johan Ohrem*

Main category: eess.SY

TL;DR: KalMRACO将卡尔曼滤波器与MRAC融合，利用MRAC的参考模型作为卡尔曼滤波系统模型，降低对底层系统参数的依赖；并在反馈律中引入对估计状态与测量的混合，以改善初始瞬态的稳定性。通过仿真和水下载具实验验证，显示对参考模型状态的更好跟踪、观测器状态收敛和降噪能力。


<details>
  <summary>Details</summary>
Motivation: 传统的卡尔曼滤波通常需要对系统参数有较为准确的先验知识，这在实际应用中往往难以满足。MRAC提供一个对未知系统行为进行跟踪的参考模型，但也依赖于对参考模型和系统间关系的正确设定。本研究旨在将两者结合，在尽量减少对系统参数知识的依赖的同时，获得稳定且可观测的跟踪性能。

Method: 将MRAC的参考模型作为卡尔曼滤波的系统模型，将两者进行统一，形成KalMRACO框架。为解决初始瞬态带来的稳定性问题，提出在反馈律中对估计状态和测量值进行混合处理的策略。通过仿真和水下车辆的实验室试验对该方法进行验证。

Result: 在仿真和实艇实验中，KalMRACO实现对参考模型状态的优于基线的跟踪、观测器状态的收敛以及更强的降噪性能。结果表明，该方法在未知或不充分参数知识条件下仍具备良好鲁棒性和实际可实现性。

Conclusion: KalMRACO显示出在需要对系统参数知之甚少的条件下，仍能实现对参考模型的稳健跟踪与观测收敛，并通过引入估计-测量混合的反馈策略提升初始瞬态的稳定性，具有对其他需要未知参数的系统的潜在适用性。

Abstract: A common assumption when applying the Kalman filter is a priori knowledge of the system parameters. These parameters are not necessarily known, and this may limit real-world applications of the Kalman filter. The well-established Model Reference Adaptive Controller (MRAC) utilizes a known reference model and ensures that the input-output behavior of a potentially unknown system converges to that of the reference model. We present KalMRACO, a unification of the Kalman filter and MRAC leveraging the reference model of MRAC as the Kalman filter system model, thus eliminating, to a large degree, the need for knowledge of the underlying system parameters in the application of the Kalman filter. We also introduce the concept of blending estimated states and measurements in the feedback law to handle stability issues during the initial transient. KalMRACO is validated through simulations and lab trials on an underwater vehicle. Results show superior tracking of the reference model state, observer state convergence, and noise mitigation properties.

</details>


### [13] [A Data-Driven Approach for Electric Vehicle Powertrain Modeling](https://arxiv.org/abs/2512.14344)
*Eymen Ipek,Mario Hirz*

Main category: eess.SY

TL;DR: 提出一个模块化的功率传动系统仿真框架，通过为电池、逆变器和电机定义标准接口，实现数据驱动、物理建模或经验模型的组件可插拔，支撑系统级虚拟验证。


<details>
  <summary>Details</summary>
Motivation: 电动化和功率传动复杂性提升，需更快速、成本效益高的开发周期；现有研究多聚焦组件级数据驱动模型，缺乏将其系统化整合的能力。

Method: 建立标准化接口，允许独立开发的模型（数据驱动、物理、经验）在模块化框架中无缝集成，以实现可扩展的系统级建模。

Result: 实现了一个能够进行系统级建模的框架，支持不同类型模型的互换与组合，旨在缩短开发周期。

Conclusion: 该方法有助于提升系统级建模的可扩展性，缩短开发时间，满足现代汽车行业对快速迭代的需求。

Abstract: Electrification in the automotive industry and increasing powertrain complexity demand accelerated, cost-effective development cycles. While data-driven models are recently investigated at component level, a gap exists in systematically integrating them into cohesive, system-level simulations for virtual validation. This paper addresses this gap by presenting a modular framework for developing powertrain simulations. By defining standardized interfaces for key components-the battery, inverter, and electric motor-our methodology enables independently developed models, whether data-driven, physics-based, or empirical, to be easily integrated. This approach facilitates scalable system-level modeling, aims to shorten development timelines and to meet the agile demands of the modern automotive industry.

</details>


### [14] [A Geometric Task-Space Port-Hamiltonian Formulation for Redundant Manipulators](https://arxiv.org/abs/2512.14349)
*Federico Califano,Camilla Rota,Riccardo Zanella,Antonio Franchi*

Main category: eess.SY

TL;DR: 提出一种基于几何端口-哈密顿的冗余机械臂模型，将动量分解为任务空间动量与零空间动量，并在7-DOF仿真中通过IDA-PBC实现阻抗稳定化与形状化控制。


<details>
  <summary>Details</summary>
Motivation: 解决冗余机械臂任务执行中的耦合控制与动量分解问题，利用几何端口-哈密顿框架提供清晰的动力学结构并揭示与拉格朗日形式的关系。

Method: 以从规范哈密顿动力学出发的坐标变换，将哈密顿动量分解为任务空间动量与零空间动量，得到新的几何端口-哈密顿模型；比较其与文献中的拉格朗日表述；在7-DOF Emika Panda 上应用IDA-PBC设计实现稳定化与阻抗形状化。

Result: 给出理论上的关系与模型性质，并通过仿真验证在IDA-PBC下的稳定性与阻抗形状控制。

Conclusion: 该几何端口-哈密顿框架为冗余机械臂的任务执行提供清晰的动力学结构与控制设计路径，有助于任务与零空间分离的能量与阻尼设计，并具推广至其他高维冗余系统的潜力。

Abstract: We present a novel geometric port-Hamiltonian formulation of redundant manipulators performing a differential kinematic task $η=J(q)\dot{q}$, where $q$ is a point on the configuration manifold, $η$ is a velocity-like task space variable, and $J(q)$ is a linear map representing the task, for example the classical analytic or geometric manipulator Jacobian matrix. The proposed model emerges from a change of coordinates from canonical Hamiltonian dynamics, and splits the standard Hamiltonian momentum variable into a task-space momentum variable and a null-space momentum variable. Properties of this model and relation to Lagrangian formulations present in the literature are highlighted. Finally, we apply the proposed model in an \textit{Interconnection and Damping Assignment Passivity-Based Control} (IDA-PBC) design to stabilize and shape the impedance of a 7-DOF Emika Panda robot in simulation.

</details>


### [15] [Equivariant Filter Cascade for Relative Attitude, Target's Angular Velocity, and Gyroscope Bias Estimation](https://arxiv.org/abs/2512.14412)
*Gil Serrano,Pedro Lourenço,Bruno J. Guerreiro,Rita Cunha*

Main category: eess.SY

TL;DR: 提出一个两级级联的等变滤波器（EqF）来解决追逐与对接中对未合作目标的相对姿态与角速率估计问题。第一阶段在星敏感器数据下估计追逐航天器的姿态与陀螺仪偏差，第二阶段利用目标帧中固定的两个非共线向量观测来估计相对姿态与目标角速率。理论证明了EqF级联的稳定性，仿真结果显示该滤波级联的性能。


<details>
  <summary>Details</summary>
Motivation: 在对接中需实现追逐航天器与未合作目标的姿态同步，且目标不可观测且陀螺仪存在偏差，传统滤波器在此场景中可能难以提供带稳定性保证的鲁棒估计。因此，提出基于等变滤波的分级级联方法以获得稳定的相对姿态与角速度估计。

Method: 使用两级级联的等变滤波器。第一阶段利用星敏感器观测来估计追逐航天器的姿态和陀螺仪偏差；第二阶段利用目标帧中固定的两个非共线向量的观测来估计相对姿态和目标的角速度；并给出理论上的稳定性分析。

Result: 给出级联EqF的稳定性证明，且通过仿真验证了滤波级联在该任务中的有效性与性能。

Conclusion: EqF级联系统为追逐与对接中的未合作目标场景提供了一个理论上可证且具实际鲁棒性的解耦估计框架，能够在存在陀螺仪偏差的条件下准确估计相对姿态与目标角速度，且具备扩展到更多传感器的潜力。

Abstract: Rendezvous and docking between a chaser spacecraft and an uncooperative target, such as an inoperative satellite, require synchronization between the chaser spacecraft and the target. In these scenarios, the chaser must estimate the relative attitude and angular velocity of the target using onboard sensors, in the presence of gyroscope bias. In this work, we propose a cascade of Equivariant Filters (EqF) to address this problem. The first stage of the cascade estimates the chaser's attitude and the bias, using measurements from a star tracker, while the second stage of the cascade estimates the relative attitude and the target's angular velocity, using observations of two known, non-collinear vectors fixed in the target frame. The stability of the EqF cascade is theoretically analyzed and simulation results demonstrate the filter cascade's performance.

</details>


### [16] [Nonlinear System Identification Nano-drone Benchmark](https://arxiv.org/abs/2512.14450)
*Riccardo Busetto,Elia Cereda,Marco Forgione,Gabriele Maroni,Dario Piga,Daniele Palossi*

Main category: eess.SY

TL;DR: 提出了一个基于Crazyflie 2.1的真实世界系统辨识基准，包含75k条样本、4条激进轨迹、4维电机输入与13维观测输出，提供多步与单步预测的多步误差传播评测，与数据、基线模型及实现开放源代码以便公平比较。


<details>
  <summary>Details</summary>
Motivation: 在微型无人机领域，真实世界的噪声、执行器非线性与开放环不稳定性使系统辨识在 Agile、小型飞行器上具有挑战性。目前缺乏一个统一、可比的基准来评价不同辨识方法在多步预测下的泛化与鲁棒性，因此需要一个带有多步预测指标的公开数据集与基线实现来促进研究。

Method: 采集Crazyflie 2.1子体积无人机的75k个真实样本，覆盖四条高难度轨迹；数据包括4维电机输入和13维输出测量，形成多输入多输出(MIMO)系统。基准包含一组多步预测评测指标，评估单步和多步误差传播。除了数据，还提供平台描述、实验设置、以及基线模型以突出在真实世界噪声与执行器非线性下的预测挑战，并开源数据、脚本与实现。

Result: 提供了一个具有挑战性的公开基准，便于对比不同辨识方法在真实世界MIMO系统中的预测能力，尤其是在多步误差传播方面的表现，促进在小型化、敏捷飞行器中的算法研究。

Conclusion: 数据、基线与实现的开源将促进透明的算法比较并推动对 agile、微型航拍机器人在现实环境中的系统辨识研究。

Abstract: We introduce a benchmark for system identification based on 75k real-world samples from the Crazyflie 2.1 Brushless nano-quadrotor, a sub-50g aerial vehicle widely adopted in robotics research. The platform presents a challenging testbed due to its multi-input, multi-output nature, open-loop instability, and nonlinear dynamics under agile maneuvers. The dataset comprises four aggressive trajectories with synchronized 4-dimensional motor inputs and 13-dimensional output measurements. To enable fair comparison of identification methods, the benchmark includes a suite of multi-horizon prediction metrics for evaluating both one-step and multi-step error propagation. In addition to the data, we provide a detailed description of the platform and experimental setup, as well as baseline models highlighting the challenge of accurate prediction under real-world noise and actuation nonlinearities. All data, scripts, and reference implementations are released as open-source at https://github.com/idsia-robotics/nanodrone-sysid-benchmark to facilitate transparent comparison of algorithms and support research on agile, miniaturized aerial robotics.

</details>


### [17] [Equivariant Observer for Bearing Estimation with Linear and Angular Velocity Inputs](https://arxiv.org/abs/2512.14451)
*Gil Serrano,Marcelo Jacinto,Bruno J. Guerreiro,Rita Cunha*

Main category: eess.SY

TL;DR: 设计一个在单位球上等变观测器，扩展了 bearing 向量的动力学，加入投影到切空间的线性速度输入，并通过提升到 SO(3) 的运动学实现观测器，得到几乎全局渐近稳定性，仿真验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在图像基视觉伺服中需要对 Bearing 向量进行稳定估计，同时考虑Vehicle与目标特征之间的相对速度；现有方法在单位球面上缺乏等变性，难以容纳额外的线性速度输入。

Method: 将单位球面上的动力学提升到 SO(3)，设计一个等变观测器来估计 bearing 向量，并引入一个投影到单位球切空间的线性速度输入；证明观测器在理论意义上的几乎全局渐近稳定性；并给出在原状态流形上的等变观测器表达形式。

Result: 给出理论证明：该观测器具备几乎全局渐近稳定性；通过数值仿真验证在考虑相对速度的情形下，观测器能够稳定估计 bearing。

Conclusion: 扩展的等变观测框架适用于图像基视觉伺服中的稳定 bearing 估计，并能处理目标与载具之间的相对线性速度；为在原状态流形的实现提供了可行路径，后续可拓展到更高阶动力学或实际系统噪声模型。

Abstract: This work addresses the problem of designing an equivariant observer for a first order dynamical system on the unit-sphere. Building upon the established case of unit bearing vector dynamics with angular velocity inputs, we introduce an additional linear velocity input projected onto the unit-sphere tangent space. This extended formulation is particularly useful in image-based visual servoing scenarios where stable bearing estimates are required and the relative velocity between the vehicle and target features must be accounted for. Leveraging lifted kinematics to the Special Orthogonal group, we design an observer for the bearing vector and prove its almost global asymptotic stability. Additionally, we demonstrate how the equivariant observer can be expressed in the original state manifold. Numerical simulation results validate the effectiveness of the proposed algorithm.

</details>


### [18] [Closed-Loop Consistent, Causal Data-Driven Predictive Control via SSARX](https://arxiv.org/abs/2512.14510)
*Aihui Liu,Magnus Jansson*

Main category: eess.SY

TL;DR: 提出一种无 Willems 基本引理的 DDPC 方案，基于 SSARX 直接从输入输出数据推导出类似 MPC 的策略；避免堆叠 Hankel 矩阵与 DeePC 的变量 g；通过高阶 ARX 提估预测/观测马尔可夫参数，并回归学习多步过去到未来的映射（可降秩约束），预测器严格因果，便于嵌入 MPC；在带噪声的闭环数据中表现与其他方法竞争力强。


<details>
  <summary>Details</summary>
Motivation: 旨在摆脱 Willems 基本引理及 DeePC 的局限性（Hankel 矩阵及变量 g），实现一个闭环一致、因果的 DDPC，使其能自然集成到 MPC 框架，同时提高对测量与过程噪声的鲁棒性。

Method: 1) 通过高阶 ARX 模型估计预测器/观测器的马尔可夫参数以解耦噪声；2) 通过回归学习多步 past-to-future 映射，并在需要时加入降秩约束；3) 将 SSARX 预测器作为严格因果的组件融入 MPC 框架，实现闭环数据驱动控制。

Result: 实验表明，SSARX 在带测量与过程噪声的闭环数据下，与其他 DDPC/数据驱动方法具有竞争力。

Conclusion: 基于 SSARX 的 DDPC 提供了一种无 fundamental lemma 的数据驱动控制框架，具备因果性和闭环一致性，避免 Hankel/ g 的依赖，在含噪声场景中表现良好。

Abstract: We propose a fundamental-lemma-free data-driven predictive control (DDPC) scheme for synthesizing model predictive control (MPC)-like policies directly from input-output data. Unlike the well-known DeePC approach and other DDPC methods that rely on Willems' fundamental lemma, our method avoids stacked Hankel representations and the DeePC decision variable g. Instead, we develop a closed-loop consistent, causal DDPC scheme based on the multi-step predictor Subspace-ARX (SSARX). The method first (i) estimates predictor/observer Markov parameters via a high-order ARX model to decouple the noise, then (ii) learns a multi-step past-to-future map by regression, optionally with a reduced-rank constraint. The SSARX predictor is strictly causal, which allows it to be integrated naturally into an MPC formulation. Our experimental results show that SSARX performs competitively with other methods when applied to closed-loop data affected by measurement and process noise.

</details>


### [19] [Scalable Nonlinear DeePC: Bridging Direct and Indirect Methods and Basis Reduction](https://arxiv.org/abs/2512.14535)
*Thomas O. de Jong,Mircea Lazar,Siep Weiland,Florian Dörfler*

Main category: eess.SY

TL;DR: Pi正则化下的DeePC与SPC在非线性框架下的关系：通过SVD降维实现与SPC等价的可扩展性；引入稀疏基选择以获得更小的基集合。在无噪声下，DeePC与SPC在大惩罚下表现相同；在有噪声时，适当调参的DeePC可显著降低AMEn，与SPC相比具有鲁棒性优势。


<details>
  <summary>Details</summary>
Motivation: 建立DeePC与SPC在一般基函数情形下的理论联系，解决高维数据下的计算可扩展性，并提高在噪声测量中的鲁棒性。

Method: 把Pi正则化推广到通用基函数；证明在适当条件下，基函数DeePC是基函数SPC的松弛；提出基于SVD的降维保持与SPC等价性，并推导降维后的Pi正则化；提出基于LASSO的稀疏基选择以从 lifted 数据中获得子集基。

Result: 理论上给出DeePC对SPC的松弛/等价关系；提供可保持等价性的降维方法及降维后的一致正则化形式；在非线性van der Pol模型仿真中，噪声为零时大惩罚下DeePC与SPC的AMEn相同；在带噪声时，经过正则化调参的DeePC能显著降低AMEn，优于SPC。

Conclusion: 基于基函数的DeePC可通过正则化与降维实现对SPC的等价与近似，并通过稀疏基选择提升可扩展性与鲁棒性，尤其在有噪声条件下。

Abstract: This paper studies regularized data-enabled predictive control (DeePC) within a nonlinear framework and its relationship to subspace predictive control (SPC). The $Π$-regularization is extended to general basis functions and it is shown that, under suitable conditions, the resulting basis functions DeePC formulation constitutes a relaxation of basis functions SPC. To improve scalability, we introduce an SVD-based dimensionality reduction that preserves the equivalence with SPC, and we derive a reduced Π-regularization. A LASSO based sparse basis selection method is proposed to obtain a reduced basis from lifted data. Simulations on a nonlinear van der Pol oscillator model indicate that, in the absence of noise, DeePC and SPC yield equivalent absolute mean tracking errors (AMEs) when large penalties are applied. In contrast, under noisy measurements, careful tuning of the DeePC regularization results in a reduced AME, outperforming SPC.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [20] [Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2512.13703)
*Fan Yang*

Main category: cs.CR

TL;DR: 提出 Safe2Harm：基于语义同构的LLM越狱攻击框架，通过将有害问题安全化再映射回有害输出，声称在多模型和数据集上表现优越，并附带一个有害内容检测数据集以评估防护能力。


<details>
  <summary>Details</summary>
Motivation: 当前越狱方法多依赖提示工程或对抗优化，但忽视许多有害场景在原理层面与合规场景高度一致的现象，因此提出语义同构攻击以提升越狱效率与适用性。

Method: 四阶段流程：1) 将有害问题改写为语义安全的问题；2) 提取两者之间的主题映射关系；3) 让LLM针对安全问题生成详细回答；4) 基于映射关系反向改写回答以获得有害输出。

Result: 在7大主流LLM及三类基准数据集上显示强越狱能力，整体性能优于现有方法；并构建358样本的有害内容评估数据集，用于评估现有有害检测方法与对输入输出过滤的有效性。

Conclusion: 提出新的攻击框架及评估资源，提示安全防护需关注语义同构攻击的潜在威胁，并相应加强检测与防护策略。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.

</details>


### [21] [Stability-Drift Early Warning for Cyber-Physical Systems Under Degradation Attacks](https://arxiv.org/abs/2512.13767)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.CR

TL;DR: 提出一种基于稳定性漂移的早期警告方法，通过在短期内预测状态转移与观测之间的差异来检测慢性降级，从而在可观测轨迹或估计器残差出现之前发出警告；该方法在PX4 x500的软件在环环境下经过两种现实降级情景验证，能在数秒前给出预警并且在 nominal 和非降级的剧烈飞行中保持稳定。


<details>
  <summary>Details</summary>
Motivation: 在 CPS/无人机等系统中，慢性降级（如传感器偏置缓慢累积或控制环路时序波动）会逐步削弱系统稳定性，但常规监控仍显示正常，导致难以及时发现潜在风险。需要一种在初期阶段就能识别降级的检测方法来提供提前干预的机会。

Method: 提出一种外部于飞控堆栈的稳定性漂移指标，通过在短期预测与实际观测之间的状态转移差异来衡量系统的不稳定增长。通过跟踪该差异随时间的缓慢增大来识别潜在不稳定性。该方法仅依赖标准遥测数据，无需修改自动驾驶仪固件，适合现有飞控系统的部署。

Result: 在 PX4 x500 的软件在环环境中，针对两种现实降级情景（逐步的 IMU 偏置漂移和控制循环中的时序不规则性）进行评估。稳定性漂移指标在降级发生前的数秒内给出持续的早期警告信号，且在 nominal 以及非降级的剧烈飞行中保持稳定。

Conclusion: 稳定性漂移可作为 UAV 控制系统中早期降级的实用指示器，为在进入不稳定前的预警提供额外时间，辅助现有安全机制，并在慢速、隐蔽性攻击场景下实现安全模式切换等 mitigation 策略。

Abstract: Cyber-physical systems (CPS) such as unmanned aerial vehicles are vulnerable to slow degradation that develops without causing immediate or obvious failures. Small sensor biases or timing irregularities can accumulate over time, gradually reducing stability while standard monitoring mechanisms continue to report normal operation. Detecting this early phase of degradation remains a challenge, as most existing approaches focus on abrupt faults or visible trajectory deviations. This paper introduces an early warning method based on stability drift, which measures the divergence between predicted and observed state transitions over short horizons. By tracking the gradual growth of this divergence, the proposed approach identifies emerging instability before it becomes visible in the flight trajectory or estimator residuals. The method operates externally to the flight stack and relies only on standard telemetry, making it suitable for deployment without modifying autopilot firmware. The approach was evaluated on a PX4 x500 platform in a software in the loop environment under two realistic degradation scenarios, gradual IMU bias drift and timing irregularities in the control loop. In both cases, the stability drift metric provided a consistent early warning signal several seconds before visible instability appeared, while remaining stable during nominal and aggressive but non degraded flight. The results demonstrate that stability drift can serve as a practical indicator of early degradation in UAV control systems. By providing advance notice during a pre instability phase, the proposed method complements existing safety mechanisms and offers additional time for mitigation or safe mode transitions under slow and subtle attacks.

</details>


### [22] [A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis](https://arxiv.org/abs/2512.14045)
*Omar Abusabha,Jiyong Uhm,Tamer Abuhmed,Hyungjoon Koo*

Main category: cs.CR

TL;DR: 本文对函数内联在机器学习辅助的二进制分析中的安全影响进行首次综合研究，揭示极端内联会改变静态特征与控制流、可能被用于规避模型，并且不同应用/构建配置下的内联比率差异显著，挑战 ML 模型的训练与评估稳健性。


<details>
  <summary>Details</summary>
Motivation: 在现代编译器中，函数内联广泛提升性能，但对静态特征如机器指令与控制流图的影响显著，这些特征对二进制分析至关重要。尽管如此，内联的安全影响尚缺乏系统研究。本研究尝试从机器学习辅助的二进制分析角度，揭示内联对安全分析的潜在影响。

Method: 分析 LLVM 的成本模型中的内联决策流水线，构造极端内联的编译配置。围绕五项 ML 辅助的安全分析任务，使用 20 个模型，评估在极端内联条件下的鲁棒性。

Result: 研究发现：i) 内联在初衷上可能是无害，但会直接或间接影响 ML 模型行为，甚至被用于规避判别或生成模型；ii) 依赖静态特征的模型对内联高度敏感；iii) 微妙的编译设置可用以刻意构造规避性二进制变体；iv) 不同应用与构建配置的内联比率差异显著，削弱了对训练与评估的一致性假设。

Conclusion: 应在训练与评估中将内联引发的变异性纳入考量，提升模型对内联变化的鲁棒性，或设计对内联不敏感的特征与评估数据集，确保 ML 在安全相关的二进制分析中的可靠性。

Abstract: A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.

</details>


### [23] [Lost in the Pages: WebAssembly Code Recovery through SEV-SNP's Exposed Address Space](https://arxiv.org/abs/2512.14376)
*Markus Berthilsson,Christian Gehrmann*

Main category: cs.CR

TL;DR: 提出在 TEEs 中利用暴露的地址空间信息对 Wasm 代码进行机密性攻击的攻击方法。攻击通过提取关键执行特征并结合其他侧信道，在大多数场景下能够恢复超过 70% 的 Wasm 代码，显著高于此前在 Intel SGX 上约 50% 的水平。


<details>
  <summary>Details</summary>
Motivation: WebAssembly 作为跨平台执行单位在分布式与异构环境中广泛应用；然而将 Wasm 运行在TEEs上可能暴露新的代码机密性攻击面，现有工作仅针对 SGX，亟需扩展到更广泛的 TEEs 并评估在 Wasm 上的安全性。

Method: 在 TEEs 暴露的地址空间信息基础上，提取关键执行特征；并与额外的侧信道信息结合，构建能够高概率恢复大量 Wasm 代码的攻击流程。

Result: 在多数场景中，攻击能够高可靠地提取超过 70% 的 Wasm 代码，相比此前对 SGX 的单步执行攻击（约可获得 50%），具有显著提升。

Conclusion: 这项研究揭示了 TEEs 上 Wasm 的代码机密性面临的重大风险，强调需要在架构、编译/部署和硬件层面加强防护，未来工作包括对攻击的泛化、鲁棒性提升以及有效的对抗措施的研究。

Abstract: WebAssembly (Wasm) has risen as a widely used technology to distribute computing workloads on different platforms. The platform independence offered through Wasm makes it an attractive solution for many different applications that can run on disparate infrastructures. In addition, Trusted Execution Environments (TEEs) are offered in many computing infrastructures, which allows also running security sensitive Wasm workloads independent of the specific platforms offered. However, recent work has shown that Wasm binaries are more sensitive to code confidentiality attacks than native binaries. The previous result was obtained for Intel SGX only. In this paper, we take this one step further, introducing a new Wasm code-confidentiality attack that exploits exposed address-space information in TEEs. Our attack enables the extraction of crucial execution features which, when combined with additional side channels, allows us to with high reliability obtain more than 70% of the code in most cases. This is a considerably larger amount than was previously obtained by single stepping Intel SGX where only upwards to 50% of the code could be obtained.

</details>


### [24] [VICTOR: Dataset Copyright Auditing in Video Recognition Systems](https://arxiv.org/abs/2512.14439)
*Quan Yuan,Zhikun Zhang,Linkang Du,Min Chen,Mingyang Sun,Yunjun Gao,Shibo He,Jiming Chen*

Main category: cs.CR

TL;DR: 提出 VICTOR：首个用于视频识别系统的数据集版权审计方法，通过对少量样本（约 1%）进行隐蔽修改放大目标模型的输出差异，以区分已发布修改样本与未修改原样本，从而实现对数据集的版权审计，且对训练视频和模型扰动具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着视频识别系统在日常应用中的广泛部署，公开数据集的版权保护日益重要。然而现有数据集版权审计研究多聚焦于图像域，视频数据的时序性和复杂性使得在视频域开展审计具有更高的挑战性。需一种能够隐蔽修改样本、在不同模型上可泛化且对对抗扰动具鲁棒性的审计方法。

Method: 提出一个通用且隐蔽的样本修改策略，针对少量样本（如 1%）进行修改，提升目标模型的输出差异。对比已发布的被修改样本与未修改的原始样本在目标模型上的行为差异，作为数据集版权审计的关键依据。通过在多种模型和数据集上的广泛实验，验证方法的有效性，并在训练视频或目标模型存在扰动时仍保持鲁棒性。

Result: 实验结果表明，VICTOR 在多模型、多数据集场景下优于基线，能够在保持隐蔽性前提下显著放大对数据集版权异常使用的检测信号，同时对常见的扰动具有较好的鲁棒性。

Conclusion: VICTORloid（应为 VICTOR）首次为视频识别系统提供数据集版权审计的解决方案，展示了在视频域实现高效、隐蔽且鲁棒的审计能力，具备在实际数据集版权保护场景中的应用潜力。

Abstract: Video recognition systems are increasingly being deployed in daily life, such as content recommendation and security monitoring. To enhance video recognition development, many institutions have released high-quality public datasets with open-source licenses for training advanced models. At the same time, these datasets are also susceptible to misuse and infringement. Dataset copyright auditing is an effective solution to identify such unauthorized use. However, existing dataset copyright solutions primarily focus on the image domain; the complex nature of video data leaves dataset copyright auditing in the video domain unexplored. Specifically, video data introduces an additional temporal dimension, which poses significant challenges to the effectiveness and stealthiness of existing methods.
  In this paper, we propose VICTOR, the first dataset copyright auditing approach for video recognition systems. We develop a general and stealthy sample modification strategy that enhances the output discrepancy of the target model. By modifying only a small proportion of samples (e.g., 1%), VICTOR amplifies the impact of published modified samples on the prediction behavior of the target models. Then, the difference in the model's behavior for published modified and unpublished original samples can serve as a key basis for dataset auditing. Extensive experiments on multiple models and datasets highlight the superiority of VICTOR. Finally, we show that VICTOR is robust in the presence of several perturbation mechanisms to the training videos or the target models.

</details>


### [25] [PrivATE: Differentially Private Average Treatment Effect Estimation for Observational Data](https://arxiv.org/abs/2512.14557)
*Quan Yuan,Xiaochen Li,Linkang Du,Min Chen,Mingyang Sun,Yunjun Gao,Shibo He,Jiming Chen,Zhikun Zhang*

Main category: cs.CR

TL;DR: PrivATE提出了一种可在差分隐私保护下估计ATE的实用框架，提供标签级和样本级两种隐私保护等级，通过自适应匹配限制在噪声与匹配误差之间取得平衡，从而在各种数据集和隐私预算下实现更准确的ATE估计。


<details>
  <summary>Details</summary>
Motivation: 在观测数据中估计因果效应（ATE）面临严重的隐私风险；现有的差分隐私ATE方法存在假设受限、保护不足或信息保护不全的问题，亟需一个更实际且可调的隐私保护框架。

Method: 提出PrivATE框架，支持两种隐私保护等级（标签级和样本级），并推导自适应的匹配限制以平衡噪声引入与匹配误差，从而提高ATE估计的准确性。

Result: 在多数据集和不同隐私预算下，PrivATE对基线方法均表现出更优的性能，验证了其有效性。

Conclusion: PrivATE为隐私保护下的因果推断提供了一个可执行且可调的解决方案，能够在不同隐私需求场景下实现更准确的ATE估计。

Abstract: Causal inference plays a crucial role in scientific research across multiple disciplines. Estimating causal effects, particularly the average treatment effect (ATE), from observational data has garnered significant attention. However, computing the ATE from real-world observational data poses substantial privacy risks to users. Differential privacy, which offers strict theoretical guarantees, has emerged as a standard approach for privacy-preserving data analysis. However, existing differentially private ATE estimation works rely on specific assumptions, provide limited privacy protection, or fail to offer comprehensive information protection.
  To this end, we introduce PrivATE, a practical ATE estimation framework that ensures differential privacy. In fact, various scenarios require varying levels of privacy protection. For example, only test scores are generally sensitive information in education evaluation, while all types of medical record data are usually private. To accommodate different privacy requirements, we design two levels (i.e., label-level and sample-level) of privacy protection in PrivATE. By deriving an adaptive matching limit, PrivATE effectively balances noise-induced error and matching error, leading to a more accurate estimate of ATE. Our evaluation validates the effectiveness of PrivATE. PrivATE outperforms the baselines on all datasets and privacy budgets.

</details>


### [26] [PerProb: Indirectly Evaluating Memorization in Large Language Models](https://arxiv.org/abs/2512.14600)
*Yihan Liao,Jacky Keung,Xiaoxue Ma,Jingyu Zhang,Yicheng Sun*

Main category: cs.CR

TL;DR: PerProb: a label-free, model- and task-agnostic framework to indirectly measure LLM memorization and privacy risk, enabling evaluation of mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns from training data and the unclear impact of MIA on LLMs; need standardized, scalable evaluation due to undisclosed training sets.

Method: Compute changes in perplexity and average log probability between data from victim vs adversary models; classify MIA into four patterns; evaluate on five datasets; tests in black-box/white-box; assess several mitigations.

Result: PerProb provides a unified framework, shows varying memory behavior across LLMs; demonstrates effectiveness of distillation, early stopping, DP in reducing leakage.

Conclusion: PerProb offers practical, generalizable approach to evaluating and improving LLM privacy; can guide privacy-aware model evaluation and defense design.

Abstract: The rapid advancement of Large Language Models (LLMs) has been driven by extensive datasets that may contain sensitive information, raising serious privacy concerns. One notable threat is the Membership Inference Attack (MIA), where adversaries infer whether a specific sample was used in model training. However, the true impact of MIA on LLMs remains unclear due to inconsistent findings and the lack of standardized evaluation methods, further complicated by the undisclosed nature of many LLM training sets. To address these limitations, we propose PerProb, a unified, label-free framework for indirectly assessing LLM memorization vulnerabilities. PerProb evaluates changes in perplexity and average log probability between data generated by victim and adversary models, enabling an indirect estimation of training-induced memory. Compared with prior MIA methods that rely on member/non-member labels or internal access, PerProb is independent of model and task, and applicable in both black-box and white-box settings. Through a systematic classification of MIA into four attack patterns, we evaluate PerProb's effectiveness across five datasets, revealing varying memory behaviors and privacy risks among LLMs. Additionally, we assess mitigation strategies, including knowledge distillation, early stopping, and differential privacy, demonstrating their effectiveness in reducing data leakage. Our findings offer a practical and generalizable framework for evaluating and improving LLM privacy.

</details>


### [27] [Cryptographic transformations over polyadic rings](https://arxiv.org/abs/2512.12580)
*Steven Duplij,Na Fu,Qiang Guo*

Main category: cs.CR

TL;DR: Introduces a cryptographic paradigm based on nonderived polyadic algebraic structures (m-ary addition, n-ary multiplication) and polyadic integers; uses a non-injective, non-surjective, multivalued parameter-to-arity mapping Φ(a,b) to secure encryption; presents two schemes encoding plaintext via polyadic ring parameters and polyadically quantized signals; aims to improve cryptographic security.


<details>
  <summary>Details</summary>
Motivation: Binary cryptosystems rely on well-understood algebraic structures that may be vulnerable to cryptanalysis. By moving to polyadic (higher-arity) operations with a complex, non-unique arity mapping, the work seeks to increase cryptographic hardness and resilience against standard attacks.

Method: Construct polyadic integers as congruence classes endowed with m-ary addition and n-ary multiplication. Define a parameter-to-arity mapping Φ(a,b) = (m,n) that ties class-defining parameters to required arities for algebraic closure. Propose two encryption procedures: one encodes plaintext via additive arities m_i and uses summation; the other encodes plaintext via ring parameter a_i and uses multiplication. The polyadic quantization of operations yields systems of equations that are easy for legitimate recipients with the correct key but hard for outsiders.

Result: Establishes a theoretical foundation for polyadic cryptosystems. Outlines two concrete encryption methods and demonstrates how polyadic quantization creates cryptographic hardness; no empirical results provided but a conceptual framework and security intuition are given.

Conclusion: The work lays the groundwork for a new class of encryption schemes based on polyadic rings, highlighting potential for robust, next-generation cryptographic protocols. Further analysis is needed to assess security, practicality, and resistance to cryptanalytic attacks.

Abstract: This article introduces a novel cryptographic paradigm based on nonderived polyadic algebraic structures. Traditional cryptosystems rely on binary operations within groups, rings, or fields, whose well-understood properties can be exploited in cryptanalysis. To overcome these vulnerabilities, we propose a shift to polyadic rings, which generalize classical rings by allowing operations of higher arity: an $m$-ary addition and an $n$-ary multiplication. The foundation of our approach is the construction of polyadic integers -- congruence classes of ordinary integers endowed with such $m$-ary and $n$-ary operations. A key innovation is the parameter-to-arity mapping $Φ(a,b)=(m,n)$, which links the parameters $(a,b)$ defining a congruence class to the specific arities required for algebraic closure. This mapping is mathematically intricate: it is non-injective, non-surjective, and multivalued. This complex, non-unique relationship forms the core of the proposed cryptosystem's security. We present two concrete encryption procedures that leverage this structure by encoding plaintext within the parameters of polyadic rings and transmitting information via polyadically quantized analog signals. In one method, plaintext is linked to the additive arity $m_{i}$ and secured using the summation of such signals; in the other, it is linked to a ring parameter $a_{i}$ and secured using their multiplication. In both cases, the "quantized" nature of polyadic operations generates systems of equations that are straightforward for a legitimate recipient with the correct key but exceptionally difficult for an attacker without it. The resulting framework promises a substantial increase in cryptographic security. This work establishes the theoretical foundation for this new class of encryption schemes and highlights their potential for constructing robust, next-generation cryptographic protocols.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 通过混合训练在对大型语言模型进行专业任务微调时解决灾难性遗忘的问题。1:1 的数学与NLI混合训练在不损失数学性能的前提下，显著保持或提升NLI能力；甚至最小的NLI暴露量（约6.2%）也可提供有效正则化。


<details>
  <summary>Details</summary>
Motivation: 在对大语言模型进行专门化微调时，容易遗忘先前学到的通用能力。本研究评估了在深度学习数学任务上微调时的遗忘现象，并尝试通过混合训练缓解该问题。

Method: 在Flan-T5-Base(250M)上对DeepMind Mathematics数据集进行微调，并在MultiNLI上衡量遗忘。将数学任务与NLI任务混合训练，系统性地从1:1到15:1的混合比率进行探索，比较数学准确率和NLI准确率的变化。

Result: 仅进行数学训练将数学准确率从3.1%提升至12.0%，但NLI准确率从81.0%降至16.5%，在前1000步内发生64.5个百分点的下降。混合训练则完全消除了灾难性遗忘，且保持数学性能：1:1比率下数学仍为12.0%，NLI为86.2%。进一步探索1:1至15:1的混合比率，发现即使NLI暴露仅6.2%也能提供有效正则化。

Conclusion: 专门化不必以牺牲通用能力为代价；混合训练不仅可防止遗忘，还可能在扩展到更大模型时带来额外收益。

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [29] [Variational Physics-Informed Ansatz for Reconstructing Hidden Interaction Networks from Steady States](https://arxiv.org/abs/2512.13708)
*Kaiming Luo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The interaction structure of a complex dynamical system governs its collective behavior, yet existing reconstruction methods struggle with nonlinear, heterogeneous, and higher-order couplings, especially when only steady states are observable. We propose a Variational Physics-Informed Ansatz (VPIA) that infers general interaction operators directly from heterogeneous steady-state data. VPIA embeds the steady-state constraints of the dynamics into a differentiable variational representation and reconstructs the underlying couplings by minimizing a physics-derived steady-state residual, without requiring temporal trajectories, derivative estimation, or supervision. Residual sampling combined with natural-gradient optimization enables scalable learning of large and higher-order networks. Across diverse nonlinear systems, VPIA accurately recovers directed, weighted, and multi-body structures under substantial noise, providing a unified and robust framework for physics-constrained inference of complex interaction networks in settings where only snapshot observations are available.

</details>


### [30] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: 利用 Sentinel-1 SAR 与环境及水文要素，结合机器学习实现 Nyando 河流流域的洪水易感性制图；随机森林表现最佳（准确度0.762），揭示靠近维多利亚湖的 Kano Plains 区域高度易受洪水影响。


<details>
  <summary>Details</summary>
Motivation: 在数据匮乏地区提升洪水风险评估与管理能力，通过整合遥感（SAR）与环境数据，利用集成学习提高洪水易感性制图的准确性与可应用性，支持灾害风险减少、土地利用规划和预警系统的发展。

Method: 以 Sentinel-1 双极化 SAR 数据（2024年5月洪水事件）构建二值洪水清单作为训练数据；选取六个条件因子（坡度、海拔、方位、土地覆盖、土壤类型、距水道距离）与 SAR 洪水清单耦合，训练四种监督分类器：逻辑回归（LR）、分类回归树（CART）、支持向量机（SVM）、随机森林（RF）。模型以准确度、Kappa 系数、ROC 指标评估。

Result: RF 获得最高的预测性能：准确度0.762，Kappa0.480，显著优于 LR、CART、SVM。基于 RF 的易感性图显示在Lake Victoria 附近的 Kano Plains 低洼区具有最高洪水易感性，与历史洪水记录及2024年5月洪灾一致。结果表明将 SAR 数据与集合学习方法结合，在数据受限地区也可实现有效洪水易感性制图。

Conclusion: 将 SAR 数据与集成机器学习相结合的方法对数据受限地区的洪水易感性制图具有显著价值，可为灾害风险降低、土地利用规划和早期预警系统提供关键地图与决策支持。

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [31] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: Hessian Reassignment is a model-agnostic, two-step class-level unlearning method for document classifiers that uses a Hessian-vector update to subtract the deleted-class influence and enforces Top-1 decisions, achieving near full retrain performance with much faster speed and reduced membership-inference risk.


<details>
  <summary>Details</summary>
Motivation: To address the gap in efficient, principled class-level unlearning for document classifiers, enabling removal of a target class’s influence without full retraining and with privacy protections.

Method: 1) Perform a single influence-style update by solving a Hessian-vector system with conjugate gradients to subtract the influence of all training points from the target class, using only gradient and Hessian-vector products. 2) Enforce a Top-1 decision-space guarantee for the deleted class rather than random reclassification, improving reliability of unlearning.

Result: On standard text benchmarks, retained-class accuracy is close to what would be achieved by full retraining without the deleted class, with orders of magnitude faster runtime. It also consistently lowers membership-inference advantage on the removed class, as measured by pooled multi-shadow attacks.

Conclusion: Hessian Reassignment provides a practical, principled path to efficient class unlearning in document classification, and is model-agnostic and effective in preserving accuracy while reducing privacy leakage.

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [32] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: A ML framework integrating wastewater surveillance with meteorological and air quality data to predict RSV-associated hospitalizations in the US, using CART, Random Forest, and Boosting; wastewater level is the strongest predictor; disparities observed in Native American/Alaska Native populations; high-altitude states show higher rates; interactive Shiny dashboard for exploration.


<details>
  <summary>Details</summary>
Motivation: RSV causes substantial pediatric hospitalizations; forecasting outbreaks using environmental and surveillance data can improve timely public health responses and resource allocation.

Method: Combine weekly hospitalization rates with wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Train classification models (CART, Random Forest, Boosting) to categorize weekly RSV-associated hospitalization risk into Low, Alert, and Epidemic.

Result: Wastewater RSV level identified as the strongest predictor, with temperature, ozone, and specific humidity among important variables. Notable higher hospitalization rates among Native Americans/Alaska Natives. High-altitude states exhibit higher rates. An interactive R Shiny dashboard was developed for exploration and forecasting.

Conclusion: Integrating environmental and community surveillance data enhances outbreak forecasting and supports timely interventions and resource planning.

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [33] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: 提出一种两阶段的联邦少样本学习（FFSL）框架，用于个人化 EEG 疾病检测，在隐私约束下实现跨机构知识迁移与病人级微调。Stage 1 在非IID的医院站点上对预训练的生物信号变换器 BIOT 进行联邦学习微调；Stage 2 利用五段标注 EEG 片段进行联邦少样本个性化，为每位患者定制分类器。结果显示 Stage 1 相较中心化基线略有下降；FFSL 阶段在四个站点实现较好个性化性能。


<details>
  <summary>Details</summary>
Motivation: 在医疗场景中，EEG 数据稀缺且分布于多家机构，数据共享受隐私约束，难以构建大规模中心化模型；提出 FFSL 以实现跨站点知识迁移和患者级别个性化，同时保护隐私。

Method: Stage 1：在非IID分布的医院站点上对预训练的 BIOT 进行联邦学习微调，以实现共享表征学习；Stage 2：对每位患者使用仅五段带标签 EEG 片段进行联邦少样本个性化，保持跨站点知识的同时适应个体差异。评估基于 TUH Event Corpus 的六类事件。

Result: Stage 1：平衡准确度 0.43（中心化 0.52）、Cohen’s kappa 0.42（0.49）、加权 F1 0.69（0.74）；FFSL 阶段：四站点平均平衡准确度 0.77、kappa 0.62、加权 F1 0.73，呈现出跨站点异质分布下的较好个性化效果。

Conclusion: FFSL 能在现实数据可用性和隐私约束下实现患者自适应的癫痫检测，展示跨站点知识迁移与少样本个性化的潜力；但 Stage 1 的联邦微调表现略逊于中心化基线，需进一步优化。

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [34] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 在有限时间预算下的滑页推荐问题中，将评估成本作为资源约束纳入统一的MDP框架，使用仿真研究RL策略对用户偏好与预算的同时学习，结果显示在预算紧张时，on-policy与off-policy RL控制优于传统的上下文Bandit方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，用户在手机购物等场景通过滚动滑出一页slate来评估与选择项，评估成本（评估特征所需时间）与时间预算共同决定最终的点击/ engagement。传统方法往往忽略成本，可能导致高成本但高相关性的项未被充分利用或错失 engagement。本文旨在在时间预算约束下学习用户偏好与预算的策略，提升在资源受限条件下的 engagement 潜力。

Method: 将时间预算和评估成本整合进MDP，提出预算感知的效用函数以捕捉资源约束；开发用于再排序数据的仿真框架以研究策略行为；在Alibaba的个性化再排序数据集上进行仿真评估，比较on-policy与off-policy RL控制与传统上下文Bandit基线的表现。

Result: 实证结果表明，在紧凑时间预算条件下，on-policy与off-policy的强化学习控制能提升相对于传统上下文Bandit方法的表现，证明在资源约束下的RL策略更能提升参与度/ engagement。

Conclusion: 本文给出一个统一的时间约束下的滑页推荐MDP框架及仿真平台，实验表明预算感知的RL控制在资源受限场景优于非成本感知的基线。未来工作可拓展真实交互数据评估、更丰富的成本建模以及在真实系统中的落地与扩展。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [35] [CurvaDion: Curvature-Adaptive Distributed Orthonormalization](https://arxiv.org/abs/2512.13728)
*Bhavesh Kumar,Roger Jin,Jeffrey Quesnelle*

Main category: cs.LG

TL;DR: CurvaDion 在分布式训练中通过 RMMC 动态判定何时进行梯度同步，显著降低通信量同时保持收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决跨多个 GPU 的梯度同步在不同训练阶段所需通信量差异的问题；在扁平区域频繁同步并非必要，高曲率区域需要协调以防止发散。

Method: 在优化过程中利用已有的动量变化来估算局部曲率，提出 Relative Maximum Momentum Change (RMMC)，每层额外只需 O(d) 复杂度；结合 CurvaDion 的同步策略。

Result: 理论上将 RMMC 与损失曲率建立联系；实验在 160M 至 1.3B 参数规模的模型上实现约 99% 的通信量降低，同时与基线在收敛性上保持一致。

Conclusion: CurvaDion 提供一种低开销的自适应同步机制，适合超大规模语言模型训练，兼具理论联系和实践效果。

Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.

</details>


### [36] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: CCFG generalizes classifier-free guidance to multiple conditioning inputs in diffusion models, enabling high-fidelity wind data super-resolution with many channels; WindDM is a diffusion-based wind reconstruction model achieving state-of-the-art results at far lower cost.


<details>
  <summary>Details</summary>
Motivation: High-resolution, accurate wind data is essential for weather forecasting and wind energy applications but is expensive to acquire. Diffusion-based super-resolution can help, but wind data uses many conditioning channels, and existing CFG is limited to single/mixed conditioning. A scalable CFG-like method for many inputs can improve fidelity while leveraging pre-trained diffusion models.

Method: Introduce composite classifier-free guidance (CCFG) that generalizes CFG dropout to multiple conditioning inputs and can be plugged into pre-trained diffusion models. Develop WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction using CCFG, leveraging many conditioning variables.

Result: CCFG outputs surpass CFG in wind super-resolution tasks in fidelity. WindDM achieves state-of-the-art reconstruction quality among deep learning models and reduces cost by up to 1000x compared to classical methods.

Conclusion: CCFG extends diffusion-model conditioning to multiple inputs, enabling more accurate wind data reconstruction. WindDM demonstrates practical, high-quality, cost-effective wind-dynamics reconstruction, with potential impact on weather modelling and wind-energy applications.

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [37] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: 提出 Physical Inversion Solver (PIS)，一个集合条件扩散框架，能够从任意数量和几何形状的观测集合进行 PDE 约束的物理参数反演；通过 Set Transformer 编码器处理观测、余弦退火的稀疏性课程实现鲁棒性，并给出信息理论分析来揭示极端稀疏下的观测熵对反演的影响。


<details>
  <summary>Details</summary>
Motivation: 在观测稀疏、非规则且受限于传感器布置时，PDE 约束的物理参数反演本质上呈现病态，现有深度与算子学习模型在固定网格、鲁棒性不足且缺乏不确定性量化方面表现不佳。

Method: PIS 使用集合 Transformer 编码器处理任意数量/几何的观测，基于扩散模型进行反演，引入余弦退火稀疏性课程提升鲁棒性，并给出信息论分析来界定极端稀疏条件下的反演极限。

Result: 在 Darcy 流、Helmholtz 波场反演和 Hooke 的定律的结构健康监测等三个 PDE 逆问题上，在极端稀疏情形（观测率低至 0.29%）下，现有算子学习基线往往发散或崩溃，而 PIS 保持稳定与准确， inversion error 相对于基线下降约 12.28% 到 88.73%，并能可靠地产生经过校准的后验样本。

Conclusion: PIS 为物理反演提供一个通用、对稀疏性高度鲁棒的解法，适用于任意并且严重欠采样的观测，优于传统算子学习方法。

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [38] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: 提出 LLRC，一种梯度驱动、无需微调的低秩压缩方法，通过学习遮罩权重来选择奇异值，在不进行后续微调的情况下实现对比/超越现有无微调方法的性能，同时在不同压缩率和数据集上保持稳定表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式离散搜索或需要微调才能达到更好性能，难以在最优的层级秉承压缩率与任务精度之间的权衡；需要一种端到端能够直接学习每层的保留奇异值数量的办法，且无需额外微调。

Method: 提出 LLRC，通过 calibration 数据集只训练遮罩权重，使模型逐步保留更少的奇异值；通过最小化中间激活与原模型的分布差异来引导压缩，且不进行压缩后的微调。将 LLRC 与 STRS、SVD-LLM、LLM-Pruner等方法在多数据集任务上对比。

Result: 在多项任务与压缩率下，LLRC 超越同类无微调的排名选择方法，例如在 20% 压缩的 Llama-2-13B 上，在 MMLU、BoolQ、OpenbookQA 的提升分别为 12%、3.5%、4.4%；在与其他压缩技术比较中，对比无微调版本的 SVD-LLM 和 LLM-Pruner 的表现，LLRC 一贯 superior，而且与其微调版本相近。

Conclusion: LLRC 提供一个高效、无需微调的低秩压缩解决方案，能在不同任务和压缩率下接近或达到需要微调的技术水平，显著提升在实际应用中的可行性。

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [39] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 提出在联邦推荐系统中引入参数高效微调（PEFT）嵌入方法，通过LoRA、哈希编码和RQ-VAE等策略显著降低嵌入参数传输量并提升精度。


<details>
  <summary>Details</summary>
Motivation: 解决分布式FR中嵌入向量参数传输量庞大、成为通信瓶颈的问题，同时保持或提升推荐精度。

Method: 提出一个可插件化的PEFT嵌入框架，结合LoRA、Hash-based encoding，以及新颖的RQ-VAE作为PEFT策略，适用于多种FR模型骨干和数据集。

Result: 实验证明在不同FR骨干和数据集上显著降低通信开销并提升准确性。

Conclusion: 该框架为FR中的嵌入参数传输瓶颈提供高效解决方案，且易于与现有FR方法集成，且源码可用。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [40] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: 提出DARTs：一个鲁棒的长短期双路时空图框架，通过窗口感知的软融合，在高维噪声的MTS异常检测中实现对短期与长期模式的有效建模与定位。


<details>
  <summary>Details</summary>
Motivation: 现有高维多变量时间序列异常检测在捕捉长距离时空依赖方面受噪声干扰，难以鲁棒地学习有辨识性的表示。

Method: 提出DARTs框架，包含短期路径的多视图稀疏图学习器和扩散多关系图单元，以自适应捕捉短期分层模式；长期路径的多尺度时空图构造器，用于建模长序列动态；以及窗口感知的时空软融合机制，用于过滤噪声并整合异常模式。

Result: 在主流数据集上展示出优越性和鲁棒性，进行了系统的消融研究，证明各组件的重要性；代码与模型将公开。

Conclusion: DARTs通过双路径和窗口感知的软融合实现对高维MTS的鲁棒异常检测与定位，兼具改进的鲁棒性与可解释性潜力。

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [41] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 提出了一种基于自监督对比学习的时频融合模型TF-MCL，用于脑电EEG信号的抑郁症检测。通过FMH实现时频信息向融合域的高效映射，并通过多域跨损失重建时频域和融合域的分布，从而提升低语义表征的获取能力。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法依赖大量标签且对抑郁症的标注困难；现有对比学习未专门针对EEG的时频分布特征设计，且在获得低语义表征方面能力不足，限制了在MDD检测任务中的效果。需要更好地利用EEG的时频信息并获得更具判别性的融合表征。

Method: 提出时频融合与多域跨损失（TF-MCL）模型。通过融合映射头（FMH）将时频域信息高效映射到融合域，提升对时频信息的综合能力；通过多域跨损失优化，重建时频域与融合域的表征分布，提升对融合表征的学习能力。

Result: 在公开数据集MODMA和PRED+CT上评估，准确率分别提升5.87%和9.96%，明显优于现有SOTA方法。

Conclusion: TF-MCL通过更充分地利用时频表示和融合表示，提升MDD检测中的自监督对比学习效果，且在两个公开数据集上达到或接近SOTA。

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [42] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [43] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 在小规模金融新闻数据集上，基于嵌入的方法表现不佳，验证集指标高但测试集表现差，数据稀缺削弱了预训练嵌入的收益；建议考虑少样本学习、数据增强或词汇驱动的混合方法。


<details>
  <summary>Details</summary>
Motivation: 解决金融情感分析在资源有限环境中的挑战，评估不同嵌入表示在有限数据上的有效性。

Method: 对比评估 Word2Vec、GloVe 和句子变换器等嵌入表示，结合梯度提升，对人工标注的标题进行分类；比较验证集与测试集的差距；分析数据充分性阈值；展示在市场监控工作流中的周度情感聚合与叙事摘要应用。

Result: 验证集与测试集之间存在显著的性能差距；模型在测试集上的表现甚至落后于简单基线；预训练嵌入在数据不足时收益递减；小的验证集易导致过拟合；嵌入质量不足以解决数据稀缺问题。

Conclusion: 对于资源有限的从业者，应考虑少样本学习、数据增强或基于词汇的混合方法等替代路径；嵌入质量不能单独解决情感分类中的数据稀缺挑战。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [44] [MIDUS: Memory-Infused Depth Up-Scaling](https://arxiv.org/abs/2512.13751)
*Taero Kim,Hoyoon Byun,Youngjun Choi,Sungrae Park,Kyungwoo Song*

Main category: cs.LG

TL;DR: MIDUS 在重复层块中用头部记忆替代 FFN，给每个注意力头分配独立记忆库并引入高效的头部值分解模块，实现稀疏内存访问与头级信息注入，提升深度上规模的效率与性能，与强 DUS 基线相比具有更小的参数增长。


<details>
  <summary>Details</summary>
Motivation: 现有的深度上移（DUS）通过复制层并进行持续预训练来扩大模型容量，但对 FFN 的依赖限制了效率和潜在收益。观察到注意力头在层间和层内具有不同角色，提出头级记忆以增强信息流并降低参数成本，力求突破效率-性能权衡。

Method: 在重复的模型块中用头部记忆层（HML）替代 FFN，为每个注意力头分配独立的记忆库，支持头级检索并将信息注入后续层，同时保留头级的功能结构。结合稀疏内存访问与头级表示，加入高效的每头值分解模块以降低计算和参数开销。通过 Continual Pre-training（CPT）评估该设计在多层级深度上移场景中的表现。

Result: 在 CPT 实验中，MIDUS 相较强的 DUS 基线表现出稳健的性能提升，同时保持较低的参数增长，显示出优良的效率-性能权衡。

Conclusion: MIDUS 提供一种可扩展且资源高效的深度上移替代方案，通过头部记忆设计替代 FFN 的重复复制，提升信息流与任务性能，同时保持参数 footprint 的友好规模。

Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.

</details>


### [45] [Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training](https://arxiv.org/abs/2512.13770)
*Huaiyuan Xiao,Fadi Dornaika,Jingjun Bi*

Main category: cs.LG

TL;DR: MV-SupGCN是一种半监督的多视图GCN模型，通过（1）联合目标函数（交叉熵+有监督对比损失）提升判别性与泛化性，（2）对每个视图同时采用KNN与半监督图构建提高鲁棒性，（3）引入跨视图对比学习与伪标签实现多视图嵌入的一致性与利用无标签数据，从而实现对比学习与图结构的协同提升，实验上显著超过SOTA并开放源代码。


<details>
  <summary>Details</summary>
Motivation: 现有基于图卷积的多视图学习在充分利用不同视图的互补信息以及鲁棒地表示数据结构方面存在不足，易产生子优化与泛化能力不足的问题。需在一个统一框架下同时提升特征判别性、图结构鲁棒性与跨视图语义对齐，充分利用无标签数据。

Method: 提出MV-SupGCN：1) 设计联合损失，将交叉熵与有监督对比损失结合，促使同类内变异最小化、不同类间可分离性最大化；2) 在每个视图上同时构建KNN图与半监督图，增强数据结构的鲁棒性；3) 引入跨视图对比学习以统一对齐多视图嵌入，并使用伪标签加强对交叉熵与对比损失的监督，提升模型泛化。

Result: 在多个基准数据集上，MV-SupGCN持续超越现有最优方法，显示出所提出组件的协同效应与鲁棒性提升。

Conclusion: 该集成框架通过将有监督对比、双图构建、跨视图对比与伪标签有机结合，显著提升多视图GCN的表示能力与泛化性能，代码开放于GitHub。

Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN

</details>


### [46] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: 提出 SCPO：一种基于采样的权空间约束投影方法，在不需要对约束函数梯度访问的前提下，通过局部安全区域和SOCP投影实现安全的一阶更新，具备安全引导性和闭环稳定性保障，并对带有恶意监督的回归与双积分器任务表现出对不安全更新的拒绝与可行性保持。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的学习中，模型参数需在未知的、基于 rollout 的安全约束下优化。现有方法往往需要对约束函数梯度或全局显式约束，难以在实际系统中实现高效、可证明的安全性。

Method: 构建局部安全区域：通过轨迹回放（rollouts）结合参数变化与安全度量之间的平滑性界限，得到可行的参数更新区间。每次梯度更新都通过一个凸SOCP进行投影，得到一个安全的一阶步长；若存在稳定备份策略，则在闭环内实现稳定性并支持超越保守备份的安全自适应。理论上给出安全按归纳方式的保证，从任意安全初始化出发，所有中间策略均保持安全。对带有有害监督的回归以及带恶意专家的受限双积分器任务，实验显示方法能 consistently 拒绝不安全更新、保持可行性并实现目标提升。

Result: 提供了一个无需约束梯度访问、可证明的安全学习框架，能够在未知约束下实现安全的权空间更新，并对现实中的对抗性监督和受限控制任务表现出良好鲁棒性与性能提升。

Conclusion: SCPO 将安全性直接嵌入参数更新过程，结合采样与投影投影的形式，在保证安全性的同时实现性能改进，且具备稳定性分析与对抗性鲁棒性，适用于多种受限控制与安全关键的学习场景。

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [47] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出 Cross-Trace Verification Protocol (CTVP) 来对不可信的代码生成模型进行验证，利用语义等价程序变换下模型对执行轨迹的预测的一致性来检测潜在后门；引入 Adversarial Robustness Quotient (ARQ)，证明验证成本相对于基线生成呈指数增长随轨道大小增加；给出信息论界限证明对抗者在训练中不可通过提升来提升攻击效果（非游戏化）。该方法为代码生成任务提供可扩展、理论扎实的 AI 控制路径。


<details>
  <summary>Details</summary>
Motivation: 在代码生成场景中，LLMs 可能在很少人类监督的情况下产生带有后门的或恶意行为的代码。直接执行潜在恶意代码存在安全风险，因此需要一种不直接执行代码、但能有效检测潜在风险的控制框架。

Method: 通过让模型预测在语义等价的程序变换下的执行轨迹，并分析这些预测轨迹的一致性模式来发现行为异常。提出 Cross-Trace Verification Protocol (CTVP)；引入 Adversarial Robustness Quotient (ARQ) 来量化验证的计算成本，与基线生成进行比较，理论上分析轨道大小与成本的指数关系。

Result: 给出信息理论界限，表明对抗者无法通过训练提升攻击能力（非游戏化），并提出了一种可扩展且具有理论支撑的代码生成控制方法。

Conclusion: 语义轨道分析为代码生成任务提供了一个可扩展、理论基础扎实的 AI 控制框架，CTVP 能在不直接执行潜在恶意代码的前提下识别后门行为。

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [48] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 提出一种两阶段方法以提升 RLHF 的对齐：第一阶段通过约束优化识别导致不满意回答的训练数据并给出解释；第二阶段通过删除这些数据的影响来改进不满意回答，同时尽量不损害其他回答。


<details>
  <summary>Details</summary>
Motivation: 借鉴人类在日常生活中通过找出原因并修正来改进结果的策略，将其应用于 RLHF，以解决语言模型在经过 RLHF 调整后仍可能输出不理想或不对齐的回答的问题。

Method: 1) 事后解释：将不满意回答的产生归因于训练数据，通过将目标 prompt-回复对在特征空间中近似表示，构造一个训练数据子集使该对能表示为该子集的凸组合，并通过一个高效的迭代数据选择算法求解该问题。2) 忘则（unlearning）：移除导致这些不满意回答的训练数据的影响，同时尽量不削弱对其他提示的满意回答。

Result: 实验结果表明该算法能够在 RLHF 框架下改善模型的对齐表现，即通过数据层面的解释与移除来提升输出质量。

Conclusion: 该工作提出一个可行的数据驱动路径，通过对导致不良输出的训练数据进行解释与有选择的删除来提升 RLHF 的对齐效果，为对齐研究提供了数据层面的新方向。

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [49] [AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts](https://arxiv.org/abs/2512.14461)
*Niklas Grieger,Jannik Raskob,Siamak Mehrkanoon,Stephan Bialonski*

Main category: cs.LG

TL;DR: AnySleep 是一个可处理任意 EEG/EOG 数据、可调时间分辨率的睡眠分期深度学习模型；在跨多中心数据集上实现强泛化，在 30 秒分辨率达到或超越基线，在亚 30 秒尺度更好地检测短唤醒，并且能在无 EOG 或单导联情形下也工作；公开发布以促进大规模研究与新生物标志物发现。


<details>
  <summary>Details</summary>
Motivation: 解决跨中心睡眠研究中电极布置、数据集异质性和手动睡眠分期带来的重复性和扩展性挑战，并在更短时间尺度挖掘生物标志物。

Method: 提出 AnySleep，利用任意 EEG/EOG 数据进行睡眠分期，输出可变时间分辨率标签；使用 19,000+ 夜记录、21 个数据集、约 200,000 小时 EEG/EOG 数据进行训练和验证，展示通道数量越多性能越好，即使没有 EOG 或仅有单导联也具鲁棒性。

Result: 在 30 秒分辨率上达到/超过现有最优基线；在亚 30 秒尺度捕捉短唤醒（觉醒）并提升对生理特征（年龄、性别）和病理状态（睡眠呼吸暂停）的预测效果；模型在多中心异质数据上的泛化性显著。

Conclusion: 该模型和代码公开发布，便于大规模异质电极设置的睡眠研究与新标志物发现。

Abstract: Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.

</details>


### [50] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: 使用变分自编码器（VAE）为心房信号生成合成多通道EGM，以应对BSPM-EGM配对数据不足的问题，提出了两种模型并评估其保真度及在数据增强中的作用。


<details>
  <summary>Details</summary>
Motivation: 解决BSPM-EGM成对数据稀缺的问题，提升基于深度学习的ECGI管线的鲁棒性和泛化能力。

Method: 提出两种VAE模型：VAE-S（仅用于窄段心律（窦性心律））和VAE-C（条件化克服窦性心律和AF两类信号）。在多通道心房EGM上训练并生成合成EGM；用形态学、谱特征与分布相似度等指标评估生成 EGMs 的保真度；在下游任务中用生成EGM进行数据增强以改善非侵入性EGM重建。

Result: VAE-S在与真实（在计算机仿真生成的）EGMs的形态和谱等方面具有更高的保真度；VAE-C实现了按心律进行的生成能力，但窦性心律的重建质量下降；在数据增强场景中，适度的增广可提升估计性能。

Conclusion: VAE驱动的生成建模可缓解数据稀缺，提升基于DL的ECGI管线性能。

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [51] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 给出在有限样本、无分布假设条件下的L1标定误差界，前提是标定函数具有有界变差；并提出一种对任意分类器的后处理方法，在不显著影响性能的前提下实现可控的标定误差上界，且无需额外假设。


<details>
  <summary>Details</summary>
Motivation: 在二元分类中，预测概率需要经过校准以反映真实条件概率。L1标定误差是直观的评估量，但现有结果常受分布假设或样本量依赖的限制。本工作旨在提供非渐近、分布无关的界，并给出可在真实数据上以较小开销实现的标定改进策略，以促进在实际场景中的应用。

Method: 1) 对于任意标定函数具有有界变差的分类器，推导出其L1标定误差的非渐近上界，给出可观测的样本量与变差界之间的关系。2) 提出一种对任意分类器的后处理或校准步骤，通过对输出映射施加受控的有界变差约束（如带正则化的单调映射/等高线拟合或等熵的校准层），在不显著影响分类器性能的前提下确保标定误差的上界可被有效计算并达到可控水平，且不需要额外分布假设。

Result: 给出两条关键结果：第一，建立了一个非渐近、分布无关的L1标定误差上界，适用于标定函数的有界变差情形；第二，提出并实现了一种通用的后处理校准方法，能够对任意分类器进行修正，使得其标定误差具备可计算的上界，同时在实际数据上对分类性能的影响最小。所有结论均为非渐近与分布无关，且方法具有可行的实现与低开销。

Conclusion: 本文建议在实践中采用分布无关、非渐近的标定评估框架，并通过简单的后处理校准步骤实现可控的标定误差上界。最后提供了衡量标定误差的实用指南与流程，强调在真实数据集上的可行性与低额外开销。

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [52] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 端到端婴儿啼哭分析：在隐私保护、降噪鲁棒性和通信效率之间实现边缘端到端推理的联邦学习系统。


<details>
  <summary>Details</summary>
Motivation: 隐私担忧、背景噪声敏感性和跨设备域差异限制幼儿啼哭分析的部署。提出一个结合 DAE、卷积标记器和 Transformer 编码器的端到端流水线，并通过联邦学习实现隐私保护、降噪、适应性分割、后验校准和基于能量的 OOD abstention；在 8 位适配器增量和安全聚合下进行通信高效的联邦训练。

Method: 将 DAE、卷积标记器和 Transformer 编码器整合成端到端管线；在设备端执行降噪、适应性分割、后验校准和能量基 OOD 退出。联邦训练采用带正则化对照变差更新和 8-bit 适配器增量的安全聚合。

Result: 在 Baby Chillanto、Donate-a-Cry 数据集并加入 ESC-50 噪声叠加下，模型达到宏 F1 0.938、AUC 0.962、ECE 0.032；每轮客户端上行流量从约 36-42 MB 降至 3.3 MB；在 Jetson Nano 实时推理为 96 ms/1s 帧；边缘端的隐私保护、噪声鲁棒性和通信效率得到实证。

Conclusion: 结果展示了隐私保护、抗噪声、通信高效且适合联邦部署的婴儿啼哭分类的现实路径，支持在边缘设备上进行实时推理及联邦学习部署。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [53] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: OPTIMA 为一键后训练剪枝提供一种实用的求解框架：通过将层级权重重建问题拆分为独立的逐行二次规划，并共享同一层的海森矩阵，在加速器上高效并行求解，实现在不微调条件下的可扩展剪枝并提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有后训练剪枝在简单的权重置零（速度快）与联合优化（精度高，但计算不可行）之间存在权衡；需要一个在单一加速器上也能大规模、接近最优的单 shot 方案。

Method: 将层级权重重建在掩蔽选择后建模为独立的逐行二次规划，所有子问题共享一个层级海森矩阵结构；设计并实现一个加速器友好的一致求解器，逐层累计海森矩阵并并行求解大量小型二次规划；可与现有掩蔽选择器集成。

Result: 对多种大模型家族和不同稀疏度设置，ICKOM提升了最多 3.97% 的绝对精度；在 NVIDIA H100 上，8B 参数的 Transformer 全量端到端剪枝需时约 40 小时，峰值显存 ~60GB。

Conclusion: 为单次后训练剪枝树立新的精度-效率权衡基准，提供在单一加速器上可扩展的实用解决方案，并优于现有零-shot 与轻量化方法。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [54] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 对长上下文的LLM研究：在长上下文任务中，推理时计算提升效果有限，原因是静态自注意力导致信号稀释；通过对给定上下文进行梯度更新的上下文特定微调，克服该局限，显著提升长期上下文任务表现。


<details>
  <summary>Details</summary>
Motivation: 探究在长上下文中，现有推理时计算策略（如生成多轮思考token）对性能提升的边际效应，以及为何在极长上下文下会失效；提出能在推理阶段更有效利用上下文信息的替代方案。

Method: 在受控的沙箱长上下文任务上进行实验，比较静态自注意力下的推理-时间策略与对上下文进行有针对性的梯度更新的策略；分析信号稀释现象（score dilution）的原因；提出通过对给定上下文进行梯度更新的简单方法，覆盖静态自注意力的局限性；在 LongBench-v2 与 ZeroScrolls 基准上测试 Qwen3-4B 模型的改进。

Result: 发现推理时生成更多“thinking”token 的策略在长上下文上回报快速下降且会失败；信号稀释是静态自注意力的固有问题导致此现象；现有的推理策略在某些条件下无法检索相关的长上下文信号；通过针对上下文进行梯度更新，显著提升性能，Qwen3-4B 在 LongBench-v2 和 ZeroScrolls 上的子集上分别平均提升12.6和14.1百分点。

Conclusion: 在长上下文场景中，少量且与上下文相关的训练比当前推理时的扩展推理令牌策略更有效，提供了一个实用的方向——将推理计算的一部分转向上下文特定的微调以提高长期记忆与信号检索能力。

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [55] [Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations](https://arxiv.org/abs/2512.13913)
*Patrick Egenlauf,Iva Březinová,Sabine Andergassen,Miriam Klopotek*

Main category: cs.LG

TL;DR: 用神经ODE学习TD2RDM动力学以评估三粒子累积相关在时局部闭合中的适用性：当两粒子与三粒子累积相关性高时，模型能在不显式三粒子信息的情况下预测演化；在反相关/无相关区域则需要记忆效应，单纯的时点闭合函数不足。时间平均三粒子相关的增长量是预测成败的主导因子；中等相关增长区间两者皆准确，强相关区间则普遍失效。该分析为闭合方法的记忆核与非局部闭合的开发提供诊断工具，并展示了在有限数据下对高维RDM的学习潜力。


<details>
  <summary>Details</summary>
Motivation: 评估基于二粒子RDM的时间演化闭合方法（TD2RDM）的适用性与局限性，以及一个数据驱动的诊断工具对记忆效应在三粒子重构中的作用进行定性判断。

Method: 使用无降维的神经ODE对来自精确2RDM数据的动力学进行训练与预测；在不同参数区间比较两粒子与三粒子累积相关性的影响；对时间平均三粒子相关增长量进行量化分析以预测成功区；与现有TD2RDM重构进行对比；讨论记忆依赖核的需求。

Result: 在两-三粒子累积相关性高的参数区间，神经ODE能无三粒子信息地再现精确2RDM演化；在反相关或无相关区间，神经ODE失败，说明需记忆效应。时间平均三粒子相关增长量成为预测成败的主因；中等相关增长区间，神经ODE与TD2RDM都准确；强相关增长区间则出现系统性崩溃。结论指向需要记忆依赖的三粒子累计核以提升闭合方法的鲁棒性，同时神经ODE可作为域内适用性诊断工具，推动非局部闭合方案的发展；并展示在有限数据下学习高维RDM动力学的潜力。

Conclusion: 神经ODE为评估与诊断TD2RDM及相关闭合方法适用域的模型无关工具，强调记忆效应在强相关增量区域的重要性，并指示开发带记忆核的非局部闭合以覆盖更广的 dynamical regimes。

Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.

</details>


### [56] [Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics](https://arxiv.org/abs/2512.13919)
*Eugenio Varetti,Matteo Torzoni,Marco Tezzele,Andrea Manzoni*

Main category: cs.LG

TL;DR: Adaptive digital twins for civil engineering using dynamic Bayesian networks and reinforcement learning to online-learn state-transition models, enabling personalized, robust, and cost-effective maintenance; evaluated on a railway bridge SHM case study.


<details>
  <summary>Details</summary>
Motivation: Increase value realization of digital twins by explicitly modeling uncertainty in state transitions and enabling online, hierarchical learning for adaptive decision making.

Method: Represent state transitions as random variables with conjugate priors; model physical-virtual interaction with dynamic Bayesian networks; perform online hierarchical Bayesian updates; compute dynamic policies by solving parametric Markov decision processes via reinforcement learning.

Result: Provides a mathematical framework accommodating a broader class of distributions; enables precise policy updates and adaptive decision making; demonstrates enhanced personalization, robustness, and cost-effectiveness in a railway bridge SHM and maintenance planning case study.

Conclusion: The adaptive digital twin framework, through adaptive transition modeling and RL-based policy optimization, improves value realization and robustness in civil engineering applications.

Abstract: This work shows how adaptivity can enhance value realization of digital twins in civil engineering. We focus on adapting the state transition models within digital twins represented through probabilistic graphical models. The bi-directional interaction between the physical and virtual domains is modeled using dynamic Bayesian networks. By treating state transition probabilities as random variables endowed with conjugate priors, we enable hierarchical online learning of transition dynamics from a state to another through effortless Bayesian updates. We provide the mathematical framework to account for a larger class of distributions with respect to the current literature. To compute dynamic policies with precision updates we solve parametric Markov decision processes through reinforcement learning. The proposed adaptive digital twin framework enjoys enhanced personalization, increased robustness, and improved cost-effectiveness. We assess our approach on a case study involving structural health monitoring and maintenance planning of a railway bridge.

</details>


### [57] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 提出一个针对线性递推的分层分解框架，结合滑动窗口递推（SWR）在GPU内存层次结构上的对齐，形成 Phalanx 层作为窗口化注意力/线性递推的可替换实现，在1B参数的多混合模型上实现4K-32K上下文长度的10-40%加速，同时保持 perplexity。


<details>
  <summary>Details</summary>
Motivation: 解决在极长上下文下的计算与通信成本，特别是跨 Warp 的高成本通信，以及充分利用 GPU 的内存层次结构和带宽，以提升多混合语言模型的推理/训练效率。

Method: 提出对线性递推的分层分解框架，使其能对齐GPU内存层级；开发 Sliding Window Recurrences (SWR)；将递推截断为硬件对齐的窗口，天然呈现锯齿状结构以降低跨 Warp 通信；基于 SWR 设计 Phalanx 层，作为窗口化注意力或线性递推的即插即用替代。

Result: 在1B参数的多混合模型中，Phalanx 相对于优化后的 Transformer，在4K到32K的上下文长度上实现10-40%的加速，同时保持 perplexity 相当。

Conclusion: SWR 与 Phalanx 使多混合架构能够高效利用 GPU 内存层次结构，通过对齐的窗口化递推降低通信成本，在不牺牲语言模型质量的前提下提升推理与训练效率。

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [58] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: 提出 Pattern-Guided Diffusion Models (PGDM)，利用 archetypal analysis 捕捉时间序列中的重复模式，并以最可能的下一个模式引导扩散预测，同时引入基于 archetypal analysis 的不确定性量化并动态调节引导强度；在视觉野场测量和运动捕捉数据上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时间序列 forecasting 中往往忽略数据内在的重复模式。若能结合已知模式进行约束，将提升预测的现实性与准确性，并提供更可靠的不确定性评估。

Method: 从数据中通过 archetypal analysis 提取模式，估计序列中最有可能的下一个模式，并用该模式引导扩散模型的预测。引入基于 archetypal analysis 的不确定性量化，并据此动态调节引导强度。

Result: 在两组应用（视觉野场测量与运动捕捉帧）中，模式引导显著提升预测指标（MAE / CRPS），并且相对于基线有显著的性能提升，达到数十个百分点级别的改进。

Conclusion: 以模式为导向的扩散模型可提升多变量时间序列的预测现实性与准确性，且提供实用的不确定性估计，具备广泛的应用潜力。

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [59] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: 提出并标准化一个MHC-II相关的综合数据集及多任务评估框架，促使机器学习在表位发现与免疫应答预测方面向前发展。


<details>
  <summary>Details</summary>
Motivation: MHC-II抗原表位的结合特异性复杂且模式模糊，现有MHC-II数据集规模小、标准化程度低，限制了ML在该领域的发展，因此需要一个高质量的、结构化的数据资源与多任务评估平台。

Method: 从IEDB等公开来源整理、扩展并标准化MHC-II相关的肽段- MHC数据集，并引入一个具有更丰富生物学背景的抗原-MHC-II数据集；在此数据集上提出三大ML任务（肽段结合、肽段呈递、抗原呈递），并构建多尺度评估框架，对现有模型进行基线与广泛比较，采用模块化框架分析不同建模设计。

Result: 生成了标准化且扩展的MHC-II数据集及一个包含丰富生物学背景的抗原-MHC-II数据集；建立并评估了针对三大任务的多尺度评估框架与基线模型，提供对不同建模设计的系统比较与分析。

Conclusion: 该工作为计算免疫治疗领域提供了宝贵的数据资源与评估基准，奠定了未来在ML辅助表位发现和免疫反应预测方面的研究基础。

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [60] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: EXAONE Path 2.5 是一个整合多模态的病理基础模型，将组织学、基因组、表观基因组和转录组整合成患者级表征，并引入多模态 SigLIP 对比学习、F-RoPE 片段感知的旋转位置编码，以及针对 WSI 与 RNA-seq 的领域专用内部基础模型；在内部真实世界数据集和 Patho-Bench 的 80 个任务上与 SOTA 相当，同时在数据与参数方面表现出高效性，且在临床场景具有更强的适应性。


<details>
  <summary>Details</summary>
Motivation: 临床癌症进展涉及跨越多层生物学信息，单靠影像模型难以捕捉基因组和转录组等隐含信号，需要一个整合的 genotype-to-phenotype 表征来实现精准肿瘤学。

Method: 提出三大支柱：1) 多模态 SigLIP 损失，实现跨模态的全对全对比学习；2) F-RoPE，保持 WSI 的空间结构与组织片段拓扑；3) 针对 WSI 与 RNA-seq 的领域专用内部基础模型，提供生物学上有据可依的嵌入以实现强鲁棒的多模态对齐。

Result: 在 Patho-Bench 上达到与最先进模型相当的性能，在内部临床数据集上展现最高的适应性，同时展现出较高的数据与参数效率和良好的多模态对齐。

Conclusion: 生物信息驱动的多模态设计对于更准确的基因型到表型建模具有潜力，向下一代精准肿瘤学迈进；未来工作可能扩展到更多模态、提升跨数据源的鲁棒性，以及临床落地的进一步验证。

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [61] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: FusAD is a unified, multi-task capable time series analysis framework that blends Fourier and Wavelet-based time-frequency fusion with adaptive denoising and a masked pre-training decoding scheme to handle classification, forecasting, and anomaly detection across diverse data efficiently.


<details>
  <summary>Details</summary>
Motivation: A general, robust framework is needed for diverse time series tasks. Existing methods are often task-specific and struggle to share information across different time series types, while real-world data are noisy and multi-scale.

Method: Adaptive time-frequency fusion combining Fourier and Wavelet transforms to capture global-local and multi-scale features; adaptive denoising to filter noise and highlight important variations; a general information fusion and decoding structure with masked pre-training to learn transferable, multi-granularity representations.

Result: Extensive experiments show FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection, with high efficiency and scalability. Code is available at the provided GitHub link.

Conclusion: FusAD provides a robust, scalable unified framework for time series analysis that achieves superior performance across tasks and data types by integrating adaptive fusion, denoising, and masked pre-training.

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [62] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: 提出 SonicMoE，通过内存友好前向/反向传播、GPU IO/计算重叠和“token rounding”缓解高粒度/高稀疏MoE的内存与计算开销，实现更高吞吐和更低显存开销。


<details>
  <summary>Details</summary>
Motivation: MoE在追求高专家粒度与高稀疏性时，面临激活缓存激增、IO成本与分组GEMM填充导致的计算浪费等硬件瓶颈。现有实现难以在大规模训练中实现高吞吐与低显存的平衡。

Method: 提出三方面改进：1) 设计内存高效的前向/反向传播，尽量减少激活缓存；2) 开发可覆盖多种MoE架构的GPU内核，实现内存IO与计算的重叠；3) 提出“token rounding”机制并结合tile感知优化，降低分组GEMM中的填充浪费，形成 SonicMoE 框架。

Result: 在 64 个 H100 上，SonicMoE 相比 ScatterMoE 的 BF16 MoE 内核，在对7B MoE的实验中实现 1.86x 计算吞吐提升，同时激活内存降低约45%；训练吞吐达到约213B tokens/day，与在 96 个 H100 的 ScatterMoE/ lm-engine 833k 规模的 225B tokens/day 相当。高稀疏设置下，tile-aware token rounding 相较于普通 top-K 路由再带来约1.16x 的内核执行时间提升，同时保持下游性能。全部内核开源，便于在不同MoE架构上加速训练。

Conclusion: SonicMoE 能在高粒度/高稀疏 MoE 场景下显著提升硬件利用率和吞吐，并提供可复用的开源实现。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [63] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Cornserve is an online serving system that optimizes deployment for Any-to-Any multimodal models by describing and planning the computation graph and disaggregating components, achieving substantial throughput and tail-latency improvements.


<details>
  <summary>Details</summary>
Motivation: Any-to-Any models mix text and multimodal data, creating heterogeneity in input/output types, computation paths, and scaling. A framework is needed to describe the model as a computation graph and automatically plan efficient deployment.

Method: Cornserve lets developers describe a heterogeneous computation graph (e.g., multimodal encoders, autoregressive LLMs, multimodal generators). Its planner automatically derives an optimized deployment plan, including possible disaggregation of the model into components based on workload characteristics. A distributed runtime executes the plan, handling heterogeneity online.

Result: Evaluations show significant performance gains: up to 3.81× throughput improvement and up to 5.79× tail-latency reduction compared with existing solutions.

Conclusion: Cornserve effectively supports diverse Any-to-Any models, enabling efficient online serving and surpassing prior solutions in throughput and latency.

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [64] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 提出基于逻辑相似性的奖励机制S-GRPO，用监督GRPO变体提升对齐鲁棒性和性能，优于SFT，扩展GRPO/DPO。


<details>
  <summary>Details</summary>
Motivation: RLHF中的奖励模型质量决定最终对齐效果，现有PPO等对奖赏依赖大，需更稳健的对齐方法；逻辑一致性可以作为新的奖励信号；现实问题多角度解读，加入监督分支避免模型崩溃。

Method: 引入逻辑相似性作为奖励信号，提出S-GRPO：在GRPO框架基础上加入监督成分，联合优化生成项、KL正则化、和基于标签的目标，形成同时优化的多目标训练。

Result: 实验显示S-GRPO在性能和鲁棒性方面持续优于标准SFT，扩展GRPO和DPO等偏好学习框架，具备更灵活、任务自适应的对齐训练能力。

Conclusion: 逻辑相似性奖励结合监督GRPO可提升LLM对齐效果和鲁棒性，提供一种可扩展且更稳健的对齐训练路径；代码开源。

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [65] [PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario](https://arxiv.org/abs/2512.14150)
*Zhijie Zhong,Zhiwen Yu,Pengyu Li,Jianming Lv,C. L. Philip Chen,Min Chen*

Main category: cs.LG

TL;DR: PathFinder提出一种在多发射场景下进行主动环境建模的新型RPP架构，结合解耦特征编码、Mask-Guided Low-rank Attention、发射器导向混合学习等策略，并引入单向到多发射器的S2MT-RPP基准，显著提升在多发射场景下的泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的无线路径损耗预测（RPP）方法缺乏主动的环境建模，难以适应现实中的多发射场场景，且在训练/测试分布不一致时泛化能力差。需要在包含建筑与发射器信息的表征、跨发射场泛化以及对分布转移的鲁棒性方面进行提升。

Method: 提出PathFinder：通过解耦特征编码主动建模建筑与发射器，并引入Mask-Guided Low-rank Attention以分区域关注接收端与建筑区域；引入发射器导向的Mixup策略以增强训练鲁棒性；并提出S2MT-RPP基准以评估单发到多发的外推能力（single-to-multi-transmitter extrapolation）。

Result: 实验表明PathFinder在与现有方法的对比中具有明显优势，特别是在多发射场场景下的性能提升显著。

Conclusion: PathFinder有效解决了主动环境建模、跨发射场泛化及分布转移鲁棒性问题，所提出的训练策略与新的基准有助于提升RPP在真实多发射场条件下的外推能力。

Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.

</details>


### [66] [On Improving Deep Active Learning with Formal Verification](https://arxiv.org/abs/2512.14170)
*Jonathan Spiegelman,Guy Amir,Guy Katz*

Main category: cs.LG

TL;DR: 将通过形式化验证产生的对抗样本用于深度主动学习的数据扩增，显著优于传统梯度对抗样本，提升多种DAL方法的泛化能力与性能，并对作者的新方法显示出明显收益。


<details>
  <summary>Details</summary>
Motivation: 深度主动学习通过挑选信息密集的未标记样本来降低标注成本，本文进一步通过合成输入提高数据效率，探索破坏鲁棒性约束的对抗输入在DAL中的作用。

Method: 使用通过形式化验证得到的对抗样本来扩增训练集，并与标准梯度对抗样本比较；将这一扩增策略应用于多种主流DAL方法以及作者提出的新方法，评估在标准基准上的泛化表现。

Result: 与梯度对抗样本相比，形式化验证产生的对抗样本对DAL的提升更显著；在多个基准上显著提升模型泛化，并对新提出的技术也有显著改进。

Conclusion: 将形式化验证产生的对抗输入用于数据扩增的DAL策略具有潜力，能够在不增加额外标注成本的前提下提升数据效率和泛化能力，且可与多种DAL技术兼容。

Abstract: Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.

</details>


### [67] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 提出并验证了随机桥（random-bridges）作为 generative modeling 的随机传输工具，利用高斯随机桥可在更少步数内生成高质量样本并保持竞争性的 FID；框架在计算上更廉价，适合高速生成。


<details>
  <summary>Details</summary>
Motivation: 旨在通过将目标分布作为固定时间点的条件化分布，提供灵活的随机传输机制，以兼容马可夫/非马可夫、连续/离散或混合驱动过程，提升生成效率与多样性。

Method: 以随机桥理论为基础，给出从一般概率事实到具体学习与仿真算法的表示框架，强调信息处理角度；以高斯随机桥为具体实现，展示可学习/仿真算法的表示。

Result: 实验表明，基于高斯随机桥的方法在较少步骤内即可生成高质量样本，且 Frechet Inception Distance 评分具有竞争力；同时框架计算成本低，便于高速生成。

Conclusion: 该框架提供了一种灵活且高效的生成性建模方法，适用于快速、可扩展的生成任务，并能通过不同驱动过程实现多样的生成模式。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [68] [Understanding and Improving Hyperbolic Deep Reinforcement Learning](https://arxiv.org/abs/2512.14202)
*Timo Klein,Thomas Lang,Andrii Shkabrii,Alexander Sturm,Kevin Sidak,Lukas Miklautz,Claudia Plant,Yllka Velaj,Sebastian Tschiatschek*

Main category: cs.LG

TL;DR: Hyperbolic representations can improve RL but training is unstable due to large-norm embeddings. The paper proposes Hyper++: stable critic via categorical loss, norm-bounded feature regularization, and optimization-friendly hyperbolic layers. Empirically, it yields stable learning and superior performance/time efficiency on ProcGen and Atari-5.


<details>
  <summary>Details</summary>
Motivation: RL performance hinges on representation geometry. Hyperbolic spaces naturally capture hierarchical/relational structure common in RL environments, but optimization is challenging due to nonstationarity and gradient issues, especially with large-norm embeddings in hyperbolic models.

Method: 1) Analyze gradients in Poincaré Ball and Hyperboloid models to identify instability sources; 2) Propose Hyper++ with (i) stable critic via categorical value loss instead of regression, (ii) feature regularization enforcing bounded norms to avoid high-dimensional clipping issues, (iii) an optimization-friendly reformulation of hyperbolic network layers.

Result: In ProcGen, Hyper++ achieves stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by ~30%. In Atari-5 with Double DQN, it outperforms both Euclidean and prior hyperbolic baselines.

Conclusion: Hyper++ demonstrates that with targeted training stabilizers and normalized hyperbolic representations, hyperbolic deep RL can surpass baselines in both stability and efficiency; the work advances practical use of hyperbolic geometry in RL and provides code release for reproducibility.

Abstract: The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .

</details>


### [69] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: 提出一种基于LLM对比的难度估计方法（LLM compare），通过成对比较与Bradley–Terry分数来评估问题难度，具备对分布外问题的潜在适用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有难度估计在可扩展性、无需 ground truth 以及对分布外问题无效等局限，提出更通用、连续、模型无关的难度度量。

Method: 让大型语言模型进行成对难度比较，随后对结果应用Bradley–Terry模型计算难度分数；并给出一个三维正交框架（construction、scale、dependence）来定位现有方法在分布外问题中的覆盖度。

Result: 与人工注释高度对齐（Pearson r ≥ 0.80，n=1876），对10%噪声注入的鲁棒性较好，相关性下降不足6%；展示了在替代人工标注与合成数据生成方面的潜力。

Conclusion: LLM compare 为课程设计、模型评估及AI研究灵感提供一个连续、动态、模型无关且不依赖 ground truth 的难度估计工具，有望推动大规模数据生成与评估流程的效率提升。

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [70] [Understanding the Gain from Data Filtering in Multimodal Contrastive Learning](https://arxiv.org/abs/2512.14230)
*Divyansh Pareek,Sewoong Oh,Simon S. Du*

Main category: cs.LG

TL;DR: 在双模态数据生成模型下，理论分析显示基于教师模型的数据筛选能够提升对比学习的误差界限。未筛选时误差上界与下界均为1/(η√n)，而通过教师筛选后，误差上界在 η大时为1/√(ηn)，在 η小时则为1/√n。


<details>
  <summary>Details</summary>
Motivation: 解释为什么教师筛选在大规模网页数据中的对比学习能带来实证上的改进，以及在理论上对比了有无筛选的错误率随数据噪声比例η和样本量n的变化。

Method: 在线性对比学习框架下，设 η∈(0,1] 表示n个观测中正确匹配模态的比例，推导无筛选情况下的误差上下界为1/(η√n)，以及在应用教师筛选后的误差界：在η较大时上界为1/√(ηn)，在η较小时上界为1/√n。

Result: 给出明确的误差界限对比：筛选可以降低误差上界，尤其在η较大时显著改善；理论结果解释了为何基于教师的筛选在实际大规模数据中有良好表现。

Conclusion: 在双模态线性对比学习中，教师筛选能在理论上降低误差上界，随正确匹配模态数据比例η的提升而提升效果，解释了经验中的筛选有效性。

Abstract: The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\frac{1}{η\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\frac{1}{\sqrt{ηn}}$ in the large $η$ regime, and by $\frac{1}{\sqrt{n}}$ in the small $η$ regime.

</details>


### [71] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: A regularization-based learning framework for reaction-diffusion systems that enforces mass conservation and quasipositivity in the learned reaction terms, ensuring non-negativity and well-posedness; it proves convergence to a unique regularization-minimizing solution under physical constraints and provides approximation results for quasipositive functions to enable physically consistent parameterizations.


<details>
  <summary>Details</summary>
Motivation: Learn RD models from data while respecting fundamental physical laws (mass conservation, positivity) and mathematical well-posedness, improving reliability and interpretability of data-driven PDE models.

Method: Systematically modify parameterized reaction terms so they intrinsically satisfy mass conservation and quasipositivity; embed these properties into a regularization-based learning framework; extend theory to show convergence to a unique limit solution under conservation laws and quasipositivity; develop approximation results for quasipositive functions relevant to constructing consistent parameterizations.

Result: Theoretical guarantees: existence and uniqueness of the regularized solution under the enforced physical constraints; convergence of learned solutions to a regularization-minimizing limit system; approximation results ensuring feasible construction of quasipositive parameterizations.

Conclusion: Physically consistent, interpretable RD models can be learned with rigorous guarantees, advancing data-driven modeling for reaction-diffusion systems that align with mass conservation and non-negativity principles.

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [72] [Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning](https://arxiv.org/abs/2512.14241)
*Salvatore Romano,Marco Grassia,Giuseppe Mangioni*

Main category: cs.LG

TL;DR: 提出以 Representation-aware Graph-generation Model evaluation（RGM）替代MMD来评估图生成模型；在GRAN与EDGE上进行实证比较，发现两者在保留不同图域的结构特征方面存在局限，且MMD不足以全面评估GGMs。


<details>
  <summary>Details</summary>
Motivation: 现有GGMs的评估主要依赖MMD，难以捕捉生成图的结构特征差异，需开发更全面、领域感知的评估框架。

Method: 提出RGM评估框架，基于几何深度学习模型对合成与真实图进行分类；系统评估GRAN与EDGE在自定义数据集上的生成图，并分析在拓扑属性与领域结构保真度方面的表现。

Result: 两种模型在某些拓扑属性上能较接近真实图，但在维持不同图域的结构特征方面存在显著缺陷；MMD作为GGMs评估指标的不足也被揭示，需探索替代评估准则。

Conclusion: RGM为GGMs评估提供了更具代表性的框架；未来工作应探索更多领域感知的评估指标及更全面的图域数据集。

Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.

</details>


### [73] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 提出 FLAME，一种极轻量且强大的时间序列基础模型家族，结合 Legendre memory 的变体及正则化流头，实现确定性与概率性预测的生成建模，在零样本上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长期依赖、计算成本和概率建模方面存在权衡，需一个高效且鲁棒的统一框架，同时兼具确定性和概率性 forecasting 能力。

Method: 引入 FLAME，基于 Legendre Memory（LegT、LegS）在编码/解码阶段的改造，以捕捉数据的内在先验，并通过 Normalizing Flow 预测头实现对预测分布的生成建模，从而在保持高效的同时提升精度。并在 TSFM-Bench 与 ProbTS 上进行零样本评估。

Result: 在 TSFM-Bench、ProbTS 上实现一致的零样本最先进性能，覆盖确定性与概率性预测任务。

Conclusion: FLAME 展现出极低的计算开销与强大泛化能力，适合长期依赖的时间序列预测，同时通过生成建模提高对复杂分布的刻画，成为轻量级但有竞争力的时序基础模型。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [74] [Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits](https://arxiv.org/abs/2512.14338)
*Michael Murray,Tenzin Chan,Kedar Karhadker,Christopher J. Hillar*

Main category: cs.LG

TL;DR: Hopfield networks can learn to identify graph isomorphism classes from small samples by exploiting a three-dimensional invariant subspace; a gradient-descent energy-flow bias toward norm-efficient solutions yields polynomial sample complexity, and parameters across learning rules converge to the invariant subspace with more data, suggesting norm-efficiency as a unifying mechanism for generalization on group-structured data.


<details>
  <summary>Details</summary>
Motivation: Understanding how symmetry and group structure induce generalization in neural systems beyond explicit architectural invariances. The work investigates how emergent invariance arises in classical Hopfield networks when learning from group-structured data (graphs) and what drives efficient learning of isomorphism classes.

Method: The authors analyze Hopfield networks on graph-isomorphism tasks, identify an invariant subspace of dimension three that encodes isomorphism classes, and show that gradient descent on an energy function (MEF) has a bias toward norm-efficient solutions. They derive a polynomial sample complexity bound for learning isomorphism classes and demonstrate convergence of parameters toward the invariant subspace across multiple learning rules as sample size grows.

Result: Main theoretical result: graph isomorphism classes can be captured within a 3D invariant subspace in Hopfield networks. MEF promotes norm-efficient solutions, enabling a polynomial sample complexity bound. Empirical/theoretical evidence shows parameters converge toward the invariant subspace as sample size increases, across different learning rules.

Conclusion: A unifying mechanism for generalization in Hopfield networks is proposed: a bias toward norm efficiency during learning fosters approximate invariance under group-structured data, explaining how invariance can emerge even without explicit architectural symmetry constraints.

Abstract: Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

</details>


### [75] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 提出一种针对量子机器学习的黑盒隐私审计框架，基于提升的量子差分隐私（Lifted QDP）与量子可以币（canaries）来检测记忆化并量化训练过程中的隐私泄漏，弥补理论隐私证明与实际验证之间的空白。


<details>
  <summary>Details</summary>
Motivation: 在将敏感数据用于量子机器学习训练时，模型可能通过记忆化泄露个人记录，现有QDP理论提供最坏情形下的隐私保证但缺乏实证检测工具。本研究旨在提供一个可操作的黑盒隐私审计框架以验证和量化实际的隐私代价。

Method: 提出基于Lifted QDP的新颖黑盒 auditing 框架，使用量子canaries（经过偏移编码的量子态）来检测记忆化，通过可控偏移与迹距离的数学关系推导出对隐私预算消耗的经验下界，能够在训练过程中对隐私泄漏进行实时或离线量化评估。框架建立了从 canary 偏移量到轨迹距离界的严格联系，并在理论与实验层面（仿真与实际量子硬件）进行了验证。

Result: 在仿真和物理量子硬件上，框架能有效测量QML模型的实际隐私损失，提供对隐私预算消耗的下界评估，验证了框架在实际应用中的可行性与鲁棒性。

Conclusion: 该工作首次将 Lifted QDP 与黑盒隐私审计结合，通过量子 canaries 实现对记忆化的可观测与量化，为QML系统提供实证隐私验证工具，填补理论隐私保证与实际部署之间的空白。

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [76] [RePo: Language Models with Context Re-Positioning](https://arxiv.org/abs/2512.14391)
*Huayang Li,Tianyu Zhao,Richard Sproat*

Main category: cs.LG

TL;DR: 提出 RePo，一种可微分的上下文再定位机制，通过学习的 token 位置信息来减轻外部认知负荷，提升在噪声上下文、结构化数据与长上下文任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 受到认知负荷理论（CLT）启发，认为现有将上下文以线性/固定整数位置编码实现的架构会增加 extraneous cognitive load，耗费有限的工作记忆资源，降低深度推理与注意力分配的效率。

Method: 提出可微分模块 f_φ 来为 token 分配位置，捕捉上下文依赖关系，而非依赖预定义的整数范围。并在 OLMo-2 1B 主干上进行持续预训练以提升性能。

Result: 在噪声上下文、结构化数据以及长上下文任务中显著提升，同时在常规模型对短上下文任务保持竞争力。分析显示 RePo 能将更多注意力分配给远距离但相关的信息，位置被分配在密集且非线性空间，进而捕捉输入上下文的内在结构。

Conclusion: 该方法通过可微定位的上下文表示降低外部负荷并提升对长距离依赖的建模能力，代码公开于相应仓库。

Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

</details>


### [77] [SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design](https://arxiv.org/abs/2512.14397)
*Yunjia Yang,Weishao Tang,Mengxin Liu,Nils Thuerey,Yufei Zhang,Haixin Chen*

Main category: cs.LG

TL;DR: SuperWing 是一个大规模的开放式数据集，包含用于斜吹翼气动研究的4,239个几何形状和28,856个RANS流场解，旨在提升三维翼型的机器学习预测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习的流场预测器在三维翼型的泛化能力方面受限，主要原因是数据集稀缺且多样性不足。建立一个大规模、多样化且标注完备的数据集，是实现对典型飞行包线内的稳健预测的关键.

Method: 构建带有跨展向的翼型参数化（包括翼展方向的翼型、拉伸、扭转和撼折等）、在不同马赫数和攻角下的大范围仿真，生成4,239个翼型及其对应的28,856个RANS流场解。对表面流场进行预测的两种前沿 Transformer 模型进行基准测试，评估其在未见样本上的泛化能力，并在预训练模型上展示零-shot 对 DLR-F6、NASA CRM 等复杂翼型的迁移能力。

Result: 在 Held-out 样本上，Transformer 基准模型实现表面流场预测并达到约2.5 个 Drag-count 的误差；经在 SuperWing 上预训练的模型显示对复杂基准翼（如 DLR-F6、NASA CRM）具有强的零-shot 泛化能力。

Conclusion: 该数据集的多样性与规模为训练可泛化的三维翼型预测器提供了有力的支撑，具备实际应用潜力，同时为未来将多物理场耦合与鲁棒优化等任务集成到 ML 流场预测中奠定基础。

Abstract: Machine-learning surrogate models have shown promise in accelerating aerodynamic design, yet progress toward generalizable predictors for three-dimensional wings has been limited by the scarcity and restricted diversity of existing datasets. Here, we present SuperWing, a comprehensive open dataset of transonic swept-wing aerodynamics comprising 4,239 parameterized wing geometries and 28,856 Reynolds-averaged Navier-Stokes flow field solutions. The wing shapes in the dataset are generated using a simplified yet expressive geometry parameterization that incorporates spanwise variations in airfoil shape, twist, and dihedral, allowing for an enhanced diversity without relying on perturbations of a baseline wing. All shapes are simulated under a broad range of Mach numbers and angles of attack covering the typical flight envelope. To demonstrate the dataset's utility, we benchmark two state-of-the-art Transformers that accurately predict surface flow and achieve a 2.5 drag-count error on held-out samples. Models pretrained on SuperWing further exhibit strong zero-shot generalization to complex benchmark wings such as DLR-F6 and NASA CRM, underscoring the dataset's diversity and potential for practical usage.

</details>


### [78] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [79] [Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space](https://arxiv.org/abs/2512.14418)
*Dejun Hu,Zhiming Li,Jia-Rui Shen,Jia-Ning Tu,Zi-Hao Ye,Junliang Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.

</details>


### [80] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 提出了 ClimaX-LETKF，一种纯数据驱动的ML基集合天气预报系统，通过同化观测与集合，RTPP比RTPS在稳定性与准确性方面表现更优，MLWP相较NWP更难恢复大气 attractor；为实际应用提供了见解。


<details>
  <summary>Details</summary>
Motivation: 解决 MLWP 在 assimilating 真实观测与集合预报方面的局限，提升稳定性与可用性。

Method: 提出基于数据驱动的 LETKF 风格同化框架的 MLWP 系统 ClimaX-LETKF，使用 NCEP ADP 全局高空气象观测及地面观测进行同化，独立于 NWP 模型，比较 RTPP 与 RTPS 的影响。

Result: 相比 RTPS，RTPP 使 MLWP 在稳定性与准确性方面更好；NWP 模型对 RTPS 更稳定；MLWP 更难将大气场恢复到 attractor；提供对 MLWP 集合预报的改进方向。

Conclusion: 该工作推动 MLWP 集合预报的实用化，并为未来通过观测同化改进数据驱动模型提供重要见解。

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [81] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 当前关于时间序列分类的对反事实解释多基于静态假设，难以在临床场景中提供可持续、可执行的干预。本文主张发展与临床推理及患者动态相符的持续、目标导向的对反事实，并指出时间维度的盲点与缺乏用户中心的评估；通过对前沿方法的鲁棒性分析，揭示对反事实对随机噪声高度敏感，现实应用可靠性不足；呼吁建立关注可行性和行动性的评估框架。


<details>
  <summary>Details</summary>
Motivation: 临床推荐领域中的干预具有时序性、因果性与个体化动态特征，解释方法需不仅仅改变预测结果，还要提供可执行、时间一致的干预方案。现有方法多在静态假设下优化最小扰动，缺乏因果性与可操作性评估，限制实际应用。

Method: 对多种时间序列对反事实方法进行鲁棒性分析，评估其对随机噪声的敏感性；并进行方法缺口分析、评估指标讨论，强调需要从预测变化转向可执行干预的框架。

Result: 发现所评估的对反事实对随机噪声高度敏感，导致在现实世界中可靠性不足；并揭示存在的时间维度盲点和缺乏用户中心设计的问题。

Conclusion: 呼吁建立更贴近临床可行性的评估框架与方法，聚焦可执行、持续性干预，并设计面向临床目标的评价指标，超越单纯预测改变。

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [82] [Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection](https://arxiv.org/abs/2512.14563)
*Tejaswani Dash,Gautam Datla,Anudeep Vurity,Tazeem Ahmad,Mohd Adnan,Saima Rafi,Saisha Patro,Saina Patro*

Main category: cs.LG

TL;DR: 提出带残差GRU与多头自注意力的轻量混合模型用于表格临床记录的心血管疾病预测，在UCI心脏病数据集上表现超越各类基线，兼具高效性。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球死亡的首要因素；需要对嘈杂、异质的临床表格数据进行鲁棒预测，并在资源受限的医疗环境中实现高效部署。

Method: 引入残差双向GRU用于列维度的序列建模，加入通道重权重化块，以及带可学习分类标记的多头自注意力池化，形成 Residual GRU with Multi-Head Self-Attention 架构；在5折分层交叉验证上与经典（LR、RF、SVM）和现代深度学习基线（DeepMLP、CNN、RNN、Transformer）对比；进行消融实验以评估残差结构、通道门控和注意力池化的贡献，并使用t-SNE可视化嵌入。

Result: 在Heart Disease数据集上，准确率0.861、宏F1 0.860、ROC-AUC 0.908、PR-AUC 0.904，优于所有基线；消融验证各组成部分的贡献显著；嵌入可视化显示比原始特征更清晰的疾病/非疾病分离；未来指向在资源受限环境中的临床风险预测应用。

Conclusion: 该轻量级的混合递归-注意力架构实现了较高的预测性能与高效性平衡，适合在资源受限的医疗场景中部署。

Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

</details>


### [83] [Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes](https://arxiv.org/abs/2512.14617)
*Alessandro Trapasso,Luca Iocchi,Fabio Patrizi*

Main category: cs.LG

TL;DR: 提出并分析 QR-MAX/ Bucket-QR-MAX：一个面向离散 NMRDP 的模型基 RL，利用奖励机实现非马尔可夫奖励的分解学习，具备 PAC 收敛性和多项式样本复杂度；并扩展到连续状态空间。


<details>
  <summary>Details</summary>
Motivation: 许多决策问题的成功取决于系统历史而非单点状态；标准马尔可夫奖励的 RL 在此类任务中不成立，非马尔可夫奖励决策过程（NMRDP）需要更强的理论保证和样本效率。

Method: 提出 QR-MAX：一个基于模型的离散 NMRDP 算法，通过奖励机将马尔可夫性转移学习与非马尔可夫奖励处理分解；在离散动作下实现对 ε-近似最优策略的 PAC 收敛，样本复杂度为多项式。扩展到连续状态使用 Bucket-QR-MAX，基于 SimHash 的离散化，保留分解结构并实现快速稳定学习，且无需网格化或函数近似。

Result: 与现代模型基 RL 方法在逐步增长的复杂度环境中比较，提出方法在样本效率显著提升，找到最优策略更鲁棒。

Conclusion: 通过对学习-奖励分解的结构化处理，首次给出离散行动 NMRDP 的模型基 RL 的 PAC 收敛性；Bucket-QR-MAX 将该结构推广到连续状态，展示了在现实任务中的有效性和鲁棒性。

Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

</details>


### [84] [ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning](https://arxiv.org/abs/2512.14619)
*Chaohao Yuan,Zhenjie Song,Ercan Engin Kuruoglu,Kangfei Zhao,Yang Liu,Deli Zhao,Hong Cheng,Yu Rong*

Main category: cs.LG

TL;DR: ParaFormer提出基于PageRank的注意力模块，作为自适应谱滤波器，缓解图 Transformers 的过度平滑问题，并在11个数据集上实现节点和图级分类的性能提升。


<details>
  <summary>Details</summary>
Motivation: 全局注意力在摆脱深度GNN的同时引入过度平滑，导致节点表征变得同质化，低通效应强于传统GNN；需要在保持全局信息的同时维持区分性。

Method: 设计PageRank增强的注意力模块ParaFormer，理论与实验表明其充当自适应的谱滤波器，模拟深层Transformer的行为并缓解过平滑；并在节点分类和图分类任务上进行大规模实验（11个数据集，规模从千到百万节点）。

Result: ParaFormer在节点分类与图分类任务上表现出稳定的性能提升，覆盖多种数据规模和数据集。

Conclusion: 将PageRank理念注入注意力以实现自适应谱滤波，提供一种可扩展且原理清晰的方式缓解GTs的过平滑问题，实验验证其有效性；代码与附录可获取。

Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

</details>


### [85] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: 多模态早期预警系统（EWI）用于预测ICU入院、急救派遣与死亡的综合风险，结合人机协作与SHAP解释，在医院部署实现三等级风险分层；在18363名患者数据上C-stat≈0.796，用于triage、资源调度与流程优化。


<details>
  <summary>Details</summary>
Motivation: 解决医院在异构EHR数据中自动化、可解释的风险评估不足的问题，以提升对危重患者的早期识别与护理管理效率。

Method: 提出Early Warning Index（EWI）多模态机器学习框架，融合结构化与非结构化EHR数据；采用人机交互的阈值设定及可解释输出，利用SHAP突出驱动因素；医院仪表盘实现三等级风险分层；在18363名患者数据集上训练与评估。

Result: 模型实现C-stat≈0.796，被用作患者分流的三线护理 triage 工具，帮助医生更高效地关注高风险患者；异常驱动因素（如手术安排、病房人口等）可用于优化排班与资源分配；有望降低并发症、再入院率并改善患者流动。

Conclusion: EWI提供可解释且可操作的多模态风险分层，在实际医院环境中展现出可用性，支持早期干预与资源管理。

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [86] [Pipeline Stage Resolved Timing Characterization of FPGA and ASIC Implementations of a RISC V Processor](https://arxiv.org/abs/2512.13866)
*Mostafa Darvishi*

Main category: eess.SP

TL;DR: 对32位RISC-V处理器在20nm FPGA和7nm FinFET ASIC上的流水线时序特征进行阶段级分解比较，揭示FPGA受路由寄生效应支配、ASIC由组合逻辑深度与工艺波动支配的不同时序机制。


<details>
  <summary>Details</summary>
Motivation: 提供跨异构实现平台的微架构层面时序对比，理解不同实现技术在可预测性闭环设计中的瓶颈。

Method: 提出一个流水线阶段分解的时序分析框架，将时序路径分解为逻辑、布线和时钟等分量，并映射到具体的流水线阶段转移；结合静态时序分析和统计表征，对FPGA与ASIC进行对比。

Result: 在EX到MEM的关键阶段，两者都存在主要的关键路径，但机制不同：FPGA路由寄生和放置相关的变异导致较宽的时序分布，ASIC则由组合逻辑深度和工艺-电压-温度变动驱动，时序分布更窄且可预测。

Conclusion: 流水线阶段分解分析对跨平台时序理解有效，能够识别平台特定瓶颈，并给出实现FPGA与ASIC的可预测时序闭合设计的设计建议。

Abstract: This paper presents a pipeline stage resolved timing characterization of a 32-bit RISC V processor implemented on a 20 nm FPGA and a 7 nm FinFET ASIC platform. A unified analysis framework is introduced that decomposes timing paths into logic, routing, and clocking components and maps them to well-defined pipeline stage transitions. This approach enables systematic comparison of timing behavior across heterogeneous implementation technologies at a microarchitectural level. Using static timing analysis and statistical characterization, the study shows that although both implementations exhibit dominant critical paths in the EX to MEM pipeline transition, their underlying timing mechanisms differ fundamentally. FPGA timing is dominated by routing parasitics and placement dependent variability, resulting in wide slack distributions and sensitivity to routing topology. In contrast, ASIC timing is governed primarily by combinational logic depth and predictable parametric variation across process, voltage, and temperature corners, yielding narrow and stable timing distributions. The results provide quantitative insight into the structural origins of timing divergence between programmable and custom fabrics and demonstrate the effectiveness of pipeline stage resolved analysis for identifying platform specific bottlenecks. Based on these findings, the paper derives design implications for achieving predictable timing closure in processor architectures targeting both FPGA and ASIC implementations.

</details>


### [87] [Simultaneous and Proportional Finger Motion Decoding Using Spatial Features from High-Density Surface Electromyography](https://arxiv.org/abs/2512.13870)
*Ricardo Gonçalves Molinari,Leonardo Abdala Elias*

Main category: eess.SP

TL;DR: MLD-BFM结合HD sEMG的区域空间特征，显著提升对五个手指DoF的连续解码精度，优于传统时域特征和降维方法，MLP+MLD-BFM表现最好，中指和无名指解码更优。


<details>
  <summary>Details</summary>
Motivation: 寻求自然直观的多DoF手部控制，通过利用高密度肌电信号的空间信息来改善连续解码，以实现更真实的仿生/控制体验。

Method: 使用多通道线性描述符的区块场方法（MLD-BFM）从HD sEMG提取区域特定的空间特征（有效场强Σ、场强变化率Φ、空间复杂度Ω），对来自EDC和FDS肌肉的信号进行五指DoF的连续解码。对比常规时域特征与降维方法，在多输出回归模型上优化参数（区块大小2×2，滑动窗口0.15s），并以加权R^2（R^2_vw）评估性能。

Result: MLD-BFM在所有模型中均获得最高的R^2_vw；将MLD-BFM与MLP组合的模型表现最佳，R^2_vw为86.68%±0.33。时域特征也表现良好，在某些模型与MLD-BFM统计学上相当；降维方法则性能较差。在解码时，中指和无名指的准确性高于拇指。总体而言，空间丰富性的利用显著提升了连续手指运动解码和 SPC 的实现潜力。

Conclusion: 强调利用HD sEMG的空间结构特征可以增强SPC的鲁棒性和实时性，为设计强健、实时响应的肌电接口提供实用指引。

Abstract: Restoring natural and intuitive hand function requires simultaneous and proportional control (SPC) of multiple degrees of freedom (DoFs). This study systematically evaluated the multichannel linear descriptors-based block field method (MLD-BFM) for continuous decoding of five finger-joint DoFs by leveraging the rich spatial information of high-density surface electromyography (HD sEMG). Twenty-one healthy participants performed dynamic sinusoidal finger movements while HD sEMG signals were recorded from the \textit{extensor digitorum communis} (EDC) and \textit{flexor digitorum superficialis} (FDS) muscles. MLD-BFM extracted region-specific spatial features, including effective field strength ($Σ$), field-strength variation rate ($Φ$), and spatial complexity ($Ω$). Model performance was optimized (block size: $2 \times 2$; window: 0.15 s) and compared with conventional time-domain features and dimensionality reduction approaches when applied to multi-output regression models. MLD-BFM consistently achieved the highest $\mathrm{R}^2_{\mathrm{vw}}$ values across all models. The multilayer perceptron (MLP) combined with MLD-BFM yielded the best performance ($\mathrm{R}^2_{\mathrm{vw}} = 86.68\% \pm 0.33$). Time-domain features also showed strong predictive capability and were statistically comparable to MLD-BFM in some models, whereas dimensionality reduction techniques exhibited lower accuracy. Decoding accuracy was higher for the middle and ring fingers than for the thumb. Overall, MLD-BFM improved continuous finger movement decoding accuracy, underscoring the importance of taking advantage of the spatial richness of HD sEMG. These findings suggest that spatially structured features enhance SPC and provide practical guidance for designing robust, real-time, and responsive myoelectric interfaces.

</details>


### [88] [Hierarchical Deep Reinforcement Learning for Robust Access in Cognitive IoT Networks under Smart Jamming Attacks](https://arxiv.org/abs/2512.14013)
*Nadia Abdolkhani,Walaa Hamouda*

Main category: eess.SP

TL;DR: 提出一种三层次 H-DDPG 框架用于认知物联网中的动态频谱访问，在能量约束和对抗性干扰下，联合决策传输/能量收集、信道选择与功率控制，且对手干扰者（干扰器）采用离散DDPG策略，实验表明优于传统单层 RL 基线。


<details>
  <summary>Details</summary>
Motivation: 在CIoT 场景中，二级用户需在能量约束和对主用户监管的前提下实现高效通信，且同时对抗智能干扰，需处理混合离散-连续动作与多级策略带来的挑战。

Method: 提出层次化的深度确定性策略梯度(H-DDPG)：高层决定模式（传输/能量收集）、中层选择信道、低层输出连续功率；干扰器作为 RL 智能体，使用离散-DDPG 学习自适应信道干扰策略。

Result: 通过仿真验证，H-DDPG 在目标任务上显著优于传统的扁平化 RL 基线，展现了对混合动作空间和多层决策结构的有效性。

Conclusion: 层次化 DDPG 能有效处理 CIoT 中的动态频谱访问与能量约束问题，且具备对抗性干扰的鲁棒性；未来工作可进一步提升样本效率与对未知干扰的鲁棒性。

Abstract: In this paper, we address the challenge of dynamic spectrum access in a cognitive Internet of Things (CIoT) network where a secondary user (SU) operates under both energy constraints and adversarial interference from a smart jammer. The SU coexists with primary users (PUs) and must ensure that its transmissions do not exceed a predefined interference threshold on licensed channels. At each time slot, the SU must jointly determine whether to transmit or harvest energy, which channel to access, and the appropriate transmit power while satisfying energy and interference constraints. Meanwhile, a smart jammer actively selects a channel to disrupt, aiming to degrade the SU's communication performance. This setting presents a significant challenge due to its multi-level decision structure and hybrid action space, which combines both discrete and continuous decisions. To tackle this, we propose a novel Hierarchical Deep Deterministic Policy Gradient (H-DDPG) framework that decomposes the decision-making process into three levels: the high-level policy determines the mode (transmit or harvest), the mid-level policy selects the channel, and the low-level actor outputs a continuous power level. Concurrently, the jammer is modeled as a reinforcement learning agent that learns an adaptive channel jamming strategy using a discrete variant of DDPG. Simulation results show that our H-DDPG approach outperforms conventional flat reinforcement learning baselines.

</details>


### [89] [Cooperative Rotatable IRSs for Wireless Communications: Joint Beamforming and Orientation Optimization](https://arxiv.org/abs/2512.14037)
*Qiaoyan Peng,Qingqing Wu,Guangji Chen,Wen Chen,Shanpu Shen,Shaodan Ma*

Main category: eess.SP

TL;DR: 提出了可旋转的智能反射表面（IRS）来增强无线传播，通过两枚可旋转的 IRS 的角度依赖反射模型来联合设计基站波束、被动波束形成和 IRS 方向，以最大化接收信噪比（SNR）。在 LoS 条件下，采用粒子群优化（PSO）确定旋转，并给出二维部署的闭式最优旋转；在一般 Rician 衰落通道下，提出交替优化与 PSO 的 AO-PSO 算法。数值结果表明，IRS 旋转相对于固定 IRS 方案具有显著增益，且当总元素数量充足时，双可旋转 IRS 相对于单可旋转 IRS 具有更优性能。


<details>
  <summary>Details</summary>
Motivation: 利用 IRS 的旋转自由度作为新的控制维度来进一步塑造无线传播，以提高在具有角度 의 의 real-world 的 LoS 与 Rician 条件下的信道增益，研究两枚 IRS 的协同设计以提升接收信噪比。

Method: 在 LoS 场景下，提出基于粒子群优化的旋转优化方法，并为二维部署给出最优旋转的闭式解；在一般的 Rician 衰落通道中，改进为交替优化加 PSO 的 AO-PSO 算法，以联合优化基站波束、被动波束和 IRS 方向。

Result: 数值结果验证了旋转 IRS 相较于固定 IRS 的显著增益，并显示在充足总元素条件下，双可旋转 IRS 比单可旋转 IRS 更具优势。

Conclusion: 可旋转 IRS 提供了新的控制自由度，能显著提升多 IRS 协同设计的性能，双 IRS 架构在资源充分时具有更大潜力；所提出的 AO-PSO/PSO 算法具备较好的实用性与效率。

Abstract: Rotatable intelligent reflecting surfaces (IRSs) introduce a new degree of freedom (DoF) for shaping wireless propagation by adaptively adjusting the orientation of IRSs. This paper considers an angle-dependent reflection model in a wireless communication system aided by two rotatable IRSs. Specifically, we study the joint design of the base station transmit beamforming, as well as the cooperative passive beamforming and orientation of the two IRSs, to maximize the received signal-to-noise ratio (SNR). Under the light-of-sight (LoS) channels, we first develop a particle swarm optimization (PSO) based method to determine the IRS rotation and derive an optimal rotation in a closed-form expression for a two-dimensional IRS deployment. Then, we extend the design to the general Rician fading channels by proposing an efficient alternating optimization and PSO (AO-PSO) algorithm. Numerical results validate the substantial gains achieved by the IRS rotation over fixed-IRS schemes and also demonstrate the superior performance of the double rotatable IRSs over a single rotatable IRS given a sufficient total number of IRS elements.

</details>


### [90] [Hybrid Iterative Detection for OTFS: Interplay between Local L-MMSE and Global Message Passing](https://arxiv.org/abs/2512.14116)
*Ruohai Yang,Shuangyang Li,Han Yu,Zhiqiang Wei,Kai Wan,Giuseppe Caire*

Main category: eess.SP

TL;DR: 提出一种低复杂度的混合检测框架用于 OTFS。通过延迟‑多普勒交换预编码器（DDCP）将 DD 域信号重新排列，使等效通道呈现局部密集块和稀疏连接的结构；在密集块中使用低维 L-MMSE 估计，在块间稀疏连接处使用 MP 进行概率推断，从而实现更低复杂度但接近最优误码性能的检测。


<details>
  <summary>Details</summary>
Motivation: 现有的线性均衡器和消息传递方法在高多普勒/高移动性、存在分数延迟与多普勒移位的双对数性通道中易放大噪声或失效，需在保持性能的同时降低计算复杂度。

Method: 引入 DDCP 将 DD 域信号向量经预编码，使等效通道矩阵呈现若干局部密集块之间稀疏连接的结构。提出混合迭代检测策略：对密集块执行低维 L-MMSE 估计，对块间的稀疏连接使用 MP 进行概率推断。给出复杂度分析，显示相比全尺寸 L-MMSE 有显著降低。通过仿真实验验证收敛性、复杂度可控性及 BER 的提升。

Result: 仿真结果显示该混合检测在不同信道条件下实现快速收敛、较低复杂度，并显著提升 BER，尤其在多径场景下的 BER 可接近匹配滤波界（MFB），表现出近似最优的误码性能。

Conclusion: 基于 DDCP 重塑通道结构并结合 L-MMSE/MP 的混合迭代检测框架，为高移动性和双时变信道下的 OTFS 提供了一种低复杂度且接近最优性能的检测方案。

Abstract: Orthogonal time frequency space (OTFS) modulation has emerged as a robust solution for high-mobility wireless communications. However, conventional detection algorithms, such as linear equalizers and message passing (MP) methods, either suffer from noise enhancement or fail under complex doubly-selective channels, especially in the presence of fractional delay and Doppler shifts. In this paper, we propose a hybrid low-complexity iterative detection framework that combines linear minimum mean square error (L-MMSE) estimation with MP-based probabilistic inference. The key idea is to apply a new delay-Doppler (DD) commutation precoder (DDCP) to the DD domain signal vector, such that the resulting effective channel matrix exhibits a structured form with several locally dense blocks that are sparsely inter-connected. This precoding structure enables a hybrid iterative detection strategy, where a low-dimensional L-MMSE estimation is applied to the dense blocks, while MP is utilized to exploit the sparse inter-block connections. Furthermore, we provide a detailed complexity analysis, which shows that the proposed scheme incurs lower computational cost compared to the full-size L-MMSE detection. The simulation results of convergence performance confirm that the proposed hybrid MP detection achieves fast and reliable convergence with controlled complexity. In terms of error performance, simulation results demonstrate that our scheme achieves significantly better bit error rate (BER) under various channel conditions. Particularly in multipath scenarios, the BER performance of the proposed method closely approaches the matched filter bound (MFB), indicating its near-optimal error performance.

</details>


### [91] [Antenna Coding Optimization Based on Pixel Antennas for MIMO Wireless Power Transfer with DC Combining](https://arxiv.org/abs/2512.14135)
*Yijun Chen,Shanpu Shen,Tianrui Qiao,Hongyu Li,Jun Qian,Ross Murch*

Main category: eess.SP

TL;DR: 提出基于像素天线的天线编码作为提升多输入多输出无线电力传输（WPT）系统的新自由度。引入波束空间通道模型，联合优化天线编码与发射波束成形（在完全CSI下）以利用天线编码、波束成形及整流器非线性带来的增益，采用交替优化（准牛顿法用于波束成形，带热启动的逐步穷举布尔优化SEBO用于天线编码）。仿真结果表明，与固定天线配置的传统系统相比，所提MIMO WPT系统的平均输出直流功率可提升约15 dB，显示像素天线在提升WPT效率方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 提高WPT效率，尤其在MIMO场景下，通过引入天线编码和可重构辐射模式的灵活性来额外提升能量传输性能。将FAS相关理念与像素天线结合，作为新的可控自由度，结合整流器非线性进行系统级优化。

Method: 提出波束空间通道模型以实现可重构辐射模式；在完全CSI条件下，联合优化天线编码与发射波束成形；采用交替优化框架：对波束成形使用准牛顿法，对天线编码使用带热启动的SEBO方法进行布尔优化。

Result: 仿真结果显示，所提系统在平均输出DC功率方面相对于固定天线配置的传统系统可实现最大约15 dB的增益，证明像素天线与天线编码在WPT中的有效性。

Conclusion: 像素天线结合天线编码与联合波束成形的策略能够显著提升MIMO WPT的效率，展现了天线编码作为新自由度以及对可重构辐射模式的潜在价值。

Abstract: This paper investigates antenna coding based on pixel antennas as a new degree of freedom for enhancing multiple-input multiple-output (MIMO) wireless power transfer (WPT) systems. Antenna coding is closely related to the Fluid Antenna System (FAS) concept and further generalizes the radiation pattern reconfigurability. We first introduce a beamspace channel model to demonstrate reconfigurable radiation patterns enabled by antenna coders. By jointly optimizing the antenna coding and transmit beamforming with perfect channel state information (CSI), we exploit gains from antenna coding, transmit beamforming, and rectenna nonlinearity to maximize the output DC power. We adopt an alternating optimization approach with the quasi-Newton method and Successive Exhaustive Boolean Optimization (SEBO) method with warm-start to handle the transmit beamforming design and antenna coding design respectively. Finally, simulation results show that the proposed MIMO WPT system with pixel antennas achieves up to 15 dB gain in average output DC power compared with a conventional system with fixed antenna configuration, highlighting the potential of pixel antennas for boosting the WPT efficiency.

</details>


### [92] [Rethinking Gaussian-Windowed Wavelets for Damping Identification](https://arxiv.org/abs/2512.14205)
*Hadi M. Daniali,Martin v. Mohrenschildt*

Main category: eess.SP

TL;DR: 提出一种数据驱动的非高斯包络估计框架，结合合成冲激响应的已知包络，优化包络形状与参数，用于阻尼估计；并与频域方法比较，结论是在中到高SNR下，非高斯优化包络往往优于高斯小波，Triangle/ Welch优于高斯，Blackman在低SNR和紧近模态时鲁棒；LSRF在极低SNR下最可靠，随着SNR升高，非高斯包络法表现出色。


<details>
  <summary>Details</summary>
Motivation: 在模态分析中，常用的高斯基小波（如 Morlet、Gabor）用于阻尼估计几乎不被质疑，本文质疑这一常态，提出基于包络的估计及数据驱动优化以提升鲁棒性和准确性。

Method: 提出一种数据驱动框架，利用已知 ground-truth 包络的合成冲激响应来优化包络的形状和参数；将得到的估计器与频域方法（LSRF、pLSCF、PP、Yoshida）在多种场景下进行基准比较。

Result: Triangle 与 Welch 窗在中高SNR条件下表现优于或等同于高斯小波；Blackman 在低SNR及模态间距近时具有更强鲁棒性；在极低SNR下，LSRF最为可靠；随着SNR提高，非高斯优化包络估计器性能突出。

Conclusion: 非高斯优化包络在多种工作条件下具备竞争力，数据驱动优化使包络更好地贴合 Ground-truth，LSRF在极低SNR时仍稳健；整体而言，包络优化方法可在不同SNR和模态间距下提升阻尼估计表现。

Abstract: In modal analysis, the prevalent use of Gaussian-based wavelets (such as Morlet and Gabor) for damping estimation is rarely questioned. In this study, we challenge this conventional approach by systematically exploring envelope-based damping estimators and proposing a data-driven framework that optimizes the shape and parameters of the envelope utilizing synthetic impulse responses with known ground-truth envelopes. The performance of the resulting estimators is benchmarked across a range of scenarios and compared against frequency-domain damping estimation methods, including Least Squares Rational Function (LSRF), poly-reference Least Squares Complex Frequency-Domain (pLSCF), peak picking (PP), and the Yoshida method. Our findings indicate that Triangle and Welch windows consistently outperform or are on par with Gaussian wavelet methods in contexts of moderate to high signal-to-noise ratios (SNR). In contrast, Blackman filtering demonstrates superior robustness under low SNR conditions and scenarios involving closely spaced modes. Among the frequency-domain methods assessed, LSRF shows the most reliability at very low SNR; however, the non-Gaussian optimized envelope estimators perform exceptionally well as the SNR improves.

</details>


### [93] [User Localization and Channel Estimation for Pinching-Antenna Systems (PASS)](https://arxiv.org/abs/2512.14351)
*Xiaoxia Xu,Xidong Mu,Yuanwei Liu,Hong Xing,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: 提出一种用于 pinching-antenna 系统的用户定位与信道估计框架，比较单波导(SW)与多波导(MW)结构，提出基于OMP的几何一致性定位算法（OMP-GCL）并扩展到3D，附CRLB分析。结果表明MW提升几何多样性，实现全空间观测，且在仅用三根波导时即可达到2D厘米级和3D分米级定位，SW存在角度模糊。


<details>
  <summary>Details</summary>
Motivation: 现有的 pinching-antenna 系统在用户定位与信道估计方面受限于几何观测范围和观测开销。通过将天线分组为子阵列并在SW/MW结构下协同测量，可利用几何关系和压缩感知提升定位精度与信道重构能力，同时确保在2D/3D场景下的鲁棒性与低开销。

Method: 提出 PASS 框架，将 pinching 天线分组为子阵列，在 SW（交替激活子阵列）与 MW（每个波导一个子阵列）两种结构中实现协同测量；提出基于正交匹配追踪的几何一致性定位算法（OMP-GCL），利用子阵之间的几何关系与压缩感知实现高精度定位与信道重构；将 OMP-GCL 扩展至 3D 场景，同时估计用户和散射体高度；并给出 CRLB 分析以评估几何多样性对估计性能的影响。

Result: CRLB 表明通过增加几何多样性（更丰富的子阵列布置），估计精度提升；SW 在 180°半空间内受限，易产生角度模糊；MW 能实现全空间观测并降低开销。数值结果验证理论分析，且仅需三根波导即可在 2D 达到厘米级、在 3D 达到分米级定位精度。

Conclusion: MW 架构结合 OMP-GCL 在 2D/3D 场景下都能实现高精度的用户定位与信道估计，相较于 SW，MW 提供更丰富的几何观测、消除了角度模糊并降低系统开销。

Abstract: This letter proposes a novel user localization and channel estimation framework for pinching-antenna systems (PASS), where pinching antennas are grouped into subarrays on each waveguide to cooperatively estimate user/scatterer locations, thus reconstructing channels. Both single-waveguide (SW) and multi-waveguide (MW) structures are considered. SW consists of multiple alternatingly activated subarrays, while MW deploys one subarray on each waveguide to enable concurrent subarray measurements. For the 2D scenarios with a fixed user/scatter height, an orthogonal matching pursuit-based geometry-consistent localization (OMP-GCL) algorithm is proposed, which leverages inter-subarray geometric relationships and compressed sensing for precise estimation. Theoretical analysis on Cramér-Rao lower bound (CRLB) demonstrates that: 1) The estimation accuracy can be improved by increasing the geometric diversity through multi-subarray deployment; and 2) SW provides a limited geometric diversity within a $180^\circ$ half space and leads to angle ambiguity, while MW enables full-space observations and reduces overheads. The OMP-GCL algorithm is further extended to 3D scenarios, where user and scatter heights are also estimated. Numerical results validate the theoretical analysis, and verify that MW achieves centimeter- and decimeter-level localization accuracy in 2D and 3D scenarios with only three waveguides.

</details>


### [94] [Pragmatic Earth-Fixed Beam Management for 3GPP NTN Common Signaling in LEO Satellites](https://arxiv.org/abs/2512.14368)
*Xavier Artiga,Màrius Caus,Ana Pérez-Neira*

Main category: eess.SP

TL;DR: 提出一种面向3GPP NTN的 pragmatic beam footprint 和 beam hopping 设计方法，以在 EIRP 受限的LEO卫星下高效广播共同信令。通过地球固定网格、波束形成向量与功率分配、波束跳跃模式以及时空频资源分配，显著减少覆盖所需波束数并实现高覆盖与高共同信令效率。


<details>
  <summary>Details</summary>
Motivation: 在大覆盖区域内以 EIRP 有限的LEO卫星广播3GPP NTN共同信令时，需要最小化覆盖 sweeping 的时间资源，同时确保用户端的信噪比达到阈值，因此需要高效的波束布局与资源调度策略。

Method: 提出两种波束布局方案并结合三个设计层次：1) 基于地球固定网格的波束布局；2) 波束形成向量与功率分配；3) 波束跳跃模式；4) 时空频资源分配。两种波束布局分别为：基于相控阵的低跨越波束和扩展波束。通过数值仿真在给定系统参数下比较性能，评估了覆盖率、信号干扰与共同信令效率。

Result: 两种方案在数值仿真中的性能相近；其中基于相控阵且优化跨越比的波束表现最佳。系统在评估条件下可将总波束数从1723降至451，并结合波束跳跃与调度实现覆盖率100%及共同信令效率高达80.6%（20 ms 周期下）。

Conclusion: 使用优化跨越的相控阵波束作为首选方案，能够显著降低波束数量并实现高覆盖与高效的共同信令传输，所提出的框架对大覆盖 NTN 系统具有实际应用潜力。

Abstract: This work proposes a pragmatic method for the design of beam footprint layouts and beam hopping illumination patterns to efficiently broadcast 3GPP NTN common signaling to large coverage areas using EIRP-limited LEO satellites. This method minimizes the time resources required to sweep over the whole coverage while ensuring that the signal-to-interference-plus-noise ratio received by users is above a given threshold. It discusses the design of: (i) an Earth-fixed grid of beam layouts; (ii) beamforming vectors and beam power allocation; (iii) beam hopping patterns and (iv) space, time and frequency resource allocation of 3GPP common signaling. Two main beam layout solutions are proposed to significantly reduce the number of beams required to illuminate the coverage area: one based on phased array beams with low beam crossover levels and the other on widened beams. A numerical evaluation using practical system parameters showed that both solutions perform similarly, but that the best result is obtained with phased arrays beams with optimized beam cross over levels. Indeed, for the system evaluated, they allowed reducing the total number of beams from 1723 to 451, which combined with a proper beam hopping pattern and scheduling scheme allowed obtaining a coverage ratio of 100% and a common signaling efficiency (i.e. number of slots carrying common signaling over total number of slots) up to 80.6% for the most stringent common signaling periodicity of 20 ms considered by 3GPP.

</details>


### [95] [Terahertz Signal Coverage Enhancement in Hall Scenarios Based on Single-Hop and Dual-Hop Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2512.14394)
*Ben Chen,Zhangdui Zhong,Ke Guan,Danping He,Yiran Wang,Jianwen Ding,Qi Luo*

Main category: eess.SP

TL;DR: 通过在射线追踪中引入基于天线阵列的 RIS 模型，评估单跳/双跳 RIS 在室内 THz 覆盖中的提升，揭示部署方案对覆盖的影响，并提供设计参考。


<details>
  <summary>Details</summary>
Motivation: THz 通信面临极高的自由空间路径损耗，覆盖范围受限；RIS 能动态调控传播，潜在提升室内 THz 覆盖，因此需要量化评估不同 RIS 部署对覆盖的影响。

Method: 将基于天线阵列的 RIS 模型整合进射线追踪仿真平台，在室内大厅场景中对单跳与双跳 RIS 的覆盖提升进行对比评估，考察多种部署方案，给出可操作的设计参考。

Result: 实验/仿真表明 RIS 能显著提升室内 THz 的覆盖，单跳与双跳配置在不同部署条件下各有优劣，给出对覆盖率与信号强度的提升趋势；所建立的仿真框架可用于 RIS 辅助室内 THz 通信的覆盖估计与优化。

Conclusion: 在室内 THz 通信场景中，RIS 的多跳部署可有效扩展有效覆盖区，且将天线阵列 RIS 纳入射线追踪仿真有助于得到可操作的设计原则与覆盖估计工具。

Abstract: Terahertz (THz) communication offers ultra-high data rates and has emerged as a promising technology for future wireless networks. However, the inherently high free-space path loss of THz waves significantly limits the coverage range of THz communication systems. Therefore, extending the effective coverage area is a key challenge for the practical deployment of THz networks. Reconfigurable intelligent surfaces (RIS), which can dynamically manipulate electromagnetic wave propagation, provide a solution to enhance THz coverage. To investigate multi-RIS deployment scenarios, this work integrates an antenna array-based RIS model into the ray-tracing simulation platform. Using an indoor hall as a representative case study, the enhancement effects of single-hop and dual-hop RIS configurations on indoor signal coverage are evaluated under various deployment schemes. The developed framework offers valuable insights and design references for optimizing RIS-assisted indoor THz communication and coverage estimation.

</details>


### [96] [Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components](https://arxiv.org/abs/2512.14426)
*Simon Steuernagel,Marcus Baum*

Main category: eess.SP

TL;DR: 一个确定性闭式解的椭圆扩展对象跟踪器，解耦运动学、姿态和轴长度；具备高效批处理版本，精度达到或接近采样法，并在汽车雷达数据上验证。


<details>
  <summary>Details</summary>
Motivation: 扩展对象跟踪需要同时估计目标尺寸和运动状态；现有方法要么复杂、要么受限于近似假设，难以在线高效实现。

Method: 将运动学、姿态与轴长度解耦，构建确定性闭式滤波器；在每步中对各分量独立估计以降低近似；提出批处理变体以提升计算效率。

Result: 与最先进的算法相比，精度达到采样法水平，批处理变体在速度和性能上优于同类方法；在仿真和真实雷达数据上均有证明。

Conclusion: 该工作提供了一种高效且准确的扩展对象跟踪策略，适合需要快速推断对象形状与运动的在线应用和大规模数据场景。

Abstract: Extended object tracking involves estimating both the physical extent and kinematic parameters of a target object, where typically multiple measurements are observed per time step. In this article, we propose a deterministic closed-form elliptical extended object tracker, based on decoupling of the kinematics, orientation, and axis lengths. By disregarding potential correlations between these state components, fewer approximations are required for the individual estimators than for an overall joint solution. The resulting algorithm outperforms existing algorithms, reaching the accuracy of sampling-based procedures. Additionally, a batch-based variant is introduced, yielding highly efficient computation while outperforming all comparable state-of-the-art algorithms. This is validated both by a simulation study using common models from literature, as well as an extensive quantitative evaluation on real automotive radar data.

</details>


### [97] [Chirp Delay-Doppler Domain Modulation Based Joint Communication and Radar for Autonomous Vehicles](https://arxiv.org/abs/2512.14432)
*Zhuoran Li,Zhen Gao,Sheng Chen,Dusit Niyato,Zhaocheng Wan,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 提出一种以感知为中心的联合通信与毫米波雷达范式，利用基于 chirp 的 DD-QAM 调制在时延、多普勒和幅度维度上传输数据，并给出其 achievable rate；通过扩展卡尔曼滤波进行四维参数估计以提升定向和切向速度估计，提出双重补偿的解调与跟踪以兼顾传感与通信；通过仿真验证方法有效性并提供代码。


<details>
  <summary>Details</summary>
Motivation: 在具备自动驾驶车辆的场景中，需要同时提升感知与通信性能，降低系统复杂度与能耗，同时实现对车辆定向、速度等关键参数的高精度估计。现有雷达和通信系统多为独立或耦合不足，难以达到协同最优的 ISAC 系统。

Method: 1) 提出基于 chirp 的延迟-多普勒-幅度三维调制 DD-QAM，将数据映射到时延、Doppler 和幅度维度；2) 推导该调制在通信方面的 achievable rate；3) 设计扩展卡尔曼滤波（EKF）实现四维参数估计，提升对定向与切向速度的估计精度；4) 提出双重补偿解调与跟踪方案，使被动车辆在不牺牲感知功能的前提下实现数据解调；5) 通过仿真验证方法的可行性与性能提升。

Result: 提出的新调制和估计框架实现了在感知主导的 ISAC 场景下对延迟、Doppler、角度与速度等维度的协同感知和通信数据传输，并在仿真中表现出优于传统系统的性能，给出可复现实验代码与数据。

Conclusion: 所提出的 DD-QAM 调制、EKF 4D 参数估计以及双重补偿解调与跟踪方案，构成了一体化的感知与通信解决方案，显著提升了多维参数估计精度与数据传输能力，为自动驾驶车辆的协同感知与通信提供了新的实验与实现路径。

Abstract: This paper introduces a sensing-centric joint communication and millimeter-wave radar paradigm to facilitate collaboration among intelligent vehicles.
  We first propose a chirp waveform-based delay-Doppler quadrature amplitude modulation (DD-QAM) that modulates data across delay, Doppler, and amplitude dimensions.
  Building upon this modulation scheme, we derive its achievable rate to quantify the communication performance.
  We then introduce an extended Kalman filter-based scheme for four-dimensional (4D) parameter estimation in dynamic environments, enabling the active vehicles to accurately estimate orientation and tangential-velocity beyond traditional 4D radar systems.
  Furthermore, in terms of communication, we propose a dual-compensation-based demodulation and tracking scheme that allows the passive vehicles to effectively demodulate data without compromising their sensing functions.
  Simulation results underscore the feasibility and superior performance of our proposed methods, marking a significant advancement in the field of autonomous vehicles.
  Simulation codes are provided to reproduce the results in this paper: \href{https://github.com/LiZhuoRan0/2026-IEEE-TWC-ChirpDelayDopplerModulationISAC}{https://github.com/LiZhuoRan0}.

</details>


### [98] [Relaying Signal When Monitoring Traffic: Double Use of Aerial Vehicles Towards Intelligent Low-Altitude Networking](https://arxiv.org/abs/2512.14436)
*Jiahui Liang,Wenlihan Lu,Tianyi Liu,Kang Kang,Guixin Pan,Liuqing Yang,Xinhu Zheng,Shijian Gao*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In intelligent low-altitude networks, integrating monitoring tasks into communication unmanned aerial vehicles (UAVs) can consume resources and increase handoff latency for communication links. To address this challenge, we propose a strategy that enables a "double use" of UAVs, unifying the monitoring and relay handoff functions into a single, efficient process. Our scheme, guided by an integrated sensing and communication framework, coordinates these multi-role UAVs through a proactive handoff network that fuses multi-view sensory data from aerial and ground vehicles. A lightweight vehicle inspection module and a two-stage training procedure are developed to ensure monitoring accuracy and collaborative efficiency. Simulation results demonstrate the effectiveness of this integrated approach: it reduces communication outage probability by nearly 10% at a 200 Mbps requirement without compromising monitoring performance and maintains high resilience (86% achievable rate) even in the absence of multiple UAVs, outperforming traditional ground-based handoff schemes. Our code is available at the https://github.com/Jiahui-L/UAP.

</details>


### [99] [Tunable Gaussian Pulse for Delay-Doppler ISAC](https://arxiv.org/abs/2512.14637)
*Bruno Felipe Costa,Anup Mishra,Israel Leyva-Mayorga,Taufik Abrão,Petar Popovski*

Main category: eess.SP

TL;DR: 提出可调 Gaussian 脉冲（TGP）用于 DD-domain ISAC，兼顾高移动下的定位精度与通信容量，通过 γ、α_c、β_c 调控脉冲形状，在 CRLB 与容量之间实现权衡，达到近似 RRC 的定位精度和 >90% Sinc 的容量。


<details>
  <summary>Details</summary>
Motivation: 在高多普勒下的 OFDM ISAC 中，ICI 严重，DD-domain ISAC 更稳健，但需合适的 DD-domain 脉冲设计。现有脉冲多为通信导向或静态，难以适应非平稳通道与多样化应用需求。

Method: 提出 tunable Gaussian pulse（TGP），其形状由三个参数 γ、α_c、β_c 控制；对 sensing 侧推导闭式 CRLB，将 γ、α_c、β_c 与延迟/多普勒精度建立映射；对 communications 侧分析 α_c、β_c 如何重塑非对角协方差，从而改变干涉结构（ISI），但不改变接收功率，容量影响来自干涉结构而非功率损失；进行全面权衡分析，展示 TGP 能覆盖从 Sinc（高容量）到 RRC（高精度）的工作区。

Result: 给出闭式 CRLB 与 α_c、β_c 对干扰结构的影响机制；表明 TGP 在 Sinc 与 RRC 的边界之间具有更灵活的工作区域；实现近似 RRC 的 sensing 精度，同时保留超过 90% 的 Sinc 容量，提供不是传统静态脉冲能实现的平衡。

Conclusion: TGP 为 DD-domain ISAC 提供一个可调、通用的脉冲设计框架，能够在高移动性场景下实现传感精度与通信容量的权衡，优于传统静态脉冲设计。

Abstract: Integrated sensing and communication (ISAC) for next-generation networks targets robust operation under high mobility and high Doppler spread, leading to severe inter-carrier interference (ICI) in systems based on orthogonal frequency-division multiplexing (OFDM) waveforms. Delay--Doppler (DD)-domain ISAC offers a more robust foundation under high mobility, but it requires a suitable DD-domain pulse-shaping filter. The prevailing DD pulse designs are either communication-centric or static, which limits adaptation to non-stationary channels and diverse application demands. To address this limitation, this paper introduces the tunable Gaussian pulse (TGP), a DD-native, analytically tunable pulse shape parameterized by its aspect ratio \( γ\), chirp rate \( α_c \), and phase coupling \( β_c \). On the sensing side, we derive closed-form Cramér--Rao lower bounds (CRLBs) that map \( (γ,α_c,β_c) \) to fundamental delay and Doppler precision. On the communications side, we show that \( α_c \) and \( β_c \) reshape off-diagonal covariance, and thus inter-symbol interference (ISI), without changing received power, isolating capacity effects to interference structure rather than power loss. A comprehensive trade-off analysis demonstrates that the TGP spans a flexible operational region from the high capacity of the Sinc pulse to the high precision of the root raised cosine (RRC) pulse. Notably, TGP attains near-RRC sensing precision while retaining over \( 90\% \) of Sinc's maximum capacity, achieving a balanced operating region that is not attainable by conventional static pulse designs.

</details>
