{"id": "2510.26036", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.26036", "abs": "https://arxiv.org/abs/2510.26036", "authors": ["Nan Gu", "Junjie Qin"], "title": "Competitive Equilibrium for Electricity Markets with Spatially Flexible Load", "comment": null, "summary": "Electric vehicle charging and geo-distributed datacenters introduce spatially\nflexible loads (FLs) that couple power, transportation, and datacenter\nnetworks. These couplings create a closed-loop feedback between locational\nmarginal prices (LMPs) and decisions of the FL systems, challenging the\nfoundations of conventional competitive equilibrium (CE) in electricity\nmarkets. This paper studies a notion of generalized competitive equilibrium\n(GCE) that aims to capture such price-demand interactions across the\ninterconnected infrastructures. We establish structural conditions under which\nthe GCE preserves key properties of the conventional CE, including existence,\nuniqueness, and efficiency, without requiring detailed knowledge of decision\nprocesses for individual FL systems. The framework generalizes to settings\nwhere the grid is coupled with multiple FL systems. Stylized examples and case\nstudies on the New York ISO grid, coupled with the Sioux Falls transportation\nand distributed datacenter networks, demonstrate the use of our theoretical\nframework and illustrate the mutual influence among the grid and the studied FL\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7ade\u4e89\u5747\u8861\uff08GCE\uff09\u6846\u67b6\uff0c\u7528\u4ee5\u63cf\u8ff0\u7535\u7f51\u4e0e\u7a7a\u95f4\u53ef\u7075\u6d3b\u8d1f\u8377\uff08\u5982\u7535\u52a8\u8f66\u5145\u7535\u4e0e\u5730\u7406\u5206\u5e03\u6570\u636e\u4e2d\u5fc3\uff09\u4e4b\u95f4\u7684\u4ef7\u683c-\u9700\u6c42\u8026\u5408\uff0c\u8bc1\u660e\u5728\u82e5\u5e72\u7ed3\u6784\u6027\u6761\u4ef6\u4e0b\uff0cGCE\u5177\u6709\u5b58\u5728\u6027\u3001\u552f\u4e00\u6027\u548c\u6548\u7387\u6027\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u591a\u8d1f\u8377\u60c5\u5f62\uff0c\u914d\u5408NYISO\u4e0eSioux Falls\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684FLs\u901a\u8fc7LMP\u4e0e\u7cfb\u7edf\u51b3\u7b56\u4e92\u76f8\u5f71\u54cd\uff0c\u5f62\u6210\u95ed\u73af\u53cd\u9988\uff0c\u6311\u6218\u4f20\u7edf\u7535\u529b\u5e02\u573a\u7684\u7ade\u4e89\u5747\u8861\u5047\u8bbe\uff1b\u9700\u8981\u4e00\u4e2a\u80fd\u6355\u6349\u8de8\u57fa\u7840\u8bbe\u65bd\u4ef7\u683c-\u9700\u6c42\u4e92\u52a8\u7684\u7406\u8bba\u6846\u67b6\u4ee5\u8bbe\u8ba1\u4e0e\u5206\u6790\u76f8\u5173\u5e02\u573a\u673a\u5236\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5bf9\u7efc\u5408\u6027\u3001\u8de8\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u7ed9\u51fa\u80fd\u4fdd\u8bc1CE\u6027\u8d28\u7684\u7ed3\u6784\u6027\u6761\u4ef6\uff1b\u6846\u67b6\u65e0\u9700\u8be6\u7ec6\u77e5\u9053\u5355\u4e2aFL\u7cfb\u7edf\u7684\u51b3\u7b56\u8fc7\u7a0b\uff1b\u5c06\u7f51\u683c\u4e0e\u591a\u7c7bFL\u7cfb\u7edf\u8026\u5408\uff1b\u901a\u8fc7\u7b80\u5316\u6848\u4f8b\u548cNYISO- Sioux Falls\u7684\u5b9e\u9645\u60c5\u666f\u6765\u6f14\u793a\u3002", "result": "\u5728\u6ee1\u8db3\u7ed9\u5b9a\u7ed3\u6784\u6761\u4ef6\u4e0b\uff0cGCE\u5b58\u5728\u3001\u552f\u4e00\u4e14\u9ad8\u6548\uff1b\u6846\u67b6\u5177\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u5904\u7406\u591a\u8d1f\u8377\u7cfb\u7edf\u8026\u5408\uff1b\u901a\u8fc7\u6848\u4f8b\u5c55\u793a\u4e86\u7f51\u683c\u4e0eFL\u7cfb\u7edf\u4e4b\u95f4\u7684\u76f8\u4e92\u5f71\u54cd\u3002", "conclusion": "GCE\u6269\u5c55\u4e86\u4f20\u7edfCE\u4ee5\u9002\u7528\u4e8e\u8026\u5408\u57fa\u7840\u8bbe\u65bd\u7684\u5e02\u573a\u5206\u6790\uff0c\u4e3a\u8bbe\u8ba1\u4e0e\u8fd0\u884c\u5177\u6709FL\u7684\u73b0\u4ee3\u7535\u529b\u5e02\u573a\u63d0\u4f9b\u7406\u8bba\u652f\u6491\u4e0e\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2510.25917", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25917", "abs": "https://arxiv.org/abs/2510.25917", "authors": ["Mehdi Karbalayghareh", "David J. Love", "Christopher G. Brinton"], "title": "Coherence-Aware Distributed Learning under Heterogeneous Downlink Impairments", "comment": "10 pages, 5 figures", "summary": "The performance of federated learning (FL) over wireless networks critically\ndepends on accurate and timely channel state information (CSI) across\ndistributed devices. This requirement is tightly linked to how rapidly the\nchannel gains vary, i.e., the coherence intervals. In practice, edge devices\noften exhibit unequal coherence times due to differences in mobility and\nscattering environments, leading to unequal demands for pilot signaling and\nchannel estimation resources. Conventional FL schemes that overlook this\ncoherence disparity can suffer from severe communication inefficiencies and\ntraining overhead. This paper proposes a coherence-aware,\ncommunication-efficient framework for joint channel training and model updating\nin practical wireless FL systems operating under heterogeneous fading dynamics.\nFocusing on downlink impairments, we introduce a resource-reuse strategy based\non product superposition, enabling the parameter server to efficiently schedule\nboth static and dynamic devices by embedding global model updates for static\ndevices within pilot transmissions intended for mobile devices. We\ntheoretically analyze the convergence behavior of the proposed scheme and\nquantify its gains in expected communication efficiency and training accuracy.\nExperiments demonstrate the effectiveness of the proposed framework under\nmobility-induced dynamics and offer useful insights for the practical\ndeployment of FL over wireless channels.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u5f02\u8d28\u76f8\u5e72\u65f6\u95f4\u7684\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4ea7\u54c1\u53e0\u52a0\u5728\u4e0b\u884c\u5b9e\u73b0\u9759\u6001\u4e0e\u52a8\u6001\u8bbe\u5907\u7684\u8d44\u6e90\u91cd\u7528\uff0c\u5e76\u5c06\u5168\u5c40\u66f4\u65b0\u5d4c\u5165\u79fb\u52a8\u8bbe\u5907\u7684 Pilot \u4e2d\uff0c\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u548c\u6536\u655b\u6027\u3002", "motivation": "\u65e0\u7ebfFL\u4e2dCSI\u65f6\u6548\u6027\u53d7\u8bbe\u5907\u79fb\u52a8\u6027\u548c\u73af\u5883\u5dee\u5f02\u5f71\u54cd\uff0c\u5bfc\u81f4\u76f8\u5e72\u65f6\u95f4\u4e0d\u4e00\u81f4\uff0c\u4f20\u7edf\u65b9\u6848\u56e0\u8d44\u6e90\u6d6a\u8d39\u800c\u4f4e\u6548\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u76f8\u5e72\u611f\u77e5\u7684\u8d44\u6e90\u91cd\u7528\u6846\u67b6\uff1a\u5728\u4e0b\u884c\u4fe1\u9053\u4e2d\u901a\u8fc7\u4ea7\u54c1\u53e0\u52a0\u5b9e\u73b0\u5bf9\u9759\u6001\u8bbe\u5907\u4e0e\u52a8\u6001\u8bbe\u5907\u7684\u540c\u65f6\u8c03\u5ea6\uff0c\u5c06\u9759\u6001\u8bbe\u5907\u7684\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u5d4c\u5165\u5230\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u7684 Pilot \u4f20\u8f93\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u8054\u5408\u4fe1\u9053\u8bad\u7ec3\u4e0e\u6a21\u578b\u66f4\u65b0\uff0c\u5e76\u7ed9\u51fa\u6536\u655b\u6027\u5206\u6790\u3002", "result": "\u7ed9\u51fa\u6536\u655b\u6027\u5206\u6790\uff0c\u5e76\u91cf\u5316\u5728\u671f\u671b\u901a\u4fe1\u6548\u7387\u548c\u8bad\u7ec3\u7cbe\u5ea6\u4e0a\u7684\u63d0\u5347\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u5728\u79fb\u52a8\u6027\u8bf1\u53d1\u7684\u52a8\u6001\u4e0b\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u6709\u7528\u89c1\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e0\u7ebfFL\u5728\u5f02\u8d28\u8870\u843d\u52a8\u529b\u5b66\u4e0b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u5f3a\u8c03\u76f8\u5e72\u611f\u77e5\u4e0e\u8d44\u6e90\u590d\u7528\u5bf9\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u672a\u6765\u5bf9\u591a\u5929\u7ebf\u3001\u9c81\u68d2\u6027\u548c\u4e0a\u884c\u94fe\u8def\u7b49\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.25781", "categories": ["cs.LG", "cs.AI", "cs.NA", "cs.NE", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.25781", "abs": "https://arxiv.org/abs/2510.25781", "authors": ["Amir Noorizadegan", "Sifan Wang", "Leevan Ling"], "title": "A Practitioner's Guide to Kolmogorov-Arnold Networks", "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising\nalternative to traditional Multilayer Perceptrons (MLPs), inspired by the\nKolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed\nactivation functions on nodes, KANs employ learnable univariate basis functions\non edges, offering enhanced expressivity and interpretability. This review\nprovides a systematic and comprehensive overview of the rapidly expanding KAN\nlandscape, moving beyond simple performance comparisons to offer a structured\nsynthesis of theoretical foundations, architectural variants, and practical\nimplementation strategies. By collecting and categorizing a vast array of\nopen-source implementations, we map the vibrant ecosystem supporting KAN\ndevelopment. We begin by bridging the conceptual gap between KANs and MLPs,\nestablishing their formal equivalence and highlighting the superior parameter\nefficiency of the KAN formulation. A central theme of our review is the\ncritical role of the basis function; we survey a wide array of choices,\nincluding B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,\nGaussian RBFs, and Fourier series, and analyze their respective trade-offs in\nterms of smoothness, locality, and computational cost. We then categorize\nrecent advancements into a clear roadmap, covering techniques for improving\naccuracy, efficiency, and regularization. Key topics include physics-informed\nloss design, adaptive sampling, domain decomposition, hybrid architectures, and\nspecialized methods for handling discontinuities. Finally, we provide a\npractical \"Choose-Your-KAN\" guide to help practitioners select appropriate\narchitectures, and we conclude by identifying current research gaps. The\nassociated GitHub repository https://github.com/AmirNoori68/kan-review\ncomplements this paper and serves as a structured reference for ongoing KAN\nresearch.", "AI": {"tldr": "KAN \u5c06\u53ef\u5b66\u4e60\u7684\u4e00\u7ef4\u57fa\u51fd\u6570\u653e\u5728\u8fb9/\u6743\u91cd\u4e0a\uff0c\u4f5c\u4e3a\u5bf9MLP\u7684\u66ff\u4ee3\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u8868\u8fbe\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff1b\u8fd9\u7bc7\u7efc\u8ff0\u7cfb\u7edf\u68b3\u7406\u4e86 KAN \u7684\u7406\u8bba\u57fa\u7840\u3001\u67b6\u6784\u53d8\u4f53\u3001\u5b9e\u73b0\u65b9\u6cd5\u53ca\u751f\u6001\uff0c\u5efa\u7acb\u4e86 KAN \u4e0e MLP \u7684\u7b49\u4ef7\u6027\u53ca\u53c2\u6570\u6548\u7387\u4f18\u52bf\uff0c\u5e76\u7ed9\u51fa\u201cChoose-Your-KAN\u201d\u6307\u5357\u548c\u5b9e\u8df5\u8981\u70b9\uff0c\u9644\u5e26\u5f00\u6e90\u5b9e\u73b0\u96c6\u5408\u7684\u6574\u7406\u3002", "motivation": "\u65e8\u5728\u5bf9\u8fc5\u901f\u6269\u5c55\u7684 KAN \u9886\u57df\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\uff0c\u8d85\u8d8a\u5355\u7eaf\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u5efa\u7acb\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5b8c\u6574\u6846\u67b6\uff0c\u8fde\u63a5 KAN \u4e0e MLP\uff0c\u8bc4\u4f30\u57fa\u51fd\u6570\u7684\u6743\u8861\u4e0e\u5b9e\u73b0\u6210\u672c\uff0c\u4fc3\u6210\u793e\u533a\u534f\u4f5c\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u68b3\u7406\u3001\u5bf9\u73b0\u6709\u5b9e\u73b0\u7684\u6536\u96c6\u4e0e\u5206\u7c7b\u3001\u7406\u8bba\u5bf9\u6bd4\u4e0e\u751f\u6001\u6574\u7406\uff0c\u5f52\u7eb3\u51fa\u67b6\u6784\u53d8\u4f53\u3001\u6b63\u5219\u5316\u3001\u8bad\u7ec3\u7b56\u7565\u7b49\u4e3b\u9898\uff1b\u5bf9\u4e0d\u540c\u57fa\u51fd\u6570\u7684\u4f18\u52a3\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff1b\u6574\u7406\u5f00\u6e90\u5b9e\u73b0\u5e76\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u5f0f\u5206\u7c7b\u3002", "result": "\u7ed9\u51fa KAN \u7684\u7406\u8bba\u7b49\u4ef7\u6027\u3001\u53c2\u6570\u6548\u7387\u3001\u57fa\u51fd\u6570\u7684\u6743\u8861\u3001\u4ee5\u53ca\u6539\u8fdb\u8def\u7ebf\uff08\u7269\u7406\u4fe1\u606f\u635f\u5931\u3001\u81ea\u9002\u5e94\u91c7\u6837\u3001\u57df\u5212\u5206\u3001\u6df7\u5408\u67b6\u6784\u7b49\uff09\uff1b\u63d0\u4f9b\u201cChoose-Your-KAN\u201d\u6307\u5357\u3001\u5b8c\u6574\u7684\u5b9e\u73b0\u6e05\u5355\u4e0e\u4e00\u4e2a\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\u7ed3\u6784\uff0c\u4fbf\u4e8e\u5b9e\u8df5\u8005\u9009\u62e9\u548c\u5b9e\u73b0\u3002", "conclusion": "\u603b\u7ed3\u5f53\u524d\u7814\u7a76\u7a7a\u7f3a\uff0c\u63d0\u51fa\u672a\u6765\u65b9\u5411\u548c\u5b9e\u8df5\u5efa\u8bae\uff0c\u5f3a\u8c03\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u3001\u5bf9\u975e\u8fde\u7eed\u6027\u5904\u7406\u7b49\u65b9\u9762\u7684\u6311\u6218\u4e0e\u673a\u4f1a\u3002"}}
{"id": "2510.26147", "categories": ["cs.IT", "eess.SP", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.26147", "abs": "https://arxiv.org/abs/2510.26147", "authors": ["Xilai Fan", "Ya-Feng Liu"], "title": "Duality-Based Fixed Point Iteration Algorithm for Beamforming Design in ISAC Systems", "comment": "6 pages, 1 figure, submitted to IEEE WCNC 2026", "summary": "In this paper, we investigate the beamforming design problem in an integrated\nsensing and communication (ISAC) system, where a multi-antenna base station\nsimultaneously serves multiple communication users while performing radar\nsensing. We formulate the problem as the minimization of the total transmit\npower, subject to signal-to-interference-plus-noise ratio (SINR) constraints\nfor communication users and mean-squared-error (MSE) constraints for radar\nsensing. The core challenge arises from the complex coupling between\ncommunication SINR requirements and sensing performance metrics. To efficiently\naddress this challenge, we first establish the equivalence between the original\nISAC beamforming problem and its semidefinite relaxation (SDR), derive its\nLagrangian dual formulation, and further reformulate it as a generalized\ndownlink beamforming (GDB) problem with potentially indefinite weighting\nmatrices. Compared to the classical DB problem, the presence of indefinite\nweighting matrices in the GDB problem introduces substantial analytical and\ncomputational challenges. Our key technical contributions include (i) a\nnecessary and sufficient condition for the boundedness of the GDB problem, and\n(ii) a tailored efficient fixed point iteration (FPI) algorithm with a provable\nconvergence guarantee for solving the GDB problem. Building upon these results,\nwe develop a duality-based fixed point iteration (Dual-FPI) algorithm, which\nintegrates an outer subgradient ascent loop with an inner FPI loop. Simulation\nresults demonstrate that the proposed Dual-FPI algorithm achieves globally\noptimal solutions while significantly reducing computational complexity\ncompared with existing baseline approaches.", "AI": {"tldr": "ISAC\u6ce2\u675f\u6210\u5f62\u901a\u8fc7\u534a\u5b9a\u677e\u5f1b(SDR)\u4e0e\u5bf9\u5076\u56fa\u5b9a\u70b9\u6cd5\u89e3\u51b3\uff0c\u5c06\u542b\u4e0d\u5b9a\u6743\u7684\u5e7f\u4e49\u4e0b\u6e38\u6ce2\u675f\u6210\u5f62(GDB)\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u89e3\u5f62\u5f0f\uff1b\u63d0\u51faDual-FPI\u7b97\u6cd5\uff0c\u542b\u5916\u5c42\u5b50\u68af\u5ea6\u4e0a\u5347\u4e0e\u5185\u5c42FPI\uff0c\u5b9e\u9a8c\u663e\u793a\u8fbe\u5230\u5168\u5c40\u6700\u4f18\u5e76\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "motivation": "\u5728ISAC\u573a\u666f\u4e2d\uff0c\u9700\u540c\u65f6\u6ee1\u8db3\u901a\u4fe1SINR\u548c\u96f7\u8fbeMSE\u7684\u8026\u5408\u7ea6\u675f\uff0c\u5bfb\u6c42\u9ad8\u6548\u4e14\u7406\u8bba\u4e0a\u6709\u4fdd\u8bc1\u7684\u6ce2\u675f\u6210\u5f62\u89e3\uff0c\u4ee5\u964d\u4f4e\u603b\u53d1\u5c04\u529f\u7387\u5e76\u5b9e\u73b0\u9ad8\u6548\u8054\u5408\u611f\u77e5\u4e0e\u901a\u4fe1.", "method": "\u5efa\u7acb\u539f\u59cbISAC\u6ce2\u675f\u6210\u5f62\u95ee\u9898\u4e0eSDR\u7684\u7b49\u4ef7\u6027\uff0c\u63a8\u5bfc\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u5e76\u8fdb\u4e00\u6b65\u5c06\u5176\u91cd\u6784\u4e3a\u5e26\u53ef\u80fd\u4e0d\u5b9a\u6743\u77e9\u9635\u7684\u5e7f\u4e49\u4e0b\u884c\u6ce2\u675f\u6210\u5f62(GDB)\u95ee\u9898\u3002\u7ed9\u51faGDB\u95ee\u9898\u7684\u5fc5\u8981\u4e14\u5145\u5206\u7684\u6709\u754c\u6027\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u5e26\u6536\u655b\u6027\u4fdd\u8bc1\u7684\u5b9a\u70b9\u8fed\u4ee3(FPI)\u7b97\u6cd5\u89e3\u51b3GDB\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51faDual-FPI\u7b97\u6cd5\uff1a\u5305\u542b\u5916\u5c42\u7684\u5b50\u68af\u5ea6\u4e0a\u5347\u5faa\u73af\u548c\u5185\u5c42\u7684FPI\u5faa\u73af\uff0c\u4ee5\u5b9e\u73b0\u8fd1\u4f3c\u6700\u4f18\u89e3\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cDual-FPI\u80fd\u591f\u83b7\u5f97\u5168\u5c40\u6700\u4f18\u89e3\uff08\u5728\u7406\u8bba\u53ca\u6761\u4ef6\u5141\u8bb8\u4e0b\uff09\uff0c\u4e14\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u672c\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u7528\u4e8eISAC\u7684\u9ad8\u6548\u5bf9\u5076\u6027\u56fa\u5b9a\u70b9\u6846\u67b6\uff0c\u5c24\u5176\u5728\u5e26\u4e0d\u5b9a\u6743\u77e9\u9635\u7684\u5e7f\u4e49\u4e0b\u884c\u6ce2\u675f\u6210\u5f62\u573a\u666f\u5177\u5907\u7406\u8bba\u4e0e\u8ba1\u7b97\u4f18\u52bf\uff0c\u5bf9\u672a\u6765\u5728\u66f4\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2510.25806", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25806", "abs": "https://arxiv.org/abs/2510.25806", "authors": ["Mustafa F. Abdelwahed", "Ahmed Shafee", "Joan Espasa"], "title": "APThreatHunter: An automated planning-based threat hunting framework", "comment": null, "summary": "Cyber attacks threaten economic interests, critical infrastructure, and\npublic health and safety. To counter this, entities adopt cyber threat hunting,\na proactive approach that involves formulating hypotheses and searching for\nattack patterns within organisational networks. Automating cyber threat hunting\npresents challenges, particularly in generating hypotheses, as it is a manually\ncreated and confirmed process, making it time-consuming. To address these\nchallenges, we introduce APThreatHunter, an automated threat hunting solution\nthat generates hypotheses with minimal human intervention, eliminating analyst\nbias and reducing time and cost. This is done by presenting possible risks\nbased on the system's current state and a set of indicators to indicate whether\nany of the detected risks are happening or not. We evaluated APThreatHunter\nusing real-world Android malware samples, and the results revealed the\npracticality of using automated planning for goal hypothesis generation in\ncyber threat hunting activities.", "AI": {"tldr": "APThreatHunter \u81ea\u52a8\u751f\u6210\u5a01\u80c1\u5047\u8bbe\uff0c\u6700\u5c0f\u5316\u4eba\u7c7b\u5e72\u9884\uff0c\u964d\u4f4e\u504f\u89c1\u3001\u65f6\u95f4\u4e0e\u6210\u672c\uff1b\u5728\u5b9e\u9645 Android \u6076\u610f\u8f6f\u4ef6\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u663e\u793a\u81ea\u52a8\u89c4\u5212\u5728\u5047\u8bbe\u751f\u6210\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u56e0\u4e3a\u624b\u5de5\u751f\u6210\u548c\u786e\u8ba4\u7684\u5047\u8bbe\u5728\u5a01\u80c1\u72e9\u730e\u4e2d\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u4ee5\u63d0\u5347\u6548\u7387\u548c\u8986\u76d6\u9762\u3002", "method": "\u63d0\u51fa APThreatHunter\uff0c\u57fa\u4e8e\u7cfb\u7edf\u5f53\u524d\u72b6\u6001\u548c\u6307\u793a\u5668\uff0c\u4f7f\u7528\u81ea\u52a8\u5316\u89c4\u5212\u751f\u6210\u653b\u51fb\u5047\u8bbe\uff0c\u5e76\u5224\u65ad\u98ce\u9669\u662f\u5426\u6b63\u5728\u53d1\u751f\uff1b\u4ee5\u5b9e\u9645 Android \u6076\u610f\u6837\u672c\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\u5728\u76ee\u6807\u5047\u8bbe\u751f\u6210\u4e0a\u7684\u81ea\u52a8\u89c4\u5212\u5177\u6709\u53ef\u884c\u6027\uff0c\u80fd\u591f\u5728\u6700\u5c0f\u4eba\u5de5\u5e72\u9884\u4e0b\u4ea7\u751f\u6709\u6548\u7684\u5a01\u80c1\u5047\u8bbe\u3002", "conclusion": "APThreatHunter \u53ef\u964d\u4f4e\u5206\u6790\u5e08\u504f\u89c1\u3001\u51cf\u5c11\u65f6\u95f4\u548c\u6210\u672c\uff0c\u8bc1\u660e\u81ea\u52a8\u5316\u89c4\u5212\u5728\u5a01\u80c1\u72e9\u730e\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u5177\u5907\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2510.25785", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.25785", "abs": "https://arxiv.org/abs/2510.25785", "authors": ["Simon A. Lee", "Cyrus Tanade", "Hao Zhou", "Juhyeon Lee", "Megha Thukral", "Minji Han", "Rachel Choi", "Md Sazzad Hissain Khan", "Baiying Lu", "Migyeong Gwak", "Mehrab Bin Morshed", "Viswam Nathan", "Md Mahbubur Rahman", "Li Zhu", "Subramaniam Venkatraman", "Sharanya Arcot Desai"], "title": "HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series", "comment": null, "summary": "Wearable sensors provide abundant physiological time series, yet the\nprinciples governing their predictive utility remain unclear. We hypothesize\nthat temporal resolution is a fundamental axis of representation learning, with\ndifferent clinical and behavioral outcomes relying on structure at distinct\nscales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical\nMasked Autoencoder), a self supervised framework that combines masked\nautoencoding with a hierarchical convolutional encoder decoder. HiMAE produces\nmulti resolution embeddings that enable systematic evaluation of which temporal\nscales carry predictive signal, transforming resolution from a hyperparameter\ninto a probe for interpretability. Across classification, regression, and\ngenerative benchmarks, HiMAE consistently outperforms state of the art\nfoundation models that collapse scale, while being orders of magnitude smaller.\nHiMAE is an efficient representation learner compact enough to run entirely on\nwatch, achieving sub millisecond inference on smartwatch class CPUs for true\nedge inference. Together, these contributions position HiMAE as both an\nefficient self supervised learning method and a discovery tool for scale\nsensitive structure in wearable health.", "AI": {"tldr": "\u63d0\u51faHiMAE\uff0c\u4e00\u79cd\u5c42\u6b21\u5316\u63a9\u7801\u81ea\u7f16\u7801\u6846\u67b6\uff0c\u5728\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u65f6\u95f4\u5e8f\u5217\u4e0a\u4ea7\u751f\u591a\u5206\u8fa8\u7387\u5d4c\u5165\uff0c\u5c06\u5c3a\u5ea6\u4f5c\u4e3a\u53ef\u89e3\u91ca\u63a2\u9488\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u9884\u6d4b\u4fe1\u53f7\uff1b\u5728\u5206\u7c7b\u3001\u56de\u5f52\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5355\u4e00\u5c3a\u5ea6\u7684SOTA\u6a21\u578b\uff0c\u4e14\u53c2\u6570\u91cf\u663e\u8457\u66f4\u5c0f\uff0c\u53ef\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e9a\u6beb\u79d2\u63a8\u65ad\u3002", "motivation": "\u65f6\u95f4\u5206\u8fa8\u7387\u88ab\u89c6\u4e3a\u8868\u793a\u5b66\u4e60\u7684\u4e00\u4e2a\u57fa\u672c\u8f74\uff0c\u4e0d\u540c\u4e34\u5e8a\u4e0e\u884c\u4e3a\u7ed3\u679c\u4f9d\u8d56\u4e8e\u4e0d\u540c\u7684\u65f6\u95f4\u5c3a\u5ea6\u7ed3\u6784\uff0c\u9700\u901a\u8fc7\u5b66\u4e60\u6846\u67b6\u6765\u63ed\u793a\u5c3a\u5ea6\u4e0e\u9884\u6d4b\u4fe1\u53f7\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u5f15\u5165HiMAE\uff0c\u7ed3\u5408\u63a9\u7801\u81ea\u7f16\u7801\u4e0e\u5c42\u6b21\u5316\u5377\u79ef\u7f16\u7801-\u89e3\u7801\u5668\uff0c\u8f93\u51fa\u591a\u5c3a\u5ea6\u5d4c\u5165\uff1b\u901a\u8fc7\u5728\u8bad\u7ec3\u65f6\u5bf9\u8f93\u5165\u7684\u4e0d\u540c\u5c3a\u5ea6\u8fdb\u884c\u63a9\u7801\u4e0e\u91cd\u5efa\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u5b66\u4e60\u5e76\u5c06\u5206\u8fa8\u7387\u4f5c\u4e3a\u89e3\u91ca\u6027\u63a2\u9488\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002", "result": "\u5728\u5206\u7c7b\u3001\u56de\u5f52\u548c\u751f\u6210\u57fa\u51c6\u4e0a\uff0cHiMAE\u6301\u7eed\u8d85\u8d8a\u5d29\u6e83\u5c3a\u5ea6\u7684SOTA\u57fa\u6a21\u578b\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u8fdc\u5c0f\u4e8e\u5bf9\u624b\uff1b\u5728\u53ef\u7a7f\u6234\u573a\u666f\u4e2d\u5b9e\u73b0\u8fb9\u7f18\u63a8\u65ad\uff0c\u80fd\u591f\u5728\u666e\u901a\u624b\u8868\u7ea7CPU\u4e0a\u63d0\u4f9b\u4e9a\u6beb\u79d2\u63a8\u65ad\u6027\u80fd\u3002", "conclusion": "HiMAE\u517c\u5177\u9ad8\u6548\u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u5c3a\u5ea6\u654f\u611f\u7ed3\u6784\u7684\u53d1\u73b0\u5de5\u5177\u5c5e\u6027\uff0c\u4e3a\u53ef\u7a7f\u6234\u5065\u5eb7\u9886\u57df\u7684\u8fb9\u7f18\u63a8\u65ad\u4e0e\u591a\u5c3a\u5ea6\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.26368", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.26368", "abs": "https://arxiv.org/abs/2510.26368", "authors": ["Mustafa Mohammed Mustafa"], "title": "Command-filter-based trajectory-tracking control of quadrotor subject to internal and external disturbances", "comment": null, "summary": "We propose a command-filter backstepping controller that integrates a\ndisturbance observer and a high-gain observer (HGO) to handle unknown internal\nand external disturbances acting on a quadrotor. To build the controller, we\nfirst define tracking errors between the measured and desired quadrotor\noutputs, which allow the system to be rewritten in a new set of state\nvariables. Using this transformed model, we apply Lyapunov theory to derive a\nbackstepping control law. To avoid repeated differentiation of states and\nvirtual controls, a first-order command filter is introduced, and a nonlinear\ndisturbance observer is added to provide disturbance estimates. Each state in\nthe controller and observer is replaced with its estimate from the HGO. The\nresulting control law enables the quadrotor to follow its path despite internal\nand external disturbances, with each subsystem allowed its own disturbance type\nfor realism. A new state transformation and Lyapunov-based derivation prevent\nthe usual explosion of complexity, while the HGO reconstructs unmeasured states\nand their rates for output feedback. The nonlinear disturbance observer\nattenuates constant and nonlinear disturbances as well as band-limited white\nnoise. The method reduces dependence on high-precision sensors and mitigates\nwind, model error, and rotor noise effects during flight. Unlike previous\nstudies that treat either disturbance rejection or partial sensing, this work\ncombines the command filter, disturbance observer, and HGO to address both\nchallenges simultaneously while avoiding the complexity growth typical of\nbackstepping designs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u547d\u4ee4\u6ee4\u6ce2\u56de\u6eaf\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u6270\u52a8\u89c2\u6d4b\u5668\u548c\u9ad8\u589e\u76ca\u89c2\u6d4b\u5668\uff0c\u5bf9\u56db\u65cb\u7ffc\u5728\u5185\u90e8\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u5b9e\u73b0\u9c81\u68d2\u8ddf\u8e2a\uff0c\u901a\u8fc7\u65b0\u72b6\u6001\u53d8\u6362\u4e0eLyapunov\u63a8\u5bfc\u964d\u4f4eBackstepping\u7684\u590d\u6742\u6027\u3002", "motivation": "\u89e3\u51b3\u672a\u77e5\u7684\u5185\u90e8/\u5916\u90e8\u5e72\u6270\u5bf9\u56db\u65cb\u7ffc\u98de\u884c\u63a7\u5236\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u907f\u514d\u56de\u6b65\u8bbe\u8ba1\u7684\u6307\u6570\u7ea7\u590d\u6742\u6027\uff0c\u5e76\u5728\u8f83\u4f4e\u4f20\u611f\u5668\u7cbe\u5ea6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u98de\u884c\u3002", "method": "\u5c06\u6d4b\u91cf\u8f93\u51fa\u4e0e\u671f\u671b\u8f93\u51fa\u4e4b\u95f4\u7684\u8ddf\u8e2a\u8bef\u5dee\u5b9a\u4e49\u5e76\u8fdb\u884c\u72b6\u6001\u53d8\u91cf\u91cd\u5199\uff0c\u57fa\u4e8e\u53d8\u6362\u6a21\u578b\u5e94\u7528Lyapunov\u7406\u8bba\u63a8\u5bfc\u56de\u6b65\u63a7\u5236\u5f8b\uff1b\u5f15\u5165\u4e00\u9636\u547d\u4ee4\u6ee4\u6ce2\u5668\u4ee5\u907f\u514d\u5bf9\u72b6\u6001\u548c\u865a\u62df\u63a7\u5236\u7684\u91cd\u590d\u5fae\u5206\uff1b\u52a0\u5165\u975e\u7ebf\u6027\u6270\u52a8\u89c2\u6d4b\u5668\u63d0\u4f9b\u6270\u52a8\u4f30\u8ba1\uff1b\u5c06\u63a7\u5236\u5668\u4e0e\u89c2\u6d4b\u5668\u4e2d\u7684\u6bcf\u4e2a\u72b6\u6001\u7528\u9ad8\u589e\u76ca\u89c2\u6d4b\u5668\u7684\u4f30\u8ba1\u503c\u66ff\u4ee3\uff1b\u5e76\u4f7f\u7528\u65b0\u72b6\u6001\u53d8\u6362\u548cLyapunov\u63a8\u5bfc\u63a7\u5236\u590d\u6742\u5ea6\u4e0d\u7206\u70b8\uff0c\u540c\u65f6HGO\u91cd\u5efa\u672a\u6d4b\u91cf\u7684\u72b6\u6001\u53ca\u5176\u53d8\u5316\u7387\u3002", "result": "\u7406\u8bba\u4e0a\u5b9e\u73b0\u5bf9\u56db\u65cb\u7ffc\u7684\u8def\u5f84\u8ddf\u8e2a\u9c81\u68d2\u6027\uff0c\u5728\u5b58\u5728\u5185\u90e8/\u5916\u90e8\u6270\u52a8\u3001\u98ce\u3001\u6a21\u578b\u8bef\u5dee\u548c\u566a\u58f0\u65f6\u4ecd\u80fd\u6267\u884c\u671f\u671b\u8f68\u8ff9\uff1b\u5404\u5b50\u7cfb\u7edf\u53ef\u5bb9\u7eb3\u5404\u81ea\u7684\u6270\u52a8\u7c7b\u578b\uff1b\u901a\u8fc7HGO\u5b9e\u73b0\u72b6\u6001\u4f30\u8ba1\uff0c\u547d\u4ee4\u6ee4\u6ce2\u4e0e\u6270\u52a8\u89c2\u6d4b\u5668\u5171\u540c\u964d\u4f4e\u5bf9\u9ad8\u7cbe\u5ea6\u4f20\u611f\u7684\u4f9d\u8d56\uff0c\u51cf\u5c11\u4e86Backstepping\u8bbe\u8ba1\u7684\u590d\u6742\u5ea6\u589e\u957f\u3002", "conclusion": "\u521b\u65b0\u70b9\u5728\u4e8e\u628a\u547d\u4ee4\u6ee4\u6ce2\u3001\u6270\u52a8\u89c2\u6d4b\u5668\u548c\u9ad8\u589e\u76ca\u89c2\u6d4b\u5668\u4e09\u8005\u7ed3\u5408\uff0c\u540c\u65f6\u89e3\u51b3\u6270\u52a8\u6291\u5236\u548c\u90e8\u5206\u89c2\u6d4b\u7684\u6311\u6218\uff0c\u4e14\u907f\u514d\u4e86\u4f20\u7edfBackstepping\u7684\u590d\u6742\u5ea6\u81a8\u80c0\u3002"}}
{"id": "2510.26071", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.26071", "abs": "https://arxiv.org/abs/2510.26071", "authors": ["Shenshen Luan", "Yumo Tian", "Xinyu Zhang", "Qingwen Zhang", "Tianheng Wang", "Yan Yang", "Shuguo Xie"], "title": "Symmetry-Driven Asynchronous Forwarding for Reliable Distributed Coordination in Toroidal Networks", "comment": null, "summary": "The proliferation of large-scale distributed systems, such as satellite\nconstellations and high-performance computing clusters, demands robust\ncommunication primitives that maintain coordination under unreliable links. The\ntorus topology, with its inherent rotational and reflection symmetries, is a\nprevalent architecture in these domains. However, conventional routing schemes\nsuffer from substantial packet loss during control-plane synchronization after\nlink failures. This paper introduces a symmetry-driven asynchronous forwarding\nmechanism that leverages the torus's geometric properties to achieve reliable\npacket delivery without control-plane coordination. We model packet flow using\na topological potential gradient and demonstrate that symmetry-breaking\nfailures naturally induce a reverse flow, which we harness for fault\ncircumvention. We propose two local forwarding strategies, Reverse Flow with\nCounter-facing Priority (RF-CF) and Lateral-facing Priority (RF-LF), that\nguarantee reachability to the destination via forward-flow phase transition\npoints, without protocol modifications or additional in-packet overhead.\nThrough percolation analysis and packet-level simulations on a 16 x 16 torus,\nwe show that our mechanism reduces packet loss by up to 17.5% under a 1% link\nfailure rate, with the RF-LF strategy contributing to 28% of successfully\ndelivered packets. This work establishes a foundational link between\ntopological symmetry and communication resilience, providing a lightweight,\nprotocol-agnostic substrate for enhancing distributed systems.", "AI": {"tldr": "\u57fa\u4e8e torus \u5bf9\u79f0\u6027\u7684\u5f02\u6b65\u8f6c\u53d1\u673a\u5236\uff0c\u5728\u65e0\u9700\u63a7\u5236\u5e73\u9762\u534f\u4f5c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5728\u4e0d\u53ef\u9760\u94fe\u8def\u4e0a\u7684\u5305\u4f20\u9012\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\uff08\u5982\u536b\u661f\u661f\u5ea7\u3001HPC \u96c6\u7fa4\uff09\u4e2d\uff0c\u94fe\u8def\u4e0d\u53ef\u9760\u65f6\u9700\u8981\u53ef\u9760\u7684\u901a\u4fe1\u539f\u8bed\u3002\u73b0\u6709\u8def\u7531\u5728\u63a7\u5236\u5e73\u9762\u6545\u969c\u6216\u540c\u6b65\u5931\u6548\u65f6\u6613\u4ea7\u751f\u8f83\u9ad8\u5305\u4e22\u5931\u3002", "method": "\u5efa\u7acb\u4ee5\u62d3\u6251\u52bf\u68af\u5ea6\u4e3a\u6a21\u578b\u7684\u6d41\u91cf\u6846\u67b6\uff0c\u63ed\u793a\u5bf9\u79f0\u6027\u7834\u7f3a\u4f1a\u8bf1\u53d1\u53cd\u5411\u6d41\uff0c\u5e76\u4ee5\u6b64\u7ed5\u8fc7\u6545\u969c\u3002\u63d0\u51fa\u4e24\u79cd\u5c40\u90e8\u8f6c\u53d1\u7b56\u7565\uff1aRF-CF\uff08\u53cd\u5411\u6d41+\u5bf9\u5411\u4f18\u5148\uff09\u4e0e RF-LF\uff08\u53cd\u5411\u6d41+\u4fa7\u5411\u4f18\u5148\uff09\u3002\u5728\u4e0d\u4fee\u6539\u534f\u8bae\u3001\u65e0\u989d\u5916\u5305\u5934\u5f00\u9500\u7684\u524d\u63d0\u4e0b\uff0c\u786e\u4fdd\u5728\u524d\u5411\u6d41\u76f8\u53d8\u70b9\u5b9e\u73b0\u5230\u8fbe\u76ee\u6807\u3002\u901a\u8fc7\u6e17\u900f\u5206\u6790\u4e0e\u5728 16\u00d716 torus \u4e0a\u7684\u5305\u7ea7\u4eff\u771f\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728 1% \u94fe\u8def\u6545\u969c\u7387\u4e0b\uff0c\u5305\u4e22\u5931\u7387\u53ef\u964d\u4f4e\u81f3\u539f\u59cb\u6c34\u5e73\u7684\u7ea682.5%\uff08\u964d\u4f4e\u5e45\u5ea6\u6700\u9ad8\u8fbe17.5%\uff09\u3002RF-LF \u7b56\u7565\u5bf9\u6210\u529f\u4ea4\u4ed8\u7684\u5305\u8d21\u732e\u7ea628%\u3002", "conclusion": "\u5c06\u62d3\u6251\u5bf9\u79f0\u6027\u4e0e\u901a\u4fe1\u9c81\u68d2\u6027\u5efa\u7acb\u8054\u7cfb\uff0c\u63d0\u4f9b\u8f7b\u91cf\u3001\u534f\u8bae\u65e0\u5173\u7684\u57fa\u7840\u8bbe\u65bd\u6765\u63d0\u5347\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.26097", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26097", "abs": "https://arxiv.org/abs/2510.26097", "authors": ["Usman Akram", "Fan Zhang", "Yang Li", "Haris Vikalo"], "title": "Robust Super-Capacity SRS Channel Inpainting via Diffusion Models", "comment": null, "summary": "Accurate channel state information (CSI) is essential for reliable multiuser\nMIMO operation. In 5G NR, reciprocity-based beamforming via uplink Sounding\nReference Signals (SRS) face resource and coverage constraints, motivating\nsparse non-uniform SRS allocation. Prior masked-autoencoder (MAE) approaches\nimprove coverage but overfit to training masks and degrade under unseen\ndistortions (e.g., additional masking, interference, clipping, non-Gaussian\nnoise). We propose a diffusion-based channel inpainting framework that\nintegrates system-model knowledge at inference via a likelihood-gradient term,\nenabling a single trained model to adapt across mismatched conditions. On\nstandardized CDL channels, the score-based diffusion variant consistently\noutperforms a UNet score-model baseline and the one-step MAE under distribution\nshift, with improvements up to 14 dB NMSE in challenging settings (e.g.,\nLaplace noise, user interference), while retaining competitive accuracy under\nmatched conditions. These results demonstrate that diffusion-guided inpainting\nis a robust and generalizable approach for super-capacity SRS design in 5G NR\nsystems.", "AI": {"tldr": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5bf9\u901a\u9053\u8fdb\u884c\u4fee\u8865\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u663e\u8457\u4f18\u4e8eMAE\u4e0eUNet\uff0c\u63d0\u53475G NR\u975e\u5747\u5300SRS\u7684\u9c81\u68d2\u6027\u4e0e\u8986\u76d6\u3002", "motivation": "\u57285G NR\u4e2d\uff0c\u57fa\u4e8e\u4e0a\u884cSRS\u7684\u4e92\u6613\u6027\u6ce2\u675f\u5f62\u6210\u53d7\u8d44\u6e90\u4e0e\u8986\u76d6\u7ea6\u675f\u5f71\u54cd\uff0c\u9700\u8981\u7a00\u758f\u975e\u5747\u5300SRS\u5206\u914d\u3002\u4f20\u7edfMAE\u5728\u8bad\u7ec3\u63a9\u7801\u4e0a\u6613\u8fc7\u62df\u5408\uff0c\u5bf9\u989d\u5916\u906e\u6321\u3001\u5e72\u6270\u3001\u88c1\u526a\u548c\u975e\u9ad8\u65af\u566a\u58f0\u7b49\u672a\u89c1\u5931\u771f\u8868\u73b0\u4e0d\u7a33\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6269\u6563\u6a21\u578b\u7684\u901a\u9053\u4fee\u590d\u6846\u67b6\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u5f15\u5165\u4f3c\u7136\u68af\u5ea6\u9879\u6574\u5408\u7cfb\u7edf\u6a21\u578b\u77e5\u8bc6\uff0c\u4f7f\u5355\u4e00\u8bad\u7ec3\u6a21\u578b\u80fd\u591f\u5728\u5339\u914d\u4e0e\u4e0d\u5339\u914d\u6761\u4ef6\u4e0b\u81ea\u9002\u5e94\u3002\u57fa\u4e8e\u6807\u51c6\u5316\u7684CDL\u901a\u9053\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5206\u5e03\u504f\u79fb\u6761\u4ef6\u4e0b\uff0c\u5206\u6570\u57fa\u6269\u6563\u53d8\u4f53\u6301\u7eed\u4f18\u4e8eUNet\u5206\u6570\u6a21\u578b\u57fa\u7ebf\u548c\u5355\u6b65MAE\uff0c\u4e14\u5728\u6311\u6218\u6027\u8bbe\u7f6e\uff08\u5982Laplace\u566a\u58f0\u3001\u7528\u6237\u5e72\u6270\uff09\u4e0bNMSE\u63d0\u5347\u53ef\u8fbe\u7ea614 dB\uff0c\u540c\u65f6\u5728\u5339\u914d\u6761\u4ef6\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u6269\u6563\u5f15\u5bfc\u7684\u901a\u9053\u4fee\u590d\u662f\u4e00\u79cd\u5bf95G NR\u8d85\u5bb9\u91cfSRS\u8bbe\u8ba1\u5177\u6709\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u7684\u901a\u7528\u65b9\u6cd5\u3002"}}
{"id": "2510.26279", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.26279", "abs": "https://arxiv.org/abs/2510.26279", "authors": ["Fuying Li", "Yajun Wang", "Zhuxian Lian", "Wen Chen"], "title": "Efficient Spectral Efficiency Maximization Design for IRS-aided MIMO Systems", "comment": null, "summary": "Driven by the growing demand for higher spectral efficiency in wireless\ncommunications, intelligent reflecting sur- faces (IRS) have attracted\nconsiderable attention for their ability to dynamically reconfigure the\npropagation environment. This work addresses the spectral efficiency\nmaximization problem in IRS-assisted multiple-input multiple-output (MIMO)\nsystems, which involves the joint optimization of the transmit precoding matrix\nand the IRS phase shift configuration. This problem is inherently challenging\ndue to its non-convex nature. To tackle it effectively, we introduce a\ncomputationally efficient algorithm, termed ADMM-APG, which integrates the\nalternating direction method of multipliers (ADMM) with the accelerated\nprojected gradient (APG) method. The proposed framework decomposes the original\nproblem into tractable subproblems, each admitting a closed-form solution while\nmaintaining low computational com- plexity. Simulation results demonstrate that\nthe ADMM-APG algorithm consistently surpasses existing benchmark methods in\nterms of spectral efficiency and computational complexity, achieving\nsignificant performance gains across a range of system configurations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a ADMM-APG \u7b97\u6cd5\u7528\u4e8e IRS \u8f85\u52a9 MIMO \u7684\u8054\u5408\u4f20\u8f93\u524d\u9988\u77e9\u9635\u4e0e IRS \u76f8\u4f4d\u79fb\u7684\u4f18\u5316\uff0c\u4ee5\u6700\u5927\u5316\u5149\u8c31\u6548\u7387\uff1b\u901a\u8fc7\u5c06\u539f\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u5e76\u7ed9\u51fa\u5c01\u95ed\u5f62\u5f0f\u89e3\uff0c\u8fbe\u5230\u6bd4\u57fa\u51c6\u65b9\u6cd5\u66f4\u9ad8\u7684\u5149\u8c31\u6548\u7387\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4e3a\u63d0\u5347\u65e0\u7ebf\u901a\u4fe1\u7684\u9891\u8c31\u6548\u7387\uff0cIRS \u80fd \u52a8\u6001\u91cd\u6784\u4f20\u64ad\u73af\u5883\uff1b\u4f46\u662f\u8054\u5408\u4f18\u5316\u53d1\u5c04 precoding \u4e0e IRS \u76f8\u4f4d\u79fb\u7684\u975e\u51f8\u95ee\u9898\u96be\u4ee5\u6c42\u89e3\uff0c\u4e9f\u9700\u9ad8\u6548\u7b97\u6cd5\u3002", "method": "\u5c06\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u4e0e\u52a0\u901f\u6295\u5f71\u68af\u5ea6\u6cd5\uff08APG\uff09\u7ed3\u5408\uff0c\u5c06\u539f\u95ee\u9898\u5206\u89e3\u6210\u82e5\u5e72\u53ef\u9010\u6b65\u6c42\u89e3\u7684\u5b50\u95ee\u9898\uff0c\u6bcf\u4e2a\u5b50\u95ee\u9898\u90fd\u5177\u5907\u95ed\u5f0f\u89e3\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u6bd4\uff0cADMM-APG \u5728\u5149\u8c31\u6548\u7387\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u5728\u4e0d\u540c\u7cfb\u7edf\u914d\u7f6e\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 ADMM-APG \u6846\u67b6\u6709\u6548\u89e3\u51b3 IRS \u8f85\u52a9 MIMO \u7684\u5149\u8c31\u6548\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u5177\u5907\u8f83\u9ad8\u7684\u6027\u80fd\u548c\u8f83\u4f4e\u7684\u5b9e\u73b0\u590d\u6742\u5ea6\uff0c\u4e3a\u672a\u6765\u5728\u9ad8\u6548IRS\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.26442", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.26442", "abs": "https://arxiv.org/abs/2510.26442", "authors": ["Xuesong Wang", "Xinyan Xie", "Mo Li", "Zhaoqian Liu"], "title": "Diffusion-Aided Bandwidth-Efficient Semantic Communication with Adaptive Requests", "comment": "Submitted to IEEE ICC 2026", "summary": "Semantic communication focuses on conveying the intrinsic meaning of data\nrather than its raw symbolic representation. For visual content, this paradigm\nshifts from traditional pixel-level transmission toward leveraging the semantic\nstructure of images to communicate visual meaning. Existing approaches\ngenerally follow one of two paths: transmitting only text descriptions, which\noften fail to capture precise spatial layouts and fine-grained appearance\ndetails; or transmitting text alongside dense latent visual features, which\ntends to introduce substantial semantic redundancy. A key challenge, therefore,\nis to reduce semantic redundancy while preserving semantic understanding and\nvisual fidelity, thereby improving overall transmission efficiency. This paper\nintroduces a diffusion-based semantic communication framework with adaptive\nretransmission. The system transmits concise text descriptions together with a\nlimited set of key latent visual features, and employs a diffusion-based\ninpainting model to reconstruct the image. A receiver-side semantic consistency\nmechanism is designed to evaluate the alignment between the reconstructed image\nand the original text description. When a semantic discrepancy is detected, the\nreceiver triggers a retransmission to request a small set of additional latent\nblocks and refine the image reconstruction. This approach significantly reduces\nbandwidth usage while preserving high semantic accuracy, achieving an efficient\nbalance between reconstruction quality and transmission overhead.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u4f20\u7684\u6269\u6563\u5f0f\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u5728\u4ec5\u53d1\u9001\u7b80\u660e\u6587\u672c\u63cf\u8ff0\u548c\u5c11\u91cf\u5173\u952e\u6f5c\u5728\u89c6\u89c9\u7279\u5f81\u7684\u524d\u63d0\u4e0b\u8fdb\u884c\u56fe\u50cf\u91cd\u5efa\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4fee\u590d\uff0c\u5e76\u5728\u63a5\u6536\u7aef\u8fdb\u884c\u8bed\u4e49\u4e00\u81f4\u6027\u68c0\u6d4b\u4ee5\u89e6\u53d1\u589e\u91cf\u4f20\u8f93\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u8bed\u4e49\u4e0e\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5e26\u5bbd\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u4e49\u901a\u4fe1\u5728\u89c6\u89c9\u5185\u5bb9\u4f20\u8f93\u4e2d\u5bf9\u8bed\u4e49\u5197\u4f59\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff1b\u6587\u672c\u63cf\u8ff0\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u7a7a\u95f4\u5e03\u5c40\u548c\u7ec6\u7c92\u5ea6\u5916\u89c2\uff0c\u4e14\u76f4\u63a5\u4f20\u8f93\u5bc6\u96c6\u6f5c\u5728\u7279\u5f81\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u8bed\u4e49\u5197\u4f59\u3002\u9700\u8981\u5728\u4fdd\u6301\u8bed\u4e49\u7406\u89e3\u4e0e\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u5197\u4f59\u3001\u63d0\u5347\u5e26\u5bbd\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6269\u6563\u5f0f\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u53d1\u9001\u7b80\u6d01\u6587\u672c\u63cf\u8ff0\u548c\u6709\u9650\u7684\u5173\u952e\u6f5c\u5728\u89c6\u89c9\u7279\u5f81\uff0c\u5229\u7528\u6269\u6563\u5f0f\u4fee\u590d\uff08inpainting\uff09\u6a21\u578b\u91cd\u5efa\u56fe\u50cf\uff1b\u63a5\u6536\u7aef\u5f15\u5165\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4f30\uff0c\u82e5\u91cd\u5efa\u7ed3\u679c\u4e0e\u539f\u59cb\u6587\u672c\u5b58\u5728\u504f\u5dee\u5219\u89e6\u53d1\u91cd\u4f20\uff0c\u8bf7\u6c42\u5c11\u91cf\u989d\u5916\u6f5c\u5728\u5757\u4ee5\u7ec6\u5316\u91cd\u5efa\u3002", "result": "\u7406\u8bba\u4e0e\u5b9e\u9a8c\u6307\u793a\u5728\u4fdd\u6301\u9ad8\u8bed\u4e49\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u5e26\u5bbd\u5f00\u9500\uff0c\u80fd\u591f\u5728\u91cd\u5efa\u8d28\u91cf\u4e0e\u4f20\u8f93\u6210\u672c\u4e4b\u95f4\u5b9e\u73b0\u6709\u6548\u6298\u8877\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u4f20\u548c\u6269\u6563\u91cd\u5efa\u5b9e\u73b0\u66f4\u9ad8\u7684\u4f20\u8f93\u6548\u7387\u4e0e\u8bed\u4e49\u4fdd\u771f\uff0c\u63d0\u51fa\u4e86\u89c6\u89c9\u8bed\u4e49\u4f20\u8f93\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.25791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25791", "abs": "https://arxiv.org/abs/2510.25791", "authors": ["Zihan Pengmei", "Costas Mavromatis", "Zhengyuan Shen", "Yunyi Zhang", "Vassilis N. Ioannidis", "Huzefa Rangwala"], "title": "The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?", "comment": "10 pages, 7 figures, with appendix", "summary": "Chain-of-thought (CoT) supervision can substantially improve transformer\nperformance, yet the mechanisms by which models learn to follow and benefit\nfrom CoT remain poorly understood. We investigate these learning dynamics\nthrough the lens of grokking by pretraining transformers on symbolic reasoning\ntasks with tunable algorithmic complexity and controllable data composition to\nstudy their generalization. Models were trained under two settings: (i)\nproducing only final answers, and (ii) emitting explicit CoT traces before\nanswering. Our results show that while CoT generally improves task performance,\nits benefits depend on task complexity. To quantify these effects, we model the\naccuracy of the logarithmic training steps with a three-parameter logistic\ncurve, revealing how the learning speed and shape vary with task complexity,\ndata distribution, and the presence of CoT supervision. We also uncover a\ntransient trace unfaithfulness phase: early in training, models often produce\ncorrect answers while skipping or contradicting CoT steps, before later\naligning their reasoning traces with answers. Empirically, we (1) demonstrate\nthat CoT accelerates generalization but does not overcome tasks with higher\nalgorithmic complexity, such as finding list intersections; (2) introduce a\nkinetic modeling framework for understanding transformer learning; (3)\ncharacterize trace faithfulness as a dynamic property that emerges over\ntraining; and (4) show CoT alters internal transformer computation\nmechanistically.", "AI": {"tldr": "CoT supervision can speed up generalization, but its benefits depend on task complexity; a transient phase exists where models produce correct answers before CoT traces align; a kinetic modeling framework and trace faithfulness emerge during training; CoT changes internal computation but does not fully overcome highly complex tasks.", "motivation": "\u63ed\u793a\u6a21\u578b\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5229\u7528\u548c\u53d7\u76ca\u4e8eChain-of-Thought\uff08CoT\uff09\u76d1\u7763\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u7b97\u6cd5\u590d\u6742\u5ea6\u4e0b\u7684\u5b66\u4e60\u52a8\u6001\uff1b\u901a\u8fc7\u53ef\u63a7\u6570\u636e\u4e0e\u590d\u6742\u5ea6\u6765\u7406\u89e3\u201cgroking\u201d\u73b0\u8c61\uff0c\u4ee5\u53ca\u4e3a CoT \u7684\u673a\u5236\u63d0\u4f9b\u53ef\u91cf\u5316\u7684\u6846\u67b6\u3002", "method": "\u5728\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u4e0a\u5bf9Transformer\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4efb\u52a1\u5177\u53ef\u63a7\u7684\u7b97\u6cd5\u590d\u6742\u5ea6\u4e0e\u6570\u636e\u5206\u5e03\uff1b\u91c7\u7528\u4e24\u79cd\u8bad\u7ec3\u8bbe\u7f6e\uff1a\u53ea\u8f93\u51fa\u6700\u7ec8\u7b54\u6848\uff1b\u5728\u56de\u7b54\u524d\u8f93\u51fa\u663e\u5f0f\u7684CoT\u63a8\u7406\u8fc7\u7a0b\uff1b\u5bf9\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u51c6\u786e\u7387\u7528\u4e09\u53c2\u6570\u903b\u8f91\u56de\u5f52\u66f2\u7ebf\u62df\u5408\uff0c\u8bc4\u4f30\u5b66\u4e60\u901f\u5ea6\u4e0e\u5f62\u72b6\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u3001\u6570\u636e\u5206\u5e03\u53caCoT\u76d1\u7763\u7684\u53d8\u5316\uff1b\u89c2\u5bdf\u5e76\u5206\u6790\u201c\u8ddf\u8e2a\u4fe1\u5ea6\u201d\u968f\u8bad\u7ec3\u7684\u52a8\u6001\u6f14\u5316\u4ee5\u53ca\u521d\u671f\u53ef\u80fd\u5b58\u5728\u7684\u63a8\u7406\u8f68\u8ff9\u4e0d\u4e00\u81f4\u9636\u6bb5\uff1b\u4ece\u5b9e\u9a8c\u5c42\u9762\u8003\u5bdfCoT\u5bf9\u5185\u90e8\u8ba1\u7b97\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff1a1) CoT Overall \u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5176\u6536\u76ca\u53d7\u4efb\u52a1\u590d\u6742\u5ea6\u9650\u5236\uff1b\u5bf9\u9ad8\u7b97\u6cd5\u590d\u6742\u5ea6\u4efb\u52a1\uff08\u5982\u627e\u51fa\u5217\u8868\u4ea4\u96c6\uff09\u4e0d\u8db3\u4ee5\u5f25\u8865\u7f3a\u5931\uff1b2) \u5f15\u5165\u4e86\u4e00\u79cd\u7528\u4e8e\u7406\u89e3Transformer\u5b66\u4e60\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u6846\u67b6\uff08kinetic modeling\uff09\uff1b3) \u63a8\u7406\u8f68\u8ff9\u7684\u4fe1\u5ea6\u662f\u4e00\u4e2a\u968f\u8bad\u7ec3\u800c\u52a8\u6001\u51fa\u73b0\u7684\u5c5e\u6027\uff0c\u521d\u671f\u6a21\u578b\u53ef\u80fd\u6b63\u786e\u7ed9\u51fa\u7b54\u6848\u4f46\u63a8\u7406\u8f68\u8ff9\u4e0e\u7b54\u6848\u4e0d\u5bf9\u9f50\uff1b4) CoT \u4f1a\u6539\u53d8\u5185\u90e8Transformer\u7684\u8ba1\u7b97\u673a\u5236\uff0c\u4fc3\u8fdb\u66f4\u5feb\u7684\u6cdb\u5316\u4f46\u5e76\u975e\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5b8c\u6574\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "CoT \u662f\u6709\u76ca\u7684\u5de5\u5177\uff0c\u4f46\u5e76\u975e\u666e\u9002\u89e3\uff0c\u5c24\u5176\u5728\u9ad8\u590d\u6742\u5ea6\u7b97\u6cd5\u4efb\u52a1\u4e0a\u53d7\u9650\u3002\u7814\u7a76\u5f3a\u8c03\u5c06\u5b66\u4e60\u52a8\u529b\u5b66\u548c\u8f68\u8ff9\u4fe1\u5ea6\u4f5c\u4e3a\u5206\u6790CoT\u6548\u5e94\u7684\u5173\u952e\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u52a8\u529b\u5b66\u5efa\u6a21\u4f5c\u4e3a\u7406\u89e3Transformer\u5b66\u4e60\u7684\u6709\u529b\u5de5\u5177\uff0c\u4e3a\u672a\u6765\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684CoT\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2510.26397", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.26397", "abs": "https://arxiv.org/abs/2510.26397", "authors": ["Ziliang Lyu", "Yiguang Hong", "Lihua Xie", "Miroslav Krstic"], "title": "Safety Margins of Inverse Optimal ISSf Controllers", "comment": null, "summary": "We investigate the gain margin of a general nonlinear system under an inverse\noptimal input-to-state safe (ISSf) controller of the form u=u0(x)+u*(x,u0),\nwhere u0 is the nominal control and u* is the inverse optimal safety filter\nthat minimally modifies the nominal controller's unsafe actions over the\ninfinite horizon. By first establishing a converse ISSf-BF theorem, we reveal\nthe equivalence among the achievability of ISSf by feedback, the achievability\nof inverse optimality, and the solvability of a Hamilton-Jacobi-Isaacs equation\nassociated with the inverse optimal ISSf gain assignment. Then we develop a\ncollection of safety margin results on the overall control u=u0+u*. In the\nabsence of disturbances, we find that standard inverse optimal safe controllers\nhave a certain degree of gain margin. Specifically, when f(x) acts safely but\nu0 acts unsafely, the gain can be decreased by up to half; and when f(x) acts\nunsafely, we establish that, if u0 acts safely, the gain can be increased\narbitrarily, whereas if u0 acts unsafely, the control recovers the full gain\nmargin [1/2,inf). It is shown, however, that under control gain variation, the\nsafe set of these controllers is locally asymptotically stable, which implies\nthat their safety is sensitive to large but bounded disturbances. To make\ninverse optimal ISSf controllers robust to gain variation, we propose a gain\nmargin improvement approach at the expense of an increased control effort. This\nimprovement allows the inverse optimal safe control to inherit the standard\ngain margin of [1/2,inf) without requiring prior knowledge of whether f(x) or\nu0 acts safely on the safety boundary, while simultaneously ensuring global\nasymptotic stability of the resulting safe set. In the presence of\ndisturbances, this improvement idea renders inverse optimal ISSf controllers\nrobust to gain variations with the same gain margin of [1/2,inf).", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e00\u822c\u975e\u7ebf\u6027\u7cfb\u7edf\u5728\u9006\u6700\u4f18\u5b89\u5168\u6ee4\u6ce2\u5668ISSf\u63a7\u5236\u4e0b\u7684\u589e\u76ca\u88d5\u5ea6\uff0c\u5efa\u7acb\u9006\u5411ISSf-BF\u7406\u8bba\u4e0eHJI\u65b9\u7a0b\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\uff0c\u7ed9\u51fa\u65e0\u6270\u52a8\u4e0e\u6709\u6270\u52a8\u6761\u4ef6\u4e0b\u7684\u589e\u76ca\u88d5\u5ea6\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u63d0\u5347\u9c81\u68d2\u6027\u53ca\u4fdd\u6301\u5168\u5c40\u7a33\u5b9a\u6027\u7684\u589e\u76ca\u88d5\u5ea6\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "\u63ed\u793a\u5728\u53cd\u9988\u5b9e\u73b0ISSf\u3001\u9006\u6700\u4f18\u6027\u3001\u4ee5\u53ca\u6c42\u89e3HJI\u7528\u4e8e\u9006\u5411ISSf\u589e\u76ca\u5206\u914d\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u63d0\u5347\u5bf9\u5e26\u6270\u52a8\u7cfb\u7edf\u5b89\u5168\u6027\u4e0e\u9c81\u68d2\u6027\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u9006\u5411ISSf-BF\u5b9a\u7406\uff0c\u5206\u6790\u589e\u76ca\u5bf9\u5b89\u5168\u96c6\u7684\u5f71\u54cd\uff0c\u63a8\u5bfc\u65e0\u6270\u52a8\u4e0b\u5728\u4e0d\u540c\u60c5\u5f62\u7684\u589e\u76ca\u88d5\u5ea6\uff08\u5982f\u5b89\u5168/u0\u4e0d\u5b89\u5168\u7b49\uff09\uff0c\u8bc1\u660egain\u53d8\u5316\u4e0b\u5b89\u5168\u96c6\u7684\u5c40\u90e8\u6e10\u8fd1\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u51fa\u589e\u76ca\u88d5\u5ea6\u6539\u8fdb\u7b56\u7565\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5728\u6270\u52a8\u5b58\u5728\u65f6\u4fdd\u6301\u540c\u7b49\u88d5\u5ea6\u4e14\u5b9e\u73b0\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u3002", "result": "\u83b7\u5f97\u5177\u4f53\u7684\u589e\u76ca\u88d5\u5ea6\u533a\u95f4[1/2, \u221e)\uff0c\u5728\u4e0d\u540c\u60c5\u5f62\u53ef\u5b9e\u73b0\u589e\u76ca\u964d\u4f4e\u6216\u4efb\u610f\u63d0\u5347\uff1b\u82e5f(x)\u4e0d\u5b89\u5168\u4e14u0\u5b89\u5168\u53ef\u4f7f\u589e\u76ca\u4efb\u610f\u589e\u5927\uff1b\u6270\u52a8\u5b58\u5728\u65f6\uff0c\u6539\u8fdb\u7b56\u7565\u4ecd\u7ef4\u6301\u76f8\u540c\u88d5\u5ea6\u5e76\u786e\u4fdd\u5b89\u5168\u96c6\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\uff1b\u540c\u65f6\u9700\u8981\u589e\u52a0\u63a7\u5236\u52aa\u529b\u4ee5\u6362\u53d6\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u9006\u6700\u4f18ISSf\u63a7\u5236\u5177\u5907\u5929\u7136\u7684\u589e\u76ca\u88d5\u5ea6\uff0c\u4e14\u53ef\u901a\u8fc7\u63d0\u51fa\u7684\u7b56\u7565\u5b9e\u73b0\u5bf9\u589e\u76ca\u53d8\u5316\u7684\u9c81\u68d2\u6027\u5e76\u786e\u4fdd\u5b89\u5168\u96c6\u5168\u5c40\u7a33\u5b9a\u6027\uff1b\u4f46\u5b9e\u73b0\u8be5\u9c81\u68d2\u6027\u9700\u4ed8\u51fa\u989d\u5916\u7684\u63a7\u5236\u4ee3\u4ef7\u3002"}}
{"id": "2510.26234", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.26234", "abs": "https://arxiv.org/abs/2510.26234", "authors": ["Mathis Engelbart", "Mike Kosek", "Lars Eggert", "J\u00f6rg Ott"], "title": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS", "comment": null, "summary": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eMedia-over-QUIC\u7684DNS\u53d1\u5e03-\u8ba2\u9605\u53d8\u4f53\uff0c\u7528\u4e8e\u63a8\u9001\u8d44\u6e90\u8bb0\u5f55\u66f4\u65b0\uff0c\u539f\u578b\u5b9e\u73b0\u663e\u793a\u5728\u964d\u4f4e\u66f4\u65b0\u6d41\u91cf\u548c\u63d0\u9ad8\u66f4\u65b0\u65f6\u6548\u65b9\u9762\u6709\u6f5c\u5728\u597d\u5904\uff0c\u4f46\u4e5f\u5e26\u6765\u989d\u5916\u72b6\u6001\u5f00\u9500\u4e0e\u9996\u6b21\u67e5\u8be2\u65f6\u7684\u4f1a\u8bdd\u5efa\u7acb\u5ef6\u8fdf\u3002", "motivation": "DNS \u4f5c\u4e3a\u4e92\u8054\u7f51\u57fa\u7840\u8bbe\u65bd\uff0c\u957f\u671f\u4ece\u9759\u6001\u76ee\u5f55\u670d\u52a1\u9010\u6b65\u6269\u5c55\u5230\u8d1f\u8f7d\u5747\u8861\u3001\u670d\u52a1\u53d1\u73b0\u7b49\u7528\u4f8b\uff1b\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u5411\u6240\u6709\u66fe\u5bf9\u76f8\u5173\u8bb0\u5f55\u611f\u5174\u8da3\u7684\u89e3\u6790\u5668\u4e3b\u52a8\u5206\u53d1\u66f4\u65b0\u6bd4\u8bf7\u6c42-\u7f13\u5b58\u6a21\u5f0f\u66f4\u5408\u9002\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8349\u6848\u7cfb\u7edf\u53ca\u534f\u8bae\uff0c\u57fa\u4e8eMedia-over-QUIC\uff0c\u5b9e\u73b0RR\u66f4\u65b0\u7684\u63a8\u9001\uff1b\u7ed9\u51fa\u539f\u578b\u5b9e\u73b0\u5e76\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u63a8\u9001\u66f4\u65b0\u80fd\u663e\u8457\u964d\u4f4e\u66f4\u65b0\u6d41\u91cf\u5e76\u7f29\u77ed\u89e3\u6790\u5668\u83b7\u53d6\u6700\u65b0\u7248\u672cRR\u7684\u65f6\u95f4\uff0c\u4ece\u800c\u652f\u6301CDN\u4e2d\u7684\u8d1f\u8f7d\u5747\u8861\u7b49\u7528\u4f8b\uff1b\u540c\u65f6\u5e26\u6765\u7aef\u70b9\u4fa7\u989d\u5916\u72b6\u6001\u7ba1\u7406\u5f00\u9500\u548c\u9996\u6b21\u67e5\u8be2\u7684\u4f1a\u8bdd\u5efa\u7acb\u5ef6\u8fdf\uff0c\u589e\u52a0\u67e5\u8be2\u5ef6\u8fdf\u3002", "conclusion": "DNS\u53ef\u4ece\u53d1\u5e03-\u8ba2\u9605\u53d8\u4f53\u4e2d\u53d7\u76ca\uff0c\u63d0\u5347\u66f4\u65b0\u65f6\u6548\u6027\u4e0e\u51cf\u5c11\u6d41\u91cf\uff0c\u4f46\u9700\u89e3\u51b3\u989d\u5916\u5f00\u9500\u548c\u9996\u6b21\u67e5\u8be2\u5ef6\u8fdf\u7b49\u6311\u6218\uff0c\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u4f18\u5316\u4f1a\u8bdd\u7ba1\u7406\u3001\u964d\u4f4e\u5f00\u9500\u7b49\u3002"}}
{"id": "2510.26166", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26166", "abs": "https://arxiv.org/abs/2510.26166", "authors": ["Juncong Zhou", "Chao Hu", "Guanlin Wu", "Zixiang Ren", "Han Hu", "Juyong Zhang", "Rui Zhang", "Jie Xu"], "title": "6D Channel Knowledge Map Construction via Bidirectional Wireless Gaussian Splatting", "comment": null, "summary": "This paper investigates the construction of channel knowledge map (CKM) from\nsparse channel measurements. Dif ferent from conventional\ntwo-/three-dimensional (2D/3D) CKM approaches assuming fixed base station\nconfigurations, we present a six-dimensional (6D) CKM framework named\nbidirectional wireless Gaussian splatting (BiWGS), which is capable of mod\neling wireless channels across dynamic transmitter (Tx) and receiver (Rx)\npositions in 3D space. BiWGS uses Gaussian el lipsoids to represent virtual\nscatterer clusters and environmental obstacles in the wireless environment. By\nproperly learning the bidirectional scattering patterns and complex attenuation\nprofiles based on channel measurements, these ellipsoids inherently cap ture\nthe electromagnetic transmission characteristics of wireless environments,\nthereby accurately modeling signal transmission under varying transceiver\nconfigurations. Experiment results show that BiWGS significantly outperforms\nclassic multi-layer perception (MLP) for the construction of 6D channel power\ngain map with varying Tx-Rx positions, and achieves spatial spectrum prediction\naccuracy comparable to the state-of-the art wireless radiation field Gaussian\nsplatting (WRF-GS) for 3D CKM construction. This validates the capability of\nthe proposed BiWGS in accomplishing dimensional expansion of 6D CKM\nconstruction, without compromising fidelity.", "AI": {"tldr": "\u63d0\u51fa\u516d\u7ef4\uff086D\uff09CKM\u6846\u67b6BiWGS\uff0c\u901a\u8fc7\u9ad8\u65af\u692d\u5706\u4f53\u8868\u793a\u865a\u62df\u6563\u5c04\u4f53\u548c\u73af\u5883\u969c\u788d\uff0c\u5728\u52a8\u6001Tx/Rx\u4f4d\u7f6e\u4e0b\u5efa\u6a21\u65e0\u7ebf\u4fe1\u9053\uff1b\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u4f18\u4e8eMLP\u76846DCKM\u6784\u5efa\uff0c\u4e143D CKM\u7684\u9884\u6d4b\u8fbe\u5230\u63a5\u8fd1WRF-GS\u7684\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf2D/3D CKM\u5728\u56fa\u5b9a\u57fa\u7ad9\u914d\u7f6e\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u5728\u52a8\u6001\u30013D\u573a\u666f\u4e2d\u5bf9\u65e0\u7ebf\u4fe1\u9053\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u91c7\u7528\u516d\u7ef4CKM\u6846\u67b6BiWGS\uff0c\u4f7f\u7528\u9ad8\u65af\u692d\u5706\u4f53\u6765\u8868\u793a\u865a\u62df\u6563\u5c04\u4f53\u7c07\u548c\u73af\u5883\u969c\u788d\uff0c\u5b66\u4e60\u53cc\u5411\u6563\u5c04\u6a21\u5f0f\u548c\u590d\u8870\u51cf\u66f2\u7ebf\u4ee5\u6355\u6349\u7535\u78c1\u4f20\u8f93\u7279\u6027\uff1b\u5728\u591a\u6837Tx/Rx\u4f4d\u7f6e\u4e0b\u5bf9\u4fe1\u9053\u8fdb\u884c\u7f16\u7801\uff0c\u8f93\u51fa6D\u7684\u4fe1\u9053\u529f\u7387\u589e\u76ca\u4e0e\u4f20\u64ad\u7279\u6027\u6620\u5c04\u3002", "result": "\u57286DCKM\u6784\u5efa\u65b9\u9762\uff0cBiWGS\u663e\u8457\u4f18\u4e8e\u4f20\u7edfMLP\u57fa\u7ebf\uff1b\u57283D CKM\u6784\u5efa\u65b9\u9762\uff0cBiWGS\u7684\u7a7a\u95f4\u8c31\u9884\u6d4b\u7cbe\u5ea6\u63a5\u8fd1\u4e8e\u5148\u7aefWRF-GS\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5bf96D CKM\u7ef4\u5ea6\u6269\u5c55\u7684\u6709\u6548\u6027\u4f46\u4e0d\u635f\u5931\u4fdd\u771f\u5ea6\u3002", "conclusion": "BiWGS\u4e3a6D CKM\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u4fdd\u771f\u5ea6\u9ad8\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u52a8\u6001Tx/Rx\u914d\u7f6e\u4e0b\u5bf9\u65e0\u7ebf\u4fe1\u9053\u8fdb\u884c\u66f4\u5168\u9762\u7684\u5efa\u6a21\u548c\u9884\u6d4b\u3002"}}
{"id": "2510.26452", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.26452", "abs": "https://arxiv.org/abs/2510.26452", "authors": ["Yi-Ting Hong", "Stefano Rini", "Luca Barletta"], "title": "PolarZero: A Reinforcement Learning Approach for Low-Complexity Polarization Kernel Design", "comment": null, "summary": "Polar codes with large kernels can achieve improved error exponents but are\nchallenging to design with low decoding com- plexity. This work investigates\nkernel construction under recursive maximum likelihood decoding (RMLD) using a\nreinforcement learning framework based on the Gumbel AlphaZero algorithm. The\nproposed method efficiently explores the design space and identifies large-size\nkernels that satisfy a given error exponent while minimizing decoding\ncomplexity. For a size-16 kernel, it achieves 17% lower decoding complexity\nthan handcrafted designs while reaching an error exponent of 0.5183 compared to\n0.5 for Arikan's kernel, demonstrating the effectiveness of the learning-based\napproach for practical polar code construction.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684Gumbel AlphaZero\u6846\u67b6\u8bbe\u8ba1\u5927\u6838\u6781\u5316\u7801\u7684\u6838\u7ed3\u6784\uff0c\u5728\u9012\u5f52\u6700\u5927\u4f3c\u7136\u89e3\u7801\u4e0b\u5b9e\u73b0\u8f83\u4f4e\u89e3\u7801\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u8bef\u5dee\u6307\u6570\u3002\u5bf9\u4e8e\u5c3a\u5bf8\u4e3a16\u7684\u6838\uff0c\u5b66\u4e60\u8bbe\u8ba1\u6bd4\u624b\u5de5\u8bbe\u8ba1\u7684\u89e3\u7801\u590d\u6742\u5ea6\u964d\u4f4e\u7ea617%\uff0c\u8bef\u5dee\u6307\u6570\u8fbe\u52300.5183\uff08\u4f18\u4e8eArikan\u6838\u76840.5\uff09\u3002", "motivation": "\u89e3\u51b3\u5927\u6838\u6781\u5316\u7801\u5728\u63d0\u5347\u8bef\u5dee\u6307\u6570\u7684\u540c\u65f6\u5b9e\u73b0\u4f4e\u89e3\u7801\u590d\u6742\u5ea6\u7684\u6311\u6218\uff1b\u901a\u8fc7\u667a\u80fd\u641c\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u4ee5\u53d1\u73b0\u7b26\u5408\u76ee\u6807\u8bef\u5dee\u6307\u6570\u4e14\u89e3\u7801\u590d\u6742\u5ea6\u66f4\u4f4e\u7684\u6838\u7ed3\u6784\u3002", "method": "\u5728\u9012\u5f52\u6700\u5927\u4f3c\u7136\u89e3\u7801\uff08RMLD\uff09\u6846\u67b6\u4e0b\uff0c\u91c7\u7528\u57fa\u4e8eGumbel AlphaZero\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6838\u8bbe\u8ba1\u8fdb\u884c\u641c\u7d22\u4e0e\u4f18\u5316\uff0c\u76ee\u6807\u662f\u5728\u7ed9\u5b9a\u8bef\u5dee\u6307\u6570\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u89e3\u7801\u590d\u6742\u5ea6\u3002", "result": "\u5bf9\u4e8e\u5c3a\u5bf8\u4e3a16\u7684\u6838\uff0c\u6240\u5f97\u5230\u7684\u5b66\u4e60\u8bbe\u8ba1\u5728\u89e3\u7801\u590d\u6742\u5ea6\u4e0a\u6bd4\u624b\u5de5\u8bbe\u8ba1\u4f4e\u7ea617%\uff0c\u4e14\u8bef\u5dee\u6307\u6570\u8fbe\u52300.5183\uff0c\u8d85\u8fc7Arikan\u6838\u76840.5\u3002", "conclusion": "\u8bc1\u660e\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u6838\u8bbe\u8ba1\u5728\u5b9e\u9645\u6781\u5316\u7801\u6784\u9020\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u6027\u80fd\u4e0e\u5b9e\u73b0\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u66f4\u4f18\u7684\u6743\u8861\u3002"}}
{"id": "2510.25856", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25856", "abs": "https://arxiv.org/abs/2510.25856", "authors": ["Brooke Elizabeth Kidmose", "Andreas Brasen Kidmose", "Cliff C. Zou"], "title": "A Critical Roadmap to Driver Authentication via CAN Bus: Dataset Review, Introduction of the Kidmose CANid Dataset (KCID), and Proof of Concept", "comment": null, "summary": "Modern vehicles remain vulnerable to unauthorized use and theft despite\ntraditional security measures including immobilizers and keyless entry systems.\nCriminals exploit vulnerabilities in Controller Area Network (CAN) bus systems\nto bypass authentication mechanisms, while social media trends have expanded\nauto theft to include recreational joyriding by underage drivers. Driver\nauthentication via CAN bus data offers a promising additional layer of\ndefense-in-depth protection, but existing open-access driver fingerprinting\ndatasets suffer from critical limitations including reliance on decoded\ndiagnostic data rather than raw CAN traffic, artificial fixed-route\nexperimental designs, insufficient sampling rates, and lack of demographic\ninformation.\n  This paper provides a comprehensive review of existing open-access driver\nfingerprinting datasets, analyzing their strengths and limitations to guide\npractitioners in dataset selection. We introduce the Kidmose CANid Dataset\n(KCID), which addresses these fundamental shortcomings by providing raw CAN bus\ndata from 16 drivers across four vehicles, including essential demographic\ninformation and both daily driving and controlled fixed-route data. Beyond\ndataset contributions, we present a driver authentication anti-theft framework\nand implement a proof-of-concept prototype on a single-board computer. Through\nlive road trials with an unaltered passenger vehicle, we demonstrate the\npractical feasibility of CAN bus-based driver authentication anti-theft\nsystems. Finally, we explore diverse applications of KCID beyond driver\nauthentication, including driver profiling for insurance and safety\nassessments, mechanical anomaly detection, young driver monitoring, and\nimpaired driving detection. This work provides researchers with both the data\nand methodological foundation necessary to develop robust, deployable driver\nauthentication systems...", "AI": {"tldr": "\u7efc\u8ff0\u73b0\u6709\u5f00\u6e90\u9a7e\u9a76\u5458\u6307\u7eb9\u6570\u636e\u96c6\u7684\u4f18\u7f3a\u70b9\uff0c\u63d0\u51faKCID\u539f\u59cbCAN\u6570\u636e\u96c6\u5e76\u7528\u4e8e\u9a7e\u9a76\u5458\u8ba4\u8bc1\u9632\u76d7\u7cfb\u7edf\u539f\u578b\uff0c\u8bc1\u5b9eCAN\u603b\u7ebf\u6307\u7eb9\u5728\u73b0\u5b9e\u573a\u666f\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63a2\u8ba8\u5176\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5f00\u6e90\u9a7e\u9a76\u5458\u6307\u7eb9\u6570\u636e\u96c6\u7684\u5173\u952e\u5c40\u9650\u6027\uff08\u4f9d\u8d56\u89e3\u7801\u8bca\u65ad\u6570\u636e\u3001\u56fa\u5b9a\u8def\u5f84\u8bbe\u8ba1\u3001\u91c7\u6837\u7387\u4f4e\u3001\u7f3a\u4e4f\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff09\uff0c\u4ee5\u63a8\u52a8\u66f4\u9c81\u68d2\u7684CAN\u603b\u7ebf\u2014\u57fa\u4e8e\u9a7e\u9a76\u5458\u8ba4\u8bc1\u7684\u9632\u76d7\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u5bf9\u73b0\u6709\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u5ba1\u5e76\u5206\u6790\u5176\u4f18\u7f3a\u70b9\uff0c\u63d0\u51faKCID\u6570\u636e\u96c6\uff1a\u5305\u542b16\u4f4d\u9a7e\u9a76\u5458\u30014\u8f86\u8f66\u7684\u539f\u59cbCAN\u603b\u7ebf\u6570\u636e\u3001\u5b8c\u6574\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u4ee5\u53ca\u65e5\u5e38\u9a7e\u9a76\u4e0e\u53d7\u63a7\u56fa\u5b9a\u8def\u7ebf\u6570\u636e\u3002\u540c\u65f6\u63d0\u51fa\u4e00\u4e2a\u9a7e\u9a76\u5458\u8ba4\u8bc1\u9632\u76d7\u6846\u67b6\u5e76\u5728\u5355\u677f\u673a\u4e0a\u5b9e\u73b0\u6982\u5ff5\u9a8c\u8bc1\uff0c\u901a\u8fc7\u771f\u5b9e\u9053\u8def\u8bd5\u9a8c\u9a8c\u8bc1\u7cfb\u7edf\u53ef\u884c\u6027\uff0c\u5e76\u63a2\u7d22KCID\u5728\u4fdd\u9669\u3001\u5b89\u5168\u8bc4\u4f30\u3001\u673a\u68b0\u5f02\u5e38\u68c0\u6d4b\u3001\u672a\u6210\u5e74\u4eba\u9a7e\u9a76\u76d1\u63a7\u548c\u75b2\u52b3/\u9152\u9a7e\u68c0\u6d4b\u7b49\u573a\u666f\u7684\u5e94\u7528\u3002", "result": "KCID\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u6838\u5fc3\u4e0d\u8db3\uff0c\u5c55\u793a\u4e86\u57fa\u4e8eCAN\u603b\u7ebf\u7684\u9a7e\u9a76\u5458\u8ba4\u8bc1\u9632\u76d7\u7cfb\u7edf\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u9a71\u52a8\u8ba4\u8bc1\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u6570\u636e\u548c\u65b9\u6cd5\u5b66\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u7528\u4e8e\u5f00\u53d1\u9c81\u68d2\u3001\u53ef\u90e8\u7f72\u9a7e\u9a76\u5458\u8ba4\u8bc1\u7cfb\u7edf\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u5b66\u57fa\u7840\uff0cKCID\u7684\u6f5c\u5728\u5e94\u7528\u8d85\u51fa\u8ba4\u8bc1\uff0c\u8fd8\u53ef\u7528\u4e8e\u9a7e\u9a76\u884c\u4e3a\u5206\u6790\u3001\u6c7d\u8f66\u5b89\u5168\u8bc4\u4f30\u7b49\u9886\u57df\u3002"}}
{"id": "2510.25793", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.25793", "abs": "https://arxiv.org/abs/2510.25793", "authors": ["Siavash M. Alamouti", "Fay Arjomandi"], "title": "Optimal Information Combining for Multi-Agent Systems Using Adaptive Bias Learning", "comment": "22 pages, 2 Figures, 62 equations, 47 references", "summary": "Modern multi-agent systems ranging from sensor networks monitoring critical\ninfrastructure to crowdsourcing platforms aggregating human intelligence can\nsuffer significant performance degradation due to systematic biases that vary\nwith environmental conditions. Current approaches either ignore these biases,\nleading to suboptimal decisions, or require expensive calibration procedures\nthat are often infeasible in practice. This performance gap has real\nconsequences: inaccurate environmental monitoring, unreliable financial\npredictions, and flawed aggregation of human judgments. This paper addresses\nthe fundamental question: when can we learn and correct for these unknown\nbiases to recover near-optimal performance, and when is such learning futile?\nWe develop a theoretical framework that decomposes biases into learnable\nsystematic components and irreducible stochastic components, introducing the\nconcept of learnability ratio as the fraction of bias variance predictable from\nobservable covariates. This ratio determines whether bias learning is\nworthwhile for a given system. We prove that the achievable performance\nimprovement is fundamentally bounded by this learnability ratio, providing\nsystem designers with quantitative guidance on when to invest in bias learning\nversus simpler approaches. We present the Adaptive Bias Learning and Optimal\nCombining (ABLOC) algorithm, which iteratively learns bias-correcting\ntransformations while optimizing combination weights through closedform\nsolutions, guaranteeing convergence to these theoretical bounds. Experimental\nvalidation demonstrates that systems with high learnability ratios can recover\nsignificant performance (we achieved 40%-70% of theoretical maximum improvement\nin our examples), while those with low learnability show minimal benefit,\nvalidating our diagnostic criteria for practical deployment decisions.", "AI": {"tldr": "Proposes ABLOC, a framework and algorithm to learn and correct for environment-dependent biases in multi-agent systems by decomposing biases into learnable and irreducible components, with a learnability ratio guiding when learning helps; provides theoretical bounds and empirical validation.", "motivation": "Address performance degradation in multi-agent systems due to biases that vary with environmental conditions; fill the gap between ignoring biases and expensive calibration; provide diagnostic criteria and a practical algorithm for bias learning and combination optimization.", "method": "Theoretically decomposes bias into learnable vs irreducible components; defines learnability ratio as the fraction of bias variance predictable from covariates. Proves performance improvement bounds based on this ratio. Introduces ABLOC algorithm that iteratively learns bias-correcting transformations while optimizing combination weights via closed-form updates, with convergence guarantees to the bounds.", "result": "The framework yields a measurable bound on achievable improvement determined by the learnability ratio. Empirical experiments show systems with high learnability ratios achieve 40%-70% of the theoretical maximum improvement; low learnability leads to minimal benefit, validating the proposed diagnostic criterion.", "conclusion": "Learning biases is beneficial only when a substantial portion of bias is predictable from observable covariates; ABLOC provides a practical method with provable guarantees to reach near-optimal bias correction for high-learnability scenarios while offering a diagnostic tool to avoid wasted effort in low-learnability settings."}}
{"id": "2510.26531", "categories": ["eess.SY", "cs.RO", "cs.SY", "93-XX"], "pdf": "https://arxiv.org/pdf/2510.26531", "abs": "https://arxiv.org/abs/2510.26531", "authors": ["David Leprich", "Mario Rosenfelder", "Markus Herrmann-Wicklmayr", "Kathrin Fla\u00dfkamp", "Peter Eberhard", "Henrik Ebel"], "title": "Efficient Collision-Avoidance Constraints for Ellipsoidal Obstacles in Optimal Control: Application to Path-Following MPC and UAVs", "comment": null, "summary": "This article proposes a modular optimal control framework for local\nthree-dimensional ellipsoidal obstacle avoidance, exemplarily applied to model\npredictive path-following control. Static as well as moving obstacles are\nconsidered. Central to the approach is a computationally efficient and\ncontinuously differentiable condition for detecting collisions with ellipsoidal\nobstacles. A novel two-stage optimization approach mitigates numerical issues\narising from the structure of the resulting optimal control problem. The\neffectiveness of the approach is demonstrated through simulations and\nreal-world experiments with the Crazyflie quadrotor. This represents the first\nhardware demonstration of an MPC controller of this kind for UAVs in a\nthree-dimensional task.", "AI": {"tldr": "Modular optimal control framework for 3D ellipsoidal obstacle avoidance integrated with MPC for UAV path-following, handling static/moving obstacles with differentiable collision detection and a two-stage optimization strategy; validated in simulation and on Crazyflie hardware.", "motivation": "Need for efficient, differentiable collision constraints in 3D obstacle avoidance for UAVs and to ensure real-time MPC feasibility with moving obstacles.", "method": "Modular optimal control framework; continuous differentiable collision detection with ellipsoids; two-stage optimization to fix numerical issues; applied to model predictive path-following; experiments with Crazyflie.", "result": "Simulations and real-world experiments validate effectiveness; first hardware demonstration of such an MPC controller for UAVs in 3D task.", "conclusion": "The proposed approach provides an effective, hardware-validated method for 3D ellipsoidal obstacle avoidance in UAV MPC, with differentiable constraints and mitigated numerical issues, enabling practical deployment."}}
{"id": "2510.26256", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.26256", "abs": "https://arxiv.org/abs/2510.26256", "authors": ["Geng Sun", "Siyi Chen", "Zemin Sun", "Long He", "Jiacheng Wang", "Dusit Niyato", "Zhu Han", "Dong In Kim"], "title": "Joint Computing Resource Allocation and Task Offloading in Vehicular Fog Computing Systems Under Asymmetric Information", "comment": "19 pages, 17 figures", "summary": "Vehicular fog computing (VFC) has emerged as a promising paradigm, which\nleverages the idle computational resources of nearby fog vehicles (FVs) to\ncomplement the computing capabilities of conventional vehicular edge computing.\nHowever, utilizing VFC to meet the delay-sensitive and computation-intensive\nrequirements of the FVs poses several challenges. First, the limited resources\nof road side units (RSUs) struggle to accommodate the growing and diverse\ndemands of vehicles. This limitation is further exacerbated by the information\nasymmetry between the controller and FVs due to the reluctance of FVs to\ndisclose private information and to share resources voluntarily. This\ninformation asymmetry hinders the efficient resource allocation and\ncoordination. Second, the heterogeneity in task requirements and the varying\ncapabilities of RSUs and FVs complicate efficient task offloading, thereby\nresulting in inefficient resource utilization and potential performance\ndegradation. To address these challenges, we first present a hierarchical VFC\narchitecture that incorporates the computing capabilities of both RSUs and FVs.\nThen, we formulate a delay minimization optimization problem (DMOP), which is\nan NP-hard mixed integer nonlinear programming problem. To solve the DMOP, we\npropose a joint computing resource allocation and task offloading approach\n(JCRATOA). Specifically, we propose a convex optimization-based method for RSU\nresource allocation and a contract theory-based incentive mechanism for FV\nresource allocation. Moreover, we present a two-sided matching method for task\noffloading by employing the matching game. Simulation results demonstrate that\nthe proposed JCRATOA is able to achieve superior performances in task\ncompletion delay, task completion ratio, system throughput, and resource\nutilization fairness, while effectively meeting the satisfying constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5206\u5c42VFC\u67b6\u6784\uff0c\u7ed3\u5408RSU\u548cFV\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u91c7\u7528\u51f8\u4f18\u5316\u8fdb\u884cRSU\u8d44\u6e90\u5206\u914d\u3001\u5951\u7ea6\u7406\u8bba\u6fc0\u52b1\u673a\u5236\u8fdb\u884cFV\u8d44\u6e90\u914d\u7f6e\uff0c\u4ee5\u53ca\u53cc\u8fb9\u5339\u914d\u7406\u8bba\u7528\u4e8e\u4efb\u52a1\u5378\u8f7d\uff0c\u4ee5\u89e3\u51b3\u4fe1\u606f\u4e0d\u5bf9\u79f0\u3001\u8d44\u6e90\u6709\u9650\u548c\u4efb\u52a1\u5f02\u8d28\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u5b8c\u6210\u7387\u3001\u541e\u5410\u91cf\u4e0e\u8d44\u6e90\u5229\u7528\u516c\u5e73\u6027\u3002", "motivation": "\u89e3\u51b3VFC\u4e2d\u8d44\u6e90\u6709\u9650\u3001\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u4efb\u52a1\u5f02\u8d28\u6027\u5e26\u6765\u7684\u6311\u6218\uff1b\u901a\u8fc7\u5206\u5c42\u67b6\u6784\u5145\u5206\u5229\u7528RSU\u4e0eFV\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u8d44\u6e90\u914d\u7f6e\u6548\u7387\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3aDMOP\uff08\u5ef6\u8fdf\u6700\u5c0f\u5316\u7684\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\uff0cNP-hard\uff09\u3002\u63d0\u51faJCRATOA\uff1a1) \u4f7f\u7528\u51f8\u4f18\u5316\u5bf9RSU\u8d44\u6e90\u8fdb\u884c\u5206\u914d\uff1b2) \u4ee5\u5951\u7ea6\u7406\u8bba\u6784\u5efaFV\u8d44\u6e90\u83b7\u53d6\u7684\u6fc0\u52b1\u673a\u5236\uff1b3) \u91c7\u7528\u53cc\u8fb9\u5339\u914d\uff08\u5339\u914d\u535a\u5f08\uff09\u5b9e\u73b0\u4efb\u52a1\u5378\u8f7d\u7684\u9ad8\u6548\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cJCRATOA\u5728\u4efb\u52a1\u5b8c\u6210\u5ef6\u8fdf\u3001\u5b8c\u6210\u7387\u3001\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u516c\u5e73\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e14\u80fd\u6709\u6548\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5c42VFC\u67b6\u6784\u4e0e\u8054\u5408\u8d44\u6e90\u5206\u914d\u53ca\u4efb\u52a1\u5378\u8f7d\u65b9\u6cd5\u80fd\u591f\u7f13\u89e3\u8d44\u6e90\u7ea6\u675f\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u5177\u5907\u5e94\u7528\u6f5c\u529b\u5e76\u4e3a\u540e\u7eed\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.26245", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26245", "abs": "https://arxiv.org/abs/2510.26245", "authors": ["Juyeop Kim", "Hyejin Shin", "Sohee Kim", "Ilmu Byun"], "title": "Design of Orthogonal Phase of Arrival Positioning Scheme Based on 5G PRS and Optimization of TOA Performance", "comment": null, "summary": "This study analyzes the performance of positioning techniques based on\nconfiguration changes of 5G New Radio signals. In 5G networks, a terminal\nposition is determined from the Time of Arrival of Positioning Reference\nSignals transmitted by base stations. We propose an algorithm that improves TOA\naccuracy under low sampling rate constraints and implement 5G PRS for\npositioning in a software defined modem. We also examine how flexible time\nfrequency resource allocation of PRS affects TOA estimation accuracy and\ndiscuss optimal PRS configurations for a given signal environment.", "AI": {"tldr": "\u901a\u8fc7\u5bf95GNR PRS\u5b9a\u4f4d\u7684\u914d\u7f6e\u53d8\u5316\u8fdb\u884c\u5206\u6790\uff0c\u63d0\u51fa\u4f4e\u91c7\u6837\u7387\u6761\u4ef6\u4e0b\u63d0\u9ad8TOA\u7cbe\u5ea6\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u8f6f\u4ef6\u5b9a\u4e49\u8c03\u5236\u89e3\u8c03\u5668\u4e2d\u5b9e\u73b05G PRS\u5b9a\u4f4d\uff1b\u7814\u7a76PRS\u65f6\u9891\u8d44\u6e90\u5206\u914d\u5bf9TOA\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u7ed9\u51fa\u7ed9\u5b9a\u4fe1\u9053\u73af\u5883\u4e0b\u7684\u6700\u4f73PRS\u914d\u7f6e\u3002", "motivation": "\u57285G\u5b9a\u4f4d\u4e2d\uff0cTOA\u57fa\u4e8ePR S\u4fe1\u53f7\u7684\u5230\u8fbe\u65f6\u95f4\uff0c\u9700\u8981\u5728\u53d7\u9650\u8d44\u6e90\uff08\u5982\u4f4e\u91c7\u6837\u7387\uff09\u548c\u53ef\u53d8\u8d44\u6e90\u5206\u914d\u7684\u73af\u5883\u4e2d\u4fdd\u6301\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u5347\u4f4e\u91c7\u6837\u7387\u4e0b\u7684TOA\u7cbe\u5ea6\u5e76\u4f18\u5316PRS\u914d\u7f6e\u4ee5\u9002\u5e94\u4e0d\u540c\u4fe1\u9053\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5728\u4f4e\u91c7\u6837\u7387\u6761\u4ef6\u4e0b\u63d0\u5347TOA\u7cbe\u5ea6\u7684\u7b97\u6cd5\uff1b\u5728\u8f6f\u4ef6\u5b9a\u4e49\u8c03\u5236\u89e3\u8c03\u5668\u4e2d\u5b9e\u73b05G PRS\u5b9a\u4f4d\uff1b\u7cfb\u7edf\u6027\u5206\u6790PRS\u7684\u65f6\u9891\u8d44\u6e90\u53ef\u914d\u7f6e\u6027\u5bf9TOA\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5bf9\u4e8e\u7ed9\u5b9a\u4fe1\u53f7\u73af\u5883\u7684\u6700\u4f18PRS\u914d\u7f6e\u7b56\u7565\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u4f4e\u91c7\u6837\u7387\u4e0bTOA\u7cbe\u5ea6\u7684\u63d0\u5347\uff0c\u4e0e5G PRS\u5b9a\u4f4d\u76f8\u5173\u7684\u7b97\u6cd5\u5728\u8f6f\u4ef6\u5b9a\u4e49\u8c03\u5236\u89e3\u8c03\u5668\u4e2d\u5f97\u5230\u5b9e\u73b0\uff1b\u63ed\u793a\u4e86PRS\u8d44\u6e90\u5206\u914d\u5bf9TOA\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u4e3a\u4e0d\u540c\u4fe1\u53f7\u73af\u5883\u7ed9\u51fa\u914d\u7f6e\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5408\u7406\u7684PRS\u65f6\u95f4-\u9891\u7387\u8d44\u6e90\u914d\u7f6e\u548c\u6709\u6548\u7684TOA\u6539\u8fdb\u7b97\u6cd5\u53ef\u4ee5\u5728\u53d7\u9650\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u9ad85G\u5b9a\u4f4d\u7684TOA\u7cbe\u5ea6\uff0c\u63d0\u4f9b\u4e86\u5728\u5b9e\u9645\u8f6f\u786c\u4ef6\u5e73\u53f0\u4e0a\u5b9e\u65bd5G PRS\u5b9a\u4f4d\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.26552", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.26552", "abs": "https://arxiv.org/abs/2510.26552", "authors": ["Shaocheng Liu", "Qi Chen", "Minquan Cheng"], "title": "Entropy Functions on Two-Dimensional Faces of Polymatroidal Region of Degree Four: Part II: Information Theoretic Constraints Breed New Combinatorial Structures", "comment": "submitted to IEEE Transactions on Information Theory", "summary": "Characterization of entropy functions is of fundamental importance in\ninformation theory. By imposing constraints on their Shannon outer bound, i.e.,\nthe polymatroidal region, one obtains the faces of the region and entropy\nfunctions on them with special structures. In this series of two papers, we\ncharacterize entropy functions on the $2$-dimensional faces of the\npolymatroidal region $\\Gamma_4$. In Part I, we formulated the problem,\nenumerated all $59$ types of $2$-dimensional faces of $\\Gamma_4$ by a\nalgorithm, and fully characterized entropy functions on $49$ types of them. In\nthis paper, i.e., Part II, we will characterize entropy functions on the\nremaining $10$ types of faces, among which $8$ types are fully characterized\nand $2$ types are partially characterized. To characterize these types of\nfaces, we introduce some new combinatorial design structures which are\ninteresting themself.", "AI": {"tldr": "Part II of a two-paper series on entropy functions on 2D faces of the polymatroidal region \u03934. Completes the remaining 10 face types (8 fully characterized, 2 partially). Introduces new combinatorial design structures.", "motivation": "To deepen understanding of entropy structures under the Shannon outer bound by characterizing entropy functions on the faces of the polymatroidal region; extend Part I and enrich combinatorial design theory.", "method": "Extend the framework from Part I: enumerate the 2D face types of \u03934 with an algorithm; analyze entropy functions constrained to each face; fully characterize 8 of the 10 remaining face types and partially characterize the other 2; introduce new combinatorial design structures to facilitate the characterization.", "result": "From the 59 possible 2D face types of \u03934, Part I fully characterized 49. Part II completes the remaining 10 face types, with 8 fully characterized and 2 partially characterized. Additionally, new combinatorial design structures are introduced to assist the characterization.", "conclusion": "A substantial step toward a complete entropy-function picture on 2D faces of \u03934. The newly introduced combinatorial designs not only assist the current classifications but may hold independent interest for related information-theoretic and combinatorial investigations."}}
{"id": "2510.25796", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.25796", "abs": "https://arxiv.org/abs/2510.25796", "authors": ["Farnoosh Namdarpour", "Joseph Y. J. Chow"], "title": "Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning", "comment": null, "summary": "Ride-pooling, also known as ride-sharing, shared ride-hailing, or\nmicrotransit, is a service wherein passengers share rides. This service can\nreduce costs for both passengers and operators and reduce congestion and\nenvironmental impacts. A key limitation, however, is its myopic\ndecision-making, which overlooks long-term effects of dispatch decisions. To\naddress this, we propose a simulation-informed reinforcement learning (RL)\napproach. While RL has been widely studied in the context of ride-hailing\nsystems, its application in ride-pooling systems has been less explored. In\nthis study, we extend the learning and planning framework of Xu et al. (2018)\nfrom ride-hailing to ride-pooling by embedding a ride-pooling simulation within\nthe learning mechanism to enable non-myopic decision-making. In addition, we\npropose a complementary policy for rebalancing idle vehicles. By employing\nn-step temporal difference learning on simulated experiences, we derive\nspatiotemporal state values and subsequently evaluate the effectiveness of the\nnon-myopic policy using NYC taxi request data. Results demonstrate that the\nnon-myopic policy for matching can increase the service rate by up to 8.4%\nversus a myopic policy while reducing both in-vehicle and wait times for\npassengers. Furthermore, the proposed non-myopic policy can decrease fleet size\nby over 25% compared to a myopic policy, while maintaining the same level of\nperformance, thereby offering significant cost savings for operators.\nIncorporating rebalancing operations into the proposed framework cuts wait time\nby up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%\ncompared to using the framework for matching decisions alone at the cost of\nincreased vehicle minutes traveled per passenger.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4eff\u771f\u4fe1\u606f\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06 ride-pooling \u878d\u5165\u5b66\u4e60\u4e0e\u89c4\u5212\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u975e myopic \u7684\u8c03\u5ea6\u51b3\u7b56\uff0c\u5e76\u901a\u8fc7\u5bf9\u672a\u4efb\u52a1\u7684 idle \u8f66\u8f86\u518d\u5e73\u8861\u7b56\u7565\u8fdb\u884c\u8865\u5145\u3002\u5bf9\u6bd4 myopic \u7b56\u7565\uff0c\u5728 NYC \u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u670d\u52a1\u7387\u63d0\u5347\u3001\u7b49\u5f85/\u8f66\u5185\u65f6\u95f4\u4e0b\u964d\u4ee5\u53ca\u8f66\u961f\u89c4\u6a21\u663e\u8457\u4e0b\u964d\uff0c\u540c\u65f6\u518d\u5e73\u8861\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u5c3d\u7ba1\u5355\u4f4d\u4e58\u5ba2\u884c\u9a76\u91cc\u7a0b\u589e\u52a0\u3002", "motivation": "\u89e3\u51b3 ride-pooling \u5e38\u89c1\u7684\u77ed\u89c6\u51b3\u7b56\u95ee\u9898\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u957f\u671f\u5f71\u54cd\uff1b\u901a\u8fc7\u5c06\u4eff\u771f\u5d4c\u5165\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4f7f\u7b56\u7565\u80fd\u591f\u8fdb\u884c\u975e\u77ed\u89c6\u7684\u5168\u5c40\u4f18\u5316\uff1b\u5e76\u5c06\u518d\u5e73\u8861\u4f5c\u4e3a\u8865\u5145\u7b56\u7565\u4ee5\u63d0\u5347\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u3002", "method": "\u5c06 ride-pooling \u4eff\u771f\u5d4c\u5165\u73b0\u6709\u7684 ride-hailing \u5b66\u4e60\u4e0e\u89c4\u5212\u6846\u67b6\uff08\u53c2\u7167 Xu et al. 2018\uff09\uff0c\u4ee5\u5b9e\u73b0\u975e myopic \u51b3\u7b56\u3002\u4f7f\u7528 n-step \u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u4ece\u4eff\u771f\u4f53\u9a8c\u4e2d\u63a8\u5bfc\u65f6\u7a7a\u72b6\u6001\u503c\uff0c\u5e76\u5728 NYC \u51fa\u79df\u8f66\u9700\u6c42\u6570\u636e\u4e0a\u8bc4\u4f30\u7b56\u7565\u3002\u540c\u65f6\u63d0\u51fa\u4e00\u4e2a\u8865\u5145\u7684 idle \u8f66\u8f86\u518d\u5e73\u8861\u7b56\u7565\uff0c\u5e76\u6bd4\u8f83\u4e24\u8005\u7684\u7ec4\u5408\u6548\u679c\u3002", "result": "\u975e myopic \u7684\u5339\u914d\u7b56\u7565\u76f8\u8f83\u4e8e myopic\uff0c\u670d\u52a1\u7387\u63d0\u5347\u9ad8\u8fbe 8.4%\uff0c\u4e58\u5ba2\u7b49\u5f85\u65f6\u95f4\u4e0e\u8f66\u5185\u65f6\u95f4\u5747\u663e\u8457\u964d\u4f4e\uff1b\u540c\u65f6\u5728\u76f8\u540c\u6027\u80fd\u6c34\u5e73\u4e0b\uff0c\u8f66\u961f\u89c4\u6a21\u53ef\u4e0b\u964d>25%\uff0c\u5177\u5907\u663e\u8457\u8fd0\u8425\u6210\u672c\u8282\u7ea6\u3002\u5c06\u518d\u5e73\u8861\u64cd\u4f5c\u7eb3\u5165\u6846\u67b6\u540e\uff0c\u7b49\u5f85\u65f6\u95f4\u4e0b\u964d\u6700\u591a 27.3%\uff0c\u8f66\u5185\u65f6\u95f4\u4e0b\u964d 12.5%\uff0c\u670d\u52a1\u7387\u63d0\u5347 15.1%\uff0c\u4f46\u5355\u4f4d\u4e58\u5ba2\u91cc\u7a0b\u53ef\u80fd\u589e\u52a0\u3002", "conclusion": "\u57fa\u4e8e\u4eff\u771f\u4fe1\u606f\u7684 RL \u6846\u67b6\u80fd\u591f\u5b9e\u73b0 ride-pooling \u4e2d\u7684\u975e\u77ed\u89c6\u4f18\u5316\uff0c\u5e76\u4e14\u518d\u5e73\u8861\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6574\u4f53\u6548\u80fd\uff0c\u663e\u793a\u51fa\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u8fd0\u8425\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2510.26578", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.26578", "abs": "https://arxiv.org/abs/2510.26578", "authors": ["Jikang Deng", "Hui Zhou", "Mohamed-Slim Alouini"], "title": "Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV Networks", "comment": null, "summary": "In post-disaster scenarios, the rapid deployment of adequate communication\ninfrastructure is essential to support disaster search, rescue, and recovery\noperations. To achieve this, uncrewed aerial vehicle (UAV) has emerged as a\npromising solution for emergency communication due to its low cost and\ndeployment flexibility. However, conventional untethered UAV (U-UAV) is\nconstrained by size, weight, and power (SWaP) limitations, making it incapable\nof maintaining the operation of a macro base station. To address this\nlimitation, we propose a heterogeneous UAV-based framework that integrates\ntethered UAV (T-UAV) and U-UAVs, where U-UAVs are utilized to enhance the\nthroughput of cell-edge ground user equipments (G-UEs) and guarantee seamless\nconnectivity during G-UEs' mobility to safe zones. It is noted that the\nintegrated access and backhaul (IAB) technique is adopted to support the\nwireless backhaul of U-UAVs. Accordingly, we formulate a two-timescale joint\nuser scheduling and trajectory control optimization problem, aiming to maximize\nthe downlink throughput under asymmetric traffic demands and G-UEs' mobility.\nTo solve the formulated problem, we proposed a two-timescale multi-agent deep\ndeterministic policy gradient (TTS-MADDPG) algorithm based on the centralized\ntraining and distributed execution paradigm. Numerical results show that the\nproposed algorithm outperforms other benchmarks, including the two-timescale\nmulti-agent proximal policy optimization (TTS-MAPPO) algorithm and MADDPG\nscheduling method, with robust and higher throughput. Specifically, the\nproposed algorithm obtains up to 12.2\\% average throughput gain compared to the\nMADDPG scheduling method.", "AI": {"tldr": "\u5728\u707e\u540e\u573a\u666f\u4e2d\uff0c\u63d0\u51fa\u4e00\u79cd\u5c06\u7ef3 tethered UAV (T-UAV) \u4e0e\u65e0\u7ef3 UAV (U-UAV) \u76f8\u7ed3\u5408\u7684\u5f02\u6784\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u96c6\u6210\u63a5\u5165\u4e0e\u56de\u4f20\uff08IAB\uff09\u6765\u4e3a U-UAV \u63d0\u4f9b\u65e0\u7ebf\u56de\u4f20\u3002\u901a\u8fc7\u4e24\u65f6\u95f4\u5c3a\u5ea6\u7684\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08TTS-MADDPG\uff09\u7b97\u6cd5\u8fdb\u884c\u8054\u5408\u8c03\u5ea6\u4e0e\u8f68\u8ff9\u4f18\u5316\uff0c\u4ee5\u63d0\u5347\u4e0b\u884c\u541e\u5410\u91cf\uff0c\u4e14\u5728\u4e0e TTS-MAPPO\u3001MADDPG \u7b49\u57fa\u7ebf\u76f8\u6bd4\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff08\u6700\u5927\u7ea6 12.2% \u7684\u5e73\u5747\u541e\u5410\u91cf\u589e\u76ca\uff09\u3002", "motivation": "\u5728\u707e\u96be\u53d1\u751f\u540e\uff0c\u5feb\u901f\u90e8\u7f72\u53ef\u9760\u901a\u4fe1\u4ee5\u652f\u6301\u641c\u7d22\u3001\u6551\u63f4\u4e0e\u6062\u590d\u884c\u52a8\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u65e0\u7ef3 UAV \u53d7\u5236\u4e8e\u5c3a\u5bf8\u3001\u91cd\u91cf\u548c\u529f\u8017\uff08SWaP\uff09\uff0c\u96be\u4ee5\u7ef4\u6301\u5b8f\u57fa\u7ad9\u7684\u8fd0\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u5177\u5907\u6301\u7eed\u4f9b\u7535\u548c\u56de\u4f20\u80fd\u529b\u7684\u7ef3 tethered UAV \u4e0e\u5177\u5907\u7075\u6d3b\u90e8\u7f72\u548c\u8f83\u9ad8\u541e\u5410\u7684\u65e0\u7ef3 UAV \u7ed3\u5408\u8d77\u6765\uff0c\u63d0\u5347\u8fb9\u7f18\u7528\u6237\u7684\u541e\u5410\u548c\u5728\u7528\u6237\u8fc1\u79fb\u5230\u5b89\u5168\u533a\u57df\u65f6\u7684\u8fde\u901a\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u65f6\u95f4\u5c3a\u5ea6\u7684\u8054\u5408\u7528\u6237\u8c03\u5ea6\u4e0e\u8f68\u8ff9\u63a7\u5236\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528 IAB \u6280\u672f\u4e3a U-UAV \u63d0\u4f9b\u65e0\u7ebf\u56de\u4f20\u3002\u6784\u5efa\u4ee5\u96c6\u4e2d\u8bad\u7ec3\u3001\u5206\u5e03\u6267\u884c\u7684\u4e24\u65f6\u95f4\u5c3a\u5ea6\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08TTS-MADDPG\uff09\u7b97\u6cd5\u6765\u6c42\u89e3\u8be5\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4e0e\u4e24\u65f6\u95f4\u5c3a\u5ea6\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08TTS-MAPPO\uff09\u53ca MADDPG \u8c03\u5ea6\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u7b97\u6cd5\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5177\u6709\u9c81\u68d2\u6027\u4e14\u541e\u5410\u91cf\u66f4\u9ad8\u3002\u4e0e MADDPG \u8c03\u5ea6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e73\u5747\u541e\u5410\u91cf\u63d0\u5347\u53ef\u8fbe 12.2%\u3002", "conclusion": "\u5c06\u7ef3 tethered UAV \u4e0e\u65e0\u7ef3 UAV \u76f8\u7ed3\u5408\u3001\u7ed3\u5408 IAB \u7684\u5f02\u6784\u6846\u67b6\uff0c\u4ee5\u53ca\u4e24\u65f6\u95f4\u5c3a\u5ea6\u7684\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u591f\u5728\u707e\u540e\u573a\u666f\u4e2d\u63d0\u9ad8\u4e0b\u884c\u541e\u5410\u5e76\u7ef4\u6301\u79fb\u52a8\u8fb9\u7f18\u7528\u6237\u7684\u8fde\u901a\u6027\uff0c\u9a8c\u8bc1\u4e86\u96c6\u6210\u63a5\u5165/\u56de\u4f20\u548c\u5f02\u6784\u65e0\u4eba\u673a\u534f\u540c\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.26473", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.26473", "abs": "https://arxiv.org/abs/2510.26473", "authors": ["Junya Shiraishi", "Shashi Raj Pandey", "Israel Leyva-Mayorga", "Petar Popovski"], "title": "Wireless Memory Approximation for Energy-efficient Task-specific IoT Data Retrieval", "comment": null, "summary": "The use of Dynamic Random Access Memory (DRAM) for storing Machine Learning\n(ML) models plays a critical role in accelerating ML inference tasks in the\nnext generation of communication systems. However, periodic refreshment of DRAM\nresults in wasteful energy consumption during standby periods, which is\nsignificant for resource-constrained Internet of Things (IoT) devices. To solve\nthis problem, this work advocates two novel approaches: 1) wireless memory\nactivation and 2) wireless memory approximation. These enable the wireless\ndevices to efficiently manage the available memory by considering the timing\naspects and relevance of ML model usage; hence, reducing the overall energy\nconsumption. Numerical results show that our proposed scheme can realize\nsmaller energy consumption than the always-on approach while satisfying the\nretrieval accuracy constraint.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65e0\u7ebf\u5185\u5b58\u6fc0\u6d3b\u4e0e\u65e0\u7ebf\u5185\u5b58\u8fd1\u4f3c\u7684DRAM\u80fd\u6548\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6309\u9700\u7ba1\u7406ML\u6a21\u578b\u5b58\u50a8\u8bbf\u95ee\u4ee5\u964d\u4f4e standby \u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u51c6\u786e\u6027\u3002", "motivation": "\u5728ML\u63a8\u7406\u573a\u666f\u4e2d\uff0cDRAM\u9700\u8981\u5468\u671f\u6027\u5237\u65b0\uff0c\u5bfc\u81f4 standby \u80fd\u8017\u663e\u8457\uff0c\u5c24\u5176\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u3002\u9700\u5728\u4e0d\u727a\u7272\u68c0\u7d22\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u80fd\u6e90\u6d6a\u8d39\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u65b9\u6cd5\uff1a1) \u65e0\u7ebf\u5185\u5b58\u6fc0\u6d3b\uff1a\u901a\u8fc7\u65e0\u7ebf\u673a\u5236\u89e6\u53d1DRAM\u8fdb\u5165\u6d3b\u52a8/\u5f85\u673a\u72b6\u6001\u7684\u63a7\u5236\uff0c\u964d\u4f4e\u975e\u5fc5\u8981\u65f6\u7684\u80fd\u8017\uff1b2) \u65e0\u7ebf\u5185\u5b58\u8fd1\u4f3c\uff1a\u901a\u8fc7\u65e0\u7ebf\u901a\u9053\u5bf9\u5b58\u50a8\u7684ML\u6a21\u578b\u8fdb\u884c\u8fd1\u4f3c/\u538b\u7f29\u8bbf\u95ee\uff0c\u7ed3\u5408\u65f6\u5e8f\u4e0e\u6a21\u578b\u4f7f\u7528\u76f8\u5173\u6027\u6765\u8c03\u5ea6\u8bbf\u95ee\uff0c\u4ece\u800c\u51cf\u5c11\u80fd\u8017\u5e76\u4fdd\u6301\u5fc5\u8981\u7684\u68c0\u7d22\u7cbe\u5ea6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u59cb\u7ec8\u5f00\u542f\u7684\u65b9\u6848\u76f8\u6bd4\uff0c\u5728\u6ee1\u8db3\u68c0\u7d22\u51c6\u786e\u6027\u7ea6\u675f\u7684\u524d\u63d0\u4e0b\uff0c\u8be5\u65b9\u6848\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u80fd\u8017\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u65e0\u7ebf/\u7269\u8054\u7f51\u573a\u666f\u4e0b\u7684\u5185\u5b58\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u80fd\u6548\u63d0\u5347\u9014\u5f84\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u68c0\u7d22\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4eDRAM\u76f8\u5173\u80fd\u8017\uff0c\u9002\u7528\u4e8eML\u63a8\u7406\u52a0\u901f\u7684\u4e0b\u4e00\u4ee3\u901a\u4fe1\u7cfb\u7edf\u3002"}}
{"id": "2510.26262", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26262", "abs": "https://arxiv.org/abs/2510.26262", "authors": ["Francesco Verde", "Donatella Darsena", "Marco Di Renzo", "Vincenzo Galdi"], "title": "Optimal transmit field distribution for partially obstructed continuous radiating surfaces in near-field communication systems", "comment": "5 pages, 5 figures, conference", "summary": "This paper deals with the optimal synthesis of aperture fields for\n(radiating) near-field communications in obstructed environments. A physically\nconsistent model based on knife-edge diffraction is used to formulate the\nproblem as a maximization in Hilbert space. The optimal solution is obtained as\na matched filter that ``matches\" the shape of a diffraction-induced kernel,\nthus linking wave propagation with signal processing methods. The framework\nsupports hardware implementation using continuous apertures such as\nmetasurfaces or lens antennas. This approach bridges physically grounded\nmodeling, signal processing, and hardware design for efficient energy focusing\nin near-field obstructed channels.", "AI": {"tldr": "\u5728\u906e\u6321\u73af\u5883\u4e2d\u7684\u8fd1\u573a\u901a\u4fe1\u4e2d\uff0c\u901a\u8fc7\u57fa\u4e8e knife-edge \u884d\u5c04\u7684\u7269\u7406\u4e00\u81f4\u6027\u5efa\u6a21\uff0c\u5c06\u5b54\u5f84\u573a\u4f18\u5316\u95ee\u9898\u5728\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u8868\u8ff0\u4e3a\u6781\u5927\u5316\uff0c\u5e76\u5f97\u5230\u4e0e\u884d\u5c04\u6838\u5f62\u72b6\u5339\u914d\u7684\u6700\u4f18\u89e3\uff0c\u4fbf\u4e8e\u7528\u8fde\u7eed\u5b54\u5f84\uff08\u5982\u8d85\u8868\u9762/\u900f\u955c\u5929\u7ebf\uff09\u5b9e\u73b0\u786c\u4ef6\u5316\u3002", "motivation": "\u65e8\u5728\u5728\u906e\u6321\u7684\u8fd1\u573a\u4fe1\u9053\u4e2d\u5b9e\u73b0\u9ad8\u6548\u80fd\u91cf\u805a\u7126\uff0c\u5c06\u6ce2\u4f20\u64ad\u7269\u7406\u89c4\u5f8b\u4e0e\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u8026\u5408\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u652f\u6301\u53ef\u5b9e\u9645\u5b9e\u73b0\u7684\u8fde\u7eed\u5b54\u5f84\u5929\u7ebf\u7ed3\u6784\u3002", "method": "\u4ee5 knife-edge \u884d\u5c04\u4e3a\u57fa\u7840\u5efa\u7acb\u7269\u7406\u6a21\u578b\uff0c\u5c07\u5b54\u5f84\u573a\u4f18\u5316\u95ee\u9898\u6295\u5f71\u5230\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u5bfc\u51fa\u6700\u4f18\u89e3\u4e3a\u5339\u914d\u884d\u5c04\u6838\u7684\u6ee4\u6ce2\u5668\uff08\u5339\u914d\u6ee4\u6ce2\u5668\uff09\u3002", "result": "\u7ed9\u51fa\u4e00\u4e2a\u53ef\u5b9e\u65bd\u7684\u6846\u67b6\uff0c\u7406\u8bba\u4e0a\u6700\u4f18\u89e3\u4e3a\u5339\u914d\u884d\u5c04\u6838\u7684\u6ee4\u6ce2\u5668\uff0c\u5f3a\u8c03\u6ce2\u4f20\u64ad\u4e0e\u4fe1\u53f7\u5904\u7406\u7684\u8026\u5408\u53ca\u5bf9\u8fde\u7eed\u5b54\u5f84\u7684\u786c\u4ef6\u5b9e\u73b0\uff08\u5982\u8d85\u8868\u9762\u3001\u900f\u955c\u5929\u7ebf\uff09\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u628a\u7269\u7406\u5efa\u6a21\u3001\u4fe1\u53f7\u5904\u7406\u548c\u786c\u4ef6\u8bbe\u8ba1\u7ed3\u5408\u8d77\u6765\uff0c\u5728\u906e\u6321\u8fd1\u573a\u901a\u9053\u4e2d\u5b9e\u73b0\u9ad8\u6548\u80fd\u91cf\u805a\u7126\u3002"}}
{"id": "2510.25878", "categories": ["cs.CR", "cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.25878", "abs": "https://arxiv.org/abs/2510.25878", "authors": ["Pavel Hub\u00e1\u010dek", "Jan V\u00e1clavek", "Michelle Yeo"], "title": "Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies", "comment": null, "summary": "The rising importance of cryptocurrencies as financial assets pushed their\napplicability from an object of speculation closer to standard financial\ninstruments such as loans. In this work, we initiate the study of secure\nprotocols that enable fiat-denominated loans collateralized by cryptocurrencies\nsuch as Bitcoin. We provide limited-custodial protocols for such loans relying\nonly on trusted arbitration and provide their game-theoretical analysis. We\nalso highlight various interesting directions for future research.", "AI": {"tldr": "\u63d0\u51fa\u4ee5\u6bd4\u7279\u5e01\u7b49\u52a0\u5bc6\u8d44\u4ea7\u4f5c\u4e3a\u62b5\u62bc\u3001\u5b9e\u73b0\u6cd5\u5b9a\u8d27\u5e01\u8d37\u6b3e\u7684\u6709\u9650\u6258\u7ba1\u534f\u8bae\uff0c\u5e76\u7ed9\u51fa\u535a\u5f08\u8bba\u5206\u6790\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u65e5\u76ca\u6210\u4e3a\u91d1\u878d\u8d44\u4ea7\uff0c\u9700\u8981\u5c06\u5176\u4f5c\u4e3a\u62b5\u62bc\u5b9e\u73b0\u7a33\u5b9a\u7684\u6cd5\u5e01\u8d37\u6b3e\uff0c\u63d0\u5347\u91d1\u878d\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u5bf9\u4fe1\u4efb\u7684\u4f9d\u8d56\u3002", "method": "\u8bbe\u8ba1\u6709\u9650\u6258\u7ba1\u7684\u62b5\u62bc\u8d37\u6b3e\u534f\u8bae\uff0c\u57fa\u4e8e\u53d7\u4fe1\u4ef2\u88c1\u7684\u5b89\u5168\u673a\u5236\uff0c\u8fdb\u884c\u5f62\u5f0f\u5316\u535a\u5f08\u8bba\u5206\u6790\uff0c\u5e76\u8ba8\u8bba\u5b9e\u73b0\u7ec6\u8282\u548c\u5b89\u5168\u6027\u3002", "result": "\u63d0\u51fa\u5177\u4f53\u7684\u5b89\u5168\u534f\u8bae\u8bbe\u8ba1\u4e0e\u7406\u8bba\u5206\u6790\uff0c\u5c55\u793a\u5728\u53d7\u4fe1\u4ef2\u88c1\u6761\u4ef6\u4e0b\u7684\u53ef\u884c\u6027\u53ca\u76f8\u5173\u6743\u8861\uff0c\u540c\u65f6\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u786e\u7acb\u4e86\u5c06\u52a0\u5bc6\u8d44\u4ea7\u4f5c\u4e3a\u62b5\u62bc\u7684\u6cd5\u5e01\u8d37\u6b3e\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u6709\u9650\u6258\u7ba1\u8bbe\u8ba1\u7684\u53ef\u884c\u6027\u5e76\u63d0\u51fa\u8fdb\u4e00\u6b65\u7814\u7a76\u8def\u7ebf\u3002"}}
{"id": "2510.25798", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25798", "abs": "https://arxiv.org/abs/2510.25798", "authors": ["Jin Seong", "Jiyun Park", "Wencke Liermann", "Hongseok Choi", "Yoonji Nam", "Hyun Kim", "Soojong Lim", "Namhoon Lee"], "title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing", "comment": "NeurIPS 2025, 38 pages, 8 figures", "summary": "The dynamic nature of information necessitates continuously updating large\nvision-language models (LVLMs). While recent knowledge editing techniques hint\nat promising directions, they often focus on editing a single modality (vision\nor language) in isolation. This prevalent practice neglects the inherent\nmultimodality of LVLMs and the continuous nature of knowledge updates,\npotentially leading to suboptimal editing outcomes when considering the\ninterplay between modalities and the need for ongoing knowledge refinement. To\naddress these limitations, we propose MemEIC, a novel method for Continual and\nCompositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional\nediting of both visual and textual knowledge sequentially. Our approach employs\na hybrid external-internal editor featuring a dual external memory for\ncross-modal evidence retrieval and dual LoRA adapters that facilitate\ndisentangled parameter updates for each modality. A key component is a\nbrain-inspired knowledge connector, activated selectively for compositional\nreasoning, that integrates information across different modalities. Experiments\ndemonstrate that MemEIC significantly improves performance on complex\nmultimodal questions and effectively preserves prior edits, setting a new\nbenchmark for CCKE in LVLMs.", "AI": {"tldr": "MemEIC proposes a continual and compositional knowledge editing framework for LVLMs, enabling sequential editing of both visual and textual knowledge with improved cross-modal reasoning and retention of prior edits.", "motivation": "There is a need to continuously update large vision-language models (LVLMs) in a multimodal setting. Existing knowledge-editing methods largely edit one modality at a time and neglect cross-modal interactions and ongoing refinement, leading to suboptimal outcomes.", "method": "MemEIC employs a hybrid external-internal editor with a dual external memory for cross-modal evidence retrieval and dual LoRA adapters to update parameters for each modality separately. A brain-inspired knowledge connector is selectively activated for compositional reasoning to integrate information across modalities.", "result": "Experiments show significant improvements on complex multimodal question answering and effective preservation of prior edits, establishing a new benchmark for continual and compositional knowledge editing in LVLMs.", "conclusion": "MemEIC enables effective, scalable compositional and continual knowledge editing in LVLMs by combining cross-modal evidence retrieval, modality-specific adapters, and a cross-modal reasoning connector, improving performance while preserving past edits."}}
{"id": "2510.26602", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.26602", "abs": "https://arxiv.org/abs/2510.26602", "authors": ["Tanmay Mishra", "Dakota Hamilton", "Mads R. Almassalkhi"], "title": "Optimal Bidding and Coordinated Dispatch of Hybrid Energy Systems in Regulation Markets", "comment": null, "summary": "The increasing integration of renewable energy sources and distributed energy\nresources (DER) into modern power systems introduces significant uncertainty,\nposing challenges for maintaining grid flexibility and reliability. Hybrid\nenergy systems (HES), composed of controllable generators, flexible loads, and\nbattery storage, offer a decentralized solution to enhance flexibility compared\nto single centralized resources. This paper presents a two-level framework to\nenable HES participation in frequency regulation markets. The upper level\nperforms a chance-constrained optimization to choose capacity bids based on\nhistorical regulation signals. At the lower level, a real-time control strategy\ndisaggregates the regulation power among the constituent resources. This\nreal-time control strategy is then benchmarked against an offline optimal\ndispatch to evaluate flexibility performance. Additionally, the framework\nevaluates the profitability of overbidding strategies and identifies thresholds\nbeyond which performance degradation may lead to market penalties or\ndisqualification. The proposed framework also compare the impact of imbalance\nof power capacities on performance and battery state of charge (SoC) through\nasymmetric HES configurations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u5c42\u6846\u67b6\u4ee5\u4f7f\u6df7\u5408\u80fd\u6e90\u7cfb\u7edf\uff08HES\uff09\u53c2\u4e0e\u9891\u7387\u8c03\u8282\u5e02\u573a\uff1a\u4e0a\u5c42\u8fdb\u884c\u57fa\u4e8e\u5386\u53f2\u8c03\u8282\u4fe1\u53f7\u7684\u6982\u7387\u7ea6\u675f\u5bb9\u91cf\u51fa\u4ef7\uff1b\u4e0b\u5c42\u8fdb\u884c\u5b9e\u65f6\u63a7\u5236\u4ee5\u5c06\u8c03\u8282\u529f\u7387\u5206\u89e3\u5230\u5404\u7ec4\u6210\u8d44\u6e90\uff0c\u5e76\u4e0e\u79bb\u7ebf\u6700\u4f18\u8c03\u5ea6\u5bf9\u6bd4\u8bc4\u4f30\u7075\u6d3b\u6027\uff0c\u540c\u65f6\u7814\u7a76\u6ea2\u4ef7\u7b56\u7565\u7684\u76c8\u5229\u6027\u4e0e\u60e9\u7f5a\u95e8\u69db\uff0c\u4ee5\u53ca\u5bf9\u79f0/\u975e\u5bf9\u79f0HES\u914d\u7f6e\u5bf9\u6027\u80fd\u548cSOC\u7684\u5f71\u54cd\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u548c\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u7684\u5e7f\u6cdb\u63a5\u5165\u5e26\u6765\u663e\u8457\u4e0d\u786e\u5b9a\u6027\uff0c\u6311\u6218\u7535\u7f51\u7684\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u3002\u6df7\u5408\u80fd\u6e90\u7cfb\u7edf\u56e0\u5176\u53ef\u63a7\u53d1\u7535\u3001\u67d4\u6027\u8d1f\u8377\u4e0e\u7535\u6c60\u50a8\u80fd\u7684\u7ec4\u5408\uff0c\u5177\u5907\u6bd4\u5355\u4e00\u8d44\u6e90\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53c2\u4e0e\u9891\u7387\u8c03\u8282\u5e02\u573a\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u5c42\u6846\u67b6\uff1a\u4e0a\u5c42\u901a\u8fc7\u6982\u7387\u7ea6\u675f\u4f18\u5316\uff0c\u57fa\u4e8e\u5386\u53f2\u8c03\u8282\u4fe1\u53f7\u9009\u62e9\u5bb9\u91cf\u51fa\u4ef7\uff1b\u4e0b\u5c42\u7ed9\u51fa\u5b9e\u65f6\u63a7\u5236\u7b56\u7565\uff0c\u5c06\u8c03\u8282\u529f\u7387\u5728\u5404\u8d44\u6e90\u95f4\u5206\u89e3\u3002\u5c06\u8be5\u5b9e\u65f6\u63a7\u5236\u7b56\u7565\u4e0e\u79bb\u7ebf\u6700\u4f18\u8c03\u5ea6\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u7075\u6d3b\u6027\u3002\u8fdb\u4e00\u6b65\u8bc4\u4f30\u6ea2\u4ef7\u51fa\u4ef7\u7684\u76c8\u5229\u6027\u53ca\u8d85\u51fa\u95e8\u69db\u65f6\u7684\u6f5c\u5728\u5e02\u573a\u60e9\u7f5a\u6216\u8d44\u683c\u53d6\u6d88\u98ce\u9669\u3002\u6846\u67b6\u8fd8\u6bd4\u8f83\u4e86 power capacities \u4e0d\u5bf9\u79f0\u914d\u7f6e\u5bf9\u6027\u80fd\u548c\u7535\u6c60\u72b6\u6001\uff08SoC\uff09\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5bf9\u975e\u5bf9\u79f0HES\u914d\u7f6e\u4e0b\u7684\u5f71\u54cd\u3002", "result": "\u521d\u6b65\u5206\u6790\u8868\u660e\uff1a\u8be5\u4e24\u5c42\u6846\u67b6\u80fd\u591f\u63d0\u5347HES\u5bf9\u9891\u7387\u8c03\u8282\u5e02\u573a\u7684\u53c2\u4e0e\u7075\u6d3b\u6027\uff0c\u4e14\u5728\u5386\u53f2\u4fe1\u53f7\u7684\u7ea6\u675f\u4e0b\u80fd\u4fdd\u6301\u5bf9\u8c03\u8282\u9700\u6c42\u7684\u53ef\u63a7\u6027\u3002\u4e0e\u79bb\u7ebf\u6700\u4f18\u8c03\u5ea6\u76f8\u6bd4\uff0c\u5b9e\u65f6\u5206\u89e3\u7b56\u7565\u5728\u5927\u90e8\u5206\u573a\u666f\u4e0b\u80fd\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u4f46\u5728\u9884\u6d4b\u8bef\u5dee\u548c\u5e02\u573a\u89c4\u5219\u7ea6\u675f\u8f83\u5f3a\u65f6\u6027\u80fd\u6709\u6240\u4e0b\u964d\u3002\u6ea2\u4ef7\u7b56\u7565\u5177\u6709\u6f5c\u5728\u76c8\u5229\u6027\uff0c\u4f46\u8d85\u51fa\u67d0\u4e9b\u95e8\u69db\u4f1a\u663e\u8457\u589e\u52a0\u8fdd\u89c4\u98ce\u9669\u751a\u81f3\u5bfc\u81f4\u5e02\u573a\u8d44\u683c\u53d7\u9650\u3002\u5bb9\u91cf\u4e0d\u5bf9\u79f0\u548c\u521d\u59cb/\u8fd0\u884c\u4e2d SOC \u7684\u7ba1\u7406\u5bf9\u7cfb\u7edf\u6027\u80fd\u548c\u80fd\u91cf\u7ba1\u7406\u6709\u660e\u663e\u5f71\u54cd\uff0c\u5408\u7406\u7684\u5bf9\u79f0\u6027\u6216\u6743\u8861\u914d\u7f6e\u80fd\u63d0\u9ad8\u7a33\u5b9a\u6027\u4e0e\u6536\u76ca\u3002", "conclusion": "\u4e24\u5c42\u6846\u67b6\u4e3aHES\u53c2\u4e0e\u9891\u7387\u8c03\u8282\u5e02\u573a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u51b3\u7b56\u4e0e\u63a7\u5236\u8def\u5f84\uff0c\u5f3a\u8c03\u5bb9\u91cf\u5e73\u8861\u4e0eSOC\u7ba1\u7406\u7684\u91cd\u8981\u6027\u3002\u672a\u6765\u5de5\u4f5c\u53ef\u5728\u9c81\u68d2\u4f18\u5316\u3001\u4e0d\u540c\u5e02\u573a\u89c4\u5219\u7684\u9002\u5e94\u6027\uff0c\u4ee5\u53ca\u66f4\u5927\u89c4\u6a21\u7684\u4eff\u771f\u4e0e\u5b9e\u9645\u8bd5\u70b9\u9a8c\u8bc1\u4e0a\u6df1\u5165\u5c55\u5f00\u3002"}}
{"id": "2510.26628", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26628", "abs": "https://arxiv.org/abs/2510.26628", "authors": ["Chuang Zhang", "Geng Sun", "Jiahui Li", "Jiacheng Wang", "Qingqing Wu", "Dusit Niyato", "Shiwen Mao", "Tony Q. S. Quek"], "title": "Low-Altitude UAV-Carried Movable Antenna for Joint Wireless Power Transfer and Covert Communications", "comment": "This paper has been submitted to IEEE Journal on Selected Areas in\n  Communications", "summary": "The proliferation of Internet of Things (IoT) networks has created an urgent\nneed for sustainable energy solutions, particularly for the battery-constrained\nspatially distributed IoT nodes. While low-altitude uncrewed aerial vehicles\n(UAVs) employed with wireless power transfer (WPT) capabilities offer a\npromising solution, the line-of-sight channels that facilitate efficient energy\ndelivery also expose sensitive operational data to adversaries. This paper\nproposes a novel low-altitude UAV-carried movable antenna-enhanced transmission\nsystem joint WPT and covert communications, which simultaneously performs\nenergy supplements to IoT nodes and establishes transmission links with a\ncovert user by leveraging wireless energy signals as a natural cover. Then, we\nformulate a multi-objective optimization problem that jointly maximizes the\ntotal harvested energy of IoT nodes and sum achievable rate of the covert user,\nwhile minimizing the propulsion energy consumption of the low-altitude UAV. To\naddress the non-convex and temporally coupled optimization problem, we propose\na mixture-of-experts-augmented soft actor-critic (MoE-SAC) algorithm that\nemploys a sparse Top-K gated mixture-of-shallow-experts architecture to\nrepresent multimodal policy distributions arising from the conflicting\noptimization objectives. We also incorporate an action projection module that\nexplicitly enforces per-time-slot power budget constraints and antenna position\nconstraints. Simulation results demonstrate that the proposed approach\nsignificantly outperforms some baseline approaches and other state-of-the-art\ndeep reinforcement learning algorithms.", "AI": {"tldr": "\u63d0\u51fa\u5728\u4f4e\u7a7aUAV\u643a\u5e26\u53ef\u79fb\u52a8\u5929\u7ebf\u8fdb\u884cWPT\u4e0e\u9690\u853d\u901a\u4fe1\u7684\u8054\u5408\u7cfb\u7edf\uff0c\u901a\u8fc7\u80fd\u91cf\u4fe1\u53f7\u4f5c\u4e3a\u5929\u7136\u63a9\u62a4\u5b9e\u73b0\u5bf9IoT\u7684\u80fd\u91cf\u8865\u5145\u548c\u5bf9\u9690\u853d\u7528\u6237\u7684\u4f20\u8f93\u94fe\u63a5\uff1b\u63d0\u51faMoE-SAC\u7b97\u6cd5\u89e3\u51b3\u591a\u76ee\u6807\u4f18\u5316\u7684\u975e\u51f8\u4e14\u65f6\u5e8f\u8026\u5408\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u7269\u8054\u7f51\u5bc6\u96c6\u90e8\u7f72\u573a\u666f\u4e2d\uff0c\u80fd\u91cf\u4f9b\u7ed9\u4e0e\u9690\u853d\u901a\u4fe1\u9700\u6c42\u5e76\u5b58\uff0c\u4f20\u7edf\u65b9\u6848\u53d7\u9650\u4e8e\u4fe1\u9053\u53ef\u89c1\u6027\u548c\u80fd\u91cf\u6548\u7387\uff0c\u9700\u5728\u80fd\u6e90\u8865\u7ed9\u3001\u8986\u76d6\u548c\u9690\u853d\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u4e14UAV\u80fd\u91cf\u4e0e\u5929\u7ebf\u4f4d\u7f6e\u7684\u52a8\u6001\u4f18\u5316\u6781\u5177\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4f4e\u7a7a UAV \u643a\u5e26\u53ef\u79fb\u52a8\u5929\u7ebf\u7684\u4f20\u8f93\u7cfb\u7edf\uff0c\u8054\u5408\u65e0\u7ebf\u80fd\u91cf\u4f20\u8f93\u4e0e\u9690\u853d\u901a\u4fe1\u3002\u5efa\u6a21\u4e00\u4e2a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff1a\u6700\u5927\u5316IoT\u8282\u70b9\u603b\u80fd\u91cf\u6536\u83b7\u548c\u9690\u853d\u7528\u6237\u7684\u541e\u5410\u7387\u4e4b\u548c\uff0c\u540c\u65f6\u6700\u5c0f\u5316UAV\u63a8\u8fdb\u80fd\u8017\u3002\u4e3a\u6c42\u89e3\u975e\u51f8\u3001\u65f6\u5e8f\u8026\u5408\u95ee\u9898\uff0c\u63d0\u51fa\u6df7\u5408\u4e13\u5bb6\u589e\u5f3a\u7684\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\uff08MoE-SAC\uff09\u7b97\u6cd5\uff0c\u91c7\u7528\u7a00\u758fTop-K\u95e8\u63a7\u7684\u6df7\u5408\u6d45\u5c42\u4e13\u5bb6\u6765\u8868\u793a\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\uff0c\u5e76\u52a0\u5165\u4e00\u4e2a\u52a8\u4f5c\u6295\u5f71\u6a21\u5757\u5f3a\u5236\u6267\u884c\u9010\u65f6\u9699\u7684\u529f\u7387\u9884\u7b97\u4e0e\u5929\u7ebf\u4f4d\u7f6e\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u53ca\u5148\u8fdb\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u80fd\u5728\u80fd\u91cf\u6536\u83b7\u548c\u9690\u853d\u901a\u4fe1\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u5e76\u964d\u4f4e UAV \u80fd\u8017\u3002", "conclusion": "\u63d0\u51fa\u7684MoE-SAC \u6846\u67b6\u7ed3\u5408\u53ef\u79fb\u52a8\u5929\u7ebf\u548c\u9690\u853d\u901a\u4fe1\u7b56\u7565\uff0c\u63d0\u4f9b\u4e00\u79cd\u5728\u80fd\u91cf\u4f9b\u7ed9\u4e0e\u9690\u853d\u4f20\u8f93\u573a\u666f\u4e2d\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9700\u89e3\u51b3\u4fe1\u9053\u5efa\u6a21\u3001\u786c\u4ef6\u5b9e\u73b0\u4e0e\u6cd5\u89c4\u7ea6\u675f\u7b49\u95ee\u9898\u3002"}}
{"id": "2510.26340", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26340", "abs": "https://arxiv.org/abs/2510.26340", "authors": ["Shih-Kai Chou", "Mengran Zhao", "Cheng-Nan Hu", "Kuang-Chung Chou", "Carolina Fortuna", "Jernej Hribar"], "title": "SABER: Symbolic Regression-based Angle of Arrival and Beam Pattern Estimator", "comment": "12 pages, 11 figures", "summary": "Accurate Angle-of-arrival (AoA) estimation is essential for next-generation\nwireless communication systems to enable reliable beamforming, high-precision\nlocalization, and integrated sensing. Unfortunately, classical high-resolution\ntechniques require multi-element arrays and extensive snapshot collection,\nwhile generic Machine Learning (ML) approaches often yield black-box models\nthat lack physical interpretability. To address these limitations, we propose a\nSymbolic Regression (SR)-based ML framework. Namely, Symbolic Regression-based\nAngle of Arrival and Beam Pattern Estimator (SABER), a constrained\nsymbolic-regression framework that automatically discovers closed-form beam\npattern and AoA models from path loss measurements with interpretability. SABER\nachieves high accuracy while bridging the gap between opaque ML methods and\ninterpretable physics-driven estimators. First, we validate our approach in a\ncontrolled free-space anechoic chamber, showing that both direct inversion of\nthe known $\\cos^n$ beam and a low-order polynomial surrogate achieve sub-0.5\ndegree Mean Absolute Error (MAE). A purely unconstrained SR method can further\nreduce the error of the predicted angles, but produces complex formulas that\nlack physical insight. Then, we implement the same SR-learned inversions in a\nreal-world, Reconfigurable Intelligent Surface (RIS)-aided indoor testbed.\nSABER and unconstrained SR models accurately recover the true AoA with\nnear-zero error. Finally, we benchmark SABER against the Cram\\'er-Rao Lower\nBounds (CRLBs). Our results demonstrate that SABER is an interpretable and\naccurate alternative to state-of-the-art and black-box ML-based methods for AoA\nestimation.", "AI": {"tldr": "SABER: a constrained symbolic-regression framework for AoA and beam-pattern estimation that delivers interpretable closed-form models with high accuracy, achieving sub-0.5\u00b0 MAE in a chamber and near-zero error in RIS indoor tests, competitive with CRLB.", "motivation": "Accurate AoA estimation is critical for beamforming, localization, and sensing, but traditional high-resolution methods demand large arrays and ML models are opaque. There is a need for interpretable, physics-informed estimators.", "method": "Develop SABER: constrained symbolic regression to discover closed-form beam-pattern and AoA models from path-loss measurements; compare with unconstrained SR; validate in an anechoic chamber and a RIS-aided indoor testbed; benchmark against CRLB.", "result": "Sub-0.5 degree MAE for cos^n and low-order polynomial surrogates; unconstrained SR reduces angle error but yields complex formulas; SABER recovers AoA with near-zero error in RIS testbed; SABER competitive with CRLB and superior interpretability.", "conclusion": "Symbolic regression-based approaches can bridge ML and physics-driven estimators, offering interpretable and accurate AoA/beam-pattern models suitable for practical RIS-assisted systems."}}
{"id": "2510.25932", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25932", "abs": "https://arxiv.org/abs/2510.25932", "authors": ["Soufiane Essahli", "Oussama Sarsar", "Imane Fouad", "Anas Motii", "Ahmed Bentajer"], "title": "FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X", "comment": "Accepted for publication in the Proceedings of the 24th IEEE\n  International Conference on Trust, Security and Privacy in Computing and\n  Communications (TrustCom 2025) Privacy track, 11 pages, 8 figures", "summary": "Social platforms distribute information at unprecedented speed, which in turn\naccelerates the spread of misinformation and threatens public discourse. We\npresent FakeZero, a fully client-side, cross-platform browser extension that\nflags unreliable posts on Facebook and X (formerly Twitter) while the user\nscrolls. All computation, DOM scraping, tokenisation, Transformer inference,\nand UI rendering run locally through the Chromium messaging API, so no personal\ndata leaves the device.FakeZero employs a three-stage training curriculum:\nbaseline fine-tuning and domain-adaptive training enhanced with focal loss,\nadversarial augmentation, and post-training quantisation. Evaluated on a\ndataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%\nmacro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of\napproximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant\nvariant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to\n14.7 MB and lowering latency to approximately 40 ms, showing that high-quality\nfake-news detection is feasible under tight resource budgets with only modest\nperformance loss.By providing inline credibility cues, the extension can serve\nas a valuable tool for policymakers seeking to curb the spread of\nmisinformation across social networks. With user consent, FakeZero also opens\nthe door for researchers to collect large-scale datasets of fake news in the\nwild, enabling deeper analysis and the development of more robust detection\ntechniques.", "AI": {"tldr": "\u63d0\u51faFakeZero\uff0c\u4e00\u79cd\u5b8c\u5168\u5728\u5ba2\u6237\u7aef\u672c\u5730\u8fd0\u884c\u7684\u8de8\u5e73\u53f0\u6d4f\u89c8\u5668\u6269\u5c55\uff0c\u7528\u4e8e\u5728Facebook\u548cX\u4e0a\u6807\u8bb0\u4e0d\u53ef\u9760\u4fe1\u606f\uff0c\u5177\u5907\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u548c\u9690\u79c1\u4fdd\u62a4\u7279\u6027\u3002", "motivation": "\u793e\u4ea4\u5e73\u53f0\u4fe1\u606f\u4f20\u64ad\u901f\u5ea6\u7a7a\u524d\uff0c\u9519\u8bef\u4fe1\u606f\u4f1a\u4fb5\u8680\u516c\u5171\u8bdd\u8bed\uff0c\u9700\u8981\u5728\u7528\u6237\u8bbe\u5907\u5c42\u9762\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u7684\u53ef\u4fe1\u4fe1\u606f\u6807\u6ce8\u4e0e\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff1a\u57fa\u7ebf\u5fae\u8c03\u4e0e\u9886\u57df\u81ea\u9002\u5e94\u8bad\u7ec3\uff0c\u7ed3\u5408\u7126\u70b9\u635f\u5931\u3001\u5bf9\u6297\u589e\u5f3a\u53ca\u540e\u8bad\u7ec3\u91cf\u5316\u3002\u6838\u5fc3\u6a21\u578b\u5305\u62ecDistilBERT-Quant\uff0867.6 MB\uff09\u548cTinyBERT-Quant\uff0814.7 MB\uff09\uff0c\u5b9e\u73b0\u5168\u672c\u5730\u63a8\u7406\uff0c\u5ef6\u8fdf\u5728\u51e0\u5341\u6beb\u79d2\u7ea7\u522b\u3002\u5bf9239,000\u6761\u5e16\u5b50\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "DistilBERT-Quant\u5728\u5b8fF1\u4e3a97.1%\u3001\u51c6\u786e\u5ea697.4%\u3001AUROC\u4e3a0.996\uff0c\u5185\u5b58\u4e0e\u5ef6\u8fdf\u9002\u4e2d\uff1bTinyBERT-Quant\u5728\u5b8fF1\u4e3a95.7%\u3001\u51c6\u786e\u5ea696.1%\u3001\u4f53\u79ef\u4e3a14.7 MB\u3001\u5ef6\u8fdf\u7ea640 ms\uff0c\u8d44\u6e90\u66f4\u53cb\u597d\u3002\u663e\u793a\u5728\u6709\u9650\u8d44\u6e90\u9884\u7b97\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5047\u65b0\u95fb\u68c0\u6d4b\u3002", "conclusion": "\u5185\u8054\u53ef\u4fe1\u5ea6\u63d0\u793a\u53ef\u5e2e\u52a9\u653f\u7b56\u5236\u5b9a\u8005\u6cbb\u7406\u8de8\u5e73\u53f0\u9519\u4fe1\u606f\u4f20\u64ad\uff1b\u7ecf\u7528\u6237\u540c\u610f\u540e\u53ef\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u5927\u89c4\u6a21\u5047\u65b0\u95fb\u6570\u636e\u6536\u96c6\uff0c\u4fc3\u8fdb\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.25800", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25800", "abs": "https://arxiv.org/abs/2510.25800", "authors": ["Jialong Sun", "Xinpeng Ling", "Jiaxuan Zou", "Jiawen Kang", "Kejia Zhang"], "title": "FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks", "comment": null, "summary": "The inherent autocorrelation of time series data presents an ongoing\nchallenge to multivariate time series prediction. Recently, a widely adopted\napproach has been the incorporation of frequency domain information to assist\nin long-term prediction tasks. Many researchers have independently observed the\nspectral bias phenomenon in neural networks, where models tend to fit\nlow-frequency signals before high-frequency ones. However, these observations\nhave often been attributed to the specific architectures designed by the\nresearchers, rather than recognizing the phenomenon as a universal\ncharacteristic across models. To unify the understanding of the spectral bias\nphenomenon in long-term time series prediction, we conducted extensive\nempirical experiments to measure spectral bias in existing mainstream models.\nOur findings reveal that virtually all models exhibit this phenomenon. To\nmitigate the impact of spectral bias, we propose the FreLE (Frequency Loss\nEnhancement) algorithm, which enhances model generalization through both\nexplicit and implicit frequency regularization. This is a plug-and-play model\nloss function unit. A large number of experiments have proven the superior\nperformance of FreLE. Code is available at\nhttps://github.com/Chenxing-Xuan/FreLE.", "AI": {"tldr": "FreLE\u901a\u8fc7\u5728\u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u663e\u5f0f\u548c\u9690\u5f0f\u7684\u9891\u7387\u6b63\u5219\u5316\uff0c\u7f13\u89e3\u957f\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u9891\u8c31\u504f\u5dee\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff1b\u5bf9\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\u666e\u904d\u5b58\u5728\u7684\u9891\u8c31\u504f\u5dee\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u9a8c\u6d4b\u91cf\uff0c\u663e\u793a\u5176\u5e7f\u6cdb\u6027\uff0c\u5e76\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u5e26\u6765\u663e\u8457\u6539\u8fdb\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u7684\u81ea\u76f8\u5173\u7279\u6027\u4f7f\u5f97\u957f\u65f6\u9884\u6d4b\u56f0\u96be\uff0c\u9891\u57df\u4fe1\u606f\u7684\u5f15\u5165\u5728\u8fd9\u7c7b\u4efb\u52a1\u4e2d\u88ab\u5e7f\u6cdb\u91c7\u7528\uff1b\u4e0d\u540c\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u7684\u201c\u4f4e\u9891\u5148\u62df\u5408\u201d\u7684\u8c31\u504f\u73b0\u8c61\u63d0\u793a\u8fd9\u662f\u4e00\u79cd\u666e\u904d\u89c4\u5f8b\u800c\u975e\u7279\u5b9a\u67b6\u6784\u7684\u7279\u6027\u3002\u4f5c\u8005\u65e8\u5728\u7edf\u4e00\u7406\u89e3\u8c31\u504f\u3001\u91cf\u5316\u5176\u666e\u904d\u6027\uff0c\u5e76\u63d0\u51fa\u53ef\u63d2\u62d4\u7684\u6b63\u5219\u5316\u65b9\u6848\u4ee5\u6539\u5584\u6cdb\u5316\u3002", "method": "\u5bf9\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u9891\u8c31\u504f\u5dee\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u6d4b\u91cf\uff1b\u63d0\u51faFreLE\uff08Frequency Loss Enhancement\uff09\u7b97\u6cd5\uff0c\u5728\u635f\u5931\u51fd\u6570\u5c42\u9762\u901a\u8fc7\u663e\u5f0f\u7684\u9891\u7387\u6b63\u5219\u5316\u548c\u9690\u5f0f\u6b63\u5219\u5316\u4e24\u6761\u7ebf\u8def\u8fdb\u884c\u9891\u57df\u4fe1\u606f\u5f15\u5bfc\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u63d2\u62d4\u7684\u6a21\u578b\u635f\u5931\u5355\u5143\u5d4c\u5165\u73b0\u6709\u6a21\u578b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u51e0\u4e4e\u6240\u6709\u6a21\u578b\u5747\u5448\u73b0\u8c31\u504f\u73b0\u8c61\uff1b\u5728\u4f7f\u7528FreLE\u540e\uff0c\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u7ed3\u6784\u5177\u6709\u9c81\u68d2\u6027\uff1b\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002", "conclusion": "FreLE\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u4e14\u6613\u4e8e\u5d4c\u5165\u7684\u635f\u5931\u51fd\u6570\u7ec4\u4ef6\uff0c\u6210\u529f\u7f13\u89e3\u4e86\u9891\u8c31\u504f\u5dee\u3001\u63d0\u5347\u957f\u671f\u9884\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u666e\u9002\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u503c\u5f97\u5728\u66f4\u591a\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u5c1d\u8bd5\u3002"}}
{"id": "2510.26701", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.26701", "abs": "https://arxiv.org/abs/2510.26701", "authors": ["Akhila Kandivalasa", "Marcos Netto"], "title": "Graph approach for observability analysis in power system dynamic state estimation", "comment": null, "summary": "The proposed approach yields a numerical method that provably executes in\nlinear time with respect to the number of nodes and edges in a graph. The\ngraph, constructed from the power system model, requires only knowledge of the\ndependencies between state-to-state and output-to-state variables within a\nstate-space framework. While graph-based observability analysis methods exist\nfor power system static-state estimation, the approach presented here is the\nfirst for dynamic-state estimation (DSE). We examine decentralized and\ncentralized DSE scenarios and compare our findings with a well-established,\nalbeit non-scalable, observability analysis method in the literature. When\ncompared to the latter in a centralized DSE setting, our method reduced\ncomputation time by 1440x.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26532", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26532", "abs": "https://arxiv.org/abs/2510.26532", "authors": ["Margarita Cabrera-Bean", "Josep Vidal", "Sergio Fernandez-Bertolin", "Albert Roso-Llorach", "Concepcion Violan"], "title": "HMM for short independent sequences: Multiple sequence Baum-Welch application", "comment": "18 pages Affiliation (1) Universitat Politecnica de Catalunya (UPC);\n  (2) IDIAP Jordi Gol", "summary": "In the classical setting, the training of a Hidden Markov Model (HMM)\ntypically relies on a single, sufficiently long observation sequence that can\nbe regarded as representative of the underlying stochastic process. In this\ncontext, the Expectation Maximization (EM) algorithm is applied in its\nspecialized form for HMMs, namely the Baum Welch algorithm, which has been\nextensively employed in applications such as speech recognition. The objective\nof this work is to present pseudocode formulations for both the training and\ndecoding procedures of HMMs in a different scenario, where the available data\nconsist of multiple independent temporal sequences generated by the same model,\neach of relatively short duration, i.e., containing only a limited number of\nsamples. Special emphasis is placed on the relevance of this formulation to\nlongitudinal studies in population health, where datasets are naturally\nstructured as collections of short trajectories across individuals with point\ndata at follow up.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25983", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25983", "abs": "https://arxiv.org/abs/2510.25983", "authors": ["J. Jon Ryu", "Pavan Yeddanapudi", "Xiangxiang Xu", "Gregory W. Wornell"], "title": "Contrastive Predictive Coding Done Right for Mutual Information Estimation", "comment": "26 pages, 5 figures", "summary": "The InfoNCE objective, originally introduced for contrastive representation\nlearning, has become a popular choice for mutual information (MI) estimation,\ndespite its indirect connection to MI. In this paper, we demonstrate why\nInfoNCE should not be regarded as a valid MI estimator, and we introduce a\nsimple modification, which we refer to as InfoNCE-anchor, for accurate MI\nestimation. Our modification introduces an auxiliary anchor class, enabling\nconsistent density ratio estimation and yielding a plug-in MI estimator with\nsignificantly reduced bias. Beyond this, we generalize our framework using\nproper scoring rules, which recover InfoNCE-anchor as a special case when the\nlog score is employed. This formulation unifies a broad spectrum of contrastive\nobjectives, including NCE, InfoNCE, and $f$-divergence variants, under a single\nprincipled framework. Empirically, we find that InfoNCE-anchor with the log\nscore achieves the most accurate MI estimates; however, in self-supervised\nrepresentation learning experiments, we find that the anchor does not improve\nthe downstream task performance. These findings corroborate that contrastive\nrepresentation learning benefits not from accurate MI estimation per se, but\nfrom the learning of structured density ratios.", "AI": {"tldr": "InfoNCE\u4e0d\u662f\u4e00\u4e2a\u6709\u6548\u7684\u4e92\u4fe1\u606f(MI)\u4f30\u8ba1\u5668\uff1b\u63d0\u51faInfoNCE-anchor\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u951a\u70b9\u5b9e\u73b0\u4e00\u81f4\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u5e76\u663e\u8457\u964d\u4f4e\u504f\u5dee\uff0c\u6784\u6210\u4e00\u4e2a\u57fa\u4e8e\u6b63\u786e\u8bc4\u5206\u89c4\u5219\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u8986\u76d6NCE\u3001InfoNCE\u548cf-\u53d8\u4f53\u3002\u5b9e\u8bc1\u8868\u660e\u5728MI\u4f30\u8ba1\u4e0aInfoNCE-anchor\uff08\u5bf9\u6570\u5206\u6570\uff09\u6700\u51c6\u786e\uff1b\u4f46\u5728\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u951a\u70b9\u5e76\u672a\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6536\u76ca\u6765\u81ea\u7ed3\u6784\u5316\u7684\u5bc6\u5ea6\u6bd4\u5b66\u4e60\u800c\u975eMI\u4f30\u8ba1\u7684\u7cbe\u51c6\u5ea6\u3002", "motivation": "\u63ed\u793aInfoNCE\u4f5c\u4e3aMI\u4f30\u8ba1\u5668\u7684\u5c40\u9650\u6027\uff0c\u5bfb\u6c42\u66f4\u53ef\u9760\u7684MI\u4f30\u8ba1\u65b9\u6cd5\u5e76\u7edf\u4e00\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u7684\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u5bf9\u6bd4\u5b66\u4e60\u4e3a\u4f55\u6709\u6548\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8f85\u52a9\u951a\u70b9\uff08anchor\uff09\u7c7b\u522b\u4ee5\u5b9e\u73b0\u4e00\u81f4\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\uff0c\u5e76\u6784\u5efa\u53ef\u63d2\u62d4\u7684MI\u4f30\u8ba1\u5668\uff08InfoNCE-anchor\uff09\uff1b\u4f7f\u7528\u6b63\u786e\u8bc4\u5206\u89c4\u5219\uff08\u5982\u5bf9\u6570\u5206\u6570\uff09\u5c06\u8be5\u6846\u67b6\u63a8\u5e7f\uff0c\u6db5\u76d6NCE\u3001InfoNCE\u53caf-divergence\u7b49\u53d8\u4f53\uff0c\u5f62\u6210\u4e00\u4e2a\u5355\u4e00\u539f\u7406\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aInfoNCE-anchor\u5728\u5bf9\u6570\u5206\u6570\u4e0b\u63d0\u4f9b\u6700\u51c6\u786e\u7684MI\u4f30\u8ba1\uff1b\u7136\u800c\u5728\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u5b9e\u9a8c\u4e2d\uff0c\u951a\u70b9\u5bf9\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u5e76\u65e0\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5bf9\u6bd4\u5b66\u4e60\u53d7\u76ca\u5e76\u975e\u6765\u81eaMI\u4f30\u8ba1\u7684\u7cbe\u786e\u6027\u3002", "conclusion": "\u5bf9\u6bd4\u5b66\u4e60\u7684\u6536\u76ca\u6765\u81ea\u5b66\u4e60\u7ed3\u6784\u5316\u7684\u5bc6\u5ea6\u6bd4\uff0c\u800c\u975e\u4e25\u683c\u7684MI\u4f30\u8ba1\u51c6\u786e\u6027\uff1bInfoNCE\u4e0d\u5e94\u88ab\u89c6\u4f5cMI\u7684\u76f4\u63a5\u6709\u6548\u4f30\u8ba1\u5668\uff0cInfoNCE-anchor\u63d0\u4f9b\u66f4\u7a33\u5065\u7684MI\u4f30\u8ba1\u5e76\u7edf\u4e00\u4e86\u76f8\u5173\u5bf9\u6bd4\u76ee\u6807\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2510.25939", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25939", "abs": "https://arxiv.org/abs/2510.25939", "authors": ["Robert A. Bridges", "Thomas R. Mitchell", "Mauricio Mu\u00f1oz", "Ted Henriksson"], "title": "SoK: Honeypots & LLMs, More Than the Sum of Their Parts?", "comment": "Systemization of Knowledge", "summary": "The advent of Large Language Models (LLMs) promised to resolve the\nlong-standing paradox in honeypot design: achieving high-fidelity deception\nwith low operational risk. However, despite a flurry of research since late\n2022, progress has been incremental, and the field lacks a cohesive\nunderstanding of the emerging architectural patterns, core challenges, and\nevaluation paradigms. To fill this gap, this Systematization of Knowledge (SoK)\npaper provides the first comprehensive overview of this new domain. We survey\nand systematize three critical, intersecting research areas: first, we provide\na taxonomy of honeypot detection vectors, structuring the core problems that\nLLM-based realism must solve; second, we synthesize the emerging literature on\nLLM-honeypots, identifying a canonical architecture and key evaluation trends;\nand third, we chart the evolutionary path of honeypot log analysis, from simple\ndata reduction to automated intelligence generation. We synthesize these\nfindings into a forward-looking research roadmap, arguing that the true\npotential of this technology lies in creating autonomous, self-improving\ndeception systems to counter the emerging threat of intelligent, automated\nattackers.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u7bc7\u7cfb\u7edf\u5316\u7efc\u8ff0\uff08SoK\uff09\uff0c\u5bf9\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bf1\u6355\u5668\u7814\u7a76\u8fdb\u884c\u68b3\u7406\uff1a\u63d0\u51fa\u68c0\u6d4b\u5411\u91cf\u7684\u5206\u7c7b\u5b66\u6846\u67b6\uff0c\u63cf\u8ff0LLM\u8bf1\u6355\u5668\u7684\u5178\u578b\u67b6\u6784\u4e0e\u8bc4\u4f30\u8d8b\u52bf\uff0c\u5e76\u8ffd\u8e2a\u4ece\u7b80\u5355\u6570\u636e\u964d\u7ef4\u5230\u81ea\u52a8\u5316\u667a\u80fd\u751f\u6210\u7684\u65e5\u5fd7\u5206\u6790\u6f14\u5316\uff0c\u6700\u7ec8\u7ed9\u51fa\u9762\u5411\u81ea\u6cbb\u3001\u81ea\u6211\u6539\u8fdb\u6b3a\u9a97\u7cfb\u7edf\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "motivation": "\u89e3\u51b3LLM\u8bf1\u6355\u5668\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u5206\u6563\u3001\u96f6\u6563\u8fdb\u5c55\u4e0e\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u5347\u5bf9\u9ad8\u4fdd\u771f\u6b3a\u9a97\u80fd\u529b\u4e0e\u4f4e\u64cd\u4f5c\u98ce\u9669\u4e4b\u95f4\u6743\u8861\u7684\u7406\u89e3\uff0c\u5e76\u63a8\u52a8\u5f62\u6210\u53ef\u843d\u5730\u7684\u5bf9\u6297\u6027\u7cfb\u7edf\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u68b3\u7406\u4e0e\u7efc\u5408\uff0c\u5c06\u7814\u7a76\u805a\u7126\u4e8e\u4e09\u5927\u4ea4\u53c9\u9886\u57df\uff1a\u4e00) \u8bf1\u6355\u5668\u68c0\u6d4b\u5411\u91cf\u7684\u5206\u7c7b\u5b66\uff1b\u4e8c) LLM\u8bf1\u6355\u5668\u7684\u5178\u578b\u67b6\u6784\u4e0e\u8bc4\u4f30\u8d8b\u52bf\uff1b\u4e09) \u8bf1\u6355\u65e5\u5fd7\u5206\u6790\u7684\u6f14\u5316\u8def\u5f84\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u9762\u5411\u672a\u6765\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "result": "\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u7814\u7a76\u8c31\u7cfb\uff1a\u660e\u786e\u7684\u68c0\u6d4b\u5411\u91cf\u5206\u7c7b\u6846\u67b6\u3001\u5bf9LLM\u8bf1\u6355\u5668\u7684 canonical \u67b6\u6784\u63cf\u8ff0\u3001\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u6307\u6807\u7684\u6f14\u5316\u8d8b\u52bf\uff0c\u4ee5\u53ca\u65e5\u5fd7\u5206\u6790\u4ece\u6570\u636e\u964d\u7ef4\u5230\u81ea\u52a8\u5316\u667a\u80fd\u751f\u6210\u7684\u6f14\u5316\uff0c\u5f62\u6210\u5bf9\u540e\u7eed\u81ea\u6cbb\u4e0e\u81ea\u6211\u6539\u8fdb\u6b3a\u9a97\u7cfb\u7edf\u7684\u8def\u7ebf\u6307\u5f15\u3002", "conclusion": "\u5f3a\u8c03\u8be5\u9886\u57df\u7684\u6f5c\u529b\u5728\u4e8e\u6784\u5efa\u81ea\u6cbb\u3001\u81ea\u6211\u6539\u8fdb\u7684\u6b3a\u9a97\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u667a\u80fd\u5316\u3001\u81ea\u52a8\u5316\u5316\u7684\u653b\u51fb\u8005\uff0c\u5e76\u7ed9\u51fa\u53ef\u64cd\u4f5c\u7684\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u5f15\u5bfc\u672a\u6765\u5728\u9ad8\u4fdd\u771f\u6b3a\u9a97\u4e0e\u4f4e\u98ce\u9669\u8fd0\u7ef4\u4e4b\u95f4\u5b9e\u73b0\u66f4\u597d\u7684\u6743\u8861\u3002"}}
{"id": "2510.25801", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25801", "abs": "https://arxiv.org/abs/2510.25801", "authors": ["Kun Chen", "Peng Shi", "Haibo Qiu", "Zhixiong Zeng", "Siqi Yang", "Wenji Mao", "Lin Ma"], "title": "Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start", "comment": "Project Page: https://github.com/Kwen-Chen/SPECS-VL", "summary": "Reinforcement learning (RL) with verifiable rewards has recently catalyzed a\nwave of \"MLLM-r1\" approaches that bring RL to vision language models. Most\nrepresentative paradigms begin with a cold start, typically employing\nsupervised fine-tuning (SFT), to initialize the policy before RL. However,\nSFT-based cold start adopts the reasoning paradigm intertwined with task\nsolution and output format, which may induce instruction-style overfitting,\nweakens out-of-distribution generalization, and ultimately affects downstream\nRL. We revisit the cold start along two views, its training method and data\nconstruction, and introduce the Generalization Factor (GF) coefficient to\nquantify the generalization capability under different methods. Our empirical\nstudy finds that preference-based training methods (e.g. DPO) generalizes\nbetter than SFT-based methods in cold start. Motivated by this, we propose\nSPECS-a Self-distilled, Preference-based Cold Start framework that decouples\nmultimodal learning: (1) generates introspective preference data pairs via\nself-distillation, avoiding reliance on larger teachers or manual annotation;\n(2) performs preference-based training to learn, focusing on shallow,\ntransferable surface-form criteria (format, structure, style) rather than\nmemorizing content; and (3) hands off to RL with verifiable rewards for deep\nreasoning results. Experimental results across multiple multimodal benchmarks\nshow that our decoupling learning framework yields consistent performance gains\nover strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.\nAdditional experiments indicate that SPECS contributes to reducing\nin-distribution \"stuckness,\" improving exploration, stabilizing training, and\nraising the performance ceiling.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u89e3\u8026\u7684\u51b7\u542f\u52a8\u6846\u67b6 SPECS\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u751f\u6210\u504f\u597d\u6570\u636e\u5e76\u91c7\u7528\u504f\u597d\u8bad\u7ec3\uff0c\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5728 RL \u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u5728 MEGA-Bench\u3001MathVista \u7b49\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u6539\u5584\u63a2\u7d22\u6027\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u4e0a\u9650\u3002", "motivation": "\u73b0\u6709\u4ee5 SFT \u4e3a\u51b7\u542f\u52a8\u7684\u7b56\u7565\u5bb9\u6613\u5bfc\u81f4\u6307\u4ee4\u98ce\u683c\u7684\u8fc7\u62df\u5408\u3001\u5bf9\u5206\u5e03\u5916(out-of-distribution)\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u8fdb\u800c\u5f71\u54cd\u540e\u7eed\u7684 RL \u8868\u73b0\u3002\u4f5c\u8005\u901a\u8fc7\u5f15\u5165 Generalization Factor (GF) \u6765\u91cf\u5316\u4e0d\u540c\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u57fa\u4e8e\u504f\u597d\u8bad\u7ec3\u7684\u65b9\u6cd5\uff08\u5982 DPO\uff09\u5728\u51b7\u542f\u52a8\u9636\u6bb5\u7684\u6cdb\u5316\u4f18\u4e8e SFT\u3002\u9700\u8981\u4e00\u79cd\u89e3\u8026\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u65e2\u80fd\u63d0\u5347\u6cdb\u5316\uff0c\u53c8\u80fd\u4e3a RL \u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6df1\u5c42\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa Generalization Factor (GF) \u8868\u5f81\u4e0d\u540c\u51b7\u542f\u52a8\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u63d0\u51fa SPECS \u6846\u67b6\uff1a\u81ea\u84b8\u998f\u751f\u6210\u5185\u7701\u5f0f\u504f\u597d\u6570\u636e\u5bf9\uff1b\u8fdb\u884c\u504f\u597d\u8bad\u7ec3\u4ee5\u5b66\u4e60\u5bf9\u8868\u5c42\u5f62\u5f0f\uff08\u683c\u5f0f\u3001\u7ed3\u6784\u3001\u98ce\u683c\uff09\u5173\u6ce8\u7684\u51c6\u5219\uff0c\u907f\u514d\u5bf9\u5185\u5bb9\u7684\u8bb0\u5fc6\u5316\uff1b\u6700\u540e\u5f15\u5bfc\u8fdb\u5165\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684 RL\uff0c\u63d0\u5347\u6df1\u5c42\u63a8\u7406\u7ed3\u679c\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\u53d6\u5f97\u7a33\u5b9a\u6027\u80fd\u63d0\u5347\uff1aMEGA-Bench \u63d0\u5347 4.1%\uff0cMathVista \u63d0\u5347 12.2%\uff1b\u989d\u5916\u5b9e\u9a8c\u8bc1\u660e SPECS \u6709\u52a9\u4e8e\u51cf\u5c11\u5206\u5e03\u5185\u7684\u201c\u5361\u6b7b\u201d\u73b0\u8c61\u3001\u6539\u5584\u63a2\u7d22\u3001\u7a33\u5b9a\u8bad\u7ec3\u5e76\u63d0\u9ad8\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "\u89e3\u8026\u5b66\u4e60\u6846\u67b6 SPECS \u80fd\u63d0\u9ad8\u51b7\u542f\u52a8\u9636\u6bb5\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\uff0c\u4f7f RL with verifiable rewards \u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u83b7\u5f97\u66f4\u4f18\u7684\u7efc\u5408\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u84b8\u998f\u4ea7\u751f\u504f\u597d\u6570\u636e\u3001\u504f\u597d\u8bad\u7ec3\u4ee5\u53ca\u5bf9\u683c\u5f0f\u5316/\u8868\u5f81\u7279\u5f81\u7684\u5173\u6ce8\uff0c\u964d\u4f4e\u5185\u5bb9\u8bb0\u5fc6\u5316\u98ce\u9669\uff0c\u63d0\u5347\u63a2\u7d22\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u5347\u6574\u4f53\u6027\u80fd\u4e0a\u9650\u3002"}}
{"id": "2510.26708", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26708", "abs": "https://arxiv.org/abs/2510.26708", "authors": ["Bowen Li", "Jiping Luo", "Themistoklis Charalambous", "Nikolaos Pappas"], "title": "Pareto-Optimal Sampling and Resource Allocation for Timely Communication in Shared-Spectrum Low-Altitude Networks", "comment": null, "summary": "Guaranteeing stringent data freshness for low-altitude unmanned aerial\nvehicles (UAVs) in shared spectrum forces a critical trade-off between two\noperational costs: the UAV's own energy consumption and the occupation of\nterrestrial channel resources. The core challenge is to satisfy the aerial data\nfreshness while finding a Pareto-optimal balance between these costs.\nLeveraging predictive channel models and predictive UAV trajectories, we\nformulate a bi-objective Pareto optimization problem over a long-term planning\nhorizon to jointly optimize the sampling timing for aerial traffic and the\npower and spectrum allocation for fair coexistence. However, the problem's\nnon-convex, mixed-integer nature renders classical methods incapable of fully\ncharacterizing the complete Pareto frontier. Notably, we show monotonicity\nproperties of the frontier, building on which we transform the bi-objective\nproblem into several single-objective problems. We then propose a new\ngraph-based algorithm and prove that it can find the complete set of Pareto\noptima with low complexity, linear in the horizon and near-quadratic in the\nresource block (RB) budget. Numerical comparisons show that our approach meets\nthe stringent timeliness requirement and achieves a six-fold reduction in RB\nutilization or a 6 dB energy saving compared to benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u4f4e\u7a7a\u65e0\u4eba\u673a\u5728\u5171\u7528\u9891\u8c31\u4e2d\u7684\u6570\u636e\u65b0\u9c9c\u6027\u4fdd\u969c\u7684\u53cc\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u9884\u6d4b\u4fe1\u9053\u548c\u8f68\u8ff9\uff0c\u5c06\u91c7\u6837\u65f6\u5e8f\u3001\u529f\u7387\u4e0e\u9891\u8c31\u5206\u914d\u5728\u957f\u671f\u89c4\u5212\u4e0b\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff1b\u901a\u8fc7\u57fa\u4e8e\u5355\u8c03\u6027\u7684\u8f6c\u5316\uff0c\u63d0\u51fa\u4e00\u79cd\u56fe\u5f62\u5316\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u4f4e\u590d\u6742\u5ea6\u4e0b\u83b7\u5f97\u5b8c\u6574\u7684Pareto\u524d\u6cbf\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u663e\u8457\u7684RB\u5229\u7528\u7387\u4e0b\u964d\u6216\u80fd\u91cf\u8282\u7701\u3002", "motivation": "\u5728\u5171\u4eab\u9891\u8c31\u73af\u5883\u4e2d\uff0c\u786e\u4fdd\u7a7a\u4e2d\u6570\u636e\u7684\u65b0\u9c9c\u6027\u9700\u8981\u5728 UAV \u81ea\u8eab\u80fd\u8017\u548c\u5bf9\u5730\u9762\u4fe1\u9053\u8d44\u6e90\u7684\u5360\u7528\u4e4b\u95f4\u6743\u8861\uff1b\u957f\u671f\u89c4\u5212\u4e0b\u7684Pareto\u4f18\u5316\u80fd\u540c\u65f6\u63d0\u5347\u6570\u636e\u65f6\u6548\u6027\u5e76\u5b9e\u73b0\u8d44\u6e90\u516c\u5e73\u5171\u5b58\u3002", "method": "\u57fa\u4e8e\u9884\u6d4b\u4fe1\u9053\u6a21\u578b\u4e0e\u9884\u6d4b\u7684 UAV \u8f68\u8ff9\uff0c\u5efa\u7acb\u4e00\u4e2a\u8de8\u8d8a\u957f\u65f6\u57df\u7684\u53cc\u76ee\u6807Pareto\u4f18\u5316\u95ee\u9898\uff0c\u8054\u5408\u4f18\u5316\u7a7a\u4e2d\u6570\u636e\u91c7\u6837\u65f6\u5e8f\u3001\u53d1\u5c04\u529f\u7387\u4e0e\u9891\u8c31\u5206\u914d\uff1b\u5229\u7528\u524d\u63d0\u7684\u5355\u8c03\u6027\u6027\u8d28\u5c06\u53cc\u76ee\u6807\u95ee\u9898\u8f6c\u5316\u4e3a\u82e5\u5e72\u5355\u76ee\u6807\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u7b97\u6cd5\uff0c\u8bc1\u660e\u80fd\u591f\u5728\u8f83\u4f4e\u590d\u6742\u5ea6\u4e0b\u627e\u5230\u5b8c\u6574\u7684Pareto\u6700\u4f18\u96c6\u5408\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u968f\u65f6\u57df\u7ebf\u6027\u3001RB\u9884\u7b97\u8fd1\u4f3c\u4e8c\u6b21\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u627e\u5230\u5b8c\u6574\u7684Pareto\u6700\u4f18\u96c6\u5408\uff1b\u6570\u503c\u5bf9\u6bd4\u8868\u660e\u5728\u6ee1\u8db3\u53ca\u65f6\u6027\u8981\u6c42\u7684\u540c\u65f6\uff0c\u53ef\u5b9e\u73b0RB\u5229\u7528\u7387\u7684\u516d\u500d\u4e0b\u964d\u6216\u76f8\u6bd4\u57fa\u7ebf\u8282\u7701\u7ea66 dB\u7684\u80fd\u91cf\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u56fe\u57faPareto\u4f18\u5316\u7b97\u6cd5\u53ef\u5728\u4f4e\u590d\u6742\u5ea6\u4e0b\u5b8c\u6574\u523b\u753bPareto\u524d\u6cbf\uff0c\u63d0\u5347\u4f4e\u7a7a UAV \u5728\u5171\u4eab\u9891\u8c31\u4e2d\u7684\u6570\u636e\u65b0\u9c9c\u6027\u4e0e\u8d44\u6e90\u5171\u5b58\u6548\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.26604", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26604", "abs": "https://arxiv.org/abs/2510.26604", "authors": ["Shahab Moradi Torkashvand", "Arina Kharazi", "Emad Sadeghi", "Seyed Hossein Hesamedin Sadeghi", "Adel Nasiri"], "title": "Statistically Adaptive Differential Protection for AC Microgrids Based on Kullback-Leibler Divergence", "comment": null, "summary": "The proliferation of inverter-based resources challenges traditional\nmicrogrid protection by introducing variable fault currents and complex\ntransients. This paper presents a statistically adaptive differential\nprotection scheme based on Kullback-Leibler divergence, implemented via a\nBartlett-corrected G-statistic computed on logarithm-transformed current\nmagnitudes. The method is a multivariate fault detection engine that employs\nthe Mahalanobis distance to distinguish healthy and faulty states, enabling\nrobust detection even in noisy environments. Detection thresholds are\nstatistically derived from a chi-squared distribution for precise control over\nthe false alarm rate. Upon detection, a lightweight classifier identifies the\nfault type by assessing per-phase G-statistics against dedicated thresholds,\nenhanced by a temporal persistence filter for security. Extensive simulations\non a modified CIGRE 14-bus microgrid show high efficacy: sub-cycle average\ndetection delays, high detection and classification accuracy across operating\nmodes, resilience to high-impedance faults up to 250 Ohms, tolerance to 10 ms\ncommunication delay, and noise levels down to a 20 dB signal-to-noise ratio.\nThese findings demonstrate a reproducible and computationally efficient\nsolution for next-generation AC microgrid protection.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e KL \u6563\u5ea6\u7684\u81ea\u9002\u5e94\u5dee\u5206\u4fdd\u62a4\u65b9\u6848\uff0c\u5229\u7528\u5bf9\u6570\u7535\u6d41\u5e45\u503c\u7684 Bartlett \u4fee\u6b63 G \u7edf\u8ba1\u91cf\u8fdb\u884c\u591a\u53d8\u91cf\u6545\u969c\u68c0\u6d4b\uff0c\u7ed3\u5408 Mahalanobis \u8ddd\u79bb\u5224\u5b9a\u5065\u5eb7/\u6545\u969c\u72b6\u6001\uff0c\u9608\u503c\u7531\u5361\u65b9\u5206\u5e03\u786e\u5b9a\u5e76\u8bbe\u5b9a\u4e25\u683c\u8bef\u8b66\u63a7\u5236\uff1b\u6545\u969c\u7c7b\u578b\u901a\u8fc7\u9010\u76f8 G \u7edf\u8ba1\u91cf\u5206\u7c7b\uff0c\u5e76\u52a0\u5165\u65f6\u5e8f\u6301\u4e45\u6027\u6ee4\u6ce2\u3002\u7ecf\u6539\u9020\u7684 CIGRE 14 \u603b\u7ebf\u5fae\u7f51\u4eff\u771f\u9a8c\u8bc1\uff0c\u5177\u5907\u5b50\u5468\u671f\u68c0\u6d4b\u3001\u8de8\u5de5\u51b5\u9ad8\u51c6\u786e\u7387\uff0c\u5bf9\u9ad8\u963b\u6297\u6545\u969c\u300110 ms \u901a\u4fe1\u5ef6\u8fdf\u548c 20 dB SNR \u7684\u9c81\u68d2\u6027\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u53ef\u91cd\u590d\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u9006\u53d8\u5355\u5143\u7684\u8d44\u6e90\u5927\u91cf\u63a5\u5165\uff0c\u4f20\u7edf\u5fae\u7f51\u4fdd\u62a4\u9762\u4e34\u53ef\u53d8\u7684\u6545\u969c\u7535\u6d41\u548c\u590d\u6742\u77ac\u6001\uff0c\u9700\u5728\u566a\u58f0\u548c\u901a\u4fe1\u5ef6\u8fdf\u73af\u5883\u4e2d\u63d0\u4f9b\u9c81\u68d2\u4e14\u8bef\u8b66\u53ef\u63a7\u7684\u4fdd\u62a4\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e KL \u6563\u5ea6\u7684\u591a\u53d8\u91cf\u5dee\u5206\u4fdd\u62a4\u6846\u67b6\uff1a\u5bf9\u6570\u7535\u6d41\u5e45\u503c\u7684 Bartlett \u4fee\u6b63 G \u7edf\u8ba1\u91cf\u5b9e\u73b0\u521d\u6b65\u6545\u969c\u68c0\u6d4b\uff1b\u4ee5 Mahalanobis \u8ddd\u79bb\u533a\u5206\u5065\u5eb7\u4e0e\u6545\u969c\u72b6\u6001\uff1b\u9608\u503c\u6309\u5361\u65b9\u5206\u5e03\u8bbe\u5b9a\u4ee5\u63a7\u5236\u5047\u8b66\u62a5\u7387\uff1b\u68c0\u6d4b\u540e\u5bf9\u6bcf\u76f8\u7684 G \u7edf\u8ba1\u91cf\u8fdb\u884c\u5206\u7c7b\u5e76\u4f7f\u7528\u4e13\u7528\u9608\u503c\uff0c\u7ed3\u5408\u65f6\u5e8f\u6301\u4e45\u6027\u6ee4\u6ce2\u63d0\u5347\u5b89\u5168\u6027\u3002", "result": "\u5728\u6539\u9020\u7684 CIGRE 14 \u603b\u7ebf\u5fae\u7f51\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u4eff\u771f\uff0c\u5b9e\u73b0\u5b50\u5468\u671f\u7ea7\u5e73\u5747\u68c0\u6d4b\u5ef6\u8fdf\u3001\u8de8\u5de5\u51b5\u7684\u9ad8\u63a2\u6d4b\u4e0e\u5206\u7c7b\u51c6\u786e\u7387\uff1b\u5bf9\u9ad8\u963b\u6297\u6545\u969c\uff08\u81f3 250 \u03a9\uff09\u5177\u9c81\u68d2\u6027\uff1b\u5bf9 10 ms \u901a\u4fe1\u5ef6\u8fdf\u548c 20 dB SNR \u7684\u566a\u58f0\u73af\u5883\u5177\u6709\u5bb9\u5fcd\u6027\u3002", "conclusion": "\u7ed9\u51fa\u4e00\u79cd\u53ef\u91cd\u590d\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u4e0b\u4e00\u4ee3\u4ea4\u6d41\u5fae\u7f51\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u9c81\u68d2\u6027\u4e0e\u53ef\u63a7\u8bef\u8b66\u6027\u3002"}}
{"id": "2510.25960", "categories": ["cs.CR", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25960", "abs": "https://arxiv.org/abs/2510.25960", "authors": ["Zeynep Yasemin Erdogan", "Shishir Nagaraja", "Chuadhry Mujeeb Ahmed", "Ryan Shah"], "title": "WaveVerif: Acoustic Side-Channel based Verification of Robotic Workflows", "comment": "11 pages, 3 figures, Corresponding Author: Prof. Shishir Nagaraja\n  (shishir.nagaraja@newcastle.ac.uk)", "summary": "In this paper, we present a framework that uses acoustic side- channel\nanalysis (ASCA) to monitor and verify whether a robot correctly executes its\nintended commands. We develop and evaluate a machine-learning-based workflow\nverification system that uses acoustic emissions generated by robotic\nmovements. The system can determine whether real-time behavior is consistent\nwith expected commands. The evaluation takes into account movement speed,\ndirection, and microphone distance. The results show that individual robot\nmovements can be validated with over 80% accuracy under baseline conditions\nusing four different classifiers: Support Vector Machine (SVM), Deep Neural\nNetwork (DNN), Recurrent Neural Network (RNN), and Convolutional Neural Network\n(CNN). Additionally, workflows such as pick-and-place and packing could be\nidentified with similarly high confidence. Our findings demonstrate that\nacoustic signals can support real-time, low-cost, passive verification in\nsensitive robotic environments without requiring hardware modifications.", "AI": {"tldr": "\u901a\u8fc7\u58f0\u5b66\u4fa7\u901a\u9053\u5206\u6790\u5b9e\u73b0\u5bf9\u673a\u5668\u4eba\u6267\u884c\u6307\u4ee4\u7684\u5b9e\u65f6\u3001\u88ab\u52a8\u9a8c\u8bc1\uff0c\u57fa\u7ebf\u6761\u4ef6\u4e0b\u5bf9\u5355\u52a8\u4f5c\u53ca\u5de5\u4f5c\u6d41\u5747\u8fbe\u5230\u7ea680%+\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4e14\u65e0\u9700\u786c\u4ef6\u4fee\u6539\u3002", "motivation": "\u89e3\u51b3\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5b9e\u65f6\u3001\u4f4e\u6210\u672c\u4e14\u975e\u4fb5\u5165\u5f0f\u9a8c\u8bc1\u9700\u6c42\uff0c\u5c24\u5176\u5728\u654f\u611f\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u8bc4\u4f30\u3002", "method": "\u5c06\u58f0\u5b66\u53d1\u5c04\u4f5c\u4e3a\u4fa7\u4fe1\u9053\uff0c\u6784\u5efa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5de5\u4f5c\u6d41\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u8bc4\u4f30\u79fb\u52a8\u901f\u5ea6\u3001\u65b9\u5411\u3001\u9ea6\u514b\u98ce\u8ddd\u79bb\u7b49\u56e0\u7d20\uff0c\u4f7f\u7528SVM\u3001DNN\u3001RNN\u3001CNN\u7b49\u56db\u79cd\u5206\u7c7b\u5668\uff0c\u5bf9\u5355\u52a8\u4f5c\u53ca\u62e3\u653e\u3001\u5305\u88c5\u7b49\u5de5\u4f5c\u6d41\u8fdb\u884c\u8bc6\u522b\u4e0e\u9a8c\u8bc1\u3002", "result": "\u5728\u57fa\u7ebf\u6761\u4ef6\u4e0b\uff0c\u5355\u52a8\u4f5c\u53ef\u5b9e\u73b0>80%\u51c6\u786e\u7387\uff1b\u56db\u79cd\u5206\u7c7b\u5668\u5747\u8fbe\u5230\u6216\u63a5\u8fd1\u8be5\u6c34\u5e73\uff1b\u62e3\u653e\u4e0e\u5305\u88c5\u7b49\u5de5\u4f5c\u6d41\u4e5f\u53ef\u4ee5\u540c\u7b49\u9ad8\u7f6e\u4fe1\u5ea6\u88ab\u8bc6\u522b\u3002", "conclusion": "\u58f0\u5b66\u4fe1\u53f7\u53ef\u4ee5\u5728\u654f\u611f\u673a\u5668\u4eba\u73af\u5883\u4e2d\u63d0\u4f9b\u5b9e\u65f6\u3001\u4f4e\u6210\u672c\u3001\u88ab\u52a8\u7684\u9a8c\u8bc1\u80fd\u529b\uff0c\u65e0\u9700\u5bf9\u786c\u4ef6\u8fdb\u884c\u4fee\u6539\u3002"}}
{"id": "2510.26712", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.26712", "abs": "https://arxiv.org/abs/2510.26712", "authors": ["Renato Quartullo", "Andrea Garulli", "Mirko Leomanni"], "title": "Time-Optimal Model Predictive Control for Linear Systems with Multiplicative Uncertainties", "comment": null, "summary": "This paper presents a time-optimal Model Predictive Control (MPC) scheme for\nlinear discrete-time systems subject to multiplicative uncertainties\nrepresented by interval matrices. To render the uncertainty propagation\ncomputationally tractable, the set-valued error system dynamics are\napproximated using a matrix-zonotope-based bounding operator. Recursive\nfeasibility and finite-time convergence are ensured through an adaptive\nterminal constraint mechanism. A key advantage of the proposed approach is that\nall the necessary bounding sets can be computed offline, substantially reducing\nthe online computational burden. The effectiveness of the method is illustrated\nvia a numerical case study on an orbital rendezvous maneuver between two\nsatellites.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u7ebf\u6027\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u7684\u65f6\u95f4\u6700\u4f18MPC\uff0c\u9488\u5bf9\u4e58\u6cd5\u4e0d\u786e\u5b9a\u6027\uff08\u533a\u95f4\u77e9\u9635\uff09\u8fdb\u884c\u9c81\u68d2\u63a7\u5236\uff0c\u5229\u7528\u77e9\u9635-zonotope\u5bf9\u96c6\u5408\u503c\u8bef\u5dee\u8fdb\u884c\u754c\u5b9a\u5e76\u79bb\u7ebf\u8ba1\u7b97\u754c\u9650\uff1b\u901a\u8fc7\u81ea\u9002\u5e94\u7ec8\u7aef\u7ea6\u675f\u786e\u4fdd\u9012\u5f52\u53ef\u884c\u6027\u548c\u6709\u9650\u65f6\u95f4\u6536\u655b\uff0c\u663e\u8457\u964d\u4f4e\u5728\u7ebf\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u536b\u661f\u8f68\u9053\u4f1a\u5408\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u5b9e\u7cfb\u7edf\u5e38\u53d7\u4e58\u6cd5\u578b\u533a\u95f4\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\uff0c\u4f20\u7edfMPC\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5728\u7ebf\u6c42\u89e3\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u4fdd\u969c\u53ef\u884c\u6027\u3002\u9700\u8981\u5728\u4fdd\u8bc1\u6536\u655b\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u65f6\u95f4\u6700\u4f18\uff0c\u5e76\u5c3d\u91cf\u964d\u4f4e\u5728\u7ebf\u8ba1\u7b97\u91cf\uff0c\u4fbf\u4e8e\u5d4c\u5165\u5f0f\u5b9e\u65bd\u3002", "method": "\u5728\u7ebf\u6027\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u4e2d\u5f15\u5165\u4e58\u6cd5\u4e0d\u786e\u5b9a\u6027\uff08\u533a\u95f4\u77e9\u9635\uff09\u5e76\u5c06\u96c6\u5408\u503c\u8bef\u5dee\u7684\u4f20\u64ad\u901a\u8fc7\u77e9\u9635-zonotope\u754c\u5b9a\u5668\u8fdb\u884c\u8fd1\u4f3c\u754c\u5b9a\uff1b\u8bbe\u8ba1\u65f6\u95f4\u6700\u4f18MPC\u76ee\u6807\u4e0e\u81ea\u9002\u5e94\u7ec8\u7aef\u7ea6\u675f\u4ee5\u4fdd\u8bc1\u9012\u5f52\u53ef\u884c\u6027\u548c\u6709\u9650\u65f6\u95f4\u6536\u655b\uff1b\u6240\u6709\u754c\u9650\u96c6\u5408\u53ef\u79bb\u7ebf\u8ba1\u7b97\uff0c\u663e\u8457\u51cf\u8f7b\u5728\u7ebf\u8ba1\u7b97\u8d1f\u62c5\uff1b\u901a\u8fc7\u4e00\u4e2a\u8f68\u9053\u4f1a\u5408\u4efb\u52a1\u7684\u6570\u503c\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7ed9\u51fa\u7406\u8bba\u4fdd\u8bc1\uff1a\u9012\u5f52\u53ef\u884c\u6027\u4e0e\u6709\u9650\u65f6\u95f4\u6536\u655b\u6027\uff1b\u5728\u7ebf\u8ba1\u7b97\u8d1f\u62c5\u660e\u663e\u964d\u4f4e\uff1b\u6570\u503c\u6848\u4f8b\uff08\u4e24\u9897\u536b\u661f\u7684\u8f68\u9053\u4f1a\u5408\uff09\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u542b\u4e0d\u786e\u5b9a\u6027\u7cfb\u7edf\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u7ebf\u6027\u7cfb\u7edf\u7684\u65f6\u95f4\u6700\u4f18MPC\u4e2d\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u8ba1\u7b97\u53cb\u597d\u7684\u89e3\u6cd5\uff0c\u77e9\u9635-zonotope\u754c\u5b9a\u4e0e\u81ea\u9002\u5e94\u7ec8\u7aef\u7ea6\u675f\u662f\u5173\u952e\u8981\u70b9\uff0c\u9002\u7528\u4e8e\u822a\u5929/\u8f68\u9053\u63a7\u5236\u7b49\u9700\u8981\u9ad8\u6548\u9c81\u68d2\u6027\u80fd\u7684\u573a\u666f\u3002"}}
{"id": "2510.26003", "categories": ["cs.CR", "94A60"], "pdf": "https://arxiv.org/pdf/2510.26003", "abs": "https://arxiv.org/abs/2510.26003", "authors": ["Eirini Poimenidou", "K. A. Draziotis"], "title": "Message Recovery Attack in NTRU via Knapsack", "comment": null, "summary": "In the present paper, we introduce a message-recovery attack based on the\nModular Knapsack Problem, applicable to all variants of the NTRU-HPS\ncryptosystem. Assuming that a fraction $\\epsilon$ of the coefficients of the\nmessage ${\\bf{m}}\\in\\{-1,0,1\\}^N$ and of the nonce vector ${\\bf\nr}\\in\\{-1,0,1\\}^N$ are known in advance at random positions, we reduce message\ndecryption to finding a short vector in a lattice that encodes an instance of a\nmodular knapsack system. This allows us to address a key question: how much\ninformation about ${\\bf m}$, or about the pair $({\\bf m},{\\bf r})$, is required\nbefore recovery becomes feasible? A FLATTER reduction successfully recovers the\nmessage, in practice when $\\epsilon\\approx 0.45$. Our implementation finds\n${\\bf m}$ within a few minutes on a commodity desktop.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u80cc\u5305\u95ee\u9898\u7684\u6d88\u606f\u6062\u590d\u653b\u51fb\uff0c\u9488\u5bf9\u6240\u6709\u7248\u672c\u7684 NTRU-HPS\uff0c\u52a0\u4e0a\u90e8\u5206\u4fe1\u606f\u516c\u5f00\u65f6\u7684\u53ef\u884c\u6027\u5206\u6790\u3002\u5bf9 \u03b5 \u7ea6\u7b49\u4e8e 0.45 \u7684\u60c5\u51b5\uff0cFLATTER \u5f52\u7ea6\u80fd\u5728\u5546\u7528\u684c\u9762\u4e0a\u51e0\u5206\u949f\u5185\u6062\u590d\u6d88\u606f\u3002", "motivation": "\u8bc4\u4f30\u5728\u90e8\u5206\u5df2\u77e5\u4fe1\u606f\uff08\u5173\u4e8e\u6d88\u606f m \u548c\u968f\u673a\u5411\u91cf r \u7684\u6bd4\u4f8b \u03b5\uff09\u4e0b\uff0cNTRU-HPS \u7684\u89e3\u5bc6\u662f\u5426\u6613\u88ab\u4eff\u5c04\u653b\u51fb\u6216\u683c\u5b50\u95ee\u9898\u7684\u65b9\u6cd5\u7834\u89e3\uff1b\u63ed\u793a\u4fe1\u606f\u6cc4\u9732\u9608\u503c\u53ca\u5176\u5b9e\u7528\u6027\uff0c\u4e3a\u540e\u7eed\u5bf9\u8be5\u5bc6\u7801\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u5c06\u89e3\u5bc6\u7b49\u4ef7\u95ee\u9898\u8f6c\u5316\u4e3a\u5728\u683c\u5b50\u4e2d\u5bfb\u627e\u77ed\u5411\u91cf\uff0c\u8be5\u683c\u5b50\u7f16\u7801\u4e00\u4e2a\u6a21\u80cc\u5305\u95ee\u9898\u7684\u5b9e\u4f8b\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u7ed3\u5408 FLATTER \u5f52\u7ea6\uff0c\u5229\u7528\u5bf9\u6d88\u606f\u4e0e\u968f\u673a\u5411\u91cf\u5b50\u96c6\u7684\u90e8\u5206\u5df2\u77e5\u4fe1\u606f\u8fdb\u884c\u7834\u89e3\uff1b\u82e5\u5df2\u77e5\u4fe1\u606f\u6bd4\u4f8b \u03b5 \u7ea6\u7b49\u4e8e 0.45\uff0c\u5219\u53ef\u5b9e\u73b0\u5b9e\u7528\u7684\u6d88\u606f\u6062\u590d\u3002", "result": "\u7406\u8bba\u4e0a\u53ef\u884c\u5e76\u5728\u5b9e\u9a8c\u4e0a\u5f97\u5230\u8bc1\u636e\uff1a\u5f53 \u03b5 \u7ea6\u7b49\u4e8e 0.45 \u65f6\uff0c\u80fd\u591f\u4ece\u6a21\u80cc\u5305\u95ee\u9898\u4e2d\u6062\u590d\u6d88\u606f\uff1b\u5b9e\u73b0\u5bf9 m \u7684\u6062\u590d\u65f6\u95f4\u5728\u666e\u901a\u684c\u9762\u8ba1\u7b97\u673a\u4e0a\u51e0\u5206\u949f\u5185\u3002", "conclusion": "\u8868\u660e\u5728\u6709\u76f8\u5f53\u6bd4\u4f8b\u7684\u5148\u9a8c\u4fe1\u606f\u65f6\uff0cNTRU-HPS \u7684\u5b89\u5168\u6027\u5b58\u5728\u663e\u8457\u9690\u60a3\uff0c\u653b\u51fb\u4f9d\u8d56\u4e8e\u5c06\u89e3\u5bc6\u95ee\u9898\u8f6c\u5316\u4e3a\u683c\u5b50\u95ee\u9898\u5e76\u5229\u7528 FLATTER \u5f52\u7ea6\u3002\u5bf9\u53c2\u6570\u9009\u62e9\u3001\u6270\u52a8\u5206\u5e03\u4ee5\u53ca\u4fe1\u606f\u6cc4\u9732\u9608\u503c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4ee5\u8bc4\u4f30\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b9e\u9645\u98ce\u9669\u4e0e\u9632\u62a4\u7b56\u7565\u3002"}}
{"id": "2510.25818", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25818", "abs": "https://arxiv.org/abs/2510.25818", "authors": ["Sungho Koh", "SeungJu Cha", "Hyunwoo Oh", "Kwanyoung Lee", "Dong-Jin Kim"], "title": "ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion", "comment": "NeurIPS 2025. Code: https://github.com/KSH00906/ScaleDiff", "summary": "Text-to-image diffusion models often exhibit degraded performance when\ngenerating images beyond their training resolution. Recent training-free\nmethods can mitigate this limitation, but they often require substantial\ncomputation or are incompatible with recent Diffusion Transformer models. In\nthis paper, we propose ScaleDiff, a model-agnostic and highly efficient\nframework for extending the resolution of pretrained diffusion models without\nany additional training. A core component of our framework is Neighborhood\nPatch Attention (NPA), an efficient mechanism that reduces computational\nredundancy in the self-attention layer with non-overlapping patches. We\nintegrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing\n(LFM) to better generate fine details. Furthermore, we apply Structure Guidance\nto enhance global structure during the denoising process. Experimental results\ndemonstrate that ScaleDiff achieves state-of-the-art performance among\ntraining-free methods in terms of both image quality and inference speed on\nboth U-Net and Diffusion Transformer architectures.", "AI": {"tldr": "ScaleDiff extends pretrained diffusion models to higher resolutions without training, using Neighborhood Patch Attention (NPA) to reduce self-attention redundancy, SDEdit integration, Latent Frequency Mixing (LFM) for detail, and Structure Guidance for global structure; claims state-of-the-art among training-free methods in quality and speed for U-Net and Diffusion Transformer architectures.", "motivation": "Diffusion models struggle to generate high-resolution images beyond training resolution, and existing training-free approaches are either computationally heavy or incompatible with modern Diffusion Transformers.", "method": "Introduce ScaleDiff with (1) Neighborhood Patch Attention (NPA) to replace or augment self-attention using non-overlapping patches for efficiency; (2) integration into an SDEdit-like denoising pipeline; (3) Latent Frequency Mixing (LFM) to enhance fine details in the latent space; (4) Structure Guidance to promote global structural consistency during denoising; (5) evaluation on both U-Net and Diffusion Transformer architectures with a training-free setup.", "result": "Empirical results show ScaleDiff achieves state-of-the-art performance among training-free methods in image quality and inference speed when operating at higher resolutions, applicable to both U-Net and Diffusion Transformer backbones.", "conclusion": "ScaleDiff provides a model-agnostic, highly efficient solution to extend pretrained diffusion models to higher resolutions without extra training, combining novel attention optimization, latent detail enhancement, and global structure guidance."}}
{"id": "2510.26102", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26102", "abs": "https://arxiv.org/abs/2510.26102", "authors": ["Lisha Shuai", "Jiuling Dong", "Nan Zhang", "Shaofeng Tan", "Haokun Zhang", "Zilong Song", "Gaoya Dong", "Xiaolong Yang"], "title": "PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy", "comment": "14 pages, 1 figures", "summary": "Local Differential Privacy (LDP) is a widely adopted privacy-protection model\nin the Internet of Things (IoT) due to its lightweight, decentralized, and\nscalable nature. However, it is vulnerable to poisoning attacks, and existing\ndefenses either incur prohibitive resource overheads or rely on domain-specific\nprior knowledge, limiting their practical deployment. To address these\nlimitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical\nframework for LDP, which departs from resource- or prior-dependent\ncountermeasures and instead leverages the inherent structural consistency of\nLDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies\nstealthy poisoning effects by re-encoding LDP-perturbed data via\nsparsification, normalization, and low-rank projection, thereby revealing both\noutput and rule poisoning attacks through structural inconsistencies in the\nreconstructed space. Theoretical analysis proves that PEEL, integrated with\nLDP, retains unbiasedness and statistical accuracy, while being robust to\nexpose both output and rule poisoning attacks. Moreover, evaluation results\nshow that LDP-integrated PEEL not only outperforms four state-of-the-art\ndefenses in terms of poisoning exposure accuracy but also significantly reduces\nclient-side computational costs, making it highly suitable for large-scale IoT\ndeployments.", "AI": {"tldr": "PEEL: A post-processing framework for Local Differential Privacy to expose poisoning by re-encoding perturbed data; uses sparsification, normalization, and low-rank projection; claims unbiasedness preserved and improves poisoning exposure with lower client-side cost.", "motivation": "LDP is vulnerable to poisoning; existing defenses are resource-intensive or domain-specific; need a light-weight, generalizable defense for IoT-scale deployments.", "method": "PEEL is a non-intrusive post-processing module integrated with LDP that re-encodes LDP-perturbed data via sparsification, normalization, low-rank projection; exploits structural consistency to reveal output and rule poisoning attacks.", "result": "Theoretical analysis shows unbiasedness and accuracy preserved; empirical evaluation shows higher poisoning exposure accuracy than four state-of-the-art defenses and reduced client-side computational costs.", "conclusion": "PEEL enables robust poisoning exposure while maintaining LDP properties and efficiency, suitable for large-scale IoT deployments."}}
{"id": "2510.26103", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26103", "abs": "https://arxiv.org/abs/2510.26103", "authors": ["Maximilian Schreiber", "Pascal Tippe"], "title": "Security Vulnerabilities in AI-Generated Code: A Large-Scale Analysis of Public GitHub Repositories", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Volume 16219 of the Lecture Notes in Computer Science series,\n  and is available online at https://doi.org/10.1007/978-981-95-3537-8_9", "summary": "This paper presents a comprehensive empirical analysis of security\nvulnerabilities in AI-generated code across public GitHub repositories. We\ncollected and analyzed 7,703 files explicitly attributed to four major AI\ntools: ChatGPT (91.52\\%), GitHub Copilot (7.50\\%), Amazon CodeWhisperer\n(0.52\\%), and Tabnine (0.46\\%). Using CodeQL static analysis, we identified\n4,241 Common Weakness Enumeration (CWE) instances across 77 distinct\nvulnerability types. Our findings reveal that while 87.9\\% of AI-generated code\ndoes not contain identifiable CWE-mapped vulnerabilities, significant patterns\nemerge regarding language-specific vulnerabilities and tool performance. Python\nconsistently exhibited higher vulnerability rates (16.18\\%-18.50\\%) compared to\nJavaScript (8.66\\%-8.99\\%) and TypeScript (2.50\\%-7.14\\%) across all tools. We\nobserved notable differences in security performance, with GitHub Copilot\nachieving better security density for Python (1,739 LOC per CWE) and\nTypeScript, while ChatGPT performed better for JavaScript. Additionally, we\ndiscovered widespread use of AI tools for documentation generation (39\\% of\ncollected files), an understudied application with implications for software\nmaintainability. These findings extend previous work with a significantly\nlarger dataset and provide valuable insights for developing language-specific\nand context-aware security practices for the responsible integration of\nAI-generated code into software development workflows.", "AI": {"tldr": "\u5bf9\u516c\u5f00 GitHub \u4ed3\u5e93\u4e2d AI \u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6f0f\u6d1e\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u6db5\u76d6 7,703 \u4efd\u4ee3\u7801\u3001\u6765\u81ea\u56db\u5927 AI \u5de5\u5177\u7684\u4ee3\u7801\uff0c\u5360\u6bd4\uff1aChatGPT 91.52%\u3001Copilot 7.50%\u3001CodeWhisperer 0.52%\u3001Tabnine 0.46%\uff0c\u901a\u8fc7 CodeQL \u9759\u6001\u5206\u6790\u8bc6\u522b 4,241 \u6761 CWE \u6f0f\u6d1e\uff0c\u6db5\u76d6 77 \u79cd\u7c7b\u578b\u3002\u7ed3\u679c\u663e\u793a 87.9% \u7684 AI \u751f\u6210\u4ee3\u7801\u672a\u68c0\u6d4b\u5230 CWE \u6f0f\u6d1e\uff0c\u4f46\u5728\u8bed\u8a00\u4e0e\u5de5\u5177\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1bPython \u6f0f\u6d1e\u7387\u6700\u9ad8\uff0cCopilot \u5728 Python/TypeScript \u4e0a\u8868\u73b0\u66f4\u597d\uff0cChatGPT \u5728 JavaScript \u4e0a\u8868\u73b0\u66f4\u4f18\uff1b\u5e76\u53d1\u73b0 39% \u7684\u6587\u4ef6\u7528\u4e8e\u81ea\u52a8\u5316\u6587\u6863\u751f\u6210\u3002", "motivation": "\u63ed\u793a AI \u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u98ce\u9669\uff0c\u652f\u6301\u9762\u5411\u8bed\u8a00\u53ca\u573a\u666f\u7684\u5b89\u5168\u5b9e\u8df5\uff0c\u6269\u5c55\u524d\u4eba\u7814\u7a76\u3001\u63d0\u4f9b\u5bf9 AI \u8f85\u52a9\u7f16\u7801\u7684\u5408\u89c4\u4e0e\u5b89\u5168\u5efa\u8bae\u3002", "method": "\u6536\u96c6 7,703 \u4efd\u6765\u81ea\u56db\u5927 AI \u5de5\u5177\u7684\u6587\u4ef6\uff0c\u4f7f\u7528 CodeQL \u9759\u6001\u5206\u6790\u8bc6\u522b CWE \u6f0f\u6d1e\uff0c\u8986\u76d6 77 \u79cd\u6f0f\u6d1e\u7c7b\u578b\uff0c\u6309\u8bed\u8a00\u4e0e\u5de5\u5177\u5bf9\u6f0f\u6d1e\u5bc6\u5ea6\u4e0e\u5206\u5e03\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5171\u8bc6\u522b 4,241 \u6761 CWE \u5b9e\u4f8b\uff0c87.9% \u7684\u4ee3\u7801\u672a\u68c0\u6d4b\u5230 CWE \u6f0f\u6d1e\uff1bPython \u6f0f\u6d1e\u7387\u4e3a 16.18%-18.50%\uff0cJavaScript \u4e3a 8.66%-8.99%\uff0cTypeScript \u4e3a 2.50%-7.14%\uff1bCopilot \u5728 Python\uff081,739 LOC/CWE\uff09 \u4e0e TypeScript \u4e0a\u5177\u6709\u66f4\u597d\u5b89\u5168\u5bc6\u5ea6\uff0cChatGPT \u5728 JavaScript \u4e0a\u8868\u73b0\u66f4\u597d\uff1b\u540c\u65f6\u53d1\u73b0 39% \u7684\u6587\u4ef6\u7528\u4e8e\u6587\u6863\u751f\u6210\u3002", "conclusion": "\u7ed3\u679c\u4e3a\u8bed\u8a00\u7279\u5b9a\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b89\u5168\u5b9e\u8df5\u63d0\u4f9b\u8bc1\u636e\uff0c\u5f3a\u8c03\u5728 AI \u4ee3\u7801\u751f\u6210\u7684\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u5b9e\u884c\u8d1f\u8d23\u4efb\u7684\u96c6\u6210\uff0c\u4e14\u8fd9\u4efd\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u672a\u6765\u7684\u7814\u7a76\u4e0e\u5b9e\u8df5\u3002"}}
{"id": "2510.26722", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26722", "abs": "https://arxiv.org/abs/2510.26722", "authors": ["Muhammad Faraz Ul Abrar", "Nicol\u00f2 Michelusi"], "title": "Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off", "comment": null, "summary": "Over-the-air (OTA) federated learning (FL) has been well recognized as a\nscalable paradigm that exploits the waveform superposition of the wireless\nmultiple-access channel to aggregate model updates in a single use. Existing\nOTA-FL designs largely enforce zero-bias model updates by either assuming\n\\emph{homogeneous} wireless conditions (equal path loss across devices) or\nforcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous}\nwireless scenarios, however, such designs are constrained by the weakest device\nand inflate the update variance. Moreover, prior analyses of biased OTA-FL\nlargely address convex objectives, while most modern AI models are highly\nnon-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient\ndescent (SGD) for general smooth non-convex objectives under wireless\nheterogeneity. We develop novel OTA-FL SGD updates that allow a structured,\ntime-invariant model bias while facilitating reduced variance updates. We\nderive a finite-time stationarity bound (expected time average squared gradient\nnorm) that explicitly reveals a bias-variance trade-off. To optimize this\ntrade-off, we pose a non-convex joint OTA power-control design and develop an\nefficient successive convex approximation (SCA) algorithm that requires only\nstatistical CSI at the base station. Experiments on a non-convex image\nclassification task validate the approach: the SCA-based design accelerates\nconvergence via an optimized bias and improves generalization over prior OTA-FL\nbaselines.", "AI": {"tldr": "OTA-FL with SGD for non-convex objectives under wireless heterogeneity uses a structured time-invariant bias to reduce variance, derives a finite-time stationarity bound, and solves a non-convex joint power-control problem via a successive convex approximation (SCA) using only statistical CSI, achieving faster convergence and better generalization on non-convex tasks.", "motivation": "Address the limitations of unbiased OTA-FL in heterogeneous wireless settings and the lack of non-convex analysis. We aim to enable scalable, convergent OTA-FL for modern non-convex models by controlling bias-variance through structured updates.", "method": "Introduce OTA-FL SGD updates with a designed, time-invariant model bias. Derive a finite-time stationarity bound that reveals bias-variance trade-off. Formulate a non-convex joint power-control problem and develop an efficient SCA algorithm requiring only statistical CSI at the base station.", "result": "Experimental validation on a non-convex image classification task shows that the SCA-based power-control design accelerates convergence by optimizing bias and yields improved generalization over existing OTA-FL baselines.", "conclusion": "A structured bias-variance trade-off mechanism for OTA-FL under wireless heterogeneity. The SCA-based power control leveraging only statistical CSI substantially improves convergence speed and generalization in non-convex settings."}}
{"id": "2510.25892", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25892", "abs": "https://arxiv.org/abs/2510.25892", "authors": ["Harris Hardiman-Mostow", "Jack Mauro", "Adrien Weihs", "Andrea L. Bertozzi"], "title": "Topology-Aware Active Learning on Graphs", "comment": null, "summary": "We propose a graph-topological approach to active learning that directly\ntargets the core challenge of exploration versus exploitation under scarce\nlabel budgets. To guide exploration, we introduce a coreset construction\nalgorithm based on Balanced Forman Curvature (BFC), which selects\nrepresentative initial labels that reflect the graph's cluster structure. This\nmethod includes a data-driven stopping criterion that signals when the graph\nhas been sufficiently explored. We further use BFC to dynamically trigger the\nshift from exploration to exploitation within active learning routines,\nreplacing hand-tuned heuristics. To improve exploitation, we introduce a\nlocalized graph rewiring strategy that efficiently incorporates multiscale\ninformation around labeled nodes, enhancing label propagation while preserving\nsparsity. Experiments on benchmark classification tasks show that our methods\nconsistently outperform existing graph-based semi-supervised baselines at low\nlabel rates.", "AI": {"tldr": "A curvature-guided graph-active-learning framework that uses Balanced Forman Curvature to select initial labels (coreset), determine when to stop exploring, and dynamically switch from exploration to exploitation, plus a localized graph rewiring to enhance label propagation; it yields superior performance at low label rates on benchmark tasks.", "motivation": "Address the scarcity of labeled data in graph-based active learning by designing principled, curvature-based mechanisms to balance exploration and exploitation, select informative initial labels, and efficiently propagate labels while preserving sparsity.", "method": "1) Build graph and construct a coreset of representative initial labels via Balanced Forman Curvature (BFC). 2) Introduce a data-driven stopping criterion indicating sufficient graph exploration. 3) Use BFC to dynamically trigger the shift from exploration to exploitation within active learning, replacing manual heuristics. 4) Propose a localized graph rewiring strategy to incorporate multiscale information around labeled nodes, improving label propagation while maintaining sparsity.", "result": "Empirical results on benchmark classification tasks show consistent outperformance of existing graph-based semi-supervised baselines at low label rates, demonstrating improved exploration, exploitation, and propagation efficiency.", "conclusion": "A principled, curvature-guided framework for graph-based active learning that achieves a better exploration\u2013exploitation balance and efficient label propagation under limited labeling budgets, with demonstrated gains on standard benchmarks."}}
{"id": "2510.26210", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26210", "abs": "https://arxiv.org/abs/2510.26210", "authors": ["Junlin Liu", "Zhaomeng Deng", "Ziming Wang", "Mengyu Yao", "Yifeng Cai", "Yutao Hu", "Ziqi Zhang", "Yao Guo", "Ding Li"], "title": "Who Moved My Transaction? Uncovering Post-Transaction Auditability Vulnerabilities in Modern Super Apps", "comment": "SaTS 2025 (Co-Located with ACM CCS 2025)", "summary": "Super apps are the cornerstones of modern digital life, embedding financial\ntransactions into nearly every aspect of daily routine. The prevailing security\nparadigm for these platforms is overwhelmingly focused on pre-transaction\nauthentication, preventing unauthorized payments before they occur. We argue\nthat a critical vulnerability vector has been largely overlooked: the fragility\nof post-transaction audit trails. We investigate the ease with which a user can\npermanently erase their transaction history from an app's interface, thereby\nconcealing unauthorized or sensitive activities from the account owner. To\nquantify this threat, we conducted an empirical study with 6 volunteers who\nperformed a cross-evaluation on six super apps. Our findings are alarming: all\nsix applications studied allow users to delete transaction records, yet a\nstaggering five out of six (83+\\%) fail to protect these records with strong\nauthentication. Only one app in our study required biometric verification for\ndeletion. This study provides the first concrete evidence of this\nnear-ubiquitous vulnerability, demonstrating a critical gap in the current\nmobile security landscape and underscoring the urgent need for a paradigm shift\ntowards ensuring post-transaction audit integrity.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u516d\u6b3e\u8d85\u7ea7\u5e94\u7528\u4e2d\u5220\u9664\u4ea4\u6613\u8bb0\u5f55\u7684\u80fd\u529b\u53ca\u5bf9\u5220\u9664\u64cd\u4f5c\u7684\u8ba4\u8bc1\u4e0d\u8db3\uff0c\u66b4\u9732\u4e86\u540e\u4ea4\u6613\u5ba1\u8ba1\u5b8c\u6574\u6027\u7684\u91cd\u8981\u6f0f\u6d1e\u3002", "motivation": "\u5f53\u524d\u5b89\u5168\u91cd\u70b9\u5728\u4e8e\u4ea4\u6613\u524d\u7684\u8eab\u4efd\u9a8c\u8bc1\uff0c\u5ffd\u7565\u4e86\u4ea4\u6613\u540e\u5bf9\u5ba1\u8ba1\u75d5\u8ff9\u7684\u4fdd\u62a4\uff1b\u82e5\u7528\u6237\u80fd\u6c38\u4e45\u5220\u9664\u4ea4\u6613\u8bb0\u5f55\uff0c\u53ef\u80fd\u9690\u85cf\u672a\u6388\u6743\u6216\u654f\u611f\u6d3b\u52a8\u3002", "method": "\u5bf96\u540d\u5fd7\u613f\u8005\u8fdb\u884c\u6a2a\u5411\u8bc4\u4f30\uff0c\u57286\u6b3e\u8d85\u7ea7\u5e94\u7528\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u5220\u9664\u4ea4\u6613\u8bb0\u5f55\u7684\u53ef\u884c\u6027\u53ca\u662f\u5426\u9700\u8981\u5f3a\u8ba4\u8bc1\u3002", "result": "\u6240\u6709\u5e94\u7528\u5747\u5141\u8bb8\u5220\u9664\u4ea4\u6613\u8bb0\u5f55\uff1b\u5176\u4e2d5/6\uff08\u7ea683%\uff09\u672a\u5bf9\u5220\u9664\u884c\u4e3a\u8fdb\u884c\u5f3a\u8ba4\u8bc1\uff1b\u4ec5\u67091\u6b3e\u5e94\u7528\u8981\u6c42\u751f\u7269\u8bc6\u522b\u9a8c\u8bc1\u624d\u53ef\u5220\u9664\u3002", "conclusion": "\u8fd9\u662f\u79fb\u52a8\u5b89\u5168\u9886\u57df\u4e2d\u4e00\u4e2a\u8fd1\u4e4e\u666e\u904d\u4f46\u88ab\u5ffd\u89c6\u7684\u6f0f\u6d1e\uff0c\u8868\u660e\u9700\u8981\u4ece\u6839\u672c\u4e0a\u5f3a\u5316\u540e\u4ea4\u6613\u5ba1\u8ba1\u7684\u5b8c\u6574\u6027\uff0c\u63a8\u52a8\u5b89\u5168 paradigm \u8f6c\u5411\u5bf9\u5ba1\u8ba1\u8f68\u8ff9\u7684\u4fdd\u62a4\u3002"}}
{"id": "2510.25924", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.25924", "abs": "https://arxiv.org/abs/2510.25924", "authors": ["Manuel Iglesias-Alonso", "Felix Schur", "Julius von K\u00fcgelgen", "Jonas Peters"], "title": "Transferring Causal Effects using Proxies", "comment": "Advances in Neural Information Processing Systems (NeurIPS 2025)\n  camera-ready version", "summary": "We consider the problem of estimating a causal effect in a multi-domain\nsetting. The causal effect of interest is confounded by an unobserved\nconfounder and can change between the different domains. We assume that we have\naccess to a proxy of the hidden confounder and that all variables are discrete\nor categorical. We propose methodology to estimate the causal effect in the\ntarget domain, where we assume to observe only the proxy variable. Under these\nconditions, we prove identifiability (even when treatment and response\nvariables are continuous). We introduce two estimation techniques, prove\nconsistency, and derive confidence intervals. The theoretical results are\nsupported by simulation studies and a real-world example studying the causal\neffect of website rankings on consumer choices.", "AI": {"tldr": "The paper studies causal effect estimation in a multi-domain setting with unobserved confounding, using a proxy for the hidden confounder. It proves identifiability and proposes two estimation methods that are consistent and enable confidence intervals, applicable even when treatment and outcome are continuous, with validation via simulations and a real-world website-ranking example.", "motivation": "Address the challenge of estimating causal effects when unobserved confounding differs across domains. Proposes a framework that leverages a proxy for the hidden confounder to achieve identifiability in the target domain where only the proxy is observed.", "method": "Model the relationship between treatment, outcome, proxy confounder, and domain, assuming discrete/categorical variables. Propose two estimation techniques (details not given in abstract), prove consistency for both, and construct confidence intervals. Use domain-shift assumptions to transfer identifiability to the target domain and handle continuous treatment/outcome.", "result": "Estimation techniques are shown to be consistent; identifiability of the causal effect established in the target domain under proxy confounding even when treatment and response are continuous; simulation studies and a real-world application (website rankings affecting consumer choices) validate the approach.", "conclusion": "The framework enables identifiable causal inference across domains with a proxy confounder, provides consistent estimators and uncertainty quantification, and is supported by simulations and a real dataset, highlighting the practical relevance for studying cross-domain causal effects."}}
{"id": "2510.25926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25926", "abs": "https://arxiv.org/abs/2510.25926", "authors": ["Kianoosh Ashouritaklimi", "Tom Rainforth"], "title": "Active Learning with Task-Driven Representations for Messy Pools", "comment": null, "summary": "Active learning has the potential to be especially useful for messy,\nuncurated pools where datapoints vary in relevance to the target task. However,\nstate-of-the-art approaches to this problem currently rely on using fixed,\nunsupervised representations of the pool, focusing on modifying the acquisition\nfunction instead. We show that this model setup can undermine their\neffectiveness at dealing with messy pools, as such representations can fail to\ncapture important information relevant to the task. To address this, we propose\nusing task-driven representations that are periodically updated during the\nactive learning process using the previously collected labels. We introduce two\nspecific strategies for learning these representations, one based on directly\nlearning semi-supervised representations and the other based on supervised\nfine-tuning of an initial unsupervised representation. We find that both\nsignificantly improve empirical performance over using unsupervised or\npretrained representations.", "AI": {"tldr": "\u5728\u4e3b\u52a8\u5b66\u4e60\u4e2d\uff0c\u56fa\u5b9a\u7684\u65e0\u76d1\u7763\u6c60\u8868\u793a\u5728\u5904\u7406\u6df7\u4e71\u6570\u636e\u6c60\u65f6\u6548\u679c\u6709\u9650\uff0c\u6587\u732e\u63d0\u51fa\u57fa\u4e8e\u4efb\u52a1\u9a71\u52a8\u7684\u8868\u793a\u5e76\u5468\u671f\u6027\u66f4\u65b0\uff0c\u7ed9\u51fa\u4e24\u79cd\u7b56\u7565\uff08\u534a\u76d1\u7763\u8868\u793a\u5b66\u4e60\u4e0e\u6709\u76d1\u7763\u5fae\u8c03\uff09\uff0c\u663e\u8457\u63d0\u5347\u76f8\u5bf9\u65e0\u76d1\u7763/\u9884\u8bad\u7ec3\u8868\u793a\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u6df7\u4e71\u6570\u636e\u6c60\u5bf9\u4e3b\u52a8\u5b66\u4e60\u7684\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff1b\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u7684\u65e0\u76d1\u7763\u8868\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u91cd\u8981\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5728\u4e3b\u52a8\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5468\u671f\u6027\u7528\u5df2\u6807\u6ce8\u6570\u636e\u66f4\u65b0\u4efb\u52a1\u9a71\u52a8\u8868\u793a\u7684\u6846\u67b6\u3002\u5177\u4f53\u5b9e\u73b0\u5305\u62ec\uff1a1) \u76f4\u63a5\u5b66\u4e60\u534a\u76d1\u7763\u8868\u793a\uff1b2) \u5728\u521d\u59cb\u65e0\u76d1\u7763\u8868\u793a\u57fa\u7840\u4e0a\u8fdb\u884c\u6709\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e24\u79cd\u4efb\u52a1\u9a71\u52a8\u8868\u793a\u7b56\u7565\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u56fa\u5b9a\u65e0\u76d1\u7763\u6216\u9884\u8bad\u7ec3\u8868\u793a\u7684\u57fa\u7ebf\u3002", "conclusion": "\u5f15\u5165\u4efb\u52a1\u9a71\u52a8\u7684\u8868\u793a\u5b66\u4e60\u80fd\u63d0\u5347\u4e3b\u52a8\u5b66\u4e60\u5728\u6df7\u4e71\u6570\u636e\u6c60\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6548\u80fd\uff0c\u63d0\u793a\u672a\u6765\u5728\u4e3b\u52a8\u5b66\u4e60\u8bbe\u8ba1\u4e2d\u5e94\u5c06\u8868\u793a\u5b66\u4e60\u4e0e\u83b7\u53d6\u6807\u6ce8\u7d27\u5bc6\u7ed3\u5408\u3002"}}
{"id": "2510.26274", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26274", "abs": "https://arxiv.org/abs/2510.26274", "authors": ["Haohua Duan", "Liyao Xiang", "Xin Zhang"], "title": "PVMark: Enabling Public Verifiability for LLM Watermarking Schemes", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Watermarking schemes for large language models (LLMs) have been proposed to\nidentify the source of the generated text, mitigating the potential threats\nemerged from model theft. However, current watermarking solutions hardly\nresolve the trust issue: the non-public watermark detection cannot prove itself\nfaithfully conducting the detection. We observe that it is attributed to the\nsecret key mostly used in the watermark detection -- it cannot be public, or\nthe adversary may launch removal attacks provided the key; nor can it be\nprivate, or the watermarking detection is opaque to the public. To resolve the\ndilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),\nenabling the watermark detection process to be publicly verifiable by third\nparties without disclosing any secret key. PVMark hinges upon the proof of\n`correct execution' of watermark detection on which a set of ZKP constraints\nare built, including mapping, random number generation, comparison, and\nsummation. We implement multiple variants of PVMark in Python, Rust and Circom,\ncovering combinations of three watermarking schemes, three hash functions, and\nfour ZKP protocols, to show our approach effectively works under a variety of\ncircumstances. By experimental results, PVMark efficiently enables public\nverifiability on the state-of-the-art LLM watermarking schemes yet without\ncompromising the watermarking performance, promising to be deployed in\npractice.", "AI": {"tldr": "PVMark\u901a\u8fc7\u96f6\u77e5\u8bc6\u8bc1\u660e\u5b9e\u73b0\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u68c0\u6d4b\u7684\u516c\u94a5\u53ef\u9a8c\u8bc1\u6027\uff0c\u4f7f\u7b2c\u4e09\u65b9\u80fd\u591f\u5728\u4e0d\u66b4\u9732\u79d8\u5bc6\u5bc6\u94a5\u7684\u60c5\u51b5\u4e0b\u9a8c\u8bc1\u68c0\u6d4b\u6b63\u786e\u6027\uff1b\u5e76\u5728\u591a\u8bed\u8a00\u3001\u591a\u6c34\u5370\u65b9\u6848\u3001\u591a\u54c8\u5e0c\u51fd\u6570\u53ca\u591aZKP\u534f\u8bae\u4e0b\u5b9e\u73b0\uff0c\u663e\u793a\u5728\u4e0d\u5f71\u54cd\u6c34\u5370\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5177\u5907\u6709\u6548\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6848\u7684\u68c0\u6d4b\u591a\u4f9d\u8d56\u79d8\u5bc6\u5bc6\u94a5\uff0c\u5bfc\u81f4\u975e\u516c\u5f00\u68c0\u6d4b\u96be\u4ee5\u88ab\u5916\u90e8\u8bc1\u660e\u5176\u771f\u5b9e\u6027\uff0c\u5b58\u5728\u4fe1\u4efb\u7f3a\u53e3\uff1b\u9700\u8981\u4e00\u4e2a\u516c\u5f00\u3001\u53ef\u9a8c\u8bc1\u7684\u68c0\u6d4b\u8fc7\u7a0b\u800c\u53c8\u4e0d\u6cc4\u9732\u5bc6\u94a5\u7684\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u201c\u68c0\u6d4b\u6b63\u786e\u6267\u884c\u201d\u8bc1\u660e\uff0c\u5c06\u6c34\u5370\u68c0\u6d4b\u7684\u6620\u5c04\u3001\u968f\u673a\u6570\u751f\u6210\u3001\u6bd4\u8f83\u4e0e\u6c42\u548c\u7b49\u8fc7\u7a0b\u8f6c\u5316\u4e3aZKP\u7ea6\u675f\uff1b\u5b9e\u73b0\u591a\u79cd\u7ec4\u5408\uff1a\u4e09\u79cd\u6c34\u5370\u65b9\u6848\u3001\u4e09\u79cd\u54c8\u5e0c\u51fd\u6570\u3001\u56db\u79cdZKP\u534f\u8bae\uff1b\u5728Python\u3001Rust\u3001Circom\u7b49\u5b9e\u73b0\u4ee5\u8986\u76d6\u5e7f\u6cdb\u573a\u666f\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\uff0cPVMark\u5b9e\u73b0\u4e86\u5bf9\u73b0\u6709\u6700\u524d\u6cbf\u6c34\u5370\u65b9\u6848\u7684\u516c\u5f00\u53ef\u9a8c\u8bc1\u6027\uff0c\u540c\u65f6\u672a\u663e\u8457\u964d\u4f4e\u6c34\u5370\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u53ef\u884c\u6027\u4e0e\u6709\u6548\u6027\u3002", "conclusion": "PVMark\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u4e0d\u6cc4\u9732\u5bc6\u94a5\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u6c34\u5370\u68c0\u6d4b\u516c\u5f00\u53ef\u9a8c\u8bc1\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u6c34\u5370\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u4e0e\u4fe1\u4efb\u5ea6\uff0c\u5177\u5907\u843d\u5730\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.25952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25952", "abs": "https://arxiv.org/abs/2510.25952", "authors": ["Tcharlies Schmitz"], "title": "Modular Linear Tokenization (MLT)", "comment": null, "summary": "This paper introduces Modular Linear Tokenization (MLT), a reversible and\ndeterministic technique for encoding high-cardinality categorical identifiers\ninto compact numerical vectors. Unlike traditional hashing or one-hot\nencodings, MLT preserves bijective mappings by leveraging modular arithmetic\nover finite fields and invertible linear transformations. The method offers\nexplicit control of dimensionality and computational scalability while\nmaintaining full reversibility, even for millions of identifiers. Experimental\nresults on the MovieLens 20M dataset show that MLT achieves comparable\npredictive performance to supervised embeddings while requiring significantly\nfewer parameters and lower training cost. An open-source implementation of MLT\nis available on PyPI (https://pypi.org/project/light-mlt/) and GitHub\n(https://github.com/tcharliesschmitz/light-mlt).", "AI": {"tldr": "Introduces Modular Linear Tokenization (MLT), a reversible, deterministic encoding for high-cardinality identifiers into compact vectors using modular arithmetic and invertible linear transforms, enabling controlled dimensionality and scalability with full reversibility.", "motivation": "Address inefficiencies of traditional hashing/one-hot for high-cardinality categorical data; need bijective, size-controllable representations that are scalable for millions of identifiers.", "method": "Encode identifiers via modular arithmetic over finite fields and invertible linear transformations to produce bijective compact vectors; deterministic, reversible mappings; explicit dimensionality control and scalability.", "result": "On MovieLens 20M, MLT achieves predictive performance comparable to supervised embeddings while using far fewer parameters and lower training cost.", "conclusion": "MLT provides a practical, reversible encoding with scalable parameter efficiency; open-source implementation available (PyPI, GitHub)."}}
{"id": "2510.26420", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26420", "abs": "https://arxiv.org/abs/2510.26420", "authors": ["Yingjia Wang", "Ting Qiao", "Xing Liu", "Chongzuo Li", "Sixing Wu", "Jianbin Li"], "title": "SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification", "comment": "8 pages,9 figures", "summary": "The rapid advancement of deep neural networks (DNNs) heavily relies on\nlarge-scale, high-quality datasets. However, unauthorized commercial use of\nthese datasets severely violates the intellectual property rights of dataset\nowners. Existing backdoor-based dataset ownership verification methods suffer\nfrom inherent limitations: poison-label watermarks are easily detectable due to\nlabel inconsistencies, while clean-label watermarks face high technical\ncomplexity and failure on high-resolution images. Moreover, both approaches\nemploy static watermark patterns that are vulnerable to detection and removal.\nTo address these issues, this paper proposes a sample-specific clean-label\nbackdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked\nsample generator, this method generates unique watermarks for each sample,\nfundamentally overcoming the vulnerability of static watermark patterns. The\ncore innovation lies in designing a composite loss function with three\ncomponents: target sample loss ensures watermark effectiveness, non-target\nsample loss guarantees trigger reliability, and perceptual similarity loss\nmaintains visual imperceptibility. During ownership verification, black-box\ntesting is employed to check whether suspicious models exhibit predefined\nbackdoor behaviors. Extensive experiments on benchmark datasets demonstrate the\neffectiveness of the proposed method and its robustness against potential\nwatermark removal attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u7279\u5b9a\u7684\u5e72\u51c0\u6807\u7b7e\u540e\u95e8\u6c34\u5370\uff08SSCL-BW\uff09\uff0c\u901a\u8fc7U-Net\u751f\u6210\u6bcf\u4e2a\u6837\u672c\u7684\u4e13\u5c5e\u6c34\u5370\uff0c\u4f7f\u7528\u4e09\u9879\u635f\u5931\u7684\u7efc\u5408\u76ee\u6807\u51fd\u6570\u5b9e\u73b0\u6709\u6548\u6027\u3001\u89e6\u53d1\u6027\u4e0e\u4e0d\u53ef\u5bdf\u89c9\u6027\uff0c\u5e76\u5728\u9ed1\u7bb1\u6d4b\u8bd5\u4e0b\u8fdb\u884c\u6240\u6709\u6743\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u5bf9\u6c34\u5370\u53bb\u9664\u653b\u51fb\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u540e\u95e8\u6c34\u5370\u5728\u6807\u7b7e\u4e0d\u4e00\u81f4\u5bfc\u81f4\u6bd2\u6807\u7b7e\u6c34\u5370\u6613\u88ab\u68c0\u6d4b\u3001\u6e05\u6d17\u6807\u7b7e\u6c34\u5370\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u5b9e\u73b0\u56f0\u96be\uff0c\u4ee5\u53ca\u9759\u6001\u6c34\u5370\u6a21\u5f0f\u6613\u88ab\u68c0\u6d4b/\u79fb\u9664\u7684\u95ee\u9898\u3002\u63d0\u51fa\u6837\u672c\u7279\u5b9a\u6c34\u5370\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2aU-Net\u6c34\u5370\u6837\u672c\u751f\u6210\u5668\uff0c\u4e3a\u6bcf\u4e2a\u6837\u672c\u751f\u6210\u72ec\u7279\u6c34\u5370\u3002\u8bbe\u8ba1\u4e09\u7ec4\u6210\u5206\u7684\u590d\u5408\u635f\u5931\uff1a\u76ee\u6807\u6837\u672c\u635f\u5931\u786e\u4fdd\u6c34\u5370\u6709\u6548\uff0c\u975e\u76ee\u6807\u6837\u672c\u635f\u5931\u4fdd\u8bc1\u89e6\u53d1\u53ef\u9760\u6027\uff0c\u611f\u77e5\u76f8\u4f3c\u6027\u635f\u5931\u4fdd\u6301\u89c6\u89c9\u4e0d\u53ef\u5bdf\u89c9\u3002\u901a\u8fc7\u9ed1\u7bb1\u6d4b\u8bd5\u8fdb\u884c\u6240\u6709\u6743\u9a8c\u8bc1\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5c55\u793a\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4ee5\u53ca\u5bf9\u6f5c\u5728\u6c34\u5370\u79fb\u9664\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6837\u672c\u7279\u5b9a\u7684\u6e05\u6d01\u6807\u7b7e\u6c34\u5370\u514b\u670d\u4e86\u9759\u6001\u6c34\u5370\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u6c34\u5370\u7684\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u5b9e\u73b0\u590d\u6742\u5ea6\u8f83\u9ad8\u4e14\u9700\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8fdb\u4e00\u6b65\u8bc4\u4f30\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.25954", "categories": ["cs.LG", "cs.AI", "J.3"], "pdf": "https://arxiv.org/pdf/2510.25954", "abs": "https://arxiv.org/abs/2510.25954", "authors": ["Lynn Metz", "Rachel Haggard", "Michael Moszczynski", "Samer Asbah", "Chris Mwase", "Patricia Khomani", "Tyler Smith", "Hannah Cooper", "Annie Mwale", "Arbaaz Muslim", "Gautam Prasad", "Mimi Sun", "Tomer Shekel", "Joydeep Paul", "Anna Carter", "Shravya Shetty", "Dylan Green"], "title": "Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi", "comment": "13 pages, 3010 words, 2 tables, 2 figures", "summary": "The reliability of routine health data in low and middle-income countries\n(LMICs) is often constrained by reporting delays and incomplete coverage,\nnecessitating the exploration of novel data sources and analytics. Geospatial\nFoundation Models (GeoFMs) offer a promising avenue by synthesizing diverse\nspatial, temporal, and behavioral data into mathematical embeddings that can be\nefficiently used for downstream prediction tasks. This study evaluated the\npredictive performance of three GeoFM embedding sources - Google Population\nDynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite\nimagery), and mobile phone call detail records (CDR) - for modeling 15 routine\nhealth programmatic outputs in Malawi, and compared their utility to\ntraditional geospatial interpolation methods. We used XGBoost models on data\nfrom 552 health catchment areas (January 2021-May 2023), assessing performance\nwith R2, and using an 80/20 training and test data split with 5-fold\ncross-validation used in training. While predictive performance was mixed, the\nembedding-based approaches improved upon baseline geostatistical methods in 13\nof 15 (87%) indicators tested. A Multi-GeoFM model integrating all three\nembedding sources produced the most robust predictions, achieving average\n5-fold cross validated R2 values for indicators like population density (0.63),\nnew HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,\n0.68, and 0.55, respectively. Prediction was poor for prediction targets with\nlow primary data availability, such as TB and malnutrition cases. These results\ndemonstrate that GeoFM embeddings imbue a modest predictive improvement for\nselect health and demographic outcomes in an LMIC context. We conclude that the\nintegration of multiple GeoFM sources is an efficient and valuable tool for\nsupplementing and strengthening constrained routine health information systems.", "AI": {"tldr": "GeoFM embeddings from multiple sources (Google PDFM, AlphaEarth, and CDR) modestly improve prediction of routine health indicators in Malawi; best performance for population density, new HIV cases, and child vaccinations; poor for data-sparse targets like TB and malnutrition; a Multi-GeoFM model yielded the strongest predictions, suggesting value in augmenting constrained health information systems.", "motivation": "In LMICs, routine health data suffer from delays and incomplete coverage. There is a need to explore novel data sources and analytics, such as geospatial foundation models (GeoFMs), to augment traditional methods and improve timely, accurate health predictions.", "method": "XGBoost models trained on 552 health catchment areas in Malawi using embedding features from three GeoFMs (Google PDFM, Google AlphaEarth, CDR). Data split 80/20 with 5-fold cross-validation. Compared against traditional geostatistical methods.", "result": "Embedding-based approaches outperformed baseline geostatistical methods in 13 of 15 indicators (87%). A Multi-GeoFM model combining all three sources yielded the strongest performance, with cross-validated R2 values ~0.63 for population density, ~0.57 for new HIV cases, and ~0.47 for child vaccinations; test-set R2 values were ~0.64, ~0.68, and ~0.55 respectively. Prediction was poorer for targets with limited primary data (e.g., TB and malnutrition).", "conclusion": "Integrating multiple GeoFM sources can efficiently enhance predictive accuracy for select health and demographic outcomes in LMICs, offering a valuable supplement to constrained routine health information systems."}}
{"id": "2510.26523", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26523", "abs": "https://arxiv.org/abs/2510.26523", "authors": ["Shuaishuai Liu", "Gergely Acs", "Gergely Bicz\u00f3k"], "title": "Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy Policies", "comment": "18 pages, 2 figures", "summary": "Smart home devices such as video doorbells and security cameras are becoming\nincreasingly common in everyday life. While these devices offer convenience and\nsafety, they also raise new privacy concerns: how these devices affect others,\nlike neighbors, visitors, or people passing by. This issue is generally known\nas interdependent privacy, where one person's actions (or inaction) may impact\nthe privacy of others, and, specifically, bystander privacy in the context of\nsmart homes. Given lax data protection regulations in terms of shared physical\nspaces and amateur joint data controllers, we expect that the privacy policies\nof smart home products reflect the missing regulatory incentives. This paper\npresents a focused privacy policy analysis of 20 video doorbell and smart\ncamera products, concentrating explicitly on the bystander aspect. We show that\nalthough some of the vendors acknowledge bystanders, they address it only to\nthe extent of including disclaimers, shifting the ethical responsibility for\ncollecting the data of non-users to the device owner. In addition, we identify\nand examine real-world cases related to bystander privacy, demonstrating how\ncurrent deployments can impact non-users. Based on our findings, we analyze\nvendor privacy policies in light of existing legal frameworks and technical\ncapabilities, and we provide practical recommendations for both policy language\nand system design to enhance transparency and empower both bystanders and\ndevice owners.", "AI": {"tldr": "\u5bf920\u6b3e\u89c6\u9891\u95e8\u94c3/\u667a\u80fd\u6444\u50cf\u5934\u7684\u9690\u79c1\u653f\u7b56\u8fdb\u884c\u805a\u7126\u5206\u6790\uff0c\u805a\u7126\u65c1\u89c2\u8005\u9690\u79c1\uff08bystander privacy\uff09\u3002\u7ed3\u679c\u663e\u793a\u5382\u5546\u591a\u4ee5\u514d\u8d23\u58f0\u660e\u5f62\u5f0f\u627f\u8ba4\u65c1\u89c2\u8005\u5b58\u5728\uff0c\u4f46\u5c06\u6570\u636e\u6536\u96c6\u7684\u4f26\u7406\u8d23\u4efb\u8f6c\u79fb\u7ed9\u8bbe\u5907\u62e5\u6709\u8005\uff0c\u653f\u7b56\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u5b58\u5728\u7f3a\u53e3\uff0c\u5e76\u7ed9\u51fa\u9762\u5411\u653f\u7b56\u8bed\u8a00\u548c\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u5728\u5171\u4eab\u7269\u7406\u7a7a\u95f4\u548c\u6570\u636e\u4e3b\u4f53\u4e0d\u5bf9\u79f0\u7684\u80cc\u666f\u4e0b\uff0c\u667a\u80fd\u5bb6\u5c45\u8bbe\u5907\u5bf9\u65c1\u89c2\u8005\u9690\u79c1\u7684\u5f71\u54cd\u65e5\u76ca\u7a81\u51fa\u3002\u73b0\u6709\u6cd5\u89c4\u5bf9\u8de8\u4e3b\u4f53\u3001\u8de8\u8bbe\u5907\u7684\u6570\u636e\u63a7\u5236\u7f3a\u4e4f\u6fc0\u52b1\uff0c\u5382\u5546\u9690\u79c1\u653f\u7b56\u5f80\u5f80\u672a\u5145\u5206\u8986\u76d6\u65c1\u89c2\u8005\u7ef4\u5ea6\uff0c\u9700\u901a\u8fc7\u5206\u6790\u653f\u7b56\u4e0e\u5b9e\u9645\u6848\u4f8b\uff0c\u63d0\u5347\u900f\u660e\u5ea6\u5e76\u4e3a\u6539\u8fdb\u63d0\u4f9b\u65b9\u5411\u3002", "method": "\u5bf920\u6b3e\u89c6\u9891\u95e8\u94c3\u4e0e\u667a\u80fd\u6444\u50cf\u5934\u4ea7\u54c1\u7684\u9690\u79c1\u653f\u7b56\u8fdb\u884c\u805a\u7126\u5206\u6790\uff0c\u8bc4\u4f30\u5176\u4e2d\u5bf9\u65c1\u89c2\u8005\u9690\u79c1\u7684\u8868\u8ff0\u4e0e\u8986\u76d6\u7a0b\u5ea6\uff1b\u7ed3\u5408\u771f\u5b9e\u4e16\u754c\u6848\u4f8b\uff0c\u68c0\u89c6\u5176\u5bf9\u975e\u7528\u6237\u7684\u6f5c\u5728\u5f71\u54cd\uff1b\u5c06\u5382\u5546\u653f\u7b56\u4e0e\u73b0\u6709\u6cd5\u5f8b\u6846\u67b6\u53ca\u6280\u672f\u80fd\u529b\u8fdb\u884c\u5bf9\u7167\uff0c\u5e76\u63d0\u51fa\u9762\u5411\u653f\u7b56\u4e0e\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6539\u8fdb\u5efa\u8bae\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u90e8\u5206\u5382\u5546\u627f\u8ba4\u65c1\u89c2\u8005\u5b58\u5728\uff0c\u4f46\u5927\u591a\u4ec5\u4ee5\u514d\u8d23\u58f0\u660e\u5f62\u5f0f\u63d0\u53ca\uff0c\u5c06\u5bf9\u975e\u7528\u6237\u6570\u636e\u7684\u6536\u96c6\u4f26\u7406\u8d23\u4efb\u8f6c\u79fb\u7ed9\u8bbe\u5907\u62e5\u6709\u8005\uff1b2) \u653f\u7b56\u8986\u76d6\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u5bf9\u65c1\u89c2\u8005\u6743\u5229\u3001\u901a\u77e5\u3001\u6570\u636e\u6700\u5c0f\u5316\u4e0e\u900f\u660e\u5ea6\u7684\u660e\u786e\u8981\u6c42\uff1b3) \u5b58\u5728\u82e5\u5e72\u771f\u5b9e\u6848\u4f8b\uff0c\u663e\u793a\u90e8\u7f72\u4f1a\u5bf9\u975e\u7528\u6237\u9020\u6210\u9690\u79c1\u5f71\u54cd\uff1b4) \u5c06\u5382\u5546\u653f\u7b56\u7f6e\u4e8e\u73b0\u6709\u6cd5\u5f8b\u6846\u67b6\u4e0e\u6280\u672f\u80fd\u529b\u4e4b\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u63d0\u51fa\u6539\u8fdb\u7b56\u7565\u4e0e\u7cfb\u7edf\u8bbe\u8ba1\u539f\u5219\u3002", "conclusion": "\u63d0\u9ad8\u900f\u660e\u5ea6\u4e0e\u8d4b\u6743\uff1a\u5efa\u8bae\u5b8c\u5584\u653f\u7b56\u8bed\u8a00\u4ee5\u660e\u793a\u65c1\u89c2\u8005\u6743\u5229\u4e0e\u8bbe\u5907\u4e3b\u63a7\u65b9\u7684\u8d23\u4efb\u8fb9\u754c\uff0c\u5f3a\u5316\u6570\u636e\u6700\u5c0f\u5316\u3001\u901a\u77e5\u4e0e\u540c\u610f\u673a\u5236\uff0c\u5e76\u5728\u8bbe\u5907\u8bbe\u8ba1\u5c42\u9762\u5f15\u5165\u65c1\u89c2\u8005\u4fdd\u62a4\u673a\u5236\uff08\u5982\u533a\u57df\u6807\u793a\u3001\u8bbf\u95ee\u63a7\u5236\u3001\u6570\u636e\u8bbf\u95ee\u5ba1\u8ba1\u3001\u53ef\u64a4\u9500\u7684\u76d1\u63a7\u8303\u56f4\u7b49\uff09\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u65c1\u89c2\u8005\u4e0e\u8bbe\u5907\u62e5\u6709\u8005\u7684\u53cc\u5411\u4fdd\u62a4\u3002"}}
{"id": "2510.26555", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26555", "abs": "https://arxiv.org/abs/2510.26555", "authors": ["Chunyi Zhang", "Jin Zeng", "Xiaoqi Li"], "title": "A Comprehensive Evaluation and Practice of System Penetration Testing", "comment": null, "summary": "With the rapid advancement of information technology, the complexity of\napplications continues to increase, and the cybersecurity challenges we face\nare also escalating. This paper aims to investigate the methods and practices\nof system security penetration testing, exploring how to enhance system\nsecurity through systematic penetration testing processes and technical\napproaches. It also examines existing penetration tools, analyzing their\nstrengths, weaknesses, and applicable domains to guide penetration testers in\ntool selection. Furthermore, based on the penetration testing process outlined\nin this paper, appropriate tools are selected to replicate attack processes\nusing target ranges and target machines. Finally, through practical case\nanalysis, lessons learned from successful attacks are summarized to inform\nfuture research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u7cfb\u7edf\u5316\u6e17\u900f\u6d4b\u8bd5\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\uff0c\u5f3a\u8c03\u6d41\u7a0b\u3001\u5de5\u5177\u8bc4\u4f30\u53ca\u5728\u5b9e\u6218\u4e2d\u7684\u843d\u5730\u5e94\u7528\uff0c\u901a\u8fc7\u6848\u4f8b\u603b\u7ed3\u7ecf\u9a8c\u6559\u8bad\u3002", "motivation": "\u5728\u4fe1\u606f\u5316\u4e0d\u65ad\u53d1\u5c55\u3001\u5a01\u80c1\u65e5\u76ca\u590d\u6742\u7684\u80cc\u666f\u4e0b\uff0c\u7cfb\u7edf\u5316\u7684\u6e17\u900f\u6d4b\u8bd5\u6709\u52a9\u4e8e\u53d1\u73b0\u4e0e\u4fee\u590d\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u5347\u9632\u5fa1\u80fd\u529b\u3002", "method": "\u5bf9\u6e17\u900f\u6d4b\u8bd5\u8fc7\u7a0b\u7684\u5404\u9636\u6bb5\u8fdb\u884c\u68b3\u7406\uff0c\u8bc4\u4f30\u73b0\u6709\u6e17\u900f\u5de5\u5177\u7684\u4f18\u52a3\u53ca\u9002\u7528\u9886\u57df\uff0c\u57fa\u4e8e\u6d41\u7a0b\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\uff0c\u5728\u76ee\u6807\u73af\u5883\u4e2d\u590d\u73b0\u5b9e\u9a8c\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u5206\u6790\u63d0\u70bc\u53ef\u64cd\u4f5c\u7684\u7ecf\u9a8c\u6559\u8bad\u3002", "result": "\u660e\u786e\u4e86\u5de5\u5177\u9009\u62e9\u7684\u7ef4\u5ea6\u3001\u6d41\u7a0b\u9a71\u52a8\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3001\u4ee5\u53ca\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u5de5\u5177\u9002\u7528\u6027\u3002\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u7684\u5bf9\u6bd4\uff0c\u603b\u7ed3\u51fa\u67d0\u4e9b\u653b\u51fb\u624b\u6bb5\u7684\u6210\u529f\u8981\u4ef6\u4e0e\u9632\u5fa1\u8981\u70b9\u3002", "conclusion": "\u6e17\u900f\u6d4b\u8bd5\u662f\u4fe1\u606f\u5b89\u5168\u4e2d\u7684\u524d\u77bb\u6027\u9632\u5fa1\u624b\u6bb5\uff0c\u9700\u5728\u89c4\u8303\u6d41\u7a0b\u3001\u5de5\u5177\u7ec4\u5408\u4e0e\u6301\u7eed\u5b66\u4e60\u65b9\u9762\u6301\u7eed\u53d1\u5c55\uff1b\u672a\u6765\u7814\u7a76\u53ef\u805a\u7126\u81ea\u52a8\u5316\u3001\u573a\u666f\u5316\u6d4b\u8bd5\u4e0e\u6301\u7eed\u98ce\u9669\u8bc4\u4f30\u3002"}}
{"id": "2510.26610", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26610", "abs": "https://arxiv.org/abs/2510.26610", "authors": ["Weixuan Chen", "Qianqian Yang"], "title": "A DRL-Empowered Multi-Level Jamming Approach for Secure Semantic Communication", "comment": null, "summary": "Semantic communication (SemCom) aims to transmit only task-relevant\ninformation, thereby improving communication efficiency but also exposing\nsemantic information to potential eavesdropping. In this paper, we propose a\ndeep reinforcement learning (DRL)-empowered multi-level jamming approach to\nenhance the security of SemCom systems over MIMO fading wiretap channels. This\napproach combines semantic layer jamming, achieved by encoding task-irrelevant\ntext, and physical layer jamming, achieved by encoding random Gaussian noise.\nThese two-level jamming signals are superposed with task-relevant semantic\ninformation to protect the transmitted semantics from eavesdropping. A deep\ndeterministic policy gradient (DDPG) algorithm is further introduced to\ndynamically design and optimize the precoding matrices for both taskrelevant\nsemantic information and multi-level jamming signals, aiming to enhance the\nlegitimate user's image reconstruction while degrading the eavesdropper's\nperformance. To jointly train the SemCom model and the DDPG agent, we propose\nan alternating optimization strategy where the two modules are updated\niteratively. Experimental results demonstrate that, compared with both the\nencryption-based (ESCS) and encoded jammer-based (EJ) benchmarks, our method\nachieves comparable security while improving the legitimate user's peak\nsignalto-noise ratio (PSNR) by up to approximately 0.6 dB.", "AI": {"tldr": "DRL-enabled multi-level jamming for SemCom security over MIMO fading wiretap channels; combines semantic-layer and physical-layer jamming; uses DDPG to optimize precoding; alternating training; comparable security to ESCS/EJ with PSNR gain ~0.6 dB.", "motivation": "Semantic communications improve efficiency but raise privacy risk as semantics can be eavesdropped; need secure SemCom that preserves task performance while protecting semantic information.", "method": "Two-level jamming: semantic-layer by encoding task-irrelevant text; physical-layer by encoding Gaussian noise; superposed with task-relevant semantic information. A DDPG agent designs precoding matrices for semantic information and jamming signals. An alternating optimization scheme trains SemCom and DRL modules iteratively.", "result": "Experimental results show security comparable to encryption-based ESCS and encoded jammer EJ benchmarks, with the legitimate user\u2019s PSNR improved by up to ~0.6 dB.", "conclusion": "The framework delivers effective semantic security with dynamic, DRL-driven precoding and dual-level jamming, preserving or enhancing legitimate performance while hindering eavesdroppers."}}
{"id": "2510.26620", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.26620", "abs": "https://arxiv.org/abs/2510.26620", "authors": ["Nicholas Pecka", "Lotfi Ben Othmane", "Renee Bryce"], "title": "Toward Automated Security Risk Detection in Large Software Using Call Graph Analysis", "comment": null, "summary": "Threat modeling plays a critical role in the identification and mitigation of\nsecurity risks; however, manual approaches are often labor intensive and prone\nto error. This paper investigates the automation of software threat modeling\nthrough the clustering of call graphs using density-based and community\ndetection algorithms, followed by an analysis of the threats associated with\nthe identified clusters. The proposed method was evaluated through a case study\nof the Splunk Forwarder Operator (SFO), wherein selected clustering metrics\nwere applied to the software's call graph to assess pertinent code-density\nsecurity weaknesses. The results demonstrate the viability of the approach and\nunderscore its potential to facilitate systematic threat assessment. This work\ncontributes to the advancement of scalable, semi-automated threat modeling\nframeworks tailored for modern cloud-native environments.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u8c03\u7528\u56fe\u8fdb\u884c\u5bc6\u5ea6\u805a\u7c7b\u548c\u793e\u533a\u68c0\u6d4b\u6765\u5b9e\u73b0\u534a\u81ea\u52a8\u5316\u5a01\u80c1\u5efa\u6a21\uff0c\u5e76\u5728 Splunk Forwarder Operator \u7684\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u89e3\u51b3\u624b\u5de5\u5a01\u80c1\u5efa\u6a21\u6210\u672c\u9ad8\u3001\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u9700\u9762\u5411\u73b0\u4ee3\u4e91\u539f\u751f\u73af\u5883\u7684\u53ef\u6269\u5c55\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u5728\u8f6f\u4ef6\u8c03\u7528\u56fe\u4e0a\u7ed3\u5408\u5bc6\u5ea6\u805a\u7c7b\u548c\u793e\u533a\u68c0\u6d4b\u7b97\u6cd5\u8fdb\u884c\u805a\u7c7b\uff0c\u518d\u5bf9\u5f97\u5230\u7684\u805a\u7c7b\u8fdb\u884c\u5a01\u80c1\u5206\u6790\uff1b\u4ee5 Splunk Forwarder Operator\uff08SFO\uff09\u4e3a\u6848\u4f8b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e94\u7528\u9009\u5b9a\u7684\u805a\u7c7b\u6307\u6807\u6765\u8bc4\u4f30\u4ee3\u7801\u5bc6\u5ea6\u4e0e\u6f5c\u5728\u7684\u5b89\u5168\u8584\u5f31\u70b9\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\uff0c\u80fd\u8bc6\u522b\u4e0e\u4ee3\u7801\u5bc6\u5ea6\u76f8\u5173\u7684\u5b89\u5168\u8584\u5f31\u70b9\uff0c\u5e76\u4fc3\u8fdb\u7cfb\u7edf\u5316\u7684\u5a01\u80c1\u8bc4\u4f30\u3002", "conclusion": "\u4e3a\u73b0\u4ee3\u4e91\u539f\u751f\u73af\u5883\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u534a\u81ea\u52a8\u5316\u5a01\u80c1\u5efa\u6a21\u6846\u67b6\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.26000", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26000", "abs": "https://arxiv.org/abs/2510.26000", "authors": ["Harin Lee", "Min-hwan Oh"], "title": "Infrequent Exploration in Linear Bandits", "comment": "NeurIPS 2025 camera-ready version", "summary": "We study the problem of infrequent exploration in linear bandits, addressing\na significant yet overlooked gap between fully adaptive exploratory methods\n(e.g., UCB and Thompson Sampling), which explore potentially at every time\nstep, and purely greedy approaches, which require stringent diversity\nassumptions to succeed. Continuous exploration can be impractical or unethical\nin safety-critical or costly domains, while purely greedy strategies typically\nfail without adequate contextual diversity. To bridge these extremes, we\nintroduce a simple and practical framework, INFEX, explicitly designed for\ninfrequent exploration. INFEX executes a base exploratory policy according to a\ngiven schedule while predominantly choosing greedy actions in between. Despite\nits simplicity, our theoretical analysis demonstrates that INFEX achieves\ninstance-dependent regret matching standard provably efficient algorithms,\nprovided the exploration frequency exceeds a logarithmic threshold.\nAdditionally, INFEX is a general, modular framework that allows seamless\nintegration of any fully adaptive exploration method, enabling wide\napplicability and ease of adoption. By restricting intensive exploratory\ncomputations to infrequent intervals, our approach can also enhance\ncomputational efficiency. Empirical evaluations confirm our theoretical\nfindings, showing state-of-the-art regret performance and runtime improvements\nover existing methods.", "AI": {"tldr": "INFEX: a modular framework for infrequent exploration in linear bandits that schedules limited exploratory actions, achieving instance-dependent regret comparable to standard algorithms while improving computational efficiency.", "motivation": "There is a practical gap between fully adaptive exploration (e.g., UCB/Thompson Sampling) and purely greedy methods: frequent exploration can be impractical or unsafe, while greedy strategies require strong contextual diversity.", "method": "INFEX runs a base exploratory policy at a prescribed schedule and largely selects greedy actions in between. It is modular and can wrap any adaptive exploration method, concentrating intensive exploration only at infrequent intervals.", "result": "Under an exploration frequency above a logarithmic threshold, INFEX achieves instance-dependent regret matching provably efficient algorithms; it also improves computational efficiency. Empirical evaluations show state-of-the-art regret and faster runtimes.", "conclusion": "INFEX provides a simple, practical, and widely applicable framework for infrequent exploration in linear bandits, enabling safer deployment without sacrificing regret performance and with reduced computation."}}
{"id": "2510.26014", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26014", "abs": "https://arxiv.org/abs/2510.26014", "authors": ["Hyeonjun Lee", "Hyungseob Shin", "Gunhee Nam", "Hyeonsoo Lee"], "title": "Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis", "comment": "Accepted to NeurIPS 2025 workshop Learning from Time Series for\n  Health (TS4H)", "summary": "Survival analysis is a task to model the time until an event of interest\noccurs, widely used in clinical and biomedical research. A key challenge is to\nmodel patient heterogeneity while also adapting risk predictions to both\nindividual characteristics and temporal dynamics. We propose a dual\nmixture-of-experts (MoE) framework for discrete-time survival analysis. Our\napproach combines a feature-encoder MoE for subgroup-aware representation\nlearning with a hazard MoE that leverages patient features and time embeddings\nto capture temporal dynamics. This dual-MoE design flexibly integrates with\nexisting deep learning based survival pipelines. On METABRIC and GBSG breast\ncancer datasets, our method consistently improves performance, boosting the\ntime-dependent C-index up to 0.04 on the test sets, and yields further gains\nwhen incorporated into the Consurv framework.", "AI": {"tldr": "\u63d0\u51fa\u53ccMoE\u6846\u67b6\u7528\u4e8e\u79bb\u6563\u65f6\u95f4\u751f\u5b58\u5206\u6790\uff0c\u901a\u8fc7\u7279\u5f81\u7f16\u7801\u5668MoE\u5b9e\u73b0\u5b50\u7fa4\u611f\u77e5\u8868\u5f81\u4ee5\u53ca hazard MoE\u5b9e\u73b0\u65f6\u5e8f\u52a8\u6001\u5efa\u6a21\uff0c\u63d0\u5347METABRIC\u548cGBSG\u6570\u636e\u96c6\u4e0a\u7684\u65f6\u95f4\u76f8\u5173C\u6307\u6570\u3002", "motivation": "\u5728\u60a3\u8005\u5f02\u8d28\u6027\u4e0e\u65f6\u95f4\u52a8\u6001\u7684\u540c\u65f6\u5efa\u6a21\u4ee5\u63d0\u5347\u4e2a\u4f53\u5316\u98ce\u9669\u9884\u6d4b\u3002", "method": "\u5f15\u5165\u7279\u5f81\u7f16\u7801\u5668MoE\u4e0ehazard MoE\u7684\u53ccMoE\u67b6\u6784\uff0c\u7ed3\u5408\u65f6\u95f4\u5d4c\u5165\uff0c\u80fd\u591f\u7075\u6d3b\u96c6\u6210\u5230\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u7ba1\u7ebf\u3002", "result": "\u5728METABRIC\u548cGBSG\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u6d4b\u8bd5\u96c6\u65f6\u95f4\u76f8\u5173C-index\u63d0\u5347\u81f3\u7ea60.04\uff1b\u5728\u4e0eConsurv\u6846\u67b6\u96c6\u6210\u540e\u83b7\u5f97\u8fdb\u4e00\u6b65\u6536\u76ca\u3002", "conclusion": "\u53ccMoE\u8bbe\u8ba1\u53ef\u6709\u6548\u540c\u65f6\u6355\u83b7\u60a3\u8005\u5f02\u8d28\u6027\u4e0e\u65f6\u95f4\u52a8\u6001\uff0c\u63d0\u5347\u79bb\u6563\u65f6\u95f4\u751f\u5b58\u5206\u6790\u7684\u9884\u6d4b\u6027\u80fd\u5e76\u5177\u5907\u826f\u597d\u6269\u5c55\u6027\u3002"}}
{"id": "2510.26792", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26792", "abs": "https://arxiv.org/abs/2510.26792", "authors": ["Tao Tao", "Maissam Barkeshli"], "title": "Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability", "comment": "10+13 pages, 8+19 figures", "summary": "We study the ability of Transformer models to learn sequences generated by\nPermuted Congruential Generators (PCGs), a widely used family of pseudo-random\nnumber generators (PRNGs). PCGs introduce substantial additional difficulty\nover linear congruential generators (LCGs) by applying a series of bit-wise\nshifts, XORs, rotations and truncations to the hidden state. We show that\nTransformers can nevertheless successfully perform in-context prediction on\nunseen sequences from diverse PCG variants, in tasks that are beyond published\nclassical attacks. In our experiments we scale moduli up to $2^{22}$ using up\nto $50$ million model parameters and datasets with up to $5$ billion tokens.\nSurprisingly, we find even when the output is truncated to a single bit, it can\nbe reliably predicted by the model. When multiple distinct PRNGs are presented\ntogether during training, the model can jointly learn them, identifying\nstructures from different permutations. We demonstrate a scaling law with\nmodulus $m$: the number of in-context sequence elements required for\nnear-perfect prediction grows as $\\sqrt{m}$. For larger moduli, optimization\nenters extended stagnation phases; in our experiments, learning moduli $m \\geq\n2^{20}$ requires incorporating training data from smaller moduli, demonstrating\na critical necessity for curriculum learning. Finally, we analyze embedding\nlayers and uncover a novel clustering phenomenon: the model spontaneously\ngroups the integer inputs into bitwise rotationally-invariant clusters,\nrevealing how representations can transfer from smaller to larger moduli.", "AI": {"tldr": "Transformer models can learn in-context prediction on sequences generated by permuted congruential generators (PCGs), including extremely large moduli and truncated outputs, revealing scalable learning laws and emergent representations.", "motivation": "To probe the limits of in-context learning by transformers on complex PRNGs (PCGs) and to understand how model size, data, and curriculum affect learning of nontrivial stateful sequences.", "method": "Train transformers with up to 50M parameters on PCG-generated sequences with modulus m up to 2^22 (\u22484 million) and datasets up to 5 billion tokens. Tasks include predicting the next PRNG output or a truncated bit. Evaluate zero-shot generalization to unseen PCG variants and joint learning across multiple PRNGs. Analyze scaling with modulus and embedding representations, including curriculum effects and clustering in input embeddings.", "result": "Transformers achieve near-perfect in-context prediction on unseen PCG variants, even when outputs are truncated to a single bit. They can jointly learn multiple PRNGs. A scaling law emerges: the number of in-context elements needed scales as sqrt(m). For larger moduli, training exhibits extended stagnation unless curriculum learning from smaller moduli is used. Embedding analyses reveal bitwise rotationally-invariant clustering of inputs, indicating transferable representations across moduli.", "conclusion": "Transformers are capable of robust in-context learning for complex PRNG sequences, but practical training at large moduli requires curriculum strategies. The learned representations show structured, transferable organization that aligns with the bitwise nature of the generators."}}
{"id": "2510.26025", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26025", "abs": "https://arxiv.org/abs/2510.26025", "authors": ["Semyon Lomaso", "Judah Goldfeder", "Mehmet Hamza Erol", "Matthew So", "Yao Yan", "Addison Howard", "Nathan Kutz", "Ravid Shwartz Ziv"], "title": "Exploring Human-AI Conceptual Alignment through the Prism of Chess", "comment": null, "summary": "Do AI systems truly understand human concepts or merely mimic surface\npatterns? We investigate this through chess, where human creativity meets\nprecise strategic concepts. Analyzing a 270M-parameter transformer that\nachieves grandmaster-level play, we uncover a striking paradox: while early\nlayers encode human concepts like center control and knight outposts with up to\n85\\% accuracy, deeper layers, despite driving superior performance, drift\ntoward alien representations, dropping to 50-65\\% accuracy. To test conceptual\nrobustness beyond memorization, we introduce the first Chess960 dataset: 240\nexpert-annotated positions across 6 strategic concepts. When opening theory is\neliminated through randomized starting positions, concept recognition drops\n10-20\\% across all methods, revealing the model's reliance on memorized\npatterns rather than abstract understanding. Our layer-wise analysis exposes a\nfundamental tension in current architectures: the representations that win\ngames diverge from those that align with human thinking. These findings suggest\nthat as AI systems optimize for performance, they develop increasingly alien\nintelligence, a critical challenge for creative AI applications requiring\ngenuine human-AI collaboration. Dataset and code are available at:\nhttps://github.com/slomasov/ChessConceptsLLM.", "AI": {"tldr": "\u5728\u68cb\u7c7b\u4efb\u52a1\u4e2d\uff0c270M\u53c2\u6570\u7684Transformer\u5728\u65e9\u671f\u5c42\u5bf9\u4eba\u7c7b\u6982\u5ff5\u7684\u7f16\u7801\u8f83\u4e3a\u51c6\u786e\uff0c\u4f46\u968f\u7740\u5c42\u6570\u52a0\u6df1\u4ee5\u63d0\u5347\u6027\u80fd\u7684\u8868\u5f81\u53d1\u751f\u201c\u5916\u661f\u5316\u201d\uff0c\u51c6\u786e\u7387\u964d\u81f350-65%\u3002\u901a\u8fc7Chess960\u6570\u636e\u96c6\uff08240\u4e2a\u4e13\u5bb6\u6807\u6ce8\u4f4d\u7f6e\u30016\u4e2a\u6982\u5ff5\uff09\u4ee5\u53ca\u53bb\u9664\u5f00\u5c40\u7406\u8bba\u7684\u968f\u673a\u8d77\u59cb\u4f4d\u7f6e\u5b9e\u9a8c\uff0c\u6982\u5ff5\u8bc6\u522b\u80fd\u529b\u4e0b\u964d\u7ea610-20%\uff0c\u63ed\u793a\u6a21\u578b\u66f4\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u62bd\u8c61\u7406\u89e3\u3002\u5c42\u6b21\u5206\u6790\u66b4\u9732\u51fa\u80dc\u5229\u8868\u5f81\u4e0e\u4eba\u7c7b\u601d\u7ef4\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u8868\u660e\u8ffd\u6c42\u6027\u80fd\u53ef\u80fd\u5bfc\u81f4\u201c\u5916\u661f\u667a\u80fd\u201d\u7684\u98ce\u9669\uff0c\u5e76\u5bf9\u9700\u8981\u4eba\u673a\u534f\u4f5c\u7684\u521b\u9020\u6027AI\u63d0\u51fa\u6311\u6218\u3002\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u53ef\u5728GitHub\u83b7\u5f97\u3002", "motivation": "\u63a2\u7a76AI\u7cfb\u7edf\u662f\u5426\u771f\u6b63\u7406\u89e3\u4eba\u7c7b\u6982\u5ff5\uff0c\u8fd8\u662f\u4ec5\u6a21\u4eff\u8868\u9762\u6a21\u5f0f\uff1b\u901a\u8fc7\u68cb\u7c7b\u4efb\u52a1\u63ed\u793a\u4e0d\u540c\u5c42\u6b21\u8868\u5f81\u4e0e\u6982\u5ff5\u7406\u89e3\u7684\u5173\u7cfb\uff1b\u901a\u8fc7Chess960\u68c0\u9a8c\u53bb\u9664\u8d77\u59cb\u7406\u8bba\u540e\u7684\u6982\u5ff5\u9c81\u68d2\u6027\u3002", "method": "\u5bf9270M\u53c2\u6570\u7684Transformer\u8fdb\u884c\u5c42\u6b21\u5206\u6790\uff0c\u8bc4\u4f306\u4e2a\u7b56\u7565\u6982\u5ff5\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff1b\u6784\u5efaChess960\u6570\u636e\u96c6\uff0c\u63d0\u4f9b240\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u4f4d\u7f6e\uff0c\u8986\u76d66\u4e2a\u6982\u5ff5\uff1b\u5728\u968f\u673a\u8d77\u59cb\u4f4d\u7f6e\u4e0b\u53bb\u9664\u5f00\u5c40\u7406\u8bba\uff0c\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u6982\u5ff5\u8bc6\u522b\u80fd\u529b\uff1b\u8fdb\u884c\u5c42\u8868\u5f81\u5bf9\u6bd4\u4e0e\u4e0e\u4eba\u7c7b\u601d\u7ef4\u7684\u4e00\u81f4\u6027\u5206\u6790\u3002", "result": "\u65e9\u671f\u5c42\u5bf9\u4eba\u7c7b\u6982\u5ff5\u7684\u7f16\u7801\u53ef\u8fbe\u7ea685%\u51c6\u786e\uff1b\u6df1\u5c42\u8868\u5f81\u5c3d\u7ba1\u63d0\u5347\u6e38\u620f\u8868\u73b0\uff0c\u5374\u5448\u73b0\u201c alien\u201d\u7279\u5f81\uff0c\u51c6\u786e\u7387\u964d\u81f350-65%\uff1b\u53bb\u9664\u5f00\u5c40\u7406\u8bba\u540e\u6982\u5ff5\u8bc6\u522b\u4e0b\u964d\u7ea610-20%\uff1b\u603b\u4f53\u8bc1\u636e\u6307\u5411\u66f4\u50cf\u8bb0\u5fc6\u6a21\u5f0f\u7684\u6982\u5ff5\u7406\u89e3\u800c\u975e\u62bd\u8c61\u901a\u7528\u7406\u89e3\u3002", "conclusion": "\u6a21\u578b\u5728\u8ffd\u6c42\u9ad8\u6027\u80fd\u65f6\uff0c\u5176\u80dc\u5229\u8868\u5f81\u4e0e\u4eba\u7c7b\u6982\u5ff5\u5b58\u5728\u663e\u8457\u5f20\u529b\uff0c\u53ef\u80fd\u53d1\u5c55\u51fa\u201c alien intelligence\u201d\uff1b\u8fd9\u5bf9\u9700\u8981\u4eba\u673a\u534f\u4f5c\u7684\u521b\u9020\u6027AI\u5e94\u7528\u63d0\u51fa\u6311\u6218\uff0c\u5f3a\u8c03\u5bf9\u9f50\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u5173\u6ce8\uff0c\u4ee5\u53ca\u5728\u9ad8\u6027\u80fd\u4e0e\u4eba\u7c7b\u6982\u5ff5\u4e4b\u95f4\u5b9e\u73b0\u66f4\u597d\u534f\u540c\u7684\u5fc5\u8981\u6027\u3002\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.26038", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26038", "abs": "https://arxiv.org/abs/2510.26038", "authors": ["Jiali Cheng", "Chirag Agarwal", "Hadi Amiri"], "title": "Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods", "comment": null, "summary": "Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.", "AI": {"tldr": "\u77e5\u8bc6\u84b8\u998f\u4f1a\u524a\u5f31\u5bf9\u504f\u7f6e\u7684 debiasing \u8f6c\u79fb\uff1b\u628a\u6559\u5e08\u6a21\u578b\u77e5\u8bc6\u6ce8\u5165\u5230\u5b66\u751f\u6a21\u578b\u5e76\u4e0d\u80fd\u63d0\u5347 debiasing\uff0c\u4e14\u6548\u679c\u5728\u4e0d\u540c\u504f\u7f6e\u7c7b\u578b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u63d0\u51fa\u6570\u636e\u589e\u5f3a\u3001\u8fed\u4ee3\u5f0f\u84b8\u998f\u548c\u6559\u5e08\u521d\u59cb\u5316\u7b49\u4e09\u6761\u6539\u8fdb\u7b56\u7565\uff1b\u9996\u6b21\u5728\u5927\u5c3a\u5ea6\u5206\u6790 KD \u5bf9 debiasing \u7684\u5f71\u54cd\u4e0e\u5185\u90e8\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u77e5\u8bc6\u84b8\u998f\u5728\u9c81\u68d2\u6027\u3001\u5bf9\u6297 spurious correlations \u7684 debiasing \u80fd\u529b\u8f6c\u79fb\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca KD \u5bf9 NLI \u4e0e\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684 debiasing \u7684\u5f71\u54cd\u548c\u673a\u5236\u3002", "method": "\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u6bd4\u8f83 debiasing \u4e0e KD \u60c5\u51b5\u4e0b\u7684\u504f\u7f6e\u8f6c\u79fb\uff0c\u5206\u6790\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0e\u5185\u90e8\u7535\u8def\uff0c\u8bc6\u522b\u5f71\u54cd KD \u884c\u4e3a\u7684\u504f\u7f6e\u7c7b\u578b\uff0c\u63d0\u51fa\u5e76\u6d4b\u8bd5\u6539\u8fdb\u7b56\u7565\u3002", "result": "1) KD \u4e4b\u540e debiasing \u80fd\u529b\u6574\u4f53\u4e0b\u964d\uff1b2) \u8bad\u7ec3\u5f97\u5230\u7684\u53bb\u504f\u6a21\u578b\u5bf9\u6559\u5e08\u77e5\u8bc6\u7684\u6ce8\u5165\u6536\u76ca\u6709\u9650\uff1b3) \u6574\u4f53\u9c81\u68d2\u6027\u53ef\u80fd\u4fdd\u6301\u7a33\u5b9a\uff0c\u4f46\u5bf9\u4e0d\u540c\u504f\u7f6e\u7c7b\u578b\u7684\u5f71\u54cd\u5dee\u5f02\u663e\u8457\uff1b4) \u6307\u51fa\u5185\u90e8\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0e\u7535\u8def\u4e0e\u8fd9\u79cd\u884c\u4e3a\u7684\u5173\u7cfb\uff1b\u5e76\u63d0\u51fa\u4e09\u6761\u63d0\u5347 debiasing \u5728\u84b8\u998f\u4e2d\u7684\u53ef\u8fc1\u79fb\u6027\u7684\u65b9\u6848\u3002", "conclusion": "KD \u53ef\u80fd\u6270\u4e71 debiasing \u7684\u8f6c\u79fb\u4e0e\u673a\u5236\uff1b\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684 debiasing \u65b9\u6cd5\u548c KD \u6d41\u7a0b\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002\u63d0\u51fa\u7684\u6570\u636e\u589e\u5f3a\u3001\u8fed\u4ee3\u5f0f\u84b8\u998f\u3001\u6559\u5e08\u6743\u91cd\u521d\u59cb\u5316\u7b49\u7b56\u7565\u6709\u52a9\u4e8e\u63d0\u5347 debiasing \u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7684\u6548\u679c\uff1b\u8fd9\u662f\u9996\u4e2a\u5728\u5927\u5c3a\u5ea6\u4e0a\u7814\u7a76 KD \u5bf9 debiasing \u53ca\u5176\u5185\u90e8\u673a\u5236\u7684\u5de5\u4f5c\u3002"}}
{"id": "2510.26064", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26064", "abs": "https://arxiv.org/abs/2510.26064", "authors": ["David Otte", "J\u00f6rg K. H. Franke", "Frank Hutter"], "title": "Towards Scaling Laws for Symbolic Regression", "comment": "Accepted at the NeurIPS 2025 Math-AI Workshop", "summary": "Symbolic regression (SR) aims to discover the underlying mathematical\nexpressions that explain observed data. This holds promise for both gaining\nscientific insight and for producing inherently interpretable and generalizable\nmodels for tabular data. In this work we focus on the basics of SR. Deep\nlearning-based SR has recently become competitive with genetic programming\napproaches, but the role of scale has remained largely unexplored. Inspired by\nscaling laws in language modeling, we present the first systematic\ninvestigation of scaling in SR, using a scalable end-to-end transformer\npipeline and carefully generated training data. Across five different model\nsizes and spanning three orders of magnitude in compute, we find that both\nvalidation loss and solved rate follow clear power-law trends with compute. We\nfurther identify compute-optimal hyperparameter scaling: optimal batch size and\nlearning rate grow with model size, and a token-to-parameter ratio of\n$\\approx$15 is optimal in our regime, with a slight upward trend as compute\nincreases. These results demonstrate that SR performance is largely predictable\nfrom compute and offer important insights for training the next generation of\nSR models.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u5c06\u7b26\u53f7\u56de\u5f52\uff08SR\uff09\u7f6e\u4e8e\u89c4\u6a21\u5316\u7814\u7a76\u4e4b\u4e0b\uff0c\u63ed\u793a\u8ba1\u7b97\u91cf\u5bf9 SR \u8868\u73b0\u7684\u5e42\u5f8b\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u968f\u6a21\u578b\u89c4\u6a21\u53d8\u5316\u7684 compute \u6700\u4f18\u8d85\u53c2\u6570\u89c4\u5f8b\u3002", "motivation": "\u53d7\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u5316\u89c4\u5f8b\u542f\u53d1\uff0c\u9996\u6b21\u7cfb\u7edf\u63a2\u7a76 SR \u5728\u5927\u89c4\u6a21\u8ba1\u7b97\u4e0b\u7684\u8868\u73b0\u8d8b\u52bf\uff0c\u4ee5\u5b9e\u73b0\u5bf9 SR \u6a21\u578b\u6027\u80fd\u7684\u53ef\u9884\u6d4b\u6027\u548c\u9ad8\u6548\u8bad\u7ec3\u3002", "method": "\u6784\u5efa\u7aef\u5230\u7aef\u7684\u53ef\u6269\u5c55 Transformer SR \u6d41\u6c34\u7ebf\uff0c\u5e76\u5728\u6709\u63a7\u5236\u5730\u751f\u6210\u7684\u8bad\u7ec3\u6570\u636e\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u8986\u76d6\u4e94\u79cd\u6a21\u578b\u89c4\u6a21\u548c\u8de8\u8d8a\u4e09\u4e2a\u6570\u91cf\u7ea7\u7684\u8ba1\u7b97\u89c4\u6a21\uff0c\u8bc4\u4f30\u9a8c\u8bc1\u635f\u5931\u4e0e\u6c42\u89e3\u7387\u5e76\u62df\u5408\u5e42\u5f8b\u5173\u7cfb\uff0c\u8fdb\u800c\u8bc6\u522b compute-\u6700\u4f18\u7684\u8d85\u53c2\u6570\uff08\u6279\u91cf\u5927\u5c0f\u3001\u5b66\u4e60\u7387\u3001token/\u53c2\u6570\u6bd4\uff09\u3002", "result": "\u9a8c\u8bc1\u635f\u5931\u548c\u6c42\u89e3\u7387\u968f\u8ba1\u7b97\u91cf\u5448\u73b0\u660e\u786e\u7684\u5e42\u5f8b\u8d8b\u52bf\uff1b\u5b58\u5728\u968f\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u7684 compute-\u6700\u4f18\u8d85\u53c2\u6570 scaling\uff0ctoken/\u53c2\u6570\u6bd4\u7ea6\u4e3a15\uff0c\u4e14\u968f\u8ba1\u7b97\u589e\u52a0\u7565\u6709\u4e0a\u5347\u8d8b\u52bf\uff1b\u7ed3\u679c\u8868\u660e SR \u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53ef\u7531\u8ba1\u7b97\u91cf\u9884\u6d4b\u3002", "conclusion": "SR \u7684\u89c4\u6a21\u89c4\u5f8b\u5b58\u5728\u5e76\u4e14\u53ef\u88ab\u7528\u4e8e\u8d44\u6e90\u89c4\u5212\u4e0e\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u4e3a\u8bad\u7ec3\u4e0b\u4e00\u4ee3\u66f4\u5927\u89c4\u6a21\u7684 SR \u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2510.26068", "categories": ["cs.LG", "cs.AI", "math.DG", "math.ST", "stat.TH", "68T05, 53B21, 65D18, 62B11", "I.2.6; I.5.1; G.1.8; G.4"], "pdf": "https://arxiv.org/pdf/2510.26068", "abs": "https://arxiv.org/abs/2510.26068", "authors": ["Di Zhang"], "title": "Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization", "comment": "9 pages", "summary": "This paper proposes a novel paradigm for machine learning that moves beyond\ntraditional parameter optimization. Unlike conventional approaches that search\nfor optimal parameters within a fixed geometric space, our core idea is to\ntreat the model itself as a malleable geometric entity. Specifically, we\noptimize the metric tensor field on a manifold with a predefined topology,\nthereby dynamically shaping the geometric structure of the model space. To\nachieve this, we construct a variational framework whose loss function\ncarefully balances data fidelity against the intrinsic geometric complexity of\nthe manifold. The former ensures the model effectively explains observed data,\nwhile the latter acts as a regularizer, penalizing overly curved or irregular\ngeometries to encourage simpler models and prevent overfitting. To address the\ncomputational challenges of this infinite-dimensional optimization problem, we\nintroduce a practical method based on discrete differential geometry: the\ncontinuous manifold is discretized into a triangular mesh, and the metric\ntensor is parameterized by edge lengths, enabling efficient optimization using\nautomatic differentiation tools. Theoretical analysis reveals a profound\nanalogy between our framework and the Einstein-Hilbert action in general\nrelativity, providing an elegant physical interpretation for the concept of\n\"data-driven geometry\". We further argue that even with fixed topology, metric\noptimization offers significantly greater expressive power than models with\nfixed geometry. This work lays a solid foundation for constructing fully\ndynamic \"meta-learners\" capable of autonomously evolving their geometry and\ntopology, and it points to broad application prospects in areas such as\nscientific model discovery and robust representation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u6d41\u5f62\u4e0a\u4f18\u5316\u5ea6\u91cf\u5f20\u91cf\u573a\u6765\u5b9e\u73b0\u52a8\u6001\u51e0\u4f55\u7ed3\u6784\uff0c\u4ece\u800c\u8d85\u8d8a\u56fa\u5b9a\u51e0\u4f55\u7684\u53c2\u6570\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u53c2\u6570\u4f18\u5316\u5c40\u9650\u4e8e\u56fa\u5b9a\u51e0\u4f55\u7a7a\u95f4\uff0c\u96be\u4ee5\u5145\u5206\u8868\u8fbe\u6570\u636e\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\uff1b\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u51e0\u4f55\u5b66\u4e60\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5728\u6d41\u5f62\u4e0a\u5f15\u5165\u53d8\u5206\u6846\u67b6\uff0c\u4f18\u5316\u5ea6\u91cf\u5f20\u91cf\u573a\uff0c\u5e76\u5c06\u6d41\u5f62\u79bb\u6563\u5316\u4e3a\u4e09\u89d2\u7f51\u683c\uff0c\u5ea6\u91cf\u7531\u8fb9\u957f\u53c2\u6570\u5316\uff0c\u7ed3\u5408\u6570\u636e\u4fdd\u771f\u5ea6\u4e0e\u51e0\u4f55\u590d\u6742\u6027\u6b63\u5219\u5316\uff1b\u5229\u7528\u81ea\u52a8\u5fae\u5206\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u6846\u67b6\u4e0e\u7231\u56e0\u65af\u5766-\u5e0c\u4f2f\u7279\u4f5c\u7528\u7684\u6df1\u523b\u7c7b\u6bd4\uff0c\u6307\u793a\u5728\u56fa\u5b9a\u62d3\u6251\u4e0b\u5ea6\u91cf\u4f18\u5316\u4e5f\u5177\u5907\u66f4\u5f3a\u8868\u8fbe\u80fd\u529b\uff1b\u63d0\u51fa\u4e86\u53ef\u5b9e\u73b0\u7684\u79bb\u6563\u65b9\u6cd5\u5e76\u4e3a\u52a8\u6001\u56fe\u5f62/\u5143\u5b66\u4e60\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "\u4e3a\u6784\u5efa\u5b8c\u5168\u52a8\u6001\u7684\u201c\u5143\u5b66\u4e60\u5668\u201d\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u5728\u79d1\u5b66\u6a21\u578b\u53d1\u73b0\u548c\u9c81\u68d2\u8868\u793a\u5b66\u4e60\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.26076", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26076", "abs": "https://arxiv.org/abs/2510.26076", "authors": ["James Meldrum", "Basem Suleiman", "Fethi Rabhi", "Muhammad Johan Alibasa"], "title": "New Money: A Systematic Review of Synthetic Data Generation for Finance", "comment": "37 pages, 5 figures, 21 tables", "summary": "Synthetic data generation has emerged as a promising approach to address the\nchallenges of using sensitive financial data in machine learning applications.\nBy leveraging generative models, such as Generative Adversarial Networks (GANs)\nand Variational Autoencoders (VAEs), it is possible to create artificial\ndatasets that preserve the statistical properties of real financial records\nwhile mitigating privacy risks and regulatory constraints. Despite the rapid\ngrowth of this field, a comprehensive synthesis of the current research\nlandscape has been lacking. This systematic review consolidates and analyses 72\nstudies published since 2018 that focus on synthetic financial data generation.\nWe categorise the types of financial information synthesised, the generative\nmethods employed, and the evaluation strategies used to assess data utility and\nprivacy. The findings indicate that GAN-based approaches dominate the\nliterature, particularly for generating time-series market data and tabular\ncredit data. While several innovative techniques demonstrate potential for\nimproved realism and privacy preservation, there remains a notable lack of\nrigorous evaluation of privacy safeguards across studies. By providing an\nintegrated overview of generative techniques, applications, and evaluation\nmethods, this review highlights critical research gaps and offers guidance for\nfuture work aimed at developing robust, privacy-preserving synthetic data\nsolutions for the financial domain.", "AI": {"tldr": "\u5bf972\u9879\u81ea2018\u5e74\u4ee5\u6765\u7684\u91d1\u878d\u9886\u57df\u5408\u6210\u6570\u636e\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u7ed3\u679c\u663e\u793aGAN\u4e3b\u5bfc\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u548c\u4fe1\u7528\u6570\u636e\uff0c\u4f46\u9690\u79c1\u8bc4\u4f30\u4e0d\u8db3\uff0c\u5f3a\u8c03\u672a\u6765\u7814\u7a76\u5728\u9690\u79c1\u4fdd\u62a4\u4e0a\u9700\u52a0\u5f3a\u3002", "motivation": "\u5728\u91d1\u878d\u573a\u666f\u4e2d\u4f7f\u7528\u654f\u611f\u6570\u636e\u9762\u4e34\u9690\u79c1\u4e0e\u5408\u89c4\u6311\u6218\uff0c\u8be5\u7efc\u8ff0\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u5de5\u4f5c\uff0c\u8bc4\u4f30\u65b9\u6cd5\u53ca\u5176\u5c40\u9650\u6027\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5b89\u5168\u7684\u5408\u6210\u6570\u636e\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8fdb\u884c\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u7eb3\u516572\u9879\u7814\u7a76\uff082018\u5e74\u4ee5\u6765\uff09\uff0c\u5bf9\u6240\u5408\u6210\u7684\u91d1\u878d\u4fe1\u606f\u7c7b\u578b\u3001\u751f\u6210\u65b9\u6cd5\u3001\u4ee5\u53ca\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u5b9e\u7528\u6027\u4e0e\u9690\u79c1\u6027\u7684\u8bc4\u4f30\u7b56\u7565\u8fdb\u884c\u5206\u7c7b\u548c\u5206\u6790\u3002", "result": "GAN\u4e3a\u4e3b\u5bfc\uff0c\u5c24\u5176\u5728\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u5e02\u573a\u6570\u636e\u548c\u8868\u683c\u5316\u7684\u4fe1\u7528\u6570\u636e\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002\u5c3d\u7ba1\u5b58\u5728\u6539\u8fdb\u73b0\u5b9e\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u65b0\u6280\u672f\uff0c\u4f46\u5bf9\u9690\u79c1\u9632\u62a4\u7684\u8bc4\u4f30\u4ecd\u663e\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4e00\u81f4\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "conclusion": "\u63d0\u4f9b\u751f\u6210\u6280\u672f\u3001\u5e94\u7528\u573a\u666f\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u7efc\u5408\u6982\u89c8\uff0c\u6307\u51fa\u7814\u7a76\u7a7a\u7f3a\u5e76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u51fa\u65b9\u5411\uff0c\u4ee5\u53d1\u5c55\u5065\u58ee\u3001\u6ce8\u91cd\u9690\u79c1\u4fdd\u62a4\u7684\u91d1\u878d\u9886\u57df\u5408\u6210\u6570\u636e\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26083", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26083", "abs": "https://arxiv.org/abs/2510.26083", "authors": ["Yuhua Jiang", "Shuang Cheng", "Yihao Liu", "Ermo Hua", "Che Jiang", "Weigao Sun", "Yu Cheng", "Feifei Gao", "Biqing Qi", "Bowen Zhou"], "title": "Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism", "comment": null, "summary": "Specialized Generalist Models (SGMs) aim to preserve broad capabilities while\nachieving expert-level performance in target domains. However, traditional LLM\nstructures including Transformer, Linear Attention, and hybrid models do not\nemploy specialized memory mechanism guided by task information. In this paper,\nwe present Nirvana, an SGM with specialized memory mechanism, linear time\ncomplexity, and test-time task information extraction. Besides, we propose the\nTask-Aware Memory Trigger ($\\textit{Trigger}$) that flexibly adjusts memory\nmechanism based on the current task's requirements. In Trigger, each incoming\nsample is treated as a self-supervised fine-tuning task, enabling Nirvana to\nadapt its task-related parameters on the fly to domain shifts. We also design\nthe Specialized Memory Updater ($\\textit{Updater}$) that dynamically memorizes\nthe context guided by Trigger. We conduct experiments on both general language\ntasks and specialized medical tasks. On a variety of natural language modeling\nbenchmarks, Nirvana achieves competitive or superior results compared to the\nexisting LLM structures. To prove the effectiveness of Trigger on specialized\ntasks, we test Nirvana's performance on a challenging medical task, i.e.,\nMagnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with\nlightweight codecs on paired electromagnetic signals and MRI images. Despite\nthe frozen Nirvana backbone, Trigger guides the model to adapt to the MRI\ndomain with the change of task-related parameters. Nirvana achieves\nhigher-quality MRI reconstruction compared to conventional MRI models as well\nas the models with traditional LLMs' backbone, and can also generate accurate\npreliminary clinical reports accordingly.", "AI": {"tldr": "Nirvana \u662f\u4e00\u79cd\u4e13\u95e8\u5316\u901a\u7528\u6a21\u578b\uff08SGM\uff09\uff0c\u5f15\u5165\u4efb\u52a1\u611f\u77e5\u5185\u5b58\u673a\u5236\u3001\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u6d4b\u8bd5\u65f6\u4efb\u52a1\u4fe1\u606f\u63d0\u53d6\uff0c\u901a\u8fc7 Trigger \u4e0e Updater \u5b9e\u73b0\u5bf9\u4efb\u52a1\u76f8\u5173\u53c2\u6570\u7684\u52a8\u6001\u9002\u914d\u3002\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u548c\u4e13\u95e8\u533b\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u7ade\u4e89\u751a\u81f3\u4f18\u4e8e\u73b0\u6709 LLM\uff1b\u5728 MRI \u4efb\u52a1\u4e2d\u51bb\u7ed3\u9aa8\u5e72\u5e76\u4f7f\u7528\u8f7b\u91cf\u7f16\u89e3\u7801\u5668\uff0cTrigger \u6307\u5bfc\u57df\u9002\u914d\uff0c\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u548c\u521d\u6b65\u4e34\u5e8a\u62a5\u544a\u3002", "motivation": "\u73b0\u6709\u7684 Transformer\u3001\u7ebf\u6027\u6ce8\u610f\u529b\u53ca\u6df7\u5408\u7ed3\u6784\u7f3a\u4e4f\u4ee5\u4efb\u52a1\u4fe1\u606f\u4e3a\u5bfc\u5411\u7684\u4e13\u7528\u5185\u5b58\u673a\u5236\uff0c\u96be\u4ee5\u5728\u4fdd\u6301\u5e7f\u6cdb\u80fd\u529b\u7684\u540c\u65f6\u8fbe\u5230\u76ee\u6807\u9886\u57df\u7684\u4e13\u5bb6\u7ea7\u6027\u80fd\uff1b\u9700\u8981\u4e00\u4e2a\u80fd\u6839\u636e\u4efb\u52a1\u4fe1\u606f\u52a8\u6001\u8c03\u8282\u8bb0\u5fc6\u5e76\u5177\u6709\u4f4e\u590d\u6742\u5ea6\u7684\u901a\u7528-\u4e13\u7528\u6df7\u5408\u6846\u67b6\u3002", "method": "\u63d0\u51fa Nirvana \u67b6\u6784\uff0c\u5305\u542b\u4e13\u95e8\u5316\u5185\u5b58\u673a\u5236\u3001Task-Aware Memory Trigger\uff08Trigger\uff09\u548c Specialized Memory Updater\uff08Updater\uff09\u3002\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u89c6\u4e3a\u81ea\u76d1\u7763\u5fae\u8c03\u4efb\u52a1\uff0c\u52a8\u6001\u8c03\u6574\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u53c2\u6570\u4ee5\u5e94\u5bf9\u9886\u57df\u79fb\u4f4d\uff0c\u4e14\u5177\u5907\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u5185\u5b58\u66f4\u65b0\u3002Trigger \u6839\u636e\u5f53\u524d\u4efb\u52a1\u9700\u6c42\u7075\u6d3b\u89e6\u53d1/\u8c03\u6574\u8bb0\u5fc6\uff0cUpdater \u6839\u636e Trigger \u6307\u5bfc\u8bb0\u5fc6\u66f4\u65b0\u3002\u5bf9\u4e8e MRI \u4efb\u52a1\uff0c\u540e\u8bad\u7ec3\u5bf9\u51bb\u7ed3\u7684 Nirvana \u9aa8\u67b6\u4f7f\u7528\u8f7b\u91cf\u7f16\u89e3\u7801\u5668\u5bf9\u6210\u5bf9\u7684\u7535\u78c1\u4fe1\u53f7\u4e0e MRI \u56fe\u50cf\u8fdb\u884c\u5bf9\u9f50\u548c\u91cd\u5efa\u3002", "result": "\u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u4e0a\uff0cNirvana \u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709 LLM \u67b6\u6784\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u8868\u73b0\u3002\u5bf9\u4e8e\u533b\u5b66\u4efb\u52a1\uff0c\u7279\u522b\u662f MRI\uff0cNirvana \u7684\u91cd\u5efa\u8d28\u91cf\u9ad8\u4e8e\u4f20\u7edf MRI \u6a21\u578b\u53ca\u4ee5\u4f20\u7edf LLM \u4f5c\u4e3a\u9aa8\u5e72\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u51c6\u786e\u7684\u521d\u6b65\u4e34\u5e8a\u62a5\u544a\u3002", "conclusion": "\u4efb\u52a1\u611f\u77e5\u7684\u5185\u5b58\u89e6\u53d1\u4e0e\u4e13\u95e8\u5185\u5b58\u66f4\u65b0\u4f7f Nirvana \u5177\u5907\u5728\u4fdd\u6301\u5e7f\u6cdb\u80fd\u529b\u7684\u540c\u65f6\u5bf9\u76ee\u6807\u9886\u57df\u8fdb\u884c\u4e13\u5bb6\u7ea7\u9002\u914d\u7684\u80fd\u529b\uff0c\u4e14\u5177\u6709\u8f83\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.26086", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26086", "abs": "https://arxiv.org/abs/2510.26086", "authors": ["Zheng Zhang", "Haonan Li", "Xingyu Li", "Hang Zhang", "Zhiyun Qian"], "title": "LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline", "comment": null, "summary": "Bug bisection has been an important security task that aims to understand the\nrange of software versions impacted by a bug, i.e., identifying the commit that\nintroduced the bug. However, traditional patch-based bisection methods are\nfaced with several significant barriers: For example, they assume that the\nbug-inducing commit (BIC) and the patch commit modify the same functions, which\nis not always true. They often rely solely on code changes, while the commit\nmessage frequently contains a wealth of vulnerability-related information. They\nare also based on simple heuristics (e.g., assuming the BIC initializes lines\ndeleted in the patch) and lack any logical analysis of the vulnerability.\n  In this paper, we make the observation that Large Language Models (LLMs) are\nwell-positioned to break the barriers of existing solutions, e.g., comprehend\nboth textual data and code in patches and commits. Unlike previous BIC\nidentification approaches, which yield poor results, we propose a comprehensive\nmulti-stage pipeline that leverages LLMs to: (1) fully utilize patch\ninformation, (2) compare multiple candidate commits in context, and (3)\nprogressively narrow down the candidates through a series of down-selection\nsteps. In our evaluation, we demonstrate that our approach achieves\nsignificantly better accuracy than the state-of-the-art solution by more than\n38\\%. Our results further confirm that the comprehensive multi-stage pipeline\nis essential, as it improves accuracy by 60\\% over a baseline LLM-based\nbisection method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u9636\u6bb5\u7ba1\u7ebf\uff0c\u7528\u4e8e\u5b9a\u4f4d\u5f15\u5165\u6f0f\u6d1e\u7684\u63d0\u4ea4\uff08BIC\uff09\uff0c\u5145\u5206\u5229\u7528\u8865\u4e01\u4fe1\u606f\u548c\u63d0\u4ea4\u6587\u672c\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u51c6\u786e\u7387\u3002\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u6027\u63d0\u5347\u8d85\u8fc738%\uff0c\u76f8\u8f83\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7b80\u5355\u4e8c\u5206\u65b9\u6cd5\u63d0\u5347\u7ea660%\u3002", "motivation": "\u4f20\u7edf\u7684\u8865\u4e01\u5316\u4e8c\u5206\u5b9a\u4f4d\u5b58\u5728\u591a\u9879\u969c\u788d\uff1aBIC\u4e0e\u8865\u4e01\u5f80\u5f80\u5e76\u4e0d\u76f4\u63a5\u4fee\u6539\u540c\u4e00\u51fd\u6570\uff1b\u9664\u4e86\u4ee3\u7801\u53d8\u66f4\uff0c\u63d0\u4ea4\u4fe1\u606f\u5305\u542b\u4e30\u5bcc\u7684\u6f0f\u6d1e\u76f8\u5173\u4fe1\u606f\u5374\u672a\u88ab\u5145\u5206\u5229\u7528\uff1b\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u7b80\u5355\u542f\u53d1\u5f0f\u4e14\u7f3a\u4e4f\u5bf9\u6f0f\u6d1e\u8bed\u4e49\u7684\u5206\u6790\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u6587\u672c\u4e0e\u4ee3\u7801\u4fe1\u606f\u3001\u5e76\u5728\u4e0a\u4e0b\u6587\u4e2d\u6bd4\u8f83\u5019\u9009\u63d0\u4ea4\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u9636\u6bb5\u7ba1\u7ebf\uff0c\u57fa\u4e8eLLMs\uff0c\u5168\u9762\u5229\u7528\u8865\u4e01\u4fe1\u606f\u3001\u5728\u4e0a\u4e0b\u6587\u4e2d\u5bf9\u6bd4\u591a\u4e2a\u5019\u9009\u63d0\u4ea4\u3001\u901a\u8fc7\u9010\u6b65\u4e0b\u7b5b\u7f29\u5c0f\u5019\u9009\u96c6\u3002\u9636\u6bb5\u5305\u62ec\uff1a1) \u5145\u5206\u5229\u7528\u8865\u4e01\u4e2d\u7684\u6587\u672c\u548c\u4ee3\u7801\u4fe1\u606f\uff1b2) \u5728\u4e0a\u4e0b\u6587\u4e2d\u5bf9\u6bd4\u5019\u9009\u63d0\u4ea4\u53ca\u5176\u76f8\u5173\u4fee\u8ba2\uff1b3) \u901a\u8fc7\u4e00\u7cfb\u5217\u4e0b\u7b5b\u6b65\u9aa4\u9010\u6b65\u7f29\u5c0f\u5019\u9009\u96c6\u5408\u76f4\u81f3\u5b9a\u4f4d BIC\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u8d85\u8fc738%\uff1b\u76f8\u6bd4\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u4e8c\u5206\u65b9\u6cd5\uff0c\u63d0\u5347\u7ea660%\uff1b\u5e76\u4e14\u7efc\u5408\u591a\u9636\u6bb5\u7ba1\u7ebf\u662f\u63d0\u5347\u6548\u679c\u7684\u5173\u952e\u3002", "conclusion": "\u8868\u660eLLMs\u5177\u5907\u540c\u65f6\u7406\u89e3\u6587\u672c\u4e0e\u4ee3\u7801\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u4e0b\u7b5b\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6f0f\u6d1e\u5b9a\u4f4d\u6548\u7387\uff0c\u63a8\u52a8\u6f0f\u6d1e\u5b9a\u4f4d\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.26099", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26099", "abs": "https://arxiv.org/abs/2510.26099", "authors": ["Nick Masi", "Randall Balestriero"], "title": "SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth", "comment": null, "summary": "The dominant paradigm in machine learning is to assess model performance\nbased on average loss across all samples in some test set. This amounts to\naveraging performance geospatially across the Earth in weather and climate\nsettings, failing to account for the non-uniform distribution of human\ndevelopment and geography. We introduce Stratified Assessments of Forecasts\nover Earth (SAFE), a package for elucidating the stratified performance of a\nset of predictions made over Earth. SAFE integrates various data domains to\nstratify by different attributes associated with geospatial gridpoints:\nterritory (usually country), global subregion, income, and landcover (land or\nwater). This allows us to examine the performance of models for each individual\nstratum of the different attributes (e.g., the accuracy in every individual\ncountry). To demonstrate its importance, we utilize SAFE to benchmark a zoo of\nstate-of-the-art AI-based weather prediction models, finding that they all\nexhibit disparities in forecasting skill across every attribute. We use this to\nseed a benchmark of model forecast fairness through stratification at different\nlead times for various climatic variables. By moving beyond globally-averaged\nmetrics, we for the first time ask: where do models perform best or worst, and\nwhich models are most fair? To support further work in this direction, the SAFE\npackage is open source and available at https://github.com/N-Masi/safe", "AI": {"tldr": "\u63d0\u51fa SAFE \u5305\u4ee5\u5728\u5730\u7403\u5c3a\u5ea6\u5bf9\u9884\u6d4b\u8fdb\u884c\u5206\u5c42\u8bc4\u4f30\uff0c\u63ed\u793a\u4e0d\u540c\u5730\u7406\u3001\u6536\u5165\u3001\u571f\u5730\u8986\u76d6\u7b49\u5206\u5c42\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5e76\u5efa\u7acb\u516c\u5e73\u6027\u57fa\u51c6\u3002", "motivation": "\u5168\u7403\u5e73\u5747\u6307\u6807\u63a9\u76d6\u5730\u7406\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u5206\u5e03\u7684\u4e0d\u5747\u5bfc\u81f4\u7684\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u5c06\u5730\u7406\u7f51\u683c\u70b9\u6309\u56fd\u5bb6\u3001\u5168\u7403\u5b50\u533a\u57df\u3001\u6536\u5165\u6c34\u5e73\u548c\u571f\u5730\u8986\u76d6\u8fdb\u884c\u5206\u5c42\uff0c\u5bf9\u591a\u79cd\u5929\u6c14\u9884\u6d4b\u6a21\u578b\u7684\u9884\u6d4b\u8fdb\u884c\u9010\u5c42\u8bc4\u4f30\uff0c\u6574\u5408\u591a\u57df\u6570\u636e\u4ee5\u652f\u6301\u5206\u5c42\u5206\u6790\u3002", "result": "\u53d1\u73b0\u591a\u6570\u6a21\u578b\u5728\u4e0d\u540c\u5206\u5c42\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u67d0\u4e9b\u56fd\u5bb6\u3001\u5730\u533a\u3001\u6536\u5165\u6c34\u5e73\u6216\u571f\u5730\u8986\u76d6\u7c7b\u578b\u7684\u9884\u6d4b\u6280\u80fd\u660e\u663e\u4e0d\u8db3\uff1b\u5efa\u7acb\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5c42\u9884\u6d4b\u516c\u5e73\u6027\u57fa\u51c6\u3002", "conclusion": "\u4e0d\u518d\u4f9d\u8d56\u5168\u5c40\u5e73\u5747\u6307\u6807\uff0c\u5f3a\u8c03\u5206\u5c42\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff1bSAFE \u5de5\u5177\u6709\u52a9\u4e8e\u8bc6\u522b\u8584\u5f31\u73af\u8282\u3001\u4fc3\u8fdb\u6a21\u578b\u7684\u516c\u5e73\u6027\u6539\u8fdb\uff0c\u4e14\u5f00\u6e90\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2510.26109", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26109", "abs": "https://arxiv.org/abs/2510.26109", "authors": ["Chenming Tang", "Hsiu-Yuan Huang", "Weijie Liu", "Saiyong Yang", "Yunfang Wu"], "title": "Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error", "comment": "Work in progress", "summary": "Reinforcement learning with verifiable rewards (RLVR) has significantly\nboosted the reasoning capability of large language models (LLMs) recently.\nHowever, existing RLVR approaches merely train LLMs based on their own\ngenerated responses and are constrained by the initial capability of LLMs, thus\nprone to exploration stagnation, in which LLMs fail to solve more training\nproblems and cannot further learn from the training data. Some work tries to\naddress this by leveraging off-policy solutions to training problems but\nrequires external guidance from experts which suffers from limited\navailability. In this work, we propose LTE (Learning to reason from Trial and\nError), an approach hinting LLMs with their previously self-generated incorrect\nanswers and problem of overlong responses, which does not require any external\nexpert guidance. Experiments validate the effectiveness of LTE, which\noutperforms the normal group relative policy optimization (GRPO) by 6.38 in\nPass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for\nQwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the\nproblem of exploration stagnation and enhances both exploitation and\nexploration during training.", "AI": {"tldr": "LTE\u901a\u8fc7\u8ba9LLM\u4ece\u81ea\u751f\u6210\u7684\u9519\u8bef\u7b54\u6848\u548c\u8fc7\u957f\u56de\u7b54\u4e2d\u5b66\u4e60\u8fdb\u884c\u4ece\u8bd5\u9519\u5b66\u4e60\uff0c\u4e0d\u4f9d\u8d56\u5916\u90e8\u4e13\u5bb6\u5f15\u5bfc\uff0c\u7f13\u89e3\u63a2\u7d22\u505c\u6ede\uff0c\u63d0\u5347\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RLVR\u4ec5\u4f9d\u8d56\u6a21\u578b\u81ea\u8eab\u751f\u6210\u56de\u7b54\u3001\u53d7\u521d\u59cb\u80fd\u529b\u9650\u5236\u800c\u6613\u4e8e\u63a2\u7d22\u505c\u6ede\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u5916\u90e8\u5f15\u5bfc\u3002", "method": "\u63d0\u51faLTE\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u4e2d\u901a\u8fc7\u63d0\u793aLLM\u56de\u987e\u5176\u5148\u524d\u7684\u9519\u8bef\u7b54\u6848\u4ee5\u53ca\u56de\u7b54\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u8fdb\u884c\u6539\u8fdb\u7684\u8bd5\u9519\u5b66\u4e60\uff1b\u65e0\u9700\u5916\u90e8\u4e13\u5bb6\u6307\u5bfc\uff0c\u5f3a\u5316\u5bf9\u9519\u5224\u548c\u56de\u7b54\u957f\u5ea6\u7684\u7ba1\u7406\uff1b\u5728Qwen3-4B-Base\u7b49\u6a21\u578b\u548c\u591a\u6570\u5b66\u79d1\u57fa\u51c6\u4e0a\u4e0eGRPO\u7684\u5bf9\u6bd4\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\uff0cLTE\u5bf9Qwen3-4B-Base\u7684Pass@1\u63d0\u9ad8\u4e866.38\uff0cPass@k\u63d0\u9ad8\u4e869.00\uff0c\u663e\u793a\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LTE\u6709\u6548\u7f13\u89e3\u63a2\u7d22\u505c\u6ede\uff0c\u63d0\u5347\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5f00\u53d1\u6027\u4e0e\u63a2\u7d22\u6027\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u4e13\u5bb6\u6307\u5bfc\uff0c\u5177\u6709\u5728RLVR\u573a\u666f\u4e2d\u63a8\u5e7f\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.26146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26146", "abs": "https://arxiv.org/abs/2510.26146", "authors": ["Kexing Liu"], "title": "maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition", "comment": null, "summary": "WiFi Channel State Information (CSI)-based human activity recognition (HAR)\nprovides a privacy-preserving, device-free sensing solution for smart\nenvironments. However, its deployment on edge devices is severely constrained\nby domain shift, where recognition performance deteriorates under varying\nenvironmental and hardware conditions. This study presents maxVSTAR (maximally\nadaptive Vision-guided Sensing Technology for Activity Recognition), a\nclosed-loop, vision-guided model adaptation framework that autonomously\nmitigates domain shift for edge-deployed CSI sensing systems. The proposed\nsystem integrates a cross-modal teacher-student architecture, where a\nhigh-accuracy YOLO-based vision model serves as a dynamic supervisory signal,\ndelivering real-time activity labels for the CSI data stream. These labels\nenable autonomous, online fine-tuning of a lightweight CSI-based HAR model,\ntermed Sensing Technology for Activity Recognition (STAR), directly at the\nedge. This closed-loop retraining mechanism allows STAR to continuously adapt\nto environmental changes without manual intervention. Extensive experiments\ndemonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated\nhardware, the baseline STAR model's recognition accuracy declined from 93.52%\nto 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored\nthe accuracy to 81.51%. These results confirm the system's capacity for\ndynamic, self-supervised model adaptation in privacy-conscious IoT\nenvironments, establishing a scalable and practical paradigm for long-term\nautonomous HAR using CSI sensing at the network edge.", "AI": {"tldr": "maxVSTAR \u662f\u4e00\u4e2a\u57fa\u4e8e\u8de8\u6a21\u6001\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u7684\u95ed\u73af\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u4e2aYOLO\u89c6\u89c9\u6a21\u578b\u5bf9CSI\u6570\u636e\u6d41\u8fdb\u884c\u5b9e\u65f6\u6807\u7b7e\uff0c\u5b9e\u73b0\u8fb9\u7f18\u7aef\u7684\u5728\u7ebf\u81ea\u76d1\u7763\u5fae\u8c03\uff08STAR\uff09\u3002\u5728\u672a\u6821\u51c6\u786c\u4ef6\u4e0a\u663e\u8457\u7f13\u89e3\u57df\u79fb\uff0c\u5c06\u51c6\u786e\u7387\u4ece49.14%\u63d0\u9ad8\u523081.51%\uff0c\u5e76\u5728\u5bf9\u7167\u57fa\u7ebf\u4e0b\u8fbe\u5230\u9690\u79c1\u53cb\u597d\u3001\u53ef\u6269\u5c55\u7684\u957f\u671fHAR\u3002", "motivation": "CSI \u57fa\u4e8e\u901a\u9053\u72b6\u6001\u4fe1\u606f\u7684HAR\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u6613\u53d7\u73af\u5883\u548c\u786c\u4ef6\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u57df\u79fb\u540e\u6027\u80fd\u4e0b\u964d\uff1b\u9700\u8981\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3001\u9690\u79c1\u53cb\u597d\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u6765\u5b9e\u73b0\u957f\u671f\u81ea\u6cbb\u7684HAR\u3002", "method": "\u63d0\u51fa\u95ed\u73af\u7684\u8de8\u6a21\u6001\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u4f7f\u7528YOLO\u7b49\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u6a21\u578b\u4f5c\u4e3a\u52a8\u6001\u76d1\u7763\u4fe1\u53f7\uff0c\u4e3aCSI\u6570\u636e\u63d0\u4f9b\u5b9e\u65f6\u6d3b\u52a8\u6807\u7b7e\uff1b\u8fb9\u7f18\u7aef\u5bf9\u8f7b\u91cf\u7ea7\u7684STAR\u6a21\u578b\u8fdb\u884c\u5728\u7ebf\u5fae\u8c03\uff0c\u5b8c\u6210\u81ea\u76d1\u7763\u7684\u5728\u7ebf\u91cd\u8bad\u7ec3\u3002", "result": "\u5728\u672a\u6821\u51c6\u786c\u4ef6\u6761\u4ef6\u4e0b\uff0c\u57fa\u7ebfSTAR\u51c6\u786e\u7387\u753193.52%\u4e0b\u964d\u81f349.14%\uff1b\u7ecf\u8fc7\u4e00\u6b21\u89c6\u89c9\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u5faa\u73af\uff0c\u51c6\u786e\u7387\u56de\u5347\u81f381.51%\uff0c\u8bc1\u660e\u4e86\u5728\u9690\u79c1\u654f\u611f\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8fdb\u884c\u52a8\u6001\u81ea\u9002\u5e94\u7684\u6709\u6548\u6027\u3002", "conclusion": "maxVSTAR\u5b9e\u73b0\u4e86\u9762\u5411\u8fb9\u7f18CSI HAR\u7684\u52a8\u6001\u81ea\u9002\u5e94\uff0c\u501f\u52a9\u89c6\u89c9\u4fe1\u53f7\u8fdb\u884c\u81ea\u76d1\u7763\u5fae\u8c03\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.26148", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26148", "abs": "https://arxiv.org/abs/2510.26148", "authors": ["Kexing Liu"], "title": "STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments", "comment": null, "summary": "Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)\npresents a privacy-preserving, contactless sensing approach suitable for smart\nhomes, healthcare monitoring, and mobile IoT systems. However, existing methods\noften encounter computational inefficiency, high latency, and limited\nfeasibility within resource-constrained, embedded mobile edge environments.\nThis paper proposes STAR (Sensing Technology for Activity Recognition), an\nedge-AI-optimized framework that integrates a lightweight neural architecture,\nadaptive signal processing, and hardware-aware co-optimization to enable\nreal-time, energy-efficient HAR on low-power embedded devices. STAR\nincorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural\nnetwork, reducing model parameters by 33% compared to conventional LSTM models\nwhile maintaining effective temporal modeling capability. A multi-stage\npre-processing pipeline combining median filtering, 8th-order Butterworth\nlow-pass filtering, and Empirical Mode Decomposition (EMD) is employed to\ndenoise CSI amplitude data and extract spatial-temporal features. For on-device\ndeployment, STAR is implemented on a Rockchip RV1126 processor equipped with an\nembedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI\nacquisition module. Experimental results demonstrate a mean recognition\naccuracy of 93.52% across seven activity classes and 99.11% for human presence\ndetection, utilizing a compact 97.6k-parameter model. INT8 quantized inference\nachieves a processing speed of 33 MHz with just 8% CPU utilization, delivering\nsixfold speed improvements over CPU-based execution. With sub-second response\nlatency and low power consumption, the system ensures real-time,\nprivacy-preserving HAR, offering a practical, scalable solution for mobile and\npervasive computing environments.", "AI": {"tldr": "STAR\u662f\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u3001\u8f7b\u91cf\u5316\u8fb9\u7f18\u7aef\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6846\u67b6\uff0c\u901a\u8fc7GRU\u4e3a\u6838\u5fc3\u7684\u8f7b\u91cf\u5316\u65f6\u5e8f\u6a21\u578b\u3001\u5206\u9636\u6bb5\u4fe1\u53f7\u964d\u566a\uff0c\u4ee5\u53ca\u786c\u4ef6\u611f\u77e5\u7684\u534f\u540c\u4f18\u5316\uff0c\u5728\u4f4e\u529f\u8017\u5d4c\u5165\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u5b9e\u65f6HAR\uff0c\u5728\u4e03\u7c7b\u6d3b\u52a8\u4e0a\u7684\u5e73\u5747\u8bc6\u522b\u7387\u8fbe\u523093.52%\u5e76\u5177\u5907\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u80fd\u6548\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709HAR\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u79fb\u52a8\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u65f6\u5ef6\u9ad8\u3001\u53ef\u884c\u6027\u5dee\u7b49\u95ee\u9898\uff1b\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u65e0\u63a5\u89e6\u76d1\u6d4b\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u5bb6\u5c45\u3001\u5065\u5eb7\u76d1\u6d4b\u548c\u79fb\u52a8\u7269\u8054\u7f51\u3002", "method": "\u63d0\u51faSTAR\u6846\u67b6\uff0c\u91c7\u7528\u8f7b\u91cfGRURNN\uff0c\u53c2\u6570\u6bd4\u4f20\u7edfLSTM\u51cf\u5c11\u7ea633%\uff0c\u5e76\u5f15\u5165\u591a\u9636\u6bb5\u9884\u5904\u7406\u6d41\u6c34\u7ebf\uff08\u4e2d\u503c\u6ee4\u6ce2\u30018\u9636\u5df4\u7279\u6c83\u65af\u4f4e\u901a\u6ee4\u6ce2\u3001\u7ecf\u9a8c\u6a21\u6001\u5206\u89e3\uff09\u4ee5\u53bb\u566a\u5e76\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\uff1b\u5728Rockchip RV1126+\u5d4c\u5165\u5f0fNPU\u4e0a\u8fdb\u884c\u7aef\u4fa7\u90e8\u7f72\uff0cCSI\u91c7\u96c6\u6a21\u5757\u4e3aESP32-S3\uff1b\u91cf\u5316\u4e3aINT8\uff0c\u63a8\u7406\u9891\u738733MHz\uff0cCPU\u5360\u7528\u7ea68%\uff0c\u4e0eCPU\u7aef\u76f8\u6bd4\u5b9e\u73b0\u516d\u500d\u52a0\u901f\uff1b\u6a21\u578b\u53c2\u6570\u7ea697.6k\u3002", "result": "\u5728\u4e03\u7c7b\u6d3b\u52a8\u4e0a\u5e73\u5747\u8bc6\u522b\u51c6\u786e\u7387\u4e3a93.52%\uff1b\u5bf9\u4eba\u4f53\u5b58\u5728\u68c0\u6d4b\u8fbe\u523099.11%\uff1bINT8\u63a8\u7406\u4e0b processing speed 33 MHz\uff0c\u529f\u8017\u4f4e\uff0c\u5177\u5907\u4e9a\u79d2\u7ea7\u54cd\u5e94\uff1b\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u9690\u79c1\u4fdd\u62a4\u7684HAR\u3002", "conclusion": "STAR\u63d0\u4f9b\u4e00\u79cd\u9762\u5411\u79fb\u52a8\u4e0e\u666e\u9002\u8ba1\u7b97\u73af\u5883\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u3001\u4f4e\u5ef6\u8fdf\u548c\u9690\u79c1\u4fdd\u62a4\u7279\u6027\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u573a\u666f\u3002"}}
{"id": "2510.26157", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26157", "abs": "https://arxiv.org/abs/2510.26157", "authors": ["Hyuntae Park", "Yeachan Kim", "SangKeun Lee"], "title": "Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment", "comment": "EMNLP 2025 (main)", "summary": "Molecule and text representation learning has gained increasing interest due\nto its potential for enhancing the understanding of chemical information.\nHowever, existing models often struggle to capture subtle differences between\nmolecules and their descriptions, as they lack the ability to learn\nfine-grained alignments between molecular substructures and chemical phrases.\nTo address this limitation, we introduce MolBridge, a novel molecule-text\nlearning framework based on substructure-aware alignments. Specifically, we\naugment the original molecule-description pairs with additional alignment\nsignals derived from molecular substructures and chemical phrases. To\neffectively learn from these enriched alignments, MolBridge employs\nsubstructure-aware contrastive learning, coupled with a self-refinement\nmechanism that filters out noisy alignment signals. Experimental results show\nthat MolBridge effectively captures fine-grained correspondences and\noutperforms state-of-the-art baselines on a wide range of molecular benchmarks,\nhighlighting the significance of substructure-aware alignment in molecule-text\nlearning.", "AI": {"tldr": "MolBridge \u4f1a\u4f7f\u7528\u57fa\u4e8e\u5b50\u7ed3\u6784\u7684\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u914d\u5408\u81ea\u6211\u7cbe\u70bc\u673a\u5236\uff0c\u5c06\u5206\u5b50\u5b50\u7ed3\u6784\u4e0e\u5316\u5b66\u77ed\u8bed\u5bf9\u9f50\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u5b50-\u6587\u672c\u8868\u793a\uff0c\u5e76\u5728\u591a\u9879\u5206\u5b50\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5b50-\u6587\u672c\u8868\u793a\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u6355\u6349\u5206\u5b50\u4e0e\u63cf\u8ff0\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u5bfc\u81f4\u5bf9\u76f8\u4f3c\u5206\u5b50\u6216\u63cf\u8ff0\u7684\u533a\u5206\u80fd\u529b\u4e0d\u8db3\u3002\u9700\u8981\u5f15\u5165\u5b50\u7ed3\u6784\u5c42\u9762\u7684\u5bf9\u9f50\u4fe1\u53f7\u6765\u589e\u5f3a\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u5728\u5206\u5b50-\u63cf\u8ff0\u5bf9\u4e2d\u589e\u52a0\u6765\u81ea\u5206\u5b50\u5b50\u7ed3\u6784\u4e0e\u5316\u5b66\u77ed\u8bed\u7684\u5bf9\u9f50\u4fe1\u53f7\uff1b\u5f15\u5165\u5b50\u7ed3\u6784\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u5b66\u4e60\u66f4\u7ec6\u7c92\u5ea6\u7684\u8de8\u6a21\u6001\u8868\u793a\uff1b\u5f15\u5165\u81ea\u6211\u7cbe\u70bc\u673a\u5236\u4ee5\u6ee4\u9664\u566a\u58f0\u5bf9\u9f50\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u79cd\u5206\u5b50\u57fa\u51c6\u4e0a\uff0cMolBridge \u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u8868\u660e\u5b50\u7ed3\u6784\u611f\u77e5\u5bf9\u9f50\u5728\u5206\u5b50-\u6587\u672c\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b50\u7ed3\u6784\u611f\u77e5\u5bf9\u9f50\u662f\u63d0\u5347\u5206\u5b50-\u6587\u672c\u7406\u89e3\u7684\u6709\u6548\u65b9\u5411\uff0cMolBridge \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u53ef\u8fdb\u4e00\u6b65\u62d3\u5c55\u5230\u5176\u4ed6\u8de8\u6a21\u6001\u6216\u9886\u57df\u3002"}}
{"id": "2510.26159", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26159", "abs": "https://arxiv.org/abs/2510.26159", "authors": ["Emilio Mastriani", "Alessandro Costa", "Federico Incardona", "Kevin Munari", "Sebastiano Spinello"], "title": "Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series", "comment": "This paper is currently under review for presentation at the IEEE\n  SAMI 2026 Conference", "summary": "In this study, we investigate the effectiveness of advanced feature\nengineering and hybrid model architectures for anomaly detection in a\nmultivariate industrial time series, focusing on a steam turbine system. We\nevaluate the impact of change point-derived statistical features,\nclustering-based substructure representations, and hybrid learning strategies\non detection performance. Despite their theoretical appeal, these complex\napproaches consistently underperformed compared to a simple Random Forest +\nXGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of\n0.976, F1-score of 0.41, and 100% early detection within the defined time\nwindow. Our findings highlight that, in scenarios with highly imbalanced and\ntemporally uncertain data, model simplicity combined with optimized\nsegmentation can outperform more sophisticated architectures, offering greater\nrobustness, interpretability, and operational utility.", "AI": {"tldr": "\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u4e14\u65f6\u95f4\u4e0a\u4e0d\u786e\u5b9a\u7684\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u4e2d\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u968f\u673a\u68ee\u6797+XGBoost\u96c6\u6210\u5e76\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u6bb5\u5904\u7406\uff0c\u4f18\u4e8e\u590d\u6742\u7684\u7279\u5f81\u5de5\u7a0b\u4e0e\u6df7\u5408\u6a21\u578b\u5728\u7206\u53d1\u6027\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30\u5148\u8fdb\u7279\u5f81\u5de5\u7a0b\u548c\u6df7\u5408\u6a21\u578b\u67b6\u6784\u5728\u591a\u53d8\u91cf\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u53d7\u9650\u7684\u6837\u672c\u4e0d\u5e73\u8861\u548c\u65f6\u95f4\u4e0d\u786e\u5b9a\u6027\u60c5\u5883\u4e0b\uff0c\u662f\u5426\u9700\u8981\u56de\u5f52\u5230\u66f4\u7b80\u5355\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002", "method": "\u6bd4\u8f83\u591a\u79cd\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u53d8\u5316\u70b9\u7684\u7edf\u8ba1\u7279\u5f81\u30012) \u57fa\u4e8e\u805a\u7c7b\u7684\u5b50\u7ed3\u6784\u8868\u5f81\u30013) \u6df7\u5408\u5b66\u4e60\u7b56\u7565\u7b49\u590d\u6742\u65b9\u6cd5\uff1b\u4ee5\u53ca 4) \u5728\u5206\u6bb5\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u7b80\u5355\u968f\u673a\u68ee\u6797+XGBoost \u96c6\u6210\u3002\u901a\u8fc7\u8bc4\u4f30 AUC-ROC\u3001F1 \u548c\u63d0\u65e9\u68c0\u6d4b\u7387\u7b49\u6307\u6807\u6765\u6bd4\u8f83\u6027\u80fd\u3002", "result": "\u590d\u6742\u65b9\u6cd5\u5747\u4e0d\u53ca\u7b80\u5355\u7684\u968f\u673a\u68ee\u6797+XGBoost \u96c6\u6210\u3002\u540e\u8005\u5728\u5206\u6bb5\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86 AUC-ROC 0.976\u3001F1 0.41\uff0c\u4ee5\u53ca\u5728\u6307\u5b9a\u65f6\u95f4\u7a97\u5185100%\u7684\u65e9\u671f\u68c0\u6d4b\u7387\u3002", "conclusion": "\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u4e14\u5b58\u5728\u65f6\u95f4\u4e0d\u786e\u5b9a\u6027\u7684\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7b80\u5316\u3001\u7ed3\u5408\u4f18\u5316\u5206\u6bb5\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u4f18\u4e8e\u8fc7\u4e8e\u590d\u6742\u7684\u67b6\u6784\u3002"}}
{"id": "2510.26184", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.26184", "abs": "https://arxiv.org/abs/2510.26184", "authors": ["Songxin Lei", "Qiongyan Wang", "Yanchen Zhu", "Hanyu Yao", "Sijie Ruan", "Weilin Ruan", "Yuyu Luo", "Huaming Wu", "Yuxuan Liang"], "title": "A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation", "comment": null, "summary": "Public resource allocation involves the efficient distribution of resources,\nincluding urban infrastructure, energy, and transportation, to effectively meet\nsocietal demands. However, existing methods focus on optimizing the movement of\nindividual resources independently, without considering their capacity\nconstraints. To address this limitation, we propose a novel and more practical\nproblem: Collaborative Public Resource Allocation (CPRA), which explicitly\nincorporates capacity constraints and spatio-temporal dynamics in real-world\nscenarios. We propose a new framework called Game-Theoretic Spatio-Temporal\nReinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:\n1) We formulate the CPRA problem as a potential game and demonstrate that there\nis no gap between the potential function and the optimal target, laying a solid\ntheoretical foundation for approximating the Nash equilibrium of this NP-hard\nproblem; and 2) Our designed GSTRL framework effectively captures the\nspatio-temporal dynamics of the overall system. We evaluate GSTRL on two\nreal-world datasets, where experiments show its superior performance. Our\nsource codes are available in the supplementary materials.", "AI": {"tldr": "A new collaborative public resource allocation problem with capacity constraints and spatio-temporal dynamics; proposes a game-theoretic spatio-temporal RL framework (GSTRL) to approximate Nash equilibrium via a potential game, showing superior performance on real datasets.", "motivation": "Existing public resource allocation methods optimize individual resource movements and ignore capacity constraints and joint spatio-temporal dynamics, making the problem NP-hard and impractical for real-world systems.", "method": "Formulate CPRA as a potential game and develop the GSTRL framework that captures spatio-temporal dynamics. The key theoretical result is that the potential function aligns with the optimal target, enabling approximation of Nash equilibrium for the NP-hard problem.", "result": "Empirical evaluation on two real-world datasets demonstrates superior performance of GSTRL; code is provided in supplementary materials.", "conclusion": "GSTRL provides a theoretically grounded and practically effective approach to CPRA by uniting game theory with spatio-temporal RL, with potential for broader impact and future extensions."}}
{"id": "2510.26188", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26188", "abs": "https://arxiv.org/abs/2510.26188", "authors": ["Avinash Kadimisetty", "Arun Rajagopalan", "Vijendra SK"], "title": "Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients", "comment": "NCMLAI 2018", "summary": "Reducing preventable hospital readmissions is a national priority for payers,\nproviders, and policymakers seeking to improve health care and lower costs. The\nrate of readmission is being used as a benchmark to determine the quality of\nhealthcare provided by the hospitals. In thisproject, we have used machine\nlearning techniques like Logistic Regression, Random Forest and Support Vector\nMachines to analyze the health claims data and identify demographic and medical\nfactors that play a crucial role in predicting all-cause readmissions. As the\nhealth claims data is high dimensional, we have used Principal Component\nAnalysis as a dimension reduction technique and used the results for building\nregression models. We compared and evaluated these models based on the Area\nUnder Curve (AUC) metric. Random Forest model gave the highest performance\nfollowed by Logistic Regression and Support Vector Machine models. These models\ncan be used to identify the crucial factors causing readmissions and help\nidentify patients to focus on to reduce the chances of readmission, ultimately\nbringing down the cost and increasing the quality of healthcare provided to the\npatients.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\uff08\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001SVM\uff09\u5728\u9ad8\u7ef4\u5065\u5eb7\u9669\u6570\u636e\u4e0a\u901a\u8fc7\u4e3b\u6210\u5206\u5206\u6790\u964d\u7ef4\u6765\u9884\u6d4b\u5168\u56e0\u8bfb\u56de\u5e76\u6bd4\u8f83AUC\uff0c\u7ed3\u679c\u663e\u793a\u968f\u673a\u68ee\u6797\u8868\u73b0\u6700\u4f73\uff0c\u5176\u5b83\u6a21\u578b\u6b21\u4e4b\u3002", "motivation": "\u76ee\u6807\u662f\u964d\u4f4e\u53ef\u9884\u9632\u7684\u4f4f\u9662\u518d\u5165\u9662\u7387\uff0c\u8bfb\u56de\u7387\u4f5c\u4e3a\u533b\u9662\u8d28\u91cf\u7684\u57fa\u51c6\uff0c\u5229\u7528 claims \u6570\u636e\u8bc6\u522b\u5173\u952e\u56e0\u7d20\uff0c\u4ee5\u4fbf\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u9700\u91cd\u70b9\u5173\u6ce8\u7684\u60a3\u8005\uff0c\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u5347\u62a4\u7406\u8d28\u91cf\u3002", "method": "\u5728\u9ad8\u7ef4\u5065\u5eb7\u4fdd\u9669\u7406\u8d54\u6570\u636e\u4e0a\u5e94\u7528\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001SVM\uff0c\u5e76\u901a\u8fc7PCA\u964d\u7ef4\u540e\u6784\u5efa\u56de\u5f52/\u5206\u7c7b\u6a21\u578b\uff1b\u4ee5AUC\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u3002", "result": "\u968f\u673a\u68ee\u6797\u8fbe\u5230\u6700\u9ad8AUC\uff0c\u5176\u6b21\u662f\u903b\u8f91\u56de\u5f52\uff0c\u518d\u662fSVM\uff1b\u4e09\u8005\u53ef\u7528\u4e8e\u8bc6\u522b\u5bfc\u81f4\u518d\u5165\u9662\u7684\u5173\u952e\u56e0\u7d20\u5e76\u5b9a\u4f4d\u9ad8\u98ce\u9669\u60a3\u8005\u3002", "conclusion": "\u8fd9\u4e9b\u6a21\u578b\u53ef\u5e2e\u52a9\u8bc6\u522b\u5173\u952e\u56e0\u7d20\u5e76\u5e2e\u52a9\u5b9a\u5411\u5e72\u9884\uff0c\u4ece\u800c\u964d\u4f4e\u518d\u5165\u9662\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3001\u63d0\u5347\u62a4\u7406\u8d28\u91cf\u3002"}}
{"id": "2510.26219", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26219", "abs": "https://arxiv.org/abs/2510.26219", "authors": ["Sekitoshi Kanai", "Tsukasa Yoshida", "Hiroshi Takahashi", "Haru Kuroki", "Kazumune Hashimoto"], "title": "Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space", "comment": "21 pages, 8 figures", "summary": "Test-time alignment of large language models (LLMs) attracts attention\nbecause fine-tuning LLMs requires high computational costs. In this paper, we\npropose a new test-time alignment method called adaptive importance sampling on\npre-logits (AISP) on the basis of the sampling-based model predictive control\nwith the stochastic control input. AISP applies the Gaussian perturbation into\npre-logits, which are outputs of the penultimate layer, so as to maximize\nexpected rewards with respect to the mean of the perturbation. We demonstrate\nthat the optimal mean is obtained by importance sampling with sampled rewards.\nAISP outperforms best-of-n sampling in terms of rewards over the number of used\nsamples and achieves higher rewards than other reward-based test-time alignment\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u5bf9\u9f50\u7684\u9ad8\u6548\u65b9\u6cd5AISP\uff08adaptive importance sampling on pre-logits\uff09\uff0c\u901a\u8fc7\u5bf9\u9884 logits \u5904\u7684\u9ad8\u65af\u6270\u52a8\u6765\u6700\u5927\u5316\u671f\u671b\u5956\u52b1\uff0c\u5e76\u5229\u7528\u91cd\u8981\u6027\u91c7\u6837\u4f30\u8ba1\u6700\u4f73\u6270\u52a8\u5747\u503c\u3002\u76f8\u6bd4best-of-n\u4e0e\u5176\u4ed6\u57fa\u4e8e\u5956\u52b1\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\uff0cAISP\u5728\u6837\u672c\u6548\u7387\u548c\u5956\u52b1\u6c34\u5e73\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8fdb\u884c\u5bf9\u9f50\u65f6\u9700\u8981\u9ad8\u6602\u5fae\u8c03\u6210\u672c\u7684\u95ee\u9898\uff1b\u5728\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u91c7\u6837\u4e0e\u63a7\u5236\u8f93\u5165\u5b9e\u73b0\u5bf9\u9f50\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u4ee3\u4ef7\u3002", "method": "\u5728\u6a21\u578b\u7684\u524d\u4e00\u9690\u85cf\u5c42\u8f93\u51fa\uff08pre-logits\uff09\u5f15\u5165\u9ad8\u65af\u6270\u52a8\uff0c\u4f18\u5316\u6270\u52a8\u5747\u503c\u4ee5\u6700\u5927\u5316\u671f\u671b\u5956\u52b1\uff1b\u6700\u4f18\u5747\u503c\u901a\u8fc7\u5bf9\u6837\u672c\u5956\u52b1\u7684\u91cd\u8981\u6027\u91c7\u6837\u6765\u4f30\u8ba1\uff1b\u4e0ebest-of-n\u548c\u5176\u4ed6\u57fa\u4e8e\u5956\u52b1\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\u6bd4\u8f83\u3002", "result": "AISP\u5728\u53ef\u7528\u6837\u672c\u6570\u91cf\u65b9\u9762\u5b9e\u73b0\u66f4\u9ad8\u7684\u5956\u52b1\uff0c\u4e14\u4f18\u4e8ebest-of-n\uff1b\u5728\u5956\u52b1\u8868\u73b0\u65b9\u9762\u4e5f\u4f18\u4e8e\u5176\u4ed6\u5956\u52b1\u9a71\u52a8\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\u3002", "conclusion": "AISP\u4e3a\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u57fa\u4e8e\u6270\u52a8\u4e0e\u91cd\u8981\u6027\u91c7\u6837\u7684\u65b0\u601d\u8def\uff0c\u80fd\u5728\u4e0d\u9ad8\u6210\u672c\u5fae\u8c03\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u5956\u52b1\u8868\u73b0\u3002"}}
{"id": "2510.26230", "categories": ["cs.LG", "cs.AI", "68T09 68T09"], "pdf": "https://arxiv.org/pdf/2510.26230", "abs": "https://arxiv.org/abs/2510.26230", "authors": ["Minyi Peng", "Darian Gunamardi", "Ivan Tjuawinata", "Kwok-Yan Lam"], "title": "MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines", "comment": "10 pages, 6 figures", "summary": "As a new and promising approach, existing machine unlearning (MU) works\ntypically emphasize theoretical formulations or optimization objectives to\nachieve knowledge removal. However, when deployed in real-world scenarios, such\nsolutions typically face scalability issues and have to address practical\nrequirements such as full access to original datasets and model. In contrast to\nthe existing approaches, we regard classification training as a sequential\nprocess where classes are learned sequentially, which we call \\emph{inductive\napproach}. Unlearning can then be done by reversing the last training sequence.\nThis is implemented by appending a projection-redistribution layer in the end\nof the model. Such an approach does not require full access to the original\ndataset or the model, addressing the challenges of existing methods. This\nenables modular and model-agnostic deployment as an output filter into existing\nclassification pipelines with minimal alterations. We conducted multiple\nexperiments across multiple datasets including image (CIFAR-10/100 using\nCNN-based model) and tabular datasets (Covertype using tree-based model).\nExperiment results show consistently similar output to a fully retrained model\nwith a high computational cost reduction. This demonstrates the applicability,\nscalability, and system compatibility of our solution while maintaining the\nperformance of the output in a more practical setting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u987a\u5e8f\u5b66\u4e60\u7684\u201c\u5f52\u7eb3\u201d\u89c6\u89d2\u7684\u673a\u5668\u64a4\u9500\u65b9\u6cd5\uff1a\u5728\u6a21\u578b\u672b\u5c3e\u8ffd\u52a0\u6295\u5f71-\u518d\u5206\u914d\u5c42\uff0c\u901a\u8fc7\u9006\u5e8f\u6700\u540e\u4e00\u4e2a\u8bad\u7ec3\u5e8f\u5217\u5b9e\u73b0\u77e5\u8bc6\u5265\u79bb\uff0c\u5177\u5907\u6a21\u5757\u5316\u3001\u6a21\u578b\u65e0\u5173\u548c\u5bf9\u539f\u59cb\u6570\u636e/\u6a21\u578b\u5c11\u8bbf\u95ee\u7684\u7279\u70b9\uff0c\u5b9e\u9a8c\u5728 CIFAR-10/100\uff08CNN\uff09\u548c Covertype\uff08\u6811\u6a21\u578b\uff09\u4e0a\u63a5\u8fd1\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u8f93\u51fa\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u673a\u5668 unlearning \u65b9\u6cd5\u591a\u805a\u7126\u4e8e\u7406\u8bba\u5b9a\u4e49\u6216\u4f18\u5316\u76ee\u6807\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5f80\u5f80\u53d7\u9650\u4e8e\u9700\u8981\u5b8c\u6574\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u548c\u6a21\u578b\u3001\u96be\u4ee5\u6269\u5c55\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u53ef\u4e0e\u73b0\u6709\u7ba1\u7ebf\u65e0\u7f1d\u96c6\u6210\u3001\u4e14\u5bf9\u8d44\u6e90\u548c\u6570\u636e\u8bbf\u95ee\u8981\u6c42\u4f4e\u7684\u64a4\u9500\u65b9\u6848\u3002", "method": "\u5c06\u5206\u7c7b\u8bad\u7ec3\u89c6\u4e3a\u9010\u7c7b\u5b66\u4e60\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u9006\u5411\u6700\u540e\u7684\u8bad\u7ec3\u5e8f\u5217\u5b9e\u73b0\u64a4\u9500\uff1b\u5728\u6a21\u578b\u672b\u7aef\u65b0\u589e\u4e00\u4e2a\u6295\u5f71-\u518d\u5206\u914d\u5c42\uff0c\u4f5c\u4e3a\u8f93\u51fa\u8fc7\u6ee4\u5668\uff0c\u5b8c\u6210\u77e5\u8bc6\u5265\u79bb\uff0c\u6a21\u5757\u5316\u3001\u65e0\u987b\u5927\u89c4\u6a21\u6570\u636e\u8bbf\u95ee\u5373\u53ef\u96c6\u6210\u5230\u73b0\u6709\u5206\u7c7b\u7cfb\u7edf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5305\u62ec CIFAR-10/100\uff08CNN \u6a21\u578b\uff09\u548c Covertype\uff08\u6811\u6a21\u578b\uff09\uff0c\u5b9e\u9a8c\u663e\u793a\u8f93\u51fa\u4e0e\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u6a21\u578b\u975e\u5e38\u63a5\u8fd1\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u6210\u672c\u4e0a\u6709\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684 applicability\u3001 scalability \u4e0e compatibility\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u5e72\u9884\u5b9e\u73b0\u6709\u6548\u7684\u77e5\u8bc6\u64a4\u56de\uff0c\u9002\u5408\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u843d\u5730\u90e8\u7f72\u3002"}}
{"id": "2510.26243", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26243", "abs": "https://arxiv.org/abs/2510.26243", "authors": ["Hieu M. Vu", "Tan M. Nguyen"], "title": "Angular Steering: Behavior Control via Rotation in Activation Space", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Controlling specific behaviors in large language models while preserving\ntheir general capabilities is a central challenge for safe and reliable\nartificial intelligence deployment. Current steering methods, such as vector\naddition and directional ablation, are constrained within a two-dimensional\nsubspace defined by the activation and feature direction, making them sensitive\nto chosen parameters and potentially affecting unrelated features due to\nunintended interactions in activation space. We introduce Angular Steering, a\nnovel and flexible method for behavior modulation that operates by rotating\nactivations within a fixed two-dimensional subspace. By formulating steering as\na geometric rotation toward or away from a target behavior direction, Angular\nSteering provides continuous, fine-grained control over behaviors such as\nrefusal and compliance. We demonstrate this method using refusal steering\nemotion steering as use cases. Additionally, we propose Adaptive Angular\nSteering, a selective variant that rotates only activations aligned with the\ntarget feature, further enhancing stability and coherence. Angular Steering\ngeneralizes existing addition and orthogonalization techniques under a unified\ngeometric rotation framework, simplifying parameter selection and maintaining\nmodel stability across a broader range of adjustments. Experiments across\nmultiple model families and sizes show that Angular Steering achieves robust\nbehavioral control while maintaining general language modeling performance,\nunderscoring its flexibility, generalization, and robustness compared to prior\napproaches. Code and artifacts are available at\nhttps://github.com/lone17/angular-steering/.", "AI": {"tldr": "\u63d0\u51fa Angular Steering\uff0c\u901a\u8fc7\u5728\u56fa\u5b9a\u4e8c\u7ef4\u5b50\u7a7a\u95f4\u5185\u65cb\u8f6c\u6fc0\u6d3b\u6765\u63a7\u5236\u6a21\u578b\u884c\u4e3a\uff0c\u63d0\u4f9b\u8fde\u7eed\u53ef\u8c03\u7684\u62d2\u7edd/\u670d\u4ece\u7b49\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7 Adaptive Angular Steering \u589e\u5f3a\u7a33\u5b9a\u6027\u3002", "motivation": "\u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u5bf9\u7279\u5b9a\u884c\u4e3a\u7684\u7a33\u5065\u63a7\u5236\u662f\u5b89\u5168\u53ef\u9760\u90e8\u7f72\u7684\u6838\u5fc3\u6311\u6218\uff1b\u73b0\u6709\u7684\u5411\u91cf\u52a0\u6cd5/\u65b9\u5411\u6291\u5236\u7b49\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e8c\u7ef4\u5b50\u7a7a\u95f4\uff0c\u6613\u5bf9\u975e\u76ee\u6807\u7279\u5f81\u4ea7\u751f\u526f\u4f5c\u7528\u5e76\u5bf9\u53c2\u6570\u654f\u611f\u3002", "method": "\u5c06\u884c\u4e3a\u5f15\u5bfc\u89c6\u4e3a\u5728\u4e00\u4e2a\u56fa\u5b9a\u7684\u4e8c\u7ef4\u5b50\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5411\u91cf\u65cb\u8f6c\uff08\u51e0\u4f55\u65cb\u8f6c\uff09\uff0c\u901a\u8fc7\u671d\u5411\u6216\u8fdc\u79bb\u76ee\u6807\u884c\u4e3a\u65b9\u5411\u6765\u5b9e\u73b0\u5e72\u9884\u3002\u63d0\u51fa Angular Steering\uff0c\u5e76\u6269\u5c55\u4e3a Adaptive Angular Steering\uff0c\u4ec5\u65cb\u8f6c\u4e0e\u76ee\u6807\u7279\u5f81\u5bf9\u9f50\u7684\u6fc0\u6d3b\u3002\u5c06\u5176\u4f5c\u4e3a\u7edf\u4e00\u7684\u65cb\u8f6c\u6846\u67b6\uff0c\u8986\u76d6\u5e76\u7b80\u5316\u65e2\u6709\u52a0\u6cd5/\u6b63\u4ea4\u5316\u7b49\u6280\u672f\u7684\u53c2\u6570\u9009\u62e9\uff0c\u4fdd\u6301\u6a21\u578b\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u591a\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u6027\u3002", "result": "\u5728\u4fdd\u6301\u901a\u7528\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684\u884c\u4e3a\u63a7\u5236\uff1b\u76f8\u6bd4\u524d\u6848\uff0cAngular Steering\u66f4\u5177\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\uff0cAdaptive \u8fdb\u4e00\u6b65\u63d0\u5347\u7a33\u5b9a\u6027\u4e0e\u4e00\u81f4\u6027\uff1b\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u5927\u5c0f\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u826f\u597d\u6cdb\u5316\u3002", "conclusion": "\u7ed9\u51fa\u4e00\u4e2a\u7edf\u4e00\u3001\u7075\u6d3b\u4e14\u7a33\u5b9a\u7684\u884c\u4e3a\u63a7\u5236\u6846\u67b6\uff0c\u53c2\u6570\u66f4\u6613\u9009\u53d6\uff0c\u9002\u7528\u8303\u56f4\u66f4\u5e7f\uff1b\u5e76\u63d0\u4f9b\u4ee3\u7801\u4e0e\u5b9e\u9a8c\u6750\u6599\u4ee5\u4f9b\u590d\u73b0\u3002"}}
{"id": "2510.26266", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26266", "abs": "https://arxiv.org/abs/2510.26266", "authors": ["Frederik M\u00f6bius Rygaard", "Shen Zhu", "Yinzhu Jin", "S\u00f8ren Hauberg", "Tom Fletcher"], "title": "Likely Interpolants of Generative Models", "comment": null, "summary": "Interpolation in generative models allows for controlled generation, model\ninspection, and more. Unfortunately, most generative models lack a principal\nnotion of interpolants without restrictive assumptions on either the model or\ndata dimension. In this paper, we develop a general interpolation scheme that\ntargets likely transition paths compatible with different metrics and\nprobability distributions. We consider interpolants analogous to a geodesic\nconstrained to a suitable data distribution and derive a novel algorithm for\ncomputing these curves, which requires no additional training. Theoretically,\nwe show that our method locally can be considered as a geodesic under a\nsuitable Riemannian metric. We quantitatively show that our interpolation\nscheme traverses higher density regions than baselines across a range of models\nand datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u63d2\u503c\u65b9\u6848\uff0c\u751f\u6210\u7684\u63d2\u503c\u8def\u5f84\u7c7b\u4f3c\u4e8e\u5728\u6570\u636e\u5206\u5e03\u7ea6\u675f\u4e0b\u7684\u6d4b\u5730\u7ebf\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5bf9\u9f50\u4e0d\u540c\u5ea6\u91cf\u548c\u5206\u5e03\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u6ca1\u6709\u5bf9\u6a21\u578b\u6216\u6570\u636e\u7ef4\u5ea6\u65bd\u52a0\u4e25\u683c\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u83b7\u5f97 principled \u7684\u63d2\u503c\u8def\u5f84\u7684\u95ee\u9898\uff0c\u671f\u671b\u83b7\u5f97\u5728\u591a\u79cd\u5ea6\u91cf\u548c\u5206\u5e03\u4e0b\u7684\u53ef\u63a7\u3001\u53ef\u89e3\u91ca\u7684\u8fc7\u6e21\u8def\u5f84\u3002", "method": "\u7ed9\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u7b97\u6cd5\u6765\u8ba1\u7b97\u8fd9\u7c7b\u66f2\u7ebf\uff0c\u4f7f\u5176\u5728\u6570\u636e\u5206\u5e03\u4e0a\u53d7\u7ea6\u675f\u4e14\u4e0e\u4e0d\u540c\u5ea6\u91cf\u517c\u5bb9\uff1b\u8be5\u7b97\u6cd5\u4e0d\u9700\u8981\u989d\u5916\u7684\u8bad\u7ec3\uff1b\u4ece\u7406\u8bba\u4e0a\u53ef\u5728\u5c40\u90e8\u89c6\u4e3a\u5728\u5408\u9002\u9ece\u66fc\u5ea6\u91cf\u4e0b\u7684\u6d4b\u5730\u7ebf\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u63d2\u503c\u5728\u9ad8\u5bc6\u5ea6\u533a\u57df\u7684\u904d\u5386\u6027\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5bf9\u4e0d\u540c\u5ea6\u91cf\u548c\u5206\u5e03\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e00\u822c\u6027\u7684\u63d2\u503c\u65b9\u6848\uff0c\u5177\u6709\u51e0\u4f55\u76f4\u89c9\u5e76\u53ef\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\uff0c\u63d0\u5347\u4e86\u5bf9\u751f\u6210\u6a21\u578b\u7684\u63a7\u5236\u4e0e\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.26278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26278", "abs": "https://arxiv.org/abs/2510.26278", "authors": ["Kim Yong Tan", "Yueming Lyu", "Ivor Tsang", "Yew-Soon Ong"], "title": "Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation", "comment": null, "summary": "Diffusion models have been successful in learning complex data distributions.\nThis capability has driven their application to high-dimensional\nmulti-objective black-box optimization problem. Existing approaches often\nemploy an external optimization loop, such as an evolutionary algorithm, to the\ndiffusion model. However, these approaches treat the diffusion model as a\nblack-box refiner, which overlooks the internal distribution transition of the\ndiffusion generation process, limiting their efficiency. To address these\nchallenges, we propose the Inference-time Multi-target Generation (IMG)\nalgorithm, which optimizes the diffusion process at inference-time to generate\nsamples that simultaneously satisfy multiple objectives. Specifically, our IMG\nperforms weighted resampling during the diffusion generation process according\nto the expected aggregated multi-objective values. This weighted resampling\nstrategy ensures the diffusion-generated samples are distributed according to\nour desired multi-target Boltzmann distribution. We further derive that the\nmulti-target Boltzmann distribution has an interesting log-likelihood\ninterpretation, where it is the optimal solution to the distributional\nmulti-objective optimization problem. We implemented IMG for a multi-objective\nmolecule generation task. Experiments show that IMG, requiring only a single\ngeneration pass, achieves a significantly higher hypervolume than baseline\noptimization algorithms that often require hundreds of diffusion generations.\nNotably, our algorithm can be viewed as an optimized diffusion process and can\nbe integrated into existing methods to further improve their performance.", "AI": {"tldr": "\u63d0\u51fa IMG\uff1a\u5728\u63a8\u65ad\u65f6\u5bf9\u6269\u6563\u8fc7\u7a0b\u8fdb\u884c\u591a\u76ee\u6807\u52a0\u6743\u91cd\u91c7\u6837\uff0c\u4ee5\u5355\u6b21\u6269\u6563\u751f\u6210\u5b9e\u73b0\u591a\u76ee\u6807\u4f18\u5316\u7684\u9ad8\u8d85\u8d8a\u91cf\uff0c\u663e\u8457\u8d85\u8d8a\u9700\u8981\u591a\u6b21\u6269\u6563\u751f\u6210\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u80fd\u5b66\u4e60\u590d\u6742\u5206\u5e03\u5e76\u88ab\u7528\u4e8e\u9ad8\u7ef4\u591a\u76ee\u6807\u9ed1\u76d2\u4f18\u5316\uff1b\u73b0\u6709\u65b9\u6cd5\u591a\u5c06\u6269\u6563\u6a21\u578b\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u4f9d\u8d56\u5916\u90e8\u8fdb\u5316\u7b97\u6cd5\u7b49\u5faa\u73af\uff0c\u672a\u5145\u5206\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u7684\u5185\u90e8\u5206\u5e03\u8f6c\u6362\uff0c\u6548\u7387\u53d7\u9650\u3002", "method": "\u5728\u6269\u6563\u751f\u6210\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u52a0\u6743\u91cd\u91c7\u6837\uff0c\u4f7f\u6837\u672c\u6309\u591a\u76ee\u6807\u671f\u671b\u503c\u6c47\u603b\u7684 Boltzmann \u5206\u5e03\u6765\u5206\u5e03\uff1b\u63a8\u5bfc\u51fa\u8be5\u591a\u76ee\u6807 Boltzmann \u5206\u5e03\u5177\u6709\u5bf9\u6570\u4f3c\u7136\u89e3\u91ca\uff0c\u662f\u5206\u5e03\u5f0f\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u7684\u6700\u4f18\u89e3\uff1b\u5b9e\u73b0 IMG \u4e8e\u591a\u76ee\u6807\u5206\u5b50\u751f\u6210\u4efb\u52a1\uff0c\u4e14\u4ec5\u9700\u4e00\u6b21\u751f\u6210\u901a\u9053\u3002", "result": "\u5728\u591a\u76ee\u6807\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\uff0cIMG \u5355\u6b21\u751f\u6210\u5373\u53ef\u5b9e\u73b0\u663e\u8457\u63d0\u9ad8\u7684\u8d85\u4f53\u79ef\uff08hypervolume\uff09\uff0c\u8d85\u8fc7\u901a\u5e38\u9700\u8981\u6570\u767e\u6b21\u6269\u6563\u8fed\u4ee3\u7684\u57fa\u7ebf\u4f18\u5316\u7b97\u6cd5\uff1b\u53ef\u89c6\u4e3a\u5bf9\u6269\u6563\u8fc7\u7a0b\u7684\u4f18\u5316\uff0c\u4e14\u53ef\u5d4c\u5165\u73b0\u6709\u65b9\u6cd5\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "IMG \u5c06\u63a8\u65ad\u65f6\u7684\u6269\u6563\u8fc7\u7a0b\u4f18\u5316\u7528\u4e8e\u591a\u76ee\u6807\u751f\u6210\uff0c\u63d0\u4f9b\u5bf9\u6570\u4f3c\u7136\u5c42\u9762\u7684\u7406\u8bba\u89e3\u91ca\uff0c\u4e14\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u4e14\u53ef\u96c6\u6210\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.26284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26284", "abs": "https://arxiv.org/abs/2510.26284", "authors": ["Xia Jiang", "Rong J. B. Zhu"], "title": "Empirical Bayesian Multi-Bandit Learning", "comment": "33 pages, 13 figures", "summary": "Multi-task learning in contextual bandits has attracted significant research\ninterest due to its potential to enhance decision-making across multiple\nrelated tasks by leveraging shared structures and task-specific heterogeneity.\nIn this article, we propose a novel hierarchical Bayesian framework for\nlearning in various bandit instances. This framework captures both the\nheterogeneity and the correlations among different bandit instances through a\nhierarchical Bayesian model, enabling effective information sharing while\naccommodating instance-specific variations. Unlike previous methods that\noverlook the learning of the covariance structure across bandits, we introduce\nan empirical Bayesian approach to estimate the covariance matrix of the prior\ndistribution.This enhances both the practicality and flexibility of learning\nacross multi-bandits. Building on this approach, we develop two efficient\nalgorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and\nebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which\nincorporate the estimated prior into the decision-making process. We provide\nthe frequentist regret upper bounds for the proposed algorithms, thereby\nfilling a research gap in the field of multi-bandit problems. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the superior\nperformance of our algorithms, particularly in complex environments. Our\nmethods achieve lower cumulative regret compared to existing techniques,\nhighlighting their effectiveness in balancing exploration and exploitation\nacross multi-bandits.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u7ecf\u9a8c\u8d1d\u53f6\u65af\u7684\u5c42\u6b21\u8d1d\u53f6\u65af\u591a\u5e26\u5bbd\u5e26\u95ee\u9898\u6846\u67b6\uff0c\u5b66\u4e60\u8de8\u5e26\u4efb\u52a1\u7684\u534f\u65b9\u5dee\u7ed3\u6784\u5e76\u5c06\u5176\u6574\u5408\u8fdb\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51fa ebmTS \u4e0e ebmUCB \u4e24\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u7ed9\u51fa\u9891\u7387\u6d3e\u7684\u7406\u8bba\u4e0a\u754c\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u591a\u4efb\u52a1/\u591a\u5b9e\u4f8b\u4e0a\u4e0b\u6587\u7684\u5e26\u81c2\u95ee\u9898\u4e2d\uff0c\u5229\u7528\u5b9e\u4f8b\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u548c\u5f02\u8d28\u6027\u4ee5\u63d0\u5347\u51b3\u7b56\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u8de8\u5e26\u7684\u534f\u65b9\u5dee\u7ed3\u6784\u53ca\u5148\u9a8c\u4fe1\u606f\u7684\u5b66\u4e60\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5efa\u6a21\u534f\u65b9\u5dee\u5e76\u5c06\u5176\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u5f0f\u7684\u5e26\u81c2\u7b56\u7565\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5c42\u6b21\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u6355\u6349\u4e0d\u540c\u5e26\u81c2\u5b9e\u4f8b\u4e4b\u95f4\u7684\u5f02\u8d28\u6027\u548c\u76f8\u5173\u6027\uff0c\u901a\u8fc7\u7ecf\u9a8c\u8d1d\u53f6\u65af\u4f30\u8ba1\u5148\u9a8c\u5206\u5e03\u7684\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u5c06\u4f30\u8ba1\u7684\u5148\u9a8c\u5d4c\u5165\u5230 Thompson Sampling (ebmTS) \u548c UCB (ebmUCB) \u7684\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u4fe1\u606f\u5171\u4eab\u4e0e\u4e2a\u4f53\u5dee\u5f02\u7684\u517c\u987e\u3002\u7ed9\u51fa\u8fd9\u4e24\u4e2a\u7b97\u6cd5\u7684\u5b9e\u73b0\u7ec6\u8282\u4e0e\u8ba1\u7b97\u6548\u7387\u5206\u6790\u3002", "result": "\u7ed9\u51fa\u9891\u7387\u6d3e\u7684\u7d2f\u79ef regrets \u4e0a\u754c\uff0c\u586b\u8865\u4e86\u8de8\u5e26\u5e26\u81c2\u95ee\u9898\u4e2d\u534f\u65b9\u5dee\u5b66\u4e60\u7684\u7814\u7a76\u7a7a\u7f3a\uff1b\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u964d\u4f4e\u7d2f\u8ba1 regret\uff0c\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u63a2\u7d22-\u5f00\u53d1\u6743\u8861\u8868\u73b0\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ecf\u9a8c\u8d1d\u53f6\u65af\u5c42\u6b21\u6a21\u578b\u53ca\u4e24\u79cd\u7b97\u6cd5\u4e3a\u591a\u5e26\u5e26\u81c2\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u4fe1\u606f\u5171\u4eab\u673a\u5236\uff0c\u8bc1\u660e\u5b66\u4e60\u534f\u65b9\u5dee\u7ed3\u6784\u5bf9\u63d0\u5347\u8de8\u4efb\u52a1\u5e26\u81c2\u51b3\u7b56\u6027\u80fd\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u5177\u5907\u5728\u73b0\u5b9e\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.26301", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26301", "abs": "https://arxiv.org/abs/2510.26301", "authors": ["Jingyuan Liu", "Fatemeh Ghaffari", "Xuchuang Wang", "Mohammad Hajiesmaili", "Carlee Joe-Wong"], "title": "Offline Clustering of Preference Learning with Active-data Augmentation", "comment": null, "summary": "Preference learning from pairwise feedback is a widely adopted framework in\napplications such as reinforcement learning with human feedback and\nrecommendations. In many practical settings, however, user interactions are\nlimited or costly, making offline preference learning necessary. Moreover,\nreal-world preference learning often involves users with different preferences.\nFor example, annotators from different backgrounds may rank the same responses\ndifferently. This setting presents two central challenges: (1) identifying\nsimilarity across users to effectively aggregate data, especially under\nscenarios where offline data is imbalanced across dimensions, and (2) handling\nthe imbalanced offline data where some preference dimensions are\nunderrepresented. To address these challenges, we study the Offline Clustering\nof Preference Learning problem, where the learner has access to fixed datasets\nfrom multiple users with potentially different preferences and aims to maximize\nutility for a test user. To tackle the first challenge, we first propose\nOff-C$^2$PL for the pure offline setting, where the learner relies solely on\noffline data. Our theoretical analysis provides a suboptimality bound that\nexplicitly captures the tradeoff between sample noise and bias. To address the\nsecond challenge of inbalanced data, we extend our framework to the setting\nwith active-data augmentation where the learner is allowed to select a limited\nnumber of additional active-data for the test user based on the cluster\nstructure learned by Off-C$^2$PL. In this setting, our second algorithm,\nA$^2$-Off-C$^2$PL, actively selects samples that target the least-informative\ndimensions of the test user's preference. We prove that these actively\ncollected samples contribute more effectively than offline ones. Finally, we\nvalidate our theoretical results through simulations on synthetic and\nreal-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u79bb\u7ebf\u504f\u597d\u5b66\u4e60\u7684\u805a\u7c7b\u6846\u67b6 Off-C^2PL\uff0c\u89e3\u51b3\u8de8\u7528\u6237\u504f\u597d\u805a\u5408\u53ca\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff1b\u5e76\u6269\u5c55\u4e3a\u4e3b\u52a8\u6570\u636e\u589e\u5f3a\u7684 A^2-Off-C^2PL\uff0c\u4ee5\u5728\u6d4b\u8bd5\u7528\u6237\u7684\u6700\u5f31\u4fe1\u606f\u7ef4\u5ea6\u4e0a\u6709\u9009\u62e9\u5730\u83b7\u53d6\u6837\u672c\u3002", "motivation": "\u5728\u79bb\u7ebf\u6761\u4ef6\u4e0b\u9700\u4ece\u591a\u7528\u6237\u7684\u504f\u597d\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u5bf9\u6d4b\u8bd5\u7528\u6237\u6700\u5927\u5316\u6548\u7528\uff1b\u73b0\u5b9e\u573a\u666f\u4e2d\u7528\u6237\u504f\u597d\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u79bb\u7ebf\u6570\u636e\u5728\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u901a\u5e38\u4e0d\u5e73\u8861\u3002\u9700\u540c\u65f6\u89e3\u51b3\u8de8\u7528\u6237\u76f8\u4f3c\u6027\u8bc6\u522b\u4e0e\u6570\u636e\u4e0d\u5e73\u8861\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa Off-C^2PL\uff1a\u7eaf\u79bb\u7ebf\u8bbe\u5b9a\u4e0b\u901a\u8fc7\u591a\u7528\u6237\u6570\u636e\u7684\u805a\u7c7b\u7ed3\u6784\u5b9e\u73b0\u504f\u597d\u5b66\u4e60\u5e76\u7ed9\u51fa\u4e0e\u6837\u672c\u566a\u58f0/\u504f\u5dee\u6743\u8861\u76f8\u5173\u7684\u5b50\u6700\u4f18\u754c\u3002\u6269\u5c55\u4e3a A^2-Off-C^2PL\uff1a\u5728\u53d7\u9650\u7684\u4e3b\u52a8\u6570\u636e\u6269\u589e\u6761\u4ef6\u4e0b\uff0c\u6839\u636e\u805a\u7c7b\u7ed3\u6784\u4e3b\u52a8\u9009\u62e9\u7528\u4e8e\u6d4b\u8bd5\u7528\u6237\u7684\u6837\u672c\uff0c\u91cd\u70b9\u5173\u6ce8\u6d4b\u8bd5\u7528\u6237\u504f\u597d\u4e2d\u7684\u4fe1\u606f\u91cf\u6700\u4f4e\u7ef4\u5ea6\u3002", "result": "\u7ed9\u51fa\u7406\u8bba\u5b50\u6700\u4f18\u754c\u7684\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u63ed\u793a\u566a\u58f0\u4e0e\u504f\u5dee\u4e4b\u95f4\u7684\u6743\u8861\uff1b\u4e3b\u52a8\u91c7\u6837\u8bbe\u8ba1\u5728\u4fe1\u606f\u91cf\u6700\u5f31\u7684\u7ef4\u5ea6\u4e0a\u66f4\u5177\u4fe1\u606f\u589e\u76ca\uff0c\u5176\u8d21\u732e\u4f18\u4e8e\u7eaf\u79bb\u7ebf\u6570\u636e\u3002\u901a\u8fc7\u5bf9\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u5728\u591a\u7528\u6237\u79bb\u7ebf\u504f\u597d\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u805a\u7c7b\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e3b\u52a8\u6570\u636e\u589e\u5f3a\u80fd\u663e\u8457\u63d0\u5347\u6837\u672c\u5229\u7528\u6548\u7387\u3002\u672a\u6765\u5de5\u4f5c\u53ef\u62d3\u5c55\u5230\u66f4\u590d\u6742\u7684\u504f\u597d\u7ed3\u6784\u3001\u5728\u7ebf\u9002\u5e94\u7b49\u573a\u666f\u3002"}}
{"id": "2510.26302", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26302", "abs": "https://arxiv.org/abs/2510.26302", "authors": ["Ziliang Chen", "Tianang Xiao", "Jusheng Zhang", "Yongsen Zheng", "Xipeng Chen"], "title": "Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens", "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal\ngeneralization by aligning images and texts in a shared embedding space, yet it\npersistently fails at compositional reasoning over objects, attributes, and\nrelations often behaving like a bag-of-words matcher. Prior causal accounts\ntypically model text as a single vector, obscuring token-level structure and\nleaving core phenomena-such as prompt sensitivity and failures on hard\nnegatives unexplained. We address this gap with a token-aware causal\nrepresentation learning (CRL) framework grounded in a sequential,\nlanguage-token SCM. Our theory extends block identifiability to tokenized text,\nproving that CLIP's contrastive objective can recover the modal-invariant\nlatent variable under both sentence-level and token-level SCMs. Crucially,\ntoken granularity yields the first principled explanation of CLIP's\ncompositional brittleness: composition nonidentifiability. We show the\nexistence of pseudo-optimal text encoders that achieve perfect modal-invariant\nalignment yet are provably insensitive to SWAP, REPLACE, and ADD operations\nover atomic concepts, thereby failing to distinguish correct captions from hard\nnegatives despite optimizing the same training objective as true-optimal\nencoders. The analysis further links language-side nonidentifiability to\nvisual-side failures via the modality gap and shows how iterated composition\noperators compound hardness, motivating improved negative mining strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u8bed\u8a00 token \u7684\u56e0\u679c\u8868\u793a\u5b66\u4e60\uff08CRL\uff09\u6846\u67b6\u5e76\u5728 token-level SCM \u4e0b\u6269\u5c55\u5757\u53ef\u8bc6\u522b\u6027\uff0c\u4ee5\u89e3\u91ca CLIP \u5728\u7ec4\u5408\u63a8\u7406\u4e2d\u7684\u8106\u5f31\u6027\u3001\u5b58\u5728\u4f2a\u6700\u4f18\u7f16\u7801\u5668\u5728\u5bf9\u9f50\u4e0b\u5bf9\u7ec4\u5408\u64cd\u4f5c\u4e0d\u654f\u611f\uff0c\u4ee5\u53ca\u7531\u6b64\u5f15\u53d1\u7684\u8d1f\u6837\u672c\u6316\u6398\u9700\u6c42\u3002", "motivation": "CLIP \u5728\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5bf9\u8c61-\u5c5e\u6027-\u5173\u7cfb\u7684\u7ec4\u5408\u63a8\u7406\u4e0a\u5e38\u5448\u73b0\u201c\u888b\u88c5\u8bcd\u5339\u914d\u201d\u7684\u5c40\u9650\u3002\u4ee5\u5f80\u7684\u56e0\u679c\u5206\u6790\u591a\u5c06\u6587\u672c\u89c6\u4e3a\u5355\u5411\u5411\u91cf\uff0c\u5ffd\u7565 token \u7ea7\u7ed3\u6784\uff0c\u96be\u4ee5\u89e3\u91ca\u63d0\u793a\u654f\u611f\u6027\u4e0e\u96be\u8d1f\u6837\u672c\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa token-aware CRL \u6846\u67b6\uff0c\u5efa\u7acb\u9010\u6b65\u7684\u8bed\u8a00 token SCM\uff1b\u5c06 block identifiability \u6269\u5c55\u5230 tokenized \u6587\u672c\uff1b\u8bc1\u660e\u5728\u53e5\u5b50\u7ea7\u548c token-level SCM \u4e0b\uff0cCLIP \u7684\u5bf9\u6bd4\u76ee\u6807\u80fd\u6062\u590d\u6a21\u6001\u4e0d\u53d8\u7684\u6f5c\u5728\u53d8\u91cf\uff1b\u8bc1\u660e\u5b58\u5728\u4f2a\u6700\u4f18\u6587\u672c\u7f16\u7801\u5668\u5728\u5b9e\u73b0\u5b8c\u7f8e\u6a21\u6001\u5bf9\u9f50\u7684\u540c\u65f6\u5bf9 SWAP\u3001REPLACE\u3001ADD \u7b49\u539f\u5b50\u6982\u5ff5\u64cd\u4f5c\u4e0d\u654f\u611f\uff0c\u4ece\u800c\u65e0\u6cd5\u533a\u5206\u6b63\u786e\u63cf\u8ff0\u548c\u56f0\u96be\u8d1f\u6837\u672c\uff1b\u5206\u6790\u8bed\u8a00\u7aef\u975e identifiability \u5982\u4f55\u901a\u8fc7\u6a21\u6001\u5dee\u8ddd\u5f71\u54cd\u89c6\u89c9\u7aef\uff0c\u5e76\u8ba8\u8bba\u8fed\u4ee3\u7684\u7ec4\u5408\u64cd\u4f5c\u5982\u4f55\u52a0\u5267\u5b66\u4e60\u96be\u5ea6\u53ca\u5bf9\u8d1f\u6837\u672c\u6316\u6398\u7684\u542f\u793a\u3002", "result": "\u7406\u8bba\u5c42\u9762\u6269\u5c55\u4e86 token-level identifiability\uff0c\u89e3\u91ca\u4e86 CLIP \u7684\u7ec4\u5408\u8106\u5f31\u6027\u53ca\u4f2a\u6700\u4f18\u7f16\u7801\u5668\u5728\u540c\u4e00\u8bad\u7ec3\u76ee\u6807\u4e0b\u7684\u5bf9\u9f50\u4e0e\u533a\u5206\u80fd\u529b\u7684\u5206\u79bb\uff1b\u63ed\u793a\u8bed\u8a00-\u89c6\u89c9\u6a21\u6001\u5dee\u8ddd\u4e0e\u8d1f\u6837\u672c\u6316\u6398\u96be\u5ea6\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u8981\u63d0\u5347\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\uff0c\u9700\u8981\u5f15\u5165\u66f4\u5f3a\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u4e0e token-level \u7ea6\u675f\uff0c\u907f\u514d\u5bf9 SWAP/REPLACE/ADD \u7b49\u64cd\u4f5c\u7684\u4e0d\u654f\u611f\uff0c\u5e76\u4fc3\u4f7f\u6a21\u578b\u5bf9\u6b63\u786e\u7ec4\u5408\u7684\u63cf\u8ff0\u505a\u51fa\u654f\u611f\u4e14\u533a\u5206\u6027\u7684\u54cd\u5e94\u3002"}}
{"id": "2510.26311", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26311", "abs": "https://arxiv.org/abs/2510.26311", "authors": ["Ruilin Tong", "Haodong Lu", "Yuhang Liu", "Dong Gong"], "title": "Model Inversion with Layer-Specific Modeling and Alignment for Data-Free Continual Learning", "comment": "Accepted in NeurIPS 2025", "summary": "Continual learning (CL) aims to incrementally train a model on a sequence of\ntasks while retaining performance on prior ones. However, storing and replaying\ndata is often infeasible due to privacy or security constraints and impractical\nfor arbitrary pre-trained models. Data-free CL seeks to update models without\naccess to previous data. Beyond regularization, we employ model inversion to\nsynthesize data from the trained model, enabling replay without storing\nsamples. Yet, model inversion in predictive models faces two challenges: (1)\ngenerating inputs solely from compressed output labels causes drift between\nsynthetic and real data, and replaying such data can erode prior knowledge; (2)\ninversion is computationally expensive since each step backpropagates through\nthe full model. These issues are amplified in large pre-trained models such as\nCLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),\ninspired by faster convergence in single-layer optimization. PMI provides\nstrong initialization for full-model inversion, substantially reducing\niterations. To mitigate feature shift, we model class-wise features via\nGaussian distributions and contrastive model, ensuring alignment between\nsynthetic and real features. Combining PMI and feature modeling, our approach\nenables continual learning of new classes by generating pseudo-images from\nsemantic-aware projected features, achieving strong effectiveness and\ncompatibility across multiple CL settings.", "AI": {"tldr": "Data-free continual learning via per-layer model inversion (PMI) and Gaussian feature modeling to generate semantic-aware pseudo-images for replay, reducing computation and mitigating feature drift in large models like CLIP.", "motivation": "Continual learning without access to previous data is necessary due to privacy, security, or practical constraints. Existing inversion approaches suffer from drift (inputs from compressed outputs) and high computational cost, especially for large pre-trained models.", "method": "Propose Per-layer Model Inversion (PMI) to provide strong initialization for full-model inversion by optimizing per-layer parameters, accelerating convergence. Model class-wise features with Gaussian distributions and a contrastive objective to align synthetic features with real features, mitigating drift. Generate pseudo-images from semantic-aware projected features for replay in CL settings.", "result": "Claimed strong effectiveness and compatibility across multiple continual learning settings, with improved efficiency and better alignment between synthetic and real features, enabling data-free replay in large models.", "conclusion": "Combining PMI and feature modeling enables effective data-free continual learning by generating informative pseudo-images that preserve prior knowledge while enabling replay in scenarios where data cannot be stored or accessed."}}
{"id": "2510.26323", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26323", "abs": "https://arxiv.org/abs/2510.26323", "authors": ["Sascha M\u00fccke"], "title": "On the Impact of Weight Discretization in QUBO-Based SVM Training", "comment": "Presented at the 7th DSO Workshop at ECML PKDD 2025", "summary": "Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,\nenabling the use of quantum annealing for model optimization. In this work, we\nstudy how the number of qubits - linked to the discretization level of dual\nweights - affects predictive performance across datasets. We compare QUBO-based\nSVM training to the classical LIBSVM solver and find that even low-precision\nQUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes\nsuperior, accuracy. While increased bit-depth enables larger regularization\nparameters, it does not always improve classification. Our findings suggest\nthat selecting the right support vectors may matter more than their precise\nweighting. Although current hardware limits the size of solvable QUBOs, our\nresults highlight the potential of quantum annealing for efficient SVM training\nas quantum devices scale.", "AI": {"tldr": "\u4f4e\u7cbe\u5ea6\u7684 QUBO \u7f16\u7801\u5373\u53ef\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u7ecf\u5178 LIBSVM \u76f8\u5f53\u751a\u81f3\u4f18\u8d8a\u7684\u51c6\u786e\u6027\uff1b\u6bd4\u7279\u6df1\u5ea6\u63d0\u9ad8\u5e76\u4e0d\u603b\u662f\u5e26\u6765\u6539\u8fdb\uff0c\u5173\u952e\u5728\u4e8e\u9009\u62e9\u5408\u9002\u7684\u652f\u6301\u5411\u91cf\u800c\u975e\u6743\u91cd\u7684\u7cbe\u786e\u6027\uff0c\u91cf\u5b50\u9000\u706b\u5bf9 SVM \u7684\u8bad\u7ec3\u5177\u6709\u6f5c\u5728\u4f18\u52bf\uff0c\u968f\u786c\u4ef6\u6269\u5c55\u800c\u589e\u5f3a\u3002", "motivation": "\u63a2\u7a76\u7528\u4e8e\u5c06 SVM \u8bad\u7ec3\u8868\u8ff0\u4e3a QUBO \u7684\u79bb\u6563\u5316\uff08\u6bd4\u7279\u6df1\u5ea6\uff09\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u7ecf\u5178\u6c42\u89e3\u5668 LIBSVM \u8fdb\u884c\u5bf9\u6bd4\uff0c\u4ee5\u8bc4\u4f30\u5728\u5f53\u524d\u53ca\u672a\u6765\u91cf\u5b50\u786c\u4ef6\u5c3a\u5ea6\u4e0b\u7684\u53ef\u884c\u6027\u3002", "method": "\u5c06 SVM \u95ee\u9898\u79bb\u6563\u5316\u4e3a QUBO\uff0c\u6539\u53d8\u6bcf\u4e2a\u5bf9\u5076\u6743\u91cd\u7684\u6bd4\u7279\u6df1\u5ea6\uff08\u5982 1 \u6bd4\u7279\u3001\u82e5\u5e72\u6bd4\u7279\u7b49\uff09\uff0c\u5728\u591a\u7ec4\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u4e0e LIBSVM \u7684\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002\u540c\u65f6\u5206\u6790\u66f4\u5927\u6b63\u5219\u5316\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u652f\u6301\u5411\u91cf\u7684\u9009\u62e9\u4e0e\u6743\u91cd\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5373\u4fbf\u662f 1 \u6bd4\u7279\u6bcf\u4e2a\u53c2\u6570\u7684\u4f4e\u7cbe\u5ea6 QUBO \u7f16\u7801\u4e5f\u80fd\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u751a\u81f3\u6709\u65f6\u4f18\u4e8e LIBSVM \u7684\u51c6\u786e\u6027\uff1b\u589e\u52a0\u4f4d\u6df1\u5ea6\u5141\u8bb8\u4f7f\u7528\u66f4\u5927\u7684\u6b63\u5219\u5316\u53c2\u6570\uff0c\u4f46\u5e76\u4e0d\u603b\u662f\u5e26\u6765\u5206\u7c7b\u6027\u80fd\u63d0\u5347\uff1b\u7814\u7a76\u8868\u660e\uff0c\u6b63\u786e\u9009\u62e9\u7684\u652f\u6301\u5411\u91cf\u53ef\u80fd\u6bd4\u7cbe\u786e\u6743\u91cd\u66f4\u4e3a\u91cd\u8981\uff1b\u5f53\u524d\u786c\u4ef6\u9650\u5236\u4e86\u53ef\u89e3\u51b3\u7684 QUBO \u5c3a\u5bf8\uff0c\u4f46\u968f\u7740\u8bbe\u5907\u89c4\u6a21\u6269\u5927\uff0c\u91cf\u5b50\u9000\u706b\u5728\u9ad8\u6548\u8bad\u7ec3 SVM \u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u91cf\u5b50\u9000\u706b\u5bf9 SVM \u7684\u8bad\u7ec3\u5177\u6709\u524d\u666f\uff0c\u5c24\u5176\u5728\u786c\u4ef6\u89c4\u6a21\u63d0\u5347\u540e\uff1b\u4f4e\u7cbe\u5ea6\u7f16\u7801\u5373\u53ef\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u672a\u6765\u5de5\u4f5c\u5e94\u805a\u7126\u4e8e\u5982\u4f55\u6709\u6548\u9009\u62e9\u652f\u6301\u5411\u91cf\u4ee5\u53ca\u5728\u66f4\u5927\u89c4\u6a21\u7684 QUBO \u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.26324", "categories": ["cs.LG", "cs.AI", "cs.DS", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.26324", "abs": "https://arxiv.org/abs/2510.26324", "authors": ["Zhiyang Xun", "Shivam Gupta", "Eric Price"], "title": "Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics", "comment": "NeurIPS 2025", "summary": "Given a noisy linear measurement $y = Ax + \\xi$ of a distribution $p(x)$, and\na good approximation to the prior $p(x)$, when can we sample from the posterior\n$p(x \\mid y)$? Posterior sampling provides an accurate and fair framework for\ntasks such as inpainting, deblurring, and MRI reconstruction, and several\nheuristics attempt to approximate it. Unfortunately, approximate posterior\nsampling is computationally intractable in general.\n  To sidestep this hardness, we focus on (local or global) log-concave\ndistributions $p(x)$. In this regime, Langevin dynamics yields posterior\nsamples when the exact scores of $p(x)$ are available, but it is brittle to\nscore--estimation error, requiring an MGF bound (sub-exponential error). By\ncontrast, in the unconditional setting, diffusion models succeed with only an\n$L^2$ bound on the score error. We prove that combining diffusion models with\nan annealed variant of Langevin dynamics achieves conditional sampling in\npolynomial time using merely an $L^4$ bound on the score error.", "AI": {"tldr": "\u5728\u5e26\u6709\u566a\u58f0\u7ebf\u6027\u89c2\u6d4b y=Ax+\u03be \u7684\u8bbe\u5b9a\u4e0b\uff0c\u63d0\u51fa\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7ed3\u5408\u9000\u706b Langevin \u52a8\u529b\u5b66\uff0c\u5728\u5bf9\u5f97\u5206\u8bef\u5dee\u4ec5\u9700 L^4 \u8303\u6570\u6709\u754c\u7684\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u591a\u9879\u5f0f\u65f6\u95f4\u4ece\u540e\u9a8c p(x|y) \u91c7\u6837\u3002", "motivation": "\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u540e\u9a8c\u91c7\u6837\u4ee5\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\uff08\u5982\u53bb\u6a21\u7cca\u3001\u4fee\u590d\u3001MRI \u91cd\u5efa\uff09\uff0c\u5e76\u514b\u670d\u4f20\u7edf\u7684 Langevin \u52a8\u529b\u5b66\u5728\u4ec5\u6709 p(x) \u7684\u5bf9\u6570\u51f9\u524d\u63d0\u4e0b\u5bf9\u5f97\u5206\u4f30\u8ba1\u8bef\u5dee\u7684\u5f3a\u4f9d\u8d56\uff1b\u4e0e\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u76f8\u6bd4\uff0c\u5c1d\u8bd5\u5728\u5e26\u6761\u4ef6\u7684\u8bbe\u7f6e\u4e2d\u83b7\u5f97\u9c81\u68d2\u6027\u4e0e\u8ba1\u7b97\u6027\u3002", "method": "\u5bf9 p(x) \u7684\u8981\u6c42\u4e3a\u5c40\u90e8\u6216\u5168\u5c40\u5bf9\u6570\u51f9\uff1b\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u4e00\u4e2a\u9000\u706b\u7248\u672c\u7684 Langevin \u52a8\u529b\u5b66\u7ed3\u5408\uff0c\u5229\u7528\u5bf9\u5f97\u5206\u8bef\u5dee\u7684 L^4 \u6709\u754c\u6027\uff0c\u5728\u6761\u4ef6\u91c7\u6837\u4e0b\u8bc1\u660e\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u884c\u7684\u540e\u9a8c\u62bd\u6837\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4ec5\u5047\u8bbe\u5f97\u5206\u8bef\u5dee\u5177\u6709 L^4 \u6709\u754c\u7684\u524d\u63d0\u4e0b\uff0c\u53ef\u4ee5\u901a\u8fc7\u6240\u63d0\u65b9\u6cd5\u5b9e\u73b0\u4ece p(x|y) \u7684\u6761\u4ef6\u540e\u9a8c\u4e2d\u91c7\u6837\uff0c\u4e14\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a\u591a\u9879\u5f0f\u3002", "conclusion": "\u5c06\u6269\u6563\u6a21\u578b\u7684\u65e0\u6761\u4ef6\u9c81\u68d2\u6027\u4e0e\u6761\u4ef6\u91c7\u6837\u7684\u9700\u6c42\u7ed3\u5408\u8d77\u6765\uff0c\u964d\u4f4e\u5bf9\u7cbe\u786e\u5f97\u5206\u4f30\u8ba1\u7684\u4f9d\u8d56\uff0c\u4e3a\u5b9e\u9645\u7684\u56fe\u50cf\u4fee\u590d\u7b49\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6848\u3002"}}
{"id": "2510.26342", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26342", "abs": "https://arxiv.org/abs/2510.26342", "authors": ["Zhigao Guo", "Feng Dong"], "title": "Linear Causal Discovery with Interventional Constraints", "comment": null, "summary": "Incorporating causal knowledge and mechanisms is essential for refining\ncausal models and improving downstream tasks such as designing new treatments.\nIn this paper, we introduce a novel concept in causal discovery, termed\ninterventional constraints, which differs fundamentally from interventional\ndata. While interventional data require direct perturbations of variables,\ninterventional constraints encode high-level causal knowledge in the form of\ninequality constraints on causal effects. For instance, in the Sachs dataset\n(Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3\nexerts a positive causal effect on Akt. Existing causal discovery methods allow\nenforcing structural constraints (for example, requiring a causal path from\nPIP3 to Akt), but they may still produce incorrect causal conclusions such as\nlearning that \"PIP3 inhibits Akt\". Interventional constraints bridge this gap\nby explicitly constraining the total causal effect between variable pairs,\nensuring learned models respect known causal influences. To formalize\ninterventional constraints, we propose a metric to quantify total causal\neffects for linear causal models and formulate the problem as a constrained\noptimization task, solved using a two-stage constrained optimization method. We\nevaluate our approach on real-world datasets and demonstrate that integrating\ninterventional constraints not only improves model accuracy and ensures\nconsistency with established findings, making models more explainable, but also\nfacilitates the discovery of new causal relationships that would otherwise be\ncostly to identify.", "AI": {"tldr": "\u63d0\u51fa interventional constraints\uff0c\u5c06\u9ad8\u5c42\u6b21\u7684\u56e0\u679c\u77e5\u8bc6\u4ee5\u5bf9\u53d8\u91cf\u95f4\u603b\u6548\u5e94\u7684\u4e0d\u7b49\u5f0f\u7ea6\u675f\u5f15\u5165\u5230\u56e0\u679c\u53d1\u73b0\u4e2d\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u53d7\u9650\u4f18\u5316\u5b9e\u73b0\u7ebf\u6027\u6a21\u578b\u7684\u603b\u56e0\u679c\u6548\u5e94\u7ea6\u675f\uff0c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u5e76\u4fc3\u8fdb\u65b0\u56e0\u679c\u5173\u7cfb\u53d1\u73b0\u3002", "motivation": "\u5728\u4ec5\u4f9d\u8d56\u89c2\u6d4b\u6570\u636e\u7684\u56e0\u679c\u53d1\u73b0\u4e2d\uff0c\u7ed3\u6784\u7ea6\u675f\u53ef\u80fd\u4e0d\u8db3\u4ee5\u907f\u514d\u9519\u8bef\u7ed3\u8bba\uff08\u5982\u5c06\u6b63\u5411\u5173\u7cfb\u8bef\u5224\u4e3a\u8d1f\u5411\uff09\uff1b\u800c\u5e72\u9884\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u3002\u5f15\u5165\u53ef\u5d4c\u5165\u9886\u57df\u77e5\u8bc6\u7684\u5e72\u9884\u7ea6\u675f\u53ef\u4f7f\u5b66\u4e60\u7684\u56e0\u679c\u6a21\u578b\u66f4\u7b26\u5408\u9886\u57df\u4e8b\u5b9e\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u7528\u4e8e\u7ebf\u6027\u56e0\u679c\u6a21\u578b\u7684\u603b\u56e0\u679c\u6548\u5e94\u5ea6\u91cf\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u4e0d\u7b49\u5f0f\u7ea6\u675f\u5e76\u5d4c\u5165\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u7684\u53d7\u9650\u4f18\u5316\u6c42\u89e3\uff0c\u4ee5\u786e\u4fdd\u5b66\u4e60\u8fc7\u7a0b\u7b26\u5408\u5df2\u77e5\u56e0\u679c\u5f71\u54cd\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982 Sachs \u6570\u636e\u96c6\uff09\u4e0a\u9a8c\u8bc1\uff0c\u52a0\u5165\u5e72\u9884\u7ea6\u675f\u53ef\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u5e76\u4e0e\u5df2\u77e5\u56e0\u679c\u7ed3\u8bba\u4fdd\u6301\u4e00\u81f4\uff0c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6709\u52a9\u4e8e\u53d1\u73b0\u65b0\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u539f\u5148\u6210\u672c\u8f83\u9ad8\u3002", "conclusion": "\u5e72\u9884\u7ea6\u675f\u4e3a\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u6709\u6548\u7684\u77e5\u8bc6\u6574\u5408\u9014\u5f84\uff0c\u517c\u987e\u7ebf\u6027\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u63d0\u9ad8\u6b63\u786e\u6027\u4e0e\u53d1\u73b0\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u5c06\u9886\u57df\u77e5\u8bc6\u8f6c\u5316\u4e3a\u91cf\u5316\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7684\u60c5\u666f\u3002"}}
{"id": "2510.26347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26347", "abs": "https://arxiv.org/abs/2510.26347", "authors": ["Sebastian Zieglmeier", "Niklas Erdmann", "Narada D. Warakagoda"], "title": "Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle", "comment": null, "summary": "Reinforcement learning (RL) algorithms are designed to optimize\nproblem-solving by learning actions that maximize rewards, a task that becomes\nparticularly challenging in random and nonstationary environments. Even\nadvanced RL algorithms are often limited in their ability to solve problems in\nthese conditions. In applications such as searching for underwater pollution\nclouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate\nreward-sparse environments, where actions frequently result in a zero reward.\nThis paper aims to address these challenges by revisiting and modifying\nclassical RL approaches to efficiently operate in sparse, randomized, and\nnonstationary environments. We systematically study a large number of\nmodifications, including hierarchical algorithm changes, multigoal learning,\nand the integration of a location memory as an external output filter to\nprevent state revisits. Our results demonstrate that a modified Monte\nCarlo-based approach significantly outperforms traditional Q-learning and two\nexhaustive search patterns, illustrating its potential in adapting RL to\ncomplex environments. These findings suggest that reinforcement learning\napproaches can be effectively adapted for use in random, nonstationary, and\nreward-sparse environments.", "AI": {"tldr": "\u5bf9\u7a00\u758f\u5956\u52b1\u3001\u968f\u673a\u6027\u548c\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u518d\u7814\u7a76\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u7684\u4fee\u6539\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u3001\u591a\u76ee\u6807\u5b66\u4e60\u548c\u5916\u90e8\u8f93\u51fa\u6ee4\u6ce2\uff08\u4f4d\u7f6e\u8bb0\u5fc6\uff09\u7b49\u6539\u8fdb\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfQ\u5b66\u4e60\u548c\u7a77\u4e3e\u641c\u7d22\u3002", "motivation": "\u5728\u968f\u673a\u3001\u975e\u5e73\u7a33\u4e14\u5956\u52b1\u7a00\u758f\u7684\u73af\u5883\u4e2d\uff0c\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u96be\u4ee5\u9ad8\u6548\u5b66\u4e60\u3002\u4ee5\u5728\u6c34\u4e0b\u63a2\u6d4b\u6c61\u67d3\u4e91\u7684\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\uff08AUVs\uff09\u4e3a\u5e94\u7528\u573a\u666f\uff0c\u5f3a\u8c03\u9700\u8981\u5728\u5956\u52b1\u7a00\u7f3a\u3001\u73af\u5883\u591a\u53d8\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6709\u6548\u63a2\u7d22\u4e0e\u9c81\u68d2\u6027\u63d0\u5347\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u8003\u5bdf\u5927\u91cf\u4fee\u6539\u7b56\u7565\uff0c\u5305\u542b\u5c42\u6b21\u5316\u7b97\u6cd5\u6539\u8fdb\u3001\u591a\u76ee\u6807\u5b66\u4e60\uff0c\u4ee5\u53ca\u5c06\u4f4d\u7f6e\u8bb0\u5fc6\u4f5c\u4e3a\u5916\u90e8\u8f93\u51fa\u6ee4\u6ce2\u5668\u4ee5\u9632\u6b62\u72b6\u6001\u91cd\u590d\u8bbf\u95ee\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u91c7\u7528\u6539\u8fdb\u7684\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u8fdb\u884c\u5b66\u4e60\uff0c\u4e0e\u4f20\u7edfQ\u5b66\u4e60\u53ca\u4e24\u79cd\u7a77\u4e3e\u641c\u7d22\u6a21\u5f0f\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u6539\u8fdb\u7684\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u7684\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfQ\u5b66\u4e60\u548c\u4e24\u79cd\u7a77\u4e3e\u641c\u7d22\u6a21\u5f0f\uff0c\u8bc1\u660e\u4e86\u5728\u7a00\u758f\u3001\u968f\u673a\u548c\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u5bf9\u7ecf\u5178RL\u65b9\u6cd5\u7684\u6709\u6548\u6539\u9020\u6f5c\u529b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u901a\u8fc7\u5bf9\u7ecf\u5178\u65b9\u6cd5\u7684\u6709\u9488\u5bf9\u6027\u4fee\u6539\u6765\u9002\u5e94\u968f\u673a\u3001\u975e\u5e73\u7a33\u4e14\u5956\u52b1\u7a00\u758f\u7684\u73af\u5883\uff0c\u5c24\u5176\u5728\u6f5c\u5728\u5e94\u7528\u5982\u6c34\u4e0b\u6c61\u67d3\u4e91\u63a2\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u660e\u663e\u7684\u9002\u7528\u6027\u4e0e\u524d\u666f\u3002"}}
{"id": "2510.26353", "categories": ["cs.LG", "I.2; I.5"], "pdf": "https://arxiv.org/pdf/2510.26353", "abs": "https://arxiv.org/abs/2510.26353", "authors": ["Albi Isufaj", "Pablo Moll\u00e1", "Helmut Prendinger"], "title": "Towards Explainable and Reliable AI in Finance", "comment": null, "summary": "Financial forecasting increasingly uses large neural network models, but\ntheir opacity raises challenges for trust and regulatory compliance. We present\nseveral approaches to explainable and reliable AI in finance. \\emph{First}, we\ndescribe how Time-LLM, a time series foundation model, uses a prompt to avoid a\nwrong directional forecast. \\emph{Second}, we show that combining foundation\nmodels for time series forecasting with a reliability estimator can filter our\nunreliable predictions. \\emph{Third}, we argue for symbolic reasoning encoding\ndomain rules for transparent justification. These approaches shift emphasize\nexecuting only forecasts that are both reliable and explainable. Experiments on\nequity and cryptocurrency data show that the architecture reduces false\npositives and supports selective execution. By integrating predictive\nperformance with reliability estimation and rule-based reasoning, our framework\nadvances transparent and auditable financial AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4ee5\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u4e3a\u6838\u5fc3\u7684\u91d1\u878dAI\u6846\u67b6\uff0c\u901a\u8fc7 Time-LLM \u8fdb\u884c\u65b9\u5411\u6027\u9884\u6d4b\u7ea6\u675f\u3001\u628a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e0e\u53ef\u9760\u6027\u4f30\u8ba1\u5668\u7ed3\u5408\u7b5b\u9009\u3001\u4ee5\u53ca\u7b26\u53f7\u5316\u63a8\u7406\u7f16\u7801\u9886\u57df\u89c4\u5219\u5b9e\u73b0\u900f\u660e justification\uff0c\u652f\u6301\u6709\u9009\u62e9\u7684\u6267\u884c\u3002", "motivation": "\u91d1\u878d\u9886\u57df\u5bf9\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u548c\u5408\u89c4\u6027\u9700\u6c42\u8d8a\u6765\u8d8a\u9ad8\uff0c\u9ed1\u7bb1\u6a21\u578b\u7684\u4fe1\u4efb\u95e8\u69db\u963b\u788d\u843d\u5730\u3002", "method": "\u4e09\u6b65\u7b56\u7565\uff1a1) Time-LLM \u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\uff0c\u907f\u514d\u9519\u8bef\u65b9\u5411\u7684\u9884\u6d4b\uff1b2) \u5c06\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e0e\u53ef\u9760\u6027\u4f30\u8ba1\u5668\u7ed3\u5408\uff0c\u4ee5\u6ee4\u9664\u4e0d\u53ef\u9760\u9884\u6d4b\uff1b3) \u4f7f\u7528\u7b26\u53f7\u5316\u63a8\u7406\u5c06\u9886\u57df\u89c4\u5219\u7f16\u7801\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u4e0e\u9884\u6d4b\u7ed3\u5408\uff0c\u5f62\u6210\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u5728\u80a1\u7968\u4e0e\u52a0\u5bc6\u8d27\u5e01\u6570\u636e\u4e0a\u663e\u793a\uff0c\u67b6\u6784\u80fd\u964d\u4f4e\u8bef\u62a5\u5e76\u5b9e\u73b0\u6709\u9009\u62e9\u7684\u6267\u884c\uff0c\u63d0\u5347\u9884\u6d4b\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u9884\u6d4b\u6027\u80fd\u3001\u53ef\u9760\u6027\u8bc4\u4f30\u4e0e\u89c4\u5219\u5316\u63a8\u7406\uff0c\u6784\u5efa\u4e86\u66f4\u53ef\u5ba1\u8ba1\u3001\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u7684\u91d1\u878dAI\u6846\u67b6\u3002"}}
{"id": "2510.26369", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26369", "abs": "https://arxiv.org/abs/2510.26369", "authors": ["Kazuma Kano", "Yuki Mori", "Shin Katayama", "Kenta Urano", "Takuro Yonezawa", "Nobuo Kawaguchi"], "title": "CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse", "comment": "7 pages, 3 figures, accepted to IPIN 2025", "summary": "Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.", "AI": {"tldr": "CorVS \u662f\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u4eba\u5458\u8eab\u4efd\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u8ddf\u8e2a\u8f68\u8ff9\u4e0e\u4f20\u611f\u5668\u6d4b\u91cf\u8fdb\u884c\u5bf9\u5e94\u6765\u5b9e\u73b0\u4ed3\u5e93\u4e2d\u7684\u4eba\u5458\u8bc6\u522b\u3002\u8be5\u65b9\u6cd5\u7531\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u8f68\u8ff9\u4e0e\u4f20\u611f\u5668\u5bf9\u7684\u5bf9\u5e94\u6982\u7387\u4e0e\u53ef\u9760\u6027\uff0c\u5e76\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u5bf9\u8fd9\u4e9b\u6982\u7387\u8fdb\u884c\u5339\u914d\uff1b\u5728\u771f\u5b9e\u4ed3\u5e93\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u7269\u6d41\u4ed3\u50a8\u573a\u666f\u4e2d\uff0c\u5355\u7eaf\u4f9d\u8d56\u5916\u89c2\u7684\u8bc6\u522b\u5728\u9690\u79c1\u548c\u73b0\u5b9e\u6761\u4ef6\u4e0b\u5f80\u5f80\u4e0d\u53ef\u884c\u6216\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u5c06\u89c6\u89c9\u8f68\u8ff9\u4e0e\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u5173\u8054\u6709\u671b\u5728\u4e0d\u4f9d\u8d56\u5916\u89c2\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u7684\u4eba\u5458\u5b9a\u4f4d\u4e0e\u8bc6\u522b\u3002\u73b0\u6709\u57fa\u4e8e\u8f68\u8ff9\u4e0e\u4f20\u611f\u5668\u5bf9\u6bd4\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u5bb9\u6613\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u5173\u8054\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff1a\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u9884\u6d4b\u6bcf\u5bf9\u89c6\u89c9\u8f68\u8ff9\u4e0e\u4f20\u611f\u5668\u6d4b\u91cf\u4e4b\u95f4\u7684\u5bf9\u5e94\u6982\u7387\u548c\u53ef\u9760\u6027\uff1b\u968f\u540e\u57fa\u4e8e\u8fd9\u4e9b\u6982\u7387\u4e0e\u53ef\u9760\u6027\u5728\u65f6\u95f4\u4e0a\u5bf9\u8f68\u8ff9\u4e0e\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u5339\u914d\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u8eab\u4efd\u5bf9\u5e94\u3002\u4e3a\u9a8c\u8bc1\u8be5\u601d\u8def\uff0c\u6784\u5efa\u5e76\u4f7f\u7528\u5305\u542b\u771f\u5b9e\u4ed3\u5e93\u4f5c\u4e1a\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5305\u542b\u771f\u5b9e\u4ed3\u5e93\u4f5c\u4e1a\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u76f8\u8f83\u4e8e\u4ec5\u57fa\u4e8e\u5916\u89c2\u7684\u8bc6\u522b\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CorVS \u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u3001\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u4eba\u5458\u8eab\u4efd\u8bc6\u522b\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u8f68\u8ff9\u4e0e\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u5bf9\u5e94\uff0c\u7f13\u89e3\u4e86\u73b0\u5b9e\u6761\u4ef6\u4e0b\u5916\u89c2\u53d7\u9650\u5e26\u6765\u7684\u6311\u6218\uff0c\u5e76\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.26392", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.26392", "abs": "https://arxiv.org/abs/2510.26392", "authors": ["Fatemeh Bazikar", "Hossein Moosaei", "Atefeh Hemmati", "Panos M. Pardalos"], "title": "Multi-Task Learning Based on Support Vector Machines and Twin Support Vector Machines: A Comprehensive Survey", "comment": null, "summary": "Multi-task learning (MTL) enables simultaneous training across related tasks,\nleveraging shared information to improve generalization, efficiency, and\nrobustness, especially in data-scarce or high-dimensional scenarios. While deep\nlearning dominates recent MTL research, Support Vector Machines (SVMs) and Twin\nSVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,\nand effectiveness with small datasets.\n  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting\nshared representations, task regularization, and structural coupling\nstrategies. Special attention is given to emerging TWSVM extensions for\nmulti-task settings, which show promise but remain underexplored. We compare\nthese models in terms of theoretical properties, optimization strategies, and\nempirical performance, and discuss applications in fields such as computer\nvision, natural language processing, and bioinformatics.\n  Finally, we identify research gaps and outline future directions for building\nscalable, interpretable, and reliable margin-based MTL frameworks. This work\nprovides a comprehensive resource for researchers and practitioners interested\nin SVM- and TWSVM-based multi-task learning.", "AI": {"tldr": "\u7efc\u8ff0\u57fa\u4e8eSVM/TWSVM\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6bd4\u8f83\u7406\u8bba\u6027\u8d28\u3001\u4f18\u5316\u7b56\u7565\u548c\u5b9e\u9a8c\u8868\u73b0\uff0c\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u5e94\u7528\u573a\u666f\u3002", "motivation": "MTL\u5728\u6570\u636e\u7a00\u7f3a\u6216\u9ad8\u7ef4\u573a\u666f\u4e2d\u901a\u8fc7\u5171\u4eab\u4fe1\u606f\u63d0\u5347\u6cdb\u5316\u3001\u6548\u7387\u4e0e\u9c81\u68d2\u6027\uff1bSVM/TWSVM\u56e0\u53ef\u89e3\u91ca\u6027\u3001\u7406\u8bba\u4e25\u8c28\u6027\u548c\u5c0f\u6570\u636e\u96c6\u6548\u679c\u800c\u4ecd\u5177\u4ef7\u503c\u3002", "method": "\u7cfb\u7edf\u6027\u56de\u987e\u4e0e\u6bd4\u8f83\uff1a\u805a\u7126\u5171\u4eab\u8868\u793a\u3001\u4efb\u52a1\u6b63\u5219\u5316\u3001\u7ed3\u6784\u8026\u5408\uff0c\u4ee5\u53ca\u5bf9\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684TWSVM\u6269\u5c55\u8fdb\u884c\u68b3\u7406\u4e0e\u8bc4\u4f30\uff1b\u5bf9\u7406\u8bba\u6027\u8d28\u3001\u4f18\u5316\u7b97\u6cd5\u548c\u5b9e\u9a8c\u6027\u80fd\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5e76\u68b3\u7406\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u751f\u7269\u4fe1\u606f\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u6c47\u603b\u5404\u6a21\u578b\u7684\u7406\u8bba\u5c5e\u6027\u3001\u4f18\u5316\u7b56\u7565\u4e0e\u7ecf\u9a8c\u8868\u73b0\uff0c\u63ed\u793a\u5b83\u4eec\u5728\u4e0d\u540c\u6570\u636e\u8bbe\u7f6e\u4e2d\u7684\u4f18\u52a3\u53ca\u9002\u7528\u573a\u666f\uff1b\u5e76\u7ed9\u51fa\u8de8\u9886\u57df\u7684\u5e94\u7528\u8981\u70b9\u3002", "conclusion": "\u6307\u51fa\u5f53\u524d\u7684\u7814\u7a76\u7a7a\u7f3a\uff0c\u63d0\u51fa\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u3001\u9c81\u68d2\u4e14\u57fa\u4e8e\u8fb9\u9645\u635f\u5931\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u7684\u672a\u6765\u65b9\u5411\uff0c\u5305\u62ec\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u3001\u53ef\u89e3\u91ca\u6027\u589e\u5f3a\u548c\u8de8\u4efb\u52a1\u534f\u540c\u7684\u7ed3\u6784\u8bbe\u8ba1\u3002"}}
{"id": "2510.26433", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26433", "abs": "https://arxiv.org/abs/2510.26433", "authors": ["Yucen Wang", "Fengming Zhang", "De-Chuan Zhan", "Li Zhao", "Kaixin Wang", "Jiang Bian"], "title": "Co-Evolving Latent Action World Models", "comment": null, "summary": "Adapting pre-trained video generation models into controllable world models\nvia latent actions is a promising step towards creating generalist world\nmodels. The dominant paradigm adopts a two-stage approach that trains latent\naction model (LAM) and the world model separately, resulting in redundant\ntraining and limiting their potential for co-adaptation. A conceptually simple\nand appealing idea is to directly replace the forward dynamic model in LAM with\na powerful world model and training them jointly, but it is non-trivial and\nprone to representational collapse. In this work, we propose CoLA-World, which\nfor the first time successfully realizes this synergistic paradigm, resolving\nthe core challenge in joint learning through a critical warm-up phase that\neffectively aligns the representations of the from-scratch LAM with the\npre-trained world model. This unlocks a co-evolution cycle: the world model\nacts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,\nwhile the LAM offers a more precise and adaptable control interface to the\nworld model. Empirically, CoLA-World matches or outperforms prior two-stage\nmethods in both video simulation quality and downstream visual planning,\nestablishing a robust and efficient new paradigm for the field.", "AI": {"tldr": "CoLA-World jointly trains a latent action model (LAM) with a pre-trained world model via a warm-up phase, enabling co-evolution and improved video simulation and downstream planning compared to two-stage methods.", "motivation": "Addresses inefficiencies and limited co-adaptation in the standard two-stage LAM+world-model pipeline. Direct joint learning risks representational collapse; requires a warm-up to align representations.", "method": "Introduce CoLA-World with a critical warm-up phase that aligns the from-scratch LAM with the pre-trained world model, enabling joint training where the world model provides gradients to shape LAM while LAM offers a precise control interface. This creates a co-evolution cycle: the world model acts as a tutor and LAM as an adaptable controller.", "result": "Empirically, CoLA-World matches or outperforms previous two-stage methods in video simulation quality and downstream visual planning.", "conclusion": "CoLA-World establishes a robust, efficient new paradigm for controllable world modeling by enabling effective joint learning and co-evolution of LAM and world model."}}
{"id": "2510.26451", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26451", "abs": "https://arxiv.org/abs/2510.26451", "authors": ["Jiayi Luo", "Qingyun Sun", "Beining Yang", "Haonan Yuan", "Xingcheng Fu", "Yanbiao Ma", "Jianxin Li", "Philip S. Yu"], "title": "Robust Graph Condensation via Classification Complexity Mitigation", "comment": null, "summary": "Graph condensation (GC) has gained significant attention for its ability to\nsynthesize smaller yet informative graphs. However, existing studies often\noverlook the robustness of GC in scenarios where the original graph is\ncorrupted. In such cases, we observe that the performance of GC deteriorates\nsignificantly, while existing robust graph learning technologies offer only\nlimited effectiveness. Through both empirical investigation and theoretical\nanalysis, we reveal that GC is inherently an intrinsic-dimension-reducing\nprocess, synthesizing a condensed graph with lower classification complexity.\nAlthough this property is critical for effective GC performance, it remains\nhighly vulnerable to adversarial perturbations. To tackle this vulnerability\nand improve GC robustness, we adopt the geometry perspective of graph data\nmanifold and propose a novel Manifold-constrained Robust Graph Condensation\nframework named MRGC. Specifically, we introduce three graph data manifold\nlearning modules that guide the condensed graph to lie within a smooth,\nlow-dimensional manifold with minimal class ambiguity, thereby preserving the\nclassification complexity reduction capability of GC and ensuring robust\nperformance under universal adversarial attacks. Extensive experiments\ndemonstrate the robustness of \\ModelName\\ across diverse attack scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86 MRGC\uff0c\u901a\u8fc7\u5728\u56fe\u6570\u636e\u6d41\u5f62\u7ea6\u675f\u4e0b\u8fdb\u884c\u56fe\u51dd\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u5e76\u5728\u5bf9\u6297\u6270\u52a8\u4e0b\u4fdd\u6301\u5206\u7c7b\u590d\u6742\u5ea6\u7684\u964d\u4f4e\u3002", "motivation": "GC \u5728\u964d\u7ef4\u7684\u540c\u65f6\u6613\u53d7\u6270\u52a8\u5f71\u54cd\uff0c\u73b0\u6709\u9c81\u68d2\u5b66\u4e60\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u5728\u4fdd\u6301\u4f4e\u7ef4\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u56fe\u6570\u636e\u6d41\u5f62\u5b66\u4e60\u6a21\u5757\uff0c\u5f15\u5bfc\u51dd\u7ec3\u56fe\u5d4c\u5165\u5e73\u6ed1\u3001\u4f4e\u7ef4\u6d41\u5f62\uff0c\u6700\u5c0f\u5316\u7c7b\u522b\u6b67\u4e49\uff0c\u5e76\u5728\u666e\u904d\u5bf9\u6297\u653b\u51fb\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e MRGC \u5728\u591a\u79cd\u653b\u51fb\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u5728\u4fdd\u6301 GC \u964d\u4f4e\u5206\u7c7b\u590d\u6742\u5ea6\u80fd\u529b\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u6d41\u5f62\u7ea6\u675f\u63d0\u5347\u9c81\u68d2\u6027\uff0cMRGC \u6709\u6548\u3002"}}
{"id": "2510.26475", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.26475", "abs": "https://arxiv.org/abs/2510.26475", "authors": ["Qiaoling Chen", "Zijun Liu", "Peng Sun", "Shenggui Li", "Guoteng Wang", "Ziming Liu", "Yonggang Wen", "Siyuan Feng", "Tianwei Zhang"], "title": "ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems", "comment": null, "summary": "Adapting large language models (LLMs) via reinforcement learning (RL) is\noften bottlenecked by the generation stage, which can consume over 75\\% of the\ntraining time. Speculative decoding (SD) accelerates autoregressive generation\nin serving systems, but its behavior under RL training remains largely\nunexplored. We identify three critical gaps that hinder the naive integration\nof SD into RL systems: diminishing speedups at large batch sizes, drafter\nstaleness under continual actor updates, and drafter-induced policy\ndegradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL\nthrough three complementary mechanisms: dynamically tuning SD configurations,\nevolving the drafter via knowledge distillation, and weighting updates by\nrollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup\nwhile preserving reward convergence and training stability, providing a\npractical solution for efficient RL-based LLM adaptation.", "AI": {"tldr": "ReSpec adapts speculative decoding (SD) to reinforcement learning (RL) training for large language models, addressing bottlenecks in generation to achieve substantial speedups without sacrificing convergence or stability.", "motivation": "RL-based fine-tuning of LLMs is often limited by autoregressive generation time. While speculative decoding can speed up generation in serving, its behavior during RL training is underexplored. The paper identifies three gaps\u2014diminishing speedups at large batch sizes, drafter staleness from continual actor updates, and policy degradation induced by the drafter\u2014that hinder naive integration of SD into RL.", "method": "Introduce ReSpec, combining three mechanisms: (1) dynamic tuning of SD configurations during RL training, (2) evolving the drafter via knowledge distillation to keep it aligned with evolving policies, and (3) weighting model updates by rollout rewards to stabilize training.", "result": "Experiments on Qwen models (3B\u201314B) show up to 4.5x speedup in training generation, while preserving reward convergence and training stability.", "conclusion": "ReSpec provides a practical and effective solution for accelerating RL-based LLM adaptation by integrating optimally tuned SD, distillation-driven drafter evolution, and reward-weighted updates, enabling more efficient RL fine-tuning of large language models."}}
{"id": "2510.26491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26491", "abs": "https://arxiv.org/abs/2510.26491", "authors": ["Erle Zhu", "Dazhi Jiang", "Yuan Wang", "Xujun Li", "Jiale Cheng", "Yuxian Gu", "Yilin Niu", "Aohan Zeng", "Jie Tang", "Minlie Huang", "Hongning Wang"], "title": "Data-Efficient RLVR via Off-Policy Influence Guidance", "comment": null, "summary": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\n\\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy\n\\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u79bb\u7ebf\u6570\u636e\u5f71\u54cd\u529b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u7a00\u758f\u968f\u673a\u6295\u5f71\u8fdb\u884c\u9ad8\u7ef4\u964d\u7ef4\uff0c\u6784\u5efaCROPI\uff08Curriculum RL with Off-Policy Influence guidance\uff09\u591a\u9636\u6bb5RL\u6846\u67b6\uff0c\u7528\u4ee5\u5728RLVR\u4e2d\u9ad8\u6548\u5730\u8fdb\u884c\u6570\u636e\u9009\u62e9\u3002\u901a\u8fc7\u79bb\u7ebf\u8f68\u8ff9\u8fd1\u4f3c\u5f71\u54cd\uff0c\u964d\u4f4e\u5728\u7ebf\u6eda\u52a8\u6210\u672c\uff0c\u57281.5B\u53c2\u6570\u6a21\u578b\u4e0a\u8fbe\u52302.66x\u6b65\u7ea7\u52a0\u901f\uff0c\u540c\u65f6\u6bcf\u9636\u6bb5\u4ec5\u7528 10% \u7684\u6570\u636e\uff0c\u76f8\u8f83\u5168\u91cf\u6570\u636e\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u6548\u7387\uff1b\u57287B\u89c4\u6a21\u6a21\u578b\u4e0a\u4e5f\u6709\u663e\u8457\u52a0\u901f\uff0c\u5145\u5206\u5c55\u793a\u5f71\u54cd-based\u6570\u636e\u9009\u62e9\u5728RLVR\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524dRLVR\u9886\u57df\u7684\u6570\u636e\u9009\u62e9\u591a\u4f9d\u8d56\u542f\u53d1\u5f0f\u4e14\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u4e0e\u6cdb\u5316\u6027\uff1b\u5728\u7ebf\u7b56\u7565\u6eda\u52a8\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5bf9\u5927\u89c4\u6a21LLMs\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u9009\u62e9\u3002\u9700\u8981\u4e00\u4e2a\u7406\u8bba\u4e0a\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u79bb\u7ebf\u4f30\u8ba1\u65b9\u6cd5\u6765\u8861\u91cf\u6570\u636e\u70b9\u5bf9\u5b66\u4e60\u76ee\u6807\u7684\u8d21\u732e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u79bb\u7ebf\u5f71\u54cd\u4f30\u8ba1\u6765\u8bc4\u4f30\u5355\u4e2a\u6570\u636e\u70b9\u5bf9\u5b66\u4e60\u76ee\u6807\u7684\u8d21\u732e\uff1b\u901a\u8fc7\u79bb\u7ebf\u6536\u96c6\u7684\u8f68\u8ff9\u8fdb\u884c\u8fd1\u4f3c\u4f30\u8ba1\uff0c\u907f\u514d\u6602\u8d35\u7684\u5728\u7ebf\u7b56\u7565\u6eda\u52a8\u3002\u4e3a\u5e94\u5bf9LLMs\u7684\u9ad8\u7ef4\u68af\u5ea6\uff0c\u91c7\u7528\u7a00\u758f\u968f\u673a\u6295\u5f71\u8fdb\u884c\u964d\u7ef4\u5b58\u50a8\u4e0e\u8ba1\u7b97\u3002\u6784\u5efaCROPI\uff08Curriculum RL with Off-Policy Influence guidance\uff09\uff0c\u5728\u591a\u9636\u6bb5\u8bad\u7ec3\u4e2d\u8fed\u4ee3\u9009\u62e9\u5bf9\u5f53\u524d\u7b56\u7565\u6700\u5177\u5f71\u54cd\u529b\u7684\u6570\u636e\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u9636\u6bb5\u5b9e\u9a8c\u4e2d\uff0cCROPI\u663e\u793a\u663e\u8457\u7684\u8bad\u7ec3\u52a0\u901f\u6548\u679c\uff1b\u57281.5B\u53c2\u6570\u6a21\u578b\u4e0a\u5b9e\u73b02.66\u00d7\u6b65\u7ea7\u52a0\u901f\uff0c\u4e14\u6bcf\u9636\u6bb5\u4ec5\u4f7f\u7528\u5168\u6570\u636e\u96c6\u768410%\u5373\u53ef\u8fbe\u5230\u8be5\u63d0\u5347\uff1b\u57287B\u53c2\u6570\u6a21\u578b\u4e0a\u4ea6\u663e\u793a\u51fa\u8bad\u7ec3\u6548\u7387\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u5f71\u54cd\u7684\u6570\u636e\u9009\u62e9\u5728RLVR\u4e2d\u7684\u53ef\u884c\u6027\u4e0e\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u79bb\u7ebf\u6570\u636e\u5f71\u54cd\u8bc4\u4f30\u7ed3\u5408\u9ad8\u6548\u964d\u7ef4\u4e0e\u5206\u9636\u6bb5\u6570\u636e\u7b5b\u9009\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347RLVR\u4e2d\u7684\u6570\u636e\u5229\u7528\u6548\u7387\uff0cCROPI\u4e3a\u9ad8\u6548\u6570\u636e\u9a71\u52a8\u7684\u591a\u9636\u6bb5RL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u6846\u67b6\u3002"}}
{"id": "2510.26501", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26501", "abs": "https://arxiv.org/abs/2510.26501", "authors": ["Mustafa Fuad Rifet Ibrahim", "Maurice Meijer", "Alexander Schlaefer", "Peer Stelldinger"], "title": "Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters", "comment": "Submitted to the 24th International Conference on Pervasive Computing\n  and Communications (PerCom 2026)", "summary": "Continuous electrocardiogram (ECG) monitoring via wearables offers\nsignificant potential for early cardiovascular disease (CVD) detection.\nHowever, deploying deep learning models for automated analysis in\nresource-constrained environments faces reliability challenges due to\ninevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen\npathologies or noisecorrupted signals, often cause erroneous, high-confidence\npredictions by standard classifiers, compromising patient safety. Existing OOD\ndetection methods either neglect computational constraints or address noise and\nunseen classes separately. This paper explores Unsupervised Anomaly Detection\n(UAD) as an independent, upstream filtering mechanism to improve robustness. We\nbenchmark six UAD approaches, including Deep SVDD, reconstruction-based models,\nMasked Anomaly Detection, normalizing flows, and diffusion models, optimized\nvia Neural Architecture Search (NAS) under strict resource constraints (at most\n512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection\nof OOD CVD classes and signals unsuitable for analysis due to noise. Results\nshow Deep SVDD consistently achieves the best trade-off between detection and\nefficiency. In a realistic deployment simulation, integrating the optimized\nDeep SVDD filter with a diagnostic classifier improved accuracy by up to 21\npercentage points over a classifier-only baseline. This study demonstrates that\noptimized UAD filters can safeguard automated ECG analysis, enabling safer,\nmore reliable continuous cardiovascular monitoring on wearables.", "AI": {"tldr": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u53ef\u7a7f\u6234ECG\u76d1\u6d4b\u573a\u666f\u4e2d\uff0c\u672c\u6587\u8bc4\u4f30\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff08UAD\uff09\u4f5c\u4e3a\u4e0a\u6e38\u8fc7\u6ee4\u5668\u4ee5\u63d0\u5347\u5bf9OOD\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002\u5bf9\u516d\u79cdUAD\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\uff0c\u5305\u62ecDeep SVDD\u3001\u91cd\u5efa\u6a21\u578b\u3001\u63a9\u7801\u5f02\u5e38\u68c0\u6d4b\u3001\u6b63\u5219\u5316\u6d41\u3001\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728512k\u53c2\u6570\u7ea6\u675f\u4e0b\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u8fdb\u884c\u4f18\u5316\u3002\u6570\u636e\u96c6\u4e3aPTB-XL\u548cBUT QDB\u3002\u7ed3\u679c\u663e\u793aDeep SVDD\u5728\u68c0\u6d4b\u6027\u80fd\u4e0e\u8ba1\u7b97\u5f00\u9500\u4e4b\u95f4\u6298\u4e2d\u6700\u4f73\uff1b\u5728\u5b9e\u9645\u90e8\u7f72\u4eff\u771f\u4e2d\uff0c\u5c06\u4f18\u5316\u540e\u7684Deep SVDD\u4e0e\u8bca\u65ad\u5206\u7c7b\u5668\u7ec4\u5408\u80fd\u4f7f\u51c6\u786e\u7387\u6bd4\u5355\u5206\u7c7b\u5668\u57fa\u7ebf\u63d0\u9ad8\u591a\u8fbe21\u4e2a\u767e\u5206\u70b9\u3002\u7ed3\u8bba\u662f\uff0c\u4f18\u5316\u540e\u7684UAD\u8fc7\u6ee4\u5668\u53ef\u63d0\u5347\u53ef\u7a7f\u6234ECG\u5206\u6790\u7684\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u3002", "motivation": "\u53ef\u7a7f\u6234ECG\u76d1\u6d4b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u5e94\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u81ea\u52a8\u5206\u6790\u65f6\uff0c\u5bb9\u6613\u53d7\u5230\u672a\u77e5\u75c5\u7406\u3001\u566a\u58f0\u7b49OOD\u6570\u636e\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u9ad8\u7f6e\u4fe1\u5ea6\u9519\u8bef\u9884\u6d4b\uff0c\u5a01\u80c1\u60a3\u8005\u5b89\u5168\u3002\u56e0\u6b64\u9700\u8981\u5728\u4e0d\u727a\u7272\u8bbe\u5907\u8d44\u6e90\u7684\u524d\u63d0\u4e0b\uff0c\u6784\u5efa\u4e00\u4e2a\u4e0a\u6e38\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u8fc7\u6ee4\u5668\u6765\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u5728PTB-XL\u4e0eBUT QDB\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8e512k\u53c2\u6570\u7684\u7ea6\u675f\uff0c\u6bd4\u8f83\u516d\u79cdUAD\u65b9\u6cd5\uff1aDeep SVDD\u3001\u57fa\u4e8e\u91cd\u5efa\u7684\u6a21\u578b\u3001Masked Anomaly Detection\u3001\u6b63\u5219\u5316\u6d41\u3001\u6269\u6563\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u5bf9\u5b83\u4eec\u8fdb\u884c\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u3002\u8bc4\u4f30\u76ee\u6807\u5305\u62ec\u5bf9OOD\u7684CVD\u7c7b\u522b\u548c\u65e0\u6cd5\u5206\u6790\u7684\u566a\u58f0\u4fe1\u53f7\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u7cfb\u7edf\u6027\u80fd\u3002", "result": "Deep SVDD\u5728\u68c0\u6d4b\u80fd\u529b\u4e0e\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u59cb\u7ec8\u5b9e\u73b0\u6700\u4f73\u6298\u4e2d\uff1b\u5728\u5c06\u4f18\u5316\u540e\u7684Deep SVDD\u4f5c\u4e3a\u4e0a\u6e38\u8fc7\u6ee4\u5668\u63a5\u5165\u8bca\u65ad\u5206\u7c7b\u5668\u7684\u90e8\u7f72\u4eff\u771f\u4e2d\uff0c\u51c6\u786e\u7387\u76f8\u5bf9\u4e8e\u4ec5\u4f7f\u7528\u5206\u7c7b\u5668\u7684\u57fa\u7ebf\u63d0\u9ad8\u6700\u9ad8\u53ef\u8fbe21\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u7ecf\u8fc7\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684NAS\u4f18\u5316\u7684UAD\u8fc7\u6ee4\u5668\u80fd\u591f\u63d0\u5347ECG\u81ea\u52a8\u5206\u6790\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\uff0c\u4f7f\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u7684\u8fde\u7eed\u5fc3\u8840\u7ba1\u76d1\u6d4b\u66f4\u53ef\u9760\u3002"}}
{"id": "2510.26510", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26510", "abs": "https://arxiv.org/abs/2510.26510", "authors": ["Youssef Attia El Hili", "Albert Thomas", "Malik Tiomoko", "Abdelhakim Benechehab", "Corentin L\u00e9ger", "Corinne Ancourt", "Bal\u00e1zs K\u00e9gl"], "title": "LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection", "comment": "27 pages, 6 figures", "summary": "Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization.", "AI": {"tldr": "LLMs can act as in-context meta-learners for model and hyperparameter selection by converting datasets into metadata and prompting for recommendations; both zero-shot and meta-informed prompting; achieves competitive performance without search; demonstrates in-context meta-learning.", "motivation": "Model and hyperparameter selection is critical yet expensive; traditional approaches rely on expert intuition or exhaustive search. The work investigates whether LLMs can leverage metadata to guide selection, reducing search cost.", "method": "Transform each dataset into interpretable metadata. Prompt an LLM to recommend model families and hyperparameters. Compare zero-shot prompting (relying on pretrained knowledge) with meta-informed prompting that includes examples of models and their performance on past tasks. Evaluate on synthetic and real-world benchmarks.", "result": "LLMs can use dataset metadata to propose competitive models and hyperparameters without explicit search. Meta-informed prompting yields further improvements, indicating in-context meta-learning capabilities.", "conclusion": "LLMs can serve as lightweight, general-purpose assistants for model selection and hyperparameter optimization, offering a promising approach to reduce search costs and enable practical AutoML-like capabilities."}}
{"id": "2510.26519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26519", "abs": "https://arxiv.org/abs/2510.26519", "authors": ["Hsiu-Yuan Huang", "Chenming Tang", "Weijie Liu", "Saiyong Yang", "Yunfang Wu"], "title": "Think Outside the Policy: In-Context Steered Policy Optimization", "comment": "Work in progress", "summary": "Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such\nas Group Relative Policy Optimization (GRPO), have achieved remarkable progress\nin improving the reasoning capabilities of Large Reasoning Models (LRMs).\nHowever, they exhibit limited exploration due to reliance on on-policy rollouts\nwhere confined to the current policy's distribution, resulting in narrow\ntrajectory diversity. Recent approaches attempt to expand policy coverage by\nincorporating trajectories generated from stronger expert models, yet this\nreliance increases computational cost and such advaned models are often\ninaccessible. To address these issues, we propose In-Context Steered Policy\nOptimization (ICPO), a unified framework that leverages the inherent in-context\nlearning capability of LRMs to provide expert guidance using existing datasets.\nICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands\nexploration beyond the current policy distribution without requiring advanced\nLRM trajectories. To further stabilize optimization, ICPO integrates Expert\nRegion Reject Sampling to filter unreliable off-policy trajectories and\nAnnealed Expert-Bonus Reward Shaping to balance early expert guidance with\nlater autonomous improvement. Results demonstrate that ICPO consistently\nenhances reinforcement learning performance and training stability on\nmathematical reasoning benchmarks, revealing a scalable and effective RLVR\nparadigm for LRMs.", "AI": {"tldr": "ICPO\u901a\u8fc7\u5728\u4e0a\u4e0b\u6587\u4e2d\u5b66\u4e60\u63d0\u4f9b\u4e13\u5bb6\u5f15\u5bfc\uff0c\u63d0\u51fa\u6df7\u5408\u7b56\u7565GRPO\u4e0e\u9690\u5f0f\u4e13\u5bb6\u5f3a\u5236\u3001\u4e13\u5bb6\u533a\u57df\u62d2\u7edd\u91c7\u6837\u548c\u9000\u706b\u578b\u4e13\u5bb6\u5956\u52b1 shaping\u7b49\u7ec4\u4ef6\uff0c\u4ee5\u6269\u5927\u63a2\u7d22\u3001\u63d0\u5347\u7a33\u5b9a\u6027\uff0c\u5728LRMs\u7684RLVR\u4efb\u52a1\uff08\u5c24\u5176\u6570\u5b66\u63a8\u7406\u57fa\u51c6\uff09\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728\u63a2\u7d22\u65b9\u9762\u53d7\u9650\uff0c\u56e0\u4ec5\u4f9d\u8d56\u5f53\u524d\u7b56\u7565\u5206\u5e03\u7684on-policy\u56de\u653e\uff1b\u4f7f\u7528\u5f3a\u5927\u4e13\u5bb6\u6a21\u578b\u6765\u6269\u5c55\u7b56\u7565\u5c3d\u7ba1\u6709\u6548\uff0c\u5374\u6210\u672c\u9ad8\u4e14\u5f80\u5f80\u4e0d\u53ef\u5f97\u3002\u9700\u8981\u4e00\u4e2a\u65e0\u9700\u989d\u5916\u6602\u8d35\u4e13\u5bb6\u8f68\u8ff9\u5373\u53ef\u63d0\u5347\u63a2\u7d22\u4e0e\u7a33\u5b9a\u6027\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faICPO\u6846\u67b6\uff0c\u5229\u7528LRMs\u7684\u5728-context\u5b66\u4e60\u80fd\u529b\uff0c\u901a\u8fc7\u73b0\u6709\u6570\u636e\u96c6\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff1b\u5f15\u5165\u6df7\u5408\u7b56\u7565GRPO\u4e0e\u9690\u5f0f\u4e13\u5bb6\u5f3a\u5236\uff1b\u5f15\u5165\u4e13\u5bb6\u533a\u57df\u62d2\u7edd\u91c7\u6837\u4ee5\u8fc7\u6ee4\u4e0d\u53ef\u9760\u7684\u79bb\u7b56\u7565\u8f68\u8ff9\uff1b\u5f15\u5165\u9000\u706b\u578b\u4e13\u5bb6\u5956\u52b1 shaping\u4ee5\u5e73\u8861\u521d\u671f\u4e13\u5bb6\u5f15\u5bfc\u548c\u540e\u671f\u81ea\u4e3b\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aICPO\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347RLVR\u7684\u6027\u80fd\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u8868\u660e\u5176\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684LRMs RLVR\u8303\u5f0f\u3002", "conclusion": "ICPO\u4e3aLRMs\u7684RLVR\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u7a33\u5b9a\u4e14\u65e0\u9700\u9ad8\u6210\u672c\u5916\u90e8\u4e13\u5bb6\u8f68\u8ff9\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6709\u671b\u63a8\u52a8\u66f4\u5e7f\u6cdb\u7684\u63a8\u7406\u4efb\u52a1\u5e94\u7528\u3002"}}
{"id": "2510.26527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26527", "abs": "https://arxiv.org/abs/2510.26527", "authors": ["Ruilin Wang", "Huixia Li", "Yuexiao Ma", "Xiawu Zheng", "Fei Chao", "Xuefeng Xiao", "Rongrong Ji"], "title": "Polybasic Speculative Decoding Through a Theoretical Perspective", "comment": null, "summary": "Inference latency stands as a critical bottleneck in the large-scale\ndeployment of Large Language Models (LLMs). Speculative decoding methods have\nrecently shown promise in accelerating inference without compromising the\noutput distribution. However, existing work typically relies on a dualistic\ndraft-verify framework and lacks rigorous theoretical grounding. In this paper,\nwe introduce a novel \\emph{polybasic} speculative decoding framework,\nunderpinned by a comprehensive theoretical analysis. Specifically, we prove a\nfundamental theorem that characterizes the optimal inference time for\nmulti-model speculative decoding systems, shedding light on how to extend\nbeyond the dualistic approach to a more general polybasic paradigm. Through our\ntheoretical investigation of multi-model token generation, we expose and\noptimize the interplay between model capabilities, acceptance lengths, and\noverall computational cost. Our framework supports both standalone\nimplementation and integration with existing speculative techniques, leading to\naccelerated performance in practice. Experimental results across multiple model\nfamilies demonstrate that our approach yields speedup ratios ranging from\n$3.31\\times$ to $4.01\\times$ for LLaMA2-Chat 7B, up to $3.87 \\times$ for\nLLaMA3-8B, up to $4.43 \\times$ for Vicuna-7B and up to $3.85 \\times$ for\nQwen2-7B -- all while preserving the original output distribution. We release\nour theoretical proofs and implementation code to facilitate further\ninvestigation into polybasic speculative decoding.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u578b\u7684\u201c\u591a\u57fa\uff08polybasic\uff09 speculative decoding\u201d\u6846\u67b6\uff0c\u7ed9\u51fa\u7406\u8bba\u6700\u4f18\u63a8\u7406\u65f6\u95f4\u7684\u754c\u5b9a\uff0c\u5e76\u5728\u591a\u6a21\u578b\u751f\u6210\u4e2d\u4f18\u5316\u80fd\u529b\u3001\u63a5\u53d7\u957f\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u7684\u6743\u8861\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5bf9\u591a\u79cd\u6a21\u578b\u80fd\u663e\u8457\u52a0\u901f\u63a8\u7406\u4e14\u4fdd\u6301\u539f\u59cb\u8f93\u51fa\u5206\u5e03\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e drafts \u4e0e verify \u7684 dualistic \u65b9\u5f0f\u5728\u7406\u8bba\u652f\u6491\u4e0d\u8db3\u7684\u95ee\u9898\uff1b\u63a8\u7406\u5ef6\u8fdf\u6210\u4e3a\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u74f6\u9888\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u4e00\u822c\u5316\u3001\u5177\u7406\u8bba\u652f\u6491\u7684\u591a\u57fa\uff08polybasic\uff09 speculative \u89e3\u6cd5\u6765\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5e76\u8bc1\u660e\u4e00\u4e2a\u57fa\u672c\u5b9a\u7406\uff0c\u523b\u753b\u591a\u6a21\u578b speculative \u89e3\u7801\u7cfb\u7edf\u7684\u6700\u4f18\u63a8\u7406\u65f6\u95f4\uff1b\u5206\u6790\u591a\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u80fd\u529b\u3001\u63a5\u53d7\u957f\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u8026\u5408\u4e0e\u6743\u8861\uff1b\u7ed9\u51fa\u80fd\u591f\u72ec\u7acb\u5b9e\u73b0\u6216\u4e0e\u73b0\u6709 speculative \u6280\u672f\u534f\u540c\u7684 polybasic \u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8df5\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u591a\u6a21\u578b\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff1a\u5bf9 LLaMA2-Chat 7B \u7684\u901f\u5ea6\u63d0\u5347\u8303\u56f4\u4e3a 3.31\u00d7\u20114.01\u00d7\uff0c\u5bf9 LLaMA3-8B \u4e3a 3.87\u00d7\uff0c\u5bf9 Vicuna-7B \u4e3a 4.43\u00d7\uff0c\u5bf9 Qwen2-7B \u4e3a 3.85\u00d7\uff1b\u4e14\u4fdd\u6301\u539f\u59cb\u8f93\u51fa\u5206\u5e03\uff0c\u4e0d\u5f15\u5165\u989d\u5916\u504f\u5dee\u3002", "conclusion": "\u7406\u8bba\u8bc1\u660e\u4e0e\u5b9e\u73b0\u76f8\u7ed3\u5408\uff0c\u8bc1\u660e\u4e86 polybasic speculative decoding \u7684\u666e\u9002\u6027\u4e0e\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6846\u67b6\u5e76\u516c\u5f00\u76f8\u5173\u8bc1\u660e\u4e0e\u5b9e\u73b0\u4ee3\u7801\uff0c\u4fbf\u4e8e\u540e\u7eed\u5728\u4e0d\u540c\u6a21\u578b\u4e0e\u573a\u666f\u4e2d\u7684\u63a8\u5e7f\u4e0e\u7814\u7a76\u3002"}}
{"id": "2510.26533", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.26533", "abs": "https://arxiv.org/abs/2510.26533", "authors": ["Adrien Weihs", "Andrea Bertozzi", "Matthew Thorpe"], "title": "Higher-Order Regularization Learning on Hypergraphs", "comment": null, "summary": "Higher-Order Hypergraph Learning (HOHL) was recently introduced as a\nprincipled alternative to classical hypergraph regularization, enforcing\nhigher-order smoothness via powers of multiscale Laplacians induced by the\nhypergraph structure. Prior work established the well- and ill-posedness of\nHOHL through an asymptotic consistency analysis in geometric settings. We\nextend this theoretical foundation by proving the consistency of a truncated\nversion of HOHL and deriving explicit convergence rates when HOHL is used as a\nregularizer in fully supervised learning. We further demonstrate its strong\nempirical performance in active learning and in datasets lacking an underlying\ngeometric structure, highlighting HOHL's versatility and robustness across\ndiverse learning settings.", "AI": {"tldr": "\u6269\u5c55 HOHL \u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u622a\u65ad\u7248\u672c\u7684\u4e00\u81f4\u6027\u5e76\u7ed9\u51fa\u6536\u655b\u901f\u7387\uff0c\u540c\u65f6\u5728\u6709\u76d1\u7763\u5b66\u4e60\u4e2d\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\u5e94\u7528\uff0c\u4e14\u5728\u4e3b\u52a8\u5b66\u4e60\u53ca\u7f3a\u4e4f\u51e0\u4f55\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63d0\u5347\u8d85\u56fe\u6b63\u5219\u5316\u7684\u9ad8\u9636\u5e73\u6ed1\u6027\u5efa\u6a21\uff0c\u89e3\u51b3 HOHL \u7684\u4e00\u81f4\u6027\u4e0e\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u5230\u975e\u51e0\u4f55\u7ed3\u6784\u6570\u636e\u7684\u9c81\u68d2\u6027\u4e0e\u5e7f\u6cdb\u5e94\u7528\u573a\u666f\u3002", "method": "\u7ed9\u51fa\u622a\u65ad\u7248\u672c\u7684\u7406\u8bba\u4e00\u81f4\u6027\u8bc1\u660e\u5e76\u63a8\u5bfc\u5728\u5168\u76d1\u7763\u5b66\u4e60\u4e2d\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\u7684\u663e\u5f0f\u6536\u655b\u901f\u7387\uff1b\u5229\u7528\u9ad8\u9636\u5e42\u6b21\u7684\u591a\u5c3a\u5ea6\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff08\u7531\u8d85\u56fe\u7ed3\u6784\u8bf1\u5bfc\uff09\u5b9e\u73b0\u9ad8\u9636\u5e73\u6ed1\u6027\uff1b\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u5728\u4e3b\u52a8\u5b66\u4e60\u4e0e\u7f3a\u4e4f\u51e0\u4f55\u7ed3\u6784\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u5305\u62ec\u622a\u65ad HOHL \u7684\u4e00\u81f4\u6027\u8bc1\u660e\u53ca\u663e\u5f0f\u6536\u655b\u901f\u7387\uff1b\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u7f3a\u4e4f\u51e0\u4f55\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e0a\u5177\u6709\u5f3a\u52b2\u7684\u7ecf\u9a8c\u6027\u80fd\uff0c\u8868\u660e HOHL \u5728\u591a\u6837\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u7684\u53ef\u7528\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "HOHL \u4f5c\u4e3a\u4e00\u79cd\u66f4\u5e7f\u6cdb\u7684\u9ad8\u9636\u6b63\u5219\u5316\u5de5\u5177\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u7406\u8bba\u4fdd\u969c\u4e0e\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b66\u4e60\u573a\u666f\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5065\u3001\u66f4\u7075\u6d3b\u7684\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2510.26557", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26557", "abs": "https://arxiv.org/abs/2510.26557", "authors": ["Jan Stenkamp", "Nina Herrmann", "Benjamin Karic", "Stefan Oehmcke", "Fabian Gieseke"], "title": "Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices", "comment": null, "summary": "Deploying machine learning models on compute-constrained devices has become a\nkey building block of modern IoT applications. In this work, we present a\ncompression scheme for boosted decision trees, addressing the growing need for\nlightweight machine learning models. Specifically, we provide techniques for\ntraining compact boosted decision tree ensembles that exhibit a reduced memory\nfootprint by rewarding, among other things, the reuse of features and\nthresholds during training. Our experimental evaluation shows that models\nachieved the same performance with a compression ratio of 4-16x compared to\nLightGBM models using an adapted training process and an alternative memory\nlayout. Once deployed, the corresponding IoT devices can operate independently\nof constant communication or external energy supply, and, thus, autonomously,\nrequiring only minimal computing power and energy. This capability opens the\ndoor to a wide range of IoT applications, including remote monitoring, edge\nanalytics, and real-time decision making in isolated or power-limited\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u53d7\u9650\u8bbe\u5907\u7684\u63d0\u5347\u6811\u538b\u7f29\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u9636\u6bb5\u5956\u52b1\u7279\u5f81\u4e0e\u9608\u503c\u7684\u590d\u7528\uff0c\u5b9e\u73b04-16x\u7684\u5185\u5b58\u538b\u7f29\uff0c\u4e0e LightGBM \u76f8\u6bd4\u4fdd\u6301\u76f8\u4f3c\u6027\u80fd\uff0c\u652f\u6301\u8fb9\u7f18\u7269\u8054\u7f51\u8bbe\u5907\u79bb\u7ebf\u81ea\u6301\u8fd0\u884c\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u666e\u904d\u5b58\u5728\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u80fd\u8017\u7684\u9650\u5236\uff0c\u8feb\u5207\u9700\u8981\u5728\u672c\u5730\u79bb\u7ebf\u8fd0\u884c\u7684\u8f7b\u91cf\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u63d0\u5347\u6811\u96c6\u6210\u5728\u6b64\u573a\u666f\u4e0b\u5f80\u5f80\u5360\u7528\u8f83\u591a\u5185\u5b58\uff0c\u9650\u5236\u4e86\u90e8\u7f72\u3002", "method": "\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u5956\u52b1\u673a\u5236\uff0c\u9f13\u52b1\u7279\u5f81\u548c\u9608\u503c\u7684\u91cd\u590d\u4f7f\u7528\uff0c\u8bbe\u8ba1\u66ff\u4ee3\u7684\u5185\u5b58\u5e03\u5c40\u4ee5\u964d\u4f4e\u5197\u4f59\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u8bad\u7ec3\u6d41\u7a0b\u4ea7\u51fa\u7d27\u51d1\u7684\u63d0\u5347\u6811\u96c6\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5728\u4fdd\u6301\u76f8\u540c\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u53ef\u5b9e\u73b0\u76f8\u5bf9\u4e8e LightGBM \u76844-16x\u5185\u5b58\u538b\u7f29\uff1b\u5e76\u4e14\u6a21\u578b\u4e00\u65e6\u90e8\u7f72\uff0c\u7269\u8054\u7f51\u8bbe\u5907\u53ef\u5728\u65e0\u9700\u6301\u7eed\u901a\u4fe1\u6216\u5916\u90e8\u80fd\u6e90\u7684\u6761\u4ef6\u4e0b\u81ea\u4e3b\u8fd0\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8fb9\u7f18\u5206\u6790\u3001\u5b9e\u65f6\u51b3\u7b56\u7b49IoT\u5e94\u7528\u5728\u8fdc\u7a0b\u6216\u4f4e\u80fd\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8f7b\u91cf\u5316ML\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u667a\u80fd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.26607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26607", "abs": "https://arxiv.org/abs/2510.26607", "authors": ["Maksim Maslov", "Alexander Kugaevskikh", "Matthew Ivanov"], "title": "Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis", "comment": null, "summary": "This paper considers the problem of regression over distributions, which is\nbecoming increasingly important in machine learning. Existing approaches often\nignore the geometry of the probability space or are computationally expensive.\nTo overcome these limitations, a new method is proposed that combines the\nparameterization of probability trajectories using a Bernstein basis and the\nminimization of the Wasserstein distance between distributions. The key idea is\nto model a conditional distribution as a smooth probability trajectory defined\nby a weighted sum of Gaussian components whose parameters -- the mean and\ncovariance -- are functions of the input variable constructed using Bernstein\npolynomials. The loss function is the averaged squared Wasserstein distance\nbetween the predicted Gaussian distributions and the empirical data, which\ntakes into account the geometry of the distributions. An autodiff-based\noptimization method is used to train the model. Experiments on synthetic\ndatasets that include complex trajectories demonstrated that the proposed\nmethod provides competitive approximation quality in terms of the Wasserstein\ndistance, Energy Distance, and RMSE metrics, especially in cases of pronounced\nnonlinearity. The model demonstrates trajectory smoothness that is better than\nor comparable to alternatives and robustness to changes in data structure,\nwhile maintaining high interpretability due to explicit parameterization via\ncontrol points. The developed approach represents a balanced solution that\ncombines geometric accuracy, computational practicality, and interpretability.\nProspects for further research include extending the method to non-Gaussian\ndistributions, applying entropy regularization to speed up computations, and\nadapting the approach to working with high-dimensional data for approximating\nsurfaces and more complex structures.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26616", "abs": "https://arxiv.org/abs/2510.26616", "authors": ["Lin Xu", "Xinyun Yuan", "Yuxuan Liang", "Suwan Yin", "Yuankai Wu"], "title": "Aeolus: A Multi-structural Flight Delay Dataset", "comment": null, "summary": "We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed\nto advance research on flight delay prediction and support the development of\nfoundation models for tabular data. Existing datasets in this domain are\ntypically limited to flat tabular structures and fail to capture the\nspatiotemporal dynamics inherent in delay propagation. Aeolus addresses this\nlimitation by providing three aligned modalities: (i) a tabular dataset with\nrich operational, meteorological, and airportlevel features for over 50 million\nflights; (ii) a flight chain module that models delay propagation along\nsequential flight legs, capturing upstream and downstream dependencies; and\n(iii) a flight network graph that encodes shared aircraft, crew, and airport\nresource connections, enabling cross-flight relational reasoning. The dataset\nis carefully constructed with temporal splits, comprehensive features, and\nstrict leakage prevention to support realistic and reproducible machine\nlearning evaluation. Aeolus supports a broad range of tasks, including\nregression, classification, temporal structure modeling, and graph learning,\nserving as a unified benchmark across tabular, sequential, and graph\nmodalities. We release baseline experiments and preprocessing tools to\nfacilitate adoption. Aeolus fills a key gap for both domain-specific modeling\nand general-purpose structured data research.Our source code and data can be\naccessed at https://github.com/Flnny/Delay-data", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26633", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26633", "abs": "https://arxiv.org/abs/2510.26633", "authors": ["Colin Doumont", "Victor Picheny", "Viacheslav Borovitskiy", "Henry Moss"], "title": "Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization", "comment": null, "summary": "Bayesian Optimization (BO) has the potential to solve various combinatorial\ntasks, ranging from materials science to neural architecture search. However,\nBO requires specialized kernels to effectively model combinatorial domains.\nRecent efforts have introduced several combinatorial kernels, but the\nrelationships among them are not well understood. To bridge this gap, we\ndevelop a unifying framework based on heat kernels, which we derive in a\nsystematic way and express as simple closed-form expressions. Using this\nframework, we prove that many successful combinatorial kernels are either\nrelated or equivalent to heat kernels, and validate this theoretical claim in\nour experiments. Moreover, our analysis confirms and extends the results\npresented in Bounce: certain algorithms' performance decreases substantially\nwhen the unknown optima of the function do not have a certain structure. In\ncontrast, heat kernels are not sensitive to the location of the optima. Lastly,\nwe show that a fast and simple pipeline, relying on heat kernels, is able to\nachieve state-of-the-art results, matching or even outperforming certain slow\nor complex algorithms.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26643", "abs": "https://arxiv.org/abs/2510.26643", "authors": ["Emmanouil Sylligardos", "John Paparrizos", "Themis Palpanas", "Pierre Senellart", "Paul Boniol"], "title": "MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection", "comment": "25 pages, 13 figures, VLDB Journal", "summary": "Anomaly detection is a fundamental task for time series analytics with\nimportant implications for the downstream performance of many applications.\nDespite increasing academic interest and the large number of methods proposed\nin the literature, recent benchmarks and evaluation studies demonstrated that\nno overall best anomaly detection methods exist when applied to very\nheterogeneous time series datasets. Therefore, the only scalable and viable\nsolution to solve anomaly detection over very different time series collected\nfrom diverse domains is to propose a model selection method that will select,\nbased on time series characteristics, the best anomaly detection methods to\nrun. Existing AutoML solutions are, unfortunately, not directly applicable to\ntime series anomaly detection, and no evaluation of time series-based\napproaches for model selection exists. Towards that direction, this paper\nstudies the performance of time series classification methods used as model\nselection for anomaly detection. In total, we evaluate 234 model configurations\nderived from 16 base classifiers across more than 1980 time series, and we\npropose the first extensive experimental evaluation of time series\nclassification as model selection for anomaly detection. Our results\ndemonstrate that model selection methods outperform every single anomaly\ndetection method while being in the same order of magnitude regarding execution\ntime. This evaluation is the first step to demonstrate the accuracy and\nefficiency of time series classification algorithms for anomaly detection, and\nrepresents a strong baseline that can then be used to guide the model selection\nstep in general AutoML pipelines. Preprint version of an article accepted at\nthe VLDB Journal.", "AI": {"tldr": "Time series anomaly detection can be improved by model selection using time series classification; a large-scale evaluation shows that selecting models via TS classification outperforms any single anomaly detector with similar run-time, suggesting a strong baseline for AutoML pipelines.", "motivation": "There is no universally best anomaly detector for heterogeneous time series; scalable solutions require selecting the most suitable detector based on time series characteristics.", "method": "Evaluate 234 configurations drawn from 16 base classifiers for anomaly detection across ~1980 time series; treat time series classification methods as a model selection layer for anomaly detection; compare against baseline anomaly detectors in terms accuracy and runtime.", "result": "Model selection methods based on TS classification outperform every single anomaly detector while maintaining similar execution times, establishing a strong baseline for model selection in TS AutoML pipelines.", "conclusion": "Time series classification-based model selection is effective for anomaly detection in heterogeneous time series; this approach provides a practical and scalable component for AutoML pipelines and serves as a benchmark for future TS anomaly-detection model selection studies."}}
{"id": "2510.26645", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26645", "abs": "https://arxiv.org/abs/2510.26645", "authors": ["Katarina Petrovi\u0107", "Lazar Atanackovic", "Viggo Moro", "Kacper Kapu\u015bniak", "\u0130smail \u0130lkan Ceylan", "Michael Bronstein", "Avishek Joey Bose", "Alexander Tong"], "title": "Curly Flow Matching for Learning Non-gradient Field Dynamics", "comment": "Accepted to NeurIPS 2025", "summary": "Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git", "AI": {"tldr": "Curly-FM \u901a\u8fc7\u5728\u975e\u96f6\u6f02\u79fb\u53c2\u8003\u8fc7\u7a0b\u4e0b\u6c42\u89e3 Schr\u00f6dinger \u6865\u95ee\u9898\uff0c\u5b66\u4e60\u975e\u68af\u5ea6\u3001\u5468\u671f\u6027\u7684\u52a8\u529b\u5b66\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u68af\u5ea6\u573a\u5339\u914d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u7cfb\u7edf\u5e38\u5448\u73b0\u975e\u68af\u5ea6\u3001\u5468\u671f\u6027\u884c\u4e3a\uff0c\u73b0\u6709\u7684\u6d41\u52a8/\u6865\u63a5\u5339\u914d\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\uff1b\u9700\u8981\u5f15\u5165\u5305\u542b\u63a8\u65ad\u901f\u5ea6\u7684\u53c2\u8003\u4fe1\u606f\u6765\u5efa\u6a21\u3002", "method": "\u63d0\u51fa Curly Flow Matching\uff0c\u91c7\u7528\u5e26\u975e\u96f6\u6f02\u79fb\u53c2\u8003\u8fc7\u7a0b\u7684 Schr\u00f6dinger \u6865\u95ee\u9898\u6765\u5b66\u4e60\u52a8\u529b\u5b66\uff1b\u7ed3\u5408\u63a8\u65ad\u7684\u901f\u5ea6\u548c\u603b\u4f53\u5206\u5e03\uff08\u6837\u672c\u8fb9\u9645\uff09\u6765\u6784\u9020\u53c2\u8003\u8fc7\u7a0b\u3002", "result": "\u5728\u5355\u7ec6\u80de\u8f68\u8ff9\u63a8\u65ad\u3001\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\u4e0e\u6d77\u6d0b\u73af\u6d41\u7b49\u4efb\u52a1\u4e2d\u5c55\u793a\uff0cCurly-FM \u80fd\u66f4\u597d\u5730\u62df\u5408\u53c2\u8003\u8fc7\u7a0b\u548c\u8fb9\u9645\u5206\u5e03\u3002", "conclusion": "\u5c06\u6d41\u52a8\u5339\u914d\u6269\u5c55\u5230\u5177\u6709\u5468\u671f\u6027\u884c\u4e3a\u7684\u7269\u7406\u7cfb\u7edf\uff0c\u63d0\u5347\u5bf9\u975e\u68af\u5ea6\u52a8\u529b\u5b66\u7684\u5efa\u6a21\u80fd\u529b\uff1b\u4ee3\u7801\u5f00\u6e90\uff0c\u53ef\u5728 GitHub \u83b7\u53d6\u3002"}}
{"id": "2510.26679", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.26679", "abs": "https://arxiv.org/abs/2510.26679", "authors": ["Tommaso d'Orsi", "Gleb Novikov"], "title": "Tight Differentially Private PCA via Matrix Coherence", "comment": "SODA 2026; equal contribution", "summary": "We revisit the task of computing the span of the top $r$ singular vectors\n$u_1, \\ldots, u_r$ of a matrix under differential privacy. We show that a\nsimple and efficient algorithm -- based on singular value decomposition and\nstandard perturbation mechanisms -- returns a private rank-$r$ approximation\nwhose error depends only on the \\emph{rank-$r$ coherence} of $u_1, \\ldots, u_r$\nand the spectral gap $\\sigma_r - \\sigma_{r+1}$. This resolves a question posed\nby Hardt and Roth~\\cite{hardt2013beyond}. Our estimator outperforms the state\nof the art -- significantly so in some regimes. In particular, we show that in\nthe dense setting, it achieves the same guarantees for single-spike PCA in the\nWishart model as those attained by optimal non-private algorithms, whereas\nprior private algorithms failed to do so.\n  In addition, we prove that (rank-$r$) coherence does not increase under\nGaussian perturbations. This implies that any estimator based on the Gaussian\nmechanism -- including ours -- preserves the coherence of the input. We\nconjecture that similar behavior holds for other structured models, including\nplanted problems in graphs.\n  We also explore applications of coherence to graph problems. In particular,\nwe present a differentially private algorithm for Max-Cut and other constraint\nsatisfaction problems under low coherence assumptions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u9ad8\u6548\u7684\u5dee\u5206\u9690\u79c1 rank-r \u8fd1\u4f3c\u7b97\u6cd5\uff0c\u57fa\u4e8e SVD \u4e0e\u5e38\u89c4\u6270\u52a8\u673a\u5236\uff0c\u8bef\u5dee\u4ec5\u4f9d\u8d56\u4e8e\u79e9-r \u76f8\u5e72\u6027\u548c\u8c31\u88c2\u9699\uff0c\u80fd\u591f\u5728\u591a\u79cd\u8bbe\u7f6e\u63a5\u8fd1\u975e\u79c1\u6709\u6700\u4f18\uff0c\u4e14\u6269\u5c55\u5230\u56fe\u7684\u79c1\u6709\u5316\u95ee\u9898\u3002", "motivation": "\u56de\u7b54 Hardt \u4e0e Roth \u63d0\u51fa\u7684\u95ee\u9898\uff0c\u7814\u7a76\u79e9-r \u76f8\u5e72\u6027\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u5bf9\u8bef\u5dee\u7684\u51b3\u5b9a\u4f5c\u7528\uff0c\u5e76\u68c0\u9a8c\u9ad8\u65af\u6270\u52a8\u5bf9\u76f8\u5e72\u6027\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u76f8\u5e72\u6027\u5728\u56fe\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7279\u5f81\u503c\u5206\u89e3\u7684\u7b80\u5355\u7b97\u6cd5\uff0c\u7ed3\u5408\u9ad8\u65af/\u62c9\u666e\u62c9\u65af\u7b49\u6807\u51c6\u6270\u52a8\u673a\u5236\uff0c\u5f97\u5230\u4e00\u4e2a\u79c1\u6709\u7684\u79e9-r \u8fd1\u4f3c\uff1b\u5bf9\u8bef\u5dee\u8fdb\u884c\u7406\u8bba\u754c\u5b9a\uff0c\u663e\u793a\u5176\u6536\u655b\u4e0e\u8c31\u88c2\u9699\u76f8\u5173\uff0c\u5e76\u8bc1\u660e\u5728\u9ad8\u65af\u673a\u5236\u4e0b\u76f8\u5e72\u6027\u4e0d\u589e\u3002", "result": "\u5728\u5bc6\u96c6\u573a\u666f\u4e0b\uff0c\u5355\u5c16\u5cf0 PCA \u7684 Wishart \u6a21\u578b\u4e0b\uff0c\u8fbe\u5230\u4e0e\u6700\u4f73\u975e\u79c1\u6709\u7b97\u6cd5\u76f8\u540c\u7684\u4fdd\u8bc1\uff1b\u76f8\u6bd4\u73b0\u6709\u79c1\u6709\u7b97\u6cd5\u6709\u663e\u8457\u6539\u5584\uff1b\u5e76\u7ed9\u51fa\u79c1\u6709\u7684 Max-Cut/ CSP \u7b97\u6cd5\u5728\u4f4e\u76f8\u5e72\u5047\u8bbe\u4e0b\u7684\u5b9e\u73b0\u3002", "conclusion": "\u79e9-r \u76f8\u5e72\u6027\u53ca\u8c31\u88c2\u9699\u6210\u4e3a\u5dee\u5206\u9690\u79c1\u4e0b\u77e9\u9635\u8fd1\u4f3c\u7684\u6838\u5fc3\u6307\u6807\uff1b\u9ad8\u65af\u6270\u52a8\u4fdd\u6301\u8f93\u5165\u76f8\u5e72\u6027\uff0c\u53ef\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u7ed3\u6784\u6a21\u578b\u5982\u56fe\u4e2d\u7684 planted \u95ee\u9898\uff1b\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u6269\u5c55\u5230\u66f4\u591a\u7ed3\u6784\u5316\u6a21\u578b\u5e76\u4f18\u5316\u5e38\u6570\u56e0\u5b50\u3002"}}
{"id": "2510.26690", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26690", "abs": "https://arxiv.org/abs/2510.26690", "authors": ["Amir Reza Mirzaei", "Yuqiao Wen", "Yanshuai Cao", "Lili Mou"], "title": "LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9 LoRA \u7684\u6df7\u5408\u7cbe\u5ea6\u540e\u91cf\u5316\u65b9\u6cd5 LoRAQuant\uff0c\u901a\u8fc7\u5bf9 LoRA \u8fdb\u884c SVD \u91cd\u53c2\u6570\u5316\uff0c\u5c06\u5173\u952e\u4fe1\u606f\u96c6\u4e2d\u5230\u7279\u5b9a\u884c\u5217\uff0c\u4ece\u800c\u5728\u91cd\u8981\u90e8\u5206\u4f7f\u7528\u9ad8\u7cbe\u5ea6\u3001\u5176\u4ed6\u90e8\u5206\u6781\u4f4e\u6bd4\u7279\u5bbd\u5ea6\uff0c\u5b9e\u73b0\u591a\u6a21\u578b\u591a\u4efb\u52a1\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u8f83\u4f4e\u6bd4\u7279\u4f4d\u4e0b\u6027\u80fd\u4e0e\u5176\u4ed6\u91cf\u5316\u65b9\u6cd5\u76f8\u6bd4\u4ecd\u5177\u7ade\u4e89\u529b\u3002", "motivation": "\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u5f80\u5f80\u9700\u8981\u540c\u65f6\u52a0\u8f7d\u591a\u4e2a LoRA \u9002\u914d\u5668\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u548c\u4efb\u52a1\u591a\u6837\u6027\uff1b\u5c3d\u7ba1\u6bcf\u4e2a\u9002\u914d\u5668\u5355\u72ec\u6210\u672c\u8f83\u4f4e\uff0c\u4f46\u805a\u5408\u6210\u672c\u5728\u89c4\u6a21\u5316\u65f6\u76f8\u5f53\u663e\u8457\u3002\u9700\u8981\u4e00\u79cd\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u4e0e\u901a\u4fe1\u5f00\u9500\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5c24\u5176\u9488\u5bf9\u5305\u542b\u591a\u9002\u914d\u5668\u7684\u573a\u666f\u3002", "method": "\u63d0\u51fa LoRAQuant\uff0c\u5bf9\u6bcf\u4e2a LoRA \u9002\u914d\u5668\u8fdb\u884c\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u91cd\u53c2\u6570\u5316\uff0c\u5c06\u4fe1\u606f\u96c6\u4e2d\u5230\u7279\u5b9a\u7684\u884c\u4e0e\u5217\uff1b\u5bf9\u91cd\u8981\u5206\u91cf\u4f7f\u7528\u8f83\u9ad8\u7684\u7cbe\u5ea6\u91cf\u5316\uff0c\u800c\u5bf9\u5176\u4f59\u5206\u91cf\u4f7f\u7528\u8d85\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u7684\u91cf\u5316\uff0c\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u7684\u540e\u91cf\u5316\u7b56\u7565\u3002", "result": "\u5728 LLaMA 2-7B\u3001LLaMA 2-13B \u4e0e Mistral 7B \u4e09\u4e2a\u6a21\u578b\u4e0a\uff0c\u8986\u76d6\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7801\u548c\u6458\u8981\u7b49\u4efb\u52a1\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLoRAQuant \u4f7f\u7528\u7684\u6bd4\u7279\u6570\u663e\u8457\u4f4e\u4e8e\u5176\u4ed6\u91cf\u5316\u65b9\u6cd5\uff0c\u4f46\u5728\u6027\u80fd\u4e0a\u53ef\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "LoRAQuant \u80fd\u5728\u591a\u9002\u914d\u5668\u3001\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u6216\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u4e0e\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u5927\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u573a\u666f\u3002"}}
{"id": "2510.26706", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26706", "abs": "https://arxiv.org/abs/2510.26706", "authors": ["Giulia DeSalvo", "Clara Mohri", "Mehryar Mohri", "Yutao Zhong"], "title": "Budgeted Multiple-Expert Deferral", "comment": null, "summary": "Learning to defer uncertain predictions to costly experts offers a powerful\nstrategy for improving the accuracy and efficiency of machine learning systems.\nHowever, standard training procedures for deferral algorithms typically require\nquerying all experts for every training instance, an approach that becomes\nprohibitively expensive when expert queries incur significant computational or\nresource costs. This undermines the core goal of deferral: to limit unnecessary\nexpert usage. To overcome this challenge, we introduce the budgeted deferral\nframework, which aims to train effective deferral algorithms while minimizing\nexpert query costs during training. We propose new algorithms for both\ntwo-stage and single-stage multiple-expert deferral settings that selectively\nquery only a subset of experts per training example. While inspired by active\nlearning, our setting is fundamentally different: labels are already known, and\nthe core challenge is to decide which experts to query in order to balance cost\nand predictive performance. We establish theoretical guarantees for both of our\nalgorithms, including generalization bounds and label complexity analyses.\nEmpirical results across several domains show that our algorithms substantially\nreduce training costs without sacrificing prediction accuracy, demonstrating\nthe practical value of our budget-aware deferral algorithms.", "AI": {"tldr": "\u63d0\u51fa\u9884\u7b97\u5316\u7684 deferral \u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u6709\u9009\u62e9\u5730\u67e5\u8be2\u5c11\u91cf\u4e13\u5bb6\u6765\u964d\u4f4e\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\uff0c\u7ed9\u51fa\u4e24\u9636\u6bb5\u548c\u5355\u9636\u6bb5\u591a\u4e13\u5bb6\u8bbe\u7f6e\u7684\u7b97\u6cd5\u53ca\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u591a\u9886\u57df\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u6210\u672c\u663e\u8457\u4e0b\u964d\u800c\u51c6\u786e\u5ea6\u4e0d\u964d\u4f4e\u3002", "motivation": "\u5728\u9700\u8981\u6602\u8d35\u4e13\u5bb6\u53c2\u4e0e\u7684\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u7684 deferral \u5b66\u4e60\u901a\u5e38\u5bf9\u6bcf\u4e2a\u6837\u672c\u90fd\u67e5\u8be2\u6240\u6709\u4e13\u5bb6\uff0c\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u9884\u7b97\u7ea6\u675f\u5728\u8bad\u7ec3\u4e2d\u6709\u9009\u62e9\u5730\u67e5\u8be2\u4e13\u5bb6\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u6210\u672c\u6548\u7387\u4e0e\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u548c\u5355\u9636\u6bb5\u591a\u4e13\u5bb6 deferral \u7684\u65b0\u7b97\u6cd5\uff0c\u7b56\u7565\u6027\u5730\u5bf9\u8bad\u7ec3\u6837\u672c\u4ec5\u67e5\u8be2\u5b50\u96c6\u4e13\u5bb6\uff1b\u7ed9\u51fa\u6cdb\u5316\u754c\u548c\u6807\u7b7e\u590d\u6742\u5ea6\u5206\u6790\uff1b\u5728\u591a\u9886\u57df\u6570\u636e\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u6210\u672c\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u8bad\u7ec3\u6210\u672c\u663e\u8457\u4e0b\u964d\u7684\u540c\u65f6\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u4fdd\u6301\u63a5\u8fd1\u6216\u672a\u964d\u4f4e\uff0c\u76f8\u6bd4\u5168\u67e5\u8be2\u8bbe\u7f6e\u5177\u6709\u660e\u663e\u7684\u6210\u672c\u6548\u76ca\u3002", "conclusion": "\u9884\u7b97\u611f\u77e5\u7684 deferral \u7b97\u6cd5\u5728\u9700\u8981\u6602\u8d35\u4e13\u5bb6\u53c2\u4e0e\u7684\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002"}}
{"id": "2510.26709", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.26709", "abs": "https://arxiv.org/abs/2510.26709", "authors": ["Chuyan Chen", "Chenyang Ma", "Zhangxin Li", "Yutong He", "Yanjie Dong", "Kun Yuan"], "title": "An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning", "comment": "8 pages, 2 figures", "summary": "Communication remains a central bottleneck in large-scale distributed machine\nlearning, and gradient sparsification has emerged as a promising strategy to\nalleviate this challenge. However, existing gradient compressors face notable\nlimitations: Rand-$K$\\ discards structural information and performs poorly in\npractice, while Top-$K$\\ preserves informative entries but loses the\ncontraction property and requires costly All-Gather operations. In this paper,\nwe propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that\naligns sparsity patterns across nodes using a lightweight sketch of the\ngradient, enabling index-free All-Reduce while preserving globally significant\ninformation. ARC-Top-$K$\\ is provably contractive and, when combined with\nmomentum error feedback (EF21M), achieves linear speedup and sharper\nconvergence rates than the original EF21M under standard assumptions.\nEmpirically, ARC-Top-$K$\\ matches the accuracy of Top-$K$\\ while reducing\nwall-clock training time by up to 60.7\\%, offering an efficient and scalable\nsolution that combines the robustness of Rand-$K$\\ with the strong performance\nof Top-$K$.", "AI": {"tldr": "ARC-Top-K proposed as an All-Reduce-compatible Top-K gradient compressor that aligns sparsity across nodes via a lightweight gradient sketch, enabling index-free communication while preserving important information. It is contractive and, with momentum EF21M, achieves linear speedup and improved convergence; empirically matches Top-K accuracy with significant wall-clock time reduction (~60%).", "motivation": "Reduce communication bottlenecks in large-scale distributed ML without sacrificing convergence or requiring expensive All-Gather; existing Rand-K loses structure, Top-K loses contraction and needs costly operations; need an All-Reduce-friendly, contractive, efficient compressor.", "method": "ARC-Top-K uses a lightweight sketch to align sparsity patterns across nodes, enabling index-free All-Reduce. It preserves globally significant information via Top-K selection under a coordinated sparsity scheme. It is contractive and combines with momentum error feedback EF21M to boost convergence.", "result": "The compressor is provably contractive. When combined with EF21M, it attains linear speedup and sharper convergence rates than the original EF21M under standard assumptions. Empirically, ARC-Top-K matches Top-K accuracy while reducing wall-clock training time by up to 60.7%.", "conclusion": "ARC-Top-K offers a robust, scalable gradient compressor that blends Rand-K robustness with Top-K performance, mitigating communication bottlenecks in distributed training while preserving convergence and accuracy."}}
{"id": "2510.26714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26714", "abs": "https://arxiv.org/abs/2510.26714", "authors": ["Jamie Lanyon", "Axel Finke", "Petros Andreou", "Georgina Cosma"], "title": "On the limitation of evaluating machine unlearning using only a single training seed", "comment": "mini paper, 2 figures", "summary": "Machine unlearning (MU) aims to remove the influence of certain data points\nfrom a trained model without costly retraining. Most practical MU algorithms\nare only approximate and their performance can only be assessed empirically.\nCare must therefore be taken to make empirical comparisons as representative as\npossible. A common practice is to run the MU algorithm multiple times\nindependently starting from the same trained model. In this work, we\ndemonstrate that this practice can give highly non-representative results\nbecause -- even for the same architecture and same dataset -- some MU methods\ncan be highly sensitive to the choice of random number seed used for model\ntraining. We therefore recommend that empirical\ncomphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should\nalso reflect the variability across different model training seeds.", "AI": {"tldr": "MU \u65b9\u6cd5\u5bf9\u968f\u673a\u79cd\u5b50\u654f\u611f\uff0c\u5355\u4e00\u79cd\u5b50\u8bc4\u4f30\u53ef\u80fd\u4e0d\u5177\u4ee3\u8868\u6027\uff0c\u6bd4\u8f83\u5e94\u5305\u542b\u591a\u79cd\u79cd\u5b50\u5e26\u6765\u7684\u53d8\u5f02\u6027\u3002", "motivation": "\u786e\u4fdd MU \u7b97\u6cd5\u8bc4\u4f30\u5177\u6709\u53ef\u91cd\u590d\u6027\u548c\u516c\u5e73\u6027\uff0c\u5355\u79cd\u5b50\u8bc4\u4f30\u53ef\u80fd\u63a9\u76d6\u65b9\u6cd5\u771f\u5b9e\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u5bf9\u591a\u79cd\u8bad\u7ec3\u79cd\u5b50\u4e0b\u7684 MU \u7b97\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u5206\u6790\uff0c\u89c2\u5bdf\u7ed3\u679c\u5bf9\u79cd\u5b50\u654f\u611f\u7a0b\u5ea6\uff0c\u63d0\u51fa\u5c06\u79cd\u5b50\u53d8\u5f02\u6027\u7eb3\u5165\u57fa\u51c6\u8bc4\u4f30\u7684\u5efa\u8bae\u3002", "result": "\u8bc1\u5b9e\u67d0\u4e9b MU \u65b9\u6cd5\u5bf9\u968f\u673a\u79cd\u5b50\u9ad8\u5ea6\u654f\u611f\uff0c\u5355\u6b21\u91cd\u590d\u7684\u8bc4\u4f30\u53ef\u80fd\u4ea7\u751f\u504f\u5dee\uff0c\u9700\u5728\u5bf9\u6bd4\u4e2d\u62a5\u544a\u79cd\u5b50\u95f4\u53d8\u5f02\u3002", "conclusion": "\u5728 MU \u8bc4\u4f30\u4e2d\u5e94\u5305\u542b\u591a\u79cd\u8bad\u7ec3\u79cd\u5b50\u5e76\u62a5\u544a\u7edf\u8ba1\u91cf\uff0c\u63d0\u5347\u53ef\u6bd4\u6027\u548c\u518d\u73b0\u6027\u3002"}}
{"id": "2510.26717", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.26717", "abs": "https://arxiv.org/abs/2510.26717", "authors": ["Tommaso d'Orsi", "Gleb Novikov"], "title": "On Purely Private Covariance Estimation", "comment": "equal contribution", "summary": "We present a simple perturbation mechanism for the release of $d$-dimensional\ncovariance matrices $\\Sigma$ under pure differential privacy. For large\ndatasets with at least $n\\geq d^2/\\varepsilon$ elements, our mechanism recovers\nthe provably optimal Frobenius norm error guarantees of\n\\cite{nikolov2023private}, while simultaneously achieving best known error for\nall other $p$-Schatten norms, with $p\\in [1,\\infty]$. Our error is\ninformation-theoretically optimal for all $p\\ge 2$, in particular, our\nmechanism is the first purely private covariance estimator that achieves\noptimal error in spectral norm.\n  For small datasets $n< d^2/\\varepsilon$, we further show that by projecting\nthe output onto the nuclear norm ball of appropriate radius, our algorithm\nachieves the optimal Frobenius norm error $O(\\sqrt{d\\;\\text{Tr}(\\Sigma) /n})$,\nimproving over the known bounds of $O(\\sqrt{d/n})$ of \\cite{nikolov2023private}\nand ${O}\\big(d^{3/4}\\sqrt{\\text{Tr}(\\Sigma)/n}\\big)$ of\n\\cite{dong2022differentially}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u6270\u52a8\u673a\u5236\uff0c\u5728\u7eaf\u5dee\u5206\u9690\u79c1\u4e0b\u53d1\u5e03d\u7ef4\u534f\u65b9\u5dee\u77e9\u9635\u03a3\uff1b\u5728\u5927\u6570\u636e\u573a\u666f(n\u2265d^2/\u03b5)\u8fbe\u5230Frobenius\u8303\u6570\u8bef\u5dee\u7684\u4fe1\u606f\u8bba\u6700\u4f18\uff0c\u5e76\u5bf9\u6240\u6709p-Schatten\u8303\u6570(p\u22651)\u8fbe\u5230\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\uff1b\u5728n<d^2/\u03b5\u65f6\uff0c\u901a\u8fc7\u6295\u5f71\u5230\u5408\u9002\u534a\u5f84\u7684\u6838\u8303\u6570\u7403\u5b9e\u73b0Frobenius\u8bef\u5deeO(\u221a(d Tr(\u03a3)/n))\uff0c\u4f18\u4e8e\u73b0\u6709\u754c\u3002", "motivation": "\u5728\u4fdd\u969c\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u53d1\u5e03\u534f\u65b9\u5dee\u77e9\u9635\u7684\u516c\u5ea6\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u673a\u5236\u9700\u8981\u5728\u4e0d\u540c\u6570\u636e\u96c6\u89c4\u6a21\u4e0b\u63d0\u4f9b\u5f3a\u4e14\u5c3d\u53ef\u80fd\u7edf\u4e00\u7684\u8bef\u5dee\u754c\uff0c\u5f25\u5408\u5927\u6570\u636e\u4e0e\u5c0f\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u5dee\u8ddd\uff1b\u73b0\u6709\u5de5\u4f5c\u5728\u67d0\u4e9b\u8303\u6570\u4e0a\u6709\u754c\uff0c\u4f46\u5728\u8c31\u8303\u6570\u7b49\u66f4\u5f3a\u8303\u6570\u4e0a\u7684\u6700\u4f18\u6027\u5c1a\u5f85\u89e3\u51b3\uff0c\u4e14\u5c0f\u6570\u636e\u60c5\u5f62\u7684\u8bef\u5dee\u754c\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u7684\u6270\u52a8\u673a\u5236\u7528\u4e8e\u516c\u5f00\u03a3\u7684\u7eafDP\u4e0b\u7684\u53d1\u5e03\uff0c\u5e76\u5728\u5927\u6570\u636e\u6761\u4ef6\u4e0b\u5b9e\u73b0Frobenius\u548c\u6240\u6709p-Schatten\u8303\u6570\u7684\u8bef\u5dee\u754c\u3002\u5bf9\u4e8en<d^2/\u03b5\uff0c\u6539\u7528\u5bf9\u8f93\u51fa\u8fdb\u884c\u6838\u8303\u6570\u7403\u6295\u5f71\u7684\u540e\u5904\u7406\uff0c\u4ee5\u83b7\u5f97Frobenius\u8bef\u5dee\u7684\u6700\u4f18\u9636O(\u221a(d Tr(\u03a3)/n))\u3002", "result": "\u5f53n\u2265d^2/\u03b5\u65f6\uff0c\u83b7\u5f97\u4e0eNikolo(v) 2023\u76f8\u540c\u7684Frobenius\u8bef\u5dee\u4e0b\u754c\u7684\u6e10\u8fdb\u4e0a\u754c\uff0c\u5e76\u5728\u6240\u6709p\u22651\u7684Schatten\u8303\u6570\u4e0b\u8fbe\u5230\u6700\u4f73\u5df2\u77e5\u8bef\u5dee\uff1b\u5bf9n<d^2/\u03b5\u65f6\uff0c\u901a\u8fc7\u6838\u8303\u6570\u7403\u6295\u5f71\u5b9e\u73b0Frobenius\u8bef\u5deeO(\u221a(d Tr(\u03a3)/n))\uff0c\u663e\u8457\u4f18\u4e8eNikolo(v) 2023\u7684O(\u221a(d/n))\u548cDong 2022\u7684O(d^{3/4}\u221a(Tr(\u03a3)/n))\u7684\u754c\u3002", "conclusion": "\u8be5\u673a\u5236\u5728\u6570\u636e\u89c4\u6a21\u65e0\u8bba\u5927\u8fd8\u662f\u5c0f\u4e0b\u5747\u63d0\u4f9b\u63a5\u8fd1\u6700\u4f18\u7684\u8bef\u5dee\u754c\uff0c\u4e14\u5728\u8c31\u8303\u6570\u4e0a\u8fbe\u5230\u4fe1\u606f\u8bba\u6700\u4f18\uff0c\u4e14\u5f15\u5165\u4e86\u5bf9\u5c0f\u6570\u636e\u96c6\u7684\u4e00\u81f4\u540e\u5904\u7406\u6b65\u9aa4\u4ee5\u83b7\u5f97\u6700\u4f73Frobenius\u8bef\u5dee\u3002"}}
{"id": "2510.26745", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26745", "abs": "https://arxiv.org/abs/2510.26745", "authors": ["Shahriar Noroozizadeh", "Vaishnavh Nagarajan", "Elan Rosenfeld", "Sanjiv Kumar"], "title": "Deep sequence models tend to memorize geometrically; it is unclear why", "comment": null, "summary": "In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.", "AI": {"tldr": "Transformer memory is better explained by a learned global geometry of atomic facts, not merely local co-occurrences; this geometry, shaped by spectral bias, simplifies reasoning and suggests ways to make memory more geometric.", "motivation": "To challenge the view of parametric memory as brute-force co-occurrence storage, and to uncover the geometric structure of embeddings, its origins, and implications for knowledge acquisition and model capacity.", "method": "Isolate a clean Transformer reasoning instance incompatible with local co-occurrence memory; analyze geometric encoding of global relations among entities (including non-co-occurring ones); connect to Node2Vec and spectral bias; discuss implications.", "result": "Found that a global embedding geometry emerges even when not more succinct than simple lookups; this geometry enables reduced reasoning complexity and is not readily explained by standard architectural or optimization pressures; spectral bias underpins this geometry.", "conclusion": "A geometric view of parametric memory provides a new lens for understanding knowledge representation, with practical headroom to enhance Transformer memory geometry; encourages revisiting assumptions about knowledge acquisition, capacity, discovery, and unlearning."}}
{"id": "2510.26771", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26771", "abs": "https://arxiv.org/abs/2510.26771", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "title": "STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization", "comment": "10 pages main text, 8 pages supplementary material", "summary": "Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations.", "AI": {"tldr": "STaMP \u901a\u8fc7\u5728\u5e8f\u5217\u7ef4\u5ea6\u6267\u884c\u7ebf\u6027\u53d8\u6362\u5e76\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u6765\u63d0\u5347\u4f4e\u4f4d\u5bbd\u6fc0\u6d3b\u91cf\u5316\u7684\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u8bed\u8a00/\u89c6\u89c9\u6a21\u578b\u3002", "motivation": "\u5728\u6fc0\u6d3b\u4f4e\u4e8e8\u4f4d\u65f6\u91cf\u5316\u7cbe\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\u9700\u8981\u65b0\u7b56\u7565\uff1b\u53ef\u9006\u7ebf\u6027\u53d8\u6362\uff08\u5982\u65cb\u8f6c\uff09\u6709\u52a9\u4e8e\u91cd\u65b0\u53c2\u6570\u5316\u901a\u9053\u548c\u6743\u91cd\u4ee5\u6539\u5584\u91cf\u5316\u3002", "method": "\u6cbf\u5e8f\u5217\u7ef4\u5ea6\u5e94\u7528\u53ef\u9006\u7684\u7ebf\u6027\u53d8\u6362\uff0c\u5e76\u5f15\u5165\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565\uff1a\u5728\u4e2d\u95f4\u6fc0\u6d3b\u4e2d\u4ec5\u4fdd\u7559\u5c11\u91cf token \u4ee5\u9ad8\u7cbe\u5ea6\uff0c\u5176\u4f59\u90e8\u5206\u4f7f\u7528\u66f4\u4f4e\u4f4d\u5bbd\uff0c\u4ece\u800c\u5728\u603b\u4f53\u6fc0\u6d3b\u4f4d\u5bbd\u8f83\u4f4e\u65f6\u4fdd\u6301\u4fe1\u606f\u3002", "result": "\u5728\u6700\u8fd1\u7684 LVM \u4e0e LLM \u67b6\u6784\u4e0a\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u4f4d\u5bbd\u6fc0\u6d3b\u91cf\u5316\u7684\u7cbe\u5ea6\uff0c\u4e14\u4e0e\u73b0\u6709\u6fc0\u6d3b/\u6743\u91cd\u91cf\u5316\u65b9\u6cd5\u53ca\u6700\u8fd1\u7684\u7279\u5f81\u53d8\u6362\u4e92\u8865\u3002", "conclusion": "STaMP \u63d0\u4f9b\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u7528\u4e8e\u63d0\u5347\u4f4e\u6bd4\u7279\u6fc0\u6d3b\u91cf\u5316\u7684\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u8bed\u8a00\u4e0e\u89c6\u89c9\u6a21\u578b\uff0c\u5e76\u53ef\u4e0e\u5176\u4ed6\u91cf\u5316\u65b9\u6cd5\u534f\u540c\u5de5\u4f5c\u3002"}}
{"id": "2510.26776", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26776", "abs": "https://arxiv.org/abs/2510.26776", "authors": ["Jungyeon Koh", "Hyeonsu Lyu", "Jonggyu Jang", "Hyun Jong Yang"], "title": "Faithful and Fast Influence Function via Advanced Sampling", "comment": null, "summary": "How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u548c logits \u7684\u4e24\u79cd\u62bd\u6837\u65b9\u6cd5\u4ee5\u9ad8\u6548\u4f30\u8ba1\u5f71\u54cd\u51fd\u6570\uff08IF\uff09\uff0c\u901a\u8fc7\u5c0f\u5b50\u96c6\u903c\u8fd1\u5168\u6570\u636e\u7684 Hessian \u8ba1\u7b97\uff0c\u4ece\u800c\u63d0\u5347 IF \u7684\u7a33\u5b9a\u6027\u4e0e\u7cbe\u5ea6\u3002", "motivation": "\u76f4\u63a5\u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u6c42\u53d6 Hessian \u4ee3\u4ef7\u9ad8\u6602\u4e14\u5bf9\u968f\u673a\u5b50\u96c6\u7684\u9ad8\u65b9\u5dee\u5bfc\u81f4 IF \u4f30\u8ba1\u4e0d\u7a33\u5b9a\uff0c\u56e0\u6b64\u9700\u63d0\u51fa\u66f4\u5177\u4ee3\u8868\u6027\u7684\u5b50\u96c6\u91c7\u6837\u7b56\u7565\u6765\u63d0\u5347 IF \u7684\u51c6\u786e\u6027\u4e0e\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u7279\u5f81\u5206\u5e03\u548c logits \u5206\u5e03\u7684\u9ad8\u7ea7\u91c7\u6837\u6280\u672f\uff0c\u4ece\u7279\u5f81\u6216 logits \u7684\u968f\u673a\u5206\u5e03\u4e2d\u9009\u53d6\u4e00\u4e2a\u5c0f\u578b\u4f46\u5177\u6709\u4ee3\u8868\u6027\u7684\u8bad\u7ec3\u5b50\u96c6\uff0c\u7528\u4ee5\u8fd1\u4f3c\u6574\u4e2a\u6570\u636e\u96c6\u7684\u5f71\u54cd\u51fd\u6570\u4f30\u8ba1\u8fc7\u7a0b\u3002", "result": "\u5728\u7c7b\u79fb\u9664\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u5e73\u5747\u7528 F1-score \u8861\u91cf\u6a21\u578b\u5fd8\u8bb0\u88ab\u79fb\u9664\u7c7b\u522b\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5176\u4ed6\u7c7b\u522b\u7684\u4e00\u81f4\u6027\u3002\u76f8\u6bd4\u57fa\u7ebf\uff0c\u65b9\u6cd5\u5728\u65f6\u95f4\u4e0a\u51cf\u5c11 30.1% \u7684\u8ba1\u7b97\u91cf\u3001\u5185\u5b58\u4f7f\u7528\u51cf\u5c11 42.2%\uff0c\u5728 F1-score \u4e0a\u63d0\u5347 2.5%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u7279\u5f81\u4e0e logits \u7684\u5b50\u96c6\u91c7\u6837\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u9ed1\u7bb1\u6a21\u578b\u89e3\u91ca\u7684\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\uff0c\u4e3a\u9ad8\u6548\u7684\u5f71\u54cd\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.26777", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26777", "abs": "https://arxiv.org/abs/2510.26777", "authors": ["Andreas Auer", "Daniel Klotz", "Sebastinan B\u00f6ck", "Sepp Hochreiter"], "title": "Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification", "comment": "NeurIPS 2025 Workshop on Recent Advances in Time Series Foundation\n  Models (BERT2S)", "summary": "Recent research on time series foundation models has primarily focused on\nforecasting, leaving it unclear how generalizable their learned representations\nare. In this study, we examine whether frozen pre-trained forecasting models\ncan provide effective representations for classification. To this end, we\ncompare different representation extraction strategies and introduce two\nmodel-agnostic embedding augmentations. Our experiments show that the best\nforecasting models achieve classification accuracy that matches or even\nsurpasses that of state-of-the-art models pre-trained specifically for\nclassification. Moreover, we observe a positive correlation between forecasting\nand classification performance. These findings challenge the assumption that\ntask-specific pre-training is necessary, and suggest that learning to forecast\nmay provide a powerful route toward constructing general-purpose time series\nfoundation models.", "AI": {"tldr": "\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u65f6\u5e8f forecasting \u6a21\u578b\u53ef\u7528\u4f5c\u5206\u7c7b\u4efb\u52a1\u7684\u901a\u7528\u8868\u793a\uff0c\u5176\u5206\u7c7b\u6027\u80fd\u53ef\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u4e13\u95e8\u9488\u5bf9\u5206\u7c7b\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff1b\u5bf9\u4e0d\u540c\u8868\u793a\u63d0\u53d6\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u5d4c\u5165\u589e\u5f3a\u65b9\u6cd5\uff1b\u7ed3\u679c\u663e\u793a\u4e24\u8005\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u6027\uff0c\u8868\u660e forecast \u5b66\u4e60\u53ef\u6784\u5efa\u901a\u7528\u7684\u65f6\u5e8f\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u63a2\u7a76\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5b66\u4e60\u7684\u8868\u793a\u662f\u5426\u80fd\u6cdb\u5316\u5230\u5206\u7c7b\u4efb\u52a1\uff0c\u4ece\u800c\u6311\u6218\u201c\u4efb\u52a1\u7279\u5b9a\u9884\u8bad\u7ec3\u201d\u5fc5\u8981\u6027\u7684\u5047\u8bbe\u3002", "method": "\u5728\u51bb\u7ed3\u7684\u9884\u6d4b\u6a21\u578b\u4e0a\u6bd4\u8f83\u591a\u79cd\u8868\u793a\u63d0\u53d6\u7b56\u7565\uff0c\u63d0\u51fa\u4e24\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5d4c\u5165\u589e\u5f3a\uff1b\u901a\u8fc7\u5206\u7c7b\u4efb\u52a1\u8bc4\u4f30\u8868\u793a\u8d28\u91cf\uff0c\u5e76\u5206\u6790\u9884\u6d4b\u80fd\u529b\u4e0e\u5206\u7c7b\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002", "result": "\u6700\u4f73\u9884\u6d4b\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u8fbe\u5230\u5e76\u53ef\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u4e3a\u5206\u7c7b\u4e13\u95e8\u9884\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff1b\u5e76\u89c2\u5bdf\u5230\u9884\u6d4b\u80fd\u529b\u4e0e\u5206\u7c7b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u6027\u3002", "conclusion": "\u5b66\u4e60\u9884\u6d4b\u53ef\u80fd\u63d0\u4f9b\u5f3a\u5927\u7684\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u8def\u5f84\uff0c\u4efb\u52a1\u7279\u5b9a\u7684\u9884\u8bad\u7ec3\u4e0d\u4e00\u5b9a\u5fc5\u8981\uff0c\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u7684 forecast \u5b66\u4e60\u6210\u4e3a\u6784\u5efa\u901a\u7528\u8868\u793a\u7684\u6709\u529b\u9014\u5f84\u3002"}}
{"id": "2510.26782", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26782", "abs": "https://arxiv.org/abs/2510.26782", "authors": ["Zaishuo Xia", "Yukuan Lu", "Xinyi Li", "Yifan Xu", "Yubei Chen"], "title": "Clone Deterministic 3D Worlds with Geometrically-Regularized World Models", "comment": null, "summary": "A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.", "AI": {"tldr": "\u901a\u8fc7\u51e0\u4f55\u6b63\u5219\u5316\u7684\u6f5c\u5728\u8868\u793a\uff0cGRWM\u663e\u8457\u63d0\u5347\u4e16\u754c\u6a21\u578b\u5728\u786e\u5b9a\u60273D\u73af\u5883\u4e2d\u7684\u957f\u65f6\u7a0b\u9884\u6d4b\u7a33\u5b9a\u6027\u548c\u4fdd\u771f\u5ea6\uff0c\u8868\u660e\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u76f4\u63a5\u63d0\u5347\u4e16\u754c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u5bf9exteroceptive\u9ad8\u7ef4\u8f93\u5165\u6613\u53d1\u751f\u4fe1\u606f\u635f\u5931\u548c\u8026\u5408\uff0c\u5bfc\u81f4\u5728\u957f\u65f6\u7a0b\u4e2d\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u901a\u8fc7\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u6765\u83b7\u5f97\u66f4\u7b26\u5408\u73af\u5883\u62d3\u6251\u7684\u6f5c\u5728\u8868\u5f81\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u7a33\u5b9a\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faGeometrically-Regularized World Models (GRWM)\uff0c\u5f3a\u5236\u611f\u77e5\u5e8f\u5217\u4e2d\u76f8\u90bb\u65f6\u95f4\u6b65\u5728\u6f5c\u5728\u7a7a\u95f4\u4fdd\u6301\u63a5\u8fd1\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u4f73\u7684\u6f5c\u5728\u6d41\u5f62\u7ed3\u6784\u3002GRWM\u53ef\u63d2\u62d4\u3001\u53ea\u9700\u5bf9\u73b0\u6709\u9aa8\u67b6\u505a\u6700\u5c0f\u6539\u52a8\uff0c\u80fd\u591f\u968f\u8f68\u8ff9\u957f\u5ea6\u7ebf\u6027\u6269\u5c55\uff0c\u517c\u5bb9\u591a\u79cd\u6f5c\u5728\u751f\u6210\u9aa8\u5e72\u3002", "result": "\u5728\u786e\u5b9a\u60273D\u8bbe\u7f6e\u548c\u957f\u65f6\u7a0b\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cGRWM\u663e\u8457\u63d0\u9ad8\u4e86 rollout \u7684\u4fdd\u771f\u5ea6\u4e0e\u7a33\u5b9a\u6027\uff0c\u5e76\u4e14\u5206\u6790\u8868\u660e\u6536\u76ca\u6765\u81ea\u5b66\u4e60\u5230\u5177\u6709\u66f4\u4f18\u51e0\u4f55\u7ed3\u6784\u7684\u6f5c\u5728\u6d41\u5f62\uff0c\u4f7f\u6f5c\u5728\u8868\u793a\u66f4\u63a5\u8fd1\u73af\u5883\u7684\u771f\u5b9e\u62d3\u6251\u3002", "conclusion": "\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u662f\u83b7\u5f97\u9c81\u68d2\u4e16\u754c\u6a21\u578b\u7684\u76f4\u63a5\u6709\u6548\u8def\u5f84\uff0c\u53ef\u5728\u4e0d\u6269\u5c55\u52a8\u529b\u5b66\u6a21\u5757\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u957f\u65f6\u7a0b\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.26788", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26788", "abs": "https://arxiv.org/abs/2510.26788", "authors": ["Penghui Qi", "Zichen Liu", "Xiangxin Zhou", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Defeating the Training-Inference Mismatch via FP16", "comment": null, "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.", "AI": {"tldr": "\u901a\u8fc7\u5c06 BF16 \u56de\u9000\u5230 FP16\uff0c\u89e3\u51b3 RL \u5fae\u8c03\u4e2d\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\u6570\u503c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u7a33\u5b9a\u3001\u6536\u655b\u66f4\u5feb\u4e14\u6027\u80fd\u66f4\u5f3a\u7684\u7ed3\u679c\u3002", "motivation": "RLHF \u7b49 RL \u5fae\u8c03\u5728\u8bad\u7ec3\u4e0e\u63a8\u7406\u9636\u6bb5\u7684\u6570\u503c\u4e0d\u4e00\u81f4\u6027\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\uff1b\u5c3d\u7ba1\u5df2\u6709\u7b97\u6cd5\u6216\u5de5\u7a0b\u5c42\u9762\u7684\u4fee\u6b63\uff0c\u6839\u672c\u539f\u56e0\u5728\u4e8e\u6d6e\u70b9\u7cbe\u5ea6\u3002BF16 \u867d\u62e5\u6709\u8f83\u5927\u52a8\u6001\u8303\u56f4\uff0c\u4f46\u5f15\u5165\u8f83\u5927\u820d\u5165\u8bef\u5dee\uff0c\u7834\u574f\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u4e00\u81f4\u6027\u3002", "method": "\u5c06\u6570\u503c\u8868\u793a\u4ece BF16 \u6539\u56de FP16\uff0c\u6846\u67b6\u5747\u652f\u6301\u4e14\u53ea\u9700\u51e0\u884c\u4ee3\u7801\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u6216\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4efb\u52a1\u3001\u7b97\u6cd5\u3001\u6846\u67b6\u4e0b\uff0cFP16 \u80fd\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u4f18\u5316\u3001\u52a0\u901f\u6536\u655b\u3001\u5e76\u5e26\u6765\u66f4\u5f3a\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u793a\u5728 RL \u5fae\u8c03\u4e2d\u5e94\u91cd\u65b0\u8003\u91cf\u7cbe\u5ea6\u53d6\u820d\uff0cFP16 \u7684\u4f7f\u7528\u5e94\u66f4\u52a0\u5e7f\u6cdb\uff0c\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\u4e0e\u6548\u7387\u3002"}}
