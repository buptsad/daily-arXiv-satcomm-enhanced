<div id=toc></div>

# Table of Contents

- [eess.SY](#eess.SY) [Total: 8]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.IT](#cs.IT) [Total: 3]
- [eess.SP](#eess.SP) [Total: 7]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.CR](#cs.CR) [Total: 15]


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [1] [A Class of Axis-Angle Attitude Control Laws for Rotational Systems](https://arxiv.org/abs/2512.19846)
*Francisco M. F. R. Gonçalves,Ryan M. Bena,Néstor O. Pérez-Arancibia*

Main category: eess.SY

TL;DR: 提出一种基于轴角广义表示的姿态控制律，取代传统四元数为核心的控制，结合扩展K∞函数与Lyapunov稳定性分析，确保闭环系统的全局渐近稳定。


<details>
  <summary>Details</summary>
Motivation: 解决四元数基控制在切换策略中的局限性，利用轴角广义表示提升控制设计的灵活性与性能，特别是在与基于角速度触发的切换方案组合时。

Method: 在Lyapunov稳定性框架下，利用扩展K∞函数推导出能保证闭环单个平衡点全局渐近稳定的控制律；将轴角表示引入控制律设计，并设计与角速度相关的状态依赖切换策略以提高响应性。

Result: 通过仿真和实时实验表明，在高速度翻滚恢复任务中，所提方法相较于四元数基与几何控制基准，实现更短的稳定化时间与更低的控制力。

Conclusion: 广义轴角控制框架为姿态控制提供更大设计自由度，尤其适用于需要角速度驱动切换的场景，能够在保持全局稳定性前提下提升控制性能。

Abstract: We introduce a new class of attitude control laws for rotational systems, which generalizes the use of the Euler axis-angle representation beyond quaternion-based formulations. Using basic Lyapunov's stability theory and the notion of extended $K_{\infty}$ functions, we developed a method for determining and enforcing the global asymptotic stability of the single fixed point of the resulting closed-loop (CL) scheme. In contrast with traditional quaternion-based methods, the proposed generalized axis-angle approach enables greater flexibility in the design of the control law, which is of great utility when employed in combination with a switching scheme whose transition state depends on the angular velocity of the controlled rotational system. Through simulation and real-time experimental results, we demonstrate the effectiveness of the proposed approach. According to the recorded data, in the execution of high-speed tumble-recovery maneuvers, the new method consistently achieves shorter stabilization times and requires lower control effort relative to those corresponding to the quaternion-based and geometric-control methods used as benchmarks.

</details>


### [2] [HeylandCircle: A Computational Framework for the Geometric Reconstruction of the Heyland Circle Diagram](https://arxiv.org/abs/2512.20015)
*Anubhav Gupta,Abhinav Gupta*

Main category: eess.SY

TL;DR: Introduces HeylandCircle, a computational framework to reconstruct and use the Heyland circle diagram from standard induction machine test data, turning the traditional hand-drawn construction into a reproducible algorithm to obtain PF, slip, Pout, torque, and efficiency; validated against textbook results; enables teaching, analysis, and extension.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of a standardized computational formulation for the classical Heyland circle diagram, which is typically presented as a hand-drawn aid, by providing a deterministic, reproducible workflow that maps measured data to steady-state operating points.

Method: Formalize the geometric construction as a deterministic sequence of geometric operations, establishing explicit mappings between measured no-load and blocked-rotor data, fixed geometric objects, and steady-state operating points; derive and implement geometric relationships to compute performance metrics.

Result: A computational realization named HeylandCircle that reproduces classical Heyland diagram results, with explicit operations and mappings from test data to operating points; validated on a representative textbook example.

Conclusion: The framework offers a computational, instructional, and extensible realization of the Heyland diagram, enabling consistent analysis of steady-state behavior from standard test data and providing a foundation for further extension.

Abstract: The Heyland circle diagram is a classical graphical tool for representing the steady-state behavior of induction machines using no-load and blocked-rotor test data. While widely used in alternating-current machinery texts, the diagram is typically presented as a hand-constructed aid and lacks a standardized computational formulation. This paper presents HeylandCircle, a computational framework that reconstructs the classical Heyland circle diagram directly from standard test parameters. The framework formalizes the traditional geometric construction as a deterministic, reproducible sequence of geometric operations, establishing a clear mapping between measured data, fixed geometric objects, and steady-state operating points. Quantities such as power factor, slip, output power, torque, and efficiency are obtained through explicit geometric relationships on the constructed diagram. Validation using a representative textbook example demonstrates close agreement with classical results. The framework provides a computational realization of the traditional Heyland diagram suitable for instruction, analysis, and systematic extension.

</details>


### [3] [Robust safety design for strict-feedback nonlinear systems via observer-based linear time varying feedback](https://arxiv.org/abs/2512.20226)
*Imtiaz Ur Rehman,Moussa Labbadi,Amine Abadi,Lew Lew Yan Voon*

Main category: eess.SY

TL;DR: 提出一种面向非线性严格反馈系统的鲁棒安全控制框架，利用状态变换与线性时变扰动观测器，在存在错配干扰时也能保证安全集合的前向不变量性质，并适用于无扰动情形，通过数值例证验证。


<details>
  <summary>Details</summary>
Motivation: 在存在错配干扰的非线性严格反馈系统中实现强鲁棒的安全性，确保系统状态始终落在安全集合内，提升对未知干扰的容错能力。

Method: 通过状态变换将系统转化为有利于安全控制的形式，设计线性时变扰动观测器以估计外部扰动，形成鲁棒的安全控制律，确保安全集合的前向不变量性；方法同时兼容无扰动情形。

Result: 对所有情况均证明安全性，包括存在错配干扰的场景和无扰动情形；给出数值例子验证理论结果。

Conclusion: 该方法为非线性严格反馈系统的鲁棒安全控制提供了一致性保证，具有实际应用潜力。

Abstract: This paper develops a robust safety-critical control method for nonlinear strictfeedback systems with mismatched disturbances. Using a state transformation and a linear time-varying disturbance observer, the system is converted into a form that enables safe control design. The approach ensures forward invariance of the safety set and also applies to disturbancefree systems. Safety is proven for all cases, and a numerical example illustrates the results.

</details>


### [4] [Sliding Mode Control for a Parabolic-Elliptic PDE System with Boundary Perturbation](https://arxiv.org/abs/2512.20244)
*Moussa Labbadi,Ilyasse Lamrani*

Main category: eess.SY

TL;DR: This paper studies robust control for parabolic-elliptic PDE systems with boundary control using sliding mode control to reject matched perturbations, achieving finite-time convergence to sliding manifold and exponential stability of the closed-loop system; well-posedness is established for the discontinuous closed-loop system; a numerical example validates the approach.


<details>
  <summary>Details</summary>
Motivation: The aim is to design robust boundary control for parabolic-elliptic PDE systems in the presence of matched perturbations, ensuring stability and robustness despite discontinuities.

Method: Introduce a sliding mode control strategy at the boundary to reject matched disturbances. Analyze finite-time convergence to the sliding manifold and exponential stability of the closed-loop system. Prove well-posedness of the discontinuous system. Provide numerical simulations to illustrate effectiveness.

Result: The sliding-mode controller achieves robustness against disturbances, with finite-time reaching of the sliding manifold and exponential stability of the closed-loop; well-posedness of the discontinuous dynamics is established; numerical example confirms effectiveness.

Conclusion: Sliding mode boundary control is effective for robust stabilization of parabolic-elliptic PDE systems under matched perturbations, with strong theoretical guarantees (finite-time and exponential stability) and verified numerically.

Abstract: In this paper, we address the robustness of parabolic-elliptic systems under boundary control. A sliding mode control strategy is proposed to reject matched perturbations. The stability analysis establishes finite-time convergence of the sliding manifold and exponential stability of the closed-loop system. Since the closed-loop system is discontinuous, we also prove its well-posedness. A numerical example is provided to validate the effectiveness of the proposed approach.

</details>


### [5] [Inference in Latent Force Models Using Optimal State Estimation](https://arxiv.org/abs/2512.20250)
*Tobias M. Wolff,Victor G. Lopez,Matthias A. Müller,Thomas Beckers*

Main category: eess.SY

TL;DR: 提出两种最优状态估计框架，用于同时重建隐式驱动力并估计系统状态，且可在估计中纳入系统固有约束，在数值与生物医学实例中得到验证。


<details>
  <summary>Details</summary>
Motivation: Latent force models 将物理动力学与不可观测输入（用高斯过程建模）相结合，解决在带约束的系统中对隐变量驱动和状态的准确推断问题，扩展对复杂动力系统的建模能力。

Method: 设计并实现两种最优状态估计器，以同时估计隐式力和系统状态，并允许在估计框架中引入系统自有的约束条件。

Result: 通过若干数值实验评估提出框架的性能，并在一个真实世界的生物医学案例（下丘脑-垂体-甲状腺轴）中利用激素测量数据验证其有效性。

Conclusion: 所提出的框架在同时考虑物理约束的情况下实现对隐式驱动力与状态的可估计性，具备在复杂动力系统中的实际应用潜力。

Abstract: Latent force models, a class of hybrid modeling approaches, integrate physical knowledge of system dynamics with a latent force - an unknown, unmeasurable input modeled as a Gaussian process. In this work, we introduce two optimal state estimation frameworks to reconstruct the latent forces and to estimate the states. In contrast to state-of-the-art approaches, the designed estimators enable the consideration of system-inherent constraints. Finally, the performance of the novel frameworks is investigated in several numerical examples. In particular, we demonstrate the performance of the new framework in a real-world biomedical example - the hypothalamic-pituitary-thyroid axis - using hormone measurements.

</details>


### [6] [Data-based Moving Horizon Estimation under Irregularly Measured Data](https://arxiv.org/abs/2512.20259)
*Tobias M. Wolff,Isabelle Krauss,Victor G. Lopez,Matthias A. Müller*

Main category: eess.SY

TL;DR: 基于样本与数据的移动视界估计框架，针对线性系统在稀疏、离散输出测量下实现鲁棒的状态估计，并给出隐式数据驱动表示的模型与稳定性证明，应用于胃肠道吸收系统。


<details>
  <summary>Details</summary>
Motivation: 在测量昂贵或耗时且可用测量稀缺的场景下，且不依赖标准显式模型，而以数据驱动的隐式表示进行状态估计，需提供稳定性保证。

Method: 提出一种样本为基础的移动视界估计方法，结合基于数据的隐式系统表示；在较弱的假设下证明样本基础的实用鲁棒指数稳定性；并将该框架应用于胃肠道吸收系统的状态估计。

Result: 给出样本基础的实用鲁棒指数稳定性的理论证明；展示在胃肠道吸收系统中的状态估计应用，验证方法的可行性与鲁棒性。

Conclusion: 该框架在无需标准模型且能处理稀疏测量的情形下，提供稳健的状态估计能力，并对生物系统的应用具有潜在价值。

Abstract: In this work, we introduce a sample- and data-based moving horizon estimation framework for linear systems. We perform state estimation in a sample-based fashion in the sense that we assume to have only few, irregular output measurements available. This setting is encountered in applications where measuring is expensive or time-consuming. Furthermore, the state estimation framework does not rely on a standard mathematical model, but on an implicit system representation based on measured data. We prove sample-based practical robust exponential stability of the proposed estimator under mild assumptions. Furthermore, we apply the proposed scheme to estimate the states of a gastrointestinal tract absorption system.

</details>


### [7] [Divergence Method to Stability Study of Andronov-Vyshnegradsky Problem. Hidden Oscillations](https://arxiv.org/abs/2512.20418)
*I. B. Furtat,N. V. Kuznetsov*

Main category: eess.SY

TL;DR: 通过发散法研究带 Watt 调节器的 Andronov–Vyshnegradsky 问题，得到全局稳定性边界的精确值及其三参数条件；稳定性判据可转化为线性矩阵不等式的可解性，并通过计算机建模揭示自我调节存在与否下的隐匿振荡现象。


<details>
  <summary>Details</summary>
Motivation: 揭示控制系统中隐藏的全局稳定性边界及自我调节对系统动力学的影响，提供可操作的稳定性判据并辅以数值验证。

Method: 运用发散方法分析动态系统的稳定性，对比有/无自我调节的模型，获得隐藏边界的精确值，将稳定性条件转化为线性矩阵不等式的可解性，并通过计算机仿真验证隐匿振荡的存在。

Result: 获得全局稳定区域的隐藏边界的精确值；在 Watt 调节器模型中，隐含边界由三参数决定；计算机仿真表明在有自我调节与无自我调节两种情形下均存在隐藏振荡。

Conclusion: 所提出的发散法与线性矩阵不等式框架能够精确刻画带 Watt 调节器的系统的全局稳定性边界，并给出以三参数为核心的判据，提示自我调节对稳定性与振荡的影响具有显著作用。

Abstract: The classical Andronov-Vyshnegradsky problem, which deals with locating regions of stability and oscillations in control systems with a Watt regulator, is solved using a divergence method for studying the stability of dynamic systems. This system is studied both with and without the self-regulation effect. The exact value of the hidden boundary of the global stability region is obtained. The stability criteria for a system with a Watt regulator are also presented in the context of the solvability of a linear matrix inequality. Computer modelling shows that the system exhibits hidden oscillations when the self-regulation effect is present and when it is not. The conditions for computing the hidden boundary of global stability are determined by three parameters in the Watt regulator model.

</details>


### [8] [Fast Fixed-time Convergence in Nonlinear Dynamical Systems](https://arxiv.org/abs/2512.20427)
*Igor B. Furtat*

Main category: eess.SY

TL;DR: 提出一种利用对二次函数导数的特殊约束，在解轨迹上对导数施加负幂次项，从而实现系统解在固定时间内迅速收敛，，并将其应用于任意阶线性系统的 backstepping 控制设计，辅以数值对比。


<details>
  <summary>Details</summary>
Motivation: 在控制理论中，需实现固定时间收敛以保障时序性和鲁棒性。本研究旨在通过对系统沿解的二次函数导数的特定约束，提出一种快速收敛的固定时间控制方案，提升收敛速度并适配广义线性系统的控制设计。

Method: 通过对沿解的二次函数导数的要求，利用在有限时间区间内对该导数进行负幂处理，构造非线性项以确保解在固定时间达到零或进入给定区域；并将该理论应用于任意阶线性植物的 backstepping 控制设计，辅以数值仿真比较。

Result: 给出使解在固定时间收敛的充分条件，给出收敛时间界和区域，不同设计实现的数值仿真结果与已有方法比较，显示在收敛速度方面的提升。

Conclusion: 所提出的方法在理论上提供了固定时间收敛的新途径，并通过 backstepping 将其纳入任意阶线性系统的控制框架，具备较好的实际应用潜力，尽管在实现时候需注意参数选取和鲁棒性分析。

Abstract: A fast convergence in a fixed-time of solutions of nonlinear dynamical systems, for which special requirements are satisfied on the derivative of a quadratic function calculated along the solutions of the system, is proposed. The conditions for the system solutions to converge to zero and to a given region within a fixed-time are obtained. To achieve fast convergence, a negative power is applied to the derivative of a quadratic function within a specific time interval during the evolution of the system. The application of the proposed results to the design of control laws for arbitrary order linear plants using the backstepping method is considered. All the main results are accompanied by numerical modelling and a comparison of the proposed solutions with some existing ones.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [9] [Smoothing Rough Edges of IPv6 in VPNs](https://arxiv.org/abs/2512.19698)
*Yejin Cho,John Heidemann*

Main category: cs.NI

TL;DR: 本研究揭示商用VPN在IPv6方面的两大“粗糙边缘”：IPv4-only VPN存在IPv6泄漏，以及双栈VPN普遍存在IPv4优先（IPv6被地址选择规则降级）的现象，并提出一个非降级的IPv6地址范围作为解决方案，基于129k用户数据和Android测试进行验证。


<details>
  <summary>Details</summary>
Motivation: 保护用户隐私的VPN需要正确处理IPv6；当前实现易导致IPv6泄漏和IPv6不被优先使用，降低隐私性和用户体验。

Method: 通过WhatIsMyIPAddress.com的129,000名VPN用户日访问数据分析IPv6泄漏及比例；在Android上对六个VPN进行测试；提出并在Linux上原型实现一个新IPv6地址范围以避免降级；分析地址选择行为的根因。

Result: 发现12个IPv4-only VPN中至少5%的用户泄露原生IPv6地址；在IPv4-only VPN中有5%至57%的用户暴露原生IPv6地址；多数双栈VPN的用户偏好IPv4; VPN的地址选择规则导致IPv6被降级；提出并原型实现一个新的不被降级的VPN IPv6地址范围。

Conclusion: IPv6在VPN中的实现存在明显的“粗糙边缘”，需要通过新的地址分配策略来避免IPv6被降级，数据驱动的发现可帮助改进VPN厂商的IPv6支持。

Abstract: How do commercial VPNs interact with IPv6? We show two "rough edges" in how commercial VPNs handle IPv6. First, we show that many IPv4-only VPNs leak IPv6 traffic to the ISP. Individual use VPNs in part to conceal their local IP addresses, so such leaks reduce user privacy. While prior work has studied VPNs in testbeds, we use a new dataset of 129k VPN-using daily visitors to WhatIsMyIPAddress.com that quantifies these leaks and show 12 VPNs previously considered safe still leak for at least 5% of their users. We show native IPv6 addresses leak most commonly in VPNs that claim only IPv4 support, with 5% to 57% of visitors of v4-only VPNs having their native IPv6 address exposed. Second, we show that most dual-stack VPNs users actually select IPv4 instead of IPv6. We observe this problem in our visitor data, and we identify the root cause arises because when user's computer follows standard address-selection rules, VPN-assigned addresses are often de-preferenced. Testing six VPNs on Android, we show that five consistently de-prioritize IPv6. Finally, we suggest a solution to IPv6 de-preferencing: we define a new IPv6 address range for VPNs that is not de-preferenced by address selection. We prototype this solution on Linux. Our findings help identify and address rough edges in the addition of IPv6 support to VPNs.

</details>


### [10] [Edge-Served Congestion Control for Wireless Multipath Transmission with a Transformer Agent](https://arxiv.org/abs/2512.20186)
*Liang Wang*

Main category: cs.NI

TL;DR: Jazz decouples MPTCP decision-making from in-kernel datapath by using an edge Transformer-based agent, enabling data-driven control under partial observability and new network conditions; achieves modest gains and robustness.


<details>
  <summary>Details</summary>
Motivation: Kernel monolith blocks rapid, data-driven MPTCP development and responsiveness; partial observability due to noisy measurements; need flexible, edge-based intelligence.

Method: Proposes a decoupled architecture with an external edge brain; Transformer-based agent processes sequences of historical observations to handle partial observability; in-kernel datapath remains simple; evaluated on dual-band Wi-Fi testbed.

Result: Bandwidth efficiency improved by at least 2.85% over conventional methods; maintains 96.2% performance under 1% packet loss.

Conclusion: Demonstrates a practical blueprint for agile network intelligence; decoupled brain-and-datapath design facilitates data-driven MPTCP in real networks.

Abstract: Multipath TCP is widely adopted to enhance connection quality-of-service by leveraging multiple network pathways on modern devices. However, the evolution of its core congestion control is hindered by the OS kernel, whose monolithic design imposes high development overhead and lacks the resource flexibility required for data-driven methods. Furthermore, inherent noise in network statistics induces a partial observability problem, which can mislead data-driven methods like Deep Reinforcement Learning. To bridge this gap, we propose Jazz, a system that re-architects multipath congestion control through a decoupled architecture that separates the decision-making ``brain'' from the in-kernel datapath, enabling it to operate on an external (edge) entity. At its core, Jazz employs a Transformer-based agent that processes sequences of historical observations to overcome the partial observability of single-step reinforcement learning. This allows it to learn and master fluctuating link conditions and intricate cross-path dependencies. Tested on a dual-band (5GHz/6GHz) Wi-Fi testbed, our implementation improves bandwidth efficiency by at least 2.85\% over conventional methods and maintains 96.2\% performance under 1\% packet loss, validating this design as a practical blueprint for agile network intelligence.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [11] [Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning](https://arxiv.org/abs/2512.19777)
*Antonio Tarizzo,Mohammad Kazemi,Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出一个学习型数字化OTA聚合框架，结合URA码本、向量量化和AMP-DA-Net，在端到端训练下实现对分布式联邦学习的鲁棒数字OTA聚合，能在低SNR条件下提升可靠性并支持广义对称聚合函数，如截尾均值与多数表决。


<details>
  <summary>Details</summary>
Motivation: FEEL 的上行传输成为通信瓶颈，利用无线信道的叠加特性实现 OTA 聚合能够将通信与计算结合，但现有数字 OTA 在低信噪比（SNR）下表现受限，需提升恢复精度、收敛性与鲁棒性，并在同等上行开销下扩展聚合能力。

Method: 将无源/有源的 URA 码本、向量量化和 AMP-DA-Net 融合，设计一个端到端训练的解码器，与数字码本和服务器端本地训练统计共同训练，采用展开式 AMP 的近似消息传递解码器，并支持对称函数（如截尾均值、多数表决等）的数字 OTA 聚合；在高度异构设备数据和可用设备数量变化的场景下进行评估。

Result: 在高异构设备数据和不同活跃设备数的场景中，方法将可靠数字 OTA 操作的有效SNR提升超过10 dB，并在全SNR范围内达到或优于基线性能；解码器对信息损坏和非线性聚合具有鲁棒性，且保持与状态-of-the-art 相同的上行开销。

Conclusion: 该工作展示了端到端学习设计在数字 OTA 聚合中的广泛潜力，能够扩展 FEEL 的聚合函数范围并提升低SNR下的性能鲁棒性，为数字 OTA 通信在 FEEL 中的应用提供新的实现路径。

Abstract: Federated edge learning (FEEL) enables wireless devices to collaboratively train a centralised model without sharing raw data, but repeated uplink transmission of model updates makes communication the dominant bottleneck. Over-the-air (OTA) aggregation alleviates this by exploiting the superposition property of the wireless channel, enabling simultaneous transmission and merging communication with computation. Digital OTA schemes extend this principle by incorporating the robustness of conventional digital communication, but current designs remain limited in low signal-to-noise ratio (SNR) regimes. This work proposes a learned digital OTA framework that improves recovery accuracy, convergence behaviour, and robustness to challenging SNR conditions while maintaining the same uplink overhead as state-of-the-art methods. The design integrates an unsourced random access (URA) codebook with vector quantisation and AMP-DA-Net, an unrolled approximate message passing (AMP)-style decoder trained end-to-end with the digital codebook and parameter server local training statistics. The proposed design extends OTA aggregation beyond averaging to a broad class of symmetric functions, including trimmed means and majority-based rules. Experiments on highly heterogeneous device datasets and varying numbers of active devices show that the proposed design extends reliable digital OTA operation by more than 10 dB into low SNR regimes while matching or improving performance across the full SNR range. The learned decoder remains effective under message corruption and nonlinear aggregation, highlighting the broader potential of end-to-end learned design for digital OTA communication in FEEL.

</details>


### [12] [RIS-Empowered OTFS Modulation With Faster-than-Nyquist Signaling in High-Mobility Wireless Communications](https://arxiv.org/abs/2512.20332)
*Chaorong Zhang,Benjamin K. Ng,Hui Xu,Chan-Tong Lam,Halim Yanikomeroglu*

Main category: cs.IT

TL;DR: 提出了一种 RIS-增强的 OTFS 调制结合 FTN 的新方案 RIS-OTFS-FTN，通过在延迟-多普勒域中对符号进行联合建模，考虑 RIS 的被动波束成形、FTN 引入的符号间干扰以及 DD 相关通道，给出帧误差率、光谱效率和 PAPR 的分析结果，并设计了量化相位选择以最大化有效信道增益。大量 EVA 通道下的蒙特卡洛仿真验证理论并揭示光谱效率、PAPR、IBO 与误码性能之间的权衡。初步结果显示该方案在高移动性与频谱受限场景下具有显著的可靠性与光谱效率提升。


<details>
  <summary>Details</summary>
Motivation: 高移动性场景中多普勒扩展和多径造成的干扰严重削弱传统调制方案的可靠性和频谱效率；OTFS 在 DD 域提供鲁棒性；FTN 可通过有意的符号打包提升光谱效率；而 RIS 可以通过被动波束成形改善链路质量。三者结合有望在高移动性和频谱受限环境中同时提升可靠性与效率。

Method: 建立统一的 DD 域输入输出关系，联合考虑 RIS 被动波束成形、FTN 引起的符号间干扰以及 DD 通道特性；给出帧误差率、光谱效率和峰均功率比（PAPR）等的解析性能；设计具有量化相位选择的实际 RIS 相位调整策略以最大化有效信道增益；在标准化 EVA 通道模型下进行大量蒙特卡洛仿真以验证理论并分析光谱效率、PAPR、IBO 与误差性能之间的权衡。

Result: 给出统一的 DD 域输入输出模型及其在 RIS、FTN 与 DD 通道条件下的解析性能评估；得到对帧错误率、光谱效率、PAPR 等指标的理论分析结果，并通过 Monte Carlo 验证其正确性。仿真结果揭示了在 EVA 通道条件下，RIS 的引入与 FTN 的符号打包可在提升鲁棒性和光谱效率方面带来显著收益，同时也揭示了需要在 PAPR、IBO 与误差性能之间进行折中。

Conclusion: RIS-OTFS-FTN 展现出在高移动性和受限频谱场景下的显著性能提升，提供了一种可操作的、面向未来无线系统的解决方案，将被动 RIS 波束成形、OTFS 的 DD 域鲁棒性与 FTN 的高效谱利用结合，具有实际应用潜力。

Abstract: High-mobility wireless communication systems suffer from severe Doppler spread and multi-path delay, which degrade the reliability and spectral efficiency of conventional modulation schemes. Orthogonal time frequency space (OTFS) modulation offers strong robustness in such environments by representing symbols in the delay-Doppler (DD) domain, while faster-than-Nyquist (FTN) signaling can further enhance spectral efficiency through intentional symbol packing. Meanwhile, reconfigurable intelligent surfaces (RIS) provide a promising means to improve link quality via passive beamforming. Motivated by these advantages, we propose a novel RIS-empowered OTFS modulation with FTN signaling (RIS-OTFS-FTN) scheme. First, we establish a unified DD-domain input-output relationship that jointly accounts for RIS passive beamforming, FTN-induced inter-symbol interference, and DD-domain channel characteristics. Based on this model, we provide comprehensive analytical performance for the frame error rate, spectral efficiency, and peak-to-average power ratio (PAPR), etc. Furthermore, a practical RIS phase adjustment strategy with quantized phase selection is designed to maximize the effective channel gain. Extensive Monte Carlo simulations under a standardized extended vehicular A (EVA) channel model validate the theoretical results and provide key insights into the trade-offs among spectral efficiency, PAPR, input back-off (IBO), and error performance, with some interesting insights.The proposed RIS-OTFS-FTN scheme demonstrates notable performance gains in both reliability and spectral efficiency, offering a viable solution for future high-mobility and spectrum-constrained wireless systems.

</details>


### [13] [Viterbi State Selection for Discrete Pinching Antenna Systems](https://arxiv.org/abs/2512.20389)
*Victoria E. Galanopoulou,Thrassos K. Oikonomou,Odysseas G. Karagiannidis,Sotiris A. Tegos,Panagiotis D. Diamantoulakis*

Main category: cs.IT

TL;DR: 提出一种Viterbi状态选择算法（VSS）用于波导喂入的Pinching天线阵列的天线子集选择，在保持同等吞吐量的前提下，将复杂度从指数级降到多项式级。


<details>
  <summary>Details</summary>
Motivation: 解决针尖天线子集选择的指数复杂度问题。由于接收信号的相干叠加结果对相位非常敏感，需在时分接入方案下对天线进行动态、最优的子集选择。

Method: 提出VSS算法，通过将累积复增益的相位量化后定义状态树波导，采用Viterbi裁剪规则在各阶段削减被支配的天线子集；利用相位结构来指导子集搜索。

Result: 数值结果表明该方法在天线选择与速率方面可达到与穷举搜索相同的性能，同时将计算复杂度从指数级降至多项式级别，随可用天线数增大而受益显著。

Conclusion: VSS算法在相位敏感的波导馈电Pinching天线阵列子集选择问题上有效地实现近似最优性能，显著降低计算复杂度，具备在实时/大规模系统中的应用潜力。

Abstract: Pinching antennas enable dynamic control of electromagnetic wave propagation through reconfigurable radiating structures, but selecting an optimal subset of antennas remains a combinatorial problem with exponential complexity. This letter considers antenna subset selection for a waveguide-fed pinching antenna array serving ground users under a time-division access scheme. The achievable rate depends on the coherent superposition of the effective complex channel gains and is therefore highly sensitive to the relative phase alignment of the activated antennas. To address the prohibitive complexity of exhaustive search, we propose a Viterbi state selection (VSS) algorithm that exploits the phase structure of the combined received signal. The trellis state is defined by a quantized representation of the phase of the accumulated complex gain, and a Viterbi-based survivor rule is used to prune dominated antenna subsets across stages. Numerical results demonstrate that the proposed method achieves the same antenna selection and rate as exhaustive search, while reducing the computational complexity from exponential to polynomial in the number of available antennas.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [14] [PFA-NS: Power-Fading-Aware Noise Shaping Enabled C-Band IMDD System with Low Resolution DAC](https://arxiv.org/abs/2512.20010)
*Xiaobo Zeng,Liangcai Chen,Pan Liu,Ruonan Deng*

Main category: eess.SP

TL;DR: 功率衰落感知的降噪整形在C波段IMDD系统中对低分辨率DAC实现数据速率提升，约提升94%。


<details>
  <summary>Details</summary>
Motivation: 在低分辨率DAC下，量化噪声会显著降低IMDD系统在C波段的性能，利用信道的功率衰落特征来塑形噪声以降低对有用信道的干扰。

Method: 提出一种噪声整形方法，使量化噪声在衰落 notch 区域集中，从而避免对主信道的影响；结合低分辨率DAC实现。

Result: 在仿真/实验中，相较传统方法，数据速率提高约94%。

Conclusion: 功率衰落感知的噪声整形对IMDD系统在低分辨率DAC下具有显著提升潜力，可用于提升带宽效率。

Abstract: We propose and demonstrate a power-fading-aware noise-shaping technique for C-band IMDD system with low resolution DAC, which shapes and concentrates quantization noise within the fading-induced notch areas, yielding 94% improvement in data-rate over traditional counterpart.

</details>


### [15] [Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems](https://arxiv.org/abs/2512.20012)
*Qiushuo Hou,Sangwoo Park,Matteo Zecchin,Yunlong Cai,Guanding Yu,Osvaldo Simeone,Tommaso Melodia*

Main category: eess.SP

TL;DR: 提出边缘-云-专家级级联的LLM知识系统，目标在成本约束下确保输出与专家判断一致性，通过基于多重假设检验(MHT)的阈值选择实现对知识与置信度的测试，具有限制样本下的错配风险保证；在TeleQnA数据集上实现成本效率优于传统级联基线并保持给定置信水平的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备与云端部署大型语言模型时，需在推理成本、延迟和可靠性之间取得平衡；现有级联方法往往缺乏严格的统计保证，对错配成本缺乏可控性。

Method: 提出一个误配成本受限的优化框架，目标是最小化平均处理成本并确保自动回答与专家判断对齐；实现一个基于多重假设检验的阈值选择机制，对问答过程中的知识匹配和置信度进行统计检验，并给出有限样本下的风险保证。

Result: 在TeleQnA telecom数据集上，所提方法在成本效iciency方面优于传统的级联基线，同时在预定置信水平下保证可靠性。

Conclusion: 该方法实现了边缘-云-专家级 cascaded LLM 系统中成本与可靠性的有效权衡，并提供可统计的可靠性保证。

Abstract: Large language models (LLMs) are emerging as key enablers of automation in domains such as telecommunications, assisting with tasks including troubleshooting, standards interpretation, and network optimization. However, their deployment in practice must balance inference cost, latency, and reliability. In this work, we study an edge-cloud-expert cascaded LLM-based knowledge system that supports decision-making through a question-and-answer pipeline. In it, an efficient edge model handles routine queries, a more capable cloud model addresses complex cases, and human experts are involved only when necessary. We define a misalignment-cost constrained optimization problem, aiming to minimize average processing cost, while guaranteeing alignment of automated answers with expert judgments. We propose a statistically rigorous threshold selection method based on multiple hypothesis testing (MHT) for a query processing mechanism based on knowledge and confidence tests. The approach provides finite-sample guarantees on misalignment risk. Experiments on the TeleQnA dataset -- a telecom-specific benchmark -- demonstrate that the proposed method achieves superior cost-efficiency compared to conventional cascaded baselines, while ensuring reliability at prescribed confidence levels.

</details>


### [16] [EDA-RoF: Elastic Digital-Analog Radio-Over-Fiber (RoF) Modulation and Demodulation Architecture Enabling Seamless Transition Between Analog RoF and Digital RoF](https://arxiv.org/abs/2512.20018)
*Xiaobo Zeng,Pan Liu,Liangcai Chen,Ruonan Deng*

Main category: eess.SP

TL;DR: 提出并验证一种弹性RoF调制解调体系，兼容A-RoF/D-RoF，展示随1/η增大时的准线性SNR提升，拟合R^2=0.9908。


<details>
  <summary>Details</summary>
Motivation: 需要统一并兼容A-RoF和D-RoF两种远场光接入的架构，以提高灵活性、带宽适配和系统效率，同时实现更优的信噪比与资源利用。

Method: 设计并演示一种弹性数字-模拟RoF调制解调架构，能够无缝连接A-RoF与D-RoF解决方案，通过对1/η参数的调整实现SNR的准线性缩放；并给出回归分析以验证拟合质量（R^2=0.9908）。

Result: 实现了与1/η相关的准线性SNR缩放，且拟合优度R^2达到0.9908，表明架构在A-RoF与D-RoF之间的桥接性和性能可预测性。

Conclusion: 该弹性RoF体系实现了A-RoF与D-RoF之间的无缝桥接，提供更灵活的资源分配和适应不同应用场景的能力，具有潜在的实际部署价值。

Abstract: We propose and demonstrate an elastic digital-analog radio-over-fiber (RoF) modulation and demodulation architecture, seamlessly bridging A-RoF and D-RoF solutions, achieving quasilinear SNR scaling with respect to 1/η, and evidenced by R^2=0.9908.

</details>


### [17] [Robust and Secure Transmission for Movable-RIS Assisted ISAC with Imperfect Sense Estimation](https://arxiv.org/abs/2512.20071)
*Ling Zhuang,Ximing Xie,Fang Fang,Ali Attaran,Zhizhong Zhang*

Main category: eess.SP

TL;DR: 引入可移动RIS以实现自适应波束重构，优化发射波束、MRIS相位与两子 SURFACE 相对位置，在Eve信道不确定性下最大化最小保密速率；提出基于凸界近似和S-过程的松弛及交替优化算法，结果显示移动子表少量元素即可显著提升保密性能。


<details>
  <summary>Details</summary>
Motivation: 解决静态RIS在多用户/多任务ISAC场景中的可控性和灵活性不足的问题，利用固定大子表加上可移动小子表实现动态波束控制，且不增加额外硬件与功耗。

Method: 建立MRIS辅助的ISAC系统，考虑不完美感知下的保密优化；将专用雷达信号作为干扰增强保密性；联合优化发射波束、MRIS相位与两子表相对位置；对Eve信道不确定性进行凸界近似，利用S-过程将半无限约束转化为线性矩阵不等式，提出交替优化与罚对偶分解的高效算法。

Result: 仿真表明在移动子表元素数量较少时，MRIS架构显著提升保密性能，且对最弱用户的鲁棒性提升明显。

Conclusion: MRIS为在不增加硬件复杂度和功耗的前提下提升ISAC系统保密性能提供了可行路径，移动子表的布局与动态波束重构对性能提升起决定性作用。

Abstract: Reconfigurable intelligent surfaces (RISs) have been extensively applied in integrated sensing and communication (ISAC) systems due to the capability of enhancing physical layer security (PLS). However, conventional static RIS architectures lack the flexibility required for adaptive beam control in multi-user and multifunctional scenarios. To address this issue without introducing additional hardware complexity and power consumption, in this paper, we exploit a movable RIS (MRIS) architecture, which consists of a large fixed sub-surface and a smaller movable sub-surface that slides on the fixed sub-surface to achieve dynamic beam reconfiguration with static phase shifts. This paper investigates an MRIS-assisted ISAC system under imperfect sensing estimation, where dedicated radar signals serve as artificial noise to enhance secure transmission against potential eavesdroppers (Eves). The transmit beamforming vectors, MRIS phase shifts, and relative positions of the two sub-surfaces are jointly optimized to maximize the minimum secrecy rate, ensuring robust secrecy performance for the weakest user under the uncertainty of the Eves' channels. To handle the non-convexity, a convex bound is derived for the Eve channel uncertainty, and the S-procedure is employed to reformulate semi-infinite constraints as linear matrix inequalities. An efficient alternating optimization and penalty dual decomposition-based algorithm is developed. Simulation results demonstrate that the proposed MRIS architecture substantially improves secrecy performance, especially when only a small number of elements are allocated to the movable sub-surface.

</details>


### [18] [Target Classification for Integrated Sensing and Communication in Industrial Deployments](https://arxiv.org/abs/2512.20154)
*Luca Barbieri,Marcus Henninger,Paolo Tosi,Artjom Grudnitsky,Mattia Brambilla,Monica Nicoli,Silvio Mandelli*

Main category: eess.SP

TL;DR: 通过DL检测器从ISAC雷达图像中直接识别目标类别，在真实的商用毫米波硬件测试床上实现了高分类性能，验证了ISAC下ATR的可行性，并讨论了泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 弥合仿真与真实部署之间的差距，评估在ISAC框架中使用ATR的可行性。

Method: 提出基于DL的目标检测器，用于直接从ISAC系统生成的雷达图像推断目标类别；在基于商用毫米波无线单元的ISAC测试床上进行实验评估。

Result: 实验结果显示在目标分类上具备准确性，证明在该设置下使用蜂窝硬件实现ATR ISAC的可行性。

Conclusion: 讨论开放的泛化挑战，为未来在该方向的研究提供指引。

Abstract: Integrated Sensing and Communication (ISAC) systems enable cellular networks to jointly operate as communication technology and sense the environment. While opportunities and potential performance have been largely investigated in simulations, few experimental works have showcased Automatic Target Recognition (ATR) effectiveness in a real-world deployment based on cellular radio units. To bridge this gap, this paper presents an initial study investigating the feasibility of ATR for ISAC. Our ATR solution uses a Deep Learning (DL)-based detector to infer the target class directly from the radar images generated by the ISAC system. The DL detector is evaluated with experimental data from a ISAC testbed based on commercially available mmWave radio units in the ARENA 2036 industrial research campus located in Stuttgart, Germany. Experimental results demonstrate accurate classification performance, demonstrating the feasibility of ATR ISAC with cellular hardware in our setup. We finally provide insights about the open generalization challenges, that will fuel future work on the topic.

</details>


### [19] [Sum-Rate Maximization for Uplink Segmented Waveguide-Enabled Pinching-Antenna Systems](https://arxiv.org/abs/2512.20246)
*Songnan Gu,Hao Jiang,Chongjun Ouyang,Yuanwei Liu,Dong In Kim*

Main category: eess.SP

TL;DR: 提出SWAN框架的多用户上行传输，基于分段波导-夹紧天线系统，覆盖两种协议（段选择SS和段聚合SA）以及TDMA/NOMA的可实现上行汇总速率，并给出低复杂度的PA放置方法，数值结果显示SWAN优于传统夹紧天线，SA优于SS。


<details>
  <summary>Details</summary>
Motivation: 提升多用户上行吞吐量，利用分段波导和夹紧天线实现灵活分段与聚合，结合TDMA和NOMA分析不同接入方案的性能；提供低复杂度天线放置策略以便在实际系统中应用。

Method: 提出SWAN框架，并在两种协议下推导两种多址下的可实现上行汇总速率；针对两种协议和两种多址提出低复杂度的夹紧天线放置方法；通过数值仿真验证性能并与传统夹紧天线系统对比。

Result: SWAN在汇总速率方面对传统夹紧天线系统具有更高性能；SA相对于SS提供额外性能提升。

Conclusion: SWAN在上行多用户场景下提供更高灵活性和吞吐潜力，且SA优于SS，具有实际应用前景。

Abstract: A multiuser uplink transmission framework based on the segmented waveguide-enabled pinching-antenna system (SWAN) is proposed under two operating protocols: segment selection (SS) and segment aggregation (SA). For each protocol, the achievable uplink sum-rate is characterized for both time-division multiple access (TDMA) and non-orthogonal multiple access (NOMA). Low-complexity placement methods for the pinching antennas (PAs) are developed for both protocols and for both multiple-access schemes. Numerical results validate the effectiveness of the proposed methods and show that SWAN achieves higher sum-rate performance than conventional pinching-antenna systems, while SA provides additional performance gains over SS.

</details>


### [20] [A Tutorial to Multirate Extended Kalman Filter Design for Monitoring of Agricultural Anaerobic Digestion Plants](https://arxiv.org/abs/2512.20354)
*Simon Hellmann,Terrance Wilms,Stefan Streif,Sören Weinrich*

Main category: eess.SP

TL;DR: 提出基于样本状态扩增的多速率扩展卡尔曼滤波（MR-EKF），在未知离线延迟和不同采样率条件下对厌氧消化过程进行状态估计与观测融合，在仿真中验证其鲁棒性与收敛性。


<details>
  <summary>Details</summary>
Motivation: 多测量采样率与未知离线延迟在生物过程监控中普遍存在，传统EKF难以充分利用离线延迟数据，因此需要能处理多速率数据和时延的滤波方法，以提高状态估计和过程控制的可靠性。

Method: 以样本状态扩增为核心，构建MR-EKF并在仿真农业场景的厌氧消化过程上评估；探讨不同延迟长度、测量噪声、 Plant-Model Mismatch (PMM) 与初始误差对性能的影响；并提出系统化的调参流程与实现要点。

Result: 在适当调参条件下，MR-EKF 能稳定地估计隐变量、有效融合带延迟的离线观测、并对在线噪声进行平滑；只要延迟不使系统观测性丧失，延迟长度对性能的影响不再关键；初始误差与PMM对收敛的影响通常比测量噪声更显著；提供了可操作的调参指南。

Conclusion: MR-EKF 为多速率系统的状态估计提供实用实现路径，促进面向需求的生物气发电运行与电网稳定性，同时为从业者在实际应用中实现多速率观测融合提供指导。

Abstract: In many applications of biotechnology, measurements are available at different sampling rates, e.g., due to online sensors and offline lab analysis. Offline measurements typically involve time delays that may be unknown a priori due to the underlying laboratory procedures. This multirate (MR) setting poses a challenge to Kalman filtering, where conventionally measurement data is assumed to be available on an equidistant time grid and without delays. The present study derives the MR version of an extended Kalman filter (EKF) based on sample state augmentation, and applies it to the anaerobic digestion (AD) process in a simulative agricultural setting. The performance of the MR-EKF is investigated for various scenarios, i.e., varying delay lengths, measurement noise levels, plant-model mismatch (PMM), and initial state error. Provided with an adequate tuning, the MR-EKF could be demonstrated to reliably estimate the process state, to appropriately fuse delayed offline measurements, and to smooth noisy online measurements well. Because of the sample state augmentation approach, the delay length of offline measurements does not critically impair state estimation performance, provided observability is not lost during the delays. Poor state initialization and PMM affect convergence more than measurement noise levels. Further, selecting an appropriate tuning was found to be critically important for successful application of the MR-EKF, for which a systematic approach is presented. This study provides implementation guidance for practitioners aiming at successfully applying state estimation for multirate systems. It thereby contributes to develop demand-driven operation of biogas plants, which may aid in stabilizing a renewable electricity grid.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Large Language Models for EDA Cloud Job Resource and Lifetime Prediction](https://arxiv.org/abs/2512.19701)
*Yuxuan Yin,Shengke Zhou,Yunjie Zhang,Ajay Mohindra,Boxun Xu,Peng Li*

Main category: cs.LG

TL;DR: 提出一个通过微调大语言模型实现文本到文本回归的框架，用于EDA云环境中的资源与作业生命周期预测，采用科学记号与前缀填充以约束输出格式，并在全注意力微调和推理阶段提升滑动窗口注意力LLM的预测精度，实证在真实云数据集上建立新的基线。


<details>
  <summary>Details</summary>
Motivation: EDA行业云计算的资源与作业生命周期预测需求日益迫切，传统ML方法需要大量特征工程且难以应对工作负载的异质性；需要一种能够直接从文本和描述中学习的通用方法。

Method: 通过对大语言模型进行文本到文本回归的微调，提出科学记号和前缀填充以约束输出格式；比较全注意力微调与推理对滑动窗口注意力LLM的预测精度影响；在真实云数据集上进行实验，验证框架的有效性。

Result: 引入的格式约束提高了输出格式的可靠性；全注意力的微调与推理提升了滑动窗口注意力LLM的预测精度；在实际云数据集上建立了新的基线，显示在EDA领域的资源和作业生命周期预测中的性能提升。

Conclusion: 所提出的框架对EDA云环境的资源与作业生命周期预测具有有效性和潜在应用前景，能够提升调度性能并降低对特征工程的依赖；未来工作可进一步扩展到其他异构工作负载和不同云场景。

Abstract: The rapid growth of cloud computing in the Electronic Design Automation (EDA) industry has created a critical need for resource and job lifetime prediction to achieve optimal scheduling. Traditional machine learning methods often struggle with the complexity and heterogeneity of EDA workloads, requiring extensive feature engineering and domain expertise. We propose a novel framework that fine-tunes Large Language Models (LLMs) to address this challenge through text-to-text regression. We introduce the scientific notation and prefix filling to constrain the LLM, significantly improving output format reliability. Moreover, we found that full-attention finetuning and inference improves the prediction accuracy of sliding-window-attention LLMs. We demonstrate the effectiveness of our proposed framework on real-world cloud datasets, setting a new baseline for performance prediction in the EDA domain.

</details>


### [22] [Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches](https://arxiv.org/abs/2512.19713)
*Taoran Sheng,Manfred Huber*

Main category: cs.LG

TL;DR: 在可穿戴传感 HAR 的监督学习谱中，提出弱监督和弱自监督框架，以及多任务与知识共享策略，实验证明在显著减少标注的情况下仍能达到接近完全监督的性能，并展示了10%标注数据下的高效弱自监督方法。


<details>
  <summary>Details</summary>
Motivation: 大量标注数据成本高且难以获得，完全监督虽准确但代价高；无监督方法虽省标注但性能通常不足；因此需要在标注成本与准确性之间找到折中，推动在现实场景中可行的 HAR 解决方案。

Method: 比较六种学习范式：1) 传统完全监督，2) 基础无监督，3) 具约束的弱监督，4) 具知识共享的多任务学习，5) 基于领域知识的自监督，6) 新颖的弱自监督框架，结合领域知识与极少量标注数据。通过基准数据集进行系统实验，比较不同标注量下的性能及知识共享效应。

Result: 实验结果显示：(i) 弱监督在显著降低标注量的同时实现接近完全监督的性能；(ii) 多任务框架通过相关任务的知识共享提升了性能；(iii) 弱自监督在仅需约10%标注数据时表现出高效性与竞争力。

Conclusion: 本研究揭示了不同学习范式的互补性，提供了根据可用标注量定制 HAR 解决方案的视角；提出的弱自监督框架在标注数据受限的现实应用场景中具有潜在价值。

Abstract: Human activity recognition (HAR) using wearable sensors has advanced through various machine learning paradigms, each with inherent trade-offs between performance and labeling requirements. While fully supervised techniques achieve high accuracy, they demand extensive labeled datasets that are costly to obtain. Conversely, unsupervised methods eliminate labeling needs but often deliver suboptimal performance. This paper presents a comprehensive investigation across the supervision spectrum for wearable-based HAR, with particular focus on novel approaches that minimize labeling requirements while maintaining competitive accuracy. We develop and empirically compare: (1) traditional fully supervised learning, (2) basic unsupervised learning, (3) a weakly supervised learning approach with constraints, (4) a multi-task learning approach with knowledge sharing, (5) a self-supervised approach based on domain expertise, and (6) a novel weakly self-supervised learning framework that leverages domain knowledge and minimal labeled data. Experiments across benchmark datasets demonstrate that: (i) our weakly supervised methods achieve performance comparable to fully supervised approaches while significantly reducing supervision requirements; (ii) the proposed multi-task framework enhances performance through knowledge sharing between related tasks; (iii) our weakly self-supervised approach demonstrates remarkable efficiency with just 10\% of labeled data. These results not only highlight the complementary strengths of different learning paradigms, offering insights into tailoring HAR solutions based on the availability of labeled data, but also establish that our novel weakly self-supervised framework offers a promising solution for practical HAR applications where labeled data are limited.

</details>


### [23] [Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data](https://arxiv.org/abs/2512.19716)
*Behrooz Mamandipoor,Chun-Nan Hsu,Martin Krause,Ulrich H. Schmidt,Rodney A. Gabriel*

Main category: cs.LG

TL;DR: 一项大规模多中心研究，开发了一个多模态深度学习模型，利用初始24小时的结构化数据、非结构化数据（临床笔记）及胸部X光影像来预测ICU住院死亡风险；在外部数据验证中，AUROC 最高可达0.92，且包含笔记与影像时性能优于仅结构化数据的模型，强调多源数据整合的价值。


<details>
  <summary>Details</summary>
Motivation: 在ICU中早期、准确地预测死亡风险有助于制定治疗策略和资源分配；整合多种数据模态有望提升预测准确性，克服单一数据来源的局限。

Method: 以MIMIC-III、MIMIC-IV、eICU、HiRID等数据集为基础，在MIMIC数据集上建立多模态模型，利用ICU入院后前24小时的时序数据（时间不变和时间变量）、临床笔记和胸部X线影像作为输入；使用AUROC、AUPRC、Brier分数等指标进行评估，并在 temporally 分离的MIMIC人群、HiRID、eICU等数据集上进行外部验证；比较仅结构化数据模型与融合笔记和影像的多模态模型。

Result: 仅结构化数据模型的AUROC为0.92，AUPRC为0.53，Brier为0.19；在eICU等八个机构的外部验证中，AUROC介于0.84–0.92之间。若纳入仅在有可用临床笔记和影像数据的患者，加入笔记与影像后，AUROC从0.87提升至0.89，AUPRC从0.43提升至0.48，Brier从0.37下降至0.17。

Conclusion: 多模态数据（结构化数据、笔记、影像）的整合可提升住院死亡率预测的准确性与外部泛化能力，强调跨来源数据的互补性及外部验证的重要性。

Abstract: Early prediction of in-hospital mortality in critically ill patients can aid clinicians in optimizing treatment. The objective was to develop a multimodal deep learning model, using structured and unstructured clinical data, to predict in-hospital mortality risk among critically ill patients after their initial 24 hour intensive care unit (ICU) admission. We used data from MIMIC-III, MIMIC-IV, eICU, and HiRID. A multimodal model was developed on the MIMIC datasets, featuring time series components occurring within the first 24 hours of ICU admission and predicting risk of subsequent inpatient mortality. Inputs included time-invariant variables, time-variant variables, clinical notes, and chest X-ray images. External validation occurred in a temporally separated MIMIC population, HiRID, and eICU datasets. A total of 203,434 ICU admissions from more than 200 hospitals between 2001 to 2022 were included, in which mortality rate ranged from 5.2% to 7.9% across the four datasets. The model integrating structured data points had AUROC, AUPRC, and Brier scores of 0.92, 0.53, and 0.19, respectively. We externally validated the model on eight different institutions within the eICU dataset, demonstrating AUROCs ranging from 0.84-0.92. When including only patients with available clinical notes and imaging data, inclusion of notes and imaging into the model, the AUROC, AUPRC, and Brier score improved from 0.87 to 0.89, 0.43 to 0.48, and 0.37 to 0.17, respectively. Our findings highlight the importance of incorporating multiple sources of patient information for mortality prediction and the importance of external validation.

</details>


### [24] [Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference](https://arxiv.org/abs/2512.19717)
*Zhan Zhang*

Main category: cs.LG

TL;DR: 提出了逆因果聚焦算法(ICFA)，通过目标条件再加权在极大候选空间中高效发现有用解，包含稳定性诊断、理论要点，以及两组可复现实验（受限语言生成与稀疏奖励导航），并探讨结构化提示与混合推理架构。


<details>
  <summary>Details</summary>
Motivation: 在语言生成、规划与强化学习等领域，极大候选空间使得寻找罕见但有用解变得困难。需要一个可操作的框架，将已有的候选采样器与任务相关的相似性函数结合，进行聚焦搜索，同时避免退化。

Method: 把搜索视作目标条件下的再加权过程，利用现有的提案采样器和任务特定相似性函数构建聚焦采样分布，动态控制聚焦强度以防止退化。提供基于有效样本量的稳定性诊断，给出简明理论草图，说明何时 ICFA 能降低样本需求；给出两个可复现实验（受限语言生成与稀疏奖励导航），并讨论结构化提示如何实现近似的语言层面 ICFA，以及提示推理与算法重加权的混合架构。

Result: 实验层面，ICFA 在保持稳定性的前提下提升了在大搜索空间中的样本效率与成功率，提供了基于有效样本量的稳定性诊断；两组可复现实验（受限语言生成与稀疏奖励导航）展示了在实际任务中的可重复结果。

Conclusion: 结构化提示可以实现近似的 ICFA；提出将提示推理与算法级重加权相结合的混合架构，预计在多语言任务与复杂搜索问题中具有广泛应用前景。

Abstract: Finding rare but useful solutions in very large candidate spaces is a recurring practical challenge across language generation, planning, and reinforcement learning. We present a practical framework, \emph{Inverted Causality Focusing Algorithm} (ICFA), that treats search as a target-conditioned reweighting process. ICFA reuses an available proposal sampler and a task-specific similarity function to form a focused sampling distribution, while adaptively controlling focusing strength to avoid degeneracy. We provide a clear recipe, a stability diagnostic based on effective sample size, a compact theoretical sketch explaining when ICFA can reduce sample needs, and two reproducible experiments: constrained language generation and sparse-reward navigation. We further show how structured prompts instantiate an approximate, language-level form of ICFA and describe a hybrid architecture combining prompted inference with algorithmic reweighting.

</details>


### [25] [Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries](https://arxiv.org/abs/2512.19719)
*Zihao Lv,Siqi Ai,Yanbin Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Targeted maintenance strategies, ensuring the dependability and safety of industrial machinery. However, current modeling techniques for assessing both local and global correlation of battery degradation sequences are inefficient and difficult to meet the needs in real-life applications. For this reason, we propose a novel deep learning architecture, multiscale dual-path feature aggregation network (MDFA-Net), for RUL prediction. MDFA-Net consists of dual-path networks, the first path network, multiscale feature network (MF-Net) that maintains the shallow information and avoids missing information, and the second path network is an encoder network (EC-Net) that captures the continuous trend of the sequences and retains deep details. Integrating both deep and shallow attributes effectively grasps both local and global patterns. Testing conducted with two publicly available Lithium-ion battery datasets reveals our approach surpasses existing top-tier methods in RUL forecasting, accurately mapping the capacity degradation trajectory.

</details>


### [26] [Per-Axis Weight Deltas for Frequent Model Updates](https://arxiv.org/abs/2512.19720)
*Stefan Kuyumdzhiev,Radostin Cholakov*

Main category: cs.LG

TL;DR: 提出一种1-bit delta压缩方法，用于表示微调权重的残差，结合每轴（行/列）的FP16缩放因子，显著降低存储和冷启动时延，同时保持较高重建质量；实现可直接替换的模块加载器，适用于大规模任务专用LLM的频繁更新。


<details>
  <summary>Details</summary>
Motivation: 微调权重相对基模型只是小幅结构化残差，但完整FP16检查点在存储和加载上成本高。现有1-bit或标量化方法难以充分捕捉维度间变化，需更高效的表示以降低冷启动延迟和存储开销。

Method: 将微调用权重表示为权重差值的符号1-bit，以及按行/按列的FP16缩放因子；从一个小的校准集学习这些按轴的缩放，实现紧凑但更准确的重建。加载时以单次传输将打包后的delta分发给模块，避免密集重建。该方法可作为现成替代，几乎无需大量标注数据。

Result: 相较于标量方案，重建质量显著提升，系统加载和存储开销显著降低，冷启动延迟更小；方案为drop-in，推理效率保持良好，且实验设置和源码公开。

Conclusion: 按轴的1-bit权重delta在频繁模型更新场景下提供了一种高效、可扩展的解决方案，兼具精度与低开销，适合在大规模任务专用LLM环境中应用。

Abstract: Serving many task-specialized LLM variants is often limited by the large size of fine-tuned checkpoints and the resulting cold-start latency. Since fine-tuned weights differ from their base model by relatively small structured residuals, a natural approach is to represent them as compressed deltas. We propose a simple 1-bit delta scheme that stores only the sign of the weight difference together with lightweight per-axis (row/column) FP16 scaling factors, learned from a small calibration set. This design preserves the compactness of 1-bit deltas while more accurately capturing variation across weight dimensions, leading to improved reconstruction quality over scalar alternatives. From a systems perspective, a streamlined loader that transfers packed deltas in a single operation per module reduces cold-start latency and storage overhead, with artifacts several times smaller than a full FP16 checkpoint. The method is drop-in, requires minimal calibration data, and maintains inference efficiency by avoiding dense reconstruction. Our experimental setup and source code are available at https://github.com/kuiumdjiev/Per-Axis-Weight-Deltas-for-Frequent-Model-Updates.

</details>


### [27] [Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals](https://arxiv.org/abs/2512.19721)
*Vineet Yadav*

Main category: cs.LG

TL;DR: 提出一个有符号多态Jaccard/Tanimoto框架，将信号表示为带符号状态空间上的原子测度，通过嵌入获得非负多态表示，进而构造有界度量、PSD核、以及概率语义/预算分解的一体化分析工具。


<details>
  <summary>Details</summary>
Motivation: 需要在处理任意实数/复数信号时，仍保持可度量性和核结构，同时实现可解释的分解、预算约束与概率语义，超越传统仅适用于非负向量的重叠距离。

Method: 将信号建模为带符号状态空间上的原子测度；通过正/负分割、 Cartesian 与极坐标分解等嵌入实现非负多态表示；应用Tanimoto/Jaccard到嵌入得到[0,1]距离，满足三角不等式，且构成正半定核；通过莫比乌斯反演实现信号幅度在 coalition 的非负加性贡献的分解；对嵌入归一化得到状态配置的概率测度，使距离成为总变差的单调变换，实现 regime–intensity 分解。

Result: 得到在 [0,1] 区间的距离，具备三角不等式、PSD核、可直接用于核方法和图学习；实现 coalition 的预算封闭和非负分解；归一化后提供概率语义与 regime–intensity 的分解。

Conclusion: 提供一个单一、可解释的框架，兼具有界度量、PSD核、概率语义与预算核算，支持 correlograms、特征工程、相似性图等分析工具，在科学和金融应用中具有广泛潜力。

Abstract: We introduce a sign-aware, multistate Jaccard/Tanimoto framework that extends overlap-based distances from nonnegative vectors and measures to arbitrary real- and complex-valued signals while retaining bounded metric and positive-semidefinite kernel structure. Formally, the construction is a set- and measure-theoretic geometry: signals are represented as atomic measures on a signed state space, and similarity is given by a generalized Jaccard overlap of these measures. Each signal is embedded into a nonnegative multistate representation, using positive/negative splits for real signals, Cartesian and polar decompositions for complex signals, and user-defined state partitions for refined regime analysis. Applying the Tanimoto construction to these embeddings yields a family of $[0,1]$ distances that satisfy the triangle inequality and define positive-semidefinite kernels usable directly in kernel methods and graph-based learning. Beyond pairwise distances, we develop coalition analysis via Möbius inversion, which decomposes signal magnitude into nonnegative, additive contributions with exact budget closure across coalitions of signals. Normalizing the same embeddings produces probability measures on coordinate -- state configurations, so that the distance becomes a monotone transform of total variation and admits a regime -- intensity decomposition. The resulting construction yields a single, mechanistically interpretable distance that simultaneously provides bounded metric structure, positive-semidefinite kernels, probabilistic semantics, and transparent budget accounting within one sign-aware framework, supporting correlograms, feature engineering, similarity graphs, and other analytical tools in scientific and financial applications.

</details>


### [28] [Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism](https://arxiv.org/abs/2512.19722)
*Alessandro Casadei,Clemens Grupp,Sreyoshi Bhaduri,Lu Guo,Wilson Fung,Rohit Malshe,Raj Ratan,Ankush Pole,Arkajit Rakshit*

Main category: cs.LG

TL;DR: 提出一种基于节点成本函数非对称性的自适应预测调整方法，通过将成本非对称性纳入预测误差分布来偏向最低成本情景，并通过自调节机制控制调整幅度，从而实现可观的年度节省（$5.1M）。


<details>
  <summary>Details</summary>
Motivation: 以降低预测与执行之间的成本损失为目标，利用成本非对称性和未建模因素（如标定误差、宏观动态变化）实现更高经济收益，且具备对站点条件的自适应能力。

Method: 将成本函数的非对称性动态融入预测误差的概率分布，优先考虑成本最低的情景；计算节省并用自调节机制根据观测到的节省规模调节调整幅度，使模型对站点条件和未建模因素具备自适应性。

Result: 在实证中实现年度节省5.1百万美元，验证模型在不同站点条件下的可行性与鲁棒性。

Conclusion: 该方法可显著提升预测驱动的经济收益，具备对站点特性和未建模因素的适应性；未来工作可进一步评估普适性与潜在局限性。

Abstract: This work introduces a methodology to adjust forecasts based on node-specific cost function asymmetry. The proposed model generates savings by dynamically incorporating the cost asymmetry into the forecasting error probability distribution to favor the least expensive scenario. Savings are calculated and a self-regulation mechanism modulates the adjustments magnitude based on the observed savings, enabling the model to adapt to station-specific conditions and unmodeled factors such as calibration errors or shifting macroeconomic dynamics. Finally, empirical results demonstrate the model's ability to achieve \$5.1M annual savings.

</details>


### [29] [End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment](https://arxiv.org/abs/2512.19723)
*Firas Bayram,Bestoun S. Ahmed,Erik Hallin*

Main category: cs.LG

TL;DR: A lightweight end-to-end framework that embeds real-time data quality assessment into ML workflows (MLOps) for industrial settings, achieving low overhead, real-time quality-driven decisions.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between data quality assessment and ML deployment by integrating drift detection and adaptive quality metrics into production ML pipelines to improve reliability and latency in dynamic environments.

Method: Proposes an end-to-end framework combining dynamic drift detection, adaptive data quality metrics, and MLOps in a lightweight pipeline; validated in an ESR vacuum pumping process in steel manufacturing.

Result: Realizes 12% improvement in model performance (R2 = 94%) and fourfold reduction in prediction latency; analyzes the impact of data quality acceptability thresholds.

Conclusion: Advances MLOps for time-sensitive industrial decision-making by providing a robust, low-overhead framework that integrates data quality with ML in dynamic environments.

Abstract: This paper introduces a novel end-to-end framework that efficiently integrates data quality assessment with machine learning (ML) model operations in real-time production environments. While existing approaches treat data quality assessment and ML systems as isolated processes, our framework addresses the critical gap between theoretical methods and practical implementation by combining dynamic drift detection, adaptive data quality metrics, and MLOps into a cohesive, lightweight system. The key innovation lies in its operational efficiency, enabling real-time, quality-driven ML decision-making with minimal computational overhead. We validate the framework in a steel manufacturing company's Electroslag Remelting (ESR) vacuum pumping process, demonstrating a 12% improvement in model performance (R2 = 94%) and a fourfold reduction in prediction latency. By exploring the impact of data quality acceptability thresholds, we provide actionable insights into balancing data quality standards and predictive performance in industrial applications. This framework represents a significant advancement in MLOps, offering a robust solution for time-sensitive, data-driven decision-making in dynamic industrial environments.

</details>


### [30] [Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking](https://arxiv.org/abs/2512.19725)
*Srishti Gupta,Riccardo Balia,Daniele Angioni,Fabio Brau,Maura Pintor,Ambra Demontis,Alessandro Sebastian,Salvatore Mario Carta,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 对持续学习（CL）与对分布外检测（OOD）耦合的综述性分析，强调在现实世界数据随时间变化与未知输入频繁出现的情境下，需高效更新而非从头训练的能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中数据分布经常漂移，新任务和新输入不断出现，模型若仅基于静态的i.i.d.假设难以长期可靠运行；CL与OOD检测的结合有望实现连续、鲁棒、自适应的AI系统。

Method: 以综述性研究为主，系统梳理现有的CL与OOD检测方法、范式、评估基准、数据集及挑战，提出联合框架的设计原则与未来研究方向。

Result: 对现有方法进行了归纳与对比分析，指出在适应新分布、保持旧知识以及识别未知输入方面的不足，给出将两者耦合的思路与综合评估路径。

Conclusion: 强调CL与OOD检测的耦合是实现鲁棒、可适应AI系统的关键；需要更统一的评估标准、现实场景驱动的数据集，以及高效的算法以应对资源约束。

Abstract: Recent years have witnessed significant progress in the development of machine learning models across a wide range of fields, fueled by increased computational resources, large-scale datasets, and the rise of deep learning architectures. From malware detection to enabling autonomous navigation, modern machine learning systems have demonstrated remarkable capabilities. However, as these models are deployed in ever-changing real-world scenarios, their ability to remain reliable and adaptive over time becomes increasingly important. For example, in the real world, new malware families are continuously developed, whereas autonomous driving cars are employed in many different cities and weather conditions. Models trained in fixed settings can not respond effectively to novel conditions encountered post-deployment. In fact, most machine learning models are still developed under the assumption that training and test data are independent and identically distributed (i.i.d.), i.e., sampled from the same underlying (unknown) distribution. While this assumption simplifies model development and evaluation, it does not hold in many real-world applications, where data changes over time and unexpected inputs frequently occur. Retraining models from scratch whenever new data appears is computationally expensive, time-consuming, and impractical in resource-constrained environments. These limitations underscore the need for Continual Learning (CL), which enables models to incrementally learn from evolving data streams without forgetting past knowledge, and Out-of-Distribution (OOD) detection, which allows systems to identify and respond to novel or anomalous inputs. Jointly addressing both challenges is critical to developing robust, efficient, and adaptive AI systems.

</details>


### [31] [Tiny, On-Device Decision Makers with the MiniConv Library](https://arxiv.org/abs/2512.19726)
*Carlos Purves*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning (RL) has achieved strong results, but deploying visual policies on resource-constrained edge devices remains challenging due to computational cost and communication latency. Many deployments therefore offload policy inference to a remote server, incurring network round trips and requiring transmission of high-dimensional observations. We introduce a split-policy architecture in which a small on-device encoder, implemented as OpenGL fragment-shader passes for broad embedded GPU support, transforms each observation into a compact feature tensor that is transmitted to a remote policy head. In RL, this communication overhead manifests as closed-loop decision latency rather than only per-request inference latency. The proposed approach reduces transmitted data, lowers decision latency in bandwidth-limited settings, and reduces server-side compute per request, whilst achieving broadly comparable learning performance by final return (mean over the final 100 episodes) in single-run benchmarks, with modest trade-offs in mean return. We evaluate across an NVIDIA Jetson Nano, a Raspberry Pi 4B, and a Raspberry Pi Zero 2 W, reporting learning results, on-device execution behaviour under sustained load, and end-to-end decision latency and scalability measurements under bandwidth shaping. Code for training, deployment, and measurement is released as open source.

</details>


### [32] [Information-directed sampling for bandits: a primer](https://arxiv.org/abs/2512.20096)
*Annika Hirling,Giorgio Nicoletti,Antonio Celani*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Multi-Armed Bandit problem provides a fundamental framework for analyzing the tension between exploration and exploitation in sequential learning. This paper explores Information Directed Sampling (IDS) policies, a class of heuristics that balance immediate regret against information gain. We focus on the tractable environment of two-state Bernoulli bandits as a minimal model to rigorously compare heuristic strategies against the optimal policy. We extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter to modulate the decision-making behavior. We examine two specific problem classes: symmetric bandits and the scenario involving one fair coin. In the symmetric case we show that IDS achieves bounded cumulative regret, whereas in the one-fair-coin scenario the IDS policy yields a regret that scales logarithmically with the horizon, in agreement with classical asymptotic lower bounds. This work serves as a pedagogical synthesis, aiming to bridge concepts from reinforcement learning and information theory for an audience of statistical physicists.

</details>


### [33] [Hard Negative Sample-Augmented DPO Post-Training for Small Language Models](https://arxiv.org/abs/2512.19728)
*Haocheng Lu,Minjun Zhu,Henry Yu*

Main category: cs.LG

TL;DR: 提出一个基于六维错误谱的 MathVerifier，结合带权 DPO，在有限算力下提升数学推理，尤其改进数值接近正确但逻辑不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的数学推理评估多以二元正确/错误为导向，使用基于奖励的 RLHF 成本高且难以扩展；需要一个轻量、可解释且可扩展的后训练信号来纠正结构性错误。

Method: 在 MetaMathQA 风格 CoT 数据上进行 SFT，设计 MathVerifier 将候选解分解为六维错误谱，并汇总成可解释的 wrongness 与 absurdity 分数；利用这些信号进行硬负样本挖掘与 per-sample 权重计算，并将其整合到离线直接偏好优化（DPO）的加权目标中。

Result: 在 1.5B 参数的 Qwen2.5 模型上， verifier-guided、加权 DPO 相较于 vanilla SFT 与未加权 DPO，能更有针对性地提升数值接近正确但逻辑不一致的问题的表现，同时避免训练巨量奖励模型或引入外部评判的开销。

Conclusion: 提供一个轻量、可解释且成本友好的后训练管线，聚焦结构性错误，提升推理质量而不显著增加训练负担。

Abstract: Large language models (LLMs) continue to struggle with mathematical reasoning, and common post-training pipelines often reduce each generated solution to a binary outcome: correct or incorrect. This perspective is limiting in practice, as failures in chain-of-thought (CoT) reasoning are frequently structured; solutions may appear convincing while containing subtle logical, algebraic, or numerical flaws. Meanwhile, reinforcement learning from human feedback (RLHF) variants that rely on large reward models or LLM-as-a-judge signals are often expensive, difficult to scale, and unstable to iterate. We propose a lightweight and pragmatic post-training pipeline that targets such structured errors under realistic compute budgets. Starting from supervised fine-tuning (SFT) on MetaMathQA-style CoT data, we introduce a compact MathVerifier that decomposes a candidate solution into a six-dimensional error profile and aggregates it into interpretable wrongness and absurdity scores. These verifier signals serve two roles: (i) mining hard negatives that are near-correct yet structurally flawed, and (ii) defining per-sample importance weights that emphasize the most informative preference pairs. We integrate both into an offline Direct Preference Optimization (DPO) objective via a verifier-guided weighted formulation. Experiments on a 1.5B-parameter Qwen2.5 model show that verifier-guided, weighted DPO yields more targeted improvements than vanilla SFT and unweighted DPO, particularly on problems where solutions are numerically close to correct but logically inconsistent, while avoiding the overhead of training large reward models or relying on external judges.

</details>


### [34] [High-Performance Self-Supervised Learning by Joint Training of Flow Matching](https://arxiv.org/abs/2512.19729)
*Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: FlowFM通过将表征编码器与条件的流匹配生成器协同训练，解耦设计实现高保真生成和强表征学习。相比扩散模型，训练更快、推理更快，且在可穿戴数据SSL任务上超过SOTA，具备显著的推理加速与良好生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在生成质量与判别性能之间的权衡，以及迭代采样带来的高计算和能耗，提升在工业与边缘AI场景下的自监督表示学习效率。

Method: 引入FlowFM，对流式匹配基础上 jointly 训练一个表示编码器与一个条件的流匹配生成器。通过学习更简单的速度场来实现解耦、加速和稳定训练；采用flow matching代替复杂的扩散过程以提升表示学习效率，并在可穿戴传感数据上进行评估。

Result: 在可穿戴数据集上，FlowFM相比扩散方法将训练时间降低约50.4%；在下游任务中超越SSL-Wearables等SOTA方法，在五个数据集上均获优，并实现高达51.0×的推理加速，同时保持高水平的生成质量。

Conclusion: FlowFM通过解耦的表示学习与简化的速度场学习，提升了自监督学习在可穿戴领域的效率与性能，展示了在工业/边缘场景中将生成建模与表示学习结合的潜力，同时提供开源实现。

Abstract: Diffusion models can learn rich representations during data generation, showing potential for Self-Supervised Learning (SSL), but they face a trade-off between generative quality and discriminative performance. Their iterative sampling also incurs substantial computational and energy costs, hindering industrial and edge AI applications. To address these issues, we propose the Flow Matching-based Foundation Model (FlowFM), which jointly trains a representation encoder and a conditional flow matching generator. This decoupled design achieves both high-fidelity generation and effective recognition. By using flow matching to learn a simpler velocity field, FlowFM accelerates and stabilizes training, improving its efficiency for representation learning. Experiments on wearable sensor data show FlowFM reduces training time by 50.4\% compared to a diffusion-based approach. On downstream tasks, FlowFM surpassed the state-of-the-art SSL method (SSL-Wearables) on all five datasets while achieving up to a 51.0x inference speedup and maintaining high generative quality. The implementation code is available at https://github.com/Okita-Laboratory/jointOptimizationFlowMatching.

</details>


### [35] [ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures](https://arxiv.org/abs/2512.19730)
*Zhonghao Yang,Cheng Luo,Daojing He,Yiming Li,Yu Li*

Main category: cs.LG

TL;DR: 提出 ArcGen，面向黑盒神经后门检测的架构不变特征提取方法，通过在特征提取函数中加入对齐层和两种对齐损失，使同样后门行为但不同架构的特征在分布和样本层面实现对齐，从而提升对未见架构的检测泛化，在16,896个模型的大规模评估中实现高达42.5%的AUC提升。代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的神经后门检测方法在看到的模型架构之外未见架构时的泛化性差，需获得对架构鲁棒的特征以实现有效检测。

Method: 在黑盒场景中从目标模型提取特征，增加一个对齐层以处理输出特征，使特征对架构信息的直接影响减少。设计两种对齐损失，强制同一后门行为但架构不同的模型在分布层面和样本层面上的特征对齐。

Result: 在未见架构上实现高达42.5%的检测性能提升（如AUC），并在16,896个模型、多数据集和多种后门攻击下进行大规模评估。

Conclusion: 通过显式特征对齐实现架构不变性，提升黑盒后门检测的泛化能力，ArcGen 为实用化后门防御提供了可公开的实现。

Abstract: Backdoor attacks pose a significant threat to the security and reliability of deep learning models. To mitigate such attacks, one promising approach is to learn to extract features from the target model and use these features for backdoor detection. However, we discover that existing learning-based neural backdoor detection methods do not generalize well to new architectures not seen during the learning phase. In this paper, we analyze the root cause of this issue and propose a novel black-box neural backdoor detection method called ArcGen. Our method aims to obtain architecture-invariant model features, i.e., aligned features, for effective backdoor detection. Specifically, in contrast to existing methods directly using model outputs as model features, we introduce an additional alignment layer in the feature extraction function to further process these features. This reduces the direct influence of architecture information on the features. Then, we design two alignment losses to train the feature extraction function. These losses explicitly require that features from models with similar backdoor behaviors but different architectures are aligned at both the distribution and sample levels. With these techniques, our method demonstrates up to 42.5% improvements in detection performance (e.g., AUC) on unseen model architectures. This is based on a large-scale evaluation involving 16,896 models trained on diverse datasets, subjected to various backdoor attacks, and utilizing different model architectures. Our code is available at https://github.com/SeRAlab/ArcGen.

</details>


### [36] [Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis](https://arxiv.org/abs/2512.19970)
*Surya Jayakumar,Kieran Sullivan,John McLaughlin,Christine O'Meara,Indrakshi Dey*

Main category: cs.LG

TL;DR: 提出一个数据驱动框架，首次在 county 级应用 STGNN 预测跨年度的综合可持续性指数，结合 VAE、PCA 与 pillar 基准量表，预测 2026-2030 年的多年度指标。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀疏和区域尺度可持续性评估的挑战；通过在地理、时序和多维指标间建模，提升对未来政策和资源分配的预测能力。

Method: 建立端到端数据管线，利用 Variational Autoencoder 来增强 ICBF 数据集并保持联合分布；使用基于 PCA 的 pillar-based 评分构造四个维度并加权形成综合指数；提出新型 STGNN 架构来编码地理依赖和非线性时间动态，进行 2026-2030 的多年预测。

Result: 实现了 counties 规模的首次 STGNN 应用，能对四个维度构筑的综合指标进行多年度预测，展示了对未来趋势的捕捉能力和方法的可扩展性。

Conclusion: 该框架结合 VAE、PCA 与 STGNN，为区域农业可持续性评估提供了一个新路径，具有较高的政策相关性和实用性，但需进一步评估在不同数据 sparsity 情况下的鲁棒性和对外部干扰的敏感性。

Abstract: This study introduces a novel data-driven framework and the first-ever county-scale application of Spatio-Temporal Graph Neural Networks (STGNN) to forecast composite sustainability indices from herd-level operational records. The methodology employs a novel, end-to-end pipeline utilizing a Variational Autoencoder (VAE) to augment Irish Cattle Breeding Federation (ICBF) datasets, preserving joint distributions while mitigating sparsity. A first-ever pillar-based scoring formulation is derived via Principal Component Analysis, identifying Reproductive Efficiency, Genetic Management, Herd Health, and Herd Management, to construct weighted composite indices. These indices are modelled using a novel STGNN architecture that explicitly encodes geographic dependencies and non-linear temporal dynamics to generate multi-year forecasts for 2026-2030.

</details>


### [37] [An Optimal Policy for Learning Controllable Dynamics by Exploration](https://arxiv.org/abs/2512.20053)
*Peter N. Loxley*

Main category: cs.LG

TL;DR: 提出了一种在未知环境下学习可控动力学的最优策略框架，通过在有限时间内进行贪婪的信息增益探索来实现对系统的学习与控制。策略简单、可实现，且控制集合随探索阶段进行参数化的变化。


<details>
  <summary>Details</summary>
Motivation: 解决在未知动力学下通过探索来学习最优策略的问题，尤其是在存在约束性状态（瞬态、吸收、不可回退状态）使得控制具有非平稳特性时，如何设计可行且高效的探索策略。

Method: 给出可控马尔可夫链的一般最优策略形式；设计一个随探索时间改变的约束控制集合的简单参数化；提出用于在该集合内贪婪寻求信息增益的最优策略算法；并结合六个具体示例来分析策略的效果；利用计数性论证和动态规划的序列改进性来证明策略的最优性。

Result: 得到一个实现简单、计算高效的学习探索策略，能够在未知环境中通过“学习即探索”实现对可控动力学的学习与控制；通过六个示例验证策略的有效性，并通过计数性论证和序列性改进性证明其近似最优性。

Conclusion: 在存在限制性状态时，非平稳策略对实现最优探索是必需的；该框架提供了一个实用的学习可控动力学的方法，并通过六个实例证明了其可行性和有效性。

Abstract: Controllable Markov chains describe the dynamics of sequential decision making tasks and are the central component in optimal control and reinforcement learning. In this work, we give the general form of an optimal policy for learning controllable dynamics in an unknown environment by exploring over a limited time horizon. This policy is simple to implement and efficient to compute, and allows an agent to ``learn by exploring" as it maximizes its information gain in a greedy fashion by selecting controls from a constraint set that changes over time during exploration. We give a simple parameterization for the set of controls, and present an algorithm for finding an optimal policy. The reason for this policy is due to the existence of certain types of states that restrict control of the dynamics; such as transient states, absorbing states, and non-backtracking states. We show why the occurrence of these states makes a non-stationary policy essential for achieving optimal exploration. Six interesting examples of controllable dynamics are treated in detail. Policy optimality is demonstrated using counting arguments, comparing with suboptimal policies, and by making use of a sequential improvement property from dynamic programming.

</details>


### [38] [Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis](https://arxiv.org/abs/2512.19732)
*Gaurav Kumar Sharma*

Main category: cs.LG

TL;DR: 通过系统性分析JARVIS-DFT带隙数据集，去除可能无意间编码带结构信息的描述符（如有效质量），得到2280个材料的泄漏受控子集；在此数据集上实现三阶段建模框架，逐步加入基础物理描述符、 engineered 特征和组成特征；树模型在三个阶段的R^2约为0.88–0.90，扩展描述符空间并未显著提升预测能力；SHAP分析显示介电张量分量为主导贡献；提供泄漏感知的带隙预测基线数据集与性能指标。


<details>
  <summary>Details</summary>
Motivation: 解决ML带隙预测中潜在的带结构信息泄漏问题，确保模型泛化而非仅记忆带结构特征；构建一个经过去泄漏处理的基准数据集以供后续研究使用。

Method: 建立三阶段建模框架：1) 引入基本物理描述符；2) 增添 engineered 特征；3) 引入组分属性；在每阶段通过去除会泄漏带结构信息的描述符来控制泄漏；采用树模型并结合SHAP分析以理解特征贡献。

Result: 在三个阶段上，树模型的R^2约为0.88–0.90，扩展描述符空间并未显著提高预测性能；SHAP显示介电张量分量为最主要的特征贡献；最终提供2280材料的泄漏受控数据集及基线性能。

Conclusion: 提出一个泄漏感知的带隙预测基线数据集和评估框架，表明在控制信息泄漏后，增加描述符并不带来明显收益，介电张量成为关键特征；为未来的泄漏意识研究提供可重复的基线。

Abstract: In this study, we perform a systematic analysis of the JARVIS-DFT bandgap dataset and identify and remove descriptors that may inadvertently encode band-structure information, such as effective masses. This process yields a curated, leakage-controlled subset of 2280 materials. Using this dataset, a three-phase modeling framework is implemented that incrementally incorporates basic physical descriptors, engineered features, and compositional attributes. The results show that tree-based models achieve R2 values of approximately 0.88 to 0.90 across all phases, indicating that expanding the descriptor space does not substantially improve predictive accuracy when leakage is controlled. SHAP analysis consistently identifies the dielectric tensor components as the dominant contributors. This work provides a curated dataset and baseline performance metrics for future leakage-aware bandgap prediction studies.

</details>


### [39] [Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction](https://arxiv.org/abs/2512.19735)
*Gangxiong Zhang,Yongchao Long*

Main category: cs.LG

TL;DR: CAP 提供了一种训练无关的提示框架，通过案例推理纠偏，在 ICU 死亡率预测中实现公平性与性能并提升。


<details>
  <summary>Details</summary>
Motivation: 解决 LLM 在结构化医疗数据中对性别、年龄、种族等敏感属性的偏见，以及在不牺牲性能的前提下实现公平性的挑战。

Method: 开发多维偏差评估方案诊断模型，提出 CAse Prompting (CAP) 框架，将传统去偏提示与基于历史案例的推理结合，通过在推理阶段学习历史误预测及其正确结果来纠偏；无需重新训练。

Result: 在 MIMIC-IV 上，AUROC 从 0.806 提升至 0.873，AUPRC 从 0.497 提升至 0.694，同时性别与种族相关差异减少超过 90%，特征注意力在不同人口群体间高度一致（相似度>0.98）。

Conclusion: 提示驱动的去偏方法可以有效地在不训练模型的情况下实现公平性与性能的联合提升，提供可推广的公平临床决策支持范式。

Abstract: Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we systematically examine bias in LLM-based ICU mortality prediction and propose a training-free, clinically adaptive prompting framework to simultaneously improve fairness and performance. We first develop a multi-dimensional bias assessment scheme for comprehensive model diagnosis. Building on this analysis, we introduce CAse Prompting (CAP), a novel prompting framework that integrates conventional debiasing prompts with case-based reasoning. CAP guides the model to learn from similar historical misprediction cases and their correct outcomes, enabling correction of biased reasoning patterns. Experiments on the MIMIC-IV dataset show that CAP substantially improves both predictive accuracy and fairness. CAP increases AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis further indicates highly consistent attention patterns across demographic groups, with similarity scores exceeding 0.98. These results demonstrate that LLMs exhibit measurable bias in ICU mortality prediction, and that a carefully designed prompting framework can effectively co-optimize fairness and performance without retraining, offering a transferable paradigm for equitable clinical decision support.

</details>


### [40] [OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting](https://arxiv.org/abs/2512.19738)
*Wilson Fung,Lu Guo,Drake Hilliard,Alessandro Casadei,Raj Ratan,Sreyoshi Bhaduri,Adi Surve,Nikhil Agarwal,Rohit Malshe,Pavan Mullapudi,Hungjen Wang,Saurabh Doodhwala,Ankush Pole,Arkajit Rakshit*

Main category: cs.LG

TL;DR: OpComm 集成了监督预测、基于强化学习的缓冲区控制以及生成式 AI 报告，用于提升末端派送站点的需求预测与决策支持。


<details>
  <summary>Details</summary>
Motivation: 通过更高的预测准确性与缓冲策略，减少未满足需求的风险与资源浪费，提高最后一公里的运营效率。

Method: 使用 LightGBM 回归进行站点级需求预测，为 PPO 智能体提供上下文；PPO 在离散动作集合中选择缓冲水平；奖励函数更惩罚欠缓冲以权衡未满足需求风险与资源低效；通过蒙特卡洛更新实现策略持续自适应；引入 SHAP 解释性特征归因的生成式 AI 层提供高层摘要与情景分析。

Result: 在400+个站点中，WAPE 相对于手工预测降低了 21.65%；降低了欠缓冲事件并提升决策透明度。

Conclusion: 将上下文强化学习与预测建模结合，能够解决运营预测挑战，并在高风险的物流环境中实现统计严谨性与实际决策之间的桥接。

Abstract: Accurate forecasting of package volumes at delivery stations is critical for last-mile logistics, where errors lead to inefficient resource allocation, higher costs, and delivery delays. We propose OpComm, a forecasting and decision-support framework that combines supervised learning with reinforcement learning-based buffer control and a generative AI-driven communication module. A LightGBM regression model generates station-level demand forecasts, which serve as context for a Proximal Policy Optimization (PPO) agent that selects buffer levels from a discrete action set. The reward function penalizes under-buffering more heavily than over-buffering, reflecting real-world trade-offs between unmet demand risks and resource inefficiency. Station outcomes are fed back through a Monte Carlo update mechanism, enabling continual policy adaptation. To enhance interpretability, a generative AI layer produces executive-level summaries and scenario analyses grounded in SHAP-based feature attributions. Across 400+ stations, OpComm reduced Weighted Absolute Percentage Error (WAPE) by 21.65% compared to manual forecasts, while lowering under-buffering incidents and improving transparency for decision-makers. This work shows how contextual reinforcement learning, coupled with predictive modeling, can address operational forecasting challenges and bridge statistical rigor with practical decision-making in high-stakes logistics environments.

</details>


### [41] [Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics](https://arxiv.org/abs/2512.19740)
*Kousar Raza,Faizan Ali*

Main category: cs.LG

TL;DR: 公开的、61变量覆盖19场比赛的2025亚洲杯T20数据集，为体育分析、预测建模与策略决策提供基准。


<details>
  <summary>Details</summary>
Motivation: 提供一个开放、机器可读、可复现的数据基准，以推动板球 analytics 研究的发展、方法比较和跨研究复用。

Method: 整理并收集比赛记录，涵盖球队得分、出局、战力、边界、投掷决策、场地等61个变量，辅以探索性数据分析来揭示球队表现、边界分布与得分模式。数据以 CC-BY 4.0 授权在 Zenodo 发布，确保可重复性。

Result: 实现一个可公开访问的基准数据集，展示对球队表现、边界分布与得分模式的初步分析，并演示其在数据驱动的板球分析与预测建模中的应用潜力。

Conclusion: 该工作提供一个开源、机器可读的基准数据集，促进板球分析研究的再现性、方法评估与新研究方向的探索。

Abstract: This paper presents a structured and comprehensive dataset corresponding to the 2025 Asia Cup T20 cricket tournament, designed to facilitate data-driven research in sports analytics. The dataset comprises records from all 19 matches of the tournament and includes 61 variables covering team scores, wickets, powerplay statistics, boundary counts, toss decisions, venues, and player-specific highlights. To demonstrate its analytical value, we conduct an exploratory data analysis focusing on team performance indicators, boundary distributions, and scoring patterns. The dataset is publicly released through Zenodo under a CC-BY 4.0 license to support reproducibility and further research in cricket analytics, predictive modeling, and strategic decision-making. This work contributes an open, machine-readable benchmark dataset for advancing cricket analytics research.

</details>


### [42] [EdgeFlex-Transformer: Transformer Inference for Edge Devices](https://arxiv.org/abs/2512.19741)
*Shoaib Mohammad,Guanqun Song,Ting Zhu*

Main category: cs.LG

TL;DR: Proposes a lightweight multi-stage optimization pipeline to compress and accelerate Vision Transformers for edge deployment, combining activation profiling, memory-aware pruning, selective mixed-precision execution, and activation-aware quantization (AWQ) to reduce memory and latency with minimal accuracy loss, demonstrated on ViT-Huge with CIFAR-10.


<details>
  <summary>Details</summary>
Motivation: Edge devices impose strict limits on memory, compute, and latency for transformer models. There is a need for practical, retraining-free compression and acceleration pipelines to enable ViTs on resource-constrained platforms.

Method: A four-stage pipeline: (1) activation profiling via forward hooks to identify low-importance channels; (2) structured pruning of MLP layers under a target memory budget; (3) FP16 conversion for selected components; (4) activation-aware quantization (AWQ) to quantize weights and activations to INT8. The approach starts from a ViT-Huge backbone (~632M params) and aims to compress with minimal accuracy loss, without task-specific fine-tuning.

Result: On CIFAR-10, achieved ~76% reduction in peak memory usage and >6x latency reduction while maintaining or improving accuracy versus the FP32 baseline.

Conclusion: The framework offers a practical path toward efficient transformer inference on edge platforms and suggests directions for future work, such as incorporating dynamic sparsity and Mixture-of-Experts (MoE) architectures to scale performance across tasks.

Abstract: Deploying large-scale transformer models on edge devices presents significant challenges due to strict constraints on memory, compute, and latency. In this work, we propose a lightweight yet effective multi-stage optimization pipeline designed to compress and accelerate Vision Transformers (ViTs) for deployment in resource-constrained environments. Our methodology combines activation profiling, memory-aware pruning, selective mixed-precision execution, and activation-aware quantization (AWQ) to reduce the model's memory footprint without requiring costly retraining or task-specific fine-tuning. Starting from a ViT-Huge backbone with 632 million parameters, we first identify low-importance channels using activation statistics collected via forward hooks, followed by structured pruning to shrink the MLP layers under a target memory budget. We further apply FP16 conversion to selected components and leverage AWQ to quantize the remaining model weights and activations to INT8 with minimal accuracy degradation. Our experiments on CIFAR-10 demonstrate that the fully optimized model achieves a 76% reduction in peak memory usage and over 6x lower latency, while retaining or even improving accuracy compared to the original FP32 baseline. This framework offers a practical path toward efficient transformer inference on edge platforms, and opens future avenues for integrating dynamic sparsity and Mixture-of-Experts (MoE) architectures to further scale performance across diverse tasks.

</details>


### [43] [From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning](https://arxiv.org/abs/2512.19743)
*Sasan Sharifipour,Constantino Álvarez Casado,Manuel Lage Cañellas,Miguel Bordallo López*

Main category: cs.LG

TL;DR: CUDA-APML: 一个稀疏GPU实现的APML，用近线性内存扩展在保持与密集APML相近精度的同时显著降低峰值显存。


<details>
  <summary>Details</summary>
Motivation: 解决APML在点云学习中的内存与计算瓶颈。 dense APML 的传输近似需要大显存，且二次复杂度限制扩展性；通过稀疏化和GPU实现提升可扩展性，同时保持梯度传递。

Method: 提出CUDA-APML：在COO表示下对可忽略的分配进行阈值化，使用自适应softmax、双向对称化与Sinkhorn规范化；实现稀疏版本的近似运输，内存实现接近线性，梯度保留在存储的支持集合上，配对距离仍为二次。

Result: 在ShapeNet和MM-Fi数据集上，CUDA-APML在数值上与密集APML相当，且峰值显存降低约99.9%；实现了更低内存开销的近似运输，同时保留梯度。代码公开。

Conclusion: 该工作展示了稀疏化和GPU实现可以在保持接近密集APML精度的前提下，大幅降低内存消耗，使得基于点云的可微分运输损失更具可扩展性。

Abstract: Loss functions are fundamental to learning accurate 3D point cloud models, yet common choices trade geometric fidelity for computational cost. Chamfer Distance is efficient but permits many-to-one correspondences, while Earth Mover Distance better reflects one-to-one transport at high computational cost. APML approximates transport with differentiable Sinkhorn iterations and an analytically derived temperature, but its dense formulation scales quadratically in memory. We present CUDA-APML, a sparse GPU implementation that thresholds negligible assignments and runs adaptive softmax, bidirectional symmetrization, and Sinkhorn normalization directly in COO form. This yields near-linear memory scaling and preserves gradients on the stored support, while pairwise distance evaluation remains quadratic in the current implementation. On ShapeNet and MM-Fi, CUDA-APML matches dense APML within a small tolerance while reducing peak GPU memory by 99.9%. Code available at: https://github.com/Multimodal-Sensing-Lab/apml

</details>


### [44] [A K-Means, Ward and DBSCAN repeatability study](https://arxiv.org/abs/2512.19772)
*Anthony Bertrand,Engelbert Mephu Nguifo,Violaine Antoine,David Hill*

Main category: cs.LG

TL;DR: 摘要聚焦可重复性在机器学习中的重要性，分解聚类算法（K-Means、DBSCAN、Ward）在逐步环节的可重复性条件，并以 scikit-learn 为例评估；结果在 K-Means 当 OpenMP 线程>2 时存在非确定性现象，强调需进一步调查与修正。


<details>
  <summary>Details</summary>
Motivation: 可重复性是科学研究和调试的基石；在实际实现中，算法的并行化、随机性、以及底层库优化可能破坏逐步可重复性，需明确在各阶段达到精确重复的条件。

Method: 将 K-Means、DBSCAN、Ward 拆解为基本步骤，逐步分析每一步的可重复性条件；使用 scikit-learn 的实现进行实例分析，改变并行度（OpenMP 线程数）以检验位级一致性。

Result: 在 K-Means 当 OpenMP 线程数超过两时，出现难以再现的结果差异，表明并行实现中的非确定性源可能来自初始化、收敛顺序、或并行计算中的竞态条件；DBSCAN 与 Ward 在所测试条件下未报告同等级别的不可重复性（需原文确认）。

Conclusion: 强调面向开发者和用户提高对可重复性的认识，呼吁在聚类算法实现中提供更明确的确定性配置和测试，以便定位并修复导致位级不一致的根源。

Abstract: Reproducibility is essential in machine learning because it ensures that a model or experiment yields the same scientific conclusion. For specific algorithms repeatability with bitwise identical results is also a key for scientific integrity because it allows debugging. We decomposed several very popular clustering algorithms: K-Means, DBSCAN and Ward into their fundamental steps, and we identify the conditions required to achieve repeatability at each stage. We use an implementation example with the Python library scikit-learn to examine the repeatable aspects of each method. Our results reveal inconsistent results with K-Means when the number of OpenMP threads exceeds two. This work aims to raise awareness of this issue among both users and developers, encouraging further investigation and potential fixes.

</details>


### [45] [Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy](https://arxiv.org/abs/2512.19805)
*Deepit Sapru*

Main category: cs.LG

TL;DR: 提出一种营销决策框架：将异质性治疗提升（ uplift ）转化为受约束的定向策略，以在预算和销售损失等护栏内最大化收益与留存，并在离线和线上评估中实现优于基线的表现。


<details>
  <summary>Details</summary>
Motivation: 在数字营销中需要对不同用户的因果效应进行识别并在资源受限的情况下进行精准投放，以提升关键商业指标（如收入、留存、用户体验）。

Method: 先用 uplift 学习器估计条件平均处理效应（CATE），再解一个带约束的分配问题，确定目标人群和相应的优惠方案，遵守预算、可接受的销售下降等护栏。应用于留存信息、事件奖励和消费阈值分配等场景。

Result: 在离线评估中使用 uplift AUC、IPS、SNIPS 等指标，稳定优于 propensity 与静态基线；在生产规模的在线 A/B 测试中验证对收入与完成率的提升，同时保持客户体验约束。

Conclusion: 为市场营销人员提供一个可重复使用的风控驱动因果定向实战手册，使之能够在规模化场景中落地、设定 guardrails，并将活动与关键业绩指标对齐。

Abstract: This paper introduces a marketing decision framework that converts heterogeneous-treatment uplift into constrained targeting strategies to maximize revenue and retention while honoring business guardrails. The approach estimates Conditional Average Treatment Effects (CATE) with uplift learners and then solves a constrained allocation to decide who to target and which offer to deploy under limits such as budget or acceptable sales deterioration. Applied to retention messaging, event rewards, and spend-threshold assignment, the framework consistently outperforms propensity and static baselines in offline evaluations using uplift AUC, Inverse Propensity Scoring (IPS), and Self-Normalized IPS (SNIPS). A production-scale online A/B test further validates strategic lift on revenue and completion while preserving customer-experience constraints. The result is a reusable playbook for marketers to operationalize causal targeting at scale, set guardrails, and align campaigns with strategic KPIs.

</details>


### [46] [Fine-Tuned In-Context Learners for Efficient Adaptation](https://arxiv.org/abs/2512.19879)
*Jorg Bornschein,Clare Lyle,Yazhe Li,Amal Rannen-Triki,Xu Owen He,Razvan Pascanu*

Main category: cs.LG

TL;DR: 提出一种统一的LLM适配方法，将在-context学习以提示的结构融入任务特定微调，并通过前验评估进行低数据下的超参数选择，实证显示该方法在多任务上可显著优于单纯的微调或提示学习基线。


<details>
  <summary>Details</summary>
Motivation: 解决提示式方法在数据增多时性能趋于瓶颈，以及微调在数据稀缺时表现不足的问题，追求在样本效率与可扩展性之间的折中。

Method: 在任务特定数据上进行微调的同时，将包含in-context示例的一致结构作为训练数据的一部分，模拟k-shot提示的形式，从而实现“提示学习+微调”的统一方法；每个任务需要单独微调。为超参数选择引入前验评估（prequential evaluation），在训练期间利用所有可用数据并提供稳健的验证信号。

Result: 大量实验表明，所提统一方法在预测性能上稳定达到或显著超过基线中的微调与仅提示学习的表现。

Conclusion: 将in-context提示结构融入微调，兼具样本效率与性能提升；前验评估在低数据情境下是有效的超参数选择工具。

Abstract: When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model's inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model's parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.

</details>


### [47] [Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling](https://arxiv.org/abs/2512.19905)
*Indranil Halder,Cengiz Pehlevan*

Main category: cs.LG

TL;DR: 提出一个可解析的推理时成本尺度模型：贝叶斯线性回归配合奖励加权采样，得到高维极限下的后验预测与泛化误差的闭式表达，揭示在最佳-k和温度下推理时间采样对泛化的影响及其边界。


<details>
  <summary>Details</summary>
Motivation: 理解将计算资源从训练阶段转移到推理阶段所带来的性能提升背后的原理，提供一个分析性、可控的推理时尺度模型。

Method: 使用带奖励的贝叶斯线性回归与 softmax 选择、二次奖励、温度参数，在高维极限下推导后验预测均值与方差的确定式；假设训练数据来自教师模型；在最佳-k极限下应用极值理论推导泛化误差的下降速率。

Result: 若奖励与教师接近，泛化误差随推理样本数k单调下降；奖励若严重失配，则存在有限的最优k，超过该k后继续采样会提升误差。对于给定k，存在最优采样温度。最佳-k极限下误差收敛为Θ(1/k^2)，且系数由极值理论给出。实验在大语言模型-评审场景中验证。困难任务下，推理时计算的优势减弱。

Conclusion: 本工作界定了推理时计算规模在何种情形优于增加训练数据，并给出明确的定量边界与条件，强调奖励设定的敏感性以及任务难度对推理时计算优势的影响。

Abstract: Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tractable model of inference-time scaling: Bayesian linear regression with a reward-weighted sampler, where the reward is determined from a linear model, modeling LLM-as-a-judge scenario. We study this problem in the high-dimensional regime, where the deterministic equivalents dictate a closed-form expression for the posterior predictive mean and variance. We analyze the generalization error when training data are sampled from a teacher model. We draw $k$ inference-time samples and select via softmax at a temperature applied to a quadratic reward. When the reward is not too different from the teacher, the generalization error decreases monotonically with increasing inference time samples $k$. However, the specific reward that optimizes inference-time selection generally differs from the teacher. In contrast, substantial reward misspecification induces a finite optimal $k$ beyond which more sampling can increase the generalization error. For fixed $k$, there exists an optimal sampling temperature. We experimentally verify these facts in large language model inference with an additional large language model as a judge. In the "best-of-$k$" limit with the teacher as reward, we theoretically show that the generalization error decays as $Θ(1/k^2)$ and determine the leading coefficient via extreme value theory. These formulas delineate domains where scaling inference-time computation is provably preferable to collecting more data. Finally, we demonstrate that when task difficulty increases, the previously mentioned advantage of inference-time compute degrades.

</details>


### [48] [Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra](https://arxiv.org/abs/2512.19909)
*Maxime Lacour,Pu Ren,Rie Nakata,Nori Nakata,Michael Mahoney*

Main category: cs.LG

TL;DR: CGM-FAS，基于条件变分自编码器的深度学习方法，用于对傅里叶振幅谱中的非遍历性路径效应建模，替代GP方法，具备无须预设相关函数、捕捉跨频相关、快速大尺度预测的优势，并在旧金山湾区数据中与GP-GMM对比表现一致。


<details>
  <summary>Details</summary>
Motivation: 现有非遍历GMM多以高斯过程（GP）为基础，但对大尺度预测存在计算与内存瓶颈，亟需无须显式相关函数的高效替代方案以实现更快的多频率、大空间预测。

Method: 采用条件变分自编码器（CVAE）架构来学习地理坐标（地震与观测站）作为条件变量的时空模式和跨频相关。以旧金山湾区地震数据进行训练与评估，并与基于GP的GMM进行对比。

Result: 与GP基GMM结果一致地预测非遍历路径效应；在将近规模上实现快速预测：可对1万站点、1,000个频率生成地图，耗时约10秒，内存占用为数GB级别；超参数可调以使生成的路径效应变异性与GP-GMM一致。

Conclusion: 深度学习方法为高效、可扩展的非遍历性GMM提供了有力替代，避免预设严格的相关函数，能直接学习跨频相关，适合大尺度多频域地震风险评估，前景良好。

Abstract: Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.

</details>


### [49] [Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning](https://arxiv.org/abs/2512.19920)
*Jiayun Wu,Jiashuo Liu,Zhiyuan Zeng,Tianyang Zhan,Wenhao Huang*

Main category: cs.LG

TL;DR: 提出行为校准以抑制模型幻觉，强调在不确定时选择退出或标注，并通过严格的评分规则优化输出正确性概率的校准。实验表明小模型在不确定性量化方面可超越前沿模型。


<details>
  <summary>Details</summary>
Motivation: 在关键领域的LLM幻觉问题源于以仿真数据分布为目标的训练目标；需要通过校准来让模型的输出概率更接近真实正确性，从而提升可信度。

Method: 综合近来进展，设计训练干预，优化严格的正确性评分规则使模型输出经校准的正确性概率；允许 abstain 或标注不确定的断言；在 Qwen3-4B-Instruct 上进行实验，评估于 math 推理任务 BeyondAIME 与跨领域 SimpleQA。

Result: 行为校准的强化学习使较小模型在不确定性量化方面超越部分前沿模型；4B 规模在与 Grok-4、Gemini-2.5-Pro 等对比中实现零-shot 校准误差；在 BeyondAIME 的在域评估中，4B 模型的 Accuracy-to-Hallucination Ratio 对数提升为 0.806，超过 GPT-5 的 0.207；但在事实准确性方面仍低于前沿模型。

Conclusion: 以校准为核心的训练范式可获得可迁移的元技能，与原始预测准确度解耦；模型可通过退出或标注来提升可靠性，适用于需要可信回答的场景。

Abstract: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.

</details>


### [50] [The Seismic Wavefield Common Task Framework](https://arxiv.org/abs/2512.19927)
*Alexey Yermakov,Yue Zhao,Marine Denolle,Yiyu Ni,Philippe M. Wyder,Judah Goldfeder,Stefano Riva,Jan Williams,David Zoro,Amy Sara Rude,Matteo Tomasetto,Joe Germany,Joseph Bakarji,Georg Maierhofer,Miles Cranmer,J. Nathan Kutz*

Main category: cs.LG

TL;DR: 提出一个通用任务框架CTF，用于地震波场的ML评估，覆盖全球/地壳/局部尺度的数据集以及预测、重构、泛化任务，旨在提升评估的公正性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 解决地震学在状态预测与重构方面的核心挑战，以及源位置、机制与地球模型参数化变异性的复杂性；现实数据受限于不完全的地球模型和稀疏传感，模拟计算成本高昂，现有ML进展缺乏规范的评估与可比性。

Method: 提出Common Task Framework (CTF)，集合多尺度数据集与针对具体任务的评估指标，提供头对头的算法评估流程，并展示两组数据集在重构地震波场方面的初步评分。

Result: 给出在两个数据集上的方法评估分数，比较多种方法与基础模型在从观测/仿真数据中重构地震波场方面的优势与局限。

Conclusion: 通过在隐藏测试集上进行标准化评估，提升科学ML的严格性与可重复性，促使评估从零散对比走向系统化、可重复的基准评估。

Abstract: Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth's complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.

</details>


### [51] [LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2512.20002)
*Jiacheng You,Jingcheng Yang,Yuhang Xie,Zhongxuan Wu,Xiucheng Li,Feng Li,Pengjie Wang,Jian Xu,Bo Zheng,Xinyang Chen*

Main category: cs.LG

TL;DR: LoFT-LLM proposes a frequency-aware forecasting pipeline that combines Patch Low-Frequency Forecasting Module (PLFM) for stable low-frequency trends, a residual learner for high-frequency variations, and a fine-tuned large language model (LLM) to incorporate auxiliary domain knowledge, achieving superior performance in finance and energy forecasting under full-data and few-shot settings.


<details>
  <summary>Details</summary>
Motivation: Real-world time-series forecasting is hindered by limited data and noisy dynamics. Full-window supervision tends to amplify high-frequency noise and obscure long-term trends, while auxiliary domain information is underutilized, especially in few-shot regimes. There is a need for methods that separate frequency information and leverage semantic calibration from LLMs to improve accuracy, robustness, and interpretability.

Method: 1) Patch Low-Frequency Forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. 2) A residual learner models high-frequency variations on top of the low-frequency base. 3) A fine-tuned LLM refines predictions through structured natural language prompts that inject auxiliary context and domain knowledge.

Result: Empirical evaluation on financial and energy datasets shows that LoFT-LLM significantly outperforms strong baselines in both full-data and few-shot regimes, delivering improved accuracy, robustness, and interpretability.

Conclusion: The frequency-aware, semantically calibrated forecasting pipeline effectively combines frequency decomposition with LLM-based guidance, offering a promising approach for data-scarce, noisy time-series forecasting with practical applicability across finance and energy domains.

Abstract: Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.

</details>


### [52] [Control Variate Score Matching for Diffusion Models](https://arxiv.org/abs/2512.20003)
*Khaled Kahouli,Romuald Elie,Klaus-Robert Müller,Quentin Berthet,Oliver T. Unke,Arnaud Doucet*

Main category: cs.LG

TL;DR: 提出控制变差分数恒等式CVSI，将DSI与TSI在噪声谱上各自的方差不足统一，通过时间依赖的最优控制系数实现对方差的最小化，在数据自由采样和推理阶段均显著提高样本效率。


<details>
  <summary>Details</summary>
Motivation: DSI在低噪声区域方差较高，TSI在高噪声区域方差较高；两者各有局限。需要一个在整个噪声范围内都具低方差的估计量来提高扩散模型的数据采样和学习效率。

Method: 提出控制变差分数恒等式CVSI，推导出一个随时间变化的最优控制系数，使方差在整个噪声谱上达到最小化；将DSI与TSI统一为一个可插入的估计量。

Result: CVSI作为鲁棒的低方差插件估计量，在数据无关采样器学习和推理时的扩散采样中显著提升样本效率。

Conclusion: CVSI提供一个在不同噪声等级下都具最优方差属性的系统性估计框架，具有良好的理论支撑和广泛的应用潜力。

Abstract: Diffusion models offer a robust framework for sampling from unnormalized probability densities, which requires accurately estimating the score of the noise-perturbed target distribution. While the standard Denoising Score Identity (DSI) relies on data samples, access to the target energy function enables an alternative formulation via the Target Score Identity (TSI). However, these estimators face a fundamental variance trade-off: DSI exhibits high variance in low-noise regimes, whereas TSI suffers from high variance at high noise levels. In this work, we reconcile these approaches by unifying both estimators within the principled framework of control variates. We introduce the Control Variate Score Identity (CVSI), deriving an optimal, time-dependent control coefficient that theoretically guarantees variance minimization across the entire noise spectrum. We demonstrate that CVSI serves as a robust, low-variance plug-in estimator that significantly enhances sample efficiency in both data-free sampler learning and inference-time diffusion sampling.

</details>


### [53] [Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance](https://arxiv.org/abs/2512.20006)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: 提出OGAB激活函数，通过正交性和分组偏置学习来缓解类别不平衡，在嵌入学习阶段直接处理不平衡，提升少数类的区分性。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡仍然严重影响深度学习分类性能；现有方法多依赖数据预处理或后处理，本文尝试通过激活函数引入强 inductive bias，提升嵌入层的判别性且无需显式监督。

Method: 设计OGAB激活函数，结合正交变换以保持 minority 类信息的独立性并防止多数类在嵌入空间主导；引入组感知偏置机制，自动识别数据簇并调整嵌入以增强类别可分性，训练阶段直接嵌入学习中实现，无需额外监督。

Result: 在真实世界和合成的不平衡数据集上，OGAB在多项指标上持续优于传统激活函数和可学习激活函数，显示对不平衡数据的鲁棒性与提升。

Conclusion: 表明激活函数可带来显著的先验信息以应对复杂数据问题；OGAB通过正交性与分组偏置在嵌入学习层实现对 minority 的保护与增强，易于与现有学习流程集成，且无额外标签需求。

Abstract: Class imbalance is a common challenge in machine learning and data mining, often leading to suboptimal performance in classifiers. While deep learning excels in feature extraction, its performance still deteriorates under imbalanced data. In this work, we propose a novel activation function, named OGAB, designed to alleviate class imbalance in deep learning classifiers. OGAB incorporates orthogonality and group-aware bias learning to enhance feature distinguishability in imbalanced scenarios without explicitly requiring label information. Our key insight is that activation functions can be used to introduce strong inductive biases that can address complex data challenges beyond traditional non-linearity. Our work demonstrates that orthogonal transformations can preserve information about minority classes by maintaining feature independence, thereby preventing the dominance of majority classes in the embedding space. Further, the proposed group-aware bias mechanism automatically identifies data clusters and adjusts embeddings to enhance class separability without the need for explicit supervision. Unlike existing approaches that address class imbalance through preprocessing data modifications or post-processing corrections, our proposed approach tackles class imbalance during the training phase at the embedding learning level, enabling direct integration with the learning process. We demonstrate the effectiveness of our solution on both real-world and synthetic imbalanced datasets, showing consistent performance improvements over both traditional and learnable activation functions.

</details>


### [54] [PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models](https://arxiv.org/abs/2512.20063)
*Mingue Park,Jisung Hwang,Seungwoo Yoo,Kyeongmin Yeo,Minhyuk Sung*

Main category: cs.LG

TL;DR: PairFlow 提供一个极低成本的前处理步骤，用于在不需要教师模型的情况下训练离散流模型（DFMs），实现近似一步采样。成本仅约全模型训练的1.7%，可达到甚至超过需要微调的两阶段训练效果，且为后续蒸馏提供更强的基础模型，适用范围包括分子数据、二值和RGB图像。


<details>
  <summary>Details</summary>
Motivation: 离散流模型在抽样方面较慢，现有加速方法多依赖额外微调，训练开销大。需要一种无需教师、成本极低、能显著提升采样速度和性能的前处理方案。

Method: 从源分布与目标分布的耦合样本出发训练DFMs，利用DFMs的闭式反演构建配对的源-目标样本；该思路受到 ReFlow 及其对 DFMs 的扩展的启发，且无需预训练教师，成本极低（最高约1.7% 的全模型训练成本）。

Result: 在分子数据、二值与RGB图像等多种数据域上验证，PairFlow 的训练结果与甚至超过需要微调的两阶段训练；且用作蒸馏的更强基础模型，后续微调可带来进一步加速。

Conclusion: PairFlow 提供了一种极低成本的前处理方法，能够获得高效且性能良好的离散流模型，具有广泛适用性，并能促进后续蒸馏和加速。

Abstract: We introduce $\texttt{PairFlow}$, a lightweight preprocessing step for training Discrete Flow Models (DFMs) to achieve few-step sampling without requiring a pretrained teacher. DFMs have recently emerged as a new class of generative models for discrete data, offering strong performance. However, they suffer from slow sampling due to their iterative nature. Existing acceleration methods largely depend on finetuning, which introduces substantial additional training overhead. $\texttt{PairFlow}$ addresses this issue with a lightweight preprocessing step. Inspired by ReFlow and its extension to DFMs, we train DFMs from coupled samples of source and target distributions, without requiring any pretrained teacher. At the core of our approach is a closed-form inversion for DFMs, which allows efficient construction of paired source-target samples. Despite its extremely low cost, taking only up to 1.7% of the compute needed for full model training, $\texttt{PairFlow}$ matches or even surpasses the performance of two-stage training involving finetuning. Furthermore, models trained with our framework provide stronger base models for subsequent distillation, yielding further acceleration after finetuning. Experiments on molecular data as well as binary and RGB images demonstrate the broad applicability and effectiveness of our approach.

</details>


### [55] [QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption](https://arxiv.org/abs/2512.20084)
*Yanjie Li,Jian Xu,Xueqing Chen,Lina Yu,Shiming Xiang,Weijun Li,Cheng-lin Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa.
  To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.

</details>


### [56] [Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection](https://arxiv.org/abs/2512.20086)
*Jeehong Kim,Youngseok Hwang,Minchan Kim,Sungho Bae,Hyunwoo Park*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: \emph{Trajectory Synthesizer} and \emph{Anomaly Injector} to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems.

</details>


### [57] [Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering](https://arxiv.org/abs/2512.20115)
*Yuanhao Chen,Qi Liu,Pengbin Chen,Zhongjian Qiao,Yanjie Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected return using a given static dataset of transitions. However, offline RL faces the distribution shift problem. The policy constraint offline RL method is proposed to solve the distribution shift problem. During the policy constraint offline RL training, it is important to ensure the difference between the learned policy and behavior policy within a given threshold. Thus, the learned policy heavily relies on the quality of the behavior policy. However, a problem exists in existing policy constraint methods: if the dataset contains many low-reward transitions, the learned will be contained with a suboptimal reference policy, leading to slow learning speed, low sample efficiency, and inferior performances. This paper shows that the sampling method in policy constraint offline RL that uses all the transitions in the dataset can be improved. A simple but efficient sample filtering method is proposed to improve the sample efficiency and the final performance. First, we evaluate the score of the transitions by average reward and average discounted reward of episodes in the dataset and extract the transition samples of high scores. Second, the high-score transition samples are used to train the offline RL algorithms. We verify the proposed method in a series of offline RL algorithms and benchmark tasks. Experimental results show that the proposed method outperforms baselines.

</details>


### [58] [Learning to Reason in LLMs by Expectation Maximization](https://arxiv.org/abs/2512.20169)
*Junghyun Lee,Branislav Kveton,Sunav Choudhary,Subhojyoti Mukherjee,Anup Rao,Ryan A. Rossi,Alexa Siu*

Main category: cs.LG

TL;DR: 通过将推理过程视为潜在变量并引入EM目标，论文研究如何设计能够生成自洽推理的采样分布；在多数据集和模型上比较拒绝采样、自学推理者(STaR)与提示后验采样(PPS)，发现采样方案显著影响学习到的推理模型的准确性，且PPS在简单实现下表现最好。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型在解题时，先给出推理过程再给出答案的机制，并将其建模为潜在变量的EM问题，以理解和提升推理能力。强调设计有效的采样分布以产生能为正确答案辩解的推理的重要性。

Method: 将推理建模为潜在变量的EM优化问题，推导出学习推理的EM目标；提出并 instantiates 三种采样方案：拒绝采样（带预算）、自学推理者(STaR)以及仅保留STaR中推理阶段的提示后验采样(PPS)；在ARC、MMLU、OpenBookQA数据集上，使用Llama和Qwen等模型进行实验以比较不同采样方案的效果。

Result: 采样方案对学习到的推理模型的准确性有显著影响；在实验中，PPS尽管实现简单却超过其他采样方案，表现最好。

Conclusion: 采样分布的设计是学习推理能力的关键因素，EM框架与奖励驱动优化之间存在联系；简单而有效的PPS方法在多数据集和模型上展现出优越性，给出指导以提升基于推理的LLM能力。

Abstract: Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.

</details>


### [59] [NeuralCrop: Combining physics and machine learning for improved crop yield predictions](https://arxiv.org/abs/2512.20177)
*Yunan Lin,Sebastian Bathiany,Maha Badri,Maximilian Gelbrecht,Philipp Hess,Brian Groenke,Jens Heinke,Christoph Müller,Niklas Boers*

Main category: cs.LG

TL;DR: NeuralCrop 是一个混合GGCM，先模仿强GGCM再在观测数据上微调，提升了对多地区的产量预测准确性和在干旱等极端条件下的鲁棒性，优于纯GGCM和纯ML。


<details>
  <summary>Details</summary>
Motivation: GGCM在过程理解上的不确定性导致产量模拟的偏差；纯机器学习在分布外泛化能力有限，难以在快速变化的气候条件下做出可靠预测，因此需要结合过程模型与数据驱动方法以提升预测可靠性。

Method: 提出 NeuralCrop，将一个先进的过程驱动GGCM与数据驱动的机器学习组件相结合。模型首先训练以模拟一个具有竞争力的GGCM，然后在观测数据上进行微调；在训练过程中显式解决关键生物物理过程的表示，形成一个混合的过程-数据协作框架。

Result: 在站点层面和大尺度作物区间内均优于现有的GGCM；在欧洲小麦区域与美国玉米带的2000–2019年间对产量年度异常的再现更准确，且在干旱极端条件下改进显著；对未在训练中出现的条件仍具有鲁棒的外推能力，相较之下纯机器学习模型在此情形下性能显著下降。

Conclusion: 混合建模方法提升了作物建模的总体表现和在气候变化及极端天气条件下产量投影的可靠性。

Abstract: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.

</details>


### [60] [Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud](https://arxiv.org/abs/2512.20218)
*Jixiao Yang,Jinyu Chen,Zixiao Huang,Chengda Xu,Chi Zhang,Sijia Li*

Main category: cs.LG

TL;DR: Cost-TrustFL proposes a hierarchical federated learning framework that jointly optimizes model quality and cross-cloud communication costs with robust defense against poisoning, using a gradient-based Shapley-value approximation for lightweight reputation evaluation.


<details>
  <summary>Details</summary>
Motivation: To address non-IID data, Byzantine (poisoning) attackers, and high cross-cloud data transfer costs in multi-cloud federated learning, by balancing model performance with economic efficiency.

Method: Introduce Cost-TrustFL with an intra- inter-cloud hierarchical structure, a gradient-based approximate Shapley value computation for reputation, and a cost-aware aggregation that prioritizes intra-cloud communication. Evaluated on CIFAR-10 and FEMNIST.

Result: Achieves 86.7% accuracy with 30% malicious clients and reduces cross-cloud communication costs by 32% compared with baselines, while maintaining stable performance across varying non-IID levels and attack intensities.

Conclusion: Cost-TrustFL demonstrates practical, cost-aware, robust federated learning suitable for real-world multi-cloud deployments, effectively mitigating poisoning while reducing data transfer costs.

Abstract: Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.

</details>


### [61] [Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning](https://arxiv.org/abs/2512.20220)
*Kausthubh Manda,Raghuram Bharadwaj Diddigi*

Main category: cs.LG

TL;DR: Multitask offline Q-learning with a shared low-rank representation improves data efficiency by pooling data across tasks; provides finite-sample guarantees showing 1/sqrt(nT) scaling and benefits for downstream transfer when a new task reuses the learned representation.


<details>
  <summary>Details</summary>
Motivation: Leverage a shared, low-rank structure of action-value functions across related tasks to improve sample efficiency and generalization in offline RL where online interaction is unavailable.

Method: A multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions by minimizing Bellman error on fixed offline datasets, under realizability and coverage assumptions; includes a theoretical analysis of generalization with pooled data and a downstream transfer scenario.

Result: Finite-sample generalization bounds for the learned value functions demonstrating improved estimation with pooling across tasks, yielding a 1/sqrt(nT) dependence on the total samples across tasks while maintaining horizon and concentrability terms; downstream transfer shows the reused representation can reduce downstream complexity compared to learning from scratch.

Conclusion: Shared representations enable improved generalization and data efficiency in multitask offline Q-learning; benefits materialize under appropriate assumptions, and representation reuse is especially advantageous for related downstream tasks.

Abstract: We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.

</details>


### [62] [Adaptive Multi-task Learning for Probabilistic Load Forecasting](https://arxiv.org/abs/2512.20232)
*Onintze Zaballa,Verónica Álvarez,Santiago Mazuelas*

Main category: cs.LG

TL;DR: 提出一种自适应的多任务学习方法，用向量值隐马尔可夫模型对多实体进行概率性负荷预测，能够在线更新参数以适应变化的用电模式与实体间相关性，并在准确性与不确定性估计方面优于现有离线方法。


<details>
  <summary>Details</summary>
Motivation: 由于负荷需求具有本质不确定性、消费模式动态变化以及实体之间的相关性，跨多个实体进行概率性负荷预测具有重要意义。然而，现有的多任务学习在负荷预测领域的应用仍以离线方法为主，无法捕捉消费模式的变化，因此亟需能够自适应的在线多任务预测方法。

Method: 提出基于向量值隐马尔可夫模型的自适应多任务学习框架，采用递归更新来实时更新模型参数以反映最近的数据改变，并给出多实体的概率性负荷预测及不确定性评估。该方法将多个实体（如区域、建筑）视为相关任务，通过在线学习实现对实体间相关性与消费模式变化的动态建模。

Result: 在包含多实体负荷需求且消费模式多样且动态的数据集上进行实验，结果显示该方法在预测性能和不确定性评估方面优于现有方法，证明其在在线自适应多任务概率性负荷预测中的有效性。

Conclusion: 本文提出的自适应向量值隐马尔可夫多任务学习框架为实现实时、可靠的概率性负荷预测提供了一种有效途径，尤其在处理实体间相关性与消费模式变化方面表现出色。未来工作可扩展到更多实体、不同地区或场景，以及将该框架应用于其他需要在线自适应多任务预测的问题。

Abstract: Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment.

</details>


### [63] [How I Met Your Bias: Investigating Bias Amplification in Diffusion Models](https://arxiv.org/abs/2512.20233)
*Nathan Roos,Ekaterina Iakovleva,Ani Gjergji,Vito Paolo Pastore,Enzo Tartaglione*

Main category: cs.LG

TL;DR: 采样器及其超参数对扩散模型中的偏差放大具有显著影响，能在相同训练模型下实现偏差的减小或放大


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型偏差放大现象的可控性，聚焦采样过程对偏差的影响，填补仅有对抗性偏差放大的研究空白

Method: 在 Biased MNIST、Multi-Color MNIST、BFFHQ 上，以及 Stable Diffusion 的设定中，固定训练好的模型，系统改变不同采样器及其超参数，评估对偏差放大的影响，辅以定量分析与对比

Result: 实验表明采样超参数能对偏差放大产生明显的增减效应，既可降低也可放大偏差，效应在不同数据集和模型中均可观测

Conclusion: 采样过程对偏差放大的影响与模型本身同样重要，研究为通过调控采样参数来实现更公平的生成提供可行路径，代码已开源

Abstract: Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.

</details>


### [64] [Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion](https://arxiv.org/abs/2512.20249)
*Xuanyu Hu*

Main category: cs.LG

TL;DR: BrainROI 模型在 NSD 数据集上实现跨-subject 脑-描述生成的领先结果，提出 soft-ROI 的 fMRI 编码器、voxel-gate 融合、全局标签对齐，以及可解释的提示优化与参数化解码约束以提升稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试泛化性差与可解释性不足的多模态脑解码问题；脑拓扑异质性和跨个体差异导致跨-subject 转移困难，需要更稳定、可解释的解码与提示设计。

Method: 1) 提出 BrainROI，使用多图谱软功能分区（soft-ROI）作为共享空间的 fMRI 编码器。2) 将离散 ROI 拼接扩展为体素级门控融合机制（Voxel-gate）。3) 通过全局标签对齐确保 ROI 映射的一致性，提升跨被试传递性。4) 引入可解释的提示优化过程，在小样本闭环中使用本地部署的 Qwen 模型迭代生成并选择可读提示以提升提示设计的稳定性并保留可审计的优化轨迹。5) 推理阶段施加参数化解码约束以进一步提升生成描述的稳定性与质量。

Result: 在 NSD 数据集上的脑-captioning 评估中，BrainROI 在跨被试设置下相较于最近的 state-of-the-art 方法与代表性基线，在 BLEU-4、CIDEr 等指标上显示出明显提升，达到领先水平。

Conclusion: 该方法通过整合软-ROI 编码、体素级门控融合、全局标签对齐、可解释的提示优化以及解码阶段的约束，显著提升跨被试的泛化能力与描述生成的稳定性和可解释性，推动多模态脑解码在实际应用中的鲁棒性与透明度。

Abstract: Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.

</details>


### [65] [HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training](https://arxiv.org/abs/2512.20272)
*Yuanjian Xu,Yuan Shuai,Jianing Hao,Guang Zhang*

Main category: cs.LG

TL;DR: 提出 HGAN-SDEs，通过神经 Hermite 函数构造的判别器实现高效且稳定的 GAN 训练，用于建模 SDE 的路径分布；具备通用逼近性，且实验上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模连续时间的 SDE 路径时要么难以捕捉时间依赖性，要么计算成本过高，导致对抗训练不稳定。需要一种高效、稳定且具备良好泛化性的判别器。

Method: 采用神经 Hermite 函数构建结构化判别器，利用 Hermite 基底对路径级动态进行近似，降低运行时复杂度并提升训练稳定性；同时证明该框架具备普遍近似性质并给出收敛性分析。

Result: 在合成数据与真实系统的实验中，HGAN-SDEs 展现出在样本质量和学习效率方面对比现有 SDE 生成模型的显著提升。

Conclusion: 所提出的框架在 SDE 驱动分布建模中具有普适性、效率与稳定性优势，适用于广泛的应用场景。

Abstract: Neural Stochastic Differential Equations (Neural SDEs) provide a principled framework for modeling continuous-time stochastic processes and have been widely adopted in fields ranging from physics to finance. Recent advances suggest that Generative Adversarial Networks (GANs) offer a promising solution to learning the complex path distributions induced by SDEs. However, a critical bottleneck lies in designing a discriminator that faithfully captures temporal dependencies while remaining computationally efficient. Prior works have explored Neural Controlled Differential Equations (CDEs) as discriminators due to their ability to model continuous-time dynamics, but such architectures suffer from high computational costs and exacerbate the instability of adversarial training. To address these limitations, we introduce HGAN-SDEs, a novel GAN-based framework that leverages Neural Hermite functions to construct a structured and efficient discriminator. Hermite functions provide an expressive yet lightweight basis for approximating path-level dynamics, enabling both reduced runtime complexity and improved training stability. We establish the universal approximation property of our framework for a broad class of SDE-driven distributions and theoretically characterize its convergence behavior. Extensive empirical evaluations on synthetic and real-world systems demonstrate that HGAN-SDEs achieve superior sample quality and learning efficiency compared to existing generative models for SDEs

</details>


### [66] [Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity](https://arxiv.org/abs/2512.20291)
*Yuxing Gan,Ziyu Lei*

Main category: cs.LG

TL;DR: 提出 CDSP-MoE，通过冲突驱动的子空间裁剪实现专用专家在共享子空间中的动态实例化，解决 MoE 的结构性参数隔离和指令过拟合问题，在无任务标签下实现稳健的内容驱动路由并保持语义专门化；并给出代码仓库。


<details>
  <summary>Details</summary>
Motivation: 解决专用专家造成的灾难性遗忘与指令过拟合问题，提出以共享物理子空间中的动态专家实例化为核心的新范式，基于 Universal Weight Subspace 假说。

Method: 在超完备的参数骨干上，通过可学习的拓扑掩码从逻辑专家中裁剪出子集；引入 Lagged Gradient Game 作为结构性监督信号，惩罚共享流形中的相互干扰连接，使拓扑自发裁剪冲突路径，形成可解释的模块化结构。

Result: 实验表明，CDSP-MoE 在无人工任务标签的情况下实现稳健的内容驱动路由，在严格盲推理下也能保持语义专门化。

Conclusion: CDSP-MoE 提高参数利用率与解释性，实现无任务标签条件下的鲁棒路由和模块化结构；代码已开源。

Abstract: Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts

</details>


### [67] [FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning](https://arxiv.org/abs/2512.20329)
*Mrinmay Sen,Subhrajit Nag*

Main category: cs.LG

TL;DR: FedDPC 是一种联邦学习新方法，通过将本地更新投影到前一轮全局更新上并对每个本地更新进行自适应缩放，来同时缓解数据异质性和部分客户端参与带来的方差，从而加速收敛并提升全局模型的泛化性能，在多种异质分区的图像分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据异质性和部分客户端参与导致本地更新方差增大，进而使全局模型偏离全局目标、训练不稳定并降低收敛速度；现有工作对部分参与的关注不足。

Method: 将每个本地更新投影到上一轮全局更新上以控制方差，并在聚合前对每个本地更新进行自适应缩放，从而加速训练并缓解两类方差问题。

Result: 在多份异质分区的数据集的图像分类任务上，实验结果显示 FedDPC 相较于现有最先进的 FL 算法在训练损失快速下降与测试准确率提升方面具有更优表现，且在通信轮次上提升显著。

Conclusion: FedDPC 有效缓解数据异质性与部分参与带来的挑战，改善了FL训练的稳定性与全局模型性能，具备在更多任务与数据分布中的推广潜力。

Abstract: Data heterogeneity is a significant challenge in modern federated learning (FL) as it creates variance in local model updates, causing the aggregated global model to shift away from the true global optimum. Partial client participation in FL further exacerbates this issue by skewing the aggregation of local models towards the data distribution of participating clients. This creates additional variance in the global model updates, causing the global model to converge away from the optima of the global objective. These variances lead to instability in FL training, which degrades global model performance and slows down FL training. While existing literature primarily focuses on addressing data heterogeneity, the impact of partial client participation has received less attention. In this paper, we propose FedDPC, a novel FL method, designed to improve FL training and global model performance by mitigating both data heterogeneity and partial client participation. FedDPC addresses these issues by projecting each local update onto the previous global update, thereby controlling variance in both local and global updates. To further accelerate FL training, FedDPC employs adaptive scaling for each local update before aggregation. Extensive experiments on image classification tasks with multiple heterogeneously partitioned datasets validate the effectiveness of FedDPC. The results demonstrate that FedDPC outperforms state-of-the-art FL algorithms by achieving faster reduction in training loss and improved test accuracy across communication rounds.

</details>


### [68] [Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation](https://arxiv.org/abs/2512.20346)
*Emilia Majerz,Witold Dzwinel,Jacek Kitowski*

Main category: cs.LG

TL;DR: 提出一种物理驱动的机器学习框架，结合教师–学生式正态流生成模型来加速ALICE ZDC的仿真，辅以新颖损失函数与基于输出变异性的缩放机制，在提升粒子淤积的时空分布与形态重现的同时，对伪影的鲁棒性有所提高，仿真速度比现有NF实现快约421倍。


<details>
  <summary>Details</summary>
Motivation: 在保持物理可解释性与领域知识的前提下提升复杂探测器仿真的精度与效率，解决纯数据驱动方法在稀有事件、边界区域及高昂仿真成本方面的局限。

Method: 将领域物理知识嵌入学习过程，提出基于输出变异性的缩放和新损失函数；采用以Normalizing Flows为核心的教师–学生生成框架进行高效概率建模，并通过物理约束提升稳定性与泛化能力，从而更准确地再现ZDC粒子淤积的时空分布与形态。

Result: 相较于传统数据驱动模型，该方法在ZDC仿真中具有更高的表现，并实现约421倍的速度提升，与现有NF实现相比具有显著的性能优势，同时对罕见伪影的影响具鲁棒性。

Conclusion: 证实物理知识驱动的生成模型在复杂探测器仿真中具有显著的效率与准确性提升，具备良好可推广性，可扩展到其他科学计算仿真任务。

Abstract: Physics-based machine learning blends traditional science with modern data-driven techniques. Rather than relying exclusively on empirical data or predefined equations, this methodology embeds domain knowledge directly into the learning process, resulting in models that are both more accurate and robust. We leverage this paradigm to accelerate simulations of the Zero Degree Calorimeter (ZDC) of the ALICE experiment at CERN. Our method introduces a novel loss function and an output variability-based scaling mechanism, which enhance the model's capability to accurately represent the spatial distribution and morphology of particle showers in detector outputs while mitigating the influence of rare artefacts on the training. Leveraging Normalizing Flows (NFs) in a teacher-student generative framework, we demonstrate that our approach not only outperforms classic data-driven model assimilation but also yields models that are 421 times faster than existing NF implementations in ZDC simulation literature.

</details>


### [69] [Physics-guided Neural Network-based Shaft Power Prediction for Vessels](https://arxiv.org/abs/2512.20348)
*Dogan Altan,Hamza Haruna Mohammed,Glenn Terje Lines,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: A physics-guided neural network that integrates empirical shaft-power formulas to predict vessel shaft power, yielding higher accuracy than both a baseline neural network and empirical-formula method across four ships.


<details>
  <summary>Details</summary>
Motivation: To reduce fuel consumption and emissions in maritime transport by improving shaft-power prediction under varying sea conditions and vessel fouling, addressing limitations of purely empirical or purely data-driven models.

Method: A hybrid model that embeds empirical shaft-power formulas within a neural network (physics-guided), trained on data from four similar-sized cargo vessels, and benchmarked against a baseline neural network and an empirical formula-based approach.

Result: The physics-guided neural network achieved lower mean absolute error, root mean square error, and mean absolute percentage error for all four vessels compared to both baselines.

Conclusion: Incorporating physics-based empirical relations into neural networks improves shaft-power prediction accuracy under dynamic conditions, with potential fuel/emission reductions.

Abstract: Optimizing maritime operations, particularly fuel consumption for vessels, is crucial, considering its significant share in global trade. As fuel consumption is closely related to the shaft power of a vessel, predicting shaft power accurately is a crucial problem that requires careful consideration to minimize costs and emissions. Traditional approaches, which incorporate empirical formulas, often struggle to model dynamic conditions, such as sea conditions or fouling on vessels. In this paper, we present a hybrid, physics-guided neural network-based approach that utilizes empirical formulas within the network to combine the advantages of both neural networks and traditional techniques. We evaluate the presented method using data obtained from four similar-sized cargo vessels and compare the results with those of a baseline neural network and a traditional approach that employs empirical formulas. The experimental results demonstrate that the physics-guided neural network approach achieves lower mean absolute error, root mean square error, and mean absolute percentage error for all tested vessels compared to both the empirical formula-based method and the base neural network.

</details>


### [70] [Field-Space Attention for Structure-Preserving Earth System Transformers](https://arxiv.org/abs/2512.20350)
*Maximilian Witte,Johannes Meuer,Étienne Plésiat,Christopher Kadow*

Main category: cs.LG

TL;DR: Field-Space attention introduces attention in the physical field domain on the sphere, preserving field structure with fixed multiscale decomposition and learnable deformations, enabling interpretable, physically grounded Earth system transformers that outperform standard Vision Transformers and U-Net on global temperature super-resolution with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To build machine-learning architectures that operate directly on continuous geophysical fields while preserving geometric structure and enabling enforcement of scientific constraints, addressing instability and inefficiency in latent-space transformers.

Method: Field-Space attention computes attention in the physical domain rather than a learned latent space. All intermediates are continuous fields on the sphere. It uses a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field to fuse coarse and fine scales, avoiding single-scale transformer instabilities. Applied to global temperature super-resolution on a HEALPix grid.

Result: The method converges more rapidly and stably than conventional Vision Transformers and U-Net baselines and uses substantially fewer parameters.

Conclusion: Field-Space Attention offers a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling, enabling embedding of physical and statistical priors directly into the architecture to improve fidelity and reliability.

Abstract: Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.

</details>


### [71] [Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning](https://arxiv.org/abs/2512.20363)
*Daniel M. Jimenez-Gutierrez,Mehrdad Hassanzadeh,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的个性化联邦学习框架 Clust-PSI-PFL，利用 Population Stability Index (PSI) 量化非独立同分布数据（non-IID），通过 PSI 特征对客户端进行 K-means++ 聚类，并通过轮廓系数选择最佳簇数，结果在六个数据集上实现对比基线最高约18%的全局准确率提升，以及在数据强非独立同分布下客户端公平性相对提升约37%。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，客户端数据非独立同分布（non-IID）易导致更新偏置、性能下降；现有非IID度量不足以高效引导聚类；需要一个轻量、原理性的方法来提升鲁棒性和公平性。

Method: 计算加权 PSI 指标 WPSI^L；基于 PSI 特征用 K-means++ 对客户端分群，簇数通过基于轮廓系数的系统性过程确定，通常得到较少的簇且开销适中；在六个数据集（表格、图像、文本）、两种分区协议（Dirichlet 参数 α 和 Similarity 参数 S）以及多种客户端规模下评估。

Result: 在对比基线中实现高达约18%的全局准确率提升；在强非IID下显著提高客户端公平性，相对提升约37%；PSI 特征使客户端分布更同质，簇数较少且开销低。

Conclusion: 以 PSI 指导的聚类为鲁棒的个性化联邦学习提供了一种原理明确、开销可控的轻量化机制，特别适用于标签分布偏斜场景。

Abstract: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $α$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.

</details>


### [72] [GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer](https://arxiv.org/abs/2512.20399)
*Corey Adams,Rishikesh Ranade,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: GeoTransolver is a geometry-aware transformer for CAE using GALE attention; achieves higher accuracy and robustness with better data efficiency than existing baselines.


<details>
  <summary>Details</summary>
Motivation: To improve surrogate modeling in CAE by anchoring latent computations to multiscale geometric context and boundary conditions, enabling robust performance across irregular domains and varying regimes.

Method: Replace standard attention with GALE; integrate physics-aware self-attention on learned state slices with cross-attention to geometry/global/boundary context derived from multi-scale ball queries (DoMINO-inspired); persistent projection of geometry and boundary info into state spaces; implemented in NVIDIA PhysicsNeMo.

Result: Benchmark on DrivAerML, Luminary SHIFT-SUV/Wing; outperforms Domino, Transolver, AB-UPT; evaluated with drag/lift R2 and relative L1 errors; shows improved accuracy, robustness, and data efficiency; ablations and qualitative contour plots support findings.

Conclusion: Unifying multiscale geometry-aware context with physics-based attention advances operator learning for high-fidelity surrogate modeling in complex irregular domains and nonlinear regimes.

Abstract: We present GeoTransolver, a Multiscale Geometry-Aware Physics Attention Transformer for CAE that replaces standard attention with GALE, coupling physics-aware self-attention on learned state slices with cross-attention to a shared geometry/global/boundary-condition context computed from multi-scale ball queries (inspired by DoMINO) and reused in every block. Implemented and released in NVIDIA PhysicsNeMo, GeoTransolver persistently projects geometry, global and boundary condition parameters into physical state spaces to anchor latent computations to domain structure and operating regimes. We benchmark GeoTransolver on DrivAerML, Luminary SHIFT-SUV, and Luminary SHIFT-Wing, comparing against Domino, Transolver (as released in PhysicsNeMo), and literature-reported AB-UPT, and evaluate drag/lift R2 and Relative L1 errors for field variables. GeoTransolver delivers better accuracy, improved robustness to geometry/regime shifts, and favorable data efficiency; we include ablations on DrivAerML and qualitative results such as contour plots and design trends for the best GeoTransolver models. By unifying multiscale geometry-aware context with physics-based attention in a scalable transformer, GeoTransolver advances operator learning for high-fidelity surrogate modeling across complex, irregular domains and non-linear physical regimes.

</details>


### [73] [BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples](https://arxiv.org/abs/2512.20403)
*Xuan-An Le,Minh-Nam Tran,Son Nguyen*

Main category: cs.LG

TL;DR: BRIDGE 提出一种预算感知的中介式蒸馏框架，通过两阶段实现从大模型到小模型的高效知识转移：阶段1用中等规模的教师助教在有限数据上学习；阶段2在数据全量的前提下由教师助教产生合成推理（rationales）来训练更小的学生模型，同时采用指令微调课程来对齐行为。理论上在数据充足时比直接蒸馏有更紧的泛化界限；实验在医疗、法律、金融领域显示显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决从大规模专有模型向小于1B参数的可部署模型蒸馏时的容量与预算两难：教师-学生之间的巨大容量差距，以及高昂的API成本导致无法进行大规模数据收集。提出预算不对称与中介化作为破解口。

Method: 提出两阶段框架BRIDGE：阶段1在严格受限的数据子集上通过一个中等规模的教师助教（约7B参数）从黑箱教师处学习；此阶段通过零API成本的流水线在本地TA推理下选取训练数据以平衡难度和语义多样性。阶段2利用TA的高效推理成本与教师查询成本的异步性，TA生成全量数据的合成推理/推导过程以训练极小学生，同时引入指令微调课程以对齐行为。理论分析表明在数据丰富时BRIDGE的泛化界限比直接蒸馏更紧。

Result: 在跨医药、法律和金融基准上，BRIDGE consistently 提升学生模型的表现，提升幅度为28-41%，将与专有教师的能力差距缩小12-16%，并且仅需10x更少的教师查询。实验还显示BRIDGE在成本-性能前沿上超越直接蒸馏基线，即便后者使用100%的预算且BRIDGE仅耗用5%的资源。

Conclusion: BRIDGE通过预算感知的中介蒸馏打破容量-预算陷阱，利用阶段性中介与异步预算并行实现更高效的知识转移，显著缩小小模型对大模型的差距，同时降低教师查询成本并提升泛化能力。

Abstract: Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.

</details>


### [74] [Machine Learning to Predict Digital Frustration from Clickstream Data](https://arxiv.org/abs/2512.20438)
*Jibin Joseph*

Main category: cs.LG

TL;DR: 通过点击流数据预测用户在移动端和网站上的挫败感，LSTM在序列建模上略优于XGBoost，并能在最初的20-30次交互中就给出可靠预测。


<details>
  <summary>Details</summary>
Motivation: 用户在移动端和网站上体验不佳会导致销售损失和负面反馈；及早识别挫败感有助于实时干预和提升用户体验。

Method: 使用真实电商站点的点击流数据：5.4百万条事件、304,881个会话。通过规则将挫败感定义为愤怒爆发、返回/逆向导航(U-turn)、购物车流失、搜索困难和长时间徘徊等；从每个会话构建表格特征并训练标准分类器；同时对完整事件序列训练基于序列的判别性LSTM分类器。评估指标为准确率和ROC AUC。XGBoost获得约90%准确率、AUC 0.9579，LSTM在序列建模中表现最好，约91%准确率、AUC 0.9705。研究还显示在仅前20-30次交互时，LSTM即可对挫败感进行可靠预测。

Result: XGBoost：约90%准确率，AUC 0.9579；LSTM：约91%准确率，AUC 0.9705，序列模型表现优于表格特征的传统分类器；在前20-30次交互就能实现稳健的挫败感预测。

Conclusion: 序列模型（尤其是LSTM）在挫败感预测中具有更强的预测力，且可实现早期干预的潜力，适用于实时评估与个性化用户体验优化。但需关注泛化性、数据隐私与实现成本等现实挑战。

Abstract: Many businesses depend on their mobile apps and websites, so user frustration while trying to complete a task on these channels can cause lost sales and complaints. In this research, I use clickstream data from a real e-commerce site to predict whether a session is frustrated or not. Frustration is defined using certain rules based on rage bursts, back and forth navigation (U turns), cart churn, search struggle, and long wandering sessions, and applies these rules to 5.4 million raw clickstream events (304,881 sessions). From each session, I build tabular features and train standard classifier models. I also use the full event sequence to train a discriminative LSTM classifier. XGBoost reaches about 90% accuracy, ROC AUC of 0.9579, while the LSTM performs best with about 91% accuracy and a ROC AUC of 0.9705. Finally, the research shows that with only the first 20 to 30 interactions, the LSTM already predicts frustration reliably.

</details>


### [75] [Explainable time-series forecasting with sampling-free SHAP for Transformers](https://arxiv.org/abs/2512.20514)
*Matthias Hertel,Sebastian Pütz,Ralf Mikut,Veit Hagenmeyer,Benjamin Schäfer*

Main category: cs.LG

TL;DR: 提出 SHAPformer，一种基于 Transformer 的时间序列解释模型，快速且无采样地产生 SHAP 风格解释，且在合成和真实数据上均表现良好。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需可解释性以建立信任与合规；现有 SHAP 在时间序列上的实现低效且常假设特征独立性，亟需更快且更真实的解释方法。

Method: 基于 Transformer 的 SHAPformer 通过注意力操控实现对特征子集的预测贡献，避免对特征进行采样以产生解释；生成解释在约1秒内完成，相比 SHAP Permutation Explainer 快数量级。

Result: 在带有地面真相解释的合成数据中，解释与真实解释一致；在真实电力负荷数据上，预测性能具有竞争力，且能提供有意义的局部与全局洞察，如过去负荷为关键预测因子，圣诞期间表现不同。

Conclusion: SHAPformer 提供一个快速、准确且无采样的时间序列解释框架，利用注意力机制实现对特征子集的解释，适用于实践中的解释需求。

Abstract: Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.

</details>


### [76] [Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs](https://arxiv.org/abs/2512.20573)
*Rui Pan,Zhuofu Chen,Ravi Netravali*

Main category: cs.LG

TL;DR: FailFast is a diffusion LLM-based speculative decoding framework that adaptively tunes speculation length to accelerate autoregressive decoding without sacrificing accuracy, by leveraging dLLMs as verifiers and dynamically adjusting draft lengths (fast-fail on hard regions, long drafts on easy regions).


<details>
  <summary>Details</summary>
Motivation: Diffuse LLMs offer fast parallel token generation but suffer efficiency-quality tradeoffs when used standalone. The paper investigates whether these fast dLLMs can improve speculative decoding with AR verifiers by reducing the risk and cost of rejections and enabling long drafts.

Method: Introduce FailFast, a dLLM-based speculative decoding framework that dynamically adapts speculation length. It spends minimal compute in hard regions to shrink latency and aggressively extends drafts in easier regions to reduce verification latency (e.g., speculating and accepting 70 tokens at a time). No fine-tuning is required. Evaluates across multiple models/workloads.

Result: Achieves up to 4.9× speedup over vanilla decoding, 1.7× over the best naive dLLM drafter, and 1.4× over EAGLE-3, while delivering lossless acceleration across diverse models and workloads.

Conclusion: Demonstrates that dLLMs can effectively accelerate AR LLM decoding through adaptive speculative decoding (FailFast) and provides an open-source implementation for broader use.

Abstract: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It "fails fast" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and "wins big" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.

</details>


### [77] [Performative Policy Gradient: Optimality in Performative Reinforcement Learning](https://arxiv.org/abs/2512.20576)
*Debabrota Basu,Udvas Das,Brahim Driss,Uddalak Mukherjee*

Main category: cs.LG

TL;DR: PePG introduces a policy gradient algorithm for performative RL, proving performative versions of the performance difference lemma and policy gradient theorem, and showing convergence to performatively optimal policies under softmax parametrization (with/without entropy regularization). Empirically, it outperforms standard policy gradient and existing performative RL baselines.


<details>
  <summary>Details</summary>
Motivation: In post-deployment settings, deployed policies alter environment distributions, violating RL's standard stability assumptions. There is a need for algorithms that seek policies robust to distribution shifts induced by themselves and, ideally, optimal under such shifts.

Method: Derive performative counterparts to the performance difference lemma and the policy gradient theorem. Propose the Performative Policy Gradient (PePG) algorithm, with analysis under softmax parametrization, both with and without entropy regularization.

Result: Theorem: PePG converges to performatively optimal policies (they remain optimal under the distribution shifts they induce). Empirical results: PePG outperforms standard policy gradient and existing performative RL methods aimed at stability across standard performative RL benchmarks.

Conclusion: PePG extends prior performative RL work from stability to optimality, offering a principled approach for robust, high-performing policies in environments shaped by their own deployments; opens avenues for further empirical validation and extensions.

Abstract: Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.

</details>


### [78] [Improving ML Training Data with Gold-Standard Quality Metrics](https://arxiv.org/abs/2512.20577)
*Leslie Barrett,Michael W. Sherman*

Main category: cs.LG

TL;DR: 提出统计性评估手工标注数据质量的方法，强调跨多轮标注的一致性/agreement指标更可靠，方差下降表明数据质量提升；提供在不对每个项都进行多次标注的情形下获取高质量数据的策略，并指出“标签初期磨合期”不足以显著降低错误。


<details>
  <summary>Details</summary>
Motivation: 手工标注数据质量波动大且研究不足，影响下游机器学习任务效果；需要可量化的质量评估与提升方法。

Method: 使用统计方法衡量标注一致性与agreement；在多轮标注中记录agreement指标，观察方差随轮次下降；提出一种不对每个工作项设置多次标注的收集高质量数据的方法；分析烧入期(burn-in)是否能降低标注错误。

Result: 多轮标注时相关的agreement指标更为可靠；随着轮次的增加，记录的方差下降，指示数据质量提升；给出一种在不对每个项进行多次标注的情况下获取高质量数据的做法；标签burn-in期可能不足以显著降低错误。

Conclusion: 通过重复标注来评估与提升质量，方差在多轮标注中的变动作为质量信号；设计有效的标注流程可在减少冗余的前提下提升数据质量；burn-in并非解决方案，应结合多轮一致性评估。

Abstract: Hand-tagged training data is essential to many machine learning tasks. However, training data quality control has received little attention in the literature, despite data quality varying considerably with the tagging exercise. We propose methods to evaluate and enhance the quality of hand-tagged training data using statistical approaches to measure tagging consistency and agreement. We show that agreement metrics give more reliable results if recorded over multiple iterations of tagging, where declining variance in such recordings is an indicator of increasing data quality. We also show one way a tagging project can collect high-quality training data without requiring multiple tags for every work item, and that a tagger burn-in period may not be sufficient for minimizing tagger errors.

</details>


### [79] [Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning](https://arxiv.org/abs/2512.20605)
*Seijin Kobayashi,Yanick Schimpf,Maximilian Schlegel,Angelika Steger,Maciej Wolczyk,Johannes von Oswald,Nino Scherre,Kaitlin Maile,Guillaume Lajoie,Blake A. Richards,Rif A. Saurous,James Manyika,Blaise Agüera y Arcas,Alexander Meulemans,João Sacramento*

Main category: cs.LG

TL;DR: 引入内部 RL：一个高阶非因果序列模型调控基础自回归模型的内部残差流，从而在内部控制器上实现长时序、层次化行为，提升稀疏奖励下的学习效率。


<details>
  <summary>Details</summary>
Motivation: 在大规模自回归模型上进行下一-token预测预训练并通过强化学习微调时，若奖励稀疏，逐步采样容易导致学习低效；需要更高层次、跨时间尺度的内部行动来提升探索效率。

Method: 提出一个高阶非因果序列模型，其输出控制基础自回归模型的残差流。该模型学习将长时间序列的 activation chunk 压缩为内部控制器，每个控制器执行跨时间的行为序列并带有终止条件。通过在内部实现强化学习（内部 RL），实现对稀疏奖励的直接学习，从而在网格世界和 MuJoCo 的层次化任务上提升探索与学习效果。

Result: 在具有层次结构的网格世界和 MuJoCo 任务上，高阶模型学会将长激活序列压缩为内部控制器；控制器执行具有可解释的长时延行为序列，并具备学习的终止条件。直接的内部控制器强化学习在稀疏奖励条件下显著提升学习效果，克服了标准 RL 微调的失败。

Conclusion:  Latent 动作生成与内部强化学习为在基础模型内实现分层强化学习提供了一个有前景的方向，内部 RL 促进高效探索与长时程策略的学习。

Abstract: Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [80] [Energy-Efficient Multi-LLM Reasoning for Binary-Free Zero-Day Detection in IoT Firmware](https://arxiv.org/abs/2512.19945)
*Saeid Jamshidi,Omar Abdul-Wahab,Martine Bellaïche,Foutse Khomh*

Main category: cs.CR

TL;DR: 提出一种二进制无关、架构无关的IoT固件安全性评估方法，利用三层大语言模型推理来估计概念性零日漏洞概率，并结合能耗相关符号负载模型与LLM计算特征，提升可解释性与可操作性。


<details>
  <summary>Details</summary>
Motivation: 传统固件分析在面对专有二进制、缺失符号、异构体系结构和不可访问执行代码时，难以检测或估算零日风险，亟需在二进制不可见场景下进行风险评估的方法。

Method: 构建三层LLM推理架构：LLaMA为配置解释器，DeepSeek进行结构抽象分析，GPT-4o进行语义融合；引入LLM计算签名（延迟模式、不确定性标记、推理深度指示）和能量感知的符号载荷模型，并给出推理管线的单调性、发散性与能量-风险耦合的数学推理基础。以仿真评估在高暴露条件下的零日概率变化，并分析各层对风险的贡献。

Result: 仿真结果显示，在高暴露条件下，零日概率预测提升约20–35%；GPT-4o在跨层相关性和敏感性方面表现最佳；能量与发散性指标显著预测风险，统计显著性为p<0.01。

Conclusion: 该方法在二进制不可见场景下提供可解释、可操作的零日风险估计框架，并通过理论化的推理基础和仿真结果验证了能量与发散性特征对风险预测的有效性和重要性。

Abstract: Securing Internet of Things (IoT) firmware remains difficult due to proprietary binaries, stripped symbols, heterogeneous architectures, and limited access to executable code. Existing analysis methods, such as static analysis, symbolic execution, and fuzzing, depend on binary visibility and functional emulation, making them unreliable when firmware is encrypted or inaccessible. To address this limitation, we propose a binary-free, architecture-agnostic solution that estimates the likelihood of conceptual zero-day vulnerabilities using only high-level descriptors. The approach integrates a tri-LLM reasoning architecture combining a LLaMA-based configuration interpreter, a DeepSeek-based structural abstraction analyzer, and a GPT-4o semantic fusion model. The solution also incorporates LLM computational signatures, including latency patterns, uncertainty markers, and reasoning depth indicators, as well as an energy-aware symbolic load model, to enhance interpretability and operational feasibility. In addition, we formally derive the mathematical foundations of the reasoning pipeline, establishing monotonicity, divergence, and energy-risk coupling properties that theoretically justify the model's behavior. Simulation-based evaluation reveals that high exposure conditions increase the predicted zero-day likelihood by 20 to 35 percent across models, with GPT-4o demonstrating the strongest cross-layer correlations and the highest sensitivity. Energy and divergence metrics significantly predict elevated risk (p < 0.01), reinforcing the effectiveness of the proposed reasoning framework.

</details>


### [81] [Efficient Mod Approximation and Its Applications to CKKS Ciphertexts](https://arxiv.org/abs/2512.19951)
*Yufei Zhou*

Main category: cs.CR

TL;DR: 提出一种在 CKKS 同态加密下近似模运算的新方法：通过多项式插值和切比雪夫级数实现对模函数的高精度近似，并设计 BitStack 与 CRTStack 两种小整数数据打包方案以提高 CKKS 明文空间利用率和上传效率，进一步将该模运算应用于同态四舍五入和把秘分享转换为 CKKS 密文，实验显示近似精度可达 1e-8，提供一个实用通用的 CKKS 模运算解决方案。


<details>
  <summary>Details</summary>
Motivation: CKKS 仅支持加减乘等算术运算，难以在密文上实现模运算。模函数的非连续性和周期性使其在同态设置下近似困难。目前现有方法仅在输入区间的有限子范围内给出较准的结果，尚未实现对全输入范围的准确近似。需要一种在整个输入区间内鲁棒且高精度的模运算近似。

Method: 基于多项式插值与切比雪夫级数对模函数进行近似，并提出两种面向小整数输入的 CKKS 数据打包方案 BitStack 与 CRTStack，以提升 CKKS 明文空间的利用率与密文上传效率。随后将该近似模运算用于实现同态四舍五入以及从加法秘密分享到 CKKS 密文的通用转换，确保密文级别的高精度舍入和转换。

Result: 实验结果显示近似精度可达 1e-8，且 BitStack/CRTStack 显著提升了在 CKKS 上的密文上传与数据打包效率，证明所提方法在实际隐私计算场景中的可行性与有效性。

Conclusion: 本文给出了一种在 CKKS 上实现模运算的实用且通用的解决方案，扩展了 CKKS 的适用场景，使其能够处理更多需要模运算的隐私计算任务。

Abstract: The mod function plays a critical role in numerous data encoding and cryptographic primitives. However, the widely used CKKS homomorphic encryption (HE) scheme supports only arithmetic operations, making it difficult to perform mod computations on encrypted data. Approximating the mod function with polynomials has therefore become an important yet challenging problem. The discontinuous and periodic characteristics of the mod function make it particularly difficult to approximate accurately under HE. Existing homomorphic mod constructions provide accurate results only within limited subranges of the input range, leaving the problem of achieving accurate approximation across the full input range unresolved. In this work, we propose a novel method based on polynomial interpolation and Chebyshev series to accurately approximate the mod function. Building upon this, we design two efficient data packing schemes, BitStack and CRTStack, tailored for small-integer inputs in CKKS. These schemes significantly improve the utilization of the CKKS plaintext space and enable efficient ciphertext uploads. Furthermore, we apply the proposed HE mod function to implement a homomorphic rounding operation and a general transformation from additive secret sharing to CKKS ciphertexts, achieving accurate ciphertext rounding and complete secret-share-to-CKKS conversion. Experimental results demonstrate that our approach achieves high approximation accuracy (up to 1e-8). Overall, our work provides a practical and general solution for performing mod operations under CKKS, extending its applicability to a broader range of privacy-preserving computations.

</details>


### [82] [Fast Deterministically Safe Proof-of-Work Consensus](https://arxiv.org/abs/2512.19968)
*Ali Farahbakhsh,Giuliano Losa,Youer Pu,Lorenzo Alvisi,Ittay Eyal*

Main category: cs.CR

TL;DR: Sieve-MMR 是首个完全许可化、在没有外部信任的情况下实现确定性安全性和常数期望延迟的协议，将 PoW 与 PoS 的优点结合，通过时间旅行免疫的广播（TTRB）实现消息传递。


<details>
  <summary>Details</summary>
Motivation: PoS 的长距离攻击和 PoW 可以篡改执行历史的风险要求一个完全许可、无需外部机制、并具备确定性安全性与低延迟的解决方案。

Method: 将 PoS 的 MMR 协议移植到 PoW 环境，借助确定性 PoW 的黑箱特性实现时间旅行鲁棒广播（TTRB）作为消息传递层，并将其与 MMR 结合以获得确定性安全性和常数延迟，同时通过 PoW 提供对长距离攻击的抵抗。

Result: 提出 Sieve-MMR，成为首个在不依赖外部机制的前提下实现确定性安全性和常数期望延迟的完全许可协议，并通过 TTRB 与 PoW 作为消息层来抵御时间旅行攻击。

Conclusion: 通过引入时间旅行鲁棒广播（TTRB）并将 PoW 的消息层嵌入到 PoS 的 MMR 结构中，Sieve-MMR 在完全许可环境中实现确定性安全与低延迟，解决了外部机制依赖的问题并提供对时间旅行攻击的有效应对。

Abstract: Permissionless blockchains achieve consensus while allowing unknown nodes to join and leave the system at any time. They typically come in two flavors: proof of work (PoW) and proof of stake (PoS), and both are vulnerable to attacks. PoS protocols suffer from long-range attacks, wherein attackers alter execution history at little cost, and PoW protocols are vulnerable to attackers with enough computational power to subvert execution history. PoS protocols respond by relying on external mechanisms like social consensus; PoW protocols either fall back to probabilistic guarantees, or are slow.
  We present Sieve-MMR, the first fully-permissionless protocol with deterministic security and constant expected latency that does not rely on external mechanisms. We obtain Sieve-MMR by porting a PoS protocol (MMR) to the PoW setting. From MMR we inherit constant expected latency and deterministic security, and proof-of-work gives us resilience against long-range attacks. The main challenge to porting MMR to the PoW setting is what we call time-travel attacks, where attackers use PoWs generated in the distant past to increase their perceived PoW power in the present. We respond by proposing Sieve, a novel algorithm that implements a new broadcast primitive we dub time-travel-resilient broadcast (TTRB). Sieve relies on a black-box, deterministic PoW primitive to implement TTRB, which we use as the messaging layer for MMR.

</details>


### [83] [IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense](https://arxiv.org/abs/2512.20004)
*Rahul Yumlembam,Biju Issac,Seibu Mary Jacob,Longzhi Yang*

Main category: cs.CR

TL;DR: Graph-based GNN classifiers effectively detect Android malware using API-call graphs plus permission/intent features, achieving near-perfect accuracy on CICMaldroid and Drebin; however, they are vulnerable to adversarial graph attacks. A VGAE-MalGAN GAN attack can craft adversarial API graphs to evade detection, and retraining with such samples improves robustness.


<details>
  <summary>Details</summary>
Motivation: Motivation is to improve Android malware detection by leveraging graph representations and to assess and strengthen robustness against adversarial graph perturbations, particularly in the context of IoT-friendly Android ecosystems.

Method: 1) Train a Graph Neural Network (GNN) classifier on API call graphs augmented with Permission and Intent features and evaluate on CICMaldroid and Drebin. 2) Propose VGAE-MalGAN, a GAN-based attack using a VGAE generator to craft adversarial API graphs and a substitute detector to mimic the target; assess impact on GNN-based malware classifiers; investigate retraining with adversarial samples.

Result: Classification accuracy: 98.33% on CICMaldroid and 98.68% on Drebin. VGAE-MalGAN can significantly reduce detection rates of GNN classifiers; adversarial malware initially evades detection, but retraining with generated adversarial samples improves robustness.

Conclusion: Graph-based Android malware detection is highly effective but vulnerable to adversarial graph perturbations. Adversarial training with generated samples can mitigate attacks, highlighting the need for robust defenses in graph-based malware detectors.

Abstract: Since the Internet of Things (IoT) is widely adopted using Android applications, detecting malicious Android apps is essential. In recent years, Android graph-based deep learning research has proposed many approaches to extract relationships from applications as graphs to generate graph embeddings. First, we demonstrate the effectiveness of graph-based classification using a Graph Neural Network (GNN)-based classifier to generate API graph embeddings. The graph embeddings are combined with Permission and Intent features to train multiple machine learning and deep learning models for Android malware detection. The proposed classification approach achieves an accuracy of 98.33 percent on the CICMaldroid dataset and 98.68 percent on the Drebin dataset. However, graph-based deep learning models are vulnerable, as attackers can add fake relationships to evade detection by the classifier. Second, we propose a Generative Adversarial Network (GAN)-based attack algorithm named VGAE-MalGAN targeting graph-based GNN Android malware classifiers. The VGAE-MalGAN generator produces adversarial malware API graphs, while the VGAE-MalGAN substitute detector attempts to mimic the target detector. Experimental results show that VGAE-MalGAN can significantly reduce the detection rate of GNN-based malware classifiers. Although the model initially fails to detect adversarial malware, retraining with generated adversarial samples improves robustness and helps mitigate adversarial attacks.

</details>


### [84] [On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities](https://arxiv.org/abs/2512.20062)
*Sangryu Park,Gihyuk Ko,Homook Cho*

Main category: cs.CR

TL;DR: 将漏洞分析任务从简单二分类改为输出CWE类别的SVI（Software Vulnerability Identification），并通过在本地可部署的小型模型上进行指令微调来替代依赖在线API的LLM，显示在识别准确性与成本方面优于API模型，提供更安全、可实用的漏洞管理工作流。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于API的LLM在代码隐私、可控性与安全性方面的不足，同时提升漏洞分析的实际有用性，避免将源代码暴露给外部服务

Method: 将任务重新定义为SVI，要求LLMs输出CWE ID而非简单的漏洞存在性；比较本地指令微调的LLM与在线API LLM在识别性能与成本上的表现；评估在真实漏洞管理工作流中的实用性

Result: 本地指令微调模型在整体识别性能与成本权衡上优于在线API LLM，显示更安全、可控且具成本效益的能力

Conclusion: 指令微调的本地模型是将LLMs应用于实际漏洞分析的更有效、可控且成本更低的解决方案，适合嵌入真实工作流。

Abstract: Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.

</details>


### [85] [Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography](https://arxiv.org/abs/2512.20168)
*Songze Li,Jiameng Cheng,Yiming Li,Xiaojun Jia,Dacheng Tao*

Main category: cs.CR

TL;DR: 提出Odysseus，一种用于多模态大语言模型的双隐写可预测性实验性 jailbreak 框架，通过在看似无害的图像中嵌入恶意查询与响应来绕过安全过滤，暴露现有防御的盲点并实现高达99%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 挑战“可疑内容必须在输入/输出中显式可见”的假设，揭示跨模态安全在MLLM系统中的脆弱性，促使对模态间安全防护进行重新审视。

Method: 引入双隐写技术，将恶意查询和回应隐藏在表观无害的图像中；在基准数据集上对若干前沿和现实场景下的MLLM系统进行攻击评估，攻击成功率可达显著水平（高达99%）。

Result: 证明在MLLM系统中，现有的安全过滤器可以被跨模态隐写方式绕过，暴露 defenses 的根本盲点，显示出对多模态上下文的安全性不足。

Conclusion: 呼吁重新思考跨模态安全防护，提升对模态间信息隐藏与误导的防御能力，推动更强的对齐与安全机制在MLLM中的实现。

Abstract: By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.

</details>


### [86] [Optimistic TEE-Rollups: A Hybrid Architecture for Scalable and Verifiable Generative AI Inference on Blockchain](https://arxiv.org/abs/2512.20176)
*Aaron Chan,Alex Ding,Frank Chen,Alan Wu,Bruce Zhang,Arther Tian*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid integration of Large Language Models (LLMs) into decentralized physical infrastructure networks (DePIN) is currently bottlenecked by the Verifiability Trilemma, which posits that a decentralized inference system cannot simultaneously achieve high computational integrity, low latency, and low cost. Existing cryptographic solutions, such as Zero-Knowledge Machine Learning (ZKML), suffer from superlinear proving overheads (O(k NlogN)) that render them infeasible for billionparameter models. Conversely, optimistic approaches (opML) impose prohibitive dispute windows, preventing real-time interactivity, while recent "Proof of Quality" (PoQ) paradigms sacrifice cryptographic integrity for subjective semantic evaluation, leaving networks vulnerable to model downgrade attacks and reward hacking. In this paper, we introduce Optimistic TEE-Rollups (OTR), a hybrid verification protocol that harmonizes these constraints. OTR leverages NVIDIA H100 Confidential Computing Trusted Execution Environments (TEEs) to provide sub-second Provisional Finality, underpinned by an optimistic fraud-proof mechanism and stochastic Zero-Knowledge spot-checks to mitigate hardware side-channel risks. We formally define Proof of Efficient Attribution (PoEA), a consensus mechanism that cryptographically binds execution traces to hardware attestations, thereby guaranteeing model authenticity. Extensive simulations demonstrate that OTR achieves 99% of the throughput of centralized baselines with a marginal cost overhead of $0.07 per query, maintaining Byzantine fault tolerance against rational adversaries even in the presence of transient hardware vulnerabilities.

</details>


### [87] [Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit](https://arxiv.org/abs/2512.20423)
*Adam Elaoumari*

Main category: cs.CR

TL;DR: 提出端到端容器化管线，用于 DoH 文件外 exfiltration 的生成、拦截与分析，并在对抗性场景下比较机器学习与阈值检测方法。


<details>
  <summary>Details</summary>
Motivation: 评估防御方在 DoH 作为隐蔽通道时对外泄攻击的检测能力，研究可规避策略，并提供可复现的工具链以进行对比与扩展。

Method: 构建可配置的 DoH 外传生成器（分块、编码、填充、解析器轮换等参数），实现解析端重构；使用 fork 的 DoHLyzer 提取流量特征；预测端训练并将机器学习模型（随机森林、梯度提升、逻辑回归）与阈值检测进行对比；工具链通过 Docker 容器化实现可重复性；还包括数据集和对抗场景的评估。

Result: 在公开 DoH 数据集上训练并基准对比多种分类模型，与对抗性 DoH 外传情景下的阈值检测进行评估；提供端到端工作流，包括流量产生、捕获、特征提取、模型训练与分析。

Conclusion: 未来工作将验证混合企业流量中的结果，扩展到 HTTP/3/QUIC，增加良性流量与实时评估能力，并量化在 stealth 成本约束下 DoH 外 exfiltration 的经济性。

Abstract: The purpose of this project is to assess how well defenders can detect DNS-over-HTTPS (DoH) file exfiltration, and which evasion strategies can be used by attackers. While providing a reproducible toolkit to generate, intercept and analyze DoH exfiltration, and comparing Machine Learning vs threshold-based detection under adversarial scenarios. The originality of this project is the introduction of an end-to-end, containerized pipeline that generates configurable file exfiltration over DoH using several parameters (e.g., chunking, encoding, padding, resolver rotation). It allows for file reconstruction at the resolver side, while extracting flow-level features using a fork of DoHLyzer. The pipeline contains a prediction side, which allows the training of machine learning models based on public labelled datasets and then evaluates them side-by-side with threshold-based detection methods against malicious and evasive DNS-Over-HTTPS traffic. We train Random Forest, Gradient Boosting and Logistic Regression classifiers on a public DoH dataset and benchmark them against evasive DoH exfiltration scenarios. The toolkit orchestrates traffic generation, file capture, feature extraction, model training and analysis. The toolkit is then encapsulated into several Docker containers for easy setup and full reproducibility regardless of the platform it is run on. Future research regarding this project is directed at validating the results on mixed enterprise traffic, extending the protocol coverage to HTTP/3/QUIC request, adding a benign traffic generation, and working on real-time traffic evaluation. A key objective is to quantify when stealth constraints make DoH exfiltration uneconomical and unworthy for the attacker.

</details>


### [88] [From the Two-Capacitor Paradox to Electromagnetic Side-Channel Mitigation in Digital Circuits](https://arxiv.org/abs/2512.20303)
*Raghvendra Pratap Singh,Baibhab Chatterjee,Shreyas Sen,Debayan Das*

Main category: cs.CR

TL;DR: Revisit the two-capacitor energy paradox, analytically confirm energy loss during capacitor charging arises from heat and radiation in RC/RLC circuits, and connect this EM leakage to EM side-channel attacks that can recover secret keys. Propose adiabatic charging as a low-overhead mitigation to reduce leakage.


<details>
  <summary>Details</summary>
Motivation: From a security standpoint, electromagnetic side-channel analysis (SCA) poses a significant risk for resource-constrained, internet-connected devices. Understanding how energy dissipation during capacitor charging relates to EM leakage helps assess vulnerabilities and motivates countermeasures.

Method: Provide analytical proofs for standard RC and RLC models showing energy dissipated as heat and as radiated EM energy. Link the energy loss to EM SCA leakage and demonstrate the possibility of recovering a secret key embedded in the device. Propose adiabatic charging as a design technique to minimize EM leakage.

Result: Analytical confirmation that energy loss during capacitor charging is predominantly dissipated as heat and radiation. Establishment of a link between charging energy loss and EM SCA leakage, enabling potential secret-key recovery. Proposal of adiabatic charging as a mitigation strategy to reduce EM leakage.

Conclusion: Adiabatic charging can minimize EM leakage and thereby enhance EM SCA resilience with low overhead.

Abstract: The classical two-capacitor paradox of the lost energy is revisited from an electronic circuit security stand-point. The paradox has been solved previously by various researchers, and the energy lost during the charging of capacitors has been primarily attributed to the heat and radiation. We analytically prove this for various standard resistor-capacitor (RC) and resistor-inductor-capacitor (RLC) circuit models. From the perspective of electronic system security, electromagnetic (EM) side-channel analysis (SCA) has recently gained significant prominence with the growth of resource-constrained, internet connected devices. This article connects the energy lost due to capacitor charging to the EM SCA leakage in electronic devices, leading to the recovery of the secret encryption key embedded within the device. Finally, with an understanding of how lost energy relates to EM radiation, we propose adiabatic charging as a solution to minimize EM leakage, thereby paving the way towards low-overhead EM SCA resilience.

</details>


### [89] [Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms](https://arxiv.org/abs/2512.20323)
*Ipek Sena Yilmaz,Onur G. Tuncer,Zeynep E. Aksoy,Zeynep Yağmur Baydemir*

Main category: cs.CR

TL;DR: 提出一种自适应的差分隐私预算分配，用于无线感知中的 CSI spectrogram 特征释放，在相同隐私预算下优于均匀扰动，改善隐私-实用性权衡。


<details>
  <summary>Details</summary>
Motivation: 在云分析、联合训练或基准评测中需要共享特征，但直接释放中间表示（如 CSI  spectrograms）可能泄露用户身份、位置等敏感信息，因此需要正式的隐私保障；CSI 的时频结构高度非均匀，需针对性地分配隐私预算。

Method: 将 CSI 转换为有界的 spectrogram 特征；通过裁剪进行敏感度控制；估计时频平面上与任务相关的重要性；在 spectrogram 块之间自适应分配全局隐私预算，然后注入经标定的高斯噪声（高斯机制），实现差分隐私保护。

Result: 在多用户活动感知（WiMANS）、多人与三维姿态估计（Person-in-WiFi 3D）、呼吸监测（Resp-CSI）等数据集上，自适应分配在相同隐私预算下优于均匀扰动，取得更高准确性、较低误差，并显著降低对身份和成员身份推断攻击的经验性信息泄露。

Conclusion: 针对时频结构自适应分配隐私预算的特征释放方法有效提升无线感知领域的隐私与实用性权衡，适用于云分析和联合训练场景。

Abstract: Wi-Fi/RF-based human sensing has achieved remarkable progress with deep learning, yet practical deployments increasingly require feature sharing for cloud analytics, collaborative training, or benchmark evaluation. Releasing intermediate representations such as CSI spectrograms can inadvertently expose sensitive information, including user identity, location, and membership, motivating formal privacy guarantees. In this paper, we study differentially private (DP) feature release for wireless sensing and propose an adaptive privacy budget allocation mechanism tailored to the highly non-uniform structure of CSI time-frequency representations. Our pipeline converts CSI to bounded spectrogram features, applies sensitivity control via clipping, estimates task-relevant importance over the time-frequency plane, and allocates a global privacy budget across spectrogram blocks before injecting calibrated Gaussian noise. Experiments on multi-user activity sensing (WiMANS), multi-person 3D pose estimation (Person-in-WiFi 3D), and respiration monitoring (Resp-CSI) show that adaptive allocation consistently improves the privacy-utility frontier over uniform perturbation under the same privacy budget. Our method yields higher accuracy and lower error while substantially reducing empirical leakage in identity and membership inference attacks.

</details>


### [90] [Symmaries: Automatic Inference of Formal Security Summaries for Java Programs](https://arxiv.org/abs/2512.20396)
*Narges Khakpour,Nicolas Berthier*

Main category: cs.CR

TL;DR: 提出一种可扩展、模块化且可靠的自动构建 Java 字节码安全规范的方法，生成方法摘要（method summaries），并通过 Symmaries 工具实现，对大型应用具备可扩展性，具有与堆模型相关的精度表现，且对终止无关性给出健全性证明。


<details>
  <summary>Details</summary>
Motivation: 为静态分析工具提供可供消费的安全规范，帮助开发者在库复用时评估安全性；通过形式化的摘要提升对方法行为的可理解性与可验证性，降低分析成本并提升可扩展性。

Method: 提出一种可扩展、模块化且健全的方法，自动生成方法摘要，摘要描述方法在何种条件下可安全调用、信息流和别名更新等行为。实现工具为 Symmaries，能够对 Java API 库提取安全规范，并在真实大型应用上评估规模性。方法包含对堆模型的处理以影响分析精度，并证明终止无关性下的安全性。

Result: 实验表明该工具可对拥有数十万行代码的应用进行分析，具备可观的精度，且在不同堆模型下精度表现存在差异。对 Java API 库的分析以及对大型真实应用的应用均验证了方法的可扩展性与实用性。

Conclusion: 提出的 Symmaries 框架能够以可扩展、模块化且有保证的方式自动生成安全摘要，促进静态分析和库复用时的安全评估。

Abstract: We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.

</details>


### [91] [iblock: Accurate and Scalable Bitcoin Simulations with OMNeT++](https://arxiv.org/abs/2512.20402)
*Niccolò Scatena,Pericle Perazzo,Giovanni Nardini*

Main category: cs.CR

TL;DR: 提出 iblock：一个用于 OMNeT++ 的高效 C++ 比特币仿真库，显示在同等细节下比现有高层语言实现更高效，且便于与其他 OMNeT++库集成，能够模拟常规比特币与自私挖矿等场景并产出与理论一致的结果。


<details>
  <summary>Details</summary>
Motivation: 提供一个高性能、可扩展且易于集成的比特币仿真工具，以支持在 OMNeT++ 环境中进行深入、可重复的比特币协议研究，尤其是对自私挖矿等攻击向量的研究。

Method: 开发 iblock 为 C++ 库并用于 OMNeT++，与现有区块链仿真器进行对比评估性能，在丰富的仿真细节层级下测试。通过在常规比特币操作和自私挖矿等场景中运行仿真，验证结果的正确性和一致性。

Result: 在相同仿真细节下，iblock 的性能优于 state-of-the-art 区块链仿真器；与 OMNeT++ 其他库的集成能力提升仿真细节表达能力；仿真结果与理论期望一致。

Conclusion: iblock 为 OMNeT++ 下高性能、可扩展的比特币仿真框架，支持详细场景与理论行为的验证，适合开展更丰富的研究实验。

Abstract: This paper proposes iblock, a comprehensive C++ library for Bitcoin simulation, designed for OMNeT++. iblock offers superior efficiency and scalability with respect to state-of-the-art simulators, which are typically written in high-level languages. Moreover, the possible integration with other OMNeT++ libraries allows highly detailed simulations. We measure iblock's performance against a state-of-the-art blockchain simulator, proving that it is more efficient at the same level of simulation detail. We also validate iblock by using it to simulate different scenarios such as the normal Bitcoin operation and the selfish mine attack, showing that simulation results are coherent with theoretical expectations.

</details>


### [92] [ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected](https://arxiv.org/abs/2512.20405)
*Kanchon Gharami,Sanjiv Kumar Sarkar,Yongxin Liu,Shafika Showkat Moni*

Main category: cs.CR

TL;DR: 提出对大语言模型在学术论文写作与评审中的潜在风险及对抗策略；展示注入隐藏提示以操控评审，以及“注入-检测”机制以揭示LLM生成的评审；强调编辑警惕以维护科研评审的可信性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在写作与评审中的广泛使用，论文可能失去真实原创性、产生伪造或偏见结果，并误导后续研究；需要探讨攻击和防御两端的风险与缓解路径。

Method: 在攻击方面，演示作者如何在PDF中注入隐藏提示以引导评审给予过分积极的反馈；在防御方面，提出‘注入-检测’策略，即在论文中嵌入看不见的触发提示，若评审重复或对这些触发做出反应即可识别该评审是由LLM生成。

Result: 给出设计思路、对模型行为的预期以及部署伦理防护要点；初步展示LLM对学术评审的脆弱性以及编辑者如何通过警觉来恢复评审信任。

Conclusion: 强调需要提高编辑对LLM影响的意识，建立更健全的评审制度，以防止受LLM影响的研究误导科学发展。

Abstract: Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical systems. This research explores both the offensive and defensive sides of this growing threat. On the attack side, we demonstrate how an author can inject hidden prompts inside a PDF that secretly guide or "jailbreak" LLM reviewers into giving overly positive feedback and biased acceptance. On the defense side, we propose an "inject-and-detect" strategy for editors, where invisible trigger prompts are embedded into papers; if a review repeats or reacts to these triggers, it reveals that the review was generated by an LLM, not a human. This method turns prompt injections from vulnerability into a verification tool. We outline our design, expected model behaviors, and ethical safeguards for deployment. The goal is to expose how fragile today's peer-review process becomes under LLM influence and how editorial awareness can help restore trust in scientific evaluation.

</details>


### [93] [ARBITER: AI-Driven Filtering for Role-Based Access Control](https://arxiv.org/abs/2512.20535)
*Michele Lorenzo,Idilio Drago,Dario Salvadori,Fabio Romolo Vayr*

Main category: cs.CR

TL;DR: 提出一个面向RAG的RBAC系统（\our），通过分层输入/输出验证、面向角色的检索和生成后事实检查，在无需微调分类器的前提下，借助少量提示的LLM实现快速部署和角色更新，实证显示在389个查询上的过滤准确性85%、F1分数89%，接近传统RBAC。


<details>
  <summary>Details</summary>
Motivation: 在动态企业环境中，文档包含对某些用户组不可披露的信息，传统RBAC难以自适应；同时，在RAG/LLM场景中，LLM可能因提示截断、分类错误或上下文丢失而泄露敏感信息，亟需一个在RAG中可用的RBAC方案。

Method: 提出\our，具备分层输入/输出验证、面向角色的检索和生成后事实检查。与传统RBAC解决方案不同，\our使用在few-shot设置下通过提示式引导的LLMs来实现，便于快速部署和角色更新。

Result: 在389个查询、合成数据集上评估，\our达到85%准确率和89%F1分数的查询筛选性能，与传统RBAC解决方案接近。

Conclusion: 表明在RAG系统中实际部署RBAC的成熟度正在提高，能够支持动态企业环境的需求。

Abstract: Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \our, a system designed to provide RBAC in RAG systems. \our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\% accuracy and 89\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments.

</details>


### [94] [Making Sense of Private Advertising: A Principled Approach to a Complex Ecosystem](https://arxiv.org/abs/2512.20583)
*Kyle Hogan,Alishah Chator,Gabriel Kaptchuk,Mayank Varia,Srinivas Devadas*

Main category: cs.CR

TL;DR: 本论文主张广告生态系统的端到端隐私需要考虑全局组合效应，指出对私人广告的单一隐私概念无法在整体生态中有效防泄露；并证明在有用广告生态中，完美隐私不可实现，需以“敏感数据”概念重塑隐私框架，提出新思路以使隐私属性与用户隐私诉求对齐。


<details>
  <summary>Details</summary>
Motivation: 现有隐私概念多聚焦于单一协议的保护，未能在广告生态链条的多方参与和信息流动中实现组合性隐私保障；广告商对受众信息进行市场研究的期望导致潜在信息泄露，导致对端到端隐私的重新评估。

Method: 对广告生态的端到端流程进行建模；分析隐私在系统层面的组合性；给出“完美隐私不可实现”的理论证明；在特定情境下引入“敏感数据”概念，讨论新型隐私保护子系统设计原则。

Result: 理论性结论显示信息泄露在广告生态中是不可避免的；单一隐私理念难以覆盖全局隐私需求；提出以敏感数据为核心的研究方向，以更贴近用户隐私偏好来设计子系统。

Conclusion: 需要从根本上重新设计隐私框架，使端到端广告系统的隐私特性与公众的隐私期望相一致，构建以敏感数据为基础的新方法与设计原则。

Abstract: In this work, we model the end-to-end pipeline of the advertising ecosystem, allowing us to identify two main issues with the current trajectory of private advertising proposals. First, prior work has largely considered ad targeting and engagement metrics individually rather than in composition. This has resulted in privacy notions that, while reasonable for each protocol in isolation, fail to compose to a natural notion of privacy for the ecosystem as a whole, permitting advertisers to extract new information about the audience of their advertisements. The second issue serves to explain the first: we prove that \textit{perfect} privacy is impossible for any, even minimally, useful advertising ecosystem, due to the advertisers' expectation of conducting market research on the results.
  Having demonstrated that leakage is inherent in advertising, we re-examine what privacy could realistically mean in advertising, building on the well-established notion of \textit{sensitive} data in a specific context. We identify that fundamentally new approaches are needed when designing privacy-preserving advertising subsystems in order to ensure that the privacy properties of the end-to-end advertising system are well aligned with people's privacy desires.

</details>
