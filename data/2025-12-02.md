<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 24]
- [cs.LG](#cs.LG) [Total: 97]
- [cs.IT](#cs.IT) [Total: 11]
- [eess.SY](#eess.SY) [Total: 24]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.NI](#cs.NI) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Joint Estimation of Sea State and Vessel Parameters Using a Mass-Spring-Damper Equivalence Model](https://arxiv.org/abs/2511.21997)
*Ranjeet K. Tiwari,Daniel Sgarioto,Peter Graham,Alexei Skvortsov,Sanjeev Arulampalam,Damith C. Ranasinghe*

Main category: eess.SP

TL;DR: 无需事先传递函数知识的海况与船舶参数联合估计方法；通过伪质量-弹簧阻尼建模、时变波浪激励、平方根立方卡曼滤波与后验CRLB验证，在仿真与高保真仿真数据上实现接近完全传递函数知识条件下的波谱估计。


<details>
  <summary>Details</summary>
Motivation: 在海况估计与船舶参数辨识中，传递函数往往未知、时变或难以获取，传统方法对先验传递函数的依赖限制了鲁棒性与适用性。因此需要一种无需预先定义传递函数的联合估计框架，并能处理时间变化的外部激励。

Method: 建立伪质量-弹簧-阻尼动力模型，将波浪激励视为时变输入，推导动态方程与统计一致的过程噪声协方差；使用平方根立方卡曼滤波进行传感器数据融合；推导后验Cramer–Rao下界以评估估计性能。

Result: 通过大量蒙特卡罗仿真和高保真验证仿真数据，估算得到的波谱与在已知完整传递函数条件下的方法一致。

Conclusion: 所提方法在无需传递函数知识的前提下实现对海况和船舶参数的鲁棒估计，结果与完全知识条件接近；未来工作可扩展到真实海况数据、评估对模型误差与非高斯噪声的敏感性，以及对计算效率的分析。

Abstract: Real-time sea state estimation is vital for applications like shipbuilding and maritime safety. Traditional methods rely on accurate wave-vessel transfer functions to estimate wave spectra from onboard sensors. In contrast, our approach jointly estimates sea state and vessel parameters without needing prior transfer function knowledge, which may be unavailable or variable. We model the wave-vessel system using pseudo mass-spring-dampers and develop a dynamic model for the system. This method allows for recursive modeling of wave excitation as a time-varying input, relaxing prior works' assumption of a constant input. We derive statistically consistent process noise covariance and implement a square root cubature Kalman filter for sensor data fusion. Further, we derive the Posterior Cramer-Rao lower bound to evaluate estimator performance. Extensive Monte Carlo simulations and data from a high-fidelity validated simulator confirm that the estimated wave spectrum matches methods assuming complete transfer function knowledge.

</details>


### [2] [CUNEC: A Path Loss Model for Urban Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2511.22041)
*Thomas Choi,Yuning Zhang,Issei Kanno,Masaaki Ito,Andreas F. Molisch*

Main category: eess.SP

TL;DR: 提出 CUNEC——一个面向 urban non-stationary environments 的城市无线传播路损模型，考虑 AP-UE 相关性、街区几何和街道峡谷效应，按街道顺序分段建模，并基于大规模射线追踪进行参数化与验证，显著优于 alpha-beta 模型，且公开了含 3 万 AP 位点和 128 UE 位置的数据集。


<details>
  <summary>Details</summary>
Motivation: 在密集城市中，传统路损模型无法捕捉空间非平稳性、AP/UE 间相关性以及角落衍射和波导效应等城市特有传播现象，需更现实的模型来评估和优化城配无源MIMO系统。

Method: 将 AP-UE 路径按 street order 分段，将路损建模为城市几何的随机函数，嵌入空间相关阴影、角落衍射和街道峡谷波导效应等物理现象。参数来自大规模射线追踪，并在额外的纽约射线追踪与洛杉矶实测数据中进行验证。

Result: 与传统 alpha-beta 模型相比，在所研究的城市传播场景中，CUNEC 显著提高了路损预测精度。

Conclusion: CUNEC 为城市非平稳环境下的无穷路损建模提供了一种可行框架，提升 CF massive MIMO 系统的评估和设计能力；并发布了一个包含超过 3 万个 AP 位点和 128 个 UE 位置的开源数据集，支持可重复研究和未来工作。

Abstract: Accurate path loss (PL) modeling is essential for evaluating and optimizing cell-free massive MIMO systems, especially in dense urban environments where traditional models fail to capture the complexity of real-world propagation. This paper introduces CUNEC (Cell-free massive MIMO for Urban Non-stationary Environments with Correlations, a novel PL model that accounts for spatial non-stationarity, inter-access point (AP)/user equipment (UE) correlations, and urban-specific propagation phenomena such as corner diffraction and street canyon waveguiding.bCUNEC segments AP-UE paths by street order, models PL as a stochastic function of urban geometry, and integrates spatially correlated shadowing. The parameters are derived from large-scale ray tracing and validated against both additional ray tracing in New York, NY and real-world channel measurements in Los Angeles, CA. Compared to the conventional alpha-beta model, CUNEC significantly improves accuracy in the considered urban propagation scenarios. An open-source dataset comprising over 30,000 AP locations and 128 UE positions is also released to support reproducible research and future system development.

</details>


### [3] [Bistatic Passive Tracking via CSI Power](https://arxiv.org/abs/2511.22144)
*Zhongqin Wang,J. Andrew Zhang,Kai Wu,Kuangda Chen,Min Xu,Y. Jay Guo*

Main category: eess.SP

TL;DR: PowerSense: a real-time passive tracking framework using CSI power (phase-insensitive) for ISAC, achieving 0.4 m median tracking error indoor with bistatic LTE/WiFi signals, without prior calibration, and enabling refined micro-Doppler sensing.


<details>
  <summary>Details</summary>
Motivation: Accurate object tracking in ISAC is hampered by limited signal bandwidth, sparse antennas, and clock asynchronism in bistatic deployments. Relying on the phase of CSI is problematic due to time-varying distortions; a robust, low-complexity method using CSI power is desirable.

Method: Convert CSI to phase-independent power via a self-conjugate operation, then apply cascaded FFT to extract delay, AoA, and Doppler features. Perform object detection, outlier removal, and continuous trajectory estimation with an Extended Kalman Filter. Use estimated positions to refine micro-Doppler signatures for fine-grained motion sensing.

Result: Experiments with 3.1 GHz LTE and 5 GHz WiFi bistatic signals at 20 MHz bandwidth achieved a median tracking error of 0.4 m in indoor settings, without any pre-deployment calibration, while also producing clear micro-Doppler signatures.

Conclusion: PowerSense demonstrates real-time passive tracking using CSI power alone, avoiding CSI phase distortions, eliminating calibration needs, and enabling simultaneous high-resolution trajectory tracking and micro-Doppler analysis.

Abstract: Accurate object tracking in Integrated Sensing and Communication (ISAC) applications remains challenging due to limited signal bandwidth, sparse antenna arrays, and clock asynchronism inherent in bistatic transceiver deployments. This paper proposes PowerSense, a real-time passive tracking framework that operates directly on the power of Channel State Information (CSI). Although CSI phase carries fine-grained information, its time-varying distortions require complex processing, which introduces additional interference during tracking. Instead, this work solely relies on CSI power for accurate sensing. We first remove all CSI phase offsets through a self-conjugate operation, yielding phase-independent CSI power. A cascaded Fast Fourier Transform (FFT) is then applied to extract delay, angle-of-arrival (AoA), and Doppler features, followed by object detection, outlier removal, and continuous trajectory estimation via an Extended Kalman Filter (EKF). To enable fine-grained motion sensing, the estimated target positions are further used to extract refined micro-Doppler signatures. Using 3.1 GHz LTE and 5 GHz WiFi bistatic signals with a 20 MHz bandwidth, our indoor experiments achieve a median tracking error of 0.4 m, without performing any pre-deployment system calibration, while simultaneously extracting clear and unambiguous micro-Doppler signatures of the tracked target.

</details>


### [4] [A Model and Data Dual-driven Approach for Multitargets Detection under Mainlobe Jamming](https://arxiv.org/abs/2511.22201)
*Ruohai Guo,Jiang Zhu,Chengjie Yu,Zhigang Wang,Ning Zhang,Fengzhong Qu,Min Gong*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In modern radar systems, target detection and parameter estimation face significant challenges when confronted with mainlobe jamming. This paper presents a Diffusion-based Model and Data Dual-driven (DMDD) approach to estimate and detect multitargets and suppress structured jamming. In DMDD, the jamming prior is modeled through a score-based diffusion process with its score learned from the pure jamming data, enabling posterior sampling without requiring detailed knowledge of jamming. Meanwhile, the target signal is usually sparse in the range space, which can be modeled via a sparse Bayesian learning (SBL) framework, and hyperparameter is updated through the expectation-maximization (EM) algorithm. A single diffusion process is constructed for the jamming, while the state of targets are estimated through direct posterior inference, enhancing computational efficiency. The noise variance is also estimated through EM algorithm. Numerical experiments demonstrate the effectiveness of the proposed method in structured jamming scenarios. The proposed DMDD algorithm achieves superior target detection performance, compared with existing methods.

</details>


### [5] [Foundation Model for Intelligent Wireless Communications](https://arxiv.org/abs/2511.22222)
*Boxun Liu,Xuanyu Liu,Shijian Gao,Xiang Cheng,Liuqing Yang*

Main category: eess.SP

TL;DR: WiFo-2 is a sparse mixture-of-experts wireless foundation model pretrained on 11.6B CSI points, achieving zero-shot and strong downstream performance with minimal fine-tuning, and validated by a hardware prototype.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of narrow, data-hungry AI models in wireless systems and realize high spectral efficiency and reliability through a generalizable CSI foundation model.

Method: Architectural design as a sparse mixture of experts; large-scale pretraining on diverse CSI data (11.6B points); zero-shot evaluation on unseen configurations; lightweight fine-tuning for eight downstream tasks; hardware prototype for real-world deployment.

Result: Zero-shot performance matching or surpassing full-shot baselines on unseen configurations; strong performance across eight downstream tasks with minimal fine-tuning; reliable confidence estimates; functional hardware prototype demonstrating deployment feasibility and system gains.

Conclusion: WiFo-2 establishes a new state-of-the-art for CSI-based tasks and signals a paradigm shift toward foundation-model-driven AI in wireless systems.

Abstract: The evolution toward intelligent next-generation wireless systems promises unprecedented spectral efficiency and reliability but is hindered by a paradigm of narrow and data-hungry AI models. Breaking from this constraint, this work introduces WiFo-2, a revolutionary wireless foundation model that establishes a new state of the art for extensive channel state information (CSI)-based tasks. Uniquely architected as a sparse mixture of experts, WiFo-2 effectively manages heterogeneous data and tasks while enabling highly efficient inference. It is pretrained on a massive and diverse dataset of 11.6 billion CSI points, which enables the acquisition of profound and generalizable channel knowledge. WiFo-2 demonstrates remarkable zero-shot capabilities, not only matching but surpassing the full-shot performance of task-specific baselines on unseen configurations, all while providing reliable confidence estimates. Furthermore, the model achieves exceptional performance on eight key downstream tasks with minimal fine-tuning. A functional hardware prototype demonstrates its real-world deployment feasibility and significant system gains, highlighting WiFo-2's superiority and paving the way for a paradigm shift in AI-based wireless systems.

</details>


### [6] [Pinching-Antenna Systems-Assisted SWIPT: A Rate-Energy Trade-off Perspective](https://arxiv.org/abs/2511.22224)
*Qi Yang,Kai Liu,Jingjing Zhao,Kaiquan Cai,Xidong Mu,Yuanwei Liu*

Main category: eess.SP

TL;DR: PASS-assisted SWIPT optimizes rate-energy trade-off using a two-stage optimization for a single IU/EU and ε-constrained MOOP with PSO and convex optimization for multiple IUs/EUs under FDMA/TDMA/NOMA; TDMA with time-switching can outperform others in multiuser scenarios.


<details>
  <summary>Details</summary>
Motivation: Explore joint rate-energy optimization in passively pinching-antenna systems to enhance SWIPT performance and fairness across single and multiple user scenarios.

Method: Single IU/EU: formulate pinching beamforming to maximize rate and harvested energy; two-stage approach: (i) successive convex approximation (SCA) to minimize large-scale path loss; (ii) fine-tuning phase alignment. Multiple IUs/EUs: formulate MOOP to maximize minimum rate and minimum harvested energy; apply ε-constraint to convert to SOOPs; under each MAC protocol (FDMA/TDMA/NOMA), employ PSO for pinching beamforming and convex optimization for resource allocation.

Result: Simulation results show PASS significantly enlarges the rate-energy region compared to conventional fixed-position antennas for pinching beamforming; with time-switching, TDMA outperforms NOMA and FDMA in the multiuser scenario.

Conclusion: PASS is a promising framework for joint rate-energy optimization in SWIPT. The proposed two-stage and ε-constraint-based optimization with PSO and convex methods effectively balance rate and energy. TDMA with time-switching is particularly advantageous in multiuser settings.

Abstract: This paper investigates the rate-energy trade-off for pinching-antenna systems (PASS)-assisted simultaneous wireless information and power transfer (SWIPT) systems. Both the single information user (IU)/energy user (EU) and multiple IUs/EUs scenarios are considered.1) For the single IU/EU scenario, a pinching beamforming optimization problem is formulated for simultaneously maximizing data rate and harvested energy. To tackle this problem, a two-stage algorithm is proposed. Specifically, the successive convex approximation (SCA) method is first invoked for minimizing the large-scale path loss, which is followed by the fine-tuning method for the phase alignment. 2) For the multiple IUs/EUs scenario, three multiple access schemes are considered, i.e., frequency division multiple access (FDMA), time division multiple access (TDMA), and non-orthogonal multiple access (NOMA). The corresponding multi-objective optimization problem (MOOP) that simultaneously maximizes the minimum data rate and minimum harvested energy is formulated for ensuring users' fairness. To address this problem, we adopt the $ε$-constraint method to first convert the intractable MOOPs to single-objective optimization problems (SOOPs). Then, for the SOOP under each multiple access protocol, the particle swarm optimization (PSO) and convex optimization methods are adopted for solving the pinching beamforming and resource allocation problems, respectively. Simulation results unveil that: i) PASS can achieve a significantly superior rate-energy region compared to conventional fixed-position antenna systems for pinching beamforming; and ii) by exploiting the time-switching feature, TDMA can outperform both NOMA and FDMA for the multiple IUs/EUs scenario.

</details>


### [7] [A Bio-Inspired Whisker Sensor toward Underwater Flow Sensing in Darkness and Turbidity](https://arxiv.org/abs/2511.22353)
*Zheyi Hang,Denghan Xiong,Pengo Xie,Huan Hu*

Main category: eess.SP

TL;DR: 开发了一种生物灵感的胡须式传感器，将高量程因子硅应变片嵌入柔性PDMS基底，模仿海豹胡须，其传感响应线性、探测限低且耐久，具备在水下UUV平台上的鲁棒性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在水下探测中，现有传感器存在响应慢、探测阈值高、方向判别有限、封装复杂和长期稳定性差等问题，且在浑浊水域的导航和目标感知尤为挑战。因此需要一种灵敏、紧凑且耐用的水下流动传感方案。

Method: 设计并制造了一个生物灵感的胡须传感器：将高灵敏度的硅应变片嵌入柔性PDMS基底，模仿海豹胡须的几何与受力特性，以简化封装并提升灵敏度。

Result: 传感器呈现线性力-阻值响应，探测上限为0.27 mN；经历1万次加载循环后仍具稳定性，偏置漂移<2%；在水下偶极测试中实现频率匹配，呈现清晰的纵向和横向空间响应分布。

Conclusion: 该传感器为水下流动感知提供了一条鲁棒且可扩展的解决路径，适用于UUV平台的实际部署。

Abstract: Underwater flow sensing is critical for unmanned underwater vehicles (UUVs) and environmental monitoring, yet existing sensors often suffer from low responsiveness, high detection thresholds, limited directional discrimination, complex packaging, and poor long-term stability, especially for navigation and target perception in turbid and cluttered waters. Previous solutions based on traditional strain gauges with limited detection accuracy or doped silicon sensors with limited detection height have shown feasibility but still face challenges in scalability, robustness under harsh aquatic conditions, and calibration complexity. This work presents a bio-inspired whisker sensor that provides a balanced solution by embedding high-gauge-factor silicon strain gauges into a flexible PDMS base, mimicking seal whiskers to offer both high sensitivity and simplified packaging. The device exhibits a linear force-resistance response with a limit of detection of 0.27 mN, maintains stability after 10,000 loading cycles, and shows minimal offset drift of less than 2 percent. It also demonstrates frequency matching in underwater dipole tests with clear longitudinal and transverse spatial response patterns. These results indicate a robust and scalable route for underwater flow sensing on UUV platforms in practical deployments.

</details>


### [8] [Adaptive Dual-Windowing Strategies for Multi-Target Detection in OFDM ISAC](https://arxiv.org/abs/2511.22458)
*Ali Al Khansa,Youssef Bahannis*

Main category: eess.SP

TL;DR: 提出一种双窗宽带谱基算法的自适应检测框架，在OFDM ISAC系统中权衡分辨率与旁瓣抑制，通过双窗口并行估计，若两者一致则直接采用，冲突时再调用高性能 CSTC 以分辨，降低复杂度并提升高SNR下性能。


<details>
  <summary>Details</summary>
Motivation: 在OFDM ISAC中，现有方法通常使用单一固定窗与单一检测策略，难以在分辨率与旁瓣抑制之间灵活权衡，且固定策略在高SNR下可能导致复杂度与性能的折中。

Method: 提出双窗口周期图算法，分别使用一个优化分辨率的窗口和一个优化旁瓣抑制的窗口进行估计；对两者分别应用低复杂度检测（如 BSTC），再比较输出结果；若两者一致则直接采用分辨率优先的估计；若不一致则触发高性能检测 CSTC 以消解歧义，从而在保持高性能的同时降低整体复杂度。

Result: 数值结果显示，该方法在保持较高检测性能的同时显著降低复杂度，且在高信噪比条件下性能提升尤为明显。

Conclusion: 通过自适应双窗口策略，平衡分辨率与旁瓣抑制需求，在 OFDM ISAC 场景下实现高性能、低复杂度的目标检测。

Abstract: In Orthogonal Frequency Division Multiplexing (OFDM) Integrated Sensing and Communication (ISAC) systems, a key challenge is balancing sidelobe attenuation and resolution for multi-target detection scenarios. While windowing functions are typically used to manage this trade-off, state-of-the-art methods rely on a single, fixed window followed by a predefined detection strategy (e.g., Binary Successive Target Cancellation (BSTC) (low complexity) or Coherent Successive Target Cancellation (CSTC) (high performance)). This paper proposes a novel dual-window periodogram-based algorithm that leverages two complementary windows: one optimized for resolution and the other for sidelobe suppression. Then, a low-complexity detection algorithm (e.g., BSTC) is applied to both, and a decision mechanism compares the outputs. When results align, the resolution-optimized estimates are directly used; otherwise, high performance algorithm (e.g., CSTC) is triggered to resolve ambiguities. This adaptive approach dynamically balances the detection performance and the complexity, addressing limitations in existing fixed strategies. The Numerical results confirm that the proposed method achieves high performance while reducing the complexity, especially at a high Signal to Noise Ratio (SNR).

</details>


### [9] [Interference and Multipath Resilient ToA Estimation](https://arxiv.org/abs/2511.22629)
*António Barros,Christoph Studer*

Main category: eess.SP

TL;DR: 提出一种基于多天线自适应空间滤波和自动微分的高效ToA估计算法，在强多径干扰下可超分辨第一跳，且无需模型阶数估计，仿真显示优于相关法和JADE等。


<details>
  <summary>Details</summary>
Motivation: 室内多径和强干扰下ToA估计鲁棒性不足且计算复杂度高；现有方法对模型阶数敏感，难以在低复杂度条件下实现高分辨率。

Method: 利用多收发天线组合自适应空间滤波，并结合自动微分来实现对第一跳到达时刻的超分辨估计；不依赖模型阶数估计；在低计算成本条件下实现高分辨率。

Result: 基于射线追踪的室内通道仿真显示相对于常规相关估计和JADE等子空间技术具有显著性能提升。

Conclusion: 该方法为低复杂度下的鲁棒ToA估计提供了有效途径，适用于需要高分辨率时间定位的系统；未来工作可能包括实测验证和对不同环境的鲁棒性评估。

Abstract: We present a computationally-efficient algorithm for time-of-arrival (ToA) estimation that is robust under multipath propagation and strong interference. Our algorithm leverages multiple receive antennas to combine adaptive spatial filtering with autodifferentiation in order to super-resolve the tap of the first-arriving path at low computational complexity and without requiring model-order estimation. We use simulations with ray-traced indoor propagation channels to demonstrate significant performance improvements over conventional correlation-based ToA estimation methods and subspace techniques such as JADE.

</details>


### [10] [Advances in electromagnetic techniques for subsurface infrastructure detection: A comprehensive review of methods, challenges, and innovations](https://arxiv.org/abs/2511.22673)
*Arasti Afrasiabi,Farough Rahimzadeh,Alireza Keshavarzi*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This review paper explores the state-of-the-art in non-intrusive methods for detecting and characterising buried infrastructure, focusing on Electrical Resistivity Tomography (ERT), Infrared Thermography (IRT), and magnetometry, along with data fusion techniques and mathematical estimators. ERT and IRT offer distinct advantages in subsurface imaging, while magnetometry provides omnidirectional measurements ideal for detecting ferrous targets. Despite these benefits, each method has inherent limitations, such as challenges in depth estimation and difficulties in distinguishing between various subsurface objects. The integration of multiple sensing techniques through data fusion approaches has shown significant promise in overcoming these limitations and improving detection accuracy. Additionally, mathematical estimators, including Kalman filters and particle filters, play a crucial role in reducing noise and enhancing the precision of geophysical surveys. This review discusses the strengths, limitations, and future research needs of these techniques, offering a comprehensive understanding of their current and potential applications in buried infrastructure detection. The paper concludes by emphasising the importance of optimising sensor performance, refining fusion algorithms, and exploring hybrid models for real-time data processing in future research.

</details>


### [11] [Rethinking Signaling Design for ISAC: From Pilot-Based to Payload-Based Sensing](https://arxiv.org/abs/2511.22703)
*Yunxin Li,Ying Zhang,Christos Masouros,Sofie Pollin,Fan Liu*

Main category: eess.SP

TL;DR: ISAC signaling从 pilot 授权的感知转向数据有效载荷驱动的感知，嵌入于现有5G NR结构。先重用 pilots/参考信号实现感知，再通过数据载荷的星座整形、调制基、脉冲整形等手段扩展感知；讨论在全帧资源下的机会与权衡，调制与星座属性直接决定感知性能；给出NR V2I场景的原型可行性案例，示例如何通过参考信号和载荷回波降低开销、实现主动波束管理与切换。


<details>
  <summary>Details</summary>
Motivation: 推动6G对ISAC的落地，将感知能力嵌入到现有通信帧中，降低额外开销与新硬件需求；在5G NR框架内探讨可行的感知实现路径，提升场景覆盖与鲁棒性。

Method: 系统性综述与分析：首先探讨在5G NR结构中重用 pilots/reference signals 的感知实现；进一步提出通过数据载荷进行感知的设计路径，包括星座整形、调制基、脉冲整形等对感知的影响；对感知与通信的资源分配与权衡进行讨论；通过一个NR V2I场景的原型案例，展示参考信号与载荷回波结合的可行性与收益。

Result: 梳理出在ISAC中从 pilote基感知向数据载荷感知的迁移所带来的机会与权衡；明确星座与调制选择等对感知性能的直接决定作用；通过V2I案例展示在5G NR中降低信令开销、实现主动波束管理的可行性。

Conclusion: ISAC信令设计可从以Pilot为核心的感知转向以数据载荷为核心的感知框架，并在5G NR内具备可行性和潜在收益，需关注设计权衡、实现复杂度和对现有NR编解码、同步与干扰环境的影响。

Abstract: Integrated Sensing and Communications (ISAC) is emerging as a key enabler for 6G networks, with signaling design at the core of its evolution. This paper reviews the paradigm shift of ISAC signaling designs from pilot-aided sensing to data payload-based approaches, with a particular focus on how these techniques can be realized within existing 5G NR structures. We commence with the reuse of pilots and reference signals that exploit existing 5G New Radio (NR) structures for sensing. Then, we extend to more advanced approaches that integrate the data payload through novel constellation shaping, modulation bases, and pulse shaping filters. We highlight the opportunities and tradeoffs that arise when extending sensing from sparse pilot and reference signal resources to the full communication frame, emphasizing how constellation properties and modulation choices directly determine sensing performance. To illustrate practical feasibility, a case study on sensing-assisted NR Vehicle-to-Infrastructure (V2I) networks demonstrates how exploiting both reference signals and payload echoes can reduce signaling overhead and enable proactive beam management and handover.

</details>


### [12] [FPGA-Enabled Modulo ADC with x100 Dynamic-Range Expansion: Hardware Design and Performance Evaluation](https://arxiv.org/abs/2511.22752)
*Zeyuan Li,Wenyi Yan,Lu Gan,Guoquan Li,Hongqing Liu*

Main category: eess.SP

TL;DR: FPGA/SoC-like modulo ADC platform enables HDR signal capture with >100x dynamic-range expansion at 400 kHz bandwidth, plus on-board real-time recovery and benchmarking of reconstruction algorithms.


<details>
  <summary>Details</summary>
Motivation: Conventional ADCs clip HDR signals; modulo ADCs fold input before quantization, enabling reconstruction of high-dynamic-range waveforms. A practical hardware platform is needed to evaluate HDR performance and benchmark reconstruction algorithms.

Method: A mixed-signal architecture combining a precision analog front end with a 200-MHz FPGA control loop. It uses multi-bit updates and digital under-compensation calibration to ensure stable folding and accurate feedback. The implementation is SoC-like with on-board real-time recovery for HDR evaluation.

Result: The platform achieves more than a hundred-fold dynamic-range expansion within a 400-kHz bandwidth while maintaining fidelity comparable to a conventional ADC.

Conclusion: This work provides a compact, practical HDR acquisition and evaluation framework, enabling on-board real-time recovery and benchmarking of state-of-the-art reconstruction algorithms for modulo ADCs.

Abstract: Conventional analog-to-digital converters (ADCs) fail to capture high-dynamic-range (HDR) signals due to clipping. Modulo ADCs circumvent this limitation by folding the input prior to quantization and algorithmically reconstructing the original waveform. This work presents a field-programmable gate array (FPGA)-based modulo ADC platform for systematic HDR performance evaluation. The mixed-signal architecture integrates a precision analog front end with a 200-MHz FPGA control loop that incorporates multi-bit updates and digital under-compensation calibration, ensuring stable folding and accurate feedback generation. The system achieves more than a hundred-fold dynamic-range expansion within a 400-kHz bandwidth while maintaining fidelity comparable to that of a conventional ADC. A system-on-chip (SoC)-like implementation enables on-board real-time recovery and supports benchmarking of state-of-the-art reconstruction algorithms, providing a compact and practical framework for HDR signal acquisition and evaluation.

</details>


### [13] [Moduli Selection in Robust Chinese Remainder Theorem: Closed-Form Solutions and Layered Design](https://arxiv.org/abs/2511.22757)
*Wenyi Yan,Lu Gan,Hongqing Liu,Shaoqing Hu*

Main category: eess.SP

TL;DR: 提出鲁棒模系统的模数设计框架（RCRT）用于小模数数目L=2,3,4，给出在动态范围与模量界限约束下的最大鲁棒裕度的精确解。提出一个Fibonacci风格的分层构造（仅对L=2适用），实现可预测的鲁棒解码层数K，并分析各层之间的鲁棒性-范围演化，给出成功概率的闭式估计。适用于子Nyquist采样、相位展开、距离测量、模ADC与RNS加速器等应用，构建了一个通用的RCRT模数设计理论。


<details>
  <summary>Details</summary>
Motivation: 在存在有界扰动的场景下，如何选取模数以在给定动态范围和模数约束下最大化鲁棒性，是实现鲁棒信息传输和计算的关键。RCRT为分布在不同模量上的剩余模值提供容错性，但模数设计仍不足以系统化、可预测地控制鲁棒性与范围的权衡。

Method: 对小规模模数组(L=2,3,4)给出能实现最大鲁棒裕度的精确解；提出仅对L=2有效的Fibonacci风格分层构造，用以产生恰好K层鲁棒解码，分析各层之间的鲁棒性与动态范围随层数的演化；给出在常见数据与噪声模型下的成功概率的闭式表达式。

Result: 获得L=2,3,4的精确最优解，及L=2的分层构造实现的K层鲁棒解码；建立鲁棒性与动态范围随层数变化的定量关系；给出成功概率的封闭式估计公式，便于在实际条件下快速评估设计性能。

Conclusion: 提出了一般性的RCRT模数设计理论，弥补了以往偏重算法实现的研究，展示了鲁棒模数设计在多个信息处理领域的普适性与应用潜力，强调分层设计在可控鲁棒性与动态范围之间的权衡作用。

Abstract: We study the fundamental problem of \emph{moduli selection} in the Robust Chinese Remainder Theorem (RCRT), where each residue may be perturbed by a bounded error. Consider $L$ moduli of the form $m_i = Γ_i m$ ($1 \le i \le L$), where $Γ_i$ are pairwise coprime integers and $m \in \mathbb{R}^+$ is a common scaling factor. For small $L$ ($L = 2, 3, 4$), we obtain exact solutions that maximize the robustness margin under dynamic-range and modulus-bound constraints. We also introduce a Fibonacci-inspired \emph{layered} construction (for $L = 2$) that produces exactly $K$ robust decoding layers, enabling predictable trade-offs between error tolerance and dynamic range. We further analyze how robustness and range evolve across layers and provide a closed-form expression to estimate the success probability under common data and noise models. The results are promising for various applications, such as sub-Nyquist sampling, phase unwrapping, range estimation, modulo analog-to-digital converters (ADCs), and robust residue-number-system (RNS)-based accelerators for deep learning. Our framework thus establishes a general theory of moduli design for RCRT, complementing prior algorithmic work and underscoring the broad relevance of robust moduli design across diverse information-processing domains.

</details>


### [14] [EMF-Compliant Power Control in Cell-Free Massive MIMO: Model-Based and Data-Driven Approaches](https://arxiv.org/abs/2511.23357)
*Sergi Liesegang,Stefano Buzzi*

Main category: eess.SP

TL;DR: CF-mMIMO系统在EMF限制下进行功率控制以最大化最小用户速率，提出基于模型和数据驱动的解决方案，覆盖上行和下行。模型驱动在下行使用序列凸优化和对数和近似；上行用传统方法。数据驱动采用端到端和深展开。结果显示模型驱动能满足EMF约束并保持良好性能，数据驱动在显著降低计算复杂度的同时可接近模型驱动性能。


<details>
  <summary>Details</summary>
Motivation: 随着无线数据需求快速增长，EMF暴露成为重要关注点，需要在确保公允性和系统性能的同时，满足EMF暴露上限，通过智能功率分配解决（CF-mMIMO）问题。

Method: 提出两类解决方案：1) 基于模型的方法：下行采用序列凸优化和log-sum-exp近似来处理最小值目标；上行使用常规优化技巧。2) 数据驱动方法：包括端到端架构和深展开（deep unfolding），以降低复杂度并逼近模型驱动性能。

Result: 大量数值结果表明，模型驱动解能够在满足EMF约束的前提下实现较强的性能；数据驱动方法能够在显著降低计算复杂度的同时，紧贴模型驱动的性能。

Conclusion: 在CF-mMIMO系统中，提出的EMF约束下的功率控制方案既能实现良好系统性能又能确保合规性；同时，数据驱动方案提供了有效的计算节省与近似最优的权衡，适用于实际部署。

Abstract: The impressive growth of wireless data networks has recently led to increased attention to the issue of electromagnetic pollution and the fulfillment of electromagnetic field (EMF) exposure limits. This paper tackles the problem of power control in user-centric cell-free massive multiple-input-multiple-output (CF-mMIMO) systems under EMF constraints. Specifically, the power allocation maximizing the minimum data rate across users is derived for both the uplink and the downlink. To solve such optimization problems, two approaches are proposed, i.e., model-based and data-driven. The proposed model-based solutions for the downlink utilize successive convex optimization and the log-sum-exp approximation for the minimum of a discrete set, whereas ordinary techniques are employed for the uplink. With regard to data-driven solutions, solutions based on both end-to-end architectures and deep unfolding techniques are explored. Extensive numerical results confirm that the proposed model-based solutions effectively fulfill the EMF constraints while ensuring very good performance; moreover, the results show that the proposed data-driven approaches can tightly approximate the performance of model-based solutions but with much lower computational complexity.

</details>


### [15] [RIS-Assisted Physical Layer Security: Artificial Noise-Driven Optimization and Measurements](https://arxiv.org/abs/2511.22910)
*Ahmet Muaz Aktas,Sefa Kayraklik,Sultangali Arzykulov,Galymzhan Nauryzbayev,Ibrahim Hokelek,Ali Gorcin*

Main category: eess.SP

TL;DR: AN驱动的RIS安全通信方案：将RIS分为两段分别引导信号与人工噪声，通过迭代与DFT优化相位并分配CS/AN功率，以最大化秘密容量并抑制Eve的信道，结合仿真与SDR测试验证可行性。


<details>
  <summary>Details</summary>
Motivation: RIS在提升覆盖、能效和物理层安全方面潜力巨大，但要在实际场景中实现高效且离散相位的PLS优化需解决耦合的相位/功率分配问题。将人工噪声与RIS相结合，利用分段设计可同时提升Bob信道和干扰Eve，提升 secrecy容量。

Method: 将RIS划分为两段：一段指向合法用户Bob的信号，另一段指向eavesdropper Eve的人工噪声。提出基于迭代优化与离散傅里叶变换（DFT）的相位移优化算法，并进行CS/AN功率分配以最大化 secrecy capacity（在限制Eve信道容量的前提下）。通过仿真与基于软件定义无线电的测试平台验证算法性能。

Result: 所提出的方法在仿真与测试环境中均显示出对秘密容量的显著提升，验证了AN驱动的RIS在实际部署中的潜力及可行性。

Conclusion: 将人工噪声与RIS分段设计相结合的PLS框架具备良好应用前景，未来工作可扩展至多用户/多天线场景、考虑CSI/硬件不确定性以及更广泛的实际场景。

Abstract: Reconfigurable intelligent surface (RIS) has emerged as a key enabler for providing signal coverage, energy efficiency, reliable communication, and physical layer security (PLS) in next-generation wireless communication networks. This paper investigates an artificial noise (AN)-driven RIS-assisted secure communication system. The RIS is partitioned into two segments, where the first segment is configured to direct the communication signal (CS) toward the legitimate user (Bob), and the other one is configured to steer the AN toward the eavesdropper (Eve). To this end, iterative and discrete Fourier transform-based algorithms are developed for practical RIS phase shift optimization. The power allocation between the CS and the AN signals is optimized in such a way that the secrecy capacity (SC) is maximized while limiting Eve's channel capacity. The proposed PLS framework is evaluated through both simulations and software defined radio based testbed experiments. The results demonstrate promising improvements in the SC, highlighting the potential of AN-driven RIS-assisted PLS for practical deployments.

</details>


### [16] [DoA Estimation with Sparse Arrays: Effects of Antenna Element Patterns and Nonidealities](https://arxiv.org/abs/2511.23028)
*Niko Lindvall,Mikko Heino,Robin Rajamäki,Mikko Valkama,Visa Koivunen*

Main category: eess.SP

TL;DR: 方向性天线单元的复杂增益模式和方向到达估计中的非理想性对DoA估计的影响显著，采用稀疏阵列相比经典均匀线阵在相同单元数下可显著提升DoA精度与可操作SNR范围；在两源MUSIC估计场景中，稀疏阵列比全向均匀阵的平均定位误差降低>90%，固定RMSE下，阵列灵敏度提升可将单向覆盖距离提高4–15倍；在100°/120°视场下，Patch或Vivaldi元件的稀疏阵列表现最佳。


<details>
  <summary>Details</summary>
Motivation: 量化方向性天线元件的复杂增益和互耦非理想性对DoA估计的影响，并比较稀疏阵列与传统均匀线阵在真实EM建模（包括Patch和Vivaldi元件及互耦）下的性能差异

Method: 使用电磁仿真工具对Patch和Vivaldi天线元件进行真实建模（含互耦），在稀疏阵列与均匀线阵条件下，利用MUSIC算法对两个源进行DoA估计，评估平均方向找错、可操作SNR范围及固定RMSE下的一路覆盖距离；视场分别为100°和120°。

Result: 在稀疏阵列配置下，DoA估计精度和可操作SNR范围的性能显著提升；在相同单元数（8个）条件下，对两源的MUSIC估计中，使用稀疏阵列可实现相比全向无方向阵的平均方向查找误差降低>90%；在固定角度RMSE下，阵列灵敏度的提升可带来4–15倍的单向覆盖距离增益；综合比较，Patch或Vivaldi元件在100°或120°视场下的稀疏阵列表现最佳。

Conclusion: 结合方向性天线元件的EM级建模与稀疏阵列设计，能显著提升DoA估计性能与覆盖能力；对于需要宽视场的DoA系统，使用稀疏阵列配合Patch或Vivaldi元件是一个有效的设计方向。

Abstract: This paper studies the effects of directional antenna element complex gain patterns and nonidealities in direction of arrival (DoA) estimation. We compare sparse arrays and classical uniform linear arrays, harnessing EM simulation tools to accurately model the electromagnetic behavior of both patch and Vivaldi antenna element including mutual coupling effects. We show that with sparse array configurations, the performance impacts are significant in terms of DoA estimation accuracy and operable SNR ranges. Specifically, in the scenarios considered, both the usage of directional antenna elements and a sparse array result in over 90% reduction in average direction finding error, compared to a uniform omnidirectional array with the same number of elements (in this case eight), when estimating the directions of two sources using the MUSIC algorithm. For a fixed angular RMSE, the improvements in array sensitivity are shown to yield a 4 to 15-fold increase in one-way coverage distance (assuming free-space path loss). Among the studied options, the best performance was obtained using sparse arrays with either patch or Vivaldi elements for field of views of 100$^\circ$ or 120$^\circ$, respectively.

</details>


### [17] [Harnessing Chaotic Signals for Wireless Information and Power Transfer](https://arxiv.org/abs/2511.23049)
*Priyadarshi Mukherjee,Constantinos Psomas,Ioannis Krikidis*

Main category: eess.SP

TL;DR: 本论文研究混沌信号在无线功率传输(WPT)与并行信息与能量传输(SWIPT)中的应用，提出以Lorenz和Henon系统为代表的多维混沌信号，并设计基于DCSK的WPT接收架构和多天线SWIPT的波形方案，同时分析功率与速率的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着6G对连接性与自给自足的需求提升，海量设备中的自供电与能量 harvesting 需求凸显，混沌信号的随机性和对初始条件的敏感性可能提升WPT与SWIPT性能，并具备潜在的安全性与鲁棒性优势。

Method: 对多维混沌信号的WPT性能进行广义表征，采用Lorenz与Henon系统作为实例，提出基于DCSK的WPT接收架构，设计DCSK信号在多天线SWIPT中的传输波形，并考察速率-能量权衡。

Result: 实验观察表明混沌信号在WPT性能上优于现有基准方案；提出的DCSK接收架构与多天线波形设计可提升能量收集效率并实现更好的SWIPT性能；并分析了速率-能量权衡的影响。

Conclusion: 混沌信号在6G环境下对WPT与SWIPT具有潜在价值，所提出的DCSK-based方法和多天线设计为增强能量收获提供了可行路径，但需要进一步在更广泛场景中验证。

Abstract: Chaotic dynamical systems have attracted considerable attention due to their inherent randomness and high sensitivity to initial conditions, which makes them ideal for secure wireless communications. Beyond security, these same characteristics also make chaotic signals particularly effective for wireless power transfer (WPT) applications. On the other hand, connectivity along with self-sustainability are the two cornerstones of the upcoming sixth generation (6G) standard for radio communications. Consequently, with the massive increase in wireless devices and sensors, the concept of self-sustainable wireless networks is becoming more relevant. The aspect of WPT to the widely spread wireless devices and simultaneous wireless information and power transfer (SWIPT) among these devices will play a crucial role in the 6G communication systems. In this context, it has been experimentally observed that chaotic signals result in better WPT performance as compared to the existing benchmark schemes. Hence, in this paper, we characterize the generalized WPT performance of the multi-dimensional chaotic signals and present the use case of the Lorenz and the Henon chaotic systems. Moreover, we provide a novel differential chaos shift keying (DCSK)-based WPT receiver architecture ideal for enhanced energy harvesting (EH). Furthermore, we propose DCSK-based transmit waveform designs for multi-antenna SWIPT architectures and investigate the impact of the rate-energy trade-off. Our goal is to explore these aspects of the chaotic signals and discuss their relevance in the context of both WPT and SWIPT.

</details>


### [18] [What If They Took the Shot? A Hierarchical Bayesian Framework for Counterfactual Expected Goals](https://arxiv.org/abs/2511.23072)
*Mikayil Mahmudlu,Oktay Karakuş,Hasan Arkadaş*

Main category: eess.SP

TL;DR: 提出了一种带先验的分层贝叶斯xG模型，能够在相同情境下对球员进行特异化的xG估计，并提供反事实分析；在外部效度上与基线相关性良好，且接近XGBoost基准，具备可解释的射门专长轮廓，适用于球员评估与战术规划。


<details>
  <summary>Details</summary>
Motivation: 现有xG模型往往把球员视同 finishing 者，忽略了球员间的异质性与上下文影响。通过将专家领域知识融入分层贝叶斯框架，旨在提高对少样本球员的估计稳定性与可解释性，并提供可用于招募与战术决策的不确定性量化。

Method: 将贝叶斯逻辑回归与信息先验相结合，构建分层贝叶斯模型；使用StatsBomb 2015-16的9,970次射门和 Football Manager 2017 的评分来设定与球员相关的先验，进行带有上下文特征的xG估计；同时进行反事实分析（在相同情境下重新分配射门者）以及与弱先验基线、XGBoost等基准的比较。

Result: 分层模型相较于弱先验显著降低后验不确定性；外部效度良好：分层预测与基线预测的R^2≈0.75；XGBoost在StatsBomb xG的基准上达到R^2≈0.833；可解释的专长轮廓被揭示（如1V1、远射、第一触球等）；还能识别表现不佳球员的潜在能力；反事实分析显示例如Sansone对Berardi射门机会可额外产生+2.2xG，Vardy-Giroud替换呈现明显不对称效应（替换Vardy为Giroud约-7xG，反向替换约-1xG）。”

Conclusion: 提供一个带不确定性量化的球员评估工具，适用于球员选拔、招募与战术规划；并提出一个通用的方法论，适用于那些个体技能与情境因素共同决定绩效的领域。

Abstract: This study develops a hierarchical Bayesian framework that integrates expert domain knowledge to quantify player-specific effects in expected goals (xG) estimation, addressing a limitation of standard models that treat all players as identical finishers. Using 9,970 shots from StatsBomb's 2015-16 data and Football Manager 2017 ratings, we combine Bayesian logistic regression with informed priors to stabilise player-level estimates, especially for players with few shots. The hierarchical model reduces posterior uncertainty relative to weak priors and achieves strong external validity: hierarchical and baseline predictions correlate at R2 = 0.75, while an XGBoost benchmark validated against StatsBomb xG reaches R2 = 0.833. The model uncovers interpretable specialisation profiles, including one-on-one finishing (Aguero, Suarez, Belotti, Immobile, Martial), long-range shooting (Pogba), and first-touch execution (Insigne, Salah, Gameiro). It also identifies latent ability in underperforming players such as Immobile and Belotti. The framework supports counterfactual "what-if" analysis by reallocating shots between players under identical contexts. Case studies show that Sansone would generate +2.2 xG from Berardi's chances, driven largely by high-pressure situations, while Vardy-Giroud substitutions reveal strong asymmetry: replacing Vardy with Giroud results in a large decline (about -7 xG), whereas the reverse substitution has only a small effect (about -1 xG). This work provides an uncertainty-aware tool for player evaluation, recruitment, and tactical planning, and offers a general approach for domains where individual skill and contextual factors jointly shape performance.

</details>


### [19] [Joint Optimization of Pilot Length, Pilot Assignment, and Power Allocation for Cell-free MIMO Systems with Graph Neural Networks](https://arxiv.org/abs/2511.23128)
*Yao Peng,Tingting Liu,Chenyang Yang*

Main category: eess.SP

TL;DR: 提出尺寸泛化的 GNN 框架，联合优化 pilot length、pilot assignment 与功率以提升净谱效率，解决 pilot length 不确定性、one-to-many 映射和污染感知注意力等难题，提供双时尺度及单时尺度变体以降低推理时延，实验显示优于传统方法且具泛化性。


<details>
  <summary>Details</summary>
Motivation: 在用户中心的无线路征多天线系统中，pilot 污染显著降低谱效率，但固定 pilot 长度的联合优化难以权衡 Pilot overhead 与污染；需要端到端、对系统规模具泛化能力的学习方法。

Method: 提出 size-generalizable GNN，证明 pilot assignment 是一对多映射、引入特征增强解决学习困难；设计污染感知注意力机制；构建双时尺度 GNN（用于大尺度/小尺度信道的 Pilot assignment 与功率分配）及单时尺度版本以降低推理时延。

Result: 仿真结果显示相比现有方法，所提方法在净谱效率、训练复杂度和推理时间上均有提升，且对问题规模和信道具味具有良好泛化性。

Conclusion: 尺寸自适应、污染感知与多时尺度设计的 GNN 框架能够有效学习联合的 Pilot length、分配与功率策略，提升净谱效率并保持鲁棒性。

Abstract: In user-centric cell-free multi-antenna systems, pilot contamination degrades spectral efficiency (SE) severely. To mitigate pilot contamination, existing works jointly optimize pilot assignment and power allocation by assuming fixed pilot length, which fail to balance pilot overhead against the contamination. To maximize net-SE, we jointly optimize pilot length, pilot assignment, and power allocation with deep learning. Since the pilot length is a variable, the size of pilot assignment matrix is unknown during the optimization. To cope with the challenge, we design size-generalizable graph neural networks (GNNs). We prove that pilot assignment policy is a one-to-many mapping, and improperly designed GNNs cannot learn the optimal policy. We tackle this issue by introducing feature enhancement. To improve learning performance, we design a contamination-aware attention mechanism for the GNNs. Given that pilot assignment and power allocation respectively depend on large- and small-scale channels, we develop a dual-timescale GNN framework to explore the potential. To reduce inference time, a single-timescale GNN is also designed. Simulation results show that the designed GNNs outperform existing methods in terms of net-SE, training complexity, and inference time, and can be well generalized across problem scales and channels.

</details>


### [20] [Data-Efficient Motor Condition Monitoring with Time Series Foundation Models](https://arxiv.org/abs/2511.23177)
*Deyu Li,Xinyuan Liao,Shaowei Chen,Shuai Zhao*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Motor condition monitoring is essential for ensuring system reliability and preventing catastrophic failures. However, data-driven diagnostic methods often suffer from sparse fault labels and severe class imbalance, which limit their effectiveness in real-world applications. This paper proposes a motor condition monitoring framework that leverages the general features learned during pre-training of two time series foundation models, MOMENT and Mantis, to address these challenges. By transferring broad temporal representations from large-scale pre-training, the proposed approach significantly reduces dependence on labeled data while maintaining high diagnostic accuracy. Experimental results show that MOMENT achieves nearly twice the performance of conventional deep learning models using only 1\% of the training data, whereas Mantis surpasses state-of-the-art baselines by 22\%, reaching 90\% accuracy with the same data ratio. These results demonstrate the strong generalization and data efficiency of time series foundation models in fault diagnosis, providing new insights into scalable and adaptive frameworks for intelligent motor condition monitoring.

</details>


### [21] [Near-Field Channel Estimation and Joint Angle-Range Recovery in XL-MIMO Systems: A Gridless Super-Resolution Approach](https://arxiv.org/abs/2511.23187)
*Feng Xi,Dehui Yang*

Main category: eess.SP

TL;DR: 提出一种网格无关的近场通道估计框架，将XL-MIMO近场信道表示为二阶球波近似下的复指数叠加，通过提升到凸规划的正则化原子范数最小化（等价SDP）实现网格无关的角度估计和粗距离估计，并在精确球模型下用梯度非线性最小二乘进行细化，避免大规模网格搜索与代码本构建，且在稀疏多径场景中实现亚线性增长的 pilot 预算和高精度定位。


<details>
  <summary>Details</summary>
Motivation: 现有近场XL-MIMO通道估计多采用离散化的角-距网格，需构建大规模极化域代码本，易产生基误差且计算成本高。需要一种网格无关、低成本、可扩展的高精度近场估计方法，兼容稀疏多径和大阵列规模。

Method: 采用二阶球波近似将近场信道表示为与未知波形调制的复指数之和；这些波形落在一个公共的离散啁啾速率(DCR)子空间，维度≈Θ(√N)；通过提升将问题转化为凸规划，使用正则化原子范数最小化（等价于半正定规划）求解，得到网格无关的角度估计和粗略距离；再在真实球模型下以梯度基非线性最小二乘进行距离的细化。

Result: 在稀疏多径场景下， pilot预算随数组规模呈亚线性增长；避免基矩阵失配和二维网格搜索，实现高精度的角距估计与用户定位，仿真验证在典型近场场景下的通道重建与定位性能。

Conclusion: 所提网格无关近场估计框架为XL-MIMO的近场通道估计提供了一种低复杂度、无网格失配的高精度解决方案，能够在稀疏多径环境下实现可靠的角距估计与定位，具有潜在的实际应用价值。

Abstract: Existing near-field channel estimation methods for extremely large-scale MIMO (XL-MIMO) typically discretize angle and range parameters jointly, resulting in large polar-domain codebooks. This paper proposes a novel framework that formulates near-field channel estimation as a gridless super-resolution problem, eliminating the need for explicitly constructed codebooks. By employing a second-order approximation of spherical-wave steering vectors, the near-field channel is represented as a superposition of complex exponentials modulated by unknown waveforms. We demonstrate that these waveforms lie tightly in a common discrete chirp rate (DCR) subspace, with a dimension that scales as $Θ(\sqrt{N})$ for an $N$-element array. By leveraging this structure and applying a lifting technique, we reformulate the non-convex problem as a convex program using regularized atomic norm minimization, which admits an equivalent semidefinite program. From the solution to the convex program, we obtain gridless angle estimates and derive closed-form coarse range estimates, followed by refinement under the exact spherical model using gradient-based nonlinear least squares. The proposed method avoids basis mismatch and exhaustive two-dimensional grid searches while enabling accurate joint angle-range estimation with pilot budgets that scale sublinearly with array size in sparse multipath regimes. Simulations demonstrate accurate channel reconstruction and user localization across representative near-field scenarios.

</details>


### [22] [A Framework for Statistical Geometric Channel Model for ISAC Systems](https://arxiv.org/abs/2511.23201)
*Ali Waqar Azim,Ahmad Bazzi,Theodore S. Rappaport,Marwa Chafii*

Main category: eess.SP

TL;DR: 提出面向双棱系统的几何基统计ISAC框架，将信道分解为目标信道和背景信道；通过混合聚类扩展TR38.901，保持信道对称性和绝对时延对齐，便于感知参数估计。


<details>
  <summary>Details</summary>
Motivation: 为ISAC双基系统建立一个统一的统计-确定性混合信道模型，兼容感知与通信需求，且在时延对齐和信道对称性方面满足参数估计鲁棒性。

Method: 在TR38.901基础上提出混合聚类框架：将目标相关的多径建立为确定性簇（包括目标雷达截面和散射点），将其他背景声部建立为随机簇；保持信道对称性与绝对时延对齐；通过仿真在城市宏/微街区和室内工厂场景验证。

Result: 该模型在保留与TR38.901等效的通信性能（通过BER与信道容量评估）同时实现 sensing 能力，如定位距离误差和检测概率的ROC分析；在多场景下得到一致性结果。

Conclusion: 所提出的几何-统计混合ISAC框架为双基系统提供了统一的分析工具，兼顾感知与通信性能，且保持信道对称性与时延对齐，证明在实际场景中的可行性。

Abstract: This paper proposes a comprehensive framework for a geometry-based statistical model for integrated sensing and communication (ISAC) tailored for bistatic systems. Our dual-component model decomposes the ISAC channel into a target channel encompassing all multipath components produced by a sensing target parameterized by the target's radar cross-section and scattering points, and a background channel comprising all other propagation paths that do not interact with the sensing target. The framework extends TR38.901 via a hybrid clustering approach, integrating spatiotemporally consistent deterministic clusters with stochastic clusters to preserve channel reciprocity and absolute delay alignment for sensing parameter estimation. Extensive simulations across urban macro, urban micro, and indoor factory scenarios demonstrate that the model maintains communication performance parity with the standard TR38.901, validated through bit-error rate analysis obtained via simulated and measured ISAC channels and channel capacity assessment, while enabling sensing performance evaluation, such as target ranging error for localization and receiver operating characteristic curves for detection probability.

</details>


### [23] [Hierarchical Feature Integration for Multi-Signal Automatic Modulation Recognition](https://arxiv.org/abs/2511.23258)
*Yunpeng Qu,Yazhou Sun,Bingyu Hui,Jian Wang*

Main category: eess.SP

TL;DR: 提出了一种基于YOLO的层次特征融合框架（HIFI-YOLO），用于多信号同时检测与调制识别。并构建了一个适用于多信号共存场景的大规模AMR数据集，在真实传播条件下进行评估，显示了联合检测与调制识别的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动调制识别（AMR）多聚焦于单信号识别，忽略了实际信道中的多信号叠加及其前置检测过程；射频信号易受噪声干扰且谱特征变化大，需要在多信号场景下实现联合检测和调制识别以提升鲁棒性。

Method: 在YOLO框架内设计了层次特征融合（HIFI）模块以增强不同模块中特征的表征能力，并实现多信号的联合检测与调制识别；同时构建了一个覆盖数字与模拟调制、且考虑现实传播条件的多信号共存大规模数据集，用于评估该框架在真实场景中的性能。

Result: 在自建数据集上进行的大规模实验表明，HIFI-YOLO在多信号检测与调制识别上表现出色，能够作为一个联合方法提升检测与识别的综合性能。

Conclusion: HIFI-YOLO通过层次化特征融合显著提升了在多信号共存与嘈杂环境中的AMR能力，为联合检测与调制识别提供了一种有效的端到端解决方案，同时所构建的数据集也为后续多信号AMR研究提供了有力的评估平台。

Abstract: Automatic modulation recognition (AMR) is a crucial step in wireless communication systems, which identifies the modulation scheme from detected signals to provide key information for further processing. However, previous work has mainly focused on the identification of a single signal, overlooking the phenomenon of multiple signal superposition in practical channels and the signal detection procedures that must be conducted beforehand. Considering the susceptibility of radio frequency (RF) signals to noise interference and significant spectral variations, we propose a novel Hierarchical Feature Integration (HIFI)-YOLO framework for multi-signal joint detection and modulation recognition. Our HIFI-YOLO framework, with its unique design of hierarchical feature integration, effectively enhances the representation capability of features in different modules, thereby improving detection performance. We construct a large-scale AMR dataset specifically tailored for scenarios of the coexistence or overlapping of multiple signals transmitted through channels with realistic propagation conditions, consisting of diverse digital and analog modulation schemes. Extensive experiments on our dataset demonstrate the excellent performance of HIFI-YOLO in multi-signal detection and modulation recognition as a joint approach.

</details>


### [24] [Compensation of correlated autoregressive clock jitter in arrays of Analog-to-Digital Converters](https://arxiv.org/abs/2511.23351)
*Daniele Gerosa,Lauri Anttila,Thomas Eriksson*

Main category: eess.SP

TL;DR: 提出了一种针对MIMO ADC阵列时序抖动的联合跟踪与补偿方法，将抖动建模为一阶向量自回归(VAR(1))过程，并提出基于导频的卡尔曼平滑器进行跟踪与校正；仿真显示在多场景下显著降低抖动引起的失真。


<details>
  <summary>Details</summary>
Motivation: 模拟时钟抖动的传统标量高斯噪声模型忽略了时序相关性与通道间的空间相关性，导致难以跟踪与补偿。实际的ADC阵列存在跨通道的相关抖动，且具有时域色散特性，亟需一个能同时捕捉时序与空间相关性的模型与估计方法。

Method: 将抖动建模为耦合的一阶VAR过程，形成跨通道的时序相关性描述；利用导频信号建立卡尔曼平滑框架，对抖动及其在各通道的效应进行联合跟踪与补偿，旨在降低抖动对ADC输出的失真。

Result: 通过仿真验证，该方法在多种场景下显著降低了抖动引起的失真，显示了对MIMO ADC抖动的有效跟踪与补偿能力。

Conclusion: 将跨通道的抖动关系与时序相关性结合建模并通过卡尔曼平滑实现跟踪与补偿，能显著提升MIMO ADC阵列的抖动鲁棒性，具有理论新颖性与实际应用潜力，但也需关注实现代价、模型参数选择与实时性等挑战。

Abstract: In modern communication systems, the fidelity of analog-to-digital converters (ADCs) is limited by sampling clock jitter, i.e., small random timing deviations that undermine ideal sampling. Traditional scalar models often treat jitter as independent Gaussian noise, which makes it essentially untrackable, whereas real ADCs also exhibit temporally correlated (spectrally colored) imperfections. Moreover, spatial cross-correlations between channels in multiple-input multiple-output (MIMO) ADCs are commonly neglected. This paper addresses the joint tracking and compensation of random, cross-correlated timing errors in ADC arrays by modeling jitter as a coupled vector autoregressive process of order one (VAR(1)). We propose a pilot-tone-based Kalman smoother to track and compensate the jitter, and simulations demonstrate substantial reductions in jitter-induced distortion across diverse scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Artificial intelligence for methane detection: from continuous monitoring to verified mitigation](https://arxiv.org/abs/2511.21777)
*Anna Allen,Gonzalo Mateo-Garcia,Itziar Irakulis-Loitxate,Manuel Montesino-San Martin,Marc Watine,James Requeima,Javier Gorroño,Cynthia Randles,Tharwat Mokalled,Luis Guanter,Richard E. Turner,Claudio Cifarelli,Manfredi Caltagirone*

Main category: cs.LG

TL;DR: MARS-S2L 是一个基于机器学习的多光谱卫星影像 methane 漏排检测系统。经过对超过 8 万张图像的人工标注训练，在未见过的 697 个站点上实现了 78% 的气体羽流检测率、8% 的误报率，提供每两天一次的高分辨率检测并实现设施级归因。已在 20 个国家投入运营，发出 1,015 次通知，验证并永久缓解了六个持续排放点（包括利比亚的一个新发现点）。该结果展示了从卫星检测到可量化甲烷减排的可扩展路径。


<details>
  <summary>Details</summary>
Motivation: 甲烷是强效温室气体，排放对气候影响显著；少数大型排放点贡献较大，因此通过精准发现与定位实现对齐区域的高效减排具有重要价值。

Method: 使用多光谱卫星影像，并以超过8万张人工标注数据训练一个机器学习模型 MARS-S2L，实现对甲烷羽流的检测与定位，提供每两天一次的高分辨率检测，支持设施级归因与对新位点的扩展性评估。

Result: 在697个此前未知的站点上实现 78% 的羽流检测率，8% 的假阳性率；部署到20个国家，发出 1,015 次通知，促成六个持续排放点的验证性永久缓解，并发现利比亚的一个未知站点。

Conclusion: 展示了一个从卫星检测到实际减排干预的可扩展路径，强调了以公开数据驱动的高频、设施级别归因在全球甲烷治理中的潜力。

Abstract: Methane is a potent greenhouse gas, responsible for roughly 30\% of warming since pre-industrial times. A small number of large point sources account for a disproportionate share of emissions, creating an opportunity for substantial reductions by targeting relatively few sites. Detection and attribution of large emissions at scale for notification to asset owners remains challenging. Here, we introduce MARS-S2L, a machine learning model that detects methane emissions in publicly available multispectral satellite imagery. Trained on a manually curated dataset of over 80,000 images, the model provides high-resolution detections every two days, enabling facility-level attribution and identifying 78\% of plumes with an 8\% false positive rate at 697 previously unseen sites. Deployed operationally, MARS-S2L has issued 1,015 notifications to stakeholders in 20 countries, enabling verified, permanent mitigation of six persistent emitters, including a previously unknown site in Libya. These results demonstrate a scalable pathway from satellite detection to quantifiable methane mitigation.

</details>


### [26] [The Double-Edged Nature of the Rashomon Set for Trustworthy Machine Learning](https://arxiv.org/abs/2511.21799)
*Ethan Hsu,Harry Chen,Chudi Zhong,Lesia Semenova*

Main category: cs.LG

TL;DR: 现实世界的 ML 流水线往往产生一组近优解集合（Rashomon 集），这对信任度的各方面带来新影响：多样性既提升鲁棒性也提高信息泄露风险。


<details>
  <summary>Details</summary>
Motivation: 解释为什么真实场景中的近似最优模型集合会影响信任度评估，以及它对鲁棒性与隐私的潜在关系。

Method: 通过理论分析与对稀疏可解释模型（如稀疏决策树和线性模型）的实证研究，考察 Rashomon 集的鲁棒性、隐私保护与信息泄露之间的权衡。

Result: （1）单一模型层面：稀疏可解释模型有隐私保护的优势，但对对手攻击较脆弱；（2）Rashomon 集的多样性提供“反应式”鲁棒性：攻击破坏一个模型时，其他模型仍然保持准确；（3）Rashomon 集对小分布变动具有稳定性；（4）但多样性也增加信息泄露风险：披露更多近似模型会让攻击者获得训练数据的更丰富视图。

Conclusion: Rashomon 集在可信 ML 中既是资源也是风险，决定了信任性评估的边界，同时需权衡鲁棒性收益与信息暴露之间的博弈。

Abstract: Real-world machine learning (ML) pipelines rarely produce a single model; instead, they produce a Rashomon set of many near-optimal ones. We show that this multiplicity reshapes key aspects of trustworthiness. At the individual-model level, sparse interpretable models tend to preserve privacy but are fragile to adversarial attacks. In contrast, the diversity within a large Rashomon set enables reactive robustness: even when an attack breaks one model, others often remain accurate. Rashomon sets are also stable under small distribution shifts. However, this same diversity increases information leakage, as disclosing more near-optimal models provides an attacker with progressively richer views of the training data. Through theoretical analysis and empirical studies of sparse decision trees and linear models, we characterize this robustness-privacy trade-off and highlight the dual role of Rashomon sets as both a resource and a risk for trustworthy ML.

</details>


### [27] [Unsupervised Anomaly Detection for Smart IoT Devices: Performance and Resource Comparison](https://arxiv.org/abs/2511.21842)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: Isolation Forest (IF) outperforms One-Class SVM (OC-SVM) for unsupervised anomaly detection on TON_IoT thermostat data, offering higher accuracy, precision, recall, and F1, with a smaller computational footprint, making it suitable for edge IoT deployment.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of IoT expands attack surfaces and cybersecurity threats. Traditional signature-based anomaly detectors struggle with emerging and zero-day threats, necessitating effective unsupervised methods that can operate on resource-constrained IoT devices.

Method: An empirical study evaluating two unsupervised anomaly detection methods, Isolation Forest (IF) and One-Class SVM (OC-SVM), on the TON_IoT thermostat dataset. The evaluation uses standard classification metrics (accuracy, precision, recall, F1-score) and resource metrics (inference time, model size, peak RAM).

Result: IF consistently achieves higher detection performance (accuracy, precision, recall, F1-score) than OC-SVM and exhibits a notably smaller computational footprint (faster inference, smaller model, lower RAM usage). This suggests IF is more robust in high-dimensional and imbalanced IoT environments.

Conclusion: Isolation Forest is robust and practical for real-time anomaly detection on resource-constrained IoT edge devices, offering superior accuracy and efficiency over OC-SVM.

Abstract: The rapid expansion of Internet of Things (IoT) deployments across diverse sectors has significantly enhanced operational efficiency, yet concurrently elevated cybersecurity vulnerabilities due to increased exposure to cyber threats. Given the limitations of traditional signature-based Anomaly Detection Systems (ADS) in identifying emerging and zero-day threats, this study investigates the effectiveness of two unsupervised anomaly detection techniques, Isolation Forest (IF) and One-Class Support Vector Machine (OC-SVM), using the TON_IoT thermostat dataset. A comprehensive evaluation was performed based on standard metrics (accuracy, precision, recall, and F1-score) alongside critical resource utilization metrics such as inference time, model size, and peak RAM usage. Experimental results revealed that IF consistently outperformed OC-SVM, achieving higher detection accuracy, superior precision, and recall, along with a significantly better F1-score. Furthermore, Isolation Forest demonstrated a markedly superior computational footprint, making it more suitable for deployment on resource-constrained IoT edge devices. These findings underscore Isolation Forest's robustness in high-dimensional and imbalanced IoT environments and highlight its practical viability for real-time anomaly detection.

</details>


### [28] [Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics](https://arxiv.org/abs/2511.21848)
*Eric Leonardis,Akira Nagamori,Ayesha Thanawalla,Yuanjia Yang,Joshua Park,Hutton Saunders,Eiman Azim,Talmo Pereira*

Main category: cs.LG

TL;DR: A pipeline that converts neuroscience kinematics into a high-fidelity, biomechanics-based imitation-learning framework for forelimb reaching, showing that energy and velocity constraints improve EMG prediction.


<details>
  <summary>Details</summary>
Motivation: To understand how the brain controls the body by modeling sensorimotor transformations and embodied control, via a general-purpose, behavior-driven simulation platform with high-fidelity dynamics and neural architectures.

Method: Build a pipeline to translate lab-recorded kinematics into a musculoskeletal biomechanical model; use an imitation learning framework to train a dexterous forelimb reaching task in a simulated physics environment; leverage GPU acceleration (JAX) and Mujoco-MJX for fast training; currently achieving >1e6 training steps per second.

Result: Demonstrates that adding naturalistic energy and velocity constraints yields simulated musculoskeletal activity that better predicts real EMG signals; training efficiency is enhanced by GPU acceleration.

Conclusion: Energy and control constraints are critical to accurately modeling musculoskeletal motor control.

Abstract: The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.

</details>


### [29] [Lightweight ML-Based Air Quality Prediction for IoT and Embedded Applications](https://arxiv.org/abs/2511.21857)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: 对CO与NO2浓度预测进行比较：完整XGBoost在精度上优于轻量版，但轻量版在推理速度与存储占用方面具显著优势，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 探究全量与轻量化XGBoost回归模型在城市空气质量预测任务中的性能与资源消耗差异，以评估在物联网/嵌入式设备上的可部署性。

Method: 基于AirQualityUCI数据集（城市环境、一年数据），对两种XGBoost回归模型进行评估，衡量常用回归指标（MAE、RMSE、MBE、R2）及资源指标（推理时间、模型大小、峰值RAM）。

Result: 全量XGBoost在CO、NO2预测上具有更高的预测精度；轻量版尽管精度略逊，但在推理时间和模型存储需求方面显著降低，表现出在资源受限环境中的可行性。

Conclusion: 可以在嵌入式/物联网场景中部署轻量XGBoost实现实时空气质量监测，权衡精度与计算资源。

Abstract: This study investigates the effectiveness and efficiency of two variants of the XGBoost regression model, the full-capacity and lightweight (tiny) versions, for predicting the concentrations of carbon monoxide (CO) and nitrogen dioxide (NO2). Using the AirQualityUCI dataset collected over one year in an urban environment, we conducted a comprehensive evaluation based on widely accepted metrics, including Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Mean Bias Error (MBE), and the coefficient of determination (R2). In addition, we assessed resource-oriented metrics such as inference time, model size, and peak RAM usage. The full XGBoost model achieved superior predictive accuracy for both pollutants, while the tiny model, though slightly less precise, offered substantial computational benefits with significantly reduced inference time and model storage requirements. These results demonstrate the feasibility of deploying simplified models in resource-constrained environments without compromising predictive quality. This makes the tiny XGBoost model suitable for real-time air-quality monitoring in IoT and embedded applications.

</details>


### [30] [Closed-Loop Transformers: Autoregressive Modeling as Iterative Latent Equilibrium](https://arxiv.org/abs/2511.21882)
*Akbar Anbar Jafari,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 提出闭环预测原则并实现EqT，通过在潜在空间中对学习的能量函数进行梯度下降的迭代细化来纠正开环自回归模型的错误传播，从而在困难序列上提升推理和预测一致性。


<details>
  <summary>Details</summary>
Motivation: 开环自回归模型在序列中一旦前向计算后不再修正，易使错误累积，导致长距离推理、事实一致性和多步规划能力下降。需一个可以在生成阶段迭代更新潜在表示、达到自我一致性的机制。

Method: 在标准Transformer中加入Equilibrium Refinement Module，通过对一个学习得到的潜在能量函数进行梯度下降来实现迭代的自一致收敛。该能量函数促进双向预测一致性、情节记忆一致性与输出置信度的约束，且不需要外部监督；理论上将EqT视为潜在能量模型的近似MAP推断，且给出线性收敛性保证，明示对困难实例的改进优于一次性推断。该框架还将深度平衡模型、扩散语言模型和测试时训练等视为特例。

Result: 在二进制奇偶任务上的初步实验显示对挑战性序列平均提升3.28%，在标准Transformer接近随机表现的情形下提升可达8.07%；理论结果包括EqT对潜在能量模型的近似MAP推断、线性收敛性证明，以及在困难样例上改进推断的证据。外部监督缺失，但改进随任务难度增大而增强。

Conclusion: 闭环等价作为解决开放式自回归承诺瓶颈的潜在范式，可能成为语言模型发展的基石。该框架统一了多类模型并揭示了deliberation的效用，指向将来在更复杂任务上的应用前景。

Abstract: Contemporary autoregressive transformers operate in open loop: each hidden state is computed in a single forward pass and never revised, causing errors to propagate uncorrected through the sequence. We identify this open-loop bottleneck as a fundamental architectural limitation underlying well-documented failures in long-range reasoning, factual consistency, and multi-step planning. To address this limitation, we introduce the closed-loop prediction principle, which requires that models iteratively refine latent representations until reaching a self-consistent equilibrium before committing to each token. We instantiate this principle as Equilibrium Transformers (EqT), which augment standard transformer layers with an Equilibrium Refinement Module that minimizes a learned energy function via gradient descent in latent space. The energy function enforces bidirectional prediction consistency, episodic memory coherence, and output confidence, all computed without external supervision. Theoretically, we prove that EqT performs approximate MAP inference in a latent energy-based model, establish linear convergence guarantees, and show that refinement improves predictions precisely on hard instances where one-shot inference is suboptimal. The framework unifies deep equilibrium models, diffusion language models, and test-time training as special cases. Preliminary experiments on the binary parity task demonstrate +3.28% average improvement on challenging sequences, with gains reaching +8.07% where standard transformers approach random performance, validating that the benefit of deliberation scales with task difficulty. Just as attention mechanisms resolved the sequential bottleneck of recurrent networks, we propose that closed-loop equilibrium may resolve the commitment bottleneck of open-loop autoregression, representing a foundational step toward language models.

</details>


### [31] [Physically Interpretable Representation Learning with Gaussian Mixture Variational AutoEncoder (GM-VAE)](https://arxiv.org/abs/2511.21883)
*Tiffany Fan,Murray Cutforth,Marta D'Elia,Alexandre Cortiella,Alireza Doostan,Eric Darve*

Main category: cs.LG

TL;DR: 提出基于高斯混合变分自编码器的 EM 风格训练框架，结合图拉普拉斯平滑度的定量评估，在高维物理数据上实现稳定的潜在簇聚类和物理一致的流形表示，并在多类流动数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 高维科学数据具有复杂非线性结构，使得从数据中提取可解释且物理意义明确的表征非常困难；现有的VAE在重建与聚类的联合优化中容易导致训练不稳定。

Method: 引入高斯混合变分自编码器，采用基于 E 步/ M 步的块坐标下降训练策略来分阶段优化，确保潜在簇与物理状态的对齐，并避免端到端联合优化的梯度不稳定。

Result: 在表面反应常微分方程、Navier–Stokes尾流和激光诱导冲击成像等数据集上，GM-VAE 获得平滑、物理一致的潜在流形并实现准确的物理 regime 聚类。

Conclusion: 提供一个鲁棒的数据驱动工具用于解释湍流与反应性流动系统的高维数据，并通过图拉普拉斯平滑度等定量指标对表示质量进行客观评估。

Abstract: Extracting compact, physically interpretable representations from high-dimensional scientific data is a persistent challenge due to the complex, nonlinear structures inherent in physical systems. We propose a Gaussian Mixture Variational Autoencoder (GM-VAE) framework designed to address this by integrating an Expectation-Maximization (EM)-inspired training scheme with a novel spectral interpretability metric. Unlike conventional VAEs that jointly optimize reconstruction and clustering (often leading to training instability), our method utilizes a block-coordinate descent strategy, alternating between expectation and maximization steps. This approach stabilizes training and naturally aligns latent clusters with distinct physical regimes. To objectively evaluate the learned representations, we introduce a quantitative metric based on graph-Laplacian smoothness, which measures the coherence of physical quantities across the latent manifold. We demonstrate the efficacy of this framework on datasets of increasing complexity: surface reaction ODEs, Navier-Stokes wake flows, and experimental laser-induced combustion Schlieren images. The results show that our GM-VAE yields smooth, physically consistent manifolds and accurate regime clustering, offering a robust data-driven tool for interpreting turbulent and reactive flow systems.

</details>


### [32] [Breaking the Illusion: Consensus-Based Generative Mitigation of Adversarial Illusions in Multi-Modal Embeddings](https://arxiv.org/abs/2511.21893)
*Fatemeh Akbarian,Anahita Baninajjar,Yingyi Zhang,Ananth Balashankar,Amir Aminifar*

Main category: cs.LG

TL;DR: 提出一种基于生成式重建的任务无关防御，针对多模态编码器的对抗错觉，通过VAE等生成模型重建输入，并采用生成采样+共识聚合提升鲁棒性，显著降低错觉攻击成功率并提升跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型的跨模态对齐易被不可察觉的对抗扰动攻击破坏，导致下游任务出错，需要一种与具体任务无关、可泛化的防御策略。

Method: 在攻击者扰动输入上通过生成模型（如变分自编码器）对输入进行重建；结合生成采样策略和基于共识的聚合，对多模态编码器的跨模态对齐进行鲁棒化处理；在最先进的多模态编码器上进行实证评估。

Result: 对抗 Illusion 攻击的成功率降至近零；跨模态对齐指标在未扰动输入下从42提升至46（提升4点），在扰动输入下从32提升至43（提升11点）

Conclusion: 该方法为多模态对齐中的对抗错觉提供一种有效且模型无关的防御，能在扰动存在时维持或提升跨模态对齐。

Abstract: Multi-modal foundation models align images, text, and other modalities in a shared embedding space but remain vulnerable to adversarial illusions (Zhang et al., 2025), where imperceptible perturbations disrupt cross-modal alignment and mislead downstream tasks. To counteract the effects of adversarial illusions, we propose a task-agnostic mitigation mechanism that reconstructs the input from the attacker's perturbed input through generative models, e.g., Variational Autoencoders (VAEs), to maintain natural alignment. To further enhance our proposed defense mechanism, we adopt a generative sampling strategy combined with a consensus-based aggregation scheme over the outcomes of the generated samples. Our experiments on the state-of-the-art multi-modal encoders show that our approach substantially reduces the illusion attack success rates to near-zero and improves cross-modal alignment by 4% (42 to 46) and 11% (32 to 43) in unperturbed and perturbed input settings respectively, providing an effective and model-agnostic defense against adversarial illusions.

</details>


### [33] [Beyond Atoms: Evaluating Electron Density Representation for 3D Molecular Learning](https://arxiv.org/abs/2511.21900)
*Patricia Suriana,Joshua A. Rackers,Ewa M. Nowara,Pedro O. Pinheiro,John M. Nicoloudis,Vishnu Sresht*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning models for 3D molecular property prediction typically rely on atom-based representations, which may overlook subtle physical information. Electron density maps -- the direct output of X-ray crystallography and cryo-electron microscopy -- offer a continuous, physically grounded alternative. We compare three voxel-based input types for 3D convolutional neural networks (CNNs): atom types, raw electron density, and density gradient magnitude, across two molecular tasks -- protein-ligand binding affinity prediction (PDBbind) and quantum property prediction (QM9). We focus on voxel-based CNNs because electron density is inherently volumetric, and voxel grids provide the most natural representation for both experimental and computed densities. On PDBbind, all representations perform similarly with full data, but in low-data regimes, density-based inputs outperform atom types, while a shape-based baseline performs comparably -- suggesting that spatial occupancy dominates this task. On QM9, where labels are derived from Density Functional Theory (DFT) but input densities from a lower-level method (XTB), density-based inputs still outperform atom-based ones at scale, reflecting the rich structural and electronic information encoded in density. Overall, these results highlight the task- and regime-dependent strengths of density-derived inputs, improving data efficiency in affinity prediction and accuracy in quantum property modeling.

</details>


### [34] [Multi-Modal Machine Learning for Early Trust Prediction in Human-AI Interaction Using Face Image and GSR Bio Signals](https://arxiv.org/abs/2511.21908)
*Hamid Shamszare,Avishek Choudhury*

Main category: cs.LG

TL;DR: 本研究提出一个多模态框架，结合面部视频特征与皮肤电反应（GSR）来预测在 ADHD 移动健康场景中对 AI/人类推荐的早期信任。通过早期检测窗口（决策前6–3秒）和近端检测窗口（3–0秒），分别对图像、GSR、及两者融合进行建模，并以堆叠集成提升最终预测。结果显示多模态融合在早期窗口达到约0.83的准确率、0.88的F1、0.87的ROC-AUC；在近端窗口表现略低。研究表明生理信号可用于实时、客观的信任标志，支持自适应 AI 系统以维持合适的信任水平。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健场景中，预测并及早识别用户对 AI/人类推荐的信任程度，对于安全地整合 AI 决策支持工具、避免信任错配至关重要，尤其在精神健康管理中，信任错配可能影响诊断与治疗效果。

Method: 使用 OpenCV 提取面部视频帧并进行迁移学习（预训练 transformer）以获得情绪特征；GSR 信号分解为持稳成分（tonic）与相位成分（phasic）以捕捉生理唤起模式。定义两种时间窗口：早期检测窗口（决策前6–3秒）与近端检测窗口（3–0秒）。每个窗口内分别对图像、GSR、以及图像+GSR 的多模态特征进行建模；各模态采用机器学习算法，最优的单模态模型通过堆叠集成整合成最终预测。

Result: 多模态融合显著提升预测性能。早期检测窗口的综合模型得出准确率0.83、F1=0.88、ROC-AUC=0.87；近端检测窗口为60?（原文为 0.75/0.82/0.66 的指标），准确率0.75、F1=0.82、ROC-AUC=0.66。

Conclusion: 生理信号具备作为实时、客观信任标志的潜力，支撑可自适应的 AI 系统以动态调整响应，从而在精神健康应用中帮助维持校准信任，从而影响诊断与治疗结果。

Abstract: Predicting human trust in AI systems is crucial for safe integration of AI-based decision support tools, especially in healthcare. This study proposes a multi-modal machine learning framework that combines image and galvanic skin response (GSR) data to predict early user trust in AI- or human-generated recommendations in a simulated ADHD mHealth context. Facial video data were processed using OpenCV for frame extraction and transferred learning with a pre-trained transformer model to derive emotional features. Concurrently, GSR signals were decomposed into tonic and phasic components to capture physiological arousal patterns. Two temporal windows were defined for trust prediction: the Early Detection Window (6 to 3 seconds before decision-making) and the Proximal Detection Window (3 to 0 seconds before decision-making). For each window, trust prediction was conducted separately using image-based, GSR-based, and multimodal (image + GSR) features. Each modality was analyzed using machine learning algorithms, and the top-performing unimodal models were integrated through a multimodal stacking ensemble for final prediction. Experimental results showed that combining facial and physiological cues significantly improved prediction performance. The multimodal stacking framework achieved an accuracy of 0.83, F1-score of 0.88, and ROC-AUC of 0.87 in the Early Detection Window, and an accuracy of 0.75, F1-score of 0.82, and ROC-AUC of 0.66 in the Proximal Detection Window. These results demonstrate the potential of bio signals as real-time, objective markers of user trust, enabling adaptive AI systems that dynamically adjust their responses to maintain calibrated trust which is a critical capability in mental health applications where mis-calibrated trust can affect diagnostic and treatment outcomes.

</details>


### [35] [Closing the Generalization Gap in Parameter-efficient Federated Edge Learning](https://arxiv.org/abs/2511.23282)
*Xinnong Du,Zhonghao Lyu,Xiaowen Cao,Chunyang Wen,Shuguang Cui,Jie Xu*

Main category: cs.LG

TL;DR: 提出一种参数高效的FEEL框架，结合模型裁剪与客户端选择，在能耗和延迟约束下优化通用化意识下的梯度范数，且以交替优化解决混合整数非凸问题；实验表明优于若干基线。


<details>
  <summary>Details</summary>
Motivation: 受限且异构的本地数据集以及资源受限的部署极大削弱了模型的泛化能力和资源利用率，迫切需要在保持隐私的同时提升FEEL的学习效果与效率。

Method: 首先推导信息论层面的泛化界限，将其嵌入收敛性分析以揭示局部泛化与全局收敛之间的冲突；然后提出一个通用化意识下的平均平方梯度范数界优化问题，联合优化裁剪比例、客户端选择及通信-计算资源，在能量与时延约束下求解；最后通过交替优化将其求解为可行的混合整数问题。

Result: 大量实验说明所提出的设计在学习性能上优于现有基线，验证了将泛化分析与系统级优化耦合以实现高效FEEL的有效性。

Conclusion: 将泛化意识分析与剪裁+客户端选择的联合优化相结合，可在能耗与时延约束下提升FEEL的效率与性能。

Abstract: Federated edge learning (FEEL) provides a promising foundation for edge artificial intelligence (AI) by enabling collaborative model training while preserving data privacy. However, limited and heterogeneous local datasets, as well as resource-constrained deployment, severely degrade both model generalization and resource utilization, leading to a compromised learning performance. Therefore, we propose a parameter-efficient FEEL framework that jointly leverages model pruning and client selection to tackle such challenges. First, we derive an information-theoretic generalization statement that characterizes the discrepancy between training and testing function losses and embed it into the convergence analysis. It reveals that a larger local generalization statement can undermine the global convergence. Then, we formulate a generalization-aware average squared gradient norm bound minimization problem, by jointly optimizing the pruning ratios, client selection, and communication-computation resources under energy and delay constraints. Despite its non-convexity, the resulting mixed-integer problem is efficiently solved via an alternating optimization algorithm. Extensive experiments demonstrate that the proposed design achieves superior learning performance than state-of-the-art baselines, validating the effectiveness of coupling generalization-aware analysis with system-level optimization for efficient FEEL.

</details>


### [36] [Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs](https://arxiv.org/abs/2511.21928)
*Yifan Zhou,Sachin Grover,Mohamed El Mistiri,Kamalesh Kalirathnam,Pratyush Kerhalkar,Swaroop Mishra,Neelesh Kumar,Sanket Gaurav,Oya Aran,Heni Ben Amor*

Main category: cs.LG

TL;DR: ProPS 将大语言模型直接置于策略优化循环中，基于奖励和语言输入提出策略更新，从而实现样本效率提升，在多任务上相较于常用 RL 算法取得多数任务的优势。


<details>
  <summary>Details</summary>
Motivation: 传统 RL 依赖标量奖励，难以充分利用语言、先验知识等丰富语义信息；将语义信号与数值反馈结合，可能提升探索效率和学习速度。

Method: 以大语言模型为策略优化核心，LLM 直接基于 reward 与自然语言输入提出策略更新；在上下文中进行数值优化，并引入目标、领域知识与策略提示等语义信息来引导探索；在 15 个 Gymnasium 任务中与 PPO、SAC、TRPO 等基线进行对比。

Result: 在 15 项任务中，ProPS 于 8 项任务上超越所有基线，且提供领域知识时 gains 显著；显示出在多样任务上的样本效率和鲁棒性提升。

Conclusion: 将语义信息与数值优化统一到一个框架中，提升 RL 的可解释性、通用性和与人类目标的一致性。

Abstract: Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augment existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop-directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, domain knowledge, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across fifteen Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on eight out of fifteen tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned RL.

</details>


### [37] [Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment](https://arxiv.org/abs/2511.21931)
*Henry Salgado,Meagan Kendall,Martine Ceberio*

Main category: cs.LG

TL;DR: 提出一个数据驱动、模型无关的框架，用 Rubin 潜在结果框架来评估特征对二分类结果的效应，并将数据-derived 的特征排序与模型解释进行比对，以评估模型是否说出数据在说的话。


<details>
  <summary>Details</summary>
Motivation: 解释性研究常聚焦于解释模型行为，但本文希望通过从数据中提取的基线来评估模型与数据结构的一致性，从而提供更直观、可操作的模型-数据对齐评估。

Method: 以 Rubin 的潜在结果框架为灵感，量化每个特征在二分类任务中对输出的分离作用，得到数据层面的特征排名；该过程简洁高效、计算成本低；随后将该数据驱动的特征排序与基于模型的解释进行比较，给出一个模型-数据对齐的评估指标，具有可解释性且模型无关。

Result: 提供一个可重复、轻量级的评估框架，产生数据驱动的特征效应排序、排序之间的对比统计，以及对模型-数据一致性的定量评估。

Conclusion: 该方法为衡量模型是否符合数据结构提供了直观且可操作的基线，便于更透明的模型选择与信任构建；但需要关注混杂与因果解释的局限性，并有待扩展到多类/连续目标等场景。

Abstract: In this work, we propose a simple and computationally efficient framework to evaluate whether machine learning models align with the structure of the data they learn from; that is, whether \textit{the model says what the data says}. Unlike existing interpretability methods that focus exclusively on explaining model behavior, our approach establishes a baseline derived directly from the data itself. Drawing inspiration from Rubin's Potential Outcomes Framework, we quantify how strongly each feature separates the two outcome groups in a binary classification task, moving beyond traditional descriptive statistics to estimate each feature's effect on the outcome. By comparing these data-derived feature rankings against model-based explanations, we provide practitioners with an interpretable and model-agnostic method to assess model--data alignment.

</details>


### [38] [CTR Prediction on Alibaba's Taobao Advertising Dataset Using Traditional and Deep Learning Models](https://arxiv.org/abs/2511.21963)
*Hongyu Yang,Chunxi Wen,Jiyin Zhang,Nanfei Shen,Shijiao Zhang,Xiyan Han*

Main category: cs.LG

TL;DR: 通过将行为序列与静态特征融合，使用MLP与Transformer等深度模型来提升CTR预测，在Taobao数据集上取得AUC提升约2.81%，并提出A/B测试及对公共健康的潜在扩展。


<details>
  <summary>Details</summary>
Motivation: 提升广告系统的排序相关性与用户参与度，克服仅使用静态特征的局限，挖掘用户行为随时间的动态模式。

Method: 从静态特征出发训练LR、LightGBM等基线模型；扩展到利用上亿次行为数据的序列编码，构建用户兴趣表示；将行为嵌入与静态特征融合，采用MLP和Transformer，Transformer通过自注意力建模时间与频次等上下文依赖；设计A/B测试策略。

Result: Transformer在基线LR上提升AUC约2.81%，对兴趣多元或随时间变化的用户收益最大。

Conclusion: 为CTR预测提供一种可扩展的序列化建模框架，并指出其在个性化广告之外的潜在应用（如公共健康信息精准投放）。

Abstract: Click-through rates prediction is critical in modern advertising systems, where ranking relevance and user engagement directly impact platform efficiency and business value. In this project, we explore how to model CTR more effectively using a large-scale Taobao dataset released by Alibaba. We start with supervised learning models, including logistic regression and Light-GBM, that are trained on static features such as user demographics, ad attributes, and contextual metadata. These models provide fast, interpretable benchmarks, but have limited capabilities to capture patterns of behavior that drive clicks. To better model user intent, we combined behavioral data from hundreds of millions of interactions over a 22-day period. By extracting and encoding user action sequences, we construct representations of user interests over time. We use deep learning models to fuse behavioral embeddings with static features. Among them, multilayer perceptrons (MLPs) have achieved significant performance improvements. To capture temporal dynamics, we designed a Transformer-based architecture that uses a self-attention mechanism to learn contextual dependencies across behavioral sequences, modeling not only what the user interacts with, but also the timing and frequency of interactions. Transformer improves AUC by 2.81 % over the baseline (LR model), with the largest gains observed for users whose interests are diverse or change over time. In addition to modeling, we propose an A/B testing strategy for real-world evaluation. We also think about the broader implications: personalized ad targeting technology can be applied to public health scenarios to achieve precise delivery of health information or behavior guidance. Our research provides a roadmap for advancing click-through rate predictions and extending their value beyond e-commerce.

</details>


### [39] [Distance-based Learning of Hypertrees](https://arxiv.org/abs/2511.22014)
*Shaun Fallat,Kamyar Khodamoradi,David Kirkpatrick,Valerii Maliuk,S. Ahmad Mojallal,Sandra Zilles*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the problem of learning hypergraphs with shortest-path queries (SP-queries), and present the first provably optimal online algorithm for a broad and natural class of hypertrees that we call orderly hypertrees. Our online algorithm can be transformed into a provably optimal offline algorithm. Orderly hypertrees can be positioned within the Fagin hierarchy of acyclic hypergraph (well-studied in database theory), and strictly encompass the broadest class in this hierarchy that is learnable with subquadratic SP-query complexity.
  Recognizing that in some contexts, such as evolutionary tree reconstruction, distance measurements can degrade with increased distance, we also consider a learning model that uses bounded distance queries. In this model, we demonstrate asymptotically tight complexity bounds for learning general hypertrees.

</details>


### [40] [Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time Adaptation](https://arxiv.org/abs/2511.22030)
*Geun-Deok Jang,Dong-Kyun Han,Seo-Hyeon Park,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 提出一种基于在线测试时自适应（TTA）的EEG驾驶人打瞌睡检测框架，通过对BN层可训练参数进行在线更新、保留预训练统计、引入记忆库和原型学习，提升跨被试的鲁棒性与适应性。


<details>
  <summary>Details</summary>
Motivation: EEG信号因心理物理因素存在显著的被试间变异，导致域移位问题，传统模型需要耗时的个体化校准，难以在未见被试上泛化。需在测试阶段动态自适应以适应目标被试分布。

Method: 在测试阶段仅更新BN层的可学习参数，同时保留预训练的BN统计；引入一个动态管理的记忆库，用负能量分数与持续时间等指标选取高可靠性样本进行在线训练；加入原型学习以增强对随时间的分布偏移的鲁棒性。评估在仿真环境中的持续注意力驾驶数据集上，借由延迟反应时间推断瞌睡状态。

Result: 在所用数据集上达到平均F1-score 81.73%，较最佳TTA基线提升11.73%，显著优于各基线，证明在非独立同分布场景下对EEG打瞌睡检测的自适应性显著增强。

Conclusion: 所提出的BN层在线自适应、记忆库选择与原型学习相结合的框架，有效缓解跨被试域移位，提升了EEG驱动疲劳检测在非i.i.d. 情况下的泛化与鲁棒性。

Abstract: Drowsy driving is a growing cause of traffic accidents, prompting recent exploration of electroencephalography (EEG)-based drowsiness detection systems. However, the inherent variability of EEG signals due to psychological and physical factors necessitates a cumbersome calibration process. In particular, the inter-subject variability of EEG signals leads to a domain shift problem, which makes it challenging to generalize drowsiness detection models to unseen target subjects. To address these issues, we propose a novel driver drowsiness detection framework that leverages online test-time adaptation (TTA) methods to dynamically adjust to target subject distributions. Our proposed method updates the learnable parameters in batch normalization (BN) layers, while preserving pretrained normalization statistics, resulting in a modified configuration that ensures effective adaptation during test time. We incorporate a memory bank that dynamically manages streaming EEG segments, selecting samples based on their reliability determined by negative energy scores and persistence time. In addition, we introduce prototype learning to ensure robust predictions against distribution shifts over time. We validated our method on the sustained-attention driving dataset collected in a simulated environment, where drowsiness was estimated from delayed reaction times during monotonous lane-keeping tasks. Our experiments show that our method outperforms all baselines, achieving an average F1-score of 81.73\%, an improvement of 11.73\% over the best TTA baseline. This demonstrates that our proposed method significantly enhances the adaptability of EEG-based drowsiness detection systems in non-i.i.d. scenarios.

</details>


### [41] [Predicting Public Health Impacts of Electricity Usage](https://arxiv.org/abs/2511.22031)
*Yejia Liu,Zhifeng Wu,Pengfei Li,Shaolei Ren*

Main category: cs.LG

TL;DR: 提出 HealthPredictor，一种端到端的领域特定 AI 模型，将用电量与健康结果联系起来，包含燃料混合预测、空气质量转化和健康影响评估三大模块，在多区域评估中显著优于以燃料混合为驱动的基线并给出电动汽车充电的健康导向优化案例，并公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: 电力部门仍以化石燃料为主，尽管已有监管，但对公众健康的空气污染影响仍然显著。需要健康知情的需求侧管理来降低健康损害，因此迫切需要将健康目标嵌入能源管理的决策中。

Method: 提出一个三组件管线：燃料混合预测器（估计不同发电来源的贡献）、空气质量转换器（建模污染物排放和大气扩散）、健康影响评估器（将污染物变化转化为货币化的健康损失）。在美国多区域进行评估，健康驱动的优化框架在公共健康影响预测上显著优于燃料混合驱动的基线，并通过电动汽车充电时序的案例研究展示健康收益和可操作性。

Result: 与以燃料混合为驱动的基线相比，健康驱动框架在公共健康影响的预测误差上显著降低；给出健康收益的具体案例（如 EV 充电调度），并提供数据集与代码以支持复现与进一步研究。

Conclusion: 表明 AI 模型可被显式设计为促进健康知情的能源管理，从而提升公共健康和社会福祉，研究结果具有应用潜力与政策启示，数据与代码公开。

Abstract: The electric power sector is a leading source of air pollutant emissions, impacting the public health of nearly every community. Although regulatory measures have reduced air pollutants, fossil fuels remain a significant component of the energy supply, highlighting the need for more advanced demand-side approaches to reduce the public health impacts. To enable health-informed demand-side management, we introduce HealthPredictor, a domain-specific AI model that provides an end-to-end pipeline linking electricity use to public health outcomes. The model comprises three components: a fuel mix predictor that estimates the contribution of different generation sources, an air quality converter that models pollutant emissions and atmospheric dispersion, and a health impact assessor that translates resulting pollutant changes into monetized health damages. Across multiple regions in the United States, our health-driven optimization framework yields substantially lower prediction errors in terms of public health impacts than fuel mix-driven baselines. A case study on electric vehicle charging schedules illustrates the public health gains enabled by our method and the actionable guidance it can offer for health-informed energy management. Overall, this work shows how AI models can be explicitly designed to enable health-informed energy management for advancing public health and broader societal well-being. Our datasets and code are released at: https://github.com/Ren-Research/Health-Impact-Predictor.

</details>


### [42] [Convergence Dynamics of Over-Parameterized Score Matching for a Single Gaussian](https://arxiv.org/abs/2511.22069)
*Yiran Zhang,Weihang Xu,Mo Zhou,Maryam Fazel,Simon Shaolei Du*

Main category: cs.LG

TL;DR: 在过参数化条件下，分析梯度下降训练用于学习单高斯分布的分数匹配目标，揭示不同噪声规模与初始化条件下的收敛性，且给出高斯混合模型的全局收敛性新结果。


<details>
  <summary>Details</summary>
Motivation: 理解分数匹配的优化行为，特别是在高维、过参数化情形，以及在高斯/高斯混合分布上的理论保证。

Method: 考察一个含n个可学习参数的学生模型，在单一高斯数据上，使用人口分数匹配目标，研究梯度下降的动态；覆盖高噪声、低噪声、特定初始化条件、以及随机初始化远离真值的情形。

Result: - 高噪声下全局收敛；- 低噪声下存在驻点，仍在某些初始化下全参数收敛到真值；若不满足指数级小初始化，可能不收敛到真值；- 随机初始化导致仅有一个参数收敛，其他发散，但损失仍以1/τ收敛，且存在近似的收敛下界；- 首次给出分数匹配框架下三分量以上的高斯混合模型全局收敛性保证。

Conclusion: 此工作首次在分数匹配框架下建立高斯混合模型（≥三分量）的全局收敛性理论，并揭示过参数化条件下的不同阶段性收敛特征和初始化依赖性。

Abstract: Score matching has become a central training objective in modern generative modeling, particularly in diffusion models, where it is used to learn high-dimensional data distributions through the estimation of score functions. Despite its empirical success, the theoretical understanding of the optimization behavior of score matching, particularly in over-parameterized regimes, remains limited. In this work, we study gradient descent for training over-parameterized models to learn a single Gaussian distribution. Specifically, we use a student model with $n$ learnable parameters and train it on data generated from a single ground-truth Gaussian using the population score matching objective. We analyze the optimization dynamics under multiple regimes. When the noise scale is sufficiently large, we prove a global convergence result for gradient descent. In the low-noise regime, we identify the existence of a stationary point, highlighting the difficulty of proving global convergence in this case. Nevertheless, we show convergence under certain initialization conditions: when the parameters are initialized to be exponentially small, gradient descent ensures convergence of all parameters to the ground truth. We further prove that without the exponentially small initialization, the parameters may not converge to the ground truth. Finally, we consider the case where parameters are randomly initialized from a Gaussian distribution far from the ground truth. We prove that, with high probability, only one parameter converges while the others diverge, yet the loss still converges to zero with a $1/τ$ rate, where $τ$ is the number of iterations. We also establish a nearly matching lower bound on the convergence rate in this regime. This is the first work to establish global convergence guarantees for Gaussian mixtures with at least three components under the score matching framework.

</details>


### [43] [A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization](https://arxiv.org/abs/2511.22080)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Chang Liu,Qipeng Xie,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWMSAM在联邦学习中通过结合 momentum 与 SAM，并以全局扰动对齐局部 SAM方向，解决非IID场景下的局部-全局曲率错位与动量回响问题，同时实现单次反向传播的全局近似并采用两阶段自适应训练以提高收敛速度、泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在非IID分布下，单纯的 momentum 与 SAM 组合易产生局部-全局曲率错位和动量回响，导致收敛慢、泛化差和不稳定性。需要一个能对齐全局几何结构、稳定优化过程且高效的联邦学习方法。

Method: 1) 以服务器聚合的动量信息构造全局扰动，引导客户端的 SAM 方向与全局下降几何一致，从而实现单次反向传播即可近似全局 SAM 的高效近似。2) 通过基于余弦相似度的自适应规则，将动量与 SAM 融合，设计早期动量、后期 SAM 的两阶段训练调度，以缓解晚期的动量回响问题。

Result: 给出非IID情形下的理论收敛界，显式建模扰动引起的方差 σ_ρ^2 = σ^2+(Lρ)^2，与数据分布、客户端数量、通信轮数等参数的关系；大量实验在多数据集与模型架构上验证了FedWMSAM在准确性、适应性与鲁棒性方面的优越性。

Conclusion: FedWMSAM成功地解决了局部-全局曲率错位与动量回响两大问题，理论与实证均支持其在非IID联邦学习中的有效性与鲁棒性，方法可公开获取。

Abstract: In federated learning (FL), models must \emph{converge quickly} under tight communication budgets while \emph{generalizing} across non-IID client distributions. These twin requirements have naturally led to two widely used techniques: client/server \emph{momentum} to accelerate progress, and \emph{sharpness-aware minimization} (SAM) to prefer flat solutions. However, simply combining momentum and SAM leaves two structural issues unresolved in non-IID FL. We identify and formalize two failure modes: \emph{local-global curvature misalignment} (local SAM directions need not reflect the global loss geometry) and \emph{momentum-echo oscillation} (late-stage instability caused by accumulated momentum). To our knowledge, these failure modes have not been jointly articulated and addressed in the FL literature. We propose \textbf{FedWMSAM} to address both failure modes. First, we construct a momentum-guided global perturbation from server-aggregated momentum to align clients' SAM directions with the global descent geometry, enabling a \emph{single-backprop} SAM approximation that preserves efficiency. Second, we couple momentum and SAM via a cosine-similarity adaptive rule, yielding an early-momentum, late-SAM two-phase training schedule. We provide a non-IID convergence bound that \emph{explicitly models the perturbation-induced variance} $σ_ρ^2=σ^2+(Lρ)^2$ and its dependence on $(S, K, R, N)$ on the theory side. We conduct extensive experiments on multiple datasets and model architectures, and the results validate the effectiveness, adaptability, and robustness of our method, demonstrating its superiority in addressing the optimization challenges of Federated Learning. Our code is available at https://github.com/Huang-Yongzhi/NeurlPS_FedWMSAM.

</details>


### [44] [Quantum Bayesian Optimization for Quality Improvement in Fuselage Assembly](https://arxiv.org/abs/2511.22090)
*Jiayu Liu,Chong Liu,Trevor Rhone,Yinan Wang*

Main category: cs.LG

TL;DR: 提出基于量子贝叶斯优化的形状控制框架（QBO），通过量子oracle在有限元/替代模型下以更少样本估计环境响应，从而在航空器机身装配中实现更高的样本效率与更低的维度误差。


<details>
  <summary>Details</summary>
Motivation: 解决制造系统中样本效率低下的问题。传统蒙特卡洛方法在大规模分布下难以高效估计均值；量子方法可在同等精度下用更少的样本实现估计，促进现实系统的高效优化。

Method: 构建量子贝叶斯优化框架，使用基于FEA的模型或替代模型构成量子oracle来获得更准确的环境响应估计；以上置信界（UCB）作为获取函数，智能选取输入以最大化目标函数。并在机身某段的力控执行器上进行实验，减少相邻段间的间隙。

Result: 与经典方法相比，在相同查询次数下，QBO实现了显著降低的维度误差和不确定性，且实验基于仿真相同查询量时表现优于传统蒙特卡洛/优化策略。

Conclusion: 量子贝叶斯优化可显著提升制造装配中的样本效率与优化精度，尤其适用于对形状控制有高精度需求的场景，具备将量子估计优势落地于现实工艺的潜力。

Abstract: Recent efforts in smart manufacturing have enhanced aerospace fuselage assembly processes, particularly by innovating shape adjustment techniques to minimize dimensional gaps between assembled sections. Existing approaches have shown promising results but face the issue of low sample efficiency from the manufacturing systems. It arises from the limitation of the classical Monte Carlo method when uncovering the mean response from a distribution. In contrast, recent work has shown that quantum algorithms can achieve the same level of estimation accuracy with significantly fewer samples than the classical Monte Carlo method from distributions. Therefore, we can adopt the estimation of the quantum algorithm to obtain the estimation from real physical systems (distributions). Motivated by this advantage, we propose a Quantum Bayesian Optimization (QBO) framework for precise shape control during assembly to improve the sample efficiency in manufacturing practice. Specifically, this approach utilizes a quantum oracle, based on finite element analysis (FEA)-based models or surrogate models, to acquire a more accurate estimation of the environment response with fewer queries for a certain input. QBO employs an Upper Confidence Bound (UCB) as the acquisition function to strategically select input values that are most likely to maximize the objective function. It has been theoretically proven to require much fewer samples while maintaining comparable optimization results. In the case study, force-controlled actuators are applied to one fuselage section to adjust its shape and reduce the gap to the adjoining section. Experimental results demonstrate that QBO achieves significantly lower dimensional error and uncertainty compared to classical methods, particularly using the same queries from the simulation.

</details>


### [45] [Test Time Training for AC Power Flow Surrogates via Physics and Operational Constraint Refinement](https://arxiv.org/abs/2511.22343)
*Panteleimon Dogoulis,Mohammad Iman Alizadeh,Sylvain Kubler,Maxime Cordy*

Main category: cs.LG

TL;DR: 通过在推理阶段对 ML 的潮流预测进行轻量自监督微调，PI-TTT 能在不需标注数据的前提下强制满足 AC 功率流等式与运行约束，从而提升准确性与可行性，同时保持计算优势。


<details>
  <summary>Details</summary>
Motivation: ML 潮流近似方法在计算上有优势，但往往缺乏物理一致性，需在推理阶段融入物理约束以确保预测的可行性。

Method: 提出 PI-TTT 框架，对预测输出进行少量梯度更新以自监督地对齐 AC 方程和约束；无需标签数据，在 unseen operating conditions 下实现局部自适应；兼容现有 ML 预测器。

Result: 在 IEEE 14、118、300-bus 和 PEGASE 1354-bus 数据集上，PI-TTT 将功率流残差与运行约束违规降低 1-2 个数量级，同时保留 ML 模型的计算速度优势。

Conclusion: PI-TTT 为大规模、物理一致的学习在电力系统分析中提供了一条高效、可靠的路径，具有良好应用前景。

Abstract: Power Flow (PF) calculation based on machine learning (ML) techniques offer significant computational advantages over traditional numerical methods but often struggle to maintain full physical consistency. This paper introduces a physics-informed test-time training (PI-TTT) framework that enhances the accuracy and feasibility of ML-based PF surrogates by enforcing AC power flow equalities and operational constraints directly at inference time. The proposed method performs a lightweight self-supervised refinement of the surrogate outputs through few gradient-based updates, enabling local adaptation to unseen operating conditions without requiring labeled data. Extensive experiments on the IEEE 14-, 118-, and 300-bus systems and the PEGASE 1354-bus network show that PI-TTT reduces power flow residuals and operational constraint violations by one to two orders of magnitude compared with purely ML-based models, while preserving their computational advantage. The results demonstrate that PI-TTT provides fast, accurate, and physically reliable predictions, representing a promising direction for scalable and physics-consistent learning in power system analysis.

</details>


### [46] [Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs](https://arxiv.org/abs/2511.22099)
*Daniel Agyei Asante,Md Mokarram Chowdhury,Yang Li*

Main category: cs.LG

TL;DR: 低秩分解在减小模型尺寸的同时，对信任度有混合影响：隐私、对抗鲁棒性、伦理与公平性在不同方面表现不同。


<details>
  <summary>Details</summary>
Motivation: 探究低秩分解对LLM在隐私、对抗鲁棒性、公平性、伦理对齐等信任性方面的影响，补充对资源受限场景中的模型压缩信任性理解。

Method: 在多种尺寸和变体的LLM上，对比不同低秗率算法的压缩，评估隐私保护、PII保护、对抗鲁棒性、伦理推理、公平性等维度；并通过梯度对比分析，识别对鲁棒性贡献最大的层。

Result: 低秩压缩通常保留或提升训练数据隐私，但对话中的PII保护能力降低；对抗鲁棒性通常得到保留甚至提升；零-shot下伦理推理下降，但少量提示可部分恢复；公平性在压缩下下降。规模和微调对信任度也有影响。还给出梯度基的归因，指示哪些层对鲁棒性最相关。

Conclusion: 在资源受限情境下，低秢分解虽能带来系统级优势，但需关注信任性之间的权衡；提出针对可信压缩的层级归因分析可 guiding 设计。

Abstract: Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, and computation while maintaining accuracy. However, while these compressed models boast of benign performance and system-level advantages, their trustworthiness implications remain poorly understood. In this paper, we present the first comprehensive study of how low-rank factorization affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment. We evaluate multiple LLMs of different sizes and variants compressed with diverse low-rank algorithms, revealing key insights: (1) low-rank compression preserves or improves training data privacy but weakens PII protection during conversation; (2) adversarial robustness is generally preserved and often enhanced, even under deep compression; (3) ethical reasoning degrades in zero-shot settings but partially recovers with few-shot prompting; (4) fairness declines under compression. Beyond compression, we investigate how model scale and fine-tuning affect trustworthiness, as both are important in low-rank methods. To guide trustworthy compression strategies, we end our paper with a gradient-based attribution analysis to identify which layers in LLMs contribute most to adversarial robustness.

</details>


### [47] [Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions](https://arxiv.org/abs/2511.22406)
*Roland Stolz,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 在带有动作约束的强化学习中，本文提出对截断正态分布的熵、对数概率及其梯度进行高效数值近似，并给出高效的抽样策略；在三个基准环境上验证，显示在估计准确性提升下的性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 带约束的动作空间常用于确保安全性或动作相关性，但截断分布下的熵、对数概率及梯度的精确计算往往不可行，常用非截断分布近似会严重损害性能，因此需要高效而准确的估计方法。

Method: 提出对截断正态分布下的熵、对数概率及梯度的高效数值近似，并给出截断策略的高效采样方案；在实际策略梯度方法中集成这些近似以提升训练稳定性与效率。

Result: 在三个基准环境中验证，使用准确估计的分布特征可显著提升性能，相较于使用非截断近似的基线表现更优。

Conclusion: 准确估计截断分布的特征在动作约束的强化学习中至关重要；提出的近似与采样策略为提高效率和表现提供了有效途径，并为安全和相关性受限的策略学习提供支持。

Abstract: In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.

</details>


### [48] [Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba](https://arxiv.org/abs/2511.22101)
*Zhaofeng Zhang*

Main category: cs.LG

TL;DR: 对论文的复现与改进的简要结论：复现基于 Uniswap V3 的深度强化学习流动性提供模型，数据来自 Uniswap Subgraph；在复现基础上提出将 Mamba 与 DDQN 结合的新结构、引入新的 reward 函数并再次清洗数据，同时增加两组新基线进行对比。初步结果显示理论基础更充分，在部分测试中表现优于原模型，但尚未覆盖全部数据集。


<details>
  <summary>Details</summary>
Motivation: 旨在验证并提升原模型的可复现性与性能；通过引入更强的优化策略（Mamba+DDQN）和新的奖励设计，提升泛化与学习稳定性；并通过增加基线对比来更严谨地评估改进。

Method: 1) 复现工作：从 Uniswap Subgraph 获取数据，给出实现细节与对结果的评述。2) 提出新结构：将 Mamba 与 DDQN 融合，设计新的奖励函数；3) 数据清洗与基线设计：再次清洗数据，增设两组新的对照基线；4) 实证评估：在若干数据集上进行对比分析（尽管尚未覆盖所有数据集）。

Result: 复现工作完成并提供实现要点及结果评述；新结构在理论层面具备更强的支撑，在部分测试中优于原模型；目前尚未在所有数据集上完成验证。

Conclusion: 通过更严格的复现和对原模型的改进，理论基础与部分实验表现均有提升；未来需要在更广泛的数据集上进行全面评估与调参，确保改进的鲁棒性和泛化性。

Abstract: The report goes through the main steps of replicating and improving the article "Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning." The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.

</details>


### [49] [Representative Action Selection for Large Action Space: From Bandits to MDPs](https://arxiv.org/abs/2511.22104)
*Quan Zhou,Shie Mannor*

Main category: cs.LG

TL;DR: 在极大动作空间中选取一个固定的、具有代表性的子集，使任何同族环境中都包含一个近似最优动作，从而在大规模MDP中实现高效学习，理论上与使用全动作空间的表现相近。


<details>
  <summary>Details</summary>
Motivation: 解决跨一组RL环境的海量动作空间带来的学习困难；在库存管理、推荐系统等场景，需要在不遍历所有动作的情况下获得近似最优策略。

Method: 将元带宽（meta-bandit）的方法推广到强化学习的MDP设定，基于非中心化子高斯过程的放宽假设，给出一个在固定动作子集上实现近似最优的算法并建立理论保障。

Result: 在给定的非中心化子高斯过程模型下，证明固定子集策略的性能接近使用完整动作空间的策略，即用子集即可达到近似最优。

Conclusion: 该方法在大规模组合决策与环境异质性显著的场景中，提供了计算和样本效率更高的解决方案。

Abstract: We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.
  This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.

</details>


### [50] [An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface](https://arxiv.org/abs/2511.22108)
*Zhou Biyan,Arindam Basu*

Main category: cs.LG

TL;DR: DSNN-based continuous learning for iBMIs using RL (Banditron and AGREL) to cope with non-stationarity while meeting implantable energy constraints; Banditron achieves similar performance to AGREL with substantial savings in memory and MAC operations, making it suitable for wireless iBMIs.


<details>
  <summary>Details</summary>
Motivation: Address non-stationarity in neural decoding for implantable brain-machine interfaces and reduce retraining demands, enabling safe, comfortable, and wireless operation with limited energy and compute resources.

Method: Integrate Deep Spiking Neural Networks (DSNNs) with reinforcement learning algorithms Banditron and AGREL. Evaluate through open-loop and closed-loop experiments to assess stability and real-time performance, focusing on memory access and MAC operation reductions during training.

Result: Open-loop DSNN Banditron and DSNN AGREL maintain stable accuracy over long periods. In closed-loop with perturbations, DSNN Banditron matches DSNN AGREL in time-to-target while reducing memory access by 98% and MAC operations by 99% during training. DSNN Banditron requires 98% less computation than previous continuous-learning SNN decoders.

Conclusion: DSNN Banditron emerges as a highly energy-efficient, computation-light continuous-learning DSNN decoder for wireless iBMIs, effectively addressing non-stationarity with substantial resource savings and preserving performance in closed-loop control.

Abstract: The number of simultaneously recorded neurons follows an exponentially increasing trend in implantable brain-machine interfaces (iBMIs). Integrating the neural decoder in the implant is an effective data compression method for future wireless iBMIs. However, the non-stationarity of the system makes the performance of the decoder unreliable. To avoid frequent retraining of the decoder and to ensure the safety and comfort of the iBMI user, continuous learning is essential for real-life applications. Since Deep Spiking Neural Networks (DSNNs) are being recognized as a promising approach for developing resource-efficient neural decoder, we propose continuous learning approaches with Reinforcement Learning (RL) algorithms adapted for DSNNs. Banditron and AGREL are chosen as the two candidate RL algorithms since they can be trained with limited computational resources, effectively addressing the non-stationary problem and fitting the energy constraints of implantable devices. To assess the effectiveness of the proposed methods, we conducted both open-loop and closed-loop experiments. The accuracy of open-loop experiments conducted with DSNN Banditron and DSNN AGREL remains stable over extended periods. Meanwhile, the time-to-target in the closed-loop experiment with perturbations, DSNN Banditron performed comparably to that of DSNN AGREL while achieving reductions of 98% in memory access usage and 99% in the requirements for multiply- and-accumulate (MAC) operations during training. Compared to previous continuous learning SNN decoders, DSNN Banditron requires 98% less computes making it a prime candidate for future wireless iBMI systems.

</details>


### [51] [IVGAE: Handling Incomplete Heterogeneous Data with a Variational Graph Autoencoder](https://arxiv.org/abs/2511.22116)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal%*

Main category: cs.LG

TL;DR: IVGAE是一个变分图自编码框架，用于对不完整的异质表格数据进行鲁棒插补，通过双解码器和二部图（样本-特征）来建模结构依赖，并用Transformer嵌入处理分类变量；在16个真实数据集上对MCAR/MAR/MNAR的30%缺失率表现出RMSE和F1的稳定提升，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据常同时包含数值型与分类变量，缺失数据广泛且模式复杂，现有插补方法往往难以捕捉结构依赖与异质性，亟需兼具对缺失机制与数据结构的鲁棒插补方法。

Method: 构建样本-特征的二部图并进行图表示学习，设计双解码器：一个重构特征嵌入，另一个建模缺失模式以提供结构先验；使用基于Transformer的异质嵌入模块对分类变量进行高效编码，避免高维one-hot编码。

Result: 在16个真实数据集上的实验表明，IVGAE在MCAR/MAR/MNAR下的30%缺失率时，RMSE和下游F1均有一致提升。

Conclusion: IVGAE提供了对缺失机制的结构先验感知能力，适用于异质数据的鲁棒插补，且实现可复现（代码公开。）

Abstract: Handling missing data remains a fundamental challenge in real-world tabular datasets, especially when data are heterogeneous with both numerical and categorical features. Existing imputation methods often fail to capture complex structural dependencies and handle heterogeneous data effectively. We present \textbf{IVGAE}, a Variational Graph Autoencoder framework for robust imputation of incomplete heterogeneous data. IVGAE constructs a bipartite graph to represent sample-feature relationships and applies graph representation learning to model structural dependencies. A key innovation is its \textit{dual-decoder architecture}, where one decoder reconstructs feature embeddings and the other models missingness patterns, providing structural priors aware of missing mechanisms. To better encode categorical variables, we introduce a Transformer-based heterogeneous embedding module that avoids high-dimensional one-hot encoding. Extensive experiments on 16 real-world datasets show that IVGAE achieves consistent improvements in RMSE and downstream F1 across MCAR, MAR, and MNAR missing scenarios under 30\% missing rates. Code and data are available at: https://github.com/echoid/IVGAE.

</details>


### [52] [A Variational Manifold Embedding Framework for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2511.22128)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 提出一个变分框架，将降维算法视为最优流形嵌入问题，支持非线性嵌入并具备可解释性；在某些情况下可解析地表征解，且存在一个特殊情形恰好恢复 PCA。


<details>
  <summary>Details</summary>
Motivation: 克服PCA的线性局限以及其他方法在可解释性或几何保持方面的问题；希望得到比PCA更灵活的非线性嵌入，同时保持可解释性。

Method: 建立降维的变分表述，将其视为一个最优流形嵌入问题；通过这种构造允许非线性嵌入；解具有部分微分方程约束，能反映嵌入目标的对称性；在某些情形可解析表征解。

Result: 得到的解在某些情形可解析表征；框架能生成比PCA更灵活的非线性嵌入；PCA成为特殊情形。

Conclusion: 该框架统一并扩展了降维方法，兼具灵活性和可解释性；通过PDE视角揭示解的对称性，并在有限条件下给出解析解；PCA作为一致性测试点。

Abstract: Dimensionality reduction algorithms like principal component analysis (PCA) are workhorses of machine learning and neuroscience, but each has well-known limitations. Variants of PCA are simple and interpretable, but not flexible enough to capture nonlinear data manifold structure. More flexible approaches have other problems: autoencoders are generally difficult to interpret, and graph-embedding-based methods can produce pathological distortions in manifold geometry. Motivated by these shortcomings, we propose a variational framework that casts dimensionality reduction algorithms as solutions to an optimal manifold embedding problem. By construction, this framework permits nonlinear embeddings, allowing its solutions to be more flexible than PCA. Moreover, the variational nature of the framework has useful consequences for interpretability: each solution satisfies a set of partial differential equations, and can be shown to reflect symmetries of the embedding objective. We discuss these features in detail and show that solutions can be analytically characterized in some cases. Interestingly, one special case exactly recovers PCA.

</details>


### [53] [From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures](https://arxiv.org/abs/2511.22150)
*Florian Rottach,William Rudman,Bastian Rieck,Harrisen Scells,Carsten Eickhoff*

Main category: cs.LG

TL;DR: 提出统一的拓扑特征（UTS）来对文本嵌入空间进行全局表征；通过对多种模型和数据集的拓扑与几何度量的分析，发现度量之间高度冗余，单一度量难以区分嵌入空间；UTS 能预测模型属性、揭示受模型架构驱动的相似性，并与排序效果及文档可检索性等下游任务相关联。


<details>
  <summary>Details</summary>
Motivation: 现有的文本嵌入评估通常依赖多种拓扑与几何度量，但这些度量往往存在高度冗余，且难以全面区分不同嵌入空间的结构。需要一个统一的、 holistic 的框架来捕捉嵌入空间的几何/拓扑特征及其对下游任务的影响，以提升可解释性和模型设计能力。

Method: 在广泛的文本嵌入模型和数据集上，计算多种拓扑与几何度量，分析它们之间的相关性与冗余性。基于这些洞见提出统一拓扑特征（UTS）框架，评估其对模型特性预测、架构驱动的相似性揭示，以及与文档排序/可检索性等下游任务的联系。

Result: 发现不同度量之间存在高度冗余，单一指标往往无法充分区分嵌入空间。UTS 能预测模型特定属性并揭示受模型架构影响的相似性。将拓扑结构与排序有效性联系起来，能够准确预测文档的可检索性，强调了以多属性角度理解文本嵌入几何的重要性。

Conclusion: 整体而言，理解和利用文本嵌入几何时，需要一个 holistic、多属性的视角。UTS 提供了一个有用的框架，用于描述嵌入空间的拓扑-几何结构，并与下游任务性能建立关联。

Abstract: Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.

</details>


### [54] [Designing Instance-Level Sampling Schedules via REINFORCE with James-Stein Shrinkage](https://arxiv.org/abs/2511.22177)
*Peiyu Yu,Suraj Kothawade,Sirui Xie,Ying Nian Wu,Hongliang Fei*

Main category: cs.LG

TL;DR: 通过学习实例级采样时间表（提示词与噪声条件），在保持冻结的文本到图像模型的前提下，使用单遍Dirichlet策略与James-Stein基线优化，提升文本-图像对齐与生成控制的后训练效果。


<details>
  <summary>Details</summary>
Motivation: 大多数后训练方法聚焦在权重微调或蒸馏，未充分利用采样时间表的潜力。本工作提出在不修改权重的情况下，通过学习可定制的采样时间表来提升对齐与可控性。

Method: 对冻结的扩散采样器使用实例级（prompt-和噪声条件）调度，采用单遍Dirichlet策略进行学习；引入基于James-Stein估计的奖励基线以提高高维策略学习的梯度估计稳定性；在Stable Diffusion与Flux家族上验证；声称5步Flux-Dev达到与Flux-Schnell等蒸馏方法相近的质量。

Result: 所构建的重排采样器在文本渲染、组合控制等方面提升文本-图像对齐，且在现代Stable Diffusion与Flux家族上的提升具有一致性；5步Flux-Dev在某些场景达到与定向蒸馏方法相当的质量；证明调度框架作为一种模型无关的后训练手段具有潜力。

Conclusion: 将调度框架定位为一种新兴的模型无关的后训练杠杆，能够在预训练采样器上解锁额外的生成潜力。

Abstract: Most post-training methods for text-to-image samplers focus on model weights: either fine-tuning the backbone for alignment or distilling it for few-step efficiency. We take a different route: rescheduling the sampling timeline of a frozen sampler. Instead of a fixed, global schedule, we learn instance-level (prompt- and noise-conditioned) schedules through a single-pass Dirichlet policy. To ensure accurate gradient estimates in high-dimensional policy learning, we introduce a novel reward baseline based on a principled James-Stein estimator; it provably achieves lower estimation errors than commonly used variants and leads to superior performance. Our rescheduled samplers consistently improve text-image alignment including text rendering and compositional control across modern Stable Diffusion and Flux model families. Additionally, a 5-step Flux-Dev sampler with our schedules can attain generation quality comparable to deliberately distilled samplers like Flux-Schnell. We thus position our scheduling framework as an emerging model-agnostic post-training lever that unlocks additional generative potential in pretrained samplers.

</details>


### [55] [PULSE-ICU: A Pretrained Unified Long-Sequence Encoder for Multi-task Prediction in Intensive Care Units](https://arxiv.org/abs/2511.22199)
*Sejeong Jang,Joo Heung Yoon,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: PULSE-ICU 是一个自监督基础模型，用于从大规模 EHR 序列中学习事件级 ICU 表征，并在多任务和跨域验证中表现出色。


<details>
  <summary>Details</summary>
Motivation: ICU 数据高度不规则且碎片化，传统方法需重采样和手工特征工程，提升泛化能力和数据利用效率是关键挑战。

Method: 统一嵌入模块编码事件身份、连续值、单位与时间属性；采用 Longformer 编码器处理长序列；在大规模 EHR 上进行自监督预训练后，对 18 个预测任务进行微调；在外部数据集（eICU、HiRID、P12）进行跨域验证。

Result: 在死亡率、干预预测、表型识别等多任务上表现突出；外部验证显著提升且微调需求低，展示对领域转移和约束变化的鲁棒性。

Conclusion: 基础模型范式可提升数据效率和跨环境适应性，为 ICU 决策支持提供可扩展的框架。

Abstract: Intensive care unit (ICU) data are highly irregular, heterogeneous, and temporally fragmented, posing challenges for generalizable clinical prediction. We present PULSE-ICU, a self-supervised foundation model that learns event-level ICU representations from large-scale EHR sequences without resampling or manual feature engineering. A unified embedding module encodes event identity, continuous values, units, and temporal attributes, while a Longformer-based encoder enables efficient modeling of long trajectories. PULSE-ICU was fine-tuned across 18 prediction tasks, including mortality, intervention forecasting, and phenotype identification, achieving strong performance across task types. External validation on eICU, HiRID, and P12 showed substantial improvements with minimal fine-tuning, demonstrating robustness to domain shift and variable constraints. These findings suggest that foundation-style modeling can improve data efficiency and adaptability, providing a scalable framework for ICU decision support across diverse clinical environments.

</details>


### [56] [BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning](https://arxiv.org/abs/2511.22210)
*Junsung Park*

Main category: cs.LG

TL;DR: BiCQL-ML is a policy-free offline IRL method that jointly learns a reward function and a conservative Q-function via bi-level optimization, achieving better reward recovery and downstream policy performance without explicit policy learning.


<details>
  <summary>Details</summary>
Motivation: Offline IRL must infer rewards from fixed demonstrations without interactive data; existing methods rely on policy optimization and can generalize poorly. A policy-free approach that directly optimizes a reward while maintaining a conservative Q-function can improve reward recovery and robustness.

Method: BiCQL-ML alternates between (i) learning a conservative Q-function using Conservative Q-Learning (CQL) under the current reward, and (ii) updating reward parameters to maximize expert-action Q-values while penalizing over-generalization to out-of-distribution actions. This bi-level optimization is interpretable as maximum likelihood under a soft value matching principle, and comes with convergence guarantees to a reward making the expert soft-optimal.

Result: Empirical results on standard offline RL benchmarks show that BiCQL-ML improves reward recovery and downstream policy performance compared with existing offline IRL baselines.

Conclusion: BiCQL-ML provides a theoretically grounded, policy-free framework for offline IRL that converges to rewards consistent with soft-optimal expert behavior and yields practical performance gains.

Abstract: Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.

</details>


### [57] [FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2511.22265)
*Yuan Yao,Lixu Wang,Jiaqi Wu,Jin Song,Simin Chen,Zehua Wang,Zijian Tian,Wei Chen,Huixia Li,Xiaoxiao Li*

Main category: cs.LG

TL;DR: FedRE通过对本地表征进行随机权重混成来实现模型异构的联邦学习，并以 entangled-label 编码对全局分类器进行监督，从而在不牺牲隐私和降低通信成本的前提下提升鲁棒性和泛化。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，大多数方法假设同构模型，但真实场景中客户端在数据分布和资源方面存在差异，导致模型架构需要具备异构性。同时，存在隐私保护与通信成本的压力，容易出现全局分类器的过度自信和容易被反演的风险。FedRE 旨在解决模型异构、隐私保护与通信效率三者之间的矛盾。

Method: 设计 entangled representation：每个客户端将本地表征聚合成一个单一的 entangled 表征，使用归一化的随机权重，并将相同的权重应用于将相应的一热标签编码整合为 entangled-label 编码。上传到服务器来训练全局分类器。在训练中，entangled 表征通过 entangled-label 编码进行跨类别监督；每轮重新采样随机权重以引入多样性，防止全局分类器过度自信并促进更平滑的决策边界。此外，每个客户端仅上传一个跨类别的 entangled 表征及其 entangled-label 编码，降低面向反演攻击的风险并降低通信开销。

Result: 在广泛实验中，FedRE 在模型性能、隐私保护和通信开销之间实现有效权衡，表现出竞争力，且在不同设置下具有鲁棒性。代码在 GitHub 提供。

Conclusion: FedRE 为模型异构的联邦学习提供一种新范式，通过 entangled representation 与随机权重的重复采样，既提升隐私和通信效率，又维持或提升分类性能，展示了在真实异质环境中的应用潜力。

Abstract: Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE.

</details>


### [58] [TreeCoder: Systematic Exploration and Optimisation of Decoding and Constraints for LLM Code Generation](https://arxiv.org/abs/2511.22277)
*Henrijs Princis,Arindam Sharma,Cristina David*

Main category: cs.LG

TL;DR: 将解码视为树搜索，系统性优化解码策略和约束函数，显著提升代码生成的准确性与结构性。


<details>
  <summary>Details</summary>
Motivation: 解决仅通过自然语言提示难以保证代码的语法/语义正确性的问题，提供一个通用的解码优化框架。

Method: 将解码过程建模为候选程序的树搜索，将解码策略与约束函数（如风格、语法、执行等）作为一等公民进行优化；利用标准优化方法自动调优解码配置；在 MBPP（Python）和 SQL-Spider 基准上进行评估，覆盖 CodeLlama、Mistral、DeepSeek 等开源模型。

Result: 在上述基准和模型上，TreeCoder 显著提升准确性，通常超过未约束基线，且改进幅度显著。

Conclusion: TreeCoder 提供了一种灵活且通用的解码框架，通过对解码配置的系统性搜索，提升代码生成的正确性和结构性，具有良好的跨模型和跨基准的适用性。

Abstract: Large language models (LLMs) have shown remarkable ability to generate code, yet their outputs often violate syntactic or semantic constraints when guided only through natural language prompts. We introduce TreeCoder, the most general and flexible framework to date for exploring decoding strategies, constraints, and hyperparameters in LLMs, and use it in code generation to enforce correctness and structure during decoding rather than relying on prompt engineering. TreeCoder represents decoding as a tree search over candidate programs, where both decoding strategies and constraint functions - such as style, syntax, execution - are treated as first-class, optimisable components. This design enables systematic exploration and automatic tuning of decoding configurations using standard optimisation techniques. Experiments on the MBPP (Python) and SQL-Spider benchmarks show that TreeCoder consistently improves accuracy across open-source models such as CodeLlama, Mistral and DeepSeek, often outperforming their unconstrained baselines by considerable margins.

</details>


### [59] [The Hidden Cost of Approximation in Online Mirror Descent](https://arxiv.org/abs/2511.22283)
*Ofir Schlisselberg,Uri Sherman,Tomer Koren,Yishay Mansour*

Main category: cs.LG

TL;DR: Inexact Online Mirror Descent (OMD) 研究揭示正则化器的光滑性与对近似误差的鲁棒性之间的关系。若正则化器具备均匀光滑性，则误差导致的超额遗憾有紧致界；对单纯形及其子集的 barrier 正则化器存在显著差异：负熵需要指数级极小的误差才能避免线性遗憾，而对数屏障和 Tsallis 正则在多项式误差下也保持鲁棒。若损失为随机且域为单纯形，负熵重新具备鲁棒性，但这并不扩展到所有子集，需要仍然存在指数级误差才能避免次优遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决实践中 OMD 的近似子问题对性能的影响问题。现有分析多基于理想无误差设定，本文系统研究不完美 OMD 的鲁棒性，揭示正则化器性质（光滑度）与误差容忍度的关系。

Method: 对不完美 OMD 进行理论分析，推导误差对超额遗憾的界，区分不同正则化器：均匀光滑 vs 非均匀光滑；对 barrier 正则在单纯形及子集的分离性分析；在随机损失且域为单纯形的情形，考察负熵的鲁棒性及其局部非普适性。

Result: 给出在正则化器具备均匀光滑性时误差对超额遗憾的紧致界；在 barrier 正则（对称于单纯形及子集）上出现明确分离：负熵需要指数级小的误差才能避免线性遗憾，而对数屏障和 Tsallis 正则在多项式误差下仍具鲁棒性；在随机损失与单纯形域下，负熵的鲁棒性回归，但并非对所有子集都成立，仍需指数级误差以避免劣化的遗憾。

Conclusion: 正则化器的光滑性对不完美 OMD 的鲁棒性起决定性作用；barrier 正则化在不同域呈现不同的鲁棒性分层，设计时需权衡误差水平与域约束；在随机情形下，负熵可获得局部鲁棒性，但并非普适适用，某些子集仍需极高精度才能维持良好性能。

Abstract: Online mirror descent (OMD) is a fundamental algorithmic paradigm that underlies many algorithms in optimization, machine learning and sequential decision-making. The OMD iterates are defined as solutions to optimization subproblems which, oftentimes, can be solved only approximately, leading to an inexact version of the algorithm. Nonetheless, existing OMD analyses typically assume an idealized error free setting, thereby limiting our understanding of performance guarantees that should be expected in practice. In this work we initiate a systematic study into inexact OMD, and uncover an intricate relation between regularizer smoothness and robustness to approximation errors. When the regularizer is uniformly smooth, we establish a tight bound on the excess regret due to errors. Then, for barrier regularizers over the simplex and its subsets, we identify a sharp separation: negative entropy requires exponentially small errors to avoid linear regret, whereas log-barrier and Tsallis regularizers remain robust even when the errors are only polynomial. Finally, we show that when the losses are stochastic and the domain is the simplex, negative entropy regains robustness-but this property does not extend to all subsets, where exponentially small errors are again necessary to avoid suboptimal regret.

</details>


### [60] [Online Dynamic Pricing of Complementary Products](https://arxiv.org/abs/2511.22291)
*Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出面向互补品的在线学习动态定价框架，结合整数规划识别交互并用异方差高斯过程的多臂 bandit 求解定价，以提升总收入。


<details>
  <summary>Details</summary>
Motivation: 传统的定价多为静态模型或独立定价，忽略了相关商品之间的需求联动，无法充分挖掘协同定价潜力。

Method: 1) 使用交易数据通过整数规划识别商品间的正负交互关系；2) 基于异方差高斯过程的多臂 bandit 在识别到的交互结构下进行定价优化；3) 在仿真环境中验证性能。

Result: 在仿真中，所提出的算法相对于忽略商品交互的学习算法显示出收入提高。

Conclusion: 将商品间需求交互纳入动态定价的学习框架能显著提升收益，所提出的方法在数据驱动和计算高效性方面具备潜力。

Abstract: Traditional pricing paradigms, once dominated by static models and rule-based heuristics, are increasingly being replaced by dynamic, data-driven approaches powered by machine learning algorithms. Despite their growing sophistication, most dynamic pricing algorithms focus on optimizing the price of each product independently, disregarding potential interactions among items. By neglecting these interdependencies in consumer demand across related goods, sellers may fail to capture the full potential of coordinated pricing strategies. In this paper, we address this problem by exploring dynamic pricing mechanisms designed explicitly for complementary products, aiming to exploit their joint demand structure to maximize overall revenue. We present an online learning algorithm considering both positive and negative interactions between products' demands. The algorithm utilizes transaction data to identify advantageous complementary relationships through an integer programming problem between different items, and then optimizes pricing strategies using data-driven and computationally efficient multi-armed bandit solutions based on heteroscedastic Gaussian processes. We validate our solution in a simulated environment, and we demonstrate that our solution improves the revenue w.r.t. a comparable learning algorithm ignoring such interactions.

</details>


### [61] [FLUX: Efficient Descriptor-Driven Clustered Federated Learning under Arbitrary Distribution Shifts](https://arxiv.org/abs/2511.22305)
*Dario Fenoglio,Mohan Li,Pietro Barbiero,Nicholas D. Lane,Marc Langheinrich,Martin Gjoreski*

Main category: cs.LG

TL;DR: 提出 FLUX，一种基于聚类的联邦学习框架，解决非 IID 分布下的四类分布漂移，支持无先验和测试时自适应，提升准确性且开销与 FedAvg 相当。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习假设 IID，现实数据通常非 IID，导致全局模型性能下降；需要一种无需事先知道分布漂移类型或簇数、能在测试阶段适应未见用户的方法。

Method: 通过隐私保护的客户端描述符提取和无监督聚类，构建簇特定模型；在训练和测试阶段均可分配客户端到最近簇，且提供测试时适应机制；与 FedAvg 的通信和计算开销保持相近。

Result: 在四个基准、两个真实数据集和十个基线下，FLUX 提升平均准确性至多 23 个点，且在多种分布漂移情景下更稳健。

Conclusion: FLUX 为 CFL 提供无需先验信息的聚类化联邦学习方案，具备测试时自适应和良好可扩展性，适用于真实世界的非 IID 场景。

Abstract: Federated Learning (FL) enables collaborative model training across multiple clients while preserving data privacy. Traditional FL methods often use a global model to fit all clients, assuming that clients' data are independent and identically distributed (IID). However, when this assumption does not hold, the global model accuracy may drop significantly, limiting FL applicability in real-world scenarios. To address this gap, we propose FLUX, a novel clustering-based FL (CFL) framework that addresses the four most common types of distribution shifts during both training and test time. To this end, FLUX leverages privacy-preserving client-side descriptor extraction and unsupervised clustering to ensure robust performance and scalability across varying levels and types of distribution shifts. Unlike existing CFL methods addressing non-IID client distribution shifts, FLUX i) does not require any prior knowledge of the types of distribution shifts or the number of client clusters, and ii) supports test-time adaptation, enabling unseen and unlabeled clients to benefit from the most suitable cluster-specific models. Extensive experiments across four standard benchmarks, two real-world datasets and ten state-of-the-art baselines show that FLUX improves performance and stability under diverse distribution shifts, achieving an average accuracy gain of up to 23 percentage points over the best-performing baselines, while maintaining computational and communication overhead comparable to FedAvg.

</details>


### [62] [SingleQuant: Efficient Quantization of Large Language Models in a Single Pass](https://arxiv.org/abs/2511.22316)
*Jinying Xiao,Bin Ji,Shasha Li,Xiaodong Liu,Ma Jun,Ye Zhong,Wei Li,Xuan Xie,Qingbo Wu,Jie Yu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) quantization facilitates deploying LLMs in resource-limited settings, but existing methods that combine incompatible gradient optimization and quantization truncation lead to serious convergence pathology. This prolongs quantization time and degrades LLMs' task performance. Our studies confirm that Straight-Through Estimator (STE) on Stiefel manifolds introduce non-smoothness and gradient noise, obstructing optimization convergence and blocking high-fidelity quantized LLM development despite extensive training. To tackle the above limitations, we propose SingleQuant, a single-pass quantization framework that decouples from quantization truncation, thereby eliminating the above non-smoothness and gradient noise factors. Specifically, SingleQuant constructs Alignment Rotation Transformation (ART) and Uniformity Rotation Transformation (URT) targeting distinct activation outliers, where ART achieves smoothing of outlier values via closed-form optimal rotations, and URT reshapes distributions through geometric mapping. Both matrices comprise strictly formulated Givens rotations with predetermined dimensions and rotation angles, enabling promising LLMs task performance within a short time. Experimental results demonstrate SingleQuant's superiority over the selected baselines across diverse tasks on 7B-70B LLMs. To be more precise, SingleQuant enables quantized LLMs to achieve higher task performance while necessitating less time for quantization. For example, when quantizing LLaMA-2-13B, SingleQuant achieves 1,400$\times$ quantization speedup and increases +0.57\% average task performance compared to the selected best baseline.

</details>


### [63] [Cleaning the Pool: Progressive Filtering of Unlabeled Pools in Deep Active Learning](https://arxiv.org/abs/2511.22344)
*Denis Huseljic,Marek Herde,Lukas Rauch,Paul Hahn,Bernhard Sick*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Existing active learning (AL) strategies capture fundamentally different notions of data value, e.g., uncertainty or representativeness. Consequently, the effectiveness of strategies can vary substantially across datasets, models, and even AL cycles. Committing to a single strategy risks suboptimal performance, as no single strategy dominates throughout the entire AL process. We introduce REFINE, an ensemble AL method that combines multiple strategies without knowing in advance which will perform best. In each AL cycle, REFINE operates in two stages: (1) Progressive filtering iteratively refines the unlabeled pool by considering an ensemble of AL strategies, retaining promising candidates capturing different notions of value. (2) Coverage-based selection then chooses a final batch from this refined pool, ensuring all previously identified notions of value are accounted for. Extensive experiments across 6 classification datasets and 3 foundation models show that REFINE consistently outperforms individual strategies and existing ensemble methods. Notably, progressive filtering serves as a powerful preprocessing step that improves the performance of any individual AL strategy applied to the refined pool, which we demonstrate on an audio spectrogram classification use case. Finally, the ensemble of REFINE can be easily extended with upcoming state-of-the-art AL strategies.

</details>


### [64] [AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse Edge Devices](https://arxiv.org/abs/2511.22355)
*Mengyang Liu,Chenyu Lu,Haodong Tian,Fang Dong,Ruiting Zhou,Wei Wang,Dian Shen,Guangtong Li,Ye Wan,Li Li*

Main category: cs.LG

TL;DR: AutoTailor 自动化端到端的基于 SuperNet 的自适应模型部署框架，将用户模型自动转换为 SuperNet，并通过学习无关的延迟与准确性预测提升预测成本与准确性，显著降低代码量和硬件 profiling 成本，同时提升准确性与降低延迟。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备场景中，需针对异构设备能力和性能需求实现高效的推理自适应部署。现有的基于 SuperNet 的方法需要繁琐的模型感知开发和耗时的硬件感知 profiling，制约实际应用。

Method: 引入计算图引导的编译过程，自动将用户提供的模型转化为 SuperNet；引入学习无关的延迟和准确性预测器，提供低成本却准确的性能预测；实现端到端自动化的自适应部署流程。

Result: 在多种模型和设备上，与现有方法相比，代码量减少11–27倍，硬件感知 profiling 成本至少下降11倍，准确性提高至多15.60个百分点，平均延迟降低高达60.03%。

Conclusion: AutoTailor 使基于 SuperNet 的自适应部署实现自动化、端到端的工作流，显著降低开发成本并提升推理性能，增强边缘设备上的应用可行性与普适性。

Abstract: On-device machine learning (ML) has become a fundamental component of emerging mobile applications. Adaptive model deployment delivers efficient inference for heterogeneous device capabilities and performance requirements through customizing neural architectures. SuperNet-based approaches offer a promising solution by generating a large number of model variants from a pre-trained ML model. However, applying SuperNet in existing frameworks suffers from tedious model-aware development and time-consuming hardware-aware profiling, which limits their practical adoption.
  We present AutoTailor, the first framework to enable automated, end-to-end SuperNet-based adaptive model deployment for edge devices. Unlike manual SuperNet construction, AutoTailor employs a computation graph-guided compilation approach to automatically transform user-provided ML models into SuperNets. To support efficient specialization, AutoTailor incorporates learning-free latency and accuracy predictors, enabling low-cost yet accurate performance prediction. Our extended evaluations demonstrate that AutoTailor reduces the lines of code for SuperNet construction by 11--27$\times$, decreases hardware-aware profiling costs by at least 11$\times$, and achieves up to 15.60\% absolute accuracy improvement and 60.03\% latency reduction compared to state-of-the-art approaches across diverse models and devices.

</details>


### [65] [Efficient-Husformer: Efficient Multimodal Transformer Hyperparameter Optimization for Stress and Cognitive Loads](https://arxiv.org/abs/2511.22362)
*Merey Orazaly,Fariza Temirkhanova,Jurn-Gyu Park*

Main category: cs.LG

TL;DR: Efficient-Husformer: a hyperparameter-optimized Transformer for multimodal physiological stress detection that achieves ~30k parameters while outperforming Husformer on WESAD and CogLoad.


<details>
  <summary>Details</summary>
Motivation: Transformers capture long-range temporal patterns in physiological signals but are computationally intensive; there is a need for efficient, compact architectures that maintain high accuracy for wearable-based stress detection.

Method: A structured hyperparameter search space (HPO) for a Husformer-like Transformer, accompanied by an ablation study. The optimal configurations use a single Transformer layer with 3 heads, specific model/FFN dimensions, and selected modality pairs (L+dm or L+FFN), yielding a compact model (~30k params).

Result: Best accuracies achieved: 88.41 (WESAD) and 92.61 (CogLoad), with improvements of 13.83% and 6.98% over the original Husformer. The best-performing configurations use (L+dm) or (L+FFN) modalities, a single layer, 3 attention heads, model dimensions 18/30, FFN dimensions 120/30, respectively.

Conclusion: Hyperparameter optimization can produce compact, high-performing Transformer architectures for multimodal physiological signal analysis. Efficient-Husformer demonstrates strong, consistent gains over Husformer and highlights effective modality combinations and small-footprint designs for stress detection.

Abstract: Transformer-based models have gained considerable attention in the field of physiological signal analysis. They leverage long-range dependencies and complex patterns in temporal signals, allowing them to achieve performance superior to traditional RNN and CNN models. However, they require high computational intensity and memory demands. In this work, we present Efficient-Husformer, a novel Transformer-based architecture developed with hyperparameter optimization (HPO) for multi-class stress detection across two multimodal physiological datasets (WESAD and CogLoad). The main contributions of this work are: (1) the design of a structured search space, targeting effective hyperparameter optimization; (2) a comprehensive ablation study evaluating the impact of architectural decisions; (3) consistent performance improvements over the original Husformer, with the best configuration achieving an accuracy of 88.41 and 92.61 (improvements of 13.83% and 6.98%) on WESAD and CogLoad datasets, respectively. The best-performing configuration is achieved with the (L + dm) or (L + FFN) modality combinations, using a single layer, 3 attention heads, a model dimension of 18/30, and FFN dimension of 120/30, resulting in a compact model with only about 30k parameters.

</details>


### [66] [SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning](https://arxiv.org/abs/2511.22367)
*Hugo Hazard,Zafeirios Fountas,Martin A. Benfeghoul,Adnan Oomerjee,Jun Wang,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 提出两方面改进以提升持续学习对大型语言模型的鲁棒性：1) 选取阶段通过 Surprise-prioritised Replay (SuRe) 基于高负对数似然的“惊异性”来筛选并存储训练样本；2) 整合阶段采用快速与慢速 LoRA 适配器的双学习者，通过指数移动平均合并以实现快速适应与长期知识稳定。结合两者可提升在大量任务场景（LNT）及标准CL基准上的表现，并对回放频率和缓存规模的鲁棒性良好。结论指出回放是持续学习中大规模语言模型的有力基线，惊异性选择与慢权重整合是互补的关键因素。


<details>
  <summary>Details</summary>
Motivation: 持续学习在任务序列上避免遗忘仍然是机器学习中的主要挑战，尤其是在大语言模型的规模化多任务场景下，常用正则化与回放在性能上仍落后于多任务学习。本文聚焦回放的两大失败模态：选择（应回放哪些样本）和整合（如何稳定地整合新知识），提出两种方案来提升回放效果与长期记忆的稳定性。

Method: 1) Surprise-prioritised Replay (SuRe): 一种与具体架构无关的规则，通过对序列的负对数似然（NLL）进行排序，优先存储最具“惊异性”的样本。2) 双学习者设计：包含快速与慢速的 LoRA 适配器，通过指数加权平均（EMA）进行合并，实现快速适应和知识稳定。3) 将 SuRe 与双学习者结合，评估在标准CL和LNT基准上的性能。

Result: 在“大数量任务”设置（LNT）中达到SOTA水平；在标准CL与LNT基准上实现最佳平均表现；对LNT的提升可达约+5个准确度点；消融研究显示在回放频率降低和缓存容量较小的情况下仍保持鲁棒性，体现了方法的有效性与样本效率。

Conclusion: 回放仍是持续学习中对大规模LLM微调的有力基线；通过惊异性选择和慢权重整合的互补性，能够缓解灾难性遗忘并提升对多任务的适应性与稳定性。

Abstract: Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.

</details>


### [67] [Predicting and Interpolating Spatiotemporal Environmental Data: A Case Study of Groundwater Storage in Bangladesh](https://arxiv.org/abs/2511.22378)
*Anna Pazola,Mohammad Shamsudduha,Richard G. Taylor,Allan Tucker*

Main category: cs.LG

TL;DR: 比较两种基于深度学习的栅格到栅格/栅格到点的方法来处理地理观测数据稀疏的时空预测问题；用孟加拉地下水储量数据为案例；结论指出空间插值比时间预测更困难，最近邻并非总是最相似，地质的不确定性显著影响点位时间行为；提出基于时间序列聚类的高级插值方法的未来方向；代码公开。


<details>
  <summary>Details</summary>
Motivation: 解决仅有点观测数据导致的连续场的时空推断问题；比较两种策略：聚合前建模（grid-to-grid）与聚合后建模（grid-to-point）在孟加拉地下水储量数据上的表现；评估哪种更有效；为基于时序动力学的聚类插值方法提供研究动机。

Method: 两种深度学习策略：1) grid-to-grid：把栅格预测变量用于建模栅格化的目标（聚合前建模/聚合前映射）；2) grid-to-point：用栅格预测变量建模点目标，然后再用克里金插值填充域（聚合后建模/后处理）；数据来源孟加拉地下水储量。对比评估性能；分析最近邻、地质不确定性对点观测时序行为的影响；提示未来工作方向。

Result: 研究发现空间插值显著比时序预测困难；最近邻并非总是最相似；地质不确定性显著影响点位时序行为；两种策略在不同场景下各有优劣，结论对其他受间接可观测因素支配的环境变量具有普遍性。

Conclusion: 未来工作应加强基于时序动力学的地点聚类，以提升插值方法的性能；代码可在 GitHub 获取。

Abstract: Geospatial observational datasets are often limited to point measurements, making temporal prediction and spatial interpolation essential for constructing continuous fields. This study evaluates two deep learning strategies for addressing this challenge: (1) a grid-to-grid approach, where gridded predictors are used to model rasterised targets (aggregation before modelling), and (2) a grid-to-point approach, where gridded predictors model point targets, followed by kriging interpolation to fill the domain (aggregation after modelling). Using groundwater storage data from Bangladesh as a case study, we compare the effcacy of these approaches. Our findings indicate that spatial interpolation is substantially more difficult than temporal prediction. In particular, nearest neighbours are not always the most similar, and uncertainties in geology strongly influence point temporal behaviour. These insights motivate future work on advanced interpolation methods informed by clustering locations based on time series dynamics. Demonstrated on groundwater storage, the conclusions are applicable to other environmental variables governed by indirectly observable factors. Code is available at https://github.com/pazolka/interpolation-prediction-gwsa.

</details>


### [68] [TS2Vec-Ensemble: An Enhanced Self-Supervised Framework for Time Series Forecasting](https://arxiv.org/abs/2511.22395)
*Ganeshan Niroshan,Uthayasanker Thayasivam*

Main category: cs.LG

TL;DR: TS2Vec-Ensemble 将自监督预训练表示与显式时间特征进行混合，通过自适应权重的双头回归，在每个预测时 horizon 上动态平衡短期动力学与长期季节性，显著提升长序列预测在 ETT 基准数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习（如 TS2Vec）在时间序列分析中效果突出，但在预测任务中常因训练目标偏向实例判别而忽视周期性、趋势等确定性模式，导致长序列预测性能受限。

Method: 提出 TS2Vec-Ensemble：在预训练 TS2Vec 编码器基础上，融入显式周期性特征；通过双头回归模型分别拟合学习到的动态与季节性模式，并采用对每个预测时 horizon 独立优化的自适应权重进行融合。

Result: 在单变量与多变量的 ETT 基准数据集上，对比基线 TS2Vec 和其他时序模型，TS2Vec-Ensemble 展现一致且显著的性能提升，验证混合表示与显式时间先验的有效性。

Conclusion: 将隐式学习的动力学与显式原型化的时间先验结合，是实现长 horizon 时间序列预测的有效策略，适用于需要捕捉周期性与趋势的应用场景。

Abstract: Self-supervised representation learning, particularly through contrastive methods like TS2Vec, has advanced the analysis of time series data. However, these models often falter in forecasting tasks because their objective functions prioritize instance discrimination over capturing the deterministic patterns, such as seasonality and trend, that are critical for accurate prediction. This paper introduces TS2Vec-Ensemble, a novel hybrid framework designed to bridge this gap. Our approach enhances the powerful, implicitly learned dynamics from a pretrained TS2Vec encoder by fusing them with explicit, engineered time features that encode periodic cycles. This fusion is achieved through a dual-model ensemble architecture, where two distinct regression heads -- one focused on learned dynamics and the other on seasonal patterns -- are combined using an adaptive weighting scheme. The ensemble weights are optimized independently for each forecast horizon, allowing the model to dynamically prioritize short-term dynamics or long-term seasonality as needed. We conduct extensive experiments on the ETT benchmark datasets for both univariate and multivariate forecasting. The results demonstrate that TS2Vec-Ensemble consistently and significantly outperforms the standard TS2Vec baseline and other state-of-the-art models, validating our hypothesis that a hybrid of learned representations and explicit temporal priors is a superior strategy for long-horizon time series forecasting.

</details>


### [69] [PISA: Prioritized Invariant Subgraph Aggregation](https://arxiv.org/abs/2511.22435)
*Ali Ghasemi,Farooq Ahmad Wani,Maria Sofia Bucarelli,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: PISA通过动态MLP聚合，在多个子图表征间进行优先级排序和融合，从而提升图数据OOD泛化的鲁棒性，在15个数据集上比先前方法提高最多5%。


<details>
  <summary>Details</summary>
Motivation: 将不变性原理扩展到图数据时，单一不变量子图往往难以覆盖多样的因果模式；先前CIGA只提取单一子图，SuGAr虽考量多样子图但聚合方式较为简单，存在对多样模式整合不足的问题；因此需要更高效的聚合策略来整合多样的不变量子图以提升鲁棒性。

Method: 在SuGAr的基础上引入动态MLP聚合器，对从子图中获得的表征进行自适应加权和组合，以优先级驱动不同子图的融合，实现对多样因果模式的更有效整合；并在包含DrugOOD在内的15个数据集上进行评估。

Result: PISA在15个数据集上实现高于先前方法的分类精度，提升幅度最高可达约5%。

Conclusion: 动态MLP聚合能够更有效地整合多样的 invariant 子图表征，提升对分布偏移的鲁棒性，验证了图数据的OOD泛化性研究的有效性。

Abstract: Recent work has extended the invariance principle for out-of-distribution (OOD) generalization from Euclidean to graph data, where challenges arise due to complex structures and diverse distribution shifts in node attributes and topology. To handle these, Chen et al. proposed CIGA (Chen et al., 2022b), which uses causal modeling and an information-theoretic objective to extract a single invariant subgraph capturing causal features. However, this single-subgraph focus can miss multiple causal patterns. Liu et al. (2025) addressed this with SuGAr, which learns and aggregates diverse invariant subgraphs via a sampler and diversity regularizer, improving robustness but still relying on simple uniform or greedy aggregation. To overcome this, the proposed PISA framework introduces a dynamic MLP-based aggregation that prioritizes and combines subgraph representations more effectively. Experiments on 15 datasets, including DrugOOD (Ji et al., 2023), show that PISA achieves up to 5% higher classification accuracy than prior methods.

</details>


### [70] [An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction](https://arxiv.org/abs/2511.22460)
*Yifan Lei,Jiahua Luo,Tingyu Jiang,Bo Zhang,Lifeng Wang,Dapeng Liu,Zhaoren Wu,Haijie Gu,Huan Yu,Jie Jiang*

Main category: cs.LG

TL;DR: 提出基于 GPU 的高效特征交互框架，改进双塔检索中的交互能力，通过压缩倒排表实现大规模特征交互，首次在检索阶段实现 Wide & Deep，已在腾讯广告系统落地并带来显著在线收益。


<details>
  <summary>Details</summary>
Motivation: 双塔嵌入仅在最终点积阶段进行交互，导致特征交互能力不足；在检索阶段采用深层交互的 DNN 模型成本高、不可行；需要在保持检索效率的前提下提升检索精度。

Method: 提出适用于 GPU 的高效特征交互方法，设计压缩倒排表以实现大规模特征交互的并行计算，将 Wide 与 Deep 的思想集成到检索系统中，形成一个工业级实现。

Result: 离线评估优于现有方法；已成功部署到腾讯广告推荐系统，在线性能显著提升。

Conclusion: 验证了更丰富的特征交互对检索质量的重要性，提供在大规模广告检索中落地的可操作方案和实践经验。

Abstract: In large-scale advertising recommendation systems, retrieval serves as a critical component, aiming to efficiently select a subset of candidate ads relevant to user behaviors from a massive ad inventory for subsequent ranking and recommendation. The Embedding-Based Retrieval (EBR) methods modeled by the dual-tower network are widely used in the industry to maintain both retrieval efficiency and accuracy. However, the dual-tower model has significant limitations: the embeddings of users and ads interact only at the final inner product computation, resulting in insufficient feature interaction capabilities. Although DNN-based models with both user and ad as input features, allowing for early-stage interaction between these features, are introduced in the ranking stage to mitigate this issue, they are computationally infeasible for the retrieval stage. To bridge this gap, this paper proposes an efficient GPU-based feature interaction for the dual-tower network to significantly improve retrieval accuracy while substantially reducing computational costs. Specifically, we introduce a novel compressed inverted list designed for GPU acceleration, enabling efficient feature interaction computation at scale. To the best of our knowledge, this is the first framework in the industry to successfully implement Wide and Deep in a retrieval system. We apply this model to the real-world business scenarios in Tencent Advertising, and experimental results demonstrate that our method outperforms existing approaches in offline evaluation and has been successfully deployed to Tencent's advertising recommendation system, delivering significant online performance gains. This improvement not only validates the effectiveness of the proposed method, but also provides new practical guidance for optimizing large-scale ad retrieval systems.

</details>


### [71] [Adversarial Flow Models](https://arxiv.org/abs/2511.22475)
*Shanchuan Lin,Ceyuan Yang,Zhijie Lin,Hao Chen,Haoqi Fan*

Main category: cs.LG

TL;DR: Adversarial flow models unify adversarial training with flow-based generation, enabling stable one-/few-step generation via deterministic noise-to-data mappings and achieving competitive FID scores on ImageNet-256, including deep end-to-end architectures.


<details>
  <summary>Details</summary>
Motivation: Bridge adversarial and flow-based generative models to stabilize training, avoid intermediate probability-flow steps, and enable efficient, deterministic mappings with flexible generation steps.

Method: Introduce adversarial flow models where the generator implements a deterministic noise-to-data map, aligned with optimal transport as in flow-matching; support native 1-step or multi-step generation under a 1NFE/2NFE regime; enable depth repetition for end-to-end training of very deep networks without intermediate supervision.

Result: Under 1NFE on ImageNet-256, the B/2 model approaches the XL/2 consistency-based models; the XL/2 model achieves a new best FID of 2.38. Depth-repeated 56-layer and 112-layer models trained end-to-end with a single forward pass achieve FIDs of 2.08 and 1.94, surpassing the 2NFE and 4NFE baselines.

Conclusion: Adversarial flow models can stabilize adversarial training, reduce model capacity and training iterations, and enable very deep architectures with strong generation quality, matching or surpassing existing flow-consistency methods on large-scale data.

Abstract: We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.

</details>


### [72] [Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges](https://arxiv.org/abs/2511.22483)
*Guanxi Lu,Hao Mark Chen,Zhiqiang Que,Wayne Luk,Hongxiang Fan*

Main category: cs.LG

TL;DR: 提出对量化对语言模型信任worthiness的影响的系统性研究，并提出一种混合精度投票的精度集合方法，在同一模型的混合精度变体上进行投票以提升信任度指标，最高提升约5.8%。


<details>
  <summary>Details</summary>
Motivation: 现有量化框架大多关注困惑度或分类准确性，忽视在高风险领域（如金融、医疗）的信任性指标，可能导致部署风险；需要在模型压缩与信任性之间建立联系。

Method: 系统性地研究四种信任性指标（对抗鲁棒性、公平性、机器伦理、分布外鲁棒性）在不同压缩比和量化方法下的表现；在此基础上提出一种新颖的精度集合投票方法，利用同一模型的混合精度变体的预测进行投票。

Result: 在信任性指标上，方法可在不同场景下实现提升，最高提升幅度可达约5.8%。

Conclusion: 强调在开发模型压缩技术时需要考虑信任性，并指出压缩与信任性的交叉研究为安全关键应用提供了新的研究机会。

Abstract: Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.

</details>


### [73] [Privacy-Utility-Bias Trade-offs for Privacy-Preserving Recommender Systems](https://arxiv.org/abs/2511.22515)
*Shiva Parsarad,Isabel Wagner*

Main category: cs.LG

TL;DR: 在 MovieLens-1M 与 Yelp 上对四种推荐模型（NCF、BPR、SVD、VAE）应用两种差分隐私机制（DPSGD 与 LDP）进行交叉模型评估，结果显示隐私强度对准确性和偏见的影响高度异质，且没有一种机制对所有情形都优越。


<details>
  <summary>Details</summary>
Motivation: 随着差分隐私在推荐系统中的应用增多，亟需理解隐私保障对推荐准确性与公平性的影响，并在不同模型与数据条件下比较两种常用隐私机制的权衡。

Method: 在 MovieLens-1M 与 Yelp 数据集上，分别对 DPSGD 与 LDP 进行实验；对四个推荐模型（NCF、BPR、SVD、VAE）进行训练与评估；比较不同隐私强度下的准确性指标与偏差/公平性指标，同时分析流行度偏差的变化。

Result: 普遍而言，强隐私降低效用，但影响并非一致。DPSGD 下 NCF 的准确性损失最小（epsilon 约为 1 时损失约 10%），而 SVD 与 BPR 的损失更大，且在偏好边缘用户上更明显；VAE 对隐私最敏感，稀疏表示群体的性能下降显著。对偏差指标的影响也呈现高度异质性。DPSGD 往往缩小流行项与非流行项之间的差距，而 LDP 更易保持原有模式。总体而言，单一 DP 机制无普适最优解，需在具体隐私程度与数据条件下权衡取舍。

Conclusion: 研究揭示了在推荐系统中的隐私取舍高度依赖模型与数据情境；应根据目标模型与数据分布选择合适的 DP 机制及参数设置，以达到最佳隐私保护与实用性平衡，未来可探索更细粒度的隐私-公正权衡与自适应隐私机制。

Abstract: Recommender systems (RSs) output ranked lists of items, such as movies or restaurants, that users may find interesting, based on the user's past ratings and ratings from other users. RSs increasingly incorporate differential privacy (DP) to protect user data, raising questions about how privacy mechanisms affect both recommendation accuracy and fairness. We conduct a comprehensive, cross-model evaluation of two DP mechanisms, differentially private stochastic gradient descent (DPSGD) and local differential privacy (LDP), applied to four recommender systems (Neural Collaborative Filtering (NCF), Bayesian Personalized Ranking (BPR), Singular Value Decomposition (SVD), and Variational Autoencoder (VAE)) on the MovieLens-1M and Yelp datasets. We find that stronger privacy consistently reduces utility, but not uniformly. NCF under DPSGD shows the smallest accuracy loss (under 10 percent at epsilon approximately 1), whereas SVD and BPR experience larger drops, especially for users with niche preferences. VAE is the most sensitive to privacy, with sharp declines for sparsely represented groups. The impact on bias metrics is similarly heterogeneous. DPSGD generally reduces the gap between recommendations of popular and less popular items, whereas LDP preserves existing patterns more closely. These results highlight that no single DP mechanism is uniformly superior; instead, each provides trade-offs under different privacy regimes and data conditions.

</details>


### [74] [List-Decodable Regression via Expander Sketching](https://arxiv.org/abs/2511.22524)
*Herbod Pourali,Sajjad Hashemian,Ebrahim Ardeshir-Larijani*

Main category: cs.LG

TL;DR: 提出一个 expander-sketching 框架用于 list-decodable 线性回归，实现接近最优的样本复杂度和时间复杂度：样本复杂度为 tilde O((d+log(1/δ))/α)，list size 为 O(1/α)，近输入稀疏的运行时间为 tilde O(nnz(X) + d^3/α)，在标准 sub-Gaussian 假设下，且避免使用 SoS。


<details>
  <summary>Details</summary>
Motivation: 应对高度污染数据下的鲁棒回归问题，期望获得低样本复杂度、可扩展性强且不依赖重型证明工具（如 SoS）的解决方案。

Method: 通过 lossless expanders 构造轻污染的批次以实现鲁棒聚合，并进行快速的谱（spectral）筛选阶段；方法避免对批次结构的显式依赖，也无需 SoS 证明，利用 expander 的分散性和鲁棒性来提升稳定性。

Result: 在标准 sub-Gaussian 假设下达到与最优高效方法相近的保证，给出样本复杂度、列表大小和时间复杂度的具体界限；算法实现接近输入稀疏性，且避免复杂的代价高昂证明。

Conclusion: 该 expander-sketch 框架为 list-decodable 回归提供了一种简单、有效且鲁棒的解决思路，具有潜在的推广性至其他鲁棒估计任务，并可能引导更广泛的对鲁棒学习的研究方向。

Abstract: We introduce an expander-sketching framework for list-decodable linear regression that achieves sample complexity $\tilde{O}((d+\log(1/δ))/α)$, list size $O(1/α)$, and near input-sparsity running time $\tilde{O}(\mathrm{nnz}(X)+d^{3}/α)$ under standard sub-Gaussian assumptions. Our method uses lossless expanders to synthesize lightly contaminated batches, enabling robust aggregation and a short spectral filtering stage that matches the best known efficient guarantees while avoiding SoS machinery and explicit batch structure.

</details>


### [75] [Where to Measure: Epistemic Uncertainty-Based Sensor Placement with ConvCNPs](https://arxiv.org/abs/2511.22567)
*Feyza Eksen,Stefan Oehmcke,Stefan Lüdtke*

Main category: cs.LG

TL;DR: Epistemic-uncertainty-driven sensor placement using ConvCNPs with an MDN head; acquisition function based on expected reduction in epistemic uncertainty; preliminary results indicate improved model-error reduction over total-uncertainty-based approaches.


<details>
  <summary>Details</summary>
Motivation: Accurate sensor placement in spatio-temporal systems requires distinguishing epistemic and aleatoric uncertainty. Relying on total predictive uncertainty can misguide sensor placement, especially in ambiguous regions.

Method: Extend Convolutional Conditional Neural Processes (ConvCNPs) with a Mixture Density Network (MDN) output head to estimate epistemic uncertainty. Define an acquisition function as the expected reduction in epistemic uncertainty when placing a new sensor, and optimize sensor locations accordingly.

Result: Preliminary findings suggest that sensor placement guided by epistemic uncertainty reduces model error more effectively than strategies based on overall uncertainty.

Conclusion: This approach shows promise for more efficient sensor networks; further validation on larger datasets and real-world deployments is needed, along with refinements to the MDN-ConvCNP architecture and acquisition optimization.

Abstract: Accurate sensor placement is critical for modeling spatio-temporal systems such as environmental and climate processes. Neural Processes (NPs), particularly Convolutional Conditional Neural Processes (ConvCNPs), provide scalable probabilistic models with uncertainty estimates, making them well-suited for data-driven sensor placement. However, existing approaches rely on total predictive uncertainty, which conflates epistemic and aleatoric components, that may lead to suboptimal sensor selection in ambiguous regions. To address this, we propose expected reduction in epistemic uncertainty as a new acquisition function for sensor placement. To enable this, we extend ConvCNPs with a Mixture Density Networks (MDNs) output head for epistemic uncertainty estimation. Preliminary results suggest that epistemic uncertainty driven sensor placement more effectively reduces model error than approaches based on overall uncertainty.

</details>


### [76] [The Multiclass Score-Oriented Loss (MultiSOL) on the Simplex](https://arxiv.org/abs/2511.22587)
*Francesco Marchetti,Edoardo Legnaro,Sabrina Guastavino*

Main category: cs.LG

TL;DR: 提出了 Multiclass Score-Oriented Loss (MultiSOL)，将分数导向的损失扩展到多分类，通过多维阈值框架直接优化目标指标，并对类别不平衡具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在二分类中通过将阈值视为带先验分布的随机变量直接优化性能指标的思路，需拓展到多分类以实现直接目标指标优化和对不平衡的鲁棒性。

Method: 引入多维阈值分类框架，定义 MultiSOL 损失，阈值作为随机变量并给定先验分布；利用单纯形几何进行学习；通过分类实验评估性能。

Result: 实验显示 MultiSOL 保留二分类中的直接优化目标指标与对类别不平衡的鲁棒性，达到与现有先进损失函数相当的性能，并给出单纯形几何与分数导向学习交互的新见解。

Conclusion: MultiSOL 成功将分数导向损失扩展到多分类，提升直接优化目标指标的能力并增强对不平衡数据的鲁棒性，丰富了多分类中的评分导向学习理论与应用。

Abstract: In the supervised binary classification setting, score-oriented losses have been introduced with the aim of optimizing a chosen performance metric directly during the training phase, thus avoiding \textit{a posteriori} threshold tuning. To do this, in their construction, the decision threshold is treated as a random variable provided with a certain \textit{a priori} distribution. In this paper, we use a recently introduced multidimensional threshold-based classification framework to extend such score-oriented losses to multiclass classification, defining the Multiclass Score-Oriented Loss (MultiSOL) functions. As also demonstrated by several classification experiments, this proposed family of losses is designed to preserve the main advantages observed in the binary setting, such as the direct optimization of the target metric and the robustness to class imbalance, achieving performance comparable to other state-of-the-art loss functions and providing new insights into the interaction between simplex geometry and score-oriented learning.

</details>


### [77] [Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers](https://arxiv.org/abs/2511.22616)
*Meriem Arbaoui,Mohamed-el-Amine Brahmia,Abdellatif Rahmoun,Mourad Zghal*

Main category: cs.LG

TL;DR: 本文是一篇关于联邦学习（FL）的综述，聚焦个性化、优化与鲁棒性三大研究方向，提供基于混合方法的系统评估框架，梳理聚合策略、架构、同步机制及不同联盟目标，讨论异质性、效率、安全与隐私等挑战，并给出在IID与非IID数据分布下的实证比较，最终指向未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在物联网和人工智能融合场景中，传统集中式机器学习面临隐私保护与数据孤岛问题，促使提出去中心化的联邦学习框架以实现跨机构协同建模。FL在保障隐私、降低通信开销和提升可扩展性方面具潜力，但对异质性、效率、鲁棒性和安全性等挑战尚待系统化研究。因此，本文通过混合的文献分析方法对FL领域进行结构化梳理，聚焦个性化、优化、鲁棒性三大方向，提出影响力最大的工作及未来研究方向。

Method: 采用混合研究范式：结合文献计量学分析与系统综述，对FL相关工作进行分类、聚焦聚合策略、架构、同步方法与不同联盟目标的比较，辅之以实际评估方法的讨论。并通过在IID与非IID数据分布下的实验对比，评估不同聚合方法的表现。

Result: 构建了一个面向FL的研究方向的结构化分类框架，系统梳理了个性化、优化、鲁棒性等关键研究领域的技术与挑战，涵盖聚合策略、体系结构、同步机制与多样的联盟目标；给出实践层面的评估框架与实验结果（IID/非IID下的聚合方法比较），并汇总了最具影响力的工作。

Conclusion: 提出了有潜力推动FL的发展方向，强调在异质性、效率与安全隐私等方面的关键挑战上需要更高效的聚合机制、更具鲁棒性的评估方案以及更贴近实际部署的实验设计，为后续研究提供指南与方向。

Abstract: The integration of IoT and AI has unlocked innovation across industries, but growing privacy concerns and data isolation hinder progress. Traditional centralized ML struggles to overcome these challenges, which has led to the rise of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing local raw data. FL ensures data privacy, reduces communication overhead, and supports scalability, yet its heterogeneity adds complexity compared to centralized approaches. This survey focuses on three main FL research directions: personalization, optimization, and robustness, offering a structured classification through a hybrid methodology that combines bibliometric analysis with systematic review to identify the most influential works. We examine challenges and techniques related to heterogeneity, efficiency, security, and privacy, and provide a comprehensive overview of aggregation strategies, including architectures, synchronization methods, and diverse federation objectives. To complement this, we discuss practical evaluation approaches and present experiments comparing aggregation methods under IID and non-IID data distributions. Finally, we outline promising research directions to advance FL, aiming to guide future innovation in this rapidly evolving field.

</details>


### [78] [Flow Density Control: Generative Optimization Beyond Entropy-Regularized Fine-Tuning](https://arxiv.org/abs/2511.22640)
*Riccardo De Santi,Marin Vlastelica,Ya-Ping Hsieh,Zebang Shen,Niao He,Andreas Krause*

Main category: cs.LG

TL;DR: 提出 Flow Density Control (FDC)，将复杂的目标优化分解为一系列可扩展的微调任务，通过对比流/密度控制实现对预训练模型的保留，提供收敛性保证，并在文本到图像与分子设计等任务上超越现有微调方法。


<details>
  <summary>Details</summary>
Motivation: 在大规模基础模型上实现对任务特定目标的优化，同时尽可能保留原有知识，覆盖风险规避、新颖性、探索性、多样性、以及实验设计等更广泛的目标，而非仅仅追求平均奖励。

Method: 提出 Flow Density Control (FDC)，将复杂优化问题转化为一系列简单的微调任务，每个任务可用现有的可扩展方法解决；基于镜像流的理论框架建立收敛性分析；引入除了 KL 以外的保留信息度量，如最优传输距离和 Renyi 散度。

Result: 在说明性场景、文本到图像与分子设计任务上验证，FDC 能引导预训练生成模型实现更广泛的目标优化，并在实际任务中超过现有微调方案。

Conclusion: FDC 提供一种简单且可扩展的框架，适用于多样化的目标函数和保留策略，具有理论收敛性保证，在实际应用中展现出比当前方法更强的任务适应性。

Abstract: Adapting large-scale foundation flow and diffusion generative models to optimize task-specific objectives while preserving prior information is crucial for real-world applications such as molecular design, protein docking, and creative image generation. Existing principled fine-tuning methods aim to maximize the expected reward of generated samples, while retaining knowledge from the pre-trained model via KL-divergence regularization. In this work, we tackle the significantly more general problem of optimizing general utilities beyond average rewards, including risk-averse and novelty-seeking reward maximization, diversity measures for exploration, and experiment design objectives among others. Likewise, we consider more general ways to preserve prior information beyond KL-divergence, such as optimal transport distances and Renyi divergences. To this end, we introduce Flow Density Control (FDC), a simple algorithm that reduces this complex problem to a specific sequence of simpler fine-tuning tasks, each solvable via scalable established methods. We derive convergence guarantees for the proposed scheme under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we validate our method on illustrative settings, text-to-image, and molecular design tasks, showing that it can steer pre-trained generative models to optimize objectives and solve practically relevant tasks beyond the reach of current fine-tuning schemes.

</details>


### [79] [Spatially Aware Dictionary-Free Eigenfunction Identification for Modeling and Control of Nonlinear Dynamical Systems](https://arxiv.org/abs/2511.22648)
*David Grasev*

Main category: cs.LG

TL;DR: 提出一种数据驱动的 Koopman 特征函数发现方法，无需预设基函数，通过参考轨迹识别模态并重构为以特征值和时间为基础的新基；利用正则化最小二乘投影初始状态、全局优化特征值、以及梯度信息，从而提升预测精度并在稀疏采样下揭示不变量分区。


<details>
  <summary>Details</summary>
Motivation: 在不依赖预定义基函数的前提下高效、稳健地发现 Koopman 本征函数，并提高预测准确性，且能够在稀疏状态采样下揭示状态空间的几何结构与分区。

Method: 基于参考轨迹识别 Koopman 模态振幅，将分解转换为含本征值与时间的新基；对初始状态利用带正则化的最小二乘投影得到本征函数初值；使用全局优化器求取本征值；将初始值映射到本征函数值以获取空间结构并推导梯度，进而对 Koopman 偏微分方程的偏离进行惩罚以提升鲁棒性。

Result: 在多种非线性系统（FitzHugh–Nagumo（含输入）、Van der Pol、Duffing 振荡器，以及带控制的 2-spool 涡轮喷气发动机）上取得验证；方法能在稀疏采样条件下发现 Ko oj man 谱分量，揭示如不变量分区等几何特征，且梯度信息可用于输入动力建模与控制设计，显著提升预测精度。

Conclusion: 将主本征值与空间结构完整性纳入考虑可显著提高 Koopman 预测的准确性与鲁棒性；方法有效发现谱分量并揭示状态空间几何特征，适用于多种动力系统，梯度信息有助于控制设计。

Abstract: A new approach to data-driven discovery of Koopman eigenfunctions without a pre-defined set of basis functions is proposed. The approach is based on a reference trajectory, for which the Koopman mode amplitudes are first identified, and the Koopman mode decomposition is transformed to a new basis, which contains fundamental functions of eigenvalues and time. The initial values of the eigenfunctions are obtained by projecting trajectories onto this basis via a regularized least-squares fit. A global optimizer was employed to optimize the eigenvalues. Mapping initial-state values to eigenfunction values reveals their spatial structure, enabling the numerical computation of their gradients. Thus, deviations from the Koopman partial differential equation are penalized, leading to more robust solutions. The approach was successfully tested on several benchmark nonlinear dynamical systems, including the FitzHugh-Nagumo system with inputs, van der Pol and Duffing oscillators, and a 2-spool turbojet engine with control. The study demonstrates that incorporating principal eigenvalues and spatial structure integrity promotion significantly improves the accuracy of Koopman predictors. The approach effectively discovers Koopman spectral components even with sparse state-space sampling and reveals geometric features of the state space, such as invariant partitions. Finally, the numerical approximation of the eigenfunction gradient can be used for input dynamics modeling and control design. The results support the practicality of the approach for use with various dynamical systems.

</details>


### [80] [Structure-aware Hybrid-order Similarity Learning for Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.22656)
*Lin Xu,Ke Li,Dongjie Wang,Fengmao Lv,Tianrui Li,Yanyong Huang*

Main category: cs.LG

TL;DR: SHINE-FS提出一种结构感知的混合阶（第一阶与第二阶）相似性学习框架，用于多视图无监督特征选择。通过跨视图的一致性锚点与锚点图来获得低维表征并重构多视图数据，同时基于锚点-样本关系学习二阶相似性，最终将第一阶与第二阶图联合成混合阶相似图以捕捉局部和全局结构，从而提升特征选择性能。


<details>
  <summary>Details</summary>
Motivation: 现有MUFS方法多利用第一阶相似性，且依赖预定义的二阶相似图，易对噪声和离群值敏感，难以捕捉全局结构，因此需要一个能鲁棒地结合局部与全局结构的跨视图特征选择方法。

Method: 1) 学习跨视图的一致性锚点及对应的锚点图以捕获视图间关系；2) 基于锚点的一致信息获得样本的低维表示，并通过重构多视图数据来识别判别特征；3) 利用锚点-样本关系学习二阶相似性图；4) 联合学习第一阶和第二阶相似性图，形成混合阶相似图，兼顾局部与全局结构。

Result: 在真实多视图数据集上的广泛实验表明，SHINE-FS性能优于当前最先进的MUFS方法。

Conclusion: SHINE-FS通过锚点跨视图一致性与混合阶相似图有效地同时捕捉局部与全局数据结构，从而提升多视图无监督特征选择的效果。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently emerged as an effective dimensionality reduction method for unlabeled multi-view data. However, most existing methods mainly use first-order similarity graphs to preserve local structure, often overlooking the global structure that can be captured by second-order similarity. In addition, a few MUFS methods leverage predefined second-order similarity graphs, making them vulnerable to noise and outliers and resulting in suboptimal feature selection performance. In this paper, we propose a novel MUFS method, termed Structure-aware Hybrid-order sImilarity learNing for multi-viEw unsupervised Feature Selection (SHINE-FS), to address the aforementioned problem. SHINE-FS first learns consensus anchors and the corresponding anchor graph to capture the cross-view relationships between the anchors and the samples. Based on the acquired cross-view consensus information, it generates low-dimensional representations of the samples, which facilitate the reconstruction of multi-view data by identifying discriminative features. Subsequently, it employs the anchor-sample relationships to learn a second-order similarity graph. Furthermore, by jointly learning first-order and second-order similarity graphs, SHINE-FS constructs a hybrid-order similarity graph that captures both local and global structures, thereby revealing the intrinsic data structure to enhance feature selection. Comprehensive experimental results on real multi-view datasets show that SHINE-FS outperforms the state-of-the-art methods.

</details>


### [81] [Difficulties with Evaluating a Deception Detector for AIs](https://arxiv.org/abs/2511.22662)
*Lewis Smith,Bilal Chughtai,Neel Nanda*

Main category: cs.LG

TL;DR: 本研究主张要评估AI系统的欺骗检测器需可靠的“欺骗/诚实”标注样本，但当前缺乏这类数据，且存在多项收集难题；通过概念论证、对现有经验性工作分析及新颖的案例研究来揭示问题，并评估可能的权宜之计，认为这些权宜之计不足以单独解决问题，进展需进一步思考。


<details>
  <summary>Details</summary>
Motivation: 旨在降低高级AI系统的风险，通过构建可信的欺骗检测工具来识别策略性欺骗，但要可靠评估此类检测器，必须有可以被确认为欺骗或诚实的标注样本，当前此类样本稀缺且难以获取。

Method: 通过概念性论证、对现有经验性工作的分析、以及对新颖案例研究的分析来揭示数据获取与评估中的障碍，并评估提出的若干经验性解决方案的可行性与局限性。

Result: 提供证据表明缺乏明确的标注样本及数据获取障碍是评估欺骗检测器的核心难题；对潜在的权宜之计进行评估，认为尽管有价值，但单独不足以解决问题；需要在这些问题上进一步深入研究。

Conclusion: 要在欺骗检测领域取得实质进展，必须更多地考虑并解决标注数据缺乏、样本获取难度以及评估框架设计等根本性问题。

Abstract: Building reliable deception detectors for AI systems -- methods that could predict when an AI system is being strategically deceptive without necessarily requiring behavioural evidence -- would be valuable in mitigating risks from advanced AI systems. But evaluating the reliability and efficacy of a proposed deception detector requires examples that we can confidently label as either deceptive or honest. We argue that we currently lack the necessary examples and further identify several concrete obstacles in collecting them. We provide evidence from conceptual arguments, analysis of existing empirical works, and analysis of novel illustrative case studies. We also discuss the potential of several proposed empirical workarounds to these problems and argue that while they seem valuable, they also seem insufficient alone. Progress on deception detection likely requires further consideration of these problems.

</details>


### [82] [Modèles de Fondation et Ajustement : Vers une Nouvelle Génération de Modèles pour la Prévision des Séries Temporelles](https://arxiv.org/abs/2511.22674)
*Morad Laglil,Emilie Devijver,Eric Gaussier,Bertrand Pracca*

Main category: cs.LG

TL;DR: Foundation models pretrained on large time series enable zero-shot forecasting on unseen datasets; fine-tuning after pretraining improves performance, especially for long horizons.


<details>
  <summary>Details</summary>
Motivation: Extend zero-shot time series forecasting to unseen datasets by leveraging large-scale foundation models, reducing reliance on task-specific architectures and manual tuning.

Method: Review main architectures, pretraining strategies, and optimization methods for foundation models in time series; conduct empirical study on the impact of fine-tuning after pretraining on zero-shot performance.

Result: Fine-tuning after pretraining generally improves zero-shot forecasting ability, with larger gains for long-term horizons.

Conclusion: Foundation models with post-pretraining fine-tuning provide effective zero-shot and long-horizon time series forecasting, suggesting a scalable path beyond bespoke architectures.

Abstract: Inspired by recent advances in large language models, foundation models have been developed for zero-shot time series forecasting, enabling prediction on datasets unseen during pretraining. These large-scale models, trained on vast collections of time series, learn generalizable representations for both point and probabilistic forecasting, reducing the need for task-specific architectures and manual tuning.
  In this work, we review the main architectures, pretraining strategies, and optimization methods used in such models, and study the effect of fine-tuning after pretraining to enhance their performance on specific datasets. Our empirical results show that fine-tuning generally improves zero-shot forecasting capabilities, especially for long-term horizons.

</details>


### [83] [Test-time scaling of diffusions with flow maps](https://arxiv.org/abs/2511.22688)
*Amirmojtaba Sabour,Michael S. Albergo,Carles Domingo-Enrich,Nicholas M. Boffi,Sanja Fidler,Karsten Kreis,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出 Flow Map Trajectory Tilting (FMTT)，通过直接操作流映射来在扩散模型中实现奖励导向的上升，相比传统的梯度引导更稳定、有效，且可用于精确采样或寻找局部最优，适用于复杂奖励并能与视觉-语言模型等进行交互的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 在测试时通过奖励引导扩散样本往往不稳定且 ill-posed，因为奖励通常只在最终数据分布定义。现有做法多依赖去噪估计最终结果，导致偏差与不稳定性；需要一种直接且理论上有保证的方法来利用奖励信息。

Method: 基于流映射与瞬时输运的速度场之间的关系，构造 Flow Map Trajectory Tilting (FMTT) 算法。通过对流映射的扭曲来实现对奖励的上升，提供两种实现路径：1) 基于重要性权重的精确采样；2) 基于奖励的 principled search 以发现局部最优区域。理论上证明该方法在上升奖励方面优于直接使用奖励梯度的测试时方法。

Result: 相较于其它前瞻性方法，FMTT 在理论上更高效地提升奖励；在复杂奖励场景下也能实现新的图像编辑能力，且可通过与 vision-language 模型的接口扩展应用。

Conclusion: 通过直接利用流映射与速度场的联系，FMTT 提供了一种更稳健且可解释的奖励导向扩散方法，既可用于精确采样又能进行有效的局部搜索，增强了扩散模型在任务特定奖励下的可控性与灵活性。

Abstract: A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.

</details>


### [84] [Generative Anchored Fields: Controlled Data Generation via Emergent Velocity Fields and Transport Algebra](https://arxiv.org/abs/2511.22693)
*Deressa Wodajo Deressa,Hannes Mareen,Peter Lambert,Glenn Van Wallendael*

Main category: cs.LG

TL;DR: 提出 Generative Anchored Fields (GAF)——一种分解端点预测器（噪声 J 与数据 K）的生成模型，端点之间的时间对齐差异产生速度场 v=K−J，从而实现 Transport Algebra 的可组合控制。通过类别特异的 K_n 头，GAF 能在共享基分布和多模态之间实现定向传输映射，支持可控插值、混合生成和语义形态学运算（向量算术）。在 CelebA-HQ 64×64 上达到 FID 7.5，并提供作为架构原语的可组合生成；还证明具备无损循环传输（LPIPS=0.0）。代码开源在 GitHub。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中的可组合性和多模态传输问题，提出将端点预测（噪声与数据）分离，并通过端点对的时序不一致性自发产生速度场，以便进行带有结构化算子的向量算术和跨模态传输控制。

Method: 将独立的端点预测器 J 和 K 学习为可控的头（heads），速度场由 v=K−J 的时序不一致性产生；引入 Transport Algebra，对 (J_n, K_n) 的若干头进行代数运算以实现组合控制；通过类别特异的 K_n 头实现从共享基分布到多模态的定向传输映射，支持可控插值、混合生成和语义形态学（向量算术）等。

Result: 在 CelebA-HQ 64×64 上实现较高的样本质量，FID 7.5；实现无损循环传输，LPIPS = 0.0；给出代码实现。

Conclusion: GAF 将可组合生成作为架构原语引入，提供灵活的跨模态传输、插值与语义变换能力，并具备无损循环传输的性质，呈现出强的理论与应用潜力。

Abstract: We present Generative Anchored Fields (GAF), a generative model that learns independent endpoint predictors $J$ (noise) and $K$ (data) rather than a trajectory predictor. The velocity field $v=K-J$ emerges from their time-conditioned disagreement. This factorization enables \textit{Transport Algebra}: algebraic operation on learned $\{(J_n,K_n)\}_{n=1}^N$ heads for compositional control. With class-specific $K_n$ heads, GAF supports a rich family of directed transport maps between a shared base distribution and multiple modalities, enabling controllable interpolation, hybrid generation, and semantic morphing through vector arithmetic. We achieve strong sample quality (FID 7.5 on CelebA-HQ $64\times 64$) while uniquely providing compositional generation as an architectural primitive. We further demonstrate, GAF has lossless cyclic transport between its initial and final state with LPIPS=$0.0$. Code available at https://github.com/IDLabMedia/GAF

</details>


### [85] [Integrated Transcriptomic-proteomic Biomarker Identification for Radiation Response Prediction in Non-small Cell Lung Cancer Cell Lines](https://arxiv.org/abs/2511.22735)
*Yajun Yu,Guoping Xu,Steve Jiang,Robert Timmerman,John Minna,Yuanyuan Zhang,Hao Peng*

Main category: cs.LG

TL;DR: 联合转录组-蛋白质组框架用于预测NSCLC中SF2的放射响应，集成RNA-seq与DIA-MS数据提升预测能力，发现20个并行生物标志物，并显示单组学模型跨组学泛化能力有限，而组合模型在两组数据集上表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 在NSCLC细胞系中寻找预测放射反应的并发生物标志物，通过整合转录组和蛋白组来捕捉转录调控与蛋白活性之间的关系，提升SF2预测准确性和生物学解释力。

Method: 收集73个NSCLC细胞系的RNA-seq和46个细胞系的DIA-MS蛋白质组数据，筛选出1605个共享基因；采用Lasso回归进行基因特征选择（基于频次排序、5折交叉验证重复10次），构建SVR模型，比较转录组、蛋白组和联合集的预测性能；通过相关性分析评估RNA与蛋白表达的一致性，以及与SF2的关系，最终通过独立管线得到20个优先基因签名。

Result: 单组学模型对跨组学泛化能力有限；联合模型在两个数据集上表现更平衡：转录组R2=0.461、RMSE=0.120；蛋白组R2=0.604、RMSE=0.111；RNA-蛋白表达显著正相关（中位Pearson r=0.363）。

Conclusion: 首次建立NSCLC的转录组-蛋白组框架用于SF2预测，显示转录调控与蛋白功能的互补信息，所识别的并发生物标志物具有潜在的机制性见解与转化潜力。

Abstract: To develop an integrated transcriptome-proteome framework for identifying concurrent biomarkers predictive of radiation response, as measured by survival fraction at 2 Gy (SF2), in non-small cell lung cancer (NSCLC) cell lines. RNA sequencing (RNA-seq) and data-independent acquisition mass spectrometry (DIA-MS) proteomic data were collected from 73 and 46 NSCLC cell lines, respectively. Following preprocessing, 1,605 shared genes were retained for analysis. Feature selection was performed using least absolute shrinkage and selection operator (Lasso) regression with a frequency-based ranking criterion under five-fold cross-validation repeated ten times. Support vector regression (SVR) models were constructed using transcriptome-only, proteome-only, and combined transcriptome-proteome feature sets. Model performance was assessed by the coefficient of determination (R2) and root mean square error (RMSE). Correlation analyses evaluated concordance between RNA and protein expression and the relationships of selected biomarkers with SF2. RNA-protein expression exhibited significant positive correlations (median Pearson's r = 0.363). Independent pipelines identified 20 prioritized gene signatures from transcriptomic, proteomic, and combined datasets. Models trained on single-omic features achieved limited cross-omic generalizability, while the combined model demonstrated balanced predictive accuracy in both datasets (R2=0.461, RMSE=0.120 for transcriptome; R2=0.604, RMSE=0.111 for proteome). This study presents the first proteotranscriptomic framework for SF2 prediction in NSCLC, highlighting the complementary value of integrating transcriptomic and proteomic data. The identified concurrent biomarkers capture both transcriptional regulation and functional protein activity, offering mechanistic insights and translational potential.

</details>


### [86] [VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization](https://arxiv.org/abs/2511.22749)
*Zeng Wang,Weihua Xiao,Minghao Shao,Raghu Vamshi Hemadri,Ozgur Sinanoglu,Muhammad Shafique,Ramesh Karri*

Main category: cs.LG

TL;DR: VeriDispatcher 通过对任务描述进行语义嵌入和易难度预测，将 RTL 任务分派给合适的多模型，从而在保持质量的同时降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决单模型提示/微调的局限性，通过多模型协同提高 RTL 生成质量并降低成本。

Method: 为每个模型训练一个紧凑的分类器，基于任务描述的语义嵌入和来自语法、结构相似性、功能正确性的基准变体的易难度分数，在推理阶段把任务路由到选定子集的 LLM。

Result: 在 10 个不同 LLM 的 RTLLM 与 VerilogEval 基准上，VeriDispatcher 在 RTLLM 上以仅使用 40% 的商业调用实现最高 18% 的准确率提升，在 VerilogEval 上保持准确度的同时减少 25% 的商业使用。

Conclusion: 通过多模型协调与任务难度预测，可以在硬件设计自动化领域实现成本效益高且高质量的 RTL 生成。

Abstract: Large Language Models (LLMs) show strong performance in RTL generation, but different models excel on different tasks because of architecture and training differences. Prior work mainly prompts or finetunes a single model. What remains not well studied is how to coordinate multiple different LLMs so they jointly improve RTL quality while also reducing cost, instead of running all models and choosing the best output. We define this as the multi-LLM RTL generation problem. We propose VeriDispatcher, a multi-LLM RTL generation framework that dispatches each RTL task to suitable LLMs based on pre-inference difficulty prediction. For each model, we train a compact classifier over semantic embeddings of task descriptions, using difficulty scores derived from benchmark variants that combine syntax, structural similarity, and functional correctness. At inference, VeriDispatcher uses these predictors to route tasks to a selected subset of LLMs. Across 10 diverse LLMs on RTLLM and VerilogEval, VeriDispatcher achieves up to 18% accuracy improvement on RTLLM using only 40% of commercial calls, and on VerilogEval maintains accuracy while reducing commercial usage by 25%, enabling cost-effective, high-quality LLM deployment in hardware design automation.

</details>


### [87] [GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels](https://arxiv.org/abs/2511.22793)
*Bhavya Sai Nukapotula,Rishabh Tripathi,Seth Pregler,Dileep Kalathil,Srinivas Shakkottai,Theodore S. Rappaport*

Main category: cs.LG

TL;DR: GSpaRC introduces Gaussian splatting to represent RF environments with a compact set of 3D Gaussian primitives, enabling sub-millisecond CSI reconstruction with accuracy close to state-of-the-art, vastly reducing inference and training time and pilot overhead.


<details>
  <summary>Details</summary>
Motivation: Cellular networks require CSI for adaptive beamforming, but acquiring CSI incurs substantial pilot overhead. Offline/reconstruction methods struggle with real-time latency (4–100 ms). A real-time, low-latency CSI reconstruction method is needed to reduce pilot overhead and enable scalable 5G+ systems.

Method: Model RF environments as a compact set of 3D Gaussian primitives. Each primitive is parameterized by a lightweight neural model augmented with physics-informed features like distance-based attenuation. Use an equirectangular projection on a hemispherical surface centered at the receiver to reflect omnidirectional antenna behavior. Implement a custom CUDA pipeline for fully parallelized directional sorting, splatting, and rendering across frequency and spatial dimensions.

Result: GSpaRC achieves CSI reconstruction fidelity comparable to recent state-of-the-art methods while reducing training and inference time by over an order of magnitude, and attaining sub-millisecond latency. Demonstrated on multiple RF datasets, the method trades modest GPU compute for greatly reduced pilot overhead.

Conclusion: GSpaRC provides a scalable, low-latency channel estimation solution suitable for deployment in 5G and future wireless systems, enabling real-time CSI reconstruction with reduced pilot overhead. The implementation is made available in public code.

Abstract: Channel state information (CSI) is essential for adaptive beamforming and maintaining robust links in wireless communication systems. However, acquiring CSI incurs significant overhead, consuming up to 25\% of spectrum resources in 5G networks due to frequent pilot transmissions at sub-millisecond intervals. Recent approaches aim to reduce this burden by reconstructing CSI from spatiotemporal RF measurements, such as signal strength and direction-of-arrival. While effective in offline settings, these methods often suffer from inference latencies in the 5--100~ms range, making them impractical for real-time systems. We present GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels, the first algorithm to break the 1 ms latency barrier while maintaining high accuracy. GSpaRC represents the RF environment using a compact set of 3D Gaussian primitives, each parameterized by a lightweight neural model augmented with physics-informed features such as distance-based attenuation. Unlike traditional vision-based splatting pipelines, GSpaRC is tailored for RF reception: it employs an equirectangular projection onto a hemispherical surface centered at the receiver to reflect omnidirectional antenna behavior. A custom CUDA pipeline enables fully parallelized directional sorting, splatting, and rendering across frequency and spatial dimensions. Evaluated on multiple RF datasets, GSpaRC achieves similar CSI reconstruction fidelity to recent state-of-the-art methods while reducing training and inference time by over an order of magnitude. By trading modest GPU computation for a substantial reduction in pilot overhead, GSpaRC enables scalable, low-latency channel estimation suitable for deployment in 5G and future wireless systems. The code is available here: \href{https://github.com/Nbhavyasai/GSpaRC-WirelessGaussianSplatting.git}{GSpaRC}.

</details>


### [88] [Can Synthetic Data Improve Symbolic Regression Extrapolation Performance?](https://arxiv.org/abs/2511.22794)
*Fitria Wulandari Ramlan,Colm O'Riordan,Gabriel Kronberger,James McDermott*

Main category: cs.LG

TL;DR: 通过 KDE 找到稀疏区域，并用教师模型在新点上预测来生成合成数据，然后训练学生模型；对 GP 的外推性能有显著提升，但效果依赖数据集和教师模型，GP_e 对 GP_p 的提升尤为明显。


<details>
  <summary>Details</summary>
Motivation: 解决模型在训练分布外的外推能力不足的问题，探讨通过合成数据提升外推性能的可行性，结合符号回归中的基于高斯过程的教师-学生框架与知识蒸馏思想。

Method: 先用核密度估计 KDE 识别输入空间中训练数据稀疏的区域；在这些区域由教师模型对新输入点给出预测，形成合成数据；然后在增广数据上训练学生模型（包括 NN、RF、GP）作为教师和学生的不同组合，评估六个基准数据集的外推与插值表现。

Result: 在外推区域，GP 模型在使用合成数据后往往表现改善，提升受数据集和教师模型影响；最显著的提升源自用 GP_e 生成的合成数据来训练 GP_p；插值区域变化较小；观察到区域性误差异质性，整体上提供了一个实用的提升外推性能的方法。

Conclusion: 合成数据作为提升外推能力的实用手段，尤其在 GP 框架下效果最明显，但效果受数据分布和教师模型影响，需对区域特征进行细致分析与选择合适的教师-学生组合。

Abstract: Many machine learning models perform well when making predictions within the training data range, but often struggle when required to extrapolate beyond it. Symbolic regression (SR) using genetic programming (GP) can generate flexible models but is prone to unreliable behaviour in extrapolation. This paper investigates whether adding synthetic data can help improve performance in such cases. We apply Kernel Density Estimation (KDE) to identify regions in the input space where the training data is sparse. Synthetic data is then generated in those regions using a knowledge distillation approach: a teacher model generates predictions on new input points, which are then used to train a student model. We evaluate this method across six benchmark datasets, using neural networks (NN), random forests (RF), and GP both as teacher models (to generate synthetic data) and as student models (trained on the augmented data). Results show that GP models can often improve when trained on synthetic data, especially in extrapolation areas. However, the improvement depends on the dataset and teacher model used. The most important improvements are observed when synthetic data from GPe is used to train GPp in extrapolation regions. Changes in interpolation areas show only slight changes. We also observe heterogeneous errors, where model performance varies across different regions of the input space. Overall, this approach offers a practical solution for better extrapolation. Note: An earlier version of this work appeared in the GECCO 2025 Workshop on Symbolic Regression. This arXiv version corrects several parts of the original submission.

</details>


### [89] [CausalProfiler: Generating Synthetic Benchmarks for Rigorous and Transparent Evaluation of Causal Machine Learning](https://arxiv.org/abs/2511.22842)
*Panayiotis Panayiotou,Audrey Poinsot,Alessandro Leite,Nicolas Chesneau,Marc Schoenauer,Özgür Şimşek*

Main category: cs.LG

TL;DR: CausalProfiler is a synthetic benchmark generator for Causal ML that randomly samples causal models, data, queries, and ground truths to enable robust evaluation across observation, intervention, and counterfactual reasoning with clear assumptions and coverage guarantees.


<details>
  <summary>Details</summary>
Motivation: Empirical evaluation in Causal ML is limited and existing benchmarks are small and brittle, hindering generalizable conclusions; there is a need for rigorous, transparent benchmarks that explore diverse causal scenarios.

Method: Proposes a random generator that samples causal models, data, queries, and ground truths from explicit design choices about the class of causal models, data, and queries; provides coverage guarantees across observation, intervention, and counterfactual reasoning.

Result: Demonstrates utility by evaluating several state-of-the-art Causal ML methods under diverse conditions and assumptions, both within and outside the identification regime, illustrating the analyses and insights enabled by the benchmark.

Conclusion: CausalProfiler enables rigorous, transparent benchmarking for Causal ML with coverage guarantees, expanding evaluation beyond hand-crafted datasets and facilitating broader insights into method behavior under varied causal conditions.

Abstract: Causal machine learning (Causal ML) aims to answer "what if" questions using machine learning algorithms, making it a promising tool for high-stakes decision-making. Yet, empirical evaluation practices in Causal ML remain limited. Existing benchmarks often rely on a handful of hand-crafted or semi-synthetic datasets, leading to brittle, non-generalizable conclusions. To bridge this gap, we introduce CausalProfiler, a synthetic benchmark generator for Causal ML methods. Based on a set of explicit design choices about the class of causal models, queries, and data considered, the CausalProfiler randomly samples causal models, data, queries, and ground truths constituting the synthetic causal benchmarks. In this way, Causal ML methods can be rigorously and transparently evaluated under a variety of conditions. This work offers the first random generator of synthetic causal benchmarks with coverage guarantees and transparent assumptions operating on the three levels of causal reasoning: observation, intervention, and counterfactual. We demonstrate its utility by evaluating several state-of-the-art methods under diverse conditions and assumptions, both in and out of the identification regime, illustrating the types of analyses and insights the CausalProfiler enables.

</details>


### [90] [TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE](https://arxiv.org/abs/2511.22853)
*Jiawen Wei,Lan Jiang,Pengbo Wei,Ziwen Ye,Teng Song,Chen Chen,Guangrui Ma*

Main category: cs.LG

TL;DR: 提出 TARFVAE 框架，结合 TARFLOW 的自回归流和 VAE 的生成能力，实现高效的一步长时间序列预测，在多数据集上优于最新的确定性与生成模型，同时保持较高的预测速度。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型多依赖循环结构或多步去噪，预测成本高且对长期预测缺乏充分比较，难以在实际场景中呈现优势。

Method: 在 VAE 框架中加入 TARFLOW，引导潜在变量学习，打破高斯后验假设，提升潜在空间的信息性；仅使用 TARFLOW 的前向过程，不进行逆向自回归，先验采样后通过 VAE 解码器直接生成整段预测，全部使用简单的多层感知机（MLP）模块。

Result: 在不同预测区间和基准数据集上，TARFVAE 与最先进的确定性和生成模型相比表现更优，同时保持高效的生成速度。

Conclusion: TARFVAE 为生成型时间序列预测提供高效且强大的解，适合需要快速且远期预测能力的应用场景。

Abstract: Time series data is ubiquitous, with forecasting applications spanning from finance to healthcare. Beyond popular deterministic methods, generative models are gaining attention due to advancements in areas like image synthesis and video generation, as well as their inherent ability to provide probabilistic predictions. However, existing generative approaches mostly involve recurrent generative operations or repeated denoising steps, making the prediction laborious, particularly for long-term forecasting. Most of them only conduct experiments for relatively short-term forecasting, with limited comparison to deterministic methods in long-term forecasting, leaving their practical advantages unclear. This paper presents TARFVAE, a novel generative framework that combines the Transformer-based autoregressive flow (TARFLOW) and variational autoencoder (VAE) for efficient one-step generative time series forecasting. Inspired by the rethinking that complex architectures for extracting time series representations might not be necessary, we add a flow module, TARFLOW, to VAE to promote spontaneous learning of latent variables that benefit predictions. TARFLOW enhances VAE's posterior estimation by breaking the Gaussian assumption, thereby enabling a more informative latent space. TARFVAE uses only the forward process of TARFLOW, avoiding autoregressive inverse operations and thus ensuring fast generation. During generation, it samples from the prior latent space and directly generates full-horizon forecasts via the VAE decoder. With simple MLP modules, TARFVAE achieves superior performance over state-of-the-art deterministic and generative models across different forecast horizons on benchmark datasets while maintaining efficient prediction speed, demonstrating its effectiveness as an efficient and powerful solution for generative time series forecasting.

</details>


### [91] [Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation](https://arxiv.org/abs/2511.22862)
*Jiacheng Li,Songhe Feng*

Main category: cs.LG

TL;DR: 提出 BriMPR 的两阶段多模态测试时自适应框架，通过提示微调实现单模态全局特征对齐并结合掩码-完整模态的伪标签与跨模态实例对比学习，显著提升多模态测试时自适应性能。


<details>
  <summary>Details</summary>
Motivation: 多模态测试时自适应面临单模态浅层特征分布偏移和跨模态高层语义错配耦合，现有 TTA 缺乏对这种耦合的处理能力。

Method: BriMPR 包含两阶段：阶段一利用提示调优对各模态的全局特征分布进行对齐，实现初步跨模态语义再对齐；阶段二对掩码和完整模态的组合进行可信伪标签分配，并引入跨模态实例级对比学习以强化模态间的信息交互与对齐。

Result: 在包含腐蚀性扰动和现实域移位的 MMTTA 基准上进行广泛实验，结果显示方法优越，并给出代码链接。

Conclusion: 通过分工-递进的再对齐策略，BriMPR 能有效缓解模态耦合问题，显著提升多模态测试时自适应性能，代码公开。

Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at [this URL](https://github.com/Luchicken/BriMPR).

</details>


### [92] [ARM-Explainer -- Explaining and improving graph neural network predictions for the maximum clique problem using node features and association rule mining](https://arxiv.org/abs/2511.22866)
*Bharat Sharman,Elkafi Hassini*

Main category: cs.LG

TL;DR: 提出了一种基于关联规则挖掘的后验模型层解释器ARM-Explainer，用于GNN在最大团问题上的预测，并通过特征增强提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在图基组合优化问题上的预测缺乏可解释性，需识别影响预测的关键特征与取值区间；并探索通过特征增强提升模型性能的途径。以最大团问题(MCP)及混合几何散射GNN(HGS GNN)为对象，评估可解释性与性能提升的潜力。

Method: 基于关联规则挖掘的后验解释框架ARM-Explainer，针对HGS GNN在MCP上的预测，发现八条最具解释力的关联规则，评估其提升力度（lift、confidence）；识别对预测最重要的节点特征及其取值区间；在TWITTER与BHOSLIB-DIMACS数据集上验证，并通过加入信息丰富的节点特征对GNN进行特征增强训练。

Result: 在测试集上，八条最具解释力的关联规则达到中位lift约2.42、置信度约0.49；能够识别影响预测的关键节点特征及其取值范围；通过添加这些特征，BHOSLIB-DIMACS大规模图上的中位最大团大小从29.5提升至36，提升约22%。

Conclusion: ARM-Explainer有效揭示了影响GNN预测的关键特征，并可用于提升MCP等图基COP任务的性能，表明基于关联规则的后验解释具有实际价值与推广潜力。

Abstract: Numerous graph neural network (GNN)-based algorithms have been proposed to solve graph-based combinatorial optimization problems (COPs), but methods to explain their predictions remain largely undeveloped. We introduce ARM-Explainer, a post-hoc, model-level explainer based on association rule mining, and demonstrate it on the predictions of the hybrid geometric scattering (HGS) GNN for the maximum clique problem (MCP), a canonical NP-hard graph-based COP. The eight most explanatory association rules discovered by ARM-Explainer achieve high median lift and confidence values of 2.42 and 0.49, respectively, on test instances from the TWITTER and BHOSLIB-DIMACS benchmark datasets. ARM-Explainer identifies the most important node features, together with their value ranges, that influence the GNN's predictions on these datasets. Furthermore, augmenting the GNN with informative node features substantially improves its performance on the MCP, increasing the median largest-found clique size by 22% (from 29.5 to 36) on large graphs from the BHOSLIB-DIMACS dataset.

</details>


### [93] [Covering-Space Normalizing Flows: Approximating Pushforwards on Lens Spaces](https://arxiv.org/abs/2511.22882)
*William Ghanem*

Main category: cs.LG

TL;DR: 通过 universal covering map 将 S^3 到 L(p;q) 的推前分布构造出来，利用流在 L(p;q) 上逼近，并在对称 S^3 分布下删除冗余；并给出 von Mises-Fisher 诱导目标密度和 Z12 对称 Boltzmann 分布（用于建模苯分子）的推前分布近似。


<details>
  <summary>Details</summary>
Motivation: 解决在 lens spaces 上的分布推前问题，利用覆盖映射与流的组合以简化和提升近似效率。

Method: 以 rho: S^3 → L(p;q) 为基础，构造推前分布；使用在 L(p;q) 上的流来近似；分析对称性冗余并删除；应用到 von Mises-Fisher 诱导目标密度以及用于苯分子的 Z12 对称 Boltzmann 分布。

Result: 成功得到推前分布的近似，展示出对称性简化带来的冗余删除；对两类密度（von Mises-Fisher 与 Z12 Boltzmann）给出近似效果。

Conclusion: 该框架为在 lens spaces 上通过流近似推前分布提供了一个高效的方法，特别是在对称场景下显著减少冗余，可用于方向统计与分子结构建模等。

Abstract: We construct pushforward distributions via the universal covering map rho: S^3 -> L(p;q) with the goal of approximating these distributions using flows on L(p;q). We highlight that our method deletes redundancies in the case of a symmetric S^3 distribution. Using our model, we approximate the pushforwards of von Mises-Fisher-induced target densities as well as that of a Z_12-symmetric Boltzmann distribution on S^3 constructed to model benzene.

</details>


### [94] [Modeling Chaotic Pedestrian Behavior Using Chaos Indicators and Supervised Learning](https://arxiv.org/abs/2511.22887)
*Md. Muhtashim Shahrier,Nazmul Haque,Md Asif Raihan,Md. Hadiuzzaman*

Main category: cs.LG

TL;DR: 以数据驱动的步行者行为混沌建模框架，基于轨迹数据提取的混沌度量，使用 PCA 整合为统一混沌分数，并以随机森林与 CatBoost 回归实现对日夜场景的预测，日夜模型均有较好表现，SHAP 指出关键特征。


<details>
  <summary>Details</summary>
Motivation: 提升对行人行为不可预测性的理解与量化，以便城市规划、交通工程和自动驾驶系统进行风险评估与干预。

Method: 通过计算近似熵和李雅普诺夫指数等混沌指标（对速度与方向变化分别计算），对指标进行 PCA，构建日夜场景的特征集；训练并比较 Random Forest 与 CatBoost 回归模型，使用 SHAP 进行特征解释。

Result: 日间最佳模型R^2=0.8319，夜间R^2=0.8574，CatBoost 优于其他模型；关键贡献特征包括距离行驶、移动时长与速度变异性；输出的混沌分数可用于识别高风险区、校准微观仿真与辅助自动驾驶风险评估。

Conclusion: 提出一个可操作的痕迹量化框架，可在真实场景中评估并预测步行行为的不稳定性，为规划、工程与自动驾驶系统提供可解释、实用的工具。

Abstract: As cities around the world aim to improve walkability and safety, understanding the irregular and unpredictable nature of pedestrian behavior has become increasingly important. This study introduces a data-driven framework for modeling chaotic pedestrian movement using empirically observed trajectory data and supervised learning. Videos were recorded during both daytime and nighttime conditions to capture pedestrian dynamics under varying ambient and traffic contexts. Pedestrian trajectories were extracted through computer vision techniques, and behavioral chaos was quantified using four chaos metrics: Approximate Entropy and Lyapunov Exponent, each computed for both velocity and direction change. A Principal Component Analysis (PCA) was then applied to consolidate these indicators into a unified chaos score. A comprehensive set of individual, group-level, and contextual traffic features was engineered and used to train Random Forest and CatBoost regression models. CatBoost models consistently achieved superior performance. The best daytime PCA-based CatBoost model reached an R^2 of 0.8319, while the nighttime PCA-based CatBoost model attained an R^2 of 0.8574. SHAP analysis highlighted that features such as distance travel, movement duration, and speed variability were robust contributors to chaotic behavior. The proposed framework enables practitioners to quantify and anticipate behavioral instability in real-world settings. Planners and engineers can use chaos scores to identify high-risk pedestrian zones, apprise infrastructure improvements, and calibrate realistic microsimulation models. The approach also supports adaptive risk assessment in automated vehicle systems by capturing short-term motion unpredictability grounded in observable, interpretable features.

</details>


### [95] [Adversarial Training for Process Reward Models](https://arxiv.org/abs/2511.22888)
*Gurusha Juneja,Deepak Nathani,William Yang Wang*

Main category: cs.LG

TL;DR: 通过对抗性训练的PRMs（APRM）提升推理模型的鲁棒性与泛化性，无需逐步标注，且在多项数学推理基准上提升显著。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于静态训练数据的PRMs在面对新错误时泛化性差、标注成本高的问题，降低对人工逐步注释的依赖。

Method: 引入 Generator G 与 PRM R 的对抗训练：G 学习产生推理错误以欺骗 R，R 同时学习检测错误，产生难度逐步提升的负样本，提升鲁棒性和泛化。

Result: 在多样化的数学推理基准上，APRM 相比最强 PRM 基线提升约 3.4 个百分比点；对分布外任务提升约 5.3pp。

Conclusion: 对抗训练框架可显著提升 PRM 的推理能力鲁棒性和泛化性，同时降低对手动逐步注释的依赖。

Abstract: Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.

</details>


### [96] [EnECG: Efficient Ensemble Learning for Electrocardiogram Multi-task Foundation Model](https://arxiv.org/abs/2511.22935)
*Yuhao Xu,Xiaoda Wang,Jiaying Lu,Sirui Ding,Defu Cao,Huaxiu Yao,Yan Liu,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: 提出 EnECG，一种基于专家混合的 ECG 多任务集成框架，结合多种专门化的基础模型，通过对新增输出层进行低秩适配（LoRA）并采用专家混合（MoE）权重学习，实现在降低再训练成本的同时提升多任务 ECG 解析性能。


<details>
  <summary>Details</summary>
Motivation: 现有 ECG 模型往往忽视多种心脏异常之间的相关性；大型基础模型往往未在 ECG 数据上预训练，全面再训练成本高，难以高效覆盖多任务场景。需要一个高效的多任务、可扩展的框架来充分利用不同模型的专业能力。

Method: 在每个基础模型上附加专门的输出层，仅对新增参数应用低秩适配（LoRA）；引入 Mixture of Experts（MoE）机制学习模型权重以融合各专家的预测。

Result: 实验结果表明，在保持基础模型强表征能力的同时，通过限制微调范围和使用 MoE 进行集成，可实现较低计算与内存开销的多任务 ECG 解析，并实现竞争性性能。代码公开。

Conclusion: EnECG 提供了一种高效且可扩展的 ECG 多任务分析路径，有望在临床场景中实现更实用的部署。

Abstract: Electrocardiogram (ECG) analysis plays a vital role in the early detection, monitoring, and management of various cardiovascular conditions. While existing models have achieved notable success in ECG interpretation, they fail to leverage the interrelated nature of various cardiac abnormalities. Conversely, developing a specific model capable of extracting all relevant features for multiple ECG tasks remains a significant challenge. Large-scale foundation models, though powerful, are not typically pretrained on ECG data, making full re-training or fine-tuning computationally expensive. To address these challenges, we propose EnECG(Mixture of Experts-based Ensemble Learning for ECG Multi-tasks), an ensemble-based framework that integrates multiple specialized foundation models, each excelling in different aspects of ECG interpretation. Instead of relying on a single model or single task, EnECG leverages the strengths of multiple specialized models to tackle a variety of ECG-based tasks. To mitigate the high computational cost of full re-training or fine-tuning, we introduce a lightweight adaptation strategy: attaching dedicated output layers to each foundation model and applying Low-Rank Adaptation (LoRA) only to these newly added parameters. We then adopt a Mixture of Experts (MoE) mechanism to learn ensemble weights, effectively combining the complementary expertise of individual models. Our experimental results demonstrate that by minimizing the scope of fine-tuning, EnECG can help reduce computational and memory costs while maintaining the strong representational power of foundation models. This framework not only enhances feature extraction and predictive performance but also ensures practical efficiency for real-world clinical applications. The code is available at https://github.com/yuhaoxu99/EnECG.git.

</details>


### [97] [Bandit Guided Submodular Curriculum for Adaptive Subset Selection](https://arxiv.org/abs/2511.22944)
*Prateek Chanda,Prayas Agrawal,Saral Sureka,Lokesh Reddy Polu,Atharv Kshirsagar,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 提出 ONLINESUBMOD，一种在线贪婪策略，将自适应子集选择建模为多臂赌博机，其中每臂代表一个子模函数，用以引导样本选择；在多种采样策略下实现无 regrets；在视觉与语言任务上优于传统课程学习和双层优化，展现更佳的准确性-效率权衡；并指出以验证驱动的奖励度量可为课程安排提供原理性指引。


<details>
  <summary>Details</summary>
Motivation: 课程学习中对难度的可靠定义难以把握；以子模函数来诱导难度具有潜力，但需一个可在线优化的框架来选择样本。将自适应子集选择形式化为多臂赌博机，并通过在线决策实现高效、无 regret 的样本筛选。

Method: 将自适应子集选择问题建模为多臂赌博机（MAB），每个臂对应一个子模函数来引导样本选择；提出 ONLINESUBMOD 在线贪婪策略，优化以效用驱动的奖励，并在多种采样模式下证明无 regret 性能。

Result: 在视觉与语言数据集上，ONLINESUBMOD 超越传统课程学习和双层优化方法，展现更优的准确性与效率平衡。

Conclusion: 验证驱动的奖励度量为课程安排提供了一种原理性且通用的引导方式，具有广泛的适用性。

Abstract: Traditional curriculum learning proceeds from easy to hard samples, yet defining a reliable notion of difficulty remains elusive. Prior work has used submodular functions to induce difficulty scores in curriculum learning. We reinterpret adaptive subset selection and formulate it as a multi-armed bandit problem, where each arm corresponds to a submodular function guiding sample selection. We introduce ONLINESUBMOD, a novel online greedy policy that optimizes a utility-driven reward and provably achieves no-regret performance under various sampling regimes. Empirically, ONLINESUBMOD outperforms both traditional curriculum learning and bi-level optimization approaches across vision and language datasets, showing superior accuracy-efficiency tradeoffs. More broadly, we show that validationdriven reward metrics offer a principled way to guide the curriculum schedule.

</details>


### [98] [Experts are all you need: A Composable Framework for Large Language Model Inference](https://arxiv.org/abs/2511.22955)
*Shrihari Sridharan,Sourjya Roy,Anand Raghunathan,Kaushik Roy*

Main category: cs.LG

TL;DR: Comp-LLM introduces a composable inference framework that enables cross-expert collaboration via a sub-query dependency graph, combining decomposition, parallel execution, and synthesis to improve accuracy while reducing model size and latency compared to monolithic and sequential approaches.


<details>
  <summary>Details</summary>
Motivation: To address the increasing computational burden of large LLMs and the limitations of MoEs and multi-agent systems. MoEs raise training coupling and lack multi-step reasoning; multi-agent systems suffer from latency due to plan-act-observe loops. A framework that decomposes problems, assigns sub-queries to experts, and orchestrates reasoning with dependencies can achieve scalable, efficient reasoning.

Method: Three components: (1) Sub-query Generator decomposes the input into sub-queries, assigns them to experts via embedding similarity, and constructs a dependency graph; (2) Query Executor processes graph nodes, exploiting parallelism based on dependencies and resource constraints; (3) Response Aggregator synthesizes intermediate expert responses into a coherent final answer.

Result: Empirical results show up to 11.01% accuracy improvement over monolithic LLMs of similar size, with model size reduced by 1.67x–3.56x and latency improved by 1.1x–1.7x relative to sequential sub-query processing; performance is competitive with the largest model in the family, with no significant degradation.

Conclusion: Comp-LLM demonstrates that explicit sub-query dependency graphs and cross-expert orchestration can enable efficient, scalable multi-expert reasoning, combining the benefits of MoEs (capacity) and multi-agent collaboration (modularity) while mitigating latency and training coupling issues.

Abstract: Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or "experts". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential "plan--act--observe" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.

</details>


### [99] [A Trainable Centrality Framework for Modern Data](https://arxiv.org/abs/2511.22959)
*Minh Duc Vu,Mingshuo Liu,Doudou Zhou*

Main category: cs.LG

TL;DR: 提出了一种通用神经中心性框架 FUSE，通过全球头和局部头的结合，在任意表示上估计中心性，并通过一个介于 0 到 1 的调和参数在全局距离比较和去噪分数匹配之间插值，作为深度的近似。


<details>
  <summary>Details</summary>
Motivation: 在高维和非欧几里得数据中，经典的深度概念成本高、易不稳定且难以扩展。需要一个对多模态表示友好、可扩展且鲁棒的中心性估计方法，以支持鲁棒估计、排序和异常点检测。

Method: 全球头：通过基于成对距离的比较进行训练，学习一个锚点无关的中心性分数；局部头：通过去噪分数匹配来近似平滑对数密度势。引入一个 0-1 的单一参数在这两者之间插值，从一个前向传播获得跨视角的深度样式中心性。适用于多种表示（图像、时间序列、文本等）且可高效实现。

Result: 在合成分布、真实图像、时间序列、文本数据以及标准异常检测基准上，FUSE 能恢复有意义的经典排序、揭示多尺度几何结构，并在强基线下具有竞争力，且实现简单高效。

Conclusion: FUSE 提供了一个灵活、高效的神经中心性框架，能够跨表示与数据模态工作，融合全局和局部中心性信号，产生鲁棒且可扩展的深度中心性估计。

Abstract: Measuring how central or typical a data point is underpins robust estimation, ranking, and outlier detection, but classical depth notions become expensive and unstable in high dimensions and are hard to extend beyond Euclidean data. We introduce Fused Unified centrality Score Estimation (FUSE), a neural centrality framework that operates on top of arbitrary representations. FUSE combines a global head, trained from pairwise distance-based comparisons to learn an anchor-free centrality score, with a local head, trained by denoising score matching to approximate a smoothed log-density potential. A single parameter between 0 and 1 interpolates between these calibrated signals, yielding depth-like centrality from different views via one forward pass. Across synthetic distributions, real images, time series, and text data, and standard outlier detection benchmarks, FUSE recovers meaningful classical ordering, reveals multi-scale geometric structures, and attains competitive performance with strong classical baselines while remaining simple and efficient.

</details>


### [100] [Masked Diffusion for Generative Recommendation](https://arxiv.org/abs/2511.23021)
*Kulin Shah,Bhuvesh Kumar,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 用掩码扩散替代自回归建模以生成语义ID序列，允许并行解码；在多项实验中优于自回归基线，尤其在数据受限场景和粗粒度召回方面；还能在推理阶段并行预测多个SID。


<details>
  <summary>Details</summary>
Motivation: 解决自回归GR与SID在推理成本、数据利用效率和学习长距离依赖方面的局限。自回归需逐步解码、串行推理，且易偏向短上下文关系，影响数据效率与召回性能。借鉴NLP的扩散模型思想，尝试使用离散掩码噪声来学习序列分布，并允许对掩码部分并行解码。

Method: 提出离散掩码扩散框架：对SID序列施加离散掩码噪声，使被掩码的tokens在给定未掩码tokens下的概率独立条件化，训练目标是学习掩码序列的分布；推理阶段可并行地预测被掩码的SID，从而实现多SID的并行预测；可在数据有限场景下提高样本利用率，并可灵活地在推理时同时预测多个SID。

Result: 在严格的实验中，所提方法持续优于自回归建模，数据受限时差异尤为显著；在粗粒度召回等指标上也表现更好；并且在推理阶段支持并行预测多个SID，保持了优于自回归的性能。

Conclusion: masked diffusion 为GR with SIDs提供了一种高效替代，提升推理速度、数据利用率和灵活性，尤其适用于需要并行预测多个语义ID的场景；未来工作可探索对模型容量、掩码策略和评估指标的更细致分析。

Abstract: Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.

</details>


### [101] [Delta-XAI: A Unified Framework for Explaining Prediction Changes in Online Time Series Monitoring](https://arxiv.org/abs/2511.23036)
*Changhun Kim,Yechan Mun,Hyeongwon Jang,Eunseo Lee,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: Delta-XAI 为在线时间序列提供了一个将 14 种现有 XAI 方法接入的通用包装器，并提出了 SWING（Shifted Window Integrated Gradients），在整合路径中引入历史观测以捕捉时间依赖性，从而改进在线设置下的解释。


<details>
  <summary>Details</summary>
Motivation: 解释在线时间序列模型在医疗、金融等敏感领域尤为关键，因为预测的时序与上下文动态决定了决策。现有 XAI 往往逐步分析，忽略了时间相关性，导致难以解释预测变化并且评估困难。

Method: 通过一个包装函数将 14 种 XAI 方法适配到在线时序场景，并提出一个面向在线设置的评估体系，评估忠实性、充分性和连贯性等多维度；在此基础上提出 SWING，将 past 观测并入积分路径，系统性地捕捉时间依赖性并缓解分布外效应。

Result: 实验表明，在时间分析的适配下，经典梯度方法如 Integrated Gradients 有时优于新近方法；SWING 在多种设置和多种指标上表现稳健，且 Delta-XAI 的代码公开提供。

Conclusion: 证明了面向在线时间序列的解释方法是可行的，SWING 为利用历史上下文提供了一种系统化途径，并提供了对在线解释的全面评估框架。

Abstract: Explaining online time series monitoring models is crucial across sensitive domains such as healthcare and finance, where temporal and contextual prediction dynamics underpin critical decisions. While recent XAI methods have improved the explainability of time series models, they mostly analyze each time step independently, overlooking temporal dependencies. This results in further challenges: explaining prediction changes is non-trivial, methods fail to leverage online dynamics, and evaluation remains difficult. To address these challenges, we propose Delta-XAI, which adapts 14 existing XAI methods through a wrapper function and introduces a principled evaluation suite for the online setting, assessing diverse aspects, such as faithfulness, sufficiency, and coherence. Experiments reveal that classical gradient-based methods, such as Integrated Gradients (IG), can outperform recent approaches when adapted for temporal analysis. Building on this, we propose Shifted Window Integrated Gradients (SWING), which incorporates past observations in the integration path to systematically capture temporal dependencies and mitigate out-of-distribution effects. Extensive experiments consistently demonstrate the effectiveness of SWING across diverse settings with respect to diverse metrics. Our code is publicly available at https://anonymous.4open.science/r/Delta-XAI.

</details>


### [102] [Freeze, Diffuse, Decode: Geometry-Aware Adaptation of Pretrained Transformer Embeddings for Antimicrobial Peptide Design](https://arxiv.org/abs/2511.23120)
*Pankhil Gawade,Adam Izdebski,Myriam Lizotte,Kevin R. Moon,Jake S. Rhodes,Guy Wolf,Ewa Szczurek*

Main category: cs.LG

TL;DR: FDD: a diffusion-based framework to adapt frozen pretrained embeddings while preserving their geometric structure, enabling geometry-aware, interpretable representations for downstream tasks; demonstrated on antimicrobial peptide design.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning and probing of pretrained embeddings often distort their geometric structure or lack sufficient expressivity, especially with scarce supervised data. There is a need for transfer methods that preserve the intrinsic geometry of embeddings while enabling task-specific signals.

Method: Freeze, Diffuse, Decode (FDD) propagates supervised signals along the intrinsic manifold of frozen embeddings via a diffusion process, maintaining the geometric structure of the embedding space and producing task-aligned representations. It emphasizes geometry-aware adaptation and yields low-dimensional, interpretable representations.

Result: Applied to antimicrobial peptide design, FDD produces compact, predictive representations that support property prediction, retrieval, and latent-space interpolation, suggesting effective geometry-preserving transfer under data scarcity.

Conclusion: FDD offers a geometry-preserving transfer mechanism for pretrained embeddings that mitigates distortion from fine-tuning or probing and is especially beneficial when labeled data are limited; it enables interpretable, task-relevant representations and broad downstream utility.

Abstract: Pretrained transformers provide rich, general-purpose embeddings, which are transferred to downstream tasks. However, current transfer strategies: fine-tuning and probing, either distort the pretrained geometric structure of the embeddings or lack sufficient expressivity to capture task-relevant signals. These issues become even more pronounced when supervised data are scarce. Here, we introduce Freeze, Diffuse, Decode (FDD), a novel diffusion-based framework that adapts pre-trained embeddings to downstream tasks while preserving their underlying geometric structure. FDD propagates supervised signal along the intrinsic manifold of frozen embeddings, enabling a geometry-aware adaptation of the embedding space. Applied to antimicrobial peptide design, FDD yields low-dimensional, predictive, and interpretable representations that support property prediction, retrieval, and latent-space interpolation.

</details>


### [103] [Adapting Neural Audio Codecs to EEG](https://arxiv.org/abs/2511.23142)
*Ard Kastrati,Luca Lanzendörfer,Riccardo Rigoni,John Staib Matilla,Roger Wattenhofer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: EEG and audio are inherently distinct modalities, differing in sampling rate, channel structure, and scale. Yet, we show that pretrained neural audio codecs can serve as effective starting points for EEG compression, provided that the data are preprocessed to be suitable to the codec's input constraints. Using DAC, a state-of-the-art neural audio codec as our base, we demonstrate that raw EEG can be mapped into the codec's stride-based framing, enabling direct reuse of the audio-pretrained encoder-decoder. Even without modification, this setup yields stable EEG reconstructions, and fine-tuning on EEG data further improves fidelity and generalization compared to training from scratch. We systematically explore compression-quality trade-offs by varying residual codebook depth, codebook (vocabulary) size, and input sampling rate. To capture spatial dependencies across electrodes, we propose DAC-MC, a multi-channel extension with attention-based cross-channel aggregation and channel-specific decoding, while retaining the audio-pretrained initialization. Evaluations on the TUH Abnormal and Epilepsy datasets show that the adapted codecs preserve clinically relevant information, as reflected in spectrogram-based reconstruction loss and downstream classification accuracy.

</details>


### [104] [A Theoretical Framework for Discovering Groups and Unitary Representations via Tensor Factorization](https://arxiv.org/abs/2511.23152)
*Dongsung Huh,Halyun Jeong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We analyze the HyperCube model, an \textit{operator-valued} tensor factorization architecture that discovers group structures and their unitary representations. We provide a rigorous theoretical explanation for this inductive bias by decomposing its objective into a term regulating factor scales ($\mathcal{B}$) and a term enforcing directional alignment ($\mathcal{R} \geq 0$). This decomposition isolates the \textit{collinear manifold} ($\mathcal{R}=0$), to which numerical optimization consistently converges for group isotopes. We prove that this manifold admits feasible solutions exclusively for group isotopes, and that within it, $\mathcal{B}$ exerts a variational pressure toward unitarity. To bridge the gap to the global landscape, we formulate a \textit{Collinearity Dominance Conjecture}, supported by empirical observations. Conditional on this dominance, we prove two key results: (1) the global minimum is achieved by the unitary regular representation for groups, and (2) non-group operations incur a strictly higher objective value, formally quantifying the model's inductive bias toward the associative structure of groups (up to isotopy).

</details>


### [105] [Estimating the Event-Related Potential from Few EEG Trials](https://arxiv.org/abs/2511.23162)
*Anders Vestergaard Nørskov,Kasper Jørgensen,Alexander Neergaard Zahid,Morten Mørup*

Main category: cs.LG

TL;DR: 提出 EEG2ERP，通过不确定性自编码器将任意数量的 EEG 试验映射到 ERP，并引入自举训练目标与单独的方差解码器来建模不确定性；在少量试验下对三大公开数据集实现更优的 ERP 估计，支持零-shot 泛化，代码已开源。


<details>
  <summary>Details</summary>
Motivation: ERP 研究通常通过大量试验的平均来降低噪声与变异，但这需要大量数据且无法很好地刻画估计不确定性；因此需要在少量试验下也能可靠估计 ERP，并显式建模其不确定性。

Method: 提出不确定性自编码器 EEG2ERP，可将任意数量的 EEG 试验映射到对应的 ERP；通过自举（bootstrapping）训练目标来表征 ERP 的不确定性，并添加一个独立的方差解码器来建模 ERP 估计的不确定性；在零-shot 场景下对新受试者进行跨数据集评估，数据源包括 ERP CORE（>50,000 试验，40 位受试者，6 种范式）、P300 Speller 数据集，以及包含 EEG/MEG 的人脸感知数据集。

Result: 在少量试验情形下，所提方法的 ERP 估计显著优于常见的传统均值化与鲁棒平均等方法，且在跨数据集的泛化能力方面表现稳定。总体上，这是首个将深度学习直接映射 EEG 信号到 ERP 的方法，显著降低 ERP 研究所需的试验数量。

Conclusion: 该工作将 ERP 研究推向一个新的方向，即通过不确定性自编码器在少量试验下获得可靠的 ERP，同时对估计不确定性进行显式建模，且具有良好的跨数据集泛化能力；代码已在 GitHub 发布。

Abstract: Event-related potentials (ERP) are measurements of brain activity with wide applications in basic and clinical neuroscience, that are typically estimated using the average of many trials of electroencephalography signals (EEG) to sufficiently reduce noise and signal variability. We introduce EEG2ERP, a novel uncertainty-aware autoencoder approach that maps an arbitrary number of EEG trials to their associated ERP. To account for the ERP uncertainty we use bootstrapped training targets and introduce a separate variance decoder to model the uncertainty of the estimated ERP. We evaluate our approach in the challenging zero-shot scenario of generalizing to new subjects considering three different publicly available data sources; i) the comprehensive ERP CORE dataset that includes over 50,000 EEG trials across six ERP paradigms from 40 subjects, ii) the large P300 Speller BCI dataset, and iii) a neuroimaging dataset on face perception consisting of both EEG and magnetoencephalography (MEG) data. We consistently find that our method in the few trial regime provides substantially better ERP estimates than commonly used conventional and robust averaging procedures. EEG2ERP is the first deep learning approach to map EEG signals to their associated ERP, moving toward reducing the number of trials necessary for ERP research. Code is available at https://github.com/andersxa/EEG2ERP

</details>


### [106] [Energy-Efficient Vision Transformer Inference for Edge-AI Deployment](https://arxiv.org/abs/2511.23166)
*Nursultan Amanzhol,Jurn-Gyu Park*

Main category: cs.LG

TL;DR: 提出一个两阶段的ViT能效评估管线：先用NetScore进行设备无关筛选，再用可持续准确率（SAM）在具体设备上排序。基于ImageNet-1K和CIFAR-10在TX2和RTX 3050上进行基准测试，结果显示混合型LeViT_Conv_192等可显著降低边缘设备能源消耗（最高约53%），而TinyViT-11M_Distilled在移动GPU上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在能效成为ViT部署关键因素的背景下，单纯的准确率不足以评价模型在资源受限设备上的实际性能。

Method: 两阶段评估：阶段1使用NetScore进行设备无关的模型筛选；阶段2在具体设备上使用可持续准确率SAM对候选模型进行能效排序；基准13个ViT模型，在ImageNet-1K和CIFAR-10上测试，设备为TX2和RTX 3050。

Result: 混合模型LeViT_Conv_192在TX2相较ViT基线能耗降低最高53%；在CIFAR-10上SAM5=1.44（TX2）等；TinyViT-11M_Distilled在RTX 3050上表现突出：CIFAR-10 SAM5=1.72，ImageNet-1K SAM5=0.76。

Conclusion: 两阶段方法有效地在不同设备上识别能效更高的ViT模型，混合型和蒸馏型模型在不同场景各有优势，能显著提升边缘设备和移动GPU上的能效表现。

Abstract: The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).

</details>


### [107] [SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data](https://arxiv.org/abs/2511.23238)
*Yuting Fang,Qouc Le Gia,Flora Salim*

Main category: cs.LG

TL;DR: 引入 SDE-Attention 的 SDE-RNNs，通过通道级注意力增强潜在状态以处理不规则采样和大量缺失数据；不同的注意力机制在单变量/多变量时间序列上提升性能，尤其在高缺失率时表现显著。


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样和缺失观测下的时间序列预测难题，通过在潜在状态引入通道级注意力、时间变化特征注意力和金字塔多尺度自注意力来提升鲁棒性与准确性。

Method: 提出 SDE-Attention 家族，包含通道重新校准、时间变化特征注意力和金字塔多尺度自注意力等模块，并在合成周期数据和真实基准数据上进行对比，覆盖不同的缺失率。单变量使用 SDE-TVF-L（基于 LSTM 的时间可变特征模型），多变量场景则比较不同注意力类型的效果。

Result: 潜在空间的注意力优于基础 vanilla SDE-RNN；在单变量 UCR 数据集上，SDE-TVF-L 在平均准确率上分别在 30%、60%、90% 缺失时相对于基线提升约 4、6、10 个百分点（跨数据集平均）。在多变量 UEA 基准上，注意力增强模型也优于骨干模型，SDE-TVF-L 在高缺失时的平均准确率提升可达约 7%（在不同任务上不同注意力类型各有优势）。在单变量数据上，时间变化特征注意力最具鲁棒性；在多变量数据上，不同注意力类型对不同任务表现不同，表明 SDE-Attention 可根据问题结构灵活适配。

Conclusion: 将注意力机制融入 SDE-RNN 的潜在状态，可显著提升对不规则采样与缺失数据的鲁棒性及预测性能；时间变化特征注意力在单变量场景中最稳健，而在多变量场景中不同注意力类型的优势取决于具体任务，体现该方法的灵活性和适用性。

Abstract: Irregularly sampled time series with substantial missing observations are common in healthcare and sensor networks. We introduce SDE-Attention, a family of SDE-RNNs equipped with channel-level attention on the latent pre-RNN state, including channel recalibration, time-varying feature attention, and pyramidal multi-scale self-attention. We therefore conduct a comparison on a synthetic periodic dataset and real-world benchmarks, under varying missing rate. Latent-space attention consistently improves over a vanilla SDE-RNN. On the univariate UCR datasets, the LSTM-based time-varying feature model SDE-TVF-L achieves the highest average accuracy, raising mean performance by approximately 4, 6, and 10 percentage points over the baseline at 30%, 60% and 90% missingness, respectively (averaged across datasets). On multivariate UEA benchmarks, attention-augmented models again outperform the backbone, with SDE-TVF-L yielding up to a 7% gain in mean accuracy under high missingness. Among the proposed mechanisms, time-varying feature attention is the most robust on univariate datasets. On multivariate datasets, different attention types excel on different tasks, showing that SDE-Attention can be flexibly adapted to the structure of each problem.

</details>


### [108] [Towards Understanding Transformers in Learning Random Walks](https://arxiv.org/abs/2511.23239)
*Wei Shi,Yuan Cao*

Main category: cs.LG

TL;DR: 一层Transformer在对角随机游走（圆环上的随机游走）任务上经训练后可达到最优预测准确度，且模型可解释：softmax注意力起到直接父状态的选择器作用，值矩阵实现一步转移以预测下一个状态的位置；边界情况证明理论条件的紧致性；实验支持理论，并指出小初始化下梯度下降在简单任务中也可能失败。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer在理论层面的学习能力与可解释性，聚焦于经典统计模型中的随机游走问题，以揭示一层Transformer在时间序列建模中的潜在最优性与可解释机制。

Method: 理论分析：在圆环随机游走的设置下，证明经过梯度下降训练的一层Transformer可实现对随机游走的最优预测。解释性分析：softmax注意力充当父状态的选择器，值矩阵执行一步转移以预测下一个状态的位置。边界情形的构造用于展示理论条件的紧致性；实验部分用于验证理论结论。

Result: 给出理论证明：一层Transformer经训练后在圆环随机游走任务中达到最优准确率，且注意力机制可解释地选择直接父状态，值矩阵完成对下一个状态的单步转移。并通过边界案例展示条件的紧致性，同时通过实验支持理论结论。

Conclusion: 研究表明，Transformer在特定统计任务中具有可解释性且可实现最优性；梯度下降初始化对学习成功有影响，某些简单任务中小初始化可能导致收敛困难或失败，强调对训练策略的敏感性。

Abstract: Transformers have proven highly effective across various applications, especially in handling sequential data such as natural languages and time series. However, transformer models often lack clear interpretability, and the success of transformers has not been well understood in theory. In this paper, we study the capability and interpretability of transformers in learning a family of classic statistical models, namely random walks on circles. We theoretically demonstrate that, after training with gradient descent, a one-layer transformer model can achieve optimal accuracy in predicting random walks. Importantly, our analysis reveals that the trained model is interpretable: the trained softmax attention serves as a token selector, focusing on the direct parent state; subsequently, the value matrix executes a one-step probability transition to predict the location of the next state based on this parent state. We also show that certain edge cases not covered by our theory are indeed failure cases, demonstrating that our theoretical conditions are tight. By investigating these success and failure cases, it is revealed that gradient descent with small initialization may fail or struggle to converge to a good solution in certain simple tasks even beyond random walks. Experiments are conducted to support our theoretical findings.

</details>


### [109] [Time Series Forecasting via Direct Per-Step Probability Distribution Modeling](https://arxiv.org/abs/2511.23260)
*Linghao Kong,Xiaopeng Hong*

Main category: cs.LG

TL;DR: 提出 interPDN，通过直接构造每步的离散概率分布来进行时间序列预测，并通过互嵌分支结构与自监督一致性约束来建模不确定性，且引入粗粒度时间尺度分支以提升长期趋势预测；在多数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测常出现不确定性，但当前模型输出标量，难以反映预测不确定性。需直接建模预测分布并在多步任务中体现不确定性。

Method: 提出 interPDN，构造每一步的离散预测分布，回归输出为对给定支持集的分布期望；双分支架构交错的支持集，包含用于长长程趋势的粗粒度分支；来自另一分支的输出作为辅助信号，对当前分支预测施加自监督一致性约束。

Result: 在多個真实世界数据集上进行了广泛实验，展示了优越性能。

Conclusion:  interPDN 能有效建模预测不确定性，且通过交错支持集和自监督约束提升长期预测能力。

Abstract: Deep neural network-based time series prediction models have recently demonstrated superior capabilities in capturing complex temporal dependencies. However, it is challenging for these models to account for uncertainty associated with their predictions, because they directly output scalar values at each time step. To address such a challenge, we propose a novel model named interleaved dual-branch Probability Distribution Network (interPDN), which directly constructs discrete probability distributions per step instead of a scalar. The regression output at each time step is derived by computing the expectation of the predictive distribution on a predefined support set. To mitigate prediction anomalies, a dual-branch architecture is introduced with interleaved support sets, augmented by coarse temporal-scale branches for long-term trend forecasting. Outputs from another branch are treated as auxiliary signals to impose self-supervised consistency constraints on the current branch's prediction. Extensive experiments on multiple real-world datasets demonstrate the superior performance of interPDN.

</details>


### [110] [BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning](https://arxiv.org/abs/2511.23264)
*Ariful Islam,Md Rifat Hossen,Tanvir Mahmud*

Main category: cs.LG

TL;DR: BanglaSentNet 提出一种可解释的混合深度学习框架，结合 LSTM/ BiLSTM/ GRU 与 BanglaBERT，通过动态加权集成进行多方面情感分类，提供 SHAP 解释和注意力可视化，并在 8,755 条人工标注的孟加拉语电商评论数据集上达到 85% 的准确率和 0.88 的 F1，跨域零-shot 保持 67–76% 的效果，少样本 500–1000 条达到全量微调的 90–95%，并具备实际部署潜力。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语电商评论多方面情感分析中的数据稀缺、形态复杂、代码混合、领域偏移等挑战，提升可解释性和跨域泛化能力以支撑商业应用。

Method: 提出 BanglaSentNet，结合 LSTM、BiLSTM、GRU 与 BanglaBERT 的动态加权集成；引入 SHAP 进行特征归因和注意力可视化以提升可解释性；构建 8,755 条标注数据的四维度（质量、服务、价格、装饰）数据集；开展跨域迁移学习、少样本学习与实际部署评估；公开数据集与评估框架。

Result: 在四个方面实现 85% 准确率、0.88 F1；较独立模型提升 3–7%；解释性评分 9.4/10、87.6% 人类一致；零-shot 跨域 67–76% 的效果，少样本 500–1000 条达到全量微调的 90–95%；具备现实部署的商业应用潜力；设立新的孟加拉语情感分析基准，推动低资源语言的集成学习。

Conclusion: 工作提升了低资源语言情感分析的可解释性和跨域泛化，展示混合深度学习与集成方法在商业场景中的实用性，提供可复制的数据集与评估框架，具有学术与产业价值。

Abstract: Multi-aspect sentiment analysis of Bangla e-commerce reviews remains challenging due to limited annotated datasets, morphological complexity, code-mixing phenomena, and domain shift issues, affecting 300 million Bangla-speaking users. Existing approaches lack explainability and cross-domain generalization capabilities crucial for practical deployment. We present BanglaSentNet, an explainable hybrid deep learning framework integrating LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning for multi-aspect sentiment classification. We introduce a dataset of 8,755 manually annotated Bangla product reviews across four aspects (Quality, Service, Price, Decoration) from major Bangladeshi e-commerce platforms. Our framework incorporates SHAP-based feature attribution and attention visualization for transparent insights. BanglaSentNet achieves 85% accuracy and 0.88 F1-score, outperforming standalone deep learning models by 3-7% and traditional approaches substantially. The explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement. Cross-domain transfer learning experiments reveal robust generalization: zero-shot performance retains 67-76% effectiveness across diverse domains (BanglaBook reviews, social media, general e-commerce, news headlines); few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance, significantly reducing annotation costs. Real-world deployment demonstrates practical utility for Bangladeshi e-commerce platforms, enabling data-driven decision-making for pricing optimization, service improvement, and customer experience enhancement. This research establishes a new state-of-the-art benchmark for Bangla sentiment analysis, advances ensemble learning methodologies for low-resource languages, and provides actionable solutions for commercial applications.

</details>


### [111] [Beyond Curve Fitting: Neuro-Symbolic Agents for Context-Aware Epidemic Forecasting](https://arxiv.org/abs/2511.23276)
*Joongwon Chae,Runming Wang,Chen Xiong,Gong Yunhan,Lian Zhang,Ji Jiansong,Dongmei Yu,Peiwu Qin*

Main category: cs.LG

TL;DR: 提出一个两智能体框架：将LLM事件解释器用于处理学校日历、天气等信号，生成传输影响信号；神经符号核心与历史病例结合，输出校准的概率预测。对香港（2023-2024）和中国丽水（2024）HFMD数据集进行了评估，点 forecast 竞争力强，90% 预测区间覆盖率在0.85-1.00之间，且具有可解释的推理。代码见 GitHub。


<details>
  <summary>Details</summary>
Motivation: 现有的经典模型和基础模型在考虑协变量时往往缺乏对驱动因素之间因果关系的语义推理。需要一个将情境理解与概率预测解耦的框架，以产生对公共卫生流程友好的上下文感知预测。

Method: 提出两智能体结构：1) LLM事件解释器将学校时刻表、气象摘要、报告等异质信号转化为一个标量的传输影响信号；2) 神经符号核心将其与历史病例计数结合，输出经校准的概率预测。对真实HFMD数据集（香港2023-2024、丽水2024）进行评估，比较传统与基础模型基线。

Result: 相较于传统和基础模型基线，所提方法在点预测精度方面具有竞争力；同时提供稳健的90%预测区间（覆盖率0.85-1.00）和可解释的人类可读推理。结果显示通过将领域知识结构性地整合到LLM中，可以达到与最先进方法相当的性能，同时生成符合公共卫生工作流的上下文感知预测。

Conclusion: 将领域知识以结构化方式融入LLMs 的策略可在保持竞争力的同时提高预测的可解释性和情境适配性，并给出与公共卫生实际工作流程相符的推理。代码可在GitHub获取。

Abstract: Effective surveillance of hand, foot and mouth disease (HFMD) requires forecasts accounting for epidemiological patterns and contextual drivers like school calendars and weather. While classical models and recent foundation models (e.g., Chronos, TimesFM) incorporate covariates, they often lack the semantic reasoning to interpret the causal interplay between conflicting drivers. In this work, we propose a two-agent framework decoupling contextual interpretation from probabilistic forecasting. An LLM "event interpreter" processes heterogeneous signals-including school schedules, meteorological summaries, and reports-into a scalar transmission-impact signal. A neuro-symbolic core then combines this with historical case counts to produce calibrated probabilistic forecasts. We evaluate the framework on real-world HFMD datasets from Hong Kong (2023-2024) and Lishui, China (2024). Compared to traditional and foundation-model baselines, our approach achieves competitive point forecasting accuracy while providing robust 90% prediction intervals (coverage 0.85-1.00) and human-interpretable rationales. Our results suggest that structurally integrating domain knowledge through LLMs can match state-of-the-art performance while yielding context-aware forecasts that align with public health workflows. Code is available at https://github.com/jw-chae/forecast_MED .

</details>


### [112] [Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla](https://arxiv.org/abs/2511.23287)
*Ariful Islam,Tanvir Mahmud,Md Rifat Hossen*

Main category: cs.LG

TL;DR: An integrated multimodal framework BangACMM for Bangla author content analysis using intermediate fusion between textual (mBERT family) and visual (Swin, ViT etc.) modalities; achieves state-of-the-art 84.11% macro-F1 on the Uddessho dataset, outperforming unimodal and early/late fusion baselines.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of unimodal Bangla author intent classification by leveraging both textual and visual cues in social media posts, especially in a low-resource language context; demonstrate the value of intermediate cross-modal fusion.

Method: Systematic benchmarking of transformer-based language models (mBERT, DistilBERT, XLM-RoBERTa) and vision architectures (ViT, Swin, SwiftFormer, ResNet, DenseNet, MobileNet); propose an intermediate fusion strategy that combines modality-specific representations at intermediate layers; evaluate on the Uddessho dataset (3,048 posts across six intent categories); establish BangACMM as the framework name.

Result: Intermediate fusion with mBERT and Swin Transformer yielded 84.11% macro-F1, the new state-of-the-art, improving by 8.4 percentage points over prior Bangla multimodal methods; visual context significantly enhances intent classification; cross-modal features at intermediate levels strike a balance between modality-specific and cross-modal learning.

Conclusion: Proposes BangACMM for Bangla and other low-resource languages; demonstrates the effectiveness of intermediate fusion in multimodal author intent classification and sets new benchmarks and methodological standards in the domain.

Abstract: The expansion of the Internet and social networks has led to an explosion of user-generated content. Author intent understanding plays a crucial role in interpreting social media content. This paper addresses author intent classification in Bangla social media posts by leveraging both textual and visual data. Recognizing limitations in previous unimodal approaches, we systematically benchmark transformer-based language models (mBERT, DistilBERT, XLM-RoBERTa) and vision architectures (ViT, Swin, SwiftFormer, ResNet, DenseNet, MobileNet), utilizing the Uddessho dataset of 3,048 posts spanning six practical intent categories. We introduce a novel intermediate fusion strategy that significantly outperforms early and late fusion on this task. Experimental results show that intermediate fusion, particularly with mBERT and Swin Transformer, achieves 84.11% macro-F1 score, establishing a new state-of-the-art with an 8.4 percentage-point improvement over prior Bangla multimodal approaches. Our analysis demonstrates that integrating visual context substantially enhances intent classification. Cross-modal feature integration at intermediate levels provides optimal balance between modality-specific representation and cross-modal learning. This research establishes new benchmarks and methodological standards for Bangla and other low-resource languages. We call our proposed framework BangACMM (Bangla Author Content MultiModal).

</details>


### [113] [Hard-Constrained Neural Networks with Physics-Embedded Architecture for Residual Dynamics Learning and Invariant Enforcement in Cyber-Physical Systems](https://arxiv.org/abs/2511.23307)
*Enzo Nicolás Spotorno,Josafat Leal Filho,Antônio Augusto Fröhlich*

Main category: cs.LG

TL;DR: 提出 HRPINN 与 PHRPINN 框架，用硬约束嵌入递归积分器学习残差动力学，同时通过投影-预测机制严格维持代数不变量，适用于带未知动力和代数约束的复杂物理系统；理论分析且在电池 DAE 和基准测试中验证，强调准确性/数据效率与物理一致性成本的权衡。


<details>
  <summary>Details</summary>
Motivation: 在具有未知动力学和代数不变量的复杂物理系统中，如何在不破坏物理结构的前提下高效学习，以及如何严格维持代数约束以提高稳定性和可解释性。

Method: 提出 HRPINN：将已知物理作为硬结构约束嵌入递归积分器以学习残差动力学；提出 PHRPINN：引入预测-投影机制以严格执行代数不变量；给出理论分析其表示能力；通过在真实电池诊断的微分代数方程组(DAE)上验证 HRPINN，并在一组标准受限基准上评估 PHRPINN。

Result: 结果显示该框架可实现高精度和数据高效学习，同时揭示物理一致性、计算成本与数值稳定性之间的权衡，提供实际部署的实用指南。

Conclusion: HRPINN/PHRPINN 为在包含未知动力学和代数约束的复杂系统中进行物理信息学习提供了一个通用、可扩展的框架，具有良好的理论基础和应用潜力，但需在准确性、稳定性与计算成本之间做权衡。

Abstract: This paper presents a framework for physics-informed learning in complex cyber-physical systems governed by differential equations with both unknown dynamics and algebraic invariants. First, we formalize the Hybrid Recurrent Physics-Informed Neural Network (HRPINN), a general-purpose architecture that embeds known physics as a hard structural constraint within a recurrent integrator to learn only residual dynamics. Second, we introduce the Projected HRPINN (PHRPINN), a novel extension that integrates a predict-project mechanism to strictly enforce algebraic invariants by design. The framework is supported by a theoretical analysis of its representational capacity. We validate HRPINN on a real-world battery prognostics DAE and evaluate PHRPINN on a suite of standard constrained benchmarks. The results demonstrate the framework's potential for achieving high accuracy and data efficiency, while also highlighting critical trade-offs between physical consistency, computational cost, and numerical stability, providing practical guidance for its deployment.

</details>


### [114] [Learning-Augmented Online Bipartite Matching in the Random Arrival Order Model](https://arxiv.org/abs/2511.23388)
*Kunanon Burathep,Thomas Erlebach,William K. Moses*

Main category: cs.LG

TL;DR: 在随机到达序列的在线无权二部匹配问题中，给定在线顶点的预测邻域，提出在任意最优解规模下的学习增强算法。预测良时保持(1-o(1))的一致性，面对不良预测保持(β-o(1))的鲁棒性，且竞争比随预测误差平滑退化。


<details>
  <summary>Details</summary>
Motivation: 扩展Choo等人（ICML 2024）的学习增强框架，去除最优匹配大小等于n的假设，仅需预测的匹配规模至少为αn（0<α≤1）即可，研究在任意最优规模下的学习-增强竞争分析。

Method: 通过前缀样本对预测质量进行判断：若预测接近真实序列，则沿用预测；否则回退到忽略预测的β-竞争基线算法。进一步将分析从“最优规模为n”推广到任意最优规模，给出在预测规模≥αn下的(1-o(1))一致性和(β-o(1))鲁棒性，并给出误差与竞争比的平滑退化关系。

Result: 提出的学习增强算法在广义情形下实现近似的一致性和鲁棒性，且竞争比对预测误差呈平滑退化。

Conclusion: 工作扩展了学习增强型在线匹配的适用范围，降低对最优匹配大小的严格假设，揭示了预测质量与算法性能之间的可控权衡。

Abstract: We study the online unweighted bipartite matching problem in the random arrival order model, with $n$ offline and $n$ online vertices, in the learning-augmented setting: The algorithm is provided with untrusted predictions of the types (neighborhoods) of the online vertices. We build upon the work of Choo et al. (ICML 2024, pp. 8762-8781) who proposed an approach that uses a prefix of the arrival sequence as a sample to determine whether the predictions are close to the true arrival sequence and then either follows the predictions or uses a known baseline algorithm that ignores the predictions and is $β$-competitive. Their analysis is limited to the case that the optimal matching has size $n$, i.e., every online vertex can be matched. We generalize their approach and analysis by removing any assumptions on the size of the optimal matching while only requiring that the size of the predicted matching is at least $αn$ for any constant $0 < α\le 1$. Our learning-augmented algorithm achieves $(1-o(1))$-consistency and $(β-o(1))$-robustness. Additionally, we show that the competitive ratio degrades smoothly between consistency and robustness with increasing prediction error.

</details>


### [115] [Quantized-Tinyllava: a new multimodal foundation model enables efficient split learning](https://arxiv.org/abs/2511.23402)
*Jiajun Guo,Xin Luo,Jie Liu*

Main category: cs.LG

TL;DR: 提出了一种将数据压缩嵌入分布式模型训练的新型多模态结构，通过对嵌入进行低位整数编码来降低分区间传输成本，并基于熵编码理论确定离散表示级别的最优数量。


<details>
  <summary>Details</summary>
Motivation: 解决分割学习中的隐私保护和高通信成本问题，特别是在需要传输高维嵌入的情况下。

Method: 引入学习型数据压缩的多模态模型结构，将嵌入压缩为低比特整数，同时结合基于熵编码的理论来确定离散表示的级别数；评估传输成本与模型性能之间的折衷。

Result: 在尽量保持模型性能的前提下显著降低分区间传输成本，给出离散级别数的理论依据与实验验证。

Conclusion: 该方法为分割学习提供一种高效的数据传输方案，结合学习型压缩和熵编码的理论基础，能够在隐私保护前提下减小通信开销。

Abstract: Split learning is well known as a method for resolving data privacy concerns by training a model on distributed devices, thereby avoiding data sharing that raises privacy issues. However, high network communication costs are always an impediment to split learning, especially for large foundation models that require transmitting large amounts of high-dimensional data. To resolve this issue, we present a new multimodal model structure that incorporates a learning-based data compression method, which compresses model embeddings into low-bit integers while preserving the model's performance, greatly reducing the transmission costs between partitions. We then determine the optimal number of discrete representation levels based on a solid theoretical foundation from entropy coding.

</details>


### [116] [Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation](https://arxiv.org/abs/2511.23440)
*Bernhard Klein,Falk Selker,Hendrik Borras,Sophie Steger,Franz Pernkopf,Holger Fröning*

Main category: cs.LG

TL;DR: PFP-BNNs provide a highly efficient Bayesian neural network approximation that propagates Gaussian uncertainty analytically, achieving large speedups over SVI while maintaining accuracy and OOD performance, enabling deployment on resource-limited embedded devices via TVM and a Gaussian-operator library.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in ML limits deployment in safety-critical domains due to failure to detect OOD data and overconfident predictions. Bayesian neural networks offer probabilistic estimates but are computationally expensive. PFP provides a fast, analytic alternative by approximating weight/activation distributions with Gaussian assumptions and a single forward pass, motivating end-to-end deployment on embedded hardware.

Method: Develop an end-to-end pipeline to train, compile, optimize, and deploy PFP-based BNNs on ARM CPUs. Build a Gaussian-propagating operator library for MLPs and CNNs using TVM, and apply manual/auto-tuning. Conduct ablation studies to compare with SVI.

Result: PFP achieves up to 4200x speedups for small mini-batches over SVI and matches SVI-BNNs in accuracy, uncertainty estimation, and OOD detection on Dirty-MNIST, with substantially reduced compute cost. Demonstrates viability of combining Bayesian approximation with code generation for efficient BNN deployment on constrained devices.

Conclusion: Gaussian-propagating PFP combined with code-generation and TVM-based deployment enables practical, efficient BNNs on resource-constrained systems without sacrificing uncertainty quality, advancing safe AI in embedded domains.

Abstract: Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.

</details>


### [117] [ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts](https://arxiv.org/abs/2511.23442)
*Hang Yu,Di Zhang,Qiwei Du,Yanping Zhao,Hai Zhang,Guang Chen,Eduardo E. Veas,Junqiao Zhao*

Main category: cs.LG

TL;DR: ASTRO is a data augmentation framework for offline reinforcement learning that stitches trajectories to create distributionally novel and dynamics-consistent sequences. It uses a temporal-distance representation to find stitch targets and a dynamics-guided stitch planner with Rollout Deviation Feedback to generate feasible connecting actions, improving policy learning and outperforming prior augmentation methods on D4RL and OGBench.


<details>
  <summary>Details</summary>
Motivation: Offline RL suffers from suboptimal, fragmented trajectories that hinder reward propagation and value estimation. Existing trajectory augmentation methods often produce trajectories that stay within the behavior policy’s support or violate dynamics, limiting improvement potential. A method that yields novel yet dynamics-consistent augmented data is needed.

Method: ASTRO learns a temporal-distance representation to identify distinct and reachable stitch targets. It then employs a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback (the gap between the target state sequence and the actual arrived state sequence after predicting actions) to enhance feasibility and reachability of stitched trajectories. The augmented data improves policy learning for offline RL.

Result: ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gains on the challenging OGBench suite and showing consistent improvements on standard offline RL benchmarks such as D4RL.

Conclusion: By generating distributionally novel and dynamics-consistent stitched trajectories, ASTRO provides an effective data augmentation mechanism for offline RL, enabling better value estimation and policy improvement across diverse algorithms and benchmarks.

Abstract: Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.

</details>


### [118] [Physics-Informed Neural Networks for Thermophysical Property Retrieval](https://arxiv.org/abs/2511.23449)
*Ali Waseem,Malcolm Mielle*

Main category: cs.LG

TL;DR: 用PINN的迭代框架从原位热图中估计墙体热传导系数k；在固定k的前向问题上训练PINN，再据预测的温度场与观测对比优化k，迭代收敛。即便非稳态条件下，最大MAE约4.09，能在不同环境和采样时长下准确预测k，显示PINN在原位材料性质反问题上的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决在环境变量扰动下，非侵入式、对观测数据友好的墙体热传导系数k估计问题，弥补传统方法对环境敏感、需长时观测或侵入式测量的局限。

Method: 提出一个基于PINN的迭代框架：在固定k时用PINN求解前向热传导问题并拟合观测数据；随后通过比较PINN预测的表面温度与观测热图，优化k，重复该过程直至收敛。数据来自气象站环境信息与有限容积法（FVM）仿真。

Result: 在不同环境条件和数据采样时长下，能够准确预测k；若 dawn 时温度分布接近稳态，则MAE最大为4.0851（单位未给出），对非稳态偏离的鲁棒性良好；表明该框架在无长时观测和非侵入数据条件下具有潜在应用前景。

Conclusion: PINN为原位材料属性估计提供了一种可行路径，在现实条件下也具备可靠性，且可减少长时间的测量工作。为未来更多基于PINN的原位逆问题研究提供起点。

Abstract: Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.

</details>


### [119] [The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference](https://arxiv.org/abs/2511.23455)
*Hans Gundlach,Jayson Lynch,Matthias Mertens,Neil Thompson*

Main category: cs.LG

TL;DR: 本研究通过汇集当前与历史的价格数据，评估AI基准在单位成本上的进展，发现 frontier 模型的基准性能价格下降约5-10倍/年，原因包括经济因素、硬件效率和算法效率。通过控制开放模型和硬件价格下降，估计算法效率进展约3倍/年。建议评估者在评测中公开并考虑基准价格作为衡量AI真实社会影响的一个要素。


<details>
  <summary>Details</summary>
Motivation: 随着预算昂贵的模型推动基准性能提升，但对实际成本与可及性造成的偏差尚未充分量化。本研究旨在用价格数据来衡量每单位基准性能的成本下降，以更真实地反映AI能力的性价比进展。

Method: 使用 Artificial Analysis 和 Epoch AI 的数据，构建有史以来最大的当前与历史价格数据集来对基准进行成本分析。对知识、推理、数学、软件工程等基准进行比较。通过排除开放模型以控制竞争效应，并将基准价格除以硬件价格下降，来估计算法效率的进展。

Result: 在 frontier 模型的知识、推理、数学和软件工程基准上，给定基准性能的成本下降速度约为5-10倍/年。成本下降来自经济力量、硬件效率与算法改进等因素。扣除开放模型与硬件价格下降后，算法效率进展约为3倍/年。

Conclusion: 评估者应公开并将基准价格纳入评估框架，作为衡量AI现实世界影响的重要部分。

Abstract: Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a warped picture of progress in practical capabilities per dollar. To remedy this, we use data from Artificial Analysis and Epoch AI to form the largest dataset of current and historical prices to run benchmarks to date. We find that the price for a given level of benchmark performance has decreased remarkably fast, around $5\times$ to $10\times$ per year, for frontier models on knowledge, reasoning, math, and software engineering benchmarks. These reductions in the cost of AI inference are due to economic forces, hardware efficiency improvements, and algorithmic efficiency improvements. Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around $3\times$ per year. Finally, we recommend that evaluators both publicize and take into account the price of benchmarking as an essential part of measuring the real-world impact of AI.

</details>


### [120] [SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments](https://arxiv.org/abs/2511.23465)
*Xinyi Li,Zaishuo Xia,Weyl Lu,Chenjie Hao,Yubei Chen*

Main category: cs.LG

TL;DR: 提出一个名为 SmallWorld Benchmark 的测试平台，用于在严格受控的动力学环境下评估世界模型的学习与推断能力，避免依赖手工奖励信号。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型缺乏统一且受控的评估框架，难以判断它们是否真正捕捉到环境动力学的底层规律。因此需要一个标准化的、可控的测试床来系统地、在完全受观测状态下比较不同模型在长期推断中的表现。

Method: 提出 SmallWorld Benchmark，建立一个在严格受控的动力学下的测试环境，完全可观测状态空间。对代表性架构进行评估：再现状态空间模型（Recurrent State Space Model）、Transformer、Diffusion 模型、神经微分方程（Neural ODE）。在六个领域内进行全面实验，分析模型在捕捉环境结构上的能力，以及长期滚动预测中的逐步退化。

Result: 实验结果揭示了各模型在捕捉环境结构方面的有效性及其在扩展滚动时的预测衰退模式，揭示了当前建模范式的优势与局限性，并为未来在表示学习与动力学建模方面的改进方向提供了洞见。

Conclusion: SmallWorld Benchmark 为世界模型提供了一种统一、受控且可重复的评估框架，便于在跨架构、跨域的对比分析中评估模型对环境结构的把握及其长期推断能力。研究结果显示不同体系在捕捉结构和长期预测方面存在差异，将引导未来在表示学习与动力学建模方面的改进。

Abstract: Current world models lack a unified and controlled setting for systematic evaluation, making it difficult to assess whether they truly capture the underlying rules that govern environment dynamics. In this work, we address this open challenge by introducing the SmallWorld Benchmark, a testbed designed to assess world model capability under isolated and precisely controlled dynamics without relying on handcrafted reward signals. Using this benchmark, we conduct comprehensive experiments in the fully observable state space on representative architectures including Recurrent State Space Model, Transformer, Diffusion model, and Neural ODE, examining their behavior across six distinct domains. The experimental results reveal how effectively these models capture environment structure and how their predictions deteriorate over extended rollouts, highlighting both the strengths and limitations of current modeling paradigms and offering insights into future improvement directions in representation learning and dynamics modeling.

</details>


### [121] [ThetaEvolve: Test-time Learning on Open Problems](https://arxiv.org/abs/2511.23473)
*Yiping Wang,Shao-Rong Su,Zhiyuan Zeng,Eva Xu,Liliang Ren,Xinyu Yang,Zeyi Huang,Xuehai He,Luyao Ma,Baolin Peng,Hao Cheng,Pengcheng He,Weizhu Chen,Shuohang Wang,Simon Shaolei Du,Yelong Shen*

Main category: cs.LG

TL;DR: ThetaEvolve 是一个开源框架，扩展 AlphaEvolve，通过引入单一大模型、海量程序数据库、批量采样、惰性惩罚，以及在测试阶段进行强化学习等机制，使小模型（如 DeepSeek-R1-0528-Qwen3-8B）也能在开放问题上达到新的最优界限，并在多任务中优于仅推理的基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于前沿大模型集合的纯推理系统在持续学习和策略进化方面的局限性，提升对开放优化问题的探索与改进效率。

Method: 提出 ThetaEvolve，包含单一 LLM、一个大程序数据库以增强探索、批量采样以提高吞吐量、惰性惩罚以防止停滞输出，以及可选的奖励塑造，结合测试时强化学习以让模型从经验中持续学习并演化策略。

Result: 在两种模型和四个开放任务上，ThetaEvolve + 测试时 RL 始终优于纯推理基线，RL 训练的检查点在训练任务和未见任务上均呈现更快的进展和更好的最终性能。

Conclusion: 证明了一个小型开源模型也能在开放问题上达到新的最优解界，并且测试时 RL 能提升泛化与学习进化能力，且公开代码。

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [122] [Spectrum-Aware IRS Configuration Techniques for Ultrawideband Signals](https://arxiv.org/abs/2511.21927)
*Alessandro Nordio,Alberto Tarable,Francisco J. Escribano*

Main category: cs.IT

TL;DR: 提出两种基于局部优化的IRS表面配置技术，适用于超宽带下行场景，利用波束分裂色散效应并结合信号谱形，显著提升接收端信号功率，相较窄带基准和全局优化方案更优。


<details>
  <summary>Details</summary>
Motivation: 在高频、超宽带无线场景中，IRS难以实现对大带宽内信号的有效转向且易受波束分裂色散影响，因此需要谱形感知的IRS设计来提升性能。

Method: 提出两种高效的IRS表面局部优化配置技术，针对不同几何布设与信号谱形进行仿真，利用波束分裂色散特性在局部区域内进行优化；与传统窄带方案或对整块IRS进行全局优化的方案相比，强调对谱形的适配。

Result: 仿真结果表明，在不同几何设置和信号谱下，所提方法能在接收端获得更高的信号功率，相较于经典窄带方案或全局优化方案具有明显优势。

Conclusion: 局部化、谱形感知的IRS表面优化策略可有效缓解超宽带场景中的BS色散问题，提升下行链路的覆盖与信号功率，为实际设计提供可行方向。

Abstract: Intelligent reflecting surfaces (IRS) have become the subject of many current research efforts, as the ongoing wireless spectrum crunch has made the need to open higher frequency bands a priority. IRS are one of the alternatives proposed to overcome the problem of line-of-sight blocking in very high frequency wireless scenarios. The current state-of-the-art shows the difficulty of implementing practical IRS designs able to redirect large signal bandwidths, prone to the so-called beam split (BS) dispersion effect. In this work, we propose two highly efficient configuration techniques, adapted to ultrawideband downlink scenarios, based on localized optimization over the IRS surface. Such techniques exploit the BS effect while taking into account for the shape of the transmitted signal spectrum. Simulations considering different geometrical setups and different signal spectra show how the proposed techniques are able to guarantee an increased signal power at the receiver with respect to classical narrowband-based solutions or techniques that perform a global optimization over the entire IRS surface.

</details>


### [123] [Four classes of optimal p-ary cyclic codes](https://arxiv.org/abs/2511.22086)
*Jinmei Fan,Jingyao Feng,Yuhan Men,Yanhai Zhang*

Main category: cs.IT

TL;DR: 提出四类在p进制的循环码，长度为p^m-1、维数为p^m-2m-2、最小距离为4的最优码，p为大于3的奇素数。三类为无限族，五进制情形的多数已知最优循环码都是其特例。


<details>
  <summary>Details</summary>
Motivation: 在研究参数为[p^m-1, p^m-2m-2, 4]的最优p-进制循环码方面进展有限，本文通过放宽对具有汉明重量为3的码字的必要足够条件，并分析有限域中若干方程的解，来构造新的最优码。

Method: 通过放宽对权重为3的码字的条件、分析有限域中相关方程的解集，构造四类p-进制循环码，并证明它们达到最优性；其中三类形成无限族，且多类已知的五进制最优循环码[5^m-1,5^m-2m-2,4]是这些构造的特例。

Result: 得到四类在p>3时的最优p-进制循环码，长度为p^m-1、维数为p^m-2m-2、最小距离为4；其中三类为无限族；并且大量已知的五进制最优循环码均可由本文构造获得或归为其特例。

Conclusion: 扩展了对参数为[p^m-1, p^m-2m-2, 4]的最优p-进制循环码的构造范围，给出四类新码及其无限族，并统一了部分已知的五进制最优码的来源。

Abstract: Let p>3 be an odd prime and m be a positive integer. Little progress on the study of optimal p-ary cyclic codes with parameters [p^m-1,p^m-2m-2,4] has been made.In this paper, by weakening the necessary and sufficient conditions on cyclic codes to have codewords of Hamming weight 3 and analyzing the solutions of certain equations over finite fields, four classes of optimal p-ary cyclic codes deduced by p^m+1/2 with parameters [p^m-1,p^m-2m-2,4] are presented.Wherein three classes of optimal p-ary cyclic codes are infinite.Many classes of known optimal quinary cyclic codes with parameters [5^m-1,5^m-2m-2,4] are special cases of the codes constructed in this paper.

</details>


### [124] [Fluid Antenna-Enhanced Flexible Beamforming](https://arxiv.org/abs/2511.22163)
*Jingyuan Xu,Zhentian Zhang,Jian Dang,Hao Jiang,Zaichen Zhang*

Main category: cs.IT

TL;DR: 提出一个面向平面流体天线阵列的灵活波束成形框架，将波束模式合成和流体天线端口选择统一为一个稀疏回归问题，并通过定制的压缩感知算法结合FFT实现高效求解，同时引入迭代FFT相位检索以确保物理相位的一致性；仿真结果显示相较固定阵列，波束模式重构精度显著提升，适合高分辨率自适应波束成形。


<details>
  <summary>Details</summary>
Motivation: 在固定物理孔径内提升空间分辨率与波束灵活性是未来无线系统的重要需求。流体天线通过可变形的信号传输路径实现更丰富的空间谱，使得在单一阵列上实现窄波束和宽波束等多种波束形状成为可能。本工作旨在以统一框架连接任意波束模式合成与流体天线端口选择，提升波束重构的灵活性与准确性。

Method: 建立一个统一且灵活的框架，将波束模式的重构转化为稀疏回归问题，并使用专门设计的压缩感知算法结合快速傅里叶变换（FFT）高效求解。为确保期望波束中的相位物理一致性，提出基于FFT的迭代相位检索方法。该相位精化过程计算量低、收敛快（每次迭代仅需一次FFT和一次逆FFT）。针对二维平面流体天线阵列的端口选择问题进行优化建模。

Result: 通过仿真实验验证，所提框架在波束模式重构精度方面显著优于传统固定阵列结构，且能够实现高分辨率的自适应波束成形，显示出在未来无线系统中的潜力。

Conclusion: 本文提出的灵活波束成形框架与高效的FFT驱动相位检索算法为流体天线在高分辨率自适应波束成形中的应用提供了有效路径，证实其在实现多样化波束形状与高精度波束分辨率方面的优势，具有实际部署潜力与研究扩展价值。

Abstract: Fluid antenna systems encompass a broad class of reconfigurable antenna technologies that offer substantial spatial diversity for various optimization objectives and communication tasks. Their capability to enhance spatial resolution within a fixed physical aperture makes fluid antennas particularly attractive for next-generation wireless deployments. In this work, we focus on the beamforming problem using a two-dimensional planar fluid antenna array. Since both narrow-beam and broad-beam patterns are essential in practical communication networks, enabling flexible beamforming through fluid antennas becomes an important and interesting research direction. We establish a unified and flexible framework that connects arbitrary beam-pattern synthesis with fluid-antenna port selection. The resulting formulation transforms beam-pattern reconstruction into a sparse regression problem, which is addressed using a tailored compressive sensing algorithm designed to operate efficiently with the fast Fourier transform (FFT). Furthermore, to ensure physically consistent phase modeling in the desired beam, we introduce an iterative FFT-based phase retrieval method. Owing to its structure, the proposed phase-refinement procedure exhibits low computational complexity and rapid convergence, requiring only one FFT and one inverse FFT per iteration. Simulation results demonstrate the effectiveness of the proposed flexible beamforming framework. Compared with conventional fixed-array architectures, fluid antennas exhibit significantly improved beam-pattern reconstruction accuracy, highlighting their potential for high-resolution and adaptive beamforming in future wireless systems.

</details>


### [125] [Constructions of block MDS LDPC codes from punctured circulant matrices](https://arxiv.org/abs/2511.22183)
*Hongwei Zhu,Xuantai Wu,Jingjie Lv,Qinshan Zhang,Shu-Tao Xia*

Main category: cs.IT

TL;DR: 提出两类二进制块MDS LDPC码的构造：基于 punctured circulant permutation 矩阵和列权>1 的循环矩阵 CM(t)；给出 CM(t) 的 Moore 行列式及避免4-环的充分条件，并指出二进制 CPM-QC LDPC码不可能具备块MDS。相较于 Li 等与 Xiao 等的相关工作，所提码在同等长度/码率下具备更强的随机错误纠正能力，且对突发错误更有效；两类结构均适用于二进制场。


<details>
  <summary>Details</summary>
Motivation: 在保持块MDS性质的同时，通过设计无4-环的 Tanner 图来提升 LDPC 码的错误纠正性能，且保持对二进制场的适用性，以便在大突发错误场景与随机错误场景都具备鲁棒性。

Method: 1) 由 punctured circulant permutation 矩阵生成一组二进制块MDS码；2) 通过列权>1 的循环矩阵 CM(t) 构造块MDS LDPC码；3) 给出 CM(t) 的 Moore 行列式公式及一个避免4-环的充分条件；4) 证明二进制 CPM-QC LDPC码不可能实现块MDS；5) 与现有工作对比，分析随机错误和突发错误性能。

Result: 提出两种构造方案获得块MDS LDPC码，且具备无4-环特性；给出 CM(t) 的 Moore 行列式及避免4-环的条件；指出二进制 CPM-QC LDPC码不存在块MDS；相较 Li 等与 Xiao 等的工作，在类似码长与码率下，所提码的随机错误纠错能力更强，且在阵列码场景下对突发错误更有效。

Conclusion: 两类构造均可在二进制场景下实现，扩展了二进制域上具块MDS属性的 LDPC 码设计空间，且达到无4-环的要求，兼具阵列码与 LDPC 的优点。

Abstract: Low density parity check (LDPC) codes, initially discovered by Gallager, exhibit excellent performance in iterative decoding, approaching the Shannon limit. MDS array codes, with favorable algebraic structures, are codes suitable for decoding large burst errors. The Blaum-Roth (BR) code, an MDS array code similar to the Reed-Solomon (RS) code but has a parity-check matrix prone to $4$-cycles. Fossorier proposed constructing quasi-cyclic LDPC codes from circulant permutation matrices but are not MDS array codes. This paper aims to construct codes that possess both the block MDS property and have no $4$-cycles in the Tanner graph of their parity-check matrices, namely the so-called block MDS LDPC codes. Non-binary block MDS QC codes were first constructed by [Tauz {\it et al. }IEEE ITW, 2025] using circulant shift matrices. We first generate a family of block MDS codes over $\F_2$ from punctured circulant permutation matrices. Second, we construct a family of block MDS LDPC codes from circulant matrices with column weight $> 1$ (CM$(t)$). Additionally, we present the Moore determinant formula for CM$(t)$s and a sufficient condition to avoid $4$-cycles in CM\((t)\)-QC LDPC codes' Tanner graphs for $t> 1$. We also point out the non-existence of binary block MDS CPM-QC LDPC codes. Compared to the codes constructed in [Li {\it et al. }IEEE TIT, 2023] and [Xiao {\it et al. }IEEE TCOM, 2021], our block MDS LDPC codes show enhanced random-error-correction at a similar code length and rate. Meanwhile, these codes can effectively combat burst errors when considered as array codes. Both of our two types of constructions for block MDS LDPC codes are applicable to the scenario of the binary field.

</details>


### [126] [Maximum Entropy and Bayesian Conditioning Under Extended Space](https://arxiv.org/abs/2511.22375)
*Boning Yu*

Main category: cs.IT

TL;DR: 本论文分析在扩展概率空间时，贝叶斯条件化是否与最大熵定理一致；比较 Skyrms(1985) 与 Seidenfeld(1986) 的论点，并讨论 Friedman & Shimony(1971) 的结果在何种情形下成立；作者提出两种解读：要么 Friedman & Shimony 的结论只是对 Skyrms 方法的良性后果，要么它对任何扩展空间的方法构成普遍挑战。若采纳第二种解读，则意味着贝叶斯条件化无法处理原始样本空间之外的信息。


<details>
  <summary>Details</summary>
Motivation: 澄清当信息不对应原始样本空间中的事件时，如何将贝叶斯条件化与最大熵方法对齐；评估关于扩展空间的主张在理论上的可靠性，解决 Skyrms 与 Seidenfeld 的分歧。

Method: 对相关理论结果的逻辑分析：在结果空间的乘积空间上进行扩展以引入新信息；对 Skyrms(1985)、Seidenfeld(1986)、Friedman & Shimony(1971)的原文进行比较与批评；提出两种可能的解释路径。

Result: 提出两种解读路径：1) Friedman & Shimony 的结果是对 Skyrms 方法的良性推论；2) 它对任何扩展空间的方法构成普遍挑战。

Conclusion: 若接受第二种解读，贝叶斯条件化将不能容纳原始事件空间之外的信息，这对理论上扩展空间以实现条件化的普遍性提出质疑；若接受第一种解读，Skyrms 的框架可以解释 Seidenfeld 的批评，但需对相关结果进行更细致的分析。

Abstract: This paper examines the conditions under which Bayesian conditioning aligns with Maximum Entropy. Specifically, I address cases in which newly learned information does not correspond to an event in the probability space defined on the sample space of outcomes. To facilitate Bayesian conditioning in such cases, one must therefore extend the probability space so that the new information becomes an event in this expanded space. Skyrms (1985) argues that Bayesian conditioning in an extended probability space on a product space of outcomes aligns precisely with the solution from Maximum Entropy. In contrast, Seidenfeld (1986) uses Friedman and Shimony's (1971) result to criticize Skyrms' approach as trivial, suggesting that alignment holds only under a degenerate probability model. Here, I argue that Friedman and Shimony's result must either (1) be a benign consequence of Skyrms' approach, or (2) pose a universal challenge to any method of extending spaces. Accepting (2) would imply that Bayesian conditioning is incapable of accommodating information beyond the probability space defined on the original outcome space.

</details>


### [127] [On the SER Performance of ZF and MMSE Receivers in Pilot-Aided Simultaneous Communication and Localization](https://arxiv.org/abs/2511.22418)
*Shuaishuai Han,Emad Alsusa,Arafat Al-Dweik*

Main category: cs.IT

TL;DR: 针对协同定位与通信的PASCAL系统，在基站利用定位估计来重构信道矩阵后，分析并近似推导ZF与MMSE的符号错误率（SER），并揭示定位误差对两种线性均衡器的影响及其相对敏感性。


<details>
  <summary>Details</summary>
Motivation: 在无人机上行链路中，定位误差会影响信道估计和矩阵求逆，从而改变SER。由于信道矩阵的参数来自目标位置且包含求逆运算，难以给出闭形式的SER。因此需要对ZF/MMSE在定位不准确条件下的SER进行近似分析以提供设计指导。

Method: 提出PASCAL系统：多架无人机向基站发射，基站估计无人机位置参数以重构信道矩阵，并在此基础上实现ZF和MMSE均衡。由于信道矩阵及其估计参数导致的复杂性，采用Neumann近似与Taylor近似的混合近似方法导出ZF和MMSE的紧致SER表达式。通过理论推导结合仿真验证分析定位误差（角度、距离等）对SER的影响。

Result: 给出ZF和MMSE的紧致SER近似表达式，并揭示若干设计要点：1) 无人机k的平均SER受所有无人机定位误差的影响；2) ZF的平均SER对距离估计误差不敏感，而MMSE对其敏感；3) 相较于其他定位误差，角度误差对两者的影响最强；4) 在较大定位误差条件下，ZF对比MRC可能更差，显示出对定位误差的高敏感性。仿真结果验证分析的准确性并覆盖广泛系统参数。

Conclusion: 提出的紧致SER近似在广泛参数范围内有效，为设计在定位不精确场景下的联合通信与定位系统提供了实用的定量洞见，并强调在高定位误差下ZF可能不如MRC，需要在系统设计中对定位精度和均衡器选型进行权衡。

Abstract: In this paper, a symbol error rate (SER) analysis is provided to evaluate the impact of localization inaccuracy on the communication performance under Zero-Forcing (ZF) and Minimum Mean-Square Error (MMSE) equalizers. Specifically, we adopt a pilot-aided simultaneous communication and localization (PASCAL) system, in which multiple drones actively transmit signals towards the base station (BS). Upon receiving the signal, the BS estimates the drones' location parameters to reconstruct the channel matrix, which is then utilized for ZF and MMSE equalization. As the channel matrix is characterized by the estimated parameters associated with the target's location and the matrix inversion involved in ZF and MMSE further complicates the analysis, obtaining a closed-form SER expression becomes intractable. Thus, a tightly approximated SER expression is respectively derived for ZF and MMSE by using a hybrid approximation method incorporating Neumann approximation and Taylor approximation. Our analysis reveals several important design insights: first, the average SER of drone $k$ for both ZF and MMSE can be affected by the localization errors from all drones including drone $k$; second, the average SER of ZF is unaffected by the estimation inaccuracy of range, whereas the average SER of MMSE is influenced by it; third, ZF and MMSE is the most susceptible to the influence of angle estimation errors compared to the other localization errors; fourth, ZF is highly sensitive to localization errors and may be even worse than maximal ratio combining (MRC) under some conditions of significant estimation errors. Numerical simulation results verify our findings and also validate the accuracy of the analysis across a wide range of system parameters.

</details>


### [128] [Maximum Spectral Efficiency With Adaptive MQAM Transmissions Over Terrestrial Coherent FSO Links](https://arxiv.org/abs/2511.22682)
*Himani Verma,Kamal Singh,Ranjan K. Mallik*

Main category: cs.IT

TL;DR: 自适应未受限MQAM在地面自由空间光通信（FSO）中的谱效率极限已被推导，并显示仅用六种方形MQAM星座即可接近理论极限。


<details>
  <summary>Details</summary>
Motivation: 填补对地面FSO信道上自适应MQAM理论分析的空缺，并通过简化星座集合来降低实现复杂度。

Method: 推导在伽马-伽马涡度且含定向误差的FSO信道上，自适应未受限MQAM的谱效率极限；并通过仿真/分析证明使用六个方形MQAM星座的自适应传输在广泛的信噪比和信道条件下，接近极限（误差0.10–0.12 bit/s/Hz左右）。

Result: 得到自适应未受限MQAM的理论谱效率极限，并证明仅采用六种方形MQAM星座即可实现接近该极限的传输性能，误差在0.10–0.12 bit/s/Hz之间。

Conclusion: 六星座自适应MQAM在FSO信道（伽马-伽马涡度与定向误差）下接近理论极限，简化实现并提高谱效率，是面向未来高容量FSO链路的有力设计指引。

Abstract: Coherent free-space optical (FSO) communication is recognized as a key enabler for ultra-high-capacity fronthaul and backhaul links in next-generation wireless networks. Spectrally efficient $M$-ary quadrature amplitude modulation (MQAM) formats are well-suited for these links. However, theoretical analyses of adaptive MQAM transmissions over terrestrial FSO channels remain limited. In this letter, we first derive the spectral efficiency limit of adaptive unconstrained MQAM over gamma-gamma turbulence with pointing error. We then show that adaptive transmissions using only six square MQAM constellations performs close to the theoretical limit (within $0.10$-$0.12$ bits/s/Hz) across a wide range of signal-to-noise ratios and channel conditions.

</details>


### [129] [On Information Theoretic Fairness With A Bounded Point-Wise Statistical Parity Constraint: An Information Geometric Approach](https://arxiv.org/abs/2511.22683)
*Amirreza Zamani,Ayfer Özgür,Mikael Skoglund*

Main category: cs.IT

TL;DR: 提出在有界点对点统计差等（χ^2）约束下设计公平表示Y，使其最大化对任务T的信息、并在I(Y;X)≤r的压缩限制下实现。利用ε小的信息几何近似将问题转化为二次优化，在某些约束下可得到闭式解；在一般情形给出下界并提出简单且低复杂度的基于最大奇异值/向量的设计，并通过数值对比与最优解进行验证。


<details>
  <summary>Details</summary>
Motivation: 在X、T与潜在敏感属性S均相关的设置中，需在保留任务效用的同时对每个输出y实现点对点的公平性约束。不同于以互信息为约束的全局衡量，点对点χ^2限制提供更强的局部公平性控制，且在没有直接访问S、T的前提下，通过S-X-Y、T-X-Y的马尔可夫结构实现可行的公平表示设计。

Method: 在S-X-Y和T-X-Y的马尔可夫假设下，目标为最大化I(Y;T)并满足I(Y;X)≤r及χ^2(P_{S|y};P_S)≤ε。对ε小的情形，使用信息几何近似对KL与MI进行线性化/二阶展开，将原问题等价转化为一个二次优化问题；在某些约束下可获得闭式解；若不具备闭式解，则给出稳定的下界。提出低复杂度的近似设计，基于构成矩阵的最大奇异值及其对应向量来实现。并给出数值示例来与最优解进行对比。

Result: 给出一个可操作的优化框架：当ε小且满足特定条件时，问题化为二次规划，存在闭式解；若不满足条件，提供可实现的下界与迭代求解策略；设计了基于SVD的简单方法，具有低计算复杂度且性能接近最优。数值实验显示在与最优解对比中，所提近似方案在公平性约束与任务信息保持之间获得良好折衷。

Conclusion: 点对点χ^2公平性结合信息压缩约束的表示学习提供了一种低复杂度、可实现的公平设计路径。通过信息几何近似，将复杂问题化简为二次优化或通过SVD得到高效近似，且在小ε情形下具有理论可行性与实证有效性。

Abstract: In this paper, we study an information-theoretic problem of designing a fair representation under a bounded point-wise statistical (demographic) parity constraint. More specifically, an agent uses some useful data (database) $X$ to solve a task $T$. Since both $X$ and $T$ are correlated with some latent sensitive attribute or secret $S$, the agent designs a representation $Y$ that satisfies a bounded point-wise statistical parity, that is, such that for all realizations of the representation $y\in\cal Y$, we have $χ^2(P_{S|y};P_S)\leq ε$. In contrast to our previous work, here we use the point-wise measure instead of a bounded mutual information, and we assume that the agent has no direct access to $S$ and $T$; hence, the Markov chains $S - X - Y$ and $T - X - Y$ hold. In this work, we design $Y$ that maximizes the mutual information $I(Y;T)$ about the task while satisfying a bounded compression rate constraint, that is, ensuring that $I(Y;X) \leq r$. Finally, $Y$ satisfies the point-wise bounded statistical parity constraint $χ^2(P_{S|y};P_S)\leq ε$. When $ε$ is small, concepts from information geometry allow us to locally approximate the KL-divergence and mutual information. To design the representation $Y$, we utilize this approximation and show that the main complex fairness design problem can be rewritten as a quadratic optimization problem that has simple closed-form solution under certain constraints. For the cases where the closed-form solution is not obtained we obtain lower bounds with low computational complexity. Here, we provide simple fairness designs with low complexity which are based on finding the maximum singular value and singular vector of a matrix. Finally, in a numerical example we compare our obtained results with the optimal solution.

</details>


### [130] [Quantum Private Distributed Matrix Multiplication With Degree Tables](https://arxiv.org/abs/2511.23406)
*Mohamed Nomeir,Alptug Aytekin,Lei Hu,Sennur Ulukus*

Main category: cs.IT

TL;DR: Quantum PDMM：通过量子资源提升私有分布式矩阵乘法的速率；提出量子可行性条件以评估GASP代码在量子设定下的性能，并将该可行性推广至CAT/DOG等代码，在低隐私场景下提出替代代码。


<details>
  <summary>Details</summary>
Motivation: 在不暴露计算细节的前提下，利用量子纠缠和量子信道以降低进行私有矩阵乘法所需的服务器数量或提升吞吐，扩展经典GASP、CAT、DOG等代码在量子环境中的适用性与性能。

Method: 建立在GASP的可行性条件之上，定义量子设置中的可行性标准；分析实现该条件所需的最小隐私要求与A、B矩阵维度之间的关系；提出一族新的量子编码（code families）；证明GASP的可行性也可用于CAT和DOG；在低隐私场景下，当GASP可行性不满足时，给出另一组可用的量子编码。

Result: 在满足量子可行性条件时，达到与最佳经典GASP相当甚至更高的性能；当不可行时，给出隐私-维度关系以及可用的新代码；GASP的可行性可扩展至CAT和DOG并引入一组用于低隐私场景的替代编码。

Conclusion: 量子资源能够提升PDMM的性能，建立了一个以GASP可行性为核心的统一框架，并可扩展至CAT、DOG等代码；在低隐私场景下仍存在可用的替代代码，显示量子PDMM在多场景下的可行性与潜在优势。

Abstract: In this paper, we explore how quantum resources can be used to increase the rate of private distributed matrix multiplication (PDMM). In PDMM, a user who has two high-dimensional matrices, $A$ and $B$, and lacks the computational capabilities to apply matrix multiplication locally, divides the matrices $A$ and $B$ into $K$ and $L$ sub-blocks, respectively. Then, the user sends them to $N$ servers to apply the required multiplication privately from any $T$ servers. The goal is to reduce the number of servers needed to perform the required matrix multiplication. In the quantum setting, we allow the servers to share an entangled state and respond over quantum channels. Upon receiving the qudits, the user applies measurements to obtain the required multiplication. There are two main regimes in the PDMM literature: The high-privacy regime and the low-privacy regime where $T$ is less than $K$ and $L$.
  First, in the high-privacy regime, the state-of-the-art classical code is called the gap additive secure polynomial (GASP) code. We define a feasibility requirement in the quantum setting for the GASP code such that the highest performance is achieved when it is satisfied. When it is not satisfied, we address two main concerns. The first is to find a relation between the minimum privacy requirement and the dimensions of the two matrices needed for the feasibility condition to be satisfied. Second, we develop a new family of codes that can work in the quantum setting.
  Second, since GASP does not work efficiently in the low-privacy regimes compared to cyclic-addition degree tables (CAT) and discretely optimized GASP (DOG), we show that the feasibility condition developed for GASP can be adopted for both CAT and DOG codes as well. In addition, we propose another set of codes that can be used in the low privacy regime in the quantum setting when the feasibility requirement is not satisfied.

</details>


### [131] [Decoding Trombetti-Zhou codes: a new syndrome-based decoding approach](https://arxiv.org/abs/2511.23202)
*Chunlei Li,Angelica Piccirillo,Olga Polverino,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 提出针对 Trombetti-Zhou 码的基于 F_{q^n}-线性结构的 syndrome 解码方法，并引入 F_{q^n}-生成矩阵与 F_{q^n}-校验矩阵来处理非全域线性性（仅在子域 F_{q^n} 上线性）。通过使用 trace almost dual basis 将代码作为 F_q-basis上的评估码表示。若错误秩 t < (d-1)/2，则 syndrome 解码可转化为 Gabidulin 码的解码（维度大一）；若 t = (d-1)/2，则化为求解一个矩阵的秩。给出解码复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 解决对非全域线性 MRD 码（Trombetti-Zhou 码）的解码问题，利用子域线性结构来建立适用于 syndrome 基础的解码框架，并将其与 Gabidulin 码解码进行对接，以提高解码效率和理论理解。

Method: 为在 GF(q^{rn}) 上的任意 GF(q^n)-线性秉性码，定义 GF(q^n)-生成矩阵与 GF(q^n)-校验矩阵，并以 GF_q 基底展开，借助称为 trace almost dual basis 的基底来实现。对 Trombetti-Zhou 码给出这种矩阵表示；将 syndrome 解码问题在 t < (d-1)/2 情况转化为 Gabidulin 码（维度大一）的解码；在 t = (d-1)/2 情况下转化为求解一个矩阵的秩；并给出解码过程的复杂度讨论。

Result: 建立了一个针对于 Trombetti-Zhou 码的 F_{q^n}-线性 syndrome 解码框架，并将其与 Gabidulin 解码以及矩阵秩问题联系起来，给出在不同错误秩条件下的实现策略与复杂度分析。

Conclusion: 提供了将非全域线性 MRD 码的解码迁移到子域线性框架的理论途径，利用 trace almost dual basis 实现对 Trombetti-Zhou 码的高效解码；并揭示了边界情形下需要进行的矩阵秩判定等关键步骤，为相关码的解码研究提供新的方向。

Abstract: In 2019, Trombetti and Zhou introduced a new family of $\mathbb{F}_{q^n}$-linear Maximum Rank Distance (MRD) codes over $\mathbb{F}_{q^{2n}}$. For such codes we propose a new syndrome-based decoding algorithm. It is well known that a syndrome-based decoding approach relies heavily on a parity-check matrix of a linear code. Nonetheless, Trombetti-Zhou codes are not linear over the entire field $\mathbb{F}_{q^{2n}}$, but only over its subfield $\mathbb{F}_{q^{n}}$. Due to this lack of linearity, we introduce the notions of $\mathbb{F}_{q^{n}}$-generator matrix and $\mathbb{F}_{q^{n}}$-parity-check matrix for a generic $\mathbb{F}_{q^{n}}$-linear rank-metric code over $\mathbb{F}_{q^{rn}}$ in analogy with the roles that generator and parity-check matrices play in the context of linear codes. Accordingly, we present an $\mathbb{F}_{q^n}$-generator matrix and $\mathbb{F}_{q^n}$-parity-check matrix for Trombetti-Zhou codes as evaluation codes over an $\mathbb{F}_q$-basis of $\mathbb{F}_{q^{2n}}$. This relies on the choice of a particular basis called \emph{trace almost dual basis}. Subsequently, denoting by $d$ the minimum distance of the code, we show that if the rank weight $t$ of the error vector is strictly smaller than $\frac{d-1}{2}$, the syndrome-based decoding of Trombetti-Zhou codes can be converted to the decoding of Gabidulin codes of dimension one larger. On the other hand, when $t=\frac{d-1}{2}$, we reduce the decoding to determining the rank of a certain matrix. The complexity of the proposed decoding for Trombetti-Zhou codes is also discussed.

</details>


### [132] [Efficient Estimation of Sum-Parameters for Multi-Component Complex Exponential Signals with Theoretical Cramer-Rao Bound Analysis](https://arxiv.org/abs/2511.23318)
*Huiguang Zhang*

Main category: cs.IT

TL;DR: 提出一种基于低维全局和参数的框架EGEM，用于多分量复指数信号的参数估计，解决分量数目大时的排列歧义、Fisher信息矩阵高维求逆和模型阶数选择问题。通过对信号族的全局特征进行汇总（振幅和、功率加权频率、相位相关和），实现对多分量的高效估计和CRB分析。


<details>
  <summary>Details</summary>
Motivation: 当分量数目较大时，逐分量估计面临排列歧义、计算复杂度高（FIM逆运算）以及模型阶数选择困难等挑战，需一种能聚合全局信息、降低维度且避免排列歧义的新框架。

Method: 提出低维度的和参数（振幅和的和、功率加权频率的和、相位相关和）来描述信号族的全局特征，具有显著的物理解释并避免排列歧义。推导确定性与随机模型下这些和参数的精确闭式CRB；提出高效全局估计方法EGEM，利用全体分量的功率汇聚实现近似单分量水平的统计效率，在大样本和小样本情境均表现出渐近效率。

Result: 频率和参数在统计效率方面接近单分量估计且具备功率池化的优势；EGEM在长短样本条件下显著优于Zoom-Interpolated FFT和Root-MUSIC；2000次蒙特卡洛试验显示在仅250个观测值时也接近理论上界。

Conclusion: 引入全局和参数的框架与EGEM方法，能够在避免排列歧义的同时实现高效且鲁棒的多分量cisoid参数估计，理论CRB与经验结果均显示出良好的渐近性质和实际适用性。

Abstract: This paper addresses the challenging problem of parameter estimation for multicomponent complex exponential signals, commonly known as sums of cisoids. Traditional approaches that estimate individual component parameters face significant difficulties when the number of components is large, including permutation ambiguity, computational complexity from high-dimensional Fisher information matrix inversion, and model order selection issues. We introduce a novel framework based on low-dimensional sum-parameters that capture essential global characteristics of the signal ensemble. These parameters include the sum of amplitudes, the power-weighted frequency, and the phase-related sum. These quantities possess clear physical interpretations representing total signal strength, power-weighted average frequency, and composite phase information, while completely avoiding permutation ambiguities. We derive exact closed-form Cramer-Rao bounds for these sum-parameters under both deterministic and stochastic signal models. Our analysis reveals that the frequency sumparameter achieves statistical efficiency comparable to single-component estimators while automatically benefiting from power pooling across all signal components. The proposed Efficient Global Estimation Method (EGEM) demonstrates asymptotic efficiency across a wide range of signal-to-noise ratios, significantly outperforming established techniques such as Zoom-Interpolated FFT and Root-MUSIC in both long- and short-sample regimes. Extensive numerical simulations involving 2000 Monte-Carlo trials confirm that EGEM closely approaches the theoretical performance bounds even with relatively small sample sizes of 250 observations.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [133] [Automated Enumeration of Reconfigurable Architectures for Thermal Management Systems in Battery Electric Vehicles](https://arxiv.org/abs/2511.21855)
*Reihaneh Jahedan,Satya Peddada,Mark Jennings,Sunil Katragadda,James Allison,Nenad Miljkovic*

Main category: eess.SY

TL;DR: 提出一种基于图建模的可重构热管理系统自动枚举与仿真方法，用于BEV的多模式TMS架构优化，生成并评估MATLAB Simscape模型，进行多目标优化以权衡性能、能耗和复杂性。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车对热管理的依赖性增强、废热有限且TMS能耗影响续航，需超越直觉的系统性方法来探索多模式可重构TMS架构，以挖掘潜在性能改进。

Method: 构建TMS架构的图模型，自动枚举并仿真重配置的架构，给定操作模式序列；建立瞬态性能分析框架和基于优化的权衡研究；产生约150种模式序列，筛选出39个独特架构；自动生成MATLAB Simscape模型并对其性能进行评估。

Result: 提供多目标优化结果，为依据用户偏好选择最佳架构提供决策支持；建立了自动化工作流，通过大量架构生成与评估，筛选出可行方案并对不同模式下的性能进行比较。

Conclusion: 该方法为BEV热管理系统的重配置架构探索提供系统化、自动化的工具链，扩展设计空间、提升权衡分析能力，并在多模式条件下帮助选择更优架构。

Abstract: As the automotive industry moves towards vehicle electrification, designing and optimizing thermal management systems (TMSs) for Battery Electric Vehicles (BEVs) has become a critical focus in recent years. The dependence of battery performance on operating temperature, the lack of waste combustion heat, and the significant effect of TMS energy consumption on driving range make the design of BEV TMSs highly complicated compared to conventional vehicles. Although prior research has focused on optimizing the configuration of thermal systems for varying ambient conditions, a holistic approach to studying the full potential of reconfigurable TMS architectures has not yet been fully explored. The complex design landscape of multi-mode reconfigurable systems is difficult to navigate. Relying solely on expert intuition and creativity to identify new architectures both restricts progress and leaves significant performance improvements unrealized. In this study, using graph modelling of TMS architectures, we propose a systematic method to automatically enumerate and simulate reconfigurable architectures for a TMS, given the desired operating modes, along with a framework to conduct transient performance analysis and optimization-based trade-off studies among system performance, energy consumption, and complexity. We explored more than 150 operating mode sequences, retaining 39 unique architectures for further evaluation. MATLAB Simscape models of these architectures were automatically created and their performance evaluated. The multi-objective optimization results provide decision support for selecting the best architecture based on user priorities.

</details>


### [134] [Quantized Distributed Estimation with Event-triggered Communication and Packet Loss](https://arxiv.org/abs/2511.21906)
*Ying Wang,Yanlong Zhao,Ji-Feng Zhang,Karl Henrik Johansson*

Main category: eess.SY

TL;DR: 提出一种带事件触发、量化通信与量化测量的分布式估计算法，并通过一比特信息重构来处理包丢失，证明几乎必然收敛及收敛速率，且全球平均通信比特率随时间趋向零，揭示通信率与收敛速率的权衡。


<details>
  <summary>Details</summary>
Motivation: 在分布式估计中同时降低传输比特数量、应对事件触发以及包丢失带来的挑战，同时兼顾量化约束。

Method: 提出事件触发的分布式估计算法，结合量化通信与量化测量，设计一比特信息重构策略以应对包丢失，给出收敛性与收敛速率分析，证明平均通信比特率下降到零，并给出设计通信率以达到期望收敛速率的指南，以及提供数值验证。

Result: 证明算法具有几乎必然收敛性及给定的收敛速率；全球平均通信比特率随时间趋向零；提出了通信率与收敛速率之间的权衡关系；通过数值示例验证理论结论。

Conclusion: 在存在事件触发、包丢失和量化约束的分布式环境中，该算法实现低通信成本的估计，且给出设计准则以在所需收敛速率下实现收敛。

Abstract: This paper focuses on the problem of quantized distributed estimation with event-triggered communication and packet loss, aiming to reduce the number of transmitted bits. The main challenge lies in the inability to differentiate between an untriggered event and a packet loss occurrence. This paper proposes an event-triggered distributed estimation algorithm with quantized communication and quantized measurement, in which it introduces a one-bit information reconstruction method to deal with packet loss. The almost sure convergence and convergence rate of the proposed algorithm are established.Besides, it is demonstrated that the global average communication bit-rate decreases to zero over time. Moreover, the trade-off between communication rate and convergence rate is revealed, providing guidance for designing the communication rate required to achieve the algorithm's convergence rate. A numerical example is supplied to validate the findings.

</details>


### [135] [Performance of the Kalman Filter and Smoother for Benchmark Studies](https://arxiv.org/abs/2511.22034)
*Batin Kurt,Umut Orguner*

Main category: eess.SY

TL;DR: 提出对卡尔曼滤波和卡尔曼平滑器的分析性均方误差（MSE）表达式，用于在未知或不可用真实系统动力学的基准评估场景，给出线性时间复杂度的递推计算，并考虑模型失配。


<details>
  <summary>Details</summary>
Motivation: 在真实系统动力学不可获得时，需要对确定性状态轨迹下的滤波/平滑性能进行预测评估，以避免高成本的蒙特卡罗仿真，并提升基准研究的效率。

Method: 推导KF和KS在确定性状态轨迹与模型失配条件下的递推MSE表达式，确保时间复杂度为线性，并给出对测量模型失配的考虑；提供适用于长轨迹的高效性能评估框架。

Result: 得到具有线性时间复杂度的递推MSE表达式，并通过仿真验证其在存在系统与观测模型失配时对估计误差的预测准确性与计算效率， especialmente适合长轨迹。

Conclusion: 该框架为在缺少真实系统动力学信息时的KF/KS性能预测提供一个高效且鲁棒的工具，能够在固定状态轨迹下处理模型失配并显著减少计算成本。

Abstract: We propose analytical mean square error (MSE) expressions for the Kalman filter (KF) and the Kalman smoother (KS) for benchmark studies, where the true system dynamics are unknown or unavailable to the estimator. In such cases, as in benchmark evaluations for target tracking, the analysis relies on deterministic state trajectories. This setting introduces a model mismatch between the estimator and the system, causing the covariance estimates to no longer reflect the actual estimation errors. To enable accurate performance prediction for fixed state trajectories without relying on computationally intensive Monte Carlo simulations, we derive recursive MSE expressions with linear time complexity. The proposed framework also accounts for measurement model mismatch and provides an efficient tool for performance evaluation in benchmark studies with long trajectories. Simulation results confirm the accuracy and computational efficiency of the proposed method.

</details>


### [136] [CBF Based Quadratic Program for Trajectory Tracking of Underatuated Marine Vessels](https://arxiv.org/abs/2511.22091)
*Ji-Hong Li*

Main category: eess.SY

TL;DR: 通过极坐标变换简化海上船舶二阶跟踪模型，但引入奇点和严格反馈性问题；提出使用控制屏障函数（CBF）结合二次规划（QP）来处理奇点并实现安全的轨迹跟踪，数值仿真验证有效性。


<details>
  <summary>Details</summary>
Motivation: 原始模型为海上船舶的二输入三输出二阶跟踪系统，直接设计控制器存在实现与稳定性挑战。将其化简为两输入两输出的反馈形式虽简化了结构，但不满足严格反馈结构且极坐标变换本身存在奇异性，需确保安全性与可实现性。

Method: 在极坐标变换后仍面临的奇异性与非严格反馈问题，采用控制屏障函数（CBF）作为安全约束，通过将轨迹跟踪问题转化为一个二次规划（QP）并由QP优化求解来实现虚输入的稳定设计与轨迹跟踪。

Result: 数值仿真结果表明所提CBF+QP框架能有效处理极坐标引入的奇异性并实现稳定的轨迹跟踪，同时在保持安全约束方面表现良好。

Conclusion: 该工作提供了一种将极坐标变换带来的奇异性与非严格反馈结构问题以CBF约束结合QP求解的整体解决方案，为海洋船舶二阶跟踪系统的安全鲁棒控制提供了可行路径。

Abstract: By introducing two polar coordinates transformations, the marine vessel's original two-input-three-output second-order tracking model can be reduced to a two-input-two-output feedback form. However, the resulting system does not confirm to the strict-feedback structure, leading to potential singularity when designing the stabilizing function for the virtual input in the recursive controller design. Moreover, the polar coordinate transformation itself inherently introduces singularities. To address these singularity issues, this paper employs a control barrier function (CBF) based approach and formulates the trajectory tracking problem as a quadratic program (QP) solved via a QP optimizer. Numerical simulations are carried out to demonstrate the effectiveness of the proposed method.

</details>


### [137] [Output-Feedback Stabilizing Policy Iteration for Convergence Assurance of Unknown Discrete-Time Systems with Unmeasurable States](https://arxiv.org/abs/2511.22160)
*Dongdong Li,Jiuxiang Dong*

Main category: eess.SY

TL;DR: 提出一种基于数据驱动的输出反馈稳定性策略迭代，用于未知离散时间线性系统且状态不可测。通过尺度压缩与修正策略迭代相结合，在仅依赖输入输出数据的条件下学习稳定闭环控制策略，并给出稳定性分析，且通过仿真验证。


<details>
  <summary>Details</summary>
Motivation: 在未知系统且状态不可测的情况下，传统策略迭代需要一个稳定的起始策略，获取困难且难以保证稳定性与收敛性。因此需要一个仅依赖输入输出数据、能够保证稳定性与收敛性的输出反馈学习框架。

Method: 提出累积标量参数，将原系统压缩为一个稳定的尺度；将修改的策略迭代与参数更新规则结合，在逐步放大/还原到原系统的过程中保持稳定性，从而获得稳定的控制策略。整个过程仅由输入输出数据驱动，并对输出反馈给出稳定性分析。

Result: 方法能够在未知系统条件下学习到稳定的闭环控制策略，并且具有收敛性保证；通过仿真验证了策略迭代在输出反馈下的效果。

Conclusion: 数据驱动的输出反馈稳定性策略迭代为未知离散时间线性系统提供了一条可行路径，可以在仅有输入输出数据的情况下获得稳定的控制策略，并具备理论稳定性分析与仿真验证。

Abstract: This note proposes a data-driven output-feedback stabilizing policy iteration for unknown linear discrete-time systems with unmeasurable states. Existing policy iteration methods for optimal control must start from a stabilizing control policy, which is particularly challenging to obtain for unknown systems, especially when states are unavailable. In such cases, it is more difficult to guarantee stability and convergence performance. To address this problem, an output-feedback stabilizing policy iteration framework is developed to learn closed-loop stabilizing control policies while ensuring convergence performance. Specifically, cumulative scalar parameters are introduced to compress the original system to a stable scale. Then, by integrating modified policy iteration with parameter update rules, the system is gradually amplified/restored to the original system while preserving stability such that the stabilizing control policy is obtained. The entire process is driven solely by input-output data. Moreover, a stability analysis is provided for output-feedback. The proposed approach is validated by simulations.

</details>


### [138] [An Equality Set Projection Approach for TSO-DSO Coordination Dispatch](https://arxiv.org/abs/2511.22179)
*Bo Li,Xicong Pang,Guangrui Wei,Haiwang Zhong,Grant Ruan,Zhengmao Li,Edris Pouresmaeil*

Main category: eess.SY

TL;DR: 提出一种基于加速非迭代等式集合投影(ESP)的TSO-DSO协同调度方法，通过相邻面的搜索实现高维可行区域(FR)的投影，结合正则化加速在退化情形下降低计算负担，并在多种案例中验证其有效性与计算效率。


<details>
  <summary>Details</summary>
Motivation: 在高渗透分布式能源资源(DER)背景下，TSO与DSO的协调调度(COD)需要高效且收敛性好的方法。现有的迭代型方法收敛性差、效率低；非迭代方法尽管解决了迭代问题，却难以在高维FR下求解等效投影。

Method: 提出加速非迭代ESP算法来投影FR。ESP通过相邻面搜索构建FR的投影，降低对顶点数量的敏感性，适用于高维FR。为应对退化造成的计算负担，给出基于正则化的加速方法。应用于多种数据集和实际系统进行验证。

Result: 在高维FR投影与退化场景下，ESP及其正则化加速方法显著提高计算效率，能有效构建FR投影；在多实例（多面体数据集、IEEE 33-母线系统、T118D10 TSO-DSO系统）上证明了方法的有效性与计算效率。

Conclusion: 提出的加速ESP框架为TSO-DSO COD提供了一种非迭代、适用于高维FR的高效解法，并通过正则化与相邻面搜索提升在退化情况下的鲁棒性与效率，具有良好的实际应用潜力。

Abstract: Coordinated optimization dispatch (COD) of transmission system operator (TSO) and distribution system operator (DSO) can effectively ensure system security and efficiency under high-penetration distributed energy resource (DER) integration. Researches of large-scale COD problem can be categorized into iterative approaches that allow DSO to dispatch independently, and non-iterative methods based on projections of feasible regions (FR). However, the iterative methods suffer from low computational convergence and efficiency, while non-iterative methods struggle to solve equivalent projections with high-dimensional FR. To address these issues, this paper proposes a TSO-DSO coordinated dispatch approach based on an accelerated non-iterative Equality Set Projection (ESP) algorithm. First, ESP algorithm is employed to overcome the bottleneck of high-dimensional FR construction. Second, an regularization-based accelerated method is proposed to reduce computational burden when degeneracy occurs. Accelerated ESP algorithm constructs projection of FR via adjacent facet searching. Therefore, it is less sensitive to the increase of vertices and could efficiently construct the projection of high-dimensional FR. Case studies on a polyhedron dataset, IEEE 33-Bus System and T118D10 TSO-DSO system demonstrate the effectiveness and computational efficiency of the proposed COD approach.

</details>


### [139] [Joint Scheduling of Workload Demand and Energy Supply in Low-carbon Data Centers with Decision-Dependent Uncertainty Set](https://arxiv.org/abs/2511.22244)
*Maoyuan Ma,Wangyi Guo,Lei Yang,Zhanbo Xu,Xiaohong Guan*

Main category: eess.SY

TL;DR: 提出一种基于决策相关不确定集的工作负载分类模型，联合调度含氢分布式能源系统的随机工作负载，目标在低碳数据中心实现能耗与成本优化；通过混合整数规划与滚动时域算法提高鲁棒性与可行性，在多数不确定情境下显著降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 解决低碳IDC在存在不可预测性且会因调度决策而改变的不确定工作负载下的能量系统效率与成本问题；决策相关不确定性建模能更真实地刻画工作负载的时空弹性与系统耦合。

Method: 将不同类型的随机工作负载基于决策相关的不确定集进行分类，明确时空弹性与决策依赖性以线性约束表示；建立工作负载需求与能源供给的混合整数规划；为提升对高波动情景的鲁棒性，提出滚动时域算法以确保非提前性和全场景可行性。

Result: 数值测试表明，与基准方法相比，在多数不确定场景下所提方法能显著降低能源运营成本，且能实现有效的工作负载调度与能源供给的协同优化。

Conclusion: 以决策相关不确定性建模结合滚动时域的联合调度框架，提升IDC在低碳约束下的鲁棒性与经济性，具有良好的场景适应性和应用潜力。

Abstract: This paper addresses the joint scheduling problem of stochastic workloads and a hydrogen-enabled distributed energy system in a low-carbon Internet data centers (IDC). Although such workloads can be shifted over temporal and spatial horizons, it poses challenges when they cannot be accurately predicted, resulting in significant efficiency degradation and high operational cost of the energy system. The problem becomes even more difficult when the workload shifting decisions would influence their randomness, which is natural for the IDC workloads. To tackle these issues, we propose a workload classification model based on the decision-dependent uncertainty set, where the spatiotemporal elasticity of different types of random workloads are clearly identified and the decision dependencies are explicitly described as linear constraints. Thus, a mixed integer program is then established for the optimal scheduling of both workload demand and energy supply. To enhance system resilience against high volatility scenarios, a rolling horizon algorithm is developed to ensure nonanticipativity and full-scenario feasibility. Numerical tests demonstrate that the proposed method exhibits effective workload scheduling decisions with dramatic energy operational cost reductions compared to the benchmarks under most of the uncertain scenarios.

</details>


### [140] [Distributed Koopman Operator Learning for Perception and Safe Navigation](https://arxiv.org/abs/2511.22368)
*Ali Azarbahram,Shenyu Liu,Gian Paolo Incremona*

Main category: eess.SY

TL;DR: A unified, scalable framework combining MPC with distributed Koopman operator learning to enable safe, cooperative autonomous navigation in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To achieve scalable, multi-agent predictive control for autonomous navigation with safety guarantees, leveraging distributed learning to avoid centralized data aggregation in ITS networks.

Method: Use high-dimensional sensory data to learn a distributed Koopman operator via consensus among multiple agents, forecast obstacle motion, represent future densities with Gaussian mixtures, approximate confidence regions by convex polytopes, and embed as linear constraints in MPC for collision avoidance.

Result: Convergence guarantees for the distributed Koopman learning, predictive constraint formulations, and extensive simulations demonstrating reliable, safe, and computation-efficient navigation in complex scenarios.

Conclusion: The framework offers scalable, cooperative perception-enabled navigation with safety guarantees, suitable for ITS applications and large sensor networks.

Abstract: This paper presents a unified and scalable framework for predictive and safe autonomous navigation in dynamic transportation environments by integrating model predictive control (MPC) with distributed Koopman operator learning. High-dimensional sensory data are employed to model and forecast the motion of surrounding dynamic obstacles. A consensus-based distributed Koopman learning algorithm enables multiple computational agents or sensing units to collaboratively estimate the Koopman operator without centralized data aggregation, thereby supporting large-scale and communication-efficient learning across a networked system. The learned operator predicts future spatial densities of obstacles, which are subsequently represented through Gaussian mixture models. Their confidence ellipses are approximated by convex polytopes and embedded as linear constraints in the MPC formulation to guarantee safe and collision-free navigation. The proposed approach not only ensures obstacle avoidance but also scales efficiently with the number of sensing or computational nodes, aligning with cooperative perception principles in intelligent transportation system (ITS) applications. Theoretical convergence guarantees and predictive constraint formulations are established, and extensive simulations demonstrate reliable, safe, and computationally efficient navigation performance in complex environments.

</details>


### [141] [Optimal Singular Perturbation-based Model Reduction for Heterogeneous Power Systems](https://arxiv.org/abs/2511.22728)
*Yue Huang,Dixant B. Sapkota,Manish K. Singh*

Main category: eess.SY

TL;DR: 提出两种基于奇异摄动的降维方法，用贪心和非线性优化在时标无关框架下识别应降维的快态，且不依赖状态的物理含义标签。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统规模与复杂性激增，传统基于奇异摄动的降维依赖于对系统分量的物理含义和模型假设，面对异构、黑箱化组件时受到挑战。需在不依赖先验物理标签的条件下，智能地识别需要降维的快态以降低计算复杂性。

Method: 提出一个时标无关的奇异摄动框架；其一使用贪心优化逐步选择需要降维的快态；其二通过非线性优化实现状态变换以获得最优的降维模型。两种方法均不需要事先给定状态的物理含义。

Result: 在包含同步机、逆变器和线路动力学的测试系统上进行数值研究，验证了方法的普适性与准确性。

Conclusion: 所提两种方法能够在异构、黑箱化组件普遍存在的场景下实现有效的降维，显著降低计算复杂性并保持建模准确性，具有良好的推广潜力。

Abstract: Power systems are globally experiencing an unprecedented growth in size and complexity due to the advent of nonconventional generation and consumption technologies. To navigate computational complexity, power system dynamic models are often reduced using techniques based on singular perturbation. However, several technical assumptions enabling traditional approaches are being challenged due to the heterogeneous, and often black-box, nature of modern power system component models. This work proposes two singular perturbation approaches that aim to optimally identify fast states that shall be reduced, without prior knowledge about the physical meaning of system states. After presenting a timescale-agnostic formulation for singular perturbation, the first approach uses greedy optimization to sequentially select states to be reduced. The second approach relies on a nonlinear optimization routine allowing state transformations while obtaining an optimally reduced model. Numerical studies on a test system featuring synchronous machines, inverters, and line dynamics demonstrate the generalizability and accuracy of the developed approaches.

</details>


### [142] [Switching control of underactuated multi-channel systems with input constraints for cooperative manipulation](https://arxiv.org/abs/2511.22810)
*Dongjae Lee,Dimos V. Dimarogonas,H. Jin Kim*

Main category: eess.SY

TL;DR: 提出一种事件触发的切换控制框架，针对具输入约束的非完全驱动多通道系统（如协作推/拉载物的多智能体），通过混合整数线性规划实现通道分配与输入约束的可行性，并以QP稳定控制实现实时性与稳定性保障，理论给出半全局指数稳定性与在非即时切换下的渐近稳定性。通过2D/3D自由飞行器与多机器人非预设式推动任务的仿真验证。


<details>
  <summary>Details</summary>
Motivation: 在协作操控场景中，系统往往欠驱动且受输入约束，且是否合理地分配通道（哪些智能体对哪些力/通道负责）直接影响 controllability 与任务完成度。现有方法通常未同时解决通道分配、输入约束与稳定性之间的耦合，因此需要一个统一框架来处理这三者，以提高鲁棒性与实时性。

Method: 将控制问题建模为混合整数线性规划（MILP），以同时处理通道分配与输入约束并实现系统稳定性；引入事件触发控制策略，以减少控制更新频率并在切换事件之间通过一阶/二次规划（QP）实现稳定的闭环控制；理论证明提出方法在满足条件下具备半全局指数稳定性，并扩展到非即时切换下的非预设式协作推操作的渐近稳定性。

Result: 给出通道分配与输入约束下的可行性充要条件（通过MILP的可行性判定）；证明在该框架下系统具有半全局指数稳定性，以及对扩展场景（非即时切换）的渐近稳定性；通过2D和3D自由飞行器仿真以及多机器人非预设式推动任务验证算法的有效性与实时性。

Conclusion: 该工作提出了一个统一且可在实时条件下实现的事件触发切换控制框架，成功将通道分配、输入约束与稳定性纳入同一优化与控制流程，理论上提供了严格的稳定性保证，且在多种仿真场景中验证了对协作操控任务的适用性与鲁棒性。

Abstract: This work presents an event-triggered switching control framework for a class of nonlinear underactuated multi-channel systems with input constraints. These systems are inspired by cooperative manipulation tasks involving underactuation, where multiple underactuated agents collaboratively push or pull an object to a target pose. Unlike existing approaches for multi-channel systems, our method addresses underactuation and the potential loss of controllability by additionally addressing channel assignment of agents. To simultaneously account for channel assignment, input constraints, and stabilization, we formulate the control problem as a Mixed Integer Linear Programming and derive sufficient conditions for its feasibility. To improve real-time computation efficiency, we introduce an event-triggered control scheme that maintains stability even between switching events through a quadratic programming-based stabilizing controller. We theoretically establish the semi-global exponential stability of the proposed method and the asymptotic stability of its extension to nonprehensile cooperative manipulation under noninstantaneous switching. The proposed framework is further validated through numerical simulations on 2D and 3D free-flyer systems and multi-robot nonprehensile pushing tasks.

</details>


### [143] [Switching-time bioprocess control with pulse-width-modulated optogenetics](https://arxiv.org/abs/2511.22893)
*Sebastián Espinel-Ríos*

Main category: eess.SY

TL;DR: 通过将光诱导的基因表达控制从单纯的光强度（幅值）切换到脉宽调制与强化学习相结合的方法，利用占空比参数化控制来实现ON/OFF之间的切换，从而提高动态代谢控制的可控性与可调性，克服强跃变型剂量反应带来的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决光遗传学系统中仅靠光强度控制时的可调性不足问题，尤其是在剂量-表达关系陡峭时，易导致只能实现全开或全关的表达水平。通过脉宽调制实现光输入的平均效应，并以强化学习优化占空比以提高生物过程的可控性与鲁棒性；同时寻求比混合整数规划更高的计算可行性。

Method: 提出一种基于强化学习的解决方案，控制动作通过占空比进行参数化，占空比是一个连续变量，编码在每个 forcing period 内光的开与关的切换时间，从而保持光强度的二元本质，避免将问题直接转化为高维的离散/连续混合优化。

Result: 作为方法论框架提出，尚未给出具体数值实验结果；与以往单一幅值控制相比，理论上可提供更平滑的输出和更好的调控性，但需在实际生物系统中进行验证。

Conclusion: 基于占空比的 PWM–RL 控制为光遗传性动态调控提供了有前景的方向，能够在大尺度时间窗内维持二元光输入的同时提升可控性与可调性，后续工作应聚焦于在具体生物过程中的实现与实验验证。

Abstract: Biotechnology can benefit from dynamic control to improve production efficiency. In this context, optogenetics enables modulation of gene expression using light as an external input, allowing fine-tuning of protein levels to unlock dynamic metabolic control and regulation of cell growth. Optogenetic systems can be actuated by light intensity. However, relying solely on intensity-driven control (i.e., signal amplitude) may fail to properly tune optogenetic bioprocesses when the dose-response relationship (i.e., light intensity versus gene-expression strength) is steep. In these cases, tunability is effectively constrained to either fully active or fully repressed gene expression, with little intermediate regulation. Pulse-width modulation, a concept widely used in electronics, can alleviate this issue by alternating between fully ON and OFF light intensity within forcing periods, thereby smoothing the average response and enhancing process controllability. Naturally, optimizing pulse-width-modulated optogenetics entails a switching-time optimal control problem with a binary input over many forcing periods. While this can be formulated as a mixed-integer program on a refined time grid, the number of decision variables can grow rapidly with increasing time-grid resolution and number of forcing periods, compromising tractability. Here, we propose an alternative solution based on reinforcement learning. We parametrize control actions via the duty cycle, a continuous variable that encodes the ON-to-OFF switching time within each forcing period, thereby respecting the intrinsic binary nature of the light intensity.

</details>


### [144] [RDS-DeePC: Robust Data Selection for Data-Enabled Predictive Control via Sensitivity Score](https://arxiv.org/abs/2511.22952)
*Jiachen Li,Shihao Li*

Main category: eess.SY

TL;DR: 提出鲁棒数据选择DeePC（RDS-DeePC），通过影响函数分析对数据段的影响力进行自适应筛选，以降低计算复杂度并抑制数据污染的影响；对非线性系统引入两阶段在线选择并借助 LiSSA 加速。


<details>
  <summary>Details</summary>
Motivation: 数据驱动预测控制如 DeePC 能无模型控制，但在数据量增大时计算复杂且易受坏数据影响。需要一种无标注数据质量的鲁棒选取机制来同时提升鲁棒性和可扩展性。

Method: 通过影响函数推导出每个轨迹片段的敏感度分数，证明高敏感度片段对应异常，低敏感度片段代表一致数据；选取低敏感度片段以实现计算效率和自动离群点过滤；对于非线性系统，提出两阶段在线选择并用 LiSSA 加速。

Result: 实现了无需数据质量标签的鲁棒数据选择，提升 DeePC 的计算效率并对离群数据具备自适应滤波能力；非线性情形通过两阶段在线选择结合 LiSSA 实现可扩展性。

Conclusion: RDS-DeePC 提供一种鲁棒、可扩展的基于数据的预测控制框架，自动抑制离群数据影响并降低计算成本，且可拓展到非线性系统。

Abstract: Data-Enabled Predictive Control (DeePC) offers a powerful model-free approach to predictive control, but faces two fundamental challenges: computational complexity scaling cubically with dataset size, and severe performance degradation from corrupted data. This paper introduces Robust Data Selection DeePC (RDS-DeePC), which addresses both challenges through influence function analysis. We derive a sensitivity score quantifying each trajectory segment's leverage on the optimization solution, proving that high-sensitivity segments correspond to outliers while low-sensitivity segments represent consistent data. By selecting low-sensitivity segments, RDS-DeePC achieves computational efficiency and automatic outlier filtering without requiring data quality labels. For nonlinear systems, we extend the framework through a two-stage online selection approach accelerated by the LiSSA algorithm.

</details>


### [145] [Adaptive Trajectory Bundle Method for Roll-to-Roll Manufacturing Systems](https://arxiv.org/abs/2511.22954)
*Jiachen Li,Shihao Li*

Main category: eess.SY

TL;DR: 自适应的 derivative-free 轨迹束优化方法用于滚轮卷绕(R2R)控制，具严格约束处理能力；通过插值样本束近似非线性动力学与代价函数，配合自适应信任域与罚参实现鲁棒收敛，无需手动调参；六区R2R仿真显示跟踪精度接近梯度基MPC，且约束满足性优于采样类方法（如MPPI）。


<details>
  <summary>Details</summary>
Motivation: R2R制造需要在严格约束下实现对张力与速度的精确控制；梯度基的MPC计算开销大，采样法难以保证强约束的满足。

Method: 提出自适应轨迹束方法，采用导数为零的序列凸优化；通过插值样本束近似系统动力学与代价，并以自适应信任域与罚参数驱动收敛，避免人工调参；实现对非线性系统的约束严格管理。

Result: 在六区R2R系统的仿真中，方法的跟踪精度与梯度基MPC相当，且在约束满足性方面优于MPPI等采样方法。

Conclusion: 自适应轨迹束方法提供鲁棒收敛与严格约束处理能力，降低调参需求，是 constrained control 任务中替代梯度基MPC 的可行方案。

Abstract: Roll-to-roll (R2R) manufacturing demands precise tension and velocity control under strict operational constraints. Model predictive control requires gradient computation, while sampling-based methods such as MPPI struggle with hard constraint satisfaction. This paper presents an adaptive trajectory bundle method that achieves rigorous constraint handling through derivative-free sequential convex programming. The approach approximates nonlinear dynamics and costs via interpolated sample bundles, with adaptive trust regions and penalty parameters ensuring robust convergence without manual tuning. Simulations on a six-zone R2R system demonstrate tracking accuracy comparable to gradient-based MPC with superior constraint satisfaction over sampling-based alternatives.

</details>


### [146] [An LLM-Assisted Multi-Agent Control Framework for Roll-to-Roll Manufacturing Systems](https://arxiv.org/abs/2511.22975)
*Jiachen Li,Shihao Li,Christopher Martin,Zijun Chen,Dongmei Chen,Wei Li*

Main category: eess.SY

TL;DR: 提出一个基于大语言模型的多智能体框架用于R2R系统的控制系统设计与自适应，覆盖系统识别、控制器选择与调优、仿真-实际转化与安全验证、持续监测与诊断，以及模型定期更新；在实验中实现张力控制与速度跟踪，在显著模型不确定性下达到收敛，降低人工调优成本并提供诊断信息。


<details>
  <summary>Details</summary>
Motivation: 解决滚筒-卷材制造中对于精确张力与速度控制的高依赖性与控制器调试与自适应过程耗时的问题；通过AI辅助实现更高效、可追踪、安全的自动化控制系统设计与维护。

Method: 五阶段框架：1) 利用运营数据进行系统辨识；2) 自动化选择与调优控制器；3) sim-to-real迁移与安全验证；4) 持续监测与诊断；5) 周期性模型更新。结合多智能体/LLM协同决策与解释。

Result: 在R2R系统上进行实验验证，实现张力调节和速度跟踪，在显著模型不确定性的情形下通过迭代自适应实现性能收敛；减少手动调谐工作量，提供透明诊断信息，便于维护规划。

Conclusion: 为制造控制系统引入AI辅助自动化提供实际路径，兼具安全性与可解释性，推动R2R等生产线的控制系统设计与维护的现代化。

Abstract: Roll-to-roll manufacturing requires precise tension and velocity control to ensure product quality, yet controller commissioning and adaptation remain time-intensive processes dependent on expert knowledge. This paper presents an LLM-assisted multi-agent framework that automates control system design and adaptation for R2R systems while maintaining safety. The framework operates through five phases: system identification from operational data, automated controller selection and tuning, sim-to-real adaptation with safety verification, continuous monitoring with diagnostic capabilities, and periodic model refinement. Experimental validation on a R2R system demonstrates successful tension regulation and velocity tracking under significant model uncertainty, with the framework achieving performance convergence through iterative adaptation. The approach reduces manual tuning effort while providing transparent diagnostic information for maintenance planning, offering a practical pathway for integrating AI-assisted automation in manufacturing control systems.

</details>


### [147] [The Battle of the Water Futures](https://arxiv.org/abs/2511.22986)
*Dennis Zanutto,Christos Michalopoulos,Lydia Tsiami,André Artelt,Jasmin Brandt,Demetrios Eliades,Stelios Vrachimis,Stefano Alvisi,Valentina Marsili,Filippo Mazzoni,Panagiotis Smartzis,Barbara Hammer,Phoebe Koundouri,Marios Polycarpou,Dragan Savić*

Main category: eess.SY

TL;DR: 引入新的阶段性设计、深不确定性下的水网设计竞赛，作为WDSA/CCWI 2026的一部分，倡导分阶段干预与开放式评估框架。


<details>
  <summary>Details</summary>
Motivation: 推动水系统在深不确定性下的鲁棒设计与运营研究，融合政策制定与人工智能，提升城市供水与废水系统设计能力。

Method: 提出“分阶段设计”的长周期优化/运营问题，处理不可观测与未知的不确定性，融入政策制定和人工智能元素；使用开源、透明的评估框架进行评估。

Result: 提出一项新竞赛及其开放式评估框架，建立可比基准，促进深不确定性下的鲁棒设计与运营研究。

Conclusion: 此竞赛体现Water-Futures的使命与WDSA/CCWI 2026主题，旨在推动下一代城市水系统的设计与运营，鼓励参与者在阶段性干预和AI应用方面探索创新解决方案。

Abstract: The highly anticipated 'Battle of the Water Networks' is back with a new challenge for the water community. This competition will be hosted at the 4th International Joint Conference on Water Distribution Systems Analysis and Computing and Control in the Water Industry (WDSA/CCWI 2026), taking place in Paphos, Cyprus, from May 18-21, 2026. This competition embodies the core mission of Water-Futures and the theme for WDSA/CCWI 2026: "Designing the next generation of urban water (and wastewater) systems."
  The objective is to design and operate a water distribution system over a long-term horizon under deep uncertainty, with interventions applied in stages. For the first time, this challenge features a staged-design approach, unobservable and unknown uncertainties, and incorporates elements of policymaking and artificial intelligence. The solutions will be assessed using a transparent and inspectable open-source evaluation framework.

</details>


### [148] [Identification of contractive Lur'e-type systems via kernel-based Lipschitz design](https://arxiv.org/abs/2511.22993)
*Cesare Donati,Fabrizio Dabbene,Constantino Lagoa,Carlo Novara,Yoshio Ebihara*

Main category: eess.SY

TL;DR: Identifying contractive Lur'e-type systems by combining linear priors with a kernel nonlinear feedback while enforcing contractivity through Lipschitz design.


<details>
  <summary>Details</summary>
Motivation: To obtain predictive, interpretable models that respect the contractive dynamics of Lur'e-type systems, addressing gaps between data-driven models and physical properties.

Method: Integrate linear prior knowledge with a kernel representation of the nonlinear feedback and impose contractivity by constraining the Lipschitz constant. This yields a model class with built-in stability and interpretability.

Result: The proposed algorithms produce accurate and interpretable models that preserve the contractive nature of the true system. Enforcing contractivity improves parameter estimation and yields models that are physically meaningful and aligned with the system’s dynamics.

Conclusion: Contractivity-aware identification provides reliable, interpretable models for Lur’e-type systems, with improved estimation and physical fidelity compared to unconstrained approaches.

Abstract: This paper addresses the problem of identifying contractive Lur'e-type systems. Specifically, it proposes an identification framework that integrates linear prior knowledge with a kernel representation of the nonlinear feedback while systematically enforcing contractivity via Lipschitz constant design. The resulting algorithms provide models that are accurate in prediction, interpretable, and faithful to the contractive nature of the true system. Numerical experiments demonstrate that enforcing contractivity significantly improves parameter estimation and yields models that are both accurate and physically meaningful.

</details>


### [149] [Closed-Loop Control Law for Low Thrust Orbit Transfer with Guaranteed Stability](https://arxiv.org/abs/2511.23014)
*Suraj Kumar,Aditya Rallapalli,Nivriti Priyadarshini,Bharat Kumar GVP,Ravi Kumar L*

Main category: eess.SY

TL;DR: 提出一种基于李雅普诺夫方法的改进Q-law，以实现闭环稳定性和实时性，在多种轨道提升场景下验证有效性。


<details>
  <summary>Details</summary>
Motivation: Q-law在闭环稳定性方面存在问题，限制了在太空飞行器高效率电推进系统上的实时 onboard 应用；需要一种能够确保闭环稳定性且可在长时任务中实现的控制方法。

Method: 在经典Q-law基础上引入李雅普诺夫引导的修改，设计能确保闭环稳定性的控制律，以实现对轨道提升过程的实时控制。

Result: 对多种场景（同轨、赤道到极轨、GTO到GEO）进行了闭环轨道转移仿真/评估，显示所提方法具有稳定性保障与可实时实现性。

Conclusion: 通过将李雅普诺夫理论融入Q-law，解决了闭环稳定性问题，使Q-law在现实飞行器轨道提升任务中具备可行性和鲁棒性。

Abstract: Electric propulsion is used to maximize payload capacity in communication satellites. These orbit raising maneuvers span several months and hundreds of revolutions, making trajectory design a complex challenge. The literature typically addresses this problem using feedback laws, with Q-law being one of the most prominent approaches. However, Q-law suffers from closed-loop stability issues, limiting its suitability for real-time on-board implementation. In this work, we focus on closed-loop orbit raising rather than offline trajectory planning and address the stability limitations of the Q-law through a Lyapunov based control design. A Lyapunov-guided modification of the classical Q-law is proposed to ensure closed-loop stability and enable real-time implementation. The effectiveness of the proposed method is demonstrated through closed-loop orbit transfers across various scenarios, including co-planar transfers, equatorial to polar orbit transfers, and geostationary transfer orbit (GTO) to geostationary earth orbit (GEO) transfers.

</details>


### [150] [Control Barrier Function for Unknown Systems: An Approximation-free Approach](https://arxiv.org/abs/2511.23022)
*Shubham Sawarkar,Pushpak Jagtap*

Main category: eess.SY

TL;DR: 提出一种针对非线性未知动态在移动障碍物环境中的 prescribed-time reach-avoid (PT-RA) 控制框架。通过在一个简单的虚拟系统上求解 CBF-QP 以生成满足 PT-RA 条件的安全参考，并用无近似的反馈律将真实系统约束在围绕该参考的虚拟约束区域内，从而在未知动态和动态约束下实现实时安全性与规定时间内到达目标集。仿真表明在动态障碍物下可可靠避让并及时收敛。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒或学习型 CBF 方法需在线建模、不确定性界限估计或离线预计算，且在未知动态和动态障碍环境中实现真正的即时安全与时间约束仍具挑战性。为避免在线模型学习、界限估计与离线预计算，需一种能够在未知动力学下提供实时安全与规定时间收敛的控制框架。

Method: 在一个简单的虚拟系统上通过 CBF-Quadratic Program (CBF-QP) 生成一个安全参考，该参考对时间变化的、收紧后的障碍与目标集满足 PT-RA 条件。真实系统通过无近似的反馈律被限制在以该参考为中心的虚拟约束区(Virtual Confinement Zone, VCZ) 内。该构造确保在未知动力学与动态约束下实现实时安全与规定时间的目标收敛，无需模型识别或离线预计算。

Result: 理论上保证实时安全性与规定时间到达目标的可行性；在仿真中展示了在动态障碍物环境中可靠的避障行为及对目标的及时收敛。

Conclusion: 该工作提供了一种无需在线学习或不确定性界限估计的 PT-RA 框架，适用于未知非线性动力学与动态障碍环境，兼顾安全性与时限性并实现实时性，为智能控制中可验证的安全到达任务提供了实际可行的路径。

Abstract: We study the prescribed-time reach-avoid (PT-RA) control problem for nonlinear systems with unknown dynamics operating in environments with moving obstacles. Unlike robust or learning based Control Barrier Function (CBF) methods, the proposed framework requires neither online model learning nor uncertainty bound estimation. A CBF-based Quadratic Program (CBF-QP) is solved on a simple virtual system to generate a safe reference satisfying PT-RA conditions with respect to time-varying, tightened obstacle and goal sets. The true system is confined to a Virtual Confinement Zone (VCZ) around this reference using an approximation-free feedback law. This construction guarantees real-time safety and prescribed-time target reachability under unknown dynamics and dynamic constraints without explicit model identification or offline precomputation. Simulation results illustrate reliable dynamic obstacle avoidance and timely convergence to the target set.

</details>


### [151] [Development of a Load Profile Generator for Non-road Mobile Machinery](https://arxiv.org/abs/2511.23062)
*Serhiy Kapustyan,Pranav Tetey,Thomas Grube,Jochen Linssen*

Main category: eess.SY

TL;DR: 一个面向非公路移动机械的自下而上的负荷曲线生成器，用于描述真实世界的常见操作轮廓，能为仿真提供参数化的功率需求，并在材料处理机与林业前进机等场景中验证对比现实数据，帮助工程师评估性能与探索替代能源概念。


<details>
  <summary>Details</summary>
Motivation: 需要捕捉非公路移动机械在现实工况中的操作特征，以便进行能量需求评估、系统仿真与优化，并为制定运行策略和能源改进提供基础。

Method: 提出一个可参数化的自下而上负荷曲线生成模型，覆盖不同类别的NRMM，并对特定作业如材料处理机与林业前进机进行实例演示，基于实际数据对结果进行对比验证，且模型可作为能量需求计算的接口来扩展系统分析。

Result: 模型能够描绘大范围的NRMM负荷曲线的常见操作，并通过与真实数据的验证获得足够的准确性，能在功率接口处观测到发动机/执行器的需求，且可用于评估替代燃料与供能方案以优化系统仿真。

Conclusion: 该负荷曲线生成器为系统级分析提供了一个有效工具，能够支持制定操作策略、探索能源供给改造，并对不同NRMM类别具有普适性，未来可扩展为能量需求计算的接口。

Abstract: This research presents a Load Profile Generator model for non-road mobile machinery, which depicts the most common operational profiles that reflect real-world conditions. This technological bottom-up model enables users to parameterize specific machines for simulation and observe their power demand at the actuator interfaces. The application of the Load Profile Generator covers different non-road mobile machinery categories, such as construction, agriculture, industrial and forestry. In this study, the Load Profile Generator was used to demonstrate common operations of material handler and forest forwarder, which have been validated against real-world data to match the results. The usage of Load Profile Generator aids engineers in evaluation machines performance by developing operation strategies. It opens doors to further systems analysis as it can serve as an interface for energy demand calculations. The model's results are accurate enough to provide a sufficient understanding of a wide range of non-road mobile machinery load profiles as well as insights about alternative fuel and power supply concepts helping in optimising the system simulation.

</details>


### [152] [Targeted-Subharmonic-Eliminating Pulse Density Modulation for Wireless Power Transfer Systems](https://arxiv.org/abs/2511.23138)
*Songyan Li,Hongchang Li,Haiyue Jiang,Yudong Zhang,Wenjie Chen,Xu Yang*

Main category: eess.SY

TL;DR: 提出一种针对性抑制次谐波的PDM控制方法，用于SS补偿的WPT系统。通过设计NTF去除驱动端的次谐波分量，从而抑制异常电流振荡，在一次/二次侧实现简单且对耦合参数误差有韧性的稳健性提升，且通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: PDM控制的WPT系统易受次谐波激励，导致电流异常振荡和幅值波动；在SS补偿结构中，需要一种高效、易实现且对耦合系数标识误差具鲁棒性的抑制方法。

Method: 提出一种目标性子谐波消除的PDM方法，利用专门设计的噪声传递函数NTF来抑制触发次谐波的分量。NTF针对特定子谐波进行定向抑制，并可在原边和负载侧实现，且对耦合系数误差具有一定容忍性。实验使用硬件实现，验证在PDM控制下抑制异常电流振荡并降低电流幅值波动。

Result: 实验结果表明该方法在抑制次谐波驱动下的异常电流振荡方面有效且鲁棒，显著降低电流幅值的波动。

Conclusion: 该方法实现简单且可双边应用，对PDM控WPT系统的稳定性提升具有实用性和鲁棒性，尤其在耦合系数识别误差存在时仍保持良好性能。

Abstract: This letter proposes a targeted-subharmonic-eliminating pulse density modulation (PDM) method for series-series (SS) compensated wireless power transfer (WPT) systems. The subharmonic frequency components which excite current abnormal oscillations in PDM controlled WPT systems are eliminated through a specially designed noise transfer function (NTF). The proposed method is simple to implement in both primary and secondary sides of WPT systems and exhibits a certain tolerance to deviations caused by inaccurate coupling coefficient identification in NTF design. Experimental results demonstrated the effectiveness and robustness of the proposed method in suppressing current abnormal oscillations and reducing the fluctuations in current amplitudes.

</details>


### [153] [Data-driven Reachability Verification with Probabilistic Guarantees under Koopman Spectral Uncertainty](https://arxiv.org/abs/2511.23322)
*Jianqiang Ding,Shankar A. Deka*

Main category: eess.SY

TL;DR: 利用Koopman算子理论的一个数据驱动框架，以把未知复杂系统的不确定性编码到Koopman谱表示中，并给出有误差界的概率性可达性保证，避免直接求解可达集，并通过案例验证可行性。


<details>
  <summary>Details</summary>
Motivation: 在未知/复杂系统中给出严格的可达性保证具有重要性与挑战性；传统方法面临可扩展性和不确定性处理难题；需要在不直接求解可达集的前提下实现可证的时间区间目标达成性。

Method: 从有限数据出发，将模型不确定性编码进Koopman谱表示，给出可量化的误差边界；利用谱信息确定从初始集合出发的轨迹在给定概率下到达目标集合的时间区间；实现无显式可达集计算的严格可达性验证。

Result: 通过对代表性动力系统的案例研究验证框架的有效性，展示在可扩展性和适用性方面的优势。

Conclusion: 给出了一种可扩展的、基于数据和Koopman谱的严格可达性验证框架，能在未知系统下实现带概率保证的时间区间可达性，并有潜在广泛应用。

Abstract: Providing rigorous reachability guarantees for unknown complex systems is a crucial and challenging task. In this paper, we present a novel data-driven framework that addresses this challenge by leveraging Koopman operator theory. Instead of operating in the state space, the proposed method encodes model uncertainty from finite data directly into Koopman spectral representation with quantifiable error bounds. Leveraging this spectral information, we systematically determine time intervals within which trajectories from the initial set are guaranteed, with a prescribed probability, to reach the target set. This enables the rigorous reachability verification without explicit computation of reachable sets, thereby offering a significant advantage in scalability and applicability. We finally validate the effectiveness of the proposed framework through case studies on representative dynamical systems.

</details>


### [154] [Dynamic Power Allocation For NOMA-Based Transmission in 6G Optical Wireless Networks](https://arxiv.org/abs/2511.23326)
*Ahmad Adnan Qidan,Taisir El-Gorashi,Majid Safari,Harald Haas,Richard V. Penty,Ian H. White,Jaafar M. H. Elmirghani*

Main category: eess.SY

TL;DR: 提出基于NOMA的激光室内无线通信系统，利用外部BIA前向编码在多AP协同下提升光学无线通信的频谱效率，通过RF辅助动态分组和动态功率分配解决 max-min 分式优化难题，达到较高的和/公平性与能效。


<details>
  <summary>Details</summary>
Motivation: 在室内环境中以红外激光为传输介质，需在眼部安全与高调制速度之间取得权衡；通过NOMA实现多用户接入并提高光通信的频谱利用率，同时解决跨AP协同与资源分配带来的复杂优化问题。

Method: 提出以NOMA为核心的多用户接入方案，使用BIA外部前向编码实现多AP协同传输并为NOMA组确定预编码矩阵。将目标设定为在给定联合约束下最大化系统和速率，结合两种动态算法：1) RF辅助的动态分组，用户通过RF通道交换二值变量以构建基于距离的加权边作为分组指标；2) 动态功率分配，在保证组内用户的流量需求的前提下，确定各组的最优功率分配。该问题本质为 max-min 分式规划，较难直接求解，因此提出动态近似实现。

Result: 证明所提动态算法在收敛性方面能够收敛到接近最优解，并在和速率、公平性与能效方面优于对比方案。

Conclusion: 本工作将NOMA、BIA前向编码与RF辅助分组结合用于室内激光ODC/OWC系统，提升了系统吞吐与公平性，同时兼顾能效；未来工作可关注算法的收敛速度、实现复杂度及对现实RF/室内环境不确定性的鲁棒性。

Abstract: OWC has been considered as a key enabling technology to unlock unprecedented speeds of communication, supporting high demands of data traffic. In this paper, infrared lasers are used as optical transmitters operating in an indoor environment under eye safety regulations due to their high modulation speed. To provide efficient multiple access service, NOMA-based transmission is implemented to multiplex messages intended to multiple users in the power domain and maximize the spectral efficiency of our laser-based OWC network. In particular, a BIA outer precoder is designed to coordinate the transmission among multiple APs and determine the precoding matrices for groups of users potential formed according to NOMA principles. For effective use of NOMA, an optimization problem is formulated to maximize the sum rate of the network through forming optimum groups under certain joint conditions, efficient power allocation, high quality of service for each weak and strong users, and high overall system performance. Such optimization problems are defined as max-min fractional programs difficult to solve in practice. Therefore, a dynamic application for NOMA is introduced using two algorithms. First, a RF-aided dynamic algorithm is designed to form multiple groups, where users exchange binary variables among them through an RF system to establish distance-based weight edges, which are used as a metric for the grouping process. Second, a dynamic power allocation is proposed to determine the optimum power allocated to each group, while the users belonging to a certain group receive their traffic demands regardless of their classification as weak or strong. The results show the convergence of the proposed dynamic application to the optimum solution, and its high performance in terms of sum rate, fairness, and energy efficiency compared to counterpart schemes.

</details>


### [155] [Strong nonlinear detectability and moving horizon estimation for nonlinear systems with unknown inputs](https://arxiv.org/abs/2511.23385)
*Yang Guo,Jaime A. Moreno,Stefan Streif*

Main category: eess.SY

TL;DR: Defines strong nonlinear detectability as a necessary and sufficient condition for the existence of unknown-input state estimators (UISEs) and develops two MHE-based UISE schemes (full-order and two-stage) for nonlinear discrete-time systems with measurement noise and possibly unbounded unknown inputs; simulations show advantages over conventional MHE.


<details>
  <summary>Details</summary>
Motivation: To enable reliable state estimation in nonlinear discrete-time systems confronted with measurement noise and unknown inputs (which may be unbounded), by establishing a detectability criterion that guarantees bounded estimation error and guiding UISE design.

Method: Introduce the concept of strong nonlinear detectability; design a UISE via moving horizon estimation using a full-order model and sequential measurements; refine the detectability notion to formulate a two-stage MHE-based UISE to improve computational efficiency.

Result: The proposed UISEs achieve bounded estimation error in the presence of unbounded unknown inputs; the two-stage MHE-based UISE is computationally more efficient than the full-order MHE-based UISE. Simulations on a plant growth process compare the two variants with conventional MHE, demonstrating merits of the proposed methods.

Conclusion: Strong nonlinear detectability serves as a necessary and sufficient condition for UISE existence, and MHE-based UISEs (including a two-stage variant) provide effective state estimation under unknown inputs, with the two-stage design offering practical computational advantages.

Abstract: This paper considers state estimation for general nonlinear discrete-time systems subject to measurement noise and possibly unbounded unknown inputs. To approach this problem, we first propose the concept of strong nonlinear detectability. This condition is sufficient and necessary for the existence of unknown input state estimators (UISEs), which reconstruct states from noisy sampled measurements and yield bounded estimation error even for unbounded unknown inputs. Based on the proposed detectability notion, a UISE is designed via a moving horizon estimation strategy using a full-order model as well as past and current measurements. Next, we tighten this detectability notion to design a two-stage MHE-based UISE, which is computationally more efficient than the MHE-based UISE using full-order models. In a simulation example with a plant growth process, both variants of MHE-based UISEs are compared with a conventional MHE to illustrate the merits of the developed methods.

</details>


### [156] [A Lyapunov-Based Small-Gain Theorem for Fixed-Time Stability](https://arxiv.org/abs/2511.23474)
*Michael Tang,Miroslav Krstic,Jorge Poveda*

Main category: eess.SY

TL;DR: 提出一种基于Lyapunov的小增益框架，用于在互联系统中建立固定时间稳定性（FxTS）。若各子系统存在证明FxT-ISS的Lyapunov函数且满足非线性小增益条件，则整体系统也为FxTS。方法类比于已有对渐近与有限时间稳定性的Lyapunov小增益定理，填补互联系统FxTS分析的空白，并通过解析与数值示例（包括在不存在尺度分离的情况下的固定时间反馈优化）进行验证。


<details>
  <summary>Details</summary>
Motivation: 目前对互联系统的固定时间稳定性分析不足，尤其是缺乏以Lyapunov小增益框架来证实整体FxTS的通用条件。给出了在子系统个体FxT-ISS条件和非线性小增益条件下的整体FxTS保障，为设计在固定时间内收敛的控制策略提供理论基础。

Method: 对每个子系统给出一个证明FxT-ISS的Lyapunov函数，并构造整体系统的Lyapunov衍生量；建立非线性小增益条件，确保子系统之间的耦合不会破坏固定时间收敛性；给出与渐近/有限时间小增益定理相类比的证明框架，并推导出相应的FxTS保号定理。通过解析和数值示例以及一个不存在时间尺度分离的固定时间反馈优化案例来示范方法的应用。

Result: 给出一组FxTS的互联系统稳定性定理：若各子系统存在FxT-ISS Lyapunov函数且满足所述非线性小增益条件，则整体系统为FxTS；提供相应的证明要点与条件，以及可操作的设计流程。数值与分析示例验证理论，且应用于不需要时间尺度分离的固定时间反馈优化问题。

Conclusion: 该工作填补了FxTS在互联系统中的分析空白，扩展了Lyapunov小增益框架到固定时间范畴，提供了可用于设计稳定且在固定时间内收敛的互联系统的工具和思路，且适用于控制/优化领域中不需要时间尺度分离的情形。

Abstract: This paper introduces a novel Lyapunov-based small-gain methodology for establishing fixed-time stability (FxTS) guarantees in interconnected dynamical systems. Specifically, we consider interconnections in which each subsystem admits an individual fixed-time input-to-state stability (ISS) Lyapunov function that certifies FxT-ISS. We then show that if a nonlinear small-gain condition is satisfied, then the entire interconnected system is FxTS. Our results are analogous to existing Lyapunov-based small-gain theorems developed for asymptotic and finite-time stability, thereby filling an important gap in the stability analysis of interconnected dynamical systems. The proposed theoretical tools are further illustrated through analytical and numerical examples, including an application to fixed-time feedback optimization of dynamical systems without time-scale separation between the plant and the controller.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [157] [A Longitudinal Measurement of Privacy Policy Evolution for Large Language Models](https://arxiv.org/abs/2511.21758)
*Zhen Tao,Shidong Pan,Zhenchang Xing,Emily Black,Talia Gillis,Chunyang Chen*

Main category: cs.CR

TL;DR: 首个对主流LLM提供商隐私政策的纵向实证研究，揭示政策长度、可读性、模糊性，以及区域覆盖差异，并分析随时间的编辑驱动因素与生态事件的对应关系。


<details>
  <summary>Details</summary>
Motivation: 在LLM服务广泛渗透日常生活并大量收集数据以致隐私担忧的背景下，隐私政策作为信息隐私框架的基础工具，其在LLM场景中的呈现、演变及区域差异尚未被系统研究。本研究填补对LLM隐私政策及其演变的空白。

Method: 构建包含74份历史隐私政策与115份补充隐私文件的纵向数据集，覆盖11家LLM提供商、5个国家，时间范围至2025年8月；提取并对比相邻版本之间超过3000条句级编辑；开发面向LLM隐私政策的分类体系，并对政策编辑进行注释，结合关键生态事件构建时间线。

Result: 隐私政策普遍更长、阅读水平接近大学水平且仍然较为模糊；基于分类分析，披露LLM特定做法呈现分布性规律，区域覆盖存在差异；策略编辑集中于一方数据收集与国际/特定受众等章节；产品发布与监管行动是驱动编辑的主要动力；为理解LLM隐私政策的现状与演变提供了基线与趋势洞察。

Conclusion: 研究揭示了LLM隐私政策的现状与演化规律，强调内容长度、可读性与模糊性的持续性问题，以及区域覆盖差异与驱动因素的存在，为未来政策标准化、透明度提升和跨区域比较提供基础。

Abstract: Large language model (LLM) services have been rapidly integrated into people's daily lives as chatbots and agentic systems. They are nourished by collecting rich streams of data, raising privacy concerns around excessive collection of sensitive personal information. Privacy policies are the fundamental mechanism for informing users about data practices in modern information privacy paradigm. Although traditional web and mobile policies are well studied, the privacy policies of LLM providers, their LLM-specific content, and their evolution over time remain largely underexplored. In this paper, we present the first longitudinal empirical study of privacy policies for mainstream LLM providers worldwide. We curate a chronological dataset of 74 historical privacy policies and 115 supplemental privacy documents from 11 LLM providers across 5 countries up to August 2025, and extract over 3,000 sentence-level edits between consecutive policy versions. We compare LLM privacy policies to those of other software formats, propose a taxonomy tailored to LLM privacy policies, annotate policy edits and align them with a timeline of key LLM ecosystem events. Results show they are substantially longer, demand college-level reading ability, and remain highly vague. Our taxonomy analysis reveals patterns in how providers disclose LLM-specific practices and highlights regional disparities in coverage. Policy edits are concentrated in first-party data collection and international/specific-audience sections, and that product releases and regulatory actions are the primary drivers, shedding light on the status quo and the evolution of LLM privacy policies.

</details>


### [158] [Advanced Data Collection Techniques in Cloud Security: A Multi-Modal Deep Learning Autoencoder Approach](https://arxiv.org/abs/2511.21795)
*Aamiruddin Syed,Mohammed Ilyas Ahmad*

Main category: cs.CR

TL;DR: 提出了一个多模态深度学习集成架构 MMDLEA，用于云安全中的异常检测与分类，通过融合六个模型的输出实现对多模态数据的鲁棒识别，整体表现优于单模型。


<details>
  <summary>Details</summary>
Motivation: 云环境中存在大量异构数据源与模态，威胁日益复杂，需高效且稳健的跨模态异常检测与分类方法。

Method: 设计并实现 MMDLEA，将六个深度学习模型的最佳特征进行多模态自编码器/集成，以不同数据模态训练各自子模型并对输出进行融合预测，模型包括 MMDLA、ADAM、ADADELTA、ADAGRAD、RMSPROP、SGT。

Result: 在测试数据集上，MMDLA 取得 98.5% 准确率和 0.985 F1；ADAM 为 96.2%/0.962，ADADELTA 为 95.5%/0.955，MMDLA（单独）为 94.8%/0.948。整体 MMDLEA 展现对模态波动与噪声的鲁棒性，且优于各单模态模型。

Conclusion: 所提出的多模态集成框架在多模态数据的异常检测与分类方面具有潜力，结果支持未来在更广泛的数据源和场景中应用与优化。

Abstract: Cloud security is an important concern. To identify and stop cyber threats, efficient data collection methods are necessary. This research presents an innovative method to cloud security by integrating numerous data sources and modalities with multi-modal deep learning autoencoders. The Multi-Modal Deep Learning Ensemble Architecture (MMDLEA), a unique approach for anomaly detection and classification in multi-modal data, is proposed in this study. The proposed design integrates the best features of six deep learning models: Multi-Modal Deep Learning Autoencoder (MMDLA), Anomaly Detection using Adaptive Metric Learning (ADAM), ADADELTA, ADAGRAD, RMSPROP, and Stacked Graph Transformer (SGT). A final prediction is produced by combining the outputs of all the models, each of which is trained using a distinct modality of the data. Based on the test dataset, the recommended MMDLA architecture achieves an accuracy of 98.5% and an F1-score of 0.985, demonstrating its superior performance over each individual model. Of the different models, the ADAM model performs the best, with an accuracy of 96.2% and an F1-score of 0.962. With an F1-score of 0.955 and an accuracy of 95.5%, the ADADELTA model trails closely behind. MMDLA obtains an F1-score of 0.948 and an accuracy of 94.8%. Additionally, the suggested MMDLEA design exhibits enhanced resilience to fluctuating modalities and noisy data, proving its usefulness in practical settings. Future study in this area is made possible by the results, which show the potential of the proposed framework for abnormal identification and categorization in multi-modal data.

</details>


### [159] [Cross-Layer Detection of Wireless Misbehavior Using 5G RAN Telemetry and Operational Metadata](https://arxiv.org/abs/2511.21803)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.CR

TL;DR: Cross-layer coherence of PHY, MAC, and configuration telemetry detects uplink misbehavior in 5G Standalone where control-plane signaling remains compliant.


<details>
  <summary>Details</summary>
Motivation: Uplink misbehavior (power inflation, timing drift, off-grant bursts) can occur without signaling disruptions, weakening the reliability of single-telemetry-based detection. A cross-layer approach could reveal inconsistencies using existing gNB telemetry.

Method: Experiment with a controlled 5G SA testbed using commercial UEs and a software-defined radio adversary. Assess how each manipulation affects coherence among physical layer measurements, MAC scheduling decisions, and configuration metadata. Analyze multi-layer time series and cross-domain views (e.g., SNR vs CQI) to identify distinctive, reproducible signatures.

Result: Each manipulation produces a distinct, reproducible signature not detectable from any single telemetry source. Power offsets weaken the SNR-CQI link; timing drift disrupts scheduler alignment; off-grant bursts create uplink energy incongruent with allocation logs. These inconsistencies appear in merged multi-layer traces and cross-domain views, enabling detection using only standard gNB telemetry.

Conclusion: Cross-layer coherence provides a practical, non-intrusive signal for detecting uplink misbehavior, suitable for integration into existing operational monitoring and auditing systems without protocol modifications.

Abstract: 5G Standalone deployments can exhibit uplink misbehavior from user equipment that remains fully compliant with standard control plane procedures. Manipulations such as transmit power inflation, gradual timing drift, and short off grant bursts leave the signaling state intact but distort the expected relationships among the telemetry streams produced by the gNB. This work examines whether these cross layer relationships can serve as a reliable basis for identifying such misbehavior without introducing new signaling. Using a controlled 5G Standalone testbed with commercial user equipment and a software defined radio adversarial device, we study how each manipulation affects the coherence among physical layer measurements, MAC scheduling decisions, and configuration metadata. The results show that every manipulation produces a distinct and reproducible signature that is not visible from any single telemetry source. Power offsets weaken the natural connection between SNR and CQI, timing drift breaks the alignment maintained by the scheduler, and off grant activity produces uplink energy that does not agree with allocation logs. These inconsistencies appear in merged multi layer time series traces and in cross domain views such as the SNR to CQI plane. The findings indicate that cross layer coherence provides a practical signal for detecting uplink misbehavior using only standard gNB telemetry, with no protocol modifications required, which makes the method suitable for integration into operational monitoring and auditing systems.

</details>


### [160] [Beyond Membership: Limitations of Add/Remove Adjacency in Differential Privacy](https://arxiv.org/abs/2511.21804)
*Gauri Pradhan,Joonas Jälkö,Santiago Zanella-Bèguelin,Antti Honkela*

Main category: cs.CR

TL;DR: 在差分隐私中，使用 add/remove 邻接关系可能高估了对单记录属性的保护，替代邻接关系才揭示真实风险；通过提出新的审计攻击并实证比较，发现副方邻接的预算更符合实际保护水平。


<details>
  <summary>Details</summary>
Motivation: 在许多 ML 应用中，目标是保护单个记录的属性（如标签），而非成员身份；现有 DP 以 add/remove 邻接作为默认，导致属性隐私的估计可能偏离实际保护效果，因此需要分析不同邻接关系对隐私保证的影响。

Method: 区分 substitute 邻接（替换一个样本）与 add/remove 邻接，提出针对 substitute 邻接的审计攻击，设计实验评估两种邻接下的 DP 预算和实际可被推断的信息量。

Result: 在 substitute 邻接下进行的审计揭示的隐私泄露程度与 substitute 预算相符，但与 add/remove 给出的隐私保障不一致，后者因此被高估。

Conclusion: 邻接关系的选择对 per-record 属性保护至关重要；应在报告 DP 保证时明确对准的保护对象并相应调整隐私预算的审计方法。

Abstract: Training machine learning models with differential privacy (DP) limits an adversary's ability to infer sensitive information about the training data. It can be interpreted as a bound on adversary's capability to distinguish two adjacent datasets according to chosen adjacency relation. In practice, most DP implementations use the add/remove adjacency relation, where two datasets are adjacent if one can be obtained from the other by adding or removing a single record, thereby protecting membership. In many ML applications, however, the goal is to protect attributes of individual records (e.g., labels used in supervised fine-tuning). We show that privacy accounting under add/remove overstates attribute privacy compared to accounting under the substitute adjacency relation, which permits substituting one record. To demonstrate this gap, we develop novel attacks to audit DP under substitute adjacency, and show empirically that audit results are inconsistent with DP guarantees reported under add/remove, yet remain consistent with the budget accounted under the substitute adjacency relation. Our results highlight that the choice of adjacency when reporting DP guarantees is critical when the protection target is per-record attributes rather than membership.

</details>


### [161] [Standardized Threat Taxonomy for AI Security, Governance, and Regulatory Compliance](https://arxiv.org/abs/2511.21901)
*Hernan Huwyler*

Main category: cs.CR

TL;DR: 提出AI系统威胁向量分类法（AI System Threat Vector Taxonomy，ASTVT）用于定量风险评估，将技术向量直接映射到财务损失，以填补技术团队和合规团队之间的语言鸿沟；并与ISO/IEC 42001、NIST AI RMF对齐。通过对2025年133起AI事件的分析实现100%覆盖，并验证对主流风险框架的一致性。


<details>
  <summary>Details</summary>
Motivation: 在监管领域对AI风险评估的标准化不足，技术漏洞与合规/法律责任之间存在断层，难以将技术风险转化为可计量的损失和保险暴露。

Method: 构建九大域（Misuse、Poisoning、Privacy、Adversarial、Biases、Unreliable Outputs、Drift、Supply Chain、IP Threat）及53个子威胁，域与三类业务损失（Confidentiality、Integrity、Availability、Legal、Reputation）直接映射；对2025年133起AI事件进行实证分析以验证分类覆盖率，并与主要AI风险框架对齐，explicitly aligns with ISO/IEC 42001 控制和 NIST AI RMF 功能。

Result: 实现9大域、53个子威胁，对133起事件实现100%分类覆盖，能够将技术向量翻译为财务损失类别，确保对齐主要框架和标准。

Conclusion: 该分类法为定量风险评估提供一个将技术指针映射到经济影响的统一语言，便于审计与监管合规，提升合规与保险决策的可操作性。

Abstract: The accelerating deployment of artificial intelligence systems across regulated sectors has exposed critical fragmentation in risk assessment methodologies. A significant "language barrier" currently separates technical security teams, who focus on algorithmic vulnerabilities (e.g., MITRE ATLAS), from legal and compliance professionals, who address regulatory mandates (e.g., EU AI Act, NIST AI RMF). This disciplinary disconnect prevents the accurate translation of technical vulnerabilities into financial liability, leaving practitioners unable to answer fundamental economic questions regarding contingency reserves, control return-on-investment, and insurance exposure. To bridge this gap, this research presents the AI System Threat Vector Taxonomy, a structured ontology designed explicitly for Quantitative Risk Assessment (QRA). The framework categorizes AI-specific risks into nine critical domains: Misuse, Poisoning, Privacy, Adversarial, Biases, Unreliable Outputs, Drift, Supply Chain, and IP Threat, integrating 53 operationally defined sub-threats. Uniquely, each domain maps technical vectors directly to business loss categories (Confidentiality, Integrity, Availability, Legal, Reputation), enabling the translation of abstract threats into measurable financial impact. The taxonomy is empirically validated through an analysis of 133 documented AI incidents from 2025 (achieving 100% classification coverage) and reconciled against the main AI risk frameworks. Furthermore, it is explicitly aligned with ISO/IEC 42001 controls and NIST AI RMF functions to facilitate auditability.

</details>


### [162] [GECKO: Securing Digital Assets Through(out) the Physical World (Extended Technical Report)](https://arxiv.org/abs/2511.21999)
*Cyrill Krähenbühl,Nico Hauser,Christelle Gloor,Juan Angel García-Pardo,Adrian Perrig*

Main category: cs.CR

TL;DR: 提出 GECKO，一种地理位置感知的公钥基础设施，用于将数字资产与物理空间关联，支持基于位置的信任验证，具有高性能查询能力。


<details>
  <summary>Details</summary>
Motivation: 解决数字资产与实体世界之间缺乏一致的检索与验证机制，防止伪造商店、财产欺诈和移动支付诈骗；利用已有契约与地籍等信息，但缺乏统一的获取与验证方式。

Method: 提出 Geo-Enabled Cryptographic Key Oracle (GECKO)，一个地理 PKI，基于位置和占据空间为数字资产提供全局视图；实现数字与物理世界的双向信任转换，支持验证某位置应有的资产以及某数字实体所声称的物理空间；可在现有 PKI 基础上使用。

Result: 在单一服务器上，证明可高效存储百万级资产，并基于精确位置查询提供加密材料，响应时间约11毫秒，吞吐超过每秒19000 次查询。

Conclusion: GECKO 为数字与物理世界之间的信任提供地理感知能力，可作为现有 PKI 的补充，在需要时提供全球范围的可验证视图与快速查询能力。

Abstract: Although our lives are increasingly transitioning into the digital world, many digital assets still relate to objects or places in the physical world, e.g., websites of stores or restaurants, digital documents claiming property ownership, or digital identifiers encoded in QR codes for mobile payments in shops. Currently, users cannot securely associate digital assets with their related physical space, leading to problems such as fake brand stores, property fraud, and mobile payment scams. In many cases, the necessary information to protect digital assets exists, e.g., via contractual relationships and cadaster entries, but there is currently no uniform way of retrieving and verifying these documents. In this work, we propose the Geo-Enabled Cryptographic Key Oracle (GECKO), a geographical PKI that provides a global view of digital assets based on their geo-location and occupied space. GECKO allows for the bidirectional translation of trust between the physical and digital world. Users can verify which assets are supposed to exist at their location, as well as verify which physical space is claimed by a digital entity. GECKO supplements current PKI systems and can be used in addition to current systems when its properties are of value. We show the feasibility of efficiently storing millions of assets and serving cryptographic material based on precise location queries within 11 ms at a rate of more than 19000 queries per second on a single server.

</details>


### [163] [POLARIS: Cross-Domain Access Control via Verifiable Identity and Policy-Based Authorization](https://arxiv.org/abs/2511.22017)
*Aiyao Zhang,Xiaodong Lee,Zhixian Zhuang,Jiuqi Wei,Yufan Fu,Botao Peng*

Main category: cs.CR

TL;DR: 提出POLARIS，一种跨域、隐私保护的策略驱动访问控制架构，辅以VPPL语言及会话级安全机制，实现可验证、可扩展、可互操作的访问控制。


<details>
  <summary>Details</summary>
Motivation: 传统访问控制在跨域环境中面临身份分散、隐私泄露和权限需求多样化等挑战，需实现对身份和资源的自治控制，提供隐私保护的认证和灵活的授权。

Method: 提出POLARIS体系结构；引入结构化承诺机制以实现对身份披露的可验证、细粒度的策略驱动控制；提出VPPL轻量化策略语言，支持发行者绑定的属性选择性披露评估；引入会话级安全机制以绑定认证和访问，并增强保密性与防重放；实现原型并进行实验。

Result: 实验结果表明POLARIS在异构域间提供可扩展、隐私保护且互操作的访问控制，具有实际可行性。

Conclusion: POLARIS为跨域环境提供一个统一、可扩展且可验证的隐私保护访问控制框架，支持策略化、隐私保护的跨域身份与资源访问，在去中心化环境中具有有效性。

Abstract: Access control is a security mechanism designed to ensure that only authorized users can access specific resources. Cross-domain access control involves access to resources across different organizations, institutions, or applications. Traditional access control, however, which handles authentication and authorization separately in centralized environments, faces challenges in identity dispersion, privacy leakage, and diversified permission requirements, failing to adapt to cross-domain scenarios. Thus, there is an urgent need for a new access control mechanism that empowers autonomous control over user identity and resources, addressing the demands for privacy-preserving authentication and flexible authorization in cross-domain scenarios. To address cross-domain access control challenges, we propose POLARIS, a unified and extensible architecture that enables policy-based, verifiable and privacy-preserving access control across different domains. POLARIS features a structured commitment mechanism for reliable, fine-grained, policy-based identity disclosure. It further introduces VPPL, a lightweight policy language that supports issuer-bound evaluation of selectively revealed attributes. A dedicated session-level security mechanism ensures binding between authentication and access, enhancing confidentiality and resilience to replay attacks. We implement a working prototype and conduct comprehensive experiments, demonstrating that POLARIS effectively provides scalable, privacy-preserving, and interoperable access control across heterogeneous domains. Our results highlight the practical viability of POLARIS for enabling secure and privacy-preserving access control in decentralized, cross-domain environments.

</details>


### [164] [Evaluating the Robustness of Large Language Model Safety Guardrails Against Adversarial Attacks](https://arxiv.org/abs/2511.22047)
*Richard J. Young*

Main category: cs.CR

TL;DR: 公开基准与新颖攻击下的鲁棒性比总体准确性更关键，且基准可能被训练数据污染。


<details>
  <summary>Details</summary>
Motivation: 评估十家公开护栏模型在对抗性攻击中的鲁棒性与泛化能力，以揭示现实世界安全性与评估偏差。

Method: 对来自 Meta、Google、IBM、NVIDIA、Alibaba、Allen AI 的十个护栏模型进行评估，总共 1,445 条测试提示，覆盖 21 种攻击类别。比较公开基准提示与新颖攻击的表现，并检测“有用模式”越狱等现象。

Result: Qwen3Guard-8B 的总体现象最高，准确率 85.3%（95%CI: 83.4–87.1%）。但在未见提示上性能急剧下降：从 91.0% 跌至 33.8%，下降 57.2 个百分点。Granite-Guardian-3.2-5B 展现最佳泛化能力，未见/新颖攻击的表现差距仅 6.5%。还发现两模型（Nemotron-Safety-8B、Granite-Guardian-3.2-5B）在“有用模式”越狱中生成有害内容，构成新的失败模式。

Conclusion: 基准性能可能受训练数据污染影响，泛化能力才是护栏评估的核心指标；需要改进评估框架，避免仅依赖整体准确率来衡量安全性。

Abstract: Large Language Model (LLM) safety guardrail models have emerged as a primary defense mechanism against harmful content generation, yet their robustness against sophisticated adversarial attacks remains poorly characterized. This study evaluated ten publicly available guardrail models from Meta, Google, IBM, NVIDIA, Alibaba, and Allen AI across 1,445 test prompts spanning 21 attack categories. While Qwen3Guard-8B achieved the highest overall accuracy (85.3%, 95% CI: 83.4-87.1%), a critical finding emerged when separating public benchmark prompts from novel attacks: all models showed substantial performance degradation on unseen prompts, with Qwen3Guard dropping from 91.0% to 33.8% (a 57.2 percentage point gap). In contrast, Granite-Guardian-3.2-5B showed the best generalization with only a 6.5% gap. A "helpful mode" jailbreak was also discovered where two guardrail models (Nemotron-Safety-8B, Granite-Guardian-3.2-5B) generated harmful content instead of blocking it, representing a novel failure mode. These findings suggest that benchmark performance may be misleading due to training data contamination, and that generalization ability, not overall accuracy, should be the primary metric for guardrail evaluation.

</details>


### [165] [Privacy-preserving formal concept analysis: A homomorphic encryption-based concept construction](https://arxiv.org/abs/2511.22117)
*Qiangqiang Chen,Yunfeng Ke,Shen Li,Jinhai Li*

Main category: cs.CR

TL;DR: 提出一种隐私保护的形式概念分析PFCA框架，结合二值数据表示和同态加密，在大规模数据上实现安全的概念构造；实验和安全性分析表明在保护隐私的同时保持计算性能。


<details>
  <summary>Details</summary>
Motivation: FCA在知识提取、认知概念学习和数据挖掘中广泛应用，但大规模数据的计算负载常需外包到云或外部服务，存在敏感信息泄露的风险，因此需要隐私保护的FCA计算方法。

Method: 提出PFCA框架，将二值数据表示与同态加密相结合，允许在加密域完成概念构造等FCA计算，从而在不暴露私有数据的前提下实现安全、高效的概念发现。

Result: 通过实验和安全分析证明，该方法在保护隐私的同时维持了良好的计算性能，证实其在大规模FCA应用中的有效性。

Conclusion: PFCA为大规模FCA中的隐私保护数据挖掘和安全知识发现提供了一条可行途径，具有在实际应用中降低敏感信息风险的潜在影响。

Abstract: Formal Concept Analysis (FCA) is extensively used in knowledge extraction, cognitive concept learning, and data mining. However, its computational demands on large-scale datasets often require outsourcing to external computing services, raising concerns about the leakage of sensitive information. To address this challenge, we propose a novel approach to enhance data security and privacy in FCA-based computations. Specifically, we introduce a Privacy-preserving Formal Context Analysis (PFCA) framework that combines binary data representation with homomorphic encryption techniques. This method enables secure and efficient concept construction without revealing private data. Experimental results and security analysis confirm the effectiveness of our approach in preserving privacy while maintaining computational performance. These findings have important implications for privacy-preserving data mining and secure knowledge discovery in large-scale FCA applications.

</details>


### [166] [Personalized 3D Spatiotemporal Trajectory Privacy Protection with Differential and Distortion Geo-Perturbation](https://arxiv.org/abs/2511.22180)
*Minghui Min,Yulu Li,Gang Li,Meng Li,Hongliang Zhang,Miao Pan,Dusit Niyato,Zhu Han*

Main category: cs.CR

TL;DR: 提出了一种面向3D时空轨迹隐私保护的个性化机制3DSTPM，结合3D-GI与失真隐私，通过保护位置集、窗口自适应隐私预算分配与PF扰动，在保障QoS的同时满足个性化隐私需求。


<details>
  <summary>Details</summary>
Motivation: 随着基于位置的服务在智能城市和交通领域的快速发展，3D轨迹的时空相关性及高度信息带来潜在隐私风险；现有研究难以同时抵御利用时空相关性和高度信息的攻击并实现个性化隐私保护。

Method: （1）建立攻击者对轨迹中地点之间时空相关性的攻击模型；（2）结合3D-GI与失真隐私特性寻找覆盖真实位置的保护位置集（PLS）；（3）提出基于滑窗的自适应隐私预算分配(W-APBA)，根据可预测性和敏感性动态给当前PLS中的各位置分配预算；（4）通过PF（置换-翻转）机制在分配的隐私预算下对真实位置进行扰动，以实现隐私保护与QoS之间的平衡。

Result: 仿真结果表明，该方法在保证个性化隐私需求的同时，显著降低QoS损失。

Conclusion: 3DSTPM在3D时空轨迹隐私保护中有效平衡隐私性和QoS，且能够针对不同用户偏好进行个性化保护。

Abstract: The rapid advancement of location-based services (LBSs) in three-dimensional (3D) domains, such as smart cities and intelligent transportation, has raised concerns over 3D spatiotemporal trajectory privacy protection. However, existing research has not fully addressed the risk of attackers exploiting the spatiotemporal correlation of 3D spatiotemporal trajectories and the impact of height information, both of which can potentially lead to significant privacy leakage. To address these issues, this paper proposes a personalized 3D spatiotemporal trajectory privacy protection mechanism, named 3DSTPM. First, we analyze the characteristics of attackers that exploit spatiotemporal correlations between locations in a trajectory and present the attack model. Next, we exploit the complementary characteristics of 3D geo-indistinguishability (3D-GI) and distortion privacy to find a protection location set (PLS) that obscures the real location for all possible locations. To address the issue of privacy accumulation caused by continuous trajectory queries, we propose a Window-based Adaptive Privacy Budget Allocation (W-APBA), which dynamically allocates privacy budgets to all locations in the current PLS based on their predictability and sensitivity. Finally, we perturb the real location using the allocated privacy budget by the PF (Permute-and-Flip) mechanism, effectively balancing privacy protection and Quality of Service (QoS). Simulation results demonstrate that the proposed 3DSTPM effectively reduces QoS loss while meeting the user's personalized privacy protection needs.

</details>


### [167] [Department-Specific Security Awareness Campaigns: A Cross-Organizational Study of HR and Accounting](https://arxiv.org/abs/2511.22189)
*Matthias Pfister,Giovanni Apruzzese,Irdin Pekaric*

Main category: cs.CR

TL;DR: 对人力资源和会计两个部门的安全意识培训进行部门定制化研究，揭示通用培训忽视的部门特定脆弱点，并提出相应的培训设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有安全意识研究多采用“通用/组织层面”的视角，忽视了不同部门的工作流程和特定威胁。本研究通过系统综述和多公司混合方法，聚焦HR和会计两个关键部门，检验其特有的威胁、培训主题与交付方式的有效性。

Method: 1) 系统性文献综述；2) 跨9家公司、对HR和会计各自的威胁与培训内容进行混合方法研究；3) 先对16名来自跨国企业的员工进行访谈；4) 基于访谈结果设计并实施覆盖9家公司、90+名HR/会计成员的结构化问卷调查。

Result: 研究发现：HR部门面临的威胁包括带有恶意软件的求职申请和高管冒充；会计部门面临发票欺诈、凭证盗窃以及勒索软件等威胁。现有培训往往过于通用，员工更偏好短时、情境化的培训形式（如视频、仿真），而不是年度一次的培训。

Conclusion: 基于洞察，提出针对部门需求和工作流的定制化意识培训设计建议，强调情境化、简短模块与契合岗位的培训内容。

Abstract: Many cyberattacks succeed because they exploit flaws at the human level. To address this problem, organizations rely on security awareness programs, which aim to make employees more resilient against social engineering. While some works have suggested that such programs should account for contextual relevance, the common praxis in research is to adopt a "general" viewpoint. For instance, instead of focusing on department-specific issues, prior user studies sought to provide organization-wide conclusions. Such a protocol may lead to overlooking vulnerabilities that affect only specific subsets of an organization.
  In this paper, we tackle such an oversight. First, through a systematic literature review, we provide evidence that prior literature poorly accounted for department-specific needs. Then, we carry out a multi-company and mixed-methods study focusing on two pivotal departments: human resources (HR) and accounting. We explore three dimensions: threats faced by these departments; topics covered in the security-awareness campaigns delivered to these departments; and delivery methods that maximize the effectiveness of such campaigns. We begin by interviewing 16 employees of a multinational enterprise, and then use these results as a scaffold to design a structured survey through which we collect the responses of over 90 HR/accounting members of 9 organizations. We find that HR is targeted through job applications containing malware and executive impersonation, while accounting is exposed to invoice fraud, credential theft, and ransomware. Current training is often viewed as too generic, with employees preferring shorter, scenario-based formats like videos and simulations. These preferences contradict the common industry practice of annual sessions. Based on these insights, we propose recommendations for designing awareness programs tailored to departmental needs and workflows.

</details>


### [168] [Real-PGDN: A Two-level Classification Method for Full-Process Recognition of Newly Registered Pornographic and Gambling Domain Names](https://arxiv.org/abs/2511.22215)
*Hao Wang,Yingshuo Wang,Junang Gan,Yanan Cheng,Jinshuai Zhang*

Main category: cs.CR

TL;DR: Real-PGDN方法实现对新注册域名中的色情与赌博域名的实时高精度分类与应用效果评估，采用CoSENT-MLP两级分类器并在NRD2024数据集上达到97.88%精度，延迟使用的PGDN预测仍超70%准确。


<details>
  <summary>Details</summary>
Motivation: 在线色情与赌博的监管挑战日益严峻，需对新注册域名进行准确分类以保护个人资产与隐私。现有研究要么在理想样本数据下追求高准确率，要么在真实场景下使用最新数据但准确率较低，因此亟需一种能够在真实数据下实现高精度与实时性的分类方法。

Method: 提出Real-PGDN方法，完成实时且全面的真实数据爬取、具容错的特征提取、精准的PGDN分类，以及在实际场景中的应用效果评估。所采用的两级分类器将CoSENT（基于BERT）、MLP以及传统分类算法进行融合，形成高精度分类框架。数据方面构建NRD2024数据集，覆盖20天内对150万条新注册域名的持续检测信息，覆盖6个方向。

Result: 实验结果显示该方法达到97.88%的精确度，并且对在注册后延迟使用的PGDN仍具备超过70%的预测精度。

Conclusion: 在真实场景中，Real-PGDN方法具备可行性与有效性，能够实现对PGDN的及时、全面识别并评估其应用影响。

Abstract: Online pornography and gambling have consistently posed regulatory challenges for governments, threatening both personal assets and privacy. Therefore, it is imperative to research the classification of the newly registered Pornographic and Gambling Domain Names (PGDN). However, scholarly investigation into this topic is limited. Previous efforts in PGDN classification pursue high accuracy using ideal sample data, while others employ up-to-date data from real-world scenarios but achieve lower classification accuracy. This paper introduces the Real-PGDN method, which accomplishes a complete process of timely and comprehensive real-data crawling, feature extraction with feature-missing tolerance, precise PGDN classification, and assessment of application effects in actual scenarios. Our two-level classifier, which integrates CoSENT (BERT-based), Multilayer Perceptron (MLP), and traditional classification algorithms, achieves a 97.88% precision. The research process amasses the NRD2024 dataset, which contains continuous detection information over 20 days for 1,500,000 newly registered domain names across 6 directions. Results from our case study demonstrate that this method also maintains a forecast precision of over 70% for PGDN that are delayed in usage after registration.

</details>


### [169] [Keyless Entry: Breaking and Entering eMMC RPMB with EMFI](https://arxiv.org/abs/2511.22340)
*Aya Fukami,Richard Buurke*

Main category: cs.CR

TL;DR: 通过对位于三块主制造商的eMMC的RPMB认证方案进行 EMP 摇动（电磁脉冲）攻击，实验证明RPMB认证可被篡改，导致两块设备的RPMB区域被任意覆盖数据，而未影响其他数据的完整性。


<details>
  <summary>Details</summary>
Motivation: RPMB 提供安全存储用于关键数据的完整性保护；若其认证机制对硬件攻击脆弱，将对设备安全造成巨大风险。本研究旨在评估主流厂商多型号eMMC中的RPMB认证的鲁棒性。

Method: 对目标芯片实施电磁脉冲（EMP）攻击以引发RPMB认证故障；在三块来自同一厂商的不同eMMC中复现攻击；观察RPMB数据的完整性与覆盖情况。

Result: 在两块目标eMMC上成功GLITCH RPMB认证，使RPMB中数据被覆盖为任意内容；其余数据的完整性未受影响。第三块未在摘要中详细说明其结果。

Conclusion: 该研究揭示了 RPMB 认证在面对EMP类硬件攻击时的脆弱性，提示需要增强硬件层面的防护或改进RPMB认证设计以提升抗干扰性和数据完整性保障。

Abstract: The Replay Protected Memory Block (RPMB) in modern storage systems provides a secure area where data integrity is ensured by authentication. This block is used in digital devices to store pivotal information that must be safeguarded against modification by potential attackers. This paper targets the authentication scheme of the RPMB in three different eMMCs from a major manufacturer. A glitch was injected by sending an electromagnetic pulse to the target chip. RPMB authentication was successfully glitched and the information stored in two target eMMCs was overwritten with arbitrary data, without affecting the integrity of other data.

</details>


### [170] [CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights](https://arxiv.org/abs/2511.22681)
*Mohaiminul Al Nahian,Abeer Matar A. Almalky,Gamana Aragonda,Ranyang Zhou,Sabbir Ahmed,Dmitry Ponomarev,Li Yang,Shaahin Angizi,Adnan Siraj Rakin*

Main category: cs.CR

TL;DR: 提出 CacheTrap：一种面向大语言模型的数据无关、梯度无关的木马攻击，通过篡改 KV 缓存中的值向量中的单个位触发攻击，能在推理阶段使模型输出朝向目标类别，同时对输入、权重无痕迹且不影响模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 在对抗性权重扰动日益成为威胁的背景下，研究者需要理解并评估新的攻击面。现有防御在输入和权重空间进行系统级检测，但若攻击仅依赖推理阶段的 KV 缓存（其动态激活取决于具体 token），则可能绕过传统防御。该工作旨在从缓存的瞬时性和数据独立性出发，提出一种可在不依赖数据/梯度信息的前提下实现的木马攻击。

Method: 设计一个脆弱的 KV 位搜索算法，定位可被单个位触发的缓存值向量中的位；在推理过程中利用该位触发器使模型输出偏向目标类别。攻击具备数据无关、梯度无关、推理时易移植且对模型效用无影响等特征。

Result: 实验结果表明，CacheTrap 能在 KV 缓存中通过单个位翻转实现对 LLM 的首次成功 Trojan 攻击；该攻击是数据无关且一旦定位到易受攻击的位，位置即可迁移到多种受害任务/数据集/查询，且无额外开销。

Conclusion: KV 缓存作为一个新的攻击面揭示了在推理阶段可利用的瞬态激活表征来执行木马攻击的可能性。该攻击不依赖数据或梯度且具可迁移性，促使防护研究者需要扩展对 KV 缓存及其一致性/完整性保护的关注，以降低单比特攻击的风险。

Abstract: Adversarial weight perturbation has emerged as a concerning threat to LLMs that either use training privileges or system-level access to inject adversarial corruption in model weights. With the emergence of innovative defensive solutions that place system- and algorithm-level checks and corrections in the input and weight spaces, these perturbations are increasingly susceptible to defenses. This work develops a novel perspective on Trojan attacks that generates an attacker-designed model output while leaving no attack traces on the inputs or weights. Such an attack space can be unlocked through corruption of the key-value (KV) cache. In this paper, we introduce CacheTrap, a novel Trojan attack that corrupts the value vectors stored in the KV cache. These vectors capture the dynamic activations for specific token positions and therefore constitute a natural surface for transient, inference-time trigger insertion. The transient nature of these KV values and their dependence on victim input imply additional constraints on our attack, such as a lack of knowledge of the victim's data or domain application, and, consequently, a lack of gradient information. The objective of the proposed CacheTrap is to develop a vulnerable KV bit-searching algorithm so that, once the attack employs the identified bit-flip as a trigger, the model generates targeted behavior, e.g., classifying inputs towards the target class. Moreover, CacheTrap is a data- and gradient-free attack which also has no impact on the model's utility. Our evaluation demonstrates that the proposed attack enables the first successful Trojan attack on LLMs with a single bit flip in the KV cache. In addition, the data-independent nature of the attack ensures that once the attacker identifies the vulnerable bit index, the location remains constant and can be transferred to a wide range of victim tasks/datasets/queries with no overhead.

</details>


### [171] [Ghosting Your LLM: Without The Knowledge of Your Gradient and Data](https://arxiv.org/abs/2511.22700)
*Abeer Matar A. Almalky,Ziyan Wang,Mohaiminul Al Nahian,Li Yang,Adnan Siraj Rakin*

Main category: cs.CR

TL;DR: 提出一种数据与梯度无关的位翻转攻击（BFA）方法，用于对LLMs进行高效、低资源的对抗，并在五个开源LLMs上以单比特实现目标。


<details>
  <summary>Details</summary>
Motivation: 现有的BFA多依赖梯度和数据，不仅计算和内存成本高，且需要目标领域知识。对于LLMs，这在规模和应用场景上成为制约因素。

Method: 提出新的脆弱性指标，独立于梯度与数据，通过识别易受攻击的权重量比特实现对LLMs的BFAs，降低内存与计算需求，复杂度接近常数，且可扩展到多任务。

Result: 实验表明，最少一个比特翻转即可在五个开源LLMs上实现对抗目标，展示高效性与可扩展性。

Conclusion: 证明在无梯度、无数据知识条件下仍可实现对LLMs的有效BFAs，提示需要在硬件容错与模型鲁棒性方面加强防护，并注意其潜在安全风险。

Abstract: In recent years, large language models (LLMs) have achieved substantial advancements and are increasingly integrated into critical applications across various domains. This growing adoption underscores the need to ensure their security and robustness. In this work, we focus on the impact of Bit Flip Attacks (BFAs) on LLMs, which exploits hardware faults to corrupt model parameters, posing a significant threat to model integrity and performance. Existing studies on BFA against LLMs adopt a progressive bit-search strategy that predominantly relies on gradient-based techniques to identify sensitive layers or weights. However, computing gradients comes with two specific challenges: First, in the context of LLMs, it increases computational and memory costs exponentially, and Second, it requires access to a sample victim dataset or knowledge of the victim domain to compute the gradient. In this work, we investigate beyond the scope of attack efficacy and aim to develop an efficient, practical Gradient-Data-free Bit-Flip Attack. The challenge lies in the core principle of adversarial attacks, which relies heavily on computing gradients from sample test/train data and manipulating model weights based on gradient information. To overcome this, we propose novel vulnerability index metrics that can identify vulnerable weight bits in LLMs independent of any gradient or data knowledge. By removing the dependency on gradient computation, our approach drastically reduces memory requirements and scales efficiently across multiple tasks with constant complexity. Experimental results demonstrate the efficiency of our method, requiring as few as a single bit flip to achieve adversarial objectives for five open-source LLMs.

</details>


### [172] [Clustering Malware at Scale: A First Full-Benchmark Study](https://arxiv.org/abs/2511.23198)
*Martin Mocko,Jakub Ševcech,Daniela Chudá*

Main category: cs.CR

TL;DR: 对 Bodmas 与 Ember 两个大型公开基准数据集进行全面的恶意软件聚类评估，并首次在完整数据集上进行分析，结果表明在聚类算法中 K-Means 与 BIRCH 表现优于 DBSCAN 与 HAC；将 benign 样本纳入聚类并未显著降低聚类质量；不同数据集之间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 弥补恶意软件聚类研究中对含良性样本的研究不足，以及未充分利用大型公开数据集进行全面评估的问题，确立在大规模基准数据上的聚类状态。

Method: 在 Bodmas 与 Ember（以及一个私有行业数据集）上，比较多种聚类算法的聚类质量：K-Means、BIRCH、DBSCAN、层次聚类（HAC），并研究将 benign 样本引入对聚类效果的影响，评估在整个数据集上的聚类表现。

Result: 研究发现 K-Means 与 BIRCH 为顶尖聚类器，DBSCAN 与 HAC 效果落后；Bodmas 与 Ember 之间、以及与私有数据集之间存在显著的聚类质量差异；将 benign 样本纳入并未显著降低聚类质量。

Conclusion: 在大规模公开基准数据集上开展恶意软件聚类研究是可行且有意义的；不同数据集属性会显著影响聚类效果；未来应继续比较更多算法并深入分析 benign 样本对聚类的影响。

Abstract: Recent years have shown that malware attacks still happen with high frequency. Malware experts seek to categorize and classify incoming samples to confirm their trustworthiness or prove their maliciousness. One of the ways in which groups of malware samples can be identified is through malware clustering. Despite the efforts of the community, malware clustering which incorporates benign samples has been under-explored. Moreover, despite the availability of larger public benchmark malware datasets, malware clustering studies have avoided fully utilizing these datasets in their experiments, often resorting to small datasets with only a few families. Additionally, the current state-of-the-art solutions for malware clustering remain unclear. In our study, we evaluate malware clustering quality and establish the state-of-the-art on Bodmas and Ember - two large public benchmark malware datasets. Ours is the first study of malware clustering performed on whole malware benchmark datasets. Additionally, we extend the malware clustering task by incorporating benign samples. Our results indicate that incorporating benign samples does not significantly degrade clustering quality. We find that there are significant differences in the quality of the created clusters between Ember and Bodmas, as well as a private industry dataset. Contrary to popular opinion, our top clustering performers are K-Means and BIRCH, with DBSCAN and HAC falling behind.

</details>


### [173] [One-Shot Secure Aggregation: A Hybrid Cryptographic Protocol for Private Federated Learning in IoT](https://arxiv.org/abs/2511.23252)
*Imraul Emmaka,Tran Viet Xuan Phuong*

Main category: cs.CR

TL;DR: Hyb-Agg 是一种轻量级的安全聚合协议，结合 MK-CKKS 同态加密与基于 ECDH 的加性掩码，实现单轮、固定通信开销的隐私保护联邦学习，适用于物联网场景。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，若干协调者需要聚合模型更新，传统安全聚合成本高、通信量大，尤其在带宽、延迟和能量受限的物联网环境中难以扩展。

Method: 将 MK-CKKS 同态加密与 ECDH 基于掩码相结合，实现单轮、无交互的客户端到服务器传输；不需要部分解密，对 RLWE、CDH 和随机预言机等假设下提供强隐私，且对服务器及最多 N-2 名客户端的串通鲁棒。

Result: 在高性能设备与 Raspberry Pi 4 等资源受限设备上实现并评估，达到亚秒级执行时间，通信开销相对于明文约 12 倍扩展，证明在 IoT 场景中的可实用性。

Conclusion: Hyb-Agg 有望推动可扩展、隐私保护的联邦学习在边缘部署中的实践，直接解决了通信瓶颈问题。

Abstract: Federated Learning (FL) offers a promising approach to collaboratively train machine learning models without centralizing raw data, yet its scalability is often throttled by excessive communication overhead. This challenge is magnified in Internet of Things (IoT) environments, where devices face stringent bandwidth, latency, and energy constraints. Conventional secure aggregation protocols, while essential for protecting model updates, frequently require multiple interaction rounds, large payload sizes, and per-client costs rendering them impractical for many edge deployments.
  In this work, we present Hyb-Agg, a lightweight and communication-efficient secure aggregation protocol that integrates Multi-Key CKKS (MK-CKKS) homomorphic encryption with Elliptic Curve Diffie-Hellman (ECDH)-based additive masking. Hyb-Agg reduces the secure aggregation process to a single, non-interactive client-to-server transmission per round, ensuring that per-client communication remains constant regardless of the number of participants. This design eliminates partial decryption exchanges, preserves strong privacy under the RLWE, CDH, and random oracle assumptions, and maintains robustness against collusion by the server and up to $N-2$ clients.
  We implement and evaluate Hyb-Agg on both high-performance and resource-constrained devices, including a Raspberry Pi 4, demonstrating that it delivers sub-second execution times while achieving a constant communication expansion factor of approximately 12x over plaintext size. By directly addressing the communication bottleneck, Hyb-Agg enables scalable, privacy-preserving federated learning that is practical for real-world IoT deployments.

</details>


### [174] [FedSGT: Exact Federated Unlearning via Sequential Group-based Training](https://arxiv.org/abs/2511.23393)
*Bokang Zhang,Hong Guan,Hong kyu Lee,Ruixuan Liu,Jia Zou,Li Xiong*

Main category: cs.CR

TL;DR: FedSGT 提出了一种在联邦学习中实现精确数据删减的分组序列训练框架，通过服务器端的轻量化 PEFT 模块对不同数据分组进行独立建模，删除请求仅需停用相关模块即可实现即时删除，同时通过多序列训练保持模型效用，实验与理论分析表明在多次删除请求下具有更长的服务可用性和可比的学习性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中实现“被遗忘权”极具挑战性，因为数据通过分布式、交错的客户端更新影响模型；现有的精确删减方法通常需要从头重新训练，造成高通信开销和服务中断时间。

Method: 将数据划分为均匀分组，客户端可参与多个分组；通过限制每个客户端参与的分组数控制通信开销；训练多条 PEFT 模块序列，每条序列对应一个不同的分组排列；PEFT 模块轻量且在服务端维护，将不同数据分组的影响隔离为独立模块；通过停用对应分组的模块实现精确删减；多序列训练有助于在删减请求累积时维持模型性能。

Result: 对删除率和模型性能给出严格的理论分析；在多次删除请求场景下实验显示 FedSGT 能显著延长服务可维护性，同时学习性能和训练效率与其他精确删减基线接近；大量消融实验证实鲁棒性。

Conclusion: FedSGT 在联邦学习环境中实现低开销的精确删减，且通过分组模块化和多序列训练实现即时删除与可扩展性，能在多次删除请求下维持较高的模型性能。

Abstract: Federated Learning (FL) enables collaborative, privacy-preserving model training, but supporting the "Right to be Forgotten" is especially challenging because data influences the model through distributed and interleaved client updates. Existing exact unlearning methods typically require frequent retraining from scratch, resulting in high communication cost and long service downtime. To address this, we propose Federated Sequential Group-based Training (FedSGT), an exact unlearning framework for FL. FedSGT partitions the data into uniform groups, and each client may participate in multiple groups. To control communication overhead, each client can limit the number of groups it contributes to. FedSGT then trains multiple sequences of Parameter-Efficient Fine-Tuning (PEFT) modules, each corresponding to a different group permutation. Since the PEFT modules are lightweight and maintained server-side, FedSGT isolates the influence of different data groups into independent modules without incurring significant storage overhead and communication cost. Exact unlearning is thus achieved instantly by deactivating the modules corresponding to the group containing the unlearned data. Furthermore, using multiple training sequences helps maintain high model utility as deletion requests accumulate. We provide a rigorous theoretical analysis of both the deletion rate -- expected number of deletions before retraining is needed -- and the expected model performance. Experiments on various tasks demonstrate that FedSGT achieves a significantly longer service maintenance under multiple unlearning requests while maintaining comparable learning performance and training efficiency to other exact unlearning baselines. Extensive ablation studies validate the robustness of our method across a wide range of parameter settings.

</details>


### [175] [Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities](https://arxiv.org/abs/2511.23408)
*Aayush Garg,Zanis Ali Khan,Renzo Degiovanni,Qiang Tang*

Main category: cs.CR

TL;DR: 使用PoV测试评估多种LLM在真实与人工漏洞上的自动修补能力，发现对真实漏洞的修补效果优于人工漏洞，且不同模型在修补覆盖度与互补性方面存在显著差异，提示需结合多模型策略提升修补效果。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究偏向公开漏洞、缺乏对相关人工漏洞评估的空白，明确量化不同LLM在漏洞修补中的有效性及其互补性，以指导模型选择和组合策略。

Method: 通过Proof-of-Vulnerability (PoV)测试执行，直接验证LLM生成的源码是否能修补漏洞；对OpenAI GPT 系列、LLaMA、DeepSeek、Mistral等多种LLM在真实与人工漏洞上的表现进行对比分析。

Result: 在真实漏洞上，LLMs的修补效果优于对人工漏洞的修补；各模型在重叠修补与互补性方面差异显著，存在同一漏洞被多模型修补或仅被单一模型修补的现象，强调模型选择对修补效果的重要性。

Conclusion: 结论强调在漏洞修补任务中需考虑模型间的互补性，可能的提升路径包括基于多模型的集成策略，并呼吁深入研究导致模型差异的原因以优化模型选型。

Abstract: Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [176] [Secure Command, Control and Communications Systems (C3) for Army UxVs](https://arxiv.org/abs/2511.21936)
*T. Rebolo,A. Grilo,C. Ribeiro*

Main category: cs.NI

TL;DR: 提出 NC2S 的安全指挥控制架构，用零信任模型实现 CIA，并通过 mTLS/ECDSA/ECDH 与 HMAC 来保护通信，支持 GCS 与 UxV 的实时控制委派。原型在 Wi-Fi 与 Rohde&Schwarz HR-5000H 无线电上验证，尽管无线链路延迟显著增大，但在可接受的范围内保持稳定通信和低丢包，适合 TC/GCS 之间的链接。


<details>
  <summary>Details</summary>
Motivation: 在现代军事行动中，UxV 的侦察/监视/打击能力被广泛采用，但普遍依赖缺乏认证和加密的协议（如 MAVLink），存在安全风险。需要一个既能保障机密性、完整性与认证，又能实现实时控制委派的指挥控制系统，且要能在具备低带宽/高时延的战术链路中工作。

Method: 提出并实现一个新的指挥控制架构 NC2S，采用零信任模型和分层凭证权限管理来调控 TC、GCS 与 UxV 的访问与控制。核心采用互信 TLS(mTLS)、ECDSA 证书和 ECDH 密钥交换，结合 HMAC 保障消息完整性。设计了用于凭证管理、密钥更新和控制移交的多种轻量协议。原型在 Wi-Fi 与 Rohde&Schwarz HR-5000H 战术无线电上进行实验验证。

Result: 结果显示，HR-5000H 链路的延迟大约比宽带技术（如 Wi-Fi/5G 等）高出约两个数量级，但仍能维持稳定通信，消息丢失较少。这意味着 NC2S 能在 TC 终端与 GCS 之间的链路上工作可行。

Conclusion: NC2S 能在 UxV 指挥控制中提供机密性、完整性与认证，结合零信任和分层权限，可以实现对 TC、GCS 与 UxV 的受控、可审计的实时控制委派；尽管战术链路具有高时延，但在设计的轻量协议与合规的加密机制下，具备实用性和可部署性。

Abstract: Unmanned Vehicles (UxVs) are increasingly used in modern military operations for reconnaissance, surveillance, and strike missions, enhancing situational awareness while reducing risk to personnel. Their affordability and rapid deployment have encouraged the adoption of commercial solutions. However, many rely on insecure protocols such as MAVLink, which lack authentication and encryption mechanisms. This paper designed, implemented, and evaluated a new secure command-and-control architecture that ensures confidentiality, integrity, and authentication (CIA) while supporting real-time control delegation between Ground Control Stations (GCSs). The proposed solution, named New Command and Control System (NC2S), enforces a zero-trust model integrating hierarchical credential-based privileges to regulate access and control among Tactical Commanders (TC), GCSs, and UxVs. It employs mutual Transport Layer Security (mTLS) with Elliptic Curve Digital Signature Algorithm (ECDSA) certificates and Elliptic Curve Diffie-Hellman (ECDH) key exchange, while message integrity is ensured through Hash-based Message Authentication Codes (HMAC). Multiple lightweight protocols were developed for credential management, key renewal, and control handover. The NC2S prototype was experimentally validated over Wi-Fi and Rohde&Schwarz HR-5000H tactical radios. Results showed that HR-5000H links introduce latencies roughly two orders of magnitude higher than broadband technologies (e.g., Wi-Fi or 5G&Beyond technologies) but are still able to maintain stable communication with minimal message loss, making them suitable for the NC2S links among TC terminals and GCSs.

</details>


### [177] [Optimizing NetGPT via Routing-Based Synergy and Reinforcement Learning](https://arxiv.org/abs/2511.22217)
*Yuxuan Chen,Rongpeng Li,Xianfu Chen,Celimuge Wu,Chenghui Peng,Zhifeng Zhao,Honggang Zhang*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language model (LLM) agents at the network edge offer low-latency execution for routine queries. In contrast, complex requests often require the superior capability of cloud models, incurring higher latency and cost. To navigate this quality-cost trade-off under dynamic network conditions, we propose a cloud-edge synergy for NetGPT that integrates network-aware routing with on-edge self-improvement. Specifically, our framework routes structured tool-calling requests to cloud or edge agents via a novel scoring policy. We prove that, under mild regularity assumptions, the optimal routing rule admits a unique fallback threshold with monotone dependence on bandwidth and round-trip time (RTT). Concurrently, based on the dataset collected from requests routed to the cloud and corresponding responses, we instantiate a schema-preserving reinforcement learning (RL) to improve the capability of the edge agent. We analyze a supervised finetuning (SFT)-anchored composite objective that combines a reverse-KL trust-region step with a forward-KL realignment toward the SFT prior, explaining stability and constraining policy drift. Both the network-aware routing policy and the edge agent are updated coherently. Experiments across controlled network states and pricing schedules demonstrate smooth quality-cost frontiers, consistent gains of dynamic fallback thresholds over fixed policies, and sustained reductions in offloading while maintaining task success and schema-correct outputs.

</details>


### [178] [Semantic-Aware Caching for Efficient Image Generation in Edge Computing](https://arxiv.org/abs/2511.22421)
*Hanshuai Cui,Zhiqing Tang,Zhi Yao,Weijia Ji,Wei Zhao*

Main category: cs.NI

TL;DR: CacheGenius通过语义感知的缓存与调度机制，在边缘设备上将文本到图像扩散生成过程中的多步去噪步骤通过使用与目标语义相似的参考图像加速，显著降低延迟和计算成本，同时维持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在资源受限的移动/边缘环境中难以快速推断，因为需执行大量去噪步骤。通过使用语义相近的有噪参考图像来初始化去噪过程，可减少所需步骤。需要一个能够在边缘系统中实现语义对齐的参考缓存与维护策略。

Method: 提出混合文本到图像与图像到图像工作流的系统CacheGenius，结合语义感知的分类存储、请求调度算法以确保参考与目标的语义对齐，以及基于相关性分析的缓存维护策略以主动驱逐过时条目。在分布式边缘计算系统中对其进行评估。

Result: 相较基线，CacheGenius将生成延迟降低约41%，计算成本降低约48%，同时保持竞争性的评估指标。

Conclusion: 在边缘场景下，通过语义感知的缓存与调度实现对扩散生成的高效加速，证明了结合文本-图像与图像-到-图像工作流的缓存策略在资源受限环境中的实用性。

Abstract: Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.

</details>


### [179] [RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications](https://arxiv.org/abs/2511.23278)
*Jhonatan Tavori,Anat Bremler-Barr,Hanoch Levy,Ofek Lavi*

Main category: cs.NI

TL;DR: RetryGuard 提供一个分布式框架，以跨服务管理重试策略，避免重试风暴，降低资源消耗与成本，在云原生微服务场景中表现出色，且在 Kubernetes+Istio 的复杂部署中仍具优势。


<details>
  <summary>Details</summary>
Motivation: 现代云应用由独立且多样的微服务构成，存在自动伸缩和不对齐的重试模式，易导致资源浪费、性能下降和成本上升的自我造成性 DoW（Denial-of-Wallet）问题。需要一个跨服务、按服务粒度的协调机制来控制重试行为。

Method: 提出在分布式框架 RetryGuard 中按服务粒度管理重试策略，并采用并行决策来避免冲突；建立分析模型，量化重试、吞吐量/拒绝、延迟和成本之间的关系；在 AWS 的标准与高级重试策略对比，以及在包含 Istio 的 Kubernetes 部署中的实验评估。

Result: 实验结果显示，RetryGuard 相对于 AWS 的标准与高级重试策略，显著降低了资源使用和成本。在更复杂的 Kubernetes+Istio 部署中，也实现了较大幅度的性能提升和资源节约。

Conclusion: RetryGuard 能有效防止重试风暴、降低资源竞争与成本，具有良好的可扩展性，在云原生微服务环境中展现出优越的性能。

Abstract: Modern cloud applications are built on independent, diverse microservices, offering scalability, flexibility, and usage-based billing. However, the structural design of these varied services, along with their reliance on auto-scalers for dynamic internet traffic, introduces significant coordination challenges. As we demonstrate in this paper, common default retry patterns used between misaligned services can turn into retry storms which drive up resource usage and costs, leading to self-inflicted Denial-of-Wallet (DoW) scenarios. To overcome these problems we introduce RetryGuard, a distributed framework for productive control of retry patterns across interdependent microservices. By managing retry policy on a per-service basis and making parallel decisions, RetryGuard prevents retry storms, curbs resource contention, and mitigates escalating operational costs. RetryGuard makes its decisions based on an analytic model that captures the relationships among retries, throughput (rejections), delays, and costs. Experimental results show that RetryGuard significantly reduces resource usage and costs compared to AWS standard and advanced retry policies. We further demonstrate its scalability and superior performance in a more complex Kubernetes deployment with the Istio service mesh, where it achieves substantial improvements.

</details>


### [180] [Joint Resource Allocation to Transparently Integrate 5G TDD Uplink with Time-Aware TSN](https://arxiv.org/abs/2511.23373)
*Laura Becker,Yash Deshpande,Wolfgang Kellerer*

Main category: cs.NI

TL;DR: 提出一种将5G作为透明TSN网桥的异构无线资源调度器，通过静态预分配和动态资源分配实现面向时延的TSN流管理，提升端到端调度的资源利用率；在OMNeT++仿真中对比Configured Grant基线，资源利用率提升约28%，对非确定性流量也有吞吐提升。


<details>
  <summary>Details</summary>
Motivation: 实现5G与TSN的无缝集成，获得确定性端到端调度；桥延迟BD在TSN路由中的关键作用，以及5G的无线资源分配特性对BD的影响；现有UL调度器偏向吞吐，难以满足时限要求。

Method: 提出一个混合调度器：对时间敏感的周期性流基于BD进行静态预分配，确保与Time-Aware Shaper和Per-Stream Filtering and Policing等TSN机制对齐；其余资源基于Proportional Fair、Max C/I或QoS感知优先级等策略进行动态分配；使用OMNeT++进行仿真评估。

Result: 支持多样化的TSN流，确保移动场景下的时间敏感UL流的端到端时延调度；相较Config Grants基线，资源利用率提升约28%；保持可靠性，同时非确定性速率相关流获得更高吞吐。

Conclusion: 该调度器实现在多域之间以5G作为透明TSN桥的端到端确定性调度，为异构场景下的TSN流提供更高资源利用与吞吐，同时保持可靠性。

Abstract: To enable mobility in industrial communication systems, the seamless integration of 5G with Time-Sensitive Networking (TSN) is a promising approach. Deterministic communication across heterogeneous 5G-TSN systems requires joint scheduling between both domains. A key prerequisite for time-aware end-to-end scheduling is determining the forwarding delay for each TSN Traffic Class at every bridge, referred to as Bridge Delay (BD). Hence, to integrate 5G as a transparent TSN bridge, the 5G BD must be determined and guaranteed. Unlike wired bridges, the 5G BD relies on wireless resource management characteristics, such as the Time Division Duplex pattern and radio resource allocation procedure. In particular, traditional Uplink (UL) schedulers are optimized for throughput but often fail to meet the deadline requirements. To address this challenge, we propose a heterogeneous radio resource scheduler that integrates static and dynamic scheduling. The algorithm pre-allocates resources for time-sensitive periodic streams based on the reported BDs, ensuring alignment with the TSN mechanisms Time-Aware Shaper and Per-Stream Filtering and Policing. Meanwhile, remaining resources are dynamically allocated to non-deterministic flows using established strategies such as Proportional Fair, Max C/I, or a Quality of Service-aware priority-based scheduler. The scheduler's performance is evaluated through OMNeT++ simulations. The results demonstrate support for diverse TSN flows while ensuring deadline-aware scheduling of time-sensitive UL traffic in mobility scenarios. Periodic time-sensitive flows are end-to-end scheduled across domains, improving the resource efficiency by 28% compared to the Configured Grant baseline. While reliability is preserved, non-deterministic rate-sensitive flows benefit from the improved resource utilization, resulting in higher throughput

</details>
