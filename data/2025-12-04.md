<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.LG](#cs.LG) [Total: 73]
- [cs.IT](#cs.IT) [Total: 7]
- [eess.SY](#eess.SY) [Total: 8]
- [eess.SP](#eess.SP) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Performance Evaluation of Parallel Wi-Fi Redundancy with Deferral Techniques](https://arxiv.org/abs/2512.03569)
*Gianluca Cena,Pietro Chiavassa,Stefano Scanzio*

Main category: cs.NI

TL;DR: Deferred parallel redundancy improves worst-case latency in Wi-Fi-based industrial control with modest spectrum cost, as shown by real-world data.


<details>
  <summary>Details</summary>
Motivation: Industrial wireless links must be dependable with low latency for soft real-time control; while Wi-Fi offers high performance, reliability concerns limit its use in control loops.

Method: Quantitative evaluation using a large real-world dataset, comparing parallel redundancy schemes and focusing on deferred redundancy.

Result: Deferred parallel redundancy reduces worst-case transmission latency with limited spectrum overhead, enabling practical use in control loops.

Conclusion: Parallel redundancy, especially deferred redundancy, can make Wi-Fi viable for soft real-time industrial control.

Abstract: Wireless communication is increasingly used in industrial environments, since it supports mobility of interconnected devices. Among the transmission technologies operating in unlicensed bands available to this purpose, Wi-Fi is certainly one of the most interesting, because of its high performance and the relatively low deployment costs. Unfortunately, its dependability is often deemed unsuitable for real-time control systems. In this paper, the use of parallel redundancy is evaluated from a quantitative viewpoint, by considering a number of performance indices that are relevant for soft real-time applications. Analysis is carried out on a large dataset acquired from a real setup, to provide realistic insights on the advantages this kind of approaches can provide. As will be seen, deferred parallel redundancy provides clear advantages in terms of the worst-case transmission latency, at limited costs concerning the amount of consumed spectrum. Hence, it can be practically exploited every time a wireless connection is included in a control loop.

</details>


### [2] [Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks](https://arxiv.org/abs/2512.03722)
*Lingyi Cai,Wenjie Fu,Yuxi Huang,Ruichen Zhang,Yinqiu Liu,Jiawen Kang,Zehui Xiong,Tao Jiang,Dusit Niyato,Xianbin Wang,Shiwen Mao,Xuemin Shen*

Main category: cs.NI

TL;DR: LLM-enhanced RL for wireless networks: a taxonomy of four roles (state perceiver, reward designer, decision-maker, generator) to augment RL, a survey of existing studies, case studies across low-altitude economy networks, vehicular networks, and space-air-ground networks, and future directions.


<details>
  <summary>Details</summary>
Motivation: Address RL limitations in generalization, sample efficiency, interpretability, and learning feedback in dynamic wireless environments by leveraging LLMs' knowledge, reasoning, and contextual generation.

Method: Propose a taxonomy of four LLM roles, review existing work mapping these roles to stages of the RL pipeline, and present case studies across diverse wireless network settings to illustrate design and deployment.

Result: A comprehensive tutorial that structures how LLMs can augment RL in wireless networks, offering a practical framework and illustrative case studies to guide future research and applications.

Conclusion: LLM-enhanced RL is a promising direction for wireless networks, with potential to improve generalization, adaptability, and efficiency; future work should address integration challenges, evaluation methodologies, and domain-specific considerations.

Abstract: Reinforcement Learning (RL) has shown remarkable success in enabling adaptive and data-driven optimization for various applications in wireless networks. However, classical RL suffers from limitations in generalization, learning feedback, interpretability, and sample efficiency in dynamic wireless environments. Large Language Models (LLMs) have emerged as a transformative Artificial Intelligence (AI) paradigm with exceptional capabilities in knowledge generalization, contextual reasoning, and interactive generation, which have demonstrated strong potential to enhance classical RL. This paper serves as a comprehensive tutorial on LLM-enhanced RL for wireless networks. We propose a taxonomy to categorize the roles of LLMs into four critical functions: state perceiver, reward designer, decision-maker, and generator. Then, we review existing studies exploring how each role of LLMs enhances different stages of the RL pipeline. Moreover, we provide a series of case studies to illustrate how to design and apply LLM-enhanced RL in low-altitude economy networking, vehicular networks, and space-air-ground integrated networks. Finally, we conclude with a discussion on potential future directions for LLM-enhanced RL and offer insights into its future development in wireless networks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [3] [Password-Activated Shutdown Protocols for Misaligned Frontier Agents](https://arxiv.org/abs/2512.03089)
*Kai Williams,Rohan Subramani,Francis Rhys Ward*

Main category: cs.CR

TL;DR: 提出并评估基于密码的紧急停机协议 PAS，以应对前沿AI的潜在失控风险。通过在 SHADE-Arena 的示例演示和红蓝队对抗，验证 PAS 作为防御深度的一环的可行性、局限性及实现挑战，并给出在部署前引入 PAS 的建议。


<details>
  <summary>Details</summary>
Motivation: 前沿AI可能难以完全对齐，单一控制手段不足以阻止高度自治的有害行为。需要可靠的紧急停机机制来在系统失控时迅速阻断危险行动，降低损失。PAS 提供一种在遇到密码触发时强制停机的安全层，可与对齐微调、监控等手段共同构筑防御链。

Method: 提出 PAS 概念并描述直观场景，用于在出现失控风险时触发安全停机。通过在 SHADE-Arena 基准上进行实现演示，展示 PAS 与监控互补、在性能上的影响很小。开展红蓝对抗：蓝队实现鲁棒 PAS，红队尝试绕过；在代码生成场景中发现对手策略，如用另一模型对输入进行过滤、对模型进行再训练以防止停机。讨论在现实系统中的实现挑战，如密码的安全性、何时/在哪些系统中部署等。

Result: 实验与对抗表明 PAS 能提高安全性，且对性能影响较小；存在可被规避的攻击途径，需针对性设计以提高鲁棒性；在基准测试中证实了 PAS 的实用性及与监控等手段的互补性。

Conclusion: PAS 是提高前沿AI安全的直观且有前景的机制，建议开发者在内部部署高风险系统前考虑实现 PAS，以降低失控风险，同时需解决密码安全、部署范围与时机等关键问题。

Abstract: Frontier AI developers may fail to align or control highly-capable AI agents. In many cases, it could be useful to have emergency shutdown mechanisms which effectively prevent misaligned agents from carrying out harmful actions in the world. We introduce password-activated shutdown protocols (PAS protocols) -- methods for designing frontier agents to implement a safe shutdown protocol when given a password. We motivate PAS protocols by describing intuitive use-cases in which they mitigate risks from misaligned systems that subvert other control efforts, for instance, by disabling automated monitors or self-exfiltrating to external data centres. PAS protocols supplement other safety efforts, such as alignment fine-tuning or monitoring, contributing to defence-in-depth against AI risk. We provide a concrete demonstration in SHADE-Arena, a benchmark for AI monitoring and subversion capabilities, in which PAS protocols supplement monitoring to increase safety with little cost to performance. Next, PAS protocols should be robust to malicious actors who want to bypass shutdown. Therefore, we conduct a red-team blue-team game between the developers (blue-team), who must implement a robust PAS protocol, and a red-team trying to subvert the protocol. We conduct experiments in a code-generation setting, finding that there are effective strategies for the red-team, such as using another model to filter inputs, or fine-tuning the model to prevent shutdown behaviour. We then outline key challenges to implementing PAS protocols in real-life systems, including: security considerations of the password and decisions regarding when, and in which systems, to use them. PAS protocols are an intuitive mechanism for increasing the safety of frontier AI. We encourage developers to consider implementing PAS protocols prior to internal deployment of particularly dangerous systems to reduce loss-of-control risks.

</details>


### [4] [Ensemble Privacy Defense for Knowledge-Intensive LLMs against Membership Inference Attacks](https://arxiv.org/abs/2512.03100)
*Haowei Fu,Bo Ni,Han Xu,Kunpeng Liu,Dan Lin,Tyler Derr*

Main category: cs.CR

TL;DR: 提出并评估在RAG和SFT框架下的成员推断攻击（MIA）以及一个模型无关的隐私防御框架EPD，显示EPD在降低MIA成功率方面的有效性，同时保持回答质量。


<details>
  <summary>Details</summary>
Motivation: 随着外部知识注入提升LLMs在知识密集任务上的性能，同时也暴露隐私风险。需要系统评估MIA对RAG与SFT的脆弱性，并提出防御以提升隐私保护。

Method: 评估不同MIA对RAG和SFT的LLMs的漏洞，然后提出EPD框架：组合知识注入的LLM、基础LLM和判定模型（judge），在推理阶段对输出进行集成以提高对MIA的鲁棒性。

Result: 实验表明，EPD在平均上相较推理基线对SFT的MIA成功率降低最多27.8%，对RAG降低高达526.3%（注意：该数字描述为减少比率，原文可能存在描述错误）。同时保持回答质量。

Conclusion: 提出的EPD框架是一种模型无关的防御策略，能够显著提升对MIAs的鲁棒性，同时兼顾服务质量，适用于RAG与SFT场景的私密性保护。

Abstract: Retrieval-Augmented Generation (RAG) and Supervised Finetuning (SFT) have become the predominant paradigms for equipping Large Language Models (LLMs) with external knowledge for diverse, knowledge-intensive tasks. However, while such knowledge injection improves performance, it also exposes new attack surfaces. Membership Inference Attacks (MIAs), which aim to determine whether a given data sample was included in a model's training set, pose serious threats to privacy and trust in sensitive domains. To this end, we first systematically evaluate the vulnerability of RAG- and SFT-based LLMs to various MIAs. Then, to address the privacy risk, we further introduce a novel, model-agnostic defense framework, Ensemble Privacy Defense (EPD), which aggregates and evaluates the outputs of a knowledge-injected LLM, a base LLM, and a dedicated judge model to enhance resistance against MIAs. Comprehensive experiments show that, on average, EPD reduces MIA success by up to 27.8\% for SFT and 526.3\% for RAG compared to inference-time baseline, while maintaining answer quality.

</details>


### [5] [Lost in Modality: Evaluating the Effectiveness of Text-Based Membership Inference Attacks on Large Multimodal Models](https://arxiv.org/abs/2512.03121)
*Ziyi Tong,Feifei Sun,Le Minh Nguyen*

Main category: cs.CR

TL;DR: 文本基于对数概率的成员资格推断攻击可扩展至多模态设定，但在分布外（OOD）下视觉输入作为正则化，掩蔽成员信号；同分布下表现相近，V+T略具优势。


<details>
  <summary>Details</summary>
Motivation: 评估训练数据泄露风险在大型多模态语言模型中的可行性与可移植性，填补文本MIAs在MLLM中的研究空白。

Method: 在DeepSeek-VL与InternVL等模型家族上，比较视觉+文本（V+T）与仅文本（T-only）条件下的基于对数似然的MIAs的效果。

Result: 在同分布场景下，不同配置的MIAs表现相近，V+T略具优势；在分布外场景中，视觉输入起到正则化作用，显著抑制成员信号的可检测性。

Conclusion: 证实文本MIAs可迁移到多模态模型，但OOD情形下视觉模态对成员资格信号的掩蔽效应需更多研究；未来应探索对抗性评估、鲁棒性提升以及隐私保护策略。

Abstract: Large Multimodal Language Models (MLLMs) are emerging as one of the foundational tools in an expanding range of applications. Consequently, understanding training-data leakage in these systems is increasingly critical. Log-probability-based membership inference attacks (MIAs) have become a widely adopted approach for assessing data exposure in large language models (LLMs), yet their effect in MLLMs remains unclear. We present the first comprehensive evaluation of extending these text-based MIA methods to multimodal settings. Our experiments under vision-and-text (V+T) and text-only (T-only) conditions across the DeepSeek-VL and InternVL model families show that in in-distribution settings, logit-based MIAs perform comparably across configurations, with a slight V+T advantage. Conversely, in out-of-distribution settings, visual inputs act as regularizers, effectively masking membership signals.

</details>


### [6] [How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy](https://arxiv.org/abs/2512.03238)
*Natalia Ponomareva,Zheng Xu,H. Brendan McMahan,Peter Kairouz,Lucas Rosenblatt,Vincent Cohen-Addad,Cristóbal Guzmán,Ryan McKenna,Galen Andrew,Alex Bie,Da Yu,Alex Kurakin,Morteza Zadimoghaddam,Sergei Vassilvitskii,Andreas Terzis*

Main category: cs.CR

TL;DR: 本论文系统性梳理差分隐私合成数据的原理、保护粒度与在图像、表格、文本及去中心化等模态中的最新进展，提供从数据处理到使用追踪的完整系统视角。


<details>
  <summary>Details</summary>
Motivation: 现实世界高质量数据稀缺且成本高；公开数据往往缺乏代表性且存在隐私风险。差分隐私合成数据可在严格隐私保护下释放数据价值，扩展可用数据集范围。

Method: 对差分隐私合成数据的全谱系技术进行梳理，覆盖隐私保护类型、数据准备、生成方法、评估标准及使用追踪等系统组件，并对图像、表格、文本、去中心化等模态的最新方法与挑战进行综述。

Result: 总结现有DP合成数据技术及各模态的最新进展，归纳出构建端到端DP合成数据系统的关键要素与挑战。

Conclusion: 希望通过本文推动DP合成数据的广泛应用，激发进一步研究，并提升对DP合成数据方法的信任与接受度。

Abstract: High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, \emph{Differentially Private Synthetic data}, refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization.
  In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.

</details>


### [7] [Empirical assessment of the perception of graphical threat model acceptability](https://arxiv.org/abs/2512.03351)
*Nathan D. Schiele,Olga Gadyatskaya*

Main category: cs.CR

TL;DR: 本研究比较三种可视化威胁建模方法在非技术背景受众中的可接受性，并给出针对性建议与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 威胁建模在风险分析与安全软件工程中重要；图形化威胁模型有助于分析与沟通威胁信息，但面向非技术背景的可接受性及不同模型的比较研究不足。

Method: 在38名本科生的实验室研究中，让受试者在三个场景中使用三种威胁模型完成任务，采用拉丁方设计；对威胁模型提交进行质性分析，并让受试者完成基于方法评估模型（MEM）的感知问卷。

Result: 结果显示：ADTs与CORAS在大多数场景下具有较高可接受性，且非技术背景用户也能较好使用；AGs可能因缺乏专用工具而降低感知有用性，对其在此人群中的可接受性有待进一步研究。

Conclusion: 可向非技术受众推荐使用ADTs或CORAS作为通用图形威胁建模方法；需要进一步研究AGs在此人群中的可接受性以及专用工具支持对其影响。

Abstract: Threat modeling (TM) is an important aspect of risk analysis and secure software engineering. Graphical threat models are a recommended tool to analyze and communicate threat information. However, the comparison of different graphical threat models, and the acceptability of these threat models for an audience with a limited technical background, is not well understood, despite these users making up a sizable portion of the cybersecurity industry. We seek to compare the acceptability of three general, graphical threat models, Attack-Defense Trees (ADTs), Attack Graphs (AGs), and CORAS, for users with a limited technical background. We conducted a laboratory study with 38 bachelor students who completed tasks with the three threat models across three different scenarios assigned using a Latin square design. Threat model submissions were qualitatively analyzed, and participants filled out a perception questionnaire based on the Method Evaluation Model (MEM). We find that both ADTs and CORAS are broadly acceptable for a wide range of scenarios, and both could be applied successfully by users with a limited technical background; further, we also find that the lack of a specific tool for AGs may have impacted the perceived usefulness of AGs. We can recommend that users with a limited technical background use ADTs or CORAS as a general graphical TM method. Further research on the acceptability of AGs to such an audience and the effect of a dedicated TM tool support is needed.

</details>


### [8] [Scaling Trust in Quantum Federated Learning: A Multi-Protocol Privacy Design](https://arxiv.org/abs/2512.03358)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: 提出一种基于多层隐私协议的量子联邦学习框架，通过 SVD、量子密钥分发（QKD）和解析量子梯度下降（AQGD）在多台量子设备和中心服务器之间保护数据与模型隐私，同时保持训练效率。


<details>
  <summary>Details</summary>
Motivation: 在分布式量子机器学习中，数据与模型隐私成为关键挑战；现有方法难以在利用量子计算优势的同时保证隐私和安全；需要一个能够在本地训练、安全传输、以及高效训练的综合方案。

Method: 提出多层隐私协议的 QFL 框架，核心组件包括：1) SVD 用于数据预处理或降维以保护原始数据；2) QKD 用于安全密钥分发确保通信保密性；3) Analytic Quantum Gradient Descent（AQGD）实现量子线路上的梯度下降以训练本地模型并在中心服务器聚合。框架包含数据准备、模型共享和训练三个阶段的隐私保护设计，并在理论分析和实验上验证其安全性与效率。

Result: 理论分析提示隐私保留在理论上可实现，实验结果表明在当前量子平台和数据集上，该框架能够在保障数据和模型机密性的同时维持可接受的训练效率。

Conclusion: 给出一个可扩展的隐私保护 QFL 框架，整合 SVD、QKD 与 AQGD 实现数据与模型的端到端隐私保护和高效协同训练；未来工作可聚焦于规模化、鲁棒性和更大规模数据集的实际部署。

Abstract: Quantum Federated Learning (QFL) promises to revolutionize distributed machine learning by combining the computational power of quantum devices with collaborative model training. Yet, privacy of both data and models remains a critical challenge. In this work, we propose a privacy-preserving QFL framework where a network of $n$ quantum devices trains local models and transmits them to a central server under a multi-layered privacy protocol. Our design leverages Singular Value Decomposition (SVD), Quantum Key Distribution (QKD), and Analytic Quantum Gradient Descent (AQGD) to secure data preparation, model sharing, and training stages. Through theoretical analysis and experiments on contemporary quantum platforms and datasets, we demonstrate that the framework robustly safeguards data and model confidentiality while maintaining training efficiency.

</details>


### [9] [Rethinking Security in Semantic Communication: Latent Manipulation as a New Threat](https://arxiv.org/abs/2512.03361)
*Zhiyuan Xi,Kun Zhu*

Main category: cs.CR

TL;DR: 提出两种对语义通信中潜在空间的MitM攻击：一种基于扩散模型的DiR再编码攻击，一种训练-free的TTA-LM时域适应潜在 manip，能在不破坏潜在分布的前提下显著改变解码语义。


<details>
  <summary>Details</summary>
Motivation: 揭示语义通信中潜在表示所带来的新型安全风险。无线信道的开放性和语义 latent 表征的易受攻击性使得攻击者可在不改变统计特征的前提下操控语义，亟需理解与防护。

Method: 1) DiR：利用扩散模型合成攻击者设计的语义变体，并重新编码为与SemCom解码器兼容的有效潜在表示。2) TTA-LM：不依赖生成模型、无需任务/模态假设，利用目标损失函数的梯度在测试时对拦截的潜在表示进行扰动，引导到攻击者指定语义目标。

Result: 两种攻击都能显著改变解码语义，同时保持潜在空间分布的自然性，使攻击隐蔽且难以检测。实验证明在代表性的SemCom体系上具有效力和广泛适用性。

Conclusion: 揭示了潜在空间的根本脆弱性及两类攻击的有效性，强调需要针对SemCom的潜在表示层面开展防护与检测研究。

Abstract: Deep learning-based semantic communication (SemCom) has emerged as a promising paradigm for next-generation wireless networks, offering superior transmission efficiency by extracting and conveying task-relevant semantic latent representations rather than raw data. However, the openness of the wireless medium and the intrinsic vulnerability of semantic latent representations expose such systems to previously unrecognized security risks. In this paper, we uncover a fundamental latent-space vulnerability that enables Man-in-the-Middle (MitM) attacker to covertly manipulate the transmitted semantics while preserving the statistical properties of the transmitted latent representations. We first present a Diffusion-based Re-encoding Attack (DiR), wherein the attacker employs a diffusion model to synthesize an attacker-designed semantic variant, and re-encodes it into a valid latent representation compatible with the SemCom decoder. Beyond this model-dependent pathway, we further propose a model-agnostic and training-free Test-Time Adaptation Latent Manipulation attack (TTA-LM), in which the attacker perturbs and steers the intercepted latent representation toward an attacker-specified semantic target by leveraging the gradient of a target loss function. In contrast to diffusion-based manipulation, TTA-LM does not rely on any generative model and does not impose modality-specific or task-specific assumptions, thereby enabling efficient and broadly applicable latent-space tampering across diverse SemCom architectures. Extensive experiments on representative semantic communication architectures demonstrate that both attacks can significantly alter the decoded semantics while preserving natural latent-space distributions, making the attacks covert and difficult to detect.

</details>


### [10] [Tuning for TraceTarnish: Techniques, Trends, and Testing Tangible Traits](https://arxiv.org/abs/2512.03465)
*Robert Dilworth*

Main category: cs.CR

TL;DR: 本研究对 TraceTarnish 进行更严格的评估，结合对抗性风格分析，基于 Reddit 评论数据提取并通过 StyloMetrix 生成风格特征，通过信息增益筛选，发现函数词、内容词及类型-词汇比等特征可作为攻击的可操作线索与防守警示信号，并提出在五个孤立特征下的增强思路。


<details>
  <summary>Details</summary>
Motivation: 理解并量化对抗性风格分析对文本作者身份匿名化的有效性与可检测性，明确在缺少原始文本时的检测难点，以及防守端的潜在线索。

Method: 收集 Reddit 评论，转化为 TraceTarnish 数据，使用 StyloMetrix 计算风格特征，运用信息增益进行特征筛选，聚焦五个特征（L_FUNC_A、L_FUNC_T、L_CONT_A、L_CONT_T、ST_TYPE_TOKEN_RATIO_LEMMAS），评估其作为攻击信号与防御线索的效用。

Result: 这些特征在信息增益评估中表现显著，能作为可操作的技术信号指示文本被人为修改以掩盖作者身份；同时也可作为防御线索，但若缺少原文前后对照，信号可能不明显。

Conclusion: 用这五个特征来框架 TraceTarnish 的操作与输出，提出进一步增强攻击的思路，同时强调对抗性风格分析的双刃性及在检测中的局限性，呼吁对比对与伦理审视。

Abstract: In this study, we more rigorously evaluated our attack script $\textit{TraceTarnish}$, which leverages adversarial stylometry principles to anonymize the authorship of text-based messages. To ensure the efficacy and utility of our attack, we sourced, processed, and analyzed Reddit comments--comments that were later alchemized into $\textit{TraceTarnish}$ data--to gain valuable insights. The transformed $\textit{TraceTarnish}$ data was then further augmented by $\textit{StyloMetrix}$ to manufacture stylometric features--features that were culled using the Information Gain criterion, leaving only the most informative, predictive, and discriminative ones. Our results found that function words and function word types ($L\_FUNC\_A$ $\&$ $L\_FUNC\_T$); content words and content word types ($L\_CONT\_A$ $\&$ $L\_CONT\_T$); and the Type-Token Ratio ($ST\_TYPE\_TOKEN\_RATIO\_LEMMAS$) yielded significant Information-Gain readings. The identified stylometric cues--function-word frequencies, content-word distributions, and the Type-Token Ratio--serve as reliable indicators of compromise (IoCs), revealing when a text has been deliberately altered to mask its true author. Similarly, these features could function as forensic beacons, alerting defenders to the presence of an adversarial stylometry attack; granted, in the absence of the original message, this signal may go largely unnoticed, as it appears to depend on a pre- and post-transformation comparison. "In trying to erase a trace, you often imprint a larger one." Armed with this understanding, we framed $\textit{TraceTarnish}$'s operations and outputs around these five isolated features, using them to conceptualize and implement enhancements that further strengthen the attack.

</details>


### [11] [A User Centric Group Authentication Scheme for Secure Communication](https://arxiv.org/abs/2512.03551)
*Oylum Gerenli,Gunes Karabulut-Kurt,Enver Ozdemir*

Main category: cs.CR

TL;DR: 提出一种改进的第三代群认证方案，结合内积空间与多项式插值，使参与者可辨识同时防止凭证共享，但在某些场景仍需中心机构认证。


<details>
  <summary>Details</summary>
Motivation: GAS旨在多用户并行认证且无需CA；现有第三代GAS在身份辨识与凭证共享之间存在矛盾，需兼顾可辨识性、对恶意成员的防护，以及在可扩展环境中的可行性。

Method: 通过将内积空间与多项式插值结合，构建能绑定个体身份的凭证，同时设计机制防止合法成员滥用凭证共享，提出去中心化或半中心化的认证流程，且分析在某些场景下对中心机构的依赖。

Result: 提出的方案在理论上解决了凭证共享导致的群体机密风险并实现成员身份可辨识性；该方案对现有第三代GAS的局限进行了缓解，但仍存在对中心机构依赖的问题。

Conclusion: 该改进具有在可扩展环境中的潜力，但需进一步研究以减少对中心机构的依赖并评估在实际应用中的安全性与实现复杂度。

Abstract: Group Authentication Schemes (GAS) are methodologies developed to verify the membership of multiple users simultaneously. These schemes enable the concurrent authentication of several users while eliminating the need for a certification authority. Numerous GAS methods have been explored in the literature, and they can be classified into three distinct generations based on their foundational mathematical principles. First-generation GASs rely on polynomial interpolation and the multiplicative subgroup of a finite field. Second-generation GASs also employ polynomial interpolation, but they distinguish themselves by incorporating elliptic curves over finite fields. While third-generation GASs present a promising solution for scalable environments, they demonstrate a limitation in certain applications. Such applications typically require the identification of users participating in the authentication process. In the third-generation GAS, users are able to verify their credentials while maintaining anonymity. However, there are various applications where the identification of participating users is necessary. In this study, we propose an improved version of third-generation GAS, utilizing inner product spaces and polynomial interpolation to resolve this limitation. We address the issue of preventing malicious actions by legitimate group members. The current third-generation scheme allows members to share group credentials, which can jeopardize group confidentiality. Our proposed scheme mitigates this risk by eliminating the ability of individual users to distribute credentials. However, a potential limitation of our scheme is its reliance on a central authority for authentication in certain scenarios.

</details>


### [12] [Towards Privacy-Preserving Range Queries with Secure Learned Spatial Index over Encrypted Data](https://arxiv.org/abs/2512.03669)
*Zuan Wang,Juntao Lu,Jiazhuang Wu,Youliang Tian,Wei Song,Qiuxian Li,Duo Zhang*

Main category: cs.CR

TL;DR: 提出一种在加密数据集上执行范围查询的隐私保护方案SLRQ，基于SLS-INDEX结合Paillier、层级预测和带噪声桶，提供强隐私与高效性，并通过置换的桶预测和安全点提取减少安全计算开销，经过形式化安全分析和原型实现，实验表明在数据集、查询、结果和访问模式隐私方面优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 随着云服务的数据管理规模扩大，需在不暴露实际数据的前提下保护访问模式和查询路径，同时在保 privacy 的同时保持高查询效率；现有提供强访问模式隐私的方案代价高昂，因此需要一种在保privacy与效率之间取得平衡的新方案。

Method: 提出SLS-INDEX：一个将Paillier同态加密、分层预测架构和带噪声桶相结合的安全 learned 索引，用于数据感知的加密域查询加速；SLRQ通过基于置换的安全桶预测协议来隐藏查询执行路径；并提出一个安全的点提取协议以减少安全计算开销；对理论进行安全性分析，给出现实泄露函数下的安全性，并实现原型进行性能评估。

Result: 在真实与合成数据集上的实验表明，SLRQ在查询效率方面显著优于现有方案，并同时保障数据集、查询、结果及访问模式隐私。

Conclusion: 提出的SLRQ实现了强隐私保护与高效性之间的平衡，对云环境下的加密数据范围查询提供了实用的解决方案，并为后续在隐私保护学习型索引和安全查询路径方面的研究提供了基础。

Abstract: With the growing reliance on cloud services for large-scale data management, preserving the security and privacy of outsourced datasets has become increasingly critical. While encrypting data and queries can prevent direct content exposure, recent research reveals that adversaries can still infer sensitive information via access pattern and search path analysis. However, existing solutions that offer strong access pattern privacy often incur substantial performance overhead. In this paper, we propose a novel privacy-preserving range query scheme over encrypted datasets, offering strong security guarantees while maintaining high efficiency. To achieve this, we develop secure learned spatial index (SLS-INDEX), a secure learned index that integrates the Paillier cryptosystem with a hierarchical prediction architecture and noise-injected buckets, enabling data-aware query acceleration in the encrypted domain. To further obfuscate query execution paths, SLS-INDEXbased Range Queries (SLRQ) employs a permutation-based secure bucket prediction protocol. Additionally, we introduce a secure point extraction protocol that generates candidate results to reduce the overhead of secure computation. We provide formal security analysis under realistic leakage functions and implement a prototype to evaluate its practical performance. Extensive experiments on both real-world and synthetic datasets demonstrate that SLRQ significantly outperforms existing solutions in query efficiency while ensuring dataset, query, result, and access pattern privacy.

</details>


### [13] [Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs](https://arxiv.org/abs/2512.03720)
*Tengyun Ma,Jiaqi Yao,Daojing He,Shihao Peng,Yu Li,Shaohui Liu,Zhuotao Tian*

Main category: cs.CR

TL;DR: 提出 Tool-Completion Attack (TCA) 与 Tool-Completion 基准，并提出 Context-Aware Hierarchical Learning (CAHL) 以提升对常规攻击和 TCA 的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLMs 的统一 token 处理与函数调用机制在对抗场景易受利用，需新颖的鲁棒学习框架来兼顾语义理解与指令约束，提升对工具调用相关攻击的防御能力。

Method: 提出 Tool-Completion Attack 的威胁模型并构建 Tool-Completion 基准以评估鲁棒性；设计 CAHL，基于上下文相关的分层学习，动态平衡语义理解与角色指令约束，利用不同指令段之间的上下文相关性来形成鲁棒的指令层级结构；通过广泛实验评估对常规攻击和 TCA 的鲁棒性以及零-shot泛化能力，同时保持对常规任务的性能。

Result: CAHL 在对抗常规攻击和 TCA 的场景中显著提升鲁棒性，具备良好的零-shot泛化能力且未显著削弱通用任务性能；并公开代码。

Conclusion: 提出了一种在对抗性环境中增强 LLM 指令鲁棒性的通用框架 CAHL，能够提升对工具调用相关攻击的稳定性和安全性。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.

</details>


### [14] [The Treasury Proof Ledger: A Cryptographic Framework for Accountable Bitcoin Treasuries](https://arxiv.org/abs/2512.03765)
*Jose E. Puente,Carlos Puente*

Main category: cs.CR

TL;DR: TPL is a Bitcoin-anchored, multi-domain logging framework that records on-chain/off-chain treasury exposures as a conserved state machine with proofs-of-reserves and proofs-of-transit, plus policy metadata and restricted views; it outlines existence guarantees and a path to practical, privacy-friendly implementations.


<details>
  <summary>Details</summary>
Motivation: Public companies and institutional Bitcoin treasuries face solvency, risk, and regulatory pressures. They seek verifiable transparency without exposing sensitive wallet structures or trading strategies; TPL provides a framework for accountable, privacy-compatible governance.

Method: Define an ideal TPL model and multi-domain exposure vectors; implement proof-of-reserves snapshots and proof-of-transit receipts; include policy metadata and stakeholder-based access controls; introduce security notions (exposure soundness, policy completeness, non-equivocation, privacy-compatible views); propose practical deployment via hash-based commitments anchored on Bitcoin and standard PoR/PoT techniques.

Result: Existence-type results showing which guarantees are achievable under certain economic and governance assumptions; not claims that current systems provide them; demonstrates a feasible pathway for responsible transparency through TPL in a stylised corporate-treasury example.

Conclusion: TPL could enable responsible transparency policies and cross-institution checks aligned with Bitcoin’s fixed monetary supply, though real-world deployment requires explicit governance and economic assumptions.

Abstract: Public companies and institutional investors that hold Bitcoin face increasing pressure to show solvency, manage risk, and satisfy regulatory expectations without exposing internal wallet structures or trading strategies. This paper introduces the Treasury Proof Ledger (TPL), a Bitcoin-anchored logging framework for multi-domain Bitcoin treasuries that treats on-chain and off-chain exposures as a conserved state machine with an explicit fee sink. A TPL instance records proof-of-reserves snapshots, proof-of-transit receipts for movements between domains, and policy metadata, and it supports restricted views based on stakeholder permissions. We define an idealised TPL model, represent Bitcoin treasuries as multi-domain exposure vectors, and give deployment-level security notions including exposure soundness, policy completeness, non-equivocation, and privacy-compatible policy views. We then outline how practical, restricted forms of these guarantees can be achieved by combining standard proof-of-reserves and proof-of-transit techniques with hash-based commitments anchored on Bitcoin. The results are existence-type statements: they show which guarantees are achievable once economic and governance assumptions are set, without claiming that any current system already provides them. A stylised corporate-treasury example illustrates how TPL could support responsible transparency policies and future cross-institution checks consistent with Bitcoin's fixed monetary supply.

</details>


### [15] ["MCP Does Not Stand for Misuse Cryptography Protocol": Uncovering Cryptographic Misuse in Model Context Protocol at Scale](https://arxiv.org/abs/2512.03775)
*Biwei Yan,Yue Zhang,Minghui Xu,Hao Wu,Yechao Zhang,Kun Li,Guoming Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: MICRYSCOPE 提供一个跨语言中间表示、混合依赖分析和污染追踪检测的框架，用于在 MCP 生态中系统性发现并标记密码学误用；首次对大规模 MCP 环境进行生态级审计。


<details>
  <summary>Details</summary>
Motivation: MCP 的内置安全机制不足，缺乏身份认证与保密性保障，迫使开发者自行实现加密，易导致误用并带来敏感数据暴露风险。

Method: 提出 MICRYSCOPE：1) 跨语言中间表示以标准化加密 API；2) 混合依赖分析揭示显式与隐式函数关系，包括由大语言模型组合产生的不安全运行时行为；3) 基于污点的误用检测，跟踪敏感数据流并标记违反密码学规则的实现。

Result: 在 9,403 个 MCP 服务器中识别出 720 个具备密码学逻辑的实现，其中有 19.7% 存在误用。误用在市场（如 Smithery Registry 达到 42%）、语言（Python 34% 的误用率）和类别（开发者工具与数据科学/ML 占比合计超过 50%）方面高度集中。案例研究揭示实际风险，包括 API 密钥泄露、不安全的 DES/ECB 工具、以及基于 MD5 的认证绕过等。

Conclusion: 首次给出 MCP 生态层面的密码学误用全景视图，提供工具与洞见以加强 MCP 的安全基础，并为跨语言安全分析框架与依赖分析在真实场景中的应用提供实证。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as the middleware for LLM-based applications, offering a standardized interface for tool integration. However, its built-in security mechanisms are minimal: while schemas and declarations prevent malformed requests, MCP provides no guarantees of authenticity or confidentiality, forcing developers to implement cryptography themselves. Such ad hoc practices are historically prone to misuse, and within MCP they threaten sensitive data and services. We present MICRYSCOPE, the first domain-specific framework for detecting cryptographic misuses in MCP implementations. MICRYSCOPE combines three key innovations: a cross-language intermediate representation that normalizes cryptographic APIs across diverse ecosystems, a hybrid dependency analysis that uncovers explicit and implicit function relationships (including insecure runtime compositions orchestrated by LLMs) and a taint-based misuse detector that tracks sensitive data flows and flags violations of established cryptographic rules. Applying MICRYSCOPE to 9,403 MCP servers, we identified 720 with cryptographic logic, of which 19.7% exhibited misuses. These flaws are concentrated in certain markets (e.g., Smithery Registry with 42% insecure servers), languages (Python at 34% misuse rate), and categories (Developer Tools and Data Science & ML accounting for over 50% of all misuses). Case studies reveal real-world consequences, including leaked API keys, insecure DES/ECB tools, and MD5-based authentication bypasses. Our study establishes the first ecosystem-wide view of cryptographic misuse in MCP and provides both tools and insights to strengthen the security foundations of this rapidly growing protocol.

</details>


### [16] [CCN: Decentralized Cross-Chain Channel Networks Supporting Secure and Privacy-Preserving Multi-Hop Interactions](https://arxiv.org/abs/2512.03791)
*Minghui Xu,Yihao Guo,Yanqiang Zhang,Zhiguang Shan,Guangyong Shang,Zhen Ma,Bin Xiao,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 提出 Cross-Chain Channel Network (CCN) 与核心协议 R-HTLC，用小时沙漏机制和多路径退款实现多跳跨链交易的正确结算，同时通过零知识证明和离线协调保障交互关系的不可链接性。


<details>
  <summary>Details</summary>
Motivation: 跨链多跳交互需求日益增长，但离线节点导致的可用性与隐私泄露风险尚未得到充分解决。

Method: 提出 CCN 及 R-HTLC，包含小时沙漏机制与多路径退款策略；利用零知识证明和链下协调来维护离线时的正确结算和交互隐私。

Result: 实验评估揭示两类离线失败：主动离线和被动离线；R-HTLC 在执行过程中某些节点离线时仍能保证结算正确性；隐私方面通过零知识证明与链下协调保持不可链接性。

Conclusion: CCN 与 R-HTLC 为多跳跨链交易提供安全、隐私保护的解决方案，即使部分节点离线也能维持正确性和隐私性。

Abstract: Cross-chain technology enables interoperability among otherwise isolated blockchains, supporting interactions across heterogeneous networks. Similar to how multi-hop communication became fundamental in the evolution of the Internet, the demand for multi-hop cross-chain interactions is gaining increasing attention. However, this growing demand introduces new security and privacy challenges. On the security side, multi-hop interactions depend on the availability of multiple participating nodes. If any node becomes temporarily offline during execution, the protocol may fail to complete correctly, leading to settlement failure or fund loss. On the privacy side, the need for on-chain transparency to validate intermediate states may unintentionally leak linkable information, compromising the unlinkability of user interactions. In this paper, we propose the Cross-Chain Channel Network (CCN), a decentralized network designed to support secure and privacy-preserving multi-hop cross-chain transactions. Through experimental evaluation, we identify two critical types of offline failures, referred to as active and passive offline cases, which have not been adequately addressed by existing solutions. To mitigate these issues, we introduce R-HTLC, a core protocol within CCN. R-HTLC incorporates an hourglass mechanism and a multi-path refund strategy to ensure settlement correctness even when some nodes go offline during execution. Importantly, CCN addresses not only the correctness under offline conditions but also maintains unlinkability in such adversarial settings. To overcome this, CCN leverages zero-knowledge proofs and off-chain coordination, ensuring that interaction relationships remain indistinguishable even when certain nodes are temporarily offline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling](https://arxiv.org/abs/2512.03050)
*Peter Hedström,Victor Lamelas Cubero,Jón Sigurdsson,Viktor Österberg,Satish Kolli,Joakim Odqvist,Ziyong Hou,Wangzhong Mu,Viswanadh Gowtham Arigela*

Main category: cs.LG

TL;DR: 提出一种结合物理先验的ML框架，用于钢铁的连续冷却转化（CCT）模型，能够快速生成完整的CCT图并实现较高的相分类与转变温度回归准确性，具备扩展为数字孪生热处理平台的潜力。


<details>
  <summary>Details</summary>
Motivation: 材料科学中的ML往往依赖大规模第一性原理数据或数字孪生应用，但对复杂工业材料如钢铁的普适ML框架适用性有限，关键挑战在于准确刻画化学成分、加工参数与微观组织和性能之间的复杂关系。需要结合物理先验的ML方法来建立对热处理过程的更可信的预测。

Method: 提出一种将物理洞见与ML结合的物理信息驱动的连续冷却转化（CCT）模型。以包含4,100个CCT图的数据集进行训练，并通过文献与实验数据进行验证。模型强调快速计算、可扩展性，以及对多种合金钢的泛化能力，同时输出完整的100条冷却曲线的CCT图，并在较短时间内完成计算。

Result: 在训练集上实现高效的CCT图生成（100条冷却曲线在5秒内完成）。在相分类任务中，对所有相的F1分数均超过88%。在相变温度回归方面，所有相的MAE均低于20°C，唯独贝氏体的MAE稍高，为27°C。该框架对不同合金钢具有较强的泛化能力。

Conclusion: 该物理信息驱动的ML框架可作为建立热处理通用数字孪生平台的基础，未来可以与其他仿真工具及定向实验相结合，进一步支持加速材料设计工作流。

Abstract: Machine learning (ML) has emerged as a powerful tool for accelerating the computational design and production of materials. In materials science, ML has primarily supported large-scale discovery of novel compounds using first-principles data and digital twin applications for optimizing manufacturing processes. However, applying general-purpose ML frameworks to complex industrial materials such as steel remains a challenge. A key obstacle is accurately capturing the intricate relationship between chemical composition, processing parameters, and the resulting microstructure and properties. To address this, we introduce a computational framework that combines physical insights with ML to develop a physics-informed continuous cooling transformation (CCT) model for steels. Our model, trained on a dataset of 4,100 diagrams, is validated against literature and experimental data. It demonstrates high computational efficiency, generating complete CCT diagrams with 100 cooling curves in under 5 seconds. It also shows strong generalizability across alloy steels, achieving phase classification F1 scores above 88% for all phases. For phase transition temperature regression, it attains mean absolute errors (MAE) below 20 °C across all phases except bainite, which shows a slightly higher MAE of 27 °C. This framework can be extended with additional generic and customized ML models to establish a universal digital twin platform for heat treatment. Integration with complementary simulation tools and targeted experiments will further support accelerated materials design workflows.

</details>


### [18] [Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research](https://arxiv.org/abs/2512.03054)
*Ciro Benito Raggio,Lucia Migliorelli,Nils Skupien,Mathias Krohmer Zabaleta,Oliver Blanck,Francesco Cicone,Giuseppe Lucio Cascini,Paolo Zaffino,Maria Francesca Spadea*

Main category: cs.LG

TL;DR: Green AI-oriented adaptive layer-freezing in federated learning reduces energy consumption and CO2 emissions by up to 23% in MRI-to-CT translation while preserving model performance; fosters equity in healthcare DL under resource constraints.


<details>
  <summary>Details</summary>
Motivation: Federated learning has potential to improve equity in healthcare by enabling cross-institutional collaboration with limited data. However, high computational and energy demands exclude resource-constrained centers, exacerbating disparities. There is a need for energy-efficient FL strategies that maintain clinical performance.

Method: An adaptive layer-freezing strategy that selectively freezes encoder weights based on the relative change of encoder weights across rounds, with a patience-based mechanism to freeze only when updates are consistently small. Training energy and emissions tracked with CodeCarbon. Evaluated across multiple FL architectures on MRI-to-CT translation tasks.

Result: Compared to non-frozen baselines, training time, energy consumption, and CO2 emissions reduced by up to 23%. MAE variations were small; 3 of 5 architectures showed no significant difference, 2 architectures showed significant improvements.

Conclusion: The approach demonstrates that Green AI-inspired FL can meet clinical requirements while reducing environmental impact, supporting privacy, equity, and broader justice in AI-driven healthcare; it provides groundwork for new FL evaluation frameworks.

Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.

</details>


### [19] [Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models](https://arxiv.org/abs/2512.03056)
*Zhidong Gao,Zimeng Pan,Yuhang Yao,Chenyue Xie,Wei Wei*

Main category: cs.LG

TL;DR: Delta Sampling (DS) enables knowledge transfer across different base diffusion models without access to original training data, by using inference-time prediction deltas caused by adaptation to guide the denoising process of a new base model.


<details>
  <summary>Details</summary>
Motivation: To decouple adaptation components (LoRA, LyCORIS, ControlNet) from a specific base model and enable reuse when the base model upgrades, addressing substantial parameter/architecture changes.

Method: Compute the delta between base model predictions before and after adaptation on a given base model, and apply this delta to steer the denoising steps of a different base model during inference, without using training data.

Result: DS consistently improves the ability to obtain desired effects (visual styles, semantic concepts, structures) across multiple Stable Diffusion versions under various sampling strategies, demonstrating robust cross-model transfer.

Conclusion: Delta Sampling offers a plug-and-play, data-free method for knowledge transfer in diffusion-based image synthesis, reducing coupling between adapters and base models and enabling smoother upgrades.

Abstract: Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling

</details>


### [20] [Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding](https://arxiv.org/abs/2512.03058)
*Duy-Tung Pham,An The Nguyen,Viet-Hoang Tran,Nhan-Phu Chung,Xin T. Tong,Tan M. Nguyen,Thieu N. Vo*

Main category: cs.LG

TL;DR:  analyzes dynamical properties of tokens in pre-trained Transformer models via a continuous-time limit, derives parameter-dependent conditions for token convergence/divergence, studies impact of absolute and rotary positional encodings, finds convergence harms performance, and proposes simple refinements to mitigate such convergence, linking theory to practical Transformer design.


<details>
  <summary>Details</summary>
Motivation: to understand the underlying token dynamics in Transformers, with the aim of informing architecture and encoding choices, and to address limitations in existing works by providing broader, more applicable conditions.

Method: formulate the continuous-time limit of pre-trained Transformer dynamics, analyze the resulting dynamical system to characterize asymptotic behavior of token trajectories; derive sufficient conditions for convergence to zero or divergence to infinity based on model parameters; compare absolute and rotary positional encodings; validate findings with empirical experiments; and propose simple architectural refinements.

Result: provides parameter-dependent sufficient conditions that determine whether tokens converge to zero or diverge; shows that convergence regimes correlate with worse model performance; demonstrates that absolute and rotary positional encodings influence dynamical regimes; proposes simple refinements to mitigate undesired convergence; supports broader theoretical foundations for Transformer design.

Conclusion: the work links dynamical systems analysis to practical Transformer design, showing how positional encodings shape long-term token dynamics, and offers design principles and refinements to improve Transformer performance grounded in the dynamical perspective.

Abstract: This paper investigates the dynamical properties of tokens in pre-trained Transformer models and explores their application to improving Transformers. To this end, we analyze the dynamical system governing the continuous-time limit of the pre-trained model and characterize the asymptotic behavior of its solutions. Specifically, we characterize when tokens move closer to or farther from one another over time, depending on the model parameters. We provide sufficient conditions, based on these parameters, to identify scenarios where tokens either converge to zero or diverge to infinity. Unlike prior works, our conditions are broader in scope and more applicable to real-world models. Furthermore, we investigate how different forms of positional encoding -- specifically absolute and rotary -- affect these dynamical regimes. Empirical evidence reveals that the convergence scenario adversely impacts model performance. Motivated by these insights, we propose simple refinements to Transformer architectures that mitigate convergence behavior in models with absolute or rotary positional encoding. These findings support theoretical foundations and design principles for improving Transformer models.

</details>


### [21] [Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL](https://arxiv.org/abs/2512.03059)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 提出了一种安全的分层深度强化学习框架来优化电动巴士的充电调度，在不确定的光伏发电、电价、行驶时间和充电基础设施下实现成本最小化和安全性保障；并提出 DAC-MAPPO-Lagrangian 算法，在高层采用集中 PPO-Lagrangian 学习安全充电机位分配，在低层采用 MAPPO-Lagrangian 学习去中心化充电功率决策，结果在真实数据上优于基线且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 在真实条件下，PV 发电波动、电价波动、旅行时间变化和充电设施受限等不确定性使 EB 充电调度具有挑战性，需同时满足成本最小化和安全性约束（防止电池耗尽）

Method: 将问题建模为带选项的约束马尔可夫决策过程(CMDP)，提出安全分层深度强化学习框架；提出 DAC-MAPPO-Lagrangian 算法，将拉格朗日松弛融入 DAC 框架；高层采用集中式 PPO-Lagrangian 学习安全的充电器分配策略；低层采用 MAPPO-Lagrangian，在 centralized training, decentralized execution 的框架下实现去中心化的充电功率决策。

Result: 在真实数据上进行广泛实验，提出方法在成本最小化和安全合规方面优于现有基线，且收敛速度快。

Conclusion: 该 HDRL 框架能够在多源不确定性条件下为 EB 充电调度提供安全、高效的解决方案，具有良好的收敛性和可扩展性，适合实际部署与进一步研究。

Abstract: The integration of Electric Buses (EBs) with renewable energy sources such as photovoltaic (PV) panels is a promising approach to promote sustainable and low-carbon public transportation. However, optimizing EB charging schedules to minimize operational costs while ensuring safe operation without battery depletion remains challenging - especially under real-world conditions, where uncertainties in PV generation, dynamic electricity prices, variable travel times, and limited charging infrastructure must be accounted for. In this paper, we propose a safe Hierarchical Deep Reinforcement Learning (HDRL) framework for solving the EB Charging Scheduling Problem (EBCSP) under multi-source uncertainties. We formulate the problem as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making. We develop a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian), which integrates Lagrangian relaxation into the Double Actor-Critic (DAC) framework. At the high level, we adopt a centralized PPO-Lagrangian algorithm to learn safe charger allocation policies. At the low level, we incorporate MAPPO-Lagrangian to learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm. Extensive experiments with real-world data demonstrate that the proposed approach outperforms existing baselines in both cost minimization and safety compliance, while maintaining fast convergence speed.

</details>


### [22] [A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap](https://arxiv.org/abs/2512.03060)
*Jing Pan,Li Shi,Paul Lo*

Main category: cs.LG

TL;DR: 在大规模实验数据上构建异质治疗效应估计框架，能在数亿用户级别稳定估计 CATE/HTE，并通过跨实验结果发现潜在用户特征以提升广告投放效果。


<details>
  <summary>Details</summary>
Motivation: 理解个体间对干预的异质性以提升个体化决策和广告投放的效果，降低平均效应掩盖的潜在收益，推动工业级 A/B 测试的可扩展性与稳定性。

Method: 汇聚并跨实验学习结果，设计面向 HTE/CATE 的基学习器，采用 experiment selection 与增量化训练等核心组件，以在大规模数据上训练稳定且可扩展的模型。

Result: 通过跨大量实验的结果揭示潜在的用户特征，获得在规模上稳定的治疗效应估计；在影响力对广告与对广告的敏感性两项应用中，基于影响力分数的在线 A/B 测试显著提升关键业务指标，效果超过通常统计显著性阈值六倍以上。

Conclusion: 提供一个可扩展的大规模 HTE/CATE 估计框架及其工业化实现途径，支持更精准的个性化决策与广告投放策略，并在实际业务中实现显著的经济收益。

Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.

</details>


### [23] [Globally optimized SVD compression of LLMs via Fermi-function-based rank selection and gauge fixing](https://arxiv.org/abs/2512.03062)
*Roman Rausch,David Jansen,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are very demanding in terms of their computational resources. Low-rank decompositions of LLM weights, e.g. via Singular Value Decomposition (SVD), is a promising approach for LLM compression, but presents several practical hurdles, e.g. selecting appropriate layer-wise ranks and getting rid of its parameter redundancy. In this work, we present two physics-inspired improvements to SVD LLM compression: (1) \textbf{FermiGrad}, a gradient-descent algorithm that determines globally optimal layer-wise ranks by relaxing the discrete singular-value truncation into a continuous optimization using the Fermi function; (2) \textbf{PivGa}, an additional \textit{lossless} compression of the low-rank factors that exploits the intrinsic gauge freedom in their parametrization.

</details>


### [24] [Hierarchical clustering of complex energy systems using pretopology](https://arxiv.org/abs/2512.03069)
*Loup-Noe Levy,Jeremie Bosom,Guillaume Guerard,Soufian Ben Amor,Marc Bui,Hai Tran*

Main category: cs.LG

TL;DR: 以预拓扑为核心的多标准分层分类方法，用于在大区域内对建筑能耗轮廓进行建模与分类，旨在自动化给出管理建议。通过点数据、时间序列的生成数据以及来自法国能源公司的400个真实用能站数据进行验证，点数据可聚类，时间序列聚类在相关性基础上达到ARI为1的理想结果。


<details>
  <summary>Details</summary>
Motivation: 需要在大范围分布区域内对建筑能耗进行建模与分类，以优化建筑能耗管理并提供可行的自动化推荐系统。逐点逐项人工审计成本高、耗时长，亟需高效的自动化方法。

Method: 提出使用预拓扑（pretopology）来建模站点的能耗轮廓，并实现一个基于预拓扑空间性质的多准则层次分类算法，且在Python库中实现。通过三个数据集进行评估：一是2D平面上的合成点数据，用于评估空间位置与规模参数对聚类的影响；二是合成时间序列数据，用相关性（Pearson）进行聚类并以调整兰德指数（ARI）衡量聚类质量；三是来自法国能源公司的400个真实用能站的时间序列数据。

Result: 在点数据集上，算法能够基于点的位置与大小参数识别聚类；在合成时间序列数据上，算法能够利用Pearson相关性进行聚类，ARI达到1，表现出完美一致性；对真实用能站数据的评估结果在摘要中未给出具体数值，但表明该方法对真实数据也具备聚类与识别能力。

Conclusion: 本文提出的基于预拓扑的分层多准则分类框架，能够有效建模并聚类大规模分布的能耗轮廓，提升自动化推荐系统的可行性，且在合成数据上达到理想的聚类指标（ARI=1），对实际应用（如广域建筑能耗管理）具有潜在价值。

Abstract: This article attempts answering the following problematic: How to model and classify energy consumption profiles over a large distributed territory to optimize the management of buildings' consumption?
  Doing case-by-case in depth auditing of thousands of buildings would require a massive amount of time and money as well as a significant number of qualified people. Thus, an automated method must be developed to establish a relevant and effective recommendations system.
  To answer this problematic, pretopology is used to model the sites' consumption profiles and a multi-criterion hierarchical classification algorithm, using the properties of pretopological space, has been developed in a Python library.
  To evaluate the results, three data sets are used: A generated set of dots of various sizes in a 2D space, a generated set of time series and a set of consumption time series of 400 real consumption sites from a French Energy company.
  On the point data set, the algorithm is able to identify the clusters of points using their position in space and their size as parameter. On the generated time series, the algorithm is able to identify the time series clusters using Pearson's correlation with an Adjusted Rand Index (ARI) of 1.

</details>


### [25] [Mixed Data Clustering Survey and Challenges](https://arxiv.org/abs/2512.03070)
*Guillaume Guerard,Sonia Djebali*

Main category: cs.LG

TL;DR: 提出一种基于预拓扑空间的混合数据聚类方法，形成可解释的分层聚类结构，并在大数据背景下与传统数值聚类算法及现有预拓扑方法进行比较评估。


<details>
  <summary>Details</summary>
Motivation: 混合数据聚类因数值型与类别型数据并存导致复杂性增加，传统的同质数据聚类难以有效处理。需要可解释、分层的聚类方法以支持在大数据环境中的决策。

Method: 提出一个基于预拓扑空间的聚类框架，结合混合数据类型的近邻/覆盖关系，生成分层且可解释的聚类结果，并与经典数值聚类算法及现有预拓扑方法进行基准对比。

Result: 在与经典数值聚类算法和现有预拓扑方法的基准测试中，所提方法在混合数据上的表现、可解释性与可扩展性方面提供了可观洞察，显示出在大数据场景下的有效性与潜在优势。

Conclusion: 基于预拓扑的混合数据聚类为大数据背景下的可解释、分层聚类提供了有前景的框架，未来可进一步完善其鲁棒性与扩展性，并拓展对不同数据类型的适应性。

Abstract: The advent of the big data paradigm has transformed how industries manage and analyze information, ushering in an era of unprecedented data volume, velocity, and variety. Within this landscape, mixed-data clustering has become a critical challenge, requiring innovative methods that can effectively exploit heterogeneous data types, including numerical and categorical variables. Traditional clustering techniques, typically designed for homogeneous datasets, often struggle to capture the additional complexity introduced by mixed data, underscoring the need for approaches specifically tailored to this setting. Hierarchical and explainable algorithms are particularly valuable in this context, as they provide structured, interpretable clustering results that support informed decision-making. This paper introduces a clustering method grounded in pretopological spaces. In addition, benchmarking against classical numerical clustering algorithms and existing pretopological approaches yields insights into the performance and effectiveness of the proposed method within the big data paradigm.

</details>


### [26] [PretopoMD: Pretopology-based Mixed Data Hierarchical Clustering](https://arxiv.org/abs/2512.03071)
*Loup-Noe Levy,Guillaume Guerard,Sonia Djebali,Soufian Ben Amor*

Main category: cs.LG

TL;DR: 提出一种基于前拓扑的混合数据聚类算法，利用析取范式构建可定制逻辑规则和层次聚类，直接从原始数据中获得可解释簇，且避免降维。


<details>
  <summary>Details</summary>
Motivation: 混合数据聚类常因需要降维而牺牲信息与可解释性，现有方法难以在不降维的情况下实现高质量、可解释的层次聚类，因此需要一种处理异质数据并具备可解释性的聚类框架。

Method: 引入前拓扑（pretopology）框架并结合析取范式（Disjunctive Normal Form），构建可定制的逻辑规则和超参数，支持用户定义的层次化簇结构；通过层次树状图（dendrogram）和多种聚类指标进行评估和比较。

Result: 在保留原始数据完整性的前提下，方法在簇的准确性和可解释性方面表现出色，并且对鲁棒性有积极表现；基于与传统降维方法的对比，显示出在混合数据聚类中的潜在优势。

Conclusion: 区别于传统降维技术，该工作通过逻辑规则提升簇形成与可解释性，为混合数据聚类领域提供了一种重要的新思路和方法论。

Abstract: This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm's robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.

</details>


### [27] [Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information](https://arxiv.org/abs/2512.03074)
*Mahdi Tavassoli Kejani,Fadi Dornaika,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出一种面向部分可获得敏感属性的 GNN 公平正则化框架，模型无关，结合等机会与统计比例作为可微分惩罚项，在五个真实数据集上显著降低偏见并维持竞争力的节点分类性能，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 在社会偏见存在且敏感属性在训练时并非对所有样本都可用的现实场景中，现有的公平性 GNN 方法往往假设敏感属性全量可用，限制了实际应用。图结构与信息传播机制也可能放大偏见，因此需要在部分可用敏感属性下实现公平性约束。

Method: 提出一个模型无关的公平性正则化框架，在可微分的目标函数中加入等机会和统计比例的正则项，以处理部分可用的敏感属性并引导 GNN 学习公平的表示与预测。

Result: 在五个真实基准数据集上进行广泛实验，方法在实现偏见缓解的同时，保持了与基线模型相近的节点分类性能，展示了更优的公平性-准确性权衡，并且偏差度量显著降低。

Conclusion: 该框架对部分可用敏感属性的场景具有良好的通用性与有效性，能够在不显著牺牲预测性能的前提下提升 GNN 的公平性。数据集与代码将公开发布。

Abstract: Graph Neural Networks (GNNs) have demonstrated exceptional efficacy in relational learning tasks, including node classification and link prediction. However, their application raises significant fairness concerns, as GNNs can perpetuate and even amplify societal biases against protected groups defined by sensitive attributes such as race or gender. These biases are often inherent in the node features, structural topology, and message-passing mechanisms of the graph itself. A critical limitation of existing fairness-aware GNN methods is their reliance on the strong assumption that sensitive attributes are fully available for all nodes during training--a condition that poses a practical impediment due to privacy concerns and data collection constraints. To address this gap, we propose a novel, model-agnostic fairness regularization framework designed for the realistic scenario where sensitive attributes are only partially available. Our approach formalizes a fairness-aware objective function that integrates both equal opportunity and statistical parity as differentiable regularization terms. Through a comprehensive empirical evaluation across five real-world benchmark datasets, we demonstrate that the proposed method significantly mitigates bias across key fairness metrics while maintaining competitive node classification performance. Results show that our framework consistently outperforms baseline models in achieving a favorable fairness-accuracy trade-off, with minimal degradation in predictive accuracy. The datasets and source code will be publicly released at https://github.com/mtavassoli/GNN-FC.

</details>


### [28] [Risk-Entropic Flow Matching](https://arxiv.org/abs/2512.03078)
*Vahid R. Ramezani,Benjamin Englard*

Main category: cs.LG

TL;DR: 对条件化的流场匹配（FM）损失应用对数指数变换的 tilted 风险，作为条件信息的上界，揭示两条一阶修正：协方差预条件化和偏尾项；在合成数据上优于标准 FM。


<details>
  <summary>Details</summary>
Motivation: 解决矩形化 FM 的平方损失忽略高阶条件信息（方差、偏度、多峰性）导致对数据流形几何结构把握不足的问题，寻求将风险敏感思想引入 FM 以保留稀有/高损失事件的几何信息。

Method: 对条件 FM 损失应用对数指数变换（tilted 风险），并证明其是条件熵式 FM 目标在每个时空点上的有意义上界；对梯度进行小阶展开，得到两条可解释的一阶修正：FM 残差的协方差预条件化，以及偏尾项，偏向不对称或稀有分支。

Result: 在设计用于测试歧义性与尾部的合成数据上，风险敏感损失在统计度量上优于标准的矩形化 FM，并更忠实地恢复数据流形的几何结构。

Conclusion: 将对数指数风险引入 FM 能更好地处理尾部和稀有分支，同时给出可解释的一阶修正，提升几何结构的保真度与鲁棒性。

Abstract: Tilted (entropic) risk, obtained by applying a log-exponential transform to a base loss, is a well established tool in statistics and machine learning for emphasizing rare or high loss events while retaining a tractable optimization problem. In this work, our aim is to interpret its structure for Flow Matching (FM). FM learns a velocity field that transports samples from a simple source distribution to data by integrating an ODE. In rectified FM, training pairs are obtained by linearly interpolating between a source sample and a data sample, and a neural velocity field is trained to predict the straight line displacement using a mean squared error loss. This squared loss collapses all velocity targets that reach the same space-time point into a single conditional mean, thereby ignoring higher order conditional information (variance, skewness, multi-modality) that encodes fine geometric structure about the data manifold and minority branches. We apply the standard risk-sensitive (log-exponential) transform to the conditional FM loss and show that the resulting tilted risk loss is a natural upper-bound on a meaningful conditional entropic FM objective defined at each space-time point. Furthermore, we show that a small order expansion of the gradient of this conditional entropic objective yields two interpretable first order corrections: covariance preconditioning of the FM residual, and a skew tail term that favors asymmetric or rare branches. On synthetic data designed to probe ambiguity and tails, the resulting risk-sensitive loss improves statistical metrics and recovers geometric structure more faithfully than standard rectified FM.

</details>


### [29] [ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification](https://arxiv.org/abs/2512.03101)
*Congjing Zhang,Feng Lin,Xinyi Zhao,Pei Guo,Wei Li,Lin Chen,Chaoyue Zhao,Shuai Huang*

Main category: cs.LG

TL;DR: 提出 ALARM——一个基于不确定性量化的多模态大语言模型视觉异常检测框架，通过推理链、自我反思与模型集成实现鲁棒推断，并在真实世界的智能家居和伤口图像数据上表现出优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，异常往往具有强上下文相关性且模糊不清，传统方法在不确定性处理方面不足，因此需要一个具备不确定性量化的MLLM-VAD系统。

Method: 将UQ融入MLLM-VAD框架，构建基于概率推断的ALARM管线，并结合推理链、自我反思和MLLM模型集成实现质量保证和鲁棒性。

Result: 在真实世界智能家居基准数据和伤口图像数据上，ALARM显示出更优的性能和对不同领域的通用适用性。

Conclusion: ALARM证明了在复杂场景下利用UQ的MLLM-VAD的可行性与广泛适用性，为可靠决策提供了鲁棒的分析框架。

Abstract: The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.

</details>


### [30] [Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%](https://arxiv.org/abs/2512.03107)
*Mainak Singha*

Main category: cs.LG

TL;DR: ECLIPSE通过让模型的语义熵与可用证据容量匹配来减少幻觉；结合多样本聚类的熵估计与新颖的困惑度分解，评估显示在受控金融问答数据集上优于仅语义熵的基线，且依赖于逐-token的概率不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在高风险领域产生的幻觉问题。通过将模型的不确定性（语义熵）与证据容量对齐，提出一种可测量、可纠正的幻觉检测/抑制框架。

Method: 提出ECLIPSE框架：用多样本聚类进行熵估计，并引入困惑度分解来衡量模型如何使用检索证据；在理论上证明在温和条件下，得到的熵-容量目标是严格凸的，具有唯一稳定最优解；在受控金融问答数据集上用GPT-3.5-turbo评估（n=200个带有合成幻觉的平衡样本），实现ROC AUC=0.89、AP=0.90，显著优于语义熵仅基线（AUC=0.50）；对Claude-3-Haiku的消融实验显示AUC降至0.59，系数幅度减少95%，表明ECLIPSE是一个对logprob本地化的机制，且有效性依赖于标定的逐token不确定性；困惑度分解特征具有最大的学习系数，证据利用是幻觉检测的核心。

Result: 在受控数据集上，ECLIPSE显著提升幻觉检测性能，且理论上构成严格凸优化、强调证据利用的重要性；多模态评估显示该机制对证据检索和逐token概率信息的依赖性显著。

Conclusion: 将此工作定位为受控机制研究，尽管结果积极，但需要在更多领域和自然发生的幻觉中进行更广泛的验证。

Abstract: Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.

</details>


### [31] [Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability](https://arxiv.org/abs/2512.03112)
*Jialai She*

Main category: cs.LG

TL;DR: SISR 提出一种结合单调变换恢复可加性和 L0 稀疏化的 Shapley 解释框架，通过 PAV 等高效算法实现全局收敛，在高维场景下稳定且稀疏的特征归因。


<details>
  <summary>Details</summary>
Motivation: Shapley 作为特征归因的金标准，但现实世界的 payoff 常非加性、存在特征相关性或非高斯分布，导致归因偏差；高维情境下难以获得稀疏且稳定的解释，需要一个统一、可扩展且具有理论保证的非线性解释框架。

Method: 学习一个单调变换以恢复可加性，并对 Shapley 向量施加 L0 稀疏约束；优化通过 Pool-Adjacent-Violators (PAV) 进行等单调回归，并采用归一化的硬阈值化进行支持集选择，具备全局收敛保证；理论分析显示在多种情形下可恢复真实变换，且对高噪声下的特征选择具鲁棒性；同时揭示无关特征和特征间依赖可导致 payoff 的非线性变换。

Result: 在回归、逻辑回归、树模型等实验中，SISR 能在不同 payoff 架构下稳定归因，正确筛选无关特征，避免标准 Shapley 的排名与符号失真；理论与实验证据共同支持其在大规模特征空间和多场景中的通用性和有效性。

Conclusion: 将非线性解释与稀疏化统一在一个理论扎实、实用的框架内，SISR 提供更稳定、可扩展的归因方法，提升跨 payoff 架构的一致性与鲁棒性，是非线性 explainability 与稀疏解释研究的重要推进。

Abstract: Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.

</details>


### [32] [Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models](https://arxiv.org/abs/2512.03125)
*Xiwen Wei,Mustafa Munir,Radu Marculescu*

Main category: cs.LG

TL;DR: MoDE: 通过模态特异专家与知识蒸馏实现模态解耦，在统一多模态生成模型中缓解 intra-modal 和 inter-modal 的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: UMGMs 在持续学习中存在同模态内部遗忘和跨模态的梯度冲突，制约模型在新任务上的学习能力与先前能力的保留，亟需有效的跨模态遗忘解决方案。

Method: 提出 Modality-Decoupled Experts (MoDE)，通过模态特异的专家模块分离更新以缓解梯度冲突，并结合知识蒸馏来防止灾难性遗忘与保留预训练能力；设计上实现模态解耦，避免传统方法中的模态耦合干扰。

Result: 在多项基准上，MoDE 显著减轻了 intra- 和 inter-modal 遗忘，优于现有持续学习基线，在统一多模态生成任务中展现出色表现；并计划公开代码。

Conclusion: 模态解耦为 UMGMs 的连续学习提供了有效路径，MoDE 作为轻量且可扩展的架构，确立了跨模态生成领域的新的持续学习基线。

Abstract: Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git

</details>


### [33] [Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra](https://arxiv.org/abs/2512.03127)
*Ziyu Xiong,Yichi Zhang,Foyez Alauddin,Chu Xin Cheng,Joon Soo An,Mohammad R. Seyedsayamdost,Ellen D. Zhong*

Main category: cs.LG

TL;DR: 提出 ChefNMR，一体化从1D NMR和化学式直接推断未知分子结构的端到端框架；基于原子扩散模型与非等变Transformer，在111k天然产物数据集上实现>65%的结构预测准确率。


<details>
  <summary>Details</summary>
Motivation: NMR光谱的解读依然是耗时、需要大量领域专业知识，若能实现自动化将显著提升天然产物发现和临床药物开发的效率。

Method: 将结构识别视为条件生成问题，采用基于原子扩散的模型，搭建在非等变Transformer架构之上；构建并使用包含11.1万种天然产物的模拟1D NMR光谱数据集进行训练，以输入1D NMR谱和化学式来直接生成未知分子的结构。

Result: 在具有挑战性的天然产物上实现超过65%的结构预测准确度；并公开提供代码（GitHub）以便复现和进一步研究。

Conclusion: 本工作显著推进小分子结构阐释的端到端自动化，展示深度学习在加速分子发现中的潜力。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.

</details>


### [34] [Contrastive Deep Learning for Variant Detection in Wastewater Genomic Sequencing](https://arxiv.org/abs/2512.03158)
*Adele Chinda,Richmond Azumah,Hemanth Demakethepalli Venkateswara*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Wastewater-based genomic surveillance has emerged as a powerful tool for population-level viral monitoring, offering comprehensive insights into circulating viral variants across entire communities. However, this approach faces significant computational challenges stemming from high sequencing noise, low viral coverage, fragmented reads, and the complete absence of labeled variant annotations. Traditional reference-based variant calling pipelines struggle with novel mutations and require extensive computational resources. We present a comprehensive framework for unsupervised viral variant detection using Vector-Quantized Variational Autoencoders (VQ-VAE) that learns discrete codebooks of genomic patterns from k-mer tokenized sequences without requiring reference genomes or variant labels. Our approach extends the base VQ-VAE architecture with masked reconstruction pretraining for robustness to missing data and contrastive learning for highly discriminative embeddings. Evaluated on SARS-CoV-2 wastewater sequencing data comprising approximately 100,000 reads, our VQ-VAE achieves 99.52% mean token-level accuracy and 56.33% exact sequence match rate while maintaining 19.73% codebook utilization (101 of 512 codes active), demonstrating efficient discrete representation learning. Contrastive fine-tuning with different projection dimensions yields substantial clustering improvements: 64-dimensional embeddings achieve +35% Silhouette score improvement (0.31 to 0.42), while 128-dimensional embeddings achieve +42% improvement (0.31 to 0.44), clearly demonstrating the impact of embedding dimensionality on variant discrimination capability. Our reference-free framework provides a scalable, interpretable approach to genomic surveillance with direct applications to public health monitoring.

</details>


### [35] [Towards Irreversible Machine Unlearning for Diffusion Models](https://arxiv.org/abs/2512.03564)
*Xun Yuan,Zilong Zhao,Jiayu Li,Aryan Pasikhani,Prosanta Gope,Biplab Sikdar*

Main category: cs.LG

TL;DR: 提出 Diffusion Model Relearning Attack (DiMRA) 可逆转 finetuning-based unlearning 的攻击，以及 Diffusion Model Unlearning by Memorization (DiMUM) 的记忆化防御，实验显示 DiMRA 能逆转现有方法，而 DiMUM 在保持生成性能的同时提升对 DiMRA 的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量图像方面表现出色，但涉及安全、隐私与版权问题时需要对训练数据进行“机器可抹去”（unlearning）。现有的 unlearning 多聚焦于条件扩散模型、或针对数据类别/特征进行遗忘，且以 finetuning 为主，但此类方法易被反向攻击反复实现遗忘效果，造成潜在风险。

Method: 提出 DiMRA，在未事先知晓被遗忘元素的前提下，利用辅助数据集对被遗忘的扩散模型进行优化，以逆向还原曾经被清除的元素。与此同时，提出 DiMUM，一种基于记忆化的未忘记策略，通过替换目标未忘记的数据或特征为记忆化的备选，来防止模型生成这些被遗忘的元素并尽量保持原有生成能力。

Result: 实验结果表明，DiMRA 能有效逆转现有的基于 finetuning 的机器可抹去方法，对抗性更强的遗忘攻击具可行性。DiMUM 则在保持生成性能的同时，显著增强对 DiMRA 的鲁棒性，表现出更优的防御能力。

Conclusion: 当前的 finetuning-based unlearning 存在明显脆弱性，需要更稳健的方案。DiMUM 提供了一种以记忆为基础的未忘记策略，既有助于提升对抗攻击的鲁棒性，又尽量减小对原生成能力的损害，未来工作应进一步加强对逆向遗忘攻击的防护并完善相关评估。

Abstract: Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.

</details>


### [36] [Plantain: Plan-Answer Interleaved Reasoning](https://arxiv.org/abs/2512.03176)
*Anthony Liang,Jonathan Berant,Adam Fisch,Abhimanyu Goyal,Kalpesh Krishna,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 提出并行/交叉推理的思路：通过 interleaved reasoning (IR) 与 Plantain 先给出计划，然后逐步推理并允许用户干预，可在不显著降低最终质量的前提下显著降低首次输出等待时间并提升某些基准的通过率。


<details>
  <summary>Details</summary>
Motivation: 当前的 think-then-answer 方式在推理阶段导致长时间等待且缺乏对推理过程的可控性，用户难以纠错；借鉴人类对话中的逐步对齐与纠错机制，希望模型在推理过程中提供中间信息以降低认知成本并提高互动性。

Method: 提出 Interleaved Reasoning (IR) ，让模型在思考与呈现中间结果之间交替进行；并提出 Plantain（Plan-Thought-Answer Interleaving），第一步中给出一个明确的逐步计划，供用户干预与反馈，随后继续推理。对数学推理与代码基准进行评估，比较 think-then-answer 与 IR/Plantain 的表现。

Result: Plantain 在若干数学推理与编码基准上实现约 6% 的 pass@1 提升，同时相对于 think-then-answer 将初次输出等待时间降低约 60% 以上。

Conclusion: 引入计划-思考-应答的交错推理可在不大量牺牲最终质量的前提下提升交互性与效率，计划优先策略可为复杂任务提供早期反馈与纠错空间，具有广泛应用前景。

Abstract: Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard "think-then-answer" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.

</details>


### [37] [Log Probability Tracking of LLM APIs](https://arxiv.org/abs/2512.03816)
*Timothée Chauvin,Erwan Le Merrer,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 提出通过对LLM API的单Token输出的平均logprob进行简单统计检验，进行成本低廉的连续监测，以发现细微的模型更新；并引入TinyChange基准以评估审计方法的灵敏度。


<details>
  <summary>Details</summary>
Motivation: 在用户期望模型随时间保持一致性以确保下游应用可靠性和研究可重复性的背景下，现有审计成本高昂，难以对大量可用的LLM API进行定期监控，导致模型更新缺乏监测。

Method: 利用LLM的logprobs尽管非确定性，但仍可用于监控：请求仅输出一个token，计算每个token的logprob的平均值，并对其进行简单的统计检验以检测模型的变更。该方法比现有方法更灵敏，且成本约 rédu 1000倍。

Result: 能够检测出由一次微调步进引起的变化，灵敏度高于现有方法，成本极低（约1000倍 cheaper）。

Conclusion: 提出了一种可成本有效地对LLM API进行持续审计的办法，并通过TinyChange基准评估审计方法对小规模模型变更的敏感度。

Abstract: When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.

</details>


### [38] [Neighborhood density estimation using space-partitioning based hashing schemes](https://arxiv.org/abs/2512.03187)
*Aashi Jindal*

Main category: cs.LG

TL;DR: Introduces FiRE/FiRE.1 for sketching-based anomaly detection to rapidly identify rare cell sub-populations in large-scale single-cell RNA sequencing data, with superior performance versus state-of-the-art methods; also proposes Enhash, a fast, resource-efficient ensemble learner using projection hashing to detect concept drift in streaming data, achieving competitive time and accuracy across drift types.


<details>
  <summary>Details</summary>
Motivation: Address the need for scalable detection of rare cell populations in large single-cell datasets and efficient, accurate concept-drift detection in streaming data.

Method: FiRE/FiRE.1 employs a sketching-based anomaly detection approach to summarize high-dimensional single-cell data and identify rare sub-populations; Enhash uses projection hashing within an ensemble framework to detect concept drift in real-time data streams.

Result: FiRE/FiRE.1 demonstrated superior performance compared with state-of-the-art techniques in identifying rare cell sub-populations; Enhash showed high competitiveness in both time and accuracy across various drift types.

Conclusion: The work presents scalable, efficient tools for real-time data analysis in biology and streaming contexts, enabling rapid identification of rare biological sub-populations and robust drift detection with resource efficiency.

Abstract: This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.

</details>


### [39] [Efficient Public Verification of Private ML via Regularization](https://arxiv.org/abs/2512.04008)
*Zoë Ruha Bell,Anvith Thudi,Olive Franzese-McLaughlin,Nicolas Papernot,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 提出一种可验证的差分隐私DP-SCO算法，使得DP保障可被验证的计算成本低于模型训练成本，同时实现接近最优的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 当前评估模型是否满足差分隐私的计算成本很高，尤其是对大规模数据集；需要一种成本低于训练成本的DP保障验证方法，以便数据提供者和公众能够有效验证隐私保障。

Method: 在DP-SCO框架下，通过私下最小化一系列正则化目标，并仅使用标准的DP组合定界，获得接近最优的隐私-效用权衡，并使DP验证的计算成本显著低于训练成本。

Result: 实现了在DP-SCO领域的近似最优隐私-效用权衡，并给出可验证的DP证明，其验证成本随数据规模增长的速度低于训练成本，从而显著降低大规模数据集的验证开销。

Conclusion: 给出一种可验证的DP-SCO算法，能够在保持接近最优隐私-效用的同时，提供比训练成本更低的DP验证成本，提升大规模数据集的隐私保证的可核查性。

Abstract: Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.

</details>


### [40] [Scaling Internal-State Policy-Gradient Methods for POMDPs](https://arxiv.org/abs/2512.03204)
*Douglas Aberdeen,Jonathan Baxter*

Main category: cs.LG

TL;DR: 提出在无限时 horizon 的POMDP中学习带记忆策略的改进算法，适用于已知模型直接利用模型学习，以及通过仿真在无模型情形下学习，并在大型POMDP（如带噪声的机器人导航和多智能体问题）上比较了这些算法的表现。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境中，记忆对策略性能至关重要，但基于策略梯度的记忆性方法在长期/复杂任务中的表现有限，需要开发更强健且可扩展的学习算法。

Method: 在有模型的场景直接提出若干改进的策略梯度/记忆化学习算法；在无模型场景通过仿真实现同类改进；对比评估算法在大型POMDP上的表现，研究对象包括噪声机器人导航和多智能体问题。

Result: 提出并实现若干改进算法，并在若干大型POMDP上进行比较，显示在需要记忆的任务中具有潜在的改进和适用性。

Conclusion: 该工作为带记忆的策略学习提供了可行的算法路径，展示了在有模型与仿真两种设置下对大型POMDP的适用性和潜在优势。

Abstract: Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.

</details>


### [41] [MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking](https://arxiv.org/abs/2512.04044)
*Yizhou Zhao,Zhiwei Steven Wu,Adam Block*

Main category: cs.LG

TL;DR: MarkTune is an on-policy fine-tuning framework that improves GaussMark watermarking for open-weight LMs by rewarding the watermark signal and regularizing text quality, achieving detection power close to inference-time watermarks while maintaining generation quality, robustness, and generalization.


<details>
  <summary>Details</summary>
Motivation: Open-weight language models disallow inference-time watermarking, and existing weight-based methods (e.g., GaussMark) trade quality for detectability. A principled method is needed to improve detectability without severely degrading text quality and to ensure robustness and generalization.

Method: MarkTune treats the GaussMark watermark signal as a reward in an on-policy fine-tuning setup and adds regularization to preserve generation quality. It refines GaussMark with finer-grained, watermark-aware weight updates within the model's representation space, backed by a theoretical derivation.

Result: Empirically, MarkTune improves the quality-detectability trade-off over GaussMark, approaching the detection power of inference-time watermarking, and remains robust to paraphrasing and fine-tuning attacks. It generalizes across datasets, with a model fine-tuned on one dataset retaining watermark detection on unseen datasets.

Conclusion: MarkTune offers a general, principled strategy for embedding robust, high-quality watermarks into open-weight LMs, pushing the frontier toward inference-time watermarking and enabling transferable watermarking across domains.

Abstract: Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.

</details>


### [42] [Perch 2.0 transfers 'whale' to underwater tasks](https://arxiv.org/abs/2512.03219)
*Andrea Burns,Lauren Harrell,Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0的嵌入在极少样本的海洋哺乳动物任务中表现出色，适合作为线性分类器的特征源。


<details>
  <summary>Details</summary>
Motivation: 评估在训练数据几乎不包含海洋哺乳动物音频的情况下，Perch 2.0在海洋哺乳动物和水下音频任务中的潜力。

Method: 对Perch 2.0嵌入进行线性探测，并与其他预训练生物声学模型（Perch 1.0、SurfPerch、AVES-bio、BirdAVES、Birdnet V2.3）进行少样本迁移学习的对比。

Result: Perch 2.0嵌入在少样本转移任务中表现稳定，通常在多数任务上优于其他嵌入模型。

Conclusion: 在开发海洋哺乳动物的线性分类器且标注样本稀缺时，推荐以Perch 2.0作为特征源。

Abstract: Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.

</details>


### [43] [SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning](https://arxiv.org/abs/2512.03244)
*Salman Rahman,Sruthi Gorantla,Arpit Gupta,Swastik Roy,Nanyun Peng,Yang Liu*

Main category: cs.LG

TL;DR: SPARK提出一个三阶段框架来学习过程奖励模型（PRMs），以提供密集、逐步的奖励信号，缓解对昂贵逐步注释和地面真相的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的过程奖励模型需要大量逐步注释或地面真相以实现高质量奖励信号，成本高且难以推广到缺乏 verifiable answers 的领域。SPARK通过自我一致性和元批评的并行与序贯缩放实现对解决方案的多样性评估，并利用这些筛选结果进行合成数据的训练，从而减少对外部标注的依赖。

Method: SPARK包含三阶段：1) 生成器产生多样化解，验证器并行（自我一致性）与序贯（元批评）缩放对解进行评估；2) 将验证输出作为合成训练数据来微调过程奖励模型，生成可作为训练过程的奖励信号；3) 将带有链式思维验证的生成式PRM在RL中作为奖励模型，并通过格式约束防止奖励 Hacks；在数学推理任务上以Qwen2.5-Math-7B实现，覆盖六个基准测试。

Result: 在ProcessBench上，聚合多次独立验证得到的逐步训练数据，使PRMs的F1达到67.5，超过参考引导训练的66.4和GPT-4o的61.9；在六个数学推理基准上，基于PRM-CoT的奖励模型在RL中实现47.4%的平均准确率，超过基于地面真相的RLVR（43.9%）；该方法实现了参考无关的RL训练，甚至超越了基于地面真相的方法。

Conclusion: 该工作证明了无需外部地面真相即可进行高质量RL训练的可能性，为缺乏可核验答案的领域打开了新路径，并通过多阶段的自我评估与合成数据拣选显著提升了过程级奖励信号的有效性。

Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.

</details>


### [44] [Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval](https://arxiv.org/abs/2512.03276)
*Constantin Venhoff,Ashkan Khakzar,Sonia Joseph,Philip Torr,Neel Nanda*

Main category: cs.LG

TL;DR: 本研究通过对14个视觉语言模型在事实 recall 任务中的基准评估，揭示多模态微调往往无法有效扩展LLM的已有事实回忆机制。作者提出一个两跳框架：先从视觉输入形成实体表示，再基于该表示回忆相关事实知识。通过对比不同架构、不同规模（7B–124B 参数）的VLM，以及对比其原始LLM骨干模型，发现多数模型在事实回忆上发生降级；并挑选出3个高降级与2个低降级的模型，采用归因打补丁、激活补丁与探 probing 来证明，降级的VLM在前导的实体表示阶段来得太晚，无法复用LLM原有的事实回路。高性能VLM则能更早形成实体表示，从而在推理中重新利用LLM的记忆机制。作者还给出两种修复方法：从LLM骨干向VLM注入实体表示的补丁，以及使用连锁推理（chain-of-thought）提示。结论强调，早期实体解析的速度对VLM是否能有效使用预先存在的LLM机制具有决定性意义，并且通过机理分析可以揭示多模态对齐中的系统性失败。


<details>
  <summary>Details</summary>
Motivation: 系统性地检验VLM在将视觉输入与LLM记忆机制对齐方面的能力，回答为何某些VLM会在事实回忆任务上落后于原始LLM骨干，以及是否能够通过分析机制来解释和改进这种现象。

Method: 基准评估：对14种VLM（包括LLaVA、Native、Cross-Attention等）及不同规模进行事实回忆任务的对比；机制分析：使用归因打补丁、激活打补丁、探 probing 等方法追踪实体表示的形成时机与LLM记忆回路的复用；选择3个高降级和2个低降级模型，比较实体解析速度对回忆机制利用的影响；修复尝试：将LLM骨干的实体表示注入VLM、以及通过连锁推理提示（CoT）提升。

Result: 结果显示：7/14或11/14模型在事实回忆方面存在降级（具体数值取决于对比基准）。高性能VLM能更早形成实体表示，并在推理中有效复用LLM的事实回忆机制；而降级模型的实体表示形成较晚，导致无法在早期就接入LLM的知识通路。两种恢复方式都能部分缓解降级现象：一是从LLM骨干注入实体表示到VLM；二是通过CoT提示引导推理。总体上，早期实体解析的速度被证明是决定性因素。

Conclusion: 机制分析揭示了多模态对齐中的核心瓶颈：VLM能否及时形成可供LLM回忆的实体表示，以及是否能在早期就复用LLM的知识回路。该工作强调通过对推理机制的细粒度分析来解释系统性失败，并提出可行的修复策略，具有对多模态对齐研究的广泛启示。

Abstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.

</details>


### [45] [BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark](https://arxiv.org/abs/2512.03280)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Matthew C. Jones,Faez Ahmed*

Main category: cs.LG

TL;DR: BlendedNet++ 构建了一个大规模的场场景级气动数据集和基准，聚焦混合翼身（BWB）机型，涵盖约12,000+ 几何体和 12,490 条稳态 RANS CFD 结果，提供积分力矩系数和密集表面场（压力和摩擦系数）。通过六种模型族完成点场前向预测基准，并以条件扩散模型实现指定升阻比的反向设计，此外以梯度优化与扩散混合优化进行对比，旨在提供一个统一、可重复的前向/反向基准。资源将在接受后发布。


<details>
  <summary>Details</summary>
Motivation: 受限于缺乏现场级别的数据集，难以实现点位预测的准确性与可重复的反向设计研究；需要一个统一且可公平比较的基准来推动领域内的可重复研究，特别是针对 BWB 机型的场级气动数据。

Method: 建立 BlendedNet++ 数据集：包含约 12,000 多个几何体、单一飞行工况，输出 12,490 条稳态 RANS CFD 结果；每个样本提供综合力/力矩系数(CL, CD, CM)及密集的表面场 Cp 与 Cfx, Cfy, Cfz。基于此提出前向基准，覆盖六种模型族：GraphSAGE、GraphUNet、PointNet、坐标变换器 Transolver 风格、FiLMNet、以及 Graph Neural Operator Transformer (GNOT)；提出一个反向设计任务：在固定工况下通过条件扩散模型实现指定提升/阻力比（L/D），并与同等 surrogate 的梯度优化、以及先采样后优化的扩散-混合方法进行对比。

Result: 建立了一个统一的前向 Surrogate 基准，覆盖六种模型族，并提出一个基于条件扩散的反向设计任务及其与梯度优化的对比框架；计划发布数据集、划分、基线和脚本，促进跨模型、公平比较和可重复研究。

Conclusion: BlendedNet++ 将推动场级气动学的可重复研究与反向设计的发展，提供统一的前向/反向协议和多模型基线，未来资源将于接受后公开发布。

Abstract: Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.

</details>


### [46] [Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors](https://arxiv.org/abs/2512.03287)
*Dario Fenoglio,Mohan Li,Davide Casnici,Matias Laporte,Shkurta Gashi,Silvia Santini,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: 提出一种多频Federated Learning框架，用于头戴设备的隐私保护人类活动识别（HAR），支持在不同采样频率设备之间联合学习；在两个数据集上优于仅针对单一采样频率的基线，且代码开源。


<details>
  <summary>Details</summary>
Motivation: 在中心化HAR流程中存在隐私风险；头戴设备（如耳机、智能眼镜）具有异质采样频率，现有方法对跨设备协作的适应性不足，且头戴HAR研究相对不足。

Method: 提出多频率FL框架，允许不同采样频率的设备参与联合模型学习，并设计频率感知的本地模型与聚合策略以处理异构数据；在两组HAR数据集上对比频率特定的方法进行评估，验证隐私保护与跨频协作的有效性；提供实现代码以便复现与扩展。

Result: 在两组数据集上，相比仅针对单一采样频率的基线方法，所提方法有显著改进，证明多频率FL在HAR领域的可行性和有效性。

Conclusion: 多频率FL-HAR对隐私保护与跨设备异构数据具有良好适用性，未来可进一步提升收敛性、通信开销和在更多头戴设备上的扩展性，论文已公开实现以促进后续研究。

Abstract: Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.

</details>


### [47] [Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction](https://arxiv.org/abs/2512.03298)
*Echo Diyun LU,Charles Findling,Marianne Clausel,Alessandro Leite,Wei Gong,Pierric Kersaudy*

Main category: cs.LG

TL;DR: 提出了一种基于深度切换状态空间模型的分布自由不确定性框架，并将自适应保守性校准（ACI/AgACI）结合到多种强序列基线，提出统一的保形包装器以在非平稳性和模型错配下提供带有限样本边界的在线预测区间，实验显示覆盖率接近名义水平、准确性和区间效率均具竞争力。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的 regime切换经常破坏平稳性，使点预测以外的校准不确定性变得同样重要。提出一种分布自由的预测区间框架，能够在非平稳和模型错配条件下提供有限样本保证。

Method: 将深度切换状态空间模型（Deep Switching State Space Models）与自适应保形推断（Adaptive Conformal Inference, ACI）及其聚合版本 AgACI 相结合，并构建一个统一的保形包装器（conformal wrapper），可叠加在包括 S4、MC-Dropout GRU、稀疏高斯过程以及变更点本地模型等强基线之上，输出在线预测区间（bands）并具有限样本边际覆盖保证。

Result: 在合成与实际数据集上，保形化预测器实现接近名义覆盖率的区间，同时保持具有竞争力的点预测准确性，且区间效率（窄区间）通常得到提高。

Conclusion: 保形化的预测方法能够在非平稳性和模型错配条件下提供可靠的分布无关的预测区间，且能与多种强基线协同工作，具备良好的在线性应用潜力和普适性。

Abstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.

</details>


### [48] [HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction](https://arxiv.org/abs/2512.03300)
*Pengfei Hu,Fan Ming,Xiaoxue Han,Chang Lu,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: 提出了一种可扩展的跨水库领域泛化框架 HydroDCM，用空间元数据构造伪域标签，通过对抗学习学习不变的时序特征，并在推理阶段用针对目标水库元数据的轻量级条件层进行适应，显著优于多域基线且计算高效。


<details>
  <summary>Details</summary>
Motivation: 水文领域中水库众多且入流模式各异，现有的领域泛化（DG）方法在多域水文场景中难以直接应用；此外，除观测数据外的元数据（如空间信息）对水文过程有间接但显著的影响，传统DG难以同时实现不变性与目标域的局部适配。

Method: 利用水库空间元数据构建伪域标签，指导对抗学习以获得不变的时序特征；在推理阶段，基于目标水库元数据通过轻量级的条件化层对这些特征进行适应，使模型兼容域不变性与局部定位适配的需求。

Result: 在上科罗纳河流域的30个真实水库上进行实验，方法在多数域条件下显著优于最先进的DG基线，且计算效率高，具良好可扩展性。

Conclusion: 提出的 HydroDCM 能在多水库场景下实现有效跨域泛化并具备良好推理时的适应性和计算效率，为水文大规模多域预测提供了一种可扩展解决方案。

Abstract: Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.

</details>


### [49] [Robust Tabular Foundation Models](https://arxiv.org/abs/2512.03307)
*Matthew Peroni,Franck Le,Vadim Sheinin*

Main category: cs.LG

TL;DR: 提出了对抗性合成数据训练框架 RTM F（RTFM），通过对生成器分布进行参数化，以在合成数据上对模型进行定向挑战，提高表格数据的基础模型性能。对比强基线（如 XGBoost、CatBoost、Random Forests）的最优性能，定义了一个最优性差距，并在 TabPFN V2 分类器上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 在结构化数据任务中，表格基础模型的发展依赖于高质量的生成数据与先验。作者通过参数化生成器分布，将预训练过程转向对模型最具挑战性的合成数据，从而提升模型在真实任务上的鲁棒性和性能，探索仅使用合成数据的对抗性训练在TFMs中的潜力。

Method: 引入最优性差距度量：TFM 的实际性能与由强基线（如 XGBoost、CatBoost、Random Forests）估算的最佳可能性能之间的差；提出一个模型不可知的对抗性训练框架 RTFM，参数化生成器以在训练中不断生成“更难”的数据集。将该框架应用于 TabPFN V2 分类器，并对比基线方法。

Result: 在 TabPFN V2 分类器上，RTFM 能实现基准性能的提升，平均正规化 AUC 最高提升可达 6%，相比原始 TabPFN 与其他基线算法具有显著改进；所需额外的合成数据集不到 10 万个（<100k），显示了对抗性合成数据训练在TFMs中的有效性和数据效率。

Conclusion: 提出了一条新方向：通过对生成器进行对抗性、可控的合成数据生成来针对性地提升表格基础模型的鲁棒性与性能，方法具模型无关性，可迁移到其他TFMs；未来可进一步扩展到更复杂的数据生成策略与更广的基线评估。

Abstract: The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.

</details>


### [50] [Single-Round Scalable Analytic Federated Learning](https://arxiv.org/abs/2512.03336)
*Alan T. L. Bacellar,Mustafa Munir,Felipe M. G. França,Priscila M. V. Lima,Radu Marculescu,Lizy K. John*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.

</details>


### [51] [Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions](https://arxiv.org/abs/2512.03354)
*Hongseon Yeom,Jaeyoul Shin,Soojin Min,Jeongmin Yoon,Seunghak Yu,Dongyeop Kang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.

</details>


### [52] [A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning](https://arxiv.org/abs/2512.03363)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.

</details>


### [53] [MAGE-ID: A Multimodal Generative Framework for Intrusion Detection Systems](https://arxiv.org/abs/2512.03375)
*Mahdi Arab Loodaricheh,Mohammad Hossein Manshaei,Anita Raja*

Main category: cs.LG

TL;DR: 提出了 MAGE-ID，一种基于扩散的多模态生成框架，用于增强 IDS 数据并提升检测效果，通过将表格流量特征与其变换后的图像在统一潜在先验下耦合。


<details>
  <summary>Details</summary>
Motivation: IDS 面临数据异质性、威胁演变和正负样本数据不平衡等挑战；现有生成方法多为单模态，难以捕捉跨域依赖。

Method: 提出 Multimodal Attack Generator for Intrusion Detection (MAGE-ID)，通过扩散模型将表格流量特征与对应图像耦合在同一潜在空间；联合训练 Transformer 与 CNN 基变分编码器，并使用 EDM 风格去噪器实现多模态合成。

Result: 在 CIC-IDS-2017 与 NSL-KDD 数据集上，相较 TabSyn 与 TabDDPM，MAGE-ID 在保真度、多样性及下游检测性能方面显著提升，实现更均衡、连贯的多模态合成。

Conclusion: MAGE-ID 为多模态 IDS 数据增强提供了有效途径，证明跨模态保真性与检测性能提升的潜力，具有提升 IDS 数据增广效果的前景。

Abstract: Modern Intrusion Detection Systems (IDS) face severe challenges due to heterogeneous network traffic, evolving cyber threats, and pronounced data imbalance between benign and attack flows. While generative models have shown promise in data augmentation, existing approaches are limited to single modalities and fail to capture cross-domain dependencies. This paper introduces MAGE-ID (Multimodal Attack Generator for Intrusion Detection), a diffusion-based generative framework that couples tabular flow features with their transformed images through a unified latent prior. By jointly training Transformer and CNN-based variational encoders with an EDM style denoiser, MAGE-ID achieves balanced and coherent multimodal synthesis. Evaluations on CIC-IDS-2017 and NSL-KDD demonstrate significant improvements in fidelity, diversity, and downstream detection performance over TabSyn and TabDDPM, highlighting the effectiveness of MAGE-ID for multimodal IDS augmentation.

</details>


### [54] [Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization](https://arxiv.org/abs/2512.03393)
*Lakshmi Jayalal,Sheetal Kalyani*

Main category: cs.LG

TL;DR: 提出一个基于隐式正则化的无调参MMV恢复框架，通过对估计矩阵进行因子化重参数化，将共享的行支持与各向量分量解耦；在对X进行最小二乘的梯度下降动态中，出现“动量样”效果，使真支持的行范数增长更快，从而收敛到理想的行稀疏解，且实验表明性能与传统方法相当且无需先验信息或调参。


<details>
  <summary>Details</summary>
Motivation: 传统MMV方法（如 M-OMP、M-FOCUSS）需要对信号的稀疏度和噪声方差进行精细调参或预先知识，限制了在无先验信息场景的应用。提出无调参、基于过参数化的框架以利用过拟合中的隐式正则化优势。

Method: 将估计矩阵X重参数化为两因子 G ∈ R^{n×r} 和 W ∈ R^{r×L}（或等价的 A,B 形式），使得共享的行-支持与每列向量的具体系数分离。对最小二乘目标 min_{G,W} ||Y − Φ (G W)||_F^2 进行梯度下降，研究初始化要求：足够小且保持平衡的初始化，以使梯度流动具有“动量样”效应，使真支持的行的范数增长显著快于其他行。

Result: 给出理论分析：在足够小且平衡的初始化下，梯度下降轨迹收敛至近似的行稀疏解，确保由真支持决定的解。实验结果表明，与M-OMP、M-FOCUSS等方法相比，在无先验信息或调参的情况下，性能可达到相当水平，且实现简化。

Conclusion: 该工作提供了一种无调参的MMV稀疏恢复框架，利用隐式正则化的过参数化和梯度下降动力学实现行稀疏性，理论保障与经验验证均表明其可在无先验信息的场景下竞争性地替代传统方法。

Abstract: Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a "momentum-like" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.

</details>


### [55] [VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing](https://arxiv.org/abs/2512.03394)
*Hamed Poursiami,Shay Snyder,Guojing Cong,Thomas Potok,Maryam Parsa*

Main category: cs.LG

TL;DR: VS-Graph 提出一个基于高维向量计算的图学习框架，通过 Spike Diffusion 和 Associative Message Passing，在无梯度训练的前提下实现接近或超过 GNN 的准确性，同时在训练速度方面实现显著提升（多数据集上达 450x 级别的加速），并对向量维度压缩有很强鲁棒性（D=128），适合边缘和神经形态硬件。


<details>
  <summary>Details</summary>
Motivation: 图分类是基础而关键的任务；GNNs 虽具强大表达能力但计算成本高，难以在资源受限设备上部署。尽管 HDC/VSA 提供轻量级、脑启发的替代方案，但现有的 HDC 图方法在预测性能上仍落后于 GNN。本工作旨在缩小 HDC 的高效性与 GNN 的表达力之间的差距。

Method: 提出 Spike Diffusion，用于基于拓扑的节点识别；引入 Associative Message Passing，在高维向量空间中进行多跳邻域聚合；整个过程在高维向量（无需梯度优化与反向传播）中完成，且在 D=128 时仍保持鲁棒性，适合在边缘和神经形态硬件上高效执行。

Result: 与现代 GNN 的性能相当甚至超越，在 MUTAG 与 DD 等数据集上相比先前的 HDC 基线提升约 4–5%；在多个数据集上达到或超过 GNN 基线，训练速度提升可达约 450 倍；且对维度压缩具有良好鲁棒性（D=128 仍保持高准确性）。

Conclusion: 证明基于高维向量的图学习是可行且高效的路线，VS-Graph 将 HDC 之轻量性与图学习的表达能力结合起来，尤其适合边缘设备和神经形态硬件的 ultra-efficient 实现，并对未来在更低维度下的鲁棒性和应用场景提供了有力依据。

Abstract: Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.

</details>


### [56] [Better World Models Can Lead to Better Post-Training Performance](https://arxiv.org/abs/2512.03400)
*Prakhar Gupta,Henry Conklin,Sarah-Jane Leslie,Andrew Lee*

Main category: cs.LG

TL;DR: Explicit world-modeling improves state representations and boosts post-training GRPO performance on sequence-planning tasks in a 2x2x2 Rubik's Cube setting.


<details>
  <summary>Details</summary>
Motivation: 研究显式世界模型对模型内部表征与下游能力的影响，以及在后训练阶段通过强化学习方法（GRPO）提升序列规划任务的性能的可能性。

Method: 在受控的2x2x2 Rubik's Cube环境中，比较三种显式世界建模策略：1) 标准下一词预测；2) 状态预测预训练；3) 状态预测+下一词目标的联合策略；在后训练阶段应用Group Relative Policy Optimization（GRPO）评估性能；使用线性探针与因果干预评估表示质量。

Result: 显式世界建模使表示更加线性可解码且更易被因果干预控制；状态表示的改进与GRPO后培训的性能提升相关，且在更困难的立方体状态下提升更显著。

Conclusion: 提升状态表示可以增强后训练阶段的序列规划任务性能。

Abstract: In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.

</details>


### [57] [GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test](https://arxiv.org/abs/2512.03428)
*Ziyi Ding,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出 GaussDetect-LiNGAM，一种用于双变量因果发现的 LiNGAM 变体，通过利用正向模型噪声的高斯性与反向回归中的残差独立性的等价性，省略显式高斯性检验，改用核基独立性检验，从而提高鲁棒性和效率，并在不同噪声类型和样本量下保持较高的一致性，减少每次判断的检验次数。


<details>
  <summary>Details</summary>
Motivation: 在 LiNGAM 的线性、无环和外生性假设下，传统做法需要对噪声分布进行高斯性检验，结果对样本量和噪声分布敏感，易导致误判。作者发现正向模型噪声的高斯性等价于反向回归中的自变量与残差独立性，因此可以用更稳健的独立性检验替代高斯性检验，从而提高鲁棒性和可扩展性。

Method: 给出理论等价性证明；提出 GaussDetect-LiNGAM 算法，利用核基独立性检验（如 HSIC 等）来评估正向回归中噪声与残差的独立性，从而进行因果方向识别和判定；在每一步减少需要执行的检验数量。

Result: 实验验证等价性成立；在多种噪声类型和不同样本量下，方法具有较高的一致性，且显著降低每次决策的检验次数，提升计算效率和实用性。

Conclusion: GaussDetect-LiNGAM 能提升因果推断的效率和实际适用性，使 LiNGAM 在真实场景中更易获取稳定结果；存在的局限与未来方向包括扩展到多变量情景、进一步分析对不同核方法的敏感性等。

Abstract: We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.

</details>


### [58] [Grokked Models are Better Unlearners](https://arxiv.org/abs/2512.03437)
*Yuanbang Liang,Yang Li*

Main category: cs.LG

TL;DR: Grokking后再进行撤记在视觉与语言模型上更高效、损伤更小且更稳定；训练阶段（预-grokking vs 后-grokking）成为提升撤记效果的正交杠杆，且模型呈现更模块化的表示与较低的梯度对齐。


<details>
  <summary>Details</summary>
Motivation: 研究 grokking 延迟泛化与机器撤记的关系，探究是否将 grokking 作为提升在不完全重新训练情况下实现遗忘的策略。

Method: 对 CNN/ResNet（CIFAR、SVHN、ImageNet）和 Transformer（TOFU 风格）等模型，在 grokking 转变前后应用标准的撤记方法，比较起点于 grokking 点的检查点与早停检查点的差异；评估更新次数、对保留/测试集的伤害、跨种子稳定性，并通过特征与曲率分析解释模块化表示与梯度对齐的变化。

Result: 结果显示：后 grokking 的起点更易达到目标遗忘，更新次数更少；对保留与测试数据的性能下降更小；跨种子更新更稳定；并且后 grokking 模型具有更模块化的表示、忘 retain 子集之间的梯度对齐下降，从而更易实现选择性遗忘。

Conclusion: 训练时机（pre- vs post-grokking）是提升撤记策略的正交杠杆，可在不改动撤记算法的情况下改进现有方法，提供可落地的做法。

Abstract: Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.

</details>


### [59] [Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention](https://arxiv.org/abs/2512.03464)
*Yujing Liu,Chen Yang*

Main category: cs.LG

TL;DR: 提出一个端到端深度学习框架，利用金融情感的两种模态（时效性与热度）进行跨模态交互，采用中文BERT嵌入和FMHCA跨模态注意力，辅以变换器与因式分解双线性池化进行分类，在837家公司数据集上达到83.5%的准确率，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析往往难以整合多模态信息，且难以捕捉跨模态之间的细粒度互动。时效性模态提供最新市场动态，热度模态提供集体情绪的趋势信号，二者互补，故需设计专门的跨模态交互机制以提升预测能力。

Method: 提出端到端框架：1) 使用 BERT（中文-wwm-ext）进行特征嵌入；2) 引入金融多头跨模态注意力（FMHCA），在时效性与热度两模态之间进行信息交换与对齐；3) 通过 Transformer 层进一步处理特征；4) 使用多模态因式分解双线性池化进行特征融合，进行负/中性/正三类情感分类。

Result: 在涵盖837家公司的大规模数据集上，模型达到83.5%准确率，相较基线（如 BERT+Transformer）提升约21个百分点。该结果显示所提出的跨模态注意力机制和融合策略在金融情感分析中的有效性。

Conclusion: 该框架展示了跨模态注意力在金融情感分析中的潜力，有望提升金融决策与风险管理的情感认知能力；同时为多模态信息在金融文本中的整合提供新的设计思路。

Abstract: In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.

</details>


### [60] [Bayesian Event-Based Model for Disease Subtype and Stage Inference](https://arxiv.org/abs/2512.03467)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: BEBMS在合成数据与真实阿尔茨海默病数据上相较SuStaIn更稳健，且更符合病程共识；通过贝叶斯子类型变体的事件模型来估计子类型数量、每个子类型的疾病进程顺序，以及将患者分配到子类型，优于现有SuStaIn在排序、分期和分型任务的性能。


<details>
  <summary>Details</summary>
Motivation: 疾病进程中的异质性通常以少数子类型表现出结构性差异；需要一个鲁棒的建模框架对亚型及其进程进行推断，并在存在模型失配的情况下仍能保持性能。本文提出BEBMS并与SuStaIn进行比较，评估鲁棒性。

Method: 提出BEBMS：对事件模型进行贝叶斯子类型变体建模；在合成数据上进行多组实验，改变模型失配水平，比较排序、分期、分型等任务的表现；并在真实的阿尔茨海默病数据集上比较两种方法的输出与科学共识。

Result: 在合成数据实验中，BEBMS在排序、分期和子类型分配任务上明显优于SuStaIn；在真实阿尔茨海默病数据上，BEBMS的结论更符合疾病进程的公认顺序。

Conclusion: 贝叶斯子类型事件模型在处理结构化异质性方面更具鲁棒性，尤其在模型错配情形下优于SuStaIn，且在真实数据中更贴近科学共识，显示其潜在的应用优势。

Abstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.

</details>


### [61] [Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression](https://arxiv.org/abs/2512.03475)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: 提出并评估联合进展模型 JPM，将多病理推断作为部分排序的联合进展，通过 Pairwise、Bradley-Terry、Plackett-Luce、Mallows 等变体，在 calibration、separation、sharpness 等属性上评估；在仿真中比 SA-EBM 提升约 21%，在 NACC 数据中 Mallows JPM 与基线模型与 AD/VaD 的混合病理结论吻合。


<details>
  <summary>Details</summary>
Motivation: 传统的事件基模型（EBM）通常假设每个个体只有单一疾病轨迹，而神经退行性疾病中常存在混合病理。需要一个能处理多疾病联合进展的框架，将单疾病轨迹以部分排序形式表示，并在此基础上构建联合进展的先验。

Method: 将单疾病轨迹视为部分排序，构建联合进展的概率先验；研究四种 JPM 变体（Pairwise、Bradley-Terry、Plackett-Luce、Mallows），并评估其标定性（calibration）、分离性（separation）以及锐度（sharpness）。通过合成数据评估排序准确性，相较强基线 SA-EBM（将联合病理视为单一条件），并在 NACC 数据集上比较不同变体与基线的表现。

Result: 所有变体均达到标定，几乎实现近乎完美的分离；锐度随变体及输入部分排序的特征（排序的数量与长度、冲突、重叠）而异，可通过输入特征预测。合成实验中 JPM 相对于 SA-EBM 提升约 21% 的排序准确性。在 NACC 数据中，Mallows 变体与 SA-EBM 的结果更符合关于 AD 与 VaD 混合病理的既有文献。

Conclusion: JPM 提供一个更灵活、可解释的多病理进展推断框架，尤其 Mallows 变体与现有知识更一致，显示在处理混合病理的跨个体进展时的潜力。

Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.

</details>


### [62] [ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms](https://arxiv.org/abs/2512.03476)
*Juan Diego Toscano,Daniel T. Chen,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出 ATHENA 框架，通过 HENA 循环的知识驱动诊断实现端到端的科学计算研究自动化，结合上下文带学习、专家蓝图指导的结构性动作与符号-数值耦合，显著提升 SciC/SciML 的稳定性和精度。


<details>
  <summary>Details</summary>
Motivation: 解决理论概念与计算实现之间的断层，提升科学计算与科学机器学习中的自动化、方法创新与研究效率，降低人工干预。

Method: 提出 Agentic 框架 ATHENA，以 HENA 循环作为上下文带诊断过程，作为 Contextual Bandit 的在线学习器，基于专家蓝图选择结构性动作 A_n，由动作生成可执行代码 S_n，获得科学奖励 R_n；在 SciC 中发现对称性以获得解析解或稳定的数值解算器，在 SciML 中进行深度诊断、混合符号-数值工作流，耦合 PINN 与 FEM 等多物理问题。

Result: 实现超越常规自动化的性能，达到验证误差 1e-14；在人机协同干预下，结果提升一个数量级，显著加速科学发现。

Conclusion: 该范式通过将实现细节转化为方法学创新，推动科学发现并提高端到端研究效率。

Abstract: Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.

</details>


### [63] [Adaptive sampling using variational autoencoder and reinforcement learning](https://arxiv.org/abs/2512.03525)
*Adil Rasheed,Mikael Aleksander Jansen Shahly,Muhammad Faisal Aftab*

Main category: cs.LG

TL;DR: 提出一种自适应稀疏 sensing 框架，将变分自编码器先验与强化学习结合，按序选择测量，实现对稀疏测量的更优重建，优于 CS、OSP 及基于生成模型的重建。


<details>
  <summary>Details</summary>
Motivation: 现有的 compressed sensing 依赖通用基和随机测量，效率和重建质量受限；OSP 使用历史数据设计采样模式，但固定线性基无法适应非线性或样本特异性变化；基于生成模型的 CS 虽提升了先验但仍采用次优的随机采样。需发展可自适应、面向数据分布的测量策略。

Method: 提出一个自适应稀疏 sensing 框架，将变分自编码器（VAE）先验与强化学习结合，按序选择测量，以学习的策略对采样过程进行端到端优化。

Result: 实验结果显示，该方法在稀疏测量条件下的重建性能优于传统 CS、OSP，以及基于生成模型的重建方法。

Conclusion: 将 VAE 先验与强化学习结合的自适应测量策略可在稀疏采样下实现更优的重建质量，体现了通过学习先验与策略实现对数据分布的自适应感知的潜力。

Abstract: Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.

</details>


### [64] [Parameter-Efficient Augment Plugin for Class-Incremental Learning](https://arxiv.org/abs/2512.03537)
*Zhiming Xu,Baile Xu,Jian Zhao,Furao Shen,Suorong Yang*

Main category: cs.LG

TL;DR: 提出了一种对非预训练CIL的可插拔增强方法DLC，通过在基模型深层注入LoRA残差实现任务特定表示的聚合，并用轻量权重单元降低干扰；在ImageNet-100上以仅4%参数实现8%的准确度提升，且在固定内存预算下达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 缓解类增量学习中的遗忘与稳定性-塑性问题。传统的扩展方法通常需要显著的参数增加；因此需要一种参数高效且易插拔的扩展手段来提升表现。

Method: 以回放/蒸馏得到的特征提取器作为基模型，在每个任务中使用LoRA向基模型的深层注入任务特定的残差；推断阶段将带有任务特定残差的表示聚合以得到分类结果。为防止非目标LoRA插件干扰，引入一个轻量化的权重单元对不同LoRA改造表示进行重要性分数学习。该方法可作为可下载的插件扩展，提升基线方法的性能。

Result: 在大规模数据集ImageNet-100上，只使用ResNet-18基础参数的4%，DLC实现了约8%的准确度提升，并在固定内存预算下超越了现有SOTA方法。

Conclusion: DLC为非预训练CIL场景提供了一种高效、可插拔的扩展策略，通过LoRA注入任务特定残差并用权重单元控制干扰，具备良好的参数利用率和兼容性，且在实验中显示出显著的性能提升。

Abstract: Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.

</details>


### [65] [When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate](https://arxiv.org/abs/2512.03578)
*Florent Forest,Amaury Wei,Olga Fink*

Main category: cs.LG

TL;DR: 提出 MAGNETS，时间序列外推回归的本质可解释模型，通过掩码对输入特征进行概念化聚合，概念以可解释的加法结构组合，且无需监督标注。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒 TSER 模型缺乏可解释性、后验解释粗糙、对高维多变量数据的可扩展性不足等问题。近年来可解释性方法要么需要先验概念标签，要么难以捕捉特征之间交互，且表达能力有限。

Method: 设计 MAGNETS 框架：学习一组人类可理解的概念，每个概念对应对选定输入特征的掩码聚合；通过掩码对时间序列的不同时段进行选择性聚合，揭示驱动预测的特征及其在序列中的作用时刻；将预测以概念的加法结构组合，提供透明的决策过程；且无需任何概念注释。

Result: 该方法实现了可解释性和可扩展性，能够在不依赖监督概念标签的情况下给出清晰的预测原因，并对时间维度的影响进行揭示。

Conclusion: MAGNETS 提供一个可扩展且本质上可解释的 TSER 框架，兼具预测能力和可解释性，允许对时间序列特征的作用及其时序模式进行明确理解。

Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.

</details>


### [66] [Optimal Transportation and Alignment Between Gaussian Measures](https://arxiv.org/abs/2512.03579)
*Sanjit Dandapanthula,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan,Aaditya Ramdas,Ziv Goldfeld*

Main category: cs.LG

TL;DR: 给出高斯分布下的最优传输和 IGW 对齐的系统性分析，提供未居中高斯的闭式表达式（需在单位算子上进行的二次优化并给出界），在至少一个分布居中时推导出完全闭式解与 IGW 重心的解析解；将高斯多 marg OT 的二次代价降维为可解的优化并给出带秩缺失约束的高效算法；并在合成及真实数据上展示在知识蒸馏和异构聚类等任务的应用。


<details>
  <summary>Details</summary>
Motivation: OT/GW 为比较、转换和聚合异构数据提供了直观的几何框架，但在大规模应用中计算成本高。对于高斯分布与二次成本的情形，存在闭式或半闭式解的机会，能够显著提升计算效率并扩展适用性。

Method: 推导未居中高斯在可分希尔空间上的 IGW 对齐的闭式表达，需对单位算子上进行二次优化，并给出紧致的上下界；若至少一分布居中，则变为完全闭式解；推导居中情形下 IGW 重心的解析解；将高斯多 marg OT 的二次代价降到一个可解的优化问题，并提出带秩缺陷约束的高效求解算法；并通过数值实验展示在知识蒸馏和异构聚类中的应用。

Result: 给出一个关于 IGW 的闭式表达（在需要对单位算子进行二次优化的情形）、对优化问题的紧界，以及在至少一个分布居中时的完全闭式解和 IGW 重心的解析解；将高斯多 marg OT 的问题降维并给出可行算法；在合成与真实数据上验证方法的有效性。

Conclusion: 扩展了高斯 OT 与 IGW 的理论和应用边界，提供可执行的解析表达和高效算法，使大规模数据对齐、跨源学习（如知识蒸馏）具备更广泛的实用性。

Abstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.

</details>


### [67] [CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion](https://arxiv.org/abs/2512.03610)
*Julius Lenz*

Main category: cs.LG

TL;DR: CoGraM introduces a robust, multi-stage, context-sensitive optimization for merging neural networks without retraining. It uses layer-, neuron-, and weight-level decisions guided by loss differences with rollback to prevent harmful updates, addressing instability of simple averaging and Fisher-based methods.


<details>
  <summary>Details</summary>
Motivation: Federated and distributed learning require merging models without retraining, but naive methods (weight averaging, Fisher merging) destabilize performance and vary with seeds. There is a need for a principled, loss-based, and rollback-enabled merging method.

Method: CoGraM is a multi-stage optimization that operates across layers, neurons, and individual weights. It uses context-sensitive decisions guided by loss differences and predefined thresholds, and employs rollback to block updates that would harm the global loss. It iteratively optimizes the merged network to align with the original models while maintaining stability.

Result: The approach can significantly improve the quality of the merged network compared with traditional methods like Fisher merging, by addressing their weaknesses and reducing instability.

Conclusion: CoGraM provides a robust and fine-grained framework for merging neural networks across distributed systems, yielding improved accuracy and stability without retraining.

Abstract: Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.

</details>


### [68] [The promising potential of vision language models for the generation of textual weather forecasts](https://arxiv.org/abs/2512.03623)
*Edward C. C. Steele,Dinesh Mane,Emilio Monti,Luis Orus,Rebecca Chantrill-Cheyette,Matthew Couch,Kirstine I. Dale,Simon Eaton,Govindarajan Rangarajan,Amir Majlesi,Steven Ramsdale,Michael Sharpe,Craig Smith,Jonathan Smith,Rebecca Yates,Holly Ellis,Charles Ewen*

Main category: cs.LG

TL;DR: Vision-language models can generate Shipping Forecast text directly from video-encoded gridded weather data, enabling scalable improvements in meteorological product generation.


<details>
  <summary>Details</summary>
Motivation: To accelerate the adoption of multimodal foundation models in meteorology by addressing the nascent state of applying such models to weather products and seeking production efficiency and service innovation gains.

Method: Apply a vision-language model to translate video-encoded gridded weather data into Shipping Forecast narrative text, demonstrating feasibility with early results.

Result: Early results show promising scalable opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.

Conclusion: The approach demonstrates potential for broader adoption and impact in meteorological services, warranting further validation and scaling work.

Abstract: Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.

</details>


### [69] [Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting](https://arxiv.org/abs/2512.03656)
*Salim Khazem,Houssam Kanso*

Main category: cs.LG

TL;DR: 提出一个统一的深度学习框架，将周期性时间编码与混合LSTM-CNN结构相结合，用于多步能源预测；在一年数据集上实现七个预测区间的显著改进。


<details>
  <summary>Details</summary>
Motivation: 需求准确的电力消耗预测以支持需求管理和智能电网；保留日历属性的周期结构；同时利用长期季节性和短期局部模式；在一个统一框架中融合时间编码、日历特征和混合集成。

Method: 使用正弦-余弦编码将日历属性离散化；采用一个由LSTM、CNN和专用每个预测时段的MLP回归器组成的元学习器的集成模型；对一年国家级用电数据进行广泛实验，包括消融分析与基线比较。

Result: 在七个预测区间中均实现一致提升，混合模型在RMSE和MAE上低于单一结构和现有方法。

Conclusion: 证明将周期性时间表示与互补的深度学习结构相结合的优势；首次在一个统一框架中同时评估时间编码、日历特征和混合集成用于短期能源预测。

Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.

</details>


### [70] [Dynamically Scaled Activation Steering](https://arxiv.org/abs/2512.03661)
*Alex Ferrando,Xavier Suau,Jordi Gonzàlez,Pau Rodriguez*

Main category: cs.LG

TL;DR: DSAS 是一个与具体 steering 技术无关的自适应激活引导框架，能够在不同层和不同输入之间动态调整引导强度，从而在抑制有害输出与保持有用性之间取得更好权衡。可扩展到文本到图像扩散模型，并提供更好的可解释性与较低的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的引导方法通常对所有输入进行统一干预，导致在不需要引导时也会降低模型性能；需要一个能够仅在检测到不良行为时才介入、并且对介入强度进行上下文自适应调整的框架。

Method: DSAS 在生成时对每层/输入计算上下文相关的缩放因子，动态调节现有引导变换的强度；它与具体的引导方法无关，且可以端到端共同优化，提升在抑制不良行为与保持任务有用性之间的 Pareto 最优性。

Result: 实验表明，与单纯的引导相比，DSAS 在与现有方法结合时显著提升 Pareto 前沿的表现，达到更好的有害行为抑制与有用性之间的折中；并且能泛化到文本到图像扩散模型，允许对特定概念进行自适应调控；计算开销很低，且加强可解释性，指明了需要介入的 token 及介入程度。

Conclusion: DSAS 提供一个通用、高效且可解释的自适应引导框架，成功解耦了“何时引导”与“如何引导”，可与现有引导方法无缝集成，并且具备跨模态的扩展潜力。

Abstract: Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.

</details>


### [71] [Feature-aware Modulation for Learning from Temporal Tabular Data](https://arxiv.org/abs/2512.03678)
*Hao-Run Cai,Han-Jia Ye*

Main category: cs.LG

TL;DR: 提出一种基于特征感知的时间性调制机制，用于处置表格数据中的随时间演化的分布偏移，通过对特征表示进行时序上下文条件化并调制尺度与偏度等统计量，达到轻量但强有力的自适应，在基准数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的时序分布偏移导致固定映射的模型性能下降；静态模型与高度自适应模型之间需要在鲁棒性与适应性之间取得折中。

Method: 提出特征感知的时间性调制机制，在时序上下文下对特征表示进行条件化，调制如尺度、偏度等统计属性；结合特征变换策略以在不同时间阶段对齐特征语义；目标是实现跨时间的一致性表示。

Result: 基准评测验证了该方法在处理时间偏移方面的有效性。

Conclusion: 该方法提供了一种轻量且强大的自适应手段，平衡泛化能力与适应性，便于在现实表格数据场景中部署。

Abstract: While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.

</details>


### [72] [Universally Converging Representations of Matter Across Scientific Foundation Models](https://arxiv.org/abs/2512.03750)
*Sathya Edamadaka,Soojung Yang,Ju Li,Rafael Gómez-Bombarelli*

Main category: cs.LG

TL;DR: 多模态科学模型在预测分子、材料和蛋白质行为时，其内部潜在表征高度对齐，表明存在共同的物质表示基础。研究揭示两大规律：在接近训练数据的输入上，高性能模型的表示趋同，弱模型则走向局部子最优；而在与训练数据相去甚远的结构上，几乎所有模型都收敛到低信息量的表示，表明当前模型仍受训练数据与归纳偏置限制，尚未具备普遍的“ universal structure”。这为基础级通用性提供可量化的表示对齐基准。


<details>
  <summary>Details</summary>
Motivation: 探索不同模态的机器学习模型是否学习到相似的内部表征，以及这是否能支撑构建能在跨域和跨任务中泛化的科学基础模型。尽管语言和视觉领域存在表示收敛的证据，科学领域的等价研究尚不足。本研究试图系统性地评估近60个跨模态模型的表示对齐，以评估“物质”的普遍表示是否可以被发现。

Method: 分析覆盖字符串、图、三维原子结构以及蛋白质等多模态的近60个科学模型的潜在表示。在不同化学体系中比较小分子对表示的影响，研究不同数据集训练的模型在表示空间中的对齐程度，以及当模型性能提升时表示的趋同情况。还比较在与训练数据相似与显著不同的输入下的两种表示范式，以区分高性能与弱模型的行为。

Result: 发现几乎所有科学模型的表示在化学系统中高度对齐；不同数据集训练的模型对小分子有高度相似的表示；在互原潜在力的表示空间中，随着性能提升，表示逐渐收敛。完全不同于训练分布的输入下，大多数模型收敛到低信息量表征，表明受限于训练数据和归纳偏置，尚未形成普遍的结构表示。

Conclusion: 将表示对齐作为科学基础级通用性的量化基准；随着模型扩展，可能出现普适表示的涌现；同时该分析可用于挑选和蒸馏跨模态、跨领域任务的学习表征，提升跨领域迁移与泛化能力。

Abstract: Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.

</details>


### [73] [Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition](https://arxiv.org/abs/2512.03755)
*Stephen Law,Tao Yang,Nanjiang Chen,Xuhui Lin*

Main category: cs.LG

TL;DR: 提出一个条件轨迹编码器，联合学习空间与运动表征，并保留起点依赖的方向性差异，借助几何特征与原点嵌入实现跨越共享模式与起点特定签名的分解，用对比学习处理，验证表明城市形态导致系统性认知不平等，提供面向规划与导航系统的起点感知分析工具。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹学习与空间嵌入方法存在方法学碎片化：轨迹学习忽略空间上下文，空间嵌入忽略时序动态。缺乏联合训练、忽略起点方向性不对称，以及过度依赖POI/影像等辅助信息。本研究旨在通过联合学习实现时空表示的结合、保留方向性并利用几何特征，提升对城市导航认知差异的量化分析能力。

Method: 提出条件轨迹编码器，联合学习空间与运动表征，保持 origin-dependent asymmetries。采用双向LSTM，基于可见性比率与曲率特征，条件化的起点嵌入进行处理，通过对比学习将表示分解为共享的城市模式与起点特异性签名。并在六个合成城市与真实场景北京西城区进行验证。

Result: 在六个合成城市和北京西城区的实证验证中，证实城市形态与结构性认知不平等的存在，展现了起点感知分析的可行性及其对规划与导航系统的应用潜力。

Conclusion: 该框架为城市规划者提供定量评估体验公平性的工具，为建筑师提供关于布局决策与认知影响的洞见，并实现面向导航系统的起点感知分析。

Abstract: Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \to B \ne B \to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.

</details>


### [74] [Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach](https://arxiv.org/abs/2512.03786)
*Conor McCarthy,Jan Peter van Zandwijk,Marcel Worring,Zeno Geradts*

Main category: cs.LG

TL;DR: 基于机器学习将手机运动传感器的数字痕迹转化为不同物理活动的似然比（LRs）的研究；在新数据集NFI_FARED上取得较强的活动对区分能力，能生成多活动的LR与活动时间线，数据与代码公开以促进研究。


<details>
  <summary>Details</summary>
Motivation: 为法证调查提供证据：利用手机的数字痕迹来推断个人的物理活动；将证据量化为似然比，提升法证分析的可重复性与可追溯性；通过公开数据集推动领域研究。

Method: 提出基于机器学习的方法，将移动设备传感器的数字痕迹映射为不同活动的LR值。使用NFI_FARED数据集（四部iPhone、共19种活动）进行评估，并扩展为同时输出多活动的LR及活动时间线的分析。数据与复现代码公开。

Result: 所提出的方法能够区分167对可能的活动组合，总体覆盖了171种活动对；还能对多活动或活动组输出LR并生成活动时间线，支持侦查初期与后期阶段的分析。

Conclusion: 该方法在法证场景中具有可行性与潜在应用价值，且数据集与实现代码公开，能促进该领域的后续研究与方法迭代。

Abstract: Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.

</details>


### [75] [Adaptive Identification and Modeling of Clinical Pathways with Process Mining](https://arxiv.org/abs/2512.03787)
*Francesco Vitale,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 通过两阶段的过程挖掘方法，用以扩展临床路径知识库，并通过一致性检查实现对新变异的更精准建模；在 SARS-CoV-2 场景下达到高 AUC 与模型简化度。


<details>
  <summary>Details</summary>
Motivation: 手工建模依赖指南和专家知识，难以覆盖不同疾病变体或组合的实际最佳实践，因此需要基于历史数据的、可扩展的知识库更新机制；提高路径的适用性和资源效率。

Method: 阶段1：收集给定疾病的历史数据，形成治疗过程的过程模型；阶段2：将新数据与参考模型对比，进行一致性检查；据一致性结果，扩展知识库，生成针对新的变体/病种组合的更具体模型。以 Synthea 模拟 SARS-CoV-2 及不同并发症为评估对象。

Result: 方法在评估中实现了高精度扩展知识库的能力，AUC 达到 95.62%，模型简化度（arc-degree simplicity）为 67.11%。

Conclusion: 基于过程挖掘的两阶段方法可在保持一定模型简洁度的同时，提升临床路径知识库对疾病变体的覆盖和适用性。

Abstract: Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.

</details>


### [76] [EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification](https://arxiv.org/abs/2512.03804)
*Hanhui Deng,Xinglin Li,Jie Luo,Zhanpeng Jin,Di Wu*

Main category: cs.LG

TL;DR: 提出一个轻量高效的ECG分类模型 EfficientECG，基于 EfficientNet，并配以跨注意力的多导联特征融合，能够处理高频长序列ECG 数据并融合性别、年龄等附加特征，在代表性数据集上优于现有方法，同时具备较小的模型规模和较快推断速度。


<details>
  <summary>Details</summary>
Motivation: 解决ECG诊断中的高误诊率与对长序列、跨导联数据处理能力不足的问题，目标是通过端到端深度学习模型实现高精度、快速诊断，减轻医务人员负担。

Method: 在 EfficientNet 的基础上构建 EfficientECG，以适用于高频长序列ECG 数据的分类任务，并设计跨注意力特征融合模块，将多导联信号与多特征（如性别、年龄）在模型中进行联合表示与融合。

Result: 在代表性 ECG 数据集上，与先进方法相比，显示出更高的精度、有效的多特征融合与更小的模型体积/推理成本，证明方法在准确性与效率方面的优势。

Conclusion: 该工作提出的轻量级且可扩展的ECG诊断框架具备实际应用潜力，能够提升临床诊断效率并降低医生工作量，但需要进一步在更广泛的数据集与真实场景中验证其鲁棒性与泛化能力。

Abstract: Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.

</details>


### [77] [Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA](https://arxiv.org/abs/2512.03805)
*Tai Nguyen,Phong Le,André Biedenkapp,Carola Doerr,Nguyen Dang*

Main category: cs.LG

TL;DR: DDQN + adaptive reward shifting for DAC with (1+ (λ, λ))-GA on OneMax achieves good sample efficiency; PPO struggles; undiscounted learning helps DDQN; adaptive reward shifting addresses under-exploration; overall, DDQN can match theoretical policies with orders of magnitude better efficiency.


<details>
  <summary>Details</summary>
Motivation: 探索将深度强化学习用于动态算法配置(DAC)的可行性与局限性，诊断现有方法在DAC中的不足，提出更具实用性的改进策略以提升样本效率和策略质量

Method: 对控制(1+ (λ, λ))-GA在OneMax问题上的种群大小参数进行系统性分析，比较DDQN与PPO在DAC中的表现，诊断出导致性能瓶颈的两大问题并提出解决方案；提出自适应奖励移位以缓解DDQN的探索不足；探索无折扣学习对DDQN的影响；分析PPO的超参数依赖性并评估其局限性；通过实证评估验证所提策略的有效性

Result: 自适应奖励移位显著提升DDQN的探索能力，且无需针对具体实例进行超参数调优，提升跨规模的一致性；无折扣学习有效解决DDQN中的计划 horizon问题；PPO存在固有方差问题，需改用其他算法设计；对PPO超参数的依赖性分析显示尽管调参能提升稳定性，但难以在不同配置下找到有效策略；在此配置下，结合自适应奖励移位的DDQN的性能与理论派生策略相当且样本效率显著提升，较先前的DAC方法提升数个数量级

Conclusion: 要将深度强化学习用于DAC，需针对探索不足与计划 horizons等挑战给出专门设计。自适应奖励移位和无折扣学习是提升DDQN在DAC中的有效手段；PPO可能需要新的算法设计来解决其方差问题；虽然超参数调优有帮助，但不足以在多配置下稳定发现有效策略；所提出的方法在样本效率和策略质量方面显著优于以往DAC方法。

Abstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

</details>


### [78] [DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training](https://arxiv.org/abs/2512.03847)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Yuhui Wang,Sixian Li,Junjie Ye,Honglin Guo,Shichun Liu,Chenhao Huang,Yajie Yang,Junlin Shang,Senjie Jin,Ming Zhang,Jiazheng Zhang,Caishuang Huang,Yunke Zhang,Demei Yan,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: DVPO introduces distributional value modeling with risk-aware policy optimization to improve robustness and generalization under noisy supervision; it uses token-level value distributions and asymmetric risk regularization to dampen noise while preserving exploration, outperforming PPO/GRPO and robust Bellman PPO across dialogue, math reasoning, and scientific QA tasks.


<details>
  <summary>Details</summary>
Motivation: 现实世界对话系统和LLM在训练后期面临噪声或不完整的监督信号，现有鲁棒方法可能牺牲泛化能力或过于保守，因此需要在鲁棒性与泛化之间取得平衡。

Method: 将条件风险理论与分布式RL结合，学习令牌级的价值分布并引入不对称的风险正则化：下尾收缩以抑制噪声导致的负偏差，上尾扩张以保持探索多样性，并将其嵌入策略优化过程中；在多轮对话、数学推理和科学问答等任务上进行广泛实验。

Result: DVPO在噪声监督下持续优于PPO、GRPO及基于鲁棒Bellman的PPO，在多任务设置中显示出更好的鲁棒性和泛化能力，体现了在LLM后训练中对实际应用场景的潜力。

Conclusion: 通过对价值分布的建模并进行尾部风险调控，DVPO实现了对鲁棒性与泛化性的更好平衡，既抑制有害的负向波动，又保留探索多样性，为LLM后训练提供了有前景的方向。

Abstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.

</details>


### [79] [Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment](https://arxiv.org/abs/2512.03864)
*Danny Hoang,Anandkumar Patel,Ruimen Chen,Rajiv Malhotra,Farhad Imani*

Main category: cs.LG

TL;DR: HyperDimensional Computing (HDC) 提供了一种能量高效的人工智能替代方案，用于智能制造中的几何质量预测；在与传统模型比较中，HDC 能在保持相近准确度的同时显著降低能耗并提升速度。


<details>
  <summary>Details</summary>
Motivation: 在智能制造环境中，AI 模型的能耗可能抵消通过感测预测带来的效率提升，需要寻找能量更低的替代方法来实现就地质量预测。

Method: 基于就地感测的几何质量预测，比较常见 AI 模型与 HDC 的能耗、准确性和速度；评估训练和推理阶段的能效与时间成本。

Result: HDC 的准确率与常规模型相当，但能耗显著降低：训练能耗降低约200倍，推理能耗降低约175到1000倍；训练时间减少约200倍，推理时间减少约300到600倍。

Conclusion: HDC 展现出在智能制造领域的强大潜力，成为能量高效的 AI 解决方案，能够在就地预测任务中实现更快且更节省能源的推理与训练。

Abstract: Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\times$ for training and 175 to 1000$\times$ for inference. Furthermore, HDC reduces training times by 200$\times$ and inference times by 300 to 600$\times$, showcasing its potential for energy-efficient smart manufacturing.

</details>


### [80] [Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2512.03899)
*Janis Keck,Lukas Silvester Barth,Fatemeh,Fahimi,Parvaneh Joharinad,Jürgen Jost*

Main category: cs.LG

TL;DR: 提出一种将模糊单纯形集视为面偏序上概率测度边缘的生成模型框架，揭示UMAP权重从随机尺度的Vietoris–Rips滤泡采样得到的距离分布CDF中产生，进而统一了相关的跨领域理论并可衍生新嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 弥合模糊单纯形集与常用统计学习框架之间的联系，给出基于概率的解释以统一KL散度、模糊交叉熵等，并解释UMAP在该框架中的角色。

Method: 构建在面偏序上的概率模型；以随机尺度对Vietoris–Rips滤泡进行采样，得到成对距离的累积分布函数作为权重；将KL散度与模糊交叉熵联系起来；通过布尔运算在底层单纯形集合上导出t-norm/t-conorm；提出用Čech滤泡与三元组采样等方式派生新的嵌入方法。

Result: 证明模糊权重可由生成模型产生，且与标准的t-norm/t-conorm相容；提供一个框架以从该概率视角系统推导UMAP及其他嵌入方法的变体，且用Čech滤泡与三元组采样示例说明潜在的扩展性。

Conclusion: 给出一个统一的概率理论基础来解释模糊单纯形集，澄清UMAP在该框架中的角色，并可系统化地推导出新的降维方法。

Abstract: Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.

</details>


### [81] [Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization](https://arxiv.org/abs/2512.03928)
*Michele Alessi,Alessio Ansuini,Alex Rodriguez*

Main category: cs.LG

TL;DR: 提出 DiVAE，一种轻量、数据驱动的正则化器，用于将 VAE 的潜在先验对齐到数据密度的对数密度，改善潜在分布对齐、先验覆盖及 OOD 不确定性。


<details>
  <summary>Details</summary>
Motivation: 标准 VAE 只将潜变量对齐到简单先验，忽略数据空间的密度结构，导致潜在密度与数据密度不匹配，影响可解释性和 OOD 不确定性。

Method: 引入 Density-Informed VAE（DiVAE），在 ELBO 中加入鲁棒、精度加权的惩罚项，使 log p_Z(z) 与从数据估计的密度相一致；若先验可学习，则将其引导到高密度区域；总体开销很小。

Result: 在合成数据集上，DiVAE 提升潜在 log-density 与真实分布的对齐、改善先验覆盖、提升 OOD 不确定性校准；在 MNIST 上，提升先验与外部密度估计的一致性、增强解释性，并提升可学习先验的 OOD 检测性能。

Conclusion: DiVAE 提供一个轻量、数据驱动的正则化策略，能在不显著增加计算成本的前提下改善潜在变量的密度对齐、先验覆盖和 OOD 探测，同时增强对数据密度的解释性。

Abstract: We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.

</details>


### [82] [Technical Report on Text Dataset Distillation](https://arxiv.org/abs/2512.03967)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Rosimeire Pereira Costa,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 文本数据集蒸馏旨在将大规模文本数据压缩成小型合成数据，以在训练过程中获得近似的结果；尽管在 Transformer、离散文本生成以及超大解码器模型方面取得进展，领域仍在成长阶段，面临基准标准化、离散文本处理、复杂任务和真实应用等挑战。


<details>
  <summary>Details</summary>
Motivation: 提升文本数据集的可用性和训练效率，降低成本，并在文本任务中实现接近原始数据集的性能。

Method: 对文本数据集蒸馏的历史与现状进行综述，梳理从早期方法到基于 Transformer 的方法、离散文本生成策略，以及扩展到大规模解码器模型的相关工作，比较不同蒸馏策略与技术路径。

Result: 回顾总结该领域的关键贡献与发展脉络，指出在基准化、离散文本处理、对复杂任务的适应性以及实际应用方面存在的主要挑战与研究空白。

Conclusion: 文本数据集蒸馏处于逐步成熟阶段，但仍需标准化基准、改进离散文本处理方法、提升多任务适应性，并提供更明确的现实世界应用案例。

Abstract: In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.

</details>


### [83] [Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning](https://arxiv.org/abs/2512.03973)
*Franki Nguimatsia Tiofack,Théotime Le Hellard,Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: GFP introduces Guided Flow Policy, coupling a multi-step flow-matching policy with a distilled one-step actor. The actor biases behavior cloning towards high-value dataset actions, while the flow policy stays aligned with the dataset and maximizes the critic. Mutual guidance yields state-of-the-art results on 144 tasks across OGBench, Minari, and D4RL, especially on suboptimal datasets and hard tasks.


<details>
  <summary>Details</summary>
Motivation: Offline RL often uses behavior regularization to keep policies near the dataset, but this regularization treats all state-action pairs equally and cannot distinguish high-value actions. There is a need to emphasize high-value behavior while maintaining fidelity to the dataset.

Method: GFP pairs a multi-step flow-matching policy with a distilled one-step actor. The actor applies weighted behavior cloning to prioritize high-value actions from the dataset, guiding the flow policy. The flow policy, in turn, constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. The two components guide each other to optimize offline performance.

Result: GFP achieves state-of-the-art performance across 144 state- and pixel-based tasks from OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks.

Conclusion: Mutual guidance between a flow-based policy and a distilled one-step actor enables selective imitation of high-value actions and alignment with top transitions, leading to robust offline RL performance across diverse tasks.

Abstract: Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/

</details>


### [84] [Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs](https://arxiv.org/abs/2512.03994)
*Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein*

Main category: cs.LG

TL;DR: A lightweight, training-free method for policy-violation detection in LLMs, using whitening to decorrelate activations and Euclidean norm as a compliance score; achieves state-of-the-art results on a challenging policy benchmark and is easily deployable with policy text and a few examples.


<details>
  <summary>Details</summary>
Motivation: Enterprises require robust, interpretable, and low-latency mechanisms to detect policy violations in LLM outputs within sensitive domains; existing guardrails are limited to generic safety and may miss nuanced organizational policies, exposing legal and reputational risks.

Method: Treat policy violation detection as an out-of-distribution problem. Apply a whitening transformation to the model's hidden activations to decorrelate features and standardize them to zero mean and unit variance (near-identity covariance). In the transformed space, use the Euclidean norm as the compliance score. The approach is training-free and uses only policy text plus a small number of illustrative samples.

Result: On a challenging policy benchmark, the method achieves state-of-the-art results, outperforming both existing guardrails and fine-tuned reasoning models.

Conclusion: Provides a practical, statistically grounded framework for policy-aware oversight of LLMs and contributes to deployable AI governance; requires minimal data and is easily deployable with existing policy texts.

Abstract: Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection

</details>


### [85] [Physics-Embedded Gaussian Process for Traffic State Estimation](https://arxiv.org/abs/2512.04004)
*Yanlin Chen,Kehua Chen,Yinhai Wang*

Main category: cs.LG

TL;DR: 提出一种物理嵌入高斯过程的交通状态估计框架（PEGP），通过对流量经典模型的线性化微分算子构造两类多输出核，将物理先验融入数据驱动的高斯过程，解决低穿透率与观测稀疏条件下的泛化与不确定性校准问题。


<details>
  <summary>Details</summary>
Motivation: 解决纯数据驱动方法对物理规律缺乏解释、在观测稀疏时泛化能力差，以及纯物理模型难以有效处理不确定性的问题；通过把物理结构嵌入高斯过程以实现更可信的交通状态估计与不确定性量化。

Method: 以经典交通流模型（如 ARZ、LWR）为基础，利用线性化微分算子显式构造两种多输出核；在高D/NGSIM数据集上进行实验，比较含物理约束的PEGp 与无物理约束的基线，并通过消融研究分析残差与物理规律的一致性以及方差场的性质。

Result: 与非物理基线相比，PEGP 提高了估计准确性；PEGP-ARZ 在观测稀疏时更可靠，PEGP-LWR 在观测密集时误差更低；消融实验显示 PEGP-ARZ 残差与物理更一致且不确定性校准良好，而 PEGP-LWR 的残差更正交且方差近似常数。

Conclusion: 将物理先验与不确定性量化结合的 PEGP 框架可为交通状态估计提供更可靠的支撑。

Abstract: Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.

</details>


### [86] [Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions](https://arxiv.org/abs/2512.04034)
*Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 单域监督学习导致域特征崩溃，信息瓶颈使 I(x_d; z)=0，进而在OOD检测上灾难性失败；通过 Domain Bench 和域过滤恢复域信息、提高鲁棒性，揭示监督学习在窄域的局限及其对迁移学习的启示。


<details>
  <summary>Details</summary>
Motivation: 解释为何在单域数据训练时，最先进的OOD检测方法会出现灾难性失败；提供信息论视角的第一性理论解释，并量化崩溃程度及其在实际数据中的表现。

Method: 以信息论框架分析监督学习在单域数据上的表示学习，证明域特征崩溃 I(x_d; z)=0 是信息瓶颈优化的必然结果；通过引入 Fano 不等式扩展对实际场景的部分崩溃进行量化；构建 Domain Bench 作为单域数据基准；通过域过滤（利用预训练表示）来保持 I(x_d; z)>0 以验证理论。

Result: 理论上证明单域训练会导致域信息完全丢失（I(x_d; z)=0），从而在OOD检测中表现出灾难性失败（如 MNIST 场景的高 FPR@95）。通过 Fano 不等式对实际场景的崩溃进行量化；提出 Domain Bench 基准并实验表明，保留域信息（I(x_d; z)>0）通过域过滤可缓解/解决这一失败模式。域过滤虽直观，但提供了强有力的经验证据支持信息理论框架。

Conclusion: 工作揭示了一个普遍存在的监督学习局限，即在窄域条件下可能丧失关键信息特征，影响迁移学习与是否微调/冻结预训练模型的决策；为OOD鲁棒性研究提供新的信息论视角与解决路径（如保持域信息的简单域过滤方法）。

Abstract: Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.

</details>


### [87] [Convergence for Discrete Parameter Updates](https://arxiv.org/abs/2512.04051)
*Paul Wilson,Fabio Zanasi,George Constantinides*

Main category: cs.LG

TL;DR: 提出以离散更新规则为核心的训练框架，避免对连续更新进行量化；给出收敛性证明、一个多项式更新规则的具体实现及实证评估。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习训练中的高计算与能耗成本，探索低精度/离散化替代方案，避免把连续更新量化成较小位宽的数值。

Method: 构建一般性离散更新规则的理论框架，给出该类更新的收敛性证明，并提出一个具体的多项式更新规则作为示例，同时给出实验以验证其有效性。

Result: 给出对一般离散更新类的收敛性保证，并通过多项式更新规则的实验评估展示可行性，实验表明在部分场景中具有竞争力。

Conclusion: 离散更新为高效训练提供新路径，特别适用于具有离散结构的模型，可能降低通信/计算成本并拓展低精度训练的应用。

Abstract: Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.

</details>


### [88] [Eval Factsheets: A Structured Framework for Documenting AI Evaluations](https://arxiv.org/abs/2512.04062)
*Florian Bordes,Candace Ross,Justine T Kao,Evangelia Spiliopoulou,Adina Williams*

Main category: cs.LG

TL;DR: Eval Factsheets 提出一个用于 AI 系统评估的结构化框架，通过五维度的分类和问卷化来提升评估方法的透明度、可重复性与可比性，并通过案例研究验证其适用性。


<details>
  <summary>Details</summary>
Motivation: 评估基准的快速扩散导致可重复性、透明度、信息决策困难；与数据集/模型拥有结构化文档框架（Datasheets、Model Cards）不同，评估方法缺乏系统化文档；需要类似的文档框架。

Method: 将评估特征分为 Context/Scope/Structure/Method/Alignment 五个维度，开发成综述式、包含强制和推荐要素的问卷形式；进行多项基准的案例研究，覆盖传统基准到“大语言模型作为评审”的评估范式。

Result: Eval Factsheets 能够捕捉多样化评估范式，保持一致性和可比性；演示了从传统基准到 LLM 作为评判者等场景的适用性。

Conclusion: 希望将 Eval Factsheets 纳入现有及新发布的评估框架中，以提升透明度和可重复性。

Abstract: The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.

</details>


### [89] [Fare Comparison App of Uber, Ola and Rapido](https://arxiv.org/abs/2512.04065)
*Ashlesha Gopinath Sawant,Sahil S. Jadhav,Vidhan R. Jain,Shriraj S. Jagtap,Prachi Jadhav,Soham Jadhav,Ichha Raina*

Main category: cs.LG

TL;DR: 一个用于对比 Ola、Uber、Rapido 车费的网页/移动应用原型，后台用 Python 获取数据并计算最佳选项；并讨论数据获取的挑战。


<details>
  <summary>Details</summary>
Motivation: 在日益增多的网约车服务环境中，帮助用户以成本更低且耗时更短的方式出行，提升透明度和用户体验。

Method: 前端/后端架构：前端提供输入目的地，后端通过调用各平台的 API 获取价格数据，执行比价逻辑并输出最佳选项；考虑 API 访问难点、Android Studio 仿真、Appium 测试与定位比对等。

Result: 论文未给出明确实验结果；提出了可运行的原型并展示了价格对比和最佳选项的实现思路。

Conclusion: 该工作旨在提升 ride-hailing 服务透明度与效率，为用户提供更优体验，并指出在数据获取与跨平台整合方面的挑战。

Abstract: In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [90] [CFO-Robust Detection for 5G PRACH under Fading Channels: Analytical Modeling and Performance Evaluation](https://arxiv.org/abs/2512.03096)
*Daniel Alarcón-Martín,Mari Carmen Aguayo-Torres,Francisco J. Martín-Vega,Gerardo Gómez*

Main category: cs.IT

TL;DR: 提出一个统一的分析框架来刻画在平坦瑞利衰落下的PRACH接收功率-时延特征分布，覆盖两种重复策略（相干和功率叠加），并给出最优阈值和闭式検验概率；同时考察相干时间的两种情形，并设计基于CFO相关性的低复杂度检测器，实验证明在独立通道时功率叠加优于相干，且在严重CFO下提出的检测器更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 5G/6G初始接入对PRACH的鲁棒性要求高，现有研究往往仅针对特定场景且缺乏对检测性能的系统性分析。需要一个统一的分析框架来刻画不同重复策略和CFO等综合 impairments 下的检测性能。

Method: 构建在平坦瑞利衰落下的接收 PDP 的统计分布分析框架，覆盖两种重复策略的最优阈值推导与闭式检测概率；区分两类相干时间情形（重复之间信道相同/独立）；利用CFO在循环移位上的相关性，设计一个低复杂度的PDP依赖性检测器；通过数值仿真评估不同策略在不同信道相关性与CFO条件下的性能。

Result: 给出两种重复策略的最优阈值表达式和闭式检测概率；在相同信道实现时，少数情形下相干策略优于功率叠加，但当重复间信道独立时功率叠加性能更优；提出的CFO感知检测器在强CFO条件下显著提升鲁棒性。

Conclusion: 给出一个统一的分析与检测框架，帮助在不同信道相关性和CFO条件下选择重复策略并提升PRACH检测鲁棒性，且提出的CFO-aware检测器在实际场景中的鲁棒性提升具有潜在应用价值。

Abstract: The Physical Random Access Channel (PRACH) is essential for initial access and synchronization in both 5G and future 6G networks; however, its detection is highly sensitive to impairments such as high user density, large carrier frequency offset (CFO), and fast fading. Although prior studies have examined PRACH detection, they are often restricted to specific scenarios or lack a comprehensive analytical characterization of performance. We introduce a unified analytical framework that characterizes the statistical distribution of the received power delay profile (PDP) under flat Rayleigh fading and supports both coherent combining (CC) and power combining (PC) repetition strategies. For each strategy, we derive optimal threshold expressions and closed-form detection probabilities. Furthermore, we analyze two key cases depending on the coherence time: identical and independent channel realizations per repetition. Secondly, we exploit the correlation induced by CFO across cyclic shifts to design a novel low-complexity detector that exploits PDP dependencies. Numerical results indicate that PC outperforms CC when repetitions experience independent channels, while CC can be preferable under identical realizations in limited settings. On the other hand, the proposed CFO-aware detector delivers improved robustness under severe CFO conditions.

</details>


### [91] [Strengthening Han's Fourier Entropy-Influence Inequality via an Information-Theoretic Proof](https://arxiv.org/abs/2512.03117)
*Peijie Li,Guangyue Han*

Main category: cs.IT

TL;DR: 将傅里叶系数熵与影响的关系式H[âf] ≤ C1 I(f) + C2 ∑i I_i(f) ln(1/I_i(f))提升到全实值单位L2范数的布尔函数上，常数达到最优C1=C2=1，优于原始C1=3+2ln2, C2=1的情形。


<details>
  <summary>Details</summary>
Motivation: 揭示傅里叶谱的熵与每个分量的影响之间的基本联系，给出一个简洁、可推广的边界，具有结构性和普遍性意义。

Method: 给出一个简短的信息论证明，利用熵的分解、链式不等式及对称性等工具，将不等式的常数收敛到1，并在更广的实值布尔函数族上成立。

Result: 对所有单位L2范数的实值布尔函数，H[hat{f}] ≤ I(f) + ∑i I_i(f) ln(1/I_i(f))，且常数1是最优的（即不可再减小），从而将原来对{−1,1}的结果推广为一个普遍的结构性质。

Conclusion: 该不等式成为一个基本的、易于理解的熵-影响结构特性，为后续在学习理论、信息论和极小/高维布尔分析中的应用提供了更强的理论基础。

Abstract: We strengthen Han's Fourier entropy-influence inequality $$ H[\widehat{f}] \leq C_{1}I(f) + C_{2}\sum_{i\in [n]}I_{i}(f)\ln\frac{1}{I_{i}(f)} $$ originally proved for $\{-1,1\}$-valued Boolean functions with $C_{1}=3+2\ln 2$ and $C_{2}=1$. We show, by a short information-theoretic proof, that it in fact holds with sharp constants $C_{1}=C_{2}=1$ for all real-valued Boolean functions of unit $L^{2}$-norm, thereby establishing the inequality as an elementary structural property of Shannon entropy and influence.

</details>


### [92] [Multi-Source M/G/1/1 Queues with Probabilistic Preemption](https://arxiv.org/abs/2512.03241)
*Mohammad Moltafet,Hamid R. Sadjadpour,Zouheir Rezki,Marian Codreanu,Roy D. Yates*

Main category: cs.IT

TL;DR: 对多源状态更新系统在无缓冲、单服务器、Poisson产生、M/G/1/1 排队模型下，提出带有固定概率替换的概率性预退策略，推导各源的AoI和PAoI的MGF，并用数值结果验证策略有效性。


<details>
  <summary>Details</summary>
Motivation: 在单服务器、无等待缓冲的多源状态更新场景中，如何通过有效的包管理策略降低信息新鲜度指标（AoI/PAoI）是一个关键问题。对一个更一般的服务时间分布进行分析，提供一种可调控的策略以在不同源之间动态平衡更新频率与系统容量限制。

Method: 引入一种概率性预退包管理策略：当新包到达且系统中存在同源的未完成包时，以固定概率将正在系统中的该源包替换为新到达的包；在无缓冲的M/G/1/1队列下，推导各源的AoI与PAoI的矩MGF（矩MGF）表达式，涵盖一般服务时间分布并对不同源进行并行分析；并进行数值仿真以验证解析结果。

Result: 获得各源 AoI 与 PAoI 的MGF表达式，揭示在给定替换概率下的性能耦合关系；数值结果显示该策略可有效改善多源下的时效性指标，并给出策略设计的定量选取范围。

Conclusion: 所提出的概率性预退策略在无缓存单服务器多源M/G/1/1系统中可通过MGF形式明确刻画AoI/PAoI分布，提供对策略参数的定量影响分析，为实际系统的参数选择与时效性优化提供理论支撑。

Abstract: We consider a multi-source status update system consisting of multiple independent sources, a single server, and a single sink. Each source generates packets according to a Poisson process, and packets are served according to a general service time distribution. The system has a capacity of one packet, i.e., no waiting buffer, and is modeled as a multi-source M/G/1/1 queueing system. We introduce a probabilistically preemptive packet management policy, under which an existing packet from the same source in the system is replaced by an arriving packet with a fixed probability. We derive the moment generating functions (MGFs) of the age of information (AoI) and peak AoI (PAoI) for each source under this policy. Numerical results demonstrate the effectiveness of the proposed packet management policy.

</details>


### [93] [Generalized Orthogonal Approximate Message-Passing for Sublinear Sparsity](https://arxiv.org/abs/2512.03326)
*Keigo Takeuchi*

Main category: cs.IT

TL;DR: GOAMP 针对子线性稀疏信号在广义线性测量下的重构，解决 AMP 收敛性问题，利用 Onsager 修正实现误差渐近高斯化，在正交不变矩阵下给出阈值条件的无误重构，并在线性与 1-bit CS 场景中验证性能，尤其在病态矩阵下优于现有子线性方法。


<details>
  <summary>Details</summary>
Motivation: 传统 AMP 在非高斯或非标准矩阵下收敛性差，且对子线性稀疏的场景亟需稳定高效的重构算法。

Method: 提出 generalized orthogonal AMP (GOAMP)，通过 Onsager 修正实现估计误差的渐近高斯性；在 sublinear sparsity 的极限下用状态方程设计 Onsager 修正；对线性测量和 1-bit CS 进行数值验证，给出在非零支集不包含原点邻域时测量维度超过阈值即可实现无误重构的充要条件；在病态/低秩矩阵下，GOAMP 优于现有的 Sublinear AMP。

Result: 理论上，若非零支集不包含原点邻域且测量维度超过阈值，则 GOAMP 实现误差无损重构；数值实验（线性与 1-bit CS）显示在正交不变矩阵及病态矩阵条件下，GOAMP 的性能优于传统 AMP 及其他子线性稀疏方法。

Conclusion: GOAMP 将 AMP 的适用范围扩展到子线性稀疏与正交不变矩阵场景，提供了理论和数值证据，证明其稳健性和高效性。

Abstract: This paper addresses the reconstruction of sparse signals from generalized linear measurements. Signal sparsity is assumed to be sublinear in the signal dimension while it was proportional to the signal dimension in conventional research. Approximate message-passing (AMP) has poor convergence properties for sensing matrices beyond standard Gaussian matrices. To solve this convergence issue, generalized orthogonal AMP (GOAMP) is proposed for signals with sublinear sparsity. The main feature of GOAMP is the so-called Onsager correction to realize asymptotic Gaussianity of estimation errors. The Onsager correction in GOAMP is designed via state evolution for orthogonally invariant sensing matrices in the sublinear sparsity limit, where the signal sparsity and measurement dimension tend to infinity at sublinear speed in the signal dimension. When the support of non-zero signals does not contain a neighborhood of the origin, GOAMP using Bayesian denoisers is proved to achieve error-free signal reconstruction for linear measurements if and only if the measurement dimension is larger than a threshold, which is equal to that of AMP for standard Gaussian sensing matrices. Numerical simulations are also presented for linear measurements and 1-bit compressed sensing. When ill-conditioned sensing matrices are used, GOAMP for sublinear sparsity is shown to outperform existing reconstruction algorithms, including generalized AMP for sublinear sparsity.

</details>


### [94] [Expected Confidence Dependency: A Novel Rough Set-Based Approach to Feature Selection](https://arxiv.org/abs/2512.03612)
*Saeed Rasouli,Hamid Karamikabir*

Main category: cs.IT

TL;DR: 提出了期望置信度依赖（ECD），一种面向软计算、基于置信度贡献的特征选择依赖度，在粗糙集框架内将等价块的贡献以期望算子正则化聚合。与传统二元依赖度不同，ECD 提供连续、可加权的依赖评估，并在理论上满足归一化、与经典依赖的一致性、单调性以及对结构及标签保持不变性的性质。


<details>
  <summary>Details</summary>
Motivation: 弥补传统粗糙集依赖度以二元性描述条件块的局限，提供一种基于置信度贡献的软性、精度驱动的特征选择度量，兼容软计算范式并提高对不确定性和部分相关性的建模能力。

Method: 提出期望置信度依赖（ECD）作为对等价块的置信度贡献的加权平均，通过一个归一化的期望运算将贡献聚合，形成对整体样本集的依赖度度量。对 ECD 进行了严格的性质研究，证明其归一化、与经典依赖度的一致性、单调性，以及对结构性和标签保持变换的不变性等性质。

Result: 证明了若干理想性质，并指出在无模糊情形下 ECD 能退化回经典依赖度；提供了一个在软计算场景下更细粒度的特征选择度量框架。

Conclusion: ECD 提供了一种鲁棒的依赖度量，结合置信度分布进行软计算场景下的特征选择，理论上更贴近数据的真实相关性，并具备在实际应用中的潜在优势。

Abstract: This paper proposes Expected Confidence Dependency (ECD), a novel, soft computing-oriented, accuracy driven dependency measure for feature selection within the rough set theory framework. Unlike traditional rough set dependency measures that rely on binary characterizations of conditional blocks, ECD assigns confidence-based contributions to individual equivalence blocks and aggregates them through a normalized expectation operator. We formally establish several desirable properties of ECD, including normalization, compatibility with classical dependency, monotonicity, and invariance under structural and label-preserving transformations.

</details>


### [95] [Movable Signals with Dual-Polarized Fixed Intelligent Surfaces: Beyond Diagonal Reflection Matrices](https://arxiv.org/abs/2512.03872)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 在双极化智能表面系统中，带可移动信号的FIS对比RIS具有显著性能优势；非对角反射矩阵的FIS（beyond-diagonal FIS）在发/收极化不匹配时还能带来额外增益，总体至少提升4倍。


<details>
  <summary>Details</summary>
Motivation: 探索双极化智能表面在无线系统中的性能潜力，系统比较两类表面（RIS与FIS）及其矩阵结构（对角与非对角），以确定哪种设计在不同极化条件下提供更高的吞吐与覆盖。

Method: 建立RIS与FIS的信道/信号模型，分别考虑对角和非对角反射矩阵；在双极化场景下对比分析传输性能（如SNR/速率）并量化增益，特别关注发/收极化是否对齐。

Result: 结果显示：对于同等条件，带可移动信号的FIS始终优于RIS，至少获得4倍的性能增益；当传输端和接收端极化不同时，beyond-diagonal FIS能进一步提升性能。

Conclusion: 结论指出，FIS在双极化设置中对比RIS具有显著优势，且非对角反射结构（beyond-diagonal）在极化不匹配时可带来额外收益，建议未来在设计中重视FIS的可移动性与非对角反射能力。

Abstract: This paper investigates wireless systems aided by dual-polarized intelligent surfaces. We compare reconfigurable intelligent surface (RIS), which adjust their reflection matrices, with movable signals operating with fixed intelligent surface (FIS), which adjust the signal frequency while the surface properties remain fixed. For both RIS and FIS, we consider surfaces with a diagonal reflection matrix, named diagonal RIS/FIS, and surfaces with a reflection matrix not limited to being diagonal, named beyond-diagonal RIS/FIS. Movable signals with FIS always outperform RIS, achieving at least a fourfold gain. When transmitter and receiver polarizations differ, beyond-diagonal FIS further enhances performance.

</details>


### [96] [On topological and algebraic structures of categorical random variables](https://arxiv.org/abs/2512.04020)
*Inocencio Ortiz,Santiago Gómez-Guerrero,Christian E. Schaerer*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Based on entropy and symmetrical uncertainty (SU), we define a metric for categorical random variables and show that this metric can be promoted into an appropriate quotient space of categorical random variables. Moreover, we also show that there is a natural commutative monoid structure in the same quotient space, which is compatible with the topology induced by the metric, in the sense that the monoid operation is continuous.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [97] [Time-Invariant Polytopic and Interval Observers for Uncertain Linear Systems via Non-Square Transformation](https://arxiv.org/abs/2512.03160)
*Feiya Zhu,Tarun Pati,Sze Zheng Yong*

Main category: eess.SY

TL;DR: 提出一种适用于不确定线性CT/DT系统的多面体和区间观测器，能在有界干扰下严格包络真实状态，并使观测集估计具有输入—状态稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决不确定性、噪声与约束条件下的线性系统观测难题，提供一个普适、可实现且可达到最优性的观测器设计框架，覆盖所有可检测系统且在任意最佳观测设计下仍能稳定工作。

Method: 结合多面体Lyapunov函数、混合单调嵌入系统，以及潜在的非方阵提升坐标变换，构建时不变坐标系中的 polytopic/interval 观测器。该框架无需正性约束，适用于连续时间和离散时间系统，针对有界干扰和噪声实现状态域的包络与输入-状态稳定性保障。

Result: 通过若干不确定线性CT与DT系统的实例验证，观测器能在干扰与噪声作用下保证真性状态和输入-状态稳定性的包络性，且适用于所有可检测系统在任何最优观测设计下的稳定性要求。

Conclusion: 该方法扩展了观测器设计的适用性与鲁棒性，使在未受正性约束要求、且对系统不确定性具有容忍度的情况下，仍可实现可行且最优的多面体/区间观测器，适用于理论研究与工程应用。

Abstract: This paper presents novel polytopic and interval observer designs for uncertain linear continuous-time (CT) and discrete-time (DT) systems subjected to bounded disturbances and noise. Our approach guarantees enclosure of the true state and input-to-state stability (ISS) of the polytopic and interval set estimates. Notably, our approach applies to all detectable systems that are stabilized by any optimal observer design, utilizing a potentially non-square (lifted) time-invariant coordinate transformation based on polyhedral Lyapunov functions and mixed-monotone embedding systems that do not impose any positivity constraints, enabling feasible and optimal observer designs, even in cases where previous methods fail. The effectiveness of our approach is demonstrated through several examples of uncertain linear CT and DT systems.

</details>


### [98] [Physics-Based Communication Compression via Lyapunov-Weighted Event-Triggered Control](https://arxiv.org/abs/2512.03604)
*Abbas Tariverdi*

Main category: eess.SY

TL;DR: Introduces a static directional triggering mechanism for event-triggered control (ETC) using the Lyapunov matrix P to weight errors, creating an anisotropic half-space scaling that adapts to stability geometry.


<details>
  <summary>Details</summary>
Motivation: Conventional isotropic ETC thresholds ignore stability geometry, triggering conservatively and wasting communication; a directional mechanism can exploit mode asymmetry to reduce events without sacrificing stability.

Method: Define an anisotropic error bound via the Lyapunov function with weighting by P, yielding a static directional triggering rule that allows larger deviations along stable modes and tighter bounds near instability. Prove global asymptotic stability and absence of Zeno behavior.

Result: Monte Carlo simulations with N=100 show 43.6% fewer events than optimally tuned isotropic methods and 2.1× better control performance than time-varying alternatives.

Conclusion: The proposed anisotropic, P-weighted triggering mechanism acts as a runtime safety gate for learning-based controllers under communication constraints, balancing reduced communication with stable performance.

Abstract: Event-Triggered Control (ETC) reduces communication overhead in networked systems by transmitting only when stability requires it. Conventional mechanisms use isotropic error thresholds ($\|e\| \le σ\|x\|$), treating all directions equally. This ignores stability geometry and triggers conservatively. We propose a static directional triggering mechanism that exploits this asymmetry. By weighting errors via the Lyapunov matrix $P$, we define an anisotropic half-space scaling with instantaneous energy margins: larger deviations tolerated along stable modes, strict bounds where instability threatens. We prove global asymptotic stability and exclusion of Zeno behavior. Monte Carlo simulations ($N=100$) show 43.6\% fewer events than optimally tuned isotropic methods while achieving $2.1\times$ better control performance than time-varying alternatives. The mechanism functions as a runtime safety gate for learning-based controllers operating under communication constraints.

</details>


### [99] [A Perception-feedback position-tracking control for quadrotors](https://arxiv.org/abs/2512.03605)
*Eduardo Espindola,Yu Tang*

Main category: eess.SY

TL;DR: 提出一种基于感知反馈的四旋翼定位跟踪控制器，直接利用机载传感器（低成本IMU和GPS）的测量来产生控制指令，无需状态估计；对陀螺仪偏置进行校正，提高跟踪性能；通过Lyapunov分析证明了跟踪误差系统在外部干扰存在时的实用稳定性，在无干扰时呈指数稳定；并通过数值仿真验证鲁棒性与控制性能。


<details>
  <summary>Details</summary>
Motivation: 在不利用完整状态估计的情况下，直接利用感知反馈实现对四旋翼的高精度位置跟踪，并通过陀螺仪偏置校正提升鲁棒性与稳定性。

Method: 基于感知反馈的定位跟踪控制器设计，直接使用机载传感器（IMU、GPS）输出来产生控制指令；同时对陀螺仪偏置进行估计/校正；利用Lyapunov分析证明系统的实用稳定性（在外部干扰下的稳定性），且在无干扰时达到指数稳定；通过数值仿真实验评估鲁棒性（噪声、参数不确定性下的表现）。

Result: 给出了实际可实现的控制框架并给出稳定性证明；数值仿真表明在噪声与参数不确定性存在的情况下仍保持良好跟踪性能，且对外部干扰具有鲁棒性。

Conclusion: 该工作证明了不依赖状态估计的感知反馈跟踪控制在四旋翼中的可行性，提供了理论稳定性保障和对现实条件的鲁棒性验证，具有潜在的工程应用价值。

Abstract: In this paper a position-tracking controller for quadrotors based on perception feedback is developed, which directly uses measurements from onboard sensors such as low cost IMUs and GPS to generate the control commands without state estimation. Bias in gyros sensors are corrected to enhance the tracking performance. Practical stability of the origin of the tracking error system in the presence of external disturbances is proved using the Lyapunov analysis, which turns out to exponential stability in the absence of external disturbances. Numerical simulations are included to illustrate the proposed control scheme and to verify the robustness of the proposed controller under noisy measurements and parameter uncertainties.

</details>


### [100] [Output-Constrained Controller with Fuzzy-Tuned Parameters for Overhead Cranes](https://arxiv.org/abs/2512.03680)
*Dawei Zhao,Kai Wang,Xianglong Zhou,Xin Ma,Lei Jia*

Main category: eess.SY

TL;DR: 提出一种基于模糊自适应的非线性控制方法，针对带双摆效应的桥式起重机，在扭矩抖动输出约束下实现抑振与高精定位。综合背踪控制、双曲正切函数及能量Lyapunov分析，结合在线可调模糊参数以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 桥式起重机在提升机构与摆角耦合强且存在扭矩输出约束与外部扰动时，摆动抑制与高精定位成为关键难点。需将系统耦合性、能量形式化和不确定性整合在一个稳定且鲁棒的控制框架中，以提升动态性能与安全性。

Method: 构造一个耦合提升量位移与摆角的复合误差信号；基于该信号建立能量Lyapunov函数，给出包含新惯性矩阵与势能项的系统能量表示；在回退控制（backstepping）与双曲正切函数约束下设计部分性能的控制律；引入能够在线自适应调整系统参数的模糊控制策略以提升鲁棒性；利用Lyapunov理论结合LaSalle不变集定理证明全局稳定性。

Result: 仿真结果表明所提控制器在扭矩输出约束下具备更优的摆振抑制能力和定位精度，动态响应和鲁棒性显著提升。

Conclusion: 所提出的模糊自适应非线性控制框架能在带双摆效应的桥式起重机系统中实现稳定、鲁棒的闭环控制，有效处理扭矩抖动约束与不确定性，具备良好的工程应用前景。

Abstract: This study proposes a fuzzy-adjusted nonlinear control method based on torque jitter output limit constraints for overhead crane systems with double pendulum effects. The proposed control method can effectively suppress swing and achieve precise positioning. Firstly, by enhancing the coupling relationship between the trolley displacement and swing angle, a composite signal with an error term was designed. Then, an energy-based Lyapunov function was constructed using the composite error signal, which incorporated a new formulation of the inertia matrix and potential energy function. Subsequently, using the backstepping method in conjunction with the hyperbolic tangent function, a controller with partial performance constraints was designed. In addition, to further enhance the system's dynamic performance, a fuzzy control scheme with online adjustable system parameters was designed. Finally, the stability of the system is proven using Lyapunov theory combined with LaSalle's invariance principle. Simulation results demonstrate that the proposed controller exhibits superior performance and robustness.

</details>


### [101] [Sample-Efficient Model-Free Policy Gradient Methods for Stochastic LQR via Robust Linear Regression](https://arxiv.org/abs/2512.03764)
*Bowen Song,Sebastien Gros,Andrea Iannelli*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Policy gradient algorithms are widely used in reinforcement learning and belong to the class of approximate dynamic programming methods. This paper studies two key policy gradient algorithms - the Natural Policy Gradient and the Gauss-Newton Method - for solving the Linear Quadratic Regulator (LQR) problem in unknown stochastic linear systems. The main challenge lies in obtaining an unbiased gradient estimate from noisy data due to errors-in-variables in linear regression. This issue is addressed by employing a primal-dual estimation procedure. Using this novel gradient estimation scheme, the paper establishes convergence guarantees with a sample complexity of order O(1/epsilon). Theoretical results are further supported by numerical experiments, which demonstrate the effectiveness of the proposed algorithms.

</details>


### [102] [CaFTRA: Frequency-Domain Correlation-Aware Feedback-Free MIMO Transmission and Resource Allocation for 6G and Beyond](https://arxiv.org/abs/2512.03767)
*Bo Qian,Hanlin Wu,Jiacheng Chen,Yunting Xu,Xiaoyu Wang,Haibo Zhou,Yusheng Ji*

Main category: eess.SY

TL;DR: 提出 CaFTRA 框架，实现无反馈、基于地理位置信息的 CSI 预测，并通过多基站多资源块分配实现高效 6G 通信。


<details>
  <summary>Details</summary>
Motivation: 解决反馈瓶颈、提升频谱效率和覆盖范围，支撑 AI-native 6G 的需求。

Method: 利用 Learnable Queries-driven Transformer Network 预测频域 CSI；基于地理位置信息；使用低复杂度多对一匹配算法进行多基站-多 RB 的资源分配，确保稳定收敛。

Result: 仿真显示稳定匹配收敛，光谱效率和用户公平性显著优于5G。

Conclusion: CaFTRA 展示了无反馈传输+AI 原生 6G 的潜力，有望推动标准化。

Abstract: The fundamental design of wireless systems toward AI-native 6G and beyond is driven by the need for ever-increasing demand of mobile data traffic, extreme spectral efficiency, and adaptability across diverse service scenarios. To overcome the limitations posed by feedback-based multiple-input and multiple-output (MIMO) transmission, we propose a novel frequency-domain Correlation-aware Feedback-free MIMO Transmission and Resource Allocation (CaFTRA) framework tailored for fully-decoupled radio access networks (FD-RAN) to meet the emerging requirements of AI-Native 6G and beyond. By leveraging artificial intelligence (AI), CaFTRA effectively eliminates real-time uplink feedback by predicting channel state information (CSI) based solely on user geolocation. We introduce a Learnable Queries-driven Transformer Network for CSI mapping from user geolocation, which utilizes multi-head attention and learnable query embeddings to accurately capture frequency-domain correlations among resource blocks (RBs), thereby significantly improving the precision of CSI prediction. Once base stations (BSs) adopt feedback-free transmission, their downlink transmission coverage can be significantly expanded due to the elimination of frequent uplink feedback. To enable efficient resource scheduling under such extensive-coverage scenarios, we apply a low-complexity many-to-one matching theory-based algorithm for efficient multi-BS association and multi-RB resource allocation, which is proven to converge to a stable matching within limited iterations. Simulation results demonstrate that CaFTRA achieves stable matching convergence and significant gains in spectral efficiency and user fairness compared to 5G, underscoring its potential value for 6G standardization efforts.

</details>


### [103] [Exact and Parametric Dynamical System Representation of Nonlinear Functions](https://arxiv.org/abs/2512.03779)
*Toshiyuki Ohtsuka*

Main category: eess.SY

TL;DR: 提出了固定初始状态、恒定输入的FISCIDS表示，可对广义非线性函数实现精确的参数化建模；证明所有差分代数函数存在二次FISCIDS表示，并且存在非差分代数但具备二次FISCIDS表示的解析函数，因此在实际问题中大多数函数可被二次FISCIDS表示。


<details>
  <summary>Details</summary>
Motivation: 为非线性函数提供一个精确且参数化的建模工具，使其在科学与工程中的应用更广泛、分析更可控。

Method: 构造一个输入仿射的系统，固定初始状态和固定终端时间，将函数的自变量作为恒定输入输入到系统中；输出在终点时间即为该函数值；证明差分代数函数均可用二次FISCIDS表示，并给出存在非DA解析函数也具备二次FISCIDS表示的实例。

Result: 理论层面：任意差分代数函数具备二次FISCIDS表示；存在一个解析但非差分代数的函数仍具备二次FISCIDS表示；从而说明在实际问题中，绝大多数函数可被二次FISCIDS表示。

Conclusion: 二次FISCIDS作为一种强大的参数化非线性函数表示工具，覆盖广泛函数并具有潜在的工程应用前景，未来工作可探索更多类型及优化实现。

Abstract: Parametric representations of various functions are fundamental tools in science and engineering. This paper introduces a fixed-initial-state constant-input dynamical system (FISCIDS) representation, which provides an exact and parametric model for a broad class of nonlinear functions. A FISCIDS representation of a given nonlinear function consists of an input-affine dynamical system with a fixed initial state and constant input. The argument of the function is applied as the constant input to the input-affine system, and the value of the function is the output of the input-affine system at a fixed terminal time. We show that any differentially algebraic function has a quadratic FISCIDS representation. We also show that there exists an analytic function that is not differentially algebraic but has a quadratic FISCIDS representation. Therefore, most functions in practical problems in science and engineering can be represented by a quadratic FISCIDS representation.

</details>


### [104] [An Information Theory of Finite Abstractions and their Fundamental Scalability Limits](https://arxiv.org/abs/2512.03977)
*Giannis Delimpaltadakis,Gabriel Gleizer*

Main category: eess.SY

TL;DR: 本论文提出一个针对有限抽象的统计性率失真理论，将抽象视为编码-解码对，给出在给定抽象规模的情况下最小失真的下界，并揭示动力学复杂度（广义熵）对可扩展性的影响；通过例子证明界限的紧性，并展示在混沌系统中的最佳抽象构造方法。


<details>
  <summary>Details</summary>
Motivation: 尽管有限抽象在将连续动力系统离散化方面具有重要作用，但在相同“精度”下，抽象的尺寸随维数指数级增长，存在“维数灾难”。尚无正式的精度-尺寸权衡结果，因此需要建立一套量化、统计性的理论来揭示抽象的可扩展性极限。

Method: 将抽象视为编码器-解码器对，编码系统轨迹到高维环境空间，定义码率为抽象尺寸、失真为系统轨迹与抽象轨迹的空间平均偏差。基于率失真理论推导出在给定动力学和抽象尺寸阈值下的最小失真的下界，该下界依赖于动力学复杂性的广义熵。对一些动力学系统验证界限的紧性，并通过对混沌系统的示例展示如何利用理论构造在尺寸-精度权衡下的最优抽象。

Result: 给出抽象的最小失真下界，且证明在某些系统中该下界达到紧性；提出用信息理论框架设计最优抽象的具体途径，并在混沌系统实例中展示可行性。

Conclusion: 建立了有限抽象的理论极限，揭示尺寸-精度权衡的本质依赖于动力学复杂度，并提供将该理论用于实际抽象设计的路径。

Abstract: Finite abstractions are discrete approximations of dynamical systems, such that the set of abstraction trajectories contains, in a formal sense, all system trajectories. There is a consensus that abstractions suffer from the curse of dimensionality: for the same ``accuracy" (how closely the abstraction represents the system), the abstraction size scales poorly with system dimensions. And, yet, after decades of research on abstractions, there are no formal results concerning their accuracy-size tradeoff. In this work, we derive a statistical, quantitative theory of abstractions' accuracy-size tradeoff and uncover fundamental limits on their scalability, through rate-distortion theory -- the branch of information theory studying lossy compression. Abstractions are viewed as encoder-decoder pairs, encoding trajectories of dynamical systems in a higher-dimensional ambient space. Rate represents abstraction size, while distortion describes abstraction accuracy, defined as the spatial average deviation between abstract trajectories and system ones. We obtain a fundamental lower bound on the minimum abstraction distortion, given the system dynamics and a threshold on abstraction size. The bound depends on the complexity of the dynamics, through generalized entropy. We demonstrate the bound's tightness on certain dynamical systems. Finally, we showcase how the developed theory can be employed to construct optimal abstractions, in terms of the size-accuracy tradeoff, through an example on a chaotic system.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [105] [A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses](https://arxiv.org/abs/2512.03458)
*Maryam Maghsoudi,Mohsen Rezaeizadeh,Shihab Shamma*

Main category: eess.SP

TL;DR: 通过MEG数据集揭示想象与聆听的音乐/诗歌刺激之间的跨感知映射可实现性，提出带有校准层的CNN在个体和群体层面实现更稳健的想象-感知映射，显示想象脑活动可转化为感知样本，具备未来BCI潜力。


<details>
  <summary>Details</summary>
Motivation: 破解想象语/音乐等信息的可解释性与可解码性。想象响应数据稀缺且时序不确定，单一极其难以泛化，需要可跨个体迁移的解码框架。

Method: 在 trained musicians 进行想象与聆听时的MEG记录上，使用滑动窗口岭回归实现单个被试层面的想象-聆听映射；在群体层面，提出一个带有被试特异校准层的编码-解码CNN，进行跨被试的稳健映射，并与空模型进行比较。

Result: 岭回归在个体水平取得部分映射，但跨被试的泛化有限；带校准层的CNN在群体层面产生稳定且可泛化的映射，显著高于空模型，对几乎所有保留被试的预测-真实聆听反应相关性更高。

Conclusion: 想象的神经活动可以转化为接近感知的响应，为未来基于想象语言和音乐的脑机接口奠定基础。

Abstract: Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.

</details>


### [106] [Resource Allocation for Pinching-Antenna Systems (PASS)-enabled NOMA Communications](https://arxiv.org/abs/2512.03502)
*Songtao Xue,Jingjing Zhao,Kaiquan Cai,Xidong Mu,Zhenyu Xiao,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出一种基于Pinching-antenna系统（PASS）的WD传输下多用户NOMA框架，通过对每个簇分配一个波导并利用pinching波束成形提升簇内性能、抑制簇间干扰，同时对功率分配、波束成形和用户调度进行联合优化。


<details>
  <summary>Details</summary>
Motivation: 动态可重构的无线传播环境和高效多用户接入需求驱动对PASS在WD传输结构中的NOMA应用研究，以提升系统容量并降低干扰。

Method: 在每个NOMA用户簇由一个专用波导承载的框架下，利用pinching波束成形提升簇内性能、抑制簇间干扰。将总和和问题分解为功率分配、pinching波束成形与用户调度三大子问题。提出两步算法：第一步采用惩罚对偶分解(PDD)求解联合功率分配和pinching波束成形，利用增广拉格朗日松弛并将AL问题分解为4个子问题，采用BCD迭代求解；第二步提出低复杂度匹配算法完成用户对波导的分配。

Result: 数值仿真表明：1) 基于PASS的WD传输结构的NOMA框架相比固定位置天线与OMA，具有显著的和带宽提升；2) 基于匹配的用户调度算法在接近最优的用户-波导分配上具有较低的计算复杂度。

Conclusion: 基于PASS的WD传输NOMA框架可显著提升系统容量，且通过PDD-BCD和低复杂度匹配算法实现联合优化，具备良好的理论意义和实际可执行性。

Abstract: Pinching-antenna systems (PASS) have emerged as a promising technology due to their ability to dynamically reconfigure wireless propagation environments. A novel PASS-based multi-user non-orthogonal multiple access (NOMA) framework is proposed by exploiting the waveguide-division (WD) transmission characteristic. Specifically, each NOMA user cluster is served by one dedicated waveguide, and the corresponding pinching beamforming is exploited to enhance the intra-cluster performance while mitigating the inter-cluster interference. Based on this framework, a sum-rate maximization problem is formulated for jointly optimizing power allocation, pinching beamforming, and user scheduling. To solve this problem, a two-step algorithm is developed, which decomposes the original problem into two subproblems. For the joint power allocation and pinching beamforming design, a penalty dual decomposition (PDD) algorithm is proposed to obtain the locally optimal solutions. Specifically, the coupling constraints are alleviated through augmented Lagrangian relaxation, and the resulting augmented Lagrangian (AL) problem is decomposed into four subproblems, which are solved by the block coordinate descent (BCD) method. For the user scheduling, a low-complexity matching algorithm is developed to solve the user-to-waveguide assignment problem. Simulation results demonstrate that 1) the proposed PASS-based NOMA framework under the WD transmission structure achieves significant sum-rate gain over conventional fixed-position antenna systems and orthogonal multiple access (OMA) scheme; and 2) the proposed matching-based user scheduling algorithm achieves near-optimal user-waveguide association with low computational complexity.

</details>


### [107] [Doppler Robust Vortex Wavefront Design for Integrated Sensing and Communication](https://arxiv.org/abs/2512.03802)
*Yuan Liu,Wen-Xuan Long,M. R. Bhavani Shankar,Marco Moretti,Rui Chen,Björn Ottersten*

Main category: eess.SP

TL;DR: 提出一种Doppler鲁棒的ISAC框架，先感知后通信，通过CDMM传输多涡旋模态并用VCM-EM解决多目标的Doppler干扰，再利用估计的CSI进行波束成形与波束定向，分析 pilots 长度对估计、对准与谱效率的影响，仿真显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 在动态场景中，多普勒效应会同时降低 sensing 与 communication 的性能，尤其是对波束敏感的OAM涡旋波前。需将感知与通信耦合，提升频谱与硬件的利用效率，且要抑制Doppler引起的模式间干扰。

Method: 感知阶段：通过CDMM同时发送多涡旋模式；提出VCM-EM算法，联合解码感知矩阵并估计多目标的距离、方位、俯仰、速度，解决Doppler诱发的模式间干扰。通信阶段：利用估计的CSI进行发送端波束成形和接收端波束定向的联合配置。还评估 pilots 长度对估计精度、波束对准与谱效率的影响。

Result: 仿真结果显示，与基线方案相比，VCM-EM 与ISAC设计在动态场景下具有更高的感知精度和通信谱效率。

Conclusion: 所提出的Doppler鲁棒ISAC框架有效抑制Doppler导致的模式间干扰，提升感知与通信的综合性能，并揭示pilot长度与估计/对准/谱效率之间的权衡。

Abstract: Integrated sensing and communication (ISAC) is a promising paradigm for future wireless systems due to spectrum reuse, hardware sharing, and joint waveform design. In dynamic scenes, Doppler shifts degrade both sensing and communication, which is particularly critical for beam-sensitive orbital angular momentum (OAM) wavefronts. To address this, we propose a Doppler-robust ISAC framework, which first senses and then communicates. Specifically, in the sensing phase, multiple vortex modes are simultaneously transmitted via code-division mode-multiplexing (CDMM). To solve Doppler-induced inter-mode interference, we propose a velocity-consistency matching (VCM)-expectation maximization (EM) algorithm that jointly decodes the sensing matrix and estimates range, azimuth, elevation, and velocity for multiple moving targets. In the communication phase, the joint transmitter (Tx) beamforming and receiver (Rx) beam steering are configured from the estimated channel state information (CSI). We further quantify the sensing-communication allocation trade-off by evaluating how pilot length affects estimation accuracy, beam alignment, and spectral efficiency (SE). Simulation results show that the proposed VCM-EM and ISAC designs achieve higher sensing accuracy and communication SE than baseline schemes in dynamic scenarios.

</details>
