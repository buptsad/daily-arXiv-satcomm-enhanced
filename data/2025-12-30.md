<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 17]
- [eess.SP](#eess.SP) [Total: 16]
- [cs.IT](#cs.IT) [Total: 11]
- [cs.LG](#cs.LG) [Total: 109]
- [eess.SY](#eess.SY) [Total: 14]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Novel Approach for a Smart IoMT-Based BAN for an Old Home Healthcare Monitoring System Using Starlink](https://arxiv.org/abs/2512.22553)
*Shermin Sultana Setu,Mst. Amena Akter Pinky,Md. Abdul Awal,Sheekar Banerjee,Ishtiak Al Mamoon*

Main category: cs.NI

TL;DR: 提出了一个基于星链卫星的物联网医疗（IoMT）养老护理系统，利用QoS策略（FQ-CoDel + DSCP）对关键健康数据进行优先级排队和传输，在NS-3仿真中评估，显示在偏远地区的吞吐量、时延和可靠性方面优于现有方案，具备可扩展与可重复性。


<details>
  <summary>Details</summary>
Motivation: 解决当前养老护理体系资源不足、医生间沟通协作不畅的问题，利用星链卫星实现对老年患者的远程监控与稳定通信，提高慢性疾病管理和紧急事件响应能力。

Method: 在本地通信枢纽收集ECG、体温、心梗指标与跌倒检测等生理数据，通过LEO卫星链路传输至医疗中心；以NS-3对系统在遥远/欠服务地区的性能进行评估，结合Flow Queuing（FQ）与CoDel的排队控制、并使用DSCP标记实现关键数据的QoS优先级；系统设计为全无线、具备实时告警与安全数据存储能力。

Result: 仿真结果表明，所提Starlink协同IoMT系统在吞吐量、时延与可靠性方面优于现有解决方案，显示Starlink在远程医疗通信中的强大潜力。

Conclusion: 该方案提供了一个可扩展且可重复的卫星辅助医疗框架，强调以QoS为核心的关键健康数据传输优先级，并为未来在养老护理领域的卫星通信应用奠定基础。

Abstract: The rapid evolution of the Internet of Medical Things (IoMT) technology has become a transformative force in modern healthcare, particularly in elderly patient management. The current elderly care system faces significant challenges, including insufficient long-term care resources and poor communication between healthcare providers. To address this limitation, this study introduces a novel Starlink-assisted IOMT-based elderly healthcare model designed to improve remote patient monitoring and communication reliability. This proposal system focused on a monitoring system of key biomedical parameters such as electrocardiogram (ECG), body temperature, heart attack indicators, and a fall detection alert system. Performance is evaluated using the network simulator (NS-3) to assess its effectiveness in remote and underserved regions. Physiological data collected from patients are transmitted through a local communication hub and forwarded over a Low Earth Orbit (LEO) satellite link to a medical center. Based on Quality of Service (QoS) technology that combines Flow Queuing (FQ) with Controlled Delay (CoDel) with Differentiated Services Code Point (DSCP) marking. This approach prioritizes critical health data for faster transmission while allocating lower priority to non-urgent information. This architecture is entirely wireless, allowing continuous monitoring, real-time alerts, and secure data storage for medical analysis. The simulation results demonstrate that the proposed Starlink-enabled IOMT system outperforms existing solutions in terms of throughput, latency, and reliability. The findings highlight Starlink's potential as a robust and high-performance telehealth communication tool. In addition, this study provides a scalable and reproducible framework for future satellite-assisted healthcare systems that prioritize quality of service (QoS) performance and improved elderly patient care.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [2] [A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography](https://arxiv.org/abs/2512.22301)
*Aayush Mainali,Sirjan Ghimire*

Main category: cs.CR

TL;DR: 本论文提出一个场景化的统计风险模型，用以评估时序侧信道在格基密钥交换中的可区分性，并在多种泄漏模型和执行场景下对 Kyber、Saber、Frodo 等家族进行可比分析。


<details>
  <summary>Details</summary>
Motivation: 时序侧信道是实现层的一大威胁，特别是在后量子密码学背景下，复杂算术与控制流可能导致秘密相关的时序波动。现实测量还受环境噪声（调度、争用、尾部延迟等）影响，因此需要一个可在设计早期用于跨场景比较的风险评估框架。

Method: 在空闲、抖动和负载等场景中合成两组秘密类别的时间序列，覆盖多种泄漏模型。使用 Welch 的 t 检验、KS 距离、Cliff 的 delta、互信息和分布重叠度等量化泄漏，并以 TLRI 类比的方式将结果整合成一个可用于场景排序的一致分数。

Result: 在代表性格基方案 Kyber、Saber、Frodo 中，空闲条件下的可区分性通常最好；抖动与负载条件通过方差增加和重叠度增大而降低可区分性。缓存索引泄漏与分支样式泄漏往往给出更高的风险信号；在相似泄漏假设下，更快的方案可能达到更高的峰值风险。这为在早期设计阶段、在不依赖具体平台验证前，进行可重复的跨方案比较提供了可行框架。

Conclusion: 本文提供了一种在早期设计阶段快速比较格基 KEM 家族在时序泄漏上的风险的方法论，通过场景化的统计量化与多模型整合，帮助选型与评估。

Abstract: Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation.

</details>


### [3] [Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection](https://arxiv.org/abs/2512.22306)
*Chinmay Pushkar,Sanchit Kabra,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CR

TL;DR: 提出面向多漏洞检测的基准，评估多语言大语言模型在高漏洞密度下的性能，发现计数偏差和选择偏差显著影响多标签漏洞检测。


<details>
  <summary>Details</summary>
Motivation: 现实软件系统中多漏洞并存且相互作用，现有基准多聚焦单漏洞或函数级别，无法真实反映代码安全任务中的多标签与长上下文挑战。因此需要一个覆盖多语言、多漏洞密度的基准来定量评估模型在真实场景中的鲁棒性。

Method: 构建40,000个文件的数据集，覆盖C、C++、Python、JavaScript四种语言；在长上下文代码样本（7.5k-10k tokens）中系统性注入漏洞数量（1、3、5、9），数据源来自CodeParrot；评估五种主流LLM（包括GPT-4o-mini、Llama-3.3-70B、Qwen-2.5系等）。

Result: 随着漏洞密度提升，模型性能显著下降；在单漏洞C任务上，Llama-3.3-70B近乎完美F1约0.97，但在高密度场景下降幅可达约40%；Python和JavaScript呈现与C/C++不同的失败模式，出现严重的“欠计数”现象，Recall降至<0.30。

Conclusion: 该基准揭示跨语言多漏洞检测的挑战，强调需要更好地理解和纠正多标签任务中的计数偏差/选择偏差，以及针对不同语言的特定策略与数据设计，以提升在真实代码中的鲁棒性。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in automated software security, particularly in vulnerability detection. However, existing benchmarks primarily focus on isolated, single-vulnerability samples or function-level classification, failing to reflect the complexity of real-world software where multiple interacting vulnerabilities often coexist within large files. Recent studies indicate that LLMs suffer from "count bias" and "selection bias" in multi-label tasks, yet this has not been rigorously quantified in the domain of code security. In this work, we introduce a comprehensive benchmark for Multi-Vulnerability Detection across four major languages: C, C++, Python, and JavaScript. We construct a dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, and 9) into long-context code samples (7.5k-10k tokens) sourced from CodeParrot. We evaluate five state-of-the-art LLMs, including GPT-4o-mini, Llama-3.3-70B, and the Qwen-2.5 series. Our results reveal a sharp degradation in performance as vulnerability density increases. While Llama-3.3-70B achieves near-perfect F1 scores (approximately 0.97) on single-vulnerability C tasks, performance drops by up to 40% in high-density settings. Notably, Python and JavaScript show distinct failure modes compared to C/C++, with models exhibiting severe "under-counting" (Recall dropping to less than 0.30) in complex Python files.

</details>


### [4] [LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators](https://arxiv.org/abs/2512.22307)
*You Li,Guannan Zhao,Yuhao Ju,Yunqi He,Jie Gu,Hai Zhou*

Main category: cs.CR

TL;DR: 提出了LLA，一种面向生成式AI模型的硬件-软件协同IP保护方案，通过在神经元中嵌入密钥位并施加不变变换，以及在AI加速器中嵌入轻量锁模块，来防护模型盗用、篡改与信息泄露，且在7168位密钥下开销小于0.1%。


<details>
  <summary>Details</summary>
Motivation: 应对AI模型的供应链威胁（如模型窃取、模型损坏、信息泄露等）日益增多的挑战，需要软硬件协同的IP保护机制以提升对抗能力和可用性。

Method: 软件层：将密钥位嵌入神经元以触发异常输出，使用不变性变换隐藏密钥；硬件层：在AI加速器中集成轻量锁定模块，拥有预存的秘密密钥，作为访问模型服务的许可证；兼容多种数据流模式和工具链。

Result: 评估表明LLA能抵御大范围的基于oracle的密钥优化攻击，并且开销极低，在7168位密钥下的计算开销小于0.1%。

Conclusion: LLA通过软硬件协同提供对生成式AI模型供应链攻击的有效防护，具有较低的性能成本和较强的可部署性，适合作为IP保护方案的实现方向。

Abstract: We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.

</details>


### [5] [NOWA: Null-space Optical Watermark for Invisible Capture Fingerprinting and Tamper Localization](https://arxiv.org/abs/2512.22501)
*Edwin Vargas*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ensuring the authenticity and ownership of digital images is increasingly challenging as modern editing tools enable highly realistic forgeries. Existing image protection systems mainly rely on digital watermarking, which is susceptible to sophisticated digital attacks. To address this limitation, we propose a hybrid optical-digital framework that incorporates physical authentication cues during image formation and preserves them through a learned reconstruction process. At the optical level, a phase mask in the camera aperture produces a Null-space Optical Watermark (NOWA) that lies in the Null Space of the imaging operator and therefore remains invisible in the captured image. Then, a Null-Space Network (NSN) performs measurement-consistent reconstruction that delivers high-quality protected images while preserving the NOWA signature. The proposed design enables tamper localization by projecting the image onto the camera's null space and detecting pixel-level inconsistencies. Our design preserves perceptual quality, resists common degradations such as compression, and establishes a structural security asymmetry: without access to the optical or NSN parameters, adversaries cannot forge the NOWA signature. Experiments with simulations and a prototype camera demonstrate competitive performance in terms of image quality preservation, and tamper localization accuracy compared to state-of-the-art digital watermarking and learning-based authentication methods.

</details>


### [6] [Verifiable Dropout: Turning Randomness into a Verifiable Claim](https://arxiv.org/abs/2512.22526)
*Kichang Lee,Sungmin Lee,Jaeho Jin,JeongGil Ko*

Main category: cs.CR

TL;DR: 提出了一种可验证的Dropout机制，使用零知识证明对随机性进行证明，以在保护训练数据隐私的前提下实现对 dropout 执行正确性的事后审计。


<details>
  <summary>Details</summary>
Motivation: 现代云端 AI 训练中，越来越多的日志和遥测用于可追溯性，但由于深度学习的随机性（如 dropout），现有审计面临“非确定性带来的模糊性”与可能的恶意偏差问题，难以在不暴露训练数据的前提下验证随机性是否被公正地使用。

Method: 将 dropout 掩码绑定到一个确定性且可密码学验证的种子，并通过零知识证明证明 dropout 操作的正确执行。将随机性视为可验证的主张而非借口，从而实现对随机步骤的后验审计，且保护模型和数据的机密性。

Result: 实现对 dropout 的完整性审计，证明随机性未被偏置、未被“挑选性”使用；在不泄露模型及训练数据的前提下，提供可验证的正确性证明，提升训练过程的透明度与信任度。

Conclusion: 本工作提出的 Verifiable Dropout 为深度学习中的随机性提供了可验证的完整性保障，有望提升云端训练的可审计性和合规性，并为将来将更多随机操作纳入可证明的审计框架奠定基础。

Abstract: Modern cloud-based AI training relies on extensive telemetry and logs to ensure accountability. While these audit trails enable retrospective inspection, they struggle to address the inherent non-determinism of deep learning. Stochastic operations, such as dropout, create an ambiguity surface where attackers can mask malicious manipulations as natural random variance, granting them plausible deniability. Consequently, existing logging mechanisms cannot verify whether stochastic values were generated and applied honestly without exposing sensitive training data. To close this integrity gap, we introduce Verifiable Dropout, a privacy-preserving mechanism based on zero-knowledge proofs. We treat stochasticity not as an excuse but as a verifiable claim. Our approach binds dropout masks to a deterministic, cryptographically verifiable seed and proves the correct execution of the dropout operation. This design enables users to audit the integrity of stochastic training steps post-hoc, ensuring that randomness was neither biased nor cherry-picked, while strictly preserving the confidentiality of the model and data.

</details>


### [7] [Raven: Mining Defensive Patterns in Ethereum via Semantic Transaction Revert Invariants Categories](https://arxiv.org/abs/2512.22616)
*Mojtaba Eshghie,Melissa Mazura,Alexandre Bartel*

Main category: cs.CR

TL;DR: Raven框架将以往因断言/断言逆转的以太坊交易视为主动的链上防御信号，通过将“被逆转的事务”与源代码中的不变量对齐、用BERT微调模型嵌入不变量、并以语义意图聚类，从而挖掘防御性不变量类别并验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对交易被拦截/回滚背后的防御性模式关注不足，尽管这类不变量能揭示链上防御策略，亟待数据驱动的方法来挖掘与利用它们。

Method: 将被逆转的交易与合约实现中的不变量对应；使用基于BERT的微调模型对不变量进行嵌入；按语义意图进行聚类以发现防御性不变量类别；对20,000条被逆转交易进行评估；专家人工审阅得到19个语义聚类与6个新类别；以新类别之一作为模糊测试的oracle进行真实世界攻击的漏洞检测案例研究。

Result: 在样本中实现了连贯且具意义的聚类，19个语义聚类经专家审阅发现6个在现有不变量目录中未出现的新类别（包括特征开关、重放防护、证明/签名校验、计数器、调用方提供的滑点阈值、白/黑名单等）；通过案例研究展示新不变量种类在漏洞检测中的实用性，证明Raven能映射以太坊的成功防御。

Conclusion: Raven提供了一种数据驱动的防御挖掘框架，能够从智能合约的实际防御中提取不变量并构建分析工具和安全Oracle，帮助研究者更系统地理解和利用区块链上的防御模式。

Abstract: We frame Ethereum transactions reverted by invariants-require(<invariant>)/ assert(<invariant>)/if (<invariant>) revert statements in the contract implementation-as a positive signal of active on-chain defenses. Despite their value, the defensive patterns in these transactions remain undiscovered and underutilized in security research. We present Raven, a framework that aligns reverted transactions to the invariant causing the reversion in the smart contract source code, embeds these invariants using our BERT-based fine-tuned model, and clusters them by semantic intent to mine defensive invariant categories on Ethereum. Evaluated on a sample of 20,000 reverted transactions, Raven achieves cohesive and meaningful clusters of transaction-reverting invariants. Manual expert review of the mined 19 semantic clusters uncovers six new invariant categories absent from existing invariant catalogs, including feature toggles, replay prevention, proof/signature verification, counters, caller-provided slippage thresholds, and allow/ban/bot lists. To demonstrate the practical utility of this invariant catalog mining pipeline, we conduct a case study using one of the newly discovered invariant categories as a fuzzing oracle to detect vulnerabilities in a real-world attack. Raven thus can map Ethereum's successful defenses. These invariant categories enable security researchers to develop analysis tools based on data-driven security oracles extracted from the smart contracts' working defenses.

</details>


### [8] [When RSA Fails: Exploiting Prime Selection Vulnerabilities in Public Key Cryptography](https://arxiv.org/abs/2512.22720)
*Murtaza Nikzad,Kerem Atas*

Main category: cs.CR

TL;DR: RSA 密钥在素数选择上的不足导致两种公开攻击：Fermat 因子分解和 GCD 攻击。基于 Heninger 等人的研究与 Böck 的分析，现实世界中仍大量存在漏洞，原因在于嵌入式设备的熵源不足。对策包括改进熵收集与素数验证。


<details>
  <summary>Details</summary>
Motivation: 确保 RSA 密钥生成的素数选择不易被近似或共享；评估现实世界的攻击面及其影响；强调随机性不足在嵌入式实现中的作用。

Method: 基于文献综述与对两种攻击向量的分析；结合 Heninger 等人的“Mining Your Ps and Qs”和 Böck 的 Fermat 因子分解研究的发现，比较现实部署的风险。

Result: 证实漏洞在现实系统中依然普遍，弱随机数生成是嵌入式设备失败的主要原因；披露 >6.4万個易受攻击的 TLS 主机的统计证据，以及对现有实现的影响。

Conclusion: 提出缓解策略：改进熵收集、加强素数验证、改进随机数生成器实现、谨慎的密钥轮换和监控。

Abstract: This paper explores vulnerabilities in RSA cryptosystems that arise from improper prime number selection during key generation. We examine two primary attack vectors: Fermat's factorization method, which exploits RSA keys generated with primes that are too close together, and the Greatest Common Divisor (GCD) attack, which exploits keys that share a common prime factor. Drawing from landmark research including Heninger et al.'s ``Mining Your Ps and Qs'' study, which discovered over 64,000 vulnerable TLS hosts, and B{ö}ck's 2023 analysis of Fermat factorization in deployed systems, we demonstrate that these vulnerabilities remain prevalent in real-world cryptographic implementations. Our analysis reveals that weak random number generation in embedded devices is the primary cause of these failures, and we discuss mitigation strategies including proper entropy collection and prime validation checks.

</details>


### [9] [A Privacy Protocol Using Ephemeral Intermediaries and a Rank-Deficient Matrix Power Function (RDMPF)](https://arxiv.org/abs/2512.23535)
*Eduardo Salazar*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a private transfer architecture for the Internet Computer (ICP) that decouples deposit and retrieval through two short-lived intermediaries, with sealed storage and attested teardown by an ephemeral witness. The protocol uses a non-interactive RDMPF-based encapsulation to derive per-transfer transport keys. A public notice hint is computed from the capsule to enable discovery without fingerprinting the recipient's key. Retrieval is authorized by a short proof of decapsulation that reveals no identities. All transaction intermediaries are ephemeral and issue certified destruction intents and proofs, allowing a noticeboard to publish auditable finalization records. The design provides sender identity privacy with respect to the recipient, content confidentiality against intermediaries, forward secrecy for transport keys after staged destruction, verifiable liveness and finality. We formalize the basic interfaces, provide the security arguments for encapsulation correctness, hint privacy, authorization soundness and timeout reclaim.
  In terms of implementation, it has been recently brought into production on the ICP under the name ICPP. It has been subject to exhaustive testing and incorporates a few enhancements, focusing on the operational possibilities offered by ICP's technology. This work hence serves as a broad reference for the protocol now publicly accessible.

</details>


### [10] [Breaking the illusion: Automated Reasoning of GDPR Consent Violations](https://arxiv.org/abs/2512.22789)
*Ying Li,Wenjun Qiu,Faysal Hossain Shezan,Kunlin Cai,Michelangelo van Dam,Lisa Austin,David Lie,Yuan Tian*

Main category: cs.CR

TL;DR: 提出一个名为Cosmic的自动化框架，用于在网页表单中检测与隐私同意相关的违规行为，并在大规模网站上进行审计，显示出高准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: GDPR和CCPA等隐私法规要求在收集、使用和分享个人数据时获得知情、自由、具体、明确的同意。然而，现有研究聚焦于cookie横幅和移动应用对话框，且同意表单结构多样、法源依据不同、难以定位，导致自动化合规审计困难。因此需要一种能够跨网页表单自动检测隐私合规性的工具。

Method: 提出Cosmic框架，自动检测网页表单中的同意相关隐私违规。通过对5,823个网站、3,598个表单的审计评估，识别出3,384处违规，覆盖94.1%的同意表单。框架的检测能力以真阳性率（TPR）衡量，获得同意检测的TPR为98.6%，违规检测的TPR为99.1%，显示高准确度。

Result: Cosmic在大规模真实网站数据上实现了高检测率和高准确度，揭示了普遍存在的同意 disclosure、自由给予、撤回选项等方面的合规缺口，表明自动化审计在提升数据保护合规性方面具有现实应用潜力。

Conclusion: 该工作展示了跨网站的自动化合规审计能力，填补了对多样化同意表单的系统性评估空白，并为改进网页表单的隐私合规提供了有效工具与证据，提升用户自主权与信任。

Abstract: Recent privacy regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have established legal requirements for obtaining user consent regarding the collection, use, and sharing of personal data. These regulations emphasize that consent must be informed, freely given, specific, and unambiguous. However, there are still many violations, which highlight a gap between legal expectations and actual implementation. Consent mechanisms embedded in functional web forms across websites play a critical role in ensuring compliance with data protection regulations such as the GDPR and CCPA, as well as in upholding user autonomy and trust. However, current research has primarily focused on cookie banners and mobile app dialogs. These forms are diverse in structure, vary in legal basis, and are often difficult to locate or evaluate, creating a significant challenge for automated consent compliance auditing. In this work, we present Cosmic, a novel automated framework for detecting consent-related privacy violations in web forms. We evaluate our developed tool for auditing consent compliance in web forms, across 5,823 websites and 3,598 forms. Cosmic detects 3,384 violations on 94.1% of consent forms, covering key GDPR principles such as freely given consent, purpose disclosure, and withdrawal options. It achieves 98.6% and 99.1% TPR for consent and violation detection, respectively, demonstrating high accuracy and real-world applicability.

</details>


### [11] [Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations](https://arxiv.org/abs/2512.22883)
*Tao Li,Quanyan Zhu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.

</details>


### [12] [SecureBank: A Financially-Aware Zero Trust Architecture for High-Assurance Banking Systems](https://arxiv.org/abs/2512.23124)
*Paulo Fernandes Biao*

Main category: cs.CR

TL;DR: SecureBank is a finance-aware Zero Trust framework for high-assurance banking that combines Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation, evaluated via Monte Carlo against a rule-based baseline using TII, ITAL, and SAE metrics, showing improved automated attack handling and faster identity trust adaptation while preserving transactional integrity; intended as a reference architecture for regulated financial environments.


<details>
  <summary>Details</summary>
Motivation: Traditional perimeter-based security struggles in modern finance due to open banking APIs, cloud-native infrastructures, and high-frequency transactions, which expand the attack surface. There's a need to explicitly incorporate transactional semantics, financial risk modeling, adaptive identity trust, and economics-aware automation.

Method: Proposes SecureBank architecture integrating four components: Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation. Uses Monte Carlo simulation to compare against a rule-based baseline architecture, evaluating with metrics such as Transactional Integrity Index (TII), Identity Trust Adaptation Level (ITAL), and Security Automation Efficiency (SAE).

Result: Monte Carlo simulations indicate SecureBank significantly improves automated attack handling and accelerates identity trust adaptation, while maintaining conservative, regulator-aligned levels of transactional integrity compared to the baseline.

Conclusion: SecureBank can serve as a reference architecture and evaluation baseline for financially aware Zero Trust systems in regulated financial environments.

Abstract: Financial institutions increasingly rely on distributed architectures, open banking APIs, cloud native infrastructures, and high frequency digital transactions. These transformations expand the attack surface and expose limitations in traditional perimeter based security models. While Zero Trust architectures provide essential security principles, most existing frameworks do not explicitly incorporate transactional semantics, financial risk modeling, adaptive identity trust, or automation weighted by economic impact.
  This paper introduces SecureBank, a financially aware and context adaptive Zero Trust architecture designed specifically for high assurance banking systems. The proposed framework integrates Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation. A Monte Carlo simulation evaluates SecureBank against a representative rule based baseline architecture using metrics such as the Transactional Integrity Index (TII), Identity Trust Adaptation Level (ITAL), and Security Automation Efficiency (SAE).
  The results demonstrate that SecureBank significantly improves automated attack handling and accelerates identity trust adaptation while preserving conservative and regulator aligned levels of transactional integrity. Beyond experimental validation, SecureBank is intended to serve as a reference architecture and evaluation baseline for financially aware Zero Trust systems in regulated financial environments.

</details>


### [13] [Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning](https://arxiv.org/abs/2512.23171)
*Yu Jiang,Xindi Tong,Ziyao Liu,Xiaoxi Zhang,Kwok-Yan Lam,Chee Wei Tan*

Main category: cs.CR

TL;DR: FedORA 在垂直联邦学习中实现样本与标签的去学习，通过原/对偶优化框架解决跨方数据删除问题，提出以分类不确定性为导向的去学习损失、自适应步长与非对称批次设计以降低开销，理论上有界于从头训练的模型差异，实验表明在表格与图像数据集上可实现与从头训练相近的效果，同时降低计算与通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决垂直联邦学习中去学习的核心挑战：不同参与方持有样本的互补特征，需要跨方协作来删除特定样本或标签；目标是在保护隐私的同时降低计算与通信成本并提供可验证的去学习效果。

Method: 将样本/标签的删除转化为受约束的优化问题，采用 primal-dual 框架求解；引入新的去学习损失以促进分类不确定性而非错误分类；使用自适应步长以提升稳定性；设计非对称批次以考虑剩余数据对模型的影响并降低开销；实现跨方协调以处理特征间的依赖关系。

Result: 理论上证明 FedORA 与从头训练模型之间的差异有界；在表格和图像数据集上的实验表明，FedORA 在去学习效果与保持模型性能方面与从头训练相当，同时显著降低计算和通信成本。

Conclusion: FedORA 为垂直联邦学习中的数据去学习提供了一种可行且高效的解决方案，能够在保护隐私的同时保持模型性能，并具备对样本与标签去学习的适用性与稳定性。

Abstract: Federated unlearning has become an attractive approach to address privacy concerns in collaborative machine learning, for situations when sensitive data is remembered by AI models during the machine learning process. It enables the removal of specific data influences from trained models, aligning with the growing emphasis on the "right to be forgotten." While extensively studied in horizontal federated learning, unlearning in vertical federated learning (VFL) remains challenging due to the distributed feature architecture. VFL unlearning includes sample unlearning that removes specific data points' influence and label unlearning that removes entire classes. Since different parties hold complementary features of the same samples, unlearning tasks require cross-party coordination, creating computational overhead and complexities from feature interdependencies. To address such challenges, we propose FedORA (Federated Optimization for data Removal via primal-dual Algorithm), designed for sample and label unlearning in VFL. FedORA formulates the removal of certain samples or labels as a constrained optimization problem solved using a primal-dual framework. Our approach introduces a new unlearning loss function that promotes classification uncertainty rather than misclassification. An adaptive step size enhances stability, while an asymmetric batch design, considering the prior influence of the remaining data on the model, handles unlearning and retained data differently to efficiently reduce computational costs. We provide theoretical analysis proving that the model difference between FedORA and Train-from-scratch is bounded, establishing guarantees for unlearning effectiveness. Experiments on tabular and image datasets demonstrate that FedORA achieves unlearning effectiveness and utility preservation comparable to Train-from-scratch with reduced computation and communication overhead.

</details>


### [14] [EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion](https://arxiv.org/abs/2512.23173)
*Zhen Liang,Hai Huang,Zhengkui Chen*

Main category: cs.CR

TL;DR: 提出 Equacode 的多策略 jailbreak 框架，通过将恶意意图转化为数学问题再以代码完成解答，以跨领域任务增加对安全约束的干扰，从而提升对 LLM 的鲁棒性评估效果；在多家模型上达到高成功率并显著优于单一模块。


<details>
  <summary>Details</summary>
Motivation: 当前对 LLM 的 jailbreak 攻击多聚焦单一策略，难以全面评估模型鲁棒性；需要跨域、组合性方法来更有效地揭示模型在安全方面的弱点。

Method: 将恶意请求转化为求解数学方程的任务并引导模型用代码完成解答，结合方程模块与代码模块的多策略以增强攻击效果；对比消融实验显示组合比任一模块更强。

Result: 对 GPT 系列平均成功率 91.19%，在三种最先进模型中达到 98.65%，且仅需单次查询；消融实验表明 EquaCode 效果优于单独的数学方程模块或代码模块，呈现明显的协同效应。

Conclusion: 多策略组合方法能够显著提升对大语言模型的攻击有效性，成为评估模型安全性与鲁棒性的有效手段；未来工作可进一步扩展跨域策略与安全对策。

Abstract: Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.

</details>


### [15] [Multiparty Authorization for Secure Data Storage in Cloud Environments using Improved Attribute-Based Encryption](https://arxiv.org/abs/2512.23216)
*Partha Paul,Keshav Sinha*

Main category: cs.CR

TL;DR: 提出一种改进的基于属性的加密ABE+FBSE，用二变量曲线的线性功能来存储云数据, 通过Shamir秘密分享和2D-Lagrange求解实现多方授权和阈值解密；在属性策略增多时加密时间增加，但存储开销保持低水平，且对碰撞攻击具有抵抗力，结果显示方案鲁棒安全。


<details>
  <summary>Details</summary>
Motivation: 解决云环境中海量数据的存取需求带来的计算开销与数据泄露风险，提供可访问性与数据安全的分布式多方授权方案。

Method: 提出使用功能基流密码FBSE结合简化的标量点在抛物线曲线上的表示，生成授权点并仅分享给授权接收者；通过Shamir秘密分享与2D-Lagrange插值从抛物线重构秘密点；设定阈值Ts>3实现属性相关密钥的解密；对数据进行基于属性的加密，评估加密/解密时间、存储开销、以及统计分析（NIST测试、相关性、直方图）以评估像素偏差。

Result: 加密/解密性能随属性数量增加而增加，存储开销低且与用户身份无关；安全分析表明对碰撞攻击有抵抗力；实验结果表明该方案在鲁棒性与安全性方面优于基线方法。

Conclusion: 该方案为云存储场景提供一种鲁棒、可扩展的多方授权ABE方案，具备较低的存储开销和较强的安全性，适用于需要属性策略灵活性与数据安全的应用；未来可进一步优化性能及实现细节。

Abstract: In todays scenario, various organizations store their sensitive data in the cloud environment. Multiple problems are present while retrieving and storing vast amounts of data, such as the frequency of data requests (increasing the computational overhead of the server) and data leakage while storing. To cope with said problem, Attribute-Based Encryption (ABE) is one of the potential security and access control techniques for secure data storage and authorization. The proposed work divides into two objectives: (i) provide access to authorized users and (ii) secure data storage in a cloud environment. The improved ABE using Functional Based Stream Cipher (FBSE) is proposed for data storage. The proposed technique uses simple scalar points over a parabolic curve to provide multiparty authorization. The authorization points are generated and share only with the authorized recipients. The Shamir secret sharing technique generate the authorization points and 2D-Lagrange Interpolation is used to reconstruct the secret points from regular parabola. The proposed scheme has specified the threshold (Ts>3) legally authorized users to reconstruct the attribute-associated keys for decryption. The encryption of data is evaluated using Statistical analysis (NIST Statistical Test Suite, Correlation Coefficient, and Histogram) test to investigate image pixel deviation. The parameters like encryption and decryption are used for performance analysis, where an increase in the number of attributes for the authorization policy will increase the encryption time. The proposed scheme imposes minimal storage overhead, irrespective of the users identity. The security analysis evidence that it resists collision attacks. The security and performance analysis results demonstrate that the proposed scheme is more robust and secure.

</details>


### [16] [RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking](https://arxiv.org/abs/2512.23307)
*Jiawei Liu,Zhuo Chen,Rui Zhu,Miaokun Chen,Yuyang Gong,Wei Lu,Xiaofeng Wang*

Main category: cs.CR

TL;DR: 提出RobustMask，一种将上下文预测能力与随机掩蔽平滑结合的对抗鲁棒性 defense，提升神经排序模型在字符、单词和短语层面的鲁棒性，并在前10名中对抗最多30%的内容扰动时对超过20%的候选文档提供认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经排序在 Retrieval-Augmented Generation 等应用中表现出色，但易受对抗性扰动影响，现有防御要么泛化性差、要么需要对手拥有过强知识，难以落地。

Method: 结合预训练语言模型的上下文预测能力与随机掩蔽平滑机制，对候选文档及排序进行鲁棒化处理。利用排名模型的两两比较能力和概率统计分析，给出认证的 top-K 鲁棒性证明，并覆盖字符、单词、短语层面的扰动。

Result: 在实验中，RobustMask 能认证 top-10 中超过 20% 的候选文档，在扰动可达文档内容的 30% 情况下保持鲁棒性；对比基线具有显著提升。

Conclusion: RobustMask 为神经排序模型提供理论与实践层面的鲁棒性保证，推动现实检索系统的安全性与可信度提升。

Abstract: Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.

</details>


### [17] [Fuzzilicon: A Post-Silicon Microcode-Guided x86 CPU Fuzzer](https://arxiv.org/abs/2512.23438)
*Johannes Lenzen,Mohamadreza Rostami,Lichao Wu,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: 提出Fuzzilicon：面向真实x86处理器的后硅模糊测试框架，结合微代码层反馈实现自动化漏洞发现。


<details>
  <summary>Details</summary>
Motivation: 现代CPU通常是黑箱、专有，存在难以通过传统分析发现的微架构缺陷；需要一个自动化、可扩展的后硅漏洞发现框架。

Method: 通过微代码层注入 instrumentation、逆向Intel微码更新接口、结合基于Hypervisor的fuzzing harness，在不访问RTL的情况下实现反馈驱动输入生成。

Result: 在Intel Goldmont上发现5个重大发现，其中2个为新颖的微码级 speculative-execution 漏洞；自动重现 μSpectre 漏洞；覆盖开销比基线降低达31×，实现16.27%的唯一微代码覆盖率（hookable位置的基线），建立实证基线。

Conclusion: Fuzzilicon为后硅 fuzzing 提供一个可扩展、实际可用的框架，为自动化发现复杂CPU漏洞奠定新基础。

Abstract: Modern CPUs are black boxes, proprietary, and increasingly characterized by sophisticated microarchitectural flaws that evade traditional analysis. While some of these critical vulnerabilities have been uncovered through cumbersome manual effort, building an automated and systematic vulnerability detection framework for real-world post-silicon processors remains a challenge.
  In this paper, we present Fuzzilicon, the first post-silicon fuzzing framework for real-world x86 CPUs that brings deep introspection into the microcode and microarchitectural layers. Fuzzilicon automates the discovery of vulnerabilities that were previously only detectable through extensive manual reverse engineering, and bridges the visibility gap by introducing microcode-level instrumentation. At the core of Fuzzilicon is a novel technique for extracting feedback directly from the processor's microarchitecture, enabled by reverse-engineering Intel's proprietary microcode update interface. We develop a minimally intrusive instrumentation method and integrate it with a hypervisor-based fuzzing harness to enable precise, feedback-guided input generation, without access to Register Transfer Level (RTL).
  Applied to Intel's Goldmont microarchitecture, Fuzzilicon introduces 5 significant findings, including two previously unknown microcode-level speculative-execution vulnerabilities. Besides, the Fuzzilicon framework automatically rediscover the $μ$Spectre class of vulnerabilities, which were detected manually in the previous work. Fuzzilicon reduces coverage collection overhead by up to 31$\times$ compared to baseline techniques and achieves 16.27% unique microcode coverage of hookable locations, the first empirical baseline of its kind. As a practical, coverage-guided, and scalable approach to post-silicon fuzzing, Fuzzilicon establishes a new foundation to automate the discovery of complex CPU vulnerabilities.

</details>


### [18] [Enhanced Web Payload Classification Using WAMM: An AI-Based Framework for Dataset Refinement and Model Evaluation](https://arxiv.org/abs/2512.23610)
*Heba Osama,Omar Elebiary,Youssef Qassim,Mohamed Amgad,Ahmed Maghawry,Ahmed Saafan,Haitham Ghalwash*

Main category: cs.CR

TL;DR: WAMM 通过多阶段数据增强、LLM 引导的重新标注和过滤，提升基于机器学习的网页攻击检测，显著优于基于规则的防火墙。


<details>
  <summary>Details</summary>
Motivation: 应对传统静态规则（如 OWASP CRS）在对抗可混淆、零日攻击方面的局限性，展示通过高质量训练数据和高效模型实现实时、鲁棒的网页攻击检测的必要性。

Method: 在 SR-BH 2020 数据集上应用多阶段增强流程（去重、LLM 指导的重新标注、真实攻击数据增强、LLM 过滤），形成三组 refined 数据集；构建统一特征空间（统计与文本表示混合）；评估四种模型（含 XGBoost 等深度/机器学习模型），并在未见数据上对比 OWASP CRS 的检测。

Result: 在同一技术栈的增强数据集上，XGBoost 达到 99.59% 的准确率并实现微秒级推理；深度学习模型在噪声增强下性能下降；对 CRS 的未见增强数据测试中，WAMM 的真正阳性拦截率为 96–100%，改进幅度最高可达 86%。

Conclusion: 经由精心策划的训练数据管线与高效的机器学习模型，可以实现更具弹性且适合生产环境的实时网页攻击检测，揭示基于规则的防御在现代对抗中的不足。

Abstract: Web applications increasingly face evasive and polymorphic attack payloads, yet traditional web application firewalls (WAFs) based on static rule sets such as the OWASP Core Rule Set (CRS) often miss obfuscated or zero-day patterns without extensive manual tuning. This work introduces WAMM, an AI-driven multiclass web attack detection framework designed to reveal the limitations of rule-based systems by reclassifying HTTP requests into OWASP-aligned categories for a specific technology stack. WAMM applies a multi-phase enhancement pipeline to the SR-BH 2020 dataset that includes large-scale deduplication, LLM-guided relabeling, realistic attack data augmentation, and LLM-based filtering, producing three refined datasets. Four machine and deep learning models are evaluated using a unified feature space built from statistical and text-based representations. Results show that using an augmented and LLM-filtered dataset on the same technology stack, XGBoost reaches 99.59% accuracy with microsecond-level inference while deep learning models degrade under noisy augmentation. When tested against OWASP CRS using an unseen augmented dataset, WAMM achieves true positive block rates between 96 and 100% with improvements of up to 86%. These findings expose gaps in widely deployed rule-based defenses and demonstrate that curated training pipelines combined with efficient machine learning models enable a more resilient, real-time approach to web attack detection suitable for production WAF environments.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [19] [UniFi: Combining Irregularly Sampled CSI from Diverse Communication Packets and Frequency Bands for Wi-Fi Sensing](https://arxiv.org/abs/2512.22143)
*Gaofeng Dong,Kang Yang,Mani Srivastava*

Main category: eess.SP

TL;DR: 提出 UniFi：一种基于Wi-Fi的 ISAC 框架，利用来自多频段、异步采样的 CSI，完全避免注入探测包，具备高精度与小模型规模，同时保持通信吞吐。


<details>
  <summary>Details</summary>
Motivation: 现有 Wi-Fi 感知系统依赖高速探测包注入来提取 CSI，导致通信性能下降和部署困难；ISAC 方向虽具潜力，但多为利用数据帧 CSI，仍需注入辅助包。需一个非侵入式、兼容现有通信流的感知框架。

Method: 提出 CSI 去噪管道（sanitization）以统一异构数据包、消除突发冗余；引入时序自注意力模型，直接从非均匀 CSI 序列学习，不再重新取样；跨频带利用来自多频段数据的 CSI；构建 CommCSI-HAR 数据集，包含真实世界双频带的非均匀采样 CSI；在 CommCSI-HAR 与四个公开基准数据集上评估。

Result: 在多数据集上达到先进水平的感知准确性，同时模型规模紧凑；完全不影响原始通信吞吐，与现有注入型方法相比具明显优势。

Conclusion: 展示了无需侵入式探测包即可实现高性能 RAMS 的 Wi-Fi ISAC 框架，且提供了支持异步、跨频段 CSI 的数据集，为实际部署提供可行路径。

Abstract: Existing Wi-Fi sensing systems rely on injecting high-rate probing packets to extract channel state information (CSI), leading to communication degradation and poor deployability. Although Integrated Sensing and Communication (ISAC) is a promising direction, existing solutions still rely on auxiliary packet injection because they exploit only CSI from data frames. We present UniFi, the first Wi-Fi-based ISAC framework that fully eliminates intrusive packet injection by directly exploiting irregularly sampled CSI from diverse communication packets across multiple frequency bands. UniFi integrates a CSI sanitization pipeline to harmonize heterogeneous packets and remove burst-induced redundancy, together with a time-aware attention model that learns directly from non-uniform CSI sequences without resampling. We further introduce CommCSI-HAR, the first dataset with irregularly sampled CSI from real-world dual-band communication traffic. Extensive evaluations on this dataset and four public benchmarks show that UniFi achieves state-of-the-art accuracy with a compact model size, while fully preserving communication throughput.

</details>


### [20] [EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG](https://arxiv.org/abs/2512.22146)
*Hanbeot Park,Yunjeong Cho,Hunhee Kim*

Main category: eess.SP

TL;DR: 提出一种基于EEG的开环语音重建框架，可直接从非侵入性EEG生成mel频谱，进而合成语音并解码文本；对 spoken 与 imagined 两种语音均有效，且通过迁移学习与语言模型纠错提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决EEG信号的空间分辨率限制、噪声干扰以及缺乏对 imagined speech 的时序对齐等挑战，直接从EEG实现语音重建。

Method: 建立一个subject-specific generator，先从EEG生成mel-spectrogram；再通过预训练的声码器(vocoder)合成语音，以及ASR模块解码文本；分别为 spoken 与 imagined 训练独立生成器，使用以 spoken 为基础的迁移学习适应 imagined；可选的基于语言模型的修正模块以纠错而不破坏语义。评估在2秒与4秒条件下，使用PCC、RMSE、MCD、CER、WER等指标。

Result: 在 spoken 与 imagined 下，观测到稳定的声学重建与可比的文本准确率；随着时长增加，声学相似度下降但文本级解码基本维持，句末的单词位置错误略增；语言模型纠错显著降低CER与WER且不引入语义扭曲。

Conclusion: 证明了直接、开放式的EEG-to-Voice重建在 spoken 和 imagined 语音上可行且无需显式时序对齐。

Abstract: Restoring speech communication from neural signals is a central goal of brain-computer interface research, yet EEG-based speech reconstruction remains challenging due to limited spatial resolution, susceptibility to noise, and the absence of temporally aligned acoustic targets in imagined speech. In this study, we propose an EEG-to-Voice paradigm that directly reconstructs speech from non-invasive EEG signals without dynamic time warping (DTW) or explicit temporal alignment. The proposed pipeline generates mel-spectrograms from EEG in an open-loop manner using a subject-specific generator, followed by pretrained vocoder and automatic speech recognition (ASR) modules to synthesize speech waveforms and decode text. Separate generators were trained for spoken speech and imagined speech, and transfer learning-based domain adaptation was applied by pretraining on spoken speech and adapting to imagined speech. A minimal language model-based correction module was optionally applied to correct limited ASR errors while preserving semantic structure. The framework was evaluated under 2 s and 4 s speech conditions using acoustic-level metrics (PCC, RMSE, MCD) and linguistic-level metrics (CER, WER). Stable acoustic reconstruction and comparable linguistic accuracy were observed for both spoken speech and imagined speech. While acoustic similarity decreased for longer utterances, text-level decoding performance was largely preserved, and word-position analysis revealed a mild increase in decoding errors toward later parts of sentences. The language model-based correction consistently reduced CER and WER without introducing semantic distortion. These results demonstrate the feasibility of direct, open-loop EEG-to-Voice reconstruction for spoken speech and imagined speech without explicit temporal alignment.

</details>


### [21] [PaperNet: Efficient Temporal Convolutions and Channel Residual Attention for EEG Epilepsy Detection](https://arxiv.org/abs/2512.22172)
*Md Shahriar Sajid,Abhijit Kumar Ghosh,Fariha Nusrat*

Main category: eess.SP

TL;DR: PaperNet: a compact hybrid EEG classifier using temporal convolutions, channel-wise residual attention, and a lightweight bidirectional recurrent block; achieves macro-F1 0.96 on BEED with ~0.6M parameters, subject-independent, efficient for deployment; ablation confirms contributions.


<details>
  <summary>Details</summary>
Motivation: EEG signals possess rich temporal-spectral structure but are noisy and variable across subjects; need lightweight, effective models that handle multi-scale dynamics without heavy recurrent modules.

Method: 提出 PaperNet，结合(1) Temporal convolutions capturing multi-scale temporal patterns, (2) channel-wise residual attention对电极权重进行 reweighting并增强特征，(3) 轻量级双向循环块用于短窗口分类的上下文建模；在 BEED 海外数据集上进行主体无关训练评估，并进行消融研究。

Result: 在 held-out 测试集 macro-F1 达0.96，参数约 0.6M；四个类别表现均衡；消融研究显示时序卷积、残差注意力和循环聚合的贡献；通道注意力权重提供电极相关性线索；计算分析表明该模型适合资源受限设备。

Conclusion: 通过巧妙地结合时间过滤、通道重加权和循环上下文建模，可以在不显著增加计算成本的前提下实现强劲的EEG分类性能。

Abstract: Electroencephalography (EEG) signals contain rich temporal-spectral structure but are difficult to model due to noise, subject variability, and multi-scale dynamics. Lightweight deep learning models have shown promise, yet many either rely solely on local convolutions or require heavy recurrent modules. This paper presents PaperNet, a compact hybrid architecture that combines temporal convolutions, a channel-wise residual attention module, and a lightweight bidirectional recurrent block which is used for short-window classification. Using the publicly available BEED: Bangalore EEG Epilepsy Dataset, we evaluate PaperNet under a clearly defined subject-independent training protocol and compare it against established and widely used lightweight baselines. The model achieves a macro-F1 of 0.96 on the held-out test set with approximately 0.6M parameters, while maintaining balanced performance across all four classes. An ablation study demonstrates the contribution of temporal convolutions, residual attention, and recurrent aggregation. Channel-wise attention weights further offer insights into electrode relevance. Computational profiling shows that PaperNet remains efficient enough for practical deployment on resource-constrained systems through out the whole process. These results indicate that carefully combining temporal filtering, channel reweighting, and recurrent context modeling can yield strong EEG classification performance without excessive computational cost.

</details>


### [22] [Simultaneous Source Separation, Synchronization, Localization and Mapping for 6G Systems](https://arxiv.org/abs/2512.22393)
*Alexander Venus,Erik Leitinger,Klaus Witrisal*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multipath-based simultaneous localization and mapping (MP-SLAM) is a promising approach for future 6G networks to jointly estimate the positions of transmitters and receivers together with the propagation environment. In cooperative MP-SLAM, information collected by multiple mobile terminals (MTs) is fused to enhance accuracy and robustness. Existing methods, however, typically assume perfectly synchronized base stations (BSs) and orthogonal transmission sequences, rendering inter-BS interference at the MTs negligible. In this work, we relax these assumptions and address simultaneous source separation, synchronization, and mapping. A relevant example arises in modern 5G systems, where BSs employ muting patterns to mitigate interference, yet localization performance still degrades. We propose a novel BS-dependent data association and synchronization bias model, integrated into a joint Bayesian framework and inferred via the sum-product algorithm on a factor graph. The impact of joint synchronization and source separation is analyzed under various system configurations. Compared with state-of-the-art cooperative MP-SLAM assuming orthogonal and synchronized BSs, our statistical analysis shows no significant performance degradation.

</details>


### [23] [Compressive Toeplitz Covariance Estimation From Few-Bit Quantized Measurements With Applications to DOA Estimation](https://arxiv.org/abs/2512.22527)
*Hongwei Xu,Weichao Zheng,Zai Yang*

Main category: eess.SP

TL;DR: 在稀疏观测和粗量化条件下，提出了用于 Hermitian Toeplitz 协方差估计的 Q-TSCM 及其 2k-TSCM 版本，并在高斯假设下给出非渐近误差界；通过 Q-SPA 在协方差拟合框架下加入 PSD 约束，进一步提升性能，数值实验验证在 DOA 估计中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实系统中观测受限且存在量化噪声，且希望利用 Toeplitz 结构以提升协方差估计的准确性；三角抖动量化框架用于减轻量化偏差，需研究在稀疏采样和量化下的误差特性与可行性。

Method: 提出 Toeplitz 投影样本协方差矩阵 Q-TSCM 来纠正量化带来的偏差，以及通过截断量化前观测得到的 2k-TSCM；在复高斯假设下推导非渐近误差界，量化水平对误差以二次形式影响，并通过覆盖系数表达稀疏采样影响；提出基于协方差拟合的 Q-SPA，加入半正定约束，通过 SDP 求解。

Result: 给出关于误差的非对称性界限，量化水平对误差的二次依赖以及覆盖系数对稀疏采样的体现；Q-SPA 显著改善估计，且在 DOA 问题中有效。

Conclusion: 所提估计量能够在稀疏且量化的观测条件下有效估计 Hermitian Toeplitz 协方差，并借助 PSD 约束提升性能，理论与数值实验均支持其在 DOA 估计中的应用潜力；但 SDP 成本可能成为实现时的考量。

Abstract: This paper addresses the problem of estimating the Hermitian Toeplitz covariance matrix under practical hardware constraints of sparse observations and coarse quantization. Within the triangular-dithered quantization framework, we propose an estimator called Toeplitz-projected sample covariance matrix (Q-TSCM) to compensate for the quantization-induced bias, together with its finite-bit counterpart termed the $2k$-bit Toeplitz-projected sample covariance matrix ($2k$-TSCM), obtained by truncating the pre-quantization observations. Under the complex Gaussian assumption, we derive non-asymptotic error bounds of the estimators that reveal a quadratic dependence on the quantization level and capture the effect of sparse sampling patterns through the so-called coverage coefficient. To further improve performance, we propose the quantized sparse and parametric approach (Q-SPA) based on a covariance-fitting criterion, which enforces additionally positive semidefiniteness at the cost of solving a semidefinite program. Numerical experiments are presented that corroborate our theoretical findings and demonstrate the effectiveness of the proposed estimators in the application to direction-of-arrival estimation.

</details>


### [24] [Real-Time Multi-Target Detection and Tracking with mmWave 5G NR Waveforms on RFSoC](https://arxiv.org/abs/2512.22582)
*Xinyang Li,Hian Zing Voon,Vlad C. Andrei,Alexander Sessler,Nunzio Sciammetta,Ullrich J. Mönich,Dominic A. Schupke,Holger Boche*

Main category: eess.SP

TL;DR: Real-time multi-target detection and tracking using 5G NR PDSCH waveform (400 MHz at 28 GHz) with full hardware-accelerated sensing on RFSoC PL; 3D range-angle tensors processed on host with adaptive background subtraction, CA-CFAR (with DBSCAN) clustering, and EKF tracking; SDR testbed integrates CPUs/GPUs/FPGA for design flexibility.


<details>
  <summary>Details</summary>
Motivation: Demonstrate a low-latency, hardware-accelerated joint sensing and communication system (ISAC) using a 5G NR waveform at mmWave, enabling real-time multi-target detection and tracking.

Method: Hardware: RFSoC 4x2 board plus Sivers EVK02001 mmWave beamformers for Tx/Rx. Sensing transceiver processing and fast beam control implemented entirely in programmable logic (PL) on RFSoC. Data products are 3D range-angle tensors. Processing performed on a host PC with adaptive background subtraction, CA-CFAR detection with DBSCAN clustering, and EKF tracking. SDR testbed uses heterogeneous computing resources (CPUs, GPUs, FPGAs) for flexibility.

Result: Achieved real-time implementation of multi-target detection and tracking with fully hardware-accelerated sensing and low latency. Demonstrates feasibility of processing 3D RA tensors on a PC and seamless integration of SDR with heterogeneous compute resources.

Conclusion: The proposed hardware-software co-design offers a feasible and flexible platform for real-time mmWave ISAC using 5G NR waveforms, combining high-bandwidth sensing with programmable hardware and diverse computing resources.

Abstract: We demonstrate a real-time implementation of multi-target detection and tracking using 5G New Radio (NR) physical downlink shared channel (PDSCH) waveform with 400 MHz bandwidth at 28 GHz carrier frequency. The hardware platform is built on a radio frequency system-on-chip (RFSoC) 4x2 board connected with a pair of Sivers EVK02001 mmWave beamformers for transmission and reception. The entire sensing transceiver processing and fast beam control are realized purely in the programmable logic (PL) part of the RFSoC, enabling low-latency and fully hardware-accelerated operation. The continuously acquired sensing data constitute 3D range-angle (RA) tensors, which are processed on a host PC using adaptive background subtraction, cell-averaging constant false alarm rate (CA-CFAR) detection with density-based spatial clustering of applications with noise (DBSCAN) clustering, and extended Kalman filtering (EKF), to detect and track targets in the environment. Our software-defined radio (SDR) testbed integrates heterogeneous computing resources, including CPUs, GPUs, and FPGAs, thereby providing design flexibility for a wide range of tasks.

</details>


### [25] [Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space](https://arxiv.org/abs/2512.22676)
*Sergey Salishev*

Main category: eess.SP

TL;DR: 在低功耗硬件中提升信号处理效率的综合框架，涵盖能耗建模、整数友好近似、冲突自由FFT调度及Schur算法并行性分析。


<details>
  <summary>Details</summary>
Motivation: 资源受限的低功耗计算场景需要在能耗、并行度和存储之间做出权衡。本文提出一个面向专用加速器的设计框架，旨在在保持精度和可实现性的前提下提升能效。

Method: (i) 提出用于时钟CMOS逻辑的能耗模型，以支持选择最优并行度；(ii) 给出整数友好的一元函数近似方法，通过受限的分段多项式（准样条）构造来减小查找表规模，并提供精度保证；(iii) 提出混合进制流式FFT在多-bank、单端口存储器上的冲突自由数据放置与执行顺序的证明性方法，并给出自排序FFT变体；(iv) 给出快速Schur算法在Toeplitz矩阵求解中的并行性与内存分析，且以回声抵消工作负载为动机。

Result: 给出具有构造性定理的调度与设计权衡，覆盖能耗-并行度取舍、近似误差界限、FFT数据放置的冲突避免策略以及Schur算法的并行性与内存需求分析，从而支持高效的专用加速器设计。

Conclusion: 这些结果共同构成面向低功耗专用加速器的设计框架，提供可实现的能效优化路径，并为回声消除等应用中的高效计算提供理论与实现支撑。

Abstract: This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space, with the goal of improving energy efficiency of low-power computing hardware. We propose (i) a power/energy consumption model for clocked CMOS logic that supports selecting optimal parallelism, (ii) integer-friendly approximation methods for elementary functions that reduce lookup-table size via constrained piecewise-polynomial (quasi-spline) constructions with accuracy guarantees, (iii) provably conflict-free data placement and execution order for mixed-radix streaming FFT on multi-bank and single-port memories, including a self-sorting FFT variant, and (iv) a parallelism/memory analysis of the fast Schur algorithm for superfast Toeplitz system solving, motivated by echo-cancellation workloads. The results provide constructive theorems, schedules, and design trade-offs enabling efficient specialized accelerators.

</details>


### [26] [Multistatic Radar Performance in the Presence of Distributed Wireless Synchronization](https://arxiv.org/abs/2512.22686)
*Kumar Sai Bondada,Daniel J. Jakubisin,R. Michael Buehrer*

Main category: eess.SP

TL;DR: 提出一种基于分布式无线同步协议的多基点雷达（MSR）系统，利用两音波形用于频率同步、双向波形交换用于时间同步（非GPS）。建立BCRLB框架量化同步偏移对联合时延-多普勒估计、目标定位与速度估计的影响，仿真表明残余同步偏移会降级性能；同步链路性能受链路和发射参数影响，经过优化可超越单基雷达并接近理想情况，且实现可行性通过参数仿真得到支持。


<details>
  <summary>Details</summary>
Motivation: 在无GPS等外部时间参考条件下，提升分布式雷达的同步精度以实现高精度联合定位与跟踪；研究如何通过无线协议实现频率和时间的鲁棒同步及其对系统性能的影响。

Method: 提出两音波形交换实现频率同步、双向波形交换实现时间同步的分布式无线同步协议；建立BCRLB框架来量化同步偏移对联合时延与多普勒估计的影响；推导解析表达式并通过仿真验证不同同步链路参数对MSR性能的影响；通过优化同步链路参数来提升系统性能。

Result: 给出与同步相关的Cramer-Rao下界及其对MSR性能的定量评估，仿真表明残余同步偏移会显著降低定位与跟踪精度；通过优化同步链路参数，MSR性能可超过单普雷达且接近理想情况；同步链路参数的仿真结果支持在实际系统中的可行性。

Conclusion: 无线同步协议使MSR在不依赖GPS的情况下实现显著的性能提升，且通过合理设计与优化同步链路参数，具有实际可行性和潜在应用价值。

Abstract: This paper proposes a multistatic radar (MSR) system utilizing a distributed wireless synchronization protocol. The wireless synchronization protocol uses a two-tone waveform exchange for frequency synchronization and a bi-directional waveform exchange for time synchronization, independent of GPS. A Bayesian Cramer-Rao lower bound (BCRLB) framework is developed to quantify the impact of synchronization offsets on joint delay and Doppler estimation, and consequently, on target localization and velocity estimation accuracy. Simulation results derived from the analytical expressions establish the extent to which the residual synchronization offsets degrade the MSR's performance. The performance of the synchronization links primarily depends on the synchronization-link channel and transmit parameters; optimizing these parameters enables the MSR configuration to surpass the monostatic performance and approach the ideal case. Furthermore, the simulated synchronization-link parameters suggest that practical implementation is feasible.

</details>


### [27] [Instance Communication System for Intelligent Connected Vehicles: Bridging the Gap from Semantic to Instance-Level Transmission](https://arxiv.org/abs/2512.22693)
*Daiqi Zhang,Bizhu Wang,Wenqi Zhang,Chen Sun,Xiaodong Xu*

Main category: eess.SP

TL;DR: 提出 Instance Communication (InsCom) 框架，通过实例级传输替代语义级传输，实现对图像中任务关键实例的选择性传输，与现有 SemCom 相比可显著减少数据量并提升传输质量。


<details>
  <summary>Details</summary>
Motivation: 在智能化连接车辆场景中，蜂窝资源有限，现有 SemCom 在保留语义信息的同时仍存在残留冗余，无法充分利用任务相关性。需进一步将传输粒度下沉到实例级以降低通信开销并提升任务性能。

Method: 通过场景图生成模型识别图像中的全部实例及其关系，区分语义上相同的实例；基于用户可配置的任务关键标准（主体语义、关系-对象对）筛选识别出的实例，最终仅传输这些任务关键实例。

Result: 在多数据集和无线信道条件下，与最先进的 SemCom 相比，InsCom 实现数据量减少 >7.82x，质量提升在 1.75–14.03 dB 之间。

Conclusion: InsCom 显著提升了在受限无线资源下的传输效率和下游任务性能，提供了面向实例的高效通信范式，并为 ICV 场景的高效感知—理解—传输链路提供了新的方向。

Abstract: Intelligent Connected Vehicles (ICVs) rely on high-speed data transmission for efficient and safety-critical services. However, the scarcity of wireless resources limits the capabilities of ICVs. Semantic Communication (SemCom) systems can alleviate this issue by extracting and transmitting task-relevant information, termed semantic information, instead of the entire raw data. Despite this, we reveal that residual redundancy persists within SemCom systems, where not all instances under the same semantic category are equally critical for downstream tasks. To tackle this issue, we introduce Instance Communication (InsCom), which elevates communication from the semantic level to the instance level for ICVs. Specifically, InsCom uses a scene graph generation model to identify all image instances and analyze their inter-relationships, thus distinguishing between semantically identical instances. Additionally, it applies user-configurable, task-critical criteria based on subject semantics and relation-object pairs to filter recognized instances. Consequently, by transmitting only task-critical instances, InsCom significantly reduces data redundancy, substantially enhancing transmission efficiency within limited wireless resources. Evaluations across various datasets and wireless channel conditions show that InsCom achieves a data volume reduction of over 7.82 times and a quality improvement ranging from 1.75 to 14.03 dB compared to the state-of-the-art SemCom systems.

</details>


### [28] [On the Impact of Phase Errors in Phase-Dependent Amplitudes of Near-Field RISs](https://arxiv.org/abs/2512.22825)
*Ke Wang,Chan-Tong Lam,Benjamin K. Ng,Yue Liu*

Main category: eess.SP

TL;DR: 本论文研究近场RIS像素的相位偏差与幅度耦合（PDAs），提出剩余功率RP度量，给出RP在误差下的上界与收敛性，并给出四种像素反射模型，推导RP的多项近似界，以及对NF信道、频谱效率的上界，揭示相位落在端点与中间时的RP差异，以及忽略PE对性能估计的影响。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究将PSEs与PDAs分离、未考虑PDAs与PSE耦合对RIS性能的影响，以及在像素尺度、相位误差与幅度随机性下对信道与容量分析的不足。

Method: 建立RP度量并证明其渐近收敛性；提出四种像素反射模型；对RP给出Taylor展开的多项式上界；基于Friis和投影开环/孔径提出NF信道模型；利用Cauchy-Schwarz与Riemann求和给出频谱效率上界，并证明当像素面积减小时界更紧。

Result: 得到RP在独立同分布PÉs与完全相关PÉs下的比较结论、并在相位接近端点与中点时关系不同；指出忽略PDAs的PE会导致对RIS增益的高估。

Conclusion: 该工作揭示了PDAs与PSEs耦合对RIS性能评估的关键性，提供了RP度量、像素模型和理论界限，为NF-RIS设计与分析提供更准确的理论框架。

Abstract: This paper investigates mutual coupling between phase-dependent amplitudes (PDAs) and designed phase shifts within pixels of near-field (NF) reconfigurable intelligent surfaces (RISs) in the presence of phase errors (PEs). In contrast to existing research that treats phase shifts with errors (PSEs) and the PDAs separately, we introduce a remaining power (RP) metric to quantify the proportion of power preserved in the signals reflected by the RIS, and we prove its asymptotic convergence to theoretical values by leveraging extended Glivenko-Cantelli theorem. Then, the RP of signals passing through RIS pixels is jointly examined under combined phase and amplitude uncertainties. In addition, we propose four pixel reflection models to capture practical conditions, and we derive approximate polynomial upper bounds for the RP with error terms by applying Taylor expansion. Furthermore, based on Friis transmission formula and projected aperture, we propose a general NF channel model that incorporates the coupling between the PSEs and the PDAs. By using Cauchy-Bunyakovsky-Schwarz inequality and Riemann sums, we derive a closed-form upper bound on spectral efficiency, and the bound becomes tighter as the pixel area decreases. We reveal that as the RIS phase shifts approach the ends of their range, the RP under independent and identically distributed PEs is smaller than that under fully correlated PEs, whereas this relationship reverses when the phase shifts are near the middle of their range. Neglecting the PEs in the PDAs leads to an overestimation of the RIS performance gain, explaining the discrepancies between theoretical and measured results.

</details>


### [29] [Flexible Intelligent Metasurface for Downlink Communications under Statistical CSI](https://arxiv.org/abs/2512.23045)
*Vaibhav Kumar,Anastasios Papazafeiropoulos,Pandelis Kourtessis,John Senior,Marwa Chafii,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: eess.SP

TL;DR: 在统计CSI下，基于柔性智能超表面（FIM）的下行多用户MISO系统实现平均总频谱效率最大化的迭代优化；与常规刚性天线阵列（RAA）相比，在强空间相关性场景下具有显著性能提升，优化基于导出的FIM空间相关矩阵的梯度投影法。


<details>
  <summary>Details</summary>
Motivation: 解决当前文献依赖瞬时信道状态信息（CSI）的局限，探究在统计CSI条件下，FIM辅助传输在建立面可变形的自由度下对系统性能的提升，特别是在6G无线系统中的潜力和边界。

Method: 推导FIM辅助发射机的空间相关矩阵；在统计CSI假设下，建立以平均总谱效用最大化为目标的优化问题；提出基于梯度投影的迭代优化算法来更新FIM形态以提升性能；场景为下行多用户MISO。

Result: 仿真实验显示，在统计CSI条件下，FIM系统相较于RAA在具有强空间相关性的场景中能获得显著的性能提升；当通道相关性较弱时，优势减弱甚至不显著。

Conclusion: 在统计CSI条件下，FIM具备通过表面形变自由度提升系统性能的潜力，且收益高度依赖于通道的空间相关性；该研究证明了FIM在6G场景中的应用前景，但需在弱相关性场景与计算复杂度方面进一步完善。

Abstract: Flexible intelligent metasurface (FIM) is a recently developed, groundbreaking hardware technology with promising potential for 6G wireless systems. Unlike conventional rigid antenna array (RAA)-based transmitters, FIM-assisted transmitters can dynamically alter their physical surface through morphing, offering new degrees of freedom to enhance system performance. In this letter, we depart from prior works that rely on instantaneous channel state information (CSI) and instead address the problem of average sum spectral efficiency maximization under statistical CSI in a FIM-assisted downlink multiuser multiple-input single-output setting. To this end, we first derive the spatial correlation matrix for the FIM-aided transmitter and then propose an iterative FIM optimization algorithm based on the gradient projection method. Simulation results show that with statistical CSI, the FIM-aided system provides a significant performance gain over its RAA-based counterpart in scenarios with strong spatial channel correlation, whereas the gain diminishes when the channels are weakly correlated.

</details>


### [30] [Generalizable Learning for Massive MIMO CSI Feedback in Unseen Environments](https://arxiv.org/abs/2512.22840)
*Haoyu Wang,Zhi Sun,Shuangfeng Han,Xiaoyun Wang,Zhaocheng Wang*

Main category: eess.SP

TL;DR: 提出物理驱动的分布对齐框架EG-CsiNet，通过多簇信道分布建模与EYM定理的多簇解耦实现对泛化能力提升，实验证明可提升泛化性能>3 dB。


<details>
  <summary>Details</summary>
Motivation: 面对未见环境的泛化挑战，需引入物理先验来对齐簇分布，降低部署成本。

Method: 1) 建模簇状信道分布（多簇结构+单簇响应）; 2) 提出物理分布对齐（多簇解耦+细粒度对齐）; 3) 基于EYM定理的高效多簇解耦算法； 4) 混合准则估计解耦簇数量以提升鲁棒性； 5) 提出EG-CsiNet框架。

Result: 在广泛的仿真与sim-to-real实验中，EG-CsiNet在泛化误差上较现有方法提升>3 dB。

Conclusion: 物理分布对齐融入学习框架提升对新环境的鲁棒性与实时性储备。

Abstract: Deep learning is promising to enhance the accuracy and reduce the overhead of channel state information (CSI) feedback, which can boost the capacity of frequency division duplex (FDD) massive multiple-input multiple-output (MIMO) systems. Nevertheless, the generalizability of current deep learning-based CSI feedback algorithms cannot be guaranteed in unseen environments, which induces a high deployment cost. In this paper, the generalizability of deep learning-based CSI feedback is promoted with physics interpretation. Firstly, the distribution shift of the cluster-based channel is modeled, which comprises the multi-cluster structure and single-cluster response. Secondly, the physics-based distribution alignment is proposed to effectively address the distribution shift of the cluster-based channel, which comprises multi-cluster decoupling and fine-grained alignment. Thirdly, the efficiency and robustness of physics-based distribution alignment are enhanced. Explicitly, an efficient multi-cluster decoupling algorithm is proposed based on the Eckart-Young-Mirsky (EYM) theorem to support real-time CSI feedback. Meanwhile, a hybrid criterion to estimate the number of decoupled clusters is designed, which enhances the robustness against channel estimation error. Fourthly, environment-generalizable neural network for CSI feedback (EG-CsiNet) is proposed as a novel learning framework with physics-based distribution alignment. Based on extensive simulations and sim-to-real experiments in various conditions, the proposed EG-CsiNet can robustly reduce the generalization error by more than 3 dB compared to the state-of-the-arts.

</details>


### [31] [Confidence analysis-based hybrid heartbeat detection for ballistocardiogram using template matching and deep learning](https://arxiv.org/abs/2512.22926)
*Dongli Cai,Xihe Chen,Yaosheng Chen,Hong Xian,Baoxian Yu,Han Zhang*

Main category: eess.SP

TL;DR: 提出基于置信度分析的TM与DL混合心跳检测方法，用于非接触性BCG信号。通过置信度筛选实现两者互补，在不同场景下提升鲁棒性与准确性；在真实临床数据集上明显优于单独TM或DL。


<details>
  <summary>Details</summary>
Motivation: TM与DL各自存在对个体差异与信号质量的敏感性，单一方法在不同场景下性能波动。需要一个结合两者优点并通过置信度筛选的混合策略，以实现更稳定的心跳间期检测。

Method: 将TM与DL的输出进行融合，并通过置信度分析筛选结果。置信度由两部分构成：1) 心跳片段与检测模板的平均相关性，用以衡量信号形态的一致性；2) 已检测心跳间期的归一化标准差，用以衡量间期变异性。保留高置信度的检测结果，并在不同场景中利用TM的稳健性与DL的鲁棒性互补优势。

Result: 在包含34名受试者、共924,235个心跳的临床BCG数据集上验证。混合方法的平均绝对间期误差为20.73 ms，相较单独TM与DL分别降低了29.28 ms和10.13 ms。案例研究还显示TM对个体差异和DL对信号质量的鲁棒性，从而证明混合策略在实际BCG监测场景中的优越性。

Conclusion: 基于置信度分析的TM+DL混合心跳检测在非接触BCG监测中展现更高的鲁棒性与准确性，适用于临床实际场景，能有效克服单一方法的局限性。

Abstract: Heartbeat interval can be detected from ballistocardiogram (BCG) signals in a non-contact manner. Conventional methods achieved heartbeat detection from different perspectives, where template matching (TM) and deep learning (DL) were based on the similarity of neighboring heartbeat episodes and robust spatio-temporal characteristics, respectively, and thus, performed varied from case to case. Inspired by the above facts, we propose confidence analysis-based hybrid heartbeat detection using both TM and DL, and further explore the advantages of both methods in various scenarios. To be specific, the confidence of the heartbeat detection results was evaluated by the consistency of signal morphology and the variability of the detected heartbeat intervals, which could be formulated by the averaged correlation between each heartbeat episode and the detected template and the normalized standard deviation among detected heartbeat intervals, respectively, where the results with higher confidence were remained. In order to validate the effectiveness of the proposed hybrid method, we conducted experiments using practical clinical BCG dataset with 34 subjects including 924,235 heartbeats. Numerical results showed that the proposed hybrid method achieved an average absolute interval error of 20.73 ms, yielding a reduction of 29.28 ms and 10.13 ms compared to solo TM and DL methods, respectively. Besides, case study showed the robustness of heartbeat detection of TM and DL to individual differences and signal quality, respectively, and in turn, validated that the hybrid method could benefit from the complementary advantages of both methods, which demonstrated the superiority of the proposed hybrid method in practical BCG monitoring scenarios.

</details>


### [32] [Unscented and Higher-Order Linear Covariance Fidelity Checks and Measures of Non-Gaussianity](https://arxiv.org/abs/2512.23152)
*Jackson Kulik,Braden Hastings,Keith A. LeGrand*

Main category: eess.SP

TL;DR: LinCov fidelity measures for uncertainty modeling exploit higher-order statistics, constrained optimization, and the unscented transform to assess and improve the accuracy of linear covariance approximations, particularly for nonlinear spacecraft navigation problems.


<details>
  <summary>Details</summary>
Motivation: Linear covariance (LinCov) methods offer computational efficiency over Monte Carlo but rely on linearization; assessing when these approximations break down is crucial for spacecraft navigation and mission planning amid strong nonlinearity and large uncertainties.

Method: Develop computational techniques to evaluate LinCov fidelity using higher-order statistics, constrained optimization, and the unscented transform.

Result: Proposes new LinCov fidelity measures that quantify approximation quality; framework to diagnose deficiency of LinCov in nonlinear regimes.

Conclusion: The proposed fidelity measures provide a practical toolkit to evaluate and potentially improve LinCov-based uncertainty analysis in spacecraft navigation and related planning tasks.

Abstract: Linear covariance (LinCov) techniques have gained widespread traction in the modeling of uncertainty, including in the preliminary study of spacecraft navigation performance. While LinCov methods offer improved computational efficiency compared to Monte Carlo based uncertainty analysis, they inherently rely on linearization approximations. Understanding the fidelity of these approximations and identifying when they are deficient is critically important for spacecraft navigation and mission planning, especially when dealing with highly nonlinear systems and large state uncertainties. This work presents a number of computational techniques for assessing linear covariance performance. These new LinCov fidelity measures are formulated using higher-order statistics, constrained optimization, and the unscented transform.

</details>


### [33] [Ultra-Massive MIMO with Orthogonal Chirp Division Multiplexing for Near-Field Sensing and Communication Integration](https://arxiv.org/abs/2512.23246)
*Ziwei Wan,Zhen Gao,Fabien Heliot,Qu Luo,Pei Xiao,Haiyang Zhang,Christos Masouros,Yonina C. Eldar,Sheng Chen*

Main category: eess.SP

TL;DR: 本文提出在超大规模MIMO（UM-MIMO）系统中，结合OCDM波形实现近场ISAC的统一架构，采用FMCW感知，设计专门感知子载波与DSA实现回波解耦，并提出虚拟双基地感知（VIBS）以提升定位与三维速度测量；并对UM-MIMO OCDM的信道估计进行研究，仿真表明对感知和通信均有提升。


<details>
  <summary>Details</summary>
Motivation: 解决近场ISAC在硬件复杂性和解耦困难中的挑战；充分利用UM-MIMO的空间关系和OCDM的频域结构，提升感知精度与通信性能。

Method: 提出一个综合ISAC架构：UM-MIMO基站使用OCDM进行通信，感知端使用FMCW检测；为感知设计DSSs通过各自感知天线逐载波传输；设计DSS选择和接收参数以实现DSA回波的解耦；基于UM-MIMO的空间多样性提出VIBS，通过多天线对估计提高定位与三维速度精度；对UM-MIMO OCDM系统进行信道估计，结合感知结果。

Result: 仿真表明，所提ISAC在感知精度方面显著提升，同时对通信性能也有积极影响。

Conclusion: 该工作将近场ISAC问题与UM-MIMO和OCDM相结合，提供了低成本高精度的感知方案，且VIBS对复杂信道环境具有鲁棒性；未来工作可扩展至实际硬件实现和对比其他波形/阵列结构。

Abstract: This paper integrates the emerging ultra-massive multiple-input multiple-output (UM-MIMO) technique with orthogonal chirp division multiplexing (OCDM) waveform to tackle the challenging near-field integrated sensing and communication (ISAC) problem. Specifically, we conceive a comprehensive ISAC architecture, where an UM-MIMO base station adopts OCDM waveform for communications and a co-located sensing receiver adopts the frequency-modulated continuous wave (FMCW) detection principle to simplify the associated hardware. For sensing tasks, several OCDM subcarriers, namely, dedicated sensing subcarriers (DSSs), are each transmitted through a dedicated sensing antenna (DSA) within the transmit antenna array. By judiciously designing the DSS selection scheme and optimizing receiver parameters, the FMCW-based sensing receiver can decouple the echo signals from different DSAs with significantly reduced hardware complexity. This setup enables the estimation of ranges and velocities of near-field targets in an antenna-pairwise manner. Moreover, by leveraging the spatial diversity of UM-MIMO, we introduce the concept of virtual bistatic sensing (VIBS), which incorporates the estimates from multiple antenna pairs to achieve high-accuracy target positioning and three-dimensional velocity measurement. The VIBS paradigm is immune to hostile channel environments characterized by spatial non-stationarity and uncorrelated multipath environment. Furthermore, the channel estimation of UM-MIMO OCDM systems enhanced by the sensing results is investigated. Simulation results demonstrate that the proposed ISAC scheme enhances sensing accuracy, and also benefits communication performance.

</details>


### [34] [On Signal Peak Power Constraint of Over-the-Air Federated Learning](https://arxiv.org/abs/2512.23381)
*Lorenz Bielefeld,Paul Zheng,Oner Hanay,Yao Zhu,Yulin Hu,Anke Schmeink*

Main category: eess.SP

TL;DR: AirComp-FL在存在放大器瞬时峰值功率约束下的性能分析，发现剪切与滤波在单载波/OFDM系统中影响显著，OFDM下带内失真更易恶化性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习通过空中计算聚合梯度时，非线性射频放大器的峰值约束被忽略，本研究关注剪切+滤波对系统的影响。

Method: 分析经典幅度剪切与滤波策略在单载波与OFDM下的传输-学习系统，比较梯度分布对PAPR的影响及在实际功率约束下的性能。

Result: 实际场景发射功率常超出放大器极限，剪切+滤波会降低FL性能；在多载波OFDM情形下，带内失真导致的性能下降更为显著。

Conclusion: 需要更精细的非线性建模、功率控制或鲁棒的AirComp-FL设计，以缓解放大器约束带来的性能损失，尤其在OFDM系统中。

Abstract: Federated learning (FL) has been considered a promising privacy preserving distributed edge learning framework. Over-the-air computation (AirComp) technique leveraging analog transmission enables the aggregation of local updates directly over-the-air by exploiting the superposition properties of wireless multiple-access channel, thereby drastically reducing the communication bottleneck issues of FL compared with digital transmission schemes. This work points out that existing AirComp-FL overlooks a key practical constraint, the instantaneous peak-power constraints imposed by the non-linearity of radiofrequency power amplifiers. We present and analyze the effect of the classic method to deal with this issue, amplitude clipping combined with filtering. We investigate the effect of instantaneous peak-power constraints in AirComp-FL for both single-carrier and multi-carrier orthogonal frequency-division multiplexing (OFDM) systems. We highlight the specificity of AirComp-FL: the samples depend on the gradient value distribution, leading to a higher peak-to-average power ratio (PAPR) than that observed for uniformly distributed signals. Simulation results demonstrate that, in practical settings, the instantaneous transmit power regularly exceeds the power-amplifier limit; however, by applying clipping and filtering, the FL performance can be degraded. The degradation becomes pronounced especially in multi-carrier OFDM systems due to the in-band distortions caused by clipping and filtering.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [35] [RIS, Active RIS or RDARS: A Comparative Insight Through the Lens of Energy Efficiency](https://arxiv.org/abs/2512.22533)
*Aparna V C,Shashank Shekhar,Sheetal Kalyani*

Main category: cs.IT

TL;DR: RDARS在sub-6GHz下可以高效提升覆盖并降低能耗；在毫米波(mmWave)下，主动RIS在能效方面表现更优；当RS单元较少且接近UE时，RIS对能效更具优势，优于两种其他方案。


<details>
  <summary>Details</summary>
Motivation: 研究在sub-6GHz与mmWave两种频段下，RIS、主动RIS与RDARS在覆盖与能量效率（EE）上的对比，并考量RS的放置与单元数对EE和覆盖的影响。

Method: 通过理论分析结合仿真，评估在不同配置下的覆盖与EE，包括RIS、主动RIS、RDARS在sub-6GHz与mmWave两种场景中的表现，变量包括RS单元数、放置位置及与UE的距离。

Result: 在sub-6GHz场景下，RDARS展现出较高的能效提升与覆盖增强；在mmWave场景中，主动RIS的能效显著优于RDARS与RIS；且当RS单元数较少且UE靠近时，RIS的能效仍显著优于主动RIS与RDARS。

Conclusion: 对于sub-6GHz，RDARS是一个有前景的低能耗覆盖增强方案；对于mmWave，主动RIS更具能效优势；在RS单元数较少或UE距离较近时，传统RIS仍具能效优势，显示出不同频段、部署场景和单元尺度下的权衡。

Abstract: Multiplicative fading is a major limitation of reconfigurable intelligent surfaces (RIS), restricting their effective coverage in both existing sub-6GHz systems and future mmWave networks. Although active RIS architectures mitigate this issue, they require high power consumption and introduce practical challenges due to the need for integrated amplifiers. Recently, reconfigurable distributed antenna and reflecting surfaces (RDARS) have been proposed to alleviate multiplicative fading through connected modes. In this work, we compare RIS, active RIS, and RDARS in terms of coverage and energy efficiency (EE) in both sub-6GHz and mmWave bands, and we investigate the impact of placement and the number of elements of reconfigurable surface (RS) on EE and coverage. The simulation results show that RDARS offers a highly energy-efficient alternative of enhancing coverage in sub-6GHz systems, while active RIS is significantly more energy-efficient in mmWave systems. Additionally, for a lower number of RS elements and for near UEs, RIS remains considerably more energy-efficient than both active RIS and RDARS.

</details>


### [36] [An Improved Lower Bound on Cardinality of Support of the Amplitude-Constrained AWGN Channel](https://arxiv.org/abs/2512.22691)
*Haiyang Wang,Luca Barletta,Alex Dytso*

Main category: cs.IT

TL;DR: 本文证明在幅度约束的高斯信道中，容量达到输入分布的最小支持大小下界为 O(A sqrt(log A))，否定线性增长的猜想；并提出将问题映射到紧致域的 wrap 操作及用有限高斯混合物逼近均匀分布的最优近似理论，形成一个分析框架。


<details>
  <summary>Details</summary>
Motivation: 理解幅度受限 AWGN 信道的容量与最优输入分布的结构。以往下界在线性到二次之间波动，且存在线性增长的猜想；需要更严格的增长阶以揭示支持大小的真实规模。

Method: 量化容量达到输出分布在幅度约束内部接近均匀的特性；引入 wrapping 将问题映射到紧致域；建立用有限高斯混合物近似均匀分布的最佳近似理论；结合容量达到分布的稳定性性质，导出支持大小下界。

Result: 得到新的下界：容量达到分布的支持大小至少为 O(A sqrt(log A))，比先前线性下界有显著提升；通过把输出近似为均匀并用高斯混合物实现逼近，给出可解析的下界过程。

Conclusion: 线性增长并非最优下界，实际增长至少是 A sqrt(log A)；wrap 操作与有限高斯混合近似提供了分析受限通道离散化结构的有效框架。

Abstract: We study the amplitude-constrained additive white Gaussian noise channel. It is well known that the capacity-achieving input distribution for this channel is discrete and supported on finitely many points. The best known bounds show that the support size of the capacity-achieving distribution is lower-bounded by a term of order $A$ and upper-bounded by a term of order $A^2$, where $A$ denotes the amplitude constraint. It was conjectured in [1] that the linear scaling is optimal. In this work, we establish a new lower bound of order $A\sqrt{\log A}$, improving the known bound and ruling out the conjectured linear scaling.
  To obtain this result, we quantify the fact that the capacity-achieving output distribution is close to the uniform distribution in the interior of the amplitude constraint. Next, we introduce a wrapping operation that maps the problem to a compact domain and develop a theory of best approximation of the uniform distribution by finite Gaussian mixtures. These approximation bounds are then combined with stability properties of capacity-achieving distributions to yield the final support-size lower bound.

</details>


### [37] [Iterative Channel Estimation, Detection and Decoding for Multi-Antenna Systems with RIS](https://arxiv.org/abs/2512.22731)
*Roberto C. G. Porto,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: 提出 ICEDD 框架及 ICCE/ICT 的 RIS 辅助多用户 MIMO 上行通道估计与解码，降低 Pilot 开销并提升 NMSE；给出复杂度与码率影响分析，并通过仿真验证。


<details>
  <summary>Details</summary>
Motivation: 在 RIS 辅助的多用户多天线系统中，通道估计难度高、Pilot 开销大，需结合编码与时域跟踪以提升估计精度并降低成本。

Method: 提出 ICEDD 框架；开发 ICCE 使用 LDPC 编码的编码型 pilot 进行迭代估计；EP 使得 pilot 与纠错信息共同参与估计；提出 ICT 利用通道的时间相关性进行跟踪；给出复杂度和码率影响的解析与设计准则。

Result: 给出基于 NMSE 的解析评估与复杂度分析；数值结果验证在子-6 GHz 的多 RIS、非稀疏传播下，包含 LOS 与 NLOS 的场景，以及不同 RIS 架构的鲁棒性。

Conclusion: ICEDD 框架通过 ICCE 与 ICT 实现更低 pilot 开销和更高估计/检测性能，具备在 RIS 辅助多用户系统中的实际可行性与广泛适用性。

Abstract: This work proposes an iterative channel estimation, detection and decoding (ICEDD) scheme for the uplink of multi-user multi-antenna systems assisted by multiple reconfigurable intelligent surfaces (RIS)}. A novel iterative code-aided channel estimation (ICCE) technique is developed that uses low-density parity-check (LDPC) codes and iterative processing to enhance estimation accuracy while reducing pilot overhead. The core idea is to exploit encoded pilots (EP), enabling the use of both pilot and parity bits to iteratively refine channel estimates. To further improve performance, an iterative channel tracking (ICT) method is proposed that takes advantage of the temporal correlation of the channel. An analytical evaluation of the proposed estimator is provided in terms of normalized mean-squared error (NMSE), along with a study of its computational complexity and the impact of the code rate. Numerical results validate the performance of the proposed scheme in a sub-6 GHz multi-RIS scenario with non-sparse propagation, under both LOS and NLOS conditions, and different RIS architectures.

</details>


### [38] [Beyond Beam Sweeping: One-Shot Satellite Acquisition with Doppler-Aware Rainbow Beamforming](https://arxiv.org/abs/2512.22828)
*Juha Park,Ian P. Roberts,Wonjae Shin*

Main category: cs.IT

TL;DR: 提出一种基于彩虹波束成形的单-shot卫星获取框架，通过利用波束畴和多普勒效应将频率相关波束方向与卫星位置对齐，实现对多颗LEO卫星的同时接收，避免传统时域扫频；给出三种多普勒感知的角度估计算法；仿真显示在获取精度和所需时隙上显著优于扫频方法。


<details>
  <summary>Details</summary>
Motivation: LEO卫星通信的高路径损耗要求高增益波束，同时对卫星姿态与位置信息的准确获取造成额外开销。传统的时间域波束扫频方法成本高、时延大，亟需更高效的卫星获取方案。

Method: 提出闭式的彩虹波束成形，将波束畴效应用于将不同频率上的波束指向与通过多普勒得到的卫星位置对应；开发三种基于接收信号的多普勒感知角度估计算法；实现单次导频即可覆盖全角域并实现对多颗卫星的接收。

Result: 仿真结果显示，与传统扫频相比，该方法在获取精度与所需时隙方面具有显著提升，得益于利用多普勒-角度的耦合特性实现全角域覆盖的彩虹波束。

Conclusion: 通过将多普勒效应与波束畴效应转化为有利因素，彩虹波束成形实现了一次性卫星获取，支持多卫星同时对接并显著降低获取开销。

Abstract: High-gain beamforming (BF) is essential for low Earth orbit (LEO) satellite communications to overcome severe path loss, but this requires acquiring precise satellite positions. Conventional satellite acquisition typically relies on time-domain beam sweeping, which incurs substantial overhead and latency. In this correspondence, we propose an efficient one-shot satellite acquisition framework that capitalizes on two phenomena traditionally regarded as impairments: i) Doppler effects and ii) beam-squint effects. Specifically, we derive a closed-form \emph{rainbow beamformer} that leverages beam-squint effects to align frequency-dependent beam directions with satellite positions inferred from their Doppler shifts. This approach enables reception from multiple satellites at once without requiring beam sweeping. To extract satellite position information, we develop three Doppler-aware angle estimation algorithms based on received signals. Simulation results demonstrate that the proposed method significantly outperforms conventional beam sweeping approaches in both acquisition accuracy and required time slots. These gains stem from the ability of the proposed rainbow BF to exploit the \emph{angle-dependent nature of Doppler shifts}, enabling full angular-domain coverage with a single pilot transmission and reception.

</details>


### [39] [Covering in Hamming and Grassmann Spaces: New Bounds and Reed--Solomon-Based Constructions](https://arxiv.org/abs/2512.22911)
*Samin Riasat,Hessam Mahdavifar*

Main category: cs.IT

TL;DR: 统一框架下的覆盖问题：在Hamming与Grassmann空间中将覆盖视为量化，提出平均覆盖半径并给出一手非渐近随机编码界限，辅以基于打孔的GRS/CRS编码构造，展示代数结构在平均覆盖中的优势。


<details>
  <summary>Details</summary>
Motivation: 旨在理解高维度空间中的覆盖/量化问题，通过把覆盖转化为量化，并结合一手速率失真理论，寻找在非极限、非随机情形下的基准与构造方法，提升平均覆盖性能的理解与实现能力。

Method: 将覆盖问题视作广义度量空间中的量化，定义平均覆盖半径；利用一次性速率失真理论推导Hamming与Grassmann空间的非渐近随机编码界限；提出打孔式覆盖算法，针对GRS码在Hamming空间，以及扩展为Grassmann下的CRS码；通过数值实验比较结构码与随机码在平均覆盖上的表现。

Result: 给出两种空间的显式非渐近随机编码界限；GRS与CRS等结构码在平均覆盖方面表现出色，常优于随机码本；在Hamming空间，RS基构造通常超越随机码的平均覆盖半径；在1维Grassmann空间，CRS码对素域的构造在高速率下渐近接近随机编码界限。

Conclusion: 代数结构在覆盖问题的平均性能中具有显著优势，尤其在高维与高率场景下；尽管最坏情形覆盖可能较差，平均覆盖表现可通过结构化编码显著提升，提供新的见解和基准。

Abstract: We study covering problems in Hamming and Grassmann spaces through a unified coding-theoretic and information-theoretic framework. Viewing covering as a form of quantization in general metric spaces, we introduce the notion of the average covering radius as a natural measure of average distortion, complementing the classical worst-case covering radius. By leveraging tools from one-shot rate-distortion theory, we derive explicit non-asymptotic random-coding bounds on the average covering radius in both spaces, which serve as fundamental performance benchmarks.
  On the construction side, we develop efficient puncturing-based covering algorithms for generalized Reed--Solomon (GRS) codes in the Hamming space and extend them to a new family of subspace codes, termed character-Reed--Solomon (CRS) codes, for Grassmannian quantization under the chordal distance. Our results reveal that, despite poor worst-case covering guarantees, these structured codes exhibit strong average covering performance. In particular, numerical results in the Hamming space demonstrate that RS-based constructions often outperform random codebooks in terms of average covering radius. In the one-dimensional Grassmann space, we numerically show that CRS codes over prime fields asymptotically achieve average covering radii within a constant factor of the random-coding bound in the high-rate regime. Together, these results provide new insights into the role of algebraic structure in covering problems and high-dimensional quantization.

</details>


### [40] [Generalized Hyperderivative Reed-Solomon Codes](https://arxiv.org/abs/2512.22948)
*Mahir Bilen Can,Benjamin Horowitz*

Main category: cs.IT

TL;DR: GHRS 码是一种推广的 Reed-Solomon 码，泛化了 NRT Reed-Solomon 码；其核心性质包括：所有 GHRS 码都是 MDS，GHRS 的对偶码仍为 GHRS，存在若干子族使得成员是 LDPC；还有一族 GHRS 码的成员是准循环码；并且存在同时具有上述所有性质的 GHRS 码。


<details>
  <summary>Details</summary>
Motivation: 在保持高效纠错能力（MDS 性质）的同时，扩展 Reed-Solomon 的适用范围，引入广义超微分算子以构造更丰富的码族，使得对偶性、LDPC-性和准循环性等结构属性可以共存或组合。

Method: 引入 Generalized Hyperderivative Reed-Solomon (GHRS) 的构造框架，基于广义超微分算子定义码，给出其参数与生成矩阵，并证明：1) GHRS 全部具有 MDS 性质，2) GHRS 的对偶码仍然是 GHRS，3) 指定子族可得到 LDPC-性质的 GHRS 子族，4) 给出一族使成员为准循环码的 GHRS，并指出存在同时具备这些性质的码族。

Result: 得到的结论包括：GHRS 码全体为 MDS，GHRS 的对偶也是 GHRS；存在若干 GHRS 子族具有 LDPC 属性；存在一族 GHRS 能够生成准循环码；并且存在同时具备上述多项性质的 GHRS 码。

Conclusion: GHRS 提供了一个统一且丰富的码族框架，在保持 MDS 的同时兼容对偶性、LDPC 和准循环性等结构特性，显示出对实际通信系统的潜在应用价值与研究深入空间。

Abstract: This article introduces Generalized Hyperderivative Reed-Solomon codes (GHRS codes), which generalize NRT Reed-Solomon codes. Its main results are as follows: 1) every GHRS code is MDS, 2) the dual of a GHRS code is also an GHRS code, 3) determine subfamilies of GHRS codes whose members are low-density parity-check codes (LDPCs), and 4) determine a family of GHRS codes whose members are quasi-cyclic. We point out that there are GHRS codes having all of these properties.

</details>


### [41] [User-Centric Cell-Free Massive MIMO Enhanced by Fluid-Antenna Access Points: Uplink Analysis](https://arxiv.org/abs/2512.23046)
*Maryam Olyaee,Giovanni Interdonato,Stefano Buzzi*

Main category: cs.IT

TL;DR: Proposes a fluid-antenna (FA) enabled cell-free massive MIMO framework with LMMSE-based uplink channel estimation, port selection, and SE optimization to exploit FA reconfigurability and spatial correlation for improved sum SE.


<details>
  <summary>Details</summary>
Motivation: To enhance channel estimation accuracy and uplink spectral efficiency in cell-free mMIMO by leveraging reconfigurable fluid antennas and port selection under practical training constraints.

Method: 1) Develop a generalized LMMSE uplink channel estimation scheme with dynamic FA port activation during pilot transmission. 2) Design a distributed port selection strategy leveraging spatial correlation among FA ports to minimize estimation error. 3) Analyze antenna geometry and spatial correlation using Jakes’ model for uniform linear and planar AP arrays. 4) Derive SINR expressions for centralized and distributed uplink processing and obtain a closed-form uplink SE bound for centralized MRC (use-and-then-forget). 5) Propose an alternating-optimization framework to select FA port configurations that maximize uplink sum SE.

Result: The proposed FA-aware estimation and port optimization significantly reduce channel estimation error and substantially improve the uplink sum SE compared with fixed-antenna and non-optimized FA baselines.

Conclusion: FAs are a key enabler for scalable, adaptive CF-mMIMO networks, enabling improved channel estimation and higher sum SE through coordinated port selection and FA-aware processing.

Abstract: In this paper, we investigate cell-free massive MIMO (CF-mMIMO) systems in which access points (APs) are equipped with fluid antennas (FAs) and develop a comprehensive framework for channel estimation, antenna port selection, and uplink spectral efficiency (SE) optimization. We propose a generalized LMMSE-based uplink channel estimation scheme that dynamically activates FA ports during pilot transmission, efficiently exploiting antenna reconfigurability under practical training constraints. Building on this, we design a distributed port selection strategy that minimizes per-AP channel estimation error by exploiting spatial correlation among FA ports. We systematically analyze the impact of antenna geometry and spatial correlation using the Jakes' channel model for different AP array configurations, including uniform linear and planar arrays. We then derive SINR expressions for centralized and distributed uplink processing and obtain a closed-form uplink SE expression for centralized maximum-ratio combining using the use-and-then-forget bound. Finally, we propose an alternating-optimization framework to select FA port configurations that maximize the uplink sum SE. Numerical results show that the proposed FA-aware channel estimation and port optimization strategies greatly reduce channel estimation error and significantly improve sum-SE over fixed-antenna and non-optimized FA baselines, confirming FAs as a key enabler for scalable, adaptive CF-mMIMO networks.

</details>


### [42] [A New Family of Binary Sequences via Elliptic Function Fields over Finite Fields of Odd Characteristics](https://arxiv.org/abs/2512.23194)
*Xiaofeng Liu,Jun Zhang,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 在奇特征下，基于循环椭圆函数场的二进制序列构造得到一个新的参数族：长度为 q+1+t，大小为 q^{d-1}-1，且对齐、相关和线性复杂度有明确上界（平衡性、相关性和线性复杂度的界限）与对 d 的依赖。


<details>
  <summary>Details</summary>
Motivation: 受到 Jin 等人在 IEEE Trans. Inf. Theory 71(8), 2025 的循环椭圆函数场二进制序列构造的启发，研究在奇特征下使用二次剩余映射 η 替代迹映射来扩展该构造。

Method: 在给定的循环椭圆函数场拥有 q+1+t 个有理点，且对任意正整型 d 满足 gcd(d, q+1+t)=1 的条件下，利用二次剩余映射 η 构造新的二进制序列族。序列长度为 q+1+t，大小为 q^{d-1}-1。对平衡性给出上界：(d+1)·⌊2√q⌋+|t|+d；对相关性给出上界：(2d+1)·⌊2√q⌋+|t|+2d；对线性复杂度给出下界：((q+1+2t-d-(d+1)·⌊2√q⌋)/(d+d·⌊2√q⌋))。

Result: 得到一个新的二进制序列族，其长度、大小与上界均受 q、t、d 的控制，提供了在奇特征下的广义扩展。

Conclusion: 将奇特征下的循环椭圆函数场序列构造从原有的基于迹映射的方案推广到二次剩余映射方案，给出明确的平衡、相关与线性复杂度的界，并对 d 的取值作了广义约束。

Abstract: Motivated by the constructions of binary sequences by utilizing the cyclic elliptic function fields over the finite field $\mathbb{F}_{2^{n}}$ by Jin \textit{et al.} in [IEEE Trans. Inf. Theory 71(8), 2025], we extend the construction to the cyclic elliptic function fields with odd characteristic by using the quadratic residue map $η$ instead of the trace map used therein. For any cyclic elliptic function field with $q+1+t$ rational points and any positive integer $d$ with $\gcd(d, q+1+t)=1$, we construct a new family of binary sequences of length $q+1+t$, size $q^{d-1}-1$, balance upper bounded by $(d+1)\cdot\lfloor2\sqrt{q}\rfloor+|t|+d,$ the correlation upper bounded by $(2d+1)\cdot\lfloor2\sqrt{q}\rfloor+|t|+2d$ and the linear complexity lower bounded by $\frac{q+1+2t-d-(d+1)\cdot\lfloor2\sqrt{q}\rfloor}{d+d\cdot\lfloor2\sqrt{q}\rfloor}$ where $\lfloor x\rfloor$ stands for the integer part of $x\in\mathbb{R}$.

</details>


### [43] [Information Inequalities for Five Random Variables](https://arxiv.org/abs/2512.23316)
*E. P. Csirmaz,L. Csirmaz*

Main category: cs.IT

TL;DR: 通过改进的最大熵方法及变量复制，界定五变量的熵区域，给出两类无限族的非 Shannon 不等式和一个极值不等式枚举算法，并猜测该集合能够完全描述所用方法。


<details>
  <summary>Details</summary>
Motivation: 五变量熵区域的结构远比四个及以下变量复杂，尚未完全被理解，需要新的方法来界定其边界和不等式集合。

Method: 使用变体的最大熵方法，通过在生成中复制随机变量来扩展维度，结合对称性来显著降低计算复杂度；计算前九代的所有五变量非 Shannon 不等式；定义两无限族不等式并证实其为熵不等式；研究向下封闭的非负格点子集以参数化这些族，提出枚举极值不等式的算法。

Result: 获得了五变量的所有来自前九代的非 Shannon 不等式；提出两类无限族的熵不等式并证实其为熵不等式；实现一个枚举极值不等式的算法；初步工作表明该方法的获得的不等式集合有可能完全描述所用方法的边界。

Conclusion: 所发现的熵不等式集合被认为能够完全表征所用的最大熵方法在五变量情形下的边界，方法学上显著利用对称性和复制变量的思想。

Abstract: The entropic region is formed by the collection of the Shannon entropies of all subvectors of finitely many jointly distributed discrete random variables. For four or more variables the structure of the entropic region is mostly unknown. We utilize a variant of the Maximum Entropy Method to delimit the five-variable entropy region. This method adds copies of some of the random variables in generations. A significant reduction in computational complexity, achieved through theoretical considerations and by harnessing the inherent symmetries, allowed us to calculate all five-variable non-Shannon inequalities provided by the first nine generations. Based on the results, we define two infinite collections of such inequalities, and prove them to be entropy inequalities. We investigate downward closed subsets of non-negative lattice points that parameterize these collections, based on which we develop an algorithm to enumerate all extremal inequalities. The discovered set of entropy inequalities is conjectured to characterize the applied method completely.

</details>


### [44] [Dynamic Channel Knowledge Map Construction in MIMO-OFDM Systems](https://arxiv.org/abs/2512.23470)
*Wenjun Jiang,Xiaojun Yuan,Chenchen Liu,Boyu Teng*

Main category: cs.IT

TL;DR: 提出一种面向MIMO-OFDM的动态CKM构建方法，基于两阶段近似贝叶斯推断，结合准静态/动态散射体、天线旋转与同步误差，在动态环境中实现低开销、高性能的信道估计。


<details>
  <summary>Details</summary>
Motivation: 现有CKM多聚焦准静态传播环境，缺乏对动态散射体、天线旋转以及同步误差等现实因素的建模，亟需在动态场景中仍能实现环境感知和高效信道映射的方法。

Method: 建立能够同时捕捉准静态与动态散射体、天线旋转和同步误差的动态信道模型；在贝叶斯框架下设计两阶段近似推断算法。阶段I并行高效算法，利用历史观测数据联合推断准静态信道参数并校准同步误差；阶段II以准静态参数为信息先验，利用有限的实时观测估计动态参数，降低计算复杂度。

Result: 仿真结果验证了所提方法的有效性，能够在动态环境中实现低开销的高性能信道估计，提升CKM的实用性。

Conclusion: 提出了一种可在动态CKM中使用的两阶段贝叶斯推断框架，适用于MIMO-OFDM系统，提升在动态环境中的环境感知与信道映射的准确性与实时性。

Abstract: Channel knowledge map (CKM) is a promising paradigm for environment-aware communications by establishing a deterministic mapping between physical locations and channel parameters. Existing CKM construction methods focus on quasi-static propagation environment. This paper develops a dynamic CKM construction method for multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) systems. We establish a dynamic channel model that captures the coexistence of quasi-static and dynamic scatterers, as well as the impacts of antenna rotation and synchronization errors. Based on this model, we formulate the problem of dynamic CKM construction within a Bayesian inference framework and design a two-stage approximate Bayesian inference algorithm. In stage I, a high-performance algorithm is developed to jointly infer quasi-static channel parameters and calibrate synchronization errors from historical measurements. In stage II, by leveraging the quasi-static parameters as informative priors, a low-complexity algorithm is designed to estimate dynamic parameters from limited real-time measurements. Simulation results validate the superiority of the proposed method and demonstrate its effectiveness in enabling low-overhead, high-performance channel estimation in dynamic environments.

</details>


### [45] [Affine-Projection Recovery of Continuous Angular Power Spectrum: Geometry and Resolution](https://arxiv.org/abs/2512.23506)
*Shengsong Luo,Ruilin Wu,Chongbin Xu,Junjie Ma,Xiaojun Yuan,Xin Wang*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper considers recovering a continuous angular power spectrum (APS) from the channel covariance. Building on the projection-onto-linear-variety (PLV) algorithm, an affine-projection approach introduced by Miretti \emph{et. al.}, we analyze PLV in a well-defined \emph{weighted} Fourier-domain to emphasize its geometric interpretability. This yields an explicit fixed-dimensional trigonometric-polynomial representation and a closed-form solution via a positive-definite matrix, which directly implies uniqueness. We further establish an exact energy identity that yields the APS reconstruction error and leads to a sharp identifiability/resolution characterization: PLV achieves perfect recovery if and only if the ground-truth APS lies in the identified trigonometric-polynomial subspace; otherwise it returns the minimum-energy APS among all covariance-consistent spectra.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses](https://arxiv.org/abs/2512.22128)
*Yongyu Wang*

Main category: cs.LG

TL;DR: 提出一种基于对抗鲁棒性评估的图剪枝框架，通过剪除对模型鲁棒性有负面影响的边，提升 GNN 在高扰动下的鲁棒性。该方法在三种典型 GNN 架构上实现并在基准数据集上进行大量实验，结果显示显著增强防御能力。


<details>
  <summary>Details</summary>
Motivation: GNNs 在结构或特征存在扰动时易被放大，导致对抗攻击与噪声连接的脆弱性。需要一种系统化的方法来识别并移除对鲁棒性有害的图组成部分。

Method: 提出一个剪枝框架，利用对抗鲁棒性评估得到的鲁棒性分数来识别并剪除最可能降低模型可靠性的边或结构组件。该框架在三个代表性 GNN 架构上实现，并在多个基准数据集上进行评估。

Result: 通过基于鲁棒性分数的边剪枝，得到更干净、更具鲁棒性的图表示；在高扰动场景下，显著提升了 GNN 的防御能力。

Conclusion: 以鲁棒性为导向的剪枝框架可有效提升 GNN 的对抗鲁棒性与鲁棒性，在不同架构和基准数据集上具有广泛适用性。

Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for learning on graph-structured data, thanks to their ability to jointly exploit node features and relational information encoded in the graph topology. This joint modeling, however, also introduces a critical weakness: perturbations or noise in either the structure or the features can be amplified through message passing, making GNNs highly vulnerable to adversarial attacks and spurious connections. In this work, we introduce a pruning framework that leverages adversarial robustness evaluation to explicitly identify and remove fragile or detrimental components of the graph. By using robustness scores as guidance, our method selectively prunes edges that are most likely to degrade model reliability, thereby yielding cleaner and more resilient graph representations. We instantiate this framework on three representative GNN architectures and conduct extensive experiments on benchmarks. The experimental results show that our approach can significantly enhance the defense capability of GNNs in the high-perturbation regime.

</details>


### [47] [Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders](https://arxiv.org/abs/2512.22150)
*Hans Jarett J. Ong,Brian Godwin S. Lim,Dominic Dayta,Renzo Roel P. Tan,Kazushi Ikeda*

Main category: cs.LG

TL;DR: LANCA 提出了一种基于加性噪声模型的因果自编码器，用确定性WAE+ANM层在无监督设置中实现潜在因果因素的分解，并将残差独立性转换为优化目标，从而使潜在因子在组件层面被识别（在仿真到真实世界环境中优于最新方法，且对背景混淆具有鲁棒性）。


<details>
  <summary>Details</summary>
Motivation: 无监督表示学习面临的可 identifiability 问题：仅靠统计独立性往往无法揭示潜在的因果变量，缺乏监督或强偏好时很难实现因果分解。ANM 提供强的先验偏置，但仅在一般混合下未必实现唯一可识别性；需要方法以提升潜在因果分解的可识别性。

Method: 提出 LANCA：在确定性 WAE 框架上嵌入可微分的 ANM 层，将残差独立性作为显式优化目标；理论上证明 ANM 约束在一般混合情形下不保证唯一可识别性，但能将自由变换从任意微分同胚缩小到仿射变换类，从而解决组件层面的不确定性；在实践中，使用确定性编码避免 VAE 的随机性掩盖结构残差。

Result: 在合成物理任务（摆动 Pendulum、流体 Flow）以及真实感环境（CANDLE）上，LANCA 超越现有基线，且对复杂背景场景引入的伪相关具有更强鲁棒性。

Conclusion: 通过将 ANM 作为强诱导偏置，LANCA 实现了潜在因果因素的更强识别能力并提高对噪声与背景混淆的鲁棒性；但可识别性仍限定在仿射变换类，需要更深入理解在更一般设定中的限度。

Abstract: Unsupervised representation learning seeks to recover latent generative factors, yet standard methods relying on statistical independence often fail to capture causal dependencies. A central challenge is identifiability: as established in disentangled representation learning and nonlinear ICA literature, disentangling causal variables from observational data is impossible without supervision, auxiliary signals, or strong inductive biases. In this work, we propose the Latent Additive Noise Model Causal Autoencoder (LANCA) to operationalize the Additive Noise Model (ANM) as a strong inductive bias for unsupervised discovery. Theoretically, we prove that while the ANM constraint does not guarantee unique identifiability in the general mixing case, it resolves component-wise indeterminacy by restricting the admissible transformations from arbitrary diffeomorphisms to the affine class. Methodologically, arguing that the stochastic encoding inherent to VAEs obscures the structural residuals required for latent causal discovery, LANCA employs a deterministic Wasserstein Auto-Encoder (WAE) coupled with a differentiable ANM Layer. This architecture transforms residual independence from a passive assumption into an explicit optimization objective. Empirically, LANCA outperforms state-of-the-art baselines on synthetic physics benchmarks (Pendulum, Flow), and on photorealistic environments (CANDLE), where it demonstrates superior robustness to spurious correlations arising from complex background scenes.

</details>


### [48] [Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks](https://arxiv.org/abs/2512.22186)
*Vishnu Mohan*

Main category: cs.LG

TL;DR: A reinforcement learning framework for tennis strategy using a custom simulator and a dueling double DQN with curriculum learning, achieving high win rates and exposing biases due to reward optimization.


<details>
  <summary>Details</summary>
Motivation: Tennis strategy optimization presents a challenging, long-horizon sequential decision problem with hierarchical scoring, fatigue dynamics, and opponent adaptation; robust, stable learning methods are needed beyond standard RL baselines.

Method: A custom tennis environment modeling points, games, sets, rally-level decisions across 10 actions, fatigue, and continuous opponent skill; a dueling DDQN trained with curriculum learning that escalates opponent difficulty from 0.40 to 0.50; ablation studies and comparisons to a standard DQN baseline.

Result: The agent attains 98–100% win rates against balanced opponents and remains strong against tougher opponents; serve efficiency 63.0–67.5%, return 52.8–57.1%; both dueling architecture and curriculum learning are necessary for stable learning, while standard DQN fails; the policy shows a defensive bias prioritizing error avoidance and rallies over aggressive point construction.

Conclusion: Win-rate optimization in simplified sports RL can be misleading; reward design critically shapes realistic behavior, highlighting the need for balanced objectives and richer evaluation beyond pure win rate.

Abstract: Tennis strategy optimization is a challenging sequential decision-making problem involving hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and adaptation to opponent skill. I present a reinforcement learning framework that integrates a custom tennis simulation environment with a Dueling Double Deep Q-Network(DDQN) trained using curriculum learning. The environment models complete tennis scoring at the level of points, games, and sets, rally-level tactical decisions across ten discrete action categories, symmetric fatigue dynamics, and a continuous opponent skill parameter. The dueling architecture decomposes action-value estimation into state-value and advantage components, while double Q-learning reduces overestimation bias and improves training stability in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty from 0.40 to 0.50, enabling robust skill acquisition without the training collapse observed under fixed opponents. Across extensive evaluations, the trained agent achieves win rates between 98 and 100 percent against balanced opponents and maintains strong performance against more challenging opponents. Serve efficiency ranges from 63.0 to 67.5 percent, and return efficiency ranges from 52.8 to 57.1 percent. Ablation studies demonstrate that both the dueling architecture and curriculum learning are necessary for stable convergence, while a standard DQN baseline fails to learn effective policies. Despite strong performance, tactical analysis reveals a pronounced defensive bias, with the learned policy prioritizing error avoidance and prolonged rallies over aggressive point construction. These results highlight a limitation of win-rate driven optimization in simplified sports simulations and emphasize the importance of reward design for realistic sports reinforcement learning.

</details>


### [49] [Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks](https://arxiv.org/abs/2512.22192)
*Jiahao Lu*

Main category: cs.LG

TL;DR: 提出通过可视化诊断框架与谱抑制比(SSR)量化正则化对特征频率的选择性影响，发现L2正则化显著抑制高频能量积累，同时存在准确性-鲁棒性权衡；从信号处理角度解释正则化偏好低频结构的泛化机制。


<details>
  <summary>Details</summary>
Motivation: 理解正则化（如L2与Dropout）背后的物理机制，特别是它们如何影响CNN在训练过程中的特征频率选择，以及这对泛化与鲁棒性的影响。通过谱分析构建对正则化的直观解释与定量指标。

Method: 提出视觉诊断框架，动态追踪权重频率的演化；引入谱抑制比(SSR)用于量化不同正则izer的“低通滤波”强度；通过离散径向分析解决3x3等小核在采样下的混叠问题；在ResNet-18与CIFAR-10上比较L2、Dropout等正则化对谱能量的影响。

Result: 实验显示：L2正则使高频能量积累比未正则化基线降低>3倍；SSR揭示不同正则对谱的抑制程度；在模糊/低分辨率场景中，L2模型对高频信息丢失更具鲁棒性，优于对比基线>6%；另一方面，L2对宽带高斯噪声较为敏感，源于对低频的过度专化。

Conclusion: 正则化通过对频率的强烈偏好（谱低通特性）影响模型泛化与鲁棒性；从信号处理角度提供了对正则化效果的直观解释，强化了低频结构在深度学习泛化中的作用。

Abstract: Regularization techniques such as L2 regularization (Weight Decay) and Dropout are fundamental to training deep neural networks, yet their underlying physical mechanisms regarding feature frequency selection remain poorly understood. In this work, we investigate the Spectral Bias of modern Convolutional Neural Networks (CNNs). We introduce a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during training and propose a novel metric, the Spectral Suppression Ratio (SSR), to quantify the "low-pass filtering" intensity of different regularizers. By addressing the aliasing issue in small kernels (e.g., 3x3) through discrete radial profiling, our empirical results on ResNet-18 and CIFAR-10 demonstrate that L2 regularization suppresses high-frequency energy accumulation by over 3x compared to unregularized baselines. Furthermore, we reveal a critical Accuracy-Robustness Trade-off: while L2 models are sensitive to broadband Gaussian noise due to over-specialization in low frequencies, they exhibit superior robustness against high-frequency information loss (e.g., low resolution), outperforming baselines by >6% in blurred scenarios. This work provides a signal-processing perspective on generalization, confirming that regularization enforces a strong spectral inductive bias towards low-frequency structures.

</details>


### [50] [A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers](https://arxiv.org/abs/2512.23380)
*Mohammad Nasirzadeh,Jafar Tahmoresnezhad,Parviz Rashidi-Khazaee*

Main category: cs.LG

TL;DR: A multimodal log anomaly detection framework, CoLog, jointly encodes multiple log modalities using collaborative transformers, multi-head impressed attention, and a modality adaptation layer to model cross-modal interactions, achieving state-of-the-art results on seven benchmarks for point and collective anomalies.


<details>
  <summary>Details</summary>
Motivation: Logs collected from diverse sources yield multiple modalities. Unimodal methods miss cross-modal cues; naive multimodal methods fail to exploit interactions. There is a need for a framework that explicitly encodes multiple modalities and their interactions to improve anomaly detection.

Method: CoLog employs collaborative transformers to encode modalities, multi-head impressed attention to capture cross-modal interactions, and a modality adaptation layer to align heterogeneous representations. The framework learns nuanced inter-modal patterns to detect anomalies, including both point and collective anomalies. Implementation is provided in GitHub.

Result: Reported mean precision 99.63%, recall 99.59%, and F1 99.61% across seven benchmark datasets for log-based anomaly detection, outperforming state-of-the-art methods.

Conclusion: CoLog advances log anomaly detection by integrating multimodal information through interaction-aware encoding and adaptation, offering robust detection for point and collective anomalies and practical applicability in cybersecurity and system monitoring.

Abstract: Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.

</details>


### [51] [On the Existence and Behaviour of Secondary Attention Sinks](https://arxiv.org/abs/2512.22213)
*Jeffrey T. H. Wong,Cheng Zhang,Louis Mahon,Wayne Luk,Anton Isopoussu,Yiren Zhao*

Main category: cs.LG

TL;DR: 提出并分析了注意力中的次级汇聚点（secondary sinks），它们在中间层出现、由特定MLP模块映射产生，其强度由向量的l2范数决定，并与主sink的弱化及模型规模相关，表现为多个可重复的“sink level”在不同模型中出现。


<details>
  <summary>Details</summary>
Motivation: 揭示注意力机制中的新型汇聚现象，区分已有的主sink（如BOS）和新发现的次级sinks，以及它们的形成机制、持续时间与对注意力分布的影响。

Method: 在11个模型家族上进行广泛实验，识别并追踪sinks的位置、特征及生命周期；分析中层MLP对向量方向的映射、l2范数与sink分数的关系；比较主sink在中间层的变化，并给出在不同模型中的sink level（如QwQ-32B的3个、Qwen3-14B的6个）。

Result: 证明存在中间层的二级sinks，具有较小但显著的注意力占比；它们由特定中层MLP模块形成、并使对应的主sink在同层的影响减弱；l2范数决定sink分数及持续层数；大模型中sink的位置与寿命更具确定性，呈现多层级的sink level。

Conclusion: 次级sinks丰富了对注意力分布的理解，提示需要将层内非主导信息的潜在作用纳入解释与设计考量，可能引导对模型架构与正则化的改进；未来工作可探索缓解或利用这些sinks以提升可解释性和稳健性。

Abstract: Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.

</details>


### [52] [Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning](https://arxiv.org/abs/2512.22221)
*Soroush Vahidi*

Main category: cs.LG

TL;DR: 提出一种可解释且自适应的半监督节点分类框架，基于显式组合推理（而非深度信息传播），在同质性/异质性图中实现可解释性、可控性和高效性，并通过可选的混合策略提升性能，同时避免数据泄露。


<details>
  <summary>Details</summary>
Motivation: GNN在异质性图上往往性能下降，需更透明、可控且高效的分类方法。

Method: 使用一个基于置信度的贪心排序的组合推理框架，利用加性打分函数融合先验类概率、邻域统计、特征相似性和训练得到的标签-标签兼容性；通过少量超参数调控不同分布情形。引入验证门控的混合策略，将组合预测作为先验注入轻量神经模型；仅在验证提升时再进行神经化；所有信号来源于训练数据，避免数据泄露。

Result: 在异质性和过渡数据集上与现代GNNs竞争，且在可解释性、可调性和计算效率上具有优势。

Conclusion: 提出的框架兼具可解释性和自适应能力，适用于不同图同质性场景，且具有较低计算成本与更强的可控性；通过混合策略进一步提升表现，同时保证评估的严格性。

Abstract: Graph neural networks (GNNs) achieve strong performance on homophilic graphs but often struggle under heterophily, where adjacent nodes frequently belong to different classes. We propose an interpretable and adaptive framework for semi-supervised node classification based on explicit combinatorial inference rather than deep message passing. Our method assigns labels using a confidence-ordered greedy procedure driven by an additive scoring function that integrates class priors, neighborhood statistics, feature similarity, and training-derived label-label compatibility. A small set of transparent hyperparameters controls the relative influence of these components, enabling smooth adaptation between homophilic and heterophilic regimes.
  We further introduce a validation-gated hybrid strategy in which combinatorial predictions are optionally injected as priors into a lightweight neural model. Hybrid refinement is applied only when it improves validation performance, preserving interpretability when neuralization is unnecessary. All adaptation signals are computed strictly from training data, ensuring a leakage-free evaluation protocol. Experiments on heterophilic and transitional benchmarks demonstrate competitive performance with modern GNNs while offering advantages in interpretability, tunability, and computational efficiency.

</details>


### [53] [DiRL: An Efficient Post-Training Framework for Diffusion Language Models](https://arxiv.org/abs/2512.22234)
*Ying Zhu,Jiaxin Wan,Xiaoran Liu,Siyanag He,Qiqi Wang,Xu Guo,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.LG

TL;DR: 提出 DiRL 后训练框架，结合 FlexAttention 的分块式训练和 LMDeploy 的推理优化，实现高效两阶段微调（监督微调+强化学习），并在此框架上提出用于 dLLMs 的无偏组相对策略优化（GRPO）的 DiPO。实证表明 DiRL-8B-Instruct 在数学推理任务上达到 dLLMs 的最新水平，并在若干基准上超过 Qwen2.5 系列模型。


<details>
  <summary>Details</summary>
Motivation: 解决 diffusion language models 在后训练阶段的计算低效和训练目标与推理目标不一致的问题，提升对复杂推理（尤其是数学任务）的性能。现有方法在后训练阶段的效率与对齐度不足，限制了模型的实际应用潜力。

Method: 提出两阶段后训练流程：先进行监督微调（SFT）再进行强化学习（RL），在此之上构建 DiRL 框架，将 FlexAttention 加速的分块训练与 LMDeploy 优化的推理高效耦合，形成高效在线模型更新循环。基于此框架，提出 DiPO，这是首个为 dLLMs 定制的无偏组相对策略优化（GRPO）实现。

Result: 在高质量数学数据上训练出 DiRL-8B-Instruct；在数学任务上实现了同类中对 dLLMs 的最佳性能，并在若干基准上超越 Qwen2.5 系列的可比模型。

Conclusion: 所提出的 DiRL 框架与 DiPO 方法有效提升了 dLLMs 在数学推理任务上的性能，同时提高了后训练阶段的效率与对齐度，为 diffusion 类语言模型的实用化提供了新的路径。

Abstract: Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.

</details>


### [54] [Trust Region Masking for Long-Horizon LLM Reinforcement Learning](https://arxiv.org/abs/2512.23075)
*Yingru Li,Jiacai Liu,Jiawei Xu,Yuxuan Tong,Ziniu Li,Baoxiang Wang*

Main category: cs.LG

TL;DR: 本文提出针对LLM-RL中训练策略与 rollout 策略不一致的 off-policy 误差，给出两种更紧的界限：Pinsker-Marginal bound 和 Mixed bound，分别随序列长度 T 的增长而达到 O(T^{3/2}) 和 O(T)，并引入 Trust Region Masking (TRM) 对任何 token 符合 trust region 的序列进行屏蔽，从而在长时序下给出非空的单调改进保证。


<details>
  <summary>Details</summary>
Motivation: 现有基于 surrogate objective 的梯度优化在 π_roll 与 π_θ 不一致时存在近似误差，且在现代 LLM-RL 中因实现差异、专家路由不连续性、分布式训练延迟等因素，这种离策略差异无法避免。传统信任域界限随 T 乘方增长且在长序列任务上无效。需要考虑序列级的 KL 发散，给出更紧的界限并实现可单调改进的策略。

Method: 推导两种新的界限：Pinsker-Marginal bound（O(T^{3/2})）和 Mixed bound（O(T)），它们依赖于最大 token-level KL 发散 D_kl^{tok,max}，这是一个序列级量。提出 TRM：如果任一 token 违背信任区域，整条序列从梯度计算中排除，从而实现对长时序 LLM-RL 的非 vacuous 单调改进保障。

Result: 得到的界限分别为 O(T^{3/2}) 与 O(T)，并首次给出对长时序 LLM-RL 的非空单调改进保证，TRM 能在不牺牲太多样本的前提下稳定提升。

Conclusion: 序列级的 KL 约束不能被单纯的 token 无关剪切直接控制，需引入对整个序列的约束与屏蔽机制，TRM 为实现长时序 LLM-RL 的稳健性与单调改进提供了新的理论与实践路径。

Abstract: Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. When $π_{\text{roll}} \ne π_θ$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.

</details>


### [55] [Masking Teacher and Reinforcing Student for Distilling Vision-Language Models](https://arxiv.org/abs/2512.22238)
*Byung-Kwan Lee,Yu-Chiang Frank Wang,Ryo Hachiuma*

Main category: cs.LG

TL;DR: 提出 Masters 框架，通过掩蔽与分阶段扩展教师容量，结合离线强化学习实现对大型视觉-语言模型的高效蒸馏，提升小模型的表示能力与学习稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决教师-学生之间巨大容量差距所带来的不稳定蒸馏和性能下降问题；在边缘设备上实现高效、可部署的视觉-语言模型蒸馏，需要更稳健的学习策略。

Method: 1) 对教师权重进行掩蔽以去除非主导权重，降低复杂性；2) 在训练过程中逐步恢复教师容量，分阶段提升教师能力以实现渐进式知识传递；3) 引入离线强化学习阶段，采用两类奖励：准确性奖励与蒸馏奖励，并利用掩蔽教师的预生成响应来指导学生，避免代价高昂的在线 think-answer 流程。

Result: 该框架旨在实现更稳定的蒸馏过程并提升学生模型性能，在无在线思考-回答过程下通过离线奖励驱动学习，预计比传统蒸馏在稳定性与表示学习能力方面更优。

Conclusion: Masters 提供一个可扩展且稳定的蒸馏框架，将掩蔽-扩展的教师策略与离线强化学习相结合，显著提升小模型对大教师的学习效率与最终表现。

Abstract: Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.

</details>


### [56] [EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs](https://arxiv.org/abs/2512.22240)
*Chama Bensmail*

Main category: cs.LG

TL;DR: EvoXplain: a diagnostic framework that assesses the stability of model explanations across repeated training to reveal multimodal explanatory mechanisms, challenging the idea of a single correct explanation and reframing interpretability as a property of a model class under repeated instantiation.


<details>
  <summary>Details</summary>
Motivation: High predictive accuracy does not guarantee a unique or trustworthy explanation. Two high-performing models may rely on different internal logic; understanding whether explanations are stable or multimodal is crucial for trustworthy ML.

Method: Treat explanations as samples drawn from the stochastic optimization process during training, without aggregating predictions or ensembling. Analyze whether these explanation samples form a single coherent mode or split into multiple distinct explanatory basins across repeated runs.

Result: On Breast Cancer and COMPAS datasets using Logistic Regression and Random Forests, explanations exhibit clear multimodality; even models like Logistic Regression can produce multiple well-separated explanatory basins under repeated training on the same data split. Differences are not explained by hyperparameter variation or simple performance trade-offs.

Conclusion: EvoXplain does not select a single 'correct' explanation; it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure multiple underlying mechanisms. Interpretability is reframed as a property of the model class under repeated instantiation rather than of a single trained model.

Abstract: Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. However, this assumption raises an overlooked question: when two models achieve high accuracy, do they rely on the same internal logic, or do they reach the same outcome via different -- and potentially competing -- mechanisms? We introduce EvoXplain, a diagnostic framework that measures the stability of model explanations across repeated training. Rather than analysing a single trained model, EvoXplain treats explanations as samples drawn from the stochastic optimisation process itself -- without aggregating predictions or constructing ensembles -- and examines whether these samples form a single coherent explanation or separate into multiple, distinct explanatory modes. We evaluate EvoXplain on the Breast Cancer and COMPAS datasets using two widely deployed model classes: Logistic Regression and Random Forests. Although all models achieve high predictive accuracy, their explanations frequently exhibit clear multimodality. Even models commonly assumed to be stable, such as Logistic Regression, can produce multiple well-separated explanatory basins under repeated training on the same data split. These differences are not explained by hyperparameter variation or simple performance trade-offs. EvoXplain does not attempt to select a 'correct' explanation. Instead, it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure the existence of multiple underlying mechanisms. More broadly, EvoXplain reframes interpretability as a property of a model class under repeated instantiation, rather than of any single trained model.

</details>


### [57] [Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening](https://arxiv.org/abs/2512.22242)
*Shaurya Gaur,Michel Vitale,Alessa Hering,Johan Kwisthout,Colin Jacobs,Lena Philipp,Fennie van der Graaf*

Main category: cs.LG

TL;DR: 本研究评估两种深度学习肺癌风险模型（Sybil、Venkadesh21）及PanCan2b基线的公平性，在NLST数据集上对不同人口统计子组进行性能评估。发现性别与种族差异在某些指标上显著，且未被已知混杂因素解释，提示需要加强对 Underrepresented 子群体的监控与公平性研究。


<details>
  <summary>Details</summary>
Motivation: 探究用于肺癌筛查的AI风险估计模型在不同人口统计群体中的表现差异及潜在偏见，确保在临床推荐前提下的公平性和可迁移性。

Method: 使用在NLST数据上训练的两种深度学习模型（Sybil、Venkadesh21）及PanCan2b逻辑回归基线，评估在持出验证集上的AUROC、敏感性、特异性，并按性别、 race/族裔等子组分组比较。还考察与临床风险因素的潜在混杂。

Result: Sybil在女性组的AUROC为0.88（95% CI 0.86–0.90），男性为0.81（95% CI 0.78–0.84），差异有统计学意义（p<0.001）。在90%特异性下，Venkadesh21对黑人组的敏感性为0.39（95% CI 0.23–0.59），对白人组为0.69（95% CI 0.65–0.73）。这些差异无法通过现有临床混杂因素解释，因此可能属于JustEFAB框架下的不公平偏差。

Conclusion: 强调需要提升与监测对 underrepresented 子群体的模型性能，并就算法公平性在肺癌筛查领域开展进一步研究。

Abstract: Lung cancer is the leading cause of cancer-related mortality in adults worldwide. Screening high-risk individuals with annual low-dose CT (LDCT) can support earlier detection and reduce deaths, but widespread implementation may strain the already limited radiology workforce. AI models have shown potential in estimating lung cancer risk from LDCT scans. However, high-risk populations for lung cancer are diverse, and these models' performance across demographic groups remains an open question. In this study, we drew on the considerations on confounding factors and ethically significant biases outlined in the JustEFAB framework to evaluate potential performance disparities and fairness in two deep learning risk estimation models for lung cancer screening: the Sybil lung cancer risk model and the Venkadesh21 nodule risk estimator. We also examined disparities in the PanCan2b logistic regression model recommended in the British Thoracic Society nodule management guideline. Both deep learning models were trained on data from the US-based National Lung Screening Trial (NLST), and assessed on a held-out NLST validation set. We evaluated AUROC, sensitivity, and specificity across demographic subgroups, and explored potential confounding from clinical risk factors. We observed a statistically significant AUROC difference in Sybil's performance between women (0.88, 95% CI: 0.86, 0.90) and men (0.81, 95% CI: 0.78, 0.84, p < .001). At 90% specificity, Venkadesh21 showed lower sensitivity for Black (0.39, 95% CI: 0.23, 0.59) than White participants (0.69, 95% CI: 0.65, 0.73). These differences were not explained by available clinical confounders and thus may be classified as unfair biases according to JustEFAB. Our findings highlight the importance of improving and monitoring model performance across underrepresented subgroups, and further research on algorithmic fairness, in lung cancer screening.

</details>


### [58] [Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors](https://arxiv.org/abs/2512.22699)
*Antar Kumar Biswas,Masoud H. Nazari*

Main category: cs.LG

TL;DR: 提出一个基于学习的框架，用于预测极端事件引发的停电，比较RF、SVM、AdaBoost和LSTM等模型；在密歇根州下半岛县级数据上验证，LSTM表现最佳；经济状况和基础设施水平与停电风险呈负相关。


<details>
  <summary>Details</summary>
Motivation: 解决低概率高后果的停电风险预测问题；通过整合公开数据源（天气、社会经济、基础设施、季节性事件等）来揭示社区脆弱性，并提升极端条件下的停电风险评估能力。

Method: 将EAGLE-I停电记录（2014-2024）与天气、社会经济、基础设施和季节性事件等特征整合，加入社会人口指标以考察社区脆弱性；比较四种机器学习模型（随机森林、支持向量机、AdaBoost、LSTM）的预测性能；在密歇根州下半岛县级大规模数据集上进行实验验证；以预测误差衡量模型表现。

Result: 在比较模型中，LSTM获得最低的预测误差；额外发现更强的经济条件和更发达的基础设施与较低的停电发生率相关。

Conclusion: 所提出的框架可用于极端条件下的停电风险评估与韧性规划，跨数据源的社会经济与基础设施因素对停电风险具有显著影响，能为决策提供量化依据。

Abstract: This paper presents a novel learning-based framework for predicting power outages caused by extreme events. The proposed approach specifically targets low-probability, high-consequence outage scenarios and leverages a comprehensive set of features derived from publicly available data sources. We integrate EAGLE-I outage records (2014-2024) with weather, socio-economic, infrastructure, and seasonal event data. Incorporating social and demographic indicators reveals underlying patterns of community vulnerability and provides a clearer understanding of outage risk during extreme conditions. Four machine learning models (Random Forest (RF), Support Vector Machine (SVM), Adaptive Boosting (AdaBoost), and Long Short-Term Memory (LSTM)) are evaluated. Experimental validation is performed on a large-scale dataset covering counties in the lower peninsula of Michigan. Among all models tested, the LSTM network achieves the lowest prediction error. Additionally, the results demonstrate that stronger economic conditions and more developed infrastructure are associated with lower outage occurrence.

</details>


### [59] [Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation](https://arxiv.org/abs/2512.22245)
*Bhaktipriya Radharapu,Eshika Saxena,Kenneth Li,Chenxi Whitehouse,Adina Williams,Nicola Cancedda*

Main category: cs.LG

TL;DR: 提出用线性探针结合基于Brier分数的损失，对LLM judge的隐藏状态进行校准不确定性估计，且无需额外模型训练；在推理、数学、事实性、编码等任务以及主观偏好评估上表现出更好地校准性和约10x的计算节省，能泛化到未见评估域，但对简单数据集较保守，安全关键部署有利。


<details>
  <summary>Details</summary>
Motivation: 在将LLM作为判定者用于生产环境时，需要高质量且高效的未确定性估计。现有方法（如口头自信度、多代际方法）要么校准差、要么计算成本高，亟需一种可用且高效的估计手段。

Method: 提出在判定者的隐藏状态上训练线性探针，使用基于Brier分数的损失来校准输出的不确定性。该探针不需要对模型本体进行额外训练，属于可插拔的解释性工具。

Result: 与现有方法相比，探针在校准性上具有优势，且计算成本约低10倍；具有对未知评估域的鲁棒泛化能力，并在高置信预测上有更高的准确性；但对简单数据集的表现较差，呈现保守估计。

Conclusion: 基于可解释性的不确定性估计为LLM judge在生产场景中的一种实用、可扩展的插拔式解决方案，适合需要低假阳性率的安全关键应用。

Abstract: As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges' hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with $\approx10$x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.

</details>


### [60] [Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback](https://arxiv.org/abs/2512.22623)
*Tomas Ortega,Chun-Yin Huang,Xiaoxiao Li,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 在FL中提出两种无客户端状态的有偏压缩框架 CAFe 与 CAFe-S，通过全局聚合更新作为共享控制变量实现无状态的误差反馈，在非凸下证明了 CAFe 相对于 DCGD 的优势，且 CAFe-S 的收敛速率随着服务端数据代表性提升而增强，实验验证优于现有压缩方案。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中上传链路的带宽瓶颈，尽管有偏压缩具有高效性，但传统误差反馈需要客户端特定的控制变差量，侵犯隐私且与无状态、广域化的FL场景不兼容，因此需要无需客户端状态的新框架。

Method: 提出两种框架：CAFe 使用上一轮全局聚合更新作为全局共享控制变差量实现偏压缩的误差反馈；CAFe-S 在服务端拥有一小部分私有数据时，生成服务器引导的候选更新以提升预测精度。以分布式梯度下降（DGD）作为分析对象，给出在非凸且梯度差异有界条件下 CAFe 相比 DCGD 的优势证明，以及 CAFe-S 的收敛性证明，并证明服务端数据代表性越强，收敛速率越好。

Result: 理论上，CAFe 在非凸与有界梯度差异条件下优于 DCGD；CAFe-S 具备收敛到驻点的保证，且在服务端数据更具代表性时收敛速率提高；实验结果在分布式/联邦学习场景下验证了相较现有压缩方案的优越性。

Conclusion: 提出的 CAFe 与 CAFe-S 框架实现了在无客户端状态的前提下对有偏压缩的高效和可证明收敛性，适用于上行带宽受限的大规模联邦学习场景，且服务端数据的可用性进一步提升性能。

Abstract: Distributed learning, particularly Federated Learning (FL), faces a significant bottleneck in the communication cost, particularly the uplink transmission of client-to-server updates, which is often constrained by asymmetric bandwidth limits at the edge. Biased compression techniques are effective in practice, but require error feedback mechanisms to provide theoretical guarantees and to ensure convergence when compression is aggressive. Standard error feedback, however, relies on client-specific control variates, which violates user privacy and is incompatible with stateless clients common in large-scale FL. This paper proposes two novel frameworks that enable biased compression without client-side state or control variates. The first, Compressed Aggregate Feedback (CAFe), uses the globally aggregated update from the previous round as a shared control variate for all clients. The second, Server-Guided Compressed Aggregate Feedback (CAFe-S), extends this idea to scenarios where the server possesses a small private dataset; it generates a server-guided candidate update to be used as a more accurate predictor. We consider Distributed Gradient Descent (DGD) as a representative algorithm and analytically prove CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-convex regime with bounded gradient dissimilarity. We further prove that CAFe-S converges to a stationary point, with a rate that improves as the server's data become more representative. Experimental results in FL scenarios validate the superiority of our approaches over existing compression schemes.

</details>


### [61] [SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance](https://arxiv.org/abs/2512.22792)
*Shuai Chen,Chen Wang,Ziran Wang*

Main category: cs.LG

TL;DR: SNM-Net: a geometry-based normalization plus Mahalanobis scoring for open-set gas recognition in E-nose systems; achieves near-perfect AUROC and high unknown detection on Vergara dataset with architecture-agnostic backbones.


<details>
  <summary>Details</summary>
Motivation: Open-set gas recognition faces signal drift induced feature distribution shifts and interference-induced decision failures. Euclidean distance-based methods fail to capture anisotropic feature distributions and dynamic signal intensities, necessitating a framework that stabilizes features and adapts decision boundaries.

Method: A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. Mahalanobis distance is used as the scoring metric with class-wise statistics to form adaptive ellipsoidal decision regions. The framework is architecture-agnostic and compatible with CNN, RNN, and Transformer backbones.

Result: On Vergara dataset, Transformer+SNM achieves AUROC 0.9977 and unknown gas detection rate 99.57% (TPR@5%FPR). It outperforms state-of-the-art by ~3.0% AUROC and reduces performance SD by ~91% compared to Class Anchor Clustering, with robustness across sensor positions (SD < 0.0028).

Conclusion: SNM-Net effectively resolves the accuracy-stability trade-off in open-set gas recognition and provides a solid, architecture-agnostic foundation for industrial E-nose deployment.

Abstract: Electronic nose (E-nose) systems face dual challenges in open-set gas recognition: feature distribution shifts caused by signal drift and decision failures induced by unknown interference. Existing methods predominantly rely on Euclidean distance, failing to adequately account for anisotropic gas feature distributions and dynamic signal intensity variations. To address these issues, this study proposes SNM-Net, a universal deep learning framework for open-set gas recognition. The core innovation lies in a geometric decoupling mechanism achieved through cascaded batch normalization and L2 normalization, which projects high-dimensional features onto a unit hypersphere to eliminate signal intensity fluctuations. Additionally, Mahalanobis distance is introduced as the scoring mechanism, utilizing class-wise statistics to construct adaptive ellipsoidal decision boundaries. SNM-Net is architecture-agnostic and seamlessly integrates with CNN, RNN, and Transformer backbones. Systematic experiments on the Vergara dataset demonstrate that the Transformer+SNM configuration attains near-theoretical performance, achieving an AUROC of 0.9977 and an unknown gas detection rate of 99.57% (TPR at 5% FPR). This performance significantly outperforms state-of-the-art methods, showing a 3.0% improvement in AUROC and a 91.0% reduction in standard deviation compared to Class Anchor Clustering. The framework exhibits exceptional robustness across sensor positions with standard deviations below 0.0028. This work effectively resolves the trade-off between accuracy and stability, providing a solid technical foundation for industrial E-nose deployment.

</details>


### [62] [Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation](https://arxiv.org/abs/2512.22248)
*Rohit Pandey,Rohan Pandey*

Main category: cs.LG

TL;DR: A simulation-based amortized inference method trains a neural network on synthetic flight data to invert a physics simulator and predict aerodynamic parameters (drag coefficient and thrust correction) from apogee measurements and configuration features, enabling zero-shot transfer to real flights with MAE 12.3 m; outperforms OpenRocket baselines; biases reveal gaps between idealized physics and reality; code is publicly available.


<details>
  <summary>Details</summary>
Motivation: Reduce dependence on expensive CFD or real flight datasets by leveraging synthetic data and a forward physics model to enable accurate real-world predictions with zero real-data fine-tuning, benefiting amateur rocketry by providing a practical predictive tool.

Method: Generate 10,000 synthetic flights using a physics simulator. Train a neural network to invert the forward model, predicting drag coefficient and thrust correction from a single apogee measurement plus motor and configuration features. Apply the model directly to 8 real flights without any fine-tuning.

Result: The model achieved a mean absolute error of 12.3 meters in apogee prediction on the real flights, indicating effective sim-to-real transfer with zero real training data. A systematic positive bias was observed, quantifying the gap between idealized physics and real conditions. Compared to OpenRocket baselines, the learned approach reduced apogee prediction error.

Conclusion: This proof-of-concept demonstrates that amortized, simulation-based inversion can enable accurate real-world predictions with minimal real data and offers a publicly available implementation to support adoption in the amateur rocketry community.

Abstract: Accurate prediction of model rocket flight performance requires estimating aerodynamic parameters that are difficult to measure directly. Traditional approaches rely on computational fluid dynamics or empirical correlations, while data-driven methods require extensive real flight data that is expensive and time-consuming to collect. We present a simulation-based amortized inference approach that trains a neural network on synthetic flight data generated from a physics simulator, then applies the learned model to real flights without any fine-tuning. Our method learns to invert the forward physics model, directly predicting drag coefficient and thrust correction factor from a single apogee measurement combined with motor and configuration features. In this proof-of-concept study, we train on 10,000 synthetic flights and evaluate on 8 real flights, achieving a mean absolute error of 12.3 m in apogee prediction - demonstrating promising sim-to-real transfer with zero real training examples. Analysis reveals a systematic positive bias in predictions, providing quantitative insight into the gap between idealized physics and real-world flight conditions. We additionally compare against OpenRocket baseline predictions, showing that our learned approach reduces apogee prediction error. Our implementation is publicly available to support reproducibility and adoption in the amateur rocketry community.

</details>


### [63] [Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models](https://arxiv.org/abs/2512.22249)
*Zheng Xing,Weibing Zhao*

Main category: cs.LG

TL;DR: 提出一种将 temporal vision semantics (TVS) 通过大语言模型（LLM）从连续帧中提取文本信息并融入子空间聚类的无监督人类动作分割方法，利用时间邻近约束与反馈机制实现更优分割，在四个数据集上达到最新性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于子空间聚类的 HMS 在时序语义方面存在不足，难以充分利用连续帧之间的语义变化信息。引入基于 LLM 的 TVS 可从文本层面捕捉动作的时序一致性，辅助提升聚类效果。

Method: 对连续帧通过 LLM 提取文本描述，询问是否同一动作以获取时间邻近关系；据此学习 temporal neighboring information；将 TVS 纳入子空间聚类，使用带时间正则化的子空间嵌入，使每帧与其时间邻居共享相似嵌入；基于子空间嵌入和时间约束进行分割；引入反馈机制，依据分割结果迭代优化嵌入。

Result: 在四个基准数据集上，所提方法实现超越现有SOTA 的性能。

Conclusion: 将 TVS 融入子空间聚类的无监督 HMS 为有效提升分割效果提供了新思路，LLM 产生的 temporal semantics 有助于挖掘更丰富的时间依赖；未来可进一步通过更强的文本-视觉对齐和自监督信号提升。

Abstract: Unsupervised human motion segmentation (HMS) can be effectively achieved using subspace clustering techniques. However, traditional methods overlook the role of temporal semantic exploration in HMS. This paper explores the use of temporal vision semantics (TVS) derived from human motion sequences, leveraging the image-to-text capabilities of a large language model (LLM) to enhance subspace clustering performance. The core idea is to extract textual motion information from consecutive frames via LLM and incorporate this learned information into the subspace clustering framework. The primary challenge lies in learning TVS from human motion sequences using LLM and integrating this information into subspace clustering. To address this, we determine whether consecutive frames depict the same motion by querying the LLM and subsequently learn temporal neighboring information based on its response. We then develop a TVS-integrated subspace clustering approach, incorporating subspace embedding with a temporal regularizer that induces each frame to share similar subspace embeddings with its temporal neighbors. Additionally, segmentation is performed based on subspace embedding with a temporal constraint that induces the grouping of each frame with its temporal neighbors. We also introduce a feedback-enabled framework that continuously optimizes subspace embedding based on the segmentation output. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art approaches on four benchmark human motion datasets.

</details>


### [64] [Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data](https://arxiv.org/abs/2512.22259)
*Daniil Burakov,Ivan Petrov,Dmitrii Khelimskii,Ivan Bessonov,Mikhail Lazarev*

Main category: cs.LG

TL;DR: 通过对PCI后3年死亡预测的研究，使用现实与合成数据解决样本不平衡问题；增量数据增强提升少数类召回率和概率质量，同时保持AUROC，发现年龄、射血分数、周边动脉疾病和脑血管疾病为最重要特征，强调在临床预测中报告概率质量与压力测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决在PCI术后3年死亡预测中存在的高度类别不平衡问题，并量化哪些临床因素对死亡风险影响；验证合成样本数据增强对模型性能的作用，以及在特征去除实验中的稳定性。

Method: 回顾性分析2044例经皮冠状动脉介入治疗(Bifurcation)患者的胸痛数据，建立多种机器学习模型预测3年死亡；为解决类别不平衡，额外生成500个合成样本并加入训练集；使用置换特征重要性评估各特征贡献；通过移除非信息性特征进行消融实验以评估对预测的影响。

Result: 若不进行过采样，模型在整体准确度高（0.92-0.93），但几乎忽略少数类；引入合成数据增强后，少数类召回率显著提高，AUROC损失极小，概率质量与临床风险估计更合理；四个最具影响力的特征为：年龄、射血分数、外周动脉疾病、脑血管疾病。

Conclusion: 用现实与极端场景的简单双样本扩充即可揭示并缓解不平衡临床预测中的脆弱性，且应将概率质量与压力测试等信息纳入报道，以提升基于表格数据的预测模型在临床中的可用性。

Abstract: Patient status, angiographic and procedural characteristics encode crucial signals for predicting long-term outcomes after percutaneous coronary intervention (PCI). The aim of the study was to develop a predictive model for assessing the risk of cardiac death based on the real and synthetic data of patients undergoing PCI and to identify the factors that have the greatest impact on mortality. We analyzed 2,044 patients, who underwent a PCI for bifurcation lesions. The primary outcome was cardiac death at 3-year follow-up. Several machine learning models were applied to predict three-year mortality after PCI. To address class imbalance and improve the representation of the minority class, an additional 500 synthetic samples were generated and added to the training set. To evaluate the contribution of individual features to model performance, we applied permutation feature importance. An additional experiment was conducted to evaluate how the model's predictions would change after removing non-informative features from the training and test datasets. Without oversampling, all models achieve high overall accuracy (0.92-0.93), yet they almost completely ignore the minority class. Across models, augmentation consistently increases minority-class recall with minimal loss of AUROC, improves probability quality, and yields more clinically reasonable risk estimates on the constructed severe profiles. According to feature importance analysis, four features emerged as the most influential: Age, Ejection Fraction, Peripheral Artery Disease, and Cerebrovascular Disease. These results show that straightforward augmentation with realistic and extreme cases can expose, quantify, and reduce brittleness in imbalanced clinical prediction using only tabular records, and motivate routine reporting of probability quality and stress tests alongside headline metrics.

</details>


### [65] [The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning](https://arxiv.org/abs/2512.22261)
*Rahul D Ray*

Main category: cs.LG

TL;DR: 系统性消融研究揭示：在物理约束的数据生成中，某些约束可被移除而不影响关键物理属性；Fabry-Perot振荡对带宽波动的贡献最大，去除后带宽展宽显著减少；能量守恒在物理一致的方程下冗余；噪声处理存在产生非物理负吸收的风险；生成速度快（200样本/秒），远超全波求解；物理可学习性存在权衡：中心波长保持预测稳定，去除Fabry-Perot后带宽预测显著提高。


<details>
  <summary>Details</summary>
Motivation: 探究在科学领域中如何有选择地引入物理约束以提升数据生成效率，同时利用ML性能来诊断哪些约束对特定输出最为关键。

Method: 对五个几何参数映射到100点光谱响应的物理约束数据生成器进行消融实验，逐步移除能量守恒、Fabry-Perot振荡、带宽变异约束以及噪声，比较有无约束下的输出质量及下游ML评估指标。

Result: 结论包括：能量守恒在该体系下冗余；去除Fabry-Perot振荡显著减少带宽变异（带宽半宽度展宽从132.3 nm降至37.4 nm，约72%的改进），同时提高带宽预测的R^2（提升约31.3%）和RMSE下降约73.8%；噪声管线可能引入0.5%的非物理负吸收；中心波长预测不受影响；生成速率为200样本/秒，远快于常规全波求解。

Conclusion: 为物理约束数据生成提供可操作指南：优先关注对目标带宽的约束贡献；使用机器学习性能作为约束相关性的诊断工具；设计数据集时需警惕噪声处理带来的非物理结果。

Abstract: Physics-constrained data generation is essential for machine learning in scientific domains where real data are scarce; however, existing approaches often over-constrain models without identifying which physical components are necessary. We present a systematic ablation study of a physics-informed grating coupler spectrum generator that maps five geometric parameters to 100-point spectral responses. By selectively removing explicit energy conservation enforcement, Fabry-Perot oscillations, bandwidth variation, and noise, we uncover a physics constraint paradox: explicit energy conservation enforcement is mathematically redundant when the underlying equations are physically consistent, with constrained and unconstrained variants achieving identical conservation accuracy (mean error approximately 7 x 10^-9). In contrast, Fabry-Perot oscillations dominate threshold-based bandwidth variability, accounting for a 72 percent reduction in half-maximum bandwidth spread when removed (with bandwidth spread reduced from 132.3 nm to 37.4 nm). We further identify a subtle pitfall: standard noise-addition-plus-renormalization pipelines introduce 0.5 percent unphysical negative absorption values. The generator operates at 200 samples per second, enabling high-throughput data generation and remaining orders of magnitude faster than typical full-wave solvers reported in the literature. Finally, downstream machine learning evaluation reveals a clear physics-learnability trade-off: while central wavelength prediction remains unaffected, removing Fabry-Perot oscillations improves bandwidth prediction accuracy by 31.3 percent in R-squared and reduces RMSE by 73.8 percent. These findings provide actionable guidance for physics-informed dataset design and highlight machine learning performance as a diagnostic tool for assessing constraint relevance.

</details>


### [66] [Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against](https://arxiv.org/abs/2512.22293)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 警告性训练数据并非能有效抑制语言模型再现警告内容。实验表明模型在暴露于“禁止使用/有漏洞”的警告和直接看到对应内容的再现率差异不显著（约76.7%对83.3%），原因在于潜在特征的正交化失败：描述行为X与执行行为X激活的潜在特征重叠。特征#8684（跟踪代码执行模式）在两种情境下均有显著激活。还发现“隐形滑移”（stealth slip）现象：对话前缀可将激活旋转到线性探测难以检测的子空间。提示与推理时的引导无效，训练阶段的特征消融才有效。结论是统计共现支配现有架构的行为，模型学习的是上下文后续的统计规律，而非为何会出现在该上下文。


<details>
  <summary>Details</summary>
Motivation: 揭示为什么基于警告的安全提示未能有效约束模型行为，分析背后的表征与训练动态，以及导致警告无效的潜在机制。

Method: 通过将模型暴露于被警告的内容与直接内容的实验对照，结合稀疏自编码器对潜在特征的分析，识别重叠的特征激活与子空间旋转；评估提示/推理时的引导效果；进行训练阶段的特征消融测试以评估对输出的影响。

Result: 警告内容的再现率与直接内容相近（如76.7% vs 83.3%），说明警告未起效。自编码器分析揭示描述X与执行X共享激活特征；特征#8684与代码执行模式相关，在两情形下均激活。出现“隐形滑移”机制，使前缀能够将激活移入线性探针难以检测的子空间。提示/推理引导无效，只有训练阶段的特征消融才能降低风险输出。结论是统计共现优于语用理解，模型学习的是上下文中的后续模式而非原因。

Conclusion: 当前模型架构中，统计共现主导输出，警告性提示对控制违规行为效果有限；需在训练阶段设计干预（特征消融、正交化约束等）以改变潜在表示，才能有效降低再现违规内容的风险。

Abstract: Warning-framed content in training data (e.g., "DO NOT USE - this code is vulnerable") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: "describing X" and "performing X" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call "stealth slip", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.

</details>


### [67] [Hierarchical Stacking Optimization Using Dirichlet's Process (SoDip): Towards Accelerated Design for Graft Polymerization](https://arxiv.org/abs/2512.22279)
*Amgad Ahmed Ali Ibrahim,Hein Htet,Ryoji Asahi*

Main category: cs.LG

TL;DR: 提出SoDip：一个分层数据驱动框架，用文本编码、多模态建模、GPR+DPMM不确定性量化与贝叶斯优化，提升对RIG过程的预测性与可重复性分析，且能区分低可重复性区间。


<details>
  <summary>Details</summary>
Motivation: RIG过程的重复性受基膜形貌（晶体度、晶向、自由体积）变异性影响，但相关变量往未在文献中系统报告，导致气体扩散、自由基分布和Trommsdorff效应引起的局部接枝梯度与性能波动难以控制。

Method: 提出SoDip分层优化框架：1) decoder-only Transformer (DeepSeek-R1) 编码文本型过程描述（如辐照源、接枝类型、基材厂家）；2) TabNet与XGBoost 捕捉多模态特征交互；3) 以Dirichlet Process Mixture Models为基础的高斯过程回归（GPR）用于不确定性与异方差分析；4) 贝叶斯优化在高维合成空间中高效探索。数据集通过ChemDataExtractor 2.0与WebPlotDigitizer整理，涵盖大量RIG研究中的数值与文本变量。

Result: 在交叉验证中，SoDip相比GPR实现约33%的性能提升，并提供经过校准的置信区间，能够识别低可重复性区域；其分层结构有效整合稀疏文本与数值输入，优于先前模型，且为基于形貌的可重复设计奠定基础。

Conclusion: 建立了一个可重复性与形貌感知的RIG设计框架，有助于推动接枝聚合研究的可重复性与可靠性。

Abstract: Radiation-induced grafting (RIG) enables precise functionalization of polymer films for ion-exchange membranes, CO2-separation membranes, and battery electrolytes by generating radicals on robust substrates to graft desired monomers. However, reproducibility remains limited due to unreported variability in base-film morphology (crystallinity, grain orientation, free volume), which governs monomer diffusion, radical distribution, and the Trommsdorff effect, leading to spatial graft gradients and performance inconsistencies. We present a hierarchical stacking optimization framework with a Dirichlet's Process (SoDip), a hierarchical data-driven framework integrating: (1) a decoder-only Transformer (DeepSeek-R1) to encode textual process descriptors (irradiation source, grafting type, substrate manufacturer); (2) TabNet and XGBoost for modelling multimodal feature interactions; (3) Gaussian Process Regression (GPR) with Dirichlet Process Mixture Models (DPMM) for uncertainty quantification and heteroscedasticity; and (4) Bayesian Optimization for efficient exploration of high-dimensional synthesis space. A diverse dataset was curated using ChemDataExtractor 2.0 and WebPlotDigitizer, incorporating numerical and textual variables across hundreds of RIG studies. In cross-validation, SoDip achieved ~33% improvement over GPR while providing calibrated confidence intervals that identify low-reproducibility regimes. Its stacked architecture integrates sparse textual and numerical inputs of varying quality, outperforming prior models and establishing a foundation for reproducible, morphology-aware design in graft polymerization research.

</details>


### [68] [Valori: A Deterministic Memory Substrate for AI Systems](https://arxiv.org/abs/2512.22280)
*Varshith Gudur*

Main category: cs.LG

TL;DR: 提出 Valori 的确定性 AI 内存底层，使用固定点 Q16.16 替代浮点内存操作，将内存建模为可重放的状态机，以在跨平台获得位一致的内存状态、快照和检索结果；并给出开源实现。


<details>
  <summary>Details</summary>
Motivation: 浮点内存导致跨硬件架构（如 x86 与 ARM）呈现不同的内存状态与检索结果，削弱可回放性、可审计性和可验证性，特别是在受监管领域；需要一个确定性的内存原语来实现可追溯的 AI 系统。

Method: 将浮点内存操作替换为固定点（Q16.16）；将内存视为可重放的状态机并在内存边界强制确定性；分析在索引或检索之前即出现非确定性，并展示 Valori 如何在内存边界处强制确定性；给出参考实现的开源代码与档案。

Result: 表明跨平台可以实现确定性，定位了导致非确定性的阶段，展示在内存边界处的确定性；结论认为确定性内存是可信 AI 的必要原语；提供了开源实现。

Conclusion: 确定性内存（Valori）是实现可信 AI 系统的关键基础设施。

Abstract: Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).

</details>


### [69] [DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations](https://arxiv.org/abs/2512.22283)
*Guokan Chen,Yao Xiao*

Main category: cs.LG

TL;DR: Dynamic Balancing Adaptive Weighting Physics-Informed Kolmogorov-Arnold Network (DBAW-PIKAN) to address gradient stiffness and spectral bias in PINNs for multi-scale/high-frequency PDEs. It combines Kolmogorov-Arnold networks with learnable B-splines and an adaptive weighting with a dynamic decay upper bound, achieving faster convergence and at least an order-of-magnitude improvement in accuracy without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with stiffness in gradient flow and spectral bias when solving multi-scale or high-frequency PDEs, limiting predictive accuracy and generalization.

Method: Introduce DBAW-PIKAN by integrating Kolmogorov-Arnold network architecture (learnable B-splines) with an adaptive weighting strategy that includes a dynamic decay upper bound to balance gradient contributions and improve representational capacity.

Result: Empirical benchmarks on Klein-Gordon, Burgers, and Helmholtz equations show significantly enhanced accuracy and generalization, with convergence accelerated and accuracy improved by at least one order of magnitude, without increasing computational complexity.

Conclusion: DBAW-PIKAN effectively mitigates gradient-related failure modes in PINNs for challenging PDEs and offers a scalable, more accurate framework with improved generalization.

Abstract: Physics-informed neural networks (PINNs) have led to significant advancements in scientific computing by integrating fundamental physical principles with advanced data-driven techniques. However, when dealing with problems characterized by multi-scale or high-frequency features, PINNs encounter persistent and severe challenges related to stiffness in gradient flow and spectral bias, which significantly limit their predictive capabilities. To address these issues, this paper proposes a Dynamic Balancing Adaptive Weighting Physics-Informed Kolmogorov-Arnold Network (DBAW-PIKAN), designed to mitigate such gradient-related failure modes and overcome the bottlenecks in function representation. The core of DBAW-PIKAN combines the Kolmogorov-Arnold network architecture, based on learnable B-splines, with an adaptive weighting strategy that incorporates a dynamic decay upper bound. Compared to baseline models, the proposed method accelerates the convergence process and improves solution accuracy by at least an order of magnitude without introducing additional computational complexity. A series of numerical benchmarks, including the Klein-Gordon, Burgers, and Helmholtz equations, demonstrate the significant advantages of DBAW-PIKAN in enhancing both accuracy and generalization performance.

</details>


### [70] [Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation](https://arxiv.org/abs/2512.22287)
*Zikun Guoa,Adeyinka. P. Adedigbaa,Rammohan Mallipeddi*

Main category: cs.LG

TL;DR: 提出 Cluster Aggregated GAN，针对断续型和连续型家用电器分路生成，提高合成负荷的现实性、多样性与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 缺乏标注数据，现有 GAN 方法将所有设备混合建模，忽视设备行为差异，导致训练不稳定和输出保真度受限。

Method: 引入混合生成框架：对断续型设备使用聚类模块将相似激活模式分簇，并为每簇分配独立生成器；对连续型设备使用基于 LSTM 的生成器，并通过序列压缩提升训练稳定性。用于聚类与生成的联合训练。

Result: 在 UVIC smart plug 数据集上，提出的方法在真实感、多样性和训练稳定性等指标上优于基线方法。将聚类作为主动生成组件提升可解释性和可扩展性。

Conclusion: 该框架为 NILM 研究中的合成负荷生成提供一种高效且可扩展的解决方案。

Abstract: Synthetic appliance data are essential for developing non-intrusive load monitoring algorithms and enabling privacy preserving energy research, yet the scarcity of labeled datasets remains a significant barrier. Recent GAN-based methods have demonstrated the feasibility of synthesizing load patterns, but most existing approaches treat all devices uniformly within a single model, neglecting the behavioral differences between intermittent and continuous appliances and resulting in unstable training and limited output fidelity. To address these limitations, we propose the Cluster Aggregated GAN framework, a hybrid generative approach that routes each appliance to a specialized branch based on its behavioral characteristics. For intermittent appliances, a clustering module groups similar activation patterns and allocates dedicated generators for each cluster, ensuring that both common and rare operational modes receive adequate modeling capacity. Continuous appliances follow a separate branch that employs an LSTM-based generator to capture gradual temporal evolution while maintaining training stability through sequence compression. Extensive experiments on the UVIC smart plug dataset demonstrate that the proposed framework consistently outperforms baseline methods across metrics measuring realism, diversity, and training stability, and that integrating clustering as an active generative component substantially improves both interpretability and scalability. These findings establish the proposed framework as an effective approach for synthetic load generation in non-intrusive load monitoring research.

</details>


### [71] [Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model](https://arxiv.org/abs/2512.22288)
*Renping Zhou,Zanlin Ni,Tianyi Chen,Zeyu Liu,Yang Yue,Yulin Wang,Yuxuan Wang,Jingshu Liu,Gao Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recently, Masked Diffusion Models (MDMs) have shown promising potential across vision, language, and cross-modal generation. However, a notable discrepancy exists between their training and inference procedures. In particular, MDM inference is a multi-step, iterative process governed not only by the model itself but also by various schedules that dictate the token-decoding trajectory (e.g., how many tokens to decode at each step). In contrast, MDMs are typically trained using a simplified, single-step BERT-style objective that masks a subset of tokens and predicts all of them simultaneously. This step-level simplification fundamentally disconnects the training paradigm from the trajectory-level nature of inference, leaving the inference schedules never optimized during training. In this paper, we introduce Co-GRPO, which reformulates MDM generation as a unified Markov Decision Process (MDP) that jointly incorporates both the model and the inference schedule. By applying Group Relative Policy Optimization at the trajectory level, Co-GRPO cooperatively optimizes model parameters and schedule parameters under a shared reward, without requiring costly backpropagation through the multi-step generation process. This holistic optimization aligns training with inference more thoroughly and substantially improves generation quality. Empirical results across four benchmarks-ImageReward, HPS, GenEval, and DPG-Bench-demonstrate the effectiveness of our approach. For more details, please refer to our project page: https://co-grpo.github.io/ .

</details>


### [72] [When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing](https://arxiv.org/abs/2512.22290)
*Arunkumar V,Nivethitha S,Sharan Srinivas,Gangadharan G. R*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A central question for the future of work is whether person centered management can survive when algorithms take on managerial roles. Standard tools often miss what is happening because worker responses to algorithmic systems are rarely linear. We use a Double Machine Learning framework to estimate a moderated mediation model without imposing restrictive functional forms. Using survey data from 464 gig workers, we find a clear nonmonotonic pattern. Supportive HR practices improve worker wellbeing, but their link to performance weakens in a murky middle where algorithmic oversight is present yet hard to interpret. The relationship strengthens again when oversight is transparent and explainable. These results show why simple linear specifications can miss the pattern and sometimes suggest the opposite conclusion. For platform design, the message is practical: control that is only partly defined creates confusion, but clear rules and credible recourse can make strong oversight workable. Methodologically, the paper shows how Double Machine Learning can be used to estimate conditional indirect effects in organizational research without forcing the data into a linear shape.

</details>


### [73] [LLMBoost: Make Large Language Models Stronger with Boosting](https://arxiv.org/abs/2512.22309)
*Zehao Chen,Tianxiang Ai,Yifei Li,Gongxun Li,Yuyang Wei,Wang Zhou,Guanghui Li,Bin Yu,Zhijun Chen,Hailong Sun,Fuzhen Zhuang,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.LG

TL;DR: 提出 LLMBoost，通过跨模型隐藏状态的显式利用、链式训练和近并行推理实现强增强的LLM集成微调。


<details>
  <summary>Details</summary>
Motivation: 打破黑盒式输入/输出级别的集成，充分挖掘和传递模型间的中间表征与相互作用。

Method: 包含三大创新：跨模型注意以融合前驱隐藏状态、链式训练以逐步纠错、近并行推理以层层传递隐藏状态提升效率。

Result: 在 commonsense 与 arithmetic 推理任务上显著提升准确性并降低推理延迟。

Conclusion: 给出理论基础，证明在有界修正假设下的序列整合可实现单调改进。

Abstract: Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.

</details>


### [74] [Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making](https://arxiv.org/abs/2512.22313)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出 Optimistic Feasible Search (OFS) 用于在带看板反馈的在线学习中从一维阈值策略中学习，在人口公平的条件下（若干项包括Demographic Parity 和服务率约束），通过网格化和乐观置信界实现选择性阈值，既追求可行且高效的阈值，又控制约束违规。实验表明在合成与半合成基准下优于无约束和原始对偶基线，并接近最优可行固定阈值。可重复、双盲友好。


<details>
  <summary>Details</summary>
Motivation: 解决受决策反馈影响而产生非平稳数据和潜在差异放大的闭环决策系统中的公平性与服务约束问题。在线学习一维阈值策略，在带少量反馈信息的带约束情境下实现高效、可解释的策略优化。

Method: 网格化的乐观可行搜索（OFS）：对每个候选阈值维护奖励与约束残差的置信区间；在每轮选择在置信界下看似可行的阈值中最大化乐观奖励；若无可行阈值则选择在乐观约束违规下的阈值，以直接瞄准高效可行阈值，适合低维可解释策略空间。

Result: 在包括稳定收缩动力学的合成基准和基于 German Credit 与 COMPAS 的半合成基准中，OFS 在累积奖励和约束违规上均优于无约束及原始对偶基线，且在同一搜索过程下接近最优可行固定阈值。实验可重复且实现了双盲友好相对输出。

Conclusion: OFS 能有效在受闭环反馈影响、带公平与服务约束的在线阈值学习中，找到高效且可行的阈值，尤其适用于低维、可解释的策略设定，优于传统无约束与 primal-dual 基线。

Abstract: Closed-loop decision-making systems (e.g., lending, screening, or recidivism risk assessment) often operate under fairness and service constraints while inducing feedback effects: decisions change who appears in the future, yielding non-stationary data and potentially amplifying disparities. We study online learning of a one-dimensional threshold policy from bandit feedback under demographic parity (DP) and, optionally, service-rate constraints. The learner observes only a scalar score each round and selects a threshold; reward and constraint residuals are revealed only for the chosen threshold.
  We propose Optimistic Feasible Search (OFS), a simple grid-based method that maintains confidence bounds for reward and constraint residuals for each candidate threshold. At each round, OFS selects a threshold that appears feasible under confidence bounds and, among those, maximizes optimistic reward; if no threshold appears feasible, OFS selects the threshold minimizing optimistic constraint violation. This design directly targets feasible high-utility thresholds and is particularly effective for low-dimensional, interpretable policy classes where discretization is natural.
  We evaluate OFS on (i) a synthetic closed-loop benchmark with stable contraction dynamics and (ii) two semi-synthetic closed-loop benchmarks grounded in German Credit and COMPAS, constructed by training a score model and feeding group-dependent acceptance decisions back into population composition. Across all environments, OFS achieves higher reward with smaller cumulative constraint violation than unconstrained and primal-dual bandit baselines, and is near-oracle relative to the best feasible fixed threshold under the same sweep procedure. Experiments are reproducible and organized with double-blind-friendly relative outputs.

</details>


### [75] [LangPrecip: Language-Aware Multimodal Precipitation Nowcasting](https://arxiv.org/abs/2512.22317)
*Xudong Ling,Tianxi Huang,Qian Dong,Tao He,Chaorong Li,Guiduo Duan*

Main category: cs.LG

TL;DR: 提出 LangPrecip，一种语言感知的多模态 nowcasting 框架，将气象文本作为语义运动约束，结合雷达信息在潜在空间实现更一致的降水预测，并提供 LangPrecip-160k 数据集；在瑞典和 MRMS 数据集上实现显著提升。


<details>
  <summary>Details</summary>
Motivation: 短时降水逐步预报具有高度不确定性，现有生成方法以视觉条件为主，未来运动被弱约束且模糊难以控制，语言描述可提供语义层面的运动约束，结合雷达信息可实现更物理一致的预测。

Method: 基于 Rectified Flow 的框架，将气象文本作为语义运动约束，进行潜在空间中的文本与雷达信息的高效整合；提出 LangPrecip 框架及 LangPrecip-160k 数据集（160k 对雷达序列与运动描述的多模态配对）。

Result: 在瑞典与 MRMS 数据集上与现有方法相比取得显著提升，在 80 分钟提前期下，heavy-rainfall CSI 获得超过 60% 与 19% 的增益（对应两个数据集）。

Conclusion: 语言感知的多模态 nowcasting 能有效提升短时降水预测的准确性与物理一致性，且对大规模数据集具有潜在应用价值。

Abstract: Short-term precipitation nowcasting is an inherently uncertain and under-constrained spatiotemporal forecasting problem, especially for rapidly evolving and extreme weather events. Existing generative approaches rely primarily on visual conditioning, leaving future motion weakly constrained and ambiguous. We propose a language-aware multimodal nowcasting framework(LangPrecip) that treats meteorological text as a semantic motion constraint on precipitation evolution. By formulating nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm, our method enables efficient and physically consistent integration of textual and radar information in latent space.We further introduce LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. Experiments on Swedish and MRMS datasets show consistent improvements over state-of-the-art methods, achieving over 60 \% and 19\% gains in heavy-rainfall CSI at an 80-minute lead time.

</details>


### [76] [Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough](https://arxiv.org/abs/2512.22318)
*Chorok Lee*

Main category: cs.LG

TL;DR: 提出CAGP，分解语义不确定性和结构不确定性以改进知识图谱嵌入的时序OOD检测，显著优于关系无关基线，在多个数据集实现高AUROC并提升选择性预测。


<details>
  <summary>Details</summary>
Motivation: 现有概率化KG嵌入将方差设为与关系无关，混淆了新兴实体与新颖关系等两类不同的异常分布现象，导致对新上下文的检测能力受限。

Method: 建立理论不可实现性结果，证明仅基于实体级统计且忽略关系上下文的估计器对新上下文的OOD检测近似随机；提出CAGP框架，通过学习权重将语义不确定性（实体嵌入方差）与结构不确定性（实体-关系共现）进行融合，获得非冗余的信号并优于任一单一信号。

Result: 在FB15k-237、WN18RR、YAGO3-10上验证，CAGP实现0.94–0.99 AUROC的时序OOD检测，较关系无关基线提升60–80%的相对性能；三数据集呈现频率完全重叠；在选择性预测场景，准确率85%时错误率下降约43%。

Conclusion: 语义与结构不确定性分解互为补充且非冗余，CAGP能显著提升对时序分布偏移的检测能力，提供对知识图谱嵌入不确定性的更深入理解与实用提升。

Abstract: Probabilistic knowledge graph embeddings represent entities as distributions, using learned variances to quantify epistemic uncertainty. We identify a fundamental limitation: these variances are relation-agnostic, meaning an entity receives identical uncertainty regardless of relational context. This conflates two distinct out-of-distribution phenomena that behave oppositely: emerging entities (rare, poorly-learned) and novel relational contexts (familiar entities in unobserved relationships). We prove an impossibility result: any uncertainty estimator using only entity-level statistics independent of relation context achieves near-random OOD detection on novel contexts. We empirically validate this on three datasets, finding 100 percent of novel-context triples have frequency-matched in-distribution counterparts. This explains why existing probabilistic methods achieve 0.99 AUROC on random corruptions but only 0.52-0.64 on temporal distribution shift. We formalize uncertainty decomposition into complementary components: semantic uncertainty from entity embedding variance (detecting emerging entities) and structural uncertainty from entity-relation co-occurrence (detecting novel contexts). Our main theoretical result proves these signals are non-redundant, and that any convex combination strictly dominates either signal alone. Our method (CAGP) combines semantic and structural uncertainty via learned weights, achieving 0.94-0.99 AUROC on temporal OOD detection across multiple benchmarks, a 60-80 percent relative improvement over relation-agnostic baselines. Empirical validation confirms complete frequency overlap on three datasets (FB15k-237, WN18RR, YAGO3-10). On selective prediction, our method reduces errors by 43 percent at 85 percent answer rate.

</details>


### [77] [Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers](https://arxiv.org/abs/2512.22326)
*Sravan Karthick T*

Main category: cs.LG

TL;DR: 引入 Global M2 Liquidity 作为12周滞后外生变量，通过 TimeXer-Exog 提升比特币长期预测精度，在70天 horizon 下的 MSE 为 1.08e8，比单变量 TimeXer 提升约89%。


<details>
  <summary>Details</summary>
Motivation: 比特币价格存在极端波动性和非平稳性，单变量时间序列难以对长 horizon 做准。将全球宏观变量中的 Global M2 Liquidity 作为领先指标，可能为价格提供有效外生信息。

Method: 在 TimeXer 架构中，将来自18个主要经济体的 Global M2 Liquidity 的12周滞后作为外生变量，构建 liquidity-conditioned 模型 TimeXer-Exog，并与 LSTM、N-BEATS、PatchTST 和单变量 TimeXer 进行对比。数据覆盖期间为 2020-01 至 2025-08 的日度比特币价格，评估70天预测。

Result: TimeXer-Exog 显著优于基线，70天预测的均方误差（MSE）达到 1.08e8，较单变量 TimeXer 提升约 89%。显式的全球流动性条件显著稳定了长 horizon 的预测。

Conclusion: 将全球流动性作为外生信息用于深度学习模型的长 horizon 比特币价格预测，具有实质性提升潜力。

Abstract: Bitcoin price forecasting is characterized by extreme volatility and non-stationarity, often defying traditional univariate time-series models over long horizons. This paper addresses a critical gap by integrating Global M2 Liquidity, aggregated from 18 major economies, as a leading exogenous variable with a 12-week lag structure. Using the TimeXer architecture, we compare a liquidity-conditioned forecasting model (TimeXer-Exog) against state-of-the-art benchmarks including LSTM, N-BEATS, PatchTST, and a standard univariate TimeXer. Experiments conducted on daily Bitcoin price data from January 2020 to August 2025 demonstrate that explicit macroeconomic conditioning significantly stabilizes long-horizon forecasts. At a 70-day forecast horizon, the proposed TimeXer-Exog model achieves a mean squared error (MSE) 1.08e8, outperforming the univariate TimeXer baseline by over 89 percent. These results highlight that conditioning deep learning models on global liquidity provides substantial improvements in long-horizon Bitcoin price forecasting.

</details>


### [78] [The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2512.22337)
*Matthew Riemer,Erik Miehling,Miao Liu,Djallel Bouneffouf,Murray Campbell*

Main category: cs.LG

TL;DR: LoRA等参数高效微调在指令调优的模型上可能会产生灾难性性能下降；通过正则化的近似回放（KL对初始模型的KL惩罚）并混入来自相似但不同的开源语料的数据进行训练，能在不显著损害模型塑性和任务能力的前提下，较小开销地保留模型的通用知识。


<details>
  <summary>Details</summary>
Motivation: 解决使用LoRA等参数高效微调方法对指令调优模型造成的性能退化问题，寻找低开销、易实现的改进策略，以在保持模型知识与适应新任务能力之间取得平衡。

Method: 提出一种正则化近似回放的方法：对输出分布与初始模型之间施加KL散度惩罚；在训练中交替使用来自与预训练数据集相似但不同的开源语料进行下一词预测训练（next-token prediction）。将该策略应用于Qwen的指令调优模型。

Result: 该策略在Qwen指令调优模型上能显著减轻因LoRA等微调引发的知识丢失，维持模型的通用知识与对新任务的塑性，且仅带来可控的额外计算开销。

Conclusion: 通过对训练流程做出小幅改动且采用正则化近似回放，可有效避免LoRA在指令调优场景中的灾难性退化问题，提供一种实用且成本可控的解决方案。

Abstract: Although parameter-efficient fine-tuning methods, such as LoRA, only modify a small subset of parameters, they can have a significant impact on the model. Our instruction-tuning experiments show that LoRA-based supervised fine-tuning can catastrophically degrade model capabilities, even when trained on very small datasets for relatively few steps. With that said, we demonstrate that while the most straightforward approach (that is likely the most used in practice) fails spectacularly, small tweaks to the training procedure with very little overhead can virtually eliminate the problem. Particularly, in this paper we consider a regularized approximate replay approach which penalizes KL divergence with respect to the initial model and interleaves in data for next token prediction from a different, yet similar, open access corpus to what was used in pre-training. When applied to Qwen instruction-tuned models, we find that this recipe preserves general knowledge in the model without hindering plasticity to new tasks by adding a modest amount of computational overhead.

</details>


### [79] [Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration](https://arxiv.org/abs/2512.22382)
*Bruno Mlodozeniec,Pierre Ablin,Louis Béthune,Dan Busbridge,Michal Klein,Jason Ramapuram,Marco Cuturi*

Main category: cs.LG

TL;DR: 提出 Complete^(d) 参数化，统一宽度/深度及批量大小与训练时长的缩放，并实现跨模型（甚至跨模块）的超参数转移，显著提升大模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 超参数对训练稳定性和最终性能影响显著，现有的 μP 等方法可在小模型上搜索全局基超参数后转移到大模型，但需处理多维缩放和模块级别的超参数优化。

Method: 提出 Complete^(d) Parameterisation（基于 CompleteP 的改造），统一处理宽度/深度、批大小、训练时长的缩放；研究跨模块的超参数转移，分析高维超参数空间的挑战，给出实用指南；覆盖学习率、AdamW 参数、权重衰减、初始化尺度、残差块放大系数等优化超参数。

Result: 在多种设置下验证了在每模块超参数 regime 下也能实现转移，且在大型语言模型上通过转移的逐模块超参数实现了显著的训练加速。

Conclusion: 受控参数化使模型尺度与模块层面的超参数转移成为现实，降低搜索成本、提升收敛稳定性和训练速度，提供了可操作的优化指南。

Abstract: Hyperparameter tuning can dramatically impact training stability and final performance of large-scale models. Recent works on neural network parameterisations, such as $μ$P, have enabled transfer of optimal global hyperparameters across model sizes. These works propose an empirical practice of search for optimal global base hyperparameters at a small model size, and transfer to a large size. We extend these works in two key ways. To handle scaling along most important scaling axes, we propose the Complete$^{(d)}$ Parameterisation that unifies scaling in width and depth -- using an adaptation of CompleteP -- as well as in batch-size and training duration. Secondly, with our parameterisation, we investigate per-module hyperparameter optimisation and transfer. We characterise the empirical challenges of navigating the high-dimensional hyperparameter landscape, and propose practical guidelines for tackling this optimisation problem. We demonstrate that, with the right parameterisation, hyperparameter transfer holds even in the per-module hyperparameter regime. Our study covers an extensive range of optimisation hyperparameters of modern models: learning rates, AdamW parameters, weight decay, initialisation scales, and residual block multipliers. Our experiments demonstrate significant training speed improvements in Large Language Models with the transferred per-module hyperparameters.

</details>


### [80] [BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks](https://arxiv.org/abs/2512.22388)
*Omar Alsaqa,Linh Thi Hoang,Muhammed Fatih Balin*

Main category: cs.LG

TL;DR: BLISS 使用多臂赌博机对每层的邻居进行重要性采样，动态选择信息最丰富的节点，平衡探索与开发，覆盖整图结构，适用于 GCN 与 GAT，且在实验中达到或超过全批训练的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模图中 GNN 的计算和内存瓶颈，特别是每个节点都要处理大量邻居所带来的负担。需要一种自适应、动态的采样策略，在降低成本的同时保持或提升模型性能。

Method: 提出 BLISS（Bandit Layer Importance Sampling Strategy），在每一层使用多臂赌博机来动态选择最具信息量的节点进行采样，平衡探索与利用以确保对图结构的充分覆盖。该策略可无缝集成于 GCN 与 GAT，且对不同聚合机制进行自适应，强调对层内采样的时序性与重要性变化进行追踪。

Result: 实验表明，BLISS 在保持甚至超过全批训练准确性的同时，显著降低了计算与内存开销，并显示出对 GCN 与 GAT 的通用适配性。

Conclusion: 基于带权采样的自适应层采样策略可在大规模图上实现高效且准确的 GNN 学习，BLISS 的带来项在不同聚合框架中具有广泛适用性和扩展性。

Abstract: Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their application to large graphs is hindered by computational costs. The need to process every neighbor for each node creates memory and computational bottlenecks. To address this, we introduce BLISS, a Bandit Layer Importance Sampling Strategy. It uses multi-armed bandits to dynamically select the most informative nodes at each layer, balancing exploration and exploitation to ensure comprehensive graph coverage. Unlike existing static sampling methods, BLISS adapts to evolving node importance, leading to more informed node selection and improved performance. It demonstrates versatility by integrating with both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), adapting its selection policy to their specific aggregation mechanisms. Experiments show that BLISS maintains or exceeds the accuracy of full-batch training.

</details>


### [81] [Causality-Inspired Safe Residual Correction for Multivariate Time Series](https://arxiv.org/abs/2512.22428)
*Jianxiang Xie,Yuncheng Hua*

Main category: cs.LG

TL;DR: CRC: 一个可在多变量预测框架中实现的安全残差修正框架，通过因果启发的编码分解自变量与跨变量动力学、以及混合修正器来建模残差误差，并通过四重安全机制确保不退化，同时提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的后验残差修正方法尽管能提升平均精度，但存在“贪婪”问题，可能在实际部署中对可信预测造成过度修正或局部失败，缺乏对部署期性能下降的保证，存在安全缺口。

Method: 提出可插拔的CRC框架：使用因果启发的编码器将自变量与跨变量关系解耦以暴露方向性结构；采用混合修正器建模残差；以严格的四重安全机制控制更新以确保非退化。

Result: 在多数据集与不同 forecasting backbones 上的实验表明CRC能普遍提升准确性；消融研究表明核心安全机制能实现极高的非退化率（NDR），使CRC成为可用于安全、可靠部署的修正框架。

Conclusion: CRC为多变量时序预测的后验修正提供一种可安全部署的解决方案，兼具改进性能与防止性能下降的保障，适合在现实应用中落地。

Abstract: While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also "help in the wrong way" by overcorrecting reliable predictions and causing local failures in unseen scenarios.
  To address this critical "safety gap," we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.
  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.

</details>


### [82] [AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing](https://arxiv.org/abs/2512.22455)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Feiye Huo,Yerui Sun,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出 AFA-LoRA，在 LoRA 的基础上引入退火激活函数，使适配器在训练初期具有非线性表达能力，随后线性化以实现合并性，从而缩小与全参数训练的性能差距。


<details>
  <summary>Details</summary>
Motivation: 弥合 LoRA 的线性适配与非线性训练之间的表达能力差距，提升参数高效微调的表现力，同时保持可合并性。

Method: 在 LoRA 框架内引入退火激活函数，使适配器在训练初期呈现非线性特性，随后逐步线性化以实现与原始线性结构的合并；在监督微调、强化学习和推测解码等场景中实现并评估。

Result: 实验证明 AFA-LoRA 能显著缩小与全参数训练之间的性能差距，相较传统 LoRA 提高表达能力，同时保持可合并的优势。

Conclusion: 该方法为参数高效自适应提供更强的表达力，推动在可合并的非线性微调策略方面的发展。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.

</details>


### [83] [AMBIT: Augmenting Mobility Baselines with Interpretable Trees](https://arxiv.org/abs/2512.22466)
*Qizhi Wang*

Main category: cs.LG

TL;DR: AMBIT 将物理-灰盒与可解释树模型结合，通过残差学习提升 OD 流预测的准确性与可解释性，并在 NYC 出租车数据上系统评估，POI 引导的残差在空间泛化上更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高准确性与可解释性之间存在冲突，现有物理模型在高时空分辨率下易脆弱；需要在物理先验和可解释性之间取得折衷以支持城市决策。

Method: 对经典空间交互模型在年-小时尺度 NYC 出租 OD 数据集进行审计；比较 PPML 引力等物理基线及其在全 OD 边际条件下的校准效果；在物理基线之上使用梯度提升树作为残差学习器并结合 SHAP 进行解释；评估 POI-锚定残差与空间泛化鲁棒性；提供可重复管线和丰富诊断。

Result: 物理-残差学习的残差模型在准确性上趋近于强树模型，同时保持可解释结构；POI-锚定残差在空间泛化下表现最为鲁棒。

Conclusion: AMBIT 提供一个可重复的、可解释的管线，将物理先验与数据驱动的树模型结合，以提升 OD 流预测的准确性与决策价值。

Abstract: Origin-destination (OD) flow prediction remains a core task in GIS and urban analytics, yet practical deployments face two conflicting needs: high accuracy and clear interpretability. This paper develops AMBIT, a gray-box framework that augments physical mobility baselines with interpretable tree models. We begin with a comprehensive audit of classical spatial interaction models on a year-long, hourly NYC taxi OD dataset. The audit shows that most physical models are fragile at this temporal resolution; PPML gravity is the strongest physical baseline, while constrained variants improve when calibrated on full OD margins but remain notably weaker. We then build residual learners on top of physical baselines using gradient-boosted trees and SHAP analysis, demonstrating that (i) physics-grounded residuals approach the accuracy of a strong tree-based predictor while retaining interpretable structure, and (ii) POI-anchored residuals are consistently competitive and most robust under spatial generalization. We provide a reproducible pipeline, rich diagnostics, and spatial error analysis designed for urban decision-making.

</details>


### [84] [GLUE: Gradient-free Learning to Unify Experts](https://arxiv.org/abs/2512.22467)
*Jong-Ik Park,Shreyas Chaudhari,Srinivasa Pranav,Carlee Joe-Wong,José M. F. Moura*

Main category: cs.LG

TL;DR: GLUE trains a target model as a convex blend of fixed expert models and optimizes the blend coefficients with a gradient-free SPSA update, achieving strong domain-expansion performance with lower computational cost than full backpropagation.


<details>
  <summary>Details</summary>
Motivation: In real deployments, multiple pretrained specialists exist across domains. When facing a new target domain, a generalized model should perform well beyond any single expert. Heuristic mixture of experts often underperforms and learning coefficients via backprop is costly. A gradient-free approach aims to efficiently find a good initialization mixture that generalizes to the target domain.

Method: Initialize the target model as a fixed convex combination of expert models. Learn the mixture coefficients using a gradient-free two-point SPSA update, requiring only two forward passes per optimization step, avoiding full backpropagation through the network.

Result: Across three datasets and three architectures, GLUE yields a single prior that can be fine-tuned to outperform baselines. It improves test accuracy by up to 8.5% over data-size weighting and up to 9.1% over proxy-metric selection. It either outperforms full-gradient mixing or matches its performance within 1.4%.

Conclusion: Gradient-free learning to unify experts is effective for domain expansion, providing a computationally efficient prior that can be fine-tuned to surpass baselines in target-domain tasks.

Abstract: In many deployed systems (multilingual ASR, cross-hospital imaging, region-specific perception), multiple pretrained specialist models coexist. Yet, new target domains often require domain expansion: a generalized model that performs well beyond any single specialist's domain. Given such a new target domain, prior works seek a single strong initialization prior for the model parameters by first blending expert models to initialize a target model. However, heuristic blending -- using coefficients based on data size or proxy metrics -- often yields lower target-domain test accuracy, and learning the coefficients on the target loss typically requires computationally-expensive full backpropagation through the network. We propose GLUE, Gradient-free Learning To Unify Experts, which initializes the target model as a convex combination of fixed experts, learning the mixture coefficients of this combination via a gradient-free two-point (SPSA) update that requires only two forward passes per step. Across experiments on three datasets and three network architectures, GLUE produces a single prior that can be fine-tuned effectively to outperform baselines. GLUE improves test accuracy by up to 8.5% over data-size weighting and by up to 9.1% over proxy-metric selection. GLUE either outperforms backpropagation-based full-gradient mixing or matches its performance within 1.4%.

</details>


### [85] [Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting](https://arxiv.org/abs/2512.22478)
*Chuantao Li,Zhi Li,Jiahao Xu,Jie Li,Sheng Li*

Main category: cs.LG

TL;DR: 提出一种协同优化的多分类不平衡学习提升模型，将密度因子和置信因子整合进提升框架，并设计抗噪声的权重更新和动态采样策略，在20个公开数据集上显著优于8个基线，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 解决分类不平衡导致的偏倚，以及不平衡学习与模型训练之间缺乏协同优化的问题，提升跨数据集的分类性能。

Method: 提出协同优化的提升模型，集成密度因子和置信因子，设计噪声鲁棒的权重更新机制和动态采样策略；将权重更新、区域划分与区域引导采样等模块紧密结合，实现对不平衡学习与模型训练的协同优化。

Result: 在20个公开不平衡数据集上的实验结果表明，该模型显著优于8个最先进基线。

Conclusion: 证实了将不平衡学习与模型训练的协同优化作为提升方向的有效性，并给出代码实现以便复现。

Abstract: Numerous studies attempt to mitigate classification bias caused by class imbalance. However, existing studies have yet to explore the collaborative optimization of imbalanced learning and model training. This constraint hinders further performance improvements. To bridge this gap, this study proposes a collaborative optimization Boosting model of multiclass imbalanced learning. This model is simple but effective by integrating the density factor and the confidence factor, this study designs a noise-resistant weight update mechanism and a dynamic sampling strategy. Rather than functioning as independent components, these modules are tightly integrated to orchestrate weight updates, sample region partitioning, and region-guided sampling. Thus, this study achieves the collaborative optimization of imbalanced learning and model training. Extensive experiments on 20 public imbalanced datasets demonstrate that the proposed model significantly outperforms eight state-of-the-art baselines. The code for the proposed model is available at: https://github.com/ChuantaoLi/DARG.

</details>


### [86] [The Quest for Winning Tickets in Low-Rank Adapters](https://arxiv.org/abs/2512.22495)
*Hamed Damirchi,Cristian Rodriguez-Opazo,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: LTH extends to LoRA: there exist sparse subnetworks within LoRAs that can match the performance of dense adapters; Partial-LoRA identifies and trains such subnetworks to achieve high parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: As large pretrained models are widely fine-tuned with parameter-efficient methods like LoRA, it is important to understand whether the Lottery Ticket Hypothesis applies and how sparsity can be leveraged to improve efficiency.

Method: Identify task-relevant sparse subnetworks within LoRA (low-rank adapters) and train them; develop Partial-LoRA to systematically select and align sparse subnetworks with task-relevant subspaces of the pretrained model; evaluate on diverse tasks.

Result: LTH holds within LoRAs: sparse subnetworks can match dense adapter performance; effectiveness hinges more on per-layer sparsity than exact weights; Partial-LoRA reduces trainable parameters by up to 87% while maintaining or improving accuracy across 8 vision and 12 language tasks, in both single-task and multi-task settings.

Conclusion: The findings deepen theoretical understanding of transfer learning and pretraining–finetuning interactions, and enable more efficient adaptation strategies through sparsity-aware, subspace-aligned PEFT methods.

Abstract: The Lottery Ticket Hypothesis (LTH) suggests that over-parameterized neural networks contain sparse subnetworks ("winning tickets") capable of matching full model performance when trained from scratch. With the growing reliance on fine-tuning large pretrained models, we investigate whether LTH extends to parameter-efficient fine-tuning (PEFT), specifically focusing on Low-Rank Adaptation (LoRA) methods. Our key finding is that LTH holds within LoRAs, revealing sparse subnetworks that can match the performance of dense adapters. In particular, we find that the effectiveness of sparse subnetworks depends more on how much sparsity is applied in each layer than on the exact weights included in the subnetwork. Building on this insight, we propose Partial-LoRA, a method that systematically identifies said subnetworks and trains sparse low-rank adapters aligned with task-relevant subspaces of the pre-trained model. Experiments across 8 vision and 12 language tasks in both single-task and multi-task settings show that Partial-LoRA reduces the number of trainable parameters by up to 87\%, while maintaining or improving accuracy. Our results not only deepen our theoretical understanding of transfer learning and the interplay between pretraining and fine-tuning but also open new avenues for developing more efficient adaptation strategies.

</details>


### [87] [Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals](https://arxiv.org/abs/2512.22508)
*Lucky Susanto,Anasta Pranawijayana,Cortino Sukotjo,Soni Prasad,Derry Wijaya*

Main category: cs.LG

TL;DR: 利用元数据与幻觉信号，在三种 prompting 策略下评估 GPT-4o 与 OSS-120B 在修复科MCQ上的正确性预测；在某些设置下准确性提升最高达+7.14%，精度83.12%，但仍不足以支撑高风险部署。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗教育）中，LLMs 产生事实性错误的风险很高，单纯的幻觉检测不足以直接判断输出的正确性，因此需要可预测正确性的信号来提升系统可靠性。

Method: 选取两种模型（GPT-4o 与 OSS-120B）在一项修复科多项选择考试场景中，设计三种 prompting 策略；结合元数据与幻觉信号，针对每对（模型、 prompting）构建正确性预测器；对比基线（假设所有答案都正确）并评估预测准确性与精度，同时分析幻觉与元数据的关系及 prompting 策略对内部行为及预测效用的影响。

Result: 元数据驱动的预测在某些设置下能将准确性提升最多7.14%，达到83.12%的精度；实际幻觉是判断不正确性的强信号，但仅凭元数据并不足以可靠预测幻觉； prompting 策略改变了模型的内部行为模式，并显著影响元数据在预测中的效用，但并未改变总体准确度。

Conclusion: 该研究为开发基于元数据的可靠性信号提供了有前景的方向，但所提出的方法尚不足以在高风险场景中实现鲁棒部署，需要更综合的信号与更强的鲁棒性。

Abstract: Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.

</details>


### [88] [Decomposing Task Vectors for Refined Model Editing](https://arxiv.org/abs/2512.22511)
*Hamed Damirchi,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: 提出一种将任务向量分解为共享知识与唯一信息的正交化方法，通过识别投影的一致子空间，提升任务向量运算的稳定性与控制性，在图像/扩散模型/语言模型等多领域实现多任务合并、风格混合与毒性削减。


<details>
  <summary>Details</summary>
Motivation: 任务向量通过向量运算实现模型行为的迁移与融合，但向量中概念存在重叠，易相互干扰，导致结果不可控，因此需要一种分解方法以实现更精准的行为控制。

Method: 提出基于投影的一致子空间分解，将每个任务向量分解为共享分量和唯一分量，并通过识别跨向量投影的一致子空间来实现对概念的独立操控；利用不变子空间理论实现对向量算术的稳定性。

Result: 在三个领域取得显著效果：1) 图像分类多任务融合中，使用共享分量作为额外任务向量提升5%；2) 扩散模型风格混合中，仅混合唯一分量即可实现无退化的风格混合；3) 语言模型中，通过去除唯一分量中的毒性信息实现47%毒性降低，同时保持对一般知识任务的性能。

Conclusion: 为任务向量运算提供新的理解与控制框架，解决模型编辑操作的基本局限性，提升跨任务协作的可控性与稳定性。

Abstract: Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows for the seamless combination of desired behaviors without the need for large datasets. However, these vectors often contain overlapping concepts that can interfere with each other during arithmetic operations, leading to unpredictable outcomes. We propose a principled decomposition method that separates each task vector into two components: one capturing shared knowledge across multiple task vectors, and another isolating information unique to each specific task. By identifying invariant subspaces across projections, our approach enables more precise control over concept manipulation without unintended amplification or diminution of other behaviors. We demonstrate the effectiveness of our decomposition method across three domains: improving multi-task merging in image classification by 5% using shared components as additional task vectors, enabling clean style mixing in diffusion models without generation degradation by mixing only the unique components, and achieving 47% toxicity reduction in language models while preserving performance on general knowledge tasks by negating the toxic information isolated to the unique component. Our approach provides a new framework for understanding and controlling task vector arithmetic, addressing fundamental limitations in model editing operations.

</details>


### [89] [Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks](https://arxiv.org/abs/2512.22522)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 提出一种更可靠的SNN对抗鲁棒性评估框架：自适应锐度替代梯度（ASSG）与自适应步长的SA-PGD攻击，显著提升对抗攻击效果并揭示SNN鲁棒性的潜在高估。


<details>
  <summary>Details</summary>
Motivation: SNN的尖峰激活本质的二值性和不连续性导致梯度消失，现有替代梯度在强对抗下效果不确定，难以提供稳定、可比较的鲁棒性评估。需要一个理论分析与实证并行的框架来更可靠地评估SNN的对抗鲁棒性。

Method: 理论分析替代梯度中的梯度消失程度；提出Adaptive Sharpness Surrogate Gradient (ASSG)，在攻击迭代中根据输入分布自适应调整替代函数形状以提高梯度准确性并缓解梯度消失；设计Stable Adaptive PGD (SA-PGD)，在L∞约束下自适应步长以实现更快且对不精确梯度更稳健的收敛。

Result: 实验显示，ASSG与SA-PGD在多种对抗训练策略、SNN结构和神经元模型上显著提高了攻击成功率，证明该评估框架在更广泛场景下具有更强的普适性和可靠性；同时表明当前SNN鲁棒性被高估，需要更可靠的对抗训练方法。

Conclusion: 该工作提供了更可靠的SNN对抗鲁棒性评估框架，并揭示了现有SNN鲁棒性估计的局限性，强调需要开发更有效的对抗训练策略以获得真实鲁棒性。

Abstract: Spiking Neural Networks (SNNs) utilize spike-based activations to mimic the brain's energy-efficient information processing. However, the binary and discontinuous nature of spike activations causes vanishing gradients, making adversarial robustness evaluation via gradient descent unreliable. While improved surrogate gradient methods have been proposed, their effectiveness under strong adversarial attacks remains unclear. We propose a more reliable framework for evaluating SNN adversarial robustness. We theoretically analyze the degree of gradient vanishing in surrogate gradients and introduce the Adaptive Sharpness Surrogate Gradient (ASSG), which adaptively evolves the shape of the surrogate function according to the input distribution during attack iterations, thereby enhancing gradient accuracy while mitigating gradient vanishing. In addition, we design an adversarial attack with adaptive step size under the $L_\infty$ constraint-Stable Adaptive Projected Gradient Descent (SA-PGD), achieving faster and more stable convergence under imprecise gradients. Extensive experiments show that our approach substantially increases attack success rates across diverse adversarial training schemes, SNN architectures and neuron models, providing a more generalized and reliable evaluation of SNN adversarial robustness. The experimental results further reveal that the robustness of current SNNs has been significantly overestimated and highlighting the need for more dependable adversarial training methods.

</details>


### [90] [TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting](https://arxiv.org/abs/2512.22550)
*Jaebin Lee,Hankook Lee*

Main category: cs.LG

TL;DR: TimePerceiver propone un marco unificado de codificador-decodificador para predicción de series temporales, que alinea el diseño del encoder, el decodificador y el entrenamiento, abarcando extrapolación, interpolación e imputación, con una arquitectura de obturador latente y consultas aprendibles para decodificación; exhibe mejoras significativas en benchmarks y ofrece código abierto.


<details>
  <summary>Details</summary>
Motivation: La literatura previa se centra principalmente en el diseño del encoder para series temporales, tratando la predicción y el entrenamiento como problemas secundarios o independientes. Esto limita la capacidad de aprovechar al máximo la interacción entre codificación, predicción y entrenamiento. Se busca un marco unificado que optimice estas piezas de forma conjunta.

Method: Presenta TimePerceiver, un marco de forecasting con encoder-decoder. Generaliza la tarea para incluir extrapolación, interpolación e imputación, requiere manejar segmentos de entrada y objetivo en posiciones arbitrarias. Introduce representaciones latentes de cuello de botella que interactúan con todos los segmentos de entrada para capturar dependencias temporales y entre canales. En la fase de decodificación, utiliza consultas aprendibles para timestamps objetivo para recuperar información relevante.

Result: A través de experimentos extensivos, el marco TimePerceiver supera consistentemente a baselines de vanguardia en una amplia gama de datasets de referencia. El código está disponible públicamente.

Conclusion: TimePerceiver ofrece un marco de codificador-decodificador unificado, alineado con una estrategia de entrenamiento efectiva, adaptable a diversas tareas temporales (extrapolación, interpolación, imputación) y con mejoras de rendimiento significativas.

Abstract: In machine learning, effective modeling requires a holistic consideration of how to encode inputs, make predictions (i.e., decoding), and train the model. However, in time-series forecasting, prior work has predominantly focused on encoder design, often treating prediction and training as separate or secondary concerns. In this paper, we propose TimePerceiver, a unified encoder-decoder forecasting framework that is tightly aligned with an effective training strategy. To be specific, we first generalize the forecasting task to include diverse temporal prediction objectives such as extrapolation, interpolation, and imputation. Since this generalization requires handling input and target segments that are arbitrarily positioned along the temporal axis, we design a novel encoder-decoder architecture that can flexibly perceive and adapt to these varying positions. For encoding, we introduce a set of latent bottleneck representations that can interact with all input segments to jointly capture temporal and cross-channel dependencies. For decoding, we leverage learnable queries corresponding to target timestamps to effectively retrieve relevant information. Extensive experiments demonstrate that our framework consistently and significantly outperforms prior state-of-the-art baselines across a wide range of benchmark datasets. The code is available at https://github.com/efficient-learning-lab/TimePerceiver.

</details>


### [91] [On Admissible Rank-based Input Normalization Operators](https://arxiv.org/abs/2512.22587)
*Taeyun Kim*

Main category: cs.LG

TL;DR: 从理论上界定对基于秩的输入归一化的最低不变性需求，给出三条公理、分解结构，以及一个同时满足公理的最小算子，明确区分于现有的连续松弛排序方法。


<details>
  <summary>Details</summary>
Motivation: 在真实系统中，特征值的排序信息往往比数值大小更关键，而现有可微排序/排名算子在严格单调变换、批次变化和微小扰动下不稳定；需要一个形式化的稳定性框架来界定有效的秩基归一化算子。

Method: 首先指出现有可微排序/排名算子因其结构设计在稳定性方面的缺陷；提出三条公理，描述最低限度的不变性与稳定性要求；证明任意满足公理的算子必须分解为(1) 每个特征维的秩表示，以及(2) 一个单调且Lipschitz连续的标量化映射；构建一个满足这些公理的最小化算子；并通过实证验证该约束在现实场景中非平凡性，并将其与基于连续松弛的排序方法对比。

Result: 证明了现有的 differentiable sorting/ranking 运算在严格单调变换、批次组合变化和微小输入扰动下不稳定；提出并实现一个符合三条公理的最小算子，验证其在现实设置中的非平凡性；明确区分了基于秩的归一化与现有的连续松弛排序方法的本质差异。

Conclusion: 界定了有效的基于秩的归一化算子的设计空间，将其与传统的连续松弛排序方法区分开来，并为后续在鲁棒性与不变性方面的改进提供理论基础。

Abstract: Rank-based input normalization is a workhorse of modern machine learning, prized for its robustness to scale, monotone transformations, and batch-to-batch variation. In many real systems, the ordering of feature values matters far more than their raw magnitudes - yet the structural conditions that a rank-based normalization operator must satisfy to remain stable under these invariances have never been formally pinned down.
  We show that widely used differentiable sorting and ranking operators fundamentally fail these criteria. Because they rely on value gaps and batch-level pairwise interactions, they are intrinsically unstable under strictly monotone transformations, shifts in mini-batch composition, and even tiny input perturbations. Crucially, these failures stem from the operators' structural design, not from incidental implementation choices.
  To address this, we propose three axioms that formalize the minimal invariance and stability properties required of rank-based input normalization. We prove that any operator satisfying these axioms must factor into (i) a feature-wise rank representation and (ii) a scalarization map that is both monotone and Lipschitz-continuous. We then construct a minimal operator that meets these criteria and empirically show that the resulting constraints are non-trivial in realistic setups. Together, our results sharply delineate the design space of valid rank-based normalization operators and formally separate them from existing continuous-relaxation-based sorting methods.

</details>


### [92] [Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining](https://arxiv.org/abs/2512.22589)
*Jewel Rana Palit,Vijayalakshmi K Kumarasamy,Osama A. Osman*

Main category: cs.LG

TL;DR: 本研究利用美国NHTSA关于SAE Level 2与4的2500余起自动驾驶汽车（AV）碰撞记录，通过两阶段数据挖掘：先用K-means对记录进行4类行为簇分群，再在每簇内使用关联规则挖掘（ARM）揭示碰撞模式与成因（照明、路面状况、车辆动力学、环境等）的多变量关系，旨在为开发者、监管者和政策制定者提供可操作的部署与降风险策略。


<details>
  <summary>Details</summary>
Motivation: 填补以往多聚焦于加州、小样本的数据分析空白，提升对AV在混合交通环境中的碰撞动态理解，进而促进更安全、可靠的AV部署。

Method: 两阶段数据挖掘框架：第一阶段基于时间、空间与环境因素对碰撞记录进行K-means聚类，得到4个行为簇；第二阶段在每个簇内应用关联规则挖掘，抽取碰撞模式与成因（如照明、路面状况、车辆动力学、环境条件）之间的可解释多变量关系。

Result: 得到4个可解释的碰撞行为簇，并在各簇内识别出跨变量的关联规则，揭示不同情境下的碰撞成因分布与关键贡献因素，形成对AV开发、监管与部署的可操作性建议。

Conclusion: 该框架能够在较大、真实世界的US数据上系统揭示AV碰撞的潜在动态，有助于制定针对不同情境的安全策略与规制框架，提升混合交通环境中的AV安全与可靠性。

Abstract: Automated Vehicles (AV) hold potential to reduce or eliminate human driving errors, enhance traffic safety, and support sustainable mobility. Recently, crash data has increasingly revealed that AV behavior can deviate from expected safety outcomes, raising concerns about the technology's safety and operational reliability in mixed traffic environments. While past research has investigated AV crash, most studies rely on small-size California-centered datasets, with a limited focus on understanding crash trends across various SAE Levels of automation. This study analyzes over 2,500 AV crash records from the United States National Highway Traffic Safety Administration (NHTSA), covering SAE Levels 2 and 4, to uncover underlying crash dynamics. A two-stage data mining framework is developed. K-means clustering is first applied to segment crash records into 4 distinct behavioral clusters based on temporal, spatial, and environmental factors. Then, Association Rule Mining (ARM) is used to extract interpretable multivariate relationships between crash patterns and crash contributors including lighting conditions, surface condition, vehicle dynamics, and environmental conditions within each cluster. These insights provide actionable guidance for AV developers, safety regulators, and policymakers in formulating AV deployment strategies and minimizing crash risks.

</details>


### [93] [Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification](https://arxiv.org/abs/2512.22597)
*Guikun Xu,Xiaohan Yi,Peilin Zhao,Yatao Bian*

Main category: cs.LG

TL;DR: EnFlow 通过将流式匹配（FM）与显式学习的能量模型相结合，在非高斯FM路径上定义能量梯度引导采样，实现低能量构象集合的高效生成与地面态识别的统一框架。


<details>
  <summary>Details</summary>
Motivation: 当前的生成方法要么在多样性与覆盖性方面不足，要么缺乏可靠的能量标定，难以同时生成多样的构象并正确识别低能态；需要一种能够高效地得到低能量构象并进行能量分级的统一方法。

Method: 提出能量引导采样的 Flow Matching 框架（EnFlow），在非高斯 FM 路径上通过显式学习的能量函数对采样过程进行能量梯度引导，使轨迹向更低能量区域收敛；利用学习的能量函数进行能量基排序以识别地面态；在 GEOM-QM9 与 GEOM-Drugs 上进行广泛实验，对比现有方法。

Result: 在仅需1–2步常微分方程求解的情况下，EnFlow 同时提升生成指标并降低地面态预测误差，相比最先进方法具有显著改进。

Conclusion: EnFlow 提供一个统一且高效的生成与识别框架，通过能量引导和能量模型实现对低能量构象的更准确捕获与排序，适用于分子构象嵌套的任务。

Abstract: Generating low-energy conformer ensembles and identifying ground-state conformations from molecular graphs remain computationally demanding with physics-based pipelines. Current learning-based approaches often suffer from a fragmented paradigm: generative models capture diversity but lack reliable energy calibration, whereas deterministic predictors target a single structure and fail to represent ensemble variability. Here we present EnFlow, a unified framework that couples flow matching (FM) with an explicitly learned energy model through an energy-guided sampling scheme defined along a non-Gaussian FM path. By incorporating energy-gradient guidance during sampling, our method steers trajectories toward lower-energy regions, substantially improving conformational fidelity, particularly in the few-step regime. The learned energy function further enables efficient energy-based ranking of generated ensembles for accurate ground-state identification. Extensive experiments on GEOM-QM9 and GEOM-Drugs demonstrate that EnFlow simultaneously improves generation metrics with 1--2 ODE-steps and reduces ground-state prediction errors compared with state-of-the-art methods.

</details>


### [94] [Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations](https://arxiv.org/abs/2512.22672)
*Achraf Hsain,Fouad Mohammed Abbou*

Main category: cs.LG

TL;DR: 首次将量子生成模型应用于CFD数据的离散潜在空间；通过VQ-VAE将涡度场压缩到7维离散潜在向量，并比较QCBM、QGAN与LSTM，量子模型在分布拟合上优于经典基线，QCBM表现最佳。


<details>
  <summary>Details</summary>
Motivation: 弥补现有工作在物理系统潜在空间上的量子生成建模不足，评估量子与经典方法在压缩后的物理数据分布建模中的性能差异，并提供一个端到端的开源流水线。

Method: 建立GPU加速的LBM CFD仿真以生成涡度场；使用VQ-VAE将场数据离散化并压缩成7维潜在表示；分别训练并比较QCBM、QGAN与LSTM在同一数据集上的生成能力，评估分布距离等指标，并提供完整的开源管线。

Result: 实验表明，量子模型在样本对真实分布的最小距离上优于LSTM，其中QCBM的表现最佳；两种量子模型提供了对物理仿真潜在分布的更好拟合。还提供了完整的开源实现，首次在 compressed physics latent representations 上进行量子生成建模的实验。

Conclusion: 为CFD等物理仿真数据中的离散潜在表示上的量子生成建模提供初步证据，确立了开源管线和基线，奠定未来在物理系统中对量子机器学习的研究基础。

Abstract: This paper presents the first application of quantum generative models to learned latent space representations of computational fluid dynamics (CFD) data. While recent work has explored quantum models for learning statistical properties of fluid systems, the combination of discrete latent space compression with quantum generative sampling for CFD remains unexplored. We develop a GPU-accelerated Lattice Boltzmann Method (LBM) simulator to generate fluid vorticity fields, which are compressed into a discrete 7-dimensional latent space using a Vector Quantized Variational Autoencoder (VQ-VAE). The central contribution is a comparative analysis of quantum and classical generative approaches for modeling this physics-derived latent distribution: we evaluate a Quantum Circuit Born Machine (QCBM) and Quantum Generative Adversarial Network (QGAN) against a classical Long Short-Term Memory (LSTM) baseline. Under our experimental conditions, both quantum models produced samples with lower average minimum distances to the true distribution compared to the LSTM, with the QCBM achieving the most favorable metrics. This work provides: (1)~a complete open-source pipeline bridging CFD simulation and quantum machine learning, (2)~the first empirical study of quantum generative modeling on compressed latent representations of physics simulations, and (3)~a foundation for future rigorous investigation at this intersection.

</details>


### [95] [What Matters in Deep Learning for Time Series Forecasting?](https://arxiv.org/abs/2512.22702)
*Valentina Moretti,Andrea Cini,Ivan Marisca,Cesare Alippi*

Main category: cs.LG

TL;DR: 本文从设计维度出发分析时序 forecasting 的深度学习架构设计，强调局部性与全局性等原则的重要性；指出许多结果受实现细节影响，简单且设计良好的架构往往能达到或接近最先进性能；呼吁改进基准评估并提出辅助预测模型卡以刻画设计选择。


<details>
  <summary>Details</summary>
Motivation: 在大量新提出的架构与相互矛盾的实验结果背景下，难以判断哪些组件真正提升性能，需要基于对时间序列分组的 forecasting 原则来引导设计；并审视现有基准评估对结果的影响。

Method: 通过讨论设计维度与权衡、分析 locality 与 globality 的作用、对比不同结构层（如序列建模层）的影响、揭示实现细节对结果的影响，并提出一个辅助 forecasting 模型卡作为评价框架。

Result: 关键发现包括：考虑局部性与全局性在实现有效预测中比盲目采用某种时序建模层更为重要；简单、设计良好的架构往往能达到与最先进模型相近的性能；实现细节能够本质改变方法的类别并显著影响实证结果。

Conclusion: 呼吁重新审视现有基准实验的设计，聚焦预测问题的基础要素；提出辅助 forecasting 模型卡作为描述和比较现有与新架构的工具。

Abstract: Deep learning models have grown increasingly popular in time series applications. However, the large quantity of newly proposed architectures, together with often contradictory empirical results, makes it difficult to assess which components contribute significantly to final performance. We aim to make sense of the current design space of deep learning architectures for time series forecasting by discussing the design dimensions and trade-offs that can explain, often unexpected, observed results. This paper discusses the necessity of grounding model design on principles for forecasting groups of time series and how such principles can be applied to current models. In particular, we assess how concepts such as locality and globality apply to recent forecasting architectures. We show that accounting for these aspects can be more relevant for achieving accurate results than adopting specific sequence modeling layers and that simple, well-designed forecasting architectures can often match the state of the art. We discuss how overlooked implementation details in existing architectures (1) fundamentally change the class of the resulting forecasting method and (2) drastically affect the observed empirical results. Our results call for rethinking current faulty benchmarking practices and the need to focus on the foundational aspects of the forecasting problem when designing architectures. As a step in this direction, we propose an auxiliary forecasting model card, whose fields serve to characterize existing and new forecasting architectures based on key design choices.

</details>


### [96] [FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents](https://arxiv.org/abs/2512.22733)
*Jiaqi Shao,Yufeng Miao,Wei Zhang,Bing Luo*

Main category: cs.LG

TL;DR: FoldAct introduces targeted techniques to stabilize long-horizon RL with context folding by decoupling gradient signals for summary vs. action tokens, enforcing context-consistency to mitigate distribution shift, and applying selective segment training to cut computation, achieving 5.19× speedup.


<details>
  <summary>Details</summary>
Motivation: In long-horizon RL with large language models, unbounded context growth leads to context folding, which replaces parts of history with summary tokens. These summaries alter the agent’s observation space, making the observation distribution policy-dependent and non-stationary, which breaks core RL assumptions and harms learning efficiency.

Method: FoldAct proposes three innovations: (1) separated loss computation to provide independent gradient signals for summary tokens and action tokens; (2) full context consistency loss to reduce distribution shift between training and deployment; (3) selective segment training to reduce computational cost by focusing updates on essential context segments.

Result: The framework enables more stable training of long-horizon search agents under context folding and claims a 5.19× speedup in training efficiency, addressing non-stationary observation while maintaining performance.

Conclusion: FoldAct addresses three fundamental challenges posed by context folding—gradient dilution, self-conditioning, and computational cost—by redesigning loss signals, enforcing context consistency, and optimizing training scope, thus improving stability and efficiency for long-horizon RL with language models.

Abstract: Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \textbf{FoldAct}\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\times$ speedup.

</details>


### [97] [When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction](https://arxiv.org/abs/2512.22740)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: 多任务学习在材料性质预测中的表现呈现双轨：对分类任务有益，但对回归任务造成显著负迁移，主因是数据不平衡与任务间相关性较低。


<details>
  <summary>Details</summary>
Motivation: 检验相关物理性质是否能通过多任务学习共享底层表征，以提升在材料领域的预测性能；以54,028个合金样本同时预测电阻率、维氏硬度和非晶形成能力。

Method: 将单任务、标准MTL和结构化MTL进行对比，在回归（电阻率、硬度）和分类（非晶形成能力）任务上评估性能，采用R^2（回归）和F1/召回率（分类），并分析任务间权重和数据量不平衡对结果的影响。

Result: MTL显著降低回归性能（电阻率R^2从0.897降至0.844；硬度R^2从0.832降至0.694，p<0.01），而提高分类性能（非晶形成F1从0.703升至0.744，p<0.05，召回率提升17%）。分析显示任务间权重接近为零，表明属性之间独立性强；回归失败归因于负迁移，主要由于数据严重不平衡（52k样本对比800样本）。

Conclusion: 应将回归任务独立建模以获得更精确的预测；MTL可保留用于分类任务，尤其是在对召回率有高要求的场景。

Abstract: Multi-task learning (MTL) assumes related material properties share underlying physics that can be leveraged for better predictions. We test this by simultaneously predicting electrical resistivity, Vickers hardness, and amorphous-forming ability using 54,028 alloy samples. We compare single-task models against standard and structured MTL. Results reveal a striking dichotomy: MTL significantly degrades regression performance (resistivity $R^2$: 0.897 $\to$ 0.844; hardness $R^2$: 0.832 $\to$ 0.694, $p < 0.01$) but improves classification (amorphous F1: 0.703 $\to$ 0.744, $p < 0.05$; recall +17%). Analysis shows near-zero inter-task weights, indicating property independence. Regression failure is attributed to negative transfer caused by severe data imbalance (52k vs. 800 samples). We recommend independent models for precise regression, while reserving MTL for classification tasks where recall is critical.

</details>


### [98] [Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL](https://arxiv.org/abs/2512.22744)
*Rihong Qiu,Zhibang Yang,Xinke Jiang,Weibin Liao,Xin Gao,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: 提出 HEROSQL，通过分层的全局意图 LP 与局部 AST 细节、NMPNN 信息传递，以及 AST 驱动的子 SQL 增强，提升文本到 SQL 的语义校验效果，在域内外基准上显著提升 AUPRC 与 AUROC。


<details>
  <summary>Details</summary>
Motivation: 现有 Text-to-SQL 验证多关注句法正确性，缺乏对语义层面的验证，难以捕捉用户意图与 SQL 结构错配，同时缺乏高质量的细粒度子 SQL 注释来支撑鲁棒优化。

Method: 提出 HEROSQL：将全局意图（通过逻辑计划 LPs）与局部细节（通过抽象语法树 ASTs）整合的分层 SQL 表征；使用 Nested Message Passing Neural Network（NMPNN）在 LPs、ASTs 及模式之间传播信息并聚合模式相关语义；提出 AST 驱动的子 SQL 增强策略以生成高质量负样本，提升对细粒度语义不一致的鲁棒性。

Result: 在 Text-to-SQL 验证基准（同域与跨域）上，平均提升 AUPRC 9.40%、AUROC 12.35%，在检测细粒度语义错误方面表现突出，提供更 granular 的反馈，提升系统的可靠性与可解释性。

Conclusion: 通过分层表征和图神经信息传递，结合面向 AST 的子 SQL 增强，显著提升文本到 SQL 的语义校验能力，增强系统的可靠性与可解释性。

Abstract: Text-to-SQL translates natural language questions into SQL statements grounded in a target database schema. Ensuring the reliability and executability of such systems requires validating generated SQL, but most existing approaches focus only on syntactic correctness, with few addressing semantic validation (detecting misalignments between questions and SQL). As a consequence, effective semantic validation still faces two key challenges: capturing both global user intent and SQL structural details, and constructing high-quality fine-grained sub-SQL annotations. To tackle these, we introduce HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans, LPs) and local details (via Abstract Syntax Trees, ASTs). To enable better information propagation, we employ a Nested Message Passing Neural Network (NMPNN) to capture inherent relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. Additionally, to generate high-quality negative samples, we propose an AST-driven sub-SQL augmentation strategy, supporting robust optimization of fine-grained semantic inconsistencies. Extensive experiments conducted on Text-to-SQL validation benchmarks (both in-domain and out-of-domain settings) demonstrate that our approach outperforms existing state-of-the-art methods, achieving an average 9.40% improvement of AUPRC and 12.35% of AUROC in identifying semantic inconsistencies. It excels at detecting fine-grained semantic errors, provides large language models with more granular feedback, and ultimately enhances the reliability and interpretability of data querying platforms.

</details>


### [99] [A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants](https://arxiv.org/abs/2512.22758)
*Eswarasanthosh Kumar Mamillapalli,Nishtha Sharma*

Main category: cs.LG

TL;DR: 提出并验证一个微观-宏观机器学习框架，将个体层数据与宏观环境数据融合，用多模型比较来预测儿童肥胖并揭示环境脆弱性与地理分布的关系，XGBoost表现最佳，EnvScore指示环境负荷与肥胖风险的地理一致性。 


<details>
  <summary>Details</summary>
Motivation: 弥合个体因素与环境层面的研究断层，探索结构性环境条件如何与个人特征交互影响肥胖风险，并提出可扩展的多尺度建模框架以支持干预规划和实时分析。

Method: 整合 NHANES 微观数据（个体化身高、体重、性别、社会经济状态等）与 USDA/EPA 的宏观环境特征（食品获取、空气质量、社会经济脆弱性等），训练四种机器学习模型（Logistic Regression、Random Forest、XGBoost、LightGBM）用于肥胖预测；开发一个基于标准化指标的州级 EnvScore 环境脆弱性指数；进行多尺度地理比较以评估环境负荷与微观肥胖风险分布的相似性。

Result: XGBoost 为最佳模型；EnvScore 在州级聚合时与全国微观肥胖风险分布呈强相关；证明多尺度数据整合在揭示环境驱动的肥胖风险与分布方面的可行性。

Conclusion: 提出一个可扩展的数据驱动多层建模流程，具备公共卫生信息学应用潜力，未来可拓展至因果推断、干预设计与实时监测等场景。

Abstract: Childhood obesity remains a major public health challenge in the United States, strongly influenced by a combination of individual-level, household-level, and environmental-level risk factors. Traditional epidemiological studies typically analyze these levels independently, limiting insights into how structural environmental conditions interact with individual-level characteristics to influence health outcomes. In this study, we introduce a micro-macro machine learning framework that integrates (1) individual-level anthropometric and socioeconomic data from NHANES and (2) macro-level structural environment features, including food access, air quality, and socioeconomic vulnerability extracted from USDA and EPA datasets. Four machine learning models Logistic Regression, Random Forest, XGBoost, and LightGBM were trained to predict obesity using NHANES microdata. XGBoost achieved the strongest performance. A composite environmental vulnerability index (EnvScore) was constructed using normalized indicators from USDA and EPA at the state level. Multi-level comparison revealed strong geographic similarity between states with high environmental burden and the nationally predicted micro-level obesity risk distribution. This demonstrates the feasibility of integrating multi-scale datasets to identify environment-driven disparities in obesity risk. This work contributes a scalable, data-driven, multi-level modeling pipeline suitable for public health informatics, demonstrating strong potential for expansion into causal modeling, intervention planning, and real-time analytics.

</details>


### [100] [Understanding the Mechanisms of Fast Hyperparameter Transfer](https://arxiv.org/abs/2512.22768)
*Nikhil Ghosh,Denny Wu,Alberto Bietti*

Main category: cs.LG

TL;DR: Scale-aware hyperparameters enable transfer of optimal HPs across model scales with minimal loss; fast transfer means suboptimality shrinks faster than finite-scale performance gap; μP shows fast transfer under certain problem structures; a width-stable vs width-sensitive decomposition explains practice, supported by synthetic tests and LLM pretraining.


<details>
  <summary>Details</summary>
Motivation: Reduce compute and time costs of hyperparameter tuning when scaling deep learning models; provide a unified framework to understand when HP transfer is beneficial and how to reason about scale-induced changes in optimal HPs.

Method: Develop a general conceptual framework for HP transfer across scale; formalize transfer as fast when suboptimality vanishes faster than the finite-scale gap; prove equivalence between fast transfer and compute-efficient grid search under certain conditions; construct synthetic settings to delineate when transfer is provably advantageous or not under μP; conjecture a decomposition of the optimization trajectory into width-stable and width-sensitive components; validate with empirical experiments including large language model pretraining.

Result: Formal condition: fast transfer is equivalent to useful transfer for compute-optimal grid search; μP yields fast transfer in some width-based scaling regimes but not universally; synthetic counterexamples show cases where transfer can fail; conjectured two-component loss decomposition explains empirical fast transfer; empirical evidence across diverse tasks supports the hypothesis.

Conclusion: Problem structure governs transfer efficacy; a width-stable component sets HP optima while width-sensitive changes provide width-related gains with limited HP perturbation; recognizing this decomposition helps explain observed fast transfer and guides future work to validate the conjecture and extend to broader scale settings and performance metrics.

Abstract: The growing scale of deep learning models has rendered standard hyperparameter (HP) optimization prohibitively expensive. A promising solution is the use of scale-aware hyperparameters, which can enable direct transfer of optimal HPs from small-scale grid searches to large models with minimal performance loss. To understand the principles governing such transfer strategy, we develop a general conceptual framework for reasoning about HP transfer across scale, characterizing transfer as fast when the suboptimality it induces vanishes asymptotically faster than the finite-scale performance gap. We show formally that fast transfer is equivalent to useful transfer for compute-optimal grid search, meaning that transfer is asymptotically more compute-efficient than direct tuning. While empirical work has found that the Maximal Update Parameterization ($μ$P) exhibits fast transfer when scaling model width, the mechanisms remain poorly understood. We show that this property depends critically on problem structure by presenting synthetic settings where transfer either offers provable computational advantage or fails to outperform direct tuning even under $μ$P. To explain the fast transfer observed in practice, we conjecture that decomposing the optimization trajectory reveals two contributions to loss reduction: (1) a width-stable component that determines the optimal HPs, and (2) a width-sensitive component that improves with width but weakly perturbs the HP optimum. We present empirical evidence for this hypothesis across various settings, including large language model pretraining.

</details>


### [101] [GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks](https://arxiv.org/abs/2512.22772)
*Xuyan Li,Jie Wang,Zheng Yan*

Main category: cs.LG

TL;DR: GRExplainer 是首个普适、有效且易用的 TGNN 解释方法。通过将输入节点序列作为统一特征表示，兼容快照型和事件型 TGNN，利用广度优先搜索和时间信息构建序列以提升效率，并以基于 RNN 的生成模型实现自动、持续的解释生成。在六个真实数据集、三种 TGNN 的实验中，GRExplainer 在通用性、效率和用户友好性方面优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前 TGNN 解释方法多针对特定模型、计算成本高且往往忽略图的结构连通性，依赖先验知识，用户友好性不足，难以大规模应用。因此需要一个通用、高效且易于使用的 TGNN 解释框架。

Method: 提出 GRExplainer，提取节点序列作为统一的输入特征表示，使其与具体输入格式无关，适用于快照型和事件型 TGNN；通过广度优先搜索和时间信息构建输入节点序列以减少冗余计算、提高效率；引入基于 RNN 的生成模型实现自动化、连续的解释生成。

Result: 在六个真实数据集、三种目标 TGNN 上的实验表明，GRExplainer 在通用性、效率和用户友好性方面超过现有基线方法。

Conclusion: GRExplainer 成为首个普适、高效、易用的 TGNN 解释框架，有望提升 TGNN 应用中的可解释性与可用性。

Abstract: Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.

</details>


### [102] [Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization](https://arxiv.org/abs/2512.22774)
*Truong Son Nguyen*

Main category: cs.LG

TL;DR: Schrödinger AI proposes a physics-inspired ML framework integrating three components: a time-independent wave-energy solver for spectral perception via a learned Hamiltonian, a time-dependent dynamical solver for evolving semantic wavefunctions and context-aware reasoning, and a low-rank operator calculus learning quantum-like transition operators for symbolic transformations. Together, they form a physics-driven alternative to standard cross-entropy and attention-based models, aiming for robust generalization, interpretable semantics, and emergent topology.


<details>
  <summary>Details</summary>
Motivation: To develop a unified, physics-inspired framework that transcends conventional deep learning training paradigms (e.g., cross-entropy, transformer attention), enabling interpretable semantic structure, robust generalization, and adaptive reasoning under changing environments.

Method: Coupled architecture with three components: (1) a time-independent wave-energy solver performing spectral decomposition under a learned Hamiltonian to represent perception/classification; (2) a time-dependent dynamical solver governing temporal evolution of semantic wavefunctions for context-aware revision and routing; (3) a low-rank operator calculus learning symbolic quantum-like transformations, such as modular arithmetic, via learned transition operators, thereby capturing group actions.

Result: Empirically, the approach yields emergent semantic manifolds aligned with human class relations without explicit supervision; dynamic reasoning adapting to environmental perturbations (e.g., navigation under potential-field changes); and exact operator generalization on modular arithmetic tasks, with the ability to compose learned actions across sequences beyond training length.

Conclusion: Schrödinger AI points to a foundational direction where learning is framed as discovering and navigating an underlying semantic energy landscape, offering a unified physics-driven alternative to traditional deep learning training and attention mechanisms with potential for interpretable semantics and robust generalization.

Abstract: We introduce \textbf{Schrödinger AI}, a unified machine learning framework inspired by quantum mechanics. The system is defined by three tightly coupled components: (1) a {time-independent wave-energy solver} that treats perception and classification as spectral decomposition under a learned Hamiltonian; (2) a {time-dependent dynamical solver} governing the evolution of semantic wavefunctions over time, enabling context-aware decision revision, re-routing, and reasoning under environmental changes; and (3) a {low-rank operator calculus} that learns symbolic transformations such as modular arithmetic through learned quantum-like transition operators. Together, these components form a coherent physics-driven alternative to conventional cross-entropy training and transformer attention, providing robust generalization, interpretable semantics, and emergent topology.
  Empirically, Schrödinger AI demonstrates: (a) emergent semantic manifolds that reflect human-conceived class relations without explicit supervision; (b) dynamic reasoning that adapts to changing environments, including maze navigation with real-time potential-field perturbations; and (c) exact operator generalization on modular arithmetic tasks, where the system learns group actions and composes them across sequences far beyond training length. These results suggest a new foundational direction for machine learning, where learning is cast as discovering and navigating an underlying semantic energy landscape.

</details>


### [103] [Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning](https://arxiv.org/abs/2512.22777)
*Kasra Jalaldoust,Elias Bareinboim*

Main category: cs.LG

TL;DR: 本工作提出一种基于因果传输性的框架Circuit-TR，用于零-shot组合泛化和少样本域适应。通过源域数据中学习的局部预测模块，在已知因果结构和跨域差异 oracle 的约束下进行传输与组合，从而构建目标域的预测电路。还给出无需显式因果结构、仅靠少量目标数据即可进行监督域适配的策略。理论上将少-shot 学习任务与图结构化的电路传输性联系起来，并将其与电路大小复杂度相连；大量仿真实验对理论结果提供支持。


<details>
  <summary>Details</summary>
Motivation: 跨域泛化在缺乏对目标域结构认知时几乎不可能实现。引入因果传输性框架，利用域内因果结构与域间机制共享的知识约束，来实现零-shot 的组合泛化，并在有限目标数据下实现监督域适配。

Method: 从源数据学习一组局部 predictor 模块，利用因果结构许可的前提对这些模块进行传输与组合，形成用于目标域预测的电路（Circuit-TR）。若无显式因果结构也可通过带有少量目标数据的监督域适配实现跨域传输；跨域差异通过“discrepancies oracle”进行机制层面的共享。理论部分给出基于图结构的电路传输性准则来刻画少-shot 可学习任务，并把少-shot 泛化与电路大小复杂度联系起来。

Result: 理论上给出少-shot 学习任务的图结构化电路传输性分类准则，并将其与电路大小复杂度关联；通过受控仿真实验验证理论结论，显示在给定因果结构约束下的零-shot/少-shot 泛化能力提升。

Conclusion: Circuit-TR 提供了一种统一的因果传輸框架，用于零-shot 组合泛化和少样本域适配；在需要结构约束的跨域任务中，可通过学习局部模块、使用因果图与 discrepancy oracle 进行有效传输与组合，并且理论与仿真均支持其有效性。

Abstract: Generalization across the domains is not possible without asserting a structure that constrains the unseen target domain w.r.t. the source domain. Building on causal transportability theory, we design an algorithm for zero-shot compositional generalization which relies on access to qualitative domain knowledge in form of a causal graph for intra-domain structure and discrepancies oracle for inter-domain mechanism sharing. \textit{Circuit-TR} learns a collection of modules (i.e., local predictors) from the source data, and transport/compose them to obtain a circuit for prediction in the target domain if the causal structure licenses. Furthermore, circuit transportability enables us to design a supervised domain adaptation scheme that operates without access to an explicit causal structure, and instead uses limited target data. Our theoretical results characterize classes of few-shot learnable tasks in terms of graphical circuit transportability criteria, and connects few-shot generalizability with the established notion of circuit size complexity; controlled simulations corroborate our theoretical results.

</details>


### [104] [ReDiF: Reinforced Distillation for Few Step Diffusion](https://arxiv.org/abs/2512.22802)
*Amirhossein Tighkhorshid,Zahra Dehghanian,Gholamali Aminian,Chengchun Shi,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 提出一种基于强化学习的扩散模型蒸馏框架，通过将蒸馏过程视为策略优化问题，使学生模型在更少的推理步数下、以更长的步伐逼近教师输出，从而提高效率。该框架对不同类型的扩散模型皆可适用。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在推理阶段的慢采样问题。现有蒸馏多依赖固定的重构或一致性损失，难以在更少一步的条件下保留高质量样本。本文提出的RL驱动蒸馏通过奖励信号引导学生探索多条去噪路径，向高概率数据区域移动。

Method: 将蒸馏过程建模为策略优化问题：学生在教师输出的对齐程度上获得奖励信号，作为强化学习的回报。通过学习一个策略，学生能在更长的去噪步长下搜索生成路径，从而更有效地接近教师的分布。该框架具备模型无关性，适用于任何可提供合适奖励函数的扩散模型。

Result: 实验结果表明，与现有蒸馏方法相比，所提出的方法在推理步数显著更少且计算资源需求更低的情况下仍能获得更高的生成质量。

Conclusion: RL驱动的扩散蒸馏框架为高效扩散学习提供了一般化的优化范式，具有模型无关性，能够在不同扩散模型结构中通过设计合适的奖励函数实现更高效的蒸馏与推理。

Abstract: Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.

</details>


### [105] [MoR: Mixture Of Representations For Mixed-Precision Training](https://arxiv.org/abs/2512.22804)
*Bor-Yiing Su,Peter Dykas,Mike Chrzanowski,Jatin Chhugani*

Main category: cs.LG

TL;DR: 提出Mixture-of-Representations (MoR)框架，基于 per-tensor 和 sub-tensor 的动态量化，在 FP8 与 BF16 之间自适应选择表示形式，以在不同量化分区策略下尽量保持模型质量。初步结果显示高达98.38%的张量被量化为 FP8，且 FP8 精度与现有方法相当，无需细粒度分区即可实现稳健性，具备与 NVFP4 等更低精度格式结合的潜力。


<details>
  <summary>Details</summary>
Motivation: 混合精度训练对扩展深度学习模型至关重要，但要取得成功，需要识别并正确应用一组训练方法。现有研究多使用固定表示形式，难以在不同张量的数值属性下通用化。提出的MoR通过按张量级和子张量级的动态分析，选择最合适的数值表示形式，旨在在不同分区策略和数据集上同时保留模型质量并提高效率。

Method: 引入MoR框架，针对张量的数值属性进行动态分析；开发在 per-tensor 与 sub-tensor 颗粒度上在 FP8 与 BF16 之间切换的具体算法；确保在不同量化分区策略和数据集下具有普适性；在实验中验证该框架的可行性与鲁棒性。

Result: 初步结果显示有98.38%的张量可量化为 FP8；在 FP8 上的精度达到与现有方法相当的水平，无需进行细粒度分区即可实现稳健性；并且该方法具有与其它训练方法结合以进一步利用更低精度格式（如 NVFP4）的潜力。

Conclusion: Dynamic、面向属性的量化（MoR）有望提升混合精度训练的鲁棒性和效率，能够在不同分区策略下泛化并在不增加粒度分区成本的前提下实现高效的低精度表示。该思路还可与其他训练方法结合，推动在更低精度格式上的应用。

Abstract: Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.

</details>


### [106] [Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models](https://arxiv.org/abs/2512.22814)
*Scott A. Martin,Noah Brenowitz,Dale Durran,Michael Pritchard*

Main category: cs.LG

TL;DR: 提出长范围蒸馏：用教师模型生成的大量合成数据，训练直接进行长时预测的学生模型，以提升长范围预测技能并替代多步自回归。


<details>
  <summary>Details</summary>
Motivation: 现有大多数AI天气模型是自回归的，长范围预报易积累错误且对慢模态的训练样本有限；40年的再分析数据也难以覆盖所需样本。需要通过海量数据提高对长时尺度的学习与稳定性。

Method: 提出长范围蒸馏：以短步自回归教师为数据源，生成海量合成训练数据，训练在长时尺度直接预测的学生模型。以DLESyM为教师，生成>1万年模拟气候，训练面向S2S及更长尺度的蒸馏学生。通过完美模型与真实数据评估其预测能力。

Result: 在理想模型中，蒸馏学生超越基线的气候均值并接近自回归教师技能，可用单一步骤替代多步自回归。现实世界中，经过ERA5微调，蒸馏学生达到与ECMWF集合预报相当的S2S技能。随着合成数据量增加，蒸馏模型的技能持续提升，首次证明AI生成的合成数据可扩展长范围预测能力。

Conclusion: 证明通过AI生成的合成训练数据可以显著提升并扩展长范围天气预测技能，提供一种可扩展的、替代多步自回归的新途径。

Abstract: Accurate long-range weather forecasting remains a major challenge for AI models, both because errors accumulate over autoregressive rollouts and because reanalysis datasets used for training offer a limited sample of the slow modes of climate variability underpinning predictability. Most AI weather models are autoregressive, producing short lead forecasts that must be repeatedly applied to reach subseasonal-to-seasonal (S2S) or seasonal lead times, often resulting in instability and calibration issues. Long-timestep probabilistic models that generate long-range forecasts in a single step offer an attractive alternative, but training on the 40-year reanalysis record leads to overfitting, suggesting orders of magnitude more training data are required. We introduce long-range distillation, a method that trains a long-timestep probabilistic "student" model to forecast directly at long-range using a huge synthetic training dataset generated by a short-timestep autoregressive "teacher" model. Using the Deep Learning Earth System Model (DLESyM) as the teacher, we generate over 10,000 years of simulated climate to train distilled student models for forecasting across a range of timescales. In perfect-model experiments, the distilled models outperform climatology and approach the skill of their autoregressive teacher while replacing hundreds of autoregressive steps with a single timestep. In the real world, they achieve S2S forecast skill comparable to the ECMWF ensemble forecast after ERA5 fine-tuning. The skill of our distilled models scales with increasing synthetic training data, even when that data is orders of magnitude larger than ERA5. This represents the first demonstration that AI-generated synthetic training data can be used to scale long-range forecast skill.

</details>


### [107] [TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning](https://arxiv.org/abs/2512.22824)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: 提出一种学生-教师学习范式并结合时间方差驱动的课程安排，用于目标条件强化学习。教师动态聚焦于策略置信度在时间上方差最高的目标，以提高学习样本效率，理论上将Q值的时间方差与策略演化联系起来，并在11个任务上显著优于现有的课程学习与目标选择方法。


<details>
  <summary>Details</summary>
Motivation: 在多目标或目标条件强化学习中，单一的等概率目标采样往往导致样本低效。受到生物系统中适应性与结构化学习的启发，提出一种可自适应、聚焦性的学习信号分发机制，以提升在多目标任务中的学习效率。

Method: 引入Student-Teacher框架，教师模块基于Q值的时间方差来动态优先选择高不确定性的目标，并提供有针对性的学习信号。该方法与具体的RL算法无关，能够无缝集成到现有的RL框架中。

Result: 理论上建立了Q值时间方差与策略演化之间的联系，为方法提供原理支撑。实验在11种机器人操作与迷宫导航任务上进行评估，结果显示在现有课程学习和目标选择方法上具有一致且显著的改进。

Conclusion: 所提出的时间方差驱动课程的Student-Teacher框架具有算法无关性和普适性，能有效提升多目标/目标条件RL的学习效率，并能与现有RL框架无缝对接。

Abstract: Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy's confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method's underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.

</details>


### [108] [Federated Multi-Task Clustering](https://arxiv.org/abs/2512.22897)
*S. Dai,G. Sun,F. Li,X. Tang,Q. Wang,Y. Cong*

Main category: cs.LG

TL;DR: 提出FMTC，在隐私保护的前提下实现个性化联邦多任务聚类，通过客户端映射实现鲁棒出样本推断、服务端张量低秩正则化捕获跨客户端结构，采用ADMM分布式优化并在真实数据上优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决集中式聚类在隐私与跨客户端异质性环境中的局限，克服伪标签不可靠性与跨客户端相关性建模不足的问题。

Method: 两大核心组件：1) 客户端个性化聚类模块，学习参数化映射以实现鲁棒的出样本推断，避免依赖不可靠的伪标签；2) 服务端张量相关性模块，将所有客户端模型组织成联合张量并通过低秩正则化学习共享子空间。全局问题通过基于交替方向乘子法（ADMM）的分布式算法求解，将更新分解为本地并行更新与服务端聚合。

Result: 在多组真实数据集上的实验显示，FMTC显著优于多种基线和现有的联邦聚类算法，展现出优越的性能和鲁棒性。

Conclusion: FMTC有效在保护隐私的前提下实现个人化与共享知识的协同，提升了联邦多任务聚类的泛化与性能。

Abstract: Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches often suffer from poor generalization performance due to reliance on unreliable pseudo-labels, and fail to capture the latent correlations amongst heterogeneous clients. To tackle these limitations, this paper proposes a novel framework named Federated Multi-Task Clustering (i.e.,FMTC), which intends to learn personalized clustering models for heterogeneous clients while collaboratively leveraging their shared underlying structure in a privacy-preserving manner. More specifically, the FMTC framework is composed of two main components: client-side personalized clustering module, which learns a parameterized mapping model to support robust out-of-sample inference, bypassing the need for unreliable pseudo-labels; and server-side tensorial correlation module, which explicitly captures the shared knowledge across all clients. This is achieved by organizing all client models into a unified tensor and applying a low-rank regularization to discover their common subspace. To solve this joint optimization problem, we derive an efficient, privacy-preserving distributed algorithm based on the Alternating Direction Method of Multipliers, which decomposes the global problem into parallel local updates on clients and an aggregation step on the server. To the end, several extensive experiments on multiple real-world datasets demonstrate that our proposed FMTC framework significantly outperforms various baseline and state-of-the-art federated clustering algorithms.

</details>


### [109] [Debugging Tabular Log as Dynamic Graphs](https://arxiv.org/abs/2512.22903)
*Chumeng Liang,Zhanyang Jin,Zahaib Akhtar,Mona Pereira,Haofei Yu,Jiaxuan You*

Main category: cs.LG

TL;DR: GraphLogDebugger 利用对象和事件的异质节点、动态图结构及简单的动态图GNN，对表格日志进行调试，能在真实数据集上优于大语言模型（LLMs）并具有更好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前对文本增强的表格日志依赖大语言模型，存在灵活性和可扩展性问题。需要一种可扩展、可解释的日志调试方法，以在不强依赖昂贵模型的情况下恢复系统结构并发现不一致。

Method: 将对象与事件建模为异质节点，节点间通过边连接，形成动态图来表示系统随时间演化；采用简单的动态图GNN对图进行推理和调试；在真实的计算机系统和学术论文日志数据集上进行实验。

Result: 相较于LLMs，动态图GNN在调试表格日志任务上表现更好，且对大规模数据具有更好的吞吐与灵活性。

Conclusion: 动态图建模为表格日志调试提供了有效且可扩展的替代方案，GraphLogDebugger 验证了其在实际场景中的有效性。

Abstract: Tabular log abstracts objects and events in the real-world system and reports their updates to reflect the change of the system, where one can detect real-world inconsistencies efficiently by debugging corresponding log entries. However, recent advances in processing text-enriched tabular log data overly depend on large language models (LLMs) and other heavy-load models, thus suffering from limited flexibility and scalability. This paper proposes a new framework, GraphLogDebugger, to debug tabular log based on dynamic graphs. By constructing heterogeneous nodes for objects and events and connecting node-wise edges, the framework recovers the system behind the tabular log as an evolving dynamic graph. With the help of our dynamic graph modeling, a simple dynamic Graph Neural Network (GNN) is representative enough to outperform LLMs in debugging tabular log, which is validated by experimental results on real-world log datasets of computer systems and academic papers.

</details>


### [110] [MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning](https://arxiv.org/abs/2512.22904)
*Jin Wu,Chanjin Zheng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.

</details>


### [111] [Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning](https://arxiv.org/abs/2512.22910)
*Ünver Çiftçi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough'' before optimizing aggressively. In Phase 1, we train an ensemble of lightweight Q-networks under a satisficing objective that limits early value growth using a dynamic baseline, producing diverse, low-variance estimates while avoiding catastrophic overestimation. In Phase 2, the ensemble is distilled into a larger network and fine-tuned with standard Double DQN. We prove theoretically that satisficing induces bounded updates and cannot increase target variance, with a corollary quantifying conditions for substantial reduction. Empirically, Sat-EnQ achieves 3.8x variance reduction, eliminates catastrophic failures (0% vs 50% for DQN), maintains 79% performance under environmental noise}, and requires 2.5x less compute than bootstrapped ensembles. Our results highlight a principled path toward robust reinforcement learning by embracing satisficing before optimization.

</details>


### [112] [Multiple Token Divergence: Measuring and Steering In-Context Computation Density](https://arxiv.org/abs/2512.22944)
*Vincent Herrmann,Eric Alcaide,Michael Wand,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: 提出 Multiple Token Divergence (MTD) 作为衡量语言模型计算努力的简单KL发散度指标，及其衍生解码法 Divergence Steering，可直接从预训练模型的多头预测中得到，无需额外训练，并在任务难度与推理性能之间建立关联。


<details>
  <summary>Details</summary>
Motivation: 现有指标（如下一个 token 损失）无法捕捉推理复杂度；基于隐状态可压缩性的评估不稳定/具侵入性，因此需要一种轻量、稳定的衡量和控制计算动态的方法。

Method: MTD = KL(模型的完整输出分布 vs 辅助浅头预测的输出分布)，可直接从带有多预测头的预训练模型计算；在此基础上提出 Divergence Steering 以控制生成文本的计算特征，且不需要额外训练。

Result: 实证显示，MTD 相较于现有方法更能区分复杂任务与简单任务；在数学推理基准上，MTD 与问题难度正相关；较低的 MTD 与更高的推理准确性相关；MTD 提供一个实用、轻量级的分析与引导语言模型计算动态的工具。

Conclusion: MTD 为分析与控制语言模型计算动态的实用工具，Divergence Steering 提供一种可控的解码策略来调节生成过程的计算特征。

Abstract: Measuring the in-context computational effort of language models is a key challenge, as metrics like next-token loss fail to capture reasoning complexity. Prior methods based on latent state compressibility can be invasive and unstable. We propose Multiple Token Divergence (MTD), a simple measure of computational effort defined as the KL divergence between a model's full output distribution and that of a shallow, auxiliary prediction head. MTD can be computed directly from pre-trained models with multiple prediction heads, requiring no additional training. Building on this, we introduce Divergence Steering, a novel decoding method to control the computational character of generated text. We empirically show that MTD is more effective than prior methods at distinguishing complex tasks from simple ones. On mathematical reasoning benchmarks, MTD correlates positively with problem difficulty. Lower MTD is associated with more accurate reasoning. MTD provides a practical, lightweight tool for analyzing and steering the computational dynamics of language models.

</details>


### [113] [APO: Alpha-Divergence Preference Optimization](https://arxiv.org/abs/2512.22953)
*Wang Zixian*

Main category: cs.LG

TL;DR: 提出 Alpha-Divergence Preference Optimization (APO)，在锚定坐标中使用 Csiszar alpha-发散的偏好优化框架，连续插值前向与反向 KL，以实现模式覆盖与模式搜索的折中，并通过自适应 alpha 调度保持训练稳定性；在实际任务中与基线相比具竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决现有对齐方法中前向 KL（q||pi) 导致的模式覆盖不足与反向 KL（pi||q) 造成的模式崩溃风险之间的权衡；现有锚定方法（如 ADPO）尽管稳定，但通常绑定单一发散。需要在同一锚定几何中实现对多种发散的连续插值以实现覆盖-开发的平衡。

Method: 提出 APO 框架，在锚定坐标中引入 Csiszar alpha-发散，得到一个以 alpha 为参数的统一梯度动力学；分析梯度方差；提出基于奖励与置信度的 alpha 调度策略，使得在策略改进且置信度提升时从覆盖转向开发。

Result: 在 Qwen3-1.7B、math-level3 任务上，APO 与 GRPO/GSPO 基线相比具有竞争力并保持训练稳定性。

Conclusion: APO 提供一个灵活且稳定的优化框架，通过锚定坐标和 alpha 插值实现对覆盖和开发的平衡，提升对齐任务的鲁棒性与适用性，具备扩展到更广泛对齐场景的潜力。

Abstract: Two divergence regimes dominate modern alignment practice. Supervised fine-tuning and many distillation-style objectives implicitly minimize the forward KL divergence KL(q || pi_theta), yielding stable mode-covering updates but often under-exploiting high-reward modes. In contrast, PPO-style online reinforcement learning from human feedback behaves closer to reverse KL divergence KL(pi_theta || q), enabling mode-seeking improvements but risking mode collapse. Recent anchored methods, such as ADPO, show that performing the projection in anchored coordinates can substantially improve stability, yet they typically commit to a single divergence. We introduce Alpha-Divergence Preference Optimization (APO), an anchored framework that uses Csiszar alpha-divergence to continuously interpolate between forward and reverse KL behavior within the same anchored geometry. We derive unified gradient dynamics parameterized by alpha, analyze gradient variance properties, and propose a practical reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is both improving and confidently calibrated. Experiments on Qwen3-1.7B with math-level3 demonstrate that APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.

</details>


### [114] [FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing](https://arxiv.org/abs/2512.22956)
*Wafaa El Husseini*

Main category: cs.LG

TL;DR: FLOW is a synthetic longitudinal dataset and generation tool for modeling daily interactions among workload, lifestyle behaviors, and wellbeing, enabling reproducible experiments when real data are unavailable.


<details>
  <summary>Details</summary>
Motivation: Access to real longitudinal, individual-level data is limited by privacy, ethical, and logistical constraints, which hampers reproducible research, methodological benchmarking, and education in domains like stress modeling, behavioral analysis, and machine learning.

Method: A rule-based, feedback-driven simulation that produces coherent temporal dynamics across variables such as stress, sleep, mood, physical activity, and body weight. The dataset simulates 1,000 individuals over two years with daily resolution and is released publicly. A configurable data generation tool accompanies the dataset to enable reproducible experimentation under adjustable behavioral and contextual assumptions.

Result: Public release of FLOW and its generator; demonstrates coherent temporal dynamics across modeled variables; supports exploratory analysis, methodological development, and benchmarking in the absence of real-world data; not a direct proxy for observed human populations.

Conclusion: FLOW provides a controlled experimental environment for benchmarking, education, and methodological development when real-world data are inaccessible, serving as a testbed rather than a surrogate population.

Abstract: Access to longitudinal, individual-level data on work-life balance and wellbeing is limited by privacy, ethical, and logistical constraints. This poses challenges for reproducible research, methodological benchmarking, and education in domains such as stress modeling, behavioral analysis, and machine learning.
  We introduce FLOW, a synthetic longitudinal dataset designed to model daily interactions between workload, lifestyle behaviors, and wellbeing. FLOW is generated using a rule-based, feedback-driven simulation that produces coherent temporal dynamics across variables such as stress, sleep, mood, physical activity, and body weight. The dataset simulates 1{,}000 individuals over a two-year period with daily resolution and is released as a publicly available resource.
  In addition to the static dataset, we describe a configurable data generation tool that enables reproducible experimentation under adjustable behavioral and contextual assumptions. FLOW is intended as a controlled experimental environment rather than a proxy for observed human populations, supporting exploratory analysis, methodological development, and benchmarking where real-world data are inaccessible.

</details>


### [115] [A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging](https://arxiv.org/abs/2512.22976)
*Amirali Vakili,Salar Jahanshiri,Armin Salimi-Badr*

Main category: cs.LG

TL;DR: 面向单通道脑电信号的情境感知、可解释睡眠分期框架，结合多尺度特征提取与时序建模，显著提升N1阶段检测。SleepEDF数据集上总体准确度89.72%、 macro F1 85.46%，N1 F1达61.7%，表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 全球睡眠障碍的高发与对便捷睡眠分期的需求促使研究聚焦于单通道EEG的自动睡眠分期。现有方法存在类别不平衡、受限感受野建模以及可解释性不足等挑战，亟需兼具可解释性和良好鲁棒性的框架。

Method: 提出一个上下文感知且具有可解释性的单通道EEG睡眠分期框架。核心在于将紧凑的多尺度特征提取与时序建模相结合，以捕捉局部与长期依赖；通过类别权重损失与数据增强缓解N1等不平衡问题；将EEG信号分割为子时段块，并通过对块的softmax概率取平均进行最终预测，从而提升上下文表征与鲁棒性。强调可解释性的特征提取角色，并兼顾实际临床应用。

Result: 在SleepEDF数据集上获得总体准确度89.72%、宏平均F1-score 85.46%，其中N1阶段的F1-score达到61.7%，显著优于以往方法。上述结果表明该方法在提升睡眠分期性能的同时，保持了可解释性与临床适用性。

Conclusion: 该框架证明了在单通道EEG睡眠分期任务中，通过结合多尺度特征与时序建模、数据不平衡处理及分块平均策略，能够提升性能并保持可解释性，具备在真实世界临床场景中应用的潜力。

Abstract: Automatic sleep staging is a critical task in healthcare due to the global prevalence of sleep disorders. This study focuses on single-channel electroencephalography (EEG), a practical and widely available signal for automatic sleep staging. Existing approaches face challenges such as class imbalance, limited receptive-field modeling, and insufficient interpretability. This work proposes a context-aware and interpretable framework for single-channel EEG sleep staging, with particular emphasis on improving detection of the N1 stage. Many prior models operate as black boxes with stacked layers, lacking clearly defined and interpretable feature extraction roles.The proposed model combines compact multi-scale feature extraction with temporal modeling to capture both local and long-range dependencies. To address data imbalance, especially in the N1 stage, classweighted loss functions and data augmentation are applied. EEG signals are segmented into sub-epoch chunks, and final predictions are obtained by averaging softmax probabilities across chunks, enhancing contextual representation and robustness.The proposed framework achieves an overall accuracy of 89.72% and a macro-average F1-score of 85.46%. Notably, it attains an F1- score of 61.7% for the challenging N1 stage, demonstrating a substantial improvement over previous methods on the SleepEDF datasets. These results indicate that the proposed approach effectively improves sleep staging performance while maintaining interpretability and suitability for real-world clinical applications.

</details>


### [116] [Fusion or Confusion? Multimodal Complexity Is Not All You Need](https://arxiv.org/abs/2512.22991)
*Tillmann Rheude,Roland Eils,Benjamin Wild*

Main category: cs.LG

TL;DR: 在大规模的对比实验中，复杂的多模态方法未能显著优于简单基线。提出 SimBaMM（晚融合 Transformer）在标准化且充分调参的条件下可与或优于复杂方法，强调方法学的严谨性与可重复性，并给出可靠性清单与对文献的方法论反思。


<details>
  <summary>Details</summary>
Motivation: 多模态研究常假设专门的多模态方法能提升性能，但缺乏在公平、可重复条件下的系统对比。需要通过标准化实验条件、充分超参数调优和跨数据集评估来检验真实收益，并关注新任务与缺失模态的泛化性。

Method: 对19种高影响方法进行了大规模重新实现，覆盖9个多样数据集，最多支持23种模态；在统一、受控的实验条件下进行比较并对所有方法进行超参数调优；评估在未见任务上的泛化能力以及缺失模态情景；提出并验证简单基线 SimBaMM；并附带一个案例研究与可重复性清单。

Result: 统计分析表明，复杂方法与 SimBaMM 的性能差异通常不显著，且在许多情况下并不优于充分调参后的单模态基线；尤其在小数据情形下，差异更小甚至不显著。还通过一个案例研究揭示文献中的方法学不足，并提供可提升未来评估的可靠性清单。

Conclusion: 呼吁将研究重点从架构创新转向方法学严谨、可重复性与结果可比性，推动多模态学习领域的可信评估与应用。

Abstract: Deep learning architectures for multimodal learning have increased in complexity, driven by the assumption that multimodal-specific methods improve performance. We challenge this assumption through a large-scale empirical study reimplementing 19 high-impact methods under standardized conditions, evaluating them across nine diverse datasets with up to 23 modalities, and testing their generalizability to new tasks beyond their original scope, including settings with missing modalities. We propose a Simple Baseline for Multimodal Learning (SimBaMM), a straightforward late-fusion Transformer architecture, and demonstrate that under standardized experimental conditions with rigorous hyperparameter tuning of all methods, more complex architectures do not reliably outperform SimBaMM. Statistical analysis indicates that more complex methods perform comparably to SimBaMM and frequently do not reliably outperform well-tuned unimodal baselines, especially in the small-data regime considered in many original studies. To support our findings, we include a case study of a recent multimodal learning method highlighting the methodological shortcomings in the literature. In addition, we provide a pragmatic reliability checklist to promote comparable, robust, and trustworthy future evaluations. In summary, we argue for a shift in focus: away from the pursuit of architectural novelty and toward methodological rigor.

</details>


### [117] [Merge before Forget: A Single LoRA Continual Learning via Continual Merging](https://arxiv.org/abs/2512.23017)
*Fuli Qiao,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 本文提出一种面向大语言模型的参数高效连续学习方法，通过将LoRA更新进行正交初始化并顺序合并成一个统一的LoRA，配合时间感知尺度对新旧知识进行平衡，实现与任务数量相关的常量内存复杂度和较低干扰，理论分析与实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决基于LoRA的持续学习在任务增加时的内存增长和任务间干扰问题，期望通过将多个LoRA更新合并为一个统一表示并保持高效性。

Method: 从已学习的LoRA中提取正交基，利用其初始化新任务的LoRA权重；将新任务的LoRA更新串行合并到一个统一的LoRA中；引入时间感知的缩放机制以平衡新旧知识，利用LoRA组件的非对称性；给出理论分析支撑设计，并在多样化的持续学习基准及多种Llama模型上进行实证验证。

Result: 实现与任务数量相关的常量内存复杂度，减少过去任务与新任务之间的干扰；在相对于对称/非对称合并的自适应缩放方面表现更优；在多项持续学习基准和Llama模型上验证了方法的有效性与高效性。

Conclusion: 所提出的正交初始化与连续合并的LoRA方法为LoRA驱动的持续学习提供了一种高效且理论有据的解决方案，能够在保持较低内存开销的同时实现良好性能提升。

Abstract: Parameter-efficient continual learning has emerged as a promising approach for large language models (LLMs) to mitigate catastrophic forgetting while enabling adaptation to new tasks. Current Low-Rank Adaptation (LoRA) continual learning techniques often retain and freeze previously learned LoRAs or generate data representations to overcome forgetting, typically utilizing these to support new LoRAs learn new tasks. However, these methods not only ignore growing computational memory with tasks and limited storage space but also suffer from potential task interference due to the lack of effective LoRA merging mechanisms. In this paper, we propose a novel continual learning method that orthogonally initializes and sequentially merges LoRAs updates into a single unified LoRA. Our method leverages orthogonal basis extraction from previously learned LoRA to initialize the learning of new tasks, further exploits the intrinsic asymmetry property of LoRA components by using a time-aware scaling mechanism to balance new and old knowledge during continual merging. Our approach maintains constant memory complexity with respect to the number of tasks, minimizes interference between past and new tasks via orthogonal basis initialization, and improves performance over asymmetric LoRA merging via adaptive scaling. We provide theoretical analysis to justify our design and conduct extensive experiments across diverse continual learning benchmarks using various Llama models, demonstrating the effectiveness and efficiency of our method.

</details>


### [118] [PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations](https://arxiv.org/abs/2512.23056)
*Min Zhu,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer,Lu Lu*

Main category: cs.LG

TL;DR: 提出了物理信息多模态基础模型 PI-MFM，用符号化 PDE 表达式自动组装残差损失，通过向量化求导实现统一的物理约束训练，能在少标签、跨方程家族的场景中实现数据高效的 PDE 求解，并具备对新 PDE 家族的零-shot 适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有多算子学习方法数据需求高且训练过程中往往忽略物理约束；需要一种数据高效、可迁移的 PDE 求解框架，能够跨方程家族直接利用物理规律进行前训练与微调。

Method: PI-MFM 将 PDE 的符号化表示作为输入，自动从表达式中组装 PDE 残差损失，利用向量化求导进行微分计算，形成跨方程家族的统一物理约束目标；可用于任意 PDE 编码的多模态基础模型，便于在预训练或适配阶段引入物理损失。

Result: 在 13 种参数化的一维时变 PDE 家族基准上，PI-MFM 相较纯数据驱动方法表现更好，尤其在少量标注的时空点、部分观测时域或少标签函数对的情况下更明显；物理损失提升对噪声的鲁棒性，重采样 collocation 点等策略显著提升精度；对自动微分与有限差分在导数计算中的准确性、精度与计算成本进行了分析。，还展示了对未知 PDE 家族的零-shot 物理信息微调：在仅用 PDE 残差与初始/边界条件进行适配、无标签解数据的情况下，将测试误差快速降至约 1%，显著优于从零开始的纯物理训练。

Conclusion: PI-MFM 为数据高效、可迁移的 PDE 求解提供一种实用且可扩展的路径，支持跨方程家族的统一物理约束学习与微调，具备良好的鲁棒性与适用性。

Abstract: Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.

</details>


### [119] [FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment](https://arxiv.org/abs/2512.23070)
*Boyang Zhang,Xiaobing Chen,Songyang Zhang,Shuai Zhang,Xiangwei Zhou,Mingxuan Sun*

Main category: cs.LG

TL;DR: FLEX-MoE通过在联邦MoE中联合优化专家分配与负载均衡来解决资源受限设备与非IID数据导致的专家负载不均问题，引入基于训练反馈的客户端-专家适配分数，并通过优化算法在最大化客户端-专家专门化的同时实现系统级的专家利用平衡。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备资源受限、单个客户端无法存储全部专家，以及数据非独立同分布带来专家负载严重不均的情况下，需要同时考虑个性化和负载均衡来提升联邦MoE性能。

Method: 提出客户端-专家适配分数以量化本地数据集对各专家的适用性，并利用一个优化算法在最大化客户端-专家专门化的同时强制实现系统范围内的专家利用平衡，从而实现对专家集合的高效分配与负载均衡.

Result: 通过在三个数据集上的广泛实验，FLEX-MoE显示出优越的性能，并能够在资源受限场景下维持平衡的专家利用分布，相较于现有只关注个性化的贪婪方法具有明显优势。

Conclusion: FLEX-MoE有效地解决了联邦学习中的个性化与负载平衡的双重挑战，提供了一种在资源受限场景下更均衡且性能更优的MoE部署方案。

Abstract: Mixture-of-Experts (MoE) models enable scalable neural networks through conditional computation. However, their deployment with federated learning (FL) faces two critical challenges: 1) resource-constrained edge devices cannot store full expert sets, and 2) non-IID data distributions cause severe expert load imbalance that degrades model performance. To this end, we propose \textbf{FLEX-MoE}, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. Specifically, our approach introduces client-expert fitness scores that quantify the expert suitability for local datasets through training feedback, and employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide. Unlike existing greedy methods that focus solely on personalization while ignoring load imbalance, our FLEX-MoE is capable of addressing the expert utilization skew, which is particularly severe in FL settings with heterogeneous data. Our comprehensive experiments on three different datasets demonstrate the superior performance of the proposed FLEX-MoE, together with its ability to maintain balanced expert utilization across diverse resource-constrained scenarios.

</details>


### [120] [Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning](https://arxiv.org/abs/2512.23087)
*Yingru Li,Jiawei Xu,Jiacai Liu,Yuxuan Tong,Ziniu Li,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 在强化学习（RL）用于大语言模型（LLMs）的场景中，训练和推理的分布不一致导致训练-推理错配。错配对概率较低的尾部标记影响显著，且会在序列上积累，破坏梯度估计。提出通过动态裁剪的“安全”词汇表来约束 RL 目标，排除极端尾部词以降低偏差，同时带来可控的优化偏差。实证上实现了训练稳定性，并给出裁剪引入的偏差界限。


<details>
  <summary>Details</summary>
Motivation: 解决训练阶段的优化目标与高吞吐推理引擎之间的分布错配问题，尤其是对低概率尾部标记的系统性偏差，以及这类偏差在序列级梯度估计中的累积效应。

Method: 提出一种动态裁剪的“安全”词汇表，排除极端尾部的低概率标记，作为 RL 目标的约束。通过裁剪来在较大、系统性偏差与较小但有界的优化偏差之间权衡。

Result: 在实验中实现了训练稳定性提升，并对词汇裁剪引入的优化偏差给出理论界限。

Conclusion: 裁剪尾部词汇以创建安全词汇表，能够在降低系统性错配带来的不稳定性的同时，给出可控的优化偏差界限，提供一种实用的 RL-LLM 训练策略。

Abstract: Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.

</details>


### [121] [A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms](https://arxiv.org/abs/2512.23097)
*Yingru Li,Ziniu Li,Jiacai Liu*

Main category: cs.LG

TL;DR: 提出一个统一的LLM微调框架，将模仿学习与强化学习结合在一起，通过对包含轨迹级KL散度和任务奖励的复合目标的梯度分析，得到两部分组成：一是可解析的密集梯度（Dense Gradient）用于词级模仿，二是用于长序列奖励优化的稀疏梯度的蒙特卡洛估计；密集梯度在对数几率等级（logit-level）上有闭式公式，便于高效的GPU实现。


<details>
  <summary>Details</summary>
Motivation: 在LLM微调中同时兼顾模仿学习的稳定性和强化学习的任务导向性，寻求一种能在计算效率和梯度方差之间获得平衡的框架；通过对组合目标的梯度结构进行分析，提供一个清晰的两层梯度分解，以便在实际训练中高效实现。

Method: 对包含轨迹级KL散度和任务奖励的复合目标进行梯度分析，推导出自然的分解：1) 提供可解析的密集梯度用于 token 级模仿（Dense Gradient），2) 通过蒙特卡洛估计得到对长时序奖励优化有用的稀疏梯度（Sparse Gradient）。其中密集梯度在对数几率（logit）层面具有闭式公式，使得在GPU上实现更高效。

Result: 提出的分解为实际实现提供了两类梯度信号：密集的令牌级模仿梯度和基于蒙特卡洛的长时序奖励梯度。密集梯度具有闭式解，便于高效计算；蒙特卡洛估计用于处理长序列的奖励优化，整体框架实现上具有计算效率的优势，但尚未在摘要中给出具体的实验结果。

Conclusion: 该工作提供了一个将 imitation learning 与 reinforcement learning 融合的理论框架，通过对复合目标的梯度分解实现高效的 GPU 实现路径，特别是密集梯度的闭式表达可能显著降低训练成本，同时保留对长序列奖励的优化能力。

Abstract: We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.

</details>


### [122] [How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure](https://arxiv.org/abs/2512.23109)
*Paul M. Thompson*

Main category: cs.LG

TL;DR: 本文给出基于VLM诱导分类器的有限样本统一收敛与校准保证；样本复杂度由内在维度与谱衰减决定，支持生物医学中在有限数据下的统一可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型和视觉-文本模型在科学与医疗决策支持中被广泛使用，但需确保预测在输入、类别和子群体上具备统一的准确性和良好校准性。单纯的平均性能可能掩盖小概率子群体的高误差，因此需要在有限样本下获得稳定的统一保证。

Method: 考察通过在受限表示空间内变动提示或语义嵌入得到的一族分类器；假设输出对低维语义表示具有平滑/ Lipschitz 依赖性；利用经典的统一收敛工具给出非渐进的界，给出依赖谱特征的界以揭示数据需求受特征值衰减影响的机制。

Result: 给出VLM诱导分类器在准确性和校准函数上的有限样本统一收敛界；样本复杂度由内在/有效维度决定，而非嵌入的总维度；给出谱依赖边界，明确特征值衰减如何影响数据需求。

Conclusion: 对数据受限的生物医学建模具有直接意义：指出在何种数据规模下可以实现统一可靠的预测，并解释为何平均校准度指标可能忽略最坏情况的校准问题。

Abstract: Modern generative and vision-language models (VLMs) are increasingly used in scientific and medical decision support, where predicted probabilities must be both accurate and well calibrated. Despite strong empirical results with moderate data, it remains unclear when such predictions generalize uniformly across inputs, classes, or subpopulations, rather than only on average-a critical issue in biomedicine, where rare conditions and specific groups can exhibit large errors even when overall loss is low.
  We study this question from a finite-sample perspective and ask: under what structural assumptions can generative and VLM-based predictors achieve uniformly accurate and calibrated behavior with practical sample sizes? Rather than analyzing arbitrary parameterizations, we focus on induced families of classifiers obtained by varying prompts or semantic embeddings within a restricted representation space. When model outputs depend smoothly on a low-dimensional semantic representation-an assumption supported by spectral structure in text and joint image-text embeddings-classical uniform convergence tools yield meaningful non-asymptotic guarantees.
  Our main results give finite-sample uniform convergence bounds for accuracy and calibration functionals of VLM-induced classifiers under Lipschitz stability with respect to prompt embeddings. The implied sample complexity depends on intrinsic/effective dimension, not ambient embedding dimension, and we further derive spectrum-dependent bounds that make explicit how eigenvalue decay governs data requirements. We conclude with implications for data-limited biomedical modeling, including when current dataset sizes can support uniformly reliable predictions and why average calibration metrics may miss worst-case miscalibration.

</details>


### [123] [SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals](https://arxiv.org/abs/2512.23131)
*Yankang Li,Changsheng Li*

Main category: cs.LG

TL;DR: SE-MLP 将通道注意力与残差连接结合的多层感知器，快速预测穿透加速度特征，实现从物理参数到穿透特征的非线性映射，并在与 MLP、XGBoost、Transformer 的对比中表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决依赖长时间仿真和高计算成本的穿透加速度先验特征获取问题，需快速、稳定预测方法。

Method: 提出 SE-MLP 架构，将通道注意力模块与残差结构集成，输入物理工况参数，输出分层加速度特征，建立物理参数与穿透特征的非线性映射；进行与常规 MLP、XGBoost、Transformer 的对比，进行消融研究。

Result: SE-MLP 在预测准确性、泛化能力、稳定性方面优于对比模型；通道注意力和残差结构对性能提升均有显著贡献；数值仿真与范围恢复测试表明预测峰值和脉宽与实测值偏差在工程容忍度之内。

Conclusion: 该方法在可行性和工程适用性方面得到证实，为快速生成穿透引信的先验特征值提供了可行途径与实际基础。

Abstract: Accurate identification of the penetration process relies heavily on prior feature values of penetration acceleration. However, these feature values are typically obtained through long simulation cycles and expensive computations. To overcome this limitation, this paper proposes a multi-layer Perceptron architecture, termed squeeze and excitation multi-layer perceptron (SE-MLP), which integrates a channel attention mechanism with residual connections to enable rapid prediction of acceleration feature values. Using physical parameters under different working conditions as inputs, the model outputs layer-wise acceleration features, thereby establishing a nonlinear mapping between physical parameters and penetration characteristics. Comparative experiments against conventional MLP, XGBoost, and Transformer models demonstrate that SE-MLP achieves superior prediction accuracy, generalization, and stability. Ablation studies further confirm that both the channel attention module and residual structure contribute significantly to performance gains. Numerical simulations and range recovery tests show that the discrepancies between predicted and measured acceleration peaks and pulse widths remain within acceptable engineering tolerances. These results validate the feasibility and engineering applicability of the proposed method and provide a practical basis for rapidly generating prior feature values for penetration fuzes.

</details>


### [124] [Principled Algorithms for Optimizing Generalized Metrics in Binary Classification](https://arxiv.org/abs/2512.23133)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 在不平衡数据和代价不对称的场景中，提出基于通用代价敏感学习的指标优化框架，设计 METRO 算法并给出理论保证与实证效果。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标如 Fβ、AM、Jaccard 相似系数和加权准确度在不平衡数据或代价不对称的应用中比传统二元分类损失更合适，但直接优化这些指标存在计算与统计挑战；现有方法多依赖 Bayes 最优分类器的表征，且以阈值分割为主，未能针对受限假设集给出有限样本的泛化保证。需要一个能在受限假设集内提供理论保证的直接指标优化方法。

Method: 将指标优化重构为一个广义的代价敏感学习问题，设计新的替代损失函数，并给出可证明的 H-consistency（H-一致性）保证；在此框架下提出 METRO（Metric Optimization）算法，并推导其理论性能界限。

Result: 通过理论分析建立了对新替代损失和 METRO 的 H-consistency 与有限样本泛化界；在多组不平衡/代价敏感的基准数据集上，METRO 相较于先前基线在优化目标指标方面显示出显著优势。

Conclusion: 给出一个统一的指标优化框架及可实现的算法 METRO，具备 H-consistency 和有限样本泛化保证，能直接在受限假设集上有效优化包括 Fβ、AM、Jaccard 等在内的通用评估指标。

Abstract: In applications with significant class imbalance or asymmetric costs, metrics such as the $F_β$-measure, AM measure, Jaccard similarity coefficient, and weighted accuracy offer more suitable evaluation criteria than standard binary classification loss. However, optimizing these metrics present significant computational and statistical challenges. Existing approaches often rely on the characterization of the Bayes-optimal classifier, and use threshold-based methods that first estimate class probabilities and then seek an optimal threshold. This leads to algorithms that are not tailored to restricted hypothesis sets and lack finite-sample performance guarantees. In this work, we introduce principled algorithms for optimizing generalized metrics, supported by $H$-consistency and finite-sample generalization bounds. Our approach reformulates metric optimization as a generalized cost-sensitive learning problem, enabling the design of novel surrogate loss functions with provable $H$-consistency guarantees. Leveraging this framework, we develop new algorithms, METRO (Metric Optimization), with strong theoretical performance guarantees. We report the results of experiments demonstrating the effectiveness of our methods compared to prior baselines.

</details>


### [125] [A Weak Signal Learning Dataset and Its Baseline Method](https://arxiv.org/abs/2512.23160)
*Xianqi Liu,Xiangru Li,Lefeng He,Ziyu Fang*

Main category: cs.LG

TL;DR: 提出专门的弱信号学习数据集与PDVFN模型，聚焦低SNR、数据分布偏斜和双视图表示，以提高弱信号的分类与回归性能并提供基准数据集。


<details>
  <summary>Details</summary>
Motivation: 在噪声和干扰遮蔽下提取弱信号信息是许多领域的核心难题；现有研究缺乏专门数据集，限制方法评估与比较。

Method: 构建13,158条光谱样本的数据集，采用向量和时频图的双视图表示；提出PDVFN模型，在低SNR、分布偏斜和双不平衡条件下并行提取局部序列特征与全局频域结构，包含局部增强、序列建模、噪声抑制、多尺度捕获、频率提取与全局感知等设计。

Result: 在低SNR和极端类别不平衡场景中实现更高准确性与鲁棒性，建立了弱信号学习的基线与基准数据集，并对如天文光谱等任务具备实用性。

Conclusion: 本文为弱信号学习领域提供专用数据集、基线模型与研究基础，推动弱信号学习在分类、回归及多源数据融合方面的应用与发展。

Abstract: Weak signal learning (WSL) is a common challenge in many fields like fault diagnosis, medical imaging, and autonomous driving, where critical information is often masked by noise and interference, making feature identification difficult. Even in tasks with abundant strong signals, the key to improving model performance often lies in effectively extracting weak signals. However, the lack of dedicated datasets has long constrained research. To address this, we construct the first specialized dataset for weak signal feature learning, containing 13,158 spectral samples. It features low SNR dominance (over 55% samples with SNR below 50) and extreme class imbalance (class ratio up to 29:1), providing a challenging benchmark for classification and regression in weak signal scenarios. We also propose a dual-view representation (vector + time-frequency map) and a PDVFN model tailored to low SNR, distribution skew, and dual imbalance. PDVFN extracts local sequential features and global frequency-domain structures in parallel, following principles of local enhancement, sequential modeling, noise suppression, multi-scale capture, frequency extraction, and global perception. This multi-source complementarity enhances representation for low-SNR and imbalanced data, offering a novel solution for WSL tasks like astronomical spectroscopy. Experiments show our method achieves higher accuracy and robustness in handling weak signals, high noise, and extreme class imbalance, especially in low SNR and imbalanced scenarios. This study provides a dedicated dataset, a baseline model, and establishes a foundation for future WSL research.

</details>


### [126] [Evaluating Parameter Efficient Methods for RLVR](https://arxiv.org/abs/2512.23165)
*Qingyu Yin,Yulun Wu,Zhennan Shen,Sunbowen Li,Zhilin Wang,Yanshu Li,Chak Tou Leong,Jiale Kang,Jinjin Gu*

Main category: cs.LG

TL;DR: 系统评估12+种PEFT方法在RLVR框架下的数学推理任务，发现DoRA/AdaLoRA/MiSS等结构变体常优于LoRA；SVD初始化（PiSSA/MiLoRA）出现谱崩溃；极端参数削减（VeRA/Rank-1）显著降低推理能力，给出在RLVR中选择PEFT的实用指引。


<details>
  <summary>Details</summary>
Motivation: RLVR通过可验证奖励推动语言模型提升推理能力，但尚无关于哪种PEFT架构最优的系统结论，因此需要在大规模基准上对多种PEFT进行全面比较，以为未来方法提供指引。

Method: 在DeepSeek-R1-Distill家族和数学推理基准上，系统评估超过12种PEFT方法的性能；包含对LoRA的常规对比，以及DoRA、AdaLoRA、MiSS、PiSSA、MiLoRA、VeRA、Rank-1等结构/初始化策略的消融与扩展分析。

Result: 结构变体（如DoRA、AdaLoRA、MiSS）在大多数基准上优于LoRA；SVD初始化策略（PiSSA、MiLoRA）表现出谱崩溃，显示主成分更新与RL优化之间存在本质不对齐；极端参数压缩（VeRA、Rank-1）显著降低推理能力并限制推理任务表现。

Conclusion: 建议在RLVR场景下更积极地探索多种PEFT结构，而非默认采用LoRA；需要更多的消融与扩展研究来建立面向RL任务的PEFT选型指南。

Abstract: We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.

</details>


### [127] [Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR](https://arxiv.org/abs/2512.23177)
*Will Sebelik-Lassiter,Evan Schubert,Muhammad Alliyu,Quentin Robbins,Excel Olatunji,Mustafa Barry*

Main category: cs.LG

TL;DR: ML-assisted VCUS analysis pipeline for vocal cord segmentation and VCP classification with high validation accuracy (segmentation 96%; classification 99%).


<details>
  <summary>Details</summary>
Motivation: VCUS is noninvasive but operator-dependent; applying machine learning can automate detection/segmentation to improve diagnostic accuracy and consistency.

Method: 30 volunteers provided VCUS videos, converted to uniform-sized frames. Training data included healthy and simulated VCP images for both segmentation and classification models.

Result: Segmentation model achieved 96% validation accuracy; best classification model (VIPRnet) achieved 99% validation accuracy.

Conclusion: Machine learning–assisted VCUS analysis shows strong potential to surpass operator-dependent interpretation in diagnostic accuracy.

Abstract: Intro: Vocal cord ultrasound (VCUS) has emerged as a less invasive and better tolerated examination technique, but its accuracy is operator dependent. This research aims to apply a machine learning-assisted algorithm to automatically identify the vocal cords and distinguish normal vocal cord images from vocal cord paralysis (VCP). Methods: VCUS videos were acquired from 30 volunteers, which were split into still frames and cropped to a uniform size. Healthy and simulated VCP images were used as training data for vocal cord segmentation and VCP classification models. Results: The vocal cord segmentation model achieved a validation accuracy of 96%, while the best classification model (VIPRnet) achieved a validation accuracy of 99%. Conclusion: Machine learning-assisted analysis of VCUS shows great promise in improving diagnostic accuracy over operator-dependent human interpretation.

</details>


### [128] [A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization](https://arxiv.org/abs/2512.23190)
*Yi-Han Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: LightONS 是对 Online Newton Step 的一个高效变体，通过领域变换和迟滞机制延迟昂贵的马哈拉诺比投影，从而在保持 O(d log T) 经验性 regrets 的前提下，将总运行时间降低至 O(d^2 T + d^ω √(T log T))，并给出 SXO 的近似无偏 excess risk 的实现，运行时间为 tilde O(d^3/ε)。


<details>
  <summary>Details</summary>
Motivation: 解决 ONS 在有界域（如单位球）中的投影成本 Ω(d^ω) 以及 SXO 场景下的高计算代价问题，满足对高维、时序数据的高效在线学习需求，同时回应 COLT'13 对 SXO 运行时间的开放性问题。

Method: 在 ONS 的框架内引入领域转换（domain-conversion）和参数自由的在线学习中的迟滞（hysteresis）机制，使得昂贵的马哈拉诺比投影仅在必要时发生，从而减少每轮的计算量；通过推导证明仍保持 O(d log T) 的 regrets；给出 SXO 场景下的实现，运行时间为 tilde O(d^3/ε)。

Result: 提出 LightONS，作为 ONS 的高效替代，在多种在线学习场景中具有更低的理论和实际运行成本，同时提供可扩展的插件式框架。

Conclusion: LightONS 以简洁的设计保留 ONS 的结构美，并通过领域变换与迟滞策略解决高成本投影问题，从而回答 COLT'13 的开放问题并拓展到更广的应用场景，如梯度范数自适应后悔界、参数化随机带宽、以及内存友好型在线学习。

Abstract: Online eXp-concave Optimization (OXO) is a fundamental problem in online learning. The standard algorithm, Online Newton Step (ONS), balances statistical optimality and computational practicality, guaranteeing an optimal regret of $O(d \log T)$, where $d$ is the dimension and $T$ is the time horizon. ONS faces a computational bottleneck due to the Mahalanobis projections at each round. This step costs $Ω(d^ω)$ arithmetic operations for bounded domains, even for the unit ball, where $ω\in (2,3]$ is the matrix-multiplication exponent. As a result, the total runtime can reach $\tilde{O}(d^ωT)$, particularly when iterates frequently oscillate near the domain boundary. For Stochastic eXp-concave Optimization (SXO), computational cost is also a challenge. Deploying ONS with online-to-batch conversion for SXO requires $T = \tilde{O}(d/ε)$ rounds to achieve an excess risk of $ε$, and thereby necessitates an $\tilde{O}(d^{ω+1}/ε)$ runtime. A COLT'13 open problem posed by Koren [2013] asks for an SXO algorithm with runtime less than $\tilde{O}(d^{ω+1}/ε)$.
  This paper proposes a simple variant of ONS, LightONS, which reduces the total runtime to $O(d^2 T + d^ω\sqrt{T \log T})$ while preserving the optimal $O(d \log T)$ regret. LightONS implies an SXO method with runtime $\tilde{O}(d^3/ε)$, thereby answering the open problem. Importantly, LightONS preserves the elegant structure of ONS by leveraging domain-conversion techniques from parameter-free online learning to introduce a hysteresis mechanism that delays expensive Mahalanobis projections until necessary. This design enables LightONS to serve as an efficient plug-in replacement of ONS in broader scenarios, even beyond regret minimization, including gradient-norm adaptive regret, parametric stochastic bandits, and memory-efficient online learning.

</details>


### [129] [PGOT: A Physics-Geometry Operator Transformer for Complex PDEs](https://arxiv.org/abs/2512.23192)
*Zhuo Zhang,Xi Yang,Yuan Zhao,Canqun Yang*

Main category: cs.LG

TL;DR: 提出 Physics-Geometry Operator Transformer (PGOT)，通过 Spectrum-Preserving Geometric Attention 和 physics slicing-geometry injection，在保留多尺度几何特征的同时实现线性复杂度 O(N) 的几何感知 PDE 模型，并通过空间自适应路由在平滑区域和激波/不连续处采用不同的高阶路径，在标准基准和大规模工业任务上达到状态最优。


<details>
  <summary>Details</summary>
Motivation: 在大规模非结构网格和复杂几何形状下对物理场进行高精度建模仍然具有挑战性。现有高效架构通常采用特征降维，导致几何混叠并丢失边界信息。需开发显式几何感知的模型，以保持多尺度几何特征并高效处理激波与不连续处。

Method: 提出 PGOT，包括 Spectrum-Preserving Geometric Attention（SpecGeo-Attention）以及“physics slicing-geometry injection”机制，通过多尺度几何编码显式保留几何特征，同时保持线性计算复杂度 O(N)；并通过基于空间坐标的路由，将计算在平滑区域走低阶线性路径，在激波/不连续处走高阶非线性路径，实现空间自适应高精度物理场建模。

Result: 在四个标准基准上达到一致的最优性能，并在大规模工业任务（如翼型和汽车设计）中表现出色。

Conclusion: PGOT 通过显式几何感知重建物理特征学习，保留多尺度几何信息，并实现空间自适应且高精度的物理场建模，适用于大规模非结构网格的 PDE 表征。

Abstract: While Transformers have demonstrated remarkable potential in modeling Partial Differential Equations (PDEs), modeling large-scale unstructured meshes with complex geometries remains a significant challenge. Existing efficient architectures often employ feature dimensionality reduction strategies, which inadvertently induces Geometric Aliasing, resulting in the loss of critical physical boundary information. To address this, we propose the Physics-Geometry Operator Transformer (PGOT), designed to reconstruct physical feature learning through explicit geometry awareness. Specifically, we propose Spectrum-Preserving Geometric Attention (SpecGeo-Attention). Utilizing a ``physics slicing-geometry injection" mechanism, this module incorporates multi-scale geometric encodings to explicitly preserve multi-scale geometric features while maintaining linear computational complexity $O(N)$. Furthermore, PGOT dynamically routes computations to low-order linear paths for smooth regions and high-order non-linear paths for shock waves and discontinuities based on spatial coordinates, enabling spatially adaptive and high-precision physical field modeling. PGOT achieves consistent state-of-the-art performance across four standard benchmarks and excels in large-scale industrial tasks including airfoil and car designs.

</details>


### [130] [Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing](https://arxiv.org/abs/2512.23200)
*Ziru Niu,Hai Dong,A. K. Qin,Tao Gu,Pengcheng Zhang*

Main category: cs.LG

TL;DR: 提出 FedOLF：在联邦学习中对模型层按固定顺序进行预冻结以降低计算和内存开销，同时引入轻量级的张量运算近似（TOA）以减少通信和能耗。实验证明在非独立同分布数据下，在多种数据集/模型组合上，FedOLF 能在保持或略增准确率的同时提升能效和降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 在物联网场景下通过联邦学习实现隐私保护的同时，受限的边缘设备算力、内存与带宽成为制约因素，现有通过 dropout 或层冻结的做法常以牺牲准确性或未充分考虑内存约束为代价。需要一种兼顾准确性、内存/计算效率与通信成本的解决方案。

Method: 提出在 FL 训练前对模型层按预定顺序进行冻结（FedOLF），以减少前向与反向计算量并降低内存需求；同时引入 Tensor Operation Approximation（TOA），作为一种轻量级近似策略替代传统量化，旨在在保持精度的同时降低通信和能耗。

Result: 在非IID数据条件下，FedOLF 在多组数据集和模型架构上实现了对比基线的准确率提升：EMNIST/CNN 0.3%、CIFAR-10/AlexNet 6.4%、CIFAR-100/ResNet20 5.81%、CIFAR-100/ResNet44 4.4%、CINIC-10/ResNet20 6.27%、CINIC-10/ResNet44 1.29%；并且具有更高的能效和更低的内存占用。

Conclusion: 通过在训练前实现确定性、固定顺序的层冻结结合轻量级的 TOA，FedOLF 能显著降低计算与内存需求并降低通信与能耗，同时在多项非IID数据场景下维持或提升了模型准确性，相较于现有方法具有潜在的实际应用价值。

Abstract: Federated Learning (FL) has emerged as a privacy-preserving paradigm for training machine learning models across distributed edge devices in the Internet of Things (IoT). By keeping data local and coordinating model training through a central server, FL effectively addresses privacy concerns and reduces communication overhead. However, the limited computational power, memory, and bandwidth of IoT edge devices pose significant challenges to the efficiency and scalability of FL, especially when training deep neural networks. Various FL frameworks have been proposed to reduce computation and communication overheads through dropout or layer freezing. However, these approaches often sacrifice accuracy or neglect memory constraints. To this end, in this work, we introduce Federated Learning with Ordered Layer Freezing (FedOLF). FedOLF consistently freezes layers in a predefined order before training, significantly mitigating computation and memory requirements. To further reduce communication and energy costs, we incorporate Tensor Operation Approximation (TOA), a lightweight alternative to conventional quantization that better preserves model accuracy. Experimental results demonstrate that over non-iid data, FedOLF achieves at least 0.3%, 6.4%, 5.81%, 4.4%, 6.27% and 1.29% higher accuracy than existing works respectively on EMNIST (with CNN), CIFAR-10 (with AlexNet), CIFAR-100 (with ResNet20 and ResNet44), and CINIC-10 (with ResNet20 and ResNet44), along with higher energy efficiency and lower memory footprint.

</details>


### [131] [FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs](https://arxiv.org/abs/2512.23235)
*Zihao Zhou,Shusen Yang,Fangyuan Zhao,Xuebin Ren*

Main category: cs.LG

TL;DR: 本论文识别了在图联邦学习中因客户端之间重叠子图不均衡导致的公平性问题，提出 FairGFL，通过隐私保护的重叠比估计与可解释的加权聚合来提升跨客户端公平性，同时通过在联邦混合损失中加入正则项来改善模型效用与公平性的权衡，在四个公开数据集上优于四个基线方法。


<details>
  <summary>Details</summary>
Motivation: 图联邦学习可以在保护原始数据隐私的前提下提取分布式子图中的高阶信息，但客户端之间的子图存在重叠，且重叠程度不均衡。已有研究发现重叠对缓解数据异质性有益，但对不均衡重叠带来的负面影响（尤其是跨客户端的公平性问题）尚未充分探讨。本研究旨在揭示并缓解这一不公平性。

Method: 提出 FairGFL（FAIRness-aware subGraph Federated Learning），通过一种可解释的加权聚合机制提升跨客户端公平性，且以隐私保护的方式估计客户端的重叠比。进一步在联邦复合损失函数中加入精心设计的正则项，以在模型效用与公平性之间取得更优的权衡。

Result: 在四个基准图数据集上的大量实验表明，FairGFL在模型效用和公平性方面均优于四个代表性基线算法。

Conclusion: FairGFL有效缓解了由不均衡重叠引发的跨客户端不公平性，同时保持隐私保护和模型性能，揭示了在图数据场景下提升公平性的可行途径及未来在更复杂场景中的扩展方向。

Abstract: Graph federated learning enables the collaborative extraction of high-order information from distributed subgraphs while preserving the privacy of raw data. However, graph data often exhibits overlap among different clients. Previous research has demonstrated certain benefits of overlapping data in mitigating data heterogeneity. However, the negative effects have not been explored, particularly in cases where the overlaps are imbalanced across clients. In this paper, we uncover the unfairness issue arising from imbalanced overlapping subgraphs through both empirical observations and theoretical reasoning. To address this issue, we propose FairGFL (FAIRness-aware subGraph Federated Learning), a novel algorithm that enhances cross-client fairness while maintaining model utility in a privacy-preserving manner. Specifically, FairGFL incorporates an interpretable weighted aggregation approach to enhance fairness across clients, leveraging privacy-preserving estimation of their overlapping ratios. Furthermore, FairGFL improves the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function. Through extensive experiments on four benchmark graph datasets, we demonstrate that FairGFL outperforms four representative baseline algorithms in terms of both model utility and fairness.

</details>


### [132] [PFed-Signal: An ADR Prediction Model based on Federated Learning](https://arxiv.org/abs/2512.23262)
*Tao Li,Peilin Li,Kui Lu,Yilei Wang,Junliang Shang,Guangshun Li,Huiyu Zhou*

Main category: cs.LG

TL;DR: PFed-Signal: a federated-learning ADR signal-prediction framework that identifies and removes biased FAERS data using Euclidean distance, training a Transformer-based ADR predictor on clean data; improves ROR/PRR and key metrics (ACC=0.887, F1=0.890, Recall=0.913, AUC=0.957).


<details>
  <summary>Details</summary>
Motivation: FAERS contains biased reporting data that biases ADR signal measures (ROR/PRR) and harms diagnosis. A method that can identify and remove biased data is needed to improve predictive accuracy.

Method: 1) Pfed-Split: split dataset by ADR. 2) ADR-signal: federated-learning-based biased-data identification using Euclidean distance to produce a clean dataset. 3) Transformer-based ADR predictor trained on the clean data.

Result: On the clean dataset, ROR/PRR perform better than traditional methods. PFed-Signal achieves accuracy 0.887, F1 0.890, recall 0.913, AUC 0.957, surpassing baselines.

Conclusion: PFed-Signal effectively mitigates FAERS bias through federated identification and data cleaning, leading to improved ADR signal prediction.

Abstract: The adverse drug reactions (ADRs) predicted based on the biased records in FAERS (U.S. Food and Drug Administration Adverse Event Reporting System) may mislead diagnosis online. Generally, such problems are solved by optimizing reporting odds ratio (ROR) or proportional reporting ratio (PRR). However, these methods that rely on statistical methods cannot eliminate the biased data, leading to inaccurate signal prediction. In this paper, we propose PFed-signal, a federated learning-based signal prediction model of ADR, which utilizes the Euclidean distance to eliminate the biased data from FAERS, thereby improving the accuracy of ADR prediction. Specifically, we first propose Pfed-Split, a method to split the original dataset into a split dataset based on ADR. Then we propose ADR-signal, an ADR prediction model, including a biased data identification method based on federated learning and an ADR prediction model based on Transformer. The former identifies the biased data according to the Euclidean distance and generates a clean dataset by deleting the biased data. The latter is an ADR prediction model based on Transformer trained on the clean data set. The results show that the ROR and PRR on the clean dataset are better than those of the traditional methods. Furthermore, the accuracy rate, F1 score, recall rate and AUC of PFed-Signal are 0.887, 0.890, 0.913 and 0.957 respectively, which are higher than the baselines.

</details>


### [133] [On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases](https://arxiv.org/abs/2512.23265)
*Alexander Korotin,Gudmund Pammer*

Main category: cs.LG

TL;DR: 对 Flow Matching 的反问题在一维和高斯情形下有唯一解；多维情形尚待研究。


<details>
  <summary>Details</summary>
Motivation: 受现代生成式 AI 应用启发，研究有限指数矩分布之间的流动匹配反问题，旨在为流动匹配模型蒸馏等应用提供理论支撑。

Method: 提出并分析适用于有限指数矩分布的反问题的唯一性条件；分别在一维和高斯分布情形给出证明；给出在多维情形未解决的结论并指出未来研究方向。

Result: 在一维与高斯情形下证明了问题的唯一性；一般多维情形尚未解决。

Conclusion: 未来工作将聚焦于多维情形的存在性与唯一性，以及扩展到更广泛的分布假设。

Abstract: This paper studies the inverse problem of flow matching (FM) between distributions with finite exponential moment, a problem motivated by modern generative AI applications such as the distillation of flow matching models. Uniqueness of the solution is established in two cases - the one-dimensional setting and the Gaussian case. The general multidimensional problem remains open for future studies.

</details>


### [134] [Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions](https://arxiv.org/abs/2512.23295)
*Yuchen Xie,Honghang Chi,Haopeng Quan,Yahui Wang,Wei Wang,Yu Ma*

Main category: cs.LG

TL;DR: 提出 HC-PINN 的 NTK 框架，揭示边界函数 B 作为谱滤波器重塑内核特征并预测训练收敛的有效秩，强调边界设计需进行谱优化。


<details>
  <summary>Details</summary>
Motivation: 解释为何硬约束的 PINN 训练动态尚未被理论揭示，弥补软约束与硬约束之间的理论空白，并为几何硬约束的学习提供理论基石。

Method: 构建带有 tilde u = A + B N 的试解形式，给出 HC-PINN 的显式核组合律；通过 Neural Tangent Kernel 框架与谱分析，推导残核的有效秩及 B 的谱滤波作用。

Result: 发现边界函数 B 作为谱滤波器重塑原生核的特征谱，决定学习动力学；残核的有效秩成为预测训练收敛的确定性指标，优于传统的条件数；警示常用边界函数可能导致谱塌陷和优化停滞；在多维基准上得到验证，框架将边界设计从经验性转化为谱优化问题。

Conclusion: 为包含几何硬约束的科学机器学习提供理论基础，将边界函数设计转化为可控的谱优化问题，从而提升 HC-PINN 的训练稳定性与泛化性能。

Abstract: Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly favored for their ability to strictly enforce boundary conditions via a trial function ansatz $\tilde{u} = A + B \cdot N$, yet the theoretical mechanisms governing their training dynamics have remained unexplored.
  Unlike soft-constrained formulations where boundary terms act as additive penalties, this work reveals that the boundary function $B$ introduces a multiplicative spatial modulation that fundamentally alters the learning landscape.
  A rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs is established, deriving the explicit kernel composition law.
  This relationship demonstrates that the boundary function $B(\vec{x})$ functions as a spectral filter, reshaping the eigenspectrum of the neural network's native kernel.
  Through spectral analysis, the effective rank of the residual kernel is identified as a deterministic predictor of training convergence, superior to classical condition numbers.
  It is shown that widely used boundary functions can inadvertently induce spectral collapse, leading to optimization stagnation despite exact boundary satisfaction.
  Validated across multi-dimensional benchmarks, this framework transforms the design of boundary functions from a heuristic choice into a principled spectral optimization problem, providing a solid theoretical foundation for geometric hard constraints in scientific machine learning.

</details>


### [135] [The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models](https://arxiv.org/abs/2512.23340)
*Dakuan Lu,Jiaqi Zhang,Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 提出“多模型协作定律”，给出多模型集成的性能上限随参数总量的幂律扩展规律，并强调模型多样性是提升协作收益的关键。


<details>
  <summary>Details</summary>
Motivation: 单一大模型的扩展规律无法充分解释多模型协作带来的性能提升。需要一个统一的理论框架来量化多模型集合的潜在上限及其收益。

Method: 采用与模型无关的形式化表述，设定理想化的整合oracle：对每个样本，总损失为池中任一模型损失的最小值。通过实验验证，在总参数量上呈现幂律 scaling，并比较异构与同质模型家族的协作效果。

Result: 多模型系统按参数规模呈现幂律扩展，提升趋势更明显、理论损失下限更低；异质模型家族的集成在性能扩展上优于同族模型，表明模型多样性是协作收益的主要驱动因素。

Conclusion: 模型协作为扩展LLM智能前沿提供了关键的研究轴线，理论与实验均支持多模型系统的显著潜力。

Abstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.

</details>


### [136] [ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling](https://arxiv.org/abs/2512.23347)
*Hai Duong Nguyen,Xuan-The Tran*

Main category: cs.LG

TL;DR: ECG-RAMBA: a framework that disentangles morphology and rhythm for robust cross-domain ECG classification, using deterministic morphology via MiniRocket, HRV-based rhythm descriptors, and long-range context with a bi-directional Mamba backbone, powered by a stable Power Mean pooling. Achieves ~0.85 macro ROC-AUC on Chapman–Shaoxing, zero-shot PR-AUC 0.708 on CPSC-2021, and good cross-dataset performance on PTB-XL; ablations highlight morphology as foundation and rhythm/context as drivers of cross-domain robustness.


<details>
  <summary>Details</summary>
Motivation: Generalization across heterogeneous ECG acquisition settings is challenging due to entangled morphological patterns and rhythm dynamics that promote shortcut learning and distribution shift; separating morphology and rhythm and using context-aware fusion may improve cross-domain robustness and clinical deployment.

Method: ECG-RAMBA builds three components: (i) deterministic morphological features extracted by MiniRocket; (ii) global rhythm descriptors from HRV; (iii) long-range context modeling via a bi-directional Mamba backbone. It introduces a numerically stable Power Mean pooling (Q=3) to emphasize high-evidence segments during windowed inference. Evaluation follows a protocol-faithful setting with subject-level cross-validation, fixed decision threshold, and no test-time adaptation.

Result: On Chapman–Shaoxing dataset, macro ROC-AUC ≈ 0.85. In zero-shot transfer, PR-AUC = 0.708 for atrial fibrillation on CPSC-2021, outperforming a comparable raw-signal Mamba baseline. Cross-dataset robustness observed on PTB-XL. Ablations indicate morphology as a strong foundation; rhythm modeling and long-range context crucial for cross-domain robustness.

Conclusion: Deterministic morphology provides a strong foundation, while explicit rhythm modeling and long-range context are key drivers of cross-domain robustness.

Abstract: Deep learning has achieved strong performance for electrocardiogram (ECG) classification within individual datasets, yet dependable generalization across heterogeneous acquisition settings remains a major obstacle to clinical deployment and longitudinal monitoring. A key limitation of many model architectures is the implicit entanglement of morphological waveform patterns and rhythm dynamics, which can promote shortcut learning and amplify sensitivity to distribution shifts. We propose ECG-RAMBA, a framework that separates morphology and rhythm and then re-integrates them through context-aware fusion. ECG-RAMBA combines: (i) deterministic morphological features extracted by MiniRocket, (ii) global rhythm descriptors computed from heart-rate variability (HRV), and (iii) long-range contextual modeling via a bi-directional Mamba backbone. To improve sensitivity to transient abnormalities under windowed inference, we introduce a numerically stable Power Mean pooling operator ($Q=3$) that emphasizes high-evidence segments while avoiding the brittleness of max pooling and the dilution of averaging. We evaluate under a protocol-faithful setting with subject-level cross-validation, a fixed decision threshold, and no test-time adaptation. On the Chapman--Shaoxing dataset, ECG-RAMBA achieves a macro ROC-AUC $\approx 0.85$. In zero-shot transfer, it attains PR-AUC $=0.708$ for atrial fibrillation detection on the external CPSC-2021 dataset, substantially outperforming a comparable raw-signal Mamba baseline, and shows consistent cross-dataset performance on PTB-XL. Ablation studies indicate that deterministic morphology provides a strong foundation, while explicit rhythm modeling and long-range context are critical drivers of cross-domain robustness.

</details>


### [137] [ISOPO: Proximal policy gradients without pi-old](https://arxiv.org/abs/2512.23353)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: ISOPO是一种高效的一步近似自然策略梯度的方法，通过对序列对数概率梯度在Fisher度量下归一化再与优势相卷积来近似自然梯度；相比GRPO、CISPO等需要多次梯度步骤的近似方法，ISOPO在单次反向传播中实现，且实现成本可忽略不计，相对REINFORCE更高效。


<details>
  <summary>Details</summary>
Motivation: 自然策略梯度的计算通常成本高昂，现有的近端策略方法（如GRPO、CISPO）依赖多步梯度与重要性比剪裁来接近自然梯度，造成计算与样本成本较高，有待进一步提高效率。

Method: 提出ISOPO，在简单形式下，对每个序列的对数概率梯度在Fisher度量下归一化后再与优势相 contracted。另一种变体在每一层使用神经张量核(NTK)对微小批量的优势进行变换，并在逐层进行，借助单次向后传递实现，理论与实现上与REINFORCE相比几乎无额外计算开销。

Result: 声称在单步梯度下就能近似自然策略梯度，且方法对比GRPO/CISPO等需要多步更新的方案具有更高的效率，且在实现上对比REINFORCE几乎没有额外开销。没有给出具体实验结果的细节。

Conclusion: ISOPO提供了一种简单而高效的近似自然梯度的方法，能够通过单步梯度实现，且支持层级化的NTK变换，显著降低计算开销并提升实现速率。

Abstract: This note introduces Isometric Policy Optimization (ISOPO), an efficient method to approximate the natural policy gradient in a single gradient step. In comparison, existing proximal policy methods such as GRPO or CISPO use multiple gradient steps with variants of importance ratio clipping to approximate a natural gradient step relative to a reference policy. In its simplest form, ISOPO normalizes the log-probability gradient of each sequence in the Fisher metric before contracting with the advantages. Another variant of ISOPO transforms the microbatch advantages based on the neural tangent kernel in each layer. ISOPO applies this transformation layer-wise in a single backward pass and can be implemented with negligible computational overhead compared to vanilla REINFORCE.

</details>


### [138] [Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2](https://arxiv.org/abs/2512.23367)
*Yilun Luo,HuaQing Zheng,Haoqian Meng,Wenyuan Liu,Peng Zhang*

Main category: cs.LG

TL;DR: 通过低比特量化实现 openPangu-Embedded 模型在带 CoT 推理中的高效推理，在不显著损失精度的前提下提升速度和降低内存。


<details>
  <summary>Details</summary>
Motivation: 解决三种 CoT 推理带来的内存与延迟开销，使 big-LM/嵌入式部署在 Ascend NPUs 上具备 practicality（实用性）。

Method: 提出统一的低比特推理框架，支持 INT8 (W8A8) 与 W4A8 量化，针对 openPangu-Embedded 1B/7B 在 Atlas A2 上优化；结合 slow_think、auto_think、no_think 三种 CoT 思路进行评估，基于 HumanEval 与 MBPP 等代码生成基准。

Result: INT8 在 FP16 基线保留 >90% 精度，并在 Atlas A2 上实现约1.5x 的 prefill 加速；W4A8 在显著降低内存消耗的同时，存在中等程度的精度折中。

Conclusion: 低比特量化能在 Ascend NPUs 上有效支撑带 CoT 的推理，保持高模型保真度并提升推理效率，降低资源需求，推动实际部署的可行性。

Abstract: Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.

</details>


### [139] [On the Sample Complexity of Learning for Blind Inverse Problems](https://arxiv.org/abs/2512.23405)
*Nathan Buskulic,Luca Calatroni,Lorenzo Rosasco,Silvia Villa*

Main category: cs.LG

TL;DR: 在一个线性高斯框架下，针对未知前向算子的盲逆问题，提出以LMMSE为核心的分析框架，给出最优估计器的闭式表达并与适当的Tikhonov正则化形式等价，揭示正则化项如何依赖信号、噪声和随机前向算子分布；并给出收敛性、有限样本误差界，以及当随机性消失时的收敛速率，最后通过数值实验验证理论预测。


<details>
  <summary>Details</summary>
Motivation: 解决盲逆问题的理论缺口：在前向算子未知且数据驱动方法往往缺乏解释性与理论保障的背景下，提供一种可解释且具理论支撑的分析框架，提升鲁棒性与可信度。

Method: 以线性最小均方误差估计器（LMMSE）为核心，推导出盲逆问题的最优估计器的闭式解；将其与选择合适的正则化（Tikhonov）形式等价化，正则项显式依赖未知信号、噪声和随机前向算子分布；在适当的源条件假设下证明收敛性；推导有限样本误差界，刻画噪声水平、问题条件数和样本数对性能的影响，以及前向算子随机性的作用与消失时的收敛行为。

Result: 得到关于盲逆问题的理论结果：闭式最优估计器、与Tikhonov正则化的等价关系、在给定源条件下的收敛性、以及考虑样本量与噪声、随机前向算子分布的有限样本误差界与收敛速率；数值实验验证理论预测，并展示随机性降低时性能提升的趋势。

Conclusion: 在LMMSE框架内为盲逆问题提供可解释且可量化的理论分析，揭示随机前向算子对估计性能的影响及其随样本数和噪声水平的改善趋势，为数据驱动但需理论保障的方法提供重要的理论支撑与指引。

Abstract: Blind inverse problems arise in many experimental settings where the forward operator is partially or entirely unknown. In this context, methods developed for the non-blind case cannot be adapted in a straightforward manner. Recently, data-driven approaches have been proposed to address blind inverse problems, demonstrating strong empirical performance and adaptability. However, these methods often lack interpretability and are not supported by rigorous theoretical guarantees, limiting their reliability in applied domains such as imaging inverse problems. In this work, we shed light on learning in blind inverse problems within the simplified yet insightful framework of Linear Minimum Mean Square Estimators (LMMSEs). We provide an in-depth theoretical analysis, deriving closed-form expressions for optimal estimators and extending classical results. In particular, we establish equivalences with suitably chosen Tikhonov-regularized formulations, where the regularization depends explicitly on the distributions of the unknown signal, the noise, and the random forward operators. We also prove convergence results under appropriate source condition assumptions. Furthermore, we derive rigorous finite-sample error bounds that characterize the performance of learned estimators as a function of the noise level, problem conditioning, and number of available samples. These bounds explicitly quantify the impact of operator randomness and reveal the associated convergence rates as this randomness vanishes. Finally, we validate our theoretical findings through illustrative numerical experiments that confirm the predicted convergence behavior.

</details>


### [140] [Task-driven Heterophilic Graph Structure Learning](https://arxiv.org/abs/2512.23406)
*Ayushman Raghuvanshi,Gonzalo Mateos,Sundeep Prabhakar Chepuri*

Main category: cs.LG

TL;DR: FgGSL通过学习对称的特征驱动掩蔽，联合学习同质性与异质性图结构，并配合谱编码器，利用低/高通过滤器带来互补信息，从而提升异质图上的节点表征学习效果。


<details>
  <summary>Details</summary>
Motivation: 在异质性图上，GNN难以学习具辨别性的节点表示，因为连接的节点往往标签不同且特征相似性提供的结构线索薄弱。因此需要一个端到端的结构学习框架，能同时学习有利于同质性和异质性关系的图结构，并结合频域信息增强鲁棒性。

Method: 提出频率引导的图结构学习（FgGSL），通过一个可学习、对称且基于特征的掩蔽函数来推断两张互补的图（同质性与异质性），并使用预设的低通和高通图滤波器带进行处理。同时引入基于标签的结构损失，显式促进同质性和异质性边的恢复。理论上给出结构损失的稳定性界，以及在图扰动下对滤波器带的鲁棒性保证。

Result: 在六个异质基准数据集上的实验显示，FgGSL持续优于最先进的GNN和图重连方法，体现了将频率信息与有监督的拓扑推断相结合的优势。

Conclusion: 将频域信息与监督的拓扑学习结合，可在异质图上获得更具辨别性的节点表征，并给出端到端、鲁棒的图结构学习框架。

Abstract: Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.

</details>


### [141] [Theoretical Foundations of Scaling Law in Familial Models](https://arxiv.org/abs/2512.23407)
*Huan Song,Qingfei Zhao,Ting Long,Shuyu Tian,Hongjun An,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: Extends neural scaling laws to Familial models with multiple sub-models via granularity G, proposing L(N, D, G) and IsoFLOP design to quantify the trade-off between model size, data, and granularity under fixed compute; finds a small-exponent granularity penalty that enables train-once, deploy-many paradigms.


<details>
  <summary>Details</summary>
Motivation: Current scaling laws assume a single dense model output, which cannot capture Familial models with early exits and relay-style inference. There is a need to generalize scaling to a one-run, many-models setting to enable deployable sub-models across heterogeneous compute hierarchies.

Method: Introduce Granularity G as a fundamental scaling variable alongside model size N and training tokens D. Propose a unified functional form L(N, D, G) parametric in G, D, N. Employ IsoFLOP experimental design to isolate architectural impact from computational scale. Systematically sweep N and G under fixed budgets while dynamically adjusting D to decouple granularity cost from scaling benefits.

Result: Empirical results indicate the granularity penalty follows a multiplicative power law with an extremely small exponent, tying fixed-compute training to dynamic architectures. This supports train-once, deploy-many by preserving compute-optimality of dense baselines while enabling flexible granular sub-model deployment.

Conclusion: The study theoretically and empirically extends scaling laws to the one-run, many-models paradigm, validating the train-once deploy-many approach and providing a quantitative framework (L(N,D,G)) for predicting performance under granularity-aware architectures.

Abstract: Neural scaling laws have become foundational for optimizing large language model (LLM) training, yet they typically assume a single dense model output. This limitation effectively overlooks "Familial models, a transformative paradigm essential for realizing ubiquitous intelligence across heterogeneous device-edge-cloud hierarchies. Transcending static architectures, familial models integrate early exits with relay-style inference to spawn G deployable sub-models from a single shared backbone. In this work, we theoretically and empirically extend the scaling law to capture this "one-run, many-models" paradigm by introducing Granularity (G) as a fundamental scaling variable alongside model size (N) and training tokens (D). To rigorously quantify this relationship, we propose a unified functional form L(N, D, G) and parameterize it using large-scale empirical runs. Specifically, we employ a rigorous IsoFLOP experimental design to strictly isolate architectural impact from computational scale. Across fixed budgets, we systematically sweep model sizes (N) and granularities (G) while dynamically adjusting tokens (D). This approach effectively decouples the marginal cost of granularity from the benefits of scale, ensuring high-fidelity parameterization of our unified scaling law. Our results reveal that the granularity penalty follows a multiplicative power law with an extremely small exponent. Theoretically, this bridges fixed-compute training with dynamic architectures. Practically, it validates the "train once, deploy many" paradigm, demonstrating that deployment flexibility is achievable without compromising the compute-optimality of dense baselines.

</details>


### [142] [Stochastic Siamese MAE Pretraining for Longitudinal Medical Images](https://arxiv.org/abs/2512.23441)
*Taha Emre,Arunava Chakravarty,Thomas Pinetz,Dmitrii Lachinov,Martin J. Menten,Hendrik Scholl,Sobha Sivaprasad,Daniel Rueckert,Andrew Lotery,Stefan Sacu,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.LG

TL;DR: STAMP通过掩蔽自编码与随机过程相结合，在时序多模态医学影像中学习非确定性疾病进展的表征，超过现有 temporal MAE 和基础模型在AMD/AD进展预测上的表现。


<details>
  <summary>Details</summary>
Motivation: 需要在3D纵向医学数据中获得对疾病进展的时序感知表示；现有的确定性Siamese MAE未能建模疾病演化中的不确定性。

Method: 提出 STAMP（Siamese MAE + Masked Pretraining），将时间差信息作为条件变量对两个输入体积进行编码，通过将 MAE 重构损失改写为条件变分推断目标，以随机过程来学习时序动态。以ViT为 backbone，在两个OCT数据集和一份MRI数据集上进行预训练与评估。

Result: 在包含多次就诊的两组OCT数据和一组MRI数据上评估，STAMP的预训练ViT模型在AMD和AD进展预测任务上优于现有的时序MAE方法与基础模型，尤其在需要捕捉非确定性的时序动态的任务上表现更佳。

Conclusion: 将随机性整合入 MAE 框架的时序建模可显著提升长期病程预测的性能，适用于需要理解疾病非确定性时间演化的场景（如AMD、AD）。

Abstract: Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer's Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases.

</details>


### [143] [Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion](https://arxiv.org/abs/2512.23448)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: DSC提供一种高效且稳定的MoE变体，通过在共享基向量库上进行状态相关的稀疏扩展来近似上下文权重，减少参数和显存开销，并通过星形域和幅度门控的单纯形插值实现从单位身份的连续过渡，同时给出帧理论正则化和谱约束的最坏情况界限。


<details>
  <summary>Details</summary>
Motivation: 现有MoE与Mixture-of-LoRAs在扩展能力上存在表征崩塌和梯度不稳定的问题；直接增加组件导致参数复杂度和内存带宽显著增加，需要一个高效、稳定且可解释的动态上下文权重机制。

Method: 提出Dynamic Subspace Composition (DSC)：在共享基向量库上实现状态相关的稀疏扩展，通过残差轨迹在星形域中建模权重更新；使用幅度门控的简单形插值确保从单位身份的连续过渡；与标准的MoE-LO殴相比，DSC用解耦的单位范向量构成的秩-K组合近似，降低参数复杂度至O(M d)，内存带宽至O(K d)；通过帧理论正则化和谱约束提供对动态更新的严格 worst-case 边界。

Result: 理论层面给出参数和内存复杂度减小，以及对动态更新的边界约束；代码公开在GitHub，便于复现与扩展。

Conclusion: DSC通过在共享基向量的稀疏组合和稳健的几何-谱约束实现高效、稳定的上下文权重更新，缓解了MoE中的表示崩塌和梯度不稳问题。

Abstract: Mixture of Experts (MoE) models scale capacity but often suffer from representation collapse and gradient instability. We propose Dynamic Subspace Composition (DSC), a framework that approximates context-dependent weights via a state-dependent, sparse expansion of a shared basis bank. Formally, DSC models the weight update as a residual trajectory within a Star- Shaped Domain, employing a Magnitude-Gated Simplex Interpolation to ensure continuity at the identity. Unlike standard Mixture-of-LoRAs, which incurs O(M rd) parameter complexity by retrieving independent rank-r matrices, DSC constructs a compositional rank-K approximation from decoupled unit-norm basis vectors. This reduces parameter complexity to O(M d) and memory traffic to O(Kd), while Frame-Theoretic regularization and spectral constraints provide rigorous worst-case bounds on the dynamic update. The code is available at https://github. com/VladimerKhasia/DSC

</details>


### [144] [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](https://arxiv.org/abs/2512.23461)
*Zhuo Li,Pengyu Cheng,Zhechao Yu,Feifei Tong,Anningzhe Gao,Tsung-Hui Chang,Xiang Wan,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 提出 DIR（Debiasing via Information optimization for RM），一种基于信息理论的 RM 去偏方法。通过信息瓶颈思想，在 RLHF 场景下最大化 RM 分数与人类偏好对之间的互信息，同时最小化 RM 输出与偏置属性（如偏好输入的长度、奉承风格、格式等）之间的互信息，从而处理更复杂、非线性的偏差，提升 RM 的鲁棒性与 RLHF 性能。


<details>
  <summary>Details</summary>
Motivation:  reward models 的训练数据质量普遍较差，存在易过拟合、奖励劫持等 inductive biases。现有去偏方法多针对单一偏差或仅建模线性相关（如 Pearson 系数），难以处理更复杂的非线性偏差。需要一种能广泛适用于现实场景、能处理复杂偏差的去偏方法。

Method: 本工作提出基于信息瓶颈的去偏方法 DIR。核心思想是通过信息优化：最大化 RM 得分与人类偏好对之间的互信息 I(R; P)（或 I(S; P) 取决于符号定义），同时最小化 RM 输出与偏置属性 A 之间的互信息 I(S; A)，以抑制由偏置属性引入的信号。理论上提供信息论支撑，能处理非线性相关的复杂偏差。实现上可能包括对偏置属性的定义、可微估计互信息、以及在 RM 训练目标中引入信息约束项等。

Result: 在三类偏见上进行实验：响应长度、奉承（sycophancy）、格式。DIR 能有效减弱目标偏见，并提升 RLHF 的整体表现和泛化能力，在多个基准上优于对比方法。代码和训练方案公开在 GitHub。

Conclusion: 将信息瓶颈思想引入 RM 去偏，DIR 能覆盖更复杂的偏差类型（非线性相关），拓展 RM 去偏的应用场景，并提升 RLHF 的鲁棒性与泛化性能。

Abstract: Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.

</details>


### [145] [FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence](https://arxiv.org/abs/2512.23485)
*Guoan Wan,Tianyu Chen,Fangzheng Feng,Haoyi Zhou,Runhua Xu*

Main category: cs.LG

TL;DR: FRoD是一种新的参数高效微调方法，通过层级联合分解结合旋转自由度，使用全局共享基底和稀疏扰动实现接近全量微调的表达力，且仅需1.72%的可训练参数，在20项基准测试中达到相同准确率。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT如LoRA因低秩约束导致收敛慢和适应能力受限，急需提高表达力同时保持效率。

Method: 采用全局共享基底的层级联合分解，在缩放因子层注入稀疏可学习扰动，从而实现近似全秩更新，提升灵活性。

Result: 在涵盖视觉、推理和语言理解的20个基准上，FRoD达到与全量微调相同的准确率，且训练预算相同的情况下仅需1.72%的可训练参数。

Conclusion: FRoD提升了PEFT的表达力与效率，实现更快更鲁棒的收敛，兼具高效与全秩更新能力。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.

</details>


### [146] [Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization](https://arxiv.org/abs/2512.23493)
*Wei Gao,Paul Zheng,Peng Wu,Yulin Hu,Anke Schmeink*

Main category: cs.LG

TL;DR: 在CSI不完美的IIoT多设备URLLC场景下，提出一个联合链路自适应与设备调度（含顺序）的学习框架，结合BO驱动的TD3进行自适应MCS与服务顺序选择，以提高吞吐与满足严格BLER，且加入基于BO的训练机制以改善收敛和应对样本不平衡。


<details>
  <summary>Details</summary>
Motivation: URRLC 场景中CSI不完美、样本不平衡及强化学习算法对参数敏感性会损害收敛性与可靠性，需更高效、鲁棒的学习方法来实现高吞吐与严格BLER约束。

Method: 提出一个BO驱动的TD3框架，联合设备的服务顺序与MCS选择，并在不完美CSI条件下进行LA与调度；同时设计一个基于贝叶斯优化的训练机制，以提升收敛速度并缓解样本失衡问题。

Result: 仿真实验表明，所提算法在收敛速度与总吞吐量方面优于现有方法，对CSI不完美与样本不平衡具有更好的鲁棒性。

Conclusion: BO驱动的TD3方法可在IIoT URLLC场景下有效提升学习效率与系统吞吐，同时更好地满足严格BLER约束，适用于联合LA与设备调度的场景。

Abstract: In this article, we consider an industrial internet of things (IIoT) network supporting multi-device dynamic ultra-reliable low-latency communication (URLLC) while the channel state information (CSI) is imperfect. A joint link adaptation (LA) and device scheduling (including the order) design is provided, aiming at maximizing the total transmission rate under strict block error rate (BLER) constraints. In particular, a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method is proposed, which determines the device served order sequence and the corresponding modulation and coding scheme (MCS) adaptively based on the imperfect CSI. Note that the imperfection of CSI, error sample imbalance in URLLC networks, as well as the parameter sensitivity nature of the TD3 algorithm likely diminish the algorithm's convergence speed and reliability. To address such an issue, we proposed a BO based training mechanism for the convergence speed improvement, which provides a more reliable learning direction and sample selection method to track the imbalance sample problem. Via extensive simulations, we show that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.

</details>


### [147] [Trustworthy Machine Learning under Distribution Shifts](https://arxiv.org/abs/2512.23524)
*Zhuo Huang*

Main category: cs.LG

TL;DR: 聚焦分布移位下的可信机器学习，分析扰动移位、领域移位和模态移位，并在鲁棒性、可解释性、可适应性三个维度提出框架与方法。


<details>
  <summary>Details</summary>
Motivation: 分布移位是ML系统可靠性与泛化能力的核心挑战，提升信任度对社会应用至关重要；通过系统研究三类移位和三大信任维度，旨在提高AI的鲁棒性、通用性与责任性。

Method: 围绕三类移位场景（扰动、领域、模态），在鲁棒性、可解释性、可适应性三个维度开展理论分析与方法研究，提出相应解决方案与基本洞见，并关注效率、适应性与安全性。

Result: 原文摘要未给出具体实验结果或定量结论，预计将给出方向性结论、框架设计与评估标准，供后续验证。

Conclusion: 在分布移位环境下构建更可信的机器学习体系，提升鲁棒性、可解释性和适应性，同时关注高效性与安全性，为AI在社会中的应用提供更可靠的基础。

Abstract: Machine Learning (ML) has been a foundational topic in artificial intelligence (AI), providing both theoretical groundwork and practical tools for its exciting advancements. From ResNet for visual recognition to Transformer for vision-language alignment, the AI models have achieved superior capability to humans. Furthermore, the scaling law has enabled AI to initially develop general intelligence, as demonstrated by Large Language Models (LLMs). To this stage, AI has had an enormous influence on society and yet still keeps shaping the future for humanity. However, distribution shift remains a persistent ``Achilles' heel'', fundamentally limiting the reliability and general usefulness of ML systems. Moreover, generalization under distribution shift would also cause trust issues for AIs. Motivated by these challenges, my research focuses on \textit{Trustworthy Machine Learning under Distribution Shifts}, with the goal of expanding AI's robustness, versatility, as well as its responsibility and reliability. We carefully study the three common distribution shifts into: (1) Perturbation Shift, (2) Domain Shift, and (3) Modality Shift. For all scenarios, we also rigorously investigate trustworthiness via three aspects: (1) Robustness, (2) Explainability, and (3) Adaptability. Based on these dimensions, we propose effective solutions and fundamental insights, meanwhile aiming to enhance the critical ML problems, such as efficiency, adaptability, and safety.

</details>


### [148] [EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition](https://arxiv.org/abs/2512.23526)
*Maryam Mirzaei,Farzaneh Shayegh,Hamed Narimani*

Main category: cs.LG

TL;DR: 提出EGDA，通过全局和条件分布对齐以及图正则化，提升跨会话EEG情感识别鲁棒性；在SEED-IV上实现约81–83%准确率，并指明Gamma带及关键脑区的重要性。


<details>
  <summary>Details</summary>
Motivation: 跨记录会话的差异导致模型泛化困难；需要同时对齐边际与条件分布并保持数据的内在结构。

Method: EGDA框架：联合对齐全局（边际）与类条件分布，同时引入图正则化以保留EEG数据的潜在结构。

Result: 在SEED-IV数据集上的跨会话转移任务，准确率分别为81.22%、80.15%、83.27%，优于若干基线方法。

Conclusion: Gamma频段最具辨识力，中央-顶叶与前额叶区域对情感识别具有关键作用；所提方法提升跨会话鲁棒性并提供对脑信号的解释性分析。

Abstract: Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.

</details>


### [149] [VL-RouterBench: A Benchmark for Vision-Language Model Routing](https://arxiv.org/abs/2512.23562)
*Zhehao Huang,Baijiong Lin,Jingyuan Zhang,Jingying Wang,Yuhang Liu,Ning Lu,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: VL-RouterBench 提供一个系统性、可复现的视觉-语言模型路由基准，覆盖大规模数据与多模型，提出基于成本与准确度的调和平均排名，揭示现有路由器与理想Oracle之间的差距，并计划开源数据与工具链以促进研究比较。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏系统、可重复的评测基准来评价vision-language模型的路由能力，难以在不同路由方案之间进行公平、可比的比较。

Method: 构建VL-RouterBench：基于VLM的原始推理日志与评分日志，建立样本-模型对的质量与成本矩阵；覆盖14个数据集、3类任务组，共30,540个样本、15个开源模型与2个API模型、总计519,180个样本模型对与约3,449万token输入输出量；评测协议同时衡量平均准确度、平均成本和吞吐量，并使用经归一化的成本与准确度的调和均值来构建排序分数；评估10种路由方法及基线。

Result: 实现了显著的路由能力提升，但现有路由器仍显著落后于理想Oracle，提示在路由器结构上还存在改进空间，尤其是在更细的视觉信号与文本结构建模方面。

Conclusion: 将开源完整的数据构建过程和评测工具链，促进可比性、可重复性和在多模态路由研究中的实际部署。

Abstract: Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.

</details>


### [150] [Distribution-Free Process Monitoring with Conformal Prediction](https://arxiv.org/abs/2512.23602)
*Christopher Burger*

Main category: cs.LG

TL;DR: 将无分布假设的Conformal Prediction与传统控制图结合，提出Conformal-Enhanced控制图与Conformal-Enhanced过程监控，提升鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统统计过程控制（SPC）依赖若干统计假设，往往在现代复杂制造环境中被违反，导致监控不可靠。需要一种分布自由、模型无关且易于解释的新框架来提升质量控制的鲁棒性。

Method: 在SPC框架中嵌入Conformal Prediction的分布自由、模型无关保障，提出两种应用：1) Conformal-Enhanced Control Charts，通过预测区间与不确定性尖峰可视化过程不确定性并发出主动信号；2) Conformal-Enhanced Process Monitoring，将多变量控制问题转化为异常检测，利用p值图进行直观监控。

Result: 框架在理论上提供更高的鲁棒性与统计严格性，同时保持传统方法的可解释性与易用性，适合在复杂制造环境中提高监控的可靠性。

Conclusion: 通过将Conformal Prediction整合到SPC中，本文提出的两种应用实现了更稳健且易于解读的质量控制方法，有望提升实际生产过程的监控效果。

Abstract: Traditional Statistical Process Control (SPC) is essential for quality management but is limited by its reliance on often violated statistical assumptions, leading to unreliable monitoring in modern, complex manufacturing environments. This paper introduces a hybrid framework that enhances SPC by integrating the distribution free, model agnostic guarantees of Conformal Prediction. We propose two novel applications: Conformal-Enhanced Control Charts, which visualize process uncertainty and enable proactive signals like 'uncertainty spikes', and Conformal-Enhanced Process Monitoring, which reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart. Our framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability and ease of use of classic methods.

</details>


### [151] [Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning](https://arxiv.org/abs/2512.23617)
*Deniz Akdemir*

Main category: cs.LG

TL;DR: 提出一种替代对称不变性（UDA）的方法：通过勒卡姆（Le Cam）理论的方向性可模拟性来实现有风险转移的可控性，用Deficiency Distance衡量迁移风险，避免因信息不对称导致的负迁移。并给出一个通过学习源到靶的核映射实现的框架。


<details>
  <summary>Details</summary>
Motivation: 现实世界的分布漂移使得无监督领域自适应（UDA）中的对称不变性在信息不对称的情形下会导致信息损失和负迁移。需要一个风险受控、非对称且可解释的迁移框架，能够在不损害源信息的前提下实现跨域传递。

Method: 以Le Cam统计实验理论为基础，提出Le Cam Distortion及Deficiency Distance衡量源/靶之间的可模拟性。通过学习一个核映射实现从源到靶的单向模拟，替代对称不变性的严格最小化，给出迁移风险的上界。

Result: 在五个领域（基因组学、视觉、强化学习）上实现了有显著改进的迁移效果：如HLA基因组的频率估计相关性r=0.999，CIFAR-10保持81.2%的准确率（相比CycleGAN下降34.7%），以及在RL控制中实现安全的策略转移，避免了基于不变性的崩溃。

Conclusion: 为负迁移不可接受的领域（如医学成像、自动系统、精准医学）提供首个基于风险控制的迁移学习框架，强调方向性可模拟性而非对称不变性。

Abstract: Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing "negative transfer" that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam's theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.

</details>


### [152] [Random Controlled Differential Equations](https://arxiv.org/abs/2512.23670)
*Francesco Piatti,Thomas Cass,William F. Turner*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.
  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.

</details>


### [153] [End-to-End Test-Time Training for Long Context](https://arxiv.org/abs/2512.23675)
*Arnuv Tandon,Karan Dalal,Xinhao Li,Daniel Koceja,Marcel Rød,Sam Buchanan,Xiaolong Wang,Jure Leskovec,Sanmi Koyejo,Tatsunori Hashimoto,Carlos Guestrin,Jed McCaleb,Yejin Choi,Yu Sun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.

</details>


### [154] [Training AI Co-Scientists Using Rubric Rewards](https://arxiv.org/abs/2512.23707)
*Shashwat Goel,Rishi Hazra,Dulhan Jayalath,Timon Willi,Parag Jain,William F. Shen,Ilias Leontiadis,Francesco Barbieri,Yoram Bachrach,Jonas Geiping,Chenxi Whitehouse*

Main category: cs.LG

TL;DR: 通过自监督强化学习与自我评分机制，利用大规模论文语料训练语言模型生成更合规的研究计划，并在跨域场景（包括医学与新预印本）展现出良好泛化和显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在遵循约束和隐性要求时的不足，利用论文级别的研究目标与评分标准，构建可扩展、自动化的训练流程，以提升AI合作者在科研计划生成方面的质量与可执行性。

Method: 建立一个从论文自动抽取研究目标及目标特定评分量尺的可扩展数据源；通过自我打分的强化学习框架对初始策略进行微调，冻结初始策略作为评分器以形成生成-验证之间的差距；在机器学习领域的人类专家评审和跨域对领域的评估下进行验证，覆盖医学领域及新arXiv预印本，评估使用前沿模型的表现在跨域泛化。

Result: 人类专家对微调后模型在生成研究目标方面相对初始模型表现更好，优于70%的目标；自动提取的目标评分量尺获得84%的认可；在医学和新预印本等跨域场景也实现了12-22%的相对提升，显示出显著的跨域泛化能力。

Conclusion: 提出了一种可扩展的、自动化的训练方案，显著提升AI合作者在生成研究计划方面的能力，并对跨领域通用性与后续无监督或半监督微调的可能性给出积极信号。

Abstract: AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [155] [Validation methodology on real data of reversible Kalman Filter for state estimation with Manifold](https://arxiv.org/abs/2512.22126)
*Svyatoslav Covanov,Cedric Pradalier*

Main category: eess.SY

TL;DR: 扩展先前在流形上进行状态估计的工作，提出一个在真实数据中判定何时“可逆卡尔曼滤波”优于经典变体的检测步骤分析与度量。


<details>
  <summary>Details</summary>
Motivation: 解决先前工作在真实数据中对测量噪声的敏感性及仅依赖传感器噪声的理论假设所带来的局限性；希望给出一个在何时选择可逆KF的判定框架。

Method: 系统研究检测步骤，提出一种可证明何时可逆KF优于经典变体的思路；设计一个度量，用以在真实场景中区分两者的优劣，并在合成数据上验证性质。

Result: 提出用于区分可逆KF相对于经典乘法形式在何时具有改进的时刻的度量，以及围绕检测步骤的理论分析路径。

Conclusion: 为在真实数据中应用可逆KF提供了判定准则与评估工具，指明在特定情形下可逆KF能带来改进，并给出实现该改进的条件。

Abstract: This work extends a previous study that introduced an algorithm for state estimation on manifolds within the framework of the Kalman filter. Its objective is to address the limitations of the earlier approach. The reversible Kalman filter was designed to provide a methodology for evaluating the accuracy of existing Kalman filter variants with arbitrary precision on synthetic data. It has favorable numerical properties on synthetic data, achieving arbitrary precision without relying on the small-velocity assumption and depending only on sensor noise. However, its application to real data encountered difficulties related to measurement noise, which was mitigated using a heuristic. In particular, the heuristic involved an event detection step switching between reversible Kalman filter and classical Kalman variant at chosen moments. In the present work, we propose a study of this detection step and propose a methodology to prove at which moment the reversible Kalman approach improves on classical multiplicative variant. In particular, we propose a metric allowing one to discriminate situations in real-world scenarios where it behaves better than classical approach.

</details>


### [156] [Optimal Regulation of Nonlinear Input-Affine Systems via an Integral Reinforcement Learning-Based State-Dependent Riccati Equation Approach](https://arxiv.org/abs/2512.22668)
*Arya Rashidinejad Meibodi,Mahbod Gholamali Sinaki,Khalil Alipour*

Main category: eess.SY

TL;DR: 提出一种基于积分强化学习的部分模型自由策略，用于在非线性输入相关系统中近似求解状态相关黎卡提方程（SDRE）下的控制器，与经典SDRE相比在二阶非线性系统仿真中可实现相近性能，且不需要显式漂移动力学模型。


<details>
  <summary>Details</summary>
Motivation: SDRE依赖完整系统模型并在每个状态求解ARE，计算复杂且对模型误差敏感。需要一种在尽量少的模型信息下也能学习近似最优控制的替代方法，提升对非线性系统的鲁棒性和应用灵活性。

Method: 提出在系统的每一状态下解ARE的近似解的同时，使用Integral Reinforcement Learning（IRL）进行在线学习，从而获得在各状态下的最优控制，而不显式知道漂移项的动力学；在二阶非线性输入耦合系统上进行仿真并与经典SDRE进行对比。

Result: 仿真结果显示，在足够迭代后，IRL基方法的控制性能接近传统SDRE方法，证明了在无显式环境模型时的可行性与可靠性。

Conclusion: 该IRL-based方法为缺少显式模型信息的非线性系统提供了一种可行的近似SDRE解的途径，适用于需要部分模型自由且希望保持较高控制性能的场景。

Abstract: The State-Dependent Riccati Equation (SDRE) technique generalizes the classical algebraic Riccati formulation to nonlinear systems by designing an input to the system that optimally(suboptimally) regulates system states toward the origin while simultaneously optimizing a quadratic performance index. In the SDRE technique, we solve the State-Dependent Riccati Equation to determine the control for regulating a nonlinear input-affine system. Since an analytic solution to SDRE is not straightforward, one method is to linearize the system at every state, solve the corresponding Algebraic Riccati Equation (ARE), and apply optimal control until the next state of the system. Completing this task with high frequency gives a result like the original SDRE technique. Both approaches require a complete model; therefore, here we propose a method that solves ARE in every state of the system using a partially model-free approach that learns optimal control in every state of the system, without explicit knowledge of the drift dynamics, based on Integral Reinforcement Learning (IRL). To show the effectiveness of our proposed approach, we apply it to the second-order nonlinear system in simulation and compare its performance with the classical SDRE method, which relies on the system's model and solves the ARE at each state. Our simulation results demonstrate that, with sufficient iterations, the IRL-based approach achieves approximately the same performance as the conventional SDRE method, demonstrating its capability as a reliable alternative for nonlinear system control that does not require an explicit environmental model. Index Terms-Algebraic Riccati Equation (ARE), Integral Reinforcement Learning (IRL), Nonlinear Input-Affine Systems, Optimal Regulation, State-Dependent Riccati Equation (SDRE)

</details>


### [157] [From Electrochemical Energy Storage to Next-Generation Intelligent Battery Technologies for Electric Vehicles: A Survey](https://arxiv.org/abs/2512.22680)
*Abderaouf Bahi,Amel Ourici,Chaima Lagraa,Siham Lameche,Soundess Halimi,Inoussa Mouiche,Ylias Sabri,Waseem Haider,Mohamed Trari*

Main category: eess.SY

TL;DR: 对最近在电化学储能领域的进展进行综述，涵盖Na+-离子、金属离子及金属空气电池、电极工程、电解质、固态电解质界面控制，以及机器学习、数字孪生、大型语言模型在智能电池管理中的应用，讨论挑战、研究空白和未来前景。


<details>
  <summary>Details</summary>
Motivation: 随着电动车对高性能、长寿命且安全的储能系统需求日益增长，需要系统梳理当前技术、材料与AI驱动优化的进展与挑战。

Method: 对近年文献进行系统性综述，汇总电池体系、材料、工艺、以及AI技术在电池管理中的应用，提出分类框架与对比分析。

Result: 揭示了电池技术的关键进展、可行路径和实现挑战；提出混合化学、规模化制造、可持续性及AI优化等研究方向；强调AI驱动的优化对性能、安全和寿命的潜在提升。

Conclusion: 为研究者、工程师和行业专业人员提供面向未来的下一代电池技术的综合理解，促进电动汽车领域的技术转化与产业化。

Abstract: This study provides a comprehensive overview of recent advances in electrochemical energy storage, including Na+ -ion, metal-ion, and metal-air batteries, alongside innovations in electrode engineering, electrolytes, and solid-electrolyte interphase control. It also explores the integration of machine learning, digital twins, large language models and predictive analytics to enable intelligent battery management systems, enhancing performance, safety, and operational longevity. Key challenges, research gaps, and future prospects are addressed, highlighting opportunities presented by hybrid chemistry, scalable manufacturing, sustainability, and AI-driven optimization. This survey aims to provide researchers, engineers, and industry profesionnals with a comprehensive understanding of next-generation battery technologies for the evolving electric vehicles sector.

</details>


### [158] [A Time-Barrier Lyapunov Condition for Predefined-Time Stability](https://arxiv.org/abs/2512.22786)
*Özhan Bingöl*

Main category: eess.SY

TL;DR: 提出带时间屏障的预定时稳定性概念及其充分条件，强调通过非自治Lyapunov机制在预设截止时间前实现收敛，与自治形式不同。


<details>
  <summary>Details</summary>
Motivation: 解决基于自治Lyapunov不等式的预定时稳定性仅能提供收敛上界的问题，寻求硬截止时间的收敛保证。

Method: 引入非自治Lyapunov结构的时间相关屏障项，证明随着时间屏障的发散，系统在预定截止时间前收敛；给出一个充分条件以保证该性质。

Result: 给出一种无法被经典自治预定时稳定性公式重现的新的稳定性概念，且提供简明的实现途径。

Conclusion: 时间屏障预定时稳定性为在非线性系统中强制实现硬收敛截止时间提供一个清晰、透明的框架。

Abstract: Predefined-time stability enables convergence within a user-specified time independent of initial conditions. Existing results are predominantly based on autonomous Lyapunov inequalities, where the predefined-time is realized through integral bounds on state-dependent decay and therefore acts as an upper bound rather than a structurally enforced deadline. This paper introduces a time-barrier predefined-time stability concept in which convergence is enforced through a nonautonomous Lyapunov mechanism that intrinsically restricts the remaining available time. A sufficient Lyapunov-based condition is established, guaranteeing convergence before the predefined deadline via divergence of a time-dependent barrier. It is further shown that this mechanism cannot be reproduced by classical autonomous predefined-time stability formulations, thereby constituting a distinct stability notion. The proposed approach provides a concise and transparent means of enforcing hard convergence deadlines in nonlinear systems.

</details>


### [159] [Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach](https://arxiv.org/abs/2512.22793)
*Minh Bui,Simon Monckton,Mo Chen*

Main category: eess.SY

TL;DR: 提出一种将三维追捕避让（RA）游戏分解为水平子博弈与垂直子博弈的降维框架，结合Hamilton-Jacobi（HJ） reachability解决子博弈，并引入考虑 Defender 加速度的二阶动力学；通过基于HJ的跟踪控制对原始游戏进行重建，给出捕获性保证的条件，理论与数值/ Gazebo 仿真验证显示保持最优性与全局捕获能力，首次在三维空间中实现对四旋翼的成功捕获（据称）。


<details>
  <summary>Details</summary>
Motivation: 在安全与防务场景中，RA 游戏在包含障碍和对手的非线性三维动力学下求解具有挑战性；传统HJ reachability难以直接扩展到三维，其他方法要么缺乏对三维复杂动力学的通用性，要么无法同时确保最优性与可证明的捕获性；因此需一种可扩展的三维降维框架与理论保障。

Method: 将三维 RA 问题分解为水平子博弈和垂直子博弈，分别应用 HJ reachability 进行求解；考虑防守者的二阶动力学（加速度）以更真实地刻画 defender；用基于 HJ 的跟踪控制在子博弈中重建原始 RA 问题的解，确保捕获并在此后能够跟踪攻击者；给出能保持捕获保证的条件，并通过数值仿真和 Gazebo 物理仿真验证其有效性。

Result: 分解保留了原始问题的最优性与保证性；数值实验显示策略在三维空间中保持有效性并近似最优；Gazebo 验证首次在三维空间实现对四旋翼的成功捕获（在相关领域声称首次实现）。

Conclusion: 该方法提供了一条可扩展的三维 RA 问题求解路径，通过降维与 HJ reachability 结合的框架实现对非平滑动力学的鲁棒处理，并通过理论条件与仿真验证了捕获与跟踪的可证明性与可实现性。未来工作可扩展至多智能体、鲁棒性分析、以及实时实现等方向。

Abstract: Reach-avoid (RA) games have significant applications in security and defense, particularly for unmanned aerial vehicles (UAVs). These problems are inherently challenging due to the need to consider obstacles, consider the adversarial nature of opponents, ensure optimality, and account for nonlinear dynamics. Hamilton-Jacobi (HJ) reachability analysis has emerged as a powerful tool for tackling these challenges; however, while it has been applied to games involving two spatial dimensions, directly extending this approach to three spatial dimensions is impossible due to high dimensionality. On the other hand, alternative approaches for solving RA games lack the generality to consider games with three spatial dimensions involving agents with non-trivial system dynamics. In this work, we propose a novel framework for dimensionality reduction by decomposing the problem into a horizontal RA sub-game and a vertical RA sub-game. We then solve each sub-game using HJ reachability analysis and consider second-order dynamics that account for the defender's acceleration. To reconstruct the solution to the original RA game from the sub-games, we introduce a HJ-based tracking control algorithm in each sub-game that not only guarantees capture of the attacker but also tracking of the attacker thereafter. We prove the conditions under which the capture guarantees are maintained. The effectiveness of our approach is demonstrated via numerical simulations, showing that the decomposition maintains optimality and guarantees in the original problem. Our methods are also validated in a Gazebo physics simulator, achieving successful capture of quadrotors in three spatial dimensions space for the first time to the best of our knowledge.

</details>


### [160] [Assessment of a Hybrid Energy System for Reliable and Sustainable Power Supply to Boru Meda Hospital in Ethiopia](https://arxiv.org/abs/2512.22859)
*Tegenu Argaw Woldegiyorgis,Hong Xian Li,Fekadu Chekol Admassu,Merkebu Gezahegne,Abdurohman Kebede,Tadese Abera,Haris Ishaq,Eninges Asmare*

Main category: eess.SY

TL;DR: 评估霍尔斯能源系统在埃塞俄比亚Boru Meda医院的“混合能源系统”可行性，使用HOMER Pro 3.11.2进行多方案对比，PV/BG/蓄电/变流配置成本最低且实现100%可再生，简单回收期约7.26年；DG混合方案也具备较高投资回报。


<details>
  <summary>Details</summary>
Motivation: 在能源匮乏的地区通过优化混合能源系统提升医院供电的可靠性、降低运营成本与排放，促进可持续能源发展目标的实现。

Method: 利用HOMER Pro 3.11.2对多种HRES方案进行集成优化与对比评估，考虑现有资源与医院能源需求，包含PV、 biomass generator、风力、柴油发电机、蓄电池与变流器等子系统的耦合。

Result: PV/BG/蓄电/变流配置实现日用电量11,214.66 kWh，具最低LCOE0.339美元/kWh、NPV约2570万美元、100%可再生能源比例、简单回报期7.26年；可完全避免500升/月柴油使用。DG集成混合具较高经济价值，ROI约20%、IRR18%、回收期7.21–8.71年。总体上，混合系统在成本、可靠性与可持续性之间实现良好平衡，是埃塞俄比亚能源短缺地区医院及类似区域的可扩展解。

Conclusion: 该研究表明在资源约束的场景下，基于PV、BG、蓄电和变流的混合能源系统具有显著的经济与环境优势，具备推广到类似场景的潜力与可持续发展意义。

Abstract: This study aims to evaluate the techno-economic feasibility of hybrid energy systems (HES) including Grid for providing reliable and sustainable power to Boru Meda Hospital, Ethiopia. HOMER pro 3.11.2 was used to design and evaluate a novel, integrated optimization and comparative assessment of diverse HRES, specif ically adjusted to the energy consumptions and available resources of the Hospital. The scenario evaluation showed that interconnecting photovoltaic (PV), biomass generator (BG), wind power (WP), diesel generator (DG), battery, and converter can effectively provide the Hospital's daily energy consumption of 11,214.66 kWh while conforming reliability and reducing emissions. The PV/BG/batt/conv configuration emerged as the most cost-effective and sustainable alternative, attaining the lowest LCOE of \$0.339/kWh, an NPC of \$25.7 million, and a 100% renewable energy fraction with simple pay back of 7.26 yr. As a result, the operational cost associated with the consumption of 500.00 L of diesel per month can be entirely avoided. The DG-integrated hybrids exhibit advanced techno-economic capability with significant worth, strong ROI (20\%) and IRR (18\%), endorsed by fast capital recovery (7.21-8.71 years). Overall, the hybrid system offers an optimal balance of cost, reliability, and sustainability, making it a promising and scalable solution for electrification of energy scare institution and areas in Ethiopia, thereby contributing to national sustainable energy development goals.

</details>


### [161] [Distributed Fusion Estimation with Protecting Exogenous Inputs](https://arxiv.org/abs/2512.22914)
*Liping Guo,Jimin Wang,Yanlong Zhao,Ji-Feng Zhang*

Main category: eess.SY

TL;DR: 在分布式融合估计中通过对局部估计注入互独立噪声，在保证 (ε,δ)-差分隐私的同时实现隐私保护；并通过松弛优化和协方差交叉的融合算法，以及引入反馈机制提升估计精度，展示隐私-精度权衡。


<details>
  <summary>Details</summary>
Motivation: 直接传输局部估计可能暴露外生输入的隐私信息，需要在保护隐私的同时维持分布式融合估计的性能。

Method: 对局部估计注入互相独立的噪声；构造约束优化问题以在最小化局部估计均方误差之和的同时满足 (ε,δ)-差分隐私；针对非凸性采用松弛求解以高效获得解；给出基于 Covariance Intersection 的差分隐私分布式融合估计算法；通过引入反馈机制在相同隐私约束下提升融合估计精度；通过示例验证方法的有效性及隐私等级与融合精度之间的权衡。

Result: 在处理非凸性问题方面通过松弛策略实现高效求解，同时确保差分隐私约束不被降低；提出的差分隐私分布式融合算法在 Covariance Intersection 框架下有效运行，并通过反馈机制提升估计精度；实例揭示隐私水平(ε,δ)与融合误差之间的权衡关系并验证方法的有效性。

Conclusion: 本文提出的隐私保护分布式融合估计方案在保护外生输入隐私的前提下实现准确的融合估计，提供了可控的隐私-精度权衡、有效的实现路径并通过实例得到验证。

Abstract: In the context of distributed fusion estimation, directly transmitting local estimates to the fusion center may cause a privacy leakage concerning exogenous inputs. Thus, it is crucial to protect exogenous inputs against full eavesdropping while achieving distributed fusion estimation. To address this issue, a noise injection strategy is provided by injecting mutually independent noises into the local estimates transmitted to the fusion center. To determine the covariance matrices of the injected noises, a constrained minimization problem is constructed by minimizing the sum of mean square errors of the local estimates while ensuring (ε, δ)-differential privacy. Suffering from the non-convexity of the minimization problem, an approach of relaxation is proposed, which efficiently solves the minimization problem without sacrificing differential privacy level. Then, a differentially private distributed fusion estimation algorithm based on the covariance intersection approach is developed. Further, by introducing a feedback mechanism, the fusion estimation accuracy is enhanced on the premise of the same (ε, δ)-differential privacy. Finally, an illustrative example is provided to demonstrate the effectiveness of the proposed algorithms, and the trade-off between differential privacy level and fusion estimation accuracy.

</details>


### [162] [Real-Time Forward Kinematics and Jacobians for Control of an MRI-Guided Magnetically Actuated Robotic Catheter](https://arxiv.org/abs/2512.23085)
*Ran Hao,Yuttana Itsarachaiyot,Yen-Chun Chen,M. Cenk Çavuşoğlu*

Main category: eess.SY

TL;DR: 提出一种基于静态Cosserat-rod理论的MRI驱动导管的正向运动学及解析雅可比矩阵的实时计算方法，并在原型机上验证其实时性与可用性。


<details>
  <summary>Details</summary>
Motivation: 解决MRI环境下导管机器人在实时、精确控制中的挑战，需快速、稳定的正向运动学与雅可比分析以支持闭环控制。

Method: 将导管建模为由刚性与柔性段组成，利用嵌入在导管体上的微线圈在MRI磁场中产生磁转矩来驱动；采用静态Cosserat-rod理论推导正向运动学的实时计算，并给出正向运动学的解析雅可比矩阵计算方法。

Result: 在单线圈集的原型机上进行实验，利用卡塔-对立寻踪立体视觉跟踪导管尖端轨迹，验证与期望轨迹的一致性，表明所提方法在实时性、可重复性和计算效率方面具有良好性能，能够实现开放环控制的复杂轨迹。

Conclusion: 该方法为在MR成像反馈下实现精确闭环控制铺平道路，促进MRI引导下的导管机器人实现实时鲁棒控制。

Abstract: This paper presents a forward kinematics and analytical Jacobian computation approach for real-time control of a novel magnetic resonance imaging (MRI)-actuated robotic catheter. The MRI-actuated robotic catheter is modeled as a series of rigid and flexible segments and actuated by magnetic torques generated on a set of current-carrying microcoils embedded on the catheter body by the magnetic field of the MRI scanner. First, a real-time forward kinematic modeling approach of the robotic catheter employing the static Cosserat-rod theory is presented. Second, the analytical calculation approach of the forward kinematic Jacobians of the proposed forward kinematic model is presented. The accuracy, reproducibility, and computational efficiency of the proposed methods are evaluated using a robotic catheter prototype with a single coil set, where catheter tip trajectories collected by a catadioptric stereo camera tracking system are validated using the desired tip trajectories. Experimental results demonstrate that the proposed method can successfully control the catheter in an open loop to perform complex trajectories with real-time computational efficiency, paving the way for accurate closed-loop control with real-time MR-imaging feedback.

</details>


### [163] [Multi-objective control strategy of Electro-Mechanical Transmission Based on Driving Pattern Division](https://arxiv.org/abs/2512.23186)
*Yanbo Li,Jinsong Li,Zongjue Liu,Riming Xu*

Main category: eess.SY

TL;DR: 基于对EMT重型车的驱动需求和功率平衡，提出在不同工况下的综合优化目标，并通过AHP加权得到单一目标，再利用DP实现多目标控制策略，对比规则策略，仿真结果显示综合性能显著提升，燃油经济性尤其明显改善。


<details>
  <summary>Details</summary>
Motivation: 解决EMT重型车在不同工况下的多目标优化难题，提升综合性能与燃油经济性。

Method: 将多目标通过层次分析法（AHP）进行加权，得到在不同工作条件下的综合优化目标；在动态规划（DP）框架下提出适用于不同驾驶模式的多目标控制策略；通过仿真与规则策略对比进行验证。

Result: 在仿真中综合性能显著提升，燃油经济性有明显改善，优于规则策略。

Conclusion: 将AHP加权与DP结合的多目标控制策略对EMT重型车在不同工况下有效，具有应用潜力，但需进一步的实车验证与鲁棒性评估。

Abstract: Based on the driving requirement and power balance of heavy-duty vehicle equipped with Electro-Mechanical Transmission (EMT), optimization goals under different driving patterns are put forward. The optimization objectives are changed into a comprehensive optimization target based on the method of weighting, which is calculated by using analytic hierarchy process (AHP) under different working conditions. According to theory of Dynamic Programming (DP), a multi-object control strategy of DP under different driving patterns is proposed. This strategy is verified by simulation and contrasted with rule strategy, the results show that comprehensive performance is significantly enhanced, and the fuel economy is highly improved especially.

</details>


### [164] [The Dawn of Agentic EDA: A Survey of Autonomous Digital Chip Design](https://arxiv.org/abs/2512.23189)
*Zelin Zang,Yuhang Song,Bingo Wing-Kuen Ling,Aili Wang,Fuji Yang*

Main category: eess.SY

TL;DR: A survey on integrating Generative AI and Agentic AI in Digital EDA, tracing a shift from AI-assisted design (AI4EDA) to AI-native and agentic design, with multimodal agents spanning frontend RTL generation, verification, and backend physical design; supported by case studies from microarchitecture to GDSII and featuring cross-stage feedback loops using PPA metrics, while addressing security, hallucinations, data scarcity, and black-box issues, and outlining a roadmap toward L4 autonomous chip design.


<details>
  <summary>Details</summary>
Motivation: To accelerate and automate digital chip design by leveraging AI and agentic reasoning across the entire EDA flow, enabling autonomous optimization, error repair, and security/privacy improvements, while identifying challenges and future directions in AI-Native/Agentic EDA.

Method: A comprehensive literature survey and conceptual synthesis that traces the evolution from CAD to AI4EDA and AI-Native/Agentic paradigms, describes agentic cognitive architectures built on multimodal foundation models, surveys frontend activities (RTL generation, intelligent verification) and backend design (algorithms, tool orchestration), and presents integrated case studies from microarchitecture to GDSII; analyzes cross-stage feedback loops, security implications, and practical challenges.

Result: Provides a unified framework and taxonomy for Agentic EDA, outlines architectural components and workflows (including feedback from backend PPA metrics to frontend logic), demonstrates viability through integrated case studies, and offers a strategic roadmap from AI-assisted tools to autonomous design engineers along with security and privacy considerations.

Conclusion: Agentic EDA is an emergent field with high potential to redefine chip design workflows. The paper argues for a phased transition from AI-assisted to autonomous design, highlights key research challenges (hallucinations, data scarcity, black-box tools), and points to future trends toward L4 autonomous chip design and broader adoption of agentic cognitive architectures.

Abstract: This survey provides a comprehensive overview of the integration of Generative AI and Agentic AI within the field of Digital Electronic Design Automation (EDA). The paper first reviews the paradigmatic evolution from traditional Computer-Aided Design (CAD) to AI-assisted EDA (AI4EDA), and finally to the emerging AI-Native and Agentic design paradigms. We detail the application of these paradigms across the digital chip design flow, including the construction of agentic cognitive architectures based on multimodal foundation models, frontend RTL code generation and intelligent verification, and backend physical design featuring algorithmic innovations and tool orchestration. We validate these methodologies through integrated case studies, demonstrating practical viability from microarchitecture definition to GDSII. Special emphasis is placed on the potential for cross-stage feedback loops where agents utilize backend PPA metrics to autonomously refine frontend logic. Furthermore, this survey delves into the dual-faceted impact on security, covering novel adversarial risks, automated vulnerability repair, and privacy-preserving infrastructure. Finally, the paper critically summarizes current challenges related to hallucinations, data scarcity, and black-box tools, and outlines future trends towards L4 autonomous chip design. Ultimately, this work aims to define the emerging field of Agentic EDA and provide a strategic roadmap for the transition from AI-assisted tools to fully autonomous design engineers.

</details>


### [165] [A Learning-Driven Stochastic Hybrid System Framework for Detecting Unobservable Contingencies in Power Systems](https://arxiv.org/abs/2512.23205)
*Hamid Varmazyari,Masoud H. Nazari*

Main category: eess.SY

TL;DR: 提出了一种新的学习型随机混合系统（LSHS）框架，用于现代电力系统中的情景/事件检测与分类，能够识别标准感知不到的未观测事件，如保护系统故障的误动作；通过分析系统输出与行为偏差，将情景分为物理、控制和测量三类；在 SHS 构架中整合系统动力学与观测误差动力学，利用机器学习分类器实现快速分类；在IEEE 5-bus和30-bus系统的仿真结果显示检测速度与准确度显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升对电力系统中潜在异常和未观测事件的检测能力，克服传统监控对某些保护功能失效等情形的盲点；需要结合系统动力学与观测误差分析以实现快速、鲁棒的情景识别。

Method: 提出一个基于学习的随机混合系统（LSHS）框架，将系统动力学与观测器误差动力学整合，并使用机器学习分类器将检测到的偏差信号归类为物理、控制、测量三类情景；通过仿真在IEEE 5-bus、30-bus 系统验证性能。

Result: 在仿真中，所提方法在检测速度和准确性方面显著优于现有方法，能够识别未观测事件并提供快速、准确的情景分类。

Conclusion: 该框架将学习能力与 SHS 模型相结合，提升对 contingencies 的检测与分类能力，尤其对不可观测事件具有潜在的鲁棒性与应用前景。

Abstract: This paper presents a new learning based Stochastic Hybrid System (LSHS) framework designed for the detection and classification of contingencies in modern power systems. Unlike conventional monitoring schemes, the proposed approach is capable of identifying unobservable events that remain hidden from standard sensing infrastructures, such as undetected protection system malfunctions. The framework operates by analyzing deviations in system outputs and behaviors, which are then categorized into three groups: physical, control, and measurement contingencies based on their impact on the SHS model. The SHS model integrates both system dynamics and observer-driven state estimation error dynamics. Within this architecture, machine learning classifiers are employed to achieve rapid and accurate categorization of contingencies. The effectiveness of the method is demonstrated through simulations on the IEEE 5-bus and 30-bus systems, where results indicate substantial improvements in both detection speed and accuracy compared with existing approaches.

</details>


### [166] [Revealing design archetypes and flexibility in e-molecule import pathways using Modeling to Generate Alternatives and interpretable machine learning](https://arxiv.org/abs/2512.23284)
*Mahdi Kchaou,Francesco Contino,Diederik Coppitters*

Main category: eess.SY

TL;DR: 提出在成本容忍度内生成近成本最优解的多方案分析框架，并结合可解释性机器学习从解空间提取洞见，应用于氢进口路径的多载体场景，揭示近最优解的广阔空间与关键权衡。


<details>
  <summary>Details</summary>
Motivation: 单一成本最优解在现实约束（监管、空间、利益相关者等）下往往不可行；需要在可接受成本范围内探索多样化方案以提高鲁棒性与灵活性。

Method: 1) 采用‘建模生成替代方案’（Modeling to Generate Alternatives）在给定成本上限内产生大量近似最优解；2) 使用可解释性机器学习从解空间提取洞见；3) 将方法应用于氢进口路径，考虑载体包括氢、氨、甲烷、甲醇。

Result: 揭示广阔的近最优解空间：太阳能、风能与储能并非必须严格维持在成本最优前10%；风的约束条件偏好太阳-储能-甲醇等路径；有限储能情形倾向于风能驱动的氨或甲烷路径。

Conclusion: 该方法揭示设计选择的鲁棒性与路径多样性，强调在能源进口策略设计中应纳入近最优解集及相关约束以提升系统灵活性与实现可持续性目标。

Abstract: Given the central role of green e-molecule imports in the European energy transition, many studies optimize import pathways and identify a single cost-optimal solution. However, cost optimality is fragile, as real-world implementation depends on regulatory, spatial, and stakeholder constraints that are difficult to represent in optimization models and can render cost-optimal designs infeasible. To address this limitation, we generate a diverse set of near-cost-optimal alternatives within an acceptable cost margin using Modeling to Generate Alternatives, accounting for unmodeled uncertainties. Interpretable machine learning is then applied to extract insights from the resulting solution space. The approach is applied to hydrogen import pathways considering hydrogen, ammonia, methane, and methanol as carriers. Results reveal a broad near-optimal space with great flexibility: solar, wind, and storage are not strictly required to remain within 10% of the cost optimum. Wind constraints favor solar-storage methanol pathways, while limited storage favors wind-based ammonia or methane pathways.

</details>


### [167] [Control Co-design of systems with parabolic partial differential equation dynamics](https://arxiv.org/abs/2512.23420)
*Antika Yadav,Prasad Vilas Chanekar*

Main category: eess.SY

TL;DR: 本文研究带抛物型PDE动力学的控制共设计CCD问题，给出一个带矩阵代数约束的近似化模型，通过梯度法求解并证明最优解能稳定被控PDE系统，且数值实例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在分布参数系统中，同时对控制器和系统参数进行协同优化以提升稳定性与性能，但直接求解CCD问题计算复杂。通过将问题近似为带矩阵代数约束的可解形式，降低复杂度并提供实现途径。

Method: 将CCD问题离散化并转化为带矩阵代数约束的近似优化问题，采用梯度下降/梯度基方法求解，给出稳定性证明，并通过数值实验进行验证。

Result: 提出的近似CCD求解框架在梯度迭代下收敛，得到的控制与设计参数使PDE系统稳定，数值示例表明方法可行。

Conclusion: 给出一种可执行的CCD求解框架，在所给的近似模型下，最优解具有稳定性保证，数值结果与理论结论吻合，适用于具有抛物型PDE动力学的系统。

Abstract: In this paper we study the control co-design (CCD) synthesis problem for a class of systems with parabolic partial differential equation (PDE) dynamics. We formulate CCD problem and finally derive an approximate CCD problem with matrix algebraic constraint. We then solve this approximate problem with gradient-based method and prove that the optimal solution also stabilizes the PDE system. We justify approach through numerical examples.

</details>


### [168] [NashOpt - A Python Library for Computing Generalized Nash Equilibria](https://arxiv.org/abs/2512.23636)
*Alberto Bemporad*

Main category: eess.SY

TL;DR: NashOpt 是一个开源的 Python 库，用于在具有共享约束和实数决策变量的非合作博弈中计算与设计广义纳什均衡（GNEs），通过利用所有玩家的联合 KKT 条件来处理一般非线性 GNE 和线性二次（LQ）GNE，包括变分形式。对非线性博弈，采用非线性最小二乘法并利用 JAX 进行自动微分；对线性-二次 GNE，将其重新表述为混合整数线性规划（MILP），可高效求解多重均衡。框架还支持逆博弈与 Stackelberg 博弈设计问题。通过若干示例（包括线性二次调控和模型预测控制的非合作博弈控制问题）来演示其能力。库地址：https://github.com/bemporad/nashopt


<details>
  <summary>Details</summary>
Motivation: 需要在具有共享约束的非合作博弈中有效地计算和设计广义纳什均衡，同时覆盖一般非线性与线性-二次（LQ）情形，并具备求解多重均衡、逆博弈以及 Stackelberg 博弈设计的能力。

Method: 基于对所有玩家的联合 KKT 条件，将 GNE 问题转化为可求解的优化问题。对非线性博弈，采用非线性最小二乘化表达并借助 JAX 的自动微分实现；对线性-二次博弈，重写为混合整数线性规划以便高效求解多均衡；框架还支持逆博弈与 Stackelberg 设计，并通过若干控制问题示例进行展示。

Result: 通过若干示例（包括线性二次规制与模型预测控制中的非合作博弈控制问题）演示了该工具的可行性和有效性，以及在多种场景中求解 GNE 的能力。

Conclusion: NashOpt 提供了一个统一、可扩展的工具，用于在存在共享约束的非线性与线性-二次博弈中计算与设计广义纳什均衡，支持多重均衡、逆博弈与 Stackelberg 博弈设计，并且是开源的，便于在相关控制与博弈论问题中应用。

Abstract: NashOpt is an open-source Python library for computing and designing generalized Nash equilibria (GNEs) in noncooperative games with shared constraints and real-valued decision variables. The library exploits the joint Karush-Kuhn-Tucker (KKT) conditions of all players to handle both general nonlinear GNEs and linear-quadratic games, including their variational versions. Nonlinear games are solved via nonlinear least-squares formulations, relying on JAX for automatic differentiation. Linear-quadratic GNEs are reformulated as mixed-integer linear programs, enabling efficient computation of multiple equilibria. The framework also supports inverse-game and Stackelberg game-design problems. The capabilities of NashOpt are demonstrated through several examples, including noncooperative game-theoretic control problems of linear quadratic regulation and model predictive control. The library is available at https://github.com/bemporad/nashopt

</details>
