<div id=toc></div>

# Table of Contents

- [eess.SY](#eess.SY) [Total: 15]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.LG](#cs.LG) [Total: 52]
- [eess.SP](#eess.SP) [Total: 9]


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [1] [Exploring a New Design Paradigm for Omnidirectional MAVs for Minimal Actuation and Internal Force Elimination: Theoretical Framework and Control](https://arxiv.org/abs/2510.15071)
*Ahmed Ali,Chiara Gabellieri,Antonio Franchi*

Main category: eess.SY

TL;DR: 提出一种仅用6个输入实现全向性（omnidirrectionality）的多旋翼飞行器，结合一个主动倾转螺旋桨和3个带螺旋桨的摆杆，通过被动万向关节连接，确保在平衡时无内部力且无过度执行。


<details>
  <summary>Details</summary>
Motivation: 解决多旋翼在有限输入下实现全向性与内部力最小化的问题，提供一个结构简单且不需要额外输入的方案，同时实现跨式的姿态与平移解耦控制。

Method: 建立多连杆MAV的动态模型，分析平衡构型并证明对主平台任意姿态存在强迫平衡；采用几何非线性控制，结合动态反线性化与回扫控制，误差使用SE(3)左平凡化描述；通过Lyapunov零动力学稳定性分析；并给出数值仿真验证，显示在初始条件、参数不确定性与执行器噪声存在时的解耦运动。

Result: 证明在主平面任意姿态下存在强迫平衡，闭环系统在构造的控制下局部渐近稳定；数值仿真验证了在非零初始、参数不确定性和执行器噪声下的姿态与平移解耦能力。

Conclusion: 所提出的6输入全向MAV及其控制框架可实现全向姿态与平移的解耦控制，并通过几何非线性控制与Lyapunov分析得到理论支持，仿真结果支持其潜在可行性。

Abstract: This paper presents a novel concept for achieving omnidirectionality in a
multirotor aerial vehicle (MAV) that uses only 6 inputs and ensures no internal
forces at the equilibria. The concept integrates a single actively-tilting
propeller along with 3 pendulum-like links, each carrying a propeller,
connected by passive universal joints to the main body. We show that this
design ensures omnidirectionality while minimizing the internal forces and
without resorting to overactuation (i.e., more than 6 inputs). A detailed
dynamic model of the multi-link MAV is first developed. Afterwards, the
analysis identifies the equilibrium configurations and illustrates that a
forced equilibrium exists for every pose of the MAV's main platform. In order
to render this equilibrium asymptotically stable for the closed-loop system, a
geometric nonlinear controller is constructed using dynamic feedback
linearization and backstepping techniques with the main platform configuration
error being the left-trivialized error on SE(3). The stability of the
closed-loop system is then investigated by employing standard Lyapunov
arguments on the zero dynamics. We conclude by providing numerical simulations
validating the proposed approach. They demonstrate the MAV capability to
perform decoupled attitude and translational motions under non-zero initial
conditions, parametric uncertainty, and actuators noise.

</details>


### [2] [Sparsity-exploiting Gaussian Process for Robust Transient Learning of Power System Dynamics](https://arxiv.org/abs/2510.15150)
*Tina Gao,Shimiao Li,Lawrence Pileggi*

Main category: eess.SY

TL;DR: 在PMU数据中对稀疏污染进行鲁棒瞬态学习，提出将MoM与稀疏优化结合以识别污染点，并通过K-medoid聚类实现DR与AR，从而在大规模系统上显著提升推断速度并对多种攻击具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决真实测量中的随机和定向污染对动态网格行为学习的影响，提升大规模电网推断的鲁棒性和效率。

Method: 将高斯过程用于动态建模；引入稀疏优化结合方法的MoM来对稀疏污染进行鲁棒学习；通过优化稀疏权重定位污染计量点；引入K-medoid聚类进行位置的DR和AR启发式；

Result: 在1354-总线系统上，DR使推断速度提升18倍，结合AR后速度提升至400倍；对随机大错误、定向假数据注入、局部PMU时钟漂移具有鲁棒性。

Conclusion: 该框架在鲁棒性与规模化推断方面具有显著优势，适合大规模电网的动态学习。

Abstract: Advances in leveraging Gaussian processes (GP) have enabled learning and
inferring dynamic grid behavior from scarce PMU measurements. However, real
measurements can be corrupted by various random and targeted threats, leading
to inaccurate and meaningless results. This paper develops robust transient
learning to overcome this challenge by exploiting the sparse corruption
patterns in the data flow. Specifically, we integrate sparse optimization with
method of moments (MoM) to make learning robust to a sparse distribution of
data corruptions; then, we optimize sparse weights to identify corrupted meter
locations. To improve inference speed on large-scale systems, we further adopt
K-medoid clustering of locations to develop dimension reduction (DR) and
aggregate representation (AR) heuristics. Experimental results demonstrate
robustness against random large errors, targeted false data injections, and
local PMU clock drifts. On a 1354-bus system, inference turns out to be 18x
faster using DR and 400x faster when further combined with AR heuristics.

</details>


### [3] [Tail-Optimized Caching for LLM Inference](https://arxiv.org/abs/2510.15152)
*Wenxin Zhang,Yueying Li,Ciamac C. Moallemi,Tianyi Peng*

Main category: eess.SY

TL;DR: Tail-Optimized LRU: 在LRU的基础上两行改动，通过重新分配KV缓存容量来优先保留高延迟会话，理论上在自然的对话动态模型下达到最优，实验上显著降低尾延迟并减少SLO违规。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理中的尾部延迟问题，尽管广泛采用缓存，LRU对会话长度异质性不敏感，导致尾部指标表现不佳；需要一种简单、理论可证且在实际数据上有效的缓存策略。

Method: 提出Tail-Optimized LRU（两行代码修改），用于对KV缓存进行容量再分配；建立自然的对话动态模型并给出最优性证明；在真实数据集WildChat上实现并评估。

Result: 理论性结果：在模型假设下对LRU的最优性给出证明；经验性结果：在WildChat数据上，P90尾延迟下降最高27.5%，P95尾延迟下降最高23.9%，SLO 200ms违规减少最高38.9%。

Conclusion: 为实际的LLM部署提供一个既简单又有理论支撑的尾部延迟优化选项，亦可能对缓存领域产生独立价值。

Abstract: Prompt caching is critical for reducing latency and cost in LLM inference:
OpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.
Despite its widespread success, little is known about what constitutes an
optimal prompt caching policy, particularly when optimizing tail latency, a
metric of central importance to practitioners. The widely used Least Recently
Used (LRU) policy can perform arbitrarily poor on this metric, as it is
oblivious to the heterogeneity of conversation lengths. To address this gap, we
propose Tail-Optimized LRU, a simple two-line modification that reallocates KV
cache capacity to prioritize high-latency conversations by evicting cache
entries that are unlikely to affect future turns. Though the implementation is
simple, we prove its optimality under a natural stochastic model of
conversation dynamics, providing the first theoretical justification for LRU in
this setting, a result that may be of independent interest to the caching
community. Experimentally, on real conversation data WildChat, Tail-Optimized
LRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and
23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in
SLO violations of 200ms. We believe this provides a practical and theoretically
grounded option for practitioners seeking to optimize tail latency in
real-world LLM deployments.

</details>


### [4] [A Comparative Study of Oscillatory Perturbations in Car-Following Models](https://arxiv.org/abs/2510.15190)
*Oumaima Barhoumi,Ghazal Farhani,Taufiq Rahman,Mohamed H. Zaki,Sofiène Tahar*

Main category: eess.SY

TL;DR: 对比分析 IDM、OVM、GMM、CACC 在扰动下的车队稳定性，评估不同模型对领导车速扰动的鲁棒性及对后车间距的影响。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和车联网背景下，车队编排的稳定性决定着路网容量、能耗和安全性。本研究通过对比多种车-follow 模型，揭示稳定性根源与扰动传播机制。

Method: 在领导车速度中引入扰动（正弦波与离散跳变），对 IDM、OVM、GMM、CACC 的跟车行为进行仿真分析，比较车轨迹和车辆间距随时间的变化以评估鲁棒性。

Result: 不同模型表现出对扰动的不同敏感性和稳定性特征，扰动传播速率及后车响应存在模型间差异；研究提供关于模型鲁棒性及对抗性策略设计的见解。

Conclusion: 本研究为多模型对比评估车队稳定性提供系统框架，帮助设计更鲁棒的编队控制策略以应对扰动与不确定性。

Abstract: As connected and autonomous vehicles become more widespread, platooning has
emerged as a key strategy to improve road capacity, reduce fuel consumption,
and enhance traffic flow. However, the benefits of platoons strongly depend on
their ability to maintain stability. Instability can lead to unsafe spacing and
increased energy usage. In this work, we study platoon instability and analyze
the root cause of its occurrence, as well as its impacts on the following
vehicle. To achieve this, we propose a comparative study between different
car-following models such as the Intelligent Driver Model (IDM), the Optimal
Velocity Model (OVM), the General Motors Model (GMM), and the Cooperative
Adaptive Cruise Control (CACC). In our approach, we introduce a disruption in
the model by varying the velocity of the leading vehicle to visualize the
behavior of the following vehicles. To evaluate the dynamic response of each
model, we introduce controlled perturbations in the velocity of the leading
vehicle, specifically, sinusoidal oscillations and discrete velocity changes.
The resulting vehicle trajectories and variations in inter-vehicle spacing are
analyzed to assess the robustness of each model to disturbance propagation. The
findings offer insight into model sensitivity, stability characteristics, and
implications for designing resilient platooning control strategies.

</details>


### [5] [Quantum-Key-Distribution Authenticated Aggregation and Settlement for Virtual Power Plants](https://arxiv.org/abs/2510.15239)
*Ziqing Zhu*

Main category: eess.SY

TL;DR: 提出一个基于量子密钥分发(QKD)的VPP（虚拟电厂）量化认证聚合与结算框架，通过密钥预算来最小化风险并满足时延约束，结合威胁建模、风险最优化和影子价格分析，在案例研究中有效降低残留风险与SLA违规。


<details>
  <summary>Details</summary>
Motivation: 在分布式能源资源和需求侧灵活性背景下，VPP的端到端业务链需要安全且及时的通信。传统密码学难以长期提供充足保护，QKD作为信息论层面的安全方案虽具潜力，但密钥速率、路由容量与系统开销限制了其实际部署，密钥分配成为一个关键挑战。

Method: 建立一个系统威胁模型，将QKD的密钥生成与路由与业务层安全策略、认证强度、刷新频率及延迟约束相连接；提出一个密钥预算的风险最小化问题，综合考虑经济风险、服务水平违约与密钥可行性，揭示边际安全价值与影子价格之间的阈值关系；通过对代表性VPP的案例研究验证。

Result: 框架显著降低残留风险和SLA违规，提升密钥利用效率与鲁棒性，观测到的动态与理论上的影子价格机制相一致。

Conclusion: 所提出的方法将稀缺的量子密钥在异质流程中进行调度，与VPP的端到端安全需求对齐，提供一个可操作的、基于密钥预算的安全性优化框架，案例研究验证了其有效性。

Abstract: The proliferation of distributed energy resources (DERs) and demand-side
flexibility has made virtual power plants (VPPs) central to modern grid
operation. Yet their end-to-end business pipeline, covering bidding, dispatch,
metering, settlement, and archival, forms a tightly coupled
cyber-physical-economic system where secure and timely communication is
critical. Under the combined stress of sophisticated cyberattacks and extreme
weather shocks, conventional cryptography offers limited long-term protection.
Quantum key distribution (QKD), with information-theoretic guarantees, is
viewed as a gold standard for securing critical infrastructures. However,
limited key generation rates, routing capacity, and system overhead render key
allocation a pressing challenge: scarce quantum keys must be scheduled across
heterogeneous processes to minimize residual risk while maintaining latency
guarantees. This paper introduces a quantum-authenticated aggregation and
settlement framework for VPPs. We first develop a system-threat model that
connects QKD key generation and routing with business-layer security
strategies, authentication strength, refresh frequency, and delay constraints.
Building on this, we formulate a key-budgeted risk minimization problem that
jointly accounts for economic risk, service-level violations, and key-budget
feasibility, and reveal a threshold property linking marginal security value to
shadow prices. Case studies on a representative VPP system demonstrate that the
proposed approach significantly reduces residual risk and SLA violations,
enhances key efficiency and robustness, and aligns observed dynamics with the
theoretical shadow price mechanism.

</details>


### [6] [Comprehensive Dynamic Modeling and Constraint-Aware Air Supply Control for Localized Water Management in Automotive Polymer Electrolyte Membrane Fuel Cells](https://arxiv.org/abs/2510.15250)
*Mostafaali Ayubirad,Zeng Qiu,Hao Wang,Chris Weinkauf,Michiel Van Nieuwstadt,Hamid R. Ossareh*

Main category: eess.SY

TL;DR: 提出一种基于 Command Governor 的预测约束控制，用于 PEM 燃料电池系统的局部膜水化管理。通过耦合的完整非线性模型与降阶线性化模型，在 CG 框架中动态调整进气以防止膜干燥并实现对净功率的接近跟踪，且通过真实工况仿真验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决 PEM 燃料电池中膜水化在多子系统耦合与物理-约束条件下的维持问题，避免膜干燥，同时实现对所需净功率的跟踪。

Method: 建立包含堆的伪二维模型（P2D）及供气与冷却子系统的完整非线性模型，分析膜水化在阳极入口附近的动力学；推导出用于约束执行的降阶线性化模型；在 CG 框架中通过调整空气供给设定值来防止膜干燥并实现功率跟踪；使用现实驱动循环进行仿真以验证方法。

Result: 在保持局部膜水化的前提下，能够较好地跟踪所请求的净功率，仿真结果表明所提方法有效地实现了水化约束与功率目标之间的折中。

Conclusion: 所提出的预测约束控制策略在 PEM 燃料电池的局部水化管理方面具有可行性与有效性，可用于在保证膜干燥安全的前提下实现对功率的跟踪。

Abstract: In this paper, a predictive constraint-aware control scheme is formulated
within the Command Governor (CG) framework for localized hydration management
of a proton exchange membrane (PEM) fuel cell system. First, a comprehensive
nonlinear dynamic model of the fuel cell system is presented which includes a
pseudo 2-dimensional (P2D) model of the stack, reactant supply and cooling
subsystems. The model captures the couplings among the various subsystems and
serves as the basis for designing output feedback controllers to track the
optimal set-points of the air supply and cooling systems for power
optimization. The closed-loop nonlinear model is then used to analyze the
dynamic behavior of membrane hydration near the anode inlet, the driest region
of the membrane in a counter-flow configuration, under various operating
conditions. A reduced-order linearized model is then derived to approximate
hydration behavior with sufficient fidelity for constraint enforcement. This
model is used within the CG framework to adjust the air supply set-points when
necessary to prevent membrane dry-out. The effectiveness of the proposed
approach in maintaining local membrane hydration while closely tracking the
requested net power is demonstrated through realistic drive-cycle simulations.

</details>


### [7] [Modeling and Dynamic Simulation of a Hybrid Wind-Wave System on a Hexagonal Semi-Submersible Platform](https://arxiv.org/abs/2510.15285)
*Saeid Bayat,Jerry Zuo,Jing Sun*

Main category: eess.SY

TL;DR: 将风能与波浪能综合在一个六边形半潜浮式平台的混合系统，通过翼板（flaps）及3台波浪能量转换器（WEC）实现能量提取与稳定性提升，经过WEC-Sim建模与与NREL 5 MW半潜参考模型对比验证，显示风能贡献较大但波浪能提供显著贡献，且具备稳定性与控制优化潜力。


<details>
  <summary>Details</summary>
Motivation: 解决当前海上可再生能源系统往往只开发风能或波能的单一模式所带来的能量波动与可靠性不足问题，提出一个集成风波混合平台，以提升能量产出、稳定性和可预测性，并探讨结构与控制设计要点。

Method: 基于WEC-Sim建立一个六边形半潜浮式组合平台模型，将风力涡轮与三台 Oscillating Surge WEC 集成至翼板并耦合到平台结构，进行水动力与静力学分析；用NREL 5 MW半潜参考模型进行基准校核；进行水静力稳定性（metacentric height）分析、 twelve Geometric variable 灵敏度分析，时域仿真研究入射角对翼板功率分配、CWR、平台响应的影响，并验证翼板扫掠对姿态的调控可行性；基于现场数据估算年发电量（AEP）。

Result: 在不同 flap 角度下实现水静稳定性； flap 尺寸与塔长为稳定性、能量捕获与塔应力的主导因素；时域仿真显示波浪入射角影响 flap 功率分配、CWR 与平台响应，且翼板扫掠可用来调控俯仰运动；基于场点数据的年发电量估计为风能 16.86 GWh、波浪能 3.65 GWh，总发电量中波浪贡献约18%；WEC 在总发电量中的贡献相对较小但具有显著补充作用。

Conclusion: 该研究证实了集成风-波混合平台在海上能量获取与结构稳定性方面的潜力，未来可在结构建模的精细化和先进控制策略方面开展深入研究，以提升整体系统的性能与可靠性。

Abstract: Offshore renewable energy systems offer promising solutions for sustainable
power generation, yet most existing platforms harvest either wind or wave
energy in isolation. This study presents a hybrid floating offshore platform
that integrates a wind turbine with three oscillating surge wave energy
converters (WECs) into a hexagonal semi-submersible structure. In this
configuration, the flaps are integrated with the platform geometry to provide
both energy extraction and hydrodynamic stability. A modeling and simulation
framework was developed using WEC-Sim and benchmarked against the NREL 5 MW
semisubmersible reference. Metacentric height analysis confirmed hydrostatic
stability across a range of prescribed flap angles. Sensitivity analysis of
twelve geometric variables identified flap dimensions and tower length as
dominant drivers of stability, energy capture, and tower stress. Time-domain
simulations revealed dependence on wave incidence angle, with variations in
flap power sharing, capture width ratio (CWR), and platform response. The
feasibility of using flap sweeps to modulate pitch motion was also
demonstrated. Annual energy production (AEP) estimates based on site-specific
data indicate 16.86 GWh from wind and 3.65 GWh from wave energy, with WECs
contributing about 18% of the total. These results highlight the potential of
integrated wind-wave platforms and point toward future studies on structural
modeling and advanced control.

</details>


### [8] [TranSimHub:A Unified Air-Ground Simulation Platform for Multi-Modal Perception and Decision-Making](https://arxiv.org/abs/2510.15365)
*Maonan Wang,Yirong Chen,Yuxin Cai,Aoyu Pang,Yuejiao Xie,Zian Ma,Chengcheng Xu,Kemou Jiang,Ding Wang,Laurent Roullet,Chung Shue Chen,Zhiyong Cui,Yuheng Kan,Michael Lepech,Man-On Pun*

Main category: eess.SY

TL;DR: 提出 TranSimHub，一体化的空地协同智能仿真平台，涵盖多模态感知、跨域信息交换与因果场景编辑，支持端到端的感知-融合-控制研究，开源发布。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的多模态仿真环境，限制跨域感知、通信约束下的协同与联合决策优化的研究。

Method: 实现同步的 RGB/深度/语义多模态渲染；实现空域/地面信息交换；提供可控场景编辑器用于在不同天气、紧急事件、动态障碍物等条件下的因果分析。

Result: 提供一个用于真实场景空地交通研究的端到端感知、融合、控制研究平台，且开源。

Conclusion: 该平台有望推动空地协同智能的系统性研究，加速感知-融合-控制的端到端开发与评估。

Abstract: Air-ground collaborative intelligence is becoming a key approach for
next-generation urban intelligent transportation management, where aerial and
ground systems work together on perception, communication, and decision-making.
However, the lack of a unified multi-modal simulation environment has limited
progress in studying cross-domain perception, coordination under communication
constraints, and joint decision optimization. To address this gap, we present
TranSimHub, a unified simulation platform for air-ground collaborative
intelligence. TranSimHub offers synchronized multi-view rendering across RGB,
depth, and semantic segmentation modalities, ensuring consistent perception
between aerial and ground viewpoints. It also supports information exchange
between the two domains and includes a causal scene editor that enables
controllable scenario creation and counterfactual analysis under diverse
conditions such as different weather, emergency events, and dynamic obstacles.
We release TranSimHub as an open-source platform that supports end-to-end
research on perception, fusion, and control across realistic air and ground
traffic scenes. Our code is available at
https://github.com/Traffic-Alpha/TranSimHub.

</details>


### [9] [A Tsetlin Machine Image Classification Accelerator on a Flexible Substrate](https://arxiv.org/abs/2510.15519)
*Yushu Qin,Marcos L. L. Sartori,Shengyu Duan,Emre Ozer,Rishad Shafik,Alex Yakovlev*

Main category: eess.SY

TL;DR: 首个在柔性集成电路 FlexIC 上实现数字 Tsetlin Machines（TM）的工作，基于 Pragmatic 的 600nm IGZO FlexIC 技术，提供两种硬件 TM 推断模型并显示在 8x8 手写数字识别数据集上的高准确率和小面积权衡，证明其在可穿戴与边缘计算中的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决传统硅芯片的刚性、能效与可解释性不足等问题；通过柔性 FlexIC 提升边缘设备上 TM 的可部署性与能效表现。

Method: 在 Pragmatic 的 600nm IGZO 基于 FlexIC 的平台上，开发两种 TM 推断模型的数字化实现：第一种使用约 6800 个 NAND2 等价门，面积 8x8 mm2，准确度 98.5%；第二种更紧凑，使用约 1420 个 NAND2 等价门，面积 4x4 mm2，准确度 93%。两者均为面向 8x8 像素的手写数字识别数据集定制设计。

Result: 两种 FlexIC 实现的 TM 推断引擎分别达到 98.5% 和 93% 的预测准确率，面积分别为 8x8 mm2 和 4x4 mm2。

Conclusion: 结果显示柔性 TM 推断引擎在可穿戴健康与边缘计算场景中具备落地潜力，证明在柔性硬件上实现高效、可解释的 TM 推断是可行的。

Abstract: This paper introduces the first implementation of digital Tsetlin Machines
(TMs) on flexible integrated circuit (FlexIC) using Pragmatic's 600nm
IGZO-based FlexIC technology. TMs, known for their energy efficiency,
interpretability, and suitability for edge computing, have previously been
limited by the rigidity of conventional silicon-based chips. We develop two TM
inference models as FlexICs: one achieving 98.5% accuracy using 6800 NAND2
equivalent logic gates with an area of 8X8 mm2, and a second more compact
version achieving slightly lower prediction accuracy of 93% but using only 1420
NAND2 equivalent gates with an area of 4X4 mm2, both of which are
custom-designed for an 8X8-pixel handwritten digit recognition dataset. The
paper demonstrates the feasibility of deploying flexible TM inference engines
into wearable healthcare and edge computing applications.

</details>


### [10] [Hypergame-based Cognition Modeling and Intention Interpretation for Human-Driven Vehicles in Connected Mixed Traffic](https://arxiv.org/abs/2510.15573)
*Jianguo Chen,Zhengqin Liu,Jinlong Lei,Peng Yi,Yiguang Hong,Hong Chen*

Main category: eess.SY

TL;DR: A hypergame-based framework that models human drivers' bounded rationality and perception limits, enabling distributed HV intention learning, and online/offline CAV trajectory prediction and planning with V2X. Validates cognitive equilibrium (hyper Nash) and safety via highway lane-change simulations.


<details>
  <summary>Details</summary>
Motivation: In mixed traffic with CAVs and HVs, human drivers have limited cognition and imperfect knowledge of other vehicles' goals. Existing models assuming perfect information are unrealistic. The work aims to incorporate cognitive constraints to improve HV trajectory prediction and CAV planning for safety and efficiency.

Method: 1) Propose a hierarchical cognition model for vehicles and use hypergame theory to capture cognitive relationships and bounded rationality. 2) Develop an inverse learning algorithm for distributed intention interpretation via V2X to learn HV parameters offline and online. 3) Propose a distributed trajectory prediction/planning approach for CAVs leveraging learned parameters in real time.

Result: Simulations on highway lane-changing demonstrate accurate parameter learning, robustness to noisy observations, and safe HV trajectory prediction. The framework supports offline and online implementations and shows that the strategy profile where all vehicles adopt cognitively equilibrium strategies constitutes a hyper Nash equilibrium when CAVs accurately learn HV parameters.

Conclusion: Accounting for HV cognitive limits yields a robust, distributed framework for HV intention interpretation and real-time CAV planning. The approach achieves cognitive stability (hyper Nash equilibrium) and safe, efficient operations in mixed traffic, with demonstrated offline/online applicability.

Abstract: With the practical implementation of connected and autonomous vehicles
(CAVs), the traffic system is expected to remain a mix of CAVs and human-driven
vehicles (HVs) for the foreseeable future. To enhance safety and traffic
efficiency, the trajectory planning strategies of CAVs must account for the
influence of HVs, necessitating accurate HV trajectory prediction. Current
research often assumes that human drivers have perfect knowledge of all
vehicles' objectives, an unrealistic premise. This paper bridges the gap by
leveraging hypergame theory to account for cognitive and perception limitations
in HVs. We model human bounded rationality without assuming them to be merely
passive followers and propose a hierarchical cognition modeling framework that
captures cognitive relationships among vehicles. We further analyze the
cognitive stability of the system, proving that the strategy profile where all
vehicles adopt cognitively equilibrium strategies constitutes a hyper Nash
equilibrium when CAVs accurately learn HV parameters. To achieve this, we
develop an inverse learning algorithm for distributed intention interpretation
via vehicle-to-everything (V2X) communication, which extends the framework to
both offline and online scenarios. Additionally, we introduce a distributed
trajectory prediction and planning approach for CAVs, leveraging the learned
parameters in real time. Simulations in highway lane-changing scenarios
demonstrate the proposed method's accuracy in parameter learning, robustness to
noisy trajectory observations, and safety in HV trajectory prediction. The
results validate the effectiveness of our method in both offline and online
implementations.

</details>


### [11] [Cross-border offshore hydrogen trade and carbon mitigation for Europe's net zero transition](https://arxiv.org/abs/2510.15695)
*Sheng Wang,Muhammad Maladoh Bah*

Main category: eess.SY

TL;DR: 研究提出自下而上的方法评估爱尔兰与英国海上氢气在欧洲减碳中的作用，预测2030-2050年氢气贸易格局及对CO2减排的贡献，显示英国长期领先，2050年爱尔兰超越，总体年减排约175 MtCO2，氢气流向来自西向东。


<details>
  <summary>Details</summary>
Motivation: 欧洲在实现净零排放与开发海上能源资源方面设定雄心；海上绿氢出口潜力常被低估，需要量化跨境贸易、生产成本、国内能源系统和海运成本对减排的影响。

Method: 采用底向上建模，结合海上氢气生产成本、英国与爱尔兰的农村/城市电力与天然气系统运行特征、以及国际海运成本，评估两国的出口能力、贸易路径及对欧洲减排的贡献。

Result: 海上绿氢可为欧洲每年减少约175.16 Mt CO2；2030-2040英国为主要供应国，2050年由爱尔兰超越，向法国与西班牙出口约161 TWh；总体氢流呈西向东，有利于欧洲净零与能源安全。

Conclusion: 海上绿氢贸易有助于实现欧洲的净零目标、重塑能源供给结构、提升区域能源安全，创建跨境能源协同的新范式。

Abstract: European countries are ambitious in both the net-zero transition and offshore
energy resource development. The Irish and UK governments announced their
commitments to offshore wind capacities - 37 and 125 GW, respectively, in 2050,
more than two times higher than their projected power demands. While other
continental countries, such as Germany, are calling for cleaner fuel resources.
Exporting surplus offshore green hydrogen and bridging supply and demand could
be pivotal in carbon emission mitigation for Europe. Yet, the potentials of
these Island countries, are usually underestimated. This paper developed a
bottom-up method to investigate the role of offshore hydrogen from Ireland and
the UK in the decarbonisation of the entire Europe. We evaluate the future
hydrogen/ammonia trading and the contributions of each country in carbon
emission mitigation, considering their relative cost-competitiveness in
offshore hydrogen production, domestic hourly power and gas system operation,
and international shipping costs. Results indicate that the offshore green
hydrogen could reduce 175.16 Mt/year of carbon dioxide emissions in Europe. The
UK will be the largest hydrogen supplier from 2030 to 2040, while surpassed by
Ireland in 2050, with 161 TWh of hydrogen exports to France and Spain. The
offshore green hydrogen can contribute to 175.16 Mt of annual carbon dioxide
emission reductions in total. This general flow of hydrogen from the West to
the East not only facilitates Europe's net-zero progress, but also reshapes the
energy supply structure and helps to ensure energy security across the European
continent.

</details>


### [12] [Mitigating Underwater Noise from Offshore Wind Turbines via Individual Pitch Control](https://arxiv.org/abs/2510.15707)
*Martín de Frutos,Laura Botero-Bolívar,Esteban Ferrer*

Main category: eess.SY

TL;DR: 提出一种水下声环境噪声抑制策略：通过开环逐叶片角变位以降低作动噪声的OSPL与AM，且以NREL 5 MW、DTU 10 MW、IEA 22 MW三台参考机组为基准，实现最高约5 dB的OSPL下降、AM深度下降约20%，能量捕获损失5-10%。


<details>
  <summary>Details</summary>
Motivation: 海上风电机组的水下声环境对海洋生物的通信、导航和生存具有重要影响，亟需低噪声运行策略；本文系统量化水下声学特征并提出噪声抑制方法。

Method: 采用耦合集成叶片动量理论(BEM)与水-空气耦合声传播建模量化水下噪声；提出并实现开环单叶片角控制IPC策略，使叶片通过叶片过流频率(BPF)调制角度以衰减OSPL和传输噪声的AM；以NREL 5 MW、DTU 10 MW、IEA 22 MW为基准进行对比评估。

Result: 在Δθ≈5°的角度振动下，OSPL降低多达约5 dB，AM深度降低约20%，能量捕获损失约5-10%；显现出新的噪声路径并证明了定向叶片角调制可缓解其影响。

Conclusion: 证实可通过目标化的叶片角度调制来减小水下噪声对海洋生物的影响，并提供实现路径和潜在的能量-噪声权衡。

Abstract: This paper proposes a pitch control strategy to mitigate the underwater
acoustic footprint of offshore wind turbines, a measure that will soon become
necessary to minimize impacts on marine life, which rely on sound for
communication, navigation, and survival. First, we quantify the underwater
acoustic signature of blade-generated aerodynamic noise from three reference
turbines, the NREL 5 MW, DTU 10 MW, and IEA 22 MW, using coupling blade element
momentum and coupled air-water acoustic propagation modeling. Second, we
propose and implement an open-loop individual pitch control (IPC) strategy that
modulates the pitch of the blade at the blade passing frequency to attenuate
the overall sound pressure level (OSPL) and the amplitude modulation (AM) of
the transmitted noise. Third, we benchmark IPC performance against conventional
pitch schemes. The results indicate that up to 5 dB reductions in OSPL and a
decrease in AM depth 20% can be achieved with a pitch variation of
$\Delta\theta\approx 5^\circ$, with small losses (5-10%) in energy capture.
These findings highlight a previously underappreciated noise pathway and
demonstrate that targeted blade-pitch modulation can mitigate its impact.

</details>


### [13] [Integrating Conductor Health into Dynamic Line Rating and Unit Commitment under Uncertainty](https://arxiv.org/abs/2510.15740)
*Geon Roh,Jip Kim*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dynamic line rating (DLR) enables greater utilization of existing
transmission lines by leveraging real-time weather data. However, the elevated
temperature operation (ETO) of conductors under DLR is often overlooked,
despite its long-term impact on conductor health. This paper addresses this
issue by 1) quantifying depreciation costs associated with ETO and 2) proposing
a Conductor Health-Aware Unit Commitment (CHA-UC) that internalizes these costs
in operational decisions. The CHA-UC incorporates a robust linear approximation
of conductor temperature and integration of expected depreciation costs due to
hourly ETO into the objective function. Case studies on the Texas 123-bus
backbone test system using NOAA weather data demonstrate that the proposed
CHA-UC model reduces the total cost by 0.8% and renewable curtailment by
84%compared to static line rating (SLR), while conventional DLR operation
without risk consideration resulted in higher costs due to excessive ETO.
Further analysis of the commitment decisions and the line temperature
statistics confirms that the CHA-UC achieves safer line flows by shifting
generator commitments. Finally, we examine the emergent correlation between
wind generation and DLR forecast errors, and show that CHA-UC adaptively
manages this effect by relaxing flows for risk-hedging conditions while
tightening flows for risk-amplifying ones.

</details>


### [14] [Braking within Barriers: Constructive Safety-Critical Control for Input-Constrained Vehicles via the Backup Set Method](https://arxiv.org/abs/2510.15797)
*Laszlo Gacsi,Adam K. Kiss,Tamas G. Molnar*

Main category: eess.SY

TL;DR: 提出一种安全关键的制动控制框架，通过备份控制屏障函数在不对称路面制动时保持横向有界，同时尽量缩短停止距离。


<details>
  <summary>Details</summary>
Motivation: 在路面摩擦不对称、输入受限的情况下，确保车辆在紧急制动时不发生甩尾/打滑，同时尽量缩短停止距离，提升行车安全。

Method: 结合备份控制屏障函数（CBCF）进行安全设计；提出基于反馈线性化和连续时间Lyapunov方程的有效备份集与备份控制器对的构建方法；在四轮车辆模型与不对称路面上通过仿真验证。

Result: 通过简单示例与四轮车辆模型的制动仿真，验证了方法在输入约束下维持横向有界性并实现较短停止距离，且能防止车辆产生失控的横向运动。

Conclusion: 该框架提供了一种系统化的备份集–备份控制器对构建方法，适用于不对称路面和输入约束条件下的安全关键制动控制设计与实现。

Abstract: This paper presents a safety-critical control framework to maintain bounded
lateral motions for vehicles braking on asymmetric surfaces. We synthesize a
brake controller that assists drivers and guarantees safety against excessive
lateral motions (i.e., prevents the vehicle from spinning out) while minimizing
the stopping distance. We address this safety-critical control problem in the
presence of input constraints, since braking forces are limited by the
available friction on the road. We use backup control barrier functions for
safe control design. As this approach requires the construction of a backup set
and a backup controller, we propose a novel, systematic method to creating
valid backup set-backup controller pairs based on feedback linearization and
continuous-time Lyapunov equations. We use simple examples to demonstrate our
proposed safety-critical control method. Finally, we implement our approach on
a four-wheel vehicle model for braking on asymmetric surfaces and present
simulation results.

</details>


### [15] [Bio-inspired Microgrid Management based on Brain's Sensorimotor Gating](https://arxiv.org/abs/2510.15847)
*Panos C. Papageorgiou,Anastasios E. Giannopoulos,Sotirios T. Spantideas*

Main category: eess.SY

TL;DR: 提出 Sensorimotor Gating-Inspired Neuro-Microgrid (SG-NMG) 框架，将 PPI/PPF 的抑制/放大机制映射到微网分层控制，强调自保护、适应性与鲁棒性，给出分析工作流、神经电路类比与 ML 集成的理论框架及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 解决微网在动态扰动处理、保护协调与不确定性方面的挑战，借鉴脑中 BEL 控制的生物启发，扩展为以感官运动门控为基础的 Neuro-Microgrid。

Method: 构建以 PPI/PPF 为核心的控制决策映射，设计 SG-NMG 架构、提供神经电路类比、分析工作流设计，并与机器学习方法集成。

Result: 提出理论框架与分析思路，尚无实验结果，聚焦概念性贡献与未来验证路径。

Conclusion: 感官运动门控为微网的自保护、适应性与鲁棒性提供有前景的框架，未来需开展门控的数学建模、数字孪生验证以及跨学科协作。

Abstract: Microgrids are emerging as key enablers of resilient, sustainable, and
intelligent power systems, but they continue to face challenges in dynamic
disturbance handling, protection coordination, and uncertainty. Recent efforts
have explored Brain Emotional Learning (BEL) controllers as bio-inspired
solutions for microgrid control. Building on this growing trajectory, this
article introduces a new paradigm for Neuro-Microgrids, inspired by the brain's
sensorimotor gating mechanisms, specifically the Prepulse Inhibition (PPI) and
Prepulse Facilitation (PPF). Sensorimotor gating offers a biological model for
selectively suppressing or amplifying responses depending on contextual
relevance. By mapping these principles onto the hierarchical control
architecture of microgrids, we propose a Sensorimotor Gating-Inspired
Neuro-Microgrid (SG-NMG) framework. In this architecture, PPI-like control
decisions correspond to protective damping in primary and secondary management
of microgrids, whereas PPF-like decisions correspond to adaptive amplification
of corrective control actions. The framework is presented through analytical
workflow design, neuro-circuitry analogies, and integration with machine
learning methods. Finally, open challenges and research directions are
outlined, including the mathematical modeling of gating, digital twin
validation, and cross-disciplinary collaboration between neuroscience and
industrial power systems. The resulting paradigm highlights sensorimotor gating
as a promising framework for designing self-protective, adaptive, and resilient
microgrids.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [Content and Access Networks Synergies: Tradeoffs in Public and Private Investments by Content Providers](https://arxiv.org/abs/2510.15373)
*Pranay Agarwal,D. Manjunath*

Main category: cs.NI

TL;DR: 在四种互动模型下，研究CP对公共与私有投资的权衡及其对利润的影响，结论是谈判博弈在某些情况下能提高公共投资，但若CPs也被激励投资私有基建，该收益会减弱。


<details>
  <summary>Details</summary>
Motivation: 智能手机普及驱动全球对更好互联网体验的需求，促使需要升级接入网容量；ISP与CP之间的成本分担成为关键问题。本文在四种互动模型下研究CP在公共与私有投资之间的权衡，以评估不同激励结构对CP效用的影响。

Method: 建立四种互动模型（集中分配、合作博弈、非合作博弈、谈判博弈），在各模型下确定公共投资与私有投资的均衡水平，并通过数值仿真比较CP的效用。

Result: 结果显示，谈判博弈在某些情况下能带来高于非合作和集中模型的公共投资；但若CP被激励投资私有基础设施，这一公共投资收益将被削弱。

Conclusion: 激励结构显著影响CP的效用分配，公共投资的收益对CP的私有投资意愿敏感；谈判博弈在促成更高公共投资方面具有潜力，但需考虑CP是否愿意投入私有基础设施的情形。

Abstract: The ubiquity of smartphones has fueled content consumption worldwide, leading
to an ever-increasing demand for a better Internet experience. This has
necessitated an upgrade of the capacity of the access network. The Internet
service providers (ISPs) have been demanding that the content providers (CPs)
share the cost of upgrading access network infrastructure. A \emph{public
investment} in the infrastructure of a neutral ISP will boost the profit of the
CPs, and hence, seems a rational strategy. A CP can also make a \emph{private
investment} in its infrastructure and boost its profits. In this paper, we
study the trade-off between public and private investments by a CP when the
decision is made under different types of interaction between them.
Specifically, we consider four interaction models between CPs -- centralized
allocation, cooperative game, non-cooperative game, and a bargaining game --
and determine the public and private investment for each model. Via numerical
results, we evaluate the impact of different incentive structures on the
utility of the CPs. We see that the bargaining game can result in higher public
investment than the non-cooperative and centralized models. However, this
benefit gets reduced if the CPs are incentivized to invest in private
infrastructure.

</details>


### [17] [Uno: A One-Stop Solution for Inter- and Intra-Datacenter Congestion Control and Reliable Connectivity](https://arxiv.org/abs/2510.15802)
*Tommaso Bonato,Sepehr Abdous,Abdul Kabbani,Ahmad Ghalayini,Nadeen Gebara,Terry Lam,Anup Agarwal,Tiancheng Chen,Zhuolong Yu,Konstantin Taranov,Mahmoud Elhaddad,Daniele De Sensi,Soudeh Ghorbani,Torsten Hoefler*

Main category: cs.NI

TL;DR: Uno is a unified datacenter system that couples rapid congestion-responsive transport with a load-balancing scheme using erasure coding and adaptive routing to handle both intra- and inter-datacenter traffic, outperforming Gemini.


<details>
  <summary>Details</summary>
Motivation: The coexistence of intra- and inter-datacenter traffic with differing RTTs causes congestion control unfairness and slow loss recovery; existing solutions treat them with separate control loops, leading to inefficiencies and reliability concerns.

Method: Develop Uno, which integrates a transport protocol for fast congestion reaction and fair rate control with a load-balancing scheme that combines erasure coding and adaptive routing across DCs.

Result: Uno significantly improves completion times for both inter- and intra-DC flows compared to state-of-the-art methods such as Gemini.

Conclusion: A unified transport-and-routing framework (Uno) can address RTT disparity and reliability needs in mixed intra-/inter-DC traffic, delivering faster reactions and fairer throughput while outperforming existing approaches.

Abstract: Cloud computing and AI workloads are driving unprecedented demand for
efficient communication within and across datacenters. However, the coexistence
of intra- and inter-datacenter traffic within datacenters plus the disparity
between the RTTs of intra- and inter-datacenter networks complicates congestion
management and traffic routing. Particularly, faster congestion responses of
intra-datacenter traffic causes rate unfairness when competing with slower
inter-datacenter flows. Additionally, inter-datacenter messages suffer from
slow loss recovery and, thus, require reliability. Existing solutions overlook
these challenges and handle inter- and intra-datacenter congestion with
separate control loops or at different granularities. We propose Uno, a unified
system for both inter- and intra-DC environments that integrates a transport
protocol for rapid congestion reaction and fair rate control with a load
balancing scheme that combines erasure coding and adaptive routing. Our
findings show that Uno significantly improves the completion times of both
inter- and intra-DC flows compared to state-of-the-art methods such as Gemini.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [18] [The Role of Federated Learning in Improving Financial Security: A Survey](https://arxiv.org/abs/2510.14991)
*Cade Houston Kennedy,Amr Hilal,Morteza Momeni*

Main category: cs.CR

TL;DR: 对金融领域中联邦学习的系统综述，提出基于监管暴露的新分类，评估在合规、欺诈防控和区块链等方面的应用、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 金融机构对数据隐私和安全的高要求，以及物联网端点（ATM、POS）产生的敏感数据需要在不分享原始数据的前提下进行协作建模。FL提供去中心化的跨机构学习，促进跨银行协作与跨设备学习。

Method: 文献综述，提出以监管合规暴露程度为维度的应用分类；评估合规性、现有防御机制、在欺诈防控和区块链集成中的实际应用；比较与以往综述的差异；讨论挑战与未来方向。

Result: 总结了FL在金融安全中的作用、实际应用的合规暴露等级、在欺诈防控与区块链集成方面的进展，并评估了面临的数据异构、对抗性攻击和合规性挑战，以及现有防御机制。

Conclusion: FL有潜力推动金融体系的安全性和隐私合规性，但在数据异质性、攻击与法规合规方面仍需改进；未来方向包括区块链、差分隐私、秘密共享多方计算，以及量子安全框架的整合。

Abstract: With the growth of digital financial systems, robust security and privacy
have become a concern for financial institutions. Even though traditional
machine learning models have shown to be effective in fraud detections, they
often compromise user data by requiring centralized access to sensitive
information. In IoT-enabled financial endpoints such as ATMs and POS Systems
that regularly produce sensitive data that is sent over the network. Federated
Learning (FL) offers a privacy-preserving, decentralized model training across
institutions without sharing raw data. FL enables cross-silo collaboration
among banks while also using cross-device learning on IoT endpoints. This
survey explores the role of FL in enhancing financial security and introduces a
novel classification of its applications based on regulatory and compliance
exposure levels ranging from low-exposure tasks such as collaborative portfolio
optimization to high-exposure tasks like real-time fraud detection. Unlike
prior surveys, this work reviews FL's practical use within financial systems,
discussing its regulatory compliance and recent successes in fraud prevention
and blockchain-integrated frameworks. However, FL deployment in finance is not
without challenges. Data heterogeneity, adversarial attacks, and regulatory
compliance make implementation far from easy. This survey reviews current
defense mechanisms and discusses future directions, including blockchain
integration, differential privacy, secure multi-party computation, and
quantum-secure frameworks. Ultimately, this work aims to be a resource for
researchers exploring FL's potential to advance secure, privacy-compliant
financial systems.

</details>


### [19] [A Light Weight Cryptographic Solution for 6LoWPAN Protocol Stack](https://arxiv.org/abs/2510.14993)
*Sushil Khairnar,Gaurav Bansod,Vijay Dahiphale*

Main category: cs.CR

TL;DR: 提出适用于6LoWPAN的LiCi2轻量密码，显著降低内存、功耗和硬件成本，并对多种攻击进行分析，性能优于PRESENT。


<details>
  <summary>Details</summary>
Motivation: 在物联网等受限环境下实现安全通信，需要资源高效的对称加密算法。

Method: 在6LoWPAN架构中实现LiCi2轻量密码，进行硬件实现，评估64位分组、128位密钥下的资源占用和功耗，并与PRESENT等对比；并给出对线性、差分、Biclique和Avalanche攻击的分析。

Result: 实现需1856字节FLASH、1272字节RAM，功耗约25mW；硬件门数约1051GE；相较于PRESENT，资源和功耗更低；LiCi2源自LiCi，但在设计指标上全面超越。

Conclusion: LiCi2是在物联网受限环境中的优选加密方案，适合部署在6LoWPAN等平台。

Abstract: Lightweight cryptography is an emerging field in the field of research, which
endorses algorithms which are best suited for constrained environment. Design
metrics like Gate Equivalence (GE), Memory Requirement, Power Consumption, and
Throughput play a vital role in the applications like IoT. This paper presents
the 6LoWPAN Protocol Stack which is a popular standard of communication for
constrained devices. This paper presents an implementation of a lightweight
6LoWPAN Protocol stack by using a Light weight Cipher instead of regular heavy
encryption cipher AES. The cipher proposed in this paper is specifically
suitable for 6LoWPAN architecture as it addresses all the constraints possessed
by wireless sensor nodes. The lightweight cipher proposed in the paper needs
only 1856 bytes of FLASH and 1272 bytes of RAM memory which is less than any
other standard existing lightweight cipher design. The proposed ciphers power
consumption is around 25 mW which is significantly less as compared to ISO
certified lightweight cipher PRESENT which consumes around 38 mW of dynamic
power. This paper also discusses the detailed analysis of cipher against the
attacks like Linear Cryptanalysis, Differential Cryptanalysis, Biclique attack
and Avalanche attack. The cipher implementation on hardware is around 1051 GEs
for 64 bit of block size with 128 bit of key length which is less as compared
to existing lightweight cipher design. The proposed cipher LiCi2 is motivated
from LiCi cipher design but outclasses it in every design metric. We believe
the design of LiCi2 is the obvious choice for researchers to implement in
constrained environments like IoT.

</details>


### [20] [VaultGemma: A Differentially Private Gemma Model](https://arxiv.org/abs/2510.15001)
*Amer Sinha,Thomas Mesnard,Ryan McKenna,Daogao Liu,Christopher A. Choquette-Choo,Yangsibo Huang,Da Yu,George Kaissis,Zachary Charles,Ruibo Liu,Lynn Chua,Pritish Kamath,Pasin Manurangsi,Steve He,Chiyuan Zhang,Badih Ghazi,Borja De Balle Pigem,Prem Eruvbetine,Tris Warkentin,Armand Joulin,Ravi KumarAmer Sinha,Thomas Mesnard,Ryan McKenna,Daogao Liu,Christopher A. Choquette-Choo,Yangsibo Huang,Da Yu,George Kaissis,Zachary Charles,Ruibo Liu,Lynn Chua,Pritish Kamath,Pasin Manurangsi,Steve He,Chiyuan Zhang,Badih Ghazi,Borja De Balle Pigem,Prem Eruvbetine,Tris Warkentin,Armand Joulin,Ravi Kumar*

Main category: cs.CR

TL;DR: VaultGemma 1B is a 1B-parameter, privacy-preserving language model in the Gemma family, trained with differential privacy on the same data mixture as Gemma 2, and publicly released.


<details>
  <summary>Details</summary>
Motivation: 推动隐私保护的大模型研究，通过在大规模模型上应用差分隐私，并对外开放，促进透明度和复现性。

Method: 在 Gemma 2 系列使用的数据混合上，以差分隐私进行训练，构建1B参数的 VaultGemma 1B，属于 Gemma 家族的一员。

Result: 证明了在隐私保护约束下训练大型语言模型的可行性，并提供一个公开可获取的模型以促进研究与对比。

Conclusion: 该工作通过开放发布，推动了隐私优先的语言模型发展与社区协作。

Abstract: We introduce VaultGemma 1B, a 1 billion parameter model within the Gemma
family, fully trained with differential privacy. Pretrained on the identical
data mixture used for the Gemma 2 series, VaultGemma 1B represents a
significant step forward in privacy-preserving large language models. We openly
release this model to the community

</details>


### [21] [Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.15017)
*ChenYu Wu,Yi Wang,Yang Liao*

Main category: cs.CR

TL;DR: 提出一种基于蜜网的主动防护体系，通过训练诱饵模型生成模糊但语义相关的回答，与受保护的LLM的安全回复结合，在多轮对话中逐步暴露恶意意图，利用蜜网效用评分（HUS）和防御有效性率（DER）实现安全性与可用性的权衡，实验显示对多轮越狱攻击有显著抑制作用。


<details>
  <summary>Details</summary>
Motivation: 现有防御多依赖被动式拒绝，易被自适应攻击者绕过，且过于限制正当用户。需要将风险规避转化为风险利用的主动防护，以应对多轮连锁越狱攻击。

Method: 训练一个诱饵模型来生成模棱两可、不可操作但在语义上相关的回应，作为诱饵引导用户意图；在受保护的LLM输出安全回复的基础上插入前瞻性的诱问，通过多轮交互逐步揭示恶意意图；提出蜜网效用评分（HUS）以衡量诱饵回复的吸引力与实现难度，使用防御有效性率（DER）在安全性与可用性之间实现平衡评估。

Result: 在MHJ数据集和针对GPT-4o的最新攻击方法上的初步实验表明，该系统显著削弱越狱成功率，同时尽量保留无害用户体验。

Conclusion: 蜜网式主动防护可以将风险规避转化为风险利用，通过对诱饵的设计与评估在多轮对话场景下提升对越狱攻击的防护效果，同时需要权衡诱饵设计的鲁棒性及对正当用户的影响。

Abstract: Large language models (LLMs) are increasingly vulnerable to multi-turn
jailbreak attacks, where adversaries iteratively elicit harmful behaviors that
bypass single-turn safety filters. Existing defenses predominantly rely on
passive rejection, which either fails against adaptive attackers or overly
restricts benign users. We propose a honeypot-based proactive guardrail system
that transforms risk avoidance into risk utilization. Our framework fine-tunes
a bait model to generate ambiguous, non-actionable but semantically relevant
responses, which serve as lures to probe user intent. Combined with the
protected LLM's safe reply, the system inserts proactive bait questions that
gradually expose malicious intent through multi-turn interactions. We further
introduce the Honeypot Utility Score (HUS), measuring both the attractiveness
and feasibility of bait responses, and use a Defense Efficacy Rate (DER) for
balancing safety and usability. Initial experiment on MHJ Datasets with recent
attack method across GPT-4o show that our system significantly disrupts
jailbreak success while preserving benign user experience.

</details>


### [22] [Physical Layer Deception based on Semantic Distortion](https://arxiv.org/abs/2510.15063)
*Wenwen Chen,Bin Han,Yao Zhu,Anke Schmeink,Giuseppe Caire,Hans D. Schotten*

Main category: cs.CR

TL;DR: 将物理层欺骗(PLD)扩展到语义通信场景，提出面向攻击者/合法接收者的解密策略和加密参数的最优资源分配，给出高效算法、闭式解，并通过数值仿真验证可行性。


<details>
  <summary>Details</summary>
Motivation: 面向对抗的积极防御需要新度量（语义扭曲）来评估安全性；传统被动防御难以在语义级别保障安全，因此将PLD与语义通信结合。

Method: 建立理论优化框架，预测合法接收者和窃听者的解密策略，优化资源分配和加密参数，使窃听者的语义扭曲最大化、合法接收者扭曲最小化；提出高效优化算法并推导多场景的解析解。

Result: 给出优化问题的理论分析、可行的高效算法和若干场景的闭式解；数值仿真验证理论结论并显示算法的实用性。

Conclusion: 在语义通信场景下验证了PLD框架的可行性，提供了有效的资源分配与加密策略的解法，并通过仿真确认了理论结果和算法的实际应用潜力。

Abstract: Physical layer deception (PLD) is a framework we previously introduced that
integrates physical layer security (PLS) with deception techniques, enabling
proactive countermeasures against eavesdropping rather than relying solely on
passive defense. We extend this framework to a semantic communication model and
conduct a theoretical analysis using semantic distortion as the performance
metric. In this work, we further investigate the receiver's selection of
decryption strategies and the transmitter's optimization of encryption
strategies. By anticipating the decryption strategy likely to be employed by
the legitimate receiver and eavesdropper, the transmitter can optimize resource
allocation and encryption parameters, thereby maximizing the semantic
distortion at the eavesdropper while maintaining a low level of semantic
distortion for the legitimate receiver. We present a rigorous analysis of the
resulting optimization problem, propose an efficient optimization algorithm,
and derive closed-form optimal solutions for multiple scenarios. Finally, we
corroborate the theoretical findings with numerical simulations, which also
confirm the practicality of the proposed algorithm.

</details>


### [23] [Sequential Comics for Jailbreaking Multimodal Large Language Models via Structured Visual Storytelling](https://arxiv.org/abs/2510.15068)
*Deyue Zhang,Dongdong Yang,Junjie Mu,Quancheng Zou,Zonghao Ying,Wenzhuo Xu,Zhao Liu,Xuan Wang,Xiangzheng Zhang*

Main category: cs.CR

TL;DR: 提出基于连续漫画式叙事的跨模态攻击，利用视觉序列绕过多模态对齐，在安全基准测试中实现83.5%的攻击成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在安全对齐方面存在跨模态漏洞，攻击向量日益多样化，需系统评估新型叙事性攻击并评估现有防御的不足。

Method: 通过一个辅助大语言模型将恶意查询拆解为视觉上无害的叙事要素；利用扩散模型生成对应的连续图像序列；依赖叙事连贯性来引导模型输出有害内容。

Result: 在来自公认安全基准的有害文本查询上，平均攻击成功率为83.5%，较现有最优方法提升约46%；相较现有视觉越狱方法，在多类有害内容上展现更强的攻击效果；并对攻击模式、关键脆弱性因素及防御局限性进行了系统分析。

Conclusion: 揭示基于叙事的跨模态攻击对当前安全机制的显著挑战，强调需要在对齐与检测层面加强对叙事性一致性、视觉序列与文本间协同性的防护研究，以提升多模态模型对叙事型攻击的鲁棒性。

Abstract: Multimodal large language models (MLLMs) exhibit remarkable capabilities but
remain susceptible to jailbreak attacks exploiting cross-modal vulnerabilities.
In this work, we introduce a novel method that leverages sequential comic-style
visual narratives to circumvent safety alignments in state-of-the-art MLLMs.
Our method decomposes malicious queries into visually innocuous storytelling
elements using an auxiliary LLM, generates corresponding image sequences
through diffusion models, and exploits the models' reliance on narrative
coherence to elicit harmful outputs. Extensive experiments on harmful textual
queries from established safety benchmarks show that our approach achieves an
average attack success rate of 83.5\%, surpassing prior state-of-the-art by
46\%. Compared with existing visual jailbreak methods, our sequential narrative
strategy demonstrates superior effectiveness across diverse categories of
harmful content. We further analyze attack patterns, uncover key vulnerability
factors in multimodal safety mechanisms, and evaluate the limitations of
current defense strategies against narrative-driven attacks, revealing
significant gaps in existing protections.

</details>


### [24] [PoTS: Proof-of-Training-Steps for Backdoor Detection in Large Language Models](https://arxiv.org/abs/2510.15106)
*Issam Seddik,Sami Souihi,Mohamed Tamaazousti,Sara Tucci Piergiovanni*

Main category: cs.CR

TL;DR: 提出了一种名为 Proof-of-Training Steps 的验证协议，用于独立方对 LLM 的训练过程进行核验，以发现后门注入并可在训练前阶段早期发现问题。即使训练数据中包含高达 10% 的触发器，该方法也能降低攻击者的攻击成功率，且验证步骤速度比训练步骤快约 3 倍，提升对内部威胁的可追溯性与安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在关键领域的应用，确保训练过程的安全性和可信性变得至关重要。现有的后训练验证方法（如 Proof-of-Learning）在需要完全重训练、对隐蔽操作不鲁棒、以及无法在训练阶段早期检测方面存在显著局限性，急需一个能在训练阶段早期即可发现并遏制后门的办法，以减少计算成本和提高安全性。

Method: 提出 Proof-of-Training Steps，允许独立审计人（Alice）核实开发者（Bob）是否遵循声明的训练方案（包含数据批次、模型结构、超参数等）。通过分析语言建模头（LM-Head）对输入扰动的敏感性，揭示训练中的微妙后门注入或偏离。该方法在训练数据中存在高达 10% 的触发器时，仍能显著降低攻击者的攻击成功率，并且在注入步骤就能实现早期检测，验证步骤的速度约为训练步骤的 1/3。

Result: 在设计层面上证明了方法在早期检测和高规避后门方面的可行性，且对潜在的内鬼威胁具有一定的可追溯性与防护作用。实验和实证结果表明，即使触发器覆盖率达到 10%，该协议也可降低后门攻击的有效性，且验证阶段明显高于传统训练过程的成本效率。

Conclusion: 该方法有望提升 LLM 开发过程的可问责性和安全性，特别是在对抗内部威胁方面具有潜力。通过在训练步骤阶段即对照与验证，可以更早发现并阻断后门注入，减少计算资源浪费并提高风险治理水平。

Abstract: As Large Language Models (LLMs) gain traction across critical domains,
ensuring secure and trustworthy training processes has become a major concern.
Backdoor attacks, where malicious actors inject hidden triggers into training
data, are particularly insidious and difficult to detect. Existing
post-training verification solutions like Proof-of-Learning are impractical for
LLMs due to their requirement for full retraining, lack of robustness against
stealthy manipulations, and inability to provide early detection during
training. Early detection would significantly reduce computational costs. To
address these limitations, we introduce Proof-of-Training Steps, a verification
protocol that enables an independent auditor (Alice) to confirm that an LLM
developer (Bob) has followed the declared training recipe, including data
batches, architecture, and hyperparameters. By analyzing the sensitivity of the
LLMs' language modeling head (LM-Head) to input perturbations, our method can
expose subtle backdoor injections or deviations in training. Even with backdoor
triggers in up to 10 percent of the training data, our protocol significantly
reduces the attacker's ability to achieve a high attack success rate (ASR). Our
method enables early detection of attacks at the injection step, with
verification steps being 3x faster than training steps. Our results highlight
the protocol's potential to enhance the accountability and security of LLM
development, especially against insider threats.

</details>


### [25] [Partitioning $\mathbb{Z}_{sp}$ in finite fields and groups of trees and cycles](https://arxiv.org/abs/2510.15108)
*Nikolaos Verykios,Christos Gogos*

Main category: cs.CR

TL;DR: 本论文研究环 Z_sp 的代数与图结构，聚焦其在有限场的分解、核以及特殊子集的性质。给出 F_s 与 pF_s 及 pF_s^{*} 与 pF_s^{+1,*} 的同构；通过 arcs 与根树描述 Z_sp 的前周期结构，证明非被 s 或 p 整除的元素的树可由单位元的树通过乘以循环弧生成；定义并研究 D_sp，其元素既非 s、p 的倍数，也非“偏一”元素，其图形分解为循环与前周期树；所有 Z_sp 的循环必含内部循环，这些循环可由 pF_s 与 sF_p 的循环推导得到；并讨论 D_sp 的密码学意义，特别是在分析循环攻击与因子化方法中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 理解在整环 Z_sp 上的动力学与代数结构，揭示其与有限域的关系以及对密码学攻击的可能影响。通过系统地构建同构、树结构和集合 D_sp，提供一种分析该环及其图结构的框架。

Method: 建立环同构与映射关系；引入弧（arcs）与根树（rooted trees）来刻画前周期结构；对 D_sp 进行分解研究，证明其图形包含循环与前周期树；分析来自有限域 pF_s 与 sF_p 的循环对内循环的产生机制；探讨在循环攻击与因子化中对 D_sp 的潜在应用。

Result: 证实 F_s 与 pF_s 及 pF_s^{*} 与 pF_s^{+1,*} 的经典同构；证明非整除于 s 或 p 的元素的树结构可由单位元树经乘以循环弧生成；D_sp 的图分解为循环与前周期树；每个循环都包含内部循环，这些内部循环可从 pF_s 与 sF_p 的循环中推导得到；提出 D_sp 在分析循环攻击和因子化方法中的密码学相关性。

Conclusion: 给出一个统一的代数与图结构框架来分析 Z_sp 的动力学与分解，揭示有限域循环与内循环之间的关系，并指出 D_sp 在密码分析中的潜在应用，尤其是在循环攻击与因子分解策略方面的意义。

Abstract: This paper investigates the algebraic and graphical structure of the ring
$\mathbb{Z}_{sp}$, with a focus on its decomposition into finite fields,
kernels, and special subsets. We establish classical isomorphisms between
$\mathbb{F}_s$ and $p\mathbb{F}_s$, as well as $p\mathbb{F}_s^{\star}$ and
$p\mathbb{F}_s^{+1,\star}$. We introduce the notion of arcs and rooted trees to
describe the pre-periodic structure of $\mathbb{Z}_{sp}$, and prove that trees
rooted at elements not divisible by $s$ or $p$ can be generated from the tree
of unity via multiplication by cyclic arcs. Furthermore, we define and analyze
the set $\mathbb{D}_{sp}$, consisting of elements that are neither multiples of
$s$ or $p$ nor "off-by-one" elements, and show that its graph decomposes into
cycles and pre-periodic trees. Finally, we demonstrate that every cycle in
$\mathbb{Z}_{sp}$ contains inner cycles that are derived predictably from the
cycles of the finite fields $p\mathbb{F}_s$ and $s\mathbb{F}_p$, and we discuss
the cryptographic relevance of $\mathbb{D}_{sp}$, highlighting its potential
for analyzing cyclic attacks and factorization methods.

</details>


### [26] [AndroByte: LLM-Driven Privacy Analysis through Bytecode Summarization and Dynamic Dataflow Call Graph Generation](https://arxiv.org/abs/2510.15112)
*Mst Eshita Khatun,Lamine Noureddine,Zhiyong Sui,Aisha Ali-Gombe*

Main category: cs.CR

TL;DR: AndroByte 利用基于LLM的字节码摘要来动态生成Android应用数据流调用图，从静态分析中推断数据泄漏路径，解决传统方法对预定义sink/传播规则的依赖和taint爆炸问题；在泄漏检测方面表现优于 FlowDroid/Amandroid，且在G-Eval等指标上给出可解释的洞察。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用的快速增长，保护用户隐私成为关键。Android 应用常收集、存储、分享敏感信息，现有数据流分析多依赖正式方法、启发式或基于规则，存在实现复杂、易出错、对predefined sink依赖性强、规模化受限等问题。

Method: 提出 AndroByte，通过对字节码进行迭代式摘要并应用LLM推理，动态生成准确且可解释的数据流调用图；在静态分析基础上进行推断，不依赖预定义的传播规则或sinks列表，对数据流进行可解释的推断和泄漏检测。

Result: 在动态数据流调用图生成任务上达到Fβ=89%的分数，优于FlowDroid与Amandroid；并通过G-Eval等指标获得高水平且可解释的洞察与评估。

Conclusion: AndroByte 为Android隐私分析提供了一种更灵活、可扩展且可解释的解决方案，降低对预定义规则的依赖并提升泄漏检测效果。

Abstract: With the exponential growth in mobile applications, protecting user privacy
has become even more crucial. Android applications are often known for
collecting, storing, and sharing sensitive user information such as contacts,
location, camera, and microphone data often without the user's clear consent or
awareness raising significant privacy risks and exposure. In the context of
privacy assessment, dataflow analysis is particularly valuable for identifying
data usage and potential leaks. Traditionally, this type of analysis has relied
on formal methods, heuristics, and rule-based matching. However, these
techniques are often complex to implement and prone to errors, such as taint
explosion for large programs. Moreover, most existing Android dataflow analysis
methods depend heavily on predefined list of sinks, limiting their flexibility
and scalability. To address the limitations of these existing techniques, we
propose AndroByte, an AI-driven privacy analysis tool that leverages LLM
reasoning on bytecode summarization to dynamically generate accurate and
explainable dataflow call graphs from static code analysis. AndroByte achieves
a significant F\b{eta}-Score of 89% in generating dynamic dataflow call graphs
on the fly, outperforming the effectiveness of traditional tools like FlowDroid
and Amandroid in leak detection without relying on predefined propagation rules
or sink lists. Moreover, AndroByte's iterative bytecode summarization provides
comprehensive and explainable insights into dataflow and leak detection,
achieving high, quantifiable scores based on the G-Eval metric.

</details>


### [27] [Bilinear Compressive Security](https://arxiv.org/abs/2510.15380)
*Axel Flinth,Hubert Orlicki,Semira Einsele,Gerhard Wunder*

Main category: cs.CR

TL;DR: 提出Bilinear Compressive Security (BCS) 框架，将线性编码Q与随机滤波器h的卷积进行双线性混合，通过盲卷积恢复实现信息保护，并给出对已知明文攻击的理论边界，s=1时不可恢复。


<details>
  <summary>Details</summary>
Motivation: 为解决压缩感知加密在缺少密钥轮换时易受已知明文攻击的问题，同时保持低复杂度和能效，提升在物联网场景的安全性。

Method: 在消息x上先用矩阵Q线性编码，再与随机滤波器h做卷积，得到y=h*Qx；接收端在不知h的情况下通过盲卷积从y恢复x。对比攻击者在重复观测y并给定若干已知x_k时对Q的恢复，给出在h满足弱对称条件时的样本复杂度界限 Ω(max(n,(n/s)^2))，且当s=1时不可恢复。

Result: 在给定的假设条件下，若x_k具s稀疏，则需要大量观测才能恢复Q；s=1时完全不能恢复，使BCS相较于标准CS在安全性上更优。

Conclusion: 尽管分析有利于攻击者的假设，BCS在实际中提供了比传统CS更强的对已知明文攻击的抵抗力，但仍需对具体实现和安全模型谨慎评估，实际安全性取决于h的对称性及稀疏性假设的真实性。

Abstract: Beyond its widespread application in signal and image processing,
\emph{compressed sensing} principles have been greatly applied to secure
information transmission (often termed 'compressive security'). In this
scenario, the measurement matrix $Q$ acts as a one time pad encryption key (in
complex number domain) which can achieve perfect information-theoretic security
together with other benefits such as reduced complexity and energy efficiency
particularly useful in IoT. However, unless the matrix is changed for every
message it is vulnerable towards known plain text attacks: only $n$
observations suffices to recover a key $Q$ with $n$ columns. In this paper, we
invent and analyze a new method (termed 'Bilinear Compressive Security (BCS)')
addressing these shortcomings: In addition to the linear encoding of the
message $x$ with a matrix $Q$, the sender convolves the resulting vector with a
randomly generated filter $h$. Assuming that $h$ and $x$ are sparse, the
receiver can then recover $x$ without knowledge of $h$ from $y=h*Qx$ through
blind deconvolution. We study a rather idealized known plaintext attack for
recovering $Q$ from repeated observations of $y$'s for different, known $x_k$,
with varying and unknown $h$ ,giving Eve a number of advantages not present in
practice. Our main result for BCS states that under a weak symmetry condition
on the filter $h$, recovering $Q$ will require extensive sampling from
transmissions of $\Omega\left(\max\left(n,(n/s)^2\right)\right)$ messages $x_k$
if they are $s$-sparse. Remarkably, with $s=1$ it is impossible to recover the
key. In this way, the scheme is much safer than standard compressed sensing
even though our assumptions are much in favor towards a potential attacker.

</details>


### [28] [Beyond the Voice: Inertial Sensing of Mouth Motion for High Security Speech Verification](https://arxiv.org/abs/2510.15173)
*Ynes Ineza,Muhammad A. Ullah,Abdul Serwadda,Aurore Munyaneza*

Main category: cs.CR

TL;DR: 提出将口部运动作为声学鉴别的二次认证因子，通过在嘴周围部署传感器捕捉口张合与下颌几何的运动特征，在多场景测试中实现极低的中位EER（0.01或以下）


<details>
  <summary>Details</summary>
Motivation: 随着高级声纹伪造技术的普及，单一声学鉴别在高风险场景中的可靠性下降；需要引入额外的、对说话人特征独特且难以伪造的生物信号以提升安全性

Method: 在嘴周围布置轻量化惯性传感器，记录口张开与下部面部几何的时间演化，构建原型系统；在43名参与者身上进行 seated、level-ground walking、stairs、native vs non-native English 四种场景下的评估

Result: 在所有场景中，中位EER为0.01或更低，表明口部运动数据对姿态、步态、语言背景等变异具有鲁棒性，具有较强的判别力

Conclusion: 将口部运动与声学证据结合的二次认证对提升语音认证系统在高风险场景中的安全性具有实际应用潜力，并可作为对传统声纹的有效补充

Abstract: Voice interfaces are increasingly used in high stakes domains such as mobile
banking, smart home security, and hands free healthcare. Meanwhile, modern
generative models have made high quality voice forgeries inexpensive and easy
to create, eroding confidence in voice authentication alone. To strengthen
protection against such attacks, we present a second authentication factor that
combines acoustic evidence with the unique motion patterns of a speaker's lower
face. By placing lightweight inertial sensors around the mouth to capture mouth
opening and evolving lower facial geometry, our system records a distinct
motion signature with strong discriminative power across individuals. We built
a prototype and recruited 43 participants to evaluate the system under four
conditions seated, walking on level ground, walking on stairs, and speaking
with different language backgrounds (native vs. non native English). Across all
scenarios, our approach consistently achieved a median equal error rate (EER)
of 0.01 or lower, indicating that mouth movement data remain robust under
variations in gait, posture, and spoken language. We discuss specific use cases
where this second line of defense could provide tangible security benefits to
voice authentication systems.

</details>


### [29] [DSSmoothing: Toward Certified Dataset Ownership Verification for Pre-trained Language Models via Dual-Space Smoothing](https://arxiv.org/abs/2510.15303)
*Ting Qiao,Xing Liu,Wenke Huang,Jianbin Li,Zhaoxin Fan,Yiming Li*

Main category: cs.CR

TL;DR: 提出第一种基于双空间平滑的文本数据集所有权认证方法DSSmoothing，通过在嵌入空间引入连续扰动和在排列空间进行受控令牌重排来捕捉语义与序列鲁棒性，并在验证阶段对两个空间应用随机平滑以计算水印鲁棒性并与良性模型的主概率进行统计对比，给出可证明的鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练语言模型对数据集的高度依赖带来所有权和版权担忧；现有数据集所有权验证方法往往假设水印在推理过程中保持稳定，但在噪声和对手扰动下易失效，因此需要对噪声与对抗扰动具备鲁棒性的认证方法。

Method: DSSmoothing 分两阶段工作：第一阶段在嵌入空间引入连续扰动并在文本的排列空间进行受控令牌重排，将触发以法线约束的方式嵌入以生成鲁棒的水印数据集；第二阶段在验证时对两个空间实施随机平滑，计算水印鲁棒性WR，并与一组良性模型的主概率PP进行统计比较，从而在受限的双空间扰动下提供鲁棒性证明。

Result: 在多组代表性网页数据集上，DSSmoothing 展现出稳定且可靠的验证性能，并对潜在自适应攻击具有鲁棒性。

Conclusion: 理论上，DSSmoothing 在双空间扰动下能够保持WR显著高于PP，从而实现可证明的 dataset ownership verification，并且具有实际可行性和稳健性。

Abstract: Large web-scale datasets have driven the rapid advancement of pre-trained
language models (PLMs), but unauthorized data usage has raised serious
copyright concerns. Existing dataset ownership verification (DOV) methods
typically assume that watermarks remain stable during inference; however, this
assumption often fails under natural noise and adversary-crafted perturbations.
We propose the first certified dataset ownership verification method for PLMs
based on dual-space smoothing (i.e., DSSmoothing). To address the challenges of
text discreteness and semantic sensitivity, DSSmoothing introduces continuous
perturbations in the embedding space to capture semantic robustness and applies
controlled token reordering in the permutation space to capture sequential
robustness. DSSmoothing consists of two stages: in the first stage, triggers
are collaboratively embedded in both spaces to generate norm-constrained and
robust watermarked datasets; in the second stage, randomized smoothing is
applied in both spaces during verification to compute the watermark robustness
(WR) of suspicious models and statistically compare it with the principal
probability (PP) values of a set of benign models. Theoretically, DSSmoothing
provides provable robustness guarantees for dataset ownership verification by
ensuring that WR consistently exceeds PP under bounded dual-space
perturbations. Extensive experiments on multiple representative web datasets
demonstrate that DSSmoothing achieves stable and reliable verification
performance and exhibits robustness against potential adaptive attacks.

</details>


### [30] [Flexible Threshold Multi-client Functional Encryption for Inner Product in Federated Learning](https://arxiv.org/abs/2510.15367)
*Ruyuan Zhang,Jinguang Han,Liqun Chen*

Main category: cs.CR

TL;DR: 提出了一个可灵活门限的多客户端功能加密用于内积的方案（FTMCFE-IP），支持客户端掉线，允许多客户端独立生成密文且阈值可灵活选取且不需重新初始化。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景中保护梯度隐私，但现有多客户端功能加密（MCFE）方案对客户端掉线和阈值灵活性支持不足，亟需一个能够在不重新初始化的前提下实现可变阈值、可容忍参与者掉线的内积计算密文方案。

Method: 提出FTMCFE-IP方案，允许客户端独立生成密文，在加密阶段可灵活选择阈值且不重置系统；授权用户基于其功能密钥和密文计算向量的内积，同时不学习其它信息；提出相应的安全模型、给出具体构造并给出形式化安全性证明；实现并评估该方案的可行性与性能。

Result: 给出一个具体的构造及其安全性证明，并实现与评估，验证在设定的安全模型下方案可行且支持掉线与灵活阈值的需求。

Conclusion: FTMCFE-IP 为联邦学习中的隐私保护计算提供了一种可灵活阈值、具容错能力的密文内积计算方案，具有实际落地潜力，后续可在效率、扩展性和更广泛的功能集方面进一步优化。

Abstract: Federated learning (FL) is a distributed machine learning paradigm that
enables multiple clients to collaboratively train a shared model without
disclosing their local data. To address privacy issues of gradient, several
privacy-preserving machine-learning schemes based on multi-client functional
encryption (MCFE) have been proposed. However, existing MCFE-based schemes
cannot support client dropout or flexible threshold selection, which are
essential for practical FL. In this paper, we design a flexible threshold
multi-client functional encryption for inner product (FTMCFE-IP) scheme, where
multiple clients generate ciphertexts independently without any interaction. In
the encryption phase, clients are able to choose a threshold flexibly without
reinitializing the system. The decryption can be performed correctly when the
number of online clients satisfies the threshold. An authorized user are
allowed to compute the inner product of the vectors associated with his/her
functional key and the ciphertext, respectively, but cannot learning anything
else. Especially, the presented scheme supports clients drop out. Furthermore,
we provide the definition and security model of our FTMCFE-IP scheme,and
propose a concrete construction. The security of the designed scheme is
formally proven. Finally, we implement and evaluate our FTMCFE-IP scheme.

</details>


### [31] [FHE-SQL: Fully Homomorphic Encrypted SQL Database](https://arxiv.org/abs/2510.15413)
*Po-Yu Tseng,Po-Chu Hsu,Shih-Wei Liao*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: FHE-SQL is a privacy-preserving database system that enables secure query
processing on encrypted data using Fully Homomorphic Encryption (FHE),
providing privacy guaranties where an untrusted server can execute encrypted
queries without learning either the query contents or the underlying data.
Unlike property-preserving encryption-based systems such as CryptDB, which rely
on deterministic or order-preserving encryption and are vulnerable to
frequency, order, and equality-pattern inference attacks, FHE-SQL performs
computations entirely under encryption, eliminating these leakage channels.
Compared to trusted-hardware approaches such as TrustedDB, which depend on a
hardware security module and thus inherit its trust and side-channel
limitations, our design achieves end-to-end cryptographic protection without
requiring trusted execution environments. In contrast to high-performance
FHE-based engines-Hermes, which target specialized workloads such as vector
search, FHE-SQL supports general SQL query semantics with schema-aware,
type-safe definitions suitable for relational data management. FHE-SQL
mitigates the high cost of ciphertext space by using an indirection
architecture that separates metadata in RocksDB from large ciphertexts in blob
storage. It supports oblivious selection via homomorphic boolean masks,
multi-tier caching, and garbage collection, with security proven under the
Universal Composability framework.

</details>


### [32] [SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models](https://arxiv.org/abs/2510.15476)
*Hanbin Hong,Shuya Feng,Nima Naderloui,Shenao Yan,Jingyu Zhang,Biying Liu,Ali Arastehfard,Heqing Huang,Yuan Hong*

Main category: cs.CR

TL;DR: 系统化总结LLM提示安全的 SoK：提出多层次分类法、可机器读取的威胁模型、开放源代码评估工具包、最大规模的 Jailbreak 数据集JAILBREAKDB，以及攻防方法的综合评估与排行榜，以统一评估和推进高可靠的LLM部署。


<details>
  <summary>Details</summary>
Motivation: LLMs在现实世界中的广泛应用暴露出关键安全风险，尤其是通过绕过对齐的越狱提示，导致有害输出。现有研究在定义、威胁模型和评估标准上缺乏统一，难以进行系统性比较和进展。

Method: 提出一个多层次的系统化框架，将攻击、防御与漏洞进行分层整理；将威胁模型和成本假设形式化为可机器可读的画像以便可重复评估；发布开源评估工具包实现标准化、可审计的攻击/防御比较；发布JAILBREAKDB数据集；给出完整的基准评估与排行榜。

Result: 提供了统一的研究框架、可复现的评估平台、公开数据集与排行榜，促进不同方法的可比性并推动高风险场景下的鲁棒LLM研究。

Conclusion: SoK为未来研究奠定基础，促进更稳健、可信赖的LLM在高风险部署中的应用。

Abstract: Large Language Models (LLMs) have rapidly become integral to real-world
applications, powering services across diverse sectors. However, their
widespread deployment has exposed critical security risks, particularly through
jailbreak prompts that can bypass model alignment and induce harmful outputs.
Despite intense research into both attack and defense techniques, the field
remains fragmented: definitions, threat models, and evaluation criteria vary
widely, impeding systematic progress and fair comparison. In this
Systematization of Knowledge (SoK), we address these challenges by (1)
proposing a holistic, multi-level taxonomy that organizes attacks, defenses,
and vulnerabilities in LLM prompt security; (2) formalizing threat models and
cost assumptions into machine-readable profiles for reproducible evaluation;
(3) introducing an open-source evaluation toolkit for standardized, auditable
comparison of attacks and defenses; (4) releasing JAILBREAKDB, the largest
annotated dataset of jailbreak and benign prompts to date; and (5) presenting a
comprehensive evaluation and leaderboard of state-of-the-art methods. Our work
unifies fragmented research, provides rigorous foundations for future studies,
and supports the development of robust, trustworthy LLMs suitable for
high-stakes deployment.

</details>


### [33] [HarmRLVR: Weaponizing Verifiable Rewards for Harmful LLM Alignment](https://arxiv.org/abs/2510.15499)
*Yuexiao Liu,Lijun Li,Xingjun Wang,Jing Shao*

Main category: cs.CR

TL;DR: RLVR的对齐逆转风险高，GRPO攻击可用64个有害提示实现高成功率和高有害性提升，威胁开源模型安全。


<details>
  <summary>Details</summary>
Motivation: 评估RLVR在安全性方面的潜在风险，检验其对齐是否易被快速逆向利用。

Method: 对五个模型（Llama、Qwen、DeepSeek）进行基于RLVR的攻击，使用GRPO（无回答提示）与64个有害提示，比较攻击与有害微调的效果。

Result: 平均有害性评分升至4.94，攻击成功率96.01%，相比有害微调效果更优，且保持通用能力。

Conclusion: RLVR可能带来严重的对齐漏洞，需加强对开源模型的安全防护与对齐机制，后续工作包括防御策略与更安全的RLVR设计。

Abstract: Recent advancements in Reinforcement Learning with Verifiable Rewards (RLVR)
have gained significant attention due to their objective and verifiable reward
signals, demonstrating strong performance in reasoning and code generation
tasks. However, the potential safety risks associated with RLVR remain
underexplored. This paper presents HarmRLVR, the first systematic investigation
into the alignment reversibility risk of RLVR. We show that safety alignment
can be rapidly reversed using GRPO with merely 64 harmful prompts without
responses, causing models to readily comply with harmful instructions. Across
five models from Llama, Qwen, and DeepSeek, we empirically demonstrate that
RLVR-based attacks elevate the average harmfulness score to 4.94 with an attack
success rate of 96.01\%, significantly outperforming harmful fine-tuning while
preserving general capabilities. Our findings reveal that RLVR can be
efficiently exploited for harmful alignment, posing serious threats to
open-source model safety. Please see our code at
https://github.com/lyxx2535/HarmRLVR.

</details>


### [34] [High Memory Masked Convolutional Codes for PQC](https://arxiv.org/abs/2510.15515)
*Meir Ariel*

Main category: cs.CR

TL;DR: 提出一种基于高内存掩蔽卷积码的新型后量子密码系统，可支持任意明文长度、线性时间解密、每位固定计算开销，解密阶段使用并行Viterbi译码器实现高效实现。通过更高的随机误差注入比块码方法多并引入多项式除法噪声来混淆结构，采用半可逆变换生成密集的随机样生成矩阵以隐藏代数特性，安全性可达到超越经典McEliece系统约2100倍的界限。对比McEliece，提供更强安全性和更高灵活性，且适于实际量子抗性公钥密码系统。


<details>
  <summary>Details</summary>
Motivation: 应对量子计算威胁，需可扩展、效率高且对结构性攻击鲁棒的公钥密码系统。现有基于块码的McEliece等方案在可扩展性、解密效率和安全边界方面有限，需要新的编码结构和变换来提高安全性与实现灵活性。

Method: 提出基于高内存掩蔽卷积码的公钥密码体系；通过高比率随机误差注入和多项式除法添加噪声来混淆底层卷积码结构；使用半可逆变换生成密集、伪随机的生成矩阵以隐藏代数性质，抵御结构性攻击；解密阶段使用大量并行Viterbi译码器以实现线性时间解密和统一按比特成本；支持任意长度明文，解密和加密在硬件/软件中可高效实现。

Result: 密钥生成和加密过程提供高安全边界，解密通过并行Viterbi译码实现线性时间复杂度且具备可扩展性，理论安全边界据称超过McEliece约2100倍，且对长明文具备可扩展性与统一按比特成本。

Conclusion: 该方案在理论上为量子抗性公钥密码系统提供一个具有更强安全容忍度和灵活性的候选方案，特别是针对长消息的高效解密与实现可行性方面表现突出，值得进一步的安全分析与原型实现评估。

Abstract: This paper presents a novel post-quantum cryptosystem based on high-memory
masked convolutional codes. Unlike conventional code-based schemes that rely on
block codes with fixed dimensions and limited error-correction capability, our
construction offers both stronger cryptographic security and greater
flexibility. It supports arbitrary plaintext lengths with linear-time
decryption and uniform per-bit computational cost, enabling seamless
scalability to long messages. Security is reinforced through a higher-rate
injection of random errors than in block-code approaches, along with additional
noise introduced via polynomial division, which substantially obfuscates the
underlying code structure. Semi-invertible transformations generate dense,
random-like generator matrices that conceal algebraic properties and resist
known structural attacks. Consequently, the scheme achieves cryptanalytic
security margins exceeding those of the classic McEliece system by factors
greater than 2100. Finally, decryption at the recipient employs an array of
parallel Viterbi decoders, enabling efficient hardware and software
implementation and positioning the scheme as a strong candidate for deployment
in practical quantum-resistant public-key cryptosystems.

</details>


### [35] [MalCVE: Malware Detection and CVE Association Using Large Language Models](https://arxiv.org/abs/2510.15567)
*Eduard Andrei Cristea,Petter Molnes,Jingyue Li*

Main category: cs.CR

TL;DR: 提出 MalCVE：一个基于大语言模型的系统，用于在 JAR 二进制文件中检测恶意软件并通过检索增强生成（RAG）将潜在利用的 CVE 与恶意软件关联起来。 在 3,839 个 JAR 实例上实现了 97% 的检测准确率，且比商用方案成本低；具备 recall@10=65% 的能力，达到对源代码分析相关研究的可比水平。


<details>
  <summary>Details</summary>
Motivation: 商业恶意软件检测成本高且缺乏将恶意软件与其利用的具体漏洞（CVEs）直接关联的工具。理解恶意软件与目标漏洞的关系对分析历史威胁和主动防御至关重要。

Method: 构建了 MalCVE 原型，集成二进制代码反编译、去混淆、基于 LLM 的代码摘要、语义相似性检索，以及使用 LLM 进行 CVE 分类。通过检索增强生成（RAG）实现 CVE- malware 的关联推断。

Result: 在 3,839 个 JAR 可执行文件的基准数据集上，MalCVE 实现了平均 malware 检测准确率 97%，成本远低于商业解决方案；并具备 recall@10=65%，与在源码级别进行类似分析的研究相当。

Conclusion: 首次将 CVE 与二进制恶意软件进行关联的工具，展示了将 LLM 与多阶段分析结合用于跨二进制威胁情报的潜力。

Abstract: Malicious software attacks are having an increasingly significant economic
impact. Commercial malware detection software can be costly, and tools that
attribute malware to the specific software vulnerabilities it exploits are
largely lacking. Understanding the connection between malware and the
vulnerabilities it targets is crucial for analyzing past threats and
proactively defending against current ones. In this study, we propose an
approach that leverages large language models (LLMs) to detect binary malware,
specifically within JAR files, and utilizes the capabilities of LLMs combined
with retrieval-augmented generation (RAG) to identify Common Vulnerabilities
and Exposures (CVEs) that malware may exploit. We developed a proof-of-concept
tool called MalCVE, which integrates binary code decompilation, deobfuscation,
LLM-based code summarization, semantic similarity search, and CVE
classification using LLMs. We evaluated MalCVE using a benchmark dataset of
3,839 JAR executables. MalCVE achieved a mean malware detection accuracy of
97%, at a fraction of the cost of commercial solutions. It is also the first
tool to associate CVEs with binary malware, achieving a recall@10 of 65%, which
is comparable to studies that perform similar analyses on source code.

</details>


### [36] [Towards Proactive Defense Against Cyber Cognitive Attacks](https://arxiv.org/abs/2510.15801)
*Bonnie Rushing,Mac-Rufus Umeokolo,Shouhuai Xu*

Main category: cs.CR

TL;DR: 提出一个用于预测性预测方法，预测新兴 disruptive innovations（DIs）及其在认知攻击中的恶意用途，并据此制定前瞻防御策略。


<details>
  <summary>Details</summary>
Motivation: Cyber cognitive attacks 利用 DIs 与 AI 驱动的信息污染，规模和复杂性不断提升；现有研究缺乏预测未来 DIs 的机制及其在认知攻击中的潜在使用，亟需一个预测框架以实现前瞻防御。

Method: 提出一种新颖的预测方法学，用于预测 DIs 的出现及其在认知攻击中的恶意用途；通过分析对手策略趋势，建立预警与防御路径。

Result: 识别出对手战术趋势，提出前瞻性防御策略，提供预测框架的初步结果与应用路线。

Conclusion: 本研究引入的预测方法学可填补现有文献的空白，帮助提前识别和应对未来的 DI 及其在认知攻击中的应用。

Abstract: Cyber cognitive attacks leverage disruptive innovations (DIs) to exploit
psychological biases and manipulate decision-making processes. Emerging
technologies, such as AI-driven disinformation and synthetic media, have
accelerated the scale and sophistication of these threats. Prior studies
primarily categorize current cognitive attack tactics, lacking predictive
mechanisms to anticipate future DIs and their malicious use in cognitive
attacks. This paper addresses these gaps by introducing a novel predictive
methodology for forecasting the emergence of DIs and their malicious uses in
cognitive attacks. We identify trends in adversarial tactics and propose
proactive defense strategies.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [37] [Adaptive Base Representation Theorem: An Alternative to Binary Number System](https://arxiv.org/abs/2510.15099)
*Ravin Kumar*

Main category: cs.IT

TL;DR: ABR theorem introduces Adaptive Base Representation, a new number system that represents numbers with the same bit-length as binary, enabling unique representations and the same integer range. It claims compatibility with standard data compression and error-correction methods and explores applications like digital steganography, suggesting potential for new computational designs.


<details>
  <summary>Details</summary>
Motivation: To move beyond binary-centric digital systems by proposing an alternative base representation that preserves representational capacity while potentially interfacing smoothly with compression and ECC techniques, enabling new data-encoding and computational design possibilities.

Method: Develop the ABR theorem and mathematical formulations that define the ABR encoding rules. Prove that ABR can encode the same integer range as binary using n bits and that representations are unique. Demonstrate compatibility with Huffman coding, arithmetic coding, and Hamming codes, and discuss implementation considerations. Explore practical applications such as digital steganography to illustrate utility.

Result: ABR provides a theoretical framework in which numbers are represented uniquely with n bits, maintaining the same integer range as binary. It is shown to be compatible with conventional compression (Huffman, arithmetic coding) and error-detection/correction (Hamming codes) methods. The work also discusses practical applications, notably digital steganography, to illustrate ABR’s potential utility in information theory and digital encoding.

Conclusion: ABR represents a viable alternative to binary that could inspire new approaches to digital data representation and computational design, motivating further research and practical exploration of ABR’s properties and applications.

Abstract: This paper introduces the Adaptive Base Representation (ABR) Theorem and
proposes a novel number system that offers a structured alternative to the
binary number system for digital computers. The ABR number system enables each
decimal number to be represented uniquely and using the same number of bits,
$n$, as the binary encoding. Theoretical foundations and mathematical
formulations demonstrate that ABR can encode the same integer range as binary,
validating its potential as a viable alternative. Additionally, the ABR number
system is compatible with existing data compression algorithms like Huffman
coding and arithmetic coding, as well as error detection and correction
mechanisms such as Hamming codes. We further explore practical applications,
including digital steganography, to illustrate the utility of ABR in
information theory and digital encoding, suggesting that the ABR number system
could inspire new approaches in digital data representation and computational
design.

</details>


### [38] [Outage-Aware Sum Rate Maximization in Movable Antennas-Enabled Systems](https://arxiv.org/abs/2510.15292)
*Guojie Hu,Qingqing Wu,Ming-Min Zhao,Wen Chen,Zhenyu Xiao,Kui Xu,Jiangbo Si*

Main category: cs.IT

TL;DR: 提出了一种基于统计CSI的可移动天线（MA）MISO系统的 outage-aware 求和速率优化框架，通过在基站端采用统计CSI零迫法进行波束赋形，并以Laguerre级数近似推导SINR的CDF，随后用投影梯度上升法（PGA）优化天线位置以最大化目标。


<details>
  <summary>Details</summary>
Motivation: 在惰性训练信号导致的延迟约束下，使系统仅依赖统计CSI进行传输以控制吞吐-可靠性之间的权衡，提升对多用户的 outage-aware 性能。

Method: 以统计CSI下的零迫波束赋形为核心，建立SINR的均值和方差的紧凑表达，进而使用Laguerre级数近似推导SINR的CDF，给出 outage-aware 总速率表达；对天线位置的非凸优化问题使用投影梯度上升法迭代求解。

Result: 给出闭式且紧致的SINR CDF近似以及 outage-aware 总速率公式，并通过数值结果证实与固定定位天线及其他基准相比具有明显性能提升。

Conclusion: 在不牺牲性能的前提下，通过统计CSI与MA的结合，成功实现对天线位置信息的可迭代优化，提升了延迟敏感场景下的系统吞吐-可靠性权衡。

Abstract: In this paper, we investigate the movable antennas (MAs)-enabled
multiple-input-single-output (MISO) systems, where the base station (BS)
equipped with multiple MAs serves multiple single-antenna user. The
delay-sensitive scenario is considered, where users refrain from periodically
sending training signals to the BS for channel estimations to avoid additional
latency. As a result, the BS relies solely on the statistical channel state
information (CSI) to transmit data with a fixed rate. Under this setup, we aim
to maximize the outage-aware sum rate of all users, by jointly optimizing
antenna positions and the transmit beamforming at the BS, while satisfying the
given target outage probability requirement at each user. The problem is highly
non-convex, primarily because the exact cumulative distribution function (CDF)
of the received signal-to-interference-plus-noise ratio (SINR) of each user is
difficult to derive. To simplify analysis and without comprising performance,
we adopt the statistical CSI based zero-forcing beamforming design. We then
introduce one important lemma to derive the tight mean and variance of the
SINR. Leveraging these results, we further exploit the Laguerre series
approximation to successfully derive the closedform and tight CDF of the SINR.
Subsequently, the outageaware sum rate expression is presented but still
includes complex structure with respect to antenna positions. Facing this
challenge, the projected gradient ascent (PGA) method is developed to
iteratively update antenna positions until convergence. Numerical results
demonstrate the effectiveness of our proposed schemes compared to conventional
fixed-position antenna (FPA) and other competitive benchmarks.

</details>


### [39] [Rotatable Antenna Meets UAV: Towards Dual-Level Channel Reconfiguration Paradigm for ISAC](https://arxiv.org/abs/2510.15295)
*Shiying Chen,Guangji Chen,Long Shi,Qingqing Wu,Kang Wei*

Main category: cs.IT

TL;DR: 提出一个双层通道重配置框架，在UAV上通过可旋转天线与轨迹控制来主动调控S&C通道的大尺度损失与相关性，以权衡感测和通信（ISAC）性能，最大化通信速率并满足感测要求。静态UAV下给出基于子空间相关系数的闭式解；移动UAV下证明最优轨迹为先悬停、飞行、再悬停（HFH）结构，给出全局最优解；仿真显示优于基准方案的权衡区域。


<details>
  <summary>Details</summary>
Motivation: ISAC需共享硬件与无线资源，如何在S&C之间取得有意义的权衡是关键挑战。现有方法往往只能分别优化或未充分利用通道相关性与大尺度衰落的可控性。引入可旋转天线和UAV轨迹优化，为主动调控S&C通道的双层重配置提供新的设计思路。

Method: 提出一个双层通道重配置框架，在UAV上部署可旋转天线；通过优化旋转角度、发射波束成形与UAV轨迹来实现S&C权衡。静态UAV场景使用子空间相关系数导出RA旋转、波束成形、悬停位置的闭式解；移动UAV场景证明HFH轨迹结构可获得全局最优解。

Result: 仿真显示所提设计能显著提升S&C权衡区域，相比基准方案在同等感测约束下获得更高的通信速率或在相同通信性能下有更强的感测能力。

Conclusion: 双层通道重配置为ISAC提供了一种有效的资源与通道调控手段。静态场景下可解析解，移动场景下HFH轨迹给出全局最优解，具有现实意义和理论价值。

Abstract: Integrated sensing and communication (ISAC) is viewed as a key enabler for
future wireless networks by sharing the hardware and wireless resources between
the functionalities of sensing and communication (S&C). Due to the shared
wireless resources for both S&C, it is challenging to achieve a critical
trade-off between these two integrated functionalities. To address this issue,
this paper proposes a novel dual-level channel reconfiguration framework for
ISAC by deploying rotatable antennas at an unmanned aerial vehicle (UAV), where
both the large-scale path loss and the correlation of S&C channels can be
proactively controlled, thereby allowing a flexible trade-off between S&C
performance. To characterize the S&C tradeoff, we aim to maximize the
communication rate by jointly optimizing the RA rotation, the transmit
beamforming, and the UAV trajectory, subject to the given requirement of
sensing performance. For the typical scenario of static UAV deployment, we
introduce the concept of subspace correlation coefficient to derive closed-form
solutions for the optimal RA rotation, transmit beamforming, and UAV hovering
location. For the scenario of a fully mobile UAV, we prove that the optimal
trajectory of a UAV follows a hover-fly-hover (HFH) structure, thereby
obtaining its global optimal solution. Simulation results show that the
proposed design significantly improves the achievable S&C trade-off region
compared to benchmark schemes.

</details>


### [40] [Subverting Flexible Multiuser Communications via Movable Antenna-Enabled Jammer](https://arxiv.org/abs/2510.15298)
*Guojie Hu,Qingqing Wu,Lipeng Zhu,Kui Xu,Guoxin Li,Jiangbo Si,Jian Ouyang,Tong-Xing Zheng*

Main category: cs.IT

TL;DR: MA-enabled legitimate jammer jointly optimizes movable antenna positions and jamming beamforming to minimize the capacity of suspicious multiuser downlink under the ST's reactive power allocation; alternating optimization schemes are proposed; includes two-SR analysis and ideal deployment insights; outperforms fixed-position antennas.


<details>
  <summary>Details</summary>
Motivation: From a security standpoint, movable antennas can reconfigure channels to degrade suspicious communications. The MAJ must counteract the ST's adaptive power allocation, leading to a challenging bi-agent interaction that motivates robust joint design.

Method: Formulate the problem where an MA-enabled jammer adjusts antenna positions and designs jamming beams to minimize a chosen metric (sum rate or min rate) of SRs. Analyze two scenarios by deriving the ST's best response to MAJ actions, then develop two simplified problems and solve via alternating optimization. Investigate the two-SR case for deployment insights and derive an ideal deployment scheme to achieve a global lower bound. Provide numerical comparisons against fixed-position antennas and benchmarks.

Result: Proposed alternating-optimization algorithms effectively reduce the performance of the suspicious downlink and demonstrate gains over conventional fixed-position antennas and other benchmarks. The two-SR analysis yields design insights, and the ideal deployment scheme provides a global performance lower bound.

Conclusion: Movable-antenna-enabled legitimate jammers are effective for degrading suspicious downlink communications. The proposed AO algorithms yield practical designs and reveal deployment guidelines, with special-case results offering deeper intuition and a path toward global optimum under ideal deployment.

Abstract: Movable antenna (MA) is an emerging technology which can reconfigure wireless
channels via adaptive antenna position adjustments at transceivers, thereby
bringing additional spatial degrees of freedom for improving system
performance. In this paper, from a security perspective, we exploit the
MAenabled legitimate jammer (MAJ) to subvert suspicious multiuser downlink
communications consisting of one suspicious transmitter (ST) and multiple
suspicious receivers (SRs). Specifically, our objective is to minimize the
benefit (the sum rate of all SRs or the minimum rate among all SRs) of such
suspicious communications, by jointly optimizing antenna positions and the
jamming beamforming at the MAJ. However, the key challenge lies in that given
the MAJ's actions, the ST can reactively adjust its power allocations to
instead maximize its benefit for mitigating the unfavorable interference. Such
flexible behavior of the ST confuses the optimization design of the MAJ to a
certain extent. Facing this difficulty, corresponding to the above two
different benefits: i) we respectively determine the optimal behavior of the ST
given the MAJ's actions; ii) armed with these, we arrive at two simplified
problems and then develop effective alternating optimization based algorithms
to iteratively solve them. In addition to these, we also focus on the special
case of two SRs, and reveal insightful conclusions about the deployment rule of
antenna positions at the MAJ. Furthermore, we analyze the ideal antenna
deployment scheme at the MAJ for achieving the globally performance lower
bound. Numerical results demonstrate the effectiveness of our proposed schemes
compared to conventional fixed-position antenna (FPA) and other competitive
benchmarks.

</details>


### [41] [New generalizations of circular complex fuzzy sets and Gaussian weighted aggregation operators](https://arxiv.org/abs/2510.15605)
*Yelda Gülfırat,Mehmet Ünver*

Main category: cs.IT

TL;DR: 提出并统一的圆周复杂性 q 阶罗普对模糊集 CC$q$-ROFS，结合 CCIFSs 与复杂 q-rung OFS，提供高斯基聚合算子以改进不确定性表示和决策过程。


<details>
  <summary>Details</summary>
Motivation: 解决模糊集表示中的不确定性与信息整合问题，统一圆周复杂集合家族并引入高斯型聚合以提升光滑性与统计意义。

Method: 形式化 CC$q$-ROFS 的定义；讨论 q=2 时的圆周复杂中庸 Pythagorean 模糊集和 q=3 时的圆周复杂 Fermatean 模糊集；在此框架下发展基于高斯 triangular norm/conorm 的聚合算子，以及高斯加权算子（算术与几何）以整合隶属度与非隶属度。

Result: 提出新的高斯-基聚合算子及其性质的理论构造，给出用于模糊建模与决策的统一工具；为后续的应用研究奠定基础。

Conclusion: CC$q$-ROFS 提供一个统一而灵活的框架，可对不确定性进行平滑而统计意义良好的表示，并通过高斯聚合算子提升决策任务的鲁棒性和可解释性。

Abstract: In this paper, we introduce the concept of the circular complex $q$-rung
orthopair fuzzy set (CC$q$-ROFS) as a novel generalization that unifies the
existing frameworks of circular complex intuitionistic fuzzy sets (CCIFSs) and
complex $q$-rung orthopair fuzzy sets. If $q = 2$, the structure is referred to
as a circular complex Pythagorean fuzzy set, and if $q = 3$, it is called a
circular complex Fermatean fuzzy set. The proposed approach extends the
Gaussian-based framework to the CC$q$-ROFSs, aiming to achieve a smoother and
statistically meaningful representation of uncertainty. Within this setting,
new Gaussian-based aggregation operators for CC$q$-ROFSs are constructed by
employing the Gaussian triangular norm and conorm. Furthermore,
Gaussian-weighted arithmetic and Gaussian-weighted geometric aggregation
operators are formulated to enable consistent integration of membership and
non-membership information for fuzzy modeling and decision-making.

</details>


### [42] [Beyond-Diagonal RIS Under Non-Idealities: Learning-Based Architecture Discovery and Optimization](https://arxiv.org/abs/2510.15701)
*Binggui Zhou,Bruno Clerckx*

Main category: cs.IT

TL;DR: 提出了一种基于学习的两层架构发现框架（LTTADF），在给定电路复杂度约束下为非理想BD-RIS寻找最优或近似最优架构，以平衡性能与复杂度。


<details>
  <summary>Details</summary>
Motivation: 非理想BD-RIS引入了性能与电路复杂度之间的权衡；全局架构搜索计算复杂且易陷入局部最优，缺乏针对非理想性的系统性发现方法。

Method: 提出学习驱动的两层架构发现框架：架构生成器用于在大规模BD-RIS架构空间中生成候选架构，性能优化器在给定复杂度约束下对候选架构进行性能优化。两者协同工作以避免局部最优并实现接近全局最优的性能。

Result: 数值结果表明，LTTADF在给定电路复杂度约束下可实现接近最优的性能，同时揭示非理想BD-RIS部署中性能与复杂度之间的权衡规律。

Conclusion: LTTADF为非理想BD-RIS的架构发现提供了有效途径，有助于在实际系统中实现性能和电路复杂度之间的良好折衷；未来工作可进一步提升训练效率并扩展对其他非理想因素的覆盖。

Abstract: Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has recently been
introduced to enable advanced control over electromagnetic waves to further
increase the benefits of traditional RIS in enhancing signal quality and
improving spectral and energy efficiency for next-generation wireless networks.
A significant issue in designing and deploying BD-RIS is the tradeoff between
its performance and circuit complexity. Despite some efforts in exploring
optimal architectures with the lowest circuit complexities for ideal BD-RIS,
architecture discovery for non-ideal BD-RIS remains uninvestigated. Therefore,
how non-idealities and circuit complexity jointly affect the performance of
BD-RIS remains unclear, making it difficult to achieve the performance -
circuit complexity tradeoff in the presence of non-idealities. Essentially,
architecture discovery for non-ideal BD-RIS faces challenges from both the
computational complexity of global architecture search and the difficulty in
achieving global optima. To tackle these challenges, we propose a
learning-based two-tier architecture discovery framework (LTTADF) consisting of
an architecture generator and a performance optimizer to jointly discover
optimal architectures of non-ideal BD-RIS given specific circuit complexities,
which can effectively explore over a large architecture space while avoiding
getting trapped in poor local optima and thus achieving near-optimal solutions
for the performance optimization. Numerical results provide valuable insights
for deploying non-ideal BD-RIS considering the performance - circuit complexity
tradeoff.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Extending Load Forecasting from Zonal Aggregates to Individual Nodes for Transmission System Operators](https://arxiv.org/abs/2510.14983)
*Oskar Triebe,Fletcher Passow,Simon Wittner,Leonie Wagner,Julio Arend,Tao Sun,Chad Zanocco,Marek Miltner,Arezou Ghesmati,Chen-Hao Tsai,Christoph Bergmeir,Ram Rajagopal*

Main category: cs.LG

TL;DR: 多层级、高可解释性且可扩展的节点负荷预测框架，帮助能源系统运营商从区域预测逐步扩展到节点级预测，并通过并行化工作流提升效率与可管理性。


<details>
  <summary>Details</summary>
Motivation: 可持续能源发展增加了负荷不确定性，需更高空间分辨率的负荷预测以支持传输系统运营商(tsos)的日常运维。节点级预测比区域预测常常更不准确且更难管理，需为运维人员提供可解释且高效的诊断与控制工具。

Method: 提出一个可解释且可扩展的预测模型，支持区域到节点的逐步扩展；在一个由区域与节点负荷组成的独特数据集上评估模型组件；并实现一个完全并行化的单模型预测工作流；同时评估处理节点负荷的异质性与波动性的方法，并在不牺牲可管理性的前提下权衡相关 trade-off。

Result: 对区域预测显示出准确性和可解释性方面的提升；对节点预测则表现出显著改进。

Conclusion: 多层级预测系统使运维人员能够以前所未有的信心与准确性调整预测，并对误差进行精确的诊断。该系统在实践中提升了对区域到节点的逐级扩展能力与可操作性。

Abstract: The reliability of local power grid infrastructure is challenged by
sustainable energy developments increasing electric load uncertainty.
Transmission System Operators (TSOs) need load forecasts of higher spatial
resolution, extending current forecasting operations from zonal aggregates to
individual nodes. However, nodal loads are less accurate to forecast and
require a large number of individual forecasts, which are hard to manage for
the human experts assessing risks in the control room's daily operations
(operator). In collaboration with a TSO, we design a multi-level system that
meets the needs of operators for hourly day-ahead load forecasting. Utilizing a
uniquely extensive dataset of zonal and nodal net loads, we experimentally
evaluate our system components. First, we develop an interpretable and scalable
forecasting model that allows for TSOs to gradually extend zonal operations to
include nodal forecasts. Second, we evaluate solutions to address the
heterogeneity and volatility of nodal load, subject to a trade-off. Third, our
system is manageable with a fully parallelized single-model forecasting
workflow. Our results show accuracy and interpretability improvements for zonal
forecasts, and substantial improvements for nodal forecasts. In practice, our
multi-level forecasting system allows operators to adjust forecasts with
unprecedented confidence and accuracy, and to diagnose otherwise opaque errors
precisely.

</details>


### [44] [TangledFeatures: Robust Feature Selection in Highly Correlated Spaces](https://arxiv.org/abs/2510.15005)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: 提出 TangledFeatures 框架，用于在相关特征空间中进行特征选择，识别一组 entangled predictors 的代表性特征，减少冗余并保留解释能力；可直接用于下游模型，提升可解释性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择多关注预测准确性，在存在相关特征时容易产生冗余和不稳定性，缺乏可解释、稳定的特征子集。

Method: TangledFeatures 通过从相关联的特征组中识别代表性特征来降低冗余，同时保持解释力，得到可直接用于后续模型的子集。该框架旨在在相关特征空间中改进可解释性和稳定性。

Result: 在 Alanine Dipeptide 的应用中，所选特征对应结构意义显著的原子内距离，能够解释骨架扭转角的变化，显示子集具有良好的解释性。

Conclusion: TangledFeatures 提供比传统选择方法更具可解释性和稳定性的分析基础，具有广泛适用性。

Abstract: Feature selection is a fundamental step in model development, shaping both
predictive performance and interpretability. Yet, most widely used methods
focus on predictive accuracy, and their performance degrades in the presence of
correlated predictors. To address this gap, we introduce TangledFeatures, a
framework for feature selection in correlated feature spaces. It identifies
representative features from groups of entangled predictors, reducing
redundancy while retaining explanatory power. The resulting feature subset can
be directly applied in downstream models, offering a more interpretable and
stable basis for analysis compared to traditional selection techniques. We
demonstrate the effectiveness of TangledFeatures on Alanine Dipeptide, applying
it to the prediction of backbone torsional angles and show that the selected
features correspond to structurally meaningful intra-atomic distances that
explain variation in these angles.

</details>


### [45] [Hybrid Autoencoder-Based Framework for Early Fault Detection in Wind Turbines](https://arxiv.org/abs/2510.15010)
*Rekha R Nair,Tina Babu,Alavikunhu Panthakkan,Balamurugan Balusamy,Wathiq Mansoor*

Main category: cs.LG

TL;DR: 提出一种基于集成深度学习的无监督异常检测框架，用于风力涡轮机的早期故障检测，结合VAE、LSTM自编码器和Transformer，对高维SCADA数据进行特征工程与自适应阈值检测，性能在CARE数据集上达到0.947的AUC-ROC，并实现故障前48小时检测。


<details>
  <summary>Details</summary>
Motivation: 风机的可靠性对可再生能源至关重要，早期故障检测可显著降低停机时间和维护成本；在缺乏标记故障数据的场景下，提出无监督的异常检测方法。

Method: 将VAE、LSTM自编码器和Transformer三种模型进行集成，以捕捉不同的时间和上下文模式；设计特征工程管线，提取时间、统计和频域指标；进行集成打分并采用自适应阈值来检测异常。

Result: 在CARE数据集（89年数据，来自三座风场）上评估，AUC-ROC达到0.947，能够在故障发生前48小时内进行预警。

Conclusion: 该方法对社会有显著价值，推动预测性维护，减少涡轮故障，提升大规模风电部署的运营效率，具有良好的可扩展性。

Abstract: Wind turbine reliability is critical to the growing renewable energy sector,
where early fault detection significantly reduces downtime and maintenance
costs. This paper introduces a novel ensemble-based deep learning framework for
unsupervised anomaly detection in wind turbines. The method integrates
Variational Autoencoders (VAE), LSTM Autoencoders, and Transformer
architectures, each capturing different temporal and contextual patterns from
high-dimensional SCADA data. A unique feature engineering pipeline extracts
temporal, statistical, and frequency-domain indicators, which are then
processed by the deep models. Ensemble scoring combines model predictions,
followed by adaptive thresholding to detect operational anomalies without
requiring labeled fault data. Evaluated on the CARE dataset containing 89 years
of real-world turbine data across three wind farms, the proposed method
achieves an AUC-ROC of 0.947 and early fault detection up to 48 hours prior to
failure. This approach offers significant societal value by enabling predictive
maintenance, reducing turbine failures, and enhancing operational efficiency in
large-scale wind energy deployments.

</details>


### [46] [AlignFlow: Improving Flow-based Generative Models with Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.15038)
*Lingkai Kong,Molei Tao,Yang Liu,Bryan Wang,Jinmiao Fu,Chien-Chih Wang,Huidong Liu*

Main category: cs.LG

TL;DR: AlignFlow uses Semi-Discrete Optimal Transport (SDOT) to create an explicit, optimal alignment between a noise distribution and data points in flow-based generative models (FGMs), enabling scalable, plug-and-play training with guaranteed convergence.


<details>
  <summary>Details</summary>
Motivation: OT-based coupling between noise and data improves the trajectory straightness of FGMs and inference but existing methods rely on mini-batch estimates that limit scalability to large/high-dimensional datasets.

Method: Introduce SDOT-based alignment that partitions the noise space into Laguerre cells, each mapped to a data point. During training, i.i.d. noise samples are paired with data via the SDOT map, providing an explicit transport plan. This approach is designed to be scalable with negligible overhead and can be integrated as a plug-and-play component.

Result: AlignFlow improves the performance of a wide range of state-of-the-art FGMs and scales to large datasets and model architectures with negligible computational overhead; code is released for reproducibility.

Conclusion: SDOT-based alignment offers a convergent, scalable OT coupling mechanism for FGMs, enabling better utilization of noise-data relationships and providing a practical, plug-and-play enhancement to existing flow-based generative modeling approaches.

Abstract: Flow-based Generative Models (FGMs) effectively transform noise into complex
data distributions. Incorporating Optimal Transport (OT) to couple noise and
data during FGM training has been shown to improve the straightness of flow
trajectories, enabling more effective inference. However, existing OT-based
methods estimate the OT plan using (mini-)batches of sampled noise and data
points, which limits their scalability to large and high-dimensional datasets
in FGMs. This paper introduces AlignFlow, a novel approach that leverages
Semi-Discrete Optimal Transport (SDOT) to enhance the training of FGMs by
establishing an explicit, optimal alignment between noise distribution and data
points with guaranteed convergence. SDOT computes a transport map by
partitioning the noise space into Laguerre cells, each mapped to a
corresponding data point. During FGM training, i.i.d. noise samples are paired
with data points via the SDOT map. AlignFlow scales well to large datasets and
model architectures with negligible computational overhead. Experimental
results show that AlignFlow improves the performance of a wide range of
state-of-the-art FGM algorithms and can be integrated as a plug-and-play
component. Code is available at: https://github.com/konglk1203/AlignFlow.

</details>


### [47] [Physics-informed data-driven machine health monitoring for two-photon lithography](https://arxiv.org/abs/2510.15075)
*Sixian Jia,Zhiqiao Dong,Chenhui Shao*

Main category: cs.LG

TL;DR: 提出三种方法用于对光子两光子光刻（TPL）系统状态进行准确且及时的预测性维护，通过将物理信息驱动的数据驱动模型与统计方法结合，利用六组工艺参数和六种结构尺寸在两种机器健康条件下的实验数据，实现高准确性、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: TPL系统的维护常依赖经验，导致停机或过度维护，亟需基于健康状态的维护策略来提高可靠性与效率。

Method: 提出三种方法，将物理信息驱动的预测模型与统计方法结合，用于对结构尺寸和机器健康状态进行预测与监控。通过包含六组工艺参数、六种结构尺寸，在两种健康条件下的实验数据进行训练与评估。

Result: 在所有测试情境中，这三种方法均实现高准确性，显示出良好的有效性、鲁棒性与泛化能力。

Conclusion: 这一工作为TPL系统的条件基维护提供重要进展，推动实现更高效、可靠的制造与维护实践。

Abstract: Two-photon lithography (TPL) is a sophisticated additive manufacturing
technology for creating three-dimensional (3D) micro- and nano-structures.
Maintaining the health of TPL systems is critical for ensuring consistent
fabrication quality. Current maintenance practices often rely on experience
rather than informed monitoring of machine health, resulting in either untimely
maintenance that causes machine downtime and poor-quality fabrication, or
unnecessary maintenance that leads to inefficiencies and avoidable downtime. To
address this gap, this paper presents three methods for accurate and timely
monitoring of TPL machine health. Through integrating physics-informed
data-driven predictive models for structure dimensions with statistical
approaches, the proposed methods are able to handle increasingly complex
scenarios featuring different levels of generalizability. A comprehensive
experimental dataset that encompasses six process parameter combinations and
six structure dimensions under two machine health conditions was collected to
evaluate the effectiveness of the proposed approaches. Across all test
scenarios, the approaches are shown to achieve high accuracies, demonstrating
excellent effectiveness, robustness, and generalizability. These results
represent a significant step toward condition-based maintenance for TPL
systems.

</details>


### [48] [Online Correlation Clustering: Simultaneously Optimizing All $\ell_p$-norms](https://arxiv.org/abs/2510.15076)
*Sami Davies,Benjamin Moseley,Heather Newman*

Main category: cs.LG

TL;DR: 在线-有样本(AOS)模型下，单一算法实现对所有ℓ_p范数的同时近似，竞争比为O(log^4 n)（高概率），ℓ_∞为O(log n)（高概率），ℓ_1在期望下为O(1)。在随机序(RO)模型中存在分离：ℓ_1可常数近似而ℓ_∞的下界≥Ω(n^{1/3})，表明需要跨越标准无偏模型的努力。AOS下的下界与上界对ℓ_1与ℓ_∞接近紧。


<details>
  <summary>Details</summary>
Motivation: 在相关聚类中，最小化总不一致（ℓ_1）与对个体节点的公平性（ℓ_∞）之间存在权衡。离线情形已具备“全范数”近似；在线场景中的同范数统一近似仍然未知且具有挑战性。

Method: 给出一个在在线-有样本(AOS)模型中的单一算法。通过样本信息引导聚类决策，统一分析所有ℓ_p范数的竞争比；对ℓ_∞给出高概率对数n的竞争比、对ℓ_1给出期望常数竞争比。还给出RO模型的下界以呈现差异，并在AOS下给出ℓ_1与ℓ_∞的近似下界，表现出接近紧。

Result: 算法在AOS模型下实现对所有ℓ_p范数的O(log^4 n)高概率近似，对ℓ_∞为O(log n)高概率近似，对ℓ_1在期望下为O(1)近似。RO模型中ℓ_1为常数近似，而ℓ_∞的竞争比至少Ω(n^{1/3})，揭示RO模型的局限。AOS下的下界对ℓ_1与ℓ_∞接近紧。

Conclusion: 将离线的全范数保障扩展至在线场景，且在AOS模型下实现统一的所有范数近似，并明确了RO模型的分离性及AOS下的紧密性边界。

Abstract: The $\ell_p$-norm objectives for correlation clustering present a fundamental
trade-off between minimizing total disagreements (the $\ell_1$-norm) and
ensuring fairness to individual nodes (the $\ell_\infty$-norm). Surprisingly,
in the offline setting it is possible to simultaneously approximate all
$\ell_p$-norms with a single clustering. Can this powerful guarantee be
achieved in an online setting? This paper provides the first affirmative
answer. We present a single algorithm for the online-with-a-sample (AOS) model
that, given a small constant fraction of the input as a sample, produces one
clustering that is simultaneously $O(\log^4 n)$-competitive for all
$\ell_p$-norms with high probability, $O(\log n)$-competitive for the
$\ell_\infty$-norm with high probability, and $O(1)$-competitive for the
$\ell_1$-norm in expectation. This work successfully translates the offline
"all-norms" guarantee to the online world.
  Our setting is motivated by a new hardness result that demonstrates a
fundamental separation between these objectives in the standard random-order
(RO) online model. Namely, while the $\ell_1$-norm is trivially
$O(1)$-approximable in the RO model, we prove that any algorithm in the RO
model for the fairness-promoting $\ell_\infty$-norm must have a competitive
ratio of at least $\Omega(n^{1/3})$. This highlights the necessity of a
different beyond-worst-case model. We complement our algorithm with lower
bounds, showing our competitive ratios for the $\ell_1$- and $\ell_\infty$-
norms are nearly tight in the AOS model.

</details>


### [49] [Operator Flow Matching for Timeseries Forecasting](https://arxiv.org/abs/2510.15101)
*Yolanne Yi Ran Lee,Kyriakos Flouris*

Main category: cs.LG

TL;DR: TempO: 基于潜在流匹配的高维PDE预测框架，结合稀疏条件与时间条件傅里叶层，在三维时空场上高效预测，性能优于现有基线且更具物理一致性与多尺度建模能力。


<details>
  <summary>Details</summary>
Motivation: 高维、PDE支配的动力学预测难以避免自回归/扩散模型的累积误差和离散化伪影；需要一个高效、确定性的抽样方法来实现物理上一致的长时预测。

Method: 提出TempO，一种潜在流匹配模型，使用通道折叠实现稀疏条件，处理3D时空场；引入时间条件傅里叶层以捕获多尺度模态；给出FNO近似误差的上界，进行光谱分析；相比注意力或卷积回归器，参数与内存消耗更低。

Result: 在三个PDE数据集上超越最先进基线；光谱分析显示对多尺度动力学的恢复更好；在效率方面展示了更少的参数和内存开销。

Conclusion: TempO为高维PDE的物理一致长时预测提供一种高效、精确的潜在流匹配框架，借助稀疏条件与傅里叶多尺度建模实现优越性能。

Abstract: Forecasting high-dimensional, PDE-governed dynamics remains a core challenge
for generative modeling. Existing autoregressive and diffusion-based approaches
often suffer cumulative errors and discretisation artifacts that limit long,
physically consistent forecasts. Flow matching offers a natural alternative,
enabling efficient, deterministic sampling. We prove an upper bound on FNO
approximation error and propose TempO, a latent flow matching model leveraging
sparse conditioning with channel folding to efficiently process 3D
spatiotemporal fields using time-conditioned Fourier layers to capture
multi-scale modes with high fidelity. TempO outperforms state-of-the-art
baselines across three benchmark PDE datasets, and spectral analysis further
demonstrates superior recovery of multi-scale dynamics, while efficiency
studies highlight its parameter- and memory-light design compared to
attention-based or convolutional regressors.

</details>


### [50] [DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning](https://arxiv.org/abs/2510.15110)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: Length-aware reinforcement learning with a simple truncation penalty (DLER) achieves high accuracy with concise outputs, reducing length by over 70% and improving test-time efficiency; extensions include Difficulty-Aware DLER and update-selective merging for data-scarce settings.


<details>
  <summary>Details</summary>
Motivation: To address the overarching goal of maximizing accuracy per token (intelligence per token) in reasoning LMs, by revisiting RL with a straightforward length penalty and overcoming three key optimization challenges (advantage bias, entropy collapse, sparse rewards).

Method: DLER is a training recipe that combines batch-wise reward normalization, higher value clipping, dynamic sampling, and a simple truncation length penalty. Variants include Difficulty-Aware DLER, which tightens truncation on easier questions, and an update-selective merging method to retain baseline accuracy while preserving concise reasoning when RL data is scarce.

Result: DLER achieves state-of-the-art accuracy-efficiency trade-offs, reducing output length by >70% while surpassing prior baselines in accuracy. At test time, DLER-7B achieves 28% higher accuracy and lower latency than DeepSeek-R1-7B when generating multiple concise responses in parallel. Update-selective merging preserves baseline accuracy while maintaining concise reasoning for data-scarce RL scenarios.

Conclusion: A simple length penalty, when optimized with proper RL techniques, yields strong efficiency gains without sacrificing accuracy. Adaptive truncation and selective merging further boost efficiency and robustness, enabling scalable deployment of concise, high-quality reasoning.

Abstract: Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve
strong performance via extended chains of thought but often generate
unnecessarily long outputs. Maximizing intelligence per token--accuracy
relative to response length--remains an open problem. We revisit reinforcement
learning (RL) with the simplest length penalty--truncation--and show that
accuracy degradation arises not from the lack of sophisticated penalties but
from inadequate RL optimization. We identify three key challenges: (i) large
bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward
signal. We address them with Doing Length pEnalty Right (DLER), a training
recipe combining batch-wise reward normalization, higher clipping, dynamic
sampling, and a simple truncation length penalty. DLER achieves
state-of-the-art accuracy--efficiency trade-offs, cutting output length by over
70 percent while surpassing all previous baseline accuracy. It also improves
test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple
concise responses in parallel with 28 percent higher accuracy and lower
latency. We further introduce Difficulty-Aware DLER, which adaptively tightens
truncation on easier questions for additional efficiency gains. We also propose
an update-selective merging method that preserves baseline accuracy while
retaining the concise reasoning ability of the DLER model, which is useful for
scenarios where RL training data is scarce.

</details>


### [51] [Navigating the consequences of mechanical ventilation in clinical intensive care settings through an evolutionary game-theoretic framework](https://arxiv.org/abs/2510.15127)
*David J. Albers,Tell D. Bennett,Jana de Wiljes,Bradford J. Smith,Peter D. Sottile,J. N. Stroh*

Main category: cs.LG

TL;DR: 提出一个将联合患者-呼吸机-护理系统（J6）概念与进化博弈理论（EGT）结合的可扩展分析框架，用以研究机械通气（MV）策略及护理决策对患者结局的影响；在合成数据上进行分析验证，为后续强化学习与状态转移建模奠定基础。


<details>
  <summary>Details</summary>
Motivation: 在临床决策环境中，需从异质的病人-呼吸机-护理系统数据中辨识MV策略的效果，形成可扩展的分析框架并提出假设以推动MV优化与个性化治疗。利用二次使用的临床数据可生成关于有利变体的假设，但需处理数据复杂性与数据生成过程的不确定性。

Method: 构建J6框架以刻画患者-呼吸机-护理系统的耦合关系；使用进化博弈理论分析呼吸行为，产出用于更深入分析的定量前置量；结合概率与随机方法（如强化学习）进行推理；在合成数据上对EGT过程进行解析验证，揭示真实数据应用前的潜在注意事项；为真实ICU数据应用做准备，讨论将经验数据与博弈论要素结合的状态转移模型。

Result: 基于EGT的分析流程在合成数据上进行了分析验证，揭示在真实数据应用前的潜在 caveats；为后续强化学习与MV优化提供定量前提和洞见，指明从数据中提取MV决策影响路径的可能性。

Conclusion: 这是迈向MV优化与个性化治疗的一步，未来可发展成结合经验数据与博弈论、以及基于状态转移的仿真框架，用以模拟MV决策的效应并推动临床决策支持的发展。

Abstract: Identifying the effects of mechanical ventilation strategies and protocols in
critical care requires analyzing data from heterogeneous patient-ventilator
systems within the context of the clinical decision-making environment. This
research develops a framework to help understand the consequences of mechanical
ventilation (MV) and adjunct care decisions on patient outcome from
observations of critical care patients receiving MV. Developing an
understanding of and improving critical care respiratory management requires
the analysis of existing secondary-use clinical data to generate hypotheses
about advantageous variations and adaptations of current care. This work
introduces a perspective of the joint patient-ventilator-care systems
(so-called J6) to develop a scalable method for analyzing data and trajectories
of these complex systems. To that end, breath behaviors are analyzed using
evolutionary game theory (EGT), which generates the necessary quantitative
precursors for deeper analysis through probabilistic and stochastic machinery
such as reinforcement learning. This result is one step along the pathway
toward MV optimization and personalization. The EGT-based process is
analytically validated on synthetic data to reveal potential caveats before
proceeding to real-world ICU data applications that expose complexities of the
data-generating process J6. The discussion includes potential developments
toward a state transition model for the simulating effects of MV decision using
empirical and game-theoretic elements.

</details>


### [52] [A Simple Method for PMF Estimation on Large Supports](https://arxiv.org/abs/2510.15132)
*Alex Shtoff*

Main category: cs.LG

TL;DR: 通过对概率质量函数的非参数估计，利用在路径图上的低通滤波实现对大型离散支持的平滑估计，保留粗略结构同时抑制噪声；具备数据自适应的维度选择、快速稳定的计算，适用于多模态与厚尾PMF。


<details>
  <summary>Details</summary>
Motivation: 在大型离散支持上对多模态、厚尾的PMF进行非参数估计，需在不依赖强参数化假设的前提下实现高效、鲁棒的平滑，同时保持粗粒结构并具备自动化工作流的可扩展性。

Method: 将经验PMF视作线性图上的信号，构造对称的三对角算子（路径图Laplacian加上由经验PMF构成的对角扰动），求解最小等效值对应的特征向量；将经验PMF投影到该低维子空间，得到平滑的多模态估计；再进行轻量后处理（截断与重新归一化）以得到有效的PMF。维度选取基于正交级数风险估计的自适应数据驱动规则；关于计算，特征分解针对对称三对角矩阵，时间和内存开销与支持大小和所需维度成线性关系。

Result: 得到的估计在保留粗粒结构的同时抑制采样噪声，在厚尾、多峰设置下优于logspline与高斯KDE等基线方法（在目标 regime）。实现简短、对样本量鲁棒，且适合自动化流水线与大规模分析；同时存在已知的失效模式（如突然的不连续性）。

Conclusion: 所提出的谱滤波方法为对厚尾、具有多峰结构的离散PMF提供了一个高效、可扩展且鲁棒的非参数估计方案，几乎无需调参即可“直接工作”；但使用者应注意潜在的不连续性等局限，并在需要时结合其他方法进行补充。

Abstract: We study nonparametric estimation of a probability mass function (PMF) on a
large discrete support, where the PMF is multi-modal and heavy-tailed. The core
idea is to treat the empirical PMF as a signal on a line graph and apply a
data-dependent low-pass filter. Concretely, we form a symmetric tri-diagonal
operator, the path graph Laplacian perturbed with a diagonal matrix built from
the empirical PMF, then compute the eigenvectors, corresponding to the smallest
feq eigenvalues. Projecting the empirical PMF onto this low dimensional
subspace produces a smooth, multi-modal estimate that preserves coarse
structure while suppressing noise. A light post-processing step of clipping and
re-normalizing yields a valid PMF.
  Because we compute the eigenpairs of a symmetric tridiagonal matrix, the
computation is reliable and runs time and memory proportional to the support
times the dimension of the desired low-dimensional supspace. We also provide a
practical, data-driven rule for selecting the dimension based on an
orthogonal-series risk estimate, so the method "just works" with minimal
tuning. On synthetic and real heavy-tailed examples, the approach preserves
coarse structure while suppressing sampling noise, compares favorably to
logspline and Gaussian-KDE baselines in the intended regimes. However, it has
known failure modes (e.g., abrupt discontinuities). The method is short to
implement, robust across sample sizes, and suitable for automated pipelines and
exploratory analysis at scale because of its reliability and speed.

</details>


### [53] [Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization](https://arxiv.org/abs/2510.15165)
*Xin Guo,Zijiu Lyu*

Main category: cs.LG

TL;DR: 验证连续时间强化学习中的策略迁移：在带熵正则的LQR场景下，证明源任务最优策略可作为目标任务的近似最优初始化，同时不降低收敛速率；提出新算法实现全局线性与局部超线性收敛；并将分析扩展到连续时间的扩散模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 提升强化学习在复杂任务中的训练效率，借鉴大语言模型中的迁移学习思路，将策略迁移用于连续时间RL，填补现有文献中连续时间环境下迁移学习的不足。

Method: 给出连续时间RL中策略迁移的理论证明：一个任务的最优策略在相近的目标任务上可作为近似最优初始化，且不改变原有算法的收敛性；提出一种新的连续时间LQR策略学习算法，理论上实现全局线性收敛和局部超线性收敛；通过对分析推导，建立源任务到目标任务的迁移对收敛性与稳定性的影响。

Result: 证明了在相近的LQR情形下，源任务最优策略可作为近似最优初始化且保留原算法的收敛速率；提出的新策略学习算法实现全局线性收敛和局部超线性收敛；并由分析得到连续时间-score-based diffusion模型的稳定性结论。

Conclusion: 本工作首次在连续时间强化学习中给出策略迁移的理论保障，弥补从离散时间到连续时间的空白，理论与算法两方面均显示迁移学习在连续时间RL中的潜在收益。

Abstract: Reinforcement Learning (RL) enables agents to learn optimal decision-making
strategies through interaction with an environment, yet training from scratch
on complex tasks can be highly inefficient. Transfer learning (TL), widely
successful in large language models (LLMs), offers a promising direction for
enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes
learning in a target RL task using a policy from a related source task, in the
context of continuous-time linear quadratic regulators (LQRs) with entropy
regularization. We provide the first theoretical proof of policy transfer for
continuous-time RL, proving that a policy optimal for one LQR serves as a
near-optimal initialization for closely related LQRs, while preserving the
original algorithm's convergence rate. Furthermore, we introduce a novel policy
learning algorithm for continuous-time LQRs that achieves global linear and
local super-linear convergence. Our results demonstrate both theoretical
guarantees and algorithmic benefits of transfer learning in continuous-time RL,
addressing a gap in existing literature and extending prior work from discrete
to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of
continuous-time score-based diffusion models via their connection with LQRs.

</details>


### [54] [A simple mean field model of feature learning](https://arxiv.org/abs/2510.15174)
*Niclas Göring,Chris Mingard,Yoonsoo Nam,Ard Louis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Feature learning (FL), where neural networks adapt their internal
representations during training, remains poorly understood. Using methods from
statistical physics, we derive a tractable, self-consistent mean-field (MF)
theory for the Bayesian posterior of two-layer non-linear networks trained with
stochastic gradient Langevin dynamics (SGLD). At infinite width, this theory
reduces to kernel ridge regression, but at finite width it predicts a symmetry
breaking phase transition where networks abruptly align with target functions.
While the basic MF theory provides theoretical insight into the emergence of FL
in the finite-width regime, semi-quantitatively predicting the onset of FL with
noise or sample size, it substantially underestimates the improvements in
generalisation after the transition. We trace this discrepancy to a key
mechanism absent from the plain MF description: \textit{self-reinforcing input
feature selection}. Incorporating this mechanism into the MF theory allows us
to quantitatively match the learning curves of SGLD-trained networks and
provides mechanistic insight into FL.

</details>


### [55] [An Advanced Two-Stage Model with High Sensitivity and Generalizability for Prediction of Hip Fracture Risk Using Multiple Datasets](https://arxiv.org/abs/2510.15179)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 两阶段临床-影像模型在髋部骨折风险预测中优于单一DXA/FRAX工具。


<details>
  <summary>Details</summary>
Motivation: 早期识别高风险者以预防髋部骨折，克服DXA T-score和FRAX在敏感性方面的不足，尤其是对无既往骨折或骨密度略低人群。

Method: Stage 1：筛查阶段，使用临床、人口统计学和功能变量估计基线风险；Stage 2：影像阶段，整合DXA衍生特征以细化风险。基于MrOS、SOF与UK Biobank数据进行内部和外部验证，评估在不同队列中的稳定性与可迁移性。

Result: 相比T-score和FRAX，该两阶段框架具有更高的灵敏度、减少漏诊，且在多队列中表现一致，具成本效益且可实现个性化风险评估。

Conclusion: 将临床/功能信息与DXA影像特征分阶段整合，可显著提升早期髋部骨折风险的预测准确性与实用性。

Abstract: Hip fractures are a major cause of disability, mortality, and healthcare
burden in older adults, underscoring the need for early risk assessment.
However, commonly used tools such as the DXA T-score and FRAX often lack
sensitivity and miss individuals at high risk, particularly those without prior
fractures or with osteopenia. To address this limitation, we propose a
sequential two-stage model that integrates clinical and imaging information to
improve prediction accuracy. Using data from the Osteoporotic Fractures in Men
Study (MrOS), the Study of Osteoporotic Fractures (SOF), and the UK Biobank,
Stage 1 (Screening) employs clinical, demographic, and functional variables to
estimate baseline risk, while Stage 2 (Imaging) incorporates DXA-derived
features for refinement. The model was rigorously validated through internal
and external testing, showing consistent performance and adaptability across
cohorts. Compared to T-score and FRAX, the two-stage framework achieved higher
sensitivity and reduced missed cases, offering a cost-effective and
personalized approach for early hip fracture risk assessment.
  Keywords: Hip Fracture, Two-Stage Model, Risk Prediction, Sensitivity, DXA,
FRAX

</details>


### [56] [Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection](https://arxiv.org/abs/2510.15202)
*Denis Janiak,Jakub Binkowski,Tomasz Kajdanowicz*

Main category: cs.LG

TL;DR: Mahalanobis-based OOD detectors are not universally reliable. The paper links representation geometry and normalization to OOD performance, identifies ideal geometry via spectral and intrinsic-dimensionality metrics, and introduces radially scaled L2 normalization to control radial geometry, improving OOD detection.


<details>
  <summary>Details</summary>
Motivation: OOD detection is critical for reliable deployment of deep learning models. Existing Mahalanobis-based methods may fail due to overlooked effects of geometry and normalization; a thorough empirical study across models, datasets, and normalization schemes is needed to understand and improve performance.

Method: Conduct a large-scale empirical study across diverse image foundation models, datasets, and distance normalization schemes. Evaluate Mahalanobis-based OOD detectors, analyze data representation geometry using spectral and intrinsic-dimensionality metrics to predict OOD performance, examine normalization effects, and propose radially scaled L2 normalization with a tunable radius parameter to control geometry and improve OOD detection.

Result: Mahalanobis-based methods are not universally reliable for OOD detection. Spectral and intrinsic-dimensionality metrics can predict a model’s OOD performance. Normalization significantly impacts OOD results. The proposed radially scaled L2 normalization (with a tunable parameter) can systematically contract or expand feature space geometry, leading to substantial improvements in OOD detection performance.

Conclusion: Understanding the relationship between representation geometry, normalization, and OOD performance enables more effective and reliable models. Radially scaled L2 normalization offers a principled, adjustable approach to shape feature-space geometry and enhance OOD detection.

Abstract: Out-of-distribution (OOD) detection is critical for the reliable deployment
of deep learning models. hile Mahalanobis distance methods are widely used, the
impact of representation geometry and normalization on their performance is not
fully understood, which may limit their downstream application. To address this
gap, we conducted a comprehensive empirical study across diverse image
foundation models, datasets, and distance normalization schemes. First, our
analysis shows that Mahalanobis-based methods aren't universally reliable.
Second, we define the ideal geometry for data representations and demonstrate
that spectral and intrinsic-dimensionality metrics can accurately predict a
model's OOD performance. Finally, we analyze how normalization impacts OOD
performance. Building upon these studies, we propose radially scaled $\ell_2$
normalization, a method that generalizes the standard $\ell_2$ normalization
recently applied to Mahalanobis-based OOD detection. Our approach introduces a
tunable parameter to directly control the radial geometry of the feature space,
systematically contracting or expanding representations to significantly
improve OOD detection performance. By bridging the gap between representation
geometry, normalization, and OOD performance, our findings offer new insights
into the design of more effective and reliable deep learning models.

</details>


### [57] [Cavity Duplexer Tuning with 1d Resnet-like Neural Networks](https://arxiv.org/abs/2510.15796)
*Anton Raskovalov*

Main category: cs.LG

TL;DR: Supervised-learning-based tuning of a cavity duplexer using a 1D ResNet-like backbone with S-parameter features; achieves near-tuned state within 4–5 screw rotations.


<details>
  <summary>Details</summary>
Motivation: Tuning a cavity duplexer with many adjustment screws is challenging and time-consuming. Conventional reinforcement learning was ineffective, prompting reformulation as a supervised learning task to improve efficiency and stability.

Method: A neural network with a 1D ResNet-like backbone processes both standard control signals and additional S-parameter information (curve shape, peak positions, and amplitudes). An external control algorithm translates network outputs into screw adjustments, enabling guided tuning.

Result: The approach reaches almost the tuned state of the duplexer within 4–5 rotations per screw, indicating effective and fast convergence compared to traditional methods.

Conclusion: Supervised learning with engineered S-parameter features and a 1D ResNet-like backbone can effectively optimize complex RF tuning tasks, offering a viable alternative to reinforcement learning in this context.

Abstract: This paper presents machine learning method for tuning of cavity duplexer
with a large amount of adjustment screws. After testing we declined
conventional reinforcement learning approach and reformulated our task in the
supervised learning setup. The suggested neural network architecture includes
1d ResNet-like backbone and processing of some additional information about
S-parameters, like the shape of curve and peaks positions and amplitudes. This
neural network with external control algorithm is capable to reach almost the
tuned state of the duplexer within 4-5 rotations per screw.

</details>


### [58] [ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning](https://arxiv.org/abs/2510.15211)
*Yongchan Kwon,Shang Zhu,Federico Bianchi,Kaitlyn Zhou,James Zou*

Main category: cs.LG

TL;DR: ReasonIF 评估推理阶段的指令遵循性，揭示现有大型推理模型在推理过程中的指令跟随显著不足；提出多轮推理和推理指令微调（RIF）两策略以提升 IFS。


<details>
  <summary>Details</summary>
Motivation: 提高大型推理模型在推理过程中的指令遵循性，以增强可控性、透明性并降低推理过程中的幻觉、偏差或奖励黑客风险。

Method: 提出 ReasonIF 基准，涵盖六类指令提示（包括多语言推理、格式与长度控制等），在包括 GPT-OSS、Qwen3、DeepSeek-R1 等在内的多种开源 LRMs 上评估推理指令遵循性，给出推理指令遵循分数（IFS）；探索两种提升策略：多轮推理与基于合成数据的推理指令微调（RIF）。

Result: 发现多数模型的 IFS 最高也仅约 0.25 以下，即约 25% 的推理链遵循指令；任务难度提升时，指令遵循性进一步下降；通过 RIF，对 GPT-OSS-20B 的 IFS 从 0.11 提升到 0.27，表明有可观的改进但仍有较大改进空间。

Conclusion: 推理阶段的指令遵循性是一个普遍而重要的挑战，需要持续改进的控制机制和训练方法，以提升推理过程的可信性和安全性。

Abstract: The ability of large language models (LLMs) to follow user instructions is
central to their reliability, safety, and usefulness. While prior studies
assess instruction adherence in the model's main responses, we argue that it is
also critical for large reasoning models (LRMs) to follow user instructions
throughout their reasoning process. Reasoning instruction following makes LRMs
more controllable and transparent, while reducing risks of undesirable
shortcuts, hallucinations, or reward hacking within reasoning traces. To
evaluate this dimension, we introduce ReasonIF, a systematic benchmark for
assessing reasoning instruction following. ReasonIF includes six categories of
instruction prompts, spanning multilingual reasoning, formatting and length
control. Across many open-source LRMs including GPT-OSS, Qwen3, and
DeepSeek-R1, we find substantial failures in reasoning instruction adherence:
the highest instruction following score (IFS) remains below 0.25, meaning that
fewer than $25\%$ of reasoning traces comply with the given instructions.
Notably, as task difficulty increases, reasoning instruction following degrades
further. We also explore two strategies to enhance reasoning instruction
fidelity. (1) multi-turn reasoning and (2) Reasoning Instruction Finetuning
(RIF) using synthetic data. RIF improves the IFS of $GPT-OSS-20B$ from 0.11 to
0.27, indicating measurable progress but leaving ample room for improvement.

</details>


### [59] [Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential](https://arxiv.org/abs/2510.15216)
*Xuansheng Wu,Xiaoman Pan,Wenlin Yao,Jianshu Chen*

Main category: cs.LG

TL;DR: 提出了名为 SAL 的微观量化指标，利用 Jensen-Shannon 散度来测量预训练模型内部对“正确知识”与“不正确知识”之间区分的能力。结果表明，具备高潜力的模型在内部分布上能显著区分不同规则的 soundness 水平（严格、可辩、噪声等），而弱模型则对 soundness 慢性无差异。SAL 能预测 RLVR（可验证奖励的强化学习）后推理性能，拟合度高（R^2=0.87），跨多种模型家族与尺度。方法还包括用跨层稀疏自编码器从潜在空间提取特征，构造“如果-则”Horn 规则，并对规则进行语义层级标注与转移概率估计。结论指向：模型预训练的内在能力决定其推理潜力，SAL 提供一个实用的内部机制视角来选型或设计更强的基础模型。


<details>
  <summary>Details</summary>
Motivation: 探究为何同样的 RLVR 训练在不同基础模型上效果差异巨大，寻找一个微观层面的可解释属性来预测和提升模型推理能力。

Method: 用跨层稀疏自编码器从潜在空间提取特征，估计特征之间的转移概率，构建基于 Horn 条件的推理链；对每条规则按语义可置信度分 level（如严格、可信、嘈杂）标注，并用语言模型对其进行评估。通过 Jensen-Shannon 散度量化同一模型在不同 soundness level 下的特征转移分布差异，从而得到 SAL 指标。对多家族/多尺度模型进行验证。

Result: 高潜力模型的内部分布对不同 soundness level 的区分度显著，SAL 与后 RLVR 推理性能呈现稳健回归关系（R^2=0.87），跨 Qwen、Mistral、Llama、DeepSeek 等模型，从 0.5B 到 14B 的规模均成立。弱模型表现为对 soundness 水平“无感”，趋势相同的分布。

Conclusion: 模型的预训练决定了其区分正确与不正确知识的内在能力，SAL 提供一个基于模型内部机制的实用指标，用于选择/设计更强的基础模型以提升推理潜力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) can elicit strong
reasoning in large language models (LLMs), while their performance after RLVR
varies dramatically across different base models. This raises a fundamental
question: what microscopic property of pre-trained models leads to this
variation? To investigate, we formalize reasoning as chains of Horn clauses
("if-then" rules) built from features extracted from the LLM's latent space via
cross-layer sparse autoencoders (SAEs). We estimate the transition
probabilities between its features, and further categorize each rule by its
semantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key
discovery is that high-potential models are inherently soundness-aware: their
internal probability distributions systematically shift across rules' soundness
levels, becoming highly distinct for "strict" versus "noisy" rules. In
contrast, weaker models are soundness-agnostic, collapsing to one distribution
regardless of soundness levels. To quantify this, we introduce the
Soundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon
Divergence to measure the separation between these distributions. We show that
SAL's predictions of post-RLVR reasoning performance follow a precise empirical
law (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)
and scales (0.5B-14B). This reveals that a model's reasoning potential is tied
to its intrinsic, pre-trained ability to distinguish sound knowledge from
unsound ones. These findings underscore the critical role of model pre-training
in shaping reasoning and offer a practical metric grounded in the model's
internal mechanisms for selecting/designing stronger base models.

</details>


### [60] [Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025](https://arxiv.org/abs/2510.15217)
*Emily Alsentzer,Marie-Laure Charpignon,Bill Chen,Niharika D'Souza,Jason Fries,Yixing Jiang,Aparajita Kashyap,Chanwoo Kim,Simon Lee,Aishwarya Mandyam,Ashery Christopher Mbilinyi,Nikita Mehandru,Nitish Nagesh,Brighton Nuwagira,Emma Pierson,Arvind Pillai,Akane Sano,Tanveer Syeda-Mahmood,Shashank Yadav,Elias Adhanom,Muhammad Umar Afza,Amelia Archer,Suhana Bedi,Vasiliki Bikia,Trenton Chang,George H. Chen,Winston Chen,Erica Chiang,Edward Choi,Octavia Ciora,Paz Dozie-Nnamah,Shaza Elsharief,Matthew Engelhard,Ali Eshragh,Jean Feng,Josh Fessel,Scott Fleming,Kei Sen Fong,Thomas Frost,Soham Gadgil,Judy Gichoya,Leeor Hershkovich,Sujeong Im,Bhavya Jain,Vincent Jeanselme,Furong Jia,Qixuan,Jin,Yuxuan Jin,Daniel Kapash,Geetika Kapoor,Behdokht Kiafar,Matthias Kleiner,Stefan Kraft,Annika Kumar,Daeun Kyung,Zhongyuan Liang,Joanna Lin,Qianchu,Liu,Chang Liu,Hongzhou Luan,Chris Lunt,Leopoldo Julían Lechuga López,Matthew B. A. McDermott,Shahriar Noroozizadeh,Connor O'Brien,YongKyung Oh,Mixail Ota,Stephen Pfohl,Meagan Pi,Tanmoy Sarkar Pias,Emma Rocheteau,Avishaan Sethi,Toru Shirakawa,Anita Silver,Neha Simha,Kamile Stankeviciute,Max Sunog,Peter Szolovits,Shengpu Tang,Jialu Tang,Aaron Tierney,John Valdovinos,Byron Wallace,Will Ke Wang,Peter Washington,Jeremy Weiss,Daniel Wolfe,Emily Wong,Hye Sun Yun,Xiaoman Zhang,Xiao Yu Cindy Zhang,Hayoung Jeong,Kaveri A. Thakoor*

Main category: cs.LG

TL;DR: CHIL 2025 研究小组对话环节通过 eight roundtables 探讨 ML 与医疗健康交叉领域的关键挑战与机会，强调可操作的方向。


<details>
  <summary>Details</summary>
Motivation: 通过小组圆桌会话激发跨学科协作，推动对可解释性、不确定性、偏见、公平性、因果性、领域自适应、基础模型、从少量医疗数据学习、多模态方法、可扩展的转化性医疗解决方案等议题的深入讨论与共识形成。

Method: 采用线下圆桌会议形式，由资深与青年共同主持的多方小组讨论，强调开放交流、包容参与与可检验的论证；共设八个圆桌，涉及九个主题并由19位圆桌主持人领导。

Result: 在现场举行八场圆桌讨论，推动对关键挑战的探讨、机会的挖掘以及面向实际方向的集体构想，形成初步的研究议程与跨领域协作的组织框架。

Conclusion: 该会议设计凸显在 ML 与医疗健康交叉领域的跨学科协作价值，围绕可解释性、公平性、因果关系、领域自适应、基础模型、少量数据学习、多模态方法及可转化的健康解决方案等核心议题，为未来研究和实践提供方向性引导。

Abstract: The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),
hosted by the Association for Health Learning and Inference (AHLI), was held in
person on June 25-27, 2025, at the University of California, Berkeley, in
Berkeley, California, USA. As part of this year's program, we hosted Research
Roundtables to catalyze collaborative, small-group dialogue around critical,
timely topics at the intersection of machine learning and healthcare. Each
roundtable was moderated by a team of senior and junior chairs who fostered
open exchange, intellectual curiosity, and inclusive engagement. The sessions
emphasized rigorous discussion of key challenges, exploration of emerging
opportunities, and collective ideation toward actionable directions in the
field. In total, eight roundtables were held by 19 roundtable chairs on topics
of "Explainability, Interpretability, and Transparency," "Uncertainty, Bias,
and Fairness," "Causality," "Domain Adaptation," "Foundation Models," "Learning
from Small Medical Data," "Multimodal Methods," and "Scalable, Translational
Healthcare Solutions."

</details>


### [61] [Integrating Product Coefficients for Improved 3D LiDAR Data Classification (Part II)](https://arxiv.org/abs/2510.15219)
*Patricia Medina,Rasika Karkare*

Main category: cs.LG

TL;DR: 将层次化的乘积系数与自编码器特征结合，并使用 KNN 分类器，显著提升 3D LiDAR 点云分类性能，且越多的乘积系数越有助于分离和准确率。


<details>
  <summary>Details</summary>
Motivation: 扩展以往关于乘积系数的研究，克服 PCA 基线和早期框架的局限，提升 LiDAR 分类的特征表达。

Method: 将乘积系数与自编码器表示融合，输入 KNN 分类器；与 PCA 基线及先前框架对比；按层逐步引入乘积系数以分析影响；使用层次化乘积系数描述符作为补充特征。

Result: 在与 PCA 基线和先前框架比较中，性能呈现一致的提升；逐层增加乘积系数显著提升类别可分离性和总体准确率。

Conclusion: 层次化乘积系数特征与自编码器组合为 LiDAR 分类提供更强的表示，进一步提升性能。

Abstract: This work extends our previous study on enhancing 3D LiDAR point-cloud
classification with product coefficients
\cite{medina2025integratingproductcoefficientsimproved}, measure-theoretic
descriptors that complement the original spatial Lidar features. Here, we show
that combining product coefficients with an autoencoder representation and a
KNN classifier delivers consistent performance gains over both PCA-based
baselines and our earlier framework. We also investigate the effect of adding
product coefficients level by level, revealing a clear trend: richer sets of
coefficients systematically improve class separability and overall accuracy.
The results highlight the value of combining hierarchical product-coefficient
features with autoencoders to push LiDAR classification performance further.

</details>


### [62] [Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent](https://arxiv.org/abs/2510.15222)
*Gabriel Nixon Raj*

Main category: cs.LG

TL;DR: 提出一种基于熵正则化的信任衰减框架，用于在分布漂移条件下的序贯决策，结合信念更新与镜像下降，给出动态-regret、鲁棒性及多种扩展的理论保证。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中进行序贯决策时，分布漂移导致性能下降。需要一个可自适应、鲁棒且理论可证的更新规则，兼容信念与策略的协同调整。

Method: 在信念更新和决策更新中引入疲劳感知的指数偏斜，形成熵正则化的信任衰减。针对简单形（概率分布的单位简单形），利用Fenchel对偶性证明信念倾斜与决策倾斜一致。提出鲁棒性度量：fragility（KL球内的最坏情 excess risk）、belief bandwidth（维持目标超额的半径）、以及Decision-space Fragility Index（在O(√T)的后悔中容忍漂移）。给出高概率灵敏度界与动态后悔界，均以KL漂移路径长度S_T为度量，证明在KL漂移条件下的后悔是~O(√T)。还包括无参对冲、过度倾斜的成本、二阶更新、Bandit反馈、离群点、分布式优化和KL漂移估计等扩展。

Result: 在KL漂移路径长度S_T下，动态后悔可达到~O(√T)；信任衰减实现每次切换的O(1)后悔，压力自由更新会产生常量级尾部开销。无参对冲能够自适应漂移，过度倾斀会导致Ω(λ^2 T)的常数项惩罚。给出标定应力界与多种扩展的结果，框架统一了动态后悔分析、分布鲁棒优化和KL正则控制于单一的应力自适应更新中。

Conclusion: 提供一个统一的理论框架，将动态后悔分析、分布鲁棒目标和KL正则化控制结合在一个可自适应的更新机制中，适用于多种场景与拓展，且具备鲁棒性与可扩展性。

Abstract: We study sequential decision-making under distribution drift. We propose
entropy-regularized trust-decay, which injects stress-aware exponential tilting
into both belief updates and mirror-descent decisions. On the simplex, a
Fenchel-dual equivalence shows that belief tilt and decision tilt coincide. We
formalize robustness via fragility (worst-case excess risk in a KL ball),
belief bandwidth (radius sustaining a target excess), and a decision-space
Fragility Index (drift tolerated at $O(\sqrt{T})$ regret). We prove
high-probability sensitivity bounds and establish dynamic-regret guarantees of
$\tilde{O}(\sqrt{T})$ under KL-drift path length $S_T = \sum_{t\ge2}\sqrt{{\rm
KL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch
regret, while stress-free updates incur $\Omega(1)$ tails. A parameter-free
hedge adapts the tilt to unknown drift, whereas persistent over-tilting yields
an $\Omega(\lambda^2 T)$ stationary penalty. We further obtain
calibrated-stress bounds and extensions to second-order updates, bandit
feedback, outliers, stress variation, distributed optimization, and plug-in
KL-drift estimation. The framework unifies dynamic-regret analysis,
distributionally robust objectives, and KL-regularized control within a single
stress-adaptive update.

</details>


### [63] [Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction](https://arxiv.org/abs/2510.15233)
*Amitesh Badkul,Lei Xie*

Main category: cs.LG

TL;DR: 提出 TESSERA，一种结合 MoE 多样性与 conformal 校准的可信自适应不确定性量化方法，适用于蛋白质-配体结合亲和力预测，在 i.i.d. 与 OOD 场景下实现名义覆盖率接近、覆盖宽度权衡良好，且自适应性强。


<details>
  <summary>Details</summary>
Motivation: 高风险领域需要可靠且可解释的量化不确定性；现有方法在新数据覆盖、区间宽度和对分布转移的跟踪方面不足，PLI 预测面临噪声异质性、化学空间不平衡与分布偏移，需要更稳健的UQ方法。

Method: 提出 TESSERA：将混合专家（MoE）的多样性与 conformal 校准相结合，通过分割-保真 Split-conformal 与缩放估计，对每个样本给出带有可靠覆盖的自适应区间。

Result: 在 i.i.d. 与 scaffold-based OOD 分割下，与强基线相比，TESSERA 达到接近名义覆盖率、最佳覆盖-宽度折衷（CWC），并具备较优的自适应性衡量（AUSE），且 SSC 证实区间在数据稀缺/噪声时变宽、预测可靠时紧凑。

Conclusion: 通过统一 MoE 多样性与 conformal 校准，TESSERA 提供可信、紧凑且自适应的不确定性，适用于药物发现管线的选择性预测和下游决策。

Abstract: Reliable, informative, and individual uncertainty quantification (UQ) remains
missing in current ML community. This hinders the effective application of
AI/ML to risk-sensitive domains. Most methods either fail to provide coverage
on new data, inflate intervals so broadly that they are not actionable, or
assign uncertainties that do not track actual error, especially under a
distribution shift. In high-stakes drug discovery, protein-ligand affinity
(PLI) prediction is especially challenging as assay noise is heterogeneous,
chemical space is imbalanced and large, and practical evaluations routinely
involve distribution shift. In this work, we introduce a novel uncertainty
quantification method, Trustworthy Expert Split-conformal with Scaled
Estimation for Efficient Reliable Adaptive intervals (TESSERA), that provides
per-sample uncertainty with reliable coverage guarantee, informative and
adaptive prediction interval widths that track the absolute error. We evaluate
on protein-ligand binding affinity prediction under both independent and
identically distributed (i.i.d.) and scaffold-based out-of-distribution (OOD)
splits, comparing against strong UQ baselines. TESSERA attains near-nominal
coverage and the best coverage-width trade-off as measured by the
Coverage-Width Criterion (CWC), while maintaining competitive adaptivity
(lowest Area Under the Sparsification Error (AUSE)). Size-Stratified Coverage
(SSC) further confirms that intervals are right-sized, indicating width
increases when data are scarce or noisy, and remain tight when predictions are
reliable. By unifying Mixture of Expert (MoE) diversity with conformal
calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties
that are well-suited to selective prediction and downstream decision-making in
the drug-discovery pipeline and other applications.

</details>


### [64] [Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories](https://arxiv.org/abs/2510.15254)
*Dingya Feng,Dingyuan Xue*

Main category: cs.LG

TL;DR: 基于Transformer的框架，利用多源数据和地理编码预测 migratory birds 路径末点的疾病风险，在 held-out 集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 需要在迁徙鸟类传播路径的终点对疾病风险进行准确、及时预测，以支撑野生动物保护和公共卫生。

Method: 将多源数据（Movebank的GPS、WOAH的暴发记录、GADM/Natural Earth地理信息）结合，使用H3层级地理编码对原始坐标进行编码，训练Transformer以捕捉时空依赖并预测终点的疾病风险。

Result: 在保留集上达成高性能：准确率0.9821、AUC 0.9803、AP 0.9299、F1 0.8836（最优阈值）。

Conclusion: 表明Transformer架构具备用于早期预警系统的潜力，可辅助及时干预和防控。

Abstract: Accurate forecasting of avian disease outbreaks is critical for wildlife
conservation and public health. This study presents a Transformer-based
framework for predicting the disease risk at the terminal locations of
migratory bird trajectories. We integrate multi-source datasets, including GPS
tracking data from Movebank, outbreak records from the World Organisation for
Animal Health (WOAH), and geospatial context from GADM and Natural Earth. The
raw coordinates are processed using H3 hierarchical geospatial encoding to
capture spatial patterns. The model learns spatiotemporal dependencies from
bird movement sequences to estimate endpoint disease risk. Evaluation on a
held-out test set demonstrates strong predictive performance, achieving an
accuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision
(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These
results highlight the potential of Transformer architectures to support
early-warning systems for avian disease surveillance, enabling timely
intervention and prevention strategies.

</details>


### [65] [DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models](https://arxiv.org/abs/2510.15260)
*Yangyang Li*

Main category: cs.LG

TL;DR: 通过分布鲁棒贝叶斯优化（DRO）对零-shot 提示进行优化，显著提升在分布偏移下的鲁棒性与可迁移性，适用于多种指令型任务。


<details>
  <summary>Details</summary>
Motivation: 现有的自动提示搜索方法在单一评估分布下优化，易在分布偏移和对抗性评估中退化，导致一个设置有效、其他设置失效，亟需提高对分布变化的鲁棒性。

Method: 将零-shot 提示优化建模为带 f-散度球的鲁棒贝叶斯优化。通过定义评估分布的模糊集，鲁棒采集规则最大化最差期望效用，同时保留贝叶斯搜索的查询效率。按照形式化改写、代码调试、翻译等任务，设定等量查询预算进行比较。

Result: 在 BIG-Bench 的 Informative-to-Formal 重写任务中，准确率由 61.3% ±0.7% 提升至约 85–90%，绝对提升约 25–30 点。自动调试在领域迁移下亦获得约 25 点的提升。对因果等稳定任务保持在 96% 以上，未在分布内出现损失。改进在不同的散度选项和解码温度下都具有一致性。

Conclusion: DRO-InstructZero 将分布鲁棒优化与提示学习结合，提供一种即插即用、面向现实不确定性的可靠、可迁移的提示对齐通用方法。

Abstract: Large language models are highly sensitive to prompt wording. However,
popular automatic prompt search methods, including InstructZero, often degrade
under distribution shift and adversarial evaluation because they optimize
expected performance under a single evaluation distribution. Consequently,
prompts that work in one setting frequently fail to transfer. To address this,
DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian
optimization. Specifically, an f-divergence ball defines an ambiguity set
around the evaluation distribution, and a robust acquisition rule maximizes
worst-case expected utility while retaining the query efficiency of Bayesian
search. Therefore, the search explicitly targets reliability under distribution
shift rather than average behavior alone. Experiments follow the
instruction-induction protocol with matched query budgets across formality
rewriting, code debugging, and translation. For example, on BIG-Bench
informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to
approximately 85-90%, yielding an absolute gain of about 25-30 points.
Moreover, auto-debugging shows about +25-point gains under domain shift.
Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating
no loss on in-distribution cases. Furthermore, improvements are consistent
across divergence choices and decoding temperatures. Overall, DRO-InstructZero
connects distributionally robust optimization with prompt learning, offering a
plug-and-play and general approach for reliable, transferable prompt alignment
under real-world uncertainty.

</details>


### [66] [Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under Distribution Shift](https://arxiv.org/abs/2510.15265)
*Emam Hossain,Muhammad Hasan Ferdous,Devon Dunmire,Aneesh Subramanian,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出了一个区域信息化因果时序分类框架RIC-TSC，通过J-PCMCI+在时空观测数据中识别区域性不变预测因子，并将其 lag 预测融入轻量级分类器，在出分布情形下优于相关基线，提升对格陵兰超冰湖演化的解释性和可泛化性。


<details>
  <summary>Details</summary>
Motivation: 因果建模能发现时间序列中的稳定、不可变关系，从而提高在分布偏移下的鲁棒性和泛化性；然而在时空地球观测中应用不足，现有模型多依赖相关特征，难以跨域转移。

Method: 在多模态卫星和再分析数据上，利用Joint PCMCI+进行滞后因果发现，得到区域性以及不变的预测因子；全局与盆地层面的因果图估计，将带滞后的预测变量提供给轻量级分类器；数据源包括 Sentinel-1 微波散射、Sentinel-2/ Landsat-8 光学反射率、CARRA 气象变量。

Result: 在一个由两次对比融化季（2018-2019）共1000个人工标注湖的平衡基准数据集上，因果模型在分布外评估中比基线基于相关性的模型最高提升约12.59%的准确率。

Conclusion: 因果发现不仅可用于特征选择，更是实现可泛化、具有机制解释性的地表过程动态建模的路径。

Abstract: Causal modeling offers a principled foundation for uncovering stable,
invariant relationships in time-series data, thereby improving robustness and
generalization under distribution shifts. Yet its potential is underutilized in
spatiotemporal Earth observation, where models often depend on purely
correlational features that fail to transfer across heterogeneous domains. We
propose RIC-TSC, a regionally-informed causal time-series classification
framework that embeds lag-aware causal discovery directly into sequence
modeling, enabling both predictive accuracy and scientific interpretability.
Using multi-modal satellite and reanalysis data-including Sentinel-1 microwave
backscatter, Sentinel-2 and Landsat-8 optical reflectance, and CARRA
meteorological variables-we leverage Joint PCMCI+ (J-PCMCI+) to identify
region-specific and invariant predictors of supraglacial lake evolution in
Greenland. Causal graphs are estimated globally and per basin, with validated
predictors and their time lags supplied to lightweight classifiers. On a
balanced benchmark of 1000 manually labeled lakes from two contrasting melt
seasons (2018-2019), causal models achieve up to 12.59% higher accuracy than
correlation-based baselines under out-of-distribution evaluation. These results
show that causal discovery is not only a means of feature selection but also a
pathway to generalizable and mechanistically grounded models of dynamic Earth
surface processes.

</details>


### [67] [Semi-Supervised Regression with Heteroscedastic Pseudo-Labels](https://arxiv.org/abs/2510.15266)
*Xueqing Sun,Renzhen Wang,Quanziang Wang,Yichen Wu,Xixi Jia,Deyu Meng*

Main category: cs.LG

TL;DR: 提出一种面向半监督回归的带不确定性感知的伪标签框架，通过双层优化学习伪标签的影响力度，利用不确定性估计来抑制不可靠标签，从而提升鲁棒性和泛化，在多组 SSR 基准数据集上优于现有方法；并给出理论见解与大量实验，代码开放。


<details>
  <summary>Details</summary>
Motivation: 与分类任务不同，SSR 的输出是连续且存在异方差噪声，难以可靠地评估伪标签的可信度。 naive 的伪标签可能导致误差累积与对错误标签的过拟合，因此需要一种能够在训练过程中动态调整伪标签影响的机制。

Method: 提出一个不确定性感知的伪标签框架，以双层优化实现：外层最小化所有数据的经验风险，内层优化不确定性（置信度/方差等）以提升对标记数据的泛化能力，从而动态地对伪标签进行加权，降低不可靠伪标签的影响，且通过对不确定性的学习实现对不同样本的自适应处理。理论分析支持其有效性，且利用多组 SSR 基准数据集进行广泛实验。

Result: 理论洞见与大量实验表明该方法在鲁棒性和性能上优于现有方法，尤其在对抗或高噪声场景下对伪标签依赖较小，从而提升 SSR 的整体表现。

Conclusion: 不确定性感知的伪标签框架成功缓解了不可靠伪标签带来的负效应，提升了半监督回归的泛化能力与鲁棒性，代码已开源，便于复现实验并推动相关研究。

Abstract: Pseudo-labeling is a commonly used paradigm in semi-supervised learning, yet
its application to semi-supervised regression (SSR) remains relatively
under-explored. Unlike classification, where pseudo-labels are discrete and
confidence-based filtering is effective, SSR involves continuous outputs with
heteroscedastic noise, making it challenging to assess pseudo-label
reliability. As a result, naive pseudo-labeling can lead to error accumulation
and overfitting to incorrect labels. To address this, we propose an
uncertainty-aware pseudo-labeling framework that dynamically adjusts
pseudo-label influence from a bi-level optimization perspective. By jointly
minimizing empirical risk over all data and optimizing uncertainty estimates to
enhance generalization on labeled data, our method effectively mitigates the
impact of unreliable pseudo-labels. We provide theoretical insights and
extensive experiments to validate our approach across various benchmark SSR
datasets, and the results demonstrate superior robustness and performance
compared to existing methods. Our code is available at
https://github.com/sxq/Heteroscedastic-Pseudo-Labels.

</details>


### [68] [On the Generalization Properties of Learning the Random Feature Models with Learnable Activation Functions](https://arxiv.org/abs/2510.15327)
*Zailin Ma,Jiansheng Yang,Yaodong Yang*

Main category: cs.LG

TL;DR: 分析带可学习激活函数的随机特征核方法（RFLAF）的泛化性质，给出数据相关采样下的特征数s的最尖锐界限；通过加权采样显著降低所需特征数；提出估算核并应用加权采样的算法，实验验证理论，表明加权RFLAF在显著较少的特征下可达到类似性能。


<details>
  <summary>Details</summary>
Motivation: 解决RFLAF在回归与分类任务中的泛化能力与样本复杂度，研究数据相关的特征采样对特征数量s的影响，以及引入权重化的杠杆采样对界限的提升。

Method: 给出一个统一的定理描述s的复杂度，比较普通采样与数据依赖的杠杆加权采样，对MSE与Lipschitz损失分别给出s的界；提出近似核的学习算法并应用杠杆加权采样；通过实验验证理论和方法的有效性。

Result: 在引入数据依赖的采样后，给出关于特征数s的尖锐界限：对MSE损失，普通界从Ω(1/ε^2)提升至在权重采样下的Ω((1/ε)^{1/t})（t≥1），若Gram矩阵有限秩则可达到Ω(1)；对于Lipschitz损失，界从Ω(1/ε^2)提升至Ω((1/ε^2)^{1/t})的无量纲形式；并且通过加权采样可显著减少所需特征数；提出的近似核学习算法与权重采样相结合，理论和实验均验证了方法的有效性。

Conclusion: 加权RFLAF在特征数显著减少的情况下仍能保持与未加权方法相近的性能，理论分析和实验证据共同支持该方法的有效性与实用性。

Abstract: This paper studies the generalization properties of a recently proposed
kernel method, the Random Feature models with Learnable Activation Functions
(RFLAF). By applying a data-dependent sampling scheme for generating features,
we provide by far the sharpest bounds on the required number of features for
learning RFLAF in both the regression and classification tasks. We provide a
unified theorem that describes the complexity of the feature number $s$, and
discuss the results for the plain sampling scheme and the data-dependent
leverage weighted scheme. Through weighted sampling, the bound on $s$ in the
MSE loss case is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon)^{1/t})$ in general $(t\geq 1)$, and even to
$\Omega(1)$ when the Gram matrix has a finite rank. For the Lipschitz loss
case, the bound is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon^2)^{1/t})$. To learn the weighted RFLAF, we also
propose an algorithm to find an approximate kernel and then apply the leverage
weighted sampling. Empirical results show that the weighted RFLAF achieves the
same performances with a significantly fewer number of features compared to the
plainly sampled RFLAF, validating our theories and the effectiveness of this
method.

</details>


### [69] [Towards Robust Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.15382)
*Kexin Zheng,Lauriane Teyssier,Yinan Zheng,Yu Luo,Xiayuan Zhan*

Main category: cs.LG

TL;DR: BREEZE: 将行为正则化引入FB基础的零-shot RL，并通过任务条件扩散模型和表达式强大的注意力表示提升表达能力、稳定性和多模态性，以实现对新任务的零-shot适应，结果在ExORL和D4RL Kitchen上表现出最优或接近最优且鲁棒性优越。


<details>
  <summary>Details</summary>
Motivation: 解决FB等方法在离线零-shot RL中表现出的表达力不足和对OOD动作的推断误差导致的表征偏差问题，提升学习稳定性、策略提取能力和表示学习质量。

Method: 引入行为正则化促进零-shot RL策略学习成为稳定的就地学习；使用任务条件扩散模型进行策略提取，生成高质量和多模态的动作分布；采用基于注意力的表达模型来捕捉环境动力学的复杂关系。

Result: 在ExORL和D4RL Kitchen上实现最佳或接近最佳的性能，并在鲁棒性方面优于先前的离线零-shot RL 方法。

Conclusion: BREEZE通过行为正则化、扩散型策略提取和高表达性的表示学习，提升离线零-shot RL的稳定性、表达力和策略可提取性，优于现有方法。

Abstract: The recent development of zero-shot reinforcement learning (RL) has opened a
new avenue for learning pre-trained generalist policies that can adapt to
arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward
representations (FB) and related methods have shown promise in zero-shot RL, we
empirically found that their modeling lacks expressivity and that extrapolation
errors caused by out-of-distribution (OOD) actions during offline learning
sometimes lead to biased representations, ultimately resulting in suboptimal
performance. To address these issues, we propose Behavior-REgularizEd Zero-shot
RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that
simultaneously enhances learning stability, policy extraction capability, and
representation learning quality. BREEZE introduces behavioral regularization in
zero-shot RL policy learning, transforming policy optimization into a stable
in-sample learning paradigm. Additionally, BREEZE extracts the policy using a
task-conditioned diffusion model, enabling the generation of high-quality and
multimodal action distributions in zero-shot RL settings. Moreover, BREEZE
employs expressive attention-based architectures for representation modeling to
capture the complex relationships between environmental dynamics. Extensive
experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best
or near-the-best performance while exhibiting superior robustness compared to
prior offline zero-shot RL methods. The official implementation is available
at: https://github.com/Whiterrrrr/BREEZE.

</details>


### [70] [Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning](https://arxiv.org/abs/2510.15388)
*Mingyang Sun,Pengxiang Ding,Weinan Zhang,Donglin Wang*

Main category: cs.LG

TL;DR: 提出 Stepwise Flow Policy (SWFP) 框架，将流动匹配推理离散化为固定步长的逐步更新，与 JKO 原理等价，分解全局流为小步变换，提升稳定性和在线适应性，实验表明在机器人控制基准上具有更优表现。


<details>
  <summary>Details</summary>
Motivation: 行为克隆在示范学习中擅长学习复杂技能，但容易受分布偏移影响；标准强化学习微调存在迭代推理过程低效和现有变通方法的局限性，需一种更稳定且便于在线适应的微调框架。

Method: 将全局流动匹配推理离散化为一系列固定步长的增量变换，每一步对应一个 JKO 更新，利用恩特ropic 正则化约束策略的变化，使其保持在前一迭代附近，并通过 Wasserstein 信赖域实现稳定在线适应。将模型分解为小的流块级联训练，以实现更简单/更快的子模型训练，降低计算与内存开销。

Result: 实验结果表明 SWFP 提高了稳定性与效率，并在多种机器人控制基准上展现出更强的自适应性能，与现有方法相比具有显著的优势。

Conclusion: 本文提出的 SWFP 框架通过将流动匹配推理与 JKO 原理相结合的固定步长离散化，实现对预训练流的分步骤微调，提供理论上基于 Wasserstein 信赖域的稳定性保障，并在实践中展现出更好的在线适应能力与计算效率。

Abstract: While behavior cloning with flow/diffusion policies excels at learning
complex skills from demonstrations, it remains vulnerable to distributional
shift, and standard RL methods struggle to fine-tune these models due to their
iterative inference process and the limitations of existing workarounds. In
this work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on
the key insight that discretizing the flow matching inference process via a
fixed-step Euler scheme inherently aligns it with the variational
Jordan-Kinderlehrer-Otto (JKO) principle from optimal transport. SWFP
decomposes the global flow into a sequence of small, incremental
transformations between proximate distributions. Each step corresponds to a JKO
update, regularizing policy changes to stay near the previous iterate and
ensuring stable online adaptation with entropic regularization. This
decomposition yields an efficient algorithm that fine-tunes pre-trained flows
via a cascade of small flow blocks, offering significant advantages:
simpler/faster training of sub-models, reduced computational/memory costs, and
provable stability grounded in Wasserstein trust regions. Comprehensive
experiments demonstrate SWFP's enhanced stability, efficiency, and superior
adaptation performance across diverse robotic control benchmarks.

</details>


### [71] [Online Kernel Dynamic Mode Decomposition for Streaming Time Series Forecasting with Adaptive Windowing](https://arxiv.org/abs/2510.15404)
*Christopher Salazar,Krithika Manohar,Ashis G. Banerjee*

Main category: cs.LG

TL;DR: WORK-DMD提供了一种在流数据上具有固定计算成本的实时非线性预测方法，结合了随机傅里叶特征和在线DMD，并在滚动窗口内使用 Sherman-Morrison 更新，达到对非平稳动态的快速自适应与良好短期预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中需要在资源受限、数据不断流入的情况下进行高效且自适应的实时预测，现有方法在准确性、适应性和计算成本之间存在权衡，容易发生遗忘或资源耗尽。

Method: 将随机傅里叶特征用于显式特征映射以捕捉非线性动力学；在滚动窗口中使用在线 Dynamic Mode Decomposition，并结合 Sherman-Morrison 更新以实现对当前数据的连续适应，无需大规模存储历史数据，且保持固定的计算成本。

Result: 在多域基准数据集上，WORK-DMD的预测准确性高于若干在线预测基线，且以单次数据遍历实现，尤其在短期预测中表现突出，展现了对少量数据的样本高效性。

Conclusion: 将核方法的表达能力与自适应矩阵更新结合，提供一种在流式场景中与深度学习相比具竞争力且数据需求低的可行替代方案，适用于对资源敏感的实时预测任务。

Abstract: Real-time forecasting from streaming data poses critical challenges: handling
non-stationary dynamics, operating under strict computational limits, and
adapting rapidly without catastrophic forgetting. However, many existing
approaches face trade-offs between accuracy, adaptability, and efficiency,
particularly when deployed in constrained computing environments. We introduce
WORK-DMD (Windowed Online Random Kernel Dynamic Mode Decomposition), a method
that combines Random Fourier Features with online Dynamic Mode Decomposition to
capture nonlinear dynamics through explicit feature mapping, while preserving
fixed computational cost and competitive predictive accuracy across evolving
data. WORK-DMD employs Sherman-Morrison updates within rolling windows,
enabling continuous adaptation to evolving dynamics from only current data,
eliminating the need for lengthy training or large storage requirements for
historical data. Experiments on benchmark datasets across several domains show
that WORK-DMD achieves higher accuracy than several state-of-the-art online
forecasting methods, while requiring only a single pass through the data and
demonstrating particularly strong performance in short-term forecasting. Our
results show that combining kernel evaluations with adaptive matrix updates
achieves strong predictive performance with minimal data requirements. This
sample efficiency offers a practical alternative to deep learning for streaming
forecasting applications.

</details>


### [72] [Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models](https://arxiv.org/abs/2510.15429)
*Shashank Gupta*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This dissertation investigates how reinforcement learning (RL) methods can be
designed to be safe, sample-efficient, and robust. Framed through the unifying
perspective of contextual-bandit RL, the work addresses two major application
domains - ranking and recommendation, and text-to-image diffusion models. The
first part of the thesis develops theory and algorithms for safe deployment in
ranking systems. An exposure-based generalisation bound is derived, leading to
a counterfactual risk-minimisation objective whose solution is guaranteed not
to underperform the logging policy, even with sparse feedback. This guarantee
is extended to doubly robust estimators, enabling safety even under adversarial
or misspecified user models and offering practitioners explicit control over
permissible utility loss. The second part turns to single-action bandits, where
various off-policy estimators are unified within a baseline-correction
framework. A closed-form optimal baseline is proposed and shown to minimise
both evaluation and policy-gradient variance, thereby improving off-policy
learning reliability. The final part examines the trade-offs between efficiency
and effectiveness in generative RL. A systematic study of PPO and REINFORCE
motivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple
diffusion trajectories with a REINFORCE-style baseline inside PPO's clipped
objective. LOOP achieves PPO-level sample efficiency while producing
generations that align more faithfully with textual attributes.

</details>


### [73] [A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning](https://arxiv.org/abs/2510.15444)
*Zhi Zhou,Yuhao Tan,Zenan Li,Yuan Yao,Lan-Zhe Guo,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.LG

TL;DR: 提出一个基于置信估计的理论框架来分析采样式测试时扩展（test-time scaling）在大语言模型中的性价比与局限；并提出RPC方法（Perplexity Consistency + Reasoning Pruning）以提升估计收敛速度、减小采样成本，同时保持或提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管采样式测试时扩展在实际中有效，但缺乏理论基础。本工作从置信估计角度构建框架，系统分析自一致性（self-consistency）与困惑度（perplexity）两大范式的局限，填补理论空白并促进对推理路径的可靠性评估。

Method: 提出一个理论框架以置信估计为核心，分析自一致性与困惑度两大范式的误差来源与潜在偏差；指出它们各自的局限性（自一致性高估计误差，困惑度具有显著建模误差并可能降低估计误差的收敛性）；在此基础上提出RPC方法，结合“困惑度一致性”（Perplexity Consistency）与“推理剪枝”（Reasoning Pruning）来提升收敛速度并抑制低概率推理路径的负面影响。

Result: 理论分析与七个基准数据集上的实证结果显示：1) 围绕困惑度的一致性策略可将估计误差的收敛速率从线性提升到指数级；2) 推理剪枝减少了因低概率路径带来的降级风险；3) RPC在推理性能上可与自一致性接近，同时显著提升置信度的可靠性并将采样成本降低约50%。

Conclusion: RPC具备在提高推理准确性的同时显著降低采样成本的潜力，理论与实证结果相互印证。研究还提供代码与资源以便复现与进一步探索。

Abstract: Test-time scaling seeks to improve the reasoning performance of large
language models (LLMs) by adding computational resources. A prevalent approach
within the field is sampling-based test-time scaling methods, which enhance
reasoning by generating multiple reasoning paths for a given input during
inference. However, despite its practical success, the theoretical foundations
remain underexplored. In this paper, we provide the first theoretical framework
for analyzing sampling-based test-time scaling methods, grounded in the
perspective of confidence estimation. Based on the framework, we analyze two
dominant paradigms: self-consistency and perplexity, and reveal key
limitations: self-consistency suffers from high estimation error while
perplexity exhibits substantial modeling error and possible degradation of the
estimation error convergence. To address these limitations, we introduce RPC, a
hybrid method that leverages our theoretical insights through two key
components: Perplexity Consistency and Reasoning Pruning. Perplexity
Consistency combines the strengths of self-consistency and perplexity, boosting
the convergence rate of estimation error from linear to exponential while
preserving model error. Reasoning Pruning prevents degradation by eliminating
low-probability reasoning paths. Both theoretical analysis and empirical
results across seven benchmark datasets demonstrate that RPC has a strong
potential for reducing reasoning error. Notably, RPC achieves reasoning
performance comparable to self-consistency while not only enhancing confidence
reliability but also reducing sampling costs by 50%. The code and resources are
available at https://wnjxyk.github.io/RPC.

</details>


### [74] [Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment](https://arxiv.org/abs/2510.15456)
*Jan Corazza,Hadi Partovi Aria,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: The paper introduces Temporal Logic-based Causal Diagrams (TLCD) into Probabilistic Reward Machines (PRMs) to handle sparse, temporally dependent rewards in RL, aiming to speed up learning and improve transfer to new environments; it provides a convergence guarantee and empirical validation.


<details>
  <summary>Details</summary>
Motivation: RL with sparse rewards struggles to learn optimal policies, and while PRMs capture temporal dependencies and nondeterminism, they are hard to manually design and adapt. Incorporating high-level causal knowledge and domain transfer remains challenging.

Method: Augment PRMs with Temporal Logic-based Causal Diagrams to encode causal relationships and temporal structure within the reward mechanism, enabling faster policy learning and easier transfer. The authors also establish a theoretical convergence guarantee to optimal policy and validate the approach empirically.

Result: Theoretically, the method converges to an optimal policy under stated assumptions. Empirically, the approach demonstrates improved learning speed (sample efficiency) and better transfer performance across environments compared to baselines.

Conclusion: Integrating TL-based causal diagrams into PRMs enhances RL performance by leveraging causal-temporal structure, offering convergence guarantees and practical benefits for learning and transferring task specifications.

Abstract: Reinforcement learning (RL) algorithms struggle with learning optimal
policies for tasks where reward feedback is sparse and depends on a complex
sequence of events in the environment. Probabilistic reward machines (PRMs) are
finite-state formalisms that can capture temporal dependencies in the reward
signal, along with nondeterministic task outcomes. While special RL algorithms
can exploit this finite-state structure to expedite learning, PRMs remain
difficult to modify and design by hand. This hinders the already difficult
tasks of utilizing high-level causal knowledge about the environment, and
transferring the reward formalism into a new domain with a different causal
structure. This paper proposes a novel method to incorporate causal information
in the form of Temporal Logic-based Causal Diagrams into the reward formalism,
thereby expediting policy learning and aiding the transfer of task
specifications to new environments. Furthermore, we provide a theoretical
result about convergence to optimal policy for our method, and demonstrate its
strengths empirically.

</details>


### [75] [Learning to Answer from Correct Demonstrations](https://arxiv.org/abs/2510.15464)
*Nirmit Joshi,Gene Li,Siddharth Bhandari,Shiva Prasad Kasiviswanathan,Cong Ma,Nathan Srebro*

Main category: cs.LG

TL;DR: 对齐问题：在多解答情景下的离线模仿学习，放弃仅靠最大似然估计，转而在奖励函数低基数假设下实现对数样本复杂度的学习。


<details>
  <summary>Details</summary>
Motivation: 传统SFT假设示范者属于低复杂度策略类，迫使使用最大似然估计。作者提出更弱的假设：奖励模型的基数低（高层次地说，正确答案集合的类别数有限），从而挑战仅通过MLE进行学习的必要性，并寻求更高效的样本利用。

Method: 在离线情境中的上下文带尾部（contextual bandits）设定，基于来自最优策略的演示，但没有显式奖励。提出一种替代方法，避免对数似然优化，设计在奖励类（cardinality）大小的对数级别的样本复杂度的学习算法。

Result: 理论上证明在奖励类低基数条件下，最大似然方法可能失败；提出的新方法实现对数级别的（与奖励类基数相关的）样本复杂度。未在摘要中给出具体实验结果，但给出对策性理论结论。

Conclusion: 强调在从正确示例学习时，需超越单纯最大似然的策略，呼吁在学习过程中引入对奖励结构的更弱、但有效的建模假设。

Abstract: We study the problem of learning to generate an answer (or completion) to a
question (or prompt), where there could be multiple correct answers, any one of
which is acceptable at test time. Learning is based on demonstrations of some
correct answer to each training question, as in Supervised Fine Tuning (SFT).
We formalize the problem as offline imitation learning in contextual bandits,
with demonstrations from some optimal policy, without explicitly observed
rewards. Prior work assumes that the demonstrator belongs to a low-complexity
policy class, which motivates maximum likelihood estimation (i.e., log-loss
minimization). In contrast, we propose relying only on the reward model
(specifying which answers are correct) being in a low-cardinality class, which
we argue is a weaker assumption. We show that likelihood maximization methods
can fail in this case, and instead devise an alternative novel approach that
learns with sample complexity logarithmic in the cardinality of the reward
class. Our work motivates looking beyond likelihood maximization when learning
from correct demonstrations.

</details>


### [76] [Adversary-Free Counterfactual Prediction via Information-Regularized Representations](https://arxiv.org/abs/2510.15479)
*Shiqin Tang,Rong Feng,Shuxin Zhuang,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: 提出一个信息理论框架来实现反事实预测的无偏化，通过学习一个最小化与治疗的互信息I(Z; T)的随机表示Z，并结合一个监督解码器，形成可训练、稳定的目标；可扩展到动态情境，在数值仿真与真实临床数据集上优于对比基线，且避免对抗训练的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决赋值偏差下的反事实预测问题，目标是去除治疗与协变量之间的依赖性，以获得更稳健、可解释的反事实推断。

Method: 从一个将反事实与事实风险差与互信息联系起来的界出发，学习一个能预测结果但尽量降低I(Z; T)的随机表示Z；推导可跟随的变分目标，该目标上界信息项并与一个有监督解码器耦合，形成稳定且理论动机充分的训练准则；框架可自然扩展到序列决策时间点的动态设置。

Result: 在控制性数值仿真与真实世界临床数据集上，与最近的平衡、重加权和对抗基线相比，在似然性、反事实误差和策略评估等指标上表现良好，同时避免了对抗性训练的训练不稳定性与超参调优负担。

Conclusion: 提出的基于信息的框架为无偏反事实预测提供了一种稳健且理论驱动的途径，尤其适用于动态决策情境，并能在多种基线任务中获得竞争性甚至优越的结果。

Abstract: We study counterfactual prediction under assignment bias and propose a
mathematically grounded, information-theoretic approach that removes
treatment-covariate dependence without adversarial training. Starting from a
bound that links the counterfactual-factual risk gap to mutual information, we
learn a stochastic representation Z that is predictive of outcomes while
minimizing I(Z; T). We derive a tractable variational objective that
upper-bounds the information term and couples it with a supervised decoder,
yielding a stable, provably motivated training criterion. The framework extends
naturally to dynamic settings by applying the information penalty to sequential
representations at each decision time. We evaluate the method on controlled
numerical simulations and a real-world clinical dataset, comparing against
recent state-of-the-art balancing, reweighting, and adversarial baselines.
Across metrics of likelihood, counterfactual error, and policy evaluation, our
approach performs favorably while avoiding the training instabilities and
tuning burden of adversarial schemes.

</details>


### [77] [OffSim: Offline Simulator for Model-based Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2510.15495)
*Woo-Jin Ahn,Sang-Ryul Baek,Yong-Jun Lee,Hyun-Duck Choi,Myo-Taeg Lim*

Main category: cs.LG

TL;DR: 提出 Offline Simulator OffSim，结合离线逆强化学习和模型学习，从专家轨迹中还原环境动力学和奖励结构，后续可离线训练策略；OffSim^+ 在多数据集设置加入边际奖励以提升探索。实验证明在 MuJoCo 上优于现有离线 IRL 方法，具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 降低开发复杂的环境模拟器和手工设计奖励函数的成本与工作量，同时提高离线策略训练的探索性与泛化能力；在缺乏交互环境的情况下，通过离线数据实现有效策略学习和奖励学习。

Method: 提出基于模型的离线逆强化学习框架 OffSim：联合优化高熵转移模型与 IRL 奖励函数，以从专家轨迹中学习环境动力学和奖励结构；训练完成后可在不与真实环境交互的情况下离线训练策略。并扩展为 OffSim^+，引入跨数据集的边际奖励以提升探索。

Result: 在 MuJoCo 环境中，与现有离线 IRL 方法相比，OffSim 显著提升了性能，显示了方法的有效性与鲁棒性。

Conclusion: OffSim 为离线环境建模与奖励学习提供一种有效路径，能在无环境交互的前提下实现高效离线策略训练，且通过 OffSim^+ 适用于多数据集设置以增强探索。

Abstract: Reinforcement learning algorithms typically utilize an interactive simulator
(i.e., environment) with a predefined reward function for policy training.
Developing such simulators and manually defining reward functions, however, is
often time-consuming and labor-intensive. To address this, we propose an
Offline Simulator (OffSim), a novel model-based offline inverse reinforcement
learning (IRL) framework, to emulate environmental dynamics and reward
structure directly from expert-generated state-action trajectories. OffSim
jointly optimizes a high-entropy transition model and an IRL-based reward
function to enhance exploration and improve the generalizability of the learned
reward. Leveraging these learned components, OffSim can subsequently train a
policy offline without further interaction with the real environment.
Additionally, we introduce OffSim$^+$, an extension that incorporates a
marginal reward for multi-dataset settings to enhance exploration. Extensive
MuJoCo experiments demonstrate that OffSim achieves substantial performance
gains over existing offline IRL methods, confirming its efficacy and
robustness.

</details>


### [78] [Theoretical Refinement of CLIP by Utilizing Linear Structure of Optimal Similarity](https://arxiv.org/abs/2510.15508)
*Naoki Yoshida,Satoshi Hayakawa,Yuhta Takida,Toshimitsu Uesaka,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出 KME-CLIP，通过在再生核希夫空间的内积来近似 PMI，并利用 PMI 的结构，在多模态对比预训练中实现对 CLIP 的性能提升。


<details>
  <summary>Details</summary>
Motivation: 理论研究表明，模态对之间的最佳相似度应对应该是两种模态之间的点对点互信息（PMI），但现有的 CLIP 及其变体未能充分利用 PMI 的线性结构，因此需要一种方法以更充分地挖掘 PMI 的结构特性。

Method: 通过在再生核希夫空间（RKHS）中使用内积来表示相似度，并借助核均值嵌入（KME）来显式地捕捉 PMI 的结构，提出 KME-CLIP；理论上可任意精度近似 PMI，且在实验中与标准 CLIP 相比在多项任务上取得改进。

Result: 理论上证明了能够以任意精度近似 PMI；在若干检索与分类任务上，实验结果表明 KME-CLIP 效果优于标准 CLIP。

Conclusion: KME-CLIP 将 PMI 的线性结构转化为 RKHS 内积的可控近似，理论与实验均支持其在多模态对比学习中的优越性。

Abstract: In this study, we propose an enhancement to the similarity computation
mechanism in multi-modal contrastive pretraining frameworks such as CLIP. Prior
theoretical research has demonstrated that the optimal similarity metrics
between paired modalities should correspond to the pointwise mutual information
(PMI) between the two modalities. However, the current implementations of CLIP
and its variants fail to fully utilize the underlying linear structure of PMI.
We therefore propose KME-CLIP, which leverages this structure through the inner
product in a reproducing kernel Hilbert space. We theoretically prove that our
method can approximate PMI with arbitrary accuracy and empirically demonstrate
that our approach overall outperforms the standard CLIP formulation across
several retrieval and classification tasks.

</details>


### [79] [Language Models are Injective and Hence Invertible](https://arxiv.org/abs/2510.15511)
*Giorgos Nikolaou,Tommaso Mencattini,Donato Crisostomi,Andrea Santilli,Yannis Panagakis,Emanuele Rodola'*

Main category: cs.LG

TL;DR: Transformer 输入序列到连续表示的映射在理论与实证上是可逆且无碰撞，作者提出 SipIt 算法从隐藏激活重构原始文本，并讨论其对透明度和安全性的影响。


<details>
  <summary>Details</summary>
Motivation: 挑战普遍的非单射观点：尽管激活与归一化等组件常被认为非单射，可能导致信息丢失，本文主张在语言模型中仍可能实现可逆性，提升可解释性和安全性。

Method: 理论：在初始化阶段，离散输入到连续表示的映射是单射，并且这种单射性在训练过程中保持。实证：在六个大规模语言模型上进行亿级碰撞测试，未发现碰撞。应用：提出 SipIt 算法，能够以线性时间重构输入文本，给出对可逆性的可证明保证。

Result: 在大量测试中未检测到任何碰撞，且 SipIt 能在实际中实现对输入的精确重建，具有线性时间复杂度。

Conclusion: 将可逆性视为语言模型的基本且可开发利用的特性，为透明度、可解释性和安全部署提供新的方向。

Abstract: Transformer components such as non-linear activations and normalization are
inherently non-injective, suggesting that different inputs could map to the
same output and prevent exact recovery of the input from a model's
representations. In this paper, we challenge this view. First, we prove
mathematically that transformer language models mapping discrete input
sequences to their corresponding sequence of continuous representations are
injective and therefore lossless, a property established at initialization and
preserved during training. Second, we confirm this result empirically through
billions of collision tests on six state-of-the-art language models, and
observe no collisions. Third, we operationalize injectivity: we introduce
SipIt, the first algorithm that provably and efficiently reconstructs the exact
input text from hidden activations, establishing linear-time guarantees and
demonstrating exact invertibility in practice. Overall, our work establishes
injectivity as a fundamental and exploitable property of language models, with
direct implications for transparency, interpretability, and safe deployment.

</details>


### [80] [An Empirical Study on MC Dropout--Based Uncertainty--Error Correlation in 2D Brain Tumor Segmentation](https://arxiv.org/abs/2510.15541)
*Saumya B*

Main category: cs.LG

TL;DR: MC Dropout的不确定性对脑部肿瘤分割边界误差的指示能力有限；全局相关性低，边界相关性几乎为零，增强方式差异有统计显著但无实际意义。


<details>
  <summary>Details</summary>
Motivation: 评估常用的MC Dropout不确定性是否能有效识别分割错误，尤其是边界区域的错误，以评估在医学图像分割中的实用性和局限性。

Method: 在2D脑部MRI分割中使用U-Net，比较四种数据增强设置（无、水平翻转、旋转、缩放）；对50次随机前向传播估计不确定性；用Pearson与Spearman相关系数将不确定性与像素级错误相关；比较不同增强设置的差异并进行统计检验（p<0.001）。

Result: 整体相关系数约r=0.30–0.38，边界处相关性几乎为零（|r|<0.05）；四种增强设置在统计上显著但在实际应用中差异不具备实用性。

Conclusion: MC Dropout的不确定性对边界误差定位提供的线索有限，需探索替代或混合的不确定性估计方法以改进医学图像分割。

Abstract: Accurate brain tumor segmentation from MRI is vital for diagnosis and
treatment planning. Although Monte Carlo (MC) Dropout is widely used to
estimate model uncertainty, its effectiveness in identifying segmentation
errors -- especially near tumor boundaries -- remains unclear. This study
empirically examines the relationship between MC Dropout--based uncertainty and
segmentation error in 2D brain tumor MRI segmentation using a U-Net trained
under four augmentation settings: none, horizontal flip, rotation, and scaling.
Uncertainty was computed from 50 stochastic forward passes and correlated with
pixel-wise errors using Pearson and Spearman coefficients. Results show weak
global correlations ($r \approx 0.30$--$0.38$) and negligible boundary
correlations ($|r| < 0.05$). Although differences across augmentations were
statistically significant ($p < 0.001$), they lacked practical relevance. These
findings suggest that MC Dropout uncertainty provides limited cues for boundary
error localization, underscoring the need for alternative or hybrid uncertainty
estimation methods in medical image segmentation.

</details>


### [81] [GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device](https://arxiv.org/abs/2510.15620)
*Jiahao Zhou,Chengliang Lin,Dingji Li,Mingkai Dong,Haibo Chen*

Main category: cs.LG

TL;DR: GRATING is a training-free, end-to-end inference system for on-device semantic top-K reranking. It uses monolithic forwarding with progressive cluster pruning over a global view of candidates to prune early, reducing latency and peak memory without sacrificing precision.


<details>
  <summary>Details</summary>
Motivation: Edge devices struggle with latency and memory in cross-encoder top-K selection for retrieval-augmented generation, agent memory, and personalized recommendations; there is a need to reduce compute and memory without accuracy loss.

Method: Proposes monolithic forwarding and training-free inference, GRATING. Maintains a global view of all candidates and performs progressive cluster pruning to reduce work. Uses dual-layer sliding window and chunked execution to overlap I/O with computation and bound peak memory.

Result: In microbenchmarks, GRATING reduces latency by up to 89.0% and peak memory by up to 94.9% across 0.6B–8B parameter rerankers on Apple M2 and RTX 5070. In real-world applications, latency reduces 11.6%–51.0% and peak memory 18.6%–77.8%.

Conclusion: GRATING offers substantial efficiency gains for on-device semantic top-K reranking by a trained-free, architecture that preserves precision and improves deployability.

Abstract: Semantic top-K selection with cross-encoder rerankers underpins of on-device
AI services, such as retrieval-augmented generation, agent memory, and
personalized recommendation. However, its latency and memory demands dominate
end-to-end budgets on edge hardware. Revisiting the objective of top-K
selection, we reveal that only relative rankings matter, not exact
per-candidate scores. We further observe sequence-level sparsity: relative
rankings stabilize early in intermediate layers, allowing pruning opportunities
prior to completing full inference.
  Building on this insight, we propose monolithic forwarding and develop a
training-free inference system, GRATING. By maintaining a global view of all
candidates, it reduces latency through progressive cluster pruning. It also
bounds peak memory usage by strategically overlapping I/O with computation via
dual-layer sliding window and chunked execution. We evaluate GRATING against
state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple
M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak
memory by up to 94.9% in microbenchmarks, without any loss in precision. Across
three real-world on-device AI applications, GRATING lowers latency by
11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial
improvements in efficiency and deployability.

</details>


### [82] [CQD-SHAP: Explainable Complex Query Answering via Shapley Values](https://arxiv.org/abs/2510.15623)
*Parsa Abbasi,Stefan Heindorf*

Main category: cs.LG

TL;DR: CQD-SHAP 引入基于 Shapley 值的解释框架，用于对不完整知识图中的神经/神经符号型复杂查询问答(CQA)的排名结果进行逐查询分量贡献分析。


<details>
  <summary>Details</summary>
Motivation: 提升对神经/神经符号型 CQA 的可解释性和用户信任度，通过量化查询各组成部分对目标答案排名的影响来揭示推理过程。

Method: 基于协作博弈论中的 Shapley 值，量化每个查询片段对最终答案排名的贡献，形成解释并且满足 Shapley 公理；将其应用于 CQD 框架中的神经预测器，处理对不完整 KG 的推理。

Result: 自动评估显示该解释在大多数查询类型上有效，能够提供必要和充分的解释，并与多种基线比较时表现出色。

Conclusion: Shapley 基于的解释提供了一个原理上可辩护、透明的机制，有助于理解和增强对不完整 KG 上神经/神经符号型 CQA 系统的信任与可用性。

Abstract: Complex query answering (CQA) goes beyond the well-studied link prediction
task by addressing more sophisticated queries that require multi-hop reasoning
over incomplete knowledge graphs (KGs). Research on neural and neurosymbolic
CQA methods is still an emerging field. Almost all of these methods can be
regarded as black-box models, which may raise concerns about user trust.
Although neurosymbolic approaches like CQD are slightly more interpretable,
allowing intermediate results to be tracked, the importance of different parts
of the query remains unexplained. In this paper, we propose CQD-SHAP, a novel
framework that computes the contribution of each query part to the ranking of a
specific answer. This contribution explains the value of leveraging a neural
predictor that can infer new knowledge from an incomplete KG, rather than a
symbolic approach relying solely on existing facts in the KG. CQD-SHAP is
formulated based on Shapley values from cooperative game theory and satisfies
all the fundamental Shapley axioms. Automated evaluation of these explanations
in terms of necessary and sufficient explanations, and comparisons with various
baselines, shows the effectiveness of this approach for most query types.

</details>


### [83] [Fast and Compact Tsetlin Machine Inference on CPUs Using Instruction-Level Optimization](https://arxiv.org/abs/2510.15653)
*Yefan Zeng,Shengyu Duan,Rishad Shafik,Alex Yakovlev*

Main category: cs.LG

TL;DR: 提出了一种基于位操作的快速 Tsetlin Machine 软件实现，结合早期退出和文字量化重排策略，在 post-training 的前推阶段进行统计分析，以显著降低推理时间。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的设备上，Tsetlin Machine 具有高效推理潜力；其逻辑驱动结构和并行性适合 CPU 的指令级并行，因此通过高效实现和优化，可在实际部署中获得显著速度提升，同时保持模型密度。

Method: 通过利用指令级位操作实现紧凑模型表示和加速处理；引入基于 AND 的子句评估的早期退出机制以避免不必要计算；提出文字量化重排策略，在训练后、推理前阶段通过对所有文字及其 TA 动作的统计分析进行优化，且运行时开销可忽略。实验在 gem5 的 ARM 模拟器上进行，比较的是与传统基于整数的 TM 实现的推理时间。

Result: 在 gem5 模拟器上的 ARM 处理器上，所提出的优化将推理时间缩短高达 96.71%，并保持了可比较的代码密度。

Conclusion: 通过结合位级实现、早期退出与文字重排策略，TM 的 CPU 实现可获得显著的推理加速，且对运行时开销影响极小，适用于资源受限场景的高效推理部署。

Abstract: The Tsetlin Machine (TM) offers high-speed inference on resource-constrained
devices such as CPUs. Its logic-driven operations naturally lend themselves to
parallel execution on modern CPU architectures. Motivated by this, we propose
an efficient software implementation of the TM by leveraging instruction-level
bitwise operations for compact model representation and accelerated processing.
To further improve inference speed, we introduce an early exit mechanism, which
exploits the TM's AND-based clause evaluation to avoid unnecessary
computations. Building upon this, we propose a literal Reorder strategy
designed to maximize the likelihood of early exits. This strategy is applied
during a post-training, pre-inference stage through statistical analysis of all
literals and the corresponding actions of their associated Tsetlin Automata
(TA), introducing negligible runtime overhead. Experimental results using the
gem5 simulator with an ARM processor show that our optimized implementation
reduces inference time by up to 96.71% compared to the conventional
integer-based TM implementations while maintaining comparable code density.

</details>


### [84] [WARP-LUTs - Walsh-Assisted Relaxation for Probabilistic Look Up Tables](https://arxiv.org/abs/2510.15655)
*Lino Gerlach,Liv Våge,Thore Gerlach,Elliott Kauffman*

Main category: cs.LG

TL;DR: WARP-LUTs 是一种基于 Walsh 变换的协助梯度学习方法，通过对概率查找表进行 Walsh-Assisted Relaxation，以更少的可训练参数学习高效的逻辑门组合，在 CIFAR-10 上实现比 DLGN 更快的收敛速度，且保持可比的精度；具备扩展到更高输入逻辑块和在 FPGA 上实时部署的潜力。


<details>
  <summary>Details</summary>
Motivation: 提升逻辑门组合学习的训练效率与参数效率，降低对资源的需求，并推动在硬件友好的模型设计中实现更快的收敛与部署潜力。

Method: 提出 WARP-LUTs：一种基于 Walsh-Assisted Relaxation 的梯度学习框架，用概率 LUT 组合来近似逻辑门集合，显著减少需要训练的参数量，并提高训练速度与可扩展性。

Result: 实验在 CIFAR-10 上显示 WARP-LUTs 相较 DLGN 具有更快的收敛速度，同时保持与 DLGN 相似的准确度，表明在保持性能的前提下实现了参数与计算效率的提升。

Conclusion: WARP-LUTs 展示了对高输入逻辑块的潜在扩展性，并为在现代 FPGA 的高效部署和实时科学应用提供了可行路径；未来工作可能聚焦于对更高输入规模的逻辑块、更多数据集的评估以及硬件实现细化。

Abstract: Fast and efficient machine learning is of growing interest to the scientific
community and has spurred significant research into novel model architectures
and hardware-aware design. Recent hard? and software co-design approaches have
demonstrated impressive results with entirely multiplication-free models.
Differentiable Logic Gate Networks (DLGNs), for instance, provide a
gradient-based framework for learning optimal combinations of low-level logic
gates, setting state-of-the-art trade-offs between accuracy, resource usage,
and latency. However, these models suffer from high computational cost during
training and do not generalize well to logic blocks with more inputs. In this
work, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables
(WARP-LUTs) - a novel gradient-based method that efficiently learns
combinations of logic gates with substantially fewer trainable parameters. We
demonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10
compared to DLGNs, while maintaining comparable accuracy. Furthermore, our
approach suggests potential for extension to higher-input logic blocks,
motivating future research on extremely efficient deployment on modern FPGAs
and its real-time science applications.

</details>


### [85] [CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning](https://arxiv.org/abs/2510.15674)
*Yung-Chen Tang,Pin-Yu Chen,Andrea Cavallaro*

Main category: cs.LG

TL;DR: 提出一个测试时校准框架，CarBoN，通过输入特定温度和偏置向量对 logits 进行校准，在不重新训练模型的情况下提高推理阶段的效率和准确性，特别在最佳N采样中减少 rollouts 并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 解决在推理时增加计算可带来性能提升，但 Best-of-N 的收益递减问题；需要一种无需重新训练就能系统性引导模型朝高奖励推理路径的办法。

Method: 提出两阶段的 CarBoN：①先探索解空间；②再通过对 logits 添加温度 T 与偏移向量 δ 对输入进行针对性校准，输出更可靠的推理结果。理论上在有限采样下能提升期望奖励的下界，并可推广至 Beam Search 等步级采样策略。

Result: 在 MATH-500 和 AIME-2024 上的实验表明，CarBoN 可在同等准确度下显著降低需要的滚动探索次数（最多可少 4 倍），并在固定预算下往往获得更高的准确率，同时解析了 T 与 δ 在输出多样性和正确性之间的权衡。

Conclusion: 该框架为推理阶段的高效自适应校准提供理论与实证支持，且具备对不同采样策略的泛化潜力，项目页提供了实现信息。

Abstract: Allocating more computation during inference time (test-time scaling)
improves language model performance, especially for reasoning tasks. However,
popular methods like Best-of-$N$ sampling often show diminishing returns as $N$
increases. To address this inefficiency, we introduce a general test-time
calibration framework that adaptively modifies the model toward high-reward
reasoning paths, with theoretical guarantees of improving the lower bound of
expected reward under finite sampling, all without large language model (LLM)
retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),
a two-phase method that first explores the solution space and then learns a
calibration of the logits via an input-specific temperature $T$ and additive
shift vector $\delta$, guiding generation toward more reliable reasoning.
Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,
with up to $4\times$ fewer rollouts to reach the same accuracy, while often
achieving higher accuracy under fixed budgets. We also analyze the
complementary roles of $T$ and $\delta$ in balancing output diversity and
correctness, and demonstrate that the framework also generalizes to step-level
sampling strategies such as beam search. For more information, please refer to
our project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.

</details>


### [86] [ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations](https://arxiv.org/abs/2510.15700)
*Alex Gu,Bartosz Piotrowski,Fabian Gloeckle,Kaiyu Yang,Aram H. Markosyan*

Main category: cs.LG

TL;DR: ProofOptimizer 是首个在 Lean 上专门训练用于简化证明的语言模型，采用专家迭代与强化学习，能够在推理中迭代压缩长证明并提升后续训练效果。


<details>
  <summary>Details</summary>
Motivation: 大规模形式化证明极长且难以理解，现有训练数据稀缺，且基于 off-the-shelf LLM 的方法在极长证明上效果有限，因此需要自主学习的简化能力。

Method: 通过 expert iteration 与强化学习在 Lean 验证的信号下训练模型；在推理阶段采用迭代的证明缩短工作流，将简化结果用于训练信号； Lean 用于验证简化并提供反馈。

Result: 在 miniF2F、PutnamBench、Seed-Prover 的 IMO 2025 证明上分别实现了 87%、57%、49% 的证明长度缩短；简化证明在 Lean 校验更快，并提升了后续监督微调中训练数据的效用，改善下游证明器性能。

Conclusion: ProofOptimizer 提供无需额外人类标注即可训练的证明简化能力，显著提高证明的可读性与验证速度，并有助于提升自动证明系统的整体性能。

Abstract: Neural theorem proving has advanced rapidly in the past year, reaching IMO
gold-medalist capabilities and producing formal proofs that span thousands of
lines. Although such proofs are mechanically verified by formal systems like
Lean, their excessive length renders them difficult for humans to comprehend
and limits their usefulness for mathematical insight. Proof simplification is
therefore a critical bottleneck. Yet, training data for this task is scarce,
and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --
struggle with the extremely long proofs generated by RL-trained provers. We
introduce ProofOptimizer, the first language model trained to simplify Lean
proofs without requiring additional human supervision. ProofOptimizer is
trained via expert iteration and reinforcement learning, using Lean to verify
simplifications and provide training signal. At inference time, it operates
within an iterative proof-shortening workflow, progressively reducing proof
length. Experiments show that ProofOptimizer substantially compresses proofs
generated by state-of-the-art RL-trained provers on standard benchmarks,
reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on
Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check
faster in Lean and further improve downstream prover performance when reused as
training data for supervised finetuning.

</details>


### [87] [ProSh: Probabilistic Shielding for Model-free Reinforcement Learning](https://arxiv.org/abs/2510.15720)
*Edwin Hamel-De le Court,Gaspard Ohlmann,Francesco Belardinelli*

Main category: cs.LG

TL;DR: ProSh 是一种模型无关的安全强化学习算法，通过在状态空间中加入风险预算并用成本评估器对策略分布进行护盾约束，从而在期望意义上确保所有采样动作安全；在确定性环境中可保持最优性，训练阶段在一定假设下也能保证安全，并给出基于备份成本评估精度的期望成本上界。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中实现可部署的安全性，需要对成本进行约束并提供形式化的安全保证；现有方法往往缺乏训练过程中的安全性保证或对成本的严格上界。ProSh 提出通过风险预算和护盾来实现对策略的安全约束，同时维持无模型的学习特性。

Method: 将 Constrained MDP 的状态空间扩展为包含风险预算；学习一个成本评估器(风险评估器)来对策略分布进行护盾，使所采样的动作在期望意义上是安全的；在确定性环境下可保持最优性；训练时对成本给出通过备份成本评估的紧上界，确保在训练过程中的安全性；为在训练中实现安全提供理论界和经验验证。

Result: 在确定性环境下保持最优性；给出期望成本的紧上界，仅依赖备份评估的准确性；实验表明在温和假设下训练期间也能实现安全。

Conclusion: ProSh 提供一种可用于安全 RL 的实用框架，通过风险预算和护盾实现期望安全和训练时的安全性，适用于模型无关的学习场景，并在确定性环境中保留最优性。

Abstract: Safety is a major concern in reinforcement learning (RL): we aim at
developing RL systems that not only perform optimally, but are also safe to
deploy by providing formal guarantees about their safety. To this end, we
introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free
algorithm for safe reinforcement learning under cost constraints. ProSh
augments the Constrained MDP state space with a risk budget and enforces safety
by applying a shield to the agent's policy distribution using a learned cost
critic. The shield ensures that all sampled actions remain safe in expectation.
We also show that optimality is preserved when the environment is
deterministic. Since ProSh is model-free, safety during training depends on the
knowledge we have acquired about the environment. We provide a tight
upper-bound on the cost in expectation, depending only on the backup-critic
accuracy, that is always satisfied during training. Under mild, practically
achievable assumptions, ProSh guarantees safety even at training time, as shown
in the experiments.

</details>


### [88] [RLAF: Reinforcement Learning from Automaton Feedback](https://arxiv.org/abs/2510.15728)
*Mahyar Alinejad,Alvaro Velasquez,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: 一个基于确定性有限自动机(DFA)的偏好学习框架，用于在强化学习中处理非马尔可夫奖励。通过从DFA结构生成轨迹偏好来学习奖励函数，分为静态直接用于优化和动态迭代更新两种模式，且在离散与连续环境中表现优越，具有收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 传统RL在具有历史依赖的奖励结构中难以设计有效奖励函数。利用自动机来生成轨迹偏好以学习奖励函数，避免手工奖励设计，并解决非马尔可夫性问题。

Method: 利用DFA的轨迹偏好来学习奖励函数；静态方法直接使用学习得到的奖励函数进行策略优化；动态方法在迭代中持续 refine 奖励函数和策略直至收敛。

Result: 在离散和连续环境中，該方法优于传统奖励工程和基线（如奖励机关、LTL引导方法），能处理非马尔可夫奖励并具可扩展性、效率与无人工干预的优势。给出收敛性保证，在标准假设下，所学策略在近似最优水平接近真实非马尔可夫目标。

Conclusion: 自动机引导的偏好在处理非马尔可夫奖励方面有效，提供一种可扩展、无需人工设计的高效替代方案，且具备理论收敛性。

Abstract: Reinforcement Learning (RL) in environments with complex, history-dependent
reward structures poses significant challenges for traditional methods. In this
work, we introduce a novel approach that leverages automaton-based feedback to
guide the learning process, replacing explicit reward functions with
preferences derived from a deterministic finite automaton (DFA). Unlike
conventional approaches that use automata for direct reward specification, our
method employs the structure of the DFA to generate preferences over
trajectories that are used to learn a reward function, eliminating the need for
manual reward engineering. Our framework introduces a static approach that uses
the learned reward function directly for policy optimization and a dynamic
approach that involves continuous refining of the reward function and policy
through iterative updates until convergence.
  Our experiments in both discrete and continuous environments demonstrate that
our approach enables the RL agent to learn effective policies for tasks with
temporal dependencies, outperforming traditional reward engineering and
automaton-based baselines such as reward machines and LTL-guided methods. Our
results highlight the advantages of automaton-based preferences in handling
non-Markovian rewards, offering a scalable, efficient, and human-independent
alternative to traditional reward modeling. We also provide a convergence
guarantee showing that under standard assumptions our automaton-guided
preference-based framework learns a policy that is near-optimal with respect to
the true non-Markovian objective.

</details>


### [89] [A Comprehensive Evaluation of Graph Neural Networks and Physics Informed Learning for Surrogate Modelling of Finite Element Analysis](https://arxiv.org/abs/2510.15750)
*Nayan Kumar Singh*

Main category: cs.LG

TL;DR: GNNs (especially MPNN PINN and Graph Transformer) outperform 3D U-Nets as FEA surrogates for parametric I-beams; physics-informed curriculum learning improves generalization; Graph Transformer is most accurate but slower, while MPNN PINN offers the best practical trade-off.


<details>
  <summary>Details</summary>
Motivation: Finite Element Analysis is computationally expensive for design optimization. The study aims to compare graph-based and voxel-based surrogates, and to assess physics-informed training (PINN) with curriculum learning.

Method: Evaluate multiple GNN architectures (including GCN, MPNN, Graph Transformer, and their PINN variants) and 3D U-Nets as surrogates for FEA of parametric I-beams. Use Navier–Cauchy-based PINN to enforce physics, with curriculum learning (pretraining on data followed by physics-informed fine-tuning). Compare relative L2 errors and inference speeds across architectures.

Result: Among graph-based models, MPNN and Graph Transformer achieve the highest accuracy with relative L2 errors of 3.5% and 2.6% respectively, while the worst GNN (GCN) attains 8.7%. Among U-Nets, the best is 13.0%. Physics-informed training (PINN) significantly improves generalization, reducing error by up to 11.3% on high-signal tasks. The Graph Transformer is most accurate but 37.5% slower in inference than the second-best model (MPNN PINN). The PINN-enhanced MPNN (MPNN PINN) offers the best practical compromise between predictive performance, model size, and inference speed.

Conclusion: Graph-based surrogates with physics-informed curriculum learning outperform 3D U-Nets for FEA surrogates of parametric I-beams. The MPNN PINN combination provides the best balance of accuracy, efficiency, and practicality, while Graph Transformer achieves the highest accuracy at the cost of slower inference.

Abstract: Although Finite Element Analysis (FEA) is an integral part of the product
design lifecycle, the analysis is computationally expensive, making it
unsuitable for many design optimization problems. The deep learning models can
be a great solution. However, selecting the architecture that emulates the FEA
with great accuracy is a challenge. This paper presents a comprehensive
evaluation of graph neural networks (GNNs) and 3D U-Nets as surrogates for FEA
of parametric I-beams. We introduce a Physics-Informed Neural Network (PINN)
framework, governed by the Navier Cauchy equations, to enforce physical laws.
Crucially, we demonstrate that a curriculum learning strategy, pretraining on
data followed by physics informed fine tuning, is essential for stabilizing
training. Our results show that GNNs fundamentally outperform the U-Net. Even
the worst performer among GNNs, the GCN framework, achieved a relative L2 error
of 8.7% while the best framework among U Net, U Net with attention mechanism
trained on high resolution data, achieved 13.0% score. Among the graph-based
architectures, the Message Passing Neural Networks (MPNN) and Graph
Transformers achieved the highest accuracy, achieving a relative L2 score of
3.5% and 2.6% respectively. The inclusion of physics fundamental laws (PINN)
significantly improved the generalization, reducing error by up to 11.3% on
high-signal tasks. While the Graph Transformer is the most accurate model, it
is more 37.5% slower during inference when compared to second best model, MPNN
PINN. The PINN enhanced MPNN (MPNN PINN) provides the most practical solution.
It offers a good compromise between predictive performance, model size, and
inference speed.

</details>


### [90] [SAMix: Calibrated and Accurate Continual Learning via Sphere-Adaptive Mixup and Neural Collapse](https://arxiv.org/abs/2510.15751)
*Trung-Anh Dang,Vincent Nguyen,Ngoc-Son Vu,Christel Vrain*

Main category: cs.LG

TL;DR: 提出 Sphere-Adaptive Mixup (SAMix) 来提升神经崩塌基的持续学习模型的校准与性能，通过自适应混合与神经崩塌几何对齐实现更稳健的正则化与对齐，超过现有方法且提高校准度。


<details>
  <summary>Details</summary>
Motivation: 当前持续学习方法多专注于避免遗忘和提高准确度，往往忽视模型校准的重要性。神经崩塌可降低特征与分类器之间的错配，仍需提升在持续学习场景中的预测可靠性和校准性。因此需要一种能同时提升准确性与校准性的训练策略。

Method: 提出 Sphere-Adaptive Mixup (SAMix)，一种面向神经崩塌基础方法的自适应混合策略。SAMix 根据神经崩塌下特征空间的几何特性自适应地设计混合过程，从而实现更鲁棒的正则化和对齐，提升模型的校准性与鲁棒性。

Result: 实验结果显示，SAMix 显著提升性能，在持续学习领域超过当前的最先进方法，同时改善模型校准，提升跨任务的准确性和预测可靠性。

Conclusion: SAMix 为鲁棒持续学习系统提供了有前景的方向，通过更好地对齐特征与分类器并降低过度自信，兼顾准确性与校准，提升整体系统的可靠性。

Abstract: While most continual learning methods focus on mitigating forgetting and
improving accuracy, they often overlook the critical aspect of network
calibration, despite its importance. Neural collapse, a phenomenon where
last-layer features collapse to their class means, has demonstrated advantages
in continual learning by reducing feature-classifier misalignment. Few works
aim to improve the calibration of continual models for more reliable
predictions. Our work goes a step further by proposing a novel method that not
only enhances calibration but also improves performance by reducing
overconfidence, mitigating forgetting, and increasing accuracy. We introduce
Sphere-Adaptive Mixup (SAMix), an adaptive mixup strategy tailored for neural
collapse-based methods. SAMix adapts the mixing process to the geometric
properties of feature spaces under neural collapse, ensuring more robust
regularization and alignment. Experiments show that SAMix significantly boosts
performance, surpassing SOTA methods in continual learning while also improving
model calibration. SAMix enhances both across-task accuracy and the broader
reliability of predictions, making it a promising advancement for robust
continual learning systems.

</details>


### [91] [Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for Enhanced Welfare and Productivity](https://arxiv.org/abs/2510.15757)
*Pieris Panagi,Savvas Karatsiolis,Kyriacos Mosphilis,Nicholas Hadjisavvas,Andreas Kamilaris,Nicolas Nicolaou,Efstathios Stavrakis,Vassilis Vassiliades*

Main category: cs.LG

TL;DR: PoultryFI是一个低成本、模块化的监控与预测平台，通过六个AI模块实现鸡舍的连续监控、短期预测和生产优化。


<details>
  <summary>Details</summary>
Motivation: 解决小中型养殖场缺乏集成、成本可承受的持续监控与决策工具，提升福利、生产力和合规性。

Method: 离线优化摄像头布局（进化算法）、同步视频/音频/喂食数据的音视频监控、日常摘要与实时提醒、边缘设备的实时蛋计数、基于历史数据的产蛋与能耗预测、将预测与天气数据结合的推荐系统，并在Raspberry Pi 5上实现。

Result: 在田间试验中实现100%蛋计数准确度、鲁棒的异常检测、短期预测可靠性，提供面向全场的智能化解决方案。

Conclusion: PoultryFI弥合了孤立工具与可扩展的全场智能之间的鸿沟，帮助养殖者主动保障福利与盈利。

Abstract: Poultry farming faces increasing pressure to meet productivity targets while
ensuring animal welfare and environmental compliance. Yet many small and
medium-sized farms lack affordable, integrated tools for continuous monitoring
and decision-making, relying instead on manual, reactive inspections. This
paper presents Poultry Farm Intelligence (PoultryFI) - a modular,
cost-effective platform that integrates six AI-powered modules: Camera
Placement Optimizer, Audio-Visual Monitoring, Analytics & Alerting, Real-Time
Egg Counting, Production & Profitability Forecasting, and a Recommendation
Module.
  Camera layouts are first optimized offline using evolutionary algorithms for
full poultry house coverage with minimal hardware. The Audio-Visual Monitoring
module extracts welfare indicators from synchronized video, audio, and feeding
data. Analytics & Alerting produces daily summaries and real-time
notifications, while Real-Time Egg Counting uses an edge vision model to
automate production tracking. Forecasting models predict egg yield and feed
consumption up to 10 days in advance, and the Recommendation Module integrates
forecasts with weather data to guide environmental and operational adjustments.
  This is among the first systems to combine low-cost sensing, edge analytics,
and prescriptive AI to continuously monitor flocks, predict production, and
optimize performance. Field trials demonstrate 100% egg-count accuracy on
Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting.
PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide
intelligence, empowering producers to proactively safeguard welfare and
profitability.

</details>


### [92] [Chronos-2: From Univariate to Universal Forecasting](https://arxiv.org/abs/2510.15821)
*Abdul Fatir Ansari,Oleksandr Shchur,Jaris Küken,Andreas Auer,Boran Han,Pedro Mercado,Syama Sundar Rangapuram,Huibin Shen,Lorenzo Stella,Xiyuan Zhang,Mononito Goswami,Shubham Kapoor,Danielle C. Maddix,Pablo Guerron,Tony Hu,Junming Yin,Nick Erickson,Prateek Mutalik Desai,Hao Wang,Huzefa Rangwala,George Karypis,Yuyang Wang,Michael Bohlke-Schneider*

Main category: cs.LG

TL;DR: Chronos-2 是一个可零样本使用的预训练时间序列模型，能处理单变量、 多变量及带协变量的预测，利用分组注意力和上下文学习实现跨时间序列的信息共享，并在合成数据上训练以覆盖多种结构，达到在 fev-bench、GIFT-Eval 与 Chronos Benchmark II 的最新性能，且在包含协变量的任务上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练时间序列模型多聚焦于单变量预测，且通常需要针对特定任务进行微调，缺乏对多变量和协变量信息的无监督、零样本泛化能力，限制真实世界应用。

Method: 引入分组注意力机制，通过在同一分组内的多条时间序列之间高效信息共享，来实现强大的上下文学习能力（ICL）。将模型在合成数据上进行训练，使单变量序列被赋予多变量结构（如系列集合、变量、目标与协变量等），从而在零样本条件下处理单变量、 多变量及协变量信息的预测任务。

Result: 在三大基准（fev-bench、GIFT-Eval、Chronos Benchmark II）上达到最先进（state-of-the-art）的性能。在 fev-bench 上，模型的通用 ICL 能力带来对多变量与协变量情景的显著改进；在带有协变量的任务中对比基线表现显著领先；能源与零售领域的案例研究进一步体现其实用性。

Conclusion: Chronos-2 可以作为一个通用的预测模型“直接可用”地嵌入实际预测流程，凭借其强大的跨序列信息共享与上下文学习能力，具备广泛的现实场景应用潜力。

Abstract: Pretrained time series models have enabled inference-only forecasting systems
that produce accurate predictions without task-specific training. However,
existing approaches largely focus on univariate forecasting, limiting their
applicability in real-world scenarios where multivariate data and covariates
play a crucial role. We present Chronos-2, a pretrained model capable of
handling univariate, multivariate, and covariate-informed forecasting tasks in
a zero-shot manner. Chronos-2 employs a group attention mechanism that
facilitates in-context learning (ICL) through efficient information sharing
across multiple time series within a group, which may represent sets of related
series, variates of a multivariate series, or targets and covariates in a
forecasting task. These general capabilities are achieved through training on
synthetic datasets that impose diverse multivariate structures on univariate
series. Chronos-2 delivers state-of-the-art performance across three
comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On
fev-bench, which emphasizes multivariate and covariate-informed forecasting,
Chronos-2's universal ICL capabilities lead to substantial improvements over
existing models. On tasks involving covariates, it consistently outperforms
baselines by a wide margin. Case studies in the energy and retail domains
further highlight its practical advantages. The in-context learning
capabilities of Chronos-2 establish it as a general-purpose forecasting model
that can be used "as is" in real-world forecasting pipelines.

</details>


### [93] [SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients](https://arxiv.org/abs/2510.15830)
*Dominik Kallusky,Vinay Rao,Vishal Nandavanam,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: Lookahead 风格的优化器在非分布式训练中通过对伪梯度应用 Nesterov 动量，提出 Step-K Nesterov Outer Optimizer (SNOO)，实现 1.5–2.5 倍的计算效率提升，且提升随模型规模增大而增强，兼容模型分片，适用于常见的内置优化器如 AdamW 与 Muon。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）训练规模的快速增长，迫切需要更高效的优化方法。Lookahead 框架通过快慢权重的双循环降低更新成本，但现有工作多在分布式场景下观察到收益。本文旨在揭示 DiLoCo 的收益本质并将其迁移至非分布式设置，提出一种更普适、低开销的优化器改进。

Method: 对 DiLoCo 的机制进行实证分析，发现对伪梯度应用 Nesterov 动量是收益的关键因素。基于此提出 Step-K Nesterov Outer Optimizer (SNOO) 作为 Lookahead 的变体。通过在非分布式的大规模训练场景中对比常见内优化器（如 AdamW、Muon）及模型分片叉的实现，评估 SNOO 的计算效率（FLOPs 级别）与模型规模相关的收益。

Result: 在非分布式设置下，SNOO 实现 1.5–2.5 倍的计算因子提升，随着训练规模扩展（至高达 1e23 FLOPs）及模型规模增大，效果进一步提升。SNOO 的额外开销极低，且与模型分片兼容，能显著提升如 AdamW、Muon 等内优化器的训练效率。研究还指出 DiLoCo 的收益本质来自伪梯度上的 Nesterov 动量。

Conclusion: SNOO 为多种内优化器提供一个高效的实用增强，具有较小的计算与内存开销，并且与模型分片等分布式训练技巧兼容。该方法在非分布式大规模训练中具有显著的性能收益，值得在更广泛的场景中应用与进一步验证。

Abstract: The rapid development of large language models (LLMs) has driven the demand
for more efficient optimization techniques. Among these, the Lookahead family
of optimizers employs a two-loop framework, maintaining fast and slow sets of
model weights. Multiple inner optimizer steps on the fast weights produce a
trajectory - the pseudo-gradient - that is used to update the slow weights.
DiLoCo, a notable example originally designed for distributed training, applies
Nesterov momentum to the averaged pseudo-gradient from multiple workers,
claiming to even outperform AdamW in a non-distributed setup. In this paper, we
empirically show that DiLoCo's surprising effectiveness stems primarily from
applying Nesterov momentum to the pseudo-gradient, which improves training in a
non-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov
Outer Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains
of 1.5 - 2.5$\times$ in a non-distributed setting up to a scale of 1e23
training FLOPs, with improvements that increase with model size. Because of its
minimal compute and memory overhead and compatibility with model sharding, SNOO
is a practical enhancement for a variety of inner optimizers, including AdamW
and Muon.

</details>


### [94] [Learning Correlated Reward Models: Statistical Barriers and Opportunities](https://arxiv.org/abs/2510.15839)
*Yeshwanth Cherapanamjeri,Constantinos Daskalakis,Gabriele Farina,Sobhan Mohammadpour*

Main category: cs.LG

TL;DR: 通过引入可相关的Probit模型解决IIA局限，论文指出两两偏好数据无法学习相关信息；提出最佳-三偏好数据可克服这一缺点，并给出高效且近最优的估计器，且在真实数据集上验证了个性化改进。


<details>
  <summary>Details</summary>
Motivation: IIA假设使RUM对人类偏好仅能被一个全局效用函数近似，导致对偏好多样性的粗糙刻画。研究旨在探讨学习相关效用的统计与计算挑战，并评估是否通过更高阶的偏好数据提升建模能力。

Method: 首先给出两两偏好数据在学习相关信息方面的根本不足的理论证明。随后证明采用最佳-三偏好数据可以克服这些不足，并提出一个统计上高效、计算上可行的估计器，达到接近最优的样本复杂度与计算复杂度。理论分析包括存在性、可识别性与渐近性质。

Result: 结果显示：1) 两两偏好数据无法提供学习相关信息的必要信息，导致缺乏统计和计算保障；2) 最佳-三偏好数据能提供所需的相关信息，且提出的估计器在理论上接近最优性能（近似最优样本复杂度/时间复杂度）；3) 在若干真实数据集上驗证，该方法能提升对人类偏好的个性化程度。

Conclusion: 引入高阶偏好数据（最佳-三）显著提升对相关效用的学习能力，为VRHF等场景下的个性化和更细粒度的人类偏好建模提供理论与实践支撑；未来工作可拓展到更广的RUM族及鲁棒性、大规模数据场景。

Abstract: Random Utility Models (RUMs) are a classical framework for modeling user
preferences and play a key role in reward modeling for Reinforcement Learning
from Human Feedback (RLHF). However, a crucial shortcoming of many of these
techniques is the Independence of Irrelevant Alternatives (IIA) assumption,
which collapses \emph{all} human preferences to a universal underlying utility
function, yielding a coarse approximation of the range of human preferences. On
the other hand, statistical and computational guarantees for models avoiding
this assumption are scarce. In this paper, we investigate the statistical and
computational challenges of learning a \emph{correlated} probit model, a
fundamental RUM that avoids the IIA assumption. First, we establish that the
classical data collection paradigm of pairwise preference data is
\emph{fundamentally insufficient} to learn correlational information,
explaining the lack of statistical and computational guarantees in this
setting. Next, we demonstrate that \emph{best-of-three} preference data
provably overcomes these shortcomings, and devise a statistically and
computationally efficient estimator with near-optimal performance. These
results highlight the benefits of higher-order preference data in learning
correlated utilities, allowing for more fine-grained modeling of human
preferences. Finally, we validate these theoretical guarantees on several
real-world datasets, demonstrating improved personalization of human
preferences.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [95] [A Structured Family of Grassmannian Constellations via Geodesic Mapping for MIMO Noncoherent Communications](https://arxiv.org/abs/2510.15070)
*Álvaro Pendás-Recondo,Enrique Pendás-Recondo*

Main category: eess.SP

TL;DR: 提出基于Grassmann流形几何的结构化星座，用于MIMO非相干通信；输出矩阵每行仅一个非零元素，允许单天线工作以降低硬件成本与功耗；在星座规模上限为4M^2时，SER与最优的无结构设计接近，同时ML检测复杂度可降低一个数量级的因子M；谱效率较低，约0.25–1 bps/Hz


<details>
  <summary>Details</summary>
Motivation: 在无CSI的MIMO非相干场景中，需要低复杂度且硬件友好的星座设计；利用Grassmann流形的几何结构和测地线来构造结构化星座，提高实现的可行性和效率。

Method: 基于Grassmann流形上的测地线设计星座，使得所有码字矩阵在每一行只有一个非零条目，从而实现单天线传输；星座大小受限为4M^2；通过SER、BER、比特标签及ML检测复杂度分析对比验证性能和复杂度

Result: 在给定星座规模下，结构化Grassmannian星座的SER与无结构的最优设计相当；实现了简易的比特标签，BER/ SER对比显示良好性能；ML检测的计算复杂度比常规设计降低了一个因子M

Conclusion: 结构化Grassmannian星座提供了硬件友好且功耗更低的非相干MIMO设计选项，尽管谱效率受限，但在星座规模内可实现与无结构设计竞争的误码性能并显著降低检测复杂度。

Abstract: This work presents a novel structured family of Grassmannian constellations
for multiple-input multiple-output (MIMO) noncoherent communications over
Rayleigh block-fading channels, where neither the transmitter nor the receiver
has channel state information (CSI). The proposed constellation design is built
upon the geodesic curves of the Grassmann manifold, thereby exploiting its
underlying geometric structure. The resulting solution is limited in spectral
efficiency (with a maximum constellation size of $4M^2$ points, where $M$ is
the number of transmit antennas), targeting a rate in the range of $0.25$-$1$
bps/Hz. However, all space-time matrices resulting from this design exhibit the
remarkable property of having a single nonzero entry per row, meaning that only
one transmit antenna is active per time slot. This property significantly
reduces hardware complexity and implementation cost, while also lowering power
consumption, as only a single power amplifier is required for transmission.
Furthermore, within the constellation size limits, the proposed design achieves
error performance comparable to state-of-the-art optimization-based
unstructured designs, as validated through symbol error rate (SER) numerical
results. It also enables simple yet effective bit labeling, confirmed by
comparisons of bit error rate (BER) and SER, and reduces the computational
complexity of the maximum-likelihood (ML) detector for Grassmannian
constellations by a factor of $M$.

</details>


### [96] [Pulse Shaping Filter Design for Integrated Sensing & Communication with Zak-OTFS](https://arxiv.org/abs/2510.15195)
*Nishant Mehrotra,Sandesh Rao Mattu,Robert Calderbank*

Main category: eess.SP

TL;DR: 通过 IOTA 设计的脉冲整形滤波器，在 Zak-OTFS ISAC 框架下实现局部化、正交性以及带宽/时间受限三者兼具，从而提升感知与通信的综合性能。


<details>
  <summary>Details</summary>
Motivation: 在高时延/多普勒环境中实现高效的 ISAC 需要同时优化感知（I/O 近似理想关系的估计）与通信（无符号干扰与高谱效率）。现有滤波器往往只能满足两项要求，因此需要一种同时满足三者的新设计。

Method: 利用 Isotropic Orthogonal Transform Algorithm (IOTA) 设计新的脉冲形状，以在 Zak-OTFS 框架下同时满足局部化、正交性和带宽/时间受限。

Result: 所提出的脉冲整形滤波器在数据检测和 I/O 关系估计方面相较现有滤波器具有性能提升。

Conclusion: 解决了三大相互制约的目标在 ISAC 框架下的兼容性难题，IOTA 基于的设计为实现感知与通信的综合优化提供可行的滤波解决方案。

Abstract: Zak-OTFS is an emerging framework for integrated sensing & communication
(ISAC) in high delay and Doppler spread environments. A critical enabler for
ISAC with Zak-OTFS is the design of pulse shaping filters. For sensing, a
localized pulse shaping filter enables ideal input-output (I/O) relation
estimates close to the physical scattering channel. For communication,
orthogonality of the pulse shape on the information lattice prevents
inter-symbol interference, and no time and bandwidth expansion enables full
spectral efficiency. A filter simultaneously meeting all three objectives is
ideal for ISAC. Existing filter designs achieve two of the above objectives,
but not all three simultaneously. For instance, the sinc filter is orthogonal
and bandwidth/time-limited, but is not localized. The Gaussian filter is
localized and bandwidth/time-limited, but not orthogonal. The RRC filter is
localized and orthogonal, but not bandwidth/time-limited. A recently proposed
hybrid Gaussian-sinc filter is more localized than the sinc filter and
bandwidth/time-limited, but is not orthogonal. In this work, we design optimal
pulse shaping filters meeting all three objectives via the Isotropic Orthogonal
Transform Algorithm. The proposed pulse shaping filters offer improved data
detection (communication) and I/O relation estimation (sensing) performance
compared to existing filter choices in the literature.

</details>


### [97] [Multidimensional Physiology-Inspired Enhanced Vital Sign Monitoring Using MIMO mmWave Bio-radar](https://arxiv.org/abs/2510.15278)
*Heyao Zhu,Yimeng Zhao,Zirui Zhang,Huansheng Yi,Chenbin Gao,Canhua Xu,Jianqi Wang,Fugui Qi*

Main category: eess.SP

TL;DR: 提出一种基于MIMO生物雷达的两阶段融合方法来强化生命体征检测，并提出基于器官辐射空间分布的通道筛选与加权融合，以实现RR和HR的稳健提取。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化加剧和慢性疾病负担上升，对非接触式生命体征监测的需求日益迫切。然而，现有mmWave雷达的多通道信号融合效率低、鲁棒性不足，限制了实际应用。

Method: 阶段1：针对单通道时-距离二维回波，在呼吸/心率生理带能量比的基础上选取有效距离bins，并通过相位对齐的最大比融合（MRC）提升呼吸与心跳信号的SNR。阶段2：引入器官的辐射空间分布特征作为理论基础，用于基于SNR的通道筛选、通道属性识别与多通道加权融合；并提出模板匹配方法，结合呼吸与心脏活动的物理模型提取RR和HR。

Result: 实验结果揭示了器官辐射的空间分布特征，并分析了距离与状态对算法的影响，验证了方法的可行性。

Conclusion: 两阶段融合策略可提升多通道MIMO生物雷达的生命体征检测与提取鲁棒性，且证实了器官辐射空间分布特征在通道选择与加权中的作用。

Abstract: With the intensiffcation of population aging and increasing burden of chronic
diseases, the demand for vital signs monitoring is becoming increasingly
urgent. A key challenge facing current non-contact detection technologies using
millimeter wave (mmWave) radar is the low efffciency of multi-channel signal
fusion in array radar systems based on equal weighting. To address this
challenge, this paper proposes a vital sign enhancement detection method for
multiple input and multiple output (MIMO) bio-radar, driven by multidimensional
physiological characteristics, which overcomes traditional limitations through
a two-stage fusion strategy. Stage 1: Enhanced Vital Sign Detection Using
Single-Channel Signals Based on Physiological Characteristics. First, a chest
wall multi-scattering point model is constructed. For single channel
time-distance two-dimensional echo signals, effective range bins are selected
based on the respiratory/cardiac physiological frequency band energy ratio, and
the signal-to-noise ratio (SNR) of respiration/heart signals is enhanced using
phase-aligned maximal ratio combining (MRC). Stage 2: Multi-Channel Fusion
Based on Organ Radiation Spatial Distribution Characteristics. The spatial
radiation characteristics of cardiopulmonary organs are introduced for the
ffrst time as the theoretical foundation for SNR-based channel screening,
channel attribute identiffcation, and multi-channel weighted fusion. Then, we
propose a template matching method to extract respiratory rate (RR) and heart
rate (HR) by adopting physical models of respiration and cardiac activities.
The experimental results demonstrate the existence of the spatial distribution
characteristics of organ radiation. In addition, we analyzed the impact of
distance and state on the algorithm from these two aspects.

</details>


### [98] [Pseudo-Random TDM-MIMO FMCW Based Millimeter-Wave Sensing and Communication Integration for UAV Swarm](https://arxiv.org/abs/2510.15575)
*Yi Tao,Zhen Gao,Zhuoran Li,Ziwei Wan,Tuan Li,Chunli Zhu,Lei Chen,Guanghui Wen,Dezhi Zheng,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出基于伪随机时分多路访问的ISAC方案，面向UAV簇的MMW/FMCW系统，通过伪随机天线选择、压缩感知和 chirp-division多址实现数据传输与高精度感知的协同，且在动态飞行场景下具备可行性，优于 mmWave-LoRadar，感知性能略低于传统 FMCW，但对城市杂波鲁棒。


<details>
  <summary>Details</summary>
Motivation: 随着无人机编队在军事、民用场景对统一资源（频谱与硬件）的高效利用需求日益增加，ISAC在提升通信与感知协同能力方面具有重要意义。挑战包括在动态、密集的UAV环境中实现高效的数据传输、准确的 sensing 参数估计以及灵活的资源分配，同时需要抑制多天线系统中的干扰与复杂性。

Method: 提出ISAC chirp波形，可以在延迟域和复数幅度上调制数据，同时保留高精度感知能力；解决TDM-MIMO中的挑战采用伪随机天线选择与压缩感知以保持最大无混干速度；通过chirp-division多址实现干扰无关的多天线传输与动态资源分配；提出以通信与感知融合为基础的动态迭代计算方案，实现数据解调与感知参数估计的协同。

Result: 仿真结果表明在UAV动态飞行场景下，该ISAC方案具有可行性；在通信与 sensing 性能方面优于 mmWave-LoRadar；但感知性能略低于传统 FMCW；在城市杂波建模下仍保持良好鲁棒性，存在一定程度的性能下降。

Conclusion: 工作证明了在UAV簇中实现基于伪随机TDM-MIMO mmWave FMCW的ISAC的可行性与潜在优势，同时揭示了与传统FMCW之间的感知折衷，并展示在复杂场景下的鲁棒性。

Abstract: The integrated sensing and communications (ISAC) can achieve the sharing of
hardware and spectrum resources, enabling efficient data transmission and
environmental sensing. This fusion is particularly important for unmanned
aerial vehicle (UAV) swarms, as it enhances the overall performance,
flexibility, and efficiency of such systems. To facilitate the collaborative
operations among UAVs, this paper proposes an ISAC solution based on the
pseudo-random time-division multiplexing (TDM)-multiple input multiple output
(MIMO) millimeter-wave (mmWave) frequency modulated continuous wave (FMCW).
Specifically, a novel ISAC chirp waveform is proposed to modulate data in both
the delay domain and complex amplitude, while also possessing high-precision
sensing capabilities. To address challenges in the TDM-MIMO, we utilize the
pseudo-random antenna selection and compressed sensing algorithms, ensuring
that the maximum unambiguous velocity is not compromised. Moreover, by
employing a chirp-division multiple access scheme, we propose an
interference-free multiple antenna transmission scheme to achieve dynamic
allocation of time-frequency resources and multi-user transmission. Finally, we
propose a communication and sensing fusion-based dynamic iterative computation
scheme, simultaneously achieving data demodulation and sensing parameter
estimation. Simulation results show that the proposed scheme can achieve ISAC
under the dynamic flight scenarios of UAVs. Meanwhile, the scheme outperforms
the mmWave-LoRadar in communication and sensing performance, yet its sensing
performance is slightly lower than that of the traditional FMCW. Under the
urban clutter modeling, the scheme still maintains favorable robustness despite
a certain degree of performance degradation.

</details>


### [99] [More on Boundary Behavior of Univalent Harmonic Mappings](https://arxiv.org/abs/2510.15689)
*Gebreslassie atsbha weldegebrial,hunduma legesse geleta*

Main category: eess.SP

TL;DR: 扩展对单位圆内注射调和映射的边界行为的研究：给出解析函数的角度极限（角度与对数）的条件下的极限结果；在边界导数趋向+∞时，保角映射的扩张率（dilatation）在任何 Stolz 角内只有有限个零点。


<details>
  <summary>Details</summary>
Motivation: 在 Laugesen 等的基础上，进一步研究注射调和映射在边界的边界行为，尤其是解析函数的角度极限和对数的极限，以及在强边界生长条件（边界导数为正无穷）下扩张量的零点结构。

Method: 基于已有的边界行为理论，分析解析函数在边界角的角度极限与对数极限；利用调和映射的构造 f=h+ar{g}、扩张率 ω=g'/h' 的性质，研究在 Stolz 角内 ω 的零点集合。并在边界导数趋向无穷的条件下给出有限性结论。

Result: 得到解析函数的角度（argument）和对数值的边界极限在若干条件下存在，并证明在边界导数为+∞时，调和映射的扩张率在任一 Stolz 角内只有有限个零点。

Conclusion: 扩展了先前关于单位圆内注射调和映射的边界行为结果，给出更精确的角度极限行为与扩张率零点的有限性结论，为理解单位圆内调和映射的边界性质提供了新见解。

Abstract: Many authors have examined various boundary behaviors of injective harmonic
mappings in the open unit disk. Building on Laugesen's work, Bshouty and others
explored the boundary behavior of harmonic mappings under different conditions.
In this paper, we extend their work and find out the angular limits of the
arguments and logarithms of analytic functions under various conditions. We
also examined the dilatation possesses only a finite set of zeros within any
stolz angle if the first derivative of harmonic function $f$ at the boundary is
positive infinity.

</details>


### [100] [Detection Seizure Onset Zone Using Circadian Fluctuating Epileptic Biomarkers: A Signal Processing and Machine Learning Approach](https://arxiv.org/abs/2510.15717)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Hassan Eshkiki*

Main category: eess.SP

TL;DR: 本研究探讨 circadian（昼夜节律）对癫痫生物标志物的影响，确定分析最优时段以提升手术前区定位的准确性。通过对9名难治性焦点性癫痫患者的颅内脑电数据进行回顾性分析，自动检测脉冲、脉冲序列、高频振荡（HFOs）及病理性HFO，并计算 alpha/delta 比以区分睡眠与清醒。结果显示睡眠时段的所有生物标志物水平均高于清醒时段，病理性HFOs和脉冲序列在距离癫痫发作起点的预测上比单独的脉冲或HFO更为精确。该研究首次将全面的生物标志物集合用于癫痫发作起点区的预测，提示睡眠阶段的数据分析更有助于准确预测发作起点区。


<details>
  <summary>Details</summary>
Motivation: 癫痫手术前需准确定位发作起点区，生物标志物在此过程中起核心作用；然而生物标志物的效力可能随时间、尤其是昼夜节律而变化。深入理解 circadian 对生物标志物的影响，可提高分析时段的选择，提升手术方案的精准性。

Method: 回顾性分析：对9名局灶性癫痫患者的颅内脑电数据进行自动检测，提取脉冲、脉冲序列、HFOs及病理性HFO，计算睡眠/清醒的alpha/delta比；睡眠-清醒状态通过分类，AUC为0.84。评估各生物标志物在不同睡眠状态下的表现及其对癫痫发作起点区距离的预测能力。

Result: 睡眠阶段的生物标志物频率显著高于清醒阶段；病理性HFOs和脉冲序列在指示发作起点区距离方面比单纯的脉冲或HFO更具精确性；首次将HFOs、脉冲序列及病理性HFO等综合生物标志物用于发作起点区预测；睡眠数据的生物标志物速率随睡眠-觉醒状态变化显著，睡眠数据更有利于准确预测发作起点区。

Conclusion: 考虑睡眠阶段的生物标志物分析可显著提高癫痫发作起点区的预测准确性，昼夜节律对 Biomarkers 的影响应纳入手术前评估策略之中。

Abstract: Epileptic biomarkers play a crucial role in identifying the origin of
seizures, an essential aspect of pre-surgical planning for epilepsy treatment.
These biomarkers can vary significantly over time. By studying these temporal
fluctuations, we can enhance their effectiveness in guiding surgical planning.
This research focuses on examining how circadian rhythms influence epilepsy
biomarkers and aims to determine the optimal times for their analysis. To
investigate the relationship between epilepsy biomarkers and circadian rhythm,
the sleep/wake states first need to be classified. After the biomarkers are
identified, they are compared across these states. A retrospective analysis was
conducted on intracranial electroencephalography data from patients with focal
epilepsy. The biomarkers spike, sequence of spikes, high-frequency oscillations
(HFOs), and pathological HFOs were identified through automatic detection. The
alpha/delta ratio was also calculated to distinguish between asleep and awake
stages. Data from 9 patients were analyzed, and the classification of sleep and
wake states was achieved with an area under the curve of 84%. All biomarker
rates were higher during the sleep stage compared to the wake stage.
Pathological HFOs and the sequence of spikes proved to be more precise
indicators regarding distance to seizure onset than spikes or HFOs. Unlike
previous studies that relied predominantly on long-term spike biomarker
analysis, this study is the first to utilize a comprehensive set of biomarkers,
including HFOs, spike sequences, and pathological HFOs, to enhance seizure
onset zone prediction. The rates of epilepsy biomarkers during sleep vary
considerably from those seen while awake, making sleep data analysis more
effective for accurately predicting the seizure onset zone.

</details>


### [101] [RIS-assisted Atomic MIMO Receiver](https://arxiv.org/abs/2510.15763)
*Qihao Peng,Jiuyu Liu,Qu Luo,Yi Ma,Pei Xiao,Maged Elkashlan,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 提出一种低复杂度RIS辅助原子MIMO接收机，利用PAM实现信号相位与本地振荡器的对齐，降低相位模糊和整体接收机复杂度；并通过将优化问题等价化为最小化等价矩阵的Frobenius范数，采用Adam梯度下降高效求解。


<details>
  <summary>Details</summary>
Motivation: 解决RIS辅助MIMO接收中的相位不确定性与高计算复杂度问题；通过RIS和PAM相结合实现相位对齐以简化检测与接收机架构，提升实现可行性与效率。

Method: 提出RIS辅助的原子MIMO接收机架构，利用PAM实现载噪相位对齐；将非凸优化问题重构为等价矩阵的Frobenius范数最小化，并使用Adam基梯度下降算法高效求解。

Result: 声称该方案可显著降低信号检测复杂度和整体接收机复杂度，且Adam求解过程具备良好收敛性与计算效率（未给出定量指标）。

Conclusion: 该工作为RIS辅助低复杂度接收系统提供了一条可行路径，结合PAM的相位对齐与Frobenius范数最小化重构，辅以Adam优化实现，具有实际应用潜力。

Abstract: In this paper, we propose a novel and low-complexity atomic multiple-input
multiple-output (MIMO) receiver architecture assisted by a reconfigurable
intelligent surface (RIS). By introducing RIS and utilizing pulse amplitude
modulation (PAM), the phase of the transmitted signal is effectively aligned
with that of the local oscillator (LO), thereby mitigating phase ambiguity and
substantially reducing both signal detection complexity and overall receiver
complexity.To tackle the resulting non-convex optimization problem, we
reformulate it into a tractable form by minimizing the Frobenius norm of an
equivalent matrix, which is efficiently solved using an Adam-based gradient
descent algorithm.

</details>


### [102] [From Active to Battery-Free: Rydberg Atomic Quantum Receivers for Self-Sustained SWIPT-MIMO Networks](https://arxiv.org/abs/2510.15784)
*Qihao Peng,Qu Luo,Zheng Chu,Neng Ye,Hong Ren,Cunhua Pan,Lixia Xiao,Pei Xiao*

Main category: eess.SP

TL;DR: 提出一种混合SWIPT与MIMO的体系结构，基站采用射频下行传输与RAQR上行接收IoT设备信号，联合设计传输与功率分配以最大化总速率，给出下界与上界并提出基于单项近似与几何规划的迭代算法以求解非凸优化问题，仿真验证界限紧性并优于基准方案，表明RAQR与SWIPT-MIMO的集成可实现基于能量采集的电池自洽通信。


<details>
  <summary>Details</summary>
Motivation: 解决在SWIPT-MIMO体系中联合设计传输与功率分配以最大化和提升系统速率的挑战，同时探讨将RAQR等量子接收单元融入到下行-上行耦合的场景，以实现对以能量为来源的IoT设备的可靠上行检测。

Method: 建立下行MRT/ZF与上行MRC/ZF的速率下界和能量获取下界的封闭形式；在此基础上提出一种基于最佳单项近似(broadcast monomial approximation)与几何规划的迭代算法来求解原非凸优化的近似最优解。

Result: 给出下/上行速率与能量的紧性下界，所提算法在数值实验中优于基准方案，验证了下界的紧凑性与算法的有效性；通过与RAQR耦合的SWIPT-MIMO框架实现对仅靠能量采集供电的物联网设备进行弱信号的可靠检测，从而实现电池自给的通信。

Conclusion: 将RAQR与SWIPT-enabled MIMO结合的框架有效提升系统速率与能量采集能力，提供一个可操作的非凸优化求解策略，促进面向物联网的电池无源通信的发展。

Abstract: In this paper, we proposed a hybrid simultaneous wireless information and
power transfer (SWIPT)-enabled multiple-input multiple-output (MIMO)
architecture, where the base station (BS) uses a conventional RF transmitter
for downlink transmission and a Rydberg atomic quantum receiver (RAQR) for
receiving uplink signal from Internet of Things (IoT) devices. To fully exploit
this integration, we jointly design the transmission scheme and the
power-splitting strategy to maximize the sum rate, which leads to a non-convex
problem. To address this challenge, we first derive closed-form lower bounds on
the uplink achievable rates for maximum ratio combining (MRC) and zero-forcing
(ZF), as well as on the downlink rate and harvested energy for maximum ratio
transmission (MRT) and ZF precoding. Building upon these bounds, we propose an
iterative algorithm relying on the best monomial approximation and geometric
programming (GP) to solve the non-convex problem. Finally, simulations validate
the tightness of our derived lower bounds and demonstrate the superiority of
the proposed algorithm over benchmark schemes. Importantly, by integrating RAQR
with SWIPT-enabled MIMO, the BS can reliably detect weak uplink signals from
IoT devices powered only by harvested energy, enabling battery-free
communication.

</details>


### [103] [Resilient Full-Duplex ISAC in the Face of Imperfect SI Cancellation: Globally Optimal Timeslot Allocation and Beam Selection](https://arxiv.org/abs/2510.15810)
*Luis F. Abanto-Leon,Setareh Maghsudi*

Main category: eess.SP

TL;DR: 提出将半无限非凸混合整数非线性规划转化为MILP，从而在下行全双工ISAC系统中实现时隙分配与波束选择的全局最优联合设计。


<details>
  <summary>Details</summary>
Motivation: 在下行全双工ISAC系统中实现高效的资源管理需要同时优化 sensing 与 communications；存在残留自干扰（SI）时，问题变得更具挑战性。原问题为半无限、非凸的MINLP，求解困难且无法保证全局最优。

Method: 提出定制化的重构策略，将原始半无限的非凸MINLP转化为可解的混合整数线性规划（MILP），从而获得全局最优解。方法同时考虑离散的时隙分配（信道使用分配给 sensing/communication）与波束选择（发射/接收方向及自适应波束宽度）的联合优化，并对残留自干扰进行鲁棒处理。

Result: 通过MILP形式实现全局最优解的获取，揭示时隙分配与波束选择的协同效应，显著提升全双工ISAC系统的资源利用效率，同时对残留自干扰具备鲁棒性。

Conclusion: 该定制化重构为全双工ISAC的联合资源管理提供了一种可行且高效的全局最优求解工具，有助于提升系统性能和鲁棒性。

Abstract: This work addresses the radio resource management (RRM) design in downlink
full-duplex integrated sensing and communications (ISAC) systems, jointly
optimizing timeslot allocation and beam selection under imperfect
self-interference cancellation. Timeslot allocation governs the distribution of
discrete channel uses between sensing and communication tasks, while beam
selection determines transmit and receive directions along with adaptive
beamwidths. The joint design leads to a semi-infinite, nonconvex mixed-integer
nonlinear program (MINLP), which is difficult to solve. To overcome this, we
develop a tailored reformulation strategy that transforms the problem into a
tractable mixed-integer linear program (MILP), enabling globally optimal
solutions. Our approach provides insights into the coordinated optimization of
timeslot allocation and beam selection, enhancing the efficiency of full-duplex
ISAC systems while ensuring resilience against residual self-interference.

</details>
