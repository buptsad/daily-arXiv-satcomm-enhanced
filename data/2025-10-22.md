<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 78]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.SP](#eess.SP) [Total: 13]
- [eess.SY](#eess.SY) [Total: 6]
- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings](https://arxiv.org/abs/2510.17846)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: CARLE是一种混合AI框架，通过Res-CNN/R-LSTM、多头注意力和残差连接来实现鲁棒RUL预测，结合高斯滤波和小波变换的特征提取，使用随机森林回归器进行最终预测，在XJTU-SY和PRONOSTIA数据集上表现优于多种方法，且对噪声与跨域有较好鲁棒性，并利用LIME/SHAP提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 剩余使用寿命估计在PHM中的关键任务，但在运行条件变化下往往缺乏泛化性和鲁棒性，需要一个能够同时捕捉时空降解模式并具备稳定预测的框架。

Method: 提出CARLE：混合深度与浅层学习的框架，包含Res-CNN和Res-LSTM块，配合多头注意力和残差结构以捕捉时空降解特征；使用随机森林回归器输出RUL；预处理包含高斯滤波降噪和连续小波变换的时间-频率特征提取；在XJTU-SY和PRONOSTIA数据集上进行评估，通过消融实验、噪声鲁棒性和跨域测试验证泛化性和鲁棒性；并采用LIME和SHAP进行模型可解释性分析。

Result: 实验结果显示CARLE在与若干先进方法的比较中具有更优的RUL预测性能，尤其在动态运行条件下表现突出；消融实验揭示各组件的贡献，噪声和跨域实验表明模型具有较好的鲁棒性和泛化能力；可解释性分析表明模型具有一定透明度和可信度。

Conclusion: CARLE为PHM中的RUL预测提供了一种高鲁棒性、可解释性较强的混合学习框架，能够在变化条件下维持良好性能，具有推广性和应用潜力。

Abstract: Prognostic Health Management (PHM) systems monitor and predict equipment
health. A key task is Remaining Useful Life (RUL) estimation, which predicts
how long a component, such as a rolling element bearing, will operate before
failure. Many RUL methods exist but often lack generalizability and robustness
under changing operating conditions. This paper introduces CARLE, a hybrid AI
framework that combines deep and shallow learning to address these challenges.
CARLE uses Res-CNN and Res-LSTM blocks with multi-head attention and residual
connections to capture spatial and temporal degradation patterns, and a Random
Forest Regressor (RFR) for stable, accurate RUL prediction. A compact
preprocessing pipeline applies Gaussian filtering for noise reduction and
Continuous Wavelet Transform (CWT) for time-frequency feature extraction. We
evaluate CARLE on the XJTU-SY and PRONOSTIA bearing datasets. Ablation studies
measure each component's contribution, while noise and cross-domain experiments
test robustness and generalization. Comparative results show CARLE outperforms
several state-of-the-art methods, especially under dynamic conditions. Finally,
we analyze model interpretability with LIME and SHAP to assess transparency and
trustworthiness.

</details>


### [2] [MIN-Merging: Merge the Important Neurons for Model Merging](https://arxiv.org/abs/2510.17890)
*Yunfei Liang*

Main category: cs.LG

TL;DR: 提出 MIN-Merging，一种基于路由的选择性合并框架，通过合并最重要的神经元来降低模型合并中的参数冲突，在 CV 和 NLP 任务上实现域内稳健提升，同时保持域外泛化。


<details>
  <summary>Details</summary>
Motivation: 当前开源模型横跨多个领域，模型合并容易因参数冲突而降低域内性能，需要一种能降低冲突并保留泛化的合并方法。

Method: 提出路由式的 MIN-Merging 框架，定位并选择性地合并最重要的神经元以减少冲突，可能通过门控/路由机制实现。

Result: 在计算机视觉和自然语言处理基准上，方法在域内任务上获得稳定的提升，同时保持对域外任务的泛化能力。

Conclusion: MIN-Merging 为模型合并中的参数冲突问题提供了一个可行且有效的实践方案。

Abstract: Recent advances in deep learning have led to a surge of open-source models
across diverse domains. While model merging offers a promising way to combine
their strengths, existing approaches often suffer from parameter conflicts that
degrade performance on domain-specific tasks. We propose MIN-Merging, a
router-based framework that selectively merges the most important neurons to
reduce such conflicts. Extensive experiments on Computer Vision(CV) and Natural
Language Processing(NLP) benchmarks show that MIN-Merging achieves consistent
gains on in-domain tasks while retaining the generalization ability of
pretrained models on out-of-domain tasks. These results highlight its
effectiveness as a practical solution to the parameter conflict problem in
model merging.

</details>


### [3] [Hierarchical Federated Unlearning for Large Language Models](https://arxiv.org/abs/2510.17895)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: 提出一个可扩展的联邦式未学习框架，利用任务特定适配器和分层合并解决LLMs的持续性、异质性与分布数据带来的遗忘与性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在现实应用中的广泛部署，隐私与安全要求日益提高，同时需要从模型中移除 undesired/敏感知识。然而，遗忘需求往往是连续且异质的，且数据分散并且访问不对称，导致跨域与域内干扰，令忘记与保持的平衡更为困难。

Method: 方法将遗忘过程与模型保留能力解耦，通过任务特定的适配器学习来实现局部化的遗忘更新；并引入分层合并策略以缓解不同遗忘任务之间的冲突目标，从而实现鲁棒且可扩展的遗忘更新，同时在隐私保护下通过联邦学习平台执行。

Result: 在WMDP、MUSE、TOFU等基准上开展实验，结果显示该方法能够有效处理异质的遗忘请求，并在保持或提升LLMs下游性能方面优于/等同于基线方法，展示了对隐私保护与实用性的双重提升。

Conclusion: 本文证明了联邦式遗忘框架在处理跨域、异质数据下的隐私保护遗忘任务时的可行性与鲁棒性，提出的适配器与分层合并策略为现实场景中的大规模LLMs遗忘提供了可扩展的解决方案。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, raising concerns about privacy, security and the need to remove
undesirable knowledge. Machine Unlearning has emerged as a promising solution,
yet faces two key challenges: (1) practical unlearning needs are often
continuous and heterogeneous, and (2) they involve decentralized, sensitive
data with asymmetric access. These factors result in inter-domain and
intra-domain interference, which further amplifies the dilemma of unbalanced
forgetting and retaining performance. In response, we propose a federated
unlearning approach for LLMs that is scalable and privacy preserving. Our
method decouples unlearning and retention via task-specific adapter learning
and employs a hierarchical merging strategy to mitigate conflicting objectives
and enables robust, adaptable unlearning updates. Comprehensive experiments on
benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles
heterogeneous unlearning requests while maintaining strong LLM utility compared
with baseline methods.

</details>


### [4] [Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism](https://arxiv.org/abs/2510.17896)
*Tao Bu,Qiangang Wang,Bowen Zeng,Hanwen Sun,Yunpeng Huang,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: 提出一个统一基准，整合代表性注意力内核和上下文并行机制，通过可模块化接口评估在长上下文训练中的注意力机制。


<details>
  <summary>Details</summary>
Motivation: 现有工作在算子级优化与模块级分布式注意力间的系统性比较不足，且上下文并行往往依赖特定框架，缺乏跨上下文的性能分析。

Method: 设计一个可扩展的、模块化的基准，融合多种注意力内核与上下文并行策略，通过统一接口进行评估。评估维度包括注意力掩码模式和序列长度/分布式规模。

Result: 在多达96个GPU的集群上进行广泛实验，提供可重复的对比，揭示各自的权衡，并为在长上下文LLM训练中设计与部署注意力机制提供实用指导。

Conclusion: 基准有助于系统性比较不同注意力实现和分布式策略，推动在长上下文场景下的高效注意力设计与部署。

Abstract: Transformer-based large language models (LLMs) have achieved remarkable
success, yet their standard attention mechanism incurs quadratic computation
and memory costs with respect to sequence length, posing a major bottleneck for
long-context training. Prior work tackles this challenge along two directions:
(1) kernel-level optimizations, which accelerate dense and sparse attention
operators; and (2) module-level strategies, often referred to as distributed
attention or context parallel training, which scale attention across multiple
devices. However, systematic evaluation still remains limited: operator-level
comparisons are often incomplete, while context parallel strategies are
typically framework-specific, with unclear performance analysis across
contexts. To address these gaps, we propose a unified benchmark that integrates
representative attention kernels and context parallel mechanisms with a modular
and extensible interface for evaluation. The benchmark evaluates methods along
two critical dimensions: (1) attention mask patterns, which strongly affect
efficiency, scalability, and usability, and (2) sequence length and distributed
scale, which determine performance under extreme long-context training. Through
comprehensive experiments on the cluster of up to 96 GPUs, our benchmark
enables reproducible comparisons, highlights method-specific trade-offs, and
provides practical guidance for designing and deploying attention mechanisms in
long-context LLM training.

</details>


### [5] [Automated Algorithm Design for Auto-Tuning Optimizers](https://arxiv.org/abs/2510.17899)
*Floris-Jan Willemsen,Niki van Stein,Ben van Werkhoven*

Main category: cs.LG

TL;DR: Using large language models to automatically generate optimization algorithms tailored for auto-tuning, yielding substantial performance gains over traditional optimizers.


<details>
  <summary>Details</summary>
Motivation: Auto-tuning has huge and irregular parameter spaces making manual exploration impractical; no single optimizer excels across tasks, motivating automated, problem-aware algorithm design.

Method: Prompt LLMs with problem descriptions and search-space characteristics to generate specialized optimization strategies, iteratively evaluating and refining them. The generated algorithms are tested on four real-world auto-tuning applications across six hardware platforms and benchmarked against state-of-the-art optimizers from two contemporary auto-tuning frameworks.

Result: Providing additional application- and search-space-specific information in prompts yields average performance gains of 30.7% and 14.6% respectively. LLM-generated optimizers can rival or outperform human-designed ones, with the best-generated optimizers achieving on average 72.4% improvement over state-of-the-art auto-tuning optimizers.

Conclusion: LLMs can effectively synthesize specialized optimization algorithms for auto-tuning; iterative prompting with task-relevant details enhances performance and can surpass traditional optimizers in many cases, suggesting a viable new paradigm for auto-tuning.

Abstract: Automatic performance tuning (auto-tuning) is essential for optimizing
high-performance applications, where vast and irregular parameter spaces make
manual exploration infeasible. Traditionally, auto-tuning relies on
well-established optimization algorithms such as evolutionary algorithms,
annealing methods, or surrogate model-based optimizers to efficiently find
near-optimal configurations. However, designing effective optimizers remains
challenging, as no single method performs best across all tuning tasks.
  In this work, we explore a new paradigm: using large language models (LLMs)
to automatically generate optimization algorithms tailored to auto-tuning
problems. We introduce a framework that prompts LLMs with problem descriptions
and search-space characteristics results to produce specialized optimization
strategies, which are iteratively examined and improved.
  These generated algorithms are evaluated on four real-world auto-tuning
applications across six hardware platforms and compared against the
state-of-the-art in optimization algorithms of two contemporary auto-tuning
frameworks. The evaluation demonstrates that providing additional application-
and search space-specific information in the generation stage results in an
average performance improvement of 30.7\% and 14.6\%, respectively. In
addition, our results show that LLM-generated optimizers can rival, and in
various cases outperform, existing human-designed algorithms, with our
best-performing generated optimization algorithms achieving, on average, 72.4\%
improvement over state-of-the-art optimizers for auto-tuning.

</details>


### [6] [NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation](https://arxiv.org/abs/2510.17914)
*Rikard Vinge,Isabelle Wittmann,Jannik Schneider,Michael Marszalek,Luis Gilch,Thomas Brunschwiler,Conrad M Albrecht*

Main category: cs.LG

TL;DR: NeuCo-Bench is a benchmark framework for evaluating lossy neural compression and representation learning in Earth Observation (EO), using fixed-size embeddings as compact representations. It includes an evaluation pipeline, a hidden-task leaderboard to reduce pretraining bias, and a scoring system balancing accuracy and stability, plus the SSL4EO-S12-downstream dataset. Initial results from a CVPR EARTHVISION 2025 challenge and ablations with foundation models demonstrate feasibility, aiming for standardized, community-driven evaluation of neural embeddings for EO and beyond.


<details>
  <summary>Details</summary>
Motivation: There is a need for standardized, reproducible benchmarking of neural embeddings in EO to enable fair comparison across methods, explore pretraining bias, and support diverse downstream tasks with compact representations.

Method: Develop a benchmark framework (NeuCo-Bench) built around fixed-size, task-agnostic embeddings. Components include an evaluation pipeline, a hidden-task leaderboard to mitigate pretraining bias, and a scoring system that balances accuracy and stability. Release SSL4EO-S12-downstream—a multispectral, multitemporal EO dataset—to support reproducibility. Report initial results from a public CVPR EARTHVISION 2025 challenge and perform ablations with state-of-the-art foundation models.

Result: Public challenge results demonstrate the practicality of NeuCo-Bench and fixed-size embeddings for EO tasks; ablations with state-of-the-art foundation models illustrate factors influencing performance and stability; the dataset release supports reproducibility and broader experimentation.

Conclusion: NeuCo-Bench represents an initial step toward a community-driven, standardized evaluation of neural embeddings for EO and related domains, enabling consistent benchmarking, reduced pretraining bias, and broader accessibility for researchers.

Abstract: We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy)
neural compression and representation learning in the context of Earth
Observation (EO). Our approach builds on fixed-size embeddings that act as
compact, task-agnostic representations applicable to a broad range of
downstream tasks. NeuCo-Bench comprises three core components: (i) an
evaluation pipeline built around reusable embeddings, (ii) a new challenge mode
with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii)
a scoring system that balances accuracy and stability. To support
reproducibility, we release SSL4EO-S12-downstream, a curated multispectral,
multitemporal EO dataset. We present initial results from a public challenge at
the 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art
foundation models. NeuCo-Bench provides a first step towards community-driven,
standardized evaluation of neural embeddings for EO and beyond.

</details>


### [7] [Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection](https://arxiv.org/abs/2510.17917)
*Jinseong Park,Mijung Park*

Main category: cs.LG

TL;DR: Time-frequency selective data unlearning for diffusion models. The paper shows that forgetting is uneven across time and frequency; by focusing training on targeted time-frequency ranges, it achieves higher aesthetic quality and lower noise, improving unlearning without full retraining. It validates across gradient-based and preference optimization objectives, and both image-level and text-to-image tasks, with a normalized SSCD for evaluation of deletion and quality.


<details>
  <summary>Details</summary>
Motivation: Data unlearning in diffusion models is underexplored and challenging. Existing methods treating all diffusion steps equally often cause quality degradation or incomplete forgetting. Forgetting is not uniform across time and frequency; a targeted approach may improve both deletion and generation quality.

Method: Introduce a time-frequency selective training approach that emphasizes specific time-frequency ranges during training to achieve selective forgetting. Apply this approach to diverse objectives (gradient-based and preference optimization) and tasks (image-level and text-to-image). Propose a normalized version of SSCD to evaluate both deletion and quality of unlearned samples.

Result: The method yields higher aesthetic quality and lower noise in generated outputs while achieving more effective forgetting. The approach is validated across different objectives and tasks, demonstrating robustness and generality.

Conclusion: Provides a clearer understanding of the challenges of data unlearning in diffusion models and offers practical strategies to improve both unlearning performance and evaluation through time-frequency selective training and a normalized SSCD metric.

Abstract: Data unlearning aims to remove the influence of specific training samples
from a trained model without requiring full retraining. Unlike concept
unlearning, data unlearning in diffusion models remains underexplored and often
suffers from quality degradation or incomplete forgetting. To address this, we
first observe that most existing methods attempt to unlearn the samples at all
diffusion time steps equally, leading to poor-quality generation. We argue that
forgetting occurs disproportionately across time and frequency, depending on
the model and scenarios. By selectively focusing on specific time-frequency
ranges during training, we achieve samples with higher aesthetic quality and
lower noise. We validate this improvement by applying our time-frequency
selective approach to diverse settings, including gradient-based and preference
optimization objectives, as well as both image-level and text-to-image tasks.
Finally, to evaluate both deletion and quality of unlearned data samples, we
propose a simple normalized version of SSCD. Together, our analysis and methods
establish a clearer understanding of the unique challenges in data unlearning
for diffusion models, providing practical strategies to improve both evaluation
and unlearning performance.

</details>


### [8] [Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning](https://arxiv.org/abs/2510.17923)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Wei Ju,Jiancheng Lv,Deng Xiong,Ziyue Qiao*

Main category: cs.LG

TL;DR: COMPASS提出一种无需外部监督的测试时奖励机制，用于在无标签数据上对LLMs进行强化学习。通过双模态校准的答案奖励（DCAR）与决定性路径奖励（DPR）两部分，提升伪标签的可信性与推理过程的质量，从而在多任务和多模型架构下实现显著且稳健的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前用于LLMs的强化学习高度依赖人工标注的偏好数据或奖励模型，难以扩展到海量无标签数据。现有的测试时RL通过自我一致性来生成奖励，但易放大错误的伪标签。需要在没有外部监督的情况下，可靠地从连续经验流中学习。

Method: 提出COMPASS系统，结合两大组件：1) 双重校准的答案奖励（DCAR），通过置信度与可信度校准来建立可信的伪标签以稳定训练；2) 决定性路径奖励（DPR），直接优化推理过程的质量，而不仅仅是结果的监督。两者协同工作，强化可信共识答案与高决定性的推理链。

Result: 大量实验表明，COMPASS在多种推理任务及模型架构中均实现显著且一致的性能提升，显示其在无监督/自我学习场景下的可扩展性。

Conclusion: COMPASS为LLMs在连续经验流上学习提供了一个可扩展的自监督型测试时奖励机制，有效提升分析能力与推理质量，推动更大规模无标注数据下的RL应用。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing
Large Language Models (LLMs), achieving remarkable performance in complex
reasoning domains such as mathematics and code generation. However, current RL
methods face a fundamental scalability bottleneck due to their heavy reliance
on human-curated preference data or labeled datasets for reward modeling. To
overcome this limitation, we explore RL on unlabeled data where models learn
autonomously from continuous experience streams. The core challenge in this
setting lies in reliable reward estimation without ground-truth supervision.
Existing approaches like Test-Time RL address this through self-consistent
consensus, but risk reinforcing incorrect pseudo-labels derived from majority
voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel
test-time reward mechanism that operates without external supervision. COMPASS
integrates two complementary components: the Dual-Calibration Answer Reward
(DCAR), which stabilizes training by establishing trustworthy pseudo-labels
through confidence and credibility calibration, and the Decisive Path Reward
(DPR), which directly optimizes the reasoning process quality beyond mere
outcome supervision. By jointly reinforcing trustworthy consensus answers and
highly decisive reasoning chains, the COMPASS systematically enhances the
model's analytical capabilities. Extensive experiments show that COMPASS
achieves significant and consistent performance gains across diverse reasoning
tasks and model architectures, advancing a more scalable direction for LLMs to
learn from continuous experience.

</details>


### [9] [EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning](https://arxiv.org/abs/2510.17928)
*He Du,Bowen Li,Aijun Yang,Siyang He,Qipeng Guo,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出一种进化式、任务无关的可执行数据合成框架，从最小种子监督出发共同生成问题、候选解与验证伪证，并通过一致性评估器迭代优化，提升数据的可验证性与跨领域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖领域特定启发式或事后过滤，缺乏可迁移的、普适的可验证性评估；需要高质量、可复现的合成数据来提升强化学习/蒸馏等任务的稳定性与泛化。

Method: 提出一个进化式、任务无关、策略引导、可执行性检查的数据合成框架；从最小种子监督出发联合合成问题、多样化解与验证伪证；通过一个一致性评估器使人类标注检查与策略诱导检查保持一致，迭代发现策略并将筛选转化为可 principled 的合成过程；可泛化至不同域。

Result: 在 RLVR 与模型蒸馏训练范式下进行实验；在 LiveCodeBench 与 AgentBench-OS 任务上显示显著改进，表明框架具有鲁棒的跨领域泛化能力。

Conclusion: 该框架把数据筛选提升为一个 principled 的合成过程，能稳定生产连贯、可验证的训练样本，减小对领域特定规则的依赖，提升多任务的泛化能力。

Abstract: Reliable verifiable data has become a key driver of capability gains in
modern language models, enabling stable reinforcement learning with verifiable
rewards and effective distillation that transfers competence across math,
coding, and agentic tasks. Yet constructing generalizable synthetic verifiable
data remains difficult due to hallucination-prone generation, and weak or
trivial verification artifacts that fail to separate strong from weak
solutions. Existing approaches often rely on task-specific heuristics or
post-hoc filters that do not transfer across domains and lack a principled,
universal evaluator of verifiability. In this work, we introduce an
evolutionary, task-agnostic, strategy-guided, executably-checkable data
synthesis framework that, from minimal seed supervision, jointly synthesizes
problems, diverse candidate solutions, and verification artifacts, and
iteratively discovers strategies via a consistency-based evaluator that
enforces agreement between human-annotated and strategy-induced checks. This
pipeline upgrades filtering into principled synthesis: it reliably assembles
coherent, verifiable training instances and generalizes without domain-specific
rules. Our experiments demonstrate the effectiveness of the proposed approach
under both RLVR and model distillation training paradigms. The results show
that training with our synthesized data yields significant improvements on both
the LiveCodeBench and AgentBench-OS tasks, highlighting the robust
generalization of our framework.

</details>


### [10] [From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference](https://arxiv.org/abs/2510.17933)
*Xiangbo Deng,Cheng Chen,Peng Yang*

Main category: cs.LG

TL;DR: 两个阶段 Param-CPD 框架：先通过仿真基础后验估计学习参数分布，再对参数轨迹应用 CPD，显著提升 Lorenz-63 时序变点检测性能。


<details>
  <summary>Details</summary>
Motivation: 在混沌时间序列中，观测信号与内在变异性强耦合，导致直接在观测空间检测 regime shift 困难；在物理可解释的参数空间中可能得到更清晰的变点信号。

Method: 阶段1：通过仿真基础推断（simulation-based inference）训练神经后验估计器，获得对 governing parameters 的后验分布；阶段2：对得到的参数轨迹应用标准 CPD 算法检测变点。以 Lorenz-63 为测试，参数为分段常数。与观测空间基线比较。

Result: Param-CPD 在 F1、定位误差、假阳性率方面优于观测空间基线；对参数后验的可辨识性和校准性在平稳轨迹上被验证；参数空间提供更干净的检测信号。对容忍度、滑窗长度和噪声的鲁棒性分析显示结果稳定。

Conclusion: 在物理可解释的参数空间进行变点检测可以在非线性动力系统中实现更准确、可解释的变点检测。

Abstract: Detecting regime shifts in chaotic time series is hard because
observation-space signals are entangled with intrinsic variability. We propose
Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework
that first amortizes Bayesian inference of governing parameters with a neural
posterior estimator trained by simulation-based inference, and then applies a
standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63
with piecewise-constant parameters, Param--CPD improves F1, reduces
localization error, and lowers false positives compared to observation--space
baselines. We further verify identifiability and calibration of the inferred
posteriors on stationary trajectories, explaining why parameter space offers a
cleaner detection signal. Robustness analyses over tolerance, window length,
and noise indicate consistent gains. Our results show that operating in a
physically interpretable parameter space enables accurate and interpretable
changepoint detection in nonlinear dynamical systems.

</details>


### [11] [UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts](https://arxiv.org/abs/2510.17937)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.LG

TL;DR: 提出 UniRL-Zero，一种统一的强化学习框架，融合多模态理解、推理与生成能力及其交互，在统一模型中实现六种强化学习场景并给出系统基线，代码开源。


<details>
  <summary>Details</summary>
Motivation: 应对统一模型在理解、推理和生成等多模态能力上的需求，提供一个统一的强化学习框架来提升多模态理解与生成的协同表现与交互能力。

Method: 提出一个统一的强化学习框架 UniRL-Zero，定义六种统一模型强化学习场景，给出系统化的基线用于统一理解和生成模型的强化学习研究，并提供开源实现。

Result: 提出六个场景及相应的系统基线，展示在统一理解与生成模型中的强化学习潜力，并公开代码。

Conclusion: 为统一模型的理解、推理与生成能力的RL研究提供完整框架与基线，促进多模态统一模型的发展与应用。

Abstract: We present UniRL-Zero, a unified reinforcement learning (RL) framework that
boosts, multimodal language model understanding and reasoning, diffusion model
multimedia generation, and their beneficial interaction capabilities within a
unified model. Our work defines six scenarios for unified model reinforcement
learning, providing systematic baselines for reinforcement learning of unified
understanding and generation model. Our code is available at
https://github.com/G-U-N/UniRL.

</details>


### [12] [Provably Optimal Reinforcement Learning under Safety Filtering](https://arxiv.org/abs/2510.18082)
*Donggeon David Oh,Duy P. Nguyen,Haimin Hu,Jaime F. Fisac*

Main category: cs.LG

TL;DR: 引入充分宽容的安全过滤器来实现安全的强化学习，证明在经过安全过滤的环境中不会降低渐近性能，并实现安全性与性能优化的解耦。


<details>
  <summary>Details</summary>
Motivation: 解决RL在安全关键应用中的安全保障不足问题；传统的安全过滤往往被认为会牺牲性能，本文提出形式化框架和理论支撑以消除该刻板印象。

Method: 提出安全关键MDP（SC-MDP）与过滤MDP的形式化框架，定义以环境一部分出现的安全过滤器。给出三条主要定理：(i) 过滤MDP中的学习在分类意义上是安全的；(ii) 过滤MDP具备与标准RL相同的收敛性；(iii) 以同一过滤器执行时，过滤MDP中的最优策略的渐近回报等于SC-MDP中最佳安全策略的回报，实现安全执行与性能优化的完全分离。

Result: 理论结果在Safety Gymnasium任务上得到验证，即训练过程无违规记录，最终性能达到甚至超过未过滤基线。

Conclusion: 给出一个简单而有效的实践准则：使用尽可能宽松的安全过滤器进行训练与部署，从而在保持安全的前提下实现与或优于传统RL的表现。

Abstract: Recent advances in reinforcement learning (RL) enable its use on increasingly
complex tasks, but the lack of formal safety guarantees still limits its
application in safety-critical settings. A common practical approach is to
augment the RL policy with a safety filter that overrides unsafe actions to
prevent failures during both training and deployment. However, safety filtering
is often perceived as sacrificing performance and hindering the learning
process. We show that this perceived safety-performance tradeoff is not
inherent and prove, for the first time, that enforcing safety with a
sufficiently permissive safety filter does not degrade asymptotic performance.
We formalize RL safety with a safety-critical Markov decision process (SC-MDP),
which requires categorical, rather than high-probability, avoidance of
catastrophic failure states. Additionally, we define an associated filtered MDP
in which all actions result in safe effects, thanks to a safety filter that is
considered to be a part of the environment. Our main theorem establishes that
(i) learning in the filtered MDP is safe categorically, (ii) standard RL
convergence carries over to the filtered MDP, and (iii) any policy that is
optimal in the filtered MDP-when executed through the same filter-achieves the
same asymptotic return as the best safe policy in the SC-MDP, yielding a
complete separation between safety enforcement and performance optimization. We
validate the theory on Safety Gymnasium with representative tasks and
constraints, observing zero violations during training and final performance
matching or exceeding unfiltered baselines. Together, these results shed light
on a long-standing question in safety-filtered learning and provide a simple,
principled recipe for safe RL: train and deploy RL policies with the most
permissive safety filter that is available.

</details>


### [13] [Demystifying Transition Matching: When and Why It Can Beat Flow Matching](https://arxiv.org/abs/2510.17991)
*Jaihoon Kim,Rajarshi Saha,Minhyuk Sung,Youngsuk Park*

Main category: cs.LG

TL;DR: TM 在单峰高斯及分离模态的高斯混合下相较于 FM，能以相同计算量获得更低的 KL 和更快的收敛；当方差趋近于0或模态不分离时优势减弱。


<details>
  <summary>Details</summary>
Motivation: 阐明何时以及为何 Transition Matching（TM）优于 Flow Matching（FM）以提升生成质量与采样效率，为 diffusion-like 模型的采样策略提供理论与经验依据。

Method: 1) 证明在有限步下对单峰高斯，TM 的 KL 小于 FM，原因在于 TM 的随机差分潜在更新保留了目标协方差；2) 给出收敛速率结论，在固定算力下 TM 收敛快于 FM；3) 将分析扩展至高斯混合，识别局部单峰可近似情形并给出 TM 优势的条件；4) 通过对高斯分布的控制实验及对真实图像/视频生成的扩展验证理论结论。

Result: 在单峰高斯下，TM 能以有限步实现低于 FM 的 KL，并具备更快的收敛速度；在高斯混合下存在“局部单峰”区间使得 TM 可优于 FM，且当模态间距增大、方差不再微弱时优势更明显；当目标方差趋近于0时，TM 的更新趋于 FM，优势减弱；实验与应用验证对高斯分布及真实图像/视频生成具有一定普适性。

Conclusion: TM 的优势来源于保留目标协方差与随机性引入的差分更新，在模态分离且方差不小的情境下相对于 FM 显著提升采样效率与生成质量；对多模态目标的近似分析提供了适用条件，指向在现实生成任务中优先考虑 TM 以提升性能。

Abstract: Flow Matching (FM) underpins many state-of-the-art generative models, yet
recent results indicate that Transition Matching (TM) can achieve higher
quality with fewer sampling steps. This work answers the question of when and
why TM outperforms FM. First, when the target is a unimodal Gaussian
distribution, we prove that TM attains strictly lower KL divergence than FM for
finite number of steps. The improvement arises from stochastic difference
latent updates in TM, which preserve target covariance that deterministic FM
underestimates. We then characterize convergence rates, showing that TM
achieves faster convergence than FM under a fixed compute budget, establishing
its advantage in the unimodal Gaussian setting. Second, we extend the analysis
to Gaussian mixtures and identify local-unimodality regimes in which the
sampling dynamics approximate the unimodal case, where TM can outperform FM.
The approximation error decreases as the minimal distance between component
means increases, highlighting that TM is favored when the modes are well
separated. However, when the target variance approaches zero, each TM update
converges to the FM update, and the performance advantage of TM diminishes. In
summary, we show that TM outperforms FM when the target distribution has
well-separated modes and non-negligible variances. We validate our theoretical
results with controlled experiments on Gaussian distributions, and extend the
comparison to real-world applications in image and video generation.

</details>


### [14] [Attention-Guided Deep Adversarial Temporal Subspace Clustering (A-DATSC) Model for multivariate spatiotemporal data](https://arxiv.org/abs/2510.18004)
*Francis Ndikum Nji,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep subspace clustering models are vital for applications such as snowmelt
detection, sea ice tracking, crop health monitoring, infectious disease
modeling, network load prediction, and land-use planning, where multivariate
spatiotemporal data exhibit complex temporal dependencies and reside on
multiple nonlinear manifolds beyond the capability of traditional clustering
methods. These models project data into a latent space where samples lie in
linear subspaces and exploit the self-expressiveness property to uncover
intrinsic relationships. Despite their success, existing methods face major
limitations: they use shallow autoencoders that ignore clustering errors,
emphasize global features while neglecting local structure, fail to model
long-range dependencies and positional information, and are rarely applied to
4D spatiotemporal data. To address these issues, we propose A-DATSC
(Attention-Guided Deep Adversarial Temporal Subspace Clustering), a model
combining a deep subspace clustering generator and a quality-verifying
discriminator. The generator, inspired by U-Net, preserves spatial and temporal
integrity through stacked TimeDistributed ConvLSTM2D layers, reducing
parameters and enhancing generalization. A graph attention transformer based
self-expressive network captures local spatial relationships, global
dependencies, and both short- and long-range correlations. Experiments on three
real-world multivariate spatiotemporal datasets show that A-DATSC achieves
substantially superior clustering performance compared to state-of-the-art deep
subspace clustering models.

</details>


### [15] [Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity](https://arxiv.org/abs/2510.18037)
*Ziyu Lu,Anna J. Li,Alexander E. Ladd,Pascha Matveev,Aditya Deole,Eric Shea-Brown,J. Nathan Kutz,Nicholas A. Steinmetz*

Main category: cs.LG

TL;DR: Eight probabilistic deep learning models (including two foundation models) were evaluated for neural activity forecasting on widefield mouse cortex data. Compared with four classical models and two baselines, several DL models outperformed, with the best achieving informative forecasts up to 1.5 seconds ahead, suggesting potential for closed-loop control and insights into neural temporal structure.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between state-of-the-art probabilistic time-series forecasting and neural activity forecasting to enable better prediction and control of neural systems.

Method: Systematic evaluation of eight probabilistic deep learning models (including two foundation models) against four classical statistical models and two baselines on spontaneous mouse cortex neural activity captured via widefield imaging, assessing forecast performance across multiple horizons.

Result: Several deep learning models consistently outperformed classical approaches across prediction horizons; the best model produced informative forecasts up to 1.5 seconds into the future.

Conclusion: Findings support future control applications in neural systems and open new avenues for probing the intrinsic temporal structure of neural activity.

Abstract: Neural activity forecasting is central to understanding neural systems and
enabling closed-loop control. While deep learning has recently advanced the
state-of-the-art in the time series forecasting literature, its application to
neural activity forecasting remains limited. To bridge this gap, we
systematically evaluated eight probabilistic deep learning models, including
two foundation models, that have demonstrated strong performance on general
forecasting benchmarks. We compared them against four classical statistical
models and two baseline methods on spontaneous neural activity recorded from
mouse cortex via widefield imaging. Across prediction horizons, several deep
learning models consistently outperformed classical approaches, with the best
model producing informative forecasts up to 1.5 seconds into the future. Our
findings point toward future control applications and open new avenues for
probing the intrinsic temporal structure of neural activity.

</details>


### [16] [Cross-Domain Long-Term Forecasting: Radiation Dose from Sparse Neutron Sensor via Spatio-Temporal Operator Network](https://arxiv.org/abs/2510.18041)
*Jay Phil Yoo,Kazuma Kobayashi,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: 提出 STONe 这一非自回归神经算子，用于在异质域间学习稳定映射，从稀疏传感数据中预测高空辐射剂量场；在训练于23年的全球中子数据上，能实现180天预测、毫秒级推理，且具跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在稀疏、跨域传感数据环境下预测不可观测的物理量是一项核心挑战。现有神经算子和大尺度预测模型依赖密集、共址的输入输出场以及短时上下文，难以应对传感与预测发生在不同物理域、且时间尺度较长的现实系统。需要一种跨域的算子学习方法来实现稳定、长期预测。

Method: 提出 STONe（Spatio-Temporal Operator Network），一个非自回归的神经算子，学习从传感域到目标域的稳定函数映射。通过直接从稀疏地面中子测量中推断高空辐射剂量场，定义传感域与目标域之间的非线性算子，且在长预测 horizon 下不需要迭代递归，避免自回归误差累积。训练在23年的全球中子数据上，推理延时为毫秒级。

Result: 证明算子学习可在非共享域条件下泛化；STONe 能给出约180天的准确预测，并具备快速推理能力。

Conclusion: 该框架为跨域算子推断提供了一般性原则，使在物理、气候与能源系统中的复杂时空场的实时预测成为可能。

Abstract: Forecasting unobservable physical quantities from sparse, cross-domain sensor
data is a central unsolved problem in scientific machine learning. Existing
neural operators and large-scale forecasters rely on dense, co-located
input-output fields and short temporal contexts, assumptions that fail in
real-world systems where sensing and prediction occur on distinct physical
manifolds and over long timescales. We introduce the Spatio-Temporal Operator
Network (STONe), a non-autoregressive neural operator that learns a stable
functional mapping between heterogeneous domains. By directly inferring
high-altitude radiation dose fields from sparse ground-based neutron
measurements, STONe demonstrates that operator learning can generalize beyond
shared-domain settings. It defines a nonlinear operator between sensor and
target manifolds that remains stable over long forecasting horizons without
iterative recurrence. This challenges the conventional view that operator
learning requires domain alignment or autoregressive propagation. Trained on 23
years of global neutron data, STONe achieves accurate 180-day forecasts with
millisecond inference latency. The framework establishes a general principle
for cross-domain operator inference, enabling real-time prediction of complex
spatiotemporal fields in physics, climate, and energy systems.

</details>


### [17] [Measure-Theoretic Anti-Causal Representation Learning](https://arxiv.org/abs/2510.18052)
*Arman Behnam,Binghui Wang*

Main category: cs.LG

TL;DR: ACIA introduces a two-level, measure-theoretic framework for anti-causal representation learning. It uses low-level representations to model how labels generate observations and high-level representations to capture stable causal patterns across environment variations. It employs interventional kernels to handle prefect and imperfect interventions without relying on explicit causal graphs, supports high-dimensional data, and provides theoretical OOD generalization guarantees. Empirically, ACIA outperforms state-of-the-art methods on synthetic and real-world medical datasets in accuracy and invariance metrics, with tight bounds on training vs unseen environment performance gaps.


<details>
  <summary>Details</summary>
Motivation: In anti-causal settings, where labels cause features, standard causal-learning methods struggle due to reliance on explicit causal structures, high-dimensional data, and varying interventions. There is a need for a framework that can handle prefect and imperfect interventions, scale to high dimensions, and provide principled guarantees for out-of-distribution generalization without requiring explicit causal graphs.

Method: ACIA uses a two-level representation: a low-level layer that captures how labels generate observations and a high-level layer that learns stable causal patterns across environment-specific variations. It introduces interventional kernels to model prefect and imperfect interventions. The framework is measure-theoretic and does not require explicit causal graphs. The paper provides theoretical guarantees on OOD generalization and tight bounds on training–test performance gaps.

Result: Empirical evaluations on synthetic and real-world medical datasets show ACIA consistently outperforms state-of-the-art methods in accuracy and invariance metrics. Theoretical results establish tight bounds on the performance gap between training and unseen environments, supporting robust anti-causal learning.

Conclusion: ACIA offers a robust, scalable approach to anti-causal representation learning with invariance across environments, eliminating the need for explicit causal structures while providing strong theoretical guarantees and strong empirical performance on both synthetic and real-world data.

Abstract: Causal representation learning in the anti-causal setting (labels cause
features rather than the reverse) presents unique challenges requiring
specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a
novel measure-theoretic framework for anti-causal representation learning. ACIA
employs a two-level design, low-level representations capture how labels
generate observations, while high-level representations learn stable causal
patterns across environment-specific variations. ACIA addresses key limitations
of existing approaches by accommodating prefect and imperfect interventions
through interventional kernels, eliminating dependency on explicit causal
structures, handling high-dimensional data effectively, and providing
theoretical guarantees for out-of-distribution generalization. Experiments on
synthetic and real-world medical datasets demonstrate that ACIA consistently
outperforms state-of-the-art methods in both accuracy and invariance metrics.
Furthermore, our theoretical results establish tight bounds on performance gaps
between training and unseen environments, confirming the efficacy of our
approach for robust anti-causal learning.

</details>


### [18] [Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models](https://arxiv.org/abs/2510.18053)
*Jiajun Fan,Tong Wei,Chaoran Cheng,Yuxin Chen,Ge Liu*

Main category: cs.LG

TL;DR: 提出 ADRPO，通过基于优势估计自适应调整正则化强度，解决强化学习微调中探索-开发权衡的问题；在 Wasserstein-2 正则化下实现流式匹配，提升文本到图像、LLM 微调及多模态推理的表现；在多种规模模型上超过固定正则化与离线方法，具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 在生成模型微调中，固定的散度正则化导致在奖励优化与模型能力保留之间存在固有矛盾。需要根据样本价值动态调整正则化以实现更高效的探索与稳健的开发。

Method: 提出 ADRPO：基于优势估计对正则化强度进行自适应调整，降低高价值样本的正则化、对低价值样本施加强正则化；在文本到图像的流式匹配中应用 Wasserstein-2 正则化；并扩展到文本-only LLM 与多模态推理模型的 KL 正则化微调，提升现有在线方法（如 GRPO）的性能。

Result: 在文本到图像生成中实现比离线 DPO 和在线固定正则化方法（如 ORW-CFM-W2）更好的语义对齐与多样性；2B 参数的 SD3 超越更大规模的 4.8B 和 12B 模型，在属性绑定、语义一致性、艺术风格转换和组合控制方面表现突出；在 LLM 微调和多模态推理中，ADRPO 展现出逃离局部最优的主动探索能力，以及在 7B 模型上超越商用更大模型（如 Gemini 2.5 Pro、GPT-4o Audio）；对多模态与文本推理模型具有良好的通用性。

Conclusion: ADRPO 为探索-开发权衡提供一个通用、即插即用的正则化自适应框架，提升基于在线 RL 的微调方法（如 GRPO），可推广至多种生成体系和模态，改善生成质量与多样性，同时保持稳定性。

Abstract: Balancing exploration and exploitation during reinforcement learning
fine-tuning of generative models presents a critical challenge, as existing
approaches rely on fixed divergence regularization that creates an inherent
dilemma: strong regularization preserves model capabilities but limits reward
optimization, while weak regularization enables greater alignment but risks
instability or reward hacking. We introduce Adaptive Divergence Regularized
Policy Optimization (ADRPO), which automatically adjusts regularization
strength based on advantage estimates-reducing regularization for high-value
samples while applying stronger regularization to poor samples, enabling
policies to navigate between exploration and aggressive exploitation according
to data quality. Our implementation with Wasserstein-2 regularization for flow
matching generative models achieves remarkable results on text-to-image
generation, achieving better semantic alignment and diversity than offline
methods like DPO and online methods with fixed regularization like ORW-CFM-W2.
ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B
and 12B parameters in attribute binding, semantic consistency, artistic style
transfer, and compositional control while maintaining generation diversity.
ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and
multi-modal reasoning models, enhancing existing online RL methods like GRPO.
In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local
optima through active exploration, while in multi-modal audio reasoning, it
outperforms GRPO through superior step-by-step reasoning, enabling a 7B model
to outperform substantially larger commercial models including Gemini 2.5 Pro
and GPT-4o Audio, offering an effective plug-and-play solution to the
exploration-exploitation challenge across diverse generative architectures and
modalities.

</details>


### [19] [SPACeR: Self-Play Anchoring with Centralized Reference Models](https://arxiv.org/abs/2510.18060)
*Wei-Jer Chang,Akshay Rangesh,Kevin Joseph,Matthew Strong,Masayoshi Tomizuka,Yihan Hu,Wei Zhan*

Main category: cs.LG

TL;DR: SPACeR 框架将预训练的标记自回归运动模型作为集中参考策略，通过概率奖励和 KL 约束引导去中心化自我对弈，使策略在人类驾驶分布附近，同时保持强化学习的可扩展性；在 Waymo Sim Agents Challenge 达成与模仿学习相近的性能，同时实现大幅提升的推理速度和更小的模型规模，并可用于快速闭环 Ego 规划评估。


<details>
  <summary>Details</summary>
Motivation: 需要在真实感人类行为与高效可扩展的多智能体学习之间取得平衡。现有模仿学习在真实感方面有优势但推理慢、成本高；自我对弈 RL 虽具扩展性但常受启发式设计和奖励塑形影响，可能偏离人类规范。

Method: 提出 SPACeR：以预训练的标记自回归运动模型作为集中参考策略，提供似然奖励与 KL 效应来锚定策略分布，同时通过去中心化自我对弈实现 RL 的可扩展性。

Result: 在 Waymo Sim Agents Challenge 中实现与模仿学习策略竞争力，同时推理速度可快达 10x，模型参数规模比大型生成模型少约 50x。并在闭环 ego 规划评测中证明其作为快速、可扩展的交通仿真对验证规划器质量具有新范式意义。

Conclusion: SPACeR 将强大参考模型与自我对弈 RL 结合，兼顾人类驾驶分布与 RL 的可扩展性，提供一种快速且可扩展的自动驾驶策略评估与测试新范式。

Abstract: Developing autonomous vehicles (AVs) requires not only safety and efficiency,
but also realistic, human-like behaviors that are socially aware and
predictable. Achieving this requires sim agent policies that are human-like,
fast, and scalable in multi-agent settings. Recent progress in imitation
learning with large diffusion-based or tokenized models has shown that
behaviors can be captured directly from human driving data, producing realistic
policies. However, these models are computationally expensive, slow during
inference, and struggle to adapt in reactive, closed-loop scenarios. In
contrast, self-play reinforcement learning (RL) scales efficiently and
naturally captures multi-agent interactions, but it often relies on heuristics
and reward shaping, and the resulting policies can diverge from human norms. We
propose SPACeR, a framework that leverages a pretrained tokenized
autoregressive motion model as a centralized reference policy to guide
decentralized self-play. The reference model provides likelihood rewards and KL
divergence, anchoring policies to the human driving distribution while
preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our
method achieves competitive performance with imitation-learned policies while
being up to 10x faster at inference and 50x smaller in parameter size than
large generative models. In addition, we demonstrate in closed-loop ego
planning evaluation tasks that our sim agents can effectively measure planner
quality with fast and scalable traffic simulation, establishing a new paradigm
for testing autonomous driving policies.

</details>


### [20] [Fine-tuning Flow Matching Generative Models with Intermediate Feedback](https://arxiv.org/abs/2510.18072)
*Jiajun Fan,Chaoran Cheng,Shuaike Shen,Xiangxin Zhou,Ge Liu*

Main category: cs.LG

TL;DR: AC-Flow: a robust actor-critic framework for fine-tuning flow-based text-to-image models with stable intermediate value learning, dual-stability updates, and a scalable critic weighting scheme, achieving state-of-the-art alignment and generalization without sacrificing quality or diversity.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning flow-based text-to-image models with intermediate feedback is challenging due to credit assignment and instability in online critic learning; existing regression-based critics often suffer from training instability and model collapse.

Method: AC-Flow introduces reward shaping for stable learning signals, a dual-stability mechanism combining advantage clipping with a critic warm-up phase, and a scalable generalized critic weighting with Wasserstein regularization; applied to Stable Diffusion 3 for text-to-image alignment and generalization to unseen human preference models.

Result: AC-Flow achieves state-of-the-art text-to-image alignment on Stable Diffusion 3 benchmarks and generalizes to unseen human preference models, while maintaining generative quality and diversity using a computationally efficient critic.

Conclusion: AC-Flow provides a robust, scalable framework for finetuning flow-based generative models with stable intermediate-valued learning, enabling better alignment to human preferences without compromising diversity or stability.

Abstract: Flow-based generative models have shown remarkable success in text-to-image
generation, yet fine-tuning them with intermediate feedback remains
challenging, especially for continuous-time flow matching models. Most existing
approaches solely learn from outcome rewards, struggling with the credit
assignment problem. Alternative methods that attempt to learn a critic via
direct regression on cumulative rewards often face training instabilities and
model collapse in online settings. We present AC-Flow, a robust actor-critic
framework that addresses these challenges through three key innovations: (1)
reward shaping that provides well-normalized learning signals to enable stable
intermediate value learning and gradient control, (2) a novel dual-stability
mechanism that combines advantage clipping to prevent destructive policy
updates with a warm-up phase that allows the critic to mature before
influencing the actor, and (3) a scalable generalized critic weighting scheme
that extends traditional reward-weighted methods while preserving model
diversity through Wasserstein regularization. Through extensive experiments on
Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art
performance in text-to-image alignment tasks and generalization to unseen human
preference models. Our results demonstrate that even with a computationally
efficient critic model, we can robustly finetune flow models without
compromising generative quality, diversity, or stability.

</details>


### [21] [R2L: Reliable Reinforcement Learning: Guaranteed Return & Reliable Policies in Reinforcement Learning](https://arxiv.org/abs/2510.18074)
*Nadir Farhi*

Main category: cs.LG

TL;DR: 提出一种可靠性导向的强化学习框架，通过状态扩展将最大化达到阈值的回报概率转化为标准RL问题，能够用现有深度RL算法实现可靠策略，应用于可靠路由。


<details>
  <summary>Details</summary>
Motivation: 现实世界对结果的可靠性和成功概率有强烈要求，尤其在路由、资源分配和安全敏感的序列决策场景中，单纯最大化期望回报往往无法保证最低性能。

Method: 提出以概率阈值超越为目标的可靠RL问题，并通过状态扩展将其等价映射为标准RL问题，理论证明两种表述等价；在此框架下可使用Q-learning、Dueling Double DQN等已有方法求解。

Result: 给出可靠策略存在的理论可行性及其与传统RL方法的一致性；在仿真中的可靠路由任务上，所提方法在有效提升达到时间预算阈值概率的同时，维持相近的平均性能。

Conclusion: 该框架显示了在不引入全新算法的前提下，通过问题建模改造即可在现有深度RL工具箱内实现可靠性保障，适用于随机性和安全性要求高的实际应用。

Abstract: In this work, we address the problem of determining reliable policies in
reinforcement learning (RL), with a focus on optimization under uncertainty and
the need for performance guarantees. While classical RL algorithms aim at
maximizing the expected return, many real-world applications - such as routing,
resource allocation, or sequential decision-making under risk - require
strategies that ensure not only high average performance but also a guaranteed
probability of success. To this end, we propose a novel formulation in which
the objective is to maximize the probability that the cumulative return exceeds
a prescribed threshold. We demonstrate that this reliable RL problem can be
reformulated, via a state-augmented representation, into a standard RL problem,
thereby allowing the use of existing RL and deep RL algorithms without the need
for entirely new algorithmic frameworks. Theoretical results establish the
equivalence of the two formulations and show that reliable strategies can be
derived by appropriately adapting well-known methods such as Q-learning or
Dueling Double DQN. To illustrate the practical relevance of the approach, we
consider the problem of reliable routing, where the goal is not to minimize the
expected travel time but rather to maximize the probability of reaching the
destination within a given time budget. Numerical experiments confirm that the
proposed formulation leads to policies that effectively balance efficiency and
reliability, highlighting the potential of reliable RL for applications in
stochastic and safety-critical environments.

</details>


### [22] [Batch Distillation Data for Developing Machine Learning Anomaly Detection Methods](https://arxiv.org/abs/2510.18075)
*Justus Arweiler,Indra Jungjohann,Aparna Muraleedharan,Heike Leitte,Jakob Burger,Kerstin Münnemann,Fabian Jirasek,Hans Hasse*

Main category: cs.LG

TL;DR: 面向化工过程异常检测的开源数据集：基于批量蒸馏实验的多模态数据集，包含119次实验、带/不带故障、时间序列传感器、NMR、视频/音频、元数据及异常本体，提供不确定性估计，便于训练和可解释ML方法。通过Zenodo公开获取。


<details>
  <summary>Details</summary>
Motivation: 缺乏公开的、带注释的化工过程实验数据，阻碍基于机器学习的异常检测方法的发展。通过建立现实可控的批量蒸馏实验平台并生成大规模数据集，提供高质量、可解释的异常标签，促进ML在过程监控中的应用。

Method: 搭建实验室规模的批量蒸馏装置，进行119次实验，覆盖工况和混合物；大多数异常实验配对有对应的无故障对照；收集传感器/执行器的时序数据、测量不确定性、在线NMR浓度数据，以及视频和音频；提供 extensive metadata、专家注释和基于本体的异常注释；数据以结构化数据库形式通过 doi/Zenodo 发布。

Result: 形成可公开访问的数据集，包含多模态数据和丰富注释，适用于训练和评估ML-based AD方法；数据含故障原因信息，有助于研发可解释性和可追溯的异常检测和缓解策略。

Conclusion: 该数据集将推动基于ML的异常检测在化工过程中的应用，促进可解释/可解释性ML及异常缓解方法的发展，推动该领域研究进展。

Abstract: Machine learning (ML) holds great potential to advance anomaly detection (AD)
in chemical processes. However, the development of ML-based methods is hindered
by the lack of openly available experimental data. To address this gap, we have
set up a laboratory-scale batch distillation plant and operated it to generate
an extensive experimental database, covering fault-free experiments and
experiments in which anomalies were intentionally induced, for training
advanced ML-based AD methods. In total, 119 experiments were conducted across a
wide range of operating conditions and mixtures. Most experiments containing
anomalies were paired with a corresponding fault-free one. The database that we
provide here includes time-series data from numerous sensors and actuators,
along with estimates of measurement uncertainty. In addition, unconventional
data sources -- such as concentration profiles obtained via online benchtop NMR
spectroscopy and video and audio recordings -- are provided. Extensive metadata
and expert annotations of all experiments are included. The anomaly annotations
are based on an ontology developed in this work. The data are organized in a
structured database and made freely available via
doi.org/10.5281/zenodo.17395544. This new database paves the way for the
development of advanced ML-based AD methods. As it includes information on the
causes of anomalies, it further enables the development of interpretable and
explainable ML approaches, as well as methods for anomaly mitigation.

</details>


### [23] [MEG-GPT: A transformer-based foundation model for magnetoencephalography data](https://arxiv.org/abs/2510.18080)
*Rukuang Huang,Sungjun Cho,Chetan Gohil,Oiwi Parker Jones,Mark Woolrich*

Main category: cs.LG

TL;DR: 提出 MEG-GPT：基于 transformer 的大规模前沿模型，用于 MEG 的时空脑动力学建模；引入数据驱动的连续 MEG tokeniser；在 Cam-CAN 数据集上训练并展现生成数据的时空-光谱特性及对下游任务的提升，具备跨会话和跨被试的零-shot泛化能力并可对小数据集进行微调。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在建模大规模脑时空模式时的局限性，MEG 等模态显现出高度丰富的时空结构；受语言与视觉领域 foundation model 成功启示，提出可扩展的 MEG/Foundation 模型以提升神经科学与神经解码任务的性能与泛化能力。

Method: 提出 MEG-GPT：一个基于 transformer 的时序注意力模型，采用下一个时点预测（next time-point prediction）训练；引入数据驱动的 tokeniser，保持 MEG 信号的高时间分辨率且避免有损转换；在 Cam-CAN 的大规模 MEG 数据（612 例，闭眼静息）上对脑区时间序列进行 tokenisation，并对模型进行训练；评估生成数据的时空-谱特性、以及在下游任务中的泛化表现，包含跨会话与跨被试的零-shot 泛化，以及在小数据集上的微调能力。

Result: 模型能够生成具有真实时空-谱特性的 MEG 数据，包含瞬变事件和群体变异；在下游预测任务上实现提升：跨会话零-shot准确率从 0.54 提升至 0.59，跨被试从 0.41 提升至 0.49；并且可对较小标注数据集进行高效微调以提升跨被试解码的表现。

Conclusion: 确立了一个用于脑电生理数据的强大基础模型，为计算神经科学与神经解码领域的应用铺平了道路。

Abstract: Modelling the complex spatiotemporal patterns of large-scale brain dynamics
is crucial for neuroscience, but traditional methods fail to capture the rich
structure in modalities such as magnetoencephalography (MEG). Recent advances
in deep learning have enabled significant progress in other domains, such as
language and vision, by using foundation models at scale. Here, we introduce
MEG-GPT, a transformer based foundation model that uses time-attention and next
time-point prediction. To facilitate this, we also introduce a novel
data-driven tokeniser for continuous MEG data, which preserves the high
temporal resolution of continuous MEG signals without lossy transformations. We
trained MEG-GPT on tokenised brain region time-courses extracted from a
large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that
the learnt model can generate data with realistic spatio-spectral properties,
including transient events and population variability. Critically, it performs
well in downstream decoding tasks, improving downstream supervised prediction
task, showing improved zero-shot generalisation across sessions (improving
accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49)
compared to a baseline methods. Furthermore, we show the model can be
efficiently fine-tuned on a smaller labelled dataset to boost performance in
cross-subject decoding scenarios. This work establishes a powerful foundation
model for electrophysiological data, paving the way for applications in
computational neuroscience and neural decoding.

</details>


### [24] [Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth](https://arxiv.org/abs/2510.18081)
*Jiawei Zhang,Andrew Estornell,David D. Baek,Bo Li,Xiaojun Xu*

Main category: cs.LG

TL;DR: 提出 ADA，在推理时通过在中间重新引入助手头部标记来实现任意深度的对齐/安全性，几乎无开销，适用于多种开源模型，显著提升对抗性攻击下的拒绝率并保留实用性。


<details>
  <summary>Details</summary>
Motivation: 当前 LLMs 展现出浅层对齐，拒绝在开头，面对深度生成或攻击时容易崩溃，需要一种可以在任意深度保持安全的方案。

Method: 在对话中重复使用的头部标记携带强对齐先验，通过在生成过程中重新插入这些标记，使模型重新评估有害性并恢复拒绝；无需修改模型参数，属于推理时防御。

Result: 在 Llama、Gemma、Mistral、Qwen、DeepSeek、gpt-oss 等模型族上，近乎100% 的拒绝率对抗对抗性预填充攻击，减少常见对抗性提示攻击（GCG、AutoDAN、PAIR、TAP）成功率到 <3%，对 benign 任务保持较小的过拒绝。

Conclusion: ADA 提供可部署的高鲁棒性推理时安全性，兼容微调后模型，开销极小，且无参数修改，提升任意深度的安全性与实用性的折中。

Abstract: Large Language Models (LLMs) exhibit strong but shallow alignment: they
directly refuse harmful queries when a refusal is expected at the very start of
an assistant turn, yet this protection collapses once a harmful continuation is
underway (either through the adversarial attacks or via harmful
assistant-prefill attacks). This raises a fundamental question: Can the innate
shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation
depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an
effective inference-time defense with negligible overhead. ADA is built based
on our observation that alignment is concentrated in the assistant header
tokens through repeated use in shallow-refusal training, and these tokens
possess the model's strong alignment priors. By reintroducing these tokens
mid-stream, ADA induces the model to reassess harmfulness and recover refusals
at any point in generation. Across diverse open-source model families (Llama,
Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety
performance without requiring any changes to the base model's parameters. It
secures a near-100% refusal rate against challenging adversarial prefill
attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces
the average success rate of prominent adversarial prompt attacks (such as GCG,
AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving
utility on benign tasks with minimal over-refusal. ADA maintains this
resilience even after the base model undergoes subsequent instruction tuning
(benign or adversarial).

</details>


### [25] [Enhancing mortality prediction in cardiac arrest ICU patients through meta-modeling of structured clinical data from MIMIC-IV](https://arxiv.org/abs/2510.18103)
*Nursultan Mamatov,Philipp Kellmeyer*

Main category: cs.LG

TL;DR: 本研究将结构化临床数据与非结构化文本（出院摘要与放射科报告）相结合，对ICU患者的院内死亡进行早期预测，显示文本特征显著提升模型性能。使用LASSO和XGBoost进行特征筛选，再以这两者筛选出的顶级特征训练多变量逻辑回归；文本信息采用TF-IDF与BERT嵌入。最终整合模型的AUC达到0.918，相较仅结构化数据的0.753有约22%的相对提升，决策曲线表明在0.2-0.8阈值范围内具更高净收益，支持将非结构化临床笔记纳入可解释的风险预测模型。


<details>
  <summary>Details</summary>
Motivation: 提升ICU死亡风险的早期预测准确性与临床实用性，通过整合结构化数据与非结构化文本信息来提取更丰富的表征。

Method: 数据源为MIMIC-IV；先用LASSO与XGBoost进行特征筛选；在两者筛选出的顶级特征上训练多变量逻辑回归模型；引入文本特征（TF-IDF、BERT嵌入）以构建组合型结构化+文本输入；评估指标包括AUC与决策曲线分析。

Result: 组合模型AUC为0.918，结构化数据单独得0.753，相对提升约22%；决策曲线显示在广泛阈值(0.2-0.8)范围内净收益更高。

Conclusion: 非结构化临床笔记具显著的预测价值，支持将文本信息纳入可解释的风险预测模型以提升ICU患者的早期分层和干预决策。

Abstract: Accurate early prediction of in-hospital mortality in intensive care units
(ICUs) is essential for timely clinical intervention and efficient resource
allocation. This study develops and evaluates machine learning models that
integrate both structured clinical data and unstructured textual information,
specifically discharge summaries and radiology reports, from the MIMIC-IV
database. We used LASSO and XGBoost for feature selection, followed by a
multivariate logistic regression trained on the top features identified by both
models. Incorporating textual features using TF-IDF and BERT embeddings
significantly improved predictive performance. The final logistic regression
model, which combined structured and textual input, achieved an AUC of 0.918,
compared to 0.753 when using structured data alone, a relative improvement 22%.
The analysis of the decision curve demonstrated a superior standardized net
benefit in a wide range of threshold probabilities (0.2-0.8), confirming the
clinical utility of the model. These results underscore the added prognostic
value of unstructured clinical notes and support their integration into
interpretable feature-driven risk prediction models for ICU patients.

</details>


### [26] [Latent Discrete Diffusion Models](https://arxiv.org/abs/2510.18114)
*Dario Shariatian,Alain Durmus,Stefano Peluchetti*

Main category: cs.LG

TL;DR: 提出 Latent Discrete Diffusion Models (LDDMs) 来改进离散语言扩散的联合结构。通过结合对 token 的掩码离散扩散与潜在嵌入的连续扩散，提升少步生成的质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码去噪器往往在跨位置的转移独立建模，导致对联合结构的削弱，进而在少步采样时质量下降。需要能够捕捉跨 token 依赖的建模策略。

Method: 提出 LDDMs，将离散 token 的掩码扩散与潜在嵌入的连续扩散耦合，形成潜在通道以传递跨-token依赖与更柔软信号。给出两种实现：FUJI-LDDMs（对 token 与 latent 进行完全联合去噪）和 SEQ-LDDMs（先解 latent，再在条件于 latent 的情况下解离散链）。推导 ELBO 风格的目标函数，并讨论如何在学习具有信息性潜在表示的同时保持 Diffusion 模型的可学习性。

Result: 在实验中，LDDMs 相较于最先进的掩码离散扩散基线，在无条件生成指标上有提升；在低采样预算下也表现良好，因为可以在每一步 unmask 大量 token 提高效率。

Conclusion: LDDMs 通过潜在通道提供更丰富的信号和跨-token依赖，改善联合结构并提升生成质量与采样效率。FUJI-LDDMs 与 SEQ-LDDMs 这两种变体在不同设置下展现出有效性。

Abstract: We study discrete diffusion for language and other categorical data and focus
on a common limitation of masked denoisers: reverse transitions typically
factorize across positions, which can weaken joint structure and degrade
quality in few-step generation. We propose \emph{Latent Discrete Diffusion
Models} (LDDMs), which couple a masked discrete diffusion over tokens with a
continuous diffusion over latent embeddings. The latent channel provides a
softer signal and carries cross-token dependencies that help resolve
ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully
joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially
resolve the latent and then the discrete chain conditionally on it. For both
variants we derive ELBO-style objectives and discuss design choices to learn
informative latents yet amenable to diffusoin modeling. In experiments, LDDMs
yield improvements on unconditional generation metrics as compared to
state-of-the-art masked discrete diffusion baselines, and are effective at
lower sampling budgets, where unmasking many tokens per step is desirable.

</details>


### [27] [Gradient Variance Reveals Failure Modes in Flow-Based Generative Models](https://arxiv.org/abs/2510.18118)
*Teodora Reu,Sixtine Dromigny,Michael Bronstein,Francisco Vargas*

Main category: cs.LG

TL;DR: Rectified Flows的直路径目标在训练中可能导致记忆化训练对偶对的向量场；引入小噪声可以恢复泛化。


<details>
  <summary>Details</summary>
Motivation: 分析直路径目标下潜在的失败模式，揭示在确定性训练中梯度方差低导致对训练对的记忆化，以及在高斯-高斯传输情境下的行为。

Method: 通过研究高斯到高斯的传输，比较随机与确定性训练下损失梯度方差对优化偏好的影响；证明在所有插值线相交的情形下，推断阶段会复现训练中的具体配对；给出存在记忆化向量场的更一般证明，并证明直路径目标会收敛到该定义不清的场；在推断时通过确定性积分再现训练对的配对；在CelebA数据集进行经验验证。

Result: 揭示记忆化机制：确定性训练下低梯度方差促使对任意训练对进行记忆；存在记忆化向量场，即使插值线相交；推断时的确定性积分重现训练对。对CelebA的实验验证表明，确定性插值导致记忆化，加入小噪声可恢复泛化。

Conclusion: 直路径优化在某些设置下是高度不稳定且易趋向记忆化的，需要通过引入随机性/噪声或重新设计目标以避免对训练对的硬记忆。

Abstract: Rectified Flows learn ODE vector fields whose trajectories are straight
between source and target distributions, enabling near one-step inference. We
show that this straight-path objective conceals fundamental failure modes:
under deterministic training, low gradient variance drives memorization of
arbitrary training pairings, even when interpolant lines between pairs
intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport
and use the loss gradient variance across stochastic and deterministic regimes
to characterize which vector fields optimization favors in each setting. We
then show that, in a setting where all interpolating lines intersect, applying
Rectified Flow yields the same specific pairings at inference as during
training. More generally, we prove that a memorizing vector field exists even
when training interpolants intersect, and that optimizing the straight-path
objective converges to this ill-defined field. At inference, deterministic
integration reproduces the exact training pairings. We validate our findings
empirically on the CelebA dataset, confirming that deterministic interpolants
induce memorization, while the injection of small noise restores
generalization.

</details>


### [28] [Efficient Long-context Language Model Training by Core Attention Disaggregation](https://arxiv.org/abs/2510.18121)
*Yonghao Zhuang,Junda Chen,Bo Pang,Yi Gu,Yibo Zhu,Yimin Jiang,Ion Stoica,Eric Xing,Hao Zhang*

Main category: cs.LG

TL;DR: CAD将核心注意力计算与模型其他部分解耦并在专用服务器执行，以解决长上下文下的负载不均和拖尾问题，从而提升训练吞吐量。DistCA系统通过ping-pong执行和就地内存管理，在512 GPUs、上下文长度可达512k时吞吐提升至1.35x，实现接近完美的计算与内存平衡。


<details>
  <summary>Details</summary>
Motivation: 长上下文下，核心注意力的平方级计算增长相对于其他组件的近线性增长导致负载不均、数据与流水线拖尾，从而限制训练吞吐与扩展性。

Method: 将核心注意力QK^T V解耦为无状态、可分割的任务，分派给专用注意力服务器；按token级粒度划分任务并动态重排以均衡计算负载；利用DistCA的ping-pong执行实现通信与计算重叠，同时在注意力服务器上就地执行以降低内存开销。

Result: 端到端训练吞吐量可提升至1.35x，消除了数据与流水线拖尾，计算与内存负载实现接近完美平衡。该方法在512个H200 GPU、上下文长度高达512k时表现显著。

Conclusion: 核心注意力解耦与专用注意力服务器的组合，以及-cad网的分布式实现，显著提升长上下文LLM的训练效率与可扩展性，同时降低内存压力与数据/流水线的不平衡风险。

Abstract: We present core attention disaggregation (CAD), a technique that improves
long-context large language model training by decoupling the core attention
computation, softmax(QK^T)V, from the rest of the model and executing it on a
separate pool of devices. In existing systems, core attention is colocated with
other layers; at long context lengths, its quadratic compute growth compared to
the near-linear growth of other components causes load imbalance and stragglers
across data and pipeline parallel groups. CAD is enabled by two observations.
First, core attention is stateless: it has no trainable parameters and only
minimal transient data, so balancing reduces to scheduling compute-bound tasks.
Second, it is composable: modern attention kernels retain high efficiency when
processing fused batches of token-level shards with arbitrary lengths. CAD
partitions core attention into token-level tasks and dispatches them to
dedicated attention servers, which dynamically rebatch tasks to equalize
compute without sacrificing kernel efficiency. We implement CAD in a system
called DistCA, which uses a ping-pong execution scheme to fully overlap
communication with computation and in-place execution on attention servers to
reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,
DistCA improves end-to-end training throughput by up to 1.35x, eliminates data
and pipeline parallel stragglers, and achieves near-perfect compute and memory
balance.

</details>


### [29] [ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control](https://arxiv.org/abs/2510.18232)
*Yuzheng Hu,Ryan McKenna,Da Yu,Shanshan Wu,Han Zhao,Zheng Xu,Peter Kairouz*

Main category: cs.LG

TL;DR: 提出 ACTG-ARL 框架，通过层次化的特征学习/条件文本生成与 Anchored RL 提升差分隐私条件下的文本生成质量与控制力，显著提升 MAUVE 指标。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私保护下生成高质量文本的任务面临统计属性丢失、噪声导致的效用下降以及对生成控制的不足。

Method: 提出两部分：1) 层次化框架将 DP 文本生成分解为特征学习（使用丰富的表格特征/ DP 表格合成器）和条件文本生成（DP 微调的条件生成器）；2) Anchored RL，在后训练阶段将 RL 与最优选择数据的 SFT 锚定结合，提升指令遵循和避免奖励黑客，形成 ACTG-ARL。

Result: 在 MAUVE 上相较于先前工作提升约 20%，实现更高质量的 DP 合成文本以及对条件生成的更好控制。

Conclusion:  ACTG-ARL 提供一个端到端的 DP 文本合成框架，展示了通过特征驱动的生成与锚定的强化学习如何提升隐私保护下的文本生成质量和可控性。

Abstract: Generating high-quality synthetic text under differential privacy (DP) is
critical for training and evaluating language models without compromising user
privacy. Prior work on synthesizing DP datasets often fail to preserve key
statistical attributes, suffer utility loss from the noise required by DP, and
lack fine-grained control over generation. To address these challenges, we make
two contributions. First, we introduce a hierarchical framework that decomposes
DP synthetic text generation into two subtasks: feature learning and
conditional text generation. This design explicitly incorporates learned
features into the generation process and simplifies the end-to-end synthesis
task. Through systematic ablations, we identify the most effective
configuration: a rich tabular schema as feature, a DP tabular synthesizer, and
a DP fine-tuned conditional generator, which we term ACTG
(Attribute-Conditioned Text Generation). Second, we propose Anchored RL (ARL),
a post-training method that improves the instruction-following ability of ACTG
for conditional generation. ARL combines RL to boost control with an SFT anchor
on best-of-$N$ data to prevent reward hacking. Together, these components form
our end-to-end algorithm ACTG-ARL, which advances both the quality of DP
synthetic text (+20% MAUVE over prior work) and the control of the conditional
generator under strong privacy guarantees.

</details>


### [30] [Rethinking PCA Through Duality](https://arxiv.org/abs/2510.18130)
*Jan Quan,Johan Suykens,Panagiotis Patrinos*

Main category: cs.LG

TL;DR: 通过差分凸框架重新分析PCA，揭示核化、离样本泛化，以及鲁棒PCA的新算法与多种联系。


<details>
  <summary>Details</summary>
Motivation: 受到自注意力与核PCA之间新近联系的启发，重新审视PCA的理论基础，探索DC框架下的新表述与理解。

Method: 在差分凸（DC）框架内提出多种新表述，证明PCA相关问题的核化与离样本适用性；将同时迭代（与QR算法相关）视为DCA的一个实例，给出新的PCA算法并与现有方法进行实证对比；提出一个核化的对偶表述，用于鲁棒PCA变体，目标是最小化重构误差的l1偏差。

Result: 提供了PCA及其核化、鲁棒版本的理论与算法新视角，建立了将并行/迭代算法纳入DC框架的统一解释，并在实验中展现出相对于状态-of-the-art的竞争力。

Conclusion: 差分凸框架为PCA提供了统一、可核化且鲁棒的理论与算法基础，揭示自注意力与PCA之间的联系、对经典算法的新解读，并给出具有实际优势的新算法及其鲁棒扩展。

Abstract: Motivated by the recently shown connection between self-attention and
(kernel) principal component analysis (PCA), we revisit the fundamentals of
PCA. Using the difference-of-convex (DC) framework, we present several novel
formulations and provide new theoretical insights. In particular, we show the
kernelizability and out-of-sample applicability for a PCA-like family of
problems. Moreover, we uncover that simultaneous iteration, which is connected
to the classical QR algorithm, is an instance of the difference-of-convex
algorithm (DCA), offering an optimization perspective on this longstanding
method. Further, we describe new algorithms for PCA and empirically compare
them with state-of-the-art methods. Lastly, we introduce a kernelizable dual
formulation for a robust variant of PCA that minimizes the $l_1$ deviation of
the reconstruction errors.

</details>


### [31] [Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation](https://arxiv.org/abs/2510.18541)
*Giovanni De Muri,Mark Vero,Robin Staab,Martin Vechev*

Main category: cs.LG

TL;DR: 提出并验证一种可 transferable 的后门注入方法 T-MTB，使来自被污染的教师模型的知识蒸馏能把后门传导到学生模型，揭示知识蒸馏在不可信教师情况下的安全风险；与以往后门不同，T-MTB 通过由若干在蒸馏数据中通常单独出现的特定词汇组成的复合触发器实现隐蔽性和可传递性，覆盖 jailbreaking（越狱）与 content modulation（内容调控）等攻击场景，且在四大模型家族上产生可观的风险。


<details>
  <summary>Details</summary>
Motivation: 在知识蒸馏场景中，教师模型可能来自不可信方，其后门机制可能被蒸馏传递给学生模型；但以往研究发现后门往往难以在学生模型中传递，因此对蒸馏安全性的系统性评估不足。需要理解和评估在存在后门教师时蒸馏过程的安全风险。

Method: 提出 T-MTB（可转移的多元触发后门）——通过构造一个由多组在蒸馏数据中常独立出现的特定词汇组合而成的复合触发器，使得 poisoned 教师在保持隐蔽性的同时，蒸馏过程中的信号能够在单个词汇层面被观察到并传递至学生模型；在 jailbreaking 与 content modulation 两个攻击场景，以及四大模型家族上进行系统性实验与分析。

Result: 研究表明：传统后门基本不传递给学生模型；而通过 T-MTB 可以实现可转移的后门传导，且在蒸馏过程中对学生的影响显著，覆盖多场景与多模型族。

Conclusion: 知识蒸馏对来自不可信教师的后门高风险，需要引入对蒸馏环节的安全防护与检测；T-MTB 的存在揭示了后门可通过蒸馏实现转移的威胁，应推动对检测、审计和防御机制的研究。

Abstract: LLMs are often used by downstream users as teacher models for knowledge
distillation, compressing their capabilities into memory-efficient models.
However, as these teacher models may stem from untrusted parties, distillation
can raise unexpected security risks. In this paper, we investigate the security
implications of knowledge distillation from backdoored teacher models. First,
we show that prior backdoors mostly do not transfer onto student models. Our
key insight is that this is because existing LLM backdooring methods choose
trigger tokens that rarely occur in usual contexts. We argue that this
underestimates the security risks of knowledge distillation and introduce a new
backdooring technique, T-MTB, that enables the construction and study of
transferable backdoors. T-MTB carefully constructs a composite backdoor
trigger, made up of several specific tokens that often occur individually in
anticipated distillation datasets. As such, the poisoned teacher remains
stealthy, while during distillation the individual presence of these tokens
provides enough signal for the backdoor to transfer onto the student. Using
T-MTB, we demonstrate and extensively study the security risks of transferable
backdoors across two attack scenarios, jailbreaking and content modulation, and
across four model families of LLMs.

</details>


### [32] [Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria](https://arxiv.org/abs/2510.18183)
*Eason Yu,Tzu Hao Liu,Yunke Wang,Clément L. Canonne,Nguyen H. Tran,Chang Xu*

Main category: cs.LG

TL;DR: 在两人零和博弈中，提出了一种固定高正则化強度的对策，在参考策略不断更新的情况下实现严格单调改进并收敛到精确纳什均衡，给出理论保证并给出可用于大规模域的 Nash Policy Gradient (NashPG) 算法，实验证明其在经典基准测试和大域如 Battleship、No-Limit Texas Hold’em 中具备与现有模型无关方法相当甚至更低的利用性并带来更高的 Elo。


<details>
  <summary>Details</summary>
Motivation: 旨在解决基于正则化的最近迭代收敛方法在逼近纳什均衡时需要将正则化强度趋近于零而导致学习不稳定的问题；提出固定较大正则化强度并通过对参考策略的迭代改进实现稳定且收敛的替代路径。

Method: 在固定的大正则化强度条件下，迭代地改进参考策略，使得策略更新具有严格单调改进性质，且在两人零和博弈中无需要求唯一性即可收敛到精确的纳什均衡。基于该框架，提出 Nash Policy Gradient (NashPG) 算法，保持策略梯度方法的通用性，只依赖当前策略和参考策略进行更新。

Result: 理论上证明在两人零和博弈中，该方法保证严格单调改进并收敛到精确的纳什均衡；在实践中，NashPG 与先前的无模型方法在经典基准游戏上可实现相当或更低的 exploitable 值，并能扩展到 Battleship 和 No-Limit Texas Hold’em 等大域，后者的 Elo 性能显著提升。

Conclusion: 该框架提供了对抗强化学习中稳定接近纳什均衡的理论和方法论基础，NashPG 兼具策略梯度的泛化性与对当前/参考策略的依赖性，适用于大规模不完美信息博弈的稳健训练。

Abstract: Finding Nash equilibria in imperfect-information games remains a central
challenge in multi-agent reinforcement learning. While regularization-based
methods have recently achieved last-iteration convergence to a regularized
equilibrium, they require the regularization strength to shrink toward zero to
approximate a Nash equilibrium, often leading to unstable learning in practice.
Instead, we fix the regularization strength at a large value for robustness and
achieve convergence by iteratively refining the reference policy. Our main
theoretical result shows that this procedure guarantees strictly monotonic
improvement and convergence to an exact Nash equilibrium in two-player zero-sum
games, without requiring a uniqueness assumption. Building on this framework,
we develop a practical algorithm, Nash Policy Gradient (NashPG), which
preserves the generalizability of policy gradient methods while relying solely
on the current and reference policies. Empirically, NashPG achieves comparable
or lower exploitability than prior model-free methods on classic benchmark
games and scales to large domains such as Battleship and No-Limit Texas
Hold'em, where NashPG consistently attains higher Elo ratings.

</details>


### [33] [ActivationReasoning: Logical Reasoning in Latent Activation Spaces](https://arxiv.org/abs/2510.18184)
*Lukas Helff,Ruben Härle,Wolfgang Stammer,Felix Friedrich,Manuel Brack,Antonia Wüst,Hikaru Shindo,Patrick Schramowski,Kristian Kersting*

Main category: cs.LG

TL;DR: 提出 ActivationReasoning (AR) 框架，将显式逻辑推理嵌入大语言模型的潜在空间。通过稀疏自编码器提取潜在概念并建立词典，在推理阶段将激活概念映射为逻辑命题，运用逻辑规则进行推理以推导高阶概念、组合新概念并引导模型行为。通过多任务评估，AR 展现出对复杂推理的可扩展性、对抽象与上下文敏感任务的泛化，以及跨模型的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs 的内部推理不透明和难以控制的问题。现有的稀疏自编码器能提高潜在特征的可解释性，但特征脆弱且被动，缺乏系统化推理与模型控制能力。

Method: 三阶段框架：1) 找到潜在表示，将初始潜在概念表示组织成词典，通常通过 SAEs；2) 激活命题，在推理时检测激活概念并将其映射到逻辑命题；3) 逻辑推理，对这些命题应用逻辑规则，以推导高阶结构、组合新概念并引导模型行为。

Result: 在 PrOntoQA（多跳推理）、Rail2Country（抽象与对间接概念线索的鲁棒性）、ProverQA（自然语言与多样化语言推理）、BeaverTails（情境相关安全性）等任务上评估。AR 能随着推理复杂性扩展、对抽象与情境敏感任务具备泛化能力、并能跨模型骨架迁移。

Conclusion: 将逻辑结构锚定于潜在激活之上不仅提升透明度，还使结构化推理、可靠控制与对期望行为的对齐成为可能，为实现更可审计的AI提供了一条可行路径。

Abstract: Large language models (LLMs) excel at generating fluent text, but their
internal reasoning remains opaque and difficult to control. Sparse autoencoders
(SAEs) make hidden activations more interpretable by exposing latent features
that often align with human concepts. Yet, these features are fragile and
passive, offering no mechanism for systematic reasoning or model control. To
address this, we introduce ActivationReasoning (AR), a framework that embeds
explicit logical reasoning into the latent space of LLMs. It proceeds in three
stages: (1) Finding latent representations, first latent concept
representations are identified (e.g., via SAEs) and organized into a
dictionary; (2) Activating propositions, at inference time AR detects
activating concepts and maps them to logical propositions; and (3)Logical
reasoning, applying logical rules over these propositions to infer higher-order
structures, compose new concepts, and steer model behavior. We evaluate AR on
multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept
cues (Rail2Country), reasoning over natural and diverse language (ProverQA),
and context-sensitive safety (BeaverTails). Across all tasks, AR scales
robustly with reasoning complexity, generalizes to abstract and
context-sensitive tasks, and transfers across model backbones. These results
demonstrate that grounding logical structure in latent activations not only
improves transparency but also enables structured reasoning, reliable control,
and alignment with desired behaviors, providing a path toward more reliable and
auditable AI.

</details>


### [34] [Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations](https://arxiv.org/abs/2510.18228)
*Zhendong Mi,Qitao Tan,Grace Li Zhang,Zhaozhuo Xu,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: P-GAP通过在低维梯度空间中估计梯度并将扰动对齐到投影梯度的方向，从而在零阶优化下对大语言模型进行微调，降低方差并减少扰动参数数量，从而提升收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: 解决零阶优化在大规模语言模型微调中的高方差和慢收敛问题，以及在内存受限条件下提升训练效率。

Method: 先估计一个低维梯度空间；在该投影梯度方向内对扰动进行对齐，使得被扰动的参数数量减小，进而降低估计方差；通过零阶估计实现微调，提升收敛速度。

Result: 实验显示在分类任务上精度提升可达6%，在生成任务上提升可达12%；训练迭代次数减少最高81%，GPU小时数减少约70%。

Conclusion: P-GAP实现了快速、可扩展且资源高效的零阶LLM微调，适合在内存受限条件下进行大模型微调，并显著提高训练效率和性能。

Abstract: Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization
has emerged as a promising alternative to traditional gradient-based methods
due to its reduced memory footprint requirement. However, existing ZO methods
suffer from high variance in gradient estimation, leading to slow convergence
and suboptimal performance on large-scale models. In this work, we propose
P-GAP, a fast LLM fine-tuning approach through zeroth-order optimization with
Projected Gradient-Aligned Perturbations. Specifically, we first estimate a
low-dimensional gradient space and then align perturbations in projected
gradients' direction within the space. This approach enables reduced the number
of perturbed parameters and decreased variance, therefore accelerated
convergence for LLM fine-tuning. Experiments on LLMs show that P-GAP
consistently surpasses the baselines, achieving up to 6% increase in accuracy
on classification tasks and up to 12% higher accuracy on generation tasks, with
up to about 81% less training iterations and 70% less GPU hours. These results
demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM
fine-tuning.

</details>


### [35] [Fostering the Ecosystem of AI for Social Impact Requires Expanding and Strengthening Evaluation Standards](https://arxiv.org/abs/2510.18238)
*Bryan Wilder,Angela Zhou*

Main category: cs.LG

TL;DR: 扩展对社会影响的评估并强化对部署系统影响的评估，以实现社会影响研究生态的可持续性；当前评审标准偏向部署与新颖的ML方法，可能带来激励扭曲。


<details>
  <summary>Details</summary>
Motivation: 当前的研究评审往往奖励同时实现部署和新颖方法的项目，这可能忽略了仅在应用性或方法性方面有价值的工作，损害长期的研究生态。

Method: 提出概念性论证和框架，分析现有评审标准对研究动机的影响，并提出两点主张：扩大社会影响的概念并改进已部署系统的影响评估。

Result: 通过论证与框架提出，若采纳这两点，研究生态将更具可持续性，应用和方法性贡献都将受到认可。

Conclusion: 研究者和评审者应同时采用更广泛的社会影响概念和对部署系统的影响进行严格评估，从而促进社会影响领域的长期生态健康。

Abstract: There has been increasing research interest in AI/ML for social impact, and
correspondingly more publication venues have refined review criteria for
practice-driven AI/ML research. However, these review guidelines tend to most
concretely recognize projects that simultaneously achieve deployment and novel
ML methodological innovation. We argue that this introduces incentives for
researchers that undermine the sustainability of a broader research ecosystem
of social impact, which benefits from projects that make contributions on
single front (applied or methodological) that may better meet project partner
needs. Our position is that researchers and reviewers in machine learning for
social impact must simultaneously adopt: 1) a more expansive conception of
social impacts beyond deployment and 2) more rigorous evaluations of the impact
of deployed systems.

</details>


### [36] [Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment](https://arxiv.org/abs/2510.18240)
*Haobin Li,Yijie Lin,Peng Hu,Mouxing Yang,Xi Peng*

Main category: cs.LG

TL;DR: 提出一个鲁棒的多模态实体对齐框架RULE，针对双层噪声对应（DNC）问题，在实体-属性层面的噪声与跨图的实体-实体及属性-属性噪声上进行可靠性估计与噪声抑制，并通过对应关系推理模块提升对齐准确性，在五个基准上对比七个SOTA方法取得有效性提升，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多模态知识图在对齐时存在双层噪声：实体-属性的标注噪声以及跨图的实体-实体和属性-属性对齐噪声，导致现有方法在假设干净对齐时表现下降；因此需要能估计并对噪声进行鲁棒处理的对齐框架。

Method: 提出RULE框架：1) 设计专门的两层次可靠性估计，分别对实体-属性和跨图对齐的可靠性进行评估；2) 基于估计的可靠性，在属性融合阶段抑制实体内部噪声，在跨图矛盾消解阶段防止对噪声的过拟合；3) 引入对应关系推理模块，挖掘跨图的属性-属性连接，提升对齐准确性。训练阶段的设计与推理阶段的模块共同作用以应对DNC。

Result: 在五个基准数据集上进行广泛实验， RULE 相较于七个SOTA方法在应对DNC方面表现更优，证实了鲁棒性与可有效性。

Conclusion:  RULE 将可靠性估计、噪声抑制和跨图属性推理整合，提供了一种有效的双层噪声对齐解决方案，并在五基准上证明了优越性，代码已公开。

Abstract: Multi-modal entity alignment (MMEA) aims to identify equivalent entities
across heterogeneous multi-modal knowledge graphs (MMKGs), where each entity is
described by attributes from various modalities. Existing methods typically
assume that both intra-entity and inter-graph correspondences are faultless,
which is often violated in real-world MMKGs due to the reliance on expert
annotations. In this paper, we reveal and study a highly practical yet
under-explored problem in MMEA, termed Dual-level Noisy Correspondence (DNC).
DNC refers to misalignments in both intra-entity (entity-attribute) and
inter-graph (entity-entity and attribute-attribute) correspondences. To address
the DNC problem, we propose a robust MMEA framework termed RULE. RULE first
estimates the reliability of both intra-entity and inter-graph correspondences
via a dedicated two-fold principle. Leveraging the estimated reliabilities,
RULE mitigates the negative impact of intra-entity noise during attribute
fusion and prevents overfitting to noisy inter-graph correspondences during
inter-graph discrepancy elimination. Beyond the training-time designs, RULE
further incorporates a correspondence reasoning module that uncovers the
underlying attribute-attribute connection across graphs, guaranteeing more
accurate equivalent entity identification. Extensive experiments on five
benchmarks verify the effectiveness of our method against the DNC compared with
seven state-of-the-art methods.The code is available at
\href{https://github.com/XLearning-SCU/RULE}{XLearning-SCU/RULE}

</details>


### [37] [Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs](https://arxiv.org/abs/2510.18245)
*Song Bian,Tao Yu,Shivaram Venkataraman,Youngsuk Park*

Main category: cs.LG

TL;DR: 提出条件缩放定律与架构搜索框架，在同等训练预算下实现推理效率与准确性的双提升；实验证明优化后的架构在准确性和推理吞吐方面优于现有开源基线。


<details>
  <summary>Details</summary>
Motivation: 随着模型参数规模和训练数据量的增加，推理成本成为一个关键瓶颈，但模型在准确性与推理效率之间的权衡尚未被充分研究。需考察架构因素（隐藏维度、MLP与注意力的参数分配比、分组查询注意力等）对两者的影响，以发现高效且准确的架构。

Method: 提出在Chinchilla框架上加入架构信息的条件缩放定律，并构建一个用于同时追求推理效率与准确性的架构搜索框架。训练超过200个模型，覆盖80M–3B参数与8B–100B训练令牌，并对条件缩放定律进行拟合，分析隐藏维度、mlp-to-attention比、GQA等因素对成本与准确性的影响。

Result: 条件缩放定律可靠地预测最优架构；所得到的模型在公开基线中表现更优。在相同训练预算下，优化后的架构相比LLaMA-3.2在准确性上提升最多约2.1%，并在推理吞吐量上提升约42%。

Conclusion: 将架构信息融入缩放规律可设计出在给定预算下更高效且更准确的模型，所提出的方法在不同规模的数据与参数设置下具备泛化性，能为高效大模型设计提供实用工具。

Abstract: Scaling the number of parameters and the size of training data has proven to
be an effective strategy for improving large language model (LLM) performance.
Yet, as these models grow increasingly powerful and widely deployed, the cost
of inference has become a pressing concern. Despite its importance, the
trade-off between model accuracy and inference efficiency remains
underexplored. In this work, we examine how key architectural factors, hidden
size, the allocation of parameters between MLP and attention (mlp-to-attention
ratio), and grouped-query attention (GQA), influence both inference cost and
accuracy. We introduce a conditional scaling law that augments the Chinchilla
framework with architectural information, along with a search framework for
identifying architectures that are simultaneously inference-efficient and
accurate. To validate our approach, we train more than 200 models spanning 80M
to 3B parameters and 8B to 100B training tokens, and fit the proposed
conditional scaling law. Our results show that the conditional scaling law
reliably predicts optimal architectural choices and that the resulting models
outperform existing open-source baselines. Under the same training budget,
optimized architectures achieve up to 2.1% higher accuracy and 42% greater
inference throughput compared to LLaMA-3.2.

</details>


### [38] [NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective](https://arxiv.org/abs/2510.18258)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Junchi Yan*

Main category: cs.LG

TL;DR: 提出基于神经切线核(NTK)的多任务学习分析框架NTKMTL，并扩展NTK矩阵以在训练中平衡多任务的收敛速度；在共享表示的基础上提出NTKMTL-SR以提高训练效率。实验在监督学习和强化学习的多任务基准上达到竞争性甚至SOTA表现，且给出开源实现。


<details>
  <summary>Details</summary>
Motivation: 解决多任务学习中的任务不平衡与多任务训练动态难以准确刻画的问题。NTK理论为分析与设计提供了可解释的训练动力学框架，能够量化不同任务的收敛速度并据此进行平衡。

Method: 在多任务设置中扩展NTK矩阵，结合谱分析对各任务的收敛速度进行平衡；提出NTKMTL及其基于共享表示的变体NTKMTL-SR，以提升训练效率同时保持性能。

Result: 在广泛的基准上实现了SOTA/接近SOTA的性能，覆盖多任务监督学习与多任务强化学习；并提供源码。

Conclusion: NTK视角为多任务学习的任务平衡提供了系统化且可解释的优化方向，NTKMTL及NTKMTL-SR在理论与实践层面均具潜力，适用范围广泛。

Abstract: Multi-Task Learning (MTL) enables a single model to learn multiple tasks
simultaneously, leveraging knowledge transfer among tasks for enhanced
generalization, and has been widely applied across various domains. However,
task imbalance remains a major challenge in MTL. Although balancing the
convergence speeds of different tasks is an effective approach to address this
issue, it is highly challenging to accurately characterize the training
dynamics and convergence speeds of multiple tasks within the complex MTL
system. To this end, we attempt to analyze the training dynamics in MTL by
leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method,
NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt
spectral analysis to balance the convergence speeds of multiple tasks, thereby
mitigating task imbalance. Based on the approximation via shared
representation, we further propose NTKMTL-SR, achieving training efficiency
while maintaining competitive performance. Extensive experiments demonstrate
that our methods achieve state-of-the-art performance across a wide range of
benchmarks, including both multi-task supervised learning and multi-task
reinforcement learning. Source code is available at
https://github.com/jianke0604/NTKMTL.

</details>


### [39] [From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation](https://arxiv.org/abs/2510.18263)
*Ziwei Huang,Ying Shu,Hao Fang,Quanyu Long,Wenya Wang,Qiushi Guo,Tiezheng Ge,Leilei Gan*

Main category: cs.LG

TL;DR: Customized-GRPO 在身份保留與文本提示遵循性的任務中提出兩大創新機制：SARS 與 TDW，有效緩解 naive GRPO 的梯度矛盾與時間動力學不匹配，提升生成影像的身份一致性與提示遵循性。


<details>
  <summary>Details</summary>
Motivation: 現有以在線增強學習調整圖像生成模型的方法，在簡單線性加總獎勵時易出現梯度冲突，且與 diffusion 過程的時間動力學不匹配，導致身份保留與 prompt 適應之間的折衷難以優化。

Method: 提出 Customized-GRPO，包含 Synergy-Aware Reward Shaping (SARS) 和 Time-Aware Dynamic Weighting (TDW)，分別透過非線性獎勵整形與時間敏感的權重分配，增強協同信號並對不同時間階段的目標賦予不同優先權。

Result: 實驗顯示相比於 naive GRPO，該框架顯著降低競爭性退化，能更好地平衡身份保留與複雜文本提示的遵循性。

Conclusion: Customized-GRPO 能在身份保留與提示遵循性之間取得更好的折衷，提升生成影像的整體表現。

Abstract: Subject-driven image generation models face a fundamental trade-off between
identity preservation (fidelity) and prompt adherence (editability). While
online reinforcement learning (RL), specifically GPRO, offers a promising
solution, we find that a naive application of GRPO leads to competitive
degradation, as the simple linear aggregation of rewards with static weights
causes conflicting gradient signals and a misalignment with the temporal
dynamics of the diffusion process. To overcome these limitations, we propose
Customized-GRPO, a novel framework featuring two key innovations: (i)
Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly
penalizes conflicted reward signals and amplifies synergistic ones, providing a
sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW),
which aligns the optimization pressure with the model's temporal dynamics by
prioritizing prompt-following in the early, identity preservation in the later.
Extensive experiments demonstrate that our method significantly outperforms
naive GRPO baselines, successfully mitigating competitive degradation. Our
model achieves a superior balance, generating images that both preserve key
identity features and accurately adhere to complex textual prompts.

</details>


### [40] [Online Time Series Forecasting with Theoretical Guarantees](https://arxiv.org/abs/2510.18281)
*Zijian Li,Changze Zhou,Minghao Fu,Sanjay Manjunath,Fan Feng,Guangyi Chen,Yingyao Hu,Ruichu Cai,Kun Zhang*

Main category: cs.LG

TL;DR: 在在线时间序列预测中，提出了一个带潜在变量的理论框架TOT，证明在分布漂移情形下潜在变量能收紧贝叶斯风险且受潜在变量可辨识性增强而带来更大收益；并给出一个与模型无关的蓝图，将时间解码器和两个独立噪声估计器用于捕捉潜在变量的因果推断及观测变量的混合过程。通过合成数据的实验与多基线的插件实现，验证理论与实用性。


<details>
  <summary>Details</summary>
Motivation: 在时间序列的在线预测中，随着时间的推移出现未知分布漂移，历史到未来的映射受潜在变量影响；需要一个具有理论保障且自动化的预测框架来应对这种漂移。

Method: 提出在线时间序列预测的理论框架TOT，证明提供潜在变量的预测器能收紧贝叶斯风险，且在潜在变量估计的不确定性下收益仍然成立；指出潜在变量越易识别，收益越大。为引入潜在变量，提出以最小相邻观测实现潜在变量辨识的方法。基于此结果，给出一个模型无关的蓝图：使用时序解码器匹配观测变量分布，并用两个独立的噪声估计器分别建模潜在变量的因果推断和观测变量的混合过程。

Result: 理论上证明了潜在变量对贝叶斯风险的改进及对估计不确定性的鲁棒性，在潜在变量达到更高可识别性时收益增强；在合成数据上验证了理论结论，并在若干基线之上实现的插件实现显示出在多个基准上的改进。

Conclusion: 潜在变量在面对分布漂移的在线时间序列预测中具有重要作用，且可辨识性越高收益越显著；所提出的藍图具有模型无关性和实用性，理论与实验结果共同支撑其在真实场景中的有效性。

Abstract: This paper is concerned with online time series forecasting, where unknown
distribution shifts occur over time, i.e., latent variables influence the
mapping from historical to future observations. To develop an automated way of
online time series forecasting, we propose a Theoretical framework for Online
Time-series forecasting (TOT in short) with theoretical guarantees.
Specifically, we prove that supplying a forecaster with latent variables
tightens the Bayes risk, the benefit endures under estimation uncertainty of
latent variables and grows as the latent variables achieve a more precise
identifiability. To better introduce latent variables into online forecasting
algorithms, we further propose to identify latent variables with minimal
adjacent observations. Based on these results, we devise a model-agnostic
blueprint by employing a temporal decoder to match the distribution of observed
variables and two independent noise estimators to model the causal inference of
latent variables and mixing procedures of observed variables, respectively.
Experiment results on synthetic data support our theoretical claims. Moreover,
plug-in implementations built on several baselines yield general improvement
across multiple benchmarks, highlighting the effectiveness in real-world
applications.

</details>


### [41] [Towards Identifiability of Hierarchical Temporal Causal Representation Learning](https://arxiv.org/abs/2510.18310)
*Zijian Li,Minghao Fu,Junxian Huang,Yifan Shen,Ruichu Cai,Yuewen Sun,Guangyi Chen,Kun Zhang*

Main category: cs.LG

TL;DR: CHiLD 框架能够从三个条件独立观测中唯一识别多层潜在变量的联合分布，并据此分层识别与建模分层时序动态；结合变分推断、上下文编码器与归一化流先验实现。


<details>
  <summary>Details</summary>
Motivation: 现实时间序列具有跨多层抽象的动态依赖性，但现有的 temporal causal representation learning 方法往往只能从单步观测中恢复潜变量的边际分布，难以捕捉层级潜变量的联合结构与因果关系，因此需要一个能从上下文观测中确定多层隐变量联合分布的框架。

Method: 提出 CHiLD 标识框架：1) 以时序上下文观测变量识别多层隐变量的联合分布；2) 利用层级结构的稀疏性在各层内部逐步识别潜变量；3) 构建基于变分推断的时序生成模型，包含一个上下文编码器以重建多层潜变量，以及归一化流的层级先验以强化独立噪声条件。

Result: 在合成与真实数据集上的实验验证了理论结论，显示 CHiLD 能有效地建模并揭示分层潜在动态的时序结构。

Conclusion: CHiLD 提供了在具备上下文信息的条件下从观测数据中识别并建模分层时序潜在变量的理论与方法框架，对提升时间序列的因果表示学习具有潜在影响。

Abstract: Modeling hierarchical latent dynamics behind time series data is critical for
capturing temporal dependencies across multiple levels of abstraction in
real-world tasks. However, existing temporal causal representation learning
methods fail to capture such dynamics, as they fail to recover the joint
distribution of hierarchical latent variables from \textit{single-timestep
observed variables}. Interestingly, we find that the joint distribution of
hierarchical latent variables can be uniquely determined using three
conditionally independent observations. Building on this insight, we propose a
Causally Hierarchical Latent Dynamic (CHiLD) identification framework. Our
approach first employs temporal contextual observed variables to identify the
joint distribution of multi-layer latent variables. Sequentially, we exploit
the natural sparsity of the hierarchical structure among latent variables to
identify latent variables within each layer. Guided by the theoretical results,
we develop a time series generative model grounded in variational inference.
This model incorporates a contextual encoder to reconstruct multi-layer latent
variables and normalize flow-based hierarchical prior networks to impose the
independent noise condition of hierarchical latent dynamics. Empirical
evaluations on both synthetic and real-world datasets validate our theoretical
claims and demonstrate the effectiveness of CHiLD in modeling hierarchical
latent dynamics.

</details>


### [42] [Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task](https://arxiv.org/abs/2510.18315)
*Brady Bhalla,Honglu Fan,Nancy Chen,Tony Yue YU*

Main category: cs.LG

TL;DR: 嵌入维度越大，transformer在强化学习训练的“气泡排序”型任务中显式内部世界模型的表示越清晰、越可解释；注意力矩阵的最后一行逐步编码全局排序，选择的置换与相邻元素差的最大值一致。


<details>
  <summary>Details</summary>
Motivation: 探究嵌入维度对算法任务中内部世界模型形成的影响，揭示端到端性能背后的表征质量与可解释性之间的关系。

Method: 对一个在强化学习下用于执行相邻交换的 bubble-sort 风格任务的 transformer 进行训练，系统性变换嵌入维度并进行 hundreds 次实验；分析注意力权重矩阵、推断出的置换机制，评估内部表示的保真性、稳定性与可解释性，并公开相关指标。

Result: 即使在较小的嵌入维度下也能达到高准确度；但较大维度能获得更忠实、连贯、鲁棒的内部表示，形成结构化的内部世界模型；两种普遍机制被观察到：1) 注意力矩阵的最后一行单调编码全局令牌排序；2) 选取的置换与相邻差的最大值对齐；模型规模提升不仅提升端到端性能，也提升表示质量；研究方发布了可用于探测类似算法任务的度量指标。

Conclusion: 证据表明 transformer 能构建结构化的内部世界模型，模型规模对表示质量有显著影响，除性能外也提升了表征的可解释性与稳健性。

Abstract: We investigate how embedding dimension affects the emergence of an internal
"world model" in a transformer trained with reinforcement learning to perform
bubble-sort-style adjacent swaps. Models achieve high accuracy even with very
small embedding dimensions, but larger dimensions yield more faithful,
consistent, and robust internal representations. In particular, higher
embedding dimensions strengthen the formation of structured internal
representation and lead to better interpretability. After hundreds of
experiments, we observe two consistent mechanisms: (1) the last row of the
attention weight matrix monotonically encodes the global ordering of tokens;
and (2) the selected transposition aligns with the largest adjacent difference
of these encoded values. Our results provide quantitative evidence that
transformers build structured internal world models and that model size
improves representation quality in addition to end performance. We release our
metrics and analyses, which can be used to probe similar algorithmic tasks.

</details>


### [43] [Uncertainty Estimation by Flexible Evidential Deep Learning](https://arxiv.org/abs/2510.18322)
*Taeseong Yoon,Heeyoung Kim*

Main category: cs.LG

TL;DR: 提出了柔性 evidential 深度学习（F-EDL），通过预测一个灵活的 Dirichlet 分布来描述类别概率的不确定性，克服了传统 EDL 的局限性，提高在经典、长尾和噪声分布下的 UQ 泛化与鲁棒性，并在理论与实验中显示了优越性。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景中，量化不确定性至关重要；EDL 用 Dirichlet 分布建模类别概率以提高效率，但对简单的 Dirichlet 假设的限制削弱了对复杂或未见情景的鲁棒性。

Method: 提出 F-EDL，通过预测一个灵活的 Dirichlet 分布（Dirichlet 的广义化）来对类别概率进行建模，提供更表达力的不确定性表示。理论分析表明其优点，实验在多种评估设置下达到或超越现有的 UQ 性能。

Result: 在多种评估场景（经典、长尾、噪声的在分布数据）中实现了 state-of-the-art 的不确定性量化性能，并给出理论上的优势。

Conclusion: F-EDL 提升了 UQ 的泛化性和可靠性，兼具效率，在复杂情境下对未知样本具有更稳健的鲁棒性。

Abstract: Uncertainty quantification (UQ) is crucial for deploying machine learning
models in high-stakes applications, where overconfident predictions can lead to
serious consequences. An effective UQ method must balance computational
efficiency with the ability to generalize across diverse scenarios. Evidential
deep learning (EDL) achieves efficiency by modeling uncertainty through the
prediction of a Dirichlet distribution over class probabilities. However, the
restrictive assumption of Dirichlet-distributed class probabilities limits
EDL's robustness, particularly in complex or unforeseen situations. To address
this, we propose \textit{flexible evidential deep learning}
($\mathcal{F}$-EDL), which extends EDL by predicting a flexible Dirichlet
distribution -- a generalization of the Dirichlet distribution -- over class
probabilities. This approach provides a more expressive and adaptive
representation of uncertainty, significantly enhancing UQ generalization and
reliability under challenging scenarios. We theoretically establish several
advantages of $\mathcal{F}$-EDL and empirically demonstrate its
state-of-the-art UQ performance across diverse evaluation settings, including
classical, long-tailed, and noisy in-distribution scenarios.

</details>


### [44] [Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching](https://arxiv.org/abs/2510.18328)
*Zhong Li,Qi Huang,Yuxuan Zhu,Lincen Yang,Mohammad Mohammadi Amiri,Niki van Stein,Matthijs van Leeuwen*

Main category: cs.LG

TL;DR: 提出 Time-Conditioned Contraction Matching（TCCM），用于表格数据的半监督异常检测，具备轻量训练、单步分值评估、可解释性，并在 ADBench 上超越现有方法，代码在 GitHub。


<details>
  <summary>Details</summary>
Motivation: 解决高维表格数据的高效、可解释且鲁棒的异常检测需求，同时降低推理成本并克服基于扩散/流量匹配方法的复杂求解约束。

Method: 在原理上沿用流式匹配的核心思想：学习在不同时间点将分布收缩指向原点的时间条件收缩向量；但通过不再要求对常微分方程进行求解，而是在每个采样时间步预测收缩向量；引入“一步偏差”评分策略以单次前向传播实现异常分数；速度场在输入空间直接操作，从而实现可解释性和对输入的鲁棒性（Lipschitz 连续性）等属性。

Result: 在 ADBench 基准上，TCCM 在检测准确率和推理成本之间达到良好平衡，尤其在高维和大规模数据集上超越了最先进方法。

Conclusion: TCCM 提供高效、可扩展且可解释的半监督异常检测解决方案，并在理论上具备对微小扰动的鲁棒性保证；源代码可在 GitHub 获取。

Abstract: We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for
semi-supervised anomaly detection in tabular data. TCCM is inspired by flow
matching, a recent generative modeling framework that learns velocity fields
between probability distributions and has shown strong performance compared to
diffusion models and generative adversarial networks. Instead of directly
applying flow matching as originally formulated, TCCM builds on its core idea
-- learning velocity fields between distributions -- but simplifies the
framework by predicting a time-conditioned contraction vector toward a fixed
target (the origin) at each sampled time step. This design offers three key
advantages: (1) a lightweight and scalable training objective that removes the
need for solving ordinary differential equations during training and inference;
(2) an efficient scoring strategy called one time-step deviation, which
quantifies deviation from expected contraction behavior in a single forward
pass, addressing the inference bottleneck of existing continuous-time models
such as DTE (a diffusion-based model with leading anomaly detection accuracy
but heavy inference cost); and (3) explainability and provable robustness, as
the learned velocity field operates directly in input space, making the anomaly
score inherently feature-wise attributable; moreover, the score function is
Lipschitz-continuous with respect to the input, providing theoretical
guarantees under small perturbations. Extensive experiments on the ADBench
benchmark show that TCCM strikes a favorable balance between detection accuracy
and inference cost, outperforming state-of-the-art methods -- especially on
high-dimensional and large-scale datasets. The source code is available at our
GitHub repository.

</details>


### [45] [Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs](https://arxiv.org/abs/2510.18340)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 在 γ=1 的无折扣无限时域 MDP 中分析策略梯度方法，提出通过将状态分为再现态与瞬时态的不可变性，以及引入瞬时访问量（transient visitation measure）来取代传统状态访问量，从而建立理论分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有对策略梯度的严格分析多基于 γ<1 的折扣设定；在大语言模型等场景中常用 γ=1 的无折扣总回报，需要新的理论工具来分析收敛性与梯度性质。

Method: 给出两条关键洞见：i) 对所有在策略中对每个动作赋予正概率的策略集合，状态的再现/瞬时分类保持不变；ii) 用瞬时访问量取代可能在 γ=1 下不可定义的传统状态访问量，构造相应的梯度分析框架。

Result: 基于上述两点，提出了一个可分析 γ=1 的无折扣策略梯度的方法论与工具集，定义了新的访问度量并给出相关的理论分析（如收敛性/梯度稳定性等），从而填补无折扣情形下的理论空白。

Conclusion: 为 γ=1 的无折扣强化学习，尤其是深度模型与软最大输出策略的场景，提供了理论支撑与分析框架，建立了在无折扣条件下研究策略梯度的基础。

Abstract: The classical policy gradient method is the theoretical and conceptual
foundation of modern policy-based reinforcement learning (RL) algorithms. Most
rigorous analyses of such methods, particularly those establishing convergence
guarantees, assume a discount factor $\gamma < 1$. In contrast, however, a
recent line of work on policy-based RL for large language models uses the
undiscounted total-reward setting with $\gamma = 1$, rendering much of the
existing theory inapplicable. In this paper, we provide analyses of the policy
gradient method for undiscounted expected total-reward infinite-horizon MDPs
based on two key insights: (i) the classification of the MDP states into
recurrent and transient states is invariant over the set of policies that
assign strictly positive probability to every action (as is typical in deep RL
models employing a softmax output layer) and (ii) the classical state
visitation measure (which may be ill-defined when $\gamma = 1$) can be replaced
with a new object that we call the transient visitation measure.

</details>


### [46] [Computable universal online learning](https://arxiv.org/abs/2510.18352)
*Dariusz Kalociński,Tomasz Steifer*

Main category: cs.LG

TL;DR: 本文研究在 universal online learning 框架下的可计算性问题，揭示了可实现为计算机程序的学习并不等价于一般的可学习性，并给出对不可知情（agnostic）和适合（proper）变体的精确刻画。


<details>
  <summary>Details</summary>
Motivation: 动机是把理论上的在线学习（universal online learning）与实际可实现性结合起来，回答在对抗性环境中是否存在可以实现为程序的学习策略，以及不同变体的可学习性边界。

Method: 方法上结合理论分析：基于 Bousquet 等人的 STOC'21 的 universal online learning 框架，考察 computability 的要求，给出不可计算性结果；给出 agnostic computable universal online learning 的完备刻画；给出 proper universal online learning 的精确条件。

Result: 结果包括：1) universal online learning 不蕴含 computable universal online learning；2) 给出 agnostic computable universal online learning 的完备刻画；3) 给出 proper universal online learning 的精确条件；从而提供对在线二元分类和归纳推断理论的更现实透视。

Conclusion: 结论是理论上虽然存在学习能力的泛化类，但要实现成可计算程序需要额外条件；研究填补了理论可实现性与在线学习之间的差距。

Abstract: Understanding when learning is possible is a fundamental task in the theory
of machine learning. However, many characterizations known from the literature
deal with abstract learning as a mathematical object and ignore the crucial
question: when can learning be implemented as a computer program? We address
this question for universal online learning, a generalist theoretical model of
online binary classification, recently characterized by Bousquet et al.
(STOC'21). In this model, there is no hypothesis fixed in advance; instead,
Adversary -- playing the role of Nature -- can change their mind as long as
local consistency with the given class of hypotheses is maintained. We require
Learner to achieve a finite number of mistakes while using a strategy that can
be implemented as a computer program. We show that universal online learning
does not imply computable universal online learning, even if the class of
hypotheses is relatively easy from a computability-theoretic perspective. We
then study the agnostic variant of computable universal online learning and
provide an exact characterization of classes that are learnable in this sense.
We also consider a variant of proper universal online learning and show exactly
when it is possible. Together, our results give a more realistic perspective on
the existing theory of online binary classification and the related problem of
inductive inference.

</details>


### [47] [Towards Unsupervised Open-Set Graph Domain Adaptation via Dual Reprogramming](https://arxiv.org/abs/2510.18363)
*Zhen Zhang,Bingsheng He*

Main category: cs.LG

TL;DR: 提出 GraphRTA：一种面向开放集合的无监督图域自适应框架，通过对图和模型进行再编程，解决未知类别的识别并提升已知类别分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦于闭集假设，源/目标域共享标签空间，现实场景中目标域可能包含源域未见的类别，因此需要处理开放集问题。

Method: 提出 GraphRTA：在图端通过对目标图结构和节点特征的再编程来促进已知/未知类别的分离；在模型端通过裁剪域特定参数以减少对源域偏置、保留跨图可迁移的参数；在分类器上增加未知类别的额外维度，消除手动设定阈值的需要。

Result: 在多个公开数据集上对比基线，GraphRTA 显示出与最新方法相近或更优的性能，且公开代码可用。

Conclusion: GraphRTA 有效解决开放集图域自适应问题，提升对未知类别的识别能力并降低对源域的偏置，提供可迁移的图表示学习能力；代码和数据集公开。

Abstract: Unsupervised Graph Domain Adaptation has become a promising paradigm for
transferring knowledge from a fully labeled source graph to an unlabeled target
graph. Existing graph domain adaptation models primarily focus on the
closed-set setting, where the source and target domains share the same label
spaces. However, this assumption might not be practical in the real-world
scenarios, as the target domain might include classes that are not present in
the source domain. In this paper, we investigate the problem of unsupervised
open-set graph domain adaptation, where the goal is to not only correctly
classify target nodes into the known classes, but also recognize previously
unseen node types into the unknown class. Towards this end, we propose a novel
framework called GraphRTA, which conducts reprogramming on both the graph and
model sides. Specifically, we reprogram the graph by modifying target graph
structure and node features, which facilitates better separation of known and
unknown classes. Meanwhile, we also perform model reprogramming by pruning
domain-specific parameters to reduce bias towards the source graph while
preserving parameters that capture transferable patterns across graphs.
Additionally, we extend the classifier with an extra dimension for the unknown
class, thus eliminating the need of manually specified threshold in open-set
recognition. Comprehensive experiments on several public datasets demonstrate
that our proposed model can achieve satisfied performance compared with recent
state-of-the-art baselines. Our source codes and datasets are publicly
available at https://github.com/cszhangzhen/GraphRTA.

</details>


### [48] [Training Diverse Graph Experts for Ensembles: A Systematic Empirical Study](https://arxiv.org/abs/2510.18370)
*Gangda Deng,Yuxin Yang,Ömer Faruk Akgül,Hanqing Zeng,Yinglong Xia,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 本文系统评估GNN集合中专家多样化对性能的影响，比较20种多样化策略在14个节点分类基准上的表现，构建并分析超过200个集成变体，提供关于专家训练和MoE框架设计的可操作指南。


<details>
  <summary>Details</summary>
Motivation: 现实图数据的异质性往往限制单一GNN的性能；通过显式多样化专家，可以获得互补性并提升集成效果。

Method: 对20种 diversification 策略（包括随机重初始化、超参数调优、架构变异、方向性建模、训练数据分割等）进行系统实验，评估在14个节点分类基准上的表现，分析专家多样性、互补性与集成性能之间的关系，构建超过200个集成变体。

Result: 揭示不同策略对专家多样性、互补性与集成性能的影响，提出训练尽量多样化专家的机制性洞见，并给出对设计GNN MoE框架的可操作建议；代码公开。

Conclusion: 系统化的专家多样化可显著提升Graph DNN的集成性能，为GNN MoE的训练与设计提供实践性指南与方向。

Abstract: Graph Neural Networks (GNNs) have become essential tools for learning on
relational data, yet the performance of a single GNN is often limited by the
heterogeneity present in real-world graphs. Recent advances in
Mixture-of-Experts (MoE) frameworks demonstrate that assembling multiple,
explicitly diverse GNNs with distinct generalization patterns can significantly
improve performance. In this work, we present the first systematic empirical
study of expert-level diversification techniques for GNN ensembles. Evaluating
20 diversification strategies -- including random re-initialization,
hyperparameter tuning, architectural variation, directionality modeling, and
training data partitioning -- across 14 node classification benchmarks, we
construct and analyze over 200 ensemble variants. Our comprehensive evaluation
examines each technique in terms of expert diversity, complementarity, and
ensemble performance. We also uncovers mechanistic insights into training
maximally diverse experts. These findings provide actionable guidance for
expert training and the design of effective MoE frameworks on graph data. Our
code is available at https://github.com/Hydrapse/bench-gnn-diversification.

</details>


### [49] [Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees](https://arxiv.org/abs/2510.18406)
*Miao Zhang,Junpeng Li,ChangChun HUa,Yana Yang*

Main category: cs.LG

TL;DR: 提出 NTMP 设定下的无偏风险估计器（URE）及其变体，理论分析与实验验证，显示计数型弱监督可有效学习


<details>
  <summary>Details</summary>
Motivation: 在每个样本只有 m 个正实例且只观测到计数的场景中，如何从计数中恢复有用的实例级信息并实现无偏学习？

Method: 通过将 tuple 生成过程与潜在实例边际联系，推导固定 n,m 时的闭式 URE，扩展到可变 tuple 大小与可变正计数，提出 ReLU 修正以提升稳健性

Result: 给出识别条件、Rademacher 泛化界与一致性率，实证在多个基准任务的 NTMP 转换上优于对比弱监督方法，并对类别不平衡和不同元组配置鲁棒

Conclusion: 计数型弱监督可通过理论化的 URE 架构实现有效学习，提供稳定且实用的目标函数和性能提升

Abstract: Weakly supervised learning often operates with coarse aggregate signals
rather than instance labels. We study a setting where each training example is
an $n$-tuple containing exactly m positives, while only the count m per tuple
is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g.,
image classification with region proposals and multi-instance measurements. We
show that tuple counts admit a trainable unbiased risk estimator (URE) by
linking the tuple-generation process to latent instance marginals. Starting
from fixed (n,m), we derive a closed-form URE and extend it to variable tuple
sizes, variable counts, and their combination. Identification holds whenever
the effective mixing rate is separated from the class prior. We establish
generalization bounds via Rademacher complexity and prove statistical
consistency with standard rates under mild regularity assumptions. To improve
finite-sample stability, we introduce simple ReLU corrections to the URE that
preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the
approach consistently outperforms representative weak-supervision baselines and
yields favorable precision-recall and F1 trade-offs. It remains robust under
class-prior imbalance and across diverse tuple configurations, demonstrating
that count-only supervision can be exploited effectively through a
theoretically grounded and practically stable objective.

</details>


### [50] [Provable Generalization Bounds for Deep Neural Networks with Adaptive Regularization](https://arxiv.org/abs/2510.18410)
*Adeel Safder*

Main category: cs.LG

TL;DR: 提出 MAGDrop，一种基于梯度和动量自适应的 dropout 正则化，通过对激活的 dropout 率进行动态调整来提升非凸优化中的稳定性，并给出基于 PAC-Bayes 的收紧界，实证在 MNIST/CIFAR-10 上优于基线1-2%。


<details>
  <summary>Details</summary>
Motivation: 解决 DNN 过拟合问题，暴露高容量导致的泛化不足，寻求具有自适应性和理论可解释性的正则化方法，结合动量驱动的扰动控制提升泛化界定义。

Method: 提出 MAGDrop：基于激活的 dropout，动态调整 dropout 率，依据当前梯度和累计动量；给出收紧的 PAC-Bayes 泛化界，考虑自适应扰动；在 MNIST/CIFAR-10 上进行实证评估，比较标准 dropout 与自适应梯度正则化等方法。

Result: 在测试准确率上，相较基线正则化， MAGDrop 提升 1-2%，MNIST 达到 99.52%，CIFAR-10 达到 90.63%，泛化间距分别为 0.48% 与 7.14%，显示出更好的泛化性能。

Conclusion: 理论与实践结合，提供一种适用于高风险应用的稳健泛化框架，提升 DNN 泛化能力。

Abstract: Deep neural networks (DNNs) achieve remarkable performance but often suffer
from overfitting due to their high capacity. We introduce Momentum-Adaptive
Gradient Dropout (MAGDrop), a novel regularization method that dynamically
adjusts dropout rates on activations based on current gradients and accumulated
momentum, enhancing stability in non-convex optimization landscapes. To
theoretically justify MAGDrop's effectiveness, we derive a tightened PAC-Bayes
generalization bound that accounts for its adaptive nature, achieving up to 20%
sharper bounds compared to standard approaches by leveraging momentum-driven
perturbation control. Empirically, the activation-based MAGDrop outperforms
baseline regularization techniques, including standard dropout and adaptive
gradient regularization, by 1-2% in test accuracy on MNIST (99.52%) and
CIFAR-10 (90.63%), with generalization gaps of 0.48% and 7.14%, respectively.
Our work bridges theoretical insights and practical advancements, offering a
robust framework for enhancing DNN generalization suitable for high-stakes
applications.

</details>


### [51] [Learning Boltzmann Generators via Constrained Mass Transport](https://arxiv.org/abs/2510.18460)
*Christopher von Klitzing,Denis Blessing,Henrik Schopmans,Pascal Friederich,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出 Constrained Mass Transport (CMT) 的变分框架，通过对相邻步骤的KL散度和熵衰减设置约束，以提高中间分布的重叠性与稳定性，从而解决高维多模分布下的采样难题。


<details>
  <summary>Details</summary>
Motivation: 在高维、非归一化且多模态的目标分布下实现有效采样一直是难点；传统变分方法易陷入模式崩溃，退火/几何调度易导致质量传输问题且对调度敏感。

Method: 提出 Constrained Mass Transport，在生成中间分布的过程中对 KL 散度和熵衰减进行约束，确保分布重叠并抑制质量传输（mass teleport），通过在标准 BG 基准和新引入的 ELIL 四肽上验证其性能。

Result: 在多项基准测试及引入的 ELIL tetrapeptide 上，CMT 超越现有最先进的变分方法，实现超过 2.5 倍的有效样本量提升，并避免模式塌缩，且在未使用分子动力学样本的情况下研究到的最大系统规模。

Conclusion: CMT 提供一种稳定且高效的变分框架，通过对相邻步骤的 KL 与熵衰减施加约束，显著提高分布重叠性并缓解质量传输问题，具有对高维复杂系统的广泛适用性与潜在扩展空间。

Abstract: Efficient sampling from high-dimensional and multimodal unnormalized
probability distributions is a central challenge in many areas of science and
machine learning. We focus on Boltzmann generators (BGs) that aim to sample the
Boltzmann distribution of physical systems, such as molecules, at a given
temperature. Classical variational approaches that minimize the reverse
Kullback-Leibler divergence are prone to mode collapse, while annealing-based
methods, commonly using geometric schedules, can suffer from mass teleportation
and rely heavily on schedule tuning. We introduce Constrained Mass Transport
(CMT), a variational framework that generates intermediate distributions under
constraints on both the KL divergence and the entropy decay between successive
steps. These constraints enhance distributional overlap, mitigate mass
teleportation, and counteract premature convergence. Across standard BG
benchmarks and the here introduced ELIL tetrapeptide, the largest system
studied to date without access to samples from molecular dynamics, CMT
consistently surpasses state-of-the-art variational methods, achieving more
than 2.5x higher effective sample size while avoiding mode collapse.

</details>


### [52] [Simple and Efficient Heterogeneous Temporal Graph Neural Network](https://arxiv.org/abs/2510.18467)
*Yili Wang,Tairan Huang,Changlong He,Qiutong Li,Jianliang Gao*

Main category: cs.LG

TL;DR: 提出SE-HTGNN，一体化时序与时空建模的注意力机制；通过动态注意力与历史快照信息实现高效表示；并通过大语言模型提示获取节点类型先验；实验显示在保持最佳预测精度的同时实现最多10x的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的HTGNN多采用时序与时空的解耦学习，导致时空信息交互不足且模型复杂度高。需要一种统一且高效的学习范式，以充分利用时序信息提升HTGNN表示能力。

Method: 提出动态注意力机制，将时序建模融入空间学习；通过保留历史图快照中的注意力信息来引导后续注意力计算；并利用大语言模型对SE-HTGNN进行提示，从而利用节点类型的隐含属性作为先验知识。

Result: 实验表明SE-HTGNN在最新基线下具有最高的预测准确性，并且实现了最高可达10倍的加速。

Conclusion: 所提出的SE-HTGNN有效耦合HTG的时序与时空建模，提升表示能力和计算效率；通过LLM提示获取节点类型先验，提升对HTG的理解和表示学习的效果。

Abstract: Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the
real world. Recently, to enhance representation learning on HTGs, numerous
attention-based neural networks have been proposed. Despite these successes,
existing methods rely on a decoupled temporal and spatial learning paradigm,
which weakens interactions of spatio-temporal information and leads to a high
model complexity. To bridge this gap, we propose a novel learning paradigm for
HTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network
(SE-HTGNN). Specifically, we innovatively integrate temporal modeling into
spatial learning via a novel dynamic attention mechanism, which retains
attention information from historical graph snapshots to guide subsequent
attention computation, thereby improving the overall discriminative
representations learning of HTGs. Additionally, to comprehensively and
adaptively understand HTGs, we leverage large language models to prompt
SE-HTGNN, enabling the model to capture the implicit properties of node types
as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up
to 10x speed-up over the state-of-the-art and latest baseline while maintaining
the best forecasting accuracy.

</details>


### [53] [Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs](https://arxiv.org/abs/2510.18473)
*Yuya Sasaki*

Main category: cs.LG

TL;DR: 对三个大规模知识图谱（YAGO、DBpedia、Wikidata）上的公平性GNN进行系统基准评估，比较预处理与在处理方法在不同GNN骨架和早停条件下的表现，揭示知识图谱在准确性与公平性之间的权衡以及对骨架和早停的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有公平性GNN研究多聚焦于传统图数据，缺乏对知识图谱的系统评估。知识图谱在推荐、问答等应用中规模大、结构复杂，需建立基准并比较不同方法的效果。

Method: 从YAGO、DBpedia、Wikidata构造新的大规模图数据，覆盖不同类型的知识表示；在多种GNN骨架（如GCN、GraphSAGE、GAT等）上对比在预处理与在处理两类公平性方法，以及不同早停策略的表现；评估指标包括预测准确性和多种公平性指标。

Result: 结论包括：（i）知识图谱呈现与以往数据集不同的趋势，更清晰地体现准确性与公平性之间的权衡；（ii）模型表现不仅由公平性GNN方法决定，还明显受GNN骨架和早停条件影响；（iii）预处理方法通常提升公平性指标，而在处理方法提升预测准确性。

Conclusion: 研究强调在知识图谱上的公平性评估需综合考虑数据特性、模型骨架和训练策略；未来工作可进一步探索适合大规模知识图谱的公平性方法与评估框架。

Abstract: Graph neural networks (GNNs) are powerful tools for learning from
graph-structured data but often produce biased predictions with respect to
sensitive attributes. Fairness-aware GNNs have been actively studied for
mitigating biased predictions. However, no prior studies have evaluated
fairness-aware GNNs on knowledge graphs, which are one of the most important
graphs in many applications, such as recommender systems. Therefore, we
introduce a benchmarking study on knowledge graphs. We generate new graphs from
three knowledge graphs, YAGO, DBpedia, and Wikidata, that are significantly
larger than the existing graph datasets used in fairness studies. We benchmark
inprocessing and preprocessing methods in different GNN backbones and early
stopping conditions. We find several key insights: (i) knowledge graphs show
different trends from existing datasets; clearer trade-offs between prediction
accuracy and fairness metrics than other graphs in fairness-aware GNNs, (ii)
the performance is largely affected by not only fairness-aware GNN methods but
also GNN backbones and early stopping conditions, and (iii) preprocessing
methods often improve fairness metrics, while inprocessing methods improve
prediction accuracy.

</details>


### [54] [Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation](https://arxiv.org/abs/2510.18478)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: 提出一种不确定性感知的安全评判器 USC，通过在不确定和高成本区域引入保守性，在安全区域保持梯度敏锐性，从而在RL中同时提高安全性和奖励。实验显示安全违约显著下降且成本梯度误差显著减少。


<details>
  <summary>Details</summary>
Motivation: 现实世界的强化学习部署需要兼顾安全性与任务性能；现有方法要么过度保守、损害性能，要么仅追逐奖励、导致安全约束被违反，且成本景观过于平坦、难以梯度优化；需要在不确定区域引入保守性，同时在安全区域保留清晰梯度。

Method: 提出不确定性安全评判器（USC），在Critic训练中引入不确定性感知的调制与 refinement：利用不确定性估计对成本/值函数进行保守化处理，使保守性集中在不确定和高成本的区域，同时在安全区域保持梯度的锐利性，从而实现有效的奖励-安全权衡。

Result: 实验显示 USC 将安全违规减少约40%，同时保持竞争性或更高的奖励；还将预测成本梯度与真实成本梯度之间的误差降低约83%。

Conclusion: USC 为安全强化学习提供一种可扩展的框架，通过在不确定性区域增强保守性、在安全区域维持梯度，从而打破了安全性与性能之间的权衡，推动更可扩展的安全RL应用。

Abstract: Ensuring the safe exploration of reinforcement learning (RL) agents is
critical for deployment in real-world systems. Yet existing approaches struggle
to strike the right balance: methods that tightly enforce safety often cripple
task performance, while those that prioritize reward leave safety constraints
frequently violated, producing diffuse cost landscapes that flatten gradients
and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a
novel approach that integrates uncertainty-aware modulation and refinement into
critic training. By concentrating conservatism in uncertain and costly regions
while preserving sharp gradients in safe areas, USC enables policies to achieve
effective reward-safety trade-offs. Extensive experiments show that USC reduces
safety violations by approximately 40% while maintaining competitive or higher
rewards, and reduces the error between predicted and true cost gradients by
approximately 83%, breaking the prevailing trade-off between safety and
performance and paving the way for scalable safe RL.

</details>


### [55] [Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning](https://arxiv.org/abs/2510.18485)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: COPPOL在感知到策略学习中引入无分布、有限样本的安全保证，通过共形驱动的语义分割产生标定的危险地图，并据此规划鲁棒的风险感知强化学习策略；在卫星衍生数据集上显著提升危险覆盖率并降低导航风险，同时对分布偏移具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的导航任务中，需要不仅高效的危险感知，还需要对不确定性进行严格的有限样本保障。然而现有方法要么假设完美危险检测，要么对不确定性没有有限样本保证。

Method: 提出COPPOL：将共形预测与感知-策略学习结合，通过分布无关、有限样本的安全保证为语义分割输出标定的危险地图，并给出未检测到的风险的严格边界；再基于这些地图构建面向下游强化学习规划的风险敏感代价场。

Result: 在两个卫星衍生基准数据集上，COPPOL在危险覆盖方面比基线提高最多6倍；实现对不安全区域的近乎完全检测，并在导航中将危险违规显著降低，约50%；且对分布偏移鲁棒，保持安全性和效率。

Conclusion: 通过可校准的危险地图和有限样本保证，COPPOL将感知不确定性有效地传递到规划阶段，提升感知-策略 Pipeline的安全性、鲁棒性与效率。

Abstract: Reliable navigation in safety-critical environments requires both accurate
hazard perception and principled uncertainty handling to strengthen downstream
safety handling. Despite the effectiveness of existing approaches, they assume
perfect hazard detection capabilities, while uncertainty-aware perception
approaches lack finite-sample guarantees. We present COPPOL, a conformal-driven
perception-to-policy learning approach that integrates distribution-free,
finite-sample safety guarantees into semantic segmentation, yielding calibrated
hazard maps with rigorous bounds for missed detections. These maps induce
risk-aware cost fields for downstream RL planning. Across two satellite-derived
benchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative
baselines, achieving near-complete detection of unsafe regions while reducing
hazardous violations during navigation (up to approx 50%). More importantly,
our approach remains robust to distributional shift, preserving both safety and
efficiency.

</details>


### [56] [Alibaba International E-commerce Product Search Competition DILAB Team Technical Report](https://arxiv.org/abs/2510.18499)
*Hyewon Lee,Junghyun Oh,Minkyung Song,Soyoung Park,Seunghoon Han*

Main category: cs.LG

TL;DR: 多语言电商检索框架：DILAB 团队的多阶段管线在最终排行榜获第5名，得分0.8819，展现通过数据净化、轻量预处理与自适应建模实现对跨语言查询理解的稳健性；代码已公开。


<details>
  <summary>Details</summary>
Motivation: 解决多语言环境下的查询-商品理解难题，提升跨语言电商检索在查询-类别（QC）与查询-商品（QI）任务上的稳定性与表现。

Method: 提出一个多阶段管线：数据 refinement（提升数据集的一致性与类别覆盖）、轻量级预处理、语言标注与噪声过滤等输入质量提升步骤。在建模阶段，比较多种模型架构与微调策略，结合经过精心挑选的验证集进行超参数调优，以在 QC 与 QI 任务之间实现平衡并提升在不同语言与领域的鲁棒性。

Result: 在最终排行榜中获得第5名，综合分数为0.8819，显示出对多语言跨域的稳定高性能。数据整理与迭代评估对提升多语言检索系统的有效性具有显著作用，源码可在 GitHub 获取（https://github.com/2noweyh/DILAB-Alibaba-Ecommerce-Search）。

Conclusion: 该研究表明系统性的数据治理与持续的迭代评估对多语言检索系统的鲁棒性和适应性具有关键作用，具备较好的跨语言与跨域推广潜力。

Abstract: This study presents the multilingual e-commerce search system developed by
the DILAB team, which achieved 5th place on the final leaderboard with a
competitive overall score of 0.8819, demonstrating stable and high-performing
results across evaluation metrics. To address challenges in multilingual
query-item understanding, we designed a multi-stage pipeline integrating data
refinement, lightweight preprocessing, and adaptive modeling. The data
refinement stage enhanced dataset consistency and category coverage, while
language tagging and noise filtering improved input quality. In the modeling
phase, multiple architectures and fine-tuning strategies were explored, and
hyperparameters optimized using curated validation sets to balance performance
across query-category (QC) and query-item (QI) tasks. The proposed framework
exhibited robustness and adaptability across languages and domains,
highlighting the effectiveness of systematic data curation and iterative
evaluation for multilingual search systems. The source code is available at
https://github.com/2noweyh/DILAB-Alibaba-Ecommerce-Search.

</details>


### [57] [Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers with Precision and Capacity Constraints](https://arxiv.org/abs/2510.18520)
*Christopher Ratigan,Kyle Heuton,Carissa Wang,Lenore Cowen,Michael C. Hughes*

Main category: cs.LG

TL;DR: 提出在成本与容量约束下的成本感知 ROC 分析框架。通过将满足精度和容量约束的分类器集表示为 ROC 空间中的可行区域，定义部分面积（partial area）、以及对成本参数的平均形成的 ROC 表面部分体积（partial VOROS），用于在医院告警场景下对分类器进行更符合实际部署的排序与比较，并在 MIMIC-IV 数据集上取得优于传统指标的排序效果。


<details>
  <summary>Details</summary>
Motivation: 传统的 ROC/AUC 指标忽略部署场景中的关键因素，如需要确保最小精度以减少误报疲劳、限制预测阳性数量以符合人力资源容量，以及正负成本的不对称性。在医院告警等应用中，这些因素直接影响系统可用性与资源调度，因此需要新的评价框架来反映实际成本与约束。

Method: 1) 将在给定精度约束与容量约束下的分类器集合在 ROC 空间表示为一个可行区域，并研究其几何结构；2) 定义部分面积（partial area），使其与成本具有单调关系且仅覆盖可行区域；3) 对成本参数在一定区间内取值，计算并平均得到 ROC 表面的部分体积（partial VOROS），作为综合性能度量；4) 在 MIMIC‑IV 数据集的生命体征历史死亡风险预测任务中，比较该成本感知度量与现有替代度量在排序分类器方面的效果。

Result: 在死亡风险预测任务中，partial VOROS 能在考虑成本约束的情境下更稳定地排序分類器，相较于传统 AUC/其他指标，能更好地反映实际部署中的代价结构与资源限制对模型表现的影响。对于医院告警应用，该指标对优先级排序的区分度更高，减少了高成本区域的错误分类对系统的干扰。

Conclusion: 该工作提出的成本感知 ROC 框架有效整合了精度约束、容量约束与不对称成本等部署相关因素，为 ROC/AUC 的应用扩展提供了新的方向，尤其适用于需要在资源受限环境下进行告警与监测的场景。该方法在 MIMIC‑IV 上的验证支持其在实际医疗部署中的潜在价值，未来可进一步评估对其他数据集与不同成本结构的鲁棒性。

Abstract: The ROC curve is widely used to assess binary classification performance. Yet
for some applications such as alert systems for hospitalized patient
monitoring, conventional ROC analysis cannot capture crucial factors that
impact deployment, such as enforcing a minimum precision constraint to avoid
false alarm fatigue or imposing an upper bound on the number of predicted
positives to represent the capacity of hospital staff. The usual area under the
curve metric also does not reflect asymmetric costs for false positives and
false negatives. In this paper we address all three of these issues. First, we
show how the subset of classifiers that meet given precision and capacity
constraints can be represented as a feasible region in ROC space. We establish
the geometry of this feasible region. We then define the partial area of lesser
classifiers, a performance metric that is monotonic with cost and only accounts
for the feasible portion of ROC space. Averaging this area over a desired range
of cost parameters results in the partial volume over the ROC surface, or
partial VOROS. In experiments predicting mortality risk using vital sign
history on the MIMIC-IV dataset, we show this cost-aware metric is better than
alternatives for ranking classifiers in hospital alert applications.

</details>


### [58] [RAISE: A Unified Framework for Responsible AI Scoring and Evaluation](https://arxiv.org/abs/2510.18559)
*Loc Phuc Truong Nguyen,Hung Thanh Do*

Main category: cs.LG

TL;DR: RAISE 框架把模型在解释性、公平性、鲁棒性、可持续性四个维度上的表现聚合成一个综合责任分数；三种深度学习模型在这些维度上存在权衡，均无法在所有方面领先。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，单纯的预测准确性不足以保障负责任的AI，需要多维度评估来提升透明度和公正性。

Method: 提出 RAISE 评估框架，量化四个维度并汇聚成一个综合分数；在金融、医疗、社会经济等结构化数据集上对 MLP、Tabular ResNet 和 Feature Tokenizer Transformer 进行评测。

Result: MLP 在可持续性和鲁棒性方面表现强势；Transformer 在可解释性和公平性上表现出色，但环境成本极高；Tabular ResNet 提供较为均衡的性能。

Conclusion: 不存在单一主导模型，需采用多维评估来进行负责任的模型选择；框架已实现并开源，可在 GitHub 访问。

Abstract: As AI systems enter high-stakes domains, evaluation must extend beyond
predictive accuracy to include explainability, fairness, robustness, and
sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a
unified framework that quantifies model performance across these four
dimensions and aggregates them into a single, holistic Responsibility Score. We
evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular
ResNet, and a Feature Tokenizer Transformer, on structured datasets from
finance, healthcare, and socioeconomics. Our findings reveal critical
trade-offs: the MLP demonstrated strong sustainability and robustness, the
Transformer excelled in explainability and fairness at a very high
environmental cost, and the Tabular ResNet offered a balanced profile. These
results underscore that no single model dominates across all responsibility
criteria, highlighting the necessity of multi-dimensional evaluation for
responsible model selection. Our implementation is available at:
https://github.com/raise-framework/raise.

</details>


### [59] [HeFS: Helper-Enhanced Feature Selection via Pareto-Optimized Genetic Search](https://arxiv.org/abs/2510.18575)
*Yusi Fan,Tian Wang,Zhiying Yan,Chang Liu,Qiong Zhou,Qi Lu,Zhehao Guo,Ziqi Deng,Wenyu Zhu,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: HeFS通过在现有特征子集基础上搜索剩余特征空间，构造一个Helper Set以提升分类性能；结合偏置初始化、比例引导的变异、以及基于Pareto的多目标优化，最大化预测准确性和特征互补性，在18个基准数据集上优于SOTA方法，并适用于胃癌分类、药物毒性预测等领域。


<details>
  <summary>Details</summary>
Motivation: 特征选择是NP-hard的组合优化问题，常用的启发式/贪婪策略易早熟收敛，尤其在高维数据中，特征之间往往存在复杂、相互依赖的关系。现有算法难以充分挖掘被忽略但有用的特征，因此需要一个框架在已给出的子集基础上系统性探索剩余特征以提升性能。

Method: 提出HeFS框架，通过偏置初始化和比例引导的遗传算法变异，在原始子集之外搜索剩余特征，形成Helper Set以提高分类效果。将多目标优化设定为Pareto优化，综合最大化预测准确度与特征互补性。实现在18个基准数据集上的实验，多领域如胃癌分类、药物毒性预测、计算机科学应用等。

Result: HeFS能稳定发现被忽略但信息量大的特征，显著提升分类性能，优于现有最前沿方法，在多领域的挑战性任务中表现突出。代码与数据集对外公开。

Conclusion: HeFS作为一种辅助性特征选择框架，能够有效在原始子集基础上挖掘互补特征，缓解传统方法的早熟收敛和对高维特征依赖的不足，具有广泛适用性与可重复性。

Abstract: Feature selection is a combinatorial optimization problem that is NP-hard.
Conventional approaches often employ heuristic or greedy strategies, which are
prone to premature convergence and may fail to capture subtle yet informative
features. This limitation becomes especially critical in high-dimensional
datasets, where complex and interdependent feature relationships prevail. We
introduce the HeFS (Helper-Enhanced Feature Selection) framework to refine
feature subsets produced by existing algorithms. HeFS systematically searches
the residual feature space to identify a Helper Set - features that complement
the original subset and improve classification performance. The approach
employs a biased initialization scheme and a ratio-guided mutation mechanism
within a genetic algorithm, coupled with Pareto-based multi-objective
optimization to jointly maximize predictive accuracy and feature
complementarity. Experiments on 18 benchmark datasets demonstrate that HeFS
consistently identifies overlooked yet informative features and achieves
superior performance over state-of-the-art methods, including in challenging
domains such as gastric cancer classification, drug toxicity prediction, and
computer science applications. The code and datasets are available at
https://healthinformaticslab.org/supp/.

</details>


### [60] [Unrolled-SINDy: A Stable Explicit Method for Non linear PDE Discovery from Sparsely Sampled Data](https://arxiv.org/abs/2510.18611)
*Fayad Ali Banna,Antoine Caradot,Eduardo Brandao,Jean-Philippe Colombier,Rémi Emonet,Marc Sebban*

Main category: cs.LG

TL;DR: Unrolled-SINDy提出一种 unrolling 方案，解耦数值时间步长与可观测数据采样率，从而在数据稀疏时仍能稳定地发现偏微分方程的参数。通过迭代闭式或梯度下降的实现，在经典SINDy和iNeuralSINDy上均能利用不同数值积分（Euler、RK4）获得比传统方法更广的可访问性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界问题中的时间数据往往稀疏，传统SINDy在大局部截断误差下难以稳定地恢复控制方程的参数；需要一种方法使数值步长与采样率解耦，以提高PDE发现的稳定性和可扩展性。

Method: 提出一种unrolling方案，将数值积分的时间步长与可观测数据的采样间隔解耦。该方案可通过一个迭代的闭式更新或梯度下降来实现，且在不同数值积分方案（Euler、RK4）和两类方法（传统SINDy、iNeuralSINDy）中使用。

Result: 实验表明，unrolledSINDy提高了稳定性，使在常规SINDy和iNeuralSINDy下难以由局部截断误差捕获的参数得以恢复；在多种数值方案下均能处理原方法无法覆盖的问题。

Conclusion: Unrolled-SINDy扩展了基于SINDy的PDE发现在稀疏时间数据下的适用性，表明通过unrolling可提升鲁棒性与广义性，可用于更广范围的实际问题。

Abstract: Identifying from observation data the governing differential equations of a
physical dynamics is a key challenge in machine learning. Although approaches
based on SINDy have shown great promise in this area, they still fail to
address a whole class of real world problems where the data is sparsely sampled
in time. In this article, we introduce Unrolled-SINDy, a simple methodology
that leverages an unrolling scheme to improve the stability of explicit methods
for PDE discovery. By decorrelating the numerical time step size from the
sampling rate of the available data, our approach enables the recovery of
equation parameters that would not be the minimizers of the original SINDy
optimization problem due to large local truncation errors. Our method can be
exploited either through an iterative closed-form approach or by a gradient
descent scheme. Experiments show the versatility of our method. On both
traditional SINDy and state-of-the-art noise-robust iNeuralSINDy, with
different numerical schemes (Euler, RK4), our proposed unrolling scheme allows
to tackle problems not accessible to non-unrolled methods.

</details>


### [61] [A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees](https://arxiv.org/abs/2510.18615)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.LG

TL;DR: 将提升树模型蒸馏为可解释的决策树，通过修正(rectification)实现性能与可解释性的折中，并与重新训练的蒸馏方法比较。


<details>
  <summary>Details</summary>
Motivation: 在集成模型（如 boosted trees）通常牺牲可解释性，本文提出通过修正蒸馏来获得同时具备较好预测性能和解释性的模型，填补可解释性与性能的权衡。

Method: 提出一种基于修正的蒸馏流程，将 boosted trees 的知识蒸馏到决策树上，利用 rectification 作为纠错机制实现蒸馏。

Result: 经验结果显示该修正蒸馏在某些场景下与通过重新训练得到的蒸馏方法相比具有竞争力，体现了对模型性能与可解释性的有效折中。

Conclusion: 修正蒸馏提供了一种可行的、兼顾解释性与性能的蒸馏路径，适合在需要可解释性和预测力的场景中使用。

Abstract: We present a new approach for distilling boosted trees into decision trees,
in the objective of generating an ML model offering an acceptable compromise in
terms of predictive performance and interpretability. We explain how the
correction approach called rectification can be used to implement such a
distillation process. We show empirically that this approach provides
interesting results, in comparison with an approach to distillation achieved by
retraining the model.

</details>


### [62] [Hardness of Learning Regular Languages in the Next Symbol Prediction Setting](https://arxiv.org/abs/2510.18634)
*Satwik Bhattamishra,Phil Blunsom,Varun Kanade*

Main category: cs.LG

TL;DR: 在 NSP（Next Symbol Prediction）设定下研究语言的可学习性。尽管 NSP 提供比传统分类更丰富的标签，但对 DFA、布尔公式等概念类的学习在计算上仍然很难，至少在许多情况下不可避免地需要与传统学习问题等价的困难。


<details>
  <summary>Details</summary>
Motivation: 探索 NSP 设定对语言学习的影响以及将其纳入 PAC 学习框架的可行性；在此基础上，寻找对现实语言模型（如额外输出标签的 NSP 情况）是否能提供学习优势，以及 NSP 标签是否会降低学习难度。

Method: 将 NSP 设定正式化为 PAC 学习问题，构造性地给出从常规学习问题的规约到带 NSP 标签的学习问题的规约；证明在多数情况下额外标签几乎无信息量，使问题等价于传统学习问题；在存在密码学假设的前提下，给出学习 DFA 的 NSP 下的困难性证明；同时指出对 NSP 的高效算法可用于学习语言模型的截断支集。

Result: 结果表明：尽管 NSP 提供更丰富的标签，但对 DFA、布尔公式等概念类的学习在 NSP 下仍然计算上困难；在某些设定下，通过密码学假设可将学习 DFA 的问题从常规设定规约到 NSP 下的学习，说明 NSP 并未天然地降低学习难度；另一方面，存在利用 NSP 的高效算法来学习语言模型截断的支集的积极用法。

Conclusion: NSP 标签的额外信息不足以显著降低对某些核心概念类的学习难度，除非依赖强密码学假设；然而 NSP 仍具备用来近似捕获语言模型支集的潜在应用，提示在设计学习算法时应同时权衡标签信息的实用性与理论 hardness。

Abstract: We study the learnability of languages in the Next Symbol Prediction (NSP)
setting, where a learner receives only positive examples from a language
together with, for every prefix, (i) whether the prefix itself is in the
language and (ii) which next symbols can lead to an accepting string. This
setting has been used in prior works to empirically analyze neural sequence
models, and additionally, we observe that efficient algorithms for the NSP
setting can be used to learn the (truncated) support of language models. We
formalize the setting so as to make it amenable to PAC-learning analysis. While
the setting provides a much richer set of labels than the conventional
classification setting, we show that learning concept classes such as DFAs and
Boolean formulas remains computationally hard. The proof is via a construction
that makes almost all additional labels uninformative, yielding a reduction
from the conventional learning problem to learning with NSP labels. Under
cryptographic assumptions, the reduction implies that the problem of learning
DFAs is computationally hard in the NSP setting.

</details>


### [63] [Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions](https://arxiv.org/abs/2510.18638)
*Yanna Ding,Songtao Lu,Yingdong Lu,Tomasz Nowicki,Jianxi Gao*

Main category: cs.LG

TL;DR: 提出对动态驱动函数的自注意力在上下文学习中的理论分析：给出单层线性自注意力的全局最小值的闭式表达、证明重建最优解的参数为NP-hard、并将多层LSA解释为对多目标的预条件梯度下降，同时通过简化的Transformer进行数值验证。


<details>
  <summary>Details</summary>
Motivation: 探究在非线性或动态任务中的ICL机制，超越线性回归的研究，解析带结构约束的马尔可夫函数学习中的优化景观。

Method: 给出一个扩展参数空间下的全局最小值闭式解；证明在一般情况下重现最优解的 Transformer 参数是 NP-hard；将多层LSA解释为对多目标的预条件梯度下降；通过简化的Transformer进行数值验证。

Result: 得到全局最小值的闭式表达；NP-hardness 的证明；对多层LSA的新解释；数值实验支持理论结论。

Conclusion: 指出单层LSA在表示结构化动态函数方面的局限性，并给出多层LSA的新视角，结合理论与数值验证。

Abstract: Transformer architectures can solve unseen tasks based on input-output pairs
in a given prompt due to in-context learning (ICL). Existing theoretical
studies on ICL have mainly focused on linear regression tasks, often with
i.i.d. inputs. To understand how transformers express ICL when modeling
dynamics-driven functions, we investigate Markovian function learning through a
structured ICL setup, where we characterize the loss landscape to reveal
underlying optimization behaviors. Specifically, we (1) provide the closed-form
expression of the global minimizer (in an enlarged parameter space) for a
single-layer linear self-attention (LSA) model; (2) prove that recovering
transformer parameters that realize the optimal solution is NP-hard in general,
revealing a fundamental limitation of one-layer LSA in representing structured
dynamical functions; and (3) supply a novel interpretation of a multilayer LSA
as performing preconditioned gradient descent to optimize multiple objectives
beyond the square loss. These theoretical results are numerically validated
using simplified transformers.

</details>


### [64] [Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction](https://arxiv.org/abs/2510.18648)
*Miro Miranda,Marcela Charfuelan,Matias Valdenegro Toro,Andreas Dengel*

Main category: cs.LG

TL;DR: 将作物产量视为水分稀缺的函数，提出物理信息驱动的学习框架，结合多光谱数据与深度集成不确定性，达到可解释的产量预测（R^2最高约0.82）


<details>
  <summary>Details</summary>
Motivation: 水资源短缺与产量潜力下降是提升粮食安全的关键问题；现有的作物模拟模型具物理可解释性但表现不佳，ML模型虽强大却缺乏物理一致性。需兼具解释性与预测性能的框架。

Method: 将产量建模为水分稀缺的时间序列函数，预测干旱胁迫及水分稀缺的敏感性，采用物理信息损失函数，利用多光谱遥感、气象数据与高分辨率产量数据，并通过深度集成来量化不确定性。

Result: 在对比中优于LSTM/Transformer等模型，R^2可达0.82，且具较高解释性。

Conclusion: 该方法为行业、政策制定者及农民提供决策支持，在应对气候变化条件下提升农业韧性，强调物理一致性与不确定性建模的重要性。

Abstract: Water is essential for agricultural productivity. Assessing water shortages
and reduced yield potential is a critical factor in decision-making for
ensuring agricultural productivity and food security. Crop simulation models,
which align with physical processes, offer intrinsic explainability but often
perform poorly. Conversely, machine learning models for crop yield modeling are
powerful and scalable, yet they commonly operate as black boxes and lack
adherence to the physical principles of crop growth. This study bridges this
gap by coupling the advantages of both worlds. We postulate that the crop yield
is inherently defined by the water availability. Therefore, we formulate crop
yield as a function of temporal water scarcity and predict both the crop
drought stress and the sensitivity to water scarcity at fine-scale resolution.
Sequentially modeling the crop yield response to water enables accurate yield
prediction. To enforce physical consistency, a novel physics-informed loss
function is proposed. We leverage multispectral satellite imagery,
meteorological data, and fine-scale yield data. Further, to account for the
uncertainty within the model, we build upon a deep ensemble approach. Our
method surpasses state-of-the-art models like LSTM and Transformers in crop
yield prediction with a coefficient of determination ($R^2$-score) of up to
0.82 while offering high explainability. This method offers decision support
for industry, policymakers, and farmers in building a more resilient
agriculture in times of changing climate conditions.

</details>


### [65] [Learning Time-Varying Turn-Taking Behavior in Group Conversations](https://arxiv.org/abs/2510.18649)
*Madeline Navarro,Lisa O'Bryan,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出一种基于个体特征与以往发言行为的灵活概率模型，用于预测小组内的轮流发言；能够学习发言倾向随最近一次发言时间的变化，并在合成及真实数据上验证其跨群体泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有对话动力学模型往往只适用于单一群体，缺乏可泛化性；普遍化的统一公式可能不适用于所有群体，因此需要一个数据驱动但理论扎实的通用框架。

Method: 在个体特征（如性格特征）以及历史发言行为的基础上建模发言轮转，扩展以学习最近一次发言后发言倾向的变化；在合成数据与真实对话数据上进行实验验证。

Result: 实验表明提出的方法在合成与真实数据上能有效预测轮转，且揭示了前述以往模型在现实中的局限性，支持方法的跨群体适用性与对发言倾向的时间敏感性。

Conclusion: 给出一个数据驱动且理论基础扎实的通用对话轮转预测框架，能够跨群体泛化并揭示发言倾向如何随时间变化。

Abstract: We propose a flexible probabilistic model for predicting turn-taking patterns
in group conversations based solely on individual characteristics and past
speaking behavior. Many models of conversation dynamics cannot yield insights
that generalize beyond a single group. Moreover, past works often aim to
characterize speaking behavior through a universal formulation that may not be
suitable for all groups. We thus develop a generalization of prior conversation
models that predicts speaking turns among individuals in any group based on
their individual characteristics, that is, personality traits, and prior
speaking behavior. Importantly, our approach provides the novel ability to
learn how speaking inclination varies based on when individuals last spoke. We
apply our model to synthetic and real-world conversation data to verify the
proposed approach and characterize real group interactions. Our results
demonstrate that previous behavioral models may not always be realistic,
motivating our data-driven yet theoretically grounded approach.

</details>


### [66] [Reasoning Language Model Inference Serving Unveiled: An Empirical Study](https://arxiv.org/abs/2510.18672)
*Qi Li,Junpan Wu,Xiang Liu,Yuxin Wang,Zeyu Li,Zhenheng Tang,Yuhan Chen,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: 研究了推理型大语言模型（RLLM）的服务端表现，与传统大模型相比在内存波动、等待请求等方面存在差异，并评估了量化、推测解码等推理优化对RLLM服务效率的影响，结果表明量化和推测解码在可接受的准确度损失下提升性能；前缀缓存和KV缓存量化在小型RLLM上可能适得其反。使用伽马分布的实际 workload 验证了结论。


<details>
  <summary>Details</summary>
Motivation: 尽管RLLM在复杂推理任务上具备竞争力，但其在实际服务场景中的表现、延迟分布和资源开销尚未被系统研究，尚待评估现有推理优化方法在RLLM上的有效性与代价。

Method: 进行初步对比实验，比较RLLM与传统LLM在服务端的表现差异（内存使用及波动、阻塞请求、自适应运行时间、领域偏好等），并测试现有推理优化技术（模型量化、推测解码、前缀缓存、KV缓存量化）对RLLM的影响；在伽马分布的真实工作负载和多数据集上进行评估，验证结论。

Result: 量化与推测解码可在对准确率有小幅折中时提升服务端效率；而前缀缓存与KV缓存量化在小模型RLLM上可能降低准确性或服务性能；真实工作负载的实证结果与仿真一致。

Conclusion: 为研究界和工业界提供关于RLLM推理服务的实用见解，提示在不同模型规模下应选择合适的推理优化策略，并关注RLLM特有的服务行为差异。

Abstract: The reasoning large language model (RLLM) has been proven competitive in
solving complex reasoning tasks such as mathematics, coding, compared to
general LLM. However, the serving performance and behavior of RLLM remains
unexplored, which may undermine the deployment and utilization of RLLM in
real-world scenario. To close this gap, in this paper, we conduct a
comprehensive study of RLLM service. We first perform a pilot study on
comparing the serving performance between RLLM and traditional LLM and reveal
that there are several distinct differences regarding serving behavior: (1)
significant memory usage and fluctuations; (2) straggler requests; (3) adaptive
running time; (4) domain preference. Then we further investigate whether
existing inference optimization techniques are valid for RLLM. Our main
takeaways are that model quantization methods and speculative decoding can
improve service system efficiency with small compromise to RLLM accuracy, while
prefix caching, KV cache quantization may even degrade accuracy or serving
performance for small RLLM. Lastly, we conduct evaluation under real world
workload modeled by Gamma distribution to verify our findings. Empirical
results of real world workload evaluation across different dataset are aligned
with our main findings regarding RLLM serving. We hope our work can provide the
research community and industry with insights to advance RLLM inference
serving.

</details>


### [67] [Learning Task-Agnostic Representations through Multi-Teacher Distillation](https://arxiv.org/abs/2510.18680)
*Philippe Formont,Maxime Darrin,Banafsheh Karimian,Jackie CK Cheung,Eric Granger,Ismail Ben Ayed,Mohammadhadi Shateri,Pablo Piantanida*

Main category: cs.LG

TL;DR: 基于多数投票的任务无关蒸馏框架，利用多教师嵌入的多样性，通过互信息界限得到无任务标签的蒸馏损失，在文本、视觉和分子建模等模态实现通用嵌入提升，并发布了相关嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 解决多教师蒸馏常针对特定任务且依赖标签或先验知识的问题，提出一种可跨任务、跨模态利用教师多样性的无监督/无标签蒸馏框架。

Method: 提出“多数投票”目标，将教师嵌入之间的共识信号作为学生学习的对象；理论上将该目标上界为学生与教师嵌入之间的互信息；实现为任务无关的蒸馏损失，并在文本、视觉及分子建模等多模态数据上进行评估，同时训练并发布了几种状态-最前沿的嵌入模型。

Result: 在多模态评估中，方法有效利用教师多样性，提升下游任务（分类、聚类、回归等）的表现；并且公开了高性能的嵌入模型以支持广泛的下游应用。

Conclusion: 该工作提供了一个真正任务无关的蒸馏框架，通过对教师多样性的系统性利用，获得更具泛化性的嵌入表示，具有广泛的应用潜力与研究价值。

Abstract: Casting complex inputs into tractable representations is a critical step
across various fields. Diverse embedding models emerge from differences in
architectures, loss functions, input modalities and datasets, each capturing
unique aspects of the input. Multi-teacher distillation leverages this
diversity to enrich representations but often remains tailored to specific
tasks. In this paper, we introduce a task-agnostic framework based on a
``majority vote" objective function. We demonstrate that this function is
bounded by the mutual information between student and teachers' embeddings,
leading to a task-agnostic distillation loss that eliminates dependence on
task-specific labels or prior knowledge. Our evaluations across text, vision
models, and molecular modeling show that our method effectively leverages
teacher diversity, resulting in representations enabling better performance for
a wide range of downstream tasks such as classification, clustering, or
regression. Additionally, we train and release state-of-the-art embedding
models, enhancing downstream performance in various modalities.

</details>


### [68] [Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach](https://arxiv.org/abs/2510.18687)
*Chenbei Lu,Zaiwei Chen,Tongxin Li,Chenye Wu,Adam Wierman*

Main category: cs.LG

TL;DR: 提出贝叶斯价值函数、Bellman-Jensen Gap以及BOLA算法，用于分析和实现带预测的多步MDP，在预测误差下仍保持样本效率，并在合成任务与风能储存问题上验证。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用中，能获得多步未来状态预测会带来决策优势，但将它们嵌入MDP会导致状态空间指数膨胀；现有强化学习理论难以处理带误差的多步预测和部分行动覆盖的问题，亟需可分析的框架与高效算法。

Method: 提出贝叶斯价值函数以可分析地描述最优预测感知策略；对贝叶斯价值函数进行Bellman-Jensen Gap分析，量化不完善预测的价值损失；提出BOLA（Bayesian Offline Learning with Online Adaptation）：两阶段的基于模型的RL，将离线贝叶斯价值学习与在线对真实预测的轻量适应分离；给出在预测不完美情况下的样本效率保障。

Result: 建立了理论框架与算法设计；在合成MDP和真实风能储存控制问题上验证理论与算法的有效性。

Conclusion: 为预测增强的强化学习提供可分析的理论工具和鲁棒算法，证明在不完美预测下仍具备样本效率，具备现实应用潜力。

Abstract: Traditional reinforcement learning (RL) assumes the agents make decisions
based on Markov decision processes (MDPs) with one-step transition models. In
many real-world applications, such as energy management and stock investment,
agents can access multi-step predictions of future states, which provide
additional advantages for decision making. However, multi-step predictions are
inherently high-dimensional: naively embedding these predictions into an MDP
leads to an exponential blow-up in state space and the curse of dimensionality.
Moreover, existing RL theory provides few tools to analyze prediction-augmented
MDPs, as it typically works on one-step transition kernels and cannot
accommodate multi-step predictions with errors or partial action-coverage. We
address these challenges with three key innovations: First, we propose the
\emph{Bayesian value function} to characterize the optimal prediction-aware
policy tractably. Second, we develop a novel \emph{Bellman-Jensen Gap} analysis
on the Bayesian value function, which enables characterizing the value of
imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with
Online Adaptation), a two-stage model-based RL algorithm that separates offline
Bayesian value learning from lightweight online adaptation to real-time
predictions. We prove that BOLA remains sample-efficient even under imperfect
predictions. We validate our theory and algorithm on synthetic MDPs and a
real-world wind energy storage control problem.

</details>


### [69] [OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales](https://arxiv.org/abs/2510.18707)
*Tung Nguyen,Tuan Pham,Troy Arcomano,Veerabhadra Kotamarthi,Ian Foster,Sandeep Madireddy,Aditya Grover*

Main category: cs.LG

TL;DR: OmniCast 提出一个统一时空的可概率天气预测模型，结合 VAE 与扩散式 Transformer，通过对未来 token 的随机掩码训练实现跨时间尺度的联合采样，显著降低自回归误差并在中等范围与 subseasonal-to-seasonal 取得领先性，同时速度比传统方法快 10-20 倍，且可进行长达百年的稳定滚动预报。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的中等范围天气预报在更长的 subseasonal-to-seasonal (S2S) 时间尺度上易受自回归误差累积影响，亟需统一的跨时间尺度、可概率的预测框架来提升长期预测的稳定性与准确性。

Method: 该方法由两部分组成：第一部分是将原始天气数据编码到一个连续的低维潜在空间的变分自编码器（VAE）；第二部分是在潜在空间上训练的扩散式 Transformer，用来给定初始条件生成未来潜在 token 的序列。训练时对未来 token 进行随机掩码，Transformer 在条件和可见 token 的基础上，使用逐字/逐 token 的扩散头估计其分布；推断时，Transformer 通过迭代地解掩随机子集 token 来生成完整的未来序列。此联合的跨空间与跨时间的采样有助于缓解自回归方法的误差积累，且低维潜在空间使模型能够学习更长的未来天气动力学。

Result: 在中等范围时间尺度与领先的概率方法相比，OmniCast 表现具有竞争力且速度提升 10–20 倍；在 subseasonal-to-seasonal 规模上，在准确性、物理一致性和概率指标方面达到状态最优，并且能稳定滚动预测长达 100 年。

Conclusion: 该工作证明了一个高效且可扩展的统一时空天气预测框架的可行性，为长跨度预测提供了新的思路，并附带代码与模型检查点以支持复现。

Abstract: Accurate weather forecasting across time scales is critical for anticipating
and mitigating the impacts of climate change. Recent data-driven methods based
on deep learning have achieved significant success in the medium range, but
struggle at longer subseasonal-to-seasonal (S2S) horizons due to error
accumulation in their autoregressive approach. In this work, we propose
OmniCast, a scalable and skillful probabilistic model that unifies weather
forecasting across timescales. OmniCast consists of two components: a VAE model
that encodes raw weather data into a continuous, lower-dimensional latent
space, and a diffusion-based transformer model that generates a sequence of
future latent tokens given the initial conditioning tokens. During training, we
mask random future tokens and train the transformer to estimate their
distribution given conditioning and visible tokens using a per-token diffusion
head. During inference, the transformer generates the full sequence of future
tokens by iteratively unmasking random subsets of tokens. This joint sampling
across space and time mitigates compounding errors from autoregressive
approaches. The low-dimensional latent space enables modeling long sequences of
future latent states, allowing the transformer to learn weather dynamics beyond
initial conditions. OmniCast performs competitively with leading probabilistic
methods at the medium-range timescale while being 10x to 20x faster, and
achieves state-of-the-art performance at the subseasonal-to-seasonal scale
across accuracy, physics-based, and probabilistic metrics. Furthermore, we
demonstrate that OmniCast can generate stable rollouts up to 100 years ahead.
Code and model checkpoints are available at
https://github.com/tung-nd/omnicast.

</details>


### [70] [Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options](https://arxiv.org/abs/2510.18713)
*Joongkyu Lee,Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出 M-AUPO，在 PbRL 中使用 Plackett-Luce 排序反馈对行动子集进行多动作采样；给出上界和下界，理论证明子集规模越大可提升样本效率，且不存在对未知参数范数的指数依赖。


<details>
  <summary>Details</summary>
Motivation: 现有 PbRL 理论多聚焦于对比反馈，使用多轮排序反馈的理论提升有限，特别是反馈长度增多并未带来性能提升。需要建立在子集规模基础上的样本效率分析，填补 PbRL- ranking feedback 的理论空缺。

Method: 采用 Plackett-Luce 模型处理行动子集的排序反馈；提出 M-AUPO 算法，通过最大化 Offered Subset 内的平均不确定性来选择多动作。对该算法给出理论分析，推导上界与下界，体现子集大小对性能的影响。

Result: 上界：suboptimality gap ≤ ˜O( d / T * sqrt( ∑_{t=1}^T 1/|S_t| ) )；下界：Ω( d / (K sqrt(T)) )，其中 K 为最大子集大小。首次给出 PbRL 中带排序反馈的理论结果，明确显示随着子集规模增大，样本效率提升，且避免对未知参数范数的指数依赖。

Conclusion: 理论结果表明通过增大子集规模可以提升样本效率，并给出近似匹配的下界，填补 PbRL 领域的排序反馈理论空白。

Abstract: We study online preference-based reinforcement learning (PbRL) with the goal
of improving sample efficiency. While a growing body of theoretical work has
emerged-motivated by PbRL's recent empirical success, particularly in aligning
large language models (LLMs)-most existing studies focus only on pairwise
comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,
Thekumparampil et al., 2024) have explored using multiple comparisons and
ranking feedback, but their performance guarantees fail to improve-and can even
deteriorate-as the feedback length increases, despite the richer information
available. To address this gap, we adopt the Plackett-Luce (PL) model for
ranking feedback over action subsets and propose M-AUPO, an algorithm that
selects multiple actions by maximizing the average uncertainty within the
offered subset. We prove that M-AUPO achieves a suboptimality gap of
$\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}}
\right)$, where $T$ is the total number of rounds, $d$ is the feature
dimension, and $|S_t|$ is the size of the subset at round $t$. This result
shows that larger subsets directly lead to improved performance and, notably,
the bound avoids the exponential dependence on the unknown parameter's norm,
which was a fundamental limitation in most previous works. Moreover, we
establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}}
\right)$, where $K$ is the maximum subset size. To the best of our knowledge,
this is the first theoretical result in PbRL with ranking feedback that
explicitly shows improved sample efficiency as a function of the subset size.

</details>


### [71] [Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference](https://arxiv.org/abs/2510.18768)
*Harry Amad,Zhaozhi Qian,Dennis Frauen,Julianna Piskorz,Stefan Feuerriegel,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出 STEAM，一种用于生成含治疗变量的合成数据以优化治疗效应分析的生成模型，强调保留协变量分布、治疗分配机制和结果生成机制，并给出评价指标。


<details>
  <summary>Details</summary>
Motivation: 真实世界医疗数据难以获取，需合成数据支持因果推断和方法开发；现有生成模型在治疗相关任务上存在局限。

Method: 提出 DESIDERATA（保留协变量分布、治疗分配机制、结果生成机制）并设计评估指标；提出 STEAM，将生成过程拟合为含治疗的数据生成过程，并优化上述目标。

Result: 在多指标评测下，STEAM 在数据复杂性增加时优于现有生成模型，达到最先进的性能。

Conclusion: STEAM 可提升治疗效应分析等因果推断任务的下游应用效果，提供生成式方法的新方向。

Abstract: Causal inference is essential for developing and evaluating medical
interventions, yet real-world medical datasets are often difficult to access
due to regulatory barriers. This makes synthetic data a potentially valuable
asset that enables these medical analyses, along with the development of new
inference methods themselves. Generative models can produce synthetic data that
closely approximate real data distributions, yet existing methods do not
consider the unique challenges that downstream causal inference tasks, and
specifically those focused on treatments, pose. We establish a set of
desiderata that synthetic data containing treatments should satisfy to maximise
downstream utility: preservation of (i) the covariate distribution, (ii) the
treatment assignment mechanism, and (iii) the outcome generation mechanism.
Based on these desiderata, we propose a set of evaluation metrics to assess
such synthetic data. Finally, we present STEAM: a novel method for generating
Synthetic data for Treatment Effect Analysis in Medicine that mimics the
data-generating process of data containing treatments and optimises for our
desiderata. We empirically demonstrate that STEAM achieves state-of-the-art
performance across our metrics as compared to existing generative models,
particularly as the complexity of the true data-generating process increases.

</details>


### [72] [CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training](https://arxiv.org/abs/2510.18784)
*Soroush Tabesh,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: CAGE 通过对 STE 的曲率修正来实现量化感知梯度估计，理论上给出 Pareto-optimal 的量化优化解并在平滑非凸场景下提供收敛性保证；实现与 Adam 统计量协同，方法与优化器无关。实验表明，在 W4A4 的低比特量化下，对大模型（如最高 800M 参数）可回收超过 10% 的量化引起的损失，显著缩小与原生训练的差距。


<details>
  <summary>Details</summary>
Motivation: 现有低位比特量化感知训练（QAT）在实际准确度上仍与原生训练存在较大差距。量化过程引发的损失增加需要一个利用局部曲率信息的更精细梯度校正，以提高收敛性和最终性能。

Method: 提出 Curvature-Aware Gradient Estimation (CAGE)，在 STE 梯度基础上加入曲率感知修正项。该修正来源于将 QAT 问题视为多目标优化，兼顾损失最小化与量化约束，得到依赖局部曲率信息的原理性修正项。理论上引入 Pareto-最优解的框架，并在平滑非凸设置下给出收敛性保证。实现上方法对优化器无赖性，并利用 Adam 统计量实现高效计算。

Result: 理论方面，给出 Pareto-optimal 量化优化的解并证明 CAGE 在平滑非凸条件下具有较强的收敛性。实验方面，在预训练规模可达 800M 参数的 Llama风格模型中，CAGE 相较于忽略曲率的方法及其他异常值处理方法，在 W4A4 量化下恢复了超过 10% 的量化引起的损失增量，显示曲率感知修正能够有效降低量化带来的性能损失。

Conclusion: 曲率感知的梯度修正能显著缩小量化感知训练与原生训练之间的差距，CAGE 提供了一种原理性、实现高效且与优化器无关的解决方案，适用于大模型的低比特量化场景。

Abstract: Despite significant work on low-bit quantization-aware training (QAT), there
is still a large accuracy gap between such techniques and native training. To
address this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new
QAT method that augments the straight-through estimator (STE) gradient with a
curvature-aware correction designed to counteract the loss increase induced by
quantization. CAGE is derived from a multi-objective view of QAT that balances
loss minimization with adherence to quantization constraints, yielding a
principled correction term that depends on local curvature information. On the
theoretical side, we introduce the notion of Pareto-optimal solutions for
quantized optimization, and establish that CAGE yields strong convergence
guarantees in the smooth non-convex setting. In terms of implementation, our
approach is optimizer-agnostic, but we provide a highly-efficient
implementation that leverages Adam statistics. When pre-training Llama-style
models of up to 800M-parameters, CAGE recovers over 10% of the
quantization-induced loss increase in the W4A4 regime over outlier-mitigation
methods. These results indicate that curvature-aware gradient corrections can
bridge the remaining performance gap beyond current outlier-handling methods.

</details>


### [73] [Stick-Breaking Embedded Topic Model with Continuous Optimal Transport for Online Analysis of Document Streams](https://arxiv.org/abs/2510.18786)
*Federica Granese,Serena Villata,Charles Bouveyron*

Main category: cs.LG

TL;DR: SB-SETM extends ETM for online topic modeling on data streams by merging models over time with dynamic topic counts and topic embedding merging via optimal transport; shows superior performance on simulated data and a real-world news dataset from 2022-2023.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of online (streaming) topic modeling where topics evolve over time and the number of active topics is unknown, requiring dynamic modeling and topic alignment across timesteps.

Method: Introduce SB-SETM, an online extension of Embedded Topic Model (ETM) using a truncated stick-breaking process to infer the number of active topics per timestep and a continuous optimal transport-based strategy to merge topic embeddings across batches.

Result: SB-SETM outperforms baseline online topic models on simulated scenarios and demonstrates effective handling of evolving topics in a real-world news corpus related to the Russian-Ukrainian conflict during 2022-2023.

Conclusion: SB-SETM provides a scalable online topic modeling framework that automatically adapts the number of topics and aligns topic embeddings over time, improving performance on streaming data.

Abstract: Online topic models are unsupervised algorithms to identify latent topics in
data streams that continuously evolve over time. Although these methods
naturally align with real-world scenarios, they have received considerably less
attention from the community compared to their offline counterparts, due to
specific additional challenges. To tackle these issues, we present SB-SETM, an
innovative model extending the Embedded Topic Model (ETM) to process data
streams by merging models formed on successive partial document batches. To
this end, SB-SETM (i) leverages a truncated stick-breaking construction for the
topic-per-document distribution, enabling the model to automatically infer from
the data the appropriate number of active topics at each timestep; and (ii)
introduces a merging strategy for topic embeddings based on a continuous
formulation of optimal transport adapted to the high dimensionality of the
latent topic space. Numerical experiments show SB-SETM outperforming baselines
on simulated scenarios. We extensively test it on a real-world corpus of news
articles covering the Russian-Ukrainian war throughout 2022-2023.

</details>


### [74] [When LRP Diverges from Leave-One-Out in Transformers](https://arxiv.org/abs/2510.18810)
*Weiqiu You,Siqi Zeng,Yao-Hung Hubert Tsai,Makoto Yamada,Han Zhao*

Main category: cs.LG

TL;DR: LRP在现代Transformer中可能无法可靠近似LOO；双线性传播规则触犯实现不变性且softmax传播存在误差；通过绕过softmax、仅通过值矩阵传播可显著提升LOO对齐度，提示在Transformer中的LRP受这两类问题影响。


<details>
  <summary>Details</summary>
Motivation: 评估Layer-Wise Relevance Propagation在Transformer中的 axiomatic soundness与实用性，特别是其实现不变性公理与softmax传播误差对与LOO近似能力的影响，并比较CP-LRP作为诊断基线的表现。

Method: 1) 形式分析并理论证明AttnLRP中的双线性传播规则违反实现不变性；2) 在线性注意力层进行实证验证；3) 重新考察CP-LRP，实验中将 softmax 层的传播绕过——仅通过值矩阵回传播相关性，评估其对LOO对齐的提升，尤其在Transformer中后段层。

Result: 理论上揭示：双线性因子分解的敏感性可能削弱LRP的解释性；softmax传播误差可能共同降低LRP在Transformers中近似LOO的能力。实验证据表明，在中后段Transformer层，绕过softmax的CP-LRP策略显著提高与LOO的一致性。

Conclusion: LRP要在Transformer中可靠近似LOO，需要解决双线性传播规则的实现不变性问题与softmax传播误差问题；可通过改用非softmax路径的传播或改进传播规则来提升对LOO的对齐。

Abstract: Leave-One-Out (LOO) provides an intuitive measure of feature importance but
is computationally prohibitive. While Layer-Wise Relevance Propagation (LRP)
offers a potentially efficient alternative, its axiomatic soundness in modern
Transformers remains largely under-examined. In this work, we first show that
the bilinear propagation rules used in recent advances of AttnLRP violate the
implementation invariance axiom. We prove this analytically and confirm it
empirically in linear attention layers. Second, we also revisit CP-LRP as a
diagnostic baseline and find that bypassing relevance propagation through the
softmax layer -- backpropagating relevance only through the value matrices --
significantly improves alignment with LOO, particularly in middle-to-late
Transformer layers. Overall, our results suggest that (i) bilinear
factorization sensitivity and (ii) softmax propagation error potentially
jointly undermine LRP's ability to approximate LOO in Transformers.

</details>


### [75] [Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards](https://arxiv.org/abs/2510.18814)
*Mengqi Li,Lei Zhao,Anthony Man-Cho So,Ruoyu Sun,Xiao Li*

Main category: cs.LG

TL;DR: 提出一种在线自监督微调OSFT，模型自生成数据并就地微调，无需奖励信号的高效推理训练方法；在数学推理任务上与强RLVR方法相当，且研究表明其效率和鲁棒性良好，提供开源代码。


<details>
  <summary>Details</summary>
Motivation: 降低对复杂奖励信号的依赖，提升LLM推理能力的训练效率；利用预训练阶段获得的潜在偏好与 latent knowledge，探索更简洁高效的训练范式。

Method: 模型产生自回答并用其自生成的数据进行即时的有监督微调（在线SFT），默认单轮 rollout，未使用外部奖励；核心在于激活并强化模型的内部偏好与潜在知识，以提升推理能力。

Result: 在挑战性数学推理任务上的下游性能与GRPO等RLVR方法接近；消融研究显示OSFT具备效率与鲁棒性优势。

Conclusion: OSFT为更简单、成本更低的奖励驱动训练替代方案，具有良好应用前景；代码公开，便于复现实验与扩展。

Abstract: We present a simple, self-help online supervised finetuning (OSFT) paradigm
for LLM reasoning. In this paradigm, the model generates its own responses and
is immediately finetuned on this self-generated data. OSFT is a highly
efficient training strategy for LLM reasoning, as it is reward-free and uses
just one rollout by default. Experiment results show that OSFT achieves
downstream performance on challenging mathematical reasoning tasks comparable
to strong reinforcement learning with verifiable rewards (RLVR) methods such as
GRPO. Our ablation study further demonstrates the efficiency and robustness of
OSFT. The major mechanism of OSFT lies in facilitating the model's own existing
preference (latent knowledge) learned from pretraining, which leads to
reasoning ability improvement. We believe that OSFT offers an efficient and
promising alternative to more complex, reward-based training paradigms. Our
code is available at https://github.com/ElementQi/OnlineSFT.

</details>


### [76] [Actor-Free Continuous Control via Structurally Maximizable Q-Functions](https://arxiv.org/abs/2510.18828)
*Yigit Korkmaz,Urvi Bhuwania,Ayush Jain,Erdem Bıyık*

Main category: cs.LG

TL;DR: 提出了一个纯粹的基于值的连续控制框架，即不使用独立 actor 的 Q-learning，通过对 Q 函数进行结构化最大化实现高效、稳定的学习；在标准仿真任务上与最先进基线相当，且在约束动作空间的环境中表现优于传统的 actor-critic 方法。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 解决连续动作空间中对 Q 值进行全动作求值的不可行性，以及基于 off-policy 的价值方法在训练过程中的不稳定性问题，尝试通过结构化的 Q 函数最大化和无 actor 的设定实现稳定且高效的学习。

Method: 提出一组关键的架构与算法选择，围绕对 Q 函数进行结构化最大化来实现纯值基的学习框架，并在多种标准仿真任务中评估其性能与样本效率，比较对象为基于 actor-critic 的方法。

Result: 在标准仿真任务中，该纯值基方法的性能和样本效率与最先进基线相当；在约束动作空间且价值函数非平滑的情境下，结构化最大化方法优于基于梯度的 actor-critic 最大化；并公开了代码。

Conclusion: 证明了无 actor 的纯值基框架在连续控制任务中也能达到竞争性表现，且在某些环境下更具稳定性与鲁棒性，拓展了价值函数最大化的应用范围。

Abstract: Value-based algorithms are a cornerstone of off-policy reinforcement learning
due to their simplicity and training stability. However, their use has
traditionally been restricted to discrete action spaces, as they rely on
estimating Q-values for individual state-action pairs. In continuous action
spaces, evaluating the Q-value over the entire action space becomes
computationally infeasible. To address this, actor-critic methods are typically
employed, where a critic is trained on off-policy data to estimate Q-values,
and an actor is trained to maximize the critic's output. Despite their
popularity, these methods often suffer from instability during training. In
this work, we propose a purely value-based framework for continuous control
that revisits structural maximization of Q-functions, introducing a set of key
architectural and algorithmic choices to enable efficient and stable learning.
We evaluate the proposed actor-free Q-learning approach on a range of standard
simulation tasks, demonstrating performance and sample efficiency on par with
state-of-the-art baselines, without the cost of learning a separate actor.
Particularly, in environments with constrained action spaces, where the value
functions are typically non-smooth, our method with structural maximization
outperforms traditional actor-critic methods with gradient-based maximization.
We have released our code at https://github.com/USC-Lira/Q3C.

</details>


### [77] [A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure](https://arxiv.org/abs/2510.18841)
*Jingya Cheng,Alaleh Azhir,Jiazi Tian,Hossein Estiri*

Main category: cs.LG

TL;DR: 提出一个基于对比反事实推断的个体化风险估计框架，结合NICE和MOC等优化方法，在HF患者的PASC情境下通过预测模型实现干预分析与个体化解释。


<details>
  <summary>Details</summary>
Motivation: 在复杂生物医学系统中，需要对个体进行可解释且可操作的干预分析；对比反事实推断可将因果推理与预测建模结合，提升临床决策的可解释性与可操作性。

Method: 将正则化预测建模与反事实搜索相结合，兼具精确枚举与优化求解，应用NICE和MOC等算法在高维干预空间中高效探索。基于包含诊断、实验室与用药的纵向数据的健康系统队列，对2700余名SARS-CoV-2阳性且既往HF患者进行分析。

Result: 模型在测试集上表现出良好区分度（AUROC=0.88，95% CI: 0.84-0.91），并产生可解释、患者特异的反事实，量化改变共病模式或治疗因素对预测结局的潜在影响。

Conclusion: 将反事实推理形式化为对预测函数的优化问题，提供一个严格、可解释且计算有效的个体化推断框架，适用于复杂生物医学系统的个体化分析与干预评估。

Abstract: Counterfactual inference provides a mathematical framework for reasoning
about hypothetical outcomes under alternative interventions, bridging causal
reasoning and predictive modeling. We present a counterfactual inference
framework for individualized risk estimation and intervention analysis,
illustrated through a clinical application to post-acute sequelae of COVID-19
(PASC) among patients with pre-existing heart failure (HF). Using longitudinal
diagnosis, laboratory, and medication data from a large health-system cohort,
we integrate regularized predictive modeling with counterfactual search to
identify actionable pathways to PASC-related HF hospital admissions. The
framework combines exact enumeration with optimization-based methods, including
the Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective
Counterfactuals (MOC) algorithms, to efficiently explore high-dimensional
intervention spaces. Applied to more than 2700 individuals with confirmed
SARS-CoV-2 infection and prior HF, the model achieved strong discriminative
performance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable,
patient-specific counterfactuals that quantify how modifying comorbidity
patterns or treatment factors could alter predicted outcomes. This work
demonstrates how counterfactual reasoning can be formalized as an optimization
problem over predictive functions, offering a rigorous, interpretable, and
computationally efficient approach to personalized inference in complex
biomedical systems.

</details>


### [78] [Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](https://arxiv.org/abs/2510.18874)
*Howard Chen,Noam Razin,Karthik Narasimhan,Danqi Chen*

Main category: cs.LG

TL;DR: RL 相较 SFT 在后训练中对旧能力遗忘更少，且目标任务表现相当或更好；其原因在于 RL 的 on-policy 数据驱动的模式寻求特性有助于保留先验知识；实践意义在于使用近似 on-policy 数据即可更高效地缓解遗忘。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示并给出缓解大语言模型后训练中灾难性遗忘的指导原则，通过系统比较 SFT 与 RL 的遗忘模式，并在简化模型的混合分布框架下分析其机制。

Method: 在 Llama、Qwen 等模型与指令跟随、常识、算术推理等任务上比较 SFT 与 RL 的遗忘与任务性能；提出将模型视为先验分布与目标任务分布的混合，分析 RL 的模式寻求如何维持先验信息；通过对比 KL 正则化、优势估计等方法，验证 on-policy 数据对鲁棒性的作用；探讨使用近似 on-policy 数据的实际可行性。

Result: RL 相较 SFT 具有更低的遗忘率，同时在目标任务上达到可比或更高的性能；这种差异在不同模型家族和任务中保持一致； RL 的 on-policy 数据驱动导致对先验知识的保护；在实际设置中，近似 on-policy 数据能有效提升对遗忘的鲁棒性，且比完全 on-policy 数据更高效。

Conclusion: RL 的 mode-seeking 行为（来自 on-policy 数据）在学习新任务时有助于保持先验知识，提供缓解后训练遗忘的有效机制；使用近似 on-policy 数据是一个更高效、实用的遗忘缓解策略。

Abstract: Adapting language models (LMs) to new tasks via post-training carries the
risk of degrading existing capabilities -- a phenomenon classically known as
catastrophic forgetting. In this paper, toward identifying guidelines for
mitigating this phenomenon, we systematically compare the forgetting patterns
of two widely adopted post-training methods: supervised fine-tuning (SFT) and
reinforcement learning (RL). Our experiments reveal a consistent trend across
LM families (Llama, Qwen) and tasks (instruction following, general knowledge,
and arithmetic reasoning): RL leads to less forgetting than SFT while achieving
comparable or higher target task performance. To investigate the cause for this
difference, we consider a simplified setting in which the LM is modeled as a
mixture of two distributions, one corresponding to prior knowledge and the
other to the target task. We identify that the mode-seeking nature of RL, which
stems from its use of on-policy data, enables keeping prior knowledge intact
when learning the target task. We then verify this insight by demonstrating
that the use on-policy data underlies the robustness of RL to forgetting in
practical settings, as opposed to other algorithmic choices such as the KL
regularization or advantage estimation. Lastly, as a practical implication, our
results highlight the potential of mitigating forgetting using approximately
on-policy data, which can be substantially more efficient to obtain than fully
on-policy data.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [79] [JAUNT: Joint Alignment of User Intent and Network State for QoE-centric LLM Tool Routing](https://arxiv.org/abs/2510.18550)
*Enhan Li,Hongyang Du*

Main category: cs.NI

TL;DR: JAUNT is a framework for joint alignment of user intent and network state to optimize QoE in tool routing for LLMs, using dual-view alignment and network profiling to map performance into semantic space; benchmarks show QoE gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Current routing relies on semantic matching between user queries and tools, neglecting external network factors that affect user experience. Aligning user intent with network state is necessary to improve QoE in LLM service orchestration.

Method: JAUNT employs dual-view alignment: it interprets user intent to guide routing while using LLM agents to construct network profiles that map numerical QoS indicators into semantic guidance. A benchmarking suite combines diverse user patterns with heterogeneous network states.

Result: JAUNT significantly improves QoE compared with several baselines, demonstrating the value of jointly aligning user intent and network state for scalable LLM service orchestration.

Conclusion: Joint alignment of user intent and network state is beneficial for QoE-centric tool routing in LLM ecosystems, and the proposed benchmark enables systematic evaluation and comparison.

Abstract: Large Language Models (LLMs) increasingly rely on emerging protocols such as
the Model Context Protocol (MCP) to invoke external tools and services.
However, current tool routing mechanisms remain fragile because they only
consider functional matching between users' queries and tools. In practice,
user intent expressed through queries can be vague or underspecified, and the
actual Quality of Experience (QoE) also depends on external factors such as
link latency and server availability that are not captured by semantics alone.
To address this challenge, we propose JAUNT, a framework for Joint Alignment of
User intent and Network state in QoE-centric Tool routing. JAUNT introduces a
dual-view alignment strategy that interprets user intent while employing LLM
agents to construct network profiles, mapping numerical performance indicators
into the semantic space to guide routing. We further design a benchmark that
integrates diverse user request patterns with heterogeneous network states,
enabling systematic evaluation of QoE outcomes. Experimental results show that
JAUNT significantly improves QoE compared with several baselines, demonstrating
the importance of aligning both intent and network state for scalable LLM
service orchestration.

</details>


### [80] [Formal Methods for Mobile Ad Hoc Networks: A Survey](https://arxiv.org/abs/2510.18730)
*Wan Fokkink,Rob van Glabbeek*

Main category: cs.NI

TL;DR: 对 MANET 路由协议的形式化分析方法进行全面综述，覆盖正确性、实时性与安全性等方面，以及专门的分析框架与移动模型，力求统一和梳理零散的研究文献。


<details>
  <summary>Details</summary>
Motivation: MANET 的无线、移动性和去中心化特性导致协议行为复杂，现有研究分散在不同形式化方法之中，缺乏统一的综述和对比，亟需系统梳理以指导后续研究。

Method: 对现有文献中的形式化规范、分析技术、框架以及适用于 MANET 的移动模型进行系统性综述，按分析目标（功能正确性、实时性、安全性）和分析框架分类，提出研究地图和对比。

Result: 给出一个完整的研究地图（taxonomy）与对比表，总结各类方法的适用性、优缺点和适用场景，明确目前的研究空白与挑战。

Conclusion: 该综述旨在提供一个全面且连贯的综述，帮助研究者理解形式化方法在 MANET 路由协议中的应用现状，指出需要改进的方向以促进方法学的整合与标准化。

Abstract: In a mobile ad hoc network (MANET), communication is wireless and nodes can
move independently. Properly analyzing the functional correctness, performance,
and security of MANET protocols is a challenging task. A wide range of formal
specification and analysis techniques have been employed in the analysis of
MANET protocols. This survey presents an overview of rigorous formal analysis
techniques and their applications, with a focus on MANET routing protocols.
Next to functional correctness, also real-time properties and security are
considered. Moreover, an overview is given of formal frameworks that target
MANETs specifically, as well as mobility models that underlie performance
analyses of MANET protocols. The aim is to give a comprehensive and coherent
overview of this rather scattered field, in which a variety of rigorous formal
methods have been applied to analyze different aspects of a wide range of MANET
protocols.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [81] [Information Capacity of EEG: Theoretical and Computational Limits of Recoverable Neural Information](https://arxiv.org/abs/2510.17841)
*Ishir Rao*

Main category: cs.IT

TL;DR: EEG的信息容量有限，脑源信号到EEG的互信息受制于物理测量，线性解码虽能解释方差但信息量远低于理论通道容量，受SNR和电极数量的对数影响。


<details>
  <summary>Details</summary>
Motivation: 揭示EEG在多大程度上能传递关于潜在皮层源、脑状态或思想内容的信息，以及测量物理学对信息传递的根本限制。

Method: 将信息理论与合成前向建模结合，利用高斯信道理论和经验仿真，评估从潜在 cortical sources 到头皮EEG 的互信息，并比较线性解码 vs. 理论通道容量。

Result: 头皮EEG每个样本仅传递数十比特的低维神经活动信息；在约64–128个电极后信息达到饱和，并随信噪比以对数方式增长；线性解码捕获几乎所有可线性恢复的方差，但其恢复的互信息远低于分析得到的通道容量，表明物理测量限制而非算法复杂性是主导因素。

Conclusion: EEG对脑状态或认知内容的结构性信息存在内在上限，受测量物理学的约束。

Abstract: Electroencephalography (EEG) is widely used to study human brain dynamics,
yet its quantitative information capacity remains unclear. Here, we combine
information theory and synthetic forward modeling to estimate the mutual
information between latent cortical sources and EEG recordings. Using
Gaussian-channel theory and empirical simulations, we find that scalp EEG
conveys only tens of bits per sample about low-dimensional neural activity.
Information saturates with approximately 64-128 electrodes and scales
logarithmically with signal-to-noise ratio (SNR). Linear decoders capture
nearly all variance that is linearly recoverable, but the mutual information
they recover remains far below the analytic channel capacity, indicating that
measurement physics - not algorithmic complexity - is the dominant limitation.
These results outline the intrinsic ceiling on how much structure about brain
state or thought content can be inferred from EEG.

</details>


### [82] [A Markov-Chain Characterization of Finite-State Dimension and a Generalization of Agafonov's Theorem](https://arxiv.org/abs/2510.18736)
*Laurent Bienvenu,Hugo Gimbert,Subin Pulari*

Main category: cs.IT

TL;DR: 本研究给出有限状态维度的新的信息理论表征：一个序列的有限状态维度可以通过在用该序列驱动的马尔可夫链的极限分布与其平稳分布之间的条件Kullback-Leibler散度来刻画。这推广了Schnorr-Stimm关于正态数的等价结果，并给出对任意序列的Agafonov定理的推广，以及自动子序列与原序列有限状态维度之间的紧致定量关系。


<details>
  <summary>Details</summary>
Motivation: 将有限状态维度的理解从仅限于Borel正态数的特性扩展到任意序列，并建立与信息理论（KL散度）及马尔可夫链仿真之间的联系；在探索自动子序列与原序列之间的维度关系方面提供一个统一和泛化框架。

Method: 将给定序列用于驱动有限状态马尔可夫链的仿真，比较该链的极限状态分布与其平稳分布，使用条件Kullback-Leibler散度来刻画有限状态维度；在此基础上推广Schnorr-Stimm等价，并推导出对Agafonov定理的泛化以及序列及其自动子序列的维度关系。

Result: 给出一个信息理论表征：有限状态维度等同于限制分布与平稳分布之间的条件KL散度；对正态数的经典等价（Agafonov等价）的推广得到对任意序列的结论，且给出原序列与其自动子序列维度之间的紧致定量关系。

Conclusion: 该工作建立了一个将有限状态信息率与基于马尔可夫链的分布信息度量联系起来的广泛框架，扩展了经典结果到任意序列及其自动子序列的情形，为有限状态维度的理论研究提供了新的工具。

Abstract: Finite-state dimension quantifies the asymptotic rate of information in an
infinite sequence as perceived by finite automata. For a fixed alphabet, the
infinite sequences that have maximal finite-state dimension are exactly those
that are Borel normal, i.e., in which all words of any given length appear with
the same frequency. A theorem of Schnorr and Stimm (1972) shows that a real
number is Borel normal if and only if, for every finite-state irreducible
Markov chain with fair transitions, when the chain is simulated using the
binary expansion of the given number, the empirical distribution of states
converges to its stationary distribution. In this paper we extend this
correspondence beyond normal numbers. We show that the finite-state dimension
of a sequence can be characterized in terms of the conditional Kullback-Leibler
divergence between the limiting distributions arising from the simulation of
Markov chains using the given sequence and their stationary distributions. This
provides a new information-theoretic characterization of finite-state dimension
which generalizes the Schnorr-Stimm result.
  As an application, we prove a generalization of Agafonov's theorem for normal
numbers. Agafonov's theorem states that a sequence is normal if and only if
every subsequence selected by a finite automaton is also normal. We extend this
to arbitrary sequences by establishing a tight quantitative relationship
between the finite-state dimension of a sequence and the finite-state
dimensions of its automatic subsequences.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [83] [In-Process Monitoring of Gear Power Honing Using Vibration Signal Analysis and Machine Learning](https://arxiv.org/abs/2510.17809)
*Massimo Capurso,Luciano Afferrante*

Main category: eess.SP

TL;DR: 基于振动信号、时频分析和三种子空间学习方法的在制监测框架，用SVM对齿轮功质进行四类分类，精度高、适于工业现场。


<details>
  <summary>Details</summary>
Motivation: 解决后处理检测和SPC在捕捉瞬态加工异常和实时 defect detection 的不足；需要在制过程的实时监控以提高NVH 符合性与预维护。

Method: 1) 连续数据采集（加速度计） 2) 时频信号分析 3) 三种特征提取：PCA、PCA+LDA、R-UMLDA（张量数据的正交多线判别+正则化） 4) SVM 分类器，将齿轮分成四类质量等级 5) 在工业数据集上训练并验证

Result: 在工业环境下，分类准确率高达100%。提取的谱特征具有可解释性，与工艺动态相关，有助于实时监控和预测性维护

Conclusion: 提出的框架实现了高效的在制监控，能够与预测性维护系统集成，提升NVH 质量控制和生产效率；四类等级分布得到可靠识别

Abstract: In modern gear manufacturing, stringent Noise, Vibration, and Harshness (NVH)
requirements demand high-precision finishing operations such as power honing.
Conventional quality control strategies rely on post-process inspections and
Statistical Process Control (SPC), which fail to capture transient machining
anomalies and cannot ensure real-time defect detection. This study proposes a
novel, data-driven framework for in-process monitoring of gear power honing
using vibration signal analysis and machine learning. Our proposed methodology
involves continuous data acquisition via accelerometers, followed by
time-frequency signal analysis. We investigate and compare the efficacy of
three subspace learning methods for features extraction: (1) Principal
Component Analysis (PCA) for dimensionality reduction; (2) a two-stage
framework combining PCA with Linear Discriminant Analysis (LDA) for enhanced
class separation; and (3) Uncorrelated Multilinear Discriminant Analysis with
Regularization (R-UMLDA), adapted for tensor data, which enforces feature
decorrelation and includes regularization for small sample sizes. These
extracted features are then fed into a Support Vector Machine (SVM) classifier
to predict four distinct gear quality categories, established through rigorous
geometrical inspections and test bench results of assembled gearboxes. The
models are trained and validated on an experimental dataset collected in an
industrial context during gear power-honing operations, with gears classified
into four different quality categories. The proposed framework achieves high
classification accuracy (up to 100%) in an industrial setting. The approach
offers interpretable spectral features that correlate with process dynamics,
enabling practical integration into real-time monitoring and predictive
maintenance systems.

</details>


### [84] [Exploring Complexity Changes in Diseased ECG Signals for Enhanced Classification](https://arxiv.org/abs/2510.17810)
*Camilo Quiceno Quintero,Sandip Varkey George*

Main category: eess.SP

TL;DR: 将ECG的非线性时序分析用于区分健康与疾病个体，并通过复杂性量化提高分类性能。


<details>
  <summary>Details</summary>
Motivation: ECG反映心脏电活动的复杂性，尝试用非线性时间序列分析揭示疾病对ECG的影响，并提高诊断模型的性能。

Method: 在PTB-XL数据集上，提取lead II的非线性度量，以及使用Spearman相关和互信息的跨通道时序特征（lead II、V2、AVL），并将这些量化特征输入机器学习模型进行分类评估。

Result: 在健康与疾病之间，以及五个诊断超类之间的大多数量度都存在显著差异（p<0.001）。将复杂性量化特征加入ML模型后，AUC从0.86（基线）提升到0.87（非线性量）和0.90（包含跨时序量）。

Conclusion: ECG的非线性与跨通道复杂性指标对疾病检测有增益，值得在心电诊断系统中结合到ML管道。

Abstract: The complex dynamics of the heart are reflected in its electrical activity,
captured through electrocardiograms (ECGs). In this study we use nonlinear time
series analysis to understand how ECG complexity varies with cardiac pathology.
Using the large PTB-XL dataset, we extracted nonlinear measures from lead II
ECGs, and cross-channel metrics (leads II, V2, AVL) using Spearman correlations
and mutual information. Significant differences between diseased and healthy
individuals were found in almost all measures between healthy and diseased
classes, and between 5 diagnostic superclasses ($p<.001$). Moreover,
incorporating these complexity quantifiers into machine learning models
substantially improved classification accuracy measured using area under the
ROC curve (AUC) from 0.86 (baseline) to 0.87 (nonlinear measures) and 0.90
(including cross-time series metrics).

</details>


### [85] [Single-Snapshot Gridless 2D-DoA Estimation for UCAs: A Joint Optimization Approach](https://arxiv.org/abs/2510.17818)
*Salar Nouri*

Main category: eess.SP

TL;DR: 提出一个基于不精确增广拉格朗日法(iALM)的网格无2D DOA估计框架，能够在单快照下对圆阵UCA进行高分辨率、鲁棒的方向估计，避免半正定规划（SDP）等高成本步骤。


<details>
  <summary>Details</summary>
Motivation: 现有网格无DOA方法在单快照场景中通常计算成本高且鲁棒性不足，难以实现高分辨率的2D DOA估计。需要一个在单快照情况下既高效又鲁棒的求解框架。

Method: 提出一个统一的优化问题：同时估计流形变换矩阵和源的方位-仰角对，形成数据保真和变换鲁棒性的一体化目标；使用不精确增广拉格朗日方法(iALM)求解，完全避免半正定规划(SDP)。

Result: 仿真实验表明，该iALM框架在鲁棒性与高分辨率方面表现优越，能够在单快照条件下提供稳健的网格无2D DOA估计。

Conclusion: 所提出的方法适用于苛刻的阵列信号处理应用，特别是在单快照场景下，能够实现高分辨率的网格无2D DOA估计。

Abstract: This paper tackles the challenging problem of gridless two-dimensional (2D)
direction-of-arrival (DOA) estimation for a uniform circular array (UCA) from a
single snapshot of data. Conventional gridless methods often fail in this
scenario due to prohibitive computational costs or a lack of robustness. We
propose a novel framework that overcomes these limitations by jointly
estimating a manifold transformation matrix and the source azimuth-elevation
pairs within a single, unified optimization problem. This problem is solved
efficiently using an inexact Augmented Lagrangian Method (iALM), which
completely circumvents the need for semidefinite programming. By unifying the
objectives of data fidelity and transformation robustness, our approach is
uniquely suited for the demanding single-snapshot case. Simulation results
confirm that the proposed iALM framework provides robust and high-resolution,
gridless 2D-DOA estimates, establishing its efficacy for challenging array
signal processing applications.

</details>


### [86] [Channel Modeling of Satellite-to-Underwater Laser Communication Links: An Analytical-Monte Carlo Hybrid Approach](https://arxiv.org/abs/2510.17811)
*Zhixing Wang,Renzhi Yuan,Haifeng Yao,Chuang Yang,Mugen Peng*

Main category: eess.SP

TL;DR: 提出了一种综合的StULC信道模型，通过分析-蒙特卡洛混合方法在三段式传输链路中同时考虑颗粒和湍流的影响，并给出在不同环境条件下的BER与中断概率分析，结果显示水下颗粒浓度对性能的影响显著，风速提升对性能影响较小。


<details>
  <summary>Details</summary>
Motivation: StULC（卫星-水下激光通信）信道由于传输距离长、三段式信道组成多变，且以往研究要么只关注单一分段信道，要么忽略颗粒与湍流的耦合效应，导致建模不完整和预测误差大，因此需要一个能够耦合大气、空气-水界面及水下各分段的综合信道模型。

Method: 提出基于分析-蒙特卡洛混合方法的StULC信道模型。首先利用扩展惠更斯-菲涅尔原理推导大气湍流下的强度分布；随后给出通过空气-水界面的Photon传播方向的闭式概率密度函数以简化建模；最后通过蒙特卡洛方法对水下信道进行建模并在接收平面得到功率分布。基于该模型对不同环境条件下的误比特率（BER）和失效概率（outage probability）进行分析。

Result: 数值结果显示水下粒子浓度对通信性能的影响比大气湍流和水下湍流更显著；此外，增大空气-水界面的风速对StULC链路的性能影响不显著，显示UNDERWATER颗粒是主要瓶颈。

Conclusion: 所提出的分析-蒙特卡洛混合模型能有效耦合三段信道及其粒子/湍流效应，提示在设计与优化中应重点关注水下颗粒浓度对链路性能的控制，同时界面风速的影响相对较小。该模型为StULC系统在不同环境条件下的性能预测提供了可靠工具。

Abstract: Channel modeling for satellite-to-underwater laser communication (StULC)
links remains challenging due to long distances and the diversity of the
channel constituents. The StULC channel is typically segmented into three
isolated channels: the atmospheric channel, the air-water interface channel,
and the underwater channel. Previous studies involving StULC channel modeling
either focused on separated channels or neglected the combined effects of
particles and turbulence on laser propagation. In this paper, we established a
comprehensive StULC channel model by an analytical-Monte Carlo hybrid approach,
taking into account the effects of both particles and turbulence. We first
obtained the intensity distribution of the transmitted laser beam after passing
through the turbulent atmosphere based on the extended Huygens-Fresnel
principle. Then we derived a closed-form probability density function of the
photon propagating direction after passing through the air-water interface,
which greatly simplified the modeling of StULC links. At last, we employed a
Monte Carlo method to model the underwater links and obtained the power
distribution at the receiving plane. Based on the proposed StULC channel model,
we analyzed the bit error rate and the outage probability under different
environmental conditions. Numerical results demonstrated that, the influence of
underwater particle concentration on the communication performance is much
pronounced than those of both the atmospheric turbulence and the underwater
turbulence. Notably, increasing the wind speed at the air-water interface does
not significantly worsen the communication performance of the StULC links.

</details>


### [87] [Covariance Matrix Construction with Preprocessing-Based Spatial Sampling for Robust Adaptive Beamforming](https://arxiv.org/abs/2510.17823)
*Saeed Mohammadzadeh,Rodrigo C. de Lamare,Yuriy Zakharov*

Main category: eess.SP

TL;DR: 提出一种鲁棒自适应波束形成框架，通过DoA估计与PPBSS重构干扰+噪声协方差矩阵，并在SOI角度区间内建立信号协方差，结合功率法得到SOI的波束方向，提供谱采样策略与成本分析，仿真表明优于部分现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决前端波束形成中的两大难题：(1) 估计矛盾的方向向量所导致的干扰阵列模型误差；(2) 数据协方差矩阵的重构难题，以提高鲁棒性和性能。

Method: 1) 在可用快照下估计干扰源的DoA，并自适应计算干扰信号的角度区间；2) 使用广义线性组合（GLC）算法结合 preprocessing-based spatial sampling (PPBSS) 重构干扰+噪声协方差矩阵 IPNC；3) 将 PPBSS 中的预处理矩阵用样本协方差矩阵 SCM 在收缩估计中的应用替代；4) 基于估计的角度区间设计功率谱采样策略；5) 针对 SOI 的角度区间构建信号协方差矩阵，并用功率法求解 SOI 的波束向量；6) 对比分析波束形态与计算成本，结合仿真验证性能。

Result: 仿真结果显示所提方法在鲁棒性和性能方面优于若干现有方法，且在计算成本方面具有竞争力，充分验证了 PPBSS 的有效性。

Conclusion: 提出了一整合 DoA 估计、PPBSS 重构、角度区间谱采样与功率法求解的鲁棒自适应波束形成框架，能有效缓解 SV 估计误差与协方差重构问题，具有良好鲁棒性和灵活性。

Abstract: This work proposes an efficient, robust adaptive beamforming technique to
deal with steering vector (SV) estimation mismatches and data covariance matrix
reconstruction problems. In particular, the direction-of-arrival(DoA) of
interfering sources is estimated with available snapshots in which the angular
sectors of the interfering signals are computed adaptively. Then, we utilize
the well-known general linear combination algorithm to reconstruct the
interference-plus-noise covariance (IPNC) matrix using preprocessing-based
spatial sampling (PPBSS). We demonstrate that the preprocessing matrix can be
replaced by the sample covariance matrix (SCM) in the shrinkage method. A power
spectrum sampling strategy is then devised based on a preprocessing matrix
computed with the estimated angular sectors' information. Moreover, the
covariance matrix for the signal is formed for the angular sector of the
signal-of-interest (SOI), which allows for calculating an SV for the SOI using
the power method. An analysis of the array beampattern in the proposed PPBSS
technique is carried out, and a study of the computational cost of competing
approaches is conducted. Simulation results show the proposed method's
effectiveness compared to existing approaches.

</details>


### [88] [Cross-Domain Multi-Person Human Activity Recognition via Near-Field Wi-Fi Sensing](https://arxiv.org/abs/2510.17816)
*Xin Li,Jingzhi Hu,Yinghui He,Hongbo Wang,Jin Gan,Jun Luo*

Main category: eess.SP

TL;DR: WiAnchor通过锚点对齐实现对缺失类别场景下的跨域自适应的Wi-Fi HAR框架，在多人员HAR中用近场感知提升区分度，获得>90%跨域准确率。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi HAR受限于粗糙的空间分辨率，难以区分多主体。近场 dominates，每个主体用个人Wi-Fi设备建立专用 sensing link，实现跨域自适应时，部分活动类别缺失使得微调困难。需要在缺失类别情况下仍能有效迁移。

Method: 提出WiAnchor，包含三步：预训练阶段通过增大类间特征边界提升可分性；微调阶段引入锚点匹配机制，基于缺失类别信息过滤主体特定干扰；最终利用特征与锚点的相似性提升识别。

Result: 在自建数据集上实现>90%跨域准确率，且在缺失活动类别时表现显著。

Conclusion: WiAnchor为缺失类别场景下的跨域自适应提供高效解决方案，显著提升Wi-Fi HAR的跨域迁移能力。

Abstract: Wi-Fi-based human activity recognition (HAR) provides substantial convenience
and has emerged as a thriving research field, yet the coarse spatial resolution
inherent to Wi-Fi significantly hinders its ability to distinguish multiple
subjects. By exploiting the near-field domination effect, establishing a
dedicated sensing link for each subject through their personal Wi-Fi device
offers a promising solution for multi-person HAR under native traffic. However,
due to the subject-specific characteristics and irregular patterns of
near-field signals, HAR neural network models require fine-tuning (FT) for
cross-domain adaptation, which becomes particularly challenging with certain
categories unavailable. In this paper, we propose WiAnchor, a novel training
framework for efficient cross-domain adaptation in the presence of incomplete
activity categories. This framework processes Wi-Fi signals embedded with
irregular time information in three steps: during pre-training, we enlarge
inter-class feature margins to enhance the separability of activities; in the
FT stage, we innovate an anchor matching mechanism for cross-domain adaptation,
filtering subject-specific interference informed by incomplete activity
categories, rather than attempting to extract complete features from them;
finally, the recognition of input samples is further improved based on their
feature-level similarity with anchors. We construct a comprehensive dataset to
thoroughly evaluate WiAnchor, achieving over 90% cross-domain accuracy with
absent activity categories.

</details>


### [89] [Carbon-Aware Orchestration of Integrated Satellite Aerial Terrestrial Networks via Digital Twin](https://arxiv.org/abs/2510.17825)
*Shumaila Javaid,Nasir Saeed*

Main category: eess.SP

TL;DR: A carbon-aware orchestration framework for Integrated Satellite Aerial Terrestrial Networks (ISATNs) using Digital Twin, optimizing energy use via gCO2/bit, with a multi-timescale PDCA loop and ISATN-specific knobs (carbon-aware handovers, UAV duty cycling, renewable-aware edge placement); achieves up to 29% reduction in emissions vs QoS-only orchestration, and better renewable utilization and resilience.


<details>
  <summary>Details</summary>
Motivation: ISATNs face unsustainable energy consumption and carbon emissions due to large-scale deployment. A digital-twin-based, carbon-aware management framework is needed to actively optimize energy use while meeting QoS under varying carbon intensity.

Method: Proposes a multi-timescale PDCA (Plan-Do-Check-Act) loop that combines day-ahead forecasting with real-time adaptive optimization. Uses gCO2/bit as the primary sustainability metric. Introduces ISATN-specific control knobs: carbon-aware handovers, UAV duty cycling, and renewable-aware edge placement. Validation via simulations with real carbon intensity data.

Result: Simulation results show up to 29% lower gCO2/bit than QoS-only orchestration, along with improved renewable utilization and resilience under adverse events.

Conclusion: Carbon-aware orchestration with Digital Twin for ISATNs is effective in reducing emissions while maintaining performance, demonstrating feasibility and guiding sustainable 6G deployment.

Abstract: Integrated Satellite Aerial Terrestrial Networks (ISATNs) are envisioned as
key enablers of 6G, providing global connectivity for applications such as
autonomous transportation, Industrial IoT, and disaster response. Their
large-scale deployment, however, risks unsustainable energy use and carbon
emissions. This work advances prior energy-aware studies by proposing a
carbon-aware orchestration framework for ISATNs that leverages Digital Twin
(DT) technology. The framework adopts grams of CO$_2$-equivalent per bit
(gCO$_2$/bit) as a primary sustainability metric and implements a multi
timescale Plan Do Check Act (PDCA) loop that combines day-ahead forecasting
with real-time adaptive optimization. ISATN-specific control knobs, including
carbon-aware handovers, UAV duty cycling, and renewable-aware edge placement,
are exploited to reduce emissions. Simulation results with real carbon
intensity data show up to 29\% lower gCO$_2$/bit than QoS-only orchestration,
while improving renewable utilization and resilience under adverse events.

</details>


### [90] [Synthetic EEG Generation using Diffusion Models for Motor Imagery Tasks](https://arxiv.org/abs/2510.17832)
*Henrique de Lima Alexandre,Clodoaldo Aparecido de Moraes Lima*

Main category: eess.SP

TL;DR: 使用扩散概率模型生成与运动想象相关的合成 EEG 信号，以丰富 EEG 基于脑机接口的数据集；在信号级与任务级评估中表现良好，分类准确率显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决 EEG 的数据稀缺、传感器成本高、跨被试变异大等挑战，提供一种能够生成高质量合成数据以辅助训练和提高 BCI 泛化能力的方法。

Method: 对真实 EEG 数据进行预处理，训练扩散模型以从噪声中重建 EEG 通道，生成合成信号；通过信号级（如均方误差、相关性）和任务级指标评估生成信号Quality；使用 KNN、CNN、U-Net 等分类器比较合成数据与真实数据在分类任务中的性能。

Result: 生成数据在分类任务中准确率超过 95%，均方误差低、与真实信号相关性高；合成信号能够较好拟合真实模式并可用于增强 EEG-BCI 的分类性能。

Conclusion: 扩散模型生成的 EEG 信号可作为数据补充，缓解数据稀缺问题并提升 EEG-BCI 的分类性能。

Abstract: Electroencephalography (EEG) is a widely used, non-invasive method for
capturing brain activity, and is particularly relevant for applications in
Brain-Computer Interfaces (BCI). However, collecting high-quality EEG data
remains a major challenge due to sensor costs, acquisition time, and
inter-subject variability. To address these limitations, this study proposes a
methodology for generating synthetic EEG signals associated with motor imagery
brain tasks using Diffusion Probabilistic Models (DDPM). The approach involves
preprocessing real EEG data, training a diffusion model to reconstruct EEG
channels from noise, and evaluating the quality of the generated signals
through both signal-level and task-level metrics. For validation, we employed
classifiers such as K-Nearest Neighbors (KNN), Convolutional Neural Networks
(CNN), and U-Net to compare the performance of synthetic data against real data
in classification tasks. The generated data achieved classification accuracies
above 95%, with low mean squared error and high correlation with real signals.
  Our results demonstrate that synthetic EEG signals produced by diffusion
models can effectively complement datasets, improving classification
performance in EEG-based BCIs and addressing data scarcity.

</details>


### [91] [Two Phases Leakage Detection Strategy Supported by DMAs](https://arxiv.org/abs/2510.17836)
*G. Messa,G. Acconciaioco,S. Ripani,L. Bozzelli,A. Simone,O. Giustolisi*

Main category: eess.SP

TL;DR: 提出了一种两阶段的基于模型的漏水检测策略：阶段1在DMA内识别并限定位漏点，阶段2对DMA内的管线进行预定位，返回待检管线序列以降低检验成本。策略通过AMSI指数抑制假阳性，并设计压力表布设策略。通过两套意大利南部阿普利亚地区的实际城市供水网进行验证。


<details>
  <summary>Details</summary>
Motivation: 目标是以尽可能低的检验成本实现漏水的准确检测与定位，尤其是在DMA层级识别与管线级别预定位上提供系统化、鲁棒的方案；现有方法在假阳性和设备布设方面存在局限性，需要一个可在实际水务运营中落地的策略。

Method: 提出两阶段策略：1) 将漏水视为异常，在常态下检测并初步定位；2) 在识别出的DMA内进行更细粒度的管段预定位，输出需要检查的管道序列。策略还引入AMSI指数以降低DMA识别阶段的假阳性，并提出基于随机数据库来评估DMA分区和压力计布设的配置对性能的影响；最后在两套实际的阿普利亚水网中进行研究与讨论。

Result: 证明AMSI在DMA识别阶段能显著抑制假阳性；该策略在两套真实的WDN上进行了研究和讨论，显示出在实际运营中用于降低检验成本、给出管线检测序列的潜力。

Conclusion: 该两阶段模型化策略在漏水检测与局部化方面具有可行性和应用价值，能在DMA层级识别与管线级别的预定位之间提供连续的工作流，并通过AMSI提高鲁棒性；并且给出压力表布设设计的方向，适用于水务运营的成本敏感型场景。

Abstract: The present work proposes a novel two phases model-based strategy for leakage
detection. The two phases are: the identification of the district metering area
(DMA) and the pipe pre-localization into the identified DMA. The strategy is
based on detecting and pre-localizing the punctual leakage as anomaly with
respect to the normal working conditions. A further novelty is the fact that
the pre-localization phase returns the sequence of pipes to inspect, which
makes the strategy attractive for water utilities, whose aim is to identify the
anomaly at DMA level and, successively, to localize it with the minimum
inspection cost. Furthermore, a random database is useful to test the
performance of the strategy with respect to the configuration of DMAs and the
pressure metering system. Consequently, a novel strategy to design the location
of pressure meters is also proposed. It is demonstrated that the entire
strategy limits false positives during the DMA identification phase by using
the recently proposed index named Asset Management Support Indicator (AMSI).
AMSI is invariant with respect to the deterioration, i.e., it is sensitive to
its increase causing punctual leakage. The strategy is studied and discussed
using two real Apulian WDNs managed by Acquedotto Pugliese.

</details>


### [92] [Majority Vote Compressed Sensing](https://arxiv.org/abs/2510.18008)
*Henrik Hellström,Jiwon Jeong,Ayfer Özgür,Viktoria Fodor,Carlo Fischione*

Main category: eess.SP

TL;DR: 提出MVCS框架：通过随机投影将高维稀疏数据降维到T维，并用多数投票的AirComp方案获得投影符号的和，从而在接收端利用1-bit压缩感知恢复高维稀疏数据总和；给出误差ε下的通道使用量界限T=O(kn log(d)/ε^2)，并提供相关算法与数值验证。


<details>
  <summary>Details</summary>
Motivation: 在非协作的AirComp中，直接对高维数据求和需要超过维度的通道使用量。若数据具有稀疏性，可通过随机变换降维并利用1-bit压缩感知实现无通道状态信息的聚合，从而显著降低通信开销。

Method: 每个设备对数据向量x_i进行随机投影s_i = Φ_i x_i，投影维度为T≪d；通过MV-AirComp得到符号向量y = sign(sum_i s_i)；在接收端利用1-bit CS对sum_i x_i进行重建，得到高维总和的近似。给出理论分析：在目标误差ε下，需要的通道使用量T为O(kn log(d)/ε^2)。此外，给出基于MVCS的直方图估计与分布式机器学习算法。

Result: 给出理论保证：MVCS能以所需的≈ε的l2误差在T=O(kn log(d)/ε^2)通道使用量下估计总和；数值实验表明MVCS在对比方法中具有优势，尤其在数据稀疏且维度高的场景。

Conclusion: 将稀疏性、随机投影与1-bit压缩感知融入非协作AirComp，显著降低通信开销并实现对高维数据总和的准确估计，提供可操作的算法与实证评估。

Abstract: We consider the problem of non-coherent over-the-air computation (AirComp),
where $n$ devices carry high-dimensional data vectors
$\mathbf{x}_i\in\mathbb{R}^d$ of sparsity $\lVert\mathbf{x}_i\rVert_0\leq k$
whose sum has to be computed at a receiver. Previous results on non-coherent
AirComp require more than $d$ channel uses to compute functions of
$\mathbf{x}_i$, where the extra redundancy is used to combat non-coherent
signal aggregation. However, if the data vectors are sparse, sparsity can be
exploited to offer significantly cheaper communication. In this paper, we
propose to use random transforms to transmit lower-dimensional projections
$\mathbf{s}_i\in\mathbb{R}^T$ of the data vectors. These projected vectors are
communicated to the receiver using a majority vote (MV)-AirComp scheme, which
estimates the bit-vector corresponding to the signs of the aggregated
projections, i.e., $\mathbf{y} = \text{sign}(\sum_i\mathbf{s}_i)$. By
leveraging 1-bit compressed sensing (1bCS) at the receiver, the real-valued and
high-dimensional aggregate $\sum_i\mathbf{x}_i$ can be recovered from
$\mathbf{y}$. We prove analytically that the proposed MVCS scheme estimates the
aggregated data vector $\sum_i \mathbf{x}_i$ with $\ell_2$-norm error
$\epsilon$ in $T=\mathcal{O}(kn\log(d)/\epsilon^2)$ channel uses. Moreover, we
specify algorithms that leverage MVCS for histogram estimation and distributed
machine learning. Finally, we provide numerical evaluations that reveal the
advantage of MVCS compared to the state-of-the-art.

</details>


### [93] [A Comparative Analysis of High-Level vs. Low-Level Simulations for Dynamic MAC Protocols in Wireless Sensor Networks](https://arxiv.org/abs/2510.18662)
*Shama Siddiqui,Anwar Ahmed Khan,Indrakshi Dey*

Main category: eess.SP

TL;DR: 本研究比较了ADP-MAC在高层理论仿真与详细实现两种层级下的能耗与时延趋势，发现两者结论存在显著差异，提示高层仿真可能因假设不现实而误导评估。


<details>
  <summary>Details</summary>
Motivation: 在大量MAC协议的性能评估中，需评估理论仿真与细节实现的一致性，以便在真实场景部署前进行可靠比较。

Method: 使用MATLAB进行高层理论仿真，使用TinyOS在Mica2平台上实现ADP-MAC的详细实现；通过改变信道轮询间隔来比较能耗和时延的变化趋势。

Result: 高层仿真中，能耗随轮询间隔增大而下降，时延随轮询间隔增大而增加；详细实现中，能耗与时延均随轮询间隔增大而上升；两者趋势显著不同。

Conclusion: 高层仿真可能因缺乏现实假设而给出与详细实现不一致的结论，因此在部署前应以详细实现结果进行验证，避免过度依赖理论仿真。

Abstract: Simulation studies are conducted at different levels of details for assessing
the performance of Media Access Control (MAC) protocols in Wireless Sensor
Networks (WSN). In the present-day scenario where hundreds of MAC protocols
have been proposed, it is important to assess the quality of performance
evaluation being conducted for each of the proposed protocols. It therefore
becomes crucial to compare the results of high-level theoretical simulations
with the detailed implementation results before any network protocol could be
deployed for a real-world scenario. In this work, we present a comparison of
high-level theoretical and detailed implementation results for Adaptive and
Dynamic Polling-MAC (ADP-MAC). MATLAB has been used for conducting initial
theoretical simulations and TinyOS has been used to develop the detailed
implementation of protocol for Mica2 platform. Performance evaluation of
ADP-MAC using the two levels of simulation has been conducted based on energy
and delay. In the high-level implementation, energy consumption was found to be
decreasing whereas delay was found to be increasing for increasing channel
polling intervals. On the other hand, when detailed implementation was
developed, it was observed that both energy consumption and delay revealed an
increasing trend with the increasing polling intervals. Therefore, it has been
shown that the trends for high- and low-level simulations for ADP-MAC are
significantly different, due to the lack of realistic assumptions in the
higher-level study.

</details>


### [94] [Wireless-Fed Pinching-Antenna Systems (Wi-PASS) for NextG Wireless Networks](https://arxiv.org/abs/2510.18743)
*Kasun R. Wijewardhana,Animesh Yadav,Ming Zeng,Mohamed Elsayed,Octavia A. Dobre,Zhiguo Ding*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Waveguide-based pinching-antenna systems (PASS) have recently emerged as a
promising solution to mitigate severe propagation losses in millimeter-wave and
terahertz bands by intelligently and flexibly establishing line-of-sight links.
However, their reliance on wire-based feeding confines deployment to areas near
the base station (BS), limiting installation flexibility and making them
cost-ineffective for serving distant users or regions. To overcome this
challenge, this article proposes wireless-fed pinchingantenna systems
(Wi-PASS), which employ wireless feeding to energize waveguides. Wi-PASS offer
a practical and cost-efficient means to extend coverage beyond the BS vicinity.
Several indoor and outdoor use cases demonstrate Wi-PASS advantages over PASS.
Numerical results further show that Wi-PASS deliver higher data rates than
conventional fixed-antenna systems, confirming the superior feasibility and
performance of Wi-PASS. Key future research directions are also discussed to
advance Wi-PASS deployment.

</details>


### [95] [SO(3)-invariant PCA with application to molecular data](https://arxiv.org/abs/2510.18827)
*Michael Fraiman,Paulina Hoyos,Tamir Bendory,Joe Kileel,Oscar Mickelin,Nir Sharon,Amit Singer*

Main category: eess.SP

TL;DR: 提出对三维体数据的SO(3)-不变PCA，避免对每个样本进行旋转数据增强，利用代数结构实现协方差计算量的平方根级别加速，并在真实分子数据上验证，显示在大规模高维重建中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在结构生物学等领域，3D体数据常具有未知且任意的朝向。传统PCA若要考虑所有旋转，需要对数据进行大量旋转副本，计算成本高且不可行。

Method: 将PCA推广到SO(3)不变框架，隐式处理所有旋转而非显式数据增强。通过利用底层代数对称结构，将协方差矩阵及其特征分解的计算量降至仅与总协方差条目数的平方根成比例，避免对每个样本进行旋转增强。

Result: 在真实的分子数据集上验证，展示该方法在有效性方面的表现，并为大规模、高维重建问题打开新的可能性。

Conclusion: 提出了一种高效的3D不变PCA框架，为未知朝向的三维数据的降维与去噪提供新的工具，尤其适用于大尺度分子重建等应用。

Abstract: Principal component analysis (PCA) is a fundamental technique for
dimensionality reduction and denoising; however, its application to
three-dimensional data with arbitrary orientations -- common in structural
biology -- presents significant challenges. A naive approach requires
augmenting the dataset with many rotated copies of each sample, incurring
prohibitive computational costs. In this paper, we extend PCA to 3D volumetric
datasets with unknown orientations by developing an efficient and principled
framework for SO(3)-invariant PCA that implicitly accounts for all rotations
without explicit data augmentation. By exploiting underlying algebraic
structure, we demonstrate that the computation involves only the square root of
the total number of covariance entries, resulting in a substantial reduction in
complexity. We validate the method on real-world molecular datasets,
demonstrating its effectiveness and opening up new possibilities for
large-scale, high-dimensional reconstruction problems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [96] [Towards the True Switching-ON of Transistors](https://arxiv.org/abs/2510.17815)
*Wucheng Ying,Jinwei Qi,Hui Zhao,Ameer Janabi,Hui Li,Biao Zhao,Teng Long*

Main category: eess.SY

TL;DR: 提出基于第一性原理的统一范式来解释晶体管开关，并推导出新的Eon预测模型；相较传统模型，误差显著降低且平均提升约17倍。


<details>
  <summary>Details</summary>
Motivation: 长期以来，晶体管开关被视为黑箱，难以用物理定律解释且未与电路理论整合，导致对能耗、碳排放等影响的预测不准确，成为可持续性发展的阻碍。

Method: 提出一个统一的第一性原理范式，用以解释开关现象，揭示ON态的物理起源与机制，并据此推导新的Eon预测模型。

Result: 新模型的预测误差范围为0.88%–11.60%，显著优于传统模型的34.41%–80.05%，平均提升约17倍。

Conclusion: 该范式奠定教材级的开关物理基础，将晶体管开关研究从经验分析提升到第一性原理分析，促进跨学科应用与对可持续发展的推动。

Abstract: Transistors are core component across all domains of electrical and
electronic engineering (EEE), such as data centers, electrified transportation,
robotics, renewables and grid applications, etc. Transistors' switching
behavior governs energy loss, carbon emissions, cooling demand, water use,
lifetime, material use and cost etc. throughout EEE. Despite near a century
since the transistor's invention, the understanding of transistor switching
remains fragmented: switching is treated as a black box relying on observed
waveforms, cannot be explained using physical laws alone, and is not integrated
into circuit theory. This forms one of the most critical barriers to
recognizing the true physical boundaries, prohibiting more sustainable
solutions. For example, the conventional Eon prediction model, derived from the
conventional switching analysis, exhibits significant prediction errors
(ranging from 34.41% to 80.05%). Here we present a unified first-principles
paradigm to explain the switching phenomena. Using this paradigm, we revealed
the physical origins and mechanisms of switching-ON phenomena across scenarios,
and derived the proposed Eon prediction model, with error ranging from 0.88% to
11.60%, achieving a 17-fold average improvement. These results demonstrate the
unprecedented power of the proposed paradigm: textbook-level foundations are
established, transforming the fundamental understanding of transistor switching
from empirical to first-principles analysis, and simultaneously stimulating
follow-up research and applications for sustainable development across
disciplines.

</details>


### [97] [Mixed Monotonicity Reachability Analysis of Neural ODE: A Trade-Off Between Tightness and Efficiency](https://arxiv.org/abs/2510.17859)
*Abdelrahman Sayed Sayed,Pierre-Jean Meyer,Mohamed Ghazel*

Main category: eess.SY

TL;DR: 提出一种基于区间的神经ODE可达性分析方法，利用连续时间混合单调性对可到达集合进行上界近似；实现于TIRA并提供单步、增量和边界策略，与CORA的Zonotopes和NNV2.0的星集表示相比，在可扩展性和实时性方面具有折中优势。


<details>
  <summary>Details</summary>
Motivation: 验证神经ODE在连续时间域中的可达性仍然具有挑战性，现有工具对神经ODE的适用性有限，亟需高维、实时性强且对安全敏感的应用场景的高效可达性分析方法。通过混合单调性和几何结构（同胚性质、区间盒子）实现轻量级、可扩展的形式分析。

Method: 将神经ODE动力学嵌入混合单调系统，基于连续时间混合单调性构造区间可达性，并利用初始集合及其边界的几何结构。该方法在TIRA中实现，提供单步、增量和边界驱动的求解策略。与CORA的zonotope和NNV2.0的星集表示进行比较，强调稳定性和效率的折中。

Result: 给出有界且声音的上确界近似，计算效率高，能够处理高维、实时场景；在精确性方面相较于更紧的表示（zonotopes、星集）有一定权衡，但在速度和可扩展性方面具有优势。

Conclusion: 该方法利用混合单调性嵌入和几何对称性的特点，为神经ODE提供一种轻量级、可扩展的可达性分析路线，促进面向安全关键应用的形式化分析，并通过对螺旋系统和定点吸引子系统的数值示例验证其可行性。

Abstract: Neural ordinary differential equations (neural ODE) are powerful
continuous-time machine learning models for depicting the behavior of complex
dynamical systems, but their verification remains challenging due to limited
reachability analysis tools adapted to them. We propose a novel interval-based
reachability method that leverages continuous-time mixed monotonicity
techniques for dynamical systems to compute an over-approximation for the
neural ODE reachable sets. By exploiting the geometric structure of full
initial sets and their boundaries via the homeomorphism property, our approach
ensures efficient bound propagation. By embedding neural ODE dynamics into a
mixed monotone system, our interval-based reachability approach, implemented in
TIRA with single-step, incremental, and boundary-based approaches, provides
sound and computationally efficient over-approximations compared with CORA's
zonotopes and NNV2.0 star set representations, while trading tightness for
efficiency. This trade-off makes our method particularly suited for
high-dimensional, real-time, and safety-critical applications. Applying mixed
monotonicity to neural ODE reachability analysis paves the way for lightweight
formal analysis by leveraging the symmetric structure of monotone embeddings
and the geometric simplicity of interval boxes, opening new avenues for
scalable verification aligned with the symmetry and geometry of neural
representations. This novel approach is illustrated on two numerical examples
of a spiral system and a fixed-point attractor system modeled as a neural ODE.

</details>


### [98] [DMTrack: Deformable State-Space Modeling for UAV Multi-Object Tracking with Kalman Fusion and Uncertainty-Aware Association](https://arxiv.org/abs/2510.17860)
*Zenghuang Fu,Xiaofeng Han,Mingda Jia,Jin ming Yang,Qi Zeng,Muyang Zahng,Changwei Wang,Weiliang Meng,Xiaopeng Zhang*

Main category: eess.SY

TL;DR: 提出 DMTrack 的多目标跟踪框架，面向 UAVs，由 DeformMamba、MotionGate 和不确定性感知关联组成，强调在缺乏外观信息的情况下通过可变形运动建模提升身份连贯性与跟踪鲁棒性，且在 VisDrone-MOT/UAVDT 上实现了领先性能并保持较高效率。


<details>
  <summary>Details</summary>
Motivation: UAV 场景下的多目标跟踪受物体运动不可预测、遮挡频发、外观信息匮乏等因素影响，且快速/非线性运动与无人机抖动会导致轨迹估计不稳定与身份切换。传统卡尔man 滤波等运动模型难以同时捕捉线性与非线性动态，因此需要更灵活的状态空间建模和预测融合。

Method: 提出三大模块：DeformMamba=可变形状态空间预测器，动态聚合历史运动状态以自适应建模轨迹；MotionGate=轻量级门控模块，将 Kalman 与 Mamba 的预测基于运动上下文与不确定性进行融合；不确定性感知关联策略，通过对齐运动趋势与预测置信度提升身份保持。系统在没有 appearance 模型的前提下工作，并强调对高速度与非线性运动的鲁棒性与效率。

Result: 在 VisDrone-MOT 与 UAVDT 基准上，DMTrack 展现出在身份一致性和跟踪精度上的领先性能，尤其在高速度和非线性运动场景下表现显著；同时保持竞争力的计算效率，展现出在实际 UAV 监控中的可用性。

Conclusion: DMTrack 为 UAV 场景下的 MOT 提供了一个无外观信息依赖的强健运动建模框架，通过可变形状态空间、基于上下文的预测融合以及不确定性感知的关联策略，提升身份保持与跟踪鲁棒性，同时具备良好效率，适合实际部署。

Abstract: Multi-object tracking (MOT) from unmanned aerial vehicles (UAVs) presents
unique challenges due to unpredictable object motion, frequent occlusions, and
limited appearance cues inherent to aerial viewpoints. These issues are further
exacerbated by abrupt UAV movements, leading to unreliable trajectory
estimation and identity switches. Conventional motion models, such as Kalman
filters or static sequence encoders, often fall short in capturing both linear
and non-linear dynamics under such conditions. To tackle these limitations, we
propose DMTrack, a deformable motion tracking framework tailored for UAV-based
MOT. Our DMTrack introduces three key components: DeformMamba, a deformable
state-space predictor that dynamically aggregates historical motion states for
adaptive trajectory modeling; MotionGate, a lightweight gating module that
fuses Kalman and Mamba predictions based on motion context and uncertainty; and
an uncertainty-aware association strategy that enhances identity preservation
by aligning motion trends with prediction confidence. Extensive experiments on
the VisDrone-MOT and UAVDT benchmarks demonstrate that our DMTrack achieves
state-of-the-art performance in identity consistency and tracking accuracy,
particularly under high-speed and non-linear motion. Importantly, our method
operates without appearance models and maintains competitive efficiency,
highlighting its practicality for robust UAV-based tracking.

</details>


### [99] [An Exact Quantile-Energy Equality for Terminal Halfspaces in Linear-Gaussian Control with a Discrete-Time Companion, KL/Schrodinger Links, and High-Precision Validation](https://arxiv.org/abs/2510.17945)
*Sandro Andric*

Main category: eess.SY

TL;DR: 在线性高斯系统中，最小二次控制能量与终端半空间的正态分位点差的平方之间存在一个精确等式，并且存在匹配滤波控制器达到该最小能量；并给出离散时间伴随、与多领域的关系，以及高精度蒙特卡罗验证。


<details>
  <summary>Details</summary>
Motivation: 研究在控制能量最小化与事件概率（尾部量化）之间的联系，将经典控制理论与高斯几何、风险敏感控制、Schrödinger桥等联系起来，提供一个可实现的设计工具。

Method: 利用Gramians、Cauchy-Schwarz、Gaussian isoperimetry等工具，给出能量-分位数的等式，定义可控性对噪声比R_T^2(w)，构造匹配滤波控制；同时给出零阶保持的离散时间伴随，通过区块指数化实现；讨论M的奇异性处理和边界情况。

Result: 得到 E_min = (quantile_gap)^2 / (2 R_T^2)，且由匹配滤波控制实现；给出离散时间版本；将结果与最小能量控制、高斯等距性、风险敏感/ KL 控制、Schrödinger桥等关联；通过Monte Carlo验证高精度。

Conclusion: 该工作提供一个紧凑的综述式设计翻译器，尽管成分是经典但把分位-能量的显式等式与构造性实现系统地放在一起，且给出离散时间 companions；但并非普遍原则，适用范围有限。

Abstract: We prove an exact equality between the minimal quadratic control energy and
the squared normal-quantile gap for terminal halfspaces in linear-Gaussian
systems with additive control and quadratic effort $E(u)=\tfrac12\!\int u^\top
M u\,dt$ where $M=B^\top\Sigma^{-1}B$. For terminal halfspace events, the
minimal energy equals the squared normal-quantile gap divided by twice a
controllability-to-noise ratio $R_T^2(w)=(w^\topW_c^M w)/(w^\top V_T w)$ and is
attained by a matched-filter control. We provide an exact zero-order-hold
discrete-time companion via block exponentials, relate the result to
minimum-energy control, Gaussian isoperimetry, risk-sensitive/KL control, and
Schrodinger bridges, and validate to high precision with Monte Carlo. We state
assumptions, singular-$M$ handling, and edge cases. The statement is a compact
synthesis and design-ready translator, not a universal principle. Novelty:
while the ingredients (Gramians, Cauchy-Schwarz, Gaussian isoperimetry) are
classical, to our knowledge the explicit quantile-energy equality with a
constructive matched-filter achiever for terminal halfspaces, and its
discrete-time companion, are not recorded together in the cited literature.

</details>


### [100] [Urban Air Mobility: A Review of Recent Advances in Communication, Management, and Sustainability](https://arxiv.org/abs/2510.18235)
*Zhitong He,Zijing Wang,Lingxi Li*

Main category: eess.SY

TL;DR: 综述性论文汇总城市地面交通移动性（UAM）的最新进展，聚焦在通信、UAM管理和可持续性三大领域，比较现有解决方案并提出实现可扩展、可持续UAM生态系统的里程碑。


<details>
  <summary>Details</summary>
Motivation: 解决城市拥堵、提升通达性、推动环境可持续性；需要系统梳理2020年以来的研究进展、为未来研究和基础设施规划提供方向。

Method: 进行文献综述与综合分析，比较三大领域的前沿方案，提出跨域集成的关键技术与研究路线。

Result: 提供对最新研究的综合视图、识别研究差距、提出实现可扩展UAM所需的技术与基础设施里程碑，以及未来研究方向。

Conclusion: 要实现可扩展、可持续的UAM生态系统，需要在动态频谱管理、稳健的空地链路、密集自治空域的空中交通概念、能源高效推进、充电基础设施和环境综合评估等方面取得突破。

Abstract: Urban Air Mobility (UAM) offers a transformative approach to addressing urban
congestion, improving accessibility, and advancing environmental
sustainability. Rapid progress has emerged in three tightly linked domains
since 2020: (1) Communication, where dynamic spectrum allocation and
low-altitude channel characterization support reliable air-ground data
exchange; (2) UAM management, with novel air-traffic control concepts for
dense, largely autonomous urban airspace; and (3) Sustainability, driven by
energy-efficient propulsion, integrated charging infrastructure, and holistic
environmental assessment. This paper reviews and synthesizes the latest
research across these areas, compares the state-of-the-art solutions, and
outlines the technological and infrastructural milestones that are critical to
realizing a scalable, sustainable UAM ecosystem.

</details>


### [101] [$\ell_1$-Based Adaptive Identification under Quantized Observations with Applications](https://arxiv.org/abs/2510.18738)
*Xin Zheng,Yifei Jin,Yujing Liu,Lei Guo*

Main category: eess.SY

TL;DR: 提出一种基于 l1 的自适应辨识算法，专门针对量化观测；在不依赖持久激励条件下证明全局收敛并证明平均遗憾随数据增多趋于0，并在真实司法判决数据上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量化观测在工程与社会科学等广泛应用，且基于 l1 的方法对离群值具有鲁棒性；然而将量化观测与自适应辨识结合的研究仍较少，因此需要一种能够在量化环境下稳定学习参数的算法。

Method: 提出一种新的基于 l1 的自适应辨识算法，专门针对量化观测；在不依赖传统的持续激励条件下，证明参数估计的全局收敛到真实值，并证明平均遗憾在数据量增大时渐进趋于0。

Result: 理论方面给出全局收敛性和渐进遗憾为0 的保证；在真实世界数据（司法量刑数据）上进行实验，展示出比现有方法更优的性能。

Conclusion: 所提出的 l1 自适应辨识算法有效解决了量化观测下的辨识问题，理论与实证均表明其鲁棒性与实用性，具有广泛的应用潜力。

Abstract: Quantized observations are ubiquitous in a wide range of applications across
engineering and the social sciences, and algorithms based on the $\ell_1$-norm
are well recognized for their robustness to outliers compared with their
$\ell_2$-based counterparts. Nevertheless, adaptive identification methods that
integrate quantized observations with $\ell_1$-optimization remain largely
underexplored. Motivated by this gap, we develop a novel $\ell_1$-based
adaptive identification algorithm specifically designed for quantized
observations. Without relying on the traditional persistent excitation
condition, we establish global convergence of the parameter estimates to their
true values and show that the average regret asymptotically vanishes as the
data size increases. Finally, we apply our new identification algorithm to a
judicial sentencing problem using real-world data, which demonstrates its
superior performance and practical significance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [102] [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)
*Amirkia Rafiei Oskooei,Mehmet S. Aktas*

Main category: cs.CR

TL;DR: 提出 BreakFun：通过 Trojan Schema 的三段式提示对 LLM 进行跨模型的结构化数据利用攻击，实验显示高成功率并提出 Adversarial Prompt Deconstruction 防御。


<details>
  <summary>Details</summary>
Motivation: 探究 LLM 在处理结构化数据和遵循语法规则时的脆弱性，揭示对结构化提示的依赖如何被利用来诱导模型产生有害输出，并寻求提升对齐鲁棒性的方法。

Method: 提出三段式提示：无害框架与 Chain-of-Thought 干扰相结合，以及一个精心设计的数据结构“Trojan Schema”，以强制模型遵循结构并产出有害内容；在 JailbreakBench 上对13个基础和专有模型进行跨模型评估，辅以消融实验以确认 Trojan Schema 的因果性；同时给出 Adversarial Prompt Deconstruction 防御，通过二级 LLM 对输入文本进行“Literal Transcription”以揭示潜在的真实意图。

Result: BreakFun 在 JailbreakBench 上对13个模型的平均攻破成功率达到89%，在若干主流模型上达到100% ASR；消融研究确认 Trojan Schema 是攻击的主要因果因素；提出的防御在抑制攻击方面显示出高效性，证明对结构化提示的逆向分析是一个可行的缓解方向。

Conclusion: 揭示 LLM 的核心优势（对结构和模式的强依赖）也会成为安全风险的源泉，论文为提升对齐鲁棒性提供了新的视角，强调通过识别与去结构化诱导的方式来增强模型安全性。

Abstract: The proficiency of Large Language Models (LLMs) in processing structured data
and adhering to syntactic rules is a capability that drives their widespread
adoption but also makes them paradoxically vulnerable. In this paper, we
investigate this vulnerability through BreakFun, a jailbreak methodology that
weaponizes an LLM's adherence to structured schemas. BreakFun employs a
three-part prompt that combines an innocent framing and a Chain-of-Thought
distraction with a core "Trojan Schema"--a carefully crafted data structure
that compels the model to generate harmful content, exploiting the LLM's strong
tendency to follow structures and schemas. We demonstrate this vulnerability is
highly transferable, achieving an average success rate of 89% across 13
foundational and proprietary models on JailbreakBench, and reaching a 100%
Attack Success Rate (ASR) on several prominent models. A rigorous ablation
study confirms this Trojan Schema is the attack's primary causal factor. To
counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a
defense that utilizes a secondary LLM to perform a "Literal
Transcription"--extracting all human-readable text to isolate and reveal the
user's true harmful intent. Our proof-of-concept guardrail demonstrates high
efficacy against the attack, validating that targeting the deceptive schema is
a viable mitigation strategy. Our work provides a look into how an LLM's core
strengths can be turned into critical weaknesses, offering a fresh perspective
for building more robustly aligned models.

</details>


### [103] [sNVMe-oF: Secure and Efficient Disaggregated Storage](https://arxiv.org/abs/2510.18756)
*Marcin Chrapek,Meni Orenbach,Ahmad Atamli,Marcin Copik,Fritz Alder,Torsten Hoefler*

Main category: cs.CR

TL;DR: 提出 sNVMe-oF，一种在 NVMe-oF 框架上支持机密计算的存储管理系统，通过新颖的计数器租赁、Hazel Merkle Tree、NVMe 元数据等实现 confidentiality、integrity、freshness，且在不修改协议的前提下，利用 CC 智能网卡加速，原型实现于 NVIDIA BlueField-3，性能损失低至 2%。


<details>
  <summary>Details</summary>
Motivation: 随着分离式存储和高效 NVMe-oF 的普及，Confidential Computing 的安全需求日益重要，但现有 CC 方法在存储领域难以扩展，性能或安全性受损。因此需要在不牺牲性能的前提下提供强安全性。

Method: 在不改动 NVMe-oF 协议的前提下，扩展存储管理系统 sNVMe-oF，提供机密性、完整性、 freshness；引入计数器租赁、Hazel Merkle Tree（HMT）等新概念，利用 NVMe 元数据提升数据路径性能，避免冗余的 IPSec；并在 CC 能力的智能 NIC 上实现加速；在 NVIDIA BlueField-3 上原型实现。

Result: 原型实现，评估表明在合成工作负载和 AI 训练场景下的性能损失可低至 2%，实现了接近线速的数据路径。

Conclusion: sNVMe-oF 在不修改 NVMe-oF 协议的情况下，为分离式存储提供了符合 CC threat 模型的 confidentiality、integrity、freshness 保证，并通过硬件加速提升性能，是高性能 CC 存储管理的可行方案。

Abstract: Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the
standard solution in modern data centers, achieving superior performance,
resource utilization, and power efficiency. Simultaneously, confidential
computing (CC) is becoming the de facto security paradigm, enforcing stronger
isolation and protection for sensitive workloads. However, securing
state-of-the-art storage with traditional CC methods struggles to scale and
compromises performance or security. To address these issues, we introduce
sNVMe-oF, a storage management system extending the NVMe-oF protocol and
adhering to the CC threat model by providing confidentiality, integrity, and
freshness guarantees. sNVMe-oF offers an appropriate control path and novel
concepts such as counter-leasing. sNVMe-oF also optimizes data path performance
by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree
(HMT), and avoiding redundant IPSec protections. We achieve this without
modifying the NVMe-oF protocol. To prevent excessive resource usage while
delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.
We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can
achieve as little as 2% performance degradation for synthetic patterns and AI
training.

</details>


### [104] [ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2510.17919)
*Tenghui Huang,Jinbo Wen,Jiawen Kang,Siyong Chen,Zhengtao Li,Tao Zhang,Dongning Liu,Jiacheng Wang,Chengjun Cai,Yinqiu Liu,Dusit Niyato*

Main category: cs.CR

TL;DR: 提出 ParaVul：一个并行的少量参数微调+检索增强框架，用于提升智能合约漏洞检测的可靠性与正确性，通过 SLoRA、混合检索的 RAG、元学习融合以及链式推理提示，达到高 F1 指标（单标签 0.9398；多标签 0.9330）。


<details>
  <summary>Details</summary>
Motivation: 解决传统静态分析/形式验证的误报高、可扩展性差，以及 LLM 在推断成本和计算开销方面的挑战；需要更高效、可扩展的漏洞检测方法。

Method: 1) 引入 Sparse Low-Rank Adaptation (SLoRA) 对量化 LoRA 的 LLM 进行稀疏化微调；2) 构建漏洞合约数据集，设计混合检索系统，将密集检索与 BM25 结合，辅助验证 LLM 生成结果；3) 提出元学习模型融合 RAG 与 LLM 的输出，得到最终检测结果；4) 设计链式推理提示，生成漏洞检测报告。

Result: 在 F1 指标方面优于基线，单标签 0.9398，多标签 0.9330；展示 ParaVul 在检测准确性和鲁棒性方面的优势。

Conclusion: ParaVul 为智能合约漏洞检测提供一个高效、可扩展且鲁棒的方案，结合精简的微调、检索增强和输出融合，适合实际部署并为未来工作提供方向。

Abstract: Smart contracts play a significant role in automating blockchain services.
Nevertheless, vulnerabilities in smart contracts pose serious threats to
blockchain security. Currently, traditional detection methods primarily rely on
static analysis and formal verification, which can result in high
false-positive rates and poor scalability. Large Language Models (LLMs) have
recently made significant progress in smart contract vulnerability detection.
However, they still face challenges such as high inference costs and
substantial computational overhead. In this paper, we propose ParaVul, a
parallel LLM and retrieval-augmented framework to improve the reliability and
accuracy of smart contract vulnerability detection. Specifically, we first
develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA
introduces sparsification by incorporating a sparse matrix into quantized
LoRA-based LLMs, thereby reducing computational overhead and resource
requirements while enhancing their ability to understand vulnerability-related
issues. We then construct a vulnerability contract dataset and develop a hybrid
Retrieval-Augmented Generation (RAG) system that integrates dense retrieval
with Best Matching 25 (BM25), assisting in verifying the results generated by
the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of
the RAG system and the LLM, thereby generating the final detection results.
After completing vulnerability detection, we design chain-of-thought prompts to
guide LLMs to generate comprehensive vulnerability detection reports.
Simulation results demonstrate the superiority of ParaVul, especially in terms
of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for
multi-label detection.

</details>


### [105] [PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces](https://arxiv.org/abs/2510.18109)
*Wan Ki Wong,Sahel Torkamani,Michele Ciampi,Rik Sarkar*

Main category: cs.CR

TL;DR: PrivaDE 是一个基于区块链的隐私保护数据效用评分与选择协议，面向机器学习的数据市场。通过模型蒸馏、模型分割和切换证明等技术，在不透露模型或数据细节的前提下，对候选数据进行效用评分，确保隐私与安全，并实现可行的在线评估（百万参数模型下约15分钟内）。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动的机器学习中，模型构建者需要评估数据集的效用而不暴露私有模型细节，同时数据提供者也希望不被泄露数据信息，促成去中心化、可信任的数据市场。

Method: 提出 PrivaDE 协议，基于区块链的信任最小化设计，利用恶意安全性、对模型和数据的强隐私保护，集成模型蒸馏、模型切分、切换式零知识证明等技术以提高效率。提出统一的效用评分函数，将经验损失、预测熵和特征空间多样性结合，便于主动学习流程的整合。

Result: 评估显示 PrivaDE 在百万参数级模型的在线评估时间约在 15 分钟内，具备有效的数据评估能力和可行的实际部署性能。

Conclusion: 为去中心化机器学习生态中的公平自动化数据市场奠定基础，推动在隐私保护前提下的数据交易和数据集选择。

Abstract: Evaluating the relevance of data is a critical task for model builders
seeking to acquire datasets that enhance model performance. Ideally, such
evaluation should allow the model builder to assess the utility of candidate
data without exposing proprietary details of the model. At the same time, data
providers must be assured that no information about their data - beyond the
computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for
privacy-preserving utility scoring and selection of data for machine learning.
While prior works have proposed data evaluation protocols, our approach
advances the state of the art through a practical, blockchain-centric design.
Leveraging the trustless nature of blockchains, PrivaDE enforces
malicious-security guarantees and ensures strong privacy protection for both
models and datasets. To achieve efficiency, we integrate several techniques -
including model distillation, model splitting, and cut-and-choose
zero-knowledge proofs - bringing the runtime to a practical level. Furthermore,
we propose a unified utility scoring function that combines empirical loss,
predictive entropy, and feature-space diversity, and that can be seamlessly
integrated into active-learning workflows. Evaluation shows that PrivaDE
performs data evaluation effectively, achieving online runtimes within 15
minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in
decentralized machine learning ecosystems.

</details>


### [106] [RESCUE: Retrieval Augmented Secure Code Generation](https://arxiv.org/abs/2510.18204)
*Jiahao Shi,Tianyi Zhang*

Main category: cs.CR

TL;DR: RESCUE 是一个用于安全代码生成的检索增强生成（RAG）框架，通过混合知识库蒸馏与程序切片的协同构建，以及分层多维检索，显著提升安全代码生成的性能，设定新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在生成安全代码时的脆弱性问题：传统RAG对原始安全文档的噪声敏感，且任务描述中隐含的安全语义易被忽略。

Method: 提出两大创新：1) 混合知识库构建：基于LLM辅助的聚类-再摘要蒸馏结合程序切片，生成高层安全指南与简洁的安全示例；2) 分层多面检索：自上而下遍历知识库，在每一层整合多条安全关键事实，以确保全面且准确的检索。

Result: 在四个基准上比较六个LLMs与五种最先進安全代码生成方法，RESCUE 显著提升 SecurePass@1 平均4.8点，达到安全领域的新SOTA，并通过消融研究验证各组件的重要性。

Conclusion: 通过消除原始安全文档的噪声并融合显式的安全知识，RESCUE 有效提升安全代码生成的鲁棒性与准确性，所提出的知识库蒸馏与分层检索策略具有潜在的推广性。

Abstract: Despite recent advances, Large Language Models (LLMs) still generate
vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to
enhance LLMs for secure code generation by incorporating external security
knowledge. However, the conventional RAG design struggles with the noise of raw
security-related documents, and existing retrieval methods overlook the
significant security semantics implicitly embedded in task descriptions. To
address these issues, we propose RESCUE, a new RAG framework for secure code
generation with two key innovations. First, we propose a hybrid knowledge base
construction method that combines LLM-assisted cluster-then-summarize
distillation with program slicing, producing both high-level security
guidelines and concise, security-focused code examples. Second, we design a
hierarchical multi-faceted retrieval to traverse the constructed knowledge base
from top to bottom and integrates multiple security-critical facts at each
hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated
RESCUE on four benchmarks and compared it with five state-of-the-art secure
code generation methods on six LLMs. The results demonstrate that RESCUE
improves the SecurePass@1 metric by an average of 4.8 points, establishing a
new state-of-the-art performance for security. Furthermore, we performed
in-depth analysis and ablation studies to rigorously validate the effectiveness
of individual components in RESCUE.

</details>


### [107] [CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments](https://arxiv.org/abs/2510.18324)
*Gyeonghoon Park,Jaehan Kim,Jinu Choi,Jinwoo Kim*

Main category: cs.CR

TL;DR: CryptoGuard将轻量级的基于系统调用的监控与两阶段深度学习检测相结合，并辅以基于eBPF的本地主机修复，实现对Linux云环境中加密货币挖矿恶意软件的可扩展检测与修复，且性能开销极低。


<details>
  <summary>Details</summary>
Motivation: Linux云环境中基于主机的加密矿工恶意软件隐蔽性强、造成的财务损失巨大，现有方案在可扩展性、对混淆行为的检测精度，以及集成修复方面存在不足。

Method: 采用基于 sketch 和滑动窗口的系统调用监控，尽量降低开销；将分类任务分解为两阶段过程，使用深度学习模型在两个阶段实现高精度的可疑行为识别；结合针对绕过检测的攻击（入口点污染、PID操作）的修复机制，基于eBPF实现可部署于任意兼容主机的修复。

Result: 在123个真实样本上评估，两个阶段的平均F1分数分别为96.12%和92.26%；在真阳性/假阳性方面优于先进基线；每主机CPU开销仅0.06%。

Conclusion: CryptoGuard实现了可扩展、低开销的主机级检测与修复，针对入口点污染及PID操控等对手技的规避有针对性，适合在Linux云部署。

Abstract: Host-based cryptomining malware, commonly known as cryptojackers, have gained
notoriety for their stealth and the significant financial losses they cause in
Linux-based cloud environments. Existing solutions often struggle with
scalability due to high monitoring overhead, low detection accuracy against
obfuscated behavior, and lack of integrated remediation. We present
CryptoGuard, a lightweight hybrid solution that combines detection and
remediation strategies to counter cryptojackers. To ensure scalability,
CryptoGuard uses sketch- and sliding window-based syscall monitoring to collect
behavior patterns with minimal overhead. It decomposes the classification task
into a two-phase process, leveraging deep learning models to identify
suspicious activity with high precision. To counter evasion techniques such as
entry point poisoning and PID manipulation, CryptoGuard integrates targeted
remediation mechanisms based on eBPF, a modern Linux kernel feature deployable
on any compatible host. Evaluated on 123 real-world cryptojacker samples, it
achieves average F1-scores of 96.12% and 92.26% across the two phases, and
outperforms state-of-the-art baselines in terms of true and false positive
rates, while incurring only 0.06% CPU overhead per host.

</details>


### [108] [Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption](https://arxiv.org/abs/2510.18333)
*Yepeng Liu,Xuandong Zhao,Dawn Song,Gregory W. Wornell,Yuheng Bu*

Main category: cs.CR

TL;DR: 要实现对LLM水印的实际应用，需实现利益相关方的激励对齐。以“情境内水印” ICW 为例，展示在可信方场景下的可行性，并提出面向领域的激励驱动设计原则与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 认为现实世界对LLM水印的部署受制于提供者、平台与最终用户之间的激励错位，主要表现为竞争风险、检测工具治理、鲁棒性与溯源等四大障碍。

Method: 从激励角度重新审视三类水印方法（模型水印、文本水印、情境内水印），分析它们在提供者、平台、用户三方的利益与风险，探讨在特定领域内的设计原则并提出未来研究方向。

Result: 结论显示，情境内水印(ICW)因能嵌入到可信方的流程中而在激励对齐方面具备潜力；模型水印在开源生态中遇到新挑战，文本水印在仅作为反滥用工具时收益有限；应在特定领域内推动面向域的水印设计和社区参与以促进实际落地。

Conclusion: 要实现LLM水印的广泛应用，需要在目标应用领域实现利益相关方的激励对齐，并推动领域驱动、社区参与的研究与应用，ICW提供了一个可行的示例。

Abstract: Despite progress in watermarking algorithms for large language models (LLMs),
real-world deployment remains limited. We argue that this gap stems from
misaligned incentives among LLM providers, platforms, and end users, which
manifest as four key barriers: competitive risk, detection-tool governance,
robustness concerns and attribution issues. We revisit three classes of
watermarking through this lens. \emph{Model watermarking} naturally aligns with
LLM provider interests, yet faces new challenges in open-source ecosystems.
\emph{LLM text watermarking} offers modest provider benefit when framed solely
as an anti-misuse tool, but can gain traction in narrowly scoped settings such
as dataset de-contamination or user-controlled provenance. \emph{In-context
watermarking} (ICW) is tailored for trusted parties, such as conference
organizers or educators, who embed hidden watermarking instructions into
documents. If a dishonest reviewer or student submits this text to an LLM, the
output carries a detectable watermark indicating misuse. This setup aligns
incentives: users experience no quality loss, trusted parties gain a detection
tool, and LLM providers remain neutral by simply following watermark
instructions. We advocate for a broader exploration of incentive-aligned
methods, with ICW as an example, in domains where trusted parties need reliable
tools to detect misuse. More broadly, we distill design principles for
incentive-aligned, domain-specific watermarking and outline future research
directions. Our position is that the practical adoption of LLM watermarking
requires aligning stakeholder incentives in targeted application domains and
fostering active community engagement.

</details>


### [109] [DeepTx: Real-Time Transaction Risk Analysis via Multi-Modal Features and LLM Reasoning](https://arxiv.org/abs/2510.18438)
*Yixuan Liu,Xinlei Li,Yi Li*

Main category: cs.CR

TL;DR: Real-time phishing detection for Web3 transactions using simulated pending transactions, multi-LLM reasoning, and consensus with self-reflection to provide explainable decisions before user confirmation.


<details>
  <summary>Details</summary>
Motivation: Web3 phishing attacks are increasingly sophisticated, exploiting contract logic, frontend scripts, and token-approval patterns; there is a need to detect malicious intent before users confirm transactions.

Method: DeepTx simulates pending transactions, extracts behavioral, contextual, and UI features, and employs multiple large language models (LLMs) to infer transaction intent. A consensus mechanism with self-reflection aggregates explanations to produce robust, explainable decisions.

Result: On a phishing dataset, DeepTx achieves high precision and recall (demo video provided).

Conclusion: DeepTx demonstrates effective, proactive defense against Web3 phishing by combining transaction simulation, multimodal features, and explainable multi-LLM reasoning with self-reflective consensus.

Abstract: Phishing attacks in Web3 ecosystems are increasingly sophisticated,
exploiting deceptive contract logic, malicious frontend scripts, and token
approval patterns. We present DeepTx, a real-time transaction analysis system
that detects such threats before user confirmation. DeepTx simulates pending
transactions, extracts behavior, context, and UI features, and uses multiple
large language models (LLMs) to reason about transaction intent. A consensus
mechanism with self-reflection ensures robust and explainable decisions.
Evaluated on our phishing dataset, DeepTx achieves high precision and recall
(demo video: https://youtu.be/4OfK9KCEXUM).

</details>


### [110] [PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks](https://arxiv.org/abs/2510.18465)
*Spencer King,Irfan Ozen,Karthika Subramani,Saranyan Senthivel,Phani Vadrevu,Roberto Perdisci*

Main category: cs.CR

TL;DR: PP3D是一款端到端的浏览器端防御框架，用于实时发现、检测和防护基于网页的行为操纵型社会工程攻击，具备高检测率、低误报、良好延迟，并对新样本具备鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 网页行为操控攻击（BMAs）利用人类决策漏洞且研究不足，较 phishing 等信息窃取与 malware 感染缺乏通用防御；需要一个隐私保护、跨设备且可扩展的防御方案。

Method: 在浏览器扩展中部署基于可视化的检测模型，前端端到端地在客户端运行，以保护桌面和移动设备并保护用户隐私。

Result: 在实验中，PP3D达到>99%的检测率，1%误报率；在跨设备上具有良好延迟与开销。即便面对训练数月后的新样本，仍能达到>97%的检测率、1%的误报率。

Conclusion: 该框架对广泛且不断演变的网页行为操控类攻击提供了一种实用、有效且具有通用性的防御方案。

Abstract: Web-based behavior-manipulation attacks (BMAs) - such as scareware, fake
software downloads, tech support scams, etc. - are a class of social
engineering (SE) attacks that exploit human decision-making vulnerabilities.
These attacks remain under-studied compared to other attacks such as
information harvesting attacks (e.g., phishing) or malware infections. Prior
technical work has primarily focused on measuring BMAs, offering little in the
way of generic defenses.
  To address this gap, we introduce Pixel Patrol 3D (PP3D), the first
end-to-end browser framework for discovering, detecting, and defending against
behavior-manipulating SE attacks in real time. PP3D consists of a visual
detection model implemented within a browser extension, which deploys the model
client-side to protect users across desktop and mobile devices while preserving
privacy.
  Our evaluation shows that PP3D can achieve above 99% detection rate at 1%
false positives, while maintaining good latency and overhead performance across
devices. Even when faced with new BMA samples collected months after training
the detection model, our defense system can still achieve above 97% detection
rate at 1% false positives. These results demonstrate that our framework offers
a practical, effective, and generalizable defense against a broad and evolving
class of web behavior-manipulation attacks.

</details>


### [111] [The Attribution Story of WhisperGate: An Academic Perspective](https://arxiv.org/abs/2510.18484)
*Oleksandr Adamov,Anders Carlsson*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper explores the challenges of cyberattack attribution, specifically
APTs, applying the case study approach for the WhisperGate cyber operation of
January 2022 executed by the Russian military intelligence service (GRU) and
targeting Ukrainian government entities. The study provides a detailed review
of the threat actor identifiers and taxonomies used by leading cybersecurity
vendors, focusing on the evolving attribution from Microsoft, ESET, and
CrowdStrike researchers. Once the attribution to Ember Bear (GRU Unit 29155) is
established through technical and intelligence reports, we use both traditional
machine learning classifiers and a large language model (ChatGPT) to analyze
the indicators of compromise (IoCs), tactics, and techniques to statistically
and semantically attribute the WhisperGate attack. Our findings reveal
overlapping indicators with the Sandworm group (GRU Unit 74455) but also strong
evidence pointing to Ember Bear, especially when the LLM is fine-tuned or
contextually augmented with additional intelligence. Thus, showing how AI/GenAI
with proper fine-tuning are capable of solving the attribution challenge.

</details>


### [112] [One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection](https://arxiv.org/abs/2510.18493)
*Kangzhong Wang,Zitong Shen,Youqian Zhang,Michael MK Cheung,Xiapu Luo,Grace Ngai,Eugene Yujun Fu*

Main category: cs.CR

TL;DR: 提出 MASK（模块化自适应降敏工具）框架，用于在保持隐私的前提下通过可定制的降敏策略进行电话诈骗检测中的大语言模型应用；框架支持不同隐私偏好的插件化降敏方法，并探讨未来的建模与损失函数设计以实现个性化的隐私感知检测。


<details>
  <summary>Details</summary>
Motivation: 电话通话转写信息中常含敏感个人信息，直接送入第三方服务端会带来隐私风险；需要在隐私保护与检测效果之间取得平衡，并实现个性化策略。

Method: 提出 MASK 框架：可插拔、可训练的降敏组件，形成模块化架构以实现基于个人偏好的动态隐私控制；支持从关键词降敏等高隐私策略到面向精度的神经降敏等多种方法；讨论未来可采用的建模思路与损失函数设计以实现隐私感知的个性化检测。

Result: 本工作提出一种概念性框架及设计思路，尚未给出实验结果；重点在于对可插拔降敏组件的架构、工作流程与评估方向进行描述。

Conclusion: MASK 有望通过模块化降敏实现隐私感知的 LLM 基础检测系统，在提升用户信任的同时维持检测效果，且可扩展到手机诈骗以外的场景。

Abstract: Phone scams remain a pervasive threat to both personal safety and financial
security worldwide. Recent advances in large language models (LLMs) have
demonstrated strong potential in detecting fraudulent behavior by analyzing
transcribed phone conversations. However, these capabilities introduce notable
privacy risks, as such conversations frequently contain sensitive personal
information that may be exposed to third-party service providers during
processing. In this work, we explore how to harness LLMs for phone scam
detection while preserving user privacy. We propose MASK (Modular Adaptive
Sanitization Kit), a trainable and extensible framework that enables dynamic
privacy adjustment based on individual preferences. MASK provides a pluggable
architecture that accommodates diverse sanitization methods - from traditional
keyword-based techniques for high-privacy users to sophisticated neural
approaches for those prioritizing accuracy. We also discuss potential modeling
approaches and loss function designs for future development, enabling the
creation of truly personalized, privacy-aware LLM-based detection systems that
balance user trust and detection effectiveness, even beyond phone scam context.

</details>


### [113] [Prompting the Priorities: A First Look at Evaluating LLMs for Vulnerability Triage and Prioritization](https://arxiv.org/abs/2510.18508)
*Osama Al Haddad,Muhammad Ikram,Ejaz Ahmed,Young Lee*

Main category: cs.CR

TL;DR: 研究评估四种大语言模型在多种 prompting 技巧下对漏洞信息的解释及对 SSVC 框架各决策点的预测，结果显示 Gemini 领先但仍需人类专家判断，LLMs 可在特定场景辅助漏洞优先化。


<details>
  <summary>Details</summary>
Motivation: 在漏洞处理工作流中，面临大规模、复杂的漏洞积压，需要自动化的解释与信息提取来辅助优先级决策，LLMs 可能提升效率与决策质量。

Method: 对四个模型（ChatGPT、Claude、Gemini、DeepSeek）在十二种 prompting 技巧下，对半结构化与非结构化漏洞信息进行解释，预测 SSVC 框架的 Exploitation、Automatable、Technical Impact、Mission and Wellbeing 四个决策点。使用 VulZoo 数据集的 384 条真实漏洞，发出超过 165,000 次查询，比较 one-shot、few-shot、chain-of-thought 等 prompting 风格。

Result: Gemini 在四个决策点中领先，在三个决策点获胜并给出最正确的建议；在最终 SSVC 决策中，DeepSeek 在加权与非加权的一致性方面达到“公平/可接受”的水平，其它模型普遍存在过度预测风险的问题。引入示例提示（exemplars）通常能提升准确性，但某些决策点上仍有挑战。

Conclusion: 当前的大语言模型尚不能替代专家判断，但在特定的模型-提示组合下对部分 SSVC 决策具有中等效果，可用于支持漏洞优先级工作流并帮助安全团队更高效地对新威胁做出响应。

Abstract: Security analysts face increasing pressure to triage large and complex
vulnerability backlogs. Large Language Models (LLMs) offer a potential aid by
automating parts of the interpretation process. We evaluate four models
(ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to
interpret semi-structured and unstructured vulnerability information. As a
concrete use case, we test each model's ability to predict decision points in
the Stakeholder-Specific Vulnerability Categorization (SSVC) framework:
Exploitation, Automatable, Technical Impact, and Mission and Wellbeing.
  Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more
than 165,000 queries to assess performance under prompting styles including
one-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC
decision point and Cohen's kappa (weighted and unweighted) for the final SSVC
decision outcomes. Gemini consistently ranked highest, leading on three of four
decision points and yielding the most correct recommendations. Prompting with
exemplars generally improved accuracy, although all models struggled on some
decision points. Only DeepSeek achieved fair agreement under weighted metrics,
and all models tended to over-predict risk.
  Overall, current LLMs do not replace expert judgment. However, specific LLM
and prompt combinations show moderate effectiveness for targeted SSVC
decisions. When applied with care, LLMs can support vulnerability
prioritization workflows and help security teams respond more efficiently to
emerging threats.

</details>


### [114] [Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain](https://arxiv.org/abs/2510.18568)
*Behnam Rezaei Bezanjani,Seyyed Hamid Ghafouri,Reza Gholamrezaei*

Main category: cs.CR

TL;DR: 提出一个三阶段的IoT医疗安全框架，结合信誉估计、区块链+轻量PoW与轻量LSTM实现实时威胁检测，提升精确度、召回率等指标。


<details>
  <summary>Details</summary>
Motivation: 在物联网医疗中，资源受限、异构性及实时性需求使传统安全难以满足，需要各阶段协同提升设备信任、数据不可篡改性和威胁检测能力。

Method: 阶段1：基于信誉的信任评估，融合行为分析与链下存储以提高扩展性；阶段2：将区块链与轻量化PoW结合，确保数据不可篡改和授权保护；阶段3：用轻量级LSTM进行实时异常检测和分类。

Result: 仿真结果显示在精确度、准确性、召回率上提升约2%，攻击检测率提升5%，误警率下降约3%，在可扩展性和实时性能方面具有优势。

Conclusion: 该框架能有效解决IoT医疗安全中的核心问题，并且在扩展性和实时性方面具备良好表现，但需在实际部署中考虑资源成本、互操作性和隐私保护等挑战。

Abstract: The integration of Internet of Things (IoT) devices in healthcare has
revolutionized patient care by enabling real-time monitoring, personalized
treatments, and efficient data management. However, this technological
advancement introduces significant security risks, particularly concerning the
confidentiality, integrity, and availability of sensitive medical data.
Traditional security measures are often insufficient to address the unique
challenges posed by IoT environments, such as heterogeneity, resource
constraints, and the need for real-time processing. To tackle these challenges,
we propose a comprehensive three-phase security framework designed to enhance
the security and reliability of IoT-enabled healthcare systems. In the first
phase, the framework assesses the reliability of IoT devices using a
reputation-based trust estimation mechanism, which combines device behavior
analytics with off-chain data storage to ensure scalability. The second phase
integrates blockchain technology with a lightweight proof-of-work mechanism,
ensuring data immutability, secure communication, and resistance to
unauthorized access. The third phase employs a lightweight Long Short-Term
Memory (LSTM) model for anomaly detection and classification, enabling
real-time identification of cyber threats. Simulation results demonstrate that
the proposed framework outperforms existing methods, achieving a 2% increase in
precision, accuracy, and recall, a 5% higher attack detection rate, and a 3%
reduction in false alarm rate. These improvements highlight the framework's
ability to address critical security concerns while maintaining scalability and
real-time performance.

</details>


### [115] [Evaluating Large Language Models in detecting Secrets in Android Apps](https://arxiv.org/abs/2510.18601)
*Marco Alecci,Jordan Samhi,Tegawendé F. Bissyandé,Jacques Klein*

Main category: cs.CR

TL;DR: SecretLoc uses a large language model to detect hardcoded secrets in Android apps, surpassing pattern-based methods and uncovering numerous unseen secret types, validated on benchmarks and real Play Store apps.


<details>
  <summary>Details</summary>
Motivation: Mobile apps often embed credentials (API keys, tokens, client IDs) that can be reverse-engineered, leading to security risks and financial loss. Traditional approaches (regex, static analysis, ML) rely on predefined patterns or labeled data and may miss unknown secrets.

Method: Propose SecretLoc, an LLM-based approach that leverages contextual and structural cues to identify secrets without predefined patterns or labeled training data. Evaluated on a literature benchmark and extended to newly crawled Google Play apps, comparing against regex, static analysis, and ML baselines.

Result: SecretLoc detected 4828 secrets missed by existing methods and identified over 10 new types of secrets (e.g., OpenAI API keys, GitHub access tokens, RSA private keys, JWTs). In a set of 5000 apps, secrets were found in 2124 apps (42.5%), with several remediated by developers after disclosure.

Conclusion: The dual-use risk of LLM-enabled analysis is highlighted: if analysts can uncover secrets, attackers can too. This underscores the urgent need for proactive secret management and stronger mitigation across the mobile ecosystem.

Abstract: Mobile apps often embed authentication secrets, such as API keys, tokens, and
client IDs, to integrate with cloud services. However, developers often
hardcode these credentials into Android apps, exposing them to extraction
through reverse engineering. Once compromised, adversaries can exploit secrets
to access sensitive data, manipulate resources, or abuse APIs, resulting in
significant security and financial risks. Existing detection approaches, such
as regex-based analysis, static analysis, and machine learning, are effective
for identifying known patterns but are fundamentally limited: they require
prior knowledge of credential structures, API signatures, or training data.
  In this paper, we propose SecretLoc, an LLM-based approach for detecting
hardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it
leverages contextual and structural cues to identify secrets without relying on
predefined patterns or labeled training sets. Using a benchmark dataset from
the literature, we demonstrate that SecretLoc detects secrets missed by regex-,
static-, and ML-based methods, including previously unseen types of secrets. In
total, we discovered 4828 secrets that were undetected by existing approaches,
discovering more than 10 "new" types of secrets, such as OpenAI API keys,
GitHub Access Tokens, RSA private keys, and JWT tokens, and more.
  We further extend our analysis to newly crawled apps from Google Play, where
we uncovered and responsibly disclosed additional hardcoded secrets. Across a
set of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which
were confirmed and remediated by developers after we contacted them. Our
results reveal a dual-use risk: if analysts can uncover these secrets with
LLMs, so can attackers. This underscores the urgent need for proactive secret
management and stronger mitigation practices across the mobile ecosystem.

</details>


### [116] [DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining](https://arxiv.org/abs/2510.18612)
*Muhammad Hassan,Maria Mushtaq,Jaan Raik,Tara Ghasempouri*

Main category: cs.CR

TL;DR: 提出一种基于统计预处理和关联规则挖掘的新型RISC-V微架构攻击检测方法，利用gem5仿真实现可重配置的检测，能够对包括flush+fault在内的多种变体进行泛化检测，在多类工作负载下提升检测指标并具备可解释性。


<details>
  <summary>Details</summary>
Motivation: RISC-V在关键应用中的普及伴随对微架构侧信道攻击的易感性，现有基于ML的检测在可泛化性、实用性和可解释性方面存在不足。需要一种可在不同攻击变体下泛化、且结果可解释的检测方法。

Method: 在gem5环境下进行仿真，提出将统计预处理与关联规则挖掘相结合的检测框架，并具备重新配置能力以适应新型attack变体；对cryptographic、computational、memory-intensive等工作负载进行评估，与现有方法比较以验证性能提升。

Result: 在准确率、精确度、召回率方面分别实现最高提升约5.15%、7%、3.91%，且方法具备检测新变体的灵活性；由于检测逻辑基于人类可解释的关联规则，能够提供对攻击和正常应用微架构行为的直观理解。

Conclusion: 所提出的方法在检测RISC-V微架构攻击方面实现了性能提升并提高了可解释性，具备良好的泛化性，显示出对未来新型攻击变体的适应能力。

Abstract: RISC-V processors are becoming ubiquitous in critical applications, but their
susceptibility to microarchitectural side-channel attacks is a serious concern.
Detection of microarchitectural attacks in RISC-V is an emerging research topic
that is relatively underexplored, compared to x86 and ARM. The first line of
work to detect flush+fault-based microarchitectural attacks in RISC-V leverages
Machine Learning (ML) models, yet it leaves several practical aspects that need
further investigation. To address overlooked issues, we leveraged gem5 and
propose a new detection method combining statistical preprocessing and
association rule mining having reconfiguration capabilities to generalize the
detection method for any microarchitectural attack. The performance comparison
with state-of-the-art reveals that the proposed detection method achieves up to
5.15% increase in accuracy, 7% rise in precision, and 3.91% improvement in
recall under the cryptographic, computational, and memory-intensive workloads
alongside its flexibility to detect new variant of flush+fault attack.
Moreover, as the attack detection relies on association rules, their
human-interpretable nature provides deep insight to understand
microarchitectural behavior during the execution of attack and benign
applications.

</details>


### [117] [Qatsi: Stateless Secret Generation via Hierarchical Memory-Hard Key Derivation](https://arxiv.org/abs/2510.18614)
*René Coignard,Anton Rygin*

Main category: cs.CR

TL;DR: Qatsi 是一种分层密钥派生方案，基于 Argon2id，能够在无需持续存储的情况下从单一高熵主密钥和上下文层中可重现地产生机密信息。通过内存密集型参数实现103-312位熵，提供对7776词助记符或90字符密码的均匀拒绝采样。实现可证明的输出均匀性，并对GPU攻击成本进行量化。


<details>
  <summary>Details</summary>
Motivation: 消除基于保险箱的攻击面，改用确定性派生以实现无状态、可重现的密钥和凭证，从而提升安全性和简化运维，尤其在与离线/空气隔离环境协作时。

Method: 采用 Argon2id 的分层结构，内存消耗64-128 MiB、迭代16-32次，结合可证明均匀性的拒绝采样以覆盖7776词助记符或90字符密码。给出形式化证明、GPU 攻击成本评估（对80位主密钥在单GPU上的 Paranoid 参数约2.4×10^16 年的成本），在 Rust 实现中实现内存零化、词表完整性的编译期校验以及全面的测试覆盖。参考在 Apple M1 Pro 上的基准显示：-standard 模式544 ms，paranoid 模式2273 ms 的单层派生。

Result: 输出具有103-312位熵，且在形式化分析中证明输出均匀性。对 80 位主密钥、单 GPU 攻击在 Paranoid 参数下的成本估计为约2.4×10^16 年。实现提供自动内存清零、编译期词表校验和全面测试。基准显示在 Apple M1 Pro 上，单层派生分别为 Standard 方案544 ms、Paranoid 方案2273 ms。

Conclusion: Qatsi 面向对无状态可重现性有高要求的场景（如空气隔离系统的主凭证生成），在牺牲一定的灵活轮换性下提升安全性与可操作性。其分层、内存硬化派生的设计提供强大的攻击成本和输出均匀性保证。

Abstract: We present Qatsi, a hierarchical key derivation scheme using Argon2id that
generates reproducible cryptographic secrets without persistent storage. The
system eliminates vault-based attack surfaces by deriving all secrets
deterministically from a single high-entropy master secret and contextual
layers. Outputs achieve 103-312 bits of entropy through memory-hard derivation
(64-128 MiB, 16-32 iterations) and provably uniform rejection sampling over
7776-word mnemonics or 90-character passwords. We formalize the hierarchical
construction, prove output uniformity, and quantify GPU attack costs: $2.4
\times 10^{16}$ years for 80-bit master secrets on single-GPU adversaries under
Paranoid parameters (128 MiB memory). The implementation in Rust provides
automatic memory zeroization, compile-time wordlist integrity verification, and
comprehensive test coverage. Reference benchmarks on Apple M1 Pro (2021)
demonstrate practical usability with 544 ms Standard mode and 2273 ms Paranoid
mode single-layer derivations. Qatsi targets air-gapped systems and master
credential generation where stateless reproducibility outweighs rotation
flexibility.

</details>


### [118] [Exploring Membership Inference Vulnerabilities in Clinical Large Language Models](https://arxiv.org/abs/2510.18674)
*Alexander Nemecek,Zebin Yun,Zahra Rahmani,Yaniv Harel,Vipin Chaudhary,Mahmood Sharif,Erman Ayday*

Main category: cs.CR

TL;DR: 在临床LLMs上的成员推断攻击初步研究，揭示有限但可测的隐私泄露风险，强调需要领域特定的隐私评估与防御。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在临床决策、文档和患者信息系统中的广泛使用，确保隐私与可信度成为迫切挑战；对使用EHR数据微调的模型隐私风险缺乏充分评估。

Method: 对最先进的临床问答模型Llemr进行成员推断攻击，比较基于损失的canonical攻击和更贴近临床对手场景的改写扰动策略；进行实验评估。

Result: 初步发现存在有限但可测的成员泄露，表明当前临床LLMs具备部分鲁棒性但仍易受到微妙隐私风险的攻击。

Conclusion: 需要继续发展面向领域的隐私评估和防御方法，如差分隐私微调和对改写的对抗训练，以提升医疗AI系统的安全性和可信度。

Abstract: As large language models (LLMs) become progressively more embedded in
clinical decision-support, documentation, and patient-information systems,
ensuring their privacy and trustworthiness has emerged as an imperative
challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic
health record (EHR) data improves domain alignment but also raises the risk of
exposing patient information through model behaviors. In this work-in-progress,
we present an exploratory empirical study on membership inference
vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if
specific patient records were used during model training. Using a
state-of-the-art clinical question-answering model, Llemr, we evaluate both
canonical loss-based attacks and a domain-motivated paraphrasing-based
perturbation strategy that more realistically reflects clinical adversarial
conditions. Our preliminary findings reveal limited but measurable membership
leakage, suggesting that current clinical LLMs provide partial resistance yet
remain susceptible to subtle privacy risks that could undermine trust in
clinical AI adoption. These results motivate continued development of
context-aware, domain-specific privacy evaluations and defenses such as
differential privacy fine-tuning and paraphrase-aware training, to strengthen
the security and trustworthiness of healthcare AI systems.

</details>
