<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.IT](#cs.IT) [Total: 4]
- [cs.LG](#cs.LG) [Total: 86]
- [eess.SY](#eess.SY) [Total: 9]
- [eess.SP](#eess.SP) [Total: 16]
- [cs.CR](#cs.CR) [Total: 11]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models](https://arxiv.org/abs/2510.24242)
*Zihan Li,Jiahao Yang,Yuxin Zhang,Zhe Chen,Yue Gao*

Main category: cs.NI

TL;DR: Grace 提出卫星-地面协同框架以实现近实时 LVLM 推理，前端在卫星部署紧凑模型，后端在地面部署更大模型，通过异步 RAG 和基于置信度的任务分发实现端到端高效推理，并显著降低时延且保持准确度。


<details>
  <summary>Details</summary>
Motivation: 受限的卫星端计算资源与短暂的卫星-ground 联系限制，使得大规模视觉-语言模型在实际卫星系统中的实际部署仍具挑战，同时希望充分挖掘 LVLM 在遥感任务（如灾害监测）中的潜力。

Method: 提出 Grace 的两阶段异步卫星-GS Retrieval-Augmented Generation（RAG）框架与任务分发算法：1) 将 GS 的知识档案以自适应更新算法在有限的数据交换期内更新并同步到卫星档案；2) 提出基于置信度的任务分发策略，决定任务在卫星端本地推理还是下放到 GS；卫星端部署紧凑 LVLM，GS 部署较大 LVLM，确保端到端性能。

Result: 基于真实卫星轨道数据的广泛实验表明，Grace 相较于现有最先进方法在平均时延上降低了 76-95%，且推理精度未受影响。

Conclusion: 卫星端与地面端模型协同的分层架构能显著提升低轨卫星遥感任务的近实时 LVLM 推理能力，适用于资源受限且需快速响应的应用场景。

Abstract: Large vision-language models (LVLMs) have recently demonstrated great
potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by
low Earth orbit (LEO) satellites. However, their deployment in real-world LEO
satellite systems remains largely unexplored, hindered by limited onboard
computing resources and brief satellite-ground contacts. We propose Grace, a
satellite-ground collaborative system designed for near-realtime LVLM inference
in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime
inference, but larger ones on ground stations (GSs) to guarantee end-to-end
performance. Grace is comprised of two main phases that are asynchronous
satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch
algorithm. Firstly, we still the knowledge archive of GS RAG to satellite
archive with tailored adaptive update algorithm during limited satellite-ground
data exchange period. Secondly, propose a confidence-based test algorithm that
either processes the task onboard the satellite or offloads it to the GS.
Extensive experiments based on real-world satellite orbital data show that
Grace reduces the average latency by 76-95% compared to state-of-the-art
methods, without compromising inference accuracy.

</details>


### [2] [A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels](https://arxiv.org/abs/2510.24595)
*Azadeh Pourkabirian,Kai Li,Photios A. Stavrou,Wei Ni*

Main category: cs.NI

TL;DR: 提出一种基于数字与模拟混合预编码的新方法，考虑角度与相位的相关性及其联合熵，用于MU-MIMO的混合预编码，提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统混合预编码常忽略角度与相位之间的相关性。通过将角度和相位建模为相关变量并引入联合角-相位熵，可更准确刻画信道变化，从而实现自适应且鲁棒的预编码。

Method: 将数字与模拟预编码相结合，对信号在多天线系统中按特定方向进行定向，同时将角度和相位建模为相关变量，假设服从双变量高斯分布，并定义联合角-相位熵以衡量不确定性，并据此引导预编码自适应调整。

Result: 仿真结果显示，与现有方法相比，和通信速率提升约18.31%，鲁棒性提升约11.47%。

Conclusion: 将联合角-相位熵用于混合预编码，可提升MU‑MIMO系统的和速率与鲁棒性，同时为角相相关信道建模提供新思路。

Abstract: Hybrid precoding is an indispensable technique to harness the full potential
of a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In
this paper, we propose a new hybrid precoding approach that combines digital
and analog precoding to optimize data transmission over multiple antennas. This
approach steers signals in specific directions, leading to maximizing sum-rate
and suppressing side-lobe interference. When dealing with complex signals,
changes in phase are naturally associated with changes in angle, and these
variations are inherently correlated. The correlation between the angle and
phase is essential for accurately determining the channel characteristics. An
important aspect of this approach is that we model the angle and phase as
correlated variables following a bivariate Gaussian distribution, and for the
first time, we define a joint angle and phase entropy to measure the
uncertainty of angle and phase variations in wireless channels. This entropy is
crucial to adapt the proposed precoding method with variations. Simulation
result validate the accuracy of our analytical findings, demonstrating 18.31%
increase in sum-rate and an 11.47% improvement in robustness compared to other
state-of-the-art methods.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [3] [Flexible Intelligent Layered Metasurfaces for Downlink Multi-user MISO Communications](https://arxiv.org/abs/2510.24190)
*Hong Niu,Jiancheng An,Chau Yuen*

Main category: cs.IT

TL;DR: 提出两层柔性可控透镜层的FILM结构，通过可变传输矩阵实现比七层SIM更高效的波域信号处理，显著提升系统容量和BER性能，且计算复杂度可控。


<details>
  <summary>Details</summary>
Motivation: 解决传统SIM对层间间距固定、深层堆叠导致功率衰减和成本高的问题，寻求更少层数但保持处理能力的灵活结构。

Method: 设计两层柔性透镜阵列，提出FILM架构；将其应用于MU-MISO系统，建立FILM诱导信道与目标信道的拟合问题；使用交替优化法求解，包括相位偏移的解析更新和形状优化的梯度下降。

Result: 通过仿真，传输FILM在总和速率方面比传统七层SIM提升超过200%，BER提升超过7 dB；并给出对总和速率的上界分析和计算复杂度的讨论。

Conclusion: 柔性两层FILM在减小层数的同时保持或提升信号处理性能，为低成本高效波域信号处理提供了可行路径。

Abstract: Stacked intelligent metasurfaces (SIMs) have recently gained attention as a
paradigm for wave-domain signal processing with reduced reliance on costly
radio-frequency (RF) chains. However, conventional SIMs rely on uniform
inter-layer spacing and require deep stacking to ensure processing capability,
resulting in severe power attenuation in practice. To address this issue, we
propose a flexible intelligent layered metasurface (FILM) architecture
consisting of two shape-controllable flexible metasurface layers. By replacing
rigid metasurfaces with flexible ones in both layers, the transmission
coefficient matrix can be dynamically adjusted, significantly decreasing the
number of required layers while maintaining signal processing performance.
Firstly, we develop a two-layer FILM-assisted multi-user multiple-input
single-output (MU-MISO) system, wherein we formulate a channel fitting problem
aimed at reducing the difference between the FILM-induced and target channels.
Then, we solve this non-convex problem by employing an alternating optimization
(AO) method, featuring closed-form phase shift updates and a gradient
descent-based shape optimization. Furthermore, we analyze the upper bound on
sum-rate and the complexity of computation to provide insights into design
trade-offs. Finally, simulation results demonstrated that the proposed
transmissive FILM architecture achieves over 200\% improvement in sum-rate and
more than 7 dB bit-error rate (BER) gain compared to the conventional
seven-layer SIMs.

</details>


### [4] [What Can Be Recovered Under Sparse Adversarial Corruption? Assumption-Free Theory for Linear Measurements](https://arxiv.org/abs/2510.24215)
*Vishal Halder,Alexandre Reiffers-Masson,Abdeldjalil Aïssa-El-Bey,Gugan Thoppe*

Main category: cs.IT

TL;DR: 在任意的 A 和 q-稀疏对抗扰动下，能统一 recover 的最大信息等价于 x* + ker(U)，其中 U 投影到删除任意 2q 行后的 A 的子矩阵行空间的交集。对 y 与 A 的对比中，所有使 ℓ0 最小化的解都落在该等价类上，给出可构造的恢复路径。


<details>
  <summary>Details</summary>
Motivation: 尽管在满足如 RIP、稀疏性等结构性假设下的精确恢复有广泛研究，但在任意 A 与 x* 的场景中，统一可恢复的信息集合仍不清晰。本工作旨在给出在对抗性稀疏误差下“可 uniformly 恢复”的最大子空间的几何刻画。

Method: 建立以 y = Ax* + e，其中 e 为 q–稀疏对抗扰动为背景，考察通过删除 A 的任意 2q 行所得到子矩阵的行空间的交集。令 U 为该交集的唯一投影矩阵，研究在所有可能的 x 向量中，谁能在不知道 e 的情况下实现对 y 的一致恢复。证明最优的可 uniformly 恢复集合为 x* + ker(U)，并且任何使 y 与 Ax 的 ℓ0 距离最小化的 x 都落在此集合内，提供了一个构建性的恢复方案。

Result: 给出一个几何-代数的界定：在对抗性 2q 行删减下的行空间交集的投影核决定了不可避免的不确定性范围，即可 uniformly 恢复的集合为 x* 的一个等价类。

Conclusion: 本文给出了在任意 A 与 x* 情况下的最大可统一恢复信息的精确定义，并基于 ker(U) 提供了一个可操作的、对抗性扰动下的构造性恢复路径。

Abstract: Let \(\bm{A} \in \mathbb{R}^{m \times n}\) be an arbitrary, known matrix and
\(\bm{e}\) a \(q\)-sparse adversarial vector. Given \(\bm{y} = \bm{A} x^* +
\bm{e}\) and \(q\), we seek the smallest set containing \(x^*\)-hence the one
conveying maximal information about \(x^*\)-that is uniformly recoverable from
\(\bm{y}\) without knowing \(\bm{e}\). While exact recovery of \(x^*\) via
strong (and often impractical) structural assumptions on \(\bm{A}\) or \(x^*\)
(for example, restricted isometry, sparsity) is well studied, recoverability
for arbitrary \(\bm{A}\) and \(x^*\) remains open. Our main result shows that
the best that one can hope to recover is \(x^* + \ker(\bm{U})\), where
\(\bm{U}\) is the unique projection matrix onto the intersection of rowspaces
of all possible submatrices of \(\bm{A}\) obtained by deleting \(2q\) rows.
Moreover, we prove that every \(x\) that minimizes the \(\ell\_0\)-norm of
\(\bm{y} - \bm{A} x\) lies in \(x^* + \ker(\bm{U})\), which then gives a
constructive approach to recover this set.

</details>


### [5] [Joint Active and Passive Beamforming with Sensing-Assisted Discrete Phase Shifts for Dual-RIS ISAC Systems](https://arxiv.org/abs/2510.24480)
*Qing Xue,Yun Lan,Jiajia Guo,Qianbin Chen,Shaodan Ma*

Main category: cs.IT

TL;DR: 提出一种半被动的双RIS辅助ISAC系统，针对最大最小SINR问题，通过联动有源与被动波束成形并对RIS离散相位进行感知辅助约束搜索，在复杂度可控的前提下实现接近理想连续相位的性能，优于单RIS与传统离散搜索方法。


<details>
  <summary>Details</summary>
Motivation: 在6G场景下需提升集成感知与通信(ISAC)性能并兼顾用户公平性，同时通过双RIS提升角度估计和覆盖能力，降低离散相位搜索的计算开销。

Method: 采用双RIS进行用户角度估计以简化问题；提出交替优化算法求解联合有源与被动波束成形；发射波束形成子问题采用半定松弛(SDR)结合二分法求解；针对RIS离散相位，采用感知辅助约束以缩小搜索空间，并为不同RIS规模提出两种低复杂度搜索策略。

Result: 数值仿真表明所提算法的性能接近理想连续相位基准，优于传统离散相位优化算法，并显著优于单RIS系统。

Conclusion: 证实双RIS在ISAC中的潜力，提供了针对离散相位RIS的规模自适应搜索策略，在性能和计算复杂度之间取得良好折衷。

Abstract: Targeting the requirements of 6G, this paper investigates a semi-passive
dual-reconfigurable intelligent surface (RIS)-assisted integrated sensing and
communication (ISAC) system, tackling the max-min user
signal-to-interference-plus-noise ratio (SINR) problem via joint active and
passive beamforming to enhance system performance and ensure user fairness.
Addressing this challenge, we first utilize dual RISs for user angle estimation
to simplify the solution process of the formulated problem, an efficient
alternating optimization algorithm is then developed. Specifically,
semi-definite relaxation and the bisection method are employed to solve the
transmit beamforming optimization subproblem. For the RIS discrete phase
shifts, a sensing-assisted approach is adopted to constrain the optimization
search space, with two distinct low-complexity search strategies introduced for
different RIS sizes. Numerical simulation results demonstrate that the proposed
algorithm achieves performance close to the ideal continuous phase shift
benchmark, outperforms conventional discrete phase shift optimization
algorithms, and exhibits a significant improvement over single-RIS systems.

</details>


### [6] [Feedback Lunch: Deep Feedback Codes for Wiretap Channels](https://arxiv.org/abs/2510.16620)
*Yingyao Zhou,Natasha Devroye,Onur Günlü*

Main category: cs.IT

TL;DR: 在高斯线道（Gaussian wiretap channel）中，利用通道输出反馈，可以通过种子式模块化编码、通用哈希和学习驱动的可靠性编码实现正的保密速率，并可在双方共享密钥的帮助下克服窃听者的优势。


<details>
  <summary>Details</summary>
Motivation: 研究在反退化（reversely-degraded）wiretap信道下，当无反馈时保密容量为零的情形，探索是否通过反馈实现正的保密传输，并推动对感知协同安全通信的编码设计。

Method: 提出一种带种子的模块化编码设计，结合通用哈希函数用于信息安全性，以及基于学习的反馈编码用于提高可靠性；分析可靠性和信息泄露之间的权衡；展示通过反馈可在合法方之间建立秘密密钥的可行性。

Result: 证实反馈使得在高斯wiretap信道上获得正的保密速率成为可能，并展示了通过密钥协商抑制窃听者的优势；提供对感知辅助安全通信的代码设计启示。

Conclusion: 这类设计为下一代综合感知与通信的安全编码提供了思路，强调在反馈可用的场景下，结合哈希和学习驱动的编码有望提升保密性与可靠性。

Abstract: We consider reversely-degraded wiretap channels, for which the secrecy
capacity is zero if there is no channel feedback. This work focuses on a seeded
modular code design for the Gaussian wiretap channel with channel output
feedback, combining universal hash functions for security and learned
feedback-based codes for reliability to achieve positive secrecy rates. We
study the trade-off between communication reliability and information leakage,
illustrating that feedback enables agreeing on a secret key shared between
legitimate parties, overcoming the security advantage of the wiretapper. Our
findings also motivate code designs for sensing-assisted secure communication,
to be used in next-generation integrated sensing and communication methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.23617)
*Phuong Q. Dao,Mark Roantree,Vuong M. Ngo*

Main category: cs.LG

TL;DR: 提出 BERT-ViT-EF 与 DTCN 的多模态情感分析框架，通过早期融合和对比学习提升文本-视觉跨模态表示，在 MVSA-Single 与 TumEmo 数据集上取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析需要同时从文本和视觉信息中提取情感线索，单模态往往受限；早期跨模态融合可促进更深层次的交互与联合表征；引入对比学习以对齐不同模态的语义表示以提升鲁棒性。

Method: 提出 BERT-ViT-EF：采用 BERT 编码文本、ViT 编码图像，通过早期融合实现跨模态交互与联合表示学习。基于此，扩展出 Dual Transformer Contrastive Network (DTCN)：在 BERT 之后再加入一个 Transformer 编码层以进一步 refine 文本上下文，并引入对比学习对齐文本与图像表征。

Result: 在 TumEmo 数据集上获得最高准确率 78.4% 与 F1 78.3%；在 MVSA-Single 上达到 76.6% 的准确率和 75.9% 的 F1；结果表明早期融合和更深层的上下文建模对提升多模态情感分析性能有效。

Conclusion: 该工作表明将早期融合与深层 Transformer 建模结合，并辅以对比学习，能提升多模态情感分析的表示能力与鲁棒性，在两个主流基准数据集上展现出竞争力的性能。

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by
jointly analyzing data from multiple modalities typically text and images
offering a richer and more accurate interpretation than unimodal approaches. In
this paper, we first propose BERT-ViT-EF, a novel model that combines powerful
Transformer-based encoders BERT for textual input and ViT for visual input
through an early fusion strategy. This approach facilitates deeper cross-modal
interactions and more effective joint representation learning. To further
enhance the model's capability, we propose an extension called the Dual
Transformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN
incorporates an additional Transformer encoder layer after BERT to refine
textual context (before fusion) and employs contrastive learning to align text
and image representations, fostering robust multimodal feature learning.
Empirical results on two widely used MSA benchmarks MVSA-Single and TumEmo
demonstrate the effectiveness of our approach. DTCN achieves best accuracy
(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on
MVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements
highlight the benefits of early fusion and deeper contextual modeling in
Transformer-based multimodal sentiment analysis.

</details>


### [8] [Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields](https://arxiv.org/abs/2510.23621)
*Alexandre Benoit*

Main category: cs.LG

TL;DR: 通过对 MACE 的端到端和分块分析，以及 e3nn 与 cuEquivariance 两种后端、不同精度设置（FP64/FP32/BF16/FP16、FP32 累加）的比较，对 SO(3) 等变模型的推理和分子动力学性能进行评估，发现使用 cuEquivariance 可将推理延迟降低约 3 倍；仅将线性层转为 BF16/FP16（在 FP32 模型内）可再获得约 4 倍的速度提升；在保持热力学 observables 的可变性前提下，长短期 MD 结果未显著偏离。提出基于混合精度和后端选择的实用策略。


<details>
  <summary>Details</summary>
Motivation: 当前关于像 MACE 这样的 SO(3) 等变 ML 力场，低精度执行和 GPU 内核优化是否能在不损害物理保真度的前提下显著降低成本，尚缺乏系统证据。需要对端到端性能、各模块瓶颈及不同数值策略进行 Profiling 与对比。

Method: 对 MACE 进行端到端与逐块 Profiling；比较 e3nn 与 cuEquivariance 后端；在推断与短时 NVT、长时 NPT 水系统及简单训练片段上，比较 FP64/FP32/BF16/FP16（FP32 累加）下的性能与数值稳定性；在重复性可控的基准下进行时间测量。

Result: cuEquivariance 将推理延迟约降低 3×；仅对线性层在 FP32 模型内转为 BF16/FP16 可再提升约 4× 的速度；NVT/NPT MD 的能量和热力学观测值在 run-to-run 的变动范围内；训练时使用半精度权重会降低力的 RMSE；未使用适配器就混合 e3nn 与 cuEqmodules 会导致表示不匹配；融合的等变核和混合精度推理可在不显著影响 MD 的情况下显著加速力场计算。

Conclusion: 建议的实用策略是在默认情况下使用 cuEquivariance + FP32，并将线性层开启 BF16/FP16（保留 FP32 累加），以获得最大吞吐，同时训练保持 FP32；在 Ampere/Hopper GPU（TF32/BF16）及内核层 FP16/BF16 路径与流水线融合方面，理论上将带来进一步收益。

Abstract: Machine-learning force fields can deliver accurate molecular dynamics (MD) at
high computational cost. For SO(3)-equivariant models such as MACE, there is
little systematic evidence on whether reduced-precision arithmetic and
GPU-optimized kernels can cut this cost without harming physical fidelity. This
thesis aims to make MACE cheaper and faster while preserving accuracy by
identifying computational bottlenecks and evaluating low-precision execution
policies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA
cuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32
accumulation) for inference, short NVT and long NPT water simulations, and toy
training runs under reproducible, steady-state timing. cuEquivariance reduces
inference latency by about $3\times$. Casting only linear layers to BF16/FP16
within an FP32 model yields roughly 4x additional speedups, while energies and
thermodynamic observables in NVT/NPT MD remain within run-to-run variability.
Half-precision weights during training degrade force RMSE. Mixing e3nn and cuEq
modules without explicit adapters causes representation mismatches. Fused
equivariant kernels and mixed-precision inference can substantially accelerate
state-of-the-art force fields with negligible impact on downstream MD. A
practical policy is to use cuEquivariance with FP32 by default and enable
BF16/FP16 for linear layers (keeping FP32 accumulations) for maximum
throughput, while training remains in FP32. Further gains are expected on
Ampere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and
pipeline fusion.

</details>


### [9] [Adversarially-Aware Architecture Design for Robust Medical AI Systems](https://arxiv.org/abs/2510.23622)
*Alyssa Gerhart,Balaji Iyangar*

Main category: cs.LG

TL;DR: 该工作概述了AI在医疗领域的对抗性攻击风险，通过皮肤科数据集的实证研究表明攻击能显著降低分类准确性；防御方法（对抗性训练、蒸馏）虽有缓解，但需权衡与清洁数据性能；呼吁在技术、伦理与政策层面开展综合治理以提升韧性与公平性。


<details>
  <summary>Details</summary>
Motivation: 确保医疗AI的安全性与公平性，避免对弱势人群的不利影响；应对对抗性攻击带来的潜在风险。

Method: 在 dermatological 数据集上进行经验性实验，进行威胁建模、基准测试和模型评估，评估多种防御策略（对抗性训练、蒸馏等）。

Result: 对抗攻击显著降低分类准确性；防御显著降低攻击成功率，但可能降低对清洁数据的性能，且效果并非全面解决方案。

Conclusion: 需要技术、伦理与政策的综合路径，以构建更鲁棒且更公平的医疗AI系统。

Abstract: Adversarial attacks pose a severe risk to AI systems used in healthcare,
capable of misleading models into dangerous misclassifications that can delay
treatments or cause misdiagnoses. These attacks, often imperceptible to human
perception, threaten patient safety, particularly in underserved populations.
Our study explores these vulnerabilities through empirical experimentation on a
dermatological dataset, where adversarial methods significantly reduce
classification accuracy. Through detailed threat modeling, experimental
benchmarking, and model evaluation, we demonstrate both the severity of the
threat and the partial success of defenses like adversarial training and
distillation. Our results show that while defenses reduce attack success rates,
they must be balanced against model performance on clean data. We conclude with
a call for integrated technical, ethical, and policy-based approaches to build
more resilient, equitable AI in healthcare.

</details>


### [10] [DiNo and RanBu: Lightweight Predictions from Shallow Random Forests](https://arxiv.org/abs/2510.23624)
*Tiago Mendonça dos Santos,Rafael Izbicki,Luís Gustavo Esteves*

Main category: cs.LG

TL;DR: 提出两种浅森林方法DiNo和RanBu，通过在已有森林基础上不增树数量、仅后处理来实现高效距离加权预测，在多数据集上达到与深度随机森林相当甚至更好精度，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 减少深度随机森林在推理和内存上的开销，提升在延迟敏感或资源受限环境下的可部署性，同时在噪声环境下保持或提升预测性能。

Method: DiNo通过观察后代祖先得到的共生距离（最近公共祖先的派生距离）来度量观测对的距离；RanBu使用对Breiman的邻近度的核平滑，将深森林转化为距离权重的浅森林预测。两者都在森林训练完成后进行，不再成长新树，只需要对带宽参数h进行轻量矩阵向量运算调参。

Result: 在三组合成基准和25个公开数据集上，RanBu达到或超过全深度随机森林的准确性，尤其在高噪声设置下，并将训练和推理时间总计减少最多95%；DiNo在低噪声情况下获得最好偏差-方差权衡，但计算成本适中；两者也可扩展至分位数回归，保持精度同时获得显著的速度提升。

Conclusion: 该工作提供了两个高效的浅森林方法，适合用于结构化表格数据，且开源实现可用于实践部署，同时为下一步工作扩展至其他模态和分位回归提供方向。

Abstract: Random Forest ensembles are a strong baseline for tabular prediction tasks,
but their reliance on hundreds of deep trees often results in high inference
latency and memory demands, limiting deployment in latency-sensitive or
resource-constrained environments. We introduce DiNo (Distance with Nodes) and
RanBu (Random Bushes), two shallow-forest methods that convert a small set of
depth-limited trees into efficient, distance-weighted predictors. DiNo measures
cophenetic distances via the most recent common ancestor of observation pairs,
while RanBu applies kernel smoothing to Breiman's classical proximity measure.
Both approaches operate entirely after forest training: no additional trees are
grown, and tuning of the single bandwidth parameter $h$ requires only
lightweight matrix-vector operations. Across three synthetic benchmarks and 25
public datasets, RanBu matches or exceeds the accuracy of full-depth random
forests-particularly in high-noise settings-while reducing training plus
inference time by up to 95\%. DiNo achieves the best bias-variance trade-off in
low-noise regimes at a modest computational cost. Both methods extend directly
to quantile regression, maintaining accuracy with substantial speed gains. The
implementation is available as an open-source R/C++ package at
https://github.com/tiagomendonca/dirf. We focus on structured tabular random
samples (i.i.d.), leaving extensions to other modalities for future work.

</details>


### [11] [Optimal Arm Elimination Algorithms for Combinatorial Bandits](https://arxiv.org/abs/2510.23992)
*Yuxiao Wen,Yanjun Han,Zhengyuan Zhou*

Main category: cs.LG

TL;DR: 提出一种三类集合消除的框架，用显式探索更新确认、活跃与被淘汰三类臂，在两类组合型带反馈问题中实现近似最优的后悔界，并给出匹配的下界，展示与基于UCB的方法在显式探索方面的不足对比。


<details>
  <summary>Details</summary>
Motivation: 将单臂消除策略迁移至组合式带反馈的情形具有挑战性；需要设计有效的探索机制以避免过早淘汰并在多臂、图反馈与上下文设置下获得理论保障。

Method: 提出一个三类臂集合的消除方案，加入显式探索以更新确认、活跃与淘汰集合；对两类问题进行实现与分析：组合多臂老虎机（CMAB） with一般图反馈，以及组合线性上下文多臂老虎机（CLCB）。通过理论分析给出近似最优的后悔界并比较与UCB等方法在显式探索方面的不足。

Result: 在两种设定中实现近似最优的后悔界，证明UCB基方法在没有足够显式探索的情况下可能失效；给出匹配的下界，确认所提框架的理论优越性。

Conclusion: 所提三类集合消除框架解决了组合型Bandits中消除策略的难题，具备理论保证并在两类重要设定中表现良好。

Abstract: Combinatorial bandits extend the classical bandit framework to settings where
the learner selects multiple arms in each round, motivated by applications such
as online recommendation and assortment optimization. While extensions of upper
confidence bound (UCB) algorithms arise naturally in this context, adapting arm
elimination methods has proved more challenging. We introduce a novel
elimination scheme that partitions arms into three categories (confirmed,
active, and eliminated), and incorporates explicit exploration to update these
sets. We demonstrate the efficacy of our algorithm in two settings: the
combinatorial multi-armed bandit with general graph feedback, and the
combinatorial linear contextual bandit. In both cases, our approach achieves
near-optimal regret, whereas UCB-based methods can provably fail due to
insufficient explicit exploration. Matching lower bounds are also provided.

</details>


### [12] [From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media](https://arxiv.org/abs/2510.23626)
*Shuang Geng,Wenli Zhang,Jiaheng Xie,Rui Wang,Sudha Ram*

Main category: cs.LG

TL;DR: 提出一种闭环的大语言模型-知识图谱框架，将预测与知识扩展在迭代学习中耦合，用于UGC驱动的抑郁预测并提升医学理解。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体UGC的实时自报告指标中，传统方法多以静态知识为基础，难以在预测过程中持续扩展领域知识。本文提出一个自学习-自我完善的闭环框架，弥合预测与知识更新之间的断层。

Method: 分两阶段循环：第一阶段在知识感知的抑郁检测中，LLM同时执行抑郁检测与实体抽取，知识图谱对实体进行表征和加权以提升预测；第二阶段进行知识 refinement and expansion，从LLM提取的新实体、关系和实体类型在专家监督下加入知识图谱，实现持续演化。使用大规模UGC用于提升预测准确性与医学理解。

Result: 在实验中显著提升预测准确性并增强医学理解；专家评估证实发现了与现有文献互补的临床相关症状、共病和社会触发因素。提出“通过预测学习”和“以学促预测”的互补关系，推动预测分析的方法论与理论理解。

Conclusion: 展示计算模型与领域知识的共同进化，为可自适应、数据驱动的知识系统提供基础，可扩展到其他动态风险监测场景。

Abstract: Social media user-generated content (UGC) provides real-time, self-reported
indicators of mental health conditions such as depression, offering a valuable
source for predictive analytics. While prior studies integrate medical
knowledge to improve prediction accuracy, they overlook the opportunity to
simultaneously expand such knowledge through predictive processes. We develop a
Closed-Loop Large Language Model (LLM)-Knowledge Graph framework that
integrates prediction and knowledge expansion in an iterative learning cycle.
In the knowledge-aware depression detection phase, the LLM jointly performs
depression detection and entity extraction, while the knowledge graph
represents and weights these entities to refine prediction performance. In the
knowledge refinement and expansion phase, new entities, relationships, and
entity types extracted by the LLM are incorporated into the knowledge graph
under expert supervision, enabling continual knowledge evolution. Using
large-scale UGC, the framework enhances both predictive accuracy and medical
understanding. Expert evaluations confirmed the discovery of clinically
meaningful symptoms, comorbidities, and social triggers complementary to
existing literature. We conceptualize and operationalize
prediction-through-learning and learning-through-prediction as mutually
reinforcing processes, advancing both methodological and theoretical
understanding in predictive analytics. The framework demonstrates the
co-evolution of computational models and domain knowledge, offering a
foundation for adaptive, data-driven knowledge systems applicable to other
dynamic risk monitoring contexts.

</details>


### [13] [Information-Theoretic Discrete Diffusion](https://arxiv.org/abs/2510.24088)
*Moongyu Jeon,Sangwoo Shin,Dongjae Jeon,Albert No*

Main category: cs.LG

TL;DR: 提出一个离散扩散模型的信息论框架，通过最小去噪分数熵和交叉熵的 I-MDSE / I-MDCE 关系，将对数似然分解为传统分数损失的时间积分，证明 DSE 与 DCE 是对数似然的紧确定估计器，并给出可扩展的应用如时间无公式、条件似然估计与马可夫蒙特卡洛比率估计；并在合成与真实数据上验证稳定性与准确性，代码公开。


<details>
  <summary>Details</summary>
Motivation: 在离散扩散模型中，需要一个信息论基础来对对数似然进行 principled 的估计与分解，弥合高斯情形的 I-MMSE 理论与离散情形的应用。通过 I-MDSE/ I-MDCE，将数据及其扩散版本之间的互信息与最优去噪分数熵/交叉熵损失联系起来，提供紧而有用的对数似然估计框架。

Method: 建立 I-MDSE 关系，将数据与扩散版本间的互信息与最小去噪分数熵损失联系；扩展到掩码扩散，建立 I-MDCE 关系，将交叉熵损失与离散掩码过程的互信息关联；给出对数似然的时间积分分解，表明 DSE 与 DCE 不是变分下界，而是对数似然的紧致估计器；提出时间无公式、条件似然估计与耦合蒙特卡洛估计等可操作扩展。

Result: 理论上证明了互信息等于以最优分数损失为被积项的时间积分，同时证明 DSE 与 DCE 是对数似然的紧确定估计器；提出了实用扩展与可重复实现；在合成与真实数据上验证了估计的准确性、方差稳定性和实用性；代码已公开。

Conclusion: 该框架为离散扩散模型的对数似然估计提供一个 principled 的信息论基础，统一并巩固了 DSE/DCE 等损失的意义，促进在条件似然估计和概率比估计等实际任务中的应用，并通过公开代码便于复现与扩展。

Abstract: We present an information-theoretic framework for discrete diffusion models
that yields principled estimators of log-likelihood using score-matching
losses. Inspired by the I-MMSE identity for the Gaussian setup, we derive
analogous results for the discrete setting. Specifically, we introduce the
Information-Minimum Denoising Score Entropy (I-MDSE) relation, which links
mutual information between data and its diffused version to the minimum
denoising score entropy (DSE) loss. We extend this theory to masked diffusion
and establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)
relation, connecting cross-entropy losses to mutual information in discrete
masked processes. These results provide a time-integral decomposition of the
log-likelihood of the data in terms of optimal score-based losses, showing that
commonly used losses such as DSE and DCE are not merely variational bounds but
tight and principled estimators of log-likelihood. The I-MDCE decomposition
further enables practical extensions, including time-free formula, conditional
likelihood estimation in prompt-response tasks, and coupled Monte Carlo
estimation of likelihood ratios. Experiments on synthetic and real-world data
confirm the accuracy, variance stability, and utility of our estimators. The
code is publicly available at https://github.com/Dongjae0324/infodis.

</details>


### [14] [Chain of Execution Supervision Promotes General Reasoning in Large Language Models](https://arxiv.org/abs/2510.23629)
*Nuo Chen,Zehua Li,Keqin Bao,Junyang Lin,Dayiheng Liu*

Main category: cs.LG

TL;DR: TracePile introduces a large 2.6M-sample corpus that converts code execution into explicit, step-by-step chain-of-execution rationales (CoE) to improve robust reasoning in LLMs, showing consistent gains across math, code, logic, and algorithm domains with two-stage finetuning.


<details>
  <summary>Details</summary>
Motivation: Code contains rich logical structure but reasoning signals are often implicit and polluted by noise; providing explicit CoE can guide models to generalize better across domains and tasks.

Method: Construct a large-scale dataset (TracePile) with 2.6 million samples that turn code execution into CoE-style rationales. enrich with variable-tracing questions and code rewritings across domains (mathematics, classical algorithms, competitive programming). Evaluate via three training setups (continue-pretraining, instruction tuning after pretraining, two-stage finetuning) on four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5, Qwen-2.5 Coder) over 20 benchmarks spanning math, code, logic, and algorithms.

Result: Across four base models and 20 benchmarks, TracePile produces consistent improvements. Notably, LLaMA3.1-8B achieves an average +7.1% on nine math datasets; two-stage fine-tuning yields clear gains on LiveCodeBench, CRUX, and MMLU.

Conclusion: Explicit CoE grounded in code execution can enhance general reasoning in LLMs, with two-stage finetuning amplifying benefits and wide-domain applicability demonstrated across math, code, and algorithmic tasks.

Abstract: Building robust and general reasoning ability is a central goal in the
development of large language models (LLMs). Recent efforts increasingly turn
to code as a rich training source, given its inherent logical structure and
diverse reasoning paradigms such as divide-and-conquer, topological ordering,
and enumeration. However, reasoning in code is often expressed implicitly and
entangled with syntactic or implementation noise, making direct training on raw
code suboptimal.To address this, we introduce TracePile, a large-scale corpus
of 2.6 million samples that transforms code execution into explicit,
step-by-step chain-of-thought-style rationales, which we call Chain of
Execution (CoE). The corpus spans domains including mathematics, classical
algorithms and algorithmic competition, and is enriched with variable-tracing
questions and code rewritings to enhance logical granularity and code
diversity. We evaluate TracePile using three training setups:
continue-pretraining, instruction tuning after pretraining, and two-stage
finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,
and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and
algorithms demonstrate consistent improvements. Notably, TracePile boosts
LLaMA3.1-8B by 7.1\% on average across nine math datasets and delivers clear
gains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.

</details>


### [15] [NUM2EVENT: Interpretable Event Reasoning from Numerical time-series](https://arxiv.org/abs/2510.23630)
*Ninghui Feng,Yiyan Qi*

Main category: cs.LG

TL;DR: 提出一种将数值时间序列映射为可解释事件的推理框架，结合AGE、EveDTS和两阶段微调，输出结构化事件假设与中间解释，显著优于强基准的事件级精度与召回。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在纯数值时间序列理解上的局限，现有方法多聚焦于预测和趋势描述，未能揭示驱动数值变化的潜在事件或给出推理过程。数据稀缺与语义对齐也是关键挑战。

Method: 提出ager行为驱动的事件提取器（AGE）、带标记的多变量 Hawkes 合成生成器（EveDTS），以及两阶段微调流程，包含时间序列编码器与结构化解码器。模型显式对数值变化进行推理，生成中间解释并给出结构化事件假设。

Result: 在多领域数据集上，所提方法在事件级精度和召回方面显著优于强基线的LLM。

Conclusion: 为量化推理与语义理解建立新方向，使LLMs能够直接从数值动态中解释和预测事件。

Abstract: Large language models (LLMs) have recently demonstrated impressive multimodal
reasoning capabilities, yet their understanding of purely numerical time-series
signals remains limited. Existing approaches mainly focus on forecasting or
trend description, without uncovering the latent events that drive numerical
changes or explaining the reasoning process behind them. In this work, we
introduce the task of number-to-event reasoning and decoding, which aims to
infer interpretable structured events from numerical inputs, even when current
text is unavailable. To address the data scarcity and semantic alignment
challenges, we propose a reasoning-aware framework that integrates an
agent-guided event extractor (AGE), a marked multivariate Hawkes-based
synthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a
time-series encoder with a structured decoder. Our model explicitly reasons
over numerical changes, generates intermediate explanations, and outputs
structured event hypotheses. Experiments on multi-domain datasets show that our
method substantially outperforms strong LLM baselines in event-level precision
and recall. These results suggest a new direction for bridging quantitative
reasoning and semantic understanding, enabling LLMs to explain and predict
events directly from numerical dynamics.

</details>


### [16] [Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling](https://arxiv.org/abs/2510.23631)
*Yuxuan Tang,Yifan Feng*

Main category: cs.LG

TL;DR: 提出 Ranked Choice Preference Optimization (RCPO) ，通过最大似然估计将排序选择（多选、Top-k 等）与偏好优化结合起来，用于对话模型的对齐。比对两两偏好方法具有更丰富反馈，且在多种模型与基准上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法大多基于两两偏好，忽略了更丰富的人类反馈形式（多选、Top-k 排序）。需要一个统一的框架来充分利用排序偏好信息，以提升对齐效果。

Method: 提出 RCPO 框架，可结合效用-based 与排序-based 的选择模型，通过最大似然估计对排序/多选数据进行学习。该框架可覆盖现有的两两方法（如 DPO、SimPO），并提供对 richer 反馈的训练目标。以两种代表性排序选择模型（多项式逻辑回归 MNL、Mallows-RMJ）为例实现，并在 Llama-3-8B-Instruct 与 Gemma-2-9B-it 上的 AlpacaEval 2 与 Arena-Hard 基准进行评估。

Result: RCPO 在实验中始终优于竞争基线。直接利用排序偏好数据并配合合适的选择模型，能获得更有效的模型对齐。

Conclusion: RCPO 为将排序/排名偏好建模引入大语言模型训练提供了一个灵活且可扩展的基础，可进一步在对齐任务中利用更丰富的反馈形式。

Abstract: Alignment of large language models (LLMs) has predominantly relied on
pairwise preference optimization, where annotators select the better of two
responses to a prompt. While simple, this approach overlooks the opportunity to
learn from richer forms of human feedback, such as multiwise comparisons and
top-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a
unified framework that bridges preference optimization with (ranked) choice
modeling via maximum likelihood estimation. The framework is flexible,
supporting both utility-based and rank-based choice models. It subsumes several
existing pairwise methods (e.g., DPO, SimPO), while providing principled
training objectives for richer feedback formats. We instantiate this framework
with two representative ranked choice models (Multinomial Logit and
Mallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across
AlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms
competitive baselines. RCPO shows how directly leveraging ranked preference
data, combined with the right choice models, yields more effective alignment.
It offers a versatile and extensible foundation for incorporating (ranked)
choice modeling into LLM training.

</details>


### [17] [LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression](https://arxiv.org/abs/2510.23632)
*Guozhong Li,Muhannad Alhumaidi,Spiros Skiadopoulos,Panos Kalnis*

Main category: cs.LG

TL;DR: 提出 LLMCOMP，一种基于解码器的有损压缩框架，利用大语言模型对科学数据进行序列建模，通过离散化、Z序排列和覆盖引导采样等技术对高分辨率时空数据进行高效、具严格误差界限的压缩。


<details>
  <summary>Details</summary>
Motivation: 面对海量高分辨率科学数据的存储与传输需求，现有高效压缩方法在严格误差边界下往往受限；解码器优先的大语言模型对复杂序列数据具有建模潜力，能够作为通用压缩器。

Method: 将3D场量化为离散标记并通过 Z-order 曲线保持局部性，采用覆盖引导采样提升训练效率；使用具有时空嵌入的自回归变换器对标记转移进行建模。在压缩阶段，模型进行前k预测，仅存储 rank 索引和回退修正以确保严格误差界限。

Result: 在多组再分析数据集上，LLMCOMP 的压缩比显著优于现有最优压缩器，在严格误差边界下实现最高约 30% 的提升。

Conclusion: 展示了将解码器端大语言模型作为高保真科学数据通用压缩器的潜力。

Abstract: The rapid growth of high-resolution scientific simulations and observation
systems is generating massive spatiotemporal datasets, making efficient,
error-bounded compression increasingly important. Meanwhile, decoder-only large
language models (LLMs) have demonstrated remarkable capabilities in modeling
complex sequential data. In this paper, we propose LLMCOMP, a novel lossy
compression paradigm that leverages decoder-only large LLMs to model scientific
data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via
Z-order curves to preserve locality, and applies coverage-guided sampling to
enhance training efficiency. An autoregressive transformer is then trained with
spatial-temporal embeddings to model token transitions. During compression, the
model performs top-k prediction, storing only rank indices and fallback
corrections to ensure strict error bounds. Experiments on multiple reanalysis
datasets show that LLMCOMP consistently outperforms state-of-the-art
compressors, achieving up to 30% higher compression ratios under strict error
bounds. These results highlight the potential of LLMs as general-purpose
compressors for high-fidelity scientific data.

</details>


### [18] [Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models](https://arxiv.org/abs/2510.23633)
*Xun Su,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: Noise Combination Sampling提出了一种从噪声子空间合成最佳噪声向量以近似测量分数，从而在扩散模型的生成过程中自然嵌入逆问题约束，避免逐步超参调优，提升小步长时的鲁棒性和性能，适用于广泛的逆问题如图像压缩。


<details>
  <summary>Details</summary>
Motivation: 现有的零-shot 逆问题求解中对观测信息的嵌入存在权衡：过度整合可能干扰生成，不足整合难以满足约束。

Method: 提出Noise Combination Sampling，通过从噪声子空间合成一个最佳噪声向量来替代扩散过程中的噪声项以近似测量分数，做到不需要逐步超参数调优，同时将条件信息自然嵌入生成过程。

Result: 在包括图像压缩等多种逆问题中有效，且在生成步数T较小时表现优于基线，计算开销几乎为零，显著提升鲁棒性和稳定性。

Conclusion: 该方法可广泛应用于逆问题求解，提供更鲁棒、稳定且高效的条件化扩散模型解决方案。

Abstract: Pretrained diffusion models have demonstrated strong capabilities in
zero-shot inverse problem solving by incorporating observation information into
the generation process of the diffusion models. However, this presents an
inherent dilemma: excessive integration can disrupt the generative process,
while insufficient integration fails to emphasize the constraints imposed by
the inverse problem. To address this, we propose \emph{Noise Combination
Sampling}, a novel method that synthesizes an optimal noise vector from a noise
subspace to approximate the measurement score, replacing the noise term in the
standard Denoising Diffusion Probabilistic Models process. This enables
conditional information to be naturally embedded into the generation process
without reliance on step-wise hyperparameter tuning. Our method can be applied
to a wide range of inverse problem solvers, including image compression, and,
particularly when the number of generation steps $T$ is small, achieves
superior performance with negligible computational overhead, significantly
improving robustness and stability.

</details>


### [19] [Monotone and Separable Set Functions: Characterizations and Neural Models](https://arxiv.org/abs/2510.23634)
*Soutrik Sarangi,Yonatan Sverdlov,Nadav Dym,Abir De*

Main category: cs.LG

TL;DR: 研究如何设计集合到向量的映射，使集合包含关系在向量空间中得到等价表达（S ⊆ T 当且仅当 F(S) ≤ F(T)），称为单调且可分离（MAS）的集合函数。作者给出关于实现MAS的向量维度上下界，证明在无穷底集时不存在MAS函数，但给出一种松弛模型“weakly MAS”的实现，并具有Holder连续性的稳定性。还提出利用MAS构建的通用模型，并能近似所有单调的集合函数。并通过实验验证在集合包含任务上优于不带此偏置的标准集合模型，且给出了开源实现。


<details>
  <summary>Details</summary>
Motivation: 动机是在集合包含问题的应用背景下，寻找能够保持集合之间部分序关系的集合到向量映射，以便在下游任务中使用连续优化或学习模型。

Method: 给出集合维度与多重集合基数的上下界；证明在无限底集情况下不存在严格的MAS函数；提出一个名为“our model”的松弛模型，实现弱MAS并具Holder连贯性；构造可以单调地构造并近似任意单调集合函数的通用模型；进行多任务实验比较，提供实现代码。

Result: 给出MAS维度界、无穷底集下的不可行性、弱MAS模型及其稳定性、可用于近似所有单调集合函数的通用模型，实验结果表明该偏置有益，代码公开。

Conclusion: 在无穷底集情形下严格的MAS不可行，但通过弱MAS等松弛定义可获得实用性强的模型，且可用于逼近与保持集合包含关系的任务；未来工作可能集中在扩展到更广泛的集合操作与分析收敛性。

Abstract: Motivated by applications for set containment problems, we consider the
following fundamental problem: can we design set-to-vector functions so that
the natural partial order on sets is preserved, namely $S\subseteq T \text{ if
and only if } F(S)\leq F(T) $. We call functions satisfying this property
Monotone and Separating (MAS) set functions. % We establish lower and upper
bounds for the vector dimension necessary to obtain MAS functions, as a
function of the cardinality of the multisets and the underlying ground set. In
the important case of an infinite ground set, we show that MAS functions do not
exist, but provide a model called our which provably enjoys a relaxed MAS
property we name "weakly MAS" and is stable in the sense of Holder continuity.
We also show that MAS functions can be used to construct universal models that
are monotone by construction and can approximate all monotone set functions.
Experimentally, we consider a variety of set containment tasks. The experiments
show the benefit of using our our model, in comparison with standard set models
which do not incorporate set containment as an inductive bias. Our code is
available in https://github.com/yonatansverdlov/Monotone-Embedding.

</details>


### [20] [Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning](https://arxiv.org/abs/2510.23635)
*Andrea Bontempelli,Matteo Busso,Leonardo Javier Malcotti,Fausto Giunchiglia*

Main category: cs.LG

TL;DR: 在真实用户条件下评估 Skeptical Learning (SKEL) 对噪声标注的鲁棒性，结果表明在平衡用户努力与数据质量方面存在挑战，但 SKEL 具有降低标注工作量并提升数据质量的潜力。


<details>
  <summary>Details</summary>
Motivation: 数字个人助理需要高质量标注来支持任务执行、问答与日常生活管理。用户标注容易带入噪声与错误，且以往对噪声标签的研究（SKEL）基于离线对比，缺乏用户端的确认。本文旨在在真实世界情境中，邀请最终用户对标签进行确认与 refinement，以评估 SKEL 的实际效果。

Method: 在四周的时间里，让大学生使用 iLog 移动应用，与现实世界数据交互，并让用户在输入标签时对信息进行确认和 refinement，以评估 SKEL 在现实场景中的表现及对标注努力与数据质量的影响。

Result: 研究揭示在用户投入（努力）与数据质量之间需要找到平衡点，同时使用 SKEL 可能降低标注的工作量并提升所收集数据的质量。

Conclusion: 在包含最终用户确认的真实条件下评估 SKEL 的可行性和效益。结果表明 SKEL 在减少标注负担的同时可改善数据质量，但需权衡用户参与成本与系统准确性。

Abstract: Any digital personal assistant, whether used to support task performance,
answer questions, or manage work and daily life, including fitness schedules,
requires high-quality annotations to function properly. However, user
annotations, whether actively produced or inferred from context (e.g., data
from smartphone sensors), are often subject to errors and noise. Previous
research on Skeptical Learning (SKEL) addressed the issue of noisy labels by
comparing offline active annotations with passive data, allowing for an
evaluation of annotation accuracy. However, this evaluation did not include
confirmation from end-users, the best judges of their own context. In this
study, we evaluate SKEL's performance in real-world conditions with actual
users who can refine the input labels based on their current perspectives and
needs. The study involves university students using the iLog mobile application
on their devices over a period of four weeks. The results highlight the
challenges of finding the right balance between user effort and data quality,
as well as the potential benefits of using SKEL, which include reduced
annotation effort and improved quality of collected data.

</details>


### [21] [Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation](https://arxiv.org/abs/2510.23636)
*Thaweerath Phisannupawong,Joshua Julian Damanik,Han-Lim Choi*

Main category: cs.LG

TL;DR: 提出一个轻量级的基于大语言模型的多模态航班延误预测框架，将轨迹数据转化为语言模态并与文本航行情报、天气和机场公告融合，实现亚分钟级误差并具备实时更新能力。


<details>
  <summary>Details</summary>
Motivation: 解决航路管理中的延误带来的效率损失；需要在塔台区域内对进入信息的时序与环境信息作出实时、上下文感知的预测。

Method: 将轨迹数据转化为语言模态，与飞行信息、天气（METAR/TAF）和NOTAM等文本信息进行跨模态融合；对轨迹-语言的跨模态适配；采用轻量级LLM实现实时推理并支持新信息的增量更新；在仿真/实际数据上评估。

Result: 在多模态融合下实现稳健的亚分钟级预测误差，充分利用延迟来源的上下文信息，显示出可扩展性和实时更新能力。

Conclusion: 语言理解与轨迹信息的跨模态适配相结合能提升延误预测的准确性和可操作性，具有实际部署潜力。

Abstract: Flight delay prediction has become a key focus in air traffic management, as
delays highlight inefficiencies that impact overall network performance. This
paper presents a lightweight large language model-based multimodal flight delay
prediction, formulated from the perspective of air traffic controllers
monitoring aircraft delay after entering the terminal area. The approach
integrates trajectory representations with textual aeronautical information,
including flight information, weather reports, and aerodrome notices, by
adapting trajectory data into the language modality to capture airspace
conditions. Experimental results show that the model consistently achieves
sub-minute prediction error by effectively leveraging contextual information
related to the sources of delay. The framework demonstrates that linguistic
understanding, when combined with cross-modality adaptation of trajectory
information, enhances delay prediction. Moreover, the approach shows
practicality and scalability for real-world operations, supporting real-time
updates that refine predictions upon receiving new operational information.

</details>


### [22] [Integrating Genomics into Multimodal EHR Foundation Models](https://arxiv.org/abs/2510.23639)
*Jonathan Amar,Edward Liu,Alessandra Breschi,Liangliang Zhang,Pouya Kheradpour,Sylvia Li,Lisa Soleymani Lehmann,Alessandro Giulianelli,Matt Edwards,Yugang Jia,David Nola,Raghav Mani,Pankaj Vats,Jesse Tetreault,T. J. Chen,Cory Y. McLean*

Main category: cs.LG

TL;DR: 将EHR基础模型与多基因风险评分（PRS）整合，利用All of Us数据构建多模态、可扩展的健康风险表征，提升疾病预测与可解释性，尤其在2型糖尿病（T2D）预测方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 通过把遗传信息嵌入EHR框架，超越只依赖临床记录的传统方法，形成更全面的健康画像，从而提升预测能力、个性化干预与健康公平性。

Method: 构建一个多模态的EHR基础模型，融入PRS等遗传数据，基于All of Us研究计划中的大规模异质数据进行训练；在生成式AI框架下扩展EHR基础模型，探究遗传与临床信号的交互；并研究在自定义任务上的迁移学习以提升架构的通用性与效率。

Result: 在All of Us数据上进行评估，模型对多种疾病的发生预测具有价值，尤其是T2D；揭示PRS与EHR数据之间的相互作用对预测的贡献，展示了跨模态信息整合的潜力与可解释性提升。

Conclusion: 为疾病预测、主动健康管理、风险分层和个性化治疗策略提供新的证据与工具，奠定将个性化、可及且具现实世界证据价值的健康干预落地的基础。

Abstract: This paper introduces an innovative Electronic Health Record (EHR) foundation
model that integrates Polygenic Risk Scores (PRS) as a foundational data
modality, moving beyond traditional EHR-only approaches to build more holistic
health profiles. Leveraging the extensive and diverse data from the All of Us
(AoU) Research Program, this multimodal framework aims to learn complex
relationships between clinical data and genetic predispositions. The
methodology extends advancements in generative AI to the EHR foundation model
space, enhancing predictive capabilities and interpretability. Evaluation on
AoU data demonstrates the model's predictive value for the onset of various
conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay
between PRS and EHR data. The work also explores transfer learning for custom
classification tasks, showcasing the architecture's versatility and efficiency.
This approach is pivotal for unlocking new insights into disease prediction,
proactive health management, risk stratification, and personalized treatment
strategies, laying the groundwork for more personalized, equitable, and
actionable real-world evidence generation in healthcare.

</details>


### [23] [Learning to Drive Safely with Hybrid Options](https://arxiv.org/abs/2510.24674)
*Bram De Cooman,Johan Suykens*

Main category: cs.LG

TL;DR: 将选项框架应用于高速公路自主驾驶，构建纵向与横向专用选项，嵌入安全与舒适约束，得到可解释的分层控制策略，混合选项在多变交通条件下优于基线动作策略。


<details>
  <summary>Details</summary>
Motivation: 弥补深度强化学习在自主驾驶中对选项框架应用的欠缺，通过分层控制与领域知识的注入，提高安全、舒适性与可解释性。

Method: 设计纵向和横向驾控的专用选项，嵌入安全与舒适约束，提出多种带选项的分层控制设置，结合最前沿的强化学习技术，分别对纵向和横向控制做动作选择，形成对组合与混合选项的策略。

Result: 在研究的方法中，混合选项的策略表现最佳，在变动交通条件下优于基线的直接动作策略。

Conclusion: 选项框架下的分层、混合选项策略具有良好表达能力、可解释性并便于整合领域知识，适合实现接近人类驾驶的灵活性与安全性。

Abstract: Out of the many deep reinforcement learning approaches for autonomous
driving, only few make use of the options (or skills) framework. That is
surprising, as this framework is naturally suited for hierarchical control
applications in general, and autonomous driving tasks in specific. Therefore,
in this work the options framework is applied and tailored to autonomous
driving tasks on highways. More specifically, we define dedicated options for
longitudinal and lateral manoeuvres with embedded safety and comfort
constraints. This way, prior domain knowledge can be incorporated into the
learning process and the learned driving behaviour can be constrained more
easily. We propose several setups for hierarchical control with options and
derive practical algorithms following state-of-the-art reinforcement learning
techniques. By separately selecting actions for longitudinal and lateral
control, the introduced policies over combined and hybrid options obtain the
same expressiveness and flexibility that human drivers have, while being easier
to interpret than classical policies over continuous actions. Of all the
investigated approaches, these flexible policies over hybrid options perform
the best under varying traffic conditions, outperforming the baseline policies
over actions.

</details>


### [24] [Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging](https://arxiv.org/abs/2510.23641)
*Aaron Wang,Zihan Zhao,Subash Katel,Vivekanand Gyanchand Sahu,Elham E Khoda,Abhijith Gandrakota,Jennifer Ngadiuba,Richard Cavanaugh,Javier Duarte*

Main category: cs.LG

TL;DR: SAL-T是一种对线性注意力进行物理信息增强的变体，用于高能粒子碰撞数据分析，保持线性时间复杂度，同时通过基于动量-角度等物理特征的区域划分和卷积层捕捉局部相关性，在Jet分类等任务上达到接近全注意力模型的性能，且资源消耗和延迟显著降低。


<details>
  <summary>Details</summary>
Motivation: 解决高能物理场景中Transformer的二次复杂度在高吞吐量环境（如CERN LHC）带来的资源与延迟挑战；通过物理启发的区域划分和局部卷积来保留重要全局与局部相关信息，同时保持线性时间复杂度。

Method: 在linformer的基础上提出SAL-T，利用基于动量和几何特征的空间分区对粒子进行聚类，并在区域之间计算注意力以聚焦物理显著的区域；引入卷积层以捕捉局部相关性，结合物理学洞察进行模型设计；在Jet分类和ModelNet10等数据集上进行评估并与标准linformer和全注意力模型进行比较。

Result: 实验结果显示，SAL-T在Jet分类任务中优于标准linformer，并在分类准确率上与全注意力变换器相当，同时显著减少资源占用和推理延迟；ModelNet10上的实验也支持这一趋势；并提供代码实现。

Conclusion: 通过物理信息驱动的区域化注意力与局部卷积，SAL-T实现了高吞吐场景下的高效且具有竞争力的Transformer变体，为LHC级别的数据分析任务提供可部署的高效替代方案。

Abstract: Transformers are very effective in capturing both global and local
correlations within high-energy particle collisions, but they present
deployment challenges in high-data-throughput environments, such as the CERN
LHC. The quadratic complexity of transformer models demands substantial
resources and increases latency during inference. In order to address these
issues, we introduce the Spatially Aware Linear Transformer (SAL-T), a
physics-inspired enhancement of the linformer architecture that maintains
linear attention. Our method incorporates spatially aware partitioning of
particles based on kinematic features, thereby computing attention between
regions of physical significance. Additionally, we employ convolutional layers
to capture local correlations, informed by insights from jet physics. In
addition to outperforming the standard linformer in jet classification tasks,
SAL-T also achieves classification results comparable to full-attention
transformers, while using considerably fewer resources with lower latency
during inference. Experiments on a generic point cloud classification dataset
(ModelNet10) further confirm this trend. Our code is available at
https://github.com/aaronw5/SAL-T4HEP.

</details>


### [25] [Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments](https://arxiv.org/abs/2510.23931)
*Miguel Fernandez-de-Retana,Unai Zulaika,Rubén Sánchez-Corcuera,Aitor Almeida*

Main category: cs.LG

TL;DR: DP-SGD reduces gradient leakage risk with moderate utility loss; PDP-SGD preserves accuracy but fails to defend against reconstruction


<details>
  <summary>Details</summary>
Motivation: Evaluate DP mechanisms against gradient leakage in federated learning

Method: Train CV models under varying privacy levels in a simulated FL setup; analyze reconstructed data from gradients; compare DP-SGD vs PDP-SGD

Result: DP-SGD mitigates gradient leakage; moderate utility loss; PDP-SGD preserves accuracy but ineffective against reconstruction attacks

Conclusion: Beyond theoretical privacy guarantees, empirical evaluation is crucial for distributed learning leakage risks; choose defenses accordingly

Abstract: Federated Learning (FL) allows for the training of Machine Learning models in
a collaborative manner without the need to share sensitive data. However, it
remains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private
information from the shared model updates. In this work, we investigate the
effectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD
and a variant based on explicit regularization (PDP-SGD) - as defenses against
GLAs. To this end, we evaluate the performance of several computer vision
models trained under varying privacy levels on a simple classification task,
and then analyze the quality of private data reconstructions obtained from the
intercepted gradients in a simulated FL environment. Our results demonstrate
that DP-SGD significantly mitigates the risk of gradient leakage attacks,
albeit with a moderate trade-off in model utility. In contrast, PDP-SGD
maintains strong classification performance but proves ineffective as a
practical defense against reconstruction attacks. These findings highlight the
importance of empirically evaluating privacy mechanisms beyond their
theoretical guarantees, particularly in distributed learning scenarios where
information leakage may represent an unassumable critical threat to data
security and privacy.

</details>


### [26] [Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs](https://arxiv.org/abs/2510.23650)
*Wei Xia*

Main category: cs.LG

TL;DR: 提出静态与动态两种零-shot logits 层去偏方法。动态方法在最小流畅度损失下将偏差降低最多 70%；logits 干预优于隐藏层方法；语义感知的 logits 干预在对齐的大语言模型上稳定且有效。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在零-shot 情况下的偏见问题，避免频繁的微调或隐藏层改动，比较 logits 层去偏与隐藏层去偏的效果与稳定性。

Method: 提出 Static 与 Dynamic 两种零-shot logits 层去偏方法，并引入 logits 干预及其语义感知变体，作为对齐 LLM 的去偏策略，评估在零-shot 场景中的偏差减少与流畅度保持。

Result: Dynamic 方法在对偏差的减少上可达 70% 级别，且对语言流畅度损失极小；logits 干预优于隐藏层方法；语义感知的 logits 干预在对齐 LLMs 上表现稳定且有效。

Conclusion: 零-shot logits 层去偏是一种有效且鲁棒的策略，尤其是语义感知的 logits 干预在对齐模型中具有较强的稳定性与效果。

Abstract: We proposed Static and Dynamic -- two zero-shot logits-layer debiasing
methods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits
intervention outperforms hidden-layer approaches. We show semantic-aware logits
intervention is stable and effective for debiasing aligned LLMs.

</details>


### [27] [SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning](https://arxiv.org/abs/2510.24200)
*Alexander Bakarsky,Dimitar I. Dimitrov,Maximilian Baader,Martin Vechev*

Main category: cs.LG

TL;DR: 提出SPEAR++，通过稀疏字典学习提升梯度反演在线性层+ReLU中的可行性，使批大小提升10x，同时保持对DP噪声与FedAvg聚合的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现实联邦学习场景中梯度反演攻击的可行性问题：原SPEAR在批量大小增长时计算复杂度指数级，难以用于实际测试与评估。希望给出一个可扩展且鲁棒的攻击方法，以评估真实系统的隐私风险。

Method: 在SPEAR的基础上引入稀疏使用字典学习（Sparsely-Used Dictionary Learning）技术，针对带ReLU的线性层梯度，采用字典学习和稀疏表示来高效重建输入，降低计算复杂度并提升对大批量的适应性，同时保持对DP噪声与FedAvg聚合的鲁棒性。

Result: 实验表明SPEAR++在保持SPEAR的核心属性（对DP噪声与FedAvg聚合的鲁棒性）的同时，能够处理大约10倍的批大小，显著提升了攻击的实用性。

Conclusion: SPEAR++使梯度反演攻击在现实的联邦学习部署中更加可行，便于进行隐私风险评估和系统鲁棒性测试；但也提示需要加强防护以抵御此类更高效的攻击。

Abstract: Federated Learning has seen an increased deployment in real-world scenarios
recently, as it enables the distributed training of machine learning models
without explicit data sharing between individual clients. Yet, the introduction
of the so-called gradient inversion attacks has fundamentally challenged its
privacy-preserving properties. Unfortunately, as these attacks mostly rely on
direct data optimization without any formal guarantees, the vulnerability of
real-world systems remains in dispute and requires tedious testing for each new
federated deployment. To overcome these issues, recently the SPEAR attack was
introduced, which is based on a theoretical analysis of the gradients of linear
layers with ReLU activations. While SPEAR is an important theoretical
breakthrough, the attack's practicality was severely limited by its exponential
runtime in the batch size b. In this work, we fill this gap by applying
State-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the
problem of gradient inversion on linear layers with ReLU activations tractable.
Our experiments demonstrate that our new attack, SPEAR++, retains all desirable
properties of SPEAR, such as robustness to DP noise and FedAvg aggregation,
while being applicable to 10x bigger batch sizes.

</details>


### [28] [The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models](https://arxiv.org/abs/2510.23652)
*Yao Lu,Yuqi Li,Wenbin Xie,Shanqing Yu,Qi Xuan,Zhaowei Zhu,Shiping Wen*

Main category: cs.LG

TL;DR: 提出一个连续层剪枝框架 CLP，通过可微分凹门控和端点调优来智能裁剪连续层段，以最大化剪枝后的信息流保留并兼容量化。


<details>
  <summary>Details</summary>
Motivation: LLMs体量巨大、计算成本高，难以在资源受限的边缘设备上部署。现有层剪枝多依赖手工度量且忽略层之间的依赖关系，容易破坏信息流导致性能下降，因此需要一个能考虑层间关系的自适应剪枝方法。

Method: 提出 CLP：包含可微分的凹门控算法，用梯度优化自动识别最佳连续待剪枝层段；以及端点调优策略，通过微调被剪段邻近的层来恢复性能。对 LLaMA2、LLaMA3、Qwen 等模型，规模从 7B 到 70B 进行广泛实验，并可与量化结合。

Result: 在 20% 剪枝率下，LLaMA3-70B 的平均性能保留率为 95.34%，相较基线提高约 4.29%–30.52% 的性能保留。该方法在多种模型和规模上显著优于现有基线，并可与量化协同降低计算成本，且性能损失极小。

Conclusion: CLP 提供了一种对信息流友好的连续层剪枝框架，显著优于现有方法，且可与量化结合进一步压缩模型。

Abstract: Although large language models (LLMs) have achieved revolutionary
breakthroughs in many fields, their large model size and high computational
cost pose significant challenges for practical deployment on
resource-constrained edge devices. To this end, layer pruning has been proposed
to reduce the computational overhead by directly removing redundant layers.
However, existing layer pruning methods typically rely on hand-crafted metrics
to evaluate and remove individual layers, while ignoring the dependencies
between layers. This can disrupt the model's information flow and severely
degrade performance. To address these issues, we propose CLP, a novel
continuous layer pruning framework that introduces two key innovations: a
differentiable concave gate algorithm that automatically identifies the best
continuous layer segments for pruning via gradient-based optimization; and a
cutoff endpoint tuning strategy that effectively restores model performance by
fine-tuning only the layers adjacent to the pruned segments. Extensive
experiments across multiple model architectures (including LLaMA2, LLaMA3 and
Qwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly
outperforms existing state-of-the-art baselines. For example, at a pruning rate
of $20\%$, CLP achieves an average performance retention of $95.34\%$ on
LLaMA3-70B, outperforming baselines by $4.29\%$-$30.52\%$. Furthermore, CLP can
be seamlessly combined with quantization to further compress the model with
only a slight performance loss.

</details>


### [29] [A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops](https://arxiv.org/abs/2510.23657)
*Saklain Niam,Tashfiqur Rahman,Md. Amjad Patwary,Mukarram Hossain*

Main category: cs.LG

TL;DR: 首次构建基于机器学习预测冷等离子体（DBD）处理下的种子发芽提升，ET 模型表现最佳，特征降维后进一步提高；揭示 hormetic 响应与放电参数的关键影响，并提供面向精准农业的决策工具（MLflow 集成）。


<details>
  <summary>Details</summary>
Motivation: 绿色生态的低温等离子体处理可提升种子发芽，但受种-等离子体-环境复杂相互作用影响，难以预测，因此需要一个可泛化的预测框架来优化处理条件。

Method: 在大豆、燕麦、向日葵、萝卜和番茄等物种下，使用 Dielectric Barrier Discharge（DBD）等离子体，比较 GB、XGB、ET 及混合模型，特征降维后 ET 表现最佳，建立了一个预测发芽提升的机器学习框架，并将其嵌入 MLflow；并对放电功率、暴露时间、剂量等变量进行分析。

Result: ET 的 R^2 0.919、RMSE 3.21、MAE 2.62，降维后 R^2 0.925；出现 hormetic 响应：<7 kV 或 <200 s 无明显提升，7–15 kV、200–500 s 时达到最大发芽，>20 kV 或过长暴露时抑制发芽；放电功率≥100 W、短暴露时间对发芽率最优；物种层面：萝卜 MAE 1.46、豆类 MAE 2.05、向日葵 MAE 3.80；品种层面：Williams MAE 1.23、Sari 1.33、Arian 2.86、Nyírség fekete 3.74；并将框架嵌入 MLflow 提供农业精准决策支持。

Conclusion: 所提出的机器学习框架可对 CP 发芽提升进行高准确度预测，并揭示关键物理参数的非线性关系，为在精准农业中利用 CP 进行种子处理提供可操作的优化策略与决策工具。

Abstract: Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet
outcomes remain difficult to predict due to complex seed--plasma--environment
interactions. This study introduces the first machine learning framework to
forecast germination uplift in soybean, barley, sunflower, radish, and tomato
under dielectric barrier discharge (DBD) plasma. Among the models tested (GB,
XGB, ET, and hybrids), Extra Trees (ET) performed best (R\textsuperscript{2} =
0.919; RMSE = 3.21; MAE = 2.62), improving to R\textsuperscript{2} = 0.925
after feature reduction. Engineering analysis revealed a hormetic response:
negligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for
200--500 s, and reduced germination beyond 20 kV or prolonged exposures.
Discharge power was also a dominant factor, with germination rate maximizing at
$\geq$100 W with low exposure time. Species and cultivar-level predictions
showed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high
consistency, while sunflower remained slightly higher variable (MAE = 3.80).
Among cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,
while Arian (2.86) and Ny\'{\i}rs\'{e}gi fekete (3.74) were comparatively
poorly captured. This framework was also embedded into MLflow, providing a
decision-support tool for optimizing CP seed germination in precision
agriculture.

</details>


### [30] [Semi-supervised and unsupervised learning for health indicator extraction from guided waves in aerospace composite structures](https://arxiv.org/abs/2510.24614)
*James Josep Perry,Pablo Garcia-Conde Ortiz,George Konstantinou,Cornelie Vergouwen,Edlyn Santha Kumaran,Morteza Moradi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Health indicators (HIs) are central to diagnosing and prognosing the
condition of aerospace composite structures, enabling efficient maintenance and
operational safety. However, extracting reliable HIs remains challenging due to
variability in material properties, stochastic damage evolution, and diverse
damage modes. Manufacturing defects (e.g., disbonds) and in-service incidents
(e.g., bird strikes) further complicate this process. This study presents a
comprehensive data-driven framework that learns HIs via two learning approaches
integrated with multi-domain signal processing. Because ground-truth HIs are
unavailable, a semi-supervised and an unsupervised approach are proposed: (i) a
diversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach
augmented with continuous auxiliary labels used as hypothetical damage proxies,
which overcomes the limitation of prior binary labels that only distinguish
healthy and failed states while neglecting intermediate degradation, and (ii) a
degradation-trend-constrained variational autoencoder (DTC-VAE), in which the
monotonicity criterion is embedded via an explicit trend constraint. Guided
waves with multiple excitation frequencies are used to monitor single-stiffener
composite structures under fatigue loading. Time, frequency, and time-frequency
representations are explored, and per-frequency HIs are fused via unsupervised
ensemble learning to mitigate frequency dependence and reduce variance. Using
fast Fourier transform features, the augmented Diversity-DeepSAD model achieved
81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%
performance, outperforming existing baselines.

</details>


### [31] [Quantum Machine Learning for Image Classification: A Hybrid Model of Residual Network with Quantum Support Vector Machine](https://arxiv.org/abs/2510.23659)
*Md. Farhan Shahriyar,Gazi Tanbhir,Abdullah Md Raihan Chy*

Main category: cs.LG

TL;DR: A hybrid quantum-classical framework using ResNet-50 for feature extraction, PCA for dimensionality reduction, and QSVM with Z/ZZ/Pauli-X quantum feature maps for potato disease image classification. The Z-feature map QSVM achieves 99.23% accuracy, outperforming classical SVM and RF in five-fold cross-validation.


<details>
  <summary>Details</summary>
Motivation: Explores the synergy between quantum machine learning and classical deep learning to tackle high-dimensional image data and potentially improve classification accuracy in plant disease detection.

Method: Extract deep features from RGB potato disease images using ResNet-50, reduce dimensionality with PCA, then classify with QSVM using multiple quantum feature maps (ZZ, Z, Pauli-X); compare against classical SVM and RF using five-fold stratified cross-validation.

Result: QSVM with the Z-feature map achieved 99.23% accuracy, outperforming both SVM and RF baselines.

Conclusion: Hybrid quantum-classical modeling can enhance image classification performance in plant disease detection, demonstrating the potential of integrating quantum computing into practical computer vision tasks.

Abstract: Recently, there has been growing attention on combining quantum machine
learning (QML) with classical deep learning approaches, as computational
techniques are key to improving the performance of image classification tasks.
This study presents a hybrid approach that uses ResNet-50 (Residual Network)
for feature extraction and Quantum Support Vector Machines (QSVM) for
classification in the context of potato disease detection. Classical machine
learning as well as deep learning models often struggle with high-dimensional
and complex datasets, necessitating advanced techniques like quantum computing
to improve classification efficiency. In our research, we use ResNet-50 to
extract deep feature representations from RGB images of potato diseases. These
features are then subjected to dimensionality reduction using Principal
Component Analysis (PCA). The resulting features are processed through QSVM
models which apply various quantum feature maps such as ZZ, Z, and Pauli-X to
transform classical data into quantum states. To assess the model performance,
we compared it with classical machine learning algorithms such as Support
Vector Machine (SVM) and Random Forest (RF) using five-fold stratified
cross-validation for comprehensive evaluation. The experimental results
demonstrate that the Z-feature map-based QSVM outperforms classical models,
achieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This
research highlights the advantages of integrating quantum computing into image
classification and provides a potential disease detection solution through
hybrid quantum-classical modeling.

</details>


### [32] [AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions](https://arxiv.org/abs/2510.23663)
*Padmanabhan Jagannathan Prajesh,Kaliaperumal Ragunath,Miriam Gordon,Bruce Rathgeber,Suresh Neethirajan*

Main category: cs.LG

TL;DR: ST-ViWT: a Spatiotemporal Vision Transformer with Wavelets that reconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 data, outperforming baselines and enabling policy-relevant CO2 mapping over agricultural regions.


<details>
  <summary>Details</summary>
Motivation: Need accurate, gap-filled, spatially explicit XCO2 maps with quantified uncertainty to support inventories, mitigation strategies, and verification in agricultural landscapes where data are sparse and heterogeneity is high.

Method: A Spatiotemporal Vision Transformer that fuses wavelet-based time-frequency representations with transformer attention across meteorology, vegetation indices, topography, and land cover. Trains on 2024 OCO-2 data; validated against TCCON; outputs 0.25-degree XCO2 surfaces with explicit uncertainties and gap-filling in sparse regions.

Result: On 2024 OCO-2 data, R2 = 0.984, RMSE = 0.468 ppm; 92.3% of gap-filled predictions within ±1 ppm. Independent TCCON validation shows bias = -0.14 ppm; r = 0.928. Captures late-summer drawdown. Across 14 poultry regions, XCO2 correlates with facility density (r = 0.43); high-density areas have larger seasonal amplitudes (9.57 ppm) and enhanced summer variability. Outperforms conventional interpolation and standard ML baselines, yielding seamless 0.25-degree CO2 surfaces with uncertainties, enabling year-round coverage.

Conclusion: Transformer-based Earth observation enables scalable, transparent, spatially explicit carbon accounting, hotspot prioritization, and policy-relevant mitigation assessment. The approach supports integration with inventories and precision livestock platforms to benchmark emissions and verify interventions.

Abstract: Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes
is essential for guiding emission mitigation strategies. We present a
Spatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that
reconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across
southern Canada, emphasizing poultry-intensive regions. The model fuses wavelet
time-frequency representations with transformer attention over meteorology,
vegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT
attains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions
lie within +/-1 ppm. Independent validation with TCCON shows robust
generalization (bias = -0.14 ppm; r = 0.928), including faithful reproduction
of the late-summer drawdown. Spatial analysis across 14 poultry regions reveals
a moderate positive association between facility density and XCO2 (r = 0.43);
high-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced
summer variability. Compared with conventional interpolation and standard
machine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces
with explicit uncertainties, enabling year-round coverage despite sparse
observations. The approach supports integration of satellite constraints with
national inventories and precision livestock platforms to benchmark emissions,
refine region-specific factors, and verify interventions. Importantly,
transformer-based Earth observation enables scalable, transparent, spatially
explicit carbon accounting, hotspot prioritization, and policy-relevant
mitigation assessment.

</details>


### [33] [Transformers from Compressed Representations](https://arxiv.org/abs/2510.23665)
*Juan C. Leon Alcazar,Mattia Soldan,Mohammad Saatialsoruji,Alejandro Pardo,Hani Itani,Juan Camilo Perez,Bernard Ghanem*

Main category: cs.LG

TL;DR: TEMPEST 通过利用压缩文件的字节流结构来设计 tokenizer/编码策略，使 transformer 可以直接从压缩数据流中学习语义表示，显著减少需要的 token 数量并降低内存与计算开销，同时保持与最先进模型相竞争的准确性。


<details>
  <summary>Details</summary>
Motivation: 在有效数据存储和传输的前提下，挖掘压缩格式中的表征学习潜力，避免对原始字节进行逐字处理或完整解码，提升效率。

Method: 利用压缩文件的字节流结构来设计 tokenization 与编码策略，使标准 transformer 直接处理压缩数据流，跳过解码步骤，减少 token 数量，从而实现高效的语义表示学习。

Result: 在多种数据集、编码方案和模态下进行广泛实验，TEMPEST 的准确性达到与当前最先进方法相当，同时在内存和计算上实现效率提升，显著降低 token 数量。

Conclusion: 证明了从压缩表示中进行语义学习的可行性与高效性，为压缩域的表示学习提供了一种新路径，TEMPEST 可作为高效的表示学习框架在相关任务中得到应用。

Abstract: Compressed file formats are the corner stone of efficient data storage and
transmission, yet their potential for representation learning remains largely
underexplored. We introduce TEMPEST (TransformErs froM comPressed
rEpreSenTations), a method that exploits the inherent byte-stream structure of
compressed files to design an effective tokenization and encoding strategy. By
leveraging this compact encoding, a standard transformer can directly learn
semantic representations from compressed data streams, bypassing the need for
raw byte-level processing or full media decoding. Our proposal substantially
reduces the number of tokens required for semantic classification, thereby
lowering both computational complexity and memory usage. Through extensive
experiments across diverse datasets, coding schemes, and modalities, we show
that TEMPEST achieves accuracy competitive wit the state-of-the-art while
delivering efficiency gains in memory and compute.

</details>


### [34] [Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization](https://arxiv.org/abs/2510.23667)
*Amin Heyrani Nobari,Lyle Regenwetter,Cyril Picard,Ligong Han,Faez Ahmed*

Main category: cs.LG

TL;DR: OAT is a foundation-model framework for topology optimization that predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. It uses a resolution- and shape-agnostic autoencoder, an implicit neural-field decoder, and a conditional latent-diffusion model trained on a large OpenTO dataset, enabling fast, resolution-free inference across diverse conditions.


<details>
  <summary>Details</summary>
Motivation: TO is computationally intensive due to complex physics and rigid constraints; existing DL methods are limited to fixed grids and boundary conditions, hindering general deployment.

Method: A resolution- and shape-agnostic autoencoder feeds into an implicit neural-field decoder; a conditional latent-diffusion model generates designs conditioned on problem parameters. Trained on OpenTO, a dataset with 2.2M optimized structures across 2M boundary-condition configurations.

Result: On four public benchmarks and two unseen tests, OAT achieves up to 90% relative improvement in mean compliance over the best prior models and delivers sub-1-second inference on a single GPU across resolutions from 64x64 to 256x256 and aspect ratios up to 10:1.

Conclusion: OAT is a general, fast, and resolution-free framework for physics-aware topology optimization and provides a large-scale dataset to spur further research in generative modeling for inverse design; code and data are publicly available.

Abstract: Structural topology optimization (TO) is central to engineering design but
remains computationally intensive due to complex physics and hard constraints.
Existing deep-learning methods are limited to fixed square grids, a few
hand-coded boundary conditions, and post-hoc optimization, preventing general
deployment. We introduce Optimize Any Topology (OAT), a foundation-model
framework that directly predicts minimum-compliance layouts for arbitrary
aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines
a resolution- and shape-agnostic autoencoder with an implicit neural-field
decoder and a conditional latent-diffusion model trained on OpenTO, a new
corpus of 2.2 million optimized structures covering 2 million unique
boundary-condition configurations. On four public benchmarks and two
challenging unseen tests, OAT lowers mean compliance up to 90% relative to the
best prior models and delivers sub-1 second inference on a single GPU across
resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These
results establish OAT as a general, fast, and resolution-free framework for
physics-aware topology optimization and provide a large-scale dataset to spur
further research in generative modeling for inverse design. Code & data can be
found at https://github.com/ahnobari/OptimizeAnyTopology.

</details>


### [35] [Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems](https://arxiv.org/abs/2510.23668)
*Fujiang Yuan,Yangrui Fan,Xiaohuan Bing,Zhen Tian,Chunhong Yuan,Yankang Li*

Main category: cs.LG

TL;DR: 基于 STL 分解的混合预测框架：将时间序列分解为趋势、季节和残差后，分别用 LSTM、ARIMA、XGBoost 进行建模，并采用乘法融合实现最终预测，在 NYC 交通流数据上显著优于单模型。


<details>
  <summary>Details</summary>
Motivation: 单一模型往往难以同时捕捉交通流数据的非线性和多尺度时间特征，需通过分解来隔离不同成分并让专门模型各自擅长，从而提高预测性能、可解释性与鲁棒性。

Method: 对原始时间序列使用 STL 进行趋势、季节和残差分解；用 LSTM 建模长期趋势，用 ARIMA 捕捉季节性周期性，用 XGBoost 预测非线性残差波动；最终通过乘法集成各子模型的预测。数据集为 2015 年 11 月至 12 月纽约市某路口的 998 条交通流记录。

Result: 混合模型在 MAE、RMSE、R^2 指标上显著优于单独的 LSTM、ARIMA 和 XGBoost。分解策略有效隔离时间特征，使各模型专业化，从而提高预测准确性、可解释性和鲁棒性。

Conclusion: 分解-协同建模的策略能有效提取时间序列的不同成分，提升交通流预测的性能与稳健性，且有助于对各组件的解释与分析。

Abstract: Accurate traffic flow forecasting is essential for intelligent transportation
systems and urban traffic management. However, single model approaches often
fail to capture the complex, nonlinear, and multi scale temporal patterns in
traffic flow data. This study proposes a decomposition driven hybrid framework
that integrates Seasonal Trend decomposition using Loess (STL) with three
complementary predictive models. STL first decomposes the original time series
into trend, seasonal, and residual components. Then, a Long Short Term Memory
(LSTM) network models long term trends, an Autoregressive Integrated Moving
Average (ARIMA) model captures seasonal periodicity, and an Extreme Gradient
Boosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The
final forecast is obtained through multiplicative integration of the sub model
predictions. Using 998 traffic flow records from a New York City intersection
between November and December 2015, results show that the LSTM ARIMA XGBoost
hybrid model significantly outperforms standalone models including LSTM, ARIMA,
and XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy
effectively isolates temporal characteristics, allowing each model to
specialize, thereby improving prediction accuracy, interpretability, and
robustness.

</details>


### [36] [DBLoss: Decomposition-based Loss Function for Time Series Forecasting](https://arxiv.org/abs/2510.23672)
*Xiangfei Qiu,Xingjian Wu,Hanyin Cheng,Xvyuan Liu,Chenjuan Guo,Jilin Hu,Bin Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time series forecasting holds significant value in various domains such as
economics, traffic, energy, and AIOps, as accurate predictions facilitate
informed decision-making. However, the existing Mean Squared Error (MSE) loss
function sometimes fails to accurately capture the seasonality or trend within
the forecasting horizon, even when decomposition modules are used in the
forward propagation to model the trend and seasonality separately. To address
these challenges, we propose a simple yet effective Decomposition-Based Loss
function called DBLoss. This method uses exponential moving averages to
decompose the time series into seasonal and trend components within the
forecasting horizon, and then calculates the loss for each of these components
separately, followed by weighting them. As a general loss function, DBLoss can
be combined with any deep learning forecasting model. Extensive experiments
demonstrate that DBLoss significantly improves the performance of
state-of-the-art models across diverse real-world datasets and provides a new
perspective on the design of time series loss functions.

</details>


### [37] [Informed Initialization for Bayesian Optimization and Active Learning](https://arxiv.org/abs/2510.23681)
*Carl Hvarfner,David Eriksson,Eytan Bakshy,Max Balandat*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Bayesian Optimization is a widely used method for optimizing expensive
black-box functions, relying on probabilistic surrogate models such as Gaussian
Processes. The quality of the surrogate model is crucial for good optimization
performance, especially in the few-shot setting where only a small number of
batches of points can be evaluated. In this setting, the initialization plays a
critical role in shaping the surrogate's predictive quality and guiding
subsequent optimization. Despite this, practitioners typically rely on
(quasi-)random designs to cover the input space. However, such approaches
neglect two key factors: (a) space-filling designs may not be desirable to
reduce predictive uncertainty, and (b) efficient hyperparameter learning during
initialization is essential for high-quality prediction, which may conflict
with space-filling designs. To address these limitations, we propose
Hyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition
strategy that balances predictive uncertainty reduction with hyperparameter
learning using information-theoretic principles. We derive a closed-form
expression for HIPE in the Gaussian Process setting and demonstrate its
effectiveness through extensive experiments in active learning and few-shot BO.
Our results show that HIPE outperforms standard initialization strategies in
terms of predictive accuracy, hyperparameter identification, and subsequent
optimization performance, particularly in large-batch, few-shot settings
relevant to many real-world Bayesian Optimization applications.

</details>


### [38] [Parallel BiLSTM-Transformer networks for forecasting chaotic dynamics](https://arxiv.org/abs/2510.23685)
*Junwen Ma,Mingyu Ge,Yisen Wang,Yong Zhang,Weicheng Fu*

Main category: cs.LG

TL;DR: 提出一种Transformer与BiLSTM并行混合框架用于混沌系统预测，通过双分支捕获全局依赖和局部时序特征，并在Lorenz系统的自主演化预测与未观测变量推断两任务上优于单分支模型。


<details>
  <summary>Details</summary>
Motivation: 混沌时间序列对初始条件极度敏感，传统方法难以同时捕捉局部特征和全局依赖，因此需要一个能整合两者的预测框架。

Method: 双分支架构，Transformer分支提取长程依赖，BiLSTM分支提取局部时序特征，在特征融合层进行融合。使用状态向量的时滞嵌入，进行自回归外推和对部分观测的未观测变量重建。两个任务：自治演化预测和未观测变量推断。

Result: 实验在Lorenz系统上表明该混合框架在两个任务上均优于单分支模型，具有更高的长期跟踪精度与稳定性，以及更强的鲁棒性和泛化能力。

Conclusion: 将Transformer与BiLSTM的互补表示融合的混合框架对混沌系统预测具有效果，验证了将全局和局部特征结合的策略的有效性。

Abstract: The nonlinear nature of chaotic systems results in extreme sensitivity to
initial conditions and highly intricate dynamical behaviors, posing fundamental
challenges for accurately predicting their evolution. To overcome the
limitation that conventional approaches fail to capture both local features and
global dependencies in chaotic time series simultaneously, this study proposes
a parallel predictive framework integrating Transformer and Bidirectional Long
Short-Term Memory (BiLSTM) networks. The hybrid model employs a dual-branch
architecture, where the Transformer branch mainly captures long-range
dependencies while the BiLSTM branch focuses on extracting local temporal
features. The complementary representations from the two branches are fused in
a dedicated feature-fusion layer to enhance predictive accuracy. As
illustrating examples, the model's performance is systematically evaluated on
two representative tasks in the Lorenz system. The first is autonomous
evolution prediction, in which the model recursively extrapolates system
trajectories from the time-delay embeddings of the state vector to evaluate
long-term tracking accuracy and stability. The second is inference of
unmeasured variable, where the model reconstructs the unobserved states from
the time-delay embeddings of partial observations to assess its
state-completion capability. The results consistently indicate that the
proposed hybrid framework outperforms both single-branch architectures across
tasks, demonstrating its robustness and effectiveness in chaotic system
prediction.

</details>


### [39] [On the Societal Impact of Machine Learning](https://arxiv.org/abs/2510.23693)
*Joachim Baumann*

Main category: cs.LG

TL;DR: 本论文（博士论文）关注机器学习的社会影响，聚焦公平性测量、偏见动态的系统性分解，以及在保持系统效用的前提下减少算法歧视的干预策略；最后讨论挑战与未来研究方向，特别是在生成式AI日益融入社会的背景下的合价值取向。


<details>
  <summary>Details</summary>
Motivation: ML系统越来越多地参与关乎后果的决策，若缺乏明确的公平性考量，可能产生歧视性影响，因此需要更合适的公平性测量、对系统偏见的预测与理解，以及有效的干预手段。

Method: 提出用于更恰当地测量公平性的框架、对ML系统进行系统性分解以预判偏见动态、以及设计在保持系统效用的同时减少算法歧视的干预策略；并在未来工作中讨论将这些方法应用于包括生成式AI的场景。

Result: 为ML系统的公平性评估提供基础性方法、提供对偏见产生机制的系统性分解，以及实现减少歧视的干预手段，同时兼顾系统效用。

Conclusion: 指出当前仍存在挑战并提出未来研究方向，强调随着生成式人工智能等技术融入社会，需进一步确保ML的社会影响与更广泛的社会价值相一致。

Abstract: This PhD thesis investigates the societal impact of machine learning (ML). ML
increasingly informs consequential decisions and recommendations, significantly
affecting many aspects of our lives. As these data-driven systems are often
developed without explicit fairness considerations, they carry the risk of
discriminatory effects. The contributions in this thesis enable more
appropriate measurement of fairness in ML systems, systematic decomposition of
ML systems to anticipate bias dynamics, and effective interventions that reduce
algorithmic discrimination while maintaining system utility. I conclude by
discussing ongoing challenges and future research directions as ML systems,
including generative artificial intelligence, become increasingly integrated
into society. This work offers a foundation for ensuring that ML's societal
impact aligns with broader social values.

</details>


### [40] [MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs for Multimodal Sarcasm Detection](https://arxiv.org/abs/2510.23727)
*Anisha Saha,Varsha Suresh,Timothy Hospedales,Vera Demberg*

Main category: cs.LG

TL;DR: 提出 MUStReason 诊断基准用于评估多模态视频语言模型在讽刺检测中的感知与推理能力，并提出 PragCoT 框架以引导模型聚焦隐含意图而非字面意义。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测需要跨模态线索和语用推理，但现有多模态模型在识别相关线索与推理方面表现不足。

Method: 构建带有模态特定线索与推理步骤注释的诊断基准 MUStReason；对 VideoLMs 在讽刺分类上的性能进行基准测试并对生成推理进行定量与定性评估，进一步提出 PragCoT 用于引导推理。

Result: 通过将任务分解为感知与推理，揭示模型在推理层面的不足，并给出定性分析；PragCoT 能提升模型将注意力聚焦于隐含意图的能力。

Conclusion: MUStReason 与 PragCoT 提供了一条评估与提升多模态讽刺推理能力的路径，强调对隐含意图的关注。

Abstract: Sarcasm is a specific type of irony which involves discerning what is said
from what is meant. Detecting sarcasm depends not only on the literal content
of an utterance but also on non-verbal cues such as speaker's tonality, facial
expressions and conversational context. However, current multimodal models
struggle with complex tasks like sarcasm detection, which require identifying
relevant cues across modalities and pragmatically reasoning over them to infer
the speaker's intention. To explore these limitations in VideoLMs, we introduce
MUStReason, a diagnostic benchmark enriched with annotations of
modality-specific relevant cues and underlying reasoning steps to identify
sarcastic intent. In addition to benchmarking sarcasm classification
performance in VideoLMs, using MUStReason we quantitatively and qualitatively
evaluate the generated reasoning by disentangling the problem into perception
and reasoning, we propose PragCoT, a framework that steers VideoLMs to focus on
implied intentions over literal meaning, a property core to detecting sarcasm.

</details>


### [41] [Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction](https://arxiv.org/abs/2510.23794)
*Jun Liu,Tao Zhou,Jiarui Li,Xiaohui Zhong,Peng Zhang,Jie Feng,Lei Chen,Hao Li*

Main category: cs.LG

TL;DR:  FuXi-ENS以可学习扰动来生成集合预测，与ECMWF-ENS在2018年全球90个热带气旋的对比中，在TC相关变量、轨迹和强度预测以及动力热场方面表现出明显优势，且集合方差更紧凑。


<details>
  <summary>Details</summary>
Motivation:  解决传统集合预报在高计算成本和无法充分表达大气非线性方面的局限性，提出基于AI的可学习扰动来改进热带气旋集合预测的潜力。

Method:  使用2018年全球90个热带气旋的全样本数据，系统性比较FuXi-ENS与ECMWF-ENS在TC相关物理变量、轨迹与强度预测及其相关的动力与热力场表现。评估集合预测的不确定性、强度偏差及大尺度环流及湿度能量分布特征。

Result:  FuXi-ENS在TC相关物理变量预测、轨迹预测准确性及集合散度（更小的集合扩展）方面优于ECMWF-ENS；但对强度的预测仍显著低于观测值。动态与热力分析显示FuXi-ENS更好地捕捉大尺度环流，湿润湍变能在热带气旋暖核周围分布更集中，而ECMWF-ENS呈现更分散的分布。

Conclusion:  使用可学习扰动的AI集合预测方法有望提升热带气旋预报技能，并为基于AI的极端天气集合预测提供有价值的见解与实践路径。

Abstract: Tropical cyclones (TCs) are highly destructive and inherently uncertain
weather systems. Ensemble forecasting helps quantify these uncertainties, yet
traditional systems are constrained by high computational costs and limited
capability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a
learnable perturbation scheme for ensemble generation, representing a novel
AI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with
ECMWF-ENS using all 90 global TCs in 2018, examining their performance in
TC-related physical variables, track and intensity forecasts, and the
associated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear
advantages in predicting TC-related physical variables, and achieves more
accurate track forecasts with reduced ensemble spread, though it still
underestimates intensity relative to observations. Further dynamical and
thermodynamical analyses reveal that FuXi-ENS better captures large-scale
circulation, with moisture turbulent energy more tightly concentrated around
the TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.
These findings highlight the potential of learnable perturbations to improve TC
forecasting skill and provide valuable insights for advancing AI-based ensemble
prediction of extreme weather events that have significant societal impacts.

</details>


### [42] [Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders](https://arxiv.org/abs/2510.23802)
*Nathan Paek,Yongyi Zang,Qihui Yang,Randal Leistikow*

Main category: cs.LG

TL;DR: 提出一种将稀疏自编码器与音频生成模型相结合的解释框架：在音频自编码器潜在表征上训练稀疏自编码器，并从SAE特征映射到离散化的声学属性（音高、幅度、音色），实现对潜在生成过程的可控操作与分析。对连续与离散潜在空间均进行验证，并分析DiffRhythm等模型以揭示在合成过程中的声学属性演化。该框架虽仅在音频模态上开展，但具备扩展到视觉潜在空间分析的潜力。


<details>
  <summary>Details</summary>
Motivation: 音频的高密度性质使得压缩可能模糊语义，且自动特征表征仍然受限，导致对音频生成模型的可解释性不足。需要将模型潜在表征映射到人类可理解的声学概念，以实现对AI音乐生成过程的可控性与分析。

Method: 在音频自编码器潜在表示上训练稀疏自编码器（SAEs），再学习从SAE特征到离散化声学属性（音高、幅度、音色）的线性映射。对连续（DiffRhythm-VAE）和离散（EnCodec、WavTokenizer）潜在空间进行验证，并分析DiffRhythm中音高、音色、响度在生成过程中的演变。

Result: 实现对AI音乐生成过程的可控操控与分析，揭示在合成过程中声学属性的出现与演化。验证对象涵盖连续与离散潜在空间的主流模型，且以DiffRhythm等文本到音乐模型为分析对象，展示不同声学属性在生成过程中的变化趋势。

Conclusion: 给出了一个可扩展的音频模态解释框架，未来可拓展至可视化潜在空间的分析，帮助理解其他视觉或跨模态的潜在生成模型的声学/语义关系。

Abstract: While sparse autoencoders (SAEs) successfully extract interpretable features
from language models, applying them to audio generation faces unique
challenges: audio's dense nature requires compression that obscures semantic
meaning, and automatic feature characterization remains limited. We propose a
framework for interpreting audio generative models by mapping their latent
representations to human-interpretable acoustic concepts. We train SAEs on
audio autoencoder latents, then learn linear mappings from SAE features to
discretized acoustic properties (pitch, amplitude, and timbre). This enables
both controllable manipulation and analysis of the AI music generation process,
revealing how acoustic properties emerge during synthesis. We validate our
approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer)
audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music
model, to demonstrate how pitch, timbre, and loudness evolve throughout
generation. While our work is only done on audio modality, our framework can be
extended to interpretable analysis of visual latent space generation models.

</details>


### [43] [How do simple rotations affect the implicit bias of Adam?](https://arxiv.org/abs/2510.23804)
*Adela DePavia,Vasileios Charisopoulos,Rebecca Willett*

Main category: cs.LG

TL;DR: 自适应梯度方法（如 Adam、Adagrad）的泛化效应依赖于数据表征；在二元分类中存在“丰富性偏置”，但对坐标预处理敏感，数据的正交旋转可能消除该偏置并使 Adam 的边界比梯度下降更差；通过引入一种正交变换的再参数化可使优化目标对数据旋转等变，从而恢复 Adam 的丰富性偏置。


<details>
  <summary>Details</summary>
Motivation: 理解自适应梯度方法对泛化的影响，尤其是在数据表征（旋转等正交变换）下；揭示 Adam 的丰富性偏置为何会因为坐标系变换而消失，以及如何通过重参数化实现等变性。

Method: 对 Adam、Adagrad 在二分类任务中的表现进行理论分析与实验验证，研究其坐标预条件化的影响；提出对优化目标进行正交变换的再参数化方法，使第一阶方法对数据旋转保持等变；通过实验证明该再参数化可以恢复 Adam 的丰富性偏置。

Result: 发现即使是很小的旋转也能使 Adam 失去相对于梯度下降的优势，趋向学习出距离 Bayes 判决边界更远的线性边界；所提出的再参数化方法让任何一阶方法具有对数据旋转的等变性，并在实验中证实其能恢复 Adam 的丰富性偏置。

Conclusion: 自适应梯度方法的对数据表征敏感性是一个现实问题；通过简单的正交变换再参数化可以提高一阶优化器对旋转的鲁棒性并恢复更接近 Bayes 边界的决策边界，强调在设计优化算法时需要考虑数据表示的影响。

Abstract: Adaptive gradient methods such as Adam and Adagrad are widely used in machine
learning, yet their effect on the generalization of learned models -- relative
to methods like gradient descent -- remains poorly understood. Prior work on
binary classification suggests that Adam exhibits a ``richness bias,'' which
can help it learn nonlinear decision boundaries closer to the Bayes-optimal
decision boundary relative to gradient descent. However, the coordinate-wise
preconditioning scheme employed by Adam renders the overall method sensitive to
orthogonal transformations of feature space. We show that this sensitivity can
manifest as a reversal of Adam's competitive advantage: even small rotations of
the underlying data distribution can make Adam forfeit its richness bias and
converge to a linear decision boundary that is farther from the Bayes-optimal
decision boundary than the one learned by gradient descent. To alleviate this
issue, we show that a recently proposed reparameterization method -- which
applies an orthogonal transformation to the optimization objective -- endows
any first-order method with equivariance to data rotations, and we empirically
demonstrate its ability to restore Adam's bias towards rich decision
boundaries.

</details>


### [44] [A Physics-informed Multi-resolution Neural Operator](https://arxiv.org/abs/2510.23810)
*Sumanta Roy,Bahador Bahmani,Ioannis G. Kevrekidis,Michael D. Shields*

Main category: cs.LG

TL;DR: 提出了一种完全数据无关、分辨率无关的物理信息算子学习框架：将输入场投影到潜在嵌入空间，由MLP近似算子，并通过物理空间的有限差分求解器强制 PDE，适用于多分辨率数据的数值验证。


<details>
  <summary>Details</summary>
Motivation: 解决高保真训练数据稀缺与不同样本网格分辨率不一致的问题，使算子学习在缺乏大量数据的情况下仍能泛化，并且跨不同离散化尺度进行推理。

Method: 在RINO框架基础上，提出一个完全数据无关的方案：将任意分辨率的输入函数投影到有限维向量空间的潜在编码，利用MLP近似算子，输入为潜在编码和时空坐标，输出物理空间解；在物理空间通过有限差分求解器强制 PDE。

Result: 在多尺度数据的数值示例中验证，输入在不同分辨率下采样（从粗到精），方法仍能有效工作并给出解。

Conclusion: 提出了一种数据无关、分辨率无关的物理信息算子学习框架，通过潜在编码与MLP近似算子结合、并在物理空间以FD求解器强制 PDE，可对多分辨率数据进行鲁棒的算子学习与推理。

Abstract: The predictive accuracy of operator learning frameworks depends on the
quality and quantity of available training data (input-output function pairs),
often requiring substantial amounts of high-fidelity data, which can be
challenging to obtain in some real-world engineering applications. These
datasets may be unevenly discretized from one realization to another, with the
grid resolution varying across samples. In this study, we introduce a
physics-informed operator learning approach by extending the Resolution
Independent Neural Operator (RINO) framework to a fully data-free setup,
addressing both challenges simultaneously. Here, the arbitrarily (but
sufficiently finely) discretized input functions are projected onto a latent
embedding space (i.e., a vector space of finite dimensions), using pre-trained
basis functions. The operator associated with the underlying partial
differential equations (PDEs) is then approximated by a simple multi-layer
perceptron (MLP), which takes as input a latent code along with spatiotemporal
coordinates to produce the solution in the physical space. The PDEs are
enforced via a finite difference solver in the physical space. The validation
and performance of the proposed method are benchmarked on several numerical
examples with multi-resolution data, where input functions are sampled at
varying resolutions, including both coarse and fine discretizations.

</details>


### [45] [Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial Processes](https://arxiv.org/abs/2510.23817)
*Pedro Cortes dos Santos,Matheus Becali Rocha,Renato A Krohling*

Main category: cs.LG

TL;DR: 通过SHAP与因果DAG的结合，在Tennessee Eastman工艺中提升故障检测的可解释性与准确性，并揭示关键工艺要素（如冷却与分离系统）对故障的作用。


<details>
  <summary>Details</summary>
Motivation: 解决复杂工业过程中的故障检测在性能与可解释性上的瓶颈；通过将可解释性分析（SHAP）与因果推断（DAG）结合，形成更透明且具操作性的监控框架。

Method: 1) 使用标准模型进行初步故障检测，发现性能与解释性不足；2) 使用SHAP确定关键特征，转化问题为更易管理的形式；3) 基于SHAP结果，应用多算法生成的有向无环图进行因果分析，揭示故障传播机制并验证与SHAP的一致性。

Result: SHAP与DAG所得结果一致，显著指出如冷却与分离系统等关键工艺要素在故障发展中的作用，提升检测准确性并提供可操作的故障源洞察。

Conclusion: 提出将预测性能力与因果理解相结合的鲁棒故障检测框架，为复杂制造环境的监控提供更清晰、可操作的解释，并推动工业系统更智能、可解释的故障检测实践。

Abstract: Industrial processes generate complex data that challenge fault detection
systems, often yielding opaque or underwhelming results despite advanced
machine learning techniques. This study tackles such difficulties using the
Tennessee Eastman Process, a well-established benchmark known for its intricate
dynamics, to develop an innovative fault detection framework. Initial attempts
with standard models revealed limitations in both performance and
interpretability, prompting a shift toward a more tractable approach. By
employing SHAP (SHapley Additive exPlanations), we transform the problem into a
more manageable and transparent form, pinpointing the most critical process
features driving fault predictions. This reduction in complexity unlocks the
ability to apply causal analysis through Directed Acyclic Graphs, generated by
multiple algorithms, to uncover the underlying mechanisms of fault propagation.
The resulting causal structures align strikingly with SHAP findings,
consistently highlighting key process elements-like cooling and separation
systems-as pivotal to fault development. Together, these methods not only
enhance detection accuracy but also provide operators with clear, actionable
insights into fault origins, a synergy that, to our knowledge, has not been
previously explored in this context. This dual approach bridges predictive
power with causal understanding, offering a robust tool for monitoring complex
manufacturing environments and paving the way for smarter, more interpretable
fault detection in industrial systems.

</details>


### [46] [ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning](https://arxiv.org/abs/2510.23818)
*Yilang Zhang,Xiaodong Yang,Yiwei Cai,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 通过将LoRA增量高秩累积并对原低秩矩阵的列进行线性缩放，得到可解析的最优高秩近似以替代逐步全微调，从而提升大模型的微调效果与收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在大模型任务特定微调中，LoRA将更新限制在低秩子空间，导致性能与收敛受限；提出一种高秩增量累积策略，兼顾效率与效果。

Method: 每次更新中识别能最小化损失的最优低秩矩阵，并通过对原始低秩矩阵的列进行缩放来构造高秩更新；证明缩放系数可解析求解且无重启地进行优化；在多达12B参数的模型上对比LoRA变体进行广泛评估。

Result: 在多任务领域（自然语言理解、常识推理、数学问题求解）以及规模高达12B的模型上，方法比现有LoRA变体具有更好的性能与更快的收敛。

Conclusion: 高秩更新累积结合线性列缩放提供了一种理论上可解析的高效微调途径，可在不显著增加计算成本的前提下提升LoRA的效果与收敛速度。

Abstract: As large language models (LLMs) continue to scale in size, the computational
overhead has become a major bottleneck for task-specific fine-tuning. While
low-rank adaptation (LoRA) effectively curtails this cost by confining the
weight updates to a low-dimensional subspace, such a restriction can hinder
effectiveness and slow convergence. This contribution deals with these
limitations by accumulating progressively a high-rank weight update from
consecutive low-rank increments. Specifically, the per update optimal low-rank
matrix is identified to minimize the loss function and closely approximate full
fine-tuning. To endow efficient and seamless optimization without restarting,
this optimal choice is formed by appropriately scaling the columns of the
original low-rank matrix. Rigorous performance guarantees reveal that the
optimal scaling can be found analytically. Extensive numerical tests with
popular LLMs scaling up to 12 billion parameters demonstrate a consistent
performance gain and fast convergence relative to state-of-the-art LoRA
variants on diverse tasks including natural language understanding, commonsense
reasoning, and mathematical problem solving.

</details>


### [47] [A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling](https://arxiv.org/abs/2510.23866)
*Paul Rosu,Muchang Bahng,Erick Jiang,Rico Zhu,Vahid Tarokh*

Main category: cs.LG

TL;DR: 将物理条件引入潜在扩散模型，用于大气数据的动态尺度下放，目标重建高分辨率的2米温度场。通过残差连接到现有UNet，并在训练目标中加入PDE损失，PDE在像素空间解码潜在表征后计算，利用有限差分近似的有效平衡以约束对流-扩散物理关系。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率大气场景下的物理一致性问题与数据驱动下放之间的矛盾，利用PDE约束提高生成场的物理可信度，同时保持扩散模型的建模能力。

Method: 在现有扩散架构基础上引入对参考UNet的残差形式；训练目标中加入PDE损失，该损失在全分辨率像素空间解码潜在表征后计算，通过有限差分近似对对流-扩散平衡进行约束；并在训练中对模型进行微调以增强物理一致性。

Result: 实验观测表明，常规扩散训练已能在很大程度上实现较低的PDE残差；在此基础上加入PDE损失可进一步正则化模型并提升生成场的物理相容性；代码库公开在Github以供复现与进一步开发。

Conclusion: 将PDE层面的物理约束整合到条件扩散模型中，可提升下放得到的高分辨率大气场的物理一致性，且提供可复现的开源实现。

Abstract: This work presents a physics-conditioned latent diffusion model tailored for
dynamical downscaling of atmospheric data, with a focus on reconstructing
high-resolution 2-m temperature fields. Building upon a pre-existing diffusion
architecture and employing a residual formulation against a reference UNet, we
integrate a partial differential equation (PDE) loss term into the model's
training objective. The PDE loss is computed in the full resolution (pixel)
space by decoding the latent representation and is designed to enforce physical
consistency through a finite-difference approximation of an effective
advection-diffusion balance. Empirical observations indicate that conventional
diffusion training already yields low PDE residuals, and we investigate how
fine-tuning with this additional loss further regularizes the model and
enhances the physical plausibility of the generated fields. The entirety of our
codebase is available on Github, for future reference and development.

</details>


### [48] [GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA](https://arxiv.org/abs/2510.23868)
*Zhichao Wang*

Main category: cs.LG

TL;DR: GIFT提出一种组相关隐式微调框架，通过将隐式奖励与显式奖励归一化并最小化它们之间的差异，转化为MSE目标，从而实现高效、稳健且具有探索性的对齐优化。


<details>
  <summary>Details</summary>
Motivation: 解决现有RLHF/隐式奖励方法在理论与实践中的挑战：非凸优化、隐式奖励中的不可控项、离线方法缺乏探索、以及需要更少超参数和更好泛化。

Method: 结合GRPO的在线多重响应生成与归一化、DPO的隐式奖励表述、以及UNA的隐式-显式奖励对齐三者，提出GIFT。通过归一化消除不可积项，将目标转化为对归一化后的奖励函数的均平方误差（MSE），实现关于策略的对齐优化。保持在策略的探索性。与GRPO相比，超参数更少、收敛更快、泛化更好、训练过拟合显著减少。

Result: 在数学推理任务等基准上，GIFT在推理和对齐方面表现优越，计算效率高，训练稳定。

Conclusion: GIFT将对齐优化从非凸问题转化为简单的MSE最小化，提供一个在策略在线、有效探索与隐式显式奖励对齐之间的折中方案，增强了对LLM对齐的可行性与泛化性，同时对减少超参数具有潜在意义。

Abstract: I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine
\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning
LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT
minimizes the discrepancy between implicit and explicit reward models. It
combines three key ideas: (1) the online multi-response generation and
normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the
implicit-explicit reward alignment principle of UNA. By jointly normalizing the
implicit and explicit rewards, GIFT eliminates an otherwise intractable term
that prevents effective use of implicit rewards. This normalization transforms
the complex reward maximization objective into a simple mean squared error
(MSE) loss between the normalized reward functions, converting a non-convex
optimization problem into a convex, stable, and analytically differentiable
formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy
and thus retains exploration capability. Compared to GRPO, it requires fewer
hyperparameters, converges faster, and generalizes better with significantly
reduced training overfitting. Empirically, GIFT achieves superior reasoning and
alignment performance on mathematical benchmarks while remaining
computationally efficient.

</details>


### [49] [RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal Regression Trees](https://arxiv.org/abs/2510.23901)
*Cristobal Heredia,Pedro Chumpitaz-Flores,Kaixun Hua*

Main category: cs.LG

TL;DR: 提出了一种名为 RS-ORT 的Reduced-Space Optimal Regression Trees 的新方法，用专门的分支定界（BB）算法在减少的空间中对回归树进行最优学习，显著提升大尺度数据上的可扩展性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于混合整数规划（MIP）的回归树学习在连续特征和大规模数据上要么受限于二值化要么计算不可行。需要一种在保证全局最优性的同时，能处理百万级数据的高效方法。

Method: 将最优回归树训练问题建模为两阶段优化，采用仅对树结构变量进行分支的 BB 思路；利用结构特性实现多种界 Tightening（闭式叶预测、经验阈值离散化、深度-1 子树解析），并辅以可分解的上界/下界与节点级并行，构建减少尺度的容错系统，训练与推理的时间复杂度与样本量正相关性减弱。

Result: 在混合特征（包含二值和连续特征）的多种回归基准上，RS-ORT 显著优于现有方法；在高达200万样本、包含连续特征的数据集上，能够在四小时内获得带有简单树形结构的保证性训练性能并具备更好的泛化能力。

Conclusion: RS-ORT 为大规模、连续特征的回归树学习提供了一个可扩展、可保证的优化框架，通过结构化的分支策略与多种界价改进，实现高效并行和优良泛化。

Abstract: Mixed-integer programming (MIP) has emerged as a powerful framework for
learning optimal decision trees. Yet, existing MIP approaches for regression
tasks are either limited to purely binary features or become computationally
intractable when continuous, large-scale data are involved. Naively binarizing
continuous features sacrifices global optimality and often yields needlessly
deep trees. We recast the optimal regression-tree training as a two-stage
optimization problem and propose Reduced-Space Optimal Regression Trees
(RS-ORT) - a specialized branch-and-bound (BB) algorithm that branches
exclusively on tree-structural variables. This design guarantees the
algorithm's convergence and its independence from the number of training
samples. Leveraging the model's structure, we introduce several bound
tightening techniques - closed-form leaf prediction, empirical threshold
discretization, and exact depth-1 subtree parsing - that combine with
decomposable upper and lower bounding strategies to accelerate the training.
The BB node-wise decomposition enables trivial parallel execution, further
alleviating the computational intractability even for million-size datasets.
Based on the empirical studies on several regression benchmarks containing both
binary and continuous features, RS-ORT also delivers superior training and
testing performance than state-of-the-art methods. Notably, on datasets with up
to 2,000,000 samples with continuous features, RS-ORT can obtain guaranteed
training performance with a simpler tree structure and a better generalization
ability in four hours.

</details>


### [50] [Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers](https://arxiv.org/abs/2510.23912)
*Marko Karbevski,Antonij Mijoski*

Main category: cs.LG

TL;DR: 在注意力的 Query-Key-Value 三元组中，Query 权重可冗余，降低非嵌入/LM头参数≥8%，在从头训练的 GPT-3 小型架构上仍保持与标准基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: 理解 Query 权重的冗余性，探索更高效的注意力表示及其在大规模模型中的潜在扩展性。

Method: 进行理论分析（在简化假设下），并在包含层归一化、跳跃连接、权衰减等条件的完整复杂度 GPT-3 小型架构上从头训练进行验证，比较简化模型与基线的表现。

Result: 理论上证明 Query 权重冗余；实证上简化模型在验证损失上与标准基线相当。

Conclusion: 该结果推动在大规模模型中对 Query 权重冗余性的进一步研究，促进参数高效的注意力设计与扩展。

Abstract: The Query, Key, Value weight triplet is a building block of current attention
mechanisms in state-of-the-art LLMs. We theoretically investigate whether this
triplet can be reduced, proving under simplifying assumptions that the Query
weights are redundant, thereby reducing the number of non-embedding/lm-head
parameters by over 8%. We validate the theory on full-complexity GPT-3 small
architectures (with layer normalization, skip connections, and weight decay)
trained from scratch, demonstrating that the reduced model achieves comparable
validation loss to standard baselines. These findings motivate the
investigation of the Query weight redundancy at scale.

</details>


### [51] [Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs](https://arxiv.org/abs/2510.23914)
*Arsenii Mustafin,Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 提出统一折扣与平均奖励MDP的几何解释框架，并在唯一且遍历的最优策略下证明平均奖励问题中的值迭代具有几何收敛速率。


<details>
  <summary>Details</summary>
Motivation: 通过将折扣奖励情形的几何解释扩展到平均奖励情形，统一两类MDP的分析，并提升对值迭代收敛性的理解。

Method: 扩展折扣情形的几何解释到平均奖励情形，建立统一分析框架；在唯一且遍历的最优策略条件下，推导值迭代的几何收敛率。

Result: 在上述条件下，平均奖励MDP的值迭代实现几何收敛，类似于折扣情形的结果。

Conclusion: 该工作实现平均奖励与折扣奖励分析的统一，展示了所提出几何框架的适用性，并将折扣情形的重要性质推广到平均奖励情形。

Abstract: The theoretical analysis of Markov Decision Processes (MDPs) is commonly
split into two cases - the average-reward case and the discounted-reward case -
which, while sharing similarities, are typically analyzed separately. In this
work, we extend a recently introduced geometric interpretation of MDPs for the
discounted-reward case to the average-reward case, thereby unifying both. This
allows us to extend a major result known for the discounted-reward case to the
average-reward case: under a unique and ergodic optimal policy, the Value
Iteration algorithm achieves a geometric convergence rate.

</details>


### [52] [ChessQA: Evaluating Large Language Models for Chess Understanding](https://arxiv.org/abs/2510.23948)
*Qianfeng Wen,Zhenwei Tang,Ashton Anderson*

Main category: cs.LG

TL;DR: ChessQA 是一个全面且动态的基准，用于评估大语言模型在棋艺理解中的五类任务（结构、技巧、短战术、位置判断、语义），超越简单的走棋质量评价，能够诊断不同规模、训练后方法和架构对棋理解的影响，并提供代码、数据集和排行榜以支持持续研究。


<details>
  <summary>Details</summary>
Motivation: 现有对棋类能力的评估多为零散且领域局限，难以准确衡量模型在棋理推理、建模与抽象上的能力，以及随模型规模、后训练方法或架构变化的差异。需要一个能够覆盖棋知识演进阶段的综合性基准，以便在可控、可比较的实验环境中诊断和比较模型表现。

Method: 构建包含五大任务类别的基准 ChessQA：结构（Structural）、主题/技巧模式（Motifs）、短期战术（Short Tactics）、位置判断（Position Judgment）和语义理解（Semantic）。任务设计大致对应棋手从理解基本规则、学习战术模式，到正确计算战术、评估棋局以及语义描述高层概念的逐步抽象过程。基准具备动态特性，提示、答案键和构建脚本可随模型进步而演化。对一系列当代 LLMs 进行评测，提供按类别的错误分析和结果。研究方还将发布代码、定期刷新数据集以及公开排行榜。

Result: 在所有五类任务中，当前的各大模型均表现出持续的弱点，存在普遍性不足，并给出按类别的错误分析与结果。基准的评测覆盖五类抽象层级，能够揭示模型在不同棋类理解维度的瓶颈。研究还展示了动态、可演化的评测生态，便于随模型改进持续对比。

Conclusion: ChessQA 提供一个更全面的诊断性工具，便于对不同模型、培训方法和架构在棋理解上的差异进行系统比较与诊断。由于其动态特性，该基准能够随模型进步持续演化，促进对棋理理解的深入研究与改进。

Abstract: Chess provides an ideal testbed for evaluating the reasoning, modeling, and
abstraction capabilities of large language models (LLMs), as it has
well-defined structure and objective ground truth while admitting a wide
spectrum of skill levels. However, existing evaluations of LLM ability in chess
are ad hoc and narrow in scope, making it difficult to accurately measure LLM
chess understanding and how it varies with scale, post-training methodologies,
or architecture choices. We present ChessQA, a comprehensive benchmark that
assesses LLM chess understanding across five task categories (Structural,
Motifs, Short Tactics, Position Judgment, and Semantic), which approximately
correspond to the ascending abstractions that players master as they accumulate
chess knowledge, from understanding basic rules and learning tactical motifs to
correctly calculating tactics, evaluating positions, and semantically
describing high-level concepts. In this way, ChessQA captures a more
comprehensive picture of chess ability and understanding, going significantly
beyond the simple move quality evaluations done previously, and offers a
controlled, consistent setting for diagnosis and comparison. Furthermore,
ChessQA is inherently dynamic, with prompts, answer keys, and construction
scripts that can evolve as models improve. Evaluating a range of contemporary
LLMs, we find persistent weaknesses across all five categories and provide
results and error analyses by category. We will release the code, periodically
refreshed datasets, and a public leaderboard to support further research.

</details>


### [53] [A Pragmatic Way to Measure Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.23966)
*Scott Emmons,Roland S. Zimmermann,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 提出通过 autorater 提示来量化 Chain-of-Thought 的 legibility（可被人理解的程度）与 coverage（是否覆盖必要推理），并用对现有 CoT 的评估来监控模型的 monitorability。实现包含一个 autorater LLM、对合成降级的自检、在前沿模型上测试，结果显示仍具高 monitorability；并公开完整 prompt 供开发者追踪设计选择对 monitorability 的影响，强调该方法是对抗性压力测试的补充。


<details>
  <summary>Details</summary>
Motivation: 在训练实践或模型架构变化后，仍能保留 CoT 的可监控性。需要可操作、可复现的指标来衡量推理过程的可读性与覆盖度，以便设计者评估不同设计对 monitorability 的影响。

Method: 提出一个 autorater prompt，使任何具备能力的大模型都能评估现有 CoT 的 legibility 与 coverage。对该 prompt 进行合成降级的自检验证后，将其应用于多家前沿模型在挑战性基准上的评估，统计 monitorability 指标，并公开完整 autorater prompt 以供开发者使用。

Result: 在多家 frontier 模型上测试，观察到它们呈现出较高的 monitorability。研究者将两项指标及完整 prompt 作为工具提供给开发者，用以跟踪设计决策对 monitorability 的影响。

Conclusion: 该方法应被视为对抗性压力测试的补充，而非替代，在提示层面分享的 prompt 仍处于初步版本，欢迎社区使用并进一步完善。它有助于衡量 CoT 的默认 monitorability，为了解设计改动对监控性的影响提供实用工具。

Abstract: While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI
safety, this opportunity could be lost through shifts in training practices or
model architecture. To help preserve monitorability, we propose a pragmatic way
to measure two components of it: legibility (whether the reasoning can be
followed by a human) and coverage (whether the CoT contains all the reasoning
needed for a human to also produce the final output). We implement these
metrics with an autorater prompt that enables any capable LLM to compute the
legibility and coverage of existing CoTs. After sanity-checking our prompted
autorater with synthetic CoT degradations, we apply it to several frontier
models on challenging benchmarks, finding that they exhibit high
monitorability. We present these metrics, including our complete autorater
prompt, as a tool for developers to track how design decisions impact
monitorability. While the exact prompt we share is still a preliminary version
under ongoing development, we are sharing it now in the hopes that others in
the community will find it useful. Our method helps measure the default
monitorability of CoT - it should be seen as a complement, not a replacement,
for the adversarial stress-testing needed to test robustness against
deliberately evasive models.

</details>


### [54] [An efficient probabilistic hardware architecture for diffusion-like models](https://arxiv.org/abs/2510.23972)
*Andraž Jelinčič,Owen Lockwood,Akhil Garlapati,Guillaume Verdon,Trevor McCourt*

Main category: cs.LG

TL;DR: 提出一种全晶体管概率计算机，在硬件层面实现强大的去噪模型，理论上可与GPU在简单图像基准上性能相当，但能耗约降低10000倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于概率AI的专用随机计算机常因建模方法有限、硬件不可扩展而难以落地，需要一个在硬件层面就能高效实现去噪与推理的方案。

Method: 提出一种全晶体管(all-transistor)概率计算机，在硬件级别实现强大的去噪模型；进行了系统级分析以评估与GPU的性能对比和能耗潜力。

Result: 系统级分析表明，该架构的设备在一个简单的图像基准上有望达到与GPU相当的性能，同时能耗比GPU低约1万倍。

Conclusion: 展示了在硬件层面实现高效概率推理与去噪能力的可行性，为概率AI硬件的能效提升提供潜在路径。

Abstract: The proliferation of probabilistic AI has promoted proposals for specialized
stochastic computers. Despite promising efficiency gains, these proposals have
failed to gain traction because they rely on fundamentally limited modeling
techniques and exotic, unscalable hardware. In this work, we address these
shortcomings by proposing an all-transistor probabilistic computer that
implements powerful denoising models at the hardware level. A system-level
analysis indicates that devices based on our architecture could achieve
performance parity with GPUs on a simple image benchmark using approximately
10,000 times less energy.

</details>


### [55] [Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.23974)
*Byeonghu Na,Minsang Park,Gyuwon Sim,Donghyeok Shin,HeeSun Bae,Mina Kang,Se Jung Kwon,Wanmo Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: DATE在扩散采样的每一步动态更新文本嵌入以提高文本-图像对齐，且不需额外训练。


<details>
  <summary>Details</summary>
Motivation: 固定的文本嵌入在整个扩散过程中不可变，限制了对生成过程的适应性与对齐性。

Method: 将更新规则作为在每一步采样时对文本嵌入的优化问题，基于中间扰动数据进行迭代更新，以提升嵌入与均值预测图像之间的对齐。

Result: 在保持模型原有生成能力的同时，提升了文本与图像的对齐，在多概念生成与文本引导的图像编辑等任务上优于固定嵌入，且代码公开。

Conclusion: DATE能够在不额外训练模型的情况下动态适应文本条件，提升文本-图像对齐能力，且具备良好的一般性。

Abstract: Text-to-image diffusion models rely on text embeddings from a pre-trained
text encoder, but these embeddings remain fixed across all diffusion timesteps,
limiting their adaptability to the generative process. We propose Diffusion
Adaptive Text Embedding (DATE), which dynamically updates text embeddings at
each diffusion timestep based on intermediate perturbed data. We formulate an
optimization problem and derive an update rule that refines the text embeddings
at each sampling step to improve alignment and preference between the mean
predicted image and the text. This allows DATE to dynamically adapts the text
conditions to the reverse-diffused images throughout diffusion sampling without
requiring additional model training. Through theoretical analysis and empirical
results, we show that DATE maintains the generative capability of the model
while providing superior text-image alignment over fixed text embeddings across
various tasks, including multi-concept generation and text-guided image
editing. Our code is available at https://github.com/aailab-kaist/DATE.

</details>


### [56] [Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling](https://arxiv.org/abs/2510.23977)
*Yohan Abeysinghe,Muhammad Akhtar Munir,Sanoojan Baliah,Ron Sarafian,Fahad Shahbaz Khan,Yinon Rudich,Salman Khan*

Main category: cs.LG

TL;DR: SynCast 在高分辨率下使用区域适配的 Transformer 主干 + 扩散式随机 Refinement 模块，整合 ERA5 与 CAMS 数据，显著提升 PM1、PM2.5、PM10 的平均与极端污染预测，改善尾部分布并在高污染区域显著提升警报能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型对罕见但危险的极端污染事件往往低估，且尾部分布描述不足，亟需结合气象与大气成分数据的高分辨率、对极端事件敏感的预测框架，以提升公共卫生预警效果。

Method: 采用区域适配的 Transformer 主干，辅以扩散式随机风格化（diffusion-based stochastic refinement）模块；将气象数据与大气组分数据进行深度融合；基于 harmonized ERA5 与 CAMS 数据集进行训练；引入领域感知目标与极值理论（EVT）以提升尾部预测能力。

Result: 在 PM1、PM2.5、PM10 的预测上获得显著提升，尤其在极端条件下对分布尾部的拟合改善明显；全球精度保持良好，同时在高污染地区的预测能力有显著增强，适合用于区域性早期警报。

Conclusion: 为下一代空气质量预警系统提供可扩展的高分辨率预测框架，支持受影响地区的气候-健康风险缓解。

Abstract: Air pollution remains a leading global health and environmental risk,
particularly in regions vulnerable to episodic air pollution spikes due to
wildfires, urban haze and dust storms. Accurate forecasting of particulate
matter (PM) concentrations is essential to enable timely public health warnings
and interventions, yet existing models often underestimate rare but hazardous
pollution events. Here, we present SynCast, a high-resolution neural
forecasting model that integrates meteorological and air composition data to
improve predictions of both average and extreme pollution levels. Built on a
regionally adapted transformer backbone and enhanced with a diffusion-based
stochastic refinement module, SynCast captures the nonlinear dynamics driving
PM spikes more accurately than existing approaches. Leveraging on harmonized
ERA5 and CAMS datasets, our model shows substantial gains in forecasting
fidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),
especially under extreme conditions. We demonstrate that conventional loss
functions underrepresent distributional tails (rare pollution events) and show
that SynCast, guided by domain-aware objectives and extreme value theory,
significantly enhances performance in highly impacted regions without
compromising global accuracy. This approach provides a scalable foundation for
next-generation air quality early warning systems and supports climate-health
risk mitigation in vulnerable regions.

</details>


### [57] [HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing](https://arxiv.org/abs/2510.23980)
*Guojing Cong,Tom Potok,Hamed Poursiami,Maryam Parsa*

Main category: cs.LG

TL;DR: 将图卷积与超维计算中的绑定/捆绑操作相结合的 hdgc，在传导学习中实现了高准确度和显著的计算加速，且对同构/异构图均有良好表现。


<details>
  <summary>Details</summary>
Motivation: 解决传导图学习中在准确性与能耗之间的权衡，利用超维计算的绑定/捆绑操作增强图特征的表达，同时在同一GPU平台下与现有GNN和HD实现进行强对比。

Method: 提出 hdgc 算法，将图卷积与绑定/捆绑操作耦合到超维计算框架中，面向二值向量学习，适用于同质/异质图，进行传导学习，并在相同GPU平台上与 gcnii（GNN实现）和 HDGL（HD实现）等方法进行比较。

Result: 在一组同质/异质图上，hdgc 在预测精度上优于主要GNN与先进HD实现；在相同目标GPU平台上，平均分别比 gcnii 快 9561.0 倍、比 HDGL 快 144.5 倍。并且多数学习在二值向量上，预期在类脑神经形态和内存计算设备上具备出色的能源性能。

Conclusion: hdgc 为传导图学习提供了一个高效、可扩展的新路径，显著提升速度且具备潜在的能源优势，适合在未来的神经形态和过程内存设备上实现。

Abstract: We present a novel algorithm, \hdgc, that marries graph convolution with
binding and bundling operations in hyperdimensional computing for transductive
graph learning. For prediction accuracy \hdgc outperforms major and popular
graph neural network implementations as well as state-of-the-art
hyperdimensional computing implementations for a collection of homophilic
graphs and heterophilic graphs. Compared with the most accurate learning
methodologies we have tested, on the same target GPU platform, \hdgc is on
average 9561.0 and 144.5 times faster than \gcnii, a graph neural network
implementation and HDGL, a hyperdimensional computing implementation,
respectively. As the majority of the learning operates on binary vectors, we
expect outstanding energy performance of \hdgc on neuromorphic and emerging
process-in-memory devices.

</details>


### [58] [STNet: Spectral Transformation Network for Solving Operator Eigenvalue Problem](https://arxiv.org/abs/2510.23986)
*Hong Wang,Jiang Yixuan,Jie Wang,Xinyi Li,Jian Luo,Huanshuo Dong*

Main category: cs.LG

TL;DR: 提出 Spectral Transformation Network (STNet)，通过谱变换与消去投影提升深度学习求解算子特征值问题的精度与效率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高维算子特征值问题受维度灾难影响，特征值分布对学习方法的收敛与准确性影响显著，因此需要利用谱信息的变换来简化问题。

Method: 在每次迭代中，STNet 使用近似的特征值与特征向量对算子进行谱变换，将原问题转化为等价的、求解更易的形式。具体包括：1）deflation 投影去除已求解特征子空间，减少搜索空间并避免收敛到已知特征向量；2）滤波变换放大目标区域的特征值、抑制其他区域，以提升求解性能。

Result: 大量实验表明 STNet 在准确性方面持续超越现有学习方法，达到当前的状态-艺术水平。

Conclusion: 通过结合 deflation 投影与谱变换滤波，STNet 有效缓解维度灾难，提升算子特征值问题的求解性能，成为深度学习框架中求解此类问题的有力工具。

Abstract: Operator eigenvalue problems play a critical role in various scientific
fields and engineering applications, yet numerical methods are hindered by the
curse of dimensionality. Recent deep learning methods provide an efficient
approach to address this challenge by iteratively updating neural networks.
These methods' performance relies heavily on the spectral distribution of the
given operator: larger gaps between the operator's eigenvalues will improve
precision, thus tailored spectral transformations that leverage the spectral
distribution can enhance their performance. Based on this observation, we
propose the Spectral Transformation Network (STNet). During each iteration,
STNet uses approximate eigenvalues and eigenfunctions to perform spectral
transformations on the original operator, turning it into an equivalent but
easier problem. Specifically, we employ deflation projection to exclude the
subspace corresponding to already solved eigenfunctions, thereby reducing the
search space and avoiding converging to existing eigenfunctions. Additionally,
our filter transform magnifies eigenvalues in the desired region and suppresses
those outside, further improving performance. Extensive experiments demonstrate
that STNet consistently outperforms existing learning-based methods, achieving
state-of-the-art performance in accuracy.

</details>


### [59] [Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept](https://arxiv.org/abs/2510.23994)
*Geoffery Agorku,Sarah Hernandez,Hayley Hames,Cade Wagner*

Main category: cs.LG

TL;DR: 通过 AIS 航迹数据预测内陆水道拖带数量的新方法：以 30 个 AIS 特征为输入，使用特征选择和多种回归模型，Poisson 回归器以 MAE=1.92 的最佳表现；在下密西西比河进行标注数据的时空匹配训练与评估，结果对港口管理与水路交通监控具有潜在应用，并计划扩展到其他河道。


<details>
  <summary>Details</summary>
Motivation: 解决内陆水道拖带数量的实时估算难题。拖船非自推进性以及现有监测系统的局限性导致难以准确快速地估计拖带数量，从而影响航道管理与物流优化。

Method: 基于 AIS 车辆跟踪数据，通过时空匹配将卫星场景中的人工标注拖带实例与 AIS 航迹匹配，构建 30 个 AIS 派生特征（覆盖 Vessel 几何、动力学与轨迹模式）。使用递推特征消除 (RFE) 进行特征选择，筛选出最具预测力的 12 项特征。训练并评估六种回归模型（包括集成、核方法和广义线性模型），并比较其性能。

Result: Poisson 回归器在测试集上获得最佳性能，MAE=1.92 架拖带，使用 12 项特征。特征重要性分析显示航向熵、速度变异性与航段长度等与拖带数量高度相关。该方法具备可扩展性，适用于海事领域态势感知，且具有在锁调度、港口管理与货运规划等场景的潜在应用。

Conclusion: 提出了一种可扩展且可落地的基于 AIS 数据的拖带数量估计框架，通过特征选择与多模型比较实现预测准确性提升；未来工作拟将概念验证拓展至不同水路环境，以评估模型的迁移能力与在不同操作与环境条件下的泛化性。

Abstract: Accurate, real-time estimation of barge quantity on inland waterways remains
a critical challenge due to the non-self-propelled nature of barges and the
limitations of existing monitoring systems. This study introduces a novel
method to use Automatic Identification System (AIS) vessel tracking data to
predict the number of barges in tow using Machine Learning (ML). To train and
test the model, barge instances were manually annotated from satellite scenes
across the Lower Mississippi River. Labeled images were matched to AIS vessel
tracks using a spatiotemporal matching procedure. A comprehensive set of 30
AIS-derived features capturing vessel geometry, dynamic movement, and
trajectory patterns were created and evaluated using Recursive Feature
Elimination (RFE) to identify the most predictive variables. Six regression
models, including ensemble, kernel-based, and generalized linear approaches,
were trained and evaluated. The Poisson Regressor model yielded the best
performance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of
the 30 features. The feature importance analysis revealed that metrics
capturing vessel maneuverability such as course entropy, speed variability and
trip length were most predictive of barge count. The proposed approach provides
a scalable, readily implementable method for enhancing Maritime Domain
Awareness (MDA), with strong potential applications in lock scheduling, port
management, and freight planning. Future work will expand the proof of concept
presented here to explore model transferability to other inland rivers with
differing operational and environmental conditions.

</details>


### [60] [Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.24012)
*Byeonghu Na,Mina Kang,Jiseok Kwak,Minsang Park,Jiwoo Shin,SeJoon Jun,Gayoung Lee,Jin-Hwa Kim,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出一个训练无关的安全引导方法 STG，在扩散模型采样阶段调整文本嵌入以遵循安全约束，从而去除不安全内容并尽量保留原始语义。


<details>
  <summary>Details</summary>
Motivation: 大规模网页抓取的数据往往包含不当、偏见或有害内容，在面对恶意文本提示时，现有模型可能输出有害内容，需要一种无额外训练的安全提升方法。

Method: 在采样过程中基于一个对最终去噪图像的安全评估函数，动态调整文本嵌入以引导模型满足安全约束；该训练过程可以视为对模型分布的安全约束对齐，且无需额外训练。

Result: 在裸体、暴力、艺术风格删除等多种安全场景中，STG持续优于训练型和训练-free基线，能够去除不安全内容同时尽量保留输入的语义意图。

Conclusion: STG通过训练无关的嵌入引导实现安全性提升，理论上与安全约束的分布对齐，且对生成质量影响最小，具备提升大规模文本-图像生成系统安全性的潜力。

Abstract: Text-to-image models have recently made significant advances in generating
realistic and semantically coherent images, driven by advanced diffusion models
and large-scale web-crawled datasets. However, these datasets often contain
inappropriate or biased content, raising concerns about the generation of
harmful outputs when provided with malicious text prompts. We propose Safe Text
embedding Guidance (STG), a training-free approach to improve the safety of
diffusion models by guiding the text embeddings during sampling. STG adjusts
the text embeddings based on a safety function evaluated on the expected final
denoised image, allowing the model to generate safer outputs without additional
training. Theoretically, we show that STG aligns the underlying model
distribution with safety constraints, thereby achieving safer outputs while
minimally affecting generation quality. Experiments on various safety
scenarios, including nudity, violence, and artist-style removal, show that STG
consistently outperforms both training-based and training-free baselines in
removing unsafe content while preserving the core semantic intent of input
prompts. Our code is available at https://github.com/aailab-kaist/STG.

</details>


### [61] [Spatio-temporal Multivariate Time Series Forecast with Chosen Variables](https://arxiv.org/abs/2510.24027)
*Zibo Liu,Zhe Jiang,Zelin Xu,Tingsong Xiao,Yupu Zhang,Zhengkun Xiao,Haibo Wang,Shigang Chen*

Main category: cs.LG

TL;DR: 提出了在时空多变量预测（STMF）中进行m-out-of-n变量选择的统一框架，通过掩蔽剪枝、优先级回放和动态外推实现预测准确性与效率的兼顾，在五个真实数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实传感场景中可用传感器数量有限，需要从n个观测位置中选取m个作为输入变量以最大化预测准确性；现有工作假设输入变量已定，缺乏对如何选择输入变量的系统研究。

Method: 提出三大技术组件并行/协同实现：1) 掩蔽变量-参数剪枝，通过基于分位数的掩蔽逐步裁剪不信息变量与注意力参数；2) 优先级变量-参数回放，回放低损失的历史样本以维持训练稳定性并保留知识；3) 动态外推机制，通过可学习的空间嵌入与邻接信息将选择输入变量的信息传播给所有变量，形成全局信息传播。

Result: 在五个真实数据集上的实验结果显示，该方法在预测准确性和模型效率方面显著优于现有最先进基线，证明了变量选择与模型优化的联合有效性。

Conclusion: 联合变量选择与模型优化可以提升STMF在现实传感应用中的预测性能与资源利用率。该框架为处理传感数据缺失与预算约束问题提供了有效路径，未来工作可扩展到更大规模场景、分析鲁棒性与自适应性等。

Abstract: Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series
of $n$ spatially distributed variables in a period of recent past to forecast
their values in a period of near future. It has important applications in
spatio-temporal sensing forecast such as road traffic prediction and air
pollution prediction. Recent papers have addressed a practical problem of
missing variables in the model input, which arises in the sensing applications
where the number $m$ of sensors is far less than the number $n$ of locations to
be monitored, due to budget constraints. We observe that the state of the art
assumes that the $m$ variables (i.e., locations with sensors) in the model
input are pre-determined and the important problem of how to choose the $m$
variables in the input has never been studied. This paper fills the gap by
studying a new problem of STMF with chosen variables, which optimally selects
$m$-out-of-$n$ variables for the model input in order to maximize the forecast
accuracy. We propose a unified framework that jointly performs variable
selection and model optimization for both forecast accuracy and model
efficiency. It consists of three novel technical components: (1) masked
variable-parameter pruning, which progressively prunes less informative
variables and attention parameters through quantile-based masking; (2)
prioritized variable-parameter replay, which replays low-loss past samples to
preserve learned knowledge for model stability; (3) dynamic extrapolation
mechanism, which propagates information from variables selected for the input
to all other variables via learnable spatial embeddings and adjacency
information. Experiments on five real-world datasets show that our work
significantly outperforms the state-of-the-art baselines in both accuracy and
efficiency, demonstrating the effectiveness of joint variable selection and
model optimization.

</details>


### [62] [GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research](https://arxiv.org/abs/2510.24035)
*Xinqi Li,Yiqun Liu,Shan Jiang,Enrong Zheng,Huaijin Zheng,Wenhao Dai,Haodong Deng,Dianhai Yu,Yanjun Ma*

Main category: cs.LG

TL;DR: GraphNet 是一个包含 2700 余个现实世界深度学习计算图及丰富元数据的数据集，覆盖六大任务类别和多种框架。提出了基准度量 Speedup Score S(t)，结合运行时加速与容错性（可调容忍度）来评估张量编译器性能，并扩展出考虑错误信息的 ES(t) 以帮助定位瓶颈。基于 CV 与 NLP 任务对默认编译器 CINN（PaddlePaddle）与 TorchInductor（PyTorch）进行实证，证明 GraphNet 的实用性。完整构建和评估管线可在 GitHub 获取。


<details>
  <summary>Details</summary>
Motivation: 在不同张量编译器优化之间缺乏一个统一、可靠的比较标准，需同时考虑运行时速度与执行正确性，并能在不同容忍度下识别潜在瓶颈；同时需要一个覆盖真实世界计算图的基准数据集来评估跨框架的广泛性。

Method: 构建 GraphNet 数据集（约 2.7K 个真实世界深度学习计算图，覆盖六大任务类别，跨多个框架），提出并定义基准度量 Speedup Score S(t)，将运行时速度与正确性在可调容忍度下耦合；扩展出错误感知的 ES(t) 值以捕捉错误信息并辅助定位瓶颈。对默认编译器 CINN（PaddlePaddle）与 TorchInductor（PyTorch）在 CV 与 NLP 样本上进行基准测试，以验证该数据集和评价指标的可行性。

Result: 实验表明 S(t) 与 ES(t) 能在不同容忍度设置下稳定反映编译器的性能改进与潜在瓶颈；对 CV 与 NLP 任务的样本给出了一组可比的基线表现，证明 GraphNet 在实际优化场景中的实用性和区分力。

Conclusion: GraphNet 提供了一个覆盖广泛、可重复的现实世界 deep learning 计算图基准及量化框架，结合 S(t) 与 ES(t) 能帮助编译器开发者更系统地评估与定位性能瓶颈，并促进在跨框架、跨任务的比较研究。完整的构建与评测工具链对外可获取。

Abstract: We introduce GraphNet, a dataset of 2.7K real-world deep learning
computational graphs with rich metadata, spanning six major task categories
across multiple deep learning frameworks. To evaluate tensor compiler
performance on these samples, we propose the benchmark metric Speedup Score
S(t), which jointly considers runtime speedup and execution correctness under
tunable tolerance levels, offering a reliable measure of general optimization
capability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t),
which incorporates error information and helps compiler developers identify key
performance bottlenecks. In this report, we benchmark the default tensor
compilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer
vision (CV) and natural language processing (NLP) samples to demonstrate the
practicality of GraphNet. The full construction pipeline with graph extraction
and compiler evaluation tools is available at
https://github.com/PaddlePaddle/GraphNet .

</details>


### [63] [Geometric Algorithms for Neural Combinatorial Optimization with Constraints](https://arxiv.org/abs/2510.24039)
*Nikolaos Karalias,Akbar Rafiey,Yifei Xu,Zhishang Luo,Behrooz Tahmasebi,Connie Jiang,Stefanie Jegelka*

Main category: cs.LG

TL;DR: A differentiable self-supervised framework for solving discrete-constrained combinatorial optimization (CO) problems by decomposing neural outputs into convex combinations of polytope corners that represent feasible solutions, enabling efficient rounding and strong empirical performance.


<details>
  <summary>Details</summary>
Motivation: SSL for CO struggles with discrete constraints; a method that integrates discrete feasibility into end-to-end differentiable learning is needed to leverage neural networks for CO tasks.

Method: Use convex geometry and Carathéodory's theorem to decompose neural network outputs into convex combinations of a small set of polytope corners (feasible corners). This yields a differentiable training signal via self-supervision and guarantees a quality-preserving rounding to feasible discrete solutions.

Result: On cardinality-constrained optimization, the approach consistently outperforms neural baselines. The paper also demonstrates applicability beyond cardinality constraints, to problems such as finding independent sets in graphs and matroid-constrained optimization.

Conclusion: The framework provides a general, end-to-end differentiable method to handle discrete constraints in CO tasks by embedding feasibility via convex decompositions, enabling effective self-supervised learning and robust rounding across a range of combinatorial problems.

Abstract: Self-Supervised Learning (SSL) for Combinatorial Optimization (CO) is an
emerging paradigm for solving combinatorial problems using neural networks. In
this paper, we address a central challenge of SSL for CO: solving problems with
discrete constraints. We design an end-to-end differentiable framework that
enables us to solve discrete constrained optimization problems with neural
networks. Concretely, we leverage algorithmic techniques from the literature on
convex geometry and Carath\'eodory's theorem to decompose neural network
outputs into convex combinations of polytope corners that correspond to
feasible sets. This decomposition-based approach enables self-supervised
training but also ensures efficient quality-preserving rounding of the neural
net output into feasible solutions. Extensive experiments in
cardinality-constrained optimization show that our approach can consistently
outperform neural baselines. We further provide worked-out examples of how our
method can be applied beyond cardinality-constrained problems to a diverse set
of combinatorial optimization tasks, including finding independent sets in
graphs, and solving matroid-constrained problems.

</details>


### [64] [Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection](https://arxiv.org/abs/2510.24043)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 提出了一种两阶段的 LKPLO 框架，用于多阶段异常检测，结合自适应损失、全局核 PCA 以及局部聚类，达到在多簇和高维数据上的 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 克服传统投影法对固定度量和单一数据结构的依赖，需引入灵活的损失、非线性化和多模态结构处理。

Method: 引入 PLO 作为可自适应的异常性度量；通过全局核 PCA 实现数据结构的线性化；随后进行局部聚类以处理多峰分布。采用 5 折交叉验证和自动超参优化。

Result: 在 10 个基准数据集上实现 SOTA，显著优于强基线，尤其在 Optdigits（多簇数据）和 Arrhythmia（高维数据）。消融分析证实核化与局部化的协同是必需的。

Conclusion: 多阶段混合架构对异常检测具有强大效能，提出了一个有价值的新工具，强调混合多阶段设计的重要性。

Abstract: This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection
framework that overcomes the coexisting limitations of conventional
projection-based methods: their reliance on a fixed statistical metric and
their assumption of a single data structure. Our framework uniquely synthesizes
three key concepts: (1) a generalized loss-based outlyingness measure (PLO)
that replaces the fixed metric with flexible, adaptive loss functions like our
proposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear
data structures; and (3) a subsequent local clustering stage to handle
multi-modal distributions. Comprehensive 5-fold cross-validation experiments on
10 benchmark datasets, with automated hyperparameter optimization, demonstrate
that Two-Stage LKPLO achieves state-of-the-art performance. It significantly
outperforms strong baselines on datasets with challenging structures where
existing methods fail, most notably on multi-cluster data (Optdigits) and
complex, high-dimensional data (Arrhythmia). Furthermore, an ablation study
empirically confirms that the synergistic combination of both the kernelization
and localization stages is indispensable for its superior performance. This
work contributes a powerful new tool for a significant class of outlier
detection problems and underscores the importance of hybrid, multi-stage
architectures.

</details>


### [65] [Mitigating Negative Transfer via Reducing Environmental Disagreement](https://arxiv.org/abs/2510.24044)
*Hui Sun,Zheng Xie,Hao-Yuan He,Ming Li*

Main category: cs.LG

TL;DR: 提出RED，一种基于因果解耦的无监督域适应方法，通过对源/目标域的非因果环境特征进行对抗性提取和域特异性分析，减少环境分歧以缓解负迁移，达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 域间分布偏移引发的负迁移一直是UDA中的核心挑战。单纯依赖相关特征容易在环境演化时失效，因此需要通过因果解耦提升跨域鲁棒性。

Method: 提出RED：对样本进行因果不变量与非因果环境特征的解耦，将域特异的非因果环境特征在对立域进行对抗性提取；基于域特异的非因果环境特征估计并减少环境分歧。

Result: 实验结果表明，RED显著缓解负迁移并达到或接近SOTA性能。

Conclusion: 通过对环境分歧的量化与控制，基于因果解耦的UDA在环境演化时更具鲁棒性，能有效降低负迁移。

Abstract: Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a
labeled source domain to an unlabeled target domain, addressing the challenge
of \emph{domain shift}. Significant domain shifts hinder effective knowledge
transfer, leading to \emph{negative transfer} and deteriorating model
performance. Therefore, mitigating negative transfer is essential. This study
revisits negative transfer through the lens of causally disentangled learning,
emphasizing cross-domain discriminative disagreement on non-causal
environmental features as a critical factor. Our theoretical analysis reveals
that overreliance on non-causal environmental features as the environment
evolves can cause discriminative disagreements~(termed \emph{environmental
disagreement}), thereby resulting in negative transfer. To address this, we
propose Reducing Environmental Disagreement~(RED), which disentangles each
sample into domain-invariant causal features and domain-specific non-causal
environmental features via adversarially training domain-specific environmental
feature extractors in the opposite domains. Subsequently, RED estimates and
reduces environmental disagreement based on domain-specific non-causal
environmental features. Experimental results confirm that RED effectively
mitigates negative transfer and achieves state-of-the-art performance.

</details>


### [66] [Causal-Aware Generative Adversarial Networks with Reinforcement Learning](https://arxiv.org/abs/2510.24046)
*Tu Anh Hoang Nguyen,Dang Nguyen,Tri-Nhan Vo,Thuc Duy Le,Sunil Gupta*

Main category: cs.LG

TL;DR: CA-GAN 是一种针对表格数据的因果感知生成框架，通过因果图提取结合条件WGAN-GP，并引入基于强化学习的目标以对齐真实与伪造数据的因果结构，在14个数据集上优于6种SOTA方法，聚焦因果保真、数据实用性和隐私保护，适用于现实场景下的私有且高效的合成数据生成。


<details>
  <summary>Details</summary>
Motivation: 隐私与合规限制往往阻碍对表格数据的使用；现有基于GAN的数据生成方法在捕捉复杂因果关系、维持数据实用性以及提供可落地的隐私保证方面存在挑战，难以用于企业部署。

Method: CA-GAN 采用两步策略：第一步从数据中提取因果图，学习数据流形中的稳健和全面的因果关系；第二步在因果图节点结构约束下使用定制的条件WGAN-GP进行生成，并通过基于强化学习的目标函数来对齐真实与伪数据所构建的因果图，确保在训练与采样阶段具备因果感知。

Result: 在14个表格数据集上，CA-GAN 相对于六种SOTA方法表现更优，评估聚焦核心数据工程指标：因果保真、实用性保真和隐私保真。

Conclusion: CA-GAN 为数据工程师提供一个实用且高性能的私有化合成数据生成解决方案，可用于基准测试数据库系统、加速软件开发与推动安全的数据驱动研究。

Abstract: The utility of tabular data for tasks ranging from model training to
large-scale data analysis is often constrained by privacy concerns or
regulatory hurdles. While existing data generation methods, particularly those
based on Generative Adversarial Networks (GANs), have shown promise, they
frequently struggle with capturing complex causal relationship, maintaining
data utility, and providing provable privacy guarantees suitable for enterprise
deployment. We introduce CA-GAN, a novel generative framework specifically
engineered to address these challenges for real-world tabular datasets. CA-GAN
utilizes a two-step approach: causal graph extraction to learn a robust,
comprehensive causal relationship in the data's manifold, followed by a custom
Conditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates
exclusively as per the structure of nodes in the causal graph. More
importantly, the generator is trained with a new Reinforcement Learning-based
objective that aligns the causal graphs constructed from real and fake data,
ensuring the causal awareness in both training and sampling phases. We
demonstrate CA-GAN superiority over six SOTA methods across 14 tabular
datasets. Our evaluations, focused on core data engineering metrics: causal
preservation, utility preservation, and privacy preservation. Our method offers
a practical, high-performance solution for data engineers seeking to create
high-quality, privacy-compliant synthetic datasets to benchmark database
systems, accelerate software development, and facilitate secure data-driven
research.

</details>


### [67] [Low-N Protein Activity Optimization with FolDE](https://arxiv.org/abs/2510.24053)
*Jacob B. Roberts,Catherine R. Ji,Isaac Donnell,Thomas D. Young,Allison N. Pearson,Graham A. Hudson,Leah S. Keiser,Mia Wesselkamper,Peter H. Winegar,Janik Ludwig,Sarah H. Klass,Isha V. Sheth,Ezechinyere C. Ukabiala,Maria C. T. Astolfi,Benjamin Eysenbach,Jay D. Keasling*

Main category: cs.LG

TL;DR: FolDE：一种 ALDE 方法，通过自然性暖启动和常量谎言批量选择实现对蛋白质优化的端到端改进，在20个蛋白目标上比基线方法提升显著，且开发为开源软件。


<details>
  <summary>Details</summary>
Motivation: 降低蛋白质优化成本，解决现有 ALDE 在每轮选取最高预测值时导致训练数据同质化的问题，从而提高预测模型在后续轮次的性能。

Method: 提出 FolDE，引入自然性基于的暖启动，将有限的活性测量与蛋白质语言模型输出结合以提升活性预测；引入常量谎言（constant-liar）批量选择器以增加批量多样性，开放源代码工作流。

Result: 在对20个蛋白目标的仿真中，FolDE 比最佳基线 ALDE 多发现23%的前10%突变体（p=0.005），找出前1%突变体的概率提升了55%；常量谎言选择器对多突变计划的批量多样性有帮助，但在基准测试中的作用有限。

Conclusion: FolDE 能通过改进训练数据多样性和引导预测模型提升端到端成功率，且完整工作流可自由获取的开源软件，使高效蛋白质优化更易于实验室使用。

Abstract: Proteins are traditionally optimized through the costly construction and
measurement of many mutants. Active Learning-assisted Directed Evolution (ALDE)
alleviates that cost by predicting the best improvements and iteratively
testing mutants to inform predictions. However, existing ALDE methods face a
critical limitation: selecting the highest-predicted mutants in each round
yields homogeneous training data insufficient for accurate prediction models in
subsequent rounds. Here we present FolDE, an ALDE method designed to maximize
end-of-campaign success. In simulations across 20 protein targets, FolDE
discovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005)
and is 55% more likely to find top 1% mutants. FolDE achieves this primarily
through naturalness-based warm-starting, which augments limited activity
measurements with protein language model outputs to improve activity
prediction. We also introduce a constant-liar batch selector, which improves
batch diversity; this is important in multi-mutation campaigns but had limited
effect in our benchmarks. The complete workflow is freely available as
open-source software, making efficient protein optimization accessible to any
laboratory.

</details>


### [68] [Learning Parameterized Skills from Demonstrations](https://arxiv.org/abs/2510.24095)
*Vedant Gupta,Haotian Fu,Calvin Luo,Yiding Jiang,George Konidaris*

Main category: cs.LG

TL;DR: 提出一个端到端的DEPS框架，用于从专家演示中发现参数化技能，联合学习参数化技能策略和元策略，利用时间变分推断与信息论正则化解决潜变量模型的退化问题，从而学得 temporally extended、语义明确且可适应的技能，且在LIBERO与MetaWorld基线任务上表现优于多任务与技能学习方法，能够发现可解释的参数化技能（如把握位置由连续参数定义）。


<details>
  <summary>Details</summary>
Motivation: 在机器人学习中，期望从专家演示中学习复用性高、可解释的参数化技能，并能够对未知任务进行泛化。现有方法常陷入潜变量退化、技能不稳定或难以解释的问题，难以在多任务场景中学习到可转移的参数化控制。

Method: 提出端到端的DEPS框架，联合学习参数化技能策略以及用于离散技能与连续参数选择的元策略。通过时间变分推断对潜变量进行建模，并结合信息理论正则化以缓解潜变量退化，促使学习到的技能在时间上保持扩展性、语义上明确且可适应。训练过程利用多任务专家演示数据，目标是提升对未见任务的泛化能力。

Result: 实验表明，从多任务专家演示中学习参数化技能显著提升对未见任务的泛化，且在LIBERO与MetaWorld基准上优于多任务基线和技能学习基线。此外，DEPS能发现可解释的参数化技能，例如一个将连续参数用于定义抓取位置的抓取技能。

Conclusion: DEPS能够在端到端学习中同时获取参数化技能和元策略，产生 temporally extended、可解释且可泛化的技能集合；通过时间变分推断与信息论正则化有效缓解潜变量退化问题，示范了从多任务演示中学习参数化技能的可行性与优势。

Abstract: We present DEPS, an end-to-end algorithm for discovering parameterized skills
from expert demonstrations. Our method learns parameterized skill policies
jointly with a meta-policy that selects the appropriate discrete skill and
continuous parameters at each timestep. Using a combination of temporal
variational inference and information-theoretic regularization methods, we
address the challenge of degeneracy common in latent variable models, ensuring
that the learned skills are temporally extended, semantically meaningful, and
adaptable. We empirically show that learning parameterized skills from
multitask expert demonstrations significantly improves generalization to unseen
tasks. Our method outperforms multitask as well as skill learning baselines on
both LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers
interpretable parameterized skills, such as an object grasping skill whose
continuous arguments define the grasp location.

</details>


### [69] [Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.24120)
*Ziyu Liu,Yijing Liu,Jianfei Yuan,Minzhi Yan,Le Yue,Honghui Xiong,Yi Yang*

Main category: cs.LG

TL;DR: 提出G2ConS，通过 chunk 选择和无额外成本的概念图降低基于图的RAG在LLM问答中的构建成本，同时保持或提升检索与回答质量。


<details>
  <summary>Details</summary>
Motivation: 缓解大语言模型在多文档检索中对文本Chunk进行实体与关系抽取造成的高昂成本；利用发现某些概念词及其相关文档对任务更具影响力，降低整体成本。

Method: 提出Graph-Guided Concept Selection (G2ConS)，包括一个LLM无关的chunk选择方法用于筛选重要文档片段，从而降低KG构建成本，以及一个LLM无成本的概念图，用以弥补因chunk选择带来的知识空缺。通过系统消融研究验证关键概念与文档的重要性。

Result: 在多个真实数据集上，G2ConS在构建成本、检索有效性和回答质量方面均优于基线方法。

Conclusion: 通过概念级选择与零成本的概念图，G2ConS实现了成本节省与性能提升，是一种有效的图检索增强策略，适用于需要多跳推理的领域。

Abstract: Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance
retrieval in Large Language Model (LLM)-based question answering. It is
especially beneficial in domains such as biomedicine, law, and political
science, where effective retrieval often involves multi-hop reasoning over
proprietary documents. However, these methods demand numerous LLM calls to
extract entities and relations from text chunks, incurring prohibitive costs at
scale. Through a carefully designed ablation study, we observe that certain
words (termed concepts) and their associated documents are more important.
Based on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its
core comprises a chunk selection method and an LLM-independent concept graph.
The former selects salient document chunks to reduce KG construction costs; the
latter closes knowledge gaps introduced by chunk selection at zero cost.
Evaluations on multiple real-world datasets show that G2ConS outperforms all
baselines in construction cost, retrieval effectiveness, and answering quality.

</details>


### [70] [EddyFormer: Accelerated Neural Simulations of Three-Dimensional Turbulence at Scale](https://arxiv.org/abs/2510.24173)
*Yiheng Du,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: EddyFormer uses a Transformer-based SEM to simulate large-scale turbulence with DNS-level accuracy at 256^3, achieving 30x speedup and strong domain generalization across larger domains and diverse flows.


<details>
  <summary>Details</summary>
Motivation: Tackle the computational infeasibility of direct numerical simulation for turbulence and the limitations of existing data-driven models in capturing multi-scale interactions and cross-domain generalization.

Method: Introduce EddyFormer, an SEM-based Transformer architecture with SEM tokenization that splits flow into grid-scale and subgrid-scale components; train on a 3D isotropic-turbulence dataset to DNS-level accuracy; evaluate on unseen domains up to 4x larger and on the The Well benchmark to test generalization and robustness.

Result: Attains DNS-level accuracy at 256^3 with ~30x speedup over DNS; preserves energy spectra, correlation/structure functions on larger unseen domains; demonstrates domain generalization and successful convergence on The Well benchmark where previous ML models fail.

Conclusion: A Transformer-based SEM framework can fuse spectral-method accuracy with the scalability of attention, enabling accurate, scalable turbulence simulations with strong cross-domain generalization.

Abstract: Computationally resolving turbulence remains a central challenge in fluid
dynamics due to its multi-scale interactions. Fully resolving large-scale
turbulence through direct numerical simulation (DNS) is computationally
prohibitive, motivating data-driven machine learning alternatives. In this
work, we propose EddyFormer, a Transformer-based spectral-element (SEM)
architecture for large-scale turbulence simulation that combines the accuracy
of spectral methods with the scalability of the attention mechanism. We
introduce an SEM tokenization that decomposes the flow into grid-scale and
subgrid-scale components, enabling capture of both local and global features.
We create a new three-dimensional isotropic turbulence dataset and train
EddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x
speedup over DNS. When applied to unseen domains up to 4x larger than in
training, EddyFormer preserves accuracy on physics-invariant metrics-energy
spectra, correlation functions, and structure functions-showing domain
generalization. On The Well benchmark suite of diverse turbulent flows,
EddyFormer resolves cases where prior ML models fail to converge, accurately
reproducing complex dynamics across a wide range of physical conditions.

</details>


### [71] [V-SAT: Video Subtitle Annotation Tool](https://arxiv.org/abs/2510.24180)
*Arpita Kundu,Joyita Chakraborty,Anindita Desarkar,Aritra Sen,Srushti Anil Patil,Vishwanathan Raman*

Main category: cs.LG

TL;DR: 提出V-SAT（视频字幕标注工具），通过整合大模型和多模态信息，自动检测并修正多种字幕质量问题，提升字幕质量并实现人机协同的高质量字幕标注。


<details>
  <summary>Details</summary>
Motivation: 随着流媒体和社交平台的视频内容激增，对准确、可读且适配动态场景的字幕需求日益增长；现有字幕生成方法多聚焦单一源（语音或OCR），在同步性、文本合规、格式统一、阅读速度等方面存在缺陷，导致后期人工修正成本高。

Method: 提出一个统一框架，融合大语言模型（LLMs）、视觉-语言模型（VLMs）、图像处理和自动语音识别（ASR），利用音视频上下文对字幕进行全面检测与纠错，并通过人机在环验证提升结果质量。

Result: 字幕质量显著提升：将SUBER分数从9.6降至3.54（解决语言模式问题后），图像模式问题的F1-score约为0.80；并通过人机协同验证确保高质量结果，提供首个综合解决方案用于鲁棒字幕标注。

Conclusion: 该框架提供端到端、可扩展的字幕质量修正与标注方案，能在动态多模态场景中实现更稳健的字幕生成与后期润色。

Abstract: The surge of audiovisual content on streaming platforms and social media has
heightened the demand for accurate and accessible subtitles. However, existing
subtitle generation methods primarily speech-based transcription or OCR-based
extraction suffer from several shortcomings, including poor synchronization,
incorrect or harmful text, inconsistent formatting, inappropriate reading
speeds, and the inability to adapt to dynamic audio-visual contexts. Current
approaches often address isolated issues, leaving post-editing as a
labor-intensive and time-consuming process. In this paper, we introduce V-SAT
(Video Subtitle Annotation Tool), a unified framework that automatically
detects and corrects a wide range of subtitle quality issues. By combining
Large Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,
and Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from
both audio and video. Subtitle quality improved, with the SUBER score reduced
from 9.6 to 3.54 after resolving all language mode issues and F1-scores of
~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality
results, providing the first comprehensive solution for robust subtitle
annotation.

</details>


### [72] [Unlocking Out-of-Distribution Generalization in Dynamics through Physics-Guided Augmentation](https://arxiv.org/abs/2510.24216)
*Fan Xu,Hao Wu,Kun Wang,Nan Wang,Qingsong Wen,Xian Wu,Wei Gong,Xibin Zhao*

Main category: cs.LG

TL;DR: SPARK 将物理参数整合到离散状态字典，借助重构自编码器进行物理引导的增广，并通过在潜空间内进行 principled 插值来生成新的训练样本；对下游预测使用 Fourier 增强的 Graph ODE，旨在提升在分布外和数据稀缺情境中的鲁棒性与长期依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: 在动力系统建模中，传统数值方法成本高，数据驱动方法在数据稀缺和迁移分布时表现不佳。提出一种物理引导的增广框架，以缓解这两大挑战。

Method: SPARK 使用重构自编码器将物理参数嵌入一个物理丰富的离散状态字典；这个字典作为物理状态的结构化表示，允许通过在潜在空间中进行有原则的插值来生成新的、物理上合理的训练样本。下游预测阶段将增强的表示与 Fourier 增强的 Graph ODE 结合，以稳健地建模扩展的数据分布并捕捉长期时序依赖。

Result: 在多样化基准数据集上的大量实验表明，SPARK 显著优于最先进的基线，尤其是在分布外和数据稀缺场景下，验证了物理引导增广范式的有效性。

Conclusion: 通过物理引导的增广范式，SPARK 能在数据稀缺与分布迁移场景中提升动力系统建模的鲁棒性与预测能力，展示了将物理先验融入数据驱动学习的潜力。

Abstract: In dynamical system modeling, traditional numerical methods are limited by
high computational costs, while modern data-driven approaches struggle with
data scarcity and distribution shifts. To address these fundamental
limitations, we first propose SPARK, a physics-guided quantitative augmentation
plugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate
physical parameters into a physics-rich discrete state dictionary. This state
dictionary then acts as a structured dictionary of physical states, enabling
the creation of new, physically-plausible training samples via principled
interpolation in the latent space. Further, for downstream prediction, these
augmented representations are seamlessly integrated with a Fourier-enhanced
Graph ODE, a combination designed to robustly model the enriched data
distribution while capturing long-term temporal dependencies. Extensive
experiments on diverse benchmarks demonstrate that SPARK significantly
outperforms state-of-the-art baselines, particularly in challenging
out-of-distribution scenarios and data-scarce regimes, proving the efficacy of
our physics-guided augmentation paradigm.

</details>


### [73] [Closing Gaps: An Imputation Analysis of ICU Vital Signs](https://arxiv.org/abs/2510.24217)
*Alisher Turubayev,Anna Shopova,Fabian Lange,Mahmut Kamalak,Paul Mattes,Victoria Ayvasky,Bert Arnrich,Bjarne Pfitzner,Robin P. van de Water*

Main category: cs.LG

TL;DR: A reusable benchmark compares 15 imputation methods and 4 amputation methods on ICU vital signs to guide ML-based clinical prediction.


<details>
  <summary>Details</summary>
Motivation: Data missingness in ICU vital sign time series degrades predictive performance; there is a need for a systematic, comprehensive comparison of imputation techniques beyond ad-hoc approaches.

Method: They establish an extensible and reusable benchmark framework, aggregating 15 imputation methods and 4 amputation methods, and evaluate them on major ICU datasets to assess impact on clinical prediction models.

Result: The benchmark provides a comparative basis for selecting imputation techniques and highlights the importance of systematic evaluation to improve prediction performance.

Conclusion: A standardized benchmark will facilitate the development and clinical adoption of better ML models by promoting rigorous comparison of imputation strategies on ICU data.

Abstract: As more Intensive Care Unit (ICU) data becomes available, the interest in
developing clinical prediction models to improve healthcare protocols
increases. However, the lack of data quality still hinders clinical prediction
using Machine Learning (ML). Many vital sign measurements, such as heart rate,
contain sizeable missing segments, leaving gaps in the data that could
negatively impact prediction performance. Previous works have introduced
numerous time-series imputation techniques. Nevertheless, more comprehensive
work is needed to compare a representative set of methods for imputing ICU
vital signs and determine the best practice. In reality, ad-hoc imputation
techniques that could decrease prediction accuracy, like zero imputation, are
still used. In this work, we compare established imputation techniques to guide
researchers in improving the performance of clinical prediction models by
selecting the most accurate imputation technique. We introduce an extensible
and reusable benchmark with currently 15 imputation and 4 amputation methods,
created for benchmarking on major ICU datasets. We hope to provide a
comparative basis and facilitate further ML development to bring more models
into clinical practice.

</details>


### [74] [PRIVET: Privacy Metric Based on Extreme Value Theory](https://arxiv.org/abs/2510.24233)
*Antoine Szatkownik,Aurélien Decelle,Beatriz Seoane,Nicolas Bereux,Léo Planche,Guillaume Charpiat,Burak Yelmen,Flora Jay,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 提出 PRIVET，一种基于样本的、任意模态的隐私泄露评估算法，通过对最近邻距离的极值统计来给每个合成样本分配隐私泄露分数，便于样本级别的隐私检测。


<details>
  <summary>Details</summary>
Motivation: 在合成数据的隐私保护研究中，现有方法多使用全局性、不可解释的风险评估，缺乏对单样本的可解释、可操作的评估手段，可能阻碍实际部署。需要一种样本级、模态无关的评估框架来检测 memorization 与隐私泄露。

Method: 提出 PRIVET：基于最近邻距离的极值统计，利用极值理论对每个合成样本分配隐私泄露分数，方法对数据模态无关，适用于高维、大样本量及样本稀缺（如基因数据）的场景。

Result: 通过实验表明 PRIVET 能可靠检测 memorization 与隐私泄露，在多种数据模态下有效，包括高维、极少样本（基因数据）以及欠拟合情形；与现有方法在数据集级别和样本级别输出方面的比较显示其优势，并给出定性与定量输出。分析还揭示了现有计算机视觉嵌入在得到近似重复样本时的距离感知方面的局限性。

Conclusion: 该方法为数据隐私的样本级评估提供了一种可解释、通用的度量手段，优于现有全局性评估，并对实际部署合成数据的隐私保护具有潜在价值，同时也暴露了嵌入表示在近似重复样本方面的局限性。

Abstract: Deep generative models are often trained on sensitive data, such as genetic
sequences, health data, or more broadly, any copyrighted, licensed or protected
content. This raises critical concerns around privacy-preserving synthetic
data, and more specifically around privacy leakage, an issue closely tied to
overfitting. Existing methods almost exclusively rely on global criteria to
estimate the risk of privacy failure associated to a model, offering only
quantitative non interpretable insights. The absence of rigorous evaluation
methods for data privacy at the sample-level may hinder the practical
deployment of synthetic data in real-world applications. Using extreme value
statistics on nearest-neighbor distances, we propose PRIVET, a generic
sample-based, modality-agnostic algorithm that assigns an individual privacy
leak score to each synthetic sample. We empirically demonstrate that PRIVET
reliably detects instances of memorization and privacy leakage across diverse
data modalities, including settings with very high dimensionality, limited
sample sizes such as genetic data and even under underfitting regimes. We
compare our method to existing approaches under controlled settings and show
its advantage in providing both dataset level and sample level assessments
through qualitative and quantitative outputs. Additionally, our analysis
reveals limitations in existing computer vision embeddings to yield
perceptually meaningful distances when identifying near-duplicate samples.

</details>


### [75] [Sparse Optimistic Information Directed Sampling](https://arxiv.org/abs/2510.24234)
*Ludovic Schwartz,Hamish Flynn,Gergely Neu*

Main category: cs.LG

TL;DR: SOIDS为稀疏随机线性带宽问题提供了在数据丰富和数据稀缺两种情形下的最优最坏情境 regrets 的自适应解，且不依赖贝叶斯假设。


<details>
  <summary>Details</summary>
Motivation: 现有算法在数据丰富或数据稀缺两端只能在一个维度上达到最优的最坏情境表现，缺乏在两端同时最优的自适应性。之前的稀疏IDS在贝叶斯设置下实现了两端的最优率，但在严格的最坏情境下无相应方法。需要一个纯粹最坏情境下也能在两种 regime 同时达到最优的算法。

Method: 提出稀疏乐观信息导向采样（SOIDS），通过引入时间相关的学习率来实现信息与后悔之间的平衡，扩展了IDS的分析以适应最坏情境的要求，且针对稀疏线性带来带来了自适应的学习策略。

Result: 理论上给出SOIDS在数据丰富与数据稀缺两种情形下都达到最优的最坏情境后悔界；在实践中对SOIDS进行了经验性验证，显示良好性能。

Conclusion: SOIDS实现了在最坏情境下的自适应性最优界限，扩展了IDS的理论保障，并在实验上表现出色，为稀疏在线决策问题提供了一个第一的纯最坏情境下的最优解。

Abstract: Many high-dimensional online decision-making problems can be modeled as
stochastic sparse linear bandits. Most existing algorithms are designed to
achieve optimal worst-case regret in either the data-rich regime, where
polynomial depen- dence on the ambient dimension is unavoidable, or the
data-poor regime, where dimension-independence is possible at the cost of worse
dependence on the num- ber of rounds. In contrast, the sparse Information
Directed Sampling (IDS) algo- rithm satisfies a Bayesian regret bound that has
the optimal rate in both regimes simultaneously. In this work, we explore the
use of Sparse Optimistic Informa- tion Directed Sampling (SOIDS) to achieve the
same adaptivity in the worst-case setting, without Bayesian assumptions.
Through a novel analysis that enables the use of a time-dependent learning
rate, we show that SOIDS can optimally balance information and regret. Our
results extend the theoretical guarantees of IDS, pro- viding the first
algorithm that simultaneously achieves optimal worst-case regret in both the
data-rich and data-poor regimes. We empirically demonstrate the good
performance of SOIDS.

</details>


### [76] [PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling](https://arxiv.org/abs/2510.24235)
*Ai Jian,Jingqing Ruan,Xing Ma,Dailin Li,QianLin Zhou,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: PaTaRM将偏好感知的任务自适应奖励模型用于RLHF：通过PAR将成对偏好转化为点级信号，并通过动态评量标尺实现全局任务一致性与实例级推理，从而实现更高效、可泛化且可解释的奖励建模，提升RLHF性能。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型分为成对（pairwise）和点对（point-wise）两类，各自存在标注和对齐问题，难以在RLHF中高效、可解释地应用。需一个统一框架同时利用对偏好信息并提供可调整的评估准则来降低标注成本并提升鲁棒性。

Method: 提出PaTaRM框架：包含偏好感知奖励（PAR）机制，用成对数据中的相对偏好来构建稳健的点级训练信号；以及任务自适应评尺系统，能够为全局任务一致性和实例级细粒度推理生成评估标准。两者结合形成统一的GRM以用于RLHF训练。

Result: 在RewardBench与RMBench上，对Qwen3-8B与Qwen3-14B模型的相对改进平均为4.7%；在IFEval与InFoBench的下游RLHF任务中平均提升13.6%。代码开源。

Conclusion: PaTaRM实现了高效、可泛化且可解释的RLHF奖励建模，能够在不同模型与基准上表现稳健，降低对显式点级标签的依赖并提供可解释的评估Rubric。

Abstract: Reward models (RMs) are central to reinforcement learning from human feedback
(RLHF), providing the critical supervision signals that align large language
models (LLMs) with human preferences. While generative reward models (GRMs)
offer greater interpretability than traditional scalar RMs, current training
paradigms remain limited. Pair-wise methods rely on binary good-versus-bad
labels, which cause mismatches for point-wise inference and necessitate complex
pairing strategies for effective application in RLHF. On the other hand,
point-wise methods require more elaborate absolute labeling with rubric-driven
criteria, resulting in poor adaptability and high annotation costs. In this
work, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a
unified framework that integrates a preference-aware reward (PAR) mechanism
with dynamic rubric adaptation. PaTaRM leverages relative preference
information from pairwise data to construct robust point-wise training signals,
eliminating the need for explicit point-wise labels. Simultaneously, it employs
a task-adaptive rubric system that flexibly generates evaluation criteria for
both global task consistency and instance-specific fine-grained reasoning. This
design enables efficient, generalizable, and interpretable reward modeling for
RLHF. Extensive experiments show that PaTaRM achieves an average relative
improvement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B
models. Furthermore, PaTaRM boosts downstream RLHF performance, with an average
improvement of 13.6% across IFEval and InFoBench benchmarks, confirming its
effectiveness and robustness. Our code is available at
https://github.com/JaneEyre0530/PaTaRM.

</details>


### [77] [Temporal Knowledge Graph Hyperedge Forecasting: Exploring Entity-to-Category Link Prediction](https://arxiv.org/abs/2510.24240)
*Edward Markai,Sina Molavipour*

Main category: cs.LG

TL;DR: 将实体类别引入 TLogic 的规则扩展以提升可解释性和准确性；在类别未知时通过数据驱动方法（LLM）生成类别，并评估检索分数的聚合方法对类别预测的影响。


<details>
  <summary>Details</summary>
Motivation: 现有时间知识图谱多侧重嵌入式方法，缺乏可解释性。引入规则化框架并以类别约束规则应用，可提升透明度与可评估性；在类别未知场景提供数据驱动的类别生成以扩展适用性。

Method: 在 TLogic 中新增以实体类别为关键参数的规则格式，限定规则仅对相关实体生效；若类别未知，通过基于大语言模型的数据驱动方法生成类别；研究并比较用于汇聚检索到的实体分数的不同聚合策略对类别预测的影响。

Result: 理论上实现高准确性与可解释的预测，提升规则透明度，最终用户能够在预测阶段评估所应用的规则；在类别驱动下规则应用更聚焦相关实体。

Conclusion: 通过将类别整合到 TLogic 的扩展提升了可解释性和性能，LLM 驱动的类别生成以及聚合策略的探索为可解释的知识推理提供了有价值的方向。

Abstract: Temporal Knowledge Graphs have emerged as a powerful way of not only modeling
static relationships between entities but also the dynamics of how relations
evolve over time. As these informational structures can be used to store
information from a real-world setting, such as a news flow, predicting future
graph components to a certain extent equates predicting real-world events. Most
of the research in this field focuses on embedding-based methods, often
leveraging convolutional neural net architectures. These solutions act as black
boxes, limiting insight. In this paper, we explore an extension to an
established rule-based framework, TLogic, that yields a high accuracy in
combination with explainable predictions. This offers transparency and allows
the end-user to critically evaluate the rules applied at the end of the
prediction stage. The new rule format incorporates entity category as a key
component with the purpose of limiting rule application only to relevant
entities. When categories are unknown for building the graph, we propose a
data-driven method to generate them with an LLM-based approach. Additionally,
we investigate the choice of aggregation method for scores of retrieved
entities when performing category prediction.

</details>


### [78] [SALS: Sparse Attention in Latent Space for KV cache Compression](https://arxiv.org/abs/2510.24273)
*Junlin Mu,Hantao Huang,Jihang Zhang,Minghui Yu,Tao Wang,Yidong Li*

Main category: cs.LG

TL;DR: SALS introduces sparse attention in latent space to compress KV cache and accelerate attention without full cache reconstruction, achieving substantial compression and speedups while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Long-context LLMs demand large KV caches, but their inference is bottlenecked by memory and bandwidth. RoPE-based low-rank compression fails due to increased variance and the need to reconstruct for RoPE. A latent-space sparse approach can reduce KV footprint without RoPE overhead.

Method: Two key insights: (1) RoPE increases variance and rank of key vectors; (2) transformed key vectors preserve representations across many layers in latent space. SALS projects KV cache into a compact latent space via low-rank projection, and performs sparse token selection using RoPE-free query-key interactions in this space. Only a subset of important tokens are reconstructed, avoiding full KV cache reconstruction. 

Result: Empirical evaluation on LLaMA2-7b-chat and Mistral-7b; scalable to RULER-128k with LLaMA3.1-8B-Instruct. SALS achieves state-of-the-art performance with competitive accuracy, 6.4x KV cache compression, 5.7x speed-up in attention vs FlashAttention2 on 4K sequences, and end-to-end throughput improvements of 1.4x and 4.5x vs GPT-fast on 4K and 32K sequences respectively.

Conclusion: SALS effectively reduces KV cache size and speeds up attention with minimal accuracy loss, and scales to very long contexts, offering practical benefits for deploying long-context LLMs.

Abstract: Large Language Models capable of handling extended contexts are in high
demand, yet their inference remains challenging due to substantial Key-Value
cache size and high memory bandwidth requirements. Previous research has
demonstrated that KV cache exhibits low-rank characteristics within the hidden
dimension, suggesting the potential for effective compression. However, due to
the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive
low-rank compression suffers severe accuracy degradation or creates a new speed
bottleneck, as the low-rank cache must first be reconstructed in order to apply
RoPE. In this paper, we introduce two key insights: first, the application of
RoPE to the key vectors increases their variance, which in turn results in a
higher rank; second, after the key vectors are transformed into the latent
space, they largely maintain their representation across most layers. Based on
these insights, we propose the Sparse Attention in Latent Space framework. SALS
projects the KV cache into a compact latent space via low-rank projection, and
performs sparse token selection using RoPE-free query-key interactions in this
space. By reconstructing only a small subset of important tokens, it avoids the
overhead of full KV cache reconstruction. We comprehensively evaluate SALS on
various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and
additionally verify its scalability on the RULER-128k benchmark with
LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA
performance by maintaining competitive accuracy. Under different settings, SALS
achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention
operator compared to FlashAttention2 on the 4K sequence. For the end-to-end
throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared
to GPT-fast on 4k and 32K sequences, respectively.

</details>


### [79] [EDC: Equation Discovery for Classification](https://arxiv.org/abs/2510.24310)
*Guus Toussaint,Arno Knobbe*

Main category: cs.LG

TL;DR: 提出一个基于方程发现（Equation Discovery）的二分类框架 EDC，通过发现可管理规模的解析函数来界定决策边界的形状与位置。实验证明 EDC 在二分类的 ED 基方法中表现优于现有方法，且与最先进方法的性能相当；所给语法允许灵活但不过拟合的边界。


<details>
  <summary>Details</summary>
Motivation: 旨在通过可解释的符号回归框架来学习二分类边界，既能自动发现目标方程的结构，又能估计其参数，从而得到可解释且具有控制复杂度的模型。

Method: 提出 EDC 方法，利用一个 modest complexity 的文法来生成决策边界的解析函数，该文法包含线性、二次、指数项以及两个特征的乘积（可捕捉 XOR 之类的依赖），以获得可管理大小的边界。模型同时学习边界的结构与参数。

Result: 在人工与真实数据上的广泛实验中，EDC 能同时发现目标方程的结构与参数；相较于当前的基于 ED 的分类方法，EDC 在二分类任务上实现更好的表现，并在总体上达到与最先进二分类方法的性能接近的水平；文法设计使边界具有较高的表达能力，同时不过拟合。

Conclusion: 给定的中等复杂度文法在实现可解释且灵活的边界方面有效，且允许引入领域特定表达以适应特定任务；该方法展示了在可控复杂度下通过符号回归实现可解释二分类的潜力。

Abstract: Equation Discovery techniques have shown considerable success in regression
tasks, where they are used to discover concise and interpretable models
(\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary
classification framework. Our proposed method EDC finds analytical functions of
manageable size that specify the location and shape of the decision boundary.
In extensive experiments on artificial and real-life data, we demonstrate how
EDC is able to discover both the structure of the target equation as well as
the value of its parameters, outperforming the current state-of-the-art
ED-based classification methods in binary classification and achieving
performance comparable to the state of the art in binary classification. We
suggest a grammar of modest complexity that appears to work well on the tested
datasets but argue that the exact grammar -- and thus the complexity of the
models -- is configurable, and especially domain-specific expressions can be
included in the pattern language, where that is required. The presented grammar
consists of a series of summands (additive terms) that include linear,
quadratic and exponential terms, as well as products of two features (producing
hyperbolic curves ideal for capturing XOR-like dependencies). The experiments
demonstrate that this grammar allows fairly flexible decision boundaries while
not so rich to cause overfitting.

</details>


### [80] [Transformers can do Bayesian Clustering](https://arxiv.org/abs/2510.24318)
*Prajit Bhaskaran,Tom Viering*

Main category: cs.LG

TL;DR: 提出了 Cluster-PFN，一种基于 Transformer 的无监督贝叶斯聚类模型，扩展了 PFN 框架以进行贝叶斯聚类。通过对来自有限高斯混合模型先验的合成数据进行训练，能够估计聚类数和簇分配的后验。相比 AIC、BIC、变分推断（VI）等手工模型选择，聚类数估计更准确，聚类质量与 VI 相当，但速度快出若干数量级；在高缺失数据的现实数据集上，能够处理复杂先验并优于基于 imputation 的方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯聚类虽然能很好地表达不确定性，但在大规模数据上计算成本高；现实数据常有缺失值，简单填充忽略不确定性，导致 suboptimal 结果。需要一个可扩展且灵活的贝叶斯聚类方法，能够处理缺失数据及其不确定性。

Method: Cluster-PFN 将 Prior-Data Fitted Networks（PFN）扩展到无监督贝叶斯聚类。模型在来自有限高斯混合模型先验的合成数据上进行训练，学习目标是后验分布：簇的数量以及每个样本的簇分配。

Result: 在聚类数估计方面优于 AIC、BIC、VI 等手工模型选择；聚类质量与 VI 相当，同时在速度上快出若干数量级；在含缺失数据的复杂先验下，能超越基于 imputation 的基线，且在真实基因组数据集上形成高缺失率场景的优势。

Conclusion: Cluster-PFN 提供了一种可扩展、灵活的贝叶斯聚类方法，能够处理复杂先验（包括缺失数据），在无监督贝叶斯聚类任务中实现更高效的性能。

Abstract: Bayesian clustering accounts for uncertainty but is computationally demanding
at scale. Furthermore, real-world datasets often contain missing values, and
simple imputation ignores the associated uncertainty, resulting in suboptimal
results. We present Cluster-PFN, a Transformer-based model that extends
Prior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained
entirely on synthetic datasets generated from a finite Gaussian Mixture Model
(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over
both the number of clusters and the cluster assignments. Our method estimates
the number of clusters more accurately than handcrafted model selection
procedures such as AIC, BIC and Variational Inference (VI), and achieves
clustering quality competitive with VI while being orders of magnitude faster.
Cluster-PFN can be trained on complex priors that include missing data,
outperforming imputation-based baselines on real-world genomic datasets, at
high missingness. These results show that the Cluster-PFN can provide scalable
and flexible Bayesian clustering.

</details>


### [81] [What do vision-language models see in the context? Investigating multimodal in-context learning](https://arxiv.org/abs/2510.24331)
*Gabriel O. dos Santos,Esther Colombini,Sandra Avila*

Main category: cs.LG

TL;DR: 在多模态视觉语言模型（VLMs）的上下文中学习（ICL）能力有限且易受训练数据、提示设计、架构与训练策略影响。当前VLM对视觉信息的利用不足，更多依赖文本线索；需改进对视觉-文本的耦合以提升从多模态上下文学习的能力。


<details>
  <summary>Details</summary>
Motivation: 探索在Vision-Language Models中是否存在类似于LLMs的就地学习能力（ICL），以及哪些因素影响其有效性，从而弥补对VLM ICL的研究空缺。

Method: 对七个模型、四种架构、三个图像字幕基准进行系统评估；分析提示设计、架构选择和训练策略对多模态ICL的影响；首次分析随着演示示例增加，VLM中的注意力模式；比较使用imag-text interleaved数据的训练和instruction tuning对ICL能力的影响。

Result: 训练在imag-text交错数据上的模型提升了ICL性能，但未有效实现演示示例中的视觉信息与文本信息的融合；instruction tuning提升了指令遵循能力，但降低了对上下文演示的依赖，表明指令对齐与在-context自适应之间存在权衡；注意力分析显示当前VLM主要关注文本线索，未能充分利用视觉信息，显示现有VLM的多模态整合能力有限。

Conclusion: 揭示了当前VLM在ICL中的关键局限性，提供改进多模态上下文学习的方向，如加强视觉与文本信息的耦合、设计更有效的多模态提示，以及在指令对齐与ICL自适应之间取得平衡。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks
from demonstration examples without parameter updates. Although it has been
extensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)
remains underexplored. In this work, we present a systematic study of ICL in
VLMs, evaluating seven models spanning four architectures on three image
captioning benchmarks. We analyze how prompt design, architectural choices, and
training strategies influence multimodal ICL. To our knowledge, we are the
first to analyze how attention patterns in VLMs vary with an increasing number
of in-context demonstrations. Our results reveal that training on imag-text
interleaved data enhances ICL performance but does not imply effective
integration of visual and textual information from demonstration examples. In
contrast, instruction tuning improves instruction-following but can reduce
reliance on in-context demonstrations, suggesting a trade-off between
instruction alignment and in-context adaptation. Attention analyses further
show that current VLMs primarily focus on textual cues and fail to leverage
visual information, suggesting a limited capacity for multimodal integration.
These findings highlight key limitations in the ICL abilities of current VLMs
and provide insights for enhancing their ability to learn from multimodal
in-context examples.

</details>


### [82] [Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning](https://arxiv.org/abs/2510.24356)
*Suman Sanyal*

Main category: cs.LG

TL;DR: PeL通过与任务无关的感知信号来优化感知接口，实现在决策与感知解耦，强调稳定性、信息性与几何受控的无标签感知属性，并提供一套任务无关的评价指标。


<details>
  <summary>Details</summary>
Motivation: 当前学习系统往往将感知和决策绑定在一起，依赖带标签的下游任务。提出分离感知与决策，专注于可验证的感知性质，以提高鲁棒性、可移植性和泛化性；希望通过不改变任务目标的前提下提升感知质量。

Method: 优化感知接口f_phi(X)->Z，使用任务无关信号进行更新，与下游决策g_theta(Z)->Y解耦。定义并追踪感知属性（对干扰的稳定性、信息含量、受控几何性），通过目标表示不变性度量来评估。 formalize separation; 证明保留充分不变量的更新与贝叶斯任务风险梯度正交；提供一组任务无关评估指标。

Result: 理论层面给出分离框架与不变量更新的正交性证明，以及一套用于证明感知质量的任务无关评估指标。结果更偏理论与方法论，尚待大量实验验证。

Conclusion: PeL提供一个明确的感知-决策分离框架，强调以无标签感知属性来提升感知质量及 downstream 泛化能力，并给出可用于验证感知质量的实际工具集，具有鲁棒性和可移植性潜力。

Abstract: We introduce Perception Learning (PeL), a paradigm that optimizes an agent's
sensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnostic
signals, decoupled from downstream decision learning
$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-free
perceptual properties, such as stability to nuisances, informativeness without
collapse, and controlled geometry, assessed via objective
representation-invariant metrics. We formalize the separation of perception and
decision, define perceptual properties independent of objectives or
reparameterizations, and prove that PeL updates preserving sufficient
invariants are orthogonal to Bayes task-risk gradients. Additionally, we
provide a suite of task-agnostic evaluation metrics to certify perceptual
quality.

</details>


### [83] [Filtering instances and rejecting predictions to obtain reliable models in healthcare](https://arxiv.org/abs/2510.24368)
*Maria Gabriela Valeriano,David Kohan Marzagão,Alfredo Montelongo,Carlos Roberto Veiga Kiffer,Natan Katz,Ana Carolina Lorena*

Main category: cs.LG

TL;DR: 提出一种两步数据驱动方法：在训练阶段利用实例难度(IH)筛选并净化数据，在推断阶段通过基于置信度的拒绝机制只保留高置信预测。结合三组真实医疗数据集的评估，展示在提升模型可靠性的同时尽量保留大量样本。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域的ML应用中，模型往往未能充分考虑不确定性，容易在低置信度时仍给出预测。需要通过数据质量提升和推断阶段的拒绝策略来提高整体可靠性。

Method: 第一步在训练阶段基于实例难度(IH)筛选并滤除难以学习的样本，从而提升数据集质量；第二步在推断阶段引入一个基于置信度的拒绝机制，只有达到可靠阈值的预测才被输出。作为对比，还使用影响值和不确定性作为基线评估过滤和拒绝的有效性。

Result: 在三组真实医疗数据集上，IH过滤结合置信度拒绝的方法显著提升了模型的可靠性，同时保持较高的样本保留率，与基线相比在预测性能与拒绝率之间取得更优的平衡。

Conclusion: 该数据驱动的两步方法为在安全关键应用中部署ML系统提供一种可行路径，兼顾预测性能与风险控制，尤其适用于对不确定性敏感的领域。

Abstract: Machine Learning (ML) models are widely used in high-stakes domains such as
healthcare, where the reliability of predictions is critical. However, these
models often fail to account for uncertainty, providing predictions even with
low confidence. This work proposes a novel two-step data-centric approach to
enhance the performance of ML models by improving data quality and filtering
low-confidence predictions. The first step involves leveraging Instance
Hardness (IH) to filter problematic instances during training, thereby refining
the dataset. The second step introduces a confidence-based rejection mechanism
during inference, ensuring that only reliable predictions are retained. We
evaluate our approach using three real-world healthcare datasets, demonstrating
its effectiveness at improving model reliability while balancing predictive
performance and rejection rate. Additionally, we use alternative criteria -
influence values for filtering and uncertainty for rejection - as baselines to
evaluate the efficiency of the proposed method. The results demonstrate that
integrating IH filtering with confidence-based rejection effectively enhances
model performance while preserving a large proportion of instances. This
approach provides a practical method for deploying ML systems in
safety-critical applications.

</details>


### [84] [Methodology for Comparing Machine Learning Algorithms for Survival Analysis](https://arxiv.org/abs/2510.24473)
*Lucas Buk Cardoso,Simone Aldrey Angelo,Yasmin Pacheco Gil Bonilha,Fernando Maia,Adeylson Guimarães Ribeiro,Maria Paula Curado,Gisele Aparecida Fernandes,Vanderlei Cunha Parro,Flávio Almeida de Magalhães Cipparrone,Alexandre Dias Porto Chiavegatto Filho,Tatiana Natasha Toporcov*

Main category: cs.LG

TL;DR: 比较六种 MLSA 在结直肠癌患者生存分析中的性能；XGB-AFT 表现最佳，GBSA 与 RSF 紧随其后。


<details>
  <summary>Details</summary>
Motivation: 在含删失数据的临床生存分析中，系统比较多种机器学习模型的预测性能及可解释性，以提升生存预测的准确性和决策支持。

Method: 使用约4.5万例来自圣保罗市癌症登记的数据；比较 Random Survival Forest、Gradient Boosting for Survival Analysis、Survival SVM、XGBoost-Cox、XGBoost-AFT、LightGBM；超参数优化采用不同采样策略；评估指标包括 C-Index、IPCW-C、时间依赖 AUC、集成 Brier 误差 IBS；用 SHAP 与置换重要性进行特征解释。

Result: XGB-AFT 最优（C-Index 0.7618；IPCW 0.7532），GBSA 与 RSF 次之；模型生成的生存曲线与分类模型预测有比较；MLSAs 展现潜力与适用性以改进生存预测并支持决策。

Conclusion: MLSAs 对生存预测具有潜力和实际应用价值，尤其在处理删失数据方面；需要进一步验证并探索解释性与外部推广性。

Abstract: This study presents a comparative methodological analysis of six machine
learning models for survival analysis (MLSA). Using data from nearly 45,000
colorectal cancer patients in the Hospital-Based Cancer Registries of S\~ao
Paulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for
Survival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),
XGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival
considering censored data. Hyperparameter optimization was performed with
different samplers, and model performance was assessed using the Concordance
Index (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score
(IBS). Survival curves produced by the models were compared with predictions
from classification algorithms, and predictor interpretation was conducted
using SHAP and permutation importance. XGB-AFT achieved the best performance
(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results
highlight the potential and applicability of MLSA to improve survival
prediction and support decision making.

</details>


### [85] [MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis Trajectories in the ICU](https://arxiv.org/abs/2510.24500)
*Yong Huang,Zhongqi Yang,Amir Rahmani*

Main category: cs.LG

TL;DR: 提出 MIMIC-Sepsis：基于 MIMIC-IV 的标准化队列与基准框架，用于可重复的败血症轨迹建模，包含时间对齐的临床变量与治疗数据，提供从 Sepsis-3 出发的预处理管线、缺失值填充策略与治疗纳入策略，并设定早期死亡预测、住院时长估计和休克发作分类等基准任务。结果显示纳入治疗变量显著提升模型性能，Transformer 架构尤为显著。


<details>
  <summary>Details</summary>
Motivation: 当前关于败血症的研究常使用过时数据集、缺乏可重复的预处理流程、对临床干预的覆盖有限，亟需一个可重复、覆盖治疗干预且可比较的基准数据集与评估任务来推动临床预测模型的发展。

Method: 从 MIMIC-IV 中构建 35,239 名 ICU 患者的队列，提供时间对齐的临床变量与标准化治疗数据（血管活性药、液体、机械通气、抗生素）。基于 Sepsis-3 设定的透明预处理管线，结构化的缺失值填充策略，以及治疗因素的纳入与释放。设定早期死亡预测、住院时长估计、休克发作分类等基准任务。

Result: 将治疗变量纳入模型能显著提升性能，且 Transformer 等复杂模型对治疗信息的利用尤为有效。作为一个稳健的平台，MIMIC-Sepsis 可用于评估关键护理预测与序列模型。

Conclusion: MIMIC-Sepsis 为在重症监护研究中评估预测与序列模型提供可重复、覆盖治疗干预的基准框架，有助于推动临床预测方法的比较与改进。

Abstract: Sepsis is a leading cause of mortality in intensive care units (ICUs), yet
existing research often relies on outdated datasets, non-reproducible
preprocessing pipelines, and limited coverage of clinical interventions. We
introduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from
the MIMIC-IV database, designed to support reproducible modeling of sepsis
trajectories. Our cohort includes 35,239 ICU patients with time-aligned
clinical variables and standardized treatment data, including vasopressors,
fluids, mechanical ventilation and antibiotics. We describe a transparent
preprocessing pipeline-based on Sepsis-3 criteria, structured imputation
strategies, and treatment inclusion-and release it alongside benchmark tasks
focused on early mortality prediction, length-of-stay estimation, and shock
onset classification. Empirical results demonstrate that incorporating
treatment variables substantially improves model performance, particularly for
Transformer-based architectures. MIMIC-Sepsis serves as a robust platform for
evaluating predictive and sequential models in critical care research.

</details>


### [86] [Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments](https://arxiv.org/abs/2510.24503)
*Mortesa Hussaini,Jan Theiß,Anthony Stein*

Main category: cs.LG

TL;DR: 提出了一种改进的FedAvg方法FLIU（Federated Learning with Individualized Updates），通过一个简单的个性化更新步骤与自适应个性化因子，在数据分布异质的联邦学习中在局部性能与对分布外样本的泛化性之间取得更好的折中；在MNIST与CIFAR-10的IID和非IID Dirichlet分布下进行实验，显示相较于传统FedAvg具有更好的局部准确性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 在分布异质的联邦学习场景中，局部模型在本地训练中易收敛于各自局部最优，导致全局聚合偏离全局最优，出现客户端漂移。现有的个性化联邦学习侧重提升局部表现，往往忽略对分布外（OOD）样本的鲁棒性与泛化评估，需要一种能够同时关注局部表现和跨分布泛化的评估与方法。

Method: 在FedAvg框架上引入FLIU，即在全局聚合后增加一个简单的个性化更新步骤，结合自适应个性化因子对每个客户端进行个性化调整。通过在MNIST和CIFAR-10上，结合IID和Dirichlet构造的非IID分布以及更具挑战性的分布情况，进行多阶段、单通信轮内的阶段性指标评估，全面比较局部表现与泛化能力。

Result: 实验结果表明，FLIU在多种数据分布下在局部性能和泛化能力上相较于FedAvg及其他基线均表现出显著改善；通过对单轮内不同阶段的分析，揭示了局部更新与全局聚合之间的协同效果与时序动态。

Conclusion: FLIU提供了一种简洁且有效的联邦学习个性化策略，能够在数据异质性环境中更好地兼顾局部最优与分布外泛化能力。该研究强调在评估中纳入阶段性、局部及全局维度的综合指标，以推动鲁棒性更强的联邦学习框架的发展。

Abstract: In the context of Federated Learning with heterogeneous data environments,
local models tend to converge to their own local model optima during local
training steps, deviating from the overall data distributions. Aggregation of
these local updates, e.g., with FedAvg, often does not align with the global
model optimum (client drift), resulting in an update that is suboptimal for
most clients. Personalized Federated Learning approaches address this challenge
by exclusively focusing on the average local performances of clients' models on
their own data distribution. Generalization to out-of-distribution samples,
which is a substantial benefit of FedAvg and represents a significant component
of robustness, appears to be inadequately incorporated into the assessment and
evaluation processes. This study involves a thorough evaluation of Federated
Learning approaches, encompassing both their local performance and their
generalization capabilities. Therefore, we examine different stages within a
single communication round to enable a more nuanced understanding of the
considered metrics. Furthermore, we propose and incorporate a modified approach
of FedAvg, designated as Federated Learning with Individualized Updates (FLIU),
extending the algorithm by a straightforward individualization step with an
adaptive personalization factor. We evaluate and compare the approaches
empirically using MNIST and CIFAR-10 under various distributional conditions,
including benchmark IID and pathological non-IID, as well as additional novel
test environments with Dirichlet distribution specifically developed to stress
the algorithms on complex data heterogeneity.

</details>


### [87] [LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis](https://arxiv.org/abs/2510.24561)
*Qingyue Zhang,Chang Chu,Tianren Peng,Qi Li,Xiangyang Luo,Zhihao Jiang,Shao-Lun Huang*

Main category: cs.LG

TL;DR: 提出面向目标域数据的数据感知LoRA初始化框架LoRA-DA，基于渐近分析，将初始化问题分解为偏差和方差两项，通过Fisher-gradient表示偏差，考虑Fisher信息的方差，给出最优初始化并实现LoRA-DA，实验显示在多项基准上优于现有初始化方法，收敛更快、鲁棒性好、开销小。


<details>
  <summary>Details</summary>
Motivation: 解决现有LoRA初始化在未充分利用目标域数据、梯度方法仅依赖一阶梯度分解且缺乏理论基础的问题，提供一个数据感知且有理论支撑的初始化框架。

Method: 从最小化微调模型与目标模型参数差异的期望出发，得到包含一个偏差项（用Fisher-gradient近似以保留各向异性）和一个方差项（由采样不确定性通过Fisher信息刻画）的优化问题，并求解得到最优初始化；实现LoRA-DA算法，从少量目标域样本中估计两项并给出LoRA初始化。

Result: 在多项基准上，LoRA-DA在最终准确率方面优于现有初始化方法；收敛更快、更稳定，对秩鲁棒，初始化开销小。

Conclusion: 给出一个具有理论基础的数据感知LoRA初始化框架及高效算法，LoRA-DA在实践中具有显著的性能与效率提升，代码将公开。

Abstract: With the widespread adoption of LLMs, LoRA has become a dominant method for
PEFT, and its initialization methods have attracted increasing attention.
However, existing methods have notable limitations: many methods do not
incorporate target-domain data, while gradient-based methods exploit data only
at a shallow level by relying on one-step gradient decomposition, which remains
unsatisfactory due to the weak empirical performance of the one-step
fine-tuning model that serves as their basis, as well as the fact that these
methods either lack a rigorous theoretical foundation or depend heavily on
restrictive isotropic assumptions. In this paper, we establish a theoretical
framework for data-aware LoRA initialization based on asymptotic analysis.
Starting from a general optimization objective that minimizes the expectation
of the parameter discrepancy between the fine-tuned and target models, we
derive an optimization problem with two components: a bias term, which is
related to the parameter distance between the fine-tuned and target models, and
is approximated using a Fisher-gradient formulation to preserve anisotropy; and
a variance term, which accounts for the uncertainty introduced by sampling
stochasticity through the Fisher information. By solving this problem, we
obtain an optimal initialization strategy for LoRA. Building on this
theoretical framework, we develop an efficient algorithm, LoRA-DA, which
estimates the terms in the optimization problem from a small set of target
domain samples and obtains the optimal LoRA initialization. Empirical results
across multiple benchmarks demonstrate that LoRA-DA consistently improves final
accuracy over existing initialization methods. Additional studies show faster,
more stable convergence, robustness across ranks, and only a small
initialization overhead for LoRA-DA. The source code will be released upon
publication.

</details>


### [88] [DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment](https://arxiv.org/abs/2510.24574)
*Hao Wang,Licheng Pan,Yuan Lu,Zhixuan Chu,Xiaoxi Li,Shuting He,Zhichao Chen,Haoxuan Li,Qingsong Wen,Zhouchen Lin*

Main category: cs.LG

TL;DR: DistDF introduces a joint-distribution Wasserstein discrepancy to align conditional forecast distributions with the label distribution in time-series forecasting, addressing bias from label autocorrelation and enabling differentiable, tractable optimization that improves performance across diverse models.


<details>
  <summary>Details</summary>
Motivation: Standard direct forecast minimizes conditional negative log-likelihood often estimated by MSE, which is biased when label autocorrelation exists. There's a need for an objective that aligns the conditional forecast distribution with the true label distribution, accounting for dependencies over time.

Method: Propose DistDF which minimizes a discrepancy between the conditional forecast distribution and the label distribution using a joint-distribution Wasserstein metric for time-series. This discrepancy upper-bounds the targeted conditional discrepancy, is differentiable and estimable from empirical samples, and integrates with gradient-based training.

Result: Extensive experiments demonstrate that DistDF improves forecasting performance across diverse models and achieves state-of-the-art results.

Conclusion: DistDF provides a principled, tractable approach to time-series forecasting by aligning conditional and label distributions via a joint-Wasserstein discrepancy, with empirical gains and publicly available code.

Abstract: Training time-series forecast models requires aligning the conditional
distribution of model forecasts with that of the label sequence. The standard
direct forecast (DF) approach resorts to minimize the conditional negative
log-likelihood of the label sequence, typically estimated using the mean
squared error. However, this estimation proves to be biased in the presence of
label autocorrelation. In this paper, we propose DistDF, which achieves
alignment by alternatively minimizing a discrepancy between the conditional
forecast and label distributions. Because conditional discrepancies are
difficult to estimate from finite time-series observations, we introduce a
newly proposed joint-distribution Wasserstein discrepancy for time-series
forecasting, which provably upper bounds the conditional discrepancy of
interest. This discrepancy admits tractable, differentiable estimation from
empirical samples and integrates seamlessly with gradient-based training.
Extensive experiments show that DistDF improves the performance diverse
forecast models and achieves the state-of-the-art forecasting performance. Code
is available at https://anonymous.4open.science/r/DistDF-F66B.

</details>


### [89] [Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges](https://arxiv.org/abs/2510.24577)
*He Yang,Fei Ren,Hai-Sui Yu,Xiaohui Chen,Pei-Zhi Zhuang*

Main category: cs.LG

TL;DR: PIELM领域发展迅速，作者提出以综述/展望的形式梳理现状、挑战与机会，强调需要更稳健、可解释、泛化的框架。


<details>
  <summary>Details</summary>
Motivation: 当前尚无系统性综述，亟需对PIELM在求解具有尖锐梯度、非线性、高频行为、约束条件、以及多物理耦合等问题中的应用与挑战进行整理、总结与展望。

Method: 以观点/经验的形式对现有PIELM研究进行综合评述，分析在PDE求解中的困难与趋势，讨论未来研究方向与可能的改进路径。

Result: 对PIELM领域的进展进行梳理，明确关键挑战与研究机会，提出构建更稳健、可解释、通用的PIELM框架的方向。

Conclusion: 尽管取得显著进展，仍存在若干迫切挑战，未来需在鲁棒性、可解释性、泛化能力及工程/科学应用的落地方面深化研究。

Abstract: We are very delighted to see the fast development of physics-informed extreme
learning machine (PIELM) in recent years for higher computation efficiency and
accuracy in physics-informed machine learning. As a summary or review on PIELM
is currently not available, we would like to take this opportunity to show our
perspective and experience for this promising research direction. We can see
many efforts are made to solve PDEs with sharp gradients, nonlinearities,
high-frequency behavior, hard constraints, uncertainty, multiphysics coupling.
Despite the success, many urgent challenges remain to be tackled, which also
provides us opportunities to develop more robust, interpretable, and
generalizable PIELM frameworks with applications in science and engineering.

</details>


### [90] [Symbolic Snapshot Ensembles](https://arxiv.org/abs/2510.24633)
*Mingyue Liu,Andrew Cropper*

Main category: cs.LG

TL;DR: 单次运行的 ILP 加载并保存中间产生的假设，基于 MDL 权重对它们进行融合，从而在多项基准上提升预测准确性约 4%，且额外开销小于 1%。


<details>
  <summary>Details</summary>
Motivation: 传统 ILP 通常通过多次训练产生一个集成，其中每次训练产生一个假设。本文提出在一次训练中保存中间假设，并使用最小描述长度（MDL）加权融合这些假设，以获得性能提升且降低计算成本。

Method: 在一次 ILP 训练过程中保存多个中间产生的假设；利用最小描述长度原理对这些假设进行加权，形成一个集合预测的权重模型，并在游戏和视觉推理等基准上评估。

Result: 在多个基准上实验显示，所提方法相比单一假设提升约 4% 的预测准确性，同时总计算开销低于 1%。

Conclusion: 单次训练并结合中间假设的 MDL 加权方法能够在保持高效的同时实现对 ILP 集成的有效提升，具有潜在的实际应用价值与扩展空间。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. Most
ILP algorithms learn a single hypothesis from a single training run. Ensemble
methods train an ILP algorithm multiple times to learn multiple hypotheses. In
this paper, we train an ILP algorithm only once and save intermediate
hypotheses. We then combine the hypotheses using a minimum description length
weighting scheme. Our experiments on multiple benchmarks, including game
playing and visual reasoning, show that our approach improves predictive
accuracy by 4% with less than 1% computational overhead.

</details>


### [91] [Pearl: A Foundation Model for Placing Every Atom in the Right Location](https://arxiv.org/abs/2510.24670)
*Genesis Research Team,Alejandro Dobles,Nina Jovic,Kenneth Leidal,Pranav Murugan,David C. Williams,Drausin Wulsin,Nate Gruver,Christina X. Ji,Korrawat Pruegsanusak,Gianluca Scarpellini,Ansh Sharma,Wojciech Swiderski,Andrea Bootsma,Richard Strong Bowen,Charlotte Chen,Jamin Chen,Marc André Dämgen,Roy Tal Dew,Benjamin DiFrancesco,J. D. Fishman,Alla Ivanova,Zach Kagin,David Li-Bland,Zuli Liu,Igor Morozov,Jeffrey Ouyang-Zhang,Frank C. Pickard IV,Kushal S. Shah,Ben Shor,Gabriel Monteiro da Silva,Maxx Tessmer,Carl Tilbury,Cyr Vetcher,Daniel Zeng,Maruan Al-Shedivat,Aleksandra Faust,Evan N. Feinberg,Michael V. LeVine,Matteus Pan*

Main category: cs.LG

TL;DR: Pearl是一个用于蛋白-配体协folding的基础模型，通过大规模合成数据、SO(3)等变扩散模块和可控推断实现更高的精度与物理有效性，在Runs N' Poses与PoseBusters基准上超越AlphaFold 3等 baselines，且在口袋条件下实现3.6x提升，数据规模与性能呈正相关。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺、架构低效、姿态物理无效及在推断阶段无法充分利用辅助信息的问题；通过大规模数据、几何对称性尊重的模型和可控模板推断提高预测质量和泛化。

Method: 提出Pearl，包含：1) 大规模合成数据训练以缓解数据稀缺；2) 引入SO(3)-等变扩散模块以本质上遵循3D旋转对称性，提高泛化和样本效率；3) 可控推断，支持多链模板系统，适用于蛋白和非聚合组件，且具备双向无条件/有条件模式。

Result: 在关键指标RMSD<2Å的准确性和物理有效性方面超越AlphaFold 3及其他开源基线，分别在Runs N' Poses和PoseBusters基准上实现14.5%和14.2%的提升；在口袋条件的真实药物靶点集上实现3.6x提升（RMSD<1Å）；模型性能与训练数据规模正相关。

Conclusion: 通过数据规模与几何对称性友好的扩散架构，Pearl实现了蛋白-配体协折叠的新关系，确立新基准，且数据规模越大，性能越好。

Abstract: Accurately predicting the three-dimensional structures of protein-ligand
complexes remains a fundamental challenge in computational drug discovery that
limits the pace and success of therapeutic design. Deep learning methods have
recently shown strong potential as structural prediction tools, achieving
promising accuracy across diverse biomolecular systems. However, their
performance and utility are constrained by scarce experimental data,
inefficient architectures, physically invalid poses, and the limited ability to
exploit auxiliary information available at inference. To address these issues,
we introduce Pearl (Placing Every Atom in the Right Location), a foundation
model for protein-ligand cofolding at scale. Pearl addresses these challenges
with three key innovations: (1) training recipes that include large-scale
synthetic data to overcome data scarcity; (2) architectures that incorporate an
SO(3)-equivariant diffusion module to inherently respect 3D rotational
symmetries, improving generalization and sample efficiency, and (3)
controllable inference, including a generalized multi-chain templating system
supporting both protein and non-polymeric components as well as dual
unconditional/conditional modes. Pearl establishes a new state-of-the-art
performance in protein-ligand cofolding. On the key metric of generating
accurate (RMSD < 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold
3 and other open source baselines on the public Runs N' Poses and PoseBusters
benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the
next best model. In the pocket-conditional cofolding regime, Pearl delivers
$3.6\times$ improvement on a proprietary set of challenging, real-world drug
targets at the more rigorous RMSD < 1 \r{A} threshold. Finally, we demonstrate
that model performance correlates directly with synthetic dataset size used in
training.

</details>


### [92] [Eigenfunction Extraction for Ordered Representation Learning](https://arxiv.org/abs/2510.24672)
*Burak Varıcı,Che-Ping Tsai,Ritabrata Ray,Nicholas M. Boffi,Pradeep Ravikumar*

Main category: cs.LG

TL;DR: 提出一个通用模块化框架，用以从上下文内核中提取有序、可识别的特征函数（特征值/特征向量），超越仅恢复线性前几个特征的表示，结合低秩近似和 Rayleigh商优化两大范式，并在合成核和真实图像数据上验证，特征值作为重要性评分用于特征选择，支撑自适应维度表示的效率-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习与非对比学习目标隐式地对输入及其上下文关系的核进行谱分解，但往往只恢复核的前若干线性特征的线性张量，无法获得完整谱及特征排序和重要性。需要一个可辨识、可排序的特征函数集合来揭示特征的重要性与组织结构。

Method: 给出一个可模块化的框架，设计若干构建块以确保与上下文内核的兼容性并具备处理现代数据规模的能力；并将低秩近似与 Rayleigh商优化这两大主流方法论映射到该框架，协同实现对特征函数的提取与排序。

Result: 在合成核上验证了框架能够提取有序且可辨识的特征函数；在真实图像数据集上，恢复的特征值可作为有效的特征重要性分数用于特征选择，从而实现自适应维度表示下的效率-准确性权衡。

Conclusion: 提供了一个通用、可扩展的框架来提取有序、可辨识的特征函数，并通过实验展示其在理解特征排序与提高表示学习效率方面的潜在价值。

Abstract: Recent advances in representation learning reveal that widely used
objectives, such as contrastive and non-contrastive, implicitly perform
spectral decomposition of a contextual kernel, induced by the relationship
between inputs and their contexts. Yet, these methods recover only the linear
span of top eigenfunctions of the kernel, whereas exact spectral decomposition
is essential for understanding feature ordering and importance. In this work,
we propose a general framework to extract ordered and identifiable
eigenfunctions, based on modular building blocks designed to satisfy key
desiderata, including compatibility with the contextual kernel and scalability
to modern settings. We then show how two main methodological paradigms,
low-rank approximation and Rayleigh quotient optimization, align with this
framework for eigenfunction extraction. Finally, we validate our approach on
synthetic kernels and demonstrate on real-world image datasets that the
recovered eigenvalues act as effective importance scores for feature selection,
enabling principled efficiency-accuracy tradeoffs via adaptive-dimensional
representations.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [93] [A Simultaneous ECG-PCG Acquisition System with Real-Time Burst-Adaptive Noise Cancellation](https://arxiv.org/abs/2510.23819)
*Avishka Herath,Malith Jayalath,Kumudu Kaushalya,Sanjana Kapukotuwa,Chathuni Wijegunawardena,Pahan Mendis,Kithmin Wickremasinghe,Duminda Samarasinghe,Wageesha N. Manamperi,Chamira U. S. Edussooriya*

Main category: eess.SY

TL;DR: 提出一个端到端的实时自适应噪声抑制系统，集成在同时采集ECG和PCG的设备中，用于在嘈杂环境中改善心音信号的质量。


<details>
  <summary>Details</summary>
Motivation: 心音诊断易受环境噪声干扰，且便携系统需具备实时性以用于资源有限的设置。

Method: 开发实时自适应噪声抑制管线并集成到双模态设备，利用医院噪声数据集与设备录音进行验证，针对PCG和ECG信号分别实现噪声抑制。

Result: 在嘈杂的医院环境中，PCG信号SNR提升37.01 dB，ECG信号提升30.32 dB。

Conclusion: 该系统可提升在资源受限环境中的心脏筛查的可靠性和可及性，便携且具备实时性。

Abstract: Cardiac auscultation is an essential clinical skill, requiring excellent
hearing to distinguish subtle differences in timing and pitch of heart sounds.
However, diagnosing solely from these sounds is often challenging due to
interference from surrounding noise, and the information may be limited.
Existing solutions that adaptively cancel external noise are either not
real-time or are computationally intensive, making them unsuitable for
implementation in a portable system. This work proposes an end-to-end system
with a real-time adaptive noise cancellation pipeline integrated into a device
that simultaneously acquires electrocardiogram (ECG) and phonocardiogram (PCG)
signals. The performance of the system is validated using real-world hospital
noise datasets and recordings captured with the dual-modality device. For PCG
and ECG signals recorded from the device in noisy hospital settings, the
proposed algorithms achieved signal-to-noise ratio improvements of 37.01 dB and
30.32 dB, respectively. These results demonstrate the systems effectiveness in
enabling reliable and accessible cardiac screening, including noisy hospital
environments typical of resource-constrained settings.

</details>


### [94] [MDP-based Energy-aware Task Scheduling for Battery-less IoT](https://arxiv.org/abs/2510.23820)
*Shahab Jahanbazi,Mateen Ashraf,Onel L. A. López*

Main category: eess.SY

TL;DR: 提出基于MDP的最优稳态阈值调度（OSTB）以提升无电池IoT在环境能量下的长期任务完成率；与ALAP相比，OSTB在仿真中实现8.6%任务完成率提升、功率失败降低65%、执行延迟下降86.29%，基于4.7 mF电容。


<details>
  <summary>Details</summary>
Motivation: 在依赖环境能量的无电池物联网设备中，能源到达具有随机性和时变性，给任务调度的长期性能带来挑战，需设计能在能量波动中实现高长期完成率的策略。

Method: 基于马尔可夫决策过程（MDP）对能量波动进行建模，识别系统组成并定义两种奖励函数，分析MDP性质并求解得到最优的稳定性阈值调度策略（OSTB）。

Result: 通过仿真证明OSTB优于广为使用的“尽量晚执行”(ALAP)策略，在4.7 mF电容条件下实现8.6%任务完成率提升、65%功率失败减少、执行延迟下降86.29%。

Conclusion: 在能源自给的无电池IoT场景中，OSTB提供一条可实现的稳态最优调度路径，能显著提升长期性能并降低能量相关风险。

Abstract: Realizing high long-term task completion rates represents a fundamental
challenge in battery-less Internet of Things (IoT) devices powered by ambient
energy harvesting. This difficulty is primarily due to the stochastic and
time-varying characteristics of the available energy, which significantly
complicate the design of optimal task scheduling policies. In this paper, we
consider a battery-less IoT device that must periodically report sensing
measurements to a monitoring center. We adopt the Markov decision process (MDP)
framework to handle energy variability while aiming to maximize the long-term
task completion rate. For this, we first identify its components and then
define two appropriate reward functions. We demonstrate the inherent properties
associated with the MDP formulation and the related optimal policy.
Subsequently, we solve the resulting optimization problem, leading to the
optimal stationary threshold-based (OSTB) scheduling. Simulation results
demonstrate that OSTB outperforms the well-known ``as late as possible'' (ALAP)
scheduling strategy. For instance, an $8.6\%$ increase in the task completion
rate, along with a $65\%$ reduction in power failures and a $86.29\%$ decrease
in execution delays during task execution are registered assuming a $4.7$ mF
capacitor.

</details>


### [95] [Carbon-Aware Optimal Power Flow with Data-Driven Carbon Emission Tracing](https://arxiv.org/abs/2510.23877)
*Zhentong Shao,Nanpeng Yu*

Main category: eess.SY

TL;DR: 提出一种碳感知的最优潮流（OPF）框架，通过数据驱动的碳追踪推导发电机到负荷的碳排放分布因子，将平均与边际碳排放以线性形式嵌入直流OPF，支持实时碳优化调度。


<details>
  <summary>Details</summary>
Motivation: 在电力系统调度中需要量化节点级碳排放以支撑碳减排策略、碳交易和市场优化，提升系统运行的碳效率与经济性。

Method: 通过数据驱动的碳追踪生成发电机–负荷碳排放分布因子，推导出平均与边际碳排放的解析公式，并将其作为线性约束整合进DC OPF模型，从而实现对节点碳排放的快速估算与碳约束的线性化处理。

Result: 在IEEE测试系统上进行仿真，验证了碳排放估算的准确性与算法的计算效率，证明该碳感知OPF适用于实时系统操作。

Conclusion: 所提方法使市场运营者在不显著牺牲电力系统性能的前提下实现碳排放的优化调度，对实现碳减排目标具有潜在应用价值。

Abstract: Quantifying locational carbon emissions in power grids is crucial for
implementing effective carbon reduction strategies for customers relying on
electricity. This paper presents a carbon-aware optimal power flow (OPF)
framework that incorporates data-driven carbon tracing, enabling rapid
estimation of nodal carbon emissions from electric loads. By developing
generator-to-load carbon emission distribution factors through data-driven
technique, the analytical formulas for both average and marginal carbon
emissions can be derived and integrated seamlessly into DC OPF models as linear
constraints. The proposed carbon-aware OPF model enables market operators to
optimize energy dispatch while reducing greenhouse gas emissions. Simulations
on IEEE test systems confirm the accuracy and computational efficiency of the
proposed approach, highlighting its applicability for real-time carbon-aware
system operations.

</details>


### [96] [Modeling and Scheduling of Fusion Patterns in Autonomous Driving Systems (Extended Version)](https://arxiv.org/abs/2510.23895)
*Hoora Sobhani,Hyoseung Kim*

Main category: eess.SY

TL;DR: 面向自主驾驶系统的DAG融合模式分析与ILP优化框架。


<details>
  <summary>Details</summary>
Motivation: 现有DAG调度对数据融合触发机制过于简化，无法覆盖实际ADS中的多样化融合模式，因此需要一个能建模多种 fusion 模式并在离线生成确定性调度的框架。

Method: 提出一个ILP基础的方法，区分三种融合类型：定时触发(timer-triggered)、等待全部(wait-for-all)、即时融合(immediate fusion)，以优化反应时间、时差、信息新鲜度和响应时间等多重实时性能，并直接生成适用于真实平台的离线确定性调度。

Result: 在真实ADS案例、树莓派实现以及随机DAG上的评估表明，该框架能覆盖比现有工作更丰富的融合模式，并在可比情景下实现显著的性能提升。

Conclusion: 提供一个全面的分析与优化框架来处理ADS DAG中的融合模式，改进离线调度的确定性与现实平台的性能表现。

Abstract: In Autonomous Driving Systems (ADS), Directed Acyclic Graphs (DAGs) are
widely used to model complex data dependencies and inter-task communication.
However, existing DAG scheduling approaches oversimplify data fusion tasks by
assuming fixed triggering mechanisms, failing to capture the diverse fusion
patterns found in real-world ADS software stacks. In this paper, we propose a
systematic framework for analyzing various fusion patterns and their
performance implications in ADS. Our framework models three distinct fusion
task types: timer-triggered, wait-for-all, and immediate fusion, which
comprehensively represent real-world fusion behaviors. Our Integer Linear
Programming (ILP)-based approach enables an optimization of multiple real-time
performance metrics, including reaction time, time disparity, age of
information, and response time, while generating deterministic offline
schedules directly applicable to real platforms. Evaluation using real-world
ADS case studies, Raspberry Pi implementation, and randomly generated DAGs
demonstrates that our framework handles diverse fusion patterns beyond the
scope of existing work, and achieves substantial performance improvements in
comparable scenarios.

</details>


### [97] [Dynamical Modeling of Temperature and Smoke Evolution in a Thermal-Runaway Event of a Large-Format Lithium-ion Battery in a Mine Tunnel](https://arxiv.org/abs/2510.23910)
*Khadija Omar Said,Yukta Pareek,Satadru Dey,Ashish Ranjan Kumar*

Main category: eess.SY

TL;DR: Reduced-order dynamic models for lithium-ion battery thermal runaway in underground mining; effectively capture temperature and smoke trends with data-driven validation.


<details>
  <summary>Details</summary>
Motivation: To enable safe and scalable analysis of TR in large-format LIBs used in underground mining, given that high-fidelity models are expensive and dangerous to run for many scenarios.

Method: Construct dynamic reduced-order models (ROMs) within a reduced-order framework to mimic the transient-state combustion event; validation shows ROMs reproduce temperature and smoke trends and align with ground-truth data.

Result: ROMs reasonably replicate temperature and smoke trends with strong alignment to the ground-truth dataset, offering efficient and scalable simulations compared to high-fidelity models.

Conclusion: Reduced-order models provide a practical, data-consistent approach for simulating TR events in mining LIBs, enabling rapid scenario analysis and risk assessment.

Abstract: Large-format lithium-ion batteries (LIBs) provide effective energy storage
solutions for high-power equipment used in underground mining operations. They
have high Columbic efficiency and minimal heat and emission footprints.
However, improper use of LIBs, accidents, or other factors may increase the
probability of thermal runaway (TR), a rapid combustion reaction that
discharges toxic and flammable substances. Several such incidents have been
documented in mines. Since repeatable TR experiments to uncover the
transient-state propagation of TR are expensive and hazardous, high-fidelity
models are usually developed to mimic the impact of these events. They are
resource-intensive and are impractical to develop for many scenarios that could
be observed in a mine. Therefore, dynamic models within a reduced-order
framework were constructed to represent the transient-state combustion event.
Reduced order models (ROMs) reasonably replicate trends in temperature and
smoke, showing strong alignment with the ground-truth dataset.

</details>


### [98] [Sample-based Moving Horizon Estimation](https://arxiv.org/abs/2510.24191)
*Isabelle Krauss,Victor G. Lopez,Matthias A. Müller*

Main category: eess.SY

TL;DR: 提出一种基于样本的移动边界估计（MHE）方法，用于在测量不规则或低频采样时对一般非线性系统进行状态估计；通过设计适合不规则输出序列的代价函数并引入样本基的增量输入/输出到状态稳定性(i-IOSS)来实现鲁棒全局指数稳定（RGES）。对线性系统给出样本基可观测性与样本基IOSS之间的联系，并利用已有的线性系统可观测性条件来验证或设计采样策略以保证RGES。最后通过仿真实验验证所提方法。


<details>
  <summary>Details</summary>
Motivation: 解决在不规则/低频采样下的系统状态估计问题，并为基于MHE的估计提供鲁棒全局指数稳定性保障，此外建立样本层面的可观测性与i-IOSS之间的联系，使线性系统的采样策略设计具有理论依据。

Method: 为不规则输出序列设计MHE的代价函数并给出样本基的i-IOSS判据，给出在该判据下的RGES证明；在线性系统中，将样本基可观测性与样本基i-IOSS建立联系，并利用线性系统已有的可观测性条件来设计/验证采样方案以满足RGES；通过仿真实验验证方法效果。

Result: 在假设满足样本基IOSS条件下，证明了所提出的样本基MHE具有鲁棒全局指数稳定性；对线性系统，给出将样本基可观测性推导至采样策略设计以确保RGES的途径；仿真实验验证方法有效性与可行性。

Conclusion: 该工作将不规则采样下的MHE稳定性问题系统化，提供了在样本层面的可观测性与IOSS框架下的理论保障，并通过仿真示例展示了方法的实用性。

Abstract: In this paper, we propose a sample-based moving horizon estimation (MHE)
scheme for general nonlinear systems to estimate the current system state using
irregularly and/or infrequently available measurements. The cost function of
the MHE optimization problem is suitably designed to accommodate these
irregular output sequences. We also establish that, under a suitable
sample-based detectability condition known as sample-based incremental
input/output-to-state stability (i-IOSS), the proposed sample-based MHE
achieves robust global exponential stability (RGES). Additionally, for the case
of linear systems, we draw connections between sample-based observability and
sample-based i-IOSS. This demonstrates that previously established conditions
for linear systems to be sample-based observable can be utilized to verify or
design sampling strategies that satisfy the conditions to guarantee RGES of the
sample-based MHE. Finally, the effectiveness of the proposed sample-based MHE
is illustrated through a simulation example.

</details>


### [99] [Survey and Tutorial of Reinforcement Learning Methods in Process Systems Engineering](https://arxiv.org/abs/2510.24272)
*Maximilian Bloor,Max Mowbray,Ehecatl Antonio Del Rio Chanona,Calvin Tsay*

Main category: eess.SY

TL;DR: 对强化学习在过程系统工程（PSE）中的应用进行系统性综述与教程，包括基础概念、主流算法族及在发酵批次/连续过程控制、过程优化与供应链等领域的应用，并对未来研究方向进行梳理。


<details>
  <summary>Details</summary>
Motivation: 在高不确定性和复杂性的过程系统中，传统方法往往难以有效控制与优化；RL提供数据驱动的策略以应对这些挑战；需要面向PSE社区系统整合现有RL进展。

Method: 通过讲解基础概念、按价值基础、策略基础、以及actor-critic等算法族组织教程；系统性综述已有在PSE领域的应用；讨论专门技术与新兴方向。

Result: 提供一个结构化的知识梳理，综合成功经验、挑战、趋势，并提出面向PSE的未来研究路线与实践建议。

Conclusion: RL具备在PSE中开发数据驱动控制策略的潜力，需进一步对接PSE具体场景，识别并填补研究空白，推动两领域的深度融合。

Abstract: Sequential decision making under uncertainty is central to many Process
Systems Engineering (PSE) challenges, where traditional methods often face
limitations related to controlling and optimizing complex and stochastic
systems. Reinforcement Learning (RL) offers a data-driven approach to derive
control policies for such challenges. This paper presents a survey and tutorial
on RL methods, tailored for the PSE community. We deliver a tutorial on RL,
covering fundamental concepts and key algorithmic families including
value-based, policy-based and actor-critic methods. Subsequently, we survey
existing applications of these RL techniques across various PSE domains, such
as in fed-batch and continuous process control, process optimization, and
supply chains. We conclude with PSE focused discussion of specialized
techniques and emerging directions. By synthesizing the current state of RL
algorithm development and implications for PSE this work identifies successes,
challenges, trends, and outlines avenues for future research at the interface
of these fields.

</details>


### [100] [Development of a Digital Twin for an Electric Vehicle Emulator Modeling, Control, and Experimental Validation](https://arxiv.org/abs/2510.24389)
*Lamine Chalal,Ahmed Rachid*

Main category: eess.SY

TL;DR: 提出一个基于能量宏观表示法（EMR）的数字孪生框架，用于缩放EV仿真器的纵向动力学建模与控制；在实验台上验证，并给出最大可容纳质量与四象限工作切换策略。


<details>
  <summary>Details</summary>
Motivation: 在EV动力传动系统的能量流与功率管理中，现有图形建模工具难以清晰地表示能量交互及推导控制结构，需一种更直观且系统化的方法来进行能量管理验证与控制设计。

Method: 构建一个包含分励直流电机、四象限DC-DC变换器、蓄电池仿真器和机械负载仿真器的数字孪生，通过牛顿第二定律建模牵引力、空气阻力和坡道阻力等动力学；采用EMR框架清晰表示能量交互并导出控制结构，设计基于EMR的能量流控制策略以通过电枢电压实现速度控制；在Lucas-Nulle测试台进行实验验证，且提出最大可容纳质量的计算方法（对180 W、1900 rpm的电机为13.5 kg），并实现双向变换器切换算法以确保四象限可靠运行。

Result: 仿真结果与实验结果高度相关，验证了EMR数字孪生在EV仿真、控制设计与能量管理验证中的有效性；给出具体的工作边界（最大质量13.5 kg）和电机与转速约束下的可靠性实现；提出的切换算法实现了四象限稳定操作。

Conclusion: 基于EMR的数字孪生框架为EV仿真、能量管理验证以及控制设计提供了一种可扩展且清晰的系统化方法，能够有效支撑多物理量耦合的能量流动建模与控制实现。

Abstract: This paper presents the development and validation of a digital twin for a
scaled-down electric vehicle (EV) emulator, designed to replicate longitudinal
vehicle dynamics under diverse operating conditions. The emulator integrates a
separately excited DC motor (SEDCM), a four-quadrant DC-DC converter, a battery
emulator, and a mechanical load emulator. The system models tractive effort,
aerodynamic drag, and gradient resistance using Newton's second law. In
contrast to conventional graphical modeling tools (e.g., block diagrams and
bond graphs), the adopted Energetic Macroscopic Representation (EMR) framework
offers clear advantages by explicitly representing energy interactions and
facilitating the systematic derivation of control structures. A control
strategy developed within this framework governs energy flow across the
powertrain, enabling accurate speed control via armature voltage regulation.
Experimental tests conducted on a Lucas-Nulle test bench show strong
correlation with simulation results. The study also introduces a methodology to
compute the maximum admissible vehicle mass - determined to be 13.5 kg for a
180 W motor operating at 1900 rpm - based on acceleration and slope
constraints. Furthermore, a switching algorithm for the bidirectional converter
ensures reliable four quadrant operation. Overall, the proposed framework
provides a scalable and effective approach for EV emulation, control design,
and energy management validation.

</details>


### [101] [Analyzing Parametric Oscillator Ising Machines through the Kuramoto Lens](https://arxiv.org/abs/2510.24416)
*Nikhat Khan,E. M. H. E. B. Ekanayake,Nicolas Casilli,Cristian Cassella,Luke Theogarajan,Nikhil Shukla*

Main category: eess.SY

TL;DR: 提出一种基于斯图特-兰道方程的同震耦合参数振荡器Ising机的规范相位描述，结合卡拉莫托相位耦合与内在相位和项，解释为何无需显式二次谐波驱动，并分析幅度异质性对自旋相互作用强度及解质量的影响，提供统一的设计视角。


<details>
  <summary>Details</summary>
Motivation: 建立一个统一、可推广的相位描述框架，以连接传统的基于振荡器的Ising机实现（如DOPO等）和Kuramoto型耦合模型，便于比较不同实现的共性与差异。

Method: 以Stuart–Landau振子作为Hopf分岔正则形式的canonical模型，推导出包含相位差耦合和共轭耦合所产生的相位和项的耦合动力学；解释为何在参数振荡器中无需显式的二次谐波驱动，并分析幅度分布不均（quasi-steady amplitude heterogeneity）对耦合强度的等效削弱，以及对解质量的潜在负面影响。

Result: 给出一个Kuramoto风格的统一相位描述，揭示标准相差耦合与额外的相位和项在parametric-oscillator Ising机中的作用；澄清实现差异的物理来源，提供对设计和优化的定量/定性见解。

Conclusion: 建立 oscillator-based Ising machines 的统一理论框架，方便对比不同实现、指导系统设计、提升解质量并促进新实现的开发。

Abstract: Networks of coupled nonlinear oscillators are emerging as powerful physical
platforms for implementing Ising machines. Yet the relationship between
parametric-oscillator implementations and traditional oscillator-based Ising
machines remains underexplored. In this work, we develop a Kuramoto-style,
canonical phase description of parametric oscillator Ising machines by starting
from the Stuart-Landau oscillator model- the canonical normal form near a Hopf
bifurcation, and a natural reduced description for many parametric oscillator
implementations such as the degenerate optical parametric oscillator (DOPO)
among others. The resulting phase dynamics combine the usual phase-difference
coupling observed in the standard Kuramoto model along with an intrinsic phase
sum term that is generated when conjugate coupling is considered. Moreover, our
formulation helps explain why explicit second-harmonic driving is unnecessary
in parametric oscillators and also reveals how quasi-steady amplitude
heterogeneity scales the original strength of the spin interaction with
potentially adverse impacts on the solution quality. Our work helps develop a
unifying view of the oscillator-based approach to designing Ising machines.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [102] [Communication in a Fractional World: MIMO MC-OTFS Precoder Prediction](https://arxiv.org/abs/2510.23832)
*Evan Allen,Karim Said,Robert Calderbank,Lingjia Liu*

Main category: eess.SP

TL;DR: 在高移动场景下，提出结合OTFS和物理信息驱动的基组展开模型，以利用过时CSI实现高机动性MIMO的预测与优化。


<details>
  <summary>Details</summary>
Motivation: 传统时频CSI反馈在高移动性下难以稳定，需利用更稳健的时-延迟-多普勒表征。OTFS提供延迟-多普勒域稳定性，本文 aims 将过时CSI 转化为有用信息以支撑MIMO在VANET/FANET等场景的长期应用。

Method: 推导OTFS输入输出关系在时间上的变化表达，并基于此建立一个物理信息约束的复杂指数基组展开模型(BEM)预测框架，以在整数和分数延迟-多普勒通道中最大化过时CSI的有用性，提升高机动性MIMO的性能。

Result: 给出OTFS I/O关系时间变化的表达式，并提出一个利用过时CSI的预测框架（物理信息驱动的BEM），理论上适用于处理整数和分数D通道以支持高移动性MIMO。

Conclusion: 将OTFS与物理信息驱动的BEM结合，可在高移动性场景中更有效地利用过时CSI，促进MIMO在VANETs/FANETs等应用中的长期可用性。

Abstract: As 6G technologies advance, international bodies and regulatory agencies are
intensifying efforts to extend seamless connectivity especially for
high-mobility scenarios such as Mobile Ad-Hoc Networks (\textit{MANETs}) types
such as Vehicular Ad-Hoc Networks (\textit{VANETs}) and Flying Ad-Hoc Networks
(\textit{FANETs}). For these environments to be considered for long term
adoption and use they must support Multiple-Input-Multiple- (MIMO) technology,
rapidly fluctuating channel conditions in these environments place a heavy
burden on traditional time-frequency CSI feedback schemes required for MIMO
precoding. This motivates a shift toward delay-Doppler representations like
those employed by Orthogonal Time-Frequency Space(OTFS) modulation, which
offers greater stability under mobility. We derive an expression for the
variation over time in the OTFS I/O relationship. We then use this to create a
physics informed complex exponential basis expansion model prediction framework
that maximizes the usefulness of outdated Channel State Information (CSI) in
the presence of integer and fractional delay-Doppler channels and facilitates
high mobility MIMO communication.

</details>


### [103] [Coordinated Multipoint Transmission in Pinching Antenna Systems](https://arxiv.org/abs/2510.23837)
*Ali Amhaz,Shreya Khisa,Mohamed Elhattab,Chadi Assi,Sanaa Sharafeddine*

Main category: eess.SP

TL;DR: 提出一种基于梯度元学习（GML）的联合优化方法，用于两基站CoMP系统中，在PINCHING天线系统（PASS）辅助下，联合优化波束成形与针夹位置以提升系统总体速率。实验显示该方法可达到近似最优解的92%，并优于基线，且相比传统CoMP获得更高的速率上界。


<details>
  <summary>Details</summary>
Motivation: 在CoMP场景中，传统均匀线阵（ULA）存在速率上限且大尺度衰落影响显著；PASS被提出以缓解衰落并提升MIMO性能，因此需要一个高效的非凸优化策略以同时优化波束与针夹布置。

Method: 将目标设定为在满足QoS约束的前提下，联合确定发送波束向量和波导上的针夹位置以最大化可实现的总速率。该优化问题高度非凸且变量耦合强，因此引入面向大规模优化设计的梯度元学习（GML）策略来求解。

Result: 数值分析表明，所提GML方法可达到约92%的最优解，且相较其他基准具有优越性；此外，与传统CoMP系统相比，所提出方法在可实现速率上界方面表现更优。

Conclusion: 在耦合且非凸的CoMP优化问题中，GML方法对PASS与CoMP的结合具有显著的性能提升潜力，证明了PASS在多基站协同传输中的应用价值。

Abstract: We study a coordinated multi-point (CoMP) transmission where two base
stations (BSs), each supported by a pinching antenna system (PASS), are
deployed to jointly serve communication users under spatial division multiple
access (SDMA) technology. Pinching Antenna technology was introduced as a
promising solution to overcome the large-scale fading that has been shown to be
an impediment in multiple-input multiple-output (MIMO) systems. To realize the
advantages of this technology in CoMP systems, which suffer from an upperbound
rate limitation when traditional uniform linear arrays (ULAs) are adopted, we
formulate an optimization problem with the aim of maximizing the achievable sum
rate by jointly determining the transmit beamforming vectors and pinching
locations on the waveguides while respecting the quality of service (QoS)
requirements of users. This problem is inherently non-convex due to the strong
coupling among its decision parameters, making it challenging to solve using
traditional optimization methods. Thus, we utilize a gradient-based
meta-learning (GML) strategy specifically designed for large-scale optimization
tasks. Finally, numerical analysis demonstrates the effectiveness of the
proposed GML approach, achieving 92 percent of the optimal solution, and the
superiority of the solution presented compared to other benchmarks. In
addition, it achieves a higher upper bound on the achievable rate compared to
conventional CoMP systems.

</details>


### [104] [Learning-based Spectral Regression for Cocoa Bean Physicochemical Property Prediction](https://arxiv.org/abs/2510.23892)
*Kebin Contreras,Emmanuel Martinez,Brayan Monroy,Sebastian Ardila,Cristian Ramirez,Mariana Caicedo,Hans Garcia,Tatiana Gelvez-Barrera,Juan Poveda-Jaramillo,Henry Arguello,Jorge Bacca*

Main category: eess.SP

TL;DR: 使用VIS-NIR光谱与学习回归模型对可可豆进行非破坏性质量评估，达到高R2与跨区域泛化的性能。


<details>
  <summary>Details</summary>
Motivation: 需要快速、非侵入、可扩展的可可豆质量评估方法，替代耗时的实验室分析，减少破坏与成本。

Method: 在传送带系统上集成VIS-NIR光谱仪，收集可可豆光谱；以标准实验室分析作为地面实况；建立回归模型；在哥伦比亚多地区及秘鲁Cusco进行地理独立评估。

Result: 各物理化学性质的R2均超过0.98；地理独立样本准确度为0.96。

Conclusion: 非破坏性方法具有可扩展性，是传统实验室方法的可替代方案，适用于可可生产链的质量评估。

Abstract: Cocoa bean quality assessment is essential for ensuring compliance with
commercial standards, protecting consumer health, and increasing the market
value of the cocoa product. The quality assessment estimates key
physicochemical properties, such as fermentation level, moisture content,
polyphenol concentration, and cadmium content, among others. This assessment
has traditionally relied on the accurate estimation of these properties via
visual or sensory evaluation, jointly with laboratory-based physicochemical
analyses, which are often time-consuming, destructive, and difficult to scale.
This creates the need for rapid, reliable, and noninvasive alternatives.
Spectroscopy, particularly in the visible and near-infrared ranges, offers a
non-invasive alternative by capturing the molecular signatures associated with
these properties. Therefore, this work introduces a scalable methodology for
evaluating the quality of cocoa beans by predicting key physicochemical
properties from the spectral signatures of cocoa beans. This approach utilizes
a conveyor belt system integrated with a VIS-NIR spectrometer, coupled with
learning-based regression models. Furthermore, a dataset is built using cocoa
bean batches from Santander, Colombia. Ground-truth reference values were
obtained through standardized laboratory analyses and following commercial
cocoa quality regulations. To further evaluate the proposed methodology's
generalization, performance is tested on samples collected from other Colombian
regions and from Cusco, Peru. Experimental results show that the proposed
models achieved R2 scores exceeding 0.98 across all physicochemical properties,
and reached 0.96 accuracy on geographically independent samples. This
non-destructive approach represents a suitable and scalable alternative to
conventional laboratory methods for quality assessment across the cocoa
production chain.

</details>


### [105] [LEO Downlink Channel Model Revisited: Scattering Geometry-Inspired Derivation](https://arxiv.org/abs/2510.23900)
*Kuan-Po Chiu,Sumit Roy*

Main category: eess.SP

TL;DR: 从第一性原理推导LEO到地面NLOS的PSD，填补几何敏感PSD表征空缺，并揭示PSD特征与传播几何的因果关系。


<details>
  <summary>Details</summary>
Motivation: 现有PSD模型缺乏几何感知的NLOS描述，难以解释PSD特征与轨道几何之间的因果关系。

Method: 基于第一性原理，结合LEO地面传播几何，推导NLOSPSD的表达，并使之能够复现已有结果、揭示几何参数对PSD的决定作用。

Result: 得到具有几何敏感性的PSD推导式，能在不同几何参数下与先前工作结果保持一致，并解释PSD特征的成因。

Conclusion: 提供一个物理一致、几何可解释的LEO到地面NLOS PSD建模框架，用于改进信道建模与系统设计。

Abstract: This paper presents a new derivation of LEO-to-ground receiver channel model
to address a clear gap in the prior art: the lack of an appropriate geometry
aware characterization of non LOS (NLOS) link model represented by the power
spectral density (PSD). Specifically, the main contribution is a coherent
derivation of the PSD from 1st principles that is able to reproduce results in
prior art and explain the causal relationship of main PSD features to the
propagation geometry parameters.

</details>


### [106] [Localized Acoustic-Event Measurement Probe: Connector Confirmation Utilizing Acoustic Signatures](https://arxiv.org/abs/2510.24017)
*Brian Skoglind,Travis Roberts,Sourabh Karmakar,Cameron Turner,Laine Mears*

Main category: eess.SP

TL;DR: 基于声学信号的电连接接合检测：通过噪声抑制与信号放大提升SNR，在现有人工连接流程上实现≥75%检测/分类效果。


<details>
  <summary>Details</summary>
Motivation: 解决手动连接中由于连接件多样、位置变化与工人疲劳等原因导致的松动或错连；现有的视觉/增强现实（AR）或电路参数测量方法在正确连接状态检测方面能力有限；接合声信号的信噪比通常较低且易被工厂环境噪声淹没。

Method: 构建背景噪声抑制的物理系统并提出“成功噪声特征放大算法”，以提升电连接接触配合声与环境声景之间的SNR，从而提高连接状态检测与判断的鲁棒性。

Result: 系统在检测与分类连接状态方面的效果超过75%。

Conclusion: 该方法无需修改现有的手动互连流程即可实现声学连接状态检测，具备在自动化生产线中落地的潜力。

Abstract: Modern consumer products are full of interconnected electrical and electronic
modules to fulfill direct and indirect needs. In an automated assembly line
still, most of these interconnections are required to be done manually due to
the large variety of connector types, connector positions, and the soft,
flexible nature of their structures. The manual connection points are the
source of partial or completely loose connections. Sometimes connections are
missed due to the application of unequal mating forces and natural human
fatigue. Subsequently, these defects can lead to unexpected downtime and
expensive rework. For successful connection detection, past approaches such as
vision verification, Augmented Reality, or circuit parameter-based measurements
have shown limited ability to detect the correct connection state. Though most
connections emit a specific noise for successful mating, the acoustic-based
verification system for electrical connection confirmation has not been
extensively researched. The main discouraging reason for such research is the
typically low signal-to-noise ratio (SNR) between the sound of a pair of
electrical connector mating and the diverse soundscape of the plant. In this
study, the authors investigated increasing the SNR between the electrical
connector mating sound and the plant soundscape to improve connection success
detection by employing a physical system for background noise mitigation and
the successful met noise signature amplification algorithm. The solution is
over 75% effective at detecting and classifying connection state. The solution
has been constructed without any modification to the existing manual
interconnection process.

</details>


### [107] [PULSE: Privileged Knowledge Transfer from Electrodermal Activity to Low-Cost Sensors for Stress Monitoring](https://arxiv.org/abs/2510.24058)
*Zihan Zhao,Masood Mortazavi,Ning Yan*

Main category: eess.SP

TL;DR: PULSE在自监督预训练阶段只使用EDA，并在推断阶段无需EDA，利用ECG、BVP、ACC、TEMP等模态实现压力检测；通过共享/私有嵌入和教师蒸馏，将EDA的表示传递给低成本传感器，在WESAD数据集上获得良好性能，同时降低硬件成本。


<details>
  <summary>Details</summary>
Motivation: 解决高成本EDA硬件在现实穿戴场景中的局限性；通过在预训练阶段使用多模态数据并保留EDA作为特权知识，实现跨模态对齐和模态不变表示，在蒸馏阶段将EDA教师的符号性唤醒表示迁移到学生编码器。

Method: 将编码器输出分为共享和私有嵌入；跨模态对齐共享嵌入并融合成模态不变表示，私有嵌入承载模态特定信息以支撑重构目标。预训练后进行知识迁移，由冻结的EDA教师将交感唤醒表示传递给学生编码器。

Result: 在WESAD数据集上，方法实现强的压力检测性能，表明特权EDA的表示能够传递给低成本传感器以提高准确性并降低硬件成本。

Conclusion: 特权EDA表示可以在无需额外成本的前提下通过蒸馏传递给常规传感器，从而提升效能并降低系统成本。

Abstract: Electrodermal activity (EDA), the primary signal for stress detection,
requires costly hardware often unavailable in real-world wearables. In this
paper, we propose PULSE, a framework that utilizes EDA exclusively during
self-supervised pretraining, while enabling inference without EDA but with more
readily available modalities such as ECG, BVP, ACC, and TEMP. Our approach
separates encoder outputs into shared and private embeddings. We align shared
embeddings across modalities and fuse them into a modality-invariant
representation. The private embeddings carry modality-specific information to
support the reconstruction objective. Pretraining is followed by knowledge
transfer where a frozen EDA teacher transfers sympathetic-arousal
representations into student encoders. On WESAD, our method achieves strong
stress-detection performance, showing that representations of privileged EDA
can be transferred to low-cost sensors to improve accuracy while reducing
hardware cost.

</details>


### [108] [Performance Analysis of Sub-band Full-duplex Cell-free Massive MIMO JCAS Systems](https://arxiv.org/abs/2510.24185)
*Kwadwo Mensah Obeng Afrane,Yang Miao,André B. J. Kokkeler*

Main category: eess.SP

TL;DR: 提出在无中心化的大规模MIMO JCAS系统中，采用子带全双工（SBFD）实现上行通信与下行雷达感知的同时进行，通过非重叠的上行/下行子带分配实现信号解耦并假设子带干扰可抑制，且每个AP能在SBFD系统中高精度估计感知参数。


<details>
  <summary>Details</summary>
Motivation: 在带内全双工的JCAS中，需解决自干扰以及上行通信信号与雷达回波之间的相互干扰；子带全双工提供一种降低干扰、实现同时通信与感知的新方案，特别适用于分布式的、面向大规模MIMO的场景。

Method: 在每个时隙内，将上行与下行分配到非重叠的子带，明确规定上行子带用于上行通信、下行子带用于雷达探测与感知信号的下行传输；通过对两子带间的干扰进行有效抑制，实现在不相互干扰的前提下对雷达回波和通信信号进行处理；假设各接入点（AP）能够在子带隔离下估计目标参数。

Result: 指出在SBFD的无中心化大规模MIMO JCAS系统中，各AP能够在高精度下估计感知参数。

Conclusion: SBFD为无中心化的大规模MIMO JCAS场景提供了一种可行的双工方案，通过子带隔离实现雷达与通信的解耦与协同，从而实现准确的目标参数估计。

Abstract: In-band Full-duplex joint communication and sensing systems require self
interference cancellation as well as decoupling of the mutual interference
between UL communication signals and radar echoes. We present sub-band
full-duplex as an alternative duplexing scheme to achieve simultaneous uplink
communication and target parameter estimation in a cell-free massive MIMO
system. Sub-band full-duplex allows uplink and downlink transmissions
simultaneously on non-overlapping frequency resources via explicitly defined
uplink and downlink sub-bands in each timeslot. Thus, we propose a sub-band
full-duplex cell-free massive MIMO system with active downlink sensing on
downlink sub-bands and uplink communication on uplink sub-band. In the proposed
system, the target illumination signal is transmitted on the downlink (radar)
sub-band whereas uplink users transmit on the uplink (communication) sub-band.
By assuming efficient suppression of inter-sub-band interference between radar
and communication sub-bands, uplink communication and radar signals can be
efficiently processed without mutual interference. We show that each AP can
estimate sensing parameters with high accuracy in SBFD cell-free massive MIMO
JCAS systems.

</details>


### [109] [Dual-Domain Constraints: Designing Covert and Efficient Adversarial Examples for Secure Communication](https://arxiv.org/abs/2510.24193)
*Tailai Wen,Da Ke,Xiang Wang,Zhitao Huang*

Main category: eess.SP

TL;DR: 提出一种针对传输信号的对抗样本生成框架，在时域和频域双域约束下实现隐蔽的对抗扰动，以干扰AMC分类并保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 在非合作通信场景中，尽管自动调制分类（AMC）有助于信号感知与识别，但也可能被窃听者利用，因此需要一种高效且隐蔽的对抗扰动来保护用户隐私。

Method: 优化对抗样本生成模型，提出在时间域和频域同时施加约束的扰动框架，确保扰动难以被过滤且具有持续有效性；通过对比实验验证方法的优越性与对抗样本的隐蔽性。

Result: 实验结果表明，该方法在提升对抗效果的同时，扰动具有良好的隐蔽性，较对比方法更不易被检测。

Conclusion: 双域约束的对抗扰动生成框架可有效保护通信链路隐私，提升对抗样本的隐蔽性与鲁棒性。

Abstract: The advancements in Automatic Modulation Classification (AMC) have propelled
the development of signal sensing and identification technologies in
non-cooperative communication scenarios but also enable eavesdroppers to
effectively intercept user signals in wireless communication environments. To
protect user privacy in communication links, we have optimized the adversarial
example generation model and introduced a novel framework for generating
adversarial perturbations for transmitted signals. This framework implements
dual-domain constraints in both the time and frequency domains, ensuring that
the adversarial perturbation cannot be filtered out. Comparative experiments
confirm the superiority of the proposed method and the concealment of the
adversarial examples it generates.

</details>


### [110] [Pilot Distortion Design for ToA Obfuscation in Uplink OFDM Communication](https://arxiv.org/abs/2510.24223)
*Mahmut Kemal Ercan,Alireza Pourafzal,Musa Furkan Keskin,Sinan Gezici,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出一种OFDM上行pilot失真设计，用以在不显著损害通信性能前提下，提升ToA观测的难度。通过对每个子载波施加复数失真向量，增大匹配错配函数(MAF)的旁瓣相对于主瓣的比值。以旁瓣峰值比与整合旁瓣能量为多目标，在功率预算和与通信最优pilot的相似性约束下，利用Dinkelbach变换与差分凸优化得到闭式KKT步。仿真表明单输入单输出OFDM链路中，优化失真提升MAF旁瓣并降低延迟估计精度，同时在较宽的SNR区间内容量损失很小。


<details>
  <summary>Details</summary>
Motivation: 研究在不损害通信性能的前提下，对上行OFDM pilot的ToA估计进行干扰/混淆，以提升ToA的不可观测性与位置隐私。

Method: 设计每子载波的复数失真向量，优化两项指标：旁瓣对峰值比(旁瓣/主瓣)与整合旁瓣能量(ISL)；受限于发射功率预算和与通信最优pilot的近似约束。基于带错估计的线性最小均方误差协方差推导出面向容量的下界；将问题转化为广义分式规划，采用Dinkelbach变换和差分凸更新，给出闭式的KKT步。

Result: 仿真（SISO-OFDM）表明优化失真提升MAF旁瓣并降低延迟估计精度（通过错配最大似然ToA估计器验证），同时在广泛的SNR区间内容量损失仅为边际量。

Conclusion: 提供一种信号级别的控制ToA可观测性的手段，在满足通信约束的前提下实现ToA的可观测性调控；无需协议修改或人为路径注入。

Abstract: We study uplink orthogonal frequency-division multiplexing (OFDM) pilot
distortion to deliberately obfuscate time-of-arrival (ToA) estimation at a
single base station while preserving communication performance. We design a
complex per-subcarrier distortion vector that increases sidelobes of the
mismatched ambiguity function (MAF) relative to its mainlobe, using two
objectives: the sidelobe-to-peak level ratio and the integrated sidelobe level.
The design is subject to a transmit-power budget and a proximity
(dissimilarity) constraint around the communication-optimal pilot.
Communication impact is quantied by a capacity-motivated lower bound obtained
from the linear minimum mean-squared error error covariance with a mismatched
channel estimate. The resulting generalized fractional program is solved with
Dinkelbach's transform and a difference-of-convex update that yields a
closed-form Karush-Kuhn-Tucker step. Simulations on a single-input
single-output OFDM link show that the optimized distortions raise MAF sidelobes
and degrade delay estimation, as validated by a mismatched maximum-likelihood
ToA estimator, while incurring only marginal capacity loss over a broad
signal-to-noise ratio range. The method requires no protocol changes or
artificial path injection and provides a signal-level mechanism to control ToA
observability under communication constraints.

</details>


### [111] [Joint Beamforming for Multi-user Multi-target FD ISAC System: A Hybrid GRQ-GA Approach](https://arxiv.org/abs/2510.24243)
*Duc Nguyen Dao,Haibin Zhang,Andre B. J. Kokkeler,Yang Miao*

Main category: eess.SP

TL;DR: 提出了一种面向全双工（FD）ISAC系统的联合波束赋形与功率分配策略，在确保感知性能的前提下最大化通信总和频率并受限于功率约束。使用广义Rayleigh商（GRQ）得到接收波束形成的闭式解，随后通过浮点遗传算法（GA）优化发送波束和功率分配。结果显示相比半双工ISAC系统，GA解在总和速率方面提升可达约98%，并优于现有基准算法，同时给出对感知性能对波束形状和多目标场景下通信-感知权衡的洞察。


<details>
  <summary>Details</summary>
Motivation: 在同时进行高效通信与目标感知的ISAC系统中，需要在有限资源下实现高质量服务和精准感知。FD ISAC系统面临自干扰和复杂的多目标/多用户波束优化问题。通过将接收波束形成的求解转化为闭式GRQ，并用GA解决发送端的非线性优化，从而降低计算复杂度并提升系统性能。

Method: 建立一个FD ISAC系统模型，针对下行/上行多用户与多目标场景。接收端波束形成通过广义Rayleigh商得到闭式解，简化优化变量。发送端通过浮点遗传算法在功率和波束向量之间搜索最优解，以最大化通信总和速率并满足功率限制，同时兼顾感知性能指标。

Result: 所提出的GA-based求解策略在数值实验中相对于基线半双工ISAC系统实现了显著的总和速率提升（高达约98%），亦优于文献中的基准算法。此外，结果揭示感知性能对波束模式和多目标场景下的通信-感知权衡。

Conclusion: 通过将闭式接收波束形成与全局搜索的GA优化相结合，本文实现了FD ISAC系统在多用户多目标场景下的高效波束设计与功率分配，显著提升通信性能并提供感知-通信之间的权衡洞察。

Abstract: In this paper, we consider a full-duplex (FD) Integrated Sensing and
Communication (ISAC) system, in which the base station (BS) performs downlink
and uplink communications with multiple users while simultaneously sensing
multiple targets. In the scope of this work, we assume a narrowband and static
scenario, aiming to focus on the beamforming and power allocation strategies.
We propose a joint beamforming strategy for designing transmit and receive
beamformer vectors at the BS. The optimization problem aims to maximize the
communication sum-rate, which is critical for ensuring high-quality service to
users, while also maintaining accurate sensing performance for detection tasks
and adhering to maximum power constraints for efficient resource usage. The
optimal receive beamformers are first derived using a closed-form Generalized
Rayleigh Quotient (GRQ) solution, reducing the variables to be optimized. Then,
the remaining problem is solved using floating-point Genetic Algorithms (GA).
The numerical results show that the proposed GA-based solution demonstrates up
to a 98% enhancement in sum-rate compared to a baseline half-duplex ISAC system
and provides better performance than a benchmark algorithm from the literature.
Additionally, it offers insights into sensing performance effects on beam
patterns as well as communicationsensing trade-offs in multi-target scenarios.

</details>


### [112] [Trajectory Design for UAV-Based Low-Altitude Wireless Networks in Unknown Environments: A Digital Twin-Assisted TD3 Approach](https://arxiv.org/abs/2510.24255)
*Jihao Luo,Zesong Fei,Xinyi Wang,Le Zhao,Yuanhao Cui,Guangxu Zhu,Dusit Niyato*

Main category: eess.SP

TL;DR: A digital twin (DT)-assisted framework enables UAVs to operate in unknown low-altitude wireless networks by building virtual environments from sensed echoes and using them for training and deployment; it combines simulated annealing-based user scheduling with twin-delayed DDPG for continuous trajectory optimization to minimize mission time while avoiding obstacles.


<details>
  <summary>Details</summary>
Motivation: In unknown topologies, reliable UAV trajectory design for LAWN is difficult due to lack of environmental knowledge. A DT approach can progressively construct virtual environments from UAV sensing data, accelerating learning, enhancing decision-making, and improving flight safety.

Method: UAVs transmit integrated sensing and communication signals to serve ground users while collecting echoes that are uploaded to a DT server to progressively construct virtual environments. These VEs are updated in real time with new sensing data to support decision-making and safety. The trajectory design uses simulated annealing for user scheduling and twin-delayed deep deterministic policy gradient (TD3) for continuous trajectory control, aiming to minimize mission completion time with obstacle avoidance.

Result: Simulation results show faster convergence, improved flight safety, and shorter mission completion time compared with baseline methods, indicating a robust and efficient solution for LAWN deployment in unknown environments.

Conclusion: A DT-assisted training and deployment framework can effectively enable safe, efficient, and rapid UAV operations for LAWN in unknown environments by leveraging real-time sensing, VE construction, and hybrid optimization for trajectory planning.

Abstract: Unmanned aerial vehicles (UAVs) are emerging as key enablers for low-altitude
wireless network (LAWN), particularly when terrestrial networks are
unavailable. In such scenarios, the environmental topology is typically
unknown; hence, designing efficient and safe UAV trajectories is essential yet
challenging. To address this, we propose a digital twin (DT)-assisted training
and deployment framework. In this framework, the UAV transmits integrated
sensing and communication signals to provide communication services to ground
users, while simultaneously collecting echoes that are uploaded to the DT
server to progressively construct virtual environments (VEs). These VEs
accelerate model training and are continuously updated with real-time UAV
sensing data during deployment, supporting decision-making and enhancing flight
safety. Based on this framework, we further develop a trajectory design scheme
that integrates simulated annealing for efficient user scheduling with the
twin-delayed deep deterministic policy gradient algorithm for continuous
trajectory design, aiming to minimize mission completion time while ensuring
obstacle avoidance. Simulation results demonstrate that the proposed approach
achieves faster convergence, higher flight safety, and shorter mission
completion time compared with baseline methods, providing a robust and
efficient solution for LAWN deployment in unknown environments.

</details>


### [113] [Towards actionable hypotension prediction- predicting catecholamine therapy initiation in the intensive care unit](https://arxiv.org/abs/2510.24287)
*Richard Koebe,Noah Saibel,Juan Miguel Lopez Alcaraz,Simon Schäfer,Nils Strodthoff*

Main category: eess.SP

TL;DR: 将昏迷、低灌注等危及生命的低血压问题从传统的阈值预测转向对升压药物启动的即时可行决策预测；利用MIMIC-III数据在15分钟预测窗口内，将催眠药物/升压药物启动作为二分类事件进行建模，体现以治疗决策为导向的预测目标。


<details>
  <summary>Details</summary>
Motivation: 当前ICU低血压预测多依赖固定的平均动脉压(MAP)阈值或MAP预测，未直接对临床治疗 escalations 进行建模。因而存在 undertreatment 与 overtreatment 的风险。以催化药物（vasoactive/inotropic）启动为目标，可以更贴合临床决策，提高干预时机的可操作性。

Method: 使用两小时MAP上下文的统计描述符、人口统计、生命体征、共病、正在进行的治疗等特征；将催化药物启动定义为15分钟预测窗口内的二分类事件；在MIMIC-III数据集上训练XGBoost模型，并通过SHAP进行解释；对结果进行子组分析。

Result: 模型在AUROC上达到0.822（0.813-0.830），明显优于仅基于MAP<65的低血压基线模型（AUROC 0.686 [0.675-0.699]）。SHAP显示近期MAP数值、MAP趋势及正在进行的治疗（镇静剂、电解质等）为主要预测因素。男性、年龄<53岁、BMI>32及无合并症/无并发药物者子组性能更好。

Conclusion: 以MAP动力学、治疗背景与患者特征来预测催化药物启动，能够提供对治疗升级的临床可操作决策，超越单纯的阈值警报。该方法在自然事件不平衡的广泛ICU队列中可行。未来工作应丰富时间与生理上下文、将标签扩展为治疗升级，并与现有低血压预测系统进行基准对比。

Abstract: Hypotension in critically ill ICU patients is common and life-threatening.
Escalation to catecholamine therapy marks a key management step, with both
undertreatment and overtreatment posing risks. Most machine learning (ML)
models predict hypotension using fixed MAP thresholds or MAP forecasting,
overlooking the clinical decision behind treatment escalation. Predicting
catecholamine initiation, the start of vasoactive or inotropic agent
administration offers a more clinically actionable target reflecting real
decision-making. Using the MIMIC-III database, we modeled catecholamine
initiation as a binary event within a 15-minute prediction window. Input
features included statistical descriptors from a two-hour sliding MAP context
window, along with demographics, biometrics, comorbidities, and ongoing
treatments. An Extreme Gradient Boosting (XGBoost) model was trained and
interpreted via SHapley Additive exPlanations (SHAP). The model achieved an
AUROC of 0.822 (0.813-0.830), outperforming the hypotension baseline (MAP < 65,
AUROC 0.686 [0.675-0.699]). SHAP analysis highlighted recent MAP values, MAP
trends, and ongoing treatments (e.g., sedatives, electrolytes) as dominant
predictors. Subgroup analysis showed higher performance in males, younger
patients (<53 years), those with higher BMI (>32), and patients without
comorbidities or concurrent medications. Predicting catecholamine initiation
based on MAP dynamics, treatment context, and patient characteristics supports
the critical decision of when to escalate therapy, shifting focus from
threshold-based alarms to actionable decision support. This approach is
feasible across a broad ICU cohort under natural event imbalance. Future work
should enrich temporal and physiological context, extend label definitions to
include therapy escalation, and benchmark against existing hypotension
prediction systems.

</details>


### [114] [Achieving Constant-Envelope Waveform in CP-OFDMA Framework](https://arxiv.org/abs/2510.24350)
*Yiming Zhu,Zhuhong Zhu,Xiaodong Xu,Hongwei Hou,Wenjin Wang,Rui Ding*

Main category: eess.SP

TL;DR: 提出与CP-OFDMA兼容的恒包络CE波形，提升功率效率并降低复杂度，同时覆盖多用户下行场景。


<details>
  <summary>Details</summary>
Motivation: OFDM易产生高包络峰值，降低功率放大器效率；现有高效波形多与CP-OFDMA不兼容，需一种在保持CP-OFDMA框架下的CE波形解决方案，支持多用户下行和NR标准。

Method: 建立一般CE FDMA信号模型，给出CP-OFDMA兼容的实现结构；设计CE约束脉冲整形滤波器以抑制带外发射；通过优化时域二值 Pilot 序列实现频域 CE 特性；提出延迟域去噪与功率延迟轮廓估计的多阶段降维LMMSE估计；设计基于最大比组合的低复杂度CE接收端LMMSE等化器；提出多用户下行CE传输方案及NR兼容的系统实现。

Result: 数值结果显示所提方案在误比特率方面接近理想情况，并显著降低相较于现有CE波形的收发机复杂度。

Conclusion: CE波形综合考虑兼容CP-OFDMA与多用户下行，能显著提升功率效率并降低实现复杂度，具备落地NR场景的潜力。

Abstract: OFDM is widely adopted in modern wireless communication systems, but its
power efficiency is limited by high envelope fluctuations. Although various
high power-efficiency waveforms have been proposed, most are incompatible with
the CP-OFDMA framework and remain ineffective in multi-user downlink
transmissions. To address this issue, we propose a constant-envelope (CE)
waveform design, which enables low-complexity transceiver architectures while
maintaining full compatibility with the prevailing CP-OFDMA framework.
Specifically, we start from a general CE FDMA signal model and develop a
CP-OFDMA-compatible waveform implementation structure, followed by the design
of an optimized CE-constrained pulse-shaping filter to suppress out-of-band
emissions. To tackle channel estimation challenge under non-flat
frequency-domain pilots induced by CE modulation, we optimize the time-domain
binary pilot sequence to achieve frequency-domain CE properties, and then
propose a multi-stage method combining delay-domain denoising with power delay
profile estimation to facilitate reduced-dimension LMMSE estimation.
Subsequently, we design a low-complexity maximum ratio combining-aided LMMSE
equalizer by exploiting the periodicity and conjugate symmetry of the CE
received signals. To mitigate the downlink peak-to-average power ratio increase
caused by FDMA, we further develop a multi-user downlink CE transmission scheme
including multiple access mechanism, downlink control information design, and
corresponding system-level implementation, which ensures compatibility with the
New Radio standard. Numerical results demonstrate that the proposed scheme
achieves bit error rate performance close to the ideal case while significantly
reducing transceiver complexity compared to existing CE waveform solutions.

</details>


### [115] [Diffusion Models for Wireless Transceivers: From Pilot-Efficient Channel Estimation to AI-Native 6G Receivers](https://arxiv.org/abs/2510.24495)
*Yuzhi Yang,Sen Yan,Weijie Zhou,Brahim Mefgouda,Ridong Li,Zhaoyang Zhang,Mérouane Debbah*

Main category: eess.SP

TL;DR: 将扩散模型用于OFDM系统的AI驱动通道估计与收发机设计，展示其潜力并给出初步验证。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在大规模OFDM系统中的通道估计瓶颈；AI方法可提升估计精度与效率，扩散模型具备从粗糙初始估计中改进的能力。

Method: 将通道估计视为生成问题，利用扩散模型进行去噪/生成，并与传统信号处理方法结合，以设计OFDM接收端的AI循环。提供一个概念性实现与验证框架。

Result: 提出扩散模型在无线通道估计中的潜在效用，给出一个proof-of-concept的案例研究，展示DMs对改进接收端性能的可行性。

Conclusion: 扩散模型在OFDM通道估计与无线收发机设计中展现出显著潜力，未来方向包括与传统方法进一步协作、提升对大规模系统的鲁棒性，以及在实际无线场景中的部署与优化。

Abstract: With the development of artificial intelligence (AI) techniques, implementing
AI-based techniques to improve wireless transceivers becomes an emerging
research topic. Within this context, AI-based channel characterization and
estimation become the focus since these methods have not been solved by
traditional methods very well and have become the bottleneck of transceiver
efficiency in large-scale orthogonal frequency division multiplexing (OFDM)
systems. Specifically, by formulating channel estimation as a generative AI
problem, generative AI methods such as diffusion models (DMs) can efficiently
deal with rough initial estimations and have great potential to cooperate with
traditional signal processing methods. This paper focuses on the transceiver
design of OFDM systems based on DMs, provides an illustration of the potential
of DMs in wireless transceivers, and points out the related research directions
brought by DMs. We also provide a proof-of-concept case study of further
adapting DMs for better wireless receiver performance.

</details>


### [116] [Quality Coefficients for Interferometric Phase Linking](https://arxiv.org/abs/2510.24512)
*Magnus Heimpel,Irena Hajnsek,Othmar Frey*

Main category: eess.SP

TL;DR: 提出三个质量指标用于多时相InSAR的相位连接：闭包相位系数、拟合优度系数和歧义系数，并将它们在统一框架内规范化、带噪声基线校正，用于像元筛选和质量控制。实验证明闭包相位系数能有效筛选稳定区域、拟合优度与既有指标相关并推广、歧义系数能标记虽拟合良但不稳定的解。


<details>
  <summary>Details</summary>
Motivation: 在多时相InSAR中，相位信息常来自分布散射体的相干矩阵，存在不一致性。需要可解释且统一的质量评估指标来筛选和控制像元，以提高相位连结的可靠性和后续处理的稳定性。

Method: 提出三项基于相干矩阵和相位连结模型的质量系数：闭包相位系数用于测量相位信息内部一致性；拟合优度系数量化给定相位连结方法的目标模型对观测相位信息的拟合程度；歧义系数比较原始拟合与正交备选解的拟合优度。把相位连结方法及这三类指标放在统一的数学框架中，给出归一化到单位区间且带噪声基线修正的计算公式，并考虑计算与算法方面的实现。

Result: 实验在TerraSAR-X数据（Visp, 瑞士）上表明：闭包相位系数可有效初筛稳定区域；拟合优度系数与现有质量指示器一致并可在更广泛情形下推广；歧义系数能发现拟合良但解不稳定的情况。三者共同实现对分布散射体的像元筛选与质量控制。

Conclusion: 这组可归一化、带噪声修正的质量指标为分布散射体的干涉处理提供了系统的像元筛选与质量控制框架，有助于提高多时相InSAR中相位连结的鲁棒性与可解释性。

Abstract: In multi-temporal InSAR, phase linking refers to the estimation of a
single-reference interferometric phase history from the information contained
in the coherence matrix of a distributed scatterer. Since the phase information
in the coherence matrix is typically inconsistent, the extent to which the
estimated phase history captures it must be assessed to exclude unreliable
pixels from further processing. We introduce three quality criteria in the form
of coefficients, for threshold-based pixel selection: a coefficient based on
closure phase that quantifies the internal consistency of the phase information
in the coherence matrix; a goodness-of-fit coefficient that quantifies how well
a resulting phase history estimate approximates the phase information according
to the characteristic optimization model of a given phase linking method; and
an ambiguity coefficient that compares the goodness of fit of the original
estimate with that of an orthogonal alternative. We formulate the phase linking
methods and these criteria within a unified mathematical framework and discuss
computational and algorithmic aspects. Unlike existing goodness-of-fit
indicators, the proposed coefficients are normalized to the unit interval with
explicit noise-floor correction, improving interpretability across stacks of
different size. Experiments on TerraSAR-X data over Visp, Switzerland, indicate
that the closure phase coefficient effectively pre-screens stable areas, the
goodness-of-fit coefficient aligns with and systematically generalizes
established quality indicators, and the ambiguity coefficient flags solutions
that fit well but are unstable. Together, the coefficients enable systematic
pixel selection and quality control in the interferometric processing of
distributed scatterers.

</details>


### [117] [Multifunctional Wideband Digital Metasurface for Secure Electromagnetic Manipulation in S-Band](https://arxiv.org/abs/2510.24597)
*Longpan Wang,Zhuoran Zhang,Zhenyuan Li,Xuetao Gan,Xudong Bai,Wen Chen,Qingqing Wu*

Main category: eess.SP

TL;DR: 提出了一种工作在S频段的宽带数字反射元表面，采用梯形和M形贴片+针式二极管实现1位相量化，2.72–3.25 GHz反射损耗<0.6 dB。通过20×20阵列演示可实现多模OAM波、波束扫描和定向探测，提升安全感知与通信能力。


<details>
  <summary>Details</summary>
Motivation: 现有元表面对高频段的依赖导致尺寸和损耗问题，难以在较低频段实现宽带、低成本的大尺度、数字化、可重构的系统。需要在S波段等更低频段实现宽带数字化与安全功能的元表面。

Method: 设计了由梯形和M形贴片构成的可电子重构宽带单比特元单元，采用针二极管实现相位可控的1-bit量化；通过数字编码序列实现多模OAM、动态波束扫描和定向探测。完成了20×20单元阵列的设计、仿真与制造验证。

Result: 在2.72–3.25 GHz范围内实现宽带的1-bit相位量化，相位差稳定在180°±25°，反射损耗<0.6 dB；20×20阵列的仿真与实物验证显示可通过编码实现OAM波生成、波束扫描与精确定向。

Conclusion: 所提出的宽带数字反射元表面可提升雷达与无线系统的频谱效率与安全性，作为高分辨率探测、抗干扰波束指向和物理层安全的有效候选解决方案。

Abstract: Digital metasurfaces have attracted significant attention in recent years due
to their ability to manipulate electromagnetic (EM) waves for secure sensing
and communication. However, most reported metasurfaces operate at relatively
high frequencies, primarily due to the constraints imposed by the physical
scale of the dielectric substrate, thus limiting their full-wave system
applications. In this work, a wideband digital reflective metasurface is
presented for capable of dynamically controlling EM waves, with multifunctional
applications in the lower-frequency S-band. The metasurface is composed of
electronically reconfigurable meta-atoms with wideband characteristics, and
designed by using trapezoidal and M-shaped patches connected by a pin diode.
Simulation results show that the proposed digital metasurface could achieve
wideband 1-bit phase quantization with a stable phase difference within 180
degree +/- 25 degree and small reflection loss below 0.6 dB from 2.72 to 3.25
GHz. To validate the proposed design, a 20x20-unit metasurface array was
designed, simulated and fabricated. By dynamically adjusting the coding
sequence, the metasurface could enable multi-mode orbital angular momentum
(OAM) beam generation, dynamic beam scanning, and precise direction finding.
These capabilities support secure sensing and secure communications through
high-resolution target detection and anti-jamming beam steering, as well as
physical-layer security. The proposed wideband metasurface may serve as an
effective candidate for enhancing spectral efficiency and security performance
in radar and wireless systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [118] [Short Ticketing Detection Framework Analysis Report](https://arxiv.org/abs/2510.23619)
*Yuyang Miao,Huijun Xing,Danilo P. Mandic,Tony G. Constantinides*

Main category: cs.CR

TL;DR: 在铁路系统中，提出一种无监督的多专家机器学习框架用于检测短票欺诈，基于四种异常检测算法对30个高风险车站进行分析，并识别出五种短票模式。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标注数据且欺诈行为稀少，需无监督方法来识别异常购票行为；多专家协作以提升检测鲁棒性与覆盖面。

Method: 采用四种算法：Isolation Forest、Local Outlier Factor、One-Class SVM、Mahalanobis Distance，构建A/B/C/D车站分类体系，在30个高风险车站上识别可疑模式并对短票欺诈进行分析。

Result: 识别出五种不同的短票模式；在30个高风险车站中成功识别可疑模式，展现框架对短票欺诈的检测能力，并指示短票务恢复的潜力。

Conclusion: 该无监督多专家框架对铁路购票欺诈检测具有应用潜力，尤其在缺乏标注数据时；可为策略制定和运输系统的恢复提供依据。未来需进一步验证、扩展数据覆盖范围，并考虑部署与可解释性。

Abstract: This report presents a comprehensive analysis of an unsupervised multi-expert
machine learning framework for detecting short ticketing fraud in railway
systems. The study introduces an A/B/C/D station classification system that
successfully identifies suspicious patterns across 30 high-risk stations. The
framework employs four complementary algorithms: Isolation Forest, Local
Outlier Factor, One-Class SVM, and Mahalanobis Distance. Key findings include
the identification of five distinct short ticketing patterns and potential for
short ticketing recovery in transportation systems.

</details>


### [119] [SAND: A Self-supervised and Adaptive NAS-Driven Framework for Hardware Trojan Detection](https://arxiv.org/abs/2510.23643)
*Zhixin Pan,Ziyu Shu,Linh Nguyen,Amberbir Alemayoh*

Main category: cs.CR

TL;DR: 提出了 SAND 框架，通过自监督学习和自适应 NAS 实现高效的硬件木马检测，提升在多样化攻击下的鲁棒性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 全球化半导体供应链导致嵌入式系统中的硬件木马成为显著安全威胁，现有基于机器学习的检测方法存在特征依赖性强、缺乏自适应性等不足，难以跨不同攻击集泛化。

Method: 引入 SAND：使用自监督学习实现自动特征提取，结合神经架构搜索（NAS）动态优化下游分类器，实现对新基准的快速适配并最小化微调。

Result: 在实验中，SAND 相较现有方法在检测准确率上提升最高可达 18.3%；对规避性木马具有较高鲁棒性，具备较强的泛化能力。

Conclusion: SAND 提供一种自监督与 NAS 驱动的自适应 HT 检测框架，能高效适应不同基准并提升检测性能与鲁棒性。

Abstract: The globalized semiconductor supply chain has made Hardware Trojans (HT) a
significant security threat to embedded systems, necessitating the design of
efficient and adaptable detection mechanisms. Despite promising machine
learning-based HT detection techniques in the literature, they suffer from ad
hoc feature selection and the lack of adaptivity, all of which hinder their
effectiveness across diverse HT attacks. In this paper, we propose SAND, a
selfsupervised and adaptive NAS-driven framework for efficient HT detection.
Specifically, this paper makes three key contributions. (1) We leverage
self-supervised learning (SSL) to enable automated feature extraction,
eliminating the dependency on manually engineered features. (2) SAND integrates
neural architecture search (NAS) to dynamically optimize the downstream
classifier, allowing for seamless adaptation to unseen benchmarks with minimal
fine-tuning. (3) Experimental results show that SAND achieves a significant
improvement in detection accuracy (up to 18.3%) over state-of-the-art methods,
exhibits high resilience against evasive Trojans, and demonstrates strong
generalization.

</details>


### [120] [Uncovering Gaps Between RFC Updates and TCP/IP Implementations: LLM-Facilitated Differential Checks on Intermediate Representations](https://arxiv.org/abs/2510.24408)
*Yifan Wu,Xuewei Feng,Yuxiang Yang,Ke Xu*

Main category: cs.CR

TL;DR: 提出一个基于大语言模型和差分模型的自动分析框架，用于检测RFC标准与内核TCP/IP实现之间在不同版本上的不一致性，并据此进行漏洞分析与修复。


<details>
  <summary>Details</summary>
Motivation: 协议栈实现的复杂性与RFC标准的持续演变导致实现与标准之间存在不一致，进而产生安全漏洞；现有基于预设模式的检测难以泛化，需一种可扩展的自动化检测方法。

Method: 建立协议与RFC更新关系的迭代建模；在不同内核版本上对代码函数进行增量分析以自动执行一致性检测与漏洞分析；结合大语言模型提取协议规范、利用差分模型对版本间差异进行对比以发现不一致；对检测框架进行大规模评估以验证在识别RFC引发的潜在漏洞方面的有效性。

Result: 通过广泛的评估，证明该框架在识别由RFC-代码不一致引发的潜在漏洞方面具有有效性。

Conclusion: 该自动化框架具备可扩展性和实用性，能够帮助检测与修复协议栈实现中的RFC不一致，并为后续在更广泛的协议集合上的应用提供基础与方向。

Abstract: As the core of the Internet infrastructure, the TCP/IP protocol stack
undertakes the task of network data transmission. However, due to the
complexity of the protocol and the uncertainty of cross-layer interaction,
there are often inconsistencies between the implementation of the protocol
stack code and the RFC standard. This inconsistency may not only lead to
differences in protocol functions but also cause serious security
vulnerabilities. At present, with the continuous expansion of protocol stack
functions and the rapid iteration of RFC documents, it is increasingly
important to detect and fix these inconsistencies. With the rise of large
language models, researchers have begun to explore how to extract protocol
specifications from RFC documents through these models, including protocol
stack modeling, state machine extraction, text ambiguity analysis, and other
related content. However, existing methods rely on predefined patterns or
rule-based approaches that fail to generalize across different protocol
specifications. Automated and scalable detection of these inconsistencies
remains a significant challenge. In this study, we propose an automated
analysis framework based on LLM and differential models. By modeling the
iterative relationship of the protocol and based on the iterative update
relationship of the RFC standard, we perform incremental code function analysis
on different versions of kernel code implementations to automatically perform
code detection and vulnerability analysis. We conduct extensive evaluations to
validate the effectiveness of our framework, demonstrating its effectiveness in
identifying potential vulnerabilities caused by RFC code inconsistencies.

</details>


### [121] [EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet](https://arxiv.org/abs/2510.23847)
*Joel Poncha Lemayian,Ghyslain Gagnon,Kaiwen Zhang,Pascal Giard*

Main category: cs.CR

TL;DR: 提出 EthVault 及相关硬件架构，实现以太坊分层确定性冷钱包的核心算法硬件化，并在 FPGA 上进行可行性验证。


<details>
  <summary>Details</summary>
Motivation: 现有钱包多为软件实现，容易遭受恶意软件与侧信道攻击，私钥存在被提取的风险；需要低资源、便携且安全的硬件化解决方案来保护密钥。

Method: 提出三部分硬件设计：1) 钥钥生成与密钥管理的硬件实现用于安全的密钥生成；2) 针对椭圆曲线密码学的抗侧信道与时序攻击的 ECC 架构；3) 子密钥派生函数（CKD）的硬件架构。并在 Xilinx Zynq UltraScale+ FPGA 上实现与评估，强调资源高效和安全性。

Result: 实验结果证实设计可行；ECC 架构实现对输入具有统一执行行为，降低侧信道风险；完整设计在 FPGA 上的资源占用为 LUTs 27%、寄存器 7%、RAM 6%。

Conclusion: 该工作提供了首个针对以太坊分层确定性冷钱包的硬件实现方案，提升私钥安全性与便携性，同时实现低资源消耗，证明在商用 FPGA 上的实现可行。

Abstract: Cryptocurrency blockchain networks safeguard digital assets using
cryptographic keys, with wallets playing a critical role in generating,
storing, and managing these keys. Wallets, typically categorized as hot and
cold, offer varying degrees of security and convenience. However, they are
generally software-based applications running on microcontrollers.
Consequently, they are vulnerable to malware and side-channel attacks, allowing
perpetrators to extract private keys by targeting critical algorithms, such as
ECC, which processes private keys to generate public keys and authorize
transactions. To address these issues, this work presents EthVault, the first
hardware architecture for an Ethereum hierarchically deterministic cold wallet,
featuring hardware implementations of key algorithms for secure key generation.
Also, an ECC architecture resilient to side-channel and timing attacks is
proposed. Moreover, an architecture of the child key derivation function, a
fundamental component of cryptocurrency wallets, is proposed. The design
minimizes resource usage, meeting market demand for small, portable
cryptocurrency wallets. FPGA implementation results validate the feasibility of
the proposed approach. The ECC architecture exhibits uniform execution behavior
across varying inputs, while the complete design utilizes only 27%, 7%, and 6%
of LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+
FPGA.

</details>


### [122] [PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs](https://arxiv.org/abs/2510.23891)
*Jiaqi Xue,Yifei Zhao,Mansour Al Ghanim,Shangqian Gao,Ruimin Sun,Qian Lou,Mengxin Zheng*

Main category: cs.CR

TL;DR: PRO是一个用于开源LLM的水印方法，通过与模型联合训练的水印策略来实现更可检测且抗干扰的水印，适用于如LLaMA-3.x、Phi-2等模型。


<details>
  <summary>Details</summary>
Motivation: 在文本水印领域，封闭源模型的水印技术已较成熟，但将其应用于开源模型面临两大挑战：无法控制解码过程导致水印嵌入困难，以及直接蒸馏闭源水印在可检测性和对下游修改的鲁棒性方面均表现不佳。需要一种能直接对开源模型做出鲁棒且易检测的水印方案。

Method: PRO通过联合训练一个水印策略模型与LLM，使生成的水印模式更易于学习并与检测准则更一致。引入正则化项以模拟下游扰动，惩罚水印可检测性的下降，从而提升在模型编辑等修改下的鲁棒性。

Result: 在开源LLM（如LLaMA-3.2、LLaMA-3、Phi-2）上的实验表明，PRO显著提升水印的可检测性，并提高对模型修改的鲁棒性。

Conclusion: PRO为开源LLM的文本水印任务提供了一种精确且鲁棒的解决思路，能够在不牺牲可检测性的前提下提升对后续修改的耐受性，促进开源模型在版权保护方面的应用。

Abstract: Text watermarking for large language models (LLMs) enables model owners to
verify text origin and protect intellectual property. While watermarking
methods for closed-source LLMs are relatively mature, extending them to
open-source models remains challenging, as developers cannot control the
decoding process. Consequently, owners of open-source LLMs lack practical means
to verify whether text was generated by their models. A core difficulty lies in
embedding watermarks directly into model weights without hurting detectability.
A promising idea is to distill watermarks from a closed-source model into an
open one, but this suffers from (i) poor detectability due to mismatch between
learned and predefined patterns, and (ii) fragility to downstream modifications
such as fine-tuning or model merging. To overcome these limitations, we propose
PRO, a Precise and Robust text watermarking method for open-source LLMs. PRO
jointly trains a watermark policy model with the LLM, producing patterns that
are easier for the model to learn and more consistent with detection criteria.
A regularization term further simulates downstream perturbations and penalizes
degradation in watermark detectability, ensuring robustness under model edits.
Experiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO
substantially improves both watermark detectability and resilience to model
modifications.

</details>


### [123] [Scalable GPU-Based Integrity Verification for Large Machine Learning Models](https://arxiv.org/abs/2510.23938)
*Marcin Spoczynski,Marcela S. Melara*

Main category: cs.CR

TL;DR: 在GPU上对大规模ML工作负载实现内置的完整性验证框架，降低CPU验证开销，并实现跨GPU厂商的一致性维护。


<details>
  <summary>Details</summary>
Motivation: 解决大规模ML工作负载（通常在GPU上运行）与传统基于CPU的安全验证之间的鸿沟，降低验证开销，同时实现跨硬件平台的安全一致性。

Method: 将完整性验证直接与GPU上的模型执行共定位，利用GPU原生的计算单元进行密码学运算（如Intel Arc XMX、NVIDIA Tensor Cores），实现跨厂商的一致性验证机制，并设想在可信执行环境（TEE）与GPU之间构建安全通道来支撑硬件无关的部署。

Result: 显著的性能提升与架构一致性；能够对超大模型（>100GB）保持验全的速度与可扩展性，降低对CPU端验证的依赖。

Conclusion: 提出一种硬件加速、跨厂商兼容的分布式ML完整性验证基础设施，可随未来TEE-GPU安全通道的发展而扩展，便于企业在不同CPU/GPU基础设施上部署。

Abstract: We present a security framework that strengthens distributed machine learning
by standardizing integrity protections across CPU and GPU platforms and
significantly reducing verification overheads. Our approach co-locates
integrity verification directly with large ML model execution on GPU
accelerators, resolving the fundamental mismatch between how large ML workloads
typically run (primarily on GPUs) and how security verifications traditionally
operate (on separate CPU-based processes), delivering both immediate
performance benefits and long-term architectural consistency. By performing
cryptographic operations natively on GPUs using dedicated compute units (e.g.,
Intel Arc's XMX units, NVIDIA's Tensor Cores), our solution eliminates the
potential architectural bottlenecks that could plague traditional CPU-based
verification systems when dealing with large models. This approach leverages
the same GPU-based high-memory bandwidth and parallel processing primitives
that power ML workloads ensuring integrity checks keep pace with model
execution even for massive models exceeding 100GB. This framework establishes a
common integrity verification mechanism that works consistently across
different GPU vendors and hardware configurations. By anticipating future
capabilities for creating secure channels between trusted execution
environments and GPU accelerators, we provide a hardware-agnostic foundation
that enterprise teams can deploy regardless of their underlying CPU and GPU
infrastructures.

</details>


### [124] [Traceable Signatures from Lattices](https://arxiv.org/abs/2510.24101)
*Nam Tran,Khoa Nguyen,Dongxi Liu,Josef Pieprzyk,Willy Susilo*

Main category: cs.CR

TL;DR: 提出一个基于格的可追踪签名，在量子安全的QROM中具有可证明的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的跟踪签名多基于数论或配对，面对量子对手时可能失效，因此需要量子安全的替代方案。

Method: 构造一个格基的可追踪签名方案，并在量子随机 Oracle 模型（QROM）下给出安全性证明。

Result: 给出在QROM中的安全性证明，利用格问题（如 SVP/SIVP 等）的困难性实现理论安全，同时保持可追踪性。

Conclusion: 该工作证明了格基方法在可追踪签名领域的可行性，提供对量子攻击的抵抗能力；后续工作可聚焦效率、实现与实际应用评估，以及对其他安全属性的扩展。

Abstract: Traceable signatures (Kiayas et al., EUROCRYPT 2004) is an anonymous digital
signature system that extends the tracing power of the opening authority in
group signatures. There are many known constructions of traceable signatures,
but all are based on number-theoretic/pairing assumptions. For such reason,
they may not be secure in the presence of quantum computers. This work revisits
the notion of traceable signatures and presents a lattice-based construction
provably secure in the quantum random oracle model (QROM).

</details>


### [125] [Demystifying Cookie Sharing Risks in WebView-based Mobile App-in-app Ecosystems](https://arxiv.org/abs/2510.24141)
*Miao Zhang,Shenao Wang,Guilin Zheng,Yanjie Zhao,Haoyu Wang*

Main category: cs.CR

TL;DR: CMCS 跨小程序 Cookie 共享漏洞揭示 web-view 共享导致的隔离破坏，影响多平台；提出 MiCoScan 静态分析工具，完成大规模测量。


<details>
  <summary>Details</summary>
Motivation: mini-program 在超应用中广泛应用且无需安装，但 web-view 共享环境引入跨域 cookies 访问风险，威胁用户隐私和小程序生态安全。

Method: 对微信、支付宝、抖音、百度四大平台的 web-view 机制进行分析；定义 CMCS 漏洞；实现 MiCoScan，基于 web-view 上下文建模与跨-webview 数据流分析；对 351,483 个小程序进行静态分析。

Result: 四个平台均存在 CMCS; 发现 45,448 个共享 web-view 域的集群，7,965 个具备特权数据传输的实例，9,877 个易受 collusion 攻击的小程序。

Conclusion: CMCS 潜在广泛且风险显著，迫切需要在小程序生态层面改进隔离机制和数据访问控制。

Abstract: Mini-programs, an emerging mobile application paradigm within super-apps,
offer a seamless and installation-free experience. However, the adoption of the
web-view component has disrupted their isolation mechanisms, exposing new
attack surfaces and vulnerabilities. In this paper, we introduce a novel
vulnerability called Cross Mini-program Cookie Sharing (CMCS), which arises
from the shared web-view environment across mini-programs. This vulnerability
allows unauthorized data exchange across mini-programs by enabling one
mini-program to access cookies set by another within the same web-view context,
violating isolation principles. As a preliminary step, we analyzed the web-view
mechanisms of four major platforms, including WeChat, AliPay, TikTok, and
Baidu, and found that all of them are affected by CMCS vulnerabilities.
Furthermore, we demonstrate the collusion attack enabled by CMCS, where
privileged mini-programs exfiltrate sensitive user data via cookies accessible
to unprivileged mini-programs. To measure the impact of collusion attacks
enabled by CMCS vulnerabilities in the wild, we developed MiCoScan, a static
analysis tool that detects mini-programs affected by CMCS vulnerabilities.
MiCoScan employs web-view context modeling to identify clusters of
mini-programs sharing the same web-view domain and cross-webview data flow
analysis to detect sensitive data transmissions to/from web-views. Using
MiCoScan, we conducted a large-scale analysis of 351,483 mini-programs,
identifying 45,448 clusters sharing web-view domains, 7,965 instances of
privileged data transmission, and 9,877 mini-programs vulnerable to collusion
attacks. Our findings highlight the widespread prevalence and significant
security risks posed by CMCS vulnerabilities, underscoring the urgent need for
improved isolation mechanisms in mini-program ecosystems.

</details>


### [126] [Your Microphone Array Retains Your Identity: A Robust Voice Liveness Detection System for Smart Speakers](https://arxiv.org/abs/2510.24393)
*Yan Meng,Jiachun Li,Matthew Pillari,Arjun Deopujari,Liam Brennan,Hafsah Shamsie,Haojin Zhu,Yuan Tian*

Main category: cs.CR

TL;DR: 利用麦阵列指纹实现被动活体检测，提出 ARRAYID，鲁棒性强且准确率高，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在智能音箱中，声音欺骗攻击普遍，现有被动活体检测受环境和用户动作影响较大，需要无需额外传感器的鲁棒方案。

Method: 提出 array fingerprint 基于圆形麦阵列的特征，并设计 ARRAYID 系统以结合多种特征实现轻量被动检测。

Result: 在包含 32780 条样本、14 种 spoofing 设备的数据集上，准确率达到 99.84%，优于现有被动检测方案。

Conclusion: 麦阵列指纹提供稳健的环境和用户移动鲁棒性，显示出实际可落地的被动活体检测潜力；未来工作可关注更广泛设备和场景的泛化。

Abstract: Though playing an essential role in smart home systems, smart speakers are
vulnerable to voice spoofing attacks. Passive liveness detection, which
utilizes only the collected audio rather than the deployed sensors to
distinguish between live-human and replayed voices, has drawn increasing
attention. However, it faces the challenge of performance degradation under the
different environmental factors as well as the strict requirement of the fixed
user gestures.
  In this study, we propose a novel liveness feature, array fingerprint, which
utilizes the microphone array inherently adopted by the smart speaker to
determine the identity of collected audios. Our theoretical analysis
demonstrates that by leveraging the circular layout of microphones, compared
with existing schemes, array fingerprint achieves a more robust performance
under the environmental change and user's movement. Then, to leverage such a
fingerprint, we propose ARRAYID, a lightweight passive detection scheme, and
elaborate a series of features working together with array fingerprint. Our
evaluation on the dataset containing 32,780 audio samples and 14 spoofing
devices shows that ARRAYID achieves an accuracy of 99.84%, which is superior to
existing passive liveness detection schemes.

</details>


### [127] [Attack on a PUF-based Secure Binary Neural Network](https://arxiv.org/abs/2510.24422)
*Bijeet Basak,Nupur Patil,Kurian Polachan,Srinivas Vivek*

Main category: cs.CR

TL;DR: 对基于PUF的对称列交换保护的BNN在忆阻交叉阵列上的方案进行攻击分析，发现存在密钥恢复攻击漏洞。攻击者可逐位恢复PUF密钥并重构模型参数，实验在 MNIST 上可恢复约85%的PUF密钥，BNN分类准确率降至约93%（原始96%），耗时数分钟。


<details>
  <summary>Details</summary>
Motivation: 评估在边缘计算场景中，带非易失忆阻器的BNN若采用PUF作为安全支撑的鲁棒性与脆弱性。原方案（Rajendran等，IEEE Embedded Systems Letters 2025）通过基于列交换的密钥控制来保护权重和偏置矩阵，需检验其对密钥泄露与模型窃取等攻击的耐受性。

Method: 提出一类受差分密码分析启发的攻击，逐位推导PUF密钥比特，观察模型准确率的变化以恢复PUF密钥，进而还原BNN的权重和偏置矩阵。评估在对MNIST数据集训练的BNN上，能在几分钟内恢复约85%的PUF密钥，并使模型准确率降至约93%（相比原模型的96%），攻击过程高效。

Result: 攻击成功实现了对PUF密钥和BNN参数的高效恢复，且在实验中达到显著的实用性，表明现有的PUF基保护并不能有效防止密钥和模型参数的泄漏。

Conclusion: 该研究揭示了基于PUF的BNN保护存在明显安全漏洞，需对保护机制进行加强（如改进PUF密钥使用、增加防窃取的冗余或对权重重构的抗攻击设计），以提升在边缘场景下对BNN的全面安全性。

Abstract: Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays
provide energy-efficient solutions for edge computing but are susceptible to
physical attacks due to memristor nonvolatility. Recently, Rajendran et al.
(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function
(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the
weight and bias matrices of the BNN layers were secured by swapping columns
based on device's PUF key bits.
  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable
to PUF-key recovery attack. As a consequence of our attack, we recover the
secret weight and bias matrices of the BNN. Our approach is motivated by
differential cryptanalysis and reconstructs the PUF key bit-by-bit by observing
the change in model accuracy, and eventually recovering the BNN model
parameters. Evaluated on a BNN trained on the MNIST dataset, our attack could
recover 85% of the PUF key, and recover the BNN model up to 93% classification
accuracy compared to the original model's 96% accuracy. Our attack is very
efficient and it takes a couple of minutes to recovery the PUF key and the
model parameters.

</details>


### [128] [Design and Optimization of Cloud Native Homomorphic Encryption Workflows for Privacy-Preserving ML Inference](https://arxiv.org/abs/2510.24498)
*Tejaswini Bollikonda*

Main category: cs.CR

TL;DR: 提出一个云原生同态加密工作流框架，通过容器化HE模块与Kubernetes编排实现弹性扩展的隐私推理，加速并降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 随着 ML 模型在云端部署，推理阶段的数据隐私成为关键挑战；同态加密提供在加密数据上计算的能力，但在大规模云管道中的开销、编排复杂度和模型兼容性阻碍实际落地，需要云原生结构与优化策略来降低成本。

Method: 设计一个将容器化HE模块与Kubernetes编排结合的架构，支持分布式加密计算的弹性扩展与并行性；并通过 ciphertext packing、多项式模数调整、算子融合等优化实现低延迟和资源节约。

Result: 实验结果显示，与传统HE管线相比，推理速度提升至 3.2×，内存利用率下降约40%。

Conclusion: 为在零信任云环境中部署安全的 MLaaS 提供了切实可行的路径，推动隐私保护的云端 ML 推理落地。

Abstract: As machine learning (ML) models become increasingly deployed through cloud
infrastructures, the confidentiality of user data during inference poses a
significant security challenge. Homomorphic Encryption (HE) has emerged as a
compelling cryptographic technique that enables computation on encrypted data,
allowing predictions to be generated without decrypting sensitive inputs.
However, the integration of HE within large scale cloud native pipelines
remains constrained by high computational overhead, orchestration complexity,
and model compatibility issues.
  This paper presents a systematic framework for the design and optimization of
cloud native homomorphic encryption workflows that support privacy-preserving
ML inference. The proposed architecture integrates containerized HE modules
with Kubernetes-based orchestration, enabling elastic scaling and parallel
encrypted computation across distributed environments. Furthermore,
optimization strategies including ciphertext packing, polynomial modulus
adjustment, and operator fusion are employed to minimize latency and resource
consumption while preserving cryptographic integrity. Experimental results
demonstrate that the proposed system achieves up to 3.2times inference
acceleration and 40% reduction in memory utilization compared to conventional
HE pipelines. These findings illustrate a practical pathway for deploying
secure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality
under zero-trust cloud conditions.

</details>
