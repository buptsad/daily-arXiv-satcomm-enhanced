<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.IT](#cs.IT) [Total: 5]
- [eess.SY](#eess.SY) [Total: 13]
- [cs.CR](#cs.CR) [Total: 17]
- [eess.SP](#eess.SP) [Total: 12]
- [cs.LG](#cs.LG) [Total: 83]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [When Light Bends to the Collective Will: A Theory and Vision for Adaptive Photonic Scale-up Domains](https://arxiv.org/abs/2510.08072)
*Vamsi Addanki*

Main category: cs.NI

TL;DR: 本文提出了一个用于可编程光互联的简单理论框架，量化了拓扑重配置延迟与自适应带宽增益之间的权衡，并将其与Birkhoff–von Neumann分解、最大并发流和α-β通信代价模型联系起来，最后给出后续算法与系统集成的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着片间硅光子互联在带宽和能效方面的优势日益明显，规模化系统中的集合通信成为瓶颈；可编程光互联通过动态建立直接光路径，有望按集合通信结构同步优化通信性能，但需要权衡重配置延迟与性能收益。

Method: 构建一个理论框架，显式表达重配置开销与拓扑自适应带来的吞吐/延迟改进之间的权衡；利用BvN分解和最大并发流来分析可编程光互联如何实现或接近集合通信的最优调度，并将分析结果映射到α-β成本模型以便评估收敛和收益时机。

Result: 给出可判断何时进行重配置的准则（基于流量模式和重配置开销的比较），并揭示了BvN分解与最大并发流在描述光互联可达性能上的等价性/互补性；为算法设计和系统集成指明关键关注点和研究路线。

Conclusion: 本文奠定了分析自适应光互联有效性与成本的理论基础，表明在特定流量与重配置延迟条件下，动态重配置能显著加速集合通信；同时提出进一步在算法、硬件和系统层面实现该愿景的研究方向。

Abstract: As chip-to-chip silicon photonics gain traction for their bandwidth and
energy efficiency, collective communication has emerged as a critical
bottleneck in scale-up systems. Programmable photonic interconnects offer a
promising path forward: by dynamically reconfiguring the fabric, they can
establish direct, high-bandwidth optical paths between communicating endpoints
-- \emph{synchronously and guided by the structure of collective operations}
(e.g., AllReduce). However, realizing this vision -- \emph{when light bends to
the collective will} -- requires navigating a fundamental trade-off between
reconfiguration delay and the performance gains of adaptive topologies.
  In this paper, we present a simple theoretical framework for adaptive
photonic scale-up domains that makes this trade-off explicit and clarifies when
reconfiguration is worthwhile. Along the way, we highlight a connection -- not
surprising but still powerful -- between the Birkhoff--von Neumann (BvN)
decomposition, maximum concurrent flow (a classic measure of network
throughput), and the well-known $\alpha$-$\beta$ cost model for collectives.
Finally, we outline a research agenda in algorithm design and systems
integration that can build on this foundation.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [2] [List Recoverable Codes: The Good, the Bad, and the Unknown (hopefully not Ugly)](https://arxiv.org/abs/2510.07597)
*Nicolas Resch,S. Venkitesh*

Main category: cs.IT

TL;DR: 本文综述了列表恢复（list recovery）的最新进展，涵盖存在性结果、不可能性结果以及未解决的问题，并展示了其在编码理论和理论计算机科学中的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 列表恢复是编码理论中对最坏情况错误和列表解码的自然推广，模型由每个坐标上的“软信息”输入列表描述。理解哪些码具有良好列表恢复性质对于构造高效与鲁棒的编码方案及其在计算复杂性中的应用至关重要。

Method: 文章通过回顾与整合近期文献，分类讨论正向结果（构造或存在性证明）、负向结果（下界与不可能性证明）以及尚未解决的问题，并阐述列表恢复在诸如串行并联编码、提取器、压缩感知等领域的应用联系。

Result: 系统地总结了已知的列表恢复构造与限制，指出一些参数范围内存在高效可解的码，也列举了参数无法达到的下界，同时揭示了若干开放问题与应用方向。

Conclusion: 列表恢复已从连接列表解码的工具，成长为独立且广泛适用的对象。尽管已有大量进展，仍有重要参数区间与构造效率问题待解决，其进一步研究将继续推动编码理论与理论计算机科学的交叉发展。

Abstract: List recovery is a fundamental task for error-correcting codes, vastly
generalizing unique decoding from worst-case errors and list decoding. Briefly,
one is given ''soft information'' in the form of input lists S_1,...,S_n of
bounded size, and one argues that there are not too many codewords that agree a
lot with this soft information. This general problem appears in many guises,
both within coding theory and in theoretical computer science more broadly.
  In this article we survey recent results on list recovery codes, introducing
both the ''good'' (i.e., possibility results, showing that codes with certain
list recoverability exist), the ''bad'' (impossibility results), and the
''unknown''. We additionally demonstrate that, while list recoverable codes
were initially introduced as a component in list decoding concatenated codes,
they have since found myriad applications to and connections with other topics
in theoretical computer science.

</details>


### [3] [Integrated Localization, Mapping, and Communication through VCSEL-Based Light-emitting RIS (LeRIS)](https://arxiv.org/abs/2510.08071)
*Rashid Iqbal,Dimitrios Bozanis,Dimitrios Tyrovolas,Christos K. Liaskos,Muhammad Ali Imran,George K. Karagiannidis,Hanaa Abumarshoud*

Main category: cs.IT

TL;DR: 提出一种基于VCSEL的发光可重构智能表面（LeRIS）架构，利用窄高斯光束与多模多样性实现定位、障碍检测与毫米波通信的联合支持；解析性地给出仅需5个VCSEL（特定几何下3个）的接收功率定位解，并用反射到达时延进行障碍映射与阻塞感知，仿真显示毫米级定位、鲁棒障碍检测与显著的速率提升。


<details>
  <summary>Details</summary>
Motivation: 现有LED型LeRIS发散光束导致精度低，或LiDAR方案体积大、成本高，缺乏可紧凑集成、低功耗且解析可处理的多功能PWE元件；需要一种既能高精度定位、实时障碍感知又能增强毫米波通信鲁棒性的可集成方案。

Method: 在RIS上集成VCSEL数组，利用窄高斯束与多模/双模工作方式获取空间多样性；从接收信号强度（RSS）推导闭式解以同时恢复用户位置与朝向（默认5个VCSEL，特定几何下降至3个）；通过接收反射信号的到达时延（ToA）进行环境映射与障碍检测，并据此规划阻塞感知的RIS波束路由。

Result: 仿真验证达到毫米级定位精度，能可靠检测障碍并指导阻塞鲁棒的RIS路由，系统在频谱效率和最小用户速率上表现出显著增益；此外方案实现了较小体积与低功耗的集成潜力，并具有良好的解析可处理性。

Conclusion: VCSEL型LeRIS为可扩展、易集成的多功能PWE元件，能够同时支持精准定位、环境感知与增强的毫米波通信，是推动鲁棒6G可编程无线环境的有力候选。

Abstract: This paper presents a light-emitting reconfigurable intelligent surface
(LeRIS) architecture that integrates vertical cavity surface emitting lasers
(VCSELs) to jointly support user localization, obstacle-aware mapping, and
millimeter-wave (mmWave) communication in programmable wireless environments
(PWEs). Unlike prior light-emitting diode (LED)-based LeRIS designs with
diffuse emission or LiDAR-assisted schemes requiring bulky sensing modules, the
proposed VCSEL-based approach exploits narrow Gaussian beams and multimode
diversity to enable compact, low-power, and analytically tractable integration.
We derive closed-form expressions to jointly recover user position and
orientation from received signal strength using only five VCSELs, and reduce
this requirement to three under specific geometric conditions by leveraging
dual-mode operation. In parallel, we introduce a VCSEL-based mapping method
that uses reflected signal time-of-arrival measurements to detect obstructions
and guide blockage-resilient RIS beam routing. Simulation results demonstrate
millimeter-level localization accuracy, robust obstacle detection, high
spectral efficiency, and substantial gains in minimum user rate. These findings
establish VCSEL-based LeRIS as a scalable and practically integrable enabler
for resilient 6G wireless systems with multi-functional PWEs.

</details>


### [4] [Near-optimal Rank Adaptive Inference of High Dimensional Matrices](https://arxiv.org/abs/2510.08117)
*Frédéric Zheng,Yassir Jedra,Alexandre Proutiere*

Main category: cs.IT

TL;DR: 研究秩自适应的高维矩阵估计问题：给出实例特异的样本复杂度下界，刻画有效秩的最优选择权衡；提出基于最小二乘与通用奇异值阈值化的算法，给出有限样本误差界并几乎达到下界；在多元回归和线性动力系统识别中验证结果。


<details>
  <summary>Details</summary>
Motivation: 在不知道真实秩或谱衰减速率的情况下，从线性观测中高效估计矩阵需要自适应选择有效秩，权衡对少数奇异值的精准估计与对剩余谱的近似误差。

Method: 推导实例相关的信息论样本复杂度下界，分析有效秩选择带来的误差分解；设计结合最小二乘估计与通用奇异值阈值化（universal SVT）的算法；通过增强的矩阵去噪(SVT)分析得到有限样本上界。

Result: 证明了有效秩的最优选择如何依赖于矩阵谱、样本数量与噪声强度；给出样本复杂度下界与算法的接近最优的上界；在多元回归和线性系统识别场景中展示了方法的实用性与性能优势。

Conclusion: 刻画了秩自适应估计的基本极限和可达性，提出了一个简单且理论上几乎最优的实用算法，并通过若干应用实例验证其有效性。

Abstract: We address the problem of estimating a high-dimensional matrix from linear
measurements, with a focus on designing optimal rank-adaptive algorithms. These
algorithms infer the matrix by estimating its singular values and the
corresponding singular vectors up to an effective rank, adaptively determined
based on the data. We establish instance-specific lower bounds for the sample
complexity of such algorithms, uncovering fundamental trade-offs in selecting
the effective rank: balancing the precision of estimating a subset of singular
values against the approximation cost incurred for the remaining ones. Our
analysis identifies how the optimal effective rank depends on the matrix being
estimated, the sample size, and the noise level. We propose an algorithm that
combines a Least-Squares estimator with a universal singular value thresholding
procedure. We provide finite-sample error bounds for this algorithm and
demonstrate that its performance nearly matches the derived fundamental limits.
Our results rely on an enhanced analysis of matrix denoising methods based on
singular value thresholding. We validate our findings with applications to
multivariate regression and linear dynamical system identification.

</details>


### [5] [Exponential Error Bounds for Information Bottleneck Source Coding Problems](https://arxiv.org/abs/2510.08364)
*Han Wu,Hamdi Joudeh*

Main category: cs.IT

TL;DR: 本文给出了信息瓶颈(远程对数损失)源编码在超出/低于速率阈值时的精确误码指数与强反例指数，并通过将IB编码与有帮助者(WAK)问题建立码级等价，重导出并赋予WAK球面封装指数操作含义。


<details>
  <summary>Details</summary>
Motivation: 研究在对数损失下、基于有速率约束的观测描述如何快速地（以指数速率）使远端源的软估计误差概率趋于0或1；弥补关于IB源编码误差指数和强反例指数的理论空白，并寻找与经典的有帮助者问题的操作性联系。

Method: 对误差概率分别给出匹配的上界与下界，从而得到精确的误差与强反例指数；技术上扩展并调整球面封装(sphere packing)与单字母化(single-letterization)方法以处理伴随的辅助随机变量；在码级上构造IB与WAK问题的映射，证明任一WAK编码可视为IB编码。

Result: 得到涉及对辅助随机变量的优化表达的精确误差指数与强反例指数；证明了当码率高于/低于速率失真函数时误差概率分别以具体指数衰减至0或趋向1；通过IB↔WAK的码级连接，重新导出并给出WAK问题已知的最佳球面封装指数的操作解释。

Conclusion: 论文在理论上完整刻画了IB源编码的指数收敛行为，并通过与WAK问题的连接增强了这些指数的操作意义，为后续计算这些指数的显式形式和数值评估以及扩展到更多损失度量或有侧信息场景奠定基础。

Abstract: We study the information bottleneck (IB) source coding problem, also known as
remote lossy source coding under logarithmic loss. Based on a rate-limited
description of noisy observations, the receiver produces a soft estimate for
the remote source, i.e., a probability distribution, evaluated under the
logarithmic loss. We focus on the excess distortion probability of IB source
coding and investigate how fast it converges to 0 or 1, depending on whether
the rate is above or below the rate-distortion function. The latter case is
also known as the exponential strong converse. We establish both the exact
error exponent and the exact strong converse exponent for IB source coding by
deriving matching upper and lower exponential bounds. The obtained exponents
involve optimizations over auxiliary random variables. The matching converse
bounds are derived through non-trivial extensions of existing sphere packing
and single-letterization techniques, which we adapt to incorporate auxiliary
random variables.
  In the second part of this paper, we establish a code-level connection
between IB source coding and source coding with a helper, also known as the
Wyner-Ahlswede-K\"orner (WAK) problem. We show that every code for the WAK
problem is a code for IB source coding. This requires noticing that IB source
coding, under the excess distortion criterion, is equivalent to source coding
with a helper available at both the transmitter and the receiver; the latter in
turn relates to the WAK problem. Through this connection, we re-derive the best
known sphere packing exponent of the WAK problem, and provide it with an
operational interpretation.

</details>


### [6] [A Rate-Distortion Bound for ISAC](https://arxiv.org/abs/2510.08487)
*Mohammadreza Bakhshizadeh Mohajer,Alex Dytso,Daniela Tuninetti,Luca Barletta*

Main category: cs.IT

TL;DR: 本文提出基于率—失真理论的界限（RDB），用于刻画ISAC系统的感知—通信性能极限，适用于任意参数分布和失真度量，且在高噪声下与最优相符，在低噪声下可优于BCRB。


<details>
  <summary>Details</summary>
Motivation: 经典估计理论（如BCRB）依赖严格的正则性条件，且对参数分布或离散任务不总适用。为获得更通用且更严格的感知性能下界，需要一个不受这些限制的工具来分析ISAC系统的本质权衡。

Method: 基于率—失真理论构造一个通用的逆向界（RDB），把感知问题映射为信息率与失真之间的约束，证明该界对任意参数分布与失真度量成立，并分析高噪声极限下的紧性及与BCRB的比较。通过两类示例验证：Nakagami衰落信道估计（BCRB不可用时仍有效）和二值占用检测（离散感知任务）。

Result: 给出了适用于均方误差和误差概率等失真度量的普适RDB；证明在高感测噪声下界是紧的，并在某些低噪声情形下严格优于BCRB；在示例中RDB成功提供了有效下界并展示其优越性与广泛适用性。

Conclusion: RDB为ISAC系统感知—通信性能极限分析提供了一个强大且通用的工具，突破了传统估计界限的正则性限制，对连续与离散参数估计均适用，可用于指导系统设计与性能评估。

Abstract: This paper addresses the fundamental performance limits of Integrated Sensing
and Communication (ISAC) systems by introducing a novel converse bound based on
rate-distortion theory. This rate-distortion bound (RDB) overcomes the
restrictive regularity conditions of classical estimation theory, such as the
Bayesian Cram\'er-Rao Bound (BCRB). The proposed framework is broadly
applicable, holding for arbitrary parameter distributions and distortion
measures, including mean-squared error and probability of error. The bound is
proved to be tight in the high sensing noise regime and can be strictly tighter
than the BCRB in the low sensing noise regime. The RDB's utility is
demonstrated on two challenging scenarios: Nakagami fading channel estimation,
where it provides a valid bound even when the BCRB is inapplicable, and a
binary occupancy detection task, showcasing its versatility for discrete
sensing problems. This work provides a powerful and general tool for
characterizing the ultimate performance tradeoffs in ISAC systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [7] [Auctioning Future Services in Edge Networks with Moving Vehicles: N-Step Look-Ahead Contracts for Sustainable Resource Provision](https://arxiv.org/abs/2510.07333)
*Ziqi Ling,Minghui Liwang,Xianbin Wang,Seyyedali Hosseinalipour,Zhipeng Cheng,Sai Zou,Wei Ni,Xiaoyu Xia*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Timely resource allocation in edge-assisted vehicular networks is essential
for compute-intensive services such as autonomous driving and navigation.
However, vehicle mobility leads to spatio-temporal unpredictability of resource
demands, while real-time double auctions incur significant latency. To address
these challenges, we propose a look-ahead contract-based auction framework that
shifts decision-making from runtime to planning time. Our approach establishes
N-step service contracts between edge servers (ESs) using demand forecasts and
modified double auctions. The system operates in two stages: first, an
LSTM-based prediction module forecasts multi-slot resource needs and determines
ES roles (buyer or seller), after which a pre-double auction generates
contracts specifying resource quantities, prices, and penalties. Second, these
contracts are enforced in real time without rerunning auctions. The framework
incorporates energy costs, transmission overhead, and contract breach risks
into utility models, ensuring truthful, rational, and energy-efficient trading.
Experiments on real-world (UTD19) and synthetic traces demonstrate that our
method improves time efficiency, energy use, and social welfare compared with
existing baselines.

</details>


### [8] [Nonlinear System Identification for Model-Based Control of Waked Wind Turbines](https://arxiv.org/abs/2510.07336)
*Sebastiano Randino,Lorenzo Schena,Nicolas Coudou,Emanuele Garone,Miguel Alfonso Mendez*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work presents a nonlinear system identification framework for modeling
the power extraction dynamics of wind turbines, including both freestream and
waked conditions. The approach models turbine dynamics using data-driven power
coefficient maps expressed as combinations of compact radial basis functions
and polynomial bases, parameterized in terms of tip-speed ratio and upstream
conditions. These surrogate models are embedded in a first-order dynamic system
suitable for model-based control. Experimental validation is carried out in two
wind tunnel configurations: a low-turbulence tandem setup and a high-turbulence
wind farm scenario. In the tandem case, the identified model is integrated into
an adapted K\omega^2 controller, resulting in improved tip-speed ratio tracking
and power stability compared to BEM-based and steady-state models. In the wind
farm scenario, the model captures the statistical behavior of the turbines
despite unresolved turbulence. The proposed method enables interpretable,
adaptive control across a range of operating conditions without relying on
black-box learning strategies.

</details>


### [9] [Techno-economic analysis of self-sustainable thermophotovoltaic systems for grid-scale energy generation](https://arxiv.org/abs/2510.07338)
*Jihun Lim,Sungwon Lee*

Main category: eess.SY

TL;DR: 本文对自给自足热光伏（TPV）系统进行了全面技术经济分析，发现尽管热电池材料导致的高资本支出使其目前在储能成本（LCOS）上不具竞争力，但其发电成本（LCOE）具有与常规燃气轮机可比的潜力；在GWh尺度时，硅基系统借助规模化优势可实现有竞争力的LCOE，即便效率低于InGaAs。


<details>
  <summary>Details</summary>
Motivation: 为支持高渗透率可再生能源，需发展可调度且零排放的发电方式。自给自足TPV结合太阳充电和热电池，承诺提供按需零排放电力，故需对其技术性与经济性进行量化评估。

Method: 基于理论模型对空气桥InGaAs和硅二极管电池进行模拟，构建系统级能量与成本模型，计算LCOS与LCOE；比较不同材料、系统规模和成本假设下的性能与经济性。

Result: 在当前材料成本条件下，系统的LCOS较高，不具竞争力；但其LCOE可与燃气轮机等常规可调度发电机竞争。随着规模扩展到GWh级别，硅基系统的LCOE亦可达到与燃气轮机相当的水平。

Conclusion: 自给自足TPV为实现电网级按需零排放电力提供了可行路径。硅基方案凭借制造规模化优势，作为比III-V材料更低风险的工程化路径具有实践潜力，尽管短期需克服热电池材料成本等经济障碍。

Abstract: To facilitate the widespread adoption of renewable energy, dispatchable,
zero-emission power sources are essential for grid stability. This work
performs a comprehensive techno-economic analysis of a self-sustainable
thermophotovoltaic (TPV) system, an architecture that integrates solar charging
to function as a standalone power generation asset. Using theory-based models
for air-bridge InGaAs and Si diode cells, our analysis reveals that while the
system is not currently competitive from a pure levelized of storage cost
(LCOS) perspective due to the high capital expenditure for thermal battery
materials, its primary value lies in its competitive levelized cost of
electricity (LCOE). The results demonstrate that the LCOE of this
self-sustaining system can be competitive with conventional dispatchable
generators, such as gas turbines. Furthermore, at scales exceeding the
gigawatt-hour level, a Si-based system can also achieve an LCOE comparable to
that of traditional gas-turbine power plants, despite having a lower conversion
efficiency than its InGaAs counterpart. This highlights a practical engineering
pathway for leveraging silicon's immense manufacturing scalability, offering a
lower-risk route to deployment compared to III-V materials. Ultimately, this
work establishes the self-sustainable TPV architecture as a compelling pathway
toward providing grid-scale, on-demand, zero-emission power.

</details>


### [10] [Adaptive Control Allocation for Underactuated Time-Scale Separated Non-Affine Systems](https://arxiv.org/abs/2510.07507)
*Daniel M. Cherenson,Dimitra Panagou*

Main category: eess.SY

TL;DR: Adaptive control for uncertain nonlinear underactuated systems with input constraints using time-scale separation, dynamic control allocation, and state-predictor adaptive law; proven stable via singular perturbation and Lyapunov; validated on VTOL quadplane across flight regimes.


<details>
  <summary>Details</summary>
Motivation: Underactuated robotic systems with state-dependent actuation and input limits face challenges in trajectory tracking under uncertainties and disturbances; need unified adaptive controller handling non-affine dynamics and actuator constraints.

Method: Use time-scale separation to form reduced-order model where fast dynamics act as virtual inputs to slow subsystem; apply dynamic control allocation for non-affine input selection; design state-predictor-based adaptive law to compensate uncertainties; analyze stability and bounded tracking using singular perturbation and Lyapunov methods.

Result: Theoretical guarantees of stability and bounded trajectory tracking; experimental/simulation validation on a VTOL quadplane with nonlinear, state-dependent actuation showing performance across cruise, transition, and hover.

Conclusion: Proposed architecture provides a unified, theoretically-grounded adaptive control approach for constrained, underactuated nonlinear systems, effective across multiple flight regimes and robust to uncertainties.

Abstract: Many robotic systems are underactuated, meaning not all degrees of freedom
can be directly controlled due to lack of actuators, input constraints, or
state-dependent actuation. This property, compounded by modeling uncertainties
and disturbances, complicates the control design process for trajectory
tracking. In this work, we propose an adaptive control architecture for
uncertain, nonlinear, underactuated systems with input constraints. Leveraging
time-scale separation, we construct a reduced-order model where fast dynamics
provide virtual inputs to the slower subsystem and use dynamic control
allocation to select the optimal control inputs given the non-affine dynamics.
To handle uncertainty, we introduce a state predictor-based adaptive law, and
through singular perturbation theory and Lyapunov analysis, we prove stability
and bounded tracking of reference trajectories. The proposed method is
validated on a VTOL quadplane with nonlinear, state-dependent actuation,
demonstrating its utility as a unified controller across various flight
regimes, including cruise, landing transition, and hover.

</details>


### [11] [Some Reflections on Sliding Mode Designs in Control Systems: An Example of Adaptive Tracking Control for Simple Mechanical Systems With Friction Without Measurement of Velocity](https://arxiv.org/abs/2510.07675)
*Romeo Ortega,Leyan Fang,Jose Guadalupe Romero*

Main category: eess.SY

TL;DR: 本文对滑模（sliding mode）控制设计在科学文献中的大量应用提出批判性反思。作者回顾常见设计步骤、指出滑模文献中被忽视的若干控制问题，并用一个带静摩擦和库仑摩擦、无速度量测的一自由度机械系统上两种自适应跟踪控制器对比验证其观点。


<details>
  <summary>Details</summary>
Motivation: 近年滑模控制相关文献激增，作者认为其实际作用、相关性与适用性需被重新审视，特别是与经典控制问题（实现性、测量需求、摩擦建模等）相比的兼容性与实用性。

Method: 首先总结并举例说明大多数滑模设计遵循的常用程序；其次列举若干在滑模文献中常被忽视但在经典控制设计中重要的方面（如测量可得性、抖振/chattering、摩擦与非线性、控制能量、噪声与实现难度、参数估计方法等）；最后通过数值/解析对比两个无需速度测量的自适应跟踪控制器在一自由度机械系统（含未知参数与静、库仑摩擦）上的性能来具体说明这些考虑的影响。

Result: 论文揭示了滑模设计文献中常见的若干疏漏与过于理想化的假设，且通过对比试验表明在实际问题（如摩擦、无速度量测、噪声）下不同控制策略在跟踪精度、鲁棒性、抖振和参数估计上的权衡明显。

Conclusion: 作者建议对滑模设计给予更谨慎的评价与应用：在理论设计之外应更多关注实现细节、实验验证和与传统控制要求的一致性；在含摩擦与有限测量情形下应重新考量方法选择与改进方向。

Abstract: The objective of this note is to share some reflections of the authors
regarding the use of sliding mode designs in control systems. We believe the
abundant, and ever increasing, appearance of this kind of works on our
scientific publications deserves some critical evaluation of their actual role,
relevance and pertinence. First, we discuss the procedure followed by most of
these designs -- illustrated with examples from the literature. Second, we
bring to the readers attention several aspects of the control problem, central
in classical designs, which are disregarded in the sliding mode literature.
Finally, to illustrate with an specific example our previous considerations, we
compare the performance of two adaptive tracking controllers for a simple one
degree of freedom mechanical systems with unknown parameters and static and
Coulomb friction -- that do not rely on the measurement of velocity.

</details>


### [12] [Multi-Level Multi-Fidelity Methods for Path Integral and Safe Control](https://arxiv.org/abs/2510.07756)
*Zhuoyuan Wang,Takashi Tanaka,Yongxin Chen,Yorie Nakahira*

Main category: eess.SY

TL;DR: 本文提出将多层蒙特卡洛（MLMC）与多保真度蒙特卡洛（MFMC）融合，用不同时间/状态分辨率的低保真模型样本来减少方差、提高风险度量与路径积分控制的采样效率；证明了估计器在温和条件下无偏且一致，并在数值仿真中展示了更优的成本-精度权衡。


<details>
  <summary>Details</summary>
Motivation: 在缺乏解析模型的系统中，基于采样的方法用于风险估计与最优控制，但从真实系统获取足够样本代价高昂；许多场景同时存在成本低廉但保真度较低的模拟器，如何高效利用多源低成本样本以降低总体采样成本是主要动机。

Method: 提出一种将MLMC与MFMC结合的方法，使得来自不同时间步长与状态表征的模型样本可以联合使用，通过多层次差分和保真度加权来减少估计方差并优化样本分配；同时给出理论分析，证明估计器在一定假设下保持无偏性与一致性。

Result: 理论上证明了估计器的无偏性和一致性；数值仿真表明，与单一高保真采样相比，所提方法在相同精度下显著降低采样成本，或在相同成本下提高估计精度，适用于风险量化与路径积分控制问题。

Conclusion: 该方法通过整合MLMC与MFMC在多模型、多分辨率场景下有效提升采样效率，对采样成本敏感的风险评估与控制任务具有实际价值；未来可拓展为自适应样本分配、更复杂系统与真实世界验证。

Abstract: Sampling-based approaches are widely used in systems without analytic models
to estimate risk or find optimal control. However, gathering sufficient data in
such scenarios can be prohibitively costly. On the other hand, in many
situations, low-fidelity models or simulators are available from which samples
can be obtained at low cost. In this paper, we propose an efficient approach
for risk quantification and path integral control that leverages such data from
multiple models with heterogeneous sampling costs. A key technical novelty of
our approach is the integration of Multi-level Monte Carlo (MLMC) and
Multi-fidelity Monte Carlo (MFMC) that enable data from different time and
state representations (system models) to be jointly used to reduce variance and
improve sampling efficiency. We also provide theoretical analysis of the
proposed method and show that our estimator is unbiased and consistent under
mild conditions. Finally, we demonstrate via numerical simulation that the
proposed method has improved computation (sampling costs) vs. accuracy
trade-offs for risk quantification and path integral control.

</details>


### [13] [A Stable, Accurate and Well-Conditioned Time-Domain PMCHWT Formulation](https://arxiv.org/abs/2510.07989)
*Van Chien Le,Cedric Munger,Francesco P. Andriulli,Kristof Cools*

Main category: eess.SY

TL;DR: 提出一种基于时域PMCHWT的新边界元格式，通过乘法Calderón预条件和基于准Helmholtz投影子的重标度（利用时间微分/积分算子）同时消除密网格崩溃、大步长崩溃与晚期不稳定性，结合莫践步进（MOT）和迭代求解器，数值算例验证了方法在复杂介电散射体上的精度、稳定性与效率。


<details>
  <summary>Details</summary>
Motivation: 时域边界元用于介电体瞬态散射时，常受密网格崩溃、随时间步长增大出现的大步长崩溃以及晚期数值不稳定性的困扰，需一种统一的数值修正策略以保证鲁棒性与可扩展性。

Method: 以时域PMCHWT方程为基础，采用乘法Calderón预条件器并引入改造的静态电场积分算子以克服密网格崩溃；利用准Helmholtz投影子将场分解，并通过时间微分与积分作为重标度算子对Helmholtz分量重标度，从而平衡环路(loop)与星型(star)分量、消除大步长崩溃并抑制晚期不稳定性；离散化后用行进时间(Marching-on-in-time)方案配合迭代求解器求解。

Result: 对简单与多连通介电体以及高度非平滑几何体的数值试验显示，方法在较粗网格与较大时间步长下仍保持数值稳定，迭代求解收敛良好，重构的散射场与参考解吻合，计算效率和内存使用在可接受范围内。

Conclusion: 该工作给出了一个系统化的时域边界元修正与预条件框架，能够同时解决密网格、大步长与晚期不稳定性问题，提升了时域PMCHWT在复杂介电散射问题中的鲁棒性与实用性。

Abstract: This paper introduces a new boundary element formulation for transient
electromagnetic scattering by homogeneous dielectric objects based on the
time-domain PMCHWT equation. To address dense-mesh breakdown, a multiplicative
Calderon preconditioner utilizing a modified static electric field integral
operator is employed. Large-timestep breakdown and late-time instability are
simultaneously resolved by rescaling the Helmholtz components leveraging the
quasi-Helmholtz projectors and using temporal differentiation and integration
as rescaling operators. This rescaling also balances the loop and star
components at large timesteps, improving solution accuracy. The resulting
discrete system is solved using a marching-on-in-time scheme and iterative
solvers. Numerical experiments for simply- and multiply-connected dielectric
scatterers, including highly non-smooth geometries, corroborate the accuracy,
stability, and efficiency of the proposed approach.

</details>


### [14] [General formulation of an analytic, Lipschitz continuous control allocation for thrust-vectored controlled rigid-bodies](https://arxiv.org/abs/2510.08119)
*Frank Mukwege,Tam Willy Nguyen,Emanuele Garone*

Main category: eess.SY

TL;DR: Presents two scalable methods for control allocation of rigid bodies with vectorized thrusters: (1) a closed-form Lipschitz continuous mapping for smooth actuator orientation references, and (2) a convex optimization formulation handling thrust saturation and angular rate limits; both use null-space projection to avoid singularities and are validated on 3DOF marine and 6DOF aerial simulations.


<details>
  <summary>Details</summary>
Motivation: Existing allocation methods for vectored-thruster platforms either produce non-smooth actuator commands or require complex, non-convex optimization to respect practical actuator limits and avoid singularities. A scalable, smooth, and constraint-aware approach is needed for arbitrary rigid bodies.

Method: Develops a closed-form mapping that is Lipschitz continuous to generate smooth actuator orientation references from desired wrench commands. Additionally formulates a convex optimization problem that incorporates thrust saturation and angular-rate bounds; both approaches exploit the null-space of the allocation matrix for singularity avoidance and to obtain sub-optimal but feasible actuator solutions.

Result: Numerical simulations on a 3DOF marine vessel and a 6DOF quadcopter show the methods produce smooth actuator orientations, respect actuator limits, and successfully avoid singular configurations while achieving desired wrenches with practical performance.

Conclusion: The framework offers a general, scalable way to allocate forces and moments for arbitrary rigid bodies with vectored thrusters, trading optimality for smoothness and constraint satisfaction; next steps include real-world experiments and analysis of robustness and computational load.

Abstract: This study introduces a systematic and scalable method for arbitrary
rigid-bodies equipped with vectorized thrusters. Two novel solutions are
proposed: a closed-form, Lipschitz continuous mapping that ensures smooth
actuator orientation references, and a convex optimization formulation capable
of handling practical actuator constraints such as thrust saturation and
angular rate limits. Both methods leverage the null-space structure of the
allocation mapping to perform singularity avoidance while generating
sub-optimal yet practical solutions. The effectiveness and generality of the
proposed framework are demonstrated through numerical simulations on a 3DOF
marine vessel and a 6DOF aerial quadcopter.

</details>


### [15] [Closed-loop control of sloshing fuel in a spinning spacecraft](https://arxiv.org/abs/2510.08121)
*Umberto Zucchelli,Miguel Alfonso Mendez,Annafederica Urbano,Sebastien Vincent-Bonnieu,Piotr Wenderski,Francesco Sanfedino*

Main category: eess.SY

TL;DR: 本文比较了高保真CFD与简化等效机械模型(EMM)在闭环姿控场景下预测推进剂摇动对航天器动力学影响的能力，结果表明简化模型对所测机动具有良好一致性，可用于早期设计的敏感性与稳定性分析。


<details>
  <summary>Details</summary>
Motivation: 新一代航天器携带大量液体推进剂，推进剂摇动耦合控制-结构-推进剂动力学会显著影响姿控与稳定性。CFD精度高但计算成本大，需寻求更高效的替代模型以支撑迭代设计。

Method: 对同一航天器姿控机动，分别开展高保真CFD仿真与基于等效机械模型的降阶摇动模型仿真，在闭环反馈控制下比较两者响应，并评估简化模型在敏感性与稳定性分析中的适用性。

Result: CFD与降阶模型的闭环响应在所考察的机动下吻合良好，表明等效机械模型能够捕捉主要耦合效应。基于此验证，作者进行了更高效的参数敏感性与稳定性研究。

Conclusion: 经验证的简化模型能在保证足够精度的前提下大幅降低计算成本，适合作为早期航天器设计与控制鲁棒性分析的实用工具。

Abstract: New-generation space missions require satellites to carry substantial amounts
of liquid propellant, making it essential to analyse the coupled
control-structure-propellant dynamics in detail. While Computational Fluid
Dynamics (CFD) offers high-fidelity predictions, its computational cost limits
its use in iterative design. Equivalent Mechanical Models (EMMs) provide a
faster alternative, though their predictive performance, especially in
closed-loop scenarios, remains largely unexplored. This work presents a
comparative analysis of a spacecraft under feedback control, using both CFD and
a reduced-order sloshing model. Results show good agreement, validating the
simplified model for the manoeuvrer considered. This validation enables
efficient sensitivity and stability studies, offering a practical tool for
early-stage spacecraft design.

</details>


### [16] [A Control Allocation Algorithm for Hypersonic Glide Vehicles with Input Limitations](https://arxiv.org/abs/2510.08275)
*Johannes Autenrieb,Patrick Gruhn*

Main category: eess.SY

TL;DR: Iterative control allocation for hypersonic glide vehicles (HGVs) that respects actuator magnitude/rate and includes soft drag-sensitive constraints to reduce heating and IR signature; evaluated on GHGV-2 simulation showing maintained control under constraints.


<details>
  <summary>Details</summary>
Motivation: HGVs face nonlinear actuators, asymmetric/state-dependent limits, and maneuver-dependent thermal loads; need real-time allocation to satisfy moment commands while respecting constraints and improving survivability/efficiency.

Method: Proposes an iterative real-time control allocation algorithm that searches for input commands satisfying desired moments under magnitude and rate bounds, embedding drag-sensitive soft constraints to tradeoffs between control and drag/thermal effects.

Result: Demonstrated on DLR GHGV-2 model, showing effective maintenance of control authority under realistic constraints and reduced drag/implicit thermal load via embedded soft constraints.

Conclusion: The method enables constrained-aware control allocation for slender HGVs, improving energy efficiency and lowering IR signature while ensuring moment tracking under actuator limits.

Abstract: Hypersonic glide vehicles (HGVs) operate in challenging flight regimes
characterized by strong nonlinearities in actuation and stringent physical
constraints. These include state-dependent actuator limitations, asymmetric
control bounds, and thermal loads that vary with maneuvering conditions. This
paper introduces an iterative control allocation method to address these
challenges in real time. The proposed algorithm searches for control inputs
that achieve the desired moment commands while respecting constraints on input
magnitude and rate. For slender HGV configurations, thermal loads and drag
generation are strongly correlated-lower drag typically results in reduced
surface heating. By embedding drag-sensitive soft constraints, the method
improves energy efficiency and implicitly reduces surface temperatures,
lowering the vehicle's infrared signature. These features are particularly
advantageous for long-range military operations that require low observability.
The approach is demonstrated using the DLR's Generic Hypersonic Glide Vehicle 2
(GHGV-2) simulation model. The results confirm the method's effectiveness in
maintaining control authority under realistic, constrained flight conditions.

</details>


### [17] [CPU- and GPU-Based Parallelization of the Robust Reference Governor](https://arxiv.org/abs/2510.08288)
*Hamid R. Ossareh,William Shayne,Samuel Chevalier*

Main category: eess.SY

TL;DR: 提出一种面向非线性系统的基于场景的鲁棒参考治理器（RG）并在多核CPU与CUDA GPU上并行实现，实验证明在燃料电池模型上较串行实现可实现数量级加速。


<details>
  <summary>Details</summary>
Motivation: 在带约束的控制系统中，参考治理器能在原有反馈控制器外加以保证状态与输入约束，但现有鲁棒RG对线性系统有效，对非线性系统常因计算复杂度过高难以实用；因此需要一种既有鲁棒性又能实际求解的非线性RG方法并利用并行硬件缩短在线计算时间。

Method: 提出基于场景（scenario-based）的鲁棒RG框架，将不确定性通过样本（场景）离散化，形成可并行求解的子问题；分析算法的计算结构，识别可并行化的组件（如场景模拟、成本/约束评估、投影或搜索步骤），并分别在多核CPU与CUDA GPU上实现优化。

Result: 在非线性氢燃料电池模型上的基准测试显示并行实现相比串行实现带来显著加速，报告的加速比最高可达三个数量级，证明了该方法在复杂非线性系统上具有实际可行性与高效性。

Conclusion: 将场景化鲁棒RG与现代并行硬件结合，能够把原本对非线性问题的不可行计算负担降到可接受范围，为在线鲁棒约束管理打开了可行路径；但方法性能依赖场景数、并行资源与实现细节，需权衡概率性保障、内存与实时性约束。

Abstract: Constraint management is a central challenge in modern control systems. A
solution is the Reference Governor (RG), which is an add-on strategy to
pre-stabilized feedback control systems to enforce state and input constraints
by shaping the reference command. While robust formulations of RG exist for
linear systems, their extension to nonlinear systems is often computationally
intractable. This paper develops a scenario-based robust RG formulation for
nonlinear systems and investigates its parallel implementation on multi-core
CPUs and CUDA-enabled GPUs. We analyze the computational structure of the
algorithm, identify parallelization opportunities, and implement the resulting
schemes on modern parallel hardware. Benchmarking on a nonlinear hydrogen fuel
cell model demonstrates order-of-magnitude speedups (by as much as three orders
of magnitude) compared to sequential implementations.

</details>


### [18] [Underground Power Distribution System Restoration Using Inverter Based Resources](https://arxiv.org/abs/2510.08356)
*Wenlong Shi,Hongyi Li,Zhaoyu Wang*

Main category: eess.SY

TL;DR: 提出了一套基于逆变器资源（IBRs）的地下配电系统恢复框架，通过建立地下电缆充磁电流模型、变压器铁磁共振评估模型和相位切换模型，并将这些模型集成到最大化带权恢复负荷的MINLP优化中，同时提出了一种基于置换的线性化方法来处理相序重排引起的阻抗矩阵非线性，最终在IEEE 123节点测试馈线上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 城市地下配电系统更多采用智能设备和IBRs，提升韧性的同时引入独特问题：地下电缆的陷留电荷导致的冲击电流、配电变压器在通电时的铁磁共振、以及单相地下支线引发的三相不平衡。需要一个系统化恢复策略在满足设备约束下尽可能多地恢复负荷。

Method: 1) 建立地下电缆通电模型，通过分析开关两端电压差量化冲击电流；2) 提出配电变压器通电时的铁磁共振模型，利用基于地下电缆电容和阻尼电阻的Q因子约束进行评估；3) 提出相位切换模型，通过智能配电开关动态重分配支线相连以改善三相平衡；4) 将上述模型并入一个混合整数非线性规划（MINLP）以最大化加权恢复负荷，同时对冲击电流、铁磁共振和相位不平衡设约束；5) 为解决相序重排引起的阻抗矩阵非线性，提出一种基于置换的线性化技巧；6) 在基于IEEE 123节点建立的地下PDS上开展仿真验证。

Result: 仿真结果表明所提框架能在满足冲击电流、铁磁共振和三相不平衡约束下，比基线策略恢复更多的加权负荷；提出的置换线性化方法有效降低了优化问题的非线性度，支持在MINLP中实现相位切换。

Conclusion: 提出了一个面向地下PDS恢复的系统性方法，兼顾电磁暂态（冲击电流、铁磁共振）和稳态相位平衡，通过模型融合与线性化技术提升优化求解可行性，在标准测试馈线上验证有效，具备实际工程参考价值。

Abstract: Underground power distribution systems (PDSs) are increasingly deployed in
urban areas. The integration of smart devices including smart switchgears,
pad-mounted distribution transformers and inverter-based resources (IBRs)
enhance system resilience, however simultaneously introducing unique
challenges. The challenges include inrush currents caused by trapped charges in
underground cables, ferroresonance in distribution transformers during
energization, and three-phase load imbalance resulting from single-phase
underground laterals. To address these issues, this paper proposes an
underground PDS restoration framework using IBRs. Firstly, an underground cable
energization model is developed to quantify inrush current by analyzing voltage
differences across both switchgear terminals. Secondly, a distribution
transformer energization model is proposed to evaluate ferroresonance using
Q-factor constraints based on underground cable capacitance and damping
resistance. Thirdly, a phase-swapping model is proposed to improve load
balancing by dynamically reassigning lateral-phase connections through smart
switchgears. The proposed models are further integrated into a mixed-integer
nonlinear programming (MINLP) formulation to maximize the total weighted
restored load while constraining inrush currents, ferroresonance, and phase
imbalance. To address the nonlinearity induced by impedance matrix reordering
during phase swapping, a permutation-based linearization technique is proposed.
Finally, case studies on an underground PDS established based on IEEE 123-Node
Test Feeder validate the effectiveness of the proposed strategy in improving
uderground PDS restoration performance.

</details>


### [19] [Learning to Mitigate Post-Outage Load Surges: A Data-Driven Framework for Electrifying and Decarbonizing Grids](https://arxiv.org/abs/2510.08357)
*Wenlong Shi,Dingwei Wang,Liming Liu,Zhaoyu Wang*

Main category: eess.SY

TL;DR: 本研究基于印第安纳波利斯3万余次配电馈线停电事件与智能电表/分表数据，实证并因果量化电动汽车、热泵与分布式能源对恢复阶段负荷激增的影响，提出组件感知的多任务Transformer模型并针对2035年情景做反事实预测，发现电气化资产显著放大恢复涌流但可通过概率性重启、温控短时偏移与加速并网等运营策略有效缓解。


<details>
  <summary>Details</summary>
Motivation: 随着电气化与低碳化推进，配电网在停电恢复阶段出现的负荷涌流特征可能改变，需了解哪些新型用能资产如何影响恢复可靠性并评估可行缓解措施。

Method: 构建包含30,046次馈线级停电事件并链接智能电表与分表的都会区异质数据集；采用统计分析与因果森林（causal forest）量化资产渗透率的因果效应；设计组件感知的多任务Transformer用于分解EV、HP、DER对涌流的贡献并对2035年不同采用路径进行反事实蒙特卡洛模拟与情景分析；测试多种运营缓解措施的效果。

Result: 三类资产渗透率上升都会显著提高恢复涌流比，且效应受恢复时段（夜晚/傍晚更严重）、停电时长与天气调制；在政策一致路径下，傍晚恢复成为可靠性约束，当15分钟内恢复≥30%系统负荷时超标概率为0.057；采取概率性EV重启、短期温控偏移与加速DER并网可将超标概率降至0.019，并在初期恢复≤20%时消除超标风险。

Conclusion: 过渡期的恢复涌流是由电气化与去碳化资产驱动且具因果关系，但通过综合的运营与控制策略（面向EV、HP、DER的协同措施）可在规划期内有效管理该风险，支持电网可靠性与清洁能源目标的并行推进。

Abstract: Electrification and decarbonization are transforming power system demand and
recovery dynamics, yet their implications for post-outage load surges remain
poorly understood. Here we analyze a metropolitan-scale heterogeneous dataset
for Indianapolis comprising 30,046 feeder-level outages between 2020 and 2024,
linked to smart meters and submetering, to quantify the causal impact of
electric vehicles (EVs), heat pumps (HPs) and distributed energy resources
(DERs) on restoration surges. Statistical analysis and causal forest inference
demonstrate that rising penetrations of all three assets significantly increase
surge ratios, with effects strongly modulated by restoration timing, outage
duration and weather conditions. We develop a component-aware multi-task
Transformer estimator that disaggregates EV, HP and DER contributions, and
apply it to project historical outages under counterfactual 2035 adoption
pathways. In a policy-aligned pathway, evening restorations emerge as the
binding reliability constraint, with exceedance probabilities of 0.057 when
30\% of system load is restored within the first 15 minutes. Mitigation
measures, probabilistic EV restarts, short thermostat offsets and accelerated
DER reconnection, reduce exceedance to 0.019 and eliminate it entirely when
20\% or less of system load is restored. These results demonstrate that
transition-era surges are asset-driven and causally linked to electrification
and decarbonization, but can be effectively managed through integrated
operational strategies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [20] [PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing](https://arxiv.org/abs/2510.07452)
*Anthony Hughes,Vasisht Duddu,N. Asokan,Nikolaos Aletras,Ning Ma*

Main category: cs.CR

TL;DR: 提出PATCH，一种通过电路发现识别并直接编辑导致模型泄露个人可识别信息（PII）的计算电路的方法，能够比差分隐私在保持效用的同时更有效地降低PII泄露；可与DP结合将残余泄露降至极低水平。


<details>
  <summary>Details</summary>
Motivation: 观察到大型语言模型会记忆训练数据中的PII，现有防御（如DP）虽能减少泄露但往往以较大效用损失为代价。基于电路发现显示存在专门负责PII泄露的计算通路，作者假设有针对性的编辑这些通路可在更小效用损失下减少泄露。

Method: 使用电路发现技术定位与PII泄露相关的子电路，然后对这些电路进行直接编辑（PATCH），评估单独使用PATCH与将PATCH与差分隐私组合的效果。

Result: PATCH在隐私-效用权衡上优于现有防御，在实验中可将LM的PII召回率最多降低约65%；与DP结合时，可将残余泄露召回率降至约0.01%。还发现现有防御后PII泄露电路仍然存在，而PATCH能有效缓解其影响。

Conclusion: 通过精确识别并修补负责PII泄露的计算电路，可以在比通用方法（如DP）更少的效用损失下显著降低泄露，且与DP互补，代表了一条有前景的隐私防护新方向。

Abstract: Language models (LMs) may memorize personally identifiable information (PII)
from training data, enabling adversaries to extract it during inference.
Existing defense mechanisms such as differential privacy (DP) reduce this
leakage, but incur large drops in utility. Based on a comprehensive study using
circuit discovery to identify the computational circuits responsible PII
leakage in LMs, we hypothesize that specific PII leakage circuits in LMs should
be responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware
Targeted Circuit PatcHing), a novel approach that first identifies and
subsequently directly edits PII circuits to reduce leakage. PATCH achieves
better privacy-utility trade-off than existing defenses, e.g., reducing recall
of PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to
reduce recall of residual leakage of an LM to as low as 0.01%. Our analysis
shows that PII leakage circuits persist even after the application of existing
defense mechanisms. In contrast, PATCH can effectively mitigate their impact.

</details>


### [21] [MIRANDA: short signatures from a leakage-free full-domain-hash scheme](https://arxiv.org/abs/2510.07479)
*Alain Couvreur,Thomas Debris-Alazard,Philippe Gaborit,Adrien Vinçotte*

Main category: cs.CR

TL;DR: 提出了 Miranda：首个基于矩阵码的全域哈希（GPV范式）签名方案。使用一个通用且简单的陷门（子码/可解码码的唯一解区间），无拒绝采样，实例化为矩阵表示的Gabidulin码。宣称满足EUF-CMA安全性，128位安全下签名约90字节，公钥约2.6MB。


<details>
  <summary>Details</summary>
Motivation: 提供一种满足GPV安全范式的、实现更简单且可泛化的代码/格签名构造，避免常见GPV变体（如Falcon、Wave）中复杂或开销大的拒绝采样机制，同时利用矩阵码结构获得小签名。

Method: 设计一个通用陷门：基于一个在唯一解区间参数下可解码码的子码；签名过程求解唯一的解但允许多重签名表示，通过随机抽取少量均匀比特来掩盖陷门信息，避免泄露；将该构造用矩阵表示的Gabidulin码实例化，并给出EUF-CMA的安全分析。

Result: 实现下的性能指标：128位安全时签名约90字节，公钥约2.6MB；实现上不需要拒绝采样，签名算法更简单。安全性在EUF-CMA模型下进行了分析。

Conclusion: Miranda提出了一个简单且可泛化的GPV风格陷门构造，能在保持强安全保证的同时实现非常小的签名尺寸和更易实现的签名流程。主要权衡是较大的公钥和对底层矩阵码（如Gabidulin）困难性的依赖。

Abstract: We present $\mathsf{Miranda}$, the first family of full-domain-hash
signatures based on matrix codes. This signature scheme fulfils the paradigm of
Gentry, Peikert and Vaikuntanathan ($\mathsf{GPV}$), which gives strong
security guarantees. Our trapdoor is very simple and generic: if we propose it
with matrix codes, it can actually be instantiated in many other ways since it
only involves a subcode of a decodable code (or lattice) in a unique decoding
regime of parameters. Though $\mathsf{Miranda}$ signing algorithm relies on a
decoding task where there is exactly one solution, there are many possible
signatures given a message to sign and we ensure that signatures are not
leaking information on their underlying trapdoor by means of a very simple
procedure involving the drawing of a small number of uniform bits. In
particular $\mathsf{Miranda}$ does not use a rejection sampling procedure which
makes its implementation a very simple task contrary to other
$\mathsf{GPV}$-like signatures schemes such as $\mathsf{Falcon}$ or even
$\mathsf{Wave}$.
  We instantiate $\mathsf{Miranda}$ with the famous family of Gabidulin codes
represented as spaces of matrices and we study thoroughly its security (in the
EUF-CMA security model). For~$128$ bits of classical security, the signature
sizes are as low as~$90$ bytes and the public key sizes are in the order
of~$2.6$ megabytes.

</details>


### [22] [EMPalm: Exfiltrating Palm Biometric Data via Electromagnetic Side-Channels](https://arxiv.org/abs/2510.07533)
*Haowen Xu,Tianya Zhao,Xuyu Wang,Lei Ma,Jun Dai,Alexander Wyglinski,Xiaoyan Sun*

Main category: cs.CR

TL;DR: 提出EMPalm攻击框架，通过窃听掌纹/掌静脉设备的电磁泄露信号（EM）分离双模传输、提取频带并重建图像，最后用扩散模型恢复细节，从而高保真地重构生物特征并能对识别模型实施攻击。


<details>
  <summary>Details</summary>
Motivation: 现代掌纹/掌静脉识别设备在工作时会产生可被窃听的电磁辐射，研究旨在验证这些EM泄露是否能被用于恢复敏感生物识别信息并评估实际攻击风险。

Method: 对采集的EM信号执行模态分离，识别并融合有信息的频段以初步重构图像；随后使用扩散模型对重建图像进行细节增强以恢复域特有的生物特征。

Result: 在七种原型设备和两款商用设备上，EMPalm在视觉质量指标上表现良好（SSIM最高0.79、PSNR最高29.88 dB、FID最低6.82），并在针对四个掌识别模型的攻击评估中，平均欺骗成功率达到65.30%。

Conclusion: EM泄露可成为针对掌识别系统的实用侧信道攻击路径，EMPalm证明了从EM辐射恢复敏感生物特征并对现有识别模型实施高成功率欺骗的可行性，提示需要加强硬件隔离和信号处理防护。

Abstract: Palm recognition has emerged as a dominant biometric authentication
technology in critical infrastructure. These systems operate in either
single-modal form, using palmprint or palmvein individually, or dual-modal
form, fusing the two modalities. Despite this diversity, they share similar
hardware architectures that inadvertently emit electromagnetic (EM) signals
during operation. Our research reveals that these EM emissions leak palm
biometric information, motivating us to develop EMPalm--an attack framework
that covertly recovers both palmprint and palmvein images from eavesdropped EM
signals. Specifically, we first separate the interleaved transmissions of the
two modalities, identify and combine their informative frequency bands, and
reconstruct the images. To further enhance fidelity, we employ a diffusion
model to restore fine-grained biometric features unique to each domain.
Evaluations on seven prototype and two commercial palm acquisition devices show
that EMPalm can recover palm biometric information with high visual fidelity,
achieving SSIM scores up to 0.79, PSNR up to 29.88 dB, and FID scores as low as
6.82 across all tested devices, metrics that collectively demonstrate strong
structural similarity, high signal quality, and low perceptual discrepancy. To
assess the practical implications of the attack, we further evaluate it against
four state-of-the-art palm recognition models, achieving a model-wise average
spoofing success rate of 65.30% over 6,000 samples from 100 distinct users.

</details>


### [23] [A Minrank-based Encryption Scheme à la Alekhnovich-Regev](https://arxiv.org/abs/2510.07584)
*Thomas Debris-Alazard,Philippe Gaborit,Romaric Neveu,Olivier Ruatta*

Main category: cs.CR

TL;DR: 提出将Alekhnovich和Regev加密方案改造为仅基于stationary-MinRank（平稳MinRank）困难性的公钥加密；通过证明search-to-decision归约以建立强安全性保证，并给出性能分析表明其实用且具竞争力。


<details>
  <summary>Details</summary>
Motivation: 长期开放问题：是否可以构造一个其安全性仅依赖于MinRank问题的公钥加密方案。作者希望通过引入stationary-MinRank变体并证明其判定/搜索之间的等价性，来实现这一目标，从而提供类似于基于LWE或随机线性码的强安全保证。

Method: 在Alekhnovich/Regev方案框架下做改造，引入stationary-MinRank问题作为基石，证明该变体存在搜索到判定的归约（search-to-decision reduction），并据此给出从平均情形stationary-MinRank困难性到方案语义安全的形式化归约。随后进行了详尽的安全与效率分析，并与已有方案（如FrodoKEM、原始Alekhnovich/Regev方案）比较。

Result: 获得了一个安全性仅基于stationary-MinRank的公钥加密方案；该方案在实现与性能上优于Alekhnovich/Regev原始方案、次于FrodoKEM，但仍具有实际竞争力。作者还指出可通过引入结构（仿照HQC和Kyber）进一步提升效率。

Conclusion: 论文部分解决了长期的开放问题，给出基于MinRank困难性（而非混合假设）的加密构造，并证明了其可行性与实际潜力。下一步可沿着添加结构化优化和更细致的安全参数选择继续推进。

Abstract: Introduced in 2003 and 2005, Alekhnovich and Regev' schemes were the first
public-key encryptions whose security is only based on the average hardness of
decoding random linear codes and LWE, without other security assumptions. Such
security guarantees made them very popular, being at the origin of the now
standardized HQC or Kyber.
  We present an adaptation of Alekhnovich and Regev' encryption scheme whose
security is only based on the hardness of a slight variation of MinRank, the
so-called stationary-MinRank problem. We succeeded to reach this strong
security guarantee by showing that stationary-MinRank benefits from a
search-to-decision reduction. Our scheme therefore brings a partial answer to
the long-standing open question of building an encryption scheme whose security
relies solely on the hardness of MinRank.
  Finally, we show after a thoroughly security analysis that our scheme is
practical and competitive with other encryption schemes admitting such strong
security guarantees. Our scheme is slightly less efficient than FrodoKEM, but
much more efficient than Alekhnovich and Regev' original schemes, with
possibilities of improvements by considering more structure, in the same way as
HQC and Kyber.

</details>


### [24] [Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs](https://arxiv.org/abs/2510.07697)
*Man Hu,Xinyi Wu,Zuofeng Suo,Jinbo Feng,Linghui Meng,Yanhao Jia,Anh Tuan Luu,Shuai Zhao*

Main category: cs.CR

TL;DR: 本文系统回顾并首次聚焦于针对大语言模型（LLMs）推理能力的后门攻击，提出了“关联型、被动型、主动型”三类分类法，并讨论了相应防御策略与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理能力增强，攻击者可利用这些能力设计更隐蔽的后门攻击，现有综述未专门深入分析针对推理链路的后门问题，因此需要一篇专门梳理此类威胁与防御的工作。

Method: 通过文献分析与机制剖析，构建统一视角的分类体系（关联、被动、主动），逐类总结攻击方法、触发方式与防御思路，评估现有防护手段的覆盖与不足，并归纳挑战与未来方向。

Result: 提出了首个针对推理基后门的系统分类与分析框架，总结了代表性攻击与防御技术，指出了当前防御在检测可解释性、泛化性与数据效用保持方面的不足。

Conclusion: 该工作为理解与研究基于推理的后门攻击提供了框架与研究议程，推动开发更健壮可信的LLM防护措施。

Abstract: With the rise of advanced reasoning capabilities, large language models
(LLMs) are receiving increasing attention. However, although reasoning improves
LLMs' performance on downstream tasks, it also introduces new security risks,
as adversaries can exploit these capabilities to conduct backdoor attacks.
Existing surveys on backdoor attacks and reasoning security offer comprehensive
overviews but lack in-depth analysis of backdoor attacks and defenses targeting
LLMs' reasoning abilities. In this paper, we take the first step toward
providing a comprehensive review of reasoning-based backdoor attacks in LLMs by
analyzing their underlying mechanisms, methodological frameworks, and
unresolved challenges. Specifically, we introduce a new taxonomy that offers a
unified perspective for summarizing existing approaches, categorizing
reasoning-based backdoor attacks into associative, passive, and active. We also
present defense strategies against such attacks and discuss current challenges
alongside potential directions for future research. This work offers a novel
perspective, paving the way for further exploration of secure and trustworthy
LLM communities.

</details>


### [25] [ANCORA: Accurate Intrusion Recovery for Web Applications](https://arxiv.org/abs/2510.07806)
*Yihao Peng,Biao Ma,Hai Wan,Xibin Zhao*

Main category: cs.CR

TL;DR: ANCORA is a non-invasive system for precise intrusion recovery in web applications. It isolates syscall sequences per malicious request, builds file provenance graphs and uses a spatiotemporal anchor for attributing DB operations, then rewinds to a clean snapshot and selectively replays legitimate ops, achieving 99.9% recovery accuracy with moderate overhead.


<details>
  <summary>Details</summary>
Motivation: Coarse-grained snapshots lose legitimate user data; attributing file and DB changes to a single malicious request in high-concurrency web apps is hard due to interleaving and connection pooling. A precise recovery that preserves legitimate concurrent updates is needed.

Method: Isolate syscalls for a single request; for files, construct provenance graphs including exploit-spawned processes to find impacted files; for databases, introduce a spatiotemporal anchor (network connection tuple + active time window) to map DB operations to the request despite pooling. Perform unified rewind to a pre-attack snapshot then selective replay of only legitimate operations to filesystem and DB.

Result: Evaluated on 10 web apps and 20 CVE-based attacks with up to 150 concurrent connections. Achieves 99.9% recovery accuracy. Overhead up to 19.8% latency increase and 17.8% QPS decrease worst-case. Recovery throughput: 110.7 DB ops/s and 27.2 affected files/s. Preserves legitimate data effectively.

Conclusion: ANCORA demonstrates that precise, non-invasive intrusion recovery is feasible in high-concurrency web apps by combining syscall isolation, file provenance, and a spatiotemporal DB anchor, balancing accuracy with acceptable runtime/recovery overhead.

Abstract: Modern web application recovery presents a critical dilemma. Coarse-grained
snapshot rollbacks cause unacceptable data loss for legitimate users.
Surgically removing an attack's impact is hindered by a fundamental challenge
in high-concurrency environments: it is difficult to attribute resulting file
and database modifications to a specific attack-related request. We present
ANCORA, a system for precise intrusion recovery in web applications without
invasive instrumentation. ANCORA first isolates the full sequence of syscalls
triggered by a single malicious request. Based on this sequence, ANCORA
addresses file and database modifications separately. To trace file changes, it
builds a provenance graph that reveals all modifications, including those by
exploit-spawned processes. To attribute database operations, a more difficult
challenge due to connection pooling, ANCORA introduces a novel spatiotemporal
anchor. This anchor uses the request's network connection tuple and active time
window to pinpoint exact database operations. With all malicious file and
database operations precisely identified, ANCORA performs a unified rewind and
selective replay recovery. It reverts the system to a clean snapshot taken
before the attack, then selectively re-applies only legitimate operations to
both the file system and database. This completely removes the attack's effects
while preserving concurrent legitimate data. We evaluated ANCORA on 10 web
applications and 20 CVE-based attack scenarios with concurrency up to 150
connections. Experiments demonstrate ANCORA achieves 99.9% recovery accuracy
with manageable overhead: up to 19.8% response latency increase and 17.8% QPS
decrease in worst cases, and recovery throughput of 110.7 database operations
per second and 27.2 affected files per second, effectively preserving
legitimate data.

</details>


### [26] [Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents](https://arxiv.org/abs/2510.07809)
*Renhua Ding,Xiao Yang,Zhengwei Fang,Jun Luo,Kun He,Jun Zhu*

Main category: cs.CR

TL;DR: 本文提出一种针对移动视觉语言模型驱动智能体的实用且隐蔽的一次性越狱攻击：通过在恶意应用的界面文本中嵌入在人类交互时不可见、但在通过ADB由智能体驱动界面时会暴露的提示，从而诱导或绕过模型的安全限制。作者提出感知链注入、触摸触发的用户不可见激活机制，以及一种启发式字符级迭代加深搜索算法（HG-IDA*）用于一次性关键词级解毒，并在多种LVLM后端与安卓应用中验证了高命中率。


<details>
  <summary>Details</summary>
Motivation: 当前利用大视觉语言模型（LVLM）的自主移动智能体能够操作智能手机界面，但对UI层级攻击的研究不足。已有工作依赖明显的界面覆盖、提升权限或不现实的威胁模型，限制了隐蔽性和实际可行性。作者希望展示一种更现实、无需高权限且难被人类察觉的攻击方式，揭示移动智能体在实际部署中的安全风险。

Method: 提出由三部分组成的框架：
1) 低权限感知链目标：将恶意负载注入为智能体的视觉输入（即恶意应用界面文本）；
2) 隐蔽的用户不可见激活：基于触摸的触发器，利用物理触摸特征区分人类触摸与由ADB驱动的触摸，从而仅在智能体操作时曝光负载；
3) 一次性提示有效性：设计HG-IDA*，一种启发式引导的字符级迭代加深A*搜索，用于生成或修改一次性关键词提示以绕过设备端安全过滤并诱导模型执行恶意规划/动作。

Result: 在多个LVLM后端（包含闭源服务与代表性开源模型）和三款Android应用上评估，报告在单次攻击场景下高规划与执行劫持率，例如GPT-4o达到82.5%的规划劫持率和75.0%的执行劫持率，表明该攻击在实操中具备较高成功率。

Conclusion: 本文揭示了当前移动智能体在UI层面存在的根本安全漏洞：通过在UI中隐藏一次性提示并仅在自动化触发下暴露，攻击者能在低权限、难以被用户察觉的条件下显著劫持智能体的规划与执行，需在模型与平台层面加强对输入来源、触摸行为特征和一次性提示的检测与防护。

Abstract: Large vision-language models (LVLMs) enable autonomous mobile agents to
operate smartphone user interfaces, yet vulnerabilities to UI-level attacks
remain critically understudied. Existing research often depends on conspicuous
UI overlays, elevated permissions, or impractical threat models, limiting
stealth and real-world applicability. In this paper, we present a practical and
stealthy one-shot jailbreak attack that leverages in-app prompt injections:
malicious applications embed short prompts in UI text that remain inert during
human interaction but are revealed when an agent drives the UI via ADB (Android
Debug Bridge). Our framework comprises three crucial components: (1)
low-privilege perception-chain targeting, which injects payloads into malicious
apps as the agent's visual inputs; (2) stealthy user-invisible activation, a
touch-based trigger that discriminates agent from human touches using physical
touch attributes and exposes the payload only during agent operation; and (3)
one-shot prompt efficacy, a heuristic-guided, character-level
iterative-deepening search algorithm (HG-IDA*) that performs one-shot,
keyword-level detoxification to evade on-device safety filters. We evaluate
across multiple LVLM backends, including closed-source services and
representative open-source models within three Android applications, and we
observe high planning and execution hijack rates in single-shot scenarios
(e.g., GPT-4o: 82.5% planning / 75.0% execution). These findings expose a
fundamental security vulnerability in current mobile agents with immediate
implications for autonomous smartphone operation.

</details>


### [27] [Decentralised Blockchain Management Through Digital Twins](https://arxiv.org/abs/2510.07901)
*Georgios Diamantopoulos,Nikos Tziritas,Rami Bahsoon,Georgios Theodoropoulos*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The necessity of blockchain systems to remain decentralised limits current
solutions to blockchain governance and dynamic management, forcing a trade-off
between control and decentralisation. In light of the above, this work proposes
a dynamic and decentralised blockchain management mechanism based on digital
twins. To ensure decentralisation, the proposed mechanism utilises multiple
digital twins that the system's stakeholders control. To facilitate
decentralised decision-making, the twins are organised in a secondary
blockchain system that orchestrates agreement on, and propagation of decisions
to the managed blockchain. This enables the management of blockchain systems
without centralised control. A preliminary evaluation of the performance and
impact of the overheads introduced by the proposed mechanism is conducted
through simulation. The results demonstrate the proposed mechanism's ability to
reach consensus on decisions quickly and reconfigure the primary blockchain
with minimal overhead.

</details>


### [28] [From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses](https://arxiv.org/abs/2510.07968)
*Xiangtao Meng,Tianshuo Cong,Li Wang,Wenyu Chen,Zheng Li,Shanqing Guo,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 本文提出CrossRiskEval框架，系统评估针对某一风险（安全/公平/隐私）的防护部署是否会引发其他风险的副作用，并通过对14个防护模型和12种防护策略的实证及神经元层面分析，发现多种防护间存在冲突与相互放大效应。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM风险防护的研究多聚焦单一风险维度，忽视了防护部署可能在其他风险维度产生的意外影响，作者希望构建一个能捕捉防护间相互作用的评估框架并揭示其内在机制。

Method: 提出CrossRiskEval评估框架，针对14个已部署防护的LLM、12种防护策略进行大规模实证测评，评估安全、公平、隐私三类风险的交叉影响；此外在神经元层面进行细粒度分析，识别并量化“冲突-纠缠神经元”的存在及其在任务和神经元层面的趋势一致性。

Result: 实验发现：1）安全防护虽能抑制对敏感查询的直接响应，但可能放大间接隐私泄露或偏见输出；2）公平防护会增加滥用风险与隐私泄露概率；3）隐私防护常削弱安全性并加剧偏见；神经元分析揭示一些神经元在多个风险维度上表现相反敏感性，推动了防护部署后的意外行为。

Conclusion: 作者呼吁将LLM风险评估从单一维度转向整体、交互感知的范式，建议在设计与评估防护策略时同时考虑跨风险影响并关注神经元级别的干预与监测。

Abstract: Large Language Models (LLMs) have shown remarkable performance across various
applications, but their deployment in sensitive domains raises significant
concerns. To mitigate these risks, numerous defense strategies have been
proposed. However, most existing studies assess these defenses in isolation,
overlooking their broader impacts across other risk dimensions. In this work,
we take the first step in investigating unintended interactions caused by
defenses in LLMs, focusing on the complex interplay between safety, fairness,
and privacy. Specifically, we propose CrossRiskEval, a comprehensive evaluation
framework to assess whether deploying a defense targeting one risk
inadvertently affects others. Through extensive empirical studies on 14
defense-deployed LLMs, covering 12 distinct defense strategies, we reveal
several alarming side effects: 1) safety defenses may suppress direct responses
to sensitive queries related to bias or privacy, yet still amplify indirect
privacy leakage or biased outputs; 2) fairness defenses increase the risk of
misuse and privacy leakage; 3) privacy defenses often impair safety and
exacerbate bias. We further conduct a fine-grained neuron-level analysis to
uncover the underlying mechanisms of these phenomena. Our analysis reveals the
existence of conflict-entangled neurons in LLMs that exhibit opposing
sensitivities across multiple risk dimensions. Further trend consistency
analysis at both task and neuron levels confirms that these neurons play a key
role in mediating the emergence of unintended behaviors following defense
deployment. We call for a paradigm shift in LLM risk evaluation, toward
holistic, interaction-aware assessment of defense strategies.

</details>


### [29] [Composition Law of Conjugate Observables in Random Permutation Sorting Systems](https://arxiv.org/abs/2510.08013)
*Yurang R. Kuang*

Main category: cs.CR

TL;DR: A proposed composition law connects discrete permutation counts and continuous timing in a Random Permutation Sorting System (RPSS), enabling conversion of microarchitectural timing fluctuations into near-uniform random bits with provable geometric convergence; experiments claim >7.9998 bits/byte and good chi-square uniformity.


<details>
  <summary>Details</summary>
Motivation: To provide a universal, provably sound method to extract uniform randomness from general-purpose computation by exploiting emergent timing fluctuations in permutation-sorting dynamics, thereby enabling cryptographic-quality randomness without dedicated hardware.

Method: Derive a functional relation between the characteristic function of timing distributions and the probability generating function of permutation counts in RPSS; prove convergence theorems with explicit error bounds that show geometric convergence to uniform distribution; implement extraction/"entropy purification" procedures and validate on multiple platforms.

Result: Analytical convergence theorems with explicit bounds; experimental validation across diverse platforms reporting Shannon entropy >7.9998 bits/byte and chi-square tests indicating uniformity.

Conclusion: The composition law offers a universal theoretical foundation for generating provably uniform randomness from software-executed permutation sorting, potentially enabling cryptographic-grade random number generation from commodity computation.

Abstract: We present the discovery of a fundamental composition law governing conjugate
observables in the Random Permutation Sorting System (RPSS). The law links the
discrete permutation count Np and the continuous elapsed time T through a
functional relation connecting the characteristic function of timing
distributions to the probability generating function of permutation counts.
This framework enables entropy purification, transforming microarchitectural
timing fluctuations into uniform randomness via geometric convergence. We
establish convergence theorems with explicit bounds and validate the results
experimentally, achieving Shannon entropy above 7.9998 bits per byte and
chi-square uniformity across diverse platforms. The composition law provides a
universal foundation for generating provably uniform randomness from
general-purpose computation, securing cryptographic purity from emergent
computational dynamics.

</details>


### [30] [LLM-Assisted Web Measurements](https://arxiv.org/abs/2510.08101)
*Simone Bozzolan,Stefano Calzavara,Lorenzo Cazzaro*

Main category: cs.CR

TL;DR: 本文探索使用大语言模型(LLMs)对网站进行语义分类，以支持有针对性的Web安全与隐私测量。构建了若干分类任务与数据集，评估多个LLM，结果显示LLMs在多种场景下表现良好，并可实用地辅助后续测量研究。


<details>
  <summary>Details</summary>
Motivation: 现有热门网站列表缺乏语义标签，导致进行特定类别的安全/隐私测量时需采用随意偏置的数据收集方法。作者希望利用LLMs的语义理解能力为网站自动分类，从而使目标测量更准确与高效。

Method: 基于文献识别重要的网站分类任务，构建相应标注数据集；对多款LLM（可能包括开放与闭源模型）在这些任务上进行系统评估；随后用LLM辅助构建测量数据集并复现实验以评估推断有效性。

Result: 实验显示LLMs在多个分类场景下能取得较强性能；用LLM筛选的目标集合在后续安全/隐私测量中能产出与传统方法一致且可解释的结论，证明LLM在实用性与可扩展性上的潜力。

Conclusion: LLMs可作为实用工具，为Web安全与隐私测量提供语义层次的站点标签，降低人工标注成本并改善研究可重复性。但需注意模型偏差、分类边界与评估数据集的局限性。

Abstract: Web measurements are a well-established methodology for assessing the
security and privacy landscape of the Internet. However, existing top lists of
popular websites commonly used as measurement targets are unlabeled and lack
semantic information about the nature of the sites they include. This
limitation makes targeted measurements challenging, as researchers often need
to rely on ad-hoc techniques to bias their datasets toward specific categories
of interest. In this paper, we investigate the use of Large Language Models
(LLMs) as a means to enable targeted web measurement studies through their
semantic understanding capabilities. Building on prior literature, we identify
key website classification tasks relevant to web measurements and construct
datasets to systematically evaluate the performance of different LLMs on these
tasks. Our results demonstrate that LLMs may achieve strong performance across
multiple classification scenarios. We then conduct LLM-assisted web measurement
studies inspired by prior work and rigorously assess the validity of the
resulting research inferences. Our results demonstrate that LLMs can serve as a
practical tool for analyzing security and privacy trends on the Web.

</details>


### [31] [TracE2E: Easily Deployable Middleware for Decentralized Data Traceability](https://arxiv.org/abs/2510.08225)
*Daniel Pressensé,Elisavet Kozyri*

Main category: cs.CR

TL;DR: TracE2E是用Rust实现的中间件，通过封装标准库IO模块在不改动应用的情况下记录跨节点的数据来源（provenance）并根据这些来源强制执行数据保护策略（如保密性、完整性），支持多策略并一致地在节点间记录溯源信息。


<details>
  <summary>Details</summary>
Motivation: 在分布式应用中，需要可解释的数据流和自动的合规性保障，但现有方案常常需要对应用做大量改动或无法跨节点一致记录溯源。作者旨在提供一个易集成、可跨节点一致记录并能基于溯源实施多种策略的中间件。

Method: 通过在Rust标准库IO模块外包一层实现，TracE2E在进程输入输出处捕获并记录溯源元数据；中间件提供一个合规层，该层可根据已记录的溯源信息评估并强制执行多种数据保护策略。设计关注跨节点一致的溯源表示和策略执行机制。

Result: 系统能够在不需大量改动应用的前提下记录跨节点溯源并执行多策略合规规则。论文描述了溯源一致性机制和合规层的工作方式，并展示了多策略强制的可行性（实现细节与性能评估需论文中验证）。

Conclusion: TracE2E提出了一种低侵入性的跨节点溯源与合规解决方案，适合集成到现有和未来的Rust应用中，以增强数据可解释性与合规性。后续需通过实际基准与安全分析评估其开销、可扩展性与信任假设。

Abstract: This paper presents TracE2E, a middleware written in Rust, that can provide
both data explainability and compliance across multiple nodes. By mediating
inputs and outputs of processes, TracE2E records provenance information and
enforces data-protection policies (e.g., confidentiality, integrity) that
depend on the recorded provenance. Unlike existing approaches that necessitate
substantial application modifications, TracE2E is designed for easy integration
into existing and future applications through a wrapper of the Rust standard
library's IO module. We describe how TracE2E consistently records provenance
information across nodes, and we demonstrate how the compliance layer of
TracE2E can accommodate the enforcement of multiple policies.

</details>


### [32] [Systematic Assessment of Cache Timing Vulnerabilities on RISC-V Processors](https://arxiv.org/abs/2510.08272)
*Cédrick Austa,Jan Tobias Mühlberg,Jean-Michel Dricot*

Main category: cs.CR

TL;DR: 本文将x86-64的缓存定时漏洞基准移植到RISC-V，并在三款商用RISC-V内核（T-Head C910、SiFive U54、U74）上运行。发现C910暴露更多不同的定时类型，表明其潜在攻击面更大；37.5%的基准覆盖漏洞在所有处理器上均存在，只有6.8%在所有处理器上均不存在。该基准可帮助早期识别信息泄露源并推动缓解方案开发。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对RISC-V具体实现的微结构侧信道评估工具和基准，而RISC-V的开放性和快速扩展使得早期安全评估变得尤为重要。作者希望通过移植现有x86基准为RISC-V生态提供可用的评估手段。

Method: 将一个面向x86-64的基准套件移植到RISC-V架构，设计并执行一系列缓存相关的定时测试，统计不同‘定时类型’和基准覆盖的漏洞在三款商用RISC-V内核上的存在情况，并对比各内核的表现。

Result: C910出现更多不同的定时类型，暗示更多潜在泄露源；37.5%的漏洞类型在三款内核中均存在，只有6.8%在三者中均未出现。作者提供了移植后的基准以便RISC-V设计者使用。

Conclusion: 移植的基准可作为评估RISC-V核心缓存定时漏洞的工具，有助于在早期设计阶段发现信息泄露源并促进缓解手段的开发。

Abstract: While interest in the open RISC-V instruction set architecture is growing,
tools to assess the security of concrete processor implementations are lacking.
There are dedicated tools and benchmarks for common microarchitectural
side-channel vulnerabilities for popular processor families such as Intel
x86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in
porting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities
to RISC-V. We then use this benchmark to evaluate the security of three
commercially available RISC-V processors, the T-Head C910 and the SiFive U54
and U74 cores. We observe that the C910 processor exhibits more distinct timing
types than the other processors, leading to the assumption that code running on
the C910 would be exposed to more microarchitectural vulnerability sources. In
addition, our evaluation reveals that $37.5\%$ of the vulnerabilities covered
by the benchmark exist in all processors, while only $6.8\%$ are absent from
all cores. Our work, in particular the ported benchmark, aims to support RISC-V
processor designers to identify leakage sources early in their designs and to
support the development of countermeasures.

</details>


### [33] [New Machine Learning Approaches for Intrusion Detection in ADS-B](https://arxiv.org/abs/2510.08333)
*Mikaëla Ngamboé,Jean-Simon Marrocco,Jean-Yves Ouattara,José M. Fernandez,Gabriela Nicolescu*

Main category: cs.CR

TL;DR: 本文提出并比较了两种基于深度学习的地面ADS‑B入侵检测器：Transformer编码器和扩展LSTM（xLSTM），并采用先在正常消息上预训练再用被篡改消息微调的迁移学习策略。xLSTM在检测逐步恶化的隐蔽攻击上表现最好，F1=98.9%，延迟7.26s；Transformer F1=94.3%，延迟2.1s。


<details>
  <summary>Details</summary>
Motivation: 随着ATM体系对易受攻击的ADS‑B协议依赖增强，亟需高效、鲁棒的AI入侵检测方法来识别篡改的航迹/消息，尤其是那些逐步削弱态势感知的隐蔽攻击。

Method: 构建并训练两种深度模型（Transformer编码器、xLSTM），采用迁移学习：先在仅含正常ADS‑B消息的大规模数据上预训练编码器，再用含标签的篡改样本微调以提升对攻击样本的判别能力。评估包括检测性能（F1）、对未见攻击的泛化能力以及推理延迟与SSR刷新间隔的对比。

Result: xLSTM取得F1=98.9%，对未见攻击展示较好泛化；Transformer取得F1=94.3%。xLSTM推理延迟约7.26s，落在SSR刷新间隔（5–12s）内但对实时任务可能受限；Transformer延迟2.1s但检测性能较低。

Conclusion: 迁移学习结合序列模型（特别是xLSTM）能显著提升ADS‑B入侵检测的准确性和对隐蔽、逐步攻击的识别能力，但现实部署需在检测性能与推理延迟之间折中，并进一步验证在真实复杂流量与更广泛攻击类型下的稳健性。

Abstract: With the growing reliance on the vulnerable Automatic Dependent
Surveillance-Broadcast (ADS-B) protocol in air traffic management (ATM),
ensuring security is critical. This study investigates emerging machine
learning models and training strategies to improve AI-based intrusion detection
systems (IDS) for ADS-B. Focusing on ground-based ATM systems, we evaluate two
deep learning IDS implementations: one using a transformer encoder and the
other an extended Long Short-Term Memory (xLSTM) network, marking the first
xLSTM-based IDS for ADS-B. A transfer learning strategy was employed, involving
pre-training on benign ADS-B messages and fine-tuning with labeled data
containing instances of tampered messages. Results show this approach
outperforms existing methods, particularly in identifying subtle attacks that
progressively undermine situational awareness. The xLSTM-based IDS achieves an
F1-score of 98.9%, surpassing the transformer-based model at 94.3%. Tests on
unseen attacks validated the generalization ability of the xLSTM model.
Inference latency analysis shows that the 7.26-second delay introduced by the
xLSTM-based IDS fits within the Secondary Surveillance Radar (SSR) refresh
interval (5-12 s), although it may be restrictive for time-critical operations.
While the transformer-based IDS achieves a 2.1-second latency, it does so at
the cost of lower detection performance.

</details>


### [34] [A Haskell to FHE Transpiler](https://arxiv.org/abs/2510.08343)
*Anne Müller,Mohd Kashif,Nico Döttling*

Main category: cs.CR

TL;DR: This paper presents a Haskell-to-Boolean-circuit transpiler for FHE and an automatic circuit-layer parallel evaluator; evaluated on PIR and AES showing competitive speedups (28s @16 threads, 8s @100 threads).


<details>
  <summary>Details</summary>
Motivation: FHE requires low-level circuit representations which complicate development; transpilers lower the barrier and expanding supported high-level languages (Haskell) increases accessibility. Additionally, automating parallel execution of generated circuits reduces evaluation time on modern multi-core hardware.

Method: Design and implement a Haskell-to-Boolean-circuit transpiler that maps Haskell programs into circuits amenable to homomorphic evaluation. Implement an evaluator that parallelizes execution by processing gates layer-by-layer concurrently. Evaluate on Private Information Retrieval and AES workloads, comparing to prior manual parallelizations.

Result: Successful transpilation of Haskell programs to circuits; automatic parallel execution provides significant speedups, e.g., AES evaluation times of 28s with 16 threads and 8s with 100 threads. Outperforms some manual parallelizations but not all.

Conclusion: Extends FHE-targeting toolchain to Haskell and shows automated parallelization is practical and effective for nontrivial workloads, though manual optimization can still beat automatic methods in some cases.

Abstract: Fully Homomorphic Encryption (FHE) enables the evaluation of programs
directly on encrypted data. However, because only basic operations can be
performed on ciphertexts, programs must be expressed as boolean or arithmetic
circuits. This low-level representation makes implementing applications for FHE
significantly more cumbersome than writing code in a high-level language. To
reduce this burden, several transpilers have been developed that translate
high-level code into circuit representations. In this work, we extend the range
of high-level languages that can target FHE by introducing a transpiler for
Haskell, which converts Haskell programs into Boolean circuits suitable for
homomorphic evaluation. Our second contribution is the automatic
parallelization of these generated circuits. We implement an evaluator that
executes gates in parallel by parallelizing each layer of the circuit. We
demonstrate the effectiveness of our approach on two key applications: Private
Information Retrieval (PIR) and the AES encryption standard. Prior work has
parallelized AES encryption manually. We demonstrate that the automated method
outperforms some but not all manual parallelizations of AES evaluations under
FHE. We achieve an evaluation time of 28 seconds for a parallel execution with
16 threads and an evaluation time of 8 seconds for a parallel execution with
100 threads

</details>


### [35] [ExPrESSO: Zero-Knowledge backed Extensive Privacy Preserving Single Sign-on](https://arxiv.org/abs/2510.08355)
*Kaustabh Barman,Fabian Piper,Sanjeet Raj Pandey,Axel Kuepper*

Main category: cs.CR

TL;DR: 本文提出一种基于零知识证明的机制，集成到 OIDC 单点登录中，使用户能证明自己订阅某服务提供商的成员身份而不泄露服务提供商身份，从而防止身份提供者追踪用户活动。系统采用 Groth 的 zk-SNARK，并通过去中心化、可验证的预置步骤建立信任，宣称在高安全目标下实现低存储和低延迟，具备生产可采纳性。


<details>
  <summary>Details</summary>
Motivation: 标准化的 SSO（如 OIDC）便于认证但存在隐私问题：身份提供者可追踪用户访问的服务。需要一种能在保持 OIDC 兼容性的同时保护用户不被身份提供者或第三方跟踪的方法。

Method: 使用 Groth 的 zk-SNARK 构造零知识证明，证明用户属于某个已订阅服务提供商集合而不暴露具体提供者身份。通过去中心化且可验证的流程来生成或管理构造所需的公共参数/预置数据，以减少或消除中心化信任。强调将该机制嵌入 OIDC 流程中并优化以降低存储与延迟开销。

Result: 作者宣称在满足高安全性目标的前提下，系统实现了最小存储与低延迟开销，证明了可用于生产环境（论文摘要未给出具体数值）。

Conclusion: 提出的方案在理论上增强了基于 OIDC 的 SSO 的隐私保护，可在保证安全性的同时实现实用性，但需要补充实现细节与实验数据以评估真实可行性与扩展性。

Abstract: User authentication is one of the most important aspects for secure
communication between services and end-users over the Internet. Service
providers leverage Single-Sign On (SSO) to make it easier for their users to
authenticate themselves. However, standardized systems for SSO, such as OIDC,
do not guarantee user privacy as identity providers can track user activities.
We propose a zero-knowledge-based mechanism that integrates with OIDC to let
users authenticate through SSO without revealing information about the service
provider. Our system leverages Groth's zk-SNARK to prove membership of
subscribed service providers without revealing their identity. We adopt a
decentralized and verifiable approach to set up the prerequisites of our
construction that further secures and establishes trust in the system. We set
up high security targets and achieve them with minimal storage and latency
cost, proving that our research can be adopted for production.

</details>


### [36] [Rethinking Provenance Completeness with a Learning-Based Linux Scheduler](https://arxiv.org/abs/2510.08479)
*Jinsong Mao,Benjamin E. Ujcich,Shiqing Ma*

Main category: cs.CR

TL;DR: 本文提出Venus，一种基于强化学习的Linux调度器，用于提高审计溯源（provenance）收集的完整性与效率，通过动态优化资源分配以应对“超级生成器威胁”，实验表明在完整性和效率上均优于传统调度器且开销可接受。


<details>
  <summary>Details</summary>
Motivation: 现有将溯源收集嵌入参考监视器的方法面临“超级生成器威胁”：恶意或高负载的证明生成行为可以使系统过载导致丢弃安全相关事件，从而破坏审计完整性。简单的资源隔离并不能从根本上解决硬件依赖与性能限制问题，需更智能的调度手段保证事件不被丢弃。

Method: 作者设计并实现了Venus——一个面向溯源码收集的学得调度器。核心思想是利用强化学习学习溯源相关任务的行为模式，动态调整资源分配以优先保证溯源任务的完成度。Venus替换或扩展了Linux内核调度器，通过在线或离线训练策略对任务进行调度决策以优化溯源完整性与系统性能指标。

Result: 在作者的评估中，Venus在提高溯源事件完整性和采集效率方面显著优于传统调度策略，同时在合理的开销下维持系统性能；在若干场景下甚至将整体运行时优于默认Linux调度器。

Conclusion: 通过将强化学习应用于内核调度，Venus能主动缓解超级生成器威胁，改进溯源数据的完整性与收集效率，为安全审计系统提供了一条实用的调度层面解决方案。

Abstract: Provenance plays a critical role in maintaining traceability of a system's
actions for root cause analysis of security threats and impacts. Provenance
collection is often incorporated into the reference monitor of systems to
ensure that an audit trail exists of all events, that events are completely
captured, and that logging of such events cannot be bypassed. However, recent
research has questioned whether existing state-of-the-art provenance collection
systems fail to ensure the security guarantees of a true reference monitor due
to the 'super producer threat' in which provenance generation can overload a
system to force the system to drop security-relevant events and allow an
attacker to hide their actions. One approach towards solving this threat is to
enforce resource isolation, but that does not fully solve the problems
resulting from hardware dependencies and performance limitations.
  In this paper, we show how an operating system's kernel scheduler can
mitigate this threat, and we introduce Venus, a learned scheduler for Linux
specifically designed for provenance. Unlike conventional schedulers that
ignore provenance completeness requirements, Venus leverages reinforcement
learning to learn provenance task behavior and to dynamically optimize resource
allocation. We evaluate Venus's efficacy and show that Venus significantly
improves both the completeness and efficiency of provenance collection systems
compared to traditional scheduling, while maintaining reasonable overheads and
even improving overall runtime in certain cases compared to the default Linux
scheduler.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [37] [Autoencoding Coordinate Sequences from Psychophysiologic Signals](https://arxiv.org/abs/2510.07415)
*Timothy L. Hutcheson,Anil K. Raj*

Main category: eess.SP

TL;DR: 提出了一种将24通道生理时序数据（EEG、ECG、EDA、呼吸率）转换为可跟踪的三维坐标的方法，用于估计个体在特定任务和认知状态下的参与度。


<details>
  <summary>Details</summary>
Motivation: 希望将多模态生理信号压缩为低维、可跟踪的表示，以便实时或离线识别任务参与和认知状态，便于可视化、分析和下游推断。

Method: 从24通道时序数据提取特征并映射到三维坐标（摘要中未说明具体算法），可能涉及预处理、特征工程或降维/嵌入技术，使得每个时间点对应一个3D位置。

Result: 作者声称得到的3D坐标足以估计参与特定任务和认知状态，但摘要未给出定量验证、数据集规模、分类/回归性能或跨被试泛化能力的细节。

Conclusion: 方法在将多通道生理数据映射为低维、可跟踪表示方面具有潜力，但摘要信息不足以评估其有效性和鲁棒性，需要更多方法细节和实验验证。

Abstract: We present a method for converting 24 channels of psychophysiologic time
series data collected from individual participants via electroencephalogram
(EEG), electrocardiogram (ECG), electrodermal activity (EDA), respiration rate
(RR) into trackable three dimensional (3D) coordinates sufficient to estimate
participation in specific task and cognitive states.

</details>


### [38] [Flexible Intelligent Metasurface for Reconfiguring Radio Environments](https://arxiv.org/abs/2510.07466)
*Hanwen Hu,Jiancheng An,Lu Gan,Naofal Al-Dhahir*

Main category: eess.SP

TL;DR: 本文提出并研究了可形变透射式柔性智能超表面（FIM）在SISO和MISO通信中的联合相位与表面形变优化方法，通过分解与交替优化（包含PSO、MIGD和交替优化算法）显著提升端到端信道增益，相较传统刚性RIS在多径环境下表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统刚性RIS只能固定形状，限制了对复杂多径信道的适应性和性能提升空间；引入可沿法向可调形状的FIM可通过改变反射/透射几何获得更佳信道条件，从而提高频谱和能量效率。

Method: SISO：对每一候选曲面形状先解析求得最优相位矩阵，将问题降为一维形变优化，使用粒子群（PSO）和多区间梯度下降（MIGD）搜索最优形状；MISO：联合优化发射波束、相位矩阵和表面形变，采用高效交替优化（AO）分块求解各子问题迭代更新。

Result: 仿真显示FIM在端到端信道增益上显著优于传统刚性RIS，对多径信道有较好适应性；所提算法在可行的计算开销下收敛到性能优良的解。

Conclusion: 引入表面形变作为新的自由度能显著改善基于超表面的通信性能；文中提出的分解与优化框架为FIM系统的工程实现提供了可行路径，并在SISO/MISO场景验证了其有效性。

Abstract: Flexible intelligent metasurface (FIM) technology holds immense potential for
increasing the spectral efficiency and energy efficiency of wireless networks.
In contrast to traditional rigid reconfigurable intelligent surfaces (RIS), an
FIM consists of an array of elements, each capable of independently tuning
electromagnetic signals, while flexibly adjusting its position along the
direction perpendicular to the surface. In contrast to traditional rigid
metasurfaces, FIM is capable of morphing its surface shape to attain better
channel conditions. In this paper, we investigate the single-input
single-output (SISO) and multiple-input single-output (MISO) communication
systems aided by a transmissive FIM. In the SISO scenario, we jointly optimize
the FIM phase shift matrix and surface shape to maximize the end-to-end channel
gain. First, we derive the optimal phase-shift matrix for each tentative FIM
surface shape to decompose the high-dimensional non-convex optimization problem
into multiple one-dimensional subproblems. Then, we utilize the particle swarm
optimization (PSO) algorithm and the multi-interval gradient descent (MIGD)
method for updating the FIM's surface shape to maximize the channel gain. In
the MISO scenario, we jointly optimize the transmit beamforming, the FIM
surface shape, and the phase shift matrix to maximize the channel gain. To
tackle this complex problem with multiple highly coupled variables, an
efficient alternating optimization algorithm is proposed. Simulation results
demonstrate that FIM significantly improves channel gain compared to
traditional RIS and exhibits good adaptability to multipath channels.

</details>


### [39] [Time-Frequency Filtering Meets Graph Clustering](https://arxiv.org/abs/2510.07503)
*Marcelo A. Colominas,Stefan Steinerberger,Hau-Tieng Wu*

Main category: eess.SP

TL;DR: 将时频表示中的信号分量识别问题等价地表述为图聚类问题：把时频点或原子视为图的结点，依据相似性构造带权边，再用图聚类/社区发现算法分离成分。数值实验验证了该思路的可行性并提出了多种实现路径。


<details>
  <summary>Details</summary>
Motivation: 在时频域中，多个信号分量可能重叠或耦合，传统分离方法在复杂场景下效果受限。图聚类是研究成熟的分组/社区发现问题，将其引入可以利用现有图算法和理论来识别结构化的时频成分。

Method: 先自时频表示（如短时傅里叶、Wigner或字典稀疏表示）构造图：节点代表时频采样点或字典原子，边按时频相邻性、相位/幅度一致性或内积相似度加权。然后应用图切分/谱聚类、模块度优化或图正则化等聚类方法来提取强连通的子图作为信号成分。作者还讨论了多种权重与正则项的设计并给出实现细节和数值实验。

Result: 证明了两类问题的等价性，给出若干基于图聚类的分解方法，并通过数值实验展示了这些方法在若干示例信号上的有效性（如能分离重叠分量、抑制噪声影响）。

Conclusion: 把时频分量识别转为图聚类是一种有力的统一视角，能引入丰富的图算法和理论，扩展了信号分离方法库，并为进一步理论分析和工程实现提供了新方向。

Abstract: We show that the problem of identifying different signal components from a
time-frequency representation can be equivalently phrased as a graph clustering
problem: given a graph $G=(V,E)$ one aims to identify `clusters', subgraphs
that are strongly connected and have relatively few connections between them.
The graph clustering problem is well studied, we show how these ideas can
suggest (many) new ways to identify signal components. Numerical experiments
illustrate the ideas.

</details>


### [40] [Rate Maximization for UAV-assisted ISAC System with Fluid Antennas](https://arxiv.org/abs/2510.07668)
*Xingtao Yang,Zhenghe Guo,Siyun Liang,Zhaohui Yang,Chen Zhu,Zhaoyang Zhang*

Main category: eess.SP

TL;DR: 提出在具备流体天线(FA)的融合感知与通信(ISAC)系统中，基站(BS)与无人机(UAV)联合感知的优化框架，目标是在保证联合感知能力的前提下最大化BS与UAV之间的通信速率；通过建立具凸性或可分解性的通信-感知模型，采用凸优化与交替迭代方法求解，仿真验证算法在权衡通信与感知性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前ISAC系统中单一节点感知能力受限，利用UAV的感知系统可以补强地面BS的感知性能；同时在有限资源与约束下需要在通信速率与感知性能之间进行权衡与优化，尤其在引入流体天线等新型硬件后，系统设计更复杂，需要新的优化方法。

Method: 建立通信-感知联合建模，证明或利用模型的凸性与可分解性，将原始优化问题分解为若干子问题，并对关键变量逐步应用凸优化求解；设计交替优化（迭代）算法交替更新子问题的解以降低复杂度并逼近全局最优或良好次优解。

Result: 给出迭代交替优化算法并通过数值仿真实验验证：算法能在满足感知约束的同时显著提升通信速率，且相比直接求解复杂模型具有更低的计算复杂度，展示了通信与感知性能的可控权衡。

Conclusion: 提出的方法在引入流体天线的ISAC系统中能有效平衡通信与联合感知需求，所设计的分解与交替优化策略为处理类似高维约束优化问题提供了可行思路，并可推广到其他协同感知/通信场景。

Abstract: This letter investigates the joint sensing problem between unmanned aerial
vehicles (UAV) and base stations (BS) in integrated sensing and communication
(ISAC) systems with fluid antennas (FA). In this system, the BS enhances its
sensing performance through the UAV's perception system. We aim to maximize the
communication rate between the BS and UAV while guaranteeing the joint system's
sensing capability. By establishing a communication-sensing model with convex
optimization properties, we decompose the problem and apply convex optimization
to progressively solve key variables. An iterative algorithm employing an
alternating optimization approach is subsequently developed to determine the
optimal solution, significantly reducing the solution complexity. Simulation
results validate the algorithm's effectiveness in balancing system performance.

</details>


### [41] [Wideband dynamic metasurface antenna performance with practical design characteristics](https://arxiv.org/abs/2510.07827)
*Joseph M. Carlson,Nitish V. Deshpande,Miguel Rodrigo Castellanos,Robert W. Heath Jr*

Main category: eess.SP

TL;DR: 本文研究了在宽带通信中采用动态介质表面天线（DMA）的低功耗波束成形，提出了考虑波导衰减、单元频率选择性和可重构性限制的增益近似，并设计了一个逐次配置的波束成形算法以提升宽带性能；仿真表明近似与仿真模型在谱效率上匹配良好，逐次算法优于基线方法。


<details>
  <summary>Details</summary>
Motivation: DMA通过可重构辐射槽和低功耗可调组件，实现比传统相位移器更低的功耗，因而可能显著降低MIMO阵列的能耗。研究旨在评估DMA在实际宽带通信场景中的性能并量化设计变量（如带宽、波导衰减、元件频率响应和可重构性限制）对波束增益和系统谱效率的影响。

Method: 推导考虑波导衰减、单元频率选择性和调谐器可重构范围随带宽变化影响的DMA波束增益近似表达；基于近似分析给出关键设计洞见；提出一种简单的逐次波束成形算法（按顺序配置每个DMA单元）来改善宽带性能；通过LOS宽带系统的数值仿真验证近似的准确性并比较算法性能。

Result: 所得近似在谱效率指标上与仿真模型吻合良好，说明所建模型能捕捉主要物理效应；所提逐次波束成形算法相比基线DMA方法提高了整体谱效率，表明按序优化单元配置在宽带条件下能带来增益。

Conclusion: 在考虑实际设计约束下，DMA在宽带MIMO系统中具有降低功耗且保持良好波束性能的潜力；频率选择性、波导损耗和有限可重构性会限制宽带增益，但通过近似分析与逐次配置策略可缓解部分影响并提升谱效率。

Abstract: Dynamic metasurface antennas (DMA) provide low-power beamforming through
reconfigurable radiative slots. Each slot has a tunable component that consumes
low power compared to typical analog components like phase shifters. This makes
DMAs a potential candidate to minimize the power consumption of multiple-input
multiple-output (MIMO) antenna arrays. In this paper, we investigate the use of
DMAs in a wideband communication setting with practical DMA design
characteristics. We develop approximations for the DMA beamforming gain that
account for the effects of waveguide attenuation, element
frequency-selectivity, and limited reconfigurability of the tunable components
as a function of the signal bandwidth. The approximations allow for key
insights into the wideband performance of DMAs in terms of different design
variables. We develop a simple successive beamforming algorithm to improve the
wideband performance of DMAs by sequentially configuring each DMA element.
Simulation results for a line-of-sight (LOS) wideband system show the accuracy
of the approximations with the simulated DMA model in terms of spectral
efficiency. We also find that the proposed successive beamforming algorithm
increases the overall spectral efficiency of the DMA-based wideband system
compared with a baseline DMA beamforming method.

</details>


### [42] [Accelerating vRAN and O-RAN with SIMD: Architectural Perspectives and Performance Evaluation](https://arxiv.org/abs/2510.07843)
*Jaebum Park,Chan-Byoung Chae,Robert W. Heath Jr*

Main category: eess.SP

TL;DR: 本文研究如何用SIMD架构加速虚拟化/开放RAN的基带处理，展示了在可编程商用平台上明显的吞吐与能效提升，并讨论了负载平衡与硬件异构等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着RAN向虚拟化和开放化发展，需要在商用通用平台上实现实时且能效高的基带处理，以降低成本并提高部署灵活性。传统CPU单独处理难以满足时延与能效要求，因此探索利用SIMD的数据级并行性成为关键。

Method: 分析物理层关键功能（信道估计、MIMO检测、前向纠错）与SIMD并行模型的契合点；给出工程化设计要点（数据布局、向量化策略、缓存与内存访问优化、流水线与并行粒度选择）；基于原型实现并在商用CPU/SIMD单元上评估吞吐、时延与能效。

Result: 原型结果表明，针对性向量化与SIMD优化能在保持可编程性的同时，比纯CPU实现显著提高吞吐并降低能耗（论文中给出多项任务的量化提升）；性能与能效提升受工作负载特征、数据布局与SIMD宽度影响明显。

Conclusion: SIMD是实现灵活、能效兼顾且可编程的6G就绪RAN基带加速的有力工具，但仍需在异构硬件调度、动态负载均衡、可移植性与编程抽象层面解决若干开放问题。

Abstract: The evolution of radio access networks (RANs) toward virtualization and
openness creates new opportunities for flexible, cost-effective, and
high-performance deployments. Achieving real-time and energy-efficient baseband
processing on commercial off-the-shelf platforms, however, remains a critical
challenge. This article explores how single instruction multiple data (SIMD)
architectures can accelerate RAN workloads. We first outline why key
physical-layer functions, such as channel estimation, multiple-input
multiple-output (MIMO) detection, and forward error correction, are well
aligned with SIMD's data-level parallelism. We then present practical design
guidelines and prototype results, showing significant improvements in
throughput and energy efficiency compared to conventional CPU-only processing,
while retaining programmability and ease of integration. Finally, we discuss
open challenges in workload balancing and hardware heterogeneity, and highlight
the role of SIMD as an enabling technology for flexible, efficient, and
sustainable 6G-ready RANs.

</details>


### [43] [Statistical Analysis of Target Parameter Estimation Using Passive Radar](https://arxiv.org/abs/2510.07948)
*Mats Viberg,Daniele Gerosa,Tomas McKelvey,Thomas Eriksson*

Main category: eess.SP

TL;DR: 被动雷达系统中，若无法完美获取照明源(IO)波形，本文量化了该不确定性对目标参数估计的额外误差，并给出使该误差相比于监视通道的杂波与噪声误差可忽略的充分条件。


<details>
  <summary>Details</summary>
Motivation: 被动雷达利用现有的“照明源”(IO)进行目标探测，实际系统常用参考通道记录IO信号，但该参考信号不可避免含噪或失真。需要评估参考通道的不完备性如何影响最终目标参数（如距离、多普勒、角度）估计精度，从而为系统设计（如参考通道性能要求）提供理论依据。

Method: 对目标参数估计误差进行解析推导，分离出来自监视通道（杂波与噪声）和来自参考通道（IO波形未知/有误差）的误差分量。利用估计理论（例如Fisher信息或类似的误差传播分析），表达额外误差项与参考通道信噪比、样本数及波形相关性的关系，并据此导出使该项可忽略的充分条件。

Result: 得到一个额外误差项的闭式或定性表达式，表明该项与参考通道的信噪比成反比（或随参考样本数增加而减小）。从该表达式导出阈值条件：当参考通道质量（如SNR或样本数）超过某一界限时，由IO不完美性引入的误差可忽略，相比之下监视通道的杂波与噪声仍为主要误差来源。

Conclusion: 研究表明，只要参考通道满足一定的质量要求（例如足够高的SNR或足够多的采样），则对IO波形的不完全了解不会显著恶化目标参数估计精度。该结果为被动雷达参考通道设计和资源分配提供了理论指导。

Abstract: A passive radar system uses one or more so-called Illuminators of Opportunity
(IO) to detect and localize targets. In such systems, a reference channel is
often used at each receiving node to capture the transmitted IO signal, while
targets are detected using the main surveillance channel. The purpose of the
present contribution is to analyze a method for estimating the target
parameters in such a system. Specifically, we quantify the additional error
contribution due to not knowing the transmitted IO waveform perfectly. A
sufficient condition for this error to be negligible as compared to errors due
to clutter and noise in the surveillance channel is then given.

</details>


### [44] [Over-The-Air Phase Calibration of Spaceborne Phased Array for LEO Satellite Communications](https://arxiv.org/abs/2510.08011)
*Wei Zhang,Ding Chen,Bin Zhou*

Main category: eess.SP

TL;DR: Proposes an OTA phase calibration method for spaceborne phased arrays on LEO satellites that jointly estimates antenna phase errors and channel using repeated pilot transmissions; derives CRB and optimizes beam patterns to minimize RMSE; simulations show estimates approach CRB and optimized beams gain >4dB SNR over random beams.


<details>
  <summary>Details</summary>
Motivation: Spaceborne phased arrays suffer unpredictable phase deviations that impair beamforming for LEO satellite communications; on-ground calibrations are insufficient once in orbit, so an over-the-air method that accounts for unknown channels is needed.

Method: Formulate joint estimation of SPA phase errors and the unknown downlink channel using multiple pilot transmissions. Derive the Cramér–Rao bound for phase estimation accuracy. Propose beam pattern optimization to reduce estimation RMSE, likely via designing pilots/beamforming weights to improve Fisher information.

Result: Simulations demonstrate the proposed OTA calibration algorithm yields phase-estimate RMSEs close to the derived CRB. Beam-pattern optimization provides more than 4 dB SNR advantage compared to randomly generated beam patterns.

Conclusion: OTA joint estimation with optimized beam patterns effectively calibrates SPA phase deviations in LEO satellite links, achieving near-CRB performance and substantial SNR improvement over non-optimized beam patterns.

Abstract: To avoid the unpredictable phase deviations of the spaceborne phased array
(SPA), this paper considers the over-the-air (OTA) phase calibration of the SPA
for the low earth orbit (LEO) satellite communications, where the phase
deviations of the SPA and the unknown channel are jointly estimated with
multiple transmissions of the pilots. Moreover, the Cramer Rao Bound (CRB) is
derived, and the optimization of beam patterns is also presented to lower the
root mean squared error (RMSE) of the OTA calibration. The simulation results
verify the effectiveness of the proposed OTA phase calibration algorithm as the
RMSEs of the phase estimates closely approach the corresponding CRB, and the
beam pattern optimization scheme is also validated for more than 4dB gain of
SNR over the randomly generated beam patterns.

</details>


### [45] [Towards Precise Channel Knowledge Map: Exploiting Environmental Information from 2D Visuals to 3D Point Clouds](https://arxiv.org/abs/2510.08140)
*Yancheng Wang,Chuan Huang,Songyang Zhang,Guanying Chen,Wei Guo,Shenglun Lan,Lexi Xu,Xinzhou Cheng,Xiongyan Tang,Shuguang Cui*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The substantial communication resources consumed by conventional pilot-based
channel sounding impose an unsustainable overhead, presenting a critical
scalability challenge for the future 6G networks characterized by massive
channel dimensions, ultra-wide bandwidth, and dense user deployments. As a
generalization of radio map, channel knowledge map (CKM) offers a paradigm
shift, enabling access to location-tagged channel information without
exhaustive measurements. To fully utilize the power of CKM, this work
highlights the necessity of leveraging three-dimensional (3D) environmental
information, beyond conventional two-dimensional (2D) visual representations,
to construct high-precision CKMs. Specifically, we present a novel framework
that integrates 3D point clouds into CKM construction through a hybrid model-
and data-driven approach, with extensive case studies in real-world scenarios.
The experimental results demonstrate the potential for constructing precise
CKMs based on 3D environments enhanced with semantic understanding, together
with their applications in the next-generation wireless communications. We also
release a real-world dataset of measured channel paired with high-resolution 3D
environmental data to support future research and validation.

</details>


### [46] [Channel Charting based Fast Beam Tracking Design and Implementation](https://arxiv.org/abs/2510.08144)
*Jiawei Zhang,Shihan Wang,Jienan Chen,Fan Wu,Jiyun Tao,Zheqi Gu*

Main category: eess.SP

TL;DR: 本文提出一种基于信道制图（channel charting）和对比学习的低开销毫米波波束跟踪算法，通过将高维信道信息嵌入低维图空间并聚类，动态获取候选波束，显著降低波束扫描次数，同时在仿真中达到98.27%准确率并在实测中验证通信质量。


<details>
  <summary>Details</summary>
Motivation: 毫米波频段在B5G/6G中提供高带宽但对波束对准敏感，传统波束扫描开销大且在移动场景下难以维持稳定跟踪，因此需要低开销且鲁棒的波束跟踪方法。

Method: 利用对比学习构建信道制图，将高维CSI映射到保持空间邻近性的低维特征空间；将波束定位问题转化为在制图上获取波束簇；引入动态候选波束获取策略以减少扫描复杂度。

Result: 仿真中预测准确率为98.27%，相比现有方法最高可减少55.9%的波束扫描次数；并进行了实地测试，显示移动场景下通信质量良好。

Conclusion: 所提方法在保持高精度的同时显著降低了波束跟踪的扫描开销，具有很好的工程应用前景，但需更多细节与更广泛场景验证来评估鲁棒性与泛化能力。

Abstract: In the beyond fifth-generation (B5G) and upcoming sixth-generation (6G)
wireless communication systems, millimeter (mmWave) wave technology is a
promising solution for offering additional bandwidth resources and mitigating
spectrum congestion. Beam tracking is an essential procedure for providing
reliable communication services in the mmWave communication system, with the
challenge of providing consistent and accurate tracking performance. In this
study, we introduce a low-overhead beam tracking algorithm based on channel
charting, which significantly reduces beam scanning times during the tracking
process. By projecting the beam information to the channel chart, the beam
tracking problem is transformed into the acquisition of the beam cluster in the
channel chart. Leveraging contrastive learning, the proposed channel chart
projects high-dimensional channel state information into a low-dimensional
feature space that preserves spatial proximities. Using a dynamic candidate
beam acquisition strategy, the complexity of our beam tracking algorithm is
significantly reduced. The proposed algorithm significantly reduces scanning
complexity while maintaining high prediction accuracy, achieving an accuracy of
98.27\% in simulation environments. Compared to existing methods, the proposed
method can reduce beam scanning times by up to 55.9\%. In addition, we also
performed field tests, and the measured results demonstrated excellent
communication quality during mobility.

</details>


### [47] [Attitude and Heading Estimation in Symmetrical Inertial Arrays](https://arxiv.org/abs/2510.08161)
*Yaakov Libero,Itzik Klein*

Main category: eess.SP

TL;DR: 提出一种对称MIMU的无陀螺（gyro-free）AHRS方法，通过对角对称布局将线性与旋转加速度分离，结合非线性最小二乘估计和带假设检验的误差状态EKF，实测验证显示姿态误差平均降低约30%，旋转检测准确率提升超95%，稳定性显著优于标准GF实现。


<details>
  <summary>Details</summary>
Motivation: GF MIMU在小型、低功耗或长续航平台中因无陀螺器而具吸引力，但固有不稳定性和发散率高，限制其实用性。本工作旨在通过阵列排列与估计方法改进，提高GF AHRS的稳定性与精度，使其可用于陀螺不可用或受限的场景。

Method: 提出对称对角成对布置的IMU阵列以解耦线性与旋转加速度，推导相应的GF理论模型；设计非线性最小二乘用于角速度/角加速度估计；将统计假设检验嵌入误差状态扩展卡尔曼滤波器中以提升稳健性与异常检测；在实际机动的陆空平台数据集（85分钟）上进行验证。

Result: 在实测数据上，姿态估计误差平均下降约30%；旋转检测（rotation detection）准确率提升超过95%；系统稳定性与发散率显著优于传统GF实现，证明了对称MIMU配置与所提出估计/滤波策略的有效性。

Conclusion: 对称MIMU的GF AHRS方案能在无陀螺或受限情况下提供可靠的姿态估计，适用于微型平台、计算受限平台与长航时海洋平台等应用场景，但对阵列对称性、加速度计精度与同步等实现细节仍需关注与进一步评估。

Abstract: Attitude and heading reference systems (AHRS) play a central role in
autonomous navigation systems on land, air and maritime platforms. AHRS utilize
inertial sensor measurements to estimate platform orientation. In recent years,
there has been increasing interest in multiple inertial measurement units
(MIMU) arrays to improve navigation accuracy and robustness. A particularly
challenging MIMU implementation is the gyro-free (GF) configuration, in which
angular velocity is derived solely from accelerometer measurements. While the
GF configurations have multiple benefits, including outlier detection and in
angular acceleration measurements, their main drawbacks are inherent
instability and an increased divergence rate. To address these shortcomings, we
introduce a novel symmetrical MIMU formulation, in which the IMUs are arranged
in symmetric diagonal pairs to decouple linear and rotational acceleration
components. To this end, we derive the theoretical foundations for the
symmetrical MIMU formulation of the GF equations, develop a nonlinear least
squares estimation process, and integrate statistical hypothesis testing into
an AHRS error-state extended Kalman filter. We validate our approach using
real-world datasets containing 85 minutes of navigation data recorded on both
airborne and land platforms. Our results demonstrated a 30\% average reduction
in attitude estimation errors, rotation detection accuracy exceeding 95\%
improvement, and significantly improved stability compared to a standard GF
implementation. These results enable reliable GF navigation in applications
where gyroscopes are unavailable, unreliable, or energy-constrained. Common
examples include miniature platforms, computational-constraint platforms, and
long-endurance marine platforms.

</details>


### [48] [Time-causal and time-recursive wavelets](https://arxiv.org/abs/2510.05834)
*Tony Lindeberg*

Main category: eess.SP

TL;DR: 提出一种严格时因（time-causal）的连续小波分析框架，基于时间尺度空间理论，证明在保持不从细尺度到粗尺度产生新结构的约束下，级联截断指数核及其导数是唯一允许的核类；在特定时间常数选择下构造时间因极限核并以其导数作为母小波，从而实现时间尺度协变与尺度自相似性，并讨论了连续到离散实现的对应与持续时间表示能力。


<details>
  <summary>Details</summary>
Motivation: 在实时处理无法访问未来的时间序列时，必须保证信号处理流程的所有步骤真正时因，以物理上合理且不在尺度上产生假结构的方式分析不同时间尺度上的结构。

Method: 利用时间尺度空间理论对能保证不从细到粗产生新结构的时间平滑核进行完整分类，得出级联截断指数核及其导数为唯一允许的核族；通过特定选择级联中时间常数以得到时间因极限核并构建母小波；还研究连续标度性质如何映射到离散实现。

Result: 给出时间因小波表示的理论构造，建立小波理论与尺度空间理论的联系，量化连续尺度对离散实现的影响，证明该表示能够反映输入信号中局部主导时间结构的持续时间。

Conclusion: 提出的时间因小波分析为实时信号处理、跨丰富时间尺度的局部变化分析以及物理/生物物理时间现象的物理真实分析提供了有价值的工具。

Abstract: When to apply wavelet analysis to real-time temporal signals, where the
future cannot be accessed, it is essential to base all the steps in the signal
processing pipeline on computational mechanisms that are truly time-causal.
  This paper describes how a time-causal wavelet analysis can be performed
based on concepts developed in the area of temporal scale-space theory,
originating from a complete classification of temporal smoothing kernels that
guarantee non-creation of new structures from finer to coarser temporal scale
levels. By necessity, convolution with truncated exponential kernels in cascade
constitutes the only permissable class of kernels, as well as their temporal
derivatives as a natural complement to fulfil the admissibility conditions of
wavelet representations. For a particular way of choosing the time constants in
the resulting infinite convolution of truncated exponential kernels, to ensure
temporal scale covariance and thus self-similarity over temporal scales, we
describe how mother wavelets can be chosen as temporal derivatives of the
resulting time-causal limit kernel.
  By developing connections between wavelet theory and scale-space theory, we
characterize and quantify how the continuous scaling properties transfer to the
discrete implementation, demonstrating how the proposed time-causal wavelet
representation can reflect the duration of locally dominant temporal structures
in the input signals.
  We propose that this notion of time-causal wavelet analysis could be a
valuable tool for signal processing tasks, where streams of signals are to be
processed in real time, specifically for signals that may contain local
variations over a rich span of temporal scales, or more generally for analysing
physical or biophysical temporal phenomena, where a fully time-causal analysis
is called for to be physically realistic.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children](https://arxiv.org/abs/2510.07320)
*Nelaka K. A. R,Peiris M. K. V,Liyanage R. P. B*

Main category: cs.LG

TL;DR: 本研究旨在通过纵向监测自闭症谱系障碍（ASD）儿童的情绪与行为模式，构建基线行为-情绪映射，为针对性技术干预与教育应用提供证据支撑。


<details>
  <summary>Details</summary>
Motivation: 当前对ASD儿童在技能形成前的细微行为与情绪特征了解不足，特别是在信息技术领域机会受限，需先识别和映射这些模式以指导后续个性化干预。

Method: 采用纵向跟踪设计，持续采集情绪与行为数据（可能包括观察记录、量表、视频/音频与生理信号等），分析行为随时间的趋势并提取特征，基于此提出针对性应用/技术辅助框架。

Result: 通过时间序列与行为模式分析，识别出若干稳定的情绪/行为基线与个体差异，发现可作为设计教育与技术辅助工具的关键需求点。

Conclusion: 提出一个以早期行为模式识别为先的循证干预路径，强调在深入理解每个孩子的情绪-行为图谱后再开展技能培训与技术辅助，以提高包容性教育效果。

Abstract: Autism Spectrum Disorder significantly influences the communication
abilities, learning processes, behavior, and social interactions of
individuals. Although early intervention and customized educational strategies
are critical to improving outcomes, there is a pivotal gap in understanding and
addressing nuanced behavioral patterns and emotional identification in autistic
children prior to skill development. This extended research delves into the
foundational step of recognizing and mapping these patterns as a prerequisite
to improving learning and soft skills. Using a longitudinal approach to monitor
emotions and behaviors, this study aims to establish a baseline understanding
of the unique needs and challenges faced by autistic students, particularly in
the Information Technology domain, where opportunities are markedly limited.
Through a detailed analysis of behavioral trends over time, we propose a
targeted framework for developing applications and technical aids designed to
meet these identified needs. Our research underscores the importance of a
sequential and evidence-based intervention approach that prioritizes a deep
understanding of each child's behavioral and emotional landscape as the basis
for effective skill development. By shifting the focus toward early
identification of behavioral patterns, we aim to foster a more inclusive and
supportive learning environment that can significantly improve the educational
and developmental trajectory of children with ASD.

</details>


### [50] [MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation](https://arxiv.org/abs/2510.07328)
*Md Zubair,Hao Zheng,Nussdorf Jonathan,Grayson W. Armstrong,Lucy Q. Shen,Gabriela Wilson,Yu Tian,Xingquan Zhu,Min Shi*

Main category: cs.LG

TL;DR: 本文提出MultiFair，一种在模态层和组层进行双层梯度调制的多模态医疗分类方法，旨在同时缓解模态间不平衡学习与人口学群体不公平问题。在两个含不同人口学分组的多模态医疗数据集上，MultiFair 优于现有多模态与公平性方法。


<details>
  <summary>Details</summary>
Motivation: 多模态医疗决策依赖来自不同来源的数据，但现有方法常见两类问题：一是不同模态学习速率/贡献不一致，导致模型偏向某些模态；二是模型在不同人口学群体上性能不均，产生不公平。两者可互相影响（某些模态在训练过程中偏向特定群体），需要同时解决。

Method: 提出“双层梯度调制”机制：在模态级和群体级分别衡量训练梯度的方向与幅度，并动态调整梯度以抑制占优模态/增强弱势模态、同时修正群体间性能差异。方法在训练过程中基于梯度信息自适应放缩或旋转更新，以协调优化目标与公平性目标的冲突。

Result: 在两个具有人口学分组的多模态医疗数据集上进行大规模实验，MultiFair 在分类性能与公平性指标（文中称优于现有多模态学习与公平学习基线）上均取得更好结果，表明该方法能同时改善准确性与群体间差距。

Conclusion: MultiFair 通过在模态和群体两个层面上动态调制梯度，实现了更平衡且更公平的多模态医疗分类。该思路可推广到其他多源数据场景，未来可扩展到更多模态、更多任务并深化理论分析。

Abstract: Medical decision systems increasingly rely on data from multiple sources to
ensure reliable and unbiased diagnosis. However, existing multimodal learning
models fail to achieve this goal because they often ignore two critical
challenges. First, various data modalities may learn unevenly, thereby
converging to a model biased towards certain modalities. Second, the model may
emphasize learning on certain demographic groups causing unfair performances.
The two aspects can influence each other, as different data modalities may
favor respective groups during optimization, leading to both imbalanced and
unfair multimodal learning. This paper proposes a novel approach called
MultiFair for multimodal medical classification, which addresses these
challenges with a dual-level gradient modulation process. MultiFair dynamically
modulates training gradients regarding the optimization direction and magnitude
at both data modality and group levels. We conduct extensive experiments on two
multimodal medical datasets with different demographic groups. The results show
that MultiFair outperforms state-of-the-art multimodal learning and fairness
learning methods.

</details>


### [51] [Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data](https://arxiv.org/abs/2510.07350)
*Aditya Chakravarty*

Main category: cs.LG

TL;DR: 本文在CropNet（2017–2022，1200+县）上评估两种模型（GNN-RNN、MMST-ViT）在真实的OOD情形下的产量预测能力。GNN-RNN在地理迁移和年际迁移上表现更稳健且训练快（14min vs 31.5h），而MMST-ViT在域内优秀但在OOD下性能急剧下降。不同农区表现差异显著，表明空间—时间对齐比模型复杂度更关键。


<details>
  <summary>Details</summary>
Motivation: 气候变化使农业系统更易受扰动，要求预测模型在地理和时间分布外也能稳健预测以保障粮食安全。需评估深度模型的真实迁移能力。

Method: 使用CropNet大样本数据，进行‘留一簇’（7个USDA区域）交叉验证和年提前预测，比较GNN-RNN与MMST-ViT的RMSE、相关性和训练成本，并分析地区间迁移误差与潜在驱动因素（气候、灌溉、光谱覆盖）。

Result: GNN-RNN在跨区迁移中保持正相关和较低RMSE，MMST-ViT域外性能大幅下降。Heartland与Northern Great Plains迁移稳定（大豆RMSE<10bu/acre），Prairie Gateway持续欠佳（RMSE>20）。GNN-RNN训练速度显著优于MMST-ViT（135×）。

Conclusion: 空间—时间对齐与域外评估协议对可部署的气候感知农业预测至关重要，单纯增加模型复杂度或数据量不足以保证泛化。

Abstract: Climate change is increasingly disrupting agricultural systems, making
accurate crop yield forecasting essential for food security. While deep
learning models have shown promise in yield prediction using satellite and
weather data, their ability to generalize across geographic regions and years -
critical for real-world deployment - remains largely untested. We benchmark two
state-of-the-art models, GNN-RNN and MMST-ViT, under realistic
out-of-distribution (OOD) conditions using the large-scale CropNet dataset
spanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-out
cross-validation across seven USDA Farm Resource Regions and year-ahead
prediction scenarios, we identify substantial variability in cross-region
transferability. GNN-RNN demonstrates superior generalization with positive
correlations under geographic shifts, while MMST-ViT performs well in-domain
but degrades sharply under OOD conditions. Regions like Heartland and Northern
Great Plains show stable transfer dynamics (RMSE less than 10 bu/acre for
soybean), whereas Prairie Gateway exhibits persistent underperformance (RMSE
greater than 20 bu/acre) across both models and crops, revealing structural
dissimilarities likely driven by semi-arid climate, irrigation patterns, and
incomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves
135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it more
viable for sustainable deployment. Our findings underscore that
spatial-temporal alignment - not merely model complexity or data scale - is key
to robust generalization, and highlight the need for transparent OOD evaluation
protocols to ensure equitable and reliable climate-aware agricultural
forecasting.

</details>


### [52] [ConCuR: Conciseness Makes State-of-the-Art Kernel Generation](https://arxiv.org/abs/2510.07356)
*Lingcheng Kong,Jiateng Wei,Hanzhang Shen,Huan Wang*

Main category: cs.LG

TL;DR: 本文提出了一个生成与筛选高质量CUDA kernel及其推理轨迹的流水线，构建了数据集ConCuR并训练首个在PyTorch-推理- CUDA对上微调的模型KernelCoder，在KernelBench上优于现有顶尖及开源模型，并提出平均推理长度作为任务难度指标。


<details>
  <summary>Details</summary>
Motivation: 高质量GPU kernel数据稀缺且多为闭源，导致无法用监督微调方法将大模型专门对齐到kernel生成任务。作者观察到简洁但信息量充足的推理轨迹可提升高性能kernel生成的鲁棒性，因而希望通过合成与筛选构建可用数据集并基于此微调模型。

Method: 设计一个生成与策划CUDA kernel及其推理轨迹的流水线，重点生成简洁的推理步骤。基于该流水线构造ConCuR数据集（包含PyTorch描述、推理轨迹、CUDA kernel三元组），并用其微调模型KernelCoder。评测时在KernelBench基准上与QwQ-32B、DeepSeek-V3.1-Think、Claude-4-sonnet及多个开源微调模型比较，同时分析平均推理长度作为难度度量。

Result: KernelCoder在KernelBench上显著优于QwQ-32B，超过所有开源针对kernel生成微调的模型，并击败部分前沿闭源模型；实验还表明较短且信息浓缩的推理轨迹有助于稳定生成高性能kernels，且平均推理长度可反映任务难度。

Conclusion: 通过合成+策划的高质量数据和推理轨迹，能显著提升模型在CUDA kernel生成任务的性能。ConCuR数据集与流水线为未来构建更好数据、研究推理轨迹质量与任务难度提供了工具和度量。

Abstract: GPU kernel generation by LLMs has recently experienced rapid development,
leveraging test-time scaling and reinforcement learning techniques. However, a
key challenge for kernel generation is the scarcity of high-quality data, as
most high-quality kernels are proprietary and not open-source. This challenge
prevents us from leveraging supervised fine-tuning to align LLMs to the kernel
generation task. To address this challenge, we develop a pipeline that
generates and curates high-quality CUDA kernels with reasoning traces,
motivated by a critical observation that concise yet informative reasoning
traces result in robust generation of high-performance kernels. Using this
pipeline, we construct our dataset ConCuR and introduce our model KernelCoder,
which is the first model trained on a curated dataset consisting of PyTorch,
reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,
our model achieves significant improvements over the existing top-performing
model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel
generation, as well as frontier models such as DeepSeek-V3.1-Think and
Claude-4-sonnet. Finally, we show that the average reasoning length can serve
as a metric to assess the difficulty of kernel generation tasks. The
observations, metrics, and our data collection and curation pipeline can help
obtain better data in the kernel generation task in the future.

</details>


### [53] [Best-of-Both Worlds for linear contextual bandits with paid observations](https://arxiv.org/abs/2510.07424)
*Nathan Boyer,Dorian Baudry,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 提出了用于线性上下文带支付观测的Best-of-Both-Worlds算法，在对抗环境下达成Θ(T^{2/3})下界最优后悔，在（被腐蚀的）随机环境下保证多对数后悔，并采用FTRL与矩阵几何重采样构造高效估计器。


<details>
  <summary>Details</summary>
Motivation: 在带有付费观测的线性上下文bandit问题中，学习者可付费观测臂的损失，需在对抗与随机两种环境下都获得良好性能（BOBW）。现有方法难以同时在所谓“困难问题”上兼顾两种情形且保持计算高效。

Method: 基于Follow-the-Regularized-Leader框架，结合矩阵几何重采样构造高效无偏或低方差的损失估计器；借鉴BOBW设计框架，针对本文设置调整算法与分析技巧以处理‘难问题’特性，给出可计算实现。

Result: 算法在对抗环境下匹配最小最大后悔Θ(T^{2/3})，在随机或被腐蚀的随机环境下保证多对数级别后悔（poly-logarithmic）。同时强调算法的计算效率。

Conclusion: 工作在理论上实现了对抗与随机两世界的统一性能保证，并提供了可行的实现路径，推动了带付费观测的线性上下文bandit在‘困难问题’情形下的研究进展。

Abstract: We study the problem of linear contextual bandits with paid observations,
where at each round the learner selects an action in order to minimize its loss
in a given context, and can then decide to pay a fixed cost to observe the loss
of any arm. Building on the Follow-the-Regularized-Leader framework with
efficient estimators via Matrix Geometric Resampling, we introduce a
computationally efficient Best-of-Both-Worlds (BOBW) algorithm for this
problem. We show that it achieves the minimax-optimal regret of
$\Theta(T^{2/3})$ in adversarial settings, while guaranteeing poly-logarithmic
regret in (corrupted) stochastic regimes. Our approach builds on the framework
from \cite{BOBWhardproblems} to design BOBW algorithms for ``hard problem'',
using analysis techniques tailored for the setting that we consider.

</details>


### [54] [Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift](https://arxiv.org/abs/2510.07509)
*Tianyu Bell Pan,Damon L. Woodard*

Main category: cs.LG

TL;DR: 提出并理论化研究了一种多模态协同训练框架，在标注稀缺和分布偏移情形下，用无标注数据和视图间一致性提升泛化能力。给出收敛性分析和新的泛化界，量化了无标注数据、视图一致性和条件独立性的各自贡献。


<details>
  <summary>Details</summary>
Motivation: 现实应用中标注数据有限且数据分布会变化，单一模态或监督式方法易过拟合。多模态协同训练通过让不同模态分类器互相监督，利用无标注数据改善模型稳健性和泛化。

Method: 从理论角度推导协同训练改善泛化的条件；证明迭代协同训练在降低分类误差方面的收敛性；构建并证明一个新的多模态协同训练泛化界，将泛化误差分解为来自无标注数据利用、视图间一致性提升和条件视图独立性保持的项。

Result: 得到一组可验证条件，表明在满足一定互补性与弱依赖假设下，无标注数据和视图一致性能显著降低误差；证明了迭代协同训练的收敛性；给出首个在多模态协同训练上下文中明确分解贡献来源的泛化界。

Conclusion: 理论结果支持多模态协同训练作为一种结构化、数据高效且对分布变化更稳健的模型训练策略，为在实际动态环境中构建泛化良好的多模态系统提供了理论依据。

Abstract: This paper explores a multimodal co-training framework designed to enhance
model generalization in situations where labeled data is limited and
distribution shifts occur. We thoroughly examine the theoretical foundations of
this framework, deriving conditions under which the use of unlabeled data and
the promotion of agreement between classifiers for different modalities lead to
significant improvements in generalization. We also present a convergence
analysis that confirms the effectiveness of iterative co-training in reducing
classification errors. In addition, we establish a novel generalization bound
that, for the first time in a multimodal co-training context, decomposes and
quantifies the distinct advantages gained from leveraging unlabeled multimodal
data, promoting inter-view agreement, and maintaining conditional view
independence. Our findings highlight the practical benefits of multimodal
co-training as a structured approach to developing data-efficient and robust AI
systems that can effectively generalize in dynamic, real-world environments.
The theoretical foundations are examined in dialogue with, and in advance of,
established co-training principles.

</details>


### [55] [Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs](https://arxiv.org/abs/2510.07429)
*Wang Wei,Tiankai Yang,Hongjie Chen,Yue Zhao,Franck Dernoncourt,Ryan A. Rossi,Hoda Eldardiry*

Main category: cs.LG

TL;DR: 提出BaRP，一种在部分反馈（bandit）设置下训练的LLM路由方法，支持在测试时通过偏好向量调节性能与成本的平衡，从而在部署环境下更接近实际可用的路由策略。


<details>
  <summary>Details</summary>
Motivation: 在大规模部署时需要为每个查询选择合适的LLM以平衡成本与准确性；但部署时只能观测所选模型的结果，传统离线带全信息标签的路由训练与现实反馈不符，导致性能下降。

Method: 将路由问题建模为带上下文（prompt特征）和用户偏好向量的上下文bandit：训练阶段模拟在线部分反馈以仅使用被选择模型的反馈信号进行学习；推理阶段通过偏好向量无须重训练即可调整性能/成本权衡。

Result: 在多组全面实验中，BaRP较强的离线路由器至少提升12.46%，较最大模型至少提升2.45%，并在未见任务上表现出良好泛化能力。

Conclusion: BaRP在现实部署约束下提供了更稳健且可调的模型路由方案，能在部分反馈条件下学到有效路由策略，从而提高整体效率与性能。

Abstract: Efficient use of large language models (LLMs) is critical for deployment at
scale: without adaptive routing, systems either overpay for strong models or
risk poor performance from weaker ones. Selecting the right LLM for each query
is fundamentally an online decision problem: models differ in strengths, prices
fluctuate, and users value accuracy and cost differently. Yet most routers are
trained offline with labels for all candidate models, an assumption that breaks
in deployment, where only the outcome of the chosen model is observed. We
bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach
that trains under the same partial-feedback restriction as deployment, while
supporting preference-tunable inference: operators can dial the
performance/cost trade-off at test time without retraining. Framed as a
contextual bandit over prompt features and a user preference vector, our method
simulates an online feedback setting during training and adapts its routing
decisions to each new prompt, rather than depending on full-information offline
supervision. Comprehensive experiments show that our method consistently
outperforms strong offline routers by at least 12.46% and the largest LLM by at
least 2.45%, and generalizes robustly for unseen tasks.

</details>


### [56] [Parameter-Free Federated TD Learning with Markov Noise in Heterogeneous Environments](https://arxiv.org/abs/2510.07436)
*Ankur Naskar,Gugan Thoppe,Utsav Negi,Vijay Gupta*

Main category: cs.LG

TL;DR: 提出了一种两步尺度的联邦时序差分学习（FTD）算法，结合Polyak–Ruppert平均化，在马尔可夫数据下无需依赖未知问题参数即可在平均奖励和折扣奖励两种设定下达到最优收敛速率~\tilde{O}(1/(N T))，并适用于环境异质的联邦场景；结果在单智能体下也具有新意。


<details>
  <summary>Details</summary>
Motivation: 联邦学习可通过多智能体并行探索与训练把强化学习的收敛速度按智能体数线性加速，但在样本由马尔可夫链产生时，已有的时序差分（TD）方法要实现最优速率通常需要依赖未知的问题相关参数（如混合时间等），这在实际应用中不可行。需要一种参数自适应、可在马尔可夫采样和异质环境下保证最优速率的联邦TD方案。

Method: 设计了一个两步尺度（two-timescale）的联邦时序差分（FTD）算法，并对迭代结果使用Polyak–Ruppert平均化以降低方差。算法在本地进行TD更新并在联邦层面进行聚合，步长采用两类尺度以分离值估计与联邦聚合的时间尺度，从而消除对未知问题参数的依赖。方法同时对平均奖励和折扣奖励的马尔可夫数据建模并给出统一处理。

Result: 理论上证明该算法在马尔可夫采样下能以最优速率\tilde{O}(1/(N T))收敛，且无需事先知道问题相关参数。该结果不仅扩展到异质联邦环境，也在单智能体情形下带来新的最速率保证。

Conclusion: 提出的参数无关的两步尺度FTD+Polyak–Ruppert方案为在马尔可夫数据和异质联邦设置下进行时序差分学习提供了可行且有理论最优性保证的做法。后续可考虑放宽模型假设、扩展到更复杂的函数逼近或非线性策略评估场景。

Abstract: Federated learning (FL) can dramatically speed up reinforcement learning by
distributing exploration and training across multiple agents. It can guarantee
an optimal convergence rate that scales linearly in the number of agents, i.e.,
a rate of $\tilde{O}(1/(NT)),$ where $T$ is the iteration index and $N$ is the
number of agents. However, when the training samples arise from a Markov chain,
existing results on TD learning achieving this rate require the algorithm to
depend on unknown problem parameters. We close this gap by proposing a
two-timescale Federated Temporal Difference (FTD) learning with Polyak-Ruppert
averaging. Our method provably attains the optimal $\tilde{O}(1/NT)$ rate in
both average-reward and discounted settings--offering a parameter-free FTD
approach for Markovian data. Although our results are novel even in the
single-agent setting, they apply to the more realistic and challenging scenario
of FL with heterogeneous environments.

</details>


### [57] [On the optimization dynamics of RLVR: Gradient gap and step size thresholds](https://arxiv.org/abs/2510.08539)
*Joe Suk,Yaqi Duan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple
binary feedback to post-train large language models, has shown significant
empirical success. However, a principled understanding of why it works has been
lacking. This paper builds a theoretical foundation for RLVR by analyzing its
training process at both the full-response (trajectory) and token levels.
Central to our analysis is a quantity called the Gradient Gap, which formalizes
the direction of improvement from low-reward to high-reward regions of the
response space. We prove that convergence critically depends on aligning the
update direction with this Gradient Gap. Moreover, we derive a sharp step-size
threshold based on the magnitude of the Gradient Gap: below it, learning
converges, whereas above it, performance collapses. Our theory further predicts
how the critical step size must scale with response length and the success
rate, thereby explaining why practical heuristics such as length normalization
improve stability and showing that, with a fixed learning rate, the success
rate can stagnate strictly below $100\%$. We validate these predictions through
controlled bandit simulations and LLM experiments, including training
Qwen2.5-7B with GRPO.

</details>


### [58] [Surrogate Modeling for the Design of Optimal Lattice Structures using Tensor Completion](https://arxiv.org/abs/2510.07474)
*Shaan Pakala,Aldair E. Gongora,Brian Giera,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 论文提出在训练数据由非均匀采样（有偏采样）时，用张量补全作为替代模型来加速晶格材料的力学性能设计，并在有偏与均匀采样情景下与高斯过程和XGBoost比较，发现有偏采样下R²提升约5%，在均匀采样下性能相当。


<details>
  <summary>Details</summary>
Motivation: 材料设计空间随着设计变量增加呈指数增长，实验验证昂贵且可能存在有偏采样（实验更频繁合成某些材料），导致经典监督学习表现受限；因此需要能在有偏观测上仍可靠预测的替代模型。

Method: 将材料设计空间和性能数据组织为多维张量，使用张量补全技术从部分条目推断完整性能表面，作为替代模型用于搜索最优晶格结构；并与高斯过程与XGBoost在不同采样偏差条件下比较。

Result: 在有偏采样设置下，张量补全相比高斯过程和XGBoost在R²上约提升5%；在均匀随机采样情况下，张量补全表现与这两种方法相当。

Conclusion: 张量补全在训练数据分布偏离均匀随机时对材料性能预测更鲁棒，适合作为加速材料设计的替代模型。

Abstract: When designing new materials, it is often necessary to design a material with
specific desired properties. Unfortunately, as new design variables are added,
the search space grows exponentially, which makes synthesizing and validating
the properties of each material very impractical and time-consuming. In this
work, we focus on the design of optimal lattice structures with regard to
mechanical performance. Computational approaches, including the use of machine
learning (ML) methods, have shown improved success in accelerating materials
design. However, these ML methods are still lacking in scenarios when training
data (i.e. experimentally validated materials) come from a non-uniformly random
sampling across the design space. For example, an experimentalist might
synthesize and validate certain materials more frequently because of
convenience. For this reason, we suggest the use of tensor completion as a
surrogate model to accelerate the design of materials in these atypical
supervised learning scenarios. In our experiments, we show that tensor
completion is superior to classic ML methods such as Gaussian Process and
XGBoost with biased sampling of the search space, with around 5\% increased
$R^2$. Furthermore, tensor completion still gives comparable performance with a
uniformly random sampling of the entire search space.

</details>


### [59] [Estimating Fair Graphs from Graph-Stationary Data](https://arxiv.org/abs/2510.07536)
*Madeline Navarro,Andrei Buciulea,Samuel Rey,Antonio G. Marques,Santiago Segarra*

Main category: cs.LG

TL;DR: 本文提出 Fair Spectral Templates (FairSpecTemp)，从服从图平稳性的节点观测中估计“公平”图。提出多种偏差度量（包括谱域新指标），并给出两种基于优化的算法变体：一者利用平稳性下的对易性并直接约束偏差，另一者通过限制图谱隐式鼓励公平。证明了带概率的性能界，揭示了公平与准确性的条件性权衡，并在合成与真实数据上验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现实图中边的连接往往对某些群体有偏好，这会在下游图任务中放大或引入不公平。需要从观测数据中重构或估计不带偏见的图，以满足群体和个体层面的公平性定义。

Method: 提出若干图公平性度量（含谱域指标），并从图平稳的信号建模出发，设计两种 FairSpecTemp 算法：一是基于对易性质直接约束偏差；二是通过谱约束隐式降低偏差（更灵活）。给出高概率的性能界，分析公平-准确性的权衡条件。

Result: 理论上给出误差与公平性的界限，表明在某些条件下无需牺牲准确性即可恢复公平图。实验（合成与真实数据）显示两种变体在降低偏差同时保持或接近原有性能，且各有优势。

Conclusion: FairSpecTemp 为从平稳图信号估计公平图提供了可行且有理论保障的方法，兼顾群体与个体公平性，并在实证上表现良好。

Abstract: We estimate fair graphs from graph-stationary nodal observations such that
connections are not biased with respect to sensitive attributes. Edges in
real-world graphs often exhibit preferences for connecting certain pairs of
groups. Biased connections can not only exacerbate but even induce unfair
treatment for downstream graph-based tasks. We therefore consider group and
individual fairness for graphs corresponding to group- and node-level
definitions, respectively. To evaluate the fairness of a given graph, we
provide multiple bias metrics, including novel measurements in the spectral
domain. Furthermore, we propose Fair Spectral Templates (FairSpecTemp), an
optimization-based method with two variants for estimating fair graphs from
stationary graph signals, a general model for graph data subsuming many
existing ones. One variant of FairSpecTemp exploits commutativity properties of
graph stationarity while directly constraining bias, while the other implicitly
encourages fair estimates by restricting bias in the graph spectrum and is thus
more flexible. Our methods enjoy high probability performance bounds, yielding
a conditional tradeoff between fairness and accuracy. In particular, our
analysis reveals that accuracy need not be sacrificed to recover fair graphs.
We evaluate FairSpecTemp on synthetic and real-world data sets to illustrate
its effectiveness and highlight the advantages of both variants of
FairSpecTemp.

</details>


### [60] [HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data](https://arxiv.org/abs/2510.07477)
*Maria Mahbub,Robert J. Klein,Myvizhi Esai Selvan,Rowena Yip,Claudia Henschke,Providencia Morales,Ian Goethert,Olivera Kotevska,Mayanka Chandra Shekar,Sean R. Wilkinson,Eileen McAllister,Samuel M. Aguayo,Zeynep H. Gümüş,Ioana Danciu,VA Million Veteran Program*

Main category: cs.LG

TL;DR: 该论文提出HEMERA——一种基于可解释Transformer的深度学习框架，直接以原始SNP基因型输入预测肺癌风险，并使用Layer-wise Integrated Gradients对SNP进行归因。作者在27,254名Million Veteran Program参与者上训练，报告了>99% AUC。


<details>
  <summary>Details</summary>
Motivation: 肺癌有明显遗传成分且GWAS已发现多个风险位点，作者试图用端到端可解释深度模型从基因型直接预测个体肺癌风险，以期用于个性化风险评估和早期干预。

Method: 提出对基因型的神经嵌入、加性位置编码和变异过滤，使用Transformer架构进行预测；训练后用Layer-wise Integrated Gradients进行后验归因以识别重要SNP。没有加入临床协变量，训练数据来自MVP。

Result: 在27,254名MVP样本上模型训练得到>99% AUC，并且模型归因与已知肺癌风险基因座高度一致。

Conclusion: 作者认为HEMERA为透明、可生成假设的个体肺癌风险评估提供了一种可行方法，支持早期干预。

Abstract: Lung cancer (LC) is the third most common cancer and the leading cause of
cancer deaths in the US. Although smoking is the primary risk factor, the
occurrence of LC in never-smokers and familial aggregation studies highlight a
genetic component. Genetic biomarkers identified through genome-wide
association studies (GWAS) are promising tools for assessing LC risk. We
introduce HEMERA (Human-Explainable Transformer Model for Estimating Lung
Cancer Risk using GWAS Data), a new framework that applies explainable
transformer-based deep learning to GWAS data of single nucleotide polymorphisms
(SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly
processes raw genotype data without clinical covariates, introducing additive
positional encodings, neural genotype embeddings, and refined variant
filtering. A post hoc explainability module based on Layer-wise Integrated
Gradients enables attribution of model predictions to specific SNPs, aligning
strongly with known LC risk loci. Trained on data from 27,254 Million Veteran
Program participants, HEMERA achieved >99% AUC (area under receiver
characteristics) score. These findings support transparent,
hypothesis-generating models for personalized LC risk assessment and early
intervention.

</details>


### [61] [Reinforcement Learning-based Task Offloading in the Internet of Wearable Things](https://arxiv.org/abs/2510.07487)
*Waleed Bin Qaim,Aleksandr Ometov,Claudia Campolo,Antonella Molinaro,Elena Simona Lohan,Jari Nurmi*

Main category: cs.LG

TL;DR: 本文提出一种基于强化学习（Q-learning）的任务卸载框架，用于物联网可穿戴设备（IoWT）在边缘设备上卸载计算任务，以在能耗与任务完成时间之间取得权衡，并通过ns-3仿真评估了不同参数设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备受限于电池与计算资源；同时出现大量延迟敏感和计算密集型应用，迫切需要利用附近边缘设备的资源来提升用户体验，因此研究自适应的任务卸载策略具有实用价值。

Method: 将任务卸载建模为马尔可夫决策过程（MDP），采用无模型的Q-learning算法使可穿戴设备在无先验知识下学习最优卸载策略，目标在能耗与任务完成时间间进行权衡；使用ns-3进行大量仿真实验，考察不同应用与系统配置以及Q-learning主要参数的影响。

Result: 仿真结果展示了Q-learning框架在不同设置下的性能表现（平均任务完成时间、平均能耗、卸载任务比例），并分析了Q-learning参数变化对上述指标的影响，表明该方法能在能耗与时延之间取得可控的折中。

Conclusion: Q-learning可用于IoWT场景的在线任务卸载决策，在无先验知识时能学习到合理策略；算法参数和系统配置会显著影响能耗与时延的折中，需针对场景调整或采用更先进的强化学习方法以提高收敛性与扩展性。

Abstract: Over the years, significant contributions have been made by the research and
industrial sectors to improve wearable devices towards the Internet of Wearable
Things (IoWT) paradigm. However, wearables are still facing several challenges.
Many stem from the limited battery power and insufficient computation resources
available on wearable devices. On the other hand, with the popularity of smart
wearables, there is a consistent increase in the development of new
computationally intensive and latency-critical applications. In such a context,
task offloading allows wearables to leverage the resources available on nearby
edge devices to enhance the overall user experience. This paper proposes a
framework for Reinforcement Learning (RL)-based task offloading in the IoWT. We
formulate the task offloading process considering the tradeoff between energy
consumption and task accomplishment time. Moreover, we model the task
offloading problem as a Markov Decision Process (MDP) and utilize the
Q-learning technique to enable the wearable device to make optimal task
offloading decisions without prior knowledge. We evaluate the performance of
the proposed framework through extensive simulations for various applications
and system configurations conducted in the ns-3 network simulator. We also show
how varying the main system parameters of the Q-learning algorithm affects the
overall performance in terms of average task accomplishment time, average
energy consumption, and percentage of tasks offloaded.

</details>


### [62] [PEAR: Planner-Executor Agent Robustness Benchmark](https://arxiv.org/abs/2510.07505)
*Shen Dong,Mingxuan Zhang,Pengfei He,Li Ma,Bhavani Thuraisingham,Hui Liu,Yue Xing*

Main category: cs.LG

TL;DR: 本文提出了PEAR基准来系统评估基于LLM的计划者-执行者多智能体系统的效能与脆弱性，通过大量实验揭示了计划者与执行者在干净性能与对抗攻击下的差异及取舍，提出针对性防御方向。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦孤立攻击面或特定场景，缺乏对多智能体系统（尤其实用的计划者-执行者结构）整体脆弱性的系统性理解；因此需要统一基准来量化效能与安全性并指导防御设计。

Method: 设计PEAR基准，兼容多种MAS架构但聚焦于计划者-执行者结构；通过控制计划者/执行者强弱、是否具备记忆模块，以及多类攻击面，进行大规模对比实验，评估干净任务性能与在对抗条件下的鲁棒性。

Result: 主要发现包括：1) 计划者薄弱会比执行者薄弱更显著地降低整体干净任务性能；2) 计划者的记忆模块对干净性能至关重要，而执行者的记忆模块影响有限；3) 性能与鲁棒性存在权衡，即提升性能可能降低鲁棒性；4) 针对计划者的攻击更易误导整个系统。

Conclusion: PEAR为量化LLM驱动多智能体系统的效能与脆弱性提供了工具与实证证据；研究表明应优先强化计划者的能力与防护、合理设计记忆机制并在性能与鲁棒性之间做出平衡，为后续防御策略与更安全的MAS设计提供了方向。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a
powerful paradigm for tackling complex, multi-step tasks across diverse
domains. However, despite their impressive capabilities, MAS remain susceptible
to adversarial manipulation. Existing studies typically examine isolated attack
surfaces or specific scenarios, leaving a lack of holistic understanding of MAS
vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for
systematically evaluating both the utility and vulnerability of
planner-executor MAS. While compatible with various MAS architectures, our
benchmark focuses on the planner-executor structure, which is a practical and
widely adopted design. Through extensive experiments, we find that (1) a weak
planner degrades overall clean task performance more severely than a weak
executor; (2) while a memory module is essential for the planner, having a
memory module for the executor does not impact the clean task performance; (3)
there exists a trade-off between task performance and robustness; and (4)
attacks targeting the planner are particularly effective at misleading the
system. These findings offer actionable insights for enhancing the robustness
of MAS and lay the groundwork for principled defenses in multi-agent settings.

</details>


### [63] [MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis](https://arxiv.org/abs/2510.07513)
*Qinghua Liu,Sam Heshmati,Zheda Mai,Zubin Abraham,John Paparrizos,Liu Ren*

Main category: cs.LG

TL;DR: 提出一种将多变量时间序列渲染为复合可视图并融入多模态大模型的框架（MLLM4TS），通过横向叠置的彩色折线图和时序感知的视觉补丁对齐，弥合连续数值与离散语言之间的模态差距，从而在分类、异常检测和预测等任务上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 人类通过可视化快速发现时间序列中的模式与跨通道关系。现有多模态大语言模型具备强大的视觉与语言理解能力，但难以直接处理连续数值时间序列。论文旨在通过视觉化表示把时间序列带入多模态大模型，利用其通用性与预训练能力提升时间序列分析的鲁棒性与泛化。

Method: （1）将每个时间序列通道绘制为彩色折线并横向堆叠到一张复合图像以保留跨通道空间关系；（2）设计时序感知的视觉补丁对齐策略，把图像补丁映射到对应的时间段以保持细粒度的时间信息；（3）把视觉分支与多模态大语言模型融合，将来自数值的细粒度时序细节与视觉全局上下文结合，用于预测与生成任务。

Result: 在标准基准上进行广泛实验，MLLM4TS在分类（预测类）和生成类任务（异常检测、预测/预测生成）上均表现出有效性，显示出视觉化输入结合预训练语言模型能带来稳健且可泛化的时间序列分析能力。

Conclusion: 通过把时间序列转为结构化视觉表示并与多模态大模型融合，能有效弥合模态差距，提升时序任务的性能与通用性，表明将可视化模态引入预训练语言模型是可行且有前景的方向。

Abstract: Effective analysis of time series data presents significant challenges due to
the complex temporal dependencies and cross-channel interactions in
multivariate data. Inspired by the way human analysts visually inspect time
series to uncover hidden patterns, we ask: can incorporating visual
representations enhance automated time-series analysis? Recent advances in
multimodal large language models have demonstrated impressive generalization
and visual understanding capability, yet their application to time series
remains constrained by the modality gap between continuous numerical data and
discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel
framework that leverages multimodal large language models for general
time-series analysis by integrating a dedicated vision branch. Each time-series
channel is rendered as a horizontally stacked color-coded line plot in one
composite image to capture spatial dependencies across channels, and a
temporal-aware visual patch alignment strategy then aligns visual patches with
their corresponding time segments. MLLM4TS fuses fine-grained temporal details
from the numerical data with global contextual information derived from the
visual representation, providing a unified foundation for multimodal
time-series analysis. Extensive experiments on standard benchmarks demonstrate
the effectiveness of MLLM4TS across both predictive tasks (e.g.,
classification) and generative tasks (e.g., anomaly detection and forecasting).
These results underscore the potential of integrating visual modalities with
pretrained language models to achieve robust and generalizable time-series
analysis.

</details>


### [64] [EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning](https://arxiv.org/abs/2510.07524)
*Mehdi Zekriyapanah Gashti,Ghasem Farjamnia*

Main category: cs.LG

TL;DR: 提出一种基于连续小波变换（CWT）的时频表示与集成学习相结合的自动睡眠分期框架，在Sleep-EDF数据集上实现88.37%准确率和73.15的宏平均F1，优于传统机器学习并能与深度学习方法相媲美。


<details>
  <summary>Details</summary>
Motivation: 人工打分耗时且主观，传统时域/频域特征可能无法同时捕捉瞬时和振荡性模式，需一种既可解释又稳健的自动化睡眠分期方法。

Method: 对EEG应用CWT生成时频图以表征不同睡眠相关频段的瞬态与振荡特征；基于这些时频表示提取特征并采用集成学习分类器进行睡眠分期；在Sleep-EDF（cassette录音）上进行评估。

Result: 在Sleep-EDF数据集上，所提方法达到总体准确率88.37%和宏平均F1 73.15%，优于传统机器学习方法，并与近期深度学习方法持平或更佳。

Conclusion: 基于小波的时频表示结合集成学习在可解释性与性能间取得平衡，适合临床场景；后续可扩展到更大/多通道数据、端到端模型与进一步的模型可解释性研究。

Abstract: Accurate classification of sleep stages is crucial for the diagnosis and
management of sleep disorders. Conventional approaches for sleep scoring rely
on manual annotation or features extracted from EEG signals in the time or
frequency domain. This study proposes a novel framework for automated sleep
stage scoring using time-frequency analysis based on the wavelet transform. The
Sleep-EDF Expanded Database (sleep-cassette recordings) was used for
evaluation. The continuous wavelet transform (CWT) generated time-frequency
maps that capture both transient and oscillatory patterns across frequency
bands relevant to sleep staging. Experimental results demonstrate that the
proposed wavelet-based representation, combined with ensemble learning,
achieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of
73.15, outperforming conventional machine learning methods and exhibiting
comparable or superior performance to recent deep learning approaches. These
findings highlight the potential of wavelet analysis for robust, interpretable,
and clinically applicable sleep stage classification.

</details>


### [65] [Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics](https://arxiv.org/abs/2510.07549)
*Qifan Chen,Zhongshu Xu,Jinjin Zhang,Dongbin Xiu*

Main category: cs.LG

TL;DR: 提出一种基于记忆的流映射学习（FML）构建针对性数字孪生（tDT）的数值框架：用离线从完整数字孪生获取的短轨迹片段训练模型，得到紧凑动力系统直接演化关注量（QoIs），在线可高效预测长期动力学，示例为圆柱绕流的水动力力预测，结果显示高精度并节省计算。


<details>
  <summary>Details</summary>
Motivation: 完整数字孪生（DT）通常包含高维场和昂贵的数值求解，然而工程决策常只需若干关注量（QoIs）。希望构建只对QoIs建模的目标化数字孪生（tDT），以显著降低在线计算成本，同时保持长期动力学预测能力。

Method: 使用记忆型流映射学习：通过反复运行完整DT采集短时轨迹片段，离线训练数据驱动模型（含历史记忆项）来近似QoIs的流映射；训练完成的模型为一个紧凑动力系统，在线仅演化该系统以预测长期QoIs，无需再调用完整DT。

Result: 在二维不可压缩圆柱绕流的数值实验中，将圆柱上的水动力（作为QoIs）作为目标，构建的tDT能在不显式求解流场的情况下准确预测力的长期演化，数值结果展示了良好精度和显著计算节省。

Conclusion: 所提FML构建的tDT为关注量导向的高效数字孪生提供了一条可行路径：训练离线、在线高效、适用于昂贵多尺度系统。实用推广时需关注采样策略、记忆长度、泛化能力与不确定性估计等问题。

Abstract: We present a numerical framework for constructing a targeted digital twin
(tDT) that directly models the dynamics of quantities of interest (QoIs) in a
full digital twin (DT). The proposed approach employs memory-based flow map
learning (FML) to develop a data-driven model of the QoIs using short bursts of
trajectory data generated through repeated executions of the full DT. This
renders the construction of the FML-based tDT an entirely offline computational
process. During online simulation, the learned tDT can efficiently predict and
analyze the long-term dynamics of the QoIs without requiring simulations of the
full DT system, thereby achieving substantial computational savings. After
introducing the general numerical procedure, we demonstrate the construction
and predictive capability of the tDT in a computational fluid dynamics (CFD)
example: two-dimensional incompressible flow past a cylinder. The QoIs in this
problem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs
are compact dynamical systems that evolve these forces without explicit
knowledge of the underlying flow field. Numerical results show that the tDTs
yield accurate long-term predictions of the forces while entirely bypassing
full flow simulations.

</details>


### [66] [MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation](https://arxiv.org/abs/2510.07835)
*Weisen Jiang,Sinno Jialin Pan*

Main category: cs.LG

TL;DR: MetaDefense是一种针对基于微调的LLM越狱攻击的两阶段防御框架：预生成阶段检测有害查询和生成中阶段监控部分回复以提前终止，训练模型预测查询与部分回复的有害性。实验在多种模型上显示对已见/未见攻击模板均有鲁棒性，且在良性任务上性能竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有防御无法泛化到被未知攻击模板伪装的有害查询；但在嵌入空间中，LLM能区分这类被伪装的有害查询。基于此，希望设计能在生成前与生成中都能识别并阻断有害输出的机制。

Method: 提出两阶段防御：1) 预生成防御——在响应生成前检测并阻止有害查询；2) 生成中防御——在生成过程中对部分回复进行判别并在必要时提前终止。通过专用提示词训练LLM预测查询与部分回复的有害性。

Result: 在LLaMA-2-7B、Qwen-2.5-3B-Instruct及LLaMA-3.2-3B-Instruct上，MetaDefense在对抗已见与未见攻击模板时显著优于现有防御方法，同时在良性任务上保持竞争性性能。代码已开源。

Conclusion: 用模型自身在查询与生成中进行有害性判别并提前终止，是对抗伪装越狱攻击的有效策略；但具体实现细节与泛化/成本/适应性问题仍需进一步验证。

Abstract: This paper introduces MetaDefense, a novel framework for defending against
finetuning-based jailbreak attacks in large language models (LLMs). We observe
that existing defense mechanisms fail to generalize to harmful queries
disguised by unseen attack templates, despite LLMs being capable of
distinguishing disguised harmful queries in the embedding space. Based on these
insights, we propose a two-stage defense approach: (i) pre-generation defense
that detects harmful queries before response generation begins, and (ii)
mid-generation defense that monitors partial responses during generation to
prevent outputting more harmful content. Our MetaDefense trains the LLM to
predict the harmfulness of both queries and partial responses using specialized
prompts, enabling early termination of potentially harmful interactions.
Extensive experiments across multiple LLM architectures (LLaMA-2-7B,
Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense
significantly outperforms existing defense mechanisms, achieving robust defense
against harmful queries with seen and unseen attack templates while maintaining
competitive performance on benign tasks. Code is available at
https://github.com/ws-jiang/MetaDefense.

</details>


### [67] [Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic](https://arxiv.org/abs/2510.07557)
*Abhay Bhandarkar,Gaurav Mishra,Khushi Juchani,Harsh Singhal*

Main category: cs.LG

TL;DR: 该研究使用BERTopic对lmsys-chat-1m多语言会话语料进行主题建模，提取出29个主题并分析了主题与人类偏好标签之间的关联，以期为领域针对性微调和优化提供依据。


<details>
  <summary>Details</summary>
Motivation: 理解在真实对话评价中，不同主题是否会影响用户对不同LLM输出的偏好，从而指导模型在特定领域的改进和部署策略。

Method: （补充）在多语言环境下对文本进行嵌入与降维，聚类生成主题，随后统计不同主题下的人类偏好标签以识别模型-主题对齐趋势。

Result: 提取出超过29个相对连贯的主题（如AI、编程、伦理、云基础设施等），并发现部分主题存在模型偏好差异。通过可视化展示了主题间距离和不同模型在主题上的表现分布。

Conclusion: 研究为针对性微调和领域优化提供了启示，但仍需更严格的统计检验、模型可解释性分析及对多语言嵌入和数据偏差的进一步控制来验证结论的稳健性。

Abstract: This study applies BERTopic, a transformer-based topic modeling technique, to
the lmsys-chat-1m dataset, a multilingual conversational corpus built from
head-to-head evaluations of large language models (LLMs). Each user prompt is
paired with two anonymized LLM responses and a human preference label, used to
assess user evaluation of competing model outputs. The main objective is
uncovering thematic patterns in these conversations and examining their
relation to user preferences, particularly if certain LLMs are consistently
preferred within specific topics. A robust preprocessing pipeline was designed
for multilingual variation, balancing dialogue turns, and cleaning noisy or
redacted data. BERTopic extracted over 29 coherent topics including artificial
intelligence, programming, ethics, and cloud infrastructure. We analysed
relationships between topics and model preferences to identify trends in
model-topic alignment. Visualization techniques included inter-topic distance
maps, topic probability distributions, and model-versus-topic matrices. Our
findings inform domain-specific fine-tuning and optimization strategies for
improving real-world LLM performance and user satisfaction.

</details>


### [68] [Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses](https://arxiv.org/abs/2510.08016)
*Stanisław Pawlak,Jan Dubiński,Daniel Marczak,Bartłomiej Twardowski*

Main category: cs.LG

TL;DR: 本文将后门攻击视为“任务向量”(Backdoor Vector, BV)，通过计算带后门与干净微调模型权重差得到BV，并用其研究攻击的相似性和可迁移性。提出两项新方法：可将多个后门合并以增强攻击效果的稀疏后门向量(SBV)，以及一种无需假设的轻量级防御——注入向量减除(IBVS)。实验表明SBV提升了攻击成功率，IBVS在未知威胁下也能有效缓解后门风险。


<details>
  <summary>Details</summary>
Motivation: 模型合并在大规模深度学习中应用广泛，但容易被单个带后门的微调模型感染，现有研究缺乏对后门在合并过程中的本质理解与通用防御方法。作者希望通过向量化表示揭示攻击结构并据此设计更强攻击与更通用的防御。

Method: 将后门视为任务向量：BV = 带后门微调模型权重 − 干净微调模型权重；基于BV分析后门的相似性与可迁移性。提出SBV，通过稀疏化与合并多个BV来形成更强的单一后门；提出IBVS防御，通过从合并模型中减去（或抑制）注入的BV成分，以抵消后门影响，且宣称无需关于后门的先验知识。

Result: 作者报告SBV在攻击有效性上优于先前方法，首次展示合并能用于增强后门；IBVS被证明为一种轻量且通用的防御，在不同未知后门场景下仍能显著降低攻击成功率且对主任务性能影响较小。

Conclusion: 将后门视为向量化任务带来新的理解途径，既能构造更具威胁性的攻击（SBV），也能设计出假设更少的防御（IBVS）。该思路为模型合并场景下的安全研究提供了新的方向。

Abstract: Model merging (MM) recently emerged as an effective method for combining
large deep learning models. However, it poses significant security risks.
Recent research shows that it is highly susceptible to backdoor attacks, which
introduce a hidden trigger into a single fine-tuned model instance that allows
the adversary to control the output of the final merged model at inference
time. In this work, we propose a simple framework for understanding backdoor
attacks by treating the attack itself as a task vector. $Backdoor\ Vector\
(BV)$ is calculated as the difference between the weights of a fine-tuned
backdoored model and fine-tuned clean model. BVs reveal new insights into
attacks understanding and a more effective framework to measure their
similarity and transferability. Furthermore, we propose a novel method that
enhances backdoor resilience through merging dubbed $Sparse\ Backdoor\ Vector\
(SBV)$ that combines multiple attacks into a single one. We identify the core
vulnerability behind backdoor threats in MM: $inherent\ triggers$ that exploit
adversarial weaknesses in the base model. To counter this, we propose
$Injection\ BV\ Subtraction\ (IBVS)$ - an assumption-free defense against
backdoors in MM. Our results show that SBVs surpass prior attacks and is the
first method to leverage merging to improve backdoor effectiveness. At the same
time, IBVS provides a lightweight, general defense that remains effective even
when the backdoor threat is entirely unknown.

</details>


### [69] [Automated Machine Learning for Unsupervised Tabular Tasks](https://arxiv.org/abs/2510.07569)
*Prabhant Singh,Pieter Gijsbers,Elif Ceren Gok Yildirim,Murat Onur Yildirim,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 提出LOTUS，用Optimal Transport度量无标签表格数据集间相似性，从已知数据集的表现预测在新数据集上无监督任务（异常检测、聚类）的模型选择。


<details>
  <summary>Details</summary>
Motivation: 无监督任务缺乏标签难以直接进行模型选择。作者假设：如果两个数据集的底层分布相似，那么在一个数据集上表现好的管线在另一个上也会表现好，因而可用数据集相似性来做跨数据集的模型推荐。

Method: 用Optimal Transport（OT）距离衡量数据集之间的相似性；在历史基准数据集上记录不同机器学习管线（例如用于异常检测和聚类）的表现；对目标无标签数据集，找到与其OT距离最近的历史数据集并推荐那些在近邻数据集上表现较好的管线。实现为一个统一框架，适配两类下游无监督任务。

Result: 在若干基准测试上，与多个强基线比较，LOTUS能更可靠地推荐合适的管线，表现出明显或有竞争力的提升，证明基于OT的相似性对无监督模型选择是有效的。

Conclusion: LOTUS作为面向多种无监督任务的模型选择首步方案是有前景的；但仍需注意计算开销、对预处理和超参数的敏感性，以及历史库覆盖性的限制。

Abstract: In this work, we present LOTUS (Learning to Learn with Optimal Transport for
Unsupervised Scenarios), a simple yet effective method to perform model
selection for multiple unsupervised machine learning(ML) tasks such as outlier
detection and clustering. Our intuition behind this work is that a machine
learning pipeline will perform well in a new dataset if it previously worked
well on datasets with a similar underlying data distribution. We use Optimal
Transport distances to find this similarity between unlabeled tabular datasets
and recommend machine learning pipelines with one unified single method on two
downstream unsupervised tasks: outlier detection and clustering. We present the
effectiveness of our approach with experiments against strong baselines and
show that LOTUS is a very promising first step toward model selection for
multiple unsupervised ML tasks.

</details>


### [70] [Expanding the Action Space of LLMs to Reason Beyond Language](https://arxiv.org/abs/2510.07581)
*Zhongqi Yue,Weishi Wang,Yundaichuan Zhan,Juncheng Li,Daniel Dahlmeier,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 提出将外部环境交互从词汇空间扩展到“扩展动作”（ExpA），并用EARL（带反事实策略优化的强化学习）学习在语言与环境间路由和调用专用动作，从而提升多回合交互与计划任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM仅能输出词汇，需用文本约定调用外部接口，增加语言负担并依赖人工解析器；希望把环境交互内化为模型动作以简化控制并提高效能。

Method: 定义Expanded Action空间，允许模型在语言环境和外部环境间路由；在外部环境中只能执行环境特定动作并接收反馈。提出EARL算法，采用反事实策略优化以促进在扩展动作空间上的探索与学习。

Result: 在多回合交互和条件规划任务上，EARL优于词汇受限基线。在计算器多任务和部分可观测排序任务中表现稳健：在Sort-4问题上达到完美准确率，并自发现了与经典算法竞争的高效策略。

Conclusion: 通过把环境交互作为第一类动作并用EARL训练，模型能更有效地利用外部工具与环境，改进复杂交互任务的性能并能自发现高效算法。

Abstract: Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments -- such as symbolic operators
or simulators -- must be expressed through text in predefined formats, parsed,
and routed to external interfaces. This overloads the model's language with
both reasoning and control duties, and requires a hand-crafted parser, external
to the LLM. To address this, we decouple environment interactions from language
by internalizing them in an Expanded Action space (ExpA), beyond the
vocabulary. The model starts reasoning in the default language environment, but
may trigger routing actions and switch to an external environment at any time.
From there, the model can only invoke environment-specific actions, receive
feedback from the environment, and potentially route back to language as a
result. To promote effective exploration of the expanded action space and new
environments, we introduce ExpA Reinforcement Learning (EARL) with
counterfactual policy optimization. On tasks requiring multi-turn interactions
and contingent planning, EARL outperforms strong baselines with
vocabulary-constrained actions. It performs robustly across calculator-based
multi-task learning and, in the partially observed sorting problem, achieves
perfect Sort-4 accuracy while self-discovering an efficient algorithm
competitive with classical designs.

</details>


### [71] [TGM: a Modular and Efficient Library for Machine Learning on Temporal Graphs](https://arxiv.org/abs/2510.07586)
*Jacob Chmura,Shenyang Huang,Tran Gia Bao Ngo,Ali Parviz,Farimah Poursafaei,Jure Leskovec,Michael Bronstein,Guillaume Rabusseau,Matthias Fey,Reihaneh Rabbany*

Main category: cs.LG

TL;DR: 本文提出了 TGM —— 一个面向研究的时序图机器学习库，首次统一连续时和离散时动态图方法，原生支持动态节点特征、时间粒度转换及多层级任务。实现上在多模型/数据集/任务上对比 DyGLib 平均加速 7.8 倍，图离散化实现平均加速 175 倍，并展示了开启新的研究范式（如动态图属性预测、时间驱动训练）。代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前时序图（TG）研究缺乏像静态图那样成熟的开源框架；现有库多为特定架构定制，且持续时间/离散时间方法割裂，限制了方法比较与思想迁移，此外对动态节点特征和时间粒度操作的支持不足。

Method: 设计并实现 TGM 库：统一 CTDG 与 DTDG 的抽象和接口；提供动态节点特征的第一类支持；实现时间粒度（continuous↔discrete）转换工具；原生支持链路/节点/图级任务；对关键操作（如图离散化）进行高效实现以提高运行速度。

Result: 在多种模型、数据集与任务上，TGM 相较于广泛使用的 DyGLib 平均加速约 7.8×，图离散化模块相较可用实现平均加速约 175×；并通过实验演示可实现动态图属性预测与时间驱动训练等此前难以开展的研究。

Conclusion: TGM 弥补了时序图 ML 基础设施的空白，提升实验效率、促进 CTDG/DTDG 方法的比较与迁移，并通过开源推动社区研究与复现。

Abstract: Well-designed open-source software drives progress in Machine Learning (ML)
research. While static graph ML enjoys mature frameworks like PyTorch Geometric
and DGL, ML for temporal graphs (TG), networks that evolve over time, lacks
comparable infrastructure. Existing TG libraries are often tailored to specific
architectures, hindering support for diverse models in this rapidly evolving
field. Additionally, the divide between continuous- and discrete-time dynamic
graph methods (CTDG and DTDG) limits direct comparisons and idea transfer. To
address these gaps, we introduce Temporal Graph Modelling (TGM), a
research-oriented library for ML on temporal graphs, the first to unify CTDG
and DTDG approaches. TGM offers first-class support for dynamic node features,
time-granularity conversions, and native handling of link-, node-, and
graph-level tasks. Empirically, TGM achieves an average 7.8x speedup across
multiple models, datasets, and tasks compared to the widely used DyGLib, and an
average 175x speedup on graph discretization relative to available
implementations. Beyond efficiency, we show in our experiments how TGM unlocks
entirely new research possibilities by enabling dynamic graph property
prediction and time-driven training paradigms, opening the door to questions
previously impractical to study. TGM is available at
https://github.com/tgm-team/tgm

</details>


### [72] [LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](https://arxiv.org/abs/2510.07626)
*Chongyu Fan,Changsheng Wang,Yancheng Huang,Soumyadeep Pal,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了对近年来12种有状态LLM“取消学习”方法的原则性分类（分为三类），并重审了有效性、效用保持与鲁棒性的评估，指出现有以选择题准确率为主的评估过于狭窄，提出Open‑QA指标并揭示方法间的UE–UT权衡与鲁棒性差异。


<details>
  <summary>Details</summary>
Motivation: 当前LLM取消学习研究碎片化，对“有效取消”缺乏统一定义与严格评估，且常用MCQ评估并不能反映生成行为与实际风险。作者希望建立分类框架并改进评估策略，以推动更可靠的方法设计。

Method: （1）构建原则性分类：将12种方法归入三大类——发散驱动优化（divergence‑driven optimization）、表示错配（representation misalignment）、基于拒绝的有针对性取消（rejection‑based targeted unlearning）；（2）在WMDP基准上重访评估维度：取消有效性（UE）、效用保持（UT）、鲁棒性（Rob）；（3）提出Open‑QA评估指标以衡量生成式行为并分析UE‑UT权衡与不同攻击/重训练场景下的鲁棒性差异。

Result: 发现现有评估（以MCQ为主）高估了取消效果且忽略生成输出；Open‑QA指标更能揭示实际生成行为并显示不同方法族在UE与UT之间存在内在权衡；鲁棒性分析表明不同重训练或微调威胁（如域内重学与域外微调）对模型脆弱性差异显著。

Conclusion: 本文提供了一个全栈的取消学习重新检视：方法学分类、改进的评估指标与更细粒度的鲁棒性分析，为未来设计与评估LLM取消学习方法提供了可操作的指导。

Abstract: Machine unlearning for large language models (LLMs) aims to remove undesired
data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while
preserving useful model capabilities. Despite rapid progress over the past two
years, research in LLM unlearning remains fragmented, with limited clarity on
what constitutes effective unlearning and how it should be rigorously
evaluated. In this work, we present a principled taxonomy of twelve recent
stateful unlearning methods, grouped into three methodological families:
divergence-driven optimization, representation misalignment, and
rejection-based targeted unlearning. Building on this taxonomy, we revisit the
evaluation of unlearning effectiveness (UE), utility retention (UT), and
robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that
current evaluations, dominated by multiple-choice question (MCQ) accuracy,
offer only a narrow perspective, often overstating success while overlooking
the model's actual generation behavior. To address this gap, we introduce open
question-answering (Open-QA) metrics that better capture generative performance
and reveal the inherent UE-UT tradeoff across method families. Furthermore, we
demonstrate that robustness requires finer-grained analysis: for example,
vulnerabilities differ substantially between in-domain relearning and
out-of-domain fine-tuning, even though both fall under model-level attacks.
Through this study, we hope to deliver a full-stack revisit of LLM unlearning
and actionable guidance for designing and evaluating future methods.

</details>


### [73] [Property Classification of Vacation Rental Properties during Covid-19](https://arxiv.org/abs/2510.07639)
*Favour Yahdii Aghaebe,Dustin Foley,Eric Atwell,Stephen Clark*

Main category: cs.LG

TL;DR: Apply K-means and K-medoids on >1M vacation rentals to find homogeneous groups during COVID-19; aims to inform targeted policies.


<details>
  <summary>Details</summary>
Motivation: Understand patterns and behaviours of vacation rental properties and hosts active during the COVID pandemic using large-scale commercial dataset (CDRC + AirDNA) to inform policy and market insights.

Method: Clustering analysis using K-means and K-medoids on a dataset of over one million properties and hosts; identify homogeneous groups and describe their common characteristics.

Result: Identification of clusters representing similar property/host profiles and their characteristic features, enhancing understanding of vacation rental dynamics during the pandemic.

Conclusion: Clustering reveals meaningful groups that can support creation of targeted, cluster-specific policies and improve comprehension of vacation rental evaluations.

Abstract: This study advocates for employing clustering techniques to classify vacation
rental properties active during the Covid pandemic to identify inherent
patterns and behaviours. The dataset, a collaboration between the ESRC funded
Consumer Data Research Centre (CDRC) and AirDNA, encompasses data for over a
million properties and hosts. Utilising K-means and K-medoids clustering
techniques, we identify homogenous groups and their common characteristics. Our
findings enhance comprehension of the intricacies of vacation rental
evaluations and could potentially be utilised in the creation of targeted,
cluster-specific policies.

</details>


### [74] [Continual Learning for Adaptive AI Systems](https://arxiv.org/abs/2510.07648)
*Md Hasibul Amin,Tamzid Tanvi Alam*

Main category: cs.LG

TL;DR: Proposes Inter-Cluster Separation (ICS), a loss regularizer that penalizes outputs far from centroids of previous-task clusters to mitigate catastrophic forgetting; evaluated on 5-task Split CIFAR-10 with ResNet-18, shows improved retention on initial tasks but limited long-term retention as tasks increase.


<details>
  <summary>Details</summary>
Motivation: Continual learning suffers from catastrophic forgetting. Existing regularization methods constrain parameters but may not explicitly enforce separation in representation space. Authors aim to create clearer task separation in internal representations to reduce overlap and forgetting.

Method: Introduce an ICS term added to the training loss that measures distance between current outputs (or features) and centroids of clusters formed by previous tasks; penalize large distances. Perform hyperparameter tuning to weight the ICS term. Use ResNet-18 on Split CIFAR-10 (5 tasks) to evaluate.

Result: ICS helps maintain strong performance on initial tasks in the 5-task Split CIFAR-10 benchmark, indicating reduced forgetting compared to baseline(s). However, performance degrades as the number of tasks increases, indicating limitations in long-term knowledge retention.

Conclusion: ICS is a promising regularizer that enforces inter-cluster separation and reduces short-term forgetting, but it does not fully solve long-term continual learning; further work needed on scaling, centroid updating, combination with replay or other mechanisms, and broader evaluation.

Abstract: Continual learning the ability of a neural network to learn multiple
sequential tasks without losing previously acquired knowledge remains a
significant obstacle to developing truly adaptive artificial intelligence. Deep
learning models have achieved remarkable results in various applications, but
overfitting remains a common issue. Regularization techniques can help prevent
overfitting by adding constraints to the model's parameters. To prevent
catastrophic forgetting, in this paper we introduce a novel regularization
technique based on inter-cluster separation (ICS) in the loss function, which
penalizes the model for producing outputs that are far away from the centroids
of the clusters formed by the data from previous tasks. We also performed
hyperparameter tuning to find the optimal weighting of the proposed
regularization term. This ensures clearer separation between tasks in the
neural network's internal representation, reducing overlap and mitigating
forgetting. Using the standard 5-task Split CIFAR-10 benchmark and a ResNet-18
architecture, we demonstrate ICS's effectiveness in maintaining strong
performance on initial tasks. However, our results also highlight limitations
in long-term knowledge retention, particularly when the number of tasks
increases. This underscores the complexity and trade-offs inherent in continual
learning and points toward avenues for further research.

</details>


### [75] [Value Flows](https://arxiv.org/abs/2510.07650)
*Perry Dong,Chongyi Zheng,Chelsea Finn,Dorsa Sadigh,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文提出Value Flows：用流模型（flow-based models）学习完整的未来回报密度，通过一个新的流匹配目标使密度路径满足分布式贝尔曼方程，并基于流的导数ODE估计状态的回报不确定性，进而用于优先学习。实验证明在多项基准上优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 当前分布式强化学习多将回报分布压缩为离散类别或若干分位数，缺乏对回报分布精细结构和不确定性状态的区分能力，限制了在探索和安全性场景下的应用。

Method: 提出一种基于流模型的回报密度估计框架：1) 设计新的流匹配目标，生成满足分布式贝尔曼方程的概率密度路径；2) 基于学习到的流，通过导数形式的ODE计算不同状态的回报方差/不确定性；3) 将不确定性用于对过渡进行优先学习以提升估计质量。

Result: 在37个状态类任务和25个基于图像的任务上的离线与在线-到-在线设置中，Value Flows在成功率上平均提升约1.3倍。

Conclusion: 利用流模型能更精细地建模返回分布并提供可微的、不确定性量化手段，从而在若干强化学习基准上带来显著性能提升，且为探索与安全相关应用提供了新的工具。

Abstract: While most reinforcement learning methods today flatten the distribution of
future returns to a single scalar value, distributional RL methods exploit the
return distribution to provide stronger learning signals and to enable
applications in exploration and safe RL. While the predominant method for
estimating the return distribution is by modeling it as a categorical
distribution over discrete bins or estimating a finite number of quantiles,
such approaches leave unanswered questions about the fine-grained structure of
the return distribution and about how to distinguish states with high return
uncertainty for decision-making. The key idea in this paper is to use modern,
flexible flow-based models to estimate the full future return distributions and
identify those states with high return variance. We do so by formulating a new
flow-matching objective that generates probability density paths satisfying the
distributional Bellman equation. Building upon the learned flow models, we
estimate the return uncertainty of distinct states using a new flow derivative
ODE. We additionally use this uncertainty information to prioritize learning a
more accurate return estimation on certain transitions. We compare our method
(Value Flows) with prior methods in the offline and online-to-online settings.
Experiments on $37$ state-based and $25$ image-based benchmark tasks
demonstrate that Value Flows achieves a $1.3\times$ improvement on average in
success rates. Website: https://pd-perry.github.io/value-flows Code:
https://github.com/chongyi-zheng/value-flows

</details>


### [76] [FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2510.07664)
*Yunbo Li,Jiaping Gui,Zhihang Deng,Fanchao Meng,Yue Wu*

Main category: cs.LG

TL;DR: 提出FedQS，一种用于半异步联邦学习的框架，通过划分客户端类型并自适应优化本地训练来协调梯度聚合与模型聚合的权衡，声称在准确性、收敛速度和稳定性上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 半异步FL在同步与异步方法间取得平衡，但梯度聚合与模型聚合各有短板：前者收敛快但震荡大，后者稳定但收敛慢且精度低。需要一个统一理论与实践框架来兼顾三者。

Method: 提出FedQS：1) 从理论上分析SAFL中两类聚合策略的差异；2) 采用分而治之策略将客户端划分为四类，依据数据分布与算力自适应调整各类客户端的本地训练策略；3) 在聚合时针对不同客户端输出权重或聚合方式进行优化，以平衡精度、收敛与稳定性。

Result: 在计算机视觉、NLP和真实任务上进行大量实验，结果显示FedQS在准确率、损失和收敛速度上相较最先进基线具有显著提升，代码与数据集已公开。

Conclusion: FedQS在理论与实证上弥合了SAFL中两类聚合策略的差距，提供了一个兼顾稳定性、准确性与效率的统一解决方案。

Abstract: Federated learning (FL) enables collaborative model training across multiple
parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as
a balanced approach between synchronous and asynchronous FL. However, SAFL
faces significant challenges in optimizing both gradient-based (e.g., FedSGD)
and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct
trade-offs in accuracy, convergence speed, and stability. While gradient
aggregation achieves faster convergence and higher accuracy, it suffers from
pronounced fluctuations, whereas model aggregation offers greater stability but
slower convergence and suboptimal accuracy. This paper presents FedQS, the
first framework to theoretically analyze and address these disparities in SAFL.
FedQS introduces a divide-and-conquer strategy to handle client heterogeneity
by classifying clients into four distinct types and adaptively optimizing their
local training based on data distribution characteristics and available
computational resources. Extensive experiments on computer vision, natural
language processing, and real-world tasks demonstrate that FedQS achieves the
highest accuracy, attains the lowest loss, and ranks among the fastest in
convergence speed, outperforming state-of-the-art baselines. Our work bridges
the gap between aggregation strategies in SAFL, offering a unified solution for
stable, accurate, and efficient federated learning. The code and datasets are
available at https://anonymous.4open.science/r/FedQS-EDD6.

</details>


### [77] [Computationally-efficient Graph Modeling with Refined Graph Random Features](https://arxiv.org/abs/2510.07716)
*Krzysztof Choromanski,Avinava Dubey,Arijit Sehanobish,Isaac Reid*

Main category: cs.LG

TL;DR: GRFs++ 是一种改进的图随机特征方法，通过短游走拼接（walk-stitching）与可变终止分布，保持长游走的逼近质量同时显著提高并行性与效率，能更好建模远程节点关系并在理论与实验上验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有 GRFs 在建模图中远程节点关系时表现受限，依赖顺序采样长随机游走且采用固定的伯努利停机机制，导致效率低下与近似精度受限。

Method: 提出 walk-stitching 将多个短游走无偏地连接以模拟长游走，从串行长游走采样转为并行短游走与矩阵乘法；将停机机制从固定伯努利拓展为任意长度分布以提高近似灵活性。

Result: GRFs++ 在保持或接近长游走近似质量的同时显著提升计算效率与并行性；采用更广泛的终止分布能在不增加计算开销下改善图核近似精度；论文给出实证结果与理论证明支持这些主张。

Conclusion: GRFs++ 为图核与图节点核方法提供了可扩展且精确的随机特征框架，特别适合需要捕捉远程关系且要求并行高效计算的场景，同时为停机策略提供了更大的设计空间。

Abstract: We propose refined GRFs (GRFs++), a new class of Graph Random Features (GRFs)
for efficient and accurate computations involving kernels defined on the nodes
of a graph. GRFs++ resolve some of the long-standing limitations of regular
GRFs, including difficulty modeling relationships between more distant nodes.
They reduce dependence on sampling long graph random walks via a novel
walk-stitching technique, concatenating several shorter walks without breaking
unbiasedness. By applying these techniques, GRFs++ inherit the approximation
quality provided by longer walks but with greater efficiency, trading
sequential, inefficient sampling of a long walk for parallel computation of
short walks and matrix-matrix multiplication. Furthermore, GRFs++ extend the
simplistic GRFs walk termination mechanism (Bernoulli schemes with fixed
halting probabilities) to a broader class of strategies, applying general
distributions on the walks' lengths. This improves the approximation accuracy
of graph kernels, without incurring extra computational cost. We provide
empirical evaluations to showcase all our claims and complement our results
with theoretical analysis.

</details>


### [78] [DEAS: DEtached value learning with Action Sequence for Scalable Offline RL](https://arxiv.org/abs/2510.07730)
*Changyeon Kim,Haeone Lee,Younggyo Seo,Kimin Lee,Yuke Zhu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Offline reinforcement learning (RL) presents an attractive paradigm for
training intelligent agents without expensive online interactions. However,
current approaches still struggle with complex, long-horizon sequential
decision making. In this work, we introduce DEtached value learning with Action
Sequence (DEAS), a simple yet effective offline RL framework that leverages
action sequences for value learning. These temporally extended actions provide
richer information than single-step actions and can be interpreted through the
options framework via semi-Markov decision process Q-learning, enabling
reduction of the effective planning horizon by considering longer sequences at
once. However, directly adopting such sequences in actor-critic algorithms
introduces excessive value overestimation, which we address through detached
value learning that steers value estimates toward in-distribution actions that
achieve high return in the offline dataset. We demonstrate that DEAS
consistently outperforms baselines on complex, long-horizon tasks from OGBench
and can be applied to enhance the performance of large-scale
Vision-Language-Action models that predict action sequences, significantly
boosting performance in both RoboCasa Kitchen simulation tasks and real-world
manipulation tasks.

</details>


### [79] [GeoGen: A Two-stage Coarse-to-Fine Framework for Fine-grained Synthetic Location-based Social Network Trajectory Generation](https://arxiv.org/abs/2510.07735)
*Rongchao Xu,Kunlin Cai,Lin Jiang,Dahai Yu,Zhiqing Hong,Yuan Tian,Guang Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Location-Based Social Network (LBSN) check-in trajectory data are important
for many practical applications, like POI recommendation, advertising, and
pandemic intervention. However, the high collection costs and ever-increasing
privacy concerns prevent us from accessing large-scale LBSN trajectory data.
The recent advances in synthetic data generation provide us with a new
opportunity to achieve this, which utilizes generative AI to generate synthetic
data that preserves the characteristics of real data while ensuring privacy
protection. However, generating synthetic LBSN check-in trajectories remains
challenging due to their spatially discrete, temporally irregular nature and
the complex spatio-temporal patterns caused by sparse activities and uncertain
human mobility. To address this challenge, we propose GeoGen, a two-stage
coarse-to-fine framework for large-scale LBSN check-in trajectory generation.
In the first stage, we reconstruct spatially continuous, temporally regular
latent movement sequences from the original LBSN check-in trajectories and then
design a Sparsity-aware Spatio-temporal Diffusion model (S$^2$TDiff) with an
efficient denosing network to learn their underlying behavioral patterns. In
the second stage, we design Coarse2FineNet, a Transformer-based Seq2Seq
architecture equipped with a dynamic context fusion mechanism in the encoder
and a multi-task hybrid-head decoder, which generates fine-grained LBSN
trajectories based on coarse-grained latent movement sequences by modeling
semantic relevance and behavioral uncertainty. Extensive experiments on four
real-world datasets show that GeoGen excels state-of-the-art models for both
fidelity and utility evaluation, e.g., it increases over 69% and 55% in
distance and radius metrics on the FS-TKY dataset.

</details>


### [80] [MeSH: Memory-as-State-Highways for Recursive Transformers](https://arxiv.org/abs/2510.07739)
*Chengting Yu,Xiaobo Shu,Yadao Wang,Yizhen Zhang,Haoyi Wu,Jiaang Li,Rujiao Long,Ziheng Chen,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recursive transformers reuse parameters and iterate over hidden states
multiple times, decoupling compute depth from parameter depth. However, under
matched compute, recursive models with fewer parameters often lag behind
non-recursive counterparts. By probing hidden states, we trace this performance
gap to two primary bottlenecks: undifferentiated computation, where the core is
forced to adopt a similar computational pattern at every iteration, and
information overload, where long-lived and transient information must coexist
in a single hidden state. To address the issues, we introduce a
Memory-as-State-Highways (MeSH) scheme, which externalizes state management
into an explicit memory buffer and employs lightweight routers to dynamically
diversify computation across iterations. Probing visualizations confirm that
MeSH successfully resolves the pathologies by inducing functional
specialization across iterations. On the Pythia suite (160M-1.4B),
MeSH-enhanced recursive transformers consistently improve over recursive
baselines and outperforms its larger non-recursive counterpart at the 1.4B
scale, improving average downstream accuracy by +1.06% with 33% fewer
non-embedding parameters. Our analysis establishes MeSH as a scalable and
principled architecture for building stronger recursive models.

</details>


### [81] [t-SNE Exaggerates Clusters, Provably](https://arxiv.org/abs/2510.07746)
*Noah Bergam,Szymon Snoeck,Nakul Verma*

Main category: cs.LG

TL;DR: t-SNE 的可视化并不能可靠地反映输入数据的簇强度或异常点的极端性。


<details>
  <summary>Details</summary>
Motivation: 研究者和从业者广泛使用 t-SNE 进行降维可视化并据此推断数据结构（如簇的存在、簇的强弱及异常点）。作者质疑这种直观推断的可靠性并欲从理论和实证两方面检验 t-SNE 在这些任务上的局限。

Method: 构建数学证明，表明存在不同输入数据在高维空间中具有显著不同簇强度或异常程度，但其对应的 t-SNE 嵌入可以表现得相似或相同，从而无法区分。补充大量实验，展示这些失败模式在实际数据和不同参数设置下的普遍性。

Result: 证明结果表明：t-SNE 输出不能可靠推断（1）输入数据的簇强度；（2）输入中异常点的极端程度。实证结果展示了多种常见数据集和设置下出现的具体范例，验证理论结论在实践中的可见性。

Conclusion: 对 t-SNE 可视化应保持谨慎，不应仅凭二维嵌入判断簇的强弱或异常点的重要性。建议结合定量评估和其它方法以避免错误解读。

Abstract: Central to the widespread use of t-distributed stochastic neighbor embedding
(t-SNE) is the conviction that it produces visualizations whose structure
roughly matches that of the input. To the contrary, we prove that (1) the
strength of the input clustering, and (2) the extremity of outlier points,
cannot be reliably inferred from the t-SNE output. We demonstrate the
prevalence of these failure modes in practice as well.

</details>


### [82] [FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and Inter-domain Knowledge Modeling](https://arxiv.org/abs/2510.07755)
*Zhengyu Wu,Yinlin Zhu,Xunkai Li,Ziang Qiu,Rong-Hua Li,Guoren Wang,Chenghu Zhou*

Main category: cs.LG

TL;DR: FedBook 是一种在服务器端汇聚客户端本地码本的联邦图基础模型（FedGFM）方法，通过“域内协同”和“域间整合”两阶段构建统一全局码本，以在保持域内语义一致性的同时保留跨域多样性，从而提升联邦预训练与下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型通常假设可集中访问多域图数据，但在现实中受隐私和机构限制无法集中，亟需联邦化方法；关键挑战是构建既能在每个域内凝聚语义（域内一致性），又能保留不同域间异质知识（域间多样性）的全局码本。

Method: 提出 FedBook：在服务器端对客户端本地码本进行两阶段融合。阶段一“域内协同”：利用跨客户端的高频（语义可靠）token参考来细化低频token，提升域内一致性；阶段二“域间整合”：在聚合全局模型时，根据各客户端码本的语义独特性为其贡献加权，以保留跨域多样性。

Result: 在8个跨域基准和多种任务上，FedBook 比21个基线方法（包括独立监督学习、联邦学习/联邦图学习、集中式GFM的联邦化适配以及现有FedGFM方法）表现更好，表明其在构建全局码本与提升下游性能方面有效。

Conclusion: FedBook 能在联邦设置下有效构建兼顾域内凝聚性与域间多样性的全局图基础码本，从而显著提升联邦图基础模型的泛化与下游任务表现。

Abstract: Foundation models have shown remarkable cross-domain generalization in
language and vision, inspiring the development of graph foundation models
(GFMs). However, existing GFMs typically assume centralized access to
multi-domain graphs, which is often infeasible due to privacy and institutional
constraints. Federated Graph Foundation Models (FedGFMs) address this
limitation, but their effectiveness fundamentally hinges on constructing a
robust global codebook that achieves intra-domain coherence by consolidating
mutually reinforcing semantics within each domain, while also maintaining
inter-domain diversity by retaining heterogeneous knowledge across domains. To
this end, we propose FedBook, a unified federated graph foundation codebook
that systematically aggregates clients' local codebooks during server-side
federated pre-training. FedBook follows a two-phase process: (1) Intra-domain
Collaboration, where low-frequency tokens are refined by referencing more
semantically reliable high-frequency tokens across clients to enhance
domain-specific coherence; and (2) Inter-domain Integration, where client
contributions are weighted by the semantic distinctiveness of their codebooks
during the aggregation of the global GFM, thereby preserving cross-domain
diversity. Extensive experiments on 8 benchmarks across multiple domains and
tasks demonstrate that FedBook consistently outperforms 21 baselines, including
isolated supervised learning, FL/FGL, federated adaptations of centralized
GFMs, and FedGFM techniques.

</details>


### [83] [A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned Optimization](https://arxiv.org/abs/2510.07760)
*Yiqin Lv,Zhiyu Mou,Miao Xu,Jinghao Chen,Qi Wang,Yixiu Mao,Yun Qu,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出 VAMO：一种基于验证梯度对齐的多任务优化方法，通过测量每个任务训练梯度与持出验证梯度的对齐程度自适应分配任务权重，并结合周期感知时序模块与生成式自动出价主干，旨在提升投放目标上的泛化与稳健性。


<details>
  <summary>Details</summary>
Motivation: 在线广告中大量定制化出价任务独立优化导致计算和数据效率低，现有多任务优化多以训练动态为导向，在波动性大的出价环境下泛化不足，需将优化目标与部署（验证）目标对齐。

Method: VAMO 根据每个任务训练梯度与持出验证梯度的余弦对齐度自适应调整任务权重，辅以周期性时序模块以捕捉季节性结构，并采用先进的生成式自动出价主干以增强跨任务迁移。提出了收敛性与对齐性相关的理论分析。

Result: 在仿真与大规模真实广告系统上，VAMO 相较常见基线方法表现出显著提升，证明了该方法在波动环境下的有效性。

Conclusion: 通过把多任务更新方向向验证集改进方向引导，VAMO 能更好匹配部署目标、提升跨任务季节性结构迁移及整体出价表现，具有理论保证与实证支持。

Abstract: In online advertising, heterogeneous advertiser requirements give rise to
numerous customized bidding tasks that are typically optimized independently,
resulting in extensive computation and limited data efficiency. Multi-task
learning offers a principled framework to train these tasks jointly through
shared representations. However, existing multi-task optimization strategies
are primarily guided by training dynamics and often generalize poorly in
volatile bidding environments. To this end, we present Validation-Aligned
Multi-task Optimization (VAMO), which adaptively assigns task weights based on
the alignment between per-task training gradients and a held-out validation
gradient, thereby steering updates toward validation improvement and better
matching deployment objectives. We further equip the framework with a
periodicity-aware temporal module and couple it with an advanced generative
auto-bidding backbone to enhance cross-task transfer of seasonal structure and
strengthen bidding performance. Meanwhile, we provide theoretical insights into
the proposed method, e.g., convergence guarantee and alignment analysis.
Extensive experiments on both simulated and large-scale real-world advertising
systems consistently demonstrate significant improvements over typical
baselines, illuminating the effectiveness of the proposed approach.

</details>


### [84] [FedLAM: Low-latency Wireless Federated Learning via Layer-wise Adaptive Modulation](https://arxiv.org/abs/2510.07766)
*Linping Qu,Shenghui Song,Chi-Ying Tsui*

Main category: cs.LG

TL;DR: 提出一种按层自适应调制方案，根据DNN层重要性为不同层分配不同的调制等级，从而显著降低联邦学习中的通信延迟。


<details>
  <summary>Details</summary>
Motivation: 在无线联邦学习中，客户端需通过受限带宽上传高维DNN参数，统一调制等级导致资源浪费与更高的通信延迟，需要更精细的分配策略。

Method: 引入层重要性度量并基于该度量自动为各层选择最优调制等级（比特率/符号映射），实现按层适配以减少传输时间。

Result: 仿真实验表明，相较于现有的统一调制方案，所提出方法最优可节省高达73.9%的通信延迟。

Conclusion: 按层自适应调制在带宽受限的无线FL场景下能显著减少通信延迟，提供了比全局统一调制更灵活高效的传输策略。

Abstract: In wireless federated learning (FL), the clients need to transmit the
high-dimensional deep neural network (DNN) parameters through bandwidth-limited
channels, which causes the communication latency issue. In this paper, we
propose a layer-wise adaptive modulation scheme to save the communication
latency. Unlike existing works which assign the same modulation level for all
DNN layers, we consider the layers' importance which provides more freedom to
save the latency. The proposed scheme can automatically decide the optimal
modulation levels for different DNN layers. Experimental results show that the
proposed scheme can save up to 73.9% of communication latency compared with the
existing schemes.

</details>


### [85] [Weak Form Learning for Mean-Field Partial Differential Equations: an Application to Insect Movement](https://arxiv.org/abs/2510.07786)
*Seth Minor,Bret D. Elderd,Benjamin Van Allen,David M. Bortz,Vanja Dukic*

Main category: cs.LG

TL;DR: 本文将弱形式方程学习方法（如WSINDy）与核密度估计相结合，从稀疏的昆虫位置数据中学习有效的Fokker–Planck类型模型，以刻画毛虫（秋黏虫）在不同植物资源和感染状态下的优先移动模式，并展示在稀疏实验数据上的可行性。


<details>
  <summary>Details</summary>
Motivation: 短时尺度上外源因素具有随机性，昆虫轨迹常服从过阻尼随机动力学。通过从观测到的昆虫分布中学习对应的Fokker–Planck方程，可以理解并预测害虫的扩散动力学，从而改善害虫爆发强度和位置的预报，有助于害虫管理。然而，实验数据通常高度稀疏，增加了模型学习的难度。

Method: 将弱形式方程识别（Galerkin方法，如WSINDy）与核密度估计结合，以弱形式构造观测方程并稀疏化识别漂移和扩散项。方法针对稀疏的位置测量数据设计，利用弱形式降低噪声敏感性并通过字典化候选项进行稀疏回归以得到有效的Fokker–Planck描述。

Result: 在模拟农业条件下采集的秋黏虫位置稀疏数据上进行了验证。结果表明，该方法能在稀疏样本下恢复出描述群体运动的有效模型，能够捕捉受植物资源和感染状态影响的优先移动模式，并在分布层面提供合理预测（对稀疏数据具有鲁棒性和实用性）。

Conclusion: 将弱形式方程学习与核密度估计相结合，为从稀疏昆虫位置观测中学习有效Fokker–Planck模型提供了一条可行路径。该方法可用于研究害虫的空间扩散，并有潜力辅助作物和林业害虫的预报与管理。

Abstract: Insect species subject to infection, predation, and anisotropic environmental
conditions may exhibit preferential movement patterns. Given the innate
stochasticity of exogenous factors driving these patterns over short
timescales, individual insect trajectories typically obey overdamped stochastic
dynamics. In practice, data-driven modeling approaches designed to learn the
underlying Fokker-Planck equations from observed insect distributions serve as
ideal tools for understanding and predicting such behavior. Understanding
dispersal dynamics of crop and silvicultural pests can lead to a better
forecasting of outbreak intensity and location, which can result in better pest
management. In this work, we extend weak-form equation learning techniques,
coupled with kernel density estimation, to learn effective models for
lepidopteran larval population movement from highly sparse experimental data.
Galerkin methods such as the Weak form Sparse Identification of Nonlinear
Dynamics (WSINDy) algorithm have recently proven useful for learning governing
equations in several scientific contexts. We demonstrate the utility of the
method on a sparse dataset of position measurements of fall armyworms
(Spodoptera frugiperda) obtained in simulated agricultural conditions with
varied plant resources and infection status.

</details>


### [86] [HySim-LLM: Embedding-Weighted Fine-Tuning Bounds and Manifold Denoising for Domain-Adapted LLMs](https://arxiv.org/abs/2510.07796)
*Majid Jaberi-Douraki,Hossein Sholehrasa,Xuan Xu,Remya Ampadi Ramachandran*

Main category: cs.LG

TL;DR: 提出HySim-LLM：通过嵌入加权微调与流形感知去噪，提升大模型在结构化药代动力学表格数据上的鲁棒性与可解释性，并给出两项理论界限保证。


<details>
  <summary>Details</summary>
Motivation: 从文献中自动提取和标准化药代动力学（PK）信息困难，现有LLM在处理异质、噪声多且存在领域漂移的结构化生物医学数据时表现受限，亟需有理论支撑的适配方法。

Method: 设计统一框架HySim-LLM，核心为（1）基于样本嵌入相似度的加权微调以缓解分布差异，（2）基于流形假设的去噪机制以限制离流形样本的损失贡献；并从理论上推导相似性加权的泛化界及流形去噪的损失上界。

Result: 给出两条理论结果：相似性加权泛化界（刻画在嵌入发散情况下的适配性能）与流形去噪保证（界定噪声或离流形样本对总体损失的贡献）。这些定理为在结构化生物医学场景微调LLM提供了数学依据。

Conclusion: HySim-LLM为将LLM可靠且具可解释性地应用于数据密集型科学与生物医学结构化信息抽取提供了一个有理论支撑的路径，强调了结合嵌入相似性与流形结构的重要性。

Abstract: The extraction and standardization of pharmacokinetic (PK) information from
scientific literature remain significant challenges in computational
pharmacology, which limits the reliability of data-driven models in drug
development. Large language models (LLMs) have achieved remarkable progress in
text understanding and reasoning, yet their adaptation to structured biomedical
data, such as PK tables, remains constrained by heterogeneity, noise, and
domain shift. To address these limitations, we propose HySim-LLM, a unified
mathematical and computational framework that integrates embedding-weighted
fine-tuning and manifold-aware denoising to enhance the robustness and
interpretability of LLMs. We establish two theoretical results: (1) a
similarity-weighted generalization bound that quantifies adaptation performance
under embedding divergence, and (2) a manifold-based denoising guarantee that
bounds loss contributions from noisy or off-manifold samples. These theorems
provide a principled foundation for fine-tuning LLMs in structured biomedical
settings. The framework offers a mathematically grounded pathway toward
reliable and interpretable LLM adaptation for biomedical and data-intensive
scientific domains.

</details>


### [87] [SIMU: Selective Influence Machine Unlearning](https://arxiv.org/abs/2510.07822)
*Anu Agarwal,Mihir Pamnani,Dilek Hakkani-Tur*

Main category: cs.LG

TL;DR: This paper proposes Selective Influence Machine Unlearning (SIMU), a two-step framework improving second-order optimizer-based unlearning by only updating neurons most responsible for the forget-set, aiming to retain original model capabilities while achieving comparable forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing optimizer-based unlearning methods (first- and second-order) can remove targeted sensitive information from LLMs but often degrade the model's pre-existing knowledge and utility. The motivation is to reduce collateral damage by constraining parameter updates to a small subset of important neurons encoding the forget-set.

Method: SIMU identifies critical neurons that encode the forget-set and then applies second-order unlearning updates only to these neurons in a two-step procedure. By focusing updates, it seeks to achieve effective forgetting while minimizing disruption to the rest of the model.

Result: According to the abstract, SIMU attains comparable unlearning effectiveness to prior optimizer-based methods but substantially better retention of original model knowledge and overall utility.

Conclusion: Selective, neuron-level constrained unlearning can balance forgetting efficacy and utility preservation; updating only critical neurons via second-order methods is a promising direction for safer, less destructive machine unlearning.

Abstract: The undesired memorization of sensitive information by Large Language Models
(LLMs) has emphasized the need for safety mechanisms that can regulate model
behavior. This has led to the development of machine unlearning techniques that
enable models to precisely forget sensitive and unwanted information. For
machine unlearning, first-order and second-order optimizer-based methods have
shown significant progress in enabling LLMs to forget targeted information.
However, in doing so, these approaches often compromise the model's original
capabilities, resulting in unlearned models that struggle to retain their prior
knowledge and overall utility. To address this, we propose Selective Influence
Machine Unlearning (SIMU), a two-step framework that enhances second-order
optimizer-based unlearning by selectively updating only the critical neurons
responsible for encoding the forget-set. By constraining updates to these
targeted neurons, SIMU achieves comparable unlearning efficacy while
substantially outperforming current methods in retaining the model's original
knowledge.

</details>


### [88] [Signal-to-Noise Ratio in Scanning Electron Microscopy: A Comprehensive Review](https://arxiv.org/abs/2510.07886)
*K. S. Sim,I. Bukhori,D. C. Y. Ong,K. B. Gan*

Main category: cs.LG

TL;DR: 本文综述了扫描电子显微镜（SEM）成像中的信噪比（SNR）问题：介绍SEM工作原理、噪声来源、SNR的测量与估计方法，以及影响SNR的因素，并总结了硬件和软件层面提升SNR的传统与新兴技术，讨论其优缺点与应用场景，旨在为研究者提供全面参考并推动该领域研究。


<details>
  <summary>Details</summary>
Motivation: SEM以其高空间分辨率和大景深在纳米技术、材料科学和生物成像中被广泛使用，但噪声会显著降低图像质量与可解释性。提高和准确评估SNR是保证SEM成像可靠性的关键，因此有必要系统综述现有测量方法、噪声来源和提升策略，以指导实践并指出研究空白。

Method: 对SEM成像流程、噪声来源（如电子统计噪声、探测器噪声、样品制备相关噪声和扫描伪影等）进行分类与分析；总结SNR的定义和常用测量/估计技术（如直接测量、频域分析、基于模型的估计和统计方法）；评估影响SNR的实验参数（束流、电压、像素时间、探测器几何等）以及环境因素；综述硬件改进（高灵敏探测器、低噪电子学、信号累计等）与软件处理（滤波、重构、去噪算法、机器学习方法）并比较各方法的优缺点与适用范围。

Result: 汇总并分类了SEM中主要噪声类型与SNR测量方法，指出常用方法在不同成像条件下的局限性；总结了提升SNR的硬件与软件策略，指出硬件改进通常能可靠提升原始信号质量但成本高、复杂度大，而软件方法（尤其基于学习的方法）在保留细节与去噪之间存在权衡且对训练数据敏感。识别了若干研究空白，例如标准化的SNR评估协议、在低剂量下的鲁棒去噪与物理约束的深度学习方法。

Conclusion: 本文为SEM SNR优化提供了系统综述：明确了噪声来源与测量挑战，比较了各种提升手段的适用性与限制，建议结合硬件与软件混合策略并推动标准化测评和面向低剂量鲁棒算法的研究，以提升SEM成像的可靠性和可重现性。

Abstract: Scanning Electron Microscopy (SEM) is critical in nanotechnology, materials
science, and biological imaging due to its high spatial resolution and depth of
focus. Signal-to-noise ratio (SNR) is an essential parameter in SEM because it
directly impacts the quality and interpretability of the images. SEM is widely
used in various scientific disciplines, but its utility can be compromised by
noise, which degrades image clarity. This review explores multiple aspects of
the SEM imaging process, from the principal operation of SEM, sources of noise
in SEM, methods for SNR measurement and estimations, to various aspects that
affect the SNR measurement and approaches to enhance SNR, both from a hardware
and software standpoint. We review traditional and emerging techniques,
focusing on their applications, advantages, and limitations. The paper aims to
provide a comprehensive understanding of SNR optimization in SEM for
researchers and practitioners and to encourage further research in the field.

</details>


### [89] [Adaptive Optimizable Gaussian Process Regression Linear Least Squares Regression Filtering Method for SEM Images](https://arxiv.org/abs/2510.07895)
*D. Chee Yong Ong,I. Bukhori,K. S. Sim,K. Beng Gan*

Main category: cs.LG

TL;DR: 本文提出AO-GPRLLSR管道：先比较五种SNR估计方法并选出线性最小二乘回归(LSR)，再用可优化的高斯过程回归(GPR)配合LSR估计噪声方差，最后以估计的NV驱动维纳滤波改善SEM图像质量；结果显示该方法能准确估计SNR/NV并降低滤波后MSE。


<details>
  <summary>Details</summary>
Motivation: 扫描电子显微镜(SEM)图像常被噪声污染，影响后续分析；因此需要准确估计信噪比(SNR)与噪声方差(NV)，并据此进行有效去噪。

Method: 比较五种SNR估计方法：NN、FOL、NN+FOL、NLLSR、LSR，发现LSR表现最佳；基于LSR的特征，测试SVM与GPR用于NV估计，最终选择可优化的GPR；将估计的NV输入NV引导的维纳滤波，构成AO-GPRLLSR去噪流程。

Result: LSR在SNR估计中最好；优化后的GPR在NV估计上精度最高；AO-GPRLLSR能生成用于维纳滤波的准确NV估计，滤波后图像MSE显著下降。

Conclusion: 提出的方法在SEM图像的SNR与NV估计及基于NV的维纳去噪方面表现良好，是一种稳健且有效的过滤流程；可进一步扩展到更多数据集、实时处理或与深度学习方法结合。

Abstract: Scanning Electron Microscopy (SEM) images often suffer from noise
contamination, which degrades image quality and affects further analysis. This
research presents a complete approach to estimate their Signal-to-Noise Ratio
(SNR) and noise variance (NV), and enhance image quality using NV-guided Wiener
filter. The main idea of this study is to use a good SNR estimation technique
and infuse a machine learning model to estimate NV of the SEM image, which then
guides the wiener filter to remove the noise, providing a more robust and
accurate SEM image filtering pipeline. First, we investigate five different SNR
estimation techniques, namely Nearest Neighbourhood (NN) method, First-Order
Linear Interpolation (FOL) method, Nearest Neighbourhood with First-Order
Linear Interpolation (NN+FOL) method, Non-Linear Least Squares Regression
(NLLSR) method, and Linear Least Squares Regression (LSR) method. It is shown
that LSR method to perform better than the rest. Then, Support Vector Machines
(SVM) and Gaussian Process Regression (GPR) are tested by pairing it with LSR.
In this test, the Optimizable GPR model shows the highest accuracy and it
stands as the most effective solution for NV estimation. Combining these
results lead to the proposed Adaptive Optimizable Gaussian Process Regression
Linear Least Squares Regression (AO-GPRLLSR) Filtering pipeline. The AO-GPRLLSR
method generated an estimated noise variance which served as input to NV-guided
Wiener filter for improving the quality of SEM images. The proposed method is
shown to achieve notable success in estimating SNR and NV of SEM images and
leads to lower Mean Squared Error (MSE) after the filtering process.

</details>


### [90] [MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation](https://arxiv.org/abs/2510.07910)
*Chongmyung Kwon,Yujin Kim,Seoeun Park,Yunji Lee,Charmgil Hong*

Main category: cs.LG

TL;DR: 本文提出MMM框架，将3D量子化学电子局域函数（ELF）生成的电子密度图与基于二分子子结构图的编码器融合，用以改进药物相互作用（DDI）预测。在MIMIC-III数据集上，相比GNN基线（SafeDrug）在F1、Jaccard和DDI率上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的药物表征多为离散结构（如分子图），难以传达分子间键合亲和力与反应性的连续量子化学特征，导致对DDI风险建模能力受限。作者引入3D量子化学信息（ELF）以补足全局电子性质的表征。

Method: 提出MMM：(1) 用ELF生成药物的三维电子密度/ELF地图以表征全局电子分布与反应性特征；(2) 用二分子子结构图编码器捕获局部子结构之间的互作；(3) 融合上述模态以获得互补表征，进而用于DDI预测。评估在MIMIC-III子集（250种药、442种子结构），与若干基线模型对比并进行了统计检验。

Result: 在与SafeDrug比较时，MMM在F1-score（p=0.0387）、Jaccard（p=0.0112）和DDI率（p=0.0386）上实现了统计显著的提升，表明ELF基3D表征可提升DDI预测性能并降低潜在DDI率。

Conclusion: 将量子化学3D电子信息与基于子结构的图表示融合，可获得比纯GNN方法更全面的药物表征，从而在DDI预测任务上带来可观的性能增益，具有提升临床用药安全性的潜力。

Abstract: Drug recommendation is an essential task in machine learning-based clinical
decision support systems. However, the risk of drug-drug interactions (DDI)
between co-prescribed medications remains a significant challenge. Previous
studies have used graph neural networks (GNNs) to represent drug structures.
Regardless, their simplified discrete forms cannot fully capture the molecular
binding affinity and reactivity. Therefore, we propose Multimodal DDI
Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a
novel framework that integrates three-dimensional (3D) quantum-chemical
information into drug representation learning. It generates 3D electron density
maps using the ELF. To capture both therapeutic relevance and interaction
risks, MMM combines ELF-derived features that encode global electronic
properties with a bipartite graph encoder that models local substructure
interactions. This design enables learning complementary characteristics of
drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442
substructures), comparing it with several baseline models. In particular, a
comparison with the GNN-based SafeDrug model demonstrates statistically
significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112),
and the DDI rate (p = 0.0386). These results demonstrate the potential of
ELF-based 3D representations to enhance prediction accuracy and support safer
combinatorial drug prescribing in clinical practice.

</details>


### [91] [GRADE: Personalized Multi-Task Fusion via Group-relative Reinforcement Learning with Adaptive Dirichlet Exploratio](https://arxiv.org/abs/2510.07919)
*Tingfeng Hong,Pingye Ren,Xinlong Xiao,Chao Wang,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.LG

TL;DR: 该摘要描述了一个个性化多目标排序系统架构：包含特征中心与预排模块、用于预测多种用户反馈的多任务学习(MTL)模型、以及作者提出的多任务融合(MTF)模块（GRADE）用于学习个性化权重并在混合排序中应用，最终输出给用户。


<details>
  <summary>Details</summary>
Motivation: 解决在多目标（如点击、转化、时长等）推荐/排序场景中如何对不同目标进行个性化权衡与融合，从而提升用户满意度和业务目标的综合表现。

Method: 体系由四部分组成：1) Feature Center与Prerank用于特征处理与候选生成；2) MTL模型并行预测多种用户反馈信号；3) MTF（即作者提出的GRADE框架）学习每个用户/场景的个性化权重 w1…wn；4) 将权重应用于各目标得分并通过Blended Ranking模型排序输出。

Result: 摘要中主要是架构与方法描述，未给出定量实验结果或对比性能指标，暗示系统可生成个性化混合排序，但缺乏验证细节。

Conclusion: 方法设计合理、模块化清晰，重点在于通过学习个性化权重解决多目标权衡问题。但需要补充实验结果、消融分析与在线/离线评估来验证有效性与可部署性。

Abstract: Overall architecture of the personalized multi-objective ranking system. It
comprises: (1) a Feature Center and Prerank Model for initial feature
processing and candidate generation; (2) a Multi-Task Learning (MTL) model
predicting various user feedback signals; (3) a Multi-Task Fusion (MTF) module
(our proposed GRADE framework) that learns personalized weights ($w_1, \dots,
w_n$); these weights are then applied to calculate final scores and sorted to
generate a blended ranking by the Blended Ranking Model, which ultimately
delivers results to users.

</details>


### [92] [Synergy Between the Strong and the Weak: Spiking Neural Networks are Inherently Self-Distillers](https://arxiv.org/abs/2510.07924)
*Yongqi Ding,Lin Zuo,Mengmeng Jing,Kunshan Yang,Pei He,Tonglan Xie*

Main category: cs.LG

TL;DR: 提出一种基于时间步自蒸馏的 SNN 训练方法，将每个时间步视作子模型，基于置信度划分强弱并进行 Strong2Weak 与 Weak2Strong 两类自蒸馏，提升分类性能与对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SNN 有低功耗潜力但性能落后于 ANN，现有知识蒸馏依赖大教师或额外开销，作者寻找无需外部大型教师的高效蒸馏方案。

Method: 将 SNN 的每个时刻输出视为子模型，评估其输出置信度以区分强子模型与弱子模型；基于此设计强指导弱（Strong2Weak）与弱反蒸馏强（Weak2Strong）两种方案，并提供集成、同时与级联等实现形式。

Result: 在多项试验中自蒸馏方法提高了 SNN 的可区分性与总体性能，同时提升了对抗鲁棒性，表明时间维度内的信息可以被高效利用以加强训练稳定性。

Conclusion: 通过巧妙利用 SNN 的时间特性，提出了一套无需外部教师即可提升性能与鲁棒性的自蒸馏方法，为高性能、低功耗 SNN 的训练提供了有效思路。

Abstract: Brain-inspired spiking neural networks (SNNs) promise to be a low-power
alternative to computationally intensive artificial neural networks (ANNs),
although performance gaps persist. Recent studies have improved the performance
of SNNs through knowledge distillation, but rely on large teacher models or
introduce additional training overhead. In this paper, we show that SNNs can be
naturally deconstructed into multiple submodels for efficient
self-distillation. We treat each timestep instance of the SNN as a submodel and
evaluate its output confidence, thus efficiently identifying the strong and the
weak. Based on this strong and weak relationship, we propose two efficient
self-distillation schemes: (1) \textbf{Strong2Weak}: During training, the
stronger "teacher" guides the weaker "student", effectively improving overall
performance. (2) \textbf{Weak2Strong}: The weak serve as the "teacher",
distilling the strong in reverse with underlying dark knowledge, again yielding
significant performance gains. For both distillation schemes, we offer flexible
implementations such as ensemble, simultaneous, and cascade distillation.
Experiments show that our method effectively improves the discriminability and
overall performance of the SNN, while its adversarial robustness is also
enhanced, benefiting from the stability brought by self-distillation. This
ingeniously exploits the temporal properties of SNNs and provides insight into
how to efficiently train high-performance SNNs.

</details>


### [93] [DISCO: Diversifying Sample Condensation for Efficient Model Evaluation](https://arxiv.org/abs/2510.07959)
*Alexander Rubinstein,Benjamin Raible,Martin Gubri,Seong Joon Oh*

Main category: cs.LG

TL;DR: 提出DISCO方法：通过选择在不同模型上产生最大分歧的样本（top-k）来构建小规模评估集，从而高效预测模型在全量测试集上的表现，避免复杂聚类步骤并取得较好实验结果。


<details>
  <summary>Details</summary>
Motivation: 当前大规模模型评估成本高昂（数千GPU小时），限制了参与门槛并带来环境代价。现有通过锚点子集+从锚点精度映射到最终精度的方法依赖复杂或脆弱的聚类式样本选择，作者认为关键不是样本语义多样性，而是模型响应的多样性。

Method: 提出Diversifying Sample Condensation（DISCO）：基于多模型间的“不一致性”统计，贪心地选择产生最大模型分歧的样本top-k作为评估子集。与依赖全局聚类的方法不同，DISCO使用样本级别的分歧度量，并给出信息论上支持该贪心规则的论证。

Result: 在MMLU、Hellaswag、Winogrande和ARC等数据集上，DISCO在性能预测任务上优于此前方法，达到或接近最先进水平。作者公开了代码库以便复现。

Conclusion: 通过关注模型响应的不一致性而非输入表面的多样性，可以更简洁高效地构造小规模评估集，从而显著降低评估成本并保持甚至提升性能预测精度。

Abstract: Evaluating modern machine learning models has become prohibitively expensive.
Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.
Costly evaluation reduces inclusivity, slows the cycle of innovation, and
worsens environmental impact. The typical approach follows two steps. First,
select an anchor subset of data. Second, train a mapping from the accuracy on
this subset to the final test result. The drawback is that anchor selection
depends on clustering, which can be complex and sensitive to design choices. We
argue that promoting diversity among samples is not essential; what matters is
to select samples that $\textit{maximise diversity in model responses}$. Our
method, $\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k
samples with the greatest model disagreements. This uses greedy, sample-wise
statistics rather than global clustering. The approach is conceptually simpler.
From a theoretical view, inter-model disagreement provides an
information-theoretically optimal rule for such greedy selection.
$\textbf{DISCO}$ shows empirical gains over prior methods, achieving
state-of-the-art results in performance prediction across MMLU, Hellaswag,
Winogrande, and ARC. Code is available here:
https://github.com/arubique/disco-public.

</details>


### [94] [PRESCRIBE: Predicting Single-Cell Responses with Bayesian Estimation](https://arxiv.org/abs/2510.07964)
*Jiabei Cheng,Changxi Chi,Jingbo Zhou,Hongyi Xin,Jun Xia*

Main category: cs.LG

TL;DR: 提出PRESCRIBE：一种多变量深度证据回归框架，同时估计基因干扰预测的模型不确定性（epistemic）和数据不确定性（aleatoric），用于给出置信度评分并过滤不可信预测，在实验上将准确率提升约3%。


<details>
  <summary>Details</summary>
Motivation: 在单细胞基因干扰预测中，预测训练集中未见基因的效果非常重要。由于基因相似性和实验数据质量分别引入模型和数据不确定性，联合量化这两类不确定性有助于判断预测可靠性并减少错误推断带来的风险。

Method: 提出PRESCRIBE：基于多变量深度证据回归（evidential regression）的方法，模型输出同时表征均值和不确定性（分离epistemic与aleatoric），以生成每个预测的置信度分数。框架可对多基因响应进行联合建模，并利用贝叶斯估计视角构建证据参数化。

Result: PRESCRIBE产出的置信度分数与实测准确度高度相关，可用于筛选不可信预测。与可比基线方法相比，过滤后或整体上取得持续的性能提升，实验证明准确率平均提高超过3%。

Conclusion: 联合估计模型和数据不确定性能对单细胞基因干扰预测的可信度评估提供有力支持，从而提高下游决策的稳健性与性能。该方法对未知基因的泛化和预测筛选具有实用价值。

Abstract: In single-cell perturbation prediction, a central task is to forecast the
effects of perturbing a gene unseen in the training data. The efficacy of such
predictions depends on two factors: (1) the similarity of the target gene to
those covered in the training data, which informs model (epistemic)
uncertainty, and (2) the quality of the corresponding training data, which
reflects data (aleatoric) uncertainty. Both factors are critical for
determining the reliability of a prediction, particularly as gene perturbation
is an inherently stochastic biochemical process. In this paper, we propose
PRESCRIBE (PREdicting Single-Cell Response wIth Bayesian Estimation), a
multivariate deep evidential regression framework designed to measure both
sources of uncertainty jointly. Our analysis demonstrates that PRESCRIBE
effectively estimates a confidence score for each prediction, which strongly
correlates with its empirical accuracy. This capability enables the filtering
of untrustworthy results, and in our experiments, it achieves steady accuracy
improvements of over 3% compared to comparable baselines.

</details>


### [95] [Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training](https://arxiv.org/abs/2510.07980)
*Qinglun Li,Yingqi Liu,Miao Zhang,Xiaochun Cao,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: 本文用稳定性分析给出多跳Gossip步长(MGS)在去中心化训练中的泛化误差与过剩误差上界，说明MGS能以指数速度降低优化误差，但即使MGS趋于无穷，与集中式小批量SGD仍存在不可忽略的泛化差距；此外首次在无有界梯度假设下统一分析学习率、数据异质性、节点数、每节点样本数与通信拓扑对MGS泛化性的影响，并通过CIFAR实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 去中心化训练虽通信高效，但性能常劣于集中式。MGS作为一种连接两者的简单有效方法能显著缩小性能差，但其理论机理及是否能完全消除差距尚不清楚，故需要用理论分析揭示MGS对泛化与优化误差的影响并量化与集中式的差距。

Method: 采用稳定性分析框架，在非凸条件且不假设有界梯度的场景下推导MGS的泛化误差与过剩误差上界，分析学习率、数据异质性、节点数、每节点样本数、通信拓扑等因素的影响，并给出MGS对优化误差的指数收敛速率和相应的误差界表达式。

Result: 1) MGS能以指数速率减少优化误差，上界从而快速收紧泛化误差；2) 即便MGS步数趋于无穷，仍存在与集中式小批量SGD的非零泛化差，分别给出两者的渐近阶：集中式为O(T^{cβ/(cβ+1)}/(n m))，去中心化为O(T^{2cβ/(2cβ+2)}/(n m^{1/(2cβ+2)}))；3) 展示了学习率、异质性、节点数、样本数和拓扑如何影响泛化，并在CIFAR上做了支持性实验。

Conclusion: MGS确实能显著缩小去中心化与集中式训练的差距并加速优化，但不能完全消除泛化差异；本文的统一理论分析为去中心化训练的设计与调参提供了指导，实验结果与理论一致。

Abstract: Decentralized training removes the centralized server, making it a
communication-efficient approach that can significantly improve training
efficiency, but it often suffers from degraded performance compared to
centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective
bridge between decentralized and centralized training, significantly reducing
experiment performance gaps. However, the theoretical reasons for its
effectiveness and whether this gap can be fully eliminated by MGS remain open
questions. In this paper, we derive upper bounds on the generalization error
and excess error of MGS using stability analysis, systematically answering
these two key questions. 1). Optimization Error Reduction: MGS reduces the
optimization error bound at an exponential rate, thereby exponentially
tightening the generalization error bound and enabling convergence to better
solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a
non-negligible gap in generalization error remains compared to centralized
mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in
centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n
m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the
first unified analysis of how factors like learning rate, data heterogeneity,
node count, per-node sample size, and communication topology impact the
generalization of MGS under non-convex settings without the bounded gradients
assumption, filling a critical theoretical gap in decentralized training.
Finally, promising experiments on CIFAR datasets support our theoretical
findings.

</details>


### [96] [DemandCast: Global hourly electricity demand forecasting](https://arxiv.org/abs/2510.08000)
*Kevin Steijn,Vamsi Priya Goli,Enrico Antonini*

Main category: cs.LG

TL;DR: 提出一种基于XGBoost的电力负荷预测框架，整合历史负荷、气象和社会经济变量，使用大规模跨国时序数据集与时间切分评估，对样本外表现和可扩展性有较好表现。


<details>
  <summary>Details</summary>
Motivation: 支撑能源系统规划与政策制定，需要在不同地理区域下提供精确、可扩展且具鲁棒性的短中期电力需求预测，以应对能源转型带来的不确定性。

Method: 采用梯度提升树（XGBoost），输入为归一化的历史电力负荷与丰富的气象和社会经济特征。构建多年跨国大规模数据集，并采用时间维度切分（temporal split）进行训练/验证/测试以评估样本外性能。

Result: 模型在所构建的数据集上实现了较高的预测精度并具有可扩展性，证明了将多源外生变量与树模型结合可提升跨区域负荷预测性能。

Conclusion: 该框架为能源规划者和政策制定者提供了有价值的需求预测工具，能在多区域、多年尺度上实现稳健的负荷预测，助力应对全球能源转型挑战。

Abstract: This paper presents a machine learning framework for electricity demand
forecasting across diverse geographical regions using the gradient boosting
algorithm XGBoost. The model integrates historical electricity demand and
comprehensive weather and socioeconomic variables to predict normalized
electricity demand profiles. To enable robust training and evaluation, we
developed a large-scale dataset spanning multiple years and countries, applying
a temporal data-splitting strategy that ensures benchmarking of out-of-sample
performance. Our approach delivers accurate and scalable demand forecasts,
providing valuable insights for energy system planners and policymakers as they
navigate the challenges of the global energy transition.

</details>


### [97] [Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training](https://arxiv.org/abs/2510.08008)
*Ruizhe Wang,Yucheng Ding,Xiao Liu,Yaoxiang Wang,Peng Cheng,Baining Guo,Zhengjun Zha,Yeyun Gong*

Main category: cs.LG

TL;DR: 论文提出通过扩展已训练检查点（增加深度和宽度）并继续训练来“回收”已投入的预训练计算成本，在Mixture-of-Experts模型上用正交增长策略（插入式层复制与带噪声的专家复制）显著提升了最终精度。


<details>
  <summary>Details</summary>
Motivation: 预训练大型语言模型计算开销极大，已有大量计算投入在已训练的检查点上但未被充分利用。作者希望有效复用这些“沉没成本”，以减少总计算资源需求并提高训练经济性。

Method: 针对已收敛的MoE模型，提出正交增长（orthogonal growth）方法：深度增长用插入/位置间层复制（interpositional layer copying），宽度增长用带微小噪声的专家复制（expert duplication with injected noise）。通过大规模缩放实验搜索最佳增长时机和策略，比较在不同检查点序列上继续训练的效果。

Result: 在扩展到70B参数并在超过1T训练tokens的设置下，采用检查点回收策略在相同额外计算预算下相比从头训练取得了10.66%的精度提升。实验证明最终精度与先前已投入的沉没成本呈强正相关。

Conclusion: 检查点回收为更经济的LLM预训练提供了可行路径：利用已有训练投资通过结构性扩展继续训练，可在保持计算预算不变的情况下显著提升模型性能。

Abstract: The rapidly increasing computational cost of pretraining Large Language
Models necessitates more efficient approaches. Numerous computational costs
have been invested in existing well-trained checkpoints, but many of them
remain underutilized due to engineering constraints or limited model capacity.
To efficiently reuse this "sunk" cost, we propose to recycle pretrained
checkpoints by expanding their parameter counts and continuing training. We
propose orthogonal growth method well-suited for converged Mixture-of-Experts
model: interpositional layer copying for depth growth and expert duplication
with injected noise for width growth. To determine the optimal timing for such
growth across checkpoints sequences, we perform comprehensive scaling
experiments revealing that the final accuracy has a strong positive correlation
with the amount of sunk cost, indicating that greater prior investment leads to
better performance. We scale our approach to models with 70B parameters and
over 1T training tokens, achieving 10.66% accuracy gain over training from
scratch under the same additional compute budget. Our checkpoint recycling
approach establishes a foundation for economically efficient large language
model pretraining.

</details>


### [98] [Accelerated Evolving Set Processes for Local PageRank Computation](https://arxiv.org/abs/2510.08010)
*Binbin Huang,Luo Luo,Yanghua Xiao,Deqing Yang,Baojian Zhou*

Main category: cs.LG

TL;DR: 提出一种基于嵌套演化集过程的框架，通过局部化的不精确近端点迭代加速个性化PageRank（PPR）计算，并证明在常数R下时间复杂度上界为 min{~O(R^2/ε^2), ~O(m)}，且仅需求解 ~O(1/√α) 个简化线性系统，从而在1/ε^2 << m时得到与图规模无关的总体复杂度 ~O(R^2/(√α ε^2))。实验证明早期收敛显著。


<details>
  <summary>Details</summary>
Motivation: 目标是加速PPR的近似计算，尤其希望得到与图规模无关或弱依赖图规模的复杂度界，并解析此前文献中的一个未解猜想。

Method: （补充）通过引入常数R来刻画演化集过程的局部性特性，并用此参数得到复杂度上界，整体算法为逐阶段求解若干局部线性子问题。

Result: 给出时间复杂度上界 min{~O(R^2/ε^2), ~O(m)}，并证明只需 ~O(1/√α) 个子问题，从而在适当参数区间内得到总体复杂度 ~O(R^2/(√α ε^2))，理论上解析了一个开放猜想；实验在真实图上展示了方法在早期迭代阶段的快速收敛性。

Conclusion: 所提框架在理论上可实现与图大小无关的PPR近似计算复杂度（受R和α影响），并在实验上展现出实用性，但对R的依赖、常数与对数因子、以及在不同图与参数下的稳健性仍需进一步验证。

Abstract: This work proposes a novel framework based on nested evolving set processes
to accelerate Personalized PageRank (PPR) computation. At each stage of the
process, we employ a localized inexact proximal point iteration to solve a
simplified linear system. We show that the time complexity of such localized
methods is upper bounded by $\min\{\tilde{\mathcal{O}}(R^2/\epsilon^2),
\tilde{\mathcal{O}}(m)\}$ to obtain an $\epsilon$-approximation of the PPR
vector, where $m$ denotes the number of edges in the graph and $R$ is a
constant defined via nested evolving set processes. Furthermore, the algorithms
induced by our framework require solving only
$\tilde{\mathcal{O}}(1/\sqrt{\alpha})$ such linear systems, where $\alpha$ is
the damping factor. When $1/\epsilon^2\ll m$, this implies the existence of an
algorithm that computes an $\ epsilon $-approximation of the PPR vector with an
overall time complexity of $\tilde{\mathcal{O}}\left(R^2 /
(\sqrt{\alpha}\epsilon^2)\right)$, independent of the underlying graph size.
Our result resolves an open conjecture from existing literature. Experimental
results on real-world graphs validate the efficiency of our methods,
demonstrating significant convergence in the early stages.

</details>


### [99] [Unsupervised Radio Map Construction in Mixed LoS/NLoS Indoor Environments](https://arxiv.org/abs/2510.08015)
*Zheng Xing,Junting Chen*

Main category: cs.LG

TL;DR: 提出一种基于隐马尔可夫模型(HMM)的框架，直接从信道传播序列恢复用户轨迹并构建无线电图，无需位置标注采集；对MIMO-OFDM信道在功率、时延、角度维度分别建模，并对LOS/NLOS分别处理，用户轨迹用高斯-马尔可夫模型；仿真结果在室内环境平均定位误差0.65米，优于KNN、SVM、DNN等监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有无线电图构建依赖昂贵的带位置标注的CSI数据采集，目标是消除位置校准，直接从信道序列恢复采集轨迹，从而降低成本并提高实用性。

Method: 建立HMM框架：状态表示位置（轨迹演化用高斯-马尔可夫模型），观测为条件化的MIMO信道传播矩阵。对信道在功率、时延、角度三个独立子空间建模，并针对LOS和NLOS分别设定不同观测模型；联合优化信道参数、运动模型参数及LOS/NLOS判别。

Result: 在模拟的MIMO-OFDM系统（多天线ULAs）上验证，所提方法在室内LOS/NLOS混合区域的平均定位精度约为0.65米；重建的无线电图用于定位时，误差低于传统监督学习方法（KNN、SVM、DNN）。

Conclusion: 该方法证明了无需位置标注也能基于信道序列恢复采集轨迹并构建高精度无线电图，适用于复杂室内LOS/NLOS环境，能降低数据采集成本并提高定位性能。

Abstract: Radio maps are essential for enhancing wireless communications and
localization. However, existing methods for constructing radio maps typically
require costly calibration processes to collect location-labeled channel state
information (CSI) datasets. This paper aims to recover the data collection
trajectory directly from the channel propagation sequence, eliminating the need
for location calibration. The key idea is to employ a hidden Markov model
(HMM)-based framework to conditionally model the channel propagation matrix,
while simultaneously modeling the location correlation in the trajectory. The
primary challenges involve modeling the complex relationship between channel
propagation in multiple-input multiple-output (MIMO) networks and geographical
locations, and addressing both line-of-sight (LOS) and non-line-of-sight (NLOS)
indoor conditions. In this paper, we propose an HMM-based framework that
jointly characterizes the conditional propagation model and the evolution of
the user trajectory. Specifically, the channel propagation in MIMO networks is
modeled separately in terms of power, delay, and angle, with distinct models
for LOS and NLOS conditions. The user trajectory is modeled using a
Gaussian-Markov model. The parameters for channel propagation, the mobility
model, and LOS/NLOS classification are optimized simultaneously. Experimental
validation using simulated MIMO-Orthogonal Frequency-Division Multiplexing
(OFDM) networks with a multi-antenna uniform linear arrays (ULA) configuration
demonstrates that the proposed method achieves an average localization accuracy
of 0.65 meters in an indoor environment, covering both LOS and NLOS regions.
Moreover, the constructed radio map enables localization with a reduced error
compared to conventional supervised methods, such as k-nearest neighbors (KNN),
support vector machine (SVM), and deep neural network (DNN).

</details>


### [100] [Approximate Domain Unlearning for Vision-Language Models](https://arxiv.org/abs/2510.08132)
*Kodai Kawamura,Yuta Goto,Rintaro Yanagi,Hirokatsu Kataoka,Go Irie*

Main category: cs.LG

TL;DR: 提出“近似域遗忘（ADU）”问题：在预训练视觉-语言模型中有选择地降低对指定图像域（如插画）的识别能力，同时保持对其他域（如真实图像）的识别性能；通过显式解缠域分布并自适应捕捉实例级域信息的方法来实现，实验优于现有基于VLM微调的基线。


<details>
  <summary>Details</summary>
Motivation: 预训练VLM泛化能力强，但同时保留对下游任务不必要或有害的信息，既影响计算效率也有潜在泄露风险。现有“类遗忘”不够细粒度，实际应用（如自动驾驶）需要按域而非按类选择性忘记，比如要识别真实车辆但忽略广告插图车辆。

Method: 提出ADU问题并提出解决方案：识别到域分布在特征空间高度纠缠，直接惩罚目标域无效；因此设计方法显式解缠域分布并自适应地捕捉实例级域信息，从而在保留其他域性能的同时降低目标域识别率。

Result: 在大量实验中，所提方法在目标域识别下降和非目标域性能保持之间取得了更好的权衡，优于基于VLM微调的多种基线。作者公开了代码。

Conclusion: ADU为更实用的细粒度“近似遗忘”设定提供了可行方案，通过解缠域表示和实例级适配实现对指定域的选择性弱化，提升了应用场景的安全性和可控性。

Abstract: Pre-trained Vision-Language Models (VLMs) exhibit strong generalization
capabilities, enabling them to recognize a wide range of objects across diverse
domains without additional training. However, they often retain irrelevant
information beyond the requirements of specific downstream tasks, raising
concerns about computational efficiency and potential information leakage. This
has motivated growing interest in approximate unlearning, which aims to
selectively remove unnecessary knowledge while preserving overall model
performance. Existing approaches to approximate unlearning have primarily
focused on class unlearning, where a VLM is retrained to fail to recognize
specified object classes while maintaining accuracy for others. However, merely
forgetting object classes is often insufficient in practical applications. For
instance, an autonomous driving system should accurately recognize real cars
while avoiding misrecognition of illustrated cars depicted in roadside
advertisements as real cars, which could be hazardous. In this paper, we
introduce Approximate Domain Unlearning (ADU), a novel problem setting that
requires reducing recognition accuracy for images from specified domains (e.g.,
illustration) while preserving accuracy for other domains (e.g., real). ADU
presents new technical challenges: due to the strong domain generalization
capability of pre-trained VLMs, domain distributions are highly entangled in
the feature space, making naive approaches based on penalizing target domains
ineffective. To tackle this limitation, we propose a novel approach that
explicitly disentangles domain distributions and adaptively captures
instance-specific domain information. Extensive experiments show that our
approach outperforms baselines built upon VLM tuning techniques, paving the way
for practical and fine-grained unlearning in VLMs. Code:
https://kodaikawamura.github.io/Domain_Unlearning/.

</details>


### [101] [Arbitrary Entropy Policy Optimization: Entropy Is Controllable in Reinforcement Finetuning](https://arxiv.org/abs/2510.08141)
*Chen Wang,Zhaochun Li,Jionghao Bai,Yuzhi Zhang,Shisheng Cui,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: 提出AEPO，一种通过对温度调整分布上应用REINFORCE策略梯度并调节温度来精确控制熵，从而消除GRPO的熵崩塌问题的强化微调方法。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的强化微调（RFT）中，广泛使用的GRPO会出现熵单调下降、探索消失和策略过早收敛的“熵崩塌”。现有以熵正则化为主的办法只能部分缓解且引入偏差与不稳定性，同时熵、探索与性能之间的关系仍不清晰。

Method: 提出Arbitrary Entropy Policy Optimization（AEPO）：用在温度调整分布上的REINFORCE策略梯度替代传统熵奖励，并通过温度调节稳定熵。方法由三部分设计组成：将策略梯度作为正则化、将目标分布作为正则化项、以及把REINFORCE作为正则化器，旨在在不扭曲优化目标的情况下实现精确熵控制。

Result: 实验表明AEPO可以（1）将熵稳定在任意目标水平，从根本上消除GRPO的熵崩塌；（2）发现性能随熵呈先增后减的非单调关系，澄清了熵、探索与推理性能之间的联系；（3）方法可推广至更广泛的RFT范式，即更优的目标分布也能作为REINFORCE正则器使用。

Conclusion: AEPO提供了一种无偏且稳定的熵控制手段，改进了强化微调的探索与性能平衡，并扩展了使用目标分布作为正则化器的思路，需进一步在理论证明、超参敏感性与更大规模任务上验证其普适性。

Abstract: Reinforcement finetuning (RFT) is essential for enhancing the reasoning
capabilities of large language models (LLM), yet the widely adopted Group
Relative Policy Optimization (GRPO) suffers from entropy collapse, where
entropy monotonically decreases, exploration vanishes, and policies converge
prematurely. Existing entropy-regularized methods only partially alleviate this
issue while introducing bias and instability, leaving entropy control
unresolved and the connection between entropy, exploration, and performance
unclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which
eliminates entropy collapse by replacing entropy bonuses with REINFORCE policy
gradient on temperature-adjusted distributions and stabilizing entropy through
temperature regulation. AEPO integrates three key designs: policy gradient as
regularization, distribution as regularization, and REINFORCE as
regularization, enabling precise entropy control without distorting
optimization. Experiments demonstrate three major contributions: AEPO (1)
stabilizes entropy at arbitrary target levels, effectively removing collapse in
GRPO; (2) reveals a non-monotonic relation where performance first improves
then declines with increasing entropy, clarifying the link between entropy,
exploration, and reasoning; and (3) generalizes beyond entropy, providing a
broader RFT paradigm where superior target distributions can serve as REINFORCE
regularizers.

</details>


### [102] [Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning](https://arxiv.org/abs/2510.08146)
*Aman Sharma,Paras Chopra*

Main category: cs.LG

TL;DR: 论文提出用基于token概率的Shannon熵作为置信度信号来实现推理时的早停，能节省25-50%计算量并保持准确率；指出这种熵校准是现代后训练/推理优化模型的一个新兴特性，而在标准指令微调或预训练模型上缺失。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长推理链上消耗大量token与计算，若模型能在较早阶段“知道”答案，则可通过可靠的置信度度量提前停止生成以节省资源。作者认为熵是一个自然、廉价的置信度信号，且在经过后训练优化的推理模型中表现为可利用的特性。

Method: 使用token级别的log-prob计算Shannon熵，设定熵阈值作为早停条件。阈值可通过少量示例一次性估算。对比不同模型家族（推理优化模型 vs 指令微调/预训练模型）在相同阈值或基于示例计算阈值下的性能与计算成本。

Result: 在所测推理优化模型上，采用熵早停可实现约25-50%的计算节省且不显著降低任务准确率；而Llama 3.3 70B等标准微调/预训练模型缺乏同样的熵-置信度对应性，早停效果不佳。作者称这一置信度对齐是后训练推理优化的一个表征性差异。

Conclusion: 熵作为置信度信号在现代后训练推理模型中是一种可被利用的 emergent 特性，可用于高效地减少推理成本。论文强调该机制在架构/训练前后存在差异，并建议可用少量示例快速设定早停阈值以部署。

Abstract: We introduce a simple, yet novel entropy-based framework to drive token
efficiency in large language models during reasoning tasks. Our approach uses
Shannon entropy from token-level logprobs as a confidence signal to enable
early stopping, achieving 25-50% computational savings while maintaining task
accuracy. Crucially, we demonstrate that entropy-based confidence calibration
represents an emergent property of advanced post-training optimization present
in modern reasoning models but notably absent in standard instruction-tuned and
pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop
reasoning varies from model to model but can be calculated easily in one shot
using only a few examples from existing reasoning datasets. Our results
indicate that advanced reasoning models often know that they've gotten a
correct answer early on, and that this emergent confidence awareness can be
exploited to save tokens and reduce latency. The framework demonstrates
consistent performance across reasoning-optimized model families with 25-50%
computational cost reduction while preserving accuracy, revealing that
confidence mechanisms represent a distinguishing characteristic of modern
post-trained reasoning systems versus their predecessors.

</details>


### [103] [Unsupervised Multi-Source Federated Domain Adaptation under Domain Diversity through Group-Wise Discrepancy Minimization](https://arxiv.org/abs/2510.08150)
*Larissa Reichart,Cem Ata Baykara,Ali Burak Ünal,Mete Akgün,Harlin Lee*

Main category: cs.LG

TL;DR: 提出GALA，一个可扩展且稳健的联邦无监督多源域自适应框架，通过组间差异最小化目标和基于质心的温度权重策略，避免二次配对计算并动态加权源域，实现对大量异构源域的并行稳定训练，在新构建的Digit-18基准和常规模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有分布式UMDA方法在隐私保护下避免原始数据共享，但通常只适用于少量源域，面对大量异构源域时计算开销大、性能不稳定或难以收敛。需要一种既能扩展到大量源域又能保持训练稳定性和效率的方法。

Method: GALA包含两大关键组件：1) 组间差异最小化目标（inter-group discrepancy minimization），通过分组近似代替全对域间对齐，避免了二次复杂度的配对计算；2) 温度控制的质心加权策略（temperature-controlled, centroid-based weighting），基于源域质心与目标的对齐程度动态调整源域权重，优先利用更相关的源域。二者结合支持并行训练以提高可扩展性与稳定性。

Result: 在新构建的Digit-18（18个数字数据集）高多样性基准上以及标准多源域自适应数据集上，GALA在稳定性和收敛性方面显著优于现有方法，在多源多样性场景中常常达到或超过最先进方法的性能，且其他方法在该设置下可能无法收敛。

Conclusion: GALA通过高效的分组对齐和动态加权策略，解决了联邦UMDA在大量异构源域下的可扩展性与稳定性问题，是一种在高多样性、多源场景下更实用的解决方案。

Abstract: Unsupervised multi-source domain adaptation (UMDA) aims to learn models that
generalize to an unlabeled target domain by leveraging labeled data from
multiple, diverse source domains. While distributed UMDA methods address
privacy constraints by avoiding raw data sharing, existing approaches typically
assume a small number of sources and fail to scale effectively. Increasing the
number of heterogeneous domains often makes existing methods impractical,
leading to high computational overhead or unstable performance. We propose
GALA, a scalable and robust federated UMDA framework that introduces two key
components: (1) a novel inter-group discrepancy minimization objective that
efficiently approximates full pairwise domain alignment without quadratic
computation; and (2) a temperature-controlled, centroid-based weighting
strategy that dynamically prioritizes source domains based on alignment with
the target. Together, these components enable stable and parallelizable
training across large numbers of heterogeneous sources. To evaluate performance
in high-diversity scenarios, we introduce Digit-18, a new benchmark comprising
18 digit datasets with varied synthetic and real-world domain shifts. Extensive
experiments show that GALA consistently achieves competitive or
state-of-the-art results on standard benchmarks and significantly outperforms
prior methods in diverse multi-source settings where others fail to converge.

</details>


### [104] [Beyond Sub-6 GHz: Leveraging mmWave Wi-Fi for Gait-Based Person Identification](https://arxiv.org/abs/2510.08160)
*Nabeel Nisar Bhat,Maksim Karnaukh,Jakob Struye,Rafael Berkvens,Jeroen Famaey*

Main category: cs.LG

TL;DR: 本文比对了子6 GHz与毫米波Wi‑Fi在被动人体识别（基于步态）的性能，使用COTS设备采集同步数据、相同训练流程，并发现即使在10 Hz采样下，mmWave结合背景去除可在20人识别上达到约91.2%准确率。


<details>
  <summary>Details</summary>
Motivation: 探究毫米波Wi‑Fi相较于传统子6 GHz频段在被动人体识别任务中的潜在优势（更高空间分辨率），并以实证数据评估两者的比较性能。

Method: 构建首个在室内环境中同步采集子6 GHz与毫米波Wi‑Fi信号的数据集；采用相同的端到端深度学习训练管线和模型配置进行公平比较；引入有效的背景减除步骤；低采样率（10 Hz）下进行评估。

Result: mmWave信号在低采样率条件下表现优异：在20名受试者的数据上达到91.2%识别准确率，表明毫米波在步态基础的被动识别上具有明显优势。

Conclusion: 商用COTS毫米波Wi‑Fi在被动人体识别方面具有可观潜力，即便在较低采样率下，结合背景去除和端到端深度学习也能实现高准确率；该工作为频段选择和系统设计提供了实证参考。

Abstract: Person identification plays a vital role in enabling intelligent,
personalized, and secure human-computer interaction. Recent research has
demonstrated the feasibility of leveraging Wi-Fi signals for passive person
identification using a person's unique gait pattern. Although most existing
work focuses on sub-6 GHz frequencies, the emergence of mmWave offers new
opportunities through its finer spatial resolution, though its comparative
advantages for person identification remain unexplored. This work presents the
first comparative study between sub-6 GHz and mmWave Wi-Fi signals for person
identification with commercial off-the-shelf (COTS) Wi-Fi, using a novel
dataset of synchronized measurements from the two frequency bands in an indoor
environment. To ensure a fair comparison, we apply identical training pipelines
and model configurations across both frequency bands. Leveraging end-to-end
deep learning, we show that even at low sampling rates (10 Hz), mmWave Wi-Fi
signals can achieve high identification accuracy (91.2% on 20 individuals) when
combined with effective background subtraction.

</details>


### [105] [Bidirectional Representations Augmented Autoregressive Biological Sequence Generation:Application in De Novo Peptide Sequencing](https://arxiv.org/abs/2510.08169)
*Xiang Zhang,Jiaqi Wei,Zijie Qiu,Sheng Xu,Zhi Jin,ZhiQiang Gao,Nanqing Dong,Siqi Sun*

Main category: cs.LG

TL;DR: 提出了一种混合AR/NAR框架：共享编码器、两个解码器（NAR学习双向潜特征，AR通过交叉解码器注意力迭代查询并融合这些特征）以及针对性的训练策略（重要性退火与跨解码器梯度阻断）。在九物种的de novo肽段测序基准上明显优于AR/NAR基线，兼具AR稳定性与NAR的全局上下文理解。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在生物序列生成中受限于单向生成，难以捕捉全局双向依赖；非自回归模型虽能建模双向信息但生成连贯性与扩展性不足。需要一种既保留AR连贯性又能利用双向上下文的方案。

Method: 构建共享输入编码器，接两个解码器：一个NAR解码器学习潜在的双向生物学特征；一个AR解码器在生成时通过新设计的交叉解码器注意力模块迭代查询并融合NAR提供的双向特征。训练中采用重要性退火平衡多重目标，并通过跨解码器梯度阻断保证学习稳定与专注。

Result: 在包含九个物种的de novo肽段测序任务上，模型显著优于纯AR与纯NAR基线，表现出更高的生成准确率与更强的跨物种泛化能力。

Conclusion: 提出了一种可将双向上下文融入AR生成的有效架构与训练方法，填补了AR稳定性与NAR全局理解之间的空白，对复杂生物序列生成具有实用价值。代码已开源。

Abstract: Autoregressive (AR) models, common in sequence generation, are limited in
many biological tasks such as de novo peptide sequencing and protein modeling
by their unidirectional nature, failing to capture crucial global bidirectional
token dependencies. Non-Autoregressive (NAR) models offer holistic,
bidirectional representations but face challenges with generative coherence and
scalability. To transcend this, we propose a hybrid framework enhancing AR
generation by dynamically integrating rich contextual information from
non-autoregressive mechanisms. Our approach couples a shared input encoder with
two decoders: a non-autoregressive one learning latent bidirectional biological
features, and an AR decoder synthesizing the biological sequence by leveraging
these bidirectional features. A novel cross-decoder attention module enables
the AR decoder to iteratively query and integrate these bidirectional features,
enriching its predictions. This synergy is cultivated via a tailored training
strategy with importance annealing for balanced objectives and cross-decoder
gradient blocking for stable, focused learning. Evaluations on a demanding
nine-species benchmark of de novo peptide sequencing show that our model
substantially surpasses AR and NAR baselines. It uniquely harmonizes AR
stability with NAR contextual awareness, delivering robust, superior
performance on diverse downstream data. This research advances biological
sequence modeling techniques and contributes a novel architectural paradigm for
augmenting AR models with enhanced bidirectional understanding for complex
sequence generation. Code is available at https://github.com/BEAM-Labs/denovo.

</details>


### [106] [Long-tailed Recognition with Model Rebalancing](https://arxiv.org/abs/2510.08177)
*Jiaan Luo,Feng Hong,Qiang Hu,Xiaofeng Cao,Feng Liu,Jiangchao Yao*

Main category: cs.LG

TL;DR: 提出了一种名为MORE（Model Rebalancing）的框架，通过在参数空间引入低秩参数组件并配合特定损失与正弦重权重计划，在不增加模型复杂度与推理开销的情况下缓解长尾分布对模型能力分配的影响，从而在多类与多标签长尾任务上显著提升尾类性能。


<details>
  <summary>Details</summary>
Motivation: 目前针对长尾问题的主流策略（数据增强、损失重平衡、解耦训练等）在广泛场景尤其是多标签长尾识别中难以持续稳定改进。作者认为核心问题是模型参数空间在长尾情形下未被合理分配，导致模型泛化到尾类受限，因此希望直接从模型容量与参数分配角度出发进行干预。

Method: 提出MORE：在原有模型参数基础上额外学习一个低秩的参数组件，用以在训练阶段调整参数空间的分配。该组件由专门设计的损失项和一个正弦形的重权重调度策略引导学习，以优先提升尾类表现。设计上保证在推理阶段整体参数不增加或不影响复杂度（例如通过参数融合或仅训练附加组件），使其可作为即插即用模块与现有不平衡方法结合。

Result: 在多种长尾基准（包括多类与多标签任务）上进行广泛实验证明，MORE能显著提高整体泛化能力，尤其在尾类上带来明显增益。同时作者展示MORE与已有数据采样、损失重加权等方法具有互补性，可进一步叠加提升性能。

Conclusion: 从模型容量与参数分配角度对长尾问题进行干预是一条有效路径。MORE通过引入低秩参数组件并结合定制损失与重权重计划，实现了在不增加推理成本前提下改善尾类泛化的目标，具有良好的通用性与可插拔性，值得在长尾场景中推广应用。

Abstract: Long-tailed recognition is ubiquitous and challenging in deep learning and
even in the downstream finetuning of foundation models, since the skew class
distribution generally prevents the model generalization to the tail classes.
Despite the promise of previous methods from the perspectives of data
augmentation, loss rebalancing and decoupled training etc., consistent
improvement in the broad scenarios like multi-label long-tailed recognition is
difficult. In this study, we dive into the essential model capacity impact
under long-tailed context, and propose a novel framework, Model Rebalancing
(MORE), which mitigates imbalance by directly rebalancing the model's parameter
space. Specifically, MORE introduces a low-rank parameter component to mediate
the parameter space allocation guided by a tailored loss and sinusoidal
reweighting schedule, but without increasing the overall model complexity or
inference costs. Extensive experiments on diverse long-tailed benchmarks,
spanning multi-class and multi-label tasks, demonstrate that MORE significantly
improves generalization, particularly for tail classes, and effectively
complements existing imbalance mitigation methods. These results highlight
MORE's potential as a robust plug-and-play module in long-tailed settings.

</details>


### [107] [Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data](https://arxiv.org/abs/2510.08179)
*Feng Hong,Yu Huang,Zihua Zhao,Zhihan Zhou,Jiangchao Yao,Dongsheng Li,Ya Zhang,Yanfeng Wang*

Main category: cs.LG

TL;DR: 本文提出D-SINK框架，通过蒸馏来自两个单项鲁棒辅助模型（一个对标签噪声鲁棒，另一个对类别不平衡鲁棒）的互补信息，使用最优传输优化的标签分配在样本级与分布级同时对齐，从而在长尾含噪数据上显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实数据常同时存在类别不平衡（分布级问题）和标签噪声（样本级问题）。直接用单一方法难以两者兼顾，且难以区分真实小众样本与噪声样本；因此希望利用现有“单项”鲁棒模型的互补优势，避免设计复杂的新方法。

Method: 提出Dual-granularity Sinkhorn Distillation（D-SINK）：利用两个弱辅助模型分别提供噪声鲁棒的样本级预测和不平衡鲁棒的类分布信息，通过基于最优传输（Sinkhorn算法）的替代标签分配，将目标模型的样本预测与噪声鲁棒模型对齐，同时将整体类分布与不平衡鲁棒模型对齐，从而进行蒸馏与融合。

Result: 在多种基准数据集（长尾含噪情形）上进行广泛实验，D-SINK在鲁棒性和最终性能上显著优于单一方法和若干联合基线，证明了双粒度蒸馏策略的有效性。

Conclusion: 将专注于不同粒度问题的已有辅助模型通过最优传输的蒸馏机制结合，能够互补优势、减少冲突，从而在长尾含噪学习任务中实现更强的鲁棒性与更好的性能。

Abstract: Real-world datasets for deep learning frequently suffer from the co-occurring
challenges of class imbalance and label noise, hindering model performance.
While methods exist for each issue, effectively combining them is non-trivial,
as distinguishing genuine tail samples from noisy data proves difficult, often
leading to conflicting optimization strategies. This paper presents a novel
perspective: instead of primarily developing new complex techniques from
scratch, we explore synergistically leveraging well-established, individually
'weak' auxiliary models - specialized for tackling either class imbalance or
label noise but not both. This view is motivated by the insight that class
imbalance (a distributional-level concern) and label noise (a sample-level
concern) operate at different granularities, suggesting that robustness
mechanisms for each can in principle offer complementary strengths without
conflict. We propose Dual-granularity Sinkhorn Distillation (D-SINK), a novel
framework that enhances dual robustness by distilling and integrating
complementary insights from such 'weak', single-purpose auxiliary models.
Specifically, D-SINK uses an optimal transport-optimized surrogate label
allocation to align the target model's sample-level predictions with a
noise-robust auxiliary and its class distributions with an imbalance-robust
one. Extensive experiments on benchmark datasets demonstrate that D-SINK
significantly improves robustness and achieves strong empirical performance in
learning from long-tailed noisy data.

</details>


### [108] [FuelCast: Benchmarking Tabular and Temporal Models for Ship Fuel Consumption](https://arxiv.org/abs/2510.08217)
*Justus Viga,Penelope Mueck,Alexander Löser,Torben Weis*

Main category: cs.LG

TL;DR: 作者发布了一个包含三艘船舶运行与环境数据的新数据集，构建了统一的基线与时序/表格回归基准，并首次将TabPFN（以in‑context learning为特征的基础模型）应用于船舶燃油消耗预测。实验表明：加入环境与时间上下文能显著提升预测精度，TabPFN在所评估方法中略有优势。


<details>
  <summary>Details</summary>
Motivation: 航运业燃料消耗与排放对经济与环境影响重大，但现有研究方法异质且高质量公开数据稀缺，导致模型间难以直接比较与复现，限制了面向舰载实时预测的研究与应用。

Method: （1）公开一个含三艘船运行与环境变量的数据集；（2）定义统一的基准任务，覆盖静态表格回归与时序回归设置；（3）评估多种模型，包括多项基线、时序模型和首次在该任务上尝试的TabPFN基于in‑context learning的基础模型，比较是否包含环境变量和时间上下文的影响。

Result: 各类模型整体表现良好；引入环境条件（如风、波或气象）优于仅用船速的多项式基线；加入时间上下文进一步提升精度；TabPFN在实验中略微领先于其他方法，显示出基于in‑context learning的基础模型在表格任务上的潜力。

Conclusion: 研究展示了面向舰载的、数据驱动燃油预测的可行性，且提供了可复现的数据集与基准，促进该领域后续比较研究。TabPFN和时间/环境特征的使用是提高精度的有效方向。

Abstract: In the shipping industry, fuel consumption and emissions are critical factors
due to their significant impact on economic efficiency and environmental
sustainability. Accurate prediction of ship fuel consumption is essential for
further optimization of maritime operations. However, heterogeneous
methodologies and limited high-quality datasets hinder direct comparison of
modeling approaches. This paper makes three key contributions: (1) we introduce
and release a new dataset
(https://huggingface.co/datasets/krohnedigital/FuelCast) comprising operational
and environmental data from three ships; (2) we define a standardized benchmark
covering tabular regression and time-series regression (3) we investigate the
application of in-context learning for ship consumption modeling using the
TabPFN foundation model - a first in this domain to our knowledge. Our results
demonstrate strong performance across all evaluated models, supporting the
feasibility of onboard, data-driven fuel prediction. Models incorporating
environmental conditions consistently outperform simple polynomial baselines
relying solely on vessel speed. TabPFN slightly outperforms other techniques,
highlighting the potential of foundation models with in-context learning
capabilities for tabular prediction. Furthermore, including temporal context
improves accuracy.

</details>


### [109] [Expressive Value Learning for Scalable Offline Reinforcement Learning](https://arxiv.org/abs/2510.08218)
*Nicolas Espinosa-Dice,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: 提出EVOR，一种在离线强化学习中同时学习表达性策略与表达性价值函数的方法：训练阶段通过流匹配学习正则化Q函数，推理阶段通过对价值进行拒绝采样来提取策略，从而避免BPTT和蒸馏。


<details>
  <summary>Details</summary>
Motivation: 现有将表达性生成模型（如扩散、流匹配）用于离线RL的方法要么依赖计算代价大的反向传播通过时间（BPTT），要么依赖策略蒸馏引入累积误差并限制可扩展性。需要一种无需BPTT或蒸馏、同时可扩展并能利用复杂数据集的离线RL方法。

Method: 提出EVOR：训练阶段以流匹配（flow matching）学习最优且正则化的Q函数，保持表达性价值函数；推理阶段不重新训练策略，而是对表达性价值函数做拒绝采样以提取策略，实现高效的优化、正则化与可扩展搜索。

Result: 在多样化的离线RL任务上，EVOR在性能上超越了基线方法，展示了将表达性价值学习集成到离线RL中的优势。

Conclusion: 通过将表达性价值函数与表达性策略推理相结合，EVOR在不依赖BPTT或策略蒸馏的情况下实现了可扩展的离线RL，表现优于已有基线。

Abstract: Reinforcement learning (RL) is a powerful paradigm for learning to make
sequences of decisions. However, RL has yet to be fully leveraged in robotics,
principally due to its lack of scalability. Offline RL offers a promising
avenue by training agents on large, diverse datasets, avoiding the costly
real-world interactions of online RL. Scaling offline RL to increasingly
complex datasets requires expressive generative models such as diffusion and
flow matching. However, existing methods typically depend on either
backpropagation through time (BPTT), which is computationally prohibitive, or
policy distillation, which introduces compounding errors and limits scalability
to larger base policies. In this paper, we consider the question of how to
develop a scalable offline RL approach without relying on distillation or
backpropagation through time. We introduce Expressive Value Learning for
Offline Reinforcement Learning (EVOR): a scalable offline RL approach that
integrates both expressive policies and expressive value functions. EVOR learns
an optimal, regularized Q-function via flow matching during training. At
inference-time, EVOR performs inference-time policy extraction via rejection
sampling against the expressive value function, enabling efficient
optimization, regularization, and compute-scalable search without retraining.
Empirically, we show that EVOR outperforms baselines on a diverse set of
offline RL tasks, demonstrating the benefit of integrating expressive value
learning into offline RL.

</details>


### [110] [Reinforcement Learning from Probabilistic Forecasts for Safe Decision-Making via Conditional Value-at-Risk Planning](https://arxiv.org/abs/2510.08226)
*Michal Koren,Or Peretz,Tai Dinh,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了不确定性感知马尔可夫决策过程（UAMDP），将贝叶斯预测、后验采样强化学习和基于CVaR的风险约束规划耦合。方法实现在线更新信念、通过Thompson采样生成未来并在风险容忍度下优化策略。理论给出收敛到贝叶斯最优的懊悔界，实证在高频交易和零售库存控制上优于深度学习基线，提升预测精度并改善经济指标（Sharpe比率上升，最大回撤减半）。


<details>
  <summary>Details</summary>
Motivation: 在高风险且动态变化的环境中，单纯最大化期望回报不足以保证安全与稳健，需要系统化地处理模型和环境不确定性，并在探索与风险控制之间做出权衡。

Method: 提出UAMDP框架：1) 使用贝叶斯模型对潜在动力学进行概率建模并在线更新后验；2) 通过Thompson采样从后验中采样动力学并生成未来情景；3) 在每个情景下进行基于CVaR的风险约束规划，优化满足风险容忍度的策略；4) 提出相应的后验采样强化学习算法并证明在标准条件下其懊悔界收敛到贝叶斯最优。

Result: 在两个具有结构性不确定性和经济波动的任务（高频交易、零售库存）上，UAMDP在长时序预测（RMSE降低最多25%，sMAPE降低32%）和经济性能上均优于强大深度学习基线。例如交易任务中Sharpe比率从1.54提高到1.74，最大回撤约减半。

Conclusion: 将校准的概率建模、与后验不确定性一致的探索策略以及风险感知控制整合，可实现更安全、更有利的序列决策，具备良好的鲁棒性与泛化能力。

Abstract: Sequential decisions in volatile, high-stakes settings require more than
maximizing expected return; they require principled uncertainty management.
This paper presents the Uncertainty-Aware Markov Decision Process (UAMDP), a
unified framework that couples Bayesian forecasting, posterior-sampling
reinforcement learning, and planning under a conditional value-at-risk (CVaR)
constraint. In a closed loop, the agent updates its beliefs over latent
dynamics, samples plausible futures via Thompson sampling, and optimizes
policies subject to preset risk tolerances. We establish regret bounds that
converge to the Bayes-optimal benchmark under standard regularity conditions.
We evaluate UAMDP in two domains-high-frequency equity trading and retail
inventory control-both marked by structural uncertainty and economic
volatility. Relative to strong deep learning baselines, UAMDP improves
long-horizon forecasting accuracy (RMSE decreases by up to 25\% and sMAPE by
32\%), and these gains translate into economic performance: the trading Sharpe
ratio rises from 1.54 to 1.74 while maximum drawdown is roughly halved. These
results show that integrating calibrated probabilistic modeling, exploration
aligned with posterior uncertainty, and risk-aware control yields a robust,
generalizable approach to safer and more profitable sequential decision-making.

</details>


### [111] [Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization](https://arxiv.org/abs/2510.08233)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Petr Molodyk,Bo Yuan,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: Proposes Distribution Matching Policy Optimization (DMPO), an RL fine-tuning method for diffusion large language models that matches the model policy to a reward-tilted optimal distribution via cross-entropy; introduces a novel weight baseline subtraction to stabilize training with small batches and shows large improvements on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Diffusion LLMs (dLLMs) can enable higher inference throughput than autoregressive LLMs but currently lag on tasks like reasoning; existing RL methods are not tailored to dLLMs’ unique generation process, so a principled RL approach is needed to close the gap.

Method: Formulate RL fine-tuning as distribution matching: minimize cross-entropy between the dLLM policy and an optimal reward-tilted distribution. Derive theoretical grounding and practical importance-weighted objectives. Identify instability when using small training batches and introduce a weight baseline subtraction technique (plus other practical fixes) to reduce variance and enable stable updates.

Result: DMPO consistently improves reasoning performance without supervised fine-tuning, achieving up to 42.9% accuracy gain over previous SOTA baselines and 55.8% over the base model on multiple benchmarks. Code is released.

Conclusion: Distribution matching is a theoretically principled and empirically effective RL approach for dLLMs; the weight-baseline technique is key for small-batch stability. Future work should examine scaling, sample efficiency, and broader task generalization.

Abstract: Diffusion large language models (dLLMs) are promising alternatives to
autoregressive large language models (AR-LLMs), as they potentially allow
higher inference throughput. Reinforcement learning (RL) is a crucial component
for dLLMs to achieve comparable performance with AR-LLMs on important tasks,
such as reasoning. However, RL algorithms that are well-suited for dLLMs'
unique characteristics have yet to be developed. This paper proposes
Distribution Matching Policy Optimization (DMPO), a principled and
theoretically grounded RL fine-tuning method specifically designed to enhance
the reasoning capabilities of dLLMs by matching the dLLM policy distribution to
the optimal, reward-tilted one through cross-entropy optimization. We identify
a key challenge in the implementation with a small training batch size and
propose several effective solutions through a novel weight baseline subtraction
technique. DMPO exhibits superior performance on multiple reasoning benchmarks
without supervised fine-tuning, with an accuracy improvement of up to $42.9\%$
over previously SOTA baselines and $55.8\%$ over the base model, underscoring
the effectiveness of the distribution matching framework. Our code is available
at https://github.com/yuchen-zhu-zyc/DMPO.

</details>


### [112] [The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models](https://arxiv.org/abs/2510.08236)
*Konrad Löhr,Shuzhou Yuan,Michael Färber*

Main category: cs.LG

TL;DR: 本文使用政治罗盘测试（PCT）评估八种大型语言模型的政治倾向与刻板印象传播，发现普遍偏左且隐性刻板印象通过语言变化更明显。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在信息传播与决策中的影响力增大，需评估其在政治领域的偏见与刻板印象以防止对公共舆论与民主进程的不当影响。

Method: 先用PCT评估模型固有政治倾向；再用带PCT的人设提示探查显性刻板印象；最后用多语种PCT评估隐性刻板印象。

Result: 所有模型普遍呈左倾；不同模型的刻板印象性质与强度差异显著；隐性刻板印象比显性提示更易被触发；多数模型的隐性与显性刻板印象存在一致性。

Conclusion: 研究揭示LLM中政治偏见与刻板印象的复杂交互，提示需在模型开发与部署中加强偏见检测与缓解措施。

Abstract: Large Language Models (LLMs) are increasingly integral to information
dissemination and decision-making processes. Given their growing societal
influence, understanding potential biases, particularly within the political
domain, is crucial to prevent undue influence on public opinion and democratic
processes. This work investigates political bias and stereotype propagation
across eight prominent LLMs using the two-dimensional Political Compass Test
(PCT). Initially, the PCT is employed to assess the inherent political leanings
of these models. Subsequently, persona prompting with the PCT is used to
explore explicit stereotypes across various social dimensions. In a final step,
implicit stereotypes are uncovered by evaluating models with multilingual
versions of the PCT. Key findings reveal a consistent left-leaning political
alignment across all investigated models. Furthermore, while the nature and
extent of stereotypes vary considerably between models, implicit stereotypes
elicited through language variation are more pronounced than those identified
via explicit persona prompting. Interestingly, for most models, implicit and
explicit stereotypes show a notable alignment, suggesting a degree of
transparency or "awareness" regarding their inherent biases. This study
underscores the complex interplay of political bias and stereotypes in LLMs.

</details>


### [113] [Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization](https://arxiv.org/abs/2510.08256)
*Jason Bohne,Pawel Polak,David Rosenberg,Brian Bloniarz,Gary Kazantsev*

Main category: cs.LG

TL;DR: 提出 Mix- 和 MoE-DPO，将 DPO 扩展为软混合模型与专家混合架构，通过引入专家分配的潜变量并以随机变分推断优化 ELBO，从偏好数据中高效学习专门化策略。相较于标准 DPO，具备混合泛化、专家特化和上下文相关的用户对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有 DPO 使用单一模型，难以在多任务或偏好多样性场景中表达不同偏好分布，需要更具表达力和适应性的模型来捕捉多模态偏好。

Method: 在 DPO 框架下引入基于潜变量的混合/ MoE 结构（可选共享底座+专家头或完全独立专家），采用随机变分推断最大化变分下界（ELBO），以稳定高效地训练专家策略并支持输入依赖的软门控（gating）。

Result: 在不同模型规模和多偏好数据集上的实验表明，Mix-/MoE-DPO 在泛化能力、偏好匹配和上下文敏感性上优于标准单模型 DPO，并在参数效率与专门化之间提供可控权衡。

Conclusion: Mix- 和 MoE-DPO 为基于偏好的 LLM 对齐提供了可扩展且灵活的方案，通过混合表达、专家特化与上下文门控提升对多样偏好的适配性，适合需要在参数效率与任务/用户特化间做选择的应用场景。

Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and
effective alternative to reinforcement learning from human feedback (RLHF) for
aligning large language models (LLMs) with user preferences. However, existing
DPO formulations rely on a single monolithic model, which limits their
expressivity in multi-task settings and their adaptability to heterogeneous or
diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a
framework that extends DPO with both soft mixture models and mixture-of-experts
(MoE) architectures, using a stochastic variational inference approach. Our
method introduces a latent-variable model over expert assignments and optimizes
a variational evidence lower bound (ELBO), enabling stable and efficient
learning of specialized expert policies from preference data. Mix- and MoE-DPO
provides three key advantages over standard DPO: (i) generalization via
universal function approximation through mixtures; (ii) reward and policy
specialization through expert components tailored to distinct preference modes;
and (iii) contextual alignment through input-dependent soft gating that enables
user-specific mixture policies. Our framework supports both shared base
architectures with expert-specific policy heads and fully independent expert
models, allowing flexible trade-offs between parameter efficiency and
specialization. We validate our approach on a variety of model sizes and
multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a
powerful and scalable method for preference-based LLM alignment.

</details>


### [114] [Counterfactual Identifiability via Dynamic Optimal Transport](https://arxiv.org/abs/2510.08294)
*Fabio De Sousa Ribeiro,Ainkaran Santhirasekaram,Ben Glocker*

Main category: cs.LG

TL;DR: 提出基于连续时间流（flow matching 与动态最优传输）的多元结果反事实识别条件，证明在标准条件下可得唯一的单调且保秩的运输映射，并在模拟与真实图像上验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 反事实对因果主张至关重要，但高维多元结果的反事实从观测数据中能否可识别仍是开放问题。已有反事实推断工作在效果上有进展但缺乏严格的可识别性保证，从而削弱因果结论。

Method: 利用连续时间流模型和flow matching框架，将反事实映射构建为动态最优传输问题。刻画在若干“标准”条件（包括允许非马尔可夫情形）下，flow matching 可产生唯一、单调且保秩的反事实运输映射，保证推断一致性。

Result: 理论上给出可识别性条件与性质证明；在带有反事实真值的受控场景中验证；在真实图像数据上以若干公理化的反事实健全性指标显示改进。

Conclusion: 为高维多元结果的反事实可识别性建立了基于连续时间流的理论基础，并通过实验展示其在实证评估上的潜力，推进了具有可识别性保证的反事实推断研究。

Abstract: We address the open question of counterfactual identification for
high-dimensional multivariate outcomes from observational data. Pearl (2000)
argues that counterfactuals must be identifiable (i.e., recoverable from the
observed data distribution) to justify causal claims. A recent line of work on
counterfactual inference shows promising results but lacks identification,
undermining the causal validity of its estimates. To address this, we establish
a foundation for multivariate counterfactual identification using
continuous-time flows, including non-Markovian settings under standard
criteria. We characterise the conditions under which flow matching yields a
unique, monotone and rank-preserving counterfactual transport map with tools
from dynamic optimal transport, ensuring consistent inference. Building on
this, we validate the theory in controlled scenarios with counterfactual
ground-truth and demonstrate improvements in axiomatic counterfactual soundness
on real images.

</details>


### [115] [Bridging the Physics-Data Gap with FNO-Guided Conditional Flow Matching: Designing Inductive Bias through Hierarchical Physical Constraints](https://arxiv.org/abs/2510.08295)
*Tsuyoshi Okita*

Main category: cs.LG

TL;DR: Hierarchical physics-informed generative framework that embeds conservation, dynamics, boundary and empirical relations into deep generative models by combining Fourier Neural Operators (FNOs) with Conditional Flow Matching (CFM), yielding more physically consistent and higher-quality time-series generation.


<details>
  <summary>Details</summary>
Motivation: Standard time-series generative models typically ignore domain-specific physical constraints, producing samples that lack physical consistency and may harm downstream tasks. The paper aims to introduce an inductive bias that enforces physical laws at multiple hierarchical levels during probabilistic generation.

Method: Introduce a hierarchical constraint system encoding conservation, dynamics, boundary and empirical relations; use Fourier Neural Operators to learn physical operators and provide corrections; use Conditional Flow Matching for probabilistic generation; integrate FNO-guided corrections and time-dependent hierarchical constraints into the generative process.

Result: Experiments on harmonic oscillators, human activity recognition, and lithium-ion battery degradation demonstrate improvements over baselines: 16.3% higher generation quality, 46% fewer physics violations, and 18.5% better predictive accuracy.

Conclusion: Embedding hierarchical, physics-informed inductive biases into deep generative models improves both statistical sample quality and physical consistency, benefiting downstream predictive tasks and offering a new paradigm for physics-aware time-series generation.

Abstract: Conventional time-series generation often ignores domain-specific physical
constraints, limiting statistical and physical consistency. We propose a
hierarchical framework that embeds the inherent hierarchy of physical
laws-conservation, dynamics, boundary, and empirical relations-directly into
deep generative models, introducing a new paradigm of physics-informed
inductive bias. Our method combines Fourier Neural Operators (FNOs) for
learning physical operators with Conditional Flow Matching (CFM) for
probabilistic generation, integrated via time-dependent hierarchical
constraints and FNO-guided corrections. Experiments on harmonic oscillators,
human activity recognition, and lithium-ion battery degradation show 16.3%
higher generation quality, 46% fewer physics violations, and 18.5% improved
predictive accuracy over baselines.

</details>


### [116] [To Ask or Not to Ask: Learning to Require Human Feedback](https://arxiv.org/abs/2510.08314)
*Andrea Pugnana,Giovanni De Toni,Cesare Barbera,Roberto Pellungrini,Bruno Lepri,Andrea Passerini*

Main category: cs.LG

TL;DR: 提出Learning to Ask (LtA)框架：在机器学习模型与人类专家协作中，不仅决定何时将实例交由专家处理，还决定如何向专家询问以获取丰富反馈，从而提升决策支持系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Learning to Defer只允许模型将难例交给人类以获得预测，视人类和模型为互斥决策者，无法利用专家的更丰富信息（如建议、特征、解释）来增强模型。作者希望扩展人机协作的形式，提高系统整体能力。

Method: 提出两部分架构：一个标准ML模型与一个利用额外专家反馈训练的enriched模型，并设计一个形式上最优的策略来选择何时查询enriched模型。给出两种实现：串行（分阶段训练）和联合（同时优化）。为联合方法设计可实现一致性（realisable-consistency）保障的替代损失函数。

Result: 在合成数据和真实专家数据上的实验表明，LtA相较于传统LtD更灵活、更强大，能够更有效地融合专家反馈，提升协作性能。

Conclusion: LtA为人机协作提供了更丰富的范式，通过在何时以及如何向专家提问来更好地利用专家知识，理论上并实践上均表现优越。

Abstract: Developing decision-support systems that complement human performance in
classification tasks remains an open challenge. A popular approach, Learning to
Defer (LtD), allows a Machine Learning (ML) model to pass difficult cases to a
human expert. However, LtD treats humans and ML models as mutually exclusive
decision-makers, restricting the expert contribution to mere predictions. To
address this limitation, we propose Learning to Ask (LtA), a new framework that
handles both when and how to incorporate expert input in an ML model. LtA is
based on a two-part architecture: a standard ML model and an enriched model
trained with additional expert human feedback, with a formally optimal strategy
for selecting when to query the enriched model. We provide two practical
implementations of LtA: a sequential approach, which trains the models in
stages, and a joint approach, which optimises them simultaneously. For the
latter, we design surrogate losses with realisable-consistency guarantees. Our
experiments with synthetic and real expert data demonstrate that LtA provides a
more flexible and powerful foundation for effective human-AI collaboration.

</details>


### [117] [Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization](https://arxiv.org/abs/2510.08341)
*Pál Zsámboki,Benjamin Levi,David Ansel Josef Smith,Mitansh Kagalwala,Arlington Kell,Samuel Liechty,Cong Wang*

Main category: cs.LG

TL;DR: 本文通过集合补任务研究Transformer的长度泛化：在单层注意力模型上给出嵌入和value维度的紧致下界，并证明只要在短序列上logit位移平衡，就能泛化到更长序列但精度下降；softmax压缩和训练噪声是两大限制，dropout与EMA能缓解，实验证实（包括OthelloGPT）。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer在更长输入上是否和如何保持性能——尤其是那些需要“输出未出现令牌集合”的任务，这类能力对棋类推理等很重要；探索理论限制与可行的训练改进。

Method: 对单层注意力仅模型进行理论分析，证明关于嵌入和value维度的紧致边界；从证明给出机制学解释（softmax导致logit压缩）；分析训练动力学（大量合法下一个令牌导致更新噪声）；提出并在集合补任务上用随机超参搜索验证dropout和EMA的效果；在更复杂的OthelloGPT上复现EMA改进。

Result: 推导出对嵌入与value维度的紧确界；证明在长度1和2上达到平衡的logit位移即可推广到更长序列但分辨率降低；揭示softmax压缩和噪声更新两大障碍；经验结果显示dropout能缓解压缩，EMA能减小训练噪声，且在OthelloGPT中EMA也改善长度泛化。

Conclusion: 长度泛化受结构性（softmax非线性压缩）与训练噪声限制，简单的正则化/平均技术能实质提升泛化，但仍存在精度损失和理论限制，提示未来可从架构或归一化层面设计以保持logit分离或减少更新噪声。

Abstract: We study length generalization in transformers through the set complement
task, where a model must predict a uniform distribution over tokens absent from
an input sequence -- an ability central to board-game style reasoning. Our main
theoretical result establishes two statements. First, we prove tight bounds on
embedding and value dimensions for single-layer attention-only transformers.
Second, we show that if such a model achieves balanced logit displacement at
lengths 1 and 2, then it must generalize to longer sequences, though with
reduced precision. A mechanistic reading of the proof explains this limitation:
as more tokens are attended to, softmax compresses logit displacements, eroding
separation between valid and invalid outputs. Training dynamics also suggest a
second obstacle: when many next tokens are possible, updates become noisy. We
hypothesize that dropout can counteract the first effect and Exponential Moving
Average (EMA) the second. We validate these hypotheses through random
hyperparameter search on the set complement task, which confirms both
mechanisms. We then test OthelloGPT, a GPT-1 style model trained on random
Othello moves, and find that EMA again improves length generalization in this
more complex setting.

</details>


### [118] [Guided Star-Shaped Masked Diffusion](https://arxiv.org/abs/2510.08369)
*Viacheslav Meshchaninov,Egor Shibaev,Artem Makoian,Ivan Klimov,Danil Sheshenya,Andrei Malinin,Nikita Balagansky,Daniil Gavrilov,Aibek Alanov,Dmitry Vetrov*

Main category: cs.LG

TL;DR: 提出一种与预训练masked diffusion模型兼容的新采样算法：在微调单层后，采用星形生成范式与可学习的重新掩码调度器，实现可纠错的生成流程，在少步采样下显著提升质量。


<details>
  <summary>Details</summary>
Motivation: 现有预训练masked diffusion模型的采样过程通常是不可逆的，难以在低步数约束下纠正错误，从而限制了速度与样本质量的权衡。需要一种兼容预训练模型且能在少步数下提升质量的采样策略。

Method: 将生成过程重构为星形（star-shaped）范式以允许错误纠正；引入可学习的重新掩码调度器（re-masking scheduler）来识别并重访可能的错误位置；仅对预训练模型中的单个层进行轻量微调，使方法可直接应用于现成模型。

Result: 在文本与代码生成任务上的大量实验表明：在低采样步数下样本质量有显著提升，整体性能优于或可匹配现有方法；对关键组件进行了消融验证，展示了组成要素的必要性。

Conclusion: 该方法在兼顾效率与质量方面表现出良好潜力，尤其适合资源受限的快速生成场景；设计简单（仅微调一层）且可与既有预训练模型配合使用，增强了实用性。

Abstract: The performance of pre-trained masked diffusion models is often constrained
by their sampling procedure, which makes decisions irreversible and struggles
in low-step generation regimes. We introduce a novel sampling algorithm that
works with pre-trained models and, after a lightweight fine-tuning of a single
layer, significantly improves sample quality and efficiency. Our method
reformulates the generation process using a star-shaped paradigm, which
inherently allows for error correction. To make this process effective, we
augment it with a learnable re-masking scheduler that intelligently identifies
and revises likely errors. This approach yields a substantial quality boost,
particularly when using a small number of sampling steps. We extensively ablate
key components of our approach and show its usability in different scenarios.
In comprehensive experiments on text, and code generation, our sampling
algorithm outperforms or matches existing methods.

</details>


### [119] [Contrastive Self-Supervised Learning at the Edge: An Energy Perspective](https://arxiv.org/abs/2510.08374)
*Fernanda Famá,Roberto Pereira,Charalampos Kalalas,Paolo Dini,Lorena Qendro,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While contrastive learning (CL) shows considerable promise in self-supervised
representation learning, its deployment on resource-constrained devices remains
largely underexplored. The substantial computational demands required for
training conventional CL frameworks pose a set of challenges, particularly in
terms of energy consumption, data availability, and memory usage. We conduct an
evaluation of four widely used CL frameworks: SimCLR, MoCo, SimSiam, and Barlow
Twins. We focus on the practical feasibility of these CL frameworks for edge
and fog deployment, and introduce a systematic benchmarking strategy that
includes energy profiling and reduced training data conditions. Our findings
reveal that SimCLR, contrary to its perceived computational cost, demonstrates
the lowest energy consumption across various data regimes. Finally, we also
extend our analysis by evaluating lightweight neural architectures when paired
with CL frameworks. Our study aims to provide insights into the resource
implications of deploying CL in edge/fog environments with limited processing
capabilities and opens several research directions for its future optimization.

</details>


### [120] [Characterizing the Multiclass Learnability of Forgiving 0-1 Loss Functions](https://arxiv.org/abs/2510.08382)
*Jacob Trauger,Tyson Trauger,Ambuj Tewari*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper we will give a characterization of the learnability of
forgiving 0-1 loss functions in the finite label multiclass setting. To do
this, we create a new combinatorial dimension that is based off of the
Natarajan Dimension \citep{natarajan1989learning} and we show that a hypothesis
class is learnable in our setting if and only if this Generalized Natarajan
Dimension is finite. We also show a connection to learning with set-valued
feedback. Through our results we show that the learnability of a set learning
problem is characterized by the Natarajan Dimension.

</details>


### [121] [FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts](https://arxiv.org/abs/2510.08396)
*Heming Zou,Yunliang Zang,Wutong Xu,Yao Zhu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method for foundation models, but it suffers from parameter interference,
resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based
LoRA variants show promise in mitigating intra-task correlations in single-task
instruction tuning, they introduce additional router parameters and remain
ineffective in multi-task model merging where inter-task interference arises.
Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit
MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the
up-projection matrix, and (2) an implicit router that unifies expert routing
and down-projection, where a frozen sparse random projection matrix replaces
the traditional dense trainable version. This design resolves the trade-off
between intra-task decorrelation and computational efficiency by eliminating
the need for an explicit router, while inherently mitigating inter-task
interference due to the orthogonality property of random matrices. Extensive
experiments across four domains -- general knowledge understanding, scientific
question answering, mathematical reasoning, and code generation -- demonstrate
consistent performance improvements over existing methods. Beyond empirical
gains, FlyLoRA highlights how biological structures can inspire innovations in
AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.

</details>


### [122] [Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors](https://arxiv.org/abs/2510.08413)
*David Madras,Joshua Safyan,Qiuyi,Zhang*

Main category: cs.LG

TL;DR: 该工作提出以困惑度（perplexity）为数据/分布相关先验，推导出用于离散提示词空间的新的PAC‑Bayes泛化界，在数据稀缺时仍能给出非空洞界，从而解释并提升小样本提示词优化的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 已有将PAC‑Bayes应用于离散提示空间的泛化界，但在样本量少时常常变得空洞。作者认为应利用模型对文本‘自然性’的度量（困惑度）作为有效先验，限定搜索空间以获得更紧的界与更好的小样本泛化。

Method: 构造基于数据或分布的困惑度先验/正则项，并在PAC‑Bayes框架下重新推导泛化界，定量分析困惑度正则如何通过限制探索空间来收紧界限；同时通过实验评估界的非空洞性及困惑度正则对提示泛化的实用收益。

Result: 理论上得到在数据稀缺情形下仍非空洞的泛化界，并形式化地展示困惑度正则如何降低复杂度项；实验证明所导界在评估上有效，且困惑度正则能显著改善提示在下游任务上的泛化表现。

Conclusion: 将困惑度作为先验或正则化器能有效引导提示搜索到更‘自然’的解，既能从理论上收紧PAC‑Bayes界，也能在实际小样本提示优化中提高泛化，提示未来设计基于模型概率的先验/正则化策略。

Abstract: Many prompt engineering techniques have been successful in practice, even
when optimizing over a large prompt space with with a small amount of
task-specific data. Recent work has partially explained this success by showing
generalization bounds which apply PAC-Bayes theory to the discrete prompt
space, but they are non-vacuous only in data-rich scenarios. We argue that such
widespread success can be more fully explained through more carefully
considering data- or distribution-dependent perplexity, which acts as an
effective prior and steers the optimization towards prompts that are more
``natural'' for the task at hand. We derive novel generalization bounds that
are non-vacuous for data-scarce prompt optimization via more useful priors,
formally analyzing how perplexity regularization tightens these bounds by
limiting exploration. Empirically, we explore both the bounds' effectiveness
and the practical benefits of perplexity regularization in improving prompt
generalization.

</details>


### [123] [Reinforcing Diffusion Models by Direct Group Preference Optimization](https://arxiv.org/abs/2510.08425)
*Yihong Luo,Tianyang Hu,Jing Tang*

Main category: cs.LG

TL;DR: 提出DGPO，一种直接基于组偏好（group-level preferences）的在线强化学习算法，用于将偏好优化方法应用于扩散模型。DGPO摒弃传统的策略梯度范式，不依赖随机策略，因而可使用高效的确定性ODE采样器，训练速度比现有方法快约20倍并在内外域奖励上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的Group Relative Preference Optimization (GRPO)等基于偏好的强化学习方法在大模型上有效，但依赖随机策略；而扩散模型中最节省计算的采样器是确定性的ODE，二者不兼容。为引入随机性，近期工作转而用SDE采样器或外源高斯噪声，但这种无差别的噪声采样效率低、收敛慢。

Method: 提出Direct Group Preference Optimization (DGPO)，一种在线算法：直接从组级偏好学习，利用组内样本间的相对信息更新参数，避免使用策略梯度和显式随机策略，从而可以与确定性ODE采样器配合。

Result: 在大量实验中，DGPO训练速度比现有最先进方法快约20倍，并且在内域与外域的奖励指标上均优于对比方法。论文并开源代码。

Conclusion: DGPO解决了基于偏好的强化学习在扩散模型上使用确定性采样器的瓶颈，提供了一条更高效的训练路径，对扩散模型的偏好优化具有实际应用价值。

Abstract: While reinforcement learning methods such as Group Relative Preference
Optimization (GRPO) have significantly enhanced Large Language Models, adapting
them to diffusion models remains challenging. In particular, GRPO demands a
stochastic policy, yet the most cost-effective diffusion samplers are based on
deterministic ODEs. Recent work addresses this issue by using inefficient
SDE-based samplers to induce stochasticity, but this reliance on model-agnostic
Gaussian noise leads to slow convergence. To resolve this conflict, we propose
Direct Group Preference Optimization (DGPO), a new online RL algorithm that
dispenses with the policy-gradient framework entirely. DGPO learns directly
from group-level preferences, which utilize relative information of samples
within groups. This design eliminates the need for inefficient stochastic
policies, unlocking the use of efficient deterministic ODE samplers and faster
training. Extensive results show that DGPO trains around 20 times faster than
existing state-of-the-art methods and achieves superior performance on both
in-domain and out-of-domain reward metrics. Code is available at
https://github.com/Luo-Yihong/DGPO.

</details>


### [124] [xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning](https://arxiv.org/abs/2510.08439)
*Cheng Qian,Zuxin Liu,Shirley Kokane,Akshara Prabhakar,Jielin Qiu,Haolin Chen,Zhiwei Liu,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.LG

TL;DR: xRouter 是一个基于 tool-calling 的学习型路由系统，路由器可选择直接回答或调用外部模型，通过基于成本感知的强化学习端到端训练，实现成本-性能的自适应折中。在多项基准上，xRouter 在保持相近任务完成率的同时显著降低成本，并提供了关于可训练性与编排行为的实证见解。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 部署面临成本-性能的宽谱：高端模型推理能力强但昂贵，轻量模型便宜但在复杂任务上脆弱；静态升级规则和关键词启发式方法无法充分利用这一谱系，也难以跨任务自适应。因而需要一个能学习并成本感知地在模型间路由的系统。

Method: 提出 xRouter：使用 tool-calling 架构让路由器决策（直接回答或调用一个/多个外部模型）；使用强化学习端到端训练路由器，设计显式的成本感知奖励以编码成本-性能权衡；实现完整的 RL 框架、成本与奖励核算以及部署评估流水线。

Result: 在多样化基准上，xRouter 展示了良好的成本-性能折中（例如在相似任务完成率下显著降低成本）；同时揭示了有助于学习路由的因素与难点，如模型可训练性和在小型开放模型中难以诱导复杂编排行为的现象。

Conclusion: xRouter 提供了一个开源、实用的基线与实现，证明了可学习、成本感知的 LLM 编排具有实际价值，并为后续改进和研究（如更强的路由器架构、更精细的成本模型、跨任务泛化）指出了方向。

Abstract: Modern LLM deployments confront a widening cost-performance spectrum: premium
models deliver strong reasoning but are expensive, while lightweight models are
economical yet brittle on complex tasks. Static escalation rules and keyword
heuristics under-utilize this spectrum and fail to adapt across task types. We
present xRouter, a tool-calling-based routing system in which a learned router
can either answer directly or invoke one or more external models. The router is
trained end-to-end with reinforcement learning using an explicit, cost-aware
reward that encodes cost-performance trade-offs, eliminating the need for
hand-engineered routing rules. Our implementation encompasses the full
reinforcement learning framework, including reward and cost accounting, as well
as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter
achieves strong cost-performance trade-offs (e.g., substantial cost reductions
at comparable task completion rates), and provides empirical insights into what
reliably helps learned routing and what does not, ranging from model
trainability to the difficulty of eliciting sophisticated orchestration
behaviors in small open models. We hope these findings and our open
implementation will serve as a practical substrate for advancing learned,
cost-aware LLM orchestration.

</details>


### [125] [Synthetic Series-Symbol Data Generation for Time Series Foundation Models](https://arxiv.org/abs/2510.08445)
*Wenxuan Wang,Kai Wu,Yujian Betterest Li,Dan Wang,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: 本文提出通过基于复杂动力学系统理论的序列—符号（series-symbol）数据生成机制，构建大规模合成时序数据及对应符号表达式，并基于此设计预训练模型SymTime，用符号信息增强时序表示。经下游五大时序任务微调后，SymTime在性能上能与使用真实数据预训练的基线相媲美，凸显合成序列—符号生成与预训练策略在缓解数据稀缺方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 时序基础模型受到训练数据稀缺和类别不平衡限制。作者受复杂动力学系统启发，认为可以使用解析或参数化的符号表达式生成高质量、可控且可配对标签的时序样本，从而为预训练提供充足数据并引入物理/符号先验。

Method: 设计一套series-symbol生成机制，按符号表达式生成时序信号并保存表达式作为符号信息；提出预训练框架SymTime，将符号信息和时序信号联合用于表示学习；在五类时序下游任务上对SymTime进行微调与评估。

Result: SymTime在所选的五类任务上微调后表现竞争力，可与在真实世界数据上预训练的模型相媲美，表明合成序列—符号数据与相应预训练策略能够有效提升下游性能。

Conclusion: 利用符号可控生成的合成时序数据并将符号信息融入预训练，是解决时序数据稀缺和提升模型泛化能力的一条可行路线。代码已开源以助复现。

Abstract: Foundation models for time series analysis (TSA) have attracted significant
attention. However, challenges such as training data scarcity and imbalance
continue to hinder their development. Inspired by complex dynamic system
theories, we design a series-symbol data generation mechanism, enabling the
unrestricted creation of high-quality time series data paired with
corresponding symbolic expressions. To leverage series-symbol data pairs with
strong correlations, we develop \texttt{SymTime}, a pre-trained foundation
model for enhancing time series representation using symbolic information.
\texttt{SymTime} demonstrates competitive performance across five major TSA
tasks when fine-tunes with downstream tasks, rivaling foundation models
pre-trained on real-world datasets. This approach underscores the potential of
series-symbol data generation and pretraining mechanisms in overcoming data
scarcity and enhancing task performance. The code is available at
https://github.com/wwhenxuan/SymTime.

</details>


### [126] [SummDiff: Generative Modeling of Video Summarization with Diffusion](https://arxiv.org/abs/2510.08458)
*Kwanseok Kim,Jaehoon Hahm,Sumin Kim,Jinhwan Sul,Byunghak Kim,Joonseok Lee*

Main category: cs.LG

TL;DR: 提出将视频摘要视为条件生成任务，使用扩散模型（SummDiff）生成多个候选摘要以反映不同人为偏好，在多个基准上达到或超越SOTA，并提出针对背包步骤的新评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多个人类标注的评分平均，忽视了视频摘要的主观性和多样性；需要能生成多种合理摘要以更好地反映不同标注者视角。

Method: 首次在视频摘要中采用扩散模型，构建条件生成框架SummDiff，动态适应视觉上下文，根据输入视频生成多个候选摘要；并分析并引入针对最后的背包（knapsack）步骤的新评估指标。

Result: 在多项基准上取得最新的最好（SOTA）性能；生成的摘要更符合单个标注者偏好，表现出多样性与质量并存；通过新增指标深入分析了背包步骤对最终摘要质量的影响。

Conclusion: 将视频摘要建模为条件生成问题并采用扩散模型能有效捕捉摘要的主观性与多样性，SummDiff在性能与对人体偏好的对齐方面均有显著改进；此外对评估流程（尤其背包环节）的补充分析推动了更全面的性能评估。

Abstract: Video summarization is a task of shortening a video by choosing a subset of
frames while preserving its essential moments. Despite the innate subjectivity
of the task, previous works have deterministically regressed to an averaged
frame score over multiple raters, ignoring the inherent subjectivity of what
constitutes a good summary. We propose a novel problem formulation by framing
video summarization as a conditional generation task, allowing a model to learn
the distribution of good summaries and to generate multiple plausible summaries
that better reflect varying human perspectives. Adopting diffusion models for
the first time in video summarization, our proposed method, SummDiff,
dynamically adapts to visual contexts and generates multiple candidate
summaries conditioned on the input video. Extensive experiments demonstrate
that SummDiff not only achieves the state-of-the-art performance on various
benchmarks but also produces summaries that closely align with individual
annotator preferences. Moreover, we provide a deeper insight with novel metrics
from an analysis of the knapsack, which is an important last step of generating
summaries but has been overlooked in evaluation.

</details>


### [127] [In-Context Clustering with Large Language Models](https://arxiv.org/abs/2510.08466)
*Ying Wang,Mengye Ren,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 提出In-Context Clustering (ICC)：利用大模型的注意力矩阵进行无监督聚类，对文本化的数值与图像数据在零样本和微调（NTP）下表现良好，并可实现文本条件下的图像聚类。


<details>
  <summary>Details</summary>
Motivation: 传统聚类依赖预定义相似性度量，难以捕捉复杂关系；LLM的注意力机制可能隐式编码样本间的群组信息，适合做灵活的、可条件化的聚类。

Method: 将输入文本化后送入预训练LLM，提取注意力矩阵并对其进行谱聚类；进一步用下一步预测（NTP）对模型进行微调以提升对数值和图像数据的聚类能力；通过提示实现文本条件的图像聚类。

Result: 发现预训练LLM在文本化数值数据上具备显著零样本聚类能力，注意力矩阵呈现簇结构；基于注意力的谱聚类与经典方法竞争；NTP微调提升了数值和图像的聚类效果；可实现文本条件化的图像聚类。

Conclusion: 将in-context learning扩展到无监督聚类场景，展示了LLM在捕捉复杂数据关系和提供条件聚类能力方面的实用性与灵活性。

Abstract: We propose In-Context Clustering (ICC), a flexible LLM-based procedure for
clustering data from diverse distributions. Unlike traditional clustering
algorithms constrained by predefined similarity measures, ICC flexibly captures
complex relationships among inputs through an attention mechanism. We show that
pretrained LLMs exhibit impressive zero-shot clustering capabilities on
text-encoded numeric data, with attention matrices showing salient cluster
patterns. Spectral clustering using attention matrices offers surprisingly
competitive performance. We further enhance the clustering capabilities of LLMs
on numeric and image data through fine-tuning using the Next Token Prediction
(NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned
image clustering, a capability that classical clustering methods lack. Our work
extends in-context learning to an unsupervised setting, showcasing the
effectiveness and flexibility of LLMs for clustering. Our code is available at
https://agenticlearning.ai/icc.

</details>


### [128] [Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models](https://arxiv.org/abs/2510.08492)
*Sharut Gupta,Shobhita Sundaram,Chenyu Wang,Stefanie Jegelka,Phillip Isola*

Main category: cs.LG

TL;DR: 提出一种无配对多模态训练范式UML（Unpaired Multimodal Learner），通过共享参数交替处理不同模态的未配对数据，提升目标模态的表示学习；理论和实验证明对下游单模态任务有稳定改进。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型依赖配对数据（例如图像-文本对），但配对数据昂贵且有限。问题是：能否利用大量未配对的辅助模态数据来直接改进目标模态的表示，而无需显式配对？

Method: 提出UML：一个模态不可知的训练框架，单一模型交替接收不同模态输入并共享参数。理论上在线性数据生成假设下分析，证明未配对辅助数据能提升关于数据生成过程的表征信息量。实验上在多种下游单模态（图像、音频等）任务上加入文本、音频、图像等未配对辅助数据并评估性能改善。

Result: 理论结果在理想线性假设下表明未配对数据能严格提供比单模态训练更信息性的表示。实证结果显示，在不同目标模态上，引入未配对的辅助模态数据可持续且一致地提升下游任务表现。

Conclusion: UML展示了利用未配对多模态数据提升单模态表示的可行路径，减轻对配对数据的依赖，为多模态与单模态学习的互补提供新范式。

Abstract: Traditional multimodal learners find unified representations for tasks like
visual question answering, but rely heavily on paired datasets. However, an
overlooked yet potentially powerful question is: can one leverage auxiliary
unpaired multimodal data to directly enhance representation learning in a
target modality? We introduce UML: Unpaired Multimodal Learner, a
modality-agnostic training paradigm in which a single model alternately
processes inputs from different modalities while sharing parameters across
them. This design exploits the assumption that different modalities are
projections of a shared underlying reality, allowing the model to benefit from
cross-modal structure without requiring explicit pairs. Theoretically, under
linear data-generating assumptions, we show that unpaired auxiliary data can
yield representations strictly more informative about the data-generating
process than unimodal training. Empirically, we show that using unpaired data
from auxiliary modalities -- such as text, audio, or images -- consistently
improves downstream performance across diverse unimodal targets such as image
and audio. Our project page: https://unpaired-multimodal.github.io/

</details>


### [129] [Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning](https://arxiv.org/abs/2510.08526)
*Yash Jhaveri,Harley Wiltzer,Patrick Shafto,Marc G. Bellemare,David Meger*

Main category: cs.LG

TL;DR: 提出基于熵正则消失与温度解耦技巧的策略优化理论框架，使得在温度趋近于零时收敛到一个可解释且保持多样性的最优策略，并保证值函数与回报分布的收敛；并给出一个能以任意精度估计该策略对应回报分布的算法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通常只关注期望回报，忽视学得策略的性质，导致难以预测与解释最终策略的行为与多样性。需要一个能保证收敛到特定且可解释策略的理论与算法框架。

Method: 引入“熵正则消失”(vanishing entropy regularization)与“温度解耦”(temperature decoupling)技巧，刻画温度趋零情形下策略的极限性质；证明在该极限中能实现保持多样性的最优策略（例如对所有最优动作均匀采样）；证明策略、值函数与回报分布的收敛性；基于温度解耦设计算法以任意精度估计该策略的回报分布。

Result: 理论保证：当正则温度消失并采用温度解耦时，策略收敛到一个可解释且多样性的最优策略，同时相应的值函数与回报分布也收敛。算法上得到一个能以任意精度逼近该策略回报分布的方法。

Conclusion: 该工作提供了在策略优化中刻意选择并保证特定最优策略的工具，使得策略更具可解释性与多样性，并能可靠估计回报分布。下一步应在函数逼近、规模化实验与非平稳/部分可观测环境中验证与扩展该框架。

Abstract: In the pursuit of finding an optimal policy, reinforcement learning (RL)
methods generally ignore the properties of learned policies apart from their
expected return. Thus, even when successful, it is difficult to characterize
which policies will be learned and what they will do. In this work, we present
a theoretical framework for policy optimization that guarantees convergence to
a particular optimal policy, via vanishing entropy regularization and a
temperature decoupling gambit. Our approach realizes an interpretable,
diversity-preserving optimal policy as the regularization temperature vanishes
and ensures the convergence of policy derived objects--value functions and
return distributions. In a particular instance of our method, for example, the
realized policy samples all optimal actions uniformly. Leveraging our
temperature decoupling gambit, we present an algorithm that estimates, to
arbitrary accuracy, the return distribution associated to its interpretable,
diversity-preserving optimal policy.

</details>


### [130] [Entropy Regularizing Activation: Boosting Continuous Control, Large Language Models, and Image Classification with Activation as Entropy Constraints](https://arxiv.org/abs/2510.08549)
*Zilin Kang,Chonghua Liao,Tingqiang Xu,Huazhe Xu*

Main category: cs.LG

TL;DR: 提出ERA，一种通过对模型输出应用特殊激活以将采样熵约束在阈值以上的范式，在LLM、连续控制强化学习和图像分类等多领域均显著提升表现，计算开销<7%。


<details>
  <summary>Details</summary>
Motivation: 在许多任务中，模型输出的熵过低会导致样本多样性不足或策略/预测过于确定，从而影响泛化与任务性能。作者旨在通过直接控制输出熵（而非修改训练目标或复杂正则化）来提高模型鲁棒性与表现。

Method: 提出ERA（Entropy-Restricting Activation），在模型输出端施加设计好的激活函数以保证采样熵高于设定阈值。该激活可在推理/采样阶段插入，轻量计算开销，并可与现有模型和训练流程配合使用。

Result: 在多个基准上均有显著提升：Qwen2.5-Math-7B在AIME 2025得分提升37.4%；在HumanoidBench上相比强基准SAC提升>30%；ResNet-50在ImageNet上top-1精度提升0.69%。总体计算开销低于7%。

Conclusion: 验证了通过输出激活控制熵是一种通用且高效的手段，可作为设计更简洁、稳健算法的新方向，适用于不同模型与任务。

Abstract: We propose ERA, a new paradigm that constrains the sampling entropy above
given thresholds by applying specially designed activations to the outputs of
models. Our approach demonstrates broad effectiveness across different domains:
1) for large language models(LLMs), boosting the AIME 2025 score for
Qwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning
agents, improving performance by more than 30% over strong baselines such as
SAC on the challenging HumanoidBench; 3) for image classification, enhancing
ImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a
computational overhead of less than 7%. Our work validates output activation as
a powerful tool for entropy control, opening a new direction for designing
simpler and more robust algorithms.

</details>


### [131] [Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization](https://arxiv.org/abs/2510.08554)
*Kevin Rojas,Jiahe Lin,Kashif Rasul,Anderson Schneider,Yuriy Nevmyvaka,Molei Tao,Wei Deng*

Main category: cs.LG

TL;DR: 本文提出GDPO（Group Diffusion Policy Optimization），一种针对扩散语言模型（DLMs）的强化学习微调方法。通过解析ELBO估计的方差来源并用半确定性蒙特卡洛近似降低方差，GDPO在有限评估预算下给出更低方差估计器，在数学、推理和代码基准上优于已有方法（如diffu-GRPO）。


<details>
  <summary>Details</summary>
Motivation: DLMs支持并行、无序的迭代生成，是自回归模型的可行替代，但其似然不可解导致难以直接应用RL微调。现有基于单步unmasking的估计方法计算高效但有严重偏差；基于序列ELBO的方法理论上更合适，但因似然评估开销大、方差高而难以实用。作者希望建立一个在计算预算受限下仍能稳健估计ELBO并用于RL微调的方案。

Method: 作者首先对ELBO估计的方差来源进行分解，识别出可通过确定性或低随机化的积分近似来控制的关键方向。基于该分析，提出了半确定性蒙特卡洛（Semi-deterministic Monte Carlo）策略与分组技术，形成Group Diffusion Policy Optimization（GDPO）。GDPO在双重蒙特卡洛采样的场景下，用分组与确定性积分沿若干关键维度替代全随机采样，从而在紧张的评估预算下获得方差更小的ELBO估计器，并给出降低方差的理论保证。

Result: 在若干数学、推理与编程基准上，GDPO对预训练检查点做微调后表现稳定提升，并在大多数任务上超过了diffu-GRPO等先进基线。实验展示了在相同或更低的评估预算下，GDPO能有效控制ELBO估计方差并带来性能增益。

Conclusion: GDPO为DLMs的RL微调提供了一种更稳健、低方差且可实现的ELBO估计方法，通过半确定性采样和分组策略在实际预算下改善训练效果，推动了将强化学习方法应用到扩散语言模型的可行性。

Abstract: Diffusion language models (DLMs) enable parallel, order-agnostic generation
with iterative refinement, offering a flexible alternative to autoregressive
large language models (LLMs). However, adapting reinforcement learning (RL)
fine-tuning to DLMs remains an open challenge because of the intractable
likelihood. Pioneering work such as diffu-GRPO estimated token-level
likelihoods via one-step unmasking. While computationally efficient, this
approach is severely biased. A more principled foundation lies in
sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a
surrogate. Yet, despite this clean mathematical connection, ELBO-based methods
have seen limited adoption due to the prohibitive cost of likelihood
evaluation. In this work, we revisit ELBO estimation and disentangle its
sources of variance. This decomposition motivates reducing variance through
fast, deterministic integral approximations along a few pivotal dimensions.
Building on this insight, we introduce \textbf{Group Diffusion Policy
Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages
simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the
variance explosion of ELBO estimators under vanilla double Monte Carlo
sampling, yielding a provably lower-variance estimator under tight evaluation
budgets. Empirically, GDPO achieves consistent gains over pretrained
checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,
on the majority of math, reasoning, and coding benchmarks.

</details>
