<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 8]
- [cs.LG](#cs.LG) [Total: 96]
- [cs.CR](#cs.CR) [Total: 13]
- [eess.SP](#eess.SP) [Total: 10]
- [eess.SY](#eess.SY) [Total: 8]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Quick Change Detection in Discrete-Time in Presence of a Covert Adversary](https://arxiv.org/abs/2601.20022)
*Amir Reza Ramtin,Philippe Nain,Don Towsley*

Main category: cs.IT

TL;DR: 针对可知参数γ的隐蔽对手，本文基于CuSum分析得到 ADD 与 AT2FA 的新渐近结果，指出隐蔽下 ADD ∝ γ，经典情况下仅为 O(log γ)，并给出高斯、指数模型的具体表达式


<details>
  <summary>Details</summary>
Motivation: 探讨在可隐蔽对手下的快速变化检测问题，解决传统假警告约束参数已知且对手可选择与之相关的后期分布导致隐蔽性挑战的情况

Method: 在CuSum基本框架下，推导平均检测延迟 (ADD) 与平均假警报时间 (AT2FA) 的渐近表达式，分析当γ→∞时后期分布趋近前期分布的情形，确定临界尺度和隐蔽性条件

Result: 得到ADD与AT2FA 的精确渐近公式，揭示在隐蔽对手情境下 ADD 为 Θ(γ) 的尺度，经典情形下为 O(log γ)。对高斯和指数模型给出ADD随KL散度和γ的渐近表达式

Conclusion: 本文在传统快速检测理论中加入隐蔽对手模型，提供了后期分布趋同情况下的新渐近结论，为隐蔽检测的理论与方法奠定基础

Abstract: We study the problem of covert quickest change detection in a discrete-time setting, where a sequence of observations undergoes a distributional change at an unknown time. Unlike classical formulations, we consider a covert adversary who has knowledge of the detector's false alarm constraint parameter $γ$ and selects a stationary post-change distribution that depends on it, seeking to remain undetected for as long as possible. Building on the theoretical foundations of the CuSum procedure, we rigorously characterize the asymptotic behavior of the average detection delay (ADD) and the average time to false alarm (AT2FA) when the post-change distribution converges to the pre-change distribution as $γ\to \infty$. Our analysis establishes exact asymptotic expressions for these quantities, extending and refining classical results that no longer hold in this regime. We identify the critical scaling laws governing covert behavior and derive explicit conditions under which an adversary can maintain covertness, defined by ADD = $Θ(γ)$, whereas in the classical setting, ADD grows only as $\mathcal{O}(\log γ)$. In particular, for Gaussian and Exponential models under adversarial perturbations of their respective parameters, we asymptotically characterize ADD as a function of the Kullback--Leibler divergence between the pre- and post-change distributions and $γ$.

</details>


### [2] [Energy Efficient Downlink mMIMO Using Dynamic Antenna and Power Adaptation](https://arxiv.org/abs/2601.20586)
*Ravi Sharan B A G,Maliha Jada,Anders Karstensen,Daniela Laselva,Jyri Hämäläinen,Silvio Mandelli*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Massive multiple-input multiple-output (mMIMO) technology and its future evolutions are expected to address the high data rate demands of sixth generation (6G) communication systems. At the same time, network energy savings (NES) is essential in reducing the operational costs and meeting the sustainability goals of network operators. In this regard, we propose a dynamic scheme for joint antenna and power adaptation to improve NES from a user scheduling and resource allocation perspective. Antenna adaptation is performed using the multiple channel state information resource signal (CSI-RS) framework. Furthermore, the recently introduced transmit power-aware link adaptation scheme, referred to as POLITE for short, is used as the power adaptation technique. The proposed scheme adapts to variations in users' instantaneous traffic and channel conditions to opportunistically maximize NES while also inherently accounting for the user throughput. Numerical simulation results show that the proposed scheme consistently achieves a balance between NES and user perceived throughput (UPT) for different network load conditions. Especially in low and light load conditions, the proposed scheme significantly improves the intra-cell interference and boosts the overall NES, while ensuring that UPT is unaffected.

</details>


### [3] [Shortest LCD embeddings of binary, ternary and quaternary linear codes](https://arxiv.org/abs/2601.20600)
*Junmin An,Ji-Hoon Hong,Jon-Lark Kim,Haeun Lim*

Main category: cs.IT

TL;DR: 通过计算所需增列数并列举最短LCD嵌入形式，论文给出了将任意线性码嵌入最优LCD码的有效工具，并成功得到多组比已知更优的LCD码。


<details>
  <summary>Details</summary>
Motivation: LCD码具有平凡的包络，和自正交码相对；研究是否能利用最短嵌入将线性码映射到最优LCD码，从而得到更优的码。

Method: 首先求出将给定线性码的生成矩阵嵌入LCD码所需增添的最少列数；随后对所有可能的最短LCD嵌入形式进行表征，并以此方法对Hamming码进行实验。

Result: 在二、三和四进制平面上分别得到新的LCD码：\([23,4,14]\)、\([23,5,12]\)、\([24,6,12]\)、\([25,5,14]\) 和 \([21,10,8]\)。这些码的最小距离比已知码高1。

Conclusion: 该论文表明，短LCD嵌入方法可将任意线性码嵌入一个最优LCD码，并且通过最小增列即可构造新的最优LCD码，提升了已知码的最小距离。

Abstract: In the recent years, there has been active research on self-orthogonal embeddings of linear codes since they yielded some optimal self-orthogonal codes. LCD codes have a trivial hull so they are counterparts of self-orthogonal codes. So it is a natural question whether one can embed linear codes into optimal LCD codes. To answer it, we first determine the number of columns to be added to a generator matrix of a linear code in order to embed the given code into an LCD code. Then we characterize all possible forms of shortest LCD embeddings of a linear code. As examples, we start from binary and ternary Hamming codes of small lengths and obtain optimal LCD codes with minimum distance 4. Furthermore, we find new ternary LCD codes with parameters including $[23, 4, 14]$, $[23, 5, 12]$, $[24, 6, 12]$, and $[25, 5, 14]$ and a new quaternary LCD $[21, 10, 8]$ code, each of which has minimum distance one greater than those of known codes. This shows that our shortest LCD embedding method is useful in finding optimal LCD codes over various fields.

</details>


### [4] [Helper-Assisted Coding for Gaussian Wiretap Channels: Deep Learning Meets PhySec](https://arxiv.org/abs/2601.20678)
*Vidhi Rana,Remi A. Chou,Taejoon Kim*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Consider the Gaussian wiretap channel, where a transmitter wishes to send a confidential message to a legitimate receiver in the presence of an eavesdropper. It is well known that if the eavesdropper experiences less channel noise than the legitimate receiver, then it is impossible for the transmitter to achieve positive secrecy rates. A known solution to this issue consists in involving a second transmitter, referred to as a helper, to help the first transmitter to achieve security. While such a solution has been studied for the asymptotic blocklength regime and via non-constructive coding schemes, in this paper, for the first time, we design explicit and short blocklength codes using deep learning and cryptographic tools to demonstrate the benefit and practicality of cooperation between two transmitters over the wiretap channel. Specifically, our proposed codes show strict improvement in terms of information leakage compared to existing codes that do not consider a helper. Our code design approach relies on a reliability layer, implemented with an autoencoder architecture based on the successive interference cancellation method, and a security layer implemented with universal hash functions. We also propose an alternative autoencoder architecture that significantly reduces training time by allowing the decoders to independently estimate messages without successively canceling interference by the receiver during training. Additionally, we show that our code design is also applicable to the multiple access wiretap channel with helpers, where two transmitters send confidential messages to the legitimate receiver.

</details>


### [5] [Reflected wireless signals under random spatial sampling](https://arxiv.org/abs/2601.20699)
*H. Paul Keeler*

Main category: cs.IT

TL;DR: 本文发现无线信号功率因墙壁反射在空间中存在无限峰值，给出理论解释和闭式解，对智慧表面设计有指导意义。


<details>
  <summary>Details</summary>
Motivation: 揭示在墙壁反射导致的功率波动对无线衰落估计的影响，并为智慧表面设计提供理论支持。

Method: 采用传播模型推导并给出简洁数学论证，随后在单射电机间双平行墙模型中进行分析并推导闭式解。

Result: 发现功率直方图中出现奇异峰值，并给出特殊位置下用Lerch超越函数的闭式表达式。

Conclusion: 随机信号源在空间中随机位置会导致功率直方图出现无限峰值，这些峰值对应于确定性传播模型的临界点。

Abstract: We present a propagation model showing that a transmitter randomly positioned in space generates unbounded peaks in the histogram of the resulting power, provided the signal strength is an oscillating or non-monotonic function of distance. Specifically, these peaks are singularities in the empirical probability density that occur at turning point values of the deterministic propagation model. We explain the underlying mechanism of this phenomenon through a concise mathematical argument. This observation has direct implications for estimating random propagation effects such as fading, particularly when reflections off walls are involved.
  Motivated by understanding intelligent surfaces, we apply this fundamental result to a physical model consisting of a single transmitter between two parallel passive walls. We analyze signal fading due to reflections and observe power oscillations resulting from wall reflections -- a phenomenon long studied in waveguides but relatively unexplored in wireless networks. For the special case where the transmitter is placed halfway between the walls, we present a compact closed-form expression for the received signal involving the Lerch transcendent function. The insights from this work can inform design decisions for intelligent surfaces deployed in cities.

</details>


### [6] [Anytime-Valid Quantum Tomography via Confidence Sequences](https://arxiv.org/abs/2601.20761)
*Aldo Cumitini,Luca Barletta,Osvaldo Simeone*

Main category: cs.IT

TL;DR: 本论文以任时有效置信序列为基础，为量子态层析提供了能够随时输出置信区间的技术，保持了对真态的覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 在逐步获取测量数据的过程中，传统QST方法无法精确量化当前估计的可信度，需要一种即刻可用且无后验偏差的置信量化工具。

Method: 在传统量子态层析基础上，引入基于最新的任时有效置信序列的理论，为每个当前点估计构建置信集，实现随数据采集的实时不确定度量化。

Result: 通过数值实验验证了提出的方法在不同测量序列下的覆盖率与传统方法相比保持一致，并且在任一时刻均满足预设置信度。

Conclusion: 本研究提出了适用于量子态断言的任时有效的置信区间框架，证明其在任何测量时刻都能保证指定置信水平的覆盖性。

Abstract: In this letter, we address the problem of developing quantum state tomography (QST) methods that remain valid at any time during a sequence of measurements. Specifically, the aim is to provide a rigorous quantification of the uncertainty associated with the current state estimate as data are acquired incrementally. To this end, the proposed framework augments existing QST techniques by associating current point estimates of the state with confidence sets that are guaranteed to contain the true quantum state with a user-defined probability. The methodology is grounded in recent statistical advances in anytime-valid confidence sequences. Numerical results confirm the theoretical coverage properties of the proposed anytime-valid QST.

</details>


### [7] [Construction and Decoding of Convolutional Codes with optimal Column Distances](https://arxiv.org/abs/2601.20825)
*Julia Lieb,Michael Schaller*

Main category: cs.IT

TL;DR: 提供了一种在任意有限域上构造列距离最优卷积码的方法，并证明其唯一性，同时提出了基于Reed-Muller的简化维特比算法。


<details>
  <summary>Details</summary>
Motivation: MDP卷积码通常需要极大有限域，而该研究致力于在任意有限域上构造列距离最优的卷积码，以降低实现复杂度。

Method: 通过将卷积码的结构与一阶Reed-Muller块码相结合，构造了最优列距离卷积码，并利用这一结构设计了简化版维特比解码算法。

Result: 证明所构造的码是满足所给参数时唯一实现最佳列距离的码，并提供了一种低复杂度的维特比算法实现。

Conclusion: 该论文给出了可实现最佳列距离的卷积码的构造，并证明在所讨论的参数下仅有此构造能达到最佳列距离，同时提出了基于一阶Reed-Muller码结构的低复杂度维特比算法。

Abstract: The construction of Maximum Distance Profile (MDP) convolutional codes in general requires the use of very large finite fields. In contrast convolutional codes with optimal column distances maximize the column distances for a given arbitrary finite field. In this paper, we present a construction of such convolutional codes. In addition, we prove that for the considered parameters the codes that we constructed are the only ones achieving optimal column distances. The structure of the presented convolutional codes with optimal column distances is strongly related to first order Reed-Muller block codes and we leverage this fact to develop a reduced complexity version of the Viterbi algorithm for these codes.

</details>


### [8] [Low-Complexity Pilot-Aided Doppler Ambiguity Estimation for OTFS Parametric Channel Estimation](https://arxiv.org/abs/2601.20827)
*Bo-Yuan Chen,Hsuan-Jung Su*

Main category: cs.IT

TL;DR: Paper addresses Doppler ambiguity in OTFS for high‑mobility NTN by proposing a low‑complexity pilot‑based detection and compensation algorithm that restores performance with near‑ML complexity.


<details>
  <summary>Details</summary>
Motivation: Emerging 5G NTN with LEO satellites produce Doppler shifts beyond the OTFS grid, causing model mismatch and making conventional MLE channel estimators ineffective.

Method: 1) Derive OTFS input‑output with aliasing to show Doppler ambiguity as a phase rotation along delay; 2) 1st stage: use pairwise phase differences of pilots to identify integer ambiguity; 3) 2nd stage: refine channel estimation via MLE; 4) Evaluate two pilot designs (EP‑GZ and DSP) for interference vs. spectral efficiency trade‑off.

Result: Simulation validates that the proposed framework eliminates the ambiguity‑induced error floor, achieving BER and NMSE similar to exhaustive search while maintaining complexity close to standard MLE.

Conclusion: Doppler ambiguity in high-mobility OTFS systems can be detected and compensated with a low-complexity, pilot‑based two‑stage scheme that removes the error floor and reaches performance close to exhaustive search while keeping standard MLE complexity.

Abstract: Orthogonal Time Frequency Space (OTFS) modulation offers robust performance in high-mobility scenarios by transforming time-varying channels into the delay-Doppler (DD) domain. However, in high-mobility environment such as emerging 5G Non-Terrestrial Networks (NTN), the extreme orbital velocities of Low Earth Orbit (LEO) satellites frequently cause the physical Doppler shifts to exceed the fundamental grid range. This Doppler ambiguity induces severe model mismatch and renders traditional MLE channel estimators ineffective. To address this challenge, this paper proposes a novel low-complexity pilot-aided Doppler ambiguity detection and compensation framework. We first mathematically derive the OTFS input-output relationship in the presence of aliasing, revealing that Doppler ambiguity manifests itself as a distinct phase rotation along the delay dimension. Leveraging this insight, we developed a two-stage estimator that utilizes pairwise phase differences between pilot symbols to identify the integer ambiguity, followed by a refined Maximum Likelihood Estimation (MLE) for channel recovery. We investigate two pilot arrangements, Embedded Pilot with Guard Zone (EP-GZ) and Data-Surrounded Pilot (DSP), to analyze the trade-off between interference suppression and spectral efficiency. Simulation results demonstrate that the proposed scheme effectively eliminates the error floor caused by ambiguity, achieving Bit Error Rate (BER) and Normalized Mean Square Error (NMSE) performance comparable to the exhaustive search benchmark while maintaining a computational complexity similar to standard MLE.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data](https://arxiv.org/abs/2601.19936)
*Minseo Kwak,Jaehyung Kim*

Main category: cs.LG

TL;DR: Gap‑K% 利用 top‑1 与目标词的对数概率差与滑动窗口结合，显著提升 LLM 预训练数据检测效果。


<details>
  <summary>Details</summary>
Motivation: LLM 预训练语料库不透明导致隐私与版权问题，需有效检测预训练数据。

Method: Gap‑K% 通过对比模型 top‑1 预测词与目标词的对数概率差，并采用滑动窗口捕捉相邻 token 的局部相关，利用预训练梯度动态进行判别。

Result: 在 WikiMIA 与 MIMIR 基准上，Gap‑K% 在各模型规模与输入长度下均实现显著优于现有基线的检测性能，达成最佳效果。

Conclusion: Gap‑K% 提供了基于优化动态的新型预训练数据检测方案，展示了更强的鲁棒性与通用性。

Abstract: The opacity of massive pretraining corpora in Large Language Models (LLMs) raises significant privacy and copyright concerns, making pretraining data detection a critical challenge. Existing state-of-the-art methods typically rely on token likelihoods, yet they often overlook the divergence from the model's top-1 prediction and local correlation between adjacent tokens. In this work, we propose Gap-K%, a novel pretraining data detection method grounded in the optimization dynamics of LLM pretraining. By analyzing the next-token prediction objective, we observe that discrepancies between the model's top-1 prediction and the target token induce strong gradient signals, which are explicitly penalized during training. Motivated by this, Gap-K% leverages the log probability gap between the top-1 predicted token and the target token, incorporating a sliding window strategy to capture local correlations and mitigate token-level fluctuations. Extensive experiments on the WikiMIA and MIMIR benchmarks demonstrate that Gap-K% achieves state-of-the-art performance, consistently outperforming prior baselines across various model sizes and input lengths.

</details>


### [10] [DecHW: Heterogeneous Decentralized Federated Learning Exploiting Second-Order Information](https://arxiv.org/abs/2601.19938)
*Adnan Ahmad,Chiara Boldrini,Lorenzo Valerio,Andrea Passarella,Marco Conti*

Main category: cs.LG

TL;DR: 提出基于参数级证据可信度的聚合方法，解决DFL异构导致的收敛慢问题，实验验证其高效性与低通信开销。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习中设备间经验与交互差异导致数据与模型初始化异构，导致局部模型参数差异大、收敛慢。

Method: 引入参数级证据可信度机制，利用局部数据的二阶信息近似得到共识权重，对邻居更新进行加权后聚合为全局邻域表示。

Result: 在计算机视觉任务的大量实验中，所提方法表现出优异的局部模型泛化能力，并降低通信成本。

Conclusion: 该方法通过对局部模型参数的证据可信度进行量化，并利用二阶信息生成共识权重，显著提升了DFL在异构环境下的收敛速度与模型泛化能力，且通信成本较低。

Abstract: Decentralized Federated Learning (DFL) is a serverless collaborative machine learning paradigm where devices collaborate directly with neighbouring devices to exchange model information for learning a generalized model. However, variations in individual experiences and different levels of device interactions lead to data and model initialization heterogeneities across devices. Such heterogeneities leave variations in local model parameters across devices that leads to slower convergence. This paper tackles the data and model heterogeneity by explicitly addressing the parameter level varying evidential credence across local models. A novel aggregation approach is introduced that captures these parameter variations in local models and performs robust aggregation of neighbourhood local updates. Specifically, consensus weights are generated via approximation of second-order information of local models on their local datasets. These weights are utilized to scale neighbourhood updates before aggregating them into global neighbourhood representation. In extensive experiments with computer vision tasks, the proposed approach shows strong generalizability of local models at reduced communication costs.

</details>


### [11] [oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction](https://arxiv.org/abs/2601.19939)
*Hyunmin Kim,Yukun Zhou,Rahul A. Jonas,Lie Ju,Sunjin Hwang,Pearse A. Keane,Siegfried K. Wagner*

Main category: cs.LG

TL;DR: 提出Oculomix，层次采样的混合增强方案，凭借患者与检查层级先验保持个体特征，提升ViT模型预测心血管事件的表现，提升AUROC约3%。


<details>
  <summary>Details</summary>
Motivation: 传统图像级混合增强方法忽视了患者特点和临床因素，导致预测性能下降；需在保留个体差异的前提下应用混合增强；

Method: 基于病人和检查层次的临床先验，限制CutMix/MixUp的混合空间于同一病人及同一检查级别，保留患者特定属性及其软性时间趋势；

Result: 在Alzeye大规模多民族人群上，Oculomix比图像级CutMix和MixUp提升AUROC约3%，证明了该方法的必要性和有效性；

Conclusion: Oculomix在Oculomics领域通过层次采样策略提升了对患者特征的保留，从而显著提升了利用ViT模型预测未来5年主要心血管事件的AUROC；

Abstract: Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.

</details>


### [12] [Latent Object Permanence: Topological Phase Transitions, Free-Energy Principles, and Renormalization Group Flows in Deep Transformer Manifolds](https://arxiv.org/abs/2601.19942)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.LG

TL;DR: 通过将Transformer的隐藏状态建模为流，并分析其协方差谱与随机矩阵偏差，研究表明在足够深度时模型出现相位转移，低维度和可重用的“Transient Class Objects”出现，解释了多步推理的几何根源。


<details>
  <summary>Details</summary>
Motivation: 深入探究Transformer多步推理的起源和机制，揭示隐藏状态动态的几何结构及其对推理能力的影响。

Method: 利用几何与统计物理视角，将隐藏层状态视为隐式黎曼流形上的流，分析层级协方差谱与随机矩阵基底偏差；使用稀疏性/局部化阶参数 Ω(h) 跟踪临界深度；将前向传播离散化为粗化映射，关联固定点与概念基底。

Result: 在 1.5B–30B 模型中出现临界深度 γ_c≈0.42，导致有效维度急剧下降、谱尾部坍塌、临时可重用对象结构(TCO)形成，并通过多层探针验证理论预测。

Conclusion: 研究发现深层Transformer模型在特定深度临界点出现相位转移，导致有效维度骤降，并出现稳定“概念基底”（概念基底）及可重用的临时对象结构 TCO，使得表示空间低熵且逻辑可分离。

Abstract: We study the emergence of multi-step reasoning in deep Transformer language models through a geometric and statistical-physics lens. Treating the hidden-state trajectory as a flow on an implicit Riemannian manifold, we analyze the layerwise covariance spectrum of activations, where $C^{(\ell)}=\mathbb{E}[h^{(\ell)}h^{(\ell)\top}]$, and track deviations from a random-matrix bulk. Across model scales (1.5B--30B), we observe a sharp reduction in effective dimensionality consistent with a phase transition: an order parameter based on sparsity/localization, $Ω(h)=1-\|h\|_1/(\sqrt{d}\|h\|_2)$, exhibits a discontinuity near a critical normalized depth $γ_c\approx 0.42$ in sufficiently large models. We formalize the forward pass as a discrete coarse-graining map and relate the appearance of stable "concept basins" to fixed points of this renormalization-like dynamics. The resulting low-entropy regime is characterized by a spectral tail collapse and by the formation of transient, reusable object-like structures in representation space, which we call Transient Class Objects (TCOs). We provide theoretical conditions connecting logical separability to spectral decay and validate the predicted signatures with layerwise probes on multiple open-weight model families.

</details>


### [13] [Emergent Specialization in Learner Populations: Competition as the Source of Diversity](https://arxiv.org/abs/2601.19943)
*Yuhao Li*

Main category: cs.LG

TL;DR: 竞争即可产生自发专业化，NichePopulation 在 6 个真实场景中平均 SI 0.75，速度与性能均优于现有 MARL 基线。


<details>
  <summary>Details</summary>
Motivation: 探讨在无显式沟通或多样性激励的情况下，学习者群体是否能通过竞争实现协调多样行为，并验证竞争是否足以导致专门化。

Method: 提出 NichePopulation 算法，将竞争排斥与生态位亲和度跟踪相结合；在加密货币交易、商品价格、天气预报、光照、城市交通和空气质量等六个领域进行实验，评估专业化指数 SI 及效应量。

Result: 平均专业化指数 SI=0.75，Cohen's d >20；λ=0 时仍达 SI>0.30；多样化群体比同质基线提升 26.5%；相比 MARL 基线（QMIX, MAPPO, IQL）性能提升 4.3 倍，速度快 4 倍。

Conclusion: 竞争单靠即可诱导学习者群体自发出现专业化，验证生态位理论；多样化群体通过方法层次分工显著优于同质基线，NichePopulation 在六个真实任务中均优于主流 MARL 方法。

Abstract: How can populations of learners develop coordinated, diverse behaviors without explicit communication or diversity incentives? We demonstrate that competition alone is sufficient to induce emergent specialization -- learners spontaneously partition into specialists for different environmental regimes through competitive dynamics, consistent with ecological niche theory. We introduce the NichePopulation algorithm, a simple mechanism combining competitive exclusion with niche affinity tracking. Validated across six real-world domains (cryptocurrency trading, commodity prices, weather forecasting, solar irradiance, urban traffic, and air quality), our approach achieves a mean Specialization Index of 0.75 with effect sizes of Cohen's d > 20. Key findings: (1) At lambda=0 (no niche bonus), learners still achieve SI > 0.30, proving specialization is genuinely emergent; (2) Diverse populations outperform homogeneous baselines by +26.5% through method-level division of labor; (3) Our approach outperforms MARL baselines (QMIX, MAPPO, IQL) by 4.3x while being 4x faster.

</details>


### [14] [NCSAM Noise-Compensated Sharpness-Aware Minimization for Noisy Label Learning](https://arxiv.org/abs/2601.19947)
*Jiayu Xu,Junbiao Pang*

Main category: cs.LG

TL;DR: 本研究展示了平坦损失面对噪声标签的正向作用，并提出 NCSAM，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决真实世界数据中噪声标签导致的深度学习性能下降

Method: 理论分析损失面平坦度与噪声标签的关系，并提出 Noise-Compensated Sharpness‑aware Minimization (NCSAM)，利用 SAM 的扰动有效补偿噪声损害

Result: 在多个基准数据集上实验表明，NCSAM 在泛化性能和噪声鲁棒性方面均优于现有最先进方法

Conclusion: 证明了对噪声标签进行合适模拟能够提升模型的泛化与鲁棒性，并通过 NCSAM 实现了显著的性能提升

Abstract: Learning from Noisy Labels (LNL) presents a fundamental challenge in deep learning, as real-world datasets often contain erroneous or corrupted annotations, \textit{e.g.}, data crawled from Web. Current research focuses on sophisticated label correction mechanisms. In contrast, this paper adopts a novel perspective by establishing a theoretical analysis the relationship between flatness of the loss landscape and the presence of label noise. In this paper, we theoretically demonstrate that carefully simulated label noise synergistically enhances both the generalization performance and robustness of label noises. Consequently, we propose Noise-Compensated Sharpness-aware Minimization (NCSAM) to leverage the perturbation of Sharpness-Aware Minimization (SAM) to remedy the damage of label noises. Our analysis reveals that the testing accuracy exhibits a similar behavior that has been observed on the noise-clear dataset. Extensive experimental results on multiple benchmark datasets demonstrate the consistent superiority of the proposed method over existing state-of-the-art approaches on diverse tasks.

</details>


### [15] [Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations](https://arxiv.org/abs/2601.20477)
*Kadircan Aksoy,Peter Jung,Protim Bhattacharjee*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.

</details>


### [16] [Probabilistic Sensing: Intelligence in Data Sampling](https://arxiv.org/abs/2601.19953)
*Ibrahim Albulushi,Saleh Bunaiyan,Suraj S. Cheema,Hesham ElSawy,Feras Al-Dirini*

Main category: cs.LG

TL;DR: 一种微秒级响应的概率化传感器框架，利用模仿自主神经系统的概率神经元实现实时自适应采样，实验显示误差极低并实现93%能耗与采样量节省。


<details>
  <summary>Details</summary>
Motivation: 通过让传感器自主决定是否采样，提高能源效率并减少信息丢失的风险。

Method: 基于模仿自主神经系统的概率神经元（p-neuron），结合模拟特征提取电路，实现微秒级响应的概率化采样决策。

Result: 在主动地震调查数据验证中，概率采样实现了0.41%的均方根误差，并在系统运行时间和生成样本数量上分别节省了93%。

Conclusion: 通过概率化采样可实现无信息损失的自适应数据获取，同时显著降低能耗与采样量。

Abstract: Extending the intelligence of sensors to the data-acquisition process - deciding whether to sample or not - can result in transformative energy-efficiency gains. However, making such a decision in a deterministic manner involves risk of losing information. Here we present a sensing paradigm that enables making such a decision in a probabilistic manner. The paradigm takes inspiration from the autonomous nervous system and employs a probabilistic neuron (p-neuron) driven by an analog feature extraction circuit. The response time of the system is on the order of microseconds, over-coming the sub-sampling-rate response time limit and enabling real-time intelligent autonomous activation of data-sampling. Validation experiments on active seismic survey data demonstrate lossless probabilistic data acquisition, with a normalized mean squared error of 0.41%, and 93% saving in the active operation time of the system and the number of generated samples.

</details>


### [17] [MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference](https://arxiv.org/abs/2601.19961)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Ruijia Wu,Li YanTao,Qiang Hui,Yuren You,Ting Lu,Chao Tan,Shaoan Zhao,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.LG

TL;DR: MeanCache是一种无训练的缓存框架，利用平均速度和JVP缓存，配合峰值抑制调度，在流匹配中显著提升计算效率和生成质量，适用于大型生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有缓存依赖瞬时速度导致高加速率下轨迹偏差与误差累积。

Method: 利用缓存的Jacobian-Vector Products构造区间平均速度，结合峰值抑制最短路径调度以优化缓存时机与JVP重用，形成无训练的MeanCache框架。

Result: 在FLUX.1、Qwen-Image和HunyuanVideo上分别实现4.12X、4.56X、3.59X的加速，同时保持或提升生成质量。

Conclusion: MeanCache通过平均速度视角和缓存JVP实现高效的流匹配推理，显著减小误差积累并提高生成质量；被证明在多大语言/视觉模型中 outperform 现有缓存基线。

Abstract: We present MeanCache, a training-free caching framework for efficient Flow Matching inference. Existing caching methods reduce redundant computation but typically rely on instantaneous velocity information (e.g., feature caching), which often leads to severe trajectory deviations and error accumulation under high acceleration ratios. MeanCache introduces an average-velocity perspective: by leveraging cached Jacobian--vector products (JVP) to construct interval average velocities from instantaneous velocities, it effectively mitigates local error accumulation. To further improve cache timing and JVP reuse stability, we develop a trajectory-stability scheduling strategy as a practical tool, employing a Peak-Suppressed Shortest Path under budget constraints to determine the schedule. Experiments on FLUX.1, Qwen-Image, and HunyuanVideo demonstrate that MeanCache achieves 4.12X and 4.56X and 3.59X acceleration, respectively, while consistently outperforming state-of-the-art caching baselines in generation quality. We believe this simple yet effective approach provides a new perspective for Flow Matching inference and will inspire further exploration of stability-driven acceleration in commercial-scale generative models.

</details>


### [18] [Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment](https://arxiv.org/abs/2601.19963)
*Canyang Zhao,Bolin Peng,J. Patrick Mayo,Ce Ju,Bing Liu*

Main category: cs.LG

TL;DR: 通过任务条件的潜在对齐，TCLA解决了BCI跨会话非平稳性问题，显著提升解码精度。


<details>
  <summary>Details</summary>
Motivation: 跨会话非平稳性导致BCI解码器在新会话中表现不佳，且新会话数据有限时重新训练困难。

Method: 使用自编码器获得源会话低维表示，随后在任务条件下对目标会话的潜在表示进行对齐，实现知识迁移。

Result: 在猕猴运动及眼动中心的移出实验中，TCLA相较于仅用目标会话数据训练的基线方法，显著提高了解码性能，使y坐标速度的决定系数提升多达0.386。

Conclusion: TCLA为在样本有限的跨会话神经解码提供一种有效的迁移学习框架，增强了解码器的稳健性。

Abstract: Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.

</details>


### [19] [Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization](https://arxiv.org/abs/2601.20226)
*Julian Gutierrez,Redouane Silvente*

Main category: cs.LG

TL;DR: 利用快速参数模型和生成式模型预测日内供需曲线，并基于此优化储能策略，发现价格压缩效应和收益递减规律。


<details>
  <summary>Details</summary>
Motivation: 为EPEX SPOT日内市场提供高效、可解释的供需曲线预测，并通过优化储能策略提升市场参与者的收益，实现更稳健的电力交易决策。

Method: 1) 快速参数化模型：利用低维网格鲁棒表示，结合最小最大体量和切比雪夫多项式捕捉弹性段，实现每日低误差、易解释的时序预测。 2) 生成式模型：学习给定气象与燃料变量下24小时订单级提交的联合分布，生成合成日常情景并聚合成供需曲线。 3) 优化模块：基于预测曲线导出价格制定的储能策略，计算收益分布并评估容量对收益的边际影响。

Result: 快速模型实现了每日低误差和透明可解释性；生成式模型提供了更全面的情景分析，尽管不适合日常操作。储能优化捕捉到收益分布，显示出价格压缩效应和容量递减收益。

Conclusion: 两种机器学习框架实现了对EPEX SPOT日内市场聚合曲线的精准预测，并在此基础上通过价格制定型储能策略实现了收益最大化，揭示了价格压缩效应：峰值降低、低谷提升以及容量增大导致收益递减。

Abstract: We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.

</details>


### [20] [Modeling Cascaded Delay Feedback for Online Net Conversion Rate Prediction: Benchmark, Insights and Solutions](https://arxiv.org/abs/2601.19965)
*Mingxuan Luo,Guipeng Xv,Sishuo Chen,Xinyu Li,Li Zhang,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Bo Zheng,Chen Lin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In industrial recommender systems, conversion rate (CVR) is widely used for traffic allocation, but it fails to fully reflect recommendation effectiveness because it ignores refund behavior. To better capture true user satisfaction and business value, net conversion rate (NetCVR), defined as the probability that a clicked item is purchased and not refunded, has been proposed.Unlike CVR, NetCVR prediction involves a more complex multi-stage cascaded delayed feedback process. The two cascaded delays from click to conversion and from conversion to refund have opposite effects, making traditional CVR modeling methods inapplicable. Moreover, the lack of open-source datasets and online continuous training schemes further hinders progress in this area.To address these challenges, we introduce CASCADE (Cascaded Sequences of Conversion and Delayed Refund), the first large-scale open dataset derived from the Taobao app for online continuous NetCVR prediction. Through an in-depth analysis of CASCADE, we identify three key insights: (1) NetCVR exhibits strong temporal dynamics, necessitating online continuous modeling; (2) cascaded modeling of CVR and refund rate outperforms direct NetCVR modeling; and (3) delay time, which correlates with both CVR and refund rate, is an important feature for NetCVR prediction.Based on these insights, we propose TESLA, a continuous NetCVR modeling framework featuring a CVR-refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss. Extensive experiments demonstrate that TESLA consistently outperforms state-of-the-art methods on CASCADE, achieving absolute improvements of 12.41 percent in RI-AUC and 14.94 percent in RI-PRAUC on NetCVR prediction. The code and dataset are publicly available at https://github.com/alimama-tech/NetCVR.

</details>


### [21] [Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers](https://arxiv.org/abs/2601.19967)
*Jinlin Liu,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.

</details>


### [22] [Learning Contextual Runtime Monitors for Safe AI-Based Autonomy](https://arxiv.org/abs/2601.20666)
*Alejandro Luque-Cerpa,Mengyuan Wang,Emil Carlsson,Sanjit A. Seshia,Devdatt Dubhashi,Hazem Torfah*

Main category: cs.LG

TL;DR: 本文设计了一个基于上下文学习的运行时监视器，利用上下文多臂老虎机算法在不同环境下挑选最优AI控制器，提供安全保证并提升决策质量，已在仿真驾驶实验中得到验证。


<details>
  <summary>Details</summary>
Motivation: 需要在陌生环境下保持AI控制器的安全性与性能，传统融合方法往往削弱各控制器在不同上下文中的优势。

Method: 构建基于上下文的运行时监视器，通过上下文多臂老虎机框架学习在不同情境下选择最合适的控制器。

Result: 在两种仿真自动驾驶场景中，比非上下文基线显著提升了安全性和性能。

Conclusion: 提出的监视框架通过上下文感知实现理论安全保证并充分利用控制器多样性，提升了AI控制集成的安全与表现。

Abstract: We introduce a novel framework for learning context-aware runtime monitors for AI-based control ensembles. Machine-learning (ML) controllers are increasingly deployed in (autonomous) cyber-physical systems because of their ability to solve complex decision-making tasks. However, their accuracy can degrade sharply in unfamiliar environments, creating significant safety concerns. Traditional ensemble methods aim to improve robustness by averaging or voting across multiple controllers, yet this often dilutes the specialized strengths that individual controllers exhibit in different operating contexts. We argue that, rather than blending controller outputs, a monitoring framework should identify and exploit these contextual strengths. In this paper, we reformulate the design of safe AI-based control ensembles as a contextual monitoring problem. A monitor continuously observes the system's context and selects the controller best suited to the current conditions. To achieve this, we cast monitor learning as a contextual learning task and draw on techniques from contextual multi-armed bandits. Our approach comes with two key benefits: (1) theoretical safety guarantees during controller selection, and (2) improved utilization of controller diversity. We validate our framework in two simulated autonomous driving scenarios, demonstrating significant improvements in both safety and performance compared to non-contextual baselines.

</details>


### [23] [BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection](https://arxiv.org/abs/2601.19992)
*Soham Sarkar,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: Bayesian meta‑learning with probabilistic prototypes and Student‑t predictive improves few‑shot industrial image anomaly detection over existing methods.


<details>
  <summary>Details</summary>
Motivation: Industrial image anomaly detection faces extreme class imbalance and scarce defective samples, especially under few‑shot settings.

Method: BayPrAnoMeta—Bayesian adaption of Proto‑MAML. It replaces deterministic prototypes with task‑specific probabilistic normality models (NIW prior → Student‑t predictive). Inner‑loop adapts via Bayesian posterior predictive likelihood, enabling uncertainty‑aware, heavy‑tailed anomaly scoring. It extends to a federated meta‑learning framework with supervised contrastive regularisation and provably converges to stationary points.

Result: On the MVTec AD benchmark, BayPrAnoMeta consistently outperforms MAML, Proto‑MAML and PatchCore in few‑shot anomaly detection, yielding significant AUROC gains.

Conclusion: BayPrAnoMeta offers a robust, uncertainty‑aware, few‑shot anomaly detection method validated on a standard industrial benchmark.

Abstract: Industrial image anomaly detection is a challenging problem owing to extreme class imbalance and the scarcity of labeled defective samples, particularly in few-shot settings. We propose BayPrAnoMeta, a Bayesian generalization of Proto-MAML for few-shot industrial image anomaly detection. Unlike existing Proto-MAML approaches that rely on deterministic class prototypes and distance-based adaptation, BayPrAnoMeta replaces prototypes with task-specific probabilistic normality models and performs inner-loop adaptation via a Bayesian posterior predictive likelihood. We model normal support embeddings with a Normal-Inverse-Wishart (NIW) prior, producing a Student-$t$ predictive distribution that enables uncertainty-aware, heavy-tailed anomaly scoring and is essential for robustness in extreme few-shot settings. We further extend BayPrAnoMeta to a federated meta-learning framework with supervised contrastive regularization for heterogeneous industrial clients and prove convergence to stationary points of the resulting nonconvex objective. Experiments on the MVTec AD benchmark demonstrate consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.

</details>


### [24] [Decomposing multimodal embedding spaces with group-sparse autoencoders](https://arxiv.org/abs/2601.20028)
*Chiraag Kaushik,Davis Barch,Andrea Fanelli*

Main category: cs.LG

TL;DR: 改进SAE以跨模态随机掩蔽+组稀疏正则化，解决“分裂词典”问题，提升多模态嵌入解释性与可控性。


<details>
  <summary>Details</summary>
Motivation: 传统SAE在多模态空间学习“分裂词典”，导致大多数稀疏特征仅适用于单一模态。

Method: 采用跨模态随机遮掩和组稀疏正则化的SAE框架，避免出现单模态词典。

Result: 新方法生成更具跨模态性的词典，减少死亡神经元，提升特征语义性，并在CLIP、CLAP嵌入上验证效果。

Conclusion: SAE在多模态嵌入空间中可通过改造来获得更好的跨模态对齐，生成更多跨模态词典，从而提升解释性与可控性。

Abstract: The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spaces (such as the popular CLIP embeddings for image/text data) have found that SAEs often learn "split dictionaries", where most of the learned sparse features are essentially unimodal, active only for data of a single modality. In this work, we study how to effectively adapt SAEs for the setting of multimodal embeddings while ensuring multimodal alignment. We first argue that the existence of a split dictionary decomposition on an aligned embedding space implies the existence of a non-split dictionary with improved modality alignment. Then, we propose a new SAE-based approach to multimodal embedding decomposition using cross-modal random masking and group-sparse regularization. We apply our method to popular embeddings for image/text (CLIP) and audio/text (CLAP) data and show that, compared to standard SAEs, our approach learns a more multimodal dictionary while reducing the number of dead neurons and improving feature semanticity. We finally demonstrate how this improvement in alignment of concepts between modalities can enable improvements in the interpretability and control of cross-modal tasks.

</details>


### [25] [A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography](https://arxiv.org/abs/2601.20291)
*Kaiyi Yang,Seonyeong Park,Gangwon Jeong,Hsuan-Kai Huang,Alexander A. Oraevsky,Umberto Villa,Mark A. Anastasio*

Main category: cs.LG

TL;DR: 研究提出通过U‑Net和 Deconv‑Net 在数据域补偿 SIR，提升三维光声成像的分辨率与细节，同时保持高效的解析重建。


<details>
  <summary>Details</summary>
Motivation: 传统PACT利用大面积超声探头可提升检测灵敏度，但忽略SIR会削弱空间分辨率。优化型重建可补偿SIR，却计算量巨大，尤其在3D应用中难以实现实时性。迫切需要一种既能准确补偿SIR又具备快速重建特性的方案。

Method: 采用机器学习方法构建两种补偿模型：U‑Net结构以及基于物理启发的 Deconv‑Net。训练数据通过快速解析生成，在低噪声、高复杂度和介质异质性条件下进行虚拟实验，随后在体内乳腺成像中检验模型效果。

Result: 虚拟成像验证显示补偿模型在不同噪声、对象复杂性和声速变化下均能恢复细节并提升分辨率。体内乳腺成像结果表明，补偿后可识别先前被SIR伪影掩蔽的细小结构。

Conclusion: 本研究首次演示了在三维光声计算机断层成像（PACT）中使用已学习的空间脉冲响应（SIR）补偿方法。通过在数据域中映射受SIR影响的测量数据为理想点探头可获取的数据，结合快速的解析重建算法，既实现了对SIR效应的补偿，又保持了高效性，从而显著提升了成像分辨率与细节可见性。

Abstract: Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.

</details>


### [26] [CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs](https://arxiv.org/abs/2601.20041)
*Shih-Hsuan Chiu,Ming-Syan Chen*

Main category: cs.LG

TL;DR: 新时代边缘LLM辅助，TONEL用鲁棒嵌入克服CiM噪声，实现精准多域检索。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上RAG面临快速增长的个人化数据导致效率瓶颈，以及CiM架构对环境噪声敏感，亟需提升检索精度与适应性。

Method: 提出任务导向的噪声鲁棒嵌入学习框架，使用噪声感知投影模型生成与CiM硬件兼容的任务特定嵌入，从而实现准确检索。

Result: 在多项个性化基准实验中，TONEL在任务特定的噪声场景下明显优于强基线，表现出更高的检索准确率和更好的域迁移性能。

Conclusion: TONEL在噪声环境下提升了RAG的检索精准度与领域适应性，验证了其在个人化与多领域应用中优越的实用性。

Abstract: Personalized virtual assistants powered by large language models (LLMs) on edge devices are attracting growing attention, with Retrieval-Augmented Generation (RAG) emerging as a key method for personalization by retrieving relevant profile data and generating tailored responses. However, deploying RAG on edge devices faces efficiency hurdles due to the rapid growth of profile data, such as user-LLM interactions and recent updates. While Computing-in-Memory (CiM) architectures mitigate this bottleneck by eliminating data movement between memory and processing units via in-situ operations, they are susceptible to environmental noise that can degrade retrieval precision. This poses a critical issue in dynamic, multi-domain edge-based scenarios (e.g., travel, medicine, and law) where both accuracy and adaptability are paramount. To address these challenges, we propose Task-Oriented Noise-resilient Embedding Learning (TONEL), a framework that improves noise robustness and domain adaptability for RAG in noisy edge environments. TONEL employs a noise-aware projection model to learn task-specific embeddings compatible with CiM hardware constraints, enabling accurate retrieval under noisy conditions. Extensive experiments conducted on personalization benchmarks demonstrate the effectiveness and practicality of our methods relative to strong baselines, especially in task-specific noisy scenarios.

</details>


### [27] [Regime-Adaptive Bayesian Optimization via Dirichlet Process Mixtures of Gaussian Processes](https://arxiv.org/abs/2601.20043)
*Yan Zhang,Xuefeng Liu,Sipeng Chen,Sascha Ranftl,Chong Liu,Shibo Li*

Main category: cs.LG

TL;DR: RAMBO利用Dirichlet过程混合GP自动划分多区域搜索空间，并通过在每个区域内独立的GP模型和自适应采集函数，在复杂非均匀任务中实现贝叶斯优化的显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统单GP在多域问题（如分子构象搜索、药物筛选和核聚变反应堆设计）中无法兼顾不同区域的光滑性与尖锐转折，导致过度平滑或噪声幻觉，误导不确定性估计。

Method: 使用Dirichlet过程混合高斯过程模型，结合收缩吉布斯采样实现高效推断，并通过自适应浓度参数调度实现从粗到细的域发现；收集的采集函数将不确定性分解为域内与域间两部分。

Result: 在合成基准和真实应用上均比现有基准方法取得一致的性能提升，特别是在分子构象优化、药物虚拟筛选和核聚变器设计等多域目标任务中。

Conclusion: RAMBO通过混合高斯过程自动识别多域问题中的潜在区域，并在每个区域内使用本地优化的超参数模型，从而在多尺度、非均匀的搜索空间中显著提升贝叶斯优化性能。

Abstract: Standard Bayesian Optimization (BO) assumes uniform smoothness across the search space an assumption violated in multi-regime problems such as molecular conformation search through distinct energy basins or drug discovery across heterogeneous molecular scaffolds. A single GP either oversmooths sharp transitions or hallucinates noise in smooth regions, yielding miscalibrated uncertainty. We propose RAMBO, a Dirichlet Process Mixture of Gaussian Processes that automatically discovers latent regimes during optimization, each modeled by an independent GP with locally-optimized hyperparameters. We derive collapsed Gibbs sampling that analytically marginalizes latent functions for efficient inference, and introduce adaptive concentration parameter scheduling for coarse-to-fine regime discovery. Our acquisition functions decompose uncertainty into intra-regime and inter-regime components. Experiments on synthetic benchmarks and real-world applications, including molecular conformer optimization, virtual screening for drug discovery, and fusion reactor design, demonstrate consistent improvements over state-of-the-art baselines on multi-regime objectives.

</details>


### [28] [PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting](https://arxiv.org/abs/2601.20845)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.

</details>


### [29] [Externally Validated Longitudinal GRU Model for Visit-Level 180-Day Mortality Risk in Metastatic Castration-Resistant Prostate Cancer](https://arxiv.org/abs/2601.20046)
*Javier Mencia-Ledo,Mohammad Noaeen,Zahra Shakeri*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Metastatic castration-resistant prostate cancer (mCRPC) is a highly aggressive disease with poor prognosis and heterogeneous treatment response. In this work, we developed and externally validated a visit-level 180-day mortality risk model using longitudinal data from two Phase III cohorts (n=526 and n=640). Only visits with observable 180-day outcomes were labeled; right-censored cases were excluded from analysis. We compared five candidate architectures: Long Short-Term Memory, Gated Recurrent Unit (GRU), Cox Proportional Hazards, Random Survival Forest (RSF), and Logistic Regression. For each dataset, we selected the smallest risk-threshold that achieved an 85% sensitivity floor. The GRU and RSF models showed high discrimination capabilities initially (C-index: 87% for both). In external validation, the GRU obtained a higher calibration (slope: 0.93; intercept: 0.07) and achieved an PR-AUC of 0.87. Clinical impact analysis showed a median time-in-warning of 151.0 days for true positives (59.0 days for false positives) and 18.3 alerts per 100 patient-visits. Given late-stage frailty or cachexia and hemodynamic instability, permutation importance ranked BMI and systolic blood pressure as the strongest associations. These results suggest that longitudinal routine clinical markers can estimate short-horizon mortality risk in mCRPC and support proactive care planning over a multi-month window.

</details>


### [30] [Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning](https://arxiv.org/abs/2601.20069)
*Chi-Yao Huang,Khoa Vo,Aayush Atul Verma,Duo Lu,Yezhou Yang*

Main category: cs.LG

TL;DR: 提出正交池化的域扩展框架，解决多目标冲突导致的潜在坍塌，在多任务基准上显著提升表现并提供可解释潜在空间


<details>
  <summary>Details</summary>
Motivation: 为解决多目标训练导致的互相冲突梯度导致潜在表示坍塌问题

Method: 引入域扩展框架，利用正交池化构建每个目标对应的正交子空间的潜在空间

Result: 在ShapeNet、MPIIGaze、Rotated MNIST等基准上验证，不仅防止了坍塌，且得到可解释、可组合的潜在空间，可直接操控概念

Conclusion: 正交池化的域扩展方法有效预防潜在表示坍塌，并提升多目标系统性能与可解释性

Abstract: Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task--a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach across diverse benchmarks--including ShapeNet, MPIIGaze, and Rotated MNIST--on challenging multi-objective problems combining classification with pose and gaze estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.

</details>


### [31] [Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery](https://arxiv.org/abs/2601.20088)
*Meng Xin,Sweta Priyadarshi,Jingyu Xin,Bilal Kartal,Aditya Vavre,Asma Kuriparambil Thekkumpate,Zijia Chen,Ameya Sunil Mahabaleshwarkar,Ido Shahaf,Akhiad Bercovich,Kinjal Patel,Suguna Varshini Velury,Chenjie Luo,Zhiyu Cheng,Jenny Chen,Chen-Han Yu,Wei Ping,Oleg Rybakov,Nima Tajbakhsh,Oluwatobi Olabiyi,Dusan Stosic,Di Wu,Song Han,Eric Chung,Sharath Turuvekere Sreenivas,Bryan Catanzaro,Yoshi Suhara,Tijmen Blankevoort,Huizi Mao*

Main category: cs.LG

TL;DR: 提出 QAD 方法，结合 KL 损失在 NVFP4 量化后恢复 LLM/VLM 准确率，克服传统 QAT 的工程与训练不稳定，且对数据需求低，实验显示可恢复至 BF16 级别


<details>
  <summary>Details</summary>
Motivation: 为大规模语言模型（LLMs）和视觉-语言模型（VLMs）实现 NVFP4 量化后准确率恢复

Method: 量化感知蒸馏（QAD），用 KL 散度损失将全精度教师模型蒸馏到量化学生模型

Result: 在多阶段后训练流程（SFT、RL、模型合并）中，QAD 显著提高稳定性与效果，且不依赖完整训练数据，恢复至接近 BF16 的准确率；在 AceReason Nemotron 系列及 Llama Nemotron Super v1 等模型上验证一致性

Conclusion: QAD 是针对多阶段后训练大模型的稳健方法，具备高效、低数据需求且易于实施的优势，适合实际部署量化模型

Abstract: This technical report presents quantization-aware distillation (QAD) and our best practices for recovering accuracy of NVFP4-quantized large language models (LLMs) and vision-language models (VLMs). QAD distills a full-precision teacher model into a quantized student model using a KL divergence loss. While applying distillation to quantized models is not a new idea, we observe key advantages of QAD for today's LLMs: 1. It shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including supervised fine-tuning (SFT), reinforcement learning (RL), and model merging, where traditional quantization-aware training (QAT) suffers from engineering complexity and training instability; 2. It is robust to data quality and coverage, enabling accuracy recovery without full training data. We evaluate QAD across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL (VLM), and Llama Nemotron Super v1, showing consistent recovery to near-BF16 accuracy.

</details>


### [32] [In-Context Reinforcement Learning From Suboptimal Historical Data](https://arxiv.org/abs/2601.20116)
*Juncheng Dong,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: DIT uses a value network to weight trajectories, enabling a transformer to learn higher‑quality policies from suboptimal offline data and outperform standard imitation learning on diverse RL tasks.


<details>
  <summary>Details</summary>
Motivation: While autoregressive transformers excel at in‐context learning, naive inference on suboptimal offline data reduces to imitation learning and yields poor performance. There is a need for a method that can correct for suboptimal behavior directly within the transformer framework.

Method: 1) Train a transformer‐based value network to estimate advantages of behavior policies that generated offline trajectories. 2) Use this value estimator to construct importance weights and then train a transformer policy by weighted maximum likelihood. 3) Deploy the pretrained transformer as a fixed policy for new RL tasks.

Result: On a suite of bandit and MDP problems, DIT outperformed baseline autoregressive training, especially when the offline data was dominated by suboptimal trajectories, demonstrating the effectiveness of incorporating value‑based importance weighting.

Conclusion: Decision Importance Transformer (DIT) demonstrates that an in-context learning framework with a value-aware weighted policy objective can outperform standard imitation learning when deployed on offline datasets containing suboptimal trajectories, achieving superior performance on both bandit and MDP benchmarks.

Abstract: Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

</details>


### [33] [A Reinforcement Learning Based Universal Sequence Design for Polar Codes](https://arxiv.org/abs/2601.20118)
*David Kin Wai Ho,Arman Fazeli,Mohamad M. Mansour,Louay M. A. Jalloul*

Main category: cs.LG

TL;DR: 本文提出基于强化学习的Polar码序列设计框架，支持至2048位码长，对5G NR序列具备竞争力，并在2048位时比传统方法提升0.2dB。


<details>
  <summary>Details</summary>
Motivation: 提升6G应用下的Polar码设计，适应多样通道与解码策略

Method: 利用强化学习的通用序列设计框架，结合物理约束、弱长期影响与多配置联合优化

Result: 代码长度可达2048，性能与5G NR序列相当，在2048码长时比beta-expansion平均提升0.2dB

Conclusion: 所提方法实现可扩展、适应式Polar码设计，适用于标准化进程，显著提升性能

Abstract: To advance Polar code design for 6G applications, we develop a reinforcement learning-based universal sequence design framework that is extensible and adaptable to diverse channel conditions and decoding strategies. Crucially, our method scales to code lengths up to $2048$, making it suitable for use in standardization. Across all $(N,K)$ configurations supported in 5G, our approach achieves competitive performance relative to the NR sequence adopted in 5G and yields up to a 0.2 dB gain over the beta-expansion baseline at $N=2048$. We further highlight the key elements that enabled learning at scale: (i) incorporation of physical law constrained learning grounded in the universal partial order property of Polar codes, (ii) exploitation of the weak long term influence of decisions to limit lookahead evaluation, and (iii) joint multi-configuration optimization to increase learning efficiency.

</details>


### [34] [Going NUTS with ADVI: Exploring various Bayesian Inference techniques with Facebook Prophet](https://arxiv.org/abs/2601.20120)
*Jovan Krajevski,Biljana Tojtovska Ribarski*

Main category: cs.LG

TL;DR: 重新实现Prophet于PyMC，以实现多推理方法并评估其在预测中的优缺点


<details>
  <summary>Details</summary>
Motivation: Facebook Prophet的默认推理方法无法满足多样化的Bayesian推理需求，且其API设计缺乏灵活性，导致无法实现自定义模型

Method: 基于PyMC实现完整的Prophet模型，支持MAP、全MCMC、变分推断等多种贝叶斯推理方法，并对模型的实现细节进行阐述

Result: 对不同推理技术在时间序列预测中的表现进行了评估，探讨了采样方案、收敛诊断、预测指标及计算效率，发现存在潜在问题并提出未来研究方向

Conclusion: 通过PyMC实现的Prophet模型为研究者提供了更灵活的贝叶斯推理工具，能够在时间序列预测中比较MAP、MCMC和变分推断的效果，为未来改进模型和算法奠定基础

Abstract: Since its introduction, Facebook Prophet has attracted positive attention from both classical statisticians and the Bayesian statistics community. The model provides two built-in inference methods: maximum a posteriori estimation using the L-BFGS-B algorithm, and Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS). While exploring various time-series forecasting problems using Bayesian inference with Prophet, we encountered limitations stemming from the inability to apply alternative inference techniques beyond those provided by default. Additionally, the fluent API design of Facebook Prophet proved insufficiently flexible for implementing our custom modeling ideas. To address these shortcomings, we developed a complete reimplementation of the Prophet model in PyMC, which enables us to extend the base model and evaluate and compare multiple Bayesian inference methods. In this paper, we present our PyMC-based implementation and analyze in detail the implementation of different Bayesian inference techniques. We consider full MCMC techniques, MAP estimation and Variational inference techniques on a time-series forecasting problem. We discuss in details the sampling approach, convergence diagnostics, forecasting metrics as well as their computational efficiency and detect possible issues which will be addressed in our future work.

</details>


### [35] [Membership Inference Attacks Against Fine-tuned Diffusion Language Models](https://arxiv.org/abs/2601.20125)
*Yuetian Chen,Kaiyuan Zhang,Yuntao Du,Edoardo Stoppa,Charles Fleming,Ashish Kundu,Bruno Ribeiro,Ninghui Li*

Main category: cs.LG

TL;DR: 提出 SAMA，利用 DLM 的多遮蔽特性设计鲁棒的成员推断攻击，实验表明相较基线提升 30% AUC，显示 DLM 存在严重隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 虽然 DLM 通过双向遮蔽实现更强建模，但其多重可遮蔽配置在隐私泄露方面尚未被系统研究，需了解其受攻击的程度。

Method: SAMA（Subset‑Aggregated Membership Attack）通过在不同遮蔽稠密度下随机取样子集，采用基于符号的统计量抵御重尾噪声，并使用逆加权聚合优先考虑稀疏遮蔽产生的更干净信号，将稀疏记忆检测转化为鲁棒投票机制。

Result: SAMA 在九个公共数据集上与最佳基线相比，AUC 绝对提升约 30%（相对提升），在低误报率设置下提升可达 8 倍，显著提高检测能力。

Conclusion: 本文发现扩散语言模型（DLM）在成员推断攻击（MIA）下存在显著漏洞，提出的 SAMA 攻击在九个数据集上相对基线提升 AUC 约 30%，低误报率下可达 8 倍。该结果凸显了 DLM 的隐私风险，亟需专门的防护策略。

Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over the best baseline, with up to 8 times improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.

</details>


### [36] [Scaling Next-Brain-Token Prediction for MEG](https://arxiv.org/abs/2601.20138)
*Richard Csaky*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce three task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.

</details>


### [37] [Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning](https://arxiv.org/abs/2601.20154)
*Bo Dai,Na Li,Dale Schuurmans*

Main category: cs.LG

TL;DR: 提出将自监督学习映射到谱空间，通过分析谱特征从而统一各类 SSL 目标，形成可指导新算法设计的理论框架。


<details>
  <summary>Details</summary>
Motivation: 本文指出现有自监督学习方法缺乏统一理论框架，导致实践中无法充分解释和优化算法。

Method: 通过谱表示角度理论探讨表征充分性，揭示已有成功 SSL 算法的谱精髓，构建统一框架以解释与分析。

Result: 构建了以谱视角为核心的统一框架，为理解现有 SSL 方法提供理论依据，也为设计更高效、更易用的表征学习算法指明路径。

Conclusion: 本文提供了一个理论基础，统一了自监督学习的核心思想，有助于系统化研究与实际应用。

Abstract: Self-supervised learning (SSL) have improved empirical performance by unleashing the power of unlabeled data for practical applications. Specifically, SSL extracts the representation from massive unlabeled data, which will be transferred to a plenty of down streaming tasks with limited data. The significant improvement on diverse applications of representation learning has attracted increasing attention, resulting in a variety of dramatically different self-supervised learning objectives for representation extraction, with an assortment of learning procedures, but the lack of a clear and unified understanding. Such an absence hampers the ongoing development of representation learning, leaving a theoretical understanding missing, principles for efficient algorithm design unclear, and the use of representation learning methods in practice unjustified. The urgency for a unified framework is further motivated by the rapid growth in representation learning methods. In this paper, we are therefore compelled to develop a principled foundation of representation learning. We first theoretically investigate the sufficiency of the representation from a spectral representation view, which reveals the spectral essence of the existing successful SSL algorithms and paves the path to a unified framework for understanding and analysis. Such a framework work also inspires the development of more efficient and easy-to-use representation learning algorithms with principled way in real-world applications.

</details>


### [38] [PASS: Ambiguity Guided Subsets for Scalable Classical and Quantum Constrained Clustering](https://arxiv.org/abs/2601.20157)
*Pedro Chumpitaz-Flores,My Duong,Ying Mao,Kaixun Hua*

Main category: cs.LG

TL;DR: PASS通过约束感知与信息几何子集选择，实现高效且准确的约束聚类，显著提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 与ML/CL约束相关的聚类方法在大规模和特殊应用（如量子聚类）中难以扩展，需寻找既满足约束又能高效处理大数据的方法。

Method: PASS通过将ML约束转换为伪点，随后使用两种选择器（约束感知边缘规则与Fisher‑Rao距离信息几何规则）对样本进行子集选择。在预算范围内选取最具信息量或与边界/CL违约相关的点，进行聚类。

Result: 在多个基准测试中，PASS在保持竞争性SSE的同时，比精确或惩罚式方法大幅降低计算成本，并在传统方法表现不佳的模式下表现优秀。

Conclusion: PASS框架在保持严格的ML/CL约束满足的同时，实现了可扩展、高质量的聚类，显著降低了计算成本并在传统方法失效的情境下仍保持竞争力。

Abstract: Pairwise-constrained clustering augments unsupervised partitioning with side information by enforcing must-link (ML) and cannot-link (CL) constraints between specific samples, yielding labelings that respect known affinities and separations. However, ML and CL constraints add an extra layer of complexity to the clustering problem, with current methods struggling in data scalability, especially in niche applications like quantum or quantum-hybrid clustering. We propose PASS, a pairwise-constraints and ambiguity-driven subset selection framework that preserves ML and CL constraints satisfaction while allowing scalable, high-quality clustering solution. PASS collapses ML constraints into pseudo-points and offers two selectors: a constraint-aware margin rule that collects near-boundary points and all detected CL violations, and an information-geometric rule that scores points via a Fisher-Rao distance derived from soft assignment posteriors, then selects the highest-information subset under a simple budget. Across diverse benchmarks, PASS attains competitive SSE at substantially lower cost than exact or penalty-based methods, and remains effective in regimes where prior approaches fail.

</details>


### [39] [Local Duality for Sparse Support Vector Machines](https://arxiv.org/abs/2601.20170)
*Penghe Zhang,Naihua Xiu,Houduo Qi*

Main category: cs.LG

TL;DR: 论文通过建立SSVM的局部对偶性理论，证明其等价于0/1‑损失SVM，并利用局部解指导hSVM与rSVM的超参数选择，实验显示SSVM在此策略下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 缺乏对在凸SVM对偶中加入稀疏性函数（如ℓ0范数）的理论支持，而该做法在经验上表现优于传统SVM。

Method: 构建了SSVM的局部对偶性理论，将其与hinge-loss SVM（hSVM）和ramp-loss SVM（rSVM）联系起来，证明了SSVM即为0/1损失SVM对偶；在特定条件下展示了hSVM全局解序列收敛到0/1损失SVM局部解，并证明0/1损失SVM的局部极小点也是rSVM的局部极小点。

Result: 1）SSVM等效于0/1‑损失SVM的对偶；2）局部解满足线性表示定理；3）在特定条件下全局hSVM解收敛至0/1‑损失SVM局部解；4）0/1损失SVM局部极小点为rSVM局部极小点；5）实验验证了SSVM在本论文提出的局部解策略下具备潜在优势。

Conclusion: 本文证明了基于卡迪纳尔性最小化的稀疏支持向量机（SSVM）等价于0/1损失SVM的对偶问题，并对其局部解满足线性表示定理进行说明。通过该理论，局部解还能为hSVM和rSVM选择超参数提供实际指导，并解释了SSVM在先前实验中优于这两种方法的原因。

Abstract: Due to the rise of cardinality minimization in optimization, sparse support vector machines (SSVMs) have attracted much attention lately and show certain empirical advantages over convex SVMs. A common way to derive an SSVM is to add a cardinality function such as $\ell_0$-norm to the dual problem of a convex SVM. However, this process lacks theoretical justification. This paper fills the gap by developing a local duality theory for such an SSVM formulation and exploring its relationship with the hinge-loss SVM (hSVM) and the ramp-loss SVM (rSVM). In particular, we prove that the derived SSVM is exactly the dual problem of the 0/1-loss SVM, and the linear representer theorem holds for their local solutions. The local solution of SSVM also provides guidelines on selecting hyperparameters of hSVM and rSVM. {Under specific conditions, we show that a sequence of global solutions of hSVM converges to a local solution of 0/1-loss SVM. Moreover, a local minimizer of 0/1-loss SVM is a local minimizer of rSVM.} This explains why a local solution induced by SSVM outperforms hSVM and rSVM in the prior empirical study. We further conduct numerical tests on real datasets and demonstrate potential advantages of SSVM by working with locally nice solutions proposed in this paper.

</details>


### [40] [Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization](https://arxiv.org/abs/2601.20172)
*James Amarel,Robyn Miller,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study how neural emulators of partial differential equation solution operators internalize physical symmetries by introducing an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states, defined as the metric-weighted overlap of loss gradients evaluated along group orbits. This quantity probes the local geometry of the learned loss landscape and goes beyond forward-pass equivariance tests by directly assessing whether learning dynamics couple physically equivalent configurations. Applying our diagnostic to autoregressive fluid flow emulators, we show that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry compatible basin. The result is a novel technique for evaluating if surrogate models have internalized symmetry properties of the known solution operator.

</details>


### [41] [MAPLE: Self-supervised Learning-Enhanced Nonlinear Dimensionality Reduction for Visual Analysis](https://arxiv.org/abs/2601.20173)
*Zeyang Huang,Takanori Fujiwara,Angelos Chatzimparmpas,Wandrille Duchemin,Andreas Kerren*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a new nonlinear dimensionality reduction method, MAPLE, that enhances UMAP by improving manifold modeling. MAPLE employs a self-supervised learning approach to more efficiently encode low-dimensional manifold geometry. Central to this approach are maximum manifold capacity representations (MMCRs), which help untangle complex manifolds by compressing variances among locally similar data points while amplifying variance among dissimilar data points. This design is particularly effective for high-dimensional data with substantial intra-cluster variance and curved manifold structures, such as biological or image data. Our qualitative and quantitative evaluations demonstrate that MAPLE can produce clearer visual cluster separations and finer subcluster resolution than UMAP while maintaining comparable computational cost.

</details>


### [42] [NeuraLSP: An Efficient and Rigorous Neural Left Singular Subspace Preconditioner for Conjugate Gradient Methods](https://arxiv.org/abs/2601.20174)
*Alexander Benanti,Xi Han,Hong Qin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Numerical techniques for solving partial differential equations (PDEs) are integral for many fields across science and engineering. Such techniques usually involve solving large, sparse linear systems, where preconditioning methods are critical. In recent years, neural methods, particularly graph neural networks (GNNs), have demonstrated their potential through accelerated convergence. Nonetheless, to extract connective structures, existing techniques aggregate discretized system matrices into graphs, and suffer from rank inflation and a suboptimal convergence rate. In this paper, we articulate NeuraLSP, a novel neural preconditioner combined with a novel loss metric that leverages the left singular subspace of the system matrix's near-nullspace vectors. By compressing spectral information into a fixed low-rank operator, our method exhibits both theoretical guarantees and empirical robustness to rank inflation, affording up to a 53% speedup. Besides the theoretical guarantees for our newly-formulated loss function, our comprehensive experimental results across diverse families of PDEs also substantiate the aforementioned theoretical advances.

</details>


### [43] [Causal-Driven Feature Evaluation for Cross-Domain Image Classification](https://arxiv.org/abs/2601.20176)
*Chen Cheng,Ang Li*

Main category: cs.LG

TL;DR: 本文用因果视角评估表示的必要性与充分性，推出段落级框架，在多域测试中显著提升OOD性能。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中测试分布往往与训练分布差异巨大，传统域不变方法假设不变性即可靠性，而不一定具因果可预测性。

Method: 提出基于因果效应的段落级评估框架，直接测量表示在分布偏移下的必要性和充分性，而非仅追求域不变性。

Result: 在多域基准上实验显示，该方法在面对挑战性域偏移时，OOD性能显著提升。

Conclusion: 本文强调因果评估在OOD分类中的价值，并证明基于必要性与充分性的段落级框架能提升对域外分布的稳健性。

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in real-world classification, where test distributions often differ substantially from training data. Most existing approaches pursue domain-invariant representations, implicitly assuming that invariance implies reliability. However, features that are invariant across domains are not necessarily causally effective for prediction.
  In this work, we revisit OOD classification from a causal perspective and propose to evaluate learned representations based on their necessity and sufficiency under distribution shift. We introduce an explicit segment-level framework that directly measures causal effectiveness across domains, providing a more faithful criterion than invariance alone.
  Experiments on multi-domain benchmarks demonstrate consistent improvements in OOD performance, particularly under challenging domain shifts, highlighting the value of causal evaluation for robust generalization.

</details>


### [44] [On the Computational Complexity of Performative Prediction](https://arxiv.org/abs/2601.20180)
*Ioannis Anagnostides,Rohan Chauhan,Ioannis Panageas,Tuomas Sandholm,Jingming Yan*

Main category: cs.LG

TL;DR: 论文证明：当 ρ>1 时，求解可执行稳定点变为 PPAD‑完整（甚至在凸域下亦然），而策略分类中的局部最优也可视为 PLS‑硬，揭示了强制执行预测的显著计算难度。


<details>
  <summary>Details</summary>
Motivation: 了解在强制执行效应（ρ>1）下进行预测模型部署时，数据分布的自我更新会带来的计算瓶颈，尤其是模型稳定点的可计算性与复杂度。

Method: 通过构造极限近似的重分布模型，利用多项式时间归约技术，将求解稳定点问题映射到 PPAD‑硬问题（如爬山问题），进而得出 PPAD‑完整性；对凸域的推广采用变分不等式框架和凸分析方法。

Result: 1) 对强制执行 ρ>1 时的 ε‑稳定点求解问题给出 PPAD‑完整性证明；2) 在二次损失函数与线性分布位移的简单设定中仍保持难度；3) 对一般凸域进行推广同样是 PPAD‑完整；4) 对策略分类的局部最优求解为 PLS‑硬。

Conclusion: 该研究揭示了在 ρ>1 的情况下，求解 ε‑可执行稳定点是 PPAD‑完整的，即与一般和博弈中的纳什均衡等价，从而体现出显著的计算难度；在凸域下也保持此困难，且在策略分类中寻找局部最优同样是 PLS‑硬。

Abstract: Performative prediction captures the phenomenon where deploying a predictive model shifts the underlying data distribution. While simple retraining dynamics are known to converge linearly when the performative effects are weak ($ρ< 1$), the complexity in the regime $ρ> 1$ was hitherto open. In this paper, we establish a sharp phase transition: computing an $ε$-performatively stable point is PPAD-complete -- and thus polynomial-time equivalent to Nash equilibria in general-sum games -- even when $ρ= 1 + O(ε)$. This intractability persists even in the ostensibly simple setting with a quadratic loss function and linear distribution shifts. One of our key technical contributions is to extend this PPAD-hardness result to general convex domains, which is of broader interest in the complexity of variational inequalities. Finally, we address the special case of strategic classification, showing that computing a strategic local optimum is PLS-hard.

</details>


### [45] [Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery](https://arxiv.org/abs/2601.20193)
*Zhipeng Zhang,Wenting Ma,Kai Li,Meng Guo,Lei Yang,Wei Yu,Hongji Cui,Yichen Zhang,Mo Zhang,Jinzhe Lin,Zhenjie Yao*

Main category: cs.LG

TL;DR: 用meta-trust变量让算法自我判断学习可靠性，降低噪声影响并提升在受损奖励下的效果。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒RL在面对噪声/错误奖励时要么过度保守、要么在不确定积累时崩溃，缺乏对自身学习可靠性的评估。

Method: 引入基于价值预测误差稳定性的meta-trust变量，通过失败时安全机制和渐进式信任恢复来实现自我调节。

Result: 在连续控制基准上实验表明，实验组比较基线获得更高平均回报，且在训练后期失败率显著下降。

Conclusion: Meta认知强化学习框架通过内部可靠性信号调节学习，显著提升了在奖励污染环境下的表现并减少了训练后期失败。

Abstract: Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.
  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.
  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.

</details>


### [46] [DeRaDiff: Denoising Time Realignment of Diffusion Models](https://arxiv.org/abs/2601.20198)
*Ratnavibusena Don Shahain Manujith,Yang Zhang,Teoh Tze Tzun,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: DeRaDiff通过在推理时动态调节KL正则化强度（单参数λ）复刻多种训练强度模型，避免了昂贵的对齐超参数扫频，且在多指标上接近全尺度对齐效果。


<details>
  <summary>Details</summary>
Motivation: 在满足高奖励与保持预训练先验平衡的KL正则化中，如何挑选最佳强度既影响性能又往往需要代价高昂的多模型对齐搜索。

Method: 在已对齐的扩散模型基础上，使用几何混合术语在反向采样阶段调节标准正则化，使得单一可调参数λ即可实时控制正则化强度；在常用调度器下可得到闭式更新。

Result: 在多项文本图像对齐与图像质量指标上，DeRaDiff在一次先前对齐后即可逼近不同正则化强度完整对齐模型的表现，显著降低计算成本。

Conclusion: DeRaDiff通过在推理时调整正则化强度，几乎等同于在不同正则化强度下重新训练模型，从而有效替代昂贵的参数搜索。

Abstract: Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to "reward hacking". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce DeRaDiff, a denoising time realignment procedure that, after aligning a pretrained model once, modulates the regularization strength during sampling to emulate models trained at other regularization strengths without any additional training or finetuning. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed form update under common schedulers and a single tunable parameter, lambda, for on the fly control. Our experiments show that across multiple text image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.

</details>


### [47] [Hyperparameter Transfer with Mixture-of-Expert Layers](https://arxiv.org/abs/2601.20205)
*Tianze Jiang,Blake Bordelon,Cengiz Pehlevan,Boris Hanin*

Main category: cs.LG

TL;DR: 提出了一种基于 DMFT 的 MoE Transformer 参数化，解决了稀疏 MoE 的 HP 复杂性，实验证明可在模型规模扩展时实现稳定的超参数迁移。


<details>
  <summary>Details</summary>
Motivation: 稀疏 MoE 增加训练复杂度与 HP 调优成本，需一种简单可靠的参数化方案以高效扩展模型。

Method: 基于动态平均场理论（DMFT）对包含 MoE 层的 Transformer 进行参数化；在固定 token 预算下对模型宽度、深度、专家数与尺寸进行实验验证，并在小规模模型上搜索 HP，然后迁移至大模型。

Result: 实验表明，该参数化方案使 51M 到 2B 参数范围内的模型实现可靠 HP 转移；小模型的 HP 可用于训练更大模型并在更长 token 长度上保持性能。

Conclusion: 利用 DMFT 推导的参数化方法，实现了跨模型规模高效、可靠的 HP 迁移，模型宽度、深度、专家数与尺寸可同步扩展；小模型 HP 导向大模型训练可在更长序列上保持良好性能。

Abstract: Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.

</details>


### [48] [Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning](https://arxiv.org/abs/2601.20209)
*Jinyang Wu,Shuo Yang,Changpeng Yang,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: Spark通过在关键决策点自适应分支，突出高质量轨迹采样，有效缩短样本量并提升鲁棒性；


<details>
  <summary>Details</summary>
Motivation: 强化学习大模型长周期训练受限于高质量轨迹短缺和计算预算，现有方法仅放大rollout并均匀分配资源，导致大量无效计算，需解决资源高效且质量优先的探索策略；

Method: Spark框架通过在关键决策点动态分支，适时激活探测选择，智能分配训练资源，减少无效步骤的计算浪费；

Result: 在多样任务（如具身规划）上实验表明Spark能以显著更少的训练样本实现更高成功率，并在未见场景中保持良好泛化；

Conclusion: Spark为长周期强化学习提供一种低样本、高效资源分配的探索机制，具备更佳性能与泛化。

Abstract: Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.

</details>


### [49] [An Accounting Identity for Algorithmic Fairness](https://arxiv.org/abs/2601.20217)
*Hadi Elzayn,Jacob Goldin*

Main category: cs.LG

TL;DR: 本文用会计公式把准确率与公平性关联，显示二者互补；实验验证多公平方法往往只在公平错误间平移，若降低准确率会扩大总不公平预算。


<details>
  <summary>Details</summary>
Motivation: 探索预测模型中准确率与公平性之间的联系，特别是二元预测任务中Accuracy与公平性不可兼得的情形。

Method: 推导一种会计等式，将模型误差校准与群体间误差不平衡联结成“总不公平预算”。通过理论推导与基准数据实验验证。

Result: 证明：对于全局校准模型，误差校准与误差不平衡的加权和等于总不公平预算；准确率提升会压缩此预算；多数公平干预仅在公平违规间替代，若牺牲准确率则扩大预算。

Conclusion: 准确率与公平性在二元预测任务中是互补关系；提升精准度可减少不公平，二元不可能性在回归任务中可得到缓解。

Abstract: We derive an accounting identity for predictive models that links accuracy with common fairness criteria. The identity shows that for globally calibrated models, the weighted sums of miscalibration within groups and error imbalance across groups is equal to a "total unfairness budget." For binary outcomes, this budget is the model's mean-squared error times the difference in group prevalence across outcome classes. The identity nests standard impossibility results as special cases, while also describing inherent tradeoffs when one or more fairness measures are not perfectly satisfied. The results suggest that accuracy and fairness are best viewed as complements in binary prediction tasks: increasing accuracy necessarily shrinks the total unfairness budget and vice-versa. Experiments on benchmark data confirm the theory and show that many fairness interventions largely substitute between fairness violations, and when they reduce accuracy they tend to expand the total unfairness budget. The results extend naturally to prediction tasks with non-binary outcomes, illustrating how additional outcome information can relax fairness incompatibilities and identifying conditions under which the binary-style impossibility does and does not extend to regression tasks.

</details>


### [50] [ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance](https://arxiv.org/abs/2601.20227)
*Zichao Yu,Ming Li,Wenyi Zhang,Difan Zou,Weiguo Gao*

Main category: cs.LG

TL;DR: ProFlow是一种零熟悉度的近端引导采样框架，通过两步MAP迭代实现物理约束与观测一致性，同时保持预训练生成模型的统计结构，实验表明其在多种PDE基准上具备优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在推断稀疏观测的物理场时难以严格满足PDE约束，且重训练成本高，因此需要一种零样本方法兼顾物理一致性、观测精度与先验统计结构。

Method: ProFlow采用双步近端引导策略：先用极点优化将生成结果投影到物理与观测一致交集，随后插值回生成轨迹，形成局部MAP迭代。

Result: 在泊松、Helmholtz、Darcy和粘性Burgers方程基准上，ProFlow在物理一致性、观测一致性以及分布统计上均优于目前主流的扩散和流模型。

Conclusion: ProFlow能够在无任务特定再训练的前提下，利用预训练的生成先验，严格满足物理约束并兼顾稀疏观测，使物理一致性与观测一致性的推断达到领先水平。

Abstract: Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.

</details>


### [51] [Certificate-Guided Pruning for Stochastic Lipschitz Optimization](https://arxiv.org/abs/2601.20231)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出CGP，利用置信度校准的Lipschitz包络显式分离子最优点，控制活跃集体积并提供停止准则，实验中优于或匹配基线。


<details>
  <summary>Details</summary>
Motivation: 研究在噪声评估下Lipschitz函数的黑盒最优化问题，现有自适应离散化方法缺乏显式最优性证明和可测进展保证。

Method: 提出Certificate-Guided Pruning (CGP)，维护显式活跃集A_t，通过置信度校准的Lipschitz包络判断子最优点并给出高概率证据；在满足边缘条件和近似最优维度α的前提下证明A_t体积按受控速率收缩，样本复杂度达到\tildeO(ε^{-(2+α)})。并分别实现CGP-Adaptive（在线学习L）、CGP-TR（信任域缩放至d>50）、CGP-Hybrid（当检测到局部平滑时切换至GP细化）。

Result: 在12个基准（d∈[2,100]）上实验表明，CGP各变体与强基线匹配或优越，并通过证书体积提供原则性停止准则。

Conclusion: CGP通过显式最优性证书和活跃集收缩，实现了在噪声下Lipschitz黑盒优化的可测进度和可停止性，且在高维与自适应环境中保持竞争性能。

Abstract: We study black-box optimization of Lipschitz functions under noisy evaluations. Existing adaptive discretization methods implicitly avoid suboptimal regions but do not provide explicit certificates of optimality or measurable progress guarantees. We introduce \textbf{Certificate-Guided Pruning (CGP)}, which maintains an explicit \emph{active set} $A_t$ of potentially optimal points via confidence-adjusted Lipschitz envelopes. Any point outside $A_t$ is certifiably suboptimal with high probability, and under a margin condition with near-optimality dimension $α$, we prove $\Vol(A_t)$ shrinks at a controlled rate yielding sample complexity $\tildeO(\varepsilon^{-(2+α)})$. We develop three extensions: CGP-Adaptive learns $L$ online with $O(\log T)$ overhead; CGP-TR scales to $d > 50$ via trust regions with local certificates; and CGP-Hybrid switches to GP refinement when local smoothness is detected. Experiments on 12 benchmarks ($d \in [2, 100]$) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.

</details>


### [52] [HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH](https://arxiv.org/abs/2601.20255)
*Yueyang Wang,Jiawei Fu,Baolong Bi,Xili Wang,Xiaoqing Liu*

Main category: cs.LG

TL;DR: 本研究提出HE‑SNR指标，取代传统PPL，为LLM中训练期间提供更有效的评估工具，并在大规模MoE模型上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 缺乏可在中训练阶段使用的指标；PPL受“长上下文税”影响，难以与下游SWE表现相关。

Method: 首先实施严谨的数据过滤策略；基于熵压缩假设，将智能定义为将不确定性压缩成低阶熵压缩状态；基于此精细熵分析构造HE‑SNR指标。

Result: 在32K/128K不同上下文窗口的工业级Mixture‑of‑Experts模型上验证，HE‑SNR在稳健性和预测性上均优于传统指标。

Conclusion: 提出HE‑SNR度量，能够在中训练阶段高效指导LLM，为软件工程任务优化提供理论与工具，验证显示其在工业级Mixture‑of‑Experts模型中具备更稳健的预测能力。

Abstract: SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the "Long-Context Tax" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders ("reasonable hesitation"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.

</details>


### [53] [C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding](https://arxiv.org/abs/2601.20257)
*Jinren Ding,Xuejian Xu,Shen Jiang,Zhitong Hao,Jinhui Yang,Peng Jiang*

Main category: cs.LG

TL;DR: C2通过交叉注意力块和约束损失改进DT，在自动竞价任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: DT在生成式自动竞价中表现出潜力，但存在对state、action与RTG三序列缺乏交叉相关性建模，且在学习过程中无法区分最优与次优行为，从而限制了性能提升。

Method: 在DT基础上添加Cross Learning Block（CLB），利用跨注意力强化state、action、return-to-go（RTG）三序列间的交叉相关性；同时设计Constraint-aware Loss（CL），将预算和Cost-Per-Acquisition（CPA）约束纳入损失，使模型在训练时仅学习最优行为轨迹。

Result: 在AuctionNet离线评测中，C2相比最先进的GAVE提升最高达3.23%，并且消融实验验证CLB和CL的互补优势，进一步证明了C2在不同预算设置下的稳健性。

Conclusion: C2框架在自动竞价任务中显著提升Decision Transformer性能，结合交叉学习块与约束损失实现更强的跨序列建模与对最优轨迹的选择性学习，整体性能优于现有GAVE。

Abstract: Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.

</details>


### [54] [Robust SDE Parameter Estimation Under Missing Time Information Setting](https://arxiv.org/abs/2601.20268)
*Long Van Tran,Truyen Tran,Phuoc Nguyen*

Main category: cs.LG

TL;DR: 通过利用前向/后向过程的非对称性，可在缺失时间戳的情况下推断时间顺序，并用最大似然估计SDE参数，实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前在金融、健康和系统生物学等领域的随机微分方程（SDE）参数估计通常依赖于精确的时间戳数据；若时间顺序信息受损或缺失，传统方法往往失效。本研究旨在探讨在时间顺序受损情况下，SDE参数估计的可行性。

Method: 首次将前向与反向过程的非对称性用于构造分数匹配（score‑matching）准则，从而在观测对之间推断正确的时间先后顺序；随后通过排序完成全序恢复，并在重建得到的序列上采用最大似然法估计SDE参数。

Result: 在合成数据与真实案例上进行的大量实验表明，该方法能够准确恢复时间顺序并实现可靠的SDE参数估计；相比现有方法，在时间信息缺失或被隐藏的场景下表现出更稳健的性能。

Conclusion: 提出的框架实现了在时间顺序未知或受损时同时恢复时间信息与估计SDE参数的目标，为隐私敏感领域的动态建模提供了新的工具。

Abstract: Recent advances in stochastic differential equations (SDEs) have enabled robust modeling of real-world dynamical processes across diverse domains, such as finance, health, and systems biology. However, parameter estimation for SDEs typically relies on accurately timestamped observational sequences. When temporal ordering information is corrupted, missing, or deliberately hidden (e.g., for privacy), existing estimation methods often fail. In this paper, we investigate the conditions under which temporal order can be recovered and introduce a novel framework that simultaneously reconstructs temporal information and estimates SDE parameters. Our approach exploits asymmetries between forward and backward processes, deriving a score-matching criterion to infer the correct temporal order between pairs of observations. We then recover the total order via a sorting procedure and estimate SDE parameters from the reconstructed sequence using maximum likelihood. Finally, we conduct extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness of our method, extending parameter estimation to settings with missing temporal order and broadening applicability in sensitive domains.

</details>


### [55] [The Forecast After the Forecast: A Post-Processing Shift in Time Series](https://arxiv.org/abs/2601.20280)
*Daojun Liang,Qi Li,Yinglong Wang,Jing Chen,Hu Zhang,Xiaoxiao Cui,Qizheng Wang,Shuo Li*

Main category: cs.LG

TL;DR: δ-Adapter——轻量级后处理插件，通过输入软编辑、输出残差校正、特征掩码和校准组件，提升时间序列预测的准确性与置信区间。


<details>
  <summary>Details</summary>
Motivation: 随着时间序列预测模型的准确度趋于饱和，传统改进方式已难以进一步提升。利用后处理手段对已部署模型进行精准微调，可为实际运营提供更可靠的预测与不确定性评估。

Method: δ-Adapter是一个轻量化、与架构无关的插件，在输入侧进行软编辑（input nudging）和输出侧进行残差校正。它通过学习稀疏、时间步感知的特征掩码实现特征选择，并提供局部下降保证、O(δ)漂移界和组合稳定性。配合Quantile Calibrator和Conformal Corrector，能够生成具有有限样本覆盖率的量化校准区间。

Result: 在多种后端架构和数据集上实验表明，δ-Adapter在保持计算成本几乎为零、无接口改动的情况下，提升了预测精度和区间校准效果。

Conclusion: δ-Adapter可以在不重新训练或修改后端模型的前提下，显著提升已部署时间序列预测器的准确性和不确定性校准，同时具有可解释性和最小的计算开销。

Abstract: Time series forecasting has long been dominated by advances in model architecture, with recent progress driven by deep learning and hybrid statistical techniques. However, as forecasting models approach diminishing returns in accuracy, a critical yet underexplored opportunity emerges: the strategic use of post-processing. In this paper, we address the last-mile gap in time-series forecasting, which is to improve accuracy and uncertainty without retraining or modifying a deployed backbone. We propose $δ$-Adapter, a lightweight, architecture-agnostic way to boost deployed time series forecasters without retraining. $δ$-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. We provide local descent guarantees, $O(δ)$ drift bounds, and compositional stability for combined adapters. Meanwhile, it can act as a feature selector by learning a sparse, horizon-aware mask over inputs to select important features, thereby improving interpretability. In addition, it can also be used as a distribution calibrator to measure uncertainty. Thus, we introduce a Quantile Calibrator and a Conformal Corrector that together deliver calibrated, personalized intervals with finite-sample coverage. Our experiments across diverse backbones and datasets show that $δ$-Adapter improves accuracy and calibration with negligible compute and no interface changes.

</details>


### [56] [Memory Retrieval in Transformers: Insights from The Encoding Specificity Principle](https://arxiv.org/abs/2601.20282)
*Viet Hung Dinh,Ming Ding,Youyang Qu,Kanchana Thilakarathna*

Main category: cs.LG

TL;DR: 研究揭示Transformer注意力层能够以关键词为线索进行记忆检索，并验证特定神经元在此过程中起关键作用，可用于提升LLM的可解释性与遗忘功能。


<details>
  <summary>Details</summary>
Motivation: 在日益严格的监管环境下，需要可解释的LLM，尤其关注关注层在记忆检索中的作用与隐私溯源。

Method: 通过对LLM注意力层进行神经元选择性分析，基于编码特异性原理假设关键词均为检索线索并验证其在注意力加权中的作用。

Result: 发现关注层中存在特定神经元对关键词的编码与检索，实现了关键字提取并为机器遗忘提供了可行路径。

Conclusion: 论文表明Transformer attention层的神经元能特异性编码并检索上下文定义关键词，支持将这些关键词提取用于后续如机器遗忘等任务。

Abstract: While explainable artificial intelligence (XAI) for large language models (LLMs) remains an evolving field with many unresolved questions, increasing regulatory pressures have spurred interest in its role in ensuring transparency, accountability, and privacy-preserving machine unlearning. Despite recent advances in XAI have provided some insights, the specific role of attention layers in transformer based LLMs remains underexplored. This study investigates the memory mechanisms instantiated by attention layers, drawing on prior research in psychology and computational psycholinguistics that links Transformer attention to cue based retrieval in human memory. In this view, queries encode the retrieval context, keys index candidate memory traces, attention weights quantify cue trace similarity, and values carry the encoded content, jointly enabling the construction of a context representation that precedes and facilitates memory retrieval. Guided by the Encoding Specificity Principle, we hypothesize that the cues used in the initial stage of retrieval are instantiated as keywords. We provide converging evidence for this keywords-as-cues hypothesis. In addition, we isolate neurons within attention layers whose activations selectively encode and facilitate the retrieval of context-defining keywords. Consequently, these keywords can be extracted from identified neurons and further contribute to downstream applications such as unlearning.

</details>


### [57] [Cheap2Rich: A Multi-Fidelity Framework for Data Assimilation and System Identification of Multiscale Physics -- Rotating Detonation Engines](https://arxiv.org/abs/2601.20295)
*Yuxuan Bao,Jan Zajac,Megan Powers,Venkat Raman,J. Nathan Kutz*

Main category: cs.LG

TL;DR: Cheap2Rich 框架通过先验低阶模型与可解释差异校正结合，成功在稀疏传感下重建 RDE 高保真状态，并解释射流驱动的差异，提供可解释的多尺度数据同化方案。


<details>
  <summary>Details</summary>
Motivation: 解决在多尺度工程问题中，低阶模型与高阶物理系统之间存在的仿真-现实差距，尤其是传统低阶模型只捕捉主导动力学，无法有效支持精确监测与控制。

Method: 构建 Cheap2Rich 框架，先使用快速低阶先验模型生成粗略状态预测，然后通过学习可解释的差异校正项（discrepancy corrections）来提升状态重建质量，利用稀疏传感器历史数据进行多尺度数据同化。

Result: 在旋转点火发动机（RDE）实验中，该方法成功从稀疏测量中重建高保真状态，且能分离与射流驱动相关的物理意义差异动力学，与传统方法相比提升了重建精度并保持可解释性。

Conclusion: 提供了一种通用的多精度数据同化框架，增强了在复杂多尺度系统中的快速设计探索、实时监测与控制能力，并兼顾了差异动力学的可解释性。

Abstract: Bridging the sim2real gap between computationally inexpensive models and complex physical systems remains a central challenge in machine learning applications to engineering problems, particularly in multi-scale settings where reduced-order models typically capture only dominant dynamics. In this work, we present Cheap2Rich, a multi-scale data assimilation framework that reconstructs high-fidelity state spaces from sparse sensor histories by combining a fast low-fidelity prior with learned, interpretable discrepancy corrections. We demonstrate the performance on rotating detonation engines (RDEs), a challenging class of systems that couple detonation-front propagation with injector-driven unsteadiness, mixing, and stiff chemistry across disparate scales. Our approach successfully reconstructs high-fidelity RDE states from sparse measurements while isolating physically meaningful discrepancy dynamics associated with injector-driven effects. The results highlight a general multi-fidelity framework for data assimilation and system identification in complex multi-scale systems, enabling rapid design exploration and real-time monitoring and control while providing interpretable discrepancy dynamics. Code for this project is is available at: github.com/kro0l1k/Cheap2Rich.

</details>


### [58] [Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction](https://arxiv.org/abs/2601.20299)
*Tianyi Alex Qiu,Micah Carroll,Cameron Allen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.

</details>


### [59] [Delayed Feedback Modeling for Post-Click Gross Merchandise Volume Prediction: Benchmark, Insights and Approaches](https://arxiv.org/abs/2601.20307)
*Xinyu Li,Sishuo Chen,Guipeng Xv,Li Zhang,Mingxuan Luo,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Chen Lin*

Main category: cs.LG

TL;DR: 本研究从CVR推进到GMV预测，构建TRACE实验基准并提出READER模型，针对复购与延迟反馈挑战，取得2.19%性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究将广告排名模型的预测目标从转换率（CVR）转变为更具商业价值的连续指标——点击后商品总价值（GMV）。现有延迟反馈研究多聚焦于CVR，而GMV由于是连续目标且单次点击可导致多次购买，导致标签累积且延迟标注问题更为复杂。

Method: 建立TRACE基准，该基准包含完整的交易序列，支持在线流式训练并模拟GMV的延迟反馈。通过对TRACE的数据分析，提出READER（RepurchasE‑Aware Dual‑branch prEdictoR）模型：利用路由器根据复购预测激活专家参数，并动态校正回归目标以缓解因标签不完整导致的欠估。

Result: 在TRACE基准上，READER相较于基线模型实现了2.19%的准确率提升，并展示了在快速变化的GMV标签分布和复购样本标签差异上的优势。

Conclusion: 提示GMV预测的延迟反馈需要在线流式建模，并且需要针对复购与单购样本分别设计模型。READER的双分支结构与动态目标校正可有效提升GMV预测性能，为未来GMV延迟反馈研究提供了新的方法和数据资源。

Abstract: The prediction objectives of online advertisement ranking models are evolving from probabilistic metrics like conversion rate (CVR) to numerical business metrics like post-click gross merchandise volume (GMV). Unlike the well-studied delayed feedback problem in CVR prediction, delayed feedback modeling for GMV prediction remains unexplored and poses greater challenges, as GMV is a continuous target, and a single click can lead to multiple purchases that cumulatively form the label. To bridge the research gap, we establish TRACE, a GMV prediction benchmark containing complete transaction sequences rising from each user click, which supports delayed feedback modeling in an online streaming manner. Our analysis and exploratory experiments on TRACE reveal two key insights: (1) the rapid evolution of the GMV label distribution necessitates modeling delayed feedback under online streaming training; (2) the label distribution of repurchase samples substantially differs from that of single-purchase samples, highlighting the need for separate modeling. Motivated by these findings, we propose RepurchasE-Aware Dual-branch prEdictoR (READER), a novel GMV modeling paradigm that selectively activates expert parameters according to repurchase predictions produced by a router. Moreover, READER dynamically calibrates the regression target to mitigate under-estimation caused by incomplete labels. Experimental results show that READER yields superior performance on TRACE over baselines, achieving a 2.19% improvement in terms of accuracy. We believe that our study will open up a new avenue for studying online delayed feedback modeling for GMV prediction, and our TRACE benchmark with the gathered insights will facilitate future research and application in this promising direction. Our code and dataset are available at https://github.com/alimama-tech/OnlineGMV .

</details>


### [60] [Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching](https://arxiv.org/abs/2601.20332)
*Fengrui Zuo,Zhiwei Ke,Yiming Liu,Wenqi Lou,Chao Wang,Xvehai Zhou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \textbf{\placeholder}\footnote{The source code is available at https://github.com/vhicrgit/Window-Diffusion.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \textit{active tokens} that are computed online, (ii) \textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\times$ inference speedup while largely preserving generation performance.

</details>


### [61] [TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs](https://arxiv.org/abs/2601.20357)
*Minjae Lee,Wonjun Kang,Byeongkeun Ahn,Christian Classen,Kevin Galim,Seunghyuk Oh,Minghao Yan,Hyung Il Koo,Kangwook Lee*

Main category: cs.LG

TL;DR: TABED是一种无训练、可插拔的Speculative Decoding增强方案，通过动态批量投票在视觉语言模型中实现显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 目前Speculative Decoding在LLM上表现良好，但在处理图像与文本混合的Large Vision-Language Models上缺乏体系化研究；现有推理方法在不同输入场景下表现不稳定。

Method: 提出Test-time Adaptive Batched Ensemble Drafting（TABED），在Speculative Decoding设定中动态组合多批次草稿，利用历史真值的偏差进行投票，减少手动策略并保持训练成本为零。

Result: 在11个多样化数据集上测试发现，TABED相较于自回归解码最快速1.74倍，单草稿方法提升5%；动态投票逻辑在保持参数共享的同时，成本极低。

Conclusion: TABED在大型视觉语言模型中通过动态批量投票提速显著，平均壁时间提升1.74倍，速度超过单一草稿方法5%；该方案无训练负担，参数共享保持低成本，并兼容多种验证与绘制方法。

Abstract: Speculative decoding (SD) has proven effective for accelerating LLM inference by quickly generating draft tokens and verifying them in parallel. However, SD remains largely unexplored for Large Vision-Language Models (LVLMs), which extend LLMs to process both image and text prompts. To address this gap, we benchmark existing inference methods with small draft models on 11 datasets across diverse input scenarios and observe scenario-specific performance fluctuations. Motivated by these findings, we propose Test-time Adaptive Batched Ensemble Drafting (TABED), which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in the SD setting. The dynamic ensemble method achieves an average robust walltime speedup of 1.74x over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. With its plug-and-play compatibility, we further enhance TABED by integrating advanced verification and alternative drafting methods. Code and custom-trained models are available at https://github.com/furiosa-ai/TABED.

</details>


### [62] [Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku](https://arxiv.org/abs/2601.20363)
*Mariia Drozdova*

Main category: cs.LG

TL;DR: 研究表明连续时间扩散/流模型可为具有全局约束的离散结构如数独分配概率，并可通过随机搜索实现约束满足，尤其是DDPM式抽样效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探究标准连续时间生成模型能否代表具有极其稀疏且全局约束的离散分布，验证其在组合约束满足问题中的可行性。

Method: 在数独的连续松弛空间上训练流匹配（flow-matching）和基于分数的模型，沿高斯概率路径进行训练，并比较ODE、SDE以及DDPM式离散化抽样。

Result: 随机采样优于确定性流；基于分数的采样最可靠；DDPM式先祖抽样取得最高有效率。模型亦可通过热启限制实现概率数独求解器，尽管样本效率低于传统求解器。

Conclusion: 连续时间生成模型（尤其是基于分数的模型和DDPM式抽样）能够为具有极稀疏全局约束的离散集合（如完整数独格）分配非零概率，并通过随机搜索实现约束满足。

Abstract: Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.

</details>


### [63] [FedRD: Reducing Divergences for Generalized Federated Learning via Heterogeneity-aware Parameter Guidance](https://arxiv.org/abs/2601.20397)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Mingjin Zhang*

Main category: cs.LG

TL;DR: FedRD在多域联邦学习环境下实现更好泛化性能


<details>
  <summary>Details</summary>
Motivation: Federated learning中不同客户数据异质导致新客户加入需大量调整，亟需使模型能泛化给未见客户端

Method: FedRD：利用参数导向的全局泛化聚合和本地去偏分类，降低优化与性能偏差

Result: 在公开多域数据集上实验，FedRD相较基线在泛化任务上显著提升

Conclusion: FedRD通过兼顾全局与局部信息有效解决联邦领域泛化中的两大偏差问题

Abstract: Heterogeneous federated learning (HFL) aims to ensure effective and privacy-preserving collaboration among different entities. As newly joined clients require significant adjustments and additional training to align with the existing system, the problem of generalizing federated learning models to unseen clients under heterogeneous data has become progressively crucial. Consequently, we highlight two unsolved challenging issues in federated domain generalization: Optimization Divergence and Performance Divergence. To tackle the above challenges, we propose FedRD, a novel heterogeneity-aware federated learning algorithm that collaboratively utilizes parameter-guided global generalization aggregation and local debiased classification to reduce divergences, aiming to obtain an optimal global model for participating and unseen clients. Extensive experiments on public multi-domain datasets demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [64] [ScatterFusion: A Hierarchical Scattering Transform Framework for Enhanced Time Series Forecasting](https://arxiv.org/abs/2601.20401)
*Wei Li*

Main category: cs.LG

TL;DR: ScatterFusion blends scattering transforms with hierarchical attention to better capture multi‑scale patterns, yielding state‑of‑the‑art forecasting performance across several datasets.


<details>
  <summary>Details</summary>
Motivation: The difficulty of modeling intricate, multi‑time‑scale dependencies that hinder accurate forecasting across various horizons.

Method: A four-part framework integrating Hierarchical Scattering Transform Module, Scale‑Adaptive Feature Enhancement, Multi‑Resolution Temporal Attention, and a Trend‑Seasonal‑Residual decomposition‑guided loss.

Result: On seven benchmark datasets, ScatterFusion reduced key error metrics notably compared to other standard algorithms, across multiple prediction horizons.

Conclusion: ScatterFusion demonstrates robust improvement in time series forecasting by effectively capturing multi-scale temporal dependencies, outperforming prevailing benchmarks.

Abstract: Time series forecasting presents significant challenges due to the complex temporal dependencies at multiple time scales. This paper introduces ScatterFusion, a novel framework that synergistically integrates scattering transforms with hierarchical attention mechanisms for robust time series forecasting. Our approach comprises four key components: (1) a Hierarchical Scattering Transform Module (HSTM) that extracts multi-scale invariant features capturing both local and global patterns; (2) a Scale-Adaptive Feature Enhancement (SAFE) module that dynamically adjusts feature importance across different scales; (3) a Multi-Resolution Temporal Attention (MRTA) mechanism that learns dependencies at varying time horizons; and (4) a Trend-Seasonal-Residual (TSR) decomposition-guided structure-aware loss function. Extensive experiments on seven benchmark datasets demonstrate that ScatterFusion outperforms other common methods, achieving significant reductions in error metrics across various prediction horizons.

</details>


### [65] [AWGformer: Adaptive Wavelet-Guided Transformer for Multi-Resolution Time Series Forecasting](https://arxiv.org/abs/2601.20409)
*Wei Li*

Main category: cs.LG

TL;DR: AWGformer：自适应小波+跨尺度注意+多分辨率预测，提升多元时序预测性能，并有理论保证。


<details>
  <summary>Details</summary>
Motivation: 为在保持计算效率的同时有效捕获不同时间尺度的模式，解决传统方法在多尺度和非平稳时序中的局限。

Method: 1) AWDM动态选择最优小波基和分解层次；2) CSFF通过可学习耦合矩阵捕捉频带间交互；3) FAMA根据频率选择性对注意力头加权；4) HPN在多分辨率层生成预测并重构。

Result: 在众多基准数据集上，AWGformer平均提升了多项指标，相比现有最优方法表现更优，并提供了收敛性证明和小波引导注意机制与经典信号处理的理论联系。

Conclusion: AWGformer通过自适应小波分解和跨尺度注意机制实现了对多元时序数据的高效多尺度特征提取，显著提升了预测精度，尤其在多尺度和非平稳序列上表现突出。

Abstract: Time series forecasting requires capturing patterns across multiple temporal scales while maintaining computational efficiency. This paper introduces AWGformer, a novel architecture that integrates adaptive wavelet decomposition with cross-scale attention mechanisms for enhanced multi-variate time series prediction. Our approach comprises: (1) an Adaptive Wavelet Decomposition Module (AWDM) that dynamically selects optimal wavelet bases and decomposition levels based on signal characteristics; (2) a Cross-Scale Feature Fusion (CSFF) mechanism that captures interactions between different frequency bands through learnable coupling matrices; (3) a Frequency-Aware Multi-Head Attention (FAMA) module that weights attention heads according to their frequency selectivity; (4) a Hierarchical Prediction Network (HPN) that generates forecasts at multiple resolutions before reconstruction. Extensive experiments on benchmark datasets demonstrate that AWGformer achieves significant average improvements over state-of-the-art methods, with particular effectiveness on multi-scale and non-stationary time series. Theoretical analysis provides convergence guarantees and establishes the connection between our wavelet-guided attention and classical signal processing principles.

</details>


### [66] [Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs](https://arxiv.org/abs/2601.20420)
*Yuhang Liu,Erdun Gao,Dong Gong,Anton van den Hengel,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: 作者提出ConCA方法，基于LLM表达近似为对数后验线性混合的假设，用稀疏先验进行无监督解混，生成多款LLM可解释概念，优于传统SAE。


<details>
  <summary>Details</summary>
Motivation: 开发易于人类理解的LLM解释是部署在关键领域的必要条件，但现有的稀疏自编码器（SAE）在理论基础上存在模糊，导致设计和评估的挑战。

Method: 提出‘Concept Component Analysis (ConCA)’ 方案，基于假设LLM表达可近似为概念的对数后验线性混合，通过无监督线性解混技术提取概念；进一步引入稀疏先验得到稀疏ConCA变体。

Result: 实现12种稀疏ConCA模型，在多种LLM上提取出有意义的概念，并在理论上展示其优于SAE的优势。

Conclusion: 稀疏ConCA为LLM解释提供了更稳健、可解释的理论框架，解决了SAE的模糊性问题。

Abstract: Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.

</details>


### [67] [Nonlinear Dimensionality Reduction with Diffusion Maps in Practice](https://arxiv.org/abs/2601.20428)
*Sönke Beier,Paula Pirker-Díaz,Friedrich Pagenkopf,Karoline Wiesner*

Main category: cs.LG

TL;DR: 本文评述 Diffusion Map 的应用细节，揭示参数和组件选择对结果影响，提出新方法挑选最重要组件，并证明传统首位组件并非总是最佳。


<details>
  <summary>Details</summary>
Motivation: Diffusion Map 在多学科已广泛应用，但其预处理、参数和组件选择的重要性尚未得到系统讨论。

Method: 对 Diffusion Map 技术进行实践导向回顾，识别常见陷阱，并展示新方法用于找出最重要的组件。

Result: 通过回顾与新技术验证，发现首个组件不一定代表数据最相关的结构。

Conclusion: 本文指出，Diffusion Map 方法的结果受预处理、参数设置和组件选择的显著影响，首个组件并不一定是最相关的；提示需全面考量这些因素。

Abstract: Diffusion Map is a spectral dimensionality reduction technique which is able to uncover nonlinear submanifolds in high-dimensional data. And, it is increasingly applied across a wide range of scientific disciplines, such as biology, engineering, and social sciences. But data preprocessing, parameter settings and component selection have a significant influence on the resulting manifold, something which has not been comprehensively discussed in the literature so far. We provide a practice oriented review of the Diffusion Map technique, illustrate pitfalls and showcase a recently introduced technique for identifying the most relevant components. Our results show that the first components are not necessarily the most relevant ones.

</details>


### [68] [TimeCatcher: A Variational Framework for Volatility-Aware Forecasting of Non-Stationary Time Series](https://arxiv.org/abs/2601.20448)
*Zhiyu Chen,Minhao Liu,Yanru Zhang*

Main category: cs.LG

TL;DR: 提出 TimeCatcher：变分编码 + 波动增强，专注非平稳长周期预测，在多品类数据上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统轻量级MLP模型假设局部平稳，而在高波动性或突发性系列中易失效，迫切需要更适应非平稳特征的框架。

Method: 在传统线性结构基础上引入变分编码器捕获历史潜在动态并加入波动敏感增强模块，专门检测并放大突发局部变化。

Result: 在交通、金融、能源及气象等九个真实数据集的实验中，TimeCatcher在长期预测尤其在高波动情境下明显优于现有最优基线模型。

Conclusion: TimeCatcher在多领域数据上验证了其优异的长周期、高波动性预测性能，成为MLP基础时序预测的领先方法。

Abstract: Recent lightweight MLP-based models have achieved strong performance in time series forecasting by capturing stable trends and seasonal patterns. However, their effectiveness hinges on an implicit assumption of local stationarity assumption, making them prone to errors in long-term forecasting of highly non-stationary series, especially when abrupt fluctuations occur, a common challenge in domains like web traffic monitoring. To overcome this limitation, we propose TimeCatcher, a novel Volatility-Aware Variational Forecasting framework. TimeCatcher extends linear architectures with a variational encoder to capture latent dynamic patterns hidden in historical data and a volatility-aware enhancement mechanism to detect and amplify significant local variations. Experiments on nine real-world datasets from traffic, financial, energy, and weather domains show that TimeCatcher consistently outperforms state-of-the-art baselines, with particularly large improvements in long-term forecasting scenarios characterized by high volatility and sudden fluctuations. Our code is available at https://github.com/ColaPrinceCHEN/TimeCatcher.

</details>


### [69] [Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations](https://arxiv.org/abs/2601.20449)
*Fatima Ezzeddine,Obaida Ammar,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 提出一种强化学习的模型无关方法，生成兼顾个体与群体公平的对抗解释，实验展示其在保持解释质量的前提下，实现了双层公平。


<details>
  <summary>Details</summary>
Motivation: 随着对机器学习模型可解释性的需求增加，Counterfactual解释因能够展示输入特征变化如何影响决策而备受关注。然而，传统CF方法往往忽视公平性，导致不同属性或不同受保护群体获得不公平处理。因此，需要一种方法在生成CF时同时保证个体与群体公平。

Method: 将生成可解释性补救措施的任务建模为优化问题。采用强化学习框架，在奖励函数中加入个体公平（相似个体相似CF）与群体公平（不同群体相似或相等recourse）约束，并使用扩展的公平度量（等选择recourse、等recourse效果）来评估。

Result: 在三个基准数据集上实验验证，所述方法在满足个体公平与群体公平的同时，保持了CF的距离和可行性。进一步量化了不同公平层面所带来的成本差异，并展示了混合公平（hybrid fairness）在XAI中的重要作用。

Conclusion: 本文提出了一种基于强化学习的模型无关方法，用以生成兼顾个体和群体公平的可解释性机器学习模型的对抗性解释（CF）。通过在优化任务中加入公平约束，实现了相似个体可得到相似的CF，并在不同受保护群体间实现了公平的recourse。实验表明，方法既保持了CF的近似性与可行性，又成功实现了两级公平。

Abstract: Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model's decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.

</details>


### [70] [An explainable framework for the relationship between dementia and glucose metabolism patterns](https://arxiv.org/abs/2601.20480)
*C. Vázquez-García,F. J. Martínez-Murcia,F. Segovia Román,A. Forte,J. Ramírez,I. Illán,A. Hernández-Segura,C. Jiménez-Mesa,Juan M. Górriz*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.

</details>


### [71] [CCMamba: Selective State-Space Models for Higher-Order Graph Learning on Combinatorial Complexes](https://arxiv.org/abs/2601.20518)
*Jiawen Chen,Qi Shao,Mingtong Zhou,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Topological deep learning has emerged for modeling higher-order relational structures beyond pairwise interactions that standard graph neural networks fail to capture. Although combinatorial complexes offer a unified topological framework, most existing topological deep learning methods rely on local message passing via attention mechanisms, which incur quadratic complexity and remain low-dimensional, limiting scalability and rank-aware information aggregation in higher-order complexes.We propose Combinatorial Complex Mamba (CCMamba), the first unified mamba-based neural framework for learning on combinatorial complexes. CCMamba reformulates message passing as a selective state-space modeling problem by organizing multi-rank incidence relations into structured sequences processed by rank-aware state-space models. This enables adaptive, directional, and long range information propagation in linear time without self attention. We further establish the theoretical analysis that the expressive power upper-bound of CCMamba message passing is the 1-Weisfeiler-Lehman test. Experiments on graph, hypergraph, and simplicial benchmarks demonstrate that CCMamba consistently outperforms existing methods while exhibiting improved scalability and robustness to depth.

</details>


### [72] [Unsupervised Ensemble Learning Through Deep Energy-based Models](https://arxiv.org/abs/2601.20556)
*Ariel Maymon,Yanir Buznah,Uri Shaham*

Main category: cs.LG

TL;DR: 提出一种仅基于预测结果的深度能量模型无监督集成方法，无需标签，理论上可在学习器条件独立时保证收敛，实验证明其在多种集成任务上均能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在许多实际场景中，获取标记数据困难、信息有限或受隐私限制，传统的有监督集成学习方法难以评估或组合学习器。无监督集成学习可在缺乏标签的情况下融合多模型的知识，提升整体性能，解决数据稀缺及隐私敏感问题。

Method: 利用深度能量模型（deep energy-based model）对各基学习器的预测结果建模，通过最小化能量函数学习一个元学习器，该元学习器能够捕捉学习器间的复杂依赖关系。该方法仅使用预测输出，无需标签或额外特征，且在学习器条件独立时具备理论收敛保证。

Result: 在标准集成数据集及针对多源专业知识融合的人工设计数据集上，本文方法均显著优于现有无监督或有监督集成方法，尤其在专家混合场景中取得了更低的误差率和更稳健的性能。

Conclusion: 本文提出了一种基于深度能量模型的无监督集成学习方法，能够在没有标签数据、学习器特征或问题特定信息的情况下构建高精度的元学习器，并在条件独立假设下提供理论保障。实验表明，该方法在多种集成场景（包括专家混合）上均表现出优于现有方法的性能，证明了其在数据稀缺或隐私敏感环境下利用集体智能的潜力。

Abstract: Unsupervised ensemble learning emerged to address the challenge of combining multiple learners' predictions without access to ground truth labels or additional data. This paradigm is crucial in scenarios where evaluating individual classifier performance or understanding their strengths is challenging due to limited information. We propose a novel deep energy-based method for constructing an accurate meta-learner using only the predictions of individual learners, potentially capable of capturing complex dependence structures between them. Our approach requires no labeled data, learner features, or problem-specific information, and has theoretical guarantees for when learners are conditionally independent. We demonstrate superior performance across diverse ensemble scenarios, including challenging mixture of experts settings. Our experiments span standard ensemble datasets and curated datasets designed to test how the model fuses expertise from multiple sources. These results highlight the potential of unsupervised ensemble learning to harness collective intelligence, especially in data-scarce or privacy-sensitive environments.

</details>


### [73] [Reinforcement Unlearning via Group Relative Policy Optimization](https://arxiv.org/abs/2601.20568)
*Efstratios Zaradoukas,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: PURGE 通过奖励惩罚禁止词汇的策略，实现高效、高鲁棒的 LLM 无痕记忆，显著降低 token 消耗，提升流畅度与鲁棒性，并在 RWKU 任务上取得 11% 有效无痕记忆与 98% 保留原工具率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在预训练阶段往往无意间记忆并保留敏感或受版权保护的数据，导致在欧盟 GDPR 和 AI Act 等法律框架下出现合规性挑战。为满足监管要求，需要一种能够在已部署模型上快速删除不合法信息而无须从头重新训练的方法。

Method: 本文提出 PURGE（Policy Unlearning through Relative Group Erasure）方法，基于 Group Relative Policy Optimization 框架将无痕记忆视为可验证的任务。该方法通过内在奖励信号惩罚模型对禁止概念的任何提及，从而实现安全且一致的忘记过程。

Result: 在 Real World Knowledge Unlearning (RWKU) 基准上，PURGE 在保持98% 原始模型效用的同时，实现11% 的无痕记忆效果；比最先进方法减少最多46倍的 token 使用量，并分别提升 5.48% 语义流畅度和 12.02% 对抗鲁棒性。

Conclusion: 将 LLM 的忘记任务构造为可验证的形式，使得无痕记忆更加可靠、高效且可扩展，为结合理论保证、安全性和实际部署效率的无痕记忆研究提供了有前景的新方向。

Abstract: During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach reduces token usage per target by up to a factor of 46 compared with SotA methods, while improving fluency by 5.48 percent and adversarial robustness by 12.02 percent over the base model. On the Real World Knowledge Unlearning (RWKU) benchmark, PURGE achieves 11 percent unlearning effectiveness while preserving 98 percent of original utility. PURGE shows that framing LLM unlearning as a verifiable task, enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.

</details>


### [74] [Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM](https://arxiv.org/abs/2601.20571)
*Anna van Elst,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.

</details>


### [75] [Ranking-aware Reinforcement Learning for Ordinal Ranking](https://arxiv.org/abs/2601.20585)
*Aiming Hao,Chen Zhu,Jiashu Zhu,Jiahong Wu,Xiangxiang Chu*

Main category: cs.LG

TL;DR: RARL用强化学习统一回归与排序，借助排名感知奖励和响应突变操作提升训练效果，实验表明效果显著。


<details>
  <summary>Details</summary>
Motivation: Ordinal regression和排序任务受限于固有的序数依赖，传统方法难以建模。

Method: 提出Ranking-Aware Reinforcement Learning（RARL）框架，统一回归与学习排序（L2R）目标，引入排名感知可验证奖励同时结合响应突变操作（RMO）以增强探索并避免停滞。

Result: 通过在三个不同基准上的大规模实验验证了RARL的有效性。

Conclusion: RARL在回归与排序任务上实现了互利提升，显著提高了性能。

Abstract: Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.

</details>


### [76] [Regularized Gradient Temporal-Difference Learning](https://arxiv.org/abs/2601.20599)
*Hyunjun Na,Donghwan Lee*

Main category: cs.LG

TL;DR: 作者提出的正则化GTD（R-GTD）解决了传统算法在特征矩阵奇异时的不收敛问题，具备理论保证且在实验中表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统GTD算法的收敛分析假设FIM非奇异，而现实中FIM往往会出现奇异，导致算法不稳定或性能下降，需要一种能在奇异情况下仍能保证收敛的方法。

Method: 通过对均方投影Bellman误差（MSPBE）最小化问题进行正则化改写，引入正则化项，从而得到R-GTD算法；该算法在更新规则中加入了正则化项，消除了对FIM非奇异性的依赖。

Result: 在理论上推导出R-GTD的收敛性证明并给出显式误差上界；实验验证表明R-GTD在FIM奇异场景下优于原始GT-D算法，表现出更稳健的下降曲线和更低的最终误差。

Conclusion: 提出的R-GTD算法在特征交互矩阵（FIM）奇异的情况下，仍能保证收敛到唯一解，显著提升了传统GTD算法在实际场景中的稳定性与性能。

Abstract: Gradient temporal-difference (GTD) learning algorithms are widely used for off-policy policy evaluation with function approximation. However, existing convergence analyses rely on the restrictive assumption that the so-called feature interaction matrix (FIM) is nonsingular. In practice, the FIM can become singular and leads to instability or degraded performance. In this paper, we propose a regularized optimization objective by reformulating the mean-square projected Bellman error (MSPBE) minimization. This formulation naturally yields a regularized GTD algorithms, referred to as R-GTD, which guarantees convergence to a unique solution even when the FIM is singular. We establish theoretical convergence guarantees and explicit error bounds for the proposed method, and validate its effectiveness through empirical experiments.

</details>


### [77] [WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport](https://arxiv.org/abs/2601.20606)
*Xinyu Wang,Ruoyu Wang,Qiangwei Peng,Peijie Zhou,Tiejun Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.

</details>


### [78] [DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration](https://arxiv.org/abs/2601.20627)
*Gilles Eerlings,Brent Zoomers,Jori Liesenborgs,Gustavo Rovelo Ruiz,Kris Luyten*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model's accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.

</details>


### [79] [A Foundation Model for Virtual Sensors](https://arxiv.org/abs/2601.20634)
*Leon Götz,Lars Frederik Peiss,Erik Sauer,Andreas Udo Sass,Thorsten Bagdonat,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 开发统一基础模型能一次性预测多个虚拟传感器，自动选择输入，计算快、内存少，实验显示大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟传感器需人工挑选输入、无法共享任务知识，且基础时序模型计算昂贵且仅能预测自身输入，亟需兼顾精度与效率的统一方法。

Method: 构建一个共享架构的基础模型，自动学习并选择每个虚拟传感器所需的输入信号，实现多传感器协同预测与可解释性。

Result: 在18亿样本的标准与应用数据集上，模型将计算时间降低415倍，内存降低951倍，并保持或提升预测质量；参数量几乎常数，支持数百传感器部署。

Conclusion: 本文提出一种统一的基础模型，实现了虚拟传感器的高效协同预测，并显著降低运算和内存成本，保持甚至提升预测质量。

Abstract: Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.

</details>


### [80] [An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems](https://arxiv.org/abs/2601.20637)
*Panayiotis Ioannou,Pietro Liò,Pietro Cicuta*

Main category: cs.LG

TL;DR: 通过先用节点ODE扩充有限噪声数据，再用符号回归提炼微分方程，可有效恢复复杂系统的动力学方程，显示出该组合方法在科学发现中的潜在价值。


<details>
  <summary>Details</summary>
Motivation: 精准建模复杂系统动力学并发现其控制微分方程是加速科学发现的核心任务；在数据稀缺或噪声严重的情况下，传统方法难以有效归纳物理规律。

Method: 作者使用两套由噪声污染的合成阻尼振荡数据。首先在仅10%训练集上训练NODE，并检验其外推能力；其次对原始及NODE生成的数据进行符号回归，评估SR在不同输入变量选择下恢复微分方程的效果。

Result: ① 在轨迹保持动态相似的前提下，NODE 能有效外推到新的边界条件；② SR 在噪声数据上能准确恢复方程，但依赖于输入变量的正确选取；③ 利用仅10%训练数据训练 NODE 并生成数据后，SR 能重构两条完整方程并给出第三条的良好近似。

Conclusion: 该研究表明，神经常微分方程（NODE）能够在边界条件变化时保持良好的外推性能，而符号回归（SR）在正确选择输入变量后能从噪声数据中恢复系统的微分方程；利用NODE生成的少量数据，SR能够重构两条主方程并近似第三条，验证了“先用NODE扩充数据，再用SR归纳物理定律”的思路具有前景。

Abstract: Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.

</details>


### [81] [Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability](https://arxiv.org/abs/2601.20642)
*Rohan Asthana,Vasileios Belagiannis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.

</details>


### [82] [MuRAL-CPD: Active Learning for Multiresolution Change Point Detection](https://arxiv.org/abs/2601.20686)
*Stefano Bertolasi,Diego Carrera,Diego Stucchi,Pasqualina Fragneto,Luigi Amedeo Bianchi*

Main category: cs.LG

TL;DR: MuRAL-CPD 在多尺度小波基础上，引入主动学习，利用用户反馈自适应优化超参数，显著提升了变点检测的准确性与可解释性，特别适用于只具备有限监督的场景。


<details>
  <summary>Details</summary>
Motivation: 传统的无监督变点检测方法无法适配特定任务的变点定义，也无法利用用户知识，导致检测准确性和可解释性有限。

Method: MuRAL-CPD 结合多分辨率小波分解与主动学习，先在多个时间尺度上检测潜在变点，再通过用户反馈迭代地优化关键超参数，实现模型对用户变点认知的对齐。

Result: 在多组真实数据集上实验，MuRAL-CPD 在有限监督环境下显著优于现有最先进方法，提升了检测准确率和可解释性。

Conclusion: 采用半监督和多分辨率策略的 MuRAL-CPD 能有效调整模型对变点的感知，弥补传统无监督方法的不足，为需要少量监督的时序分析场景提供了更优的解决方案。

Abstract: Change Point Detection (CPD) is a critical task in time series analysis, aiming to identify moments when the underlying data-generating process shifts. Traditional CPD methods often rely on unsupervised techniques, which lack adaptability to task-specific definitions of change and cannot benefit from user knowledge. To address these limitations, we propose MuRAL-CPD, a novel semi-supervised method that integrates active learning into a multiresolution CPD algorithm. MuRAL-CPD leverages a wavelet-based multiresolution decomposition to detect changes across multiple temporal scales and incorporates user feedback to iteratively optimize key hyperparameters. This interaction enables the model to align its notion of change with that of the user, improving both accuracy and interpretability. Our experimental results on several real-world datasets show the effectiveness of MuRAL-CPD against state-of-the-art methods, particularly in scenarios where minimal supervision is available.

</details>


### [83] [Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models](https://arxiv.org/abs/2601.20687)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Xiaobo Xia,Ming-Kun Xie,Dong-Dong Wu,Biao Liu,Yuheng Jia,Xin Geng,Masashi Sugiyama,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 通过教师单次查询和 anchor‑conditioned 自己排序，PU‑RL 蒸馏在本地生成高质量偏好信号，实现低成本且稳定的 RL 对齐，适用于小模型本地部署。


<details>
  <summary>Details</summary>
Motivation: 小模型在本地部署普遍，但由于隐私、成本和延迟等限制，RL 对齐阶段往往被放弃，现有方法多需昂贵的人类偏好标注或昂贵的外部奖励模型，而这两者在本地环境中并不实用。

Method: 提出正负样本（PU）RL 蒸馏方法：对每个提示，只查询一次教师生成 anchor 结果；在本地采样多条学生候选答案，通过 anchor‑conditioned 自我排序生成成对或列表偏好；然后在本地完成直接偏好优化或组相对策略优化的完整训练循环。

Result: 在低成本设置下进行实验，证明该方法始终保持较强的性能，并在理论上证明偏好信号具有秩一致性与集中性，支持稳定的偏好优化。

Conclusion: 该方法在不需要人类标签或外部奖励模型的前提下，成功实现了 On‑Premise 小模型的 RL 对齐，弥补了传统部署与对齐之间的差距。

Abstract: Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.

</details>


### [84] [Optimal Transport Group Counterfactual Explanations](https://arxiv.org/abs/2601.20692)
*Enrique Valero-Leal,Bernd Bischl,Pedro Larrañaga,Concha Bielza,Giuseppe Casalicchio*

Main category: cs.LG

TL;DR: 本文通过学习最优传输映射来生成集群反事实解释，避免了传统方法的局限：不再为每个样本单独优化、减少对模型线性假设的依赖、并显著提升几何一致性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有集群反事实解释方法固定组优化、依赖强模型假设或无法很好控制几何变形。

Method: 学习一个显式的最优传输映射，该映射将组内样本映射至其反事实，目标是最小化整个组的运输成本。对于线性分类器，作者通过数学优化证明该映射可表述为凸优化问题（如QP、QCQP）。当模型无法利用线性假设时，方法仍能显著优于现有基线。

Result: 实验结果表明，所提出的方法在常见数据集上能准确泛化，保持组几何结构，并且相较于基线方法仅产生轻微额外运输成本；在非线性模型下其表现仍明显优于基线。

Conclusion: 本文提出的基于最优传输的集群反事实解释方法能够在不重新优化的情况下，将任意组内样本映射至其反事实，具有较少的参数，能够很好地保留原组的几何结构，并且在基线方法之上只产生可忽略的额外运输成本。

Abstract: Group counterfactual explanations find a set of counterfactual instances to explain a group of input instances contrastively. However, existing methods either (i) optimize counterfactuals only for a fixed group and do not generalize to new group members, (ii) strictly rely on strong model assumptions (e.g., linearity) for tractability or/and (iii) poorly control the counterfactual group geometry distortion. We instead learn an explicit optimal transport map that sends any group instance to its counterfactual without re-optimization, minimizing the group's total transport cost. This enables generalization with fewer parameters, making it easier to interpret the common actionable recourse. For linear classifiers, we prove that functions representing group counterfactuals are derived via mathematical optimization, identifying the underlying convex optimization type (QP, QCQP, ...). Experiments show that they accurately generalize, preserve group geometry and incur only negligible additional transport cost compared to baseline methods. If model linearity cannot be exploited, our approach also significantly outperforms the baselines.

</details>


### [85] [Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?](https://arxiv.org/abs/2601.20694)
*Hao Liang,Jiayu Cheng,Sean R. Sinclair,Yali Du*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Exogenous MDPs (Exo-MDPs) capture sequential decision-making where uncertainty comes solely from exogenous inputs that evolve independently of the learner's actions. This structure is especially common in operations research applications such as inventory control, energy storage, and resource allocation, where exogenous randomness (e.g., demand, arrivals, or prices) drives system behavior. Despite decades of empirical evidence that greedy, exploitation-only methods work remarkably well in these settings, theory has lagged behind: all existing regret guarantees for Exo-MDPs rely on explicit exploration or tabular assumptions. We show that exploration is unnecessary. We propose Pure Exploitation Learning (PEL) and prove the first general finite-sample regret bounds for exploitation-only algorithms in Exo-MDPs. In the tabular case, PEL achieves $\widetilde{O}(H^2|Ξ|\sqrt{K})$. For large, continuous endogenous state spaces, we introduce LSVI-PE, a simple linear-approximation method whose regret is polynomial in the feature dimension, exogenous state space, and horizon, independent of the endogenous state and action spaces. Our analysis introduces two new tools: counterfactual trajectories and Bellman-closed feature transport, which together allow greedy policies to have accurate value estimates without optimism. Experiments on synthetic and resource-management tasks show that PEL consistently outperforming baselines. Overall, our results overturn the conventional wisdom that exploration is required, demonstrating that in Exo-MDPs, pure exploitation is enough.

</details>


### [86] [Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs](https://arxiv.org/abs/2601.20704)
*Melika Mobini,Vincent Holst,Floriano Tori,Andres Algaba,Vincent Ginis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models are increasingly used to curate bibliographies, raising the question: are their reference lists distinguishable from human ones? We build paired citation graphs, ground truth and GPT-4o-generated (from parametric knowledge), for 10,000 focal papers ($\approx$ 275k references) from SciSciNet, and added a field-matched random baseline that preserves out-degree and field distributions while breaking latent structure. We compare (i) structure-only node features (degree/closeness/eigenvector centrality, clustering, edge count) with (ii) 3072-D title/abstract embeddings, using an RF on graph-level aggregates and Graph Neural Networks with node features. Structure alone barely separates GPT from ground truth (RF accuracy $\approx$ 0.60) despite cleanly rejecting the random baseline ($\approx$ 0.89--0.92). By contrast, embeddings sharply increase separability: RF on aggregated embeddings reaches $\approx$ 0.83, and GNNs with embedding node features achieve 93\% test accuracy on GPT vs.\ ground truth. We show the robustness of our findings by replicating the pipeline with Claude Sonnet 4.5 and with multiple embedding models (OpenAI and SPECTER), with RF separability for ground truth vs.\ Claude $\approx 0.77$ and clean rejection of the random baseline. Thus, LLM bibliographies, generated purely from parametric knowledge, closely mimic human citation topology, but leave detectable semantic fingerprints; detection and debiasing should target content signals rather than global graph structure.

</details>


### [87] [Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions](https://arxiv.org/abs/2601.20714)
*Raul de la Rosa,Ivana Dusparic,Nicolas Cardozo*

Main category: cs.LG

TL;DR: MORPHIN通过漂移检测与动态超参数调整，在保持先前策略的前提下，实现对奖励函数和动作空间变化的即时自适应，使RL在非平稳环境中收敛更快、效率提升约1.7倍。


<details>
  <summary>Details</summary>
Motivation: RL在奖励函数改变或动作空间扩展的非平稳环境中难以保持性能，需一种无需完整重训练的自适应框架。

Method: 采用概念漂移检测与动态调整学习率、探索率，实现对奖励函数变化和动作空间扩展的即时自适应，同时保留已有策略。

Result: 在Gridworld和交通信号控制模拟中，MORPHIN比标准Q-learning快1.7倍收敛，持续适应。

Conclusion: MORPHIN实现了快速收敛和持续自适应，在非平稳环境中优于传统Q-learning。

Abstract: Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.

</details>


### [88] [Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis](https://arxiv.org/abs/2601.20729)
*Anchen Sun,Zhibin Chen,Xiaodong Cai*

Main category: cs.LG

TL;DR: 采用Mean Teacher半监督框架训练Cox‑MT，在癌症TCGA数据上充分利用未标记样本，显著提升预测效果


<details>
  <summary>Details</summary>
Motivation: 解决ANN‑Cox模型在高维特征下受限于标记样本稀缺的问题

Method: 基于Mean Teacher框架的深度半监督学习，利用标记与未标记数据训练单模态与多模态Cox‑MT模型

Result: 在TCGA四种癌症数据上，单模态Cox‑MT显著优于Cox‑nnet；未标记样本增多进一步提升性能；多模态Cox‑MT性能更佳

Conclusion: Cox‑MT有效利用未标记数据显著提升生存预测准确度，优于仅用标记数据训练的ANN‑Cox模型

Abstract: The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.

</details>


### [89] [HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs](https://arxiv.org/abs/2601.20745)
*Guoan Wang,Feiyu Wang,Zongwei Lv,Yikun Zong,Tong Yang*

Main category: cs.LG

TL;DR: Hestia通过温度化软化量化步骤和Hessian曲率信号实现细粒度温度退火，显著提升极低比特LLM的量化效果。


<details>
  <summary>Details</summary>
Motivation: 现有QAT方法过早硬化梯度导致优化困难，尤其在极低比特量化中需要更平滑的梯度流以获得更好表现。

Method: 使用温度控制的softmax取代硬四舍五入，并通过张量级Hessian跟踪实现细粒度温度退火。

Result: 在Llama-3.2实验中，Hestia在1B和3B模型上分别提高5.39%和4.34%的零样本性能。

Conclusion: Hestia通过Hessian引导的可微分量化训练框架，在极低位LLMs的量化过程中，采用温度控制的softmax放松实现梯度流通，并使用张量级希尔伯特跟踪作为曲率信号驱动温度退火，显著提升量化模型性能；在Llama-3.2上对1B和3B模型分别实现5.39%和4.34%的零样本提升。

Abstract: As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.

</details>


### [90] [Supervised Guidance Training for Infinite-Dimensional Diffusion Models](https://arxiv.org/abs/2601.20756)
*Elizabeth L. Baker,Alexander Denker,Jes Frellsen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.

</details>


### [91] [Less is More: Clustered Cross-Covariance Control for Offline RL](https://arxiv.org/abs/2601.20765)
*Nan Qiao,Sheng Yue,Shuning Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: C^4方案通过缓冲区划分和梯度纠正抑制TD交叉协方差，提高离线RL在小样本、离域数据环境中的稳定性与收益，提升幅度可达30%。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中由于分布偏移导致的样本分布不平衡和过多离域数据带来的优化偏差和性能下降

Method: ① 采用划分缓冲采样（Partitioned Buffer Sampling）限制更新至局部回放区块，抑制TD交叉协方差对更新方向的干扰；② 引入基于梯度的纠正惩罚（Gradient-based Corrective Penalty）在每次更新中抵消协方差诱导的偏差。两条策略构成Clustered Cross-Covariance Control for TD (C^4)方案；同时证明缓冲划分保持最大化目标下界，并缓解极端离域区的过度保守。

Result: 实验表明该方法在小规模数据集及强调离域区域的划分上，比现有方法在稳定性和收益上提升最多30%。

Conclusion: 通过缓冲区划分与梯度纠正双重机制，显著减小TD交叉协方差带来的负面影响，提升离线RL在分布偏移环境下的学习效果与鲁棒性。

Abstract: A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.

</details>


### [92] [COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI](https://arxiv.org/abs/2601.20772)
*Shakhyar Gogoi*

Main category: cs.LG

TL;DR: COMET‑SG1 是一种轻量化自回归模型，利用线性编码与记忆转移保持长期预测稳健，实验显示其长期漂移低于传统模型，适合边缘 AI 应用。


<details>
  <summary>Details</summary>
Motivation: 满足边缘部署对长期稳定性和有限参数空间的需求，避免预测误差随时间累积。

Method: 通过线性行为空间编码、基于记忆的转移估计以及确定性状态更新构建轻量级自回归回归模型。

Result: 在非平稳合成时间序列数据上，COMET‑SG1 在短期准确度与 MLP、LSTM、k‑NN 相当，且在长期预测中漂移显著减少，参数占用小且可兼容定点算术。

Conclusion: COMET‑SG1 在边缘与嵌入式 AI 系统中实现了稳健的自回归时间序列预测，且长期漂移显著低于传统基线模型。

Abstract: COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon behavior under fully autoregressive inference, a critical requirement for edge deployment where prediction errors accumulate over time. Experiments on non-stationary synthetic time-series data demonstrate that COMET-SG1 achieves competitive short-horizon accuracy while exhibiting significantly reduced long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines. With a compact parameter footprint and operations compatible with fixed-point arithmetic, COMET-SG1 provides a practical and interpretable approach for stable autoregressive prediction in edge and embedded AI applications.

</details>


### [93] [Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying](https://arxiv.org/abs/2601.20773)
*Rubén Jiménez,Oriol Pujol*

Main category: cs.LG

TL;DR: 本文提出将硬标签拷贝转化为符号距离回归，利用α‑治理平滑和正则化，提供两种仅标签的距离估计算法，实验表明在保真和泛化上优于传统方法，并能输出不确定性信号。


<details>
  <summary>Details</summary>
Motivation: 在无需访问原始训练数据或模型内部的限制下，传统的黑盒拷贝只能依靠硬标签输出，导致对决策边界几何结构的重建效率低下。

Method: 提出基于距离的拷贝框架，将硬标签监督替换为教师决策边界的符号距离，形成光滑的回归问题。通过α‑治理的平滑与正则化方案，控制Hölder/Lipschitz性质，并提供两种无模型的算法在仅标签访问条件下估计符号距离。

Result: 在合成数据和UCI基准测试上，相较于硬标签基线，一致提升了模型的保真度和泛化准确性，同时输出的距离可作为不确定性信号。

Conclusion: 通过将拷贝任务转化为利用符号距离的平滑回归，可显著提升黑盒复制质量并提供有效的不确定性估计。

Abstract: Deployed machine learning systems must continuously evolve as data, architectures, and regulations change, often without access to original training data or model internals. In such settings, black-box copying provides a practical refactoring mechanism, i.e. upgrading legacy models by learning replicas from input-output queries alone. When restricted to hard-label outputs, copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently. We propose a distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry. We develop an $α$-governed smoothing and regularization scheme with Hölder/Lipschitz control over the induced target surface, and introduce two model-agnostic algorithms to estimate signed distances under label-only access. Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty-related signals for black-box replicas.

</details>


### [94] [When More Data Doesn't Help: Limits of Adaptation in Multitask Learning](https://arxiv.org/abs/2601.20774)
*Steve Hanneke,Mingyue Xu*

Main category: cs.LG

TL;DR: 多任务学习的难度不因每个任务数据量足而可逆转，提出更严格的不可适应性定理并讨论未来可能的最优适配思路。


<details>
  <summary>Details</summary>
Motivation: 审视多任务学习在大样本情境下的统计限制，弥补既有限制理论的不足。

Method: 对比多任务学习的理论极限，基于样本聚合方法的不可行性定理进行扩展。

Result: 证明在任意大样本量下，缺少分布信息时，聚合样本方法仍无法保证最优风险。

Conclusion: 即使每个任务拥有无限样本，当前方法在多任务学习中仍无法实现最佳风险，这是一个更强的不可适应性结果。

Abstract: Multitask learning and related frameworks have achieved tremendous success in modern applications. In multitask learning problem, we are given a set of heterogeneous datasets collected from related source tasks and hope to enhance the performance above what we could hope to achieve by solving each of them individually. The recent work of arXiv:2006.15785 has showed that, without access to distributional information, no algorithm based on aggregating samples alone can guarantee optimal risk as long as the sample size per task is bounded.
  In this paper, we focus on understanding the statistical limits of multitask learning. We go beyond the no-free-lunch theorem in arXiv:2006.15785 by establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task. This improvement conveys an important message that the hardness of multitask learning cannot be overcame by having abundant data per task. We also discuss the notion of optimal adaptivity that may be of future interests.

</details>


### [95] [Active Learning for Decision Trees with Provable Guarantees](https://arxiv.org/abs/2601.20775)
*Arshia Soltani Moakhar,Tanapoom Laoaron,Faraz Ghahremani,Kiarash Banihashem,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: 在满足特定假设的决策树模型下，本文提供了先前未知的低标签复杂度主动学习算法，并给出了理论上近乎最优的误差阈值依赖性。


<details>
  <summary>Details</summary>
Motivation: 提高决策树在主动学习中的标签效率，解决以往缺乏理论对齐的标签复杂度分析问题。

Method: 首先分析决策树的分歧系数，随后设计了一种可实现乘法误差约束的通用主动学习算法，并将其用于决策树；通过结合上述两部分得到标签查询量为 polylog(N) 的算法。

Result: 在两条假设下，提出的算法实现了仅需多项式对数次标签查询即可达到 (1+ε) 近似，且提供了相应的下界说明误差参数依赖的最优性。

Conclusion: 本文证明了在满足两条自然假设的前提下，基于决策树的主动学习可以实现多项式对数标签复杂度，并给出了相应的下界证明其误差容忍度的依赖性近乎最优。

Abstract: This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+ε)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $ε$ is close to optimal.

</details>


### [96] [Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces](https://arxiv.org/abs/2601.20800)
*Kaito Baba,Yoshihiko Ozaki,Shuhei Watanabe*

Main category: cs.LG

TL;DR: condPED-ANOVA补充了原PED-ANOVA在条件搜索空间中的局限，提供了精确且可解释的超参数重要性评估。


<details>
  <summary>Details</summary>
Motivation: 原PED-ANOVA仅适用于无条件搜索空间，无法准确反映超参数在依赖关系下的真实重要性，导致现有方法在条件场景中产生误导。

Method: 基于条件哈希、闭式估计量推导而成的condPED-ANOVA，结合了原始PED-ANOVA的稳定性与条件激活的准确性。

Result: 实验表明，与现行HPI估计器相比，condPED-ANOVA在条件设置下始终给出可解释且与结构一致的重要性评估。

Conclusion: condPED-ANOVA提供了一种有效处理条件搜索空间中超参数重要性的框架，保持了原PED-ANOVA在高性能区域的优势并补偿了条件依赖带来的偏差。

Abstract: We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.

</details>


### [97] [Reinforcement Learning via Self-Distillation](https://arxiv.org/abs/2601.20802)
*Jonas Hübotter,Frederike Lübeck,Lejs Behric,Anton Baumann,Marco Bagatella,Daniel Marta,Ido Hakimi,Idan Shenfeld,Thomas Kleine Buening,Carlos Guestrin,Andreas Krause*

Main category: cs.LG

TL;DR: SDPO uses a model’s own predictions conditioned on rich feedback as a self‑teacher, turning text explanations into dense learning signals, which yields superior performance compared to prior RLVR methods and increases efficiency in both training and inference.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR learns only from scalar outcome rewards, causing a credit‑assignment bottleneck; verifiable environments often provide detailed textual feedback that is currently underutilized.

Method: SDPO treats the model conditioned on textual feedback as a self‑teacher, distilling its in‑context next‑token predictions back into the policy, thereby converting rich textual corrections into dense gradients without an external reward model.

Result: On scientific reasoning, tool use, and LiveCodeBench v6 competitive programming, SDPO outperforms strong RLVR baselines. It also excels on standard scalar‑reward environments using failed‑attempt rollouts as implicit feedback, and at test time reaches the discovery probability of best‑of‑k sampling or multi‑turn dialogue with 3× fewer attempts.

Conclusion: Self-Distillation Policy Optimization (SDPO) surpasses prior reinforcement learning with verifiable rewards (RLVR) methods by leveraging rich textual feedback as a dense learning signal, improving sample efficiency, final accuracy and discovery probability across diverse tasks.

Abstract: Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

</details>


### [98] [GNN Explanations that do not Explain and How to find Them](https://arxiv.org/abs/2601.20815)
*Steve Azzolin,Stefano Teso,Bruno Lepri,Andrea Passerini,Sagar Malhotra*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.

</details>


### [99] [Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning](https://arxiv.org/abs/2601.20829)
*Minwu Kim,Safal Shrestha,Keith Ross*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.

</details>


### [100] [Reward Models Inherit Value Biases from Pretraining](https://arxiv.org/abs/2601.20838)
*Brian Christian,Jessica A. F. Thompson,Elle Michelle Yang,Vincent Adam,Hannah Rose Kirk,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the "Big Two" psychological axes, we show a robust preference of Llama RMs for "agency" and a corresponding robust preference of Gemma RMs for "communion." This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.

</details>


### [101] [$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval](https://arxiv.org/abs/2601.20844)
*Zihao Wang,Hang Yin,Lihui Liu,Hanghang Tong,Yangqiu Song,Ginny Wong,Simon See*

Main category: cs.LG

TL;DR: 研究指出子集嵌入所需维度仅呈对数级别；基于嵌入的检索瓶颈主要是学习难度，而非几何限制。


<details>
  <summary>Details</summary>
Motivation: 为了解析子集成员关系嵌入所需空间维度的极限，并解释为何基于嵌入的检索性能受限。

Method: 作者对 ℓ₂、内积和余弦相似度等不同“距离”概念，推导出 MED 的紧致理论界限，并通过数值模拟（将子集嵌入设为所含元素嵌入的质心）验证该理论。

Result: 实验结果与理论一致，实证显示 MED 随元素数量呈对数增长，验证了对几何约束不是瓶颈的假设。

Conclusion: 本文证明在向量空间中嵌入子集成员关系所需的最小维度（MED）仅与元素数量对数成正比，说明检索受限主要源自可学习性而非几何限制。

Abstract: This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of "distances" or "similarities," including the $\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.

</details>


### [102] [Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation](https://arxiv.org/abs/2601.20848)
*Weixin Chen,Li Chen,Yuhan Zhao*

Main category: cs.LG

TL;DR: Cofair是一种一次训练即可实现后期灵活公平调节的推荐系统框架，实验验证其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有公平感知方法在训练时就固定公平约束，现实中公平需求随时间变化，重复训练成本高。

Method: 通过共享表示层和公平性条件适配器生成针对不同公平水平的用户嵌入，并加入用户级正则化以保证公平水平递增。

Result: 实验表明，Cofair在多数据集和多基准模型上能够实现动态公平控制，且公平-准确曲线与最先进基线相当或更好。

Conclusion: Cofair提供了一种单训练、可在推理后动态控制公平性的推荐框架，避免了针对不同公平需求重复训练。

Abstract: Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.

</details>


### [103] [Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation](https://arxiv.org/abs/2601.20854)
*Aníbal Silva,Moisés Santos,André Restivo,Carlos Soares*

Main category: cs.LG

TL;DR: 研究将Transformer嵌入VAE，提升表格数据生成的逼真度与多样性平衡，并发现Transformer块相似性，解码器关系接近线性。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知机构成的VAE难以捕捉表格数据中混合类型特征之间的关系，而Transformer的注意力机制更适合捕捉复杂特征交互。

Method: 将Transformer嵌入VAE的不同组件，对57个OpenML CC18数据集进行实验，比较模型在逼真度与多样性方面的表现，并分析Transformer块相似性。

Result: 在潜在层和解码器中使用Transformer可实现逼真度与多样性的权衡；Transformer在各层中的块高度相似，解码器中输入输出关系近似线性。

Conclusion: 文中发现将Transformer整合进VAE的潜在层和解码器可在逼真度与多样性之间取得权衡，并且Transformer在所有层中表现出相近的块，尤其在解码器中输入输出关系近似线性。

Abstract: Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.

</details>


### [104] [Evolutionary Strategies lead to Catastrophic Forgetting in LLMs](https://arxiv.org/abs/2601.20861)
*Immanuel Abdi,Akshat Gupta,Micah Mok,Alexander Lu,Nicholas Lee,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: ES有潜力逼近梯度方法，但在线训练中遗忘严重，需要改进以适用于持续学习。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏部署后持续学习能力，传统梯度方法在大规模LLM训练中对内存需求极高；进化策略作为梯度无关替代方案得以复活，但其在持续学习中的遗忘问题尚不清楚。

Method: 对进化策略（ES）进行全面实验，在数学与推理任务上评估其性能与遗忘曲线；与梯度基方法GRPO在计算预算相当的情况下对比更新稀疏度及ℓ2范数。

Result: ES在相同计算预算下可逼近GRPO的性能，但持续训练时显著遗忘先前学习；原因是ES更新更密集、ℓ2范数更大。

Conclusion: 梯度自由算法如进化策略在持续学习场景下容易遗忘先前能力，需要进一步研究强化其保留机制。

Abstract: One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [105] [Benchmarking LLAMA Model Security Against OWASP Top 10 For LLM Applications](https://arxiv.org/abs/2601.19970)
*Nourin Shahin,Izzat Alsmadi*

Main category: cs.CR

TL;DR: 本论文基准评估了多种Llama模型对10类安全漏洞的检测性能，发现小型专用模型检测率最高且延迟最低，同时提供了公开数据集供复现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从研究原型转为企业系统，其安全漏洞对数据隐私和系统完整性构成严重威胁，亟需系统性基准评估方法。

Method: 采用FABRIC测试平台在NVIDIA A30 GPU上，对五个标准Llama模型和五个Llama Guard变体，用100条覆盖十类OWASP LLM安全漏洞的对抗提示进行威胁检测准确率、响应安全性和计算开销评估。

Result: 在十项漏洞类别的测试中，Llama-Guard-3-1B检测率达到76%，平均延迟0.165s；相比之下，基础模型Llama-3.1-8B检测率为0%，延迟高至0.754s；实验表明模型规模越小，安全效果越好。

Conclusion: 本研究显示，小型专用模型（如Llama-Guard-3-1B）在安全检测上优于大型通用模型，且延迟更低，表明模型规模与安全性能呈负相关；研究还提供了开放数据集，便于后续验证与改进。

Abstract: As large language models (LLMs) move from research prototypes to enterprise systems, their security vulnerabilities pose serious risks to data privacy and system integrity. This study benchmarks various Llama model variants against the OWASP Top 10 for LLM Applications framework, evaluating threat detection accuracy, response safety, and computational overhead. Using the FABRIC testbed with NVIDIA A30 GPUs, we tested five standard Llama models and five Llama Guard variants on 100 adversarial prompts covering ten vulnerability categories. Our results reveal significant differences in security performance: the compact Llama-Guard-3-1B model achieved the highest detection rate of 76% with minimal latency (0.165s per test), whereas base models such as Llama-3.1-8B failed to detect threats (0% accuracy) despite longer inference times (0.754s). We observe an inverse relationship between model size and security effectiveness, suggesting that smaller, specialized models often outperform larger general-purpose ones in security tasks. Additionally, we provide an open-source benchmark dataset including adversarial prompts, threat labels, and attack metadata to support reproducible research in AI security, [1].

</details>


### [106] [Reference-Free Spectral Analysis of EM Side-Channels for Always-on Hardware Trojan Detection](https://arxiv.org/abs/2601.20163)
*Mahsa Tahghigh,Hassan Salmani*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Always-on hardware Trojans (HTs) pose a critical risk to trusted microelectronics, yet most side-channel detection methods rely on unavailable golden references. We present a reference-free approach that combines time-frequency EM analysis with Gaussian Mixture Models (GMMs). By applying Short-Time Fourier Transform (STFT) at multiple window sizes, we show that HT-free circuits exhibit fluctuating statistical structure, while always-on HTs leave persistent footprints with fewer, more consistent mixture components. Results on AES-128 demonstrate feasibility without requiring reference models.

</details>


### [107] [IoT Device Identification with Machine Learning: Common Pitfalls and Best Practices](https://arxiv.org/abs/2601.20548)
*Kahraman Kostas,Rabia Yasa Kostas*

Main category: cs.CR

TL;DR: 本文评估了设备识别中的常见错误，比较了不同识别策略，并给出可执行的改进方案，帮助研究者提升IoT安全模型的可复现性。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍存在识别误差和实验设计缺陷，导致结果缺乏可复现性，为了提升IoT安全模型质量，迫切需要系统性评估与指导。

Method: 通过深入分析设备识别方法（唯一式 vs 类别式）、数据异质性、特征提取挑战和评估指标，系统评估并识别传统研究中的缺陷，随后给出改进建议。

Result: 揭示了如不当数据增强、会话标识混乱等具体错误，并为研究者提供了加强可复现性与通用性的实操手册。

Conclusion: 本文提出了针对IoT安全模型可复现性与泛化性的实用指南，指出并纠正现有文献中的关键错误，提升设备识别过程的可靠性。

Abstract: This paper critically examines the device identification process using machine learning, addressing common pitfalls in existing literature. We analyze the trade-offs between identification methods (unique vs. class based), data heterogeneity, feature extraction challenges, and evaluation metrics. By highlighting specific errors, such as improper data augmentation and misleading session identifiers, we provide a robust guideline for researchers to enhance the reproducibility and generalizability of IoT security models.

</details>


### [108] [Eliciting Least-to-Most Reasoning for Phishing URL Detection](https://arxiv.org/abs/2601.20270)
*Holly Trikilis,Pasindu Marasinghe,Fariza Rashid,Suranga Seneviratne*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Phishing continues to be one of the most prevalent attack vectors, making accurate classification of phishing URLs essential. Recently, large language models (LLMs) have demonstrated promising results in phishing URL detection. However, their reasoning capabilities that enabled such performance remain underexplored. To this end, in this paper, we propose a Least-to-Most prompting framework for phishing URL detection. In particular, we introduce an "answer sensitivity" mechanism that guides Least-to-Most's iterative approach to enhance reasoning and yield higher prediction accuracy. We evaluate our framework using three URL datasets and four state-of-the-art LLMs, comparing against a one-shot approach and a supervised model. We demonstrate that our framework outperforms the one-shot baseline while achieving performance comparable to that of the supervised model, despite requiring significantly less training data. Furthermore, our in-depth analysis highlights how the iterative reasoning enabled by Least-to-Most, and reinforced by our answer sensitivity mechanism, drives these performance gains. Overall, we show that this simple yet powerful prompting strategy consistently outperforms both one-shot and supervised approaches, despite requiring minimal training or few-shot guidance. Our experimental setup can be found in our Github repository github.sydney.edu.au/htri0928/least-to-most-phishing-detection.

</details>


### [109] [SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks](https://arxiv.org/abs/2601.20310)
*Xin Zhang,Zijin Yang,Kejiang Chen,Linfeng Ma,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: 将潜在水印与语义码绑定，使用对比学习训练掩码器，提升黑盒攻击抵御力，兼容现有水印方案。


<details>
  <summary>Details</summary>
Motivation: 现有的基于潜在空间的水印因可见性太强，容易被黑盒攻击者篡改，从而威胁图像来源与可信任性。

Method: 构建语义掩码器，用对比学习得到对同一prompt具有不变性的语义码，随后通过重塑与置换来调制潜在表示，再加入传统潜在水印，使水印与图像语义绑定。

Result: 在四种主流潜在水印方法上，SemBind明显降低了黑盒伪造导致的误接受率，且保持了图像质量，提供可调的安全-鲁棒性权衡。

Conclusion: SemBind是第一套能抵御黑盒伪造的潜在水印防御框架，兼容现有方法并显著提升图像来源可靠性。

Abstract: Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.

</details>


### [110] [UnlearnShield: Shielding Forgotten Privacy against Unlearning Inversion](https://arxiv.org/abs/2601.20325)
*Lulu Xue,Shengshan Hu,Wei Lu,Ziqi Zhou,Yufei Song,Jianhong Cheng,Minghui Li,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 针对未学习反演脆弱性的首个防御方案：在余弦空间施加方向扰动并约束，兼顾模型质量与数据遗忘。


<details>
  <summary>Details</summary>
Motivation: 当前的未学习技术在删除数据后易被攻击者通过反演重构敏感信息，导致隐私泄露，缺乏相应的防护手段。

Method: 通过在余弦表示空间中引入方向性扰动，并利用约束模块对其进行规范化，以实现准确度与忘记效果的双重平衡。

Result: 实验结果表明，UnlearnShield在隐私保护、模型准确率和忘记效果之间取得了良好权衡，显著降低了反演风险。

Conclusion: UnlearnShield提供了一种针对未学习反演的首创防护机制，在有效抹除指定数据的同时保持了模型的高准确率。

Abstract: Machine unlearning is an emerging technique that aims to remove the influence of specific data from trained models, thereby enhancing privacy protection. However, recent research has uncovered critical privacy vulnerabilities, showing that adversaries can exploit unlearning inversion to reconstruct data that was intended to be erased. Despite the severity of this threat, dedicated defenses remain lacking. To address this gap, we propose UnlearnShield, the first defense specifically tailored to counter unlearning inversion. UnlearnShield introduces directional perturbations in the cosine representation space and regulates them through a constraint module to jointly preserve model accuracy and forgetting efficacy, thereby reducing inversion risk while maintaining utility. Experiments demonstrate that it achieves a good trade-off among privacy protection, accuracy, and forgetting.

</details>


### [111] [LIFT: Byzantine Resilient Hub-Sampling](https://arxiv.org/abs/2601.20368)
*Mohamed Amine Legheraba,Nour Rachdi,Maria Gradinariu Potop-Butucaru,Sébastien Tixeuil*

Main category: cs.CR

TL;DR: Elevator在Byzantine场景易受攻击；LIFT通过安全伪随机数强化Hub选取，可提升到10%恶意节点容忍度。


<details>
  <summary>Details</summary>
Motivation: 验证与提升Elevator在面对Byzantine攻击时的鲁棒性，填补其未研究安全性空白。

Method: 评估Elevator在Byzantine环境下性能，并设计LIFT改进Hub选择使用加密安全PRNG。

Result: Elevator仅需2% Byzantine节点即可被破坏；LIFT可抵御最多10%的Byzantine节点。

Conclusion: Elevator脆弱于Byzantine攻击，LIFT通过安全PRNG实现对抗到10% Byzantine节点，提供更可靠的分布式系统基石。

Abstract: Recently, a novel peer sampling protocol, Elevator, was introduced to construct network topologies tailored for emerging decentralized applications such as federated learning and blockchain. Elevator builds hub-based topologies in a fully decentralized manner, randomly selecting hubs among participating nodes. These hubs, acting as central nodes connected to the entire network, can be leveraged to accelerate message dissemination. Simulation results have shown that Elevator converges rapidly (within 3--4 cycles) and exhibits robustness against crash failures and churn. However, its resilience to Byzantine adversaries has not been investigated. In this work, we provide the first evaluation of Elevator under Byzantine adversaries and show that even a small fraction (2%) of Byzantine nodes is sufficient to subvert the network. As a result, we introduce LIFT, a new protocol that extends Elevator by employing a cryptographically secure pseudo-random number generator (PRNG) for hub selection, thereby mitigating Byzantine manipulation. In contrast, LIFT withstands adversarial infiltration and remains robust with up to 10% Byzantine nodes. These results highlight the necessity of secure randomness in decentralized hub formation and position LIFT as a more reliable building block for Byzantine-resilient decentralized systems.

</details>


### [112] [A High-Performance Fractal Encryption Framework and Modern Innovations for Secure Image Transmission](https://arxiv.org/abs/2601.20374)
*Sura Khalid Salsal,Eman Shaker Mahmood,Farah Tawfiq Abdul Hussien,Maryam Mahdi Alhusseini,Azhar Naji Alyahya,Nikolai Safiullin*

Main category: cs.CR

TL;DR: 研究提出分形-傅里叶结合的图像加密方案，实验验证其在速度、保真度及安全性方面优于传统方法，并给出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有图像加密方案在安全、图像保真与计算效率之间存在权衡，数字时代日益严峻的数据安全威胁促使寻找性能更优的技术。


Method: 先通过数学建模将图像分形特征与傅里叶系数相结合，构造分形加密映射；随后设计快速逆变换实现解密；在实验中与常见公钥/对称加密方法进行对比。


Result: 实验表明相较传统加密，分形加密在加密/解密时间显著降低，且图像结构和质量保持更高；安全性评估显示抗解密攻击能力优于基线。


Conclusion: 本文提出并验证了一种基于分形与傅里叶变换的图像加密算法，证明其在安全性与解密效率上相对传统方法有显著提升；同时保持图像质量。


Abstract: The current digital era, driven by growing threats to data security, requires a robust image encryption technique. Classical encryption algorithms suffer from a trade-off among security, image fidelity, and computational efficiency. This paper aims to enhance the performance and efficiency of image encryption. This is done by proposing Fractal encryption based on Fourier transforms as a new method of image encryption, leveraging state-of-the-art technology. The new approach considered here intends to enhance both security and efficiency in image encryption by comparing Fractal Encryption with basic methods. The suggested system also aims to optimise encryption/ decryption times and preserve image quality. This paper provides an introduction to Image Encryption using the fractal-based method, its mathematical formulation, and its comparative efficiency against publicly known traditional encryption methods. As a result, after filling the gaps identified in previous research, it has significantly improved both its encryption/decryption time and image fidelity compared to other techniques. In this paper, directions for future research and possible improvements are outlined for attention.

</details>


### [113] [Towards Quantum-Safe O-RAN -- Experimental Evaluation of ML-KEM-Based IPsec on the E2 Interface](https://arxiv.org/abs/2601.20378)
*Mario Perera,Michael Mackay,Max Hashem Eiza,Alessandro Raschellà,Nathan Shone,Mukesh Kumar Maheshwari*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As Open Radio Access Network (O-RAN) deployments expand and adversaries adopt 'store-now, decrypt-later' strategies, operators need empirical data on the cost of migrating critical control interfaces to post-quantum cryptography (PQC). This paper experimentally evaluates the impact of integrating a NIST-aligned module-lattice KEM (ML-KEM, CRYSTALS-Kyber) into IKEv2/IPsec protecting the E2 interface between the 5G Node B (gNB) and the Near-Real-Time RAN Intelligent Controller (Near-RT RIC). Using an open-source testbed built from srsRAN, Open5GS, FlexRIC and strongSwan (with liboqs), we compare three configurations: no IPsec, classical ECDH-based IPsec, and ML-KEM-based IPsec. The study focuses on IPsec tunnel-setup latency and the runtime behaviour of Near-RT RIC xApps under realistic signalling workloads. Results from repeated, automated runs show that ML-KEM integration adds a small overhead to tunnel establishment, which is approximately 3~5 ms in comparison to classical IPsec, while xApp operation and RIC control loops remain stable in our experiments. These findings indicate that ML-KEM based IPsec on the E2 interface is practically feasible and inform quantum-safe migration strategies for O-RAN deployments.

</details>


### [114] [Fuzzy Private Set Union via Oblivious Key Homomorphic Encryption Retrieval](https://arxiv.org/abs/2601.20400)
*Jean-Guillaume Dumas,Aude Maignan,Luiza Soezima*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Private Set Multi-Party Computations are protocols that allow parties to jointly and securely compute functions: apart from what is deducible from the output of the function, the input sets are kept private. Then, a Private Set Union (PSU), resp. Intersection (PSI), is a protocol that allows parties to jointly compute the union, resp. the intersection, between their private sets. Now a structured PSI, is a PSI where some structure of the sets can allow for more efficient protocols. For instance in Fuzzy PSI, elements only need to be close enough, instead of equal, to be part of the intersection. We present in this paper, Fuzzy PSU protocols (FPSU), able to efficiently take into account approximations in the union. For this, we introduce a new efficient sub-protocol, called Oblivious Key Homomorphic Encryption Retrieval (OKHER), improving on Oblivious Key-Value Retrieval (OKVR) techniques in our setting. In the fuzzy context, the receiver set $X=\{x_i\}_{1..n}$ is replaced by ${\mathcal B}_δ(X)$, the union of $n$ balls of dimension $d$ with radius $δ$, centered at the $x_i$. The sender set is just its $m$ points of dimension $d$. Then the FPSU functionality corresponds to $X \sqcup \{y \in Y, y \notin {\mathcal B}_δ(X)\}$. Thus, we formally define the FPSU functionality and security properties, and propose several protocols tuned to the patterns of the balls using the $l_\infty$ distance. Using our OKHER routine and homomorphic encryption, we are for instance able to obtain a FPSU protocols with an asymptotic communication volume bound ranging from $O(dm\log(δ{n}))$ to $O(d^2m\log(δ^2n))$, depending on the receiver data set structure.

</details>


### [115] [TÄMU: Emulating Trusted Applications at the (GlobalPlatform)-API Layer](https://arxiv.org/abs/2601.20507)
*Philipp Mao,Li Shi,Marcel Busch,Mathias Payer*

Main category: cs.CR

TL;DR: 提出TÄMU平台通过API层插装实现TA动态分析，跨TEE实现高效模糊测试，实验证实发现多零日漏洞，填补TEE安全生态中动态分析空白。


<details>
  <summary>Details</summary>
Motivation: 移动设备的安全核心代码托管在TEE中的可信应用内，若出现漏洞可危害整个系统。由于TEE闭源且多样化，动态分析困难，测试资源只能依赖静态分析。缺乏动态分析手段导致未知漏洞长期潜伏，亟需一种可跨框架、可动态探测TA漏洞的方法。

Method: 1）构建TÄMU rehosting平台，在TEE API层对可信应用进行插装；2）利用GlobalPlatform标准化的TEE API实现跨框架兼容；3）对剩余特定TEE的API引入greedy high‑level emulation，通过手工优先级分配提升模糊测试覆盖率；4）实现并部署在四个TEE上，执行模糊测试及调试，收集漏洞。

Result: 在对四款TEE内的67个TA进行仿真与模糊测试后，TÄMU共发现17个零日漏洞，涉及11个TA，验证了其在不同TEE间的可迁移性与效果。

Conclusion: 本文提出的TÄMU平台通过在API层插装技术，为移动TEE中的可信应用实现动态分析（模糊测试与调试），弥补了闭源、碎片化TEE生态下缺乏动态分析手段的缺口。实验展示了对67个可信应用的成功仿真，并在四种TEE实现上发现了11个可信应用的17个零日漏洞，证明了该方法在实际中的有效性与普适性。

Abstract: Mobile devices rely on Trusted Execution Environments (TEEs) to execute security-critical code and protect sensitive assets. This security-critical code is modularized in components known as Trusted Applications (TAs). Vulnerabilities in TAs can compromise the TEE and, thus, the entire system. However, the closed-source nature and fragmentation of mobile TEEs severely hinder dynamic analysis of TAs, limiting testing efforts to mostly static analyses. This paper presents TÄMU, a rehosting platform enabling dynamic analysis of TAs, specifically fuzzing and debugging, by interposing their execution at the API layer. To scale to many TAs across different TEEs, TÄMU leverages the standardization of TEE APIs, driven by the GlobalPlatform specifications. For the remaining TEE-specific APIs not shared across different TEEs, TÄMU introduces the notion of greedy high-level emulation, a technique that allows prioritizing manual rehosting efforts based on the potential coverage gain during fuzzing. We implement TÄMU and use it to emulate 67 TAs across four TEEs. Our fuzzing campaigns yielded 17 zero-day vulnerabilities across 11 TAs. These results indicate a deficit of dynamic analysis capabilities across the TEE ecosystem, where not even vendors with source code unlocked these capabilities for themselves. TÄMU promises to close this gap by bringing effective and practical dynamic analysis to the mobile TEE domain.

</details>


### [116] [Supply Chain Insecurity: Exposing Vulnerabilities in iOS Dependency Management Systems](https://arxiv.org/abs/2601.20638)
*David Schmidt,Sebastian Schrittwieser,Edgar Weippl*

Main category: cs.CR

TL;DR: iOS依赖管理系统被攻击者利用，CocoaPods依赖混淆与域名劫持导致数百万用户面临远程代码执行风险。


<details>
  <summary>Details</summary>
Motivation: 探究iOS软件供应链中依赖管理系统的安全风险，尤其是CDN、CocoaPods等在移动应用中的攻击面与现有研究的不足。

Method: 对9,212个iOS应用进行静态分析，识别内部包名版本泄露；利用这些信息模拟CocoaPods依赖混淆攻击及域名劫持；进一步检索公共GitHub仓库中存在易受攻击依赖的项目。

Result: 多家主流应用泄露内部依赖信息，单个被劫持的CocoaPod库可影响63个应用，累计受到影响的用户数已达数百万；公开GitHub仓库中安全依赖的使用率也得到量化。

Conclusion: iOS依赖管理系统存在严重安全隐患，攻击者可利用依赖信息泄露或域名劫持实施远程代码执行，影响数百万用户。

Abstract: Dependency management systems are a critical component in software development, enabling projects to incorporate existing functionality efficiently. However, misconfigurations and malicious actors in these systems pose severe security risks, leading to supply chain attacks. Despite the widespread use of smartphone apps, the security of dependency management systems in the iOS software supply chain has received limited attention. In this paper, we focus on CocoaPods, one of the most widely used dependency management systems for iOS app development, but also examine the security of Carthage and Swift Package Manager (SwiftPM). We demonstrate that iOS apps expose internal package names and versions. Attackers can exploit this leakage to register previously unclaimed dependencies in CocoaPods, enabling remote code execution (RCE) on developer machines and build servers. Additionally, we show that attackers can compromise dependencies by reclaiming abandoned domains and GitHub URLs. Analyzing a dataset of 9,212 apps, we quantify how many apps are susceptible to these vulnerabilities. Further, we inspect the use of vulnerable dependencies within public GitHub repositories. Our findings reveal that popular apps disclose internal dependency information, enabling dependency confusion attacks. Furthermore, we show that hijacking a single CocoaPod library through an abandoned domain could compromise 63 iOS apps, affecting millions of users. Finally, we compare iOS dependency management systems with Cargo, Go modules, Maven, npm, and pip to discuss mitigation strategies for the identified threats.

</details>


### [117] [Decentralized Identity in Practice: Benchmarking Latency, Cost, and Privacy](https://arxiv.org/abs/2601.20716)
*Abylay Satybaldy,Kamil Tylinski,Jiahua Xu*

Main category: cs.CR

TL;DR: 对以太坊、Hedera、XRP Ledger 的 DID 进行统一基准：以太坊性能最高但费用高；XRP 低费但泄露多；Hedera 低延迟低费低泄露。


<details>
  <summary>Details</summary>
Motivation: DID 在工业应用普及，但跨平台的操作性与隐私表现尚缺乏系统评估，迫切需要对主流链的实际表现进行对比。

Method: 在统一的实验环境中，使用官方 SDK 对每个链实现 DID 的创建、更新、撤销等生命周期操作进行打样，测量请求延迟、交易费用，并采用基于熵的 Metadata‑Leakage Score 评估元数据泄露。

Result: 以太坊在链下创建速度快，却在链上操作时延迟及费用最高；XRP Ledger 的延迟稳定、费用低，但元数据泄露较多；Hedera 具备最低链上延迟、低费用且元数据泄露最少，但 SDK 侧处理偶有波动。

Conclusion: 本文通过统一实验框架对以太坊、Hedera 与 XRP Ledger 三大主流分布式账本下的 DID 处理器进行基准测试，揭示了各平台在延迟、成本和元数据泄露方面的关键取舍，为选择与配置满足性能与隐私需求的 DID 系统提供了实证依据。

Abstract: Decentralized Identifiers (DIDs) are increasingly deployed on distributed ledgers, yet systematic cross-platform evidence on their operational behavior remains limited. We present an empirical benchmarking study of three prominent ledger-based DID methods - Ethereum, Hedera, and XRP Ledger - using reference Software Development Kits (SDKs) under a unified experimental setup. We measure latency, transaction cost, and on-chain metadata exposure, normalizing latency by each platform's block or consensus interval and cost by its native value transfer fee. Privacy leakage is quantified using a Metadata-Leakage Score (MLS), an entropy-based measure expressed in bits per operation.
  Our results reveal distinct architectural trade-offs. Ethereum enables near-instant, off-chain DID creation, but incurs the highest latency and cost for on-chain lifecycle operations. XRPL delivers deterministic and stable latency with fixed, low fees, yet exhibits higher metadata leakage due to more verbose transaction payloads. Hedera achieves the lowest on-chain latency and low fees with minimal metadata leakage, while occasional variance arises from SDK-side processing and confirmation pipelines.
  Overall, the findings show that ledger architecture and SDK workflows play a major role in shaping DID latency, cost, and metadata exposure, complementing the effects of the underlying consensus mechanism. These results provide evidence-based insights to support informed selection and configuration of DID systems under performance and privacy constraints.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [118] [Holographic & Channel-Aware Distributed Detection of a Non-cooperative Target](https://arxiv.org/abs/2601.20124)
*Domenico Ciuonzo,Alessio Zappone,Marco Di Renzo,Ciro D'Elia*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work investigates Distributed Detection (DD) in Wireless Sensor Networks (WSNs), where spatially distributed sensors transmit binary decisions over a shared flat-fading channel. To enhance fusion efficiency, a reconfigurable metasurface is positioned in the near-field of a few receive antennas, enabling a holographic architecture that harnesses large-aperture gains with minimal RF hardware. A generalized likelihood ratio test is derived for fixed metasurface settings, and two low-complexity joint design strategies are proposed to optimize both fusion and metasurface configuration. These suboptimal schemes achieve a balance between performance, complexity, and system knowledge. The goal is to ensure reliable detection of a localized phenomenon at the fusion center, under energy-efficient constraints aligned with IoT requirements. Simulation results validate the effectiveness of the proposed holographic fusion, even under simplified designs.

</details>


### [119] [WirelessJEPA: A Multi-Antenna Foundation Model using Spatio-temporal Wireless Latent Predictions](https://arxiv.org/abs/2601.20190)
*Viet Chu,Omar Mashaal,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: WirelessJEPA 通过 Joint Embedding Predictive Architecture 与 2D 天线-时间表示及创新遮掩策略，直接从 IQ 数据学习通用特征，六项任务验证其稳健性与优秀泛化，表明 JEPA 方向有望构建通用无线基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有无线模型往往依赖手工对比增广，难以获得通用而鲁棒的特征表示；本工作寻求通过直接从真实多天线 IQ 数据学习通用表示来实现多任务迁移与泛化。

Method: 提出 WirelessJEPA，基于 Joint Embedding Predictive Architecture，设计了 2D 天线-时间表示，将多天线 IQ 流重塑为结构化格子；使用块遮掩与稀疏计算处理未遮掩区块，并引入空间时间遮掩几何以注入天线和时间的先验偏置。

Result: 在六项下游任务中，WirelessJEPA 展现出稳健的性能和强劲的任务泛化能力，证明了基于 JEPA 的方法在无线信号学习任务中的有效性。

Conclusion: JEPA-based learning证明是构建可泛化无线基础模型的有前景方向，既能从真实 IQ 数据中学习通用表示，又能在六项下游任务上展现稳健的性能与卓越的泛化能力。

Abstract: We propose WirelessJEPA, a novel wireless foundation model (WFM) that uses the Joint Embedding Predictive Architecture (JEPA). WirelessJEPA learns general-purpose representations directly from real-world multi-antenna IQ data by predicting latent representations of masked signal regions. This enables multiple diverse downstream tasks without reliance on carefully engineered contrastive augmentations. To adapt JEPA to wireless signals, we introduce a 2D antenna time representation that reshapes multi-antenna IQ streams into structured grids, allowing convolutional processing with block masking and efficient sparse computation over unmasked patches. Building on this representation, we propose novel spatio temporal mask geometries that encode inductive biases across antennas and time. We evaluate WirelessJEPA across six downstream tasks and demonstrate it's robust performance and strong task generalization. Our results establish that JEPA-based learning as a promising direction for building generalizable WFMs.

</details>


### [120] [User Localization via Active Sensing with Electromagnetically Reconfigurable Antennas](https://arxiv.org/abs/2601.20501)
*Ruizhi Zhang,Yuchen Zhang,Ying Zhang*

Main category: eess.SP

TL;DR: 提出一种双时标ERA激活感知体系，利用球谐重构与注意力+LSTM学习，显著提升用户定位精度，优于仅用数字波束或单阶段感知的方法。


<details>
  <summary>Details</summary>
Motivation: 传统数字波束成形在用户定位中受限，缺乏足够的测量多样性与信息量，故需引入可电磁可重构天线以增强测量丰度，同时控制感知开销。

Method: 采用双时标混合设计，数字组合器每个阶段更新一次，ERA模式在子阶段通过球谐表示重构；结合注意力机制提取特征与LSTM时序学习，形成自适应感知策略，逐步细化UE位置估计。

Result: 仿真表明，该混合ERA感知框架在定位精度上显著超越传统数字波束成形及单阶段感知基线。

Conclusion: 基于ERA的主动感知框架消除了单一数字波束所带来的定位信息不足，从而实现了更高精度的用户定位。

Abstract: This paper presents an end-to-end deep learning framework for electromagnetically reconfigurable antenna (ERA)-aided user localization with active sensing, where ERAs provide additional electromagnetic reconfigurability to diversify the received measurements and enhance localization informativeness.
  To balance sensing flexibility and overhead, we adopt a two-timescale design: the digital combiner is updated at each stage, while the ERA patterns are reconfigured at each substage via a spherical-harmonic representation. The proposed mechanism integrates attention-based feature extraction and LSTM-based temporal learning, enabling the system to learn an optimized sensing strategy and progressively refine the UE position estimate from sequential observations. Simulation results show that the proposed approach consistently outperforms conventional digital beamforming-only and single-stage sensing baselines in terms of localization accuracy. These results highlight the effectiveness of ERA-enabled active sensing for user localization in future wireless systems.

</details>


### [121] [Vehicular Wireless Positioning -- A Survey](https://arxiv.org/abs/2601.20547)
*Sharief Saleh,Satyam Dwivedi,Russ Whiton,Peter Hammarberg,Musa Furkan Keskin,Julia Equi,Hui Chen,Florent Munier,Olof Eriksson,Fredrik Gunnarsson,Fredrik Tufvesson,Henk Wymeersch*

Main category: eess.SP

TL;DR: 综述车载无线定位技术（GNSS、5G、Wi‑Fi、UWB、蓝牙、V2V），评估其在不同使用场景下的性能需求，梳理技术演进与标准化，阐述多源融合策略与主要挑战，为未来车用定位研究提供路线图。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境下实现精确可靠的定位是实现成本可控的高度自动驾驶和车联网必不可少的基础，但单一定位技术存在误差与遮挡问题，亟需多技术集成方案。

Method: 综合文献调研，归类定位技术及算法，评估典型车用定位场景与性能需求，梳理技术演进与标准化进程，并讨论多源融合方法与面临的技术挑战。

Result: 构建了无线定位与车用感知/运动传感器融合的完整技术框架，系统性列出关键技术、标准、应用案例，并指明了研究空白与未来趋势。

Conclusion: 本文通过对GNSS、5G、Wi‑Fi、UWB、蓝牙及V2V等无线定位技术的系统综述和与车辆感知与运动传感器的融合研究，得出无线定位结合感知传感器是一条提升车用定位精度与鲁棒性的关键路径。

Abstract: The rapid advancement of connected and autonomous vehicles has driven a growing demand for precise and reliable positioning systems capable of operating in complex environments. Meeting these demands requires an integrated approach that combines multiple positioning technologies, including wireless-based systems, perception-based technologies, and motion-based sensors. This paper presents a comprehensive survey of wireless-based positioning for vehicular applications, with a focus on satellite-based positioning (such as global navigation satellite systems (GNSS) and low-Earth-orbit (LEO) satellites), cellular-based positioning (5G and beyond), and IEEE-based technologies (including Wi-Fi, ultrawideband (UWB), Bluetooth, and vehicle-to-vehicle (V2V) communications). First, the survey reviews a wide range of vehicular positioning use cases, outlining their specific performance requirements. Next, it explores the historical development, standardization, and evolution of each wireless positioning technology, providing an in-depth categorization of existing positioning solutions and algorithms, and identifying open challenges and contemporary trends. Finally, the paper examines sensor fusion techniques that integrate these wireless systems with onboard perception and motion sensors to enhance positioning accuracy and resilience in real-world conditions. This survey thus offers a holistic perspective on the historical foundations, current advancements, and future directions of wireless-based positioning for vehicular applications, addressing a critical gap in the literature.

</details>


### [122] [Precoding Design for Multi-User MIMO Joint Communications and Sensing](https://arxiv.org/abs/2601.20647)
*Charlotte Muth,Shrinivas Chimmalgi,Laurent Schmalen*

Main category: eess.SP

TL;DR: 在多用户MIMO JCAS中，通信信号可用于测量且不必损害通讯，峰度限制测量效果。


<details>
  <summary>Details</summary>
Motivation: 多用户MIMO联合通信与感知系统中，通信与测量信道可能互相干扰，需要探究如何在保证通信质量的同时实现良好检测。

Method: 派生检测概率和通信信噪比指标，分析通信与感知的互相影响，并通过仿真实例验证理论。

Result: 证明利用通信信号进行测量可在信道干扰存在时保持通信性能，并指出峰度是测量性能的瓶颈。

Conclusion: 使用通信信号进行测量，在产生信道干扰时可以避免通信性能下降；然而通信信号发射字母表的峰度会限制测量性能。

Abstract: We investigate precoding for multi-user (MU) multiple-input multiple-output (MIMO) joint communications and sensing (JCAS) systems, taking into account the potential interference between sensing and communication channels. We derive indicators for the sensing and communication performance, i.e., the detection probability and the communication signal-to-interference-and-noise ratio (SINR) for general input signals. Our results show that the use of the communication signal for sensing can prevent a loss in communication performance if channel interference occurs, while the kurtosis of the transmit alphabet of the communication signal limits the sensing performance. We present simulation results of example setups.

</details>


### [123] [Deep Learning based Three-stage Solution for ISAC Beamforming Optimization](https://arxiv.org/abs/2601.20667)
*Qian Gao,Ruikang Zhong,Yuanwei Liu*

Main category: eess.SP

TL;DR: 三阶段深度学习方法拆解ISAC波束优化：特征提取→强化学习波束图案搜索→监督学习重构成形向量，提升通信率并满足检测需求。


<details>
  <summary>Details</summary>
Motivation: 1) 传统ISAC系统只能通过单一目标实现，而多用户多目标混合通信需要更灵活的波束设计；2) 目标检测与通信的双重需求导致传统优化方法计算复杂且难以实时满足。

Method: ①使用无监督学习对可变CSI进行特征提取，得到固定维度的潜在特征；②用强化学习搜索最优波束图案；③通过监督学习将得到的波束图案重构为波束成形向量。

Result: 三阶段方案相较基线RL算法在实现通信率最大化与目标检测之间取得更优平衡，模拟验证其优越性。

Conclusion: 利用深度学习分层框架可高效解决多用户 ISAC 波束成形中的复杂优化问题，显著提升通信与感知性能。

Abstract: In this paper, a general ISAC system where the base station (BS) communicates with multiple users and performs target detection is considered. Then, a sum communication rate maximization problem is formulated, subjected to the constraints of transmit power and the minimum sensing rates of users. To solve this problem, we develop a framework that leverages deep learning algorithms to provide a three-stage solution for ISAC beamforming. The three-stage beamforming optimization solution includes three modules: 1) an unsupervised learning based feature extraction algorithm is proposed to extract fixed-size latent features while keeping its essential information from the variable channel state information (CSI); 2) a reinforcement learning (RL) based beampattern optimization algorithm is proposed to search the desired beampattern according to the extracted features; 3) a supervised learning based beamforming reconstruction algorithm is proposed to reconstruct the beamforming vector from beampattern given by the RL agent. Simulation results demonstrate that the proposed three-stage solution outperforms the baseline RL algorithm by optimizing the intuitional beampattern rather than beamforming.

</details>


### [124] [Grover's Search-Inspired Quantum Reinforcement Learning for Massive MIMO User Scheduling](https://arxiv.org/abs/2601.20688)
*Ruining Fan,Xingyu Huang,Mouli Chakraborty,Avishek Nag,Anshu Mukherjee*

Main category: eess.SP

TL;DR: 使用Grover搜索加速的量子强化学习实现了更高效的mMIMO用户调度，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: mMIMO用户调度面临计算复杂度高、可扩展性差与CSI开销大等挑战。

Method: 采用Grover搜索增强的强化学习机制，通过量子门电路实现策略更新与决策单元的层级架构。

Result: 仿真表明该方法在收敛性方面表现良好，显著优于经典CNN和Quantum Deep Learning基准。

Conclusion: 本工作证明了基于Grover搜索的量子强化学习框架能有效解决mMIMO用户调度问题，呈现出优于传统CNN和QDL的性能。

Abstract: The efficient user scheduling policy in the massive Multiple Input Multiple Output (mMIMO) system remains a significant challenge in the field of 5G and Beyond 5G (B5G) due to its high computational complexity, scalability, and Channel State Information (CSI) overhead. This paper proposes a novel Grover's search-inspired Quantum Reinforcement Learning (QRL) framework for mMIMO user scheduling. The QRL agent can explore the exponentially large scheduling space effectively by applying Grover's search to the reinforcement learning process. The model is implemented using our designed quantum-gate-based circuit, which imitates the layered architecture of reinforcement learning, where quantum operations act as policy updates and decision-making units. Moreover, the simulation results demonstrate that the proposed method achieves proper convergence and significantly outperforms classical Convolutional Neural Networks (CNN) and Quantum Deep Learning (QDL) benchmarks.

</details>


### [125] [Sequential Processing Strategies in Fronthaul Constrained Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2601.20721)
*Vida Ranjbar,Robbert Beerten,Marc Moonen,Sofie Pollin*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In a cell-free massive MIMO (CFmMIMO) network with a daisy-chain fronthaul, the amount of information that each access point (AP) needs to communicate with the next AP in the chain is determined by the location of the AP in the sequential fronthaul. Therefore, we propose two sequential processing strategies to combat the adverse effect of fronthaul compression on the sum of users' spectral efficiency (SE): 1) linearly increasing fronthaul capacity allocation among APs and 2) Two-Path users' signal estimation. The two strategies show superior performance in terms of sum SE compared to the equal fronthaul capacity allocation and Single-Path sequential signal estimation.

</details>


### [126] [Multi-Mode Pinching Antenna Systems Enabled Multi-User Communications](https://arxiv.org/abs/2601.20780)
*Xiaoxia Xu,Xidong Mu,Yuanwei Liu,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: 本研究提出多模 PINCH‑Antenna 系统，实现多用户波导内模式域多路复用；设计了模式选择的 PA 分组与光波导路径优化，给出了两极化方案与粒子群优化算法；仿真显示相较传统方案总速率有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统光波导仅能通过单一导模实现通信，难以满足多用户需求。通过多模传输，可在同一波导内部同时承载多条数据流，实现更高的频谱利用率与并行通信效率。

Method: ①基于物理模型推导 PA 的模式选择辐射特性；②提出基于模式的 PA 分组方案，并结合基带波束成形与天线布局在总速率最大化约束下共同优化；③在两极化非泄漏极限下给出渠道正交解，简化为最大比传输；④对于一般多天线情形，引入零迫波束和粒子群优化（PSO‑ZF）算法求解。

Result: 仿真表明，多模 PASS 在非泄漏与弱泄漏两种工作模式下均优于单模 PASS 与固定天线设计，能够显著提升总速率并满足用户最低速率约束。

Conclusion: 本文证明了多模 pinching‑antenna 系统（PASS）在多用户下行链路中的优势，尤其在非泄漏和弱泄漏两种模式下相较传统单模 PASS 与固定天线结构可显著提升总数据吞吐量。

Abstract: This paper proposes a novel multi-mode pinching-antenna systems (PASS) framework. Multiple data streams can be transmitted within a single waveguide through multiple guided modes, thus facilitating efficient multi-user communications through the mode-domain multiplexing. A physic model is derived, which reveals the mode-selective power radiation feature of pinching antennas (PAs). A two-mode PASS enabled two-user downlink communication system is investigated. Considering the mode selectivity of PA power radiation, a practical PA grouping scheme is proposed, where each PA group matches with one specific guided mode and mainly radiates its signal sequentially. Depending on whether the guided mode leaks power to unmatched PAs or not, the proposed PA grouping scheme operates in either the non-leakage or weak-leakage regime. Based on this, the baseband beamforming and PA locations are jointly optimized for sum rate maximization, subject to each user's minimum rate requirement. 1) A simple two-PA case in non-leakage regime is first considered. To solve the formulated problem, a channel orthogonality based solution is proposed. The channel orthogonality is ensured by large-scale and wavelength-scale equality constraints on PA locations. Thus, the optimal beamforming reduces to maximum-ratio transmission (MRT). Moreover, the optimal PA locations are obtained via a Newton-based one-dimension search algorithm that enforces two-scale PA-location constraints by Newton's method. 2) A general multi-PA case in both non-leakage and weak-leakage regimes is further considered. A low-complexity particle-swarm optimization with zero-forcing beamforming (PSO-ZF) algorithm is developed, thus effectively tackling the high-oscillatory and strong-coupled problem. Simulation results demonstrate the superiority of the proposed multi-mode PASS over conventional single-mode PASS and fixed-antenna structures.

</details>


### [127] [Statistical Properties of Target Localization Using Passive Radar Systems](https://arxiv.org/abs/2601.20817)
*Mats Viberg,Daniele Gerosa,Tomas McKelvey,Thomas Eriksson*

Main category: eess.SP

TL;DR: ECA 借助参考通道取消干扰，在参考通道 SNR 高于阈值时能实现高效目标检测和定位。


<details>
  <summary>Details</summary>
Motivation: 被动雷达系统具有低成本、隐蔽操作等优势，但需要解决直接路径干扰与杂波问题；扩展取消算法可有效提高目标检测与定位性能。

Method: 使用扩展取消算法（ECA），每个接收节点（RN）利用单独的参考通道（RC）捕获的照明机信号，先取消直接路径干扰和来自不需要目标的场景杂波，再对目标参数进行估计。

Result: 推导了高 SNR 条件下 ECA 参数估计的统计特性，并给出参考通道 SNR 的充分条件；仿真结果与理论吻合，证明了在设定 SNR 阈值以上系统表现优秀。

Conclusion: 在参考通道的信噪比满足一定阈值时，扩展取消算法（ECA）能够在高 SNR 场景下实现统计上高效的目标参数估计，从而为被动雷达系统的性能预测与设计提供可靠依据。

Abstract: Passive Radar Systems have received tremendous attention during the past few decades, due to their low cost and ability to remain covert during operation. Such systems do not transmit any energy themselves, but rely on a so-called Illuminator-of-Opportunity (IO), for example a commercial TV station. A network of Receiving Nodes (RN) receive the direct signal as well as reflections from possible targets. The RNs transmit information to a Central Node (CN), that performs the final target detection, localization and tracking. A large number of methods and algorithms for target detection and localization have been proposed in the literature. In the present contribution, the focus is on the seminal Extended Cancelation Algorithm (ECA), in which each RN estimates target parameters after canceling interference from the direct-path as well as clutter from unwanted stationary objects. This is done by exploiting a separate Reference Channel (RC), which captures the IO signal without interference apart from receiver noise. We derive the statistical properties of the ECA parameter estimates under the assumption of a high Signal-to-Noise Ratio (SNR), and we give a sufficient condition for the SNR in the RC to enable statistically efficient estimates. The theoretical results are corroborated through computer simulations, which show that the theory agrees well with empirical results above a certain SNR threshold. The results can be used to predict the performance of passive radar systems in given scenarios, which is useful for feasibility studies as well as system design.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [128] [Control systems for synthetic biology and a case-study in cell fate reprogramming](https://arxiv.org/abs/2601.20135)
*Domitilla Del Vecchio*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper gives an overview of the use of control systems engineering in synthetic biology, motivated by applications such as cell therapy and cell fate reprogramming for regenerative medicine. A ubiquitous problem in these and other applications is the ability to control the concentration of specific regulatory factors in the cell accurately despite environmental uncertainty and perturbations. The paper describes the origin of these perturbations and how they affect the dynamics of the biomolecular ``plant'' to be controlled. A variety of biomolecular control implementations are then introduced to achieve robustness of the plant's output to perturbations and are grouped into feedback and feedforward control architectures. Although sophisticated control laws can be implemented in a computer today, they cannot be necessarily implemented inside the cell via biomolecular processes. This fact constraints the set of feasible control laws to those realizable through biomolecular processes that can be engineered with synthetic biology. After reviewing biomolecular feedback and feedforward control implementations, mostly focusing on the author's own work, the paper illustrates the application of such control strategies to cell fate reprogramming. Within this context, a master regulatory factor needs to be controlled at a specific level inside the cell in order to reprogram skin cells to pluripotent stem cells. The article closes by highlighting on-going challenges and directions of future research for biomolecular control design.

</details>


### [129] [C-AoEI-Aware Cross-Layer Optimization in Satellite IoT Systems: Balancing Data Freshness and Transmission Efficiency](https://arxiv.org/abs/2601.20183)
*Yuhua Zhao,Tiejun Lv,Ke Wang*

Main category: eess.SY

TL;DR: 提出 C‑AoEI 量化措施，结合跨层优化与自适应编码，显著提升卫星 IoT 的可靠性和低时延性能。


<details>
  <summary>Details</summary>
Motivation: 在卫星物联网（S-IoT）中，传播延迟、动态衰落以及带宽短缺构成了三难困境。传统的 Layer‑coded Hybrid ARQ（L‑HARQ）提升可靠性，但其回溯式解码会导致年龄不确定，削弱了标准年龄信息（AoI）指标的意义，无法明确数据新鲜度与传输效率之间的权衡。

Method: 提出以 Cross‑layer Age of Error Information（C‑AoEI）为核心的新指标，导出其闭式表达式，阐明信息新鲜度与系统参数、信道动态之间的显式关联；基于此在多GBS场景下设计了包级编码 L‑HARQ 方案，并制定了联合优化编码与决策阈值的自适应算法。

Result: 实验仿真表明，该框架相较传统方案，可提升 31.8% 的传输效率，C‑AoEI 降低 17.2%；并在跨区干扰和信道波动下保持稳健。

Conclusion: 为下一代低时延、效率兼顾的卫星物联网协议提供稳固理论与实践路径。

Abstract: Satellite-based Internet of Things (S-IoT) faces a fundamental trilemma: propagation delay, dynamic fading, and bandwidth scarcity. While Layer-coded Hybrid ARQ (L-HARQ) enhances reliability, its backtracking decoding introduces age ambiguity, undermining the standard Age of Information (AoI) metric and obscuring the critical trade-off between data freshness and transmission efficiency. To bridge this gap, we propose a novel cross-layer optimization framework centered on a new metric, the Cross-layer Age of Error Information (C-AoEI). We derive a closed-form expression for C-AoEI, explicitly linking freshness to system parameters, establishing an explicit analytical connection between freshness degradation and channel dynamics. Building on this, we develop a packet-level encoded L-HARQ scheme for multi-GBS scenarios and an adaptive algorithm that jointly optimizes coding and decision thresholds. Extensive simulations demonstrate the effectiveness of our proposed framework: it achieves 31.8% higher transmission efficiency and 17.2% lower C-AoEI than conventional schemes. The framework also proves robust against inter-cell interference and varying channel conditions, providing a foundation for designing efficient, latency-aware next-generation S-IoT protocols.

</details>


### [130] [A Data-Driven Krasovskii-Based Approach for Safety Controller Design of Time-Delayed Uncertain Polynomial Systems](https://arxiv.org/abs/2601.20298)
*Omid Akbarzadeh,MohammadHossein Ashoori,Amy Nejati,Abolfazl Lavaei*

Main category: eess.SY

TL;DR: 利用输入-状态数据，采用Krasovskii证书与SOS优化，实现在未知、多项式、离散时间系统中对扰动和时间延迟的鲁棒安全控制。


<details>
  <summary>Details</summary>
Motivation: 解决未知动力学、未知扰动、固定时间延迟的离散时间输入-仿射多项式系统的鲁棒安全控制问题，填补现有研究空白。

Method: 基于Krasovskii控制障碍证书的数据驱动框架，将时间延迟聚合进障碍构造，通过只利用有限时间窗口的输入-状态数据，构造鲁棒Krasovskii CBC，并通过数据驱动的SOS优化实现鲁棒安全控制器设计。

Result: 在有限观测数据下成功合成鲁棒Krasovskii CBC和对应的鲁棒安全控制器，证明在存在未知扰动和固定延迟的情况下能够保证无穷时域的安全性。案例验证显示了方法对三组实验（包括两项物理系统）的有效性。

Conclusion: 本文提供了一种从数据直接推导鲁棒安全控制方案的新方法，有效应对未知系统、时间延迟及扰动的挑战，可在实际工程中实现全面的安全保障。

Abstract: We develop a data-driven framework for the synthesis of robust Krasovskii control barrier certificates (RK-CBC) and corresponding robust safety controllers (R-SC) for discrete-time input-affine uncertain polynomial systems with unknown dynamics, while explicitly accounting for unknown-but-bounded disturbances and time-invariant delays using only observed input-state data. Although control barrier certificates have been extensively studied for safety analysis of control systems, existing work on unknown systems with time delays, particularly in the presence of disturbances, remains limited. The challenge of safety synthesis for such systems stems from two main factors: first, the system's mathematical model is unavailable; and second, the safety conditions should explicitly incorporate the effects of time delays on system evolution during the synthesis process, while remaining robust to unknown disturbances. To address these challenges, we develop a data-driven framework based on Krasovskii control barrier certificates, extending the classical CBC formulation for delay-free systems to explicitly account for time delays by aggregating delayed components within the barrier construction. The proposed framework relies solely on input-state data collected over a finite time horizon, enabling the direct synthesis of RK-CBC and R-SC from observed trajectories without requiring an explicit system model. The synthesis is cast as a data-driven sum-of-squares (SOS) optimization program, yielding a structured design methodology. As a result, robust safety is guaranteed in the presence of unknown disturbances and time delays over an infinite time horizon. The effectiveness of the proposed method is demonstrated through three case studies, including two physical systems.

</details>


### [131] [Efficient Trajectory Design and Communication Scheduling for Dual-UAV Jamming-Aided Secure Communication Networks](https://arxiv.org/abs/2601.20314)
*Xinran Wang,Peng Wu,Xiaopeng Yuan,Yulin Hu,Anke Schmeink*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study dual-unmanned aerial vehicle (UAV) jamming-aided secure communication networks, in which one UAV delivers confidential data to multiple ground users (GUs), while a cooperative UAV provides protective interference against a ground eavesdropper. To enforce fairness, we maximize the minimum secrecy throughput across GUs by jointly designing trajectories and communication scheduling. The key difficulty lies in the continuous-time nature of UAV trajectories and the tight space-time coupling between the transmitter and the jammer, which jointly render the problem infinite-dimensional and nonconvex. To address these challenges, we characterize, for the first time, the structure of the optimal trajectories and rigorously prove that they follow a collaborative successive hover-and-fly (co-SHF) structure, where the two UAVs visit a limited number of synchronized co-hovering point pairs, and during each flight segment at least one UAV moves at maximum speed. Leveraging this structure, we reformulate the problem into a finite-dimensional form, without loss of optimality, over hovering and turning points, hovering durations, and scheduling. For tractability, we adopt a minimum-distance approximation of continuous anti-collision constraints and employ concave lower bounds on secrecy throughput within a successive convex approximation (SCA) method, which converges and, thanks to the co-SHF reduction in optimization variables and constraints, achieves low computational complexity. Numerical results show that, compared with time-discretization and no-jamming benchmarks, the proposed co-SHF design improves the min-secrecy and user fairness while requiring significantly less runtime.

</details>


### [132] [Reducing End-to-End Latency of Cause-Effect Chains with Shared Cache Analysis](https://arxiv.org/abs/2601.20427)
*Yixuan Zhu,Yinkang Gao,Bo Zhang,Xiaohang Gong,Binze Jiang,Lei Gong,Wenqi Lou,Teng Wang,Chao Wang,Xi Li,Xuehai Zhou*

Main category: eess.SY

TL;DR: 本文提出基于调度与链结构的细粒度共享缓存分析框架，显著降低多核共享缓存环境下因果链的终端延迟，实现最大 34% 的改进


<details>
  <summary>Details</summary>
Motivation: 分析多核平台共享缓存情况下因果链的边缘延迟实现所遇到的挑战

Method: 利用调度信息与因果链结构特征，在基本块层面构造细粒度可伸缩的跨核内存访问上下文进行时间敏感的共享缓存分析，获得更精确的 WCET（TSC-WCET），进一步推算终端延迟

Result: 在双核和四核系统以及不同缓存配置下实验表明，平均最大终端延迟可分别降低 34% 与 26%，实现更精准的延迟预测

Conclusion: 构建细粒度的跨核内存访问上下文，充分利用因果链结构和调度信息，显著提升共享缓存 WCET 分析精度，从而有效降低多核因果链终端延迟

Abstract: Cause-effect chains, as a widely used modeling method in real-time embedded systems, are extensively applied in various safety-critical domains. End-to-end latency, as a key real-time attribute of cause-effect chains, is crucial in many applications. But the analysis of end-to-end latency for cause-effect chains on multicore platforms with shared caches still presents an unresolved issue. Traditional methods typically assume that the worst-case execution time (WCET) of each task in the cause-effect chain is known. However, in the absence of scheduling information, these methods often assume that all shared cache accesses result in misses, leading to an overestimation of WCET and, consequently, affecting the accuracy of end-to-end latency. However, effectively integrating scheduling information into the WCET analysis process of the chains may introduce two challenges: first, how to leverage the structural characteristics of the chains to optimize shared cache analysis, and second, how to improve analysis accuracy while avoiding state space explosion.
  To address these issues, this paper proposes a novel end-to-end latency analysis framework designed for multi-chain systems on multicore platforms with shared caches. This framework extracts scheduling information and structural characteristics of cause-effect chains, constructing fine-grained and scalable inter-core memory access contexts at the basic block level for time-sensitive shared cache analysis. This results in more accurate WCET (TSC-WCET) estimates, which are then used to derive the end-to-end latency. Finally, we conduct experiments on dual-core and quad-core systems with various cache configurations, which show that under certain settings, the average maximum end-to-end latency of cause-effect chains is reduced by up to 34% and 26%.

</details>


### [133] [A Timing-Anomaly Free Dynamic Scheduling on Heterogeneous Systems](https://arxiv.org/abs/2601.20445)
*Yixuan Zhu,Yinkang Gao,Lei Gong,Binze Jiang,Xiaohang Gong,Zihan Wang,Cheng Tang,Wenqi Lou,Teng Wang,Chao Wang,Xi Li,Xuehai Zhou*

Main category: eess.SY

TL;DR: 提出 Deterministic Dynamic Execution，利用确定性约束消除时序异常，提供安全紧凑的 WCRT 估计，并通过实验验证其优势


<details>
  <summary>Details</summary>
Motivation: 传统动态调度因时序异常导致 WCRT 分析过于保守或不安全，需要耗时的状态空间搜索；缺乏消除时序异常且能给出安全 WCRT 估计的动态算法

Method: 基于正式定义的异构系统执行进度模型，施加确定性执行约束，形成一种部分限制资源调度和任务执行顺序的动态调度方法，并提供两种生成约束的方案（基于现有调度算法生成的执行轨迹的直接提取，以及启发式构造约束）

Result: 实验表明相比传统动态调度，所提出算法既消除了时序异常，又显著降低了 WCRT 与响应时间抖动

Conclusion: 首次提出了无时序异常的异构系统动态调度算法，保证了安全且紧凑的 WCRT 估计，并能消除传统动态调度中的时序异常

Abstract: Heterogeneous systems commonly adopt dynamic scheduling algorithms to improve resource utilization and enhance scheduling flexibility. However, such flexibility may introduce timing anomalies, wherein locally reduced execution times can lead to an increase in the overall system execution time. This phenomenon significantly complicates the analysis of Worst-Case Response Time (WCRT), rendering conventional analysis either overly pessimistic or unsafe, and often necessitating exhaustive state-space exploration to ensure correctness.
  To address this challenge, this paper presents the first timing-anomaly-free dynamic scheduling algorithm for heterogeneous systems, referred to as Deterministic Dynamic Execution. It achieves a safe and tight WCRT estimate through a single offline simulation execution. The core idea is to apply deterministic execution constraints, which partially restrict the resource allocation and execution order of tasks at runtime. Based on a formally defined execution progress model for heterogeneous system scheduling, we prove the correctness of the proposed execution constraints and their ability to eliminate timing anomalies. Furthermore, we propose two methods to generate execution constraints. The first method derives execution constraints directly from the execution traces produced by existing scheduling algorithms. The second method is a heuristic-based approach that constructs execution constraints, enabling further reduction of the WCRT. Experimental results on synthetically generated DAG task sets under various system configurations demonstrate that, compared to traditional dynamic scheduling algorithms, our approach not only eliminates timing anomalies but also effectively reduces both the WCRT and response time jitter.

</details>


### [134] [Tilt-based Aberration Estimation in Transmission Electron Microscopy](https://arxiv.org/abs/2601.20561)
*Jilles S. van Hulst,Erik M. Franken,Bart J. Janssen,W. P. M. H.,Heemels,Duarte J. Antunes*

Main category: eess.SY

TL;DR: 新型Kalman滤波+A-optimal倾斜设计，结合EM估计噪声，显著降低TEM校准时间并提升成像精度。


<details>
  <summary>Details</summary>
Motivation: TEM光束畸变随时间漂移导致成像失真，需频繁校准；现有校准方式耗时且不考虑样品特定噪声。

Method: 采用卡尔曼滤波器结合实验设计的A‑optimal性，对激发电子束倾斜与图像位移的关系进行建模；离线求解非凸最优化，使用梯度递归滞后（re‑ceding horizon）和多起点策略；利用EM估计样品噪声参数，进而定制倾斜序列。

Result: 在真实TEM设置下验证，优化倾斜序列比传统方法提升显著；畸变漂移模型与实际一致，校准时间从数分钟缩短到不到一分钟。

Conclusion: 该方法成功实现了显微镜畸变和漂移的实时估计，显著缩短了对准过程时长，并提升了图像质量。

Abstract: Transmission electron microscopes (TEMs) enable atomic-scale imaging but suffer from aberrations caused by lens imperfections and environmental conditions, reducing image quality. These aberrations can be compensated by adjusting electromagnetic lenses, but this requires accurate estimates of the aberration coefficients, which can drift over time. This paper introduces a method for the estimation of aberrations in TEM by leveraging the relationship between an induced electron beam tilt and the resulting image shift. The method uses a Kalman filter (KF) to estimate the aberration coefficients from a sequence of image shifts, while accounting for the drift of the aberrations over time. The applied tilt sequence is optimized by minimizing the trace of the predicted error covariance in the KF, which corresponds to the A-optimality criterion in experimental design. We show that this optimization can be performed offline, as the cost criterion is independent of the actual measurements. The resulting non-convex optimization problem is solved using a gradient-based, receding-horizon approach with multi-starts. Additionally, we develop an approach to estimate specimen-dependent noise properties using expectation maximization (EM), which are then used to tailor the tilt pattern optimization to the specific specimen being imaged. The proposed method is validated on a real TEM set-up with several optimized tilt patterns. The results show that optimized patterns significantly outperform naive approaches and that the aberration and drift model accurately captures the underlying physical phenomena. In total, the alignment time is reduced from typically several minutes to less than a minute compared to the state-of-the-art.

</details>


### [135] [Distributed Learning over Noisy Communication Networks](https://arxiv.org/abs/2601.20723)
*Emrah Akyol,Marcos Vasconcelos*

Main category: eess.SY

TL;DR: 本研究揭示二元协调博弈在噪声通信下的学习动态，并给出通过重发/重复编码调节通信预算与协调质量的理论与实验支持。


<details>
  <summary>Details</summary>
Motivation: 研究在图上二元协调博弈中，噪声通信链路如何影响学习动态；

Method: 在BSC与BEC两种误码/擦除模型下，分别分析快通信与快照两种操作模式，并引入有限通信预算与整数K重发/重复编码框架；

Result: 快通信模式下动力学等价于加权聚合作用的Gibbs采样；快照模式虽然不可逆，但其漂移与快模式相同；K预算模型实现两者渐进；实验验证资源与协调质量之间的权衡；

Conclusion: 噪声通信对协调博弈的学习动态具有可控且可量化的影响，可通过退化系数与有效边权解释；

Abstract: We study binary coordination games over graphs under log-linear learning when neighbor actions are conveyed through explicit noisy communication links. Each edge is modeled as either a binary symmetric channel (BSC) or a binary erasure channel (BEC). We analyze two operational regimes. For binary symmetric and binary erasure channels, we provide a structural characterization of the induced learning dynamics. In a fast-communication regime, agents update using channel-averaged payoffs; the resulting learning dynamics coincide with a Gibbs sampler for a scaled coordination potential, where channel reliability enters only through a scalar attenuation coefficient. In a snapshot regime, agents update from a single noisy realization and ignore channel statistics; the induced Markov chain is generally nonreversible, but admits a high-temperature expansion whose drift matches that of the fast Gibbs sampler with the same attenuation. We further formalize a finite-$K$ communication budget, which interpolates between snapshot and fast behavior as the number of channel uses per update grows. This viewpoint yields a communication-theoretic interpretation in terms of retransmissions and repetition coding, and extends naturally to heterogeneous link reliabilities via effective edge weights. Numerical experiments illustrate the theory and quantify the tradeoff between communication resources and steady-state coordination quality.

</details>
