<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 71]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 15]
- [eess.SY](#eess.SY) [Total: 15]
- [eess.SP](#eess.SP) [Total: 13]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [An Integrated Approach to Neural Architecture Search for Deep Q-Networks](https://arxiv.org/abs/2510.19872)
*Iman Rahmani,Saman Yazdannik,Morteza Tayefi,Jafar Roshanian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The performance of deep reinforcement learning agents is fundamentally
constrained by their neural network architecture, a choice traditionally made
through expensive hyperparameter searches and then fixed throughout training.
This work investigates whether online, adaptive architecture optimization can
escape this constraint and outperform static designs. We introduce NAS-DQN, an
agent that integrates a learned neural architecture search controller directly
into the DRL training loop, enabling dynamic network reconfiguration based on
cumulative performance feedback. We evaluate NAS-DQN against three
fixed-architecture baselines and a random search control on a continuous
control task, conducting experiments over multiple random seeds. Our results
demonstrate that NAS-DQN achieves superior final performance, sample
efficiency, and policy stability while incurring negligible computational
overhead. Critically, the learned search strategy substantially outperforms
both undirected random architecture exploration and poorly-chosen fixed
designs, indicating that intelligent, performance-guided search is the key
mechanism driving success. These findings establish that architecture
adaptation is not merely beneficial but necessary for optimal sample efficiency
in online deep reinforcement learning, and suggest that the design of RL agents
need not be a static offline choice but can instead be seamlessly integrated as
a dynamic component of the learning process itself.

</details>


### [2] [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)
*Junfeng Gong,Zhiyi Wei,Junying Chen,Cheng Liu,Huawei Li*

Main category: cs.LG

TL;DR: 提出 ReGraphT，一个训练无、检索增强生成框架，通过将 CUDA 优化轨迹组织为结构化推理图并使用蒙特卡洛图搜索，在小语言模型上转移 LLM 级推理能力，同时提供 CUDA 专用基准，实验显示对比基线显著提升，平均约 2.33x 加速，接近 LLM 性能且保护隐私。


<details>
  <summary>Details</summary>
Motivation: 尽管 CUDA 优化工具和域库不断发展，但充分利用 GPU 的大规模并行性仍然困难。LLMs 能生成优化 CUDA 代码，但云端 API 的代码泄漏风险和本地部署的高开销限制了其实际应用，SLMs 更轻量、隐私友好，但在复杂 CUDA 生成任务中的推理能力不足，因此需要一种方法把 LLM 的推理能力转移给更小的模型。

Method: 提出训练无的检索增强生成（Retrieval-Augmented Generation, RAG）框架 ReGraphT：将 CUDA 优化轨迹组织成结构化推理图，把多步优化视为状态转移，结合蒙特卡洛图搜索（Monte Carlo Graph Search, MCGS）高效搜索；设计面向 CUDA 的难度分级基准以评估推理复杂度；并与 DeepSeek-Coder-V2-Lite-Instruct、Qwen2.5-Coder-7B-Instruct 等 SLM 进行对比评估。

Result: 实验显示 ReGraphT 在 CUDAEval 与 ParEval 上对比 HPC 领域的微调模型和其他检索增强方法获得平均 2.33 倍速度提升；与 DeepSeek-Coder-V2-Lite-Instruct 与 Qwen2.5-Coder-7B-Instruct 组合时，SLMs 的性能接近 LLM，且减少了隐私风险与计算开销。

Conclusion: ReGraphT 提供一种训练无、可检索增强的路径，将 LLM 级推理能力有效转移到更小的语言模型，辅以专用 CUDA 基准来更全面评估推理能力，并在隐私保护和计算资源方面具有显著优势，推动面向 GPU 优化的 SLM 研究与应用。

Abstract: Despite significant evolution of CUDA programming and domain-specific
libraries, effectively utilizing GPUs with massively parallel engines remains
difficult. Large language models (LLMs) show strong potential in generating
optimized CUDA code from sequential code. However, using LLMs in practice faces
two major challenges: cloud-based APIs pose risks of code leakage, and local
deployment is often computationally expensive and inefficient. These drawbacks
have spurred interest in small language models (SLMs), which are more
lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs
can achieve performance comparable to LLMs on specific tasks. While SLMs can
match LLMs on domain-specific tasks, their limited reasoning abilities lead to
suboptimal performance in complex CUDA generation according to our experiments.
To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented
generation framework that transfers LLM-level reasoning to smaller models.
ReGraphT organizes CUDA optimization trajectories into a structured reasoning
graph, modeling the combined CUDA optimizations as state transitions, and
leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also
present a CUDA-specific benchmark with difficulty tiers defined by reasoning
complexity to evaluate models more comprehensively. Experiments show that
ReGraphT outperforms HPC-specific fine-tuned models and other
retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval
and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and
Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level
performance without the associated privacy risks or excessive computing
overhead.

</details>


### [3] [FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning](https://arxiv.org/abs/2510.19893)
*Shiqi Dai,Wei Dai,Jiaee Cheong,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出 FairGRPO，一种公平导向的分层强化学习框架，用于临床诊断，以减少不同人群之间的差异、提高F1，并在缺失 demographic 标签时通过无监督聚类发现潜在群体；并发布 FairMedGemma-4B，具备较强的公平性与性能。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI在不同人群中的性能差异以及由以多数人群为主的数据集导致的偏差放大问题；通过引入自适应重要性权重和无监督分群来实现对异质临床人群的公平学习；应对缺乏人口统计标签的情况。

Method: 在强化学习框架中使用分层策略与自适应优势权重，基于表示、任务难度和数据源对优势进行加权；通过无监督聚类发现潜在的群体以处理缺失标签；在7个数据集、5种模态的临床诊断任务上进行评估；基于 FairGRPO 构建 FairMedGemma-4B VLLM。

Result: 与所有 vanilla 与偏差缓解的 RL 基线相比，减少预测公平性差距27.2%，F1 提升12.49%；训练过程显示公平性在优化中逐步改善；基线RL在训练中公平性下降；在多模态临床诊断任务上取得稳健提升，且 FairMedGemma-4B 达到状态-艺术水平并显著降低人口群体差异。

Conclusion: FairGRPO 能有效实现对异质临床人群的公平学习，缓解 RL 基于医疗诊断的偏差，且具备可扩展性；提出的 FairMedGemma-4B 为公平性导向的临床大模型提供了新基线。

Abstract: Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.

</details>


### [4] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: 提出 FINDER 框架，用于在噪声数据集上进行分类，通过将数据视为随机场、构造随机特征并使用 Kosambi-Karhunen-Loève 展开（KLE）及特征空间的特征分解进行分类，适用于低信噪比和数据稀缺情境。


<details>
  <summary>Details</summary>
Motivation: 解决噪声、样本稀缺等现实数据问题对分类性能的挑战，建立一个在特征学习和推断阶段都能对数据随机性进行理论分析的框架，提升鲁棒性和泛化能力。

Method: 将经验数据视为随机场的实现，映射到合适的希尔伯特空间；利用 KLE 将随机特征分解为不可约分量；通过特征相关算子的谱分析进行分类，数据来自不同类别的区域在谱上呈现差异。

Result: 在数据不足的科学领域取得突破性结果，具体为阿尔茨海默病分期分类与遥感森林砍伐检测等任务的状态-艺术水平提升。

Conclusion: 给出 FINDER 的适用条件、潜在失败模式及局限性，并讨论何时可能优于现有方法，以及未来改进方向。

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [5] [Beyond the Ideal: Analyzing the Inexact Muon Update](https://arxiv.org/abs/2510.19933)
*Egor Shulgin,Sultan AlRashed,Francesco Orabona,Peter Richtárik*

Main category: cs.LG

TL;DR: 本文首次在可实现的近似正交化环境下分析 Muon 的更新，给出近似误差对性能的显式界限，并揭示与步长和动量的耦合关系；NanoGPT 实验验证理论预测。


<details>
  <summary>Details</summary>
Motivation: 现有理论多基于理想化的、计算上不可行的 SVD 更新，难以解释实践中的近似正交化对训练效率的影响，导致理论与实践脱节。本研究将近似正交化纳入可分析框架，提供对性能下降的定量评估。

Method: 在线性最小化 Oracle (LMO) 框架内引入可实现的加性误差模型，推导出关于 LMO 不精确度的性能界限；分析步长与动量之间的耦合关系，解释需要将实现参数（如 Newton-Schulz 迭代步数）与学习率调度共同优化。

Result: 给出显式的性能下降界限，误差越大，性能下降越明显；在较低精度下需要更小的步长但可采用更大动量；提出近似实现与学习率/动量的共调参策略。

Conclusion: 将近似正交化从单纯的实现细节提升为理论-实践的共同关注点，强调近似精度是需与学习计划耦合的关键参数；实验结果（NanoGPT）验证了理论中的耦合关系，为实际训练提供参数选择指南。

Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware
alternative to AdamW, demonstrating strong performance in large-scale training
of neural networks. However, a critical theory-practice disconnect exists:
Muon's efficiency relies on fast, approximate orthogonalization, yet all prior
theoretical work analyzes an idealized, computationally intractable version
assuming exact SVD-based updates. This work moves beyond the ideal by providing
the first analysis of the inexact orthogonalized update at Muon's core. We
develop our analysis within the general framework of Linear Minimization Oracle
(LMO)-based optimization, introducing a realistic additive error model to
capture the inexactness of practical approximation schemes. Our analysis yields
explicit bounds that quantify performance degradation as a function of the LMO
inexactness/error. We reveal a fundamental coupling between this inexactness
and the optimal step size and momentum: lower oracle precision requires a
smaller step size but larger momentum parameter. These findings elevate the
approximation procedure (e.g., the number of Newton-Schulz steps) from an
implementation detail to a critical parameter that must be co-tuned with the
learning schedule. NanoGPT experiments directly confirm the predicted coupling,
with optimal learning rates clearly shifting as approximation precision
changes.

</details>


### [6] [Are Greedy Task Orderings Better Than Random in Continual Linear Regression?](https://arxiv.org/abs/2510.19941)
*Matan Tsipory,Ran Levinstein,Itay Evron,Mark Kong,Deanna Needell,Daniel Soudry*

Main category: cs.LG

TL;DR: Greedy dissimilarity-based task orderings improve empirical convergence in continual linear regression, with theoretical bounds in high-rank settings and repetition-dependent behavior in general rank: single-pass greedy can fail, but repeated passes yield convergence ~k^{-1/3}, contrasting with random orders ~k^{-1/2}.


<details>
  <summary>Details</summary>
Motivation: To understand and formalize task orderings that maximize dissimilarity between consecutive tasks in continual learning, using insights from the Kaczmarz method, and to bridge gaps in theoretical and empirical understanding of greedy versus random orderings.

Method: Formalize greedy dissimilarity-based orderings within the Kaczmarz framework; develop geometric and algebraic intuition; conduct empirical experiments on linear regression with random data and linear probing on CIFAR-100; derive theoretical results: loss bound in high-rank regression; repetition-dependent separation in general rank; compare convergence rates.

Result: Empirically, greedy orderings outperform random ones in terms of average loss across tasks for both synthetic linear regression data and CIFAR-100 linear probes. Theoretically, a loss bound is established for greedy orderings in high-rank settings, analogous to random orderings. In general rank, a repetition-dependent separation emerges: single-pass greedy orderings may fail catastrophically, while repeated passes converge at rate O(k^{-1/3}); random orders achieve O(k^{-1/2}).

Conclusion: Greedy and random task orderings exhibit nuanced differences. Greedy dissimilarity-based ordering can accelerate learning under repetition, but can be fragile in single-pass scenarios, especially in lower-rank regimes. Overall, the work clarifies when greedy advantages hold and how repetition shapes convergence.

Abstract: We analyze task orderings in continual learning for linear regression,
assuming joint realizability of training data. We focus on orderings that
greedily maximize dissimilarity between consecutive tasks, a concept briefly
explored in prior work but still surrounded by open questions. Using tools from
the Kaczmarz method literature, we formalize such orderings and develop
geometric and algebraic intuitions around them. Empirically, we demonstrate
that greedy orderings converge faster than random ones in terms of the average
loss across tasks, both for linear regression with random data and for linear
probing on CIFAR-100 classification tasks. Analytically, in a high-rank
regression setting, we prove a loss bound for greedy orderings analogous to
that of random ones. However, under general rank, we establish a
repetition-dependent separation. Specifically, while prior work showed that for
random orderings, with or without replacement, the average loss after $k$
iterations is bounded by $\mathcal{O}(1/\sqrt{k})$, we prove that single-pass
greedy orderings may fail catastrophically, whereas those allowing repetition
converge at rate $\mathcal{O}(1/\sqrt[3]{k})$. Overall, we reveal nuances
within and between greedy and random orderings.

</details>


### [7] [Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets](https://arxiv.org/abs/2510.19950)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出一个基于椭圆不确定性集合的鲁棒强化学习框架，用以对冲市场冲击引起的模型错配问题。通过隐式和显式的闭式解，获得最坏情景下的鲁棒评估，从而在单资产与多资产交易任务中实现更高的Sharpe比率并对交易量增加保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 训练阶段常在历史数据上学习策略，价格不受策略影响；但在实际部署时，交易行为会改变价格，市场冲击导致评估与真实环境不一致。传统鲁棒RL多使用对称的不确定性结构，未能捕捉市场冲击的方向性特征，亟需更贴近金融市场的建立。

Method: 提出一类椭圆型不确定性集合；推导最坏不确定性下的隐式和显式闭式解，从而实现高效、可扩展的鲁棒策略评估；方法上强调对方向性冲击的建模与数值可解性。

Result: 在单资产与多资产交易任务上实验表明，所提方法在提高夏普比率方面优于基线，并且在交易量增加时仍保持鲁棒性。

Conclusion: 通过更贴近市场冲击的椭圆不确定性集合，提供一种可解释、可扩展的鲁棒RL评估框架，提升金融市场中的策略鲁棒性和可推广性。

Abstract: In financial applications, reinforcement learning (RL) agents are commonly
trained on historical data, where their actions do not influence prices.
However, during deployment, these agents trade in live markets where their own
transactions can shift asset prices, a phenomenon known as market impact. This
mismatch between training and deployment environments can significantly degrade
performance. Traditional robust RL approaches address this model
misspecification by optimizing the worst-case performance over a set of
uncertainties, but typically rely on symmetric structures that fail to capture
the directional nature of market impact. To address this issue, we develop a
novel class of elliptic uncertainty sets. We establish both implicit and
explicit closed-form solutions for the worst-case uncertainty under these sets,
enabling efficient and tractable robust policy evaluation. Experiments on
single-asset and multi-asset trading tasks demonstrate that our method achieves
superior Sharpe ratio and remains robust under increasing trade volumes,
offering a more faithful and scalable approach to RL in financial markets.

</details>


### [8] [Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency](https://arxiv.org/abs/2510.19980)
*Renzhao Liang,Sizhe Xu,Chenggang Xie,Jingru Chen,Feiyang Ren,Shu Yang,Takahiro Yabe*

Main category: cs.LG

TL;DR: Truncating historical data can unexpectedly improve time series forecasting; the proposed AMRC method reduces redundant feature learning and boosts accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of deep learning models on long historical sequences, where noise and redundant fluctuations may be learned, and to leverage information bottleneck ideas for more robust forecasting.

Method: Adaptive Masking Loss with Representation Consistency (AMRC). It has two components: (1) Dynamic masking loss to identify and emphasize highly discriminative temporal segments during training; (2) Representation consistency constraint to stabilize relationships among inputs, labels, and predictions, guided by information bottleneck principles.

Result: Empirical results show that AMRC suppresses redundant feature learning and significantly improves forecasting performance across datasets, challenging the conventional long-sequence assumption and offering theoretical and methodological advancements.

Conclusion: AMRC provides both theoretical insights and practical techniques for efficient, robust time series forecasting, questioning the necessity of long histories and offering a new approach to model training and representation learning.

Abstract: Time series forecasting plays a pivotal role in critical domains such as
energy management and financial markets. Although deep learning-based
approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the
prevailing "long-sequence information gain hypothesis" exhibits inherent
limitations. Through systematic experimentation, this study reveals a
counterintuitive phenomenon: appropriately truncating historical data can
paradoxically enhance prediction accuracy, indicating that existing models
learn substantial redundant features (e.g., noise or irrelevant fluctuations)
during training, thereby compromising effective signal extraction. Building
upon information bottleneck theory, we propose an innovative solution termed
Adaptive Masking Loss with Representation Consistency (AMRC), which features
two core components: 1) Dynamic masking loss, which adaptively identified
highly discriminative temporal segments to guide gradient descent during model
training; 2) Representation consistency constraint, which stabilized the
mapping relationships among inputs, labels, and predictions. Experimental
results demonstrate that AMRC effectively suppresses redundant feature learning
while significantly improving model performance. This work not only challenges
conventional assumptions in temporal modeling but also provides novel
theoretical insights and methodological breakthroughs for developing efficient
and robust forecasting models.

</details>


### [9] [On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization](https://arxiv.org/abs/2510.19953)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出基于函数评估的无偏梯度估计器，改造零阶优化（ZOO）中对梯度的偏差问题；通过将方向导数重写为望远镜级数并从精心设计的分布中采样，构造出在保持较低方差的同时无偏的梯度估计器。给出四种具体构造的最优尺度分布与步长，并证明在光滑非凸目标下，使用这些估计器的 SGD 达到最优复杂度；实验显示在合成任务和语言模型微调中优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有ZOO方法中梯度估计的偏差问题，寻求仅依赖函数评估即可获得无偏梯度估计，同时控制估计方差以保证实际优化效果。

Method: 将方向导数重写为望远镜级数，设计合适的采样分布以得到无偏的梯度估计器；提出四种具体构造，给出最优的尺度分布与扰动步长；对估计量的偏差与方差进行理论分析，证明在光滑非凸目标下SGD结合该估计量达到最优复杂度；通过合成任务与语言模型微调的实验验证性能提升。

Result: 得到一族无偏的梯度估计器，完全基于函数评估且方差受控；理论上推导出四种构造的最优尺度分布与扰动步长，证实使用该估计器的SGD在光滑非凸目标上具备最优复杂度；实验结果显示在精度与收敛性方面优于常见的带偏差的ZOO方法。

Conclusion: 证明了可仅通过函数查询获得无偏梯度估计，从而提升零阶优化的理论与实践性能，尤其在光滑非凸问题上实现了最优复杂度并在实验中展现出更好的收敛性与准确性。

Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic
optimization when gradients are unavailable or expensive to compute. A
potential limitation of existing ZOO methods is the bias inherent in most
gradient estimators unless the perturbation stepsize vanishes. In this paper,
we overcome this biasedness issue by proposing a novel family of unbiased
gradient estimators based solely on function evaluations. By reformulating
directional derivatives as a telescoping series and sampling from carefully
designed distributions, we construct estimators that eliminate bias while
maintaining favorable variance. We analyze their theoretical properties, derive
optimal scaling distributions and perturbation stepsizes of four specific
constructions, and prove that SGD using the proposed estimators achieves
optimal complexity for smooth non-convex objectives. Experiments on synthetic
tasks and language model fine-tuning confirm the superior accuracy and
convergence of our approach compared to standard methods.

</details>


### [10] [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://arxiv.org/abs/2510.19975)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出并分析方向对齐扰动(DAP)以降低两点零阶梯度估计的渐近方差，并扩展δ-无偏扰动的收敛性研究，结合理论与实验表明在特定条件下优于传统固定长度扰动。


<details>
  <summary>Details</summary>
Motivation: 解决两点零阶梯度估计的方差与效率问题；现有研究多聚焦固定长度扰动，忽略了扰动方向对齐的潜在优势，本文将扰动分布通过约束泛函优化来寻找最优解。

Method: 把扰动分布优化问题建模为约束泛函优化，推导在扰动步长趋于0时方差最小化的方向性扰动分布，提出方向对齐扰动(DAP)方案并给出理论性质与收敛性分析；扩展到δ-无偏扰动的收敛性与复杂度界，辅以合成与真实任务的实验评估。

Result: 理论上证明方向对齐扰动在某些方向上比固定长度更具准确性；实证结果显示DAP在特定条件下优于传统方法，且给出收敛性与复杂度界的扩展。

Conclusion: DAP提供更灵活的扰动策略，透过方向对齐提升估计精度，适用于更广泛的扰动类型与问题设置；未来工作包括更广泛应用与对比研究。

Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and
identify the distribution of random perturbations that minimizes the
estimator's asymptotic variance as the perturbation stepsize tends to zero. We
formulate it as a constrained functional optimization problem over the space of
perturbation distributions. Our findings reveal that such desired perturbations
can align directionally with the true gradient, instead of maintaining a fixed
length. While existing research has largely focused on fixed-length
perturbations, the potential advantages of directional alignment have been
overlooked. To address this gap, we delve into the theoretical and empirical
properties of the directionally aligned perturbation (DAP) scheme, which
adaptively offers higher accuracy along critical directions. Additionally, we
provide a convergence analysis for stochastic gradient descent using
$\delta$-unbiased random perturbations, extending existing complexity bounds to
a wider range of perturbations. Through empirical evaluations on both synthetic
problems and practical tasks, we demonstrate that DAPs outperform traditional
methods under specific conditions.

</details>


### [11] [Towards Strong Certified Defense with Universal Asymmetric Randomization](https://arxiv.org/abs/2510.19977)
*Hanbin Hong,Ashish Kundu,Ali Payani,Binghui Wang,Yuan Hong*

Main category: cs.LG

TL;DR: Proposes UCAN, a universal framework to achieve certified adversarial robustness using anisotropic noise in randomized smoothing. It converts any isotropic smoothing method into an anisotropic one, supports various noise distributions and l_p norms, and introduces three Noise Parameter Generators (NPGs) to tailor per-dimension noise; shows substantial gains in certified accuracy on MNIST, CIFAR-10, and ImageNet with publicly available code.


<details>
  <summary>Details</summary>
Motivation: Current randomized smoothing largely uses isotropic noise, which ignores input heterogeneity and may limit robustness. Anisotropic noise can better align perturbation defenses with data dimensions, potentially increasing certified robustness.

Method: A universal framework that augments existing smoothing methods by injecting anisotropic noise distributions. The theory supports multiple noise types across different l_p norms and any classifier, providing provable robustness bounds. Introduces three Noise Parameter Generators (NPGs) to optimize per-dimension noise parameters.

Result: Empirical results show significant improvements in certified accuracy at large radii, up to 182.6% on MNIST, CIFAR-10, and ImageNet. Code is available for reproduction.

Conclusion: UCAN offers a versatile, practical enhancement to randomized smoothing by enabling anisotropic noise, compatible with existing methods and classifiers, and capable of delivering substantial robustness gains through data-dimension-aware noise optimization via NPGs.

Abstract: Randomized smoothing has become essential for achieving certified adversarial
robustness in machine learning models. However, current methods primarily use
isotropic noise distributions that are uniform across all data dimensions, such
as image pixels, limiting the effectiveness of robustness certification by
ignoring the heterogeneity of inputs and data dimensions. To address this
limitation, we propose UCAN: a novel technique that \underline{U}niversally
\underline{C}ertifies adversarial robustness with \underline{A}nisotropic
\underline{N}oise. UCAN is designed to enhance any existing randomized
smoothing method, transforming it from symmetric (isotropic) to asymmetric
(anisotropic) noise distributions, thereby offering a more tailored defense
against adversarial attacks. Our theoretical framework is versatile, supporting
a wide array of noise distributions for certified robustness in different
$\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the
classifier's prediction over perturbed inputs with provable robustness bounds
through tailored noise injection. Additionally, we develop a novel framework
equipped with three exemplary noise parameter generators (NPGs) to optimally
fine-tune the anisotropic noise parameters for different data dimensions,
allowing for pursuing different levels of robustness enhancements in
practice.Empirical evaluations underscore the significant leap in UCAN's
performance over existing state-of-the-art methods, demonstrating up to
$182.6\%$ improvement in certified accuracy at large certified radii on MNIST,
CIFAR10, and ImageNet datasets.\footnote{Code is anonymously available at
\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}

</details>


### [12] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: 在一个现实场景下的RFID/RSSI数据上，使用决策树对12个实验区进行定位分类，结果整体准确率34.2%，多区域F1分数>0.4，存在类别不平衡与稀有区分类困难的问题，提出通过天线布局改进和传感融合来提升低覆盖区域的分类鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为防务资产的存储与安保合规提供可行的定位监测方案；现有传感器特异性不足、存在长程探测、欺骗与伪造等风险，因此需要在CAD建模的现实场景中通过RSSI数据进行 zone-level 定位与异常检测研究。

Method: 在CAD建模的平面布局上收集多达约980,000条RSSI读取，针对12个区域（LabZoneA-L）进行多类分类。针对数据不平衡采用类别权重；在分层子样本中训练至5,000个平衡观测，使用邻接关系的混淆矩阵来解释物理相邻区域的混淆现象；采用决策树作为监督学习模型。

Result: 总体准确率34.2%，一些区域（如F、G、H等）F1分数>0.4；罕见类别如LabZoneC易被误分类，即使引入类别权重；使用邻接混淆矩阵帮助解读区域间的物理邻接关系。

Conclusion: 基于RSSI的决策树在现实仿真中可用于区域级异常检测或错误放置监控的初步应用；在低覆盖/低信号区域的鲁棒性仍受限，需通过改进天线布局、增加传感器或进行传感融合来提升性能。

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [13] [SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph](https://arxiv.org/abs/2510.20022)
*Jiazheng Li,Yawei Wang,David Yan,Yijun Tian,Zhichao Xu,Huan Song,Panpan Xu,Lin Lee Cheong*

Main category: cs.LG

TL;DR: SALT 提出一种基于轨迹图的细粒度优势分配框架，针对基于群体的强化学习算法（如 GRPO），利用结果奖励推导逐步优势，并以插件方式嵌入，几乎无额外开销，在多任务多模型规模的基准上提升性能。


<details>
  <summary>Details</summary>
Motivation: LLMs 在多步、长时任务上的能力仍受限，且当前大多基于稀疏结果奖励的 RL 训练遇到困难，尤其是在缺乏 critic 的群体基 RL（如 GRPO）中。统一奖励可能混淆有益与有害动作，导致训练不稳定和子优策略，需要从结果信号中提取更细粒度的阶段性优势。

Method: SALT 通过把同一提示的轨迹构建为图，量化每一步的质量并据此分配优势，得到更细粒度的行动梯度。它是一个轻量的、可插拔的模块，无需修改 rollout 流程，且额外计算开销极小，能够与现有的群体基 RL 算法无缝对接。

Result: 在 WebShop、ALFWorld、AppWorld 等基准上，覆盖不同模型规模的实验中，SALT 均能稳定提升性能。还对设计选择进行了系统分析，给出可操作的见解与洞察。

Conclusion: SALT 提供了一种仅依赖结果奖励即可推导的细粒度优势分配方案，提升群体 RL 的训练稳定性与策略质量，且作为插件方式易于广泛应用于现有工作流。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
enabling language agents to excel at single-turn tasks. However, their
application to complex, multi-step, and long-horizon tasks remains challenging.
While reinforcement learning (RL) offers a promising avenue for addressing
these challenges, mainstream approaches typically rely solely on sparse,
outcome-based rewards, a limitation that becomes especially problematic for
group-based RL algorithms lacking critic models, such as Group Relative Policy
Optimization (GRPO). In such methods, uniformly rewarding or penalizing all
actions within a trajectory can lead to training instability and suboptimal
policies, because beneficial and detrimental actions are often entangled across
multi-step interactions. To address this challenge, we propose SALT, a novel
and lightweight framework that provides a finer-grained advantage assignment,
derived solely from outcome rewards. We achieve this by constructing a graph
from trajectories of the same prompt, which allows us to quantify the quality
of each step and assign advantages accordingly. Crucially, SALT is designed as
a plug-and-play module that seamlessly integrates with existing group-based RL
algorithms, requiring no modifications to the rollout procedure and introducing
negligible computational overhead. Extensive experiments on the WebShop,
ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that
SALT consistently improves performance. We also conduct a thorough analysis to
validate the design choices behind SALT and offer actionable insights.

</details>


### [14] [Speculative Sampling for Parametric Temporal Point Processes](https://arxiv.org/abs/2510.20031)
*Marin Biloš,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 提出一种基于拒绝采样的算法，在不改动模型或重新训练的前提下，能够从现有的时序点过程模型并行、精确地对未来多步进行采样，并提供理论保证与在真实数据集上的速率提升。


<details>
  <summary>Details</summary>
Motivation: 解决自回归时序点过程（TPP）在采样时的序列化瓶颈，寻求在不修改模型架构和不再训练的情况下实现并行且精确的多步采样，以提升大规模场景的生成效率。

Method: 提出一种基于拒绝采样的算法，使得可以从现有TPP模型中并行地、精确地采样未来多步值，无需任何架构改动或再训练；给出理论保证，辅以在真实数据集上的实验验证以展示加速效果。

Result: 理论上具备收敛性和正确性保证，实验上在真实数据集上实现了相对显著的速度提升，证实了在不牺牲表达能力的前提下实现并行生成的可行性。

Conclusion: 该方法将TPP的表达能力与高效并行生成结合起来，为大规模TPP应用提供一种无重训练代价的高效采样方案，填补了表达性与采样效率之间的空白。

Abstract: Temporal point processes are powerful generative models for event sequences
that capture complex dependencies in time-series data. They are commonly
specified using autoregressive models that learn the distribution of the next
event from the previous events. This makes sampling inherently sequential,
limiting efficiency. In this paper, we propose a novel algorithm based on
rejection sampling that enables exact sampling of multiple future values from
existing TPP models, in parallel, and without requiring any architectural
changes or retraining. Besides theoretical guarantees, our method demonstrates
empirical speedups on real-world datasets, bridging the gap between expressive
modeling and efficient parallel generation for large-scale TPP applications.

</details>


### [15] [Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards](https://arxiv.org/abs/2510.20055)
*Yuwei Cheng,Zifeng Zhao,Haifeng Xu*

Main category: cs.LG

TL;DR: Contextual MDP-based ad bidding with delayed Poisson rewards; a two-stage ML estimator with data-splitting for efficient estimation; an RL algorithm yielding near-optimal regret; validated by simulations.


<details>
  <summary>Details</summary>
Motivation: Online advertising requires accounting for delayed and long-term effects, cumulative ad impacts (reinforcement/fatigue), and customer heterogeneity. Previous work rarely addresses these jointly, limiting personalized bidding accuracy.

Method: Model ad bidding as a Contextual Markov Decision Process with delayed Poisson rewards. Use a two-stage maximum likelihood estimator with data-splitting to control estimation error. Develop a reinforcement learning algorithm to derive efficient personalized bidding policies and prove regret bounds.

Result: The proposed approach achieves a near-optimal regret bound of tilde O(d H^2 sqrt(T)) where d is contextual dimension, H rounds, T customers; simulation studies validate the theoretical results.

Conclusion: The framework provides principled, efficient, personalized bidding strategies with controlled estimation error and solid theoretical guarantees, demonstrated via simulations.

Abstract: Online advertising platforms use automated auctions to connect advertisers
with potential customers, requiring effective bidding strategies to maximize
profits. Accurate ad impact estimation requires considering three key factors:
delayed and long-term effects, cumulative ad impacts such as reinforcement or
fatigue, and customer heterogeneity. However, these effects are often not
jointly addressed in previous studies. To capture these factors, we model ad
bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson
rewards. For efficient estimation, we propose a two-stage maximum likelihood
estimator combined with data-splitting strategies, ensuring controlled
estimation error based on the first-stage estimator's (in)accuracy. Building on
this, we design a reinforcement learning algorithm to derive efficient
personalized bidding strategies. This approach achieves a near-optimal regret
bound of $\tilde{O}{(dH^2\sqrt{T})}$, where $d$ is the contextual dimension,
$H$ is the number of rounds, and $T$ is the number of customers. Our
theoretical findings are validated by simulation experiments.

</details>


### [16] [Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs](https://arxiv.org/abs/2510.20064)
*Hongyi Liu,Jiaji Huang,Zhen Jia,Youngsuk Park,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出一个在线学习框架用于草稿模型选择，在回顾意义上对每个查询与最佳草稿模型竞争；可以无额外查询地评估所有草稿，随着草稿数量增加对现有带宽方法实现指数级提升；方法可应用于单草稿、多草稿和草稿树等变体。


<details>
  <summary>Details</summary>
Motivation: 在 speculative decoding 场景中，草稿模型的数量日益增多导致选择质量与延迟之间的权衡变得更为复杂。需要一个能够对每个查询与最优草稿在回顾意义上竞争的在线学习方法，并且在扩展到多草稿设置时仍具高效性。

Method: 给出一种在线学习算法，能够在每个查询中对所有草稿模型进行准确评估，而无需对目标模型进行额外查询；该方法可与单草稿、多草稿及草稿树等 speculative 解码形式无缝结合，并设计了系统高效的在线学习器以降低计算与延迟开销。

Result: 理论上对每个查询的最佳草稿具有竞争保证（对手在回顾意义上的最佳草稿）。相比现有基于 Bandit 的方法，草稿数量增加时优势呈指数级提升。大量在开源 LLM 与多样数据集上的实验表明，该方法在多领域、尤其需要长推理链的场景中，显著优于 EAGLE3 与 BanditSpec。

Conclusion: 所提出的在线草稿模型选择框架具有通用性、高效性和可扩展性，能够提升不同 speculative decoding 变体的推理效率和性能。

Abstract: Speculative decoding is widely used in accelerating large language model
(LLM) inference. In this work, we focus on the online draft model selection
problem in speculative decoding. We design an algorithm that provably competes
with the best draft model in hindsight for each query in terms of either the
token acceptance probability or expected acceptance length. In particular, we
show that we can accurately evaluate all draft models, instead of only the
chosen model without incurring additional queries to the target model, which
allows us to improve exponentially over the existing bandit-based approach as
the number of draft models increases. Our approach is generically applicable
with any speculative decoding methods (single draft, multi-drafts and
draft-trees). Moreover, we design system-efficient versions of online learners
and demonstrate that the overhead in computation and latency can be
substantially reduced. We conduct extensive experiments on open-source LLMs and
diverse datasets, demonstrating that our methods substantially outperform the
state-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains
where specialized domain-expert drafters are available, especially when long
reasoning chains are required.

</details>


### [17] [Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics](https://arxiv.org/abs/2510.20068)
*Ram Dyuthi Sristi,Sowmya Manojna Narasimha,Jingya Huang,Alice Despatin,Simon Musall,Vikash Gilja,Gal Mishne*

Main category: cs.LG

TL;DR: 提出 Coupled Transformer Autoencoder (CTAE)，一种基于 Transformer 的序列模型，用于同时建模跨区域神经数据的非线性非平稳动力学，并在同一框架中将各脑区潜在表示分为共享与私有子空间，从而提升解码行为变量的能力。


<details>
  <summary>Details</summary>
Motivation: 现有对齐/多视角方法往往忽略时间结构；动力学潜变量模型虽能捕捉时间依赖，但多限定于单区域、线性读出或难以区分共享与私有信号，亟需一个能处理多区域时序数据并分离共享/私有结构的模型。

Method: CTAE 使用 Transformer 编码器/解码器以捕获长距离时序依赖，并将每个区域的潜在空间正交分解为共享子空间和私有子空间，联合优化以同时建模动态结构与跨区域信号分离。

Result: 在两组高密度电生理数据集（来自运动皮层和来自感知皮层）上评估，CTAE 提取的表征更有效地解码行为变量，优于现有方法。

Conclusion: CTAE 提供了一个统一框架，能够处理非线性、非平稳的跨区域动力学，并明确区分共享与区域特异信号，为多区域神经数据分析与解码提供新工具。

Abstract: Simultaneous recordings from thousands of neurons across multiple brain areas
reveal rich mixtures of activity that are shared between regions and dynamics
that are unique to each region. Existing alignment or multi-view methods
neglect temporal structure, whereas dynamical latent variable models capture
temporal dependencies but are usually restricted to a single area, assume
linear read-outs, or conflate shared and private signals. We introduce the
Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both
(i) non-stationary, non-linear dynamics and (ii) separation of shared versus
region-specific structure in a single framework. CTAE employs transformer
encoders and decoders to capture long-range neural dynamics and explicitly
partitions each region's latent space into orthogonal shared and private
subspaces. We demonstrate the effectiveness of CTAE on two high-density
electrophysiology datasets with simultaneous recordings from multiple regions,
one from motor cortical areas and the other from sensory areas. CTAE extracts
meaningful representations that better decode behavioral variables compared to
existing approaches.

</details>


### [18] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: ShapeX通过将时间序列划分为由shapelet驱动的片段并用Shapley值评估其显著性，提供比现有PHTSE更具因果解释力的时序分类解释框架。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，现有解释方法多聚焦逐时间点的属性分配，忽视shapelet驱动的关键子序列对分类的决定性作用，因此需要一个能识别关键shapelet并给出因果解释的框架。

Method: ShapeX通过Shapelet Describe-and-Detect (SDD) 学习多样化的shapelet集合，并将时间序列划分为shapelet驱动的片段，使用Shapley值衡量这些片段的显著性，输出可解释的因果级别解释。

Result: 在合成和真实数据集上，ShapeX在识别最相关子序列方面优于现有方法，并提高了解释的精确性和因果保真度。

Conclusion: ShapeX提供了一种以shapelet为核心、可解释且具有因果解释力的时间序列分类解释框架，为高风险领域的透明度和信任提供支撑。

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [19] [Competition is the key: A Game Theoretic Causal Discovery Approach](https://arxiv.org/abs/2510.20106)
*Amartya Roy,Souvik Chakraborty*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Causal discovery remains a central challenge in machine learning, yet
existing methods face a fundamental gap: algorithms like GES and GraN-DAG
achieve strong empirical performance but lack finite-sample guarantees, while
theoretically principled approaches fail to scale. We close this gap by
introducing a game-theoretic reinforcement learning framework for causal
discovery, where a DDQN agent directly competes against a strong baseline (GES
or GraN-DAG), always warm-starting from the opponent's solution. This design
yields three provable guarantees: the learned graph is never worse than the
opponent, warm-starting strictly accelerates convergence, and most importantly,
with high probability the algorithm selects the true best candidate graph. To
the best of our knowledge, our result makes a first-of-its-kind progress in
explaining such finite-sample guarantees in causal discovery: on synthetic SEMs
(30 nodes), the observed error probability decays with n, tightly matching
theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,
Dream, and Andes, our method consistently improves upon GES and GraN-DAG while
remaining theoretically safe. Remarkably, it scales to large graphs such as
Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these
results establish a new class of RL-based causal discovery algorithms that are
simultaneously provably consistent, sample-efficient, and practically scalable,
marking a decisive step toward unifying empirical performance with rigorous
finite-sample theory.

</details>


### [20] [On pattern classification with weighted dimensions](https://arxiv.org/abs/2510.20107)
*Ayatullah Faruk Mollah*

Main category: cs.LG

TL;DR: 提出了一种基于逐维加权的距离度量用于模式分类， 将该加权方案嵌入KNN， 并在多组合成及真实数据集上进行验证，尤其在基因表达数据上达到显著提升。


<details>
  <summary>Details</summary>
Motivation: 高维、多样本分布情形下，距离度量对模式分类影响显著。传统欧氏距离在高维/小样本场景容易受维度权重分布和相关性影响，亟需对不同维度赋予不同权重并研究其对分类边界的影响。

Method: 提出逐维加权方案，对每个维度设定权重并结合带权Minkowski距离，将该距离作为KNN的距离度量；分析距离范数与权重对分类的影响，并进行可视化展示。

Result: 在多组实验中，改进的权重距离优于传统KNN，与相同实验设置相比提升稳定且显著；在基因表达数据集上的交叉验证中，分类准确率提升约10%，对不同k值均有收益。通过对最近邻区域的形状和尺度进行调控，解释性与鲁棒性均有所提升。

Conclusion: 以加权维度距离扩展KNN的适用性，形成对带权Minkowski距离的有效泛化；该方法在高维低样本场景（如基因表达数据）具有显著的分类性能提升和更合理的最近邻选择。

Abstract: Studies on various facets of pattern classification is often imperative while
working with multi-dimensional samples pertaining to diverse application
scenarios. In this notion, weighted dimension-based distance measure has been
one of the vital considerations in pattern analysis as it reflects the degree
of similarity between samples. Though it is often presumed to be settled with
the pervasive use of Euclidean distance, plethora of issues often surface. In
this paper, we present (a) a detail analysis on the impact of distance measure
norms and weights of dimensions along with visualization, (b) a novel weighting
scheme for each dimension, (c) incorporation of this dimensional weighting
schema into a KNN classifier, and (d) pattern classification on a variety of
synthetic as well as realistic datasets with the developed model. It has
performed well across diverse experiments in comparison to the traditional KNN
under the same experimental setups. Specifically, for gene expression datasets,
it yields significant and consistent gain in classification accuracy (around
10%) in all cross-validation experiments with different values of k. As such
datasets contain limited number of samples of high dimensions, meaningful
selection of nearest neighbours is desirable, and this requirement is
reasonably met by regulating the shape and size of the region enclosing the k
number of reference samples with the developed weighting schema and appropriate
norm. It, therefore, stands as an important generalization of KNN classifier
powered by weighted Minkowski distance with the present weighting schema.

</details>


### [21] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 通过一个完全解耦的原型学习框架解决自监督学习中的原型聚 collapse 问题：将原型建模为高斯混合模型并用在线EM更新，与编码器损失解耦，从而提升原型多样性和下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有的原型自监督方法易出现部分原型收敛到冗余表示的现象，源于编码器和原型的联合优化，导致捷径学习；现有对策（过度参数化或额外正则化）往往只缓解症状而非根本原因。

Method: 提出完全解耦的训练策略：在独立于编码器损失的前提下，约束原型学习。将原型建模为高斯混合模型并通过在线EM式过程更新，进行独立于编码器的目标优化。

Result: 在不显式正则化的情况下消除原型聚合，确保原型多样性并提升下游任务表现。

Conclusion: 解耦训练为原型式自监督提供一个简单而有力的解决方案，减少对原型过度参数化和耦合优化的需求，且具有潜在的广泛适用性与扩展空间。

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [22] [There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance](https://arxiv.org/abs/2510.20119)
*Arian Prabowo,Flora D. Salim*

Main category: cs.LG

TL;DR: TSFMs尚未超越轻量化基线，原因在于简单迁移NLP/CV流水线到时序数据。要实现真正的泛化与推理能力，需从第一性原理出发设计覆盖时序不变性的系统性数据集。


<details>
  <summary>Details</summary>
Motivation: 当前时序领域缺乏富含概念的海量数据集（如“苹果”等），单纯的抓取式数据聚合不能有效支持时序模型的泛化与推理。需要建立一个时序不变性本体论，确保表示空间覆盖关键不变性并保留时间语义，从而实现对时序数据的全面理解。

Method: 提出基于第一性原理的时序不变性本体设计，并据此构建系统化的数据集，覆盖广泛且互补的时序不变性，以保证表示的完整性和时间语义的保留；通过对齐数据集设计与模型学习，提升TSFMs的泛化、推理能力以及潜在的“ emergent”行为。

Result: 这是一个概念性、结构性论证，尚未给出大规模实验结果。若能以不变性覆盖为驱动进行数据集设计，理论上可提升TSFMs的表现并实现更强的泛化和推理能力。

Conclusion: 要实现TSFMs的泛化与真正的 emergent 行为，关键在于以第一性原理为基础的时序不变性覆盖的代表性数据集设计与本体驱动的模型对齐。

Abstract: Timeseries foundation models (TSFMs) have multiplied, yet lightweight
supervised baselines and even classical models often match them. We argue this
gap stems from the naive importation of NLP or CV pipelines. In language and
vision, large web-scale corpora densely capture human concepts i.e. there are
countless images and text of apples. In contrast, timeseries data is built to
complement the image and text modalities. There are no timeseries dataset that
contains the concept apple. As a result, the scrape-everything-online paradigm
fails for TS. We posit that progress demands a shift from opportunistic
aggregation to principled design: constructing datasets that systematically
span the space of invariance that preserve temporal semantics. To this end, we
suggest that the ontology of timeseries invariances should be built based on
first principles. Only by ensuring representational completeness through
invariance coverage can TSFMs achieve the aligned structure necessary for
generalisation, reasoning, and truly emergent behaviour.

</details>


### [23] [ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push](https://arxiv.org/abs/2510.20157)
*Xiaoming Wu,Teng Liu,Xin Wang,Ming Yang,Jiguo Yu*

Main category: cs.LG

TL;DR: 提出一种自适应差分隐私的去中心化学习方法 ADP-VRSGP，通过方差自适应和学习率自适应、渐进梯度融合、以及推送-聚合来提升隐私保护下的训练效率与收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决固定方差噪声在去中心化学习中对模型性能与训练效率的影响，提出能够自适应调整噪声方差与学习率并提供节点级个性化隐私保障的方法。

Method: 提出 ADP-VRSGP，结合步进衰减的噪声方差和学习率、基于历史梯度的渐进梯度融合、以及去中心化的 push-sum 与聚合以适应时变拓扑。理论分析给出在合适学习率下的鲁棒收敛性，实验显示优于基线。

Result: 理论上证明收敛性与稳定性提升，实验结果在多种场景下优于基线，展示了隐私保护下去中心化学习的有效性。

Conclusion: 该方法有效应对去中心化隐私学习的挑战，兼具收敛性与实证性能提升，适用于可变拓扑和节点级隐私需求。

Abstract: Differential privacy is widely employed in decentralized learning to
safeguard sensitive data by introducing noise into model updates. However,
existing approaches that use fixed-variance noise often degrade model
performance and reduce training efficiency. To address these limitations, we
propose a novel approach called decentralized learning with adaptive
differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).
This method dynamically adjusts both the noise variance and the learning rate
using a stepwise-decaying schedule, which accelerates training and enhances
final model performance while providing node-level personalized privacy
guarantees. To counteract the slowed convergence caused by large-variance noise
in early iterations, we introduce a progressive gradient fusion strategy that
leverages historical gradients. Furthermore, ADP-VRSGP incorporates
decentralized push-sum and aggregation techniques, making it particularly
suitable for time-varying communication topologies. Through rigorous
theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence
with an appropriate learning rate, significantly improving training stability
and speed. Experimental results validate that our method outperforms existing
baselines across multiple scenarios, highlighting its efficacy in addressing
the challenges of privacy-preserving decentralized learning.

</details>


### [24] [Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](https://arxiv.org/abs/2510.20468)
*Tomáš Souček,Sylvestre-Alvise Rebuffi,Pierre Fernandez,Nikola Jovanović,Hady Elsahar,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.LG

TL;DR: 提出一种对后置水印技术的水印伪造攻击与评估框架：通过学习偏好模型来判定水印，利用反向传播对单张水印图像进行输入优化以移除或伪造水印，跨多种模型验证，代码公开。


<details>
  <summary>Details</summary>
Motivation: 在数字内容日益增长、生成模型泛滥和法律压力增大的背景下，水印的真实性与可追溯性愈发重要。然而，现有研究多关注水印的鲁棒性与去除攻击，水印伪造（把他人水印窃取后用于恶意内容）仍研究不足。本工作旨在揭示后置水印系统的潜在安全性漏洞。

Method: 提出一个偏好模型，用排序损失在纯程序化生成的图像上进行训练，以判定图像是否被水印标记。该模型不需要真实水印数据。然后通过反向传播对输入图像进行优化，展示该模型能够在仅有一张水印图像的条件下，移除或伪造水印，且不需要对水印模型的知识。最后在多种后置水印模型上进行实验评估，验证攻击的有效性并对现有水印方法的安全性提出质疑。

Result: 实验结果显示，该方法能够在多种后置水印模型下成功伪造水印，证明当前水印方法的潜在脆弱性；并且所提出的偏好模型对识别水印的能力具有一定的泛化性。

Conclusion: 水印伪造在后置水印场景中具有现实威胁性，现有水印方案需加强对抗伪造的鲁棒性与安全性；未来工作应关注更强的安全性设计以及对抗性评估标准的建立。

Abstract: Recent years have seen a surge in interest in digital content watermarking
techniques, driven by the proliferation of generative models and increased
legal pressure. With an ever-growing percentage of AI-generated content
available online, watermarking plays an increasingly important role in ensuring
content authenticity and attribution at scale. There have been many works
assessing the robustness of watermarking to removal attacks, yet, watermark
forging, the scenario when a watermark is stolen from genuine content and
applied to malicious content, remains underexplored. In this work, we
investigate watermark forging in the context of widely used post-hoc image
watermarking. Our contributions are as follows. First, we introduce a
preference model to assess whether an image is watermarked. The model is
trained using a ranking loss on purely procedurally generated images without
any need for real watermarks. Second, we demonstrate the model's capability to
remove and forge watermarks by optimizing the input image through
backpropagation. This technique requires only a single watermarked image and
works without knowledge of the watermarking model, making our attack much
simpler and more practical than attacks introduced in related work. Third, we
evaluate our proposed method on a variety of post-hoc image watermarking
models, demonstrating that our approach can effectively forge watermarks,
questioning the security of current watermarking approaches. Our code and
further resources are publicly available.

</details>


### [25] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 通过在强化学习奖励中显式引入人类价值信号，RLEV 将 LLM 的优化直接对齐到可量化的人类偏好，扩展自 RLVR 的正确性奖励框架，提升在多种 RL 算法与模型规模上的表现，同时展示对高低价值任务自适应的终止策略与对噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的 LLM 对齐往往以正确性为主，但不同任务的重要性不同，单纯的正确性奖励无法区分任务的优先级。需要利用可量化的人类价值信号，将价值权重纳入奖励函数，以更好地对齐模型行为。

Method: 在可评估的考试风格数据上，使用包含 ground-truth 价值标签的样本，将价值信号直接引入奖励函数；在多种 RL 算法和模型规模上进行实验评估；分析端到端序列的价值权重梯度放大效应，进行消融验证以确认价值对齐的因果关系；并考察对难度等噪声价值信号的鲁棒性。

Result: 相较于仅依赖正确性奖励的基线，RLEV 在多种 RL 算法与模型尺度上表现更优；不仅提升了按价值加权的正确性（value-weighted accuracy），还学习了对高低价值提示的差异化终止策略（对低价值提示简洁、对高价值提示详细）；消融研究表明收益来自于价值对齐的因果关系；对难度标签等噪声信号仍具鲁棒性。

Conclusion: 将明确的人类价值函数直接纳入 RL 奖励，是连接人类优先级与 LLM 行为的务实路径，价值权重化的学习促进更符合人类偏好的对齐。

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [26] [Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents](https://arxiv.org/abs/2510.20199)
*Jane H. Lee,Baturay Saglam,Spyridon Pougkakiotis,Amin Karbasi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 提出一种基于优化确定性等价（OCE）的风险感知约束强化学习框架，结合强拉格朗日对偶，可包装于PPO等现有RL求解器，给出收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习多以期望奖励为优化目标，忽略尾部风险和灾难性事件，尤其在高风险场景需要对极端事件进行鲁棒或风险控制。

Method: 通过引入优化确定性等价物（OCE）来构造风险敏感的目标与约束，并在参数化的强拉格朗日对偶框架下证明与原始约束问题等价，在此基础上给出可直接嵌入现有RL求解器（如PPO）的算法流程，并在较常见的假设下证明收敛性。

Result: 给出理论上的等价性证明（在满足适当的约束资格条件时），以及一个可实现的算法框架，并通过若干数值实验验证所提框架在风险感知方面的性质。

Conclusion: 该工作提供了一个系统化且可实用的风险感知约束RL解决方案，具有理论保障和与现有RL工作流的良好兼容性，适用于需要对尾部风险进行控制的高风险应用。

Abstract: Constrained optimization provides a common framework for dealing with
conflicting objectives in reinforcement learning (RL). In most of these
settings, the objectives (and constraints) are expressed though the expected
accumulated reward. However, this formulation neglects risky or even possibly
catastrophic events at the tails of the reward distribution, and is often
insufficient for high-stakes applications in which the risk involved in
outliers is critical. In this work, we propose a framework for risk-aware
constrained RL, which exhibits per-stage robustness properties jointly in
reward values and time using optimized certainty equivalents (OCEs). Our
framework ensures an exact equivalent to the original constrained problem
within a parameterized strong Lagrangian duality framework under appropriate
constraint qualifications, and yields a simple algorithmic recipe which can be
wrapped around standard RL solvers, such as PPO. Lastly, we establish the
convergence of the proposed algorithm under common assumptions, and verify the
risk-aware properties of our approach through several numerical experiments.

</details>


### [27] [Approximate Replicability in Learning](https://arxiv.org/abs/2510.20200)
*Max Hopkins,Russell Impagliazzo,Christopher Ye*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion
that algorithms should remain stable under a resampling of their inputs (given
access to shared randomness). While a strong and interesting notion of
stability, the cost of replicability can be prohibitive: there is no replicable
algorithm, for instance, for tasks as simple as threshold learning (Bun et al.
STOC '23). Given such strong impossibility results we ask: under what
approximate notions of replicability is learning possible?
  In this work, we propose three natural relaxations of replicability in the
context of PAC learning: (1) Pointwise: the learner must be consistent on any
fixed input, but not across all inputs simultaneously, (2) Approximate: the
learner must output hypotheses that classify most of the distribution
consistently, (3) Semi: the algorithm is fully replicable, but may additionally
use shared unlabeled samples. In all three cases, for constant replicability
parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are
achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires
$\Theta(d^2/\alpha^2)$ labeled samples.

</details>


### [28] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: Moderate ability to rank cancer risk in dogs using routine lab data; but poor clinical utility for reliable diagnosis due to weak signal and confounding by aging/inflammation; performance ceiling identified; multi-modal data needed.


<details>
  <summary>Details</summary>
Motivation: Assess feasibility of cancer risk classification in dogs using inexpensive, routinely collected lab data under real-world conditions, including irrelevant cancer-type grouping and post-diagnosis samples.

Method: Systematic benchmark of 126 pipelines combining machine learning models, feature selection, and data balancing; patient-level data split to prevent leakage; final model: Logistic Regression with class weighting and recursive feature elimination; interpretability via SHAP.

Result: AUROC 0.815 (CI 0.793-0.836), F1 0.25, PPV 0.15, NPV 0.98, recall 0.79; high ranking ability but poor clinical classification performance; SHAP indicated non-specific drivers (age, inflammation, anemia).

Conclusion: Detectable cancer signal exists in routine labs but is too weak and confounded by aging and inflammatory conditions to rely on for discrimination; establishes a performance ceiling for this data modality in isolation and emphasizes need for multi-modal data integration.

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [29] [CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks](https://arxiv.org/abs/2510.20219)
*Ke Xing,Yanjie Dong,Xiaoyi Fan,Runhao Zeng,Victor C. M. Leung,M. Jamal Deen,Xiping Hu*

Main category: cs.LG

TL;DR: 提出了CO-PFL，一种基于贡献的联邦学习框架，通过梯度方向差异与预测偏差的双子空间分析来动态估计客户端贡献并对全局聚合进行加权，同时引入参数化个性化与掩码化动量优化以提升个性化和收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决数据异质性和本地数据稀缺问题下多客户端协同训练的挑战；传统联邦学习依赖单一共识模型，且以等贡献假设进行聚合，忽略各客户端更新的实际效用和可靠性，导致个性化和聚合偏差。

Method: 对每个客户端的贡献进行联合评估，通过分析梯度方向差异与预测偏差，结合梯度子空间和数据子空间的信息，来为全局聚合分配带权重；引入参数逐变量的个性化机制并结合掩码化的动量优化，以提升个性化适配和优化稳定性；通过构建定制化子模型并保持更新的稳定性来减轻聚合偏差，强化全局协调。

Result: 在四个基准数据集CIFAR10、CIFAR10C、CINIC10和Mini-ImageNet上的实验结果表明，CO-PFL在个性化准确性、鲁棒性、可扩展性和收敛稳定性方面持续超越现有最优方法。

Conclusion: CO-PFL通过贡献度的精细估算和双子空间分析，缓解聚合偏差，增强全局协同，并通过参数化个性化和掩码动量优化提升本地子模型的稳定更新与性能。

Abstract: Personalized federated learning (PFL) addresses a critical challenge of
collaboratively training customized models for clients with heterogeneous and
scarce local data. Conventional federated learning, which relies on a single
consensus model, proves inadequate under such data heterogeneity. Its standard
aggregation method of weighting client updates heuristically or by data volume,
operates under an equal-contribution assumption, failing to account for the
actual utility and reliability of each client's update. This often results in
suboptimal personalization and aggregation bias. To overcome these limitations,
we introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that
dynamically estimates each client's contribution for global aggregation. CO-PFL
performs a joint assessment by analyzing both gradient direction discrepancies
and prediction deviations, leveraging information from gradient and data
subspaces. This dual-subspace analysis provides a principled and discriminative
aggregation weight for each client, emphasizing high-quality updates.
Furthermore, to bolster personalization adaptability and optimization
stability, CO-PFL cohesively integrates a parameter-wise personalization
mechanism with mask-aware momentum optimization. Our approach effectively
mitigates aggregation bias, strengthens global coordination, and enhances local
performance by facilitating the construction of tailored submodels with stable
updates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C,
CINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses
state-of-the-art methods in in personalization accuracy, robustness,
scalability and convergence stability.

</details>


### [30] [QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models](https://arxiv.org/abs/2510.20222)
*Hao Wang,Baojun Ma*

Main category: cs.LG

TL;DR: 提出了QKCV注意力，通过在QKV框架中引入静态类别嵌入C来强调类别相关信息，作为插件模块提升多种注意力模型在真实时间序列数据集上的预测准确性；并提出仅更新静态嵌入C即可实现高效微调的方法，降低计算开销并获得更好的微调性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列通常包含类别信息，能够揭示数据的固有模式；传统的QKV注意力未充分利用静态类别信息，导致模型对类别特定模式的捕捉能力受限。通过引入静态类别嵌入并结合QKV，期望提升对类别相关模式的建模能力并实现高效微调。

Method: 提出Query-Key-Category-Value (QKCV) 注意力，将静态类别嵌入C并入Q/K/V计算中，使注意力计算能够直接利用类别信息；作为一个通用的插件模块应用于多种注意力驱动的时序模型（如 Vanilla Transformer、Informer、PatchTST、TFT）。同时提出在单变量时间序列的基础模型微调场景中仅更新静态嵌入C，其它权重保持不变，以降低计算开销并提升微调性能。

Result: 在多种真实世界数据集与多种模型上，QKCV实现了对注意力模型的预测准确性的提升，且在微调单变量时间序列基础模型时，显示出仅更新C即可获得更高效的微调效果和竞争力的性能表现。

Conclusion: QKCV为时序注意力模型提供了一种简单而高效的范式，通过引入静态类别信息来增强类别特征的建模能力，具备良好的通用性与可扩展性，且在微调阶段可显著降低计算开销。

Abstract: In real-world time series forecasting tasks, category information plays a
pivotal role in capturing inherent data patterns. This paper introduces QKCV
(Query-Key-Category-Value) attention, an extension of the traditional QKV
framework that incorporates a static categorical embedding C to emphasize
category-specific information. As a versatile plug-in module, QKCV enhances the
forecasting accuracy of attention-based models (e.g., Vanilla Transformer,
Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV
demonstrates remarkable adaptability in fine-tuning univariate time series
foundation model by solely updating the static embedding C while preserving
pretrained weights, thereby reducing computational overhead and achieving
superior fine-tuning performance.

</details>


### [31] [Sparse Local Implicit Image Function for sub-km Weather Downscaling](https://arxiv.org/abs/2510.20228)
*Yago del Valle Inclan Redondo,Enrique Arriaga-Varela,Dmitry Lyamzin,Pablo Cervantes,Tiago Ramalho*

Main category: cs.LG

TL;DR: SpLIIF uses implicit neural representations to enable arbitrary downscaling of weather variables. Trained on sparse stations and topography over Japan, it outperforms both an interpolation baseline and CorrDiff, with up to ~50% improvement for temperature and ~10–20% for wind.


<details>
  <summary>Details</summary>
Motivation: Accurately downscaling weather variables from sparse observations is challenging, especially across complex terrain such as Japan, and existing baselines (interpolation, CorrDiff) may not generalize well to in-distribution and out-of-distribution conditions. A flexible representation that supports arbitrary downscaling could improve high-resolution weather reconstructions.

Method: Train an implicit neural representation model (SpLIIF) using sparse weather stations and topography data over Japan. Evaluate temperature and wind downscaling accuracy in both in-distribution and out-of-distribution settings. Compare against an interpolation baseline and CorrDiff.

Result: The model achieves up to ~50% better accuracy than both CorrDiff and the baseline for temperature downscaling, and ~10–20% better for wind downscaling.

Conclusion: SpLIIF demonstrates that implicit neural representations can effectively enable arbitrary downscaling of weather variables from sparse observations, offering substantial gains over current baselines in both temperature and wind predictions over Japan.

Abstract: We introduce SpLIIF to generate implicit neural representations and enable
arbitrary downscaling of weather variables. We train a model from sparse
weather stations and topography over Japan and evaluate in- and
out-of-distribution accuracy predicting temperature and wind, comparing it to
both an interpolation baseline and CorrDiff. We find the model to be up to 50%
better than both CorrDiff and the baseline at downscaling temperature, and
around 10-20% better for wind.

</details>


### [32] [Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach](https://arxiv.org/abs/2510.20235)
*Woohyeon Byeon,Giseung Park,Jongseong Chae,Amir Leshem,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出了一种可证明收敛且实用的多目标强化学习框架，采用最大-最小准则。将最大-最小MORL建模为两人零和正则化的连续博弈，基于镜像下降算法实现策略更新，保证全局最后一次迭代收敛。给出在严格/近似策略评估下的迭代复杂度以及样本复杂度界，并通过自适应正则化进一步提升性能。在表格设置下实验验证收敛性，深度RL实现显著优于多数基线。


<details>
  <summary>Details</summary>
Motivation: 解决多目标强化学习中对最大-最小准则的收敛性与实用性难题，结合博弈论和镜像下降，提供可证明的全局收敛性与良好实践表现。

Method: 将最大-最小多目标强化学习表述为两人零和且带正则化的连续博弈；提出基于镜像下降的高效算法，简化策略更新并确保全局最后迭代收敛；对该算法给出理论分析，包括严格/近似策略评估下的迭代复杂度和样本复杂度界，并引入自适应正则化以提升性能。

Result: 给出对算法的全面理论分析，包含在精确和近似策略评估下的迭代复杂度与样本复杂度边界；证明全局最后迭代收敛；在表格设置下验证收敛性，在深度强化学习上实现显著优于现有基线的表现。

Conclusion: 该工作提供了一个可证收敛且具有实用性的MORL框架，通过将问题映射到正则化的连续博弈并采用镜像下降实现，理论与实验均显示良好收敛性与性能提升，并且自适应正则化进一步增强效果。

Abstract: In this paper, we propose a provably convergent and practical framework for
multi-objective reinforcement learning with max-min criterion. From a
game-theoretic perspective, we reformulate max-min multi-objective
reinforcement learning as a two-player zero-sum regularized continuous game and
introduce an efficient algorithm based on mirror descent. Our approach
simplifies the policy update while ensuring global last-iterate convergence. We
provide a comprehensive theoretical analysis on our algorithm, including
iteration complexity under both exact and approximate policy evaluations, as
well as sample complexity bounds. To further enhance performance, we modify the
proposed algorithm with adaptive regularization. Our experiments demonstrate
the convergence behavior of the proposed algorithm in tabular settings, and our
implementation for deep reinforcement learning significantly outperforms
previous baselines in many MORL environments.

</details>


### [33] [Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction](https://arxiv.org/abs/2510.20236)
*Teng Jiek See,Daokun Zhang,Mario Boley,David K. Chalmers*

Main category: cs.LG

TL;DR: 提出了一种层间知识混合(LKM)的自监督蒸馏方法，通过最小化GNN各层隐藏嵌入的均值绝对差来综合多跳、多尺度信息，显著提升分子属性预测的准确性且计算开销很小。


<details>
  <summary>Details</summary>
Motivation: 在分子性质预测中，GNN的准确性提升往往伴随模型复杂度和计算成本的增加。本研究希望通过高效的蒸馏方式提升现有GNN的性能，且对训练和推理的额外成本几乎可忽略。

Method: 在GNN的层之间对隐藏表示进行自监督对齐，最小化相邻层之间隐藏嵌入的平均绝对距离，以实现多跳与多尺度信息的高效聚合。\n实验使用三种GNN结构(DimeNet++, MXMNet, PAMNet)以及QM9、MD17、Chignolin等数据集进行评估。

Result: LKM显著降低了预测误差：QM9上MAE降低至多达9.8%，MD17能源预测下降约45.3%，Chignolin下降约22.9%。在保持现有模型架构的前提下，计算成本几乎无增加。

Conclusion: LKM是一种高效、可泛化的自知识蒸馏方法，能够通过层间嵌入对齐提升GNN在化学性质预测任务上的准确性，同时避免显著的计算负担。

Abstract: Graph Neural Networks (GNNs) are the currently most effective methods for
predicting molecular properties but there remains a need for more accurate
models. GNN accuracy can be improved by increasing the model complexity but
this also increases the computational cost and memory requirement during
training and inference. In this study, we develop Layer-to-Layer Knowledge
Mixing (LKM), a novel self-knowledge distillation method that increases the
accuracy of state-of-the-art GNNs while adding negligible computational
complexity during training and inference. By minimizing the mean absolute
distance between pre-existing hidden embeddings of GNN layers, LKM efficiently
aggregates multi-hop and multi-scale information, enabling improved
representation of both local and global molecular features. We evaluated LKM
using three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using
datasets of quantum chemical properties (QM9, MD17 and Chignolin). We found
that the LKM method effectively reduces the mean absolute error of quantum
chemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17
Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to
significantly improve the accuracy of GNNs for chemical property prediction
without any substantial increase in training and inference cost.

</details>


### [34] [What Does It Take to Build a Performant Selective Classifier?](https://arxiv.org/abs/2510.20242)
*Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: 提出一个有限样本下的选择性分类差距的五源分解，揭示单调后校准对缩小差距影响有限，强调需要能重新排序预测的评分机制与分布鲁棒训练。通过两种模拟数据和实际视觉/语言数据进行验证，给出可操作的设计指南。


<details>
  <summary>Details</summary>
Motivation: 在选择性分类中，模型在不确定输入上拒绝，以提高可靠性。但很少有方法达到理想的“正确性顺序”阈值（完美有序 oracle）的水平。

Method: 给出一个有限样本的五源分解：贝叶斯噪声、近似误差、排序误差、统计噪声、实现或分布偏移引发的 slack。并通过理论分析和受控实验对分解进行验证。

Result: 揭示单调后校准对缩小 gap 的影响有限，真正能改善 gap 的是能有效重新排序预测的评分机制；数据分布偏移引入额外的 slack；提供一个可量化的错误预算与设计指南。

Conclusion: 要更接近理想 oracle，须采用能重新排序的评分机制、具备特征感知的校准方法，以及分布鲁棒的训练，才能在现实场景中收敛到更优的选择性分类。

Abstract: Selective classifiers improve model reliability by abstaining on inputs the
model deems uncertain. However, few practical approaches achieve the
gold-standard performance of a perfect-ordering oracle that accepts examples
exactly in order of correctness. Our work formalizes this shortfall as the
selective-classification gap and present the first finite-sample decomposition
of this gap to five distinct sources of looseness: Bayes noise, approximation
error, ranking error, statistical noise, and implementation- or shift-induced
slack. Crucially, our analysis reveals that monotone post-hoc calibration --
often believed to strengthen selective classifiers -- has limited impact on
closing this gap, since it rarely alters the model's underlying score ranking.
Bridging the gap therefore requires scoring mechanisms that can effectively
reorder predictions rather than merely rescale them. We validate our
decomposition on synthetic two-moons data and on real-world vision and language
benchmarks, isolating each error component through controlled experiments. Our
results confirm that (i) Bayes noise and limited model capacity can account for
substantial gaps, (ii) only richer, feature-aware calibrators meaningfully
improve score ordering, and (iii) data shift introduces a separate slack that
demands distributionally robust training. Together, our decomposition yields a
quantitative error budget as well as actionable design guidelines that
practitioners can use to build selective classifiers which approximate ideal
oracle behavior more closely.

</details>


### [35] [Optimistic Task Inference for Behavior Foundation Models](https://arxiv.org/abs/2510.20264)
*Thomas Rupf,Marco Bagatella,Marin Vlastelica,Andreas Krause*

Main category: cs.LG

TL;DR: 提出 OpTI-BFM，一种在测试时通过与环境交互进行任务推断的乐观性决策准则，直接对奖励函数的不确定性建模并指导数据收集。与线性带宽的上置信界相关的遗憾界给出理论保障；在零-shot 基准上实验显示该方法可在少量回合内识别并优化未见的奖励函数，且计算开销低，代码开源。


<details>
  <summary>Details</summary>
Motivation: BFMs 在零-shot 强化学习中需要根据奖励函数推导或标注数据，导致数据效率低下。为减少对大规模奖励数据的依赖，需在测试时通过环境交互进行任务推断。

Method: 提出 OpTI-BFM，通过对奖励函数的不确定性进行乐观建模，驱动BFMs在数据采集阶段进行高信息量的探索。与线性带宾带上置信界的理论框架建立联系，从而给出遗憾界。具体验证依赖于先验线性特征和 BFMs 的后验更新。

Result: 在既定的零-shot 基准上评估，OpTI-BFM 能使基于 BFMs 的策略在少量episodes内识别并优化未见的奖励函数，且计算开销显著降低，表现可与传统依赖奖励数据的方式媲美。

Conclusion: 该方法为 BFMs 的任务推断提供一种数据高效的解决方案，具有理论保证并且可扩展性良好。提供了开源实现以便复现与进一步研究。

Abstract: Behavior Foundation Models (BFMs) are capable of retrieving high-performing
policy for any reward function specified directly at test-time, commonly
referred to as zero-shot reinforcement learning (RL). While this is a very
efficient process in terms of compute, it can be less so in terms of data: as a
standard assumption, BFMs require computing rewards over a non-negligible
inference dataset, assuming either access to a functional form of rewards, or
significant labeling efforts. To alleviate these limitations, we tackle the
problem of task inference purely through interaction with the environment at
test-time. We propose OpTI-BFM, an optimistic decision criterion that directly
models uncertainty over reward functions and guides BFMs in data collection for
task inference. Formally, we provide a regret bound for well-trained BFMs
through a direct connection to upper-confidence algorithms for linear bandits.
Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and
observe that it enables successor-features-based BFMs to identify and optimize
an unseen reward function in a handful of episodes with minimal compute
overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.

</details>


### [36] [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271)
*Udit Saxena*

Main category: cs.LG

TL;DR: 提出高效可微的 Euler Characteristic Curve（ECC）实现及其在深度学习中的端到端应用，通过 Ampere GPU 优化内核和 PyTorch differentiable 层实现显著加速和可微性。


<details>
  <summary>Details</summary>
Motivation: 拓扑特征能够捕捉全局几何结构，但在深度学习中的广泛应用受限于计算效率和可微性；需要一个高效、可微的 ECC 框架以便端到端训练。

Method: 开发面向 Ampere 的 CUDA 内核，利用 128 字节对齐访问和分层共享内存累加实现 ECC 计算的高吞吐；实现一个可微的 PyTorch 层，通过对阈值进行单向可微学习的 Differentiable Euler Characteristic Transform 风格的 sigmoid 松弛；提出批处理和多 GPU 的扩展方案。

Result: 在合成网格上相比以往 GPU 实现，取得 16-2000× 的加速；实现了可微的端到端学习路径并提供可扩展的多 GPU/批处理方案。

Conclusion: 该工作为将 ECC 应用于深度学习提供高效、可微的实现与端到端训练能力，并给出未来在应用广度（如下游任务和多 GPU 扩展）方面的扩展路径。

Abstract: Topological features capture global geometric structure in imaging data, but
practical adoption in deep learning requires both computational efficiency and
differentiability. We present optimized GPU kernels for the Euler
Characteristic Curve (ECC) computation achieving 16-2000\"O speedups over prior
GPU implementations on synthetic grids, and introduce a differentiable PyTorch
layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs
use 128B-coalesced access and hierarchical shared-memory accumulation. Our
PyTorch layer learns thresholds in a single direction via a Differentiable
Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream
relevance, including applications highlighted by prior ECC work, and outline
batching/multi-GPU extensions to broaden adoption.

</details>


### [37] [Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2510.20272)
*Tristan Cinquin,Geoff Pleiss,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: PRM-guided tree search for mathematical reasoning in LLMs does not outperform Best-of-N; PRMs are unreliable predictors of state values, degrade with depth, and generalize poorly, making tree search ineffective without better reward modeling. Monte Carlo tree search and beam search fare better among alternatives.


<details>
  <summary>Details</summary>
Motivation: To determine whether process reward model (PRM) guided tree search can improve mathematical reasoning over the standard chain-of-thought with Best-of-N in LLMs, and to understand the reliability and generalization of PRMs in guiding search.

Method: An adaptive algorithm to maximize PRM scores over an intractable action space; applied PRM-guided tree search to 23 diverse mathematical problems using Qwen2.5-Math-7B-Instruct and its associated PRM; comparisons made against Best-of-N (BoN), Monte Carlo tree search (MCTS), and beam search.

Result: (PRM-guided tree search) showed no statistically significant improvements over BoN despite higher computational costs; MCTS and beam search outperform other PRM-guided methods; PRMs poorly approximate state values with reliability degrading as reasoning depth increases; PRMs generalize poorly out-of-distribution.

Conclusion: The underperformance is attributed to the greater reliance on unreliable PRM scores in tree search; implies that different reward modeling is needed before tree search can effectively enhance mathematical reasoning in LLMs.

Abstract: While chain-of-thought prompting with Best-of-N (BoN) selection has become
popular for mathematical reasoning in large language models (LLMs), its linear
structure fails to capture the branching and exploratory nature of complex
problem-solving. In this work, we propose an adaptive algorithm to maximize
process reward model (PRM) scores over the intractable action space, and
investigate whether PRM-guided tree search can improve mathematical reasoning
by exploring multiple partial solution paths. Across $23$ diverse mathematical
problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case
study, we find that: (1) PRM-guided tree search shows no statistically
significant improvements over BoN despite higher costs, (2) Monte Carlo tree
search and beam search outperform other PRM-guided tree search methods, (3)
PRMs poorly approximate state values and their reliability degrades with
reasoning depth, and (4) PRMs generalize poorly out of distribution. This
underperformance stems from tree search's greater reliance on unreliable PRM
scores, suggesting different reward modeling is necessary before tree search
can effectively enhance mathematical reasoning in LLMs.

</details>


### [38] [SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series](https://arxiv.org/abs/2510.20273)
*Qitai Tan,Yiyun Chen,Mo Li,Ruiwen Gu,Yilin Su,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出一个名为 SynTSBench 的合成数据驱动评估框架，用以系统地评估时间序列预测模型的基本建模能力，尤其在特征分解、鲁棒性和与理论最优基线的对比等三大维度上。实验表明当前深度学习模型并未在所有时间特征类型上普遍接近最优基线。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在时间序列预测中的黑箱问题与现有评估框架的不足，提供可解释的、可控的评估体系，帮助理解不同模型在不同预测场景的强弱与适用性。

Method: 提出可编程特征配置的合成数据驱动评估范式 SynTSBench，分离混淆因素，建立包含三个核心分析维度的评估系统：1) 时间特征分解与能力映射，评估模型对各类模式的学习能力；2) 数据异常下的鲁棒性分析，量化噪声容忍与异常恢复能力；3) 理论最优基线对比，确定各模式类型的性能边界并与理论最优进行直接对比。

Result: 实验结果显示，当前的深度学习模型并未在所有类型的时间特征上普遍达到最优基线。

Conclusion: 提供了一个可解释且可对比的评估框架，有助于理解和比较不同时序预测模型在多种模式下的表现差异，代码已开源。

Abstract: Recent advances in deep learning have driven rapid progress in time series
forecasting, yet many state-of-the-art models continue to struggle with robust
performance in real-world applications, even when they achieve strong results
on standard benchmark datasets. This persistent gap can be attributed to the
black-box nature of deep learning architectures and the inherent limitations of
current evaluation frameworks, which frequently lack the capacity to provide
clear, quantitative insights into the specific strengths and weaknesses of
different models, thereby complicating the selection of appropriate models for
particular forecasting scenarios. To address these issues, we propose a
synthetic data-driven evaluation paradigm, SynTSBench, that systematically
assesses fundamental modeling capabilities of time series forecasting models
through programmable feature configuration. Our framework isolates confounding
factors and establishes an interpretable evaluation system with three core
analytical dimensions: (1) temporal feature decomposition and capability
mapping, which enables systematic evaluation of model capacities to learn
specific pattern types; (2) robustness analysis under data irregularities,
which quantifies noise tolerance thresholds and anomaly recovery capabilities;
and (3) theoretical optimum benchmarking, which establishes performance
boundaries for each pattern type-enabling direct comparison between model
predictions and mathematical optima. Our experiments show that current deep
learning models do not universally approach optimal baselines across all types
of temporal features.The code is available at
https://github.com/TanQitai/SynTSBench

</details>


### [39] [ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows](https://arxiv.org/abs/2510.20279)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Ziheng Qin,Bangyuan Zhu,Shengbin Huang,Xuanlei Zhao,Panpan Zhang,Xiaojiang Peng,Yuzhang Shang,Jianfei Yang,Zheng Zhu,Tianlong Chen,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: 构建了 CS-54k：一个源自14k篇CC-许可论文的高质量科学问答语料库，用于评估和训练AI在科学研究中的端到端工作流，并衍生出 CS-4k（评估基准）与 CS-50k（训练数据集）。实验显示域数据质量和对齐训练比单纯扩大模型规模更关键，开源模型在 CS-50k 上经监督和强化学习训练后表现显著提升，甚至7B规模模型在合适训练下可超越部分大模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在科学研究中的潜在应用日益扩大，需要端到端的评估基准来衡量AI对完整科学研究流程的协作能力，而非孤立子任务。为此，构建一个高质量、可扩展的科学问答语料库及相关基准，以支持训练与评估AI研究助手。

Method: 通过一个可扩展的“以论文为基础”的管线，结合检索增强生成（RAG）与多阶段质量控制，构建CS-54k。由此派生出两类子集：CS-4k用于评估AI在科学研究中的协作能力，CS-50k用于大规模训练数据集。对使用CS-50k进行有监督训练和强化学习的开源模型进行广泛实验，比较与更大且封闭的系统。

Result: CS-4k能够将现有SOTA大语言模型分层为不同的能力等级；基于CS-50k进行有监督和强化学习训练的开源模型表现显著提升；甚至在7B规模下，若训练得当，能够超越GPT-4.1、GPT-4o、Gemini 2.5 Pro等更大模型。

Conclusion: 高质量的领域对齐数据和训练，对提升研究助手能力比单纯扩大预训练规模更为关键。研究者因此发布CS-4k和CS-50k，期望推动AI系统成为CS研究中的可靠协作者。

Abstract: As large language models (LLMs) advance, the ultimate vision for their role
in science is emerging: we could build an AI collaborator to effectively assist
human beings throughout the entire scientific research process. We refer to
this envisioned system as ResearchGPT. Given that scientific research
progresses through multiple interdependent phases, achieving this vision
requires rigorous benchmarks that evaluate the end-to-end workflow rather than
isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of
scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It
is constructed through a scalable, paper-grounded pipeline that combines
retrieval-augmented generation (RAG) with multi-stage quality control to ensure
factual grounding. From this unified corpus, we derive two complementary
subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to
assist scientific research, and CS-50k, a large-scale training dataset.
Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs
into distinct capability tiers. Open models trained on CS-50k with supervised
training and reinforcement learning demonstrate substantial improvements. Even
7B-scale models, when properly trained, outperform many larger proprietary
systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that
making AI models better research assistants relies more on domain-aligned
training with high-quality data than on pretraining scale or general benchmark
performance. We release CS-4k and CS-50k in the hope of fostering AI systems as
reliable collaborators in CS research.

</details>


### [40] [InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling](https://arxiv.org/abs/2510.20302)
*Yuhang Wang*

Main category: cs.LG

TL;DR: 提出了一种混合架构 InvDec，将时序编码与变量级解码进行 principled 分离，通过 patch-based 时序编码器、变量维度反转解码器、延迟变量嵌入和自适应残差融合来实现高维多变量时间序列预测的高效跨变量建模。在高维数据集上显示显著提升，同时在低维数据集保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列预测需同时建模时间模式和跨变量依赖。通道独立方法（如 PatchTST）擅长时序建模但忽略变量相关性；纯变量注意力方法（如 iTransformer）牺牲了时序编码。需要在时序编码与变量解码之间实现有原则的分离以兼顾两端。

Method: 提出 InvDec（反向解码器）：在时序编码端采用 patch 基的编码器，在变量维度上采用“反向/倒置”的解码器进行变体自注意力；引入延迟变量嵌入，在时序编码后再丰富变量特征；设计自适应残差融合机制，在不同维度数据集上动态平衡时序与变异信息。以 PatchTST 为骨架构成 InvDec-PatchTST。

Result: 在七个基准数据集上的广泛实验证明对高维数据集有显著提升：与 PatchTST 相比，在 Electricity（321 个变量）上实现 20.9% 的 MSE 降幅，在 Weather 上实现 4.3% 的提升，在 Traffic 上实现 2.7% 的收益；在低维 ETT 数据集仍保持具有竞争力。消融研究验证了各组件的有效性，分析显示 InvDec 的优势随数据维度的增加而增强，证明跨变量建模在变量数量增多时变得关键。

Conclusion: 通过对时序编码和变量解码的分离，InvDec 能在高维多变量时间序列预测中实现对跨变量关系的有效建模，并且对数据维度的提升具有更显著的收益，具备较强的泛化潜力。

Abstract: Multivariate time series forecasting requires simultaneously modeling
temporal patterns and cross-variate dependencies. Channel-independent methods
such as PatchTST excel at temporal modeling but ignore variable correlations,
while pure variate-attention approaches such as iTransformer sacrifice temporal
encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that
achieves principled separation between temporal encoding and variate-level
decoding. InvDec combines a patch-based temporal encoder with an inverted
decoder operating on the variate dimension through variate-wise self-attention.
We introduce delayed variate embeddings that enrich variable-specific
representations only after temporal encoding, preserving temporal feature
integrity. An adaptive residual fusion mechanism dynamically balances temporal
and variate information across datasets of varying dimensions. Instantiating
InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven
benchmarks demonstrate significant gains on high-dimensional datasets: 20.9%
MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and
2.7% gain on Traffic compared to PatchTST, while maintaining competitive
performance on low-dimensional ETT datasets. Ablation studies validate each
component, and analysis reveals that InvDec's advantage grows with dataset
dimensionality, confirming that cross-variate modeling becomes critical as the
number of variables increases.

</details>


### [41] [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)
*Fengyuan Yu,Yuyuan Li,Xiaohua Feng,Junjie Fang,Tao Wang,Chaochao Chen*

Main category: cs.LG

TL;DR: 提出 LEGO 框架，用 Embedding Calibration 与 Flexible Combination 实现多属性未学习，基于互信息最小化提供同时未学习的理论保证，并具备并行化与动态适应能力；在三个真实数据集的三种推荐模型上证实有效性与效率。


<details>
  <summary>Details</summary>
Motivation: 现实场景中隐私保护需求涉及多项敏感属性且呈动态变化，单属性未学习无法满足多属性及动态需求，需高效并发处理与快速适应。

Method: 将多属性未学习分为两步：1) Embedding Calibration，去除用户嵌入中与目标属性相关的信息；2) Flexible Combination，将各属性的未学习嵌入组合成一个统一嵌入，保护所有敏感属性。以互信息最小化作为理论框架，确保对多属性的同时未学习；Embedding Calibration 可并行执行，Flexible Combination 具备灵活性与高效性。

Result: 在三个真实数据集上对三种推荐模型进行了广泛实验，证实了 LEGO 在准确性与效率上的平衡，并提供了开源代码和附录以便复现。

Conclusion: LEGO 能实现多属性同时未学习，适应动态未学习需求，具有理论保证与实证支持，且具备良好的扩展性与实用性。

Abstract: With the growing demand for safeguarding sensitive user information in
recommender systems, recommendation attribute unlearning is receiving
increasing attention. Existing studies predominantly focus on single-attribute
unlearning. However, privacy protection requirements in the real world often
involve multiple sensitive attributes and are dynamic. Existing
single-attribute unlearning methods cannot meet these real-world requirements
due to i) CH1: the inability to handle multiple unlearning requests
simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic
unlearning needs. To address these challenges, we propose LEGO, a lightweight
and efficient multiple-attribute unlearning framework. Specifically, we divide
the multiple-attribute unlearning process into two steps: i) Embedding
Calibration removes information related to a specific attribute from user
embedding, and ii) Flexible Combination combines these embeddings into a single
embedding, protecting all sensitive attributes. We frame the unlearning process
as a mutual information minimization problem, providing LEGO a theoretical
guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step
framework, where Embedding Calibration can be performed in parallel and
Flexible Combination is flexible and efficient, we address CH2. Extensive
experiments on three real-world datasets across three representative
recommendation models demonstrate the effectiveness and efficiency of our
proposed framework. Our code and appendix are available at
https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.

</details>


### [42] [Synthetic Data for Robust Runway Detection](https://arxiv.org/abs/2510.20349)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Fabrice Jimenez,Thomas Oberlin*

Main category: cs.LG

TL;DR: 通过商用飞行仿真器生成合成数据并与少量真实标注数据相结合，训练跑道检测的目标检测模型，并在夜间等未见条件下通过定制域自适应提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在工业/关键应用中，标注成本高且需覆盖罕见场景，合成数据可低成本地扩展训练条件与环境，同时缓解风控对真实数据的依赖。

Method: 利用商业飞行仿真器生成受控的合成跑道图像，结合少量真实标注数据，设计数据融合策略；在夜间等极端条件下引入定制的域自适应方法以缓解合成-真实分布偏移。

Result: 在合成数据+少量真实数据的条件下，标准目标检测模型实现较高精度；通过定制域自适应，增强模型对夜间等未在真实数据中充分覆盖场景的鲁棒性。

Conclusion: 合成数据与有限真实数据的联合训练对关键应用是可行且有效，但需针对具体任务设计恰当的域自适应策略与仿真场景的真实性提升。

Abstract: Deep vision models are now mature enough to be integrated in industrial and
possibly critical applications such as autonomous navigation. Yet, data
collection and labeling to train such models requires too much efforts and
costs for a single company or product. This drawback is more significant in
critical applications, where training data must include all possible conditions
including rare scenarios. In this perspective, generating synthetic images is
an appealing solution, since it allows a cheap yet reliable covering of all the
conditions and environments, if the impact of the synthetic-to-real
distribution shift is mitigated. In this article, we consider the case of
runway detection that is a critical part in autonomous landing systems
developed by aircraft manufacturers. We propose an image generation approach
based on a commercial flight simulator that complements a few annotated real
images. By controlling the image generation and the integration of real and
synthetic data, we show that standard object detection models can achieve
accurate prediction. We also evaluate their robustness with respect to adverse
conditions, in our case nighttime images, that were not represented in the real
data, and show the interest of using a customized domain adaptation strategy.

</details>


### [43] [Ask a Strong LLM Judge when Your Reward Model is Uncertain](https://arxiv.org/abs/2510.20369)
*Zhenghao Xu,Qin Lu,Qingru Zhang,Liang Qiu,Ilgee Hong,Changlong Yu,Wenlin Yao,Yao Liu,Haoming Jiang,Lihong Li,Hyokun Yun,Tuo Zhao*

Main category: cs.LG

TL;DR: 提出一种基于不确定性路由的混合奖励模型与强大但代价高的LLM评审框架，用以高效进行在线RLHF。通过将优势估计的策略梯度转化为成对偏好分类，进行不确定性量化，若不确定则交由LLM评审，确定则优先使用快速RM，从而提高性价比与对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统奖励模型在对人类偏好训练时易受奖励劫持、对OOD泛化差的问题，以及强大LLM评审在在线RLHF中高推理成本的限制；寻求在成本不显著增加的前提下，充分利用强大判断能力的同时保持效率。

Method: 将PG的优势估计形式化为成对偏好分类，并引入不确定性量化机制。对每对样本计算不确定性，分路路由：不确定样本送往成本高但精度高的LLM评审；确定样本由快速RM评估。实验通过RM基准和在线RLHF下游对齐任务验证方法在成本相同的情况下优于随机分配评审的策略。

Result: 不确定性路由策略在同等成本下显著优于随机调用评审的基线，并在上线RLHF的对齐任务中提升性能。

Conclusion: 通过不确定性驱动的路由实现了快速RM与强大LLM评审在在线RLHF中的高效协同，提升对齐质量的同时控制推理成本。

Abstract: Reward model (RM) plays a pivotal role in reinforcement learning with human
feedback (RLHF) for aligning large language models (LLMs). However, classical
RMs trained on human preferences are vulnerable to reward hacking and
generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM
judges equipped with reasoning capabilities demonstrate superior
generalization, even without additional training, but incur significantly
higher inference costs, limiting their applicability in online RLHF. In this
work, we propose an uncertainty-based routing framework that efficiently
complements a fast RM with a strong but costly LLM judge. Our approach
formulates advantage estimation in policy gradient (PG) methods as pairwise
preference classification, enabling principled uncertainty quantification to
guide routing. Uncertain pairs are forwarded to the LLM judge, while confident
ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our
uncertainty-based routing strategy significantly outperforms random judge
calling at the same cost, and downstream alignment results showcase its
effectiveness in improving online RLHF.

</details>


### [44] [Hierarchical Time Series Forecasting with Robust Reconciliation](https://arxiv.org/abs/2510.20383)
*Shuhei Aikawa,Aru Suzuki,Kei Yoshitake,Kanata Teshigawara,Akira Iwabuchi,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: 对分层时间序列的鲁棒一致性预测：在估计协方差矩阵存在不确定性的情况下，提出以最大化期望二次误差的最小化为目标的鲁棒对齐框架，转化为半正定规划（SDP），实验显示优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 分层时间序列预测需要一致性（父节点等于子节点之和）。现有方法基于逐序列基预测再进行对齐，依赖协方差矩阵估计；由于样本有限，真实协方差未知而估计误差会削弱预测性能。因此，需要考虑协方差估计不确定性的鲁棒对齐方案。

Method: 引入协方差矩阵的一个不确定性集合，构造一个以该集合为参数的目标函数，最小化在该集合上的最坏情形期望二次误差。问题可等价转化为半正定规划（SDP），以求解鲁棒对齐矩阵。

Result: 提出了一个可解的鲁棒对齐（reconciliation）框架，能够在协方差估计不确定性下实现更好的预测性能。数值实验表明，与现有的分层预测的对齐方法相比，鲁棒方法在预测精度上更优。

Conclusion: 将不确定性直接融入分层对齐过程，能提升预测的准确性和一致性；提出的 SDP 可行解法为鲁棒分层预测提供了有效工具。

Abstract: This paper focuses on forecasting hierarchical time-series data, where each
higher-level observation equals the sum of its corresponding lower-level time
series. In such contexts, the forecast values should be coherent, meaning that
the forecast value of each parent series exactly matches the sum of the
forecast values of its child series. Existing hierarchical forecasting methods
typically generate base forecasts independently for each series and then apply
a reconciliation procedure to adjust them so that the resulting forecast values
are coherent across the hierarchy. These methods generally derive an optimal
reconciliation, using a covariance matrix of the forecast error. In practice,
however, the true covariance matrix is unknown and has to be estimated from
finite samples in advance. This gap between the true and estimated covariance
matrix may degrade forecast performance. To address this issue, we propose a
robust optimization framework for hierarchical reconciliation that accounts for
uncertainty in the estimated covariance matrix. We first introduce an
uncertainty set for the estimated covariance matrix and formulate a
reconciliation problem that minimizes the worst-case expected squared error
over this uncertainty set. We show that our problem can be cast as a
semidefinite optimization problem. Numerical experiments demonstrate that the
proposed robust reconciliation method achieved better forecast performance than
existing hierarchical forecasting methods, which indicates the effectiveness of
integrating uncertainty into the reconciliation process.

</details>


### [45] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出相对基准概率（RBP）作为衡量语言模型排序能力的度量，建立相对基准 Scaling Law，补充交叉熵视角，验证在四个数据集、四个模型家族上覆盖五个数量级规模。


<details>
  <summary>Details</summary>
Motivation: 交叉熵只评估正确token的绝对概率，忽略正确与错误token之间的相对排序，这在贪心采样等场景尤为重要，因此需要从相对排序角度研究规模化规律。

Method: 提出 RBP 指标，衡量正确token在顶级预测中的排序概率；基于该指标提出 Relative-Based Scaling Law；在四个数据集、四个模型家族、覆盖五个数量级规模的实验进行验证。

Result: RBP 随模型规模显著提升，出现可重复的缩放规律；与交叉熵视角互补，支持对 emergent phenomena 的解释和建立基础理论的可能性。

Conclusion: Relative-Based Scaling Law 为大语言模型的规模效应提供新视角，补充交叉熵视角，具有实践和理论价值，可以帮助实践开发和理论探索。

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [46] [Why DPO is a Misspecified Estimator and How to Fix It](https://arxiv.org/abs/2510.20413)
*Aditya Gopalan,Sayak Ray Chowdhury,Debangshu Banerjee*

Main category: cs.LG

TL;DR: DPO is a supervised direct-alignment method that can be misspecified when the target reward cannot be represented by the policy class, leading to adverse effects; AuxDPO adds auxiliary variables to bridge toward RLHF and mitigates misspecification, with empirical gains in bandit and LLM alignment tasks.


<details>
  <summary>Details</summary>
Motivation: Understand the limitations of direct preference optimization (DPO) relative to two-stage RLHF, and address misspecification when the true reward function lies outside the parametric policy class.

Method: Provide a theoretical and geometric analysis of DPO as an estimation problem over reward functions induced by a policy class; relate two-stage RLHF to natural-gradient steps in policy space; introduce AuxDPO by adding auxiliary variables to the DPO loss to steer optimization toward RLHF solutions; validate with experiments on didactic bandit settings and LLM alignment tasks.

Result: Identified misspecification issues in DPO when the true reward cannot be realized by the policy class, including preference order reversal and sensitivity to data distribution; proposed AuxDPO to mitigate these issues; demonstrated improved performance of AuxDPO in didactic bandits and LLM alignment tasks.

Conclusion: Direct optimization methods like DPO can suffer from misspecification; incorporating auxiliary variables via AuxDPO provides a principled path toward RLHF-like solutions and yields empirical gains in alignment settings.

Abstract: Direct alignment algorithms such as Direct Preference Optimization (DPO)
fine-tune models based on preference data, using only supervised learning
instead of two-stage reinforcement learning with human feedback (RLHF). We show
that DPO encodes a statistical estimation problem over reward functions induced
by a parametric policy class. When the true reward function that generates
preferences cannot be realized via the policy class, DPO becomes misspecified,
resulting in failure modes such as preference order reversal, worsening of
policy reward, and high sensitivity to the input preference data distribution.
On the other hand, we study the local behavior of two-stage RLHF for a
parametric class and relate it to a natural gradient step in policy space. Our
fine-grained geometric characterization allows us to propose AuxDPO, which
introduces additional auxiliary variables in the DPO loss function to help move
towards the RLHF solution in a principled manner and mitigate the
misspecification in DPO. We empirically demonstrate the superior performance of
AuxDPO on didactic bandit settings as well as LLM alignment tasks.

</details>


### [47] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: Addresses imbalanced mark distributions in marked temporal point processes (MTPP) by learning thresholds to adjust mark probabilities with respect to priors, predicting the mark first then the time; introduces a neural MTPP model enabling efficient time sampling and probability estimation without expensive numerical integration; shows superior performance over baselines on real datasets; code released.


<details>
  <summary>Details</summary>
Motivation: In real-world MTPP applications, the distribution of event marks is highly imbalanced, causing poor prediction performance for rare marks. A method that directly predicts marks from their probabilities tends to neglect this imbalance. The motivation is to improve next-event mark prediction, especially for rare marks, by reweighting or thresholding the probabilities according to prior distributions.

Method: Introduce a thresholding method that tunes the mark probability by normalizing with the mark's prior probability to optimize mark prediction. Use a two-stage approach: first predict the mark, then the time. Develop a novel neural MTPP model that supports effective time sampling and accurate estimation of mark probability without costly numerical improper integration.

Result: Extensive experiments on real-world datasets demonstrate superior performance of the proposed method against various baselines for both the next event mark and time prediction.

Conclusion: Thresholded, prior-normalized mark probabilities combined with a two-stage prediction (mark first, then time) and a neural MTPP framework yield improved next-event prediction performance, particularly under imbalanced mark distributions; the code is publicly available.

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [48] [An Empirical Study of Sample Selection Strategies for Large Language Model Repair](https://arxiv.org/abs/2510.20428)
*Xuran Li,Jingyi Wang*

Main category: cs.LG

TL;DR: 本研究系统比较了五种用于LLM行为修复的数据采样策略，并提出了SAPS方法，结果显示在保持 detoxification 与 保留模型效用的前提下，SAPS以更少的数据取得更好或相近的修复效果。对比中，随机采样在大规模或鲁棒模型上也有效；高开销的方法（CCS、GraNd）收益有限。数据样本比例取决于模型规模与修复方法，提示把数据选择视为修复流程中的可调组件。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型易生成有害或带偏的输出，后期修复成为提升安全性与信任度的实际手段。与此同时，修复数据的选取成本高，因此需要高效的数据筛选策略来提升修复效果。

Method: 系统评估五种样本选择方法：随机采样、K-Center、梯度范数选择(GraNd)、分层覆盖(CCS)以及作者提出的语义感知优先采样(SAPS)。通过毒性降低、WikiText-2与LAMBADA上的困惑度、以及三个综合指标（Repair Proximity Score, RPS；Overall Performance Score, OPS；Repair Efficiency Score, RES）来衡量修复效果与权衡。

Result: 实验结果表明SAPS在减毒化、保持实用性与效率之间实现最佳平衡，能够在数据量显著减少的情况下获得可比或更优的修复结果。随机采样对大型/鲁棒模型仍具有效性；但如CCS和GraNd这类开销较高的方法收益有限。修复所需的数据比例随模型规模与修复方法而异，提示将数据选择作为修复管线中的可调参数。

Conclusion: 基于选择的数据修复方法为提高LLM可靠性提供了一种高效、可扩展的范式。样本选择应作为修复管线的可调组件，帮助在安全性与实用性之间进行权衡。

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems,
yet they can produce toxic or biased outputs that undermine safety and trust.
Post-hoc model repair provides a practical remedy, but the high cost of
parameter updates motivates selective use of repair data. Despite extensive
prior work on data selection for model training, it remains unclear which
sampling criteria are most effective and efficient when applied specifically to
behavioral repair of large generative models. Our study presents a systematic
analysis of sample prioritization strategies for LLM repair. We evaluate five
representative selection methods, including random sampling, K-Center,
gradient-norm-based selection(GraNd), stratified coverage (CCS), and a
Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair
effectiveness and trade-offs are assessed through toxicity reduction,
perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair
Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair
Efficiency Score (RES). Experimental results show that SAPS achieves the best
balance between detoxification, utility preservation, and efficiency,
delivering comparable or superior repair outcomes with substantially less data.
Random sampling remains effective for large or robust models, while
high-overhead methods such as CCS and GraNd provide limited benefit. The
optimal data proportion depends on model scale and repair method, indicating
that sample selection should be regarded as a tunable component of repair
pipelines. Overall, these findings establish selection-based repair as an
efficient and scalable paradigm for maintaining LLM reliability.

</details>


### [49] [Explainable Benchmarking through the Lense of Concept Learning](https://arxiv.org/abs/2510.20439)
*Quannian Zhang,Michael Röder,Nikit Srivastava,N'Dah Jean Kouagou,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 提出 explainable benchmarking 的新范式，针对知识图谱问答系统，利用 PruneCEL 生成可解释的性能原因；在 explainable benchmarking 任务上优于现有概念学习方法，且用户研究表明解释有助于预测系统行为，代码与数据开放。


<details>
  <summary>Details</summary>
Motivation: 传统基准评估常用少量指标，难以提供深入的分析与改进线索，且评价过程容易带有偏差。需要自动化、可解释的基准分析以揭示系统行为的原因与使用场景。

Method: 提出 explainable benchmarking 的范式，并为知识图谱问答系统给出首个实现，利用大规模知识图谱的概念学习方法 PruneCEL 生成对系统性能的解释；并与现有概念学习方法比较；通过一项涉及 41 名参与者的任务驱动用户研究评估解释的可用性。

Result: PruneCEL 在 explainable benchmarking 的任务上相对于最先进概念学习方法提升最多 0.55 点 F1；任务驱动用户研究显示 41 名参与者中有 80% 能在给定解释的条件下准确预测系统行为；代码与数据公开可得。

Conclusion: 首次给出 explainable benchmarking 的实现范式并在知识图谱问答领域取得初步成功，表明可解释的基准分析具备实际价值并具扩展潜力。

Abstract: Evaluating competing systems in a comparable way, i.e., benchmarking them, is
an undeniable pillar of the scientific method. However, system performance is
often summarized via a small number of metrics. The analysis of the evaluation
details and the derivation of insights for further development or use remains a
tedious manual task with often biased results. Thus, this paper argues for a
new type of benchmarking, which is dubbed explainable benchmarking. The aim of
explainable benchmarking approaches is to automatically generate explanations
for the performance of systems in a benchmark. We provide a first instantiation
of this paradigm for knowledge-graph-based question answering systems. We
compute explanations by using a novel concept learning approach developed for
large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL
outperforms state-of-the-art concept learners on the task of explainable
benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41
participants shows that in 80\% of the cases, the majority of participants can
accurately predict the behavior of a system based on our explanations. Our code
and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025

</details>


### [50] [Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models](https://arxiv.org/abs/2510.20477)
*Rui Zhu,Song-Lin Lv,Zi-Kang Wang,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: Bi-CoG 提出了一种可插拔、简单有效的半监督微调策略，结合跨模型和对模型内部的两层一致性以及基于错误感知的动态伪标签分配，在保持较低偏差的同时提升高质量伪标签的生成。对 vision-language 模型的标签稀缺场景具有显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 在标注数据稀缺的场景中，单纯的预测一致性约束和预设阈值会导致模型偏差和对超参数的敏感性。通过引入双重一致性（跨模型与模型内部）以及动态、错误感知的伪标签分配，旨在提高伪标签质量并降低对超参数的依赖。

Method: Bi-CoG 采用可插拔的自训练框架：同时利用跨模型一致性与 intra-model（内部）一致性来筛选高质量伪标签，并引入基于错误概率的动态伪标签分配策略，减少低质量伪标签对模型更新的影响。理论分析支撑其收敛性和有效性，实验在 14 个数据集上验证其优越性。

Result: 理论分析表明该方法在伪标签质量与偏差控制之间达到更优的权衡。实验证明 Bi-CoG 持续且显著地提升了现有方法在 14 个数据集上的性能。

Conclusion: Bi-CoG 提供了一种简单、可任意嵌入现有预训练视觉-语言模型的半监督微调方案，通过双重一致性与动态伪标签分配实现高质量伪标签、低偏差和对超参数的鲁棒性，在跨数据集场景中具备良好泛化能力。

Abstract: Exploiting unlabeled data through semi-supervised learning (SSL) or
leveraging pre-trained models via fine-tuning are two prevailing paradigms for
addressing label-scarce scenarios. Recently, growing attention has been given
to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,
forming the emerging paradigm of semi-supervised fine-tuning. However, existing
methods often suffer from model bias and hyperparameter sensitivity, due to
reliance on prediction consistency or pre-defined confidence thresholds. To
address these limitations, we propose a simple yet effective plug-and-play
methodology named
$\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided
Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,
by simultaneously exploiting inter-model and intra-model consistency, along
with an error-aware dynamic pseudo-label assignment strategy. Both theoretical
analysis and extensive experiments over 14 datasets demonstrate the
effectiveness of Bi-CoG, which consistently and significantly improves the
performance of existing methods.

</details>


### [51] [Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval](https://arxiv.org/abs/2510.20486)
*Fangjian Zhang,Xiaoyong Zhuge,Wenlan Wang,Haixia Xiao,Yuying Zhu,Siyang Cheng*

Main category: cs.LG

TL;DR: 提出 Hurdle-IMDL 框架以同时应对遥感数据中的标签分布不均问题，包括零膨胀与长尾两部分；通过 hurdle 模型处理非雨样本并将学习目标转化为无偏的理想逆模型，提升对强降雨的检索与整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于人工智能的遥感检知在标签分布不均（尤其是强降雨样本稀少、非雨样本占多数）时容易偏向常见样本，导致对罕见事件（如强降雨）的检出与估计性能下降。

Method: 采用分治策略，将降雨分布的不平衡分解为零膨胀与长尾两部分。对零膨胀采用 hurdle 模型进行处理；对长尾通过 IMDL 将学习目标转化为无偏的理想逆模型，以矫正学习过程中的偏差。

Result: 在统计指标和中国东部雨况案例研究中，Hurdle-IMDL 相较传统、代价敏感、生成式和多任务学习方法具有更优性能，显著缓解系统性低估，并提升对重雨到极端降雨的检索与估计。

Conclusion: IMDL 提供一种可泛化的思路，用于解决环境变量分布的不均衡问题，提升对罕见但高影响事件的检索与估计，并可推广至其他环境变量的检索任务。

Abstract: Artificial intelligence has advanced quantitative remote sensing, yet its
effectiveness is constrained by imbalanced label distribution. This imbalance
leads conventionally trained models to favor common samples, which in turn
degrades retrieval performance for rare ones. Rainfall retrieval exemplifies
this issue, with performance particularly compromised for heavy rain. This
study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.
Following a divide-and-conquer strategy, imbalance in the rain distribution is
decomposed into two components: zero inflation, defined by the predominance of
non-rain samples; and long tail, defined by the disproportionate abundance of
light-rain samples relative to heavy-rain samples. A hurdle model is adopted to
handle the zero inflation, while IMDL is proposed to address the long tail by
transforming the learning object into an unbiased ideal inverse model.
Comprehensive evaluation via statistical metrics and case studies investigating
rainy weather in eastern China confirms Hurdle-IMDL's superiority over
conventional, cost-sensitive, generative, and multi-task learning methods. Its
key advancements include effective mitigation of systematic underestimation and
a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a
generalizable approach for addressing imbalance in distributions of
environmental variables, enabling enhanced retrieval of rare yet high-impact
events.

</details>


### [52] [SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment](https://arxiv.org/abs/2510.20540)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出SheafAlign，一种基于层束理论的去中心化多模态对齐框架，替代单一对齐空间，通过多比较空间和去中心化对比学习实现高鲁棒性与低通信成本。


<details>
  <summary>Details</summary>
Motivation: 现实分布式场景中模态之间并不具备全局互冗余，强依赖于跨模态对齐的单一潜空间会损失信息；需要在分布式环境中实现高效且保留共享与独特信息的对齐。

Method: 用层束（sheaf）结构表达模态对等关系，构建两两模态间的关系以及局部一致性；采用去中心化的对比学习目标，在多个比较空间上进行训练。

Result: 在多模态传感数据集上实现更强的零-shot泛化、跨模态对齐以及对缺失模态的鲁棒性，且通信成本比最先进的基线降低约50%。

Conclusion: 通过放弃对所有模态之间全局互冗余的依赖，SheafAlign 能同时保留共享信息与模态特有信息，提升去中心化多模态对齐的效能与鲁棒性。

Abstract: Conventional multimodal alignment methods assume mutual redundancy across all
modalities, an assumption that fails in real-world distributed scenarios. We
propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal
alignment that replaces single-space alignment with multiple comparison spaces.
This approach models pairwise modality relations through sheaf structures and
leverages decentralized contrastive learning-based objectives for training.
SheafAlign overcomes the limitations of prior methods by not requiring mutual
redundancy among all modalities, preserving both shared and unique information.
Experiments on multimodal sensing datasets show superior zero-shot
generalization, cross-modal alignment, and robustness to missing modalities,
with 50\% lower communication cost than state-of-the-art baselines.

</details>


### [53] [A Unified Framework for Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.20542)
*Jacopo Di Ventura,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 提出零-shot强化学习的统一框架，建立清晰记法与分类，将方法分为直接表示与组合表示，提供对现有工作的可比性与未来研究的清晰路径。


<details>
  <summary>Details</summary>
Motivation: 零-shot RL尚缺统一分析视角，难以横向比较不同方法及其泛化能力，因此需要一个统一的框架来整理、比较并提炼核心原则。

Method: 构建统一记法与公理化框架，将现有方法分为两大类：直接表示（端到端地将奖励映射到策略的表示学习）与组合表示（通过对值函数的子结构进行分解来构造表示）。在此框架内，分析共性原则、差异，并对后继特征（successor-feature）方法给出扩展界，以便在零-shot场景下评估与比较。

Result: 从结构出发统一对比现有工作，提炼共性与差异，并提供对后继特征方法的新理论界限，为零-shot RL的评估与改进提供参考。

Conclusion: 统一框架为零-shot RL的未来研究提供 principled 基础，指明发展更具通用性智能体的明确方向。

Abstract: Zero-shot reinforcement learning (RL) has emerged as a setting for developing
general agents in an unsupervised manner, capable of solving downstream tasks
without additional training or planning at test-time. Unlike conventional RL,
which optimizes policies for a fixed reward, zero-shot RL requires agents to
encode representations rich enough to support immediate adaptation to any
objective, drawing parallels to vision and language foundation models. Despite
growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation
introduces a consistent notation and taxonomy that organizes existing
approaches and allows direct comparison between them. Central to our framework
is the classification of algorithms into two families: direct representations,
which learn end-to-end mappings from rewards to policies, and compositional
representations, which decompose the representation leveraging the substructure
of the value function. Within this framework, we highlight shared principles
and key differences across methods, and we derive an extended bound for
successor-feature methods, offering a new perspective on their performance in
the zero-shot regime. By consolidating existing work under a common lens, our
framework provides a principled foundation for future research in zero-shot RL
and outlines a clear path toward developing more general agents.

</details>


### [54] [Embedding the MLOps Lifecycle into OT Reference Models](https://arxiv.org/abs/2510.20590)
*Simon Schindler,Christoph Binder,Lukas Lürzer,Stefan Huber*

Main category: cs.LG

TL;DR: 通过系统化的适配，将MLOps引入OT环境，借助RAMI 4.0与ISA-95等参考模型实现迁移与集成，而非直接照搬现有MLOps实践。


<details>
  <summary>Details</summary>
Motivation: 工业领域对MLOps的采用日益增多，但OT系统的特性（安全、可靠性、现场部署等）使直接迁移困难，需以现有参考模型为桥梁实现MLOps在OT中的落地。

Method: 分析在OT环境中整合MLOps的主要障碍；评估RAMI 4.0与ISA-95的适用性；将MLOps生命周期组件与RAMI 4.0进行详细映射，并通过真实用例进行演示。

Result: 研究表明直接将标准MLOps实践应用于OT不可行；通过基于RAMI 4.0/ISA-95的结构化适配，可以为OT中的MLOps集成提供路径，给出具体的组件映射与实现要点。

Conclusion: 以现有参考模型为支点进行结构化适配，是在OT环境中实现MLOps整合的可行途径；未来工作应聚焦于完善映射、标准化接口与安全/可靠性约束。

Abstract: Machine Learning Operations (MLOps) practices are increas- ingly adopted in
industrial settings, yet their integration with Opera- tional Technology (OT)
systems presents significant challenges. This pa- per analyzes the fundamental
obstacles in combining MLOps with OT en- vironments and proposes a systematic
approach to embed MLOps prac- tices into established OT reference models. We
evaluate the suitability of the Reference Architectural Model for Industry 4.0
(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for
MLOps integration and present a detailed mapping of MLOps lifecycle compo-
nents to RAMI 4.0 exemplified by a real-world use case. Our findings
demonstrate that while standard MLOps practices cannot be directly transplanted
to OT environments, structured adaptation using existing reference models can
provide a pathway for successful integration.

</details>


### [55] [Generalizable Reasoning through Compositional Energy Minimization](https://arxiv.org/abs/2510.20607)
*Alexandru Oarga,Yilun Du*

Main category: cs.LG

TL;DR: 提出一个基于子问题能量景观的组合推理框架，通过并行能量最小化提升采样质量，显著提升对更大更复杂推理任务的泛化。


<details>
  <summary>Details</summary>
Motivation: 端到端训练易导致对分布外推理的泛化不足，需通过可组合的、可扩展的推理结构来引入外部约束，从而在更复杂问题上保持性能。

Method: 以较小、可解决的子问题为单位，学习它们的能量景观；在推理时将子问题的能量景观组合成全局能量景观；引入Parallel Energy Minimization（PEM）提升新能量景观的样本质量；在多种推理任务上评估。

Result: 相比现有方法，在更大规模和更复杂的推理任务上具有更强泛化能力，达到或超过最先端水平。

Conclusion: 能量景观的组合与PEM为推理泛化提供一个可扩展的路径，允许在推理阶段加入更多约束，提升对复杂问题的鲁棒性。

Abstract: Generalization is a key challenge in machine learning, specifically in
reasoning tasks, where models are expected to solve problems more complex than
those encountered during training. Existing approaches typically train
reasoning models in an end-to-end fashion, directly mapping input instances to
solutions. While this allows models to learn useful heuristics from data, it
often results in limited generalization beyond the training distribution. In
this work, we propose a novel approach to reasoning generalization by learning
energy landscapes over the solution spaces of smaller, more tractable
subproblems. At test time, we construct a global energy landscape for a given
problem by combining the energy functions of multiple subproblems. This
compositional approach enables the incorporation of additional constraints
during inference, allowing the construction of energy landscapes for problems
of increasing difficulty. To improve the sample quality from this newly
constructed energy landscape, we introduce Parallel Energy Minimization (PEM).
We evaluate our approach on a wide set of reasoning problems. Our method
outperforms existing state-of-the-art methods, demonstrating its ability to
generalize to larger and more complex problems. Project website can be found
at: https://alexoarga.github.io/compositional_reasoning/

</details>


### [56] [Convergence Analysis of SGD under Expected Smoothness](https://arxiv.org/abs/2510.20608)
*Yuta Kawamoto,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 在期望光滑性（ES）条件下，SGD 仍可获得收敛性保障；给出 O(1/K) 的收敛速率及明确的残差项，并通过对 ES 的细化解释与采样相关常数实现对全梯度范数平方的界，统一并扩展了相关研究的框架。


<details>
  <summary>Details</summary>
Motivation: 传统分析要么假设有界方差，要么对噪声过于粗糙，不能很好地刻画真实随机梯度的性质。期望光滑性（ES）提供一个更灵活的中间假设，将随机梯度的二阶矩与目标值和全梯度联系起来，便于在更广泛的设置下分析 SGD 的收敛行为。需要一个自洽且可操作的分析框架，既能细化对 ES 的理解，又能覆盖并统一近年的相关工作。

Method: 给出一个自包含的SGD收敛分析，在 ES 条件下(1) 对 ES 进行解释和采样相关常数的细化；(2) 推导出全梯度范数平方的期望上界；(3) 在多种步长选择下证明 O(1/K) 的收敛率，并给出显式残差项。所有证明放在附录，以实现对 Khaled & Richtárik (2020) 与 Umeda & Iiduka (2025) 等工作的统一与扩展。

Result: 得到对期望全梯度范数平方的界，并在不同步长策略下确立 O(1/K) 的收敛速率以及相应的显式残差。该分析框架对 ES 条件的解释和常数给出更清晰的理解，统一并扩展了相关研究的思路。

Conclusion: 提出一个自洽、可操作的分析框架，表明 ES 条件可以作为对传统有界方差/均匀噪声假设的灵活替代，并成功地将近期工作的思路统一与扩展，为在更广泛随机优化场景下应用 ES 提供理论基础。

Abstract: Stochastic gradient descent (SGD) is the workhorse of large-scale learning,
yet classical analyses rely on assumptions that can be either too strong
(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)
condition has emerged as a flexible alternative that ties the second moment of
stochastic gradients to the objective value and the full gradient. This paper
presents a self-contained convergence analysis of SGD under ES. We (i) refine
ES with interpretations and sampling-dependent constants; (ii) derive bounds of
the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates
with explicit residual errors for various step-size schedules. All proofs are
given in full detail in the appendix. Our treatment unifies and extends recent
threads (Khaled and Richt\'arik, 2020; Umeda and Iiduka, 2025).

</details>


### [57] [Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets](https://arxiv.org/abs/2510.20609)
*Timur Galimzyanov,Olga Kolomyttseva,Egor Bogomolov*

Main category: cs.LG

TL;DR: 对代码生成任务中的检索设计进行系统评估，考察在不同上下文窗口与预算约束下的检索配置（分块策略、相似度评分、分割粒度）的影响。核心结论是：对代码-代码检索（PL-PL）时，BM25+逐词分块最优且快；对自然语言-代码检索（NL-PL）密集向量编码器（如Voyager-3）表现最好但延迟显著；最优分块大小随上下文而变，32-64 行在小预算下效果最好，16000 tokens 时整文件检索具有竞争力；基于简单的行级分块即可在多预算下与语法感知分割相当；检索延迟在不同配置间差异高达约200倍；BM25+逐词分块在质量与延迟折中最佳。给出基于任务需求与资源约束的实证性RAG实现建议。


<details>
  <summary>Details</summary>
Motivation: 在真实的计算预算约束下，如何设计高效的代码领域检索（RAG）以支持代码相关生成任务（如代码补全、缺陷定位）成为关键问题。本研究通过对两项任务的广泛对比，揭示在不同上下文窗口、分块策略及编码粒度下的性能权衡。

Method: 在Long Code Arena的两项任务（代码补全、缺陷定位）上，系统性比较了不同检索配置：上下文窗口大小、分块策略、相似度评分、分割粒度等。对比对象包括：PL-PL场景下的稀疏BM25与密集向量的表现；NL-PL场景下Voyager-3等专有密集编码的表现；并评估32-64行、整文件等不同分块粒度在小预算与大预算下的效果，以及行级分块与语法感知分割的对比。

Result: (1) PL-PL：BM25+逐词分块最有效且最快，远优于密集方法；(2) NL-PL：Voyager-3等密集编码在准确性上占优，但延迟约高出100倍以上；(3) 最优分块大小随上下文扩展而变化：小预算下32-64行最优，16000 tokens时整文件检索具备竞争力；(4) 简单的逐行分块在不同预算下与语法感知分割表现相近；(5) 各配置的检索延迟差异可达约200x；BM25+逐词分块在质量与延迟之间的折中最佳。

Conclusion: 针对代码导向的RAG系统，需基于具体任务需求、模型能力和计算预算做出权衡选择。若追求更低延迟且预算有限，BM25+逐词分块是首选；若可接受较高延迟以换取更高检索质量且使用密集编码器，则NL-PL方案更具潜力。分块粒度应依据可用上下文规模进行调整，32–64行在小预算下效果佳，整文件在极大上下文下具备竞争力；简单的行级分块与语法感知分割在多数预算下可实现类似性能。最终给出面向不同任务需求和计算约束的实证驱动的RAG实现建议。

Abstract: We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.

</details>


### [58] [MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation](https://arxiv.org/abs/2510.20615)
*Yang Han,Pengyu Wang,Kai Yu,Xin Chen,Lu Chen*

Main category: cs.LG

TL;DR: MS-BART提出一个统一的跨模态建模框架，将质谱与分子结构映射到共享词汇，通过大规模自监督预训练和化学反馈机制，在实验谱和真实谱之间提升鲁棒性与分子生成的准确性，在多个基准上达到SOTA并显著加速推断。


<details>
  <summary>Details</summary>
Motivation: 结构解析受到标注谱稀缺和原始谱信号的复杂性与异质性制约。将谱和分子结构统一成共享表示以实现跨模态学习，并通过预训练提升泛化，再通过针对真实谱的微调与化学反馈缓解分子幻觉问题。

Method: 提出MS-BART：将质谱数据与分子结构映射到共享的token词汇；基于可靠计算的指纹-分子数据集进行跨模态大规模预训练；采用多任务目标（去噪与翻译）提升泛化；finetuning时以MIST预测的指纹对实验谱进行迁移，并引入化学反馈机制引导输出更接近参考分子。

Result: 在MassSpecGym和NPLIB1等基准上，MS-BART在5/12个关键指标上达到SOTA，并且推断速度比扩散式方法快一个数量级；通过系统性消融实验验证各模块的有效性与鲁棒性。

Conclusion: 尽管finetuning缓解了分布差异，模型仍存在分子幻觉与对齐不足的问题。化学反馈机制提升了对真实谱的鲁棒性，未来工作将聚焦于进一步对齐、减少幻觉并扩展到更多数据集与任务。

Abstract: Mass spectrometry (MS) plays a critical role in molecular identification,
significantly advancing scientific discovery. However, structure elucidation
from MS data remains challenging due to the scarcity of annotated spectra.
While large-scale pretraining has proven effective in addressing data scarcity
in other domains, applying this paradigm to mass spectrometry is hindered by
the complexity and heterogeneity of raw spectral signals. To address this, we
propose MS-BART, a unified modeling framework that maps mass spectra and
molecular structures into a shared token vocabulary, enabling cross-modal
learning through large-scale pretraining on reliably computed
fingerprint-molecule datasets. Multi-task pretraining objectives further
enhance MS-BART's generalization by jointly optimizing denoising and
translation task. The pretrained model is subsequently transferred to
experimental spectra through finetuning on fingerprint predictions generated
with MIST, a pre-trained spectral inference model, thereby enhancing robustness
to real-world spectral variability. While finetuning alleviates the
distributional difference, MS-BART still suffers molecular hallucination and
requires further alignment. We therefore introduce a chemical feedback
mechanism that guides the model toward generating molecules closer to the
reference structure. Extensive evaluations demonstrate that MS-BART achieves
SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is
faster by one order of magnitude than competing diffusion-based methods, while
comprehensive ablation studies systematically validate the model's
effectiveness and robustness.

</details>


### [59] [On Optimal Hyperparameters for Differentially Private Deep Transfer Learning](https://arxiv.org/abs/2510.20616)
*Aki Rehn,Linzh Zhao,Mikko A. Heikkilä,Antti Honkela*

Main category: cs.LG

TL;DR: 差分隐私（DP）下的迁移学习超参数C（裁剪上限）与B（批量大小）之间存在理论与经验的显著错位；通过分析梯度分布与累积DP噪声，揭示在强隐私约束下更大的C可能更优；单一(C,B)设定对跨任务鲁棒性差，且在隐私强/弱、计算预算充裕与否之间切换时性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前在私有数据上对预训练模型进行微调的DP方法依赖关键超参数C和B，但理论通常认为强隐私应使用更小的裁剪上限C，而实证结果却显示在强隐私下更大C更好。需揭示梯度分布变化和累积DP噪声对结果的影响，以指导实际调参。

Method: 从理论上将裁剪视为对梯度的重加权，分析固定训练轮数下B的选择如何影响DP噪声累积与梯度分布；通过跨任务的实验评估单一(C,B)设定的鲁棒性，比较不同隐私等级与计算预算下的表现，并解释为何常用的B调优启发式在受限预算下失效。

Result: 发现C与隐私等级之间的关系在理论与实践中不一致；梯度分布随训练变化，累积DP噪声更能解释B的优劣；跨任务使用同一(C,B)显著降低性能，尤其是在隐私强/弱和计算预算存在较大差异时。

Conclusion: 在DP微调中需同时考虑任务特性、隐私约束与计算预算对梯度裁剪和噪声的影响，单一启发式难以泛化，应针对具体场景单独调参，并重新评估裁剪对梯度的重加权效应与累积噪声的建模。

Abstract: Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained
model on private data, is the current state-of-the-art approach for training
large models under privacy constraints. We focus on two key hyperparameters in
this setting: the clipping bound $C$ and batch size $B$. We show a clear
mismatch between the current theoretical understanding of how to choose an
optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes
(larger $C$ performs better under strong privacy), caused by changes in the
gradient distributions. Assuming a limited compute budget (fixed epochs), we
demonstrate that the existing heuristics for tuning $B$ do not work, while
cumulative DP noise better explains whether smaller or larger batches perform
better. We also highlight how the common practice of using a single $(C,B)$
setting across tasks can lead to suboptimal performance. We find that
performance drops especially when moving between loose and tight privacy and
between plentiful and limited compute, which we explain by analyzing clipping
as a form of gradient re-weighting and examining cumulative DP noise.

</details>


### [60] [H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition](https://arxiv.org/abs/2510.20627)
*Lukas Miklautz,Chengzhi Shi,Andrii Shkabrii,Theodoros Thirimachos Davarakis,Prudence Lam,Claudia Plant,Jennifer Dy,Stratis Ioannidis*

Main category: cs.LG

TL;DR: 提出H-SPLID，通过将显著特征与非显著特征分离的显式子空间，提升鲁棒性与特征压缩的学习效果。


<details>
  <summary>Details</summary>
Motivation: 解决模型对背景等非显著特征的敏感性，提升低维且任务相关的表示。

Method: 提出H-SPLID，使用显式分解将显著/非显著特征映射到两个子空间；推导在输入扰动下的预测偏差上界与 salient 子空间维度及输入与表征的HSIC；并在图像分类上进行评估。

Result: 在实验中，模型更多依赖显著输入分量，对扰动影响非显著特征（如背景）时的敏感性下降；与基线对比具鲁棒性提升；代码开放。

Conclusion: 显式分解与低维表征压缩提升鲁棒性与表示学习的效率，提供了鲁棒性与隐表示压缩之间的联系。

Abstract: We introduce H-SPLID, a novel algorithm for learning salient feature
representations through the explicit decomposition of salient and non-salient
features into separate spaces. We show that H-SPLID promotes learning
low-dimensional, task-relevant features. We prove that the expected prediction
deviation under input perturbations is upper-bounded by the dimension of the
salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between
inputs and representations. This establishes a link between robustness and
latent representation compression in terms of the dimensionality and
information preserved. Empirical evaluations on image classification tasks show
that models trained with H-SPLID primarily rely on salient input components, as
indicated by reduced sensitivity to perturbations affecting non-salient
features, such as image backgrounds. Our code is available at
https://github.com/neu-spiral/H-SPLID.

</details>


### [61] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: 提出 xTime 框架，通过知识蒸馏和混合专家提升极端事件预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 极端事件在现实时间序列中具有重要影响，但由于数据不平衡和对中间前导信息的忽视，现有模型往往难以准确预测极端事件。

Method: 引入知识蒸馏以从低稀有度事件的模型中传递信息，并采用混合专家（MoE）机制动态选择并融合来自不同稀有度等级的专家输出。

Result: 在多数据集上验证，极端事件预测准确性显著提升，提升区间约为 3% 到 78%，显示对高稀有事件的预测能力的显著改善。

Conclusion: 通过结合知识蒸馏和 MoE，xTime 能更好利用稀有事件相关信息，缓解数据不平衡，具备较强的泛化潜力与应用前景。

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [62] [From Masks to Worlds: A Hitchhiker's Guide to World Models](https://arxiv.org/abs/2510.20668)
*Jinbin Bai,Yu Lei,Hecong Wu,Yuchen Zhu,Shufan Li,Yi Xin,Xiangtai Li,Molei Tao,Aditya Grover,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: 以构建世界模型为目标的路线图：从早期的多模态掩码模型出发，过渡到统一架构，再到交互生成模型，最终走向记忆增强系统，聚焦生成核心、交互环和记忆能力三要素。


<details>
  <summary>Details</summary>
Motivation: 提供一个清晰的研究框架，聚焦核心组件，避免忙于列举相关论文，为实现真正的世界模型提供一致的设计方向。

Method: 通过分析与整合相关研究沿革，讲述从模态统一到单一范式的架构、再到互动生成与记忆扩展的演进路径，突出核心机制的作用。

Result: 提出一个可操作的研究路线图，强调将生成能力、互动循环与长期记忆结合，以构建更稳定、可持续的世界模型体系。

Conclusion: 在生成核心、交互循环和记忆系统三要素的整合下，该路线被视为实现真正世界模型最具前景的路径。

Abstract: This is not a typical survey of world models; it is a guide for those who
want to build worlds. We do not aim to catalog every paper that has ever
mentioned a ``world model". Instead, we follow one clear road: from early
masked models that unified representation learning across modalities, to
unified architectures that share a single paradigm, then to interactive
generative models that close the action-perception loop, and finally to
memory-augmented systems that sustain consistent worlds over time. We bypass
loosely related branches to focus on the core: the generative heart, the
interactive loop, and the memory system. We show that this is the most
promising path towards true world models.

</details>


### [63] [Separating the what and how of compositional computation to enable reuse and continual learning](https://arxiv.org/abs/2510.20709)
*Haozhe Shan,Sun Minni,Lea Duncker*

Main category: cs.LG

TL;DR: 提出一种两系统框架来实现RNN的持续学习与快速成分化（组合性）泛化：一个系统推断要执行的计算，另一个系统实现如何执行；通过无监督在线学习建立一个可再用的任务单位词汇库。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中技能重用与“再组合”计算的问题，结合神经科学研究中的任务范式，提出一个可质量化描述任务的生成模型，并在此基础上实现对计算过程的分离学习，旨在避免灾难性遗忘并实现正向/逆向迁移与对新任务的快速组合泛化。

Method: 提出一个两系统框架：what系统通过一个概率生成模型描述任务的组成性，任务的离散“阶段(epoch)”构成可共享的词汇库；通过无监督在线学习在单次试验内逐步建立词汇库并将潜在的阶段结构推断为试验内随时间变化的计算上下文。与之配套，how系统是一个低秩成分可按what系统推断的上下文进行组合的RNN，低秩组件在新任务出现时被创建、学习和重用。

Result: 在示例任务集上展示了框架的有效性与竞争性性能，显示了对前向与后向迁移的潜力，以及对未见任务的快速组合泛化，实现持续学习而较少灾难性遗忘。

Conclusion: 两系统框架为持续学习与成分化泛化提供了一个有效机制，能够通过分工实现对计算过程的可塑性、可迁移性与快速适应性，具有广泛的应用潜力与未来扩展空间。

Abstract: The ability to continually learn, retain and deploy skills to accomplish
goals is a key feature of intelligent and efficient behavior. However, the
neural mechanisms facilitating the continual learning and flexible
(re-)composition of skills remain elusive. Here, we study continual learning
and the compositional reuse of learned computations in recurrent neural network
(RNN) models using a novel two-system approach: one system that infers what
computation to perform, and one that implements how to perform it. We focus on
a set of compositional cognitive tasks commonly studied in neuroscience. To
construct the what system, we first show that a large family of tasks can be
systematically described by a probabilistic generative model, where
compositionality stems from a shared underlying vocabulary of discrete task
epochs. The shared epoch structure makes these tasks inherently compositional.
We first show that this compositionality can be systematically described by a
probabilistic generative model. Furthermore, We develop an unsupervised online
learning approach that can learn this model on a single-trial basis, building
its vocabulary incrementally as it is exposed to new tasks, and inferring the
latent epoch structure as a time-varying computational context within a trial.
We implement the how system as an RNN whose low-rank components are composed
according to the context inferred by the what system. Contextual inference
facilitates the creation, learning, and reuse of low-rank RNN components as new
tasks are introduced sequentially, enabling continual learning without
catastrophic forgetting. Using an example task set, we demonstrate the efficacy
and competitive performance of this two-system learning framework, its
potential for forward and backward transfer, as well as fast compositional
generalization to unseen tasks.

</details>


### [64] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 通过约束分数优化（CSO）对 JHFRAT 的再建模，利用 EHR 变量提升住院跌倒风险预测，并在可解释性与鲁棒性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 提高 JHFRAT 的预测性能，同时保持临床可解释性；使风险评估结果与临床相关指标对齐，并更好地用于资源分配。

Method: 回顾性分析，样本来自三家约翰霍普金斯健康系统医院，共 54,209 住院记录；建立 CSO 模型在 JHFRAT 数据及额外 EHR 变量上的约束分数优化；与可解释的约束逻辑回归、以及黑箱模型 XGBoost 进行比较。

Result: CSO 的 AUC-ROC 达到 0.91，优于 JHFRAT 的 0.86；带/不带 EHR 变量的 CSO 表现相似；XGBoost 达到 0.94，但 CSO 对风险标签变异更鲁棒。

Conclusion: 基于证据的方法为医疗系统在住院跌倒预防、风险评估与资源分配方面提供稳健的、可解释的数据驱动优化基础。

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [65] [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](https://arxiv.org/abs/2510.20725)
*Jasmine Bayrooti,Sattar Vakili,Amanda Prorok,Carl Henrik Ek*

Main category: cs.LG

TL;DR: 在强化学习中对带有高斯边际分布的模型，提出对 Thompson 采样（TS）的无后悔保证，在具有高斯过程先验的奖励和转移的分段RL中，得到对KH轮的无后悔界限 allo?sqrt{KHGamma(KH)}，Γ表示GP模型的复杂度。


<details>
  <summary>Details</summary>
Motivation: 填补在复杂时序结构（如RL）下对TS理论基础的空缺，建立在瞬时策略和模型不确定性下的无后悔性质；理解结构假设对TS性能的影响。

Method: 在分段RL中对奖励和转移应用联合高斯过程先验，使用TS策略；应对值函数非高斯性、Bellman更新的递归性等挑战；将椭圆潜势引理等经典工具扩展到多输出情形。

Result: 给出对KH轮的无后悔界，记为 allo{ot KH Gamma(KH)}； Γ(KH) 表示GP模型的复杂度函数，结合观测数量和时间结构得到界限。

Conclusion: 这项工作推进了对RL中TS的理论理解，揭示了结构假设与模型不确定性如何影响有限时序的表现。

Abstract: Thompson sampling (TS) is a powerful and widely used strategy for sequential
decision-making, with applications ranging from Bayesian optimization to
reinforcement learning (RL). Despite its success, the theoretical foundations
of TS remain limited, particularly in settings with complex temporal structure
such as RL. We address this gap by establishing no-regret guarantees for TS
using models with Gaussian marginal distributions. Specifically, we consider TS
in episodic RL with joint Gaussian process (GP) priors over rewards and
transitions. We prove a regret bound of
$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,
where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis
addresses several challenges, including the non-Gaussian nature of value
functions and the recursive structure of Bellman updates, and extends classical
tools such as the elliptical potential lemma to multi-output settings. This
work advances the understanding of TS in RL and highlights how structural
assumptions and model uncertainty shape its performance in finite-horizon
Markov Decision Processes.

</details>


### [66] [Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process](https://arxiv.org/abs/2510.20736)
*Tsai Hor Chan,Feng Wu,Yihang Chen,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出一种基于 Dirichlet 过程的多模态学习框架，利用高斯混合模型的混合权重由 DP 动态分配，以平衡单模态表达与跨模态对齐并提升融合性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中过度强调边缘分布对齐可能抑制单模态有效表示的问题。借助 DP 的 richer-gets-richer 特性，自动放大最突出 Features，同时实现跨模态对齐与 intra-modal 表征的协同。

Method: 假设每个模态遵循多变量高斯混合模型，并通过 Dirichlet 过程来计算各组件的混合权重；通过动态分配特征贡献，DP 促进对最突出特征的放大，从而实现多模态特征融合。

Result: 在若干多模态数据集上，所提模型优于竞争方法。消融分析证实 DP 在对齐模态分布和对关键超参数鲁棒性方面的有效性。代码在 https://github.com/HKU-MedAI/DPMM.git 可获得（匿名公开）。

Conclusion: 以 DP 为驱动的多模态学习框架，能在保持良好单模态表达的同时实现跨模态对齐，提供自适应权重分配以提升鲁棒性和泛化潜力。

Abstract: Developing effective multimodal fusion approaches has become increasingly
essential in many real-world scenarios, such as health care and finance. The
key challenge is how to preserve the feature expressiveness in each modality
while learning cross-modal interactions. Previous approaches primarily focus on
the cross-modal alignment, while over-emphasis on the alignment of marginal
distributions of modalities may impose excess regularization and obstruct
meaningful representations within each modality. The Dirichlet process (DP)
mixture model is a powerful Bayesian non-parametric method that can amplify the
most prominent features by its richer-gets-richer property, which allocates
increasing weights to them. Inspired by this unique characteristic of DP, we
propose a new DP-driven multimodal learning framework that automatically
achieves an optimal balance between prominent intra-modal representation
learning and cross-modal alignment. Specifically, we assume that each modality
follows a mixture of multivariate Gaussian distributions and further adopt DP
to calculate the mixture weights for all the components. This paradigm allows
DP to dynamically allocate the contributions of features and select the most
prominent ones, leveraging its richer-gets-richer property, thus facilitating
multimodal feature fusion. Extensive experiments on several multimodal datasets
demonstrate the superior performance of our model over other competitors.
Ablation analysis further validates the effectiveness of DP in aligning
modality distributions and its robustness to changes in key hyperparameters.
Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git

</details>


### [67] [MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs](https://arxiv.org/abs/2510.20762)
*Jan Sobotka,Luca Baroni,Ján Antolík*

Main category: cs.LG

TL;DR: MEIcoder 是一种生物信息驱动的解码框架，利用神经元特异的最激发输入（MEIs）、结构相似性损失（SSIM）和对抗训练，在V1中可从单细胞活动解码视觉刺激，且在数据稀缺场景下实现高保真重建，并提出了一个包含超过16万样本的基准以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 在灵长类动物或人类中获取高吞吐量神经记录极为困难，导致深度学习解码模型面临数据稀缺的问题；需要将生物学先验引入解码框架以提高数据效率和鲁棒性。

Method: 提出 MEIcoder 框架，结合神经元特异的 MEIs、以结构相似性损失为主的评估约束，以及对抗训练来提升生成质量；通过消融实验验证 MEIs 的关键作用，并在规模扩展实验中展示对少量神经元的鲁棒性，以及如何在仅有 1,000–2,500 个神经元和少于 1,000 条训练样本时实现高保真重建；并在一个包含超过 160,000 条样本的统一基准上进行评测。

Result: 在 V1 的单细胞记录上实现了对自然视觉刺激的高保真重建，达到或接近现有方法的最佳性能，尤其在小数据场景下显著优于对比方法；MEI 作为性能的主要驱动因素；能够用 1k–2.5k 神经元和少于 1k 的训练样本实现可靠解码。

Conclusion: 实验结果表明早期视觉系统的可观测神经信号可以在数据受限的情况下进行可靠解码，为神经科学研究和神经工程应用提供可操作的洞见，并推动相关基准的建立以促进未来研究。

Abstract: Decoding visual stimuli from neural population activity is crucial for
understanding the brain and for applications in brain-machine interfaces.
However, such biological data is often scarce, particularly in primates or
humans, where high-throughput recording techniques, such as two-photon imaging,
remain challenging or impossible to apply. This, in turn, poses a challenge for
deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
biologically informed decoding method that leverages neuron-specific most
exciting inputs (MEIs), a structural similarity index measure loss, and
adversarial training. MEIcoder achieves state-of-the-art performance in
reconstructing visual stimuli from single-cell activity in primary visual
cortex (V1), especially excelling on small datasets with fewer recorded
neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
of the performance, and in scaling experiments, we show that MEIcoder can
reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
neurons and less than 1,000 training data points. We also propose a unified
benchmark with over 160,000 samples to foster future research. Our results
demonstrate the feasibility of reliable decoding in early visual system and
provide practical insights for neuroscience and neuroengineering applications.

</details>


### [68] [Out-of-distribution Tests Reveal Compositionality in Chess Transformers](https://arxiv.org/abs/2510.20783)
*Anna Mészáros,Patrik Reizinger,Ferenc Huszár*

Main category: cs.LG

TL;DR: 270M 参数的棋类 Transformer 展示了组合性泛化和规则外推能力，在 OOD 题设中表现出较高质量的走棋，并在 Chess960 变体中展现基本策略适应，但仍落后于显式搜索的符号 AI；训练过程显示模型在初期仅移动自身棋子，表明出现了组合性理解。


<details>
  <summary>Details</summary>
Motivation: 检验现代决策 Transformer 是否真正掌握棋类规则与组合结构，以及在 OOD 和变体下的泛化能力，与符号 AI 的显式搜索进行对比。

Method: 训练一个 270M 参数的棋类 Transformer，设计 OOD 场景来揭示系统性泛化失败的可能；在 Chess960 变体上测试；将结果与显式搜索的符号 AI 算法比较；分析训练动态。

Result: Transformer 展示组合性泛化和规律外推，能在 OOD 谜题中给出高质量走棋；对 Chess960 的适应较弱于符号 AI，但与人类对战时差距缩小；训练初期模型仅移动自己的棋子，揭示出现的组合性理解。

Conclusion: 模型确实捕捉到棋盘规则的部分结构，具备一定的 OOD 处理能力，但在需要显式搜索的变体场景仍不足；需改进的方向包括结合显式搜索或引入更强的归纳偏置来提升对变体的适应性。

Abstract: Chess is a canonical example of a task that requires rigorous reasoning and
long-term planning. Modern decision Transformers - trained similarly to LLMs -
are able to learn competent gameplay, but it is unclear to what extent they
truly capture the rules of chess. To investigate this, we train a 270M
parameter chess Transformer and test it on out-of-distribution scenarios,
designed to reveal failures of systematic generalization. Our analysis shows
that Transformers exhibit compositional generalization, as evidenced by strong
rule extrapolation: they adhere to fundamental syntactic rules of the game by
consistently choosing valid moves even in situations very different from the
training data. Moreover, they also generate high-quality moves for OOD puzzles.
In a more challenging test, we evaluate the models on variants including
Chess960 (Fischer Random Chess) - a variant of chess where starting positions
of pieces are randomized. We found that while the model exhibits basic strategy
adaptation, they are inferior to symbolic AI algorithms that perform explicit
search, but gap is smaller when playing against users on Lichess. Moreover, the
training dynamics revealed that the model initially learns to move only its own
pieces, suggesting an emergent compositional understanding of the game.

</details>


### [69] [BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](https://arxiv.org/abs/2510.20792)
*Liang Ye,Shengqin Chen,Jiazhu Dai*

Main category: cs.LG

TL;DR: BadGraph 提出面向文本引导的图生成的潜在扩散模型后门攻击，通过文本触发器污染训练数据，在推理阶段触发时注入攻击者指定的子图，同时对清洁输入几乎不影响。


<details>
  <summary>Details</summary>
Motivation: 当前对条件性、尤其是文本引导的图生成中的后门研究不足，且已有工作多关注图像扩散与无条件图生成，亟需评估在潜在扩散模型中的安全风险。

Method: 通过文本触发器对训练数据进行污染，隐匿植入后门，使得在推理阶段触发器出现时输出攻击者指定的子图；后门植入发生在 VAE 与扩散的训练阶段而非预训练阶段；在四个基准数据集（PubChem、ChEBI-20、PCDes、MoMu）上进行广泛实验以验证效果与隐蔽性。

Result: 少于 10% 的污染率即可实现约 50% 的攻击成功率，24% 污染率可实现超过 80% 的成功率；对良性样本几乎无性能下降；消融分析表明后门是在 VAE 与扩散训练阶段植入。

Conclusion: 揭示了文本引导图生成的潜在扩散模型中的安全脆弱性，强调药物发现等应用场景的风险，呼吁对这类模型的后门防御研究。

Abstract: The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.

</details>


### [70] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: 提出通过快速筛选关键权重矩阵、利用梯度信号的奇异值、以及对矩阵行进行多子空间聚类来加速LASER的搜索，实现无需微调就可快速适配下游任务，且在仅用100个样本的条件下便能获得较大性能提升，最高提升可达约24.6个百分点。


<details>
  <summary>Details</summary>
Motivation: 降低LASER在每个候选矩阵上对全量数据进行前向推理的高开销，提升下游任务的适配速度与鲁棒性。

Method: (1) 只需检查少量、精心挑选的矩阵；(2) 通过分析每个矩阵的奇异值梯度来定位需要降维/分解的矩阵；(3) 将矩阵的行聚成多个子空间并对每个簇分别进行分解，以扩展因式分解的搜索空间；(4) 用前100个样本计算梯度并评估最终精度，说明适应主要受提示风格影响；(5) 仅需单步梯度更新和对顶级候选层及分解技术的快速扫描。

Result: 显著降低了原始LASER的搜索开销，能在不进行微调的情况下实现对新数据集的快速、鲁棒适配；在实验中，表现提升可达到24.6个百分点，并且用极少的数据量即可完成评估与微调前的调优。

Conclusion: 该工作给出一种快速且鲁棒的下游任务适配框架，表明在极少数据和单步梯度更新条件下也能显著提升大模型的下游性能，从而提高其实际部署价值。

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


### [71] [KL-Regularized Reinforcement Learning is Designed to Mode Collapse](https://arxiv.org/abs/2510.20817)
*Anthony GX-Chen,Jatin Prakash,Jeff Guo,Rob Fergus,Rajesh Ranganath*

Main category: cs.LG

TL;DR: Reassesses how forward vs reverse KL regularization affects diversity in RL for language models. The KL choice does not simply imply mode-seeking vs mass-covering; the optimal target distribution depends on the regularization coefficient and reward/reference scales. Low regularization and equal rewards tend to yield unimodal targets, making diversity hard by construction. The authors propose a simple, scalable algorithm that adjusts rewards to shape a target distribution over high-quality modes, enabling higher quality and diversity in LLMs and Chemical Language Models, with both KLs usable when naive settings fail.


<details>
  <summary>Details</summary>
Motivation: The common belief that reverse KL promotes mode-seeking and forward KL promotes covering multiple modes does not reliably translate to RL with KL regularization in language models. A deeper theoretical and empirical understanding is needed to control diversity in generated sequences, especially when post-training models.

Method: Theoretical analysis of how KL type, regularization strength, and reward scales determine the optimal target distribution; empirical experiments showing unimodality under low regularization and equal rewards; development of a simple, scalable post-training algorithm that nudges rewards to shape the target distribution over high-quality modes; evaluation on Large Language Models and Chemical Language Models.

Result: Clarifies that mode coverage depends on multiple factors beyond the KL choice; both forward and reverse KL can yield diverse outputs when reg and reward scales are tuned. The proposed method improves solution quality and diversity without extra diversity signals, working for both KLs and applicable in post-training. Demonstrates effectiveness on LLMs and Chemical Language Models.

Conclusion: The choice of KL regularization defines a family of target distributions rather than a fixed mode behavior. Achieving diversity requires careful tuning of regularization strength and reward scales. The proposed simple algorithm provides a principled, scalable way to obtain high-quality, diverse modes in language and chemical language modeling, applicable to either forward or reverse KL when naive settings fail.

Abstract: It is commonly believed that optimizing the reverse KL divergence results in
"mode seeking", while optimizing forward KL results in "mass covering", with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [72] [Rediscovering Recurring Routing Results](https://arxiv.org/abs/2510.20297)
*Xiao Song,John Heidemann*

Main category: cs.NI

TL;DR: Fenrir is a system to rediscover recurring routing results and quantify changes, enabling cross-domain analysis of routing dynamics and applicability to various networking problems.


<details>
  <summary>Details</summary>
Motivation: Routing performance depends on multiple factors (latency, cost, data sovereignty). Traffic Engineering with BGP reflects not only a single observer’s policy but the aggregate of Internet-wide policies; there is a need to uncover and quantify recurring routing changes that may occur beyond the observer’s immediate view.

Method: Fenrir uses active measurements, data cleaning, and weighting to detect routing changes that can occur several hops away from the observer, quantify the degree of change, and identify routing 'modes' that may reappear. The authors demonstrate generalization by applying five instances of three system types to illustrate applicability across different problems.

Result: Fenrir can be applied to various problems (anycast catchments in a root DNS service, route optimization for two multi-homed enterprises, and website selection for two top-10 web services). The methods for detecting and quantifying routing changes are shown to be helpful in answering operational questions such as the impact of traffic engineering, changes by third parties, and whether the current routing is new or resembles a previous mode.

Conclusion: Fenrir offers a generalizable framework for detecting and characterizing routing dynamics, enabling operators to assess TE effects, detect third-party routing changes, and recognize recurring routing modes across diverse networking contexts.

Abstract: Routing is central to networking performance, including: (1) latency in
anycast services and websites served from multiple locations,(2) networking
expenses and throughput in multi-homed enterprises, (3) the ability to keep
traffic domestic when considering data sovereignty. However, understanding and
managing how routing affects these services is challenging. Operators use
Traffic Engineering (TE) with BGP to optimize network performance, but what
they get is the result of all BGP policies throughout the Internet, not just
their local choices. Our paper proposes Fenrir, a new system to rediscover
recurring routing results. Fenrir can discover changes in network routing, even
when it happens multiple hops away from the observer. Fenrir also provides new
methods to quantify the degree of routing change, and to identify routing
"modes" that may reappear. Second, we show that Fenrir can be applied to many
different problems: we use five instances of three different types of systems
to illustrate the generalization: anycast catchments showing in a root DNS
service, route optimization for two multi-homed enterprises, and website
selection for two of the top-10 web services. Each type requires different
types of active measurements, data cleaning and weighting. We demonstrate
Fenrir's methods of detecting and quantifying change are helpful because they
all face similar operational questions: How much effect did traffic engineering
have? Did a third-party change alter my routing? In either case, is the current
routing new, or is it like a routing mode I saw before?

</details>


### [73] [Trust, But Verify: An Empirical Evaluation of AI-Generated Code for SDN Controllers](https://arxiv.org/abs/2510.20703)
*Felipe Avencourt Soares,Muriel F. Franco,Eder J. Scheid,Lisandro Z. Granville*

Main category: cs.NI

TL;DR: AI-generated POX controller code can be functional across three increasing-complexity networking tasks; ChatGPT and DeepSeek deliver higher consistency and quality, while Copilot and BlackBox.ai need more manual adjustments.


<details>
  <summary>Details</summary>
Motivation: Evaluate the reliability of generative AI tools in producing correct and functional network control software (POX controllers) in programmable networks, addressing a key gap between AI text/code generation and real-world networking correctness.

Method: Define three networking tasks with increasing complexity. For each task, apply zero-shot and few-shot prompting to four AI tools (ChatGPT, Copilot, DeepSeek, BlackBox.ai). Generate source code for a POX controller, then test the output in emulated Mininet topologies, assessing functionality, correctness, and the need for manual fixes.

Result: All evaluated models produced functional POX controllers. ChatGPT and DeepSeek showed higher consistency and code quality; Copilot and BlackBox.ai required more adjustments.

Conclusion: AI-generated network controller code can be functional, but tool-dependent quality varies. Some tools yield more reliable results with less manual intervention, while others require substantial post-generation refinement. This suggests cautious integration of AI-generated code in networking deployments and highlights the need for evaluation protocols.

Abstract: Generative Artificial Intelligence (AI) tools have been used to generate
human-like content across multiple domains (e.g., sound, image, text, and
programming). However, their reliability in terms of correctness and
functionality in novel contexts such as programmable networks remains unclear.
Hence, this paper presents an empirical evaluation of the source code of a POX
controller generated by different AI tools, namely ChatGPT, Copilot, DeepSeek,
and BlackBox.ai. To evaluate such a code, three networking tasks of increasing
complexity were defined and for each task, zero-shot and few-shot prompting
techniques were input to the tools. Next, the output code was tested in
emulated network topologies with Mininet and analyzed according to
functionality, correctness, and the need for manual fixes. Results show that
all evaluated models can produce functional controllers. However, ChatGPT and
DeepSeek exhibited higher consistency and code quality, while Copilot and
BlackBox.ai required more adjustments.

</details>


### [74] [AI-Enabled Digital Twins for Next-Generation Networks: Forecasting Traffic and Resource Management in 5G/6G](https://arxiv.org/abs/2510.20796)
*John Sengendo,Fabrizio Granelli*

Main category: cs.NI

TL;DR: AI-driven DTN with LSTM to forecast traffic and proactively manage resources, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional heuristic resource management in 5G/6G networks by using Digital Twin Networks with AI for autonomous, adaptive control.

Method: Integrate an LSTM neural network into the Digital Twin framework to forecast network traffic patterns and proactively allocate resources; evaluate via analytical experiments against baseline methods.

Result: AI-Enabled DT framework shows superior performance compared to baseline methods.

Conclusion: Embedding AI capabilities within Digital Twins enables autonomous, adaptive, high-performance network management for future mobile networks.

Abstract: As 5G and future 6G mobile networks become increasingly more sophisticated,
the requirements for agility, scalability, resilience, and precision in
real-time service provisioning cannot be met using traditional and
heuristic-based resource management techniques, just like any advancing
technology. With the aim of overcoming such limitations, network operators are
foreseeing Digital Twins (DTs) as key enablers, which are designed as dynamic
and virtual replicas of network infrastructure, allowing operators to model,
analyze, and optimize various operations without any risk of affecting the live
network. However, for Digital Twin Networks (DTNs) to meet the challenges faced
by operators especially in line with resource management, a driving engine is
needed. In this paper, an AI (Artificial Intelligence)-driven approach is
presented by integrating a Long Short-Term Memory (LSTM) neural network into
the DT framework, aimed at forecasting network traffic patterns and proactively
managing resource allocation. Through analytical experiments, the AI-Enabled DT
framework demonstrates superior performance benchmarked against baseline
methods. Our study concludes that embedding AI capabilities within DTs paves
the way for fully autonomous, adaptive, and high-performance network management
in future mobile networks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [75] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tramèr,Erik Jenner*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [76] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: 提出一个以策略为驱动的RAG架构用于受监管工作流的审计就绪生成，形成三个组成部分的三联结构以实现输出符合政策、证据可溯源、可验证地披露给审计者。


<details>
  <summary>Details</summary>
Motivation: 在受监管环境中，需对模型输出进行合规性与出处证据的可验证性，防错与审计缺口，需兼容如EU AI Act等法规要求。

Method: 提出三部式体系：I Contracts/Control（SHRDLU式）约束输出遵循法律与内部策略；II Manifests/Trails（Memex式）将所有引用的证据进行加密锚定，确保可验证的溯源；III Receipts/Verification（Xanadu式）提供最终、可携带的合规证明（COSE/JOSE）。输出采取前置门控，绑定可验证证据；设定未验证目标的NO-GO门控前提与可选的Pilot预注册。

Result: 尚未给出实验结果，提供的是设计与目标指标：目标相对减少confident errors≥20%、p95 latency≤900 ms、成本≤2.2x等，以及可选的NO-GO门控预注册的试点。

Conclusion: 该设计在现有RAG/ guardrails基础上增加可审计、可重复、凭证支持的合规性检查，适用于制药、医疗器械、金融、法律、公共部门等对审计跟踪要求高的领域；未来评估可能公开负向结果。

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


### [77] [A Proactive Insider Threat Management Framework Using Explainable Machine Learning](https://arxiv.org/abs/2510.19883)
*Selma Shikonde,Mike Wa Nkongolo*

Main category: cs.CR

TL;DR: 提出 IT-XML 框架，将 CRISP-DM 与隐藏马尔可夫模型结合，用于预测、评估并改进组织的内部威胁防护，结合问卷数据与模型解释性，达到约 92% 的分类准确率和 97-98% 的置信度。


<details>
  <summary>Details</summary>
Motivation: 内部威胁在数据敏感行业尤为突出，现有安全成熟度评估不足以实现主动防护；需要一个可解释的、面向数据驱动的框架来提升决策质量。

Method: 定量研究，采用在线问卷评估员工对内部威胁模式、访问控制、隐私和政策的认知；将 CRISP-DM 流程与 HMM 相结合进行模式识别；使用 Random Forest 判断安全成熟度，SHAP/LIME 提供可解释性；在三家大型数据密集型组织中进行评估。

Result: 所有组织在发展中级安全成熟度水平，置信度 97-98%；分类准确率 91.7%；识别到审计日志访问限制为最关键控制；随机森林表明供应商漏洞通知和定期审计日志审阅是提升韧性的关键因素（重要性0.081、0.052）；SHAP/LIME 提升模型透明度。

Conclusion: IT-XML 框架提供基于调查的评估、HMM 驱动的模式识别和证据驱动的建议，有望加强内部威胁管理并支持主动防护决策。

Abstract: Over the years, the technological landscape has evolved, reshaping the
security posture of organisations and increasing their exposure to
cybersecurity threats, many originating from within. Insider threats remain a
major challenge, particularly in sectors where cybersecurity infrastructure,
expertise, and regulations are still developing. This study proposes the
Insider Threat Explainable Machine Learning (IT-XML) framework, which
integrates the Cross-Industry Standard Process for Data Mining (CRISP-DM) with
Hidden Markov Models (HMM) to enhance proactive insider threat management and
decision-making. A quantitative approach is adopted using an online
questionnaire to assess employees' knowledge of insider threat patterns, access
control, privacy practices, and existing policies across three large
data-sensitive organisations. The IT-XML framework provides assessment
capabilities through survey-based data, HMM-driven pattern recognition for
security maturity classification, and evidence-based recommendations for
proactive threat mitigation. The framework classified all organisations at the
developing security maturity level with 97-98% confidence and achieved a
classification accuracy of 91.7%, identifying audit log access limits as the
most critical control. Random Forest analysis highlighted vendor breach
notifications (0.081) and regular audit log reviews (0.052) as key determinants
of resilience. Explainability methods such as SHAP and LIME improved model
transparency and interpretability, demonstrating the framework's potential to
strengthen insider threat management practices.

</details>


### [78] [Analysis and Comparison of Known and Randomly Generated S-boxes for Block Ciphers](https://arxiv.org/abs/2510.19885)
*James Kim*

Main category: cs.CR

TL;DR: 用代数结构和有限域理论构造的S盒具备可证明的密码学属性；与随机构造对比，评估随机性弱点，并在简单SPN中与成熟设计对比。


<details>
  <summary>Details</summary>
Motivation: 通过严格的代数/有限域构造替代纯随机方法，以获得高非线性、APN特性和雪崩效应等性质，同时量化随机方法的不足。

Method: 生成大量基于代数与有限域的S盒，与随机产生的S盒及带循环约束的随机排列进行对比；在简单的SPN框架下评估非线性度、APN、雪崩性与平衡性，给出随机排列的平均性能度量并与成熟设计比较。

Result: 数学构造的S盒在理论属性上提供可证明的保证；实验/分析显示在若干指标上优于随机方法。随机排列的性能较为分散，带循环约束的随机排列的性能介于完全随机与成熟设计之间；在简单SPN设置中，与知名设计进行对比，体现出代数构造的竞争力。

Conclusion: 总体上，基于代数与有限域理论的系统性S盒构造在提供安全性保证方面优于纯随机方法，并为随机性研究提供了量化基准。

Abstract: Mathematically constructed S-boxes arise from algebraic structures and finite
field theory to ensure strong, provable cryptographic properties. These
mathematically grounded constructions allow for generation of thousands of
S-Boxes with high nonlinearity, APN properties, and balanced avalanche
characteristics, unlike fully random methods, which lack such theoretical
guarantees in exchange for low complexity and more varied results. In this
work, we compare mathematically constructed constructions with randomly
generated ones to evaluate the relative weakness of the latter. We also
establish an average measure of performance for randomly generated
permutations, as well as random with forced cycle constraints, and compare them
to well-established designs in a simple SPN setting.

</details>


### [79] [SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment](https://arxiv.org/abs/2510.19979)
*Tushar Nayan,Ziqi Zhang,Ruimin Sun*

Main category: cs.CR

TL;DR: 提出SecureInfer，一种在异构TEE-GPU架构上进行高性能LLM推理的混合框架，通过信息论和威胁信息驱动的分区，将安全敏感组件放入SGX�������� enclave，其余线性运算在加密后移交GPU执行，再在enclave内恢复，从而在保护模型隐私的同时保持较高性能。


<details>
  <summary>Details</summary>
Motivation: 在移动与边缘设备上部署大语言模型时，如何在不牺牲性能的前提下保护模型隐私，避免模型被提取，且应对使用不可信的加速器（如GPU）时的安全-性能权衡。

Method: 提出一个基于信息论与威胁信息驱动的分区策略的混合TEE-GPU体系结构SecureInfer。按照 outsourcing 方案，将涉及非线性层、注意力头投影、FNN变换和LoRA适配器等安全敏感组件放入SGX enclave；而线性运算（矩阵乘法）在加密后由GPU执行，结果以加密形式在enclave中恢复并保护。以LLaMA-2为原型实现进行性能与安全性评估。

Result: 原型在LLaMA-2上的实现表明，在提供强安全保障的同时，性能具有可接受范围，具备在设备端进行安全推理的实际可行性。

Conclusion: SecureInfer通过将高敏感计算置于TEEs中、将大量线性运算交给未受信任加速器并在加密状态下处理，实现在隐私保护与性能之间更优的权衡，为落地的安全本地推理提供了可行路径，但仍需关注对更广泛模型的适配性及潜在的安全风险（如侧信道攻击、TEE实现的信任边界等）。

Abstract: With the increasing deployment of Large Language Models (LLMs) on mobile and
edge platforms, securing them against model extraction attacks has become a
pressing concern. However, protecting model privacy without sacrificing the
performance benefits of untrusted AI accelerators, such as GPUs, presents a
challenging trade-off. In this paper, we initiate the study of high-performance
execution on LLMs and present SecureInfer, a hybrid framework that leverages a
heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate
privacy-critical components while offloading compute-intensive operations to
untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts
an information-theoretic and threat-informed partitioning strategy:
security-sensitive components, including non-linear layers, projection of
attention head, FNN transformations, and LoRA adapters, are executed inside an
SGX enclave, while other linear operations (matrix multiplication) are
performed on the GPU after encryption and are securely restored within the
enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and
evaluate it across performance and security metrics. Our results show that
SecureInfer offers strong security guarantees with reasonable performance,
offering a practical solution for secure on-device model inference.

</details>


### [80] [zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements](https://arxiv.org/abs/2510.20007)
*To-Wen Liu,Matthew Green*

Main category: cs.CR

TL;DR: 提出一种名为 zk-agreements 的协议，将零知识证明、私有两方计算和智能合约结合，在区块链环境中实现隐私保留的可计算执行与强制执行，解决透明性与保密性的矛盾。


<details>
  <summary>Details</summary>
Motivation: 当前数字交易规模巨大，但纸质/文本契约在自动化、可执行性和纠纷解决方面形成瓶颈；自然语言合约易产生歧义且缺乏计算可验证性；区块链智能合约虽提升执行力，但易暴露敏感条款，阻碍企业采用。

Method: 将零知识证明用于保护私有条款、采用安全两方计算实现对私有合规性的隐私评估、结合智能合约以保障自动执行，三者协同实现隐私保护与计算可执行性。

Result: 理论上实现隐私保留的可计算可执行合约，提升区块链上合约的可验证性与自动化，同时兼顾隐私与透明性的需求。

Conclusion: 通过在区块链合约中整合隐私保护与可计算强制执行，缓解透明性与保密性之间的张力，促进企业对可计算隐私友好合约的采用。

Abstract: Digital transactions currently exceed trillions of dollars annually, yet
traditional paper-based agreements remain a bottleneck for automation,
enforceability, and dispute resolution. Natural language contracts introduce
ambiguity, require manual processing, and lack computational verifiability, all
of which hinder efficient digital commerce. Computable legal contracts,
expressed in machine-readable formats, offer a potential solution by enabling
automated execution and verification. Blockchain-based smart contracts further
strengthen enforceability and accelerate dispute resolution; however, current
implementations risk exposing sensitive agreement terms on public ledgers,
raising serious privacy and competitive intelligence concerns that limit
enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from
paper-based trust to cryptographic trust while preserving confidentiality. Our
design combines zero-knowledge proofs to protect private agreement terms,
secure two-party computation to enable private compliance evaluation, and smart
contracts to guarantee automated enforcement. Together, these components
achieve both privacy preservation and computational enforceability, resolving
the fundamental tension between transparency and confidentiality in
blockchain-based agreements.

</details>


### [81] [SAID: Empowering Large Language Models with Self-Activating Internal Defense](https://arxiv.org/abs/2510.20129)
*Yulong Chen,Yadong Liu,Jiawen Zhang,Mu Li,Chao Huang,Jie Wen*

Main category: cs.CR

TL;DR: 提出 Self-Activating Internal Defense (SAID) 的训练-free 安全防御框架，通过模型内部的意图蒸馏、最优安全前缀探测和保守聚合来主动识别并中和恶意意图，显著降低有害输出且保持正常任务性能，计算开销低。


<details>
  <summary>Details</summary>
Motivation: LLMs 仍易受越狱攻击，现有防御多依赖外部干预（输入过滤、输出修改），泛化性差、降低模型效用且计算成本高。需一种训练无关、鲁棒且可扩展的防御方法。

Method: 提出三阶段管线：1) 模型本身的意图蒸馏，将核心语义提取；2) 最优安全前缀探测，唤醒潜在的安全感知；3) 保守聚合策略，确保鲁棒决策。该方法为训练-free 的内部防御。

Result: 在五个开源大模型上对六种高级越狱攻击进行广泛评估，SAID 显著优于现有防御，降低有害输出，同时保持对无害任务的性能，且计算开销最小。

Conclusion: 激活 LLM 内在的安全机制是构建更安全、可扩展的对齐AI系统的更稳健路径。

Abstract: Large Language Models (LLMs), despite advances in safety alignment, remain
vulnerable to jailbreak attacks designed to circumvent protective mechanisms.
Prevailing defense strategies rely on external interventions, such as input
filtering or output modification, which often lack generalizability and
compromise model utility while incurring significant computational overhead. In
this work, we introduce a new, training-free defense paradigm, Self-Activating
Internal Defense (SAID), which reframes the defense task from external
correction to internal capability activation. SAID uniquely leverages the LLM's
own reasoning abilities to proactively identify and neutralize malicious intent
through a three-stage pipeline: model-native intent distillation to extract
core semantics, optimal safety prefix probing to activate latent safety
awareness, and a conservative aggregation strategy to ensure robust
decision-making. Extensive experiments on five open-source LLMs against six
advanced jailbreak attacks demonstrate that SAID substantially outperforms
state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves
this while preserving model performance on benign tasks and incurring minimal
computational overhead. Our work establishes that activating the intrinsic
safety mechanisms of LLMs is a more robust and scalable path toward building
safer and more reliable aligned AI systems.

</details>


### [82] [Separating Pseudorandom Generators from Logarithmic Pseudorandom States](https://arxiv.org/abs/2510.20131)
*Mohammed Barhoush*

Main category: cs.CR

TL;DR: 本文给出量子黑箱分离：在一个反向访问的量子伪随机状态(PRS)与伪随机发生器(PRG)之间，存在对所有黑箱构造的不可相容性，在对数规模和线性规模的PRS下均成立。直接推论为PRG与若干由对数PRS推导出的公钥数字签名与量子公钥加密等原语之间的分离。


<details>
  <summary>Details</summary>
Motivation: 澄清在量子设定中，PRG是否能从对数规模的PRS中实现，以及PRG是否能从线性规模的PRS中实现；填补PRG与PRS在量子黑箱模型中的分离空白，揭示PRG在密码学应用中的底层地位。

Method: 给出一个具有反向访问的单位量子伪任意态黑箱（unitary quantum oracle），证明在该oracle相对下不存在从PRS（对数或线性规模）到PRG的黑箱构造。该结果再推导出PRG与由对数PRS所隐含的若干原语（如数字签名、量子公钥加密）的分离。

Result: 在两种PRS规模下建立了量子黑箱分离：PRG不可从PRS（无论对数还是线性规模）以黑箱方式实现。该结论直接导致PRG与若干由对数PRS隐含的原语的分离。

Conclusion: 揭示在量子环境中，PRG在黑箱模型下相对于对数规模PRS具备严格更强的能力；也指出要把PRG从PRS中替代，需要超越纯粹的黑箱方法，对量子安全原语的结构依赖进行更深入的研究。

Abstract: Pseudorandom generators (PRGs) are a foundational primitive in classical
cryptography, underpinning a wide range of constructions. In the quantum
setting, pseudorandom quantum states (PRSs) were proposed as a potentially
weaker assumption that might serve as a substitute for PRGs in cryptographic
applications. Two primary size regimes of PRSs have been studied:
logarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to
powerful cryptographic applications, such as digital signatures and quantum
public-key encryption, that have not been realized from their linear
counterparts. However, PRGs have only been black-box separated from linear
PRSs, leaving open the fundamental question of whether PRGs are also separated
from logarithmic PRSs.
  In this work, we resolve this open problem. We establish a quantum black-box
separation between (quantum-evaluable) PRGs and PRSs of either size regime.
Specifically, we construct a unitary quantum oracle with inverse access
relative to which no black-box construction of PRG from (logarithmic or linear)
PRS exists. As a direct corollary, we obtain separations between PRGs and
several primitives implied by logarithmic PRSs, including digital signatures
and quantum public-key encryption.

</details>


### [83] [Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](https://arxiv.org/abs/2510.20223)
*Divyanshu Kumar,Shreyas Jena,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress,
yet remain critically vulnerable to adversarial attacks that exploit weaknesses
in cross-modal processing. We present a systematic study of multimodal
jailbreaks targeting both vision-language and audio-language models, showing
that even simple perceptual transformations can reliably bypass
state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts
across three high-risk safety categories harmful content, CBRN (Chemical,
Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation
Material) tested against seven frontier models. We explore the effectiveness of
attack techniques on MLLMs, including FigStep-Pro (visual keyword
decomposition), Intelligent Masking (semantic obfuscation), and audio
perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe
vulnerabilities: models with almost perfect text-only safety (0\% ASR) suffer
>75\% attack success under perceptually modified inputs, with FigStep-Pro
achieving up to 89\% ASR in Llama-4 variants. Audio-based attacks further
uncover provider-specific weaknesses, with even basic modality transfer
yielding 25\% ASR for technical queries. These findings expose a critical gap
between text-centric alignment and multimodal threats, demonstrating that
current safeguards fail to generalize across cross-modal attacks. The
accessibility of these attacks, which require minimal technical expertise,
suggests that robust multimodal AI safety will require a paradigm shift toward
broader semantic-level reasoning to mitigate possible risks.

</details>


### [84] [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)
*Yu Hin Chan,Hao Yang,Shiyu Shen,Xingyu Fan,Shengzhe Lyu,Patrick S. Y. Hung,Ray C. C. Cheung*

Main category: cs.CR

TL;DR: 提出了一种硬件加速的混合同态加密（HHE）架构，用于在边缘设备实现低功耗、端到端的隐私保护机器学习推理，并在PYNQ-Z2上对MNIST进行实验，显著降低客户端加密延迟并提升硬件吞吐量。


<details>
  <summary>Details</summary>
Motivation: FHE在边缘场景的计算和通信开销高，难以落地。通过将对称加密与FHE结合的HHE，以及硬件加速，降低客户端成本和服务器端负担，从而实现更高效、可部署的PPML。

Method: 设计并实现一个以轻量对称加密为核心、兼容FHE的硬件加速器，构建端到端HHE框架并整合到完整的PPML流水线；包含多项微架构优化以提升性能与能效，同时在PYNQ‑Z2平台上对MNIST进行评估。

Result: 实验结果显示：客户端加密延迟降低超过50x，硬件吞吐量较现有FPGA基HHE加速器提升约2x。

Conclusion: 证明低功耗、硬件加速的HHE在边缘部署的可行性，提供软硬件协同设计方法学，促进可扩展、安全的边缘PPML系统的发展。

Abstract: Privacy-preserving machine learning (PPML) is an emerging topic to handle
secure machine learning inference over sensitive data in untrusted
environments. Fully homomorphic encryption (FHE) enables computation directly
on encrypted data on the server side, making it a promising approach for PPML.
However, it introduces significant communication and computation overhead on
the client side, making it impractical for edge devices. Hybrid homomorphic
encryption (HHE) addresses this limitation by combining symmetric encryption
(SE) with FHE to reduce the computational cost on the client side, and
combining with an FHE-friendly SE can also lessen the processing overhead on
the server side, making it a more balanced and efficient alternative. Our work
proposes a hardware-accelerated HHE architecture built around a lightweight
symmetric cipher optimized for FHE compatibility and implemented as a dedicated
hardware accelerator. To the best of our knowledge, this is the first design to
integrate an end-to-end HHE framework with hardware acceleration. Beyond this,
we also present several microarchitectural optimizations to achieve higher
performance and energy efficiency. The proposed work is integrated into a full
PPML pipeline, enabling secure inference with significantly lower latency and
power consumption than software implementations. Our contributions validate the
feasibility of low-power, hardware- accelerated HHE for edge deployment and
provide a hardware- software co-design methodology for building scalable,
secure machine learning systems in resource-constrained environments.
Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x
reduction in client-side encryption latency and nearly a 2x gain in hardware
throughput compared to existing FPGA-based HHE accelerators.

</details>


### [85] [Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates](https://arxiv.org/abs/2510.20300)
*Haojie Ji,Long Jin,Haowen Li,Chongshi Xin,Te Hu*

Main category: cs.CR

TL;DR: 提出基于格式保持加密的高精度隐私保护方法，用于汽车定位数据，解决重复加密带来的精度损失，提升隐私安全，同时在保护数据的同时保证解密还原度。


<details>
  <summary>Details</summary>
Motivation: 随着汽车位置数据在导航、驾驶监控和车辆调度中的广泛共享，存在用户行为识别、敏感位置推断、轨迹重建等隐私风险，需要有效的隐私保护机制。

Method: 基于格式保持加密（FPE）的地理坐标处理，设计汽车坐标数据密钥映射机制以降低重复加密/解密导致的精度损失，并通过实验评估解密恢复、相对距离保持率RDR、热点区域数量等指标。

Result: 平均相对距离保持率RDR为0.0844；关键区域热点数量下降98.9%；解密恢复精度达到100%；表明所提方法在保持高精度的同时提升数据安全性。

Conclusion: 该工作提供了一种适用于自动驾驶数据共享的高精度隐私保护解决方案，兼顾数据可用性与安全性。

Abstract: There are increasing risks of privacy disclosure when sharing the automotive
location data in particular functions such as route navigation, driving
monitoring and vehicle scheduling. These risks could lead to the attacks
including user behavior recognition, sensitive location inference and
trajectory reconstruction. In order to mitigate the data security risk caused
by the automotive location sharing, this paper proposes a high-precision
privacy protection mechanism based on format-preserving encryption (FPE) of
geographical coordinates. The automotive coordinate data key mapping mechanism
is designed to reduce to the accuracy loss of the geographical location data
caused by the repeated encryption and decryption. The experimental results
demonstrate that the average relative distance retention rate (RDR) reached
0.0844, and the number of hotspots in the critical area decreased by 98.9%
after encryption. To evaluate the accuracy loss of the proposed encryption
algorithm on automotive geographical location data, this paper presents the
experimental analysis of decryption accuracy, and the result indicates that the
decrypted coordinate data achieves a restoration accuracy of 100%. This work
presents a high-precision privacy protection method for automotive location
data, thereby providing an efficient data security solution for the sensitive
data sharing in autonomous driving.

</details>


### [86] [Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses](https://arxiv.org/abs/2510.20314)
*Wu Yichao,Wang Yirui,Ding Panpan,Wang Hailong,Zhu Bingqian,Liu Chun*

Main category: cs.CR

TL;DR: 对 DRL 在对抗性攻击下的安全性与鲁棒性进行系统综述，提出基于扰动类型和攻击目标的分类框架，梳理攻击与防御方法，提供未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度强化学习在自动驾驶、智能制造、智慧医疗等领域的广泛应用，其在动态环境中的安全性与鲁棒性成为关键挑战；对抗性攻击可能导致性能下降甚至危险决策，因此需要提升稳健性。

Method: 提出基于扰动类型与攻击目标的对抗攻击分类框架；详细评述主流对 DRL 的攻击方法（状态空间、动作空间、奖励函数、模型空间等扰动）以及鲁棒性训练策略（对抗训练、竞争性训练、鲁棒学习、对抗检测、蒸馏防御等）并比较其优劣；展望未来方向。

Result: 给出一个系统性的综述与对比，总结现有攻击与防御方法的优点、局限性及适用场景，构建 DRL 对抗鲁棒性研究的理论框架。

Conclusion: 指明未来研究重点在提升泛化能力、降低计算复杂度、增强可扩展性与可解释性等方面，为该领域研究者提供参考与方向。

Abstract: With the wide application of deep reinforcement learning (DRL) techniques in
complex fields such as autonomous driving, intelligent manufacturing, and smart
healthcare, how to improve its security and robustness in dynamic and
changeable environments has become a core issue in current research. Especially
in the face of adversarial attacks, DRL may suffer serious performance
degradation or even make potentially dangerous decisions, so it is crucial to
ensure their stability in security-sensitive scenarios. In this paper, we first
introduce the basic framework of DRL and analyze the main security challenges
faced in complex and changing environments. In addition, this paper proposes an
adversarial attack classification framework based on perturbation type and
attack target and reviews the mainstream adversarial attack methods against DRL
in detail, including various attack methods such as perturbation state space,
action space, reward function and model space. To effectively counter the
attacks, this paper systematically summarizes various current robustness
training strategies, including adversarial training, competitive training,
robust learning, adversarial detection, defense distillation and other related
defense techniques, we also discuss the advantages and shortcomings of these
methods in improving the robustness of DRL. Finally, this paper looks into the
future research direction of DRL in adversarial environments, emphasizing the
research needs in terms of improving generalization, reducing computational
complexity, and enhancing scalability and explainability, aiming to provide
valuable references and directions for researchers.

</details>


### [87] [NeuPerm: Disrupting Malware Hidden in Neural Network Parameters by Leveraging Permutation Symmetry](https://arxiv.org/abs/2510.20367)
*Daniel Gilkarov,Ran Dubin*

Main category: cs.CR

TL;DR: NeuPerm leverages neural network permutation symmetry to disrupt hidden malware in shared pretrained models with negligible impact on performance, effective against state-of-the-art attacks, including LLMs.


<details>
  <summary>Details</summary>
Motivation: Pretrained model sharing brings significant value but also cyber threats: attackers can embed self-executing malware in model parameters or as a dependency. There is a need for lightweight defenses that preserve model utility while mitigating such threats.

Method: NeuPerm applies a permutation-symmetry-based perturbation to neural network parameters to disrupt concealed malware without noticeably affecting model performance. It avoids heavy processes like quantization and is demonstrated on large models including LLMs.

Result: Empirical results show NeuPerm disrupts state-of-the-art parameter- or dependency-based malware attacks with minimal degradation to utility; the method works on LLMs as well.

Conclusion: NeuPerm offers a practical, scalable defense for model-sharing ecosystems, enabling safer reuse of pretrained models with low overhead; source code is available at the provided GitHub link.

Abstract: Pretrained deep learning model sharing holds tremendous value for researchers
and enterprises alike. It allows them to apply deep learning by fine-tuning
models at a fraction of the cost of training a brand-new model. However, model
sharing exposes end-users to cyber threats that leverage the models for
malicious purposes. Attackers can use model sharing by hiding self-executing
malware inside neural network parameters and then distributing them for
unsuspecting users to unknowingly directly execute them, or indirectly as a
dependency in another software. In this work, we propose NeuPerm, a simple yet
effec- tive way of disrupting such malware by leveraging the theoretical
property of neural network permutation symmetry. Our method has little to no
effect on model performance at all, and we empirically show it successfully
disrupts state-of-the-art attacks that were only previously addressed using
quantization, a highly complex process. NeuPerm is shown to work on LLMs, a
feat that no other previous similar works have achieved. The source code is
available at https://github.com/danigil/NeuPerm.git.

</details>


### [88] [Risk Psychology & Cyber-Attack Tactics](https://arxiv.org/abs/2510.20657)
*Rubens Kim,Stephan Carney,Yvonne Fonken,Soham Hans,Sofia Hirschmann,Stacy Marsella,Peggy Wu,Nikolos Gurney*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We examine whether measured cognitive processes predict cyber-attack
behavior. We analyzed data that included psychometric scale responses and
labeled attack behaviors from cybersecurity professionals who conducted
red-team operations against a simulated enterprise network. We employed
multilevel mixed-effects Poisson regression with technique counts nested within
participants to test whether cognitive processes predicted technique-specific
usage. The scales significantly predicted technique use, but effects varied by
technique rather than operating uniformly. Neither expertise level nor
experimental treatment condition significantly predicted technique patterns,
indicating that cognitive processes may be stronger drivers of technique
selection than training or experience. These findings demonstrate that
individual cognitive differences shape cyber-attack behavior and support the
development of psychology-informed defense strategies.

</details>


### [89] [RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines](https://arxiv.org/abs/2510.20768)
*Austin Jia,Avaneesh Ramesh,Zain Shamsi,Daniel Zhang,Alex Liu*

Main category: cs.CR

TL;DR: 通过将 PageRank 等源可信度算法引入检索增强生成，提升 RAG 在 CTI 场景中的鲁棒性；在 MS MARCO 上验证降低恶意文档权重、提升可信内容。


<details>
  <summary>Details</summary>
Motivation: CTI 场景中 RAG 易受投毒攻击；现有防御在新兴威胁和对手模仿格式时效果不足，需提升源可信度以抑制恶意信息。

Method: 在语料库上应用源可信度排序（以 PageRank 为例），结合检索结果重新排序；在 MS MARCO 数据集及 CTI 文档/信息流上评估对恶意文档的权重下降和可信内容的提升。

Result: 量化显示对恶意文档赋予更低的权威分，对可信内容提升，MS MARCO 上与 CTI 数据的初步证明。

Conclusion: 将源可信度引入 RAG 能显著提升 CTI 场景下的鲁棒性，具备在真实系统中的应用潜力，未来工作包括更大规模评估和对抗性测试。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant
architectural pattern to operationalize Large Language Model (LLM) usage in
Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to
poisoning attacks, and previously proposed defenses can fail for CTI contexts
as cyber threat information is often completely new for emerging attacks, and
sophisticated threat actors can mimic legitimate formats, terminology, and
stylistic conventions. To address this issue, we propose that the robustness of
modern RAG defenses can be accelerated by applying source credibility
algorithms on corpora, using PageRank as an example. In our experiments, we
demonstrate quantitatively that our algorithm applies a lower authority score
to malicious documents while promoting trusted content, using the standardized
MS MARCO dataset. We also demonstrate proof-of-concept performance of our
algorithm on CTI documents and feeds.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [90] [Excitation of Looped Bistable Bands for High-Speed Linear Actuation](https://arxiv.org/abs/2510.19834)
*Sareum Kim,Josie Hughes*

Main category: eess.SY

TL;DR: 通过环形的双稳态带状弹簧实现高频线性位移放大，在共振条件下将输入振动转化为放大的线性运动，展示其在软体机器人中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在软体机器人领域，寻找能将振动能量高效放大为线性位移的高速执行元件。

Method: 将带状弹簧环形化以形成两关节，置于直线导轨并由曲柄机构驱动，改变驱动频率以研究共振时的放大效应。

Result: 观测到在共振条件下输入的振动被显著放大为线性位移，验证了环形双稳态带状弹簧的高速度往复线性运动能力。

Conclusion: 双稳态带状弹簧具有作为高速度线性执行元件的潜力，未来工作可优化结构、耦合与阻尼以实现更稳定的高效放大。

Abstract: Soft robotics increasingly relies on smart materials and innovative
structures, with bistable tape springs emerging as a promising option. These
structures exhibit intriguing dynamic behaviors, such as oscillation, due to
their inherent bistability. This paper explores the high-speed linear
amplification of motion achieved through the excitation of a looped bistable
tape spring. When looped, the tape spring forms two distinct joints,
facilitating smooth oscillation. Mounted on a linear guide and driven by a
crank mechanism with varying frequency, the system converts input oscillations
into amplified linear motion at resonance. This study highlights the potential
of bistable tape springs high speed reciprocating linear motion.

</details>


### [91] [SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance](https://arxiv.org/abs/2510.20113)
*Haowei Lou,Chengkai Huang,Hye-young Paik,Yongquan Hu,Aaron Quigley,Wen Hu,Lina Yao*

Main category: eess.SY

TL;DR: SpeechAgent是一个面向语言障碍用户的移动端智能助理，通过将大语言模型推理与高级语音处理模块结合，实现端到端的可定制、低延迟的日常沟通辅助。


<details>
  <summary>Details</summary>
Motivation: 许多患有构音障碍、口吃和失语者在日常沟通中受限。尽管ASR和TTS有进展，但可访问的网页/移动基础设施仍然不足，阻碍技术的实际应用。需要在移动端实现实时、易用的辅助沟通工具。

Method: 提出SpeechAgent，将大语言模型驱动的推理与语音处理模块整合，建立面向多种言语障碍的自适应支持。设计可在移动端/边缘设备实现的结构化部署流水线，确保实时、几乎无感知的延迟，同时保持高准确性与语音质量。

Result: 在真实的受损语音数据集和边缘设备延迟分析上评估，SpeechAgent在效果与易用性方面得到验证，显示出对日常个性化辅助沟通的可行性。

Conclusion: 证明了在日常生活中提供个性化、可用的语音辅助通信的可行性，并为无障碍沟通的广泛应用奠定基础。

Abstract: Speech is essential for human communication, yet millions of people face
impairments such as dysarthria, stuttering, and aphasia conditions that often
lead to social isolation and reduced participation. Despite recent progress in
automatic speech recognition (ASR) and text-to-speech (TTS) technologies,
accessible web and mobile infrastructures for users with impaired speech remain
limited, hindering the practical adoption of these advances in daily
communication. To bridge this gap, we present SpeechAgent, a mobile SpeechAgent
designed to facilitate people with speech impairments in everyday
communication. The system integrates large language model (LLM)- driven
reasoning with advanced speech processing modules, providing adaptive support
tailored to diverse impairment types. To ensure real-world practicality, we
develop a structured deployment pipeline that enables real-time speech
processing on mobile and edge devices, achieving imperceptible latency while
maintaining high accuracy and speech quality. Evaluation on real-world impaired
speech datasets and edge-device latency profiling confirms that SpeechAgent
delivers both effective and user-friendly performance, demonstrating its
feasibility for personalized, day-to-day assistive communication.

</details>


### [92] [Interpolatory Approximations of PMU Data: Dimension Reduction and Pilot Selection](https://arxiv.org/abs/2510.20116)
*Sean Reiter,Mark Embree,Serkan Gugercin,Vassilis Kekatos*

Main category: eess.SY

TL;DR: 用低秩矩阵逼近对PMU数据进行降维，ID通过少量行/列重构完整数据，配合DEIM实现在线误差估计和故障检测。


<details>
  <summary>Details</summary>
Motivation: 提升PMU数据的传输带宽和实时监测能力，需在有限测量下实现数据重建与异常检测。

Method: 采用插值矩阵分解(ID)替代PCA/SVD，只用少量行（PMU数据流）和/或少量列（时间快照）来重构完整数据；通过离散经验插值方法(DEIM)选择用于ID的行列，并得到可计算的重建误差界。

Result: 在合成PMU数据上对ID和DEIM进行了数值测试，DEIM在数据压缩方面表现出色，并验证了基于DEIM的故障检测方法。

Conclusion: ID结合DEIM提供了一种有效的实时数据压缩与故障检测框架，适用于电力系统的实时监控。

Abstract: This work investigates the reduction of phasor measurement unit (PMU) data
through low-rank matrix approximations. To reconstruct a PMU data matrix from
fewer measurements, we propose the framework of interpolatory matrix
decompositions (IDs). In contrast to methods relying on principal component
analysis or singular value decomposition, IDs recover the complete data matrix
using only a few of its rows (PMU datastreams) and/or a few of its columns
(snapshots in time). This compression enables the real-time monitoring of power
transmission systems using a limited number of measurements, thereby minimizing
communication bandwidth. The ID perspective gives a rigorous error bound on the
quality of the data compression. We propose selecting rows and columns used in
an ID via the discrete empirical interpolation method (DEIM), a greedy
algorithm that aims to control the error bound. This bound leads to a
computable estimate for the reconstruction error during online operations. A
violation of this estimate suggests a change in the system's operating
conditions, and thus serves as a tool for fault detection. Numerical tests
using synthetic PMU data illustrate DEIM's excellent performance for data
compression, and validate the proposed DEIM-based fault-detection method.

</details>


### [93] [Design Optimization and Global Impact Assessment of Solar-Thermal Direct Air Carbon Capture](https://arxiv.org/abs/2510.20135)
*Zhiyuan Fan,Bolun Xu*

Main category: eess.SY

TL;DR: 太阳热直接空气捕集（DAC）结合砂基热能存储可实现高利用率（>80%）和低削减成本（160–200 USD/吨CO2）， stand-alone 太阳能DAC在高日照、沙质地区尤具潜力；6000吨/年模块设计土地需求<1平方公里，全球在砂质地貌的潜在产能>26亿吨/年；在有沉积盆地储存CO2时，太阳能DAC成本可能低于地热供热。


<details>
  <summary>Details</summary>
Motivation: 解决 decarbonization 与日益增长的全球能源需求的双重挑战，降低 DAC 的能量强度和成本，探索以集中式太阳热能和低成本砂基热储为基础的可扩展 DAC 方案。

Method: 将集中太阳热技术与砂基热储结合，进行可行性与经济性分析，比较并网和独立供能两种配置；采用短循环吸附剂以匹配日照可用性；提出6000吨/年的模块化设计并评估土地利用和全球潜力。

Result: 太阳热DAC可实现>80%的年利用率；CO2 去除成本约为160–200美元/吨；独立供能（仅靠太阳能）系统在高日照、沙质地区表现尤佳，对温度与湿度的环境扰动较小；单模块设计土地<1平方公里，全球砂质地貌潜在产能>26 Gt/年；在适合的沉积盆地储存 CO2 的地区，太阳能供能DAC成本低于地热加热方案。

Conclusion: 太阳热DAC配合砂基热储是一种有竞争力且具扩展性的CO2移除路线，特别适合日照充足、沙质地区；在合适盆地及地质条件下，具备显著放大潜力和较低土地占用，未来需重点在材料、系统集成与大规模部署路径上推进。

Abstract: The dual challenge of decarbonizing the economy and meeting rising global
energy demand underscores the need for scalable and cost-effective carbon
dioxide removal technologies. Direct air capture (DAC) is among the most
promising approaches, but its high energy intensity, particularly the thermal
energy required for sorbent regeneration, remains a critical barrier to cost
reduction and sustainable deployment. This study explores solar-thermal DAC
systems that combine concentrated solar thermal technology with low-cost
sand-based thermal energy storage to meet this demand. We analyze the
techno-economic performance of such systems in both grid-connected and
stand-alone configurations. Results show that solar-thermal DAC can achieve
annual capacity factors exceeding 80% and CO2 removal costs as low as 160-200
USD per ton, making it competitive with leading DAC technologies. The proposed
system operates most efficiently with short-cycle sorbents that align with
solar availability. The stand-alone Solar-DAC systems, which rely solely on
solar energy for both electricity and thermal energy, are particularly
promising in regions with high solar capacity and sandy terrain, exhibiting
minimal ambient sensitivity from temperature and humidity. An optimal 6000
ton/yr modular system design takes <1 km2 land-use requirement and potentially
>26 Gt/year DAC capacity is identified for sandy terrain alone globally. In
areas with sedimentary basins suitable for CO2 storage, solar-powered DAC
offers a lower-cost alternative to geothermal heating, which often faces
geological and economic constraints.

</details>


### [94] [Soft Switching Expert Policies for Controlling Systems with Uncertain Parameters](https://arxiv.org/abs/2510.20152)
*Junya Ikemoto*

Main category: eess.SY

TL;DR: 提出一种两阶段的仿真驱动强化学习算法，用于在不确定和时变系统参数条件下控制真实系统。第一阶段在仿真中学习多种策略，第二阶段在真实系统上通过基于观测的在线凸优化实现策略的平滑切换，并通过数值实验验证可行性。


<details>
  <summary>Details</summary>
Motivation: 现实系统中的仿真-现实差距和参数不确定性使单阶段学习难以直接迁移到真实系统，需要一个能在变化参数下保持鲁棒的学习与切换机制。

Method: 阶段一：在含不同参数的仿真环境中学习多种控制策略。阶段二：在真实系统上，以观测为输入，采用在线凸优化框架对已有策略进行平滑切换，从而实现对参数变化的自适应响应。

Result: 通过数值实验验证了方法的可行性和潜在优势，显示在仿真-现实转移中有效缓解现实 gap 的趋势。

Conclusion: 该两阶段框架为在不确定和时变系统参数下进行鲁棒控制提供了一种可行的解决思路，能够缓解仿真与现实之间的差距，并为后续研究提供方向。

Abstract: This paper proposes a simulation-based reinforcement learning algorithm for
controlling systems with uncertain and varying system parameters. While
simulators are useful for safely learning control policies for physical
systems, mitigating the reality gap remains a major challenge. To address the
challenge, we propose a two-stage algorithm. In the first stage, multiple
control policies are learned for systems with different parameters in a
simulator. In the second stage, for a real system, the control policies learned
in the first stage are smoothly switched using an online convex optimization
algorithm based on observations. Our proposed algorithm is demonstrated through
numerical experiments.

</details>


### [95] [From Bundles to Backstepping: Geometric Control Barrier Functions for Safety-Critical Control on Manifolds](https://arxiv.org/abs/2510.20202)
*Massimiliano de Sa,Pio Ong,Aaron D. Ames*

Main category: eess.SY

TL;DR: 提出在流形上构建几何控制屏障函数（CBF）的通用理论，针对束上的系统，能够回收标准优化CBF控制器及其光滑版本，并将动能基CBF回退法推广至黎曼流形，给出几何力学系统的可构造CBF综合技术及易于验证的条件，并在SO(3)上的欠驱动卫星系统中给出应用示例。


<details>
  <summary>Details</summary>
Motivation: 在机器人与航天应用中，CBF在欧几里得空间有成熟理论，但在常见的流形上缺乏一般公式和可构造的综合工具。本工作旨在建立在束上的几何CBF理论，并使其可用于控制仿射系统，填补理论与实现之间的空白。

Method: 发展在束上的几何CBF理论；对控制仿射系统，能够回收已知的优化CBF控制器及其光滑版本；将动能基CBF回退法推广到黎曼流形，提出用于几何力学系统的可构造CBF综合技术，并给出成功的可验证条件；通过利用机械结构避免在高阶切丛上的计算；并将方法应用于SO(3)上的欠驱动卫星进行演示。

Result: 建立并展示了几何CBF在束上的理论框架，能够回收传统优化CBF控制器及光滑版本，给出对几何力学系统的可构造CBF综合路线和可验证条件；通过实例验证，尤其是在SO(3)的欠驱动卫星上。

Conclusion: 为几何力学系统提供一个可构造的CBF设计框架，利用机械结构提升计算效率，避免高阶切丛计算，拓展CBF工具到带有流形约束的应用领域，如航天器姿态控制等。

Abstract: Control barrier functions (CBFs) have a well-established theory in Euclidean
spaces, yet still lack general formulations and constructive synthesis tools
for systems evolving on manifolds common in robotics and aerospace
applications. In this paper, we develop a general theory of geometric CBFs on
bundles and, for control-affine systems, recover the standard
optimization-based CBF controllers and their smooth analogues. Then, by
generalizing kinetic energy-based CBF backstepping to Riemannian manifolds, we
provide a constructive CBF synthesis technique for geometric mechanical
systems, as well as easily verifiable conditions under which it succeeds.
Further, this technique utilizes mechanical structure to avoid computations on
higher-order tangent bundles. We demonstrate its application to an
underactuated satellite on SO(3).

</details>


### [96] [Observer-based Differentiators for Noisy Signals](https://arxiv.org/abs/2510.20234)
*Van Huynh,Hieu Trinh,Riley Bain*

Main category: eess.SY

TL;DR: 提出了一系列基于观测器的微分器，用以在存在噪声的情况下估计信号的导数。


<details>
  <summary>Details</summary>
Motivation: 在带噪声的环境中精确求导困难，作者通过将观测器作为微分器来实现对信号导数的稳健估计，提升在控制与诊断等场景中的可用性。

Method: 给出多种类型的基于观测器的微分器设计，作为对导数估计的实现手段，覆盖不同的观测器结构以抵抗噪声影响。

Result: 所提出的观测器微分器能够在信号带噪声时仍对导数给出估计值。

Conclusion: 基于观测器的微分器为噪声环境中的导数估计提供了一种稳健的方案，具有应用潜力。

Abstract: We present a collection of different types of observation systems that work
as differentiators. These observer-based differentiators can produce estimates
for derivatives of a given signal, even though the given signal is prone to
noise.

</details>


### [97] [On MIMO Stability Analysis Methods Applied to Inverter-Based Resources Connected to Power Systems](https://arxiv.org/abs/2510.20384)
*Anton A. Stoorvogel,Saeed Lotfifard,Ali Saberi*

Main category: eess.SY

TL;DR: 对基于变流器的资源的小信号稳定性分析方法进行批判性综述，聚焦其适用性、正确与错误的实现以及固有局限性和易混淆点。


<details>
  <summary>Details</summary>
Motivation: 随着并网的变流器资源比例上升，传统小信号稳定性分析方法的假设与适用性受到挑战，需要澄清各方法的目标、适用条件及易误解之处，以提升分析的准确性与可用性。

Method: 通过系统性文献回顾，梳理并划分主流的小信号稳定性分析方法（如特征值分析、阻抗法、时域仿真、降阶模型等），评估它们的目标、关键假设、实现要点与局限，结合示例阐释常见误用与误解来源。

Result: 提出一个对比框架，明确各方法的适用场景、优点与局限，揭示常见误解与错误做法，给出正确实现与解读的要点，帮助研究者在评估 IBR 主导电网的稳定性时做出更一致的选择。

Conclusion: 为 IBR 主导电网的小信号稳定性评估提供清晰的使用指南，强调方法与模型假设的一致性、谨慎解读以及可能需要的标准化实践，以避免误诊。

Abstract: This paper presents a critical review of methods
  commonly employed in the literature for small signal stability analysis of
  inverter based resources (IBRs). It discusses the intended purposes
  of these methods and outlines both their proper and improper
  implementations. The paper provides insights into the applicability
  of these techniques, clarifies their inherent limitations, and
  discusses and illustrates common sources of misinterpretation.

</details>


### [98] [Interlacing in Controllers Implementation: Frequency Analysis](https://arxiv.org/abs/2510.20394)
*Julian Salt*

Main category: eess.SY

TL;DR: 提出在LTI控制器实现中使用交错(interlacing)技术，并比较不同结构以降低资源受限环境中的计算负担，同时给出获取与实部/复数极点相关的模块的程序，利用离散提升(discrete lifting)建立时变系统模型，并提出一个高效的双速率(dual-rate)频响计算方法来刻画带交错控制器的控制环路，附以实例验证。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的实现环境中，如何以低代价实现和分析LTI控制器的行为，尤其在需要处理不同极点结构（实极点与复极点）以及时变特性的场景下，寻找有效的建模、计算和分析手段。

Method: 提出并比较多种用于LTI控制器的交错实现结构；给出提取与实际与复数极点相关的块的实用化步骤；通过合适的离散提升方法将时变系统建模为等效的时变离散系统；发展一种高效的双速率频响计算算法以描述带交错控制器的控制环路特性；通过实例对理论方法进行演示。

Result: 给出用于不同极点（实、复数）对应块的获取新程序；实现了一个高效的双速率频响计算框架以分析带交错控制器的系统特性；证明了在资源受限环境下的计算节约与可行性，并通过实例验证了理论要点。

Conclusion: 将交错控制器与离散提升技术结合，并辅以双速率频响分析，为资源受限条件下的LTI控制器实现提供了一条高效的分析与设计路径，具有较强的实用性与可扩展性。

Abstract: The main goal of this contribution is to explain how to use interlacing
techniques for LTI controllers implementation and analyze different struc-
tures in this environment. These considerations lead to an important com-
putation saving in constrained resource environments. It has been also intro-
duced new procedures for obtaining the blocks related to different real and
complex controllers poles. The resultant time-varying system is modeled using
proper discrete lifting techniques and a new and efficient dual-rate fre-
quency response computation allows to determine the characteristics of the
control loop with interlaced controller. Examples illustrate the theoretical
proposals.

</details>


### [99] [A Multifunctional Capacitive Sensing Platform for Wireless Vascular and Heart Monitoring](https://arxiv.org/abs/2510.20415)
*Parviz Zolfaghari,Beril Yagmur Koca,Taher Abbasiasl,Hakan Urey,Hadi Mirzajani*

Main category: eess.SY

TL;DR: A battery-free, antenna-integrated capacitive sensing platform (MAiCaS) enables passive, wireless cardiovascular monitoring by using the antenna parasitic capacitance as a strain sensor; demonstrated in epicardial patch, stent, and vascular graft with wireless S11-based readout and high sensitivity.


<details>
  <summary>Details</summary>
Motivation: Conventional cardiovascular monitoring systems require separate sensors and wireless modules, leading to increased complexity, size, and power consumption. There is a need for a compact, scalable, battery-free solution that can measure mechanical and hemodynamic signals in real time and remotely.

Method: Fabrication via a cleanroom-free, single-step UV laser patterning on a flexible PDMS substrate. The device uses the parasitic capacitance of an inductive antenna as a strain-sensitive element, integrating sensing, telemetry, and mechanics into a monolithic platform. Validation includes epicardial patch, stent, and graft configurations; wireless interrogation via resonance frequency shifts and S11 measurements under both unrolled and rolled states, across strain, pressure, and deformation.

Result: In vitro tests show resonance frequency shifts under physiological conditions; stable performance on skin, in PBS, human serum, and simulated vascular environments. Repeatability and aging tests indicate long-term reliability under cyclic loading. Calibration curves show high sensitivity: 2.9 MHz per 1% strain, 0.43 MHz per mmHg, and 309.6 kHz per μm for epicardial patch, graft, and stent configurations. Human experiments validated operation of the monolithic sensor architecture for battery-free monitoring of vascular dynamics.

Conclusion: MAiCaS provides a scalable, cost-effective, battery-free solution for continuous cardiovascular monitoring with potential for remote diagnostics, post-surgical follow-up, and management of vascular health.

Abstract: We present a multifunctional, antenna-integrated capacitive sensing (MAiCaS)
platform for passive, wireless, and real-time cardiovascular monitoring. Unlike
conventional systems that require separate sensors and wireless modules, our
device unifies sensing, telemetry, and mechanical functionality into a compact
and scalable design by exploiting the parasitic capacitance of an inductive
antenna as a strain-sensitive element. The sensor is fabricated using a
cleanroom-free, single-step UV laser patterning process on a flexible PDMS
substrate, reducing manufacturing complexity and enabling high reproducibility.
The MAiCaS is suitable for three different applications: as a sensor for
epicardial strain measurement, a stent as a sensor, and a vascular graft
sensor. We demonstrate MAiCaS's versatility by validating its wireless
resonance-based response to strain, pressure, and deformation across unrolled
and rolled forms. In vitro experiments demonstrated consistent resonance
frequency shifts under physiological conditions, with stable performance on
skin, in PBS, human serum, and simulated vascular environments. Repeatability
and aging tests confirmed its long-term reliability and elasticity under cyclic
loading. Calibration curves revealed high sensitivity across all
configurations, with wireless interrogation achieved through S11 parameter
measurements and resonance frequency shift as the output metric. The
sensitivity of the device was measured to be 2.9 MHz per 1% strain, 0.43
MHz/mmHg, and 309.6kHz/\textmu m for epicardial patch, graft, and stent
integrated sensor, respectively. The operation of MAiCaS was evaluated in a
human experiment. This monolithic sensor architecture provides a scalable and
cost-effective solution for battery-free monitoring of vascular dynamics, with
potential for remote diagnostics, post-surgical follow-up, and continuous
cardiovascular health management.

</details>


### [100] [Behavior-Aware Online Prediction of Obstacle Occupancy using Zonotopes](https://arxiv.org/abs/2510.20437)
*Alvaro Carrizosa-Rendon,Jian Zhou,Erik Frisk,Vicenc Puig,Fatiha Nejjari*

Main category: eess.SY

TL;DR: 基于 EKF+LP 的在线方法，估计出围绕车辆的紧凑 zonotopic 控制行动集合，并通过可达性分析预测未来占用区域。


<details>
  <summary>Details</summary>
Motivation: 在无先验信息的非结构化环境中，单从运动观测预测周围车辆的占用集对于自动驾驶的安全至关重要。

Method: 分两阶段：第一阶段使用扩展卡尔曼滤波（EKF）和线性规划（LP）来估计一个紧凑的 zonotopic 控制行动集合；第二阶段通过可达性分析将该集合传播以预测未来占用。

Result: 在城市环境的仿真中验证，方法能给出准确且紧凑的占用预测，并且不依赖先验假设或事先训练数据。

Conclusion: 该在线方法为在不依赖历史数据的情况下实现安全且实时的周围车辆占用预测提供了一种有效途径。

Abstract: Predicting the motion of surrounding vehicles is key to safe autonomous
driving, especially in unstructured environments without prior information.
This paper proposes a novel online method to accurately predict the occupancy
sets of surrounding vehicles based solely on motion observations. The approach
is divided into two stages: first, an Extended Kalman Filter and a Linear
Programming (LP) problem are used to estimate a compact zonotopic set of
control actions; then, a reachability analysis propagates this set to predict
future occupancy. The effectiveness of the method has been validated through
simulations in an urban environment, showing accurate and compact predictions
without relying on prior assumptions or prior training data.

</details>


### [101] [Safe Decentralized Density Control of Multi-Robot Systems using PDE-Constrained Optimization with State Constraints](https://arxiv.org/abs/2510.20643)
*Longchen Niu,Gennaro Notomista*

Main category: eess.SY

TL;DR: 去中心化优化型密度控制器用于多机器人系统的集合不变性约束，通过去中心化控制屏障函数确保局部安全性可导出全局安全性。在将机器人建模为遵循Fokker-Planck方程的空间密度函数并显式考虑定位与运动噪声的同时，提出比集中式方法更低的计算与通信开销的实现方案。已在四架四轴飞行器上通过仿真与实验验证。


<details>
  <summary>Details</summary>
Motivation: 在多机器人系统中实现全局安全性需克服对集中式控制的高通信、计算和对完美信息的依赖。通过去中心化的控制屏障函数，若局部安全约束得以满足，可以保证全局安全，从而提升在现实受限条件下的部署可行性。

Method: 提出去中心化优化密度控制框架；设计去中心化控制屏障函数（DCBF），推导充分条件，使局部约束确保全局安全；将机器人建模为由Fokker-Planck方程描述的空间概率密度函数，显式对定位与运动噪声进行建模；通过仿真与实际实验对比，验证其在资源受限环境中的性能。

Result: 在仿真和四架四轴飞行器的实验中，控制器实现了全局安全性约束，且相较传统集中式方案显示出更低的计算和通信开销，适合在通信和定位条件受限场景下使用。

Conclusion: 去中心化密度控制框架可在现实中实现全局安全的同时降低对高质量通信与定位的依赖，具备实际部署的可行性，适用于多机器人协同的安全控制任务。

Abstract: In this paper, we introduce a decentralized optimization-based density
controller designed to enforce set invariance constraints in multi-robot
systems. By designing a decentralized control barrier function, we derived
sufficient conditions under which local safety constraints guarantee global
safety. We account for localization and motion noise explicitly by modeling
robots as spatial probability density functions governed by the Fokker-Planck
equation. Compared to traditional centralized approaches, our controller
requires less computational and communication power, making it more suitable
for deployment in situations where perfect communication and localization are
impractical. The controller is validated through simulations and experiments
with four quadcopters.

</details>


### [102] [Sugar Shack 4.0: Implementation of a Cyber-Physical System for Logistic and Sanitary Automation in a Maple Syrup Boiling Center](https://arxiv.org/abs/2510.20682)
*Thomas Bernard,François Grondin,Jean-Michel Lavoie*

Main category: eess.SY

TL;DR: 设计并部署了一个面向过程的cyber-physical系统，用于集中枫糖煮糖中心的工厂级物流、可追溯性和清洁，采用事件驱动编排与本地服务器，实现共享管路的优先级仲裁、确定性例程与丰富传感，提升卫生、追溯和运营效率。


<details>
  <summary>Details</summary>
Motivation: 替代手工、零散的操作，落地工业4.0理念在枫糖煮糖场景中的应用，目标是实现模块化、可扩展的 plant-scale 自动化，提升卫生安全、追溯、降低人工工作量。

Method: 设计并部署基于本地服务器的事件驱动编排系统，采用可重复使用的设备抽象和集中联锁，结合共享管路的优先级仲裁；实现对交付、反渗透集成、蒸发器进料与渗滤液管理的确定性流程；通过在线流量、温度、糖度等传感测量驱动路由决策，执行清洗/冲洗（CIP）和转运的自动化；实现对仓储的动态分配、可追溯至蒸发器入口的完整记录。

Result: 2025生产季中，系统共排队431次作业、完成908次topstock/downstock平衡循环；渗滤液储备从22712 L增至约41640 L；消除了手工实践下的中季污染事件；账单与报告的行政工作从30+小时降至约1小时。

Conclusion: 展示了超越传统架构的模块化、工厂级自动化的实用路径，并为类似工厂或相关行业打下可复用组件的基础；也为 maple 4.0 技术在枫糖煮糖中心的首次科学文献化整合提供了实证。

Abstract: This paper presents the design and deployment of a process-aware
cyber-physical system that automates plant-level logistics, traceability, and
sanitation in a centralized maple-syrup boiling center. The system replaces
ad-hoc, manual operations with event-driven orchestration on a local server,
employing reusable device abstractions and a centralized interlock with
priority-based arbitration for shared piping. It implements deterministic
routines for delivery, reverse osmosis integration, evaporator feed, and
permeate management. The system is sensor rich: inline measurements of flow,
temperature, and sugar concentration (degrees Brix) drive routing decisions and
trigger systematic post-transfer rinses (cleaning-in-place), ensuring
consistent hygiene and complete, immediate traceability up to the evaporator
inlet. During the 2025 production season, the system queued 431 operations
without incident; executed 908 \enquote{topstock} and \enquote{downstock}
balancing cycles; increased usable permeate reserves from 22,712 to
approximately 41,640 L through dynamic storage assignment; eliminated
mid-season contamination incidents previously observed under manual practice;
and reduced administrative effort for billing and reporting from more than 30
hours to roughly 1 hour through automatic documentation. These results
demonstrate a practical path to modular, plant-scale automation beyond
traditional architectures, and lay the groundwork for packaging reusable
elements for similar plants or adjacent industries. This work is part of a
larger project involving the first scientifically-documented integration of
Industry 4.0 technologies in a maple syrup boiling center.

</details>


### [103] [Learning Optimal Power Flow with Pointwise Constraints](https://arxiv.org/abs/2510.20777)
*Damian Owerko,Anna Scaglione,Alejandro Ribeiro*

Main category: eess.SY

TL;DR: 提出一种面向OPF的学习参数化训练方法，在求解中直接将学习参数化插入带点对点约束的OPF，通过对偶域的增广拉格朗日与对偶梯度上升实现训练，实验表明能显著降低约束违约，且在边界情形和大规模母线系统中效果尤为突出。


<details>
  <summary>Details</summary>
Motivation: 解决OPF中的约束满足问题，改进以往仅对问题实例平均约束的监督学习方法，提升鲁棒性和可扩展性，特别是在包含大量母线的系统中。

Method: 把学习参数化直接嵌入OPF并在对偶域进行训练，使用增广拉格朗日和对偶梯度上升来实现点对点（instancewise）约束的优化和收敛。

Result: 数值实验显示点对点约束训练能降低约束违约；在边界情形（最难满足约束的 realization）中效果最好；在多母线的电力系统中收益最明显。

Conclusion: 点对点约束训练的对偶域方法可提高OPF的约束满足性和鲁棒性，尤其适用于大规模系统。

Abstract: Training learning parameterizations to solve optimal power flow (OPF) with
pointwise constraints is proposed. In this novel training approach, a learning
parameterization is substituted directly into an OPF problem with constraints
required to hold over all problem instances. This is different from existing
supervised learning methods in which constraints are required to hold across
the average of problem instances. Training with pointwise constraints is
undertaken in the dual domain with the use of augmented Lagrangian and dual
gradient ascent algorithm. Numerical experiments demonstrate that training with
pointwise constraints produces solutions with smaller constraint violations.
Experiments further demonstrated that pointwise constraints are most effective
at reducing constraint violations in corner cases - defined as those
realizations in which constraints are most difficult to satisfy. Gains are most
pronounced in power systems with large numbers of buses.

</details>


### [104] [Bilevel Analysis of Cost and Emissions Externalities from Data Center Load Shifting](https://arxiv.org/abs/2510.20805)
*Aron Brenner,Rahman Khorramfar,Nathan Engelman Lado,Line Roald,Saurabh Amin*

Main category: eess.SY

TL;DR: 在一个三结点电力系统中，数据中心通过双层优化分析负载位移对系统运行的影响，推导出关于数据中心和全系统目标的分段线性表达式，揭示拥塞与再生能源饱和下的阈值行为；给出在去中心化决策与社会最优之间可能一致或偏离的充分条件，以及拓扑与发电机不对称性对激励对齐的影响，并提出改进协同的方向。


<details>
  <summary>Details</summary>
Motivation: 探究灵活的数据中心在价格与边际排放强度信号作用下的负载响应，如何与电网约束与市场清算互动，从而评估去中心化决策是否会带来社会福利的提升或成本外部性。

Method: 建立一个双层优化框架：数据中心在权衡电力成本与边际排放强度的加权目标下进行负载位移优化；系统运营方在传输与发电约束下进行经济调度。对一个简化的三结点系统推导出关于数据中心负载位移的闭式分段线性目标表达，并揭示因拥塞和可再生能源饱和导致的 regime 变化。

Result: 给出数据中心与系统级目标随负载位移的分段线性表达，揭示在不同拥塞/可再生情形下的阈值切换；提出在何种条件下数据中心的去中心化决策会与社会最优对齐或偏离，并描述由拓扑与发电机不对称性引发的外部性及激励错配。

Conclusion: 为分析在碳感知激励下的去中心化灵活性提供一个可解析的起点，强调需要改进数据中心与系统运行之间的协调机制，以确保灵活负载的社会福利增益。

Abstract: Data centers are emerging as large, flexible electricity consumers capable of
shifting computational workloads across locations in response to economic and
environmental signals. While this flexibility has potential for emissions
reduction, its impact on power system operations depends critically on how such
behavior interacts with network constraints and market signals. We develop a
bilevel optimization framework in which a data center minimizes a weighted
combination of electricity cost and marginal emissions intensity (LME), while
the system operator clears economic dispatch under transmission and generation
constraints. Focusing on a stylized three-bus power system, we derive
closed-form, piecewise-linear expressions for both the data center and
system-wide objectives as functions of the data centers' load shift. These
expressions capture threshold-driven regime changes due to congestion and
renewable saturation. We identify sufficient conditions under which the data
center's decentralized decisions align with or diverge from socially optimal
behavior and characterize the resulting externalities. Our results reveal how
system topology and generator asymmetry affect incentive alignment and provide
insight into when marginal price or emissions signals may fail to guide
flexible loads toward socially beneficial outcomes. Our results offer a
tractable starting point for analyzing decentralized flexibility under
carbon-aware incentives and suggest directions for improving coordination
between flexible loads and system operations.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [105] [SSL-SE-EEG: A Framework for Robust Learning from Unlabeled EEG Data with Self-Supervised Learning and Squeeze-Excitation Networks](https://arxiv.org/abs/2510.19829)
*Meghna Roy Chowdhury,Yi Ding,Shreyas Sen*

Main category: eess.SP

TL;DR: 提出 SSL-SE-EEG 框架，将自监督学习与 SE-Net 相结合，通过将 EEG 转换为结构化的二维图像表示以提升鲁棒性和数据效率，在多数据集上达到领先准确度，具备实时低功耗潜力。


<details>
  <summary>Details</summary>
Motivation: 解决 EEG 在实际应用中的噪声伪影、数据缺失以及高标注成本等挑战，提升特征提取与鲁棒性，降低对标注数据的依赖。

Method: 将自监督学习与 SE-Net 集成用于 EEG 特征提取；将原始 EEG 信号转换为结构化的 2D 图像，以便深度学习处理；在 MindBigData、TUH-AB、SEED-IV、BCI-IV 等数据集上进行验证。

Result: 在 MindBigData 上达到 91% 的准确率，在 TUH-AB 上达到 85% 的准确率；并在 SEED-IV 与 BCI-IV 等数据集上进行验证，显示出较强的泛化性和竞争力。

Conclusion: SSL-SE-EEG 为低功耗、可扩展的 EEG 处理提供了一条可行路径，适用于生物医学信号分析、神经工程和未来的脑机接口。

Abstract: Electroencephalography (EEG) plays a crucial role in brain-computer
interfaces (BCIs) and neurological diagnostics, but its real-world deployment
faces challenges due to noise artifacts, missing data, and high annotation
costs. We introduce SSL-SE-EEG, a framework that integrates Self-Supervised
Learning (SSL) with Squeeze-and-Excitation Networks (SE-Nets) to enhance
feature extraction, improve noise robustness, and reduce reliance on labeled
data. Unlike conventional EEG processing techniques, SSL-SE-EEG} transforms EEG
signals into structured 2D image representations, suitable for deep learning.
Experimental validation on MindBigData, TUH-AB, SEED-IV and BCI-IV datasets
demonstrates state-of-the-art accuracy (91% in MindBigData, 85% in TUH-AB),
making it well-suited for real-time BCI applications. By enabling low-power,
scalable EEG processing, SSL-SE-EEG presents a promising solution for
biomedical signal analysis, neural engineering, and next-generation BCIs.

</details>


### [106] [MATLAB-Simulated Dataset for Automatic Modulation Classification in Wireless Fading Channels](https://arxiv.org/abs/2510.19985)
*M. M. Sadman Shafi,Tasnia Siddiqua Ahona,Ashraful Islam Mridha*

Main category: eess.SP

TL;DR: A labeled synthetic dataset for wireless modulation classification across Rayleigh and Rician channels with multiple sampling frequencies, plus extensive feature extraction and MATLAB scripts for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Address challenge of accurate modulation classification under dynamic channels without transmitter knowledge by providing a realistic, labeled dataset for ML benchmarking.

Method: Generated signals with five modulation schemes; passed through Rayleigh and Rician channels with impairments; 1000 symbols per signal; extracted broad feature set (statistical, time- and frequency-domain, spectrogram-based, spectral correlation, BRISK/MSER/GLCM); organized into 10 CSV files across 2 channels x 5 sampling frequencies; provided MATLAB scripts.

Result: A ready-to-use, reproducible benchmark dataset for modulation classification and related wireless-communication ML tasks.

Conclusion: The dataset advances ML research in modulation classification by offering realistic propagation scenarios and diverse feature descriptors, enabling robust model development and evaluation.

Abstract: Accurate modulation classification is a core challenge in cognitive radio,
adaptive communications, spectrum analysis, and related domains, especially
under dynamic channels without transmitter knowledge. To address this need,
this article presents a labeled synthetic dataset designed for wireless
modulation classification under realistic propagation scenarios. The signals
were generated in MATLAB by modulating randomly generated bitstreams using five
digital modulation schemes: BPSK, QPSK, 16-QAM, 64-QAM, and 256-QAM. These
signals were then transmitted through Rayleigh and Rician fading channels with
standardized parameters, along with additional impairments to enhance realism
and diversity. Each modulated signal contains 1000 symbols. A comprehensive set
of features was extracted from the signals, encompassing statistical,
time-domain, frequency-domain, spectrogram-based, spectral correlation-based,
and image-processing-based descriptors such as BRISK, MSER, and GLCM. The
dataset is organized into 10 CSV files covering two channel types (Rayleigh and
Rician) across five sampling frequencies: 1 MHz, 10 MHz, 100 MHz, 500 MHz, and
1 GHz. To facilitate reproducibility and encourage further experimentation, the
MATLAB scripts used for signal generation and feature extraction are also
provided. This dataset serves as a valuable benchmark for developing and
evaluating machine learning models in modulation classification, signal
identification, and wireless communication research.

</details>


### [107] [NanoHydra: Energy-Efficient Time-Series Classification at the Edge](https://arxiv.org/abs/2510.20038)
*Cristian Cioflan,Jose Fonseca,Xiaying Wang,Luca Benini*

Main category: eess.SP

TL;DR: NanoHydra is a TinyML time-series classifier using lightweight binary random convolutional kernels on a GAP9 multi-core MCU, delivering high accuracy with ultra-low energy for wearable devices.


<details>
  <summary>Details</summary>
Motivation: Resource-constrained edge devices require efficient TinyML methods for real-time, privacy-preserving time-series classification without sacrificing accuracy.

Method: Use lightweight binary random convolutional kernels to extract features from data streams, with parallelized inference on the eight-core GAP9 cluster.

Result: Achieves up to 94.47% accuracy on ECG5000; 0.33 ms per 1-second ECG; 7.69 μJ per inference; ~18x more energy-efficient than the state-of-the-art.

Conclusion: NanoHydra demonstrates that binary random convolutional kernels on ultra-low-power MCUs enable accurate Time Series Classification with long device lifetimes, suitable for smart wearables.

Abstract: Time series classification (TSC) on extreme edge devices represents a
stepping stone towards intelligent sensor nodes that preserve user privacy and
offer real-time predictions. Resource-constrained devices require efficient
TinyML algorithms that prolong the device lifetime of battery-operated devices
without compromising the classification accuracy. We introduce NanoHydra, a
TinyML TSC methodology relying on lightweight binary random convolutional
kernels to extract meaningful features from data streams. We demonstrate our
system on the ultra-low-power GAP9 microcontroller, exploiting its eight-core
cluster for the parallel execution of computationally intensive tasks. We
achieve a classification accuracy of up to 94.47% on ECG5000 dataset,
comparable with state-of-the-art works. Our efficient NanoHydra requires only
0.33 ms to accurately classify a 1-second long ECG signal. With a modest energy
consumption of 7.69 uJ per inference, 18x more efficient than the
state-of-the-art, NanoHydra is suitable for smart wearable devices, enabling a
device lifetime of over four years.

</details>


### [108] [Semantic Communication for Task Execution and Data Reconstruction in Multi-User Scenarios](https://arxiv.org/abs/2510.20067)
*Maximilian H. V. Tillmann,Avinash Kankari,Carsten Bockelmann,Armin Dekorsy*

Main category: eess.SP

TL;DR: 提出一个面向多用户的语义通信系统，通过对任务执行与数据重构的互信息最大化进行联合优化，并在特定假设下将SSIM损失与互信息目标等价，从而在固定资源下通过增加重构权重达到在保持任务性能近似不变的前提下显著提升重构效果。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习推动的语义通信发展，研究重点从单一的任务执行或数据重构扩展到两者的联合优化，尤其是在多用户场景和资源受限条件下，需要兼顾任务完成质量与感知层面的数据质量以提升人机交互体验。

Method: 将任务执行与数据重构目标统一为互信息最大化的目标，并引入一个对两者的凸组合权重；在特定假设下，证明数据重构的SSIM损失可以从互信息最大化目标推导得到，从而将感知层面纳入优化；在固定资源前提下分析权重对两目标的影响，指出提升重构权重在一定范围内对任务执行影响较小且可显著提升数据重构。

Result: 给出理论联系：在某些条件下，SSIM损失可由互信息最大化推导得到；并且在资源固定的情况下，适当增大重构权重可以在保持任务执行性能近似不变的前提下显著提升数据重构质量。

Conclusion: 提出一个统一的多用户语义通信框架，用以同时实现任务执行与数据重构，揭示两者之间的权衡关系，并为权重选择提供定量化的设计指引，以实现资源友好的性能平衡。

Abstract: Semantic communication has gained significant attention with the advances in
machine learning. Most semantic communication works focus on either task
execution or data reconstruction, with some recent works combining the two. In
this work, we propose a semantic communication system for concurrent task
execution and data reconstruction for a multi-user scenario, which we formulate
as the maximization of mutual information. To investigate the trade-off between
the two objectives, we formulate a joint objective as a convex combination of
task execution and data reconstruction. We show that under specific
assumptions, the \ac{SSIM} loss can be obtained from the mutual information
maximization objective for data reconstruction, which takes human visual
perception into account. Furthermore, for constant resource use, we show that
by increasing the weight of the reconstruction objective up to a certain point,
the task execution performance can be kept nearly constant, while the data
reconstruction can be significantly improved.

</details>


### [109] [RIS-Aided mmWave O-RAN: Coverage Extension and User Mobility Handling](https://arxiv.org/abs/2510.20088)
*Tawfik Osman,Aditya S. Shekhawat,Abhradeep Roy,Georgios C. Trichopoulos,Ahmed Alkhateeb*

Main category: eess.SP

TL;DR: An RIS-assisted O-RAN 5G mmWave system with a 32x32 1-bit RIS (1024 elements) at 28 GHz, integrated with the O-RAN E2 interface, validated by indoor/outdoor field trials showing significant signal gains (indoor 9–20 dB, outdoor 6–18 dB). Two UE mobility management algorithms are proposed to jointly track/adapt RIS and UE beams in real time, demonstrating practical feasibility of RIS in O-RAN for enhanced coverage and reliability.


<details>
  <summary>Details</summary>
Motivation: RIS can redirect electromagnetic waves to improve coverage and SNR, and integrating RIS control with O-RAN can enable dynamic beam management without altering standard 5G signaling, especially for mmWave FR2 networks.

Method: Design and implement a modular tiled 32x32 1-bit RIS (1024 elements) at 28 GHz; integrate with O-RAN E2 interface for dynamic RIS configuration; perform indoor/outdoor field trials; develop two UE mobility management algorithms that use UE-received power to jointly track and adapt RIS and UE beams in real time on a RIS O-RAN testbed.

Result: Field trials show substantial received power gains: 9–20 dB indoors and 6–18 dB outdoors; mobility management algorithms enable real-time joint RIS/UE beam tracking.

Conclusion: Demonstrates the practical feasibility of integrating RIS into O-RAN systems to enhance coverage, mobility support, and link reliability in next-generation cellular networks.

Abstract: Reconfigurable Intelligent Surfaces (RISs) can redirect electromagnetic waves
to desired directions to enhance signal coverage and/or improve signal-to-noise
ratio (SNR) at the user equipment (UE). We present the design, implementation,
and evaluation of an RIS-assisted O-RAN 5G system operating in the FR2
millimeter wave (mmWave) frequency band. We first introduce the design of 1,024
element (32 $\times$ 32) 1-bit RIS operating at the 28 GHz band, utilizing a
modular and scalable tiled architecture. Then we demonstrate how the O-RAN E2
interface can be leveraged to dynamically control RIS configurations without
modifying standard 5G signaling procedures. To evaluate the RIS-assisted 5G
system, we conducted extensive field trials in both indoor and outdoor
environments. The results of the O-RAN link coverage trials show that the
deployed RIS provides substantial received signal power gains, ranging from 9
to 20 dB and 6 to 18 dB in indoor and outdoors scenarios, respectively.
Handling UE mobility in RIS-assisted systems is challenging due to the need for
joint RIS and UE beam management. For that, we develop two UE mobility
management algorithms and evaluate them in real-time operation using the RIS
O-RAN testbed. These algorithms leverage the received signal power at the UE to
jointly track and adapt the RIS and UE beams in real time as the UE moves. The
findings draw important insights into the practical feasibility of integrating
RIS into O-RAN systems to enhance coverage, mobility support, and link
reliability in next-generation cellular networks.

</details>


### [110] [Signal Design for OTFS Dual-Functional Radar and Communications with Imperfect CSI](https://arxiv.org/abs/2510.20112)
*Borui Du,Yumeng Zhang,Christos Masouros,Bruno Clerckx*

Main category: eess.SP

TL;DR: 提出了一种面向 DFRC-OTFS 的信号设计优化框架，通过对 pilot-symbol 设计与数据功率分配的联合优化，结合雷达的 ISL 指标和通信的信道容量下界，在考虑通道估计误差与数据符号随机性的前提下最大化感知与通信的加权指标；通过交替优化求解，仿真表明在感知-通信性能区间显著提升（ISL 抑制约 9.44 dB，SINR 约 4.82 dB）


<details>
  <summary>Details</summary>
Motivation: OTFS 在移动场景下对感知和通信的潜力突出，但在 DFRC 场景中的最优信号设计尚未充分研究。

Method: 建立一个联合优化框架，包含 pilot 设计与数据功率分配；以模糊度函数的旁瓣水平 ISL 作为雷达性能度量，考虑数据符号随机性与 pilot 符号的确定性；对通信给出在 OTFS 条件下的信道容量下界，考虑信道估计误差；通过加权和的形式最大化感知与通信指标，采用交替优化求解。

Result: 仿真结果显示所提信号在感知与通信性能区域显著优于传统方案，给出至少 9.44 dB 的 ISL 抑制提升和 4.82 dB 的 SINR 提升。

Conclusion: 本文提出的 DFRC-OTFS 信号设计框架有望在移动场景中提升双任务性能，拓展感知-通信协同的可行范围。

Abstract: Orthogonal time frequency space (OTFS) offers significant advantages in
managing mobility for both wireless sensing and communication systems, making
it a promising candidate for dual-functional radar-communication (DFRC).
However, the optimal signal design that fully exploits OTFS's potential in DFRC
has not been sufficiently explored. This paper addresses this gap by
formulating an optimization problem for signal design in DFRC-OTFS,
incorporating both pilot-symbol design for channel estimation and data-power
allocation. Specifically, we employ the integrated sidelobe level (ISL) of the
ambiguity function as a radar metric, accounting for the randomness of the data
symbols alongside the deterministic pilot symbols. For communication, we derive
a channel capacity lower bound metric that considers channel estimation errors
in OTFS. We maximize the weighted sum of sensing and communication metrics and
solve the optimization problem via an alternating optimization framework.
Simulations indicate that the proposed signal significantly improves the
sensing-communication performance region compared with conventional signal
schemes, achieving at least a 9.44 dB gain in ISL suppression for sensing, and
a 4.82 dB gain in the signal-to-interference-plus-noise ratio (SINR) for
communication.

</details>


### [111] [Active Localization of Close-range Adversarial Acoustic Sources for Underwater Data Center Surveillance](https://arxiv.org/abs/2510.20122)
*Adnan Abdullah,David Blow,Sara Rampazzi,Md Jahidul Islam*

Main category: eess.SP

TL;DR: 提出了一种几何感知的真实时间声学威胁定位框架，针对水下数据中心的近场对手声源，结合固定水听器和移动水听器，使用LC-MAP生成先验并通过UKF实现TDOA-FDOA的联合滤波，能够实现亚米级定位、90%以上成功率，收敛时间显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 水下数据基础设施具备自然冷却与物理安全优势，但易受声学注入攻击影响数据完整性与可用性；传统TDOA/FDOA在动态多源/有限布设下难以泛化，亟需一个能在静态与移动传感器混合配置下提供物理一致初始状态的定位框架。

Method: 提出LC-MAP (Locus-Conditioned Maximum A-Posteriori) 用以生成声学信息丰富且几何一致的先验，确保初始状态物理可行；将其整合入无迹卡尔曼滤波(UKF)管道，以应对非线性测量和噪声；传感器配置为固定水听器 + 载荷于 surveillance 机器人；通过Monte Carlo、Gazebo物理仿真和现场试验评估。

Result: 在3D定位上实现亚米级精度与>90%成功率，收敛时间比基线方法缩短近一半；在不同设置下具有鲁棒的收敛性与实时性能。

Conclusion: 该框架实现了几何感知、实时声源定位的自主监控能力，提升水下基础设施的声学威胁检测与响应水平，为海上/水下安保提供了一种可行的威胁定位解决方案。

Abstract: Underwater data infrastructures offer natural cooling and enhanced physical
security compared to terrestrial facilities, but are susceptible to acoustic
injection attacks that can disrupt data integrity and availability. This work
presents a comprehensive surveillance framework for localizing and tracking
close-range adversarial acoustic sources targeting offshore infrastructures,
particularly underwater data centers (UDCs). We propose a heterogeneous
receiver configuration comprising a fixed hydrophone mounted on the facility
and a mobile hydrophone deployed on a dedicated surveillance robot. While using
enough arrays of static hydrophones covering large infrastructures is not
feasible in practice, off-the-shelf approaches based on time difference of
arrival (TDOA) and frequency difference of arrival (FDOA) filtering fail to
generalize for this dynamic configuration. To address this, we formulate a
Locus-Conditioned Maximum A-Posteriori (LC-MAP) scheme to generate acoustically
informed and geometrically consistent priors, ensuring a physically plausible
initial state for a joint TDOA-FDOA filtering. We integrate this into an
unscented Kalman filtering (UKF) pipeline, which provides reliable convergence
under nonlinearity and measurement noise. Extensive Monte Carlo analyses,
Gazebo-based physics simulations, and field trials demonstrate that the
proposed framework can reliably estimate the 3D position and velocity of an
adversarial acoustic attack source in real time. It achieves sub-meter
localization accuracy and over 90% success rates, with convergence times nearly
halved compared to baseline methods. Overall, this study establishes a
geometry-aware, real-time approach for acoustic threat localization, advancing
autonomous surveillance capabilities of underwater infrastructures.

</details>


### [112] [Sensing Security in Near-Field ISAC: Exploiting Scatterers for Eavesdropper Deception](https://arxiv.org/abs/2510.20140)
*Jiangong Chen,Xia Lei,Kaitao Meng,Kawon Han,Yuchen Zhang,Christos Masouros,Athina P. Petropulu*

Main category: eess.SP

TL;DR: 提出一种定位欺骗（LD）方案，在近场ISAC场景中通过对散射体的过度照明来误导具感知能力的窃听者，将散射体错当成目标，同时借助FP与SDR实现通信、感知与感知安全的三方折中优化。


<details>
  <summary>Details</summary>
Motivation: 在近场集成感知与通信（NF ISAC）系统中，环境中的已知散射体可能被窃听者用于感知，但如何在不依赖窃听者信息前提下提升感知安全性仍是挑战。提出的LD方案旨在通过对散射体施加更高 probing 功率来干扰Eve的感知，并实现对三方性能的灵活权衡。

Method: 提出定位欺骗（LD）方案：对散射体注入比对目标更高的探测功率，使Eve难以区分散射体与目标；合法接收端可去除已知散射体。通过分数编程（FP）框架与半正定松弛（SDR），在满足合法雷达SINR约束的前提下，最大化总速率和散射体功率的加权和以实现通信、感知、感知安全之间的三方折中。为评估安全性，采用Cramér–Rao界（CRB）和均方误差（MSE）等指标，并引入Kullback–Leibler散度（KLD）衡量Eve端目标与散射体的可辨性差距。通过仿真验证LD方案可灵活调整波束形成以实现三方权衡，且在感知安全方面显著提高Eve端混淆和漏检的概率。

Result: 仿真结果显示，LD方案能够在不同权重设定下实现对通信、感知与感知安全的三方折中；就感知安全而言，散射体导致的干扰信号增强了Eve端的混淆效应，甚至可能使实际目标被错检。使用CRB/MSE评估感知性能并用KLD量化信息层面的安全提升，证实了方案在信息理论层面的有效性。

Conclusion: 该LD方案为NF ISAC场景提供了一种无需获取Eve信息即可实现感知安全的灵活折中策略，且可通过调节对散射体的照射功率与波束形成来适应不同性能需求。未来工作可扩展至更复杂场景、实际系统实现以及对鲁棒性和对抗性攻击的评估。

Abstract: In this paper, we explore sensing security in near-field (NF) integrated
sensing and communication (ISAC) scenarios by exploiting known scatterers in
the sensing scene. We propose a location deception (LD) scheme where scatterers
are deliberately illuminated with probing power that is higher than that
directed toward targets of interest, with the goal of deceiving potential
eavesdroppers (Eves) with sensing capability into misidentifying scatterers as
targets. While the known scatterers can be removed at the legitimate sensing
receiver, our LD approach causes Eves to misdetect targets. Notably, this
deception is achieved without requiring any prior information about the Eves'
characteristics or locations. To strike a flexible three-way tradeoff among
communication, sensing, and sensing-security performance, the sum rate and
power allocated to scatterers are weighted and maximized under a legitimate
radar signal-to-interference-plus-noise ratio (SINR) constraint. We employ the
fractional programming (FP) framework and semidefinite relaxation (SDR) to
solve this problem. To evaluate the security of the proposed LD scheme, the
Cramer-Rao Bound (CRB) and mean squared error (MSE) metrics are employed.
Additionally, we introduce the Kullback-Leibler Divergence (KLD) gap between
targets and scatterers at Eve to quantify the impact of the proposed LD
framework on Eve's sensing performance from an information-theoretical
perspective. Simulation results demonstrate that the proposed LD scheme can
flexibly adjust the beamforming strategy according to performance requirements,
thereby achieving the desired three-way tradeoff. In particular, in terms of
sensing security, the proposed scheme significantly enhances the clutter signal
strength at Eve's side, leading to confusion or even missed detection of the
actual target.

</details>


### [113] [A Survey of OTFS-Based Index Modulation Techniques: Challenges, Benefits, and Future Directions for 6G and Beyond](https://arxiv.org/abs/2510.20265)
*Burak Ahmet Ozden,Erdogan Aydin,Emir Aslandogan,Haci Ilhan,Ertugrul Basar,Miaowen Wen,Marco Di Renzo,Vincent Poor*

Main category: eess.SP

TL;DR: 这是关于 OTFS-IM 系统的全面综述，涵盖架构、检测、性能指标、变体及未来挑战。


<details>
  <summary>Details</summary>
Motivation: OTFS 在 DD 域的时频变换可揭示信道多样性、对抗快变信道；指数调制（IM）通过资源索引传输信息以提升性能。将两者结合有望在 6G+ 场景中实现高容量、鲁棒的无线通信；因此需要对现有 OTFS-IM 方案进行系统梳理与比较。

Method: 对现有 OTFS‑IM 方案进行系统分类，按系统架构、检测方法、以及容量、PAPR、分集、复杂度、CSI 不完备、频谱效率、outage 等进行评估；描述并比较 OTFS‑IM 的变体（如 OTFS‑based 空间移位键控、空间调制、四元/等）及其他编码索引调制；并给出性能对比、计算复杂度、能效、带宽利用、吞吐等指标；最后讨论挑战与未来方向。

Result: 给出 OTFS‑IM 的分类、各变体的原理与系统模型、检测策略，以及在复杂度、误码性能、容量、能耗、频谱效率、吞吐等方面的比较；总结出当前技术的优势与权衡，以及在 CSI、硬件、同步、安防等方面的现状与不足。

Conclusion: OTFS‑IM 作为一种有潜力的高效通信范式，但实现仍面临多方面挑战；未来工作应聚焦在降低复杂度、提高鲁棒性、改善信道估计与同步、硬件实现限制，以及与其他先进技术的集成。

Abstract: Orthogonal time frequency space (OTFS) is a two-dimensional modulation
technique that uses the delay-Doppler (DD) domain and is a candidate for
providing robust, high-capacity wireless communications for envisioned 6G and
beyond networks. The OTFS technique maps data to the DD domain instead of the
traditional time-frequency domain, enabling it to fully utilize channel
diversity and transform fast time-varying channels into nearly static channels.
Index modulation (IM) is a communication paradigm that conveys information not
only through conventional modulation symbols but also by encoding data bits in
the indices of the selected communication resources to improve error
performance, spectral efficiency, and energy efficiency. In this survey, a
comprehensive review of work on OTFS-based wireless communication systems is
presented. In particular, the existing OTFS-IM schemes are reviewed and
systematically categorized according to their system architectures, detection
methods, and performance aspects such as capacity, peak-to-average power ratio,
diversity, complexity, imperfect channel state information, spectral
efficiency, and outage probability. Furthermore, the operating principles and
system models of OTFS-IM variants-including OTFS-based space shift keying,
OTFS-based spatial modulation, OTFS-based quadrature spatial modulation,
OTFS-based media-based modulation, and OTFS-based code index modulation-are
described, followed by a comparative performance analysis in terms of
computational complexity, error performance, capacity, energy saving, spectral
efficiency, and throughput. Finally, the challenges, benefits, and future
directions for OTFS-IM systems are discussed, covering key aspects such as
complexity, efficiency, latency, channel estimation, hardware constraints,
synchronization, security, and potential integration with other advanced
wireless communication techniques.

</details>


### [114] [Near-Field 3D Localization and MIMO Channel Estimation with Sub-Connected Planar Arrays](https://arxiv.org/abs/2510.20274)
*Kangda Zhi,Tianyu Yang,Songyan Xue,Giuseppe Caire*

Main category: eess.SP

TL;DR: 提出一种三阶段算法，结合OMP和SBL，对近场极大规模XL-MIMO在多天线用户场景下的通道估计与3D定位进行鲁棒设计。通过子阵列划分+DFT字典的OMP进行子阵列信道估计，利用MUSIC在LS准则下估计用户阵列的中心位置，随后以该中心位置构建精 refined 的字典并采用SBL完成MIMO通道估计，在 pilots开销和估计精度上优于若干基准。


<details>
  <summary>Details</summary>
Motivation: 在近场XL-MIMO场景下，上行MIMO通道呈全列秩且现有针对远场或仅支持单天线用户的近场字典难以有效估计，需要一种能够兼顾多天线用户与近场特性的通道估计与定位方案，兼顾低开销。

Method: 1) 将XL-MIMO划分为若干子阵列，使用OMP在DFT字典下求解子阵列通道的CS问题；2) 结合估计出的子阵列通道，采用一维MUSIC在LS准则下估计用户阵列的中心位置；3) 以估计出的中心位置构建 refined 的定位辅助字典，再利用SBL完成MIMO通道的估计。

Result: 实验结果显示，与若干基准相比，所提算法在 pilots开销与估计精度方面均表现显著优越。

Conclusion: 提出的三阶段算法有效处理近场XL-MIMO下的通道估计与3D定位问题，能够在降低 pilots 开销的同时提高估计精度，并通过将定位信息融入稀疏贝叶斯学习实现对MIMO通道的精细估计。

Abstract: This paper investigates the design of channel estimation and 3D localization
algorithms in a challenging scenario, where a sub-connected planar extremely
large-scale multiple-input multiple-output (XL-MIMO) communicates with
multi-antenna users. In the near field, the uplink MIMO channel is of full
column rank and therefore can not be estimated effectively by applying existing
codebooks that are designed for the far-field case or for the near-field case
but limited to single antenna users. To solve this problem, we propose a
three-stage algorithm aided by orthogonal matching pursuit (OMP) and sparse
Bayesian learning (SBL). Specifically, we firstly partition the XL-MIMO into
subarrays and use OMP to solve the compressed sensing (CS) problem about
subarray channel estimation with the Discrete Fourier Transform (DFT)-based
dictionary matrix. Secondly, exploiting the estimated subarray channels and
employing one-dimensional multiple signal classification (MUSIC), we estimate
the central location of the user array under the Least Squares (LS) criterion.
Finally, we utilize the estimated central location to construct a refined
location-aided dictionary matrix and obtain the MIMO channel estimation using
SBL. Results exhibit the significant superiority of the proposed algorithm
compared with several benchmarks, in terms of both the pilot overhead and
estimation accuracy.

</details>


### [115] [A Transformer Inspired AI-based MIMO receiver](https://arxiv.org/abs/2510.20363)
*András Rácz,Tamás Borsos,András Veres,Benedek Csala*

Main category: eess.SP

TL;DR: 提出 AttDet，一种 Transformer 风格的 MIMO 检测方法，将每个发射层视为一个 token，通过自注意力机制学习跨流干扰。注意力分数来自估计信道矩阵，表征信道相关性；值由匹配滤波输出初始化并迭代更新。整体上实现模型可解释性与数据驱动灵活性的结合。


<details>
  <summary>Details</summary>
Motivation: 在高维 MIMO 检测场景中，需在可解释性与性能之间取得平衡，同时有效建模跨流干扰。利用自注意力机制以信道相关性为核心信号，设计出一种可解释且可训练的检测框架。

Method: 把每个发射层作为一个 token，Queries 和 Keys 直接来自估计信道矩阵，注意力分数量化各流之间的相关性；Values 初始为匹配滤波输出并通过迭代更新进行 refine；整个系统在 Transformer 风格的框架下实现。

Result: 在面向真实 5G 信道模型、较高阶调制与编码的仿真中，AttDet 能达到接近最优 BER/BLER 的性能，同时保持可预测的多项式复杂度。

Conclusion: AttDet 将模型驱动的灵活性与基于模型的可解释性结合起来，提供一种在现实系统条件下具有竞争力的 MIMO 检测方案。

Abstract: We present AttDet, a Transformer-inspired MIMO (Multiple Input Multiple
Output) detection method that treats each transmit layer as a token and learns
inter-stream interference via a lightweight self-attention mechanism. Queries
and keys are derived directly from the estimated channel matrix, so attention
scores quantify channel correlation. Values are initialized by matched-filter
outputs and iteratively refined. The AttDet design combines model-based
interpretability with data-driven flexibility. We demonstrate through
link-level simulations under realistic 5G channel models and high-order, mixed
QAM modulation and coding schemes, that AttDet can approach near-optimal
BER/BLER (Bit Error Rate/Block Error Rate) performance while maintaining
predictable, polynomial complexity.

</details>


### [116] [Inference-Optimal ISAC via Task-Oriented Feature Transmission and Power Allocation](https://arxiv.org/abs/2510.20429)
*Biao Dong,Bin Cao,Qinyu Zhang*

Main category: eess.SP

TL;DR: 在压缩-估计（CE）框架的集成感知与通信（ISAC）系统中，使用判别增益（DG）作为推理性能的指标，DG最优的发送接收机设计在功率分配上表现优于均方误差（MSE）最优设计，呈现水位填充式结构并揭示信号与通信之间的权衡；在低信噪比下尤为省功率。


<details>
  <summary>Details</summary>
Motivation: ISAC 系统需要同时实现高效的感知与通信，且推理性能通常是最终目标。将MSE作为优化目标可能与实际的推理误差概率不完全一致。通过将推理误差概率界建模为DG的单调函数，提出以DG为核心的传输设计，以获得更直接的推理性能提升。

Method: 在CE框架下，将推理性能表示为误差概率界的单调函数，基于DG推导出DG最优与MSE最优的闭式传输设计；揭示两者的水位填充型结构和感知与通信的明确权衡；通过数值实验验证在低SNR下DG最优设计在功率利用效率上的优势。

Result: DG最优设计在功率分配上更具选择性地聚焦于信息量大的特征，因而在保持推理性能的同时节省发射功率，且在低SNR下效果尤为突出；与MSE最优设计相比，提供更好的推理指标与更高的能效。

Conclusion: 将DG作为推理性能的核心指标，能够得到比以MSE为目标更符合推理目标的传输设计，CE框架下的DG优化揭示了明确的S&C权衡与水位填充结构，为ISAC系统的资源分配与设计提供了新的理论与实践方向。

Abstract: This work is concerned with the coordination gain in integrated sensing and
communication (ISAC) systems under a compress-and-estimate (CE) framework,
wherein inference performance is leveraged as the key metric. To enable
tractable transceiver design and resource optimization, we characterize
inference performance via an error probability bound as a monotonic function of
the discriminant gain (DG). This raises the natural question of whether
maximizing DG, rather than minimizing mean squared error (MSE), can yield
better inference performance. Closed-form solutions for DG-optimal and
MSE-optimal transceiver designs are derived, revealing water-filling-type
structures and explicit sensing and communication (S\&C) tradeoff. Numerical
experiments confirm that DG-optimal design achieves more power-efficient
transmission, especially in the low signal-to-noise ratio (SNR) regime, by
selectively allocating power to informative features and thus saving transmit
power for sensing.

</details>


### [117] [An Accelerated Mixed Weighted-Unweighted MMSE Approach for MU-MIMO Beamforming](https://arxiv.org/abs/2510.20507)
*Xi Gao,Akang Wang,Junkai Zhang,Qihong Duan,Jiang Xue*

Main category: eess.SP

TL;DR: 提出了一种名为 A-MMMSE 的高效并行算法，用于下行多用户MIMO中基于加权和速率(WSR)最大化的预编码设计。通过基于区块坐标梯度下降的更新，避免矩阵求逆，改用矩阵乘法，便于GPU加速。并给出两阶段的热启动策略以加速收敛。所得结果表明，A-MMMSE 在达到 WSR 最优性方面与传统 WMMSE/Reduced-WMMSE 相当，但计算时间显著降低。


<details>
  <summary>Details</summary>
Motivation: WSR 最大化是下行 MU-MIMO 的核心问题；传统的 WMMSE 算法计算复杂度高，需要对矩阵进行三次幂次的求逆，造成低延迟场景下难以应用。需要一个高度并行、低复杂度、易于 GPU 加速的方法。

Method: 提出基于区块坐标下降的预编码更新方案，在每一步通过区块坐标梯度下降更新预编码矩阵，避免矩阵求逆，仅使用矩阵乘法，从而天然适合 GPU 加速。还提出基于求和均方误差（sum MSE）最小化问题的两阶段热启动策略，以加速收敛。将该算法命名为 Accelerated Mixed weighted-unweighted sum-MSE minimization (A-MMMSE)。

Result: 仿真结果显示，A-MMMSE 在 WSR 性能上与传统 WMMSE 及其增强版 Reduced-WMMSE 相匹配，但在多种系统配置下显著降低了计算时间；算法收敛至一个驻点。

Conclusion: A-MMMSE 提供了一种高效、并行友好的下行 MU-MIMO WSR 优化替代方案，能够在不牺牲性能的前提下实现快速收敛并适应 GPU 加速，适合低时延场景。

Abstract: Precoding design based on weighted sum-rate (WSR) maximization is a
fundamental problem in downlink multi-user multiple-input multiple-output
(MU-MIMO) systems. While the weighted minimum mean-square error (WMMSE)
algorithm is a standard solution, its high computational complexity--cubic in
the number of base station antennas due to matrix inversions--hinders its
application in latency-sensitive scenarios. To address this limitation, we
propose a highly parallel algorithm based on a block coordinate descent
framework. Our key innovation lies in updating the precoding matrix via block
coordinate gradient descent, which avoids matrix inversions and relies solely
on matrix multiplications, making it exceptionally amenable to GPU
acceleration. We prove that the proposed algorithm converges to a stationary
point of the WSR maximization problem. Furthermore, we introduce a two-stage
warm-start strategy grounded in the sum mean-square error (MSE) minimization
problem to accelerate convergence. We refer to our method as the Accelerated
Mixed weighted-unweighted sum-MSE minimization (A-MMMSE) algorithm. Simulation
results demonstrate that A-MMMSE matches the WSR performance of both
conventional WMMSE and its enhanced variant, reduced-WMMSE, while achieving a
substantial reduction in computational time across diverse system
configurations.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [118] [A Location-Aware Hybrid Deep Learning Framework for Dynamic Near-Far Field Channel Estimation in Low-Altitude UAV Communications](https://arxiv.org/abs/2510.20277)
*Wenli Yuan,Kan Yu,Xiaowu Liu,Kaixuan Li,Qixun Zhang,Zhiyong Feng*

Main category: cs.IT

TL;DR: 提出一种位置感知混合深度学习框架用于近地UAV通道估计，在近场球面波前条件下利用定位信息提升估计精度，显著优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 低空UAV通信中，由于动态的空地链路、高机动性以及大规模天线阵列，通道估计困难增大；传统方法多基于远场假设，无法捕捉近场的几何与时变特性，且未利用实时位置等几何先验信息。

Method: 提出一个位置感知的混合深度学习架构：CNN用于空间特征提取，BiLSTM用于建模时序演化，多头自注意力聚焦于区分性通道分量；并将发射端/接收端位置嵌入作为几何先验，结合近场球面波前的距离信息以提升泛化。

Result: 大量仿真表明该方法在与现有基准的对比中具有显著优势，平均NMSE下降至少30.25%。

Conclusion: 提出的定位感知混合深度学习框架在近场 UAV 通道估计中有效，能充分利用几何先验提升性能和泛化能力。

Abstract: In low altitude UAV communications, accurate channel estimation remains
challenging due to the dynamic nature of air to ground links, exacerbated by
high node mobility and the use of large scale antenna arrays, which introduce
hybrid near and far field propagation conditions. While conventional estimation
methods rely on far field assumptions, they fail to capture the intricate
channel variations in near-field scenarios and overlook valuable geometric
priors such as real-time transceiver positions. To overcome these limitations,
this paper introduces a unified channel estimation framework based on a
location aware hybrid deep learning architecture. The proposed model
synergistically combines convolutional neural networks (CNNs) for spatial
feature extraction, bidirectional long short term memory (BiLSTM) networks for
modeling temporal evolution, and a multihead self attention mechanism to
enhance focus on discriminative channel components. Furthermore, real-time
transmitter and receiver locations are embedded as geometric priors, improving
sensitivity to distance under near field spherical wavefronts and boosting
model generalization. Extensive simulations validate the effectiveness of the
proposed approach, showing that it outperforms existing benchmarks by a
significant margin, achieving at least a 30.25% reduction in normalized mean
square error (NMSE) on average.

</details>


### [119] [Moving or Predicting? RoleAware-MAPP: A Role-Aware Transformer Framework for Movable Antenna Position Prediction to Secure Wireless Communications](https://arxiv.org/abs/2510.20293)
*Wenxu Wang,Xiaowu Liu,Wei Gong,Yujia Zhao,Kaixuan Li,Qixun Zhang,Zhiyong Feng,Kan Yu*

Main category: cs.IT

TL;DR: 提出 RoleAware-MAPP，通过Transformer框架结合角色感知嵌入、物理信息语义特征和复合损失，在移动天线的物理层安全场景中显著提升保密性。


<details>
  <summary>Details</summary>
Motivation: 移动天线在物理层安全中具有潜力，但现实部署受限于高在线优化复杂度、机械移动与信道变化不同步，以及缺乏结合领域知识的学习方法。

Method: 将MA定位问题建模为预测任务，提出RoleAware-MAPP：1) 角色感知嵌入以建模用户意图，2) 物理信息语义特征提取信道传播特性，3) 复合损失函数优先优化保密性而非几何准确度。

Result: 在3GPP场景仿真中，平均 secrecy rate为0.3569 bps/Hz，Secrecy Capacity为81.52%，相较 strongest baseline 提升约48.4%与5.39个百分点，且对不同用户速率和噪声条件具有鲁棒性。

Conclusion: 通过将领域知识融入表征与损失设计，RoleAware-MAPP显著提升MA在PLS中的性能和鲁棒性。

Abstract: Movable antenna (MA) technology provides a promising avenue for actively
shaping wireless channels through dynamic antenna positioning, thereby enabling
electromagnetic radiation reconstruction to enhance physical layer security
(PLS). However, its practical deployment is hindered by two major challenges:
the high computational complexity of real time optimization and a critical
temporal mismatch between slow mechanical movement and rapid channel
variations. Although data driven methods have been introduced to alleviate
online optimization burdens, they are still constrained by suboptimal training
labels derived from conventional solvers or high sample complexity in
reinforcement learning. More importantly, existing learning based approaches
often overlook communication-specific domain knowledge, particularly the
asymmetric roles and adversarial interactions between legitimate users and
eavesdroppers, which are fundamental to PLS. To address these issues, this
paper reformulates the MA positioning problem as a predictive task and
introduces RoleAware-MAPP, a novel Transformer based framework that
incorporates domain knowledge through three key components: role-aware
embeddings that model user specific intentions, physics-informed semantic
features that encapsulate channel propagation characteristics, and a composite
loss function that strategically prioritizes secrecy performance over mere
geometric accuracy. Extensive simulations under 3GPP-compliant scenarios show
that RoleAware-MAPP achieves an average secrecy rate of 0.3569 bps/Hz and a
strictly positive secrecy capacity of 81.52%, outperforming the strongest
baseline by 48.4% and 5.39 percentage points, respectively, while maintaining
robust performance across diverse user velocities and noise conditions.

</details>


### [120] [Ergodic Mutual Information and Outage Probability for SIM-Assisted Holographic MIMO Communications](https://arxiv.org/abs/2510.20307)
*Anastasios Papazafeiropoulos,Pandelis Kourtessis,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.IT

TL;DR: 摘要通过利用大尺寸随机矩阵理论分析堆叠智能介质表面（SIM）辅助的MIMO系统的互信息分布与 outage 概率，给出MI的分布、outage的紧凑闭式表达，并提出基于梯度下降的最小化策略；仿真验证结果显示相较传统MIMO与单层SIM存在性能提升，且该优化方法比AO更快，开销更低。


<details>
  <summary>Details</summary>
Motivation: 当前文献尚缺乏对SIM辅助MIMO系统的 ergodic mutual information (EMI) 与 outage 概率的研究；SIM通过波传播提升容量，需要对MI分布和失效率进行分析并提出有效的优化策略。

Method: 采用大随机矩阵理论推导MI分布；基于统计CSI推导出outage概率的紧凑闭式表达；利用梯度下降法对outage概率进行最小化优化。

Result: 理论上推导出MI分布与outage概率的表达，给出一个快速收敛的梯度下降优化算法；仿真验证与昂贵的AO比较，显示与传统MIMO及单层SIM相比的性能提升，且优化过程耗时较少。

Conclusion: 所提出的SIM框架与梯度下降优化策略可实现对SIM-MIMO系统的性能提升，提供对未来多层智能表面的设计与分析的有价值见解。

Abstract: Stacked intelligent metasurface (SIM) is a promising enabler for
next-generation high-capacity networks that exhibit better performance compared
to its single-layer counterpart by means of just wave propagation. However, the
study of ergodic mutual information (EMI) and outage probability for
SIM-assisted multiple-input-multiple-output (MIMO) systems is not available in
the literature. To this end, we obtain the distribution of the MI by using
large random matrix theory (RMT) tools. Next, we derive a tight closed-form
expression for the outage probability based on statistical channel state
information (CSI). Moreover, we apply the gradient descent method for the
minimization of the outage probability. Simulation results verify the
analytical results and provide fundamental insights such as the performance
enhancements compared to conventional MIMO systems and the single-layer
counterpart. Notably the proposed optimization algorithm is faster than the
alternating optimization (AO) benchmark by saving significant overhead.

</details>


### [121] [Robust Analog Lagrange Coded Computing: Theory and Algorithms via Discrete Fourier Transforms](https://arxiv.org/abs/2510.20379)
*Rimpi Borah,J. Harshan*

Main category: cs.IT

TL;DR: 提出一种安全的 ALCC 框架，利用 DFT 码的纠错能力对抗 Byz 故障并提升精度，同时在任务分发方面结合信任信息进行优化，另外对协同攻击和浮点噪声的鲁棒性进行了分析。


<details>
  <summary>Details</summary>
Motivation: 在 Analog Lagrange Coded Computing (ALCC) 中，尽管原始方案能实现数据隐私保护和对拖延节点的韧性，但对拜占庭节点的错误返回缺乏鲁棒性，存在完整性威胁。需要一种能够在模拟数据上仍维持准确性且对 Byz 攻击具有鲁棒性的框架。

Method: 1) 采用离散傅里叶变换(DFT)码的纠错算法构建新的 ALCC 重建策略，从而在有限数量的拜占庭节点存在时提高计算精度；2) 基于关于 DFT 解码器性能的理论结果，提出在主服务器已知工作节点信任特征的情形下的任务分发策略，以提升准确性；3) 研究提出框架对协同攻击的鲁棒性，揭示通过利用浮点实现带来的精度噪声可以设计出有趣的攻击策略。

Result: 在存在有界数量的拜占庭工作节点时，提出的重建策略提升了计算精度；结合信任信息的任务分发策略在拥有信任特征时显著改善了准确性；对协同攻击的鲁棒性分析揭示了浮点精度噪声带来的潜在攻击路径，提供了对系统鲁棒性的全面评估。

Conclusion: 将 ALCC 扩展为对 Byz 与协同攻击等完整性威胁具鲁棒性的安全框架，并结合 DFT 码的纠错能力与信任感知的任务调度以提升实用性与准确性，同时也指出浮点噪声可能带来新的攻击向量及其相应的安全考量。

Abstract: Analog Lagrange Coded Computing (ALCC) is a recently proposed computational
paradigm wherein certain computations over analog datasets are efficiently
performed using distributed worker nodes through floating point representation.
While the vanilla version of ALCC is known to preserve the privacy of the
datasets from the workers and also achieve resilience against stragglers, it is
not robust against Byzantine workers that return erroneous results.
Highlighting this vulnerability, we propose a secure ALCC framework that is
resilient against a wide range of integrity threats from the Byzantine workers.
As a foundational step, we use error-correction algorithms for Discrete Fourier
Transform (DFT) codes to build novel reconstruction strategies for ALCC thereby
improving its computational accuracy in the presence of a bounded number of
Byzantine workers. Furthermore, capitalizing on some theoretical results on the
performance of the DFT decoders, we propose novel strategies for distributing
the ALCC computational tasks to the workers, and show that such methods
significantly improve the accuracy when the workers' trust profiles are
available at the master server. Finally, we study the robustness of the
proposed framework against colluding attacks, and show that interesting attack
strategies can be executed by exploiting the inherent precision noise owing to
floating point implementation.

</details>


### [122] [Simultaneous Wireless Information and Power Transfer for Fluid Antenna Systems](https://arxiv.org/abs/2510.20569)
*Feilong Zhang,Jianxin Dai,Zhaohui Yang,Kai-Kit Wong,Lingyuxiu Li,Jianglin Ye*

Main category: cs.IT

TL;DR: 提出将流体天线与固定天线结合的MISO系统，通过天线位置优化和发射协方差矩阵优化，在SWIPT场景中提升能量收获效率，同时保持IR的SINR约束。


<details>
  <summary>Details</summary>
Motivation: 提升能量收获效率和通信速率，通过动态调节天线位置以提高能量转移效率；结合流体天线的可调性与SWIPT的双向需求。

Method: 建立含MISO流体天线的系统模型，考虑IR与ER在同一无线信息与能量传输场景，优化发射协方差矩阵及发射/接收端流体天线的位置，在给定IR的SINR约束下最大化ER收到的能量。

Result: 仿真表明，与传统固定天线相比，流体天线系统显著提升ER的能量收获效率。

Conclusion: 通过对天线位置的优化与协方差矩阵设计，流体天线在SWIPT场景中具有潜力提升能量传输效率并保持信息传输质量。

Abstract: Fluid antenna is a promising wireless communication technology that enhances
communication rate by changing the antenna positions. This article proposes a
new communication system that combines multiple-input single-output (MISO)
fluid antennas with traditional fixed-position antennas, utilizing antenna
position optimization to improve energy harvesting efficiency. In this model,
we consider simultaneous wireless information and power transfer (SWIPT) which
transmits identical signals from the base station to both information receiver
(IR) and energy receiver (ER). We strive to enhance the power delivered to the
ER by fine-tuning the positions of transmit and receive fluid antennas, along
with optimizing the transmit covariance matrix, subject to a given minimum
signal-to-interference-plus-noise ratio (SINR) constraint at the IR. Simulation
results indicate that fluid antenna systems significantly enhance the energy
harvesting efficiency of the ER compared to traditional fixed-position
antennas.

</details>


### [123] [Super-Linear Growth of the Capacity-Achieving Input Support for the Amplitude-Constrained AWGN Channel](https://arxiv.org/abs/2510.20723)
*Haiyang Wang*

Main category: cs.IT

TL;DR: 该工作给出一个新的分析下界，证明在振幅受限的 AWGN 通道中，容量最优输入的支集大小 K(A) 相对于振幅约束 A 是超线性增长的。基于 Dytso 等人与 Mattingly 等人的工作，结合输出分布向均匀分布的总变差收敛性以及高斯混合近似的定量限界，给出 K(A) 增长的低界。


<details>
  <summary>Details</summary>
Motivation: 理解并定量刻画在振幅约束下，容量可达输入分布的支点数目如何随约束增大而增长。虽然历史上 Smith (1971) 已知最优输入为有限支点的离散分布，但关于 K(A) 的严格增长速率仍不清晰，本研究旨在给出更强的下界并填补这一空白。

Method: 在分析框架中，将输出分布在总变差意义下收敛到均匀分布，并利用高斯混合近似的定量界限，结合对信息泄漏和容量相关量的界限，推导出对 K(A) 的下界，显示其随 A 增长呈现超线性增长。此方法在继承 Dytso et al. (2019) 与 Mattingly et al. (2018) 的思路基础上，加入对分布近似的严格定量控制。

Result: 给出一个新的分析性下界，证明 K(A) 相对于振幅 A 的增长为超线性，即支集点数随约束增大增长的速率超过线性。该结论依赖于输出分布向均匀分布的总变差收敛和高斯混合近似的定量界。

Conclusion: 该结论进一步深化了对振幅约束 AWGN 通道容量可达分布的结构理解，明确了支集大小的增长趋势，并为未来给出更紧的界限和理解提供了理论基础。

Abstract: We study the growth of the support size of the capacity-achieving input
distribution for the amplitude-constrained additive white Gaussian noise (AWGN)
channel. While it is known since Smith (1971) that the optimal input is
discrete with finitely many mass points, tight bounds on the number of support
points $K(A)$ as the amplitude constraint $A$ increases remain open. Building
on recent work by Dytso \emph{et al.} (2019) and Mattingly \emph{et al.}
(2018), we derive a new analytical lower bound showing that $K(A)$ grows
super-linearly in $A$. Our approach combines total-variation convergence of the
output distribution to the uniform law with quantitative limits on Gaussian
mixture approximation.

</details>


### [124] [MIMO-Zak-OTFS with Superimposed Spread Pilots](https://arxiv.org/abs/2510.20734)
*Abhishek Bairwa,Ananthanarayanan Chockalingam*

Main category: cs.IT

TL;DR: 提出在MIMO-Zak-OTFS系统中使用超叠加扩频导频，设计能在互相关域分离跨发射天线的导频序列，从而通过简单读出实现有效通道估计，并通过通道估计与检测的Turbo迭代来降低导频-数据干扰，实验结果在2x2和3x3 MIMO下显示三次Turbo迭代可获得较好的估计/检测性能。


<details>
  <summary>Details</summary>
Motivation: 在MIMO Zak-OTFS 系统中，导频与数据往往叠加在同一帧，若要在多天线场景中实现准确的通道估计，需要使不同发射天线的导频在接收端能够分离，从而获得有效通道信息并提升检测性能。

Method: 提出一种跨互相关域的扩频导频设计，使来自不同发射天线的导频序列在 cross-ambiguity 域中被分离，并通过一个简单的读出操作实现有效通道 taps 的估计。为进一步降低导频数据干扰的影响，方法在通道估计与检测之间进行 turbo 迭代，进行多轮信息更新。系统使用高斯- sinc 脉冲整形，在 Zak-OTFS 甚域对Vehicular-A 通道模型进行仿真。

Result: 在2x2和3x3 MIMO-Zak-OTFS设定下，采用高斯- sinc 脉冲整形的仿真结果表明，所提出的导频设计与估计方案在三轮 turbo 迭代下能够实现非常好的估计与检测性能。

Conclusion: 该导频设计有效实现了跨天线导频分离、降低导频-数据干扰、提升通道估计与检测性能，适用于 MIMO-Zak-OTFS 系统在移动场景中的应用。

Abstract: In this paper, we consider the problem of spread pilot design and effective
channel estimation in multiple-input multiple-output Zak-OTFS (MIMO-Zak-OTFS)
with superimposed spread pilots, where data and spread pilot signals are
superimposed in the same frame. To achieve good estimation performance in a
MIMO setting, the spread pilots at different transmit antennas need to be
effectively separated at the receiver. Towards this, we propose a spread pilot
design that separates the pilot sequences in the cross-ambiguity domain and
enables the estimation of the effective channel taps by a simple read-off
operation. To further alleviate the effect of pilot-data interference on
performance, we carry out turbo iterations between channel estimation and
detection. Simulation results for $2\times 2$ and $3\times 3$ MIMO-Zak-OTFS
with Gaussian-sinc pulse shaping filter for vehicular-A channel model show that
the proposed pilot design and estimation scheme with three turbo iterations can
achieve very good estimation/detection performance.

</details>
