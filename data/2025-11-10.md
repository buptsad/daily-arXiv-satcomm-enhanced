<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 56]
- [eess.SP](#eess.SP) [Total: 6]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CR](#cs.CR) [Total: 11]
- [eess.SY](#eess.SY) [Total: 9]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: KV缓存管理对LLM推理质量影响显著，需兼顾位置编码一致性而非单纯缓存大小；延迟策略应保持连续上下文块，避免破坏RoPE等位置信号。


<details>
  <summary>Details</summary>
Motivation: 研究KV缓存的管理策略与模型架构的上下文窗口限制及位置编码的耦合，以及缓存健康的综合评估，以提升多轮对话/推理中的稳定性与生成质量。

Method: 在一个stateful基准测试框架中进行经验分析，比较不同缓存淘汰策略对生成质量的影响，关注缓存大小、位置结构、以及对RoPE等位置编码的影响。

Result: 当KV缓存接近或超过训练时的上下文窗口（如8192 token的Llama 3），生成质量会显著下降；即使是高保留率的淘汰策略（如99% via AttentionTop）若破坏位置连贯性也会恶化性能；保持连续上下文块的简单策略（如保留初始“要点”/gist）往往比复杂或破坏位置的策略更能保持生成连贯性。

Conclusion: 应设计尊重模型架构极限、保持位置信息结构完整性的淘汰策略，并从整体“缓存健康”角度评估，而非仅以缓存大小作为唯一指标。

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [2] [Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction](https://arxiv.org/abs/2511.04723)
*Mohamadreza Akbari Pour,Mohamad Sadeq Karimi,Amir Hossein Mazloumi*

Main category: cs.LG

TL;DR: 提出一个将TCN用于局部时序特征提取、并用Bi-LSTM增强的修改TFT框架，结合多时间窗口策略用于RUL预测，能够连接短期与长期依赖，并在基准数据集上实现平均RMSE降低最多5.5%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉细粒度时间依赖和跨时间动态关注关键特征方面存在不足，难以在多工况下实现鲁棒的剩余使用寿命预测。需要同时涵盖短期与长期依赖并强调重要时间模式。

Method: 将Temporal Convolutional Networks（TCN）用于局部时序特征提取，并对Temporal Fusion Transformer（TFT）进行修改，增设Bi-LSTM编码-解码器以增强长短期依赖的建模；同时引入多时间窗口策略以适应不同工况。

Result: 在基准数据集上，提出的方法使平均RMSE降低最多5.5%，相较于现有最优方法具有更高的预测准确性。

Conclusion: 该框架填补了当前方法在融合短期与长期依赖、强调关键时序模式以及在多工况下的自适应性方面的不足，展示了先进时序Transformer在RUL预测中的潜力。

Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime,
and optimizing maintenance in industrial systems. Remaining Useful Life (RUL)
prediction is a key component of this process; however, many existing models
struggle to capture fine-grained temporal dependencies while dynamically
prioritizing critical features across time for robust prognostics. To address
these challenges, we propose a novel framework that integrates Temporal
Convolutional Networks (TCNs) for localized temporal feature extraction with a
modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.
This architecture effectively bridges short- and long-term dependencies while
emphasizing salient temporal patterns. Furthermore, the incorporation of a
multi-time-window methodology improves adaptability across diverse operating
conditions. Extensive evaluations on benchmark datasets demonstrate that the
proposed model reduces the average RMSE by up to 5.5%, underscoring its
improved predictive accuracy compared to state-of-the-art methods. By closing
critical gaps in current approaches, this framework advances the effectiveness
of industrial prognostic systems and highlights the potential of advanced
time-series transformers for RUL prediction.

</details>


### [3] [Regularized GLISp for sensor-guided human-in-the-loop optimization](https://arxiv.org/abs/2511.04751)
*Matteo Cercola,Michele Lomuscio,Dario Piga,Simone Formentin*

Main category: cs.LG

TL;DR: 在偏好优化中引入传感器信息的灰盒化GLISp，提升收敛速度与解的质量


<details>
  <summary>Details</summary>
Motivation: 解决偏好基优化把系统视为黑盒的问题，充分利用可测的传感描述符与物理信息来改进学习

Method: 在GLISp框架中引入物理信息假设函数和最小二乘正则化，将可测传感描述符融入偏好学习，形成带正则化的灰盒结构

Result: 在分析基准和人机协同车辆悬架调优任务中，收敛更快，最终解优于基线GLISp

Conclusion: 灰盒化的偏好基优化在保持探索灵活性的同时提升了性能，可作为集成传感信息的有效策略

Abstract: Human-in-the-loop calibration is often addressed via preference-based
optimization, where algorithms learn from pairwise comparisons rather than
explicit cost evaluations. While effective, methods such as Preferential
Bayesian Optimization or Global optimization based on active preference
learning with radial basis functions (GLISp) treat the system as a black box
and ignore informative sensor measurements. In this work, we introduce a
sensor-guided regularized extension of GLISp that integrates measurable
descriptors into the preference-learning loop through a physics-informed
hypothesis function and a least-squares regularization term. This injects
grey-box structure, combining subjective feedback with quantitative sensor
information while preserving the flexibility of preference-based search.
Numerical evaluations on an analytical benchmark and on a human-in-the-loop
vehicle suspension tuning task show faster convergence and superior final
solutions compared to baseline GLISp.

</details>


### [4] [When Data Falls Short: Grokking Below the Critical Threshold](https://arxiv.org/abs/2511.04760)
*Vaibhav Singh,Eugene Belilovsky,Rahaf Aljundi*

Main category: cs.LG

TL;DR: KD在低数据和分布转移场景下可诱发/加速grokking，并有望在联合分布与持续预训练中缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺和分布漂移背景下理解grokking的机制并探索知识蒸馏对实现快速泛化的作用。

Method: 通过一系列实验：(1) 在低于临界阈值的数据条件下观察grokking是否可观测；(2) 研究从已出现grokking的模型对不同分布(p1到p2)进行KD以诱发/加速grokking；(3) 研究在联合分布(p1, p2)下的训练，比较标准监督与KD的泛化表现；(4) 在持续预训练场景中分析从p1迁移到p2时KD对泛化与灾难性遗忘的影响。

Result: 结果表明：KD可从已grok的模型对新分布进行知识转移，降低数据不足情形下的泛化门槛并加速grokking；在(p1, p2)联合分布下，单独任一分布数据不足时，标准监督训练难以泛化，但从单分布grokked模型进行KD可实现泛化；在持续预训练中，KD不仅加速泛化，还显著缓解灾难性遗忘，即使只有10%的数据也能获得强性能。

Conclusion: KD在低数据与分布演变场景中对grokking的机制具有关键作用，提供在实际部署中适应新分布的有效策略。

Abstract: In this paper, we investigate the phenomenon of grokking, where models
exhibit delayed generalization following overfitting on training data. We focus
on data-scarce regimes where the number of training samples falls below the
critical threshold, making grokking unobservable, and on practical scenarios
involving distribution shift. We first show that Knowledge Distillation (KD)
from a model that has already grokked on a distribution (p1) can induce and
accelerate grokking on a different distribution (p2), even when the available
data lies below the critical threshold. This highlights the value of KD for
deployed models that must adapt to new distributions under limited data. We
then study training on the joint distribution (p1, p2) and demonstrate that
while standard supervised training fails when either distribution has
insufficient data, distilling from models grokked on the individual
distributions enables generalization. Finally, we examine a continual
pretraining setup, where a grokked model transitions from p1 to p2, and find
that KD both accelerates generalization and mitigates catastrophic forgetting,
achieving strong performance even with only 10% of the data. Together, our
results provide new insights into the mechanics of grokking under knowledge
transfer and underscore the central role of KD in enabling generalization in
low-data and evolving distribution settings.

</details>


### [5] [FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow](https://arxiv.org/abs/2511.04768)
*Rubens Lacouture,Nathan Zhang,Ritvik Sharma,Marco Siracusa,Fredrik Kjolstad,Kunle Olukotun,Olivia Hsu*

Main category: cs.LG

TL;DR: FuseFlow 是一个将 PyTorch 的稀疏模型编译成适用于可重构数据流架构的 fuse 稀疏数据流图的编译器，首次支持跨表达式的稀疏操作融合，并通过数据流优化和设计空间探索提升对稀疏模型的执行性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模稀疏模型在硬件上的执行效率问题，提供一个端到端的编译-优化框架，探索最佳融合粒度。

Method: 将稀疏模型从 PyTorch 转换为可在 RDAs 上执行的 fuse 稀疏数据流图，支持跨表达式融合、并行化、数据流排序、稀疏性分块等，并使用周期级别数据流模拟器进行微架构分析；在四个真实应用上进行设计空间探索，提供 prune 的启发式策略。

Result: 在与未融合基线比较中实现性能提升，GPT-3 + BigBird 块稀疏注意力场景取得约 2.7x 加速；发现完全融合并非对所有稀疏模型最优，融合粒度需根据模型特性调整。

Conclusion: FuseFlow 为稀疏模型在可重构数据流架构上的优化提供了系统化的融合与分析工具，揭示了融合粒度与模型特征的关系，并支持快速设计空间探索和子配置 prune 的方法。

Abstract: As deep learning models scale, sparse computation and specialized dataflow
hardware have emerged as powerful solutions to address efficiency. We propose
FuseFlow, a compiler that converts sparse machine learning models written in
PyTorch to fused sparse dataflow graphs for reconfigurable dataflow
architectures (RDAs). FuseFlow is the first compiler to support general
cross-expression fusion of sparse operations. In addition to fusion across
kernels (expressions), FuseFlow also supports optimizations like
parallelization, dataflow ordering, and sparsity blocking. It targets a
cycle-accurate dataflow simulator for microarchitectural analysis of fusion
strategies. We use FuseFlow for design-space exploration across four real-world
machine learning applications with sparsity, showing that full fusion (entire
cross-expression fusion across all computation in an end-to-end model) is not
always optimal for sparse models-fusion granularity depends on the model
itself. FuseFlow also provides a heuristic to identify and prune suboptimal
configurations. Using Fuseflow, we achieve performance improvements, including
a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse
attention.

</details>


### [6] [Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting](https://arxiv.org/abs/2511.04789)
*Xiaoda Wang,Yuji Zhao,Kaiqiao Han,Xiao Luo,Sanne van Rooij,Jennifer Stevens,Lifang He,Liang Zhan,Yizhou Sun,Wei Wang,Carl Yang*

Main category: cs.LG

TL;DR: CNODE 使用神经常微分方程对帕金森病的脑形态变化进行连续、个性化的进程预测，通过学习患者特定的起始时间和进展速度，将个体轨迹对齐到共享的进展轨迹，在PPMI数据集上对比基线方法显示出更优的长期预测能力。


<details>
  <summary>Details</summary>
Motivation: 帕金森病在脑形态学上表现出异质且随时间演化的模式，现有基于RNN/Transformer的方法受制于不规则、稀疏的MRI数据，难以捕捉个体异质性（发病时间、进展速率、症状严重度等）。需要一个可以处理连续时间、适应不规则采样并能对个体差异进行建模的预测框架。

Method: 提出CNODE（条件神经ODE），以神经微分方程建模脑形态变化的连续时间过程；联合学习患者特定的起始时间和进展速度，将个体轨迹对齐到一个共享的进展轨迹。模型在帕金森病进展标志物计划（PPMI）数据集上进行验证，并与现有的最先进基线方法比较。

Result: 实验结果表明，CNODE在预测帕金森病的纵向进展方面优于基线方法，显示出更强的预测性。

Conclusion: CNODE实现了对帕金森病进展的连续、个性化预测，能够捕捉个体异质性和不规则采样问题，具有推动机制性理解、治疗开发及数字孪生预测应用的潜力。

Abstract: Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry
patterns. Modeling these longitudinal trajectories enables mechanistic insight,
treatment development, and individualized 'digital-twin' forecasting. However,
existing methods usually adopt recurrent neural networks and transformer
architectures, which rely on discrete, regularly sampled data while struggling
to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.
Moreover, these methods have difficulty capturing individual heterogeneity
including variations in disease onset, progression rate, and symptom severity,
which is a hallmark of PD. To address these challenges, we propose CNODE
(Conditional Neural ODE), a novel framework for continuous, individualized PD
progression forecasting. The core of CNODE is to model morphological brain
changes as continuous temporal processes using a neural ODE model. In addition,
we jointly learn patient-specific initial time and progress speed to align
individual trajectories into a shared progression trajectory. We validate CNODE
on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental
results show that our method outperforms state-of-the-art baselines in
forecasting longitudinal PD progression.

</details>


### [7] [Causal Structure and Representation Learning with Biomedical Applications](https://arxiv.org/abs/2511.04790)
*Caroline Uhler,Jiaqi Zhang*

Main category: cs.LG

TL;DR: 提出一个统计/计算框架，将表示学习与因果推断耦合，利用观测与扰动的多模态数据进行因果结构学习和变量学习，并设计最优扰动。


<details>
  <summary>Details</summary>
Motivation: 尽管表示学习在预测任务中效果显著，但在因果任务（如预测扰动效果）上可能失败，因此需要将表示学习与因果推断结合。多模态数据（观测与扰动、成像与测序、在单细胞、组织、器官水平等）提供机会，用于生物医学问题。

Method: 提出一个统计与计算框架，用于因果结构与表示学习：1) 使用观测与扰动数据进行对观测因变量的因果发现；2) 通过多模态视图学习因果变量；3) 设计最优扰动。

Result: 论文为框架性工作，未给出具体实验结果；主要提出方法、问题设定与潜在应用。

Conclusion: 该工作为在生物医学场景中结合多模态数据的因果发现与表示学习提供新方向，强调学习因果结构和变量的能力，并据此设计有效扰动。

Abstract: Massive data collection holds the promise of a better understanding of
complex phenomena and, ultimately, better decisions. Representation learning
has become a key driver of deep learning applications, as it allows learning
latent spaces that capture important properties of the data without requiring
any supervised annotations. Although representation learning has been hugely
successful in predictive tasks, it can fail miserably in causal tasks including
predicting the effect of a perturbation/intervention. This calls for a marriage
between representation learning and causal inference. An exciting opportunity
in this regard stems from the growing availability of multi-modal data
(observational and perturbational, imaging-based and sequencing-based, at the
single-cell level, tissue-level, and organism-level). We outline a statistical
and computational framework for causal structure and representation learning
motivated by fundamental biomedical questions: how to effectively use
observational and perturbational data to perform causal discovery on observed
causal variables; how to use multi-modal views of the system to learn causal
variables; and how to design optimal perturbations.

</details>


### [8] [Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes](https://arxiv.org/abs/2511.05330)
*Jan-Hendrik Ewering,Robin E. Herrmann,Niklas Wahlström,Thomas B. Schön,Thomas Seel*

Main category: cs.LG

TL;DR: 提出一个全贝叶斯的非保守哈密顿高斯过程框架，用输入-输出数据学习动力学，借助降秩GP近似实现高效训练与预测，并估计隐藏状态密度、GP超参数与阻尼等结构超参数；在非线性仿真中与依赖动量测量的最新方法比较，展示可行性。


<details>
  <summary>Details</summary>
Motivation: 在数据受限的情况下，将物理先验（如能量守恒/哈密顿结构）嵌入学习模型，以获得物理一致性和不确定性量化。传统将哈密顿动力学与GP结合常需速度数据，而实际情形往往无速率观测，因此需从输入-输出数据学习动力学并估计隐藏状态和结构参数。

Method: 提出非保守哈密顿GP框架，进行完整贝叶斯推断以估计未知隐藏状态的后验、GP超参数及阻尼等结构超参数；利用降秩GP近似降低计算复杂度，从而实现高效的预测与训练；在仅有输入输出数据的条件下学习。

Result: 在一个非线性仿真实验中评估，与依赖 momentum 测量的前沿方法进行比较，显示在缺少速度数据的情形下也能实现可行的动力学建模并提供不确定性量化。

Conclusion: 证明了从输入-输出数据学习非保守哈密顿GP动力学的可行性，且通过降秩近似实现计算效率提升；实验结果表明与以动量测量为基础的方法具备竞争力，扩展了物理约束学习的适用性。

Abstract: Embedding non-restrictive prior knowledge, such as energy conservation laws,
in learning-based approaches is a key motive to construct physically consistent
models from limited data, relevant for, e.g., model-based control. Recent work
incorporates Hamiltonian dynamics into Gaussian Process (GP) regression to
obtain uncertainty-quantifying models that adhere to the underlying physical
principles. However, these works rely on velocity or momentum data, which is
rarely available in practice. In this paper, we consider dynamics learning with
non-conservative Hamiltonian GPs, and address the more realistic problem
setting of learning from input-output data. We provide a fully Bayesian scheme
for estimating probability densities of unknown hidden states, of GP
hyperparameters, as well as of structural hyperparameters, such as damping
coefficients. Considering the computational complexity of GPs, we take
advantage of a reduced-rank GP approximation and leverage its properties for
computationally efficient prediction and training. The proposed method is
evaluated in a nonlinear simulation case study and compared to a
state-of-the-art approach that relies on momentum measurements.

</details>


### [9] [PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference](https://arxiv.org/abs/2511.04805)
*Yushu Zhao,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: PuzzleMoE introduces a training-free MoE compression framework that combines sparse expert merging with a dual-mask to capture shared vs. expert-specific parameters, and a bit-packed encoding to reduce mask overhead. It achieves up to 50% compression with maintained accuracy and up to 1.28× speedup.


<details>
  <summary>Details</summary>
Motivation: MoE models scale language models efficiently but incur high memory overhead from storing all expert parameters. Existing compression methods (expert dropping/merging) struggle to preserve accuracy at high compression. A training-free, hardware-friendly compression approach is desired to enable practical deployment.

Method: 1) Sparse expert merging using a dual-mask to identify element-wise weight redundancy and specialization, capturing both shared and expert-specific parameters. 2) Bit-packed encoding that reuses underutilized exponent bits to avoid storing binary masks/signs, reducing memory overhead and enabling efficient GPU inference. 3) Training-free approach, avoiding fine-tuning.

Result: MoE models can be compressed by up to 50% while maintaining accuracy across tasks. It outperforms prior MoE compression methods by up to 16.7% on MMLU at a 50% compression ratio and achieves up to 1.28× inference speedup.

Conclusion: PuzzleMoE provides a training-free, hardware-friendly MoE compression solution that reduces memory footprint and accelerates inference without sacrificing accuracy, demonstrating strong cross-task performance gains.

Abstract: Mixture-of-Experts (MoE) models have shown strong potential in scaling
language models efficiently by activating only a small subset of experts per
input. However, their widespread deployment remains limited due to the high
memory overhead associated with storing all expert parameters, particularly as
the number of experts increases. To address this challenge, prior works have
explored expert dropping and merging strategies, yet they often suffer from
performance drop at high compression ratios. In this paper, we introduce
PuzzleMoE, a training-free MoE compression method that achieves both high
accuracy and efficient inference through two key innovations: First, PuzzleMoE
performs sparse expert merging by identifying element-wise weight redundancy
and specialization. It uses a dual-mask to capture both shared and
expert-specific parameters. Second, to avoid the overhead of storing binary
masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses
underutilized exponent bits, enabling efficient MoE inference on GPUs.
Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up
to 50% while maintaining accuracy across various tasks. Specifically, it
outperforms prior MoE compression methods by up to 16.7% on MMLU at 50%
compression ratio, and achieves up to 1.28\times inference speedup.

</details>


### [10] [SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning](https://arxiv.org/abs/2511.05355)
*Tzu-Yuan Huang,Armin Lederer,Dai-Jie Wu,Xiaobing Dai,Sihua Zhang,Stefan Sosnowski,Shao-Hua Sun,Sandra Hirche*

Main category: cs.LG

TL;DR: SAD-Flower 通过在流上加入虚拟控制输入，给出可验证的安全、可采纳性和动态一致性保障，无需重新训练，且在 unseen 约束下具测试时适用，实验上优于以生成模型为基线的方法以实现约束满足。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配（FM）在状态和动作约束的形式化保障以及动力学一致性方面存在缺口，难以保证轨迹的安全性与可执行性，且现有方法往往需要额外训练或无法处理测试时遇到的新约束。

Method: 在流的系统中引入一个虚拟控制输入，对流的演化进行扩展。基于非线性控制理论，利用该虚拟输入实现对状态约束、动作约束和动态一致性的形式化保障，并在测试阶段无需重新训练来满足 unseen 的约束。

Result: 在多个任务中的广泛实验表明，SAD-Flower 在确保约束满足方面优于多种基于生成模型的基线方法。

Conclusion: SAD-Flower 提供了一种无重新训练的、安全、可采纳且动力学一致的轨迹生成框架，通过对流的虚拟输入的引入，实现在保证约束与可执行性的前提下提升性能。

Abstract: Flow matching (FM) has shown promising results in data-driven planning.
However, it inherently lacks formal guarantees for ensuring state and action
constraints, whose satisfaction is a fundamental and crucial requirement for
the safety and admissibility of planned trajectories on various systems.
Moreover, existing FM planners do not ensure the dynamical consistency, which
potentially renders trajectories inexecutable. We address these shortcomings by
proposing SAD-Flower, a novel framework for generating Safe, Admissible, and
Dynamically consistent trajectories. Our approach relies on an augmentation of
the flow with a virtual control input. Thereby, principled guidance can be
derived using techniques from nonlinear control theory, providing formal
guarantees for state constraints, action constraints, and dynamic consistency.
Crucially, SAD-Flower operates without retraining, enabling test-time
satisfaction of unseen constraints. Through extensive experiments across
several tasks, we demonstrate that SAD-Flower outperforms various
generative-model-based baselines in ensuring constraint satisfaction.

</details>


### [11] [Autoencoding Dynamics: Topological Limitations and Capabilities](https://arxiv.org/abs/2511.04807)
*Matthew D. Kvalheim,Eduardo D. Sontag*

Main category: cs.LG

TL;DR: Autoencoder 通过编码器E: R^n→R^ℓ与解码器D: R^ℓ→R^n，将D∘E尽量在数据流形M上接近恒等。论文探讨把M嵌入潜在空间及从潜在空间重构的拓扑限制与能力，并分析在M作为不变量流形时对动力系统的自编码能力。


<details>
  <summary>Details</summary>
Motivation: 理解在数据嵌入与动力系统建模中，编码-解码对能否忠实再现数据及其动力学，并揭示与拓扑结构相关的限制。研究存在连续E、D使D∘E在M上近似恒等的必要条件与约束，以及在M为不变量流形时的自编码潜力。

Method: 以拓扑与几何分析为主，分析D∘E接近id_M的可行性及其限制，考察潜在空间维度、M的拓扑特征对自编码结构的影响，并讨论当M为不变量流形时自编码动力系统的能力与框架。

Result: 给出若干拓扑障碍和能力边界：包括对嵌入、同伦性、不可约性等拓扑性质的限制，以及在某些条件下仍可实现对M及其动力学的有效自编码的情形；提出评估自编码能力的理论框架。

Conclusion: 自编码器在理论上可以在数据流形上实现接近恒等及对称动力学的自编码，但存在显著的拓扑限制。通过增加潜在维度或满足特定条件，可缓解部分限制；实际应用需权衡流形拓扑与表示容量。

Abstract: Given a "data manifold" $M\subset \mathbb{R}^n$ and "latent space"
$\mathbb{R}^\ell$, an autoencoder is a pair of continuous maps consisting of an
"encoder" $E\colon \mathbb{R}^n\to \mathbb{R}^\ell$ and "decoder" $D\colon
\mathbb{R}^\ell\to \mathbb{R}^n$ such that the "round trip" map $D\circ E$ is
as close as possible to the identity map $\mbox{id}_M$ on $M$. We present
various topological limitations and capabilites inherent to the search for an
autoencoder, and describe capabilities for autoencoding dynamical systems
having $M$ as an invariant manifold.

</details>


### [12] [Adversarially Robust Multitask Adaptive Control](https://arxiv.org/abs/2511.05444)
*Kasra Fallah,Leonardo F. Toso,James Anderson*

Main category: cs.LG

TL;DR: 在对抗性污染条件下，提出聚类化的多任务自适应LQR学习框架，结合聚类、系统辨识和鲁棒聚合，给出非渐近的 regret 上界。结果表明，诚实系统数量的增加可显著降低 CE 控制下的累积 regret，即便簇内存在有界比例的对手系统。


<details>
  <summary>Details</summary>
Motivation: 多系统在模型不确定性与对手性攻击下需要协同学习鲁棒控制策略。传统方法易受恶意更新影响，因此需要将聚类、辨识和鲁棒聚合结合，以提升在对抗环境中的稳健性和学习效率。

Method: 提出聚类化的多任务方法，将聚类与系统辨识相结合，并引入鲁棒聚合机制来抑制被污染的模型更新的影响。理论分析考察聚类精度、簇内异质性和对手行为对 CE-LQR 任务的期望 regret 的影响，给出非渐近界，表明在簇内诚实系统数增加时 regret 以其倒数收敛，并在簇内有界的对手比例条件下仍成立。

Result: 得到非渐近的 regret 上界，且该上界随着簇内诚实系统数的增加而下降；在簇内对手比例有界的前提下，该下降性质保持。分析还揭示了聚类精度和簇内异质性对 regret 的影响。

Conclusion: 聚类+鲁棒聚合的多系统学习框架能够提升对抗性环境下多任务 LQR 学习的稳健性与效率，理论上给出清晰的收敛速率与对簇内组成的依赖关系。

Abstract: We study adversarially robust multitask adaptive linear quadratic control; a
setting where multiple systems collaboratively learn control policies under
model uncertainty and adversarial corruption. We propose a clustered multitask
approach that integrates clustering and system identification with resilient
aggregation to mitigate corrupted model updates. Our analysis characterizes how
clustering accuracy, intra-cluster heterogeneity, and adversarial behavior
affect the expected regret of certainty-equivalent (CE) control across LQR
tasks. We establish non-asymptotic bounds demonstrating that the regret
decreases inversely with the number of honest systems per cluster and that this
reduction is preserved under a bounded fraction of adversarial systems within
each cluster.

</details>


### [13] [Sharp Minima Can Generalize: A Loss Landscape Perspective On Data](https://arxiv.org/abs/2511.04808)
*Raymond Fan,Bryce Sandlund,Lin Myat Ko*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The volume hypothesis suggests deep learning is effective because it is
likely to find flat minima due to their large volumes, and flat minima
generalize well. This picture does not explain the role of large datasets in
generalization. Measuring minima volumes under varying amounts of training data
reveals sharp minima which generalize well exist, but are unlikely to be found
due to their small volumes. Increasing data changes the loss landscape, such
that previously small generalizing minima become (relatively) large.

</details>


### [14] [A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification](https://arxiv.org/abs/2511.04814)
*Sebastian Ojeda,Rafael Velasquez,Nicolás Aparicio,Juanita Puentes,Paula Cárdenas,Nicolás Andrade,Gabriel González,Sergio Rincón,Carolina Muñoz-Camargo,Pablo Arbeláez*

Main category: cs.LG

TL;DR: ESCAPE构建了一个包含超过8万条肽段、来自27个数据库的扩展标准化数据集，并提出基于变换器的多标签分类模型，在序列和结构信息的辅助下实现对抗菌、抗真菌、抗病毒和抗寄生虫等活性的多标签预测，取得相对于第二优方法约2.56%的相对平均精度提升，达到新的状态-of-the-art。


<details>
  <summary>Details</summary>
Motivation: 现有抗菌肽相关数据集碎片化、注释不一致且缺乏标准化基准，阻碍计算方法的发展与新候选物的发现。因此需要一个大规模、统一、可复现的评估框架，以推动AI驱动的抗菌肽研究。

Method: 1) ESCAPE汇集并整合超过80,000条肽段，来自27个经验证的资源库。2) 将抗菌肽与负序列分离，并将其功能注释纳入一个生物学上连贯的多标签层次结构，覆盖细菌、真菌、病毒和寄生虫的活性。3) 基于ESCAPE，提出一个Transformer模型，结合序列和结构信息，进行多标签活性预测。

Result: 所提出的方法在多标签肽分类任务中相对于第二优方法实现最高约2.56%的相对平均精度提升，达到新的_state-of-the-art_。

Conclusion: ESCAPE提供了一个全面、可重复的评估框架，能够推进AI驱动的抗菌肽研究，促进新候选物的高效发现与筛选。

Abstract: Antimicrobial peptides have emerged as promising molecules to combat
antimicrobial resistance. However, fragmented datasets, inconsistent
annotations, and the lack of standardized benchmarks hinder computational
approaches and slow down the discovery of new candidates. To address these
challenges, we present the Expanded Standardized Collection for Antimicrobial
Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000
peptides from 27 validated repositories. Our dataset separates antimicrobial
peptides from negative sequences and incorporates their functional annotations
into a biologically coherent multilabel hierarchy, capturing activities across
antibacterial, antifungal, antiviral, and antiparasitic classes. Building on
ESCAPE, we propose a transformer-based model that leverages sequence and
structural information to predict multiple functional activities of peptides.
Our method achieves up to a 2.56% relative average improvement in mean Average
Precision over the second-best method adapted for this task, establishing a new
state-of-the-art multilabel peptide classification. ESCAPE provides a
comprehensive and reproducible evaluation framework to advance AI-driven
antimicrobial peptide research.

</details>


### [15] [Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.04834)
*Jiwoo Shin,Byeonghu Na,Mina Kang,Wonhyeok Choi,Il-chul Moon*

Main category: cs.LG

TL;DR: The paper introduces a method to replace negative prompts in training-free defenses with implicit negative embeddings via concept inversion to improve defense against harmful prompts in text-to-image models.


<details>
  <summary>Details</summary>
Motivation: Address incompatibility between fine-tuning to unlearn harmful concepts and training-free guidance with negative prompts by proposing a concept inversion-based implicit negative embedding method that can be integrated into existing pipelines.

Method: Use concept inversion to derive implicit negative embeddings, replacing explicit negative prompts in training-free methods, and integrate without modifying either approach.

Result: Experiments on nudity and violence benchmarks show improved defense success rate and preservation of input prompt semantics across defense pipelines.

Conclusion: Implicit negative embeddings via concept inversion provide a robust, plug-in defense that reconciles the two paradigms and yields consistent improvements.

Abstract: Recent advances in text-to-image generative models have raised concerns about
their potential to produce harmful content when provided with malicious input
text prompts. To address this issue, two main approaches have emerged: (1)
fine-tuning the model to unlearn harmful concepts and (2) training-free
guidance methods that leverage negative prompts. However, we observe that
combining these two orthogonal approaches often leads to marginal or even
degraded defense performance. This observation indicates a critical
incompatibility between two paradigms, which hinders their combined
effectiveness. In this work, we address this issue by proposing a conceptually
simple yet experimentally robust method: replacing the negative prompts used in
training-free methods with implicit negative embeddings obtained through
concept inversion. Our method requires no modification to either approach and
can be easily integrated into existing pipelines. We experimentally validate
its effectiveness on nudity and violence benchmarks, demonstrating consistent
improvements in defense success rate while preserving the core semantics of
input prompts.

</details>


### [16] [Sublinear iterations can suffice even for DDPMs](https://arxiv.org/abs/2511.04844)
*Matthew S. Zhang,Stephen Huan,Jerry Huang,Nicholas M. Boffi,Sitan Chen,Sinho Chewi*

Main category: cs.LG

TL;DR: 提出了 denoising diffusion randomized midpoint method (DDRaM)，在 DDPM 采样上实现子线性复杂度的理论保证，并在实验中表现良好，是首次针对纯 DDPM 采样得到子线性复杂度界的工作。


<details>
  <summary>Details</summary>
Motivation: 现有对 DDPM 的分析多基于指数欧拉离散化，通常与维度或初始 Fisher 信息线性相关的保证，存在较高的计算成本。本文希望通过引入带随机中点的 DDRaM，并结合 shifted composition rule 框架，获得更好的离散化性质和更低的计算复杂度。

Method: 提出 DDRaM，利用额外的随机化中点来更好地近似 SDE；在 shifted composition rule 的分析框架下，假设适当的光滑性条件，证明对收敛性仅需子线性数量的分数评分评估（~√d 级别）。与以往仅在基于 ODE 的采样中获得的子线性界不同，DDRaM 给出针对纯 DDPM 的子线性界，并给出与实际用法一致的实现。

Result: 在理论上得到 sublinear 复杂度界，近似耗费为 ~O~(√d) 次分数评估即可确保收敛；实验验证表明该方法在预训练图像合成模型上具有实际性能优势。

Conclusion: DDRaM 提供了对 DDPM 采样的更优离散化性质和理论保证，并在实验中展现出优于基线的方法，证明了使用随机中点的策略在纯 DDPM 框架中的有效性和实用性。

Abstract: SDE-based methods such as denoising diffusion probabilistic models (DDPMs)
have shown remarkable success in real-world sample generation tasks. Prior
analyses of DDPMs have been focused on the exponential Euler discretization,
showing guarantees that generally depend at least linearly on the dimension or
initial Fisher information. Inspired by works in log-concave sampling (Shen and
Lee, 2019), we analyze an integrator -- the denoising diffusion randomized
midpoint method (DDRaM) -- that leverages an additional randomized midpoint to
better approximate the SDE. Using a recently-developed analytic framework
called the "shifted composition rule", we show that this algorithm enjoys
favorable discretization properties under appropriate smoothness assumptions,
with sublinear $\widetilde{O}(\sqrt{d})$ score evaluations needed to ensure
convergence. This is the first sublinear complexity bound for pure DDPM
sampling -- prior works which obtained such bounds worked instead with
ODE-based sampling and had to make modifications to the sampler which deviate
from how they are used in practice. We also provide experimental validation of
the advantages of our method, showing that it performs well in practice with
pre-trained image synthesis models.

</details>


### [17] [Investigating U.S. Consumer Demand for Food Products with Innovative Transportation Certificates Based on Stated Preferences and Machine Learning Approaches](https://arxiv.org/abs/2511.04845)
*Jingchen Bi,Rodrigo Mesa-Arango*

Main category: cs.LG

TL;DR: 使用机器学习模型估算美国市场中具创新运输证书的食品购买者行为；通过两轮实验识别消费者重视的运输属性，提出五类证书：运输方式、物联网IoT、安全措施、能源来源、必须按时到达（MABDs）；结果显示对安全和能源证书偏好显著；价格、产品类型、证书及决策者因素均影响购买决策；给出数据驱动的食品供应链改进建议。


<details>
  <summary>Details</summary>
Motivation: 理解运输相关证书如何影响消费者在食品购买中的选择，以及明确哪些运输属性被消费者重视，基于先前关于供应链可追溯性的偏好研究。

Method: 应用机器学习模型来估计消费者行为；进行第二轮实验以定位具体的运输属性，采用偏好研究设计，考察五类证书并加入产品特性与决策者因素等控制变量。

Result: 消费者对安全和能源证书表现出显著偏好；价格、产品类型、证书及决策者因素对购买决策具有显著影响。

Conclusion: 提供基于数据的食品供应链改进建议，强调运输证书，尤其是安全与能源证书在影响消费者需求方面的重要性。

Abstract: This paper utilizes a machine learning model to estimate the consumer's
behavior for food products with innovative transportation certificates in the
U.S. Building on previous research that examined demand for food products with
supply chain traceability using stated preference analysis, transportation
factors were identified as significant in consumer food purchasing choices.
Consequently, a second experiment was conducted to pinpoint the specific
transportation attributes valued by consumers. A machine learning model was
applied, and five innovative certificates related to transportation were
proposed: Transportation Mode, Internet of Things (IoT), Safety measures,
Energy Source, and Must Arrive By Dates (MABDs). The preference experiment also
incorporated product-specific and decision-maker factors for control purposes.
The findings reveal a notable inclination toward safety and energy certificates
within the transportation domain of the U.S. food supply chain. Additionally,
the study examined the influence of price, product type, certificates, and
decision-maker factors on purchasing choices. Ultimately, the study offers
data-driven recommendations for improving food supply chain systems.

</details>


### [18] [SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion](https://arxiv.org/abs/2511.04854)
*Alvaro Prat,Leo Zhang,Charlotte M. Deane,Yee Whye Teh,Garrett M. Morris*

Main category: cs.LG

TL;DR: 基于碎片的SE(3)扩散模型SigmaDock，通过将配体分解为刚性碎片，在结合口袋内重新组装，实现分子对接的高效、可泛化生成，达到SOTA水平并优于传统物理对接。


<details>
  <summary>Details</summary>
Motivation: 现有生成式对接在化学不可行输出、泛化性差和计算成本高方面受限；需要一种能结合结构化先验、稳定训练且对新靶更具鲁棒性的对接方法，以提升准确性和可扩展性。

Method: 将配体分解为嵌入刚性碎片的分段结构，利用SE(3)上的Riema nnian扩散模型在结合口袋内学习重新组装这些碎片；通过在碎片层面定义扩散过程，利用几何先验，避免复杂的扩散路径和不稳定训练。

Result: 在PoseBusters数据集上实现Top-1成功率（RMSD<2 Å且PB有效）>79.9%，显著高于近来DL方法的12.7–30.8%；具备对未见蛋白的良好泛化，且首次在PB分割下实现超越经典物理对接的DL方法，显著提升了深度学习在分子建模中的可靠性与可行性。

Conclusion: 以碎片级SE(3)扩散为核心的对接新范式展示了高精度、良好泛化和较低成本的潜力，为基于深度学习的分子对接树立了新基准，并有望推动药物发现流程的实用化。

Abstract: Determining the binding pose of a ligand to a protein, known as molecular
docking, is a fundamental task in drug discovery. Generative approaches promise
faster, improved, and more diverse pose sampling than physics-based methods,
but are often hindered by chemically implausible outputs, poor
generalisability, and high computational cost. To address these challenges, we
introduce a novel fragmentation scheme, leveraging inductive biases from
structural chemistry, to decompose ligands into rigid-body fragments. Building
on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion
model that generates poses by learning to reassemble these rigid bodies within
the binding pocket. By operating at the level of fragments in SE(3), SigmaDock
exploits well-established geometric priors while avoiding overly complex
diffusion processes and unstable training dynamics. Experimentally, we show
SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates
(RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8%
reported by recent deep learning approaches, whilst demonstrating consistent
generalisation to unseen proteins. SigmaDock is the first deep learning
approach to surpass classical physics-based docking under the PB train-test
split, marking a significant leap forward in the reliability and feasibility of
deep learning for molecular modelling.

</details>


### [19] [Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2511.04856)
*Thore Gerlach,Michael Schenk,Verena Kain*

Main category: cs.LG

TL;DR: 提出了一种连续半量子 Boltzmann 机（CSQBM），将指数族先验与量子 Boltzmann 分布结合用于连续行动强化学习；具解析梯度，便于嵌入 Actor-Critic；并提出基于 CSQBM 的连续 Q 学习框架，通过对分布采样替代全局最大化以提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决连续动作强化学习中的表示能力、资源消耗及训练不稳定性问题。通过混合量子-经典模型降低量子比特需求，同时获得可解析梯度，便于与现有强化学习框架无缝集成。

Method: 构建 CSQBM：可见单元引入指数族先验，隐单元服从量子 Boltzmann 分布；推导并给出连续变量的解析梯度，方便直接接入 Actor-Critic；提出基于 CSQBM 分布的连续 Q 学习框架，用采样替代全局最大化以提高学习稳定性。

Result: 给出理论可行性、可解析梯度及与 Actor-Critic 的直接整合性证据；通过对 CSQBM 的采样策略实现连续控制中的稳定性提升，减少对全局最大化的依赖。

Conclusion: CSQBM 提供一个高表达力、资源效率兼具的量子-经典混合模型用于连续强化学习，结合解析梯度和基于采样的学习框架，具备良好的理论与应用前景。

Abstract: We introduce theoretically grounded Continuous Semi-Quantum Boltzmann
Machines (CSQBMs) that supports continuous-action reinforcement learning. By
combining exponential-family priors over visible units with quantum Boltzmann
distributions over hidden units, CSQBMs yield a hybrid quantum-classical model
that reduces qubit requirements while retaining strong expressiveness.
Crucially, gradients with respect to continuous variables can be computed
analytically, enabling direct integration into Actor-Critic algorithms.
Building on this, we propose a continuous Q-learning framework that replaces
global maximization by efficient sampling from the CSQBM distribution, thereby
overcoming instability issues in continuous control.

</details>


### [20] [FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting](https://arxiv.org/abs/2511.04865)
*Esha Sharma,Lauren Davis,Julie Ivy,Min Chi*

Main category: cs.LG

TL;DR: FoodRL is an RL-based meta-learning ensemble framework that clusters and dynamically weights diverse forecasting models based on recent performance and contextual information to forecast highly volatile in-kind donations for humanitarian supply chains.


<details>
  <summary>Details</summary>
Motivation: Forecasting volatile in-kind donations is challenging due to non-stationarity, seasonality, and concept drift caused by disasters; traditional models struggle to maintain accuracy, hindering equitable and efficient resource distribution in humanitarian contexts.

Method: FoodRL clusters multiple forecasting models and uses a reinforcement learning-based meta-learning approach to weight and combine them dynamically according to recent performance and contextual signals, enabling adaptive ensemble forecasts. Evaluation is conducted on multi-year data from two structurally distinct U.S. food banks.

Result: FoodRL consistently outperforms baseline methods, especially during disruption periods, and demonstrates the potential to translate improved forecasts into substantial social impact (e.g., estimated uplift equivalent to 1.7 million meals annually).

Conclusion: Adaptive RL-based meta-learning for ensemble forecasting can enhance reliability in humanitarian supply chains under volatility and concept drift, with strong practical and social implications.

Abstract: Food banks are crucial for alleviating food insecurity, but their
effectiveness hinges on accurately forecasting highly volatile in-kind
donations to ensure equitable and efficient resource distribution. Traditional
forecasting models often fail to maintain consistent accuracy due to
unpredictable fluctuations and concept drift driven by seasonal variations and
natural disasters such as hurricanes in the Southeastern U.S. and wildfires in
the West Coast. To address these challenges, we propose FoodRL, a novel
reinforcement learning (RL) based metalearning framework that clusters and
dynamically weights diverse forecasting models based on recent performance and
contextual information. Evaluated on multi-year data from two structurally
distinct U.S. food banks-one large regional West Coast food bank affected by
wildfires and another state-level East Coast food bank consistently impacted by
hurricanes, FoodRL consistently outperforms baseline methods, particularly
during periods of disruption or decline. By delivering more reliable and
adaptive forecasts, FoodRL can facilitate the redistribution of food equivalent
to 1.7 million additional meals annually, demonstrating its significant
potential for social impact as well as adaptive ensemble learning for
humanitarian supply chains.

</details>


### [21] [You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models](https://arxiv.org/abs/2511.04902)
*Shuvendu Roy,Hossein Hajimirsadeghi,Mengyao Zhai,Golnoosh Samei*

Main category: cs.LG

TL;DR: Curriculum-guided, label-free RL with data curation (CuMa) improves reasoning across 0.5B-7B models, reducing reliance on base model capabilities and enabling more robust unsupervised reasoning.


<details>
  <summary>Details</summary>
Motivation: Examine whether label-free reinforcement learning methods generalize to smaller base models with limited reasoning, and identify factors (e.g., training data difficulty, rollout diversity) that influence success.

Method: Introduce curriculum learning to gradually increase problem difficulty and mask non-majority rollouts. Develop a data curation pipeline to generate samples with predefined difficulty levels. Validate across models from 0.5B to 7B parameters and make code publicly available as CuMa.

Result: CuMa yields consistent improvements across all tested model sizes and reasoning strengths, mitigating degradation seen in weaker models and producing more robust unsupervised RL performance.

Conclusion: A simple, effective framework for label-free RL—Curriculum-based training with curated data—boosts reasoning capabilities in resource-constrained models and provides a practical path to bootstrap reasoning without external supervision.

Abstract: Recent advances in large language models have demonstrated the promise of
unsupervised reinforcement learning (RL) methods for enhancing reasoning
capabilities without external supervision. However, the generalizability of
these label-free RL approaches to smaller base models with limited reasoning
capabilities remains unexplored. In this work, we systematically investigate
the performance of label-free RL methods across different model sizes and
reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals
critical limitations: label-free RL is highly dependent on the base model's
pre-existing reasoning capability, with performance often degrading below
baseline levels for weaker models. We find that smaller models fail to generate
sufficiently long or diverse chain-of-thought reasoning to enable effective
self-reflection, and that training data difficulty plays a crucial role in
determining success. To address these challenges, we propose a simple yet
effective method for label-free RL that utilizes curriculum learning to
progressively introduce harder problems during training and mask no-majority
rollouts during training. Additionally, we introduce a data curation pipeline
to generate samples with predefined difficulty. Our approach demonstrates
consistent improvements across all model sizes and reasoning capabilities,
providing a path toward more robust unsupervised RL that can bootstrap
reasoning abilities in resource-constrained models. We make our code available
at https://github.com/BorealisAI/CuMa

</details>


### [22] [Efficient Swap Multicalibration of Elicitable Properties](https://arxiv.org/abs/2511.04907)
*Lunjia Hu,Haipeng Luo,Spandan Senapati,Vatsal Sharan*

Main category: cs.LG

TL;DR: 对可多校准性进行了一次普适化升级：将多校准从群体成员资格扩展到任意有界假设类，引入 swap 多校准，并给出一个以 oracle 为基础的高效算法，在 r≥2 的情况下实现泛化的误差界，特别对 r=2 đạt到 T^{−1/3} 的 ℓ_2-swap 多校准误差，显著改进了既有界限并解决了相关开放问题。


<details>
  <summary>Details</summary>
Motivation: 扩展多校准的理论框架以覆盖更广的假设空间（不仅限于群体函数），并提升算法的效率与可实现性；将可 elicitable property 与多校准紧密结合，提出更强的 swap 形式以提高预测的公平性在实际在线设置中的可行性。

Method: 提出 swap multicalibration 的新概念；在拥有有界序列鲁棒性测度的前提下，设计以 online agnostic learner 为核心的 oracle-efficient 算法，利用顺序 Rademacher 复杂度分析误差界，证明在 r≥2 时可以达到 T^{1/(r+1)} 的 ℓ_r-swap 多校准误差。

Result: 给出在有界顺序 Rademacher 复杂度的假设类下的高概率误差界；特例 r=2 得到 T^{1/3} 的 ℓ_2-swap 多校准误差，显著优于 NR23、GMS25、LSS25a 的旧界，并解决了 GJRR24 关于是否存在 sqrt(T) 级别的 ℓ_2- mean 多校准的开放问题。

Conclusion: 为可 elicitable property 的多校准提供了一个更普适的框架，扩展至任意有界假设类，提出可实现的 swap 多校准及其算法；理论上提升了对多校准的理解与可操作性，并通过对顺序 Rademacher 复杂度的应用打开了公平学习中的新工具与新方向。

Abstract: Multicalibration [HJKRR18] is an algorithmic fairness perspective that
demands that the predictions of a predictor are correct conditional on
themselves and membership in a collection of potentially overlapping subgroups
of a population. The work of [NR23] established a surprising connection between
multicalibration for an arbitrary property $\Gamma$ (e.g., mean or median) and
property elicitation: a property $\Gamma$ can be multicalibrated if and only if
it is elicitable, where elicitability is the notion that the true property
value of a distribution can be obtained by solving a regression problem over
the distribution. In the online setting, [NR23] proposed an inefficient
algorithm that achieves $\sqrt T$ $\ell_2$-multicalibration error for a
hypothesis class of group membership functions and an elicitable property
$\Gamma$, after $T$ rounds of interaction between a forecaster and adversary.
  In this paper, we generalize multicalibration for an elicitable property
$\Gamma$ from group membership functions to arbitrary bounded hypothesis
classes and introduce a stronger notion -- swap multicalibration, following
[GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when
given access to an online agnostic learner, achieves $T^{1/(r+1)}$
$\ell_r$-swap multicalibration error with high probability (for $r\ge2$) for a
hypothesis class with bounded sequential Rademacher complexity and an
elicitable property $\Gamma$. For the special case of $r=2$, this implies an
oracle-efficient algorithm that achieves $T^{1/3}$ $\ell_2$-swap
multicalibration error, which significantly improves on the previously
established bounds for the problem [NR23, GMS25, LSS25a], and completely
resolves an open question raised in [GJRR24] on the possibility of an
oracle-efficient algorithm that achieves $\sqrt{T}$ $\ell_2$-mean
multicalibration error by answering it in a strongly affirmative sense.

</details>


### [23] [Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application](https://arxiv.org/abs/2511.04918)
*A. Ganapathi Rao,Sathish Krishna Anumula,Aditya Kumar Singh,Renukhadevi M,Y. Jeevan Nagendra Kumar,Tammineni Rama Tulasi*

Main category: cs.LG

TL;DR: 将 ML 与传统统计模型结合的混合模型可显著提升预测准确性、鲁棒性与可解释性，揭示跨领域数据分析的新路径。


<details>
  <summary>Details</summary>
Motivation: 解决传统统计模型在预测力、灵活性和解释性方面的局限，探究 ML 与统计学的互补性及在预测分析与决策中的潜在应用。

Method: 对 ML 与统计模型之间的联系进行理论与经验性探究；通过构建或分析混合模型，展示新算法在性能、可扩展性和鲁棒性方面的提升。

Result: 混合模型在预测准确性、鲁棒性和可解释性方面具有显著改进，能够更好地适应不同数据情境。

Conclusion: 将机器学习与传统统计学相结合的混合建模是有效且有前景的方向，值得在更多应用场景中推广与深入研究。

Abstract: It involves the completely novel ways of integrating ML algorithms with
traditional statistical modelling that has changed the way we analyze data, do
predictive analytics or make decisions in the fields of the data. In this
paper, we study some ML and statistical model connections to understand ways in
which some modern ML algorithms help 'enrich' conventional models; we
demonstrate how new algorithms improve performance, scale, flexibility and
robustness of the traditional models. It shows that the hybrid models are of
great improvement in predictive accuracy, robustness, and interpretability

</details>


### [24] [Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding](https://arxiv.org/abs/2511.04934)
*Hadi Reisizadeh,Jiajun Ruan,Yiwei Chen,Soumyadeep Pal,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 大语言模型去学习方法在现实解码下难以实现真正遗忘，提出 leak@$k$ 作为新元评估以衡量在采样生成时遗忘知识的再现概率；在 TOFU、MUSE、WMDP 基准上进行大规模评估，发现现有方法的知识泄漏在现实解码下仍然存在，需更鲁棒的去学习策略。


<details>
  <summary>Details</summary>
Motivation: 确保合规与伦理，避免私有、有害、违法或受版权保护内容的再现；当前的去学习方法常以确定性解码评估，但忽略了 probabilistic 解码中的再现风险。

Method: 提出 leak@$k$ 元评估，测量在从模型采样 k 次时忘记知识再现的概率；在 TOFU、MUSE、WMDP 三个基准上对主流去学习方法进行大规模比较分析。

Result: 多数现有去学习方法在现实解码下仍能重新显现被遗忘的知识，知识泄露跨任务与方法普遍存在；现有方法的忘却效果有限。

Conclusion: 需要更鲁棒的去学习方法；leak@$k$ 提供一个更贴近实际应用的评估框架，推动对 LLM 去学习的研究方向。

Abstract: Unlearning in large language models (LLMs) is critical for regulatory
compliance and for building ethical generative AI systems that avoid producing
private, toxic, illegal, or copyrighted content. Despite rapid progress, in
this work we show that \textit{almost all} existing unlearning methods fail to
achieve true forgetting in practice. Specifically, while evaluations of these
`unlearned' models under deterministic (greedy) decoding often suggest
successful knowledge removal using standard benchmarks (as has been done in the
literature), we show that sensitive information reliably resurfaces when models
are sampled with standard probabilistic decoding. To rigorously capture this
vulnerability, we introduce \texttt{leak@$k$}, a new meta-evaluation metric
that quantifies the likelihood of forgotten knowledge reappearing when
generating $k$ samples from the model under realistic decoding strategies.
Using three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the
first large-scale, systematic study of unlearning reliability using our newly
defined \texttt{leak@$k$} metric. Our findings demonstrate that knowledge
leakage persists across methods and tasks, underscoring that current
state-of-the-art unlearning techniques provide only limited forgetting and
highlighting the urgent need for more robust approaches to LLM unlearning.

</details>


### [25] [Structural Properties, Cycloid Trajectories and Non-Asymptotic Guarantees of EM Algorithm for Mixed Linear Regression](https://arxiv.org/abs/2511.04937)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: 本工作在两组件混合线性回归（2MLR）中，考虑未知混合权重和回归参数的情形，给出 EM 算法在所有信噪比（SNR）下的显式更新及其结构特性与螺线轨迹分析；在无噪声时，回归参数的 EM 路径呈螺线形，并量化在高 SNR 时与螺线轨迹的偏离；给出随时间的收敛阶（线性或在接近真实值时的二次收敛）及非渐近的有限样本与总体更新之间的统计误差界，建立一个基于轨迹的分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究在已知且平衡混合权重的条件下给出全局收敛性与高信噪比情形的超线性收敛性，但在完全未知的设定下 EM 的轨迹和收敛阶尚不清楚，需要揭示在不同 SNR 下的显式更新、结构性质与轨迹特征，并给出非渐近的理论保证。

Method: 推导 2MLR 中未知混合权重与回归参数的 EM 更新公式，覆盖所有 SNR 情况；对更新的结构性质与螺线轨迹进行分析；在无噪声下建立子最优角的递推关系，并在高 SNR 下量化与螺线轨迹的偏离；给出有限样本与总体 EM 更新之间的统计误差界，并证明在有限样本下任意初始化均可收敛；提出基于轨迹的分析框架来研究 EM 的性能。

Result: 在无噪声情形下，EM 的回归参数轨迹呈螺线并可通过子最优角的递推关系来描述；高SNR下能定量化该轨迹与螺线的偏离；收敛阶在不同情形下不同：当估计向量与真实值近似正交时呈线性收敛；角度较小时在总体层面呈二次收敛；给出非渐近的有限样本与总体更新之间的统计误差界，证实在有限样本中任意初始化也能收敛。

Conclusion: 提出了一种面向混合线性回归中未知设定的 EM 的轨迹为核心的新分析框架，揭示了 EM 的结构性质和收敛行为，并扩展了对 EM 在有限样本条件下的理论理解与应用性。

Abstract: This work investigates the structural properties, cycloid trajectories, and
non-asymptotic convergence guarantees of the Expectation-Maximization (EM)
algorithm for two-component Mixed Linear Regression (2MLR) with unknown mixing
weights and regression parameters. Recent studies have established global
convergence for 2MLR with known balanced weights and super-linear convergence
in noiseless and high signal-to-noise ratio (SNR) regimes. However, the
theoretical behavior of EM in the fully unknown setting remains unclear, with
its trajectory and convergence order not yet fully characterized. We derive
explicit EM update expressions for 2MLR with unknown mixing weights and
regression parameters across all SNR regimes and analyze their structural
properties and cycloid trajectories. In the noiseless case, we prove that the
trajectory of the regression parameters in EM iterations traces a cycloid by
establishing a recurrence relation for the sub-optimality angle, while in high
SNR regimes we quantify its discrepancy from the cycloid trajectory. The
trajectory-based analysis reveals the order of convergence: linear when the EM
estimate is nearly orthogonal to the ground truth, and quadratic when the angle
between the estimate and ground truth is small at the population level. Our
analysis establishes non-asymptotic guarantees by sharpening bounds on
statistical errors between finite-sample and population EM updates, relating
EM's statistical accuracy to the sub-optimality angle, and proving convergence
with arbitrary initialization at the finite-sample level. This work provides a
novel trajectory-based framework for analyzing EM in Mixed Linear Regression.

</details>


### [26] [Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2511.04971)
*Esha Chowdhury*

Main category: cs.LG

TL;DR: 结合ML/DL模型对糖尿病患者CVD风险进行预测，XGBoost达到0.905准确率，LSTM亦为0.905；显示高准确性并具备潜在临床应用。


<details>
  <summary>Details</summary>
Motivation: 应对糖尿病与心血管疾病日益增加的共病风险，提升预测准确性以支持个性化风险管理与临床决策。

Method: 使用BRFSS数据预处理（去重、缺失值处理、分类/数值特征识别）并应用PCA进行特征提取；开展多种ML模型（DT、RF、KNN、SVM、AdaBoost、XGBoost）和多种DL模型（ANN、DNN、RNN、CNN、LSTM、BiLSTM、GRU及CNN+LSTM/BiLSTM/GRU等混合结构）。

Result: XGBoost得到最高准确率0.9050；若干DL模型达到召回率1.00，LSTM的准确率最高为0.9050；整体显示ML/DL在糖尿病患者CVD风险预测中的潜力。

Conclusion: ML与DL模型在糖尿病患者CVD风险预测中具有较高的预测能力，能辅助临床决策与个性化风险管理；需关注模型可解释性、数据集偏倚、外部验证及临床落地实施等问题。

Abstract: Accurate prediction of cardiovascular disease (CVD) risk is crucial for
healthcare institutions. This study addresses the growing prevalence of
diabetes and its strong link to heart disease by proposing an efficient CVD
risk prediction model for diabetic patients using machine learning (ML) and
hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by
removing duplicates, handling missing values, identifying categorical and
numerical features, and applying Principal Component Analysis (PCA) for feature
extraction. Several ML models, including Decision Trees (DT), Random Forest
(RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and
XGBoost, were implemented, with XGBoost achieving the highest accuracy of
0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep
Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural
Networks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and
Gated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM,
BiLSTM, and GRU, were also explored. Some of these models achieved perfect
recall (1.00), with the LSTM model achieving the highest accuracy of 0.9050.
Our research highlights the effectiveness of ML and DL models in predicting CVD
risk among diabetic patients, automating and enhancing clinical
decision-making. High accuracy and F1 scores demonstrate these models'
potential to improve personalized risk management and preventive strategies.

</details>


### [27] [Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces](https://arxiv.org/abs/2511.04973)
*Siyuan Li,Yifan Sun,Lei Cheng,Lewen Wang,Yang Liu,Weiqing Liu,Jianlong Li,Jiang Bian,Shikai Fang*

Main category: cs.LG

TL;DR: FAR-TS 提出一种快速且可扩展的多变量时间序列生成框架。通过将序列分解为数据自适应的跨通道静态基底和离散化的时间系数，再以自回归 Transformer 在离散 token 上建模，实现任意长度序列的生成，显著比扩散模型更快且能保留跨通道相关性和具可解释的潜在空间。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散基模型在多变量时间序列生成中的慢速推理和对固定长度窗口的限制，需一种更快、可控且能捕捉跨通道相关性的生成方法。

Method: 将时间序列分解为数据自适应基底（捕捉静态跨通道相关性）和时间系数（向量量化成离散 token）；使用 LLaMA 风格的自回归 Transformer 对离散 token 序列进行建模，从而实现对任意长度序列的快速生成。

Result: 与 Diffusion-TS 相比，生成速度数量级提升，同时保持跨通道相关性和可解释的潜在空间，能够实现高质量、灵活的时间序列合成。

Conclusion: FAR-TS 为多变量时间序列生成提供了一种简单而高效的框架，兼具速度、可控性与解释性，适用于数据增强、仿真与隐私保护等场景。

Abstract: Generative models for multivariate time series are essential for data
augmentation, simulation, and privacy preservation, yet current
state-of-the-art diffusion-based approaches are slow and limited to
fixed-length windows. We propose FAR-TS, a simple yet effective framework that
combines disentangled factorization with an autoregressive Transformer over a
discrete, quantized latent space to generate time series. Each time series is
decomposed into a data-adaptive basis that captures static cross-channel
correlations and temporal coefficients that are vector-quantized into discrete
tokens. A LLaMA-style autoregressive Transformer then models these token
sequences, enabling fast and controllable generation of sequences with
arbitrary length. Owing to its streamlined design, FAR-TS achieves
orders-of-magnitude faster generation than Diffusion-TS while preserving
cross-channel correlations and an interpretable latent space, enabling
high-quality and flexible time series synthesis.

</details>


### [28] [Scaling Up ROC-Optimizing Support Vector Machines](https://arxiv.org/abs/2511.04979)
*Gimun Bae,Seung Jun Shin*

Main category: cs.LG

TL;DR: A scalable ROC-SVM using incomplete U-statistics and low-rank kernel approximation to reduce computational cost while maintaining AUC performance, with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the prohibitive O(n^2) training cost of ROC-SVM in imbalanced binary classification, enabling scalable AUC-optimized learning.

Method: Replace exact U-statistic computation with incomplete U-statistics to estimate AUC, and extend to nonlinear classification via a low-rank kernel approximation to allow efficient learning in RKHS. Provide theoretical error bounds for the approximation.

Result: Empirical results on synthetic and real datasets show comparable AUC performance to the original ROC-SVM but with substantially reduced training time.

Conclusion: The proposed scalable ROC-SVM achieves similar predictive performance with much greater efficiency, making AUC-optimized classification feasible for large-scale and imbalanced datasets.

Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the
area under the ROC curve (AUC) and has become an attractive alternative of the
conventional binary classification under the presence of class imbalance.
However, its practical use is limited by high computational cost, as training
involves evaluating all $O(n^2)$. To overcome this limitation, we develop a
scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby
substantially reducing computational complexity. We further extend the
framework to nonlinear classification through a low-rank kernel approximation,
enabling efficient training in reproducing kernel Hilbert spaces. Theoretical
analysis establishes an error bound that justifies the proposed approximation,
and empirical results on both synthetic and real datasets demonstrate that the
proposed method achieves comparable AUC performance to the original ROC-SVM
with drastically reduced training time.

</details>


### [29] [Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk](https://arxiv.org/abs/2511.04980)
*Rongbin Ye,Jiaqi Chen*

Main category: cs.LG

TL;DR: 通过使用 SHAP/LIME 等可解释性方法，将高性能“黑箱”模型应用于受监管的金融环境，并提出一个五维可解释性框架，以在模型性能与可解释性之间权衡。


<details>
  <summary>Details</summary>
Motivation: 金融行业在模型预测性与监管可解释性之间存在矛盾。希望在不牺牲预测性能的前提下，使复杂模型具备可解释性，以满足 OCC/CFPB 等监管要求。

Method: 对不同模型应用 SHAP/LIME 等可解释性框架，比较不同模型的可解释性与预测性能；提出并应用一个五维框架（固有可解释性、全局解释、局部解释、一致性、复杂性）来评估模型可解释性。

Result: 更复杂、性能更好的模型在使用 SHAP/LIME 后可以达到与简单模型相同的可解释性水平；五维框架提供了比单纯准确性更细致的可解释性评估，并证明在监管金融场景中应用高性能模型的可行性。

Conclusion: 通过现代可解释性技术，能够在受监管的金融环境中使用高性能 ML 模型，同时提供一个结构化的评估框架来权衡模型性能与可解释性。

Abstract: The financial industry faces a significant challenge modeling and risk
portfolios: balancing the predictability of advanced machine learning models,
neural network models, and explainability required by regulatory entities (such
as Office of the Comptroller of the Currency, Consumer Financial Protection
Bureau). This paper intends to fill the gap in the application between these
"black box" models and explainability frameworks, such as LIME and SHAP.
Authors elaborate on the application of these frameworks on different models
and demonstrates the more complex models with better prediction powers could be
applied and reach the same level of the explainability, using SHAP and LIME.
Beyond the comparison and discussion of performances, this paper proposes a
novel five dimensional framework evaluating Inherent Interpretability, Global
Explanations, Local Explanations, Consistency, and Complexity to offer a
nuanced method for assessing and comparing model explainability beyond simple
accuracy metrics. This research demonstrates the feasibility of employing
sophisticated, high performing ML models in regulated financial environments by
utilizing modern explainability techniques and provides a structured approach
to evaluate the crucial trade offs between model performance and
interpretability.

</details>


### [30] [Deep Progressive Training: scaling up depth capacity of zero/one-layer models](https://arxiv.org/abs/2511.04981)
*Zhiqi Bu*

Main category: cs.LG

TL;DR: 通过零/一层的渐进深度扩展实现高效大模型训练，在保持几乎相同损失的前提下显著降低计算量（对GPT-2的实验，约80% 计算节省，相当于5×加速，与60层7B参数的全量训练相比损失几乎无差）


<details>
  <summary>Details</summary>
Motivation: 深度模型的精度提升伴随计算成本的快速上升，如何在训练阶段自适应地增加模型容量以实现高效扩展，是大规模模型训练的关键问题。

Method: 提出零层/一层渐进训练策略，对深度扩展的初始化、超参数迁移、学习率调度和扩展时机进行理论与实践分析，并在GPT-2上验证。

Result: 在GPT-2上零层/一层渐进训练可节省约80%的计算，等价于约5×加速，同时取得接近完整60层模型(7B参数)的损失水平。

Conclusion: 渐进的深度扩展策略为大规模模型训练提供了一种高效权衡，能够在不显著损失的前提下提升训练速度和资源利用率。

Abstract: Model depth is a double-edged sword in deep learning: deeper models achieve
higher accuracy but require higher computational cost. To efficiently train
models at scale, an effective strategy is the progressive training, which
scales up model capacity during training, hence significantly reducing
computation with little to none performance degradation. In this work, we study
the depth expansion of large models through the lens of optimization theory and
feature learning, offering insights on the initialization of new layers,
hyperparameter transfer, learning rate schedule, and timing of model expansion.
Specifically, we propose zero/one-layer progressive training for the optimal
tradeoff between computation and loss. For example, zero/one-layer progressive
training on GPT2 can save $\approx 80\%$ compute, or equivalently accelerate
$\approx 5\times$ while achieving almost the same loss, compared to to a fully
trained 60-layer model with 7B parameters.

</details>


### [31] [Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models](https://arxiv.org/abs/2511.04988)
*Runsheng Ren,Jing Li,Yanxiu Li,Shixun Huang,Jun Shen,Wanqing Li,John Le,Sheng Wang*

Main category: cs.LG

TL;DR: 一个面向碳价预测的混合框架：将结构断点检测、信号小波去噪与深度学习模型相结合，以提升在非平稳时间序列上的预测准确性。最优组合为 PELT-WT-TCN，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 碳价格预测面临结构性断点与高频噪声，现有方法往往把去噪与建模分离，且对先进深度学习架构的系统性比较不足，限制鲁棒性与泛化性。需要一个能在结构性变化与多尺度信号特征下仍具高精度与可解释性的预测框架。

Method: 提出一个综合混合框架：采用 Bai-Perron、ICSS、PELT 三种结构断点检测；进行小波信号去噪；比较三种前馈/序列模型（LSTM、GRU、TCN）在单变量和多变量数据上的表现。数据源为欧盟碳排放许可 EUAs（2007-2024），并引入能源价格与政策等外生特征，构建不同数据集进行对比。

Result: 实验结果显示：PELT-WT-TCN 达到最高预测精度，相较于基线模型（Breakpoints with Wavelet and LSTM）RMSE下降 22.35%、MAE下降 18.63%；相较同一基线中未分解的原始 LSTM，RMSE下降 70.55%、MAE下降 74.42%。

Conclusion:  将结构感知与多尺度分解融入深度学习架构，能显著提升碳价预测及其他非平稳金融时间序列的准确性与可解释性。

Abstract: Accurately forecasting carbon prices is essential for informed energy market
decision-making, guiding sustainable energy planning, and supporting effective
decarbonization strategies. However, it remains challenging due to structural
breaks and high-frequency noise caused by frequent policy interventions and
market shocks. Existing studies, including the most recent baseline approaches,
have attempted to incorporate breakpoints but often treat denoising and
modeling as separate processes and lack systematic evaluation across advanced
deep learning architectures, limiting the robustness and the generalization
capability. To address these gaps, this paper proposes a comprehensive hybrid
framework that integrates structural break detection (Bai-Perron, ICSS, and
PELT algorithms), wavelet signal denoising, and three state-of-the-art deep
learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot
prices from 2007 to 2024 and exogenous features such as energy prices and
policy indicators, the framework constructs univariate and multivariate
datasets for comparative evaluation. Experimental results demonstrate that our
proposed PELT-WT-TCN achieves the highest prediction accuracy, reducing
forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the
state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by
70.55% in RMSE and 74.42% in MAE compared to the original LSTM without
decomposition from the same baseline study. These findings underscore the value
of integrating structural awareness and multiscale decomposition into deep
learning architectures to enhance accuracy and interpretability in carbon price
forecasting and other nonstationary financial time series.

</details>


### [32] [BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records](https://arxiv.org/abs/2511.04998)
*Daniel S. Lee,Mayra S. Haedo-Cruz,Chen Jiang,Oshin Miranda,LiRong Wang*

Main category: cs.LG

TL;DR: BiPETE 是一种结合旋转位置编码和正弦嵌入的 Transformer，针对不规则EHR时序进行单病种预测；在两个人群中无需大规模预训练，显著提升 ASUD 风险预测并具可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录中不规则就诊时间和缺乏统一结构对时序建模的挑战，提升疾病风险预测的性能与可解释性。

Method: 在 Transformer 编码器中引入双位置嵌入：旋转位置编码用于编码相对就诊时间，正弦嵌入用于保留就诊顺序；将两者整合形成 BiPETE；无大规模预训练，利用 depression 与 PTSD 两个 cohorts 的数据预测 ASUD；使用 Integrated Gradients 进行预测解释，并与基线模型比较；进行消融研究以验证双嵌入的重要性。

Result: 在抑郁队列中 AUPRC 提升约 34%，在 PTSD 队列提升约 50%；消融研究证实 dual positional encoding 的有效性；Integrated Gradients 揭示了与 ASUD 风险和保护相关的临床特征，如炎性、血液、代谢指标、特定药物和共病等，提升对风险评估过程的理解。

Conclusion: 提出一个实用且可解释的框架，基于 EHR 数据实现较强的疾病风险预测性能，且有望帮助临床风险评估和干预设计。

Abstract: Transformer-based deep learning models have shown promise for disease risk
prediction using electronic health records(EHRs), but modeling temporal
dependencies remains a key challenge due to irregular visit intervals and lack
of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder
or BiPETE for single-disease prediction, which integrates rotary positional
embeddings to encode relative visit timing and sinusoidal embeddings to
preserve visit order. Without relying on large-scale pretraining, BiPETE is
trained on EHR data from two mental health cohorts-depressive disorder and
post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and
substance use disorders (ASUD). BiPETE outperforms baseline models, improving
the area under the precision-recall curve (AUPRC) by 34% and 50% in the
depression and PTSD cohorts, respectively. An ablation study further confirms
the effectiveness of the dual positional encoding strategy. We apply the
Integrated Gradients method to interpret model predictions, identifying key
clinical features associated with ASUD risk and protection, such as abnormal
inflammatory, hematologic, and metabolic markers, as well as specific
medications and comorbidities. Overall, these key clinical features identified
by the attribution methods contribute to a deeper understanding of the risk
assessment process and offer valuable clues for mitigating potential risks. In
summary, our study presents a practical and interpretable framework for disease
risk prediction using EHR data, which can achieve strong performance.

</details>


### [33] [Multi-agent Coordination via Flow Matching](https://arxiv.org/abs/2511.05005)
*Dongsu Lee,Daehee Lee,Amy Zhang*

Main category: cs.LG

TL;DR: 提出 MAC-Flow：先学习联合行为的流表示，再蒸馏为去中心化的一步策略，实现高效快速推理与良好协作性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在协同建模丰富性与实时性之间取舍，扩散模型强表达但慢，高斯策略快但对多智能体交互鲁棒性不足。

Method: 用流模型学习联合行为的离线表示，然后将其蒸馏成去中心化的一步策略以实现实时执行。通过在四个基准、12个环境、34个数据集上的评估比较性能与推理速度。

Result: 相比扩散型 MARL，推理速度约提升 14.5 倍，同时保持良好性能；推理速度接近先前高斯策略的离线 MARL 方法。

Conclusion: MAC-Flow 缓解性能与计算成本之间的权衡，提供一个在离线 MARL 场景中既高效又具协调性的解决方案。

Abstract: This work presents MAC-Flow, a simple yet expressive framework for
multi-agent coordination. We argue that requirements of effective coordination
are twofold: (i) a rich representation of the diverse joint behaviors present
in offline data and (ii) the ability to act efficiently in real time. However,
prior approaches often sacrifice one for the other, i.e., denoising
diffusion-based solutions capture complex coordination but are computationally
slow, while Gaussian policy-based solutions are fast but brittle in handling
multi-agent interaction. MAC-Flow addresses this trade-off by first learning a
flow-based representation of joint behaviors, and then distilling it into
decentralized one-step policies that preserve coordination while enabling fast
execution. Across four different benchmarks, including $12$ environments and
$34$ datasets, MAC-Flow alleviates the trade-off between performance and
computational cost, specifically achieving about $\boldsymbol{\times14.5}$
faster inference compared to diffusion-based MARL methods, while maintaining
good performance. At the same time, its inference speed is similar to that of
prior Gaussian policy-based offline multi-agent reinforcement learning (MARL)
methods.

</details>


### [34] [OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data](https://arxiv.org/abs/2511.05028)
*Dongjin Park,Hasung Yeo,Joon-Woo Lee*

Main category: cs.LG

TL;DR: OvA-LP 是一套在PEFT式FFT中从源头抑制漂移的极简框架。通过冻结编码器进行线性探测并使用一对一对多的头部设计、两阶段流程，有效保留特征几何、解耦 logits，显著提升在强非IID场景下的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: FFT在分布异质性下易产生局部漂移，导致全局模型偏差和方差放大。现有的聚合或个性化方法多在事后纠偏，且在极端非IID下较不稳定。因此，需要在模型微调前或同时抑制漂移的机制，以提升鲁棒性。

Method: 在冻结的编码器上进行线性探测，采用一对多(one-vs-all)头部，并通过一个简单的两阶段流程实现漂移抑制；保持预训练特征的几何结构，解耦 logits 以防止导致漂移放大的机制。为提高效率，预计算编码器特征，使每轮成本与编码器大小相关性降低。

Result: 在 CIFAR-100 的 100 客户端实验中，OvA-LP 在 shard-1、shard-2、Bernoulli-Dirichlet 分区均值上保持 IID 准确率的 95.9%，而最先进的 FFT 基线分别为 PFPT 10.1% 和 FFT-MoE 34.5%（同条件下）。在对称和非对称标签噪声下也表现出鲁棒性。

Conclusion: OvA-LP 为在异质性背景下进行鲁棒 FFT 提供了一个原理性且高效的基础，尤其通过源头抑制漂移、保持特征几何与日志解耦以及预计算特征实现了显著的性能与成本优势。

Abstract: Federated fine-tuning (FFT) adapts foundation models to decentralized data
but remains fragile under heterogeneous client distributions due to local
drift, i.e., client-level update divergences that induce systematic bias and
amplified variance in the global model. Existing aggregation and
personalization methods largely correct drift post hoc, which proves brittle
under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework
that is, to our knowledge, the first explicitly designed to suppress drift at
its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing
on a frozen encoder with a one-vs-all head and a simple two-stage procedure,
preserving pretrained feature geometry and decoupling logits to prevent the
mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over
shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of
its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1%
(PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains
resilience under both symmetric and asymmetric label noise. In addition,
precomputing encoder features makes per-round cost nearly independent of
encoder size. Together, these results demonstrate that OvA-LP provides a
principled and efficient basis for robust FFT under heterogeneity.

</details>


### [35] [Usando LLMs para Programar Jogos de Tabuleiro e Variações](https://arxiv.org/abs/2511.05114)
*Álvaro Guglielmin Becker,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: 该论文评估三种大型语言模型（Claude、DeepSeek、ChatGPT）在编写棋盘游戏代码及其变体方面的能力，以及为现有游戏设计新变体的能力。


<details>
  <summary>Details</summary>
Motivation: 实现棋盘游戏程序通常耗时，LLMs具备从上下文生成代码的潜力以加速该过程；通过系统性评估，比较不同模型在代码生成、规则实现和变体设计上的表现与差异。

Method: 让三种模型在给定任务下生成棋盘游戏的代码及其变体，可能包括描述、规则、组件、胜负条件等要素；对生成的代码进行评估，关注正确性、完整性、可玩性与创新性等指标，并比较模型在不同任务与游戏类型上的表现。

Result: 摘要未给出具体实验结果，本文仅提出研究目标和评估方向，实际结果需阅读全文以获取。

Conclusion: 摘要未给出明确结论，需阅读全文以了解模型在棋盘游戏代码生成方面的实际能力与局限。

Abstract: Creating programs to represent board games can be a time-consuming task.
Large Language Models (LLMs) arise as appealing tools to expedite this process,
given their capacity to efficiently generate code from simple contextual
information. In this work, we propose a method to test how capable three LLMs
(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as
new variants of existing games.

</details>


### [36] [QuAnTS: Question Answering on Time Series](https://arxiv.org/abs/2511.05124)
*Felix Divo,Maurice Kraus,Anh Q. Nguyen,Hao Xue,Imran Razzak,Flora D. Salim,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.LG

TL;DR: 提出 QuAnTS，一种用于时间序列问答（TSQA）的大规模数据集，针对人体运动的骨架轨迹，提供问答对并评估基线与人类表现。


<details>
  <summary>Details</summary>
Motivation: 在视觉与文本问答日益火爆的背景下，时间序列问答研究相对不足，亟需通过文本与时间序列的交互提升数据可访问性、决策支持和系统透明度。

Method: 构建并公开大规模的 QuAnTS 数据集，涵盖关于骨架轨迹的人体运动的多种问答；通过广泛实验验证数据集的规范性与覆盖性，评估现有与新提出的基线模型，并提供人类基线以作为实际可用性的参考。

Result: 证实数据集结构良好、覆盖全面；在基线评估中展示了当前方法的性能瓶颈，同时给出人类表现作为参照。

Conclusion: 为TSQA的深入研究奠定基础，推动未来通过文本与时间序列的交互来改进决策和提高系统的透明度。

Abstract: Text offers intuitive access to information. This can, in particular,
complement the density of numerical time series, thereby allowing improved
interactions with time series models to enhance accessibility and
decision-making. While the creation of question-answering datasets and models
has recently seen remarkable growth, most research focuses on question
answering (QA) on vision and text, with time series receiving minute attention.
To bridge this gap, we propose a challenging novel time series QA (TSQA)
dataset, QuAnTS, for Question Answering on Time Series data. Specifically, we
pose a wide variety of questions and answers about human motion in the form of
tracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is
well-formed and comprehensive through extensive experiments. Thoroughly
evaluating existing and newly proposed baselines then lays the groundwork for a
deeper exploration of TSQA using QuAnTS. Additionally, we provide human
performances as a key reference for gauging the practical usability of such
models. We hope to encourage future research on interacting with time series
models through text, enabling better decision-making and more transparent
systems.

</details>


### [37] [DL101 Neural Network Outputs and Loss Functions](https://arxiv.org/abs/2511.05131)
*Fernando Berzal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The loss function used to train a neural network is strongly connected to its
output layer from a statistical point of view. This technical report analyzes
common activation functions for a neural network output layer, like linear,
sigmoid, ReLU, and softmax, detailing their mathematical properties and their
appropriate use cases. A strong statistical justification exists for the
selection of the suitable loss function for training a deep learning model.
This report connects common loss functions such as Mean Squared Error (MSE),
Mean Absolute Error (MAE), and various Cross-Entropy losses to the statistical
principle of Maximum Likelihood Estimation (MLE). Choosing a specific loss
function is equivalent to assuming a specific probability distribution for the
model output, highlighting the link between these functions and the Generalized
Linear Models (GLMs) that underlie network output layers. Additional scenarios
of practical interest are also considered, such as alternative output
encodings, constrained outputs, and distributions with heavy tails.

</details>


### [38] [Consecutive Preferential Bayesian Optimization](https://arxiv.org/abs/2511.05163)
*Aras Erarslan,Carlos Sevilla Salcedo,Ville Tanskanen,Anni Nisov,Eero Päiväkumpu,Heikki Aisala,Kaisu Honkapää,Arto Klami,Petrus Mikkola*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Preferential Bayesian optimization allows optimization of objectives that are
either expensive or difficult to measure directly, by relying on a minimal
number of comparative evaluations done by a human expert. Generating candidate
solutions for evaluation is also often expensive, but this cost is ignored by
existing methods. We generalize preference-based optimization to explicitly
account for production and evaluation costs with Consecutive Preferential
Bayesian Optimization, reducing production cost by constraining comparisons to
involve previously generated candidates. We also account for the perceptual
ambiguity of the oracle providing the feedback by incorporating a
Just-Noticeable Difference threshold into a probabilistic preference model to
capture indifference to small utility differences. We adapt an
information-theoretic acquisition strategy to this setting, selecting new
configurations that are most informative about the unknown optimum under a
preference model accounting for the perceptual ambiguity. We empirically
demonstrate a notable increase in accuracy in setups with high production costs
or with indifference feedback.

</details>


### [39] [Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy](https://arxiv.org/abs/2511.05169)
*Simon Baur,Tristan Ruhwedel,Ekin Böke,Zuzanna Kobus,Gergana Lishkova,Christoph Wetz,Holger Amthauer,Christoph Roderburg,Frank Tacke,Julian M. Rogasch,Wojciech Samek,Henning Jann,Jackie Ma,Johannes Eschrich*

Main category: cs.LG

TL;DR: 多模态深度学习整合实验室指标、SR-PET/CT影像及CT/病理特征，用于PRRT后神经内分泌肿瘤的PFS预测，显著优于单模态模型，需外部验证以支持临床风险分层随访。


<details>
  <summary>Details</summary>
Motivation: 在PRRT治疗的转移性NET患者中，准确预测无进展生存期（PFS）有助于个体化治疗和随访策略，减少过度或不足治疗。

Method: 回顾性单中心研究，纳入116例接受177Lu-DOTATOC治疗的转移性NET患者。收集临床特征、实验室指标和治疗前SR-PET/CT。训练七种模型：单模态（实验室、SR-PET、CT）与多模态融合。通过特征重要性分析和梯度/热力图评估可解释性。以AUROC和AUPRC评估性能。

Result: 短PFS<1年占36%，长PFS>1年占64%。短PFS与一些特征相关性：基线Chromogranin A升高、gamma-GT升高、PRRT循环次数较少。仅用实验室生物标志物的随机森林AUROC为0.59±0.02；SR-PET或CT的三维CNN AUROC分别为0.42±0.03和0.54±0.01，表现较差。多模态融合模型（实验室、SR-PET、CT，且带有预训练CT分支）AUROC0.72±0.01、AUPRC0.80±0.01，优于单模态。

Conclusion: 将SR-PET、CT与实验室生物标志物结合的多模态深度学习在PRRT后PFS预测中优于单模态方法。若得到外部验证，可能支持基于风险的随访策略。

Abstract: Peptide receptor radionuclide therapy (PRRT) is an established treatment for
metastatic neuroendocrine tumors (NETs), yet long-term disease control occurs
only in a subset of patients. Predicting progression-free survival (PFS) could
support individualized treatment planning. This study evaluates laboratory,
imaging, and multimodal deep learning models for PFS prediction in PRRT-treated
patients. In this retrospective, single-center study 116 patients with
metastatic NETs undergoing 177Lu-DOTATOC were included. Clinical
characteristics, laboratory values, and pretherapeutic somatostatin receptor
positron emission tomography/computed tomographies (SR-PET/CT) were collected.
Seven models were trained to classify low- vs. high-PFS groups, including
unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches.
Explainability was evaluated by feature importance analysis and gradient maps.
Forty-two patients (36%) had short PFS (< 1 year), 74 patients long PFS (>1
year). Groups were similar in most characteristics, except for higher baseline
chromogranin A (p = 0.003), elevated gamma-GT (p = 0.002), and fewer PRRT
cycles (p < 0.001) in short-PFS patients. The Random Forest model trained only
on laboratory biomarkers reached an AUROC of 0.59 +- 0.02. Unimodal
three-dimensional convolutional neural networks using SR-PET or CT performed
worse (AUROC 0.42 +- 0.03 and 0.54 +- 0.01, respectively). A multimodal fusion
model laboratory values, SR-PET, and CT -augmented with a pretrained CT branch
- achieved the best results (AUROC 0.72 +- 0.01, AUPRC 0.80 +- 0.01).
Multimodal deep learning combining SR-PET, CT, and laboratory biomarkers
outperformed unimodal approaches for PFS prediction after PRRT. Upon external
validation, such models may support risk-adapted follow-up strategies.

</details>


### [40] [Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models](https://arxiv.org/abs/2511.05171)
*Davide Marincione,Donato Crisostomi,Roberto Dessi,Emanuele Rodolà,Emanuele Rossi*

Main category: cs.LG

TL;DR: 通过将 NatureLM 与其基础语言模型进行简单的权重插值的模型合并，既保留域内专业性，又恢复指令跟随能力，从而显著提升对未知物种的零-shot 分类的性能，达到新的开集零-shot领域的水平。


<details>
  <summary>Details</summary>
Motivation: 在生物声学中，面向跨物种任务的基础模型具有潜力，但对领域特定微调可能牺牲指令遵循的灵活性；需要兼具领域专业性和广义指令响应能力的模型。

Method: 提出一种简单的模型合并策略，通过对 NatureLM 与其基础语言模型进行输出插值/参数融合，在保持域内知识的同时提升指令跟随能力。

Result: 所合并模型在零-shot 通用化方面显著提升，相对提升超过 200%，在未见物种的闭集零-shot 分类中创下新的 state-of-the-art。

Conclusion: 模型合并策略可在不显著损失领域知识的情况下，平衡领域专业性与指令跟随能力，提升对未见物种的零-shot 泛化能力。

Abstract: Foundation models capable of generalizing across species and tasks represent
a promising new frontier in bioacoustics, with NatureLM being one of the most
prominent examples. While its domain-specific fine-tuning yields strong
performance on bioacoustic benchmarks, we observe that it also introduces
trade-offs in instruction-following flexibility. For instance, NatureLM
achieves high accuracy when prompted for either the common or scientific name
individually, but its accuracy drops significantly when both are requested in a
single prompt. We address this by applying a simple model merging strategy that
interpolates NatureLM with its base language model, recovering
instruction-following capabilities with minimal loss of domain expertise.
Finally, we show that the merged model exhibits markedly stronger zero-shot
generalization, achieving over a 200% relative improvement and setting a new
state-of-the-art in closed-set zero-shot classification of unseen species.

</details>


### [41] [Associative Poisoning to Generative Machine Learning](https://arxiv.org/abs/2511.05177)
*Mathias Lundteigen Mohus,Jingyue Li,Zhirong Yang*

Main category: cs.LG

TL;DR: 提出一种新的数据投毒技术“联想中毒”，通过仅 perturb 训练数据来操纵生成输出中具体特征对的统计关联，而无需控制训练过程。理论上可行且隐蔽，实验在两种最先进的生成模型上验证了能有目的地增强或抑制特征关联，同时保持边际分布和输出质量，从而难以被直观看出。并讨论现有防御的局限性并给出新的对策思路。


<details>
  <summary>Details</summary>
Motivation: 在生成模型普及的背景下，数据投毒成为现实威胁。现有投毒攻击要么导致生成数据的大范围劣化，要么需要对训练过程有控制，导致在现实场景中的可行性受限。本研究提出一种无需修改训练过程的细粒度统计操控攻击，旨在破坏输出特征之间的统计关系，以达到隐蔽且具有针对性的污染。

Method: 给出对“联想中毒”的正式数学表述，证明在理论上可实现且具有隐蔽性。通过对训练数据的局部干扰来操控生成输出中的某些特征对统计关联，同时保持目标特征的边际分布不变，并确保输出质量不显著下降。通过两种前沿生成模型进行实验评估，验证该攻击能有效增大或降低特征对的联想度。

Result: 理论层面证明了攻击的可行性与隐蔽性，并在两种最先进模型上给出实证结果：攻击能在不改变边际分布的前提下诱导或抑制特定特征对的关联关系，同时保持输出质量，难以通过视觉检测发现。

Conclusion: 该研究表明生成系统在图像合成、合成数据集生成与自然语言处理等场景易受微妙、隐蔽性强的统计污染的影响，威胁到输出统计性质的完整性。对现有防御策略的局限性进行了分析，并提出一种新的对策方向以应对该风险。

Abstract: The widespread adoption of generative models such as Stable Diffusion and
ChatGPT has made them increasingly attractive targets for malicious
exploitation, particularly through data poisoning. Existing poisoning attacks
compromising synthesised data typically either cause broad degradation of
generated data or require control over the training process, limiting their
applicability in real-world scenarios. In this paper, we introduce a novel data
poisoning technique called associative poisoning, which compromises
fine-grained features of the generated data without requiring control of the
training process. This attack perturbs only the training data to manipulate
statistical associations between specific feature pairs in the generated
outputs. We provide a formal mathematical formulation of the attack and prove
its theoretical feasibility and stealthiness. Empirical evaluations using two
state-of-the-art generative models demonstrate that associative poisoning
effectively induces or suppresses feature associations while preserving the
marginal distributions of the targeted features and maintaining high-quality
outputs, thereby evading visual detection. These results suggest that
generative systems used in image synthesis, synthetic dataset generation, and
natural language processing are susceptible to subtle, stealthy manipulations
that compromise their statistical integrity. To address this risk, we examine
the limitations of existing defensive strategies and propose a novel
countermeasure strategy.

</details>


### [42] [ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy](https://arxiv.org/abs/2511.05221)
*David Bertram,Anja Ophey,Sinah Röttgen,Konstantin Kuffer,Gereon R. Fink,Elke Kalbe,Clint Hansen,Walter Maetzler,Maximilian Kapsecker,Lara M. Reimer,Stephan Jonas,Andreas T. Damgaard,Natasha B. Bertelsen,Casper Skjaerbaek,Per Borghammer,Karolien Groenewald,Pietro-Luca Ratti,Michele T. Hu,No émie Moreau,Michael Sommerauer,Katarzyna Bozek*

Main category: cs.LG

TL;DR: Open-source ActiTect：基于腕带actigraphy的自动化RBD检测工具，在多中心数据上实现强泛化，AUROC介于0.84-0.95之间，适合大规模可穿戴筛查。


<details>
  <summary>Details</summary>
Motivation: iRBD是α-synucleinopathies的早期产物性标志，需可扩展的筛查方案。现有分析管线对不同设备的适配性不足，亟需自动化、可重复且开源的检测工具。

Method: 开发了ActiTect：自动化预处理、睡眠–觉醒检测、跨设备数据对齐、可解释的运动特征提取。模型在78人队列上进行嵌套交叉验证（AUROC=0.95），在31人（AUROC=0.86）、113人（AUROC=0.84）、57人（AUROC=0.94）的独立外部队列上测试。留一数据集交叉验证AUROC为0.84-0.89。对关键预测特征进行稳定性分析；最终形成多中心的预训练模型。

Result: 在内部与外部数据集上均表现出稳定的泛化能力，表明该工具可作为广泛部署的资源。

Conclusion: 开源且易用的ActiTect可促进RBD检测在可穿戴设备领域的广泛应用，并支持独立验证与协同改进，推动构建通用的RBD检测模型。

Abstract: Isolated rapid eye movement sleep behavior disorder (iRBD) is a major
prodromal marker of $\alpha$-synucleinopathies, often preceding the clinical
onset of Parkinson's disease, dementia with Lewy bodies, or multiple system
atrophy. While wrist-worn actimeters hold significant potential for detecting
RBD in large-scale screening efforts by capturing abnormal nocturnal movements,
they become inoperable without a reliable and efficient analysis pipeline. This
study presents ActiTect, a fully automated, open-source machine learning tool
to identify RBD from actigraphy recordings. To ensure generalizability across
heterogeneous acquisition settings, our pipeline includes robust preprocessing
and automated sleep-wake detection to harmonize multi-device data and extract
physiologically interpretable motion features characterizing activity patterns.
Model development was conducted on a cohort of 78 individuals, yielding strong
discrimination under nested cross-validation (AUROC = 0.95). Generalization was
confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two
independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To
assess real-world robustness, leave-one-dataset-out cross-validation across the
internal and external cohorts demonstrated consistent performance (AUROC range
= 0.84-0.89). A complementary stability analysis showed that key predictive
features remained reproducible across datasets, supporting the final pooled
multi-center model as a robust pre-trained resource for broader deployment. By
being open-source and easy to use, our tool promotes widespread adoption and
facilitates independent validation and collaborative improvements, thereby
advancing the field toward a unified and generalizable RBD detection model
using wearable devices.

</details>


### [43] [The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss](https://arxiv.org/abs/2511.05236)
*Rui Wu,Lizheng Wang,Yongjun Li*

Main category: cs.LG

TL;DR: 提出一个零SRE的扩散框架BELM-MDCM，使SRE通过可解析可逆机制消除，确保因果信息守恒和忠实abduction，利用Targeted Modeling和Hybrid Training Objective实现因果偏置，达到个体级反事实的高保真性与更高准确度。


<details>
  <summary>Details</summary>
Motivation: 结构因果模型中对潜在外生噪声的精确推断（忠实abduction）长期存在计算挑战。扩散模型尽管具强大表征能力，但其默认设计在推理任务上造成信息损失，形成结构重建误差（SRE）。需要一个在因果理论上可行且可逆的扩展来实现无SRE的推断。

Method: 提出因果信息守恒（CIC）作为忠实abduction的必要条件；设计BELM-MDCM，这是一种通过解析可逆机制消除SRE的扩散框架；采用Targeted Modeling实现结构正则化，使用Hybrid Training Objective注入强因果 inductive bias。

Result: 在严格实验中，Zero-SRE框架达到最先进的准确度，同时实现高保真、个体层面的反事实推断，满足深度因果探究的需求。

Conclusion: 该工作将现代生成模型的能力与经典因果理论结合，提出一个更严格的标准，奠定了因果推断与生成模型耦合的新基础。

Abstract: Judea Pearl's vision of Structural Causal Models (SCMs) as engines for
counterfactual reasoning hinges on faithful abduction: the precise inference of
latent exogenous noise. For decades, operationalizing this step for complex,
non-linear mechanisms has remained a significant computational challenge. The
advent of diffusion models, powerful universal function approximators, offers a
promising solution. However, we argue that their standard design, optimized for
perceptual generation over logical inference, introduces a fundamental flaw for
this classical problem: an inherent information loss we term the Structural
Reconstruction Error (SRE). To address this challenge, we formalize the
principle of Causal Information Conservation (CIC) as the necessary condition
for faithful abduction. We then introduce BELM-MDCM, the first diffusion-based
framework engineered to be causally sound by eliminating SRE by construction
through an analytically invertible mechanism. To operationalize this framework,
a Targeted Modeling strategy provides structural regularization, while a Hybrid
Training Objective instills a strong causal inductive bias. Rigorous
experiments demonstrate that our Zero-SRE framework not only achieves
state-of-the-art accuracy but, more importantly, enables the high-fidelity,
individual-level counterfactuals required for deep causal inquiries. Our work
provides a foundational blueprint that reconciles the power of modern
generative models with the rigor of classical causal theory, establishing a new
and more rigorous standard for this emerging field.

</details>


### [44] [An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones](https://arxiv.org/abs/2511.05265)
*Taihelong Zeng,Yun Lin,Yuhe Shi,Yan Li,Zhiqing Wei,Xuanru Ji*

Main category: cs.LG

TL;DR: 提出了一种层次化的 Actor-Critic 深度强化学习框架，用于解决带无人机协作的 TSP-D 问题。通过 Transformer 编码器、最小门控单元解码器、基于最近邻稀疏注意力的编码策略，以及异步优势演员-评论家训练，在各尺度 TSP-D 实例上实现了与先进启发式方法相当甚至更优的解和更短的运行/训练时间。


<details>
  <summary>Details</summary>
Motivation: TSP-D 及其 NP-hard 的组合复杂性推动对高效求解方法的需求。深度强化学习提供自监督策略学习和自适应决策的理论框架，能够在考虑车与无人机协同的约束下提升解的质量与求解效率。

Method: 提出一个两阶段的层次化 DRL框架：1) Transformer 风格的编码器，加入经过优化的 k 最近邻稀疏注意力机制并整合全局节点特征；2) Minimal Gated Unit 解码器用于生成解序列；3) 异步优势演员-评论家（A3C）训练框架对策略与价值函数进行学习。

Result: 在规模从 N=10 到 100 的基准 TSP-D 实例上，所提模型能够在较短的平均计算时间内获得与高性能启发式算法和现有强化学习方法相当甚至更优的解；且相比其他强化学习基准，训练时间显著减少，最终性能优越。

Conclusion: 该框架证明了在带无人机协作的复杂路线优化问题上，层次化 DRL 与高效编码/解码组件的结合具有竞争力，既提升解的质量又提升训练与求解效率，显示出在大规模 TSP-D 以及相关协同运输任务中的应用潜力。

Abstract: The emergence of truck-drone collaborative systems in last-mile logistics has
positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal
extension of classical routing optimization, where synchronized vehicle
coordination promises substantial operational efficiency and reduced
environmental impact, yet introduces NP-hard combinatorial complexity beyond
the reach of conventional optimization paradigms. Deep reinforcement learning
offers a theoretically grounded framework to address TSP-D's inherent
challenges through self-supervised policy learning and adaptive
decision-making. This study proposes a hierarchical Actor-Critic deep
reinforcement learning framework for solving the TSP-D problem. The
architecture consists of two primary components: a Transformer-inspired encoder
and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel,
optimized k-nearest neighbors sparse attention mechanism specifically for
focusing on relevant spatial relationships, further enhanced by the integration
of global node features. The Minimal Gated Unit decoder processes these encoded
representations to efficiently generate solution sequences. The entire
framework operates within an asynchronous advantage actor-critic paradigm.
Experimental results show that, on benchmark TSP-D instances of various scales
(N=10 to 100), the proposed model can obtain competitive or even superior
solutions in shorter average computation times compared to high-performance
heuristic algorithms and existing reinforcement learning methods. Moreover,
compared to advanced reinforcement learning algorithm benchmarks, the proposed
framework significantly reduces the total training time required while
achieving superior final performance, highlighting its notable advantage in
training efficiency.

</details>


### [45] [Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting](https://arxiv.org/abs/2511.05289)
*Marius Fracarolli,Michael Staniek,Stefan Riezler*

Main category: cs.LG

TL;DR: 通过数据增强缓解TSF中成员资格推断攻击（MIA），ZOO-PCA在降低攻击误报/真阳性比方面表现最佳且不降低模型性能。


<details>
  <summary>Details</summary>
Motivation: 在包含电子健康记录（EHR）的时间序列预测任务中，需在提供强隐私保护和维持高预测性能之间取得平衡，同时提高对MIA的鲁棒性。

Method: 通过用合成数据对模型进行再训练来削弱基于损失的MIA攻击；比较ZOO、ZOO-PCA（ZOO的PCA约束变体）和MixUp等数据增强策略对模型鲁棒性和准确率的影响。

Result: ZOO-PCA在降低MIA的真正阳性率与假阳性率之比方面最优，同时对测试集的预测性能无显著下降。

Conclusion: 合成数据与特定的数据增强策略（尤其是ZOO-PCA）可提高对MIA的鲁棒性且不明显损害泛化能力，推荐作为防御MIA的有效方法。

Abstract: Balancing strong privacy guarantees with high predictive performance is
critical for time series forecasting (TSF) tasks involving Electronic Health
Records (EHR). In this study, we explore how data augmentation can mitigate
Membership Inference Attacks (MIA) on TSF models. We show that retraining with
synthetic data can substantially reduce the effectiveness of loss-based MIAs by
reducing the attacker's true-positive to false-positive ratio. The key
challenge is generating synthetic samples that closely resemble the original
training data to confuse the attacker, while also introducing enough novelty to
enhance the model's ability to generalize to unseen data. We examine multiple
augmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO
constrained by Principal Component Analysis (ZOO-PCA), and MixUp - to
strengthen model resilience without sacrificing accuracy. Our experimental
results show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA
attacks without sacrificing performance on test data.

</details>


### [46] [Attention and Compression is all you need for Controllably Efficient Language Models](https://arxiv.org/abs/2511.05313)
*Jatin Prakash,Aahlad Puli,Rajesh Ranganath*

Main category: cs.LG

TL;DR: CAT（Compress & Attend Transformer）是一种简单的变换器架构，通过对序列块进行压缩并进行密集注意力来解码，从而实现可调的质量与计算效率之间的权衡。相较于现有的高效方法，CAT在多种预算下表现更优，且在某些场景可与Dense Transformer媲美甚至更快、内存更少。


<details>
  <summary>Details</summary>
Motivation: 解决注意力的二次方成本带来的计算与内存瓶颈，以及现有高效方法在质量、记忆与延迟之间固定折中所带来的不灵活性；希望实现一个在不同内存/延迟预算下都可自适应的架构，避免手工设计的混合注意力或复杂的递归状态更新规则。

Method: CAT仅使用两项简单组件：密集注意力和压缩。通过对序列的压缩块进行注意力计算，CAT在较短序列上解码，从而实现计算与内存的节省。通过引入多种块大小进行训练，CAT能够在测试时以单一自适应架构在不同质量-计算权衡之间切换，而无需重新训练。

Result: 在标准语言建模任务、在-context recall和长上下文理解等方面进行了穷举评估，单一自适应CAT模型在不同计算-内存预算下优于现有高效基线（包括混合架构）。在模型尺度上，CAT与Dense Transformer在语言建模上的表现相当，同时速度提升1.4-3倍，总内存需求降低2-9倍。

Conclusion: CAT提供了一种简单而强大的自适应长上下文建模方案，在不重新训练的前提下实现灵活的质量与计算权衡，并能在规模化应用中显著降低资源消耗，同时保持或接近Dense Transformer的性能。

Abstract: The quadratic cost of attention in transformers motivated the development of
efficient approaches: namely sparse and sliding window attention, convolutions
and linear attention. Although these approaches result in impressive reductions
in compute and memory, they often trade-off with quality, specifically
in-context recall performance. Moreover, apriori fixing this quality-compute
tradeoff means being suboptimal from the get-go: some downstream applications
require more memory for in-context recall, while others require lower latency
and memory. Further, these approaches rely on heuristic choices that
artificially restrict attention, or require handcrafted and complex recurrent
state update rules, or they must be carefully composed with attention at
specific layers to form a hybrid architecture that complicates the design
process, especially at scale. To address above issues, we propose Compress &
Attend Transformer (CAT), a conceptually simple architecture employing two
simple ingredients only: dense attention and compression. CAT decodes chunks of
tokens by attending to compressed chunks of the sequence so far. Compression
results in decoding from a reduced sequence length that yields compute and
memory savings, while choosing a particular chunk size trades-off quality for
efficiency. Moreover, CAT can be trained with multiple chunk sizes at once,
unlocking control of quality-compute trade-offs directly at test-time without
any retraining, all in a single adaptive architecture. In exhaustive
evaluations on common language modeling tasks, in-context recall, and
long-context understanding, a single adaptive CAT model outperforms existing
efficient baselines, including hybrid architectures, across different
compute-memory budgets. Further, a single CAT matches dense transformer in
language modeling across model scales while being 1.4-3x faster and requiring
2-9x lower total memory usage.

</details>


### [47] [Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval](https://arxiv.org/abs/2511.05325)
*Janet Jenq,Hongda Shen*

Main category: cs.LG

TL;DR: 通过在产品图像上直接渲染相关文本实现vision-text压缩，提升跨模态检索的准确性。该方法在三个垂直领域的数据集和六个模型上显示出单模态和多模态检索的显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP等视觉语言模型对Typographic攻击的脆弱性，反向攻击逻辑：通过在图像上嵌入有效文本来强化图像-文本对齐，从而提升零-shot的电商检索性能。

Method: 将产品元数据（标题、描述等）可视化地渲染到产品图像上，进行vision-text压缩；在鞋类、手袋、交易卡等三个领域数据集上评估六个前沿视觉模型的检出能力，比较单模态和多模态检索性能。

Result: 在所有数据集和模型族中，单模态和多模态检索准确率均实现持续提升，验证了该策略对零样本多模态检索的有效性。

Conclusion: 将产品元数据直观呈现在图像上是一种简单且有效的零-shot多模态检索增强方法，适用于电子商务场景。

Abstract: Multimodal product retrieval systems in e-commerce platforms rely on
effectively combining visual and textual signals to improve search relevance
and user experience. However, vision-language models such as CLIP are
vulnerable to typographic attacks, where misleading or irrelevant text embedded
in images skews model predictions. In this work, we propose a novel method that
reverses the logic of typographic attacks by rendering relevant textual content
(e.g., titles, descriptions) directly onto product images to perform
vision-text compression, thereby strengthening image-text alignment and
boosting multimodal product retrieval performance. We evaluate our method on
three vertical-specific e-commerce datasets (sneakers, handbags, and trading
cards) using six state-of-the-art vision foundation models. Our experiments
demonstrate consistent improvements in unimodal and multimodal retrieval
accuracy across categories and model families. Our findings suggest that
visually rendering product metadata is a simple yet effective enhancement for
zero-shot multimodal retrieval in e-commerce applications.

</details>


### [48] [Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media](https://arxiv.org/abs/2511.05357)
*Mikhail Tsukerman,Konstantin Grotov,Pavel Ginzburg*

Main category: cs.LG

TL;DR: Diffusion-model-based inverse design for metasurfaces that maps target scattering profiles to structured geometries, enabling fast, diverse designs.


<details>
  <summary>Details</summary>
Motivation: 在避免高成本迭代优化的同时，解决电磁逆设计中的非唯一性问题，利用条件扩散模型直接从目标差分散射截面生成结构化介质几何形状。

Method: 采用1D U-Net并结合Feature-wise Linear Modulation（FiLM），以目标角散射模式为条件，训练于1.1万组仿真 metasurface 数据，输出2x2的介质球结构；通过扩散采样实现对设计的多样性并优先选择可行解。

Result: 在未见目标上的中位数MPE低于19%（最佳1.39%），性能优于CMA-ES进化优化；设计时间从小时级缩短至秒级。

Conclusion: 显示扩散模型在电磁逆设计中的潜力，能够快速探索复杂的 metasurface 架构并促进新一代光子与无线通信系统的开发。代码公开可用。

Abstract: We present a conditional diffusion model for electromagnetic inverse design
that generates structured media geometries directly from target differential
scattering cross-section profiles, bypassing expensive iterative optimization.
Our 1D U-Net architecture with Feature-wise Linear Modulation learns to map
desired angular scattering patterns to 2x2 dielectric sphere structure,
naturally handling the non-uniqueness of inverse problems by sampling diverse
valid designs. Trained on 11,000 simulated metasurfaces, the model achieves
median MPE below 19% on unseen targets (best: 1.39%), outperforming CMA-ES
evolutionary optimization while reducing design time from hours to seconds.
These results demonstrate that employing diffusion models is promising for
advancing electromagnetic inverse design research, potentially enabling rapid
exploration of complex metasurface architectures and accelerating the
development of next-generation photonic and wireless communication systems. The
code is publicly available at
https://github.com/mikzuker/inverse_design_metasurface_generation.

</details>


### [49] [Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction](https://arxiv.org/abs/2511.05396)
*Yiting He,Zhishuai Liu,Weixin Wang,Pan Xu*

Main category: cs.LG

TL;DR: 在线强化学习在鲁棒马尔可夫决策过程（RMDP）中的探索挑战，通过引入 supremal visitation ratio 衡量训练与部署动态的错位，给出在线情形下的子线性遗憾界，并提出在基于 f-散度的不确定性下有效的算法及其匹配的下界与实证验证。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习环境中训练和部署之间存在转移动力学错位的实际问题，传统方法对探索的挑战往往被生成模型或现成数据所掩盖，缺乏对在线探索难度的理论分析和算法保障。

Method: 引入 supremal visitation ratio 量化训练动态与部署动态之间的错位；在带有 f-散度转移不确定性的在线 RMDP 中，提出一个计算可行的算法，获得子线性遗憾并推导上/下界；给出与 supremal visitation ratio 与交互轮数的最优依赖关系的下界；通过数值实验验证理论结果。

Result: 提出的算法在在线 RMDP 设置下实现对数级别以下的遗憾增长（子线性），并给出与 supremal visitation ratio 的最优依赖关系，与交互轮数的低/上界一致；理论结果由实验得到支持。

Conclusion: 工作将探索难度量化（supremal visitation ratio）引入在线 RMDP，证明在动态错位情景下存在计算上可行且理论最优的学习算法，并通过实证验证其有效性。

Abstract: Off-dynamics reinforcement learning (RL), where training and deployment
transition dynamics are different, can be formulated as learning in a robust
Markov decision process (RMDP) where uncertainties in transition dynamics are
imposed. Existing literature mostly assumes access to generative models
allowing arbitrary state-action queries or pre-collected datasets with a good
state coverage of the deployment environment, bypassing the challenge of
exploration. In this work, we study a more realistic and challenging setting
where the agent is limited to online interaction with the training environment.
To capture the intrinsic difficulty of exploration in online RMDPs, we
introduce the supremal visitation ratio, a novel quantity that measures the
mismatch between the training dynamics and the deployment dynamics. We show
that if this ratio is unbounded, online learning becomes exponentially hard. We
propose the first computationally efficient algorithm that achieves sublinear
regret in online RMDPs with $f$-divergence based transition uncertainties. We
also establish matching regret lower bounds, demonstrating that our algorithm
achieves optimal dependence on both the supremal visitation ratio and the
number of interaction episodes. Finally, we validate our theoretical results
through comprehensive numerical experiments.

</details>


### [50] [ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids](https://arxiv.org/abs/2511.05420)
*Emad Efatinasab,Nahal Azadi,Davide Dalle Pezze,Gian Antonio Susto,Chuadhry Mujeeb Ahmed,Mirco Rampazzo*

Main category: cs.LG

TL;DR: 提出一种基于持续学习的智能电网故障预测框架 ProDER，通过原型对比学习、对数蒸馏和原型引导回放记忆，结合四个现实评估场景，在类增量和域增量任务中实现了对新故障类型和新区域的鲁棒适应，显示在多种CL方法中表现最佳，只带来极小的准确率下降。


<details>
  <summary>Details</summary>
Motivation: 随着智能电网向更大规模、更多元化的运行条件发展，及时准确地预测故障变得更加关键。然而现有基于AI的故障预测模型在面对新故障类型和新运行区域时往往难以保持可靠性，需具备持续学习能力以跟随环境演化。

Method: 提出一个持续学习框架 ProDER，在原型对比学习、对数蒸馏和原型引导回放记忆的基础上，结合原型为基础的特征正则化、日志 输出蒸馏与原型驱动的回放机制，设计四个基于类增量与域增量的评估场景以模拟电网条件演化。

Result: 在所比较的持续学习技术中，ProDER表现最佳，对故障类型预测的准确率下降仅为 0.045，对故障区域预测的下降仅为 0.015，显示其在演化环境中的鲁棒性与实用性。

Conclusion: CL在智能电网中的可扩展、现实场景故障预测具有实际意义；ProDER作为统一的回放式持续学习框架，能有效整合原型正则化、蒸馏与原型记忆等机制，提升在类增量与域增量任务中的适应性。

Abstract: As smart grids evolve to meet growing energy demands and modern operational
challenges, the ability to accurately predict faults becomes increasingly
critical. However, existing AI-based fault prediction models struggle to ensure
reliability in evolving environments where they are required to adapt to new
fault types and operational zones. In this paper, we propose a continual
learning (CL) framework in the smart grid context to evolve the model together
with the environment. We design four realistic evaluation scenarios grounded in
class-incremental and domain-incremental learning to emulate evolving grid
conditions. We further introduce Prototype-based Dark Experience Replay
(ProDER), a unified replay-based approach that integrates prototype-based
feature regularization, logit distillation, and a prototype-guided replay
memory. ProDER achieves the best performance among tested CL techniques, with
only a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone
prediction. These results demonstrate the practicality of CL for scalable,
real-world fault prediction in smart grids.

</details>


### [51] [APP: Accelerated Path Patching with Task-Specific Pruning](https://arxiv.org/abs/2511.05442)
*Frauke Andersen,William Rudman,Ruochen Zhang,Carsten Eickhoff*

Main category: cs.LG

TL;DR: 提出 Accelerated Path Patching (APP) 与 Contrastive-FLAP 剪枝相结合，用以加速电路发现并保持任务性能，但得到的电路覆盖度高且与以往电路存在较大重叠.


<details>
  <summary>Details</summary>
Motivation: 降低电路发现在可解释性工作流中的计算成本，尤其对较小模型的分析，解决 Path Patching 的计算密集性与电路分析深度不足问题。

Method: 先应用 Contrastive-FLAP 剪枝以降低搜索空间；该剪枝基于因果中介分析框架对任务相关的注意力头赋予更高的剪枝分数；随后对剩余注意力头应用传统 Path Patching，达到更快的电路发现。

Result: Contrastive-FLAP 将任务相关头保留得更好，生成的稀疏模型性能高于传统剪枝；整体用于电路发现的搜索空间平均缩小约 56%；在剩余头上应用 Path Patching 的速度提升约 59.63%-93.27%，相对于对密集模型的 Path Patching；但 APP 得到的电路在最小性约束下往往较大，且与以往 Path Patching 电路存在较高重叠，性能相近。

Conclusion: APP 提供了显著的计算节省，同时保留了任务性能，但在所有权衡中，得到的电路与已有电路的相似性较高，需进一步提高电路的最小性及差异性以增强可解释性的独立性。

Abstract: Circuit discovery is a key step in many mechanistic interpretability
pipelines. Current methods, such as Path Patching, are computationally
expensive and have limited in-depth circuit analysis for smaller models. In
this study, we propose Accelerated Path Patching (APP), a hybrid approach
leveraging our novel contrastive attention head pruning method to drastically
reduce the search space of circuit discovery methods. Our Contrastive-FLAP
pruning algorithm uses techniques from causal mediation analysis to assign
higher pruning scores to task-specific attention heads, leading to higher
performing sparse models compared to traditional pruning techniques. Although
Contrastive-FLAP is successful at preserving task-specific heads that existing
pruning algorithms remove at low sparsity ratios, the circuits found by
Contrastive-FLAP alone are too large to satisfy the minimality constraint
required in circuit analysis. APP first applies Contrastive-FLAP to reduce the
search space on required for circuit discovery algorithms by, on average, 56\%.
Next, APP, applies traditional Path Patching on the remaining attention heads,
leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to
the dense model. Despite the substantial computational saving that APP
provides, circuits obtained from APP exhibit substantial overlap and similar
performance to previously established Path Patching circuits

</details>


### [52] [Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models](https://arxiv.org/abs/2511.05460)
*Sarkar Snigdha Sarathi Das,Palash Goyal,Mihir Parmar,Yiwen Song,Long T. Le,Lesly Miculicich,Jinsung Yoon,Rui Zhang,Hamid Palangi,Tomas Pfister*

Main category: cs.LG

TL;DR: Synapse：一种基于自适应权重与分位数采样的多模型仲裁框架，用于时间序列预报，能显著胜过单一TSFM及常见集成方法。


<details>
  <summary>Details</summary>
Motivation: 不同TSFM在任务、领域和预测区间上表现差异显著，单一模型难以覆盖所有场景，需通过仲裁机制整合多模型的专长。

Method: 分析不同TSFM在多种 forecasting settings 下的专门化性能及模型选择与预测 horizon 分布对仲裁效果的影响，提出Synapse：动态池化TSFM、基于相对性能分配权重、并自适应从组成模型分位输出中采样以构建鲁棒预测分布。

Result: 实验结果表明，Synapse 在多数情形优于流行的集成方法和单一TSFM，证实其在时间序列预测中的有效性。

Conclusion: 通过动态仲裁与分位数采样构建鲁棒分布，Synapse 能有效利用不同TSFM的专长来提升预测性能与鲁棒性，仲裁策略具有广泛的研究价值。

Abstract: Pre-trained Time Series Foundational Models (TSFMs) represent a significant
advance, capable of forecasting diverse time series with complex
characteristics, including varied seasonalities, trends, and long-range
dependencies. Despite their primary goal of universal time series forecasting,
their efficacy is far from uniform; divergent training protocols and data
sources cause individual TSFMs to exhibit highly variable performance across
different forecasting tasks, domains, and horizons. Leveraging this
complementary expertise by arbitrating existing TSFM outputs presents a
compelling strategy, yet this remains a largely unexplored area of research. In
this paper, we conduct a thorough examination of how different TSFMs exhibit
specialized performance profiles across various forecasting settings, and how
we can effectively leverage this behavior in arbitration between different time
series models. We specifically analyze how factors such as model selection and
forecast horizon distribution can influence the efficacy of arbitration
strategies. Based on this analysis, we propose Synapse, a novel arbitration
framework for TSFMs. Synapse is designed to dynamically leverage a pool of
TSFMs, assign and adjust predictive weights based on their relative,
context-dependent performance, and construct a robust forecast distribution by
adaptively sampling from the output quantiles of constituent models.
Experimental results demonstrate that Synapse consistently outperforms other
popular ensembling techniques as well as individual TSFMs, demonstrating
Synapse's efficacy in time series forecasting.

</details>


### [53] [SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning](https://arxiv.org/abs/2511.05462)
*Xiaodong Wang,Jing Huang,Kevin J Liang*

Main category: cs.LG

TL;DR: The paper connects unsupervised clustering methods with classical mixture models to derive SiamMM, achieving state-of-the-art results in self-supervised learning and revealing alignment with ground-truth labels, with implications for data quality.


<details>
  <summary>Details</summary>
Motivation: Clustering-based self-supervised methods are effective but heuristic; a principled framework is needed to understand and improve them and to identify mislabeling.

Method: The authors establish theoretical links between clustering approaches and mixture models, and introduce SiamMM, a model that leverages this framework to enhance clustering-based self-supervised learning.

Result: SiamMM achieves state-of-the-art performance on multiple self-supervised benchmarks; the learned clusters show strong resemblance to unseen ground-truth labels, indicating potential mislabeling in data.

Conclusion: A mixture-model perspective provides principled improvements for clustering-based self-supervised learning and offers practical benefits in model performance and data quality assessment.

Abstract: Recent studies have demonstrated the effectiveness of clustering-based
approaches for self-supervised and unsupervised learning. However, the
application of clustering is often heuristic, and the optimal methodology
remains unclear. In this work, we establish connections between these
unsupervised clustering methods and classical mixture models from statistics.
Through this framework, we demonstrate significant enhancements to these
clustering methods, leading to the development of a novel model named SiamMM.
Our method attains state-of-the-art performance across various self-supervised
learning benchmarks. Inspection of the learned clusters reveals a strong
resemblance to unseen ground truth labels, uncovering potential instances of
mislabeling.

</details>


### [54] [On Flow Matching KL Divergence](https://arxiv.org/abs/2511.05480)
*Maojiang Su,Jerry Yao-Chieh Hu,Sophia Pi,Han Liu*

Main category: cs.LG

TL;DR: 给出 flow-matching 的非渐近、确定性上界：若 L2 损失被界定为 ε^2，则真实分布与估计分布之间的 KL 散度被上界为 A1 ε + A2 ε^2；常数仅取决于数据与速度场的正则性。结果隐含 Flow Matching Transformers 在 TV 距离下的统计收敛率，且接近最小极大估计（minimax）的效率，与扩散模型在 TV 距离下的统计效率相当，并有数值验证。


<details>
  <summary>Details</summary>
Motivation: 建立 flow-matching 框架的有限样本（非渐近）理论保证，量化 L2 损失转化为 KL 散度和 TV 收敛的关系，并将其与扩散模型的统计效率进行对比，以评估该方法的理论与实践价值。

Method: 推导一个确定性非渐近上界：在 L2 flow-matching 损失被上界为 ε^2 时，得到 KL 散度的上界 A1 ε + A2 ε^2；上型子包括数据与速度场的正则性。进一步推导在 TV 距离下的统计收敛率，并与近似极小极大（minimax）界进行对比；并通过合成数据与学习得到的速度场进行数值验证。

Result: 得到 KL 上界：KL(P||Q) ≤ A1 ε + A2 ε^2，其中 A1、A2 仅依赖于数据与速度场的正则性；推导出 Flow Matching Transformers 在 TV 距离下的统计收敛率；表明 Flow Matching 在估计光滑分布方面接近 minimax 效率，与扩散模型在 TV 距离下的统计效率相当；并通过数值实验（合成与学习得到的速度场）验证理论。

Conclusion: 结论是 Flow Matching 具有良好的统计效率，理论上可在非渐近情形给出可靠的上界，并在 TV 距离下与扩散模型相当；数值研究支持理论预测，强调该框架在学习流场驱动的分布估计中的潜力。

Abstract: We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler
(KL) divergence of the flow-matching distribution approximation. In particular,
if the $L_2$ flow-matching loss is bounded by $\epsilon^2 > 0$, then the KL
divergence between the true data distribution and the estimated distribution is
bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$
depend only on the regularities of the data and velocity fields. Consequently,
this bound implies statistical convergence rates of Flow Matching Transformers
under the Total Variation (TV) distance. We show that, flow matching achieves
nearly minimax-optimal efficiency in estimating smooth distributions. Our
results make the statistical efficiency of flow matching comparable to that of
diffusion models under the TV distance. Numerical studies on synthetic and
learned velocities corroborate our theory.

</details>


### [55] [SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive Cross-Component Learning](https://arxiv.org/abs/2511.05482)
*Kang Yang,Yuanlin Yang,Yuning Chen,Sikai Yang,Xinyu Zhang,Wan Du*

Main category: cs.LG

TL;DR: A calibration-free wireless soil sensing system (SoilX) that jointly estimates six components (M, N, P, K, C, Al) to mitigate texture-related recalibration. It introduces Contrastive Cross-Component Learning (3CL) with Orthogonality Regularizer and Separation Loss to disentangle cross-component interference and a tetrahedral, antenna-switching array to robustly measure soil dielectric permittivity regardless of placement. Empirical results show 23.8%-31.5% error reduction over baselines and good generalization to unseen fields.


<details>
  <summary>Details</summary>
Motivation: Current wireless soil sensing methods require recalibration to handle soil texture variations (aluminosilicates and organic carbon). This limits practicality. A calibration-free approach that can accurately measure multiple components across textures is needed.

Method: Jointly estimate six soil properties (M, N, P, K, C, Al) by explicitly modeling C and Al to decouple texture effects. Propose Contrastive Cross-Component Learning (3CL) with Orthogonality Regularizer and Separation Loss to disentangle interference among components. Develop a tetrahedral antenna array with an antenna-switching mechanism to robustly measure soil dielectric permittivity independent of device placement.

Result: SoilX reduces estimation errors by 23.8% to 31.5% compared to baselines and generalizes well to unseen fields.

Conclusion: Calibration-free multi-component soil sensing is feasible via explicit texture-component modeling, cross-component disentanglement learning, and robust antenna design, enabling practical deployment in precision agriculture.

Abstract: Precision agriculture demands continuous and accurate monitoring of soil
moisture (M) and key macronutrients, including nitrogen (N), phosphorus (P),
and potassium (K), to optimize yields and conserve resources. Wireless soil
sensing has been explored to measure these four components; however, current
solutions require recalibration (i.e., retraining the data processing model) to
handle variations in soil texture, characterized by aluminosilicates (Al) and
organic carbon (C), limiting their practicality. To address this, we introduce
SoilX, a calibration-free soil sensing system that jointly measures six key
components: {M, N, P, K, C, Al}. By explicitly modeling C and Al, SoilX
eliminates texture- and carbon-dependent recalibration. SoilX incorporates
Contrastive Cross-Component Learning (3CL), with two customized terms: the
Orthogonality Regularizer and the Separation Loss, to effectively disentangle
cross-component interference. Additionally, we design a novel tetrahedral
antenna array with an antenna-switching mechanism, which can robustly measure
soil dielectric permittivity independent of device placement. Extensive
experiments demonstrate that SoilX reduces estimation errors by 23.8% to 31.5%
over baselines and generalizes well to unseen fields.

</details>


### [56] [DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction](https://arxiv.org/abs/2511.05483)
*Abigail Lin*

Main category: cs.LG

TL;DR: 提出 DGTN（Diffused Graph-Transformer Network），通过可学习的扩散机制实现GNN结构先验与Transformer注意力的双向协同，以预测蛋白质突变对热力学稳定性的ΔΔG，并在 ProTherm 与 SKEMPI 基准上达到最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往分别处理序列和结构信息，未能有效捕捉局部几何结构与全局序列模式之间的耦合，导致对稳定性预测的性能受限。通过将结构先验的GNN嵌入与Transformer注意力以扩散方式协同学习，期望提升序列-结构耦合的建模能力。

Method: 提出双向扩散框架：GNN嵌入用于引导Transformer注意力的可学习扩散核，Transformer表示再通过注意力调制的图更新反向 refining GNN消息传递。理论分析证明该耦合在近似界面上优于独立处理，收敛性与收敛速率在扩散步数T下为O(1/√T)。在 ProTherm 与 SKEMPI 数据集上进行评估，使用端到端学习。

Result: 在 ProTherm 与 SKEMPI 基准上达到最优性能：Pearson Rho=0.87，RMSE=1.21 kcal/mol，比最佳基线提升约6.2%；消融实验显示扩散机制对相关性提升约4.8点。

Conclusion: 给出一个通过可学习扩散整合异质蛋白表示的原理性框架，并给出收敛性证明与理论界限，实验上实现了对蛋白质突变热力学稳定性预测的显著提升。

Abstract: Predicting the effect of amino acid mutations on enzyme thermodynamic
stability (DDG) is fundamental to protein engineering and drug design. While
recent deep learning approaches have shown promise, they often process sequence
and structure information independently, failing to capture the intricate
coupling between local structural geometry and global sequential patterns. We
present DGTN (Diffused Graph-Transformer Network), a novel architecture that
co-learns graph neural network (GNN) weights for structural priors and
transformer attention through a diffusion mechanism. Our key innovation is a
bidirectional diffusion process where: (1) GNN-derived structural embeddings
guide transformer attention via learnable diffusion kernels, and (2)
transformer representations refine GNN message passing through
attention-modulated graph updates. We provide rigorous mathematical analysis
showing this co-learning scheme achieves provably better approximation bounds
than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves
state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with
6.2% improvement over best baselines. Ablation studies confirm the diffusion
mechanism contributes 4.8 points to correlation. Our theoretical analysis
proves the diffused attention converges to optimal structure-sequence coupling,
with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work
establishes a principled framework for integrating heterogeneous protein
representations through learnable diffusion.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [57] [OPF-Based Optimal Power System Network Restoration Considering Frequency Dynamics](https://arxiv.org/abs/2511.04777)
*Dawn Virginillo,Asja Derviškadić,Mario Paolone*

Main category: eess.SP

TL;DR: 将静态最优恢复序列与动态频率约束结合起来，证明静态最优解在动态稳定性上可能不可行；在IEEE 9-母线系统的应用中展示动态规划的重要性。


<details>
  <summary>Details</summary>
Motivation: 应对电网在恢复阶段的低同频惯性与日益复杂性，重新评估并确保静态与动态稳定性在PSR计划中的有效性。

Method: 提出包含频率动力学的最优PSR问题；用穷举树搜索验证静态版本的全局最优切换约束；将动态问题应用于IEEE 9-Bus模型，比较静态与动态结果，验证静态最优可能侵犯动态约束。

Result: 静态最优解可能违反动态约束，需将频率动力学纳入PSR规划；动态 formulation在IEEE 9-Bus上证明了动态考虑的重要性，静态解的可行性被动摇。

Conclusion: 将频率动态性纳入PSR规划是必要的，静态优化不足以保证恢复过程的稳定性和安全性，需采用动态约束驱动的恢复序列来确保系统在再分区与再同步过程中的稳定性。

Abstract: Due to recent blackout and system split incidents in power grids worldwide,
as well as increased system complexity in view of the energy transition, there
has been increasing interest in re-evaluating existing Power System Restoration
(PSR) plans. In restoration scenarios, due to low island inertia, it is
necessary to ensure not only the static, but also the dynamic stability of the
system. In this paper, we pose and solve a formulation of the optimal PSR
problem including frequency dynamics. We validate the switching constraints for
global optimality within a static version of the formulation using a
brute-force tree search method. We apply the dynamic problem formulation to the
IEEE 9-Bus model, and show that the optimal switching sequence using the static
formulation would violate dynamic constraints, illustrating the importance of
dynamic considerations in PSR planning.

</details>


### [58] [Joint Power Allocation and Radiation Optimization in NOMA-Assisted Pinching Antenna Systems](https://arxiv.org/abs/2511.04861)
*Nikoloz Vashakidze,Chadi Assi,Mohamed Elhattab,Ali Ghrayeb,Maurice J. Khabbaz*

Main category: eess.SP

TL;DR: 提出一种在带有NOMA的下行PASS系统中，对发射功率分配和辐射系数进行联合优化以最大化系统和。


<details>
  <summary>Details</summary>
Motivation: 在非正交多址和可调谐PASS辐射的场景中，存在非凸且复杂的优化问题，需要在功率分配与辐射功率之间进行协同优化以提升系统容量。

Method: 将原问题分解为两子问题：一是BS对NOMA用户的功率分配的闭式解；二是对PA辐射功率的收敛近似优化（SCA）。通过交替优化AO迭代求解，并比较两种解码序：穷举搜索的高复杂度和基于事先信道信息的低复杂度方案。

Result: 所提方法在数值仿真中显著提升系统和相较于广泛采用的等功率分配PASS方案。

Conclusion: 联合功率分配与辐射功率优化、以及解码序的选择，对PASS-NOMA系统的性能提升具有显著作用，且所提算法可在迭代中收敛。

Abstract: This paper explores a joint optimization of transmit power allocation and
radiation coefficients in a downlink Pinching Antenna SyStem (PASS) employing
Non-Orthogonal Multiple Access (NOMA). By leveraging the PASS-enabled flexible
channel adjustment and NOMA's power allocation adaptability, a sum rate
maximization problem is formulated with the objective of simultaneously
optimizing base station (BS)'s transmit power coefficients and pinching antenna
(PA)'s radiation powers. Due to its non-convexity and complexity, the
formulated optimization problem is challenging to solve directly. Hence, we
decompose the main problem into two sub-problems, namely transmit power
allocation sub-problem and PA radiation power allocation sub-problem. In the
first sub-problem, closed-form solutions are derived for the BS's power
allocation among NOMA users. Meanwhile, in the second sub-problem, we optimize
the PA's radiation power utilizing successive convex approximation (SCA). These
two sub-problems are solved alternatively using Alternating Optimization (AO)
until convergence. It should be noted that decoding order plays a significant
role in NOMA-assisted PASS. Hence, two variations of decoding order are
considered, namely: i) a high-complexity exhaustive search approach, and, ii) a
low-complexity alternative that utilizes pre-determined channel information.
Numerical results show that our proposed approach substantially improves the
system's sum-rate compared to widely adopted equal power allocation PASS
schemes.

</details>


### [59] [Two-timescale Beamforming Optimization for Downlink Multi-user Holographic MIMO Surfaces](https://arxiv.org/abs/2511.04908)
*Haochen Wu,Yuanbin Chen,Yang Ming,Zhaocheng Wang*

Main category: eess.SP

TL;DR: 提出一种两时域优化的 HMIMOS 系统：通过统计 CSI 进行慢变参数的波束成形优化，并利用瞬时 CSI 设计预编码，从而降低计算与信令开销，同时满足用户 QoS 要求。


<details>
  <summary>Details</summary>
Motivation: 随着超材料与超表面的快速发展，HMIMOS 能通过密集辐射单元实现高精度波束成形，但若依赖传统基于瞬时 CSI 的优化，计算量和信令开销呈现指数级增长，难以在实际系统中落地。

Method: 提出两时域优化方案。第一步在慢变的统计 CSI 上对基站和用户端的波束形状进行优化，采用约束随机凸逼近（CSSCA）算法；第二步利用瞬时 CSI 设计预编码矩阵，以确保系统性能，同时因 HMIMOS 仅有少量馈电端口，计算成本显著降低。

Result: 通过仿真验证，该方法在功率消耗和 QoS 满足度方面优于若干基线方案，且显著降低了计算复杂度与信令开销。

Conclusion: 所提出的两时域优化框架在保持 QoS 的前提下有效降低 HMIMOS 的实现成本，具有较强的实用潜力与应用前景。

Abstract: Benefiting from the rapid development of metamaterials and metasurfaces, the
holographic multiple-input and multiple-output surface (HMIMOS) has been
regarded as a promising solution for future wireless networks recently. By
densely packing numerous radiation elements together, HMIMOS is capable of
realizing accurate beamforming with low hardware complexity. However, enormous
radiation elements on the HMIMOS lead to high computational complexity and
signaling overhead when applying traditional beamforming schemes relying on
instantaneous channel state information (CSI). To tackle this problem, we
propose a two-timescale optimization scheme to minimize the required
transmission power under the constraint of all users' quality-of-service (QoS).
Specifically, the beampatterns at the base station (BS) and the user equippment
(UE) are optimized over the slowly changing statistical CSI based on the
constrained stochastic successive convex approximation (CSSCA) algorithm. Then,
the instantaneous CSI is utilized to design the precoding matrix in order to
ensure the system performance without significant increase in computational
cost, due to the small number of feeds on the HMIMOS. Simulation results
demonstrate the effectiveness of our proposed method compared to other
baselines.

</details>


### [60] [Average and Worst-case Analysis of MIMO Beamforming Loss due to Hardware Impairments](https://arxiv.org/abs/2511.05090)
*Xuan Chen,Matthieu Crussière,Luc Le Magoarou*

Main category: eess.SP

TL;DR: 研究硬件不可理想（单元增益失配和单元间距偏差）对MIMO波束成形的影响；给出最劣劣性配置和平均性能解析表达式，平均信噪比下降有限但在最坏情形下可能显著加剧；为MIMO系统鲁棒性提供设计指引。


<details>
  <summary>Details</summary>
Motivation: 在实际天线阵列中，硬件不可理想（如增益失配、单元间距误差）会影响波束成形与系统性能，需要定量化地评估其对平均与最坏情形性能的影响，以指导鲁棒设计与容差规格。

Method: 对两类硬件失配（逐单元增益失配、单元间距偏差）建立分析模型，推导导致最坏性能的失配配置，并给出平均情形性能的解析表达式。通过数值仿真与理论分析对比，验证结论。

Result: 平均SNR降幅相对有限；但在最坏配置下性能可出现显著损失。给出可量化的鲁棒性极限，为现实硬件情形下的MIMO系统设计提供定量参考。

Conclusion: 应在鲁棒性设计中考虑最坏情形失配的潜在影响，单纯依赖平均性能可能高估系统能力；需制定容差标准与校准策略，以降低极端失配带来的风险。

Abstract: In this paper, we investigate the impact of hardware impairments in antenna
arrays on the beamforming performance of multi-input multi-output (MIMO)
communication systems. We consider two types of imperfections: per-element gain
mismatches and inter-element spacing deviations. We analytically determine the
impairment configurations that result in the worst-case degradation. In
addition, the analytical expression of the average-case performance is also
derived for comparison. Simulation and theoretical results show that the
average SNR degradation remains relatively limited, whereas the worst-case
scenarios can exhibit substantially higher losses. These findings provide clear
insight into the robustness limits of MIMO systems under practical hardware
imperfections.

</details>


### [61] [Near-optimal Reconfigurable Intelligent Surface Configuration: Blind Beamforming with Sensing](https://arxiv.org/abs/2511.05132)
*Son Dinh-Van,Nam Phuong Tran,Matthew D. Higgins*

Main category: eess.SP

TL;DR: 提出一种基于RSS的盲波束成形算法BORN，通过对接收信道SNR的二次结构进行建模来实现RIS的无CSI配置，具备O(N log N)样本复杂度的近优性能，且在实验和实地测试中显著优于现有盲波束算法，尤其在NLOS场景。


<details>
  <summary>Details</summary>
Motivation: 在缺乏通道状态信息(CSI)与几何模型的情况下实现RIS配置，兼容商用硬件，因此需要无CSI盲波束成形方法，并尽可能降低样本与计算开销。

Method: 提出两阶段算法：感知阶段估计基于RSS的二次模型；优化阶段在估计的二次模型上求解RIS配置。算法利用接收信号功率的内在二次结构，理论分析给出在Rademacher分布下若二阶系数矩阵为低秩时的可学习性结论，并证明样本复杂度为O(N log N)。

Result: 理论上证明在仅需O(N log N)样本的条件下可实现近似最优性能；大量仿真与实测数据表明BORN在整体性能上接近最优，显著优于现有盲波束算法，尤其在背景信道较弱的NLOS场景。

Conclusion: BORN提供了一个具有理论保证且样本高效的盲盲波束成形框架，证实在现实场景中对RIS的无CSI配置具有实用性和鲁棒性，并在弱背景信道下具备明显优势。

Abstract: Blind beamforming has emerged as a promising approach to configure
reconfigurable intelligent surfaces (RISs) without relying on channel state
information (CSI) or geometric models, making it directly compatible with
commodity hardware. In this paper, we propose a new blind beamforming
algorithm, so-called Blind Optimal RIS Beamforming with Sensing
(\textsc{BORN}), that operates using only received signal strength (RSS). In
contrast to existing methods that rely on majority-voting mechanisms,
\textsc{BORN} exploits the intrinsic quadratic structure of the received
signal-to-noise ratio (SNR). The algorithm proceeds in two stages:
\emph{sensing}, where a quadratic model is estimated from RSS measurements, and
\emph{optimization}, where the RIS configuration is obtained using the
estimated quadratic model. Our novelties are twofold. Firstly, we show for the
first time, that \textsc{BORN} can achieve provable near-optimal performance
using only $O(N \log_2(N))$ samples, where $N$ is the number of RIS elements.
As a by-product of our analysis, we show that quadratic models are learnable
under Rademacher feature distributions when the second-order coefficient matrix
is low-rank. This result, to our knowledge, has not been established in prior
matrix sensing literature. Secondly, extensive simulations and real-world field
tests demonstrate that \textsc{BORN} achieves near-optimal performance,
substantially outperforming state-of-the-art blind beamforming algorithms,
particularly in scenarios with a weak background channel such as
non-line-of-sight (NLOS).

</details>


### [62] [Look Before Switch: Sensing-Assisted Handover in 5G NR V2I Networks](https://arxiv.org/abs/2511.05195)
*Yunxin Li,Fan Liu,Haoqiu Xiong,Zhenkun Wang,Narengerile,Christos Masouros*

Main category: eess.SP

TL;DR: 提出了一种基于感知的ISAC手over框架，通过感知实现精准定位和预测机动，减少信令开销和手over中断时间，提升V2I的稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 解决高移动性5G NR V2I场景中的信令开销、手over中断与波束对齐难题，利用ISAC实现精准波束形成和预测性切换。

Method: 提出感知辅助手over框架，设计两种触发算法：距离基触发，基于估计的空间定位；概率基触发，利用IMM-EKF对车辆机动进行预测。并提出感知辅助的NR帧结构与协议以在 Vehicular Mobility 下实现快速同步与接入。

Result: 通过基于真实地图数据的链路级仿真，框架将平均手over中断时间降低超过50%，手over率下降，整体通信性能提升。

Conclusion: 验证ISAC感知在高 mobility V2I 场景下降低开销、提升稳定性与性能的有效性，为未来NR帧结构与车载通信协议提供设计思路。

Abstract: Integrated Sensing and Communication (ISAC) has emerged as a promising
solution in addressing the challenges of high-mobility scenarios in 5G NR
Vehicle-to-Infrastructure (V2I) communications. This paper proposes a novel
sensing-assisted handover framework that leverages ISAC capabilities to enable
precise beamforming and proactive handover decisions. Two sensing-enabled
handover triggering algorithms are developed: a distance-based scheme that
utilizes estimated spatial positioning, and a probability-based approach that
predicts vehicle maneuvers using interacting multiple model extended Kalman
filter (IMM-EKF) tracking. The proposed methods eliminate the need for uplink
feedback and beam sweeping, thus significantly reducing signaling overhead and
handover interruption time. A sensing-assisted NR frame structure and
corresponding protocol design are also introduced to support rapid
synchronization and access under vehicular mobility. Extensive link-level
simulations using real-world map data demonstrate that the proposed framework
reduces the average handover interruption time by over 50%, achieves lower
handover rates, and enhances overall communication performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [63] [Improving Injection-Throttling Mechanisms for Congestion Control for Data-center and Supercomputer Interconnects](https://arxiv.org/abs/2511.05149)
*Cristina Olmedilla,Jesus Escudero-Sahuquillo,Pedro J. Garcia,Francisco J. Quiles,Jose Duato*

Main category: cs.NI

TL;DR: Refines DCQCN's closed-loop congestion control to improve detection, signaling, and throttling, reducing control overhead and unnecessary throttling of non-congesting flows.


<details>
  <summary>Details</summary>
Motivation: Congestion remains a major bottleneck in modern high-performance interconnection networks; existing DCQCN principles are static and not fully aligned with dynamic traffic patterns, leading to suboptimal performance.

Method: Propose refinements to the DCQCN mechanism focusing on more accurate congestion detection, signaling, and injection throttling, aiming to reduce control traffic and avoid throttling non-congesting flows.

Result: Demonstrates that the refined design lowers control overhead and reduces unnecessary throttling while maintaining effective congestion control.

Conclusion: A more responsive and selective DCQCN design can better accommodate dynamic traffic, improving overall system performance without compromising fairness.

Abstract: Over the past decade, Supercomputers and Data centers have evolved
dramatically to cope with the increasing performance requirements of
applications and services, such as scientific computing, generative AI, social
networks or cloud services. This evolution have led these systems to
incorporate high-speed networks using faster links, end nodes using multiple
and dedicated accelerators, or a advancements in memory technologies to bridge
the memory bottleneck. The interconnection network is a key element in these
systems and it must be thoroughly designed so it is not the bottleneck of the
entire system, bearing in mind the countless communication operations that
generate current applications and services. Congestion is serious threat that
spoils the interconnection network performance, and its effects are even more
dramatic when looking at the traffic dynamics and bottlenecks generated by the
communication operations mentioned above. In this vein, numerous congestion
control (CC) techniques have been developed to address congestion negative
effects. One popular example is Data Center Quantized Congestion Notification
(DCQCN), which allows congestion detection at network switch buffers, then
marking congesting packets and notifying about congestion to the sources, which
finally apply injection throttling of those packets contributing to congestion.
While DCQCN has been widely studied and improved, its main principles for
congestion detection, notification and reaction remain largely unchanged, which
is an important shortcoming considering congestion dynamics in current
high-performance interconnection networks. In this paper, we revisit the DCQCN
closed-loop mechanism and refine its design to leverage a more accurate
congestion detection, signaling, and injection throttling, reducing control
traffic overhead and avoiding unnecessary throttling of non-congesting flows.

</details>


### [64] [EPFL-REMNet: Efficient Personalized Federated Digital Twin Towards 6G Heterogeneous Radio Environme](https://arxiv.org/abs/2511.05238)
*Peide Li,Liu Cao,Lyutianyang Zhang,Dongyu Wei,Ye Hu,Qipeng Xie*

Main category: cs.NI

TL;DR: 提出 EPFL-REMNet，一种高效的个性化联邦学习框架，在共享骨干网+本地化个性化头部结构下，解决非IID条件下REM数字孪生的精度与上行带宽问题，在90个客户端的三类Non-IID场景下优于 FedAvg 等方法。


<details>
  <summary>Details</summary>
Motivation: 随着 REM 从5G的同质环境转向B5G/6G的异质场景，数据的非独立同分布导致联邦学习在精度和通信效率上显著下降。需要一种结合共享骨干和本地化个性化头部的方案，以提升数字孪生的保真度并降低上行开销。

Method: 设计“共享骨干+轻量化个性化头部”的模型结构；服务器与客户端仅传输压缩后的共享骨干，个性化头部在客户端本地维护；基于无线电环境复杂度构建轻/中/重三类Non-IID场景，数据分布在90个客户端上；评估数字孪生保真度（准确率）与上行开销，并与FedAvg及最新方法进行对比。

Result: 在所有Non-IID设定下，EPFL-REMNet 同时实现更高的数字孪生保真度和更低的上行开销；显著减小不同数据集间的性能差异，提升“长尾客户”的本地地图精度，增强数字孪生的整体完整性。

Conclusion: 该方法有效缓解REM中Non-IID挑战，提供高保真且高效的6G异质无线环境数字孪生解决方案，具有在更广泛应用场景中的潜力。

Abstract: Radio Environment Map (REM) is transitioning from 5G homogeneous environments
to B5G/6G heterogeneous landscapes. However, standard Federated Learning (FL),
a natural fit for this distributed task, struggles with performance degradation
in accuracy and communication efficiency under the non-independent and
identically distributed (Non-IID) data conditions inherent to these new
environments. This paper proposes EPFL-REMNet, an efficient personalized
federated framework for constructing a high-fidelity digital twin of the 6G
heterogeneous radio environment. The proposed EPFL-REMNet employs a"shared
backbone + lightweight personalized head" model, where only the compressed
shared backbone is transmitted between the server and clients, while each
client's personalized head is maintained locally. We tested EPFL-REMNet by
constructing three distinct Non-IID scenarios (light, medium, and heavy) based
on radio environment complexity, with data geographically partitioned across 90
clients. Experimental results demonstrate that EPFL-REMNet simultaneously
achieves higher digital twin fidelity (accuracy) and lower uplink overhead
across all Non-IID settings compared to standard FedAvg and recent
state-of-the-art methods. Particularly, it significantly reduces performance
disparities across datasets and improves local map accuracy for long-tail
clients, enhancing the overall integrity of digital twin.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [65] [Adjoint and duality for rank-metric codes in a skew polynomial framework](https://arxiv.org/abs/2511.05084)
*José Gómez-Torrecillas,F. J. Lobillo,Gabriel Navarro,Paolo Santonastaso*

Main category: cs.IT

TL;DR: 系统性研究了嵌入到伪多项式商环中的转置与对偶性，给出明确的伪多项式描述以构造转置与对偶，并确定 MRD 码家族的伴随/对偶；计算核参数，给出新的无限参数集合，在其中多数 MRD 码与既有构造不等价。


<details>
  <summary>Details</summary>
Motivation: 通过研究非交换主理想环中的对称性与对偶性，提升对 MRD 码的分类和新构造的发现能力。

Method: 在伪多项式商环中系统性地研究转置与对偶性，给出转置与对偶的显式伪多项式描述；据此确定 MRD 码家族的伴随与对偶；计算核参数并比较新旧构造的等价性。

Result: 明确给出 MRD 码家族的伴随与对偶结构，计算核参数；证明存在一组新的无限参数，使大量 MRD 码与现有构造在参数上不等价。

Conclusion: 伪多项式商环中的转置与对偶性能系统地产生并区分 MRD 码族，提供了可操作的描述以扩展文献中的新构造集合与分类。

Abstract: Skew polynomial rings provide a fundamental example of noncommutative
principal ideal domains. Special quotients of these rings yield matrix algebras
that play a central role in the theory of rank-metric codes. Recent
breakthroughs have shown that specific subsets of these quotients produce the
largest known families of maximum rank distance (MRD) codes. In this work, we
present a systematic study of transposition and duality operations within
quotients of skew polynomial rings. We develop explicit skew-polynomial
descriptions of the transpose and dual code constructions, enabling us to
determine the adjoint and dual codes associated with the MRD code families
recently introduced by Sheekey et al. Building on these results, we compute the
nuclear parameters of these codes, and prove that, for a new infinite set of
parameters, many of these MRD codes are inequivalent to previously known
constructions in the literature.

</details>


### [66] [Shortest self-orthogonal embeddings of binary linear codes](https://arxiv.org/abs/2511.05440)
*Junmin An,Nathan Kaplan,Jon-Lark Kim,Jinquan Luo,Guodong Wang*

Main category: cs.IT

TL;DR: 通过对码的 hull 属性的研究，给出二进制线性码的最短自正交嵌入长度，并以海明码和 Reed-Muller 码为例，证明海明码的最短自正交嵌入是自对偶的，并给出两种从海明码构造自对偶码的算法，获得若干新参数的自对偶码。


<details>
  <summary>Details</summary>
Motivation: 研究最短自正交嵌入的长度及存在性，以便在给定二进制码上构造自正交自对偶码并扩展最优自正交码的参数表；利用 hull 的性质将问题具体化。

Method: 利用线性码的 hull 的性质，确定任意二进制码的最短自正交嵌入长度；提出两种从海明码构造自对偶码的算法；通过嵌入与截短等操作构造具体码并比较等效类。

Result: 证明二进制海明码的最短自正交嵌入是自对偶；给出从海明码构造自对偶码的两种算法，分别构造出自对偶码 [22,11,6]（shortened Golay 码）和 [52,26,8] 等码；并从 [15,11,3] 的海明码和 [31,26,3] 的海明码分别得到自对偶码，且通过最短自正交嵌入获得大量维数为7、8 的最优自正交码，给出若干新参数如 [91,8,42], [98,8,46], [114,8,54], [191,8,94]；

Conclusion: 基于 hull 的方法能够精确确定最短自正交嵌入的长度并生成新的自正交码族，扩展了维数为8的自正交码的新参数集合，为今后在其他码族中的同类问题提供途径。

Abstract: There has been recent interest in the study of shortest self-orthogonal
embeddings of binary linear codes, since many such codes are optimal
self-orthogonal codes. Several authors have studied the length of a shortest
self-orthogonal embedding of a given binary code $\mathcal C$, or equivalently,
the minimum number of columns that must be added to a generator matrix of
$\mathcal C$ to form a generator matrix of a self-orthogonal code. In this
paper, we use properties of the hull of a linear code to determine the length
of a shortest self-orthogonal embedding of any binary linear code. We focus on
the examples of Hamming codes and Reed-Muller codes. We show that a shortest
self-orthogonal embedding of a binary Hamming code is self-dual, and propose
two algorithms to construct self-dual codes from Hamming codes $\mathcal H_r$.
Using these algorithms, we construct a self-dual $[22, 11, 6]$ code, called the
shortened Golay code, from the binary $[15, 11, 3]$ Hamming code $\mathcal
H_4$, and construct a self-dual $[52, 26, 8]$ code from the binary $[31, 26,
3]$ Hamming code $\mathcal H_5$. We use shortest SO embeddings of linear codes
to obtain many inequivalent optimal self-orthogonal codes of dimension $7$ and
$8$ for several lengths. Four of the codes of dimension $8$ that we construct
are codes with new parameters such as $[91, 8, 42],\, [98, 8, 46],\,[114, 8,
54]$, and $[191, 8, 94]$.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [67] [Jailbreaking in the Haystack](https://arxiv.org/abs/2511.04707)
*Rishi Rajesh Shah,Chen Henry Wu,Shashwat Saxena,Ziqian Zhong,Alexander Robey,Aditi Raghunathan*

Main category: cs.CR

TL;DR: NINJA是一种面向长上下文语言模型的注入性对策破坏方法，通过在有害目标后附加模型生成的无害内容来实现越狱攻击。实验表明，在 HarmBench 上该方法显著提升对 LLaMA、Qwen、Mistral、Gemini 等模型的攻击成功率；并且在固定计算预算下，增大上下文长度可以比增加尝试次数更有效地提升成功率。这一发现提示，即使无害的长上下文若经过精心的目标定位，也会暴露现代LMs的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 探究扩展上下文的长序列LM在对齐安全方面的潜在风险，揭示长上下文如何被利用来规避安全约束，推动更稳健的防御设计。

Method: 提出NINJA攻击：通过在有害目标之外附加由模型生成的无害内容，利用有害目标在上下文中的位置信息来增强越狱机会；在 HarmBench 基准上对多模型进行评估，比较不同上下文长度与尝试次数的权衡；强调该方法低资源、可迁移且检测性较低；并给出在固定算力预算下的计算最优性结论。

Result: NINJA显著提高了跨开源与专有模型的攻击成功率；相比先前的越狱方法，该方法在资源、可迁移性和检测难度方面具有优势；在固定计算预算下，增加上下文长度比增加best-of-N数量更有效。

Conclusion: 长上下文的无害内容若经过精确的目标定位，会显著放大现代LM的安全脆弱性，提示需要在模型设计与对齐策略中充分考虑上下文规模与位置敏感性，促使未来的安全防护研究关注上下文相关性。

Abstract: Recent advances in long-context language models (LMs) have enabled
million-token inputs, expanding their capabilities across complex tasks like
computer-use agents. Yet, the safety implications of these extended contexts
remain unclear. To bridge this gap, we introduce NINJA (short for
Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by
appending benign, model-generated content to harmful user goals. Critical to
our method is the observation that the position of harmful goals play an
important role in safety. Experiments on standard safety benchmark, HarmBench,
show that NINJA significantly increases attack success rates across
state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral,
and Gemini. Unlike prior jailbreaking methods, our approach is low-resource,
transferable, and less detectable. Moreover, we show that NINJA is
compute-optimal -- under a fixed compute budget, increasing context length can
outperform increasing the number of trials in best-of-N jailbreak. These
findings reveal that even benign long contexts -- when crafted with careful
goal positioning -- introduce fundamental vulnerabilities in modern LMs.

</details>


### [68] [SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking](https://arxiv.org/abs/2511.04711)
*Wenyuan Yang,Yichen Sun,Changzheng Chen,Zhixuan Chu,Jiaheng Zhang,Yiming Li,Dacheng Tao*

Main category: cs.CR

TL;DR: SWAP: 通过对软提示进行顺序水印嵌入，将水印放置在一个更复杂、与主任务决策空间不同的空间中，以实现对软提示的版权保护。该水印利用防御方指定的分布外（OOD）类别的有序组合，结合零-shot预测能力进行隐形嵌入，并通过假设检验的验证协议进行可控的水印检测。实验在11个数据集上显示出有效性、无害性与对潜在自适应攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在大规模 vision-language 模型（如 CLIP）中，软提示用于高效地将模型适配到特定任务，但这部分知识具有版权敏感性。现有的模型所有权审计方法在处理软提示时往往无效，原因是提示学习的特性使非侵入性审计易产生假阳性，侵入式方法（如后门）在功能性触发与可控性上存在重大挑战。因此，需要一种能在不干扰主任务的前提下对软提示进行可靠识别与验证的版权保护方案。

Method: 提出顺序水印（SWAP）用于软提示的水印化。SWAP 将水印嵌入一个更复杂、与主任务输出空间不同的空间，具体做法是通过一系列防御者定义的分布外（OOD）类别的有序组合来编码水印，利用 CLIP 的零-shot 预测能力将水印信号映射到输出空间。随后设计基于假设检验的验证协议，对水印进行检测，并给出水印存在的成功条件的理论分析。

Result: 在11个数据集上的大量实验表明，SWAP 能实现有效、水印无害且对潜在自适应攻击具有鲁棒性。与传统非侵入式审计及现有的后门方法相比，SWAP 能在不改变主预测标签的情况下隐藏水印，并提供可验证的统计推断。

Conclusion: SWAP 为软提示的版权保护提供了一种可行、稳健的方案，理论基础与实证验证共同支持其在大规模视觉语言模型中的应用潜力，且具有较强的抗自适应攻击能力与无害性。

Abstract: Large-scale vision-language models, especially CLIP, have demonstrated
remarkable performance across diverse downstream tasks. Soft prompts, as
carefully crafted modules that efficiently adapt vision-language models to
specific tasks, necessitate effective copyright protection. In this paper, we
investigate model copyright protection by auditing whether suspicious
third-party models incorporate protected soft prompts. While this can be viewed
as a special case of model ownership auditing, our analysis shows that existing
techniques are ineffective due to prompt learning's unique characteristics.
Non-intrusive auditing is inherently prone to false positives when independent
models share similar data distributions with victim models. Intrusive
approaches also fail: backdoor methods designed for CLIP cannot embed
functional triggers, while extending traditional DNN backdoor techniques to
prompt learning suffers from harmfulness and ambiguity challenges. We find that
these failures in intrusive auditing stem from the same fundamental reason:
watermarking operates within the same decision space as the primary task yet
pursues opposing objectives. Motivated by these findings, we propose sequential
watermarking for soft prompts (SWAP), which implants watermarks into a
different and more complex space. SWAP encodes watermarks through a specific
order of defender-specified out-of-distribution classes, inspired by the
zero-shot prediction capability of CLIP. This watermark, which is embedded in a
more complex space, keeps the original prediction label unchanged, making it
less opposed to the primary task. We further design a hypothesis-test-guided
verification protocol for SWAP and provide theoretical analyses of success
conditions. Extensive experiments on 11 datasets demonstrate SWAP's
effectiveness, harmlessness, and robustness against potential adaptive attacks.

</details>


### [69] [P-MIA: A Profiled-Based Membership Inference Attack on Cognitive Diagnosis Models](https://arxiv.org/abs/2511.04716)
*Mingliang Hou,Yinuo Wang,Teng Guo,Zitao Liu,Wenzhou Dou,Jiaqi Zheng,Renqiang Luo,Mi Tian,Weiqi Luo*

Main category: cs.CR

TL;DR: 提出基于灰盒成员推断攻击的 P-MIA 框架，利用可解释性可视化暴露的内部知识状态向量，对认知诊断模型（CDMs）的隐私风险进行系统评估，并可用于审计未学习效果。


<details>
  <summary>Details</summary>
Motivation: CDMs 在敏感学生数据上训练，存在隐私风险；尽管 MIA 在其他领域有研究，但对 CDMs 的研究尚缺乏，需量化风险并提供可行的审计工具。

Method: 提出一种灰盒威胁模型，利用模型可解释性特征（如雷达图等可视化）反向推导内部知识状态向量；设计 P-MIA 框架，结合最终预测概率和暴露的内部向量作为特征进行攻击；在三个真实数据集上对主流 CDMs 进行对比实验，验证相对于黑盒基线的优势，并评估机器未学习方法的有效性。

Result: 灰盒攻击显著优于黑盒基线；P-MIA 可用于评估未学习等保护机制的效果，揭示其局限性，并具备一定的审计应用价值。

Conclusion: 本研究揭示了 CDMs 的潜在隐私风险，尤其是当外部可解释性暴露内部状态时；P-MIA 提供了一实用的审计工具，推动隐私保护研究，同时指出现有未学习方法的局限性并提出未来改进方向。

Abstract: Cognitive diagnosis models (CDMs) are pivotal for creating fine-grained
learner profiles in modern intelligent education platforms. However, these
models are trained on sensitive student data, raising significant privacy
concerns. While membership inference attacks (MIA) have been studied in various
domains, their application to CDMs remains a critical research gap, leaving
their privacy risks unquantified. This paper is the first to systematically
investigate MIA against CDMs. We introduce a novel and realistic grey box
threat model that exploits the explainability features of these platforms,
where a model's internal knowledge state vectors are exposed to users through
visualizations such as radar charts. We demonstrate that these vectors can be
accurately reverse-engineered from such visualizations, creating a potent
attack surface. Based on this threat model, we propose a profile-based MIA
(P-MIA) framework that leverages both the model's final prediction
probabilities and the exposed internal knowledge state vectors as features.
Extensive experiments on three real-world datasets against mainstream CDMs show
that our grey-box attack significantly outperforms standard black-box
baselines. Furthermore, we showcase the utility of P-MIA as an auditing tool by
successfully evaluating the efficacy of machine unlearning techniques and
revealing their limitations.

</details>


### [70] [Trustworthiness Calibration Framework for Phishing Email Detection Using Large Language Models](https://arxiv.org/abs/2511.04728)
*Daniyal Ganiuly,Assel Smaiyl*

Main category: cs.CR

TL;DR: 提出信任度校准框架（TCF）用于评估钓鱼检测模型的信任性，构建信任度指数（TCI）并引入跨数据集稳定性（CDS），在五个公开语料与三种模型上评估后发现GPT-4的信任性最佳、对比基础模型更稳健，但可靠性与原始准确度不直接相关，强调在实际部署中需要信任导向的评估。


<details>
  <summary>Details</summary>
Motivation: 在现实部署中，单纯依赖文本分类的准确率无法充分反映模型的鲁棒性与信任度。钓鱼攻击的演化性、对抗性以及安全系统对可靠性的高要求，促使需要一个可复现的信任评估框架。

Method: 提出信任度校准框架（TCF），从校准、一致性、鲁棒性三个维度构建信任度指标，并形成Trustworthiness Calibration Index（TCI）。引入Cross-Dataset Stability（CDS）衡量跨数据集的信任度稳定性。基于五个公开语料（SecureMail 2025、Phishing Validation 2024、CSDMC2010、Enron-Spam、Nazario）以及三种模型（DeBERTa-v3-base、LLaMA-3-8B、GPT-4）进行评估。

Result: 实验结果显示GPT-4在整体信任度表现最佳，其次是LLaMA-3-8B，再是DeBERTa-v3-base。统计分析表明可靠性与原始准确度呈独立变化，强调信任导向评估的重要性。跨数据集稳定性（CDS）也得到证实。

Conclusion: 该框架为基于LLM的钓鱼检测提供透明、可复现的依赖性评估基础，有助于在真实部署中评估和比较模型的信任性与 dependable性。

Abstract: Phishing emails continue to pose a persistent challenge to online
communication, exploiting human trust and evading automated filters through
realistic language and adaptive tactics. While large language models (LLMs)
such as GPT-4 and LLaMA-3-8B achieve strong accuracy in text classification,
their deployment in security systems requires assessing reliability beyond
benchmark performance. To address this, this study introduces the
Trustworthiness Calibration Framework (TCF), a reproducible methodology for
evaluating phishing detectors across three dimensions: calibration,
consistency, and robustness. These components are integrated into a bounded
index, the Trustworthiness Calibration Index (TCI), and complemented by the
Cross-Dataset Stability (CDS) metric that quantifies stability of
trustworthiness across datasets. Experiments conducted on five corpora, such as
SecureMail 2025, Phishing Validation 2024, CSDMC2010, Enron-Spam, and Nazario,
using DeBERTa-v3-base, LLaMA-3-8B, and GPT-4 demonstrate that GPT-4 achieves
the strongest overall trust profile, followed by LLaMA-3-8B and
DeBERTa-v3-base. Statistical analysis confirms that reliability varies
independently of raw accuracy, underscoring the importance of trust-aware
evaluation for real-world deployment. The proposed framework establishes a
transparent and reproducible foundation for assessing model dependability in
LLM-based phishing detection.

</details>


### [71] [Confidentiality in a Card-Based Protocol Under Repeated Biased Shuffles](https://arxiv.org/abs/2511.05111)
*Do Hyun Kim,Ahmet Cetinkaya*

Main category: cs.CR

TL;DR: 对 Bert den Boer's Five Card Trick 的保密性进行概率分析，揭示非均匀洗牌引起的信息泄露，给出条件概率与利用马尔科夫链特征值的严格界限，且扩展到恶意洗牌情形。


<details>
  <summary>Details</summary>
Motivation: 在卡牌两人协同计算的安全性研究中，确保随机切牌的无偏性并量化泄露程度，给出降低泄露的重复要求。

Method: 建模非均匀分布导致的泄露，推导在不同状态下对对方选择猜测的条件概率；用马尔科夫链的特征分解推导混合时间和泄露上界；在含恶意洗牌的情形扩展分析。

Result: 给出具体的猜测的条件概率量化，以及通过特征值分析得到重复执行偏向洗牌的严格上界，以将泄露降至可接受水平。

Conclusion: 洗牌偏差确实会导致信息泄露，需通过重复随机化来控制泄露；研究可推广到恶意洗牌者的情形，提供了分析框架。

Abstract: In this paper, we provide a probabilistic analysis of the confidentiality in
a card-based protocol. We focus on Bert den Boer's original Five Card Trick to
develop our approach. Five Card Trick was formulated as a secure two-party
computation method, where two players use colored cards with identical backs to
calculate the logical AND operation on the bits that they choose. In this
method, the players first arrange the cards privately, and then shuffle them
through a random cut. Finally, they reveal the shuffled arrangement to
determine the result of the operation. An unbiased random cut is essential to
prevent players from exposing their chosen bits to each other. However, players
typically choose to move cards within the deck even though not moving any cards
should be equally likely. This unconscious behavior results in a biased,
nonuniform shuffling-distribution in the sense that some arrangements of cards
are slightly more probable after the cut. Such a nonuniform distribution
creates an opportunity for a malicious player to gain advantage in guessing the
other player's choice. We provide the conditional probabilities of such guesses
as a way to quantify the information leakage. Furthermore, we utilize the
eigenstructure of a Markov chain to derive tight bounds on the number of times
the biased random cuts must be repeated to reduce the leakage to an acceptable
level. We also discuss the generalization of our approach to the setting where
shuffling is conducted by a malicious player.

</details>


### [72] [Zero Trust Security Model Implementation in Microservices Architectures Using Identity Federation](https://arxiv.org/abs/2511.04925)
*Rethish Nair Rajendran,Sathish Krishna Anumula,Dileep Kumar Rai,Sachin Agrawal*

Main category: cs.CR

TL;DR: 该论文倡导在微服务中应用零信任与统一身份联盟，通过OIDC、OAuth 2.0和SPIFFE/SPIRE等实现端到端信任和最小攻击面，并以实验评估支持其安全性和DevSecOps合规性。


<details>
  <summary>Details</summary>
Motivation: 动机在于传统基于边界的安全策略在分布式微服务环境中的局限性，需通过零信任和统一身份来保护人和工作负载的身份，降低攻击面。

Method: 提出一个基于行业标准认证授权和端到端信任身份技术的解决框架，结合OIDC、OAuth 2.0、SPIFFE/SPIRE的工作负载身份，以及在多域环境中的策略执行和互操作性。

Result: 实验评估显示较小的攻击面、和谐的策略执行以及跨域互操作性，联邦身份与零信任结合实现对身份验证和授权的严格遵循，并符合DevSecOps的自动化、可扩展、可弹性部署的要求。

Conclusion: 该研究提供了一个落地路线图，指向在云原生技术中应用零信任的组织，同时确保互操作性和合规性，强调在微服务生态中采用统一身份和策略驱动的安全治理。

Abstract: The microservice bombshells that have been linked with the microservice
expansion have altered the application architectures, offered agility and
scalability in terms of complexity in security trade-offs. Feeble legacy-based
perimeter-based policies are unable to offer safeguard to distributed workloads
and temporary interaction among and in between the services. The article itself
is a case on the need of the Zero Trust Security Model of micro services
ecosystem, particularly, the fact that human and workloads require identity
federation. It is proposed that the solution framework will be based on
industry-standard authentication and authorization and end-to-end trust
identity technologies, including Authorization and OpenID connect (OIDC),
Authorization and OAuth 2.0 token exchange, and Authorization and SPIFFE/ SPIRE
workload identities. Experimental evaluation is a unique demonstration of a
superior security position of making use of a smaller attack surface, harmony
policy enforcement, as well as interoperability across multi- domain
environments. The research results overlay that the federated identity combined
with the Zero Trust basics not only guarantee the rules relating to
authentication and authorization but also fully complies with the latest
DevSecOps standards of microservice deployment, which is automated, scaled, and
resilient. The current project offers a stringent roadmap to the organizations
that desire to apply Zero Trust in cloud-native technologies but will as well
guarantee adherence and interoperability.

</details>


### [73] [The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective](https://arxiv.org/abs/2511.04946)
*Lei Chen,Erci Xu,Yiming Sun,Shengyu Fan,Xianglong Deng,Guiming Shi,Guang Fan,Liang Kong,Yilan Zhu,Shoumeng Yan,Mingzhe Zhang*

Main category: cs.CR

TL;DR: 存储I/O对全同态加密（FHE）性能的影响分析：I/O瓶颈可将ASIC性能降低至约357倍，GPU性能下降至约22倍。


<details>
  <summary>Details</summary>
Motivation: FHE使在加密数据上进行计算成为可能，显著提升隐私保护，但关于I/O对FHE应用部署的影响研究不足。本研究旨在揭示存储I/O对FHE工作负载的性能影响并总结现状教训。

Method: 分析FHE应用的存储I/O行为，量化I/O对不同硬件平台的影响，并基于现有资料提炼关键经验教训。

Result: 核心结果显示：存储I/O对ASIC的性能降幅高达约357×，对GPU的性能降幅高达约22×，并据此提出若干对部署与优化的启示。

Conclusion: 存储I/O在FHE部署中具有决定性影响，需进一步研究面向存储-I/O的优化策略与设计，以提升FHE系统的实际性能与可用性。

Abstract: Fully Homomorphic Encryption (FHE) allows computations to be performed on
encrypted data, significantly enhancing user privacy. However, the I/O
challenges associated with deploying FHE applications remains understudied. We
analyze the impact of storage I/O on the performance of FHE applications and
summarize key lessons from the status quo. Key results include that storage I/O
can degrade the performance of ASICs by as much as 357$\times$ and reduce GPUs
performance by up to 22$\times$.

</details>


### [74] [Chasing One-day Vulnerabilities Across Open Source Forks](https://arxiv.org/abs/2511.05097)
*Romain Lefeuvre,Charly Reux,Stefano Zacchiroli,Olivier Barais,Benoit Combemale*

Main category: cs.CR

TL;DR: 提出一种基于提交级别的漏洞传播分析方法，利用 Software Heritage 的全局代码图，检测 Fork 仓库中尚未修复的漏洞，以发现“一天漏洞”。


<details>
  <summary>Details</summary>
Motivation: 现有的漏洞分析多依赖依赖信息的追踪，难以在分叉的代码库中追踪漏洞的引入与修复，造成一日漏洞未被发现。需要在提交级别进行跨分叉的漏洞传播分析，以提高发现和修复的时效性。

Method: 构建一种从 Software Heritage 获得的全球代码图，按提交级别传播漏洞信息并进行自动化影响分析；在包含漏洞提交的开发历史中筛选出有潜在被影响的 Fork，并对大规模分叉进行严格过滤，然后对结果集合进行人工评估。

Result: 从 OSV 统计的包含漏洞提交的仓库出发，识别出约 2.2 百万个至少含一个漏洞提交的 Fork；经严格过滤，得到 356 对潜在的漏洞-分叉对；对活跃且受欢迎的 GitHub Fork 进行分析，人工评估了 65 对，发现 3 个高严重性漏洞，显示该方法在实际场景中的应用价值。

Conclusion: 该方法在跨分叉的提交级别漏洞传播分析方面具有可行性和有效性，能够辅助开发者及时发现并处置未修复的漏洞分叉，具有进一步推广和集成的潜力。

Abstract: Tracking vulnerabilities inherited from third-party open-source components is
a well-known challenge, often addressed by tracing the threads of dependency
information. However, vulnerabilities can also propagate through forking: a
repository forked after the introduction of a vulnerability, but before it is
patched, may remain vulnerable in the fork well after being fixed in the
original project. Current approaches for vulnerability analysis lack the
commit-level granularity needed to track vulnerability introductions and fixes
across forks, potentially leaving one-day vulnerabilities undetected. This
paper presents a novel approach to help developers identify one-day
vulnerabilities in forked repositories. Leveraging the global graph of public
code, as captured by the Software Heritage archive, the approach propagates
vulnerability information at the commit level and performs automated impact
analysis. This enables automatic detection of forked projects that have not
incorporated fixes, leaving them potentially vulnerable. Starting from 7162
repositories that, according to OSV, include vulnerable commits in their
development histories, we identify 2.2 M forks, containing at least one
vulnerable commit. Then we perform a strict filtering, allowing us to find 356
___vulnerability, fork___ pairs impacting active and popular GitHub forks, we
manually evaluate 65 pairs, finding 3 high-severity vulnerabilities,
demonstrating the impact and applicability of this approach.

</details>


### [75] [TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS](https://arxiv.org/abs/2511.05100)
*Arslan Mumtaz,Mridula Singh*

Main category: cs.CR

TL;DR: TRICK is a new secure positioning primitive that achieves Verifiable Multilateration-like guarantees with minimal infrastructure by using two-way ranging to a trusted LEO satellite and multiple one-way ranges from GNSS broadcast signals, enabling spoofing detection with negligible computation overhead.


<details>
  <summary>Details</summary>
Motivation: GNSS security is challenged by spoofing; existing cryptographic approaches (e.g., TESLA-based navigation message authentication) do not defend against time-of-arrival manipulations. A lightweight mechanism is needed that preserves accuracy while providing strong spoofing resilience.

Method: The approach combines two-way range measurements with a trusted LEO satellite and multiple one-way ranges from broadcast signals to form ellipsoidal constraints. A formal analysis proves that these constraints yield the same security guarantees as Verifiable Multilateration (VM) but with minimal infrastructure and messaging.

Result: A rigorous proof and analytical results show that TRICK can reliably detect spoofing while imposing negligible computation overhead, effectively restoring VM-level guarantees using limited infrastructure.

Conclusion: TRICK provides a practical, low-cost secure positioning primitive that closes the gap left by traditional VM and cryptographic solutions, offering robust spoofing detection with minimal resource requirements.

Abstract: Global Navigation Satellite Systems (GNSS) provide Positioning, Navigation,
and Timing (PNT) information to over 4 billion devices worldwide. Despite its
pervasive use in safety critical and high precision applications, GNSS remains
vulnerable to spoofing attacks. Cryptographic enhancements, such as the use of
TESLA protocol in Galileo, to provide navigation message authentication do not
mitigate time of arrival manipulations. In this paper, we propose TRICK, a
primitive for secure positioning that closes this gap by introducing a
fundamentally new approach that only requires two way communications with a
single reference node along with multiple broadcast signals. Unlike classical
Verifiable Multilateration (VM), which requires establishing two way
communication with each reference nodes, our solution relies on only two
measurements with a trusted Low Earth Orbiting (LEO) satellite and combines
broadcast navigation signals. We rigorously prove that combining the LEO
satellite based two way range measurements and multiple one way ranges such as
from broadcast signals of GNSS into ellipsoidal constraint restores the same
guarantees as offered by VM whilst using minimal infrastructure and message
exchanges. Through detailed analysis, we show that our approach reliably
detects spoofing attempts while adding negligible computation overhead.

</details>


### [76] [PhantomFetch: Obfuscating Loads against Prefetcher Side-Channel Attacks](https://arxiv.org/abs/2511.05110)
*Xingzhi Zhang,Buyi Lv,Yimin Lu,Kai Bu*

Main category: cs.CR

TL;DR: PhantomFetch 提供一种硬件无关的防御机制，通过混淆受害者对 IP-stride 预取器条目的敏感加载，从而在保持预取效果的同时防止侧信号泄露，且开销极小。


<details>
  <summary>Details</summary>
Motivation: IP-stride 预取器易被侧信道攻击利用泄露秘密，现有防御要么改硬件要么影响预取速度，存在成本与可用性折中。需要一种硬件不可知且开销低的防御方案。

Method: 提出 PhantomFetch，一种“预取保持”并硬件无关的防御，通过直接破坏训练有素的预取条目与受害者的秘密相关加载之间的耦合来实现安全。具体做法是对受害者的敏感加载效果进行混淆，避免被预取器利用，且设计为对现有设备友好。

Result: 实验结果表明 PhantomFetch 能以极低的开销保护 IP-stride 预取器，降低或消除秘密泄露的风险。

Conclusion: PhantomFetch 是首个具备预取保持能力且硬件无关的防御，降低了重新制造成本并提升对现成设备的适用性，证明了在不大幅牺牲性能的前提下实现对 IP-stride 预取器的保护的可行性。

Abstract: The IP-stride prefetcher has recently been exploited to leak secrets through
side-channel attacks. It, however, cannot be simply disabled for security with
prefetching speedup as a sacrifice. The state-of-the-art defense tries to
retain the prefetching effect by hardware modification. In this paper, we
present PhantomFetch as the first prefetching-retentive and hardware-agnostic
defense. It avoids potential remanufacturing cost and enriches applicability to
off-the-shelf devices. The key idea is to directly break the exploitable
coupling between trained prefetcher entries and the victim's secret-dependent
loads by obfuscating the sensitive load effects of the victim. The experiment
results show that PhantomFetch can secure the IP-stride prefetcher with only
negligible overhead.

</details>


### [77] [BLADE: Behavior-Level Anomaly Detection Using Network Traffic in Web Services](https://arxiv.org/abs/2511.05193)
*Zhibo Dong,Yong Huang,Shubao Sun,Wentao Cui,Zhihua Wang*

Main category: cs.CR

TL;DR: 提出 BLADE：一种基于流自编码器的无监督流量异常检测系统，能同时检测流级和行为级攻击，利用多流视角的应用层模式，通过伪标签和一类分类器实现对正常行为的建模，并在自定义数据集与 CIC-IDS2017 上取得高 F1 分数。


<details>
  <summary>Details</summary>
Motivation: 现有检测多聚焦于单一流的流量异常，难以识别看似正常但跨流出现的行为级攻击；需要从多流角度捕捉应用层的特征。

Method: 通过流自编码器学习潜在特征并计算重建损失；对潜在表示进行无监督聚类得到伪操作标签；基于重建损失计算异常分数；将时间戳、伪标签、异常分数组合成多流 triplets，输入单类分类器以建模正常行为并检测异常。

Result: 在自定义数据集和 CIC-IDS2017 上分别获得 F1 分数 0.9732 和 0.9801，优于传统的单流异常检测基线。

Conclusion: BLADE 能同时检测流级和行为级异常，且在多流视角下实现对应用层操作模式的有效建模与识别。

Abstract: With their widespread popularity, web services have become the main targets
of various cyberattacks. Existing traffic anomaly detection approaches focus on
flow-level attacks, yet fail to recognize behavior-level attacks, which appear
benign in individual flows but reveal malicious purpose using multiple network
flows. To transcend this limitation, we propose a novel unsupervised traffic
anomaly detection system, BLADE, capable of detecting not only flow-level but
also behavior-level attacks in web services. Our key observation is that
application-layer operations of web services exhibit distinctive communication
patterns at the network layer from a multi-flow perspective. BLADE first
exploits a flow autoencoder to learn a latent feature representation and
calculates its reconstruction losses per flow. Then, the latent representation
is assigned a pseudo operation label using an unsupervised clustering method.
Next, an anomaly score is computed based on the reconstruction losses. Finally,
the triplets of timestamps, pseudo labels, and anomaly scores from multiple
flows are aggregated and fed into a one-class classifier to characterize the
behavior patterns of legitimate web operations, enabling the detection of
flow-level and behavior-level anomalies. BLADE is extensively evaluated on both
the custom dataset and the CIC-IDS2017 dataset. The experimental results
demonstrate BLADE's superior performance, achieving high F1 scores of 0.9732
and 0.9801, respectively, on the two datasets, and outperforming traditional
single-flow anomaly detection baselines.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [78] [Computationally Efficient Spline-Based Modeling of DER Dynamics for Voltage Stability in Active Distribution Networks](https://arxiv.org/abs/2511.04917)
*Shadrack T. Asiedu,Tara Aryal,Zongjie Wang,Hossein Moradi Rekabdarkolaee,Timothy M. Hansen*

Main category: eess.SY

TL;DR: 提出一种基于B样条的DER动力学建模方法，通过将离散数据转换为连续可微函数，利用简单线性回归估计低阶线性ODE，并采用向后欧拉离散化，显著降低计算成本，同时保持高拟合度，适合实时运维。


<details>
  <summary>Details</summary>
Motivation: 鉴于DERs在传输层的普及，传统EMT模型虽准确但计算成本高；SysID等数据驱动方法需要求解复杂ODE/传递函数，不利于实时应用。

Method: 使用B样条将离散系统数据平滑为连续函数；在此基础上通过线性回归估计低阶线性ODE；将得到的微分方程进行向后欧拉离散化，以便整合进离散时间的发电调度模型。

Result: 拟合优度GoF约98.74%，接近SysID的99.03%，但速度提升约4.8倍；单次执行时间<1分钟，具备实时应用潜力。

Conclusion: 该方法在保持较高准确性的同时显著降低计算成本，适合实时电力系统运行中的动态建模，并可嵌入离散时间调度框架。

Abstract: The increasing integration of Distributed Energy Resources (DERs) into power
systems necessitates the accurate representation of their dynamic behavior at
the transmission level. Traditional electromagnetic transient models (EMT),
while effective, face scalability challenges due to their reliance on detailed
system information. Data-driven approaches, such as System Identification
(SysID), offer a promising alternative by modeling system dynamics without
detailed system knowledge. However, SysID and similar methods are
computationally intensive, requiring the computation of complex ordinary
differential equations (ODEs) or transfer functions estimation. This makes them
less effective for real-time operation. We therefore propose a novel
data-driven approach that simplifies the modeling of DERs dynamics by
leveraging B-splines to transform discrete system data into continuous
differentiable functions. This enables the estimation of lower order linear
ordinary differential equations with simple linear regression to represent the
underlying dynamics at a very low computational cost. Furthermore, the
extracted dynamic equations are discretized by the backward Euler method for
potential integration into discrete-time power dispatch models. Validation
results indicate a goodness-of-fit (GoF) of 98.74%, comparable to the 99.03%
GoF of the SysID method, yet, 4.8 times faster. Our proposed model's execution
time of less than one minute makes it more suitable for real-time applications
in power system operations.

</details>


### [79] [IoT and Predictive Maintenance in Industrial Engineering: A Data-Driven Approach](https://arxiv.org/abs/2511.04923)
*P. Vijaya Bharati,J. S. V. Siva Kumar,Sathish K Anumula,P Vamshi Krishna,Sangam Malla*

Main category: eess.SY

TL;DR: IoT驱动的预测性维护在工业4.0中通过数据分析和机器学习提升设备可靠性和运营效率，减少停机时间。


<details>
  <summary>Details</summary>
Motivation: 为降低维护成本、减少故障停机、提升生产连续性，推动 IoT 与数据驱动的预测性维护集成。

Method: 论述了IoT数据的实时收集、数据处理与预测建模的系统化流程，强调数据分析技术与机器学习在集成中的作用。

Result: 结论指出实现预测性维护可显著提升运营效率、降低停机时间和成本，支持在现代工业中的广泛应用。

Conclusion: IoT 与数据分析驱动的预测性维护是现代工业的关键组成部分，应被广泛采用并持续优化。

Abstract: Fourth Industrial Revolution has brought in a new era of smart manufacturing,
wherein, application of Internet of Things , and data-driven methodologies is
revolutionizing the conventional maintenance. With the help of real-time data
from the IoT and machine learning algorithms, predictive maintenance allows
industrial systems to predict failures and optimize machines life. This paper
presents the synergy between the Internet of Things and predictive maintenance
in industrial engineering with an emphasis on the technologies, methodologies,
as well as data analytics techniques, that constitute the integration. A
systematic collection, processing, and predictive modeling of data is
discussed. The outcomes emphasize greater operational efficiency, decreased
downtime, and cost-saving, which makes a good argument as to why predictive
maintenance should be implemented in contemporary industries.

</details>


### [80] [Strategic Decision-Making Under Uncertainty through Bi-Level Game Theory and Distributionally Robust Optimization](https://arxiv.org/abs/2511.04940)
*Jiachen Shen,Jian Shi,Lei Fan,Chenye Wu,Dan Wang,Choong Seon Hong,Zhu Han*

Main category: eess.SY

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: In strategic scenarios where decision-makers operate at different
hierarchical levels, traditional optimization methods are often inadequate for
handling uncertainties from incomplete information or unpredictable external
factors. To fill this gap, we introduce a mathematical framework that
integrates bi-level game theory with distributionally robust optimization
(DRO), particularly suited for complex network systems. Our approach leverages
the hierarchical structure of bi-level games to model leader-follower
interactions while incorporating distributional robustness to guard against
worst-case probability distributions. To ensure computational tractability, the
Karush-Kuhn-Tucker (KKT) conditions are used to transform the bi-level
challenge into a more manageable single-level model, and the
infinite-dimensional DRO problem is reformulated into a finite equivalent. We
propose a generalized algorithm to solve this integrated model. Simulation
results validate our framework's efficacy, demonstrating that under high
uncertainty, the proposed model achieves up to a 22\% cost reduction compared
to traditional stochastic methods while maintaining a service level of over
90\%. This highlights its potential to significantly improve decision quality
and robustness in networked systems such as transportation and communication
networks.

</details>


### [81] [Neural Operators for Power Systems: A Physics-Informed Framework for Modeling Power System Components](https://arxiv.org/abs/2511.05216)
*Ioannis Karampinis,Petros Ellinas,Johanna Vorwerk,Spyros Chatzivasileiadis*

Main category: eess.SY

TL;DR: 提出以 DeepONet 为核心的神经算子框架，用于电力系统动态仿真；引入 PI-DeepONet 将物理约束嵌入训练以提升泛化和数据效率，显著加速并稳定性更好地替代高阶 ODE 求解器。


<details>
  <summary>Details</summary>
Motivation: 解决大规模或在线场景下传统ODE求解器速度瓶颈，需快速、准确的动态仿真以用于稳定性评估、数字孪生和实时控制；希望通过学习映射实现不逐步积分的全轨迹预测。

Method: 使用 DeepONet 学习从系统状态和时变输入到完整轨迹的映射，且通过引入物理信息约束（残差嵌入训练损失）形成 PI-DeepONet，以提升数据效率和对物理规律的遵循；在与 PINNs 的对比中评估稳定性和可扩展性。

Result: DeepONet 与 PI-DeepONet 在多种场景下实现高精度预测，相比高阶 ODE 求解器可实现超过 30 倍的加速；相对于 PINNs，具有更好的稳定性和可扩展性。

Conclusion: 神经算子为实现实时、物理感知的电力系统动态仿真提供有前景的路径，PI-DeepONet 通过物理约束进一步增强泛化与鲁棒性，适合大规模、实时应用。

Abstract: Modern power systems require fast and accurate dynamic simulations for
stability assessment, digital twins, and real-time control, but classical ODE
solvers are often too slow for large-scale or online applications. We propose a
neural-operator framework for surrogate modeling of power system components,
using Deep Operator Networks (DeepONets) to learn mappings from system states
and time-varying inputs to full trajectories without step-by-step integration.
To enhance generalization and data efficiency, we introduce Physics-Informed
DeepONets (PI-DeepONets), which embed the residuals of governing equations into
the training loss. Our results show that DeepONets, and especially
PI-DeepONets, achieve accurate predictions under diverse scenarios, providing
over 30 times speedup compared to high-order ODE solvers. Benchmarking against
Physics-Informed Neural Networks (PINNs) highlights superior stability and
scalability. Our results demonstrate neural operators as a promising path
toward real-time, physics-aware simulation of power system dynamics.

</details>


### [82] [Voltage-Independent Active-Power Droop Coefficient for Enhanced Andronov-Hopf Oscillator Grid-Forming Inverters](https://arxiv.org/abs/2511.05252)
*Hamed Rezazadeh,Mohammad Monfared,Meghdad Fazeli,Saeed Golestan*

Main category: eess.SY

TL;DR: 提出一种增强型主动功率-环自激振荡控制策略EAHO，解决传统AHO受振幅依赖的功率下垂系数导致的功率共享误差的问题，在不改变核心动态优势的前提下实现更好的频率与电压支撑及在并网/岛型两种工作模式下的功率共享准确性，并通过小信号分析和2.5kVA实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统的AHO由于功率下垂系数依赖振荡器电压幅值，导致在扰动下无法维持稳定的一致支撑和准确的有功/无功分享，亟需一个不以电压幅值为变量的下垂策略以提升GFM逆变器的稳定性与协同性。

Method: 提出EAHO，使有功下垂系数不再随电压幅值变化，并保留原AHO的关键动态特性；开展对比分析和小信号分析，以及在2.5 kVA单相逆变器上进行实验验证，评估其在不同电网条件下的稳态和瞬态性能。

Result: 与传统AHO相比，EAHO在稳态功率支撑、频率和电压支撑、以及不同工作模式下的并网/岛型下的功率共享精度方面均有提升，并且通过对比分析与实验验证证实其稳定性和良好鲁棒性。

Conclusion: EAHO在不改变核心动态优势的同时，实现更准确的功率共享和增强的有功/无功支撑，适用于多种电网条件下的GFM逆变器。

Abstract: In recent years, virtual oscillator control, particularly the Andronov-Hopf
oscillator (AHO), has received widespread attention for controlling
grid-forming (GFM) inverters due to their superior dynamic response. However,
traditional AHO systems feature droop coefficients that are dependent on the
oscillator voltage amplitude, limiting their ability to maintain consistent
grid support during disturbances and resulting in power-sharing inaccuracies.
This paper presents an enhanced AHO (EAHO) strategy, where the active power
droop coefficient is no longer a function of the voltage amplitude and retains
the key dynamic benefits of the original AHO. The EAHO improves both frequency
and voltage support and ensures accurate power sharing with other GFM inverters
in grid-connected and stand-alone modes. Extensive comparative and small-signal
analyses, alongside experimental validation on 2.5 kVA single-phase inverters,
confirm the EAHO's improved steady-state performance, enhanced active and
reactive power support, and stable operation under varying grid conditions.

</details>


### [83] [Privacy-Preserving Cramér-Rao Lower Bound](https://arxiv.org/abs/2511.05327)
*Jieming Ke,Jimin Wang,Ji-Feng Zhang*

Main category: eess.SY

TL;DR: 提出隐私保护的Cramer-Rao下界理论，基于Fisher信息矩阵作为隐私度量，给出隐私约束下的可识别性准则、CR下界及其可达性，并把理论扩展到多传感器多测量系统，提出可加性原则和分布式算法，辅以数值验证。


<details>
  <summary>Details</summary>
Motivation: 在需要保护隐私的应用场景中，如何在隐私约束下达到尽可能高的识别性能是核心问题。该工作填补了隐私约束下CR下界的理论框架、可达性分析，以及多传感系统的分布式实现方法。

Method: 以Fisher信息矩阵作为隐私度量，推导隐私约束下的identifiability criterion；在可识别情形下推导并证明隐私保护CR下界及其可达性；将CR下界与多传感器多测量系统的分布式Fisher信息矩阵联系起来，提出跨空间与时间的可加性原则，并据此设计分布式识别算法。

Result: 给出隐私保护CR下界的明确表达式与可达性证明；建立多传感系统的可加性关系；提出实现该下界的分布式算法，并通过数值示例验证理论和算法效果。

Conclusion: 隐私保护CR下界理论提供了在隐私约束下的识别极限，且可加性原则使得分布式系统能够实现该极限，数值结果支持所提算法的有效性。

Abstract: This paper establishes the privacy-preserving Cram\'er-Rao (CR) lower bound
theory, characterizing the fundamental limit of identification accuracy under
privacy constraint. An identifiability criterion under privacy constraint is
derived by using Fisher information matrix as the privacy metric. In the
identifiable case, the privacy-preserving CR lower bound is established and its
attainability is demonstrated, thereby ensuring the existence of the
privacy-preserving Fisher information matrix with explicit expression. Then,
the privacy-preserving CR lower bound theory is extended to the multi-sensor
multi-measurement system. Specifically, the additivity principle of
privacy-preserving Fisher information matrices across both spatial and temporal
dimensions is established, building a relationship between privacy-preserving
CR lower bounds for the multi-sensor multi-measurement system and its
subsystems. Using this additivity principle, distributed identification
algorithms capable of achieving the privacy-preserving CR lower bound are
further proposed. Numerical examples are provided to demonstrate the
privacy-preserving CR lower bound and show the effectiveness of the proposed
algorithms.

</details>


### [84] [Efficient CNN Inference on Ultra-Low-Power MCUs via Saturation-Aware Convolution](https://arxiv.org/abs/2511.05347)
*Shiming Li,Luca Mottola,Yuan Yao,Stefanos Kaxiras*

Main category: eess.SY

TL;DR: 提出了一种饱和感知卷积，通过改变卷积计算顺序并插入饱和检测，在超低功耗MCU上跳过不必要计算，最大可实现24%的推理时间节省且不改变准确率，已集成到MCUNet TinyEngine并在 Cortex-M0+ 上验证。


<details>
  <summary>Details</summary>
Motivation: 在 ultra-low-power MCUs 上的 CNN 推理面临能耗与延迟的权衡。量化 CNN 的某些神经元在输出达到饱和边界后仍执行大量计算，存在冗余。需要一种在不损失输出正确性的前提下减少计算量的策略。

Method: 提出 saturation-aware convolution，通过调整卷积内计算顺序以促进更早饱和，并在关键位置插入饱和检测来跳过后续无效计算。将该实现集成到 TinyEngine，面向 MCUNet 框架；在 Cortex-M0+ MCU 上对 7 个公开模型进行评估。

Result: 实现该方法后，在严格保持准确率不变的前提下，推理时间获得可观提升，实验覆盖 7 个开源 CNN 模型，最高达到 24% 的推理时间节省。

Conclusion: 该方法证明了在超低功耗边缘设备上通过有条件地跳过不必要的计算可以显著提升推理效率，同时保持模型性能，为在低功耗硬件上的高效深度学习推理提供了可行路径，并可扩展到其他架构和硬件。

Abstract: Deploying lightweight CNN inference tasks on ultra-low-power MCUs is often
not limited by space constraint, thanks to the compact size of models, yet
inference latency is crucial for preserving energy. We reveal that quantized
CNN inference on ultra-low-power MCUs executes unnecessary computations in
neurons that produce saturated output values: often times, these neurons still
produce the correct output value without fully completing the computation,
since the neuron value is too extreme and is eventually systematically clamped
at the boundaries allowed by the neuron. We show that with carefully designed
condition checks, it is possible to identify and skip these unnecessary
computations without impacting the neuron output. Based on this, we present
saturation-aware convolution: an inference technique whereby computations in
convolution kernels are executed in an altered order to induce earlier
saturation, and saturation checks are inserted to omit unnecessary
computations. We integrate our implementation into MCUNet's TinyEngine, the
state-of-the-art neural network code generation and inference framework, and
conduct experiments on a Cortex-M0+ MCU. The result based on 7 open-source CNN
models displays up to 24% inference time saving, with strictly zero impact on
neural network accuracy.

</details>


### [85] [Coherency Control in Power Systems](https://arxiv.org/abs/2511.05391)
*Rodrigo Bernal,Ignacio Ponce,Federico Milano*

Main category: eess.SY

TL;DR: 实现具有固定幅值比和相位偏移的协同性输出控制，基于注入电流的复频CF等效性来对IBR进行资源无关的协同性管理。


<details>
  <summary>Details</summary>
Motivation: 在现代电力系统中提升IBR之间的动态协同与阻尼，降低扰动传播并提升稳定性。

Method: 将复频CF的等效性作为设备间协性的定义；设计输出电流以与参考相比具有定量幅值和恒定相位偏移的控制；实现对任意资源类型的协同性；在两区域与IEEE 39-bus等系统上进行案例分析，评估延迟、噪声敏感性及抑振与扰动传播之间的权衡。

Result: 证明所提出的协同性控制可以改善阻尼和动态行为，并表明其对不同资源的技术无关性使得协同性成为可行的直接控制目标，同时给出实现中的实际考虑。

Conclusion: 将协同性作为IBR直接控制目标是可行且具有应用潜力的方向，尤其在多区域和大型系统的仿真验证中表现出积极的阻尼与稳态改进。

Abstract: This paper proposes a coherency control strategy for Inverter-Based Resources
(IBRs) to establish coherence among power system devices. Using the equivalence
of the Complex Frequency (CF) of the injected currents as the definition for
coherency among devices, the control enforces an output current with a
proportional magnitude and a constant phase shift relative to a reference. This
formulation makes the control technology-agnostic, enabling coherency with any
type of resource. Case studies based on the two-area and IEEE 39-bus systems
demonstrate the controller's potential to improve damping and overall dynamic
behavior. The paper further evaluates practical implementation aspects
including delay/noise sensitivity and the trade-off between oscillation
mitigation and disturbance propagation. This work establishes coherency as a
viable direct control objective for IBRs in modern power systems.

</details>


### [86] [A Tilting-Rotor Enhanced Quadcopter Fault-Tolerant Control Based on Non-Linear Model Predictive Control](https://arxiv.org/abs/2511.05445)
*Yanchao Wang,Xu You,Mehdi Baghdadi*

Main category: eess.SY

TL;DR: 提出一种基于悬臂转子四轴飞行器的容错控制策略，采用非线性模型预测控制(NMPC)以在单个或多个螺旋桨失效时维持姿态和位置稳定性，并通过扩展状态观测器(ESO)预测故障后的模型偏差，在下一时步对模型进行修正，实现主动容错控制；通过仿真与不含观测器的传统四轴和仅含观测器的 tilt-rotor 进行对比，结果表明在故障情况下，倾转桨四轴能维持定位控制且不牺牲偏航稳定性。


<details>
  <summary>Details</summary>
Motivation: 在无人机系统中，螺旋桨失效或部分失效会导致姿态和定位不稳定，传统四轴在此情况下性能降低甚至失控。倾转桨结构具备一定冗余和可控性，但需有效控制策略以实现容错并保持稳定性，尤其在偏航方面。

Method: 使用非线性模型预测控制(NMPC)来同时优化姿态与定位，结合扩展状态观测器(ESO)在故障发生后预测模型偏差，并在下一个采样时刻对系统模型进行纠正，从而实现主动容错控制。通过搭建倾转桨四轴飞控原型并进行仿真评估，比较对象包括传统四轴和在无观测器条件下的 tilt-rotor 四轴。

Result: 仿真结果表明，在螺旋桨失效场景下，倾转桨四轴在保持定位控制的同时保持了偏航稳定性，相较于传统四轴，性能得到显著提升；带 ESO 的倾转桨系统在相同条件下表现更优，证明了观测器对模型偏差的有效补偿能力。

Conclusion: 该工作提出的基于NMPC结合ESO的容错控制策略，能够在螺旋桨失效时为倾转桨四轴提供鲁棒的定位与姿态控制，并保持偏航稳定性。该方法展示了在动态不确定和故障情形下的可行性，具有一定的工程实现潜力和应用价值。

Abstract: This paper proposes a fault-tolerant control strategy based on a tilt-rotor
quadcopter prototype, utilizing nonlinear model predictive control to maintain
both attitude and position stability in the event of rotor failure. The control
strategy employs an extended state observer to predict model deviations
following a fault and adjusts the original model in the subsequent time step,
thereby achieving active fault-tolerant control. The proposed method is
evaluated through simulations and compared to both traditional quadcopter and
tilt-rotor quadcopter without observer under identical conditions. The results
demonstrate that the tilt-rotor quadcopter can maintain position control
without sacrificing yaw stability, unlike traditional quadcopters.

</details>
