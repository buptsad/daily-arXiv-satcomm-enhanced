<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 13]
- [eess.SP](#eess.SP) [Total: 10]
- [eess.SY](#eess.SY) [Total: 13]
- [cs.LG](#cs.LG) [Total: 84]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.19558)
*Mohammed Talha Alam,Nada Saadi,Fahad Shamshad,Nils Lukas,Karthik Nandakumar,Fahkri Karray,Samuele Poppi*

Main category: cs.CR

TL;DR: 提出 SPQR 基准，用单一分数评估文本到图像扩散模型在 benign 微调后的安全性、效用和鲁棒性，揭示现有安全方法在微调下的脆弱性，并给出跨语言/领域/OD 的分析。


<details>
  <summary>Details</summary>
Motivation: 现有安全性对抗常在部署后进行微调时失效，需验证在 benign 微调下的稳健性。

Method: 提出 SPQR 指标，进行多语言、领域特定、分布外分析和类别分解，比较不同安全对齐方法在微调后的表现。

Result: SPQR 提供一个可复现的单分数评估框架，能总结安全性、实用性和鲁棒性在微调后的保持情况，展示了在某些场景下安全对齐会下降的情况。

Conclusion: SPQR 可作为 T2I 安全对齐方法的简洁且综合的基准，便于横向比较和研究进展。

Abstract: Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.

</details>


### [2] [Synthetic Data: AI's New Weapon Against Android Malware](https://arxiv.org/abs/2511.19649)
*Angelo Gaspar Diniz Nogueira,Kayua Oleques Paim,Hendrio Bragança,Rodrigo Brandão Mansilha,Diego Kreutz*

Main category: cs.CR

TL;DR: MalSynGen uses a conditional GAN to generate synthetic tabular data for Android malware detection, addressing data scarcity and obsolescence, and improving classifier performance while maintaining data fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of Android devices and malware, coupled with the high cost of real malware labeling, necessitates high-quality synthetic data to train robust detectors.

Method: A conditional GAN (cGAN) is trained to produce synthetic tabular features that preserve the statistical properties of real malware-related data; the approach is evaluated across multiple datasets and metrics for data fidelity, utility in classification, and computational efficiency.

Result: Synthetic data from MalSynGen improves Android malware classifiers, demonstrates generalization across datasets, and provides efficient data generation.

Conclusion: MalSynGen offers a viable solution to data obsolescence and quality issues in Android malware detection, enabling better ML-based defense through synthetic data generation.

Abstract: The ever-increasing number of Android devices and the accelerated evolution of malware, reaching over 35 million samples by 2024, highlight the critical importance of effective detection methods. Attackers are now using Artificial Intelligence to create sophisticated malware variations that can easily evade traditional detection techniques. Although machine learning has shown promise in malware classification, its success relies heavily on the availability of up-to-date, high-quality datasets. The scarcity and high cost of obtaining and labeling real malware samples presents significant challenges in developing robust detection models. In this paper, we propose MalSynGen, a Malware Synthetic Data Generation methodology that uses a conditional Generative Adversarial Network (cGAN) to generate synthetic tabular data. This data preserves the statistical properties of real-world data and improves the performance of Android malware classifiers. We evaluated the effectiveness of this approach using various datasets and metrics that assess the fidelity of the generated data, its utility in classification, and the computational efficiency of the process. Our experiments demonstrate that MalSynGen can generalize across different datasets, providing a viable solution to address the issues of obsolescence and low quality data in malware detection.

</details>


### [3] [Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning](https://arxiv.org/abs/2511.19654)
*Stephen C. Gravereaux,Sheikh Rabiul Islam*

Main category: cs.CR

TL;DR: LoRA 微调的大规模语言模型在生成可解释的恶意软件分类决策方面，能在一定程度上接近完全微调模型的表现；完整微调总体分数最高，但中等规模的 LoRA 在两项指标上超越全量微调，同时在显著降低模型规模（约81% 参数）与训练时间（>80%）的情况下保持竞争力，15.5%的可训练参数比例实现了较高效率。


<details>
  <summary>Details</summary>
Motivation: 解决在恶意软件检测中引入可解释性与可信度的挑战，比较全微调与 LoRA 微调在解释质量与资源效率上的权衡，以便在资源受限环境中的部署。

Method: 构建包含五种 LoRA 配置与一个全微调基线的评估框架，基于 BLEU、ROUGE 和语义相似性等指标对解释质量进行基准评测；比较不同配置在“人类可解释的决策与解释”上的表现。

Result: 全微调在总体评分上最高，BLEU 与 ROUGE 相对 LoRA 的提升可达约10%；中等规模的 LoRA 模型在两项指标上超过全微调，且在参数规模下降约81%、训练时间降低>80%的前提下，仍保持竞争力（以一个 15.5% 可训练参数的 LoRA 模型为例）。

Conclusion: LoRA 为解释性与资源效率之间提供了实用的折衷，适合在资源受限环境下部署，同时仍能提供可解释且具有较好透明度的恶意软件分类解释，提升分析师信心与系统可扩展性。

Abstract: This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.

</details>


### [4] [BASICS: Binary Analysis and Stack Integrity Checker System for Buffer Overflow Mitigation](https://arxiv.org/abs/2511.19670)
*Luis Ferreirinha,Iberia Medeiros*

Main category: cs.CR

TL;DR: 提出一种基于 MemStaCe 的二进制级漏洞发现与修复框架，结合模型检查与混合执行，利用 trampoline 补丁和 crash-inducing 输入验证，在 BASICS 工具中实现并在 Juliet/SARD 数据集及实际应用中达到约 87% 以上的检测与修复准确性，优于 CWE Checker。


<details>
  <summary>Details</summary>
Motivation: CPS 对关键基础设施的依赖需要高可靠性，但用 C 编写的 CPS 软件易受缓冲区溢出等漏洞影响，传统二进制漏洞发现方法在可扩展性和准确性方面存在瓶颈，需无缝的自动化验证与修复方法。

Method: 构建 Memory State Space MemStaCe，基于二进制程序的控制流图和 concolic 执行对 C 函数调用与循环进行模拟；使用 LTL 形式化安全属性以建模与漏洞相关函数的正确行为；通过对不满足属性的 counterexample 路径识别漏洞；以 trampoline 基于的二进制补丁修复漏洞并用通过 concolic 运行生成的 crash-inducing 输入来验证修复效果；最终在 BASICS 工具中实现并对公开数据集与实际应用评估。

Result: 在 Juliet C/C++、SARD 数据集及真实应用上，检测与修复的准确性与精确性均超过 87%；并且在与 CWE Checker 的对比中实现了优越表现。

Conclusion: 该方法实现了对二进制级漏洞的自动分析、修复与验证，提升了 CPS 软件在实际部署中的安全性与可靠性，并对现有检测工具提供了显著改进。

Abstract: Cyber-Physical Systems have played an essential role in our daily lives, providing critical services such as power and water, whose operability, availability, and reliability must be ensured. The C programming language, prevalent in CPS development, is crucial for system control where reliability is critical. However, it is also commonly susceptible to vulnerabilities, particularly buffer overflows. Traditional vulnerability discovery techniques often struggle with scalability and precision when applied directly to the binary code of C programs, which can thereby keep programs vulnerable. This work introduces a novel approach designed to overcome these limitations by leveraging model checking and concolic execution techniques to automatically verify security properties of a program's stack memory in binary code, trampoline techniques to perform automated repair of the issues, and crash-inducing inputs to verify if they were successfully removed. The approach constructs a Memory State Space - MemStaCe- from the binary program's control flow graph and simulations, provided by concolic execution, of C function calls and loop constructs. The security properties, defined in LTL, model the correct behaviour of functions associated with vulnerabilities and allow the approach to identify vulnerabilities in MemStaCe by analysing counterexample traces that are generated when a security property is violated. These vulnerabilities are then addressed with a trampoline-based binary patching method, and the effectiveness of the patches is checked with crash-inducing inputs extracted during concolic execution. We implemented the approach in the BASICS tool for BO mitigation and evaluated using the Juliet C/C++ and SARD datasets and real applications, achieving an accuracy and precision above 87%, both in detection and correction. Also, we compared it with CWE Checker, outperforming it.

</details>


### [5] [CrypTorch: PyTorch-based Auto-tuning Compiler for Machine Learning with Multi-party Computation](https://arxiv.org/abs/2511.19711)
*Jinyu Liu,Gang Tan,Kiwan Maeng*

Main category: cs.CR

TL;DR: CrypTorch 是一个面向 MPC-based 机器学习的编译器，能够将近似实现（如 Softmax、GELU）与 MPC 运行时解耦并自动选择以在性能和准确度之间实现折中，从而显著提升端到端性能，相较 CrypTen 具有更大提升。


<details>
  <summary>Details</summary>
Motivation: 现有框架在实现 MPC 机器学习时对 Softmax、GELU 等运算采用近似，导致成为性能瓶颈；这些近似往往不够准确或过慢，且难以在现有框架内定位与修复。需要一个能把近似与 MPC 运行时解耦、可扩展并自动优化近似选择的编译器，以提升性能与准确性。

Method: 构建为 PyTorch 2 编译器的扩展 CrypTorch，通过将近似运算与 MPC 运行时解耦、提供易扩展的近似接口、并实现自动选取策略来在性能和准确性之间做权衡。该框架还具备自动调优能力，在实际基线基准上进行端到端优化。

Result: 自动调优本身即可在不损失精度的前提下带来1.20–1.7×的即时加速；在允许一定精度下降时可达1.31–1.8×的加速；相比流行框架 CrypTen，端到端可实现3.22–8.6×的加速。

Conclusion: 将近似运算从 MPC 运行时中解耦并以编译器层实现自动化的近似选择，能显著提升基于 MPC 的机器学习的性能；结合更强的工程实践与前沿实践，CrypTorch 为端到端性能优化提供了有力的框架和方法。

Abstract: Machine learning (ML) involves private data and proprietary model parameters. MPC-based ML allows multiple parties to collaboratively run an ML workload without sharing their private data or model parameters using multi-party computing (MPC). Because MPC cannot natively run ML operations such as Softmax or GELU, existing frameworks use different approximations. Our study shows that, on a well-optimized framework, these approximations often become the dominating bottleneck. Popular approximations are often insufficiently accurate or unnecessarily slow, and these issues are hard to identify and fix in existing frameworks. To tackle this issue, we propose a compiler for MPC-based ML, CrypTorch. CrypTorch disentangles these approximations with the rest of the MPC runtime, allows easily adding new approximations through its programming interface, and automatically selects approximations to maximize both performance and accuracy. Built as an extension to PyTorch 2's compiler, we show that CrypTorch's auto-tuning alone provides 1.20--1.7$\times$ immediate speedup without sacrificing accuracy, and 1.31--1.8$\times$ speedup when some accuracy degradation is allowed, compared to our well-optimized baseline. Combined with better engineering and adoption of state-of-the-art practices, the entire framework brings 3.22--8.6$\times$ end-to-end speedup compared to the popular framework, CrypTen.

</details>


### [6] [Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts](https://arxiv.org/abs/2511.19727)
*Steven Peh*

Main category: cs.CR

TL;DR: 提出名为 Prompt Fencing 的新架构，通过对提示段落进行密码学签名以建立安全边界，从而防止提示注入攻击，实验在两家主流 LLM 上将攻击成功率从 86.7% 降至 0%，并且总开销为 0.224 秒。该方法可平台无关地部署为现有 LLM 基础设施之上，未来模型将具备本地化 fence Awareness。


<details>
  <summary>Details</summary>
Motivation: LLMs 在生产部署中易受提示注入攻击，构成最严重的安全威胁之一；需要在提示中建立明确的信任与内容边界。当前模型缺乏 fence awareness，因此需引入可验证的元数据来区分可信指令与未可信内容。

Method: 对提示段落进行带有签名元数据的装饰，包括信任等级和内容类型；通过建立 fence 生成与验证管线实现对提示的加密认证与验证；在仿真中通过指示来实现感知；测量开销为 0.130sFence generation 与 0.094s验证，总共 0.224s；方法平台无关，能在现有 LLM 基础设施上分步部署。

Result: 在 300 个测试用例、两家领先 LLM 提供商的实验中，未检测到注入攻击成功（成功率从 86.7% 降至 0%），证明了该方法的有效性；并给出 100 个样本的总开销。

Conclusion: 该方法可作为现有 LLM 基础设施之上的安全层实现渐进式部署；未来模型应具备本地化 fence awareness 以实现最佳安全性；方法具平台无关特性，且可逐步推进。

Abstract: Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.

</details>


### [7] [Cross-LLM Generalization of Behavioral Backdoor Detection in AI Agent Supply Chains](https://arxiv.org/abs/2511.19874)
*Arun Chowdary Sanna*

Main category: cs.CR

TL;DR: 首次系统研究跨大模型的行为后门检测，揭示单模型在跨LLMs泛化方面存在显著差距；通过引入模型身份作为特征实现跨模型高准确度检测，并发布多LLM轨迹数据集与检测框架。


<details>
  <summary>Details</summary>
Motivation: 企业工作流对共享工具库和预训练组件的依赖带来供应链层面的后门风险。现有工作多聚焦于单一LLM的后门检测，缺乏跨模型的泛化研究，危及部署多模型系统的安全性。

Method: 对六个生产级LLM进行大规模实验（GPT-5.1、Claude Sonnet 4.5、Grok 4.1、Llama 4 Maverick、GPT-OSS 120B、DeepSeek Chat V3.1），收集1198条执行轨迹与36组跨模型实验。比较单模型检测器在同分布与跨模型上的表现，分析时间特征与结构特征的鲁棒性，提出将模型身份作为附加特征以实现跨模型检测。

Result: 单模型检测在训练分布内准确率为92.7%，跨模型仅49.2%，泛化差距43.4个百分点，接近随机猜测；时间特征（如变异系数CV>0.8）是关键信号，结构特征较为稳定。引入模型身份等元信息后，检测准确率提升至90.6%，实现对所有评估模型的跨模型一致性。

Conclusion: 提出模型感知的后门检测框架，提升跨模型鲁棒性，并公开多LLM轨迹数据集与检测框架，促进可重复性研究。

Abstract: As AI agents become integral to enterprise workflows, their reliance on shared tool libraries and pre-trained components creates significant supply chain vulnerabilities. While previous work has demonstrated behavioral backdoor detection within individual LLM architectures, the critical question of cross-LLM generalization remains unexplored, a gap with serious implications for organizations deploying multiple AI systems. We present the first systematic study of cross-LLM behavioral backdoor detection, evaluating generalization across six production LLMs (GPT-5.1, Claude Sonnet 4.5, Grok 4.1, Llama 4 Maverick, GPT-OSS 120B, and DeepSeek Chat V3.1). Through 1,198 execution traces and 36 cross-model experiments, we quantify a critical finding: single-model detectors achieve 92.7% accuracy within their training distribution but only 49.2% across different LLMs, a 43.4 percentage point generalization gap equivalent to random guessing. Our analysis reveals that this gap stems from model-specific behavioral signatures, particularly in temporal features (coefficient of variation > 0.8), while structural features remain stable across architectures. We show that model-aware detection incorporating model identity as an additional feature achieves 90.6% accuracy universally across all evaluated models. We release our multi-LLM trace dataset and detection framework to enable reproducible research.

</details>


### [8] [Improving the Identification of Real-world Malware's DNS Covert Channels Using Locality Sensitive Hashing](https://arxiv.org/abs/2511.20229)
*Pascal Ruffing,Denis Petrov,Sebastian Zillien,Steffen Wendzel*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Nowadays, malware increasingly uses DNS-based covert channels in order to evade detection and maintain stealthy communication with its command-and-control servers. While prior work has focused on detecting such activity, identifying specific malware families and their behaviors from captured network traffic remains challenging due to the variability of DNS. In this paper, we present the first application of Locality Sensitive Hashing to the detection and identification of real-world malware utilizing DNS covert channels. Our approach encodes DNS subdomain sequences into statistical similarity features that effectively capture anomalies indicative of malicious activity. Combined with a Random Forest classifier, our method achieves higher accuracy and reduced false positive rates than prior approaches, while demonstrating improved robustness and generalization to previously unseen or modified malware samples. We further demonstrate that our approach enables reliable classification of malware behavior (e.g., uploading or downloading of files), based solely on DNS subdomains.

</details>


### [9] [Frequency Bias Matters: Diving into Robust and Generalized Deep Image Forgery Detection](https://arxiv.org/abs/2511.19886)
*Chi Liu,Tianqing Zhu,Wanlei Zhou,Wei Zhao*

Main category: cs.CR

TL;DR: 提出基于频率对齐的两步方法，揭示频率偏差是深度伪造检测泛化与鲁棒性问题的潜在根源，并提供可双向使用的攻击与防御机制，在12个检测器、8个伪造模型、5项指标的广泛实验中得到验证。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 生成的图像伪造越来越普遍，检测在未知 GAN 与开放世界下的鲁棒性与泛化能力成为核心问题。尽管已有研究尝试提升这两者，但往往未明确两者的根本原因及其关系，也缺乏能同时服务于攻击和防御的通用方法。

Method: 基于频率层面的分析，发现 DNN 伪造检测器存在对频率的偏置，这可能导致对新型伪造样本的泛化与对噪声样本的鲁棒性下降。提出两步频率对齐方法，旨在消除真实与伪造图像之间的频率差异，作为黑盒攻击以挑战现有检测器，亦可作为普适防御以提升检测器的可靠性。实现了对应的攻击与防御算法，并在12个检测器、8个伪造模型、5项评估指标上进行了实验验证。

Result: 频率对齐显著提升了检测系统的鲁棒性和泛化能力，并证实该方法在双向应用（攻击与防御）上的可行性；实验覆盖广泛设置，表明频率偏差是一个具有解释力的根源。

Conclusion: 频率偏差是深度伪造检测在泛化与鲁棒性方面的潜在根源；提出的两步频率对齐方法为防御提供统一、普适的解决方案，同时也可用于反向攻击，揭示了频域特性在伪造检测与对抗中的关键作用，未来可扩展到更广的取证场景。

Abstract: As deep image forgery powered by AI generative models, such as GANs, continues to challenge today's digital world, detecting AI-generated forgeries has become a vital security topic. Generalizability and robustness are two critical concerns of a forgery detector, determining its reliability when facing unknown GANs and noisy samples in an open world. Although many studies focus on improving these two properties, the root causes of these problems have not been fully explored, and it is unclear if there is a connection between them. Moreover, despite recent achievements in addressing these issues from image forensic or anti-forensic aspects, a universal method that can contribute to both sides simultaneously remains practically significant yet unavailable. In this paper, we provide a fundamental explanation of these problems from a frequency perspective. Our analysis reveals that the frequency bias of a DNN forgery detector is a possible cause of generalization and robustness issues. Based on this finding, we propose a two-step frequency alignment method to remove the frequency discrepancy between real and fake images, offering double-sided benefits: it can serve as a strong black-box attack against forgery detectors in the anti-forensic context or, conversely, as a universal defense to improve detector reliability in the forensic context. We also develop corresponding attack and defense implementations and demonstrate their effectiveness, as well as the effect of the frequency alignment method, in various experimental settings involving twelve detectors, eight forgery models, and five metrics.

</details>


### [10] [Zero-Knowledge Proof Based Verifiable Inference of Models](https://arxiv.org/abs/2511.19902)
*Yunxiao Wang*

Main category: cs.CR

TL;DR: 提出一个零知识框架，在不暴露模型参数的前提下验证深度学习推断，支持线性/非线性层，使用递归零知识证明和Fiat-Shamir，得到zkSNARK，且对DeepSeek模型实现ZK-DeepSeek，实验表明高效、灵活。


<details>
  <summary>Details</summary>
Motivation: 当前AI推断验证需要暴露内部参数，但模型参数成本高、知识产权保护需求强，需一种无需信任设置的零知识验证方法来验证推断正确性。

Method: 基于递归构成的零知识证明框架，不需要信任设置，覆盖矩阵乘法、归一化、softmax、SiLU等线性/非线性层，结合Fiat-Shamir将多段证明变为zkSNARK并提供常数大小证明。将DeepSeek模型改写为ZK-DeepSeek以便可验证。

Result: 理论上实现可证明性和常数大小证据；在实践中将DeepSeek改写为ZK-DeepSeek并通过实验展示在真实工作负载中的效率与灵活性。

Conclusion: 本文提出的零知识框架可在不泄露模型内部参数的情况下实现高效的深度学习推断验证，且通过无信任设置和zkSNARK实现具备可部署性；对AI安全、跨机构验证等场景具有应用前景。

Abstract: Recent advances in artificial intelligence (AI), particularly deep learning, have led to widespread adoption across various applications. Yet, a fundamental challenge persists: how can we verify the correctness of AI model inference when model owners cannot (or will not) reveal their parameters? These parameters represent enormous training costs and valuable intellectual property, making transparent verification difficult. In this paper, we introduce a zero-knowledge framework capable of verifying deep learning inference without exposing model internal parameters. Built on recursively composed zero-knowledge proofs and requiring no trusted setup, our framework supports both linear and nonlinear neural network layers, including matrix multiplication, normalization, softmax, and SiLU. Leveraging the Fiat-Shamir heuristic, we obtain a succinct non-interactive argument of knowledge (zkSNARK) with constant-size proofs. To demonstrate the practicality of our approach, we translate the DeepSeek model into a fully SNARK-verifiable version named ZK-DeepSeek and show experimentally that our framework delivers both efficiency and flexibility in real-world AI verification workloads.

</details>


### [11] [Hey there! You are using WhatsApp: Enumerating Three Billion Accounts for Security and Privacy](https://arxiv.org/abs/2511.20252)
*Gabriel K. Gegenhuber,Philipp É. Frenzel,Maximilian Günther,Johanna Ullrich,Aljosha Judmayer*

Main category: cs.CR

TL;DR: WhatsApp 的联系人查询机制暴露出大规模的号码枚举风险，研究量化了规模与影响，揭示数据泄露残留、密钥重用等问题，并记录速率限制的改进进展。


<details>
  <summary>Details</summary>
Motivation: 在全球最大的即时通讯平台上，用户通过地址簿查询联系人是否注册，导致潜在隐私暴露与滥用风险；需要评估可枚举性、影响范围及有效的缓解策略。

Method: 通过向 WhatsApp 服务器查询手机号码以判断账号是否注册，统计单位时间的查询量（達百萬級/小時），对 2021 年 Facebook 数据泄露中的号码活跃性进行比对，并分析跨设备/跨号码的 X25519 密钥重用现象，最后评估速率限制的改进与协同修复过程。

Result: 实验表明 WhatsApp 在大规模枚举面前仍具高可用性，未发现有效的阻塞或严格的速率限制；约一半来自 2021 年数据泄露的号码仍在 WhatsApp 活跃；能够对全球用户进行宏观统计，揭示元数据层面的隐私风险；发现某些 X25519 密钥在不同设备/号码间重复使用；更新版本指明速率限制问题已通过协作修复得到解决。

Conclusion: 该研究强调端对端加密并不能消除对元数据与账户可用性信息的暴露风险，提出需要加强速率限制、数据披露控制与密钥管理等防护，同时表明对大规模枚举的持续监测与修复工作的重要性。

Abstract: WhatsApp, with 3.5 billion active accounts as of early 2025, is the world's largest instant messaging platform. Given its massive user base, WhatsApp plays a critical role in global communication.
  To initiate conversations, users must first discover whether their contacts are registered on the platform. This is achieved by querying WhatsApp's servers with mobile phone numbers extracted from the user's address book (if they allowed access). This architecture inherently enables phone number enumeration, as the service must allow legitimate users to query contact availability. While rate limiting is a standard defense against abuse, we revisit the problem and show that WhatsApp remains highly vulnerable to enumeration at scale. In our study, we were able to probe over a hundred million phone numbers per hour without encountering blocking or effective rate limiting.
  Our findings demonstrate not only the persistence but the severity of this vulnerability. We further show that nearly half of the phone numbers disclosed in the 2021 Facebook data leak are still active on WhatsApp, underlining the enduring risks associated with such exposures. Moreover, we were able to perform a census of WhatsApp users, providing a glimpse on the macroscopic insights a large messaging service is able to generate even though the messages themselves are end-to-end encrypted. Using the gathered data, we also discovered the re-use of certain X25519 keys across different devices and phone numbers, indicating either insecure (custom) implementations, or fraudulent activity.
  In this updated version of the paper, we also provide insights into the collaborative remediation process through which we confirmed that the underlying rate-limiting issue had been resolved.

</details>


### [12] [Can LLMs Make (Personalized) Access Control Decisions?](https://arxiv.org/abs/2511.20284)
*Friederike Groschupp,Daniele Lain,Aritra Dhar,Lara Magdalena Lazier,Srdjan Čapkun*

Main category: cs.CR

TL;DR: 本研究提出用大语言模型（LLMs）进行动态、上下文感知的访问控制决策，以减轻用户在安装和运行时的认知负担，并与用户隐私偏好对齐。通过一项用户研究构建数据集（307条自然语言隐私陈述与14,682条用户决策），并比较一般和个性化两版LLMs的决策（并收集1,446条决策的用户反馈）。结果显示：在多数情况下，LLMs可较好反映用户偏好，最高可达86%的一致性；个性化偏好虽提升与个人决策的一致性，但也可能违反某些安全最佳实践。最后给出在实际NL语言基础访问控制系统中平衡个性化、安全性与实用性的设计与风险考量。


<details>
  <summary>Details</summary>
Motivation: 随着系统日益复杂与自动化，现有的访问控制决策需用户在安装/运行时做出，造成认知负担和不理性决策。应探索如何利用LLMs的推理能力实现动态、上下文感知且符合用户安全偏好的访问控制。

Method: 通过一项用户研究，收集307条自然语言隐私陈述与14,682条用户访问控制决策，并与两版LLMs（一般与个性化）对比决策，同时收集1,446条决策的用户反馈。评估LLMs在匹配多数用户决策与个人决策方面的表现。

Result: 一般LLMs在与多数用户决策的一致性方面可达高达86%；个性化偏好有助于提高与个人决策的一致性，但可能与安全最佳实践发生冲突。

Conclusion: 基于结果，提出在实现基于自然语言的访问控制系统时的设计与风险考量，重点在于在个性化、安全性与效用之间的权衡。

Abstract: Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.
  Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.

</details>


### [13] [A Reality Check on SBOM-based Vulnerability Management: An Empirical Study and A Path Forward](https://arxiv.org/abs/2511.20313)
*Li Zhou,Marc Dacier,Charalambos Konstantinou*

Main category: cs.CR

TL;DR: 提出一个面向SSC安全的两阶段方法：先使用带锁文件的强包管理器生成高保真SBOM，再通过函数调用分析来消除漏洞扫描中的大量假阳性，从而实现低噪声、可操作的漏洞报告。


<details>
  <summary>Details</summary>
Motivation: SBOM的准确性和漏洞扫描的可用性直接影响软件供应链安全，但现有做法在生成SBOM和应用漏洞分析时存在显著不一致性和高误报。通过大规模实证（2,414个开源仓库）来评估现状并提出可落地的解决方案。

Method: 在大规模数据集上比较了在使用锁文件与强包管理器时生成的SBOM的准确性；衡量下游漏洞扫描的误报率，发现97.5%的误报来自不可达代码中的漏洞。随后采用函数调用分析对代码路径进行 prune，实证降低63.3%的误报。提出一个两阶段流程：第一阶段用锁文件+强包管理器生成高保真SBOM；第二阶段用函数调用分析提升漏洞报告的可用性。

Result: 高保真SBOM可作为安全分析的可靠基础；下游漏洞扫描存在极高误报，主因是不可达代码中的漏洞被错误标记；函数调用分析能够显著降低误报比例（约63.3%），从而显著提升报告的可操作性。

Conclusion: 提出并验证一个实用的两阶段SSC安全方案：第一阶段通过锁文件生成高保真SBOM；第二阶段通过函数调用分析丰富并净化漏洞报告，使开发者能够更有效地响应安全事件。

Abstract: The Software Bill of Materials (SBOM) is a critical tool for securing the software supply chain (SSC), but its practical utility is undermined by inaccuracies in both its generation and its application in vulnerability scanning. This paper presents a large-scale empirical study on 2,414 open-source repositories to address these issues from a practical standpoint. First, we demonstrate that using lock files with strong package managers enables the generation of accurate and consistent SBOMs, establishing a reliable foundation for security analysis. Using this high-fidelity foundation, however, we expose a more fundamental flaw in practice: downstream vulnerability scanners produce a staggering 97.5\% false positive rate. We pinpoint the primary cause as the flagging of vulnerabilities within unreachable code. We then demonstrate that function call analysis can effectively prune 63.3\% of these false alarms. Our work validates a practical, two-stage approach for SSC security: first, generate an accurate SBOM using lock files and strong package managers, and second, enrich it with function call analysis to produce actionable, low-noise vulnerability reports that alleviate developers' alert fatigue.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [14] [Latent-space metrics for Complex-Valued VAE out-of-distribution detection under radar clutter](https://arxiv.org/abs/2511.19805)
*Y. A. Rouzoumka,E. Terreaux,C. Morisseau,J. -P. Ovarlez,C. Ren*

Main category: eess.SP

TL;DR: 提出了基于复杂值变分自编码器（CVAE）的雷达OOD探测框架，提出CVAE-MSE、潜在分数（Mahalanobis、KLD），并与经典ANMF-Tyler探测器对比，在合成与实验雷达数据上评估各检测器的优劣与适用性。


<details>
  <summary>Details</summary>
Motivation: 在复杂雷达环境中进行鲁棒的ODD（Out-Of-Distribution）探测，传统检测器在分布偏离时性能下降。CVAE通过重构与潜在变量表达，可能提升对异常分布的检测能力，并希望在实际雷达数据中验证其可行性。

Method: 训练一个复杂值CVAE，对输入雷达信号进行重构，提出CVAE-MSE作为重构误差指标；并基于潜在变量分布计算Mahalanobis距离和Kullback-Leibler散度（KLD）作为额外分数；将其与经典ANMF-FP检测器在合成与实验雷达数据上进行对比评估。

Result: CVAE相关指标在某些情境下优于ANMF-FP，显示出对复杂环境的鲁棒性提升，但在其他场景下也存在劣势，表明单一指标并非通用最优，需要结合多种检测信号进行综合判断。

Conclusion: CVAE为雷达OOD探测提供了一种有潜力的补充工具，与传统检测器具有互补性。未来工作可聚焦于多指标融合、模型改进及对不同干扰条件的自适应性研究。

Abstract: We investigate complex-valued Variational AutoEncoders (CVAE) for radar Out-Of-Distribution (OOD) detection in complex radar environments. We proposed several detection metrics: the reconstruction error of CVAE (CVAE-MSE), the latent-based scores (Mahalanobis, Kullback-Leibler divergence (KLD)), and compared their performance against the classical ANMF-Tyler detector (ANMF-FP). The performance of all these detectors is analyzed on synthetic and experimental radar data, showing the advantages and the weaknesses of each detector.

</details>


### [15] [Parallel Delay-Doppler Estimation via Order-Reversed Two-Stage Prony Method](https://arxiv.org/abs/2511.19866)
*Yutaka Jitsumatsu,Liangchen Sun*

Main category: eess.SP

TL;DR: 提出了一种基于Prony的并行两阶段延时-多普勒估计方法，应用于OTFS系统。通过延时优先和多普勒优先的并行估计并融合结果，解决了类似路径特征造成的模糊性；仿真显示在多种条件下具有更高的精度与鲁棒性，并对V2V/ISAC等未来应用具潜在意义。


<details>
  <summary>Details</summary>
Motivation: 在OTFS信道中，延时-多普勒估计易受路径特征相似性影响，导致解模糊和精度下降，需要一种鲁棒且准确的估计方案以满足高动态与多径场景的需求，特别是面向V2V和ISAC等应用。

Method: 提出一种Prony基的并行两阶段估计框架：先进行延时优先估计与多普勒优先估计，并在并行阶段完成两路估计后进行融合以消除模糊性。该方法在OTFS系统的延时-多普勒域中实现，利用Prony系列参数提取实现高效的参数估计与融合。

Result: 通过仿真验证，该方法在各种条件下表现出比对比方法更高的精度与鲁棒性，能够有效降低路径特征相似带来的歧义。

Conclusion: 该工作为OTFS系统中的延时-多普勒估计提供了一种有效的解决思路，尤其在V2V及ISAC等未来应用场景中具有潜在的应用价值与扩展性。

Abstract: This paper proposes a Prony-based parallel two-stage method for delay-Doppler estimation in OTFS systems. By performing delay-first and Doppler-first estimations in parallel and fusing the results, the method resolves ambiguities caused by similar path characteristics. The simulation results demonstrate the superior accuracy and robustness of the proposed method under various conditions. This method provides a promising solution for future applications such as Vehicle-to-Vehicle (V2V) and Integrated Sensing and Communication (ISAC).

</details>


### [16] [AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload](https://arxiv.org/abs/2511.19943)
*Akash Doshi,Pinar Sen,Kirill Ivanov,Wei Yang,June Namgoong,Runxin Wang,Rachel Wang,Taesang Yoo,Jing Jiang,Tingfang Ji*

Main category: eess.SP

TL;DR: 面向 HARQ-ACK 非均匀分布的上行信道编码：基于 Transformer 的联合源通道编码、每码字功率整形和基于 Neyman-Pearson 的 NACK 保护，适用于 5G NR 上行，在衰落信道下实现较基线显著的功率节省。


<details>
  <summary>Details</summary>
Motivation: 考虑到上行 HARQ-ACK 位是非均匀分布，传统假设输入为均匀分布导致的资源浪费，提出利用源先验信息进行联合源通道编码以提升性能，并确保 NACK 的低误码率以避免重传失败，同时提升覆盖和功率效率。

Method: 提出基于 Transformer 的编码器，使用新颖的“free-lunch”训练算法；引入逐码字功率整形以利用源分布且对 HARQ-ACK 分布变化鲁棒；将 Neyman-Pearson 测试扩展到具有多信息比特的编码位系统，实现 ACK/NACK 的不等错误保护（UEP）；将设计应用于符合 5G NR 的上行无线场景，给出接收端的最优接收机和低复杂度相干近似。

Result: 与 NR 基线相比，在达到目标误码率的前提下，平均发送功率降低 3–6 dB；最大发送功率降低 2–3 dB，显著提升覆盖和功率效率。

Conclusion: 证明在 5G NR 上行衰落信道中，结合 DL 端到端的联合源通道编码、功率整形以及 UEP 机制，能够在显著降低平均和最大功率的同时维持目标误码率，具有实际部署潜力；未来工作包括在更广泛的场景下验证鲁棒性、降低复杂度并评估标准化影响。

Abstract: Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel "free-lunch" training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.

</details>


### [17] [Cross-Modal Semantic Communication for Heterogeneous Collaborative Perception](https://arxiv.org/abs/2511.20000)
*Mingyi Lu,Guowei Liu,Le Liang,Chongtao Guo,Hao Ye,Shi Jin*

Main category: eess.SP

TL;DR: A cross-modal semantic communication (CMSC) framework for collaborative perception in connected autonomous vehicles (CAVs) that maps heterogeneous sensor features into a unified semantic space, enabling robust encoding, transmission, and fusion; it yields stronger perception performance than existing methods, especially under low SNR.


<details>
  <summary>Details</summary>
Motivation: To overcome sensor heterogeneity across CAVs that hinders unified communication and effective information fusion in collaborative perception, aiming to improve robustness and overall perception performance.

Method: Transform heterogeneous perceptual features from different sensor modalities into a unified semantic space; perform encoding, transmission, and decoding entirely within this semantic space to enable seamless fusion of shared information across vehicles.

Result: CMSC achieves significantly stronger perception performance than existing methods, with especially notable gains in low signal-to-noise ratio (SNR) regimes.

Conclusion: CMSC provides a robust cross-modal semantic representation that facilitates inter-vehicle collaboration across diverse sensors, improving perception under challenging communication conditions; practical deployment will require careful design of semantic mappings, synchronization, and bandwidth considerations.

Abstract: Collaborative perception, an emerging paradigm in autonomous driving, has been introduced to mitigate the limitations of single-vehicle systems, such as limited sensor range and occlusion. To improve the robustness of inter-vehicle data sharing, semantic communication has recently further been integrated into collaborative perception systems to enhance overall performance. However, practical deployment of such systems is challenged by the heterogeneity of sensors across different connected autonomous vehicles (CAVs). This diversity in perceptual data complicates the design of a unified communication framework and impedes the effective fusion of shared information. To address this challenge, we propose a novel cross-modal semantic communication (CMSC) framework to facilitate effective collaboration among CAVs with disparate sensor configurations. Specifically, the framework first transforms heterogeneous perceptual features from different sensor modalities into a unified and standardized semantic space. Subsequently, encoding, transmission, and decoding are performed within this semantic space, enabling seamless and effective information fusion. Extensive experiments demonstrate that CMSC achieves significantly stronger perception performance than existing methods, particularly in low signal-to-noise ratio (SNR) regimes.

</details>


### [18] [Joint Bit-Partitioning and Modulation Design for Digital AirComp](https://arxiv.org/abs/2511.20113)
*Xiaojing Yan,Carlo Fischione*

Main category: eess.SP

TL;DR: 将输入比特划分为若干组并映射到单个调制符号，通过多时隙传输实现数字AirComp中的任意函数计算；提出uniform bit-partitioning和importance-adaptive bit-partitioning(IABP)两种方法，前者通过max-min优化并由CCCP求解一系列SOCP子问题，后者在比特位重要性基础上进行自适应分区与调制设计，外层用模拟退火优化分区，内层用CCCP求解调制；结果表明两种方法在噪声信道中鲁棒，IABP在乘积计算场景下对比Sequential Modulation for AirComp误差可降低最多5 dB。


<details>
  <summary>Details</summary>
Motivation: 在数字AirComp中的ChannelComp框架下，降低调制设计复杂度并提升计算鲁棒性，同时提升对噪声信道的鲁棒性与计算精度。

Method: 把输入比特序列分成若干组，每组映射到一个调制符号，通过多时隙传输实现所需的函数计算。uniform bit-partitioning：比特均匀分布，采用max-min优化，CCCP求解一系列SOCP子问题。IABP：比特位的重要性决定分配，调制与分区联合优化，外层用 simulated annealing 更新分区，内层用 CCCP 求解调制设计。

Result: 两种方法在噪声信道下均显示鲁棒性；IABP相较于Sequential Modulation for AirComp在计算误差方面提升显著，达峰值约5 dB，尤其在乘积计算场景。

Conclusion: 引入的比特分区调制设计有效降低调制设计复杂度并提升AirComp的计算鲁棒性，IABP在需要对比度较高的乘积计算中具有更优表现，验证了位分区的ChannelComp框架的可行性。

Abstract: For digital over-the-air computation, the ChannelComp framework has recently been proposed to design digital modulations to compute any arbitrary function over a multiple access channel. To reduce modulation design complexity while increasing computation reliability, this paper integrates a bit-partitioning procedure into ChannelComp. The key process is to partition the input bit sequence into several groups, map each group to a single modulation symbol and transmit the encoded symbol sequence across multiple time slots. With the objective to maximize a worst-case constellation distance, we develop two bit-partitioning methods. In uniform bit-partitioning, bits are evenly distributed across groups and modulation is designed via a max-min optimization, which is handled by a CCCP that solves a sequence of second-order cone programming subproblems. In importance-adaptive bit-partitioning (IABP), the bit allocation is adapted to the significance of individual bit positions, and the modulation and partitioning are jointly optimized. To keep the overall complexity manageable, simulated annealing is employed in the outer loop to update the partitioning, while a CCCP-based solver is used in the inner loop for modulation design. Numerical results show that both methods provide robust computation in noisy channels, and IABP achieves up to a 5 dB reduction in computation error compared to Sequential Modulation for AirComp, especially for product computation.

</details>


### [19] [Optimal Waveform Design for Continuous Aperture Array (CAPA)-aided ISAC Systems](https://arxiv.org/abs/2511.20203)
*Junjie Ye,Zhaolin Wang,Yuanwei Liu,Peichang Zhang,Lei Huang,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: 提出一种基于连续孔径阵列(CAPA)的ISAC框架，设计最优连续波形以实现指向性目标感知的波束模式并抑制多用户干扰（MUI）。通过Green函数推导CAPA定向波束，基于波数域优化获得参考感知波形，并在此基础上对感知波束与MUI之间的权衡进行加权泛函规划，利用拉格朗日变换与变分法得到最优波形结构，拉格朗日乘子通过二分法给出。数值结果表明CAPA相较于传统离散阵列在感知精度与通信可靠性方面具有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决在联合感知和通信（ISAC）场景中，离散阵列的波束形成对感知与干扰抑制的限制，提出利用连续孔径阵列以提升感知与通信的综合性能。

Method: 1) 基于Green函数推导CAPA的定向波束模式；2) 通过波数域优化得到参考感知波形；3) 在参考波形基础上建立对感知波束失配与MUI之间权衡的加权泛函规划；4) 采用拉格朗日变换与变分法推导最优波形结构，并用二分法求解相应的拉格朗日乘子；5) 分析最优波形对参考波形、信道相关与信道符号相关性的影响。

Result: 得到的最优波形结构表明波形受参考感知波形、信道相关性以及信道-符号相关性共同影响。数值结果验证了所提CAPA在感知准确性和通信可靠性方面对比离散阵列ISAC的显著提升。

Conclusion: 提出的CAPA-ISAC总体设计与最优波形推导为连续孔径阵列在ISAC中的应用提供了有力理论与实验依据，显示出在多目标感知和多用户干扰抑制方面的潜在优势。

Abstract: A novel continuous-aperture-array (CAPA)-aided integrated sensing and communication (ISAC) framework is proposed. Specifically, an optimal continuous ISAC waveform is designed to form a directive beampattern for multi-target sensing while suppressing the multi-user interference (MUI). To achieve the goal of optimal waveform design, the directional beampattern of CAPA is first derived based on Green's function, whereafter a reference sensing waveform is obtained through wavenumber-domain optimization. Based on the reference sensing waveform, a weighted functional programming on the tradeoff between sensing beampattern mismatch and MUI is formulated. To solve the resulting problem, an optimal CAPA-ISAC waveform structure is analytically derived using a Lagrangian-transformation and calculus-of-variations method, where the Lagrangian multiplier associated with the optimal waveform structure is determined via Bisection search. The obtained optimal waveform reveals that it is concurrently affected by the reference sensing waveform, the channel correlations and the channel-symbol correlations. Finally, numerical results validate the effectiveness of the proposed system and waveform design, demonstrating that CAPA can achieve significant performance gains against the ISAC designs based on conventional spatially discrete array in both sensing accuracy and communication reliability.

</details>


### [20] [Rectified Flow for Vision-Aided mmWave V2I Beam Prediction](https://arxiv.org/abs/2511.20265)
*Can Zheng,Jiguang He,Chung G. Kang,Guofa Cai,Chongwen Huang,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出基于 rectified flow 的流匹配框架，用于 V2I 视觉辅助波束预测；通过 ODE 向量场学习连续潜在流，实现平滑轨迹和快速采样；引入终端流约束以提升全局一致性并稳定长时预测；在 top-K 精度上显著优于 RNN/LSTM 基线，接近大语言模型方法，并在 GPU/CPU 上分别实现约 10x 的推理加速。


<details>
  <summary>Details</summary>
Motivation: V2I 场景下需要高精度、低延迟的波束预测。将离散波束索引序列建模带来不稳定性与复杂性，连续潜在流与 ODE 向量场为预测提供更平滑、可控的轨迹，同时快速采样以提升实时性；终端约束确保在有限步积分下仍具全局一致性，改善长期预测稳定性。

Method: 提出基于 rectified flow 的流匹配框架。将视觉信息映射到潜在流，通过一个由常微分方程描述的向量场来驱动该流；使用终端流约束在有限步积分下实现全局一致性，并提高稳定性；训练以提升 top-K 精度与推理速度。

Result: 与 RNN、LSTM 基线相比，FM 能显著提升 top-K 精度；接近大语言模型方法的性能；在相同 GPU 上实现约 10x 的推理加速，在 CPU 上实现约 10^4x 的加速。

Conclusion: 基于 rectified flow 的流匹配框架在视觉辅助波束预测中提供了稳定且高效的推理，克服离散序列建模的局限，具备良好的长时稳定性与可扩展性，适合对实时性要求较高的 V2I 应用。

Abstract: This paper proposes a flow matching (FM) framework based on rectified flow for vision-aided beam prediction in vehicle-to-infrastructure (V2I) links. Instead of modeling discrete beam index sequences, the method learns a continuous latent flow governed by an ordinary differential equation (ODE)-based vector field, enabling smooth beam trajectories and fast sampling. A terminal flow constraint enforces global consistency under finite-step integration, stabilizing long-term prediction. The resulting FM-based model significantly improves top-K accuracy over RNN and LSTM baselines, approaches the performance of large language model-based approaches, and achieves inference speedups on the order of 10 x and 10^4 x on identical GPU and CPU deployments, respectively.

</details>


### [21] [Log-Mu Fading Process: Second-Order Statistics for Diversity-Combining Techniques](https://arxiv.org/abs/2511.20298)
*Godfred Kumi Tenkorang,Michel Daoud Yacoub*

Main category: eess.SP

TL;DR: 本文推导了对数mu衰落信道下的多路分集技术的二阶统计量。对纯选择并行（PSC）给出级交率（LCR）和平均衰落持续时间（AFD）的闭式表达；对等增益并行（EGC）和最大比合并（MRC）给出确切的多维积分表达式。考虑M个不平衡、独立且非同分布(i.n.i.d.)的对数mu衰落信道。用蒙特卡洛仿真验证理论结果，结果与仿真一致，验证了所提表达式的准确性。


<details>
  <summary>Details</summary>
Motivation: 在对数mu衰落信道中获取多路分集系统的二阶统计量，以便评估信道随时间的波动对系统性能的影响。

Method: 导出LCR和AFD；PSC得到闭式表达；EGC和MRC得到确切的多维积分表达；分析对象是M个不平衡、独立且非同分布的对数mu衰落信道；用蒙特卡洛仿真来验证理论推导。

Result: 给出闭式LCR/AFD（PSC）以及EGC/MRC的多维积分表达式；仿真验证与理论高度一致。

Conclusion: 所提解析表达式可准确描述对数mu衰落信道下的二阶统计量，并通过仿真证实，适用于评估和设计采用PSC/EGC/MRC的分集系统在此衰落模型中的性能。

Abstract: This paper derives second-order statistics for diversity-combining techniques over Log-mu fading channels. Closed-form expressions for the level crossing rate (LCR) and average fading duration (AFD) are derived for pure selection combining (PSC), while exact multidimensional integral expressions are obtained for equal gain combining (EGC) and maximal ratio combining (MRC). The analysis considers M unbalanced, independent, and non-identically distributed (i.n.i.d.) Log-mu fading channels. Monte Carlo simulations are conducted to validate the theoretical results, demonstrating excellent agreement and confirming the accuracy of the proposed expressions.

</details>


### [22] [Digital Twin-Assisted High-Precision Massive MIMO Localization in Urban Canyons](https://arxiv.org/abs/2511.20453)
*Ziqin Zhou,Hui Chen,Gerhard Steinböck,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出一个基于数字孪生（DT）与RANSAC的三阶段鲁棒定位算法，在城市峡谷场景中通过将NLOS信息转化为几何信息实现高精度定位并降低部署成本。


<details>
  <summary>Details</summary>
Motivation: 在城市高楼环境中，测量噪声和严重的NLOS导致传统定位难以可靠；需要利用非 LOS 及多径信息来提升定位鲁棒性并降低对直接 LOS 的依赖。

Method: 三阶段流程：1) 用DT进行几何路径关联；2) 用RANSAC筛选出LOS和单次单次NLOS路径，排除多跳多径离群；3) 在内点集上进行优化，估计位置和时钟偏置。

Result: 仿真验证该DT驱动的几何信息化处理使NLOS路径成为有价值的几何信息，提升定位精度，降低对直接LOS的依赖，并降低系统部署成本。

Conclusion: 该方法在实用部署方面具有潜在效益，能够提升在复杂城市环境中的定位鲁棒性并降低成本。

Abstract: High-precision wireless localization in urban canyons is challenged by noisy measurements and severe non-line-of-sight (NLOS) propagation. This paper proposes a robust three-stage algorithm synergizing a digital twin (DT) model with the random sample consensus (RANSAC) algorithm to overcome these limitations. The method leverages the DT for geometric path association and employs RANSAC to identify reliable line-of-sight (LOS) and single-bounce NLOS paths while rejecting multi-bounce outliers. A final optimization on the resulting inlier set estimates the user's position and clock bias. Simulations validate that by effectively turning NLOS paths into valuable geometric information via the DT, the approach enables accurate localization, reduces reliance on direct LOS, and significantly lowers system deployment costs, making it suitable for practical deployment.

</details>


### [23] [Near-Field Multipath MIMO Channels: Modeling Reflectors and Exploiting NLOS Paths](https://arxiv.org/abs/2511.20572)
*Mohamadreza Delbari,George C. Alexandropoulos,Robert Schober,H. Vincent Poor,Vahid Jamali*

Main category: eess.SP

TL;DR: 提出并验证一个扩展的近场MIMO信道模型，考虑大尺寸反射面的不完美反射，揭示NLOS分量在近场多用户MIMO中的关键性，单靠LOS不足以实现多路复用。


<details>
  <summary>Details</summary>
Motivation: 近场通信在大尺度MIMO/大物理天线场景中具有潜在性能提升，但现有NLOS近场模型多基于点散射假设，不能真实反映大面积反射面的影响，因此需要更真实的统计模型。

Method: 提出广义统计近场MIMO信道模型，扩展点散射框架以考虑对大面积反射面的不完美反射；利用该模型分析反射面的物理特性对近场信道的影响；在多用户场景给出解析结论并通过仿真验证。

Result: 仿真和分析结果表明NLOS分量贡献在多数现实条件下不容忽视，能显著影响近场MIMO的多路复用能力；模型准确性得到验证。

Conclusion: 近场大面积MIMO系统设计应同时考虑NLOS分量；忽略它们可能导致错估性能，未来工作可扩展到其他参数如极化、频率依赖等。

Abstract: Near-field (NF) communications is receiving renewed interest in the context of multiple-input multiple-output (MIMO) systems involving large physical apertures with respect to the signal wavelength. While line-of-sight (LOS) links are typically expected to dominate in NF scenarios, the impact of non-LOS (NLOS) components at both in centimeter- and millimeter-wave frequencies may be in general non-negligible. Moreover, although weaker than the LOS path, NLOS links may be essential for achieving multiplexing gains in MIMO systems. The commonly used NF channel models for NLOS links in the literature are based on the point scattering assumption, which is not valid for large reflectors such as walls, ceilings, and the ground. In this paper, we develop a generalized statistical NF MIMO channel model that extends the widely adopted point scattering framework to account for imperfect reflections from large surfaces. This model is then leveraged to investigate how the physical characteristics of these reflectors influence the resulting NF MIMO channel. In addition, using the proposed channel model, we analytically demonstrate for a multi-user scenario that, even when users are located within the NF regime, relying solely on LOS NF links may be insufficient to achieve multiplexing gains, thus exploiting NLOS links becomes essential. Our simulation results validate the accuracy of the proposed model and show that, in many practical settings, the contribution of NLOS components is non-negligible and must be carefully accounted for in the system design.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [24] [Power sector models featuring individual BEV profiles: Assessing the time-accuracy trade-off](https://arxiv.org/abs/2511.19449)
*Adeline Guéret*

Main category: eess.SY

TL;DR: 对于大规模电动汽车车队的电力系统建模，使用若干个具体的 BEV 配置档比单一聚合档能保留信息；存在“档位阈值”：太少会扭曲最优解，超过某阈值收益递减。给出实用规则：对于 500 万到 2000 万级别的 BEV 车队，每个档位覆盖约 20 万到 25 万辆车，既能保Accuracy又控制运行时间。


<details>
  <summary>Details</summary>
Motivation: 随着电动化广泛推进，跨部门耦合（sector coupling）的建模需要更细粒度的 BEV 表征以避免聚合带来的信息损失；研究应回答：需要多少个档位来在准确性和计算开销之间取得平衡。

Method: 在包含逐个 BEV 配置档的模型框架中，与仅用一个聚合 BEV 配置档的基线进行对比，考察不同档位数量对结果的影响；评估是否选择特定子集的 BEV 配置会改变最优解；以车队规模在 5–20 百万级别的情景进行实验。

Result: 发现过少的 BEV 配置会导致最优解失真；但达到某一阈值后，增加更多配置对鲁棒性提升的收益显著下降。给出一个经验法则：对于 5–20 百万辆的车队，确保每个配置档覆盖约 20–25 万辆车，能在保持结果准确性的同时控制运行时。

Conclusion: 在大规模车队的 sector coupling 模型中，采用每档 20–25 万车辆的分档策略可在准确性与计算成本之间取得良好平衡，为实务分析提供可操作的指南。

Abstract: Electrifying passenger cars will impact future power systems. To understand the challenges and opportunities that arise, it is necessary to reflect "sector coupling" in the modeling space. This paper focuses on a specific modeling approach that includes dozens of individual BEV profiles rather than one aggregated BEV profile. Although including additional BEV profiles increases model complexity and runtime, it avoids losing information in the aggregation process. We investigate how many profiles are needed to ensure the accuracy of the results and the extent to which fewer profiles can be traded for runtime efficiency gains. We also examine whether selecting specific profiles influences optimal results. We demonstrate that including too few profiles may result in distorted optimal solutions. However, beyond a certain threshold, adding more profiles does not significantly enhance the robustness of the results. More generally, for fleets of 5 to 20 million BEVs, we derive a rule of thumb consisting in including enough profiles such that each profile represents 200,000 to 250,000 vehicles, ensuring accurate results without excessive runtime.

</details>


### [25] [Strong Duality and Dual Ascent Approach to Continuous-Time Chance-Constrained Stochastic Optimal Control](https://arxiv.org/abs/2511.19451)
*Apurva Patil,Alfredo Duarte,Fabrizio Bisetti,Takashi Tanaka*

Main category: eess.SY

TL;DR: 提出基于强对偶的连续时间连续空间机会约束随机最优控制框架。通过退出时间将机会约束变为指标函数的期望，结合路径积分方法以梯度上升求解对偶问题，并用开环轨迹样本进行数值实现；在机器人导航任务中给出数值对比。


<details>
  <summary>Details</summary>
Motivation: 在连续时间/连续空间的随机最优控制问题中引入明确的失败概率约束，而不采用保守近似；利用退出时间的性质以及对偶性建立一个可数值求解的框架，提升对约束的处理效率与准确性。

Method: 将机会约束通过退出时间的框架表达；将约束转化为目标函数中指标函数的期望，并通过对偶化将其写成对偶问题；对偶函数由参数化的HJB偏微分方程解表示；在给定系统动力学和成本的假设下证明原问题与对偶问题存在强对偶性；采用路径积分方法通过梯度上升法，结合开环轨迹采样来数值求解对偶问题；在基于空间导航的移动机器人运动规划中进行仿真，并将路径积分解与有限差分法进行比较。

Result: 理论层面：建立了在特定条件下原始机会约束问题与对偶问题之间的强对偶性；数值层面：路径积分法可通过梯度上升和开环样本实现对偶问题的求解，且在仿真中与有限差分法进行对比。

Conclusion: 给出一种将连续时间/空间的机会约束问题以对偶-路径积分的组合方式求解的系统框架，避免保守近似并提供可数值实现，对移动机器人等场景的机会约束最优控制具有应用价值。

Abstract: The paper addresses a continuous-time continuous-space chance-constrained stochastic optimal control (SOC) problem where the probability of failure to satisfy given state constraints is explicitly bounded. We leverage the notion of exit time from continuous-time stochastic calculus to formulate a chance-constrained SOC problem. Without any conservative approximation, the chance constraint is transformed into an expectation of an indicator function which can be incorporated into the cost function by considering a dual formulation. We then express the dual function in terms of the solution to a Hamilton-Jacobi-Bellman partial differential equation parameterized by the dual variable. Under a certain assumption on the system dynamics and cost function, it is shown that a strong duality holds between the primal chance-constrained problem and its dual. The Path integral approach is utilized to numerically solve the dual problem via gradient ascent using open-loop samples of system trajectories. We present simulation studies on chance-constrained motion planning for spatial navigation of mobile robots and the solution of the path integral approach is compared with that of the finite difference method.

</details>


### [26] [State Feedback Controllers with Operational Constraints](https://arxiv.org/abs/2511.19683)
*Eugene Lavretsky*

Main category: eess.SY

TL;DR: 提出一种用于多输入多输出线性时不变系统的状态反馈控制器设计，在静态/动态伺服控制器中对输入/输出施加最小/最大约束，包含反 windup，核心基于 Nagumo 定理、比较引理和最小范数控制器，方法与控制障碍函数相关，通过仿真在航空航天场景验证。


<details>
  <summary>Details</summary>
Motivation: 在伺服跟踪问题中需要确保输入输出约束的安全性与可实现性，尤其在航空航天等关键系统中，需在保持性能的同时满足前向不变性与抗饱和能力。

Method: 基于 Nagumo 前向不变性定理、输入/输出约束包含的比较引理以及最小范数最优控制器，推导静态与动态伺服控制器的约束设计；静态控制器对选定输出的分量施加与控制输入同维度的输出限制；动态控制器对系统输入/输出施加约束并引入带积分的 anti-windup 逻辑；与控制障碍函数（CBF）的方法直接相关。

Result: 通过仿真实验对比研究，展示所提约束控制在实现约束同时提升系统性能的优势，特别针对空天等高危飞行控制系统。

Conclusion: 给出一个系统化的受约束伺服控制设计框架，适用于多输入多输出线性系统，具有抗饱和的能力和与 CBF 的关系，为安全关键系统提供可行的实现路径。

Abstract: In this paper, a state feedback control design with min/max operational limiting constraints is developed for multi-input-multi-output linear time invariant systems. Specifically, servo-tracking control problems with input and output constraints are considered. For static servo-controllers, the output design limits are imposed component-wise on the system selected output, which is of the same dimension as the control input. For dynamic servo-controllers, operational constraints are applied to the system inputs and outputs. The proposed control solution also includes an anti-windup protection logic for dynamic servo-controllers with integral action. The developed method is based on the Nagumo Theorem for forward invariance, the Comparison Lemma for inclusion of input/output inequality constraints, and on the min-norm optimal controllers for synthesis. The derived design is similar and directly related to the method of Control Barrier Functions. Simulation trade studies are presented to illustrate benefits of the proposed control methodology for aerial flight critical systems.

</details>


### [27] [Understanding Risk and Revenue in the Nordic 15-minute mFRR market: An EV Aggregation Study](https://arxiv.org/abs/2511.19715)
*Theodor Hagström,Lars Herre*

Main category: eess.SY

TL;DR: 对聚合电动汽车车队参与北欧15分钟mFRR能源激活市场的风险意识型协同优化框架进行分析。将车队作为具有时变功率与能量包络的虚拟电池建模，通过对日调度与季度量化的mFRR竞价进行风险控制的随机优化。结果表明：与独立日调度基线相比，协同优化在两种价格情景下提升期望利润并降低下行风险，具体表现为：减少日前购电、向mFRR下侧采购转移、并使充电计划趋向平滑以维持mFRR上行资格。盈利提升来自更高的mFRR下收入及对日前头寸的减 unwind。本文还讨论了 bidding 的运营含义，并提出两种扩展：滚动45分钟再优化和V2G框架。


<details>
  <summary>Details</summary>
Motivation: 随着 decarbonisation、分散化和间歇性对电力系统的影响，市场时间单位（MTU）缩短、关口下放，降低了需求侧聚合商的准入门槛。EV车队作为关键灵活性资源，与日调度和快速调峰市场的耦合在提升系统 granularity 与风险管理方面具有重要意义。

Method: 将车队灵活性表示为具有时变功率和能量包络的虚拟电池，建立以CVaR为目标的风险感知随机优化模型，联合日前调度与季度mFRR竞价的协同决策。以合成家庭充电样本和两天的日前价格为数据基础，在两种价格情景下比较独立日调基线与对比的协同优化（在保守可用性约束下）及带CVaR目标的方案。进一步做利润分解以分析收益来源。

Result: 协同优化在两种价格情景下都提高了期望利润并降低下行风险；模型在日前购电量上做出更保守的选择，增加对mFRR下的采购并通过充电计划的平滑化来维持mFRR上行的资格。利润提升来源于更高的mFRR下收入以及对日前头寸 unwind 的降低。

Conclusion: 结果支持对EV车队参与短MTU灵活性市场的协同优化策略的有效性，并就 bidding 的操作性提出影响。未来扩展包括滚动45分钟再优化以及V2G框架，以进一步提升盈利与风险控制能力。

Abstract: Decarbonisation, decentralisation, and intermittency are driving the development of flexibility markets towards shorter market time units (MTU). Shorter MTUs and shorter gate closures lower the entrance barriers of demand side aggregators that face significant uncertainty on longer time scales. We study the business case for aggregated EV fleets participating in the Nordic 15-minute mFRR Energy Activation Market (EAM). Motivated by increasing system granularity and rapid EV uptake, we represent fleet flexibility as a virtual battery with time-varying power and energy envelopes and formulate a risk-aware stochastic optimisation that co-ordinates day-ahead scheduling with quarter-hour mFRR bidding. Using synthetic residential charging cohorts and observed day-ahead prices on two stylised days, we compare an independent day-ahead baseline to a co-optimised strategy under conservative availability and a CVaR-augmented objective. Across both price cases, co-optimisation increases expected profit and lowers downside risk: the model buys less energy day-ahead and shifts procurement toward mFRR down while flattening the charging plan to retain eligibility for mFRR up. Profit decomposition shows that the uplift is driven by higher mFRR down revenues and reduced reliance on unwinding day-ahead positions. We discuss operational implications for bidding and outline two extensions: rolling 45-minute re-optimisation and a V2G framework.

</details>


### [28] [Multi-Hypotheses Ego-Tracking for Resilient Navigation](https://arxiv.org/abs/2511.19770)
*Peter Iwer Hoedt Karstensen,Roberto Galeazzi*

Main category: eess.SY

TL;DR: 提出一种鲁棒导航体系，通过多假设估计与泊松二项窗计数检测实现异常识别与隔离；以状态机协调运行/诊断/缓解，并在攻击发生时利用微分平直性进行轨迹重规划以降低性能损失；通过案例研究展示对偏置传感器的有效检测、状态估计的维持及在持续欺骗攻击下恢复到标称操作。


<details>
  <summary>Details</summary>
Motivation: RF定位系统易受 spoofing、传感器操控的影响，亟需在对抗条件下保持导航可用性与鲁棒性。

Method: 将多假设估计、泊松二项式窗计数检测结合，进行异常识别与隔离；用状态机在正常、诊断、缓解之间切换实现自适应响应；在检测到攻击时，基于微分平直性实现轨迹重规划以进行信息收集并最小化性能损失。

Result: 通过案例研究，证明能够有效检测偏置传感器、维持状态估计的连贯性，并在持续的欺骗攻击下恢复到名义操作。

Conclusion: 提出的鲁棒导航框架在对抗性条件下提高传感器冗余利用、快速隔离故障、以及执行信息获取行动，从而实现对鲁棒性的综合增强。

Abstract: Autonomous robots relying on radio frequency (RF)-based localization such as global navigation satellite system (GNSS), ultra-wide band (UWB), and 5G integrated sensing and communication (ISAC) are vul- nerable to spoofing and sensor manipulation. This paper presents a resilient navigation architecture that combines multi-hypothesis estimation with a Poisson binomial windowed-count detector for anomaly identi- fication and isolation. A state machine coordinates transitions between operation, diagnosis, and mitigation, enabling adaptive response to adversarial conditions. When attacks are detected, trajectory re-planning based on differential flatness allows information-gathering maneuvers minimizing performance loss. Case studies demonstrate effective detection of biased sensors, maintenance of state estimation, and recovery of nominal operation under persistent spoofing attacks

</details>


### [29] [Occlusion-Aware Multi-Object Tracking via Expected Probability of Detection](https://arxiv.org/abs/2511.20239)
*Jan Krejčí,Oliver Kost,Yuxuan Xia,Lennart Svensson,Ondřej Straka*

Main category: eess.SY

TL;DR: 提出了一种在多目标场景中考虑遮挡的检测概率建模，并将其整合到 MBM 拟合跟踪中，通过对每个对象在存在性条件下的减少 Palm 密度期望来估计可见性与检测概率。


<details>
  <summary>Details</summary>
Motivation: 在传感器检测中，目标之间的遮挡导致检测概率相互依赖，传统点对象模型忽略该依赖，导致跟踪不准确。需要一个可计算且清晰的框架来描述对象集合对单个对象检测的影响。

Method: 通过在存在性条件下对对象的检测概率进行期望，利用 reduced Palm density 表示对其他对象存在性的条件化影响；并在多伯努利混合（MBM）滤波器（带标记）中实现该概率的自适应分配，形成一个对所有对象不确定性进行系统建模的跟踪框架。

Result: 以可视化追踪为应用场景，展示了该方法在遮挡情况下对检测概率的适配性及对跟踪鲁棒性的提升，相对于传统模型具有更清晰的误差源分解。

Conclusion: 将对象遮挡影响系统地纳入检测概率建模，提供一个清晰、可管理的多对象数据关联与跟踪框架，具有普适性和扩展性。

Abstract: This paper addresses multi-object systems, where objects may occlude one another relative to the sensor. The standard point-object model for detection-based sensors is enhanced so that the probability of detection considers the presence of all objects. A principled tracking method is derived, assigning each object an expected probability of detection, where the expectation is taken over the reduced Palm density, which means conditionally on the object's existence. The assigned probability thus considers the object's visibility relative to the sensor, under the presence of other objects. Unlike existing methods, the proposed method systematically accounts for uncertainties related to all objects in a clear and manageable way. The method is demonstrated through a visual tracking application using the multi-Bernoulli mixture (MBM) filter with marks.

</details>


### [30] [LLM-Driven Transient Stability Assessment: From Automated Simulation to Neural Architecture Design](https://arxiv.org/abs/2511.20276)
*Lianzhe Hu,Yu Wang,Bikash Pal*

Main category: eess.SY

TL;DR: An LLM-driven, end-to-end workflow automates TSA using LLMs and a professional simulator (ANDES) to generate disturbance scenarios and autonomously design TSA models via performance-guided feedback. On IEEE 39-bus, LLM-NND achieves 93.71% accuracy on four-class TSA with 4.78M parameters and <0.95 ms latency, outperforming a manually designed DenseNet (80.05% with 25.9M params). Ablations show synergy of retrieval, reasoning, and feedback. The approach promises scalable automation extendable to OPF, fault analysis, and market operations.


<details>
  <summary>Details</summary>
Motivation: Power system transient stability assessment (TSA) suffers from limited automation and inadequate intelligence in scenario generation, model design, and interpretation. There is a need for end-to-end, scalable automation that can convert natural-language inputs into disturbance scenarios and optimally design lightweight models that meet real-time requirements.

Method: Proposes an LLM-driven agentic framework that (1) uses LLMs with a professional simulator (ANDES) to automatically extract and filter disturbance scenarios from natural language, and (2) introduces an LLM-driven Neural Network Design (LLM-NND) pipeline that iteratively designs and optimizes TSA models via performance-guided, closed-loop feedback. Key components include domain-grounded retrieval, reasoning augmentation, and feedback mechanisms to ensure robust automation.

Result: Empirical evaluation on the IEEE 39-bus system shows LLM-NND achieving 93.71% test accuracy for four-class TSA with 4.78M parameters and real-time inference latency (<0.95 ms/sample). The baseline DenseNet (25.9M parameters) achieves 80.05% accuracy. Ablation studies confirm the necessity of synergy among domain-grounded retrieval, reasoning augmentation, and feedback for robustness.

Conclusion: LLM agents can reliably accelerate TSA research from scenario generation and data acquisition to model design and interpretation, offering a scalable paradigm extendable to other power system tasks such as optimal power flow, fault analysis, and market operations.

Abstract: This paper presents an LLM-driven, end-to-end workflow that addresses the lack of automation and intelligence in power system transient stability assessment (TSA). The proposed agentic framework integrates large language models (LLMs) with a professional simulator (ANDES) to automatically generate and filter disturbance scenarios from natural language, and employs an LLM-driven Neural Network Design (LLM-NND) pipeline to autonomously design and optimize TSA models through performance-guided, closed-loop feedback. On the IEEE 39-bus system, the LLM-NND models achieve 93.71% test accuracy on four-class TSA with only 4.78M parameters, while maintaining real-time inference latency (less than 0.95 ms per sample). Compared with a manually designed DenseNet (25.9M parameters, 80.05% accuracy), the proposed approach jointly improves accuracy and efficiency. Ablation studies confirm that the synergy among domain-grounded retrieval, reasoning augmentation, and feedback mechanisms is essential for robust automation. The results demonstrate that LLM agents can reliably accelerate TSA research from scenario generation and data acquisition to model design and interpretation, offering a scalable paradigm that is readily extensible to other power system tasks such as optimal power flow, fault analysis, and market operations.

</details>


### [31] [SAFE-IMM: Robust and Lightweight Radar-Based Object Tracking on Mobile Platforms](https://arxiv.org/abs/2511.20294)
*Dnyandeep Mandaokar,Bernhard Rinner*

Main category: eess.SY

TL;DR: SAFE-IMM 是一种轻量级的 IMM 变体，结合协方差感知门控，在混合模型跳跃时仅在理论上有界的前提下才启用 WTA，旨在资源受限平台上的实时跟踪。实验表明在 nuScenes 前端雷达数据上具有高精度、低 ID 切换率、鲁棒性与实时性。


<details>
  <summary>Details</summary>
Motivation: 解决在 maneuvering 目标跟踪中，传统 IMM 基于高斯混合的融合在机动期间响应慢，以及 WTA 方法虽然快速但可能导致跟踪不连续的问题；需要一种在响应性、平滑性和鲁棒性之间取得实际平衡的轻量化方案。

Method: 提出 SAFE-IMM：一种带有安全协方差感知门控的 IMM 变体。仅在从混合到获胜者的“跳跃”被证明有界时，才允许使用 wta 策略。方法强调简单整合、数值稳定且对杂波鲁棒，便于在资源受限的平台实现。实验在 nuScenes 前雷达数据和仿真中验证。

Result: 在实时率下实现高精度跟踪，显著减少 ID 切换，同时保持良好性能，与现有方法具备可比甚至更优的鲁棒性。

Conclusion: SAFE-IMM 在保持对响应性和光滑性之间的平衡方面提供了一个简单、稳定、易集成的方案，适用于资源受限的移动平台和真实场景跟踪。

Abstract: Tracking maneuvering targets requires estimators that are both responsive and robust. Interacting Multiple Model (IMM) filters are a standard tracking approach, but fusing models via Gaussian mixtures can lag during maneuvers. Recent winnertakes-all (WTA) approaches react quickly but may produce discontinuities. We propose SAFE-IMM, a lightweight IMM variant for tracking on mobile and resource-limited platforms with a safe covariance-aware gate that permits WTA only when the implied jump from the mixture to the winner is provably bounded. In simulations and on nuScenes front-radar data, SAFE-IMM achieves high accuracy at real-time rates, reducing ID switches while maintaining competitive performance. The method is simple to integrate, numerically stable, and clutter-robust, offering a practical balance between responsiveness and smoothness.

</details>


### [32] [Adaptive Meshing for CPA Lyapunov Function Synthesis](https://arxiv.org/abs/2511.20443)
*Amy K. Strong,Samuel Akinwande,Leila Bridgeman*

Main category: eess.SY

TL;DR: 提出三种网格化策略来提高 CPA-Lyapunov 函数综合的计算效率，并在二维与三维非线性系统上比较其有效性。


<details>
  <summary>Details</summary>
Motivation: 在非线性系统的 CPA-Lyapunov 综合中，随着网格细化，线性规划求解成本显著增加，尤其在高维情形。需要更高效的网格构建方法以在保持判定能力的同时降低计算量。

Method: 提出三种网格化方法：自适应网格、利用系统模型的网格化，以及二者的组合；在二维和三维非线性动力系统上通过数值实验比较它们的效果。

Result: 通过对两维与三维非线性系统的数值示例比较三种方法的效用，评估了在保持可行性前提下的计算效率改进与权衡。

Conclusion: 结果表明通过更智慧的网格化策略可以降低 CPA-Lyapunov 综合的计算成本，同时不同策略在准确性与计算负担之间存在权衡，组合方法可能提供更优的综合表现。

Abstract: Continuous piecewise affine (CPA) Lyapunov function synthesis is one method to perform Lyapunov stability analysis for nonlinear systems. This method first generates a mesh over the region of interest in the system's state space and then solves a linear program (LP), which enforces constraints on each vertex of the mesh, to synthesize a Lyapunov function. Finer meshes broaden the class of Lyapunov function candidates, but CPA function synthesis is more computationally expensive for finer meshes -- particularly so in higher dimensional systems. This paper explores methods to mesh the region of interest more efficiently so that a Lyapunov function can be synthesized using less computational effort. Three methods are explored -- adaptive meshing, meshing using knowledge of the system model, and a combination of the two. Numerical examples for two and three dimensional nonlinear dynamical systems are used to compare the efficacy of the three methods.

</details>


### [33] [Learning Control Barrier Functions with Deterministic Safety Guarantees](https://arxiv.org/abs/2511.20463)
*Amy K. Strong,Ali Kashani,Claus Danielson,Leila Bridgeman*

Main category: eess.SY

TL;DR: A data-driven design of barrier functions using ReLU networks to obtain deterministically safe, control-invariant sets for discrete-time Lipschitz systems, via a continuous piecewise affine barrier and iterative convex overbounding (ICO) optimization, validated on 2D autonomous and non-autonomous dynamics.


<details>
  <summary>Details</summary>
Motivation: Safety-critical control requires guarantees that system trajectories stay within safe sets despite nonlinear and potentially unmodeled dynamics. Barrier functions provide invariance, but computing valid BFs for nonlinear/non-autonomous systems from data with deterministic guarantees is challenging.

Method: Construct continuous piecewise affine barrier functions using ReLU neural networks. Introduce a classifier term to yield a relaxed barrier condition. Formulate a data-driven constrained optimization problem and solve it via iterative convex overbounding (ICO) which reduces nonconvex steps to a sequence of convex problems using sampled one-step trajectories.

Result: Demonstrates the method on two-dimensional autonomous and non-autonomous dynamical systems, showing effective design of BFs with deterministic safety guarantees.

Conclusion: The approach offers a practical, data-driven framework for designing CPA barrier functions with deterministic safety guarantees for Lipschitz, discrete-time systems, leveraging ICO to handle nonconvex optimization and data-driven constraints.

Abstract: Barrier functions (BFs) characterize safe sets of dynamical systems, where hard constraints are never violated as the system evolves over time. Computing a valid safe set and BF for a nonlinear (and potentially unmodeled), non-autonomous dynamical system is a difficult task. This work explores the design of BFs using data to obtain safe sets with deterministic assurances of control invariance. We leverage ReLU neural networks (NNs) to create continuous piecewise affine (CPA) BFs with deterministic safety guarantees for Lipschitz continuous, discrete-time dynamical system using sampled one-step trajectories. The CPA structure admits a novel classifier term to create a relaxed \ac{bf} condition and construction via a data driven constrained optimization. We use iterative convex overbounding (ICO) to solve this nonconvex optimization problem through a series of convex optimization steps. We then demonstrate our method's efficacy on two-dimensional autonomous and non-autonomous dynamical systems.

</details>


### [34] [Causal Feature Selection for Weather-Driven Residential Load Forecasting](https://arxiv.org/abs/2511.20508)
*Elise Zhang,François Mirallès,Stéphane Dellacherie,Di Wu,Benoit Boulet*

Main category: eess.SY

TL;DR: 本文比较了因果特征选择与传统特征选择在居住用电负荷预测中的天气特征筛选效果，结果显示PCMCI因果选择在提升模型简约性和对极端天气鲁棒性方面优于无选择和非因果选择，且适用于GRU、TCN、PatchTST等模型。


<details>
  <summary>Details</summary>
Motivation: 天气对用电需求影响显著，但过多气象变量会增加模型复杂度并可能降低预测性能。因此需要一个 principled 的特征选择框架，以筛选最具信息量且对日常调度与可靠性最有帮助的天气驱动变量，尤其在短期负荷预测场景中。

Method: 以加拿大安大略省南部的两个开源数据集为基础：IESO 按 Forward Sortation Areas 的小时用电量，以及 ERA5 天气再分析数据。比较三种特征选择策略（无特征选择、非因果选择、PCMCI因果选择）在城市级预测中对 GRU、TCN、PatchTST 三种时序模型的影响。特征分析显示，非因果选择偏好基于相关性的辐射与湿度变量；PCMCI因果选择则聚焦直接热驱动并去除间接协变量。报告预测精度与极端天气鲁棒性的诊断，强调将因果特征选择作为现代负荷预测器的实用补充。

Result: 实验结果指向 PCMCI 因果选择更倾向于保留直接热驱动、裁剪冗余特征，从而提升在极端天气条件下的鲁棒性；相比之下，非因果选择更易被相关性驱动的辐射、湿度等变量主导。不同预测模型在相同选择策略下显示出对简约特征的兼容性，支持因果特征选择在短期居住负荷预测中的实用性。

Conclusion: 将因果特征选择视为与现代预测器互补的有效工具，有望提升天气特征在日常规划与可靠性评估中的利用效率与鲁棒性，尤其在极端天气场景下表现突出。该方法可集成到现有负荷预测流水线，但需关注数据质量、计算成本以及地区适用性等潜在限制。

Abstract: Weather is a dominant external driver of residential electricity demand, but adding many meteorological covariates can inflate model complexity and may even impair accuracy. Selecting appropriate exogenous features is non-trivial and calls for a principled selection framework, given the direct operational implications for day-to-day planning and reliability. This work investigates whether causal feature selection can retain the most informative weather drivers while improving parsimony and robustness for short-term load forecasting. We present a case study on Southern Ontario with two open-source datasets: (i) IESO hourly electricity consumption by Forward Sortation Areas; (ii) ERA5 weather reanalysis data. We compare different feature selection regimes (no feature selection, non-causal selection, PCMCI-causal selection) on city-level forecasting with three different time series forecasting models: GRU, TCN, PatchTST. In the feature analysis, non-causal selection prioritizes radiation and moisture variables that show correlational dependence, whereas PCMCI-causal selection emphasizes more direct thermal drivers and prunes the indirect covariates. We detail the evaluation pipeline and report diagnostics on prediction accuracy and extreme-weather robustness, positioning causal feature selection as a practical complement to modern forecasters when integrating weather into residential load forecasting.

</details>


### [35] [From Features to States: Data-Driven Selection of Measured State Variables via RFE-DMDc](https://arxiv.org/abs/2511.20552)
*Haoyu Wang,Andrea Alfonsi,Roberto Ponciroli,Richard Vilim*

Main category: eess.SY

TL;DR: 提出 RFE-DMDc 框架，通过递归特征消除选择最小、物理意义清晰的变量集合，并结合 DMDc 构建线性状态空间模型；并与 GA-DMDc 基线在 RLC 与综合能源系统上对比，显示在降低计算成本的同时保持预测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在工程系统中，基于第一性原理的建模往往不可行，需数据驱动的数字孪生用于控制与诊断，因此需要一个紧凑、可解释且鲁棒的状态集合和相应线性模型。

Method: 先应用递归特征消除(RFE)选择一个最小且物理意义明确的状态变量集合；引入跨子系统选择以缓解多组件系统中的特征覆盖问题；再用 DMDc 构建线性状态空间模型。以 GA-DMDc 为基线，在共同的准确性成本下同时优化状态集合与模型拟合。对 RLC 基准与多热耦合组件的真实能源系统进行验证，得到约 10 个变量的紧凑集合，测试误差与 GA-DMDc 相当但计算时间低一个数量级。

Result: RFE-DMDc 在多组件系统中成功回收紧凑的状态集合（约 10 个变量），实现与 GA-DMDc 相似的预测性能，同时显著降低计算成本；所选变量具有跨子系统的物理可解释性，模型展现出竞争力的预测能力、计算效率及对过拟合的鲁棒性。

Conclusion: 数据驱动的 RFE-DMDc 为具有大量候选变量的多组件系统提供一种可扩展、低开销且可解释的数字孪生建模方法，适用于控制与诊断任务，并具备良好的推广潜力。

Abstract: The behavior of a dynamical system under a given set of inputs can be captured by tracking the response of an optimal subset of process variables (\textit{state variables}). For many engineering systems, however, first-principles, model-based identification is impractical, motivating data-driven approaches for Digital Twins used in control and diagnostics. In this paper, we present RFE-DMDc, a supervised, data-driven workflow that uses Recursive Feature Elimination (RFE) to select a minimal, physically meaningful set of variables to monitor and then derives a linear state-space model via Dynamic Mode Decomposition with Control (DMDc). The workflow includes a cross-subsystem selection step that mitigates feature \textit{overshadowing} in multi-component systems. To corroborate the results, we implement a GA-DMDc baseline that jointly optimizes the state set and model fit under a common accuracy cost on states and outputs. Across a truth-known RLC benchmark and a realistic Integrated Energy System (IES) with multiple thermally coupled components and thousands of candidate variables, RFE-DMDc consistently recovers compact state sets (\(\approx 10\) variables) that achieve test errors comparable to GA-DMDc while requiring an order of magnitude less computational time. The selected variables retain clear physical interpretation across subsystems, and the resulting models demonstrate competitive predictive accuracy, computational efficiency, and robustness to overfitting.

</details>


### [36] [Exploring Urban Air Mobility Adoption Potential in San Francisco Bay Area Region A Systems of Systems Level Case Study on Passenger Waiting Times and Travel Efficiency](https://arxiv.org/abs/2511.20603)
*Winfrey Paul Sagayam Dennis*

Main category: eess.SY

TL;DR: UAM with eVTOL could dramatically cut regional travel times in SF Bay Area; a MATLAB multi-agent model suggests up to 80% time reduction during peak demand, with fleet size, demand volume, and turnaround time as key levers for cost, waiting times, and user acceptance.


<details>
  <summary>Details</summary>
Motivation: Explore the feasibility and systems-level adoption of Urban Air Mobility (UAM) in a congested metropolitan area, addressing whether eVTOL air taxis can alleviate ground congestion and how a systems-of-systems perspective affects implementation.

Method: Develop a MATLAB-based multi-agent simulation to evaluate fleet operations and model demand arrival with a Poisson process under stochastic passenger flows and turnaround constraints; compare UAM to conventional ground transport across regional nodes (San Francisco, Oakland, San Jose, Palo Alto airports).

Result: The simulation indicates substantial travel-time savings (up to ~80%) during peak demand when using UAM. Critical factors—fleet size, passenger request volumes, and turnaround time—significantly influence waiting times, operating costs, and user acceptance.

Conclusion: UAM deployment in the SF Bay Area shows strong potential under appropriate fleet scheduling and operational parameters. Successful adoption hinges on optimizing fleet size and turnaround, and managing demand to balance travel-time savings with costs and acceptance.

Abstract: Urban Air mobility has gained momentum with recent advancements in the electric vertical take-off and landing (eVTOL) vehicles, offering faster point-to-point air taxi services that could help relieve traffic congestion in chronically overburdened cities. The research assesses the feasibility and systems-of-systems level adoption potential of UAM operations in the San Francisco Bay Area by comparing passenger departure, waiting, travel, and arrival times across key regional nodes, including San Francisco, Oakland, San Jose, and Palo Alto airports, with conventional ground transportation. A multi-agent simulation was developed in MATLAB to evaluate the fleet operations and to model demand arrival using a Poisson process under stochastic passenger flows and turnaround constraints. Results indicate that utilizing UAM during peak demand could reduce total travel times up to eighty percent across the region. The findings of this paper highlight the critical operational factors for fleet schedule optimization. Especially how the fleet size, passengers' request volumes, and turnaround time directly influence waiting time, operating cost, and overall user acceptance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Quantifying Modality Contributions via Disentangling Multimodal Representations](https://arxiv.org/abs/2511.19470)
*Padegal Amit,Omkar Mahesh Kashyap,Namitha Rayasam,Nidhi Shekhar,Surabhi Narayan*

Main category: cs.LG

TL;DR: 用部分信息分解（PID）来分解多模态模型内部嵌入的信息，区分独特、冗余和协同信息，并用迭代比例拟合（IPFP）实现无再训练的推断分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于准确率的度量混淆了模态本质信息性与模态之间交互带来的作用，需要一种在表示层面上分解贡献的原理化方法，尤其是在跨注意力结构中模态相互影响的情形。

Method: 将部分信息分解应用于内部嵌入，分解预测信息为独有、冗余与协同成分；为实现可扩展的推理分析，开发基于迭代比例拟合的算法，在不重新训练的前提下计算层级和数据集级别的贡献。

Result: 提供了一种可扩展的推理分析方法，使得从表示层面理解模态贡献成为可能，相较于以结果为导向的度量，具有更清晰、可解释的洞察力。

Conclusion: 利用PID对多模态交互中的模态贡献进行表示层面的分解，结合IPFP实现的无再训练分析，为跨模态架构中的独特、冗余与协同信息提供了 principled 的解释框架。

Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.

</details>


### [38] [PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer](https://arxiv.org/abs/2511.19472)
*Ruogu Ding,Xin Ning,Ulf Schlichtmann,Weikang Qian*

Main category: cs.LG

TL;DR: PrefixGPT 使用GPT架构直接从头生成优化的前缀加法器，通过将拓扑表示为二维坐标序列并引入合法性掩码确保设计在生成时就有效。采用解码器式Transformer，先在随机合成的有效前缀加法器库上进行预训练，再微调以提升设计质量。


<details>
  <summary>Details</summary>
Motivation: 在高速前缀加法器设计中，设计空间庞大且受严格规则约束，传统优化方法难以高效探索并找到全局最优解。

Method: 将加法器的拓扑表示为二维坐标序列，生成阶段应用 legality mask（合法性掩码）确保每一步生成的设计都是有效的；使用定制的解码器式Transformer架构，先进行对随机合成的有效前缀加法器进行预训练以掌握设计规律，然后在目标设计空间上进行微调以提高优化质量。

Result: 相较于现有工作，PrefixGPT 找到了新的最优设计，ADP 提升约7.7%，并且在探索质量方面表现出更低的平均ADP，降低幅度高达79.1%。

Conclusion: 证明了GPT风格模型在硬件设计中的潜力：先掌握复杂设计原则，再用于高效的设计优化。

Abstract: Prefix adders are widely used in compute-intensive applications for their high speed. However, designing optimized prefix adders is challenging due to strict design rules and an exponentially large design space. We introduce PrefixGPT, a generative pre-trained Transformer (GPT) that directly generates optimized prefix adders from scratch. Our approach represents an adder's topology as a two-dimensional coordinate sequence and applies a legality mask during generation, ensuring every design is valid by construction. PrefixGPT features a customized decoder-only Transformer architecture. The model is first pre-trained on a corpus of randomly synthesized valid prefix adders to learn design rules and then fine-tuned to navigate the design space for optimized design quality. Compared with existing works, PrefixGPT not only finds a new optimal design with a 7.7% improved area-delay product (ADP) but exhibits superior exploration quality, lowering the average ADP by up to 79.1%. This demonstrates the potential of GPT-style models to first master complex hardware design principles and then apply them for more efficient design optimization.

</details>


### [39] [WavefrontDiffusion: Dynamic Decoding Schedule or Improved Reasoning](https://arxiv.org/abs/2511.19473)
*Haojin Yang,Rui Hu,Zequn Sun,Rui Zhou,Yujun Cai,Yiwei Wang*

Main category: cs.LG

TL;DR: 提出 WavefrontDiffusion，一种动态解码方法，通过从已确定位置向外扩展的波前来更新，达到比标准扩散和 BlockDiffusion 更高的语义保真度与性能，在四个推理与代码生成基准上达到最优。


<details>
  <summary>Details</summary>
Motivation: 现有去噪策略（标准扩散与 BlockDiffusion）在生成质量上各有局限：前者可能导致上下文不完整和提前结束，后者的固定块结构可能破坏连贯语义单元和推理过程。需要一种既保持成本、又能更好捕捉语义结构的自适应去噪/解码策略。

Method: 提出 WavefrontDiffusion：一种动态解码方法，从已完成位置向外扩展波前的活动标记，按自适应调度更新，保持计算成本等同于基于块的方法。该策略与语义结构的自然流向一致，提升输出的连贯性。

Result: 在四个推理与代码生成基准上实现了最先进的性能，输出具有更高的语义保真度，证明自适应调度对于更连贯且高效的生成有显著价值。

Conclusion: 自适应调度的解码策略，如 WavefrontDiffusion，能够在保持计算成本的前提下提升生成的连贯性与语义保真度，具有广泛应用潜力。

Abstract: Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.

</details>


### [40] [Exploiting the Experts: Unauthorized Compression in MoE-LLMs](https://arxiv.org/abs/2511.19480)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.LG

TL;DR: MoE-LLMs存在可被裁剪再定位，从而规避许可和安全约束的风险；通过专家归因和主动学习微调评估裁剪与重定位的任务相关性与结果，提出抗压缩和有选择的微调策略来提高安全性。


<details>
  <summary>Details</summary>
Motivation: 系统性研究MoE在特定任务下的可裁剪性及其对安全性的影响，揭示专家子集对任务的重要性与知识损失/恢复之间的权衡，强调MoE模块化的双重用途（威胁与防御）。

Method: 提出一个专家归因框架以识别对特定任务最负责的专家子集；在此基础上评估通过裁剪和基于主动学习的微调进行重对齐的性能权衡；并提出基于 entangled expert training 与选择性微调的防御策略。

Result: 发现知识损失-恢复的权衡：可 isolating 某些专家以维持任务精度，但若不进行针对性重新对齐，仍会出现显著退化；裁剪-重对齐的效果取决于目标任务与数据的可获得性及模型的模块耦合程度。

Conclusion: MoE 的专家裁剪既是潜在威胁也可成为防御对象，需开发使模型更难被无授权压缩与微调的训练与部署策略；本文首次给出系统性的安全专门化评估框架，揭示了 MoE 模块化的双重用途。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.

</details>


### [41] [Quality analysis and evaluation prediction of RAG retrieval based on machine learning algorithms](https://arxiv.org/abs/2511.19481)
*Ruoxin Zhang,Zhizhao Wen,Chao Wang,Chenchen Tang,Puyang Xu,Yifan Jiang*

Main category: cs.LG

TL;DR: 以检索增强生成为背景，提出基于特征工程与粒子群优化的 XGBoost 回归模型以提升检索质量对生成效果的影响，实验表明 VMD-PSO BiLSTM 在多指标上优于对比模型.


<details>
  <summary>Details</summary>
Motivation: 检索模块质量直接决定 RAG 的输出准确性；现有模型在处理表格特征方面存在瓶颈，需通过建立检索相关性与生成质量之间的关系来提升系统性能.

Method: 在特征工程与粒子群优化基础上构建 XGBoost 回归模型；进行文档相关性、语义相似度、冗余与多样性之间的相关性分析；提出 VMD-PSO-BiLSTM 等模型用于回归与序列建模，并与决策树、AdaBoost 等模型对比.

Result: 文档相关性对回答质量有显著正相关（r=0.66），语义相似性与冗余对多样性呈显著负相关（分别为 -0.89 和 -0.88）。VMD PSO BiLSTM 在所有评估指标上优于对比模型，MSE、RMSE、MAE、MAPE 显著降低，R2 提高，表明预测精度、稳定性与解释能力更强。

Conclusion: 该方法为优化检索质量、提升 RAG 系统生成效果提供有效路径，对相关技术的应用具有重要意义。

Abstract: With the rapid evolution of large language models, retrieval enhanced generation technology has been widely used due to its ability to integrate external knowledge to improve output accuracy. However, the performance of the system is highly dependent on the quality of the retrieval module. If the retrieval results have low relevance to user needs or contain noisy information, it will directly lead to distortion of the generated content. In response to the performance bottleneck of existing models in processing tabular features, this paper proposes an XGBoost machine learning regression model based on feature engineering and particle swarm optimization. Correlation analysis shows that answer_quality is positively correlated with doc_delevance by 0.66, indicating that document relevance has a significant positive effect on answer quality, and improving document relevance may enhance answer quality; The strong negative correlations between semantic similarity, redundancy, and diversity were -0.89 and -0.88, respectively, indicating a trade- off between semantic similarity, redundancy, and diversity. In other words, as the former two increased, diversity significantly decreased. The experimental results comparing decision trees, AdaBoost, etc. show that the VMD PSO BiLSTM model is superior in all evaluation indicators, with significantly lower MSE, RMSE, MAE, and MAPE compared to the comparison model. The R2 value is higher, indicating that its prediction accuracy, stability, and data interpretation ability are more outstanding. This achievement provides an effective path for optimizing the retrieval quality and improving the generation effect of RAG system, and has important value in promoting the implementation and application of related technologies.

</details>


### [42] [OmniTFT: Omni Target Forecasting for Vital Signs and Laboratory Result Trajectories in Multi Center ICU Data](https://arxiv.org/abs/2511.19485)
*Wanzhe Xu,Yutong Dai,Yitao Yang,Martin Loza,Weihang Zhang,Yang Cui,Xin Zeng,Sung Joon Park,Kenta Nakai*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate multivariate time-series prediction of vital signs and laboratory results is crucial for early intervention and precision medicine in intensive care units (ICUs). However, vital signs are often noisy and exhibit rapid fluctuations, while laboratory tests suffer from missing values, measurement lags, and device-specific bias, making integrative forecasting highly challenging. To address these issues, we propose OmniTFT, a deep learning framework that jointly learns and forecasts high-frequency vital signs and sparsely sampled laboratory results based on the Temporal Fusion Transformer (TFT). Specifically, OmniTFT implements four novel strategies to enhance performance: sliding window equalized sampling to balance physiological states, frequency-aware embedding shrinkage to stabilize rare-class representations, hierarchical variable selection to guide model attention toward informative feature clusters, and influence-aligned attention calibration to enhance robustness during abrupt physiological changes. By reducing the reliance on target-specific architectures and extensive feature engineering, OmniTFT enables unified modeling of multiple heterogeneous clinical targets while preserving cross-institutional generalizability. Across forecasting tasks, OmniTFT achieves substantial performance improvement for both vital signs and laboratory results on the MIMIC-III, MIMIC-IV, and eICU datasets. Its attention patterns are interpretable and consistent with known pathophysiology, underscoring its potential utility for quantitative decision support in clinical care.

</details>


### [43] [Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification](https://arxiv.org/abs/2511.19486)
*Lei Wang,Zikun Ye,Jinglong Zhao*

Main category: cs.LG

TL;DR: 提出一个将微调（fine-tuning）与修正（rectification）相结合的框架，针对有限标注样本在两阶段之间的最优分配进行数据驱动分配。微调目标从均方误差改为预测误差方差，以利于后续修正阶段。并基于经验尺度定律推导出最佳样本分割策略。实验表明该联合方式在估计和推断性能上优于仅使用微调或修正的单独策略。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，LLMs在市场研究与社会科学中具备潜力，但需要更好的对齐与偏差修正，同时标注数据稀缺，需高效的数据分配策略来提升下游推断的准确性与鲁棒性。

Method: 提出一个将微调与修正结合的框架，并将微调目标设定为最小化预测误差方差；利用经验尺度定律通过数据驱动的方法在两阶段之间分配有限标注样本。

Result: 实证分析显示，与仅使用微调或仅使用修正相比，该框架在估计与推断性能上具有显著改进。

Conclusion: 联合优化的微调与修正框架及其数据分割策略能够在数据受限场景下提升LLMs的下游表现与推断鲁棒性。

Abstract: Driven by recent advances in artificial intelligence (AI), a growing body of work demonstrates the potential of using large language models (LLMs) to generate human-like responses in market research and social science applications. Two primary approaches can be applied to improve the performance of LLMs: fine-tuning, which aligns LLM predictions more closely with human responses, and rectification, which corrects biases in LLM outputs. In this paper, we develop a framework that combines fine-tuning and rectification, and optimally allocates limited labeled samples across the two stages. Unlike the conventional objective that minimizes the mean squared prediction errors, we propose to minimize the variance of the prediction errors as the fine-tuning objective, which is optimal for the downstream rectification stage. Building on this insight, we leverage empirical scaling laws to develop a data-driven method for optimally splitting samples between the fine-tuning and rectification stages. Empirical analysis validates our framework, demonstrating improved estimation and inference performance compared to using either fine-tuning or rectification alone.

</details>


### [44] [The Generalized Proximity Forest](https://arxiv.org/abs/2511.19487)
*Ben Shaw,Adam Rustad,Sofia Pelagalli Maia,Jake S. Rhodes,Kevin R. Moon*

Main category: cs.LG

TL;DR: Generalized Proximity Forest (PF) extends Random Forest (RF) proximities to any supervised distance-based learning, including regression and a meta-learning framework for imputation, with demonstrated advantages over RF and k-NN.


<details>
  <summary>Details</summary>
Motivation: RF proximities are powerful but limited by RF's own model; extending proximities to broader contexts (time series, regression, imputation) can enhance supervised distance-based tasks and provide versatile tools for data analysis.

Method: Introduce the generalized PF model, along with a regression-specific PF variant; propose using generalized PF as a meta-learning framework to enable imputation with any pre-trained classifier; conduct experiments comparing generalized PF to RF and k-NN across relevant tasks.

Result: Show that generalized PF offers unique advantages over traditional RF proximities and k-NN in supervised distance-based tasks, including outlier detection, missing data imputation, and regression, with benefits also seen in visualization contexts.

Conclusion: Generalized PF broadens the applicability of RF proximities to all supervised distance-based learning scenarios and can serve as a versatile meta-learning tool for imputation, potentially outperforming standard RF and k-NN in key tasks.

Abstract: Recent work has demonstrated the utility of Random Forest (RF) proximities for various supervised machine learning tasks, including outlier detection, missing data imputation, and visualization. However, the utility of the RF proximities depends upon the success of the RF model, which itself is not the ideal model in all contexts. RF proximities have recently been extended to time series by means of the distance-based Proximity Forest (PF) model, among others, affording time series analysis with the benefits of RF proximities. In this work, we introduce the generalized PF model, thereby extending RF proximities to all contexts in which supervised distance-based machine learning can occur. Additionally, we introduce a variant of the PF model for regression tasks. We also introduce the notion of using the generalized PF model as a meta-learning framework, extending supervised imputation capability to any pre-trained classifier. We experimentally demonstrate the unique advantages of the generalized PF model compared with both the RF model and the $k$-nearest neighbors model.

</details>


### [45] [Generative Model-Aided Continual Learning for CSI Feedback in FDD mMIMO-OFDM Systems](https://arxiv.org/abs/2511.19490)
*Guijun Liu,Yuwen Cao,Tomoaki Ohtsuki,Jiguang He,Shahid Mumtaz*

Main category: cs.LG

TL;DR: 提出一种基于GAN的持续学习框架用于CSI反馈，将GAN生成器作为记忆单元以缓解灾难性遗忘，提升DAE在mMIMO-OFDM中的泛化能力，且内存开销低，能与其它CSI反馈模型兼容。


<details>
  <summary>Details</summary>
Motivation: 由于用户移动导致CSI分布随环境动态变化，现有DAE需重新训练；返回到已见环境时会发生灾难性遗忘。需要一种在保持旧任务性能的同时学习新任务的持续学习方法。

Method: 在GAN框架中，将生成器作为记忆单元，保存过去环境的知识，通过对新环境进行增量训练而不显著削弱旧知识；结合DAE的CSI反馈任务，且具有低额外内存开销，能够与其他CSI反馈模型集成。

Result: 仿真结果表明该方法提升了DAE框架在多样场景下的泛化能力，内存开销低；同样具备与其他先进CSI反馈模型的无缝集成能力，体现鲁棒性和适应性。

Conclusion: GAN驱动的记忆性持续学习为CSI反馈提供一种高效的记忆机制，能在动态mMIMO-OFDM环境中实现稳健性能并具备良好扩展性。

Abstract: Deep autoencoder (DAE) frameworks have demonstrated their effectiveness in reducing channel state information (CSI) feedback overhead in massive multiple-input multiple-output (mMIMO) orthogonal frequency division multiplexing (OFDM) systems. However, existing CSI feedback models struggle to adapt to dynamic environments caused by user mobility, requiring retraining when encountering new CSI distributions. Moreover, returning to previously encountered environments often leads to performance degradation due to catastrophic forgetting. Continual learning involves enabling models to incorporate new information while maintaining performance on previously learned tasks. To address these challenges, we propose a generative adversarial network (GAN)-based learning approach for CSI feedback. By using a GAN generator as a memory unit, our method preserves knowledge from past environments and ensures consistently high performance across diverse scenarios without forgetting. Simulation results show that the proposed approach enhances the generalization capability of the DAE framework while maintaining low memory overhead. Furthermore, it can be seamlessly integrated with other advanced CSI feedback models, highlighting its robustness and adaptability.

</details>


### [46] [OpenCML: End-to-End Framework of Open-world Machine Learning to Learn Unknown Classes Incrementally](https://arxiv.org/abs/2511.19491)
*Jitendra Parmar,Praveen Singh Thakur*

Main category: cs.LG

TL;DR: 提出一个开放世界连续学习框架：先发现未知类别再对新类别进行增量学习，能在四次迭代中达到平均准确率82.54%，最低65.87%，并优于现有开放世界学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统闭世界假设下的模型难以在后续任务中保留先前知识并适应新类别，因此需要在开放、连续学习场景中具备发现未知类别和增量学习能力的系统。

Method: 设计一个两阶段、相互连接的框架：第一阶段发现数据中的未知类别并创建新类别；第二阶段对每个新类别进行增量学习以完成分类。该框架在开放世界学习环境中实现 continual learning，允许模型持续扩展知识并提升性能。

Result: 在开放世界学习任务中，该框架超越了现有方法；在连续学习评估中，四轮迭代后达到最高平均准确率82.54%，最低为65.87%。

Conclusion: 该方法提供了一种有效的开放世界持续学习解决方案，能够发现新类别并将其增量整合到模型中，从而实现长期性能提升。

Abstract: Open-world machine learning is an emerging technique in artificial intelligence, where conventional machine learning models often follow closed-world assumptions, which can hinder their ability to retain previously learned knowledge for future tasks. However, automated intelligence systems must learn about novel classes and previously known tasks. The proposed model offers novel learning classes in an open and continuous learning environment. It consists of two different but connected tasks. First, it discovers unknown classes in the data and creates novel classes; next, it learns how to perform class incrementally for each new class. Together, they enable continual learning, allowing the system to expand its understanding of the data and improve over time. The proposed model also outperformed existing approaches in open-world learning. Furthermore, it demonstrated strong performance in continuous learning, achieving a highest average accuracy of 82.54% over four iterations and a minimum accuracy of 65.87%.

</details>


### [47] [A Systematic Study of Compression Ordering for Large Language Models](https://arxiv.org/abs/2511.19495)
*Shivansh Chhawri,Rahul Mahadik,Suparna Rooj*

Main category: cs.LG

TL;DR: 对比单独与组合的模型压缩，发现压缩顺序对最终性能影响显著，P-KD-Q（剪枝-蒸馏-量化）在3.68x压缩比下维持良好指令遵循与语言理解；过早量化会造成不可逆信息损失，削弱后续提升。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型的压缩常用技术包括知识蒸馏、结构化裁剪、低位比特量化，但它们的相互作用和有效排序尚不清楚。本研究系统地评估单一与组合压缩在资源受限场景中的性能与 trade-offs。

Method: 在 Qwen2.5 3B 模型上设计多条压缩管线，覆盖单一技术与三-technique 序列，使用 perplexity、G-Eval、clarity、prompt alignment、compression ratio等指标比较。

Result: 量化提供最大的独立压缩；裁剪带来中等程度的质量下降；技术顺序对结果影响显著，P-KD-Q 达到约3.68x 压缩，同时保持指令遵循与语言理解能力；早量化会导致不可逆信息丢失，限制后续训练效果。

Conclusion: 在资源受限环境中，应重视压缩顺序设计，P-KD-Q 为在保持较好性能前提下实现高压缩的一种有效路径。

Abstract: Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.

</details>


### [48] [PeriodNet: Boosting the Potential of Attention Mechanism for Time Series Forecasting](https://arxiv.org/abs/2511.19497)
*Bowen Zhao,Huanlai Xing,Zhiwen Xiao,Jincheng Peng,Li Feng,Xinhan Wang,Rong Qu,Hui Li*

Main category: cs.LG

TL;DR: PeriodNet提出一种新的时序注意力结构，结合周期注意力和稀疏周期注意力，配合迭代分组机制和周期扩散器，对单变量与多变量时间序列的预测表现优于六个状态-of-the-art模型，且在长度720的序列上相对提升约22%。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力机制在NLP中表现出色，但在时间序列预测中的应用尚未充分发挥潜力。需要一种更契合时间序列局部特征、周期模式与全局依赖的注意力结构，并高效地实现跨变量建模。

Method: 提出PeriodNet：包含周期注意力与稀疏周期注意力以分析相邻周期；引入迭代分组机制实现跨变量去冗；重新设计Transformer编码器架构并提出周期扩散器以实现更精确的多周期预测。

Result: 在八个数据集上进行实验，PeriodNet在单变量与多变量TSF任务中均超越六个主要竞争模型，使用MSE与MAE评估；在序列长度为720时，相较基于传统编码器-解码器Transformer的模型实现约22%的相对提升。

Conclusion: PeriodNet通过整合周期信息、稀疏关注和跨变量高效建模，提升了时间序列预测的准确性，展示了对复杂周期性与跨变量依赖的有效捕获能力。

Abstract: The attention mechanism has demonstrated remarkable potential in sequence modeling, exemplified by its successful application in natural language processing with models such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT). Despite these advancements, its utilization in time series forecasting (TSF) has yet to meet expectations. Exploring a better network structure for attention in TSF holds immense significance across various domains. In this paper, we present PeriodNet with a brand new structure to forecast univariate and multivariate time series. PeriodNet incorporates period attention and sparse period attention mechanism for analyzing adjacent periods. It enhances the mining of local characteristics, periodic patterns, and global dependencies. For efficient cross-variable modeling, we introduce an iterative grouping mechanism which can directly reduce the cross-variable redundancy. To fully leverage the extracted features on the encoder side, we redesign the entire architecture of the vanilla Transformer and propose a period diffuser for precise multi-period prediction. Through comprehensive experiments conducted on eight datasets, we demonstrate that PeriodNet outperforms six state-of-the-art models in both univariate and multivariate TSF scenarios in terms of mean square error and mean absolute error. In particular, PeriodNet achieves a relative improvement of 22% when forecasting time series with a length of 720, in comparison to other models based on the conventional encoder-decoder Transformer architecture.

</details>


### [49] [Prompt Fairness: Sub-group Disparities in LLMs](https://arxiv.org/abs/2511.19956)
*Meiyu Zhong,Noel Teku,Ravi Tandon*

Main category: cs.LG

TL;DR: 研究探讨提示公平性在大语言模型中的体现，提出以信息论度量子群内部变异和跨群一致性，并在多代生成与提示中和中性的干预下降低群体间输出差异。


<details>
  <summary>Details</summary>
Motivation: 随着提示风格的差异化，LLMs 对同一问题可能给出不同质量的回答，存在系统性偏差，需要量化与缓解以实现更公平的服务。

Method: 提出信息论指标来衡量子群内部变异性（subgroup sensitivity）和跨群一致性（cross-group consistency），基于 Demographic 子群进行实证分析；提出两种干预：多数投票（多代生成）与提示中性化（prompt neutralization）；在实验中比较干预前后的跨群距离（0.28 -> 0.17–0.22区间，最大差距降至0.22，很多低于0.17）。

Result: 发现某些子群在内部变异性和对其他群体的差异性上均高于其他群体，存在结构性不平等；干预后跨群距离显著下降，输出更稳定。

Conclusion: 通过多代生成和提示中性化等策略，可以有效提升模型输出在不同用户群体中的公平性与稳定性。

Abstract: Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.

</details>


### [50] [Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data](https://arxiv.org/abs/2511.19498)
*Yi Zhang,Tianxiang Xu,Zijian Li,Chao Zhang,Kunyu Zhang,Zhan Gao,Meinuo Li,Xiaohan Zhang,Qichao Qi,Bing Chen*

Main category: cs.LG

TL;DR: A hierarchical dual-strategy framework for selective knowledge unlearning in LLMs that deletes specialized medical knowledge while preserving core medical competencies, achieving high forgetting with minimal parameter changes.


<details>
  <summary>Details</summary>
Motivation: Privacy risks from training data memorization in large language models, particularly in healthcare where patient information is sensitive; need methods to erase specific knowledge to meet regulatory and ethical standards.

Method: A two-pronged approach combining geometric-constrained gradient updates to selectively modulate target parameters and concept-aware token-level interventions that distinguish preservation-critical from unlearning-targeted tokens using a four-level medical concept hierarchy.

Result: On MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets, the method achieves 82.7% forgetting rate and 88.5% knowledge preservation, modifying only 0.1% of parameters, indicating strong privacy guarantees with minimal model alteration.

Conclusion: The framework provides effective selective knowledge unlearning that balances privacy protection with medical competence preservation, supporting regulatory compliance and ethical standards in clinical research.

Abstract: Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.

</details>


### [51] [Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection](https://arxiv.org/abs/2511.19499)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 对比GAN与扩散模型在伪造图像检测中的跨生成架构泛化差异，提出TriDetect半监督检测方法，通过Sinkhorn-Knopp簇分配与跨视图一致性提升对未见生成器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着StyleGAN、Midjourney、DALL-E等生成模型的快速发展，合成图像的真实性显著提高，对数字媒体真实性构成挑战。现有伪造检测器在跨架构（如从GAN到扩散模型）时往往泛化不足，因不同架构产生的伪造痕迹具有本质差异。

Method: 给出GAN与DM在优化目标上的本质差异如何导致不同的覆盖行为的理论分析。基于分析提出TriDetect，一种半监督二分类方法，通过Sinkhorn-Knopp实现对“fake”类别的平衡簇分配，并引入跨视图一致性来促使模型学习基础的架构差异。对两项标准基准与三项野外数据集进行实验，与13个基线进行对比，验证对未见生成器的泛化能力。

Result: 实验表明TriDetect在对未见生成器的泛化能力上优于基线，且在两项标准基准和三项野外数据集上表现稳健。

Conclusion: 从架构层面揭示了GAN与DM在伪造痕迹上的不同覆盖行为，TriDetect通过挖掘潜在的架构模式提升半监督伪造检测的泛化性，为跨架构伪造检测提供了有效途径。

Abstract: The rapid advancement of generators (e.g., StyleGAN, Midjourney, DALL-E) has produced highly realistic synthetic images, posing significant challenges to digital media authenticity. These generators are typically based on a few core architectural families, primarily Generative Adversarial Networks (GANs) and Diffusion Models (DMs). A critical vulnerability in current forensics is the failure of detectors to achieve cross-generator generalization, especially when crossing architectural boundaries (e.g., from GANs to DMs). We hypothesize that this gap stems from fundamental differences in the artifacts produced by these \textbf{distinct architectures}. In this work, we provide a theoretical analysis explaining how the distinct optimization objectives of the GAN and DM architectures lead to different manifold coverage behaviors. We demonstrate that GANs permit partial coverage, often leading to boundary artifacts, while DMs enforce complete coverage, resulting in over-smoothing patterns. Motivated by this analysis, we propose the \textbf{Tri}archy \textbf{Detect}or (TriDetect), a semi-supervised approach that enhances binary classification by discovering latent architectural patterns within the "fake" class. TriDetect employs balanced cluster assignment via the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism, encouraging the model to learn fundamental architectural distincts. We evaluate our approach on two standard benchmarks and three in-the-wild datasets against 13 baselines to demonstrate its generalization capability to unseen generators.

</details>


### [52] [Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma](https://arxiv.org/abs/2511.19504)
*Subramanyam Sahoo,Aman Chadha,Vinija Jain,Divya Chaudhary*

Main category: cs.LG

TL;DR: Formalizes the Alignment Trilemma in RLHF: cannot achieve epsilon-representativeness, polynomial tractability, and delta-robustness simultaneously. Proves super-polynomial lower bound for representativeness plus robustness; current RLHF sacrifices representativeness by using small, homogeneous samples; explains pathologies; suggests relaxations to navigate trade-offs.


<details>
  <summary>Details</summary>
Motivation: Understand fundamental trade-offs in aligning large language models with diverse human values using RLHF, balancing safety, fairness, scalability, and robustness.

Method: Complexity-theoretic analysis that combines statistical learning theory and robust optimization. Proves Omega(2^{d_context}) lower bound for achieving joint representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) at global scale. Analyzes sample regimes (1e3–1e4 vs 1e7–1e8) and interprets observed RLHF pathologies.

Result: Establishes a formal trilemma with a super-polynomial lower bound, showing current RLHF setups trade representativeness for tractability and robustness. Provides a unified explanation for preference collapse, sycophancy, and bias amplification.

Conclusion: Recommend strategic relaxations of alignment requirements and deliberate trade-offs to navigate the trilemma, outlining concrete directions for practical RLHF deployment.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.

</details>


### [53] [Row-stochastic matrices can provably outperform doubly stochastic matrices in decentralized learning](https://arxiv.org/abs/2511.19513)
*Bing Liu,Boao Kong,Limin Lu,Kun Yuan,Chengcheng Zhao*

Main category: cs.LG

TL;DR: 在异质节点权重 λ 下，提出在加权希尔伯特空间 L^2(λ; R^d) 中的分析框架，揭示两种策略的收敛差异：将权重嵌入局部损失以获得统一权重（从而得到一个行/列双随机矩阵的等价性），或者保持原始损失但使用 λ 引导的行随机矩阵。尽管已有工作表明两者在全球损失的期望下降方向上相同，但 Euclidean 空间下的保证是否紧致尚不清楚。通过新框架，得到比欧几里得分析更紧的收敛率界，且在该几何下，行随机矩阵为自伴而双随机矩阵并非自伴，从而产生额外的罚项来放大共识误差，使收敛速度受到影响。研究还给出在谱隙较小的情况下，行随机设计仍然可以更快收敛的充分条件，并利用 Rayleigh 商和 Loewner 顺序的特征值比较，推导出拓扑结构的设计准则。


<details>
  <summary>Details</summary>
Motivation: 揭示异质权重条件下两种权重处理策略在收敛性和拓扑设计上的根本差异，质疑在 Euclidean 框架下得到的等价性是否鲁棒，以及是否存在更紧的收敛性保证。

Method: 提出并利用加权希尔伯特空间 L^2(λ; R^d) 的分析框架，推导出严格的收敛率界，并分析行随机矩阵与双随机矩阵在该几何下的自伴性差异及由此引入的罚项；通过 Rayleigh 商和 Loewner-order 的谱比较来得到拓扑条件和设计指南。

Result: 得到比欧几里得分析更紧的收敛界；在权重加权几何下，行随机矩阵为自伴，双随机矩阵非自伴，因而存在额外罚项来扩大共识误差，从而解释两种策略在收敛行为上的差异；给出在谱隙较小但仍可快速收敛的充要或充分条件，并给出利用 Rayleigh 商和 Loewner 比较得到的拓扑设计准则。

Conclusion: 提出了一个新的加权几何框架，揭示了谱隙之外的罚项对收敛行为的影响，并给出可操作的拓扑设计指南，帮助在异质权重环境下选择更快的分布式学习策略。

Abstract: Decentralized learning often involves a weighted global loss with heterogeneous node weights $λ$. We revisit two natural strategies for incorporating these weights: (i) embedding them into the local losses to retain a uniform weight (and thus a doubly stochastic matrix), and (ii) keeping the original losses while employing a $λ$-induced row-stochastic matrix. Although prior work shows that both strategies yield the same expected descent direction for the global loss, it remains unclear whether the Euclidean-space guarantees are tight and what fundamentally differentiates their behaviors. To clarify this, we develop a weighted Hilbert-space framework $L^2(λ;\mathbb{R}^d)$ and obtain convergence rates that are strictly tighter than those from Euclidean analysis. In this geometry, the row-stochastic matrix becomes self-adjoint whereas the doubly stochastic one does not, creating additional penalty terms that amplify consensus error, thereby slowing convergence. Consequently, the difference in convergence arises not only from spectral gaps but also from these penalty terms. We then derive sufficient conditions under which the row-stochastic design converges faster even with a smaller spectral gap. Finally, by using a Rayleigh-quotient and Loewner-order eigenvalue comparison, we further obtain topology conditions that guarantee this advantage and yield practical topology-design guidelines.

</details>


### [54] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 通过自动化管线生成1,500个基于FITD的多轮越狱场景，构建大规模基准，评估七个模型在有无对话历史条件下的鲁棒性，结果显示GPT家族对上下文敏感度高、Gemini 2.5 Flash异常强健、Claude 3 Haiku具备较强但不完美的防护；提示需要改进对叙事性操控的防御。


<details>
  <summary>Details</summary>
Motivation: 解决多轮对话攻击的可扩展数据集缺乏问题，利用心理学原理系统化评估大模型安全性；当前防御多依赖人工数据集，难以扩展，应发展可重复的、大规模的评估基准。

Method: 将FITD技巧具体化为可复现的模板，自动生成1,500个跨领域场景的多轮对话数据；在三个大型LLM家族中的七个模型上，分别在有历史（多轮）和无历史（单轮）条件下评估攻击成功率（ASR）。

Result: GPT家族在有历史上下文条件下攻击成功率显著上升，最高可增至32个百分点；Gemini 2.5 Flash几乎完全抵御此类攻击，Anthropic Claude 3 Haiku能抵御但并非完美；不同模型的上下文鲁棒性存在显著差异。

Conclusion: 安全架构对话上下文的处理存在明显差异，当前防御体系需提升对叙事性操控的鲁棒性，发展能抵御多轮叙事攻击的防御策略。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [55] [iRadioDiff: Physics-Informed Diffusion Model for Indoor Radio Map Construction and Localization](https://arxiv.org/abs/2511.20015)
*Xiucheng Wang,Tingwei Yuan,Yang Cao,Nan Cheng,Ruijin Sun,Weihua Zhuang*

Main category: cs.LG

TL;DR: iRadioDiff is a sampling-free diffusion-based framework for indoor radio-map construction, conditioned on AP positions with physics-informed prompts and multipath priors, achieving state-of-the-art RM construction and RSS-based localization with strong generalization.


<details>
  <summary>Details</summary>
Motivation: Indoor environments are heterogeneous and rich in multipath, making high-fidelity radio map construction challenging due to slow EM solvers and limitations of learning-based methods that rely on sparse measurements or homogeneous-material assumptions.

Method: A diffusion-based generative model that is sampling-free and conditioned on AP positions. It uses physics-informed prompts encoded by material reflection and transmission coefficients, and incorporates multipath-critical priors (diffraction points, strong transmission boundaries, LoS contours) through conditional channels and boundary-weighted objectives to model nonstationary field discontinuities and ensure physical consistency.

Result: Experiments show state-of-the-art performance in indoor RM construction and RSS-based indoor localization, with good generalization across layouts and material configurations.

Conclusion: iRadioDiff enables accurate, efficient construction of physically consistent indoor radio maps and demonstrates strong generalization; code is available at the provided GitHub URL.

Abstract: Radio maps (RMs) serve as environment-aware electromagnetic (EM) representations that connect scenario geometry and material properties to the spatial distribution of signal strength, enabling localization without costly in-situ measurements. However, constructing high-fidelity indoor RMs remains challenging due to the prohibitive latency of EM solvers and the limitations of learning-based methods, which often rely on sparse measurements or assumptions of homogeneous material, which are misaligned with the heterogeneous and multipath-rich nature of indoor environments. To overcome these challenges, we propose iRadioDiff, a sampling-free diffusion-based framework for indoor RM construction. iRadioDiff is conditioned on access point (AP) positions, and physics-informed prompt encoded by material reflection and transmission coefficients. It further incorporates multipath-critical priors, including diffraction points, strong transmission boundaries, and line-of-sight (LoS) contours, to guide the generative process via conditional channels and boundary-weighted objectives. This design enables accurate modeling of nonstationary field discontinuities and efficient construction of physically consistent RMs. Experiments demonstrate that iRadioDiff achieves state-of-the-art performance in indoor RM construction and received signal strength based indoor localization, which offers effective generalization across layouts and material configurations. Code is available at https://github.com/UNIC-Lab/iRadioDiff.

</details>


### [56] [Communication-Efficient Learning for Satellite Constellations](https://arxiv.org/abs/2511.20220)
*Ruxandra-Stefania Tudose,Moritz H. W. Grüss,Grace Ra Kim,Karl H. Johansson,Nicola Bastianello*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.

</details>


### [57] [Learning to Solve Weighted Maximum Satisfiability with a Co-Training Architecture](https://arxiv.org/abs/2511.19544)
*Kaidi Wan,Minghao Liu,Yong Lai*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Wepropose SplitGNN, a graph neural network (GNN)-based
  approach that learns to solve weighted maximum satisfiabil ity (MaxSAT) problem. SplitGNN incorporates a co-training
  architecture consisting of supervised message passing mech anism and unsupervised solution boosting layer. A new graph
  representation called edge-splitting factor graph is proposed
  to provide more structural information for learning, which is
  based on spanning tree generation and edge classification. To
  improve the solutions on challenging and weighted instances,
  we implement a GPU-accelerated layer applying efficient
  score calculation and relaxation-based optimization. Exper iments show that SplitGNN achieves 3* faster convergence
  and better predictions compared with other GNN-based ar chitectures. More notably, SplitGNN successfully finds solu tions that outperform modern heuristic MaxSAT solvers on
  much larger and harder weighted MaxSAT benchmarks, and
  demonstrates exceptional generalization abilities on diverse
  structural instances.

</details>


### [58] [When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics](https://arxiv.org/abs/2511.19548)
*Yiven,Zhu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, "brain-based" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies "true" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.

</details>


### [59] [Online Sparse Feature Selection in Data Streams via Differential Evolution](https://arxiv.org/abs/2511.19555)
*Ruiyang Xu*

Main category: cs.LG

TL;DR: 提出了一种在数据流中进行在线稀疏特征选择的新方法 ODESFS，结合基于潜在因子分析的缺失值填充与基于差分进化的特征重要性评估，能在缺失数据下持续 outperform 现有 OSFS/OS2FS 的方法，选出最优子集并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 在高维流数据处理中，数据缺失普遍存在，现有的 OSFS 方法在缺失数据场景下的特征评估表现不足，导致性能下降，需要在缺失数据下实现更稳健的在线特征选择。

Method: 提出 Online Differential Evolution for Sparse Feature Selection (ODESFS)。核心包含两点：一是采用潜在因子分析模型对缺失值进行填充；二是利用差分进化算法对特征的重要性进行评估以进行选择，并将其应用于数据流中的稀疏特征选择。

Result: 在六个真实数据集上进行全面实验，结果表明 ODESFS 在准确率上持续优于现有的 OSFS 与 OS2FS 方法，能够选出最优的特征子集并提升预测性能。

Conclusion: ODESFS 有效解决了带缺失值的在线稀疏特征选择问题，相较于现有方法，具有更稳健的特征评估和更高的预测准确性。

Abstract: The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.

</details>


### [60] [Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport](https://arxiv.org/abs/2511.19561)
*Zecheng Pan,Zhikang Chen,Ding Li,Min Zhang,Sen Cui,Hongshuo Jin,Luqi Tao,Yi Yang,Deheng Ye,Yu Zhang,Tingting Zhu,Tianling Ren*

Main category: cs.LG

TL;DR: 提出基于最优传输的掩码融合（OTMF）的模型合并框架，通过发现任务向量之间的公共掩码来对齐语义几何，避免直接在权重空间插值导致的分布偏移，实现对多任务的可扩展、持续融合，具有优越的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 直接在权重空间进行参数插值会引入显著的特征空间分布偏移，削弱任务特异性知识；需要一种能够对齐任务模型的语义几何结构、保留各任务结构身份且实现可扩展融合的方法。

Method: 提出OTMF框架，通过最优传输寻找任务向量的公共掩码，选择性地提取可迁移且与任务无关的组件，同时保护每个任务的独特结构身份。然后在持续融合范式下，能够增量地集成新任务向量而不回访先前任务，确保内存占用有界并实现对越来越多任务的高效融合。

Result: 在多种视觉与语言基准上进行了系统实验，结果显示OTMF在准确性和效率方面达到或接近最先进水平，证明其在模型融合中的实用性与理论价值。

Conclusion: OTMF提供了一个以最优传输为理论支撑的、可扩展的多任务模型融合框架。通过在任务向量之间学习公共掩码来对齐语义几何，克服了直接权重插值带来的分布偏移，并实现高效的持续融合与记忆高效性，同时在多领域任务上实现了优越的性能。

Abstract: Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.

</details>


### [61] [An Invariant Latent Space Perspective on Language Model Inversion](https://arxiv.org/abs/2511.19569)
*Wentao Ye,Jiaqi Hu,Haobo Wang,Xinpeng Ti,Zhiqing Xiao,Hao Chen,Liyao Li,Lei Feng,Sai Wu,Junbo Zhao*

Main category: cs.LG

TL;DR:  Inv^2A 将 LLM 视为不变解码器，训练轻量逆编码器将输出映射到去噪伪表示，并通过多输出拼接提高信息密度，利用对比对齐与有监督强化实现源不变性与循环不变性，在9数据集上BLEU平均提升4.77%，且减少对大规模逆向语料的依赖；代码可用。


<details>
  <summary>Details</summary>
Motivation:  Language model inversion（LMI）对用户隐私与系统安全构成现实威胁，但现有防御往往保护不足，迫切需要从潜在空间的一致性角度提出新型攻击框架。

Method:  提出不变潜在空间假设（ILSH）：源不变性（同源提示的多样输出应保留一致语义）与循环不变性（输入<->输出的循环映射在共享潜在空间内自洽）。Inv^2A 将 LLM 视为不变解码器，仅学习轻量的逆编码器，将输出映射到去噪的伪表示；在表示层对多个输出进行稀疏拼接以提高信息密度。训练分两阶段：对比对齐（源不变性）与有监督强化（循环不变性），还可选训练-free 的邻域搜索用于局部精细化。

Result:  在9个数据集上，Inv^2A 相比基线平均提升BLEU约4.77%，并减少对大规模逆向语料的依赖。分析还表明主流防御策略对该攻击保护有限。

Conclusion:  提出一项高效的逆向攻击框架并证实潜在空间一致性的重要性，强调需要更强的防护策略；代码与数据已在公开仓库提供。

Abstract: Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings should be self-consistent within a shared latent space (cyclic invariance). Accordingly, we present Inv^2A, which treats the LLM as an invariant decoder and learns only a lightweight inverse encoder that maps outputs to a denoised pseudo-representation. When multiple outputs are available, they are sparsely concatenated at the representation layer to increase information density. Training proceeds in two stages: contrastive alignment (source invariance) and supervised reinforcement (cyclic invariance). An optional training-free neighborhood search can refine local performance. Across 9 datasets covering user and system prompt scenarios, Inv^2A outperforms baselines by an average of 4.77% BLEU score while reducing dependence on large inverse corpora. Our analysis further shows that prevalent defenses provide limited protection, underscoring the need for stronger strategies. The source code and data involved in this paper can be found in https://github.com/yyy01/Invariant_Attacker.

</details>


### [62] [Neural Tractability via Structure: Learning-Augmented Algorithms for Graph Combinatorial Optimization](https://arxiv.org/abs/2511.19573)
*Jialiang Li,Weitong Chen,Mingyu Guo*

Main category: cs.LG

TL;DR: 提出一个将神经模型的推理效率与搜索算法的解质量保证结合起来的框架。通过参数化算法（PA）作为搜索组件，利用参数化分析识别结构上困难的部分，由神经模型提供 advisory 信号，再由 PA 搜索高效地在易于处理的部分进行系统搜索，从而获得比仅使用神经 solver 更好的解质量，且对分布外数据具有更好的泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器在推理速度和对分布内解的质量方面表现出色，但在绝对最优性方面往往落后于传统基于搜索的算法。需要一个框架在保持推理效率的同时提升解的质量，并提升对分布外数据的鲁棒性。

Method: 提出以参数化算法（PA）为核心的搜索组件。通过参数化分析识别问题实例中的结构性困难部分，将神经模型生成的 advisory 信号融入到 PA 的搜索策略中，使其能够系统地、有效地搜索剩余的结构性易解部分。该框架对神经模型的具体选择是不可知的，具备模型无关性。

Result: 在多种组合优化任务上进行实验，结果显示该框架得到的解质量优于仅使用神经求解器的方法，与商用求解器的解质量相当；并且由于将神经模型仅用于探索性 advisory 信号，框架对分布外数据具有更好的泛化能力。

Conclusion: 将推理高效的神经模型与带有解质量保证的搜索算法结合起来，可以获得比单独神经求解器更高的解质量，同时保持较强的推理效率与对新分布的鲁棒性，且框架对神经模型的选择具备一定的自由度。

Abstract: Neural models have shown promise in solving NP-hard graph combinatorial optimization (CO) problems. Once trained, they offer fast inference and reasonably high-quality solutions for in-distribution testing instances, but they generally fall short in terms of absolute solution quality compared to classical search-based algorithms that are admittedly slower but offer optimality guarantee once search finishes.
  We propose a novel framework that combines the inference efficiency and exploratory power of neural models with the solution quality guarantee of search-based algorithms. In particular, we use parameterized algorithms (PAs) as the search component. PAs are dedicated to identifying easy instances of generally NP-hard problems, and allow for practically efficient search by exploiting structural simplicity (of the identified easy instances). Under our framework, we use parameterized analysis to identify the structurally hard parts of a CO instance. The neural model handles the hard parts by generating advisory signals based on its data-driven understanding. The PA-based search component then integrates the advisory signals to systematically and efficiently searches through the remaining structurally easy parts. Notably, our framework is agnostic to the choice of neural model and produces strictly better solutions than neural solvers alone.
  We examine our framework on multiple CO tasks. Empirical results show that it achieves superior solution quality, competitive with that of commercial solvers. Furthermore, by using the neural model only for exploratory advisory signals, our framework exhibits improved out-of-distribution generalization, addressing a key limitation of existing neural CO solvers.

</details>


### [63] [Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks](https://arxiv.org/abs/2511.19636)
*Shihan Feng,Cheng Zhang,Michael Xi,Ethan Hsu,Lesia Semenova,Chudi Zhong*

Main category: cs.LG

TL;DR: 提出鲁棒的 Rashomon Concept Bottleneck 模型，训练多种准确但在概念上不同的模型，利用轻量适配器和多样性正则实现不从头重新训练即可获得多样推理的集合。


<details>
  <summary>Details</summary>
Motivation: 在深度模型中普遍存在的 Rashomon 效应导致多种近似最优解在性能上相近且行为差异隐匿。需要一种框架来揭示、比较并对齐不同解的推理过程，以便审计和理解模型的决策依据。

Method: 在不从零开始重新训练的情况下，结合轻量级适配器模块与多样性正则化训练目标，学习一组准确但使用不同人类可理解概念的模型。通过为每个模型引入不同概念信号并对其依赖进行正则化，实现对同一任务的多样推理路径。

Result: 得到一组在同一任务上性能等效但推理过程不同的概念基础模型，能够揭示概念依赖和决策机制的差异，便于审计、比较与对齐。

Conclusion: 提供一个系统化探索深度模型数据驱动推理多样性的框架，为 equally accurate 方案间的对齐与审计提供新工具。

Abstract: Modern neural networks rarely have a single way to be right. For many tasks, multiple models can achieve identical performance while relying on different features or reasoning patterns, a property known as the Rashomon Effect. However, uncovering this diversity in deep architectures is challenging as their continuous parameter spaces contain countless near-optimal solutions that are numerically distinct but often behaviorally similar. We introduce Rashomon Concept Bottleneck Models, a framework that learns multiple neural networks which are all accurate yet reason through distinct human-understandable concepts. By combining lightweight adapter modules with a diversity-regularized training objective, our method constructs a diverse set of deep concept-based models efficiently without retraining from scratch. The resulting networks provide fundamentally different reasoning processes for the same predictions, revealing how concept reliance and decision making vary across equally performing solutions. Our framework enables systematic exploration of data-driven reasoning diversity in deep models, offering a new mechanism for auditing, comparison, and alignment across equally accurate solutions.

</details>


### [64] [Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles](https://arxiv.org/abs/2511.19656)
*Kaiyi Ji*

Main category: cs.LG

TL;DR: 在光滑的非凸-强凸双层优化设定下，提出新的难例并给出严格的下界：确定性一阶零保持算法至少需要 Ω(κ^{3/2} ε^{-2}) 次调用以找到 ε-近似驻点；随机情形至少需要 Ω(κ^{5/2} ε^{-4}) 次随机一阶查询。结果表明双层优化的上下界之间存在显著差距，并提示在简化情形下（如二次下层目标）仍需研究以确定标准一阶查询下的最优复杂度。


<details>
  <summary>Details</summary>
Motivation: 弥补双层优化中下界研究的不足，量化在标准一阶/随机一阶模型下的基本复杂度，并与现有单层非凸和非凸-强凸极值/极小值的下界比较，揭示更高的理论极限。

Method: 构造新的困难实例，针对光滑的非凸-强凸双层目标；在确定性和随机一阶查询模型下，建立零保持（zero-respecting）算法的最坏情形并推导时间复杂度下界；对 κ（条件数）和 ε 的依赖给出 Ω 界。

Result: 给出下界 Ω(κ^{3/2} ε^{-2})（确定性）和 Ω(κ^{5/2} ε^{-4})（随机）; 这些下界超过了目前已知的单层非凸和非凸-强凸极值问题的下界，显示双层优化的理论极限远未被满足。

Conclusion: 当前上界与下界之间差距显著，需进一步研究以缩小差距；即便在简化情形（如二次下层）也值得系统研究，以明确在标准一阶模型下的最优复杂度。

Abstract: Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $Ω(κ^{3/2}ε^{-2})$ oracle calls to find an $ε$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $Ω(κ^{5/2}ε^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.

</details>


### [65] [Structured Noise Modeling for Enhanced Time-Series Forecasting](https://arxiv.org/abs/2511.19657)
*Sepideh Koohfar*

Main category: cs.LG

TL;DR: 提出一个forecast-blur-denoise框架，通过可学习的高斯过程模块生成平滑相关扰动，辅以精细化模型恢复高分辨率时间细节；联合训练实现分工协作，且可作为轻量化的预训练增强模块使用。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列具有多尺度的时序模式（宏观趋势到细粒度波动），现有神经模型难以同时捕捉这些交互动态，导致预测不稳定和下游应用可靠性下降。

Method: 引入forecast-blur-denoise框架：用可学习的高斯过程模块生成平滑的相关扰动，促使预测骨干捕捉长期结构；专门的精细化模型恢复高分辨率时间细节；端到端联合训练实现自然的能力分工，且blur-denoise层可作为轻量化的预训练增强层。

Result: 在电力、交通和太阳能数据集上实现多步预测的准确性和稳定性提升；模块化设计便于对预训练模型的轻量化增强，适用于数据受限场景。

Conclusion: 增强细粒度时间预测的可靠性与可解释性，为能源、基础设施等时间驱动决策系统中的可信AI提供支持。

Abstract: Time-series forecasting remains difficult in real-world settings because temporal patterns operate at multiple scales, from broad contextual trends to fast, fine-grained fluctuations that drive critical decisions. Existing neural models often struggle to represent these interacting dynamics, leading to unstable predictions and reduced reliability in downstream applications. This work introduces a forecast-blur-denoise framework that improves temporal fidelity through structured noise modeling. The approach incorporates a learnable Gaussian Process module that generates smooth, correlated perturbations, encouraging the forecasting backbone to capture long-range structure while a dedicated refinement model restores high-resolution temporal detail. Training the components jointly enables natural competence division and avoids the artifacts commonly produced by isotropic corruption methods. Experiments across electricity, traffic, and solar datasets show consistent gains in multi-horizon accuracy and stability. The modular design also allows the blur-denoise layer to operate as a lightweight enhancement for pretrained models, supporting efficient adaptation in limited-data scenarios. By strengthening the reliability and interpretability of fine-scale temporal predictions, this framework contributes to more trustworthy AI systems used in forecasting-driven decision support across energy, infrastructure, and other time-critical domains.

</details>


### [66] [Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds](https://arxiv.org/abs/2511.19664)
*Jiaxin Shi,Michalis K. Titsias*

Main category: cs.LG

TL;DR: 提出将重加权损失视为多层时间依赖的变分下界级联的理论解释，能够提升数据对数似然的下界并降低数据-模型KL散度，从而得到对任何扩散模型可应用的重加权目标，尤其在掩码化离散扩散的像素级图像建模中取得显著改善，接近连续扩散模型的样本质量。


<details>
  <summary>Details</summary>
Motivation: 解释广泛用于扩散模型的重加权损失的理论来源，并提供一个统一的框架，将连续高斯扩散与掩码化离散扩散纳入同一重加权目标，从而提升训练效能与样本质量。

Method: 构建一个时间依赖的变分下界的级联，逐步对数据对数似然进行下界推导，证明该级联下界优于标准ELBO并降低数据-模型KL；通过将这些下界组合，得到可应用于任意扩散模型的重加权目标；在掩码化扩散中具体实现并比较。

Result: 在掩码化扩散的像素级图像建模中，使用新的重加权训练损失显著优于以往的训练损失，样本质量接近连续扩散模型；提供了对掩码化图像模型常用简单加权策略的理论解释。

Conclusion: 该框架不仅统一了连续与离散/掩码化扩散的重加权目标，还理论化地支撑了常见的掩码化图像模型加权策略，并在实际任务中提升了训练效果与生成质量。

Abstract: We derive a new theoretical interpretation of the reweighted losses that are widely used for training diffusion models. Our method is based on constructing a cascade of time-dependent variational lower bounds on the data log-likelihood, that provably improves upon the standard evidence lower bound and results in reduced data-model KL-divergences. Combining such bounds gives rise to reweighted objectives that can be applied to any generative diffusion model including both continuous Gaussian diffusion and masked (discrete) diffusion models. Then, we showcase this framework in masked diffusion and report significant improvements over previous training losses in pixel-space image modeling, approaching sample quality comparable to continuous diffusion models. Our results also provide a theoretical justification for the simple weighting scheme widely used in masked image models.

</details>


### [67] [TiCT: A Synthetically Pre-Trained Foundation Model for Time Series Classification](https://arxiv.org/abs/2511.19694)
*Chin-Chia Michael Yeh,Uday Singh Saini,Junpeng Wang,Xin Dai,Xiran Fan,Jiarui Sun,Yujie Fan,Yan Zheng*

Main category: cs.LG

TL;DR: TiCT（Time-series in-Context Transformer）是一种基于Transformer的时间序列分类模型，通过仅在推理阶段使用上下文示例完成分类，而不对模型权重进行更新。它在合成数据上进行预训练，结合Bit编码的标签表示和输出注意力机制来处理任意类别数，并通过Mixup风格的数据增强提升泛化与噪声鲁棒性。在UCR数据集上的评估显示，与有监督方法相比具有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 随着时间序列数据广泛存在，出现了对通用 foundation 模型用于分类的强烈需求。然而，针对大规模时间序列模型的工作多聚焦于预测任务，缺乏针对分类任务的、无需微调就可使用的解决方案，且标注成本高。

Method: 提出TiCT架构：1) 采用可扩展的基于比特的标签编码和特殊的输出注意力机制，能够处理任意数量的类别；2) 通过合成数据进行预训练，结合受到Mixup启发的过程与数据增强，提升模型的泛化能力与对噪声的鲁棒性。推理阶段仅使用上下文示例，不更新模型权重。

Result: 在UCR数据集上的广泛评估表明，TiCT在分类任务上达到与最先进的有监督方法相竞争的性能，同时保持无权重更新的推理特性。

Conclusion: 证明了在合成数据上预训练、并通过以上下文为核心的分类机制实现无微调推断的时间序列分类的可行性，拓宽了 foundation 模型在时间序列领域的应用场景，显著降低标注成本。

Abstract: The ubiquity of time series data creates a strong demand for general-purpose foundation models, yet developing them for classification remains a significant challenge, largely due to the high cost of labeled data. Foundation models capable of in-context learning (ICL) offer a powerful solution, adapting to new tasks with minimal examples and reducing the need for extensive retraining. However, prior work on large-scale time series models has predominantly focused on forecasting, leaving a critical gap for versatile, fine-tuning-free classification. To address this, we introduce TiCT (Time-series in-Context Transformer), a transformer-based model pre-trained exclusively on synthetic data to perform in-context classification. We make two primary technical contributions: 1) a novel architecture featuring a scalable bit-based label encoding and a special output attention mechanism to handle an arbitrary number of classes; and 2) a synthetic pre-training framework that combines a Mixup-inspired process with data augmentation to foster generalization and noise invariance. Extensive evaluations on the UCR Archive show that TiCT achieves competitive performance against state-of-the-art supervised methods. Crucially, this is accomplished using only in-context examples at inference time, without updating a single model weight.

</details>


### [68] [Training-Free Active Learning Framework in Materials Science with Large Language Models](https://arxiv.org/abs/2511.19730)
*Hongchen Wang,Rafael Espinosa Castañeda,Jay R. Werber,Yao Fehlis,Edward Kim,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 提出一个基于大语言模型的主动学习框架LLM-AL，通过文本描述或简短数字输入进行迭代式few-shot提示，与传统ML对比，在四个材料数据集上显著降低实验数量（>70%）并优于传统模型，且在非确定性影响下表现稳定。


<details>
  <summary>Details</summary>
Motivation: 解决传统主动学习在冷启动和领域特定特征工程上的局限，利用LLMs的预训练知识和通用表示来从文本描述直接提出实验，提升泛化性和可解释性。

Method: 在迭代的few-shot设置下，提出两种提示策略：1) 针对结构化/成分性特征的数据使用简洁数值输入；2) 针对实验/过程特征的数据使用扩展文本描述以提供上下文。将LLM-AL与传统ML在四个材料数据集上进行对比，评估稳定性与鲁棒性。

Result: 在所有数据集上，LLM-AL把达到顶尖候选的实验数量减少超过70%，并始终优于传统ML。LLM-AL倾向于更广泛、探索性搜索，同时以更少迭代达到最优解。非确定性导致的稳定性方面，表现与传统ML波动在可接受范围内。

Conclusion: LLM-AL可作为通用替代的主动学习管线，提升实验选择的效率和可解释性，同时具备潜在的LLM驱动自动发现能力。

Abstract: Active learning (AL) accelerates scientific discovery by prioritizing the most informative experiments, but traditional machine learning (ML) models used in AL suffer from cold-start limitations and domain-specific feature engineering, restricting their generalizability. Large language models (LLMs) offer a new paradigm by leveraging their pretrained knowledge and universal token-based representations to propose experiments directly from text-based descriptions. Here, we introduce an LLM-based active learning framework (LLM-AL) that operates in an iterative few-shot setting and benchmark it against conventional ML models across four diverse materials science datasets. We explored two prompting strategies: one using concise numerical inputs suited for datasets with more compositional and structured features, and another using expanded descriptive text suited for datasets with more experimental and procedural features to provide additional context. Across all datasets, LLM-AL could reduce the number of experiments needed to reach top-performing candidates by over 70% and consistently outperformed traditional ML models. We found that LLM-AL performs broader and more exploratory searches while still reaching the optima with fewer iterations. We further examined the stability boundaries of LLM-AL given the inherent non-determinism of LLMs and found its performance to be broadly consistent across runs, within the variability range typically observed for traditional ML approaches. These results demonstrate that LLM-AL can serve as a generalizable alternative to conventional AL pipelines for more efficient and interpretable experiment selection and potential LLM-driven autonomous discovery.

</details>


### [69] [DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning](https://arxiv.org/abs/2511.19750)
*Julien T. T. Vignoud,Valérian Rousset,Hugo El Guedj,Ignacio Aleman,Walid Bennaceur,Batuhan Faik Derinbay,Eduard Ďurech,Damien Gengler,Lucas Giordano,Felix Grimberg,Franziska Lippoldt,Christina Kopidaki,Jiafan Liu,Lauris Lopata,Nathan Maire,Paul Mansat,Martin Milenkoski,Emmanuel Omont,Güneş Özgün,Mina Petrović,Francesco Posa,Morgan Ridel,Giorgio Savini,Marcel Torne,Lucas Trognon,Alyssa Unell,Olena Zavertiaieva,Sai Praneeth Karimireddy,Tahseen Rabbani,Mary-Anne Hartley,Martin Jaggi*

Main category: cs.LG

TL;DR: 提出开源的分布式协作学习平台 DISCO，能在浏览器端本地训练模型、无需分享原始数据、面向非技术用户，支持联邦与去中心化范式及多种聚合策略，提升隐私保护与模型可及性。


<details>
  <summary>Details</summary>
Motivation: 因隐私、知识产权与法律约束等原因，数据难以共享，导致统计能力分散、模型偏差与可及性不平等。本研究旨在通过无需数据分享、易上手且跨平台的解决方案，扩大参与范围与公平性。

Method: 通过网页应用在本地浏览器训练模型，跨平台（含手机）出厂即用。模块化设计提供联邦与去中心化训练范式、不同隐私保障等级，以及多种权重聚合策略，支持模型个性化与偏差鲁棒性。代码在 GitHub（epfml/disco），展示网页界面在 discolab.ai。

Result: 提出一个系统级框架与实现要点，强调可访问性、隐私保护与可扩展性；尚未在摘要中给出具体的实证评估结果细节。

Conclusion: DISCO 提供一个无数据共享的开源协作学习解决方案，面向非技术用户，跨平台且具备隐私保护与定制化能力。

Abstract: Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai

</details>


### [70] [When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements](https://arxiv.org/abs/2511.19794)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出了一种保守的、与计算预算友好的评估协议，用于在机器学习中对小幅度改进进行统计显著性检验。通过成对多种子运行、BCa自举置信区间以及对每个种子差异的符号置换检验，在有限的预算下评估0.6-2.0点的改进是否真实可信。


<details>
  <summary>Details</summary>
Motivation: 当前的ML研究常报告在基准上1-2个百分点的改进，但这些改进高度受随机种子、数据顺序和实现细节影响，且往往缺乏不确定性估计和显著性检验，难以判断是否是真正的算法进步。

Method: 在现实计算预算下，提出一种基于成对多种子（paired multi-seed）运行、偏倚校正与加速的BCa自举置信区间，以及对每个种子差异进行符号翻转的置换检验的评估协议。协议保守，作为避免夸大结论的护栏。

Result: 在CIFAR-10、CIFAR-10N、AG News上用合成的无改进、少量收益和中等收益场景进行了实例验证。单次运行和非配对t检验在0.6-2.0点改进的情况下经常被认为显著，且在文本数据上尤为明显；但仅有3个种子时，成对协议在这些设置中从未宣称显著。

Conclusion: 对于在紧张预算下追求小幅改进的研究，采用保守的评估默认更安全，能有效避免过度声称。

Abstract: Recent machine learning papers often report 1-2 percentage point improvements from a single run on a benchmark. These gains are highly sensitive to random seeds, data ordering, and implementation details, yet are rarely accompanied by uncertainty estimates or significance tests. It is therefore unclear when a reported +1-2% reflects a real algorithmic advance versus noise.
  We revisit this problem under realistic compute budgets, where only a few runs are affordable. We propose a simple, PC-friendly evaluation protocol based on paired multi-seed runs, bias-corrected and accelerated (BCa) bootstrap confidence intervals, and a sign-flip permutation test on per-seed deltas. The protocol is intentionally conservative and is meant as a guardrail against over-claiming.
  We instantiate it on CIFAR-10, CIFAR-10N, and AG News using synthetic no-improvement, small-gain, and medium-gain scenarios. Single runs and unpaired t-tests often suggest significant gains for 0.6-2.0 point improvements, especially on text. With only three seeds, our paired protocol never declares significance in these settings. We argue that such conservative evaluation is a safer default for small gains under tight budgets.

</details>


### [71] [Terminal Velocity Matching](https://arxiv.org/abs/2511.19797)
*Linqi Zhou,Mathias Parger,Ayaan Haque,Jiaming Song*

Main category: cs.LG

TL;DR: TVM generalizes flow matching to terminal-time transitions to enable high-fidelity one-/few-step diffusion models, with a theoretical 2-Wasserstein bound under Lipschitzness, practical fixes for non-Lipschitz transformers, and efficient fused-attention for Jacobian-vector backpropagation; achieves state-of-the-art FID on ImageNet with 1–4 NFEs.


<details>
  <summary>Details</summary>
Motivation: 解决多步扩散模型在推理成本与训练稳定性之间的权衡，提升单步/少步生成的保真度，并将流式匹配推广到终端时间。

Method: 提出 Terminal Velocity Matching (TVM)，对任意两个扩散时间步之间的过渡建模，并在终端时间进行正则化；在模型为 Lipschitz 时给出对数据与模型分布的2-Wasserstein距离上界；鉴于 Diffusion Transformer 缺乏 Lipschitz 性，给出最小的结构改动以实现稳定的单阶段训练；提出融合注意力核以实现对 Jacobian-Vector Product 的反向传播，从而提高在变换器上的训练效率与可扩展性。

Result: 在 ImageNet-256x256 上，1 次功能评估（NFE）得到 3.29 的 FID，4 次 NFE 得到 1.99 的 FID；在 ImageNet-512x512 上，1-NFE FID 为 4.32，4-NFE FID 为 2.94，达到单/少步模型从头训练的最新性能。

Conclusion: TVM 为流式匹配提供了一个在理论与实证上的新范式，兼具对偶时间的终端正则化和实际可行的高效训练策略，显著提升了单步/少步扩散模型在大尺度数据集上的性能。

Abstract: We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.

</details>


### [72] [Scalable Data Attribution via Forward-Only Test-Time Inference](https://arxiv.org/abs/2511.19803)
*Sibo Ma,Julian Nyarko*

Main category: cs.LG

TL;DR: 提出一种快速数据归因方法，在不对每次查询进行反向传播的情况下，保持与第一阶对比目标的一致性。通过在训练阶段进行短期梯度传播来模拟每个训练样本的参数影响，随后仅使用前向评估就能对任意查询得到归因。实现上把推理阶段的计算转移到训练阶段的仿真中，贴合实际部署场景。


<details>
  <summary>Details</summary>
Motivation: 在大规模模型中实现数据归因以便调试、审计和数据估值，但传统影响函数方法在推理时需要昂贵的反向传播或Hessian求逆，难以扩展。本文旨在在保持一阶对比目标的前提下提升可扩展性，使归因成本在推理端大幅降低。

Method: 在训练阶段对每个训练样本进行短期梯度传播以仿真其对模型参数的影响；训练完成后，采用仅需前向计算的方式对任意查询进行归因读取，不再在推理时进行反向传播。设计上将大量推理成本转移至训练阶段的仿真，与现实部署（大规模查询、有限数据源）相吻合。

Result: 在标准MLP基准上，该归因估计器在LOO与LDS等归因指标上达到与或超过如TRAK等先进基线，并且推理成本数量级降低。该方法在保持影响函数的保真度同时实现第一阶的可扩展性。

Conclusion: 将影响函数的保真度与第一阶可扩展性结合，提出可在大规模预训练模型中实现实时数据归因的理论框架与实用方法。

Abstract: Data attribution seeks to trace model behavior back to the training examples that shaped it, enabling debugging, auditing, and data valuation at scale. Classical influence-function methods offer a principled foundation but remain impractical for modern networks because they require expensive backpropagation or Hessian inversion at inference. We propose a data attribution method that preserves the same first-order counterfactual target while eliminating per-query backward passes. Our approach simulates each training example's parameter influence through short-horizon gradient propagation during training and later reads out attributions for any query using only forward evaluations. This design shifts computation from inference to simulation, reflecting real deployment regimes where a model may serve billions of user queries but originate from a fixed, finite set of data sources (for example, a large language model trained on diverse corpora while compensating a specific publisher such as the New York Times). Empirically, on standard MLP benchmarks, our estimator matches or surpasses state-of-the-art baselines such as TRAK on standard attribution metrics (LOO and LDS) while offering orders-of-magnitude lower inference cost. By combining influence-function fidelity with first-order scalability, our method provides a theoretical framework for practical, real-time data attribution in large pretrained models.

</details>


### [73] [Provably Outlier-resistant Semi-parametric Regression for Transferable Calibration of Low-cost Air-quality Sensors](https://arxiv.org/abs/2511.19810)
*Divyansh Chaurasia,Manoj Daram,Roshan Kumar,Nihal Thukarama Rao,Vipul Sangode,Pranjal Srivastava,Avnish Tripathi,Shoubhik Chakraborty,Akanksha,Ambasht Kumar,Davender Sethi,Sachchida Nand Tripathi,Purushottam Kar*

Main category: cs.LG

TL;DR: RESPIRE是一种鲁棒且可解释的低成本CO传感器校准方法，能在跨站点、跨季节和跨传感器环境中提高预测，与法规级监测对比；并公开代码。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多站点部署中低成本传感器校准成本高、困难的问题；特别是在地理多样性情况下，传统基线校准对跨域泛化能力不足。

Method: 提出RESPIRE校准框架，具备对异常值鲁棒的训练算法、可解释的模型，以及可标记模型过拟合的能力。基于四个站点、两季、六传感器包的数据，对LCAQ传感器进行CO浓度的现场标定，同时与监管级监测器对比。开源实现代码在GitHub。

Result: 在跨站点、跨季节、跨传感器设置中的预测性能有所提升；四站点、两季、六传感器包的实证数据表明方法的稳健性和泛化能力；实现可重复性，代码公开.

Conclusion: RESPIRE为LCAQ传感器的可大规模部署提供鲁棒且可解释的校准方案，降低现场人工校准成本，改善跨域泛化，并通过开源代码促进采用与复现。

Abstract: We present a case study for the calibration of Low-cost air-quality (LCAQ) CO sensors from one of the largest multi-site-multi-season-multi-sensor-multi-pollutant mobile air-quality monitoring network deployments in India. LCAQ sensors have been shown to play a critical role in the establishment of dense, expansive air-quality monitoring networks and combating elevated pollution levels. The calibration of LCAQ sensors against regulatory-grade monitors is an expensive, laborious and time-consuming process, especially when a large number of sensors are to be deployed in a geographically diverse layout. In this work, we present the RESPIRE technique to calibrate LCAQ sensors to detect ambient CO (Carbon Monoxide) levels. RESPIRE offers specific advantages over baseline calibration methods popular in literature, such as improved prediction in cross-site, cross-season, and cross-sensor settings. RESPIRE offers a training algorithm that is provably resistant to outliers and an explainable model with the ability to flag instances of model overfitting. Empirical results are presented based on data collected during an extensive deployment spanning four sites, two seasons and six sensor packages. RESPIRE code is available at https://github.com/purushottamkar/respire.

</details>


### [74] [GED-Consistent Disentanglement of Aligned and Unaligned Substructures for Graph Similarity Learning](https://arxiv.org/abs/2511.19837)
*Zhentao Zhan,Xiaoliang Xu,Jingjing Wang,Junmei Wang*

Main category: cs.LG

TL;DR: 提出 GCGSim，一致性地学习 GED 的图相似性，通过图级匹配和子结构级编辑成本，克服基于节点嵌入的局限性。


<details>
  <summary>Details</summary>
Motivation: GED 的本质是全局结构对齐，但现有基于 GNN 的方法以节点嵌入为中心，难以捕捉全局最优对齐并易将编辑成本误归因于局部节点信号，从而导致匹配不充分和成本误解。

Method: 提出 GCGSim 框架，围绕图级匹配和子结构级编辑成本建立 GED 一致的图相似性学习，通过解耦子结构表示实现更可解释的对齐与成本估计。并在四个基准数据集上进行广泛实验以验证性能。

Result: 在四个基准数据集上达到或接近最新的性能水平，且分析表明所学的子结构表示具备解耦性和语义意义，有助于提升 GED 估计的准确性与可解释性。

Conclusion: GCGSim 通过 GED 一致性视角进行图相似性学习，有效解决节点级信号干扰与全局对齐建模不足的问题，且提供更具解释性的子结构表征。

Abstract: Graph Similarity Computation (GSC) is a fundamental graph related task where Graph Edit Distance (GED) serves as a prevalent metric. GED is determined by an optimal alignment between a pair of graphs that partitions each into aligned (zero-cost) and unaligned (cost-incurring) substructures. Due to NP-hard nature of exact GED computation, GED approximations based on Graph Neural Network(GNN) have emerged. Existing GNN-based GED approaches typically learn node embeddings for each graph and then aggregate pairwise node similarities to estimate the final similarity. Despite their effectiveness, we identify a mismatch between this prevalent node-centric matching paradigm and the core principles of GED. This discrepancy leads to two critical limitations: (1) a failure to capture the global structural correspondence for optimal alignment, and (2) a misattribution of edit costs driven by spurious node level signals. To address these limitations, we propose GCGSim, a GED-consistent graph similarity learning framework centering on graph-level matching and substructure-level edit costs. Specifically, we make three core technical contributions. Extensive experiments on four benchmark datasets show that GCGSim achieves state-of-the-art performance. Our comprehensive analyses further validate that the framework effectively learns disentangled and semantically meaningful substructure representations.

</details>


### [75] [Cisco Time Series Model Technical Report](https://arxiv.org/abs/2511.19841)
*Liang Gou,Archit Khare,Praneet Pabolu,Prachi Patel,Joseph Ross,Hercy Shen,Yuhan,Song,Jingze Sun,Kristal Curtis,Vedant Dharnidharka,Abhinav Mathur,Hao Yang*

Main category: cs.LG

TL;DR: 提出了 Cisco Time Series Model，一种单变量的零-shot 预测模型，基于 TimesFM 的多分辨率解码器结构，作为时间序列 foundation 模型。已在超大规模数据集上训练，优于可观测性数据集上的基线，在通用预测基准上表现相近，并且在长上下文输入下预测更准确。


<details>
  <summary>Details</summary>
Motivation: 解决单变量时间序列的零-shot预测问题，并通过多分辨率输入的结构来提升在可观测性域中的预测能力和对长上下文的利用。希望将时间序列模型提升为 foundation 模型，通过跨域数据和大规模预训练获得更强的泛化能力。

Method: 在现有的 TimesFM 基础上，设计并实现一个支持多分辨率输入的解码器-只有模型，将其整合为一个多分辨率的时间序列 foundation 模型。对模型进行大规模预训练，数据点数量超过 3e11，其中来自可观测性域的数据占比超过一半。通过在可观测性数据集和通用基准 GIFT-Eval 上进行定量和定性评估，验证其性能。

Result: 在可观测性数据集上获得优于现有方法的性能，同时在通用预测基准上保持与强基线相近的性能；多分辨率结构显著提升对长上下文输入的预测准确性。

Conclusion: 多分辨率解码器结构有助于提升单变量时间序列零-shot预测的泛化能力，尤其在可观测性域和长上下文场景中表现更佳，显示出成为时间序列领域 foundation 模型的潜力。

Abstract: We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.

</details>


### [76] [Frailty-Aware Transformer for Recurrent Survival Modeling of Driver Retention in Ride-Hailing Platforms](https://arxiv.org/abs/2511.19893)
*Shuoyan Xu,Yu Zhang,Eric J. Miller*

Main category: cs.LG

TL;DR: 提出 FACT，一种将空闲行为建模为重复生存过程的 Frailty-Aware Cox Transformer，结合因果掩蔽的 Transformer 与驾驶员嵌入，能捕捉长期依赖与个体异质性，在多伦多数据上实现最优的时间依赖性评估。


<details>
  <summary>Details</summary>
Motivation: 在 ride-hailing 平台中，驾驶员空闲行为可视为高频的重复事件，但现有生存分析多用于单次事件，且难以捕捉长期依赖和司机异质性，因此需要一个能处理重复事件、长时依赖与异质性的模型。

Method: 将 idle 行为建模为重复生存过程，提出 Frailty-Aware Cox Transformer (FACT)：使用 Transformer 捕捉长期时序依赖并采用因果掩蔽来保证因果性；加入驾驶员特定嵌入以建模潜在异质性；并以生存分析框架融合节律性与群体协变量，针对重复事件进行预测。

Result: 在多伦多数据集上，FACT 达到最高的时间依赖性 C 指数并具有最低的 Brier Score，显著优于经典和深度学习生存模型。

Conclusion: 该方法可实现更精准的风险估计，支持平台的留存策略与政策层面的洞见。

Abstract: Ride-hailing platforms are characterized by high-frequency, behavior-driven environments. Although survival analysis has been applied to recurrent events in other domains, its use in modeling ride-hailing driver behavior remains largely unexplored. This study formulates idle behavior as a recurrent survival process using large-scale platform data and proposes a Transformer-based framework that captures long-term temporal dependencies with causal masking and incorporates driver-specific embeddings to model latent heterogeneity. Results on Toronto ride-hailing data demonstrate that the proposed Frailty-Aware Cox Transformer (FACT) achieves the highest time-dependent C-indices and lowest Brier Scores, outperforming classical and deep learning survival models. This approach enables more accurate risk estimation, supports platform retention strategies, and provides policy-relevant insights.

</details>


### [77] [EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning](https://arxiv.org/abs/2511.19935)
*Songlin Zhao,Michael Pitts,Zhuwei Qin*

Main category: cs.LG

TL;DR: 提出 EfficientXpert，结合前向剪枝掩码（Foresight Mask）和高效的适配器更新算法（Partial Brain Surgeon），在 LoRA 微调中实现从通用模型到稀疏、域适应专家的一步转换；在健康与法律任务中，在 40% 稀疏下保留接近 Dense 的性能并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的持续增长，面向专业领域（如法律、医疗、金融）的定制化变体对资源的需求与部署难度日益增加。现有的模型剪枝方法要么对不同领域的泛化能力不足，要么带来较高的开销，因此亟需域自适应、低开销的剪枝与适配策略。

Method: 提出 EfficientXpert：采用传播感知的剪枝准则 Foresight Mask，以及高效的适配器更新算法 Partial Brain Surgeon，将两者集成到 LoRA 微调流程中，使得对通用预训练模型可在单步内转换为稀疏、领域专用的专家模型。

Result: 在健康与法律任务上，方法可在 40% 稀疏度下保持接近 98% 的密集模型性能，并显著优于现有最先进方法。进一步分析显示领域相关的结构性偏移显著削弱了通用剪枝掩码的效果，强调需要针对每个领域的自适应剪枝策略。

Conclusion: 域相关的结构性变化降低了通用剪枝掩码的有效性，需发展面向具体领域的自适应剪枝与快速适配机制，以实现高效、域定制的模型压缩与部署。

Abstract: The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.

</details>


### [78] [Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning](https://arxiv.org/abs/2511.19941)
*Shenjun Zhong,Zhifeng Chen,Zhaolin Chen*

Main category: cs.LG

TL;DR: 使用强化学习优化MRF的翻转角时间表，得到非周期性优化模式，提升指纹区分度，并可能缩短重复时间以加速成像。


<details>
  <summary>Details</summary>
Motivation: MRF的信号依赖于瞬态动态和可调采集参数，优化翻转角是高维序列决策问题，RL有潜力自动化参数选择以提高指纹不可辨性分离度。

Method: 构建一个RL框架来优化翻转角序列，训练以提高指纹在参数空间中的可分离性；观察得到的最优序列呈现非周期性模式；评估对指纹区分度的影响，及对重复时间的影响潜力。

Result: 学习得到的翻转角序列在指纹区分度上提升，且表现出非周期性模式；可能减少重复时间，提升采集速度。

Conclusion: RL优化的翻转角序列可显著提升MRF的指纹可区分性并有望加速成像，但需要进一步在实际数据和硬件约束下验证。

Abstract: Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.

</details>


### [79] [Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning](https://arxiv.org/abs/2511.19942)
*Jingchu Gai,Guanning Zeng,Huaqing Zhang,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 提出差分平滑（differential smoothing）作为一种 principled RL 微调正则化方法，解决大语言模型在 RL 微调中出现的多样性崩溃问题，通过在正确轨迹上修正奖励，理论与实验证明其在正确性和多样性之间取得权衡，优于 vanilla RL 与熵基启发式，在1B-7B参数模型和 CountDown、AIME24 等任务上实现稳定提升。


<details>
  <summary>Details</summary>
Motivation: RL 微调往往导致输出缺乏多样性（多样性崩溃），现有启发式方法往往在正确性与多样性之间取舍且对任务不稳健，需要一个具有理论支撑且普适有效的解决方案。

Method: 在理论层面分析 RL 微调中的选择偏差与强化偏差如何导致多样性崩溃；提出 differential smoothing，对奖励进行选择性修改，仅对正确轨迹进行干预，并给出可证明的提升正确性与多样性的理论结果；通过大量实验验证该方法在不同规模模型和任务上的有效性，且优于常用熵基 heuristics。

Result: 理论上给出多样性崩溃的产生机制（选择偏差和强化偏差）以及对正确轨迹进行奖励修正的有效性；在 1B–7B 参数量级的模型，以及 CountDown 与现实世界的数学推理任务中，差分平滑实现了稳定的性能提升，Pass@1 与 Pass@k 均有提升，且在 AIME24 数据集上最高提升约 6.7%。

Conclusion: 差分平滑提供了一个理论扎根、普适有效的解决 RL 微调中多样性崩溃的方法，优于 vanilla RL 与熵基启发式，建议在实际应用中优先采用以兼顾正确性与多样性。

Abstract: It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.

</details>


### [80] [ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models](https://arxiv.org/abs/2511.19959)
*Yujia Wang,Yuanpu Cao,Jinghui Chen*

Main category: cs.LG

TL;DR: ParaBlock通过在联邦区块坐标下降中引入计算与通信的双线程并行实现，提高对LLM微调的通信效率，同时保持与标准的联邦区块坐标下降相同的收敛速率，并在指令遵循和数学推理任务上获得良好性能。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的联邦学习场景中，单个区块通常包含大量参数，导致资源受限的客户端通信延迟成为瓶颈。现有的联邦区块坐标下降在高维模型下难以高效通信，需要新的机制来降低通信开销并保留收敛性。

Method: 提出ParaBlock：为通信与计算建立两条并行线程，使得区块坐标下降的更新与通信可以并行进行；给出理论收敛性证明，表明在等价设定下其收敛速率与标准方法一致。

Result: 理论上，ParaBlock保持与标准Fed BCD相同的收敛速率；在对通用指令跟随和数学推理任务的LLM微调上，实验结果显示ParaBlock在保持强性能的同时显著提升通信效率。

Conclusion: ParaBlock为LLM的联邦训练/微调提供一种高效的通信方案，在不牺牲收敛性和模型性能的前提下，显著降低了通信负担，具有推广到大规模联邦学习的潜力。

Abstract: Federated learning (FL) has been extensively studied as a privacy-preserving training paradigm. Recently, federated block coordinate descent scheme has become a popular option in training large-scale models, as it allows clients to train only a subset of the model locally instead of the entire model. However, in the era of large language models (LLMs), even a single block can contain a significant number of parameters, posing substantial communication latency, particularly for resource-constrained clients. To address this challenge in federated training/fine-tuning LLMs, we propose ParaBlock, a novel approach that establishes two parallel threads for communication and computation to enhance communication efficiency. We theoretically prove that the proposed ParaBlock achieves the same convergence rate as the standard federated block coordinate descent methods. Empirical evaluations on fine-tuning LLMs on general instruction following and mathematical reasoning confirm that ParaBlock not only maintains strong performance but also significantly improves communication efficiency.

</details>


### [81] [Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.19966)
*Yujia Wang,Fenglong Ma,Jinghui Chen*

Main category: cs.LG

TL;DR: 提出 FedEcho：在异步联邦学习中通过不确定性感知蒸馏来提升在大延迟和数据异质性下的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 异步FL可减少等待但易受慢参与者的过时更新和数据分布异质性影响，现有方法往往只解决其中一个问题，导致两者冲突。

Method: 提出 FedEcho 框架，结合不确定性感知蒸馏。服务器通过评估来自慢客户端的预测不确定性，动态调节其预测影响力，在保留多样信息的同时降低过时更新的负面作用。

Result: 通过大量实验，FedEcho 在异步FL基线上表现更优，具鲁棒性且无需访问私有数据。

Conclusion: 不确定性感知蒸馏使异步FL在大延迟和异质数据下更稳健，平衡了全部客户端的贡献并缓解了过时更新与数据异质性带来的负面影响。

Abstract: Asynchronous federated learning (FL) has recently gained attention for its enhanced efficiency and scalability, enabling local clients to send model updates to the server at their own pace without waiting for slower participants. However, such a design encounters significant challenges, such as the risk of outdated updates from straggler clients degrading the overall model performance and the potential bias introduced by faster clients dominating the learning process, especially under heterogeneous data distributions. Existing methods typically address only one of these issues, creating a conflict where mitigating the impact of outdated updates can exacerbate the bias created by faster clients, and vice versa. To address these challenges, we propose FedEcho, a novel framework that incorporates uncertainty-aware distillation to enhance the asynchronous FL performances under large asynchronous delays and data heterogeneity. Specifically, uncertainty-aware distillation enables the server to assess the reliability of predictions made by straggler clients, dynamically adjusting the influence of these predictions based on their estimated uncertainty. By prioritizing more certain predictions while still leveraging the diverse information from all clients, FedEcho effectively mitigates the negative impacts of outdated updates and data heterogeneity. Through extensive experiments, we demonstrate that FedEcho consistently outperforms existing asynchronous federated learning baselines, achieving robust performance without requiring access to private client data.

</details>


### [82] [On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices](https://arxiv.org/abs/2511.19986)
*Lianming Huang,Haibo Hu,Qiao Li,Nan Guan,Chun Jason Xue*

Main category: cs.LG

TL;DR: 提出一种按需多任务稀疏框架，通过块级可重用结构跨任务对齐，降低任务切换的I/O开销，并在实际自驾平台上实现6.6倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备上部署大模型时，单任务稀疏化忽略任务切换带来的I/O成本；需要减少冷启动和切换延迟。

Method: 将权重分解为可重用的块粒度单元，跨任务对齐稀疏结构，以实现最大重叠；按需加载仅为下一个任务所需的少量块的差异集合。

Result: 在真实自驾平台上，切换效率显著提升，任务切换平均加速超过6.6X，相比现有稀疏化方法。

Conclusion: 通过动态加载最小差异块，实现稀疏性与多任务之间的高重用性，缓解冷启动问题，提升边缘平台的任务切换性能。

Abstract: Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.

</details>


### [83] [RankOOD - Class Ranking-based Out-of-Distribution Detection](https://arxiv.org/abs/2511.19996)
*Dishanika Denipitiyage,Naveen Karunanayake,Suranga Seneviratne,Sanjay Chawla*

Main category: cs.LG

TL;DR: RankOOD是一种基于排序的OOD检测方法，通过使用Plackett-Luce损失对模型进行二阶段训练以利用ID数据中的隐含排序模式，在TinyImageNet近OOD评估中达到SOTA，FPR95降低4.3%。


<details>
  <summary>Details</summary>
Motivation: OOD检测在现实场景中需要充分利用ID数据的结构信息。尽管常用的交叉熵训练会产生ID类别的排序模式，但现有方法往往忽略这一潜在信号。作者提出将ID类别的排序信息显式建模为一个排列预测任务，以提升对OOD样本的区分能力。

Method: 1) 使用初始分类器提取每个类别的排序列表（rank list）。2) 以Plackett-Luce损失对模型进行再训练，将每个类别的固定排列作为待预测变量，通过排序概率来进行学习。该框架使OOD样本更可能违背ID的排序约束，从而提高检测能力。

Result: 在近OOD TinyImageNet基准上实现了SOTA性能，FPR95下降约4.3%。

Conclusion: 基于排序的PL损失可以有效利用ID数据的隐含排序模式来改善OOD检测，RankOOD提供了一种新的两阶段训练范式，显著提升近OOD场景的检测性能。

Abstract: We propose RankOOD, a rank-based Out-of-Distribution (OOD) detection approach based on training a model with the Placket-Luce loss, which is now extensively used for preference alignment tasks in foundational models. Our approach is based on the insight that with a deep learning model trained using the Cross Entropy Loss, in-distribution (ID) class prediction induces a ranking pattern for each ID class prediction. The RankOOD framework formalizes the insight by first extracting a rank list for each class using an initial classifier and then uses another round of training with the Plackett-Luce loss, where the class rank, a fixed permutation for each class, is the predicted variable. An OOD example may get assigned with high probability to an ID example, but the probability of it respecting the ranking classification is likely to be small. RankOOD, achieves SOTA performance on the near-ODD TinyImageNet evaluation benchmark, reducing FPR95 by 4.3%.

</details>


### [84] [REWA: Witness-Overlap Theory -- Foundations for Composable Binary Similarity Systems](https://arxiv.org/abs/2511.19998)
*Nikit Phadke*

Main category: cs.LG

TL;DR: 提出REWA，一种基于见证重叠的普适相似性理论，能将任意单调的见证重叠相似性映射为对数编码且保持前k排序，在模块化设计下可组合多种结构化变换，统一 Bloom filters、minhash、LSH 等方法。


<details>
  <summary>Details</summary>
Motivation: 寻求一个统一、可组合且具有理论保证的相似性框架，覆盖图、时序、因果、拓扑、符号模式、嵌入近邻等场景，超越单纯的哈希/近似技巧。

Method: 提出REWA系统：有限见证集W(v)，对每个见证生成半随机比特分配，见证重叠Δ(u,v)=|W(u)∩W(v)|与期望相似性单调相关；在具备最终见证集的重叠缺口条件时，给出top-k排序可用的位数m=O(log(|V|/δ))的编码，并实现可组合管线，使任意序列的结构性、时序性、因果、拓扑、信息理论或学习变换均可聚合为离散的见证集合。

Result: 给出可证明的可还原性/可比性定理（top-k保持）以及完整证明，包含明确常数，强调模块化设计与可重复组合；理论框架可统一多种相似性工具（Bloom Filter、MinHash、LSH位图、随机投影、摘要等）的核心思想。

Conclusion: REWA为基于见证重叠的相似性系统提供原理化基础，强调以可复用原语构建设计；并讨论局限性及未来扩展，如多比特编码、加权见证、非集合表示等。

Abstract: REWA introduces a general theory of similarity based on witness-overlap structures. We show that whenever similarity between concepts can be expressed as monotone witness overlap -- whether arising from graph neighborhoods, causal relations, temporal structure, topological features, symbolic patterns, or embedding-based neighborhoods -- it admits a reduction to compact encodings with provable ranking preservation guarantees. REWA systems consist of: (1) finite witness sets $W(v)$, (2) semi-random bit assignments generated from each witness, and (3) monotonicity of expected similarity in the overlap $Δ(u, v) = |W(u) \cap W(v)|$. We prove that under an overlap-gap condition on the final witness sets -- independent of how they were constructed -- top-$k$ rankings are preserved using $m = O(\log(|V|/δ))$ bits. The witness-set formulation is compositional: any sequence of structural, temporal, causal, topological, information-theoretic, or learned transformations can be combined into pipelines that terminate in discrete witness sets. The theory applies to the final witness overlap, enabling modular construction of similarity systems from reusable primitives. This yields a vast design space: millions of composable similarity definitions inherit logarithmic encoding complexity. REWA subsumes and unifies Bloom filters, minhash, LSH bitmaps, random projections, sketches, and hierarchical filters as special cases. It provides a principled foundation for similarity systems whose behavior is governed by witness overlap rather than hash-function engineering. This manuscript presents the axioms, the main reducibility theorem, complete proofs with explicit constants, and a detailed discussion of compositional design, limitations, and future extensions including multi-bit encodings, weighted witnesses, and non-set representations.

</details>


### [85] [Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting](https://arxiv.org/abs/2511.20004)
*Peining Zhang,Hongchen Qin,Haochen Zhang,Ziqi Guo,Guiling Wang,Jinbo Bi*

Main category: cs.LG

TL;DR: 零-shot 预测：Time-series foundation model Sundial 在 LAI 的遥感时间序列预测中，能在不任务特定微调的情况下超越训练良好的 LSTM，当输入上下文窗口足够长（覆盖一个以上两个完整季节循环）时。


<details>
  <summary>Details</summary>
Motivation: 探究通用预训练的时间序列基础模型是否能作为即插即用的预测器，在没有任务特定微调的情况下，对遥感时间序列进行高质量预测，特别是农业/环境应用中的 LAI 预测。

Method: 在 HiQ 数据集（美国，2000-2022）上，系统比较统计基线、一个全监督的 LSTM，以及 Sundial 基础模型，在多种评估协议下进行零-shot 预测；并变更输入上下文窗口长度以考察对预测性能的影响。

Result: 在零-shot 设置中，Sundial 能在输入上下文足够长（覆盖一个以上两个完整季节循环）时超过经过完整训练的 LSTM，首次显示通用型基础模型在没有任务特定微调的情况下可优于专门监督模型进行遥感时间序列预测。

Conclusion: 这表明预训练的时间序列基础模型具有作为高效“即插即用”预测器的潜力，可在农业与环境应用中直接用于遥感时间序列的预测任务。

Abstract: This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.

</details>


### [86] [Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering](https://arxiv.org/abs/2511.20030)
*Haoran Zheng,Renchi Yang,Hongtao Wang,Jianliang Xu*

Main category: cs.LG

TL;DR: 提出 DGF 框架用于多模态属性图的聚类，通过特征级去噪和三重对比学习提升鲁棒性，在八个数据集上优于主流基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有多视图聚类过度依赖跨模态属性高相关性、忽视模态特有噪声与低相关性的挑战；需要结合图信号处理理论实现特征级去噪并提升节点表征鲁棒性。

Method: 提出 Dual Graph Filtering (DGF) 方案，在节点表示学习中引入特征级去噪组件；并设计 tri-cross 对比学习策略，在实例层面跨模态、邻域与社区维度进行对比学习，以获得鲁棒且具辨别性的节点嵌入。

Result: 在八个基准 MMAG 数据集上，DGF 能持续且显著地优于广泛的 state-of-the-art 基线，提升聚类质量。

Conclusion: DGF 有效克服了传统图滤波在多模态图聚类中的局限，证实特征级去噪与三重对比学习的组合能显著提升聚类性能。

Abstract: Multimodal Attributed Graphs (MMAGs) are an expressive data model for representing the complex interconnections among entities that associate attributes from multiple data modalities (text, images, etc.). Clustering over such data finds numerous practical applications in real scenarios, including social community detection, medical data analytics, etc. However, as revealed by our empirical studies, existing multi-view clustering solutions largely rely on the high correlation between attributes across various views and overlook the unique characteristics (e.g., low modality-wise correlation and intense feature-wise noise) of multimodal attributes output by large pre-trained language and vision models in MMAGs, leading to suboptimal clustering performance.
  Inspired by foregoing empirical observations and our theoretical analyses with graph signal processing, we propose the Dual Graph Filtering (DGF) scheme, which innovatively incorporates a feature-wise denoising component into node representation learning, thereby effectively overcoming the limitations of traditional graph filters adopted in the extant multi-view graph clustering approaches. On top of that, DGF includes a tri-cross contrastive training strategy that employs instance-level contrastive learning across modalities, neighborhoods, and communities for learning robust and discriminative node representations. Our comprehensive experiments on eight benchmark MMAG datasets exhibit that DGF is able to outperform a wide range of state-of-the-art baselines consistently and significantly in terms of clustering quality measured against ground-truth labels.

</details>


### [87] [RED-F: Reconstruction-Elimination based Dual-stream Contrastive Forecasting for Multivariate Time Series Anomaly Prediction](https://arxiv.org/abs/2511.20044)
*PengYu Chen,Xiaohou Shi,Yuan Chang,Yan Sun,Sajal K. Das*

Main category: cs.LG

TL;DR: 提出 RED-F：一个双流对比预测框架用于多元时间序列异常预测，结合重建-去除模型（REM）与双流预测模型（DFM），通过混合时频的 REM 提纯正常模式，并以对比预测强化前导信号，同时引入 MSP 目标提升对远期上下文的利用，在六个真实数据集上展现出色性能。


<details>
  <summary>Details</summary>
Motivation: 无监督方法在仅以正常数据训练时，容易重建正常模式，从而掩盖微弱的异常前导信号，需一套能够分离或放大前导信号的框架以提升异常预测。

Method: 提出 REM 采用混合时频机制对序列进行去噪/提纯，得到纯净的正常模式基线；DFM 将该基线与原始序列（保留前导信号）作为并行输入；通过对比预测将绝对信号检测转化为相对轨迹比较以放大前导信号；MSP 目标利用远期上下文提升预测灵敏度。

Result: 在六个真实数据集上的广泛实验表明 RED-F 在异常预测任务中具备优越能力。

Conclusion: RED-F 通过重建-去除与对比预测相结合，有效放大微弱前导信号，提供在多元时间序列中的鲁棒无监督异常预测。

Abstract: The proactive prediction of anomalies (AP) in mul- tivariate time series (MTS) is a critical challenge to ensure system dependability. The difficulty lies in identifying subtle anomaly precursors concealed within normal signals. However, existing unsupervised methods, trained exclusively on normal data, demonstrate a fundamental propensity to reconstruct normal patterns. Consequently, when confronted with weak precursors, their predictions are dominated by the normal pattern, submerging the very signal required for prediction. To contend with the limitation, we propose RED-F, a Reconstruction- Elimination based Dual-stream Contrastive Forecasting frame- work, comprising the Reconstruction-Elimination Model (REM) and the Dual-stream Contrastive Forecasting Model (DFM). The REM utilizes a hybrid time-frequency mechanism to mitigate the precursor, generating a purified, normal-pattern baseline. The DFM then receives this purified baseline and the original sequence which retains the precursor as parallel inputs. At the core of our framework, RED-F employs a contrastive forecast that transforms the difficult task of absolute signal detection into a simpler, more robust task of relative trajectory comparison by computing the divergence between these two predictive streams. This contrastive mechanism serves to amplify the faint precursor signal. Furthermore, the DFM is trained with a novel Multi-Series Prediction (MSP) objective, which leverages distant future con- text to enhance its predictive sensitivity. Extensive experiments on six real-world datasets demonstrate the superior capability of RED-F in anomaly prediction tasks.

</details>


### [88] [SOMBRL: Scalable and Optimistic Model-Based RL](https://arxiv.org/abs/2511.20066)
*Bhavya Sukhija,Lenart Treven,Carmelo Sferrazza,Florian Dörfler,Pieter Abbeel,Andreas Krause*

Main category: cs.LG

TL;DR: 提出 SOMBRL，一种可扩展且基于乐观原理的模型基强化学习方法。通过学习带不确定性的动力学模型并贪心地最大化外在奖励与 epistemic 不确定性的加权和，实现对未知系统的高效探索，在多种设置下具有 sublinear regret，并在仿真与硬件平台上展现强劲性能。


<details>
  <summary>Details</summary>
Motivation: 在模型未知且需在线交互学习的模型基强化学习（MBRL）中，探索效率和样本效率受限。需要一种理论上有界、可扩展且能有效探索的策略来应对不确定性与复杂动力学。

Method: 基于乐观原理，SOMBRL 学习一个带不确定性的动力学模型；通过对外在奖励与 epistemic 不确定性的加权和进行贪心优化来规划或学习策略。该方法与具体策略优化器/规划器解耦，适用于各种任务设置，在常规正则性假设下可获得子线性轨迹误差/累积奖励的 regret。

Result: 在状态基与视觉控制环境中，SOMBRL 展现出与基线相比更强的探索效率与任务性能；在动态 RC 小车的硬件实验中也优于现有最先进方法，体现了基于不 certainty 的探索策略的现实效用。

Conclusion: SOMBRL 提供一种可扩展且理论性强的探索解决方案，适用于多种 MBRL 场景，理论保证与实验结果共同支撑其在提升探索效率和长期回报方面的有效性。

Abstract: We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.

</details>


### [89] [QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression](https://arxiv.org/abs/2511.20099)
*Lei Huang,Rui Zhang,Jiaming Guo,Yang Zhang,Di Huang,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 提出 CRUX 作为一个结构化中间表示，用以把自然语言描述转换为 Verilog 代码，并通过两阶段训练提升 CRUX 与 Verilog 生成质量，CRUX-V 在多项基准上达到 SOTA 并可作为其他模型的提示空间。


<details>
  <summary>Details</summary>
Motivation: 自由文本描述往往含糊、冗余且无结构，难以直接生成准确的 Verilog 代码；需要一个既保留用户意图又便于机器推理的中间表示。

Method: 提出 CRUX 作为结构化表达的中间空间，捕捉用户意图的本质语义并组织表达以实现精准的 Verilog 代码生成。设计两阶段训练框架：联合表达建模（Joint Expression Modeling）和双空间优化（Dual-Space Optimization），以提升 CRUX 和 Verilog 代码的质量。

Result: 在多个 Verilog 生成基准上，CRUX-V 相比通用模型达到最先进水平，尤其在具有挑战性的设计任务中表现突出。CRUX 空间在作为其他代码模型的输入提示时也具备可移植性与附加收益，证明其在缩小自然语言描述与 Verilog 生成之间的差距方面的有效性。

Conclusion: CRUX 提供一种可迁移、提高 HDL 生成的结构化中间表示，且可作为更广泛模型的提示空间，进一步提升 HDL 自动生成的效果。

Abstract: Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstream Verilog code generation. We treat hardware code generation as a complex transformation from an open-ended natural language space to a domain-specific, highly constrained target space. To bridge this gap, we introduce Core Refined Understanding eXpression (CRUX), a structured intermediate space that captures the essential semantics of user intent while organizing the expression for precise Verilog code generation. We further design a two-stage training framework, comprising Joint Expression Modeling and Dual-Space Optimization, to enhance the quality of both CRUX and Verilog code. Experiments across multiple Verilog generation benchmarks demonstrate that our model, CRUX-V, achieves state-of-the-art performance among general models, particularly under challenging design tasks. Furthermore, the CRUX space proves transferable and beneficial when used as input prompts for other code models, highlighting its effectiveness in narrowing the gap between free-form natural language descriptions and precise Verilog generation.

</details>


### [90] [Multivariate Forecasting of Bitcoin Volatility with Gradient Boosting: Deterministic, Probabilistic, and Feature Importance Perspectives](https://arxiv.org/abs/2511.20105)
*Grzegorz Dudek,Mateusz Kasprzyk,Paweł Pełka*

Main category: cs.LG

TL;DR: 用 LightGBM 对比经济学与机器学习基线，进行比特币实现波动率的点预测与分位数预测，使用 69 个预测因子，揭示交易量、滞后波动、投资者关注与市值等为主要驱动。


<details>
  <summary>Details</summary>
Motivation: 在高度非线性、强波动性特征的加密市场中，传统模型可能不足以捕捉复杂关系，Light Gradient Boosting Machine 具备处理非线性与提供可解释性的潜力；同时探索两类概率预测途径以生成预测区间。

Method: 采用 69 个市场、行为与宏观指标作为输入，对实现波动率进行点预测和分位数预测；概率预测包括直接分位回归（binary/ pinball 损失函数）以及通过残差仿真将点预测转化为预测分布；通过增益基和置换特征重要性分析来识别主导因素。

Result: 实验结果显示，LightGBM 能有效捕捉加密市场的非线性和高波动性特征；在概率预测方面，两种方法均能生成有用的预测分布；特征重要性分析一致指向交易量、滞后波动、投资者关注与市值等为关键驱动。

Conclusion: LightGBM 为加密市场波动率预测提供了有力的工具，兼具可解释性，可用于风险管理和交易策略的决策支持，未来可扩展更多特征与模型对比。

Abstract: This study investigates the application of the Light Gradient Boosting Machine (LGBM) model for both deterministic and probabilistic forecasting of Bitcoin realized volatility. Utilizing a comprehensive set of 69 predictors -- encompassing market, behavioral, and macroeconomic indicators -- we evaluate the performance of LGBM-based models and compare them with both econometric and machine learning baselines. For probabilistic forecasting, we explore two quantile-based approaches: direct quantile regression using the pinball loss function, and a residual simulation method that transforms point forecasts into predictive distributions. To identify the main drivers of volatility, we employ gain-based and permutation feature importance techniques, consistently highlighting the significance of trading volume, lagged volatility measures, investor attention, and market capitalization. The results demonstrate that LGBM models effectively capture the nonlinear and high-variance characteristics of cryptocurrency markets while providing interpretable insights into the underlying volatility dynamics.

</details>


### [91] [CLIMATEAGENT: Multi-Agent Orchestration for Complex Climate Data Science Workflows](https://arxiv.org/abs/2511.20109)
*Hyeonjae Kim,Chenyue Li,Wen Deng,Mengxi Jin,Wen Huang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: ClimateAgent—a multi-agent framework for end-to-end climate data analytics with dynamic API awareness and self-correction; introduces Climate-Agent-Bench-85; claims superior task completion and report quality.


<details>
  <summary>Details</summary>
Motivation: To address climate-specific context gaps in generic LLM agents and static pipelines, enabling robust, automated climate data workflows.

Method: Architectural design with Orchestrate-Agent, Plan-Agent, Data-Agents that introspect APIs to generate download scripts, and Coding-Agent for code, visualizations, and reports; built-in self-correction loop; introduces Climate-Agent-Bench-85 benchmark across climate tasks.

Result: On Climate-Agent-Bench-85, achieved 100% task completion and a report quality score of 8.32; outperformed GitHub-Copilot (6.27) and GPT-5 baseline (3.26).

Conclusion: Demonstrates that multi-agent orchestration with dynamic API awareness and self-correcting execution significantly advances reliable, end-to-end climate data analytics.

Abstract: Climate science demands automated workflows to transform comprehensive questions into data-driven statements across massive, heterogeneous datasets. However, generic LLM agents and static scripting pipelines lack climate-specific context and flexibility, thus, perform poorly in practice. We present ClimateAgent, an autonomous multi-agent framework that orchestrates end-to-end climate data analytic workflows. ClimateAgent decomposes user questions into executable sub-tasks coordinated by an Orchestrate-Agent and a Plan-Agent; acquires data via specialized Data-Agents that dynamically introspect APIs to synthesize robust download scripts; and completes analysis and reporting with a Coding-Agent that generates Python code, visualizations, and a final report with a built-in self-correction loop. To enable systematic evaluation, we introduce Climate-Agent-Bench-85, a benchmark of 85 real-world tasks spanning atmospheric rivers, drought, extreme precipitation, heat waves, sea surface temperature, and tropical cyclones. On Climate-Agent-Bench-85, ClimateAgent achieves 100% task completion and a report quality score of 8.32, outperforming GitHub-Copilot (6.27) and a GPT-5 baseline (3.26). These results demonstrate that our multi-agent orchestration with dynamic API awareness and self-correcting execution substantially advances reliable, end-to-end automation for climate science analytic tasks.

</details>


### [92] [On the Limits of Momentum in Decentralized and Federated Optimization](https://arxiv.org/abs/2511.20168)
*Riccardo Zaccone,Sai Praneeth Karimireddy,Carlo Masone*

Main category: cs.LG

TL;DR: 带动量的本地方法在循环参与的去中心化场景中，无法保证收敛，且比 1/t 更快下降的学习率也会收敛到与初始化和异质性界相关的常数。


<details>
  <summary>Details</summary>
Motivation: 研究在分布式 SGD 的本地方法中引入动量的效果，特别是在联邦学习中，动量是否能缓解统计异质性并保证在异质性存在下的收敛性。

Method: 对带动量的循环参与的去中心化设置进行理论分析，证明动量会受统计异质性影响；推导出若学习率下降的速率快于 Θ(1/t) 将导致收敛到与初始化和异质性上界相关的常数；并通过数值结果和深度学习实验验证。

Result: 证明动量在循环参与下仍受异质性影响；任意快于 1/t 的下降序列导致收敛到依赖初始值和异质性上界的常数；数值与深度学习实验支持理论。

Conclusion: 在具有循环客户端参与的去中心化设置中，动量并不能克服统计异质性对收敛性的影响；要实现收敛，需要额外的方法或对学习率的更谨慎控制，特别是超过 1/t 阈值的下降并不有利。

Abstract: Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $Θ\left(1/t\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.

</details>


### [93] [Learning Subgroups with Maximum Treatment Effects without Causal Heuristics](https://arxiv.org/abs/2511.20189)
*Lincen Yang,Zhong Li,Matthijs van Leeuwen,Saber Salehkaleybar*

Main category: cs.LG

TL;DR: 将最大平均处理效果的子组发现迁移到分割型模型下，将问题转化为标准监督学习，从而可用 CART 等分割方法直接学习具有最大处理效应的子组；实验证明优于多种基线，且避免因果启发式。


<details>
  <summary>Details</summary>
Motivation: 在因果推断的潜在结果框架下，最大治疗效应子组发现常受限于点估计后再拟合树或依赖啰嗦的因果启发式方法；需要一个基于结构因果模型、分割型假设的系统性方法来定位高效的子组。

Method: 在分割型（partition-based）模型假设下，最优子组发现等价于恢复数据产生的模型，因此可视为标准的回归/分类学习问题；因此可以采用现有的分割方法来学习子组。本论文以 CART 为例来学习最大治疗效应的子组。

Result: 在大量合成和半合成数据集上，与广泛的基线相比，该方法不依赖因果启发式，能够更准确地识别具有最大治疗效应的子组。

Conclusion: 基于 SCM 的分割型子组发现提供一种无须额外因果启发式的通用框架，能够利用现有的树方法提升子组发现的性能，且代码可用。

Abstract: Discovering subgroups with the maximum average treatment effect is crucial for targeted decision making in domains such as precision medicine, public policy, and education. While most prior work is formulated in the potential outcome framework, the corresponding structural causal model (SCM) for this task has been largely overlooked. In practice, two approaches dominate. The first estimates pointwise conditional treatment effects and then fits a tree on those estimates, effectively turning subgroup estimation into the harder problem of accurate pointwise estimation. The second constructs decision trees or rule sets with ad-hoc 'causal' heuristics, typically without rigorous justification for why a given heuristic may be used or whether such heuristics are necessary at all. We address these issues by studying the problem directly under the SCM framework. Under the assumption of a partition-based model, we show that optimal subgroup discovery reduces to recovering the data-generating models and hence a standard supervised learning problem (regression or classification). This allows us to adopt any partition-based methods to learn the subgroup from data. We instantiate the approach with CART, arguably one of the most widely used tree-based methods, to learn the subgroup with maximum treatment effect. Finally, on a large collection of synthetic and semi-synthetic datasets, we compare our method against a wide range of baselines and find that our approach, which avoids such causal heuristics, more accurately identifies subgroups with maximum treatment effect. Our source code is available at https://github.com/ylincen/causal-subgroup.

</details>


### [94] [In-Context Compositional Learning via Sparse Coding Transformer](https://arxiv.org/abs/2511.20194)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.LG

TL;DR: 提出将注意力重构为稀疏编码框架的字典分解，以获得更强的组合泛化能力，并在 S-RAVEN/RAVEN 上表现出对某些组合泛化任务的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Transformer 缺乏处理组合结构任务的固有结构性归纳偏置，需提升对组合规则的学习和应用能力。

Method: 将注意力映射视为对输入在编码字典和解码字典上的投影，强制系数稀疏；用上下文示例的系数线性组合来估计目标问题的系数；输出通过解码字典的线性组合产生。

Result: 在某些组合泛化任务中，该方法在保持 Transformer 原有性能的同时，提升了对组合规则的学习与应用能力，且在 S-RAVEN/RAVEN 上表现有竞争力，尤其在标准 Transformer 无法很好泛化的场景。

Conclusion: 稀疏编码风格的注意力提升了结构化表示和组合泛化能力，显示出在 S-RAVEN/RAVEN 这类基准上的潜力，表明未来将结构性偏置引入注意力可能是提升泛化的有效方向。

Abstract: Transformer architectures have achieved remarkable success across language, vision, and multimodal tasks, and there is growing demand for them to address in-context compositional learning tasks. In these tasks, models solve the target problems by inferring compositional rules from context examples, which are composed of basic components structured by underlying rules. However, some of these tasks remain challenging for Transformers, which are not inherently designed to handle compositional tasks and offer limited structural inductive bias. In this work, inspired by the principle of sparse coding, we propose a reformulation of the attention to enhance its capability for compositional tasks. In sparse coding, data are represented as sparse combinations of dictionary atoms with coefficients that capture their compositional rules. Specifically, we reinterpret the attention block as a mapping of inputs into outputs through projections onto two sets of learned dictionary atoms: an encoding dictionary and a decoding dictionary. The encoding dictionary decomposes the input into a set of coefficients, which represent the compositional structure of the input. To enhance structured representations, we impose sparsity on these coefficients. The sparse coefficients are then used to linearly combine the decoding dictionary atoms to generate the output. Furthermore, to assist compositional generalization tasks, we propose estimating the coefficients of the target problem as a linear combination of the coefficients obtained from the context examples. We demonstrate the effectiveness of our approach on the S-RAVEN and RAVEN datasets. For certain compositional generalization tasks, our method maintains performance even when standard Transformers fail, owing to its ability to learn and apply compositional rules.

</details>


### [95] [Leveraging weights signals - Predicting and improving generalizability in reinforcement learning](https://arxiv.org/abs/2511.20234)
*Olivier Moulin,Vincent Francois-lavet,Paul Elbers,Mark Hoogendoorn*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.

</details>


### [96] [Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling](https://arxiv.org/abs/2511.20257)
*Zhiguo Zhang,Xiaoliang Ma,Daniel Schlesinger*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.

</details>


### [97] [Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits](https://arxiv.org/abs/2511.20273)
*Areeb Ahmad,Abhinav Joshi,Ashutosh Modi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

</details>


### [98] [HVAdam: A Full-Dimension Adaptive Optimizer](https://arxiv.org/abs/2511.20277)
*Yiheng Zhang,Shaowu Wu,Yuanzhuo Xu,Jiajun Wu,Shang Xu,Steve Drew,Xiaoguang Niu*

Main category: cs.LG

TL;DR: 提出可连续调节自适应性的优化器Anon，并通过增量延迟更新(IDU)实现对SGD和Adam的插值，具备理论收敛性并在多任务上优于现有优化器。


<details>
  <summary>Details</summary>
Motivation: 解决自适应优化器在泛化方面通常不如SGD的问题，尤其是前置预条件中的自适应性限制了在不同优化景观中的适应性；需要一个自适应性可控且具有鲁棒性的统一框架。

Method: 设计Anon，使自适应性在SGD-like和Adam-like行为之间连续插值，甚至可超越两者；引入增量延迟更新(IDU)，相比AMSGrad的硬最大跟踪更灵活，提升对梯度噪声的鲁棒性。

Result: 理论层面，在凸和非凸设定下均可证明收敛；在实际任务中，在图像分类、扩散模型和语言建模任务上优于现有优化器。

Conclusion: 自适应性是一种可调节的设计原则，Anon提供了一个统一且可靠的框架，能够在经典与现代优化器之间桥接并超越它们的优点。

Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.

</details>


### [99] [Geometry of Decision Making in Language Models](https://arxiv.org/abs/2511.20315)
*Abhinav Joshi,Divyanshu Bhatt,Ashutosh Modi*

Main category: cs.LG

TL;DR: 研究通过在多层变换模型中估计“内在维度”（ID），揭示MCQA任务中隐藏表示的几何演化：早层处于低维流形，中层扩张，后层收敛到与决策相关的低维表示。


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs内部决策过程与泛化能力的几何根源，利用ID这一度量探索隐藏表示在不同层的维度变化及其与任务决策的关系。

Method: 对28个开源权重的Transformer模型进行跨层ID估计，使用多种ID估计器，并同时量化各层在MCQA任务上的性能，比较不同层的几何结构与决策相关性。

Result: 在跨模型的广泛分析中观察到一致的ID模式：早层在低维流形上工作，中层扩展其表征空间，后层再度压缩并收敛至与决策相关的表示，指示语言输入被投影到结构化低维流形以支撑决策。

Conclusion: 研究揭示了在LLMs中泛化与推理的几何实现：模型通过分层的维度走向，将输入映射到任务对齐的低维流形，从而在MCQA等任务中实现有效推理。

Abstract: Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.

</details>


### [100] [MXtalTools: A Toolkit for Machine Learning on Molecular Crystals](https://arxiv.org/abs/2511.20327)
*Michael Kilgour,Mark E. Tuckerman,Jutta Rogal*

Main category: cs.LG

TL;DR: MXtalTools 是一个模块化、CUDA 加速的 Python 工具包，用于数据驱动的分子晶体建模和机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 现有分子晶体建模缺乏一个统一的、可扩展的数据整理、参数化、采样、训练和推断的工作流，MXtalTools 旨在填补这一空白，促进高通量、端到端的晶体结构分析与建模。

Method: 提供数据收集、整理和去重等数据集工作流；晶体参数化和表示；晶体结构采样与优化；端到端可微分的晶体采样、构建与分析；模块化函数可与现有工作流集成，支持在 CUDA 上加速。

Result: 支持高吞吐晶体建模、易于集成的新建模管线、以及可扩展的机器学习研究；开源代码在 GitHub，文档详尽。

Conclusion: MXtalTools 提供一个灵活、可扩展的框架，便于研究者在分子晶体的机器学习应用中搭建端到端工作流，促进数据驱动的晶体研究。

Abstract: We present MXtalTools, a flexible Python package for the data-driven modelling of molecular crystals, facilitating machine learning studies of the molecular solid state. MXtalTools comprises several classes of utilities: (1) synthesis, collation, and curation of molecule and crystal datasets, (2) integrated workflows for model training and inference, (3) crystal parameterization and representation, (4) crystal structure sampling and optimization, (5) end-to-end differentiable crystal sampling, construction and analysis. Our modular functions can be integrated into existing workflows or combined and used to build novel modelling pipelines. MXtalTools leverages CUDA acceleration to enable high-throughput crystal modelling. The Python code is available open-source on our GitHub page, with detailed documentation on ReadTheDocs.

</details>


### [101] [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)
*Chang Gao,Chujie Zheng,Xiong-Hui Chen,Kai Dang,Shixuan Liu,Bowen Yu,An Yang,Shuai Bai,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: Soft Adaptive Policy Optimization (SAPO) replaces hard clipping in group-based policy optimization for RL in LLMs with a temperature-controlled soft gate, improving stability and sample efficiency; it achieves better Pass@1 on math benchmarks and shows gains across Qwen3-VL models.


<details>
  <summary>Details</summary>
Motivation: RL-based training of LLMs suffers from high variance in token-level importance ratios, especially in Mixture-of-Experts models. Hard clipping in GSPO/GRPO discards useful learning signals and hampers stability and sample efficiency.

Method: Introduce SAPO with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates. SAPO is sequence-coherent and token-adaptive, maintaining a continuous trust region instead of brittle hard clipping. It down-weights offending tokens rather than suppressing entire sequences, preserving learning signals from near on-policy tokens.

Result: Empirical results on mathematical reasoning benchmarks show improved training stability and higher Pass@1 under similar training budgets. SAPO also yields consistent performance gains across the Qwen3-VL model series for diverse tasks and model sizes.

Conclusion: SAPO provides a more reliable, scalable, and effective RL optimization strategy for training LLMs.

Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.

</details>


### [102] [Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning](https://arxiv.org/abs/2511.20349)
*M. E. A. Kherchouche,F. Galpin,T. Dumas,F. Schnitzler,D. Menard,L. Zhang*

Main category: cs.LG

TL;DR: 提出两种面向VVC intra分割的机器学习方法以加速RDO搜索：基于回归的RD成本预测和基于强化学习的分割决策，具有尺寸无关性并利用相邻块RD成本信息，均通过阈值实现分割选择。


<details>
  <summary>Details</summary>
Motivation: 旨在降低VVC中RDO的穷举搜索计算量，同时保持对CU分割的鲁棒性；希望方法与CU尺寸无关并充分利用邻近块的上下文信息。

Method: 第一种方法：回归预测CU的归一化RD成本；第二种方法：将分割决策建模为MDP并使用DQN进行轨迹学习的强化学习方法；两者均设置预定阈值以决定当前CU的分割。

Result: 摘要指出两种方法已被提出并进行比较，且具备尺寸无关性和使用邻域RD成本的输入特征，但未给出具体的实验数字或结论。

Conclusion: 从摘要可推断，这两种方法可用于减少RDO搜索的计算量并以阈值实现分割选择，但具体RD性能影响在摘要中未给出定量结论。

Abstract: In this paper, a complexity study is conducted for Versatile Video Codec (VVC) intra partitioning to accelerate the exhaustive search involved in Rate-Distortion Optimization (RDO) process. To address this problem, two main machine learning techniques are proposed and compared. Unlike existing methods, the proposed approaches are size independent and incorporate the Rate-Distortion (RD) costs of neighboring blocks as input features. The first method is a regression based technique that predicts normalized RD costs of a given Coding Unit (CU). As partitioning possesses the Markov property, the associated decision-making problem can be modeled as a Markov Decision Process (MDP) and solved by Reinforcement Learning (RL). The second approach is a RL agent learned from trajectories of CU decision across two depths with Deep Q-Network (DQN) algorithm. Then a pre-determined thresholds are applied for both methods to select a suitable split for the current CU.

</details>


### [103] [MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers](https://arxiv.org/abs/2511.20382)
*Audrey Pei-Hsuan Chen*

Main category: cs.LG

TL;DR: MoRE uses PEFT with frozen transformers and lightweight adapters to align heterogeneous omics into a shared latent space, achieving competitive integration with far fewer trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Multi-omics data present extreme dimensionality, modality heterogeneity, and cohort-specific batch effects; there is a need for general-purpose, efficient omics foundation models.

Method: Freeze pre-trained transformer backbones, attach modality-specific adapters and a task-adaptive fusion layer; train with a masked modeling objective plus supervised contrastive and batch-invariant alignment losses to produce cross-sample, cross-modality embeddings; compare against scGPT, scVI, and Harmony with scArches.

Result: MoRE yields structure-preserving embeddings with competitive batch robustness and biological conservation; generalizes to unseen cell types and platforms; substantially reduces trainable parameters relative to fully fine-tuned models.

Conclusion: MoRE demonstrates the practicality of a general-purpose omics foundation model via parameter-efficient fine-tuning, enabling robust cross-modality integration across diverse conditions.

Abstract: Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with scArches, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.

</details>


### [104] [Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI](https://arxiv.org/abs/2511.20395)
*M. C. Schoppema,B. H. M. van der Velden,A. Hürriyetoğlu,M. D. Klijnstra,E. J. Faassen,A. Gerssen,H. J. van der Fels-Klerx*

Main category: cs.LG

TL;DR: 提出了一种可解释的深度学习模型，用以预测荷兰泽兰河口TTX污染，基于气象和水文特征；日照时长、全球辐射、水温和氯离子浓度为主要驱动因素；该方法有助于食品行业与监管部门进行早期风险预测与监控。


<details>
  <summary>Details</summary>
Motivation: TTX在温带海域贝类中污染，给食品安全和经济带来风险，需早期预测并明确驱动因子以便监控与治理。

Method: 建立一个可解释的深度学习模型，输入为 meteorological 与 hydrological 特征，输出为TTX污染的有无；通过可解释性分析识别关键变量，如日出/日落时间、全球辐射、水温、氯离子浓度等。

Result: 结果显示日出/日落时间、全球辐射、水温和氯离子浓度对TTX污染贡献最大；“日照时长”这一综合变量（由日长与全球辐射组成）被视为重要驱动因素。

Conclusion: 所提出的解释性深度学习模型可作为食品行业与监管机构的实用工具，用以监测和降低贝类中TTX等海洋毒素的风险。

Abstract: Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.

</details>


### [105] [Model-Based Learning of Whittle indices](https://arxiv.org/abs/2511.20397)
*Joël Charles-Rebuffé,Nicolas Gast,Bruno Gaujal*

Main category: cs.LG

TL;DR: BLINQ is a model-based algorithm that learns the Whittle indices of an indexable, communicating and unichain MDP by building an empirical MDP and applying an extended Whittle-index algorithm; it is provably convergent with finite-sample bounds and demonstrates superior sample efficiency and lower computational cost compared to Q-learning in experiments.


<details>
  <summary>Details</summary>
Motivation: To efficiently learn Whittle indices for complex indexable MDPs, reducing sample and computational requirements relative to standard Q-learning, and enabling practical deployment.

Method: Construct an empirical MDP from data, compute Whittle indices via an extended algorithm, prove convergence to the true indices and provide finite-time learning bounds; analyze computational complexity; compare with Q-learning empirically.

Result: BLINQ achieves significantly better sample efficiency than Q-learning and incurs lower total computational cost for a reasonable number of samples; results hold even when Q-learning is accelerated with pre-trained networks.

Conclusion: BLINQ offers a provably convergent, sample-efficient, and computationally efficient approach to learning Whittle indices for indexable MDPs, outperforming conventional Q-learning in empirical evaluations.

Abstract: We present BLINQ, a new model-based algorithm that learns the Whittle indices of an indexable, communicating and unichain Markov Decision Process (MDP). Our approach relies on building an empirical estimate of the MDP and then computing its Whittle indices using an extended version of a state-of-the-art existing algorithm. We provide a proof of convergence to the Whittle indices we want to learn as well as a bound on the time needed to learn them with arbitrary precision. Moreover, we investigate its computational complexity. Our numerical experiments suggest that BLINQ significantly outperforms existing Q-learning approaches in terms of the number of samples needed to get an accurate approximation. In addition, it has a total computational cost even lower than Q-learning for any reasonably high number of samples. These observations persist even when the Q-learning algorithms are speeded up using pre-trained neural networks to predict Q-values.

</details>


### [106] [Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets](https://arxiv.org/abs/2511.20407)
*Kasper Green Larsen,Natascha Schalburg*

Main category: cs.LG

TL;DR: 首次给出投票分类器的边距泛化界限，且在候选集规模、边距、具有该边距的训练点比例、训练样本量及失效概率之间实现渐近紧致的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决对投票型分类器的泛化分析缺乏理论界限的问题，提供一种与边距相关的通用上界，适用于大规模假设集合下的学习情景。

Method: 通过把投票分类器的决策规则与边距分布结合，构建包含假设集尺寸、边距阈值、样本分布和失败概率的泛化界限，并证明该界限在渐近极限下是紧致的。

Result: 提出并证明一个渐近紧致的边距泛化界限，定量描述泛化误差上界如何随投票集合尺寸、边距大小、达到边距的训练点比例、训练样本数以及失败概率的变化而变化。

Conclusion: 该工作为投票分类器的边距理论提供了第一份紧致化的界限，揭示了边距分析在复合假设空间中的适用性与极限，具有理论与潜在应用价值。

Abstract: We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability.

</details>


### [107] [Diffusion for Fusion: Designing Stellarators with Generative AI](https://arxiv.org/abs/2511.20445)
*Misha Padidar,Teresa Huang,Andrew Giuliani,Marina Spivak*

Main category: cs.LG

TL;DR: 使用扩散模型在 QUASR 数据集上生成具备准对称性的扭转星形设计，并探索在未见特征下的快速设计能力。


<details>
  <summary>Details</summary>
Motivation: 在受控核聚变研究中，设计星形装置需要大量计算，且数据集规模扩展后，急需高效的生成模型来快速给出高质量设计，以缩短设计周期。

Method: 利用条件扩散模型对 QUASR 数据库的星形设计进行学习，设定目标特征（横截比、平均转动变换）。在训练后，将模型应用于训练中未出现的目标特征，评估设计对准对称性和目标特征的偏差，并提出评估协议。

Result: 生成的星形装置设计对准准对称性和目标特征的偏差小于5%，在某些情况下接近子1%级别的潜力，显示该生成模型具备对未见特征的泛化能力。

Conclusion: 生成建模为星形装置设计提供了一条可行的快速高效路径；本研究为研究社区提供了开放问题和评估协议，未来可进一步降低偏差并探索更多特征与目标。

Abstract: Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.

</details>


### [108] [Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2511.20456)
*Shreevanth Krishnaa Gopalakrishnan,Stephen Hailes*

Main category: cs.LG

TL;DR: 系统性评估CSI深度学习模型在对白盒、黑盒/迁移与通用对抗扰动等威胁模型下的鲁棒性，发现小模型在清洁数据上表现相近但鲁棒性显著不足；物理可实现的信号空间扰动比未约束的特征空间攻击更难以实现攻击成功；对抗训练可提升鲁棒性但对清洁准确率有中等折衷。


<details>
  <summary>Details</summary>
Motivation: CSI基于人体感知的系统易受对抗扰动影响，需建立在真实无线信道中的鲁棒性基线，为在无线感知中的安全部署提供定量评估和设计指引。

Method: 在三份公开数据集上，比较紧凑的时序自编码器模型与更大规模的深度架构；针对白盒、黑盒/迁移与通用扰动等多种威胁模型进行对比，并引入对物理信道可实现性的约束，评估两类模型在不同训练设置下的鲁棒性；并通过对抗训练评估鲁棒性提升。

Result: 结果显示：1) 小模型在清洁数据上与大模型等效，但鲁棒性显著较差；2) 受物理信道约束的扰动显著降低攻击成功率，优于无约束的特征空间攻击；3) 对抗训练提升平均鲁棒性，但对清洁性能有中等程度的降低；4) 研究提供鲁棒性估算的基线与设计原则，利于跨领域可靠的人类感知系统。

Conclusion: 结论强调模型规模和训练策略对鲁棒性的决定性影响，建议在CSI基于感知的系统设计中综合考虑物理约束、威胁模型和对抗训练以实现更安全、可信的无线感知。

Abstract: Machine learning has become integral to Channel State Information (CSI)-based human sensing systems and is expected to power applications such as device-free activity recognition and identity detection in future cellular and Wi-Fi generations. However, these systems rely on models whose decisions can be subtly perturbed, raising concerns for security and reliability in ubiquitous sensing. Quantifying and understanding the robustness of such models, defined as their ability to maintain accurate predictions under adversarial perturbations, is therefore critical before wireless sensing can be safely deployed in real-world environments.
  This work presents a systematic evaluation of the robustness of CSI deep learning models under diverse threat models (white-box, black-box/transfer, and universal perturbations) and varying degrees of attack realism. We establish a framework to compare compact temporal autoencoder models with larger deep architectures across three public datasets, quantifying how model scale, training regime, and physical constraints influence robustness. Our experiments show that smaller models, while efficient and equally performant on clean data, are markedly less robust. We further confirm that physically realizable signal-space perturbations, designed to be feasible in real wireless channels, significantly reduce attack success compared to unconstrained feature-space attacks. Adversarial training mitigates these vulnerabilities, improving mean robust accuracy with only moderate degradation in clean performance across both model classes. As wireless sensing advances towards reliable, cross-domain operation, these findings provide quantitative baselines for robustness estimation and inform design principles for secure and trustworthy human-centered sensing systems.

</details>


### [109] [NVIDIA Nemotron Parse 1.1](https://arxiv.org/abs/2511.20478)
*Kateryna Chumachenko,Amala Sanjay Deshmukh,Jarno Seppanen,Ilia Karmanov,Chia-Chih Chen,Lukas Voegtle,Philipp Fischer,Marek Wawrzos,Saeid Motiian,Roman Ageev,Kedi Wu,Alexandre Milesi,Maryam Moosaei,Krzysztof Pawelec,Padmavathy Subramanian,Mehrzad Samadi,Xin Yu,Celina Dear,Sarah Stoddard,Jenna Diamond,Jesse Oliver,Leanna Chraghchian,Patrick Skelly,Tom Balough,Yao Xu,Jane Polak Scowcroft,Daniel Korzekwa,Darragh Hanley,Sandip Bhaskar,Timo Roman,Karan Sapra,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron-Parse-1.1 是一个轻量级的文档解析与 OCR 模型，基于 Nemoretriever-Parse-1.0 的改进，覆盖通用 OCR、Markdown、表格解析以及图片/图表文本提取，支持更长输出序列，参数量 885M（含 256M 语言解码器），公开权重与优化容器，另有 Nemotron-Parse-1.1-TC 提供 20% 的速度提升。


<details>
  <summary>Details</summary>
Motivation: 提升对多样化文档的高效理解与解析能力，提供一个可部署、资源友好的 OCR 解决方案，覆盖文本块定位、语义分类与结构化信息抽取等需求。

Method: 采用编码器-解码器架构，总体 885M 参数，其中包含一个 256M 的语言解码器，扩展输出序列长度，输出文本的边界框与语义类别；支持一般 OCR、Markdown 解析、表格解析以及图片/图表文本提取；公开模型权重、优化 NIM 容器和数据子集，附带 Nemotron-VLM-v2 数据集扩展；另提供 Nemotron-Parse-1.1-TC，在降低视觉 token 长度的情况下实现约 20% 的推理加速，质量下降极小。

Result: 在公开基准上具有竞争力的准确性，证明其作为轻量级 OCR 解决方案的有效性，并在多类文档场景（包括表格和图像文本）表现出改进。

Conclusion: Nemotron-Parse-1.1 提供一个可公开获取、轻量且高效的文档解析框架，适合部署于资源受限环境，并通过 Huggingface 权重和数据子集促进研究与应用；1.1-TC 版本则在速度方面提供额外的提升。

Abstract: We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.

</details>


### [110] [MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology](https://arxiv.org/abs/2511.20490)
*Kiril Vasilev,Alexandre Misrahi,Eeshaan Jain,Phil F Cheng,Petros Liakopoulos,Olivier Michielin,Michael Moor,Charlotte Bunne*

Main category: cs.LG

TL;DR: MTBBench is a multimodal, longitudinal MTB-style benchmark for oncology LLMs that uses clinician-validated ground truth to test how LLMs handle complex, real-world decision-making; results show current models struggle with reliability and time-resolved data, while an agentic framework with foundation-model tools yields notable task gains.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are largely unimodal and decontextualized, failing to capture the multi-agent, longitudinal, multimodal nature of Molecular Tumor Boards and precision oncology decision-making.

Method: Introduce MTBBench, simulating MTB-style decisions with challenging multimodal and longitudinal oncology questions; clinician-validated ground truth via a co-developed app; benchmark multiple LLMs; provide an agentic framework with tool-use capabilities to enhance reasoning.

Result: LLMs frequently hallucinate and struggle with time-resolved, multimodal reasoning and reconciling conflicting evidence; agentic tooling yields task-level gains of up to 9.0% (multimodal) and 11.2% (longitudinal).

Conclusion: MTBBench provides a realistic, challenging testbed for advancing multimodal LLM reasoning, reliability, and tool-use in MTB-based precision oncology workflows.

Abstract: Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.

</details>


### [111] [DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning](https://arxiv.org/abs/2511.20509)
*Mihaela Hudişteanu,Edwige Cyffers,Nikita P. Kalinin*

Main category: cs.LG

TL;DR: 提出了 DP-MicroAdam，一种内存高效且具稀疏支持的自适应差分隐私优化器，给出收敛性证明并在一系列基准数据集上取得优越于现有自适应DP优化器的表现，且与 DP-SGD 相当甚至超越。


<details>
  <summary>Details</summary>
Motivation: 在非私有训练中自适应优化器通常带来更快的收敛和更好性能；然而在差分隐私训练中，DP-SGD 主导且需要高计算与大量超参数调优。需要一种兼具自适应性、内存友好性与差分隐私保护的优化器。

Method: 提出 DP-MicroAdam，证明在随机非凸问题下具有收敛性，达到 O(1/√T) 的速率（带隐私相关常数），并通过对比 CIFAR-10、ImageNet 等数据集及私有微调的实验，展示在与差分隐私相关的约束下的性能提升与稳定性改善。

Result: 理论上，DP-MicroAdam 在随机非凸优化下达到最优的 O(1/√T) 收敛速率（在隐私常数下）。在实际实验中，DP-MicroAdam 超越了现有自适应DP优化器，并在 CIFAR-10、ImageNet 以及私有微调的预训练 Transformer 上，与 DP-SGD 相比表现不劣甚至优于它。

Conclusion: 表明在差分隐私设置下，自适应优化具有提升性能与稳定性的潜力，DP-MicroAdam 为私有训练提供一个有效的自适应优化解。

Abstract: Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\mathcal{O}(1/\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.

</details>


### [112] [Adam Simplified: Bias Correction Simplified](https://arxiv.org/abs/2511.20516)
*Sam Laing,Antonio Orvieto*

Main category: cs.LG

TL;DR: Bias-correction in Adam may be unnecessary for optimal performance; it can hurt without proper learning rate scheduling; acts as a form of implicit LR scheduling dependent on beta1, beta2; universal inclusion is not warranted.


<details>
  <summary>Details</summary>
Motivation: Investigate the empirical necessity of Adam's bias-correction, which is often assumed to help but lacks rigorous understanding.

Method: Systematic ablations on vision and language modeling tasks; compare Adam with and without bias-correction across hyperparameters; reinterpret bias correction as implicit LR scheduling; analyze effect of smoothing parameters beta1, beta2.

Result: Under optimal hyperparameters, bias correction yields no improvement; without LR scheduling, it can be detrimental; bias correction behaves as an implicit, smoothing-parameter dependent LR scheduler.

Conclusion: Bias-correction should not be universally applied; decisions should depend on hyperparameter configuration and learning-rate schedule.

Abstract: The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.

</details>


### [113] [Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media](https://arxiv.org/abs/2511.20543)
*Alhasan Abdellatif,Hannah P. Menke,Ahmed H. Elsheikh,Florian Doster,Kamaljit Singh*

Main category: cs.LG

TL;DR: UFNO-FiLM 通过 FiLM 解耦标量输入并采用空间加权损失，在 UFNO 基础上显著提升预测精度，在地下多相渗流场景实现 21% MAE 降低


<details>
  <summary>Details</summary>
Motivation: 解决 UFNO 将标量输入当作空间场处理导致信息冗余与常量信号进入傅里叶域的问题，同时通过加权损失提升在物理重要区域的学习效果。

Method: 在 UFNO 框架中引入 FiLM 层以解耦标量输入与空间特征，并对特征进行条件调制；引入空间加权损失以优先关注关键区域；在地下多相渗流的仿真场景中评估性能。

Result: 与原 UFNO 相比，气体饱和度的 MAE 降低约 21%。

Conclusion: UFNO‑FiLM 通过解耦标量输入并聚焦关键区域学习，显著提升地下渗流场景中的预测准确性，证明方法的有效性。

Abstract: The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.

</details>


### [114] [A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent](https://arxiv.org/abs/2511.20584)
*Shuo Xie,Tianhao Wang,Beining Wu,Zhiyuan Li*

Main category: cs.LG

TL;DR: 将自适应优化器与归一化最速下降（NSD）之间的关系形式化：在非凸和随机设置中，提出自适应光滑性和自适应梯度方差的概念，作为收敛性和加速性的核心测度，并给出在非欧几里得几何下的维度无关收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有文献中自适应优化家族和NSD之间存在密切联系，但两者的几何假设（光滑性定义）不同。需要一个统一、几何感知的框架，以在非凸与随机情形下解释收敛性并探讨加速可能性。

Method: 将自适应光滑性推广到非凸设定，并证明它能精确表征自适应优化器的收敛性；在凸设定中，证明结合Nesterov动量时，自适应光滑性可实现加速；提出与自适应光滑性并列的自适应梯度方差概念，用于随机优化，并给出在某些非欧几里得几何下的维度无关收敛性。

Result: 自适应光滑性全面刻画自适应优化器的收敛性并可在凸情形下实现带Nesterov动量的加速；自适应梯度方差与自适应光滑性并行，导出在非欧几里得几何下的维度无关的随机优化收敛性，优于以往在标准光滑性与梯度方差框架下的结果。

Conclusion: 提出的自适应光滑性与自适应梯度方差构成了解释自适应优化与NSD之间关系的核心几何框架，扩展了非凸与随机优化的理论边界，并揭示了在特定几何结构下的加速与维度无关性的新可能。

Abstract: Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.

</details>


### [115] [Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models](https://arxiv.org/abs/2511.20587)
*Karim Kadry,Abdallah Abdelwahed,Shoaib Goraya,Ajay Manicka,Naravich Chutisilp,Farhad Nezami,Elazer Edelman*

Main category: cs.LG

TL;DR: Anatomica 是一个推理时框架，结合局部坐标域与可微惩罚，以及持久同调，来引导潜在扩散模型生成具有多类解剖学的体素映射，实现几何与拓扑的可控性。


<details>
  <summary>Details</summary>
Motivation: 实现对复杂解剖结构的可解释、可控生成，以便为虚拟试验和机器学习工作流设计合成数据。

Method: 使用不同维度的立方控制域来切出相关子结构，基于这些子结构计算可微惩罚以约束采样；通过体素级矩和大小、形状、位置等几何特征；通过持久同调强制实现连通分支、环和空腔等拓扑特征；在潜在扩散模型中结合神经场解码器提取子结构以高效控制解剖属性。

Result: 证明通过局部约束和拓扑约束在推理阶段对解剖结构进行几何与拓扑控制的可行性；框架可灵活应用于多种解剖系统、跨维度与坐标系，便于为虚拟试验和ML工作流设计合成数据。

Conclusion: 该方法提供一个灵活的框架，通过局部控制域与持久同调实现对解剖图的几何与拓扑的可控生成，促进解剖数据的理性设计与应用。

Abstract: We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.

</details>


### [116] [Latent Diffusion Inversion Requires Understanding the Latent Space](https://arxiv.org/abs/2511.20592)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 提出针对潜在空间扩散模型的记忆性分析与维度排序，改进成员身份推断攻击的准确性。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型的记忆性研究中，忽略了自编码器几何对潜在编码的影响；本文揭示潜在编码的非均匀记忆性及单个维度的贡献差异，并探讨其对隐私攻击的影响。

Method: 提出一种基于解码回拉（pullback）的对潜在维度贡献排序的方法，识别对记忆性贡献最大的维度；在score-based成员身份推断攻击中移除记忆性较低的维度以提高攻击性能；在CIFAR-10、CelebA、ImageNet-1K、 Pokémon、MS-COCO、Flickr等数据集上验证，给出AUROC及TPR@FPR等指标。

Result: 移除低记忆性维度后，平均AUROC提升约2.7%，TPR@1%FPR提升约6.42%，在多数据集上表现稳健。

Conclusion: 强调自编码器几何在LDM记忆性中的作用，提供分析 diffusion-based 生成模型隐私风险的新视角，并为隐私防护提供方向。

Abstract: The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.

</details>


### [117] [The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting](https://arxiv.org/abs/2511.20601)
*Heman Shakeri*

Main category: cs.LG

TL;DR: 提出“Driver-Blindness”概念：在血糖预测中，深度序列模型往往未能有效利用临床驱动因子（胰岛素、餐 meal、活动等），导致多变量模型对比单变量基线的性能提升不足。作者将Δ_drivers定义为多变量模型相对于匹配的单变量基线的性能增益，文献中Δ_drivers通常接近零。


<details>
  <summary>Details</summary>
Motivation: 揭示为何现有深度模型未能充分利用生理学上重要的驱动因子，量化驱动因素对预测性能的贡献，推动模型设计和评估的改进，使预测更具临床意义。

Method:  formalize Δ_drivers 的框架并对相关文献进行综合分析，提出导致驱动利用不足的三大因素C1-C3：架构偏好自相关性、数据质量偏差及生理异质性。基于此 synthesis 出缓解策略（如生理特征编码器、因果正则化、个性化），并建议未来工作在报告中系统呈现 Δ_drivers 以避免驱动盲模型继续被误认为是最先进。

Result: 研究发现跨文献的Δ_drivers接近零，原因在于C1-C3的相互作用：模型架构偏好对自相关的偏重、数据驱动因素的噪声与混淆以及人群层面的生理异质性。提出的缓解策略能部分缓解驱动盲，但仍需在实践中验证其有效性和普适性。

Conclusion: 应在未来工作中强制报告 Δ_drivers，并结合生理特征编码、因果正则化与个性化等策略，以提升多变量模型对关键生理驱动因子的利用率，从而提升临床可用性和研究透明度。

Abstract: Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.

</details>


### [118] [DiFR: Inference Verification Despite Nondeterminism](https://arxiv.org/abs/2511.20621)
*Adam Karvonen,Daniel Reuter,Roy Rinberg,Luke Marks,Adrià Garriga-Alonso,Keri Warr*

Main category: cs.LG

TL;DR: 提出 Token-DiFR 和 Activation-DiFR，通过对比受信任参考实现的输出及激活指纹来验证大模型推理的正确性，能够在较少样本的情况下检测量化和实现错误，并提供开源集成。


<details>
  <summary>Details</summary>
Motivation: 随着对大语言模型推理需求的增长，提供者与用户需要可靠地验证推理输出是否正确且未被篡改。但同一推理过程的重复执行常因数值噪声产生差异，难以区分合理波动和实际问题，因此需要可证实的、零附加成本的输出证据。

Method: Token-DiFR 通过将生成 token 与使用相同随机种子、经过信任参考实现的预测进行对比来验证推理输出；采样种子同步使有效输出受限，输出 token 本身可作为正确性的可审计证据。Activation-DiFR 使用随机正交投影将激活压缩为指纹以便后续验证，适用于需要样本高效的前向验证。

Result: Token-DiFR 可在前 300 个输出 token 内对 4-bit 量化实现检测，AUC > 0.999；Activation-DiFR 仅用 2 个输出 token 就实现 AUC > 0.999，且与现有方法相比通信开销降低 25-75%。并提供与 vLLM 的开源集成以促进实际部署。

Conclusion: 该工作提供高效且鲁棒的推理结果验证方案，能够在不同实现和量化情景下提升推理过程的可验证性和可信度，且有实际部署的落地支持。

Abstract: As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.

</details>


### [119] [ROOT: Robust Orthogonalized Optimizer for Neural Network Training](https://arxiv.org/abs/2511.20626)
*Wei He,Kai Han,Hang Zhou,Hanting Chen,Zhicheng Liu,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: ROOT提出鲁棒正交化优化器，通过自适应牛顿迭代实现维度鲁棒的正交化，并结合近端优化框架抑制异常点噪声，提升大语言模型训练的稳定性与收敛速度，在噪声和非凸场景下显著优于Muon与Adam等优化器。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型训练中，随着模型规模增加，对数值精度的敏感性与训练不稳定性日益突出。基于动量的正交化优化器在鲁棒性方面存在维度脆弱性与对离群噪声的易感性。需要一种在不同体系结构下都具备维度鲁棒性并对异常点噪声具备鲁棒性的优化器。

Method: 提出两大鲁棒机制：1) 维度鲁棒正交化：使用自适应牛顿迭代，结合对矩阵规模的细粒度系数，确保在不同矩阵尺寸下保持稳定的正交化精度；2) 近端优化框架：通过近端优化抑制离群噪声，同时尽量保留有意义的梯度方向。

Result: 大量实验表明，与Muon及基于Adam的优化器相比，ROOT在鲁棒性、收敛速度和最终性能方面均有显著提升，尤其在嘈杂和非凸场景中表现突出。

Conclusion: 为现代大规模模型训练开发鲁棒且精确的优化器树立了新范式，代码将在GitHub上开源。

Abstract: The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.

</details>


### [120] [Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model](https://arxiv.org/abs/2511.20636)
*Ziyue Wang,Yayati Jadhav,Peter Pak,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 从图像直接生成 G-code，跳过 CAD/3D 模型，开启端到端的数据驱动原型化。


<details>
  <summary>Details</summary>
Motivation: CAD 建模在设计迭代中是主要瓶颈，修改 CAD 以适配快速原型化成本高、周期长。直接从图像到可执行 G-code 的方法可降低门槛、加速设计到成品的循环。

Method: 提出 Image2Gcode：以图像为输入，提取切片级结构线索；使用去噪扩散概率模型（DDPM）对 G-code 序列进行逐步去噪，将高斯噪声转化为可执行的打印轨迹和挤出参数，直接从视觉输入映射到本地工具路径，无需显式 3D 模型。并可与 2D–3D 重建模块耦合，形成从概念到物体的端到端流水线。

Result: 直接由二维图像生成 G-code，省略 CAD/STL 等中间环节，降低进入门槛，提升快速原型设计与分布式制造的效率；支持按需原型、远程制造，且具有一定的计算效率；可与上游的 2D–3D 重建流程协同。

Conclusion: 该框架在设计迭代、修复工作流与分布式制造方面提升可及性，展示了从视觉输入直接映射到原生工具路径的可行性，推动端到端制造流程的发展。

Abstract: Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [121] [The Quality of Information: A Weighted Entropy Approach to Near-Optimal Mastermind](https://arxiv.org/abs/2511.19446)
*Serkan Gür*

Main category: cs.IT

TL;DR: 使用加权熵启发式的Mastermind信息理论策略，结合Belis-Guias框架与遗传算法优化，达到同类启发式方法中的最优性能。


<details>
  <summary>Details</summary>
Motivation: 提高Mastermind求解的信息价值估计，通过对反馈类型赋予上下文相关效用，利用遗传算法发现可解释的权重模式，显著提升平均猜测次数与上限。

Method: 基于加权熵启发式，采用Belis-Guias框架对每种反馈类型分配上下文相关的效用；以遗传算法优化权重，形成单一固定向量；后又提出分阶段权重，在不同回合使用不同的效用向量。

Result: 在单一权重向量下平均猜测4.3565次，最大5次；在分阶段权重下平均4.3488次，最大6次，接近理论最优4.3403，误差不足0.2%。

Conclusion: 所提出方法在仍具备经典一步到位启发式方法的计算效率前提下，显著提升了信息价值估计的效果；实现可复现的完整实现与优化参数。

Abstract: This paper presents a novel class of information-theoretic strategies for solving the game of Mastermind, achieving state-of-the-art performance among known heuristic methods. The core contribution is the application of a weighted entropy heuristic, based on the Belis-Guias, u framework, which assigns context- dependent utility values to each of the possible feedback types. A genetic algorithm optimization approach discovers interpret-able weight patterns that reflect strategic game dynamics. First, I demonstrate that a single, fixed vector of optimized weights achieves a remarkable 4.3565 average guesses with a maximum of 5. Building upon this, I introduce a stage-weighted heuristic with distinct utility vectors for each turn, achieving 4.3488 average guesses with a maximum of 6, approaching the theoretical optimum of 4.3403 by less than 0.2%. The method retains the computational efficiency of classical one-step-ahead heuristics while significantly improving performance through principled information valuation. A complete implementation and all optimized parameters are provided for full reproducibility.

</details>


### [122] [The Semiotic Channel Principle: Measuring the Capacity for Meaning in LLM Communication](https://arxiv.org/abs/2511.19550)
*Davide Picca*

Main category: cs.IT

TL;DR: 提出一个半符号框架，将大型语言模型视为随机半符号发动机，其输出需要人类主动且非对称地解读。使用信息理论刻画表达丰富性（源熵）与可解读性（消息与人类解读之间的互信息）之间的权衡，并引入生成复杂性参数λ来驱动该权衡。这一核心张力源自它们对λ的不同响应。定义一个由受众和情境参数化的半符号通道，并提出意义传输的容量约束（通过优化λ实现最大可解读性）。提供从模型分析到可观测文本的可度量框架，并在四个应用场景中演示其效用：模型画像、提示/情境设计优化、基于歧义的风险分析、以及自适应半符号系统。结论认为该容量驱动的半符号方法为理解、评估和设计LLM媒介的沟通提供了一个严格、可操作的工具。


<details>
  <summary>Details</summary>
Motivation: 驱动研究的核心动机是将LLM的输出理解为一种半符号过程，关注可观测文本而非模型内在细节，建立一个可量化的框架来衡量表达丰富性与解读稳定性之间的权衡，以提高对LLM沟通效果的评估与设计能力。

Method: 提出一个半符号框架：将表达丰富性定义为源熵，将 decipherability 定义为消息与人类解读之间的互信息；引入生成复杂性参数λ，形成对λ的依赖以决定整体现象；将意义传输建立在一个由受众与情境参数化的半符号通道上，并设定容量约束，通过优化λ来实现最大可解读性；强调从模型内部不可观的性质转向对文本产出本身的可观测分析，并提供可用于经验测量的工具。

Result: 作为框架性工作，论文展示了在四个应用场景中的潜在效用：①对模型进行画像；②优化提示与情境设计；③基于模糊性与歧义的风险分析；④构建自适应半符号系统。这些应用共同体现了该方法的可操作性与普遍性。作者主张该容量驱动的半符号分析能够提供一个严谨且可执行的工具箱，用于理解、评估和设计LLM介导的沟通。

Conclusion: 容量驱动的半符号方法为理解、评估和设计LLM-mediated沟通提供了一个严格且可操作的工具箱，适用于模型画像、提示优化、风险分析和自适应系统等场景。

Abstract: This paper proposes a novel semiotic framework for analyzing Large Language Models (LLMs), conceptualizing them as stochastic semiotic engines whose outputs demand active, asymmetric human interpretation. We formalize the trade-off between expressive richness (semiotic breadth) and interpretive stability (decipherability) using information-theoretic tools. Breadth is quantified as source entropy, and decipherability as the mutual information between messages and human interpretations. We introduce a generative complexity parameter (lambda) that governs this trade-off, as both breadth and decipherability are functions of lambda. The core trade-off is modeled as an emergent property of their distinct responses to $λ$. We define a semiotic channel, parameterized by audience and context, and posit a capacity constraint on meaning transmission, operationally defined as the maximum decipherability by optimizing lambda. This reframing shifts analysis from opaque model internals to observable textual artifacts, enabling empirical measurement of breadth and decipherability. We demonstrate the framework's utility across four key applications: (i) model profiling; (ii) optimizing prompt/context design; (iii) risk analysis based on ambiguity; and (iv) adaptive semiotic systems. We conclude that this capacity-based semiotic approach offers a rigorous, actionable toolkit for understanding, evaluating, and designing LLM-mediated communication.

</details>


### [123] [One-Shot Coding and Applications](https://arxiv.org/abs/2511.19556)
*Yanxiao Liu*

Main category: cs.IT

TL;DR: 该摘要描述了一种面向“一次信息论”的 achievability 研究框架，利用泊松函数表示以及泊松匹配引理，将一类统一的一次性编码方案扩展到更复杂场景，并在此基础上对照并推导出对记忆性或遍历性系统的渐近结论。


<details>
  <summary>Details</summary>
Motivation: 旨在给出明确的一次性 achievability 结果，这些结果能够在对记忆无关或遍历系统应用时推导出一阶、二阶等渐近结论；同时通过扩展泊松基方法，使其适用于更广泛的信息处理场景。

Method: 回顾并扩展泊松函数表示（Poisson functional representation）及其相关的泊松匹配引理（Poisson matching lemma），提出适用于更复杂场景的扩展版本，并给出在一次信息单位长度情形下的显式编码方案，形成一个可用于多类信息论问题的一体化框架。

Result: 主要贡献在于将泊松函数表示的适用性扩展到原始版本无法直接应用的更复杂情境，进而发展出更广泛的一次性编码方案的统一方法，能够覆盖多种信息论问题的单次分析。

Conclusion: 该工作为一次信息论中的证明工具箱提供了更广泛的适用性，建立了从一次性可证明的 achievability 推导渐近结果的途径，尤其适用于具有记忆或非简单统计特性的系统的分析。

Abstract: One-shot information theory addresses scenarios in source coding and channel coding where the signal blocklength is assumed to be 1. In this case, each source and channel can be used only once, and the sources and channels are arbitrary and not required to be memoryless or ergodic. We study the achievability part of one-shot information theory, i.e., we consider explicit coding schemes in the oneshot scenario. The objective is to derive one-shot achievability results that can imply existing (first-order and second-order) asymptotic results when applied to memoryless sources and channels, or applied to systems with memory that behave ergodically.
  Poisson functional representation was first proposed as a one-shot channel simulation technique by Li and El Gamal [118] for proving a strong functional representation lemma. It was later extended to the Poisson matching lemma by Li and Anantharam [117], which provided a unified one-shot coding scheme for a broad class of information-theoretic problems. The main contribution of this thesis is to extend the applicability of Poisson functional representation to various more complicated scenarios, where the original version cannot be applied directly and further extensions must be developed.

</details>


### [124] [Computer-aided Characterization of Fundamental Limits of Coded Caching with Linear Coding](https://arxiv.org/abs/2511.19639)
*Niccolò Brembilla,Yinbin Ma,Pietro Belotti,Federico Malucelli,Daniela Tuninetti*

Main category: cs.IT

TL;DR: 本论文提出一个电脑辅助框架，用非香农型不等式处理可表示多态胞结构（以及线性码），在对称性和问题约束下简化LP求解，以得到更紧的线性编码下的缓存-加载权衡界限，并在某些情形证明了可实现解的最优性。


<details>
  <summary>Details</summary>
Motivation: 在线性编码约束下表征编码缓存系统的基本极限；现有分析方法往往保守，难以获得紧致界限，因此需要利用非香农型不等式和多态胞结构来提升界限，并探讨放置与传送阶段的线性编码下的最优性。

Method: 提出一个电脑辅助框架，结合可表示多态胞的非香农型不等式、对称性、以及缓存编码问题的特定约束，将问题转化为受控的线性规划，减少变量与约束；并在放置与传送阶段假设线性编码、采用小规模需求子集来推导反例界与可行点的最优性。

Result: 获得比以往分析方法更紧的对偶界，证明在线性编码放置与传送约束下，某些可实现的内存-负载权衡点是最优的；并指出小规模有结构的需求子集与最小公共信息构造可能足以刻画线性编码下的最优权衡。

Conclusion: 该框架为在线性缓存编码约束下系统极限的计算方法提供了可行路径，表明通过有限的结构与小规模需求集即可接近甚至达到最优界，并可能启发设计基于线性编码的缓存策略。

Abstract: Inspired by prior work by Tian and by Cao and Xu, this paper presents an efficient computer-aided framework to characterize the fundamental limits of coded caching systems under the constraint of linear coding. The proposed framework considers non-Shannon-type inequalities which are valid for representable polymatroids (and hence for linear codes), and leverages symmetric structure and problem-specific constraints of coded caching to reduce the complexity of the linear program. The derived converse bounds are tighter compared to previous known analytic methods, and prove the optimality of some achievable memory-load tradeoff points under the constraint of linear coding placement and delivery. These results seem to indicate that small, structured demand subsets combined with minimal common information constructions may be sufficient to characterize optimal tradeoffs under linear coding.

</details>


### [125] [Joint Satellite Power Consumption and Handover Optimization for LEO Constellations](https://arxiv.org/abs/2511.19745)
*Yassine Afif,Mohammed Almekhlafi,Antoine Lesage-Landry,Gunes Karabulut Kurt*

Main category: cs.IT

TL;DR: Proposes a handover-aware optimization for LEO-to-user links to maximize total transmission rate while controlling handoffs.


<details>
  <summary>Details</summary>
Motivation: LEO constellation handoffs cause frequent signaling and energy overhead; as constellations grow, there is a need to balance coverage, throughput, and handoff costs.

Method: Formulates the problem as a mixed-integer concave linear program (MICP) that jointly optimizes total transmit power, user-satellite associations, and a penalty for handoff events; solvable with standard solvers.

Result: Monte Carlo simulations show significant gains in user throughput (≈40%) with a controlled increase in handoff frequency compared to a naive nearest-satellite association baseline.

Conclusion: A handover-aware optimization framework can substantially improve user rates without triggering disproportionate handoff growth, offering a practical solution for LEO-to-user systems.

Abstract: In satellite constellation-based communication systems, continuous user coverage requires frequent handoffs due to the dynamic topology induced by the Low Earth Orbit (LEO) satellites. Each handoff between a satellite and ground users introduces additional signaling and power consumption, which can become a significant burden as the size of the constellation continues to increase. This work focuses on the optimization of the total transmission rate in a LEO-to-user system, by jointly considering the total transmitted power, user-satellite associations, and power consumption, the latter being handled through a penalty on handoff events. We consider a system where LEO satellites serve users located in remote areas with no terrestrial connectivity, and formulate the power allocation problem as a mixed-integer concave linear program (MICP) subject to power and association constraints. Our approach can be solved with off-the-shelf solvers and is benchmarked against a naive baseline where users associate to their closest visible satellite. Extensive Monte Carlo simulations demonstrate the effectiveness of the proposed method in controlling the handoff frequency while maintaining high user throughput. These performance gains highlight the effectiveness of our handover-aware optimization strategy, which ensures that user rates improve significantly, by about 40%, without incurring a disproportionate rise in the handoff frequency.

</details>


### [126] [Two-Step Decoding of Binary $2\times2$ Sum-Rank-Metric Codes](https://arxiv.org/abs/2511.19812)
*Hao Wu,Bocong Chen,Guanghui Zhang,Hongwei Liu*

Main category: cs.IT

TL;DR: 提出一种两步解码策略：先对 C2 做唯一解码，然后对 C1 做一次错误/擦除解码即可将二进制和和秩度码 SR(C1, C2) 的解码简化为对分量码的解码；去除了 d1 ≥ 2/3 d_sr 的限制；总体复杂度为 T2+T1；在黑箱模型下该折衷是渐近最优的；在 BCH 或 Goppa（F4 上）实现时复杂度为 O(l^2)。


<details>
  <summary>Details</summary>
Motivation: 回应 Chen–Cheng–Qi 的开放问题：是否能将 SR(C1, C2) 的解码完全降维到对分量的 Hamming 解码而不需要额外的约束 d1≥2/3 d_sr；如果可行，则提升解码效率与简化实现。

Method: 提出两步解码流程：第一步对 C2 进行唯一解码；第二步对 C1 进行一次错误/擦除解码；分析表明不再需要 d1≥2/3 d_sr；给出总复杂度 T2+T1；给出在黑箱模型下的渐近最优性证明。

Result: 给出结论：可将 SR(C1, C2) 的解码完全降维到对 C2、C1 的解码；唯一解码界为 floor((d_sr - 1)/2)；实现对 BCH/Goppa over F4 的简化，时间复杂度 O(l^2)。

Conclusion: 限制性假设被移除，解码器在理论和实际复杂度上都具备优势；在具体代数构造，如 BCH/Goppa over F4，具有高效实现。

Abstract: We resolve an open problem posed by Chen--Cheng--Qi (IEEE Trans.\ Inf.\ Theory, 2025): can decoding of binary sum-rank-metric codes $\SR(C_1,C_2)$ with $2\times2$ matrix blocks be reduced entirely to decoding the constituent Hamming-metric codes $C_1$ and $C_2$ without the additional requirement $d_1\ge\tfrac{2}{3}d_{\mathrm{sr}}$ that underlies their fast decoder? We answer this in the affirmative by exhibiting a simple two-step procedure: first uniquely decode $C_2$, then apply a single error/erasure decoding of $C_1$.This shows that the restrictive hypothesis $d_1\ge\tfrac{2}{3}d_{\mathrm{sr}}$ is theoretically unnecessary.The resulting decoder achieves unique decoding up to $\lfloor (d_{\mathrm{sr}}-1)/2\rfloor$ with overall cost $T_2+T_1$, where $T_2$ and $T_1$ are the complexities of the Hamming decoders for $C_2$ and $C_1$, respectively. We further show that this reduction is asymptotically optimal in a black-box model, as any sum-rank decoder must inherently decode the constituent Hamming codes.For BCH or Goppa instantiations over $\F_4$, the decoder runs in $O(\ell^2)$ time.

</details>


### [127] [On hierarchical secure aggregation against relay and user collusion](https://arxiv.org/abs/2511.20117)
*Min Xu,Xuejiao Han,Kai Wan,Gennian Ge*

Main category: cs.IT

TL;DR: This work analyzes hierarchical secure aggregation (HSA) in federated learning under relay and user collusion, derives fundamental limits on communication loads, provides collusion-resilient schemes, and links HSA with network function computation to advance information-theoretic security and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable privacy-preserving federated learning with hierarchical secure aggregation in the presence of colluding relays and users, while optimizing communication efficiency.

Method: Modeling a homogeneous network where each user connects to n relays and each relay serves m users. A two-phase communication protocol is analyzed: users send masked data to relays, relays process and forward to a server to enable exact sum recovery. The paper derives lower bounds on per-link communication load, constructs collusion-resilient schemes when colluding parties are below thresholds, and addresses large random-key requirements. It also provides a key-size bound and achievability results for cyclic networks, and connects HSA to network function computation.

Result: Fundamental limits on per-link communication loads and collusion thresholds are established. When the number of colluding relays/users is below critical values, communication-optimal schemes are constructed. A lower bound on required key size is derived, with achievability proven in cyclic networks. The work also links HSA with network function computation to broaden theoretical insights.

Conclusion: The study advances the theoretical limits of communication efficiency and information-theoretic security in secure aggregation by integrating hierarchical secure aggregation with network function computation, and by providing practical insights for key-size requirements and collusion-resilient schemes.

Abstract: Secure aggregation (SA) is fundamental to privacy preservation in federated learning (FL), enabling model aggregation while preventing disclosure of individual user updates. This paper addresses hierarchical secure aggregation (HSA) against relay and user collusion in homogeneous networks, where each user connects to $n$ relays and each relay serves $m$ users. In the two-phase communication framework, users transmit masked data to relays, which then process and forward compiled messages to the server for exact sum recovery. The primary objective is to devise a transmission scheme such that the server can finish the aggregation task, while any group of $T_h$ colluding relays and $T_u$ colluding users cannot reveal any information about the data owned by the non-colluding users. In this study, we establish fundamental limits on the communication load, defined as the ratio of transmitted information size to original data size, for each user-relay link and each relay-server link. Achievable thresholds for collusion resilience are also derived. When the number of colluding relays and users falls below certain critical thresholds, we construct communication-optimal schemes using methods from network function computation. A limitation of these schemes is their reliance on large random keys. To address this, we derive a lower bound on the required key size and prove its achievability in cyclic networks, where users are connected to relays in a cyclic wrap-around manner. By establishing a connection between HSA and network function computation, this work advances the theoretical limits of communication efficiency and information-theoretic security in secure aggregation.

</details>
