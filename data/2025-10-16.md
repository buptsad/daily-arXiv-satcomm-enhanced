<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 8]
- [cs.CR](#cs.CR) [Total: 10]
- [eess.SY](#eess.SY) [Total: 15]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.IT](#cs.IT) [Total: 8]
- [cs.LG](#cs.LG) [Total: 71]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Effective Connectivity-Based Unsupervised Channel Selection Method for EEG](https://arxiv.org/abs/2510.12910)
*Neda Abdollahpour,N. Sertac Artan,Ian Daly,Mohammadreza Yazdchi,Zahra Baharlouei*

Main category: eess.SP

TL;DR: 提出基于有效连接的ICEC准则来进行EEG通道无监督选择，并在CSP-SVM框架下对多数据集进行评估，结果在不同EC指标下表现优越且显著减少选用通道数。


<details>
  <summary>Details</summary>
Motivation: 在高维EEG数据中筛选最具信息量的通道以提升计算效率和鲁棒性；现有方法可能未充分考虑通道间的交互强度，因此需要一种能够量化通道间有效连接并据此进行通道选择的无监督方法。

Method: 提出ICEC准则，通过五种有效连接指标PDC、GPDC、RPDC、DTF、dDTF来量化每个通道的有效连接强度；构建无监督的通道选择算法；在CSP特征提取与SVM分类框架下对选通道进行评估。

Result: 在三份EEG数据集上，基于五种EC指标的ICEC方法实现了明显的准确率提升并显著减少了选通道数，具体表现为：82%（13/22通道）、86.01%（29/59通道）、87.56%（48/118通道）；并在与其他CSP基方法的比较中达到更高的准确率。

Conclusion: 基于ICEC的无监督通道选择能够有效捕捉通道间的有向信息，提升分类性能的同时显著减少参与通道数，对EEG分析的计算效率和鲁棒性具有潜在应用价值。

Abstract: Analyzing neural data such as Electroencephalography (EEG) data often
involves dealing with high-dimensional datasets, where not all channels provide
equally meaningful informa- tion. Selecting the most relevant channels is
crucial for improving computational efficiency and ensuring robust insights
into neural dynamics. This study introduces the Importance of Channels based on
Effective Connectivity (ICEC) criterion for quantifying effective connectivity
(EC) in each channel. Effective connectivity refers to the causal influence one
neural region exerts over another, providing insights into the directional flow
of information. Using this criterion, we propose an unsupervised channel
selection method that accounts for the intensity of interactions among
channels. To evaluate the proposed channel selection method, we applied it to
three well-known EEG datasets across four categories. The assessment involved
calculating the ICEC criterion using five effective connectivity metrics:
partial directed coherence (PDC), generalized PDC (GPDC), renormalized PDC
(RPDC), directed transfer function (DTF), and direct DTF (dDTF). To focus on
the effect of channel selection, we employed the Common Spatial Pattern (CSP)
algorithm for feature extraction and a Support Vector Machine (SVM) for
classification across all participants. Results were compared with other
CSP-based methods. The evaluation included comparing participant- specific
accuracies with and without the proposed method across five effective
connectivity metrics. The results showed consistent performance improvements
and a significant reduction in the number of selected electrodes for all
participants. Compared to state-of-the-art methods, our approach achieved the
highest accuracies: 82% (13 out of 22 channels), 86.01% (29 out of 59
channels), and 87.56% (48 out of 118 channels) across three datasets.

</details>


### [2] [Enabling Full Duplex ISAC Leveraging Waveform Domain Separability](https://arxiv.org/abs/2510.12912)
*Abdelali Arous,Hamza Haif,Huseyin Arslan*

Main category: eess.SP

TL;DR: 提出一种基于仿射域的自干扰消除（SIC）技术，利用 OFDM 与 AFDM 在同一调制器下实现通信与雷达的一体化，在仿射域将干扰近似为 AWGN，从而实现高效SI抑制并通过迭代窗函数和时域扩展降低残留干扰，提升探测概率、目标距离和速度估计精度，同时保持高光谱效率与低复杂度。


<details>
  <summary>Details</summary>
Motivation: 在单发/单天线全双工（IBFD）ISAC系统中，雷达接收端的自干扰（SI）成为制约性能的主要瓶颈，因此需要新的SIC方案以实现双功能峰值性能。

Method: 设计一个双功能帧：通信使用 OFDM，雷达使用 AFDM，且两者由同一调制器产生。分析在时间域的广义平稳性使其在仿射域呈现为宽平稳过程，并证明干扰的 OFDM 信号在仿射域近似为 AWGN。于是将接收信号投影到仿射域进行SI减除；对残留SI进行迭代低复杂度的窗口化，选择性锁定雷达信号以减小处理空间；随后将经 SIC 处理的信号转化为后编码时间域并进行时域扩展，使得SI在延迟和多普勒轴上逐步降低。

Result: 理论和仿真结果显示，该方法在探测概率、目标距离和速度的RMSE方面显著优于基线方法，同时保持较高的频谱效率和较低的计算复杂度。

Conclusion: 提出的SI消除框架有效实现ISAC系统中雷达与通信的协同工作，显著降低自干扰并提升关键性能指标，同时保持实现复杂度的友好性。

Abstract: Integrated sensing and communication (ISAC) in monostatic in-band full-duplex
(IBFD) systems encounters significant challenges due to self-interference (SI)
at the radar receiver during concurrent communication and radar operations.
This paper proposes a novel waveform-domain self-interference cancellation
(SIC) technique that leverages the unique properties of orthogonal frequency
division multiplexing (OFDM) and affine frequency division multiplexing (AFDM)
signals. The proposed approach designs the integrated dual-functionality frame
to utilize OFDM for communication and AFDM for radar sensing, both generated
using the same modulator block. Then, we establish the conditions under which a
wide sense stationary (WSS) process in the time domain appears as WSS in the
affine domain and demonstrate that the interfering OFDM signal behaves as an
additive white Gaussian noise (AWGN) in this domain. Exploiting this property,
the received signal is projected into the affine domain, where the SI appears
as AWGN, enabling its subtraction with minimal residual interference. To
further mitigate the residual SI, an iterative low-complexity windowing scheme
is applied, selectively locking onto the radar signal to reduce the processed
signal space. A subsequent time-domain spreading step is applied after
converting the SIC-processed signal into the post-coded time domain, wherein
the SI diminishes separately across the delay and Doppler axes. The proposed
method demonstrates superior performance in terms of detection probability,
target range and velocity root mean square error (RMSE), while maintaining high
spectral efficiency and minimal computational complexity.

</details>


### [3] [Passive Microwave Tag Classification Using RF Fingerprinting and Machine Learning](https://arxiv.org/abs/2510.12930)
*Cory Hilton,Mohammad Rashid,Faiz Sherman,Steven Bush,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: 通过利用二极管非线性响应的频谱特征，与机器学习相结合，实现对低成本无线标签的唯一识别；在双站雷达系统中实现对两标签的实时分类，达到95%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决低成本、简单无线标签的唯一性识别问题；利用制造扰动导致的非线性响应差异作为独特指纹，以区分不同设备。

Method: 设计2.0 GHz的标签（两天线+单二极管），标签的非线性响应可用无限幂级数表示，因制造差异导致系数微小不同，表现为谱响应差异。 interrogator 发射包含多音调的信号，标签将非线性谱响应重新发回，接收端处理并提取谱特征，随后将特征输入多种机器学习算法进行分类。以2.0 GHz 802.11 Wi‑Fi带为场景，在双静态雷达系统下测试，使用常见的802.11训练场信号。

Result: 实现了在两标签之间的实时分类，分类准确率达到95%。

Conclusion: 证实利用二极管非线性谱差异作为指纹来唯一标识低成本无线标签的可行性；该方法为低成本标签的设备鉴别提供了一个有效的机器学习驱动的指纹识别框架，适用于无线传感和物联网场景。

Abstract: We present an approach to identifying wireless microwave tags using radio
frequency (RF) fingerprinting and machine learning. The tags are designed for
low cost and simplicity, consisting of only two antennas and a single nonlinear
element (a diode). An interrogating transceiver transmits a signal consisting
of a set of individual frequency tones that is captured by the tag. The signal
response of the diode is nonlinear, and can be represented by an infinite power
series, the coefficients of which are similar but not identical for different
physical diodes due to small manufacturing perturbations. The small differences
in the signal responses manifest in the spectral signal response of the tag,
which is retransmitted back to the interrogating transceiver. Input into
machine learning algorithms, the slight differences in the spectral responses
of the diodes can be used to uniquely identify devices. To demonstrate the
concept, we designed 2.0 GHz tags consisting of patch antennas and a single
diode, along with a bi-static radar system operating at the 2.0 GHz 802.11
Wi-Fi band transmitting multi-tone continuous wave signals representing common
802.11 training fields. The received signals were processed using a set of
algorithms for comparison purposes. A real-time classification accuracy of 95%
between two tags was achieved.

</details>


### [4] [Towards Spectrally Efficient and Physically Reconfigurable Architectures for Multibeam-Waveform Co-Design in Joint Communication and Sensing](https://arxiv.org/abs/2510.12968)
*Najme Ebrahimi,Arun Paidmarri,Alexandra Gallyas-Sanhueza,Yuan Ma,Haoling Li,Basem Abdelaziz Abdelmagid,Tzu-Yuan Huang,Hua Wang*

Main category: eess.SP

TL;DR: 研究多波束JCAS架构在时间、频率、码域与直接射频域中的波形塑形与波束成形优化，比较OFDM、FMA、TMA、直接RF/MMW调制与CDMA在光谱效率、波束正交性、延迟与AoA估计等指标的权衡，并给出面向功率、延迟、跨波束/多用户干扰与快速重构的前端选型框架。


<details>
  <summary>Details</summary>
Motivation: 随着毫米波和亚太材系统的发展，JCAS平台需要在同一信道中实现高吞吐与高角度定位的统一优化，亟需跨时频/码域与射频前端的协同设计。

Method: 从多波束架构出发，系统性比较在时间、频率、码域以及直接RF域的波形塑形与波束成形方法，评估OFDM、FMA、TMA、直接RF/MMW调制、CDMA等在谱效率、波束正交性、时延与AoA估计方面的性能，构建权衡框架，并提出面向功率、延迟、干扰抑制与快速重构的JCAS前端选型框架。

Result: 揭示不同架构在波束灵活性、效率、定位精度、分辨率与系统复杂度方面的架构特定权衡；给出一个用于在功耗、延迟、跨波束与多用户干扰场景中可快速配置并实现高效重构的前端选型框架。

Conclusion: 多波束JCAS设计需在波束可控性、谱效率与定位精度之间进行权衡，提出的前端选型框架有助于在不同应用场景下实现快速、低功耗且对干扰鲁棒的联合通信与感知。

Abstract: Joint Communication and Sensing (JCAS) platforms are emerging as a foundation
of next-generation mmWave (MMW) and sub-THz systems, enabling both
high-throughput data transfer and angular localization within a shared signal
path. This paper investigates multibeam architectures for JCAS that
simultaneously optimize waveform shaping and beamforming across the time,
frequency, code, and direct analog/ radio frequency (RF) domains. The paper
compares Orthogonal Frequency-Division Multiplexing (OFDM), Frequency Modulated
Arrays (FMA), Time-Modulated Arrays (TMA), direct RF/MMW modulation, and
Code-Division Multiple Access (CDMA)-based systems with respect to spectral
efficiency, beam orthogonality, latency, and Angle-of-Arrival (AoA) estimation
accuracy. The results highlight architecture-specific tradeoffs among beam
agility, efficiency, accuracy and resolution, and complexity. It also provides
a framework for selecting JCAS front ends optimized for power, latency,
inter-beam and multi-user interference, and rapid system reconfiguration

</details>


### [5] [Constellation Design in OFDM-ISAC over Data Payloads: From MSE Analysis to Experimentation](https://arxiv.org/abs/2510.13101)
*Kawon Han,Kaitao Meng,Alexandra Chatzicharistou,Christos Masouros*

Main category: eess.SP

TL;DR: 给出OFDM-ISAC中多目标延时估计的MSE极限及两种接收机下星座统计的影响，提出受接收机架构约束的感知-通信权衡星座设计，并通过仿真与实验验证。


<details>
  <summary>Details</summary>
Motivation: 在OFDM-基于ISAC的系统中，需要同时提升目标延迟估计和通信性能，但多目标场景下信号星座统计对感知性能的影响尚未清晰，需给出理论极限并提供设计指南。

Method: 建立面向随机载荷的OFDM-ISAC感知-通信分析框架，推导匹配滤波( MF )与互相关滤波( RF )接收的延时MSE闭式表达；分析在多目标场景下星座的四阶矩与二阶矩对MF、RF的影响；提出在特定接收架构下的ISAC星座设计以实现灵活的感知与通信权衡；通过仿真和原型实验验证理论结论。

Result: 得到MF与RF两类接收的延时MSE闭式公式；MF需考虑星座的四阶矩在多目标时的影响，RF仅依赖星座的逆二阶矩且与目标数无关；提出的星座设计在理论和实验中均表现出可控的感知-通信权衡。

Conclusion: 理论与实验共同支持，一类面向OFDM-ISAC的星座设计可在给定接收架构下实现对感知与通信权衡的灵活控制，为实际系统提供设计指南。

Abstract: Orthogonal frequency division multiplexing (OFDM) is one of the most widely
adopted waveforms for integrated sensing and communication (ISAC) systems,
owing to its high spectral efficiency and compatibility with modern
communication standards. This paper investigates the sensing performance of
OFDM-based ISAC for multi-target delay (range) estimation under specific radar
receiver processing schemes. An estimation-theoretic framework is developed to
characterize sensing performance with random communication payloads. We
establish the fundamental limit of delay estimation accuracy by deriving the
closed-form expression of the mean-square error (MSE) achieved using matched
filtering (MF) and reciprocal filtering (RF) receivers. The results show that,
in multi-target scenarios, the impact of signal constellations on the delay
estimation MSE differs across receivers: MF performance depends on the
fourth-order moment of the zero-mean, unit-power constellation in the presence
of multiple targets, whereas RF performance depends on its inverse second-order
moment, irrespective of the number of targets. Building on this analysis, we
present a ISAC constellation design under specific receiver architecture that
brings a receiver-dependent flexible trade-off between sensing and
communication in OFDM-ISAC systems. The theoretical findings are validated
through simulations and proof-of-concept experiments, and also the sensing and
communication performance trade-off is experimentally shown with the proposed
constellation design.

</details>


### [6] [Oscillator Drift Compensation by Line-of-Sight Tracking for Distributed Multisensor ISAC](https://arxiv.org/abs/2510.13442)
*Lorenz Mohr,Marc Miranda,Sebastian Semper,Julia Beuster,Carsten Andrich,Sebastian Giehl,Christian Schneider,Reiner S. Thomä*

Main category: eess.SP

TL;DR: 提出基于卡尔曼滤波的多传感器LoS跟踪后处理同步方法，平滑相位进展并纠正时变漂移，提升分布式ISAC系统中的相干性与估计鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在分布式ISAC系统中实现高精度Doppler估计需要各节点相干的相位进展，但实际观测中存在非平滑的相位进展和漂移，导致多传感器同步困难。

Method: 扩展传统几何漂移补偿算法，使用卡尔曼滤波对LoS进行跟踪以平滑相位、纠正时变漂移，同时在后处理阶段评估相对残余功率作为独立指标；并在HRPE输出上进行鲁棒性对比。

Result: 与传统LoS估计启发式方法相比，残留功率降低>5 dB，延迟-多普勒估计RMSE降低约60%。

Conclusion: 所提出的方法提高了分布式ISAC系统中后处理同步的鲁棒性和精度，适用于含多径的场景。

Abstract: We observed synchronization mismatches in the form of non-smooth phase
progressions and drifts within mobile multisensor channel sounding
measurements. However, performing Doppler estimation in a distributed
multisensor integrated sensing and communications (ISAC) system requires
coherence among the nodes, which implies a continuously differentiable phase
progression of the received signals. To correct the sounding data in
post-processing, we extend traditional geometry-based drift compensation
algorithms by utilizing Kalman filtering for line-of-sight (LoS) tracking,
which improves the robustness of the LoS estimate in multipath scenarios. This
approach smooths the phase progression and enables the correction of
time-varying drifts while preserving relative sensor motion. Furthermore, we
propose using the relative residual power after high-resolution parameter
estimation (HRPE) as a metric for ground-truth-independent comparison of
post-processing synchronization methods for recorded channel sounding data.
Results show that the proposed approach outperforms traditional LoS estimation
heuristics, reducing the relative residual power by more than 5 dB and the
delay-Doppler estimate root mean square errors (RMSEs) by approximately 60 %.

</details>


### [7] [Radio over Fiber with Cascaded Structure: Algorithm for Uplink Positioning](https://arxiv.org/abs/2510.13495)
*Dexin Kong,Diana Pamela Moya Osorio,Erik G. Larsson*

Main category: eess.SP

TL;DR: 提出了一种级联光纤射频（RoF）结构用于室内高密度子THz通信的定位与测距，通过最大似然与非线性最小二乘估计推导RoF传播距离和RoF与用户设备之间的到达时间，线性PA下给出CRLB，仿真验证在非线性PA级联放大和传输下估计器鲁棒性良好，且有助于高分辨率室内定位；数值评估采用测量的PMF高密度聚乙烯光纤特性。


<details>
  <summary>Details</summary>
Motivation: 推动低成本、高速的子THz RoF通信及室内高分辨率定位，利用多包层级联的PMF技术在室内场景中实现更可靠的射频信号传输与定位能力。

Method: 提出两类估计算法：最大似然估计（MLE）和非线性最小二乘估计（NLS），用于估计RoF中的传输距离以及RoF与用户设备之间的到达时间；在PA线性时导出Cramér–Rao下界（CRLB）以作为性能基准；对级联非线性PA与传输通道的影响进行仿真分析；在数值评估中引入测量得到的PMF特性。

Result: 估计器在遇到PA非线性级联放大及传输通道影响时仍能取得令人满意的性能，表明该RoF结构具有潜在的低成本高分辨率室内定位能力；仿真结果与线性PA的CRLB基准相符，且系统可用于提升室内定位精度。

Conclusion: 该工作证明了级联PMF- RoF结构在室内高分辨率定位中的可行性与潜力，结合测量的PMF特性，能为低成本、低复杂度的高带宽RoF系统提供定位能力的有力支撑；未来工作可进一步优化对非线性PA的鲁棒性及在实际室内场景中的实现。

Abstract: Recent advancements in polymer microwave fiber (PMF) technology have created
significant opportunities for robust, low-cost, and high-speed sub-terahertz
(THz) radio-over- fiber communications. Recognizing these potential benefits,
this paper explores a novel radio-over-fiber (RoF) structure that interconnects
multiple radio units (RUs) in cascade via fiber, envi- sioning its application
in indoor scenarios. This structure creates a number of research opportunities
when considering cascaded distortion effects introduced by non-linear power
amplifiers (PAs) at the RUs and the propagation channel over the fiber. We
propose maximum-likelihood and non-linear least-squares algorithms to estimate
the propagation distance along the RoF and the time-of-arrival between the RoF
and the user equipment. For the case of linear PAs, we derive the Cram\'er-Rao
lower bound to benchmark the performance of the estimators. Finally, we
investigate the use of the system for uplink positioning. Our simulation
results demonstrate that the proposed estimators perform satisfactorily even
with the cascaded effects of non- linear PAs, and that the deployment of this
RoF structure can enable new cost-effective opportunities for high-resolution
positioning in indoor scenarios. In the numerical evaluation, we also use
measured PMF characteristics for high-density polyethylene fibers.

</details>


### [8] [A Robust EDM Optimization Approach for 3D Single-Source Localization with Angle and Range Measurements](https://arxiv.org/abs/2510.13498)
*Mingyu Zhao,Qingna Li,Hou-Duo Qi*

Main category: eess.SP

TL;DR: 提出了一种鲁棒的欧氏距离矩阵（EDM）优化模型，将测量中的距离、角度与最小绝对偏差结合用于3D单源定位（3DSSL），首次将角度测量转化为简易的距离箱约束。


<details>
  <summary>Details</summary>
Motivation: 在源定位中，距离、角度和鲁棒性（对噪声的抵抗）是关键要素，但将三者整合在一个计算可行的模型中具有挑战性。本文提出一种鲁棒EDM模型，以同时利用三者信息并提高定位鲁棒性与精度。

Method: 将角度测量转化为两维非线性优化问题，从而得到未知源到传感器的距离的下界和上界，进而将角度信息表示为距离的箱约束；在此基础上构建鲁棒EDM优化模型，并提出高效算法以求解。

Result: 通过大量数值实验，与3DSSL的领先求解器比较，展示了新EDM模型在定位质量上的优势与鲁棒性提升。

Conclusion: 新颖的EDM框架实现了三要素（距离、角度、鲁棒性）的一体化建模，并将角度信息有效地转化为距离箱约束，提供了对3DSSL的高效且鲁棒的解决方案。

Abstract: For the problem of source localization, three elements usually play a very
important role in accurate localization. They are the range measurements, the
angle measurements and the least absolute deviation criterion, which is
regarded as a robust metric for denoising the measurements. Building the three
elements into a computationally tractable model is challenging. In this paper,
we introduce a robust Euclidean Distance Matrix (EDM) optimization model that
simultaneously incorporates the three elements. For the first time, we show
that for the case of 3D single-source localization (3DSSL), the angle
measurements can be represented as a simple box constraint of distances. It is
achieved by reducing each of the 3D angle measurements to a two-dimensional
nonlinear optimization problem, whose global minimum and maximum solutions can
be characterized and utilized to get the lower and upper bounds of the
distances from the unknown source to the sensors. We further develop an
efficient algorithm. The high quality of the localization by the new EDM model
is assessed through extensive numerical experiments in comparison with leading
solvers for 3DSSL.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [9] [The Beautiful Deception: How 256 Bits Pretend to be Infinity](https://arxiv.org/abs/2510.12802)
*Alexander Towell*

Main category: cs.CR

TL;DR: 核心观点：真随机性不可在有限表示中实现；通过懒惰求值和有限自动机，256位熵可以在有界计算者面前表现出近似无限的随机性；真随机预言机不可行。


<details>
  <summary>Details</summary>
Motivation: 揭示密码学中随机性的本质与真随机预言机不可达性，批判性分析“随机性”的本质，提示在设计密码系统时对随机源的界限性认识。

Method: 理论推理与形式证明，结合懒惰求值的构造演示一个有限自动机如何冒充无限随机性；并提供Python实现以实证展示对有界观察者的不可区分性。

Result: 给出结论：真预言机不可行；256位熵生成的序列在有界观察者面前几乎不可分辨于无限随机性；提供实现示例支持观点。

Conclusion: 密码学中的“随机性”是计算困难性的伪象；有限表示配合正确的构造可实现对无限随机性的近似，但不存在真正的真随机预言机，这对密码系统设计具有深远影响。

Abstract: How do you store infinity in 256 bits? This paper explores the fundamental
deception at the heart of computational cryptography: using finite information
to simulate infinite randomness. We prove why true random oracles are
impossible, then show how lazy evaluation creates a beautiful lie -- a finite
automaton that successfully pretends to be infinite. We reveal that
``randomness'' in cryptography is actually computational hardness in disguise,
demonstrating through Python implementations how 256 bits of entropy can
generate sequences indistinguishable from infinite randomness to any
computationally bounded observer.How do you store infinity in 256 bits? This
paper explores the fundamental deception at the heart of computational
cryptography: using finite information to simulate infinite randomness. We
prove why true random oracles are impossible, then show how lazy evaluation
creates a beautiful lie -- a finite automaton that successfully pretends to be
infinite. We reveal that ``randomness'' in cryptography is actually
computational hardness in disguise, demonstrating through Python
implementations how 256 bits of entropy can generate sequences
indistinguishable from infinite randomness to any computationally bounded
observer.

</details>


### [10] [We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice](https://arxiv.org/abs/2510.12812)
*Aleksandar Petrov,Pierre Fernandez,Tomáš Souček,Hady Elsahar*

Main category: cs.CR

TL;DR: 该工作给出在PSNR和线性鲁棒性约束下的图像水印容量上界，理论容量远超现有模型，实验也显示两者之间存在持续差距，并通过 ChunkySeal 将容量提升至1024比特，同时保持图像质量和鲁棒性，表明容量尚未饱和，未来仍有结构与训练策略创新空间。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的水印方法容量受限，难以突破数百比特的水平；需要明确水印容量的理论极限，以评估当前方法是否接近极限，并指导未来设计。

Method: 推导在PSNR与线性鲁棒性约束下的容量上界；通过在可分析的简单设置中验证理论与经验的差距；并训练 ChunkySeal（VideoSeal 的扩展版）以实现容量的放大（达到1024比特）。

Result: 理论容量比现有模型高出若干数量级；经验结果也显示即使在易分析的设定中，理论极限与实际实现之间的差距仍然存在；ChunkySeal 将容量提升4倍至1024比特，同时保持视觉质量与鲁棒性。

Conclusion: 现代方法尚未充分挖掘水印容量，存在显著的创新空间，未来可通过架构改进和训练策略实现更高容量和鲁棒性。

Abstract: Despite rapid progress in deep learning-based image watermarking, the
capacity of current robust methods remains limited to the scale of only a few
hundred bits. Such plateauing progress raises the question: How far are we from
the fundamental limits of image watermarking? To this end, we present an
analysis that establishes upper bounds on the message-carrying capacity of
images under PSNR and linear robustness constraints. Our results indicate
theoretical capacities are orders of magnitude larger than what current models
achieve. Our experiments show this gap between theoretical and empirical
performance persists, even in minimal, easily analysable setups. This suggests
a fundamental problem. As proof that larger capacities are indeed possible, we
train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4
times to 1024 bits, all while preserving image quality and robustness. These
findings demonstrate modern methods have not yet saturated watermarking
capacity, and that significant opportunities for architectural innovation and
training strategies remain.

</details>


### [11] [ARTeX: Anonymity Real-world-assets Token eXchange](https://arxiv.org/abs/2510.12821)
*Jaeseong Lee,Junghee Lee*

Main category: cs.CR

TL;DR: 提出 ARTeX 框架的一个用于 Real-World Asset (RWA) 代币交易的新平台，旨在在提高交易者匿名性的同时增强对非法活动的防护，试图解决现有隐私方法在 RWA 场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 区块链的透明性使交易者在快速增长的 RWA 代币市场中难以实现匿名性，现有用于 fungible tokens (FT) 的混币服务与用于 NFT 的隐私研究在 RWA 场景存在局限性，需提供兼顾隐私与合规的新解决方案。

Method: 提出 ARTeX 平台的概念设计与体系结构，结合隐私保护技术与合规性强化机制，针对 RWA 代币交易场景进行设计评估，提供理论分析与架构描述，尚未给出实证结果。

Result: 提出了一个新平台 ARTeX 的概念性设计与体系结构，解决隐私问题的同时强调对非法活动的防护；目前尚未报告实验或实证结果。

Conclusion: ARTeX 具备在 RWA 代币交易中提升匿名性并强化对非法活动防护的潜力，但需要在具体隐私实现、监管合规与可操作性方面给出更详细的技术方案与实验评估。

Abstract: This paper addresses one of the most noteworthy issues in the recent virtual
asset market, the privacy concerns related to token transactions of Real-World
Assets tokens, known as RWA tokens. Following the advent of Bitcoin, the
virtual asset market has experienced explosive growth, spawning movements to
link real-world assets with virtual assets. However, due to the transparency
principle of blockchain technology, the anonymity of traders cannot be
guaranteed. In the existing blockchain environment, there have been instances
of protecting the privacy of fungible tokens (FTs) using mixer services.
Moreover, numerous studies have been conducted to secure the privacy of
non-fungible tokens (NFTs). However, due to the unique characteristics of RWA
tokens and the limitations of each study, it has been challenging to achieve
the goal of anonymity protection effectively. This paper proposes a new token
trading platform, the ARTeX, designed to resolve these issues. This platform
not only addresses the shortcomings of existing methods but also ensures the
anonymity of traders while enhancing safeguards against illegal activities.

</details>


### [12] [SimKey: A Semantically Aware Key Module for Watermarking Language Models](https://arxiv.org/abs/2510.12828)
*Shingo Kodama,Haya Diwan,Lucas Rosenblatt,R. Teal Witter,Niv Cohen*

Main category: cs.CR

TL;DR: SimKey 通过将水印密钥与前文语义绑定，提升文本水印对改写的鲁棒性，并防止将无关文本误指为水印文本，从而实现更稳健、可扩展的水印方案。


<details>
  <summary>Details</summary>
Motivation: 现有的文本水印多数依赖伪随机密钥对下一个单词采样进行引导，易受 paraphrase、重新排序等表面性编辑影响，且若被他人附加无关文本，仍可能错误地将其归因于模型所有者，因此需要一种语义层面的密钥绑定以提升鲁棒性和防误 attribution。

Method: 提出 SimKey，一个语义密钥模块，将密钥生成与先前上下文的语义含义绑定。通过对语义嵌入进行局部敏感哈希（LSH），确保在保持语义的改写（如同义改写、局部改动）时仍能产生同一密钥，而语义上偏移或无关文本则产生不同密钥。将 SimKey 与现有的水印方案集成，提升对改写/翻译的鲁棒性，同时防止无关且有害内容被错误归因。

Result: 在对改写与翻译等攻击场景下，SimKey 显著提高水印的鲁棒性，降低误 attribution 的可能性，并在实现层面展示了良好的可集成性与扩展性。

Conclusion: 引入语义感知的密钥绑定是水印技术的一个实用且可扩展的方向，SimKey 能与现有水印方案无缝整合，提升对文本来源的可靠识别能力。

Abstract: The rapid spread of text generated by large language models (LLMs) makes it
increasingly difficult to distinguish authentic human writing from machine
output. Watermarking offers a promising solution: model owners can embed an
imperceptible signal into generated text, marking its origin. Most leading
approaches seed an LLM's next-token sampling with a pseudo-random key that can
later be recovered to identify the text as machine-generated, while only
minimally altering the model's output distribution. However, these methods
suffer from two related issues: (i) watermarks are brittle to simple
surface-level edits such as paraphrasing or reordering; and (ii) adversaries
can append unrelated, potentially harmful text that inherits the watermark,
risking reputational damage to model owners. To address these issues, we
introduce SimKey, a semantic key module that strengthens watermark robustness
by tying key generation to the meaning of prior context. SimKey uses
locality-sensitive hashing over semantic embeddings to ensure that paraphrased
text yields the same watermark key, while unrelated or semantically shifted
text produces a different one. Integrated with state-of-the-art watermarking
schemes, SimKey improves watermark robustness to paraphrasing and translation
while preventing harmful content from false attribution, establishing
semantic-aware keying as a practical and extensible watermarking direction.

</details>


### [13] [Towards Trusted Service Monitoring: Verifiable Service Level Agreements](https://arxiv.org/abs/2510.13370)
*Fernando Castillo,Eduardo Brito,Sebastian Werner,Pille Pullonen-Raudvere,Jonathan Heiss*

Main category: cs.CR

TL;DR: 提出通过TEE与零知识证明实现可验证的SLA违规主张，提升信任与自动化合规。


<details>
  <summary>Details</summary>
Motivation: 在服务导向环境中，提供者自报指标存在信任与激励冲突，亟需可验证、隐私保护的合规证明以减少争议并实现自动化。

Method: 将SLA条款转化为可验证谓词；在可信执行环境（TEE）中进行监控，收集带时间戳的遥测并组织成Merkle树，输出带签名的鉴证；用零知识证明聚合服务级指标（SLI）以评估合规性，在不暴露底层数据的前提下向利益相关方、仲裁方或保险方提供可验证的证明。

Result: 原型实现可线性扩展至每小时百万级事件，单次违规声明的证明生成与验证接近常数时间，支持信任最小化的SLA执行与自动化合规验证。

Conclusion: 建立了服务生态中真正可信的密码学基础，确保完整性、真实性与有效性，并为利益相关方、仲裁方、保险方提供可验证的证明。

Abstract: Service Level Agreement (SLA) monitoring in service-oriented environments
suffers from inherent trust conflicts when providers self-report metrics,
creating incentives to underreport violations. We introduce a framework for
generating verifiable SLA violation claims through trusted hardware monitors
and zero-knowledge proofs, establishing cryptographic foundations for genuine
trustworthiness in service ecosystems. Our approach starts with
machine-readable SLA clauses converted into verifiable predicates and monitored
within Trusted Execution Environments. These monitors collect timestamped
telemetry, organize measurements into Merkle trees, and produce signed
attestations. Zero-knowledge proofs aggregate Service-Level Indicators to
evaluate compliance, generating cryptographic proofs verifiable by
stakeholders, arbitrators, or insurers in disputes, without accessing
underlying data. This ensures three security properties: integrity,
authenticity, and validity. Our prototype demonstrates linear scaling up to
over 1 million events per hour for measurements with near constant-time proof
generation and verification for single violation claims, enabling trustless SLA
enforcement through cryptographic guarantees for automated compliance
verification in service monitoring.

</details>


### [14] [Local Differential Privacy for Federated Learning with Fixed Memory Usage and Per-Client Privacy](https://arxiv.org/abs/2510.12908)
*Rouzbeh Behnia,Jeremiah Birrell,Arman Riasi,Reza Ebrahimi,Kaushik Dutta,Thang Hoang*

Main category: cs.CR

TL;DR: 提出 L-RDP 的本地差分隐私方法，以在联邦学习中实现低内存开销和对异步参与的严格隐私保障。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端更新和全局模型容易泄露隐私；现有本地差分隐私在分布式场景下资源开销高、对异步参与的隐私保障不足，制约在医疗等敏感领域的应用。

Method: 设计一个面向 LDP 的本地差分隐私框架 L-RDP，确保恒定且低内存占用以降低掉线风险，并通过对间歇性参与进行隐私量化与记账，提供每个客户端的严格隐私保障。

Result: 提出的方法提供在资源受限的联邦学习环境中实现低内存成本与对异步参与的隐私保护的理论框架与实现要点；尚需通过实验评估其隐私强度与系统性能。

Conclusion: L-RDP 可提升本地差分隐私在联邦学习中的实用性，帮助合规场景（如 HIPAA、GDPR）下的模型训练，但需进一步的实证验证与实装细化。

Abstract: Federated learning (FL) enables organizations to collaboratively train models
without sharing their datasets. Despite this advantage, recent studies show
that both client updates and the global model can leak private information,
limiting adoption in sensitive domains such as healthcare. Local differential
privacy (LDP) offers strong protection by letting each participant privatize
updates before transmission. However, existing LDP methods were designed for
centralized training and introduce challenges in FL, including high resource
demands that can cause client dropouts and the lack of reliable privacy
guarantees under asynchronous participation. These issues undermine model
generalizability, fairness, and compliance with regulations such as HIPAA and
GDPR. To address them, we propose L-RDP, a DP method designed for LDP that
ensures constant, lower memory usage to reduce dropouts and provides rigorous
per-client privacy guarantees by accounting for intermittent participation.

</details>


### [15] [From base cases to backdoors: An Empirical Study of Unnatural Crypto-API Misuse](https://arxiv.org/abs/2510.13102)
*Victor Olaiya,Adwait Nadkarni*

Main category: cs.CR

TL;DR: 对大型实证的定性分析，揭示在 Android 应用中 crypto-API 使用的非自然变体，以及现有工具对这些变体的检测能力有限，提出两份详细的非自然使用分类法和 17 条关键发现，并给出四条未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有工具多仅检测最基础的 cryptographic API 滥用，难以覆盖非平凡变体；需要了解开发者在真实环境中的使用与滥用模式，以及非自然用法的样貌，从而指导工具设计。

Method: 从 20,508 个 Android 应用中提取 140,431 次 crypto-API 调用，提出一个直观的复杂度度量用于分层；从每一层抽样 5,704 次代表性调用；通过手工逆向、最小化示例和本地代码分析进行定性研究。

Result: 形成两份详细的非自然 crypto-API 滥用分类法；17 条关键发现揭示高异常的滥用、回避型代码，以及主流工具难以理解即使是略为非常规的用法；提出基于复杂度分层的采样和分析框架。

Conclusion: 提出四条关键Takeaways，指导未来在检测非自然 crypto-API 滥用方面的研究与工具开发。

Abstract: Tools focused on cryptographic API misuse often detect the most basic
expressions of the vulnerable use, and are unable to detect non-trivial
variants. The question of whether tools should be designed to detect such
variants can only be answered if we know how developers use and misuse
cryptographic APIs in the wild, and in particular, what the unnatural usage of
such APIs looks like. This paper presents the first large-scale study that
characterizes unnatural crypto-API usage through a qualitative analysis of
5,704 representative API invocations. We develop an intuitive complexity metric
to stratify 140,431 crypto-API invocations obtained from 20,508 Android
applications, allowing us to sample 5,704 invocations that are representative
of all strata, with each stratum consisting of invocations with similar
complexity/naturalness. We qualitatively analyze the 5,704 sampled invocations
using manual reverse engineering, through an in-depth investigation that
involves the development of minimal examples and exploration of native code.
Our study results in two detailed taxonomies of unnatural crypto-API misuse,
along with 17 key findings that show the presence of highly unusual misuse,
evasive code, and the inability of popular tools to reason about even mildly
unconventional usage. Our findings lead to four key takeaways that inform
future work focused on detecting unnatural crypto-API misuse.

</details>


### [16] [Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning](https://arxiv.org/abs/2510.13322)
*Baogang Song,Dongdong Zhao,Jianwen Xiang,Qiben Xu,Zizhuo Yu*

Main category: cs.CR

TL;DR: 提出可撤销的后门攻击框架，通过对注入与移除的双层优化实现高ASR且可在后续移除后门。


<details>
  <summary>Details</summary>
Motivation: 当前后门攻击往往易被静态分析检测，存在安全隐患与持续性风险。本文提出的可撤销后门为攻击提供更灵活性，同时也挑战系统的防护能力。

Method: 将触发器优化问题建模为双层优化，模拟注入与unlearning过程，采用确定性分区降低采样方差，并通过PCGrad缓解梯度冲突。

Result: 在 CIFAR-10 与 ImageNet 上，所提方法保持与最先进后门攻击相当的攻击成功率，同时能够在 unlearning 之后有效移除后门行为。

Conclusion: 为后门攻击研究开启新方向，并对机器学习系统安全提出新的挑战与考验。

Abstract: Backdoor attacks pose a persistent security risk to deep neural networks
(DNNs) due to their stealth and durability. While recent research has explored
leveraging model unlearning mechanisms to enhance backdoor concealment,
existing attack strategies still leave persistent traces that may be detected
through static analysis. In this work, we introduce the first paradigm of
revocable backdoor attacks, where the backdoor can be proactively and
thoroughly removed after the attack objective is achieved. We formulate the
trigger optimization in revocable backdoor attacks as a bilevel optimization
problem: by simulating both backdoor injection and unlearning processes, the
trigger generator is optimized to achieve a high attack success rate (ASR)
while ensuring that the backdoor can be easily erased through unlearning. To
mitigate the optimization conflict between injection and removal objectives, we
employ a deterministic partition of poisoning and unlearning samples to reduce
sampling-induced variance, and further apply the Projected Conflicting Gradient
(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on
CIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to
state-of-the-art backdoor attacks, while enabling effective removal of backdoor
behavior after unlearning. This work opens a new direction for backdoor attack
research and presents new challenges for the security of machine learning
systems.

</details>


### [17] [Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers](https://arxiv.org/abs/2510.13462)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Haoyu Gao,Zhendong Zhao,Yilong Chen*

Main category: cs.CR

TL;DR: 提出 BadSwitch 后门框架，利用 MoE 路由中的专家偏好嵌入触发，实现对 MoE LLM 的高效、隐蔽操控，跨多种 MoE 架构，表现出高成功率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: MoE 架构通过稀疏路由实现高效计算，但专家分工导致任务偏好，可能开启安全隐患。现有后门攻击多依赖数据污染或模型编辑，缺乏对 MoE 路由层面的系统性后门研究。

Method: 提出任务耦合的动态触发优化结合敏感性 Top-S 专家跟踪，预训练阶段同时优化触发嵌入，并识别 S 个最敏感的专家，将 Top-K 门控限制在这些专家上，以实现对路由的精确控制。

Result: 在 Switch Transformer、QwenMoE、DeepSeekMoE 三种架构上，BadSwitch 能以高效方式劫持预训练模型，ASR 达到接近或达到 100%，且清洁准确率在基线中最高。在 AGNews 数据集上，对文本级和模型级防御具备较强鲁棒性，达到 94.07% ASR、87.18% ACC。

Conclusion: 揭示 MoE 系统的安全风险，推动对 MoE 安全性研究与防护的重视，并提供对 MoE 专家激活模式的洞察，供后续研究改进与防护设计使用。

Abstract: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures
achieve impressive performance and efficiency by dynamically routing inputs to
specialized subnetworks, known as experts. However, this sparse routing
mechanism inherently exhibits task preferences due to expert specialization,
introducing a new and underexplored vulnerability to backdoor attacks. In this
work, we investigate the feasibility and effectiveness of injecting backdoors
into MoE-based LLMs by exploiting their inherent expert routing preferences. We
thus propose BadSwitch, a novel backdoor framework that integrates task-coupled
dynamic trigger optimization with a sensitivity-guided Top-S expert tracing
mechanism. Our approach jointly optimizes trigger embeddings during pretraining
while identifying S most sensitive experts, subsequently constraining the Top-K
gating mechanism to these targeted experts. Unlike traditional backdoor attacks
that rely on superficial data poisoning or model editing, BadSwitch primarily
embeds malicious triggers into expert routing paths with strong task affinity,
enabling precise and stealthy model manipulation. Through comprehensive
evaluations across three prominent MoE architectures (Switch Transformer,
QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack
pre-trained models with up to 100% success rate (ASR) while maintaining the
highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch
exhibits strong resilience against both text-level and model-level defense
mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our
analysis of expert activation patterns reveals fundamental insights into MoE
vulnerabilities. We anticipate this work will expose security risks in MoE
systems and contribute to advancing AI safety.

</details>


### [18] [How Blind and Low-Vision Users Manage Their Passwords](https://arxiv.org/abs/2510.13538)
*Alexander Ponticello,Filipo Sharevski,Simon Anell,Katharina Krombholz*

Main category: cs.CR

TL;DR: BLV 用户普遍使用密码管理器，但主要受益于便捷性，安全性提升（如强密码生成）因可及性不足而难以实现；现有工具未能满足BLV用户的自主性和可访问性需求，导致对密码管理器的低利用甚至不安全的替代做法。需要在可访问性和可用性上改进以建立信任并维持用户自主性。


<details>
  <summary>Details</summary>
Motivation: 研究目标在于理解盲/低视力（BLV）用户在密码管理中的挑战，以及密码管理工具如何更好地服务这一群体，确保安全性与可用性并重。

Method: 通过对33名BLV参与者的定性访谈进行分析，探究他们的密码管理行为、使用密码管理器的经历、以及在可访问性方面遇到的障碍。

Result: 所有参与者在一定程度上使用密码管理器，且普遍认为其可访问性较好，推动采用的主要是存储和检索密码的便利性。相反，利用密码管理器来生成强随机密码的安全优势未被广泛采用，原因是缺乏实际的可访问性。密码管理器未能满足BLV用户对自主性的基本需求，因为软件的不可访问性和厂商对可访问性问题的忽视。密码管理器的低使用率导致BLV用户采取不安全的做法，如重复使用易预测的密码或通过在盲文中记录重要凭据等“以安全隐蔽性保障安全”的做法。

Conclusion: 需要在密码管理器中实现实际可访问性和可用性改进，以建立BLV用户的信任并推动安全实践，同时保持用户的自主性。

Abstract: Managing passwords securely and conveniently is still an open problem for
many users. Existing research has examined users' password management
strategies and identified pain points, such as security concerns, leading to
insecure practices. We investigate how Blind and Low-Vision (BLV) users tackle
this problem and how password managers can assist them. This paper presents the
results of a qualitative interview study with N = 33 BLV participants. We found
that all participants utilize password managers to some extent, which they
perceive as fairly accessible. However, the adoption is mainly driven by the
convenience of storing and retrieving passwords. The security advantages -
generating strong, random passwords - were avoided mainly due to the absence of
practical accessibility. Password managers do not adhere to BLV users'
underlying needs for agency, which stem from experiences with inaccessible
software and vendors who deprioritize accessibility issues. Underutilization of
password managers leads BLV users to adopt insecure practices, such as reusing
predictable passwords or resorting to 'security through obscurity' by writing
important credentials in braille. We conclude our analysis by discussing the
need to implement practical accessibility and usability improvements for
password managers as a way of establishing trust and secure practices while
maintaining BLV users' agency.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [19] [Coherent Load Profile Synthesis with Conditional Diffusion for LV Distribution Network Scenario Generation](https://arxiv.org/abs/2510.12832)
*Alistair Brash,Junyi Lu,Bruce Stephen,Blair Brown,Robert Atkinson,Craig Michie,Fraser MacIntyre,Christos Tachtatzis*

Main category: eess.SY

TL;DR: 提出条件扩散模型，用于在低压配电变电站层面合成日尺度的有功和无功功率曲线，解决真实且一致的负荷数据缺乏问题，并提升子区域规划与运行情景分析的可信度。


<details>
  <summary>Details</summary>
Motivation: 低压层次的功率可视性不足，给规划和拥堵管理带来挑战；传统负荷分析方法（典型曲线、采样、生成模型）要么简化负荷行为，要么忽略变电站间的耦合关系；随着低碳技术接入，基础负荷的多样性日益增加，需更真实的联合分布数据。

Method: 提出条件扩散模型，用于在低压配电变电站层面合成日尺度的有功与无功负荷曲线；以历史典型样本/实例作为条件，评估在时序和统计现实性方面的保真度，并结合潮流计算进行评估；与朴素基线和先进模型进行对比，验证其在子区域电网规划和运行中的可用性。

Result: 合成的负荷曲线在单独与作为簇群的一部分时均具备可行性，与更广域电力系统耦合结果保持一致，展示了在规划和运行情景中生成现实场景的有效性；在时间序列、统计现实性以及潮流建模方面表现出色。

Conclusion: 条件扩散模型在生成低压配电变电站层面的日负荷曲线方面有效，能够更好地反映变电站之间的耦合关系，提升子区域规划与运行的可信度，相较于朴素和其他先进模型具有显著优势。

Abstract: Limited visibility of power distribution network power flows at the low
voltage level presents challenges to both distribution network operators from a
planning perspective and distribution system operators from a congestion
management perspective. Forestalling these challenges through scenario analysis
is confounded by the lack of realistic and coherent load data across
representative distribution feeders. Load profiling approaches often rely on
summarising demand through typical profiles, which oversimplifies the
complexity of substation-level operations and limits their applicability in
specific power system studies. Sampling methods, and more recently generative
models, have attempted to address this through synthesising representative
loads from historical exemplars; however, while these approaches can
approximate load shapes to a convincing degree of fidelity, the co-behaviour
between substations, which ultimately impacts higher voltage level network
operation, is often overlooked. This limitation will become even more
pronounced with the increasing integration of low-carbon technologies, as
estimates of base loads fail to capture load diversity. To address this gap, a
Conditional Diffusion model for synthesising daily active and reactive power
profiles at the low voltage distribution substation level is proposed. The
evaluation of fidelity is demonstrated through conventional metrics capturing
temporal and statistical realism, as well as power flow modelling. The results
show synthesised load profiles are plausible both independently and as a cohort
in a wider power systems context. The Conditional Diffusion model is
benchmarked against both naive and state-of-the-art models to demonstrate its
effectiveness in producing realistic scenarios on which to base sub-regional
power distribution network planning and operations.

</details>


### [20] [Non-Gaussian Distribution Steering in Nonlinear Dynamics with Conjugate Unscented Transformation](https://arxiv.org/abs/2510.12946)
*Daniel C. Qi,Kenshiro Oguri,Puneet Singla,Maruthi R. Akella*

Main category: eess.SY

TL;DR: 提出一种基于优化线性反馈的非线性系统非高斯分布控制方法，利用共轭无偏变换（Conjugate Unscented Transformation）量化非高斯分布的高阶矩，并通过序列凸规划求解，以直接控制并约束与不确定性相关的 sigma 点，从而实现对整个分布的控制。


<details>
  <summary>Details</summary>
Motivation: 在天体力学等高度非线性系统中，高斯分布易演化为非高斯分布，传统控制往往仅关注均值和方差，无法直接控制分布的形状与高阶矩。需要一种能在非高斯背景下对不确定性分布进行显式控制的方法。

Method: 采用共轭无偏变换（Conjugate Unscented Transformation）来量化非高斯分布的高阶统计矩；将控制问题表述为对 sigma 点的优化约束，以反映对整个分布及其矩的控制；设计一个基于序列凸规划的求解算法；并给出线性反馈控制形式的实现。

Result: 在两体和三体问题的数值示例中，能够直接控制单个矩，并且在非高斯分布下对矩的近似保持较高准确度，控制器时间轴内对不确定性分布的形状有效塑造。

Conclusion: 该方法为非线性系统中不确定性分布的显式形状控制提供了可行途径，通过 sigma 点约束的优化实现对高阶矩的直接操控，适用于强非线性动力学场景如天体力学等。

Abstract: In highly nonlinear systems such as the ones commonly found in astrodynamics,
Gaussian distributions generally evolve into non-Gaussian distributions. This
paper introduces a method for effectively controlling non-Gaussian
distributions in nonlinear environments using optimized linear feedback
control. This paper utilizes Conjugate Unscented Transformation to quantify the
higher-order statistical moments of non-Gaussian distributions. The formulation
focuses on controlling and constraining the sigma points associated with the
uncertainty quantification, which would thereby reflect the control of the
entire distribution and constraints on the moments themselves. This paper
develops an algorithm to solve this problem with sequential convex programming,
and it is demonstrated through a two-body and three-body example. The examples
show that individual moments can be directly controlled, and the moments are
accurately approximated for non-Gaussian distributions throughout the
controller's time horizon in nonlinear dynamics.

</details>


### [21] [Enhancing Profit and CO2 Mitigation: Commercial Direct Air Capture Design and Operation with Power Market Volatility](https://arxiv.org/abs/2510.12949)
*Zhiyuan Fan,Elizabeth Dentzer,James Glynn,David S. Goldberg,Julio Friedmann,Bolun Xu*

Main category: eess.SY

TL;DR: 商业DAC可以通过利用电力市场的价格波动在低价时段运作，从而实现成本较低的脱碳，但利润驱动可能降低总去除量；结果强调环境条件、地点、技术周期和激励设计对DAC经济性与政策的深远影响。


<details>
  <summary>Details</summary>
Motivation: 新冠碳中和目标下，DAC被视为弥补剩余排放的关键手段之一，但因能耗巨大而面临成本挑战。本文在以现货电力市场为基础的商业运营框架下，评估多种DAC技术在不同地区的盈利与去碳潜力，以及对电力系统与政策的影响。

Method: 建立四种DAC技术的商业运营模型，结合批发电力市场价格、碳激励、地点气象条件（温度、相对湿度）以及不同生命周期参数，在加州、德州与纽约三个地点进行仿真，分析在利润驱动下的运行策略、容量因子、总CO2去除量的变化，并比较周期短、灵活性高的技术对价格波动的利用程度，探讨激励设计与电力定价中的碳税对DAC的影响。

Result: 研究发现：DAC可利用电价波动在低价窗口盈利，尤其在周期短、灵活性高的技术中效果更明显。环境因素（温度、湿度）对去除能力有显著影响，利润驱动决策可能降低容量因子与总去除量。电力市场中的持续低价窗口往往与低排放时段（如加州的太阳能鸭曲线）协同。对激励设计存在最优解，然而对电力定价征收碳税在电力市场中对DAC不利。

Conclusion: 若要实现大规模DAC部署，需将收益设计与气候目标对齐，优化能够与低价格和低排放时段协同的激励措施，优先考虑周期短、灵活性高的DAC技术，并综合考虑环境条件对去除容量的影响，以及政策工具（非碳税或组合激励）在确保净零目标中的作用。

Abstract: Current decarbonization efforts are falling short of meeting the net-zero
greenhouse gas (GHG) emission target, highlighting the need for substantial
carbon dioxide removal methods such as direct air capture (DAC). However,
integrating DACs poses challenges due to their enormous power consumption. This
study assesses the commercial operation of various DAC technologies that earn
revenue using monetized carbon incentives while purchasing electricity from
wholesale power markets. We model four commercial DAC technologies and examine
their operation in three representative locations including California, Texas,
and New York. Our findings reveal that commercial DAC operations can take
financial advantage of the volatile power market to operate only during
low-price periods strategically, offering a pathway to facilitate a
cost-efficient decarbonization transition. The ambient operational environment
such as temperature and relative humidity has non-trivial impact on abatement
capacity. Profit-driven decisions introduce climate-economic trade-offs that
might decrease the capacity factor of DAC and reduce total CO2 removal. These
implications extend throughout the entire lifecycle of DAC developments and
influence power systems and policies related to full-scale DAC implementation.
Our study shows that DAC technologies with shorter cycle spans and higher
flexibility can better exploit the electricity price volatility, while power
markets demonstrate persistent low-price windows that often synergize with low
grid emission periods, like during the solar "duck curve" in California. An
optimal incentive design exists for profit-driven operations while carbon-tax
policy in electricity pricing is counterproductive for DAC systems.

</details>


### [22] [Model predictive control lowers barriers to adoption of heat-pump water heaters: A field study](https://arxiv.org/abs/2510.12955)
*Levi D. Reyes Premer,Elias N. Pergantis,Leo Semmelmann,Davide Ziviani,Kevin J. Kircher*

Main category: eess.SY

TL;DR: 以MPC控制的120 V热泵热水器实现高效预热与避免热阻加热，显著降低能耗与电价成本。


<details>
  <summary>Details</summary>
Motivation: 降低初始布线成本（不必安装240 V回路）同时减少热水需求对电网的影响；提升120 V HPWH的舒适性与经济性。

Method: 开发并在实际住宅中现场测试一个模型预测控制（MPC）系统，该系统通过一个预测大型用水冲程的预测器集合来实现预热，并通过时间序列出错的负载切换来降低高峰对电网的影响，与传统的240 V HPWH及其标准控制进行对比。

Result: 相较于带有标准控制的240 V HPWH，在时段定价（TOU）下节省约23%，在逐时定价下节省约28%；相较于120 V HPWH普遍采用的恒定高温贮存方式，MPC节能约37%；在真实住户中实现了验证，文中也分析了实施挑战、成本与简单回本期。

Conclusion: 120 V HPWH若配合MPC系统，可在不使用阻尼加热元件的前提下实现显著的能源与成本节约，具有良好的经济潜力，需在不同安装情景下评估实施成本与可行性。

Abstract: Electric heat-pump water heaters (HPWHs) could reduce the energy costs,
emissions, and power grid impacts associated with water heating, the
second-largest energy use in United States housing. However, most HPWHs today
require 240 V circuits to power the backup resistance heating elements they use
to maintain comfort during large water draws. Installing a 240 V circuit can
increase the up-front cost of a HPWH by half or more. This paper develops and
field-tests the first control system that enables a 120 V HPWH to efficiently
maintain comfort without resistance heating elements. The novel model
predictive control (MPC) system enables pre-heating in anticipation of large
water draws, which it forecasts using an ensemble of machine learning
predictors. By shifting electrical load over time, MPC also reduces energy
costs on average by 23% and 28% under time-of-use pricing and hourly pricing,
respectively, relative to a 240 V HPWH with standard controls. Compared to the
increasingly common practice in 120 V HPWHs of storing water at a constant,
high temperature (60 {\deg}C) to ensure comfort, MPC saves 37% energy on
average. In addition to demonstrating MPC's benefits in a real, occupied house,
this paper discusses implementation challenges and costs. A simple payback
analysis suggests that a 120 V HPWH, operated by the MPC system developed here,
would be economically attractive in most installation scenarios.

</details>


### [23] [Identifying Best Candidates for Busbar Splitting](https://arxiv.org/abs/2510.13000)
*Giacomo Bastianel,Dirk Van Hertem,Hakan Ergun,Line Roald*

Main category: eess.SY

TL;DR: 提出一组指标来高效识别与排序可用于母线分割BuS的有前景的候选母线，并通过混合整数凸二次BuS模型与AC OPF验证其可行性，从而在不同规模的案例中证明 BuS 能降低总发电成本。


<details>
  <summary>Details</summary>
Motivation: 随着用电需求上升和可再生能源接入，输电网拥塞加剧；BuS与最优传输开关可以缓解拥塞、降低发电成本，但BuS需要大量二元变量，对大型电网逐一分析不可行，因此需要快速识别潜在候选母线。

Method: 提出一组指标用于识别并对潜在BuS候选母线进行排序；对识别出的候选母线，使用混合整数凸二次BuS模型计算最优拓扑，并以非线性非凸的AC OPF验证其可行性。

Result: 在不同规模的测试案例中，指标可识别出在拓扑优化后能降低总发电成本的母线，显示无需逐个测试每个母线即可有效筛选BuS候选。

Conclusion: 该指标集提高了BuS候选筛选的效率，降低需要测试的母线数量，提升大规模电网场景下的可操作性。

Abstract: Rising electricity demand and the growing integration of renewables are
intensifying congestion in transmission grids. Grid topology optimization
through busbar splitting (BuS) and optimal transmission switching can alleviate
grid congestion and reduce the generation costs in a power system. However, BuS
optimization requires a large number of binary variables, and analyzing all the
substations for potential new topological actions is computationally
intractable, particularly in large grids. To tackle this issue, we propose a
set of metrics to identify and rank promising candidates for BuS, focusing on
finding buses where topology optimization can reduce generation costs. To
assess the effect of BuS on the identified buses, we use a combined
mixed-integer convex-quadratic BuS model to compute the optimal topology and
test it with the non-linear non-convex AC optimal power flow (OPF) simulation
to show its AC feasibility. By testing and validating the proposed metrics on
test cases of different sizes, we show that they are able to identify busbars
that reduce the total generation costs when their topology is optimized. Thus,
the metrics enable effective selection of busbars for BuS, with no need to test
every busbar in the grid, one at a time.

</details>


### [24] [Comparison of Forced and Unforced Rendezvous, Proximity Operations, and Docking Under Model Mismatch](https://arxiv.org/abs/2510.13004)
*Robert Muldrow,Channing Ludden,Christopher Petersen*

Main category: eess.SY

TL;DR: CW方程的简化RPOD模型与高保真RPOD模型的比较表明，单纯的无驱动轨道不必然更省燃料；模型不匹配会影响GNC脉冲需求，需在设计中权衡有驱动与无驱动的燃料效率以延长任务寿命。


<details>
  <summary>Details</summary>
Motivation: 随着航天行业扩张，对更高燃料效率、成本效益和更长任务寿命的RPOD模型需求增加，需评估CW简化模型与高保真RPOD模型之间的差异。

Method: 将CW方程预测轨迹与更高保真RPOD模型的轨迹进行对比；在若干相似任务参数的测试案例中，比较自然运动圆周（NMC）与对应的受迫运动；分析为维持CW零燃料轨迹所需的GNC冲量，评估模型不匹配的影响。

Result: 研究表明，单纯的无驱动运动并不天然比受驱动运动更节省燃料；CW模型的错配会影响需要的脉冲数量和规模，进而影响燃料利用率；因此在设计中需考虑有驱动与无驱动的权衡，以及对CW简化模型的局限性进行修正，以实现潜在的更长轨道操作。

Conclusion: CW简化模型在RPOD燃料预算中的适用性依赖于对模型不匹配的补偿；要实现更长任务寿命，需结合高保真RPOD模型开展GNC规划，并明确无驱动与有驱动路径的燃料效率权衡。

Abstract: This paper compares the required fuel usage for forced and unforced motion of
a chaser satellite engaged in Rendezvous, Proximity Operations, and Docking
(RPOD) maneuvers. Improved RPOD models are vital, particularly as the space
industry expands and demands for improved fuel efficiency, cost effectiveness,
and mission life span increase. This paper specifically examines the Clohessy-
Wiltshire (CW) Equations and the extent of model mismatch by comparing pre-
dicted trajectories from this model with a more computationally complex, higher
fidelity RPOD model. This paper assesses several test cases of similar mission
parameters, in each case comparing natural motion circumnavigation (NMC) with
comparable forced motion circumnavigation. The Guidance, Navigation, and Con-
trol (GNC) impulse maneuvers required to maintain the supposedly zero fuel CW
trajectories is representative of the extent of CW model mismatch. This paper
demonstrates that unforced motions are not inherently more fuel efficient than
forced motions, thus permitting extended orbital operations given the higher
fuel efficiency.

</details>


### [25] [Data to Certificate: Guaranteed Cost Control with Quantization-Aware System Identification](https://arxiv.org/abs/2510.13024)
*Shahab Ataei,Dipankar Maity,Debdipta Goswami*

Main category: eess.SY

TL;DR: 对云辅助系统识别和控制中量化数据的影响进行理论分析，给出基于量化分辨率的模型误差界与一个基于LMI的保证代价鲁棒控制器。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的控制场景（如微型无人机）中，将状态与输入数据上传云端会产生量化误差，需评估其对LTI识别和控制的影响，并提供具备性能保障的控制策略。

Method: 推导基于量化数据的最坏情况识别误差界，证明该界仅依赖量化数据及分辨率；在此误差下，提出基于LMI的保证代价鲁棒控制器。

Result: 给出一个依赖于量化数据及分辨率的模型误差的基本界限，并给出一个通过LMIs实现的保证成本鲁棒控制器。

Conclusion: 在量化数据条件下，仍可通过鲁棒控制设计实现性能保障，本研究建立了量化云端数据下的系统辨识与控制的理论框架。

Abstract: Cloud-assisted system identification and control have emerged as practical
solutions for low-power, resource-constrained control systems such as
micro-UAVs. In a typical cloud-assisted setting, state and input data are
transmitted from local agents to a central computer over low-bandwidth wireless
links, leading to quantization. This paper investigates the impact of state and
input data quantization on a linear time invariant (LTI) system identification,
derives a worst-case bound on the identification error, and develops a robust
controller for guaranteed cost control. We establish a fundamental bound on the
model error that depends only on the quantized data and quantization
resolution, and develop a linear matrix inequality (LMI) based guaranteed cost
robust controller under this error bound.

</details>


### [26] [Decision-dependent Robust Charging Infrastructure Planning for Light-duty Truck Electrification at Industrial Sites: Scheduling and Abandonment](https://arxiv.org/abs/2510.13100)
*Yifu Ding,Ruicheng Ao,Pablo Duenas-Martinez,Thomas Magnanti*

Main category: eess.SY

TL;DR: Two-stage robust MILP for industrial-site EV charging: selecting charger types/locations and scheduling, with abandonment and uncertainty in parking times; achieves near-optimal results across scenarios.


<details>
  <summary>Details</summary>
Motivation: Industrial sites rely on diesel-powered light-duty trucks, causing substantial GHG emissions. Electrification with robust planning is needed to handle uncertain parking durations, waiting times, and behavioral responses (range anxiety, overnight charging) to reduce emissions.

Method: Develop a two-stage robust charging infrastructure planning model (MILP) that selects charger types/locations and schedules opportunity charging. Includes an abandonment constraint when waiting exceeds a threshold, models overnight charging and range anxiety, and uses a decision-dependent robust uncertainty set for parking durations. Decomposes the year into monthly subproblems and solves with heuristic approaches.

Result: Applied to an open-pit mining site with eight zones and ~200 trucks. The yearly dataset yields an optimality gap <0.1% within reasonable computation time under diverse uncertainty scenarios.

Conclusion: The model provides a scalable framework for planning and operating electric charging infrastructure at industrial sites, accounting for behavioral responses and uncertain parking durations, and enabling effective GHG reductions through robust, decomposed scheduling.

Abstract: Many industrial sites rely on diesel-powered light-duty trucks to transport
workers and small-scale facilities, which has resulted in a significant amount
of greenhouse emissions (GHGs). To address this, we developed a two-stage
robust charging infrastructure planning model for electrifying light-duty
trucks at industrial sites. The model is formulated as a mixed-integer linear
programming (MILP) that optimizes the charging infrastructure, selected from
multiple charger types and potential locations, and determines opportunity
charging schedules for each truck based on the chosen infrastructure. Given the
strict stopping points and schedules at industrial sites, we introduced a
scheduling problem with abandonment, where trucks forgo charging if their
waiting times exceed a maximum threshold. We also further incorporated the
impacts of overnight charging and range anxiety on waiting and abandonment
behaviors. To represent the stochastic and heterogeneous parking durations of
trucks, we constructed a decision-dependent robust uncertainty set in which
parking time variability flexibly depends on charging choices. We applied the
model in a case study of an open-pit mining site, which plans charger
installations in eight zones and schedules a fleet of around 200 trucks. By
decomposing the problem into monthly subproblems and using heuristic
approaches, for the whole-year dataset, the model achieves an optimality gap of
less than 0.1 % within a reasonable computation time under diverse uncertainty
scenarios.

</details>


### [27] [Safe Driving in Occluded Environments](https://arxiv.org/abs/2510.13114)
*Zhuoyuan Wang,Tongyao Jia,Pharuj Rajborirug,Neeraj Ramesh,Hiroyuki Okuda,Tatsuya Suzuki,Soummya Kar,Yorie Nakahira*

Main category: eess.SY

TL;DR: 提出基于概率不变性的潜在风险安全证书框架，能在遮挡环境中限制潜在风险概率，结合模型预测控制或数据驱动策略以实现长期安全和透明性。


<details>
  <summary>Details</summary>
Motivation: 遮挡导致隐性风险，传统模型驱动的集合不变方法需要对风险状态全观测，数据驱动难以学习直接从传感输入到安全决策的映射。引入概率不变性以放宽观测要求并提供安全保障。

Method: 提出线性动作约束来自概率不变性，约束潜在风险概率在容忍界内；可将这些约束集成到MPC或数据驱动策略中；在CARLA仿真中与现有技术对比，理论和实验分析。

Result: 理论和实验分析表明可在遮挡环境中实现长期安全且不过于保守，并对暴露风险保持透明性。

Conclusion: 此方法提供一个可解释且可嵌入的概率不变性框架，提升在遮挡场景下的实时安全性，同时缓解潜在风险并保持对暴露风险的透明性。

Abstract: Ensuring safe autonomous driving in the presence of occlusions poses a
significant challenge in its policy design. While existing model-driven control
techniques based on set invariance can handle visible risks, occlusions create
latent risks in which safety-critical states are not observable. Data-driven
techniques also struggle to handle latent risks because direct mappings from
risk-critical objects in sensor inputs to safe actions cannot be learned
without visible risk-critical objects. Motivated by these challenges, in this
paper, we propose a probabilistic safety certificate for latent risk. Our key
technical enabler is the application of probabilistic invariance: It relaxes
the strict observability requirements imposed by set-invariance methods that
demand the knowledge of risk-critical states. The proposed techniques provide
linear action constraints that confine the latent risk probability within
tolerance. Such constraints can be integrated into model predictive controllers
or embedded in data-driven policies to mitigate latent risks. The proposed
method is tested using the CARLA simulator and compared with a few existing
techniques. The theoretical and empirical analysis jointly demonstrate that the
proposed methods assure long-term safety in real-time control in occluded
environments without being overly conservative and with transparency to exposed
risks.

</details>


### [28] [Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution Time](https://arxiv.org/abs/2510.13279)
*Fuma Omori,Atsushi Yano,Takuya Azumi*

Main category: eess.SY

TL;DR: 提出基于分区调度的DAG任务概率可调度分析方法，并对四种装箱启发式进行了比较，结果显示在随机DAG集上新方法具有更高的可调度性和更短的分析时间，Item-Centric Worst-Fit-Decreasing表现最好。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等对安全性要求极高的场景中，基于最坏情形的保证往往过保守；DAG任务的实际执行通常低于Worst-Case，因而需要概率保证；将单处理器的概率分析成果扩展到多核/分区调度以提升可行性和分析效率。

Method: 提出一种任务集划分方法，在分区调度框架下对DAG任务集合进行可调度性保证分析；以随机生成的DAG任务集进行实验，对比四种装箱启发式算法，重点考察分析时间和可调度性。

Result: 实验显示，与现有针对DAG的概率分析相比，新方法可调度的任务集合数量增多，平均分析时间降低；四种装箱启发式中，Item-Centric Worst-Fit-Decreasing达到调度最多任务集的效果。

Conclusion: 基于分区调度的DAG概率可调度分析方法具有更高的可扩展性和分析效率，且启发式选择对可调度性有显著影响；为现实系统（如自动驾驶）提供更现实的实时性保证。

Abstract: Autonomous driving systems, critical for safety, require real-time guarantees
and can be modeled as DAGs. Their acceleration features, such as caches and
pipelining, often result in execution times below the worst-case. Thus, a
probabilistic approach ensuring constraint satisfaction within a probability
threshold is more suitable than worst-case guarantees for these systems. This
paper considers probabilistic guarantees for DAG tasks by utilizing the results
of probabilistic guarantees for single processors, which have been relatively
more advanced than those for multi-core processors. This paper proposes a task
set partitioning method that guarantees schedulability under the partitioned
scheduling. The evaluation on randomly generated DAG task sets demonstrates
that the proposed method schedules more task sets with a smaller mean analysis
time compared to existing probabilistic schedulability analysis for DAGs. The
evaluation also compares four bin-packing heuristics, revealing Item-Centric
Worst-Fit-Decreasing schedules the most task sets.

</details>


### [29] [Multipolar dynamics of social segregation: Data validation on Swedish vaccination statistics](https://arxiv.org/abs/2510.13396)
*Luka Baković,David Ohlin,Emma Tegling*

Main category: eess.SY

TL;DR: 对多极意见动力学模型在两个相关变量数据集上的有效性验证；以瑞典疫苗接种率与政治参与为例，模型能够捕捉意见隔离；偏置的空间相关性对结果必需；偏置混合导致意见分布更均匀，多数意见的渗透增加（投票或接种）。


<details>
  <summary>Details</summary>
Motivation: 验证并扩展多极模型，使其在带有两个相关变量的数据集上也能应用并再现现实中的意见结构和空间偏置模式。

Method: 提出并测试一种将该模型应用于两相关变量数据集的一般方法；以瑞典 COVID-19 疫苗接种率与政治参与的数据为案例，分析偏置的空间相关性及其对结果的影响。

Result: 模型成功捕捉数据所示的意见隔离；偏置的空间相关性对得到这一结果是必要的；偏置的混合会使意见分布更均匀，并提高多数意见（投票或接种）的渗透。

Conclusion: 在带两相关变量的数据集上， multipolar 模型能有效再现意见隔离现象；空间偏置相关性是关键因素，偏置混合改变了最终的意见分布，从而影响群体行为决策。

Abstract: We perform a validation analysis on the multipolar model of opinion dynamics.
A general methodology for using the model on datasets of two correlated
variables is proposed and tested using data on the relationship between
COVID-19 vaccination rates and political participation in Sweden. The model is
shown to successfully capture the opinion segregation demonstrated by the data
and spatial correlation of biases is demonstrated as necessary for the result.
A mixing of the biases on the other hand leads to a more homogeneous opinion
distribution, and greater penetration of the majority opinion, which here
corresponds to a decision to vote or vaccinate.

</details>


### [30] [On the Flexibility Potential of a Swiss Distribution Grid: Opportunities and Limitations](https://arxiv.org/abs/2510.13449)
*Jan Brändle,Julie Rousseau,Pulkit Nahata,Gabriela Hug*

Main category: eess.SY

TL;DR: Aggregated distribution-grid flexibility can be boosted by heat pumps and PV, but its growth is non-linear and seasonally varying due to feeder overloads and grid topology.


<details>
  <summary>Details</summary>
Motivation: Rapid electrification and distributed generation increase flexible loads; understanding their aggregated potential is crucial for maintaining grid stability, illustrated via the Walenstadt grid case study.

Method: Use the Swiss distribution grid of Walenstadt as a case study; quantify aggregated flexibility with heat pumps and PV; analyze time-varying (seasonal) flexibility; run future scenarios to study non-linear effects and topology constraints.

Result: Heat pumps and PV substantially enhance distribution-grid flexibility; aggregated flexibility varies over time and season; in simulations, flexibility does not increase linearly or monotonically with higher penetration, due to feeder overloads and network constraints.

Conclusion: Grid topology and constraints fundamentally limit aggregated flexibility; planning must consider non-linear, time-varying potential and seasonality when integrating distributed energy resources.

Abstract: The growing integration of distributed renewable generation and the
electrification of heating and transportation are rapidly increasing the number
of flexible devices within modern distribution grids. Leveraging the aggregated
flexibility of these small-scale distributed resources is essential to
maintaining future grid-wide stability. This work uses the Swiss distribution
grid of Walenstadt as a case study to provide insights into the aggregated
flexibility potential of distribution grids. It demonstrates that incorporating
devices such as heat pumps and photovoltaic systems significantly enhances
distribution grid flexibility. It investigates the time-varying nature of
aggregated flexibility and highlights how it can vary seasonally. Furthermore,
simulations of future scenarios reveal that aggregated flexibility does not
increase linearly or monotonically with higher levels of flexible device
penetration. This is primarily due to the overloading of individual feeders,
which underscores the impact of grid topology and network constraints on the
aggregated flexibility potential.

</details>


### [31] [Quantifying the Impact of Missing Risk Markets for Decarbonized Power Systems with Long Duration Energy Storage](https://arxiv.org/abs/2510.13514)
*Andreas C. Makrides,Adam Suski,Elina Spyrou*

Main category: eess.SY

TL;DR:  incomplete risk markets raise investment barriers for long-duration energy storage (LDES); using a two-stage stochastic equilibrium with risk-averse agents shows lower welfare, reduced reliability, and slower deployment; policy hedges on revenue risk can lower capital costs and accelerate zero-carbon reliability tech adoption. 


<details>
  <summary>Details</summary>
Motivation:  Decarbonising electricity requires reliable, sustainable supply. However, missing risk markets create revenue uncertainty that deters investment in reliability-enhancing technologies like LDES. This study quantifies these effects to inform policy design.

Method:  Develop a two-stage stochastic equilibrium model with risk-averse market participants that independently size power and energy capacity. Apply the model to a deeply decarbonised Great Britain power system case study to assess welfare, reliability, investment, and financing implications under incomplete risk markets.

Result:  Incomplete risk markets reduce social welfare and reliability, discourage investment in LDES and other technologies with volatile revenue streams. Revenue volatility induces sizable risk premiums and higher financing costs for LDES, hindering large-scale deployment.

Conclusion:  Policy mechanisms that hedge revenue risk can lower the cost of capital and accelerate investment in reliability-enhancing, zero-carbon technologies. 

Abstract: The transition to a fully decarbonised electricity system depends on
integrating new technologies that ensure reliability alongside sustainability.
However, missing risk markets hinder investment in reliability-enhancing
technologies by exposing investors to revenue uncertainty. This study provides
the first quantitative assessment of how missing risk markets affect investment
decisions in power systems that depend on long-duration energy storage (LDES)
for reliability. We develop a two-stage stochastic equilibrium model with
risk-averse market participants, which independently sizes power and energy
capacity. We apply the method to a case study of a deeply decarbonised power
system in Great Britain. The results show that incomplete risk markets reduce
social welfare, harm reliability, and discourage investment in LDES and other
technologies with volatile revenue streams. Revenue volatility leads to
substantial risk premiums and higher financing costs for LDES, creating a
barrier to its large-scale deployment. These findings demonstrate the
importance of policy mechanisms that hedge revenue risk to lower the cost of
capital and accelerate investment in reliability-enhancing, zero-carbon
technologies

</details>


### [32] [Channel Estimation under Large Doppler Shifts in NOMA-Based Air-Ground Communications](https://arxiv.org/abs/2510.13563)
*Ayten Gürbüz,Giuseppe Caire*

Main category: eess.SY

TL;DR: 简短摘要：在民航空管场景下，研究多天线NOMA系统的信道估计与检测问题，基于现实几何随机信道模型评估高多普勒和远距离带来的挑战，比较Zadoff-Chu序列与时分法的信道估计，以及ZF/MMSE+SIC在信道老化下的 outage 性能，发现最佳估计-检测组合随飞行阶段变化。


<details>
  <summary>Details</summary>
Motivation: 尽管NOMA能提升光谱效率，但航空场景存在高速移动引起的大多普勒效应与长距离传输导致的低信噪比，需用真实的空地信道模型来评估信道估计、老化及检测策略在多机场/多机同时传输中的可行性。

Method: 采用基于专门飞行测量的几何随机空地信道模型，多架飞机同时向地面站发送数据；在高载波频偏和信道随时间变化的情况下研究地面站的信道估计；在发射端不同前补偿精度下比较Zadoff-Chu序列与时分法的信道估计；在信道老化场景下计算零迫检测(ZF)与带成功干扰消除(SIC)的最小均方误差(MMSE)检测的 outage 概率，以及评估不同阶段的性能。

Result: 研究发现不同飞行阶段的理想信道估计-检测组合不同，原因在于起降阶段与巡航阶段的信道传播特性显著不同；Zadoff-Chu序列与时分法在前端补偿精度的影响下各有优劣，且ZF与MMSE-SIC的鲁棒性会随信道老化程度变化而改变。

Conclusion: 结论强调，在NOMA的空地数据链路中，应结合飞行阶段的信道特征选择最优的信号估计与检测策略，并采用现实的空地信道模型以准确评估系统性能。

Abstract: This paper investigates a multiple antenna system with non-orthogonal
multiple access (NOMA) for the exchange of air traffic management data between
commercial aircraft pilots and ground-based air traffic controllers. While NOMA
techniques enhance spectral efficiency, their application to aircraft
communications is challenged by the high speed of the aircraft (up to 214 m/s)
and the long communication ranges (up to 250 km), resulting in significant
Doppler shifts and low signal-to-noise ratios, respectively. To accurately
assess these challenges, we employ a realistic geometry-based stochastic
air-ground channel model, derived from dedicated flight measurement campaigns.
In this paper, multiple aircraft simultaneously transmit data to the ground
station. We focus on the channel estimation problem at the ground station under
high carrier frequency offsets and the effects of channel aging due to
channel's time-varying nature. For the channel estimation problem, we compare
the Zadoff-Chu sequences with time-division approach under varying carrier
frequency offset pre-compensation accuracies at the aircraft transmitter. For
the channel aging problem and performance evaluation of channel estimators, we
compute the outage probability for both the zero-forcing detector and the
minimum mean squared error detector with successive interference cancellation.
The results show that the favorable channel estimator-detector combinations
differ between the takeoff & landing phase and the enroute cruise phase of the
flight, due to the distinct channel propagation characteristics of each phase.

</details>


### [33] [A 0.62 μW/sensor 82 fps Time-to-Digital Impedance Measurement IC with Unified Excitation/Readout Front-end for Large-Scale Piezo-Resistive Sensor Array](https://arxiv.org/abs/2510.13682)
*Jiayang Li,Qingyu Zhang,Sohmyung Ha,Dai Jiang,Andreas Demosthenous,Yu Wu*

Main category: eess.SY

TL;DR: A fast impedance-measurement IC for large-scale piezoresistive sensor arrays using a unified differential time-to-digital demodulation architecture, with pre-saturation adaptive bias, achieving high throughput and low power.


<details>
  <summary>Details</summary>
Motivation: To enable high-speed, power-efficient impedance readout across large piezoresistive sensor arrays for real-time sensing applications.

Method: A unified differential time-to-digital demodulation readout that measures impedance directly through the excitation circuit, coupled with a pre-saturation adaptive bias technique to improve power efficiency.

Result: Scans 253 sensors in 12.2 ms (82 fps) at 125 kHz, consuming 158 μW (7.5 nJ per sensor). Measurement range 20 Ω–500 kΩ with 0.5% error and up to 71.1 dB SNR.

Conclusion: The proposed architecture delivers fast, power-efficient, and accurate impedance sensing for large sensor arrays, enabling real-time monitoring with high SNR across a wide impedance range.

Abstract: This paper presents a fast impedance measurement IC for large-scale
piezo-resistive sensor array. It features a unified differential
time-to-digital demodulation architecture that readout impedance directly
through the excitation circuit. The proposed pre-saturation adaptive bias
technique further improves power efficiency. The chip scans 253 sensors in 12.2
ms (82 fps) at 125 kHz, consuming 158 {\mu}W (7.5 nJ/sensor). With loads from
20 {\Omega} to 500 k{\Omega}, it achieves 0.5% error and up to 71.1 dB SNR.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [34] [Toward Hyper-Dimensional Connectivity in Beyond 6G: A Conceptual Framework](https://arxiv.org/abs/2510.12896)
*Ekram Hossain,Angelo Vera-Rivera*

Main category: cs.NI

TL;DR: 提出面向B6G的超维连接愿景框架，聚焦将通信、认知、计算和 cyber-physical 集成以支持超沉浸式互联网应用，并给出用例、需求、技术支撑要素映射及未来研究议程。


<details>
  <summary>Details</summary>
Motivation: 在5G已部署、6G研究起步的背景下，需明确面向未来的创新驱动和长期目标，为超沉浸式应用提供技术路线。

Method: 提出一个概念性框架，定义B6G的核心连接维度，给出潜在用例与系统级需求，映射可能的技术支撑要素，并提出未来研究议程。

Result: 给出B6G的概念框架、相关技术定义、潜在用例与需求、技术支撑要素的初步映射，以及一个面向未来的研究路线图。

Conclusion: 此类概念性讨论有助于识别创新驱动、塑造长期目标，并为未来移动宽带技术的研究方向提供指引。

Abstract: Cellular wireless networks enable mobile broadband connectivity for
Internet-based applications through their radio access and core network
infrastructure. While Fifth-Generation (5G) cellular systems are currently
being deployed, ongoing research on cellular technologies primarily focuses on
Sixth-Generation (6G) networks to set the stage for developing standards for
these systems. Therefore, the time has come to articulate the visions for
beyond 6G (B6G) systems. In this article, we present a visionary framework
toward hyper-dimensional connectivity in B6G that enables wireless access to
hyper-immersive Internet technologies. Our contributions include a conceptual
framework for B6G cellular systems with jointly integrated communication,
cognition, computing, and cyber-physical capabilities as core connectivity
dimensions, a set of technical definitions outlining potential use cases and
system-level requirements, a mapping of prospective technology enablers, and a
forward-looking research agenda for B6G systems. The conceptual discussions in
this article would be helpful for identifying innovation drivers, shaping
long-term technical goals, and defining research agendas for the future of
mobile broadband technologies.

</details>


### [35] [Fair Ordering](https://arxiv.org/abs/2510.13664)
*Muhammad Haseeb,Jinkun Geng,Radhika Mittal,Aurojit Panda,Srinivas Narayana,Anirudh Sivaraman*

Main category: cs.NI

TL;DR: 提出一种把时钟不确定性纳入考虑的公平排序模型，利用统计学习估计事件的 likely-happened-before 概率，形成 xrightarrow{p} 概念，用于在并发情境下实现更公平的事件排序。


<details>
  <summary>Details</summary>
Motivation: 解决时钟同步局限导致的公平排序难题，倡导接受时钟漂移与噪声，利用统计分布来判断事件先后，从而在无墙钟信息的情况下给出概率化的先后判定。

Method: 构建每个时钟的偏移分布，学习它们的统计特性；通过比较两个时间戳在墙钟下的先后概率来推导 likely-happened-before 关系 xrightarrow{p}，并讨论该关系的非传递性及在并发情形中的使用。

Result: 提出一个初步的概率化先后判定框架，为在线公正排序、随机性公平全序等方向奠定基础，指示在存在时钟噪声的系统中也能实现更接近墙钟的顺序性。

Conclusion: 通过接受而非消除时钟不确定性，提出了新的排序范式 xrightarrow{p}，并展望未来的研究方向，如在线公平排序、在主机层面的公平支持等。

Abstract: A growing class of applications demands \emph{fair ordering/sequencing} of
events which ensures that events generated earlier by one client are processed
before later events from other clients. However, achieving such sequencing is
fundamentally challenging due to the inherent limitations of clock
synchronization. We advocate for an approach that embraces, rather than
eliminates, clock variability. Instead of attempting to remove error from a
timestamp, Tommy, our proposed system, leverages a statistical model to compare
two noisy timestamps probabilistically by learning per-clock offset
distributions. Our preliminary statistical model computes the probability that
one event precedes another w.r.t. the wall-clock time without access to the
wall-clock. This serves as a foundation for a new relation:
\emph{likely-happened-before} denoted by $\xrightarrow{p}$ where $p$ represents
the probability of an event to have happened before another. The
$\xrightarrow{p}$ relation provides a basis for ordering multiple events which
are otherwise considered \emph{concurrent} by the typical
\emph{happened-before} ($\rightarrow$) relation. We highlight various related
challenges including intransitivity of $\xrightarrow{p}$ relation as opposed to
the transitive $\rightarrow$ relation. We also outline several research
directions: online fair sequencing, stochastically fair total ordering,
host-level support for fairness and more.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [36] [On the performance of Active STAR-RIS-Assisted Cell-Free Massive MIMO Systems with Phase Errors and Channel Aging](https://arxiv.org/abs/2510.13171)
*Jun Qian,Ross Murch,Khaled B. Letaief*

Main category: cs.IT

TL;DR: Active STAR-RIS aided cell-free massive MIMO with amplification; analyzes phase errors and channel aging; derives MMSE channel estimates and closed-form downlink SE; shows STAR-RIS mitigates impairments; provides design guidelines for resource block length; increasing APs/elements and amplification improves performance.


<details>
  <summary>Details</summary>
Motivation: To overcome attenuation in RIS cascaded links using active amplification, and to understand how phase errors and channel aging affect active STAR-RIS-assisted cell-free massive MIMO systems.

Method: Model system with spatially correlated Rayleigh fading. Derive minimum mean square error (MMSE) channel estimates. Obtain closed-form expressions for downlink spectral efficiency (SE). Analyze the impact of phase errors (uniformly distributed) and channel aging. Propose practical guidelines for resource-block-length design. Evaluate how increasing APs, STAR-RIS elements, and amplification factor affects performance.

Result: Analytical framework enables assessment of phase error and aging effects. Active STAR-RISs can compensate adverse effects. Guidelines for resource-block-length design proposed. Increasing APs/elements and amplification alleviates degradation.

Conclusion: Active STAR-RISs are effective in mitigating phase errors and channel aging in cell-free massive MIMO. System performance improves with more APs and STAR elements and higher amplification. The study provides actionable design guidelines for robust operation under impairments.

Abstract: Active reconfigurable intelligent surfaces (RISs) employ amplification to
overcome attenuation caused by the RIS cascaded link. In this paper, we analyze
the effects of phase errors and channel aging in active simultaneously
transmitting and reflecting (STAR) RIS-assisted cell-free massive
multiple-input multiple-output (MIMO) systems. By leveraging a spatially
correlated Rayleigh fading model, this paper derives minimum mean square error
estimate-based channel estimates and formulates closed-form expressions for
downlink spectral efficiency. This analytical framework enables a comprehensive
evaluation of the effects of channel aging and uniformly distributed phase
errors on system performance. The results demonstrate that active STAR-RISs can
effectively compensate for the adverse effects of phase errors and channel
aging. To counteract the impact of channel aging, we propose practical
guidelines for resource-block-length design. Also, an increase in APs and
STAR-RIS elements, along with a larger amplification factor, can alleviate
performance degradation.

</details>


### [37] [A Dimension-Keeping Semi-Tensor Product Framework for Compressed Sensing](https://arxiv.org/abs/2510.13180)
*Qi Qi,Abdelhamid Tayebi,Daizhan Cheng,Jun-e Feng*

Main category: cs.IT

TL;DR: 提出一种名为 DK-STP-CS 的压缩感知方法，通过维度保持半张量积在设计测量矩阵时利用组内相关性并保持组间无关性，从而实现降维同时保持信号重建能力；在图像压缩与重建中实现显著的降噪与更高的视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 解决传统 CS 对测量矩阵无关性和高维度限制的依赖，通过捕获组内相关性并保持组间无关性来提升测量矩阵设计的有效性，同时在资源受限环境中实现降维与稳健重建。

Method: 将 Dimension-Keeping Semi-Tensor Product (DK-STP) 算法融入测量矩阵设计，进行维度降维的同时保持信号可恢复性；在图像任务中通过对信号分组来利用 intra-group 相关性并维持 inter-group 的无关性。

Result: 实验结果显示 DK-STP-CS 相较传统 CS 与 STP-CS 在 PSNR 上显著提升，具有更好的降噪效果与视觉保真度；在噪声、采样率变化等条件下表现鲁棒，证实其在资源受限环境中的潜在应用。

Conclusion: DK-STP-CS 提供一种有效的降维同时保持重建能力的压缩感知设计路径，适用于图像压缩与重建及资源受限场景，普遍优于现有方法。

Abstract: In compressed sensing (CS), sparse signals can be reconstructed from
significantly fewer samples than required by the Nyquist-Shannon sampling
theorem. While non-sparse signals can be sparsely represented in appropriate
transformation domains, conventional CS frameworks rely on the incoherence of
the measurement matrix columns to guarantee reconstruction performance. This
paper proposes a novel method termed Dimension-Keeping Semi-Tensor Product
Compressed Sensing (DK-STP-CS), which leverages intra-group correlations while
maintaining inter-group incoherence to enhance the measurement matrix design.
Specifically, the DK-STP algorithm is integrated into the design of the sensing
matrix, enabling dimensionality reduction while preserving signal recovery
capability. For image compression and reconstruction tasks, the proposed method
achieves notable noise suppression and improves visual fidelity. Experimental
results demonstrate that DK-STP-CS significantly outperforms traditional CS and
STP-CS approaches, as evidenced by higher Peak Signal-to-Noise Ratio (PSNR)
values between the reconstructed and original images. The robustness of
DK-STP-CS is further validated under noisy conditions and varying sampling
rates, highlighting its potential for practical applications in
resource-constrained environments.

</details>


### [38] [Movable and Reconfigurable Antennas for 6G: Unlocking Electromagnetic-Domain Design and Optimization](https://arxiv.org/abs/2510.13209)
*Lipeng Zhu,Haobin Mao,Ge Yan,Wenyan Ma,Zhenyu Xiao,Rui Zhang*

Main category: cs.IT

TL;DR: 通过可移动天线（MAs）与可重构天线（RAs）实现对位置、朝向、辐射、极化和频响的动态控制，提供丰富的电磁域自由度，提升6G系统性能，综述应用场景、硬件架构、设计方法，并结合仿真与现场测试结果。


<details>
  <summary>Details</summary>
Motivation: 为应对6G对更高容量、覆盖和能效的需求，利用天线的动态可调性与重构性来拓展电磁域自由度，提升系统灵活性与性能潜力。

Method: 对已有研究进行综述，系统分析MAs与RAs的应用场景、硬件架构与设计方法，并结合场测试与仿真结果进行对比分析。

Result: 相较于传统固定或非重构天线，MAs/RAs在性能上展现出明显优势，证明其在未来6G系统中的潜力与可行性。

Conclusion: 将MAs与RAs视为实现6G中更灵活、可控天线的关键技术，未来需在实现性、成本、可靠性、集成度与标准化方面推进研究。

Abstract: The growing demands of 6G mobile communication networks necessitate advanced
antenna technologies. Movable antennas (MAs) and reconfigurable antennas (RAs)
enable dynamic control over antenna's position, orientation, radiation,
polarization, and frequency response, introducing rich electromagnetic-domain
degrees of freedom for the design and performance enhancement of wireless
systems. This article overviews their application scenarios, hardware
architectures, and design methods. Field test and simulation results highlight
their performance benefits over conventional fixed/non-reconfigurable antennas.

</details>


### [39] [Non-Linear Precoding via Dirty Paper Coding for Near-Field Downlink MISO Communications](https://arxiv.org/abs/2510.13485)
*Akash Kulkarni,Rajshekhar V Bhat*

Main category: cs.IT

TL;DR: 在极大阵列的近场6G NFC中，基于Dirty Paper Coding的非线性预编码显著提升系统和速率，相比ZF在多种近场配置下效果更好，尤以紧邻用户场景为甚。


<details>
  <summary>Details</summary>
Motivation: 在6G的极大阵列与太赫兾频段下，近场区域扩展至典型用户距离，需要通过高分辨率的波束聚焦实现多用户通信的干扰抑制；现有线性预编码（如ZF）在抑制干扰时需较大发射功率，性能受限。

Method: 提出基于DPC的非线性预编码框架，目标最大化和速率；建立DPC与ZF的和速率最大化优化问题，导出并分析两者的最优功率分配策略，给出DPC求解过程并与ZF对比。

Result: 仿真结果显示，在多种近场配置下，DPC相对于ZF可获得显著的和速率增益，且近场中距离较近的用户场景增益最为明显。

Conclusion: 在6G极大阵列近场通信场景中，DPC预编码提供显著的性能提升，适用于高分辨率波束聚焦的NFC；未来工作可进一步研究实现复杂度、鲁棒性等方面。

Abstract: In 6G systems, extremely large-scale antenna arrays operating at terahertz
frequencies extend the near-field region to typical user distances from the
base station, enabling near-field communication (NFC) with fine spatial
resolution through beamfocusing. Existing multiuser NFC systems predominantly
employ linear precoding techniques such as zero-forcing (ZF), which suffer from
performance degradation due to the high transmit power required to suppress
interference. This paper proposes a nonlinear precoding framework based on
Dirty Paper Coding (DPC), which pre-cancels known interference to maximize the
sum-rate performance. We formulate and solve the corresponding sum-rate
maximization problems, deriving optimal power allocation strategies for both
DPC and ZF schemes. Extensive simulations demonstrate that DPC achieves
substantial sum-rate gains over ZF across various near-field configurations,
with the most pronounced improvements observed for closely spaced users.

</details>


### [40] [Simulating Mediumband Wireless Communication Systems: A Concise Description](https://arxiv.org/abs/2510.13532)
*Dushyantha A Basnayaka*

Main category: cs.IT

TL;DR: 提出针对中等带宽场景的端到端无线仿真方法，在MATLAB中对PHY子系统进行详细建模，以超越常见的基带离散时域简化，关注深衰落避免等现象。


<details>
  <summary>Details</summary>
Motivation: 当前文献多在离散时域基带模型下对信号处理流程进行简化，忽略脉冲整形、上/下变频、载波和符号定时同步等关键PHY操作，难以真实再现中等带宽通信的物理层特性。需要一个系统化、便于实现的仿真框架来捕捉中等带宽下的动态特性。

Method: 在MATLAB中给出从单发TX到单接RX的端到端仿真步骤，逐步描述并实现脉冲整形、上变频、混频、载波同步、符号定时同步等PHY子系统的操作与参数设置，并讨论深衰落回避的仿真处理。

Result: 提供可操作的仿真框架与实施要点，帮助初学者和专家在MATLAB中实现中等带宽场景的尽可能真实的物理层仿真。

Conclusion: 该方法提升了仿真对中等带宽无线系统物理层行为的贴近度，便于评估设计在深衰落环境中的鲁棒性与性能，并为未来的系统研究提供可重复的实现路径。

Abstract: In this paper, we describe the necessary procedures for accurately simulating
digital wireless communication systems operating in the mediumband, aimed at
both beginners and experts. In the research literature, digital wireless
communication systems are typically simulated in the discrete-time complex
baseband domain, where pulse shaping, upconversion, mixing, carrier
synchronization, and symbol timing synchronization are often ignored. These
assumptions are indeed sufficient in most cases, but to capture the essence of
communication in the mediumband, certain physical layer (PHY) operations should
be simulated in detail. In this paper, we concisely describe how to simulate a
mediumband wireless communication scenario from a single transmitter (TX) to a
single receiver (RX) in MATLAB, elaborating the operation of key PHY
subsystems. The approach described here ensures that the simulated system
captures the delicate dynamics of mediumband wireless communication, including
the effect of deep fading avoidance.

</details>


### [41] [Local Information-Theoretic Security via Euclidean Geometry](https://arxiv.org/abs/2510.13661)
*Emmanouil M. Athanasakos,Nicholas Kalouptsidis,Hariprasad Manjunath*

Main category: cs.IT

TL;DR: 提出基于欧氏信息理论的局部安全性分析框架：在离散记忆无记忆信道的安全传输中，通过对信息泄漏和编码成本设定上界，将原本非凸优化问题近似转化为二次规划，并通过线性规划求解拉格朗日乘子，得到近似的局部 secrecy capacity 公式与局部收缩系数（由广义特征值定义）。给出与全局度量的界限，并通过多模态通道和二进制对称信道的数值分析验证。


<details>
  <summary>Details</summary>
Motivation: 在离散记忆无记忆无线窃信道的局部性质研究中，提出以欧氏信息理论为工具的分析框架，旨在在可控近似下获得局部的可实现安全传输能力及其对泄漏的敏感性评估。

Method: 将最大化合法信息率且对信息泄漏与秘密消息编码代价设定上界的非凸问题，利用局部几何近似将其转化为可求解的二次规划；通过求解线性规划获取最优拉格朗日乘子；KKT 条件给出约束，基于信道导出的矩阵的广义特征值构成约束；推导近似的局部 secrecy capacity 公式；定义并分析一类新的秘密局部收缩系数，等同于矩阵铅笔的最大广义特征值。

Result: 给出一个解析的近似局部 secrecy capacity 公式，并给出局部系数与全局系数之间的界限；通过对多模态通道与二进制对称信道的数值分析，验证框架的有效性并提供直观结论。

Conclusion: 该框架提供了一个可行的、可分析的手段来评估信道在局部区域的安全传输性能，揭示了局部收缩系数与广义特征值的关系，以及如何将局部问题转化为可解的优化问题并给出近似解。

Abstract: This paper introduces a methodology based on Euclidean information theory to
investigate local properties of secure communication over discrete memoryless
wiretap channels. We formulate a constrained optimization problem that
maximizes a legitimate user's information rate while imposing explicit upper
bounds on both the information leakage to an eavesdropper and the informational
cost of encoding the secret message. By leveraging local geometric
approximations, this inherently non-convex problem is transformed into a
tractable quadratic programming structure. It is demonstrated that the optimal
Lagrange multipliers governing this approximated problem can be found by
solving a linear program. The constraints of this linear program are derived
from Karush-Kuhn-Tucker conditions and are expressed in terms of the
generalized eigenvalues of channel-derived matrices. This framework facilitates
the derivation of an analytical formula for an approximate local secrecy
capacity. Furthermore, we define and analyze a new class of secret local
contraction coefficients. These coefficients, characterized as the largest
generalized eigenvalues of a matrix pencil, quantify the maximum achievable
ratio of approximate utility to approximate leakage, thus measuring the
intrinsic local leakage efficiency of the channel. We establish bounds
connecting these local coefficients to their global counterparts defined over
true mutual information measures. The efficacy of the proposed framework is
demonstrated through detailed analysis and numerical illustrations for both
general multi-mode channels and the canonical binary symmetric wiretap channel.

</details>


### [42] [Combinatorial Bounds for List Recovery via Discrete Brascamp--Lieb Inequalities](https://arxiv.org/abs/2510.13775)
*Joshua Brakensiek,Yeyuan Chen,Manik Dhar,Zihan Zhang*

Main category: cs.IT

TL;DR: 本文给出多类线性码（包括随机线性码、随机Reed–Solomon码、显式折叠RS码和显式一变量多重性码）的列表可恢复性界限。对于码率R和距离参数ρ≈1−R−ε接近容量时，列表大小L在所有情形上上界为 (ℓ/(R+ε))^{O(R/ε)}，并适用于平均半径情形；零误差情形下界与上界完全匹配。该结果回答了是否存在多项式依赖于ℓ的L界的问题。


<details>
  <summary>Details</summary>
Motivation: 列表可恢复性是列表解码的自然推广，在给定码的前提下，研究在接近容量时能容忍多大错配而仍能有限地列出候选码。长期未解的问题是是否存在对ℓ多项式上界的统一界，以及对多种重要码族的普适性界。

Method: 将离散信息的布鲁姆–李布不等式引入列表可恢复性问题，通过将局部坐标的约束与全局 recovered list 的结构联系起来，得到统一的上界。还证明对折叠Reed–Solomon码的Chen和Zhang 的结果可以推广成一个新的布鲁安–李布类型不等式。

Result: 对于码率R、接近容量的ρ=1−R−ε，本文给出所有考察的码族（随机线性码、随机Reed–Solomon码、显式折叠Reed–Solomon码、显式一变量多重性码）的列表上界：L ≤ (ℓ/(R+ε))^{O(R/ε)}，并且同样适用于平均半径情形。在零误差情形下，上界与已知下界完全匹配。该结果解决了L是否能被多项式在ℓ上界的问题。

Conclusion: 引入的离散布鲁姆–李布不等式为列表可恢复性研究提供了新工具，能够将局部坐标信息转化为对全局列表的约束。相关技术可能推广到更多编码族，并为近容量极限下的列表可恢复性研究提供统一框架。

Abstract: In coding theory, the problem of list recovery asks one to find all codewords
$c$ of a given code $C$ which such that at least $1-\rho$ fraction of the
symbols of $c$ lie in some predetermined set of $\ell$ symbols for each
coordinate of the code. A key question is bounding the maximum possible list
size $L$ of such codewords for the given code $C$.
  In this paper, we give novel combinatorial bounds on the list recoverability
of various families of linear and folded linear codes, including random linear
codes, random Reed--Solomon codes, explicit folded Reed--Solomon codes, and
explicit univariate multiplicity codes. Our main result is that in all of these
settings, we show that for code of rate $R$, when $\rho = 1 - R - \epsilon$
approaches capacity, the list size $L$ is at most
$(\ell/(R+\epsilon))^{O(R/\epsilon)}$. These results also apply in the
average-radius regime. Our result resolves a long-standing open question on
whether $L$ can be bounded by a polynomial in $\ell$. In the zero-error regime,
our bound on $L$ perfectly matches known lower bounds.
  The primary technique is a novel application of a discrete entropic
Brascamp--Lieb inequality to the problem of list recovery, allowing us to
relate the local structure of each coordinate with the global structure of the
recovered list. As a result of independent interest, we show that a recent
result by Chen and Zhang (STOC 2025) on the list decodability of folded
Reed--Solomon codes can be generalized into a novel Brascamp--Lieb type
inequality.

</details>


### [43] [From Random to Explicit via Subspace Designs With Applications to Local Properties and Matroids](https://arxiv.org/abs/2510.13777)
*Joshua Brakensiek,Yeyuan Chen,Manik Dhar,Zihan Zhang*

Main category: cs.IT

TL;DR: 扩展了一个统一阈值框架到可子空间设计的码，建立了局部性质与随机线性码之间的局部等价，给出同时具备随机线性局部性质的显式折叠线性码；对最大可恢复张量码的纠错消去模式给出多项式时间判定，依赖马森(matroid)相关问题的正答案；并对子空间设计的存在性与极限给出新见解，证明在代数闭域上不存在更好的子空间设计。


<details>
  <summary>Details</summary>
Motivation: 理解和量化码的局部性质（如列表解码/列表恢复）的阈值率，并把随机码的局部性质与结构化可实现的子空间设计码联系起来，进一步拓展到 matroid 理论和张量码等领域。

Method: 在 Levi–Mosheiff–Shagrithaya 的统一框架基础之上，扩展到可子空间设计的码，建立随机线性码与子空间设计码在局部属性上的等价关系（上下界的轻微速率损失可控）。给出显式折叠线性码的构造，能够同时实现随机线性码的所有局部性质。将局部性质的传递性应用到最大可恢复张量码的可纠错擦除模式的判定，前提是对 Mason 的马陀问题有阳性回答。进一步分析子空间设计的存在性， tightened 对 Guruswami–Kopparty 构造的评估，证明在代数闭域上不存在更好的子空间设计。

Result: 1) 本文给出局部等价：随机线性码的任何局部性质对所有可子空间设计码成立，且可通过小幅度的速率折让实现。2) 构造出显式的 folded linear codes，能够同时达到随机线性码的所有局部性质。3) 证明在最大可恢复张量码中，正多项式时间内可识别可纠错的擦除模式，条件是对 Mason 的马陀问题给出肯定答案，同时该结果亦适用于泛型二部兼容性与矩阵完成的马陀。4) 对子空间设计的存在性与极限进行更精确分析，表明在代数闭域上不存在更好的子空间设计。

Conclusion: 该工作把随机码的局部性质与结构化子空间设计码的性质统一起来，提供显式构造以实现随机码的局部性能，推动了在张量码和马陀理论中的算法性与复杂性结果；同时给出子空间设计的严格极限，指明未来在改进构造方面的边界。

Abstract: In coding theory, a common question is to understand the threshold rates of
various local properties of codes, such as their list decodability and list
recoverability. A recent work Levi, Mosheiff, and Shagrithaya (FOCS 2025) gave
a novel unified framework for calculating the threshold rates of local
properties for random linear and random Reed--Solomon codes.
  In this paper, we extend their framework to studying the local properties of
subspace designable codes, including explicit folded Reed-Solomon and
univariate multiplicity codes. Our first main result is a local equivalence
between random linear codes and (nearly) optimal subspace design codes up to an
arbitrarily small rate decrease. We show any local property of random linear
codes applies to all subspace design codes. As such, we give the first explicit
construction of folded linear codes that simultaneously attain all local
properties of random linear codes. Conversely, we show that any local property
which applies to all subspace design codes also applies to random linear codes.
  Our second main result is an application to matroid theory. We show that the
correctable erasure patterns in a maximally recoverable tensor code can be
identified in deterministic polynomial time, assuming a positive answer to a
matroid-theoretic question due to Mason (1981). This improves on a result of
Jackson and Tanigawa (JCTB 2024) who gave a complexity characterization of
$\mathsf{RP} \cap \mathsf{coNP}$ assuming a stronger conjecture. Our result
also applies to the generic bipartite rigidity and matrix completion matroids.
  As a result of additional interest, we study the existence and limitations of
subspace designs. In particular, we tighten the analysis of family of subspace
designs constructioned by Guruswami and Kopparty (Combinatorica 2016) and show
that better subspace designs do not exist over algebraically closed fields.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Lifting Manifolds to Mitigate Pseudo-Alignment in LLM4TS](https://arxiv.org/abs/2510.12847)
*Liangwei Nathan Zheng,Wenhao Liang,Wei Emma Zhang,Miao Xu,Olaf Maennel,Weitong Chen*

Main category: cs.LG

TL;DR: 提出 TimeSUP 来缓解 LLM4TS 的伪对齐问题，揭示其根源在预训练锥形效应与时间序列低维流形的交互，且在多条管线中提升长序列预测。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在时间序列任务中的伪对齐现象的根本原因，并将其与语言模型中的锥形效应联系起来，寻求提高跨模态嵌入区分度与共享性的解决方案。

Method: 1) 分析 pretrained LLM 的 cone effect 与时间序列低维流形的关系；2) 提出 TimeSUP，通过扩大时间序列流形以更贴合语言嵌入的本质维度，使模型能更清晰地区分时间信号，同时保留跨模态的共性；3) 将 TimeSUP 无缝集成到现有四条 LLM4TS 管线中。

Result: TimeSUP 在长期预测任务中持续超越当前最先进的 LLM4TS 方法和轻量基线，且具有良好通用性，能够无缝集成到四条管线，显著提升预测性能。

Conclusion: TimeSUP 能有效缓解伪对齐现象，通过扩展时间序列流形以匹配语言嵌入的内在维度，提升对时间信号的区分和跨模态共性学习，具备广泛适用性。

Abstract: Pseudo-Alignment is a pervasive challenge in many large language models for
time series (LLM4TS) models, often causing them to underperform compared to
linear models or randomly initialised backbones. However, there is limited
discussion in the community for the reasons that pseudo-alignment occurs. In
this work, we conduct a thorough investigation into the root causes of
pseudo-alignment in LLM4TS and build a connection of pseudo-alignment to the
cone effect in LLM. We demonstrate that pseudo-alignment arises from the
interplay of cone effect within pretrained LLM components and the intrinsically
low-dimensional manifold of time-series data. In addition, we also introduce
\textit{\textbf{TimeSUP}}, a novel technique designed to mitigate this issue
and improve forecast performance in existing LLM4TS approaches. TimeSUP
addresses this by increasing the time series manifold to more closely match the
intrinsic dimension of language embeddings, allowing the model to distinguish
temporal signals clearly while still capturing shared structures across
modalities. As a result, representations for time and language tokens remain
distinct yet exhibit high cosine similarity, signifying that the model
preserves each modality unique features while learning their commonalities in a
unified embedding space. Empirically, TimeSUP consistently outperforms
state-of-the-art LLM4TS methods and other lightweight baselines on long-term
forecasting performance. Furthermore, it can be seamlessly integrated into four
existing LLM4TS pipelines and delivers significant improvements in forecasting
performance.

</details>


### [45] [FedGTEA: Federated Class-Incremental Learning with Gaussian Task Embedding and Alignment](https://arxiv.org/abs/2510.12927)
*Haolin Li,Hoda Bidkhori*

Main category: cs.LG

TL;DR: 提出了 FedGTEA 框架用于联邦类增量学习，通过高斯任务嵌入(CATE)实现任务知识编码与不确定性建模，服务端使用 2-Wasserstein 距离衡量任务间差距并应用 Wasserstein 损失以实现任务间分离，同时保护隐私。实验表明在分类性能和遗忘抑制方面优于强基线，且具可扩展性和通信效率。


<details>
  <summary>Details</summary>
Motivation: 在多客户端数据分布异质性和类增量场景下，解决遗忘问题并实现可扩展、隐私友好的联邦学习框架。

Method: 客户端使用 Cardinality-Agnostic Task Encoder (CATE) 生成高斯分布的任务嵌入，嵌入参数规模不随任务数量增长而增加；服务端通过 2-Wasserstein 距离度量高斯嵌入之间的任务间差距，构造 Wasserstein 损失以强制任务间的分离，同时通过不传输潜在嵌入来保护任务级隐私。

Result: 在热门数据集上的大量实验显示 FedGTEA 能获得更好的分类性能并显著缓解遗忘，普遍优于现有强基线。

Conclusion: 提出的 FedGTEA 提供一个可扩展、隐私友好且对任务增量鲁棒的联邦学习框架，能在多任务场景中提升表示学习和遗忘控制能力。

Abstract: We introduce a novel framework for Federated Class Incremental Learning,
called Federated Gaussian Task Embedding and Alignment (FedGTEA). FedGTEA is
designed to capture task-specific knowledge and model uncertainty in a scalable
and communication-efficient manner. At the client side, the
Cardinality-Agnostic Task Encoder (CATE) produces Gaussian-distributed task
embeddings that encode task knowledge, address statistical heterogeneity, and
quantify data uncertainty. Importantly, CATE maintains a fixed parameter size
regardless of the number of tasks, which ensures scalability across long task
sequences. On the server side, FedGTEA utilizes the 2-Wasserstein distance to
measure inter-task gaps between Gaussian embeddings. We formulate the
Wasserstein loss to enforce inter-task separation. This probabilistic
formulation not only enhances representation learning but also preserves
task-level privacy by avoiding the direct transmission of latent embeddings,
aligning with the privacy constraints in federated learning. Extensive
empirical evaluations on popular datasets demonstrate that FedGTEA achieves
superior classification performance and significantly mitigates forgetting,
consistently outperforming strong existing baselines.

</details>


### [46] [Learning at the Speed of Physics: Equilibrium Propagation on Oscillator Ising Machines](https://arxiv.org/abs/2510.12934)
*Alex Gower*

Main category: cs.LG

TL;DR: 在物理硬件上通过等势传播（EP）对振荡自旋机（OIMs）进行学习，实现能量下降驱动的高效神经拟合，达到接近传统算法的准确率且对硬件噪声有健壮性。


<details>
  <summary>Details</summary>
Motivation: 寻找利用能量下降原理的物理系统来加速机器学习，尤其是通过振荡自旋机实现对能量基模型（EBM）的优化、采样和梯度下降的统一框架，并探究在硬件上实现的可行性与效率。

Method: 在振荡自旋机上应用等势传播（EP），通过局部可更新的能量函数实现无反向传播的学习规则，将优化和采样整合到单一体系的总能量景观中；并在MNIST和Fashion-MNIST数据集上评估该方法的分类准确性，同时考察参数量化和相位噪声等现实硬件约束的鲁棒性。

Result: 在MNIST上达到约97.2% ±0.1的准确率，在Fashion-MNIST上约88.0% ±0.1的准确率；且在参数量化和相位噪声等现实硬件条件下表现鲁棒，显示OIMs作为快速、低功耗的神经拟合底层是可行的。

Conclusion: 将OIMs作为实现EBMs与采样的高效物理硬件平台，显示了在没有全局反向传播的前提下进行局部学习的可行性，为将EBMs落地于物理硬件提供了实际路径，可能推动在神经形态计算中的高效学习应用。

Abstract: Physical systems that naturally perform energy descent offer a direct route
to accelerating machine learning. Oscillator Ising Machines (OIMs) exemplify
this idea: their GHz-frequency dynamics mirror both the optimization of
energy-based models (EBMs) and gradient descent on loss landscapes, while
intrinsic noise corresponds to Langevin dynamics - supporting sampling as well
as optimization. Equilibrium Propagation (EP) unifies these processes into
descent on a single total energy landscape, enabling local learning rules
without global backpropagation. We show that EP on OIMs achieves competitive
accuracy ($\sim 97.2 \pm 0.1 \%$ on MNIST, $\sim 88.0 \pm 0.1 \%$ on
Fashion-MNIST), while maintaining robustness under realistic hardware
constraints such as parameter quantization and phase noise. These results
establish OIMs as a fast, energy-efficient substrate for neuromorphic learning,
and suggest that EBMs - often bottlenecked by conventional processors - may
find practical realization on physical hardware whose dynamics directly perform
their optimization.

</details>


### [47] [An Investigation of Memorization Risk in Healthcare Foundation Models](https://arxiv.org/abs/2510.12950)
*Sana Tonekaboni,Lena Stempfle,Adibvafa Fallahpour,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 提出一套黑箱评估测试来衡量在大规模去识别化电子健康记录（EHR）上训练的基础模型在隐私相关记忆方面的风险，覆盖嵌入与生成阶段，区分泛化与有害记忆，并对公开EHR基模型进行验证，且开源工具包便于复现。


<details>
  <summary>Details</summary>
Motivation: 基础模型可能在训练数据中记忆敏感患者信息，带来隐私泄露风险，需系统化评估其记忆行为，特别是在临床敏感场景及易受伤害人群中的隐私保护挑战。

Method: 提出一套黑箱评估框架，包含嵌入级别和生成级别的记忆探测方法，区分模型泛化与有害 memorization；在公开的EHR基础模型上进行验证，并发布开源工具包以促进可重复、协作性的隐私评估。

Result: 在公开EHR基础模型上验证了该评估框架的可行性，能够识别并区分潜在的隐私相关记忆风险与一般泛化行为，提供了可操作的隐私评估工具。

Conclusion: 该框架有助于在医疗AI应用中实现可重复的隐私评估，提升对记忆风险的理解与治理，并推动基于开源工具的协作隐私评估生态发展。

Abstract: Foundation models trained on large-scale de-identified electronic health
records (EHRs) hold promise for clinical applications. However, their capacity
to memorize patient information raises important privacy concerns. In this
work, we introduce a suite of black-box evaluation tests to assess
privacy-related memorization risks in foundation models trained on structured
EHR data. Our framework includes methods for probing memorization at both the
embedding and generative levels, and aims to distinguish between model
generalization and harmful memorization in clinically relevant settings. We
contextualize memorization in terms of its potential to compromise patient
privacy, particularly for vulnerable subgroups. We validate our approach on a
publicly available EHR foundation model and release an open-source toolkit to
facilitate reproducible and collaborative privacy assessments in healthcare AI.

</details>


### [48] [Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile Apps](https://arxiv.org/abs/2510.13405)
*Chen Gong,Yan Zhuang,Zhenzhe Zheng,Yiliu Chen,Sheng Wang,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning (ML) models are increasingly integrated into modern mobile
apps to enable personalized and intelligent services. These models typically
rely on rich input features derived from historical user behaviors to capture
user intents. However, as ML-driven services become more prevalent, recording
necessary user behavior data imposes substantial storage cost on mobile apps,
leading to lower system responsiveness and more app uninstalls. To address this
storage bottleneck, we present AdaLog, a lightweight and adaptive system
designed to improve the storage efficiency of user behavior log in ML-embedded
mobile apps, without compromising model inference accuracy or latency. We
identify two key inefficiencies in current industrial practices of user
behavior log: (i) redundant logging of overlapping behavior data across
different features and models, and (ii) sparse storage caused by storing
behaviors with heterogeneous attribute descriptions in a single log file. To
solve these issues, AdaLog first formulates the elimination of feature-level
redundant data as a maximum weighted matching problem in hypergraphs, and
proposes a hierarchical algorithm for efficient on-device deployment. Then,
AdaLog employs a virtually hashed attribute design to distribute heterogeneous
behaviors into a few log files with physically dense storage. Finally, to
ensure scalability to dynamic user behavior patterns, AdaLog designs an
incremental update mechanism to minimize the I/O operations needed for adapting
outdated behavior log. We implement a prototype of AdaLog and deploy it into
popular mobile apps in collaboration with our industry partner. Evaluations on
real-world user data show that AdaLog reduces behavior log size by 19% to 44%
with minimal system overhead (only 2 seconds latency and 15 MB memory usage),
providing a more efficient data foundation for broader adoption of on-device
ML.

</details>


### [49] [A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning](https://arxiv.org/abs/2510.12957)
*Noor Islam S. Mohammad*

Main category: cs.LG

TL;DR: 提出一个将多模态注意力特征融合、Grad-CAM++局部解释与Reveal-to-Revise偏见修正循环结合的多模态XAI框架，在多模态MNIST扩展上实现高准确性、高F1和较高解释保真度。


<details>
  <summary>Details</summary>
Motivation: 标准基准（如MNIST）难以揭示潜在偏见和多模态特征的复杂性，制约高风险场景的可信度。

Method: 提出多模态XAI框架，包含注意力增强的特征融合、基于Grad-CAM++的局部解释，以及Reveal-to-Revise的偏见检测与修正反馈回路；在多模态MNIST扩展数据集上进行评估。

Result: 分类准确性93.2%、F1-score 91.6%、解释保真度IoU-XAI 78.1%；相较单模态与非可解释基线有所提升；消融研究显示将可解释性与偏见感知学习结合有助于鲁棒性与人类对齐。

Conclusion: 在性能、透明度与公平性之间架起桥梁，为敏感领域的可信AI提供实用路径。

Abstract: Standard benchmark datasets, such as MNIST, often fail to expose latent
biases and multimodal feature complexities, limiting the trustworthiness of
deep neural networks in high-stakes applications. We propose a novel multimodal
Explainable AI (XAI) framework that unifies attention-augmented feature fusion,
Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop for
bias detection and mitigation. Evaluated on multimodal extensions of MNIST, our
approach achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1%
explanation fidelity (IoU-XAI), outperforming unimodal and non-explainable
baselines. Ablation studies demonstrate that integrating interpretability with
bias-aware learning enhances robustness and human alignment. Our work bridges
the gap between performance, transparency, and fairness, highlighting a
practical pathway for trustworthy AI in sensitive domains.

</details>


### [50] [Balancing Performance and Reject Inclusion: A Novel Confident Inlier Extrapolation Framework for Credit Scoring](https://arxiv.org/abs/2510.12967)
*Athyrson Machado Ribeiro,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: CI-EX 提出了一种面向拒绝推断的边际化异常外推框架，通过对 rejected 客户分布进行识别与标注，以提升 RI 指标同时保持 AUC 的竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决拒绝推断中的样本偏差问题：直接从接受样本外推拒绝样本的行为可能导致分布差异带来的偏差。需要避免盲目外推，改进对拒绝样本分布的估计与标注。

Method: CI-EX 通过迭代式的异常检测模型来识别拒绝样本的分布，并基于一个监督分类模型得到的概率，将与接受人群分布最接近的拒绝个体赋予标签，从而实现对拒绝样本的可信外推。

Result: 在两个大型真实世界信用数据集上进行实验，评估指标包括 AUC、Kickout 及本文新提出的 Area under the Kickout。结果显示 RI 方法通常在 AUC 与 RI 指标之间存在折中，但 CI-EX 在 RI 特定指标上持续优于现有的信用领域 RI 模型，同时在大多数实验中保持了对 AUC 的竞争力。

Conclusion: CI-EX 提供了一个对拒绝样本分布进行更保守且可解释外推的框架，提升了 RI 指标的表现，同时不显著牺牲 AUC，揭示了 RI 与 AUC 之间的权衡关系。

Abstract: Reject Inference (RI) methods aim to address sample bias by inferring missing
repayment data for rejected credit applicants. Traditional approaches often
assume that the behavior of rejected clients can be extrapolated from accepted
clients, despite potential distributional differences between the two
populations. To mitigate this blind extrapolation, we propose a novel Confident
Inlier Extrapolation framework (CI-EX). CI-EX iteratively identifies the
distribution of rejected client samples using an outlier detection model and
assigns labels to rejected individuals closest to the distribution of the
accepted population based on probabilities derived from a supervised
classification model. The effectiveness of our proposed framework is validated
through experiments on two large real-world credit datasets. Performance is
evaluated using the Area Under the Curve (AUC) as well as RI-specific metrics
such as Kickout and a novel metric introduced in this work, denoted as Area
under the Kickout. Our findings reveal that RI methods, including the proposed
framework, generally involve a trade-off between AUC and RI-specific metrics.
However, the proposed CI-EX framework consistently outperforms existing RI
models from the credit literature in terms of RI-specific metrics while
maintaining competitive performance in AUC across most experiments.

</details>


### [51] [A Connection Between Score Matching and Local Intrinsic Dimension](https://arxiv.org/abs/2510.12975)
*Eric Yeats,Aaron Jacobson,Darryl Hannan,Yiran Jia,Timothy Doster,Henry Kvinge,Scott Mahan*

Main category: cs.LG

TL;DR: 本论文提出将去噪分数匹配损失作为局部内在维数（LID）的估计量，通过理论证明LID是去噪分数匹配损失的下界，并将等价隐式分数匹配损失与正态维度近似的关系联系到最近的LID估计器FLIPD；在流形基准和Stable Diffusion 3.5上的实验显示，该估计量在准确性和内存占用方面具备竞争力，且在规模扩大和量化水平提升时表现更优。


<details>
  <summary>Details</summary>
Motivation: 高维、复杂数据的局部内在维数难以量化，现有基于扩散模型的LID估计需要大量前向推理或梯度计算，资源受限场景下不可行，因此需要一个可扩展且高效的LID估计方法。

Method: 给出理论结果：LID是去噪分数匹配损失的下界；等价的隐式分数匹配损失通过正态维度近似LID，并与最近的LID估计器FLIPD相关。提出使用去噪分数匹配损失作为LID估计量；在流形基准和Stable Diffusion 3.5上进行实验评估以比较精度与内存开销。

Result: 实验证明，去噪分数匹配损失是一个具有竞争力且可扩展的LID估计量；在问题规模增大和量化水平提升时，获得更高的准确性与更低的内存 footprint，相较于现有基于扩散模型的方法。

Conclusion: 该方法为在资源受限场景中进行LID估计提供了一条高效路径，并揭示了LID与分数匹配损失及FLIPD之间的联系，促进对数据局部几何的理解。

Abstract: The local intrinsic dimension (LID) of data is a fundamental quantity in
signal processing and learning theory, but quantifying the LID of
high-dimensional, complex data has been a historically challenging task. Recent
works have discovered that diffusion models capture the LID of data through the
spectra of their score estimates and through the rate of change of their
density estimates under various noise perturbations. While these methods can
accurately quantify LID, they require either many forward passes of the
diffusion model or use of gradient computation, limiting their applicability in
compute- and memory-constrained scenarios.
  We show that the LID is a lower bound on the denoising score matching loss,
motivating use of the denoising score matching loss as a LID estimator.
Moreover, we show that the equivalent implicit score matching loss also
approximates LID via the normal dimension and is closely related to a recent
LID estimator, FLIPD. Our experiments on a manifold benchmark and with Stable
Diffusion 3.5 indicate that the denoising score matching loss is a highly
competitive and scalable LID estimator, achieving superior accuracy and memory
footprint under increasing problem size and quantization level.

</details>


### [52] [Reference-Specific Unlearning Metrics Can Hide the Truth: A Reality Check](https://arxiv.org/abs/2510.12981)
*Sungjun Cho,Dasol Hwang,Frederic Sala,Sangheum Hwang,Kyunghyun Cho,Sungmin Cha*

Main category: cs.LG

TL;DR: FADE是一种评估模型未学习效果的分布等价性指标，通过比较生成样本上的双向似然分配来量化输出分布的一致性，解决对参考响应的依赖性及隐藏盲点。


<details>
  <summary>Details</summary>
Motivation: 现有未学习评估多依赖预设参考或分类器输出，未直接衡量未学习后模型在整体输出分布上的行为是否与从未接触该数据的模型等价，导致对未学习效果的系统性盲点。

Method: 提出FADE（Functional Alignment for Distributional Equivalence），通过在生成样本上计算未学习模型与参考模型的双向似然分配来衡量分布对齐度，覆盖整个输出分布，且不依赖单一固定参考。

Result: 在TOFU基准的LLM未学习和UnlearnCanvas基准的文本到图像扩散模型未学习实验中，接近传统指标高分的方法并未实现真正的分布等价，甚至在某些情况下比未学习前更远离金标准，暴露了现有评估的不足。

Conclusion: FADE为未学习评估提供了更稳健的分布层次指标，有助于推动开发和评估真正有效的未学习方法。

Abstract: Current unlearning metrics for generative models evaluate success based on
reference responses or classifier outputs rather than assessing the core
objective: whether the unlearned model behaves indistinguishably from a model
that never saw the unwanted data. This reference-specific approach creates
systematic blind spots, allowing models to appear successful while retaining
unwanted knowledge accessible through alternative prompts or attacks. We
address these limitations by proposing Functional Alignment for Distributional
Equivalence (FADE), a novel metric that measures distributional similarity
between unlearned and reference models by comparing bidirectional likelihood
assignments over generated samples. Unlike existing approaches that rely on
predetermined references, FADE captures functional alignment across the entire
output distribution, providing a principled assessment of genuine unlearning.
Our experiments on the TOFU benchmark for LLM unlearning and the UnlearnCanvas
benchmark for text-to-image diffusion model unlearning reveal that methods
achieving near-optimal scores on traditional metrics fail to achieve
distributional equivalence, with many becoming more distant from the gold
standard than before unlearning. These findings expose fundamental gaps in
current evaluation practices and demonstrate that FADE provides a more robust
foundation for developing and assessing truly effective unlearning methods.

</details>


### [53] [CSI-4CAST: A Hybrid Deep Learning Model for CSI Prediction with Comprehensive Robustness and Generalization Testing](https://arxiv.org/abs/2510.12996)
*Sikai Cheng,Reza Zandehshahvar,Haoruo Zhao,Daniel A. Garcia-Ulloa,Alejandro Villena-Rodriguez,Carles Navarro Manchón,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出 CSI-4CAST 及 CSI-RRG：一个混合深度学习架构用于高效鲁棒的 CSI 预测，并提供大规模基准数据集与评测协议。


<details>
  <summary>Details</summary>
Motivation: 在 mMIMO 场景下，实时下行 CSI 预测需对非高斯噪声、不同信道条件的泛化和计算效率具鲁棒性，但现有深度学习方法仍存在改进空间。

Method: 将 CNN 残差、Adaptive correction 层、ShuffleNet 模块与 Transformers 结合，形成能同时捕捉局部与长程依赖的 CSI 预测框架；同时构建并发布 CSI-RRG 基准数据集，涵盖 3,060 个现实场景、超过 30 万样本，包含 TDD 与 FDD、多信道模型、延迟扩散、速率、噪声类型及强度。

Result: 实验结果表明，CSI-4CAST 在预测准确性和计算成本方面优于基线，在 88.9% 的 TDD 场景和 43.8% 的 FDD 场景中达到最佳或接近最佳；相对强基线 LLM4CP，FLOPs 下降 5x（与 3x 的对比）。CSI-RRG 还提供关于信道因素对模型性能与泛化能力的洞察。

Conclusion: 公开的数据集和评测协议有望为鲁棒高效 CSI 预测研究建立标准化基准，推动相关方法在鲁棒性、泛化性和计算效率方面的提升。

Abstract: Channel state information (CSI) prediction is a promising strategy for
ensuring reliable and efficient operation of massive multiple-input
multiple-output (mMIMO) systems by providing timely downlink (DL) CSI. While
deep learning-based methods have advanced beyond conventional model-driven and
statistical approaches, they remain limited in robustness to practical
non-Gaussian noise, generalization across diverse channel conditions, and
computational efficiency. This paper introduces CSI-4CAST, a hybrid deep
learning architecture that integrates 4 key components, i.e., Convolutional
neural network residuals, Adaptive correction layers, ShuffleNet blocks, and
Transformers, to efficiently capture both local and long-range dependencies in
CSI prediction. To enable rigorous evaluation, this work further presents a
comprehensive benchmark, CSI-RRG for Regular, Robustness and Generalization
testing, which includes more than 300,000 samples across 3,060 realistic
scenarios for both TDD and FDD systems. The dataset spans multiple channel
models, a wide range of delay spreads and user velocities, and diverse noise
types and intensity degrees. Experimental results show that CSI-4CAST achieves
superior prediction accuracy with substantially lower computational cost,
outperforming baselines in 88.9% of TDD scenarios and 43.8% of FDD scenario,
the best performance among all evaluated models, while reducing FLOPs by 5x and
3x compared to LLM4CP, the strongest baseline. In addition, evaluation over
CSI-RRG provides valuable insights into how different channel factors affect
the performance and generalization capability of deep learning models. Both the
dataset (https://huggingface.co/CSI-4CAST) and evaluation protocols
(https://github.com/AI4OPT/CSI-4CAST) are publicly released to establish a
standardized benchmark and to encourage further research on robust and
efficient CSI prediction.

</details>


### [54] [Max It or Miss It: Benchmarking LLM On Solving Extremal Problems](https://arxiv.org/abs/2510.12997)
*Binxin Gao,Jingjun Han*

Main category: cs.LG

TL;DR: ExtremBench揭示了大语言模型在极值求解中的推理能力与现有数学基准之间的错位；当前评估可能无法全面覆盖推理能力的全谱。


<details>
  <summary>Details</summary>
Motivation: 系统性评估LLMs在优化推理（受约束条件下的极值求解）方面的能力，揭示现有基准在覆盖极值推理能力方面的不足与偏差。

Method: 将从中国数学奥林匹克的不等式练习中提取并转换成93道标准化的极值寻找问题，形成ExtremBench。对多种开源模型家族（如Qwen3、GPT-OSS、DeepSeek）进行广泛评测，并与AIME25、MATH-500等基准进行对比分析。

Result: 实验结果显示，模型的极值求解推理能力并不总是与当前数学基准（如AIME25、MATH-500）一致；有些模型在一般数学推理方面表现突出，但在极值求解上表现欠佳，反之亦然。这揭示了评估实践中的关键差距，即现有基准可能未能充分覆盖数学推理能力的全谱。

Conclusion: ExtremBench为评估优化推理提供了新的方向，强调需构建更全面的基准来捕捉LLMs在受约束条件下的极值及相关推理能力。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) with remarkable
reasoning capabilities, particularly in mathematical domains, through
intermediate chain-of-thought (CoT) reasoning before generating final answers.
However, the specific sources and mechanisms underlying these reasoning
capabilities remain insufficiently understood. Optimization reasoning, i.e.
finding extrema under constraints, represents a fundamental abstraction that
underpins critical applications in planning, control, resource allocation, and
prompt search. To systematically evaluate this capability, we introduce
ExtremBench, a benchmark dataset for solving mathematical extremal problems,
curated from inequality exercises used for Chinese Mathematical Olympiad and
transformed into $93$ standardized extrema-finding problems. We conduct
extensive evaluations across various state-of-the-art open-source model
families, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that
LLMs' extremal-solving reasoning capabilities do not always align with those of
current mathematical benchmarks such as AIME25 and MATH-500, with some models
showing strong general mathematical reasoning but poor extremal-solving skills,
and vice versa. This discrepancy highlights a critical gap in current
evaluation practices and suggests that existing benchmarks may not
comprehensively capture the full spectrum of mathematical reasoning abilities.

</details>


### [55] [Time-Varying Optimization for Streaming Data Via Temporal Weighting](https://arxiv.org/abs/2510.13052)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi,Erik G. Larsson*

Main category: cs.LG

TL;DR: 分析时间变化优化中的权重化学习：对流数据流的两种权重策略（均匀与折扣）下，研究梯度下降下的跟踪误差（TE）界限；均匀权重TE随时间趋于0，速率为O(1/t)；折扣权重导致存在非零误差下界，由折扣因子和每时间步的更新次数决定；通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中从流数据学习，传统固定目标优化难以适应，提出一种结构化的、基于权重的时变优化框架，以显式捕捉数据流来源对时变目标的影响，并比较两种权重策略对跟踪性能的影响。

Method: 构建两种权重策略的时间变目标：对过去数据样本的加权平均损失；在每个时间步用梯度下降更新参数；推导并给出在这两种权重下的跟踪误差（TE）的严格上界或渐近界；分析均匀权重下TE的渐近行为（O(1/t)收敛到0)以及折扣权重下存在的误差下界及其依赖关系（折扣因子与每时步的更新次数）；通过数值仿真实验验证理论。

Result: 给出两种权重策略下的TE界限与行为结论：均匀权重使TE随时间衰减并趋于0，速率为O(1/t)；折扣权重导致TE存在非零的稳态误差下界，该下界受折扣因子及每时间步的梯度更新次数影响。

Conclusion: 通过结构化的权重化时变优化框架，揭示权重设计对跟踪性能的关键影响。均匀权重在渐近追踪方面优于折扣权重，因为后者引入的误差下界。理论结果得到数值实验的支持，强调在流数据学习中选择权重策略的重要性。

Abstract: Classical optimization theory deals with fixed, time-invariant objective
functions. However, time-varying optimization has emerged as an important
subject for decision-making in dynamic environments. In this work, we study the
problem of learning from streaming data through a time-varying optimization
lens. Unlike prior works that focus on generic formulations, we introduce a
structured, \emph{weight-based} formulation that explicitly captures the
streaming-data origin of the time-varying objective, where at each time step,
an agent aims to minimize a weighted average loss over all the past data
samples. We focus on two specific weighting strategies: (1) uniform weights,
which treat all samples equally, and (2) discounted weights, which
geometrically decay the influence of older data. For both schemes, we derive
tight bounds on the ``tracking error'' (TE), defined as the deviation between
the model parameter and the time-varying optimum at a given time step, under
gradient descent (GD) updates. We show that under uniform weighting, the TE
vanishes asymptotically with a $\mathcal{O}(1/t)$ decay rate, whereas
discounted weighting incurs a nonzero error floor controlled by the discount
factor and the number of gradient updates performed at each time step. Our
theoretical findings are validated through numerical simulations.

</details>


### [56] [Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment](https://arxiv.org/abs/2510.13023)
*Joshua R. Tempelman,Adam J. Wachtor,Eric B. Flynn*

Main category: cs.LG

TL;DR: An integrated end-to-end ML workflow for automated ultrasonic weld inspection under industrial noise, combining reduced-order Helmholtz-based data generation, transfer learning, diffusion-based distribution alignment, and U-Net inversion.


<details>
  <summary>Details</summary>
Motivation: Overcome limited labeled data and environmental variability that corrupt measurements in industrial NDE weld inspection.

Method: 1) Generate synthetic dataset via a reduced-order Helmholtz model (Lamb wave theory) across weld heterogeneity and cracks. 2) Refine with transfer learning using a small set of full 3D elastodynamic sims. 3) Handle out-of-distribution LDV noise with guided diffusion to map to in-distribution representations. 4) Use U-Net-based segmentation and inversion for end-to-end weld defect detection/quantification.

Result: End-to-end solution demonstrated for real data; shows viability of training with limited data and coping with noise; diffusion-guided alignment yields robust inversion.

Conclusion: The framework addresses data curation and signal corruption, enabling practical automated weld inspection in industrial settings and potentially generalizing to other NDE tasks.

Abstract: Automated ultrasonic weld inspection remains a significant challenge in the
nondestructive evaluation (NDE) community to factors such as limited training
data (due to the complexity of curating experimental specimens or high-fidelity
simulations) and environmental volatility of many industrial settings
(resulting in the corruption of on-the-fly measurements). Thus, an end-to-end
machine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,
industrial) settings has remained an elusive goal. This work addresses the
challenges of data curation and signal corruption by proposing workflow
consisting of a reduced-order modeling scheme, diffusion based distribution
alignment, and U-Net-based segmentation and inversion. A reduced-order
Helmholtz model based on Lamb wave theory is used to generate a comprehensive
dataset over varying weld heterogeneity and crack defects. The relatively
inexpensive low-order solutions provide a robust training dateset for inversion
models which are refined through a transfer learning stage using a limited set
of full 3D elastodynamic simulations. To handle out-of-distribution (OOD)
real-world measurements with varying and unpredictable noise distributions,
i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distribution
representations of OOD experimental LDV scans which are subsequently processed
by the inversion models. This integrated framework provides an end-to-end
solution for automated weld inspection on real data.

</details>


### [57] [Information Shapes Koopman Representation](https://arxiv.org/abs/2510.13025)
*Xiaoyuan Cheng,Wenxuan Yuan,Yiming Yang,Yuanzhao Zhang,Sibo Cheng,Yi He,Zhuo Sun*

Main category: cs.LG

TL;DR: 通过信息论拉格朗日框架平衡简约性与表达性，以改进 Koopman 学习中的潜在子空间选择；使用潜在互信息促进简约、von Neumann 熵促进模式多样性，提出稳定且可解释的 Koopman 表征及新算法，并在多类动力系统上实现并验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Koopman 算子提供强大动力学建模能力，但无限维特性使得在深度模型中选择合适的有限子空间变得困难。潜表示需要在表达能力和简洁性之间取得平衡，这与信息瓶颈问题密切相关。

Method: 提出信息论拉格朗日（information-theoretic Lagrangian），同时最大化潜在互信息以提升简约性与利用 von Neumann 熵防止潜在表示坍缩、维持模式多样性。给出一个具体的优化算法以同时实现简约性与表达性，并通过对数理分析和可视化验证理论预测。实现公开且在多类动力系统上进行评估。

Result: 所提出的方法在稳定性、可解释性和预测性能上优于现有 Koopman 学习方法；学习到的流形与理论预期一致，实验结果与可视化均支持信息瓶颈与熵约束对表示的正向作用；公开实现便于复现实验。

Conclusion: 信息论驱动的拉格朗日框架为 Koopman 学习提供一个稳定且可解释的表示，潜在互信息与 von Neumann 熵的结合有效解决子空间选择难题并提升泛化能力。

Abstract: The Koopman operator provides a powerful framework for modeling dynamical
systems and has attracted growing interest from the machine learning community.
However, its infinite-dimensional nature makes identifying suitable
finite-dimensional subspaces challenging, especially for deep architectures. We
argue that these difficulties come from suboptimal representation learning,
where latent variables fail to balance expressivity and simplicity. This
tension is closely related to the information bottleneck (IB) dilemma:
constructing compressed representations that are both compact and predictive.
Rethinking Koopman learning through this lens, we demonstrate that latent
mutual information promotes simplicity, yet an overemphasis on simplicity may
cause latent space to collapse onto a few dominant modes. In contrast,
expressiveness is sustained by the von Neumann entropy, which prevents such
collapse and encourages mode diversity. This insight leads us to propose an
information-theoretic Lagrangian formulation that explicitly balances this
tradeoff. Furthermore, we propose a new algorithm based on the Lagrangian
formulation that encourages both simplicity and expressiveness, leading to a
stable and interpretable Koopman representation. Beyond quantitative
evaluations, we further visualize the learned manifolds under our
representations, observing empirical results consistent with our theoretical
predictions. Finally, we validate our approach across a diverse range of
dynamical systems, demonstrating improved performance over existing Koopman
learning methods. The implementation is publicly available at
https://github.com/Wenxuan52/InformationKoopman.

</details>


### [58] [Bridging Idealized and Operational Models: An Explainable AI Framework for Earth System Emulators](https://arxiv.org/abs/2510.13030)
*Pouria Behnoudfar,Charlotte Moser,Marc Bocquet,Sibo Cheng,Nan Chen*

Main category: cs.LG

TL;DR: 提出一个可解释的AI框架，将高分辨率的地球系统模型、以及稀疏输出的理想化模型通过重构潜在数据同化连接起来，以纠正 CMIP6 对厄尔尼诺的时空偏差，并实现物理洞察和不确定性定量。


<details>
  <summary>Details</summary>
Motivation: 现有高分辨率操作模型在极端事件和统计分布上存在系统性偏差；理想化/低维模型虽能精准刻画特定过程，但各自领域内孤立，缺乏跨层级整合。需要一个跨模型层级、可解释的AI框架来吸收不同复杂度模型的优点，提升全球尺度的准确性与可解释性。

Method: 开发一个可解释的地球系统仿真器框架，通过重构潜在数据同化机制，将理想化模型的稀疏输出来引导并桥接到高分辨率操作模型，形成桥接模型，兼具高分辨率变量覆盖与理想化模型的统计优势，同时提供机理层面的解释。

Result: 在 CMIP6 的厄尔尼诺时空模式偏差上取得显著改进，利用理想化模型的统计正确性提升全球预测的准确性与一致性。

Conclusion: 强调继续发展理想化模型并推动跨组沟通，提出以可解释AI驱动的数字孪生与不确定性量化为核心的新路径。

Abstract: Computer models are indispensable tools for understanding the Earth system.
While high-resolution operational models have achieved many successes, they
exhibit persistent biases, particularly in simulating extreme events and
statistical distributions. In contrast, coarse-grained idealized models isolate
fundamental processes and can be precisely calibrated to excel in
characterizing specific dynamical and statistical features. However, different
models remain siloed by disciplinary boundaries. By leveraging the
complementary strengths of models of varying complexity, we develop an
explainable AI framework for Earth system emulators. It bridges the model
hierarchy through a reconfigured latent data assimilation technique, uniquely
suited to exploit the sparse output from the idealized models. The resulting
bridging model inherits the high resolution and comprehensive variables of
operational models while achieving global accuracy enhancements through
targeted improvements from idealized models. Crucially, the mechanism of AI
provides a clear rationale for these advancements, moving beyond black-box
correction to physically insightful understanding in a computationally
efficient framework that enables effective physics-assisted digital twins and
uncertainty quantification. We demonstrate its power by significantly
correcting biases in CMIP6 simulations of El Ni\~no spatiotemporal patterns,
leveraging statistically accurate idealized models. This work also highlights
the importance of pushing idealized model development and advancing
communication between modeling communities.

</details>


### [59] [An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting](https://arxiv.org/abs/2510.13050)
*Shreya Agrawal,Mohammed Alewi Hassen,Emmanuel Asiedu Brempong,Boris Babenko,Fred Zyda,Olivia Graham,Di Li,Samier Merchant,Santiago Hincapie Potes,Tyler Russell,Danny Cheresnick,Aditya Prakash Kakkirala,Stephan Rasp,Avinatan Hassidim,Yossi Matias,Nal Kalchbrenner,Pramod Gupta,Jason Hickey,Aaron Bell*

Main category: cs.LG

TL;DR: Global MetNet is an operational global machine-learning-based precipitation nowcasting system that forecasts ~5 km, 15-minute resolution up to 12 hours ahead, outperforming standard hourly forecasts, and is deployed globally (millions of users), enabling real-time applications with lower latency.


<details>
  <summary>Details</summary>
Motivation: Traditional NWP methods have high latency and coarse resolution; ML-based nowcasting is common in the Global North but poorly applicable to the Global South due to sparse radar; there is a need to reduce global disparities in forecast quality and leverage satellite data for sparse regions.

Method: Global MetNet combines the CORRA dataset from GPM, geostationary satellite data, and global NWP data to generate precipitation forecasts for the next 12 hours at ~0.05 degree (~5 km) spatial resolution and 15-minute temporal resolution, operating globally as an ML-based nowcasting model.

Result: The model significantly outperforms industry-standard hourly forecasts across key metrics (e.g., critical success index and fractions skill score) for all precipitation rates and lead times; it performs better in data-sparse regions than the best high-resolution NWP models in the US; forecasts are generated in under a minute and validated against ground radar and satellite data; deployed for millions of users via Google Search.

Conclusion: Global MetNet represents a key step toward reducing global disparities in forecast quality by integrating sparse, high-resolution satellite observations into weather forecasting and enabling real-time, global nowcasting.

Abstract: Precipitation nowcasting, which predicts rainfall up to a few hours ahead, is
a critical tool for vulnerable communities in the Global South frequently
exposed to intense, rapidly developing storms. Timely forecasts provide a
crucial window to protect lives and livelihoods. Traditional numerical weather
prediction (NWP) methods suffer from high latency, low spatial and temporal
resolution, and significant gaps in accuracy across the world. Recent machine
learning-based nowcasting methods, common in the Global North, cannot be
extended to the Global South due to extremely sparse radar coverage. We present
Global MetNet, an operational global machine learning nowcasting model. It
leverages the Global Precipitation Mission's CORRA dataset, geostationary
satellite data, and global NWP data to predict precipitation for the next 12
hours. The model operates at a high resolution of approximately 0.05{\deg}
(~5km) spatially and 15 minutes temporally. Global MetNet significantly
outperforms industry-standard hourly forecasts and achieves significantly
higher skill, making forecasts useful over a much larger area of the world than
previously available. Our model demonstrates better skill in data-sparse
regions than even the best high-resolution NWP models achieve in the US.
Validated using ground radar and satellite data, it shows significant
improvements across key metrics like the critical success index and fractions
skill score for all precipitation rates and lead times. Crucially, our model
generates forecasts in under a minute, making it readily deployable for
real-time applications. It is already deployed for millions of users on Google
Search. This work represents a key step in reducing global disparities in
forecast quality and integrating sparse, high-resolution satellite observations
into weather forecasting.

</details>


### [60] [Generalist++: A Meta-learning Framework for Mitigating Trade-off in Adversarial Training](https://arxiv.org/abs/2510.13361)
*Yisen Wang,Yichuan Mo,Hongjun Wang,Junyi Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出Generalist框架，通过将总体泛化目标分解为多个子任务并分配给基学习器，最后通过插值合并成全局学习器，同时周期性地将全局参数重新分布回基学习器，以提升对抗训练中的鲁棒性与泛化性，缓解自然准确率下降和跨攻击的鲁棒性迁移问题。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练虽有效但在自然准确性下降和不同范数攻击的鲁棒性迁移方面存在局限，亟需将泛化目标分解为多任务并实现协同提升。

Method: 将泛化目标分解成多个子任务，分配给专门的基学习器；每个基学习器在各自目标上成为专家；训练阶段通过插值将它们的参数组合成一个可用的全局学习器；周期性地将全局参数重新分布回基学习器以防止优化漂移；提出三种变体以适应不同应用场景。

Result: 理论分析和大量实验表明，与基线方法相比，Generalist可以实现更低的泛化误差并显著缓解权衡问题。

Conclusion: Generalist为构建更鲁棒的分类器提供了一个有希望的方向，未来可能实现更全面的鲁棒性。

Abstract: Despite the rapid progress of neural networks, they remain highly vulnerable
to adversarial examples, for which adversarial training (AT) is currently the
most effective defense. While AT has been extensively studied, its practical
applications expose two major limitations: natural accuracy tends to degrade
significantly compared with standard training, and robustness does not transfer
well across attacks crafted under different norm constraints. Unlike prior
works that attempt to address only one issue within a single network, we
propose to partition the overall generalization goal into multiple sub-tasks,
each assigned to a dedicated base learner. By specializing in its designated
objective, each base learner quickly becomes an expert in its field. In the
later stages of training, we interpolate their parameters to form a
knowledgeable global learner, while periodically redistributing the global
parameters back to the base learners to prevent their optimization trajectories
from drifting too far from the shared target. We term this framework Generalist
and introduce three variants tailored to different application scenarios. Both
theoretical analysis and extensive experiments demonstrate that Generalist
achieves lower generalization error and significantly alleviates the trade-off
problems compared with baseline methods. Our results suggest that Generalist
provides a promising step toward developing fully robust classifiers in the
future.

</details>


### [61] [Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games](https://arxiv.org/abs/2510.13060)
*Anupam Nayak,Tong Yang,Osman Yagan,Gauri Joshi,Yuejie Chi*

Main category: cs.LG

TL;DR: KL正则化在对抗性博弈中的样本效率提升：提出OMG（矩阵博弈）与SOMG（马尔可夫博弈）两种基于最优反应采样的算法，结合乐观奖励，得到关于T的对数级别的 regrets，并且对正则化强度β的倒数有依赖性地提升，同时保持标准的无β独立的sqrt(T) regret。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，基于固定参考策略的反向KL正则化被广泛用于保持参考策略的期望特性并促进探索；在对齐研究中，KL正则化与预训练语言模型的参考策略已在自对弈中取得经验性成功，但理论上对其在博弈设置中的收益尚不清楚，需要给出能解释样本效率提升的理论分析。

Method: 针对矩阵博弈提出OMG：基于最优反应采样并引入乐观奖金。将其扩展到马尔可夫博弈得到SOMG：同样使用最优反应采样，并引入一种新概念“超乐观奖金”。

Result: 两类问题均得到对数阶时间Horizon T的regret界，并且该界面对KL正则化强度β的倒数成正比地提升，同时仍保留在正则化与非正则化设置下的标准的无β独立的˜O(√T) regret上界。

Conclusion: 理论分析证明在博弈-强化学习框架中，KL正则化确实能提升样本效率，上述两种算法为在含参考策略的情况下实现更高效学习提供了量化的保证，并将正则化理论与对齐中的预训练模型结合的方向做了系统性的理论铺垫。

Abstract: Reverse Kullback-Leibler (KL) divergence-based regularization with respect to
a fixed reference policy is widely used in modern reinforcement learning to
preserve the desired traits of the reference policy and sometimes to promote
exploration (using uniform reference policy, known as entropy regularization).
Beyond serving as a mere anchor, the reference policy can also be interpreted
as encoding prior knowledge about good actions in the environment. In the
context of alignment, recent game-theoretic approaches have leveraged KL
regularization with pretrained language models as reference policies, achieving
notable empirical success in self-play methods. Despite these advances, the
theoretical benefits of KL regularization in game-theoretic settings remain
poorly understood. In this work, we develop and analyze algorithms that
provably achieve improved sample efficiency under KL regularization. We study
both two-player zero-sum Matrix games and Markov games: for Matrix games, we
propose OMG, an algorithm based on best response sampling with optimistic
bonuses, and extend this idea to Markov games through the algorithm SOMG, which
also uses best response sampling and a novel concept of superoptimistic
bonuses. Both algorithms achieve a logarithmic regret in $T$ that scales
inversely with the KL regularization strength $\beta$ in addition to the
standard $\widetilde{\mathcal{O}}(\sqrt{T})$ regret independent of $\beta$
which is attained in both regularized and unregularized settings

</details>


### [62] [Absolute indices for determining compactness, separability and number of clusters](https://arxiv.org/abs/2510.13065)
*Adil M. Bagirov,Ramiz M. Aliguliyev,Nargiz Sultanova,Sona Taheri*

Main category: cs.LG

TL;DR: 提出一组绝对的聚类有效性指标，用于同时衡量簇的紧凑性与簇间边界，以更准确地识别真实簇数，并与现有相对指标比较。


<details>
  <summary>Details</summary>
Motivation: 当前聚类有效性多为相对指标，依赖于任务与数据结构，且难以在不同数据集间给出一致的簇数判断，因此需要绝对的度量来稳定识别真实簇数。

Method: 提出紧凑性函数用于每个簇；为簇对定义邻域点集合，基于此计算簇间的边距；将紧凑性和边距合成为绝对的聚类有效性指标，从而判断簇的分布结构和真实簇数。

Result: 在若干合成数据与真实数据集上进行实验，并与常用的聚类有效性指标进行对比，显示新指标在识别真实簇数和描述分布特征方面具有优势。

Conclusion: 绝对聚类指标为簇的紧凑性与可分离性提供直接量化，提升对真实簇数的识别能力，未来工作可扩展到更多数据集和更广的应用场景。

Abstract: Finding "true" clusters in a data set is a challenging problem. Clustering
solutions obtained using different models and algorithms do not necessarily
provide compact and well-separated clusters or the optimal number of clusters.
Cluster validity indices are commonly applied to identify such clusters.
Nevertheless, these indices are typically relative, and they are used to
compare clustering algorithms or choose the parameters of a clustering
algorithm. Moreover, the success of these indices depends on the underlying
data structure. This paper introduces novel absolute cluster indices to
determine both the compactness and separability of clusters. We define a
compactness function for each cluster and a set of neighboring points for
cluster pairs. This function is utilized to determine the compactness of each
cluster and the whole cluster distribution. The set of neighboring points is
used to define the margin between clusters and the overall distribution margin.
The proposed compactness and separability indices are applied to identify the
true number of clusters. Using a number of synthetic and real-world data sets,
we demonstrate the performance of these new indices and compare them with other
widely-used cluster validity indices.

</details>


### [63] [NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models](https://arxiv.org/abs/2510.13068)
*Konstantinos Barmpas,Na Lee,Alexandros Koliousis,Yannis Panagakis,Dimitrios A. Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: NeuroRVQ introduces a scalable codebook-based tokenizer for EEG foundation models that preserves high-frequency dynamics, enabling better reconstruction and downstream performance.


<details>
  <summary>Details</summary>
Motivation: EEG signals span multiple temporal/spectral scales and contain high-frequency dynamics. Existing neural tokenizers fail to preserve these details, limiting reconstruction fidelity and downstream generalization.

Method: A NeuroRVQ tokenizer with (i) multi-scale feature extraction to capture the full frequency spectrum, (ii) hierarchical residual vector quantization (RVQ) codebooks for high-resolution encoding, and (iii) a phase- and amplitude-aware loss function. Integrated into a large brainwave model for efficient compression and accurate reconstruction across frequency bands, supporting robust masked generative modeling.

Result: Lower reconstruction error than existing LBMs and improved performance on various downstream tasks; demonstrates robust generative masked modeling and general-purpose applicability in brainwave modeling.

Conclusion: NeuroRVQ provides a strong codebook-based prior for brainwave models, enabling advances in neural decoding, generative modeling, and multimodal biosignal integration.

Abstract: Electroencephalography (EEG) captures neural activity across multiple
temporal and spectral scales, yielding signals that are rich but complex for
representation learning. Recently, EEG foundation models trained to predict
masked signal-tokens have shown promise for learning generalizable
representations. However, their performance is hindered by their signal
tokenization modules. Existing neural tokenizers fail to preserve
high-frequency dynamics, limiting their ability to reconstruct EEG signals with
high fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM)
centered on a codebook-based tokenizer. Our tokenizer integrates: (i)
multi-scale feature extraction modules that capture the full frequency neural
spectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks for
high-resolution encoding; and, (iii) an EEG signal phase- and amplitude-aware
loss function for efficient training. This design enables efficient EEG
compression while supporting accurate reconstruction across all frequency
bands, leading to robust generative masked modeling. Our empirical results
demonstrate that NeuroRVQ achieves lower reconstruction error and outperforms
existing LBMs on a variety of downstream tasks. More broadly, NeuroRVQ
tokenizer establishes a strong prior for codebook-based general-purpose
brainwave models, enabling advances in neural decoding, generative modeling and
multimodal biosignal integration.

</details>


### [64] [DeepCausalMMM: A Deep Learning Framework for Marketing Mix Modeling with Causal Inference](https://arxiv.org/abs/2510.13087)
*Aditya Puttaparthi Tirumala*

Main category: cs.LG

TL;DR: DeepCausalMMM 将 GRU 时序建模、DAG 学习和 Hill 饱和曲线结合，用以改进营销组合建模，具备数据驱动超参、跨区域建模、鲁棒性损失与丰富可视化的特性。


<details>
  <summary>Details</summary>
Motivation: 传统 MMM 多基于线性或分层贝叶斯模型，假设渠道独立，难以捕捉时序动态和非线性饱和效应，亟需一种结合深度学习、因果推断和营销科学的新方法来提升预测与预算优化。

Method: 使用门控循环单元(GRU) 自动学习广告滞后和携带效应(adstock)等时序模式；通过有向无环图(DAG)学习（如 NOTEARS）来学习渠道之间的因果结构；采用 Hill 方程的饱和曲线建模收益递减，并用于预算分配优化；具备数据驱动超参与变换、跨区域参数、鲁棒损失与正则化、全面响应曲线分析与 14+ 的交互式可视化仪表板。

Result: 原文摘要未给出实证结果，但明确提出通过 GRU 学习时序与广义因果结构学习来提升对渠道影响的捕捉、对饱和与携带效应的建模能力，以及完整的可视化与分析工具集，强调数据驱动的超参估计与多区域建模的优势。

Conclusion: 该工作提供了一整套结合时序、因果与非线性饱和的 MMM 框架，强调数据驱动、区域共享与个体差异、鲁棒性与可视化，用于改善营销效果估计与预算优化，并支持深入的响应曲线分析。

Abstract: Marketing Mix Modeling (MMM) is a statistical technique used to estimate the
impact of marketing activities on business outcomes such as sales, revenue, or
customer visits. Traditional MMM approaches often rely on linear regression or
Bayesian hierarchical models that assume independence between marketing
channels and struggle to capture complex temporal dynamics and non-linear
saturation effects [@Hanssens2005; @Ng2021Bayesian].
  DeepCausalMMM is a Python package that addresses these limitations by
combining deep learning, causal inference, and advanced marketing science. The
package uses Gated Recurrent Units (GRUs) to automatically learn temporal
patterns such as adstock (carryover effects) and lag, while simultaneously
learning statistical dependencies and potential causal structures between
marketing channels through Directed Acyclic Graph (DAG) learning
[@Zheng2018NOTEARS; @Gong2024CausalMMM]. Additionally, it implements Hill
equation-based saturation curves to model diminishing returns and optimize
budget allocation.
  Key innovations include: (1) a data-driven design where hyperparameters and
transformations (e.g., adstock decay, saturation curves) are learned or
estimated from data with sensible defaults, rather than requiring fixed
heuristics or manual specification, (2) multi-region modeling with both shared
and region-specific parameters, (3) robust statistical methods including Huber
loss and advanced regularization, (4) comprehensive response curve analysis for
understanding channel saturation, and (5) an extensive visualization suite with
14+ interactive dashboards for business insights.

</details>


### [65] [Neural Triangular Transport Maps: A New Approach Towards Sampling in Lattice QCD](https://arxiv.org/abs/2510.13112)
*Andrey Bryutkin,Youssef Marzouk*

Main category: cs.LG

TL;DR: 提出一种基于MRNN的稀疏三角传输映射框架，用以在格点场论中高效地采样 Boltzmann 分布，利用局部过去实现线性时间并行评估，并研究节点排序对稀疏性与性能的影响。


<details>
  <summary>Details</summary>
Motivation: 在格点场论中，采样 Boltzmann 分布常受多模态性和长程相关性困扰；常规归一化流在大格点上内存开销高且难以保持表达力。需要一种能有效利用格点图结构、实现可扩展性与高效并行的方法来改进采样。

Method: 提出稀疏三角传输映射，明确利用周期性边界条件下的格点图条件独立性；引入等价的完全稀疏与近似稀疏之间的权衡；将每个三角映射分量限定在局部过去以实现站点级并行计算、线性时间复杂度并保持可逆性；以二维 φ^4 理论作为受控评估场景，分析节点排序对稀疏性和性能的影响；并与混合蒙特卡洛（HMC）及 RealNVP 等流模型进行比较。

Result: 通过在 φ^4/2D 场景下的实验证据，展示节点排序对映射的稀疏性和性能有显著影响；框架实现了局部约束下的并行评估和线性时间复杂度，并在与 HMC、RealNVP 的对比中表现出有竞争力的采样效率与保持表达力的能力。

Conclusion: 框架提供了对 exact sparsity 与 approximate sparsity 之间的清晰权衡，能够在保持可逆和表达能力的前提下实现大尺度格点问题的高效采样，且通过节点排序等因素可以进一步优化稀疏性与性能，指明未来在排序策略与更广泛基准上的研究方向。

Abstract: Lattice field theories are fundamental testbeds for computational physics;
yet, sampling their Boltzmann distributions remains challenging due to
multimodality and long-range correlations. While normalizing flows offer a
promising alternative, their application to large lattices is often constrained
by prohibitive memory requirements and the challenge of maintaining sufficient
model expressivity. We propose sparse triangular transport maps that explicitly
exploit the conditional independence structure of the lattice graph under
periodic boundary conditions using monotone rectified neural networks (MRNN).
We introduce a comprehensive framework for triangular transport maps that
navigates the fundamental trade-off between \emph{exact sparsity} (respecting
marginal conditional independence in the target distribution) and
\emph{approximate sparsity} (computational tractability without fill-ins).
Restricting each triangular map component to a local past enables site-wise
parallel evaluation and linear time complexity in lattice size $N$, while
preserving the expressive, invertible structure. Using $\phi^4$ in two
dimensions as a controlled setting, we analyze how node labelings (orderings)
affect the sparsity and performance of triangular maps. We compare against
Hybrid Monte Carlo (HMC) and established flow approaches (RealNVP).

</details>


### [66] [On the Reasoning Abilities of Masked Diffusion Language Models](https://arxiv.org/abs/2510.13117)
*Anej Svete,Ashish Sabharwal*

Main category: cs.LG

TL;DR: MDMs的文本生成在有限精度对数宽度下与多项式加 padding 的循环/变换器等价，能够解决所有带CoT的变换器能够解决的问题，并在并行推理上对某些类别（如正规语言）比CoT更高效。


<details>
  <summary>Details</summary>
Motivation: 系统性地揭示MDMs在并行推理中的能力与极限，建立与Chain-of-Thought及Pad-PLT的联系，以明确其计算能力、效率边界和适用场景。

Method: 在有限精度对数宽度设定下，将MDMs与带多项式填充的PLTs进行理论等价性分析；证明MDMs等价于CoT增强的变换器在可解决的问题集合上，且对某些问题（包括正规语言）存在并行生成带来的效率优势。

Result: 给出MDMs与多项式填充的PLTs在该设定下的等价性，并证明MDMs能够解决所有CoT增强变换器能解决的问题；并指出在包含正规语言的类别中，MDMs由于并行化可实现比CoT变换器更高效的推理。

Conclusion: MDMs在并行生成框架下具有明确的计算优势和理论可解决性，与CoT与Pad-PLT框架紧密相关，为理解文本生成中并行推理的潜力提供了新维度。

Abstract: Masked diffusion models (MDMs) for text offer a compelling alternative to
traditional autoregressive language models. Parallel generation makes them
efficient, but their computational capabilities and the limitations inherent to
their parallelism remain largely unexplored. To this end, we characterize what
types of reasoning problems MDMs can provably solve and how efficiently. We do
this by connecting MDMs to the well-understood reasoning frameworks of chain of
thought (CoT) and padded looped transformers (PLTs) in the finite-precision
log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact,
equivalent in this setting, and that MDMs can solve all problems that
CoT-augmented transformers can. Moreover, we showcase classes of problems
(including regular languages) for which MDMs are inherently more efficient than
CoT transformers, where parallel generation allows for substantially faster
reasoning.

</details>


### [67] [Cluster-Based Client Selection for Dependent Multi-Task Federated Learning in Edge Computing](https://arxiv.org/abs/2510.13132)
*Jieping Luo,Qiyue Li,Zhizhang Liu,Hang Qi,Jiaying Yin,Jingjin Wu*

Main category: cs.LG

TL;DR: 在移动边缘计算场景下，提出 CoDa-FL，通过簇聚和依赖感知的客户端选择、以及基于地球搬运距离（EMD）的数据分布簇化和有向无环图的任务调度，以降低完成多任务学习的总时间、通信与计算成本，并在异质 MEC 设置下实现更快收敛和更高精度。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，面对数据分布异质性与任务依赖性，需优化客户端选择与任务调度以缩短完成多任务的总时间，特别是在资源受限的移动边缘计算环境中。

Method: 提出 CoDa-FL：1) 基于地球搬运距离（EMD）对客户端进行簇聚以反映本地数据分布；2) 推导簇内 EMD 与收敛轮数之间的直接关系，从而简化最优解的获取；3) 引入有向无环图（DAG）任务调度机制以有效管理任务依赖；4) 实现簇导向的客户端选择与依赖感知的任务分配。

Result: 数值实验表明，与现有基准相比，CoDa-FL 在异质 MEC 设置下实现更快的收敛、降低通信与计算成本、并提高学习精度。

Conclusion: CoDa-FL 在异质 MEC 场景中优于现有方法，能够更有效地处理数据分布异质性与任务依赖，提升联邦学习的总体效率。

Abstract: We study the client selection problem in Federated Learning (FL) within
mobile edge computing (MEC) environments, particularly under the dependent
multi-task settings, to reduce the total time required to complete various
learning tasks. We propose CoDa-FL, a Cluster-oriented and Dependency-aware
framework designed to reduce the total required time via cluster-based client
selection and dependent task assignment. Our approach considers Earth Mover's
Distance (EMD) for client clustering based on their local data distributions to
lower computational cost and improve communication efficiency. We derive a
direct and explicit relationship between intra-cluster EMD and the number of
training rounds required for convergence, thereby simplifying the otherwise
complex process of obtaining the optimal solution. Additionally, we incorporate
a directed acyclic graph-based task scheduling mechanism to effectively manage
task dependencies. Through numerical experiments, we validate that our proposed
CoDa-FL outperforms existing benchmarks by achieving faster convergence, lower
communication and computational costs, and higher learning accuracy under
heterogeneous MEC settings.

</details>


### [68] [Behavioral Embeddings of Programs: A Quasi-Dynamic Approach for Optimization Prediction](https://arxiv.org/abs/2510.13158)
*Haolin Pan,Jinyuan Dong,Hongbin Zhang,Hongyu Lin,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: 提出了一种准动态程序表示框架，通过对 IR 施加多种优化序列来探测程序的优化敏感性，生成程序行为光谱（Program Behavior Spectrum），再用产品量化将高维连续光谱离散为子词，使用多任务 Transformer（PQ-BERT）进行预训练以学习行为编码的深层语法，在两个编译优化任务上优于静态基线，并开源代码。


<details>
  <summary>Details</summary>
Motivation: 静态表示高效但对后续代码变换的行为预测能力有限；动态表示信息丰富但代价高且不可重复性强。需要一种兼具效率、确定性与对优化敏感性/行为变化的高信息量表示的折中方案。

Method: 提出 Program Behavior Spectrum，通过对程序的 IR 施加多样化的优化序列来探测并量化静态特征在不同优化下的变化，得到高维、连续的行为光谱。采用 Product Quantization 将该连续光谱离散成结构化的子词（子编码），再用一个多任务 Transformer 模型 PQ-BERT 对这些行为编码进行预训练，学习其深层上下文语法。

Result: 在两个代表性的编译优化任务（Best Pass Prediction 和 -Oz Benefit Prediction）上，该方法优于最先进的静态基线。

Conclusion: 提供了一种将准动态信息融入到高效、可重复的程序表示中的方法，证明通过行为光谱和量化/变换模型的结合，可以提升与编译优化相关的预测任务性能，并给出开源实现。

Abstract: Learning effective numerical representations, or embeddings, of programs is a
fundamental prerequisite for applying machine learning to automate and enhance
compiler optimization. Prevailing paradigms, however, present a dilemma. Static
representations, derived from source code or intermediate representation (IR),
are efficient and deterministic but offer limited insight into how a program
will behave or evolve under complex code transformations. Conversely, dynamic
representations, which rely on runtime profiling, provide profound insights
into performance bottlenecks but are often impractical for large-scale tasks
due to prohibitive overhead and inherent non-determinism. This paper transcends
this trade-off by proposing a novel quasi-dynamic framework for program
representation. The core insight is to model a program's optimization
sensitivity. We introduce the Program Behavior Spectrum, a new representation
generated by probing a program's IR with a diverse set of optimization
sequences and quantifying the resulting changes in its static features. To
effectively encode this high-dimensional, continuous spectrum, we pioneer a
compositional learning approach. Product Quantization is employed to discretize
the continuous reaction vectors into structured, compositional sub-words.
Subsequently, a multi-task Transformer model, termed PQ-BERT, is pre-trained to
learn the deep contextual grammar of these behavioral codes. Comprehensive
experiments on two representative compiler optimization tasks -- Best Pass
Prediction and -Oz Benefit Prediction -- demonstrate that our method
outperforms state-of-the-art static baselines. Our code is publicly available
at https://github.com/Panhaolin2001/PREP/.

</details>


### [69] [Information-Theoretic Criteria for Knowledge Distillation in Multimodal Learning](https://arxiv.org/abs/2510.13182)
*Rongrong Xie,Yizhou Xu,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 提出跨模态互补性假说（CCH）来解读跨模态蒸馏的有效性，基于互信息关系给出判断标准并通过高斯模型理论与多模态数据的实证验证，提供教师模态选择的实用指南。


<details>
  <summary>Details</summary>
Motivation: 在多模态数据日益丰富的背景下，尽管跨模态KD在多领域取得了进展，但理论理解不足，导致并非在所有情形下都能提升弱模态的性能，需要一个能解释并指导实践的理论框架。

Method: 提出CCH，给出判据MI(T,S) > MI(S,Y)表示教师模态与学生模态的互信息应大于学生模态与标签的互信息；在联接高斯模型中理论验证，并在图像、文本、视频、音频及癌症组学等多模态数据集上进行经验验证。

Result: CCH在理论上得到支持并在多模态数据集上得到实证验证，能够作为选择教师模态的可操作准则，从而提升弱模态的学习效果。

Conclusion: 建立了跨模态KD的理论框架，并给出基于CCH的实际指南，帮助设计更高效的跨模态蒸馏系统。

Abstract: The rapid increase in multimodal data availability has sparked significant
interest in cross-modal knowledge distillation (KD) techniques, where richer
"teacher" modalities transfer information to weaker "student" modalities during
model training to improve performance. However, despite successes across
various applications, cross-modal KD does not always result in improved
outcomes, primarily due to a limited theoretical understanding that could
inform practice. To address this gap, we introduce the Cross-modal
Complementarity Hypothesis (CCH): we propose that cross-modal KD is effective
when the mutual information between teacher and student representations exceeds
the mutual information between the student representation and the labels. We
theoretically validate the CCH in a joint Gaussian model and further confirm it
empirically across diverse multimodal datasets, including image, text, video,
audio, and cancer-related omics data. Our study establishes a novel theoretical
framework for understanding cross-modal KD and offers practical guidelines
based on the CCH criterion to select optimal teacher modalities for improving
the performance of weaker modalities.

</details>


### [70] [CleverCatch: A Knowledge-Guided Weak Supervision Model for Fraud Detection](https://arxiv.org/abs/2510.13205)
*Amirhossein Mozafari,Kourosh Hashemi,Erfan Shafagh,Soroush Motamedi,Azar Taheri Tayebi,Mohammad A. Tayebi*

Main category: cs.LG

TL;DR: 以知识引导的弱监督模型 CleverCatch 在医疗保健欺诈检测上取得提升，通过将专家规则嵌入神经嵌入空间实现更高AUC/召回率，并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决标注数据稀缺、欺诈行为持续演变以及医疗记录的高维性带来的挑战；在高风险领域寻求可解释的异常检测方法。

Method: CleverCatch 将领域规则以软嵌入的形式融入神经编码器框架；在代表合规与违规的合成数据上联合训练，学习对规则与样本在同一嵌入空间对齐的编码；采用知识引导的弱监督学习。

Result: 在大型真实数据集上优于四个最先进的异常检测基线，AUC 提升约1.3%、召回率提升约3.4%；消融实验证实专家规则的互补作用；模型更具透明度。

Conclusion: 将专家规则嵌入学习过程可提升检测准确性并提高可解释性，适用于医疗保健欺诈等高风险场景。

Abstract: Healthcare fraud detection remains a critical challenge due to limited
availability of labeled data, constantly evolving fraud tactics, and the high
dimensionality of medical records. Traditional supervised methods are
challenged by extreme label scarcity, while purely unsupervised approaches
often fail to capture clinically meaningful anomalies. In this work, we
introduce CleverCatch, a knowledge-guided weak supervision model designed to
detect fraudulent prescription behaviors with improved accuracy and
interpretability. Our approach integrates structured domain expertise into a
neural architecture that aligns rules and data samples within a shared
embedding space. By training encoders jointly on synthetic data representing
both compliance and violation, CleverCatch learns soft rule embeddings that
generalize to complex, real-world datasets. This hybrid design enables
data-driven learning to be enhanced by domain-informed constraints, bridging
the gap between expert heuristics and machine learning. Experiments on the
large-scale real-world dataset demonstrate that CleverCatch outperforms four
state-of-the-art anomaly detection baselines, yielding average improvements of
1.3\% in AUC and 3.4\% in recall. Our ablation study further highlights the
complementary role of expert rules, confirming the adaptability of the
framework. The results suggest that embedding expert rules into the learning
process not only improves detection accuracy but also increases transparency,
offering an interpretable approach for high-stakes domains such as healthcare
fraud detection.

</details>


### [71] [Performance Evaluation of Ising and QUBO Variable Encodings in Boltzmann Machine Learning](https://arxiv.org/abs/2510.13210)
*Yasushi Hasegawa,Masayuki Ohzeki*

Main category: cs.LG

TL;DR: Ising (-1,+1) 与 QUBO (0,1) 编码在玻尔兹曼机学习中导致信息几何的差异；Ising 导致更各向同性的曲率、SGD 收敛更快，而 QUBO 出现病态条件，需 NGD 或预处理来缓解


<details>
  <summary>Details</summary>
Motivation: 理解变量编码如何影响学习动力学与信息几何，以便在玻尔兹曼机中做出更好的编码与预处理选择

Method: 固定模型、采样器和步长，对比两种编码；通过可视化模型样本的经验矩，对比 Fisher 信息矩阵（FIM）等价于充分统计量协方差；分析特征值结构与谱熵，比较 SGD 与 NGD 的表现

Result: QUBO 诱导更大的第一/二阶统计量之间的跨项，导致 FIM 出现更多小特征值方向、谱熵降低，从而使 SGD 收敛较慢；NGD 通过使用 FIM 度量重新参数化，在两种编码下的收敛趋于相似；在 SGD 的情况下，Ising 编码具有更各向同性的曲率、并实现更快的收敛；对 QUBO，中心化/缩放或 NGD 预处理可缓解曲率问题

Conclusion: 编码表示方式会重塑信息几何和有限时间内的学习动力学；给出实际指南：选择合适的编码与预处理以提升学习效率，必要时采用 NGD 以抵消对数不对称的曲率病态。

Abstract: We compare Ising ({-1,+1}) and QUBO ({0,1}) encodings for Boltzmann machine
learning under a controlled protocol that fixes the model, sampler, and step
size. Exploiting the identity that the Fisher information matrix (FIM) equals
the covariance of sufficient statistics, we visualize empirical moments from
model samples and reveal systematic, representation-dependent differences. QUBO
induces larger cross terms between first- and second-order statistics, creating
more small-eigenvalue directions in the FIM and lowering spectral entropy. This
ill-conditioning explains slower convergence under stochastic gradient descent
(SGD). In contrast, natural gradient descent (NGD)-which rescales updates by
the FIM metric-achieves similar convergence across encodings due to
reparameterization invariance. Practically, for SGD-based training, the Ising
encoding provides more isotropic curvature and faster convergence; for QUBO,
centering/scaling or NGD-style preconditioning mitigates curvature pathologies.
These results clarify how representation shapes information geometry and
finite-time learning dynamics in Boltzmann machines and yield actionable
guidelines for variable encoding and preprocessing.

</details>


### [72] [Towards Understanding Valuable Preference Data for Large Language Model Alignment](https://arxiv.org/abs/2510.13212)
*Zizhuo Zhang,Qizhou Wang,Shanshan Ye,Jianing Zhu,Jiangchao Yao,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出截断影响函数（TIF）来评估偏好数据对单个模型的影响力，数据质量具有模型依赖性，并给出两种简单且与TIF相关的评分函数，结合以实现更高效的数据筛选，从而在同样或更少数据下实现更好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有偏好数据质量评估多依赖外部奖励模型或现成LLM，往往忽略单个数据点对特定模型的实际影响。本研究旨在对数据点的个体影响进行更准确的评估，并探究数据筛选应与具体模型适配。

Method: 提出截断影响函数（TIF）以降低过度评分的问题；提出两种更简单且模型相关的评分函数（SFs）来近似TIF；将两者结合以抵消各自的误差源，形成一个简单但有效的偏好数据选择规则；在多种对齐基准和多家LLM上进行实验。

Result: TIF减少了传统评估的过高估分，显示偏好数据质量确实依赖于模型；两种SF与TIF呈正相关，存在一定偏差，但通过组合可获得比单一SF更稳健的数据选择；在实验中使用更少的数据即可获得更好/更普遍的对齐性能。

Conclusion: 偏好数据质量是模型特定属性；提出的简单、模型依赖的评分策略可供偏好数据筛选使用，提升对齐效果并体现方法的普遍性。

Abstract: Large language model (LLM) alignment is typically achieved through learning
from human preference comparisons, making the quality of preference data
critical to its success. Existing studies often pre-process raw training
datasets to identify valuable preference pairs using external reward models or
off-the-shelf LLMs, achieving improved overall performance but rarely examining
whether individual, selected data point is genuinely beneficial. We assess data
quality through individual influence on validation data using our newly
proposed truncated influence function (TIF), which mitigates the over-scoring
present in traditional measures and reveals that preference data quality is
inherently a property of the model. In other words, a data pair that benefits
one model may harm another. This leaves the need to improve the preference data
selection approaches to be adapting to specific models. To this end, we
introduce two candidate scoring functions (SFs) that are computationally
simpler than TIF and positively correlated with it. They are also model
dependent and can serve as potential indicators of individual data quality for
preference data selection. Furthermore, we observe that these SFs inherently
exhibit errors when compared to TIF. To this end, we combine them to offset
their diverse error sources, resulting in a simple yet effective data selection
rule that enables the models to achieve a more precise selection of valuable
preference data. We conduct experiments across diverse alignment benchmarks and
various LLM families, with results demonstrating that better alignment
performance can be achieved using less data, showing the generality of our
findings and new methods.

</details>


### [73] [Hypernetworks for Perspectivist Adaptation](https://arxiv.org/abs/2510.13259)
*Daniil Ignatev,Denis Paperno,Massimo Poesio*

Main category: cs.LG

TL;DR: 提出一种参数高效的 perspectivist 分类方法，使用 hypernetwork+adapters，将其应用于不同基模型，能够在较少参数下与专门模型竞争，且具备架构无关性。


<details>
  <summary>Details</summary>
Motivation: perspective-aware classification 在不同用户视角下进行分类时存在显著的参数效率瓶颈，亟需更高效的参数利用与模型灵活性。

Method: 将 hypernetwork 与 adapters 的组合应用于 perspectivist 分类，并实现一个架构无关的解决方案，能够对任意基模型直接接入并使用。

Result: 该方案在采用用户视角进行 hate speech 与 toxicity 检测方面，与专门模型的性能相竞争，同时参数显著减少。

Conclusion: 该方法具备架构无关性，可广泛应用于各种基模型，便于在不同场景实现 perspective-aware 分类。

Abstract: The task of perspective-aware classification introduces a bottleneck in terms
of parametric efficiency that did not get enough recognition in existing
studies. In this article, we aim to address this issue by applying an existing
architecture, the hypernetwork+adapters combination, to perspectivist
classification. Ultimately, we arrive at a solution that can compete with
specialized models in adopting user perspectives on hate speech and toxicity
detection, while also making use of considerably fewer parameters. Our solution
is architecture-agnostic and can be applied to a wide range of base models out
of the box.

</details>


### [74] [BlendFL: Blended Federated Learning for Handling Multimodal Data Heterogeneity](https://arxiv.org/abs/2510.13266)
*Alejandro Guerra-Manzanares,Omar El-Herraoui,Michail Maniatakos,Farah E. Shamout*

Main category: cs.LG

TL;DR: BlendFL is a novel federated learning framework that blends horizontal and vertical FL to handle multimodal data heterogeneity across clients, supports decentralized inference, and uses an adaptive BlendAvg aggregation to prioritize better-performing clients.


<details>
  <summary>Details</summary>
Motivation: Real-world collaborative machine learning faces multimodal data heterogeneity and privacy constraints. Traditional FL (horizontal/vertical) assumes data distributions that do not hold in practice, especially when not all modalities or samples are present at every client.

Method: BlendFL allows each client to leverage horizontal and/or vertical FL depending on its available data in a synchronized, non-restrictive manner. It introduces a decentralized inference mechanism for on-device collaborative inference and proposes BlendAvg, an adaptive global aggregation strategy that weighs client updates by performance.

Result: Empirical evaluation on a large multimodal real-world medical dataset and a standard multimodal benchmark shows BlendFL achieves superior performance for both multimodal and unimodal classification. Ablation studies indicate faster convergence compared to traditional FL methods.

Conclusion: BlendFL effectively handles multimodal data heterogeneity in privacy-preserving collaborative learning settings and shows promise for real-world applications in healthcare and finance, with faster convergence and improved accuracy.

Abstract: One of the key challenges of collaborative machine learning, without data
sharing, is multimodal data heterogeneity in real-world settings. While
Federated Learning (FL) enables model training across multiple clients,
existing frameworks, such as horizontal and vertical FL, are only effective in
`ideal' settings that meet specific assumptions. Hence, they struggle to
address scenarios where neither all modalities nor all samples are represented
across the participating clients. To address this gap, we propose BlendFL, a
novel FL framework that seamlessly blends the principles of horizontal and
vertical FL in a synchronized and non-restrictive fashion despite the asymmetry
across clients. Specifically, any client within BlendFL can benefit from either
of the approaches, or both simultaneously, according to its available dataset.
In addition, BlendFL features a decentralized inference mechanism, empowering
clients to run collaboratively trained local models using available local data,
thereby reducing latency and reliance on central servers for inference. We also
introduce BlendAvg, an adaptive global model aggregation strategy that
prioritizes collaborative model updates based on each client's performance. We
trained and evaluated BlendFL and other state-of-the-art baselines on three
classification tasks using a large-scale real-world multimodal medical dataset
and a popular multimodal benchmark. Our results highlight BlendFL's superior
performance for both multimodal and unimodal classification. Ablation studies
demonstrate BlendFL's faster convergence compared to traditional approaches,
accelerating collaborative learning. Overall, in our study we highlight the
potential of BlendFL for handling multimodal data heterogeneity for
collaborative learning in real-world settings where data privacy is crucial,
such as in healthcare and finance.

</details>


### [75] [To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models](https://arxiv.org/abs/2510.13290)
*Anna Hedström,Salim I. Amoukou,Tom Bewley,Saumitra Mishra,Manuela Veloso*

Main category: cs.LG

TL;DR: MERA (Mechanistic Error Reduction with Abstention) is a principled framework that steers language models via selective, adaptive interventions. It optimizes both the direction of intervention and when/how much to steer, enabling improved performance or abstention when correction is not confidently possible; it can be stacked on top of existing steering methods as a general, efficient approach.


<details>
  <summary>Details</summary>
Motivation: Existing steering techniques use fixed, manually tuned strengths that often lead to under- or over-steering. There is a need for a principled method to (i) choose the intervention direction and (ii) calibrate when and how much to steer, with the ability to abstain when corrections are not confident.

Method: Propose MERA as a framework that optimizes the intervention direction and calibrates steering magnitude and abstention policy. It is validated across diverse datasets and LM families, showing safe, effective, non-degrading error correction. MERA can be composed with existing steering techniques to further enhance performance.

Result: MERA achieves safer and more effective error correction, outperforming baselines across datasets and LM families, while not degrading performance. It also serves as a plug-in to augment existing steering methods, demonstrating strong generalization.

Conclusion: MERA is a general-purpose, efficient approach to mechanistic activation steering that can be layered on top of current steering techniques to enhance their performance and enable principled, adaptive abstention when corrections are uncertain.

Abstract: We introduce Mechanistic Error Reduction with Abstention (MERA), a principled
framework for steering language models (LMs) to mitigate errors through
selective, adaptive interventions. Unlike existing methods that rely on fixed,
manually tuned steering strengths, often resulting in under or oversteering,
MERA addresses these limitations by (i) optimising the intervention direction,
and (ii) calibrating when, and how much to steer, thereby provably improving
performance or abstaining when no confident correction is possible. Experiments
across diverse datasets, and LM families demonstrate safe, effective,
non-degrading error correction, and that MERA outperforms existing baselines.
Moreover, MERA can be applied on top of existing steering techniques to further
enhance their performance, establishing it as a general-purpose, and efficient
approach to mechanistic activation steering.

</details>


### [76] [Federated Conditional Conformal Prediction via Generative Models](https://arxiv.org/abs/2510.13297)
*Rui Xu,Sihong Xie*

Main category: cs.LG

TL;DR: 提出Fed-CCP，通过生成模型在联邦设置实现条件覆盖的预测集合，以适应本地数据异质性并维持全局一致性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中客户端分布差异导致标准Conformal Prediction的输入条件覆盖弱化，需要条件化的可靠性评估以反映本地不确定性。

Method: 利用生成模型（如流模型或扩散模型）近似条件数据分布，允许各客户端本地校准分数以实现条件覆盖；通过联邦聚合在不共享原始数据的前提下保持全局一致性。

Result: 在真实数据集上实验表明Fed-CCP实现更具自适应性的预测集合，能够对不同输入更准确地控制覆盖。

Conclusion: Fed-CCP为联邦学习中的条件覆盖提供一种可扩展且隐私友好的解决方案，能够结合本地数据异质性实现更具适应性的预测集。

Abstract: Conformal Prediction (CP) provides distribution-free uncertainty
quantification by constructing prediction sets that guarantee coverage of the
true labels. This reliability makes CP valuable for high-stakes federated
learning scenarios such as multi-center healthcare. However, standard CP
assumes i.i.d. data, which is violated in federated settings where client
distributions differ substantially. Existing federated CP methods address this
by maintaining marginal coverage on each client, but such guarantees often fail
to reflect input-conditional uncertainty. In this work, we propose Federated
Conditional Conformal Prediction (Fed-CCP) via generative models, which aims
for conditional coverage that adapts to local data heterogeneity. Fed-CCP
leverages generative models, such as normalizing flows or diffusion models, to
approximate conditional data distributions without requiring the sharing of raw
data. This enables each client to locally calibrate conformal scores that
reflect its unique uncertainty, while preserving global consistency through
federated aggregation. Experiments on real datasets demonstrate that Fed-CCP
achieves more adaptive prediction sets.

</details>


### [77] [Km-scale dynamical downscaling through conformalized latent diffusion models](https://arxiv.org/abs/2510.13301)
*Alessandro Brusaferri,Andrea Ballarino*

Main category: cs.LG

TL;DR: 用扩散模型进行高分辨率降尺度，并通过共形化分位回归得到局部自适应、有限样本有效的预测区间，在ERA5-意大利区域的2-km降尺度中提升网格点层面的不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型用于降尺度时缺乏有限样本保证的置信区间，导致网格点级不确定性估计易过度自信，影响在实际运营中的可信度。因此需要在生成模型基础上加入统计校准以获得可靠的概率预测。

Method: 先用扩散模型生成降尺度样本，再对样本进行条件分位估计，并结合共形化分位回归，得到局部自适应的预测区间，且具有有限样本的边际有效性。

Result: 在ERA5数据、意大利区域、降尺度至2公里网格的实验中，网格点级的不确定性覆盖率显著改善，概率分数稳定性增强，相较DM基线更具可信度。

Conclusion: 将共形化预测与生成降尺度相结合，能提升高分辨率气象场降尺度的可信度和可用性，具备实际应用潜力与扩展性。

Abstract: Dynamical downscaling is crucial for deriving high-resolution meteorological
fields from coarse-scale simulations, enabling detailed analysis for critical
applications such as weather forecasting and renewable energy modeling.
Generative Diffusion models (DMs) have recently emerged as powerful data-driven
tools for this task, offering reconstruction fidelity and more scalable
sampling supporting uncertainty quantification. However, DMs lack finite-sample
guarantees against overconfident predictions, resulting in miscalibrated
grid-point-level uncertainty estimates hindering their reliability in
operational contexts. In this work, we tackle this issue by augmenting the
downscaling pipeline with a conformal prediction framework. Specifically, the
DM's samples are post-processed to derive conditional quantile estimates,
incorporated into a conformalized quantile regression procedure targeting
locally adaptive prediction intervals with finite-sample marginal validity. The
proposed approach is evaluated on ERA5 reanalysis data over Italy, downscaled
to a 2-km grid. Results demonstrate grid-point-level uncertainty estimates with
markedly improved coverage and stable probabilistic scores relative to the DM
baseline, highlighting the potential of conformalized generative models for
more trustworthy probabilistic downscaling to high-resolution meteorological
fields.

</details>


### [78] [RockNet: Distributed Learning on Ultra-Low-Power Devices](https://arxiv.org/abs/2510.13320)
*Alexander Gräfe,Fabian Mager,Marco Zimmerling,Sebastian Trimpe*

Main category: cs.LG

TL;DR: RockNet is a distributed TinyML framework that enables on-device training on ultra-low-power microcontrollers for timeseries classification, achieving state-of-the-art accuracy without offline pretraining and demonstrating up to 2x accuracy improvements and up to 90% reductions in memory, latency, and energy as the number of devices scales to 20.


<details>
  <summary>Details</summary>
Motivation: As CPS adopt ML, training on resource-constrained devices faces privacy, latency, and connectivity challenges. Cloud-based/offline pretraining is often impractical for ultra-low-power systems that require on-device learning and fast adaptation across multiple devices.

Method: RockNet designs a distributed learning architecture across multiple CPS devices, using specialized compute-efficient classifiers and minimal communication for parallel training. It couples distributed ML with tailored wireless multi-hop protocols to mitigate communication bottlenecks, and supports training from scratch without offline pretraining.

Result: Hardware experiments on a 20-device testbed show RockNet achieves state-of-the-art accuracy for neural-network microcontroller training, surpassing the latest methods by up to 2x. The distributed setup reduces per-device memory, latency, and energy by up to 90% when scaling from 1 to 20 devices.

Conclusion: A tight integration of distributed ML, distributed computing, and communication makes on-device training feasible on ultra-low-power hardware with state-of-the-art accuracy, enabling privacy-preserving and low-latency CPS applications.

Abstract: As Machine Learning (ML) becomes integral to Cyber-Physical Systems (CPS),
there is growing interest in shifting training from traditional cloud-based to
on-device processing (TinyML), for example, due to privacy and latency
concerns. However, CPS often comprise ultra-low-power microcontrollers, whose
limited compute resources make training challenging. This paper presents
RockNet, a new TinyML method tailored for ultra-low-power hardware that
achieves state-of-the-art accuracy in timeseries classification, such as fault
or malware detection, without requiring offline pretraining. By leveraging that
CPS consist of multiple devices, we design a distributed learning method that
integrates ML and wireless communication. RockNet leverages all devices for
distributed training of specialized compute efficient classifiers that need
minimal communication overhead for parallelization. Combined with tailored and
efficient wireless multi-hop communication protocols, our approach overcomes
the communication bottleneck that often occurs in distributed learning.
Hardware experiments on a testbed with 20 ultra-low-power devices demonstrate
RockNet's effectiveness. It successfully learns timeseries classification tasks
from scratch, surpassing the accuracy of the latest approach for neural network
microcontroller training by up to 2x. RockNet's distributed ML architecture
reduces memory, latency and energy consumption per device by up to 90 % when
scaling from one central device to 20 devices. Our results show that a tight
integration of distributed ML, distributed computing, and communication
enables, for the first time, training on ultra-low-power hardware with
state-of-the-art accuracy.

</details>


### [79] [When In Doubt, Abstain: The Impact of Abstention on Strategic Classification](https://arxiv.org/abs/2510.13327)
*Lina Alkarmi,Ziyuan Huang,Mingyan Liu*

Main category: cs.LG

TL;DR: Abstention in strategic binary classification improves robustness and can deter manipulation; with optimal abstention, the principal's utility is not worse than in non-abstention settings.


<details>
  <summary>Details</summary>
Motivation: Algorithmic decision systems are vulnerable to strategic manipulation by agents seeking favorable outcomes. This work investigates abstention as a tool to mitigate manipulation within a Stackelberg framework.

Method: Binary-classifier setting where the principal (classifier) commits to an abstention policy; followers manipulate observable features to influence outcomes; analysis of how abstention affects agents' incentives and the principal's utility.

Result: Optimal abstention guarantees the principal's utility (or loss) is no worse than in a non-abstention setting. Abstention can deter manipulation, increasing the cost of manipulation for agents, especially for those less qualified, when manipulation costs are sufficiently high.

Conclusion: Abstention is a valuable mechanism to reduce adverse effects of strategic behavior in algorithmic decision making and should be considered in policy design to maintain performance and robustness.

Abstract: Algorithmic decision making is increasingly prevalent, but often vulnerable
to strategic manipulation by agents seeking a favorable outcome. Prior research
has shown that classifier abstention (allowing a classifier to decline making a
decision due to insufficient confidence) can significantly increase classifier
accuracy. This paper studies abstention within a strategic classification
context, exploring how its introduction impacts strategic agents' responses and
how principals should optimally leverage it. We model this interaction as a
Stackelberg game where a principal, acting as the classifier, first announces
its decision policy, and then strategic agents, acting as followers, manipulate
their features to receive a desired outcome. Here, we focus on binary
classifiers where agents manipulate observable features rather than their true
features, and show that optimal abstention ensures that the principal's utility
(or loss) is no worse than in a non-abstention setting, even in the presence of
strategic agents. We also show that beyond improving accuracy, abstention can
also serve as a deterrent to manipulation, making it costlier for agents,
especially those less qualified, to manipulate to achieve a positive outcome
when manipulation costs are significant enough to affect agent behavior. These
results highlight abstention as a valuable tool for reducing the negative
effects of strategic behavior in algorithmic decision making systems.

</details>


### [80] [Thompson Sampling via Fine-Tuning of LLMs](https://arxiv.org/abs/2510.13328)
*Nicolas Menet,Aleksandar Terzić,Andreas Krause,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出一种基于 Thompson 采样的高效策略 ToSFiT，用于大离散空间中的贝叶斯优化，通过直接对候选项达到最大回报的概率进行参数化，并利用在线微调的提示语言模型来近似后验分布，理论上给出对数变分 Thompson 采样的 regret bound，并在 FAQ、蛋白质稳定性搜索、量子电路设计等任务上展现样本效率提升且计算开销基本不变。


<details>
  <summary>Details</summary>
Motivation: 在无梯度的大离散空间中，最大化采集函数成本高，传统方法难以扩展；需要一种可扩展且可结合先验知识的方法。

Method: 直接对候选项成为最大回报的概率进行参数化的 Thompson Sampling；通过提示条件化的大语言模型的先验知识，增量地将其调整为后验；提出变分式 Thompson Sampling 的新 regret bound；ToSFiT 的关键在于对最大化后验概率的细致自适应。

Result: 在三个任务上验证，在线微调显著提升样本效率，计算效率几乎不变；

Conclusion: ToSFiT 结合理论保证与经验优势，是处理大规模离散搜索问题的可行方案，尤其当可获取强先验知识的语言模型时。

Abstract: Bayesian optimization in large unstructured discrete spaces is often hindered
by the computational cost of maximizing acquisition functions due to the
absence of gradients. We propose a scalable alternative based on Thompson
sampling that eliminates the need for acquisition function maximization by
directly parameterizing the probability that a candidate yields the maximum
reward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the
prior knowledge embedded in prompt-conditioned large language models, and
incrementally adapts them toward the posterior. Theoretically, we derive a
novel regret bound for a variational formulation of Thompson Sampling that
matches the strong guarantees of its standard counterpart. Our analysis reveals
the critical role of careful adaptation to the posterior probability of
maximality--a principle that underpins our ToSFiT algorithm. Empirically, we
validate our method on three diverse tasks: FAQ response refinement, thermally
stable protein search, and quantum circuit design. We demonstrate that online
fine-tuning significantly improves sample efficiency, with negligible impact on
computational efficiency.

</details>


### [81] [A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control](https://arxiv.org/abs/2510.13367)
*Nikita Kachaev,Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovelev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Transformers can be strong baselines for online model-free RL if design choices are carefully made, including input conditioning, actor-critic sharing, and sequential data processing, with stable training across vector and image, fully and partially observable tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers are powerful but underexplored in online model-free RL due to sensitivity to training setups and architectural decisions; the work seeks practical guidelines to harness their potential for continuous control.

Method: Systematically study key design questions in online model-free RL with transformers: how to condition inputs, how to share components between actor and critic, and how to slice sequential data for training; evaluate across fully and partially observable tasks and both vector- and image-based settings.

Result: Identifies stable architectural and training strategies that enable competitive performance of transformer-based agents in online RL across different settings.

Conclusion: Transformers are viable baselines for online model-free RL when appropriate design choices are made; provides concrete guidance for practitioners.

Abstract: Despite their effectiveness and popularity in offline or model-based
reinforcement learning (RL), transformers remain underexplored in online
model-free RL due to their sensitivity to training setups and model design
decisions such as how to structure the policy and value networks, share
components, or handle temporal information. In this paper, we show that
transformers can be strong baselines for continuous control in online
model-free RL. We investigate key design questions: how to condition inputs,
share components between actor and critic, and slice sequential data for
training. Our experiments reveal stable architectural and training strategies
enabling competitive performance across fully and partially observable tasks,
and in both vector- and image-based settings. These findings offer practical
guidance for applying transformers in online RL.

</details>


### [82] [Contrastive Learning-Based Dependency Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2510.13368)
*Yue Xing,Yingnan Deng,Heyao Liu,Ming Wang,Yun Zi,Xiaoxuan Sun*

Main category: cs.LG

TL;DR: 提出一种将依赖建模与对比学习结合的云服务异常检测框架，通过图嵌入和图卷积提取时序/结构特征，利用对比学习与时间一致性约束实现稳健检测，在公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在云服务环境中，复杂依赖关系和多样异常模式导致检测困难，需要同时建模服务间的依赖和时间演化，并具备鲁棒性以应对噪声和少量标注场景。

Method: 将服务交互抽象为依赖图，利用嵌入函数提取时序和结构特征，使用图卷积聚合邻域信息，构建正负样本对进行对比学习，设计时间一致性约束以抑制短期波动，联合对比损失与时间一致性损失进行优化。

Result: 在公开数据集上从超参数、环境和数据敏感性方面进行系统评估，显示在精确率、召回率、F1、AUC等关键指标上显著优于现有方法，且在标注稀缺、监控噪声和流量波动条件下保持鲁棒性。

Conclusion: 证明将依赖建模与对比学习整合的有效性，提供完整的云服务异常检测技术方案，展示在复杂环境中的适应性和稳定性。

Abstract: This paper addresses the challenges of complex dependencies and diverse
anomaly patterns in cloud service environments by proposing a dependency
modeling and anomaly detection method that integrates contrastive learning. The
method abstracts service interactions into a dependency graph, extracts
temporal and structural features through embedding functions, and employs a
graph convolution mechanism to aggregate neighborhood information for
context-aware service representations. A contrastive learning framework is then
introduced, constructing positive and negative sample pairs to enhance the
separability of normal and abnormal patterns in the representation space.
Furthermore, a temporal consistency constraint is designed to maintain
representation stability across time steps and reduce the impact of short-term
fluctuations and noise. The overall optimization combines contrastive loss and
temporal consistency loss to ensure stable and reliable detection across
multi-dimensional features. Experiments on public datasets systematically
evaluate the method from hyperparameter, environmental, and data sensitivity
perspectives. Results show that the proposed approach significantly outperforms
existing methods on key metrics such as Precision, Recall, F1-Score, and AUC,
while maintaining robustness under conditions of sparse labeling, monitoring
noise, and traffic fluctuations. This study verifies the effectiveness of
integrating dependency modeling with contrastive learning, provides a complete
technical solution for cloud service anomaly detection, and demonstrates strong
adaptability and stability in complex environments.

</details>


### [83] [Assessing the robustness of heterogeneous treatment effects in survival analysis under informative censoring](https://arxiv.org/abs/2510.13397)
*Yuxin Wang,Dennis Frauen,Jonas Schweisthal,Maresa Schröder,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出一个假设较少的（assumption-lean）框架，在生存分析中面对信息性删失时，对条件平均治疗效应（CATE）进行鲁棒性评估，使用部分识别来给出CATE的有信息界限。并引入一种元学习器，基于任意机器学习模型估计界限，具有双鲁棒性和准oracle效率。通过数值实验和癌症药物试验的应用验证其价值。


<details>
  <summary>Details</summary>
Motivation: 临床研究中高比例的退出可能与生存时间相关，导致信息性删失，从而使治疗效应估计偏倚。现有方法往往依赖强假设（如非信息性删失）来获得点估计，因此需要更鲁棒、假设需求更少的框架。

Method: 提出一种假设较少的框架，使用部分识别来对CATE给出界限，而非强假设下的点估计。开发一种新颖的元学习器，用以估计这些界限，允许使用任意机器学习模型，并具备双鲁棒性和准oracle效率等理论性质。通过并行/多模型的估计策略实现界限的估计。

Result: 界限能够标识在存在信息性删失时仍可能有效治疗的患者子组，提供比点估计更稳健的结论。数值实验及在癌症药物试验中的应用证明了该元学习器的实用性与理论特性。

Conclusion: 该框架为在存在删失时评估治疗效应的鲁棒性提供了实用工具，促进生存数据在医学和流行病学中的证据生成。

Abstract: Dropout is common in clinical studies, with up to half of patients leaving
early due to side effects or other reasons. When dropout is informative (i.e.,
dependent on survival time), it introduces censoring bias, because of which
treatment effect estimates are also biased. In this paper, we propose an
assumption-lean framework to assess the robustness of conditional average
treatment effect (CATE) estimates in survival analysis when facing censoring
bias. Unlike existing works that rely on strong assumptions, such as
non-informative censoring, to obtain point estimation, we use partial
identification to derive informative bounds on the CATE. Thereby, our framework
helps to identify patient subgroups where treatment is effective despite
informative censoring. We further develop a novel meta-learner that estimates
the bounds using arbitrary machine learning models and with favorable
theoretical properties, including double robustness and quasi-oracle
efficiency. We demonstrate the practical value of our meta-learner through
numerical experiments and in an application to a cancer drug trial. Together,
our framework offers a practical tool for assessing the robustness of estimated
treatment effects in the presence of censoring and thus promotes the reliable
use of survival data for evidence generation in medicine and epidemiology.

</details>


### [84] [SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with {Thermal} IR {(LWIR/MWIR)} and RGB](https://arxiv.org/abs/2510.13404)
*Muhammad Ishfaq Hussain,Ma Van Linh,Zubia Naz,Unse Fatima,Yeongmin Ko,Moongu Jeon*

Main category: cs.LG

TL;DR: 提出一种从 LWIR 数据合成类似 SWIR 的对比/结构线索，并将其与真实的 LWIR 和 RGB 三模态进行融合，采用模态特异编码器+软最大门控的融合头，在公开 RGB-LWIR 基准与私有 RGB-MWIR-SWIR 数据集上实现对比度、边缘和结构保真度的提升，且保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 在能见度低、受大气干扰的场景中，RGB+LWIR 的融合难以提供充分的场景信息；SWIR 具有更强穿透性和材料分辨力，但公开数据稀缺，限制了 SWIR 融合的研究与应用。本研究通过从 LWIR 数据中合成接近 SWIR 的对比/结构信息，缓解数据不足问题并提升三模态融合效果。

Method: 生成“合成 SWIR”对比/结构线索（非真实光谱复现），再基于合成 SWIR、LWIR、RGB 构建多模态融合框架，采用模态特异编码器与软最大门控的融合头；在公开 RGB-LWIR 基准（M3FD、TNO、CAMEL、MSRS、RoadScene）和私有 RGB-MWIR-SWIR 数据集上进行评估，并与公平的 trimodal 基线（LP、LatLRR、GFF）及级联的 U2Fusion/SwinFusion 变体比较，关注融合图像的对比度、边缘定义与结构保真度，并声称可实现实时性能。

Result: 实验结果表明，使用合成 SWIR 的三模态融合在对比度、边缘清晰度和结构保真度方面优于对照组，且在多套基准与私有数据集上均具备实时性。相对于同行的三模态方法与级联变体，本文的方法展现出显著性能优势。

Conclusion: 基于从 LWIR 合成的 SWIR 对比/结构信息的多模态融合框架具有现实世界应用潜力，特别是在监控与自主系统场景；同时缓解了 SWIR 数据不足的问题并实现端到端高效融合。但需要对合成 SWIR 的谱学一致性、鲁棒性以及跨数据集泛化能力进行更深入评估与验证。

Abstract: Enhancing scene understanding in adverse visibility conditions remains a
critical challenge for surveillance and autonomous navigation systems.
Conventional imaging modalities, such as RGB and thermal infrared (MWIR /
LWIR), when fused, often struggle to deliver comprehensive scene information,
particularly under conditions of atmospheric interference or inadequate
illumination. To address these limitations, Short-Wave Infrared (SWIR) imaging
has emerged as a promising modality due to its ability to penetrate atmospheric
disturbances and differentiate materials with improved clarity. However, the
advancement and widespread implementation of SWIR-based systems face
significant hurdles, primarily due to the scarcity of publicly accessible SWIR
datasets. In response to this challenge, our research introduces an approach to
synthetically generate SWIR-like structural/contrast cues (without claiming
spectral reproduction) images from existing LWIR data using advanced contrast
enhancement techniques. We then propose a multimodal fusion framework
integrating synthetic SWIR, LWIR, and RGB modalities, employing an optimized
encoder-decoder neural network architecture with modality-specific encoders and
a softmax-gated fusion head. Comprehensive experiments on public {RGB-LWIR
benchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private real
RGB-MWIR-SWIR dataset} demonstrate that our synthetic-SWIR-enhanced fusion
framework improves fused-image quality (contrast, edge definition, structural
fidelity) while maintaining real-time performance. We also add fair trimodal
baselines (LP, LatLRR, GFF) and cascaded trimodal variants of
U2Fusion/SwinFusion under a unified protocol. The outcomes highlight
substantial potential for real-world applications in surveillance and
autonomous systems.

</details>


### [85] [When Embedding Models Meet: Procrustes Bounds and Applications](https://arxiv.org/abs/2510.13406)
*Lucas Maystre,Alvaro Ortega Gonzalez,Charles Park,Rares Dolga,Tudor Berariu,Yu Zhao,Kamil Ciosek*

Main category: cs.LG

TL;DR: 若两个嵌入集合的点积对近似保持，则存在一个正交变换可将它们对齐，且对齐误差有界；通过Procrustes后处理实现嵌入空间的互操作性，适用于再训练保持兼容性、跨模型文本检索和混合模态检索。


<details>
  <summary>Details</summary>
Motivation: 同源数据上训练的不同嵌入模型常导致表示不互操作，给模型再训练、部分升级和多模态检索带来挑战；若能在保 geometry 的前提下实现跨模型对齐，可提高系统的灵活性与可组合性。

Method: 证明若两组向量的成对点积在一定范围内近似保持，则存在一个近似正交变换使两组嵌入尽量贴合，并给出对齐误差的紧界；提出简单的对齐策略：Procrustes后处理，保持各自嵌入空间的几何结构。

Result: 给出对齐误差的紧界，并在三类应用中验证有效性：在再训练之间保持兼容性、将不同模型用于文本检索的联合、以及改善跨模态检索，达到SOTA或接近SOTA的性能。

Conclusion: Procrustes后处理能够在不破坏各自嵌入空间几何性的前提下实现模型之间的互操作性，且在多种应用场景中表现出色，便于实际部署。

Abstract: Embedding models trained separately on similar data often produce
representations that encode stable information but are not directly
interchangeable. This lack of interoperability raises challenges in several
practical applications, such as model retraining, partial model upgrades, and
multimodal search. Driven by these challenges, we study when two sets of
embeddings can be aligned by an orthogonal transformation. We show that if
pairwise dot products are approximately preserved, then there exists an
isometry that closely aligns the two sets, and we provide a tight bound on the
alignment error. This insight yields a simple alignment recipe, Procrustes
post-processing, that makes two embedding models interoperable while preserving
the geometry of each embedding space. Empirically, we demonstrate its
effectiveness in three applications: maintaining compatibility across
retrainings, combining different models for text retrieval, and improving
mixed-modality search, where it achieves state-of-the-art performance.

</details>


### [86] [Modeling Adoptive Cell Therapy in Bladder Cancer from Sparse Biological Data using PINNs](https://arxiv.org/abs/2510.13431)
*Kayode Olumoyin,Katarzyna Rejniak*

Main category: cs.LG

TL;DR: 在数据稀疏的肿瘤治疗场景中，改进的PINN通过嵌入物理与生物约束，学习时间变6治的相互作用与ODE模型参数的时变形式，能够在少量训练样本下逼近解并实现良好泛化。


<details>
  <summary>Details</summary>
Motivation: 解决肿瘤数据稀缺的问题，通过将动力学规律和生物学约束融入PINN，提高对治疗动态与参数随时间变化的推断能力。

Method: 在PINN框架中加入 observed biological constraints 作为正则化项，扩展以学习时间变化的治疗相互作用和组合治疗的ODE参数，适用于间歇性给药的场景。

Result: 该修改的PINN能够收敛到ODE解，估计时间变的参数，并在MSE、MAE、MAPE等指标上表现良好。

Conclusion: 通过将物理信息与生物约束结合的PINN，为数据稀缺的肿瘤治疗动力学推断提供一个鲁棒且可泛化的学习框架。

Abstract: Physics-informed neural networks (PINNs) are neural networks that embed the
laws of dynamical systems modeled by differential equations into their loss
function as constraints. In this work, we present a PINN framework applied to
oncology. Here, we seek to learn time-varying interactions due to a combination
therapy in a tumor microenvironment. In oncology, experimental data are often
sparse and composed of a few time points of tumor volume. By embedding
inductive biases derived from prior information about a dynamical system, we
extend the physics-informed neural networks (PINN) and incorporate observed
biological constraints as regularization agents. The modified PINN algorithm is
able to steer itself to a reasonable solution and can generalize well with only
a few training examples. We demonstrate the merit of our approach by learning
the dynamics of treatment applied intermittently in an ordinary differential
equation (ODE) model of a combination therapy. The algorithm yields a solution
to the ODE and time-varying forms of some of the ODE model parameters. We
demonstrate a strong convergence using metrics such as the mean squared error
(MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE).

</details>


### [87] [Hybrid Interval Type-2 Mamdani-TSK Fuzzy System for Regression Analysis](https://arxiv.org/abs/2510.13437)
*Ashish Bhatia,Renato Cordeiro de Amorim,Vito De Feo*

Main category: cs.LG

TL;DR: 提出了一种结合 Mamdani 可解释性与 TSK 精度的混合模糊回归方法，采用带有模糊和精确分量的混合规则结构和双支配类型，在多数据集上达到有竞争力的 RMSE 改进，体现了可解释性与准确性的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决回归分析中可解释性与准确性之间的权衡，以及传统方法在现实数据的不确定性、模糊性方面的局限；相比深度学习的可解释性不足，模糊系统需提高精度。

Method: 提出一种混合规则结构，结合形如 Mamdani 的可解释性组件和形如 Takagi-Sugeno-Kang 的输出精度组件，并引入双支配类型以提升规则输出的准确性与可解释性协调。该方法在规则中保留部分传统 Mamdani 的可解释性，同时通过改进的规则输出提升精度。

Result: 在六个数据集上，提出的方法在4个数据集上获得最佳模糊方法分数；在2个数据集上优于不透明模型；在1个数据集上获得最佳综合分数；RMSE 改善范围约0.4%至19%。

Conclusion: 该混合方法在可解释性与准确性之间提供了平衡，形成了一个多场景、可扩展的预测建模工具，成功解决了模糊系统固有的可解释性与精度之间的矛盾，显示出对不同数据集的鲁棒性与实用性。

Abstract: Regression analysis is employed to examine and quantify the relationships
between input variables and a dependent and continuous output variable. It is
widely used for predictive modelling in fields such as finance, healthcare, and
engineering. However, traditional methods often struggle with real-world data
complexities, including uncertainty and ambiguity. While deep learning
approaches excel at capturing complex non-linear relationships, they lack
interpretability and risk over-fitting on small datasets. Fuzzy systems provide
an alternative framework for handling uncertainty and imprecision, with Mamdani
and Takagi-Sugeno-Kang (TSK) systems offering complementary strengths:
interpretability versus accuracy. This paper presents a novel fuzzy regression
method that combines the interpretability of Mamdani systems with the precision
of TSK models. The proposed approach introduces a hybrid rule structure with
fuzzy and crisp components and dual dominance types, enhancing both accuracy
and explainability. Evaluations on benchmark datasets demonstrate
state-of-the-art performance in several cases, with rules maintaining a
component similar to traditional Mamdani systems while improving precision
through improved rule outputs. This hybrid methodology offers a balanced and
versatile tool for predictive modelling, addressing the trade-off between
interpretability and accuracy inherent in fuzzy systems. In the 6 datasets
tested, the proposed approach gave the best fuzzy methodology score in 4
datasets, out-performed the opaque models in 2 datasets and produced the best
overall score in 1 dataset with the improvements in RMSE ranging from 0.4% to
19%.

</details>


### [88] [Rectify and Align GPS Points to Parking Spots via Rank-1 Constraint](https://arxiv.org/abs/2510.13439)
*Jiaxing Deng,Junbiao Pang,Zhicheng Wang,Haitao Yu*

Main category: cs.LG

TL;DR: 提出一种基于无监督的低秩矩阵方法，利用道路两旁平行的物理约束，对GPS点中的错误进行修正并与停车位对齐，在大规模停车点数据上实现定位误差的统一修正。


<details>
  <summary>Details</summary>
Motivation: 城市中高层建筑导致GPS点相对于实际停车位产生漂移，加之低成本GPS设备的固有误差，难以在无监督条件下从大量停车点中纠正少量错误点，提升定位精度。

Method: 提出基于低秩假设的无监督纠错与对齐框架，结合停车位与道路边平行的物理约束，进行统一的纠错与对齐操作。该方法为“非传统但简单有效”的停车点GPS纠偏与定位对齐策略。

Result: 通过广泛实验验证，该方法在解决实际的停车点GPS误差问题上具备显著优势；数据集与代码公开。

Conclusion: 所提出的方法简单而有效，能处理各种类型的GPS点误差，并可应用于停车点定位与城市开发等相关应用领域。

Abstract: Parking spots are essential components, providing vital mobile resources for
residents in a city. Accurate Global Positioning System (GPS) points of parking
spots are the core data for subsequent applications,e.g., parking management,
parking policy, and urban development. However, high-rise buildings tend to
cause GPS points to drift from the actual locations of parking spots; besides,
the standard lower-cost GPS equipment itself has a certain location error.
Therefore, it is a non-trivial task to correct a few wrong GPS points from a
large number of parking spots in an unsupervised approach. In this paper,
motivated by the physical constraints of parking spots (i.e., parking spots are
parallel to the sides of roads), we propose an unsupervised low-rank method to
effectively rectify errors in GPS points and further align them to the parking
spots in a unified framework. The proposed unconventional rectification and
alignment method is simple and yet effective for any type of GPS point errors.
Extensive experiments demonstrate the superiority of the proposed method to
solve a practical problem. The data set and the code are publicly accessible
at:https://github.com/pangjunbiao/ITS-Parking-spots-Dataset.

</details>


### [89] [Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers](https://arxiv.org/abs/2510.13444)
*Nico Pelleriti,Christoph Spiegel,Shiwei Liu,David Martínez-Rubio,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 提出了一个学习增强的SOS判定算法，使用Transformer预测近最小的单项式基，从而显著减小相关的半定规划规模，在200个基准数据集上达到>100x的加速，并解决了现有方法难以处理的实例。


<details>
  <summary>Details</summary>
Motivation: SOS判定是一个NP-hard问题，现有方法需要求解规模随单项式基的线性或二次增长的SDP，难以扩展。通过学习驱动的基选择，可以降低 SDP 的维度，从而提升求解可行性与速度。

Method: 训练一个Transformer模型，输入多项式并输出一个近似最小的单项式基集合，使得对应的SOS条件形成更小的SDP。核心包括：1) 生成超过1亿个SOS多项式的高效训练数据集；2) 设计并训练与任务匹配的Transformer架构；3) 引入稳健的回退机制确保正确终止，并对其理论性质进行分析。

Result: 在200多个基准数据集上验证，平均获得超过100倍的加速，相较于最先进的求解器，并能处理竞争方法失败的实例。

Conclusion: 提出了首个学习增强的SOS判定算法，显著提升SOS编程的实际可扩展性，为SOS问题的实用性提供了新的方向。

Abstract: Certifying nonnegativity of polynomials is a well-known NP-hard problem with
direct applications spanning non-convex optimization, control, robotics, and
beyond. A sufficient condition for nonnegativity is the Sum of Squares (SOS)
property, i.e., it can be written as a sum of squares of other polynomials. In
practice, however, certifying the SOS criterion remains computationally
expensive and often involves solving a Semidefinite Program (SDP), whose
dimensionality grows quadratically in the size of the monomial basis of the SOS
expression; hence, various methods to reduce the size of the monomial basis
have been proposed. In this work, we introduce the first learning-augmented
algorithm to certify the SOS criterion. To this end, we train a Transformer
model that predicts an almost-minimal monomial basis for a given polynomial,
thereby drastically reducing the size of the corresponding SDP. Our overall
methodology comprises three key components: efficient training dataset
generation of over 100 million SOS polynomials, design and training of the
corresponding Transformer architecture, and a systematic fallback mechanism to
ensure correct termination, which we analyze theoretically. We validate our
approach on over 200 benchmark datasets, achieving speedups of over $100\times$
compared to state-of-the-art solvers and enabling the solution of instances
where competing approaches fail. Our findings provide novel insights towards
transforming the practical scalability of SOS programming.

</details>


### [90] [$L_2$-Regularized Empirical Risk Minimization Guarantees Small Smooth Calibration Error](https://arxiv.org/abs/2510.13450)
*Masahiro Fujisawa,Futoshi Futami*

Main category: cs.LG

TL;DR: 给出理论证明：在L2正则化的经验风险最小化（ERM）下，可以直接控制平滑校准误差（smCE），无需后置校准或专门的校准正则；给出基于优化误差、正则化强度和Rademacher复杂度的有限样本泛化界，并对核希尔伯特空间中的模型（核岭回归与逻辑回归）给出具体保证；实验验证这些结论，代码公开。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，预测概率的校准对可靠性至关重要，但现有工作对标准训练过程如何产生良好校准缺乏深入理解。本工作首次给出理论证明，表明常见的L2正则化ER M能直接控制平滑校准误差。

Method: 给出smCE的泛化界的理论推导，基于优化误差、正则化强度和Rademacher复杂度；将理论实例化到再生核希尔伯特空间（RKHS），推导出核岭回归与逻辑回归的具体保证；通过实验验证理论结论并评估校准性质。

Result: 给出smCE的有限样本泛化界，依赖于优化误差、正则化参数和Rademacher复杂度；对RKHS中的核岭回归和逻辑回归给出具体保证；实验结果与理论一致，证实L2正则化的ERM可在无额外校准策略的情况下得到良好校准模型；提供复现实验的代码。

Conclusion: L2正则化的ERM能够在没有Boosting或后置重新校准的情况下，产生良好校准的模型，揭示了校准与标准ERM之间的理论联系，并给出适用于核方法的具体保证及泛化边界。

Abstract: Calibration of predicted probabilities is critical for reliable machine
learning, yet it is poorly understood how standard training procedures yield
well-calibrated models. This work provides the first theoretical proof that
canonical $L_{2}$-regularized empirical risk minimization directly controls the
smooth calibration error (smCE) without post-hoc correction or specialized
calibration-promoting regularizer. We establish finite-sample generalization
bounds for smCE based on optimization error, regularization strength, and the
Rademacher complexity. We then instantiate this theory for models in
reproducing kernel Hilbert spaces, deriving concrete guarantees for kernel
ridge and logistic regression. Our experiments confirm these specific
guarantees, demonstrating that $L_{2}$-regularized ERM can provide a
well-calibrated model without boosting or post-hoc recalibration. The source
code to reproduce all experiments is available at
https://github.com/msfuji0211/erm_calibration.

</details>


### [91] [Towards Blackwell Optimality: Bellman Optimality Is All You Can Get](https://arxiv.org/abs/2510.13476)
*Victor Boone,Adrienne Tuynman*

Main category: cs.LG

TL;DR: 本研究探讨在MDP中识别不同阶数的最优策略（从平均收益最优到Blackwell最优）的可行性，给出逐阶的学习算法、可在有限时间内停止识别的条件，以及一个在可行时触发的可行停止规则。


<details>
  <summary>Details</summary>
Motivation: 传统的平均收益最优性往往是渐近性质，引入即时损失的量度后形成一系列的最优性阶次。本研究旨在识别各阶次的最优策略，并区分何时能在有限时间内完成识别。

Method: 对每一个阶次，构造一个学习算法，使得错误概率趋于0；并分析MDP的类别，判断该识别是否能在有限时间内停止。

Result: 存在与阶次相对应的学习算法，错误概率可收敛为0；能在有限时间内停止识别的MDP仅限于具有唯一Bellman最优策略的情形，与所考察的最优性阶次无关；并给出一个可行的停止规则，在可能时在有限时间触发。

Conclusion: 本文提供了一个统一框架，用于识别MDP中不同最优性阶次的可行性，并给出与之配套的实用停止机制。有限时间识别的充要条件仅依赖于Bellman最优策略的唯一性。

Abstract: Although average gain optimality is a commonly adopted performance measure in
Markov Decision Processes (MDPs), it is often too asymptotic. Further
incorporating measures of immediate losses leads to the hierarchy of bias
optimalities, all the way up to Blackwell optimality. In this paper, we
investigate the problem of identifying policies of such optimality orders. To
that end, for each order, we construct a learning algorithm with vanishing
probability of error. Furthermore, we characterize the class of MDPs for which
identification algorithms can stop in finite time. That class corresponds to
the MDPs with a unique Bellman optimal policy, and does not depend on the
optimality order considered. Lastly, we provide a tractable stopping rule that
when coupled to our learning algorithm triggers in finite time whenever it is
possible to do so.

</details>


### [92] [Tahakom LLM guidelines and receipts: from pre-training data to an Arabic LLM](https://arxiv.org/abs/2510.13481)
*Areej AlOtaibi,Lina Alyahya,Raghad Alshabanah,Shahad Alfawzan,Shuruq Alarefei,Reem Alsabti,Nouf Alsubaie,Abdulaziz Alhuzaymi,Lujain Alkhelb,Majd Alsayari,Waad Alahmed,Omar Talabay,Jalal Alowibdi,Salem Alelyani,Adel Bibi*

Main category: cs.LG

TL;DR: 提出面向阿拉伯语言的LLM训练框架，聚焦数据收集与净化、分词器设计及评估框架改进，并公开数据与方法以促进协作。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯NLP面临数据稀缺、分词与评估不足等挑战，需要系统化、透明的研究来提升阿拉伯LLM的性能与可重复性。

Method: 1) 设计并执行阿拉伯预训练数据的收集与筛选流程；2) 对不同分词器设计进行对比评估；3) 审视现有阿拉伯评估框架的局限，提出系统性纠正方法；4) 公开数据与研究方法，促进协作。

Result: 提供关键洞见并提出纠正性方法论，同时实现数据与方法的公开化。

Conclusion: 强调透明性与开放协作对阿拉伯语言模型发展的作用。

Abstract: Large Language Models (LLMs) have significantly advanced the field of natural
language processing, enhancing capabilities in both language understanding and
generation across diverse domains. However, developing LLMs for Arabic presents
unique challenges. This paper explores these challenges by focusing on critical
aspects such as data curation, tokenizer design, and evaluation. We detail our
approach to the collection and filtration of Arabic pre-training datasets,
assess the impact of various tokenizer designs on model performance, and
examine the limitations of existing Arabic evaluation frameworks, for which we
propose a systematic corrective methodology. To promote transparency and
facilitate collaborative development, we share our data and methodologies,
contributing to the advancement of language modeling, particularly for the
Arabic language.

</details>


### [93] [DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation](https://arxiv.org/abs/2510.13497)
*Zexin Wang,Lin Shi,Haoyu Wu,Junru Luo,Xiangzeng Kong,Jun Qi*

Main category: cs.LG

TL;DR: 提出基于 CLIP 框架的多模态 EEG-文本模型 DistilCLIP-EEG，用于癫痫检测，并通过蒸馏实现轻量化部署。


<details>
  <summary>Details</summary>
Motivation: 现有大多数癫痫检测方法依赖单模态 EEG，未利用文本描述等多模态信息，可能限制了表示能力与鲁棒性。需要更高效、可在资源受限环境中部署的多模态模型。

Method: 将 EEG 编码器（Conformer 架构）与文本编码器对齐在共享潜在空间，使用 Learnable BERT（BERT-LP）进行提示学习；基于 CLIP 的跨模态对比学习；引入知识蒸馏，教师为 DistilCLIP-EEG，学生模型更轻量化。对 TUSZ、AUBMC、CHB-MIT 数据集进行评估。

Result: 教师与学生在所有数据集上的准确率均超过 97%，F1 分数持续高于 0.94；学生模型的参数量与模型规模约为教师的 58.1%，显著降低了复杂度与存储需求，同时保持高性能。

Conclusion: 所提出的多模态 DistilCLIP-EEG 在 EEG 基于癫痫检测中展现出鲁棒性与高效性，适合在资源受限环境中部署，奠定了跨模态学习用于 EEG 相关任务的坚实基础。

Abstract: Epilepsy is a prevalent neurological disorder marked by sudden, brief
episodes of excessive neuronal activity caused by abnormal electrical
discharges, which may lead to some mental disorders. Most existing deep
learning methods for epilepsy detection rely solely on unimodal EEG signals,
neglecting the potential benefits of multimodal information. To address this,
we propose a novel multimodal model, DistilCLIP-EEG, based on the CLIP
framework, which integrates both EEG signals and text descriptions to capture
comprehensive features of epileptic seizures. The model involves an EEG encoder
based on the Conformer architecture as a text encoder, the proposed Learnable
BERT (BERT-LP) as prompt learning within the encoders. Both operate in a shared
latent space for effective cross-modal representation learning. To enhance
efficiency and adaptability, we introduce a knowledge distillation method where
the trained DistilCLIP-EEG serves as a teacher to guide a more compact student
model to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MIT
datasets, both the teacher and student models achieved accuracy rates exceeding
97%. Across all datasets, the F1-scores were consistently above 0.94,
demonstrating the robustness and reliability of the proposed framework.
Moreover, the student model's parameter count and model size are approximately
58.1% of those of the teacher model, significantly reducing model complexity
and storage requirements while maintaining high performance. These results
highlight the potential of our proposed model for EEG-based epilepsy detection
and establish a solid foundation for deploying lightweight models in
resource-constrained settings.

</details>


### [94] [Offline and Online KL-Regularized RLHF under Differential Privacy](https://arxiv.org/abs/2510.13512)
*Yulian Wu,Rushil Thareja,Praneeth Vepakomma,Francesco Orabona*

Main category: cs.LG

TL;DR: 在隐私保护前提下，离线与在线强化学习从人类反馈中学习（RLHF）在KL正则化下的理论分析，给出离线的子最优界与下界，以及在线的后悔界，并包含对无隐私场景的推导与实现验证。


<details>
  <summary>Details</summary>
Motivation: 理解在ε-LDP隐私约束下，KL正则化的RLHF的学习性能与理论保证；填补在线/离线场景在隐私保护下的理论空白，并提供实现验证。

Method: 离线：基于悲观性原则的算法，给出在单策略可控性假设下的子最优性界 tilde O(1/[(e^ε-1)^2 n])；并给出匹配下界以证明最优性。在线：提出乐观算法，得到对数后悔界 O(d_F log(N_F T)/(e^ε-1)^2)；其中 T 为总时间步，N_F 为奖励函数空间基数，d_F 为 RLHF 的变体 eluder 维度。同时给出在线 KL-regularized RLHF 在无隐私情形的分析作为副产品。实现方面在离线场景进行实验以验证理论结果，并开源代码。

Result: 离线：获得子最优性界与匹配下界；在线：获得对数级后悔界；并给出无隐私场景的在线分析；提供离线实现以验证理论，代码已开源。

Conclusion: 本工作在ε-LDP约束下给出 KL-正则化 RLHF 的离线/在线理论保障，首次给出在线 KL-regularized RLHF 的分析（在无隐私场景亦成立的推导），并提供实现与代码，以促进隐私保护下的 RLHF 研究。

Abstract: In this paper, we study the offline and online settings of reinforcement
learning from human feedback (RLHF) with KL-regularization -- a widely used
objective function in large language model alignment -- under the $\epsilon$
local differential privacy ($\epsilon$-LDP) model on the label of the human
preference. In the offline setting, we design an algorithm based on the
principle of pessimism and derive a new suboptimality gap of
$\tilde{O}(1/[(e^\epsilon-1)^2 n])$ on the KL-regularized objective under
single-policy concentrability. We also prove its optimality by providing a
matching lower bound where $n$ is the sample size.
  In the online setting, we are the first one to theoretically investigate the
problem of KL-regularized RLHF with LDP. We design an optimism-based algorithm
and derive a logarithmic regret bound of $O(d_{\mathcal{F}}\log
(N_{\mathcal{F}}\cdot T) /(e^\epsilon-1)^2 )$, where $T$ is the total time
step, $N_{\mathcal{F}}$ is cardinality of the reward function space
$\mathcal{F}$ and $d_{\mathcal{F}}$ is a variant of eluder dimension for RLHF.
As a by-product of our analysis, our results also imply the first analysis for
online KL-regularized RLHF without privacy. We implement our algorithm in the
offline setting to verify our theoretical results and release our open source
code at: https://github.com/rushil-thareja/PPKL-RLHF-Official.

</details>


### [95] [K-Merge: Online Continual Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2510.13537)
*Donald Shenaj,Ondrej Bohdal,Taha Ceritli,Mete Ozay,Pietro Zanuttigh,Umberto Michieli*

Main category: cs.LG

TL;DR: 提出一种无数据的在线持续合并方法，用于在设备端有限存储条件下选择并合并LoRA，以在引入新LoRA时维持旧任务性能，实验表明优于其他策略。


<details>
  <summary>Details</summary>
Motivation: 解决当新LoRA到来且设备只能存储有限数量的适配器时，如何在不访问原始数据的情况下实现高效的选择与合并，同时避免对已支持任务性能造成显著下降。

Method: 提出一种数据无关的策略，在新LoRA到来时选取应合并的子集并进行合并，确保在存储预算内，这一过程兼顾新旧任务的性能，且计算开销低。

Result: 在真实世界任务上的广泛实验表明，该策略优于替代方案，在存储和计算约束下实现更优性能。

Conclusion: 该方法为设备端在紧张存储/计算资源下的在线持续合并提供可行解决方案，支持逐步引入新任务的LoRA，同时保持对已支持任务的性能。

Abstract: On-device deployment of Large Language Models (LLMs) frequently leverages
Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight
resource constraints. To address the limited storage capacity of mobile
devices, recent works have explored model merging techniques to fuse multiple
LoRAs into a single one. In practice, however, LoRAs are often delivered
incrementally, as users request support for new tasks (e.g., novel problem
types or languages). This scenario introduces a new challenge: on-device online
continual merging, where the objective is to incorporate new LoRAs while
preserving the performance on previously supported tasks. In this paper, we
propose a data-free and computationally efficient strategy for selecting and
merging LoRAs when a new one becomes available, assuming the device can store
only a limited number of adapters. Extensive experiments across real-world
tasks demonstrate the superiority of our approach compared to alternative
strategies while adhering to the storage budget and compute limitations of
on-device settings.

</details>


### [96] [ProtoTopic: Prototypical Network for Few-Shot Medical Topic Modeling](https://arxiv.org/abs/2510.13542)
*Martin Licht,Sara Ketabi,Farzad Khalvati*

Main category: cs.LG

TL;DR: ProtoTopic is a prototypical-network-based topic model that improves coherence and diversity for medical abstracts in low-data settings.


<details>
  <summary>Details</summary>
Motivation: Medical domain often has topics with limited data, and standard topic models struggle with medical text; there is a need for data-efficient, explainable topic modeling.

Method: ProtopicTopic adapts prototypical networks to topic modeling by creating topic prototypes and classifying abstracts based on distances to prototypes, enabling few-shot, interpretable topic generation.

Result: ProtoTopic achieves higher topic coherence and diversity than two strong baselines on medical abstracts, demonstrating effective topic generation even with limited data.

Conclusion: ProtoTopic shows promise for robust, explainable topic modeling in medicine under data scarcity and could extend to other domains with limited labeled data.

Abstract: Topic modeling is a useful tool for analyzing large corpora of written
documents, particularly academic papers. Despite a wide variety of proposed
topic modeling techniques, these techniques do not perform well when applied to
medical texts. This can be due to the low number of documents available for
some topics in the healthcare domain. In this paper, we propose ProtoTopic, a
prototypical network-based topic model used for topic generation for a set of
medical paper abstracts. Prototypical networks are efficient, explainable
models that make predictions by computing distances between input datapoints
and a set of prototype representations, making them particularly effective in
low-data or few-shot learning scenarios. With ProtoTopic, we demonstrate
improved topic coherence and diversity compared to two topic modeling baselines
used in the literature, demonstrating the ability of our model to generate
medically relevant topics even with limited data.

</details>


### [97] [Multi-Objective $\textit{min-max}$ Online Convex Optimization](https://arxiv.org/abs/2510.13560)
*Rahul Vaze,Sumiran Mishra*

Main category: cs.LG

TL;DR: 提出在多目标在线凸优化中的min-max regrets框架，结合Hedge与OGD在未知分布的i.i.d.情形，给出期望的min-max regret界限为O(√(T log K))。


<details>
  <summary>Details</summary>
Motivation: 将单目标OCO扩展到有K个损失序列的情形，研究在对所有时间片都要静态选择一个动作以最小化上述K个序列的总损失的极端绩效度量（min-max regret）在未知分布的i.i.d.输入下的行为与代价。

Method: 提出一种将Hedge（混合专家）与在线梯度下降（OGD）相结合的简单算法，用于在每个时间步在动作空间中选择策略，以适应K条损失序列的并行挑战。

Result: 证明该算法在i.i.d.设定下的期望min-max regret 为O(√(T log K))。

Conclusion: 该工作给出了一个简单且有效的多目标OCO算法，在未知分布的i.i.d.情形下实现对K条序列的跟踪能力，且渐进误差随时间和序列数量以对数和平方根的方式受控。

Abstract: In online convex optimization (OCO), a single loss function sequence is
revealed over a time horizon of $T$, and an online algorithm has to choose its
action at time $t$, before the loss function at time $t$ is revealed. The goal
of the online algorithm is to incur minimal penalty (called $\textit{regret}$
compared to a static optimal action made by an optimal offline algorithm
knowing all functions of the sequence in advance.
  In this paper, we broaden the horizon of OCO, and consider multi-objective
OCO, where there are $K$ distinct loss function sequences, and an algorithm has
to choose its action at time $t$, before the $K$ loss functions at time $t$ are
revealed. To capture the tradeoff between tracking the $K$ different sequences,
we consider the $\textit{min-max}$ regret, where the benchmark (optimal offline
algorithm) takes a static action across all time slots that minimizes the
maximum of the total loss (summed across time slots) incurred by each of the
$K$ sequences. An online algorithm is allowed to change its action across time
slots, and its {\it min-max} regret is defined as the difference between its
$\textit{min-max}$ cost and that of the benchmark. The $\textit{min-max}$
regret is a stringent performance measure and an algorithm with small regret
needs to `track' all loss function sequences closely at all times.
  We consider this $\textit{min-max}$ regret in the i.i.d. input setting where
all loss functions are i.i.d. generated from an unknown distribution. For the
i.i.d. model we propose a simple algorithm that combines the well-known
$\textit{Hedge}$ and online gradient descent (OGD) and show via a remarkably
simple proof that its expected $\textit{min-max}$ regret is $O(\sqrt{T \log
K})$.

</details>


### [98] [DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning](https://arxiv.org/abs/2510.13567)
*Omayma Moussadek,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 提出 DOLFIN：基于 Vision Transformer 的分布式在线 LoRA 的联邦增量学习框架，结合 DualGPM 防止遗忘，在隐私保护前提下实现低通信开销和高准确性，在多数据集和 Dirichlet 异质性设定下优于六个基线.


<details>
  <summary>Details</summary>
Motivation: 在联邦持续学习中，需要在模型性能、隐私保护和通信效率之间取得平衡。现有方法在分布式、异质数据环境下往往难以同时实现高准确性与避免遗忘。

Method: 将 Vision Transformers 与低秩适配器 LoRA 融合，提出分布式在线学习框架 DOLFIN；使用 Orthogonal LoRA 降低通信与参数增量；引入 DualGradient Projection Memory (DualGPM) 以防止遗忘；在隐私保护前提下提高数据效率。

Result: 在 CIFAR-100、ImageNet-R、ImageNet-A、CUB-200 的两组 Dirichlet 异质性设置下，DOLFIN 在最终平均准确率方面显著优于六个强基线，且内存开销与基线相当。

Conclusion: 正交低秩适配器与 DualGPM 的结合为隐私保护下的联邦持续学习提供一个可扩展、高效的解决方案，适用于分布式场景。

Abstract: Federated continual learning (FCL) enables models to learn new tasks across
multiple distributed clients, protecting privacy and without forgetting
previously acquired knowledge. However, current methods face challenges
balancing performance, privacy preservation, and communication efficiency. We
introduce a Distributed Online LoRA for Federated INcremental learning method
DOLFIN, a novel approach combining Vision Transformers with low-rank adapters
designed to efficiently and stably learn new tasks in federated environments.
Our method leverages LoRA for minimal communication overhead and incorporates
DualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on
CIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet
heterogeneity settings, DOLFIN consistently surpasses six strong baselines in
final average accuracy while matching their memory footprint. Orthogonal
low-rank adapters offer an effective and scalable solution for
privacy-preserving continual learning in federated settings.

</details>


### [99] [Selective Adversarial Attacks on LLM Benchmarks](https://arxiv.org/abs/2510.13570)
*Ivan Dubrovsky,Anastasia Orlova,Illarion Iov,Nina Gubina,Irena Gureeva,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 存在选择性对抗扰动，可在不显著影响其他模型的情况下改变MMLU基准的相对排名，从而影响评估的公正性、可再现性与透明度；提出了用于评估选择性的协议、约束及替代-LMM管道。


<details>
  <summary>Details</summary>
Motivation: 在LLM评估日益支配信任与部署的背景下，现有对抗鲁棒性多聚焦通用文本攻击，忽视了可能仅对部分模型有效的选择性攻击，可能削弱排行榜的公平性与可重复性。

Method: 将选择性对抗攻击形式化，结合TextAttack框架的常规攻击，提出选择性评估协议；引入增强选择性的自定义约束；提出一个替代型LLM管道以生成选择性扰动。

Result: 实证发现存在选择性对抗攻击，能够显著改变模型的相对排名，挑战基于排行榜的评估的公平性、可重复性与透明度。

Conclusion: 呼吁在LLM评估中采用扰动感知的报告与鲁棒性诊断，强调即使微小编辑也可能改变比较结论。

Abstract: Benchmarking outcomes increasingly govern trust, selection, and deployment of
LLMs, yet these evaluations remain vulnerable to semantically equivalent
adversarial perturbations. Prior work on adversarial robustness in NLP has
emphasized text attacks that affect many models equally, leaving open the
question of whether it is possible to selectively degrade or enhance
performance while minimally affecting other models. We formalize this problem
and study selective adversarial attacks on MMLU - a widely used benchmark
designed to measure a language model's broad general knowledge and reasoning
ability across different subjects. Using canonical attacks integrated into
TextAttack framework, we introduce a protocol for selectivity assessment,
develop a custom constraint to increase selectivity of attacks and propose a
surrogate-LLM pipeline that generates selective perturbations. Empirically, we
find that selective adversarial attacks exist and can materially alter relative
rankings, challenging the fairness, reproducibility, and transparency of
leaderboard-driven evaluation. Our results motivate perturbation-aware
reporting and robustness diagnostics for LLM evaluation and demonstrate that
even subtle edits can shift comparative judgments.

</details>


### [100] [ArtNet: Hierarchical Clustering-Based Artificial Netlist Generator for ML and DTCO Application](https://arxiv.org/abs/2510.13582)
*Andrew B. Kahng. Seokhyeong Kang,Seonghyeon Park,Dooseok Yoon*

Main category: cs.LG

TL;DR: ArtNet 提出一种人工网表生成器用于数据增强，改善 ML/DTCO 的训练数据多样性与设计流时效，从而提升 PPA 优化与设计启用的效率。


<details>
  <summary>Details</summary>
Motivation: 在先进制程中，训练数据稀缺与长的设计流周转时间限制了机器学习和设计-技术协同优化的效果，亟需提升模型泛化与设计空间探索能力。

Method: ArtNet 通过复制网表的关键拓扑特征来生成现实、具有代表性的人工数据集，提升 ML 模型的鲁棒性与对目标参数的拟合度；在 CNN 驱动的 DRV 预测中用于数据增强；在 DTCO 场景下以“迷你脑”网表实现对全尺寸设计指标的接近匹配。

Result: 通过数据增强，CNN 基于 DRV 的 F1 分数提升了 0.16；DTCO 场景中 ArtNet 生成的迷你脑实现了高达 97.94% 的 PPA 匹配度，与目标全尺寸块设计的设计指标高度接近。

Conclusion: ArtNet 能提高数据多样性和模型泛化，促进更高效的 PPA 优化和流程/设计启用的探索。

Abstract: In advanced nodes, optimization of power, performance and area (PPA) has
become highly complex and challenging. Machine learning (ML) and
design-technology co-optimization (DTCO) provide promising mitigations, but
face limitations due to a lack of diverse training data as well as long design
flow turnaround times (TAT). We propose ArtNet, a novel artificial netlist
generator designed to tackle these issues. Unlike previous methods, ArtNet
replicates key topological characteristics, enhancing ML model generalization
and supporting broader design space exploration for DTCO. By producing
realistic artificial datasets that moreclosely match given target parameters,
ArtNet enables more efficient PPAoptimization and exploration of flows and
design enablements. In the context of CNN-based DRV prediction, ArtNet's data
augmentationimproves F1 score by 0.16 compared to using only the original
(real) dataset. In the DTCO context, ArtNet-generated mini-brains achieve a PPA
match up to 97.94%, demonstrating close alignment with design metrics of
targeted full-scale block designs.

</details>


### [101] [EEGChaT: A Transformer-Based Modular Channel Selector for SEEG Analysis](https://arxiv.org/abs/2510.13592)
*Chen Wang,Yansen Wang,Dongqi Han,Zilong Wang,Dongsheng Li*

Main category: cs.LG

TL;DR: 提出一种基于 Transformer 的 SEEG 通道选择模块 EEGChaT，通过 Channel Aggregation Tokens (CATs) 汇聚通道信息，并利用改进的 Attention Rollout 产生可解释的通道重要性评分，在 DuIN 数据集上与现有分类模型结合可实现高达 17% 的绝对准确率提升，且通道权重与人工筛选有显著重叠。


<details>
  <summary>Details</summary>
Motivation: 在 SEEG 数据中通道数量众多且相关性各异，传统通道选择方法难以扩展且缺乏解释性，因此需要一种高效且可解释的通道选择机制来提升解码性能。

Method: 提出 EEGChaT，通过 Channel Aggregation Tokens 汇聚多通道信息；使用改进的 Attention Rollout 生成定量的通道重要性分数，作为通道选择信号，并将 CATs 集成到现有分类模型中。

Result: 在 DuIN 数据集上，与现有分类模型结合时实现显著的解码准确率提升，最高可达 17% 的绝对提升；得到的通道权重与人工选择的通道高度重合，支持方法的可解释性；表明 EEGChaT 对高维 SEEG 分析具有通用性和可扩展性。

Conclusion: EEGChaT 提供了一种有效且可泛化的通道选择解决方案，兼具性能提升和对神经信号相关性的洞察，适用于高维 SEEG 分析与 BCI 应用。

Abstract: Analyzing stereoelectroencephalography (SEEG) signals is critical for
brain-computer interface (BCI) applications and neuroscience research, yet
poses significant challenges due to the large number of input channels and
their heterogeneous relevance. Traditional channel selection methods struggle
to scale or provide meaningful interpretability for SEEG data. In this work, we
propose EEGChaT, a novel Transformer-based channel selection module designed to
automatically identify the most task-relevant channels in SEEG recordings.
EEGChaT introduces Channel Aggregation Tokens (CATs) to aggregate information
across channels, and leverages an improved Attention Rollout technique to
compute interpretable, quantitative channel importance scores. We evaluate
EEGChaT on the DuIN dataset, demonstrating that integrating EEGChaT with
existing classification models consistently improves decoding accuracy,
achieving up to 17\% absolute gains. Furthermore, the channel weights produced
by EEGChaT show substantial overlap with manually selected channels, supporting
the interpretability of the approach. Our results suggest that EEGChaT is an
effective and generalizable solution for channel selection in high-dimensional
SEEG analysis, offering both enhanced performance and insights into neural
signal relevance.

</details>


### [102] [Physics-augmented Multi-task Gaussian Process for Modeling Spatiotemporal Dynamics](https://arxiv.org/abs/2510.13601)
*Xizhuo Zhang,Bing Yao*

Main category: cs.LG

TL;DR: 提出一个物理增强的多任务高斯过程框架P-M-GP，结合几何感知的多任务GP与物理定律正则化，用于时空高维动态系统的建模，并在三维心脏电动力学任务上验证，显著提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 需要在不规则几何域、快速时变和多相关物理变量的联预测场景下有效建模时空数据；传统高斯过程难以同时捕捉几何结构、任务相关性和物理约束，因此需要将几何先验与物理定律融入模型以提升保真度。

Method: 提出几何感知的多任务高斯过程模型（M-GP），以有效捕捉内在时空结构与任务之间的相关性；引入基于物理的正则化，使预测符合支配方程的动力学原理；将框架应用于三维心脏电动力学建模任务，可能涉及在流形上的核设计、图结构嵌入与多任务共克里金等策略，并通过一个物理约束项强化一致性。

Result: 数值实验表明，与现有方法相比，P-M-GP在预测精度上显著提升，体现出将物理约束和几何先验在时空动态建模中的协同效应。

Conclusion: 通过将物理规律约束与几何先验融入多任务高斯过程，P-M-GP能更准确、鲁棒地建模复杂时空动态系统；在三维心脏电动力学任务上取得显著改进，且具有向其它领域推广的潜力。

Abstract: Recent advances in sensing and imaging technologies have enabled the
collection of high-dimensional spatiotemporal data across complex geometric
domains. However, effective modeling of such data remains challenging due to
irregular spatial structures, rapid temporal dynamics, and the need to jointly
predict multiple interrelated physical variables. This paper presents a
physics-augmented multi-task Gaussian Process (P-M-GP) framework tailored for
spatiotemporal dynamic systems. Specifically, we develop a geometry-aware,
multi-task Gaussian Process (M-GP) model to effectively capture intrinsic
spatiotemporal structure and inter-task dependencies. To further enhance the
model fidelity and robustness, we incorporate governing physical laws through a
physics-based regularization scheme, thereby constraining predictions to be
consistent with governing dynamical principles. We validate the proposed P-M-GP
framework on a 3D cardiac electrodynamics modeling task. Numerical experiments
demonstrate that our method significantly improves prediction accuracy over
existing methods by effectively incorporating domain-specific physical
constraints and geometric prior.

</details>


### [103] [Towards Robust Knowledge Removal in Federated Learning with High Data Heterogeneity](https://arxiv.org/abs/2510.13606)
*Riccardo Santi,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 提出一种快速去除分布式学习中某客户影响的方法，基于任务算术(Task Arithmetic)和神经切线核(Neural Tangent Kernel)以最小化停机时间。


<details>
  <summary>Details</summary>
Motivation: 应对隐私合规和安全要求，能够在不长时间停机的情况下从模型中清除单一客户端的贡献，同时降低多轮通信带来的开销。

Method: 将Task Arithmetic与NTK结合，直接在模型近似或线性化层面对某客户的影响进行分离与消除，减少或消除需要多轮通信的过程。

Result: 摘要未给出具体实验结果，但声称可以快速删除客户影响，降低停机时间，提升系统可用性。

Conclusion: 所提方法有望实现隐私保护的高效分布式学习中的快速贡献去除，减少对系统服务的干扰。

Abstract: Nowdays, there are an abundance of portable devices capable of collecting
large amounts of data and with decent computational power. This opened the
possibility to train AI models in a distributed manner, preserving the
participating clients' privacy. However, because of privacy regulations and
safety requirements, elimination upon necessity of a client contribution to the
model has become mandatory. The cleansing process must satisfy specific
efficacy and time requirements. In recent years, research efforts have produced
several knowledge removal methods, but these require multiple communication
rounds between the data holders and the process coordinator. This can cause the
unavailability of an effective model up to the end of the removal process,
which can result in a disservice to the system users. In this paper, we
introduce an innovative solution based on Task Arithmetic and the Neural
Tangent Kernel, to rapidly remove a client's influence from a model.

</details>


### [104] [Manifold Decoders: A Framework for Generative Modeling from Nonlinear Embeddings](https://arxiv.org/abs/2510.13622)
*Riddhish Thakare,Kingdom Mutala Akugri*

Main category: cs.LG

TL;DR: 提出一个框架来为经典NLDR方法（如t-SNE、Isomap、LLE）构建神经解码器，实现双向映射，并在学习到的流形空间内引入基于扩散的生成过程。通过CelebA上与自编码器及标准扩散模型基线的比较，揭示解码器在重建上可行但通常不及端到端自编码器；在流形约束下的扩散生成样本质量较差，表明离散且稀疏的经典NLDR嵌入不利于连续插值。结论指出将生成能力回填到NLDR方法中存在固有挑战。


<details>
  <summary>Details</summary>
Motivation: 解决NLDR方法（一维降维可视化）的单向性问题，寻求将降维映射回高维空间的能力，以便用于生成任务；并探索在学习到的流形上进行生成的可行性及局限性。

Method: 提出一个系统化框架，用于为主流NLDR方法构建神经解码器，以实现双向映射；在此框架下实现一个在学习到的流形空间内工作的扩散式生成过程；在CelebA数据集上评估重建和生成性能，并与自编码器和标准扩散模型基线进行比较。

Result: 解码器能够对原始数据进行重建，但重建质量通常落后于端到端优化的自编码器；在流形约束下进行的扩散生成样本质量较差，原因在于NLDR嵌入的离散性和稀疏性不利于连续插值。

Conclusion: 将生成能力移植到NLDR方法中存在内在难题，NLDR更多是面向可视化与分析的工具，二者的耦合需要新的设计思想，当前方法在平衡重建与生成之间存在明显权衡。

Abstract: Classical nonlinear dimensionality reduction (NLDR) techniques like t-SNE,
Isomap, and LLE excel at creating low-dimensional embeddings for data
visualization but fundamentally lack the ability to map these embeddings back
to the original high-dimensional space. This one-way transformation limits
their use in generative applications. This paper addresses this critical gap by
introducing a system- atic framework for constructing neural decoder
architectures for prominent NLDR methods, enabling bidirectional mapping for
the first time. We extend this framework by implementing a diffusion-based
generative process that operates directly within these learned manifold spaces.
Through experiments on the CelebA dataset, we evaluate the reconstruction and
generative performance of our approach against autoencoder and standard
diffusion model baselines. Our findings reveal a fundamental trade- off: while
the decoders successfully reconstruct data, their quality is surpassed by
end-to-end optimized autoencoders. Moreover, manifold-constrained diffusion
yields poor-quality samples, suggesting that the discrete and sparse nature of
classical NLDR embeddings is ill-suited for the continuous inter- polation
required by generative models. This work highlights the inherent challenges in
retrofitting generative capabilities onto NLDR methods designed primarily for
visualization and analysis.

</details>


### [105] [Multivariate Time Series Forecasting with Gate-Based Quantum Reservoir Computing on NISQ Hardware](https://arxiv.org/abs/2510.13634)
*Wissal Hamhoum,Soumaya Cherkaoui,Jean-Frederic Laprade,Ola Ahmed,Shengrui Wang*

Main category: cs.LG

TL;DR: Gate-based quantum reservoir computing for multivariate time series (MTS-QRC) using a Trotterized transverse-field Ising evolution with injection and memory qubits. Demonstrates competitive forecasting on Lorenz-63 and ENSO datasets and shows hardware noise can act as a regularizer on NISQ devices, enabling practical MTS forecasting on IBM hardware.


<details>
  <summary>Details</summary>
Motivation: Extend quantum reservoir computing to practical multivariate time series forecasting under near-term hardware constraints, and design a hardware-aware, gate-based QRC that can operate within current device connectivity and depth limits.

Method: A gate-based QRC architecture that pairs injection and memory qubits. Evolution is implemented via a Trotterized nearest-neighbor transverse-field Ising model optimized for device connectivity and depth. Empirical evaluation on Lorenz-63 and ENSO datasets, including runs on IBM hardware (Heron R2).

Result: MTS-QRC achieves MSE of 0.0087 (Lorenz-63) and 0.0036 (ENSO). It matches classical reservoir computing performance on Lorenz-63 and surpasses learned RNNs on both datasets; NVAR and clustered ESN perform better in some settings. On IBM Heron R2, the method maintains accuracy with realistic depths and even outperforms a noiseless simulator on ENSO. Singular value analysis indicates device noise concentrates variance in feature directions, acting as an implicit regularizer for linear readout.

Conclusion: Gate-based QRC for multivariate time series forecasting is practical on NISQ hardware, supporting broader applicability and motivating systematic studies on when and how hardware noise benefits QRC readouts.

Abstract: Quantum reservoir computing (QRC) offers a hardware-friendly approach to
temporal learning, yet most studies target univariate signals and overlook
near-term hardware constraints. This work introduces a gate-based QRC for
multivariate time series (MTS-QRC) that pairs injection and memory qubits and
uses a Trotterized nearest-neighbor transverse-field Ising evolution optimized
for current device connectivity and depth. On Lorenz-63 and ENSO, the method
achieves a mean square error (MSE) of 0.0087 and 0.0036, respectively,
performing on par with classical reservoir computing on Lorenz and above
learned RNNs on both, while NVAR and clustered ESN remain stronger on some
settings. On IBM Heron R2, MTS-QRC sustains accuracy with realistic depths and,
interestingly, outperforms a noiseless simulator on ENSO; singular value
analysis indicates that device noise can concentrate variance in feature
directions, acting as an implicit regularizer for linear readout in this
regime. These findings support the practicality of gate-based QRC for MTS
forecasting on NISQ hardware and motivate systematic studies on when and how
hardware noise benefits QRC readouts.

</details>


### [106] [What is the objective of reasoning with reinforcement learning?](https://arxiv.org/abs/2510.13651)
*Damek Davis,Benjamin Recht*

Main category: cs.LG

TL;DR: 将面向二元奖励的强化学习算法在大型语言模型中的执行过程，统一看作是在概率正确回答的单调变换上进行随机梯度上升。具体地，拒绝采样相关算法对应的变换是对数函数，GRPO算法对应的变换是平方根的反正弦函数 arcsin。


<details>
  <summary>Details</summary>
Motivation: 揭示在大模型上的二元奖励强化学习中，不同算法的梯度更新其实对应一个相同的优化对象的单调变换形式，从而统一理解它们的学习动力学与设计取向。

Method: 给出一个理论分析框架，将梯度上升视为对 p = P(正确回答|提示) 的某个单调变换 g(p) 的梯度的上升；推导出在拒绝采样与 GRPO 等具体算法下，g(p) 分别为 log(p) 和 arcsin(sqrt(p))。

Result: 确立了一种统一的视角：多种知名的 RLLM（二元奖励）算法可视为对单一被单调变换的概率目标进行的梯度上升；并明确了具体的变换形式。

Conclusion: 该框架有助于理解不同算法之间的关系、比较其学习动力学，并为设计新算法时的奖励变换选择提供理论依据。

Abstract: We show that several popular algorithms for reinforcement learning in large
language models with binary rewards can be viewed as stochastic gradient ascent
on a monotone transform of the probability of a correct answer given a prompt.
In particular, the transformation associated with rejection sampling algorithms
is the logarithm and that associated with the GRPO algorithm is the arcsine of
the square root.

</details>


### [107] [Time Series Foundation Models: Benchmarking Challenges and Requirements](https://arxiv.org/abs/2510.13654)
*Marcel Meyer,Sascha Kaltenpoth,Kevin Zalipski,Oliver Müller*

Main category: cs.LG

TL;DR: 现有时序基金模型的评估面临数据泄露、基准样本代表性不足及跨时空泛化等挑战，需引入真正未来数据外部测试等鲁棒评估以确保公正性。


<details>
  <summary>Details</summary>
Motivation: 为保障TSFM评估的可信性，系统揭示现有基准在数据划分、数据重叠、信息泄露、对全球冲击记忆化等方面的不足，防止评估偏差与错误的知识迁移。

Method: 对现有TSFM评估实践进行综合分析与批判性综述，识别数据划分、数据集重叠、信息泄露、外部冲击导致的记忆化等风险，并提出以未来数据为基础的真正外样本评估等鲁棒方法论方向。

Result: 研究发现评估实践存在流程混乱、数据分区不清、潜在信息泄露与全局模式记忆等问题，可能导致性能高估并错误地将全球知识迁移到局部时间序列。

Conclusion: 呼吁设计新的、原则性强的评估框架，如在真正的未来数据上进行测试，以提升TSFM评估的可信度、可重复性与公正性。

Abstract: Time Series Foundation Models (TSFMs) represent a new paradigm for time
series forecasting, offering zero-shot forecasting capabilities without the
need for domain-specific pre-training or fine-tuning. However, as with Large
Language Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive
training sets, it becomes more and more challenging to ensure the integrity of
benchmarking data. Our investigation of existing TSFM evaluation highlights
multiple challenges, ranging from the representativeness of the benchmark
datasets, over the lack of spatiotemporal evaluation, to risks of information
leakage due to overlapping and obscure datasets, and the memorization of global
patterns caused by external shocks like economic crises or pandemics. Our
findings reveal widespread confusion regarding data partitions, risking
inflated performance estimates and incorrect transfer of global knowledge to
local time series. We argue for the development of robust evaluation
methodologies to prevent pitfalls already observed in LLM and classical time
series benchmarking, and call upon the research community to design new,
principled approaches, such as evaluations on truly out-of-sample future data,
to safeguard the integrity of TSFM assessment.

</details>


### [108] [Adam or Gauss-Newton? A Comparative Study In Terms of Basis Alignment and SGD Noise](https://arxiv.org/abs/2510.13680)
*Bingbin Liu,Rachit Bansal,Depen Morwani,Nikhil Vyas,David Alvarez-Melis,Sham M. Kakade*

Main category: cs.LG

TL;DR: 本文对比分析了基于 Adam 与 Gauss-Newton 的对角预条件方法，在预条件器的基底选择与小批量梯度噪声的影响下，揭示了两者在不同设置下的表现差异：在全批量情况下，存在 Adam 优于 GN^{-1} 与 GN^{-1/2} 的实例；在随机情形下，Adam 的行为与 GN^{-1/2} 相近，理论与经验均支持该结论。


<details>
  <summary>Details</summary>
Motivation: 旨在厘清对角预条件方法在深度学习训练中的表现差异，尤其关注预条件器基底的选择和梯度噪声对优化效果的影响，并将基于 Adam 的统计预估与基于 Gauss-Newton 的曲率信息进行系统比较。希望为选择对角预条件器提供理论与实践指南。

Method: 在二次目标函数与逻辑回归问题上，考虑四个象限中各种符号情形，比较 Adam、GN^{-1}、GN^{-1/2} 及其不同基底的对角近似。分析全批量与随机梯度两种设置，给出在不同条件下的理论结论，并在凸与非凸目标上给出经验验证。

Result: 理论层面：存在在某些情形下 Adam 优于两类 Gauss-Newton 对角预条件器的情况；随机/噪声场景下，Adam 的行为与 GN^{-1/2} 在高斯数据假设的线性回归中相似。经验层面：在凸与非凸任务上，理论结论得到支持，表明对角预条件策略的相对优劣取决于目标特性、基底选择以及小批量噪声水平。

Conclusion: 不能简单地宣称某一对角预条件器始终优于另一种，而应根据目标函数性质、是否全批量、以及数据分布与噪声水平选择合适的对角近似；本工作为在不同基底与噪声条件下的对角预条件器提供了统一的理论框架和实证依据。

Abstract: Diagonal preconditioners are computationally feasible approximate to
second-order optimizers, which have shown significant promise in accelerating
training of deep learning models. Two predominant approaches are based on Adam
and Gauss-Newton (GN) methods: the former leverages statistics of current
gradients and is the de-factor optimizers for neural networks, and the latter
uses the diagonal elements of the Gauss-Newton matrix and underpins some of the
recent diagonal optimizers such as Sophia.
  In this work, we compare these two diagonal preconditioning methods through
the lens of two key factors: the choice of basis in the preconditioner, and the
impact of gradient noise from mini-batching. To gain insights, we analyze these
optimizers on quadratic objectives and logistic regression under all four
quadrants. We show that regardless of the basis, there exist instances where
Adam outperforms both GN$^{-1}$ and GN$^{-1/2}$ in full-batch settings.
Conversely, in the stochastic regime, Adam behaves similarly to GN$^{-1/2}$ for
linear regression under a Gaussian data assumption. These theoretical results
are supported by empirical studies on both convex and non-convex objectives.

</details>


### [109] [Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking](https://arxiv.org/abs/2510.13694)
*Yuchun Miao,Liang Ding,Sen Zhang,Rong Bao,Lefei Zhang,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈的奖励建模框架InfoRM，以在奖励建模阶段过滤偏好无关信息，缓解奖励误泛化；并引入IB空间下的分布级正则IBL，以及用于衡量奖励黑客程度的MOP指标，理论上IBL等价于悲观RL在IB潜空间的目标；通过MOP实现在线诊断与早停等实用手段，在多数据集/模型上验证有效性。


<details>
  <summary>Details</summary>
Motivation: RLHF在对齐语言模型价值观时仍存在奖励黑客的问题，原因在于奖励建模的误泛化和RL优化阶段缺乏合适的正则化。需要一个信息论基础的框架来过滤偏好无关信息，并通过分布级正则化拓展优化空间，同时提供诊断工具以量化和抑制奖励黑客。

Method: 提出InfoRM：基于信息瓶颈的奖励建模框架，过滤信息以减少对偏好无关特征的过拟合；观察InfoRM的IB潜在空间中奖励黑客样本表现为Mahalanobis距离的异常点，进而提出IBL：分布级正则化，惩罚此类偏离并扩展优化空间；证明IBL在IB潜空间下等价于悲观RL目标；提出MOP：用于量化奖励黑客严重程度的统计度量，便于超参调优和在线 mitigation（如早停）。

Result: 在多种大语言模型和数据集上进行广泛实验证明InfoRM和IBL具有普遍性和有效性，MOP作为诊断工具具有可靠性，整体提升RLHF的对齐性和鲁棒性。

Conclusion: 本文提出的InfoRM-IBL体系为缓解奖励黑客提供了理论和实践上的新路径，通过信息瓶颈过滤冗余信息并以分布级正则化扩展优化空间，同时给出可量化的MOP工具，推动RLHF在不同场景下的可靠应用。

Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in
aligning language models with human values, reward hacking-or reward
over-optimization-remains a major challenge. We identify two key obstacles to
its mitigation: (1) reward misgeneralization in reward modeling, where reward
models overfit to spurious, preference-irrelevant features; and (2) the lack of
suitable regularization during RL optimization, as existing token-level
constraints often over-restrict the policy space. To address these issues, we
propose InfoRM, an information-theoretic reward modeling framework based on the
Information Bottleneck (IB) principle, which filters out preference-irrelevant
information to alleviate reward misgeneralization. We further observe that
reward-hacked responses manifest as pronounced outliers in InfoRM's IB latent
space, measured by Mahalanobis distance from the SFT-induced distribution.
Motivated by this, we introduce IBL, a distribution-level regularization that
penalizes such deviations, effectively expanding the optimization landscape
while maintaining alignment. We prove that IBL is theoretically equivalent to
the pessimistic RL objective within the IB latent space. Finally, we present
Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying
reward hacking severity, enabling principled hyperparameter tuning and online
mitigation such as early stopping. Extensive experiments across diverse LLMs
and datasets confirm the generality of our findings, the effectiveness of
InfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively
advancing the state of RLHF.

</details>


### [110] [Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents](https://arxiv.org/abs/2510.13704)
*Johan Obando-Ceron,Walter Mayor,Samuel Lavoie,Scott Fujimoto,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 提出使用 simplicial embeddings 的几何诱导偏置来提高强化学习样本效率与最终性能，适用于 FastTD3、FastSAC、PPO，在多种连续与离散控制环境中均能提升且不牺牲运行速度。


<details>
  <summary>Details</summary>
Motivation: 现有通过大规模环境并行化来加速墙钟时间的做法在达到目标表现前仍需大量环境交互；因此需要更高效的表示来提升泛化和样本效率。

Method: 引入轻量级表示层，将嵌入约束在单纯形（simplicial）结构，产生稀疏离散特征，从而稳定 critic bootstrapping 并强化策略梯度；把 simplicial embeddings 应用于 FastTD3、FastSAC 与 PPO，提升样本效率与最终性能。

Result: 在多种连续与离散控制环境中，s simplicial embeddings 能一致提升样本效率和最终性能，且不影响运行时速度。

Conclusion: 通过引入几何结构偏置的 simplicial embeddings，可提升多算法、多环境下的强化学习样本效率与鲁棒性。

Abstract: Recent works have proposed accelerating the wall-clock training time of
actor-critic methods via the use of large-scale environment parallelization;
unfortunately, these can sometimes still require large number of environment
interactions to achieve a desired level of performance. Noting that
well-structured representations can improve the generalization and sample
efficiency of deep reinforcement learning (RL) agents, we propose the use of
simplicial embeddings: lightweight representation layers that constrain
embeddings to simplicial structures. This geometric inductive bias results in
sparse and discrete features that stabilize critic bootstrapping and strengthen
policy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial
embeddings consistently improve sample efficiency and final performance across
a variety of continuous- and discrete-control environments, without any loss in
runtime speed.

</details>


### [111] [Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe](https://arxiv.org/abs/2510.13713)
*Christophe Roux,Max Zimmer,Alexandre d'Aspremont,Sebastian Pokutta*

Main category: cs.LG

TL;DR: Convex relaxation of pruning masks with Frank-Wolfe for LLM pruning; improves per-layer pruning error without full retraining, via rounding the relaxed solution to integrality.


<details>
  <summary>Details</summary>
Motivation: Pruning reduces compute/storage but retraining is expensive; LLM pruning uses layer-wise masks on small calibration data, but the combinatorial mask selection is intractable and existing greedy methods ignore weight interactions.

Method: Relax the pruning mask constraints into a convex set and solve the resulting problem with the Frank-Wolfe algorithm; apply a layer-wise scheme and, after optimization, round the relaxed (continuous) solution to an integral pruning mask.

Result: Significant reduction in per-layer pruning error; outperforms strong baselines on state-of-the-art GPT architectures; memory-efficient.

Conclusion: With FW convergence guarantees, the rounded relaxed solution yields an approximate solution to the original combinatorial problem, combining theoretical justification with practical performance.

Abstract: Pruning is a common technique to reduce the compute and storage requirements
of Neural Networks. While conventional approaches typically retrain the model
to recover pruning-induced performance degradation, state-of-the-art Large
Language Model (LLM) pruning methods operate layer-wise, minimizing the
per-layer pruning error on a small calibration dataset to avoid full
retraining, which is considered computationally prohibitive for LLMs. However,
finding the optimal pruning mask is a hard combinatorial problem and solving it
to optimality is intractable. Existing methods hence rely on greedy heuristics
that ignore the weight interactions in the pruning objective. In this work, we
instead consider the convex relaxation of these combinatorial constraints and
solve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method
drastically reduces the per-layer pruning error, outperforms strong baselines
on state-of-the-art GPT architectures, and remains memory-efficient. We provide
theoretical justification by showing that, combined with the convergence
guarantees of the FW algorithm, we obtain an approximate solution to the
original combinatorial problem upon rounding the relaxed solution to
integrality.

</details>


### [112] [Assessing the Geographic Generalization and Physical Consistency of Generative Models for Climate Downscaling](https://arxiv.org/abs/2510.13722)
*Carlo Saccardi,Maximilian Pierzyna,Haitz Sáez de Ocáriz Borde,Simone Monaco,Cristian Meo,Pietro Liò,Rudolf Saathof,Geethu Joseph,Justin Dauwels*

Main category: cs.LG

TL;DR: 尽管一些深度学习降尺度模型在常规指标上表现良好，但在地理泛化与物理一致性方面存在显著缺陷；引入的功率谱密度（PSD）损失在经验上提升了跨区域泛化和对小尺度物理结构的重建能力。


<details>
  <summary>Details</summary>
Motivation: 需要更可靠的气候降尺度方法，并确保物理约束的一致性。当前以标准机器学习指标评估模型，难以反映大气物理特性与跨区域泛化能力。

Method: 基于现有前沿 DL 降尺度模型（如 CorrDiff）进行基准测试，提出以物理为导向的诊断，并引入 PSD 损失以促进对小尺度结构的再现，从而提升地理泛化与物理一致性。

Result: 结果显示：只在欧洲地理区域训练的模型难以泛化到伊比利亚、摩洛哥、斯堪的纳维亚等区域；预测场的散度和旋度等二阶变量难以从速度场预测中准确捕获；这种问题甚至在分布内地理样本中也存在；PSD 损失在经验上提高了地理泛化和对小尺度结构的重建能力。

Conclusion: 物理感知的评估框架揭示了 DL 降尺度的局限性；PSD 损失提供一种简单而有效的改进途径，并有公开代码支持复现实验。

Abstract: Kilometer-scale weather data is crucial for real-world applications but
remains computationally intensive to produce using traditional weather
simulations. An emerging solution is to use deep learning models, which offer a
faster alternative for climate downscaling. However, their reliability is still
in question, as they are often evaluated using standard machine learning
metrics rather than insights from atmospheric and weather physics. This paper
benchmarks recent state-of-the-art deep learning models and introduces
physics-inspired diagnostics to evaluate their performance and reliability,
with a particular focus on geographic generalization and physical consistency.
Our experiments show that, despite the seemingly strong performance of models
such as CorrDiff, when trained on a limited set of European geographies (e.g.,
central Europe), they struggle to generalize to other regions such as Iberia,
Morocco in the south, or Scandinavia in the north. They also fail to accurately
capture second-order variables such as divergence and vorticity derived from
predicted velocity fields. These deficiencies appear even in in-distribution
geographies, indicating challenges in producing physically consistent
predictions. We propose a simple initial solution: introducing a power spectral
density loss function that empirically improves geographic generalization by
encouraging the reconstruction of small-scale physical structures. The code for
reproducing the experimental results can be found at
https://github.com/CarloSaccardi/PSD-Downscaling

</details>


### [113] [Asymptotically optimal reinforcement learning in Block Markov Decision Processes](https://arxiv.org/abs/2510.13748)
*Thomas van Vuren,Fiona Sloothaak,Maarten G. Wolf,Jaron Sanders*

Main category: cs.LG

TL;DR: 在可被聚类还原潜在状态的区块马尔可夫决策过程(BMDP)中，提出一个两阶段 RL 算法。通过随机探索学习潜在结构再采用面向揭示结构的乐观策略，得到对这类 BMDP 的渐近最优/接近最优的遗憾界，达到 O(√T + n)，相比之前的 O(√T + n^2) 有显著提升。并且对同类问题给出无更低遗憾的下界，证明该方法在此类问题上的渐近最优性。


<details>
  <summary>Details</summary>
Motivation: 高维状态与动作空间使 RL 的样本效率和计算成本不可接受。很多环境具有可利用的结构性（潜在状态被观测到的定义）——通过聚类等方法可以有效地恢复潜在结构。本文的动机是对这类结构化 RL 进行理论遗憾分析，量化聚类对学习性能的影响。

Method: 提出一个两阶段算法：第一阶段通过随机探索来学习潜在结构并进行聚类，将观测映射到潜在状态；第二阶段在揭示的结构上使用乐观策略（以 uncovered 的结构为基础进行策略优化）。对该算法给出理论遗憾界，表明在可聚类的 BMDP 类中，准确的潜在状态估计能显著加速学习。

Result: 对一大类易于聚类的 BMDP，遗憾界为 O(√T + n)，其中 T 为时间步数，n 为观测空间基数。该界比先前的 O(√T + n^2) 有所改善，尤其在 n 较大时更明显。还给出相应的下界，表明在该类问题上不存在普遍更低的遗憾，因此该算法在该类问题上是渐近最优的。

Conclusion: 证明了通过聚类学习潜在状态能够有效提速 RL，在 BMDP 的特定结构性类别中，利用结构信息的学习策略可以显著降低样本复杂度并达到渐近最优。

Abstract: The curse of dimensionality renders Reinforcement Learning (RL) impractical
in many real-world settings with exponentially large state and action spaces.
Yet, many environments exhibit exploitable structure that can accelerate
learning. To formalize this idea, we study RL in Block Markov Decision
Processes (BMDPs). BMDPs model problems with large observation spaces, but
where transition dynamics are fully determined by latent states. Recent
advances in clustering methods have enabled the efficient recovery of this
latent structure. However, a regret analysis that exploits these techniques to
determine their impact on learning performance remained open. We are now
addressing this gap by providing a regret analysis that explicitly leverages
clustering, demonstrating that accurate latent state estimation can indeed
effectively speed up learning.
  Concretely, this paper analyzes a two-phase RL algorithm for BMDPs that first
learns the latent structure through random exploration and then switches to an
optimism-guided strategy adapted to the uncovered structure. This algorithm
achieves a regret that is $O(\sqrt{T}+n)$ on a large class of BMDPs susceptible
to clustering. Here, $T$ denotes the number of time steps, $n$ is the
cardinality of the observation space, and the Landau notation $O(\cdot)$ holds
up to constants and polylogarithmic factors. This improves the best prior
bound, $O(\sqrt{T}+n^2)$, especially when $n$ is large. Moreover, we prove that
no algorithm can achieve lower regret uniformly on this same class of BMDPs.
This establishes that, on this class, the algorithm achieves asymptotic
optimality.

</details>


### [114] [The Art of Scaling Reinforcement Learning Compute for LLMs](https://arxiv.org/abs/2510.13786)
*Devvrit Khatri,Lovish Madaan,Rishabh Tiwari,Rachit Bansal,Sai Surya Duvvuri,Manzil Zaheer,Inderjit S. Dhillon,David Brandfonbrener,Rishabh Agarwal*

Main category: cs.LG

TL;DR: Large-scale RL for LLMs requires predictive scaling; through 400k GPU-hours, the study builds a framework to analyze and predict RL scaling, introduces ScaleRL, and demonstrates extrapolation to 100k GPU-hours.


<details>
  <summary>Details</summary>
Motivation: There is no principled, predictive scaling methodology for RL compute in LLMs despite increasing compute budgets; need to understand how algorithmic changes affect asymptotic performance and efficiency.

Method: Perform a large-scale systematic study, fit sigmoidal compute–performance curves for RL training, and ablate a wide range of design choices (loss aggregation, normalization, curriculum, off-policy algorithms) to assess effects on asymptote and efficiency.

Result: Not all recipes reach the same asymptotic performance; loss aggregation/normalization/curriculum/off-policy choices mainly affect compute efficiency rather than the asymptote; stable recipes show predictable scaling trajectories enabling extrapolation from smaller runs; a best-practice ScaleRL recipe is proposed and validated by scaling to 100,000 GPU-hours and predicting validation performance.

Conclusion: Provides a scientific framework for analyzing RL scaling and a practical recipe that improves predictability of RL training, bringing RL scaling closer to the predictability seen in pre-training.

Abstract: Reinforcement learning (RL) has become central to training large language
models (LLMs), yet the field lacks predictive scaling methodologies comparable
to those established for pre-training. Despite rapidly rising compute budgets,
there is no principled understanding of how to evaluate algorithmic
improvements for scaling RL compute. We present the first large-scale
systematic study, amounting to more than 400,000 GPU-hours, that defines a
principled framework for analyzing and predicting RL scaling in LLMs. We fit
sigmoidal compute-performance curves for RL training and ablate a wide range of
common design choices to analyze their effects on asymptotic performance and
compute efficiency. We observe: (1) Not all recipes yield similar asymptotic
performance, (2) Details such as loss aggregation, normalization, curriculum,
and off-policy algorithm primarily modulate compute efficiency without
materially shifting the asymptote, and (3) Stable, scalable recipes follow
predictable scaling trajectories, enabling extrapolation from smaller-scale
runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and
demonstrate its effectiveness by successfully scaling and predicting validation
performance on a single RL run scaled up to 100,000 GPU-hours. Our work
provides both a scientific framework for analyzing scaling in RL and a
practical recipe that brings RL training closer to the predictability long
achieved in pre-training.

</details>
