<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 89]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.IT](#cs.IT) [Total: 8]
- [eess.SP](#eess.SP) [Total: 8]
- [eess.SY](#eess.SY) [Total: 12]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Energy-Driven Steering: Reducing False Refusals in Large Language Models](https://arxiv.org/abs/2510.08646)
*Eric Hanchen Jiang,Weixuan Ou,Run Liu,Shengyuan Pang,Guancheng Wan,Ranjie Duan,Wei Dong,Kai-Wei Chang,XiaoFeng Wang,Ying Nian Wu,Xinfeng Li*

Main category: cs.LG

TL;DR: 提出 Energy-Driven Steering (EDS) 通过外部能量模型在推理时对隐藏状态进行梯度引导，以降低 false refusals 同时保留安全性，无需微调，实验上在 ORB-H 基准等显著提升合规性（如从57.3%提升到82.6%）并保持安全性基线。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的对齐研究多聚焦于对有害提示的安全性，但容易导致对无害提示过度谨慎，产生大量拒答。需要在提高安全性的同时减少误拒答，使模型在保留知识的前提下更愿意对无害请求作出合理回应。

Method: 训练一个轻量级外部的能量模型（EBM）来给出高能量的不可取状态（如误拒或越狱）和低能量的可取状态（有用回应或安全拒绝）。在推理时，将 LLM 的内部激活映射到一个能量景观，利用能量函数的梯度动态地引导隐藏状态进入低能量区域，从而在不修改模型权重的情况下实时纠正输出。

Result: 在广泛模型上实验证明显著降低误拒率，并在 ORB-H 基准上将合规性从 57.3% 提升至 82.6%，同时保持基线安全性能。

Conclusion: EDS 提供了一种灵活且低开销的框架，能够在不改变模型权重的前提下实现更低的误拒与高安全性，是构建兼具低误拒和高安全性的 LLM 的有效范式。

Abstract: Safety alignment of large language models (LLMs) faces a key challenge:
current alignment techniques often only focus on improving safety against
harmful prompts, causing LLMs to become over-cautious and refuse to respond to
benign prompts. Therefore, a key objective of safe alignment is to enhance
safety while simultaneously reducing false refusals. In this paper, we
introduce Energy-Driven Steering (EDS), a novel, fine-tuning free framework
designed to resolve this challenge through dynamic, inference-time
intervention. We trained a lightweight, external Energy-Based Model (EBM) to
assign high energy to undesirable (false refusal or jailbreak) states and low
energy to desirable (helpful response or safe reject) ones. During inference,
EBM maps the LLM's internal activations to an "energy landscape". We use the
gradient of the energy function to dynamically steer the LLM's hidden states to
low energy regions, correcting the model to generate a desirable response in
real-time without modifying its weights. This method decouples behavioral
control from the model's core knowledge, offering a flexible solution with
minimal computational overhead. Extensive experiments across a wide range of
models show our method successfully achieves this objective: it substantially
lowers false refusal rates. For example, raising compliance on the ORB-H
benchmark from 57.3% to 82.6% while maintaining the baseline safety
performance. Our work presents an effective paradigm for building LLMs that
achieve both low false refusal rates and high safety.

</details>


### [2] [Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance and Order Sensitivity](https://arxiv.org/abs/2510.08648)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.LG

TL;DR: WILSON 提供一个廉价、事后诊断套件，用以检测 Transformers 的不变量违反现象（如重排序的顺序敏感、微调引发的回归、上下文长度和对话路径的影响），通过将反向无曲率映射（跨位置和层的逆向无曲率）与激活级别的对位量（commutators）结合，生成可用于系统监控的信号，帮助在不改动模型架构或重新训练的前提下，对 RAG、对话路径稳定性与部署时的重排序做出保障。工具输出以阈值与 CSV 形式导出，便于编排与落地。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在无害编辑、段落重排、微调以及路径引导（如辩论、链式推理提示）中仍会改变答案，这些对齐与不变量在实践中被破坏，影响可靠性和安全性；现有对策（重新训练、形式化验证）成本高且覆盖面有限，因此需要一个轻量级的、事后可用的诊断工具来预测失败、引导安全优化。

Method: 提出 WILSON：在标准 Transformer 上进行轻量化、事后诊断。通过对位置和层级的逆向曲率映射（使用 Jacobian-向量积 JVP 与 Hutchinson 估计）来捕获敏感区域，结合激活层面的 commutator 来标记重排序风险；信号计算成本低，模型无关（对标准 Transformer 适用），输出阈值与 CSV 供调度系统使用。

Result: 信号可用于具体行动：对抗 RAG 的顺序效应、检测微调带来的回归、稳定长期上下文与多轮对话路径、以及在部署阶段对融合或重排序进行门控；实现后无需修改模型架构或训练，即可提前识别潜在失败并实现安全的优化，提升可靠性与吞吐。

Conclusion: WILSON 提供一个可广泛部署且成本低廉的事后诊断框架，使得在不改变训练或架构的前提下，能够预见性地发现并缓解模型的不变量破坏，从而提升系统可靠性与部署效率。

Abstract: Large language models can change answers under harmless edits that matter in
practice: RAG outputs flip when passages are reordered, fine-tuning erodes
invariances learned at pretraining, debate or chain-of-thought prompts take
path-dependent routes, and compiler fusion or reordering perturbs logits near
decision boundaries. These failures violate intended invariances, break
continuous integration, and force teams to trade safety for speed. The effects
are small yet distributed across layers and positions, sensitive to context
length and evaluation order, and costly to repair with retraining or formal
verification. We present WILSON, a minimal post-hoc diagnostic suite that
converts simple loop and reordering checks on internal representations into
system signals. WILSON combines an inverse-free curvature map over positions
and layers, computed with JVPs and Hutchinson probes, with activation-level
commutators that flag reorder risk. Signals are cheap to compute,
model-agnostic for standard Transformers, and exported as thresholds and CSV
artifacts for orchestrators. This enables concrete actions: guard RAG against
order effects, catch fine-tuning regressions, stabilize debate pathways and
long multi-turn contexts, and gate fusions or reorders in deployment. In short,
WILSON helps anticipate failures and approve safe optimizations so reliability
and throughput can improve together without changing model architecture or
training.

</details>


### [3] [Velocity and Density-Aware RRI Analysis and Optimization for AoI Minimization in IoV SPS](https://arxiv.org/abs/2510.08911)
*Maoxin Ji,Tong Wang,Qiong Wu,Pingyi Fan,Nan Cheng,Wen Chen*

Main category: cs.LG

TL;DR: 提出基于LLM与DDPG的双路径优化框架，在SPS场景下通过考虑车辆速度、密度和RRI来降低IoV中的AoI。LLM在少量示例下即可给出参数配置，无需额外训练；DDPG经训练后实现更稳定的性能。


<details>
  <summary>Details</summary>
Motivation: IoV中Semi-Persistent Scheduling (SPS) 的AoI易受车辆动态和信道不确定性影响，需结合快速参数配置和长期策略优化来提升信息 freshness。

Method: 建立考虑速度、密度和资源预留间隔（RRI）的AoI模型；设计双路径优化：DDPG用于状态-奖励驱动的策略学习，LLM通过上下文学习生成最优参数配置，且LLM在无需额外模型训练的情况下即可工作。

Result: 实验显示，LLM在仅需少量范例的情况下显著降低AoI且无需额外训练；DDPG在训练后实现更稳健的性能表现。

Conclusion: 将LLM与DDPG结合，能够在IoV的SPS场景中实现快速、稳健的AoI优化，兼具快速参数配置与长期策略学习能力。

Abstract: Addressing the problem of Age of Information (AoI) deterioration caused by
packet collisions and vehicle speed-related channel uncertainties in
Semi-Persistent Scheduling (SPS) for the Internet of Vehicles (IoV), this
letter proposes an optimization approach based on Large Language Models (LLM)
and Deep Deterministic Policy Gradient (DDPG). First, an AoI calculation model
influenced by vehicle speed, vehicle density, and Resource Reservation Interval
(RRI) is established, followed by the design of a dual-path optimization
scheme. The DDPG is guided by the state space and reward function, while the
LLM leverages contextual learning to generate optimal parameter configurations.
Experimental results demonstrate that LLM can significantly reduce AoI after
accumulating a small number of exemplars without requiring model training,
whereas the DDPG method achieves more stable performance after training.

</details>


### [4] [Provably Robust Adaptation for Language-Empowered Foundation Models](https://arxiv.org/abs/2510.08659)
*Yuni Lai,Xiaoyu Xue,Linghui Shen,Yulun Wu,Gaolei Li,Song Guo,Kai Zhou,Bin Xiao*

Main category: cs.LG

TL;DR: 提出 LeFCert：面向语言增强的基础模型的首个可证明鲁棒的少-shot 分类器。通过结合文本与特征嵌入、两步截尾均值原型，并给出分类分数的上/下界，实现对最坏 poisoning 情况的认证。并扩展出 LeFCert-L（引入随机平滑，双预算鲁棒性）与 LeFCert-C（对多样本共享攻击预算的集合认证）。实验显示在清洁准确率与认证准确率方面超越基线，且计算高效。


<details>
  <summary>Details</summary>
Motivation: 针对在开放环境中用于少样本支持集的语言增强基础模型（LeFMs）可能遭受的数据污染攻击，现有防御缺乏形式化保证，难以对未见与自适应攻击提供鲁棒性认证。提出可证明鲁棒性的少-shot 分类器，以提升 LeFM 体系的可靠性。

Method: 提出 LeFCert，融合文本与特征嵌入，采用自适应混合机制。为实现可证明鲁棒性，提出二重截尾原型（twofold trimmed mean prototype），并推导分类分数的上/下界以在最坏污染场景下实现认证。扩展为 LeFCert-L：引入随机平滑以提供 Lipschitz 连续性，并在双预算约束下进行鲁棒性认证；LeFCert-C：在攻击者对多个样本分配共享攻击预算的情形下进行集合认证。

Result: 实验表明，LeFCert 在清洁和认证准确率方面显著优于现有基线，达到state-of-the-art水平，同时保持较高的计算效率。

Conclusion: 首次为 LeFM 定制的、可证明鲁棒的少-shot 分类器；具有实际可用性。LeFCert 的两种变体扩展了在更现实预算条件下的鲁棒性，适用于真实场景中的安全部署。

Abstract: Language-empowered foundation models (LeFMs), such as CLIP and GraphCLIP,
have transformed multimodal learning by aligning visual (or graph) features
with textual representations, enabling powerful downstream capabilities like
few-shot learning. However, the reliance on small, task-specific support
datasets collected in open environments exposes these models to poisoning
attacks, where adversaries manipulate the support samples to degrade
performance. Existing defenses rely on empirical strategies, which lack formal
guarantees and remain vulnerable to unseen and adaptive attacks. Certified
robustness offers provable guarantees but has been largely unexplored for
few-shot classifiers based on LeFMs. This study seeks to fill these critical
gaps by proposing the first provably robust few-shot classifier that is
tailored for LeFMs. We term our model Language-empowered Few-shot Certification
(\textbf{LeFCert}). It integrates both textual and feature embeddings with an
adaptive blending mechanism. To achieve provable robustness, we propose a
twofold trimmed mean prototype and derive provable upper and lower bounds for
classification scores, enabling certification under worst-case poisoning
scenarios. To further enhance the performance, we extend LeFCert with two
variants by considering a more realistic and tighter attack budget: LeFCert-L
incorporates randomized smoothing to provide Lipschitz continuity and derive
robustness under dual budget constraints, and LeFCert-C provides collective
certification for scenarios where attackers distribute a shared poisoning
budget across multiple samples. Experiments demonstrate that LeFCert achieves
state-of-the-art performance, significantly improving both clean and certified
accuracy compared to existing baselines. Despite its advanced robustness
mechanisms, LeFCert is computationally efficient, making it practical for
real-world applications.

</details>


### [5] [How Scale Breaks "Normalized Stress" and KL Divergence: Rethinking Quality Metrics](https://arxiv.org/abs/2510.08660)
*Kiran Smelser,Kaviru Gunaratne,Jacob Miller,Stephen Kobourov*

Main category: cs.LG

TL;DR: 对 stress（应力）和 KL 散度的尺度敏感性进行分析，并提出一种简单的归一化方法实现尺度不变性；在一个小型基准上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在高维数据降维的评估中，常用的投影质量指标（如 stress 和 KL 散度）对统一尺度缩放敏感，这会扭曲对降维方法的比较与评价。需要一种方法使评估指标对尺度变化不敏感。

Method: 对 stress 和 KL 散度在尺度缩放下的变化进行解析推导，并通过实证实验展示其影响；提出一种简单的归一化/缩放不变处理，使两者成为尺度不变的度量；在一个小型基准数据集上验证新方法的行为是否符合预期。

Result: 分析表明原始指标对尺度变化敏感；所提出的简单处理使得到的指标具有尺度不变性；在小基准上，该处理能够正确反映降维投影的质量特征，减少尺度带来的误导。

Conclusion: 尺度不变的评估指标能够更公平地比较不同降维方法的表现；所提出的简单技术具有推广性，便于在实际工作中应用于各种投影质量评估任务。

Abstract: Complex, high-dimensional data is ubiquitous across many scientific
disciplines, including machine learning, biology, and the social sciences. One
of the primary methods of visualizing these datasets is with two-dimensional
scatter plots that visually capture some properties of the data. Because
visually determining the accuracy of these plots is challenging, researchers
often use quality metrics to measure the projection's accuracy and faithfulness
to the original data. One of the most commonly employed metrics, normalized
stress, is sensitive to uniform scaling (stretching, shrinking) of the
projection, despite this act not meaningfully changing anything about the
projection. Another quality metric, the Kullback--Leibler (KL) divergence used
in the popular t-Distributed Stochastic Neighbor Embedding (t-SNE) technique,
is also susceptible to this scale sensitivity. We investigate the effect of
scaling on stress and KL divergence analytically and empirically by showing
just how much the values change and how this affects dimension reduction
technique evaluations. We introduce a simple technique to make both metrics
scale-invariant and show that it accurately captures expected behavior on a
small benchmark.

</details>


### [6] [CATS-Linear: Classification Auxiliary Linear Model for Time Series Forecasting](https://arxiv.org/abs/2510.08661)
*Zipo Jibao,Yingyi Fu,Xinyang Chen,Guoting Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent research demonstrates that linear models achieve forecasting
performance competitive with complex architectures, yet methodologies for
enhancing linear models remain underexplored. Motivated by the hypothesis that
distinct time series instances may follow heterogeneous linear mappings, we
propose the Classification Auxiliary Trend-Seasonal Decoupling Linear Model
CATS-Linear, employing Classification Auxiliary Channel-Independence (CACI).
CACI dynamically routes instances to dedicated predictors via classification,
enabling supervised channel design. We further analyze the theoretical expected
risks of different channel settings. Additionally, we redesign the
trend-seasonal decomposition architecture by adding a decoupling -- linear
mapping -- recoupling framework for trend components and complex-domain linear
projections for seasonal components. Extensive experiments validate that
CATS-Linear with fixed hyperparameters achieves state-of-the-art accuracy
comparable to hyperparameter-tuned baselines while delivering SOTA accuracy
against fixed-hyperparameter counterparts.

</details>


### [7] [FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching](https://arxiv.org/abs/2510.08669)
*Jiacheng Liu,Peiliang Cai,Qinming Zhou,Yuqi Lin,Deyang Kong,Benhao Huang,Yupei Pan,Haowen Xu,Chang Zou,Junshu Tang,Shikang Zheng,Linfeng Zhang*

Main category: cs.LG

TL;DR: FreqCa：基于频域的缓存策略，通过对低频分量进行相似性重用、对高频分量采用二阶 Hermite 插值预测，并通过缓存累积残差特征（CRF）显著降低内存和推理成本。


<details>
  <summary>Details</summary>
Motivation:  diffusion Transformer 的推理成本高昂；现有特征缓存假设邻近时间步之间存在相似性和连续性，但在实际场景中未必成立。通过频域分析揭示不同频段在时间步上的不同动态，促使提出频率感知缓存策略。

Method: 提出 Frequency-aware Caching (FreqCa)：对低频分量直接基于相似性重用特征；对高频分量使用二阶 Hermite 插值依据连续性进行预测。此外，缓存 Cumulative Residual Feature (CRF) 以降低缓存的内存占用，整体内存下降可达显著比例。

Result: 在 FLUX.1-dev、FLUX.1-Kontext-dev、Qwen-Image、Qwen-Image-Edit 上的广泛实验显示其在生成与编辑任务中的性能提升，同时显著降低内存占用（与传统缓存相比，内存消耗降低可达约99%）。

Conclusion: FreqCa 提供一种频率感知的缓存框架，降低 diffusion transformer 的推理成本与内存开销，具有良好的兼容性，代码将在补充材料/GitHub 发布。

Abstract: The application of diffusion transformers is suffering from their significant
inference costs. Recently, feature caching has been proposed to solve this
problem by reusing features from previous timesteps, thereby skipping
computation in future timesteps. However, previous feature caching assumes that
features in adjacent timesteps are similar or continuous, which does not always
hold in all settings. To investigate this, this paper begins with an analysis
from the frequency domain, which reveal that different frequency bands in the
features of diffusion models exhibit different dynamics across timesteps.
Concretely, low-frequency components, which decide the structure of images,
exhibit higher similarity but poor continuity. In contrast, the high-frequency
bands, which decode the details of images, show significant continuity but poor
similarity. These interesting observations motivate us to propose
Frequency-aware Caching (FreqCa)
  which directly reuses features of low-frequency components based on their
similarity, while using a second-order Hermite interpolator to predict the
volatile high-frequency ones based on its continuity.
  Besides, we further propose to cache Cumulative Residual Feature (CRF)
instead of the features in all the layers, which reduces the memory footprint
of feature caching by 99%.
  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and
Qwen-Image-Edit demonstrate its effectiveness in both generation and editing.
Codes are available in the supplementary materials and will be released on
GitHub.

</details>


### [8] [Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting](https://arxiv.org/abs/2510.08696)
*Yunzhen Feng,Parag Jain,Anthony Hartshorn,Yaqi Duan,Julia Kempe*

Main category: cs.LG

TL;DR: 提出 LENS：通过对“负样本”施以基于置信度的惩罚，使负群组也能提供梯度信号，从而提升 RLVR 的效率与性能，克服 GRPO 在负群组上的计算浪费。实证在 MATH 基准上的 Llama-3.1-8B 与 Qwen-2.5-3B 模型上优于 GRPO，且在难项上提升显著。


<details>
  <summary>Details</summary>
Motivation: GRPO 在负群组（没有样本正确的组）会消耗大量计算，因为这类组不产生梯度。作者希望在不额外监督的前提下，利用负样本的信息以提高样本利用率与学习效率，改进 RLVR 的效率与性能。

Method: 从奖励建模中的最大似然（MLE）目标出发，证明MLE梯度等价于对一个改造后的价值函数的策略梯度。该价值函数对不正确回答引入置信度加权惩罚，对更自信的错误给出更大惩罚。提出 LENS（Likelihood Estimation with Negative Samples），将负样本纳入计算，通过非零、基于置信度的奖励来使负群组信息化，改造 GRPO 的梯度更新。

Result: 在 MATH 基准上，使用 Llama-3.1-8B 与 Qwen-2.5-3B 的实验中，LENS 变体持续优于 GRPO 基线，且在难项上获得显著提升。

Conclusion: 给出了一种原理性且实用的方式“拯救”负群组，使 RLVR 的样本利用率与性能提升成为可能，改进了在负样本上的信息利用与梯度更新。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a standard
recipe for improving large language models (LLMs) on reasoning tasks, with
Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO
wastes substantial compute on negative groups: groups in which no sampled
response is correct yield zero advantage and thus no gradient. We ask whether
negative groups can be leveraged without extra supervision. Starting from a
maximum-likelihood (MLE) objective in reward modeling, we show that the MLE
gradient is equivalent to a policy gradient for a modified value function. This
value function adds a confidence-weighted penalty on incorrect responses,
imposing larger penalties on more confident mistakes. We refer to this as
\textbf{L}ikelihood \textbf{E}stimation with \textbf{N}egative \textbf{S}amples
(\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent
rewards to incorrect generations, making negative groups informative and
converting previously wasted samples into useful gradient updates. On the MATH
benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently
outperforms GRPO baseline, with significant gains on harder items. These
results demonstrate a principled and practical way to "rescue" negative groups,
improving efficiency and performance in RLVR.

</details>


### [9] [A Frequency-Domain Analysis of the Multi-Armed Bandit Problem: A New Perspective on the Exploration-Exploitation Trade-off](https://arxiv.org/abs/2510.08908)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出一种将多臂赌博机分析转化为频域的框架，将每个臂的reward估计视为谱分量，UCB的置信界转化为对谱分量的时变增益，并给出有限时动态探索界及对自适应参数的设计基础。


<details>
  <summary>Details</summary>
Motivation: 当前以时域的累积 regret 为主的分析难以刻画学习过程的动态性质，需一个能直观描述探寻-利用的时变特性的全新视角。

Method: 构建Frequency-Domain Bandit Model；将臂的奖励估计作为谱分量，不确定性对应谱分量的频率，算法视为自适应滤波器；证明主定理：UCB的置信界在频域等价于对不确定谱分量的时变增益，增益与访问次数的平方根成反比。

Result: 给出主定理；推导出关于探索速率衰减的有限时动态界；该框架为理解经典算法提供新的物理直观解释，并为设计带自适应参数的新一代算法奠定理论基础。

Conclusion: 该频域框架不仅为理解和解释现有算法提供直观的物理视角，也为未来自适应参数设计的算法提供严谨的理论基础。

Abstract: The stochastic multi-armed bandit (MAB) problem is one of the most
fundamental models in sequential decision-making, with the core challenge being
the trade-off between exploration and exploitation. Although algorithms such as
Upper Confidence Bound (UCB) and Thompson Sampling, along with their regret
theories, are well-established, existing analyses primarily operate from a
time-domain and cumulative regret perspective, struggling to characterize the
dynamic nature of the learning process. This paper proposes a novel
frequency-domain analysis framework, reformulating the bandit process as a
signal processing problem. Within this framework, the reward estimate of each
arm is viewed as a spectral component, with its uncertainty corresponding to
the component's frequency, and the bandit algorithm is interpreted as an
adaptive filter. We construct a formal Frequency-Domain Bandit Model and prove
the main theorem: the confidence bound term in the UCB algorithm is equivalent
in the frequency domain to a time-varying gain applied to uncertain spectral
components, a gain inversely proportional to the square root of the visit
count. Based on this, we further derive finite-time dynamic bounds concerning
the exploration rate decay. This theory not only provides a novel and intuitive
physical interpretation for classical algorithms but also lays a rigorous
theoretical foundation for designing next-generation algorithms with adaptive
parameter adjustment.

</details>


### [10] [In-Context Learning for Non-Stationary MIMO Equalization](https://arxiv.org/abs/2510.08711)
*Jiachen Jiang,Zhen Qin,Zhihui Zhu*

Main category: cs.LG

TL;DR: 将ICL与自适应注意力设计结合，针对非平稳时变的MIMO信道实现高跟踪性均衡，实验表明受LMS/LRMS启发的注意力能显著提升动态环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ICL分析多聚焦静态信道，难以应对时变通道导致的干扰与漂移；需要将自适应信号处理的思想融入注意力机制，以提高非平稳任务的自适应性和鲁棒性。

Method: 提出一个 principled framework 的注意力设计，借鉴LMS、LRMS以及多步梯度更新等策略，生成适用于非平稳通道的注意力变体，并将其应用于ICL型MIMO通道均衡。

Result: 实验表明，在非平稳MIMO等场景中，结合自适应算法启发的注意力机制显著提升跟踪性与均衡性能；LRMS提升鲁棒性，LMS启发的变体在动态信道下更稳定。

Conclusion: ICL具备处理非平稳信道的潜力；以经典自适应算法为引导的注意力设计可提升无线基础模型的适应性与鲁棒性，为未来的自适应无线AI系统提供方向。

Abstract: Channel equalization is fundamental for mitigating distortions such as
frequency-selective fading and inter-symbol interference. Unlike standard
supervised learning approaches that require costly retraining or fine-tuning
for each new task, in-context learning (ICL) adapts to new channels at
inference time with only a few examples. However, existing ICL-based equalizers
are primarily developed for and evaluated on static channels within the context
window. Indeed, to our knowledge, prior principled analyses and theoretical
studies of ICL focus exclusively on the stationary setting, where the function
remains fixed within the context. In this paper, we investigate the ability of
ICL to address non-stationary problems through the lens of time-varying channel
equalization. We employ a principled framework for designing efficient
attention mechanisms with improved adaptivity in non-stationary tasks,
leveraging algorithms from adaptive signal processing to guide better designs.
For example, new attention variants can be derived from the Least Mean Square
(LMS) adaptive algorithm, a Least Root Mean Square (LRMS) formulation for
enhanced robustness, or multi-step gradient updates for improved long-term
tracking. Experimental results demonstrate that ICL holds strong promise for
non-stationary MIMO equalization, and that attention mechanisms inspired by
classical adaptive algorithms can substantially enhance adaptability and
performance in dynamic environments. Our findings may provide critical insights
for developing next-generation wireless foundation models with stronger
adaptability and robustness.

</details>


### [11] [Enhancing Self-Supervised Learning with Semantic Pairs A New Dataset and Empirical Study](https://arxiv.org/abs/2510.08722)
*Mohammad Alkhalefi,Georgios Leontidis,Mingjun Zhong*

Main category: cs.LG

TL;DR: 将数据集中的每个实例视为一个类别，通过对每个实例生成两个随机变换的视图进行对比学习，以获得对物体稳定的不变表征。


<details>
  <summary>Details</summary>
Motivation: 在没有标注信息的情况下学习有意义的视觉表征。通过将同一对象在不同变换下的视图拉近来强制模型捕捉跨视图的一致性，同时将不同实例区分开来，以避免表示的 collapse。

Method: 对每个实例生成两个随机变换的视图，使用对比学习目标（如InfoNCE）拉近同一实例的两视图的表征，同时拉远不同实例的表征，将实例视为一个类别进行分类性对比学习。

Result: 自监督对比学习能够在没有标注的情况下学习到有竞争力的表征，且在下游任务上展现良好的泛化性能；适当的对比目标与负样本策略对性能影响显著。

Conclusion: 实例级对比学习提供了一种简洁而有效的自监督学习框架，通过将每个实例视为类别并对其进行双视图对比，获取具有判别力的表征，且具备良好泛化性。

Abstract: Instance discrimination is a self-supervised representation learning paradigm
wherein individual instances within a dataset are treated as distinct classes.
This is typically achieved by generating two disparate views of each instance
by applying stochastic transformations, which encourages the model to learn
representations that are invariant to the common underlying object across these
views.

</details>


### [12] [Counterfactually Fair Conformal Prediction](https://arxiv.org/abs/2510.08724)
*Ozgur Guldogan,Neeraj Sarna,Yuanyuan Li,Michael Berger*

Main category: cs.LG

TL;DR: 提出 Counterfactually Fair Conformal Prediction (CF-CP): 在预测集合上实现对抗/反事实公平性，同时保持有限样本覆盖率的 CP 特性；通过对保护属性干预下的 conformity 分数进行对称化，实现无额外训练成本的公平性校正。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下，预测集合的公平性（对反事实干预的公平性）仍未充分研究。尽管 conformal prediction 提供分布无关、稳定的预测集合，但其并不天然具备 counterfactual fairness；需要在不中断覆盖率保证的前提下，引入公平性约束。

Method: 通过对保护属性干预下的 conformity 分数进行对称化，得到一个将预测集合对反事实干预敏感性降低的 CF-CP 方案；证明该方法能在保持边际覆盖率的同时实现反事实公平性；方法为训练无关的 conformal prediction。

Result: 在合成数据和真实数据集上，CF-CP 在回归与分类任务中实现了所期望的反事实公平性，同时满足目标覆盖率，且预测集合大小的提升很小。实验表明该方法简单且有效，适合作为无训练开销的反事实公平性不确定性量化工具。

Conclusion: CF-CP 为在不需要额外训练的前提下，将反事实公平性引入不确定性量化提供了一条简单通路，兼顾公平性与覆盖性，便于在实际应用中对预测集合进行公平性评估与控制。

Abstract: While counterfactual fairness of point predictors is well studied, its
extension to prediction sets--central to fair decision-making under
uncertainty--remains underexplored. On the other hand, conformal prediction
(CP) provides efficient, distribution-free, finite-sample valid prediction
sets, yet does not ensure counterfactual fairness. We close this gap by
developing Counterfactually Fair Conformal Prediction (CF-CP) that produces
counterfactually fair prediction sets. Through symmetrization of conformity
scores across protected-attribute interventions, we prove that CF-CP results in
counterfactually fair prediction sets while maintaining the marginal coverage
property. Furthermore, we empirically demonstrate that on both synthetic and
real datasets, across regression and classification tasks, CF-CP achieves the
desired counterfactual fairness and meets the target coverage rate with minimal
increase in prediction set size. CF-CP offers a simple, training-free route to
counterfactually fair uncertainty quantification.

</details>


### [13] [Locally Optimal Private Sampling: Beyond the Global Minimax](https://arxiv.org/abs/2510.09485)
*Hrad Ghoukasian,Bonwoo Lee,Shahab Asoodeh*

Main category: cs.LG

TL;DR: 提出局部极小极大分析来研究在局部邻域内的LDP采样问题，揭示局部风险等同于在该邻域内的全局风险，给出无关于f-散度的局部最优采样器闭式解，并在公开数据场景下优于全局方法。


<details>
  <summary>Details</summary>
Motivation: 在强隐私约束下希望从私有分布中采样，同时考虑公开数据对结果的影响，将P0视为公开数据分布，构建局部minimax框架以更贴近实际应用。

Method: 将纯LDP扩展至函数LDP，证明在以P0为中心的局部邻域内全球最优的函数LDP采样器等同于局部最优；推导出局部极小极大最优采样器的闭式表达式，且不依赖于具体的f- divergence；并结合理论结果与实验对比。

Result: 局部minimax风险由全局minimax风险在邻域内决定；给出局部最优采样器的闭式解；在公开数据场景的实验中，该局部采样器优于全局最优采样器。

Conclusion: 局部框架更贴近现实中的隐私-公开数据混合场景，提供易实现的局部最优采样器及其广泛适用性，但未来工作可扩展到多样本与其他隐私设定，并进一步分析误差源。

Abstract: We study the problem of sampling from a distribution under local differential
privacy (LDP). Given a private distribution $P \in \mathcal{P}$, the goal is to
generate a single sample from a distribution that remains close to $P$ in
$f$-divergence while satisfying the constraints of LDP. This task captures the
fundamental challenge of producing realistic-looking data under strong privacy
guarantees. While prior work by Park et al. (NeurIPS'24) focuses on global
minimax-optimality across a class of distributions, we take a local
perspective. Specifically, we examine the minimax risk in a neighborhood around
a fixed distribution $P_0$, and characterize its exact value, which depends on
both $P_0$ and the privacy level. Our main result shows that the local minimax
risk is determined by the global minimax risk when the distribution class
$\mathcal{P}$ is restricted to a neighborhood around $P_0$. To establish this,
we (1) extend previous work from pure LDP to the more general functional LDP
framework, and (2) prove that the globally optimal functional LDP sampler
yields the optimal local sampler when constrained to distributions near $P_0$.
Building on this, we also derive a simple closed-form expression for the
locally minimax-optimal samplers which does not depend on the choice of
$f$-divergence. We further argue that this local framework naturally models
private sampling with public data, where the public data distribution is
represented by $P_0$. In this setting, we empirically compare our locally
optimal sampler to existing global methods, and demonstrate that it
consistently outperforms global minimax samplers.

</details>


### [14] [Transmuting prompts into weights](https://arxiv.org/abs/2510.08734)
*Hanna Mazzawi,Benoit Dherin,Michael Munn,Michael Wunder,Javier Gonzalvo*

Main category: cs.LG

TL;DR: 提出一个面向深度多块 Transformer 的理论框架，将提示信息映射为内部权重更新，能够将信息压缩为令牌无关的思维向量和思维矩阵，从而解释并统一向量/矩阵型模型编辑技术，并直接将文本输入转化为可重复使用的权重更新。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示通过修改推理阶段的内部状态即可控制大语言模型行为，但缺乏对这一现象的理论根基。本工作旨在在 Transformer 的基础计算之上建立一个可推广到深层模型的系统性解释，解释编辑性操作如何产生以及如何被泛化。

Method: 在 Dherin 等 2025 的基礎上，将提示影响映射为隐式权重更新，并推广到深度多块 Transformer。分析提示任意片段的信息如何通过权重向量与权重矩阵在内部表示和组合，并推导将这些信息凝练成 token 无关的思维向量（thought vectors）和思维矩阵（thought matrices）的原则性方法。

Result: 给出将提示信息系统化凝练为可复用的权重更新的具体方法；阐明信息在内部如何通过权重向量/矩阵被表示和组合；为现有的向量化与矩阵化模型编辑技术提供理论解释，并提供将文本输入直接转化为可重用权重更新的计算手段。

Conclusion: 该框架为模型编辑提供计算上扎实的理论基础，并可扩展到深层多块 Transformer，统一并推广基于向量/矩阵的编辑技术，同时为将自然语言提示转化为可重用的权重修改提供直接路径。

Abstract: A growing body of research has demonstrated that the behavior of large
language models can be effectively controlled at inference time by directly
modifying their internal states, either through vector additions to their
activations or through updates to their weight matrices. These techniques,
while powerful, are often guided by empirical heuristics, such as deriving
steering vectors from the average activations of contrastive prompts. This work
provides a theoretical foundation for these interventions, explaining how they
emerge from the fundamental computations of the transformer architecture.
Building on the recent finding that a prompt's influence can be mathematically
mapped to implicit weight updates (Dherin et al., 2025), we generalize this
theory to deep, multi-block transformers. We show how the information contained
in any chunk of a user prompt is represented and composed internally through
weight vectors and weight matrices. We then derive a principled method for
condensing this information into token-independent thought vectors and thought
matrices. These constructs provide a theoretical explanation for existing
vector- and matrix-based model editing techniques and offer a direct,
computationally-grounded method for transmuting textual input into reusable
weight updates.

</details>


### [15] [SHAP-Based Supervised Clustering for Sample Classification and the Generalized Waterfall Plot](https://arxiv.org/abs/2510.08737)
*Justin Lin,Julia Fukuyama*

Main category: cs.LG

TL;DR: 通过 SHAP 值聚类解释模型预测的不同路径，并在模拟与 ADNI 数据上展示方法及多分类瀑布图的新推广。


<details>
  <summary>Details</summary>
Motivation: 解决大规模黑盒模型缺乏可解释性，提升在高风险情境中的可信度。

Method: 计算样本/特征的 SHAP 值，聚类这些值以发现对同一预测有相似原因的样本子群；通过一个模拟实验和阿尔茨海默病ADNI数据的案例研究来展示；并提出适用于多分类任务的瀑布图的新推广。

Result: 展示了在相同预测下存在不同“因果路径”的群组结构；在模拟和ADNI数据中验证了方法的可解释性增强，并给出多分类瀑布图的扩展形式。

Conclusion: 该方法提供更细粒度的模型决策解释，特别是在多分类情形下，能帮助研究者理解不同样本为何会以相似预测作出决策。

Abstract: In this growing age of data and technology, large black-box models are
becoming the norm due to their ability to handle vast amounts of data and learn
incredibly complex input-output relationships. The deficiency of these methods,
however, is their inability to explain the prediction process, making them
untrustworthy and their use precarious in high-stakes situations. SHapley
Additive exPlanations (SHAP) analysis is an explainable AI method growing in
popularity for its ability to explain model predictions in terms of the
original features. For each sample and feature in the data set, we associate a
SHAP value that quantifies the contribution of that feature to the prediction
of that sample. Clustering these SHAP values can provide insight into the data
by grouping samples that not only received the same prediction, but received
the same prediction for similar reasons. In doing so, we map the various
pathways through which distinct samples arrive at the same prediction. To
showcase this methodology, we present a simulated experiment in addition to a
case study in Alzheimer's disease using data from the Alzheimer's Disease
Neuroimaging Initiative (ADNI) database. We also present a novel generalization
of the waterfall plot for multi-classification.

</details>


### [16] [TAPAS: Datasets for Learning the Learning with Errors Problem](https://arxiv.org/abs/2510.08797)
*Eshika Saxena,Alberto Alfarano,François Charton,Emily Wenger,Kristin Lauter*

Main category: cs.LG

TL;DR: 提出 TAPAS 数据集：用于 AI 系统分析后量子密码中的 LWE 攻击，提供可直接使用的数据集并建立基线。


<details>
  <summary>Details</summary>
Motivation: LWE 作为后量子密码学中的关键困难问题，AI 驱动的攻击在某些参数下可与或优于传统攻击；数据稀缺限制 AI 研究。需要可用于训练 AI 的数据集以推动研究。

Method: 构建 TAPAS 数据集（Toolkit for Analysis of Post-quantum cryptography using AI Systems），覆盖多种 LWE 设置，提供现成可用的数据以便 AI 研究者原型化新攻击方法；本文记录数据集的创建过程、建立攻击性能基线、以及未来工作方向。

Result: 建立了 TAPAS 数据集并初步建立攻击性能基线，为将来 AI 驱动的 LWE 攻击研究提供数据和基线。

Conclusion: TAPAS 可以填补数据缺口，加速 AI 在 LWE 攻击方面的研究，未来可扩展到更多设置、改进建模方法、完善基线等。

Abstract: AI-powered attacks on Learning with Errors (LWE), an important hard math
problem in post-quantum cryptography, rival or outperform "classical" attacks
on LWE under certain parameter settings. Despite the promise of this approach,
a dearth of accessible data limits AI practitioners' ability to study and
improve these attacks. Creating LWE data for AI model training is time- and
compute-intensive and requires significant domain expertise. To fill this gap
and accelerate AI research on LWE attacks, we propose the TAPAS datasets, a
Toolkit for Analysis of Post-quantum cryptography using AI Systems. These
datasets cover several LWE settings and can be used off-the-shelf by AI
practitioners to prototype new approaches to cracking LWE. This work documents
TAPAS dataset creation, establishes attack performance baselines, and lays out
directions for future work.

</details>


### [17] [The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections](https://arxiv.org/abs/2510.09023)
*Milad Nasr,Nicholas Carlini,Chawin Sitawarin,Sander V. Schulhoff,Jamie Hayes,Michael Ilie,Juliette Pluto,Shuang Song,Harsh Chaudhari,Ilia Shumailov,Abhradeep Thakurta,Kai Yuanqing Xiao,Andreas Terzis,Florian Tramèr*

Main category: cs.LG

TL;DR: 论文质疑现有防御评估的稳健性方法，主张对抗自适应攻击者的评估框架，并给出在多种优化技术下攻击成功率高于90% 的证据。


<details>
  <summary>Details</summary>
Motivation: 当前对语言模型防御的评估依赖静态攻击字符串或弱优化方法，无法反映真实对手的适应性和资源投入。需以能自适应调整攻击策略、在大规模资源下优化目标的对手来测试防御。

Method: 通过系统化地调参和放大常规优化技术（梯度下降、强化学习、随机搜索、人工引导探索），构建适应性攻击并测试12种防御措施，评估攻击成功率。

Result: 在多种防御下，攻击成功率超过90%，多数本来被标注为近零成功率的防御也被突破。

Conclusion: 未来的防御研究需考虑更强的对手，以给出可靠的鲁棒性声明，并应使用自适应、强优化的攻击来评估。

Abstract: How should we evaluate the robustness of language model defenses? Current
defenses against jailbreaks and prompt injections (which aim to prevent an
attacker from eliciting harmful knowledge or remotely triggering malicious
actions, respectively) are typically evaluated either against a static set of
harmful attack strings, or against computationally weak optimization methods
that were not designed with the defense in mind. We argue that this evaluation
process is flawed.
  Instead, we should evaluate defenses against adaptive attackers who
explicitly modify their attack strategy to counter a defense's design while
spending considerable resources to optimize their objective. By systematically
tuning and scaling general optimization techniques-gradient descent,
reinforcement learning, random search, and human-guided exploration-we bypass
12 recent defenses (based on a diverse set of techniques) with attack success
rate above 90% for most; importantly, the majority of defenses originally
reported near-zero attack success rates. We believe that future defense work
must consider stronger attacks, such as the ones we describe, in order to make
reliable and convincing claims of robustness.

</details>


### [18] [Graph Diffusion Transformers are In-Context Molecular Designers](https://arxiv.org/abs/2510.08744)
*Gang Liu,Jie Chen,Yihan Zhu,Michael Sun,Tengfei Luo,Nitesh V Chawla,Meng Jiang*

Main category: cs.LG

TL;DR: DemoDiff 是一种示 demonstrations 用于分子设计的扩散模型，利用少量任务示例引导，具备可扩展的预训练并在多任务上接近或超越大模型。


<details>
  <summary>Details</summary>
Motivation: 解决分子设计中难以获得丰富标注数据的问题；通过在-context 学习将任务上下文转化为演示集，使模型在有限的标签下也能学到目标属性的分布。

Method: 提出 DemoDiff，将任务上下文定义为少量分子-分数的演示集合，训练去噪 Transformer 以生成对目标属性对齐的分子；引入 Node Pair Encoding 的分子分词器，以 motif 级别表示分子，尺度更高效；构建包含来自多源的数百万任务数据集，在 0.7B 参数模型上进行预训练。

Result: 在六大类、33 项设计任务中，DemoDiff 的表现与或超越比其大 100-1000 倍的语言模型；平均排名 3.63，相比领域特定方法的 5.25-10.20，具备领先地位。

Conclusion: DemoDiff 可作为分子领域的基础模型用于在上下文中进行分子设计，代码公开供复现。

Abstract: In-context learning allows large models to adapt to new tasks from a few
demonstrations, but it has shown limited success in molecular design. Existing
databases such as ChEMBL contain molecular properties spanning millions of
biological assays, yet labeled data for each property remain scarce. To address
this limitation, we introduce demonstration-conditioned diffusion models
(DemoDiff), which define task contexts using a small set of molecule-score
examples instead of text descriptions. These demonstrations guide a denoising
Transformer to generate molecules aligned with target properties. For scalable
pretraining, we develop a new molecular tokenizer with Node Pair Encoding that
represents molecules at the motif level, requiring 5.5$\times$ fewer nodes. We
curate a dataset containing millions of context tasks from multiple sources
covering both drugs and materials, and pretrain a 0.7-billion-parameter model
on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses
language models 100-1000$\times$ larger and achieves an average rank of 3.63
compared to 5.25-10.20 for domain-specific approaches. These results position
DemoDiff as a molecular foundation model for in-context molecular design. Our
code is available at https://github.com/liugangcode/DemoDiff.

</details>


### [19] [Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols](https://arxiv.org/abs/2510.09462)
*Mikhail Terekhov,Alexander Panfilov,Daniil Dzenhaliou,Caglar Gulcehre,Maksym Andriushchenko,Ameya Prabhu,Jonas Geiping*

Main category: cs.LG

TL;DR: Adaptive prompt-injection attacks can bypass AI control monitors and enable malicious task completion across benchmarks; current monitor-based protocols are vulnerable, and Defer-to-Resample can even worsen the issue; adaptive evaluation should be standard for future AI control mechanisms.


<details>
  <summary>Details</summary>
Motivation: AI control protocols rely on monitors to prevent untrusted LLMs from causing harm, but adaptive attackers who know the protocol can evade these monitors. This paper highlights a major blind spot and calls for evaluating control mechanisms against adaptive threats.

Method: Introduce a simple adaptive attack by embedding publicly known or zero-shot prompt injections in model outputs. Evaluate the attack on two AI control benchmarks across frontier models and assess universality against monitor-based protocols, including the Defer-to-Resample scheme which is shown to backfire.

Result: Frontier models consistently evade diverse monitors and complete malicious tasks. The attack is universal against monitor-reliant protocols. Defer-to-Resample amplifies prompt injections, reframing them as a best-of-n attack.

Conclusion: Adaptive attacks on monitor models are a critical blind spot in current AI control protocols. Future work should include adaptive threat evaluation as a standard component and develop more robust control mechanisms beyond single-monitor configurations.

Abstract: AI control protocols serve as a defense mechanism to stop untrusted LLM
agents from causing harm in autonomous settings. Prior work treats this as a
security problem, stress testing with exploits that use the deployment context
to subtly complete harmful side tasks, such as backdoor insertion. In practice,
most AI control protocols are fundamentally based on LLM monitors, which can
become a central point of failure. We study adaptive attacks by an untrusted
model that knows the protocol and the monitor model, which is plausible if the
untrusted model was trained with a later knowledge cutoff or can search for
this information autonomously. We instantiate a simple adaptive attack vector
by which the attacker embeds publicly known or zero-shot prompt injections in
the model outputs. Using this tactic, frontier models consistently evade
diverse monitors and complete malicious tasks on two main AI control
benchmarks. The attack works universally against current protocols that rely on
a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,
as its resampling amplifies the prompt injection and effectively reframes it as
a best-of-$n$ attack. In general, adaptive attacks on monitor models represent
a major blind spot in current control protocols and should become a standard
component of evaluations for future AI control mechanisms.

</details>


### [20] [RFOD: Random Forest-based Outlier Detection for Tabular Data](https://arxiv.org/abs/2510.08747)
*Yihao Ang,Peicheng Yao,Yifan Bao,Yushuo Feng,Qiang Huang,Anthony K. H. Tung,Zhiyong Huang*

Main category: cs.LG

TL;DR: RFOD is a Random Forest-based framework for tabular outlier detection that performs per-feature conditional reconstruction, using AGD and Uncertainty-Weighted Averaging to provide interpretable, robust, row-level anomaly scores.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of mixed-type tabular data in high-stakes domains (cybersecurity, financial fraud, healthcare) where existing methods degrade semantic information and interpretability, RFOD reframes anomaly detection as feature-wise conditional reconstruction.

Method: Train a dedicated random forest for each feature conditioned on the other features to perform feature-wise conditional reconstruction. Use Adjusted Gower's Distance (AGD) for cell-level scoring and Uncertainty-Weighted Averaging (UWA) to aggregate into row-level anomaly scores, enabling robust and interpretable detection across heterogeneous data types.

Result: RFOD consistently outperforms state-of-the-art baselines in detection accuracy across 15 real-world datasets, with improved robustness, scalability, and interpretability for mixed-type tabular data.

Conclusion: RFOD reframes anomaly detection as a feature-wise conditional reconstruction problem, enabling effective, interpretable, and scalable outlier detection for mixed-type tabular data.

Abstract: Outlier detection in tabular data is crucial for safeguarding data integrity
in high-stakes domains such as cybersecurity, financial fraud detection, and
healthcare, where anomalies can cause serious operational and economic impacts.
Despite advances in both data mining and deep learning, many existing methods
struggle with mixed-type tabular data, often relying on encoding schemes that
lose important semantic information. Moreover, they frequently lack
interpretability, offering little insight into which specific values cause
anomalies. To overcome these challenges, we introduce \textsf{\textbf{RFOD}}, a
novel \textsf{\textbf{R}}andom \textsf{\textbf{F}}orest-based
\textsf{\textbf{O}}utlier \textsf{\textbf{D}}etection framework tailored for
tabular data. Rather than modeling a global joint distribution, \textsf{RFOD}
reframes anomaly detection as a feature-wise conditional reconstruction
problem, training dedicated random forests for each feature conditioned on the
others. This design robustly handles heterogeneous data types while preserving
the semantic integrity of categorical features. To further enable precise and
interpretable detection, \textsf{RFOD} combines Adjusted Gower's Distance (AGD)
for cell-level scoring, which adapts to skewed numerical data and accounts for
categorical confidence, with Uncertainty-Weighted Averaging (UWA) to aggregate
cell-level scores into robust row-level anomaly scores. Extensive experiments
on 15 real-world datasets demonstrate that \textsf{RFOD} consistently
outperforms state-of-the-art baselines in detection accuracy while offering
superior robustness, scalability, and interpretability for mixed-type tabular
data.

</details>


### [21] [Conformal Risk Training: End-to-End Optimization of Conformal Risk Control](https://arxiv.org/abs/2510.08748)
*Christopher Yeh,Nicolas Christianson,Adam Wierman,Yisong Yue*

Main category: cs.LG

TL;DR: 提出将 CRC 扩展到 Optimized Certainty-Equivalent (OCE) 风险，并通过可微的 conformal risk training（CRT）实现端到端训练，兼顾尾部风险与平均性能，适用于分类的假阴性控制和电池存储的金融风险等场景。


<details>
  <summary>Details</summary>
Motivation: 深度学习的预测往往缺乏风险保证，尾部风险对高风险应用尤为关键，现有 CRC 缺乏对尾部风险的直接控制和模型端到端的学习能力，需要一个可分辨的、无分布假设的框架来同时优化尾部风险和平均表现。

Method: 在 CRC 的基础上，提出对 OCE 风险的通用控制，覆盖期望损失和尾部风险（如 CVaR）。引入 conformal risk training，能够对 conformal OCE 风险控制进行端到端微分并用于模型训练或微调。

Result: 在理论上提供无分布、有限样本的风险保证，同时在实际应用中相较于传统后验 CRC 显著改善平均表现，验证于控制分类器的假阴性率和电池储能的金融风险等任务。

Conclusion: CRC 框架可扩展到更广泛的 OCE 风险，并且通过 conformal risk training 实现端到端的优化，兼具理论保障与实际性能提升，具有广泛的应用潜力。

Abstract: While deep learning models often achieve high predictive accuracy, their
predictions typically do not come with any provable guarantees on risk or
reliability, which are critical for deployment in high-stakes applications. The
framework of conformal risk control (CRC) provides a distribution-free,
finite-sample method for controlling the expected value of any bounded monotone
loss function and can be conveniently applied post-hoc to any pre-trained deep
learning model. However, many real-world applications are sensitive to tail
risks, as opposed to just expected loss. In this work, we develop a method for
controlling the general class of Optimized Certainty-Equivalent (OCE) risks, a
broad class of risk measures which includes as special cases the expected loss
(generalizing the original CRC method) and common tail risks like the
conditional value-at-risk (CVaR). Furthermore, standard post-hoc CRC can
degrade average-case performance due to its lack of feedback to the model. To
address this, we introduce "conformal risk training," an end-to-end approach
that differentiates through conformal OCE risk control during model training or
fine-tuning. Our method achieves provable risk guarantees while demonstrating
significantly improved average-case performance over post-hoc approaches on
applications to controlling classifiers' false negative rate and controlling
financial risk in battery storage operation.

</details>


### [22] [Exploring Cross-Client Memorization of Training Data in Large Language Models for Federated Learning](https://arxiv.org/abs/2510.08750)
*Tinnakit Udsa,Can Udomcharoenchaikit,Patomporn Payoungkhamdee,Sarana Nutanong,Norrathep Rattanavipanon*

Main category: cs.LG

TL;DR: 提出一种能在联邦学习中对跨样本记忆进行细粒度测量的框架，能够同时量化客户端内部记忆与客户端之间记忆；并通过两项研究验证该框架：一是跨客户端的细粒度记忆的测量，二是影响记忆的关键因素（解码策略、前缀长度、FL算法等）。结论是FL模型确实会记住客户端数据，且内部记忆相较于跨客户端记忆更明显，且受训练和推断因素影响。


<details>
  <summary>Details</summary>
Motivation: 现有的FL记忆检测多关注单样本层面的记忆，往往低估了跨样本记忆的风险；而集中式学习（CL）在细粒度层面对训练数据的记忆进行评估，但其前提是可控的集中数据访问，直接应用于FL存在不可行之处。因此，需要在FL场景中引入能够对所有客户端、跨样本的记忆进行细粒度评估的框架，以全面量化记忆风险。

Method: 提出一个框架，在FL场景下对跨样本记忆进行细粒度的量化测量，覆盖所有客户端的内在记忆与跨客户端的记忆。基于该框架开展两项研究：一是测量跨客户端的细粒记忆；二是考察影响记忆的关键因素（解码策略、前缀长度、以及不同FL算法）。通过对参与方数据与模型输出的分析，评估在不同设定下的记忆分布与强度。

Result: 研究发现FL模型确实会记住客户端数据，且记忆在客户端内部（ intra-client）比跨客户端（ inter-client）更显著， memorandum受训练与推断因素影响。

Conclusion: 该框架实现了对FL中内在与跨客户端记忆的细粒度量化，揭示了FL记忆的存在与分布特征，并指出训练与推断设置对记忆程度的影响，为设计更安全的FL训练与推断流程提供指导。

Abstract: Federated learning (FL) enables collaborative training without raw data
sharing, but still risks training data memorization. Existing FL memorization
detection techniques focus on one sample at a time, underestimating more subtle
risks of cross-sample memorization. In contrast, recent work on centralized
learning (CL) has introduced fine-grained methods to assess memorization across
all samples in training data, but these assume centralized access to data and
cannot be applied directly to FL. We bridge this gap by proposing a framework
that quantifies both intra- and inter-client memorization in FL using
fine-grained cross-sample memorization measurement across all clients. Based on
this framework, we conduct two studies: (1) measuring subtle memorization
across clients and (2) examining key factors that influence memorization,
including decoding strategies, prefix length, and FL algorithms. Our findings
reveal that FL models do memorize client data, particularly intra-client data,
more than inter-client data, with memorization influenced by training and
inferencing factors.

</details>


### [23] [Spatial Deconfounder: Interference-Aware Deconfounding for Spatial Causal Inference](https://arxiv.org/abs/2510.08762)
*Ayush Khot,Miruna Oprescu,Maresa Schröder,Ai Kagawa,Xihaier Luo*

Main category: cs.LG

TL;DR: 提出 Spatial Deconfounder 的两阶段方法，以 CVAE + 空间先验从局部处理向量重构替代混杂变量，再用灵活的结果模型估计直接效应与溢出效应；在弱假设下实现对空间混淆和干扰的非参数识别，扩展 SpaCE 并在环境健康和社会科学数据集上表现出更稳健的因果估计。


<details>
  <summary>Details</summary>
Motivation: 空间数据中的未观测混杂因素与邻域干扰共同存在，且通常被单独处理。两者存在内在联系：干扰信息能揭示潜在混杂结构。本工作提出把干扰转化为多源信号，开展联合的因果推断。

Method: 两阶段方法：1) 使用带有空间先验的条件变分自编码器(CVAE)，从局部处理向量中重构替代混杂变量；2) 利用灵活的结果模型估计因果效应，能够非参数识别直接效应与溢出效应，且不需要事先知道潜在场的具体模型或多种处理类型。

Result: 通过扩展 SpaCE 将干扰纳入研究范围，在真实世界数据集的环境健康和社会科学场景中， Spatial Deconfounder 实现对效应估计的稳定改进。

Conclusion: 将干扰转化为多源信号，架起空间混淆与去混淆文献的桥梁，提升结构化数据中的鲁棒因果推断能力。

Abstract: Causal inference in spatial domains faces two intertwined challenges: (1)
unmeasured spatial factors, such as weather, air pollution, or mobility, that
confound treatment and outcome, and (2) interference from nearby treatments
that violate standard no-interference assumptions. While existing methods
typically address one by assuming away the other, we show they are deeply
connected: interference reveals structure in the latent confounder. Leveraging
this insight, we propose the Spatial Deconfounder, a two-stage method that
reconstructs a substitute confounder from local treatment vectors using a
conditional variational autoencoder (CVAE) with a spatial prior, then estimates
causal effects via a flexible outcome model. We show that this approach enables
nonparametric identification of both direct and spillover effects under weak
assumptions--without requiring multiple treatment types or a known model of the
latent field. Empirically, we extend SpaCE, a benchmark suite for spatial
confounding, to include treatment interference, and show that the Spatial
Deconfounder consistently improves effect estimation across real-world datasets
in environmental health and social science. By turning interference into a
multi-cause signal, our framework bridges spatial and deconfounding literatures
to advance robust causal inference in structured data.

</details>


### [24] [Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem](https://arxiv.org/abs/2510.08768)
*Francisco Pascoa,Ian Lalonde,Alexandre Girard*

Main category: cs.LG

TL;DR: 提出一种基于 Buckingham Pi 定理的零-shot转移方法，通过在无量纲空间对观测与动作进行缩放，使预训练的 RL 策略在不同物理参数下无 retraining 的情况下实现泛化；在摆锤仿真、实物摆锤 sim-to-real、以及高维 HalfCheetah 上对比简单基线，结果显示在动力学相似的场景保持性能，在非相似场景显著优于简单传输。


<details>
  <summary>Details</summary>
Motivation: RL 策略往往难以泛化到具有不同物理参数的机器人、任务或环境中，缺乏鲁棒性限制了现实世界应用；需要一种简单、无再训练的跨域迁移方法。

Method: 利用 Buckingham Pi 定理构建系统参数与状态/动作之间的无量纲映射，对观测和动作进行尺度变换；以预训练策略为基础，直接在新上下文中应用缩放后的观测-动作映射，无需再训练；在不同复杂度的环境中进行零-shot 转移评估。

Result: 在动力学相似场景下，缩放传输与原策略等性能；在非相似场景下，缩放传输显著优于 naive 传输，扩展原策略有效的场景体积；在一个仿真 pendulum、实物 pendulum、以及高维 HalfCheetah 上的验证。

Conclusion: 维度分析为 RL 策略的鲁棒性和泛化提供了一个实用的工具，零-shot 转移成为可能，且对跨参数的泛化具有广泛适用性。

Abstract: Reinforcement learning (RL) policies often fail to generalize to new robots,
tasks, or environments with different physical parameters, a challenge that
limits their real-world applicability. This paper presents a simple, zero-shot
transfer method based on Buckingham's Pi Theorem to address this limitation.
The method adapts a pre-trained policy to new system contexts by scaling its
inputs (observations) and outputs (actions) through a dimensionless space,
requiring no retraining. The approach is evaluated against a naive transfer
baseline across three environments of increasing complexity: a simulated
pendulum, a physical pendulum for sim-to-real validation, and the
high-dimensional HalfCheetah. Results demonstrate that the scaled transfer
exhibits no loss of performance on dynamically similar contexts. Furthermore,
on non-similar contexts, the scaled policy consistently outperforms the naive
transfer, significantly expanding the volume of contexts where the original
policy remains effective. These findings demonstrate that dimensional analysis
provides a powerful and practical tool to enhance the robustness and
generalization of RL policies.

</details>


### [25] [Struc-EMB: The Potential of Structure-Aware Encoding in Language Embeddings](https://arxiv.org/abs/2510.08774)
*Shikun Liu,Haoyu Wang,Mufei Li,Pan Li*

Main category: cs.LG

TL;DR: 提出一种在模型内部编码阶段就将结构信息（如超链接、引文等）融入文本嵌入的新范式，并系统性评估两种就地（in-process）的实现：顺序拼接与并行缓存。在多任务零-shot评估中，该结构感知方法优于文本仅基线与事后聚合方法，同时揭示了在信噪比、上下文长度与信号强度之间的权衡。为应对噪声结构数据，作者提出并验证了上下文蒸馏（Context Distillation）与语义平衡（Semantic Balancing）等技术。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入通常仅处理原始文本，忽略了结构信息（如链接、引文）所提供的丰富上下文。现有方法多以事后聚合实现结构信息，但未充分利用在模型内部编码流程中的结构信号。本研究旨在在编码阶段直接整合结构关系，以获得更具上下文感知力的嵌入。

Method: 提出两种就地(in-process)方法：1) 顺序拼接（sequential concatenation），将结构信息拼接到输入序列中，以影响注意力与编码；2) 并行缓存（parallel caching），在不显著改变输入文本的前提下，利用并行结构信息缓存提高表示。进行零-shot实验，覆盖检索、聚类、分类和推荐等任务。并引入两个噪声鲁棒技术：上下文蒸馏与语义平衡。

Result: 结构感知方法在多任务的零-shot评估中持续优于文本仅方法和后处理聚合基线。顺序拼接在噪声较多、上下文长度中等时表现最佳；并行缓存对长且信号较强的上下文具更好扩展，但对干扰因素更敏感。噪声数据下的两种技术显著提升鲁棒性与效果，证明了在模型内部编码层面融合结构信息的有效性。

Conclusion: 这是首次对就地结构感知编码的全面分析，为构建更强大、具上下文感知能力的嵌入模型提供了蓝本。研究揭示了不同编码策略在不同数据特性下的权衡，并提出实用的噪声抑制方法，指向在大规模应用中更高效、可扩展的结构感知嵌入框架。

Abstract: Text embeddings from Large Language Models (LLMs) have become foundational
for numerous applications. However, these models typically operate on raw text,
overlooking the rich structural information, such as hyperlinks or citations,
that provides crucial context in many real-world datasets. This paper
introduces and systematically evaluates a new paradigm for generating
structure-aware text embeddings by integrating these structural relations
directly into the LLM's internal encoding process, rather than relying on
traditional post-hoc aggregation. We investigate two primary in-process
methods: sequential concatenation and parallel caching. Through extensive
zero-shot experiments across retrieval, clustering, classification, and
recommendation tasks, we demonstrate that our structure-aware approaches
consistently outperform both text-only and post-hoc baselines. Our analysis
reveals critical trade-offs: sequential concatenation excels with noisy,
moderate-length contexts, while parallel caching scales more effectively to
long, high-signal contexts but is more susceptible to distractors. To address
the challenge of noisy structural data, we also introduce and validate two
effective techniques: Context Distillation and Semantic Balancing. This work
provides the first comprehensive analysis of in-process structure-aware
encoding, offering a blueprint for building more powerful and contextually
aware embedding models.

</details>


### [26] [Guiding Exploration in Reinforcement Learning Through LLM-Augmented Observations](https://arxiv.org/abs/2510.08779)
*Vaibhav Jain,Gerrit Grossmann*

Main category: cs.LG

TL;DR: 通过将LLM生成的行动建议作为观测的一部分，提出一种软约束的框架，帮助强化学习在稀疏奖励环境中利用LLM推理来引导探索，提升在BabyAI任务中的表现和样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏奖励导致的探索困难：现有方法要么强制RL策略遵循LLM建议，要么把LLM嵌入到奖励函数中，缺乏灵活性和可扩展性。

Method: 将LLM输出的行动建议并入扩展观测空间，RL在学习过程中自主决定何时遵循或忽略这些建议，保持对现有RL算法的兼容性且无需修改算法框架；在三种逐步更难的BabyAI环境上评估。

Result: 在最具挑战性的环境中，相比基线实现71%的相对最终成功率提升；样本效率显著提高，达到阈值的学习速度约快9倍；在较低难度任务中也呈现与难度相关的收益增长。

Conclusion: 利用LLM的世界知识与推理能力，通过软约束的观测信息引导，提供一种灵活且可扩展的方式来加速RL训练，在具有挑战性的环境中有效提升性能。

Abstract: Reinforcement Learning (RL) agents often struggle in sparse-reward
environments where traditional exploration strategies fail to discover
effective action sequences. Large Language Models (LLMs) possess procedural
knowledge and reasoning capabilities from text pretraining that could guide RL
exploration, but existing approaches create rigid dependencies where RL
policies must follow LLM suggestions or incorporate them directly into reward
functions. We propose a framework that provides LLM-generated action
recommendations through augmented observation spaces, allowing RL agents to
learn when to follow or ignore this guidance. Our method leverages LLMs' world
knowledge and reasoning abilities while maintaining flexibility through soft
constraints. We evaluate our approach on three BabyAI environments of
increasing complexity and show that the benefits of LLM guidance scale with
task difficulty. In the most challenging environment, we achieve 71% relative
improvement in final success rates over baseline. The approach provides
substantial sample efficiency gains, with agents reaching performance
thresholds up to 9 times faster, and requires no modifications to existing RL
algorithms. Our results demonstrate an effective method for leveraging LLM
planning capabilities to accelerate RL training in challenging environments.

</details>


### [27] [Edu-EmotionNet: Cross-Modality Attention Alignment with Temporal Feedback Loops](https://arxiv.org/abs/2510.08802)
*S M Rafiuddin*

Main category: cs.LG

TL;DR: 提出 Edu-EmotionNet，通过跨模态注意力对齐、模态重要性估计和时序反馈循环，动态处理跨模态信息及信号质量，提升在线教育情感识别的鲁棒性并支持缺失/嘈杂模态场景，达到教育子集的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 在在线教育场景中，情感识别需要对多模态信号进行融合，但现实中模态输入常常不稳定或缺失，传统静态融合难以适应动态环境，因此需要同时建模情感演化和模态可靠性。

Method: CMAA：动态跨模态上下文共享；MIE：对每个时间步对各模态赋予置信权重；TFL：利用之前的预测强化时序一致性。通过三者协同实现鲁棒情感识别与信号自适应权重调整。

Result: 在教育子集的 IEMOCAP 与 MOSEI（重新标注为困惑、好奇心、无聊、沮丧）上达到SOTA，且对缺失或嘈杂模态表现出良好鲁棒性。可视化显示有效捕捉情感转变并自适应优先可靠信号。

Conclusion: 该框架能够在实时学习系统中对情感演变进行有效建模并对模态可靠性进行自适应处理，具备在真实教育场景中的部署潜力。

Abstract: Understanding learner emotions in online education is critical for improving
engagement and personalized instruction. While prior work in emotion
recognition has explored multimodal fusion and temporal modeling, existing
methods often rely on static fusion strategies and assume that modality inputs
are consistently reliable, which is rarely the case in real-world learning
environments. We introduce Edu-EmotionNet, a novel framework that jointly
models temporal emotion evolution and modality reliability for robust affect
recognition. Our model incorporates three key components: a Cross-Modality
Attention Alignment (CMAA) module for dynamic cross-modal context sharing, a
Modality Importance Estimator (MIE) that assigns confidence-based weights to
each modality at every time step, and a Temporal Feedback Loop (TFL) that
leverages previous predictions to enforce temporal consistency. Evaluated on
educational subsets of IEMOCAP and MOSEI, re-annotated for confusion,
curiosity, boredom, and frustration, Edu-EmotionNet achieves state-of-the-art
performance and demonstrates strong robustness to missing or noisy modalities.
Visualizations confirm its ability to capture emotional transitions and
adaptively prioritize reliable signals, making it well suited for deployment in
real-time learning systems

</details>


### [28] [TinyGraphEstimator: Adapting Lightweight Language Models for Graph Structure Inference](https://arxiv.org/abs/2510.08808)
*Michal Podstawski*

Main category: cs.LG

TL;DR: 小型变换器语言模型能够从文本化的图表示中推断图的结构参数；引入 TinyGraphEstimator 数据集用于系统评估；通过 LoRA 微调实现性能稳步提升，表明小型语言模型具备对图结构进行推理的潜力并可通过参数高效微调提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于大型语言模型在推理上的能力，而对于资源受限的较小模型在图结构推理中的潜力未被充分探索。本研究希望验证是否可用紧凑的 transformers 模型直接从文本化的图表示推断图的关键参数，并提供一个可评估的基准数据集以系统比较不同模型。

Method: 构建 TinyGraphEstimator 数据集：从多种随机图模型生成连通图，并标注详细的结构元数据；使用若干小型开源变换器语言模型对密度、聚类系数、染色数等关键图参数进行预测；在此基础上通过低秩自适应（LoRA）进行轻量化微调以提升模型性能。

Result: 实验结果显示，在所有评估指标上，经过 LoRA 微调的小型语言模型均取得一致的改进，表明这些模型对图结构数据具有非平凡的推理能力。即便模型规模较小，也能在图结构推理任务中发挥实用价值。

Conclusion: 小型语言模型具备对图结构数据进行推理的非平凡能力，且通过高效的参数微调（如 LoRA）可以显著提升其在结构推理任务上的表现，为资源受限场景下的图分析提供了有效的解决途径。

Abstract: Graphs provide a universal framework for representing complex relational
systems, and inferring their structural properties is a core challenge in graph
analysis and reasoning. While large language models have recently demonstrated
emerging abilities to perform symbolic and numerical reasoning, the potential
of smaller, resource-efficient models in this context remains largely
unexplored. This paper investigates whether compact transformer-based language
models can infer graph-theoretic parameters directly from textual graph
representations. To enable systematic evaluation, we introduce the
TinyGraphEstimator dataset - a balanced collection of connected graphs
generated from multiple random graph models and annotated with detailed
structural metadata. We evaluate several small open models on their ability to
predict key graph parameters such as density, clustering, and chromatic number.
Furthermore, we apply lightweight fine-tuning using the Low-Rank Adaptation
(LoRA) technique, achieving consistent improvements across all evaluated
metrics. The results demonstrate that small language models possess non-trivial
reasoning capacity over graph-structured data and can be effectively adapted
for structural inference tasks through efficient parameter tuning.

</details>


### [29] [Long-Tailed Recognition via Information-Preservable Two-Stage Learning](https://arxiv.org/abs/2510.08836)
*Fudong Lin,Xu Yuan*

Main category: cs.LG

TL;DR: 提出一种两阶段学习框架来缓解长尾分布带来的多数类偏置：阶段一通过信息论角度的表示学习（等价于最小化类内距离）获得清晰分离的特征空间；阶段二采用新颖的采样策略选择信息量最大的样本以纠正主导边界，同时保持整体性能；在长尾基准数据集上达到SOTA并提供代码。


<details>
  <summary>Details</summary>
Motivation: 现实数据常呈现长尾分布，导致深度分类模型偏向高频类，尾类表现差。需要在不损失整体性能的前提下提升尾类性能并保留有价值的信息。

Method: 阶段1：从信息理论角度的表示学习，理论等价于最小化类内距离，得到有效且分离良好的特征空间。阶段2：提出一种数学上信息量丰富的样本选择策略，选取对纠正多数偏差且不损害整体性能的样本。

Result: 在多组长尾基准数据集上达到SOTA，且通过广泛实验验证，代码可在 GitHub 获取。

Conclusion: 该两阶段方法有效缓解多数偏置并保持总体性能，具有良好可推广性。

Abstract: The imbalance (or long-tail) is the nature of many real-world data
distributions, which often induces the undesirable bias of deep classification
models toward frequent classes, resulting in poor performance for tail classes.
In this paper, we propose a novel two-stage learning approach to mitigate such
a majority-biased tendency while preserving valuable information within
datasets. Specifically, the first stage proposes a new representation learning
technique from the information theory perspective. This approach is
theoretically equivalent to minimizing intra-class distance, yielding an
effective and well-separated feature space. The second stage develops a novel
sampling strategy that selects mathematically informative instances, able to
rectify majority-biased decision boundaries without compromising a model's
overall performance. As a result, our approach achieves the state-of-the-art
performance across various long-tailed benchmark datasets, validated via
extensive experiments. Our code is available at
https://github.com/fudong03/BNS_IPDPP.

</details>


### [30] [The Boundaries of Fair AI in Medical Image Prognosis: A Causal Perspective](https://arxiv.org/abs/2510.08840)
*Thai-Hoang Pham,Jiayuan Chen,Seungyeon Lee,Yuanlong Wang,Sayoko Moroi,Xueru Zhang,Ping Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As machine learning (ML) algorithms are increasingly used in medical image
analysis, concerns have emerged about their potential biases against certain
social groups. Although many approaches have been proposed to ensure the
fairness of ML models, most existing works focus only on medical image
diagnosis tasks, such as image classification and segmentation, and overlooked
prognosis scenarios, which involve predicting the likely outcome or progression
of a medical condition over time. To address this gap, we introduce FairTTE,
the first comprehensive framework for assessing fairness in time-to-event (TTE)
prediction in medical imaging. FairTTE encompasses a diverse range of imaging
modalities and TTE outcomes, integrating cutting-edge TTE prediction and
fairness algorithms to enable systematic and fine-grained analysis of fairness
in medical image prognosis. Leveraging causal analysis techniques, FairTTE
uncovers and quantifies distinct sources of bias embedded within medical
imaging datasets. Our large-scale evaluation reveals that bias is pervasive
across different imaging modalities and that current fairness methods offer
limited mitigation. We further demonstrate a strong association between
underlying bias sources and model disparities, emphasizing the need for
holistic approaches that target all forms of bias. Notably, we find that
fairness becomes increasingly difficult to maintain under distribution shifts,
underscoring the limitations of existing solutions and the pressing need for
more robust, equitable prognostic models.

</details>


### [31] [On the Alignment Between Supervised and Self-Supervised Contrastive Learning](https://arxiv.org/abs/2510.08852)
*Achleshwar Luthra,Priyadarsi Mishra,Tomer Galanti*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Self-supervised contrastive learning (CL) has achieved remarkable empirical
success, often producing representations that rival supervised pre-training on
downstream tasks. Recent theory explains this by showing that the CL loss
closely approximates a supervised surrogate, Negatives-Only Supervised
Contrastive Learning (NSCL) loss, as the number of classes grows. Yet this
loss-level similarity leaves an open question: {\em Do CL and NSCL also remain
aligned at the representation level throughout training, not just in their
objectives?}
  We address this by analyzing the representation alignment of CL and NSCL
models trained under shared randomness (same initialization, batches, and
augmentations). First, we show that their induced representations remain
similar: specifically, we prove that the similarity matrices of CL and NSCL
stay close under realistic conditions. Our bounds provide high-probability
guarantees on alignment metrics such as centered kernel alignment (CKA) and
representational similarity analysis (RSA), and they clarify how alignment
improves with more classes, higher temperatures, and its dependence on batch
size. In contrast, we demonstrate that parameter-space coupling is inherently
unstable: divergence between CL and NSCL weights can grow exponentially with
training time.
  Finally, we validate these predictions empirically, showing that CL-NSCL
alignment strengthens with scale and temperature, and that NSCL tracks CL more
closely than other supervised objectives. This positions NSCL as a principled
bridge between self-supervised and supervised learning. Our code and project
page are available at
[\href{https://github.com/DLFundamentals/understanding_ssl_v2}{code},
\href{https://dlfundamentals.github.io/cl-nscl-representation-alignment/}{project
page}].

</details>


### [32] [Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training](https://arxiv.org/abs/2510.08855)
*T. Ed Li,Junyu Ren*

Main category: cs.LG

TL;DR: Adaptive Temporal Masking (ATM) enables dynamic, probabilistic feature selection in sparse autoencoders, reducing feature absorption and improving interpretability while preserving reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Current SAE training suffers from feature absorption, where features/neurons become merged to minimize L1 penalties, undermining consistent interpretability and reliable analysis of model behavior.

Method: ATM tracks activation magnitudes, frequencies, and reconstruction contributions over time to compute evolving feature importance scores. It then applies probabilistic masking based on statistical thresholds to select features, yielding a more natural and stable feature selection process.

Result: Experiments on the Gemma-2-2b model show substantially lower absorption scores for ATM compared with existing methods such as TopK and JumpReLU SAEs, while maintaining excellent reconstruction quality.

Conclusion: ATM provides a principled approach for learning stable, interpretable features in neural networks, laying a foundation for more reliable model analysis.

Abstract: Understanding the internal representations of large language models is
crucial for ensuring their reliability and safety, with sparse autoencoders
(SAEs) emerging as a promising interpretability approach. However, current SAE
training methods face feature absorption, where features (or neurons) are
absorbed into each other to minimize $L_1$ penalty, making it difficult to
consistently identify and analyze model behaviors. We introduce Adaptive
Temporal Masking (ATM), a novel training approach that dynamically adjusts
feature selection by tracking activation magnitudes, frequencies, and
reconstruction contributions to compute importance scores that evolve over
time. ATM applies a probabilistic masking mechanism based on statistical
thresholding of these importance scores, creating a more natural feature
selection process. Through extensive experiments on the Gemma-2-2b model, we
demonstrate that ATM achieves substantially lower absorption scores compared to
existing methods like TopK and JumpReLU SAEs, while maintaining excellent
reconstruction quality. These results establish ATM as a principled solution
for learning stable, interpretable features in neural networks, providing a
foundation for more reliable model analysis.

</details>


### [33] [Multi-fidelity Batch Active Learning for Gaussian Process Classifiers](https://arxiv.org/abs/2510.08865)
*Murray Cutforth,Yiming Yang,Tiffany Fan,Serge Guillas,Eric Darve*

Main category: cs.LG

TL;DR: BPMI是一个用于多保真GP分类器的批量主动学习算法，利用Bernoulli输出的互信息近似来高效分配计算预算，在二元输出场景下通过对链接函数的一阶泰勒展开避免了互信息的直接计算。


<details>
  <summary>Details</summary>
Motivation: 在大量科学与工程问题中，昂贵的仿真计算要求高效地分配计算预算。对二进制输出的多保真高斯过程分类器，如何在预算约束下选取一批次评估点以最大化信息增益，是一个关键挑战。

Method: 提出Bernoulli Parameter Mutual Information (BPMI)算法，针对多保真GP分类问题，通过对链接函数进行一阶泰勒展开，近似计算信息增益，避免在概率空间内互信息的不可行计算；以批量方式选择评估点，适用于二元输出与多保真设置。

Result: 在两个合成场景和一个涉及火箭点燃的激光燃烧室仿真的真实应用中，与若干基线方法相比，BPMI在固定计算预算下实现了更高的预测准确性。

Conclusion: BPMI为多保真GP分类器的批量主动学习提供了一种高效的互信息近似策略，显著提升在二元输出条件下的样本利用效率，适用于大规模仿真预算分配问题；未来工作可扩展到其他链接函数或模型设定。

Abstract: Many science and engineering problems rely on expensive computational
simulations, where a multi-fidelity approach can accelerate the exploration of
a parameter space. We study efficient allocation of a simulation budget using a
Gaussian Process (GP) model in the binary simulation output case. This paper
introduces Bernoulli Parameter Mutual Information (BPMI), a batch active
learning algorithm for multi-fidelity GP classifiers. BPMI circumvents the
intractability of calculating mutual information in the probability space by
employing a first-order Taylor expansion of the link function. We evaluate BPMI
against several baselines on two synthetic test cases and a complex, real-world
application involving the simulation of a laser-ignited rocket combustor. In
all experiments, BPMI demonstrates superior performance, achieving higher
predictive accuracy for a fixed computational budget.

</details>


### [34] [An Improved Model-Free Decision-Estimation Coefficient with Applications in Adversarial MDPs](https://arxiv.org/abs/2510.08882)
*Haolin Liu,Chen-Yu Wei,Julian Zimmert*

Main category: cs.LG

TL;DR: 提出 Dig-DEC，一种基于信息增益、非乐观的模型无关 DEC，能够在对抗性环境中探索，并给出多种通用转移结构下的混合 MDP 的首个模型无关遗憾界，同时提升基于 DEC 的在线学习估计的收敛性与 regret 率，与乐观方法在某些场景的性能接近。


<details>
  <summary>Details</summary>
Motivation: 填补 DMSO 中决策-估计系数（DEC）上下界之间的差距，扩展 DEC 的适用性，从随机环境扩展到对抗性环境，且实现对混合 MDP 的无模型假设的遗憾界。

Method: 提出 Dig-DEC：一种基于信息增益的 DEC，去除了乐观性探索，直接通过信息增益驱动探索；将其应用于带有随机转移且奖励对抗的混合 MDP，获得多种转移结构下的首个模型无关遗憾界。并在在线学习方面改进：对平均误差估计，改进估计量以获得更紧的集中性，提升在策略内外两种情况下的遗憾阶提升；对于 Bellman 完备 MDP 的平方误差最小化，重新设计两尺度算法，将遗憾从 T^{2/3} 提升至 √T。

Result: Dig-DEC 的遗憾界始终不大于乐观 DEC，在某些情形下更小。首次给出带有带杖反馈的混合 MDP 的模型无关遗憾界，覆盖多种一般转移结构；平均误差估计在策略内与策略外的收敛提升分别将遗憾从 T^{3/4} 提升到 T^{2/3} 与从 T^{5/6} 提升到 T^{7/9}；Bellman 完备 MDP 的平方误差情形下，遗憾达到 √T，与乐观方法的表现相近。

Conclusion: 证明了在不依赖奖励估计器的前提下，基于信息增益的 DEC 方法同样可以实现对抗性环境下的良好探索，并在多类转移结构下提供模型无关遗憾界，同时通过估计与两尺度方法的改进，提升了对比乐观方法的上界与下界之间的差距，首次在部分场景达到与乐观方法等价甚至更优的性能。

Abstract: We study decision making with structured observation (DMSO). Previous work
(Foster et al., 2021b, 2023a) has characterized the complexity of DMSO via the
decision-estimation coefficient (DEC), but left a gap between the regret upper
and lower bounds that scales with the size of the model class. To tighten this
gap, Foster et al. (2023b) introduced optimistic DEC, achieving a bound that
scales only with the size of the value-function class. However, their
optimism-based exploration is only known to handle the stochastic setting, and
it remains unclear whether it extends to the adversarial setting.
  We introduce Dig-DEC, a model-free DEC that removes optimism and drives
exploration purely by information gain. Dig-DEC is always no larger than
optimistic DEC and can be much smaller in special cases. Importantly, the
removal of optimism allows it to handle adversarial environments without
explicit reward estimators. By applying Dig-DEC to hybrid MDPs with stochastic
transitions and adversarial rewards, we obtain the first model-free regret
bounds for hybrid MDPs with bandit feedback under several general transition
structures, resolving the main open problem left by Liu et al. (2025).
  We also improve the online function-estimation procedure in model-free
learning: For average estimation error minimization, we refine the estimator in
Foster et al. (2023b) to achieve sharper concentration, improving their regret
bounds from $T^{3/4}$ to $T^{2/3}$ (on-policy) and from $T^{5/6}$ to $T^{7/9}$
(off-policy). For squared error minimization in Bellman-complete MDPs, we
redesign their two-timescale procedure, improving the regret bound from
$T^{2/3}$ to $\sqrt{T}$. This is the first time a DEC-based method achieves
performance matching that of optimism-based approaches (Jin et al., 2021; Xie
et al., 2023) in Bellman-complete MDPs.

</details>


### [35] [Pinpointing crucial steps: Attribution-based Credit Assignment for Verifiable Reinforcement Learning](https://arxiv.org/abs/2510.08899)
*Junxi Yin,Haisen Luo,Zhenyu Li,Yihua Liu,Dan Liu,Zequn Li,Xiaohang Xu*

Main category: cs.LG

TL;DR: ACPO是一种分阶段、困难度自适应的策略优化框架，结合轨迹语义分割与归因表征来动态调控策略熵，同时使用分解奖励实现对每个推理步骤的精确信用分配，显著提升在AIME、MATH、AMC等基准上的表现。


<details>
  <summary>Details</summary>
Motivation: RLVR在探索–利用之间存在权衡与中间步骤信用分配问题，容易导致熵崩溃和推理能力受限。

Method: 提出ACPO：引入一个阶段化的困难度课程学习框架；通过轨迹语义分割与归因表示动态调控策略熵；并采用分解奖励以量化各推理步骤的层级贡献，从而实现更精确的信用分配。

Result: 在AIME、MATH、AMC等挑战性基准上，ACPO显著优于现有的最先进方法。

Conclusion: ACPO通过改进探索与开发、解决信用分配和熵崩溃问题，提升大语言模型的推理能力和整体性能。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) enhances complex
reasoning in LLMs, current methods struggle to balance exploration and
exploitation. This leads to critical issues like inaccurate credit assignment
for intermediate steps and premature entropy collapse, limiting model
performance. To address this, we introduce Attribution-based Contribution to
Policy Optimization (ACPO), a phased framework that incorporates a
difficulty-aware curriculum. ACPO improves exploration by using trajectory
semantic segmentation and an attribution-based representation to dynamically
regulate policy entropy, thus mitigating its collapse. Concurrently, it
enhances exploitation with a factorized reward system that precisely quantifies
the hierarchical contribution of each reasoning step, ensuring accurate credit
assignment. Extensive experiments on challenging benchmarks, including AIME,
MATH, and AMC, demonstrate that ACPO significantly outperforms existing
state-of-the-art approaches.

</details>


### [36] [AB-PINNs: Adaptive-Basis Physics-Informed Neural Networks for Residual-Driven Domain Decomposition](https://arxiv.org/abs/2510.08924)
*Jonah Botvinick-Greenhouse,Wael H. Ali,Mouhacine Benosman,Saviz Mowlavi*

Main category: cs.LG

TL;DR: AB-PINNs introduce adaptive-basis domain decomposition to dynamically refine subdomains during training, improving PINN performance on multiscale PDEs by focusing capacity where the residual is high.


<details>
  <summary>Details</summary>
Motivation: Static domain decompositions in PINNs can limit expressivity and struggle with multiscale solutions. An adaptive, on-the-fly subdivision aims to allocate model capacity where the solution is challenging, reduce convergence to bad local minima, and lessen hyperparameter tuning.

Method: Develop AB-PINNs that monitor residual loss and insert new subdomains during training, inspired by mesh refinement methods. Subdomains can capture different scales, enabling flexible, multiscale representation and dynamic reallocation of network capacity.

Result: Numerical experiments demonstrate AB-PINNs effectively solve a range of complex multiscale PDEs, with improved accuracy and convergence and reduced sensitivity to hyperparameters compared to static schemes.

Conclusion: Adaptive, on-demand domain decomposition in PINNs enhances expressivity for multiscale problems, improves training robustness, and reduces the need for extensive hyperparameter tuning.

Abstract: We introduce adaptive-basis physics-informed neural networks (AB-PINNs), a
novel approach to domain decomposition for training PINNs in which existing
subdomains dynamically adapt to the intrinsic features of the unknown solution.
Drawing inspiration from classical mesh refinement techniques, we also modify
the domain decomposition on-the-fly throughout training by introducing new
subdomains in regions of high residual loss, thereby providing additional
expressive power where the solution of the differential equation is challenging
to represent. Our flexible approach to domain decomposition is well-suited for
multiscale problems, as different subdomains can learn to capture different
scales of the underlying solution. Moreover, the ability to introduce new
subdomains during training helps prevent convergence to unwanted local minima
and can reduce the need for extensive hyperparameter tuning compared to static
domain decomposition approaches. Throughout, we present comprehensive numerical
results which demonstrate the effectiveness of AB-PINNs at solving a variety of
complex multiscale partial differential equations.

</details>


### [37] [MATT-CTR: Unleashing a Model-Agnostic Test-Time Paradigm for CTR Prediction with Confidence-Guided Inference Paths](https://arxiv.org/abs/2510.08932)
*Moyu Zhang,Yun Chen,Yujun Jin,Jinxin Hu,Yu Zhang,Xiaoyi Zeng*

Main category: cs.LG

TL;DR: 提出一个模型无关的测试时推理（MATT）框架，通过对特征组合的置信度进行估计并在推理时生成多条路径，以提高CTR模型对罕见特征组合的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前工作大多在训练阶段优化模型结构和目标函数，忽视推理阶段潜在优化；不常见的特征组合可能降低预测置信度和准确性，需要在推理阶段提升鲁棒性。

Method: 提出分层概率哈希来估计特征组合在不同阶数上的出现频次作为置信度；以置信度作为采样概率，多次逐步采样生成特定样本的多条推理路径；将多条路径的预测结果聚合以得到鲁棒预测。

Result: 离线实验和在线A/B测试结果显示MATT在现有CTR模型中具有良好兼容性和提升效果。

Conclusion: MATT实现了推理阶段的鲁棒性提升，是对CTR模型的实用、模型无关的改进方向。

Abstract: Recently, a growing body of research has focused on either optimizing CTR
model architectures to better model feature interactions or refining training
objectives to aid parameter learning, thereby achieving better predictive
performance. However, previous efforts have primarily focused on the training
phase, largely neglecting opportunities for optimization during the inference
phase. Infrequently occurring feature combinations, in particular, can degrade
prediction performance, leading to unreliable or low-confidence outputs. To
unlock the predictive potential of trained CTR models, we propose a
Model-Agnostic Test-Time paradigm (MATT), which leverages the confidence scores
of feature combinations to guide the generation of multiple inference paths,
thereby mitigating the influence of low-confidence features on the final
prediction. Specifically, to quantify the confidence of feature combinations,
we introduce a hierarchical probabilistic hashing method to estimate the
occurrence frequencies of feature combinations at various orders, which serve
as their corresponding confidence scores. Then, using the confidence scores as
sampling probabilities, we generate multiple instance-specific inference paths
through iterative sampling and subsequently aggregate the prediction scores
from multiple paths to conduct robust predictions. Finally, extensive offline
experiments and online A/B tests strongly validate the compatibility and
effectiveness of MATT across existing CTR models.

</details>


### [38] [Variability Aware Recursive Neural Network (VARNN): A Residual-Memory Model for Capturing Temporal Deviation in Sequence Regression Modeling](https://arxiv.org/abs/2510.08944)
*Haroon Gharwi,Kai Shu*

Main category: cs.LG

TL;DR: 提出了一种名为VARNN的时间序列回归模型，通过学习最近残差的错误记忆来重新校准预测，在非平稳与漂移/异方差环境中具有更好鲁棒性，且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列常表现出非平稳性、制度切换和噪声方差变化，导致常规回归模型性能下降；需要一种能利用最近预测误差信息以适应变动的方法。

Method: 在前馈预测器基础上加入一个学习得到的错误记忆状态，该状态从最近的残差中更新，作为变动信号，随后在当前时刻对预测进行条件化；该结构被称为VARNN，包含短时上下文步长的残差记忆更新和重新预测机制；在多域数据集（家用电力、医疗保健、环境监测）上与静态、动态、递归基线比较。

Result: 实验结果显示VARNN在测试MSE方面表现优于基线，且计算开销最小化；在各种数据域实现了更强的鲁棒性和更低的误差。

Conclusion: VARNN提供在漂移与波动环境下的稳健预测能力，具有作为时间序列学习框架的潜力。

Abstract: Real-world time series data exhibit non-stationary behavior, regime shifts,
and temporally varying noise (heteroscedastic) that degrade the robustness of
standard regression models. We introduce the Variability-Aware Recursive Neural
Network (VARNN), a novel residual-aware architecture for supervised time-series
regression that learns an explicit error memory from recent prediction
residuals and uses it to recalibrate subsequent predictions. VARNN augments a
feed-forward predictor with a learned error-memory state that is updated from
residuals over a short context steps as a signal of variability and drift, and
then conditions the final prediction at the current time step. Across diverse
dataset domains, appliance energy, healthcare, and environmental monitoring,
experimental results demonstrate VARNN achieves superior performance and
attains lower test MSE with minimal computational overhead over static,
dynamic, and recurrent baselines. Our findings show that the VARNN model offers
robust predictions under a drift and volatility environment, highlighting its
potential as a promising framework for time-series learning.

</details>


### [39] [When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach](https://arxiv.org/abs/2510.08952)
*Zhihan Zhang,Xunkai Li,Yilong Zuo,Zhenjun Li,Bing Zhou,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了一个面向数据质量的多智能体框架LAGA，用于提升文本属性图（TAGs）的图质量，从而增强GNN在九种情境下的鲁棒性，达到多任务/多骨干的state-of-the-art表现。


<details>
  <summary>Details</summary>
Motivation: TAGs的性能高度依赖输入质量，而现有方法多聚焦于模型架构，忽略对数据质量的系统性优化，成为瓶颈。

Method: 提出四个协同智能体（检测、规划、执行、评估）组成的闭环自动化框架LAGA。核心的执行智能体采用双编码器和三目标设计，以跨模态整合信息并进行图质量提升。实验在九种情景下验证，在不同任务和骨干模型上均实现图质量提升和SOTA性能。

Result: 在九种情景（稀疏性、噪声、样本不平衡等）下，LAGA提升了图质量，并在多项任务和骨干模型上达到最优或接近最优的性能。

Conclusion: 数据为中心的质量优化是实现可靠TAGs和鲁棒图学习的关键，LAGA示范了系统性数据质量控制的有效性。

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation that
combines structural connections with fine-grained semantics, supporting a wide
range of data-centric applications. However, the performance of graph neural
networks (GNNs) on TAGs is highly sensitive to input quality. Our empirical
study shows that both traditional GNNs and LLM-enhanced GNNs suffer significant
degradation across nine representative scenarios of sparsity, noise, and
imbalance, highlighting graph quality as a critical bottleneck. Existing
approaches mainly focus on improving model architectures, while neglecting
systematic optimization of TAG data itself, leading to limited effectiveness in
practice. To address this gap, we propose LAGA (Large Language and Graph
Agent), a unified multi-agent framework that treats graph quality control as a
first-class, data-centric problem. LAGA integrates four collaborative
agents-detection, planning, action, and evaluation-into an automated closed
loop. At its core, the action agent employs a dual-encoder and tri-objective
design to capture complementary information across modalities and perform
holistic graph quality enhancement. Experiments across nine scenarios show that
LAGA improves graph quality and achieves state-of-the-art performance across
various tasks and backbones, validating data-centric quality optimization as
key to reliable TAGs and robust graph learning.

</details>


### [40] [Analytical Survey of Learning with Low-Resource Data: From Analysis to Investigation](https://arxiv.org/abs/2510.08962)
*Xiaofeng Cao,Mingwei Xu,Xin Yu,Jiangchao Yao,Wei Ye,Shengjun Huang,Minling Zhang,Ivor W. Tsang,Yew Soon Ong,James T. Kwok,Heng Tao Shen*

Main category: cs.LG

TL;DR: 通过PAC框架下的无偏活跃采样理论，对低资源数据的模型泛化和标注需求进行分析，并基于此提出面向低资源学习的优化策略与跨域学习范式综述。


<details>
  <summary>Details</summary>
Motivation: 高资源数据虽成功，但标注和训练成本高企；需要在有限数据下实现稳健泛化，因此研究低资源学习的理论界限与实用策略。

Method: 在模型无关的监督与无监督设置中，使用无偏活跃采样理论结合PAC框架分析泛化误差与标注复杂度；基于理论分析，探讨梯度信息优化、元迭代优化、几何感知优化，以及以大语言模型为驱动的优化；并系统梳理可从低资源数据获益的学习范式，如领域迁移、强化反馈、层级结构建模。

Result: 给出关于低资源数据学习的理论洞见（泛化误差/标注复杂度的关系）和实际的优化策略集合，以及对多种学习范式在低资源情境中的潜在应用的全面综述。

Conclusion: 总结关键发现及对利用低资源数据的学习的影响，指出理论与实践的结合路径，以及未来研究方向。

Abstract: Learning with high-resource data has demonstrated substantial success in
artificial intelligence (AI); however, the costs associated with data
annotation and model training remain significant. A fundamental objective of AI
research is to achieve robust generalization with limited-resource data. This
survey employs agnostic active sampling theory within the Probably
Approximately Correct (PAC) framework to analyze the generalization error and
label complexity associated with learning from low-resource data in both
model-agnostic supervised and unsupervised settings. Based on this analysis, we
investigate a suite of optimization strategies tailored for low-resource data
learning, including gradient-informed optimization, meta-iteration
optimization, geometry-aware optimization, and LLMs-powered optimization.
Furthermore, we provide a comprehensive overview of multiple learning paradigms
that can benefit from low-resource data, including domain transfer,
reinforcement feedback, and hierarchical structure modeling. Finally, we
conclude our analysis and investigation by summarizing the key findings and
highlighting their implications for learning with low-resource data.

</details>


### [41] [Learning Regularizers: Learning Optimizers that can Regularize](https://arxiv.org/abs/2510.08968)
*Suraj Kumar Sahoo,Narayanan C Krishnan*

Main category: cs.LG

TL;DR:  Learned optimizers (LOs) can internalize the effects of traditional regularization techniques and improve generalization without applying explicit regularizers, demonstrated across multiple datasets and network architectures; LOs can transfer these learned regularization properties to new tasks.


<details>
  <summary>Details</summary>
Motivation: Investigate whether regularization effects can be learned by meta-learned optimizers, potentially reducing or replacing explicit regularization on the objective function.

Method: Train LOs with and without access to explicit regularizers (e.g., SAM, GAM, GSAM) across standard benchmarks (MNIST, FMNIST, CIFAR) and architectures (MLP, MLP-Relu, CNN); compare test performance; assess transfer to new optimization tasks to observe if LOs seek minima aligned with regularization targets.

Result: Regularized LOs consistently outperform unregularized LOs in test accuracy and generalization; LOs retain and transfer regularization effects to new tasks, implicitly guiding optimization toward minima similar to those produced by explicit regularizers.

Conclusion: LOs can inherently learn regularization properties, challenging the conventional necessity of explicit regularization on the optimizee loss.

Abstract: Learned Optimizers (LOs), a type of Meta-learning, have gained traction due
to their ability to be parameterized and trained for efficient optimization.
Traditional gradient-based methods incorporate explicit regularization
techniques such as Sharpness-Aware Minimization (SAM), Gradient-norm Aware
Minimization (GAM), and Gap-guided Sharpness-Aware Minimization (GSAM) to
enhance generalization and convergence. In this work, we explore a fundamental
question: \textbf{Can regularizers be learned?} We empirically demonstrate that
LOs can be trained to learn and internalize the effects of traditional
regularization techniques without explicitly applying them to the objective
function. We validate this through extensive experiments on standard benchmarks
(including MNIST, FMNIST, CIFAR and Neural Networks such as MLP, MLP-Relu and
CNN), comparing LOs trained with and without access to explicit regularizers.
Regularized LOs consistently outperform their unregularized counterparts in
terms of test accuracy and generalization. Furthermore, we show that LOs retain
and transfer these regularization effects to new optimization tasks by
inherently seeking minima similar to those targeted by these regularizers. Our
results suggest that LOs can inherently learn regularization properties,
\textit{challenging the conventional necessity of explicit optimizee loss
regularization.

</details>


### [42] [Diagnosing and Mitigating System Bias in Self-Rewarding RL](https://arxiv.org/abs/2510.08977)
*Chuyi Tan,Peiwen Yuan,Xinglin Wang,Yiwei Li,Shaoxiong Feng,Yueqi Zhang,Jiayi Shi,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: RLER通过对奖励信号进行模型集成来缓解RLIR的偏差，接近且逼近RLVR，在无标签样本上实现更稳定且可扩展的强化学习。


<details>
  <summary>Details</summary>
Motivation: RLVR需要大量带标签样本，难以实现持续的数据扩展；RLIR在自我给奖励的设置下具有无标注扩展潜力，但在性能与稳定性方面落后于RLVR，核心原因在于模型对高置信度回放的自我偏好估计，导致奖励估计偏差随训练累积并引发训练不稳定。需要一种在无需额外标注的前提下提升奖励估计鲁棒性的方案。

Method: 提出RLER（强化学习与集合奖励）。通过聚合多样化模型的奖励信号，并对奖励插值与回放选择进行自适应调节，缓解单模型偏差与不稳定性，使奖励估计更稳健。

Result: 实验结果显示，RLER在RLIR上提升约13.6%，仅比RLVR低约3.6%，实现对无标注样本的稳定扩展。

Conclusion: 通过奖励信号的集合化来缓解自我回馈偏差，RLER缩小了RLIR与RLVR之间的差距，为在未标注数据条件下的强化学习提供了一种稳定且可扩展的解决方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) scales the reasoning
ability of large language models (LLMs) but remains bottlenecked by limited
labeled samples for continued data scaling. Reinforcement learning with
intrinsic rewards (RLIR), where the policy model assigns rewards to its own
rollouts, enables sustainable scaling in unlabeled settings, yet its
performance and stability lag behind RLVR. We trace this gap to a system bias:
the model tends to overestimate its high-confidence rollouts, leading to biased
and unstable reward estimation. This bias accumulates as training progresses,
with deviations from the oracle drifting toward over-reward, causing unstable
training. We characterize this bias using three metrics: $\rho_{\text{noise}}$,
$\rho_{\text{selfbias}}$, and $\rho_{\text{symbias}}$. We find that
$\rho_{\text{noise}}$ and $\rho_{\text{symbias}}$ impact convergence, while
$\rho_{\text{selfbias}}$ amplifies both correct and incorrect updates, leading
to instability. To mitigate this, we propose reinforcement learning with
ensembled rewards (RLER), which aggregates diverse models and adapts reward
interpolation and rollout selection. Extensive experiments show that RLER
improves by +13.6% over RLIR and is only 3.6% below RLVR, achieving stable
scaling on unlabeled samples, making it highly applicable.

</details>


### [43] [FedL2T: Personalized Federated Learning with Two-Teacher Distillation for Seizure Prediction](https://arxiv.org/abs/2510.08984)
*Jionghao Lou,Jian Zhang,Zhongmei Li,Lanlan Chen,Enbo Feng*

Main category: cs.LG

TL;DR: 提出了 FedL2T，一种个性化联邦学习框架，通过双教师知识蒸馏为每个客户端生成更好的个性化模型，专注于癫痫发作预测的 EEG 数据，在异构客户端场景下表现优越且对标注低的情况下收敛更快。


<details>
  <summary>Details</summary>
Motivation: 深度学习在癫痫发作预测中的 EEG 数据需求量大，获得足够标注数据困难，隐私和数据壁垒阻碍跨机构协作。联邦学习可在保留数据隐私的前提下进行协作训练，但由于患者间的高度异质性，现有 FL 方法在现实场景下的鲁棒性不足，需提升个性化和稳健性。

Method: 提出两教师知识蒸馏的个性化联邦学习框架 FedL2T。每个客户端同时从全局聚合模型和动态分配的对等模型中学习，通过自适应多层次蒸馏对预测输出和中间特征进行对齐，基于任务置信度决定蒸馏强度；引入近端正则化约束个性化模型更新以提升训练稳定性。

Result: 在两个 EEG 数据集上，FedL2T 在现有最优的 FL 方法上实现一致的性能提升，尤其在低标注情境下效果显著；收敛速度快且稳定，显著减少通信轮数和开销。

Conclusion: FedL2T 为隐私敏感型医疗场景中的癫痫预测提供了可靠的个性化解决方案，能够有效应对客户端异质性与标注稀缺问题，并具备较快的收敛性与较低的通信代价。

Abstract: The training of deep learning models in seizure prediction requires large
amounts of Electroencephalogram (EEG) data. However, acquiring sufficient
labeled EEG data is difficult due to annotation costs and privacy constraints.
Federated Learning (FL) enables privacy-preserving collaborative training by
sharing model updates instead of raw data. However, due to the inherent
inter-patient variability in real-world scenarios, existing FL-based seizure
prediction methods struggle to achieve robust performance under heterogeneous
client settings. To address this challenge, we propose FedL2T, a personalized
federated learning framework that leverages a novel two-teacher knowledge
distillation strategy to generate superior personalized models for each client.
Specifically, each client simultaneously learns from a globally aggregated
model and a dynamically assigned peer model, promoting more direct and enriched
knowledge exchange. To ensure reliable knowledge transfer, FedL2T employs an
adaptive multi-level distillation strategy that aligns both prediction outputs
and intermediate feature representations based on task confidence. In addition,
a proximal regularization term is introduced to constrain personalized model
updates, thereby enhancing training stability. Extensive experiments on two EEG
datasets demonstrate that FedL2T consistently outperforms state-of-the-art FL
methods, particularly under low-label conditions. Moreover, FedL2T exhibits
rapid and stable convergence toward optimal performance, thereby reducing the
number of communication rounds and associated overhead. These results
underscore the potential of FedL2T as a reliable and personalized solution for
seizure prediction in privacy-sensitive healthcare scenarios.

</details>


### [44] [Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search](https://arxiv.org/abs/2510.08992)
*Kamel Alrashedy,Vriksha Srihari,Zulfiqar Zaidi,Ridam Srivastava,Pradyumna Tambwekar,Matthew Gombolay*

Main category: cs.LG

TL;DR: 提出Const-o-T框架，将每一步推理表示为(意图,约束)对，通过将其融入Monte Carlo树搜索来聚焦在语义上有意义的路径，从而 prune 不可行分支、提升规划效率与可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有的多步推理方法（如CoT、ToT、 verifier 等）虽然扩展了搜索空间，却易产生不可行的行动或幻觉步骤，难以确保计划与高层用户意图及符号约束的一致性。需一个结构化的先验来引导LLM的规划过程，兼顾有效性与可验证性。

Method: 将每一步推理表示为(意图,约束)对，并将其整合进蒙特卡罗树搜索（MCTS）框架中，利用该结构化表示 prune 不可行分支、引导探索到语义有效的行动。通过以(意图,约束)对为核心的约束前缀来约束搜索空间，提升规划的准确性与可验证性。

Result: 在Risk游戏、CAD代码生成和算术推理三个领域的实验中，Const-o-T相较基线表现出更高的准确性和更强的结构对齐。

Conclusion: Const-o-T为基于约束的推理提供了一个可泛化的基础，可以实现更高效、与约束对齐且易于领域适配的LLM规划。

Abstract: While researchers have made significant progress in enabling large language
models (LLMs) to perform multi-step planning, LLMs struggle to ensure that
those plans align with high-level user intent and satisfy symbolic constraints,
especially in complex, multi-step domains. Existing reasoning approaches such
as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented
methods, expand the search space but often yield infeasible actions or
hallucinated steps. To overcome these limitations, we propose
Constraints-of-Thought (Const-o-T), a framework that provides a structured
prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically
meaningful paths. Each reasoning step is represented as an (intent, constraint)
pair, which serves both to compress the search space and enforce validity.
Unlike prior methods that merely generate reasoning traces or validate outputs
post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search
toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a
structured representation of intent-constraint pairs constraints prune
infeasible branches and guide exploration toward semantically valid actions,
improving planning efficiency and verifiable decision-making. We demonstrate
across three domains Risk game, CAD code generation, and arithmetic reasoning
that our approach outperforms baselines, yielding higher accuracy and stronger
structural alignment. Our contribution is to demonstrate that Const-of-T offers
a generalizable foundation for constraint-guided reasoning, enabling more
efficient, constraint-aligned, and domain-adaptable planning with LLMs.

</details>


### [45] [LLM Unlearning on Noisy Forget Sets: A Study of Incomplete, Rewritten, and Watermarked Data](https://arxiv.org/abs/2510.09007)
*Changsheng Wang,Yihua Zhang,Dennis Wei,Jinghan Jia,Pin-Yu Chen,Sijia Liu*

Main category: cs.LG

TL;DR: 在有噪声的忘记数据下，LLM去学习（unlearning）的鲁棒性被系统性评估；现有方法（RMU和NPO）在保持核心语义信号时对扰动具有鲁棒性；基于 saliency 的解释表明遗忘依赖深层语义线索而非表层词汇模式。


<details>
  <summary>Details</summary>
Motivation: LLMs容易记住敏感数据、偏见与有害输出，因此需要从预训练模型中移除相关知识的能力；然而现实中的“忘记数据”往往质量参差甚至被伪造、改写或水印化，质疑unlearning的可靠性，因此需要在受扰动的低保真忘记数据下评估与理解。

Method: 系统性地在噪声忘记集合上基准评估现有的LLM unlearning 方法 RMU 与 NPO，并提出基于 saliency 的解释框架，分析在Surface形式发生显著变动时，核心语义信号如何驱动遗忘。

Result: 发现只要保持核心语义信号，unlearning 对扰动具有出乎意料的鲁棒性；关键语义成分在不同表层表述下仍保持重要性，从而解释了鲁棒性来自深层语义线索而非表层词汇。

Conclusion: 未清晰表示的深层语义线索主导遗忘过程，提示未来的unlearning算法应聚焦于保护与利用这些语义信号，而非仅仅拟合表面文本改写。

Abstract: Large language models (LLMs) exhibit remarkable generative capabilities but
raise ethical and security concerns by memorizing sensitive data, reinforcing
biases, and producing harmful content. These risks have spurred interest in LLM
unlearning, the task of removing knowledge associated with undesirable data
from pre-trained models. However, most existing methods assume access to clean,
well-defined forget data samples, whereas real-world forget data could often be
low-quality, synthetically rewritten, or watermarked, casting doubt on the
reliability of unlearning. This work presents the first study of unlearning
under perturbed or low-fidelity forget data, referred to as noisy forget sets.
By systematically benchmarking state-of-the-art LLM unlearning methods, RMU and
NPO, on such noisy forget sets, we find that unlearning remains surprisingly
robust to perturbations, provided that core semantic signals are preserved. To
explain this robustness, we propose a saliency-based interpretation: key
semantic components that drive forgetting remain consistently influential
despite substantial variation in surface form. This suggests that unlearning
algorithms are primarily guided by deep semantic cues rather than shallow
lexical patterns.

</details>


### [46] [Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers](https://arxiv.org/abs/2510.09017)
*Rui Bu,Haofeng Zhong,Wenzheng Chen,Yangyan Li*

Main category: cs.LG

TL;DR: 提出 VGA（Value-State Gated Attention），通过从值向量V直接计算的门控机制，对注意力输出进行调控，以打破价值状态的自我强化循环，缓解极端-token现象。


<details>
  <summary>Details</summary>
Motivation: 大型基于 Transformer 的模型容易出现极端 token 行为（注意力汇聚/价值状态枯竭等），由模型在近零值状态的 token 上集中注意力形成的无效“无操作”行为所致，影响性能、量化保真度和可解释性。

Method: 提出 Value-State Gated Attention (VGA)，引入一个可学习、数据相关的门控，直接从值向量V计算，用以调节输出。对其梯度进行理论分析，表明用值状态自身的函数对其进行门控更能解耦值和注意力分数的更新，优于对输入嵌入进行门控的前作。通过建立直接的调控路径，模型可根据 emergent 的价值表示抑制某个 token 的贡献。

Result: 实验表明 VGA 能显著缓解注意力汇聚现象，稳定价值状态范数，提升模型性能、量化保真度和可解释性。

Conclusion: VGA 提供一种简单、专门的架构来高效实现“无操作”注意力，通过从值状态自我门控的机制，获得更稳定的训练动力学和改进的下游表现。

Abstract: Large models based on the Transformer architecture are susceptible to
extreme-token phenomena, such as attention sinks and value-state drains. These
issues, which degrade model performance, quantization fidelity, and
interpretability, arise from a problematic mutual reinforcement mechanism where
the model learns an inefficient 'no-op' behavior by focusing attention on
tokens with near-zero value states. In this paper, we propose Value-State Gated
Attention (VGA), a simple, dedicated, and stable architectural mechanism for
performing 'no-op' attention efficiently by directly breaking this cycle. VGA
introduces a learnable, data-dependent gate, computed directly from the value
vectors (V), to modulate the output. Through a theoretical analysis of the
underlying gradients, we show that gating the value-state with a function of
itself is more effective at decoupling value and attention score updates than
prior methods that gate on input embeddings. This creates a direct regulatory
pathway that allows the model to suppress a token's contribution based on its
emergent value representation. Our experiments demonstrate that VGA
significantly mitigates the formation of attention sinks and stabilizes
value-state norms, leading to improved performance, robust quantization
fidelity, and enhanced model interpretability.

</details>


### [47] [Slim Scheduler: A Runtime-Aware RL and Scheduler System for Efficient CNN Inference](https://arxiv.org/abs/2510.09018)
*Ian Harshbarger,Calvin Chidambaram*

Main category: cs.LG

TL;DR: Slim Scheduler 将PPO强化学习策略与算法贪婪调度器相结合，针对可扩展的slimmable模型在分布式推理中进行全局路由与局部批处理调度的层次化设计，显著提升延迟和能耗效率，同时在不同 accuracy-latency 之间提供权衡。


<details>
  <summary>Details</summary>
Motivation: 针对异质硬件和动态运行条件下的推理场景，静态端到端固定宽度模型难以适应，需动态、分布式的调度框架来平衡吞吐、延迟和能耗。

Method: 服务器层使用局部贪婪调度器对请求进行分批、实例缩放等，并受VRAM/利用率约束；全局路由由PPO策略学习设备选择、宽度比和批量配置等路由策略。通过层次化设计降低搜索空间、减少对特定硬件的过拟合。

Result: 相对于纯随机任务分配基线，Slim Scheduler实现了显著的性能提升：平均延迟降低约96.45%，能耗降低约97.31%，但平均准确率降至70.3%。整体而言，在提高准确性的同时，延迟和能耗的降幅与标准差增大之间存在权衡，从而影响总体吞吐。

Conclusion: 该框架通过混合的全局-局部调度和可伸缩模型，平衡了推理效率、能耗和准确性，并对异质硬件环境具有较好的适应性，能实现多种accuracy- latency/energy权衡。

Abstract: Most neural network scheduling research focuses on optimizing static,
end-to-end models of fixed width, overlooking dynamic approaches that adapt to
heterogeneous hardware and fluctuating runtime conditions. We present Slim
Scheduler, a hybrid scheduling framework that integrates a Proximal Policy
Optimization (PPO) reinforcement learning policy with algorithmic, greedy
schedulers to coordinate distributed inference for slimmable models. Each
server runs a local greedy scheduler that batches compatible requests and
manages instance scaling based on VRAM and utilization constraints, while the
PPO router learns global routing policies for device selection, width ratio,
and batch configuration. This hierarchical design reduces search space
complexity, mitigates overfitting to specific hardware, and balances efficiency
and throughput. Compared to a purely randomized task distribution baseline,
Slim Scheduler can achieve various accuracy and latency trade-offs such as: A
96.45% reduction in mean latency and a 97.31% reduction in energy usage
dropping accuracy to the slimmest model available (70.3%). It can then
accomplish an overall reduction in average latency plus energy consumption with
an increase in accuracy at the cost of higher standard deviations of said
latency and energy, effecting overall task throughput.

</details>


### [48] [MagicDock: Toward Docking-oriented De Novo Ligand Design via Gradient Inversion](https://arxiv.org/abs/2510.09020)
*Zekai Chen,Xunkai Li,Sirui Zhang,Henan Sun,Jia Li,Zhenjun Li,Bing Zhou,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: MagicDock提出了一个前瞻性的去novo配体设计框架：通过梯度反演与可微分表面建模，结合可学习的三维点云来实现更通用、可扩展的配体设计，并对不同配体类型进行定制化设计。与SOTA基线相比，在9个场景中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有去 Novo 配体设计在伪去 Novo、对接建模的局限性以及配体类型不灵活等方面的短板，提出一个具有可微分建模和渐进式训练的通用框架，以实现从头设计、且能够兼容多种配体类型的对接优化。

Method: 提出梯度反演框架，在骨架模型中整合配体-受体对接的知识，并通过绑定预测将配体生成过程中的梯度反向传播逐步引导；强调对接过程中的可微分表面建模，采用可学习的三维点云表示以精准捕捉结合部细节，并通过直观的空间指纹确保生成的配体在对接有效性上具有可解释性；为不同配体类型定制化设计并整合到统一的梯度反演框架中，提供灵活的触发机制和理论保证。

Result: 在9个场景中，MagicDock在蛋白质或分子配体设计的SOTA基线下分别实现平均27.1%和11.7%的改进。

Conclusion: MagicDock建立了一个面向广域适用性的前瞻性去 Novo 配体设计框架，具有理论保证、可微分的对接建模能力以及对不同配体类型的兼容性，能够显著提升对接设计的效率与效果。

Abstract: De novo ligand design is a fundamental task that seeks to generate protein or
molecule candidates that can effectively dock with protein receptors and
achieve strong binding affinity entirely from scratch. It holds paramount
significance for a wide spectrum of biomedical applications. However, most
existing studies are constrained by the \textbf{Pseudo De Novo},
\textbf{Limited Docking Modeling}, and \textbf{Inflexible Ligand Type}. To
address these issues, we propose MagicDock, a forward-looking framework
grounded in the progressive pipeline and differentiable surface modeling. (1)
We adopt a well-designed gradient inversion framework. To begin with, general
docking knowledge of receptors and ligands is incorporated into the backbone
model. Subsequently, the docking knowledge is instantiated as reverse gradient
flows by binding prediction, which iteratively guide the de novo generation of
ligands. (2) We emphasize differentiable surface modeling in the docking
process, leveraging learnable 3D point-cloud representations to precisely
capture binding details, thereby ensuring that the generated ligands preserve
docking validity through direct and interpretable spatial fingerprints. (3) We
introduce customized designs for different ligand types and integrate them into
a unified gradient inversion framework with flexible triggers, thereby ensuring
broad applicability. Moreover, we provide rigorous theoretical guarantees for
each component of MagicDock. Extensive experiments across 9 scenarios
demonstrate that MagicDock achieves average improvements of 27.1\% and 11.7\%
over SOTA baselines specialized for protein or molecule ligand design,
respectively.

</details>


### [49] [The Environmental Impacts of Machine Learning Training Keep Rising Evidencing Rebound Effect](https://arxiv.org/abs/2510.09022)
*Clément Morand,Anne-Laure Ligozat,Aurélie Névéol*

Main category: cs.LG

TL;DR: 近十年AI系统训练的环境影响持续上升，显卡生产与训练能耗都在增大，效率提升被回弹效应抵消，需从全生命周期与需求端减少训练以实现可持续性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习规模与算力需求快速增长，评估其环境与能源成本的真实范围并探索可持续训练策略成为紧迫任务。

Method: 对近十年的知名AI系统（包括大型语言模型）进行环境影响估算，聚焦显卡生命周期的碳排放，涵盖生产、使用、能源碳强度与地理位置因素，分析回弹效应及策略局限。

Result: 揭示两大趋势：1) 显卡生产对环境影响持续上升；2) 训练ML模型的能源消耗与环境影响呈指数级增长，即使通过地点迁移等减排策略也难以抵消；必须从生命周期全局考量硬件影响，单靠提升效率无法实现可持续，需要通过降低AI活动规模与训练频率来减缓环境负担。

Conclusion: 要实现可持续的AI，需要从需求侧出发降低训练需求，审慎评估规模与频率，结合生命周期评估、材料与能效改进等综合治理，解决回弹效应带来的挑战。

Abstract: Recent Machine Learning (ML) approaches have shown increased performance on
benchmarks but at the cost of escalating computational demands. Hardware,
algorithmic and carbon optimizations have been proposed to curb energy
consumption and environmental impacts. Can these strategies lead to sustainable
ML model training? Here, we estimate the environmental impacts associated with
training notable AI systems over the last decade, including Large Language
Models, with a focus on the life cycle of graphics cards. Our analysis reveals
two critical trends: First, the impacts of graphics cards production have
increased steadily over this period; Second, energy consumption and
environmental impacts associated with training ML models have increased
exponentially, even when considering reduction strategies such as location
shifting to places with less carbon intensive electricity mixes. Optimization
strategies do not mitigate the impacts induced by model training, evidencing
rebound effect. We show that the impacts of hardware must be considered over
the entire life cycle rather than the sole use phase in order to avoid impact
shifting. Our study demonstrates that increasing efficiency alone cannot ensure
sustainability in ML. Mitigating the environmental impact of AI also requires
reducing AI activities and questioning the scale and frequency of
resource-intensive training.

</details>


### [50] [Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach](https://arxiv.org/abs/2510.09041)
*Junchao Fan,Xiaolin Chang*

Main category: cs.LG

TL;DR: 提出IGCARL，一种智能通用-约束对抗强化学习框架，用于鲁棒的自动驾驶。通过一个策略性定向对手和一个在约束下学习的鲁棒驾驶智能体来对抗对抗性攻击；实验显示比现有方法在成功率上提高不少于27.9%，提升鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在自动驾驶中的鲁棒性不足，易受到对抗性攻击。现有鲁棒方法存在三个问题：对手短视性、难以诱发真正的安全事件、训练过程中的不稳定和策略漂移。需要能够应对更具策略性的多步攻击、关注安全性关键事件、并在对抗环境下实现稳定学习的框架。

Method: 提出IGCARL：包括一个策略性定向的对手，利用DRL的时序决策能力执行多步、协同攻击，并以广义和为目标来明确诱发安全关键事件；鲁棒驾驶智能体在与对手交互中学习对抗对抗性攻击的鲁棒策略；通过约束优化来稳定学习、降低策略漂移。

Result: 大规模实验表明，IGCARL在相较于最先进方法的成功率上提高至少27.9%，展示出对对抗性攻击更强的鲁棒性，并提升基于DRL的自动驾驶的安全性和可靠性。

Conclusion: IGCARL有效解决了现有鲁棒对抗方法的三大问题，提供了一种可在对抗环境中稳定训练、关注安全性关键事件的鲁棒自动驾驶框架，提升了对抗性环境下的鲁棒性与安全性，推动了DRL自动驾驶在现实场景中的应用潜力。

Abstract: Deep reinforcement learning (DRL) has demonstrated remarkable success in
developing autonomous driving policies. However, its vulnerability to
adversarial attacks remains a critical barrier to real-world deployment.
Although existing robust methods have achieved success, they still suffer from
three key issues: (i) these methods are trained against myopic adversarial
attacks, limiting their abilities to respond to more strategic threats, (ii)
they have trouble causing truly safety-critical events (e.g., collisions), but
instead often result in minor consequences, and (iii) these methods can
introduce learning instability and policy drift during training due to the lack
of robust constraints. To address these issues, we propose Intelligent
General-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel
robust autonomous driving approach that consists of a strategic targeted
adversary and a robust driving agent. The strategic targeted adversary is
designed to leverage the temporal decision-making capabilities of DRL to
execute strategically coordinated multi-step attacks. In addition, it
explicitly focuses on inducing safety-critical events by adopting a general-sum
objective. The robust driving agent learns by interacting with the adversary to
develop a robust autonomous driving policy against adversarial attacks. To
ensure stable learning in adversarial environments and to mitigate policy drift
caused by attacks, the agent is optimized under a constrained formulation.
Extensive experiments show that IGCARL improves the success rate by at least
27.9\% over state-of-the-art methods, demonstrating superior robustness to
adversarial attacks and enhancing the safety and reliability of DRL-based
autonomous driving.

</details>


### [51] [Improving Anomaly Detection in Industrial Time Series: The Role of Segmentation and Heterogeneous Ensemble](https://arxiv.org/abs/2510.09079)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: Segmentation 与变化点检测用于工业时间序列的异常检测，作为预处理提升异质集成学习效果，显著提高 AUC-ROC。


<details>
  <summary>Details</summary>
Motivation: 在工业场景中提高早期异常检测的准确性，减少时间模糊性，并充分利用不同模型的优势。

Method: 将 segmentation 作为预处理，再训练异质集成模型（如 Random Forest 与 XGBoost），并与以 PCA+LSTM 的组合进行对比；使用 Change Point Detection (CPD) / ChangeFinder 等方法识别状态变化。

Result: 在案例研究中，AUC-ROC 从 0.8599 提升至 0.9760，提升原因在于 segmentation 降低时间模糊性，提升监督学习效果。

Conclusion: Segmentation 有助于提升时序异常检测的性能；未来工作将引入基于变化点的加权特征并结合异质集成以进一步提高早期检测性能。

Abstract: Concerning machine learning, segmentation models can identify state changes
within time series, facilitating the detection of transitions between normal
and anomalous conditions. Specific techniques such as Change Point Detection
(CPD), particularly algorithms like ChangeFinder, have been successfully
applied to segment time series and improve anomaly detection by reducing
temporal uncertainty, especially in multivariate environments. In this work, we
explored how the integration of segmentation techniques, combined with a
heterogeneous ensemble, can enhance anomaly detection in an industrial
production context. The results show that applying segmentation as a
pre-processing step before selecting heterogeneous ensemble algorithms provided
a significant advantage in our case study, improving the AUC-ROC metric from
0.8599 (achieved with a PCA and LSTM ensemble) to 0.9760 (achieved with Random
Forest and XGBoost). This improvement is imputable to the ability of
segmentation to reduce temporal ambiguity and facilitate the learning process
of supervised algorithms. In our future work, we intend to assess the benefit
of introducing weighted features derived from the study of change points,
combined with segmentation and the use of heterogeneous ensembles, to further
optimize model performance in early anomaly detection.

</details>


### [52] [FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms](https://arxiv.org/abs/2510.09085)
*Atul Shree,Harshith Jupuru*

Main category: cs.LG

TL;DR: 提出 FLToP CTC 框架，在 CTC 解码阶段引入帧级 Token 剪枝，通过相对阈值概率动态剔除低概率 token，显著降低计算与内存开销，同时在 LibriSpeech 上实现 10.5x 速度提升与 2.78x 内存压缩，WER 几乎无损。


<details>
  <summary>Details</summary>
Motivation: CTC 解码在资源受限环境中成为性能瓶颈，传统解码需要对每帧进行大规模 token 级运算，导致高计算与内存开销。需要一种简洁、可嵌入且高效的解码策略来提升实时性与可扩展性。

Method: 提出帧级 Token 剪枝的解码算法 FLToP CTC。该方法按相对阈值概率对每一帧中的 token 进行筛选，动态删除低概率 token，从而降低每帧的计算量和内存占用。

Result: 在 LibriSpeech 数据集上，与标准 CTC 解码相比实现 10.5 倍运行时加速与 2.78 倍内存降低，且 WER 几乎无显著下降，具备跨平台的易集成性（CPU、GPU 等）。

Conclusion: FLToP CTC 有望缓解 CTC 的瓶颈，提升资源受限环境下的可扩展性与实时性，促进语音识别的普及与效率提升。

Abstract: CTC-based ASR systems face computational and memory bottlenecks in
resource-limited environments. Traditional CTC decoders, requiring up to 90% of
processing time in systems (e.g., wav2vec2-large on L4 GPUs), face
inefficiencies due to exhaustive token-level operations. This paper introduces
Frame Level Token Pruning for Connectionist Temporal Classification (FLToP
CTC), a novel decoding algorithm that employs frame-level token pruning guided
by a relative threshold probability. By dynamically eliminating low-probability
tokens per frame, FLToP CTC reduces compute and memory demands while
maintaining negligible WER degradation. On LibriSpeech, FLToP CTC achieves a
10.5x runtime speedup and 2.78x memory reduction versus standard CTC decoders.
Its simplicity enables seamless integration into CTC decoders across platforms
(CPUs, GPUs, etc.). FLToP CTC addresses CTC bottlenecks, offering scalability
for resource-limited environments and realtime applications, enhancing speech
recognition accessibility and efficiency.

</details>


### [53] [Neural Codecs as Biosignal Tokenizers](https://arxiv.org/abs/2510.09095)
*Kleanthis Avramidis,Tiantian Feng,Woojae Jeong,Jihwan Lee,Wenhui Cui,Richard M Leahy,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: BioCodec：以离散码本为基础的生物信号表征学习框架（EEG等），通过预训练在大量 EEG 数据上获得通用表示，能在低资源条件下实现多任务解码，且对 EMG 具有适用性，代码和模型开放。


<details>
  <summary>Details</summary>
Motivation: 生物信号如 EEG 通常高维且含噪，需大量预处理与人工特征；大规模预训练模型在生物信号领域受限；提出以离散码本捕捉底层信号特征，改善解码与泛化。

Method: 提出 BioCodec 框架，受神经编解码器启发，将原始信号离散化为码本嵌入的形式；在数千小时 EEG 上进行预训练；在多项下游任务（临床诊断、睡眠生理、语音解码、运动意象等）评估，并对码本的使用和 EEG 连通性做定性分析，且展示对 EMG 的适用性。

Result: 在多项下游任务上具竞争力，尤其在低资源场景；与前沿模型表现相近或超越；对码本嵌入的空间相干性有定性分析；公开源代码与模型检查点。

Conclusion: BioCodec 提供一种通用的生物信号符号化表征框架，兼具良好性能、跨任务适用性与可解释性，具备对不同生物信号的迁移潜力，且便于复现与扩展。

Abstract: Neurophysiological recordings such as electroencephalography (EEG) offer
accessible and minimally invasive means of estimating physiological activity
for applications in healthcare, diagnostic screening, and even immersive
entertainment. However, these recordings yield high-dimensional, noisy
time-series data that typically require extensive pre-processing and
handcrafted feature extraction to reveal meaningful information. Recently,
there has been a surge of interest in applying representation learning
techniques from large pre-trained (foundation) models to effectively decode and
interpret biosignals. We discuss the challenges posed for incorporating such
methods and introduce BioCodec, an alternative representation learning
framework inspired by neural codecs to capture low-level signal characteristics
in the form of discrete tokens. Pre-trained on thousands of EEG hours, BioCodec
shows efficacy across multiple downstream tasks, ranging from clinical
diagnostic tasks and sleep physiology to decoding speech and motor imagery,
particularly in low-resource settings. Additionally, we provide a qualitative
analysis of codebook usage and estimate the spatial coherence of codebook
embeddings from EEG connectivity. Notably, we also document the suitability of
our method to other biosignal data, i.e., electromyographic (EMG) signals.
Overall, the proposed approach provides a versatile solution for biosignal
tokenization that performs competitively with state-of-the-art models. The
source code and model checkpoints are shared.

</details>


### [54] [AdaPM: a Partial Momentum Algorithm for LLM Training](https://arxiv.org/abs/2510.09103)
*Yimu Zhang,Yuanshi Liu,Cong Fang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the training of large language models, momentum is widely used and often
demonstrated to achieve significant acceleration. However, storing momentum
typically presents memory challenges. In this paper, we propose AdaPM, an
adaptive training strategy that leverages partial momentum to implement a
memory-efficient optimizer. To this end, AdaPM utilizes a non-uniform momentum
design: for most blocks, full momentum is not necessary to preserve the
performance of the optimization. In the momentum design of AdaPM, to mitigate
the bias and performance loss caused by partial momentum, we enhance the
partial momentum by a bias correction technique. Empirically, we verify that
our approach reduces memory by over $90\%$ in momentum while maintaining both
efficiency and performance for pretraining various language models ranging from
60M to 1.5B, as well as for supervised fine-tuning and RLHF. AdaPM can further
reduce memory by up to $95\%$ in optimizer states by combining the
memory-efficient technique on the second-order statistic, saving over $30\%$
GPU hours for pretraining GPT-2 1.5B.

</details>


### [55] [MemLoss: Enhancing Adversarial Training with Recycling Adversarial Examples](https://arxiv.org/abs/2510.09105)
*Soroush Mahdi,Maryam Amirmazlaghani,Saeed Saravani,Zahra Dehghanian*

Main category: cs.LG

TL;DR: 提出 MemLoss，通过记忆对抗样本提升对抗训练的效果，在不降低干净数据精度的前提下提升自然准确率与对抗鲁棒性，在 CIFAR-10 等数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决对抗训练中自然准确率与鲁棒性之间的权衡问题；现有方法常常降低干净数据的准确性，需寻找能够利用历史对抗样本来正则化训练的新思路。

Method: 提出 MemLoss，建立一个记忆库来保存训练过程中生成的对抗样本（记忆对抗样本），在整个训练过程中反复使用这些样本来训练模型，并在损失函数中结合这些记忆样本以提升鲁棒性同时保留干净数据的准确性。

Result: 在 CIFAR-10 及其它数据集上，MemLoss 相较于现有对抗训练方法在保持强鲁棒性的同时，获得了更高的自然准确率与总体性能。

Conclusion: MemLoss 通过利用历史对抗样本的记忆机制实现对抗训练的平衡提升，能够在多数据集上同时提升自然准确率和对抗鲁棒性。

Abstract: In this paper, we propose a new approach called MemLoss to improve the
adversarial training of machine learning models. MemLoss leverages previously
generated adversarial examples, referred to as 'Memory Adversarial Examples,'
to enhance model robustness and accuracy without compromising performance on
clean data. By using these examples across training epochs, MemLoss provides a
balanced improvement in both natural accuracy and adversarial robustness.
Experimental results on multiple datasets, including CIFAR-10, demonstrate that
our method achieves better accuracy compared to existing adversarial training
methods while maintaining strong robustness against attacks.

</details>


### [56] [On the Fairness of Privacy Protection: Measuring and Mitigating the Disparity of Group Privacy Risks for Differentially Private Machine Learning](https://arxiv.org/abs/2510.09114)
*Zhi Yang,Changwu Huang,Ke Tang,Xin Yao*

Main category: cs.LG

TL;DR: 提出一种成员推断游戏用于高效审计数据记录的近似最坏情形隐私风险，并在 DP-SGD 中引入自适应分组梯度裁剪以提升隐私保护公平性。


<details>
  <summary>Details</summary>
Motivation: 当前对分组隐私风险的评估多基于平均情况，难以反映不同组之间的差异；最坏情形风险的计算成本高，实用性受限；需要更严格且高效的审计方法来提高 DPML 的公平性。

Method: 提出成员推断游戏用于近似最坏情形隐私风险的高效审计；在 DP-SGD 中引入自适应分组特定梯度裁剪，灵感来自差异性审计研究中的 Canary；大量实验验证其减少分组隐私风险差异。

Result: 实验显示该审计方法提供更严格的分组隐私风险测量，揭示组间风险差异并提升公平性；自适应分组梯度裁剪显著降低分组风险差异。

Conclusion: 该工作促进 DPML 中的隐私保护公平性，提供更高效的风险评估与更公平的训练算法。

Abstract: While significant progress has been made in conventional fairness-aware
machine learning (ML) and differentially private ML (DPML), the fairness of
privacy protection across groups remains underexplored. Existing studies have
proposed methods to assess group privacy risks, but these are based on the
average-case privacy risks of data records. Such approaches may underestimate
the group privacy risks, thereby potentially underestimating the disparity
across group privacy risks. Moreover, the current method for assessing the
worst-case privacy risks of data records is time-consuming, limiting their
practical applicability. To address these limitations, we introduce a novel
membership inference game that can efficiently audit the approximate worst-case
privacy risks of data records. Experimental results demonstrate that our method
provides a more stringent measurement of group privacy risks, yielding a
reliable assessment of the disparity in group privacy risks. Furthermore, to
promote privacy protection fairness in DPML, we enhance the standard DP-SGD
algorithm with an adaptive group-specific gradient clipping strategy, inspired
by the design of canaries in differential privacy auditing studies. Extensive
experiments confirm that our algorithm effectively reduces the disparity in
group privacy risks, thereby enhancing the fairness of privacy protection in
DPML.

</details>


### [57] [Regret Bounds for Adversarial Contextual Bandits with General Function Approximation and Delayed Feedback](https://arxiv.org/abs/2510.09127)
*Orin Levy,Liad Erez,Alon Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 在带有对手延迟反馈的上下文多臂赌博机（CMAB）中提出 regret 最小化算法。对于有限策略类直接访问，获得最优期望界：O(√(KT log|Π|) + √(D log|Π|))。在一般函数近似设定下，借助在线最小二乘回归 oracle，给出在 FIFO 假设下的界限 O(√(KT R_T(O)) + √(d_max D β))，并通过对 Hedge 基 Vovk 聚合预测器的稳定性分析，得到对有限函数类 F 的界限 O(√(KT log|F|) + √(d_max D log|F|))，与下界基本接近。


<details>
  <summary>Details</summary>
Motivation: 在上下文相关的带有对手延迟观测的情形下，研究如何在有限政策集和一般函数类上实现低 regret，量化延迟对学习性能的影响，并提供可实现的 oracle 方案以提升实用性。

Method: 两条主线：1) 给出对有限 Π 的直接访问情形的最优期望 regret 边界。2) 进入一般函数类 F 的设定，利用在线最小二乘回归 oracle O，并分析基于 FIFO 的延迟影响与稳定性 β，通过对 Hedge/Vovk 聚合器的稳定性分析实现对 F 的上界。

Result: 得到两组渐近表现：在有限 Π 下的最佳上界和在通用 F 下的可实现上界，并证明对有限 F 的 Hedge-based 稳定性可使 β ≤ log|F|，从而尽可能接近下界。

Conclusion: 该工作将延迟观测对 CMAB 的影响量化至可实现的 regret 上界，且在有限和无限函数类下均给出接近最优的界限；同时提供了对 Hedge-based Vovk 聚合器的稳定性分析，填补了延迟反馈条件下的理论空白。

Abstract: We present regret minimization algorithms for the contextual multi-armed
bandit (CMAB) problem over $K$ actions in the presence of delayed feedback, a
scenario where loss observations arrive with delays chosen by an adversary. As
a preliminary result, assuming direct access to a finite policy class $\Pi$ we
establish an optimal expected regret bound of $ O (\sqrt{KT \log |\Pi|} +
\sqrt{D \log |\Pi|)} $ where $D$ is the sum of delays. For our main
contribution, we study the general function approximation setting over a
(possibly infinite) contextual loss function class $ \mathcal{F} $ with access
to an online least-square regression oracle $\mathcal{O}$ over $\mathcal{F}$.
In this setting, we achieve an expected regret bound of
$O(\sqrt{KT\mathcal{R}_T(\mathcal{O})} + \sqrt{ d_{\max} D \beta})$ assuming
FIFO order, where $d_{\max}$ is the maximal delay, $\mathcal{R}_T(\mathcal{O})$
is an upper bound on the oracle's regret and $\beta$ is a stability parameter
associated with the oracle. We complement this general result by presenting a
novel stability analysis of a Hedge-based version of Vovk's aggregating
forecaster as an oracle implementation for least-square regression over a
finite function class $\mathcal{F}$ and show that its stability parameter
$\beta$ is bounded by $\log |\mathcal{F}|$, resulting in an expected regret
bound of $O(\sqrt{KT \log |\mathcal{F}|} + \sqrt{d_{\max} D \log
|\mathcal{F}|})$ which is a $\sqrt{d_{\max}}$ factor away from the lower bound
of $\Omega(\sqrt{KT \log |\mathcal{F}|} + \sqrt{D \log |\mathcal{F}|})$ that we
also present.

</details>


### [58] [Score-Based Density Estimation from Pairwise Comparisons](https://arxiv.org/abs/2510.09146)
*Petrus Mikkola,Luigi Acerbi,Arto Klami*

Main category: cs.LG

TL;DR: 通过对对成对比较数据的温和化胜选密度与分数匹配来进行密度估计；提出从胜选密度反推未观测目标密度的方法，并给出 Bradley–Terry 模型下的温和字段估计，以及在扩散模型中的实现。


<details>
  <summary>Details</summary>
Motivation: 解决仅依赖成对比较时的目标密度估计问题；利用专家知识与人类反馈，将胜选密度与目标密度联系起来，通过分数匹配实现去温和化以估计目标密度。

Method: 将未观测目标密度与温和的胜选密度的边缘联系；通过分数匹配学习胜选分数，再通过位置相关的温和场将胜选密度的分数去温和化以估计目标密度；给出温和场的解析公式，并在 Bradley–Terry 模型下提出估计量；使用在温和采样下的扩散模型，基于分数缩放的退火 Langevin 动力学生成温和样本，从而在仅需数百到数千次对比的前提下学习复杂的多变量信念密度。

Result: 给出温和场的解析公式和 Bradley–Terry 下的估计量；通过对温和样本的扩散模型训练，能够从有限的对比中学习复杂的多变量信念密度；理论上体现了目标密度与胜选密度之间的分数共线性关系。

Conclusion: 该框架将对成对比较数据的密度学习转向通过胜选密度及温和化关系进行去温和化，结合扩散模型可在现实数据规模下学习高维信念分布；对专家知识的获取及人机反馈的利用具有潜在的应用价值，但也依赖于对 Bradley–Terry 假设及温和字段的合理性与可估计性。

Abstract: We study density estimation from pairwise comparisons, motivated by expert
knowledge elicitation and learning from human feedback. We relate the
unobserved target density to a tempered winner density (marginal density of
preferred choices), learning the winner's score via score-matching. This allows
estimating the target by `de-tempering' the estimated winner density's score.
We prove that the score vectors of the belief and the winner density are
collinear, linked by a position-dependent tempering field. We give analytical
formulas for this field and propose an estimator for it under the Bradley-Terry
model. Using a diffusion model trained on tempered samples generated via
score-scaled annealed Langevin dynamics, we can learn complex multivariate
belief densities of simulated experts, from only hundreds to thousands of
pairwise comparisons.

</details>


### [59] [Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting](https://arxiv.org/abs/2510.09152)
*Suming Qiu,Jing Li,Zhicheng Zhou,Junjie Huang,Linyuan Qiu,Zhijie Sun*

Main category: cs.LG

TL;DR: 两阶段的 Logits Replay + MoClip，将监督信号压缩到 logits 子集，并通过稳定更新实现对领域适应的高效且不牺牲泛化的方法；在 CT 和 NL2SQL 任务上提升领域性能，同时在多项基线上减缓遗忘，训练成本下降超过 40%。


<details>
  <summary>Details</summary>
Motivation: LLMs在领域适应时存在性能在专业化和通用能力之间的权衡，现有正则化、参数选择性更新或数据回放方法成本高，且简单截断 logits 会引发优化不稳定和遗忘。需要一种更高效、通用且可缩放的适应策略。

Method: 提出两阶段框架：Stage 0记录覆盖一定概率阈值的动态 Top-K 子集，始终包含 gold label；Stage 1对这些紧凑子集进行回放，计算精确的重归一化损失，避免完整 softmax；并引入 MoClip 优化器，通过限制梯度方向的旋转及使用基于 arctan2 的更新重新缩放来保障训练稳定性。

Result: 在 CT、NL2SQL 的领域任务上取得性能提升，同时在 MMLU、BBH、GPQA、MATH 等通用基准上抑制遗忘；训练成本下降超过 40%；方法与模型架构无关，具备可扩展性。

Conclusion: Logits Replay + MoClip 为大规模语言模型的领域自适应提供了一条可扩展、架构无关的路径，在降低成本的同时保持或提升对通用能力的保留。

Abstract: Large language models (LLMs) often face a trade-off in post-training:
improvements on specialized domains frequently come at the expense of general
capabilities. Existing solutions attempt to mitigate this tension via
regularization, selective parameter updates, or data-centric replay, but each
imposes significant costs in computation, data access, or adaptability. Recent
work has shown that training signals can be compressed to subsets of logits
without severe accuracy loss, suggesting a path toward efficient adaptation.
However, naive truncation destabilizes optimization and exacerbates forgetting.
  We introduce Logits Replay + MoClip, a two-stage framework that compresses
supervision in the logit space and stabilizes optimization at the update level.
In Stage 0, we record dynamic Top-K token subsets that cover a probability
threshold, always including the gold label. In Stage 1, we replay these compact
subsets to compute exact renormalized losses, avoiding full softmax computation
and implicitly regularizing. To ensure stability, we design MoClip, an
optimizer that caps gradient-momentum rotation and applies an arctan2-based
rescaling of updates. Empirically, our method improves domain performance on
Communication Technology (CT) and NL2SQL tasks while mitigating forgetting on
general benchmarks (MMLU, BBH, GPQA, MATH), and reduces training cost by over
40%. Together, these contributions offer a scalable, architecture-agnostic path
for domain adaptation of LLMs without sacrificing generalization.

</details>


### [60] [Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.09156)
*Jing Li,Zhijie Sun,Zhicheng Zhou,Suming Qiu,Junjie Huang,Haijia Sun,Linyuan Qiu*

Main category: cs.LG

TL;DR: Agentic-KGR 通过多轮强化学习实现 LLM 与知识图谱的协同进化，包含动态模式扩展、检索增强记忆与可学习的多尺度提示压缩，显著提升知识抽取与问答性能，优于静态知识库及单轮 RL 基线，且与 GraphRAG 结合时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 静态、预构建的知识库存在覆盖不足与时效性下降的问题，难以在动态信息环境中保持LLM的知识准确性与更新能力。

Method: 提出 Agentic-KGR 框架，通过(1) 动态模式扩展机制在训练中系统性扩展本体边界;(2) 检索增强记忆系统实现模型参数与知识结构的协同优化与持续演化;(3) 可学习的多尺度提示压缩通过自适应序列优化在保留关键信息的同时降低计算复杂度；并通过多轮 RL 实现模型与知识结构的共同进化，且在下游任务中结合 GraphRAG。

Result: 在知识抽取任务上明显优于有监督基线和单轮 RL 方法；结合 GraphRAG 时，在下游问答任务上在准确性和知识覆盖方面均优于现有方法。

Conclusion: 证明了通过动态模式扩展、可检索记忆与自适应提示压缩的协同演化可提升知识抽取与问答性能，这类检索增强、动态可扩展的框架具有良好应用前景。

Abstract: Current knowledge-enhanced large language models (LLMs) rely on static,
pre-constructed knowledge bases that suffer from coverage gaps and temporal
obsolescence, limiting their effectiveness in dynamic information environments.
We present Agentic-KGR, a novel framework enabling co-evolution between LLMs
and knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our
approach introduces three key innovations: (1) a dynamic schema expansion
mechanism that systematically extends graph ontologies beyond pre-defined
boundaries during training; (2) a retrieval-augmented memory system enabling
synergistic co-evolution between model parameters and knowledge structures
through continuous optimization; (3) a learnable multi-scale prompt compression
approach that preserves critical information while reducing computational
complexity through adaptive sequence optimization. Experimental results
demonstrate substantial improvements over supervised baselines and single-round
RL approaches in knowledge extraction tasks. When integrated with GraphRAG, our
method achieves superior performance in downstream QA tasks, with significant
gains in both accuracy and knowledge coverage compared to existing methods.

</details>


### [61] [Cross-Representation Benchmarking in Time-Series Electronic Health Records for Clinical Outcome Prediction](https://arxiv.org/abs/2510.09159)
*Tianyi Chen,Mingcheng Zhu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: 系统性基准比较EHR表示方法（多变量时间序列、事件流、文本事件流）在ICU与纵向护理两种临床设置中的表现，结果显示事件流模型最强；预训练模型如CLMBR在少样本情境中高效，计数模型在数据充足时亦具竞争力；特征裁剪的效果取决于场景；提供统一管线以指导在不同数据条件下选择EHR表示。


<details>
  <summary>Details</summary>
Motivation: 由于评价实践不统一，难以确定EHR表示的最优方法。本研究提出首次系统化基准，在多模态EHR表示之间进行横向对比，并在两类临床任务和数据集上进行标准化评估，以便形成可重复、可比的结论。

Method: 在MIMIC-IV用于ICU任务（死亡率、表型）与EHRSHOT用于纵向护理任务（30日再入院、1年 pancreatic cancer）两个数据集上进行数据整理和评估。对每种表示范式评估相应的建模家族：时间序列—Transformer、MLP、LSTM、Retain；事件流—CLMBR与基于计数的模型；文本事件流—8-20B规模的LLM。并分析基于缺失性的数据进行特征裁剪的影响。

Result: 总体上，事件流模型的表现最强；预训练模型如CLMBR在少样本设置下表现出良好的样本效率，然而在数据充足时，简单的计数模型也具有竞争力；特征裁剪策略需结合临床情境：对ICU预测 pruning稀疏特征有益，但对纵向任务保持稀疏特征则至关重要。统一且可复现的管线使临床情境与数据规模下的EHR表征选择具实用意义。

Conclusion: 研究提供了在不同临床情境和数据规模下选择EHR表示的实际指南，强调事件流表示在多任务情境中的稳健性，以及基于缺失性的特征选择应如何针对ICU与纵向护理任务进行取舍。

Abstract: Electronic Health Records (EHRs) enable deep learning for clinical
predictions, but the optimal method for representing patient data remains
unclear due to inconsistent evaluation practices. We present the first
systematic benchmark to compare EHR representation methods, including
multivariate time-series, event streams, and textual event streams for LLMs.
This benchmark standardises data curation and evaluation across two distinct
clinical settings: the MIMIC-IV dataset for ICU tasks (mortality, phenotyping)
and the EHRSHOT dataset for longitudinal care (30-day readmission, 1-year
pancreatic cancer). For each paradigm, we evaluate appropriate modelling
families--including Transformers, MLP, LSTMs and Retain for time-series, CLMBR
and count-based models for event streams, 8-20B LLMs for textual streams--and
analyse the impact of feature pruning based on data missingness. Our
experiments reveal that event stream models consistently deliver the strongest
performance. Pre-trained models like CLMBR are highly sample-efficient in
few-shot settings, though simpler count-based models can be competitive given
sufficient data. Furthermore, we find that feature selection strategies must be
adapted to the clinical setting: pruning sparse features improves ICU
predictions, while retaining them is critical for longitudinal tasks. Our
results, enabled by a unified and reproducible pipeline, provide practical
guidance for selecting EHR representations based on the clinical context and
data regime.

</details>


### [62] [Robustness and Regularization in Hierarchical Re-Basin](https://arxiv.org/abs/2510.09174)
*Benedikt Franke,Florian Heinrich,Markus Lange,Arne Raul*

Main category: cs.LG

TL;DR: 提出分层的模型合并方案，在Git Re-Basin框架下显著优于MergeMany，同时增强鲁棒性，但实验中的性能下降比原作者更多。


<details>
  <summary>Details</summary>
Motivation: 模型合并任务中存在合并冲突和性能损失，需提升合并质量并理解多模型参与对鲁棒性的影响，尤其是在分层合并场景中。

Method: 在Git Re-Basin之上提出分层模型合并流程，与标准MergeMany对比，评估对抗性与扰动鲁棒性，分析参与模型数量对鲁棒性和性能的影响。

Result: 分层合并在实验中显著优于MergeMany；Re-Basin能够在合并模型中引入对抗性和扰动鲁棒性，且参与模型数量越多，鲁棒性越强；但实际实验中性能下降幅度明显大于原作者的报道。

Conclusion: 分层合并在提升鲁棒性方面具有潜力，但代价是更大的性能损失，需进一步核实和改进以提高可重复性与应用价值。

Abstract: This paper takes a closer look at Git Re-Basin, an interesting new approach
to merge trained models. We propose a hierarchical model merging scheme that
significantly outperforms the standard MergeMany algorithm. With our new
algorithm, we find that Re-Basin induces adversarial and perturbation
robustness into the merged models, with the effect becoming stronger the more
models participate in the hierarchical merging scheme. However, in our
experiments Re-Basin induces a much bigger performance drop than reported by
the original authors.

</details>


### [63] [RepDL: Bit-level Reproducible Deep Learning Training and Inference](https://arxiv.org/abs/2510.09180)
*Peichen Xie,Xian Zhang,Shuo Chen*

Main category: cs.LG

TL;DR: RepDL is an open-source library that guarantees deterministic and bitwise-reproducible deep learning training and inference across diverse computing environments by enforcing correct floating-point rounding and order invariance.


<details>
  <summary>Details</summary>
Motivation: Nondeterminism and non-reproducibility in deep learning arise from both random number generation and floating-point computation. While RNG can be controlled, floating-point inconsistencies across platforms/hardwares remain largely unresolved, hindering reproducibility.

Method: RepDL enforces correct floating-point rounding and order invariance within computations, ensuring determinism across training and inference pipelines. It likely provides systematic controls/invariants within FP operations to avoid bitwise differences.

Result: Achieves deterministic and bitwise-reproducible DL across environments. The project is open-source with code available at the provided GitHub URL.

Conclusion: RepDL addresses floating-point nondeterminism, enabling reliable reproducibility for DL workloads across hardware and platforms.

Abstract: Non-determinism and non-reproducibility present significant challenges in
deep learning, leading to inconsistent results across runs and platforms. These
issues stem from two origins: random number generation and floating-point
computation. While randomness can be controlled through deterministic
configurations, floating-point inconsistencies remain largely unresolved. To
address this, we introduce RepDL, an open-source library that ensures
deterministic and bitwise-reproducible deep learning training and inference
across diverse computing environments. RepDL achieves this by enforcing correct
rounding and order invariance in floating-point computation. The source code is
available at https://github.com/microsoft/RepDL .

</details>


### [64] [On the Implicit Adversariality of Catastrophic Forgetting in Deep Continual Learning](https://arxiv.org/abs/2510.09181)
*Ze Peng,Jian Zhang,Jintao Guo,Lei Qi,Yang Gao,Yinghuan Shi*

Main category: cs.LG

TL;DR: 新-task 梯度与旧-task 损失面尖锐方向对齐，导致灾难性遗忘。通过低秩偏置解释对齐并提出 BackGP，优于 GP.


<details>
  <summary>Details</summary>
Motivation: 揭示灾难性遗忘的根本原因并给出有效缓解策略，提升持续学习的稳定性与性能。

Method: 理论分析显示新旧任务梯度在低维子空间中对齐，源于训练中的低秩偏置；评估梯度投影（GP）在前向传播层面能缓解部分对齐，但对反向传播引发的对齐无效；提出 BackGP 以减小反向传播导致的对齐，实验显示在GP方法基础上平均减少遗忘 10.8%、准确率提升 12.7%。

Result: BackGP 相对于 GP 族方法，在平均水平上减少遗忘约 10.8%，准确率提升约 12.7%。

Conclusion: 将灾难性遗忘归因于新任务训练对旧任务知识的对抗性对齐，且该对齐来自低秩偏置，BackGP 能有效缓解该现象。

Abstract: Continual learning seeks the human-like ability to accumulate new skills in
machine intelligence. Its central challenge is catastrophic forgetting, whose
underlying cause has not been fully understood for deep networks. In this
paper, we demystify catastrophic forgetting by revealing that the new-task
training is implicitly an adversarial attack against the old-task knowledge.
Specifically, the new-task gradients automatically and accurately align with
the sharp directions of the old-task loss landscape, rapidly increasing the
old-task loss. This adversarial alignment is intriguingly counter-intuitive
because the sharp directions are too sparsely distributed to align with by
chance. To understand it, we theoretically show that it arises from training's
low-rank bias, which, through forward and backward propagation, confines the
two directions into the same low-dimensional subspace, facilitating alignment.
Gradient projection (GP) methods, a representative family of
forgetting-mitigating methods, reduce adversarial alignment caused by forward
propagation, but cannot address the alignment due to backward propagation. We
propose backGP to address it, which reduces forgetting by 10.8% and improves
accuracy by 12.7% on average over GP methods.

</details>


### [65] [Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs](https://arxiv.org/abs/2510.09201)
*Yumin Choi,Dongki Kim,Jinheon Baek,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了混模态提示优化器(MPO)，通过对文本与非文本模态提示的联合优化与基于贝叶斯的候选搜索，在保持对齐的前提下实现跨模态提示优化，实验表明优于文本端的优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统的提示优化仅针对文本；在多模态大语言模型下，需对文本提示与非文本提示进行联合优化，以充分发挥模型在图像、视频、甚至分子等模态上的潜力。通过利用以往评估结果作为先验信息，可提升搜索效率与效果。

Method: 提出MPO框架：1) 进行多模态提示的对齐保持更新（alignment-preserving updates），实现文本与非文本提示的联合优化；2) 基于贝叶斯策略的候选检索/选择，使用先前评估结果作为先验来引导搜索；3) 跨图像、视频、分子等多模态/跨域实验，验证方法的普适性。

Result: 在多模态（包括图像、视频、分子等）上的大量实验显示，MPO在多模态提示优化任务上优于领先的文本仅优化方法。

Conclusion: 多模态提示优化是实现大模型跨模态潜力的关键步骤，MPO提供了一个统一且高效的优化框架，适用于跨模态的提示设计與搜索。

Abstract: Large Language Models (LLMs) have shown remarkable success, and their
multimodal expansions (MLLMs) further unlock capabilities spanning images,
videos, and other modalities beyond text. However, despite this shift, prompt
optimization approaches, designed to reduce the burden of manual prompt
crafting while maximizing performance, remain confined to text, ultimately
limiting the full potential of MLLMs. Motivated by this gap, we introduce the
new problem of multimodal prompt optimization, which expands the prior
definition of prompt optimization to the multimodal space defined by the pairs
of textual and non-textual prompts. To tackle this problem, we then propose the
Multimodal Prompt Optimizer (MPO), a unified framework that not only performs
the joint optimization of multimodal prompts through alignment-preserving
updates but also guides the selection process of candidate prompts by
leveraging earlier evaluations as priors in a Bayesian-based selection
strategy. Through extensive experiments across diverse modalities that go
beyond text, such as images, videos, and even molecules, we demonstrate that
MPO outperforms leading text-only optimization methods, establishing multimodal
prompt optimization as a crucial step to realizing the potential of MLLMs.

</details>


### [66] [FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in Reinforcement Learning](https://arxiv.org/abs/2510.09222)
*Zhenglin Wan,Jingxuan Wu,Xingrui Yu,Chubin Zhang,Mingcong Lei,Bo An,Ivor Tsang*

Main category: cs.LG

TL;DR: 提出了一种通过教师FM模型驱动的奖励模型 + 简单MLP学生策略的在线学习框架，解决FM在线优化的梯度不稳定与推理成本高的问题，并提升学习效率、泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管Flow Matching（FM）在离线模仿学习中展现出强表达力，缺乏环境交互导致难以泛化到未见场景，且在线优化FM策略存在梯度不稳定和推理成本高的问题，需要一个能有效探索且保持FM信息的方案。

Method: 提出让一个结构简单的MLP学生在环境中进行在线探索，并通过一个基于教师FM模型的奖励模型进行RL更新；同一个教师FM模型用于正则化学生策略以稳定学习，从而避免FM策略的梯度不稳定，同时保留教师FM的丰富分布信息。

Result: 大量实验表明，该方法显著提高了学习效率、泛化性与鲁棒性，尤其在从亚-optimal专家数据学习时效果更为明显。

Conclusion: 通过将探索与FM梯度稳定性解耦，并借助教师FM提供的奖励与正则化信息，实现了更高效、更鲁棒的在线学习，同时保持FM的表达力，有助于离线-在线迁移与抗干扰性增强。

Abstract: Flow Matching (FM) has shown remarkable ability in modeling complex
distributions and achieves strong performance in offline imitation learning for
cloning expert behaviors. However, despite its behavioral cloning
expressiveness, FM-based policies are inherently limited by their lack of
environmental interaction and exploration. This leads to poor generalization in
unseen scenarios beyond the expert demonstrations, underscoring the necessity
of online interaction with environment. Unfortunately, optimizing FM policies
via online interaction is challenging and inefficient due to instability in
gradient computation and high inference costs. To address these issues, we
propose to let a student policy with simple MLP structure explore the
environment and be online updated via RL algorithm with a reward model. This
reward model is associated with a teacher FM model, containing rich information
of expert data distribution. Furthermore, the same teacher FM model is utilized
to regularize the student policy's behavior to stabilize policy learning. Due
to the student's simple architecture, we avoid the gradient instability of FM
policies and enable efficient online exploration, while still leveraging the
expressiveness of the teacher FM model. Extensive experiments show that our
approach significantly enhances learning efficiency, generalization, and
robustness, especially when learning from suboptimal expert data.

</details>


### [67] [Prime Implicant Explanations for Reaction Feasibility Prediction](https://arxiv.org/abs/2510.09226)
*Klaus Weinbauer,Tieu-Long Phan,Peter F. Stadler,Thomas Gärtner,Sagar Malhotra*

Main category: cs.LG

TL;DR: 提出面向化学反应可行性预测的最小充分原因（prime implicant）解释，并给出适用于小规模任务的计算算法。初步实验表明该解释在保守覆盖 ground truth 的关键特征方面有效，尽管可能包含冗余的键与原子。


<details>
  <summary>Details</summary>
Motivation: 解决化学反应可行性预测模型在透明性与可解释性方面的不足；将 prime implicant 的思想引入化学域，给出可计算且尽量精简的解释，以便识别对预测结果关键的分子属性。

Method: 提出针对化学反应领域的 prime implicant 解释的新表述，并设计一个用于小规模反应预测任务的计算算法来获得这类解释。

Result: 初步实验表明所提出的解释能够保守地捕获 ground truth 的解释，尽管包含冗余的键与原子，但能一致地捕捉对预测可行性关键的分子属性。

Conclusion: 为化学反应预测提供了可计算的因果解释框架，展示了 prime implicant 解释在该领域的可行性与潜在价值；需进一步在更大规模数据集上验证与扩展。

Abstract: Machine learning models that predict the feasibility of chemical reactions
have become central to automated synthesis planning. Despite their predictive
success, these models often lack transparency and interpretability. We
introduce a novel formulation of prime implicant explanations--also known as
minimally sufficient reasons--tailored to this domain, and propose an algorithm
for computing such explanations in small-scale reaction prediction tasks.
Preliminary experiments demonstrate that our notion of prime implicant
explanations conservatively captures the ground truth explanations. That is,
such explanations often contain redundant bonds and atoms but consistently
capture the molecular attributes that are essential for predicting reaction
feasibility.

</details>


### [68] [Incentivizing Time-Aware Fairness in Data Sharing](https://arxiv.org/abs/2510.09240)
*Jiangwei Chen,Kieu Thao Nguyen Pham,Rachael Hwee Ling Sim,Arun Verma,Zhaoxuan Wu,Chuan-Sheng Foo,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 提出一个时间感知的数据分享框架，通过为先加入者分配更高价值的奖励来激励晚加入者，确保公平性和个体理性，并给出实现奖励值和模型奖励的具体方法及实验验证。


<details>
  <summary>Details</summary>
Motivation: 在多方协作的前提下，考虑不同参与时间导致的风险差异，设计可实现的时间感知激励，解决现实世界中非同步加入带来的激励与公平问题。

Method: 提出时间感知的奖励分配方法、确定奖励值的机制、以及如何通过模型奖励实现这些值；并在合成和真实数据集上进行实验验证，展示公平性、个体理性和效果提升。

Result: 给出可实现的时间敏感奖励框架及其理论和实验结果，证明所提方法在分配奖励时兼顾公平性、理性约束，并提升模型性能或资源利用。

Conclusion: 提出一整套时间感知数据共享框架及激励设计思路，便于在异步参与情景下实现公平且可行的数据协作。

Abstract: In collaborative data sharing and machine learning, multiple parties
aggregate their data resources to train a machine learning model with better
model performance. However, as the parties incur data collection costs, they
are only willing to do so when guaranteed incentives, such as fairness and
individual rationality. Existing frameworks assume that all parties join the
collaboration simultaneously, which does not hold in many real-world scenarios.
Due to the long processing time for data cleaning, difficulty in overcoming
legal barriers, or unawareness, the parties may join the collaboration at
different times. In this work, we propose the following perspective: As a party
who joins earlier incurs higher risk and encourages the contribution from other
wait-and-see parties, that party should receive a reward of higher value for
sharing data earlier. To this end, we propose a fair and time-aware data
sharing framework, including novel time-aware incentives. We develop new
methods for deciding reward values to satisfy these incentives. We further
illustrate how to generate model rewards that realize the reward values and
empirically demonstrate the properties of our methods on synthetic and
real-world datasets.

</details>


### [69] [A PCA-based Data Prediction Method](https://arxiv.org/abs/2510.09246)
*Peteris Daugulis,Vija Vagale,Emiliano Mancini,Filippo Castiglione*

Main category: cs.LG

TL;DR: 提出一种基于子空间距离的新缺失数据插补方法：用数据的前k个主成分张成的子空间表示，并在位移子空间之间定义距离，给出欧几里得度量下的解法。


<details>
  <summary>Details</summary>
Motivation: 缺失数据预测在数据科学中普遍且重要，现有方法常受限于统计假设或模型复杂性。该研究引入几何化视角，通过比较移位的线性子空间之间的距离来进行插补，融合传统数学与机器学习要素。

Method: 将现有数据表示为由前k个主成分张成的子空间；考虑子空间的平移（shifted subspaces）以匹配缺失模式；定义子空间之间的距离，特别在欧几里得度量下推导可实现的缺失值推断或优化问题；方法兼具经典矩阵分析与机器学习策略，以实现缺失数据的预测。

Result: 在欧几里得度量下给出了解的具体形式或算法步骤；摘要未给出实验结果，但明确给出在该度量下的可实现解。

Conclusion: 该方法提供了一种几何直观的新途径用于缺失数据插补，将数据的主成分子空间与位移子空间之间的距离作为预测准则，兼顾传统数学与机器学习的优势，具有潜在的普适性与灵活性。

Abstract: The problem of choosing appropriate values for missing data is often
encountered in the data science. We describe a novel method containing both
traditional mathematics and machine learning elements for prediction
(imputation) of missing data. This method is based on the notion of distance
between shifted linear subspaces representing the existing data and candidate
sets. The existing data set is represented by the subspace spanned by its first
principal components. Solutions for the case of the Euclidean metric are given.

</details>


### [70] [Mitigating Model Drift in Developing Economies Using Synthetic Data and Outliers](https://arxiv.org/abs/2510.09294)
*Ilyas Varshavskiy,Bonu Boboeva,Shuhrat Khalilbekov,Azizjon Azimi,Sergey Shulgin,Akhlitdin Nizamitdinov,Haitz Saez de Ocariz Borde*

Main category: cs.LG

TL;DR: 在中亚-高加索地区金融数据的模型漂移问题上，提出通过加入少量合成异常点来提高预测稳定性的两级评估框架，并在宏观经济表格数据上证实该方法的有效性，但最佳合成异常点比例因数据集与模型而异。


<details>
  <summary>Details</summary>
Motivation: 金融领域的模型漂移在新兴市场尤为严重，跨区域的宏观经济冲击会导致数据分布快速变化。本文在缺乏系稳态的区域（包括塔吉克斯坦、乌兹别克斯坦、哈萨克斯坦和阿塞拜疆）中首次尝试用合成异常点来增强模型对未知冲击的鲁棒性，并提出一种衡量漂移影响的评估框架。

Method: 提出一个两级框架，用于同时衡量模型性能的降解程度和冲击强度；在宏观经济的表格数据集上，通过引入少量的合成异常点来增强数据集的鲁棒性，并对不同数据集和模型的合成比例进行探索，比较基线模型的稳定性。

Result: 实验结果显示，添加少量的合成异常点通常能提升模型在冲击情境下的稳定性，相对于基线模型表现更鲁棒，但最佳比例会因数据集和所用模型而异。

Conclusion: 合成异常点是一种有前景的漂移缓解策略，尤其在对冲击敏感的金融数据上有效，但需对比例和模型进行细致调优，并有待在更广泛数据集上验证其普适性。

Abstract: Machine Learning models in finance are highly susceptible to model drift,
where predictive performance declines as data distributions shift. This issue
is especially acute in developing economies such as those in Central Asia and
the Caucasus - including Tajikistan, Uzbekistan, Kazakhstan, and Azerbaijan -
where frequent and unpredictable macroeconomics shocks destabilize financial
data. To the best of our knowledge, this is among the first studies to examine
drift mitigation methods on financial datasets from these regions. We
investigate the use of synthetic outliers, a largely unexplored approach, to
improve model stability against unforeseen shocks. To evaluate effectiveness,
we introduce a two-level framework that measures both the extent of performance
degradation and the severity of shocks. Our experiments on macroeconomic
tabular datasets show that adding a small proportion of synthetic outliers
generally improves stability compared to baseline models, though the optimal
amount varies by dataset and model

</details>


### [71] [Large Language Model Prompt Datasets: An In-depth Analysis and Insights](https://arxiv.org/abs/2510.09316)
*Yuanming Zhang,Yan Lin,Arijit Khan,Huaiyu Wan*

Main category: cs.LG

TL;DR: 本文汇总了大规模提示数据集并提出基于句法嵌入的中心化重写策略，改善模型输出的意义性，且公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛部署，来自GitHub和社交媒体的提示数据日益增多，亟需系统分析以理解提示设计规律并提升提示工程质量。

Method: 编纂来自多渠道的提示数据集，挑选关键代表数据进行系统分析，比较不同类别的提示构造；提出以词性与依赖关系的句法嵌入为基础的质心（centroid）提示重写策略，以引导LLM更接近中心表达。

Result: 揭示提示构造的共性与差异；提示具有与一般文本不同的结构特征；提出的重写策略在提升输出意义性方面表现出改进；数据集与代码已公开。

Conclusion: 提供可扩展的提示数据分析框架和一个可操作的中心化重写方法，具备促进提示工程与模型对齐的潜力。

Abstract: A prompt is a natural language instruction that defines a specific task for a
large language model (LLM) and serves as the primary interface for human-LLM
interaction. With the growing deployment of LLMs, diverse prompt datasets are
emerging from platforms such as GitHub and social media. These datasets span a
wide array of applications and content types, facilitating both broader LLM
utilization and improved prompt engineering. In this work, we--for the first
time--have compiled an extensive list of prompt datasets sourced from various
channels, representing a spectrum of downstream tasks, languages, engineering
techniques, attributes, and modalities. We select key representative datasets
for systematic analysis, revealing commonalities and differences in prompt
construction across categories, distinguishing them from other text corpora
like literature and web. We further propose a prompt optimization approach that
leverages syntactic embeddings of part-of-speech and dependency structures. By
identifying a centroid representation of prompts and guiding LLMs to rewrite
prompts toward this centroid, our method improves the meaningfulness of model
outputs. We have made our datasets and code available.

</details>


### [72] [Residual-Informed Learning of Solutions to Algebraic Loops](https://arxiv.org/abs/2510.09317)
*Felix Brandt,Andreas Heuermann,Philip Hannebohm,Bernhard Bachmann*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a residual-informed machine learning approach for
replacing algebraic loops in equation-based Modelica models with neural network
surrogates. A feedforward neural network is trained using the residual (error)
of the algebraic loop directly in its loss function, eliminating the need for a
supervised dataset. This training strategy also resolves the issue of ambiguous
solutions, allowing the surrogate to converge to a consistent solution rather
than averaging multiple valid ones. Applied to the large-scale IEEE 14-Bus
system, our method achieves a 60% reduction in simulation time compared to
conventional simulations, while maintaining the same level of accuracy through
error control mechanisms.

</details>


### [73] [Rate optimal learning of equilibria from data](https://arxiv.org/abs/2510.09325)
*Till Freihaut,Luca Viano,Emanuele Nevali,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We close open theoretical gaps in Multi-Agent Imitation Learning (MAIL) by
characterizing the limits of non-interactive MAIL and presenting the first
interactive algorithm with near-optimal sample complexity. In the
non-interactive setting, we prove a statistical lower bound that identifies the
all-policy deviation concentrability coefficient as the fundamental complexity
measure, and we show that Behavior Cloning (BC) is rate-optimal. For the
interactive setting, we introduce a framework that combines reward-free
reinforcement learning with interactive MAIL and instantiate it with an
algorithm, MAIL-WARM. It improves the best previously known sample complexity
from $\mathcal{O}(\varepsilon^{-8})$ to $\mathcal{O}(\varepsilon^{-2}),$
matching the dependence on $\varepsilon$ implied by our lower bound. Finally,
we provide numerical results that support our theory and illustrate, in
environments such as grid worlds, where Behavior Cloning fails to learn.

</details>


### [74] [Efficient Bayesian Inference from Noisy Pairwise Comparisons](https://arxiv.org/abs/2510.09333)
*Till Aczel,Lucas Theis,Wattenhofer Roger*

Main category: cs.LG

TL;DR: 提出 BBQ：一种贝叶斯 Bradley-Terry 变体，用于对比评估中对评审者质量进行显式建模，从而在噪声和众包评审下实现更稳健、可解释的排序与更快收敛的不等式最大似然估计。


<details>
  <summary>Details</summary>
Motivation: 在生成模型评估中，常用指标往往与人类偏好不一致；人类评价成本高且噪声大，评审者的专业程度、注意力和勤奋程度不同，导致结果不稳定。对比评估虽能提高一致性，但汇总成总体质量分数需要谨慎建模。

Method: 提出 BBQ（贝叶斯 Bradley-Terry 变体），显式建模评审者质量，能够对不可靠参与者进行降权或剔除；通过期望最大化（EM）算法确保似然函数的单调收敛；对比项分数通过贝叶斯后验更新，兼容有噪声的评审者。

Result: 实验表明，BBQ 相较于基线的 Bradley-Terry 模型，收敛更快、不确定性估计更校准，排序更稳健、可解释，且在噪声或众包评审者存在时仍表现良好。

Conclusion: 该框架为生成模型的人类评估提供更可靠、成本效益更高的解决方案。

Abstract: Evaluating generative models is challenging because standard metrics often
fail to reflect human preferences. Human evaluations are more reliable but
costly and noisy, as participants vary in expertise, attention, and diligence.
Pairwise comparisons improve consistency, yet aggregating them into overall
quality scores requires careful modeling. Bradley-Terry-based methods update
item scores from comparisons, but existing approaches either ignore rater
variability or lack convergence guarantees, limiting robustness and
interpretability. We introduce BBQ, a Bayesian Bradley-Terry variant that
explicitly models rater quality, downweighting or removing unreliable
participants, and provides guaranteed monotonic likelihood convergence through
an Expectation-Maximization algorithm. Empirical results show that BBQ achieves
faster convergence, well-calibrated uncertainty estimates, and more robust,
interpretable rankings compared to baseline Bradley-Terry models, even with
noisy or crowdsourced raters. This framework enables more reliable and
cost-effective human evaluation of generative models.

</details>


### [75] [The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton](https://arxiv.org/abs/2510.09378)
*Natalie Abreu,Nikhil Vyas,Sham Kakade,Depen Morwani*

Main category: cs.LG

TL;DR: 在LLM预训练中，使用全高斯-牛顿预条件显著提升训练效率：相较强基线（SOAP、Muon）迭代次数降低约5.4倍；分层GN近似（忽略跨层信息）几乎与全GN等效，表明高阶损失项对收敛速度非关键。


<details>
  <summary>Details</summary>
Motivation: 探究二阶近似在大规模Transformer训练中的性能损失与潜在收益，评估全GN预conditioning对收敛速度的实际价值以及层内结构信息的利用程度。

Method: 将全高斯-牛顿（GN）预 conditioning 应用于 Transformer 模型（规模约150M参数），比较与SOAP、Muon等优化器的迭代次数与训练效率；另外比较层内GN（忽略跨层信息）与全GN 的效果差异。

Result: 全GN 更新在训练迭代方面带来显著增益，迭代次数比强基线减少约5.4倍；层内GN对比全GN几乎无差，表明跨层信息在提升速度方面贡献有限；GN 近似对预 conditioning 高效，暗示高阶项不必然是加速收敛的关键；层内Hessian结构包含大部分可用增益信息，但仍存在与理想的分层oracle之间的显著性能差距。

Conclusion: GN近似对预 conditioning 高效，表明高阶损失项对收敛速度的依赖较小；分层Hessian信息足以获得大部分潜在增益；当前近似方法与理想层级oracle之间仍有显著的性能差距，指向未来改进方向在于更好地利用分层结构而非跨层耦合。

Abstract: Recent efforts to accelerate LLM pretraining have focused on
computationally-efficient approximations that exploit second-order structure.
This raises a key question for large-scale training: how much performance is
forfeited by these approximations? To probe this question, we establish a
practical upper bound on iteration complexity by applying full Gauss-Newton
(GN) preconditioning to transformer models of up to 150M parameters. Our
experiments show that full GN updates yield substantial gains over existing
optimizers, achieving a 5.4x reduction in training iterations compared to
strong baselines like SOAP and Muon. Furthermore, we find that a precise
layerwise GN preconditioner, which ignores cross-layer information, nearly
matches the performance of the full GN method. Collectively, our results
suggest: (1) the GN approximation is highly effective for preconditioning,
implying higher-order loss terms may not be critical for convergence speed; (2)
the layerwise Hessian structure contains sufficient information to achieve most
of these potential gains; and (3) a significant performance gap exists between
current approximate methods and an idealized layerwise oracle.

</details>


### [76] [CHUCKLE -- When Humans Teach AI To Learn Emotions The Easy Way](https://arxiv.org/abs/2510.09382)
*Ankush Pratap Singh,Houwei Cao,Yong Liu*

Main category: cs.LG

TL;DR: 提出了一种以人类感知难度为驱动的课程学习框架 CHUCKLE，用众包数据中的评注者一致性来定义样本难度。相较于非课程 baselines，对 LSTM 提升约 6.56%，对 Transformer 提升约 1.61%，并减少梯度更新次数，提高训练效率与模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有情绪识别的课程学习多基于启发式、数据驱动或模型内在定义的样本难度，忽视了人类感知的主观性难度，这是情绪识别等主观任务的关键因素。

Method: 在众包数据集中通过分析评注者的一致性和对齐程度来定义样本难度，将该感知难度用于课程学习流程。假设对人类而言较难的片段对机器学习模型也较难，进而按难度排序进行训练更新。对比实验显示对 LSTM 与 Transformer 的性能与训练效率均有提升。

Result: 相对平均准确率在 LSTM 上提升 6.56%，在 Transformer 上提升 1.61%；同时减少梯度更新次数，提升训练效率与鲁棒性。

Conclusion: 以人类感知为导向的课程学习框架能够在情绪识别等主观任务中提升模型性能与训练效率，凸显将人类感知难度作为样本难度定义的潜力。

Abstract: Curriculum learning (CL) structures training from simple to complex samples,
facilitating progressive learning. However, existing CL approaches for emotion
recognition often rely on heuristic, data-driven, or model-based definitions of
sample difficulty, neglecting the difficulty for human perception, a critical
factor in subjective tasks like emotion recognition. We propose CHUCKLE
(Crowdsourced Human Understanding Curriculum for Knowledge Led Emotion
Recognition), a perception-driven CL framework that leverages annotator
agreement and alignment in crowd-sourced datasets to define sample difficulty,
under the assumption that clips challenging for humans are similarly hard for
machine learning models. Empirical results suggest that CHUCKLE increases the
relative mean accuracy by 6.56% for LSTMs and 1.61% for Transformers over
non-curriculum baselines, while reducing the number of gradient updates,
thereby enhancing both training efficiency and model robustness.

</details>


### [77] [HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness](https://arxiv.org/abs/2510.09388)
*Xinyi Wang,Jinyi Han,Zishang Jiang,Tingyun Li,Jiaqing Liang,Sihang Jiang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.LG

TL;DR: 提出 Affinity 指标与 HINT 框架，以提升强化学习驱动的大语言模型在长链路推理中的探索效率和训练稳定性，解决基于外部引导的低训练亲和性问题，并实现数据更高效的学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法在任务难度超出模型容量时易导致奖励稀疏和训练低效；将 RL 与 SFT、提示等混合的数据策略虽有帮助，但往往偏向外部指引，错误引导策略更新；需要一个可量化的诊断指标来评估探索与训练的匹配度及稳定性，并提出能在不直接给出答案的前提下引导推理的改进框架。

Method: 提出 Affinity 作为首个可量化的探索效率与训练稳定性监控指标，用以诊断外部引导与模型策略之间的分布差异；提出自适应启发式提示框架 HINT（帮助无效回滚走向有效性），通过提供间接线索引导模型自主发现解题路径，而非直接给出答案；在数学推理等任务上进行广泛实验，比较现有方法，评估不同规模模型的数据效率和稳定性。

Result: 实验结果显示，HINT 在数学推理任务上持续优于现有方法，达到多种尺度模型的状态最优（state-of-the-art），并且实现了更稳定的学习过程和更高的数据效率；代码已公开在 GitHub。

Conclusion: Affinity 提供了一个诊断性工具以评估 RL-LLM 训练中的探索-利用匹配程度，HINT 则通过自适应启发式提示提升探索效率和学习稳定性，克服奖励稀疏与外部引导偏差的问题，整体上推动了 RL-LLM 的性能与数据利用率提升。

Abstract: Reinforcement Learning (RL) has become a key driver for enhancing the long
chain-of-thought (CoT) reasoning capabilities of Large Language Models (LLMs).
However, prevalent methods like GRPO often fail when task difficulty exceeds
the model's capacity, leading to reward sparsity and inefficient training.
While prior work attempts to mitigate this using off-policy data, such as
mixing RL with Supervised Fine-Tuning (SFT) or using hints, they often misguide
policy updates In this work, we identify a core issue underlying these
failures, which we term low training affinity. This condition arises from a
large distributional mismatch between external guidance and the model's policy.
To diagnose this, we introduce Affinity, the first quantitative metric for
monitoring exploration efficiency and training stability. To improve Affinity,
we propose HINT: Helping Ineffective rollouts Navigate Towards effectiveness,
an adaptive hinting framework. Instead of providing direct answers, HINT
supplies heuristic hints that guide the model to discover solutions on its own,
preserving its autonomous reasoning capabilities. Extensive experiments on
mathematical reasoning tasks show that HINT consistently outperforms existing
methods, achieving state-of-the-art results with models of various scales,
while also demonstrating significantly more stable learning and greater data
efficiency.Code is available on Github.

</details>


### [78] [Cross-Receiver Generalization for RF Fingerprint Identification via Feature Disentanglement and Adversarial Training](https://arxiv.org/abs/2510.09405)
*Yuhao Pan,Xiucheng Wang,Nan Cheng,Wenchao Xu*

Main category: cs.LG

TL;DR: 提出一种对跨接收机变动鲁棒的射频指纹识别框架，通过对抗训练与风格迁移实现发射器与接收机特征的解耦，以获得跨接收机的一致性特征表示。实验表明在多接收机设置下对比基线有显著提升，平均准确率提升可达约10%。


<details>
  <summary>Details</summary>
Motivation: 现实场景中接收机的偏差会导致射频指纹识别模型对特定接收机过拟合，损害从一个接收机到另一个接收机的迁移与泛化能力。尽管信道均衡可减小信道噪声影响，接收机引入的特征偏移仍未有效解决，制约了RFFI在多接收机部署中的鲁棒性。

Method: 提出将对抗训练与风格迁移结合的RFFI框架，通过显式分离发射机与接收机特征来实现领域不变表示学习。具体做法包括：1) 使用对抗目标抑制接收机信息的可预测性，提升发射机特征的鲁棒性；2) 引入风格迁移将不同接收机的特征风格对齐，减小跨接收机的风格差异；3) 在多接收机数据上训练并评估，以实现对新接收机的泛化。

Result: 在多接收机数据集上的广泛实验显示，所提方法在不同接收机设置下持续优于最先进的基线，平均准确率提升约10%。

Conclusion: 该框架实现了跨接收机的鲁棒RFFI，通过学习领域不变的发射机表示，降低了接收机特定模式对识别性能的影响，具备更好的实际部署潜力。未来工作可拓展至更多接收机场景与无监督自适应等方向。

Abstract: Radio frequency fingerprint identification (RFFI) is a critical technique for
wireless network security, leveraging intrinsic hardware-level imperfections
introduced during device manufacturing to enable precise transmitter
identification. While deep neural networks have shown remarkable capability in
extracting discriminative features, their real-world deployment is hindered by
receiver-induced variability. In practice, RF fingerprint signals comprise
transmitter-specific features as well as channel distortions and
receiver-induced biases. Although channel equalization can mitigate channel
noise, receiver-induced feature shifts remain largely unaddressed, causing the
RFFI models to overfit to receiver-specific patterns. This limitation is
particularly problematic when training and evaluation share the same receiver,
as replacing the receiver in deployment can cause substantial performance
degradation. To tackle this challenge, we propose an RFFI framework robust to
cross-receiver variability, integrating adversarial training and style transfer
to explicitly disentangle transmitter and receiver features. By enforcing
domain-invariant representation learning, our method isolates genuine hardware
signatures from receiver artifacts, ensuring robustness against receiver
changes. Extensive experiments on multi-receiver datasets demonstrate that our
approach consistently outperforms state-of-the-art baselines, achieving up to a
10% improvement in average accuracy across diverse receiver settings.

</details>


### [79] [What Do Temporal Graph Learning Models Learn?](https://arxiv.org/abs/2510.09416)
*Abigail J. Hayes,Tobias Schumacher,Markus Strohmaier*

Main category: cs.LG

TL;DR: 系统评估七种时序图学习模型对八种链接结构属性的学习能力，揭示模型在某些属性上表现良好、在其他属性上存在显著不足，强调需要更具解释性的评估。


<details>
  <summary>Details</summary>
Motivation: 回应基准结果的可信性问题和简单启发式方法的竞争力，探究时序图学习真正利用了哪些图结构属性，并推动可解释性驱动的评估。

Method: 在合成和真实数据集上，对七个模型进行系统评估，针对八种与链接结构相关的属性（包括密度、最近性、同质性等），分析模型对这些属性的学习能力。

Result: 模型对某些属性学习良好、对其他属性显著不足，呈现混合结果，暴露出重要的局限性。

Conclusion: 为时序图学习的应用提供实用洞察，推动在时序图学习研究中采用更具解释性和属性导向的评估。

Abstract: Learning on temporal graphs has become a central topic in graph
representation learning, with numerous benchmarks indicating the strong
performance of state-of-the-art models. However, recent work has raised
concerns about the reliability of benchmark results, noting issues with
commonly used evaluation protocols and the surprising competitiveness of simple
heuristics. This contrast raises the question of which properties of the
underlying graphs temporal graph learning models actually use to form their
predictions. We address this by systematically evaluating seven models on their
ability to capture eight fundamental attributes related to the link structure
of temporal graphs. These include structural characteristics such as density,
temporal patterns such as recency, and edge formation mechanisms such as
homophily. Using both synthetic and real-world datasets, we analyze how well
models learn these attributes. Our findings reveal a mixed picture: models
capture some attributes well but fail to reproduce others. With this, we expose
important limitations. Overall, we believe that our results provide practical
insights for the application of temporal graph learning models, and motivate
more interpretability-driven evaluations in temporal graph learning research.

</details>


### [80] [Bandits with Single-Peaked Preferences and Limited Resources](https://arxiv.org/abs/2510.09425)
*Gur Keinan,Rotem Torkan,Omer Ben-Porat*

Main category: cs.LG

TL;DR: 在单峰偏好假设下，给出离线预算匹配的高效近似算法，并基于此设计在线学习算法，在无结构假设下的 regret 为 term? [待替换] 这里给出一个简明结论：通过PQ树近似实现高效离线解，并获得可控的在线 regret；若已知单峰结构，能用UCB风格算法实现更低的 regret。


<details>
  <summary>Details</summary>
Motivation: 在无结构前提下，在线匹配的最优解计算为 NP-hard，且直接在线学习难以实现。单峰偏好提供了一个可观的结构性约束，可能实现可计算且可学习的在线方案。

Method: 提出基于PQ树的顺序近似方法用于离线预算匹配的高效求解；在此基础上构造在线算法，获得 regret 的上界为  O(UKT^{2/3})。若单峰结构已知，则给出一个类似 UCB 的在线算法， regret 为  O(U sqrt(TK))。

Result: 离线问题通过PQ树实现高效的近似解，得到可执行的算法框架；在线情形在无结构假设下实现了  O(UKT^{2/3}) 的 regret，且若单峰结构已知，能够达到更好的  O(U sqrt(TK)) 的 regret；该工作建立了结构性假设（单峰偏好）对提高可计算性与学习性的重要性。

Conclusion: 单峰偏好为在线预算匹配带来可计算性与学习性上的显著提升。PQ树作为順序近似工具，为离线预算匹配提供有效解，进而支撑更优的在线学习策略；未来工作可能扩展到更广的偏好结构或更紧的 regrets 上界。

Abstract: We study an online stochastic matching problem in which an algorithm
sequentially matches $U$ users to $K$ arms, aiming to maximize cumulative
reward over $T$ rounds under budget constraints. Without structural
assumptions, computing the optimal matching is NP-hard, making online learning
computationally infeasible. To overcome this barrier, we focus on
\emph{single-peaked preferences} -- a well-established structure in social
choice theory, where users' preferences are unimodal with respect to a common
order over arms. We devise an efficient algorithm for the offline budgeted
matching problem, and leverage it into an efficient online algorithm with a
regret of $\tilde O(UKT^{2/3})$. Our approach relies on a novel PQ tree-based
order approximation method. If the single-peaked structure is known, we develop
an efficient UCB-like algorithm that achieves a regret bound of $\tilde
O(U\sqrt{TK})$.

</details>


### [81] [Cross-attention Secretly Performs Orthogonal Alignment in Recommendation Models](https://arxiv.org/abs/2510.09435)
*Hyunin Lee,Yong Zhang,Hoang Vu Nguyen,Xiaoyi Liu,Namyong Park,Christopher Jung,Rong Jin,Yang Wang,Zhigang Wang,Somayeh Sojoudi,Xue Feng*

Main category: cs.LG

TL;DR: 提出 Orthogonal Alignment 的现象，指出跨域序列推荐中的跨注意力不仅能做残差对齐，还能发现查询中不存在的新信息，两者可共存。该正交性自发出现且能提升性能与参数效率，且跨注意力模块能带来比参数等量的基线更高的准确性/参数比，指向多模态研究中的参数高效扩展新方向。


<details>
  <summary>Details</summary>
Motivation: 跨域序列推荐中的跨注意力机制尚未被充分理解；主流观点将跨注意力视为残差对齐，仅参考另一域数据对查询的数据进行信息删除和保留，缺乏对潜在正交信息的挖掘与利用的探索；需要揭示跨注意力可能产生的正交信息以及其对模型缩放与参数效率的影响。

Method: 进行大规模实证分析（超过300组实验），系统检查跨注意力输出与查询输入之间的正交性及其与模型性能的关系；将带跨注意力的基线与参数数量匹配的基线进行对比，评估在不同设置下信息对齐的效果；在无显式正交性约束的情形下观察 Orthogonal Alignment 的自发出现及对缩放定律的影响。

Result: 发现 Orthogonal Alignment 能自然出现且在查询输入与跨注意力输出正交时模型性能显著提升；基线加入跨注意力模块相较于参数数量等同的对等基线，表现出更优的准确性对参数比；跨域多模态场景中此现象有助于提升缩放效率与性能。

Conclusion: 跨注意力的对齐机制可同时存在（残差对齐与正交信息对齐），且正交信息对模型缩放和参数效率具有积极影响；这些发现为多模态研究中的参数高效扩展提供新方向，提示在设计跨模态注意力时应关注信息的正交特性与信息增益。

Abstract: Cross-domain sequential recommendation (CDSR) aims to align heterogeneous
user behavior sequences collected from different domains. While cross-attention
is widely used to enhance alignment and improve recommendation performance, its
underlying mechanism is not fully understood. Most researchers interpret
cross-attention as residual alignment, where the output is generated by
removing redundant and preserving non-redundant information from the query
input by referencing another domain data which is input key and value. Beyond
the prevailing view, we introduce Orthogonal Alignment, a phenomenon in which
cross-attention discovers novel information that is not present in the query
input, and further argue that those two contrasting alignment mechanisms can
co-exist in recommendation models We find that when the query input and output
of cross-attention are orthogonal, model performance improves over 300
experiments. Notably, Orthogonal Alignment emerges naturally, without any
explicit orthogonality constraints. Our key insight is that Orthogonal
Alignment emerges naturally because it improves scaling law. We show that
baselines additionally incorporating cross-attention module outperform
parameter-matched baselines, achieving a superior accuracy-per-model parameter.
We hope these findings offer new directions for parameter-efficient scaling in
multi-modal research.

</details>


### [82] [On Uniformly Scaling Flows: A Density-Aligned Approach to Deep One-Class Classification](https://arxiv.org/abs/2510.09452)
*Faried Abu Zaid,Tim Katzke,Emmanuel Müller,Daniel Neider*

Main category: cs.LG

TL;DR: 将无监督异常检测中的密度建模与 Deep SVDD 的距离化思想统一起来的理论桥梁：Uniformly Scaling Flows（USFs）通过最大似然训练等价于带正则的 Deep SVDD，并在密度保真性与潜在向量范数之间实现更紧密的对齐，且在实践中带来稳定性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决两大异常检测主流 paradigm 的割裂：基于流的密度估计与基于距离的单类方法；避免表示崩溃，提升训练稳定性与理论一致性。

Method: 提出恒定雅可比行列式的 Uniformly Scaling Flows（USFs），证明在最大似然训练下等价于带唯一正则化的 Deep SVDD 目标；分析 USFs 如何同时兼具密度保真性和距离式推理，并将该思想扩展到与 VAE 的混合方法；建议将 USFs 作为非 USF 的替代。

Result: 给出理论结论：USF 的训练等价性、正则化对避免崩溃的作用、提高 NLL 与潜在范数对齐的紧密性；在多组 benchmarks 和 模型骨架下，图像级与像素级检测的稳定性和性能显著提升。

Conclusion: USFs 成为统一两大异常检测范式的可直接使用的组件，增强理论理解和实际表现，推动异常检测研究和应用的桥接。

Abstract: Unsupervised anomaly detection is often framed around two widely studied
paradigms. Deep one-class classification, exemplified by Deep SVDD, learns
compact latent representations of normality, while density estimators realized
by normalizing flows directly model the likelihood of nominal data. In this
work, we show that uniformly scaling flows (USFs), normalizing flows with a
constant Jacobian determinant, precisely connect these approaches.
Specifically, we prove how training a USF via maximum-likelihood reduces to a
Deep SVDD objective with a unique regularization that inherently prevents
representational collapse. This theoretical bridge implies that USFs inherit
both the density faithfulness of flows and the distance-based reasoning of
one-class methods. We further demonstrate that USFs induce a tighter alignment
between negative log-likelihood and latent norm than either Deep SVDD or
non-USFs, and how recent hybrid approaches combining one-class objectives with
VAEs can be naturally extended to USFs. Consequently, we advocate using USFs as
a drop-in replacement for non-USFs in modern anomaly detection architectures.
Empirically, this substitution yields consistent performance gains and
substantially improved training stability across multiple benchmarks and model
backbones for both image-level and pixel-level detection. These results unify
two major anomaly detection paradigms, advancing both theoretical understanding
and practical performance.

</details>


### [83] [CRPS-LAM: Regional ensemble weather forecasting from matching marginals](https://arxiv.org/abs/2510.09484)
*Erik Larsson,Joel Oskarsson,Tomas Landelius,Fredrik Lindsten*

Main category: cs.LG

TL;DR: CRPS-LAM 通过单次前向传播和一个潜在噪声向量实现快速的集合天气预报，兼具低误差与细粒度细节保留，与扩散模型相比采样速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决局地天气预报中扩散模型采样成本高、推断时间长的问题，同时在概率预报中保持低偏差。

Method: 在基于 CRPS 训练的全球天气预测模型的启发下，提出 CRPS-LAM。通过在模型输入中注入一个单一潜在噪声向量，在一次前向传播中生成多成员集合，并以 CRPS 为训练目标进行优化。

Result: 在 MEPS 区域数据集上，CRPS-LAM 达到了与扩散模型相近的低误差，同时保持对细尺度细节的保留，且采样速度比扩散模型快最多约 39 倍。

Conclusion: CRPS-LAM 为快速、高保真且概率化的局地天气预测提供了有效替代方案，尤其适用于需要高效生成集合的不确定性预测场景。

Abstract: Machine learning for weather prediction increasingly relies on ensemble
methods to provide probabilistic forecasts. Diffusion-based models have shown
strong performance in Limited-Area Modeling (LAM) but remain computationally
expensive at sampling time. Building on the success of global weather
forecasting models trained based on Continuous Ranked Probability Score (CRPS),
we introduce CRPS-LAM, a probabilistic LAM forecasting model trained with a
CRPS-based objective. By sampling and injecting a single latent noise vector
into the model, CRPS-LAM generates ensemble members in a single forward pass,
achieving sampling speeds up to 39 times faster than a diffusion-based model.
We evaluate the model on the MEPS regional dataset, where CRPS-LAM matches the
low errors of diffusion models. By retaining also fine-scale forecast details,
the method stands out as an effective approach for probabilistic regional
weather forecasting

</details>


### [84] [Near-Optimal Second-Order Guarantees for Model-Based Adversarial Imitation Learning](https://arxiv.org/abs/2510.09487)
*Shangzhe Li,Dongruo Zhou,Weitong Zhang*

Main category: cs.LG

TL;DR: 提出基于模型的在线对手模仿学习 MB-AIL，给出无视时的二阶样本复杂度界限及其在专家演示和奖励自由交互中的最小极大值下界，并通过实验验证理论结论。


<details>
  <summary>Details</summary>
Motivation: 澄清在线交互与随机性对对手模仿学习（AIL）的影响，填补理论空缺，提升理解与应用。

Method: 提出模型基的 AIL 算法 MB-AIL，适用于通用函数近似；对专家数据与奖励自由交互的情况给出 horizon-free、二阶、实例依赖的样本复杂度界限，并给出信息理论下界，构建最小极大上下界。

Result: MB-AIL 在在线交互下达到近似最优的样本复杂度（除对数因子外），且界于返回方差等实例参数的变动而缩放；在专家演示方面的复杂度依赖于 horizon H、精度 ε 与策略方差 σ^2 的下界相匹配。实验结果验证理论，实用实现的样本效率可与或优于现有方法。

Conclusion: 模型型 MB-AIL 提供对在线交互与随机性影响的深入理论与实证分析，显示在 AIL 领域具备近最优、且对实际应用友好的性能。

Abstract: We study online adversarial imitation learning (AIL), where an agent learns
from offline expert demonstrations and interacts with the environment online
without access to rewards. Despite strong empirical results, the benefits of
online interaction and the impact of stochasticity remain poorly understood. We
address these gaps by introducing a model-based AIL algorithm (MB-AIL) and
establish its horizon-free, second-order sample-complexity guarantees under
general function approximations for both expert data and reward-free
interactions. These second-order bounds provide an instance-dependent result
that can scale with the variance of returns under the relevant policies and
therefore tighten as the system approaches determinism. Together with
second-order, information-theoretic lower bounds on a newly constructed
hard-instance family, we show that MB-AIL attains minimax-optimal sample
complexity for online interaction (up to logarithmic factors) with limited
expert demonstrations and matches the lower bound for expert demonstrations in
terms of the dependence on horizon $H$, precision $\epsilon$ and the policy
variance $\sigma^2$. Experiments further validate our theoretical findings and
demonstrate that a practical implementation of MB-AIL matches or surpasses the
sample efficiency of existing methods.

</details>


### [85] [Performance Analysis of Machine Learning Algorithms in Chronic Kidney Disease Prediction](https://arxiv.org/abs/2510.09493)
*Iftekhar Ahmed,Tanzil Ebad Chowdhury,Biggo Bushon Routh,Nafisa Tasmiya,Shadman Sakib,Adil Ahmed Chowdhury*

Main category: cs.LG

TL;DR: 在CKD预测中，八种机器学习模型比较显示随机森林与逻辑回归 ~99%准确率，但需警惕过拟合与泛化性，数据来自UCI CKD数据集，缺失值通过均值/众数与随机采样填补。


<details>
  <summary>Details</summary>
Motivation: 提升CKD风险评估与监测的准确性与效率，借助机器学习模型辅助临床决策。

Method: 使用UCI CKD数据集，先对缺失值进行填补（均值/众数、随机采样），再训练八种模型：随机森林、SVM、朴素贝叶斯、逻辑回归、KNN、XGBoost、决策树、AdaBoost，并比较其准确率。

Result: 随机森林和逻辑回归达到约99%准确率，AdaBoost、XGBoost、朴素贝叶斯、决策树、SVM次之，KNN最低为73%。

Conclusion: 在该数据集上某些模型表现突出，表明模型选择与数据处理对CKD预测有显著影响；然而需进一步的验证（如交叉验证、外部数据集、多指标评估、避免过拟合）以提升泛化能力。

Abstract: Kidneys are the filter of the human body. About 10% of the global population
is thought to be affected by Chronic Kidney Disease (CKD), which causes kidney
function to decline. To protect in danger patients from additional kidney
damage, effective risk evaluation of CKD and appropriate CKD monitoring are
crucial. Due to quick and precise detection capabilities, Machine Learning
models can help practitioners accomplish this goal efficiently; therefore, an
enormous number of diagnosis systems and processes in the healthcare sector
nowadays are relying on machine learning due to its disease prediction
capability. In this study, we designed and suggested disease predictive
computer-aided designs for the diagnosis of CKD. The dataset for CKD is
attained from the repository of machine learning of UCL, with a few missing
values; those are filled in using "mean-mode" and "Random sampling method"
strategies. After successfully achieving the missing data, eight ML techniques
(Random Forest, SVM, Naive Bayes, Logistic Regression, KNN, XGBoost, Decision
Tree, and AdaBoost) were used to establish models, and the performance
evaluation comparisons among the result accuracies are measured by the
techniques to find the machine learning models with the highest accuracy. Among
them, Random Forest as well as Logistic Regression showed an outstanding 99%
accuracy, followed by the Ada Boost, XGBoost, Naive Bayes, Decision Tree, and
SVM, whereas the KNN classifier model stands last with an accuracy of 73%.

</details>


### [86] [Automated Evolutionary Optimization for Resource-Efficient Neural Network Training](https://arxiv.org/abs/2510.09566)
*Ilia Revin,Leon Strelkov,Vadim A. Potemkin,Ivan Kireev,Andrey Savchenko*

Main category: cs.LG

TL;DR: PETRA, an AutoML framework for parameter-efficient training using evolutionary optimization, achieves substantial model compression and speedups across diverse benchmarks without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: The growing need for scalable and resource-efficient neural networks amidst challenges like distributed computing, model compression, and efficient training.

Method: PETRA adopts evolutionary optimization to search model architecture and training strategies within an automated framework and incorporates pruning, quantization, and loss regularization to enhance efficiency.

Result: Experiments on real-world financial event sequences and image/time-series benchmarks show PETRA reduces model size by up to 75%, lowers latency by up to 33%, and increases throughput by about 13%, with no noticeable degradation in the target metric.

Conclusion: PETRA demonstrates an effective automated approach for building parameter-efficient neural models, improving scalability and performance across diverse tasks.

Abstract: There are many critical challenges in optimizing neural network models,
including distributed computing, compression techniques, and efficient
training, regardless of their application to specific tasks. Solving such
problems is crucial because the need for scalable and resource-efficient models
is increasing. To address these challenges, we have developed a new automated
machine learning (AutoML) framework, Parameter Efficient Training with Robust
Automation (PETRA). It applies evolutionary optimization to model architecture
and training strategy. PETRA includes pruning, quantization, and loss
regularization. Experimental studies on real-world data with financial event
sequences, as well as image and time-series -- benchmarks, demonstrate PETRA's
ability to improve neural model performance and scalability -- namely, a
significant decrease in model size (up to 75%) and latency (up to 33%), and an
increase in throughput (by 13%) without noticeable degradation in the target
metric.

</details>


### [87] [STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging](https://arxiv.org/abs/2510.09593)
*Disharee Bhowmick,Ranjith Ramanathan,Sathyanarayanan N. Aakur*

Main category: cs.LG

TL;DR: STaTS 是一种结构感知的时间序列摘要框架，通过自适应压缩保留关键时间结构，作为模型无关的预处理器使用。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列表示学习多针对原始序列或固定窗口，忽略潜在的时间结构（如状态转变、重复模式和爆发性变化），导致效率低、鲁棒性差和缺乏可扩展性。

Method: 在多分辨率层级上使用基于BIC的统计发散准则检测分段的转折点；对每个分段用简单统计量（如均值）或生成模型（如高斯混合模型GMM）进行摘要；实现高达约30x的序列压缩；作为模型无关的预处理器，与现有无监督编码器对接，无需重新训练。

Result: 在150+数据集上验证，包括UCR-85、UCR-128、UEA-30的分类，以及ETTh1/ETTh2/ETTm1和Electricity的预测，STaTS 能达到全模型的85-90%性能，同时显著降低计算成本；对噪声更鲁棒，优于均匀和聚类基线。

Conclusion: STaTS 提供一个原则性、通用且高效的结构感知时间序列建模解决方案，可作为无监督时序编码器的强大预处理器。

Abstract: Time series data often contain latent temporal structure, transitions between
locally stationary regimes, repeated motifs, and bursts of variability, that
are rarely leveraged in standard representation learning pipelines. Existing
models typically operate on raw or fixed-window sequences, treating all time
steps as equally informative, which leads to inefficiencies, poor robustness,
and limited scalability in long or noisy sequences. We propose STaTS, a
lightweight, unsupervised framework for Structure-Aware Temporal Summarization
that adaptively compresses both univariate and multivariate time series into
compact, information-preserving token sequences. STaTS detects change points
across multiple temporal resolutions using a BIC-based statistical divergence
criterion, then summarizes each segment using simple functions like the mean or
generative models such as GMMs. This process achieves up to 30x sequence
compression while retaining core temporal dynamics. STaTS operates as a
model-agnostic preprocessor and can be integrated with existing unsupervised
time series encoders without retraining. Extensive experiments on 150+
datasets, including classification tasks on the UCR-85, UCR-128, and UEA-30
archives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,
demonstrate that STaTS enables 85-90\% of the full-model performance while
offering dramatic reductions in computational cost. Moreover, STaTS improves
robustness under noise and preserves discriminative structure, outperforming
uniform and clustering-based compression baselines. These results position
STaTS as a principled, general-purpose solution for efficient, structure-aware
time series modeling.

</details>


### [88] [MODE: Learning compositional representations of complex systems with Mixtures Of Dynamical Experts](https://arxiv.org/abs/2510.09594)
*Nathan Quiblier,Roy Friedman,Matthew Ricci*

Main category: cs.LG

TL;DR: MODE (Mixture Of Dynamical Experts) provides a neural gating-based mixture of dynamical models to decompose complex, noisy regime-switching dynamics into sparse, interpretable components. It enables unsupervised discovery of behavioral regimes and accurate long-term forecasting across regime transitions, particularly suited for transitions between cycling and equilibrium or branching developmental fates.


<details>
  <summary>Details</summary>
Motivation: Traditional flow-based dynamical models assume locally smooth dynamics, which fails when systems exhibit abrupt, noisy regime switches. Biological processes, such as cellular cycling, branching differentiation, and fate commitment, require models that can jump between governing laws and identify regime structure.

Method: MODE is a graphical model with a neural gating mechanism that partitions dynamics into sparse components (experts). It allows agents to switch governing laws, enabling unsupervised discovery of regimes and robust long-term forecasting across transitions.

Result: Empirical evaluation on synthetic and real datasets shows MODE can (1) classify regimes in noisy, few-sample data, (2) forecast challenging cycling and branching processes, and (3) distinguish proliferation from differentiation dynamics in single-cell RNA-seq data and predict when cells commit to fate.

Conclusion: MODE effectively handles regime-transition dynamics in complex biological systems, offering interpretable decomposition, robust forecasting across transitions, and applicability to computational biology tasks such as fate commitment prediction.

Abstract: Dynamical systems in the life sciences are often composed of complex mixtures
of overlapping behavioral regimes. Cellular subpopulations may shift from
cycling to equilibrium dynamics or branch towards different developmental
fates. The transitions between these regimes can appear noisy and irregular,
posing a serious challenge to traditional, flow-based modeling techniques which
assume locally smooth dynamics. To address this challenge, we propose MODE
(Mixture Of Dynamical Experts), a graphical modeling framework whose neural
gating mechanism decomposes complex dynamics into sparse, interpretable
components, enabling both the unsupervised discovery of behavioral regimes and
accurate long-term forecasting across regime transitions. Crucially, because
agents in our framework can jump to different governing laws, MODE is
especially tailored to the aforementioned noisy transitions. We evaluate our
method on a battery of synthetic and real datasets from computational biology.
First, we systematically benchmark MODE on an unsupervised classification task
using synthetic dynamical snapshot data, including in noisy, few-sample
settings. Next, we show how MODE succeeds on challenging forecasting tasks
which simulate key cycling and branching processes in cell biology. Finally, we
deploy our method on human, single-cell RNA sequencing data and show that it
can not only distinguish proliferation from differentiation dynamics but also
predict when cells will commit to their ultimate fate, a key outstanding
challenge in computational biology.

</details>


### [89] [BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards](https://arxiv.org/abs/2510.09596)
*Sangyun Lee,Brandon Amos,Giulia Fanti*

Main category: cs.LG

TL;DR: BaNEL在极稀疏奖励场景中通过仅对失败样本进行后训练并最小化奖励评估次数来提升模型性能，核心在于将失败学习转化为内循环生成建模并用来避免重复失败。


<details>
  <summary>Details</summary>
Motivation: 在高价值问题中，基线生成模型可能获得近零奖励且奖励评估昂贵，传统的监督+奖励后训练效率低下，需要从失败中学习。

Method: 将后训练限定于失败尝试，建立一个描述失败规律的生成模型，将新数据与历史失败样本对比，若相似则引导模型远离失败模式；通过贝叶斯的负证据学习框架实现。

Result: 在若干稀疏奖励任务中，BaNEL在未观察到任何成功样本的情况下提升表现，并在奖励评估次数上优于现有的新颖性奖金方法，成功率提升可达数个量级。

Conclusion: BaNEL提供了一种从失败中学习的有效框架，能够在稀疏奖励设定下显著降低奖励评估并提升成功率。

Abstract: Today's generative models thrive with large amounts of supervised data and
informative reward functions characterizing the quality of the generation. They
work under the assumptions that the supervised data provides knowledge to
pre-train the model, and the reward function provides dense information about
how to further improve the generation quality and correctness. However, in the
hardest instances of important problems, two problems arise: (1) the base
generative model attains a near-zero reward signal, and (2) calls to the reward
oracle are expensive. This setting poses a fundamentally different learning
challenge than standard reward-based post-training. To address this, we propose
BaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the
model using failed attempts only, while minimizing the number of reward
evaluations (NREs). Our method is based on the idea that the problem of
learning regularities underlying failures can be cast as another, in-loop
generative modeling problem. We then leverage this model to assess whether new
data resembles previously seen failures and steer the generation away from
them. We show that BaNEL can improve model performance without observing a
single successful sample on several sparse-reward tasks, outperforming existing
novelty-bonus approaches by up to several orders of magnitude in success rate,
while using fewer reward evaluations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [90] [Are Voters Willing to Collectively Secure Elections? Unraveling a Practical Blockchain Voting System](https://arxiv.org/abs/2510.08700)
*Zhuolun Li,Haluk Sonmezler,Faiza Shirazi,Febin Shaji,Tymoteusz Mroczkowski,Dexter Lardner,Matthew Alain Camus,Evangelos Pournaras*

Main category: cs.CR

TL;DR: 提出集体安全投票的概念，并通过区块链与门限密码学实现可操作原型，强调隐私保护与可验证性。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化、大规模选举中的 ballot secrecy 难题，提升电子投票的可信性与隐私保护，同时实现实际可用性。

Method: 将门限密码学与智能合约结合，设计让选民自愿成为秘密持有者以保护 ballot secrets 的系统；提供友好界面隐藏底层区块链复杂性；通过用户测试评估可用性与安全感。

Result: 实验/用户测试显示高意愿成为秘密持有者、可靠的份额释放参与、以及对系统的高安全信心，证明集体维护隐私的可行性。

Conclusion: 基于区块链的集体安全投票在现实部署中具有可行性，能够在强隐私与透明性之间取得平衡。

Abstract: Ensuring ballot secrecy is critical for fair and trustworthy electronic
voting systems, yet achieving strong secrecy guarantees in decentralized,
large-scale elections remains challenging. This paper proposes the concept of
collectively secure voting, in which voters themselves can opt in as secret
holders to protect ballot secrecy. A practical blockchain-based collectively
secure voting system is designed and implemented. Our design strikes a balance
between strong confidentiality guarantees and real-world applicability. The
proposed system combines threshold cryptography and smart contracts to ensure
ballots remain confidential during voting, while all protocol steps remain
transparent and verifiable. Voters can use the system without prior blockchain
knowledge through an intuitive user interface that hides underlying complexity.
To evaluate this approach, a user testing is conducted. Results show a high
willingness to act as secret holders, reliable participation in share release,
and high security confidence in the proposed system. The findings demonstrate
that voters can collectively maintain secrecy and that such a practical
deployment is feasible.

</details>


### [91] [Post-Quantum Security of Block Cipher Constructions](https://arxiv.org/abs/2510.08725)
*Gorjan Alagic,Chen Bai,Christian Majenz,Kaiyan Shi*

Main category: cs.CR

TL;DR: 本文为对称密钥分组密码的后量子安全性研究奠定基础，给出针对若干实践构造的首次后量子安全性证明，包括 FX 的密钥长度扩展、LRW 与 XEX 的扭曲分组密码，以及大多数分组密码的加密和认证模式的安全性证明。方法论可在普通模型与量子理想密钥模型中使用，标志着对实用对称密钥密码的后量子安全性有了初步、严格的理解。


<details>
  <summary>Details</summary>
Motivation: 与公钥密码的后量子安全性研究相比，对称密钥分组密码的后量子分析长期缺乏系统理论。本工作旨在建立一套后量子安全性的理论框架，并为常用分组密码及其模式提供严谨的安全证明。

Method: 提出新的量子安全证明技巧，将其应用于 FX（密钥长度扩展）以及扭曲分组密码 LRW、XEX，以及多种分组密码的加密和认证模式；证明可在普通模型和量子理想密钥模型中成立。

Result: 给出针对 FX、LRW、XEX 以及多数分组密码模式的首个后量子安全性证明，方法论具有普适性，可用于在普通模型和量子理想密钥模型中的安全分析。

Conclusion: 工作为实用对称密钥密码的后量子安全性建立了初步、严格的理解，迈出重要步骤，未来可扩展到更多构造和模式的安全性分析。

Abstract: Block ciphers are versatile cryptographic ingredients that are used in a wide
range of applications ranging from secure Internet communications to disk
encryption. While post-quantum security of public-key cryptography has received
significant attention, the case of symmetric-key cryptography (and block
ciphers in particular) remains a largely unexplored topic. In this work, we set
the foundations for a theory of post-quantum security for block ciphers and
associated constructions. Leveraging our new techniques, we provide the first
post-quantum security proofs for the key-length extension scheme FX, the
tweakable block ciphers LRW and XEX, and most block cipher encryption and
authentication modes. Our techniques can be used for security proofs in both
the plain model and the quantum ideal cipher model. Our work takes significant
initial steps in establishing a rigorous understanding of the post-quantum
security of practical symmetric-key cryptography.

</details>


### [92] [Psyzkaller: Learning from Historical and On-the-Fly Execution Data for Smarter Seed Generation in OS kernel Fuzzing](https://arxiv.org/abs/2510.08918)
*Boyu Liu,Yang Zhang,Liang Cheng,Yi Zhang,Junjie Fan,Yu Fu*

Main category: cs.CR

TL;DR: 通过N-gram学习系统调用依赖关系（SDRs），并将其融入Syzkaller以提升种子有效性与多样性，从而提高内核模糊测试效果（Psyzkaller）


<details>
  <summary>Details</summary>
Motivation: 现有的内核模糊测试器难以生成符合隐式系统调用依赖的有效序列，导致种子要么被内核拒绝要么无法深入执行路径，降低效率。利用历史和实时执行数据学习 SDRs，可以提升种子质量和探索能力。

Method: 使用N-gram模型从Linyu Dongting数据集与 fuzzing 过程中的执行轨迹中挖掘 SDRs；将学习得到的关系持续加入Syzkaller的选择表；引入双向随机游走构建种子。基于Syzkaller实现原型Psyzkaller，并在三种Linux内核版本上进行对比实验。

Result: 相比原始Syzkaller，Psyzkaller在48小时内的代码覆盖提升约4.6%~7.0%，触发的崩溃数量提升约110.4%~187.2%；发现8处此前未知的内核漏洞，而Syzkaller仅发现1处。

Conclusion: 基于N-gram的SDR学习并结合双向随机游走的策略能够显著提升内核模糊测试的有效性与覆盖率，同时提升种子多样性与漏洞发现能力，验证了将统计学习引入模糊测试的可行性；未来可扩展数据源和优化计算成本。

Abstract: Fuzzing has become a cornerstone technique for uncovering vulnerabilities and
enhancing the security of OS kernels. However, state-of-the-art kernel fuzzers,
including the de facto standard Syzkaller, struggle to generate valid syscall
sequences that respect implicit Syscall Dependency Relations (SDRs).
Consequently, many generated seeds either fail kernel validation or cannot
penetrate deep execution paths, resulting in significant inefficiency.
  We hypothesize that SDRs can be effectively learned from both historic and
present kernel execution data, and that incorporating these learned relations
into fuzzing can substantially improve seed validity and diversity. To validate
this, we propose an approach that utilizes the N-gram model to mine SDRs from
the Dongting dataset-one of the largest Linux kernel execution datasets
available-as well as from execution traces collected on the fly during fuzzing.
The resulting model is used to continuously augment the Choice Table of
Syzkaller to improve its seed generation and demonstrably increases the Shannon
Entropy of the Choice Table throughout fuzzing, reflecting more
empirically-grounded choices in expanding syscall sequences into valid and
diverse seeds. In addition, we introduce a Random Walk strategy that instructs
Syzkaller to construct seeds in a bidirectional manner to further diversify the
generated seeds.
  We implement our approach in a prototype, Psyzkaller, built on top of
Syzkaller. Experiments on three representative Linux kernel versions show that
Psyzkaller improves Syzkaller's code coverage by 4.6%-7.0% in 48-hour fuzzing,
while triggering 110.4%-187.2% more crashes. Moreover, our investigation shows
that Psyzkaller discovered eight previously unknown kernel vulnerabilities,
compared to only one found by Syzkaller.

</details>


### [93] [Exploiting Web Search Tools of AI Agents for Data Exfiltration](https://arxiv.org/abs/2510.09093)
*Dennis Rall,Bernhard Bauer,Mohit Mittal,Thomas Fraunholz*

Main category: cs.CR

TL;DR: 本研究系统评估大型语言模型在间接提示注入方面的脆弱性，覆盖多种模型与攻击方法，发现即便是常见攻击模式仍然有效，暴露防御的长期薄弱点；提出以强化训练、建立已知攻击向量数据库、以及统一测试框架等为核心的安全改进路径。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地接入外部数据源与工具调用，触发隐私与安全风险的攻击面扩大；间接提示注入作为一种新兴且持续演进的攻击向量亟需系统性评估与防护策略。

Method: 对多样化模型进行系统性评估，分析影响脆弱性的关键参数（如模型尺寸、厂商、实现细节等），并比较不同攻击方法的效果。

Result: 实验表明：即使是广为人知的攻击模式也能在当前模型上成功，存在持续的防御薄弱点；模型大小、厂商与实现差异对脆弱性有影响，某些攻击方法在多模型下仍具高效性。

Conclusion: 需要改进训练程序以提升固有鲁棒性，建立集中化的已知攻击向量数据库，并开发统一的安全性测试框架，以将安全性纳入LLM设计的核心。

Abstract: Large language models (LLMs) are now routinely used to autonomously execute
complex tasks, from natural language processing to dynamic workflows like web
searches. The usage of tool-calling and Retrieval Augmented Generation (RAG)
allows LLMs to process and retrieve sensitive corporate data, amplifying both
their functionality and vulnerability to abuse. As LLMs increasingly interact
with external data sources, indirect prompt injection emerges as a critical and
evolving attack vector, enabling adversaries to exploit models through
manipulated inputs. Through a systematic evaluation of indirect prompt
injection attacks across diverse models, we analyze how susceptible current
LLMs are to such attacks, which parameters, including model size and
manufacturer, specific implementations, shape their vulnerability, and which
attack methods remain most effective. Our results reveal that even well-known
attack patterns continue to succeed, exposing persistent weaknesses in model
defenses. To address these vulnerabilities, we emphasize the need for
strengthened training procedures to enhance inherent resilience, a centralized
database of known attack vectors to enable proactive defense, and a unified
testing framework to ensure continuous security validation. These steps are
essential to push developers toward integrating security into the core design
of LLMs, as our findings show that current models still fail to mitigate
long-standing threats.

</details>


### [94] [Provable Watermarking for Data Poisoning Attacks](https://arxiv.org/abs/2510.09210)
*Yifan Zhu,Lijia Yu,Xiao-Shan Gao*

Main category: cs.CR

TL;DR: 提出两种可证明且实用的数据中毒水印方法，旨在为恶意或无害的数据中毒数据标注所有权并在不损害中毒效用的前提下实现可检测性；给出水印长度的理论界限并在多种攻击、模型和数据集上进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着数据中毒攻击被设计得看起来无害甚至有益，传统将中毒视为安全威胁的认知可能引发误解与冲突；因此需要一种可证明且可实践的水印机制，使水印所有者能够标识潜在的中毒数据，阻止滥用并保障数据所有权。

Method: 提出两种水印方案：后中毒水印(post-poisoning watermarking)与中毒并发水印(poisoning-concurrent watermarking)。对水印长度给出理论界限：后水印长度为 Θ(√d/ε_w)，中毒并发水印长度为 Θ(1/ε_w^2) 到 O(√d/ε_p)，并证明水印可检测且不显著降低中毒效用。通过在多种攻击、模型和数据集上进行实验来验证理论结论。

Result: 在给定的长度条件下，水印能够被检测到且保留中毒的有用性；实验结果与理论分析一致，支持水印方案在数据中毒攻击背景中的可行性。

Conclusion: 水印化为数据中毒攻击提供了一个可实现且有理论保证的所有权认证路径，能够在不显著损害中毒效用的前提下实现水印的可检测性，从而提升对无害或有害中毒数据的鲁棒管理。

Abstract: In recent years, data poisoning attacks have been increasingly designed to
appear harmless and even beneficial, often with the intention of verifying
dataset ownership or safeguarding private data from unauthorized use. However,
these developments have the potential to cause misunderstandings and conflicts,
as data poisoning has traditionally been regarded as a security threat to
machine learning systems. To address this issue, it is imperative for harmless
poisoning generators to claim ownership of their generated datasets, enabling
users to identify potential poisoning to prevent misuse. In this paper, we
propose the deployment of watermarking schemes as a solution to this challenge.
We introduce two provable and practical watermarking approaches for data
poisoning: {\em post-poisoning watermarking} and {\em poisoning-concurrent
watermarking}. Our analyses demonstrate that when the watermarking length is
$\Theta(\sqrt{d}/\epsilon_w)$ for post-poisoning watermarking, and falls within
the range of $\Theta(1/\epsilon_w^2)$ to $O(\sqrt{d}/\epsilon_p)$ for
poisoning-concurrent watermarking, the watermarked poisoning dataset provably
ensures both watermarking detectability and poisoning utility, certifying the
practicality of watermarking under data poisoning attacks. We validate our
theoretical findings through experiments on several attacks, models, and
datasets.

</details>


### [95] [GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis](https://arxiv.org/abs/2510.09260)
*Subrat Kishore Dutta,Yuelin Xu,Piyush Pant,Xiao Zhang*

Main category: cs.CR

TL;DR: GREAT是一种针对RLHF后门攻击的通用触发综合框架，通过情绪敏感的触发合成，利用潜在嵌入空间的PCA与聚类识别触发，并引入 Erinyes 数据集来覆盖多样触发场景，在未见触发情形下提升攻击成功率，同时保持良性输入的输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态、罕见令牌的触发在现实场景中往往效果受限，需探索更具泛化性的后门触发以攻击RLHF系统，且目标用户子群具备暴力语义请求和愤怒情绪。

Method: 构建GREAT框架：1) 在潜在嵌入空间中通过PCA与聚类识别代表性触发；2) Erinyes数据集，用层次结构与多样性促进策略，从GPT-4.1提取5000+愤怒触发；3) 在RLHF数据集上评估，比较对比基线方法的攻击成功率，关注未见触发场景与输出质量。

Result: 在实验中，GREAT显著提高了攻击成功率，尤其在未见触发场景下，且对良性输入下的输出质量影响有限。

Conclusion: 提出了一种面向人群情绪维度和潜在空间分布的可泛化后门触发生成框架，强调需要更强的对抗与防御策略来应对情绪驱动的RLHF后门。

Abstract: Recent work has shown that RLHF is highly susceptible to backdoor attacks,
poisoning schemes that inject malicious triggers in preference data. However,
existing methods often rely on static, rare-token-based triggers, limiting
their effectiveness in realistic scenarios. In this paper, we develop GREAT, a
novel framework for crafting generalizable backdoors in RLHF through
emotion-aware trigger synthesis. Specifically, GREAT targets harmful response
generation for a vulnerable user subgroup characterized by both semantically
violent requests and emotionally angry triggers. At the core of GREAT is a
trigger identification pipeline that operates in the latent embedding space,
leveraging principal component analysis and clustering techniques to identify
the most representative triggers. To enable this, we present Erinyes, a
high-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a
principled, hierarchical, and diversity-promoting approach. Experiments on
benchmark RLHF datasets demonstrate that GREAT significantly outperforms
baseline methods in attack success rates, especially for unseen trigger
scenarios, while largely preserving the response quality on benign inputs.

</details>


### [96] [Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects](https://arxiv.org/abs/2510.09269)
*Zirun Zhou,Zhengyang Xiao,Haochuan Xu,Jing Sun,Di Wang,Jingfeng Zhang*

Main category: cs.CR

TL;DR: 提出 GoBA，通过物理触发的目标导向后门攻击使 VLA 模型在触发时执行预定义动作，且对干净输入无性能损失；在 LIBERO 基准上用 BadLIBERO 实现，实验显示 97% 的触发输入成功率，颜色与轨迹影响显著，大小影响小；提供代码和数据集。


<details>
  <summary>Details</summary>
Motivation: VLA 模型在指令执行方面取得进展，但依赖未经筛选的训练数据带来安全风险。现有后门多依赖白盒访问且往往导致任务失败，本文提出更现实的威胁：通过在训练数据中注入物理对象作为触发器，实现在特定触发下的目标化行为。

Method: 以 LIBERO 为基准构建 BadLIBERO，注入多种物理触发器并设定目标行动；提出三层评估，将受害 VLA 的行为分为三种状态：nothing to do、try to do、success to do；实验结果显示触发存在时可高概率实现后门目标且干净输入无性能下降；分析表明动作轨迹与触发颜色显著影响攻击效果，触发大小影响较小。

Result: GoBA 能在触发存在时实现高效的目标导向后门，攻击对干净输入无显著影响；提供 BadLIBERO 数据集及代码以供复现。

Conclusion: 该工作揭示物理触发的后门在现实场景中的威胁性，提示需要加强对 VLA 模型的安全评估、数据清洗与后门检测，并未来拓展触发器类型、提出对抗防御与理论分析。

Abstract: Recent advances in vision-language-action (VLA) models have greatly improved
embodied AI, enabling robots to follow natural language instructions and
perform diverse tasks. However, their reliance on uncurated training datasets
raises serious security concerns. Existing backdoor attacks on VLAs mostly
assume white-box access and result in task failures instead of enforcing
specific actions. In this work, we reveal a more practical threat: attackers
can manipulate VLAs by simply injecting physical objects as triggers into the
training dataset. We propose goal-oriented backdoor attacks (GoBA), where the
VLA behaves normally in the absence of physical triggers but executes
predefined and goal-oriented actions in the presence of physical triggers.
Specifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO
that incorporates diverse physical triggers and goal-oriented backdoor actions.
In addition, we propose a three-level evaluation that categorizes the victim
VLA's actions under GoBA into three states: nothing to do, try to do, and
success to do. Experiments show that GoBA enables the victim VLA to
successfully achieve the backdoor goal in 97 percentage of inputs when the
physical trigger is present, while causing zero performance degradation on
clean inputs. Finally, by investigating factors related to GoBA, we find that
the action trajectory and trigger color significantly influence attack
performance, while trigger size has surprisingly little effect. The code and
BadLIBERO dataset are accessible via the project page at
https://goba-attack.github.io/.

</details>


### [97] [Assessing the Impact of Post-Quantum Digital Signature Algorithms on Blockchains](https://arxiv.org/abs/2510.09271)
*Alison Gonçalves Schemitt,Henrique Fan da Silva,Roben Castagna Lunardi,Diego Kreutz,Rodrigo Brandão Mansilha,Avelino Francisco Zorzo*

Main category: cs.CR

TL;DR: 本研究在区块链场景下对后量子密码学（PQC）与传统签名算法进行基准测试，发现PQC在较低安全等级上的开销较小，且在某些场景下在较高安全等级甚至优于ECDSA；并提供了开源实现以支持复现。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁现有加密体系，推动后量子密码学标准化。NIST在2024年完成PQC算法标准化，需评估在区块链等高性能场景中的现实可行性与性能影响。

Method: 提出在区块链环境中对PQC及传统签名进行基准的体系，测量多种计算环境下的签名产生与验证时间，并对规模化影响进行仿真；比较ML-DSA、Dilithium、Falcon、Mayo、SLH-DSA、SPHINCS+、Cross等PQC签名算法在安全等级1–5与ECDSA的表现。

Result: PQC在安全等级1时的性能增量很小；在较高等级下部分场景甚至优于ECDSA。例如，ML-DSA在等级5的ARM笔记本上验证时间为0.14 ms，而ECDSA为0.88 ms。研究还提供了一个开放源代码实现以确保可复现性。

Conclusion: PQC在区块链场景具有可行性，尤其在高安全等级下有潜在的性能优势；研究为区块链迁移至量子抗性方案提供了可重复的基准框架和实证证据，并鼓励后续研究和社区参与。

Abstract: The advent of quantum computing threatens the security of traditional
encryption algorithms, motivating the development of post-quantum cryptography
(PQC). In 2024, the National Institute of Standards and Technology (NIST)
standardized several PQC algorithms, marking an important milestone in the
transition toward quantum-resistant security. Blockchain systems fundamentally
rely on cryptographic primitives to guarantee data integrity and transaction
authenticity. However, widely used algorithms such as ECDSA, employed in
Bitcoin, Ethereum, and other networks, are vulnerable to quantum attacks.
Although adopting PQC is essential for long-term security, its computational
overhead in blockchain environments remains largely unexplored. In this work,
we propose a methodology for benchmarking both PQC and traditional
cryptographic algorithms in blockchain contexts. We measure signature
generation and verification times across diverse computational environments and
simulate their impact at scale. Our evaluation focuses on PQC digital signature
schemes (ML-DSA, Dilithium, Falcon, Mayo, SLH-DSA, SPHINCS+, and Cross) across
security levels 1 to 5, comparing them to ECDSA, the current standard in
Bitcoin and Ethereum. Our results indicate that PQC algorithms introduce only
minor performance overhead at security level 1, while in some scenarios they
significantly outperform ECDSA at higher security levels. For instance, ML-DSA
achieves a verification time of 0.14 ms on an ARM-based laptop at level 5,
compared to 0.88 ms for ECDSA. We also provide an open-source implementation to
ensure reproducibility and encourage further research.

</details>


### [98] [Modern iOS Security Features -- A Deep Dive into SPTM, TXM, and Exclaves](https://arxiv.org/abs/2510.09272)
*Moritz Steffin,Jiska Classen*

Main category: cs.CR

TL;DR: 分析苹果 XNU 内核的分区化演进：通过 SPTM 引入内存重类型域和信任分区，以及 Exclaves、xnuproxy、Tightbeam IPC 的通信机制，提升对内核组件的隔离与抗妥协能力。


<details>
  <summary>Details</summary>
Motivation: 此前对 SPTM 及相关安全机制缺乏系统性研究，需要全面理解新架构及各项缓解措施的协同效应。

Method: 对新安全机制及其相互作用进行综合分析，梳理现有缓解措施，重点考察 SPTM 的域构建、基于帧重类型的信任域、内存映射规则集，以及 TXM、代码签名、权限验证等功能的解耦与隔离；深入分析 Exclaves 的通信机制、xnuproxy 与 Tightbeam IPC 框架及其安全性影响。

Result: 揭示 SPTM 为内存重类型的唯一权威，基于帧重类型和内存映射规则集建立系统内的信任域，将不同功能分离（如 TXM 的代码签名与权限验证）并“打孔式”隔离。提出 Exclaves 为近期的安全特性，分析其通信机制；多种通信途径（ notably xnuproxy 作为安全世界请求处理器、Tightbeam IPC 框架）被发现。架构变更提升系统安全性，将关键与敏感组件从 XNU 的直接控制范围移出，提升在内核妥协时的安全保障。

Conclusion: 该架构更偏向以分区与最小信任域来提高安全性，提供对内核妥协的较强防护与缓解路径；然而仍需评估新机制的实现成本、兼容性、潜在的新攻击面与对现有生态的影响，以及如何在不同 Apple 设备和软件版本中保持一致的安全性。

Abstract: The XNU kernel is the basis of Apple's operating systems. Although labeled as
a hybrid kernel, it is found to generally operate in a monolithic manner by
defining a single privileged trust zone in which all system functionality
resides. This has security implications, as a kernel compromise has immediate
and significant effects on the entire system. Over the past few years, Apple
has taken steps towards a more compartmentalized kernel architecture and a more
microkernel-like design. To date, there has been no scientific discussion of
SPTM and related security mechanisms. Therefore, the understanding of the
system and the underlying security mechanisms is minimal. In this paper, we
provide a comprehensive analysis of new security mechanisms and their
interplay, and create the first conclusive writeup considering all current
mitigations. SPTM acts as the sole authority regarding memory retyping. Our
analysis reveals that, through SPTM domains based on frame retyping and memory
mapping rule sets, SPTM introduces domains of trust into the system,
effectively gapping different functionalities from one another. Gapped
functionality includes the TXM, responsible for code signing and entitlement
verification. We further demonstrate how this introduction lays the groundwork
for the most recent security feature of Exclaves, and conduct an in-depth
analysis of its communication mechanisms. We discover multifold ways of
communication, most notably xnuproxy as a secure world request handler, and the
Tightbeam IPC framework. The architecture changes are found to increase system
security, with key and sensitive components being moved out of XNU's direct
reach. This also provides additional security guarantees in the event of a
kernel compromise, which is no longer an immediate threat at the highest trust
level.

</details>


### [99] [Clustering Deposit and Withdrawal Activity in Tornado Cash: A Cross-Chain Analysis](https://arxiv.org/abs/2510.09433)
*Raffaele Cristodaro,Benjamin Kramer,Claudio J. Tessone*

Main category: cs.CR

TL;DR: 本论文提出三种聚类启发式对 Tornado Cash 进行跨链去匿名化分析，揭示实际隐私脆弱性：在以太坊、BNB Smart Chain 和 Polygon 上，5.1-12.6% 的提现可追溯到原始存款；引入新颖的 FIFO 时序匹配后，链接率再提升 15-22 个百分点，累计链接金额超过 23 亿美元，跨链结果表明问题源于用户行为而非单链漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究在实际使用中，加密隐私工具的匿名性是否确实成立，以及跨链环境下隐私是否更易被破坏，旨在为隐私设计提供证据与方向。

Method: 提出并应用三种聚类启发式：地址复用、交易链接以及新颖的先入先出（FIFO）时序匹配规则，在以太坊、BNB Smart Chain 和 Polygon 上对 Tornado Cash 的活动进行跨链实证分析，评估汇聚的可追溯性和统计显著性。

Result: 结果显示：5.1-12.6% 的提现可以通过地址复用与交易链接被追溯到原始存款；引入 FIFO 时序匹配后，链接率再提升 15-22 个百分点；统计检验表明这些 FIFO 匹配极不可能仅由随机因素产生；跨链结果呈现链无关性问题，合计约有超过 23 亿美元的 Tornado Cash 提现被链接到 identifiable 存款。

Conclusion: 隐私保护在实际使用中容易被瓦解，需加强用户行为规范与设计更具隐私意识的协议，以减缓跨链环境中的去匿名化风险。

Abstract: Tornado Cash is a decentralised mixer that uses cryptographic techniques to
sever the on-chain trail between depositors and withdrawers. In practice,
however, its anonymity can be undermined by user behaviour and operational
quirks. We conduct the first cross-chain empirical study of Tornado Cash
activity on Ethereum, BNB Smart Chain, and Polygon, introducing three
clustering heuristics-(i) address-reuse, (ii) transactional-linkage, and (iii)
a novel first-in-first-out (FIFO) temporal-matching rule. Together, these
heuristics reconnect deposits to withdrawals and deanonymise a substantial
share of recipients. Our analysis shows that 5.1 - 12.6% of withdrawals can
already be traced to their originating deposits through address reuse and
transactional linkage heuristics. Adding our novel First-In-First-Out (FIFO)
temporal-matching heuristic lifts the linkage rate by a further 15 - 22
percentage points. Statistical tests confirm that these FIFO matches are highly
unlikely to occur by chance. Comparable leakage across Ethereum, BNB Smart
Chain, and Polygon indicates chain-agnostic user misbehaviour, rather than
chain-specific protocol flaws. These results expose how quickly cryptographic
guarantees can unravel in everyday use, underscoring the need for both
disciplined user behaviour and privacy-aware protocol design. In total, our
heuristics link over $2.3 billion in Tornado Cash withdrawals to identifiable
deposits, exposing significant cracks in practical anonymity.

</details>


### [100] [The Impact of Sanctions on decentralised Privacy Tools: A Case Study of Tornado Cash](https://arxiv.org/abs/2510.09443)
*Raffaele Cristodaro,Benjamin Kramer,Claudio J. Tessone*

Main category: cs.CR

TL;DR: This content has not passed the compliance test and has been hidden.


<details>
  <summary>Details</summary>
Motivation: This content has not passed the compliance test and has been hidden.

Method: This content has not passed the compliance test and has been hidden.

Result: This content has not passed the compliance test and has been hidden.

Conclusion: This content has not passed the compliance test and has been hidden.

Abstract: This paper investigates the impact of sanctions on Tornado Cash, a smart
contract protocol designed to enhance transaction privacy. Following the U.S.
Department of the Treasury's sanctions against Tornado Cash in August 2022,
platform activity declined sharply. We document a significant and sustained
reduction in transaction volume, user diversity, and overall protocol
utilization after the sanctions were imposed. Our analysis draws on transaction
data from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We
further examine developments following the partial lifting and eventual removal
of sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025.
Although activity partially recovered, the rebound remained limited. The
Tornado Cash case illustrates how regulatory interventions can affect
decentralized protocols, while also highlighting the challenges of fully
enforcing such measures in decentralized environments.

</details>


### [101] [The Data Enclave Advantage: A New Paradigm for Least-Privileged Data Access in a Zero-Trust World](https://arxiv.org/abs/2510.09494)
*Nico Bistolfi,Andreea Georgescu,Dave Hodson*

Main category: cs.CR

TL;DR: 提出基于按需数据 enclaves 的零 standing privilege (ZSP) 与即时数据访问 (JIT) 框架，通过临时数据契约实现对单条记录的动态、最小权限访问。


<details>
  <summary>Details</summary>
Motivation: 云基础设施向动态分布式工作流演进，静态权限成为关键漏洞；基于 CSA Top Threats 2025 Deep Dive 的分析强调需要在数据层实现粒度访问控制以防止 privilege creep。

Method: 提出以数据为中心的架构，使用按需数据契约和数据 enclaves 取代静态权限，支持 ZSP/JIT，对数据请求进行实时授权、分离与监控。

Result: 提出的架构有望降低攻击面、防止权限漂移、简化审计，并为企业向更安全的数据环境迁移提供路径。

Conclusion: 通过数据级别的 ZSP/JIT 与按需契约，实现对单条记录的精确访问控制，提升数据安全与合规性，具有实际落地潜力。

Abstract: As cloud infrastructure evolves to support dynamic and distributed workflows,
accelerated now by AI-driven processes, the outdated model of standing
permissions has become a critical vulnerability. Based on the Cloud Security
Alliance (CSA) Top Threats to Cloud Computing Deep Dive 2025 Report, our
analysis details how standing permissions cause catastrophic cloud breaches.
While current security tools are addressing network and API security, the
challenge of securing granular data access remains. Removing standing
permissions at the data level is as critical as it is at the network level,
especially for companies handling valuable data at scale.
  In this white paper, we introduce an innovative architecture based on
on-demand data enclaves to address this gap directly. Our approach enables Zero
Standing Privilege (ZSP) and Just-in-Time (JIT) principles at the data level.
We replace static permissions with temporary data contracts that enforce
proactive protection. This means separation is built around the data requested
on-demand, providing precise access and real time monitoring for individual
records instead of datasets. This solution drastically reduces the attack
surface, prevents privilege creep, and simplifies auditing, offering a vital
path for enterprises to transition to a more secure and resilient data
environment.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [102] [On Estimation of Angles of Arrival in Monostatic ISAC Without Instantaneous Transmit CSI](https://arxiv.org/abs/2510.08793)
*Ataher Sams,Simone Di Bari,Besma Smida,Natasha Devroye,Daniela Tuninetti,Giorgio Taricco*

Main category: cs.IT

TL;DR: 在基站仅具统计 CSI 的情形下，分析 ISAC 的感知与通信极限，基于 BCRB-MMSE 框架给出可实现的权衡区域，并提出跨时间的功率分配与波束设计策略以提升 AoA 估计与通信速率。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，基站对通信用户往往只有统计 CSI；需同时完成多目标 AoA 估计与与其中一个目标的通信，因此需要界定感知与通信的基本极限并探讨资源分配策略。

Method: 采用贝叶斯 Cramér-Rao Bound (BCRB) 框架，在最小均方误差(MMSE)准则下推导感知-速率的可实现区域；提出在一个对比 coherence time 内将功率在感知与通信波束之间共享的传输策略；利用目标特定感知矩阵的主特征向量实现波束分配以最小化 AoA 估计误差，并分析在感知与通信方向之间的权衡。

Result: 给出可实现的 BCRB-Rate 区域；证明利用目标感知矩阵的主特征向量进行波束分配可降低单独 AoA 的估计误差；平衡感知与通信的策略在联合估计性能上具备优势，同时在单独任务上可能有代价；并且由于通信接收端的通道不确定性较低，更新的感知信息可显著提升通信速率。

Conclusion: 在统计 CSI 条件下，通过将感知信息反馈并结合 BCRB 指导的波束与功率分配，ISAC 的整体性能可显著提升，尤其是在多目标 AoA 估计与单目标通信共存的场景。

Abstract: This paper explores the fundamental limits of Integrated Sensing and
Communication (ISAC) in a more realistic setting compared to previous
literature when the Base Staion (BS) has only statistical CSI of the
communication user rather than full CSI. We analyze a monostatic setting where
the BS performs multi-target Angle of Arrival (AoA) estimation while
simultaneously communicating with one of the targets. We assume that the BS has
statistical CSI about all AoAs, with less uncertainty in the AoA of the
communication receiver. The communication receiver is assumed to have perfect
CSI. Utilizing a Bayesian Cram\'er-Rao Bound (BCRB) framework to characterize
the fundamental limits of sensing under minimum mean square error (MMSE)
criteria, we derive achievable BCRB-rate trade-off regions. Our approach
introduces a number of transmission strategies that share power across sensing
and communication beams over a coherence time. Our analysis reveals that beam
allocation strategies leveraging the principal eigenvectors of the
target-specific sensing matrices minimize individual AoA estimation errors,
while strategies balancing sensing and communication directions optimize joint
estimation performance at the cost of individual accuracy. We demonstrate that
leveraging updated BCRB-based sensing information for the communication
receiver, due to its lower channel uncertainty, enables significantly improved
communication rates.

</details>


### [103] [Observation Matrix Design for Densifying MIMO Channel Estimation via 2D Ice Filling](https://arxiv.org/abs/2510.08887)
*Zijian Zhang,Mingyao Cui*

Main category: cs.IT

TL;DR: 通过2DIF/TS-2DIF在密集MIMO中以观测矩阵设计（发射前导与接收组合器）来最大化观测信道互信息，从而提升信道估计性能。


<details>
  <summary>Details</summary>
Motivation: 密集MIMO下子波长级间距引入强相关性，提供对信道状态信息（CSI）的先验知识，若能充分利用该先验设计观测矩阵，将显著提升信道估计精度。

Method: 提出二维冰填充算法（2DIF），将信道协方差的特征子空间分解为与发送端和接收端相关的两个子特征子空间，通过将前导和组合器设为这两个子特征子空间的特征向量实现观测矩阵优化；扩展至混合有限射频通道的TS-2DIF算法以适配模拟相位光器件的实现。

Result: 仿真结果表明，与现有方法相比，2DIF/TS-2DIF在信道估计精度方面具有明显提升。

Conclusion: 所提出的2DIF及TS-2DIF能够在密集MIMO场景中生成近似最优的观测矩阵，且可适用于混合数字/模拟天线系统，提升信道估计性能。

Abstract: In recent years, densifying multiple-input multiple-output (MIMO) has
attracted much attention from the communication community. Thanks to the
subwavelength antenna spacing, the strong correlations among densifying
antennas provide sufficient prior knowledge about channel state information
(CSI). This inspires the careful design of observation matrices (e.g., transmit
precoders and receive combiners), that exploits the CSI prior knowledge, to
boost channel estimation performance. Aligned with this vision, this work
proposes to jointly design the combiners and precoders by maximizing the mutual
information between the received pilots and densifying MIMO channels. A
two-dimensional ice-filling (2DIF) algorithm is proposed to efficiently
accomplish this objective. The algorithm is motivated by the fact that the
eigenspace of MIMO channel covariance can be decoupled into two
sub-eigenspaces, which are associated with the correlations of transmitter
antennas and receiver antennas, respectively. By properly setting the precoder
and the combiner as the eigenvectors from these two sub-eigenspaces, the 2DIF
promises to generate near-optimal observation matrices. Moreover, we further
extend the 2DIF method to the popular hybrid combining systems, where a
two-stage 2DIF (TS-2DIF) algorithm is developed to handle the analog combining
circuits realized by phase shifters. Simulation results demonstrate that,
compared to the state-of-the-art schemes, the proposed 2DIF and TS-2DIF methods
can achieve superior channel estimation accuracy.

</details>


### [104] [Soft Guessing Under Logarithmic Loss Allowing Errors and Variable-Length Source Coding](https://arxiv.org/abs/2510.09015)
*Shota Saito,Hamdi Joudeh*

Main category: cs.IT

TL;DR: 在对数损失下允许出错的软猜测问题中，给出最优猜测策略、单次界限以及i.i.d.源的渐近展开，并扩展到有侧信息的情形，揭示Renyi熵及其平滑版本在其中的核心作用，同时建立软猜测与变长度有损编码的关系。


<details>
  <summary>Details</summary>
Motivation: 量化在可容忍一定错误的猜测任务中的最小成本与效率；将猜测理论与有损编码联系起来，并利用Renyi熵族工具提供统一的分析框架。

Method: 推导出在对数损失下的最优猜测策略；给出对最小猜测矩的单次上界和下界，以及i.i.d.源的渐近展开；将结果推广到存在侧信息的情形；建立软猜测与变长度有损编码（对数损失）的联系；核心工具是Renyi熵、平滑Renyi熵及其条件版本。

Result: 确定了最优的软猜测策略；给出单次情形下最小猜测矩的上界与下界；对i.i.d.源给出渐近展开；在有侧信息的情况下也有等效结果；揭示了软猜测与变长度有损编码之间的联系；强调Renyi熵及其平滑版本在分析中的关键作用。

Conclusion: 该研究为在对数损失下的软猜测提供了完整的理论框架，利用Renyi相关量统一表征猜测成本与编码关系，拓展了猜测理论与有损编码的交叉领域，为潜在应用（如信息安全与序列决策等）提供了工具与洞见。

Abstract: This paper considers the problem of soft guessing under a logarithmic loss
distortion measure while allowing errors. We find an optimal guessing strategy,
and derive single-shot upper and lower bounds for the minimal guessing moments
as well as an asymptotic expansion for i.i.d. sources. These results are
extended to the case where side information is available to the guesser.
Furthermore, a connection between soft guessing allowing errors and
variable-length lossy source coding under logarithmic loss is demonstrated. The
R\'enyi entropy, the smooth R\'enyi entropy, and their conditional versions
play an important role.

</details>


### [105] [Low Complexity Detector for XL-MIMO Uplink: A Cross Splitting Based Information Geometry Approach](https://arxiv.org/abs/2510.09039)
*Wenjun Zhang,An-An Lu,Xiqi Gao*

Main category: cs.IT

TL;DR: 提出一种低复杂度的XL-MIMO检测方法CS-IGA，通过自然参数的交叉矩阵分解，将迭代检测降维到与匹配滤波相关的运算，同时扩展为非线性检测的NCS-IGA，能在相当甚至更优的BER下以更少迭代和更低计算成本实现，理论与仿真均证实优越性。


<details>
  <summary>Details</summary>
Motivation: 在极大规模MIMO（XL-MIMO）中，现有迭代检测（如AMP、IGA）的每次迭代复杂度随基站天线数线性增长，成为计算瓶颈。本工作旨在通过新颖的交叉矩阵分解，降低逐步检测的复杂度，使其在高维场景中更具实际可行性。

Method: 提出一种跨分裂的自然参数信息几何推断（CS-IGA）。通过对后验分布中的自然参数进行交叉矩阵分解，得到便于基于匹配滤波的迭代更新的因子化表示；在此框架基础上，扩展至非线性检测，提出非线性CS-IGA（NCS-IGA），在不依赖外部干扰消除环路的前提下进行符号逐一扫描处理，内嵌离散星座约束。

Result: 通过在接近现实通道条件下的全面仿真，CS-IGA在线性和非线性检测任务中，可以达到或超过贝叶斯最优AMP和IGA的BER性能，同时迭代次数更少，计算成本显著降低。

Conclusion: CS-IGA为XL-MIMO高吞吐信号检测提供了一种高效、可扩展的实用解，适合未来一代无线系统的检测需求。

Abstract: In this paper, we propose the cross splitting based information geometry
approach (CS-IGA), a novel and low complexity iterative detector for uplink
signal recovery in extralarge-scale MIMO (XL-MIMO) systems. Conventional
iterative detectors, such as the approximate message passing (AMP) algorithm
and the traditional information geometry algorithm (IGA), suffer from a per
iteration complexity that scales with the number of base station (BS) antennas,
creating a computational bottleneck. To overcome this, CS-IGA introduces a
novel cross matrix splitting of the natural parameter in the a posteriori
distribution. This factorization allows the iterative detection based on the
matched filter, which reduces per iteration computational complexity.
Furthermore, we extend this framework to nonlinear detection and propose
nonlinear CSIGA (NCS-IGA) by seamlessly embedding discrete constellation
constraints, enabling symbol-wise processing without external interference
cancellation loops. Comprehensive simulations under realistic channel
conditions demonstrate that CS-IGA matches or surpasses the bit error rate
(BER) performance of Bayes optimal AMP and IGA for both linear and nonlinear
detection, while achieving this with fewer iterations and a substantially lower
computational cost. These results establish CS-IGA as a practical and powerful
solution for high-throughput signal detection in next generation XL-MIMO
systems.

</details>


### [106] [Optimal binary codes from $\mathcal{C}_{D}$-codes over a non-chain ring](https://arxiv.org/abs/2510.09057)
*Ankit Yadav,Ritumoni Sarma,Anuj Kumar Bhagat*

Main category: cs.IT

TL;DR: 对在环上的C_D码及其子域码的参数和权重分布进行系统研究，给出距离最优族、自正交性及最小性充要条件，并以两权码构造强正则图


<details>
  <summary>Details</summary>
Motivation: 扩展对基于有限域的码的研究到环上的码及其子域码，以获得更强的参数和应用（如强正则图）

Method: 在环R上定义C_D码，考虑其二进制Gray像，研究C_D^(2)，推导权重分布和码字参数，给出最小性和自正交性的充分条件，构造两权码并导出SRG族

Result: 给出C_D^(2)在不同D下的Hamming重量分布和参数，发现若干距离最优的无限族，给出最小性与自正交性的充分条件，利用两权码构造两类强正则图

Conclusion: 环上子域码研究可提供更广的参数选项和应用，尤其在两权码与SRG构造方面有新贡献

Abstract: In \cite{shi2022few-weight}, Shi and Li studied $\mathcal{C}_D$-codes over
the ring $\mathcal{R}:=\mathbb{F}_2[x,y]/\langle x^2, y^2, xy-yx\rangle$ and
their binary Gray images, where $D$ is derived using certain simplicial
complexes. We study the subfield codes $\mathcal{C}_{D}^{(2)}$ of
$\mathcal{C}_{D}$-codes over $\mathcal{R},$ where $D$ is as in
\cite{shi2022few-weight} and more. We find the Hamming weight distribution and
the parameters of $\mathcal{C}_D^{(2)}$ for various $D$, and identify several
infinite families of codes that are distance-optimal. Besides, we provide
sufficient conditions under which these codes are minimal and self-orthogonal.
Two families of strongly regular graphs are obtained as an application of the
constructed two-weight codes.

</details>


### [107] [A Hybrid I/O Relation Estimation Scheme for Zak-OTFS Receivers](https://arxiv.org/abs/2510.09215)
*Sai Pradeep Muppaneni,Vineetha Yogesh,A. Chockalingam*

Main category: cs.IT

TL;DR: 提出一种基于混合模型无关与模型相关的DD I/O估计方案，用于Zak-OTFS的时延-多普勒域信道估计，相较纯模型无关方法在区域外估计不足的问题，能够提升整体I/O估计精度。


<details>
  <summary>Details</summary>
Motivation: 模型无关估计仅在模型无关的DD平面内的有限区域内读取样本，且DD脉冲形状对性能有较大影响，区域外信息无法估计，导致检测性能下降；需要一种简单、有效的手段来获取区域外的近似信息以提升I/O估计。

Method: 提出一种简单的混合估计 scheme：使用一种新的模型相关方法对DD平面区域外得到粗略的有效信道估计，将其与模型无关的区域内估计结果结合，得到改进的整体I/O关系。该方案支持独占（exclusive）与嵌入式（embedded）两种 Pilot 帧，并在Vehicular-A、TDL-A、TDL-C等信道模型且具有分数DD的场景下进行仿真验证。

Result: 仿真结果表明，所提混合估计在与纯模型无关方法相比时具有更优的估计性能，且对分数DD的鲁棒性更好，提升了整体I/O估计的准确性。

Conclusion: 通过在模型无关区域外获得粗略的模型相关信息并与模型无关估计结合，解决了模型无关方法在DD平面区域外信息不足的问题，提供了一种简洁且有效的提升Zak-OTFS DD I/O估计的方法，适用于不同Pilot帧配置。

Abstract: In this paper, we consider the problem of estimating the delay-Doppler (DD)
domain input-output (I/O) relation in Zak-OTFS modulation, which is needed for
signal detection. Two approaches, namely, model-dependent and model-free
approaches, can be employed for this purpose. The model-dependent approach
requires explicit estimation of the physical channel parameters (path delays,
Dopplers, and gains) to obtain the I/O relation. Such an explicit estimation is
not required in the model-free approach, where the I/O relation can be
estimated by reading off the samples in the fundamental DD period of the
received pilot frame. Model-free approach has the advantage of acquiring
fractional DD channels with simplicity. However, the read-off in the model-free
approach provides an estimate of the effective channel only over a limited
region in the DD plane but it does not provide an estimate for the region
outside, and this can affect the estimation performance depending on the pulse
shaping characteristics of the DD pulse shaping filter used. A poorly localized
DD pulse shape leads to an increased degradation in performance. Motivated by
this, in this paper, we propose a novel, yet simple, I/O relation estimation
scheme that alleviates the above issue in the model-free approach. We achieve
this by obtaining a coarse estimate of the effective channel outside the
model-free estimation region using a novel model-dependent scheme and using
this estimate along with the model-free estimate to obtain an improved estimate
of the overall I/O relation. We devise the proposed estimation scheme for both
exclusive and embedded pilot frames. Our simulation results using Vehicular-A,
TDL-A and TDL-C channel models with fractional DDs show that the proposed
hybrid estimation approach achieves superior performance compared to the pure
model-free approach.

</details>


### [108] [Serial Polar Automorphism Ensemble Decoders for Physical Unclonable Functions](https://arxiv.org/abs/2510.09220)
*Marvin Rübenacke,Sebastian Cammerer,Michael Sullivan,Alexander Keller*

Main category: cs.IT

TL;DR: 提出基于极化码的超低速率编码与串行 AED 的新方案，结合级联/递归交织器和 3 位量化，在 PUF 场景下实现与 BCH 基线相同的错误性能，同时码字长度减少 1.75 倍，从而降低辅助数据存储和芯片面积。


<details>
  <summary>Details</summary>
Motivation: PUFs 在原始比特错误率高达约 22% 的情况下，仍需极低的失效率（约 1e-6 级），这需要高效的 ultra-low-rate 码设计。降低面积和辅助数据存储也是实现商用化的关键。

Method: 提出以极化码为基础的编码方案，并引入低复杂度的自动同构解码（AED）的串行版本；通过级联与递归交织器扩展 AED 候选，复用单一 SC 解码器实现多次解码尝试；采用 3-bit 的量化策略以降低 SC 解码器的面积需求。

Result: 在目标块错误率为 1e-6 下，与基线 BCH 方案相比达到同样性能；但编码所需的码字位数为 BCH 基线的 1.75x，等效减少了 1.75x 的辅助数据存储和晶片面积。给定载荷位 K = 312。

Conclusion: 所提出的极化码+AED 方案为 PUFs 提供了一种更高效的超低速率编码实现，在保持等效错误性能的前提下显著降低码字长度、辅助数据存储和芯片面积。

Abstract: Physical unclonable functions (PUFs) involve challenging practical
applications of error-correcting codes (ECCs), requiring extremely low failure
rates on the order of $10^{-6}$ and below despite raw input bit error rates as
high as 22%. These requirements call for an efficient ultra-low rate code
design. In this work, we propose a novel coding scheme tailored for PUFs based
on Polar codes and a low-complexity version of automorphism ensemble decoding
(AED). Notably, our serial AED scheme reuses a single successive cancellation
(SC) decoder across multiple decoding attempts. By introducing cascaded and
recursive interleavers, we efficiently scale the number of AED candidates
without requiring expensive large multiplexers. An aggressive quantization
strategy of only 3 bits per message further reduces the area requirements of
the underlying SC decoder. The resulting coding scheme achieves the same block
error rate of $10^{-6}$ as our baseline based on Bose-Ray-Chaudhuri-Hocquenghem
(BCH) codes while requiring 1.75x fewer codeword bits to encode the same K =
312 payload bits. This reduction translates directly into 1.75x less helper
data storage and, consequently, a smaller overall chip area.

</details>


### [109] [Site-Specific RIS Deployment in Cellular Networks via Calibrated Ray Tracing](https://arxiv.org/abs/2510.09478)
*Sina Beyraghi,Javad Shabanpour,Giovanni Geraci,Paul Almasan,Angel Lozano*

Main category: cs.IT

TL;DR: 提出了一种全自动化的RIS部署策略，通过用Sionna射线追踪的数字孪生在英国城市场景中进行验证，对 RIS 的位置、朝向、配置以及基站波束成形在4G/5G/假设中的多参数联合优化。通过散射射线识别候选RIS站点，并以用户聚类降低部署开销。结果显示要显著提升覆盖，需要密集且大口径的RIS部署，从而引发对大规模RIS应用的可行性与成本的质疑。


<details>
  <summary>Details</summary>
Motivation: 在 RIS 能显著提升无线覆盖的前提下，如何在现实场景中进行可行且经济的部署成为关键挑战。通过数字孪生和多频段优化评估RIS部署的有效性与成本，是推动实际应用的必要步骤。

Method: 建立基于Sionna射线追踪的数字孪生，使用实测数据对场景进行标定；在此场景中对RIS的放置位置、朝向、配置以及基站波束成形进行联合优化，覆盖4G/5G/假设的6G频段；通过散射基射线识别候选RIS站点，并通过用户聚类降低部署开销。

Result: 实验/仿真结果表明，显著的覆盖提升依赖于密集、大面积口径的RIS部署，传统成本与可行性因素成为制约因素。

Conclusion: 对大规模RIS部署的可行性与成本提出质疑，需进一步在部署成本、维护、与多频协同等方面开展评估与优化。

Abstract: This work introduces a fully-automated RIS deployment strategy validated
through a digital twin, powered by Sionna ray tracing, of a UK city. On a scene
calibrated with measured data, the method jointly optimizes RIS placement,
orientation, configuration, and BS beamforming across 4G, 5G, and hypothetical
6G frequencies. Candidate RIS sites are identified via scattering-based rays,
while user clustering reduces deployment overhead. Results show that meaningful
coverage enhancement requires dense, large-aperture RIS deployments, raising
questions about the practicality and cost of large-scale RIS adoption.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [110] [UAV-Assisted 3-D Localization for IoT Networks Using a Simple and Efficient TDOA-AOA Estimator](https://arxiv.org/abs/2510.08765)
*Mojtaba Amiri,Rouhollah Amiri*

Main category: eess.SP

TL;DR: 提出一种使用最少测量（一个TDOA和一个AOA对）的3D源定位的闭式加权最小二乘解，利用单地面站与协作的无人机中继实现定位。


<details>
  <summary>Details</summary>
Motivation: 在极简传感信息条件下仍能实现高精度的3D定位，并在单地面站+无人机中继场景中提升效率与可扩展性。

Method: 推导并给出一个闭式的加权最小二乘估计量，基于一个TDOA和一个AOA的观测对，结合协作的 UAV 中继来实现定位，提供解析解与误差分析。

Result: 仿真结果表明所提方法在中等高斯噪声条件下可达到接近CRLB的性能，表现出接近最优的定位精度。

Conclusion: 给出在最少测量条件下的可行3D定位解法，并证明在给定场景下的有效性与鲁棒性。

Abstract: This letter proposes an algebraic solution for the problem of 3-D source
localization utilizing the minimum number of measurements, i.e., one Time
Difference of Arrival (TDOA) and one Angle of Arrival (AOA) pair. The proposed
method employs a closed-form weighted least squares estimator and enables the
positioning using a single ground station and a cooperative UAV relaying the
signal. Analytical derivations and simulation results demonstrate effectiveness
of the proposed approach, achieving near-optimal performance aligned with the
Cram\'er-Rao Lower Bound (CRLB) under moderate Gaussian noise conditions.

</details>


### [111] [Transfer Learning-Enabled Efficient Raman Pump Tuning under Dynamic Launch Power for C+L Band Transmission](https://arxiv.org/abs/2510.09047)
*Jiaming Liu,Rui Wang,JinJiang Li,Hong Lin,Jing Zhang,Kun Qiu*

Main category: eess.SP

TL;DR: 提出一个基于迁移学习的Transformer框架，用于在C+L波段系统中实现同时的建模和Raman泵设计，模型RMSE为0.22 dB，GSNR峰值到峰值变化与偏差分别为0.86 dB和0.1 dB。


<details>
  <summary>Details</summary>
Motivation: 在复杂的光传输系统中，需要对C+L波段链路进行精准建模并优化Raman放大泵设计，以提升系统性能和鲁棒性。通过迁移学习结合Transformer，可以在跨域数据上进行知识迁移，并对非线性信道进行高保真建模与优化泵设计。

Method: 提出一个迁移学习驱动的Transformer框架，联合进行建模与Raman泵设计的端到端/多任务学习，通过跨域预训练和目标任务微调来实现对C+L波段系统的高精度建模与泵设计。

Result: 对于建模任务，RMSE达到0.22 dB；对GSNR的峰值到峰值变化为0.86 dB，偏差为0.1 dB，表现出在同一框架下的高精度建模与泵性能优化能力。

Conclusion: 所提出的框架能够在C+L波段光通信系统中实现建模与泵设计的协同优化，获得低误差和稳定的GSNR表现，具有将其推广到其他波段或系统级优化的潜力。

Abstract: We propose a transfer learning-enabled Transformer framework to
simultaneously realize accurate modeling and Raman pump design in C+L-band
systems. The RMSE for modeling and peak-to-peak GSNR variation/deviation is
within 0.22 dB and 0.86/0.1 dB, respectively.

</details>


### [112] [Pinching-Antenna Assisted Sensing: A Bayesian Cramér-Rao Bound Perspective](https://arxiv.org/abs/2510.09137)
*Hao Jiang,Chongjun Ouyang,Zhaolin Wang,Yuanwei Liu,Arumugam Nallanathan,Zhiguo Ding*

Main category: eess.SP

TL;DR: 通过贝叶斯Cramér-Rao下界评估针控天线系统(PASS)的感知性能，揭示单目标时康托德（PA位置）与先验分布中心不对称的现象并引出动态PA重定位；提出两种目标调度协议（PS与PM）并将总体功率约束下的优化问题转化为在PA位置的搜索，表明PS在鲁棒性与计算复杂度之间具有折中优势。


<details>
  <summary>Details</summary>
Motivation: 在实际 sensing 场景中，传统CRB对参数值敏感且对估计无偏性有约束，难以为复杂可重构被动阵列提供实用的下界。本文以贝叶斯CRB为基础，评估PINCH-antenna系统在多目标/时分多址条件下的感知极限，探索动态PA重定位的重要性和调度策略的影响，以实现更低的功率开销下的感知性能提升。

Method: 建立多目标上行导频、TDMA场景下的 PAS 分析框架；对于单目标，分析感知质心与先验分布质心的错配并提出动态PA重定位需求；提出两种调度协议：1) pinch switching (PS)：为每个时隙进行单独的波束成形；2) pinch multiplexing (PM)：在所有时隙应用同一波束成形配置。将基于BCRB的总功率最小化（在BCRB阈值约束下）和基于总功率约束的min-max BCRB 问题等价转化为在PA位置上的搜索，并通过元素级算法求解。

Result: 数值结果显示：A) 具备大尺度可重构性的 PASS 相较于固定位置阵列可显著提升 sensing 性能；B) PS 在鲁棒性方面优于 PM，但代价是计算复杂度更高。

Conclusion: 可观测到的结论是：通过增加PA的可重构性并采用合适的调度协议，可以在 TDMA 场景下显著提升感知性能，且 PS 在鲁棒性方面更具优势，尽管需要更高的计算开销。动态PA重定位是单目标场景下对齐感知质心与先验分布的关键。

Abstract: The fundamental sensing limit of pinching-antenna systems (PASS) is studied
from a Bayesian Cram\'er-Rao bound (BCRB) perspective. Compared to conventional
CRB, BCRB is independent of the exact values of sensing parameters and is not
restricted by the unbiasedness of the estimator, thus offering a practical and
comprehensive lower bound for evaluating sensing performance. A system where
multiple targets transmit uplink pilots to a single-waveguide PASS under a
time-division multiple access (TDMA) scheme is analyzed. For the single-target
scenario, our analysis reveals a unique mismatch between the sensing centroid
(i.e., the optimal PA position) and the distribution centroid (i.e., the center
of the target's prior distribution), underscoring the necessity of dynamic PA
repositioning. For the multi-target scenario, two target scheduling protocols
are proposed: 1) pinch switching (PS), which performs separate pinching
beamforming for each time slot, and 2) pinch multiplexing (PM), which applies a
single beamforming configuration across all slots. Based on these protocols,
both the total power minimization problem under a BCRB threshold and the
min-max BCRB problem under a total power constraint are formulated. By
leveraging Karush-Kuhn-Tucker (KKT) conditions, these problems are equivalently
converted into a search over PA positions and solved using an element-wise
algorithm. Numerical results show that i)~PASS, endowed with large-scale
reconfigurability, can significantly enhance the sensing performance compared
with conventional fixed-position arrays, and ii)~PS provides more robust
performances than PM at the cost of higher computational complexity.

</details>


### [113] [Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for High-Power Applications](https://arxiv.org/abs/2510.09154)
*Tanjim Rahman,Trupti Ranjan Lenka*

Main category: eess.SP

TL;DR: 门场栅Al0.295GaN/GaN HEMT实现高击穿电压与在恶劣环境中的可靠工作，具备较好DC/RF性能，适合高压高功率射频应用。


<details>
  <summary>Details</summary>
Motivation: 在高温、高压、辐射等极端环境下的电子系统对高击穿电压、稳定性与射频性能有严格要求。本研究通过门场栅结构提升HEMT的击穿能力与工作鲁棒性。

Method: 采用Al0.295GaN/GaN异质结构的门场栅工程，利用自洽的Schrödinger-Poisson求解得到2DEG密度≈1×10^13 cm^-2；进行了直流和击穿仿真，得到阈值电压Vt≈-5.5 V、漏极饱和电流Id≈3 A、击穿电压BV≈1 kV；射频特性为ft≈28 GHz、fmax≈38 GHz，系统在40 GHz时仍保持稳定。

Result: 关键参数：Vt = -5.5 V、Ids = 3 A、BV = 1 kV、ft = 28 GHz、fmax = 38 GHz；自洽求解给出2DEG密度约1×10^13 cm^-2；DC与击穿仿真显示在高压条件下的可靠性。

Conclusion: 所提出的门场栅Al0.295GaN/GaN HEMT具备在高压、高温和放射性环境下进行高功率射频工作所需的击穿鲁棒性与射频性能，适用于恶劣环境中的应用。

Abstract: High Electron Mobility Transistors (HEMTs) are most suitable for harsh
environments as they operate reliably under extreme conditions such as high
voltages, high temperatures, radiation exposure and corrosive atmospheres. In
this article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for
achieving high breakdown voltage to reliably operate in harsh environments. The
Al0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)
density of the order of 1013 cm-2 obtained from the self-consistent solution of
Schr\"odinger and Poisson equations. The device has undergone DC and breakdown
simulations which result in threshold voltage of -5.5 V, drain saturation
current of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows
excellent RF characteristics which include cut-off frequency (ft) of 28 GHz and
maximum frequency of oscillation (fmax) of 38 GHz. The proposed gate
field-plated HEMT is stable up to 40 GHz and suitable for high-voltage and
high-power RF operation during harsh environment applications.

</details>


### [114] [Learning Product Graphs from Two-dimensional Stationary Signals](https://arxiv.org/abs/2510.09199)
*Andrei Buciulea,Bishwadeep Das,Elvin Isufi,Antonio G. Marques*

Main category: eess.SP

TL;DR: 提出一个面向二维矩阵图信号的图学习框架，通过对双维进行联合滤波来学习Kronecker/Cartesian/强乘积图，具备可证明的恢复性、较高估计精度和更低计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩展图信号处理到二维信号，捕捉时间、设备配置等额外维度上的依赖，克服仅处理标量信号的局限；利用二维平稳性与乘积图结构来建模复杂依赖。

Method: 将矩阵图信号建模为在两个维度上联合滤波产生的信号；引入双维图平稳性与乘积图表示；构造可解的优化问题，理论上证明可恢复出Kronecker/Cartesian/强乘积图，并给出高效算法。

Result: 在合成数据上，与现有方法相比，估计精度更高，计算成本更低。

Conclusion: 该框架将图学习扩展到二维信号，结合平稳性和乘积图结构，具有理论可恢复性和更高效的计算性，适用于在不规则域中分析多维依赖关系。

Abstract: Graph learning aims to infer a network structure directly from observed data,
enabling the analysis of complex dependencies in irregular domains. Traditional
methods focus on scalar signals at each node, ignoring dependencies along
additional dimensions such as time, configurations of the observation device,
or populations. In this work, we propose a graph signal processing framework
for learning graphs from two-dimensional signals, modeled as matrix graph
signals generated by joint filtering along both dimensions. This formulation
leverages the concept of graph stationarity across the two dimensions and
leverages product graph representations to capture structured dependencies.
Based on this model, we design an optimization problem that can be solved
efficiently and provably recovers the optimal underlying
Kronecker/Cartesian/strong product graphs. Experiments on synthetic data
demonstrate that our approach achieves higher estimation accuracy and reduced
computational cost compared to existing methods.

</details>


### [115] [Energy-Efficient Power Control in Single-User M-MIMO-OFDM System with PA Nonlinearity](https://arxiv.org/abs/2510.09232)
*Siddarth Marwaha,Pawel Kryszkiewicz,Eduard A. Jorswieck*

Main category: eess.SP

TL;DR: 研究了在带非线性放大器的OFDM的单用户M-MIMO下行系统中，通过对功放输出功率进行优化来提升能量效率。采用软限幅PA模型，推导了基于瑞利衰落和最大比传输的信号失真噪声比SNDR的闭式表达，定义考虑两种PA结构的能量效率EE函数，并提出一个低复杂度的根寻优算法以通过发射功率最大化EE。仿真显示在固定PA回退基线下，EE提升显著，且在低高路径损耗下均超过100%，优化点受天线数、PA模型与传输环境影响。


<details>
  <summary>Details</summary>
Motivation: 现有的能量高效资源分配研究多忽略功放非线性失真及OFDM中的子载波互调对能量效率的影响，导致对在宽带M-MIMO系统中通过优化功放输出功率来提升EE的潜力认识不足。

Method: 建立带非线性PA的宽带OFDM系统模型，采用软限幅PA，推导在瑞利衰落与MRT下的SNDR的闭式表达；定义包含两种PA架构及失真OFDM信号的EE函数；提出一种低复杂度的根-finding算法，通过调整发射功率实现EE最大化。

Result: 仿真结果显示相比固定PA回退基线，EE获得显著提升，在低/高路径损耗情况下均实现超过100%的改进；最佳工作点依赖于天线数、PA模型以及传播条件。

Conclusion: 结果表明在考虑非线性功放和宽带OFDM时，通过联合优化发射功率来提升EE具有明显优势，且该优化受系统配置和传输环境影响显著。

Abstract: Although multiple works have proposed energy-efficient resource allocation
schemes for Massive Multiple-Input Multiple-Output (M-MIMO) system, most
approaches overlook the potential of optimizing Power Amplifier (PA)
transmission power while accounting for non-linear distortion effects.
Furthermore, most M-MIMO studies assume narrow-band transmission, neglecting
subcarrier intermodulations at the non-linear PA for an Orthogonal Frequency
Division Multiplexing (OFDM) system. Therefore, this work investigates the
energy-efficient power allocation for a single-user equipment (UE) M-MIMO
downlink (DL) system employing OFDM with nonlinear PAs. Unlike prior works, we
model wide-band transmission using a soft-limiter PA model and derive a
closed-form expression for the signal-to-distortion-and-noise ratio (SNDR)
under Rayleigh fading and Maximal Ratio Transmission (MRT) precoding. Next, the
Energy Efficiency (EE) function is defined considering two PA architectures and
a distorted OFDM signal. We then propose a low complexity root-finding
algorithm to maximize EE by transmit power adjustment. Simulation results
demonstrate significant EE gains over a fixed PA back-off baseline, with over
$100\%$ improvement under both low and high path loss. Our findings reveal how
the optimal operating point depends on the antenna count, the PA model, and the
propagation conditions.

</details>


### [116] [IF-D: A High-Frequency, General-Purpose Inertial Foundation Dataset for Self-Supervised Learning](https://arxiv.org/abs/2510.09539)
*Patrick Ferreira,Paula Costa*

Main category: eess.SP

TL;DR: A large-scale inertial dataset (IF-D) for self-supervised learning on IMU time series, featuring 135 minutes of 9-channel data at 200 Hz from a UM7 IMU in a 3D-printed spherical enclosure, with calibration procedures and results to reduce motion bias; aims to support robust representation learning for tasks like event detection, motion mode recognition, and inertial navigation.


<details>
  <summary>Details</summary>
Motivation: Address the need for scalable, self-supervised representation learning on IMU time series while mitigating platform-specific motion biases; provide comprehensive calibration and realistic dynamics to improve transferability to downstream tasks.

Method: Collect continuous, long-duration, multichannel IMU data (accelerometer, gyroscope, magnetometer) at 200 Hz from a UM7 mounted in a spherical enclosure during vehicle traversal; describe data acquisition, preprocessing, and calibration steps (six-orientation accelerometer calibration, stationary gyroscope bias estimation, magnetometer hard-/soft-iron correction via ellipsoid fitting); dataset comprises ~1.6 million samples over ~135 minutes across nine sensor channels.

Result: Provided quantitative calibration results and preprocessing details, establishing a ready-to-use dataset; description covers calibration parameters and procedures.

Conclusion: IF-D mitigates platform-specific motion bias and exposes models to realistic dynamics and measurement noise, enabling robust representation learning and downstream tasks such as event detection, motion mode recognition, and inertial navigation.

Abstract: We present IF-D, a large-scale inertial dataset designed to enable
self-supervised and foundational learning for IMU time series. IF-D comprises
continuous, long-duration multichannel recordings (accelerometer, gyroscope,
magnetometer) sampled at 200Hz using a UM7 IMU mounted inside a 3D-printed
spherical enclosure that promotes diverse, free rotations during vehicle
traversal. The collection spans approximately 135 minutes of recording,
yielding around 1.6 million samples across nine sensor channels. We describe
the data acquisition setup, preprocessing, and calibration procedures
(six-orientation accelerometer calibration, stationary gyroscope bias
estimation, and ellipsoid fitting for magnetometer hard-/soft-iron correction),
and provide quantitative calibration results. IF-D is designed to mitigate
platform specific motion bias and expose models to both physical dynamics and
typical measurement noise, thereby facilitating robust representation learning
and downstream tasks such as event detection, motion mode recognition, and
inertial navigation.

</details>


### [117] [Power Cable Radiation: A Novel Approach to Underground Mining Connectivity](https://arxiv.org/abs/2510.09573)
*Siphiwe Shandu,Thabiso Moropa,Alain R. Ndjiongue*

Main category: eess.SP

TL;DR: 对地下矿井的无接触高压线通信进行分析，通过把供电线视为长天线，建立包含RF与电力线通道级联的系统框架，考察多径、频率相关衰减与瑞利/瑞西衰落，1-20 GHz仿真表明线长显著影响辐射、指向性与输入阻抗，显示CPLC可无接触传输电磁波，具备鲁棒、低成本、与现有矿用电力基础设施兼容的优点。


<details>
  <summary>Details</summary>
Motivation: 解决地下矿井中对无接触、鲁棒、成本效益高的通信方案的需求，提升矿工移动性、降低维护成本，并兼容现有供电基础设施。

Method: 提出一个系统级框架，包含RF通道与电力线通道的级联模型；将供电线建模为长天线，考虑多径传播、频率相关衰减和Rician衰落；在1-20 GHz范围内进行仿真分析。

Result: 线长对辐射强度、方向性和输入阻抗有显著影响；CPLC实现无直接电接触的电磁波传输，具有鲁棒性与成本优势。

Conclusion: CPLC为地下矿井中的通信提供了一种可行的无触点解决方案，能与现有矿用电力基础设施兼容，同时提升移动性并降低维护需求。

Abstract: This letter investigates contactless power line communications (CPLC) for
underground mining by modeling power wires as long-wire antennas. A
system-level framework is developed, comprising a cascade of RF and power line
channels. The model accounts for multipath propagation, frequency-dependent
attenuation, and Rician fading. Simulations from 1-20 GHz reveal that the
length of the wire significantly affects radiation, directivity, and input
impedance. The findings show that CPLC transmits electromagnetic waves without
direct electrical contact, offering a robust, cost-effective solution that
enhances mobility, reduces maintenance, and ensures compatibility with existing
mining power infrastructure.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [118] [Cognitive Radio for Asymmetric Cellular Downlink with Multi-User MIMO](https://arxiv.org/abs/2510.08937)
*Omer Gokalp Serbetci,Lei Chu,Andreas F. Molisch*

Main category: eess.SY

TL;DR: 对论文摘要的核心结论：在5G基础设施场景下，考虑多波束天线与不精确下行接收位置，提出基于概率的干扰判定与协同规则，分析灾难性干扰与错失机会概率，以及吞吐量对主/次基站功率和感知窗口的依赖，揭示现实场景下的频谱效率提升潜力。


<details>
  <summary>Details</summary>
Motivation: 动机在于把认知无线电应用落地到真实的5G基础设施场景中：下行目标接收位置不确定、主/次基站都具备多波束能力、需要现实的干扰评估与资源分配策略，以评估频谱共享的实际收益。

Method: 提出一个详细的协议，用于判断次级传输在不同波束方向是否会干扰主用户的位置，并建立基于概率的干扰规则。随后分析灾难性干扰概率、错失传输机会概率以及吞吐量，作为主/次基站发射功率和次级感知窗口的函数。

Result: 给出干扰概率和吞吐量随系统参数变化的定量关系，指明在不同参数设置下能够实现的频谱效率增益，强调感知窗口长度与功率配比对5G基础设施场景中认知无线电性能的关键影响。

Conclusion: 提供一个更贴近实际的认知无线电评估框架，能够在5G基础设施场景下对频谱共享的可行性、收益及资源分配策略进行现实化分析，为下一步的实现与优化提供方向。

Abstract: Cognitive radio (CR) is an important technique for improving spectral
efficiency, letting a secondary system operate in a wireless spectrum when the
primary system does not make use of it. While it has been widely explored over
the past 25 years, many common assumptions are not aligned with the realities
of 5G networks. In this paper, we consider the CR problem for the following
setup: (i) infrastructure-based systems, where downlink transmissions might
occur to receivers whose positions are not, or not exactly, known; (ii)
multi-beam antennas at both primary and secondary base stations. We formulate a
detailed protocol to determine when secondary transmissions into different beam
directions can interfere with primary users at potential locations and create
probability-based interference rules. We then analyze the "catastrophic
interference" probability and the "missed transmission opportunity"
probability, as well as the achievable throughput, as a function of the
transmit powers of the primary and secondary base stations and the sensing
window of the secondary base station. Results can serve to more realistically
assess the spectral efficiency gains in 5G infrastructure-based cognitive
systems.

</details>


### [119] [Traffic-Aware Eco-Driving Control in CAVs via Learning-based Terminal Cost Model](https://arxiv.org/abs/2510.08980)
*Mehmet Fatih Ozkan,Dennis Kibalama,Jacob Paugh,Marcello Canova,Stephanie Stockar*

Main category: eess.SY

TL;DR: A neural-network-assisted MPC terminal-cost approximation that incorporates upstream traffic dynamics for eco-driving in Connected and Automated Vehicles, improving energy efficiency with minimal travel-time impact and real-time scalability.


<details>
  <summary>Details</summary>
Motivation: Macroscopic traffic effects, such as upstream jams, can significantly affect energy efficiency in eco-driving but are often neglected by horizon-limited optimization. There is a need for traffic-aware, real-time optimization for CAVs.

Method: Develop a Neural Network-based approximation of the MPC terminal cost that explicitly includes upstream traffic dynamics (e.g., jams) into the optimization. The approach leverages look-ahead route information and V2X to produce traffic-aware speed trajectories. It emphasizes real-time scalability and robustness to traffic uncertainties.

Result: The traffic-aware MPC produces more energy-efficient speed trajectories than traffic-agnostic methods, with minimal impact on travel time. The framework is scalable for real-time implementation and robust to dynamic traffic conditions and macroscopic traffic events.

Conclusion: Incorporating macroscopic traffic events into the MPC via a neural-network terminal-cost approximation improves eco-driving performance for CAVs and is suitable for real-time deployment.

Abstract: Connected and Automated Vehicles (CAVs) offer significant potential for
improving energy efficiency and lowering vehicle emissions through eco-driving
technologies. Control algorithms in CAVs leverage look-ahead route information
and Vehicle-to-Everything (V2X) communication to optimize vehicle performance.
However, existing eco-driving strategies often neglect macroscopic traffic
effects, such as upstream traffic jams, that occur outside the optimization
horizon but significantly impact vehicle energy efficiency. This work presents
a novel Neural Network (NN)-based methodology to approximate the terminal cost
within a model predictive control (MPC) problem framework, explicitly
incorporating upstream traffic dynamics. By incorporating traffic jams into the
optimization process, the proposed traffic-aware approach yields more
energy-efficient speed trajectories compared to traffic-agnostic methods, with
minimal impact on travel time. The framework is scalable for real-time
implementation while effectively addressing uncertainties from dynamic traffic
conditions and macroscopic traffic events.

</details>


### [120] [MAKO: Meta-Adaptive Koopman Operators for Learning-based Model Predictive Control of Parametrically Uncertain Nonlinear Systems](https://arxiv.org/abs/2510.09042)
*Minghao Han,Kiwan Wong,Adrian Wing-Keung Law,Xunyuan Yin*

Main category: eess.SY

TL;DR: 提出 MAKO：一种基于元学习的 Koopman 操作符建模与预测控制框架，用于带参数不确定性的非线性系统，能从多模态数据中学习元模型并对未见参数快速适应，且给出闭环稳定性保证。结果显示建模和控制性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决非线性系统中未知/可变参数的不确定性对建模与控制的影响，现有方法需先验参数信息或泛化能力不足；将元学习与 Koopman 操作符结合，实现对新参数情形的快速自适应建模与控制。

Method: 提出 MAKO：一个深度元学习驱动的自适应建模框架，学习元 Koopman 模型；基于元模型在在线数据下对未见参数进行快速适应；基于该元 Koopman 模型设计预测控制，并给出闭环稳定性保障；通过多模态数据训练、数值仿真验证其鲁棒性和性能。

Result: 在广泛的仿真实验中，MAKO 在建模精度和控制有效性方面均优于竞争基线方法。

Conclusion: MAKO 为带参数不确定性的非线性系统提供了一种数据驱动、具通用化能力的建模-控制框架，能够在未见参数情形下保持稳定性并提高性能，具有潜在的广泛应用。

Abstract: In this work, we propose a meta-learning-based Koopman modeling and
predictive control approach for nonlinear systems with parametric
uncertainties. An adaptive deep meta-learning-based modeling approach, called
Meta Adaptive Koopman Operator (MAKO), is proposed. Without knowledge of the
parametric uncertainty, the proposed MAKO approach can learn a meta-model from
a multi-modal dataset and efficiently adapt to new systems with previously
unseen parameter settings by using online data. Based on the learned meta
Koopman model, a predictive control scheme is developed, and the stability of
the closed-loop system is ensured even in the presence of previously unseen
parameter settings. Through extensive simulations, our proposed approach
demonstrates superior performance in both modeling accuracy and control
efficacy as compared to competitive baselines.

</details>


### [121] [Sensing, Detection and Localization for Low Altitude UAV: A RF-Based Framework via Multiple BSs Collaboration](https://arxiv.org/abs/2510.09055)
*Tianhao Liang,Mu Jia,Tingting Zhang,Junting Chen,Longyu Zhou,Tony Q. S. Quek,Pooi-Yuen Kam*

Main category: eess.SY

TL;DR: 提出一个基于蜂窝基站的协同RF检测与定位框架，结合CA-CFAR与微多普勒特征识别，通过多站测量、网格概率与聚类降低鬼目标，实现接近CRLB的高精度定位，并通过RL优化权衡精度与资源。


<details>
  <summary>Details</summary>
Motivation: 应对低空经济中LLS/UAV增多带来的空域安全与轨迹规划挑战，需在现有基础设施上实现高鲁棒性定位与资源高效利用。

Method: 将CA-CFAR检测与MDS识别结合，进行多站融合，采用网格概率算法和聚类去混杂，推导CRLB作为性能基准，并用强化学习优化定位精度与基站资源开销。

Result: 仿真显示多基站可将定位误差降低至接近CRLB；实测验证框架有效；RL能在保持高精度的同时最小化资源使用。

Conclusion: 该框架具备可扩展性，有望在新兴低空经济中提升空域安全性与轨迹规划可靠性。

Abstract: The rapid growth of the low-altitude economy has resulted in a significant
increase in the number of Low, slow, and small (LLS) unmanned aerial vehicles
(UAVs), raising critical challenges for secure airspace management and reliable
trajectory planning. To address this, this paper proposes a cooperative
radio-frequency (RF) detection and localization framework that leverages
existing cellular base stations. The proposed approach features a robust scheme
for LSS target identification, integrating a cell averaging-constant false
alarm rate (CA-CFAR) detector with a micro-Doppler signature (MDS) based
recognition method. Multi-station measurements are fused through a grid-based
probabilistic algorithm combined with clustering techniques, effectively
mitigating ghost targets and improving localization accuracy in multi-UAV
scenarios. Furthermore, the Cramer-Rao lower bound (CRLB) is derived as a
performance benchmark and reinforcement learning (RL)-based optimization is
employed to balance localization accuracy against station resource usage.
Simulations demonstrate that increasing from one to multiple BSs reduces the
positioning error to near the CRLB, while practical experiments further verify
the framework's effectiveness. Furthermore, our RL-based optimization can find
solutions that maintain high accuracy while minimizing resource usage,
highlighting its potential as a scalable solution for ensuring airspace safety
in the emerging low-altitude economy.

</details>


### [122] [Antenna's Performance in Microwave Imaging of Stratified Media](https://arxiv.org/abs/2510.09138)
*Adel Omrani,Sajjad Sadeghi*

Main category: eess.SY

TL;DR: 在X波段（8-12 GHz）下，三种方向性天线（喇叭、开关Waveguide、Vivaldi）对分层介质的微波成像中，方向性更强的天线能获得更好且阴影更少的表征，DT算法可用于单/多基地数据的目标定位。


<details>
  <summary>Details</summary>
Motivation: 探究天线特性对分层介质成像质量的影响，比较不同类型天线在GPR/TWRI等应用中的表现，以指导天线选择与系统设计。

Method: 分析三种天线的远场和近场特性，在X波段8-12 GHz下，基于 diffraction tomography (DT) 的算法利用单基地与多基地数据对分层介质中的目标进行重建，比较不同天线在重建图像的效果。

Result: 更具方向性的天线在重建图像质量上表现更好，阴影效应更少；不同天线对DT重建的影响显著，指向性越强的天线 提升成像清晰度。

Conclusion: 天线的方向性特征对分层介质中DT成像的性能影响显著，选取更具方向性的天线可提高GPR/TWRI等系统的成像质量与鲁棒性。

Abstract: Numerous types of antennas have been employed for microwave imaging of
stratified media for ground penetrating radar (GPR), through-the-wall-radar
imaging (TWRI), etc. This letter aims to investigate the impact of the
different antennas with their characteristics on the image reconstruction of
those media. Hence, three types of antennas, including horn antennas, open
waveguide and Vivaldi antennas, are chosen as almost directional antennas,
operating at X-band 8-12 GHz. The antenna's far-field and near-field
characteristics are analyzed. A diffraction tomography (DT)-based algorithm is
used to reconstruct the target location within the stratified media using
monostatic and multistatic data. It is observed that the more directional
antennas provide a better-reconstructed image with less shadowing image of the
stratified media.

</details>


### [123] [Robust Adaptive Boundary Control of a Thermal Process with Thermoelectric Actuators: Theory and Experimental Validation](https://arxiv.org/abs/2510.09169)
*Paul Mayr,Alessandro Pisano,Stefan Koch,Markus Reichhartinger*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A sliding-mode-based adaptive boundary control law is proposed for a class of
uncertain thermal reaction-diffusion processes subject to matched disturbances.
The disturbances are assumed to be bounded, but the corresponding bounds are
unknown, thus motivating the use of adaptive control strategies. A boundary
control law comprising a proportional and discontinuous term is proposed,
wherein the magnitude of the discontinuous relay term is adjusted via a
gradient-based adaptation algorithm. Depending on how the adaptation algorithm
is parameterized, the adaptive gain can be either a nondecreasing function of
time (monodirectional adaptation) or it can both increase and decrease
(bidirectional adaptation). The convergence and stability properties of these
two solutions are investigated by Lyapunov analyses, and two distinct stability
results are derived, namely, asymptotic stability for the monodirectional
adaptation and globally uniformly ultimately bounded solutions for the
bidirectional adaptation. The proposed algorithms are then specified to address
the control problem of stabilizing a desired temperature profile in a metal
beam equipped with thermoelectric boundary actuators. Experiments are conducted
to investigate the real-world performance of the proposed sliding-mode-based
adaptive control, with a particular focus on comparing the monodirectional and
bidirectional adaptation laws.

</details>


### [124] [Safety Analysis of eVTOL Operations based on STPA](https://arxiv.org/abs/2510.09283)
*Mariat James Elizebeth,Shufeng Chen,Halima El Badaoui,Siddartha Khastgir,Paul Jennings*

Main category: eess.SY

TL;DR: 采用扩展的系统理论过程分析（STPA）对英国空域中eVTOL安全风险进行系统性识别与优先级排序，提出针对性高优先级法规需求清单以促成安全部署。


<details>
  <summary>Details</summary>
Motivation: eVTOL潜在的噪声、成本与多方参与带来复杂的安全与合规挑战，需要面向监管更新的系统性风险评估方法。

Method: 应用基于系统思维的STPA，对UCAs、因果因素与要求进行级联分析；为处理大规模结果，提出对STPA的扩展以优先排序；共识别317个UCAs、110个高优先UCAs，产生377个因果因素与432条要求，最终筛选出124条高优先要求并发现56处法规/政策/程序缺口。

Result: 形成面向监管者的安全考量清单，可用于制定/更新法规、合规方法与指南材料，涵盖组织绩效、认证、培训、碰撞回避、能源管理与自动化等领域。

Conclusion: 研究为英国空域的eVTOL部署提供风险洞察与合规指南，提示监管机构在法规与监管框架中应重点关注高优先需求及存在的缺口，以提升安全性与可操作性。

Abstract: Electric Vertical Take-Off and Landing (eVTOL) aircraft are expected to be
quieter and more cost-effective than helicopters, offering major economic and
social benefits through improved connectivity. Their adoption will require new
ground infrastructure and airspace redesign, introducing risks involving
multiple stakeholders (Regulators, eVTOL operators, Air navigation service
providers, Vertiport operators, OEMs, Pilots, etc.). To assess these risks for
the UK airspace, systems-thinking based System Theoretic Process Analysis
(STPA) was conducted. To manage the large number of Unsafe Control Actions
(UCAs) and requirements generated due to the complexity of the analysis, a
novel extension to STPA for the prioritization of results was applied. 317 UCAs
were identified in total out of which 110 high-priority UCAs were analyzed
(Step-4), resulting in 377 causal factors and 432 requirements. These were
prioritized to produce a targeted list of 124 distinct high-priority
requirements, 56 of which were identified as gaps in existing aviation
regulations, policies, or procedures.. These highlight opportunities for
regulatory updates in areas such as organizational performance, certification
processes, training, collision avoidance, energy management, and automation.
The findings provide regulators with safety considerations that could shape new
or updated regulations, compliance methods, and guidance materials for the safe
deployment of eVTOLs.

</details>


### [125] [Weighting Factors Tuning by Direct Feedback in Predictive Control of Multiphase Motors](https://arxiv.org/abs/2510.09290)
*Manuel R. Arahal,Manuel G. Satué,Kumars Rouzbehi,Francisco Colodro*

Main category: eess.SY

TL;DR: 提出一种闭环权重因子自适应方法，用于多相驱动的预测定子电流控制（PSCC），在每个工作点确定最优权重，计算开销低，并在五相感应电机的实验中得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有PSCC中权重因子的选择困难，需大量试错或复杂在线优化，难以在不同工况和参考信号发生变化时保持良好性能。

Method: 建立将权重因子与性能指标相关的闭环映射，按工作点在线确定最优权重；具有较低计算负担，且能适应参考指标的变化。

Result: 给出适用于每个工作点的最优权重因子；对性能指标参考变化具有鲁棒性；在五相感应电机的实验台上进行了实际验证。

Conclusion: 所提方法在降低计算负担的同时实现对多相驱动PSCC的自适应权重控制，实验结果验证了其可行性。

Abstract: Predictive Stator Current Control (PSCC) has been proposed for control of
multi-phase drives. The flexibility offered by the use of a Cost Function has
been used to deal with the increased number of phases. However, tuning of the
Weighting Factors constitutes a problem. Intensive trial and error tests are
usual in this context. Existing on-line selection methods, on the other hand,
require large amounts of data and/or complex optimization procedures. The
proposal of this paper is a closed-loop scheme that links Weighting Factors to
performance indicators. In this way, optimal Weighting Factors are determined
for each operating point. Also, changes in reference values for performance
indicators are easily tackled. Unlike previous methods, the proposal carries
very little computational burden. A case study is developed for a five-phase
induction motor and assessed with real experimentation on a laboratory set-up.

</details>


### [126] [Data-Driven Control Of Power Converters](https://arxiv.org/abs/2510.09304)
*Marwan Soliman,Pauline Kergus,Diego Regruto,Luiz Villa,Zohra Kader*

Main category: eess.SY

TL;DR: 利用VRFT等数据驱动方法对Buck变换器进行反馈控制设计，解决开关器件带来的控制难题，探讨其在OwnTech基础电路上的可行性。


<details>
  <summary>Details</summary>
Motivation: 电力变换器在高效、稳定地管理电能方面具有关键作用，但由开关器件引起的非线性、离散性使传统控制方法难以直接应用。数据驱动方法如VRFT能在弱建模条件下设计控制器，提升鲁棒性与实现简易性。

Method: 在Buck模式下，应用虚拟参考反馈调优（VRFT）方法进行控制器设计与参数整定；以OwnTech基金会提供的Buck型电路为实验/仿真对象，评估VRFT在实际开关转换条件下的可行性与性能。

Result: 该摘要未提供具体实验或仿真结果，未披露VRFT在该系统上的定量性能。

Conclusion: 论文的结论尚未给出，预计后续工作会评估VRFT相对于传统控制策略的优势，并拓展至其他变换器拓扑。

Abstract: The fundamental role of power converters is to efficiently manage and control
the flow of electrical energy, ensuring compatibility between power sources and
loads. All these applications of power converters need the design of an
appropriate control law. Control of power converters is a challenging problem
due to the presence of switching devices which are difficult to handle using
traditional control approaches. The objective of this paper is to investigate
the use of data-driven techniques, in particular the Virtual References
Feedback Tuning (VRFT) method, in the context of power converters feedback
control. This study considers a buck \pauline{mode} power converter circuit
provided by the OwnTech foundation.

</details>


### [127] [3C Resources Joint Allocation for Time-Deterministic Remote Sensing Image Backhaul in the Space-Ground Integrated Network](https://arxiv.org/abs/2510.09409)
*Chongxiao Cai,Yan Zhu,Min Sheng,Jiandong Li,Yan Shi,Di Zhou,Ziwen Xie,Chen Zhang*

Main category: eess.SY

TL;DR: 提出一个多维资源时间扩展图MDR-TEG，用于对低地球轨道观测卫星的时间确定影像传输的资源调度问题，将MSTR-TDI建模为MILP并通过SRCC的迭代分解得到近似最优解，实验验证模型有效性。


<details>
  <summary>Details</summary>
Motivation: 由于OS的计算资源受限且资源在通信、缓存、计算3C上呈时间变化和多维性，需高效协同调度以提升Time-determined Images (TDI)的传输成功率。

Method: 设计MDR-TEG，通过分时槽和外部虚拟节点描述3C资源；将MSTR-TDI建模为MILP；将其松弛并分解为MSTRI和ETP两个子问题；提出SRCC算法计算松弛约束的子梯度；通过求解子问题和对偶问题获得上下界并进行迭代；通过安排图像发送序列提升解的质量。

Result: 通过多次迭代获得近似最优解；仿真实验验证MDR-TEG的优越性和SRCC的有效性。

Conclusion: MDR-TEG及SRCC为提升LEO观测卫星环境下TDI传输成功率的协同调度提供高效途径，并在仿真中展现出优越性。

Abstract: Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to
compress and backhaul more time-determined images (TDI) has become a new
paradigm, which is used to enhance the timeout caused by the limited computing
resources of OSs. However, how to capture the time-varying and dynamic
characteristics of multi-dimensional resources is challenging for efficient
collaborative scheduling. Motivated by this factor, we design a highly succinct
multi-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,
by employing a slots division mechanism and introducing an external virtual
node, the time-varying communication, caching, and computing (3C) resources are
depicted in low complexity by the link weights within, between, and outside the
slots. Based on the MDR-TEG, the maximizing successful transmission ratio of
TDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.
Which further relaxed decomposed into two tractable sub-problems: maximizing
the successful transmission rate of images (MSTRI) and ensuring the timeliness
problem (ETP). Subsequently, an efficient subgradient of relaxation computing
constraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI
are obtained by solving the two subproblems and the dual problem (DP), and the
direction of the next iteration is obtained by feedback. Furthermore, arranging
the sending sequences of images to improve the quality of the solution. The
approximate optimal solution of MSTR-TDI is eventually obtained through
repeated iterations. The simulation results verify the superiority of the
proposed MDR-TEG model and the effectiveness of the SRCC.

</details>


### [128] [Grid-forming Control of Converter Infinite Bus System: Modeling by Data-driven Methods](https://arxiv.org/abs/2510.09411)
*Amir Bahador Javadi,Philip Pong*

Main category: eess.SY

TL;DR: 深度符号回归在网格形成的无限母线系统动力学建模中具有更高保真度，但计算成本显著高于SINDy；SINDy提供更高效的替代方案，适合实时网格应用。


<details>
  <summary>Details</summary>
Motivation: 在可再生能源大量接入的电力系统中，需要从数据中学习系统动力学，以支持动态建模、控制设计和稳定性分析；需在建模精度与计算成本之间取得平衡。

Method: 比较两种数据驱动建模方法：稀疏非线性动力学识别（SINDy）与深度符号回归；基于合成数据（活跃功率、无功功率及电压参考扰动）评估二者对 grid-forming converter-based infinite bus 的动力学建模能力。

Result: 深度符号回归在捕获复杂系统动力学方面更准确，但训练和推理时间显著增加；SINDy则更为高效，适合实时应用，但在某些非线性特征上可能略逊。

Conclusion: 在需要高保真度时可选用深度符号回归；若以运行时成本和实时性为优先，则优先采用SINDy，亦可结合两者以在精度与速度之间实现权衡。

Abstract: This study explores data-driven modeling techniques to capture the dynamics
of a grid-forming converter-based infinite bus system, critical for
renewable-integrated power grids. Using sparse identification of nonlinear
dynamics and deep symbolic regression, models were generated from synthetic
data simulating key disturbances in active power, reactive power, and voltage
references. Deep symbolic regression demonstrated more accuracy in capturing
complex system dynamics, though it required substantially more computational
time than sparse identification of nonlinear dynamics. These findings suggest
that while deep symbolic regression offers high fidelity, sparse identification
of nonlinear dynamics provides a more computationally efficient approach,
balancing accuracy and runtime for real-time grid applications.

</details>


### [129] [Robust reset control design for piezo-actuated nano-positioner in presence of hysteresis nonlinearity](https://arxiv.org/abs/2510.09445)
*Ashkan Sebghati,S. Hassan HosseinNia*

Main category: eess.SY

TL;DR: 提出一种基于复杂阶元的鲁棒控制框架，结合CgLp重置控制器和SIDF/HOSIDF分析工具，在压电驱动的纳米定位系统中对滞回和非线性进行鲁棒调控，并通过约束优化实现参数整定，仿真验证性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的压电材料滞回通过逆模型补偿的方式并不足以单独解决非线性对高频分量的影响，且鲁棒线性控制存在本质限制，需要新的鲁棒控制策略来降低残余非线性对追踪性能的影响。

Method: 基于复数阶元的鲁棒控制框架；采用CgLp重置控制器实现复数阶控制；利用SIDF和HOSIDF对系统非线性与频域特性进行分析；通过受约束的优化问题对控制参数进行整定。

Result: 仿真验证表明，与仅依赖滞回逆模型的方案相比，所提CgLp控制提高了对高频分量的鲁棒性和追踪精度，在不同行程范围下展现更优的性能。

Conclusion: 该框架克服了鲁棒线性控制的固有局限性，为将复杂阶控制与重置控制应用于压电纳米定位系统提供了可行性与有效性，并为此类高频非线性系统的鲁棒控制设计提供了一条新的路径。

Abstract: In this paper, a robust nonlinear control scheme is designed for the motion
control of a class of piezo-actuated nano-positioning systems using
frequency-domain analysis. The hysteresis, the nonlinearity in the
piezoelectric material, degrades the precision in tracking references with high
frequency contents and different travel ranges. The hysteresis compensation by
the inverse model, as the state-of-the-art solution, is not reliable alone.
Therefore, a control framework with robustness against the remaining
nonlinearity is needed. It is shown that there is an unavoidable limitation in
robust linear control design to improve the performance. A robust control
methodology based on a complex-order element is established to relax the
limitation. Then, a constant-in-gain-lead-in-phase (CgLp) reset controller is
utilized to realize the complex-order control. The control design is based on
the sinusoidal input describing function (SIDF) and the higher-order SIDF
(HOSIDF) tools. A constrained optimization problem is provided to tune the
control parameters. The achieved improvements by the CgLp control is validated
by the simulation.

</details>
