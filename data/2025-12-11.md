<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 3]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.NI](#cs.NI) [Total: 2]
- [eess.SY](#eess.SY) [Total: 9]
- [eess.SP](#eess.SP) [Total: 10]
- [cs.LG](#cs.LG) [Total: 57]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [SURA: Secure Unsourced Random Access](https://arxiv.org/abs/2512.09104)
*Mohammad Javad Ahmadi,Rafael F. Schaefer,H. Vincent Poor*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work introduces security for unsourced random access (URA) by employing wiretap-inspired physical layer techniques. To achieve confidentiality, the proposed system opportunistically exploits intrinsic features of feedback-aided URA without adding any overhead or altering its original structure or operational characteristics. As a result, the proposed system preserves the low-cost advantages of URA, including low delay and minimal signaling overhead, while providing secure communication. To secure transmission, each user generates a secret key and an artificial noise sequence from the feedback signal that the BS broadcasts in previous transmission rounds. This feedback depends on the BS-user channel, making it a private signal for each user. The secure transmission is performed by three actions: encrypting the data using the secret key, sending only the parity bits of the LDPC encoded secret key to allow the legitimate receiver to recover it, and masking these parity bits with the artificial noise. For reception, a receiver algorithm is designed for the legitimate user, and a leakage analysis is provided to quantify the information available to the eavesdropper. The simulation results show that meaningful secrecy is achieved in URA without modifying its structure and with negligible impact on standard performance.

</details>


### [2] [$t$-Fold $s$-Blocking Sets and $s$-Minimal Codes](https://arxiv.org/abs/2512.09457)
*Hao Chen,Xu Pan,Conghui Xie*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Blocking sets and minimal codes have been studied for many years in projective geometry and coding theory. In this paper, we provide a new lower bound on the size of $t$-fold $s$-blocking sets without the condition $t \leq q$, which is stronger than the classical result of Beutelspacher in 1983. Then a lower bound on lengths of projective $s$-minimal codes is also obtained. It is proved that $(s+1)$-minimal codes are certainly $s$-minimal codes. We generalize the Ashikhmin-Barg condition for minimal codes to $s$-minimal codes. Many infinite families of $s$-minimal codes satisfying and violating this generalized Ashikhmin-Barg condition are constructed. We also give several examples which are binary minimal codes, but not $2$-minimal codes.

</details>


### [3] [Binary and Non-Binary Self-Dual Sequences and Maximum Period Single-Track Gray Codes](https://arxiv.org/abs/2512.09655)
*Tuvi Etzion*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Binary self-dual sequences have been considered and analyzed throughout the years, and they were used for various applications. Motivated by a construction for single-track Gray codes, we examine the structure and recursive constructions for binary and non-binary self-dual sequences. The feedback shift registers that generate such sequences are discussed. The connections between these sequences and maximum period single-track codes are discussed. Maximum period non-binary single-track Gray codes of length $p^t$ and period $p^{p^t}$ are constructed. These are the first infinite families of maximum period codes presented in the literature.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [4] [EMMap: A Systematic Framework for Spatial EMFI Mapping and Fault Classification on Microcontrollers](https://arxiv.org/abs/2512.09049)
*Gandham Sai Santhosh,Siddhartha Sanjay Naik,Ritwik Badola,Chester Rebeiro*

Main category: cs.CR

TL;DR: 提出一个与平台无关的 Spatial EMFI Mapping 与故障分类框架，并给出在 Xtensa LX6 (ESP32) 与两块 ChipWhisper 板上的初步演示，旨在建立可重复的工作流以分析 EMFI 时的空间敏感性与故障类型。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏统一的方法学来系统性映射空间敏感性及对故障行为进行分类。基于 O'Flynn 与 Kuhnapfel 等的洞见，提出一个平台无关的框架以理解探针位置如何影响故障结果，并提供可重复的分析流程。

Method: 提出一个平台无关的 Spatial EMFI Mapping 与 Fault Classification 框架；构建探针空间位置与故障输出之间的映射；基于已有研究进行故障分类；在三个目标板上进行 pilot 实验（包括 Xtensa LX6/ESP32 与两块 ChipWhisper 板）以演示方法的可应用性；强调可重复性与跨架构可移植性。

Result: 给出一个框架性研究路径并提供演示性实验，证明框架可用于将空间定位与故障类型统一成一个可重复的分析流程；初步观察指向在跨嵌入式体系结构中进行 EMFI 易感性分析的可行性。

Conclusion: 该工作提供一个统一、可重复的 EMFI 空间映射与故障分类的工作流，为后续在不同嵌入式架构上的系统评估奠定基础，未来可扩展至更多目标板与量化的评估指标。

Abstract: Electromagnetic Fault Injection (EMFI) is a powerful technique for inducing bit flips and instruction-level perturbations on microcontrollers, yet existing literature lacks a unified methodology for systematically mapping spatial sensitivity and classifying resulting fault behaviors. Building on insights from O'Flynn and Kuhnapfel et al., we introduce a platform-agnostic framework for Spatial EMFI Mapping and Fault Classification, aimed at understanding how spatial probe position influences fault outcomes. We present pilot experiments on three representative microcontroller targets including the Xtensa LX6 (ESP32) and two ChipWhisper boards not as definitive evaluations, but as illustrative demonstrations of how the proposed methodology can be applied in practice. These preliminary observations motivate a generalized and reproducible workflow that researchers can adopt when analyzing EMFI susceptibility across diverse embedded architectures.

</details>


### [5] [Exposing Vulnerabilities in Counterfeit Prevention Systems Utilizing Physically Unclonable Surface Features](https://arxiv.org/abs/2512.09150)
*Anirudh Nakra,Nayeeb Rashid,Chau-Wai Wong,Min Wu*

Main category: cs.CR

TL;DR: 本论文提出一个针对纸面物理不可仿制特征（Paper-PUF）的认证的正式操作框架，揭示在物理和数字域的系统级漏洞，并通过物理拒绝服务与数字伪造攻击来证明攻击有效性，强调需要强有力的防护措施来实现可靠的防伪认证。


<details>
  <summary>Details</summary>
Motivation: 伪造品对公共卫生与安全构成重大风险，而基于纸面表面不可克隆细微特征的防伪手段具有潜在的低成本与可普及性，但要将其从可行性研究落地为真实世界的安全解决方案，需对系统进行全面的风险建模与分析。

Method: 提出一个面向纸-PUF认证的运营框架，并据此在物理与数字两个层面进行系统性漏洞分析，设计并验证物理层的拒绝服务攻击和数字层的伪造攻击，以评估认证体系的鲁棒性，进而给出阶段性安全分析以指导未来防伪系统设计。

Result: 通过攻击设计与分析，揭示现有基于纸-PUF的认证方法在现实环境中的漏洞，证明攻击可破坏认证流程，强调需要有效的安全对策；所提出的框架为对纸-PUF系统的全面安全评估提供了基础。

Conclusion: 该框架可用于对纸-PUF防伪系统进行系统性、阶段性的安全评估，帮助研究者和设计者在未来的防伪方案中提前识别与缓解潜在攻击路径。

Abstract: Counterfeit products pose significant risks to public health and safety through infiltrating untrusted supply chains. Among numerous anti-counterfeiting techniques, leveraging inherent, unclonable microscopic irregularities of paper surfaces is an accurate and cost-effective solution. Prior work of this approach has focused on enabling ubiquitous acquisition of these physically unclonable features (PUFs). However, we will show that existing authentication methods relying on paper surface PUFs may be vulnerable to adversaries, resulting in a gap between technological feasibility and secure real-world deployment. This gap is investigated through formalizing an operational framework for paper-PUF-based authentication. Informed by this framework, we reveal system-level vulnerabilities across both physical and digital domains, designing physical denial-of-service and digital forgery attacks to disrupt proper authentication. The effectiveness of the designed attacks underscores the strong need for security countermeasures for reliable and resilient authentication based on paper PUFs. The proposed framework further facilitates a comprehensive, stage-by-stage security analysis, guiding the design of future counterfeit prevention systems. This analysis delves into potential attack strategies, offering a foundational understanding of how various system components, such as physical features and verification processes, might be exploited by adversaries.

</details>


### [6] [Analysis of the Security Design, Engineering, and Implementation of the SecureDNA System](https://arxiv.org/abs/2512.09233)
*Alan T. Sherman,Jeremy J. Romanik Romano,Edward Zieglar,Enis Golaszewski,Jonathan D. Fuchs,William E. Byrd*

Main category: cs.CR

TL;DR: 对SecureDNA系统的安全设计与实现进行评估，发现关键协议的结构性弱点，并提出改进方案；版本升级到1.1.0后将通过SCEP+修复。


<details>
  <summary>Details</summary>
Motivation: 深入了解系统设计、工程实现中的安全性，评估密钥管理、证书基础设施、认证与速率限制，以及对互认证、请求处理和豁免处理协议的形式化分析。

Method: 基于对源代码（v1.0.8）的逆向分析，结合形式化方法对互认证、基本请求与豁免处理协议进行分析，并评估速率限制与TLS中的绑定性。

Result: 主要发现：SCEP实现的互认证其实是单向认证； hazards数据库与密钥服务器无法知晓对方身份，削弱防御深度；缺乏有效的密码绑定使TLS通道中数据库响应的篡改/重放难以检测；实现上不允许重新连接，但若采用更强绑定，安全性可提升。软件v1.1.0通过提出的SCEP+协议修复SCEP。

Conclusion: 识别的结构性弱点削弱了防御深度，建议增加强绑定以强化认证与消息完整性；版本1.1.0实现了该改进并通过形式化分析验证。

Abstract: We analyze security aspects of the SecureDNA system regarding its system design, engineering, and implementation. This system enables DNA synthesizers to screen order requests against a database of hazards. By applying novel cryptography, the system aims to keep order requests and the database of hazards secret. Discerning the detailed operation of the system in part from source code (Version 1.0.8), our analysis examines key management, certificate infrastructure, authentication, and rate-limiting mechanisms. We also perform the first formal-methods analysis of the mutual authentication, basic request, and exemption-handling protocols.
  Without breaking the cryptography, our main finding is that SecureDNA's custom mutual authentication protocol SCEP achieves only one-way authentication: the hazards database and keyservers never learn with whom they communicate. This structural weakness violates the principle of defense in depth and enables an adversary to circumvent rate limits that protect the secrecy of the hazards database, if the synthesizer connects with a malicious or corrupted keyserver or hashed database. We point out an additional structural weakness that also violates the principle of defense in depth: inadequate cryptographic bindings prevent the system from detecting if responses, within a TLS channel, from the hazards database were modified. Consequently, if a synthesizer were to reconnect with the database over the same TLS session, an adversary could replay and swap responses from the database without breaking TLS. Although the SecureDNA implementation does not allow such reconnections, it would be stronger security engineering to avoid the underlying structural weakness. We identify these vulnerabilities and suggest and verify mitigations, including adding strong bindings. Software Version 1.1.0 fixes SCEP with our proposed SCEP+ protocol.

</details>


### [7] [ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data](https://arxiv.org/abs/2512.09321)
*Ruiqi Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 提出 ObliInjection，一种针对多源输入的提示注入攻击，解决源段排序不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 在存在多源输入的应用中，攻击者往往只能控制部分源且不知段落顺序，导致现有单源攻击在此场景中效果有限，需要新的思路来评估与实现注入攻击。

Method: 引入 order-oblivious loss 来衡量无论段落顺序如何都倾向完成攻击任务的概率，并提出 orderGCG 算法来最小化该损失并优化污染段。

Result: 在三个数据集、十二个LLMs上进行系统实验；当仅有 1/6 到 1/100 的段落被污染时也能实现显著的攻击效果，显示方法对多源输入的鲁棒性。

Conclusion: 首次将提示注入攻击扩展到多源输入场景，提供新的目标函数和优化算法，强调对多源、分布式输入系统的防护需求。

Abstract: Prompt injection attacks aim to contaminate the input data of an LLM to mislead it into completing an attacker-chosen task instead of the intended task. In many applications and agents, the input data originates from multiple sources, with each source contributing a segment of the overall input. In these multi-source scenarios, an attacker may control only a subset of the sources and contaminate the corresponding segments, but typically does not know the order in which the segments are arranged within the input. Existing prompt injection attacks either assume that the entire input data comes from a single source under the attacker's control or ignore the uncertainty in the ordering of segments from different sources. As a result, their success is limited in domains involving multi-source data.
  In this work, we propose ObliInjection, the first prompt injection attack targeting LLM applications and agents with multi-source input data. ObliInjection introduces two key technical innovations: the order-oblivious loss, which quantifies the likelihood that the LLM will complete the attacker-chosen task regardless of how the clean and contaminated segments are ordered; and the orderGCG algorithm, which is tailored to minimize the order-oblivious loss and optimize the contaminated segments. Comprehensive experiments across three datasets spanning diverse application domains and twelve LLMs demonstrate that ObliInjection is highly effective, even when only one out of 6-100 segments in the input data is contaminated.

</details>


### [8] [Proof of Trusted Execution: A Consensus Paradigm for Deterministic Blockchain Finality](https://arxiv.org/abs/2512.09409)
*Kyle Habib,Vladislav Kapitsyn,Giovanni Mazzeo,Faisal Mehrban*

Main category: cs.CR

TL;DR: PoTE通过可验证执行的信任执行环境实现共识，避免分叉和时钟瓶颈，提升吞吐，替代PoW/PoS的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的PoW/PoS共识在能耗、确认延迟、以及投票权集中等方面存在结构性瓶颈。PoW需大量计算带来高能耗与延迟；PoS提高效率但引入股权集中、长范围攻击、’看不见的股权’等风险，且吞吐受槽时间及多轮选举的上限限制。需要一种在执行可验证性基础上实现全球一致性的新范式。

Method: 提出PoTE： validators 在异构的基于VM的TEE中运行同一规范程序，其测量值公开记录；每个节点产出厂商背书的证明，将 enclave 代码哈希与区块内容绑定；执行结果是确定性的，出块者由公开随机性唯一推导，因此无需分叉、无槽时瓶颈、单轮验证即可 commits 区块。给出PoTE共识客户端的设计、参考实现，并将性能与“万亿级去中心化交易所”的严格吞吐需求进行对比评估。

Result: 设计与参考实现已给出，且对PoTE在高吞吐需求场景下的性能进行了评估。

Conclusion: PoTE提供基于可验证执行的新型共识范式，能消除分叉与槽时瓶颈、实现单轮确认；但其安全性和去中心化水平依赖于TEE的可信性、供应链与硬件厂商信任模型，需要权衡潜在的集聚风险与攻击面。

Abstract: Current blockchain consensus protocols -- notably, Proof of Work (PoW) and Proof of Stake (PoS) -- deliver global agreement but exhibit structural constraints. PoW anchors security in heavy computation, inflating energy use and imposing high confirmation latency. PoS improves efficiency but introduces stake concentration, long-range and "nothing-at-stake" vulnerabilities, and a hard performance ceiling shaped by slot times and multi-round committee voting. In this paper, we propose Proof of Trusted Execution (PoTE), a consensus paradigm where agreement emerges from verifiable execution rather than replicated re-execution. Validators operate inside heterogeneous VM-based TEEs, each running the same canonical program whose measurement is publicly recorded, and each producing vendor-backed attestations that bind the enclave code hash to the block contents. Because the execution is deterministic and the proposer is uniquely derived from public randomness, PoTE avoids forks, eliminates slot.time bottlenecks, and commits blocks in a single round of verification. We present the design of a PoTE consensus client, describe our reference implementation, and evaluate its performance against the stringent throughput requirements of the Trillion decentralized exchange.

</details>


### [9] [Reference Recommendation based Membership Inference Attack against Hybrid-based Recommender Systems](https://arxiv.org/abs/2512.09442)
*Xiaoxiao Chi,Xuyun Zhang,Yan Wang,Hongsheng Hu,Wanchun Dou*

Main category: cs.CR

TL;DR: 提出了一种基于度量的成员身份推断攻击（MIA），面向混合式/混合型推荐系统，利用个性化特征生成参考推荐并通过目标推荐、历史交互与参考推荐之间的关系来推断数据成员身份，理论与实验均显示方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有MIAs多针对混合推荐系统的两种算法的情形，未充分利用推荐系统的个性化特征来设计攻击。对于以同一算法结合用户-物品历史与属性的混合型推荐系统，成员资格推断的研究空缺明确。

Method: 提出基于参考推荐的相对成员身份度量（relative membership metric）。核心思路是：1) 依据个性化特征为目标用户生成一个参考推荐；2) 使用目标用户的历史交互、目标推荐和参考推荐之间的关系计算成员身份得分；3) 理论分析与实验验证该度量在混合型推荐系统上的有效性。

Result: 通过理论推导和实证实验，证明所提度量在混合型推荐系统中的MIAs具有显著效果，能够比现有MIAs更准确地判断目标用户数据是否用于训练，揭示个性化信息在成员身份推断中的利用潜力。

Conclusion: 度量驱动的MIA能够在混合型（同一算法、结合用户-物品历史与属性）推荐系统中有效推断数据成员身份，提示需要在隐私保护方面考虑个性化信号下的攻击向量，并为未来防护研究指明方向。

Abstract: Recommender systems have been widely deployed across various domains such as e-commerce and social media, and intelligently suggest items like products and potential friends to users based on their preferences and interaction history, which are often privacy-sensitive. Recent studies have revealed that recommender systems are prone to membership inference attacks (MIAs), where an attacker aims to infer whether or not a user's data has been used for training a target recommender system. However, existing MIAs fail to exploit the unique characteristic of recommender systems, and therefore are only applicable to mixed recommender systems consisting of two recommendation algorithms. This leaves a gap in investigating MIAs against hybrid-based recommender systems where the same algorithm utilizing user-item historical interactions and attributes of users and items serves and produces personalised recommendations. To investigate how the personalisation in hybrid-based recommender systems influences MIA, we propose a novel metric-based MIA. Specifically, we leverage the characteristic of personalisation to obtain reference recommendation for any target users. Then, a relative membership metric is proposed to exploit a target user's historical interactions, target recommendation, and reference recommendation to infer the membership of the target user's data. Finally, we theoretically and empirically demonstrate the efficacy of the proposed metric-based MIA on hybrid-based recommender systems.

</details>


### [10] [Comparative Analysis of Hash-based Malware Clustering via K-Means](https://arxiv.org/abs/2512.09539)
*Aink Acrie Soe Thein,Nikolaos Pitropakis,Pavlos Papadopoulos,Sam Grierson,Sana Ullah Jan*

Main category: cs.CR

TL;DR: TLSH与IMPHASH在恶意软件聚类中提供更具分辨力的簇，SSDeep在广义分类任务上更高效。


<details>
  <summary>Details</summary>
Motivation: 数字设备普及导致攻击面扩大，迫切需要更精细的恶意软件聚类以提升检测鲁棒性与自适应防御能力。

Method: 使用K-means对基于三种哈希指纹的特征进行对比评估，针对已有恶意软件家族和特征进行聚类分析。

Result: TLSH和IMPHASH产生更分离、语义上更有意义的簇；SSDeep在广义分类任务上更高效。

Conclusion: 研究结果可为开发更鲁棒的威胁检测和自适应安全机制提供指导。

Abstract: With the adoption of multiple digital devices in everyday life, the cyber-attack surface has increased. Adversaries are continuously exploring new avenues to exploit them and deploy malware. On the other hand, detection approaches typically employ hashing-based algorithms such as SSDeep, TLSH, and IMPHash to capture structural and behavioural similarities among binaries. This work focuses on the analysis and evaluation of these techniques for clustering malware samples using the K-means algorithm. More specifically, we experimented with established malware families and traits and found that TLSH and IMPHash produce more distinct, semantically meaningful clusters, whereas SSDeep is more efficient for broader classification tasks. The findings of this work can guide the development of more robust threat-detection mechanisms and adaptive security mechanisms.

</details>


### [11] [Chasing Shadows: Pitfalls in LLM Security Research](https://arxiv.org/abs/2512.09549)
*Jonathan Evertz,Niklas Risse,Nicolai Neuer,Andreas Müller,Philipp Normann,Gaetano Sapia,Srishti Gupta,David Pape,Soumya Shaw,Devansh Srivastav,Christian Wressnegger,Erwin Quiring,Thorsten Eisenhofer,Daniel Arp,Lea Schönherr*

Main category: cs.CR

TL;DR: 提出九大常见陷阱并在72篇论文中调查其普遍性，展示对评估与可重复性的影响，並给出改进指南。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全研究中的快速普及暴露出可重复性、严格性和评估方面的新挑战，传统ML研究中的陷阱不足以覆盖LLMs时代。

Method: 识别九大陷阱，覆盖数据收集、预训练、微调、提示和评估等阶段；在2023-2024年顶级安全与软件工程期刊的72篇论文中进行系统调查；通过四个实证案例研究分析陷阱影响；给出可操作的改进建议。

Result: 所有论文中至少包含一个陷阱，且每个陷阱在多篇论文中出现；仅有15.7%的陷阱被明确讨论；案例研究显示陷阱可导致评估失真、性能膨胀与可重复性下降。

Conclusion: 提出面向未来研究的实践性指南，提升方法学严谨性、评估可靠性与可重复性；为LLM安全研究提供制度化的改进路径。

Abstract: Large language models (LLMs) are increasingly prevalent in security research. Their unique characteristics, however, introduce challenges that undermine established paradigms of reproducibility, rigor, and evaluation. Prior work has identified common pitfalls in traditional machine learning research, but these studies predate the advent of LLMs. In this paper, we identify \emph{nine} common pitfalls that have become (more) relevant with the emergence of LLMs and that can compromise the validity of research involving them. These pitfalls span the entire computation process, from data collection, pre-training, and fine-tuning to prompting and evaluation.
  We assess the prevalence of these pitfalls across all 72 peer-reviewed papers published at leading Security and Software Engineering venues between 2023 and 2024. We find that every paper contains at least one pitfall, and each pitfall appears in multiple papers. Yet only 15.7\% of the present pitfalls were explicitly discussed, suggesting that the majority remain unrecognized. To understand their practical impact, we conduct four empirical case studies showing how individual pitfalls can mislead evaluation, inflate performance, or impair reproducibility. Based on our findings, we offer actionable guidelines to support the community in future work.

</details>


### [12] [Defining Cost Function of Steganography with Large Language Models](https://arxiv.org/abs/2512.09769)
*Hanzhou Wu,Yige Wang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we make the first attempt towards defining cost function of steganography with large language models (LLMs), which is totally different from previous works that rely heavily on expert knowledge or require large-scale datasets for cost learning. To achieve this goal, a two-stage strategy combining LLM-guided program synthesis with evolutionary search is applied in the proposed method. In the first stage, a certain number of cost functions in the form of computer program are synthesized from LLM responses to structured prompts. These cost functions are then evaluated with pretrained steganalysis models so that candidate cost functions suited to steganography can be collected. In the second stage, by retraining a steganalysis model for each candidate cost function, the optimal cost function(s) can be determined according to the detection accuracy. This two-stage strategy is performed by an iterative fashion so that the best cost function can be collected at the last iteration. Experiments show that the proposed method enables LLMs to design new cost functions of steganography that significantly outperform existing works in terms of resisting steganalysis tools, which verifies the superiority of the proposed method. To the best knowledge of the authors, this is the first work applying LLMs to the design of advanced cost function of steganography, which presents a novel perspective for steganography design and may shed light on further research.

</details>


### [13] [FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning](https://arxiv.org/abs/2512.09872)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.CR

TL;DR: FlipLLM is a reinforcement learning–based, architecture-agnostic framework for discovering minimal, high-impact bit-flip attack (BFA) locations in language and vision foundation models. It uses sensitivity-guided pruning and Q-learning to efficiently identify critical bit sets, achieving up to 2.5x faster discovery than state-of-the-art methods and causing drastic performance drops with very few flips. ECC SECDED can mitigate these effects, underscoring hardware-level defense value. 


<details>
  <summary>Details</summary>
Motivation: As foundation models scale (LLMs and VLMs), they remain vulnerable to hardware-based bit-flip attacks. Existing BFA discovery methods lack generalizability and scalability, failing to explore the vast parameter space and complex interdependencies within modern models within reasonable time. There is a need for a scalable, generalizable method to identify vulnerable bit locations across architectures and tasks to inform defenses.

Method: Formulate BFA discovery as a sequential decision-making problem using a RL framework. Employ sensitivity-guided layer pruning to reduce search space and apply Q-learning to select minimal bit sets that cause high impact. The approach is architecture-agnostic and evaluated on multiple text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, DeepSeek-V2 7B), VLMs (LLaVA 1.6), and datasets (MMLU, MMLU-Pro, VQAv2, TextVQA).

Result: FlipLLM identifies critical bits that are vulnerable to BFAs significantly faster than SOTA methods (up to 2.5x). Flipping the identified bits reduces LLaMA 3.1 8B accuracy from 69.9% to ~0.2%; LLaVA VQA score drops from 78% to nearly 0% with 5–7 bits flipped. Hardware protections like ECC SECDED at the identified locations fully mitigate the attack, highlighting practical defense implications. 

Conclusion: FlipLLM provides the first scalable, adaptive methodology for exploring BFA vulnerability in both language and multimodal foundation models, enabling comprehensive hardware-security evaluation and informing defense strategies.

Abstract: Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.

</details>


### [14] [ByteShield: Adversarially Robust End-to-End Malware Detection through Byte Masking](https://arxiv.org/abs/2512.09883)
*Daniel Gibert,Felip Manyà*

Main category: cs.CR

TL;DR: 提出一种基于字节级掩码的确定性遮蔽防御，通过对输入文件生成多版本并进行阈值投票来提升对对抗性样本的鲁棒性，在 EMBER 与 BODMAS 上优于随机化/去随机化平滑防御，同时保持对干净样本的高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端恶意软件检测器易受对抗性攻击；随机化和去/去随机化平滑防御对大规模对抗性载荷仍有漏洞；需要一种结构化、覆盖输入的掩码策略，以在多版本中削弱对抗载荷对模型决策的影响。

Method: 提出一种确定性字节掩码策略，对输入文件按步长滑动掩码，生成多个遮蔽版本；对每个版本独立分类；应用阈值投票输出最终判定；通过覆盖输入以充分或部分遮蔽对抗载荷，并确保部分版本仍保留原始意图以供投票抑制对抗影响。

Result: 在 EMBER 与 BODMAS 数据集上，该遮蔽防御对包含多种功能性保持操作的对抗样本效果优于随机化与(去)随机化平滑防御，并且能在干净样本上维持较高准确率。

Conclusion: 字节级确定性掩码结合投票机制为端到端恶意软件检测提供了一种有效的鲁棒防御框架，能够在不同对抗场景中对载荷进行遮蔽并通过集体决策降低其影响。

Abstract: Research has proven that end-to-end malware detectors are vulnerable to adversarial attacks. In response, the research community has proposed defenses based on randomized and (de)randomized smoothing. However, these techniques remain susceptible to attacks that insert large adversarial payloads. To address these limitations, we propose a novel defense mechanism designed to harden end-to-end malware detectors by leveraging masking at the byte level. This mechanism operates by generating multiple masked versions of the input file, independently classifying each version, and then applying a threshold-based voting mechanism to produce the final classification. Key to this defense is a deterministic masking strategy that systematically strides a mask across the entire input file. Unlike randomized smoothing defenses, which randomly mask or delete bytes, this structured approach ensures coverage of the file over successive versions. In the best-case scenario, this strategy fully occludes the adversarial payload, effectively neutralizing its influence on the model's decision. In the worst-case scenario, it partially occludes the adversarial payload, reducing its impact on the model's predictions. By occluding the adversarial payload in one or more masked versions, this defense ensures that some input versions remain representative of the file's original intent, allowing the voting mechanism to suppress the influence of the adversarial payload. Results achieved on the EMBER and BODMAS datasets demonstrate the suitability of our defense, outperforming randomized and (de)randomized smoothing defenses against adversarial examples generated with a wide range of functionality-preserving manipulations while maintaining high accuracy on clean examples.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [15] [Tyche: A Hybrid Computation Framework of Illumination Pattern for Satellite Beam Hopping](https://arxiv.org/abs/2512.09312)
*Ziheng Yang,Kun Qiu,Zhe Chen,Wenjun Zhu,Yue Gao*

Main category: cs.NI

TL;DR: 提出了 Tyche：一个混合计算框架，结合 Monte Carlo Tree Search Beam Hopping（MCTS-BH）和贪婪 Beam Hopping（G-BH），用于高吞吐量卫星的光照模式计算。通过滑动窗口和剪枝等技术，将单次对37个单元的光照模式计算时间从传统算法的数分钟级降低至12秒，使得在背景并行计算中也能实现实时近似，且在评估中吞吐量提升最高可达约98.76%。


<details>
  <summary>Details</summary>
Motivation: 解决高通量卫星（HTS）在光束跳变（beam hopping）中计算有效照明模式的计算瓶颈。现有方法如遗传算法对37个单元需超过300秒，且对300多个单元的实际应用不可行；多智能体深度强化学习在单元数超过40时收敛性差。

Method: 提出 Tyche 框架，其中使用 MCTS-BH 算法来计算光照模式，并在后台运行；同时加入滑动窗口与剪枝等技术以显著降低计算时间。为确保实时性，提供一个在 MCTS-BH 计算完成前能给出建议解的贪婪 Beam Hopping（G-BH）算法作为 provisional 解决方案。

Result: 在37个单元上，MCTS-BH 仅需约12秒即可完成一个光照模式的计算。评估显示，Tyche 可使吞吐量提升高达 98.76%，相比现有方案有显著改进。

Conclusion: Tyche 提供了一种在大规模 HTS 场景下实现接近实时的光照模式计算的可行解决方案。通过将 MCTS-BH 的全局搜索与 G-BH 的快速近似相结合，并辅以滑动窗口与剪枝，能够显著提升吞吐量并降低单次计算时间，使得大规模单元的光束跳变更具实用性。

Abstract: High-Throughput Satellites (HTS) use beam hopping to handle non-uniform and time-varying ground traffic demand. A significant technical challenge in beam hopping is the computation of effective illumination patterns. Traditional algorithms, like the genetic algorithm, require over 300 seconds to compute a single illumination pattern for just 37 cells, whereas modern HTS typically covers over 300 cells, rendering current methods impractical for real-world applications. Advanced approaches, such as multi-agent deep reinforcement learning, face convergence issues when the number of cells exceeds 40. In this paper, we introduce Tyche, a hybrid computation framework designed to address this challenge. Tyche incorporates a Monte Carlo Tree Search Beam Hopping (MCTS-BH) algorithm for computing illumination patterns and employs sliding window and pruning techniques to significantly reduce computation time. Specifically, MCTS-BH can compute one illumination pattern for 37 cells in just 12 seconds. To ensure real-time computation, we use a Greedy Beam Hopping (G-BH) algorithm, which provides a provisional solution while MCTS-BH completes its computation in the background. Our evaluation results show that MCTS-BH can increase throughput by up to 98.76%, demonstrating substantial improvements over existing solutions.

</details>


### [16] [Eunomia: A Multicontroller Domain Partitioning Framework in Hierarchical Satellite Network](https://arxiv.org/abs/2512.09345)
*Qi Zhang,Kun Qiu,Zhe Chen,Wenjun Zhu,Xiaofan Xu,Ping Du,Yue Gao*

Main category: cs.NI

TL;DR: 提出 Eunomia 三步域划分框架，通过移动感知的 FOV 分割与混合控制平面实现低延迟、低开销的域划分，结合谱聚类和 Kuhn–Munkres 分配，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在 6G 场景下，LEO 高机动性和视场（FOV）限制使域划分面临可扩展性与 signaling 负担的挑战。集中式控制和纯分布式架构各自存在瓶颈，需新的高效域划分方案。

Method: 三步域划分：1) 基于移动感知的 FOV 分割约束域；2) 使用 ground stations 与 MEO 卫星的混合控制平面，降低 signaling；3) 构建控制开销关系图并进行谱聚类优化域分配，结合 Kuhn–Munkres 算法实现控制器分配。

Result: 在 Plotinus 仿真平台上实现，Eunomia 将请求丢失降低至最高 58.3%，控制开销降低最高 50.3%，算法执行时间降低最高 77.7%，显著优于现有方案。

Conclusion: Eunomia 提供可扩展、低时延、低开销的域划分框架，适合 6G 需求，确保单跳信令并实现更均衡的流量分布。

Abstract: With the rise of mega-satellite constellations, the integration of hierarchical non-terrestrial and terrestrial networks has become a cornerstone of 6G coverage enhancements. In these hierarchical satellite networks, controllers manage satellite switches within their assigned domains. However, the high mobility of LEO satellites and field-of-view (FOV) constraints pose fundamental challenges to efficient domain partitioning. Centralized control approaches face scalability bottlenecks, while distributed architectures with onboard controllers often disregard FOV limitations, leading to excessive signaling overhead. LEO satellites outside a controller's FOV require an average of five additional hops, resulting in a 10.6-fold increase in response time. To address these challenges, we propose Eunomia, a three-step domain-partitioning framework that leverages movement-aware FOV segmentation within a hybrid control plane combining ground stations and MEO satellites. Eunomia reduces control plane latency by constraining domains to FOV-aware regions and ensures single-hop signaling. It further balances traffic load through spectral clustering on a Control Overhead Relationship Graph and optimizes controller assignment via the Kuhn-Munkres algorithm. We implement Eunomia on the Plotinus emulation platform with realistic constellation parameters. Experimental results demonstrate that Eunomia reduces request loss by up to 58.3%, control overhead by up to 50.3\%, and algorithm execution time by 77.7% significantly outperforming current state-of-the-art solutions.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [17] [Characterizing Human Feedback-Based Control in Naturalistic Driving Interactions via Gaussian Process Regression with Linear Feedback](https://arxiv.org/abs/2512.09097)
*Rachel DiPirro,Rosalyn Devonport,Dan Calderone,Chishang "Mario'' Yang,Wendy Ju,Meeko Oishi*

Main category: eess.SY

TL;DR: 基于驾驶仿真数据，通过高斯过程回归推断人类驾驶员的状态反馈策略，构建控制器并比较不同驾驶人群的差异，以指导社会性自洽的自动驾驶设计。


<details>
  <summary>Details</summary>
Motivation: 解释为何需要理解人车交互及未标记交叉口的策略以提升安全性；推动面向社会合作者的自动驾驶系统设计。

Method: 在驾驶仿真环境中收集自然决策数据；将人类驾驶员反应建模为基于状态的反馈控制器，采用高斯过程回归学习；使用线性与非线性先验加权组合来估算控制器的反馈增益；分析个体增益与行为的对应，以及不同人群的差异。

Result: 确定增益如何映射到观测到的驾驶行为，揭示不同驾驶人群之间的差异，验证数据驱动分析对理解政策形成的有效性。

Conclusion: 数据驱动的驾驶员策略分析有助于未来设计社会化响应性强的车辆，使自动驾驶系统更好地与人类驾驶员协同工作。

Abstract: Understanding driver interactions is critical to designing autonomous vehicles to interoperate safely with human-driven cars. We consider the impact of these interactions on the policies drivers employ when navigating unsigned intersections in a driving simulator. The simulator allows the collection of naturalistic decision-making and behavior data in a controlled environment. Using these data, we model the human driver responses as state-based feedback controllers learned via Gaussian Process regression methods. We compute the feedback gain of the controller using a weighted combination of linear and nonlinear priors. We then analyze how the individual gains are reflected in driver behavior. We also assess differences in these controllers across populations of drivers. Our work in data-driven analyses of how drivers determine their policies can facilitate future work in the design of socially responsive autonomy for vehicles.

</details>


### [18] [MPC for momentum counter-balanced and zero-impulse contact with a free-spinning satellite](https://arxiv.org/abs/2512.09213)
*Theofania Karampela,Rishie Seshadri,Florian Dörfler,Sarah H. Q. Li*

Main category: eess.SY

TL;DR: 提出了一种非线性模型预测控制（MPC）框架，用以使服务卫星在与自旋目标卫星接触时实现零冲击，且协调服务卫星的两大模块（矩动力生成和操作模块）以处理两者之间的耦合，且通过蒙特卡洛仿真验证对比先前方法的优势。


<details>
  <summary>Details</summary>
Motivation: 在轨道服务任务中，目标卫星自由自旋且需要与之接触的场景对控制提出高约束与耦合挑战；需要能够在保持约束的同时实现零冲击接触并实现自旋同步。

Method: 建立一个非线性MPC框架，对服务卫星的两大模块及其耦合动力学进行建模，显式包含动作和状态约束；通过数值蒙特卡洛试验评估控制性能，并与先前控制方法进行对比。

Result: MPC可强制执行其他控制方法难以覆盖的约束，能在多种受限条件下维持自旋同步和零冲击接触，并在观测和执行噪声、接触位置变更等情形下保持鲁棒性；蒙特卡洛仿真验证了其有效性。

Conclusion: 所提出的MPC框架对实现OOS任务中的零冲击接触具有可行性和鲁棒性，成功协调两大模块并处理耦合动力学，为在轨服务任务中的自旋目标接触提供了更可靠的控制方案。

Abstract: In on-orbit robotics, a servicer satellite's ability to make contact with a free-spinning target satellite is essential to completing most on-orbit servicing (OOS) tasks. This manuscript develops a nonlinear model predictive control (MPC) framework that generates feasible controls for a servicer satellite to achieve zero-impulse contact with a free-spinning target satellite. The overall maneuver requires coordination between two separately actuated modules of the servicer satellite: (1) a moment generation module and (2) a manipulation module. We apply MPC to control both modules by explicitly modeling the cross-coupling dynamics between them. We demonstrate that the MPC controller can enforce actuation and state constraints that prior control approaches could not account for. We evaluate the performance of the MPC controller by simulating zero-impulse contact scenarios with a free-spinning target satellite via numerical Monte Carlo (MC) trials and comparing the simulation results with prior control approaches. Our simulation results validate the effectiveness of the MPC controller in maintaining spin synchronization and zero-impulse contact under operation constraints, moving contact location, and observation and actuation noise.

</details>


### [19] [Electric Arc Furnaces Scheduling under Electricity Price Volatility with Reinforcement Learning](https://arxiv.org/abs/2512.09293)
*Ruonan Pi,Zhiyuan Fan,Bolun Xu*

Main category: eess.SY

TL;DR: 在电弧炉(EAF)运行中，提出一种基于强化学习的实时控制框架以应对电价波动，并将确定性调度问题以MILP形式建模，利用Q学习实现多单元EAF的实时控制；在NY州数据上，与基线规则控制器和完美价格预测的MILP基准比较，RL仅需非预测信息即可实现约90%利润水平。


<details>
  <summary>Details</summary>
Motivation: 解决EAF在电价波动下的优化调度问题，传统MILP在价格不确定性和实时性方面存在局限；需要一个能在无后验信息条件下近似最优解且具实时性的控制方法。

Method: 将确定性EAF调度问题建模为MILP；开发针对多单元EAF的Q学习算法用于实时控制；设计专门的奖励函数以平滑起动惩罚；以纽约州真实数据进行训练与评估，并与基线规则控制和假设完美价格的MILP基准进行比较。

Result: 在多单元和单单元场景的非预测控制下，所提RL算法在利润上可达到完美MILP基准约90%的水平。

Conclusion: RL框架对EAF的实时高效运行具有潜力，能在价格波动条件下接近最优MILP基准；显示了在复杂约束（如共享喂给容量）下的可行性与有效性。

Abstract: This paper proposes a reinforcement learning-based framework for optimizing the operation of electric arc furnaces (EAFs) under volatile electricity prices. We formulate the deterministic version of the EAF scheduling problem into a mixed-integer linear programming (MILP) formulation, and then develop a Q-learning algorithm to perform real-time control of multiple EAF units under real-time price volatility and shared feeding capacity constraints. We design a custom reward function for the Q-learning algorithm to smooth the start-up penalties of the EAFs. Using real data from EAF designs and electricity prices in New York State, we benchmark our algorithm against a baseline rule-based controller and a MILP benchmark, assuming perfect price forecasts. The results show that our reinforcement learning algorithm achieves around 90% of the profit compared to the perfect MILP benchmark in various single-unit and multi-unit cases under a non-anticipatory control setting.

</details>


### [20] [Time-Discretized Simulation of Vehicle Platoons for Safety Analysis with Guaranteed Error Bounds](https://arxiv.org/abs/2512.09416)
*Yuhao Chen,Ahmet Cetinkaya*

Main category: eess.SY

TL;DR: 基于仿真的研究，分析在无线包丢失和突发制动情况下，满足串联稳定性的控制参数对车辆最小距离的影响；提出两种离散时间步长选择方法以控制距离近似误差，并比较不同控制参数的表现。


<details>
  <summary>Details</summary>
Motivation: 在车队编队中，无线通信的包丢失可能导致安全风险，需评估不同控制参数对维持安全距离的影响。

Method: 时间离散化仿真：在不同时间步长下，使用两种选择策略来确保距离误差在给定界内；计算随时间的绝对最小车辆间距；对一组能保证串联稳定性的控制参数进行比较。

Result: 数值示例显示，在同一组串稳定性保证的控制参数中，某些在同时发生的包丢失与突发制动条件下比其他参数表现更好。

Conclusion: 提供一种仿真框架用于在包丢失和突发制动场景下比较不同控制参数的安全性与鲁棒性，同时给出时间步长选择策略以确保距离误差界限。

Abstract: Wireless communication is essential to achieve coordinated control in vehicle platoons. However, packet losses in wireless communication can cause critical safety issues when they occur in conjunction with sudden brakes. In this paper, we propose simulation-based methods that allow the study of such safety issues by determining the absolute minimum distance between vehicles over time for various control parameters that guarantee string stability. For our proposed time-discretized simulations, we provide two methods for selecting different time-step intervals to ensure that the error in distance approximation remains within specified bounds at all times. Through numerical examples we demonstrate that among control parameters that guarantee string stability some perform better than others under simultaneously occurring packet losses and sudden brakes.

</details>


### [21] [Personalized Building Climate Control with Contextual Preferential Bayesian Optimization](https://arxiv.org/abs/2512.09481)
*Wenbin Wang,Jicheng Shi,Colin N. Jones*

Main category: eess.SY

TL;DR: Contextual preferential Bayesian optimization for tuning building climate controllers using binary preferences; achieves up to 23% utility gain on MPC in BOPTEST and personalizes tuning to occupant types.


<details>
  <summary>Details</summary>
Motivation: Occupant utility is latent and hard to measure; external context (e.g., outdoor temperature) varies over time, complicating controller tuning; need efficient, real-time optimization.

Method: Proposed contextual preferential Bayesian optimization leveraging binary preference feedback and contextual information to tune an economic MPC controller in real time.

Result: Two-month BOPTEST simulation shows the method outperforms the baseline controller with up to 23% improvement in utility; adapts to different occupant types, enabling personalized controller tuning.

Conclusion: The approach enables efficient, real-time, and personalized controller tuning for building climate control under changing contextual conditions.

Abstract: Efficient tuning of building climate controllers to optimize occupant utility is essential for ensuring overall comfort and satisfaction. However, this is a challenging task since the latent utility are difficult to measure directly. Time-varying contextual factors, such as outdoor temperature, further complicate the problem. To address these challenges, we propose a contextual preferential Bayesian optimization algorithm that leverages binary preference feedback together with contextual information to enable efficient real-time controller tuning. We validate the approach by tuning an economic MPC controller on BOPTEST, a high-fidelity building simulation platform. Over a two-month simulation period, our method outperforms the baseline controller and achieves an improvement of up to 23% in utility. Moreover, for different occupant types, we demonstrate that the algorithm automatically adapts to individual preferences, enabling personalized controller tuning.

</details>


### [22] [Instantaneous Complex Phase and Frequency: Conceptual Clarification and Equivalence between Formulations](https://arxiv.org/abs/2512.09574)
*César García-Veloso,Mario Paolone,Federico Milano*

Main category: eess.SY

TL;DR: 本文综述了瞬时复数相位和频率的两种定义及其在特定假设下的一致性，提出统一的符号和术语以避免混淆。


<details>
  <summary>Details</summary>
Motivation: 澄清现有基于分析信号与空向量的两种基本定义及其在不同前提下的等效性，以减少歧义。

Method: 系统对比两种定义及其前提，给出二者之间的关系，并在适用条件下证明等价性；随后提出统一的记号与术语。

Result: 明确了在特定假设下两种定义的等价性，并提出了统一的记号和命名以避免混乱。

Conclusion: 建议采用统一的符号系统以避免在研究中对瞬时相位与频率的混淆，并强调在给定条件下的等价性。

Abstract: This letter seeks to clarify the different existing definitions of both instantaneous complex phase and frequency as well as their equivalence when specific hypotheses hold. To achieve this, the two fundamental definitions, i.e., those based on either the use of (i) analytic signals or (ii) space vectors, together with the premises used for their formulation, are presented and their relationship shown. Lastly, an unified notation and terminology to avoid confusion is proposed.

</details>


### [23] [Real-Time Non-Smooth MPC for Switching Systems: Application to a Three-Tank Process](https://arxiv.org/abs/2512.09611)
*Hendrik Alsmeier,Felix Häusser,Andreas Knödler,Armin Nurkanović,Anton Pozharskiy,Moritz Diehl,Rolf Findeisen*

Main category: eess.SY

TL;DR: 提出一种基于非光滑模型预测控制的实时实现框架，采用Filippov系统建模、有限元离散、切换检测，将问题转化为带互补约束的有限维最优控制，通过对光滑化的同胚（同胚）方法求解，且通过额外模式避免非 Lipschitz 点，实现无混合整数的实时控制，且在三罐实验中有效处理切换、实现跨参考跟踪、边界处理和约束满足。


<details>
  <summary>Details</summary>
Motivation: 克服非平滑切换系统的实时MPC困难，避免混合整数形式带来的计算复杂度；在物理系统（三罐）中实现可实时执行和鲁棒的切换处理。

Method: 将Filippov系统用于非光滑切换建模；使用有限元对时间离散化，对切换事件进行检测；将问题建模为带互补约束的有限维最优控制问题；通过对光滑非线性程序的同胚/路径跟踪求解；引入额外模式以避免非 Lipschitz点与未定义函数值；基于NOSNOC框架实现完整控制器。

Result: 在硬件上实现可在实时范围内的跟踪，能够一致地跟踪参考变化、正确处理边界并实现约束满足；相对于传统混合整数方法，避免离散化导致的组合爆炸；对模型失配的影响研究表明跟踪性能与计算时间仍维持在选定采样时间的实时性内。

Conclusion: 该工作将非光滑系统的MPC问题有效地转化为无混合整数的可实时求解问题，验证了在物理三罐系统中的可行性与鲁棒性，为更广泛的非光滑切换系统实时控制提供了框架与实践路径。

Abstract: Real-time model predictive control of non-smooth switching systems remains challenging due to discontinuities and the presence of discrete modes, which complicate numerical integration and optimization. This paper presents a real-time feasible non-smooth model predictive control scheme for a physical three-tank process, implemented without mixed-integer formulations. The approach combines Filippov system modeling with finite elements and switch detection for time discretization, leading to a finite-dimensional optimal control problem formulated as a mathematical program with complementarity constraints. The mathematical program is solved via a homotopy of smooth nonlinear programs. We introduce modeling adjustments that make the three-tank dynamics numerically tractable, including additional modes to avoid non-Lipschitz points and undefined function values. Hardware experiments demonstrate efficient handling of switching events, mode-consistent tracking across reference changes, correct boundary handling, and constraint satisfaction. Furthermore, we investigate the impact of model mismatch and show that the tracking performance and computation times remain within real-time limits for the chosen sampling time. The complete controller is implemented using the non-smooth optimal control framework NOSNOC

</details>


### [24] [RIS-Assisted Coordinated Multi-Point ISAC for Low-Altitude Sensing Coverage](https://arxiv.org/abs/2512.09625)
*Ying Zhang,Zeqi Hao,Tingting Zhang*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The low-altitude economy (LAE) has emerged and developed in various fields, which has gained considerable interest. To ensure the security of LAE, it is essential to establish a proper sensing coverage scheme for monitoring the unauthorized targets. Introducing integrated sensing and communication (ISAC) into cellular networks is a promising solution that enables coordinated multiple base stations (BSs) to significantly enhance sensing performance and extend coverage. Meanwhile, deploying a reconfigurable intelligent surface (RIS) can mitigate signal blockages between BSs and low-altitude targets in urban areas. Therefore, this paper focuses on the low-altitude sensing coverage problem in RIS-assisted coordinated multi-point ISAC networks, where a RIS is employed to enable multiple BSs to sense a prescribed region while serving multiple communication users. A joint beamforming and phase shifts design is proposed to minimize the total transmit power while guaranteeing sensing signal-to-noise ratio and communication spectral efficiency. To tackle this non-convex optimization problem, an efficient algorithm is proposed by using the alternating optimization and semi-definite relaxation techniques. Numerical results demonstrate the superiority of our proposed scheme over the baseline schemes.

</details>


### [25] [Adaptive Optimal Control for Avatar-Guided Motor Rehabilitation in Virtual Reality](https://arxiv.org/abs/2512.09667)
*Francesco De Lellis,Maria Lombardi,Egidio De Benedetto,Pasquale Arpaia,Mario di Bernardo*

Main category: eess.SY

TL;DR: 提出一个基于控制理论的虚拟现实自主 avatar 指导康复框架，面向居家中风患者，通过非线性人机耦合控制实现实时自适应的姿态引导，结合 Hogan 最小 jerk 模型的多目标最优控制与基于平滑度的“能力指数”以动态调整增益，已通过仿真与初步试验验证，具备可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 康复治疗存在可及性不足、成本高、护理连续性差等挑战，超过一半患者无法定期到诊治疗，亟需居家、可监控的个性化康复系统。

Method: 在虚拟现实环境中通过非线性的人机循环控制实现avatar的实时自适应。采用多目标最优控制在跟随患者运动与引导理想运动轨迹之间取得平衡；以 Hogan 的最小 jerk 模型定义理想运动框架；构建数据驱动的“能力指数”并利用平滑度指标动态调整控制增益；系统由临床人员远程监控，支持居家治疗。

Result: 通过数值仿真和初步试验验证其可行性，显示出自适应、参与性强且可扩展的居家康复潜力，且控制理论具有较好的可解释性。

Conclusion: 该框架展示了将可解释的控制理论与自适应的个性化康复结合的潜力，能够推动远程康复的实现，但需要进一步在大规模临床试验中评估有效性与安全性，并完善数据驱动指标与阈值设定。

Abstract: A control-theoretic framework for autonomous avatar-guided rehabilitation in virtual reality, based on interpretable, adaptive motor guidance through optimal control, is presented. The framework faces critical challenges in motor rehabilitation due to accessibility, cost, and continuity of care, with over 50% of patients inability to attend regular clinic sessions. The system enables post-stroke patients to undergo personalized therapy in immersive virtual reality at home, while being monitored by clinicians. The core is a nonlinear, human-in-the-loop control strategy, where the avatar adapts in real time to the patient's performance. Balance between following the patient's movements and guiding them to ideal kinematic profiles based on the Hogan minimum-jerk model is achieved through multi-objective optimal control. A data-driven "ability index" uses smoothness metrics to dynamically adjust control gains according to the patient's progress. The system was validated through simulations and preliminary trials, and shows potential for delivering adaptive, engaging and scalable remote physiotherapy guided by interpretable control-theoretic principles.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [26] [New Algorithm for Structured OFDM Channel Estimation using Subgroup Duality](https://arxiv.org/abs/2512.08947)
*Demerson N. Gonçalves,João T. Dias*

Main category: eess.SP

TL;DR: A group-theoretic approach to OFDM channel estimation by modeling subcarriers as Z_N; using the annihilator H^⊥ to constrain the impulse response in the dual domain; proposes a low-complexity energy-concentration estimator; shows gains over LS and LMMSE with competitive performance and lower complexity.


<details>
  <summary>Details</summary>
Motivation: Address computational burden and interpretability in structured channel estimation; leverage cyclic group structure to constrain estimates and improve accuracy.

Method: Model subcarriers as a cyclic group Z_N; select a nulling subgroup H; analyze the channel in terms of its annihilator H^⊥ in the dual domain; develop a low-complexity estimator that detects structure by scanning energy concentration across candidate annihilators; compare against LS and LMMSE baselines.

Result: Simulations show consistent gains in mean squared error, bit error rate, and throughput; the proposed method achieves competitive performance with substantially lower complexity and preserved interpretability compared to baselines.

Conclusion: Group-theoretic structure provides a principled framework for OFDM channel estimation; energy-concentration based detection of annihilators enables efficient, interpretable estimation with practical gains.

Abstract: This paper presents a group-theoretic framework for structured channel estimation in Orthogonal Frequency Division Multiplexing (OFDM). By modeling subcarriers as the cyclic group \(\mathbb{Z}_N\), we show that nulling a subgroup \(H \subseteq \mathbb{Z}_N\) constrains the channel impulse response to its annihilator \(H^\perp\) in the dual domain. A low-complexity estimator is proposed that detects such structure by evaluating energy concentration across candidate annihilators. Simulations demonstrate consistent gains in mean squared error, bit error rate, and throughput compared with least-squares and linear minimum mean square error baselines, achieving competitive performance with substantially lower complexity and preserved interpretability.

</details>


### [27] [A New Particle Filter for Target Tracking in MIMO OFDM Integrated Sensing and Communications](https://arxiv.org/abs/2512.09098)
*Shixiong Wang,Wei Dai,Geoffrey Ye Li*

Main category: eess.SP

TL;DR: 提出一种基于优化的粒子滤波框架，避免显式似然模型，显著降低计算量与跟踪误差，并在MIMO-OFDM ISAC场景中展现潜力。


<details>
  <summary>Details</summary>
Motivation: MIMO脉冲-Doppler 雷达的目标跟踪面临三大难题：1) 原始雷达数据缺乏可靠的似然模型；2) 将 nuisance 参数（如复杂路径增益）并入状态向量引发的计算与统计复杂性；3) 从快照中提取范围、多普勒、角度等测量的高计算成本。

Method: 提出一种新型粒子滤波框架，以优化导向的贝叶斯解读为基础，使用定制化成本函数来评估每个假设状态，而非依赖显式似然关系。

Result: 与现有方案相比，框架在运行时间与跟踪误差两方面均实现显著降低。

Conclusion: 将该粒子滤波框架应用于MIMO-OFDM系统以实现感知与通信的融合（ISAC），实验表明在宽带、较长在目标时间和大天线孔径条件下，MIMO-OFDM+脉冲-多普勒处理对ISAC具有较大潜力。

Abstract: Particle filtering for target tracking using multi-input multi-output (MIMO) pulse-Doppler radars faces three long-standing obstacles: a) the absence of reliable likelihood models for raw radar data; b) the computational and statistical complications that arise when nuisance parameters (e.g., complex path gains) are augmented into state vectors; and c) the prohibitive computational burden of extracting noisy measurements of range, Doppler, and angles from snapshots. Motivated by an optimization-centric interpretation of Bayes' rule, this article addresses these challenges by proposing a new particle filtering framework that evaluates each hypothesized state using a tailored cost function, rather than relying on an explicit likelihood relation. The framework yields substantial reductions in both running time and tracking error compared to existing schemes. In addition, we examine the implementation of the proposed particle filter in MIMO orthogonal frequency-division multiplexing (OFDM) systems, aiming to equip modern communication infrastructure with integrated sensing and communications (ISAC) capabilities. Experiments suggest that MIMO-OFDM with pulse-Doppler processing holds considerable promise for ISAC, particularly when wide bandwidth, extended on-target time, and large antenna aperture are utilized.

</details>


### [28] [A Hybrid Residue Floating Numerical Architecture for High Precision Arithmetic on FPGAs](https://arxiv.org/abs/2512.09155)
*Mostafa Darvishi*

Main category: eess.SP

TL;DR: 提出一种混合残差浮点数架构HRFNA，将携带无损（carry-free）残差通道与轻量级浮点尺度因子相结合，在FPGA上实现模块乘法、指数管理与混合重建的高效实现，提升吞吐并降低逻辑门数量。


<details>
  <summary>Details</summary>
Motivation: 在FPGA上，浮点运算成本高主要来自宽数据通路与归一化逻辑。需要一种在保持动态范围的前提下降低代价的表示方法，以提高吞吐并降低资源消耗。

Method: 提出HRFNA的完整数学框架，推导有界误差归一化规则，并给出适用于FPGA的模块化乘法、指数管理与混合重构的微架构。对Xilinx ZCU104实现，进行Vitis仿真、RTL综合与片上ILA追踪以验证周期级正确性。

Result: 实现相比IEEE 754单精度基线，在ZCU104上获得≈2.1×吞吐提升和38–52%的查表逻辑（LUT）资源降低，同时在长迭代序列中保持数值稳定性，证明HRFNA在现代FPGA设备上具备高效可扩展的替代浮点计算的潜力。

Conclusion: HRFNA提供一种在FPGA上高效且可扩展的浮点替代方案，通过混合残差通道和缩放机制实现较低成本的动态范围表达，并在实际硬件实现中达到显著性能与资源优势。

Abstract: Floating point arithmetic remains expensive on FPGA platforms due to wide datapaths and normalization logic, motivating alternative representations that preserve dynamic range at lower cost. This work introduces the Hybrid Residue Floating Numerical Architecture (HRFNA), a unified arithmetic system that combines carry free residue channels with a lightweight floating point scaling factor. We develop the full mathematical framework, derive bounded error normalization rules, and present FPGA optimized microarchitectures for modular multiplication, exponent management, and hybrid reconstruction. HRFNA is implemented on a Xilinx ZCU104, with Vitis simulation, RTL synthesis, and on chip ILA traces confirming cycle accurate correctness. The architecture achieves over 2.1 times throughput improvement and 38-52 percent LUT reduction compared to IEEE 754 single precision baselines while maintaining numerical stability across long iterative sequences. These results demonstrate that HRFNA offers an efficient and scalable alternative to floating point computation on modern FPGA devices.

</details>


### [29] [Secure Wireless Communication Using Distributed Coherent Transmission and Spatial Signal Decomposition](https://arxiv.org/abs/2512.09194)
*Anton Schlegel,Jason M/ Merlo,Samuel Wagner,John B. Lancaster,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: 两发分布式相控阵通过将符号分解为两个伪随机向量并在目标区域方向进行相干叠加，在空间上实现安全的信息传输；实验结果表明在广义方向SER低、在其他位置显著较高，相较传统波束成形可更好地控制空间泄漏。


<details>
  <summary>Details</summary>
Motivation: 提升无线物理层安全性，通过分布式协同传输和波束赋形，将信息能量聚焦在特定空间区域，减少对未授权接收者的可解码能力；提出在两发分布式阵列中实现符号分解与相干叠加的新思路。

Method: 使用两元素分布式相控阵；每个发射机发送符号的一个分量，该符号分解为两个伪随机信号向量；两向量在目标处相干叠加以恢复原始符号；通过分布式波束成形将能量聚焦在目标区域；在50λ阵列、工作频率3 GHz下进行仿真与实测，评估二维空间的符号错误率SER，并与传统单数据发送波束成形进行对比。

Result: 在目标区域实现低SER，信息可解码；而在该区域之外SER显著提高，呈现空间安全区域。实验结果显示：提出方法在广侧（broadside）SER为0.0082，其他位置SER>0.25；相比之下，传统波束成形在所有测量位置SER为0。

Conclusion: 证明分布式相控阵实现的符号分解与相干聚合可以实现空间上受限的安全信息传输，具备实验可行性并优于传统单数据波束成形在抑制空间泄漏方面的性能。

Abstract: We present a new approach to secure wireless communications using coherent distributed transmission of signals that are spatially decomposed between a two-element distributed antenna array. High-accuracy distributed coordination of microwave wireless systems supports the ability to transmit different parts of a signal from separate transmitters such that they combine coherently at a designated destination. In this paper we explore this concept using a two-element coherent distributed phased array where each of the two transmitters sends a separate component of a communication signal where each symbol is decomposed into a sum of two pseudo-random signal vectors, the coherent summation of which yields the intended symbol. By directing the transmission to an intended receiver using distributed beamforming, the summation of the two vector components is largely confined to a spatial region at the destination receiver. We implement the technique in a 50 wavelength array operating at 3 GHz. We evaluate the symbol error ratio. (SER) in two-dimensional space through simulation and measurement, showing the approach yields a spatially confined secure region where the information is recoverable(i.e., the received signal has low SER), and outside of which the information is unrecoverable (high SER). The proposed system is also compared against a traditional beamforming system where each node sends the same data. We validate experimentally that our approach achieves a low SER of 0.0082 at broadside and a SER above 0.25 at all other locations compared to a traditional beamforming approach that achieves a SER of 0 at all locations measured.

</details>


### [30] [Joint Channel Estimation and Localization in Pinching-Antenna OFDM Systems: The Blessing of Multipath](https://arxiv.org/abs/2512.09432)
*Min Liu,Yue Xiao,Shuaixin Yang,Gang Wu,Xianfu Lei,Wei Xiang*

Main category: eess.SP

TL;DR: 提出一种面向上行Pinching-antenna系统的多用户OFDM联合定位与通道估计框架。在多径环境下，利用CP将时域分散转化为频域叠加的正弦序列，并通过混合推断实现高精度估计。


<details>
  <summary>Details</summary>
Motivation: PASS能够灵活重构大规模无线通道并具备定位潜力，但在多径环境下的联合定位与通道估计仍面临干扰、时延提取和硬件开销等挑战。

Method: 建立多用户OFDM上行PASS模型；以期望传播（EP）减小多用户干扰；使用正交匹配追踪（OMP）或混合贝叶斯传播-变分推断（BP-VI）提取路径时延；结合几何信息进行迭代定位并将估计的通道矩阵反馈给EP；推导并比较CRLB。

Result: 仿真结果显示所提框架接近CRLB，定位性能可与协作多基站系统相当，同时显著降低RF链数量与硬件复杂度。

Conclusion: 所提出的混合推断框架实现了高精度联合定位与通道估计，并在降低硬件成本的同时逼近理论极限。

Abstract: Pinching-antenna systems (PASS) have recently attracted considerable attention owing to their capability of flexibly reconfiguring large-scale wireless channels. Motivated by this potential, we investigate the issue of joint localization and channel estimation for the uplink PASS in the presence of multipath dispersion. To this end, a comprehensive multi-user orthogonal frequency division multiplexing (OFDM) uplink PASS model is first established, where the use of a cyclic prefix (CP) enables the multipath-induced time-domain dispersion to be transformed into a set of superimposed sinusoids in the frequency domain. Building upon this model, we propose a hybrid inference framework capable of accurately estimating both channel parameters and user locations. Specifically, expectation propagation is first employed to mitigate multi-user interference, while the path delays are then extracted from noisy channel state information using an orthogonal matching pursuit (OMP) based approach, or a hybrid belief propagation-variational inference (BP-VI) algorithm. Then the estimated delays are subsequently refined through the embedded geometric information via an iterative localization procedure, wherein the estimated channel matrices are recursively fed back to EP. Furthermore, the Cramer-Rao lower bound (CRLB) is derived to characterize the fundamental estimation limits. Finally, simulation results validate that our proposed framework closely approaches the CRLB, with performance comparable to cooperative multi-base station localization, with significantly fewer RF chains and reduced hardware complexity.

</details>


### [31] [Analytical and DNN-Aided Performance Evaluation of IRS-Assisted THz Communication Systems](https://arxiv.org/abs/2512.09515)
*Soumendu Das,Nagendra Kumar,Dharmendra Dixit*

Main category: eess.SP

TL;DR: 提出了一个IRS辅助的THz通信系统分析框架，给出端到端链路的OP、ACC、ASER的闭式解，以及对矩形QAM和六角QAM的误码率分析；考虑相位误差与量化误差的影响；采用Laguerre级数展开对IRS信道进行近似，并将源端-IRS与IRS-目的端信道建模为独立同分布的α-μ衰落；并引入基于深度学习的性能预测框架，进行高SNR渐近分析与蒙特卡洛验证。


<details>
  <summary>Details</summary>
Motivation: 在高频段（THz）通信中，直接链路易受强衰减与对准误差影响，IRS可在缺失直接路径时提供无源连接。对IRS链路的精确建模与端到端性能预测对系统设计至关重要。

Method: 将IRS信道用Laguerre级数展开进行近似；源端-IRS与IRS-目的端信道建模为独立同分布的α-μ衰落；推导端到端的OP、ACC、ASER的闭式表达，覆盖矩形QAM与六角QAM；考虑随机相位合成与相位量化误差的影响；构建基于深度学习的性能预测框架以快速评估指标；在高SNR条件下进行渐近分析，给出编码增益与分集阶数；通过蒙特卡洛仿真验证理论结果。

Result: 获得端到端链路的闭式OP、ACC、ASER表达式，覆盖RQAM和HQAM的误差概率；给出高SNR极限下的编码增益和分集阶数；定量分析相位误差与量化误差对性能的影响；提出的DNN预测框架实现快速评估，理论结果与仿真高度一致。

Conclusion: 本文为IRS辅助THz通信系统提供了一整套理论分析、误差分析、及快速评估工具，展示了信道建模、相位误差与量化误差对系统性能的影响，并通过深度学习框架实现高效性能预测，且高SNR分析为系统设计提供指导。

Abstract: This paper investigates the performance of an intelligent reflecting surface (IRS)-assisted terahertz (THz) communication system, where the IRS facilitates connectivity between the source and destination nodes in the absence of a direct transmission path. The source-IRS and IRS-destination links are subject to various challenges, including atmospheric attenuation, asymmetric $α$-$μ$ distributed small-scale fading, and beam misalignment-induced pointing errors. The IRS link is characterized using the Laguerre series expansion (LSE) approximation, while both the source-IRS and IRS-destination channels are modeled as independent and identically distributed (i.i.d.) $α$-$μ$ fading channels. Furthermore, closed-form analytical expressions are derived for the outage probability (OP), average channel capacity (ACC), and average symbol error rate (ASER) for rectangular QAM (RQAM) and hexagonal QAM (HQAM) schemes over the end-to-end (e2e) link. The impact of random co-phasing and phase quantization errors are also examined. In addition to the theoretical analysis, deep neural network-based frameworks are developed to predict key performance metrics, facilitating fast and accurate system evaluation without computationally intensive analytical computations. Moreover, the asymptotic analysis in the high-signal-to-noise ratio (SNR) regime yields closed-form expressions for coding gain and diversity order, providing further insights into performance trends. Finally, Monte Carlo simulations validate the theoretical formulations and present a comprehensive assessment of system behavior under practical conditions.

</details>


### [32] [CKM-Enabled Joint Spatial-Doppler Domain Clutter Suppression for Low-Altitude UAV ISAC](https://arxiv.org/abs/2512.09560)
*Zihan Xu,Zhiwen Zhou,Di Wu,Xiaoli Xu,Yong Zeng*

Main category: eess.SP

TL;DR: 提出一种基于克布勒角度图(CLAM)的低空场景的杂波抑制方法，结合OFDM ISAC系统。通过CLAM建立区域性的主要杂波角度数据库，在目标检测和参数估计前抑制杂波，且提出两步式CLAM在时域-空域的联合抑制，以提升对慢速低空UAV的探测性能。


<details>
  <summary>Details</summary>
Motivation: 低空环境（城市/山区）的动态杂波与慢速目标使得传统基于多普勒差分或信号强度的杂波抑制方法效果不佳。需要区域性、场景适配的杂波知识来提升成像/感知性能。

Method: 引入CLAM（杂波角度图）作为区域性、场景特定的杂波知识库，记录覆盖区域的主要杂波角度。利用CLAM在感测前对杂波分量进行抑制；在目标和杂波方向接近时，提出两步式CLAM联动的时空-多普勒域杂波抑制算法，以增强鲁棒性。

Result: 通过仿真表明：所提方法能有效抑制杂波，显著提升对慢速低空UAV目标的探测和参数估计精度。

Conclusion: 基于CLAM的杂波抑制为ISAC系统提供了区域性、场景特定的前瞻性知识，提升低-altitude环境下对慢速目标的感知可靠性，尤其在动态杂波条件下优于传统方法。

Abstract: The rapid development of low-altitude economy has placed higher demands on the sensing of small-sized unmanned aerial vehicle (UAV) targets. However, the complex and dynamic low-altitude environment, like the urban and mountainous areas, makes clutter a significant factor affecting the sensing performance. Traditional clutter suppression methods based on Doppler difference or signal strength are inadequate for scenarios with dynamic clutter and slow-moving targets like low-altitude UAVs. In this paper, motivated by the concept of channel knowledge map (CKM), we propose a novel clutter suppression technique for orthogonal frequency division multiplexing (OFDM) integrated sensing and communication (ISAC) system, by leveraging a new type of CKM named clutter angle map (CLAM). CLAM is a site-specific database, containing location-specific primary clutter angles for the coverage area of the ISAC base station (BS). With CLAM, the sensing signal components corresponding to the clutter environment can be effectively removed before target detection and parameter estimation, which greatly enhances the sensing performance. Besides, to take into account the scenarios when the targets and clutters are in close directions so that pure CLAM-based spatial domain clutter suppression is no longer effective, we further propose a two-step CLAM-enabled joint spatial-Doppler domain clutter suppression algorithm. Simulation results demonstrate that the proposed technique effectively suppresses clutter and enhances target sensing performance, achieving accurate parameter estimation for sensing slow-moving low-altitude UAV targets.

</details>


### [33] [On the Ambiguity Function of OFDM-based ISAC Signals Under Non-Ideal Power Amplifiers](https://arxiv.org/abs/2512.09803)
*Eya Gourar,Yahia Medjahdi,Laurent Clavier,Abdul Karim Gizzini,Patrick Sondi*

Main category: eess.SP

TL;DR: PA非线性对OFDM ISAC中的感知性能带来上限，PSK与QAM在非线性下均受损，失真扭曲重塑AF并降低检测概率。


<details>
  <summary>Details</summary>
Motivation: 研究功率放大器非线性对综合感知与通信(ISAC)中感知与通信性能的影响，比较常模PSK与恒模QAM在OFDM框架下的S&C性能，揭示高PAPR对雷达感知与通信的冲突。

Method: 引入信号失真比SDR，结合理论分析的辐射/干扰对AF的影响，重点分析在零多普勒旁瓣和零延迟旁瓣处的失真表现；通过仿真验证。

Result: PA失真在两种调制下均对感知性能设定了上界，扭曲重塑AF、降低检测概率，削弱单峰/统一幅度信号的感知优势，并使OFDM在非均匀包络条件下的感知表现下降。

Conclusion: 需在ISAC系统设计中把PA非线性及PAPR等因素纳入权衡，考虑采用鲁棒调制/信号设计或放大器线性度优化，以缓解失真对感知任务的影响。

Abstract: Integrated Sensing and Communications (ISAC) has garnered significant attention as a promising technology for next-generation wireless and vehicular communications. Among candidate waveforms, Orthogonal Frequency Division Multiplexing (OFDM) has been extensively investigated over the past decade for its robustness against frequency-selective fading and its favorable ranging performance. However, the waveform's sensing and communication (S&C) performance depends strongly on the modulation scheme; while variable-amplitude constellations such as quadrature amplitude (QAM) are more efficient for communication, constant-modulus modulations such as phase shift keying (PSK) are more suitable for sensing. Yet, it remains unclear whether these findings persist under power amplifier (PA) nonlinearity. Because OFDM signals exhibit a high peak-to-average power ratio (PAPR), they require highly linear PAs to avoid distortion, which conflicts with radar requirements, where high transmit power is always beneficial for sensing. In this work, we analyze the effect of PA-induced distortions on the sensing task for PSK and QAM constellations. By introducing the Signal-to-Distortion Ratio (SDR), we examine the extent of the distortion limitation on the ranging task. We complement simulation results with a theoretical characterization of the ambiguity function (AF), thereby explicitly demonstrating how distortion artifacts manifest in the zero-Doppler sidelobes (i.e, ranging sidelobes) and the zero-delay sidelobes. Simulations show that PA distortions impose a palpable performance ceiling for both constellations, reshape the AF, and reduce detection probability, diminishing the theoretical advantage of unimodular signaling and further compromising the OFDM sensing performance with non-uniform envelope signals.

</details>


### [34] [Energy-Efficient Federated Learning with Relay-Assisted Aggregation in IIoT Networks](https://arxiv.org/abs/2512.09827)
*Hamid Reza Hashempour,Mostafa Nozari,Gilberto Berardinelli,Yanjiao Li,Jie Zhang,Hien Quoc Ngo,Shashi Raj Pandey*

Main category: eess.SP

TL;DR: 引入中继辅助的联邦学习框架，利用局部聚合减少传输开销并提升能效与收敛速度，结合分解式能耗优化、SN分组、中继选择与SPCA求解，且扩展至ICSI，显著提升收敛性与能效。


<details>
  <summary>Details</summary>
Motivation: 在工业物联网环境中，存在严格的时延与能耗约束，现有联邦学习难以在低功耗和低延迟条件下实现鲁棒的全局模型收敛，因此需要结合通信协作与能耗优化的解决方案。

Method: 提出一个能效驱动的传输框架，允许SN通过直接上传或通过解码转发的中继进行局部模型更新。中继在转发前执行部分聚合以减少通信开销。将问题分解为计算能耗和通信能耗的子问题；对设备进行单跳/两跳分组，并进行中继选择；通过优化发射功率最大化EE，并采用SPCA实现参数的联合配置；并扩展至在不完全信道信息下的情形。

Result: 仿真显示该框架显著提升收敛速度，且将轮次延迟约束满足的SN数量最大化以提升鲁棒性；单跳情形下的 outage从10^-2降至10^-6；在能耗方面，SPCA相较未聚合合作至少节省2倍、相较单跳最高节省6倍。

Conclusion: 通过中继协作与部分聚合实现了在IIoT约束下的高效且鲁棒的FL，SPCA为联合优化的关键工具，ICSI扩展提升了在现实信道不确定性下的适用性与性能。

Abstract: This paper presents an energy-efficient transmission framework for federated learning (FL) in industrial Internet of Things (IIoT) environments with strict latency and energy constraints. Machinery subnetworks (SNs) collaboratively train a global model by uploading local updates to an edge server (ES), either directly or via neighboring SNs acting as decode-and-forward relays. To enhance communication efficiency, relays perform partial aggregation before forwarding the models to the ES, significantly reducing overhead and training latency. We analyze the convergence behavior of this relay-assisted FL scheme. To address the inherent energy efficiency (EE) challenges, we decompose the original non-convex optimization problem into sub-problems addressing computation and communication energy separately. An SN grouping algorithm categorizes devices into single-hop and two-hop transmitters based on latency minimization, followed by a relay selection mechanism. To improve FL reliability, we further maximize the number of SNs that meet the roundwise delay constraint, promoting broader participation and improved convergence stability under practical IIoT data distributions. Transmit power levels are then optimized to maximize EE, and a sequential parametric convex approximation (SPCA) method is proposed for joint configuration of system parameters. We further extend the EE formulation to the imperfect channel state information (ICSI). Simulation results demonstrate that the proposed framework significantly enhances convergence speed, reduces outage probability from 10-2 in single-hop to 10-6 and achieves substantial energy savings, with the SPCA approach reducing energy consumption by at least 2x compared to unaggregated cooperation and up to 6x over single-hop transmission.

</details>


### [35] [A Speculative GLRT-Backed Approach for Adversarial Resilience on Deep Learning-Based Array Processing](https://arxiv.org/abs/2512.09893)
*Nian-Cin Wang,Rajeev Sahay*

Main category: eess.SP

TL;DR: 提出一种对抗鲁棒的猜测性阵列处理框架：用低延迟的深度学习分类器进行快速推断，并以理论上有据可查的GLRT验证器进行后续确认；利用接收阵列的二阶统计量，在Lp范数扰动下具有空间不变量性，从而实现对对抗扰动的鲁棒性与对DL预测的理论性验证；实证结果表明在多种Lp界、扰动设计和扰动强度下，该框架相较多种基线具有更优性能。


<details>
  <summary>Details</summary>
Motivation: 在阵列处理任务中需要低延迟的信号检测与DoA估计；深度学习虽具高推理速度，但缺乏统计保证且对对抗扰动敏感；而GLRT在统计学上有保证但计算成本高。两者结合以实现速度与可靠性的折中。

Method: 提出一个猜测性阵列处理框架：以低延迟的DL分类器进行快速推断，并通过理论支撑的GLRT验证器对DL输出进行后续确认。核心在于利用接收阵列的二阶统计量（协方差等）作为GLRT的输入，这些统计量对Lp范数扰动具有空间不变性，能提供对DL预测的对抗鲁棒性与有效验证。通过对多种Lp界、扰动设计及扰动强度的广泛实验来验证理论结论。

Result: 实验结果表明，在多种Lp bounds、扰动设计和扰动幅度下，所提出的对抗鲁棒猜测性阵列处理框架相较于多种前沿基线具有更优的检测/DoA推断性能，验证了理论分析的正确性与实用性。

Conclusion: 该框架实现了低延迟推断与理论保障的结合，为在对抗性无线环境中的阵列处理任务提供更可靠的解决方案；二阶统计量的空间不变性为DL预测提供了理论支撑的鲁棒性，并为未来在更复杂的对抗场景中的扩展奠定基础。

Abstract: Classical array processing methods such as the generalized likelihood ratio test (GLRT) provide statistically grounded solutions for signal detection and direction-of-arrival (DoA) estimation, but their high computational cost limits their use in low-latency settings. Deep learning (DL) has recently emerged as an efficient alternative, offering fast inference for array processing tasks. However, DL models lack statistical guarantees and, moreover, are highly susceptible to adversarial perturbations, raising fundamental concerns about their reliability in adversarial wireless environments. To address these challenges, we propose an adversarially resilient speculative array processing framework that consists of a low-latency DL classifier backed by a theoretically-grounded GLRT validator, where DL is used for fast speculative inference and later confirmed with the GLRT. We show that second order statistics of the received array, which the GLRT operates on, are spatially invariant to L-p bounded adversarial perturbations, providing adversarial robustness and theoretically-grounded validation of DL predictions. Empirical evaluations under multiple L-p bounds, perturbation designs, and perturbation magnitudes corroborate our theoretical findings, demonstrating the superior performance of our proposed framework in comparison to multiple state-of-the-art baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization](https://arxiv.org/abs/2512.08950)
*Aseel Rawashdeh*

Main category: cs.LG

TL;DR: 在小规模离散环境中，基于贝叶斯的 ATM 能以更低方差实现等价或更好的标量回报；但在更大、更复杂的 mHealth 场景中，标准和贝叶斯 ATM 均表现不佳，揭示 ATM 建模假设与真实领域的错配。结果指向：在低数据情境下不确定性建模有价值；需要新的 RL 算法来显式建模因果结构、连续状态以及观测成本约束下的延迟反馈。


<details>
  <summary>Details</summary>
Motivation: 在移动健康（mHealth）干预中，需在提高干预效果与降低用户负担之间取得权衡。状态测量成本高且往往不可得，且在稀疏/嘈杂的数据中，基于时序差分的 Q 学习易引发不稳定。因而需要引入不确定性意识的学习方法以提升数据不足情形下的稳定性与样本效率。

Method: 提出对 ATM 的贝叶斯扩展：用卡尔曼滤波风格的贝叶斯更新替代标准的 TD‑Q 学习，维持对 Q 值的不确定性估计，并在 Action‑Contingent Noiselessly Observable MDP（ACNO‑MDP）框架中实现控制动作与测量动作的解耦，提升学习的鲁棒性与样本效率；在 toy 环境与临床动机的测试床上进行评估。

Result: 在小型离散环境中，贝叶斯 ATM 达到与原方法相当甚至更好的标量化回报，同时方差显著降低、策略行为更稳定；但在更大、复杂的 mHealth 设置中，标准与贝叶斯 ATM 均表现不佳，表明 ATM 的建模假设无法有效覆盖真实领域的结构性挑战。结论强调不确定性意识方法在低数据情境的价值，同时指出需要新的 RL 算法来显式建模因果结构、连续状态以及在观测成本约束下的延迟反馈。

Conclusion: 不确定性感知的学习方法对低数据情境有意义，但现有 ATM 的建模假设在真实的 mHealth 场景中不足以应对结构性挑战，需要发展能够处理因果结构、连续状态和延迟反馈的更广泛的 RL 框架，特别是在存在观测成本约束时。

Abstract: Reinforcement learning in mobile health (mHealth) interventions requires balancing intervention efficacy with user burden, particularly when state measurements (for example, user surveys or feedback) are costly yet essential. The Act-Then-Measure (ATM) heuristic addresses this challenge by decoupling control and measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework. However, the standard ATM algorithm relies on a temporal-difference-inspired Q-learning method, which is prone to instability in sparse and noisy environments. In this work, we propose a Bayesian extension to ATM that replaces standard Q-learning with a Kalman filter-style Bayesian update, maintaining uncertainty-aware estimates of Q-values and enabling more stable and sample-efficient learning. We evaluate our method in both toy environments and clinically motivated testbeds. In small, tabular environments, Bayesian ATM achieves comparable or improved scalarized returns with substantially lower variance and more stable policy behavior. In contrast, in larger and more complex mHealth settings, both the standard and Bayesian ATM variants perform poorly, suggesting a mismatch between ATM's modeling assumptions and the structural challenges of real-world mHealth domains. These findings highlight the value of uncertainty-aware methods in low-data settings while underscoring the need for new RL algorithms that explicitly model causal structure, continuous states, and delayed feedback under observation cost constraints.

</details>


### [37] [Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis](https://arxiv.org/abs/2512.08952)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.

</details>


### [38] [An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings](https://arxiv.org/abs/2512.08954)
*Yuhao Xu,Jiaying Lu,Sirui Ding,Defu Cao,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: 在心电图分析中，基于 foundation 模型的方法表现突出，达到80%的顶尖性能，提供了可复现的基准数据与代码，显示其潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 解决 ECG 分析中对领域知识的强依赖的瓶颈，评估自监督与 foundation 模型在心电图分析中的有效性。

Method: 对语言模型/通用时序模型/ECG foundation 模型与传统时序深度学习模型在 ECG 任务上的性能进行基准评估，使用公开数据集并比较不同模型，提供深入分析。

Result: general time-series/ECG foundation 模型达到最高80% 的性能率，表明其在 ECG 分析中的有效性；提供全面的实验结果与见解。

Conclusion: foundation 模型在生理波形分析方面具有潜在价值，但也存在局限性，需要进一步研究以提升领域知识整合与任务适应性。数据与代码公开，提升可复现性。

Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: "Are Foundation Models Useful for ECG Analysis?" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at https://github.com/yuhaoxu99/ECGMultitasks-Benchmark.

</details>


### [39] [LLM4XCE: Large Language Models for Extremely Large-Scale Massive MIMO Channel Estimation](https://arxiv.org/abs/2512.08955)
*Renbin Li,Shuangshuang Li,Peihao Dong*

Main category: cs.LG

TL;DR: 提出了用于XL-MIMO通道估计的LLM4XCE框架，通过将大语言模型的语义建模能力与并行特征空间注意力（Parallel Feature-Spatial Attention）相结合，利用对前两层Transformer的微调实现高效训练，在混合场条件下显著提升估计精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在极大规模天线（XL-MIMO）场景中，近场与远场共存的混合场道理环境给信道估计带来挑战，传统方法难以很好泛化；另一方面，LLMs在下游任务中的语义理解能力可用于任务导向的信道表示学习，从而提升对复杂空间-时间特性的理解与泛化。

Method: 提出LLM4XCE框架，设计嵌入模块与Parallel Feature-Spatial Attention实现导向特征的深度融合，将 pilot 特征与空间结构转化为对LLM输入的语义丰富表示；仅微调顶层两层Transformer，以高效地捕捉 pilot 数据中的潜在依赖。

Result: 大量仿真实验表明，在混合场条件下，LLM4XCE显著优于现有方法，获得更高的信道估计精度和更好的泛化性能。

Conclusion: 证明基于LLM的语义建模可用于XL-MIMO信道估计，所提出的方法在建模能力与训练效率之间取得良好折中，且对后续任务导向的通道表示学习具有潜在价值。

Abstract: Extremely large-scale massive multiple-input multiple-output (XL-MIMO) is a key enabler for sixth-generation (6G) networks, offering massive spatial degrees of freedom. Despite these advantages, the coexistence of near-field and far-field effects in hybrid-field channels presents significant challenges for accurate estimation, where traditional methods often struggle to generalize effectively. In recent years, large language models (LLMs) have achieved impressive performance on downstream tasks via fine-tuning, aligning with the semantic communication shift toward task-oriented understanding over bit-level accuracy.
  Motivated by this, we propose Large Language Models for XL-MIMO Channel Estimation (LLM4XCE), a novel channel estimation framework that leverages the semantic modeling capabilities of large language models to recover essential spatial-channel representations for downstream tasks. The model integrates a carefully designed embedding module with Parallel Feature-Spatial Attention, enabling deep fusion of pilot features and spatial structures to construct a semantically rich representation for LLM input. By fine-tuning only the top two Transformer layers, our method effectively captures latent dependencies in the pilot data while ensuring high training efficiency. Extensive simulations demonstrate that LLM4XCE significantly outperforms existing state-of-the-art methods under hybrid-field conditions, achieving superior estimation accuracy and generalization performance.

</details>


### [40] [DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability](https://arxiv.org/abs/2512.08956)
*Kumarjit Pathak,Karthik K,Sachin Madan,Jitin Kapila*

Main category: cs.LG

TL;DR: DW-KNN 是一种双权重 KNN 变体，将指数距离与邻居有效性结合，用于提升在异质特征空间中的实例级可解释性、抑制噪声以及降低超参数敏感性。


<details>
  <summary>Details</summary>
Motivation: 在特征空间的异质性下，标准 KNN 假设所有最近邻同等可靠，导致预测不稳定与解释性不足；需要一种可解释且鲁棒的近邻方法，尤其适用于高风险场景。

Method: 提出 DW-KNN，通过对距离使用指数加权并同时对邻居有效性进行双重加权，提升实例级可解释性，抑制噪声或错标签样本，且降低对超参数的敏感性。

Result: 在9个数据集上，DW-KNN 平均准确率为 0.8988，排第2名，仅落后最优 Ensemble KNN 0.2% 之内；交叉验证方差最低（0.0156）；统计显著性检验显示相较于紧凑性加权 KNN 提升 +4.09%、相对核加权 KNN 提升 +1.13%，p<0.001。

Conclusion: 该方法提供一个简单而有效的替代方案，适用于需要可解释预测的高风险应用，尤其相比复杂自适应方案具有更高的透明度和鲁棒性。

Abstract: K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.
  We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.
  Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\%) and Kernel weighted KNN (+1.13\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.

</details>


### [41] [LUMOS: Large User MOdels for User Behavior Prediction](https://arxiv.org/abs/2512.08957)
*Dhruv Nigam*

Main category: cs.LG

TL;DR: LUMOS：一个端到端的 transformer 多任务学习框架，通过跨注意力和多模态表示，以原始用户行为数据学习多个任务，在大规模数据上提升预测性能并带来商业收益。


<details>
  <summary>Details</summary>
Motivation: 传统方法对每个任务需要独立的模型和大量的手工特征工程，成本高且难以扩展，因此寻求一个端到端、可扩展的多任务学习框架。

Method: - 使用 transformer backbone；- 通过跨注意力机制将预测条件化为未来已知事件；- 使用多模态 tokenization，将交易、事件上下文、静态人口统计等信息融合，通过专门的嵌入通道处理。

Result: - 在生产数据集（275B 用户活动 token，2.5亿用户）上评估，五个任务均优于基线；- ROC-AUC 平均提升 0.025；MAPE 降低 4.6%；- 在线 A/B 测试显示 DAU 提升 3.15%。

Conclusion: 端到端多任务学习结合跨注意力和多模态表达在大规模真实数据上有效，减少任务专用模型和手工特征的需求，同时带来显著的业务收益。

Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.

</details>


### [42] [EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications](https://arxiv.org/abs/2512.08959)
*Ard Kastrati,Josua Bürki,Jonas Lauer,Cheng Xuan,Raffaele Iaquinto,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 统一的 EEG 基础模型基准框架，覆盖 11 项诊断任务与 14 个公开数据集；实现最小预处理、标准化评估，便于对比经典基线与现代 foundation 模型；结果显示在某些场景 foundation 模型表现强，但简单模型在临床分布迁移下仍具竞争力；并公开数据与代码以促进复现与应用。


<details>
  <summary>Details</summary>
Motivation: 解决 EEG 基础模型在临床任务上的评估缺乏统一基准的问题，提供跨任务、跨数据集的一致评估与可重复性，以促进 foundation 模型在临床场景的落地与比较。

Method: 构建一个统一的基准框架：采用最小化的预处理、标准化的评估协议；覆盖 11 项诊断任务和 14 个公开 EEG 数据集；进行经典基线与现代 foundation 模型的并列比较；公开准备好的数据和代码以便复用。

Result: 在某些设置下，foundation 模型表现出色；但在临床分布迁移下，简单模型往往仍具竞争力。

Conclusion: 该框架提升可重复性与采用性，提供可访问、可扩展的数据与代码格式；强调在 EEG 基础模型评估中需综合考虑多任务与分布差异，避免盲目追求复杂模型。

Abstract: We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.

</details>


### [43] [Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces](https://arxiv.org/abs/2512.08960)
*Yueer Zhou,Yichen Wu,Ying Wei*

Main category: cs.LG

TL;DR: LoRA在持续学习中遭遇灾难性遗忘，主要由新任务梯度对历史权重轨迹的对抗性更新驱动。PS-LoRA通过在优化子空间中对齐更新并引入双重正则化来解决冲突，并通过基于幅度的合并策略将序列适配器整合为稳健表示。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA在连续学习场景中的灾难性遗忘问题，核心在于降低不同任务梯度之间的冲突，从而在保持旧知识的基础上有效适应新任务。

Method: 提出PS-LoRA（Parameter Stability LoRA），通过双重正则化目标惩罚冲突方向并限制更新幅度，以确保更新与先前知识的一致性；另外实现基于幅度的适配器合并策略，在不重新训练的情况下将序列适配器合并成更稳健的表示。

Result: 在自然语言处理与视觉基准上，PS-LoRA在保持表示稳定性的同时实现对新领域的高效适应，性能超过现有最优方法（SOTA）。

Conclusion: 通过对齐优化子空间中的更新、抑制冲突方向以及有效的适配器合并，PS-LoRA缓解持续学习中的参数更新冲突，提升稳定性与高效性。

Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.

</details>


### [44] [SEA: Spectral Edge Attacks on Graph Neural Networks](https://arxiv.org/abs/2512.08964)
*Yongyu Wang*

Main category: cs.LG

TL;DR: 提出 Spectral Edge Attacks (SEA) 的对抗性攻击，通过谱嵌入识别图中最脆弱方向，给边/非边赋予鲁棒性分数，提供两种攻击变体：Spade 删除和 Spade 添加；无梯度需求，能嵌入现有 GNN。


<details>
  <summary>Details</summary>
Motivation: GNN 对结构扰动极为敏感，现有结构攻击多依赖梯度或局部连通性，未充分利用谱域的脆弱方向。需要一个以谱鲁棒性为导向、能普遍嵌入不同GNN架构的攻击框架。

Method: 计算谱嵌入以捕捉输入流形中最脆弱的方向；基于谱鲁棒性对每条边/非边打分；Spade-guided deletion：删除得分最高的边；Spade-guided addition：在谱空间中最不相容的节点之间新增边；两者都在图级别进行、模型感知但无梯度需求，且可无缝嵌入现有GNN 架构。

Result: 在标准基准数据集上验证 SEA 的有效性，攻击能够显著降低目标 GNN 的性能，与现有结构攻击相比具有竞争力且不依赖梯度信息。

Conclusion: SEA 提供了一种基于谱鲁棒性的无梯度对抗性攻击框架，能够识别输入 manifold 的脆弱方向并通过删除/添加边进行结构扰动，便于集成到现有 GNN 模型中以评估鲁棒性。

Abstract: Graph Neural Networks (GNNs) achieve strong performance on graph-structured data, but are notoriously vulnerable to small, carefully crafted perturbations of the graph structure. Most existing structure-based attacks rely on gradient-based heuristics or local connectivity patterns, and treat edges as equally important candidates for manipulation. In this paper, we propose Spectral Edge Attacks (SEA), a new family of adversarial attacks that explicitly leverage spectral robustness evaluation to guide structural perturbations. Our key idea is to compute a spectral embedding that captures the most fragile directions of the input manifold and to use it to assign a robustness score to each edge or non-edge. Based on these scores, we introduce two complementary attack variants: (i) a Spade-guided deletion attack that removes the most spectrally robust edges, and (ii) a Spade-guided addition attack that inserts edges between nodes that are maximally incompatible in the fragile spectral space. Both attacks operate at the graph level, are model-aware but conceptually simple, and can be plugged into existing GNN architectures without requiring gradients. We describe the spectral formulation, the attack algorithms, and experiments on benchmarks.

</details>


### [45] [Financial Instruction Following Evaluation (FIFE)](https://arxiv.org/abs/2512.08965)
*Glenn Matlin,Siddharth,Anirudh JM,Aditya Shukla,Yahya Hassan,Sudheer Chava*

Main category: cs.LG

TL;DR: FIFE 是一个高难度的财政分析指令遵循基准，包含 88 条人类撰写提示，使用可验证的约束链式奖励信号评估53种模型在零样本下的表现。结果显示开源权重模型普遍优于顶尖专有系统，但均未达到完美合规，研究团队公开数据与代码以促进金融领域强化学习研究。


<details>
  <summary>Details</summary>
Motivation: 现有指令遵循基准往往在一般场景取得良好成绩，但在高风险、相互依赖的金融任务中对指令遵循的准确性与合规性要求极高。需要一个可验证、分层约束的评估体系来衡量模型在金融分析任务中的遵循能力与鲁棒性，以推动强化学习与安全应用的发展。

Method: 提出 FIFE 基准：88 条人工撰写的金融分析提示；引入带有可验证的链式约束的验证系统，用于给出细粒度的奖励信号。对 53 种模型进行零样本评估，覆盖专有、开源权重与开源模型。评估指标包含严格（strict）与宽松（loose）两种评分维度。

Result: 结果显示：最佳开源权重模型在 strict 维度获得 76.1、loose 维度 79.5；领先的专有系统为 65.9（strict）/70.5（loose），而最佳的开源模型仅 45.5（strict）/48.9（loose）。总体看法是，即便是表现最好的模型也未达到对于 FIFE 的完全合规要求，存在显著差距。

Conclusion: 作者公开数据集与代码，作为开放资源促进金融领域的强化学习研究，强调在高难度金融任务中的指令遵循仍有较大改进空间，未来需在可验证约束和细粒度奖励信号方面取得突破。

Abstract: Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.

</details>


### [46] [CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing](https://arxiv.org/abs/2512.08967)
*Zixia Wang,Gaojie Jin,Jia Hu,Ronghui Mu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.

</details>


### [47] [StructuredDNA: A Bio-Physical Framework for Energy-Aware Transformer Routing](https://arxiv.org/abs/2512.08968)
*Mustapha Hamdi*

Main category: cs.LG

TL;DR: StructuredDNA提出一种基于能量最小化的稀疏Transformer路由框架，用以替代Dense MoE，在能耗和计算需求方面实现显著节省，并在BioASQ与WikiText-103上验证了强鲁棒性与领域无关的推广能力。


<details>
  <summary>Details</summary>
Motivation: 应对大型计算模型的快速扩展带来的能量与计算成本激增，借鉴生物系统的低能量配置下结构与功能耦合的特性，提出一种能量感知的稀疏路由机制以提升效率。

Method: 以语义能量最小化为核心，抛弃密集Mixture-of-Experts路由，构建基于生物物理原理的能量引导路由层；将输入动态分组为语义性密码子（codons），路由仅选取单个专家，通过全局能量泛函最小化来同时优化聚合性(cohesion)、不确定性和计算成本。

Result: 在BioASQ（K=50）上实现能源利用密度(EUD)下降97.7%且语义稳定性指数(SSI)达0.998；在WikiText-103上提出语义扩展定律，通过增大专家粒度（K=2048）实现对开放领域的泛化，同时保持>99%的能效；总体上表现出对领域的鲁棒性与可扩展性。

Conclusion: StructuredDNA为生物物理原理与Transformer稀疏路由之间提供明确联系，指向未来基于能量意识的模块化和可扩展计算系统。亦指出本研究为概念验证的局限性，并给出对更大模型、数据集与硬件平台的扩展方向。代码实现公开，可用于进一步研究与应用。

Abstract: The rapid scaling of large computational models has led to a critical increase in energy and compute costs. Inspired by biological systems where structure and function emerge from low-energy configurations, we introduce StructuredDNA, a sparse architecture framework for modular, energy-aware Transformer routing. StructuredDNA replaces dense Mixture-of-Experts routing with a bio-physical, energy-guided routing layer based on semantic energy minimization. Inputs are dynamically grouped into semantic codons, and routing selects a single expert by minimizing a global energy functional that combines cohesion, uncertainty, and computational cost.
  We validate StructuredDNA on both specialized (BioASQ) and open-domain benchmarks (WikiText-103). On BioASQ (K = 50), we achieve a 97.7% reduction in Energy Utilization Density (EUD) and a Semantic Stability Index (SSI) of 0.998. We further demonstrate a Semantic Scaling Law on WikiText-103, showing that the architecture generalizes to open domains by scaling expert granularity (K = 2048) while maintaining more than 99% energy efficiency. StructuredDNA thus establishes a robust, domain-agnostic paradigm for future sparse computational frameworks.
  StructuredDNA provides an explicit link between bio-physical principles and sparse expert routing in Transformer architectures, and points toward future energy-aware, modular, and scalable computational systems. We discuss limitations of this proof-of-concept study and outline directions for scaling the approach to larger models, datasets, and hardware platforms. The StructuredDNA implementation is available at https://github.com/InnoDeep-repos/StructuredDNA .

</details>


### [48] [Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs](https://arxiv.org/abs/2512.08976)
*Isha Chaturvedi,Anjana Nair,Yushen Li,Adhitya Rajendra Kumar,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma*

Main category: cs.LG

TL;DR: 提出 Contrastive Region Masking (CRM)，一种训练无关的诊断工具，用于在逐步链式思维推理中揭示多模态大语言模型对特定视觉区域的依赖，通过对比掩蔽与非掩蔽的推理轨迹实现因果、逐步归因。


<details>
  <summary>Details</summary>
Motivation: 现有评估多模态大语言模型的多数聚焦于最终答案或注意力热力图，缺乏对推理过程的因果性与逐步可信性的测量。CRM 通过逐步掩蔽注释区域并对比推理轨迹，提供对模型推理过程的诊断性断言。

Method: 对注释的视觉区域执行系统性掩蔽，比较掩蔽条件与未掩蔽基线下的推理轨迹，获得逐步因果归因。将此方法应用于 VisArgs 等数据集以分析不同模型在各推理步骤对视觉区域的依赖及鲁棒性。

Result: CRM 揭示了多模态模型的不同失败模式：部分模型保持推理结构但在证据缺失时产生幻觉，其他模型则对视觉线索高度绑定并在扰动下崩溃。该方法将评估焦点从答案正确性转向推理的可信度，促使将视觉基准转化为诊断工具。

Conclusion: 强调需要新的多模态评估框架，关注推理的可信性、鲁棒性与证据依赖，从而提升对多模态模型的全面评估与改进。

Abstract: We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.

</details>


### [49] [Graph Deep Learning for Intracranial Aneurysm Blood Flow Simulation and Risk Assessment](https://arxiv.org/abs/2512.09013)
*Paul Garnier,Pablo Jeken-Rico,Vincent Lannelongue,Chiara Faitini,Aurèle Goetz,Lea Chanvillard,Ramy Nemer,Jonathan Viquerat,Ugo Pelissier,Philippe Meliga,Jacques Sédat,Thomas Liebig,Yves Chau,Elie Hachem*

Main category: cs.LG

TL;DR: Graph neural network surrogate reproduces full-field intracranial hemodynamics from vascular geometry in under one minute per cardiac cycle, enabling real-time, high-resolution flow fields for aneurysm risk assessment.


<details>
  <summary>Details</summary>
Motivation: CFD provides accurate hemodynamics but is slow and requires specialist expertise; 4D Flow MRI lacks sufficient spatial resolution and is expensive, hindering bedside assessment.

Method: Graph Transformer-based autoregressive model trained on high-fidelity, patient-specific aneurysm simulations. Inputs are vascular geometries; outputs are velocity fields, wall shear stress, and oscillatory shear index. The model generalizes across unseen geometries and inflow conditions without mesh-specific calibration.

Result: Inference time <1 minute per cardiac cycle with high-fidelity flow field predictions. Generalizes to unseen geometries and inflow conditions. Compatible with imaging pipelines and facilitates interpretable predictions.

Conclusion: Transforms high-fidelity simulations into a deployable, data-driven clinical decision support tool, enabling near real-time, high-resolution aneurysm analysis at the bedside.

Abstract: Intracranial aneurysms remain a major cause of neurological morbidity and mortality worldwide, where rupture risk is tightly coupled to local hemodynamics particularly wall shear stress and oscillatory shear index. Conventional computational fluid dynamics simulations provide accurate insights but are prohibitively slow and require specialized expertise. Clinical imaging alternatives such as 4D Flow MRI offer direct in-vivo measurements, yet their spatial resolution remains insufficient to capture the fine-scale shear patterns that drive endothelial remodeling and rupture risk while being extremely impractical and expensive.
  We present a graph neural network surrogate model that bridges this gap by reproducing full-field hemodynamics directly from vascular geometries in less than one minute per cardiac cycle. Trained on a comprehensive dataset of high-fidelity simulations of patient-specific aneurysms, our architecture combines graph transformers with autoregressive predictions to accurately simulate blood flow, wall shear stress, and oscillatory shear index. The model generalizes across unseen patient geometries and inflow conditions without mesh-specific calibration. Beyond accelerating simulation, our framework establishes the foundation for clinically interpretable hemodynamic prediction. By enabling near real-time inference integrated with existing imaging pipelines, it allows direct comparison with hospital phase-diagram assessments and extends them with physically grounded, high-resolution flow fields.
  This work transforms high-fidelity simulations from an expert-only research tool into a deployable, data-driven decision support system. Our full pipeline delivers high-resolution hemodynamic predictions within minutes of patient imaging, without requiring computational specialists, marking a step-change toward real-time, bedside aneurysm analysis.

</details>


### [50] [Improving Multi-Class Calibration through Normalization-Aware Isotonic Techniques](https://arxiv.org/abs/2512.09054)
*Alon Arad,Saharon Rosset*

Main category: cs.LG

TL;DR: 提出基于单调归一化的多分类校准新方法NA-FIR和SCIR，通过在优化中引入归一化约束或将问题建模为累积双变量单调回归，以提升NLL和ECE。


<details>
  <summary>Details</summary>
Motivation: 在多分类校准中，基于一-vs-rest的非参数方法往往劣于参数方法，且需要显式处理概率归一化以避免不一致的预测概率。

Method: NA-FIR 在优化过程中直接纳入概率归一化约束；SCIR 将多类校准建模为累积双变量单调回归。

Result: 在多种文本和图像分类数据集及不同模型架构上，所提方法稳定提升负对数似然（NLL）和校准误差（ECE）等评估指标。

Conclusion: 引入归一化感知的单调性约束可显著提升多类概率校准的性能，提升预测概率的可信度，具有实际应用潜力。

Abstract: Accurate and reliable probability predictions are essential for multi-class supervised learning tasks, where well-calibrated models enable rational decision-making. While isotonic regression has proven effective for binary calibration, its extension to multi-class problems via one-vs-rest calibration produced suboptimal results when compared to parametric methods, limiting its practical adoption. In this work, we propose novel isotonic normalization-aware techniques for multiclass calibration, grounded in natural and intuitive assumptions expected by practitioners. Unlike prior approaches, our methods inherently account for probability normalization by either incorporating normalization directly into the optimization process (NA-FIR) or modeling the problem as a cumulative bivariate isotonic regression (SCIR). Empirical evaluation on a variety of text and image classification datasets across different model architectures reveals that our approach consistently improves negative log-likelihood (NLL) and expected calibration error (ECE) metrics.

</details>


### [51] [A Diffusion-Based Framework for High-Resolution Precipitation Forecasting over CONUS](https://arxiv.org/abs/2512.09059)
*Marina Vicens-Miquel,Amy McGovern,Aaron J. Hill,Efi Foufoula-Georgiou,Clement Guilloteau,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 提出基于扩散模型的深度学习框架，系统比较三种残差预测输入（MRMS数据、HRRR修正、两者组合），在CONUS区域以1 km分辨率实现1–12小时预报，普遍优于HRRR基线，且在不同时间尺度表现不同，具备 calibrated 不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 准确降水预报对防灾减灾关键，需评估不同数据源对预测技能的贡献，理解数据源在不同时段的作用，并提升长期预测的可靠性。

Method: 使用扩散式深度学习框架，输入来源有三种残差预测策略：1) 仅MRMS历史观测的全数据驱动模型；2) 仅HRRR预报的纠正模型；3) MRMS与选定HRRR变量的混合模型。统一设置下在CONUS评估，输出1 km，起始1小时并通过自回归展开到12小时；使用像素级和时空统计指标评估，包含对极端降水阈值的技能评估，以及对残差学习的 calibrated 不确定性量化。

Result: 在所有预报时效中，DL框架在像素级和时空统计指标上持续优于HRRR基线；混合模型在最短时效具有最佳表现，HRRR纠正模型在较长时效表现更强且可维持至12小时；对极端降水的预测能力显著提升，并且不确定性被良好校准。

Conclusion: 通过提升.predict skill、可靠性与跨区域适用性，推动基于DL的降水预测的发展，尤其在紧急准备方面具有实际作用。

Abstract: Accurate precipitation forecasting is essential for hydrometeorological risk management, especially for anticipating extreme rainfall that can lead to flash flooding and infrastructure damage. This study introduces a diffusion-based deep learning (DL) framework that systematically compares three residual prediction strategies differing only in their input sources: (1) a fully data-driven model using only past observations from the Multi-Radar Multi-Sensor (MRMS) system, (2) a corrective model using only forecasts from the High-Resolution Rapid Refresh (HRRR) numerical weather prediction system, and (3) a hybrid model integrating both MRMS and selected HRRR forecast variables. By evaluating these approaches under a unified setup, we provide a clearer understanding of how each data source contributes to predictive skill over the Continental United States (CONUS). Forecasts are produced at 1-km spatial resolution, beginning with direct 1-hour predictions and extending to 12 hours using autoregressive rollouts. Performance is evaluated using both CONUS-wide and region-specific metrics that assess overall performance and skill at extreme rainfall thresholds. Across all lead times, our DL framework consistently outperforms the HRRR baseline in pixel-wise and spatiostatistical metrics. The hybrid model performs best at the shortest lead time, while the HRRR-corrective model outperforms others at longer lead times, maintaining high skill through 12 hours. To assess reliability, we incorporate calibrated uncertainty quantification tailored to the residual learning setup. These gains, particularly at longer lead times, are critical for emergency preparedness, where modest increases in forecast horizon can improve decision-making. This work advances DL-based precipitation forecasting by enhancing predictive skill, reliability, and applicability across regions.

</details>


### [52] [Modular Deep-Learning-Based Early Warning System for Deadly Heatwave Prediction](https://arxiv.org/abs/2512.09074)
*Shangqing Xu,Zhiyuan Zhao,Megha Sharma,José María Martín-Olalla,Alexander Rodríguez,Gregory A. Wellenius,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 提出 DeepTherm，一种不依赖热相关死亡历史的模块化深度学习早期预警系统，用双路径预测来分离基线死亡率和与热浪相关的异常事件，在西班牙多地区数据上实现鲁棒且可权衡错报警和漏报警的性能。


<details>
  <summary>Details</summary>
Motivation: 城市热浪对公共健康的威胁日益严重，现有预测热浪发生或归因历史死亡的模型在预测即将来临的致死热浪方面存在挑战，且需要可用数据、空间/时间鲁棒性和决策成本的平衡。

Method: DeepTherm 采用双预测管线的模块化深度学习框架，通过将基线死亡率（无热浪情形）与所有原因导致的死亡率中的非热浪事件/异常事件分离来实现对致死热浪的预测，在西班牙的真实世界数据上进行跨地区、跨时段、跨人群的评估。

Result: 在西班牙数据上的评估显示 DeepTherm 具有一致、鲁棒且高准确度的性能，能够在不同区域、时间段和人群中保持表现，并允许在漏警与误警之间进行权衡。

Conclusion: DeepTherm 提供了一种无需历史热相关死亡数据的致命热浪预测的灵活、可扩展解决方案，具有在现实世界早警系统中的潜在应用价值。

Abstract: Severe heatwaves in urban areas significantly threaten public health, calling for establishing early warning strategies. Despite predicting occurrence of heatwaves and attributing historical mortality, predicting an incoming deadly heatwave remains a challenge due to the difficulty in defining and estimating heat-related mortality. Furthermore, establishing an early warning system imposes additional requirements, including data availability, spatial and temporal robustness, and decision costs. To address these challenges, we propose DeepTherm, a modular early warning system for deadly heatwave prediction without requiring heat-related mortality history. By highlighting the flexibility of deep learning, DeepTherm employs a dual-prediction pipeline, disentangling baseline mortality in the absence of heatwaves and other irregular events from all-cause mortality. We evaluated DeepTherm on real-world data across Spain. Results demonstrate consistent, robust, and accurate performance across diverse regions, time periods, and population groups while allowing trade-off between missed alarms and false alarms.

</details>


### [53] [Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting](https://arxiv.org/abs/2512.09076)
*Moazzam Umer Gondal,Hamad ul Qudous,Asma Ahmad Farhan*

Main category: cs.LG

TL;DR: 轻量级添加模型（Facebook Prophet 与 NeuralProphet）在北京PM2.5/PM10预测中表现出强竞争力，FBP优于NP及其他基线，R^2>0.94，强调可解释性与易部署。


<details>
  <summary>Details</summary>
Motivation: 在深度学习和复杂混合管线占主导的背景下，探究可解释且轻量的加法模型是否能够提供稳定且易部署的空气污染预测。

Method: 使用多年的污染物与气象数据，进行相关性、互信息、mRMR等特征选择；泄漏安全的尺度缩放；按时间顺序分割数据；训练FBP与NP及其含前因特征，另有LSTM、LightGBM、SARIMAX作为基线；以7天持出集评估MAE、RMSE、R^2。

Result: FBP在测试集对PM2.5/PM10的R^2均>0.94，优于NP、SARIMAX及学习型基线；NP表现略逊于FBP但具备滞后特征的优势。

Conclusion: 可解释的轻量模型仍具备竞争力，提供准确、透明且易部署的预测方案，适用于城市空气污染预测的实际应用。

Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.

</details>


### [54] [GS-KAN: Parameter-Efficient Kolmogorov-Arnold Networks via Sprecher-Type Shared Basis Functions](https://arxiv.org/abs/2512.09084)
*Oscar Eliasson*

Main category: cs.LG

TL;DR: GS-KAN 在 Kolmogorov-Arnold Networks 的基础上提出的一种轻量级架构，通过对每层的共享父函数应用可学习的线性变换来生成边函数，从而实现参数高效的学习。相较于原始 KAN，GS-KAN 通过边函数的参数共享显著降低参数量，同时在连续函数逼近、表格回归和高维图像分类任务中实现更优或竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 解决 KAN 的参数爆炸问题与高维场景上的部署困难，同时在保持拟合能力的同时提高参数效率。

Method: 在每一层仅维护一个可学习的父函数，并对图中每条边应用可学习的线性变换来生成边函数；这相当于用一个共享的父函数通过线性变换得到多样化的边函数。此设计借鉴 Sprecher 的超叠加/重叠定理的思想，以实现边函数的唯一性与高效表征。通过在函数逼近、表格回归和图像分类任务上与 MLP 与 KAN 基线进行对比评估。

Result: 在连续函数逼近任务中，GS-KAN 优于 MLP 和标准 KAN，且参数效率更高；在表格回归和高维分类任务中，与 KAN 竞争并在某些场景优于 MLP；在高维且受严格参数约束的场景中实现可部署性，显示出良好参数受限情况下的可行性。

Conclusion: 通过引入共享父函数并对边应用学习性线性变换，GS-KAN 在保持或提升表达能力的同时显著降低参数量，为高维和参数受限应用提供一个实用的 KAN 变体，并提供开源实现。

Abstract: The Kolmogorov-Arnold representation theorem offers a theoretical alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate functions on edges rather than nodes. While recent implementations such as Kolmogorov-Arnold Networks (KANs) demonstrate high approximation capabilities, they suffer from significant parameter inefficiency due to the requirement of maintaining unique parameterizations for every network edge. In this work, we propose GS-KAN (Generalized Sprecher-KAN), a lightweight architecture inspired by David Sprecher's refinement of the superposition theorem. GS-KAN constructs unique edge functions by applying learnable linear transformations to a single learnable, shared parent function per layer. We evaluate GS-KAN against existing KAN architectures and MLPs across synthetic function approximation, tabular data regression and image classification tasks. Our results demonstrate that GS-KAN outperforms both MLPs and standard KAN baselines on continuous function approximation tasks while maintaining superior parameter efficiency. Additionally, GS-KAN achieves competitive performance with existing KAN architectures on tabular regression and outperforms MLPs on high-dimensional classification tasks. Crucially, the proposed architecture enables the deployment of KAN-based architectures in high-dimensional regimes under strict parameter constraints, a setting where standard implementations are typically infeasible due to parameter explosion. The source code is available at https://github.com/rambamn48/gs-impl.

</details>


### [55] [Learning Unmasking Policies for Diffusion Language Models](https://arxiv.org/abs/2512.09106)
*Metod Jazbec,Theo X. Olausson,Louis Béthune,Pierre Ablin,Michael Kirchhof,Joao Monterio,Victor Turrisi,Jason Ramapuram,Marco Cuturi*

Main category: cs.LG

TL;DR: 提出通过强化学习训练的掩码扩散采样策略，以替代手工启发式采样，提升 dLLMs 的采样质量与并行解掩效率，且具备对新模型和更长序列的转移性；但对域外数据鲁棒性和准确性-效率权衡的微调较具挑战。


<details>
  <summary>Details</summary>
Motivation: 解决掩码扩散中在并行解掩时对要替换的令牌的采样决策问题，避免手工阈值等启发式策略的调参负担和在较大缓冲区时性能下降。

Method: 将掩码扩散采样建模为马尔可夫决策过程，提出基于单层变换器的轻量化策略，将 dLLM 的置信度映射到解掩决策；通过强化学习对该策略进行训练。

Result: 训练得到的策略在半自回生成下与最优启发式方法媲美，在全扩散情形中优于它们；具有对新底层 dLLM 与更长序列长度的部分转移性；但在跨域数据上表现下降，且精细调参以权衡准确性与效率具有挑战性。

Conclusion: 强化学习训练的掩码扩散采样策略具有提升 dLLMs 采样效率与质量的潜力，并具一定的泛化能力，但需解决域外鲁棒性与权衡调参的难题。

Abstract: Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.

</details>


### [56] [Spectral Embedding via Chebyshev Bases for Robust DeepONet Approximation](https://arxiv.org/abs/2512.09165)
*Muhammad Abid,Omer San*

Main category: cs.LG

TL;DR: SEDONet uses a fixed Chebyshev spectral dictionary as the trunk input to DeepONet, replacing coordinate-based MLP trunks to better capture non-periodic, bounded-domain features. It achieves lower relative L2 errors (≈30-40% improvement over DeepONet) across PDE benchmarks (Poisson, Burgers, advection-diffusion, Allen–Cahn, Lorenz-96) and preserves high-frequency/boundary-localized features.


<details>
  <summary>Details</summary>
Motivation: Standard DeepONet trunks based on fully connected networks on raw coordinates struggle to represent sharp gradients, boundary layers, and non-periodic structures in PDEs on bounded domains with Dirichlet/Neumann conditions. A non-periodic spectral embedding (Chebyshev) provides a principled inductive bias tailored to such domains.

Method: Replace DeepONet trunk inputs with a fixed Chebyshev spectral dictionary, enabling non-periodic spectral embedding. Compare against DeepONet and FEDONet and Fourier-embedded variants on diverse PDE benchmarks (elliptic, parabolic, advective, multiscale). Analyze relative L2 errors and spectral content.

Result: SEDONet consistently achieves the lowest relative L2 errors across datasets, with average improvements of ~30-40% over baseline DeepONet and meaningful gains over Fourier-embedded variants on non-periodic geometries. Spectral analyses show better preservation of high-frequency and boundary-localized features.

Conclusion: A simple, parameter-neutral modification—the Chebyshev spectral embedding—provides a robust, efficient framework for non-periodic operator learning on bounded domains, improving surrogate modeling of PDEs without heavy architectural changes.

Abstract: Deep Operator Networks (DeepONets) have become a central tool in data-driven operator learning, providing flexible surrogates for nonlinear mappings arising in partial differential equations (PDEs). However, the standard trunk design based on fully connected layers acting on raw spatial or spatiotemporal coordinates struggles to represent sharp gradients, boundary layers, and non-periodic structures commonly found in PDEs posed on bounded domains with Dirichlet or Neumann boundary conditions. To address these limitations, we introduce the Spectral-Embedded DeepONet (SEDONet), a new DeepONet variant in which the trunk is driven by a fixed Chebyshev spectral dictionary rather than coordinate inputs. This non-periodic spectral embedding provides a principled inductive bias tailored to bounded domains, enabling the learned operator to capture fine-scale non-periodic features that are difficult for Fourier or MLP trunks to represent. SEDONet is evaluated on a suite of PDE benchmarks including 2D Poisson, 1D Burgers, 1D advection-diffusion, Allen-Cahn dynamics, and the Lorenz-96 chaotic system, covering elliptic, parabolic, advective, and multiscale temporal phenomena, all of which can be viewed as canonical problems in computational mechanics. Across all datasets, SEDONet consistently achieves the lowest relative L2 errors among DeepONet, FEDONet, and SEDONet, with average improvements of about 30-40% over the baseline DeepONet and meaningful gains over Fourier-embedded variants on non-periodic geometries. Spectral analyses further show that SEDONet more accurately preserves high-frequency and boundary-localized features, demonstrating the value of Chebyshev embeddings in non-periodic operator learning. The proposed architecture offers a simple, parameter-neutral modification to DeepONets, delivering a robust and efficient spectral framework for surrogate modeling of PDEs on bounded domains.

</details>


### [57] [Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach](https://arxiv.org/abs/2512.09198)
*Phevos Paschalidis,Vasiliki Stoumpou,Lisa Everest,Yu Ma,Talhat Azemi,Jawad Haider,Steven Zweibel,Eleftherios M. Protopapas,Jeff Mather,Maciej Tysarowski,George E. Sarris,Robert C. Hagberg,Howard L. Haronian,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 提出一个数据驱动的个性化 THV 处方模型，旨在在 TAVR 中通过选择合适的瓣膜类型来最小化永久起搏器植入（PPI）风险。通过整合美国与希腊的患者数据（人口统计、CT、超声）并进行叶级分析，模型在内部美国队列实现 PPI 降低26%，在外部希腊验证队列降幅为16%；宣称为首个统一的个性化 THV 选择策略。


<details>
  <summary>Details</summary>
Motivation: TAVR 的瓣膜选择对 PPI 风险有显著影响，但现有指南在瓣膜类型处方上缺乏统一、个性化的方法。跨国数据整合与对异质人群的叶级分析有望改进风险评估与处方决策。

Method: 建立一个数据驱动的临床支持工具，整合三种数据源（患者人口统计、CT、超声）并对美希两国的记录系统差异进行数据对齐与 harmonization。提出叶级分析以利用人群异质性，避免对不确定的反事实风险进行基准比较。开发一个可实际临床使用的处方模型，用于在不同病人情境下最小化 PPI 风险的阀膜类型选择。对内部美国队列与外部希腊验证队列进行评估。

Result: 内部美国人群中 PPI 率相对当前护理实践降低约26%，外部希腊验证队列降低约16%。

Conclusion: 据称这是首个统一的、个性化的 THV 处方策略，用于 TAVR 中的瓣膜类型选择，具有潜在提升临床结局的潜力，但需进一步多中心外部验证与可重复性分析。

Abstract: Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.

</details>


### [58] [LLMs for Analog Circuit Design Continuum (ACDC)](https://arxiv.org/abs/2512.09199)
*Yasaman Esfandiari,Jocelyn Rego,Austin Meyer,Jonathan Gallagher,Mia Levy*

Main category: cs.LG

TL;DR: LLMs在类比电路设计中具潜力用于AI辅助，但在现实工程任务上暴露出对数据格式的敏感性、设计生成的不稳定性以及对未见电路配置的泛化能力有限等挑战。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在需要领域知识、物理约束和结构化表示的人机协同环境中的可靠性与一致性，以及它们在实际工程工作流中的鲁棒性，聚焦于模拟电路设计。

Method: 比较不同数据表示对模型行为的影响；在不同训练条件下对小模型（如T5、GPT-2）与大型基础模型（如Mistral-7B、GPT-oss-20B）进行评估；重点考察AI辅助设计中人类在环的应用情景。

Result: 可靠性挑战包括对数据格式的敏感性、生成设计的不稳定性、对未见电路配置的泛化能力有限。

Conclusion: 提供关于LLMs作为辅助工具在结构化、现实世界应用中的潜力与局限的早期证据，强调在设计可部署的基础模型时需关注数据表示、模型规模与训练条件等因素。

Abstract: Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.

</details>


### [59] [Contrastive Learning for Semi-Supervised Deep Regression with Generalized Ordinal Rankings from Spectral Seriation](https://arxiv.org/abs/2512.09267)
*Ce Wang,Weihang Dai,Hanru Bai,Xiaomeng Li*

Main category: cs.LG

TL;DR: 提出一种半监督对比学习回归的新框架，利用未标记数据构建混合样本的特征相似性矩阵，通过光谱排序（spectral seriation）得到样本的序关系，并通过带边界的排序误差理论实现鲁棒性；引入有标签样本对排序进行正则化，使用动态规划选择鲁棒特征以降低矩阵扰动；将得到的序关系用于对未标记样本进行对比学习并监督预测，从而在多数据集上超越现有半监督回归方法，实验与理论均提供证明，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有对比回归方法高度依赖标记信息来恢复特征之间的有序关系，导致在半监督场景中的应用受限且成本高。通过同时利用标记与未标记数据构建样本间关系，并在误差受控的前提下恢复样本的序关系，可以降低对人工标注的需求并提升表示学习的鲁棒性。

Method: 在每个小批量中结合标记与未标记样本构建特征相似性矩阵；通过光谱排序（seriation）在给定误差容忍度内恢复未标记样本的序关系；有标签样本对排序进行正则化以提高可靠性；使用动态规划选择鲁棒特征以减少矩阵扰动；将恢复的序关系用于对未标记样本的对比学习，并将序关系作为额外监督信号提升预测。理论上给出收敛/误差边界的保证并在多数据集上给出实验验证。

Result: 理论保证成立并在若干数据集上实验表明，该方法相较现有的半监督深度回归方法具有更稳健的表示能力和更好的回归性能，且能够有效扩大可利用的数据规模以提升学习效果。

Conclusion: 将对比回归扩展到半监督场景，通过未标记数据提取样本序关系并结合有标签信息进行正则化，结合鲁棒特征选择实现更稳定的表示学习；理论与实验均验证了其有效性，代码已开源。

Abstract: Contrastive learning methods enforce label distance relationships in feature space to improve representation capability for regression models. However, these methods highly depend on label information to correctly recover ordinal relationships of features, limiting their applications to semi-supervised regression. In this work, we extend contrastive regression methods to allow unlabeled data to be used in the semi-supervised setting, thereby reducing the dependence on costly annotations. Particularly we construct the feature similarity matrix with both labeled and unlabeled samples in a mini-batch to reflect inter-sample relationships, and an accurate ordinal ranking of involved unlabeled samples can be recovered through spectral seriation algorithms if the level of error is within certain bounds. The introduction of labeled samples above provides regularization of the ordinal ranking with guidance from the ground-truth label information, making the ranking more reliable. To reduce feature perturbations, we further utilize the dynamic programming algorithm to select robust features for the matrix construction. The recovered ordinal relationship is then used for contrastive learning on unlabeled samples, and we thus allow more data to be used for feature representation learning, thereby achieving more robust results. The ordinal rankings can also be used to supervise predictions on unlabeled samples, serving as an additional training signal. We provide theoretical guarantees and empirical verification through experiments on various datasets, demonstrating that our method can surpass existing state-of-the-art semi-supervised deep regression methods. Our code have been released on https://github.com/xmed-lab/CLSS.

</details>


### [60] [Goal inference with Rao-Blackwellized Particle Filters](https://arxiv.org/abs/2512.09269)
*Yixuan Wang,Dan P. Guralnik,Warren E. Dixon*

Main category: cs.LG

TL;DR: RBPF-based intent inference for mobile agents with closed-loop dynamics; introduces Gaussian-mixture and reduced estimators for intent once-observed; analyzes information leakage and KL-divergence bounds; shows reduced estimator nearly matches full estimator; experiments show fast intent recovery and motivates intent obfuscation.


<details>
  <summary>Details</summary>
Motivation: Infer an agent's eventual goal from noisy trajectory observations, a fundamental estimation problem, with a sample-efficient method and theoretical leakage guarantees.

Method: Use a Rao-Blackwellized Particle Filter leveraging a closed-form linear-Gaussian substructure to marginalize analytically and update only particle weights; develop two estimators: a Gaussian mixture model using RBPF weights and a reduced version restricting the mixture to the effective sample; derive lower bounds on KL divergence to quantify leakage.

Result: Demonstrate that the two estimators enable fast and accurate intent recovery for compliant agents; provide computable lower bounds on information leakage; show the reduced estimator performs nearly as well as the complete one.

Conclusion: Highlight potential for designing intent-obfuscating controllers and outline future work on robust obfuscation and privacy-preserving controller design.

Abstract: Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.

</details>


### [61] [Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices](https://arxiv.org/abs/2512.09313)
*Yuki Oda,Yuta Ono,Hiroshi Nakamura,Hideki Takase*

Main category: cs.LG

TL;DR: 提出一种适用于异构物联网设备的分层切点Split Learning框架Hetero-SplitEE，通过异构早期出口和两种训练策略实现并行/顺序协作，能在保持精度的同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 大模型的持续规模化提升了训练所需的计算资源，现有Split Learning假设客户端同质且切点统一，导致在现实物联网系统中难以部署。

Method: 在层级训练中引入异构早期出口，使每个客户端可选择不同的cut layer以匹配其计算能力；提出两种协作训练策略：Sequential策略（串行训练、共享服务器模型以降低计算开销）与Averaging策略（并行训练、定期跨层聚合）。

Result: 在CIFAR-10、CIFAR-100、STL-10数据集上评估，使用ResNet-18，实验显示在维持竞争精度的同时，能够有效支持异构计算资源的环境。

Conclusion: 提出的方法可在异构IoT生态中实现高效、可扩展的协作深度学习部署，提升现实场景下的可用性。

Abstract: The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.

</details>


### [62] [Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design](https://arxiv.org/abs/2512.09329)
*Amin Tavakoli,Raswanth Murugan,Ozan Gokdemir,Arvind Ramanathan,Frances Arnold,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出一种快速、通用的监督微调（SFT）方案，用 PLM 自带的数据与轻量化筛选管线在蛋白质序列建模上提升保真性、可靠性与新颖性，且不依赖昂贵的实验数据集。


<details>
  <summary>Details</summary>
Motivation: 蛋白质数据相较自然语言数据更难获得高质量带注释的数据；需要一种普适可扩展的 SFT 路径来提升 PLM 在蛋白质设计中的性能。

Method: 利用 PLM 自身生成训练数据，结合轻量化筛选管线和领域特定筛选器，对输出进行筛选并识别可用于体外评估的候选序列；与 SFT 结合，使模型生成更稳定、功能性更强的酶并探索超越自然变体的序列空间；在 GenSLM 与 tryptophan synthase 家族上验证，方法对 PLM 与蛋白质系统不敏感。

Result: 经 SFT 的模型产生的序列在目标设计约束和新兴蛋白质性质评估上显示更高的新颖性与改进的特性。

Conclusion: 该策略对 PLM 及蛋白质系统具有普遍性，可用于实现更稳定、功能性强的蛋白质设计，同时扩大对序列空间的探索。

Abstract: Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.

</details>


### [63] [Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality](https://arxiv.org/abs/2512.09355)
*Junru Zhou,Yicheng Wang,Pan Li*

Main category: cs.LG

TL;DR: Node-anchored Subgraph GNNs with expressive power below 3-WL can approximate Strong Branching scores, but practical costs are high; current methods favor MPNNs/heuristics; need efficiency-preserving expressivity.


<details>
  <summary>Details</summary>
Motivation: Investigate the expressivity-efficiency trade-off for GNN-based MILP branching to bridge theory and practice.

Method: Theoretical proof showing node-anchored Subgraph GNNs with expressivity strictly below 3-WL can approximate Strong Branching scores; comprehensive empirical evaluation on four MILP benchmarks comparing against MPNNs and heuristics.

Result: Theoretically, subgraph GNNs with sub-3-WL expressivity suffice to approximate Strong Branching scores. Empirically, these models incur O(n) complexity, causing memory bottlenecks and slower solving times than MPNNs and heuristics.

Conclusion: Expressivity alone is not enough for practical gains; efficiency-preserving expressivity is needed; future work should target reducing computational costs while preserving decision quality.

Abstract: Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.

</details>


### [64] [A Granular Framework for Construction Material Price Forecasting: Econometric and Machine-Learning Approaches](https://arxiv.org/abs/2512.09360)
*Boge Lyu,Qianye Yin,Iris Denise Tommelein,Hanyang Liu,Karnamohit Ranka,Karthik Yeluripati,Junzhe Shi*

Main category: cs.LG

TL;DR: 在建筑成本预测领域，提出基于 CSI MasterFormat 的六位分纲六级预测框架，整合原材料价格、商品指数和宏观变量，并对四种时序模型进行对比，结果显示引入解释变量可显著提升预测性能，LSTM 为最佳模型，具备跨 CSI 分部的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 建筑材料价格的持续波动对成本估算、预算编制和项目交付造成重大风险，需提供粒度更高、可扩展的预测方法。以 CSI MasterFormat 作为目标数据结构实现对六位分段的详细成本预测，以支持更精细的成本管理。

Method: 比较 four time-series 模型：LSTM、ARIMA、VECM、Chronos-Bolt；在基线配置（仅 CSI 数据）与扩展配置（加入原材料价格、商品指数、宏观变量）下评估；数据覆盖六位 CSI 分段，并在多个 CSI 部门进行验证，Division 06 作为示例；使用 RMSE 和 MAPE 作为评估指标。

Result: 加入解释变量后所有模型的预测性能显著提升；在对比中 LSTM 具有最高准确性，RMSE 最低可达 1.390，MAPE 为 0.957，较 ARIMA 的提升可达约 59%；跨分部的验证证实框架的可扩展性，Division 06 提供了详细实例。

Conclusion: 该框架为业主和承包商提供更可靠的 Definitive 水平成本估算和预算改进方法，具有良好的可扩展性，适合在实际工程项目中应用与推广。

Abstract: The persistent volatility of construction material prices poses significant risks to cost estimation, budgeting, and project delivery, underscoring the urgent need for granular and scalable forecasting methods. This study develops a forecasting framework that leverages the Construction Specifications Institute (CSI) MasterFormat as the target data structure, enabling predictions at the six-digit section level and supporting detailed cost projections across a wide spectrum of building materials. To enhance predictive accuracy, the framework integrates explanatory variables such as raw material prices, commodity indexes, and macroeconomic indicators. Four time-series models, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Vector Error Correction Model (VECM), and Chronos-Bolt, were evaluated under both baseline configurations (using CSI data only) and extended versions with explanatory variables. Results demonstrate that incorporating explanatory variables significantly improves predictive performance across all models. Among the tested approaches, the LSTM model consistently achieved the highest accuracy, with RMSE values as low as 1.390 and MAPE values of 0.957, representing improvements of up to 59\% over the traditional statistical time-series model, ARIMA. Validation across multiple CSI divisions confirmed the framework's scalability, while Division 06 (Wood, Plastics, and Composites) is presented in detail as a demonstration case. This research offers a robust methodology that enables owners and contractors to improve budgeting practices and achieve more reliable cost estimation at the Definitive level.

</details>


### [65] [KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction](https://arxiv.org/abs/2512.09365)
*Jiayu Qin,Zhengquan Luo,Guy Tadmor,Changyou Chen,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 提出一个基于最优传输的伪标签生成框架，整合分子、蛋白质、基因和通路等多源生物数据，缓解标签稀缺和单模态局限性，提升MPI预测及零-shot能力。


<details>
  <summary>Details</summary>
Motivation: MPI数据标注稀缺且现有模型多依赖单一模态的分子/蛋白特征，缺乏对基因、代谢通路等生物上下文的利用，限制预测性能与泛化。

Method: 聚合分子、蛋白、基因和通路层面的相互作用等异构数据，提出基于最优传输的伪标签生成策略，利用已知相互作用的分布来引导对未标注对的标签分配，将伪标签作为跨模态桥梁，提升MPI预测。

Result: 在多个MPI数据集（含虚拟筛选和蛋白检索任务）上实现对比方法显著提升的预测准确性与对未知交互的零-shot能力。

Conclusion: 该框架为利用多源生物数据解决传统单模态学习局限性提供新范式，为计算生物学和药物发现的未来发展铺平道路。

Abstract: Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.

</details>


### [66] [CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning](https://arxiv.org/abs/2512.09368)
*Mingyuan Li,Chunyu Liu,Zhuojun Li,Xiao Liu,Guangsheng Yu,Bo Du,Jun Shen,Qiang Wu*

Main category: cs.LG

TL;DR: 提出 CFLight，通过引入 CF 学习和因果模型在交通信号控制中提升安全性，能在不安全事件发生时回退并评估替代行动，从而实现接近零碰撞的控制，优于传统 RL 和安全 RL 方法。


<details>
  <summary>Details</summary>
Motivation: 解决交通信号控制中以效率优先、缺乏可解释性的挑战；RL 常忽视安全性，CF 学习在因果分析中的潜力可用于提升 RL 的安全性和解释性；在城市交叉口高风险场景，需要更安全且可解释的策略。

Method: 提出结构化因果模型来预测在执行不同行动后的结果，设计 CF 模块并与额外的 X 模块整合，形成 CFLight 算法；核心问题是“当发生不安全事件时，回退并选择替代行动，之后期是否仍会发生不安全事件？”通过框架实现回溯与评估，进行近零碰撞的控制策略；在真实与合成数据集上进行广泛实验，数据与代码在 GitHub。

Result: CFLight 在减少碰撞、提升整体交通性能方面优于传统 RL 方法和最近的安全 RL 模型；证明了在现实世界和仿真数据上的有效性，具有较强鲁棒性和解释性，且展现出一种通用且安全的 RL 框架的潜力。

Conclusion: CFLight 提供一个通用且安全的 RL 框架，适用于交通信号控制之外的其他领域；通过 CF 学习的引入增强安全性和可解释性，并有望推广至其他领域。

Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.

</details>


### [67] [Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs](https://arxiv.org/abs/2512.09369)
*Yezi Liu,William Youngwoo Chung,Hanning Chen,Calvin Yeung,Mohsen Imani*

Main category: cs.LG

TL;DR: PathHD提出一种无编码器的知识图谱推理框架，使用超维计算(HDC)对关系路径进行编码，单次将LLM作为 adjudication 以输出最终答案及证据路径，显著提升效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的LLM推理往往依赖重型神经编码器来嵌入并评估符号路径，或通过多次调用LLM来排序候选，导致高延迟、显存开销以及难以追踪的决策过程。需要更高效、可解释且可扩展的KG-LLM推理方案。

Method: PathHD通过三大要素实现： (i) 采用有序且非交换的绑定算子实现路径组合； (ii) 使用经过校准的相似性在超向量层面进行鲁棒检索； (iii) 进行一次性LLM裁决以给出最终答案及带证据的路径，避免逐路径得分。具体实现包括将关系路径编码为块对角的GHRR超向量、以分块余弦相似性进行排序和Top-K裁剪、以及单次LLM问答以产生带有证据的结果。

Result: 在WebQSP、CWQ、GrailQA数据集上，PathHD在一次LLM调用的前提下达到与强基线相当或更优的Hits@1；端到端延迟降低约40-60%，显存降低约3-5倍，且能够提供可解释的、路径关联的推理证据以帮助错误诊断与控制。

Conclusion: 通过精心设计的HDC表示，KG-LLM推理在准确性、效率和可解释性之间达到更优的权衡，展示了无编码器的高效KG推理的新范式。

Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\%$ and GPU memory by $3-5\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.

</details>


### [68] [Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM](https://arxiv.org/abs/2512.09378)
*Xun Li,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出一种基于轻量级去噪扩散概率模型（LDPM）的联邦蒸馏辅助车辆边缘缓存方案，鲁棒性强且显著降低通信开销，提升缓存命中率。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在隐私保护方面具有优势，但需要频繁的模型传输，通信开销大且车辆可能在训练期间离开RSU覆盖区导致训练失败，需在隐私保护与通信效率之间取得平衡。

Method: 提出基于LDPM的联邦蒸馏辅助车辆边缘缓存框架：在边缘节点进行本地训练后，利用蒸馏方式汇聚知识，而非直接传输原始模型，结合LDPM生成高效、隐私友好的内容兴趣分布预测，并据此设计缓存策略以降低延迟与提高命中率，系统对车辆速度变化具有鲁棒性。

Result: 通过仿真/仿真场景验证，所提方案对车辆速度变化具有良好鲁棒性，显著降低通信开销，并提升缓存命中率。

Conclusion: 联邦蒸馏结合LDPM为车辆边缘缓存提供一种高效、隐私友好且对移动性鲁棒的解决方案，优于传统FL在通信成本与掉线容忍方面的局限性。

Abstract: Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.

</details>


### [69] [Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting](https://arxiv.org/abs/2512.09398)
*Hongjun Wang,Jiawei Yong,Jiawei Wang,Shintaro Fukushima,Renhe Jiang*

Main category: cs.LG

TL;DR: 提出 ConFormer 的交通预测框架，并通过东京与加州数据集整合事故与规制信息，结合图传播与引导归一化，动态调整时空关系，优于 STAEFormer，且更高效。


<details>
  <summary>Details</summary>
Motivation: 外部因素如交通事故与法规影响交通，但现有模型对数据整合不足，需更丰富的数据集与模型以提升预测准确性和效率。

Method: 构建两个融合事故与规制信息的数据集；提出 ConFormer（Conditional Transformer），在图传播基础上加入引导归一化层，基于历史模式动态调整时空节点关系。

Result: 在预测性能与效率上均超越 STAEFormer，具有更低的计算成本与更少的参数；在多项指标上对主流时空基线保持一致优势。

Conclusion: 数据集增强与改进的模型有望推动交通预测领域的发展，未来工作可进一步扩展外部因素的整合与模型的可扩展性。

Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.

</details>


### [70] [Cauchy-Schwarz Fairness Regularizer](https://arxiv.org/abs/2512.09467)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出一种基于 Cauchy–Schwarz 发散的公平正则化方法（CS 正则化），以统一现有在制正则化的性质并提升稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有的公平正则化方法因距离度量多样且设计选择不同，导致难以推理行为和跨任务表现不一致。需要基于一组可控属性的性质来指导正则化设计，建立一个理论上更稳健、适用性更广的正则化框架。

Method: 提出 CS 公平正则化器，惩罚预测分布在敏感属性条件下的经验 CS 发散。给出高斯对比下的理论界限，表明在高斯近似下 CS 发散的界限比 KL、MMD、以及基于 Demographic Parity 的均值差更紧凑。进一步给出一个分布不依赖的、基于核的估计方法，能够自然扩展到多种敏感属性。通过在四个表格数据集和一个图像数据集上的大量实验，评估该正则化对 Demographic Parity 和 Equal Opportunity 的提升，同时保持竞争力的准确率，并在超参数设置下展现更稳定的公平-效用权衡。

Result: 在实证部分，CS 正则化在四个表格数据集和一个图像数据集上，系统性地提升了 Demographic Parity 与 Equal Opportunity 指标，且保持了与基线方法相似的分类准确率。相较于先前的正则化方法，CS 正则在超参数敏感性方面表现出更稳定的权衡，即在不同正则强度下仍能维持良好公平性与准确性之间的折中。

Conclusion: CS 正则化提供了理论与实验两方面的优势，成为设计公平正则化的一种更稳健、可扩展的选择。其分布无关的核估计与对多属性的扩展能力，使其具备广泛应用前景，且与现有方法相比在稳定性和一致性方面具有明确优势。

Abstract: Group fairness in machine learning is often enforced by adding a regularizer that reduces the dependence between model predictions and sensitive attributes. However, existing regularizers are built on heterogeneous distance measures and design choices, which makes their behavior hard to reason about and their performance inconsistent across tasks. This raises a basic question: what properties make a good fairness regularizer? We address this question by first organizing existing in-process methods into three families: (i) matching prediction statistics across sensitive groups, (ii) aligning latent representations, and (iii) directly minimizing dependence between predictions and sensitive attributes. Through this lens, we identify desirable properties of the underlying distance measure, including tight generalization bounds, robustness to scale differences, and the ability to handle arbitrary prediction distributions. Motivated by these properties, we propose a Cauchy-Schwarz (CS) fairness regularizer that penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Under a Gaussian comparison, we show that CS divergence yields a tighter bound than Kullback-Leibler divergence, Maximum Mean Discrepancy, and the mean disparity used in Demographic Parity, and we discuss how these advantages translate to a distribution-free, kernel-based estimator that naturally extends to multiple sensitive attributes. Extensive experiments on four tabular benchmarks and one image dataset demonstrate that the proposed CS regularizer consistently improves Demographic Parity and Equal Opportunity metrics while maintaining competitive accuracy, and achieves a more stable utility-fairness trade-off across hyperparameter settings compared to prior regularizers.

</details>


### [71] [Representation Invariance and Allocation: When Subgroup Balance Matters](https://arxiv.org/abs/2512.09496)
*Anissa Alloula,Charles Jones,Zuzanna Wakefield-Skorniewska,Francesco Quinzan,Bartłomiej Papież*

Main category: cs.LG

TL;DR: Balancing subgroup representation in training data is not universally beneficial; a latent separation hypothesis links a model's sensitivity to subgroup representation to how well subgroups are separated in the latent space of a pre-trained model, enabling principled data collection and balancing decisions for foundation model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Common wisdom assumes that balancing subgroup representation yields optimal generalisation across populations, but recent empirical results show that imbalanced data can sometimes improve subgroup performance or that missing an entire subgroup may not hurt others. A systematic framework is needed to understand when and why data balance helps.

Method: Systematic study across four vision and language models by varying training data composition to assess subgroup performance sensitivity. Formal development of the latent separation hypothesis, including theoretical analysis and empirical validation. Practical application to foundation model fine-tuning, using latent subgroup separation metrics to guide data collection and balancing decisions.

Result: Subgroup performance is not monotonically improved by balancing; in some cases, imbalance can enhance performance for certain subgroups, while the absence of a subgroup may have limited impact on others. The latent separation between subgroups in the pre-trained model's latent space predicts sensitivity to subgroup representation. Empirical validation across multiple models supports the hypothesis, and a practical metric is proposed to inform data-balancing decisions in fine-tuning.

Conclusion: The latent separation hypothesis provides a predictive framework to understand and control how data balance affects subgroup performance, with practical implications for data collection and balancing in foundation model fine-tuning.

Abstract: Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.

</details>


### [72] [Contextual Dynamic Pricing with Heterogeneous Buyers](https://arxiv.org/abs/2512.09513)
*Thodoris Lykouris,Sloan Nietert,Princewill Okoroafor,Chara Podimata,Julian Zimmert*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We initiate the study of contextual dynamic pricing with a heterogeneous population of buyers, where a seller repeatedly posts prices (over $T$ rounds) that depend on the observable $d$-dimensional context and receives binary purchase feedback. Unlike prior work assuming homogeneous buyer types, in our setting the buyer's valuation type is drawn from an unknown distribution with finite support size $K_{\star}$. We develop a contextual pricing algorithm based on optimistic posterior sampling with regret $\widetilde{O}(K_{\star}\sqrt{dT})$, which we prove to be tight in $d$ and $T$ up to logarithmic terms. Finally, we refine our analysis for the non-contextual pricing case, proposing a variance-aware zooming algorithm that achieves the optimal dependence on $K_{\star}$.

</details>


### [73] [QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder](https://arxiv.org/abs/2512.09517)
*Nabil Anan Orka,Ehtashamul Haque,Maftahul Jannat,Md Abdul Awal,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: QuanvNeXt 是一种端到端的全量量卷积模型，用于基于 EEG 的抑郁诊断，具有跨残差块以提升特征关系且参数高效。在两个公开数据集上达到领先性能，同时对高斯噪声下的预测进行不确定性分析并具后验可解释性，能识别区分健康对照与重度抑郁的谱时空模式，优于 InceptionTime。


<details>
  <summary>Details</summary>
Motivation: 解决 EEG 基于抑郁诊断的准确性、鲁棒性和可解释性不足的问题；通过引入跨特征关系强化与高效端到端结构，提高诊断性能并实现更好的不确定性校准和可解释性。

Method: 提出 QuanvNeXt，采用端到端的全量量卷积结构，并引入 Cross Residual 块以降低特征同质性、增强跨特征关系。对两个公开数据集进行评估，比较 InceptionTime；进行高斯噪声下的不确定性分析（ECE 指标），以及后验可解释性分析以识别区分健康与抑郁的谱时空模式。

Result: 在两数据集上平均准确率 93.1%、平均 AUC-ROC 97.2%，优于 InceptionTime 的 91.7%/95.9%。不确定性分析显示 ECE 较低（数据集1 0.0436，数据集2 0.1159，ε=0.1 时），预测校准良好。后验可解释性分析证实 QuanvNeXt 能学习区分健康人群与重度抑郁的谱时空模式。

Conclusion: QuanvNeXt 提供了一种高效、可靠的 EEG 基于抑郁诊断的方法，在特征关系建模与可解释性方面具有显著优势。

Abstract: This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (ε = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis.

</details>


### [74] [Latent-Autoregressive GP-VAE Language Model](https://arxiv.org/abs/2512.09535)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 提出了一种将时间序列动力学迁移至连续潜在空间的完全潜在自回归模型：在高斯过程（GP）驱动的变分自编码器（VAE）中实现，语言生成通过非自回归解码器并行完成；在正则化ELBO框架下给出方法学与训练流程，且在受控的POC评估中，模型稳定训练，序列与并行采样行为一致，表明语言模型的部分时间结构可由潜在空间的几何概率结构支撑。


<details>
  <summary>Details</summary>
Motivation: 旨在解决语言模型中的时间结构建模问题，通过将序列动力学放到潜在连续空间中，由GP先验把控，从而实现并行的语言生成，减少对显式神经自回归操作的依赖，并在受控环境下验证方法的可训练性与行为一致性。

Method: 提出一个完全潜在自回归框架，核心是基于GP的因果先验、结构化的近似后验以及基于正则化ELBO的训练协议。具体包括：将序列动力学从观测空间转移至潜在空间、使用非自回归解码器实现并行语言生成、设计结构化的近似后验以配合GP先验、以及在POC框架下进行稳定性与一致性评估。

Result: 通过POC实验表明模型可以稳定训练，序列采样与并行采样的行为在不同变体中表现出一致性，证明潜在空间的几何结构可部分承担语言模型的时间结构。

Conclusion: 结果提示部分时间结构可以由潜在空间的概率几何来支撑，而非完全依赖明确的神经自回归操作，从而为将来更高效、可解释的语言生成模型提供新路径。

Abstract: We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.

</details>


### [75] [Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models](https://arxiv.org/abs/2512.09591)
*Magnus Ruud Kjaer,Rahul Thapa,Gauri Ganjoo,Hyatt Moore,Poul Joergen Jennum,Brandon M. Westover,James Zou,Emmanuel Mignot,Bryan He,Andreas Brink-Kjaer*

Main category: cs.LG

TL;DR: 提出了 Stanford Sleep Bench，这是一组大型 PSG 数据集，并系统评估自监督表征学习（SSRL）在睡眠分析中的预训练方法，在四个下游任务上比较表现，结果显示对大多数任务多方法表现相近，但对死亡率和疾病预测，对比学习优于其他方法且收敛更快，同时将公开数据、权重和评估代码以促进重复性研究。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个涵盖多任务的大规模共享数据集与基准，以及对睡眠相关任务中 SSRL 方法的系统评估，阻碍睡眠基础模型的开发与比较。

Method: 构建 Stanford Sleep Bench：包含 17,467 份 PSG 记录，总时长超过 16.3 万小时，来自一家大型睡眠诊所。数据涵盖 13 项临床疾病预测任务，以及睡眠分期、呼吸暂停诊断、年龄估计等标准任务。系统性地对多种 SSRL 预训练方法在四个下游任务上的表现进行评估。

Result: 在睡眠分期、呼吸暂停诊断、年龄估计等任务上，多种预训练方法的表现相近；在死亡率和疾病预测任务中，对比学习显著优于其他方法，且预训练收敛更快。

Conclusion: 将公开 Stanford Sleep Bench、预训练模型权重、训练流程和评估代码，以提升可重复性并推动睡眠研究的发展。

Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.

</details>


### [76] [Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks](https://arxiv.org/abs/2512.09621)
*Jingbo Zhang,Maoxin Ji,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen*

Main category: cs.LG

TL;DR: 提出了三方协同语义通信（TCSC）框架，在高速公路IoV中通过V2I/V2V实现语义任务卸载；将问题建模为MINLP并分解为两子问题：基于MAPPO-PDN的语义符号数量优化与LP的卸载比例优化，仿真结果显示优于对比算法。


<details>
  <summary>Details</summary>
Motivation: 在物联网车辆场景中，语义通信与边缘计算的结合可显著降低任务时延和带宽压力，尤其在高速度、低时延要求的公路场景需要高效的任务分发与资源分配方案。

Method: 提出Tripartite Cooperative Semantic Communication (TCSC)框架，允许车辆用户通过V2I与V2V进行语义任务卸载。将优化问题建模为混合整数非线性规划（MINLP），并分解为两部分：1) 使用基于参数分布噪声的多智能体近端策略优化（MAPPO-PDN）求解语义符号数量；2) 使用线性规划（LP）求解卸载比例。

Result: 仿真结果表明该方案在相较于其他算法时具有更优的表现，证明了MAPPO-PDN与LP联合求解在TCSC框架中的有效性。

Conclusion: TCSC框架通过V2I/V2V协同实现高效的语义任务卸载，提升IoV场景下的边缘计算性能，尤其在高速公路等场景下具有显著优势。

Abstract: Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.

</details>


### [77] [Membership and Dataset Inference Attacks on Large Audio Generative Models](https://arxiv.org/abs/2512.09654)
*Jakub Proboszcz,Paweł Kochanski,Karol Korszun,Donato Crisostomi,Giorgio Strano,Emanuele Rodolà,Kamil Deja,Jan Dubinski*

Main category: cs.LG

TL;DR: 数据集推断（DI）在音频生成模型的版权保护中比单样本成员推断更具实用性。


<details>
  <summary>Details</summary>
Motivation: 应对生成音频模型在版权领域的潜在滥用，需可验证证据来判断艺术家作品是否用于训练。

Method: 在开源音频生成模型上进行成员推断攻击（MIA）；并聚合多样样本的证据形成数据集推断（DI）；比较DI与MIA的有效性。

Result: 单样本MIA在大规模多样数据集上信号薄弱；DI能通过跨样本证据实现对艺术家作品是否参与训练的推断。

Conclusion: DI对版权保护和数据集可追溯性具发展潜力，值得在音频领域继续研究。

Abstract: Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.

</details>


### [78] [Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power](https://arxiv.org/abs/2512.09673)
*Yuzhu Chen,Tian Qin,Xinmei Tian,Fengxiang He,Dacheng Tao*

Main category: cs.LG

TL;DR: Equivariant networks can restrict expressivity in small models, but increasing capacity can recover expressivity and even reduce overall hypothesis space complexity, potentially improving generalization.


<details>
  <summary>Details</summary>
Motivation: To understand how symmetry-induced inductive bias (equivariance) affects the expressive power of neural networks, particularly for 2-layer ReLU nets, and to compare fully equivariant versus layer-wise equivariant architectures.

Method: Analyze the geometry of 2-layer ReLU networks by examining boundary hyperplanes and channel vectors; construct a counterexample showing expressivity limitation due to equivariance; then show that enlarging the model size compensates this drawback; assess the resulting hypothesis space complexity.

Result: Demonstrates that equivariance constraints can strictly limit expressivity in small networks, but this limitation can be mitigated by larger models; even with larger architecture, the corresponding hypothesis space can have lower complexity, suggesting better generalization for equivariant networks.

Conclusion: Equivariance may constrain expressivity at small scales, but with increased capacity, it can preserve or enhance expressivity while yielding a simpler hypothesis space and improved generalization; thus the benefits of equivariant designs are eventuated through appropriate model sizing.

Abstract: Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.

</details>


### [79] [A data-driven approach to linking design features with manufacturing process data for sustainable product development](https://arxiv.org/abs/2512.09690)
*Jiahang Li,Lucas Cazzonelli,Jacqueline Höllig,Markus Doellken,Sven Matthiesen*

Main category: cs.LG

TL;DR: 面向工业物联网的设计-制造数据联动：建立一个数据驱动的框架，通过将设计特征与制造过程数据关联，提供自动化的设计改进建议，并结合可持续性指标促进可持续产品开发。


<details>
  <summary>Details</summary>
Motivation: 设计决策对制造结果（如错误率、能耗、加工时间）具有重要影响，但现有方法多在单一领域（设计或制造）内应用，缺乏跨设计与制造数据的整合以提升数据驱动的产品设计与治理效率，因此需要一个能持续收集并整合设计特征与制造过程数据的系统。

Method: 提出一个全面的系统架构，确保持续的数据收集与集成；建立设计特征与制造过程数据之间的联动关系；以该联动作为基础开发一个机器学习模型，能够给出自动化的设计改进建议；并将制造过程数据与可持续性指标进行整合以扩展到可持续产品开发。

Result: 提出一个数据驱动的框架与体系结构草案，并建立设计特征与制造过程数据的联动基础，开发用于自动化设计改进建议的机器学习模型；提出将可持续性指标纳入数据驱动设计优化的思路与潜在收益。尚无给出具体的经验性验证结果。

Conclusion: 通过在IIoT环境中将制造过程数据与设计特征及可持续性指标进行整合，本文提出的框架可实现数据驱动的设计优化与可持续产品开发的新路径；该方法具有提升制造效率、减少资源消耗与环境影响的潜在前景。

Abstract: The growing adoption of Industrial Internet of Things (IIoT) technologies enables automated, real-time collection of manufacturing process data, unlocking new opportunities for data-driven product development. Current data-driven methods are generally applied within specific domains, such as design or manufacturing, with limited exploration of integrating design features and manufacturing process data. Since design decisions significantly affect manufacturing outcomes, such as error rates, energy consumption, and processing times, the lack of such integration restricts the potential for data-driven product design improvements. This paper presents a data-driven approach to mapping and analyzing the relationship between design features and manufacturing process data. A comprehensive system architecture is developed to ensure continuous data collection and integration. The linkage between design features and manufacturing process data serves as the basis for developing a machine learning model that enables automated design improvement suggestions. By integrating manufacturing process data with sustainability metrics, this approach opens new possibilities for sustainable product development.

</details>


### [80] [Mixture of Lookup Key-Value Experts](https://arxiv.org/abs/2512.09723)
*Zongcheng Wang*

Main category: cs.LG

TL;DR: MoLKV 在 MoLE 的基础上引入带有键值对的专家，利用输入相关的查询与当前序列的缓存键值对进行上下文感知的专家输出，从而缓解 MoLE 的上下文无关选择限制，并在小规模评估中显著降低验证损失。


<details>
  <summary>Details</summary>
Motivation: MoLE 的核心局限在于专家选择仅基于输入 token id，缺乏上下文信息，可能限制模型性能，且对于资源受限设备需要将大部分专家放置在外部存储。需要一种在设备上保持低 RAM 使用、同时具备上下文感知能力的专家机制。

Method: 提出 MoLKV：将每个专家设计为一个键值对。给定输入时，输入派生的查询与当前序列中的缓存键值对交互，产生上下文感知的专家输出。通过对激活的少量专家进行加载，结合缓存机制，实现在资源受限设备上的高效推断。

Result: 在小规模评估中，MoLKV 显著降低了验证损失，显示出相较于 MoLE 的性能提升。

Conclusion: 通过将专家设计为上下文相关的键值对，并利用查询-键值的交互，MoLKV 成功缓解了 MoLE 的上下文无关局限，适合在资源受限设备上进行高效推断，同时保持低 RAM 需求。

Abstract: Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.

</details>


### [81] [Circuits, Features, and Heuristics in Molecular Transformers](https://arxiv.org/abs/2512.09757)
*Kristof Varadi,Mark Marosi,Peter Antal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.

</details>


### [82] [Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems](https://arxiv.org/abs/2512.09780)
*Aoxiang Ma,Salah Ghamizi,Jun Cao,Pedro Rodriguez*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Battery energy storage systems (BESS) have become increasingly vital in three-phase unbalanced distribution grids for maintaining voltage stability and enabling optimal dispatch. However, existing deep learning approaches often lack explicit three-phase representation, making it difficult to accurately model phase-specific dynamics and enforce operational constraints--leading to infeasible dispatch solutions. This paper demonstrates that by embedding detailed three-phase grid information--including phase voltages, unbalanced loads, and BESS states--into heterogeneous graph nodes, diverse GNN architectures (GCN, GAT, GraphSAGE, GPS) can jointly predict network state variables with high accuracy. Moreover, a physics-informed loss function incorporates critical battery constraints--SoC and C-rate limits--via soft penalties during training. Experimental validation on the CIGRE 18-bus distribution system shows that this embedding-loss approach achieves low prediction errors, with bus voltage MSEs of 6.92e-07 (GCN), 1.21e-06 (GAT), 3.29e-05 (GPS), and 9.04e-07 (SAGE). Importantly, the physics-informed method ensures nearly zero SoC and C-rate constraint violations, confirming its effectiveness for reliable, constraint-compliant dispatch.

</details>


### [83] [Knowledge Diversion for Efficient Morphology Control and Policy Transfer](https://arxiv.org/abs/2512.09796)
*Fu Feng,Ruixiao Shi,Yucheng Xie,Jianlu Shen,Jing Wang,Xin Geng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \textit{learngenes} and morphology- and task-specific \textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\times$ reduction in model size for single-agent deployment.

</details>


### [84] [Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers](https://arxiv.org/abs/2512.09800)
*Zhaolan Huang,Kaspar Schleiser,Gyungmin Myung,Emmanuel Baccelli*

Main category: cs.LG

TL;DR: Ariel-ML is a Rust-based embedded toolkit that enables parallel inference on multi-core 32-bit MCUs for TinyML models, benchmarking shows improved latency over prior work with similar memory footprints to C/C++ toolkits.


<details>
  <summary>Details</summary>
Motivation: The embedded landscape is shifting to multi-core MCUs and Rust, while TinyML grows on edge devices. There is a lack of a Rust-based embedded platform that automates parallelization of inference on multi-core MCUs for arbitrary TinyML models.

Method: Develop Ariel-ML by integrating a generic TinyML pipeline with an embedded Rust software platform that exploits multi-core capabilities across Arm Cortex-M, RISC-V, and ESP-32. The project is open source and evaluated using a zoo of TinyML models to benchmark performance.

Result: Ariel-ML outperforms prior art in inference latency and achieves comparable memory footprints to existing embedded C/C++ toolkits.

Conclusion: Ariel-ML provides a practical foundation for TinyML practitioners and resource-constrained embedded Rust developers, bridging Rust-based embedded software with parallelized TinyML inference on multi-core MCUs.

Abstract: Low-power microcontroller (MCU) hardware is currently evolving from single-core architectures to predominantly multi-core architectures. In parallel, new embedded software building blocks are more and more written in Rust, while C/C++ dominance fades in this domain. On the other hand, small artificial neural networks (ANN) of various kinds are increasingly deployed in edge AI use cases, thus deployed and executed directly on low-power MCUs. In this context, both incremental improvements and novel innovative services will have to be continuously retrofitted using ANNs execution in software embedded on sensing/actuating systems already deployed in the field. However, there was so far no Rust embedded software platform automating parallelization for inference computation on multi-core MCUs executing arbitrary TinyML models. This paper thus fills this gap by introducing Ariel-ML, a novel toolkit we designed combining a generic TinyML pipeline and an embedded Rust software platform which can take full advantage of multi-core capabilities of various 32bit microcontroller families (Arm Cortex-M, RISC-V, ESP-32). We published the full open source code of its implementation, which we used to benchmark its capabilities using a zoo of various TinyML models. We show that Ariel-ML outperforms prior art in terms of inference latency as expected, and we show that, compared to pre-existing toolkits using embedded C/C++, Ariel-ML achieves comparable memory footprints. Ariel-ML thus provides a useful basis for TinyML practitioners and resource-constrained embedded Rust developers.

</details>


### [85] [Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering](https://arxiv.org/abs/2512.09810)
*Adithya K Moorthy,V Vijaya Saradhi,Bhanu Prasad*

Main category: cs.LG

TL;DR: 提出了在图构建阶段实现公平性的新方法，针对 kNN 和 epsilon-neighborhood 图进行公平化设计，以在谱聚类中实现人口统计公平性；通过在最近邻筛选阶段嵌入公平约束来保持几何一致性，提升聚类的公平性且无需改动聚类算法。


<details>
  <summary>Details</summary>
Motivation: 传统图聚类方法在构造图的过程中可能对敏感群体产生偏置，导致不公平的聚类结果。因此，需要在图的最早阶段引入公平约束，以在局部邻域中实现对敏感特征的比例代表性，从而获得更公平的谱聚类。

Method: 提出公平 kNN 和公平 epsilon-neighborhood 图，在构图阶段强制人口统计平等（demographic parity）约束，使各敏感特征组在每个节点的邻域中按比例得到表示；同时保持局部几何的一致性，确保图的拓扑特征能反映公平的群体比例。

Result: 在三个合成数据集、七个真实表格数据集和三个真实图像数据集上进行了大量实验，所提出的公平图构建方法在图聚类任务上超越了当前基线。

Conclusion: 图构造阶段的拓扑公平性对实现公平谱聚类至关重要，公平预处理足以带来更公平的聚类结果，无需修改聚类算法本身。

Abstract: Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.

</details>


### [86] [Predicting the Containment Time of California Wildfires Using Machine Learning](https://arxiv.org/abs/2512.09835)
*Shashank Bhardwaj*

Main category: cs.LG

TL;DR: 将野火持续时间视为回归问题，比较随机森林、XGBoost、LSTM等模型在FRAP数据集上的预测效果，结果显示XGBoost在静态特征上最佳，LSTM因缺乏时间特征而表现不佳。


<details>
  <summary>Details</summary>
Motivation: 应对加剧的加州野火，提供更准确、可操作的持续时间预测以优化资源分配和应急响应；现有研究多关注风险与蔓延，较少对持续时间进行连续量化预测。

Method: 使用三份FRAP公开数据集，构建并比较基线集成回归、随机森林、XGBoost，以及LSTM模型；评估在不同特征可用性下的预测性能。

Result: XGBoost略优于随机森林，可能因为其对静态特征的处理更强；LSTM表现 inferior，原因是数据集中缺乏时间序列特征。

Conclusion: 根据可用特征，野火管理者可选择最合适的模型以实现更准确的持续时间预测和资源分配，提升应急效率。

Abstract: California's wildfire season keeps getting worse over the years, overwhelming the emergency response teams. These fires cause massive destruction to both property and human life. Because of these reasons, there's a growing need for accurate and practical predictions that can help assist with resources allocation for the Wildfire managers or the response teams. In this research, we built machine learning models to predict the number of days it will require to fully contain a wildfire in California. Here, we addressed an important gap in the current literature. Most prior research has concentrated on wildfire risk or how fires spread, and the few that examine the duration typically predict it in broader categories rather than a continuous measure. This research treats the wildfire duration prediction as a regression task, which allows for more detailed and precise forecasts rather than just the broader categorical predictions used in prior work. We built the models by combining three publicly available datasets from California Department of Forestry and Fire Protection's Fire and Resource Assessment Program (FRAP). This study compared the performance of baseline ensemble regressor, Random Forest and XGBoost, with a Long Short-Term Memory (LSTM) neural network. The results show that the XGBoost model slightly outperforms the Random Forest model, likely due to its superior handling of static features in the dataset. The LSTM model, on the other hand, performed worse than the ensemble models because the dataset lacked temporal features. Overall, this study shows that, depending on the feature availability, Wildfire managers or Fire management authorities can select the most appropriate model to accurately predict wildfire containment duration and allocate resources effectively.

</details>


### [87] [Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime](https://arxiv.org/abs/2512.09850)
*Simone Cuonzo,Nina Deliu*

Main category: cs.LG

TL;DR: 提出 Conformal Bandits 框架，将 Conformal Prediction 融合进 bandit 问题，兼顾 regret 与有限样本覆盖性质。


<details>
  <summary>Details</summary>
Motivation: 现有的贪婪/探索策略（如 Thompson Sampling、UCB）常依赖分布假设或渐近保证，难以在有限样本下同时提供统计覆盖，且对小差距情形的性能不足。

Method: 将 Conformal Prediction 融入 bandit 策略，提供可控的覆盖概率的预测区间，并在探索-利用权衡中保持较低 regret；在具体应用中结合隐藏马尔可夫模型以捕捉市场的 regime-switching。

Result: 通过仿真和投资组合分配场景展示，在小-gap 设置中比传统策略更具性价比，并在覆盖性方面优于经典 UCB；引入隐马尔可夫模型提升探索-利用权衡和风险调整回报，同时保持覆盖保证。

Conclusion: Conformal Bandits 框架实现了 regret 与统计覆盖的兼容，且在现实场景（如金融市场的 regime-switching）中通过隐藏马尔可夫模型增强性能，同时保留覆盖属性。

Abstract: We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.
  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.

</details>


### [88] [HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression](https://arxiv.org/abs/2512.09886)
*Gustavo Coelho Haase,Paulo Henrique Dourado da Silva*

Main category: cs.LG

TL;DR: HPM-KD 是一个六组件的知识蒸馏框架，解决超参数依赖、容量差距、多教师协同与资源利用问题，提升压缩比同时维持准确性，训练更快，且开源在 DeepBridge 库中。


<details>
  <summary>Details</summary>
Motivation: 在知识蒸馏领域，超参数敏感性、教师与学生容量差距、多人教师协同的低效，以及计算资源利用不足，是当前知识蒸馏面临的核心挑战。

Method: 提出六个协同组件：自适应配置管理（元学习驱动，避免手动调参）、渐进蒸馏链（自动确定中间模型）、注意力加权多教师集成（按样本动态分配权重）、元学习温度调度（训练过程自适应温度）、并行处理管线（负载均衡实现并行化）、跨实验共享优化内存（复用优化状态）。

Result: 在 CIFAR-10、CIFAR-100 与表格数据集上实现 10x–15x 的模型压缩，同时保持约 85% 的精度保留；无需手动调参；通过并行化将训练时间缩短 30–40%；消融研究表明各组件的独立贡献（0.10–0.98 ppt）；框架作为开源 DeepBridge 库的一部分对外可用。

Conclusion: HPM-KD 提供一个可扩展且高效的知识蒸馏解决方案，在高压缩与稳定精度之间达到良好折衷，降低调参成本与训练时间；在多数据集上验证有效性，且可在 DeepBridge 中获取。

Abstract: Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.

</details>


### [89] [Analysis of Dirichlet Energies as Over-smoothing Measures](https://arxiv.org/abs/2512.09890)
*Anna Bison,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 归一化图拉普拉斯的 Dirichlet 能量不满足节点相似性公理；未归一化的能量与谱性质揭示两者的本质差异，并为选择与 GNN 架构谱兼容的度量提供指引。


<details>
  <summary>Details</summary>
Motivation: 澄清基于图拉普拉斯的过平滑度量中，哪一个度量与 GNN 的谱性质和节点相似性公理兼容，消除在监测动态时的歧义。

Method: 对未归一化与归一化图拉普拉斯下的 Dirichlet 能量进行形式化的谱性质分析，检验是否符合 Rusch 等人的节点相似性公理，揭示关键差异。

Result: 证明归一化拉普拉斯的 Dirichlet 能量不满足节点相似性公理；揭示两者在谱性质上的关键差异，提出在 GNN 架构中选择谱兼容度量的要点。

Conclusion: 应优先采用与谱结构兼容的度量；归一化处理改变能量的节点相似性性质，从而影响对动态监测的解读；此研究消除了相关模糊性。

Abstract: We analyze the distinctions between two functionals often used as over-smoothing measures: the Dirichlet energies induced by the unnormalized graph Laplacian and the normalized graph Laplacian. We demonstrate that the latter fails to satisfy the axiomatic definition of a node-similarity measure proposed by Rusch \textit{et al.} By formalizing fundamental spectral properties of these two definitions, we highlight critical distinctions necessary to select the metric that is spectrally compatible with the GNN architecture, thereby resolving ambiguities in monitoring the dynamics.

</details>


### [90] [Provably Learning from Modern Language Models via Low Logit Rank](https://arxiv.org/abs/2512.09892)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 提出并分析一个高效学习算法，用于从对数输出查询中学习近似低对数位秩的生成模型，为接近现代语言模型的结构提供首个端到端学习保证。


<details>
  <summary>Details</summary>
Motivation: 经验观察：现代语言模型的对数分布在不同 token 上呈现近似低秩结构，这一特性可能使学习问题变得可控。研究在真实 API 访问模型的对数输出查询下，能否获得可证明的学习保证。

Method: 建立一个带对数输出(query)的学习模型，定义“近似低对数位秩”假设；提出一种高效算法，利用该结构从对数输出查询中学习模型；对误差与秩的近似性给出分析。

Result: 提出的算法在查询下能够高效学习任意近似低对数位秩模型，理论上给出多项式时间/多项式样本量的学习保证；对接近现代 LMs 的结构有鲁棒性。

Conclusion: 为 plausibly 捕捉现代语言模型的生成模型提供首个端到端学习保证，支持低对数位秩假说的实用性及理论价值。

Abstract: While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.
  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.

</details>


### [91] [FALCON: Few-step Accurate Likelihoods for Continuous Flows](https://arxiv.org/abs/2512.09914)
*Danyal Rehman,Tara Akhound-Sadegh,Artem Gazizov,Yoshua Bengio,Alexander Tong*

Main category: cs.LG

TL;DR: FALCON 通过少步采样和可近似准确的似然估计，提升连续流模型在分子 Boltzmann 采样中的效率与可用性。


<details>
  <summary>Details</summary>
Motivation: 统计物理中对处于热力学平衡的分子状态进行高效抽样一直具有挑战性；现有的基于连续正则分布流(CNF)的 Boltzmann Generators 似然计算开销高，限制了实际应用。

Method: 提出 FALCON，一种在连续流训练中引入混合目标以增强可逆性，从而实现少步采样，同时保持足够的似然性以用于重要性采样；并显示其在分子 Boltzmann 采样中的有效性。

Result: FALCON 在分子 Boltzmann 采样任务中优于现有 CNF 模型，且在等效性能下速度提升约两个数量级（约100倍），显著高于当前最先进的流模型。

Conclusion: 该方法为分子系统的 Boltzmann 采样提供了一个更高效且可实际部署的解决方案，通过实现少步采样与可用的似然性，提升了生成模型在统计物理中的应用潜力。

Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.

</details>


### [92] [Closing the Train-Test Gap in World Models for Gradient-Based Planning](https://arxiv.org/abs/2512.09929)
*Arjun Parthasarathy,Nimit Kalra,Rohun Agrawal,Yann LeCun,Oumayma Bounou,Pavel Izmailov,Micah Goldblum*

Main category: cs.LG

TL;DR: 通过在训练阶段进行数据合成来缩小训练目标（下一状态预测）与测试阶段（序列动作优化）之间的差距，从而提升世界模型的梯度规划效率与性能，在多任务上以极低的时间预算接近或超越CEM。


<details>
  <summary>Details</summary>
Motivation: 当前基于世界模型的梯度规划在训练目标和测试时的使用之间存在错位，即训练以下一状态预测为目标，而测试时需估计一系列动作。为弥合此train-test gap，提出训练时的数据合成技术以显著提升梯度规划的性能。

Method: 在离线阶段通过大规模专家轨迹训练世界模型，并引入训练时的数据合成技巧，使模型在测试时更适合进行梯度基规划，从而提高效率与效果。

Result: 在对象操作和导航任务中，所提方法在测试阶段优于或等同于经典的梯度-free优化方法CEM，且以仅10%的时间预算达到该性能。

Conclusion: 训练阶段的数据合成能够有效缩小训练目标与测试时应用之间的差距，提升梯度基规划的性能与计算效率，使基于世界模型的规划更具实用性。

Abstract: World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.

</details>
