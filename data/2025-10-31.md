<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 7]
- [cs.LG](#cs.LG) [Total: 77]
- [eess.SY](#eess.SY) [Total: 9]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.CR](#cs.CR) [Total: 16]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Robust Super-Capacity SRS Channel Inpainting via Diffusion Models](https://arxiv.org/abs/2510.26097)
*Usman Akram,Fan Zhang,Yang Li,Haris Vikalo*

Main category: eess.SP

TL;DR: 使用扩散模型对通道进行修补，在分布偏移下显著优于MAE与UNet，提升5G NR非均匀SRS的鲁棒性与覆盖。


<details>
  <summary>Details</summary>
Motivation: 在5G NR中，基于上行SRS的互易性波束形成受资源与覆盖约束影响，需要稀疏非均匀SRS分配。传统MAE在训练掩码上易过拟合，对额外遮挡、干扰、裁剪和非高斯噪声等未见失真表现不稳。

Method: 提出一种扩散模型的通道修复框架，在推理阶段通过引入似然梯度项整合系统模型知识，使单一训练模型能够在匹配与不匹配条件下自适应。基于标准化的CDL通道进行评估。

Result: 在分布偏移条件下，分数基扩散变体持续优于UNet分数模型基线和单步MAE，且在挑战性设置（如Laplace噪声、用户干扰）下NMSE提升可达约14 dB，同时在匹配条件下保持竞争力。

Conclusion: 扩散引导的通道修复是一种对5G NR超容量SRS设计具有鲁棒性和泛化性的通用方法。

Abstract: Accurate channel state information (CSI) is essential for reliable multiuser
MIMO operation. In 5G NR, reciprocity-based beamforming via uplink Sounding
Reference Signals (SRS) face resource and coverage constraints, motivating
sparse non-uniform SRS allocation. Prior masked-autoencoder (MAE) approaches
improve coverage but overfit to training masks and degrade under unseen
distortions (e.g., additional masking, interference, clipping, non-Gaussian
noise). We propose a diffusion-based channel inpainting framework that
integrates system-model knowledge at inference via a likelihood-gradient term,
enabling a single trained model to adapt across mismatched conditions. On
standardized CDL channels, the score-based diffusion variant consistently
outperforms a UNet score-model baseline and the one-step MAE under distribution
shift, with improvements up to 14 dB NMSE in challenging settings (e.g.,
Laplace noise, user interference), while retaining competitive accuracy under
matched conditions. These results demonstrate that diffusion-guided inpainting
is a robust and generalizable approach for super-capacity SRS design in 5G NR
systems.

</details>


### [2] [6D Channel Knowledge Map Construction via Bidirectional Wireless Gaussian Splatting](https://arxiv.org/abs/2510.26166)
*Juncong Zhou,Chao Hu,Guanlin Wu,Zixiang Ren,Han Hu,Juyong Zhang,Rui Zhang,Jie Xu*

Main category: eess.SP

TL;DR: 提出六维（6D）CKM框架BiWGS，通过高斯椭圆体表示虚拟散射体和环境障碍，在动态Tx/Rx位置下建模无线信道；实验显示显著优于MLP的6DCKM构建，且3D CKM的预测达到接近WRF-GS的水平。


<details>
  <summary>Details</summary>
Motivation: 解决传统2D/3D CKM在固定基站配置下的局限性，提升在动态、3D场景中对无线信道的建模能力。

Method: 采用六维CKM框架BiWGS，使用高斯椭圆体来表示虚拟散射体簇和环境障碍，学习双向散射模式和复衰减曲线以捕捉电磁传输特性；在多样Tx/Rx位置下对信道进行编码，输出6D的信道功率增益与传播特性映射。

Result: 在6DCKM构建方面，BiWGS显著优于传统MLP基线；在3D CKM构建方面，BiWGS的空间谱预测精度接近于先端WRF-GS方法，验证了对6D CKM维度扩展的有效性但不损失保真度。

Conclusion: BiWGS为6D CKM的构建提供了一个高效且保真度高的框架，能够在动态Tx/Rx配置下对无线信道进行更全面的建模和预测。

Abstract: This paper investigates the construction of channel knowledge map (CKM) from
sparse channel measurements. Dif ferent from conventional
two-/three-dimensional (2D/3D) CKM approaches assuming fixed base station
configurations, we present a six-dimensional (6D) CKM framework named
bidirectional wireless Gaussian splatting (BiWGS), which is capable of mod
eling wireless channels across dynamic transmitter (Tx) and receiver (Rx)
positions in 3D space. BiWGS uses Gaussian el lipsoids to represent virtual
scatterer clusters and environmental obstacles in the wireless environment. By
properly learning the bidirectional scattering patterns and complex attenuation
profiles based on channel measurements, these ellipsoids inherently cap ture
the electromagnetic transmission characteristics of wireless environments,
thereby accurately modeling signal transmission under varying transceiver
configurations. Experiment results show that BiWGS significantly outperforms
classic multi-layer perception (MLP) for the construction of 6D channel power
gain map with varying Tx-Rx positions, and achieves spatial spectrum prediction
accuracy comparable to the state-of-the art wireless radiation field Gaussian
splatting (WRF-GS) for 3D CKM construction. This validates the capability of
the proposed BiWGS in accomplishing dimensional expansion of 6D CKM
construction, without compromising fidelity.

</details>


### [3] [Design of Orthogonal Phase of Arrival Positioning Scheme Based on 5G PRS and Optimization of TOA Performance](https://arxiv.org/abs/2510.26245)
*Juyeop Kim,Hyejin Shin,Sohee Kim,Ilmu Byun*

Main category: eess.SP

TL;DR: 通过对5GNR PRS定位的配置变化进行分析，提出低采样率条件下提高TOA精度的算法，并在软件定义调制解调器中实现5G PRS定位；研究PRS时频资源分配对TOA估计精度的影响，并给出给定信道环境下的最佳PRS配置。


<details>
  <summary>Details</summary>
Motivation: 在5G定位中，TOA基于PR S信号的到达时间，需要在受限资源（如低采样率）和可变资源分配的环境中保持高定位精度。本研究旨在提升低采样率下的TOA精度并优化PRS配置以适应不同信道环境。

Method: 提出一种在低采样率条件下提升TOA精度的算法；在软件定义调制解调器中实现5G PRS定位；系统性分析PRS的时频资源可配置性对TOA估计的影响，提出对于给定信号环境的最优PRS配置策略。

Result: 实现了在低采样率下TOA精度的提升，与5G PRS定位相关的算法在软件定义调制解调器中得到实现；揭示了PRS资源分配对TOA估计的影响，为不同信号环境给出配置方向。

Conclusion: 研究表明，合理的PRS时间-频率资源配置和有效的TOA改进算法可以在受限条件下显著提高5G定位的TOA精度，提供了在实际软硬件平台上实施5G PRS定位的可行路径。

Abstract: This study analyzes the performance of positioning techniques based on
configuration changes of 5G New Radio signals. In 5G networks, a terminal
position is determined from the Time of Arrival of Positioning Reference
Signals transmitted by base stations. We propose an algorithm that improves TOA
accuracy under low sampling rate constraints and implement 5G PRS for
positioning in a software defined modem. We also examine how flexible time
frequency resource allocation of PRS affects TOA estimation accuracy and
discuss optimal PRS configurations for a given signal environment.

</details>


### [4] [Optimal transmit field distribution for partially obstructed continuous radiating surfaces in near-field communication systems](https://arxiv.org/abs/2510.26262)
*Francesco Verde,Donatella Darsena,Marco Di Renzo,Vincenzo Galdi*

Main category: eess.SP

TL;DR: 在遮挡环境中的近场通信中，通过基于 knife-edge 衍射的物理一致性建模，将孔径场优化问题在希尔伯特空间中表述为极大化，并得到与衍射核形状匹配的最优解，便于用连续孔径（如超表面/透镜天线）实现硬件化。


<details>
  <summary>Details</summary>
Motivation: 旨在在遮挡的近场信道中实现高效能量聚焦，将波传播物理规律与信号处理方法耦合，通过统一的理论框架支持可实际实现的连续孔径天线结构。

Method: 以 knife-edge 衍射为基础建立物理模型，將孔径场优化问题投影到希尔伯特空间，导出最优解为匹配衍射核的滤波器（匹配滤波器）。

Result: 给出一个可实施的框架，理论上最优解为匹配衍射核的滤波器，强调波传播与信号处理的耦合及对连续孔径的硬件实现（如超表面、透镜天线）的可行性。

Conclusion: 该方法把物理建模、信号处理和硬件设计结合起来，在遮挡近场通道中实现高效能量聚焦。

Abstract: This paper deals with the optimal synthesis of aperture fields for
(radiating) near-field communications in obstructed environments. A physically
consistent model based on knife-edge diffraction is used to formulate the
problem as a maximization in Hilbert space. The optimal solution is obtained as
a matched filter that ``matches" the shape of a diffraction-induced kernel,
thus linking wave propagation with signal processing methods. The framework
supports hardware implementation using continuous apertures such as
metasurfaces or lens antennas. This approach bridges physically grounded
modeling, signal processing, and hardware design for efficient energy focusing
in near-field obstructed channels.

</details>


### [5] [SABER: Symbolic Regression-based Angle of Arrival and Beam Pattern Estimator](https://arxiv.org/abs/2510.26340)
*Shih-Kai Chou,Mengran Zhao,Cheng-Nan Hu,Kuang-Chung Chou,Carolina Fortuna,Jernej Hribar*

Main category: eess.SP

TL;DR: SABER: a constrained symbolic-regression framework for AoA and beam-pattern estimation that delivers interpretable closed-form models with high accuracy, achieving sub-0.5° MAE in a chamber and near-zero error in RIS indoor tests, competitive with CRLB.


<details>
  <summary>Details</summary>
Motivation: Accurate AoA estimation is critical for beamforming, localization, and sensing, but traditional high-resolution methods demand large arrays and ML models are opaque. There is a need for interpretable, physics-informed estimators.

Method: Develop SABER: constrained symbolic regression to discover closed-form beam-pattern and AoA models from path-loss measurements; compare with unconstrained SR; validate in an anechoic chamber and a RIS-aided indoor testbed; benchmark against CRLB.

Result: Sub-0.5 degree MAE for cos^n and low-order polynomial surrogates; unconstrained SR reduces angle error but yields complex formulas; SABER recovers AoA with near-zero error in RIS testbed; SABER competitive with CRLB and superior interpretability.

Conclusion: Symbolic regression-based approaches can bridge ML and physics-driven estimators, offering interpretable and accurate AoA/beam-pattern models suitable for practical RIS-assisted systems.

Abstract: Accurate Angle-of-arrival (AoA) estimation is essential for next-generation
wireless communication systems to enable reliable beamforming, high-precision
localization, and integrated sensing. Unfortunately, classical high-resolution
techniques require multi-element arrays and extensive snapshot collection,
while generic Machine Learning (ML) approaches often yield black-box models
that lack physical interpretability. To address these limitations, we propose a
Symbolic Regression (SR)-based ML framework. Namely, Symbolic Regression-based
Angle of Arrival and Beam Pattern Estimator (SABER), a constrained
symbolic-regression framework that automatically discovers closed-form beam
pattern and AoA models from path loss measurements with interpretability. SABER
achieves high accuracy while bridging the gap between opaque ML methods and
interpretable physics-driven estimators. First, we validate our approach in a
controlled free-space anechoic chamber, showing that both direct inversion of
the known $\cos^n$ beam and a low-order polynomial surrogate achieve sub-0.5
degree Mean Absolute Error (MAE). A purely unconstrained SR method can further
reduce the error of the predicted angles, but produces complex formulas that
lack physical insight. Then, we implement the same SR-learned inversions in a
real-world, Reconfigurable Intelligent Surface (RIS)-aided indoor testbed.
SABER and unconstrained SR models accurately recover the true AoA with
near-zero error. Finally, we benchmark SABER against the Cram\'er-Rao Lower
Bounds (CRLBs). Our results demonstrate that SABER is an interpretable and
accurate alternative to state-of-the-art and black-box ML-based methods for AoA
estimation.

</details>


### [6] [HMM for short independent sequences: Multiple sequence Baum-Welch application](https://arxiv.org/abs/2510.26532)
*Margarita Cabrera-Bean,Josep Vidal,Sergio Fernandez-Bertolin,Albert Roso-Llorach,Concepcion Violan*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the classical setting, the training of a Hidden Markov Model (HMM)
typically relies on a single, sufficiently long observation sequence that can
be regarded as representative of the underlying stochastic process. In this
context, the Expectation Maximization (EM) algorithm is applied in its
specialized form for HMMs, namely the Baum Welch algorithm, which has been
extensively employed in applications such as speech recognition. The objective
of this work is to present pseudocode formulations for both the training and
decoding procedures of HMMs in a different scenario, where the available data
consist of multiple independent temporal sequences generated by the same model,
each of relatively short duration, i.e., containing only a limited number of
samples. Special emphasis is placed on the relevance of this formulation to
longitudinal studies in population health, where datasets are naturally
structured as collections of short trajectories across individuals with point
data at follow up.

</details>


### [7] [Statistically Adaptive Differential Protection for AC Microgrids Based on Kullback-Leibler Divergence](https://arxiv.org/abs/2510.26604)
*Shahab Moradi Torkashvand,Arina Kharazi,Emad Sadeghi,Seyed Hossein Hesamedin Sadeghi,Adel Nasiri*

Main category: eess.SP

TL;DR: 提出一种基于 KL 散度的自适应差分保护方案，利用对数电流幅值的 Bartlett 修正 G 统计量进行多变量故障检测，结合 Mahalanobis 距离判定健康/故障状态，阈值由卡方分布确定并设定严格误警控制；故障类型通过逐相 G 统计量分类，并加入时序持久性滤波。经改造的 CIGRE 14 总线微网仿真验证，具备子周期检测、跨工况高准确率，对高阻抗故障、10 ms 通信延迟和 20 dB SNR 的鲁棒性，具有较高的可重复性与计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着基于逆变单元的资源大量接入，传统微网保护面临可变的故障电流和复杂瞬态，需在噪声和通信延迟环境中提供鲁棒且误警可控的保护方案。

Method: 提出一个基于 KL 散度的多变量差分保护框架：对数电流幅值的 Bartlett 修正 G 统计量实现初步故障检测；以 Mahalanobis 距离区分健康与故障状态；阈值按卡方分布设定以控制假警报率；检测后对每相的 G 统计量进行分类并使用专用阈值，结合时序持久性滤波提升安全性。

Result: 在改造的 CIGRE 14 总线微网上进行广泛仿真，实现子周期级平均检测延迟、跨工况的高探测与分类准确率；对高阻抗故障（至 250 Ω）具鲁棒性；对 10 ms 通信延迟和 20 dB SNR 的噪声环境具有容忍性。

Conclusion: 给出一种可重复、计算高效的下一代交流微网保护解决方案，具有良好鲁棒性与可控误警性。

Abstract: The proliferation of inverter-based resources challenges traditional
microgrid protection by introducing variable fault currents and complex
transients. This paper presents a statistically adaptive differential
protection scheme based on Kullback-Leibler divergence, implemented via a
Bartlett-corrected G-statistic computed on logarithm-transformed current
magnitudes. The method is a multivariate fault detection engine that employs
the Mahalanobis distance to distinguish healthy and faulty states, enabling
robust detection even in noisy environments. Detection thresholds are
statistically derived from a chi-squared distribution for precise control over
the false alarm rate. Upon detection, a lightweight classifier identifies the
fault type by assessing per-phase G-statistics against dedicated thresholds,
enhanced by a temporal persistence filter for security. Extensive simulations
on a modified CIGRE 14-bus microgrid show high efficacy: sub-cycle average
detection delays, high detection and classification accuracy across operating
modes, resilience to high-impedance faults up to 250 Ohms, tolerance to 10 ms
communication delay, and noise levels down to a 20 dB signal-to-noise ratio.
These findings demonstrate a reproducible and computationally efficient
solution for next-generation AC microgrid protection.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: KAN 将可学习的一维基函数放在边/权重上，作为对MLP的替代，提供更强的表达力和可解释性；这篇综述系统梳理了 KAN 的理论基础、架构变体、实现方法及生态，建立了 KAN 与 MLP 的等价性及参数效率优势，并给出“Choose-Your-KAN”指南和实践要点，附带开源实现集合的整理。


<details>
  <summary>Details</summary>
Motivation: 旨在对迅速扩展的 KAN 领域进行系统综述，超越单纯的性能比较，建立理论与实践的完整框架，连接 KAN 与 MLP，评估基函数的权衡与实现成本，促成社区协作。

Method: 通过系统文献梳理、对现有实现的收集与分类、理论对比与生态整理，归纳出架构变体、正则化、训练策略等主题；对不同基函数的优劣进行对比分析；整理开源实现并构建知识图谱式分类。

Result: 给出 KAN 的理论等价性、参数效率、基函数的权衡、以及改进路线（物理信息损失、自适应采样、域划分、混合架构等）；提供“Choose-Your-KAN”指南、完整的实现清单与一个可操作的知识结构，便于实践者选择和实现。

Conclusion: 总结当前研究空缺，提出未来方向和实践建议，强调在准确性、效率、对非连续性处理等方面的挑战与机会。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [9] [HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series](https://arxiv.org/abs/2510.25785)
*Simon A. Lee,Cyrus Tanade,Hao Zhou,Juhyeon Lee,Megha Thukral,Minji Han,Rachel Choi,Md Sazzad Hissain Khan,Baiying Lu,Migyeong Gwak,Mehrab Bin Morshed,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出HiMAE，一种层次化掩码自编码框架，在可穿戴传感器时间序列上产生多分辨率嵌入，将尺度作为可解释探针，用于评估不同时间尺度的预测信号；在分类、回归和生成任务上优于仅使用单一尺度的SOTA模型，且参数量显著更小，可在边缘设备上实现亚毫秒推断。


<details>
  <summary>Details</summary>
Motivation: 时间分辨率被视为表示学习的一个基本轴，不同临床与行为结果依赖于不同的时间尺度结构，需通过学习框架来揭示尺度与预测信号之间的关系。

Method: 引入HiMAE，结合掩码自编码与层次化卷积编码-解码器，输出多尺度嵌入；通过在训练时对输入的不同尺度进行掩码与重建，实现自监督学习并将分辨率作为解释性探针。该方法能够在资源受限的设备上高效运行。

Result: 在分类、回归和生成基准上，HiMAE持续超越崩溃尺度的SOTA基模型，且模型规模远小于对手；在可穿戴场景中实现边缘推断，能够在普通手表级CPU上提供亚毫秒推断性能。

Conclusion: HiMAE兼具高效自监督学习与尺度敏感结构的发现工具属性，为可穿戴健康领域的边缘推断与多尺度表示学习提供了新途径。

Abstract: Wearable sensors provide abundant physiological time series, yet the
principles governing their predictive utility remain unclear. We hypothesize
that temporal resolution is a fundamental axis of representation learning, with
different clinical and behavioral outcomes relying on structure at distinct
scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical
Masked Autoencoder), a self supervised framework that combines masked
autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces
multi resolution embeddings that enable systematic evaluation of which temporal
scales carry predictive signal, transforming resolution from a hyperparameter
into a probe for interpretability. Across classification, regression, and
generative benchmarks, HiMAE consistently outperforms state of the art
foundation models that collapse scale, while being orders of magnitude smaller.
HiMAE is an efficient representation learner compact enough to run entirely on
watch, achieving sub millisecond inference on smartwatch class CPUs for true
edge inference. Together, these contributions position HiMAE as both an
efficient self supervised learning method and a discovery tool for scale
sensitive structure in wearable health.

</details>


### [10] [The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?](https://arxiv.org/abs/2510.25791)
*Zihan Pengmei,Costas Mavromatis,Zhengyuan Shen,Yunyi Zhang,Vassilis N. Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: CoT supervision can speed up generalization, but its benefits depend on task complexity; a transient phase exists where models produce correct answers before CoT traces align; a kinetic modeling framework and trace faithfulness emerge during training; CoT changes internal computation but does not fully overcome highly complex tasks.


<details>
  <summary>Details</summary>
Motivation: 揭示模型在学习过程中如何利用和受益于Chain-of-Thought（CoT）监督，以及在不同算法复杂度下的学习动态；通过可控数据与复杂度来理解“groking”现象，以及为 CoT 的机制提供可量化的框架。

Method: 在符号推理任务上对Transformer进行预训练，任务具可控的算法复杂度与数据分布；采用两种训练设置：只输出最终答案；在回答前输出显式的CoT推理过程；对训练过程中的准确率用三参数逻辑回归曲线拟合，评估学习速度与形状随任务复杂度、数据分布及CoT监督的变化；观察并分析“跟踪信度”随训练的动态演化以及初期可能存在的推理轨迹不一致阶段；从实验层面考察CoT对内部计算的影响。

Result: 实证结果表明：1) CoT Overall 提升任务性能，但其收益受任务复杂度限制；对高算法复杂度任务（如找出列表交集）不足以弥补缺失；2) 引入了一种用于理解Transformer学习的动力学建模框架（kinetic modeling）；3) 推理轨迹的信度是一个随训练而动态出现的属性，初期模型可能正确给出答案但推理轨迹与答案不对齐；4) CoT 会改变内部Transformer的计算机制，促进更快的泛化但并非在所有任务上完整提升性能。

Conclusion: CoT 是有益的工具，但并非普适解，尤其在高复杂度算法任务上受限。研究强调将学习动力学和轨迹信度作为分析CoT效应的关键维度，并提出动力学建模作为理解Transformer学习的有力工具，为未来在复杂任务上的CoT设计与评估提供方向。

Abstract: Chain-of-thought (CoT) supervision can substantially improve transformer
performance, yet the mechanisms by which models learn to follow and benefit
from CoT remain poorly understood. We investigate these learning dynamics
through the lens of grokking by pretraining transformers on symbolic reasoning
tasks with tunable algorithmic complexity and controllable data composition to
study their generalization. Models were trained under two settings: (i)
producing only final answers, and (ii) emitting explicit CoT traces before
answering. Our results show that while CoT generally improves task performance,
its benefits depend on task complexity. To quantify these effects, we model the
accuracy of the logarithmic training steps with a three-parameter logistic
curve, revealing how the learning speed and shape vary with task complexity,
data distribution, and the presence of CoT supervision. We also uncover a
transient trace unfaithfulness phase: early in training, models often produce
correct answers while skipping or contradicting CoT steps, before later
aligning their reasoning traces with answers. Empirically, we (1) demonstrate
that CoT accelerates generalization but does not overcome tasks with higher
algorithmic complexity, such as finding list intersections; (2) introduce a
kinetic modeling framework for understanding transformer learning; (3)
characterize trace faithfulness as a dynamic property that emerges over
training; and (4) show CoT alters internal transformer computation
mechanistically.

</details>


### [11] [Optimal Information Combining for Multi-Agent Systems Using Adaptive Bias Learning](https://arxiv.org/abs/2510.25793)
*Siavash M. Alamouti,Fay Arjomandi*

Main category: cs.LG

TL;DR: Proposes ABLOC, a framework and algorithm to learn and correct for environment-dependent biases in multi-agent systems by decomposing biases into learnable and irreducible components, with a learnability ratio guiding when learning helps; provides theoretical bounds and empirical validation.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in multi-agent systems due to biases that vary with environmental conditions; fill the gap between ignoring biases and expensive calibration; provide diagnostic criteria and a practical algorithm for bias learning and combination optimization.

Method: Theoretically decomposes bias into learnable vs irreducible components; defines learnability ratio as the fraction of bias variance predictable from covariates. Proves performance improvement bounds based on this ratio. Introduces ABLOC algorithm that iteratively learns bias-correcting transformations while optimizing combination weights via closed-form updates, with convergence guarantees to the bounds.

Result: The framework yields a measurable bound on achievable improvement determined by the learnability ratio. Empirical experiments show systems with high learnability ratios achieve 40%-70% of the theoretical maximum improvement; low learnability leads to minimal benefit, validating the proposed diagnostic criterion.

Conclusion: Learning biases is beneficial only when a substantial portion of bias is predictable from observable covariates; ABLOC provides a practical method with provable guarantees to reach near-optimal bias correction for high-learnability scenarios while offering a diagnostic tool to avoid wasted effort in low-learnability settings.

Abstract: Modern multi-agent systems ranging from sensor networks monitoring critical
infrastructure to crowdsourcing platforms aggregating human intelligence can
suffer significant performance degradation due to systematic biases that vary
with environmental conditions. Current approaches either ignore these biases,
leading to suboptimal decisions, or require expensive calibration procedures
that are often infeasible in practice. This performance gap has real
consequences: inaccurate environmental monitoring, unreliable financial
predictions, and flawed aggregation of human judgments. This paper addresses
the fundamental question: when can we learn and correct for these unknown
biases to recover near-optimal performance, and when is such learning futile?
We develop a theoretical framework that decomposes biases into learnable
systematic components and irreducible stochastic components, introducing the
concept of learnability ratio as the fraction of bias variance predictable from
observable covariates. This ratio determines whether bias learning is
worthwhile for a given system. We prove that the achievable performance
improvement is fundamentally bounded by this learnability ratio, providing
system designers with quantitative guidance on when to invest in bias learning
versus simpler approaches. We present the Adaptive Bias Learning and Optimal
Combining (ABLOC) algorithm, which iteratively learns bias-correcting
transformations while optimizing combination weights through closedform
solutions, guaranteeing convergence to these theoretical bounds. Experimental
validation demonstrates that systems with high learnability ratios can recover
significant performance (we achieved 40%-70% of theoretical maximum improvement
in our examples), while those with low learnability show minimal benefit,
validating our diagnostic criteria for practical deployment decisions.

</details>


### [12] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: 提出一种基于仿真信息的强化学习方法，将 ride-pooling 融入学习与规划框架，以实现非 myopic 的调度决策，并通过对未任务的 idle 车辆再平衡策略进行补充。对比 myopic 策略，在 NYC 数据上实现了服务率提升、等待/车内时间下降以及车队规模显著下降，同时再平衡策略进一步提升性能，尽管单位乘客行驶里程增加。


<details>
  <summary>Details</summary>
Motivation: 解决 ride-pooling 常见的短视决策问题，无法充分评估长期影响；通过将仿真嵌入学习过程，使策略能够进行非短视的全局优化；并将再平衡作为补充策略以提升系统整体性能。

Method: 将 ride-pooling 仿真嵌入现有的 ride-hailing 学习与规划框架（参照 Xu et al. 2018），以实现非 myopic 决策。使用 n-step 时序差分学习从仿真体验中推导时空状态值，并在 NYC 出租车需求数据上评估策略。同时提出一个补充的 idle 车辆再平衡策略，并比较两者的组合效果。

Result: 非 myopic 的匹配策略相较于 myopic，服务率提升高达 8.4%，乘客等待时间与车内时间均显著降低；同时在相同性能水平下，车队规模可下降>25%，具备显著运营成本节约。将再平衡操作纳入框架后，等待时间下降最多 27.3%，车内时间下降 12.5%，服务率提升 15.1%，但单位乘客里程可能增加。

Conclusion: 基于仿真信息的 RL 框架能够实现 ride-pooling 中的非短视优化，并且再平衡策略可进一步提升整体效能，显示出该方法在实际运营中的潜在价值。

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [13] [MemEIC: A Step Toward Continual and Compositional Knowledge Editing](https://arxiv.org/abs/2510.25798)
*Jin Seong,Jiyun Park,Wencke Liermann,Hongseok Choi,Yoonji Nam,Hyun Kim,Soojong Lim,Namhoon Lee*

Main category: cs.LG

TL;DR: MemEIC proposes a continual and compositional knowledge editing framework for LVLMs, enabling sequential editing of both visual and textual knowledge with improved cross-modal reasoning and retention of prior edits.


<details>
  <summary>Details</summary>
Motivation: There is a need to continuously update large vision-language models (LVLMs) in a multimodal setting. Existing knowledge-editing methods largely edit one modality at a time and neglect cross-modal interactions and ongoing refinement, leading to suboptimal outcomes.

Method: MemEIC employs a hybrid external-internal editor with a dual external memory for cross-modal evidence retrieval and dual LoRA adapters to update parameters for each modality separately. A brain-inspired knowledge connector is selectively activated for compositional reasoning to integrate information across modalities.

Result: Experiments show significant improvements on complex multimodal question answering and effective preservation of prior edits, establishing a new benchmark for continual and compositional knowledge editing in LVLMs.

Conclusion: MemEIC enables effective, scalable compositional and continual knowledge editing in LVLMs by combining cross-modal evidence retrieval, modality-specific adapters, and a cross-modal reasoning connector, improving performance while preserving past edits.

Abstract: The dynamic nature of information necessitates continuously updating large
vision-language models (LVLMs). While recent knowledge editing techniques hint
at promising directions, they often focus on editing a single modality (vision
or language) in isolation. This prevalent practice neglects the inherent
multimodality of LVLMs and the continuous nature of knowledge updates,
potentially leading to suboptimal editing outcomes when considering the
interplay between modalities and the need for ongoing knowledge refinement. To
address these limitations, we propose MemEIC, a novel method for Continual and
Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional
editing of both visual and textual knowledge sequentially. Our approach employs
a hybrid external-internal editor featuring a dual external memory for
cross-modal evidence retrieval and dual LoRA adapters that facilitate
disentangled parameter updates for each modality. A key component is a
brain-inspired knowledge connector, activated selectively for compositional
reasoning, that integrates information across different modalities. Experiments
demonstrate that MemEIC significantly improves performance on complex
multimodal questions and effectively preserves prior edits, setting a new
benchmark for CCKE in LVLMs.

</details>


### [14] [FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks](https://arxiv.org/abs/2510.25800)
*Jialong Sun,Xinpeng Ling,Jiaxuan Zou,Jiawen Kang,Kejia Zhang*

Main category: cs.LG

TL;DR: FreLE通过在损失函数中加入显式和隐式的频率正则化，缓解长序列预测中的频谱偏差，提升泛化能力；对现有主流模型普遍存在的频谱偏差进行了大规模实验测量，显示其广泛性，并在多项实验中带来显著改进，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 时间序列的自相关特性使得长时预测困难，频域信息的引入在这类任务中被广泛采用；不同模型中普遍存在的“低频先拟合”的谱偏现象提示这是一种普遍规律而非特定架构的特性。作者旨在统一理解谱偏、量化其普遍性，并提出可插拔的正则化方案以改善泛化。

Method: 对现有主流模型在长期预测任务中的频谱偏差进行大规模实证测量；提出FreLE（Frequency Loss Enhancement）算法，在损失函数层面通过显式的频率正则化和隐式正则化两条线路进行频域信息引导，作为一个可插拔的模型损失单元嵌入现有模型。

Result: 大量实验表明几乎所有模型均呈现谱偏现象；在使用FreLE后，模型在长期预测任务上的泛化能力显著提升，且对不同数据集和模型结构具有鲁棒性；提供了开源代码以确保可复现性。

Conclusion: FreLE作为一种通用且易于嵌入的损失函数组件，成功缓解了频谱偏差、提升长期预测性能，具有较强的普适性和可扩展性，值得在更多时间序列任务中尝试。

Abstract: The inherent autocorrelation of time series data presents an ongoing
challenge to multivariate time series prediction. Recently, a widely adopted
approach has been the incorporation of frequency domain information to assist
in long-term prediction tasks. Many researchers have independently observed the
spectral bias phenomenon in neural networks, where models tend to fit
low-frequency signals before high-frequency ones. However, these observations
have often been attributed to the specific architectures designed by the
researchers, rather than recognizing the phenomenon as a universal
characteristic across models. To unify the understanding of the spectral bias
phenomenon in long-term time series prediction, we conducted extensive
empirical experiments to measure spectral bias in existing mainstream models.
Our findings reveal that virtually all models exhibit this phenomenon. To
mitigate the impact of spectral bias, we propose the FreLE (Frequency Loss
Enhancement) algorithm, which enhances model generalization through both
explicit and implicit frequency regularization. This is a plug-and-play model
loss function unit. A large number of experiments have proven the superior
performance of FreLE. Code is available at
https://github.com/Chenxing-Xuan/FreLE.

</details>


### [15] [Contrastive Predictive Coding Done Right for Mutual Information Estimation](https://arxiv.org/abs/2510.25983)
*J. Jon Ryu,Pavan Yeddanapudi,Xiangxiang Xu,Gregory W. Wornell*

Main category: cs.LG

TL;DR: InfoNCE不是一个有效的互信息(MI)估计器；提出InfoNCE-anchor，通过引入辅助锚点实现一致的密度比估计并显著降低偏差，构成一个基于正确评分规则的统一框架，能覆盖NCE、InfoNCE和f-变体。实证表明在MI估计上InfoNCE-anchor（对数分数）最准确；但在自监督表示学习的下游任务中锚点并未带来性能提升，表明对比学习的收益来自结构化的密度比学习而非MI估计的精准度。


<details>
  <summary>Details</summary>
Motivation: 揭示InfoNCE作为MI估计器的局限性，寻求更可靠的MI估计方法并统一对比学习目标的理论框架，解释对比学习为何有效。

Method: 提出一个辅助锚点（anchor）类别以实现一致的密度比估计，并构建可插拔的MI估计器（InfoNCE-anchor）；使用正确评分规则（如对数分数）将该框架推广，涵盖NCE、InfoNCE及f-divergence等变体，形成一个单一原理框架。

Result: 实验显示InfoNCE-anchor在对数分数下提供最准确的MI估计；然而在自监督表示学习实验中，锚点对下游任务表现并无提升，验证了对比学习受益并非来自MI估计的精确性。

Conclusion: 对比学习的收益来自学习结构化的密度比，而非严格的MI估计准确性；InfoNCE不应被视作MI的直接有效估计器，InfoNCE-anchor提供更稳健的MI估计并统一了相关对比目标的理论框架。

Abstract: The InfoNCE objective, originally introduced for contrastive representation
learning, has become a popular choice for mutual information (MI) estimation,
despite its indirect connection to MI. In this paper, we demonstrate why
InfoNCE should not be regarded as a valid MI estimator, and we introduce a
simple modification, which we refer to as InfoNCE-anchor, for accurate MI
estimation. Our modification introduces an auxiliary anchor class, enabling
consistent density ratio estimation and yielding a plug-in MI estimator with
significantly reduced bias. Beyond this, we generalize our framework using
proper scoring rules, which recover InfoNCE-anchor as a special case when the
log score is employed. This formulation unifies a broad spectrum of contrastive
objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single
principled framework. Empirically, we find that InfoNCE-anchor with the log
score achieves the most accurate MI estimates; however, in self-supervised
representation learning experiments, we find that the anchor does not improve
the downstream task performance. These findings corroborate that contrastive
representation learning benefits not from accurate MI estimation per se, but
from the learning of structured density ratios.

</details>


### [16] [Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start](https://arxiv.org/abs/2510.25801)
*Kun Chen,Peng Shi,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao,Lin Ma*

Main category: cs.LG

TL;DR: 提出一个解耦的冷启动框架 SPECS，通过自蒸馏生成偏好数据并采用偏好训练，提升多模态模型在 RL 任务中的泛化与鲁棒性，并在 MEGA-Bench、MathVista 等基准上取得显著性能提升，同时改善探索性、训练稳定性与性能上限。


<details>
  <summary>Details</summary>
Motivation: 现有以 SFT 为冷启动的策略容易导致指令风格的过拟合、对分布外(out-of-distribution)泛化能力下降，进而影响后续的 RL 表现。作者通过引入 Generalization Factor (GF) 来量化不同方法的泛化能力，并发现基于偏好训练的方法（如 DPO）在冷启动阶段的泛化优于 SFT。需要一种解耦的学习框架，既能提升泛化，又能为 RL 提供更可靠的深层推理能力。

Method: 提出 Generalization Factor (GF) 表征不同冷启动方法的泛化能力；提出 SPECS 框架：自蒸馏生成内省式偏好数据对；进行偏好训练以学习对表层形式（格式、结构、风格）关注的准则，避免对内容的记忆化；最后引导进入带可验证奖励的 RL，提升深层推理结果。

Result: 在多个多模态基准上取得稳定性能提升：MEGA-Bench 提升 4.1%，MathVista 提升 12.2%；额外实验证明 SPECS 有助于减少分布内的“卡死”现象、改善探索、稳定训练并提高性能上限。

Conclusion: 解耦学习框架 SPECS 能提高冷启动阶段的泛化能力与鲁棒性，使 RL with verifiable rewards 在多模态任务上获得更优的综合表现。该方法通过自蒸馏产生偏好数据、偏好训练以及对格式化/表征特征的关注，降低内容记忆化风险，提升探索与训练稳定性，并提升整体性能上限。

Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of "MLLM-r1" approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing
supervised fine-tuning (SFT), to initialize the policy before RL. However,
SFT-based cold start adopts the reasoning paradigm intertwined with task
solution and output format, which may induce instruction-style overfitting,
weakens out-of-distribution generalization, and ultimately affects downstream
RL. We revisit the cold start along two views, its training method and data
construction, and introduce the Generalization Factor (GF) coefficient to
quantify the generalization capability under different methods. Our empirical
study finds that preference-based training methods (e.g. DPO) generalizes
better than SFT-based methods in cold start. Motivated by this, we propose
SPECS-a Self-distilled, Preference-based Cold Start framework that decouples
multimodal learning: (1) generates introspective preference data pairs via
self-distillation, avoiding reliance on larger teachers or manual annotation;
(2) performs preference-based training to learn, focusing on shallow,
transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.
Additional experiments indicate that SPECS contributes to reducing
in-distribution "stuckness," improving exploration, stabilizing training, and
raising the performance ceiling.

</details>


### [17] [ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion](https://arxiv.org/abs/2510.25818)
*Sungho Koh,SeungJu Cha,Hyunwoo Oh,Kwanyoung Lee,Dong-Jin Kim*

Main category: cs.LG

TL;DR: ScaleDiff extends pretrained diffusion models to higher resolutions without training, using Neighborhood Patch Attention (NPA) to reduce self-attention redundancy, SDEdit integration, Latent Frequency Mixing (LFM) for detail, and Structure Guidance for global structure; claims state-of-the-art among training-free methods in quality and speed for U-Net and Diffusion Transformer architectures.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle to generate high-resolution images beyond training resolution, and existing training-free approaches are either computationally heavy or incompatible with modern Diffusion Transformers.

Method: Introduce ScaleDiff with (1) Neighborhood Patch Attention (NPA) to replace or augment self-attention using non-overlapping patches for efficiency; (2) integration into an SDEdit-like denoising pipeline; (3) Latent Frequency Mixing (LFM) to enhance fine details in the latent space; (4) Structure Guidance to promote global structural consistency during denoising; (5) evaluation on both U-Net and Diffusion Transformer architectures with a training-free setup.

Result: Empirical results show ScaleDiff achieves state-of-the-art performance among training-free methods in image quality and inference speed when operating at higher resolutions, applicable to both U-Net and Diffusion Transformer backbones.

Conclusion: ScaleDiff provides a model-agnostic, highly efficient solution to extend pretrained diffusion models to higher resolutions without extra training, combining novel attention optimization, latent detail enhancement, and global structure guidance.

Abstract: Text-to-image diffusion models often exhibit degraded performance when
generating images beyond their training resolution. Recent training-free
methods can mitigate this limitation, but they often require substantial
computation or are incompatible with recent Diffusion Transformer models. In
this paper, we propose ScaleDiff, a model-agnostic and highly efficient
framework for extending the resolution of pretrained diffusion models without
any additional training. A core component of our framework is Neighborhood
Patch Attention (NPA), an efficient mechanism that reduces computational
redundancy in the self-attention layer with non-overlapping patches. We
integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing
(LFM) to better generate fine details. Furthermore, we apply Structure Guidance
to enhance global structure during the denoising process. Experimental results
demonstrate that ScaleDiff achieves state-of-the-art performance among
training-free methods in terms of both image quality and inference speed on
both U-Net and Diffusion Transformer architectures.

</details>


### [18] [Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off](https://arxiv.org/abs/2510.26722)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi*

Main category: cs.LG

TL;DR: OTA-FL with SGD for non-convex objectives under wireless heterogeneity uses a structured time-invariant bias to reduce variance, derives a finite-time stationarity bound, and solves a non-convex joint power-control problem via a successive convex approximation (SCA) using only statistical CSI, achieving faster convergence and better generalization on non-convex tasks.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of unbiased OTA-FL in heterogeneous wireless settings and the lack of non-convex analysis. We aim to enable scalable, convergent OTA-FL for modern non-convex models by controlling bias-variance through structured updates.

Method: Introduce OTA-FL SGD updates with a designed, time-invariant model bias. Derive a finite-time stationarity bound that reveals bias-variance trade-off. Formulate a non-convex joint power-control problem and develop an efficient SCA algorithm requiring only statistical CSI at the base station.

Result: Experimental validation on a non-convex image classification task shows that the SCA-based power-control design accelerates convergence by optimizing bias and yields improved generalization over existing OTA-FL baselines.

Conclusion: A structured bias-variance trade-off mechanism for OTA-FL under wireless heterogeneity. The SCA-based power control leveraging only statistical CSI substantially improves convergence speed and generalization in non-convex settings.

Abstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a
scalable paradigm that exploits the waveform superposition of the wireless
multiple-access channel to aggregate model updates in a single use. Existing
OTA-FL designs largely enforce zero-bias model updates by either assuming
\emph{homogeneous} wireless conditions (equal path loss across devices) or
forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous}
wireless scenarios, however, such designs are constrained by the weakest device
and inflate the update variance. Moreover, prior analyses of biased OTA-FL
largely address convex objectives, while most modern AI models are highly
non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient
descent (SGD) for general smooth non-convex objectives under wireless
heterogeneity. We develop novel OTA-FL SGD updates that allow a structured,
time-invariant model bias while facilitating reduced variance updates. We
derive a finite-time stationarity bound (expected time average squared gradient
norm) that explicitly reveals a bias-variance trade-off. To optimize this
trade-off, we pose a non-convex joint OTA power-control design and develop an
efficient successive convex approximation (SCA) algorithm that requires only
statistical CSI at the base station. Experiments on a non-convex image
classification task validate the approach: the SCA-based design accelerates
convergence via an optimized bias and improves generalization over prior OTA-FL
baselines.

</details>


### [19] [Topology-Aware Active Learning on Graphs](https://arxiv.org/abs/2510.25892)
*Harris Hardiman-Mostow,Jack Mauro,Adrien Weihs,Andrea L. Bertozzi*

Main category: cs.LG

TL;DR: A curvature-guided graph-active-learning framework that uses Balanced Forman Curvature to select initial labels (coreset), determine when to stop exploring, and dynamically switch from exploration to exploitation, plus a localized graph rewiring to enhance label propagation; it yields superior performance at low label rates on benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of labeled data in graph-based active learning by designing principled, curvature-based mechanisms to balance exploration and exploitation, select informative initial labels, and efficiently propagate labels while preserving sparsity.

Method: 1) Build graph and construct a coreset of representative initial labels via Balanced Forman Curvature (BFC). 2) Introduce a data-driven stopping criterion indicating sufficient graph exploration. 3) Use BFC to dynamically trigger the shift from exploration to exploitation within active learning, replacing manual heuristics. 4) Propose a localized graph rewiring strategy to incorporate multiscale information around labeled nodes, improving label propagation while maintaining sparsity.

Result: Empirical results on benchmark classification tasks show consistent outperformance of existing graph-based semi-supervised baselines at low label rates, demonstrating improved exploration, exploitation, and propagation efficiency.

Conclusion: A principled, curvature-guided framework for graph-based active learning that achieves a better exploration–exploitation balance and efficient label propagation under limited labeling budgets, with demonstrated gains on standard benchmarks.

Abstract: We propose a graph-topological approach to active learning that directly
targets the core challenge of exploration versus exploitation under scarce
label budgets. To guide exploration, we introduce a coreset construction
algorithm based on Balanced Forman Curvature (BFC), which selects
representative initial labels that reflect the graph's cluster structure. This
method includes a data-driven stopping criterion that signals when the graph
has been sufficiently explored. We further use BFC to dynamically trigger the
shift from exploration to exploitation within active learning routines,
replacing hand-tuned heuristics. To improve exploitation, we introduce a
localized graph rewiring strategy that efficiently incorporates multiscale
information around labeled nodes, enhancing label propagation while preserving
sparsity. Experiments on benchmark classification tasks show that our methods
consistently outperform existing graph-based semi-supervised baselines at low
label rates.

</details>


### [20] [Transferring Causal Effects using Proxies](https://arxiv.org/abs/2510.25924)
*Manuel Iglesias-Alonso,Felix Schur,Julius von Kügelgen,Jonas Peters*

Main category: cs.LG

TL;DR: The paper studies causal effect estimation in a multi-domain setting with unobserved confounding, using a proxy for the hidden confounder. It proves identifiability and proposes two estimation methods that are consistent and enable confidence intervals, applicable even when treatment and outcome are continuous, with validation via simulations and a real-world website-ranking example.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of estimating causal effects when unobserved confounding differs across domains. Proposes a framework that leverages a proxy for the hidden confounder to achieve identifiability in the target domain where only the proxy is observed.

Method: Model the relationship between treatment, outcome, proxy confounder, and domain, assuming discrete/categorical variables. Propose two estimation techniques (details not given in abstract), prove consistency for both, and construct confidence intervals. Use domain-shift assumptions to transfer identifiability to the target domain and handle continuous treatment/outcome.

Result: Estimation techniques are shown to be consistent; identifiability of the causal effect established in the target domain under proxy confounding even when treatment and response are continuous; simulation studies and a real-world application (website rankings affecting consumer choices) validate the approach.

Conclusion: The framework enables identifiable causal inference across domains with a proxy confounder, provides consistent estimators and uncertainty quantification, and is supported by simulations and a real dataset, highlighting the practical relevance for studying cross-domain causal effects.

Abstract: We consider the problem of estimating a causal effect in a multi-domain
setting. The causal effect of interest is confounded by an unobserved
confounder and can change between the different domains. We assume that we have
access to a proxy of the hidden confounder and that all variables are discrete
or categorical. We propose methodology to estimate the causal effect in the
target domain, where we assume to observe only the proxy variable. Under these
conditions, we prove identifiability (even when treatment and response
variables are continuous). We introduce two estimation techniques, prove
consistency, and derive confidence intervals. The theoretical results are
supported by simulation studies and a real-world example studying the causal
effect of website rankings on consumer choices.

</details>


### [21] [Active Learning with Task-Driven Representations for Messy Pools](https://arxiv.org/abs/2510.25926)
*Kianoosh Ashouritaklimi,Tom Rainforth*

Main category: cs.LG

TL;DR: 在主动学习中，固定的无监督池表示在处理混乱数据池时效果有限，文献提出基于任务驱动的表示并周期性更新，给出两种策略（半监督表示学习与有监督微调），显著提升相对无监督/预训练表示的表现。


<details>
  <summary>Details</summary>
Motivation: 解决混乱数据池对主动学习的适应性不足的问题；现有方法依赖固定的无监督表示，无法捕捉与任务相关的重要信息。

Method: 提出一个在主动学习过程中周期性用已标注数据更新任务驱动表示的框架。具体实现包括：1) 直接学习半监督表示；2) 在初始无监督表示基础上进行有监督微调。

Result: 实验结果显示，两种任务驱动表示策略在多种设置下显著优于使用固定无监督或预训练表示的基线。

Conclusion: 引入任务驱动的表示学习能提升主动学习在混乱数据池中的鲁棒性和效能，提示未来在主动学习设计中应将表示学习与获取标注紧密结合。

Abstract: Active learning has the potential to be especially useful for messy,
uncurated pools where datapoints vary in relevance to the target task. However,
state-of-the-art approaches to this problem currently rely on using fixed,
unsupervised representations of the pool, focusing on modifying the acquisition
function instead. We show that this model setup can undermine their
effectiveness at dealing with messy pools, as such representations can fail to
capture important information relevant to the task. To address this, we propose
using task-driven representations that are periodically updated during the
active learning process using the previously collected labels. We introduce two
specific strategies for learning these representations, one based on directly
learning semi-supervised representations and the other based on supervised
fine-tuning of an initial unsupervised representation. We find that both
significantly improve empirical performance over using unsupervised or
pretrained representations.

</details>


### [22] [Modular Linear Tokenization (MLT)](https://arxiv.org/abs/2510.25952)
*Tcharlies Schmitz*

Main category: cs.LG

TL;DR: Introduces Modular Linear Tokenization (MLT), a reversible, deterministic encoding for high-cardinality identifiers into compact vectors using modular arithmetic and invertible linear transforms, enabling controlled dimensionality and scalability with full reversibility.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies of traditional hashing/one-hot for high-cardinality categorical data; need bijective, size-controllable representations that are scalable for millions of identifiers.

Method: Encode identifiers via modular arithmetic over finite fields and invertible linear transformations to produce bijective compact vectors; deterministic, reversible mappings; explicit dimensionality control and scalability.

Result: On MovieLens 20M, MLT achieves predictive performance comparable to supervised embeddings while using far fewer parameters and lower training cost.

Conclusion: MLT provides a practical, reversible encoding with scalable parameter efficiency; open-source implementation available (PyPI, GitHub).

Abstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and
deterministic technique for encoding high-cardinality categorical identifiers
into compact numerical vectors. Unlike traditional hashing or one-hot
encodings, MLT preserves bijective mappings by leveraging modular arithmetic
over finite fields and invertible linear transformations. The method offers
explicit control of dimensionality and computational scalability while
maintaining full reversibility, even for millions of identifiers. Experimental
results on the MovieLens 20M dataset show that MLT achieves comparable
predictive performance to supervised embeddings while requiring significantly
fewer parameters and lower training cost. An open-source implementation of MLT
is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub
(https://github.com/tcharliesschmitz/light-mlt).

</details>


### [23] [Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi](https://arxiv.org/abs/2510.25954)
*Lynn Metz,Rachel Haggard,Michael Moszczynski,Samer Asbah,Chris Mwase,Patricia Khomani,Tyler Smith,Hannah Cooper,Annie Mwale,Arbaaz Muslim,Gautam Prasad,Mimi Sun,Tomer Shekel,Joydeep Paul,Anna Carter,Shravya Shetty,Dylan Green*

Main category: cs.LG

TL;DR: GeoFM embeddings from multiple sources (Google PDFM, AlphaEarth, and CDR) modestly improve prediction of routine health indicators in Malawi; best performance for population density, new HIV cases, and child vaccinations; poor for data-sparse targets like TB and malnutrition; a Multi-GeoFM model yielded the strongest predictions, suggesting value in augmenting constrained health information systems.


<details>
  <summary>Details</summary>
Motivation: In LMICs, routine health data suffer from delays and incomplete coverage. There is a need to explore novel data sources and analytics, such as geospatial foundation models (GeoFMs), to augment traditional methods and improve timely, accurate health predictions.

Method: XGBoost models trained on 552 health catchment areas in Malawi using embedding features from three GeoFMs (Google PDFM, Google AlphaEarth, CDR). Data split 80/20 with 5-fold cross-validation. Compared against traditional geostatistical methods.

Result: Embedding-based approaches outperformed baseline geostatistical methods in 13 of 15 indicators (87%). A Multi-GeoFM model combining all three sources yielded the strongest performance, with cross-validated R2 values ~0.63 for population density, ~0.57 for new HIV cases, and ~0.47 for child vaccinations; test-set R2 values were ~0.64, ~0.68, and ~0.55 respectively. Prediction was poorer for targets with limited primary data (e.g., TB and malnutrition).

Conclusion: Integrating multiple GeoFM sources can efficiently enhance predictive accuracy for select health and demographic outcomes in LMICs, offering a valuable supplement to constrained routine health information systems.

Abstract: The reliability of routine health data in low and middle-income countries
(LMICs) is often constrained by reporting delays and incomplete coverage,
necessitating the exploration of novel data sources and analytics. Geospatial
Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse
spatial, temporal, and behavioral data into mathematical embeddings that can be
efficiently used for downstream prediction tasks. This study evaluated the
predictive performance of three GeoFM embedding sources - Google Population
Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite
imagery), and mobile phone call detail records (CDR) - for modeling 15 routine
health programmatic outputs in Malawi, and compared their utility to
traditional geospatial interpolation methods. We used XGBoost models on data
from 552 health catchment areas (January 2021-May 2023), assessing performance
with R2, and using an 80/20 training and test data split with 5-fold
cross-validation used in training. While predictive performance was mixed, the
embedding-based approaches improved upon baseline geostatistical methods in 13
of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three
embedding sources produced the most robust predictions, achieving average
5-fold cross validated R2 values for indicators like population density (0.63),
new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,
0.68, and 0.55, respectively. Prediction was poor for prediction targets with
low primary data availability, such as TB and malnutrition cases. These results
demonstrate that GeoFM embeddings imbue a modest predictive improvement for
select health and demographic outcomes in an LMIC context. We conclude that the
integration of multiple GeoFM sources is an efficient and valuable tool for
supplementing and strengthening constrained routine health information systems.

</details>


### [24] [Infrequent Exploration in Linear Bandits](https://arxiv.org/abs/2510.26000)
*Harin Lee,Min-hwan Oh*

Main category: cs.LG

TL;DR: INFEX: a modular framework for infrequent exploration in linear bandits that schedules limited exploratory actions, achieving instance-dependent regret comparable to standard algorithms while improving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: There is a practical gap between fully adaptive exploration (e.g., UCB/Thompson Sampling) and purely greedy methods: frequent exploration can be impractical or unsafe, while greedy strategies require strong contextual diversity.

Method: INFEX runs a base exploratory policy at a prescribed schedule and largely selects greedy actions in between. It is modular and can wrap any adaptive exploration method, concentrating intensive exploration only at infrequent intervals.

Result: Under an exploration frequency above a logarithmic threshold, INFEX achieves instance-dependent regret matching provably efficient algorithms; it also improves computational efficiency. Empirical evaluations show state-of-the-art regret and faster runtimes.

Conclusion: INFEX provides a simple, practical, and widely applicable framework for infrequent exploration in linear bandits, enabling safer deployment without sacrificing regret performance and with reduced computation.

Abstract: We study the problem of infrequent exploration in linear bandits, addressing
a significant yet overlooked gap between fully adaptive exploratory methods
(e.g., UCB and Thompson Sampling), which explore potentially at every time
step, and purely greedy approaches, which require stringent diversity
assumptions to succeed. Continuous exploration can be impractical or unethical
in safety-critical or costly domains, while purely greedy strategies typically
fail without adequate contextual diversity. To bridge these extremes, we
introduce a simple and practical framework, INFEX, explicitly designed for
infrequent exploration. INFEX executes a base exploratory policy according to a
given schedule while predominantly choosing greedy actions in between. Despite
its simplicity, our theoretical analysis demonstrates that INFEX achieves
instance-dependent regret matching standard provably efficient algorithms,
provided the exploration frequency exceeds a logarithmic threshold.
Additionally, INFEX is a general, modular framework that allows seamless
integration of any fully adaptive exploration method, enabling wide
applicability and ease of adoption. By restricting intensive exploratory
computations to infrequent intervals, our approach can also enhance
computational efficiency. Empirical evaluations confirm our theoretical
findings, showing state-of-the-art regret performance and runtime improvements
over existing methods.

</details>


### [25] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: 提出双MoE框架用于离散时间生存分析，通过特征编码器MoE实现子群感知表征以及 hazard MoE实现时序动态建模，提升METABRIC和GBSG数据集上的时间相关C指数。


<details>
  <summary>Details</summary>
Motivation: 在患者异质性与时间动态的同时建模以提升个体化风险预测。

Method: 引入特征编码器MoE与hazard MoE的双MoE架构，结合时间嵌入，能够灵活集成到现有深度学习生存管线。

Result: 在METABRIC和GBSG数据集上实现性能提升，测试集时间相关C-index提升至约0.04；在与Consurv框架集成后获得进一步收益。

Conclusion: 双MoE设计可有效同时捕获患者异质性与时间动态，提升离散时间生存分析的预测性能并具备良好扩展性。

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [26] [Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability](https://arxiv.org/abs/2510.26792)
*Tao Tao,Maissam Barkeshli*

Main category: cs.LG

TL;DR: Transformer models can learn in-context prediction on sequences generated by permuted congruential generators (PCGs), including extremely large moduli and truncated outputs, revealing scalable learning laws and emergent representations.


<details>
  <summary>Details</summary>
Motivation: To probe the limits of in-context learning by transformers on complex PRNGs (PCGs) and to understand how model size, data, and curriculum affect learning of nontrivial stateful sequences.

Method: Train transformers with up to 50M parameters on PCG-generated sequences with modulus m up to 2^22 (≈4 million) and datasets up to 5 billion tokens. Tasks include predicting the next PRNG output or a truncated bit. Evaluate zero-shot generalization to unseen PCG variants and joint learning across multiple PRNGs. Analyze scaling with modulus and embedding representations, including curriculum effects and clustering in input embeddings.

Result: Transformers achieve near-perfect in-context prediction on unseen PCG variants, even when outputs are truncated to a single bit. They can jointly learn multiple PRNGs. A scaling law emerges: the number of in-context elements needed scales as sqrt(m). For larger moduli, training exhibits extended stagnation unless curriculum learning from smaller moduli is used. Embedding analyses reveal bitwise rotationally-invariant clustering of inputs, indicating transferable representations across moduli.

Conclusion: Transformers are capable of robust in-context learning for complex PRNG sequences, but practical training at large moduli requires curriculum strategies. The learned representations show structured, transferable organization that aligns with the bitwise nature of the generators.

Abstract: We study the ability of Transformer models to learn sequences generated by
Permuted Congruential Generators (PCGs), a widely used family of pseudo-random
number generators (PRNGs). PCGs introduce substantial additional difficulty
over linear congruential generators (LCGs) by applying a series of bit-wise
shifts, XORs, rotations and truncations to the hidden state. We show that
Transformers can nevertheless successfully perform in-context prediction on
unseen sequences from diverse PCG variants, in tasks that are beyond published
classical attacks. In our experiments we scale moduli up to $2^{22}$ using up
to $50$ million model parameters and datasets with up to $5$ billion tokens.
Surprisingly, we find even when the output is truncated to a single bit, it can
be reliably predicted by the model. When multiple distinct PRNGs are presented
together during training, the model can jointly learn them, identifying
structures from different permutations. We demonstrate a scaling law with
modulus $m$: the number of in-context sequence elements required for
near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization
enters extended stagnation phases; in our experiments, learning moduli $m \geq
2^{20}$ requires incorporating training data from smaller moduli, demonstrating
a critical necessity for curriculum learning. Finally, we analyze embedding
layers and uncover a novel clustering phenomenon: the model spontaneously
groups the integer inputs into bitwise rotationally-invariant clusters,
revealing how representations can transfer from smaller to larger moduli.

</details>


### [27] [Exploring Human-AI Conceptual Alignment through the Prism of Chess](https://arxiv.org/abs/2510.26025)
*Semyon Lomaso,Judah Goldfeder,Mehmet Hamza Erol,Matthew So,Yao Yan,Addison Howard,Nathan Kutz,Ravid Shwartz Ziv*

Main category: cs.LG

TL;DR: 在棋类任务中，270M参数的Transformer在早期层对人类概念的编码较为准确，但随着层数加深以提升性能的表征发生“外星化”，准确率降至50-65%。通过Chess960数据集（240个专家标注位置、6个概念）以及去除开局理论的随机起始位置实验，概念识别能力下降约10-20%，揭示模型更依赖记忆而非抽象理解。层次分析暴露出胜利表征与人类思维之间的张力，表明追求性能可能导致“外星智能”的风险，并对需要人机协作的创造性AI提出挑战。数据集与代码可在GitHub获得。


<details>
  <summary>Details</summary>
Motivation: 探究AI系统是否真正理解人类概念，还是仅模仿表面模式；通过棋类任务揭示不同层次表征与概念理解的关系；通过Chess960检验去除起始理论后的概念鲁棒性。

Method: 对270M参数的Transformer进行层次分析，评估6个策略概念的识别准确率；构建Chess960数据集，提供240个专家标注的位置，覆盖6个概念；在随机起始位置下去除开局理论，比较不同方法的概念识别能力；进行层表征对比与与人类思维的一致性分析。

Result: 早期层对人类概念的编码可达约85%准确；深层表征尽管提升游戏表现，却呈现“ alien”特征，准确率降至50-65%；去除开局理论后概念识别下降约10-20%；总体证据指向更像记忆模式的概念理解而非抽象通用理解。

Conclusion: 模型在追求高性能时，其胜利表征与人类概念存在显著张力，可能发展出“ alien intelligence”；这对需要人机协作的创造性AI应用提出挑战，强调对齐与可解释性的关注，以及在高性能与人类概念之间实现更好协同的必要性。数据集与代码已公开。

Abstract: Do AI systems truly understand human concepts or merely mimic surface
patterns? We investigate this through chess, where human creativity meets
precise strategic concepts. Analyzing a 270M-parameter transformer that
achieves grandmaster-level play, we uncover a striking paradox: while early
layers encode human concepts like center control and knight outposts with up to
85\% accuracy, deeper layers, despite driving superior performance, drift
toward alien representations, dropping to 50-65\% accuracy. To test conceptual
robustness beyond memorization, we introduce the first Chess960 dataset: 240
expert-annotated positions across 6 strategic concepts. When opening theory is
eliminated through randomized starting positions, concept recognition drops
10-20\% across all methods, revealing the model's reliance on memorized
patterns rather than abstract understanding. Our layer-wise analysis exposes a
fundamental tension in current architectures: the representations that win
games diverge from those that align with human thinking. These findings suggest
that as AI systems optimize for performance, they develop increasingly alien
intelligence, a critical challenge for creative AI applications requiring
genuine human-AI collaboration. Dataset and code are available at:
https://github.com/slomasov/ChessConceptsLLM.

</details>


### [28] [Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods](https://arxiv.org/abs/2510.26038)
*Jiali Cheng,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 知识蒸馏会削弱对偏置的 debiasing 转移；把教师模型知识注入到学生模型并不能提升 debiasing，且效果在不同偏置类型上存在显著差异；提出数据增强、迭代式蒸馏和教师初始化等三条改进策略；首次在大尺度分析 KD 对 debiasing 的影响与内部机制。


<details>
  <summary>Details</summary>
Motivation: 理解知识蒸馏在鲁棒性、对抗 spurious correlations 的 debiasing 能力转移中的作用，以及 KD 对 NLI 与图像分类中的 debiasing 的影响和机制。

Method: 在自然语言推理和图像分类任务中进行大规模实验，比较 debiasing 与 KD 情况下的偏置转移，分析注意力模式与内部电路，识别影响 KD 行为的偏置类型，提出并测试改进策略。

Result: 1) KD 之后 debiasing 能力整体下降；2) 训练得到的去偏模型对教师知识的注入收益有限；3) 整体鲁棒性可能保持稳定，但对不同偏置类型的影响差异显著；4) 指出内部注意力模式与电路与这种行为的关系；并提出三条提升 debiasing 在蒸馏中的可迁移性的方案。

Conclusion: KD 可能扰乱 debiasing 的转移与机制；为设计更有效的 debiasing 方法和 KD 流程提供新见解。提出的数据增强、迭代式蒸馏、教师权重初始化等策略有助于提升 debiasing 在蒸馏过程中的效果；这是首个在大尺度上研究 KD 对 debiasing 及其内部机制的工作。

Abstract: Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.

</details>


### [29] [Towards Scaling Laws for Symbolic Regression](https://arxiv.org/abs/2510.26064)
*David Otte,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 本工作将符号回归（SR）置于规模化研究之下，揭示计算量对 SR 表现的幂律关系，并提出随模型规模变化的 compute 最优超参数规律。


<details>
  <summary>Details</summary>
Motivation: 受语言模型规模化规律启发，首次系统探究 SR 在大规模计算下的表现趋势，以实现对 SR 模型性能的可预测性和高效训练。

Method: 构建端到端的可扩展 Transformer SR 流水线，并在有控制地生成的训练数据上进行实验。覆盖五种模型规模和跨越三个数量级的计算规模，评估验证损失与求解率并拟合幂律关系，进而识别 compute-最优的超参数（批量大小、学习率、token/参数比）。

Result: 验证损失和求解率随计算量呈现明确的幂律趋势；存在随模型规模增加的 compute-最优超参数 scaling，token/参数比约为15，且随计算增加略有上升趋势；结果表明 SR 的性能在很大程度上可由计算量预测。

Conclusion: SR 的规模规律存在并且可被用于资源规划与超参数调优，为训练下一代更大规模的 SR 模型提供了理论与实践基础。

Abstract: Symbolic regression (SR) aims to discover the underlying mathematical
expressions that explain observed data. This holds promise for both gaining
scientific insight and for producing inherently interpretable and generalizable
models for tabular data. In this work we focus on the basics of SR. Deep
learning-based SR has recently become competitive with genetic programming
approaches, but the role of scale has remained largely unexplored. Inspired by
scaling laws in language modeling, we present the first systematic
investigation of scaling in SR, using a scalable end-to-end transformer
pipeline and carefully generated training data. Across five different model
sizes and spanning three orders of magnitude in compute, we find that both
validation loss and solved rate follow clear power-law trends with compute. We
further identify compute-optimal hyperparameter scaling: optimal batch size and
learning rate grow with model size, and a token-to-parameter ratio of
$\approx$15 is optimal in our regime, with a slight upward trend as compute
increases. These results demonstrate that SR performance is largely predictable
from compute and offer important insights for training the next generation of
SR models.

</details>


### [30] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出一种新的机器学习范式，通过在流形上优化度量张量场来实现动态几何结构，从而超越固定几何的参数优化。


<details>
  <summary>Details</summary>
Motivation: 传统参数优化局限于固定几何空间，难以充分表达数据中的几何结构；通过数据驱动的几何学习提高表达能力和鲁棒性。

Method: 在流形上引入变分框架，优化度量张量场，并将流形离散化为三角网格，度量由边长参数化，结合数据保真度与几何复杂性正则化；利用自动微分实现高效优化。

Result: 理论分析揭示框架与爱因斯坦-希伯特作用的深刻类比，指示在固定拓扑下度量优化也具备更强表达能力；提出了可实现的离散方法并为动态图形/元学习提供基础。

Conclusion: 为构建完全动态的“元学习器”奠定基础，并在科学模型发现和鲁棒表示学习等领域具有广泛应用前景。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [31] [New Money: A Systematic Review of Synthetic Data Generation for Finance](https://arxiv.org/abs/2510.26076)
*James Meldrum,Basem Suleiman,Fethi Rabhi,Muhammad Johan Alibasa*

Main category: cs.LG

TL;DR: 对72项自2018年以来的金融领域合成数据研究的系统性综述，结果显示GAN主导生成时间序列和信用数据，但隐私评估不足，强调未来研究在隐私保护上需加强。


<details>
  <summary>Details</summary>
Motivation: 在金融场景中使用敏感数据面临隐私与合规挑战，该综述旨在系统梳理现有工作，评估方法及其局限性，以促进更安全的合成数据解决方案。

Method: 进行系统性综述，纳入72项研究（2018年以来），对所合成的金融信息类型、生成方法、以及用于评估数据实用性与隐私性的评估策略进行分类和分析。

Result: GAN为主导，尤其在生成时间序列市场数据和表格化的信用数据方面表现突出。尽管存在改进现实性和隐私保护的新技术，但对隐私防护的评估仍显不足，缺乏一致的评估标准。

Conclusion: 提供生成技术、应用场景和评估方法的综合概览，指出研究空缺并为未来工作提出方向，以发展健壮、注重隐私保护的金融领域合成数据解决方案。

Abstract: Synthetic data generation has emerged as a promising approach to address the
challenges of using sensitive financial data in machine learning applications.
By leveraging generative models, such as Generative Adversarial Networks (GANs)
and Variational Autoencoders (VAEs), it is possible to create artificial
datasets that preserve the statistical properties of real financial records
while mitigating privacy risks and regulatory constraints. Despite the rapid
growth of this field, a comprehensive synthesis of the current research
landscape has been lacking. This systematic review consolidates and analyses 72
studies published since 2018 that focus on synthetic financial data generation.
We categorise the types of financial information synthesised, the generative
methods employed, and the evaluation strategies used to assess data utility and
privacy. The findings indicate that GAN-based approaches dominate the
literature, particularly for generating time-series market data and tabular
credit data. While several innovative techniques demonstrate potential for
improved realism and privacy preservation, there remains a notable lack of
rigorous evaluation of privacy safeguards across studies. By providing an
integrated overview of generative techniques, applications, and evaluation
methods, this review highlights critical research gaps and offers guidance for
future work aimed at developing robust, privacy-preserving synthetic data
solutions for the financial domain.

</details>


### [32] [Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism](https://arxiv.org/abs/2510.26083)
*Yuhua Jiang,Shuang Cheng,Yihao Liu,Ermo Hua,Che Jiang,Weigao Sun,Yu Cheng,Feifei Gao,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: Nirvana 是一种专门化通用模型（SGM），引入任务感知内存机制、线性时间复杂度和测试时任务信息提取，通过 Trigger 与 Updater 实现对任务相关参数的动态适配。在自然语言任务和专门医学任务上表现竞争甚至优于现有 LLM；在 MRI 任务中冻结骨干并使用轻量编解码器，Trigger 指导域适配，获得更高质量的重建和初步临床报告。


<details>
  <summary>Details</summary>
Motivation: 现有的 Transformer、线性注意力及混合结构缺乏以任务信息为导向的专用内存机制，难以在保持广泛能力的同时达到目标领域的专家级性能；需要一个能根据任务信息动态调节记忆并具有低复杂度的通用-专用混合框架。

Method: 提出 Nirvana 架构，包含专门化内存机制、Task-Aware Memory Trigger（Trigger）和 Specialized Memory Updater（Updater）。每个输入样本视为自监督微调任务，动态调整与任务相关的参数以应对领域移位，且具备线性时间复杂度的内存更新。Trigger 根据当前任务需求灵活触发/调整记忆，Updater 根据 Trigger 指导记忆更新。对于 MRI 任务，后训练对冻结的 Nirvana 骨架使用轻量编解码器对成对的电磁信号与 MRI 图像进行对齐和重建。

Result: 在多种自然语言建模基准上，Nirvana 实现了与现有 LLM 架构相当或更优的表现。对于医学任务，特别是 MRI，Nirvana 的重建质量高于传统 MRI 模型及以传统 LLM 作为骨干的模型，并且能够生成准确的初步临床报告。

Conclusion: 任务感知的内存触发与专门内存更新使 Nirvana 具备在保持广泛能力的同时对目标领域进行专家级适配的能力，且具有较低的计算复杂度。

Abstract: Specialized Generalist Models (SGMs) aim to preserve broad capabilities while
achieving expert-level performance in target domains. However, traditional LLM
structures including Transformer, Linear Attention, and hybrid models do not
employ specialized memory mechanism guided by task information. In this paper,
we present Nirvana, an SGM with specialized memory mechanism, linear time
complexity, and test-time task information extraction. Besides, we propose the
Task-Aware Memory Trigger ($\textit{Trigger}$) that flexibly adjusts memory
mechanism based on the current task's requirements. In Trigger, each incoming
sample is treated as a self-supervised fine-tuning task, enabling Nirvana to
adapt its task-related parameters on the fly to domain shifts. We also design
the Specialized Memory Updater ($\textit{Updater}$) that dynamically memorizes
the context guided by Trigger. We conduct experiments on both general language
tasks and specialized medical tasks. On a variety of natural language modeling
benchmarks, Nirvana achieves competitive or superior results compared to the
existing LLM structures. To prove the effectiveness of Trigger on specialized
tasks, we test Nirvana's performance on a challenging medical task, i.e.,
Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with
lightweight codecs on paired electromagnetic signals and MRI images. Despite
the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI
domain with the change of task-related parameters. Nirvana achieves
higher-quality MRI reconstruction compared to conventional MRI models as well
as the models with traditional LLMs' backbone, and can also generate accurate
preliminary clinical reports accordingly.

</details>


### [33] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: 提出一种基于大语言模型的多阶段管线，用于定位引入漏洞的提交（BIC），充分利用补丁信息和提交文本，显著提升定位准确率。相较现有方法，准确性提升超过38%，相较一个基于LLM的简单二分方法提升约60%。


<details>
  <summary>Details</summary>
Motivation: 传统的补丁化二分定位存在多项障碍：BIC与补丁往往并不直接修改同一函数；除了代码变更，提交信息包含丰富的漏洞相关信息却未被充分利用；现有方法多依赖简单启发式且缺乏对漏洞语义的分析。需要一种能够同时处理文本与代码信息、并在上下文中比较候选提交的新方法。

Method: 提出一个多阶段管线，基于LLMs，全面利用补丁信息、在上下文中对比多个候选提交、通过逐步下筛缩小候选集。阶段包括：1) 充分利用补丁中的文本和代码信息；2) 在上下文中对比候选提交及其相关修订；3) 通过一系列下筛步骤逐步缩小候选集合直至定位 BIC。

Result: 实验结果显示，该方法在准确性方面显著优于最先进的现有解决方案，提升超过38%；相比一个基于LLM的基线二分方法，提升约60%；并且综合多阶段管线是提升效果的关键。

Conclusion: 表明LLMs具备同时理解文本与代码信息的能力，结合多阶段下筛策略可以显著提升漏洞定位效率，推动漏洞定位研究的发展。

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [34] [SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth](https://arxiv.org/abs/2510.26099)
*Nick Masi,Randall Balestriero*

Main category: cs.LG

TL;DR: 提出 SAFE 包以在地球尺度对预测进行分层评估，揭示不同地理、收入、土地覆盖等分层中的表现差异，并建立公平性基准。


<details>
  <summary>Details</summary>
Motivation: 全球平均指标掩盖地理与社会经济分布的不均导致的模型性能差异。

Method: 将地理网格点按国家、全球子区域、收入水平和土地覆盖进行分层，对多种天气预测模型的预测进行逐层评估，整合多域数据以支持分层分析。

Result: 发现多数模型在不同分层中存在显著差异；某些国家、地区、收入水平或土地覆盖类型的预测技能明显不足；建立了一个开源的分层预测公平性基准。

Conclusion: 不再依赖全局平均指标，强调分层评估的重要性；SAFE 工具有助于识别薄弱环节、促进模型的公平性改进，且开源以支持可重复性。

Abstract: The dominant paradigm in machine learning is to assess model performance
based on average loss across all samples in some test set. This amounts to
averaging performance geospatially across the Earth in weather and climate
settings, failing to account for the non-uniform distribution of human
development and geography. We introduce Stratified Assessments of Forecasts
over Earth (SAFE), a package for elucidating the stratified performance of a
set of predictions made over Earth. SAFE integrates various data domains to
stratify by different attributes associated with geospatial gridpoints:
territory (usually country), global subregion, income, and landcover (land or
water). This allows us to examine the performance of models for each individual
stratum of the different attributes (e.g., the accuracy in every individual
country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of
state-of-the-art AI-based weather prediction models, finding that they all
exhibit disparities in forecasting skill across every attribute. We use this to
seed a benchmark of model forecast fairness through stratification at different
lead times for various climatic variables. By moving beyond globally-averaged
metrics, we for the first time ask: where do models perform best or worst, and
which models are most fair? To support further work in this direction, the SAFE
package is open source and available at https://github.com/N-Masi/safe

</details>


### [35] [Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error](https://arxiv.org/abs/2510.26109)
*Chenming Tang,Hsiu-Yuan Huang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: LTE通过让LLM从自生成的错误答案和过长回答中学习进行从试错学习，不依赖外部专家引导，缓解探索停滞，提升在六个数学基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有RLVR仅依赖模型自身生成回答、受初始能力限制而易于探索停滞的问题，且缺乏可扩展的外部引导。

Method: 提出LTE方法，在训练中通过提示LLM回顾其先前的错误答案以及回答过长的问题，进行改进的试错学习；无需外部专家指导，强化对错判和回答长度的管理；在Qwen3-4B-Base等模型和多数学科基准上与GRPO的对比。

Result: 在六个数学基准上，LTE对Qwen3-4B-Base的Pass@1提高了6.38，Pass@k提高了9.00，显示显著的性能提升。

Conclusion: LTE有效缓解探索停滞，提升训练过程中的开发性与探索性，且无需外部专家指导，具有在RLVR场景中推广应用的潜力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has significantly
boosted the reasoning capability of large language models (LLMs) recently.
However, existing RLVR approaches merely train LLMs based on their own
generated responses and are constrained by the initial capability of LLMs, thus
prone to exploration stagnation, in which LLMs fail to solve more training
problems and cannot further learn from the training data. Some work tries to
address this by leveraging off-policy solutions to training problems but
requires external guidance from experts which suffers from limited
availability. In this work, we propose LTE (Learning to reason from Trial and
Error), an approach hinting LLMs with their previously self-generated incorrect
answers and problem of overlong responses, which does not require any external
expert guidance. Experiments validate the effectiveness of LTE, which
outperforms the normal group relative policy optimization (GRPO) by 6.38 in
Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for
Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the
problem of exploration stagnation and enhances both exploitation and
exploration during training.

</details>


### [36] [maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition](https://arxiv.org/abs/2510.26146)
*Kexing Liu*

Main category: cs.LG

TL;DR: maxVSTAR 是一个基于跨模态教师-学生架构的闭环自适应框架，通过一个YOLO视觉模型对CSI数据流进行实时标签，实现边缘端的在线自监督微调（STAR）。在未校准硬件上显著缓解域移，将准确率从49.14%提高到81.51%，并在对照基线下达到隐私友好、可扩展的长期HAR。


<details>
  <summary>Details</summary>
Motivation: CSI 基于通道状态信息的HAR在边缘设备部署时易受环境和硬件变化的影响，导致域移后性能下降；需要无需人工干预、隐私友好的自适应方法来实现长期自治的HAR。

Method: 提出闭环的跨模态教师-学生框架，使用YOLO等高精度视觉模型作为动态监督信号，为CSI数据提供实时活动标签；边缘端对轻量级的STAR模型进行在线微调，完成自监督的在线重训练。

Result: 在未校准硬件条件下，基线STAR准确率由93.52%下降至49.14%；经过一次视觉引导的自适应循环，准确率回升至81.51%，证明了在隐私敏感的物联网环境中进行动态自适应的有效性。

Conclusion: maxVSTAR实现了面向边缘CSI HAR的动态自适应，借助视觉信号进行自监督微调，具备良好的可扩展性与实际应用潜力。

Abstract: WiFi Channel State Information (CSI)-based human activity recognition (HAR)
provides a privacy-preserving, device-free sensing solution for smart
environments. However, its deployment on edge devices is severely constrained
by domain shift, where recognition performance deteriorates under varying
environmental and hardware conditions. This study presents maxVSTAR (maximally
adaptive Vision-guided Sensing Technology for Activity Recognition), a
closed-loop, vision-guided model adaptation framework that autonomously
mitigates domain shift for edge-deployed CSI sensing systems. The proposed
system integrates a cross-modal teacher-student architecture, where a
high-accuracy YOLO-based vision model serves as a dynamic supervisory signal,
delivering real-time activity labels for the CSI data stream. These labels
enable autonomous, online fine-tuning of a lightweight CSI-based HAR model,
termed Sensing Technology for Activity Recognition (STAR), directly at the
edge. This closed-loop retraining mechanism allows STAR to continuously adapt
to environmental changes without manual intervention. Extensive experiments
demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated
hardware, the baseline STAR model's recognition accuracy declined from 93.52%
to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored
the accuracy to 81.51%. These results confirm the system's capacity for
dynamic, self-supervised model adaptation in privacy-conscious IoT
environments, establishing a scalable and practical paradigm for long-term
autonomous HAR using CSI sensing at the network edge.

</details>


### [37] [STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments](https://arxiv.org/abs/2510.26148)
*Kexing Liu*

Main category: cs.LG

TL;DR: STAR是一种用于实时、轻量化边缘端人类活动识别的框架，通过GRU为核心的轻量化时序模型、分阶段信号降噪，以及硬件感知的协同优化，在低功耗嵌入设备上实现隐私保护的实时HAR，在七类活动上的平均识别率达到93.52%并具备低延迟和高能效。


<details>
  <summary>Details</summary>
Motivation: 解决现有HAR方法在资源受限的嵌入式移动边缘环境中的计算效率低、时延高、可行性差等问题；实现隐私保护的无接触监测，适用于智能家居、健康监测和移动物联网。

Method: 提出STAR框架，采用轻量GRURNN，参数比传统LSTM减少约33%，并引入多阶段预处理流水线（中值滤波、8阶巴特沃斯低通滤波、经验模态分解）以去噪并提取时空特征；在Rockchip RV1126+嵌入式NPU上进行端侧部署，CSI采集模块为ESP32-S3；量化为INT8，推理频率33MHz，CPU占用约8%，与CPU端相比实现六倍加速；模型参数约97.6k。

Result: 在七类活动上平均识别准确率为93.52%；对人体存在检测达到99.11%；INT8推理下 processing speed 33 MHz，功耗低，具备亚秒级响应；实现了实时、隐私保护的HAR。

Conclusion: STAR提供一种面向移动与普适计算环境的可扩展解决方案，具有显著的计算效率、低延迟和隐私保护特性，适合资源受限的边缘设备场景。

Abstract: Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)
presents a privacy-preserving, contactless sensing approach suitable for smart
homes, healthcare monitoring, and mobile IoT systems. However, existing methods
often encounter computational inefficiency, high latency, and limited
feasibility within resource-constrained, embedded mobile edge environments.
This paper proposes STAR (Sensing Technology for Activity Recognition), an
edge-AI-optimized framework that integrates a lightweight neural architecture,
adaptive signal processing, and hardware-aware co-optimization to enable
real-time, energy-efficient HAR on low-power embedded devices. STAR
incorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural
network, reducing model parameters by 33% compared to conventional LSTM models
while maintaining effective temporal modeling capability. A multi-stage
pre-processing pipeline combining median filtering, 8th-order Butterworth
low-pass filtering, and Empirical Mode Decomposition (EMD) is employed to
denoise CSI amplitude data and extract spatial-temporal features. For on-device
deployment, STAR is implemented on a Rockchip RV1126 processor equipped with an
embedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI
acquisition module. Experimental results demonstrate a mean recognition
accuracy of 93.52% across seven activity classes and 99.11% for human presence
detection, utilizing a compact 97.6k-parameter model. INT8 quantized inference
achieves a processing speed of 33 MHz with just 8% CPU utilization, delivering
sixfold speed improvements over CPU-based execution. With sub-second response
latency and low power consumption, the system ensures real-time,
privacy-preserving HAR, offering a practical, scalable solution for mobile and
pervasive computing environments.

</details>


### [38] [Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment](https://arxiv.org/abs/2510.26157)
*Hyuntae Park,Yeachan Kim,SangKeun Lee*

Main category: cs.LG

TL;DR: MolBridge 会使用基于子结构的对比学习，并配合自我精炼机制，将分子子结构与化学短语对齐，从而获得更细粒度的分子-文本表示，并在多项分子基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的分子-文本表示模型通常难以捕捉分子与描述之间的细粒度对齐，导致对相似分子或描述的区分能力不足。需要引入子结构层面的对齐信号来增强细粒度对齐能力。

Method: 在分子-描述对中增加来自分子子结构与化学短语的对齐信号；引入子结构感知对比学习以学习更细粒度的跨模态表示；引入自我精炼机制以滤除噪声对齐信号。

Result: 在多种分子基准上，MolBridge 显著优于最先进的基线，表明子结构感知对齐在分子-文本学习中的有效性。

Conclusion: 子结构感知对齐是提升分子-文本理解的有效方向，MolBridge 提供了一种可扩展的框架，可进一步拓展到其他跨模态或领域。

Abstract: Molecule and text representation learning has gained increasing interest due
to its potential for enhancing the understanding of chemical information.
However, existing models often struggle to capture subtle differences between
molecules and their descriptions, as they lack the ability to learn
fine-grained alignments between molecular substructures and chemical phrases.
To address this limitation, we introduce MolBridge, a novel molecule-text
learning framework based on substructure-aware alignments. Specifically, we
augment the original molecule-description pairs with additional alignment
signals derived from molecular substructures and chemical phrases. To
effectively learn from these enriched alignments, MolBridge employs
substructure-aware contrastive learning, coupled with a self-refinement
mechanism that filters out noisy alignment signals. Experimental results show
that MolBridge effectively captures fine-grained correspondences and
outperforms state-of-the-art baselines on a wide range of molecular benchmarks,
highlighting the significance of substructure-aware alignment in molecule-text
learning.

</details>


### [39] [Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series](https://arxiv.org/abs/2510.26159)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 在高度不平衡且时间上不确定的工业时间序列中，使用简单的随机森林+XGBoost集成并对数据进行分段处理，优于复杂的特征工程与混合模型在爆发性异常检测中的表现。


<details>
  <summary>Details</summary>
Motivation: 评估先进特征工程和混合模型架构在多变量工业时间序列异常检测中的有效性，尤其是在受限的样本不平衡和时间不确定性情境下，是否需要回归到更简单、可解释的模型。

Method: 比较多种方法：1) 基于变化点的统计特征、2) 基于聚类的子结构表征、3) 混合学习策略等复杂方法；以及 4) 在分段数据上训练的简单随机森林+XGBoost 集成。通过评估 AUC-ROC、F1 和提早检测率等指标来比较性能。

Result: 复杂方法均不及简单的随机森林+XGBoost 集成。后者在分段数据上实现了 AUC-ROC 0.976、F1 0.41，以及在指定时间窗内100%的早期检测率。

Conclusion: 在高度不平衡且存在时间不确定性的场景中，模型简化、结合优化分段策略，能够提供更高的鲁棒性、可解释性和可操作性，优于过于复杂的架构。

Abstract: In this study, we investigate the effectiveness of advanced feature
engineering and hybrid model architectures for anomaly detection in a
multivariate industrial time series, focusing on a steam turbine system. We
evaluate the impact of change point-derived statistical features,
clustering-based substructure representations, and hybrid learning strategies
on detection performance. Despite their theoretical appeal, these complex
approaches consistently underperformed compared to a simple Random Forest +
XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of
0.976, F1-score of 0.41, and 100% early detection within the defined time
window. Our findings highlight that, in scenarios with highly imbalanced and
temporally uncertain data, model simplicity combined with optimized
segmentation can outperform more sophisticated architectures, offering greater
robustness, interpretability, and operational utility.

</details>


### [40] [A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation](https://arxiv.org/abs/2510.26184)
*Songxin Lei,Qiongyan Wang,Yanchen Zhu,Hanyu Yao,Sijie Ruan,Weilin Ruan,Yuyu Luo,Huaming Wu,Yuxuan Liang*

Main category: cs.LG

TL;DR: A new collaborative public resource allocation problem with capacity constraints and spatio-temporal dynamics; proposes a game-theoretic spatio-temporal RL framework (GSTRL) to approximate Nash equilibrium via a potential game, showing superior performance on real datasets.


<details>
  <summary>Details</summary>
Motivation: Existing public resource allocation methods optimize individual resource movements and ignore capacity constraints and joint spatio-temporal dynamics, making the problem NP-hard and impractical for real-world systems.

Method: Formulate CPRA as a potential game and develop the GSTRL framework that captures spatio-temporal dynamics. The key theoretical result is that the potential function aligns with the optimal target, enabling approximation of Nash equilibrium for the NP-hard problem.

Result: Empirical evaluation on two real-world datasets demonstrates superior performance of GSTRL; code is provided in supplementary materials.

Conclusion: GSTRL provides a theoretically grounded and practically effective approach to CPRA by uniting game theory with spatio-temporal RL, with potential for broader impact and future extensions.

Abstract: Public resource allocation involves the efficient distribution of resources,
including urban infrastructure, energy, and transportation, to effectively meet
societal demands. However, existing methods focus on optimizing the movement of
individual resources independently, without considering their capacity
constraints. To address this limitation, we propose a novel and more practical
problem: Collaborative Public Resource Allocation (CPRA), which explicitly
incorporates capacity constraints and spatio-temporal dynamics in real-world
scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal
Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:
1) We formulate the CPRA problem as a potential game and demonstrate that there
is no gap between the potential function and the optimal target, laying a solid
theoretical foundation for approximating the Nash equilibrium of this NP-hard
problem; and 2) Our designed GSTRL framework effectively captures the
spatio-temporal dynamics of the overall system. We evaluate GSTRL on two
real-world datasets, where experiments show its superior performance. Our
source codes are available in the supplementary materials.

</details>


### [41] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 本研究使用机器学习（逻辑回归、随机森林、SVM）在高维健康险数据上通过主成分分析降维来预测全因读回并比较AUC，结果显示随机森林表现最佳，其它模型次之。


<details>
  <summary>Details</summary>
Motivation: 目标是降低可预防的住院再入院率，读回率作为医院质量的基准，利用 claims 数据识别关键因素，以便更准确地识别需重点关注的患者，降低成本并提升护理质量。

Method: 在高维健康保险理赔数据上应用逻辑回归、随机森林、SVM，并通过PCA降维后构建回归/分类模型；以AUC评估模型性能，比较不同模型。

Result: 随机森林达到最高AUC，其次是逻辑回归，再是SVM；三者可用于识别导致再入院的关键因素并定位高风险患者。

Conclusion: 这些模型可帮助识别关键因素并帮助定向干预，从而降低再入院率并降低成本、提升护理质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [42] [Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space](https://arxiv.org/abs/2510.26219)
*Sekitoshi Kanai,Tsukasa Yoshida,Hiroshi Takahashi,Haru Kuroki,Kazumune Hashimoto*

Main category: cs.LG

TL;DR: 提出一种在测试时进行对齐的高效方法AISP（adaptive importance sampling on pre-logits），通过对预 logits 处的高斯扰动来最大化期望奖励，并利用重要性采样估计最佳扰动均值。相比best-of-n与其他基于奖励的测试时对齐方法，AISP在样本效率和奖励水平上表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决在大语言模型中进行对齐时需要高昂微调成本的问题；在推理阶段通过采样与控制输入实现对齐，以降低计算代价。

Method: 在模型的前一隐藏层输出（pre-logits）引入高斯扰动，优化扰动均值以最大化期望奖励；最优均值通过对样本奖励的重要性采样来估计；与best-of-n和其他基于奖励的测试时对齐方法比较。

Result: AISP在可用样本数量方面实现更高的奖励，且优于best-of-n；在奖励表现方面也优于其他奖励驱动的测试时对齐方法。

Conclusion: AISP为测试时对齐提供一种高效、基于扰动与重要性采样的新思路，能在不高成本微调的前提下提高奖励表现。

Abstract: Test-time alignment of large language models (LLMs) attracts attention
because fine-tuning LLMs requires high computational costs. In this paper, we
propose a new test-time alignment method called adaptive importance sampling on
pre-logits (AISP) on the basis of the sampling-based model predictive control
with the stochastic control input. AISP applies the Gaussian perturbation into
pre-logits, which are outputs of the penultimate layer, so as to maximize
expected rewards with respect to the mean of the perturbation. We demonstrate
that the optimal mean is obtained by importance sampling with sampled rewards.
AISP outperforms best-of-n sampling in terms of rewards over the number of used
samples and achieves higher rewards than other reward-based test-time alignment
methods.

</details>


### [43] [MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines](https://arxiv.org/abs/2510.26230)
*Minyi Peng,Darian Gunamardi,Ivan Tjuawinata,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出一种基于顺序学习的“归纳”视角的机器撤销方法：在模型末尾追加投影-再分配层，通过逆序最后一个训练序列实现知识剥离，具备模块化、模型无关和对原始数据/模型少访问的特点，实验在 CIFAR-10/100（CNN）和 Covertype（树模型）上接近完全重新训练的输出，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有机器 unlearning 方法多聚焦于理论定义或优化目标，但在实际部署中往往受限于需要完整访问原始数据和模型、难以扩展。需要一种更实用、可与现有管线无缝集成、且对资源和数据访问要求低的撤销方案。

Method: 将分类训练视为逐类学习的过程，通过逆向最后的训练序列实现撤销；在模型末端新增一个投影-再分配层，作为输出过滤器，完成知识剥离，模块化、无须大规模数据访问即可集成到现有分类系统。

Result: 在多个数据集上验证，包括 CIFAR-10/100（CNN 模型）和 Covertype（树模型），实验显示输出与完全重新训练的模型非常接近，同时在计算成本上有显著降低。

Conclusion: 该方法展示了在实际系统中的 applicability、 scalability 与 compatibility，能够以最小干预实现有效的知识撤回，适合在现实场景中落地部署。

Abstract: As a new and promising approach, existing machine unlearning (MU) works
typically emphasize theoretical formulations or optimization objectives to
achieve knowledge removal. However, when deployed in real-world scenarios, such
solutions typically face scalability issues and have to address practical
requirements such as full access to original datasets and model. In contrast to
the existing approaches, we regard classification training as a sequential
process where classes are learned sequentially, which we call \emph{inductive
approach}. Unlearning can then be done by reversing the last training sequence.
This is implemented by appending a projection-redistribution layer in the end
of the model. Such an approach does not require full access to the original
dataset or the model, addressing the challenges of existing methods. This
enables modular and model-agnostic deployment as an output filter into existing
classification pipelines with minimal alterations. We conducted multiple
experiments across multiple datasets including image (CIFAR-10/100 using
CNN-based model) and tabular datasets (Covertype using tree-based model).
Experiment results show consistently similar output to a fully retrained model
with a high computational cost reduction. This demonstrates the applicability,
scalability, and system compatibility of our solution while maintaining the
performance of the output in a more practical setting.

</details>


### [44] [Angular Steering: Behavior Control via Rotation in Activation Space](https://arxiv.org/abs/2510.26243)
*Hieu M. Vu,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出 Angular Steering，通过在固定二维子空间内旋转激活来控制模型行为，提供连续可调的拒绝/服从等行为，并通过 Adaptive Angular Steering 增强稳定性。


<details>
  <summary>Details</summary>
Motivation: 在保持通用能力的同时实现对特定行为的稳健控制是安全可靠部署的核心挑战；现有的向量加法/方向抑制等方法受限于二维子空间，易对非目标特征产生副作用并对参数敏感。

Method: 将行为引导视为在一个固定的二维子空间中进行向量旋转（几何旋转），通过朝向或远离目标行为方向来实现干预。提出 Angular Steering，并扩展为 Adaptive Angular Steering，仅旋转与目标特征对齐的激活。将其作为统一的旋转框架，覆盖并简化既有加法/正交化等技术的参数选择，保持模型稳定性。通过多模型家族和规模的实验验证其鲁棒性与泛化性。

Result: 在保持通用语言模型性能的前提下实现稳健的行为控制；相比前案，Angular Steering更具灵活性和鲁棒性，Adaptive 进一步提升稳定性与一致性；在不同模型家族和大小上的实验显示良好泛化。

Conclusion: 给出一个统一、灵活且稳定的行为控制框架，参数更易选取，适用范围更广；并提供代码与实验材料以供复现。

Abstract: Controlling specific behaviors in large language models while preserving
their general capabilities is a central challenge for safe and reliable
artificial intelligence deployment. Current steering methods, such as vector
addition and directional ablation, are constrained within a two-dimensional
subspace defined by the activation and feature direction, making them sensitive
to chosen parameters and potentially affecting unrelated features due to
unintended interactions in activation space. We introduce Angular Steering, a
novel and flexible method for behavior modulation that operates by rotating
activations within a fixed two-dimensional subspace. By formulating steering as
a geometric rotation toward or away from a target behavior direction, Angular
Steering provides continuous, fine-grained control over behaviors such as
refusal and compliance. We demonstrate this method using refusal steering
emotion steering as use cases. Additionally, we propose Adaptive Angular
Steering, a selective variant that rotates only activations aligned with the
target feature, further enhancing stability and coherence. Angular Steering
generalizes existing addition and orthogonalization techniques under a unified
geometric rotation framework, simplifying parameter selection and maintaining
model stability across a broader range of adjustments. Experiments across
multiple model families and sizes show that Angular Steering achieves robust
behavioral control while maintaining general language modeling performance,
underscoring its flexibility, generalization, and robustness compared to prior
approaches. Code and artifacts are available at
https://github.com/lone17/angular-steering/.

</details>


### [45] [Likely Interpolants of Generative Models](https://arxiv.org/abs/2510.26266)
*Frederik Möbius Rygaard,Shen Zhu,Yinzhu Jin,Søren Hauberg,Tom Fletcher*

Main category: cs.LG

TL;DR: 提出一种通用的插值方案，生成的插值路径类似于在数据分布约束下的测地线，无需额外训练即可对齐不同度量和分布。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成模型在没有对模型或数据维度施加严格假设的情况下难以获得 principled 的插值路径的问题，期望获得在多种度量和分布下的可控、可解释的过渡路径。

Method: 给出一个新颖的算法来计算这类曲线，使其在数据分布上受约束且与不同度量兼容；该算法不需要额外的训练；从理论上可在局部视为在合适黎曼度量下的测地线。

Result: 在多种模型与数据集上，所提出的插值在高密度区域的遍历性优于基线方法，且对不同度量和分布具有鲁棒性。

Conclusion: 该方法提供了一种一般性的插值方案，具有几何直觉并可在不额外训练的情况下实现，提升了对生成模型的控制与可解释性。

Abstract: Interpolation in generative models allows for controlled generation, model
inspection, and more. Unfortunately, most generative models lack a principal
notion of interpolants without restrictive assumptions on either the model or
data dimension. In this paper, we develop a general interpolation scheme that
targets likely transition paths compatible with different metrics and
probability distributions. We consider interpolants analogous to a geodesic
constrained to a suitable data distribution and derive a novel algorithm for
computing these curves, which requires no additional training. Theoretically,
we show that our method locally can be considered as a geodesic under a
suitable Riemannian metric. We quantitatively show that our interpolation
scheme traverses higher density regions than baselines across a range of models
and datasets.

</details>


### [46] [Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation](https://arxiv.org/abs/2510.26278)
*Kim Yong Tan,Yueming Lyu,Ivor Tsang,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出 IMG：在推断时对扩散过程进行多目标加权重采样，以单次扩散生成实现多目标优化的高超越量，显著超越需要多次扩散生成的基线方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能学习复杂分布并被用于高维多目标黑盒优化；现有方法多将扩散模型视为黑箱，依赖外部进化算法等循环，未充分利用扩散过程的内部分布转换，效率受限。

Method: 在扩散生成过程中进行加权重采样，使样本按多目标期望值汇总的 Boltzmann 分布来分布；推导出该多目标 Boltzmann 分布具有对数似然解释，是分布式多目标优化问题的最优解；实现 IMG 于多目标分子生成任务，且仅需一次生成通道。

Result: 在多目标分子生成任务中，IMG 单次生成即可实现显著提高的超体积（hypervolume），超过通常需要数百次扩散迭代的基线优化算法；可视为对扩散过程的优化，且可嵌入现有方法以进一步提升性能。

Conclusion: IMG 将推断时的扩散过程优化用于多目标生成，提供对数似然层面的理论解释，且在实际任务中展现出高效且可集成的优势。

Abstract: Diffusion models have been successful in learning complex data distributions.
This capability has driven their application to high-dimensional
multi-objective black-box optimization problem. Existing approaches often
employ an external optimization loop, such as an evolutionary algorithm, to the
diffusion model. However, these approaches treat the diffusion model as a
black-box refiner, which overlooks the internal distribution transition of the
diffusion generation process, limiting their efficiency. To address these
challenges, we propose the Inference-time Multi-target Generation (IMG)
algorithm, which optimizes the diffusion process at inference-time to generate
samples that simultaneously satisfy multiple objectives. Specifically, our IMG
performs weighted resampling during the diffusion generation process according
to the expected aggregated multi-objective values. This weighted resampling
strategy ensures the diffusion-generated samples are distributed according to
our desired multi-target Boltzmann distribution. We further derive that the
multi-target Boltzmann distribution has an interesting log-likelihood
interpretation, where it is the optimal solution to the distributional
multi-objective optimization problem. We implemented IMG for a multi-objective
molecule generation task. Experiments show that IMG, requiring only a single
generation pass, achieves a significantly higher hypervolume than baseline
optimization algorithms that often require hundreds of diffusion generations.
Notably, our algorithm can be viewed as an optimized diffusion process and can
be integrated into existing methods to further improve their performance.

</details>


### [47] [Empirical Bayesian Multi-Bandit Learning](https://arxiv.org/abs/2510.26284)
*Xia Jiang,Rong J. B. Zhu*

Main category: cs.LG

TL;DR: 提出一个基于经验贝叶斯的层次贝叶斯多带宽带问题框架，学习跨带任务的协方差结构并将其整合进决策过程，提出 ebmTS 与 ebmUCB 两种高效算法，给出频率派的理论上界，并在合成和真实数据集上显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多任务/多实例上下文的带臂问题中，利用实例之间的相关性和异质性以提升决策性能，但现有方法往往忽略跨带的协方差结构及先验信息的学习。因此需要一个能够同时建模协方差并将其用于强化学习式的带臂策略的框架。

Method: 提出一个层次贝叶斯模型，捕捉不同带臂实例之间的异质性和相关性，通过经验贝叶斯估计先验分布的协方差矩阵，将估计的先验嵌入到 Thompson Sampling (ebmTS) 和 UCB (ebmUCB) 的决策过程中，从而实现信息共享与个体差异的兼顾。给出这两个算法的实现细节与计算效率分析。

Result: 给出频率派的累积 regrets 上界，填补了跨带带臂问题中协方差学习的研究空缺；在合成与真实数据集上的广泛实验显示，所提方法在复杂环境中显著降低累计 regret，相较现有方法具有更好的探索-开发权衡表现。

Conclusion: 所提出的经验贝叶斯层次模型及两种算法为多带带臂学习提供了更灵活的信息共享机制，证明学习协方差结构对提升跨任务带臂决策性能具有重要作用，具备在现实多任务场景中的潜在应用价值。

Abstract: Multi-task learning in contextual bandits has attracted significant research
interest due to its potential to enhance decision-making across multiple
related tasks by leveraging shared structures and task-specific heterogeneity.
In this article, we propose a novel hierarchical Bayesian framework for
learning in various bandit instances. This framework captures both the
heterogeneity and the correlations among different bandit instances through a
hierarchical Bayesian model, enabling effective information sharing while
accommodating instance-specific variations. Unlike previous methods that
overlook the learning of the covariance structure across bandits, we introduce
an empirical Bayesian approach to estimate the covariance matrix of the prior
distribution.This enhances both the practicality and flexibility of learning
across multi-bandits. Building on this approach, we develop two efficient
algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and
ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which
incorporate the estimated prior into the decision-making process. We provide
the frequentist regret upper bounds for the proposed algorithms, thereby
filling a research gap in the field of multi-bandit problems. Extensive
experiments on both synthetic and real-world datasets demonstrate the superior
performance of our algorithms, particularly in complex environments. Our
methods achieve lower cumulative regret compared to existing techniques,
highlighting their effectiveness in balancing exploration and exploitation
across multi-bandits.

</details>


### [48] [Offline Clustering of Preference Learning with Active-data Augmentation](https://arxiv.org/abs/2510.26301)
*Jingyuan Liu,Fatemeh Ghaffari,Xuchuang Wang,Mohammad Hajiesmaili,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出离线偏好学习的聚类框架 Off-C^2PL，解决跨用户偏好聚合及数据不平衡问题；并扩展为主动数据增强的 A^2-Off-C^2PL，以在测试用户的最弱信息维度上有选择地获取样本。


<details>
  <summary>Details</summary>
Motivation: 在离线条件下需从多用户的偏好数据中学习并对测试用户最大化效用；现实场景中用户偏好存在差异，且离线数据在不同维度上通常不平衡。需同时解决跨用户相似性识别与数据不平衡两大挑战。

Method: 提出 Off-C^2PL：纯离线设定下通过多用户数据的聚类结构实现偏好学习并给出与样本噪声/偏差权衡相关的子最优界。扩展为 A^2-Off-C^2PL：在受限的主动数据扩增条件下，根据聚类结构主动选择用于测试用户的样本，重点关注测试用户偏好中的信息量最低维度。

Result: 给出理论子最优界的稳定性分析，揭示噪声与偏差之间的权衡；主动采样设计在信息量最弱的维度上更具信息增益，其贡献优于纯离线数据。通过对合成数据和真实数据集的仿真验证了方法的有效性。

Conclusion: 提供了一个可在多用户离线偏好学习场景下的聚类框架，并证明主动数据增强能显著提升样本利用效率。未来工作可拓展到更复杂的偏好结构、在线适应等场景。

Abstract: Preference learning from pairwise feedback is a widely adopted framework in
applications such as reinforcement learning with human feedback and
recommendations. In many practical settings, however, user interactions are
limited or costly, making offline preference learning necessary. Moreover,
real-world preference learning often involves users with different preferences.
For example, annotators from different backgrounds may rank the same responses
differently. This setting presents two central challenges: (1) identifying
similarity across users to effectively aggregate data, especially under
scenarios where offline data is imbalanced across dimensions, and (2) handling
the imbalanced offline data where some preference dimensions are
underrepresented. To address these challenges, we study the Offline Clustering
of Preference Learning problem, where the learner has access to fixed datasets
from multiple users with potentially different preferences and aims to maximize
utility for a test user. To tackle the first challenge, we first propose
Off-C$^2$PL for the pure offline setting, where the learner relies solely on
offline data. Our theoretical analysis provides a suboptimality bound that
explicitly captures the tradeoff between sample noise and bias. To address the
second challenge of inbalanced data, we extend our framework to the setting
with active-data augmentation where the learner is allowed to select a limited
number of additional active-data for the test user based on the cluster
structure learned by Off-C$^2$PL. In this setting, our second algorithm,
A$^2$-Off-C$^2$PL, actively selects samples that target the least-informative
dimensions of the test user's preference. We prove that these actively
collected samples contribute more effectively than offline ones. Finally, we
validate our theoretical results through simulations on synthetic and
real-world datasets.

</details>


### [49] [Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens](https://arxiv.org/abs/2510.26302)
*Ziliang Chen,Tianang Xiao,Jusheng Zhang,Yongsen Zheng,Xipeng Chen*

Main category: cs.LG

TL;DR: 提出一个面向语言 token 的因果表示学习（CRL）框架并在 token-level SCM 下扩展块可识别性，以解释 CLIP 在组合推理中的脆弱性、存在伪最优编码器在对齐下对组合操作不敏感，以及由此引发的负样本挖掘需求。


<details>
  <summary>Details</summary>
Motivation: CLIP 在跨模态对齐方面表现出色，但在对象-属性-关系的组合推理上常呈现“袋装词匹配”的局限。以往的因果分析多将文本视为单向向量，忽略 token 级结构，难以解释提示敏感性与难负样本的问题。

Method: 提出 token-aware CRL 框架，建立逐步的语言 token SCM；将 block identifiability 扩展到 tokenized 文本；证明在句子级和 token-level SCM 下，CLIP 的对比目标能恢复模态不变的潜在变量；证明存在伪最优文本编码器在实现完美模态对齐的同时对 SWAP、REPLACE、ADD 等原子概念操作不敏感，从而无法区分正确描述和困难负样本；分析语言端非 identifiability 如何通过模态差距影响视觉端，并讨论迭代的组合操作如何加剧学习难度及对负样本挖掘的启示。

Result: 理论层面扩展了 token-level identifiability，解释了 CLIP 的组合脆弱性及伪最优编码器在同一训练目标下的对齐与区分能力的分离；揭示语言-视觉模态差距与负样本挖掘难度之间的联系。

Conclusion: 要提升跨模态对齐的鲁棒性，需要引入更强的负采样策略与 token-level 约束，避免对 SWAP/REPLACE/ADD 等操作的不敏感，并促使模型对正确组合的描述做出敏感且区分性的响应。

Abstract: Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal
generalization by aligning images and texts in a shared embedding space, yet it
persistently fails at compositional reasoning over objects, attributes, and
relations often behaving like a bag-of-words matcher. Prior causal accounts
typically model text as a single vector, obscuring token-level structure and
leaving core phenomena-such as prompt sensitivity and failures on hard
negatives unexplained. We address this gap with a token-aware causal
representation learning (CRL) framework grounded in a sequential,
language-token SCM. Our theory extends block identifiability to tokenized text,
proving that CLIP's contrastive objective can recover the modal-invariant
latent variable under both sentence-level and token-level SCMs. Crucially,
token granularity yields the first principled explanation of CLIP's
compositional brittleness: composition nonidentifiability. We show the
existence of pseudo-optimal text encoders that achieve perfect modal-invariant
alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations
over atomic concepts, thereby failing to distinguish correct captions from hard
negatives despite optimizing the same training objective as true-optimal
encoders. The analysis further links language-side nonidentifiability to
visual-side failures via the modality gap and shows how iterated composition
operators compound hardness, motivating improved negative mining strategies.

</details>


### [50] [Model Inversion with Layer-Specific Modeling and Alignment for Data-Free Continual Learning](https://arxiv.org/abs/2510.26311)
*Ruilin Tong,Haodong Lu,Yuhang Liu,Dong Gong*

Main category: cs.LG

TL;DR: Data-free continual learning via per-layer model inversion (PMI) and Gaussian feature modeling to generate semantic-aware pseudo-images for replay, reducing computation and mitigating feature drift in large models like CLIP.


<details>
  <summary>Details</summary>
Motivation: Continual learning without access to previous data is necessary due to privacy, security, or practical constraints. Existing inversion approaches suffer from drift (inputs from compressed outputs) and high computational cost, especially for large pre-trained models.

Method: Propose Per-layer Model Inversion (PMI) to provide strong initialization for full-model inversion by optimizing per-layer parameters, accelerating convergence. Model class-wise features with Gaussian distributions and a contrastive objective to align synthetic features with real features, mitigating drift. Generate pseudo-images from semantic-aware projected features for replay in CL settings.

Result: Claimed strong effectiveness and compatibility across multiple continual learning settings, with improved efficiency and better alignment between synthetic and real features, enabling data-free replay in large models.

Conclusion: Combining PMI and feature modeling enables effective data-free continual learning by generating informative pseudo-images that preserve prior knowledge while enabling replay in scenarios where data cannot be stored or accessed.

Abstract: Continual learning (CL) aims to incrementally train a model on a sequence of
tasks while retaining performance on prior ones. However, storing and replaying
data is often infeasible due to privacy or security constraints and impractical
for arbitrary pre-trained models. Data-free CL seeks to update models without
access to previous data. Beyond regularization, we employ model inversion to
synthesize data from the trained model, enabling replay without storing
samples. Yet, model inversion in predictive models faces two challenges: (1)
generating inputs solely from compressed output labels causes drift between
synthetic and real data, and replaying such data can erode prior knowledge; (2)
inversion is computationally expensive since each step backpropagates through
the full model. These issues are amplified in large pre-trained models such as
CLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),
inspired by faster convergence in single-layer optimization. PMI provides
strong initialization for full-model inversion, substantially reducing
iterations. To mitigate feature shift, we model class-wise features via
Gaussian distributions and contrastive model, ensuring alignment between
synthetic and real features. Combining PMI and feature modeling, our approach
enables continual learning of new classes by generating pseudo-images from
semantic-aware projected features, achieving strong effectiveness and
compatibility across multiple CL settings.

</details>


### [51] [On the Impact of Weight Discretization in QUBO-Based SVM Training](https://arxiv.org/abs/2510.26323)
*Sascha Mücke*

Main category: cs.LG

TL;DR: 低精度的 QUBO 编码即可在某些数据集上达到与经典 LIBSVM 相当甚至优越的准确性；比特深度提高并不总是带来改进，关键在于选择合适的支持向量而非权重的精确性，量子退火对 SVM 的训练具有潜在优势，随硬件扩展而增强。


<details>
  <summary>Details</summary>
Motivation: 探究用于将 SVM 训练表述为 QUBO 的离散化（比特深度）对预测性能的影响，并与经典求解器 LIBSVM 进行对比，以评估在当前及未来量子硬件尺度下的可行性。

Method: 将 SVM 问题离散化为 QUBO，改变每个对偶权重的比特深度（如 1 比特、若干比特等），在多组数据集上评估预测准确性，并与 LIBSVM 的结果进行比较。同时分析更大正则化参数对性能的影响，以及支持向量的选择与权重精度之间的关系。

Result: 即便是 1 比特每个参数的低精度 QUBO 编码也能实现具有竞争力甚至有时优于 LIBSVM 的准确性；增加位深度允许使用更大的正则化参数，但并不总是带来分类性能提升；研究表明，正确选择的支持向量可能比精确权重更为重要；当前硬件限制了可解决的 QUBO 尺寸，但随着设备规模扩大，量子退火在高效训练 SVM 方面具有潜力。

Conclusion: 量子退火对 SVM 的训练具有前景，尤其在硬件规模提升后；低精度编码即可获得有竞争力的性能，未来工作应聚焦于如何有效选择支持向量以及在更大规模的 QUBO 上实现更好的性能。

Abstract: Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,
enabling the use of quantum annealing for model optimization. In this work, we
study how the number of qubits - linked to the discretization level of dual
weights - affects predictive performance across datasets. We compare QUBO-based
SVM training to the classical LIBSVM solver and find that even low-precision
QUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes
superior, accuracy. While increased bit-depth enables larger regularization
parameters, it does not always improve classification. Our findings suggest
that selecting the right support vectors may matter more than their precise
weighting. Although current hardware limits the size of solvable QUBOs, our
results highlight the potential of quantum annealing for efficient SVM training
as quantum devices scale.

</details>


### [52] [Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics](https://arxiv.org/abs/2510.26324)
*Zhiyang Xun,Shivam Gupta,Eric Price*

Main category: cs.LG

TL;DR: 在带有噪声线性观测 y=Ax+ξ 的设定下，提出通过扩散模型结合退火 Langevin 动力学，在对得分误差仅需 L^4 范数有界的条件下，能够多项式时间从后验 p(x|y) 采样。


<details>
  <summary>Details</summary>
Motivation: 实现高质量的后验采样以用于图像修复任务（如去模糊、修复、MRI 重建），并克服传统的 Langevin 动力学在仅有 p(x) 的对数凹前提下对得分估计误差的强依赖；与无条件扩散模型相比，尝试在带条件的设置中获得鲁棒性与计算性。

Method: 对 p(x) 的要求为局部或全局对数凹；将扩散模型与一个退火版本的 Langevin 动力学结合，利用对得分误差的 L^4 有界性，在条件采样下证明多项式时间可行的后验抽样。

Result: 理论结果表明，在仅假设得分误差具有 L^4 有界的前提下，可以通过所提方法实现从 p(x|y) 的条件后验中采样，且时间复杂度为多项式。

Conclusion: 将扩散模型的无条件鲁棒性与条件采样的需求结合起来，降低对精确得分估计的依赖，为实际的图像修复等应用提供可行的后验采样方案。

Abstract: Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and
a good approximation to the prior $p(x)$, when can we sample from the posterior
$p(x \mid y)$? Posterior sampling provides an accurate and fair framework for
tasks such as inpainting, deblurring, and MRI reconstruction, and several
heuristics attempt to approximate it. Unfortunately, approximate posterior
sampling is computationally intractable in general.
  To sidestep this hardness, we focus on (local or global) log-concave
distributions $p(x)$. In this regime, Langevin dynamics yields posterior
samples when the exact scores of $p(x)$ are available, but it is brittle to
score--estimation error, requiring an MGF bound (sub-exponential error). By
contrast, in the unconditional setting, diffusion models succeed with only an
$L^2$ bound on the score error. We prove that combining diffusion models with
an annealed variant of Langevin dynamics achieves conditional sampling in
polynomial time using merely an $L^4$ bound on the score error.

</details>


### [53] [Linear Causal Discovery with Interventional Constraints](https://arxiv.org/abs/2510.26342)
*Zhigao Guo,Feng Dong*

Main category: cs.LG

TL;DR: 提出 interventional constraints，将高层次的因果知识以对变量间总效应的不等式约束引入到因果发现中，通过两阶段受限优化实现线性模型的总因果效应约束，提升模型准确性、可解释性并促进新因果关系发现。


<details>
  <summary>Details</summary>
Motivation: 在仅依赖观测数据的因果发现中，结构约束可能不足以避免错误结论（如将正向关系误判为负向）；而干预数据获取成本高。引入可嵌入领域知识的干预约束可使学习的因果模型更符合领域事实，提升下游任务表现与可解释性。

Method: 提出用于线性因果模型的总因果效应度量，将其转化为不等式约束并嵌入优化问题，采用两阶段的受限优化求解，以确保学习过程符合已知因果影响。

Result: 在真实数据集（如 Sachs 数据集）上验证，加入干预约束可提升模型准确性并与已知因果结论保持一致，增强可解释性，并有助于发现新的因果关系，原先成本较高。

Conclusion: 干预约束为因果发现提供有效的知识整合途径，兼顾线性模型的可解释性、提高正确性与发现潜力，适用于将领域知识转化为量化不等式约束的情景。

Abstract: Incorporating causal knowledge and mechanisms is essential for refining
causal models and improving downstream tasks such as designing new treatments.
In this paper, we introduce a novel concept in causal discovery, termed
interventional constraints, which differs fundamentally from interventional
data. While interventional data require direct perturbations of variables,
interventional constraints encode high-level causal knowledge in the form of
inequality constraints on causal effects. For instance, in the Sachs dataset
(Sachs et al.\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3
exerts a positive causal effect on Akt. Existing causal discovery methods allow
enforcing structural constraints (for example, requiring a causal path from
PIP3 to Akt), but they may still produce incorrect causal conclusions such as
learning that "PIP3 inhibits Akt". Interventional constraints bridge this gap
by explicitly constraining the total causal effect between variable pairs,
ensuring learned models respect known causal influences. To formalize
interventional constraints, we propose a metric to quantify total causal
effects for linear causal models and formulate the problem as a constrained
optimization task, solved using a two-stage constrained optimization method. We
evaluate our approach on real-world datasets and demonstrate that integrating
interventional constraints not only improves model accuracy and ensures
consistency with established findings, making models more explainable, but also
facilitates the discovery of new causal relationships that would otherwise be
costly to identify.

</details>


### [54] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: 对稀疏奖励、随机性和非平稳环境下的强化学习进行再研究，提出一种基于蒙特卡洛的修改方法，通过层次化、多目标学习和外部输出滤波（位置记忆）等改进，显著优于传统Q学习和穷举搜索。


<details>
  <summary>Details</summary>
Motivation: 在随机、非平稳且奖励稀疏的环境中，标准强化学习难以高效学习。以在水下探测污染云的自主水下航行器（AUVs）为应用场景，强调需要在奖励稀缺、环境多变的条件下实现有效探索与鲁棒性提升。

Method: 系统性地考察大量修改策略，包含层次化算法改进、多目标学习，以及将位置记忆作为外部输出滤波器以防止状态重复访问；在此基础上采用改进的蒙特卡洛方法进行学习，与传统Q学习及两种穷举搜索模式进行对比。

Result: 改进的基于蒙特卡洛的方法在实验中显著优于传统Q学习和两种穷举搜索模式，证明了在稀疏、随机和非平稳环境中对经典RL方法的有效改造潜力。

Conclusion: 强化学习可以通过对经典方法的有针对性修改来适应随机、非平稳且奖励稀疏的环境，尤其在潜在应用如水下污染云探测任务中具有明显的适用性与前景。

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [55] [Towards Explainable and Reliable AI in Finance](https://arxiv.org/abs/2510.26353)
*Albi Isufaj,Pablo Mollá,Helmut Prendinger*

Main category: cs.LG

TL;DR: 提出一个以可解释性和可靠性为核心的金融AI框架，通过 Time-LLM 进行方向性预测约束、把时间序列基础模型与可靠性估计器结合筛选、以及符号化推理编码领域规则实现透明 justification，支持有选择的执行。


<details>
  <summary>Details</summary>
Motivation: 金融领域对预测的可解释性、可审计性和合规性需求越来越高，黑箱模型的信任门槛阻碍落地。

Method: 三步策略：1) Time-LLM 通过提示引导，避免错误方向的预测；2) 将时间序列基础模型与可靠性估计器结合，以滤除不可靠预测；3) 使用符号化推理将领域规则编码成可解释的推理路径，并与预测结合，形成可审计的决策过程。

Result: 实验在股票与加密货币数据上显示，架构能降低误报并实现有选择的执行，提升预测的可靠性与可解释性。

Conclusion: 通过融合预测性能、可靠性评估与规则化推理，构建了更可审计、符合监管要求的金融AI框架。

Abstract: Financial forecasting increasingly uses large neural network models, but
their opacity raises challenges for trust and regulatory compliance. We present
several approaches to explainable and reliable AI in finance. \emph{First}, we
describe how Time-LLM, a time series foundation model, uses a prompt to avoid a
wrong directional forecast. \emph{Second}, we show that combining foundation
models for time series forecasting with a reliability estimator can filter our
unreliable predictions. \emph{Third}, we argue for symbolic reasoning encoding
domain rules for transparent justification. These approaches shift emphasize
executing only forecasts that are both reliable and explainable. Experiments on
equity and cryptocurrency data show that the architecture reduces false
positives and supports selective execution. By integrating predictive
performance with reliability estimation and rule-based reasoning, our framework
advances transparent and auditable financial AI systems.

</details>


### [56] [CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse](https://arxiv.org/abs/2510.26369)
*Kazuma Kano,Yuki Mori,Shin Katayama,Kenta Urano,Takuro Yonezawa,Nobuo Kawaguchi*

Main category: cs.LG

TL;DR: CorVS 是一种数据驱动的人员身份识别方法，通过将视觉跟踪轨迹与传感器测量进行对应来实现仓库中的人员识别。该方法由一个深度学习模型预测轨迹与传感器对的对应概率与可靠性，并在时间维度上对这些概率进行匹配；在真实仓库数据集上验证了其在实际场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在物流仓储场景中，单纯依赖外观的识别在隐私和现实条件下往往不可行或鲁棒性不足。将视觉轨迹与传感器数据进行关联有望在不依赖外观的前提下实现稳定的人员定位与识别。现有基于轨迹与传感器对比的方法在真实世界条件下容易失效，因此需要一种更鲁棒的关联方法。

Method: 提出一个数据驱动框架：首先训练一个深度学习模型，预测每对视觉轨迹与传感器测量之间的对应概率和可靠性；随后基于这些概率与可靠性在时间上对轨迹与传感器数据进行匹配，得到最终的身份对应。为验证该思路，构建并使用包含真实仓库作业的数据集进行评估。

Result: 在包含真实仓库作业的数据集上验证了方法的有效性，展示了在真实应用场景中的实用性与鲁棒性，尤其相较于仅基于外观的识别在现实条件下的性能提升。

Conclusion: CorVS 提供了一种鲁棒的、基于数据驱动的人员身份识别方案，通过将视觉轨迹与传感器数据进行对应，缓解了现实条件下外观受限带来的挑战，并在工业场景中展示了良好的实用性。

Abstract: Worker location data is key to higher productivity in industrial sites.
Cameras are a promising tool for localization in logistics warehouses since
they also offer valuable environmental contexts such as package status.
However, identifying individuals with only visual data is often impractical.
Accordingly, several prior studies identified people in videos by comparing
their trajectories and wearable sensor measurements. While this approach has
advantages such as independence from appearance, the existing methods may break
down under real-world conditions. To overcome this challenge, we propose CorVS,
a novel data-driven person identification method based on correspondence
between visual tracking trajectories and sensor measurements. Firstly, our deep
learning model predicts correspondence probabilities and reliabilities for
every pair of a trajectory and sensor measurements. Secondly, our algorithm
matches the trajectories and sensor measurements over time using the predicted
probabilities and reliabilities. We developed a dataset with actual warehouse
operations and demonstrated the method's effectiveness for real-world
applications.

</details>


### [57] [Multi-Task Learning Based on Support Vector Machines and Twin Support Vector Machines: A Comprehensive Survey](https://arxiv.org/abs/2510.26392)
*Fatemeh Bazikar,Hossein Moosaei,Atefeh Hemmati,Panos M. Pardalos*

Main category: cs.LG

TL;DR: 综述基于SVM/TWSVM的多任务学习，比较理论性质、优化策略和实验表现，指出未来研究方向与应用场景。


<details>
  <summary>Details</summary>
Motivation: MTL在数据稀缺或高维场景中通过共享信息提升泛化、效率与鲁棒性；SVM/TWSVM因可解释性、理论严谨性和小数据集效果而仍具价值。

Method: 系统性回顾与比较：聚焦共享表示、任务正则化、结构耦合，以及对多任务场景下的TWSVM扩展进行梳理与评估；对理论性质、优化算法和实验性能进行对比，并梳理在计算机视觉、自然语言处理和生物信息学中的应用。

Result: 汇总各模型的理论属性、优化策略与经验表现，揭示它们在不同数据设置中的优劣及适用场景；并给出跨领域的应用要点。

Conclusion: 指出当前的研究空缺，提出可扩展、可解释、鲁棒且基于边际损失的多任务学习框架的未来方向，包括更高效的优化、可解释性增强和跨任务协同的结构设计。

Abstract: Multi-task learning (MTL) enables simultaneous training across related tasks,
leveraging shared information to improve generalization, efficiency, and
robustness, especially in data-scarce or high-dimensional scenarios. While deep
learning dominates recent MTL research, Support Vector Machines (SVMs) and Twin
SVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,
and effectiveness with small datasets.
  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting
shared representations, task regularization, and structural coupling
strategies. Special attention is given to emerging TWSVM extensions for
multi-task settings, which show promise but remain underexplored. We compare
these models in terms of theoretical properties, optimization strategies, and
empirical performance, and discuss applications in fields such as computer
vision, natural language processing, and bioinformatics.
  Finally, we identify research gaps and outline future directions for building
scalable, interpretable, and reliable margin-based MTL frameworks. This work
provides a comprehensive resource for researchers and practitioners interested
in SVM- and TWSVM-based multi-task learning.

</details>


### [58] [Co-Evolving Latent Action World Models](https://arxiv.org/abs/2510.26433)
*Yucen Wang,Fengming Zhang,De-Chuan Zhan,Li Zhao,Kaixin Wang,Jiang Bian*

Main category: cs.LG

TL;DR: CoLA-World jointly trains a latent action model (LAM) with a pre-trained world model via a warm-up phase, enabling co-evolution and improved video simulation and downstream planning compared to two-stage methods.


<details>
  <summary>Details</summary>
Motivation: Addresses inefficiencies and limited co-adaptation in the standard two-stage LAM+world-model pipeline. Direct joint learning risks representational collapse; requires a warm-up to align representations.

Method: Introduce CoLA-World with a critical warm-up phase that aligns the from-scratch LAM with the pre-trained world model, enabling joint training where the world model provides gradients to shape LAM while LAM offers a precise control interface. This creates a co-evolution cycle: the world model acts as a tutor and LAM as an adaptable controller.

Result: Empirically, CoLA-World matches or outperforms previous two-stage methods in video simulation quality and downstream visual planning.

Conclusion: CoLA-World establishes a robust, efficient new paradigm for controllable world modeling by enabling effective joint learning and co-evolution of LAM and world model.

Abstract: Adapting pre-trained video generation models into controllable world models
via latent actions is a promising step towards creating generalist world
models. The dominant paradigm adopts a two-stage approach that trains latent
action model (LAM) and the world model separately, resulting in redundant
training and limiting their potential for co-adaptation. A conceptually simple
and appealing idea is to directly replace the forward dynamic model in LAM with
a powerful world model and training them jointly, but it is non-trivial and
prone to representational collapse. In this work, we propose CoLA-World, which
for the first time successfully realizes this synergistic paradigm, resolving
the core challenge in joint learning through a critical warm-up phase that
effectively aligns the representations of the from-scratch LAM with the
pre-trained world model. This unlocks a co-evolution cycle: the world model
acts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,
while the LAM offers a more precise and adaptable control interface to the
world model. Empirically, CoLA-World matches or outperforms prior two-stage
methods in both video simulation quality and downstream visual planning,
establishing a robust and efficient new paradigm for the field.

</details>


### [59] [Robust Graph Condensation via Classification Complexity Mitigation](https://arxiv.org/abs/2510.26451)
*Jiayi Luo,Qingyun Sun,Beining Yang,Haonan Yuan,Xingcheng Fu,Yanbiao Ma,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了 MRGC，通过在图数据流形约束下进行图凝练，以提高鲁棒性并在对抗扰动下保持分类复杂度的降低。


<details>
  <summary>Details</summary>
Motivation: GC 在降维的同时易受扰动影响，现有鲁棒学习方法效果有限，需要在保持低维复杂度的前提下提升鲁棒性。

Method: 提出三种图数据流形学习模块，引导凝练图嵌入平滑、低维流形，最小化类别歧义，并在普遍对抗攻击条件下实现鲁棒性。

Result: 通过大量实验，证明 MRGC 在多种攻击场景下的鲁棒性。

Conclusion: 在保持 GC 降低分类复杂度能力的同时，通过流形约束提升鲁棒性，MRGC 有效。

Abstract: Graph condensation (GC) has gained significant attention for its ability to
synthesize smaller yet informative graphs. However, existing studies often
overlook the robustness of GC in scenarios where the original graph is
corrupted. In such cases, we observe that the performance of GC deteriorates
significantly, while existing robust graph learning technologies offer only
limited effectiveness. Through both empirical investigation and theoretical
analysis, we reveal that GC is inherently an intrinsic-dimension-reducing
process, synthesizing a condensed graph with lower classification complexity.
Although this property is critical for effective GC performance, it remains
highly vulnerable to adversarial perturbations. To tackle this vulnerability
and improve GC robustness, we adopt the geometry perspective of graph data
manifold and propose a novel Manifold-constrained Robust Graph Condensation
framework named MRGC. Specifically, we introduce three graph data manifold
learning modules that guide the condensed graph to lie within a smooth,
low-dimensional manifold with minimal class ambiguity, thereby preserving the
classification complexity reduction capability of GC and ensuring robust
performance under universal adversarial attacks. Extensive experiments
demonstrate the robustness of \ModelName\ across diverse attack scenarios.

</details>


### [60] [ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems](https://arxiv.org/abs/2510.26475)
*Qiaoling Chen,Zijun Liu,Peng Sun,Shenggui Li,Guoteng Wang,Ziming Liu,Yonggang Wen,Siyuan Feng,Tianwei Zhang*

Main category: cs.LG

TL;DR: ReSpec adapts speculative decoding (SD) to reinforcement learning (RL) training for large language models, addressing bottlenecks in generation to achieve substantial speedups without sacrificing convergence or stability.


<details>
  <summary>Details</summary>
Motivation: RL-based fine-tuning of LLMs is often limited by autoregressive generation time. While speculative decoding can speed up generation in serving, its behavior during RL training is underexplored. The paper identifies three gaps—diminishing speedups at large batch sizes, drafter staleness from continual actor updates, and policy degradation induced by the drafter—that hinder naive integration of SD into RL.

Method: Introduce ReSpec, combining three mechanisms: (1) dynamic tuning of SD configurations during RL training, (2) evolving the drafter via knowledge distillation to keep it aligned with evolving policies, and (3) weighting model updates by rollout rewards to stabilize training.

Result: Experiments on Qwen models (3B–14B) show up to 4.5x speedup in training generation, while preserving reward convergence and training stability.

Conclusion: ReSpec provides a practical and effective solution for accelerating RL-based LLM adaptation by integrating optimally tuned SD, distillation-driven drafter evolution, and reward-weighted updates, enabling more efficient RL fine-tuning of large language models.

Abstract: Adapting large language models (LLMs) via reinforcement learning (RL) is
often bottlenecked by the generation stage, which can consume over 75\% of the
training time. Speculative decoding (SD) accelerates autoregressive generation
in serving systems, but its behavior under RL training remains largely
unexplored. We identify three critical gaps that hinder the naive integration
of SD into RL systems: diminishing speedups at large batch sizes, drafter
staleness under continual actor updates, and drafter-induced policy
degradation.
  To address these gaps, we present ReSpec, a system that adapts SD to RL
through three complementary mechanisms: dynamically tuning SD configurations,
evolving the drafter via knowledge distillation, and weighting updates by
rollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup
while preserving reward convergence and training stability, providing a
practical solution for efficient RL-based LLM adaptation.

</details>


### [61] [Data-Efficient RLVR via Off-Policy Influence Guidance](https://arxiv.org/abs/2510.26491)
*Erle Zhu,Dazhi Jiang,Yuan Wang,Xujun Li,Jiale Cheng,Yuxian Gu,Yilin Niu,Aohan Zeng,Jie Tang,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: 提出了一种基于影响函数的离线数据影响力估计方法，并结合稀疏随机投影进行高维降维，构建CROPI（Curriculum RL with Off-Policy Influence guidance）多阶段RL框架，用以在RLVR中高效地进行数据选择。通过离线轨迹近似影响，降低在线滚动成本，在1.5B参数模型上达到2.66x步级加速，同时每阶段仅用 10% 的数据，相较全量数据训练显著提高效率；在7B规模模型上也有显著加速，充分展示影响-based数据选择在RLVR中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR领域的数据选择多依赖启发式且缺乏理论保证与泛化性；在线策略滚动成本高，难以对大规模LLMs实现高效数据选择。需要一个理论上可解释且可扩展的离线估计方法来衡量数据点对学习目标的贡献。

Method: 提出基于影响函数的离线影响估计来评估单个数据点对学习目标的贡献；通过离线收集的轨迹进行近似估计，避免昂贵的在线策略滚动。为应对LLMs的高维梯度，采用稀疏随机投影进行降维存储与计算。构建CROPI（Curriculum RL with Off-Policy Influence guidance），在多阶段训练中迭代选择对当前策略最具影响力的数据样本进行训练。

Result: 在多阶段实验中，CROPI显示显著的训练加速效果；在1.5B参数模型上实现2.66×步级加速，且每阶段仅使用全数据集的10%即可达到该提升；在7B参数模型上亦显示出训练效率提升，验证了基于影响的数据选择在RLVR中的可行性与潜力。

Conclusion: 基于影响函数的离线数据影响评估结合高效降维与分阶段数据筛选，能够显著提升RLVR中的数据利用效率，CROPI为高效数据驱动的多阶段RL提供了一个可行框架。

Abstract: Data selection is a critical aspect of Reinforcement Learning with Verifiable
Rewards (RLVR) for enhancing the reasoning capabilities of large language
models (LLMs). Current data selection methods are largely heuristic-based,
lacking theoretical guarantees and generalizability. This work proposes a
theoretically-grounded approach using influence functions to estimate the
contribution of each data point to the learning objective. To overcome the
prohibitive computational cost of policy rollouts required for online influence
estimation, we introduce an off-policy influence estimation method that
efficiently approximates data influence using pre-collected offline
trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we
employ sparse random projection to reduce dimensionality and improve storage
and computation efficiency. Leveraging these techniques, we develop
\textbf{C}urriculum \textbf{R}L with \textbf{O}ff-\textbf{P}olicy
\text{I}nfluence guidance (\textbf{CROPI}), a multi-stage RL framework that
iteratively selects the most influential data for the current policy.
Experiments on models up to 7B parameters demonstrate that CROPI significantly
accelerates training. On a 1.5B model, it achieves a 2.66x step-level
acceleration while using only 10\% of the data per stage compared to
full-dataset training. Our results highlight the substantial potential of
influence-based data selection for efficient RLVR.

</details>


### [62] [Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters](https://arxiv.org/abs/2510.26501)
*Mustafa Fuad Rifet Ibrahim,Maurice Meijer,Alexander Schlaefer,Peer Stelldinger*

Main category: cs.LG

TL;DR: 在资源受限的可穿戴ECG监测场景中，本文评估无监督异常检测（UAD）作为上游过滤器以提升对OOD数据的鲁棒性。对六种UAD方法进行基准，包括Deep SVDD、重建模型、掩码异常检测、正则化流、扩散模型，并在512k参数约束下通过神经架构搜索（NAS）进行优化。数据集为PTB-XL和BUT QDB。结果显示Deep SVDD在检测性能与计算开销之间折中最佳；在实际部署仿真中，将优化后的Deep SVDD与诊断分类器组合能使准确率比单分类器基线提高多达21个百分点。结论是，优化后的UAD过滤器可提升可穿戴ECG分析的安全性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴ECG监测在资源受限的设备上应用深度学习进行自动分析时，容易受到未知病理、噪声等OOD数据的影响，导致高置信度错误预测，威胁患者安全。因此需要在不牺牲设备资源的前提下，构建一个上游的无监督异常检测过滤器来提升系统鲁棒性。

Method: 在PTB-XL与BUT QDB数据集上，基于512k参数的约束，比较六种UAD方法：Deep SVDD、基于重建的模型、Masked Anomaly Detection、正则化流、扩散模型，并通过神经架构搜索（NAS）对它们进行资源约束下的优化。评估目标包括对OOD的CVD类别和无法分析的噪声信号的检测能力，以及在部署场景中的系统性能。

Result: Deep SVDD在检测能力与资源消耗之间始终实现最佳折中；在将优化后的Deep SVDD作为上游过滤器接入诊断分类器的部署仿真中，准确率相对于仅使用分类器的基线提高最高可达21个百分点。

Conclusion: 经过资源约束下的NAS优化的UAD过滤器能够提升ECG自动分析的鲁棒性和安全性，使可穿戴设备上的连续心血管监测更可靠。

Abstract: Continuous electrocardiogram (ECG) monitoring via wearables offers
significant potential for early cardiovascular disease (CVD) detection.
However, deploying deep learning models for automated analysis in
resource-constrained environments faces reliability challenges due to
inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen
pathologies or noisecorrupted signals, often cause erroneous, high-confidence
predictions by standard classifiers, compromising patient safety. Existing OOD
detection methods either neglect computational constraints or address noise and
unseen classes separately. This paper explores Unsupervised Anomaly Detection
(UAD) as an independent, upstream filtering mechanism to improve robustness. We
benchmark six UAD approaches, including Deep SVDD, reconstruction-based models,
Masked Anomaly Detection, normalizing flows, and diffusion models, optimized
via Neural Architecture Search (NAS) under strict resource constraints (at most
512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection
of OOD CVD classes and signals unsuitable for analysis due to noise. Results
show Deep SVDD consistently achieves the best trade-off between detection and
efficiency. In a realistic deployment simulation, integrating the optimized
Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21
percentage points over a classifier-only baseline. This study demonstrates that
optimized UAD filters can safeguard automated ECG analysis, enabling safer,
more reliable continuous cardiovascular monitoring on wearables.

</details>


### [63] [LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection](https://arxiv.org/abs/2510.26510)
*Youssef Attia El Hili,Albert Thomas,Malik Tiomoko,Abdelhakim Benechehab,Corentin Léger,Corinne Ancourt,Balázs Kégl*

Main category: cs.LG

TL;DR: LLMs can act as in-context meta-learners for model and hyperparameter selection by converting datasets into metadata and prompting for recommendations; both zero-shot and meta-informed prompting; achieves competitive performance without search; demonstrates in-context meta-learning.


<details>
  <summary>Details</summary>
Motivation: Model and hyperparameter selection is critical yet expensive; traditional approaches rely on expert intuition or exhaustive search. The work investigates whether LLMs can leverage metadata to guide selection, reducing search cost.

Method: Transform each dataset into interpretable metadata. Prompt an LLM to recommend model families and hyperparameters. Compare zero-shot prompting (relying on pretrained knowledge) with meta-informed prompting that includes examples of models and their performance on past tasks. Evaluate on synthetic and real-world benchmarks.

Result: LLMs can use dataset metadata to propose competitive models and hyperparameters without explicit search. Meta-informed prompting yields further improvements, indicating in-context meta-learning capabilities.

Conclusion: LLMs can serve as lightweight, general-purpose assistants for model selection and hyperparameter optimization, offering a promising approach to reduce search costs and enable practical AutoML-like capabilities.

Abstract: Model and hyperparameter selection are critical but challenging in machine
learning, typically requiring expert intuition or expensive automated search.
We investigate whether large language models (LLMs) can act as in-context
meta-learners for this task. By converting each dataset into interpretable
metadata, we prompt an LLM to recommend both model families and
hyperparameters. We study two prompting strategies: (1) a zero-shot mode
relying solely on pretrained knowledge, and (2) a meta-informed mode augmented
with examples of models and their performance on past tasks. Across synthetic
and real-world benchmarks, we show that LLMs can exploit dataset metadata to
recommend competitive models and hyperparameters without search, and that
improvements from meta-informed prompting demonstrate their capacity for
in-context meta-learning. These results highlight a promising new role for LLMs
as lightweight, general-purpose assistants for model selection and
hyperparameter optimization.

</details>


### [64] [Think Outside the Policy: In-Context Steered Policy Optimization](https://arxiv.org/abs/2510.26519)
*Hsiu-Yuan Huang,Chenming Tang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ICPO通过在上下文中学习提供专家引导，提出混合策略GRPO与隐式专家强制、专家区域拒绝采样和退火型专家奖励 shaping等组件，以扩大探索、提升稳定性，在LRMs的RLVR任务（尤其数学推理基准）中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在探索方面受限，因仅依赖当前策略分布的on-policy回放；使用强大专家模型来扩展策略尽管有效，却成本高且往往不可得。需要一个无需额外昂贵专家轨迹即可提升探索与稳定性的统一框架。

Method: 提出ICPO框架，利用LRMs的在-context学习能力，通过现有数据集提供专家指导；引入混合策略GRPO与隐式专家强制；引入专家区域拒绝采样以过滤不可靠的离策略轨迹；引入退火型专家奖励 shaping以平衡初期专家引导和后期自主改进。

Result: 实验结果显示ICPO在数学推理基准上显著提升RLVR的性能与训练稳定性，表明其是一个可扩展且有效的LRMs RLVR范式。

Conclusion: ICPO为LRMs的RLVR提供一个可扩展、稳定且无需高成本外部专家轨迹的统一框架，有望推动更广泛的推理任务应用。

Abstract: Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such
as Group Relative Policy Optimization (GRPO), have achieved remarkable progress
in improving the reasoning capabilities of Large Reasoning Models (LRMs).
However, they exhibit limited exploration due to reliance on on-policy rollouts
where confined to the current policy's distribution, resulting in narrow
trajectory diversity. Recent approaches attempt to expand policy coverage by
incorporating trajectories generated from stronger expert models, yet this
reliance increases computational cost and such advaned models are often
inaccessible. To address these issues, we propose In-Context Steered Policy
Optimization (ICPO), a unified framework that leverages the inherent in-context
learning capability of LRMs to provide expert guidance using existing datasets.
ICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands
exploration beyond the current policy distribution without requiring advanced
LRM trajectories. To further stabilize optimization, ICPO integrates Expert
Region Reject Sampling to filter unreliable off-policy trajectories and
Annealed Expert-Bonus Reward Shaping to balance early expert guidance with
later autonomous improvement. Results demonstrate that ICPO consistently
enhances reinforcement learning performance and training stability on
mathematical reasoning benchmarks, revealing a scalable and effective RLVR
paradigm for LRMs.

</details>


### [65] [Polybasic Speculative Decoding Through a Theoretical Perspective](https://arxiv.org/abs/2510.26527)
*Ruilin Wang,Huixia Li,Yuexiao Ma,Xiawu Zheng,Fei Chao,Xuefeng Xiao,Rongrong Ji*

Main category: cs.LG

TL;DR: 提出一个多模型的“多基（polybasic） speculative decoding”框架，给出理论最优推理时间的界定，并在多模型生成中优化能力、接受长度与计算成本的权衡，实验证明对多种模型能显著加速推理且保持原始输出分布。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于 drafts 与 verify 的 dualistic 方式在理论支撑不足的问题；推理延迟成为大规模部署的瓶颈，需要一个更一般化、具理论支撑的多基（polybasic） speculative 解法来提高推理效率。

Method: 提出并证明一个基本定理，刻画多模型 speculative 解码系统的最优推理时间；分析多模型生成过程中的模型能力、接受长度与计算成本之间的耦合与权衡；给出能够独立实现或与现有 speculative 技术协同的 polybasic 框架，并结合理论分析和实践实验进行验证。

Result: 在多模型实验中实现显著加速：对 LLaMA2-Chat 7B 的速度提升范围为 3.31×‑4.01×，对 LLaMA3-8B 为 3.87×，对 Vicuna-7B 为 4.43×，对 Qwen2-7B 为 3.85×；且保持原始输出分布，不引入额外偏差。

Conclusion: 理论证明与实现相结合，证明了 polybasic speculative decoding 的普适性与有效性，提供可扩展的框架并公开相关证明与实现代码，便于后续在不同模型与场景中的推广与研究。

Abstract: Inference latency stands as a critical bottleneck in the large-scale
deployment of Large Language Models (LLMs). Speculative decoding methods have
recently shown promise in accelerating inference without compromising the
output distribution. However, existing work typically relies on a dualistic
draft-verify framework and lacks rigorous theoretical grounding. In this paper,
we introduce a novel \emph{polybasic} speculative decoding framework,
underpinned by a comprehensive theoretical analysis. Specifically, we prove a
fundamental theorem that characterizes the optimal inference time for
multi-model speculative decoding systems, shedding light on how to extend
beyond the dualistic approach to a more general polybasic paradigm. Through our
theoretical investigation of multi-model token generation, we expose and
optimize the interplay between model capabilities, acceptance lengths, and
overall computational cost. Our framework supports both standalone
implementation and integration with existing speculative techniques, leading to
accelerated performance in practice. Experimental results across multiple model
families demonstrate that our approach yields speedup ratios ranging from
$3.31\times$ to $4.01\times$ for LLaMA2-Chat 7B, up to $3.87 \times$ for
LLaMA3-8B, up to $4.43 \times$ for Vicuna-7B and up to $3.85 \times$ for
Qwen2-7B -- all while preserving the original output distribution. We release
our theoretical proofs and implementation code to facilitate further
investigation into polybasic speculative decoding.

</details>


### [66] [Higher-Order Regularization Learning on Hypergraphs](https://arxiv.org/abs/2510.26533)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 扩展 HOHL 的理论基础，证明截断版本的一致性并给出收敛速率，同时在有监督学习中作为正则化项应用，且在主动学习及缺乏几何结构的数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升超图正则化的高阶平滑性建模，解决 HOHL 的一致性与稳定性问题，并扩展到非几何结构数据的鲁棒性与广泛应用场景。

Method: 给出截断版本的理论一致性证明并推导在全监督学习中作为正则化项的显式收敛速率；利用高阶幂次的多尺度拉普拉斯算子（由超图结构诱导）实现高阶平滑性；并在实际任务中验证其在主动学习与缺乏几何结构数据上的性能。

Result: 理论结果包括截断 HOHL 的一致性证明及显式收敛速率；实验结果显示在主动学习和缺乏几何结构的数据集上具有强劲的经验性能，表明 HOHL 在多样化学习设置中的可用性与鲁棒性。

Conclusion: HOHL 作为一种更广泛的高阶正则化工具，展现出优越的理论保障与广泛的应用潜力，适用于多种学习场景以实现更稳健、更灵活的学习性能。

Abstract: Higher-Order Hypergraph Learning (HOHL) was recently introduced as a
principled alternative to classical hypergraph regularization, enforcing
higher-order smoothness via powers of multiscale Laplacians induced by the
hypergraph structure. Prior work established the well- and ill-posedness of
HOHL through an asymptotic consistency analysis in geometric settings. We
extend this theoretical foundation by proving the consistency of a truncated
version of HOHL and deriving explicit convergence rates when HOHL is used as a
regularizer in fully supervised learning. We further demonstrate its strong
empirical performance in active learning and in datasets lacking an underlying
geometric structure, highlighting HOHL's versatility and robustness across
diverse learning settings.

</details>


### [67] [Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices](https://arxiv.org/abs/2510.26557)
*Jan Stenkamp,Nina Herrmann,Benjamin Karic,Stefan Oehmcke,Fabian Gieseke*

Main category: cs.LG

TL;DR: 提出一种面向受限设备的提升树压缩方案，通过在训练阶段奖励特征与阈值的复用，实现4-16x的内存压缩，与 LightGBM 相比保持相似性能，支持边缘物联网设备离线自持运行。


<details>
  <summary>Details</summary>
Motivation: 物联网设备普遍存在计算、内存和能耗的限制，迫切需要在本地离线运行的轻量化机器学习模型。提升树集成在此场景下往往占用较多内存，限制了部署。

Method: 在训练阶段引入奖励机制，鼓励特征和阈值的重复使用，设计替代的内存布局以降低冗余，采用改进的训练流程产出紧凑的提升树集成模型。

Result: 实验表明在保持相同性能的前提下，可实现相对于 LightGBM 的4-16x内存压缩；并且模型一旦部署，物联网设备可在无需持续通信或外部能源的条件下自主运行。

Conclusion: 该方法为边缘分析、实时决策等IoT应用在远程或低能环境中提供了可行的轻量化ML方案，扩展了资源受限设备上的智能应用场景。

Abstract: Deploying machine learning models on compute-constrained devices has become a
key building block of modern IoT applications. In this work, we present a
compression scheme for boosted decision trees, addressing the growing need for
lightweight machine learning models. Specifically, we provide techniques for
training compact boosted decision tree ensembles that exhibit a reduced memory
footprint by rewarding, among other things, the reuse of features and
thresholds during training. Our experimental evaluation shows that models
achieved the same performance with a compression ratio of 4-16x compared to
LightGBM models using an adapted training process and an alternative memory
layout. Once deployed, the corresponding IoT devices can operate independently
of constant communication or external energy supply, and, thus, autonomously,
requiring only minimal computing power and energy. This capability opens the
door to a wide range of IoT applications, including remote monitoring, edge
analytics, and real-time decision making in isolated or power-limited
environments.

</details>


### [68] [Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis](https://arxiv.org/abs/2510.26607)
*Maksim Maslov,Alexander Kugaevskikh,Matthew Ivanov*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper considers the problem of regression over distributions, which is
becoming increasingly important in machine learning. Existing approaches often
ignore the geometry of the probability space or are computationally expensive.
To overcome these limitations, a new method is proposed that combines the
parameterization of probability trajectories using a Bernstein basis and the
minimization of the Wasserstein distance between distributions. The key idea is
to model a conditional distribution as a smooth probability trajectory defined
by a weighted sum of Gaussian components whose parameters -- the mean and
covariance -- are functions of the input variable constructed using Bernstein
polynomials. The loss function is the averaged squared Wasserstein distance
between the predicted Gaussian distributions and the empirical data, which
takes into account the geometry of the distributions. An autodiff-based
optimization method is used to train the model. Experiments on synthetic
datasets that include complex trajectories demonstrated that the proposed
method provides competitive approximation quality in terms of the Wasserstein
distance, Energy Distance, and RMSE metrics, especially in cases of pronounced
nonlinearity. The model demonstrates trajectory smoothness that is better than
or comparable to alternatives and robustness to changes in data structure,
while maintaining high interpretability due to explicit parameterization via
control points. The developed approach represents a balanced solution that
combines geometric accuracy, computational practicality, and interpretability.
Prospects for further research include extending the method to non-Gaussian
distributions, applying entropy regularization to speed up computations, and
adapting the approach to working with high-dimensional data for approximating
surfaces and more complex structures.

</details>


### [69] [Aeolus: A Multi-structural Flight Delay Dataset](https://arxiv.org/abs/2510.26616)
*Lin Xu,Xinyun Yuan,Yuxuan Liang,Suwan Yin,Yuankai Wu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed
to advance research on flight delay prediction and support the development of
foundation models for tabular data. Existing datasets in this domain are
typically limited to flat tabular structures and fail to capture the
spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this
limitation by providing three aligned modalities: (i) a tabular dataset with
rich operational, meteorological, and airportlevel features for over 50 million
flights; (ii) a flight chain module that models delay propagation along
sequential flight legs, capturing upstream and downstream dependencies; and
(iii) a flight network graph that encodes shared aircraft, crew, and airport
resource connections, enabling cross-flight relational reasoning. The dataset
is carefully constructed with temporal splits, comprehensive features, and
strict leakage prevention to support realistic and reproducible machine
learning evaluation. Aeolus supports a broad range of tasks, including
regression, classification, temporal structure modeling, and graph learning,
serving as a unified benchmark across tabular, sequential, and graph
modalities. We release baseline experiments and preprocessing tools to
facilitate adoption. Aeolus fills a key gap for both domain-specific modeling
and general-purpose structured data research.Our source code and data can be
accessed at https://github.com/Flnny/Delay-data

</details>


### [70] [Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization](https://arxiv.org/abs/2510.26633)
*Colin Doumont,Victor Picheny,Viacheslav Borovitskiy,Henry Moss*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Bayesian Optimization (BO) has the potential to solve various combinatorial
tasks, ranging from materials science to neural architecture search. However,
BO requires specialized kernels to effectively model combinatorial domains.
Recent efforts have introduced several combinatorial kernels, but the
relationships among them are not well understood. To bridge this gap, we
develop a unifying framework based on heat kernels, which we derive in a
systematic way and express as simple closed-form expressions. Using this
framework, we prove that many successful combinatorial kernels are either
related or equivalent to heat kernels, and validate this theoretical claim in
our experiments. Moreover, our analysis confirms and extends the results
presented in Bounce: certain algorithms' performance decreases substantially
when the unknown optima of the function do not have a certain structure. In
contrast, heat kernels are not sensitive to the location of the optima. Lastly,
we show that a fast and simple pipeline, relying on heat kernels, is able to
achieve state-of-the-art results, matching or even outperforming certain slow
or complex algorithms.

</details>


### [71] [MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection](https://arxiv.org/abs/2510.26643)
*Emmanouil Sylligardos,John Paparrizos,Themis Palpanas,Pierre Senellart,Paul Boniol*

Main category: cs.LG

TL;DR: Time series anomaly detection can be improved by model selection using time series classification; a large-scale evaluation shows that selecting models via TS classification outperforms any single anomaly detector with similar run-time, suggesting a strong baseline for AutoML pipelines.


<details>
  <summary>Details</summary>
Motivation: There is no universally best anomaly detector for heterogeneous time series; scalable solutions require selecting the most suitable detector based on time series characteristics.

Method: Evaluate 234 configurations drawn from 16 base classifiers for anomaly detection across ~1980 time series; treat time series classification methods as a model selection layer for anomaly detection; compare against baseline anomaly detectors in terms accuracy and runtime.

Result: Model selection methods based on TS classification outperform every single anomaly detector while maintaining similar execution times, establishing a strong baseline for model selection in TS AutoML pipelines.

Conclusion: Time series classification-based model selection is effective for anomaly detection in heterogeneous time series; this approach provides a practical and scalable component for AutoML pipelines and serves as a benchmark for future TS anomaly-detection model selection studies.

Abstract: Anomaly detection is a fundamental task for time series analytics with
important implications for the downstream performance of many applications.
Despite increasing academic interest and the large number of methods proposed
in the literature, recent benchmarks and evaluation studies demonstrated that
no overall best anomaly detection methods exist when applied to very
heterogeneous time series datasets. Therefore, the only scalable and viable
solution to solve anomaly detection over very different time series collected
from diverse domains is to propose a model selection method that will select,
based on time series characteristics, the best anomaly detection methods to
run. Existing AutoML solutions are, unfortunately, not directly applicable to
time series anomaly detection, and no evaluation of time series-based
approaches for model selection exists. Towards that direction, this paper
studies the performance of time series classification methods used as model
selection for anomaly detection. In total, we evaluate 234 model configurations
derived from 16 base classifiers across more than 1980 time series, and we
propose the first extensive experimental evaluation of time series
classification as model selection for anomaly detection. Our results
demonstrate that model selection methods outperform every single anomaly
detection method while being in the same order of magnitude regarding execution
time. This evaluation is the first step to demonstrate the accuracy and
efficiency of time series classification algorithms for anomaly detection, and
represents a strong baseline that can then be used to guide the model selection
step in general AutoML pipelines. Preprint version of an article accepted at
the VLDB Journal.

</details>


### [72] [Curly Flow Matching for Learning Non-gradient Field Dynamics](https://arxiv.org/abs/2510.26645)
*Katarina Petrović,Lazar Atanackovic,Viggo Moro,Kacper Kapuśniak,İsmail İlkan Ceylan,Michael Bronstein,Avishek Joey Bose,Alexander Tong*

Main category: cs.LG

TL;DR: Curly-FM 通过在非零漂移参考过程下求解 Schrödinger 桥问题，学习非梯度、周期性的动力学，超越传统的梯度场匹配方法。


<details>
  <summary>Details</summary>
Motivation: 现实系统常呈现非梯度、周期性行为，现有的流动/桥接匹配方法难以捕捉；需要引入包含推断速度的参考信息来建模。

Method: 提出 Curly Flow Matching，采用带非零漂移参考过程的 Schrödinger 桥问题来学习动力学；结合推断的速度和总体分布（样本边际）来构造参考过程。

Result: 在单细胞轨迹推断、计算流体力学与海洋环流等任务中展示，Curly-FM 能更好地拟合参考过程和边际分布。

Conclusion: 将流动匹配扩展到具有周期性行为的物理系统，提升对非梯度动力学的建模能力；代码开源，可在 GitHub 获取。

Abstract: Modeling the transport dynamics of natural processes from population-level
observations is a ubiquitous problem in the natural sciences. Such models rely
on key assumptions about the underlying process in order to enable faithful
learning of governing dynamics that mimic the actual system behavior. The de
facto assumption in current approaches relies on the principle of least action
that results in gradient field dynamics and leads to trajectories minimizing an
energy functional between two probability measures. However, many real-world
systems, such as cell cycles in single-cell RNA, are known to exhibit
non-gradient, periodic behavior, which fundamentally cannot be captured by
current state-of-the-art methods such as flow and bridge matching. In this
paper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is
capable of learning non-gradient field dynamics by designing and solving a
Schr\"odinger bridge problem with a non-zero drift reference process -- in
stark contrast to typical zero-drift reference processes -- which is
constructed using inferred velocities in addition to population snapshot data.
We showcase Curly-FM by solving the trajectory inference problems for single
cells, computational fluid dynamics, and ocean currents with approximate
velocities. We demonstrate that Curly-FM can learn trajectories that better
match both the reference process and population marginals. Curly-FM expands
flow matching models beyond the modeling of populations and towards the
modeling of known periodic behavior in physical systems. Our code repository is
accessible at: https://github.com/kpetrovicc/curly-flow-matching.git

</details>


### [73] [Tight Differentially Private PCA via Matrix Coherence](https://arxiv.org/abs/2510.26679)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: 提出一个简单高效的差分隐私 rank-r 近似算法，基于 SVD 与常规扰动机制，误差仅依赖于秩-r 相干性和谱裂隙，能够在多种设置接近非私有最优，且扩展到图的私有化问题。


<details>
  <summary>Details</summary>
Motivation: 回答 Hardt 与 Roth 提出的问题，研究秩-r 相干性在差分隐私下对误差的决定作用，并检验高斯扰动对相干性的影响，探索相干性在图问题中的应用。

Method: 使用基于特征值分解的简单算法，结合高斯/拉普拉斯等标准扰动机制，得到一个私有的秩-r 近似；对误差进行理论界定，显示其收敛与谱裂隙相关，并证明在高斯机制下相干性不增。

Result: 在密集场景下，单尖峰 PCA 的 Wishart 模型下，达到与最佳非私有算法相同的保证；相比现有私有算法有显著改善；并给出私有的 Max-Cut/ CSP 算法在低相干假设下的实现。

Conclusion: 秩-r 相干性及谱裂隙成为差分隐私下矩阵近似的核心指标；高斯扰动保持输入相干性，可能推广到其他结构模型如图中的 planted 问题；未来工作包括扩展到更多结构化模型并优化常数因子。

Abstract: We revisit the task of computing the span of the top $r$ singular vectors
$u_1, \ldots, u_r$ of a matrix under differential privacy. We show that a
simple and efficient algorithm -- based on singular value decomposition and
standard perturbation mechanisms -- returns a private rank-$r$ approximation
whose error depends only on the \emph{rank-$r$ coherence} of $u_1, \ldots, u_r$
and the spectral gap $\sigma_r - \sigma_{r+1}$. This resolves a question posed
by Hardt and Roth~\cite{hardt2013beyond}. Our estimator outperforms the state
of the art -- significantly so in some regimes. In particular, we show that in
the dense setting, it achieves the same guarantees for single-spike PCA in the
Wishart model as those attained by optimal non-private algorithms, whereas
prior private algorithms failed to do so.
  In addition, we prove that (rank-$r$) coherence does not increase under
Gaussian perturbations. This implies that any estimator based on the Gaussian
mechanism -- including ours -- preserves the coherence of the input. We
conjecture that similar behavior holds for other structured models, including
planted problems in graphs.
  We also explore applications of coherence to graph problems. In particular,
we present a differentially private algorithm for Max-Cut and other constraint
satisfaction problems under low coherence assumptions.

</details>


### [74] [LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits](https://arxiv.org/abs/2510.26690)
*Amir Reza Mirzaei,Yuqiao Wen,Yanshuai Cao,Lili Mou*

Main category: cs.LG

TL;DR: 提出了一种针对 LoRA 的混合精度后量化方法 LoRAQuant，通过对 LoRA 进行 SVD 重参数化，将关键信息集中到特定行列，从而在重要部分使用高精度、其他部分极低比特宽度，实现多模型多任务的参数高效微调，实验表明在较低比特位下性能与其他量化方法相比仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，往往需要同时加载多个 LoRA 适配器以实现个性化和任务多样性；尽管每个适配器单独成本较低，但聚合成本在规模化时相当显著。需要一种在保持预测性能的同时显著降低存储与通信开销的量化方法，尤其针对包含多适配器的场景。

Method: 提出 LoRAQuant，对每个 LoRA 适配器进行奇异值分解（SVD）重参数化，将信息集中到特定的行与列；对重要分量使用较高的精度量化，而对其余分量使用超低比特宽度的量化，采用混合精度的后量化策略。

Result: 在 LLaMA 2-7B、LLaMA 2-13B 与 Mistral 7B 三个模型上，覆盖数学推理、编码和摘要等任务的实验表明，LoRAQuant 使用的比特数显著低于其他量化方法，但在性能上可达到甚至超过对比方法。

Conclusion: LoRAQuant 能在多适配器、低比特量化的情况下，保持或提升任务性能，同时显著降低存储与计算开销，适用于大模型的参数高效微调场景。

Abstract: Low-Rank Adaptation (LoRA) has become a popular technique for
parameter-efficient fine-tuning of large language models (LLMs). In many
real-world scenarios, multiple adapters are loaded simultaneously to enable LLM
customization for personalized user experiences or to support a diverse range
of tasks. Although each adapter is lightweight in isolation, their aggregate
cost becomes substantial at scale. To address this, we propose LoRAQuant, a
mixed-precision post-training quantization method tailored to LoRA.
Specifically, LoRAQuant reparameterizes each adapter by singular value
decomposition (SVD) to concentrate the most important information into specific
rows and columns. This makes it possible to quantize the important components
to higher precision, while quantizing the rest to ultra-low bitwidth. We
conduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B
models on mathematical reasoning, coding, and summarization tasks. Results show
that our LoRAQuant uses significantly lower bits than other quantization
methods, but achieves comparable or even higher performance.

</details>


### [75] [Budgeted Multiple-Expert Deferral](https://arxiv.org/abs/2510.26706)
*Giulia DeSalvo,Clara Mohri,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 提出预算化的 deferral 框架，在训练阶段通过有选择地查询少量专家来降低成本，同时保持预测性能，给出两阶段和单阶段多专家设置的算法及理论保证，并在多领域实验中验证成本显著下降而准确度不降低。


<details>
  <summary>Details</summary>
Motivation: 在需要昂贵专家参与的预测任务中，现有的 deferral 学习通常对每个样本都查询所有专家，成本高昂。研究旨在通过预算约束在训练中有选择地查询专家，以实现更高的成本效率与预测性能。

Method: 提出两阶段和单阶段多专家 deferral 的新算法，策略性地对训练样本仅查询子集专家；给出泛化界和标签复杂度分析；在多领域数据上进行实验以验证成本与准确性的权衡。

Result: 实验结果显示，在训练成本显著下降的同时，预测准确性保持接近或未降低，相比全查询设置具有明显的成本效益。

Conclusion: 预算感知的 deferral 算法在需要昂贵专家参与的应用场景中具有实际价值，能够在保持性能的同时显著降低训练成本。

Abstract: Learning to defer uncertain predictions to costly experts offers a powerful
strategy for improving the accuracy and efficiency of machine learning systems.
However, standard training procedures for deferral algorithms typically require
querying all experts for every training instance, an approach that becomes
prohibitively expensive when expert queries incur significant computational or
resource costs. This undermines the core goal of deferral: to limit unnecessary
expert usage. To overcome this challenge, we introduce the budgeted deferral
framework, which aims to train effective deferral algorithms while minimizing
expert query costs during training. We propose new algorithms for both
two-stage and single-stage multiple-expert deferral settings that selectively
query only a subset of experts per training example. While inspired by active
learning, our setting is fundamentally different: labels are already known, and
the core challenge is to decide which experts to query in order to balance cost
and predictive performance. We establish theoretical guarantees for both of our
algorithms, including generalization bounds and label complexity analyses.
Empirical results across several domains show that our algorithms substantially
reduce training costs without sacrificing prediction accuracy, demonstrating
the practical value of our budget-aware deferral algorithms.

</details>


### [76] [An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning](https://arxiv.org/abs/2510.26709)
*Chuyan Chen,Chenyang Ma,Zhangxin Li,Yutong He,Yanjie Dong,Kun Yuan*

Main category: cs.LG

TL;DR: ARC-Top-K proposed as an All-Reduce-compatible Top-K gradient compressor that aligns sparsity across nodes via a lightweight gradient sketch, enabling index-free communication while preserving important information. It is contractive and, with momentum EF21M, achieves linear speedup and improved convergence; empirically matches Top-K accuracy with significant wall-clock time reduction (~60%).


<details>
  <summary>Details</summary>
Motivation: Reduce communication bottlenecks in large-scale distributed ML without sacrificing convergence or requiring expensive All-Gather; existing Rand-K loses structure, Top-K loses contraction and needs costly operations; need an All-Reduce-friendly, contractive, efficient compressor.

Method: ARC-Top-K uses a lightweight sketch to align sparsity patterns across nodes, enabling index-free All-Reduce. It preserves globally significant information via Top-K selection under a coordinated sparsity scheme. It is contractive and combines with momentum error feedback EF21M to boost convergence.

Result: The compressor is provably contractive. When combined with EF21M, it attains linear speedup and sharper convergence rates than the original EF21M under standard assumptions. Empirically, ARC-Top-K matches Top-K accuracy while reducing wall-clock training time by up to 60.7%.

Conclusion: ARC-Top-K offers a robust, scalable gradient compressor that blends Rand-K robustness with Top-K performance, mitigating communication bottlenecks in distributed training while preserving convergence and accuracy.

Abstract: Communication remains a central bottleneck in large-scale distributed machine
learning, and gradient sparsification has emerged as a promising strategy to
alleviate this challenge. However, existing gradient compressors face notable
limitations: Rand-$K$\ discards structural information and performs poorly in
practice, while Top-$K$\ preserves informative entries but loses the
contraction property and requires costly All-Gather operations. In this paper,
we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that
aligns sparsity patterns across nodes using a lightweight sketch of the
gradient, enabling index-free All-Reduce while preserving globally significant
information. ARC-Top-$K$\ is provably contractive and, when combined with
momentum error feedback (EF21M), achieves linear speedup and sharper
convergence rates than the original EF21M under standard assumptions.
Empirically, ARC-Top-$K$\ matches the accuracy of Top-$K$\ while reducing
wall-clock training time by up to 60.7\%, offering an efficient and scalable
solution that combines the robustness of Rand-$K$\ with the strong performance
of Top-$K$.

</details>


### [77] [On the limitation of evaluating machine unlearning using only a single training seed](https://arxiv.org/abs/2510.26714)
*Jamie Lanyon,Axel Finke,Petros Andreou,Georgina Cosma*

Main category: cs.LG

TL;DR: MU 方法对随机种子敏感，单一种子评估可能不具代表性，比较应包含多种种子带来的变异性。


<details>
  <summary>Details</summary>
Motivation: 确保 MU 算法评估具有可重复性和公平性，单种子评估可能掩盖方法真实性能差异。

Method: 对多种训练种子下的 MU 算法进行实验分析，观察结果对种子敏感程度，提出将种子变异性纳入基准评估的建议。

Result: 证实某些 MU 方法对随机种子高度敏感，单次重复的评估可能产生偏差，需在对比中报告种子间变异。

Conclusion: 在 MU 评估中应包含多种训练种子并报告统计量，提升可比性和再现性。

Abstract: Machine unlearning (MU) aims to remove the influence of certain data points
from a trained model without costly retraining. Most practical MU algorithms
are only approximate and their performance can only be assessed empirically.
Care must therefore be taken to make empirical comparisons as representative as
possible. A common practice is to run the MU algorithm multiple times
independently starting from the same trained model. In this work, we
demonstrate that this practice can give highly non-representative results
because -- even for the same architecture and same dataset -- some MU methods
can be highly sensitive to the choice of random number seed used for model
training. We therefore recommend that empirical
comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should
also reflect the variability across different model training seeds.

</details>


### [78] [On Purely Private Covariance Estimation](https://arxiv.org/abs/2510.26717)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: 提出了一种简单的扰动机制，在纯差分隐私下发布d维协方差矩阵Σ；在大数据场景(n≥d^2/ε)达到Frobenius范数误差的信息论最优，并对所有p-Schatten范数(p≥1)达到最优或接近最优；在n<d^2/ε时，通过投影到合适半径的核范数球实现Frobenius误差O(√(d Tr(Σ)/n))，优于现有界。


<details>
  <summary>Details</summary>
Motivation: 在保障隐私的前提下发布协方差矩阵的公度差分隐私（DP）机制需要在不同数据集规模下提供强且尽可能统一的误差界，弥合大数据与小数据条件下的差距；现有工作在某些范数上有界，但在谱范数等更强范数上的最优性尚待解决，且小数据情形的误差界不理想。

Method: 提出一个简单的扰动机制用于公开Σ的纯DP下的发布，并在大数据条件下实现Frobenius和所有p-Schatten范数的误差界。对于n<d^2/ε，改用对输出进行核范数球投影的后处理，以获得Frobenius误差的最优阶O(√(d Tr(Σ)/n))。

Result: 当n≥d^2/ε时，获得与Nikolo(v) 2023相同的Frobenius误差下界的渐进上界，并在所有p≥1的Schatten范数下达到最佳已知误差；对n<d^2/ε时，通过核范数球投影实现Frobenius误差O(√(d Tr(Σ)/n))，显著优于Nikolo(v) 2023的O(√(d/n))和Dong 2022的O(d^{3/4}√(Tr(Σ)/n))的界。

Conclusion: 该机制在数据规模无论大还是小下均提供接近最优的误差界，且在谱范数上达到信息论最优，且引入了对小数据集的一致后处理步骤以获得最佳Frobenius误差。

Abstract: We present a simple perturbation mechanism for the release of $d$-dimensional
covariance matrices $\Sigma$ under pure differential privacy. For large
datasets with at least $n\geq d^2/\varepsilon$ elements, our mechanism recovers
the provably optimal Frobenius norm error guarantees of
\cite{nikolov2023private}, while simultaneously achieving best known error for
all other $p$-Schatten norms, with $p\in [1,\infty]$. Our error is
information-theoretically optimal for all $p\ge 2$, in particular, our
mechanism is the first purely private covariance estimator that achieves
optimal error in spectral norm.
  For small datasets $n< d^2/\varepsilon$, we further show that by projecting
the output onto the nuclear norm ball of appropriate radius, our algorithm
achieves the optimal Frobenius norm error $O(\sqrt{d\;\text{Tr}(\Sigma) /n})$,
improving over the known bounds of $O(\sqrt{d/n})$ of \cite{nikolov2023private}
and ${O}\big(d^{3/4}\sqrt{\text{Tr}(\Sigma)/n}\big)$ of
\cite{dong2022differentially}.

</details>


### [79] [Deep sequence models tend to memorize geometrically; it is unclear why](https://arxiv.org/abs/2510.26745)
*Shahriar Noroozizadeh,Vaishnavh Nagarajan,Elan Rosenfeld,Sanjiv Kumar*

Main category: cs.LG

TL;DR: Transformer memory is better explained by a learned global geometry of atomic facts, not merely local co-occurrences; this geometry, shaped by spectral bias, simplifies reasoning and suggests ways to make memory more geometric.


<details>
  <summary>Details</summary>
Motivation: To challenge the view of parametric memory as brute-force co-occurrence storage, and to uncover the geometric structure of embeddings, its origins, and implications for knowledge acquisition and model capacity.

Method: Isolate a clean Transformer reasoning instance incompatible with local co-occurrence memory; analyze geometric encoding of global relations among entities (including non-co-occurring ones); connect to Node2Vec and spectral bias; discuss implications.

Result: Found that a global embedding geometry emerges even when not more succinct than simple lookups; this geometry enables reduced reasoning complexity and is not readily explained by standard architectural or optimization pressures; spectral bias underpins this geometry.

Conclusion: A geometric view of parametric memory provides a new lens for understanding knowledge representation, with practical headroom to enhance Transformer memory geometry; encourages revisiting assumptions about knowledge acquisition, capacity, discovery, and unlearning.

Abstract: In sequence modeling, the parametric memory of atomic facts has been
predominantly abstracted as a brute-force lookup of co-occurrences between
entities. We contrast this associative view against a geometric view of how
memory is stored. We begin by isolating a clean and analyzable instance of
Transformer reasoning that is incompatible with memory as strictly a storage of
the local co-occurrences specified during training. Instead, the model must
have somehow synthesized its own geometry of atomic facts, encoding global
relationships between all entities, including non-co-occurring ones. This in
turn has simplified a hard reasoning task involving an $\ell$-fold composition
into an easy-to-learn 1-step geometric task.
  From this phenomenon, we extract fundamental aspects of neural embedding
geometries that are hard to explain. We argue that the rise of such a geometry,
despite optimizing over mere local associations, cannot be straightforwardly
attributed to typical architectural or optimizational pressures.
Counterintuitively, an elegant geometry is learned even when it is not more
succinct than a brute-force lookup of associations.
  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry
stems from a spectral bias that -- in contrast to prevailing theories -- indeed
arises naturally despite the lack of various pressures. This analysis also
points to practitioners a visible headroom to make Transformer memory more
strongly geometric. We hope the geometric view of parametric memory encourages
revisiting the default intuitions that guide researchers in areas like
knowledge acquisition, capacity, discovery and unlearning.

</details>


### [80] [STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization](https://arxiv.org/abs/2510.26771)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.LG

TL;DR: STaMP 通过在序列维度执行线性变换并采用混合精度来提升低位宽激活量化的精度，适用于语言/视觉模型。


<details>
  <summary>Details</summary>
Motivation: 在激活低于8位时量化精度下降的问题需要新策略；可逆线性变换（如旋转）有助于重新参数化通道和权重以改善量化。

Method: 沿序列维度应用可逆的线性变换，并引入混合精度策略：在中间激活中仅保留少量 token 以高精度，其余部分使用更低位宽，从而在总体激活位宽较低时保持信息。

Result: 在最近的 LVM 与 LLM 架构上评估，显著提升低位宽激活量化的精度，且与现有激活/权重量化方法及最近的特征变换互补。

Conclusion: STaMP 提供一种新范式，用于提升低比特激活量化的可行性，适用于语言与视觉模型，并可与其他量化方法协同工作。

Abstract: Quantization is the key method for reducing inference latency, power and
memory footprint of generative AI models. However, accuracy often degrades
sharply when activations are quantized below eight bits. Recent work suggests
that invertible linear transformations (e.g. rotations) can aid quantization,
by reparameterizing feature channels and weights. In this paper, we propose
\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a
novel strategy that applies linear transformations along the \textit{sequence}
dimension to exploit the strong local correlation in language and visual data.
By keeping a small number of tokens in each intermediate activation at higher
precision, we can maintain model accuracy at lower (average) activations
bit-widths. We evaluate STaMP on recent LVM and LLM architectures,
demonstrating that it significantly improves low bit width activation
quantization and complements established activation and weight quantization
methods including recent feature transformations.

</details>


### [81] [Faithful and Fast Influence Function via Advanced Sampling](https://arxiv.org/abs/2510.26776)
*Jungyeon Koh,Hyeonsu Lyu,Jonggyu Jang,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 提出基于特征和 logits 的两种抽样方法以高效估计影响函数（IF），通过小子集逼近全数据的 Hessian 计算，从而提升 IF 的稳定性与精度。


<details>
  <summary>Details</summary>
Motivation: 直接对整个数据集求取 Hessian 代价高昂且对随机子集的高方差导致 IF 估计不稳定，因此需提出更具代表性的子集采样策略来提升 IF 的准确性与可用性。

Method: 提出两种基于特征分布和 logits 分布的高级采样技术，从特征或 logits 的随机分布中选取一个小型但具有代表性的训练子集，用以近似整个数据集的影响函数估计过程。

Result: 在类移除实验中验证，平均用 F1-score 衡量模型忘记被移除类别的能力，同时保持对其他类别的一致性。相比基线，方法在时间上减少 30.1% 的计算量、内存使用减少 42.2%，在 F1-score 上提升 2.5%。

Conclusion: 所提出的基于特征与 logits 的子集采样策略显著提升了基于影响函数的黑箱模型解释的效率与稳定性，为高效的影响估计提供了有效途径。

Abstract: How can we explain the influence of training data on black-box models?
Influence functions (IFs) offer a post-hoc solution by utilizing gradients and
Hessians. However, computing the Hessian for an entire dataset is
resource-intensive, necessitating a feasible alternative. A common approach
involves randomly sampling a small subset of the training data, but this method
often results in highly inconsistent IF estimates due to the high variance in
sample configurations. To address this, we propose two advanced sampling
techniques based on features and logits. These samplers select a small yet
representative subset of the entire dataset by considering the stochastic
distribution of features or logits, thereby enhancing the accuracy of IF
estimations. We validate our approach through class removal experiments, a
typical application of IFs, using the F1-score to measure how effectively the
model forgets the removed class while maintaining inference consistency on the
remaining classes. Our method reduces computation time by 30.1% and memory
usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.

</details>


### [82] [Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification](https://arxiv.org/abs/2510.26777)
*Andreas Auer,Daniel Klotz,Sebastinan Böck,Sepp Hochreiter*

Main category: cs.LG

TL;DR: 冻结的预训练时序 forecasting 模型可用作分类任务的通用表示，其分类性能可媲美甚至超越专门针对分类预训练的模型；对不同表示提取策略进行比较，并提出两种与模型无关的嵌入增强方法；结果显示两者之间存在正相关性，表明 forecast 学习可构建通用的时序基础模型。


<details>
  <summary>Details</summary>
Motivation: 探究通过预训练的时间序列预测模型学习的表示是否能泛化到分类任务，从而挑战“任务特定预训练”必要性的假设。

Method: 在冻结的预测模型上比较多种表示提取策略，提出两种模型无关的嵌入增强；通过分类任务评估表示质量，并分析预测能力与分类性能的相关性。

Result: 最佳预测模型在分类准确率上达到并可媲美甚至超越为分类专门预训练的最先进模型；并观察到预测能力与分类性能之间存在正相关性。

Conclusion: 学习预测可能提供强大的通用时间序列基础模型路径，任务特定的预训练不一定必要，时间序列领域的 forecast 学习成为构建通用表示的有力途径。

Abstract: Recent research on time series foundation models has primarily focused on
forecasting, leaving it unclear how generalizable their learned representations
are. In this study, we examine whether frozen pre-trained forecasting models
can provide effective representations for classification. To this end, we
compare different representation extraction strategies and introduce two
model-agnostic embedding augmentations. Our experiments show that the best
forecasting models achieve classification accuracy that matches or even
surpasses that of state-of-the-art models pre-trained specifically for
classification. Moreover, we observe a positive correlation between forecasting
and classification performance. These findings challenge the assumption that
task-specific pre-training is necessary, and suggest that learning to forecast
may provide a powerful route toward constructing general-purpose time series
foundation models.

</details>


### [83] [Clone Deterministic 3D Worlds with Geometrically-Regularized World Models](https://arxiv.org/abs/2510.26782)
*Zaishuo Xia,Yukuan Lu,Xinyi Li,Yifan Xu,Yubei Chen*

Main category: cs.LG

TL;DR: 通过几何正则化的潜在表示，GRWM显著提升世界模型在确定性3D环境中的长时程预测稳定性和保真度，表明改进表示学习直接提升世界模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型对exteroceptive高维输入易发生信息损失和耦合，导致在长时程中性能下降。需要通过改进表示学习来获得更符合环境拓扑的潜在表征，以提升预测稳定性和保真度。

Method: 提出Geometrically-Regularized World Models (GRWM)，强制感知序列中相邻时间步在潜在空间保持接近，从而获得更佳的潜在流形结构。GRWM可插拔、只需对现有骨架做最小改动，能够随轨迹长度线性扩展，兼容多种潜在生成骨干。

Result: 在确定性3D设置和长时程预测任务中，GRWM显著提高了 rollout 的保真度与稳定性，并且分析表明收益来自学习到具有更优几何结构的潜在流形，使潜在表示更接近环境的真实拓扑。

Conclusion: 改进表示学习是获得鲁棒世界模型的直接有效路径，可在不扩展动力学模块的前提下提升长时程预测的可靠性。

Abstract: A world model is an internal model that simulates how the world evolves.
Given past observations and actions, it predicts the future of both the
embodied agent and its environment. Accurate world models are essential for
enabling agents to think, plan, and reason effectively in complex, dynamic
settings. Despite rapid progress, current world models remain brittle and
degrade over long horizons. We argue that a central cause is representation
quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or
entangled latents make dynamics learning unnecessarily hard. We therefore ask
whether improving representation learning alone can substantially improve
world-model performance. In this work, we take a step toward building a truly
accurate world model by addressing a fundamental yet open problem: constructing
a model that can fully clone and overfit to a deterministic 3D world. We
propose Geometrically-Regularized World Models (GRWM), which enforces that
consecutive points along a natural sensory trajectory remain close in latent
representation space. This approach yields significantly improved latent
representations that align closely with the true topology of the environment.
GRWM is plug-and-play, requires only minimal architectural modification, scales
with trajectory length, and is compatible with diverse latent generative
backbones. Across deterministic 3D settings and long-horizon prediction tasks,
GRWM significantly increases rollout fidelity and stability. Analyses show that
its benefits stem from learning a latent manifold with superior geometric
structure. These findings support a clear takeaway: improving representation
learning is a direct and useful path to robust world models, delivering
reliable long-horizon predictions without enlarging the dynamics module.

</details>


### [84] [Defeating the Training-Inference Mismatch via FP16](https://arxiv.org/abs/2510.26788)
*Penghui Qi,Zichen Liu,Xiangxin Zhou,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 通过将 BF16 回退到 FP16，解决 RL 微调中的训练与推理数值不一致问题，从而获得更稳定、收敛更快且性能更强的结果。


<details>
  <summary>Details</summary>
Motivation: RLHF 等 RL 微调在训练与推理阶段的数值不一致性导致不稳定性；尽管已有算法或工程层面的修正，根本原因在于浮点精度。BF16 虽拥有较大动态范围，但引入较大舍入误差，破坏训练和推理的一致性。

Method: 将数值表示从 BF16 改回 FP16，框架均支持且只需几行代码，无需修改模型结构或学习算法。

Result: 在多任务、算法、框架下，FP16 能提供更稳定的优化、加速收敛、并带来更强的性能。

Conclusion: 提示在 RL 微调中应重新考量精度取舍，FP16 的使用应更加广泛，以提升鲁棒性与效率。

Abstract: Reinforcement learning (RL) fine-tuning of large language models (LLMs) often
suffers from instability due to the numerical mismatch between the training and
inference policies. While prior work has attempted to mitigate this issue
through algorithmic corrections or engineering alignments, we show that its
root cause lies in the floating point precision itself. The widely adopted
BF16, despite its large dynamic range, introduces large rounding errors that
breaks the consistency between training and inference. In this work, we
demonstrate that simply reverting to \textbf{FP16} effectively eliminates this
mismatch. The change is simple, fully supported by modern frameworks with only
a few lines of code change, and requires no modification to the model
architecture or learning algorithm. Our results suggest that using FP16
uniformly yields more stable optimization, faster convergence, and stronger
performance across diverse tasks, algorithms and frameworks. We hope these
findings motivate a broader reconsideration of precision trade-offs in RL
fine-tuning.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [85] [Competitive Equilibrium for Electricity Markets with Spatially Flexible Load](https://arxiv.org/abs/2510.26036)
*Nan Gu,Junjie Qin*

Main category: eess.SY

TL;DR: 提出了一种广义竞争均衡（GCE）框架，用以描述电网与空间可灵活负荷（如电动车充电与地理分布数据中心）之间的价格-需求耦合，证明在若干结构性条件下，GCE具有存在性、唯一性和效率性，并可推广到多负荷情形，配合NYISO与Sioux Falls的案例研究进行验证。


<details>
  <summary>Details</summary>
Motivation: 现实中的FLs通过LMP与系统决策互相影响，形成闭环反馈，挑战传统电力市场的竞争均衡假设；需要一个能捕捉跨基础设施价格-需求互动的理论框架以设计与分析相关市场机制。

Method: 构建一个对综合性、跨系统的框架，给出能保证CE性质的结构性条件；框架无需详细知道单个FL系统的决策过程；将网格与多类FL系统耦合；通过简化案例和NYISO- Sioux Falls的实际情景来演示。

Result: 在满足给定结构条件下，GCE存在、唯一且高效；框架具可扩展性，可处理多负荷系统耦合；通过案例展示了网格与FL系统之间的相互影响。

Conclusion: GCE扩展了传统CE以适用于耦合基础设施的市场分析，为设计与运行具有FL的现代电力市场提供理论支撑与分析工具。

Abstract: Electric vehicle charging and geo-distributed datacenters introduce spatially
flexible loads (FLs) that couple power, transportation, and datacenter
networks. These couplings create a closed-loop feedback between locational
marginal prices (LMPs) and decisions of the FL systems, challenging the
foundations of conventional competitive equilibrium (CE) in electricity
markets. This paper studies a notion of generalized competitive equilibrium
(GCE) that aims to capture such price-demand interactions across the
interconnected infrastructures. We establish structural conditions under which
the GCE preserves key properties of the conventional CE, including existence,
uniqueness, and efficiency, without requiring detailed knowledge of decision
processes for individual FL systems. The framework generalizes to settings
where the grid is coupled with multiple FL systems. Stylized examples and case
studies on the New York ISO grid, coupled with the Sioux Falls transportation
and distributed datacenter networks, demonstrate the use of our theoretical
framework and illustrate the mutual influence among the grid and the studied FL
systems.

</details>


### [86] [Command-filter-based trajectory-tracking control of quadrotor subject to internal and external disturbances](https://arxiv.org/abs/2510.26368)
*Mustafa Mohammed Mustafa*

Main category: eess.SY

TL;DR: 提出一种命令滤波回溯控制器，结合扰动观测器和高增益观测器，对四旋翼在内部和外部干扰下实现鲁棒跟踪，通过新状态变换与Lyapunov推导降低Backstepping的复杂性。


<details>
  <summary>Details</summary>
Motivation: 解决未知的内部/外部干扰对四旋翼飞行控制的影响，同时避免回步设计的指数级复杂性，并在较低传感器精度条件下实现鲁棒飞行。

Method: 将测量输出与期望输出之间的跟踪误差定义并进行状态变量重写，基于变换模型应用Lyapunov理论推导回步控制律；引入一阶命令滤波器以避免对状态和虚拟控制的重复微分；加入非线性扰动观测器提供扰动估计；将控制器与观测器中的每个状态用高增益观测器的估计值替代；并使用新状态变换和Lyapunov推导控制复杂度不爆炸，同时HGO重建未测量的状态及其变化率。

Result: 理论上实现对四旋翼的路径跟踪鲁棒性，在存在内部/外部扰动、风、模型误差和噪声时仍能执行期望轨迹；各子系统可容纳各自的扰动类型；通过HGO实现状态估计，命令滤波与扰动观测器共同降低对高精度传感的依赖，减少了Backstepping设计的复杂度增长。

Conclusion: 创新点在于把命令滤波、扰动观测器和高增益观测器三者结合，同时解决扰动抑制和部分观测的挑战，且避免了传统Backstepping的复杂度膨胀。

Abstract: We propose a command-filter backstepping controller that integrates a
disturbance observer and a high-gain observer (HGO) to handle unknown internal
and external disturbances acting on a quadrotor. To build the controller, we
first define tracking errors between the measured and desired quadrotor
outputs, which allow the system to be rewritten in a new set of state
variables. Using this transformed model, we apply Lyapunov theory to derive a
backstepping control law. To avoid repeated differentiation of states and
virtual controls, a first-order command filter is introduced, and a nonlinear
disturbance observer is added to provide disturbance estimates. Each state in
the controller and observer is replaced with its estimate from the HGO. The
resulting control law enables the quadrotor to follow its path despite internal
and external disturbances, with each subsystem allowed its own disturbance type
for realism. A new state transformation and Lyapunov-based derivation prevent
the usual explosion of complexity, while the HGO reconstructs unmeasured states
and their rates for output feedback. The nonlinear disturbance observer
attenuates constant and nonlinear disturbances as well as band-limited white
noise. The method reduces dependence on high-precision sensors and mitigates
wind, model error, and rotor noise effects during flight. Unlike previous
studies that treat either disturbance rejection or partial sensing, this work
combines the command filter, disturbance observer, and HGO to address both
challenges simultaneously while avoiding the complexity growth typical of
backstepping designs.

</details>


### [87] [Safety Margins of Inverse Optimal ISSf Controllers](https://arxiv.org/abs/2510.26397)
*Ziliang Lyu,Yiguang Hong,Lihua Xie,Miroslav Krstic*

Main category: eess.SY

TL;DR: 本文研究一般非线性系统在逆最优安全滤波器ISSf控制下的增益裕度，建立逆向ISSf-BF理论与HJI方程之间的等价性，给出无扰动与有扰动条件下的增益裕度分析，并提出提升鲁棒性及保持全局稳定性的增益裕度改进策略。


<details>
  <summary>Details</summary>
Motivation: 揭示在反馈实现ISSf、逆最优性、以及求解HJI用于逆向ISSf增益分配之间的等价性，并提升对带扰动系统安全性与鲁棒性的理解。

Method: 通过建立逆向ISSf-BF定理，分析增益对安全集的影响，推导无扰动下在不同情形的增益裕度（如f安全/u0不安全等），证明gain变化下安全集的局部渐近稳定性，并提出增益裕度改进策略以提升鲁棒性，在扰动存在时保持同等裕度且实现全局渐近稳定。

Result: 获得具体的增益裕度区间[1/2, ∞)，在不同情形可实现增益降低或任意提升；若f(x)不安全且u0安全可使增益任意增大；扰动存在时，改进策略仍维持相同裕度并确保安全集全局渐近稳定；同时需要增加控制努力以换取更好的鲁棒性。

Conclusion: 逆最优ISSf控制具备天然的增益裕度，且可通过提出的策略实现对增益变化的鲁棒性并确保安全集全局稳定性；但实现该鲁棒性需付出额外的控制代价。

Abstract: We investigate the gain margin of a general nonlinear system under an inverse
optimal input-to-state safe (ISSf) controller of the form u=u0(x)+u*(x,u0),
where u0 is the nominal control and u* is the inverse optimal safety filter
that minimally modifies the nominal controller's unsafe actions over the
infinite horizon. By first establishing a converse ISSf-BF theorem, we reveal
the equivalence among the achievability of ISSf by feedback, the achievability
of inverse optimality, and the solvability of a Hamilton-Jacobi-Isaacs equation
associated with the inverse optimal ISSf gain assignment. Then we develop a
collection of safety margin results on the overall control u=u0+u*. In the
absence of disturbances, we find that standard inverse optimal safe controllers
have a certain degree of gain margin. Specifically, when f(x) acts safely but
u0 acts unsafely, the gain can be decreased by up to half; and when f(x) acts
unsafely, we establish that, if u0 acts safely, the gain can be increased
arbitrarily, whereas if u0 acts unsafely, the control recovers the full gain
margin [1/2,inf). It is shown, however, that under control gain variation, the
safe set of these controllers is locally asymptotically stable, which implies
that their safety is sensitive to large but bounded disturbances. To make
inverse optimal ISSf controllers robust to gain variation, we propose a gain
margin improvement approach at the expense of an increased control effort. This
improvement allows the inverse optimal safe control to inherit the standard
gain margin of [1/2,inf) without requiring prior knowledge of whether f(x) or
u0 acts safely on the safety boundary, while simultaneously ensuring global
asymptotic stability of the resulting safe set. In the presence of
disturbances, this improvement idea renders inverse optimal ISSf controllers
robust to gain variations with the same gain margin of [1/2,inf).

</details>


### [88] [Efficient Collision-Avoidance Constraints for Ellipsoidal Obstacles in Optimal Control: Application to Path-Following MPC and UAVs](https://arxiv.org/abs/2510.26531)
*David Leprich,Mario Rosenfelder,Markus Herrmann-Wicklmayr,Kathrin Flaßkamp,Peter Eberhard,Henrik Ebel*

Main category: eess.SY

TL;DR: Modular optimal control framework for 3D ellipsoidal obstacle avoidance integrated with MPC for UAV path-following, handling static/moving obstacles with differentiable collision detection and a two-stage optimization strategy; validated in simulation and on Crazyflie hardware.


<details>
  <summary>Details</summary>
Motivation: Need for efficient, differentiable collision constraints in 3D obstacle avoidance for UAVs and to ensure real-time MPC feasibility with moving obstacles.

Method: Modular optimal control framework; continuous differentiable collision detection with ellipsoids; two-stage optimization to fix numerical issues; applied to model predictive path-following; experiments with Crazyflie.

Result: Simulations and real-world experiments validate effectiveness; first hardware demonstration of such an MPC controller for UAVs in 3D task.

Conclusion: The proposed approach provides an effective, hardware-validated method for 3D ellipsoidal obstacle avoidance in UAV MPC, with differentiable constraints and mitigated numerical issues, enabling practical deployment.

Abstract: This article proposes a modular optimal control framework for local
three-dimensional ellipsoidal obstacle avoidance, exemplarily applied to model
predictive path-following control. Static as well as moving obstacles are
considered. Central to the approach is a computationally efficient and
continuously differentiable condition for detecting collisions with ellipsoidal
obstacles. A novel two-stage optimization approach mitigates numerical issues
arising from the structure of the resulting optimal control problem. The
effectiveness of the approach is demonstrated through simulations and
real-world experiments with the Crazyflie quadrotor. This represents the first
hardware demonstration of an MPC controller of this kind for UAVs in a
three-dimensional task.

</details>


### [89] [Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV Networks](https://arxiv.org/abs/2510.26578)
*Jikang Deng,Hui Zhou,Mohamed-Slim Alouini*

Main category: eess.SY

TL;DR: 在灾后场景中，提出一种将绳 tethered UAV (T-UAV) 与无绳 UAV (U-UAV) 相结合的异构框架，并采用集成接入与回传（IAB）来为 U-UAV 提供无线回传。通过两时间尺度的多智能体深度确定性策略梯度（TTS-MADDPG）算法进行联合调度与轨迹优化，以提升下行吞吐量，且在与 TTS-MAPPO、MADDPG 等基线相比取得显著提升（最大约 12.2% 的平均吞吐量增益）。


<details>
  <summary>Details</summary>
Motivation: 在灾难发生后，快速部署可靠通信以支持搜索、救援与恢复行动至关重要。传统的无绳 UAV 受制于尺寸、重量和功耗（SWaP），难以维持宏基站的运作，因此需要将具备持续供电和回传能力的绳 tethered UAV 与具备灵活部署和较高吞吐的无绳 UAV 结合起来，提升边缘用户的吞吐和在用户迁移到安全区域时的连通性。

Method: 提出一个两时间尺度的联合用户调度与轨迹控制优化问题，采用 IAB 技术为 U-UAV 提供无线回传。构建以集中训练、分布执行的两时间尺度多智能体深度确定性策略梯度（TTS-MADDPG）算法来求解该优化问题，并与两时间尺度多智能体近端策略优化（TTS-MAPPO）及 MADDPG 调度方法进行对比。

Result: 数值结果显示所提算法优于基线，具有鲁棒性且吞吐量更高。与 MADDPG 调度方法相比，平均吞吐量提升可达 12.2%。

Conclusion: 将绳 tethered UAV 与无绳 UAV 相结合、结合 IAB 的异构框架，以及两时间尺度的多智能体深度强化学习策略，能够在灾后场景中提高下行吞吐并维持移动边缘用户的连通性，验证了集成接入/回传和异构无人机协同的有效性。

Abstract: In post-disaster scenarios, the rapid deployment of adequate communication
infrastructure is essential to support disaster search, rescue, and recovery
operations. To achieve this, uncrewed aerial vehicle (UAV) has emerged as a
promising solution for emergency communication due to its low cost and
deployment flexibility. However, conventional untethered UAV (U-UAV) is
constrained by size, weight, and power (SWaP) limitations, making it incapable
of maintaining the operation of a macro base station. To address this
limitation, we propose a heterogeneous UAV-based framework that integrates
tethered UAV (T-UAV) and U-UAVs, where U-UAVs are utilized to enhance the
throughput of cell-edge ground user equipments (G-UEs) and guarantee seamless
connectivity during G-UEs' mobility to safe zones. It is noted that the
integrated access and backhaul (IAB) technique is adopted to support the
wireless backhaul of U-UAVs. Accordingly, we formulate a two-timescale joint
user scheduling and trajectory control optimization problem, aiming to maximize
the downlink throughput under asymmetric traffic demands and G-UEs' mobility.
To solve the formulated problem, we proposed a two-timescale multi-agent deep
deterministic policy gradient (TTS-MADDPG) algorithm based on the centralized
training and distributed execution paradigm. Numerical results show that the
proposed algorithm outperforms other benchmarks, including the two-timescale
multi-agent proximal policy optimization (TTS-MAPPO) algorithm and MADDPG
scheduling method, with robust and higher throughput. Specifically, the
proposed algorithm obtains up to 12.2\% average throughput gain compared to the
MADDPG scheduling method.

</details>


### [90] [Optimal Bidding and Coordinated Dispatch of Hybrid Energy Systems in Regulation Markets](https://arxiv.org/abs/2510.26602)
*Tanmay Mishra,Dakota Hamilton,Mads R. Almassalkhi*

Main category: eess.SY

TL;DR: 提出一种两层框架以使混合能源系统（HES）参与频率调节市场：上层进行基于历史调节信号的概率约束容量出价；下层进行实时控制以将调节功率分解到各组成资源，并与离线最优调度对比评估灵活性，同时研究溢价策略的盈利性与惩罚门槛，以及对称/非对称HES配置对性能和SOC的影响。


<details>
  <summary>Details</summary>
Motivation: 可再生能源和分布式能源资源的广泛接入带来显著不确定性，挑战电网的灵活性和可靠性。混合能源系统因其可控发电、柔性负荷与电池储能的组合，具备比单一资源更高的灵活性，能够更好地参与频率调节市场。

Method: 提出一个两层框架：上层通过概率约束优化，基于历史调节信号选择容量出价；下层给出实时控制策略，将调节功率在各资源间分解。将该实时控制策略与离线最优调度进行对比，评估灵活性。进一步评估溢价出价的盈利性及超出门槛时的潜在市场惩罚或资格取消风险。框架还比较了 power capacities 不对称配置对性能和电池状态（SoC）的影响，以及对非对称HES配置下的影响。

Result: 初步分析表明：该两层框架能够提升HES对频率调节市场的参与灵活性，且在历史信号的约束下能保持对调节需求的可控性。与离线最优调度相比，实时分解策略在大部分场景下能接近最优性能，但在预测误差和市场规则约束较强时性能有所下降。溢价策略具有潜在盈利性，但超出某些门槛会显著增加违规风险甚至导致市场资格受限。容量不对称和初始/运行中 SOC 的管理对系统性能和能量管理有明显影响，合理的对称性或权衡配置能提高稳定性与收益。

Conclusion: 两层框架为HES参与频率调节市场提供了有效的决策与控制路径，强调容量平衡与SOC管理的重要性。未来工作可在鲁棒优化、不同市场规则的适应性，以及更大规模的仿真与实际试点验证上深入展开。

Abstract: The increasing integration of renewable energy sources and distributed energy
resources (DER) into modern power systems introduces significant uncertainty,
posing challenges for maintaining grid flexibility and reliability. Hybrid
energy systems (HES), composed of controllable generators, flexible loads, and
battery storage, offer a decentralized solution to enhance flexibility compared
to single centralized resources. This paper presents a two-level framework to
enable HES participation in frequency regulation markets. The upper level
performs a chance-constrained optimization to choose capacity bids based on
historical regulation signals. At the lower level, a real-time control strategy
disaggregates the regulation power among the constituent resources. This
real-time control strategy is then benchmarked against an offline optimal
dispatch to evaluate flexibility performance. Additionally, the framework
evaluates the profitability of overbidding strategies and identifies thresholds
beyond which performance degradation may lead to market penalties or
disqualification. The proposed framework also compare the impact of imbalance
of power capacities on performance and battery state of charge (SoC) through
asymmetric HES configurations.

</details>


### [91] [Graph approach for observability analysis in power system dynamic state estimation](https://arxiv.org/abs/2510.26701)
*Akhila Kandivalasa,Marcos Netto*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The proposed approach yields a numerical method that provably executes in
linear time with respect to the number of nodes and edges in a graph. The
graph, constructed from the power system model, requires only knowledge of the
dependencies between state-to-state and output-to-state variables within a
state-space framework. While graph-based observability analysis methods exist
for power system static-state estimation, the approach presented here is the
first for dynamic-state estimation (DSE). We examine decentralized and
centralized DSE scenarios and compare our findings with a well-established,
albeit non-scalable, observability analysis method in the literature. When
compared to the latter in a centralized DSE setting, our method reduced
computation time by 1440x.

</details>


### [92] [Pareto-Optimal Sampling and Resource Allocation for Timely Communication in Shared-Spectrum Low-Altitude Networks](https://arxiv.org/abs/2510.26708)
*Bowen Li,Jiping Luo,Themistoklis Charalambous,Nikolaos Pappas*

Main category: eess.SY

TL;DR: 提出一种面向低空无人机在共用频谱中的数据新鲜性保障的双目标优化框架，利用预测信道和轨迹，将采样时序、功率与频谱分配在长期规划下进行联合优化；通过基于单调性的转化，提出一种图形化算法，能够在低复杂度下获得完整的Pareto前沿，并在仿真中实现显著的RB利用率下降或能量节省。


<details>
  <summary>Details</summary>
Motivation: 在共享频谱环境中，确保空中数据的新鲜性需要在 UAV 自身能耗和对地面信道资源的占用之间权衡；长期规划下的Pareto优化能同时提升数据时效性并实现资源公平共存。

Method: 基于预测信道模型与预测的 UAV 轨迹，建立一个跨越长时域的双目标Pareto优化问题，联合优化空中数据采样时序、发射功率与频谱分配；利用前提的单调性性质将双目标问题转化为若干单目标问题，提出一种基于图的算法，证明能够在较低复杂度下找到完整的Pareto最优集合，时间复杂度随时域线性、RB预算近似二次。

Result: 算法能够找到完整的Pareto最优集合；数值对比表明在满足及时性要求的同时，可实现RB利用率的六倍下降或相比基线节省约6 dB的能量，优于基准方案。

Conclusion: 所提出的图基Pareto优化算法可在低复杂度下完整刻画Pareto前沿，提升低空 UAV 在共享频谱中的数据新鲜性与资源共存效率，具有良好的可扩展性与实用性。

Abstract: Guaranteeing stringent data freshness for low-altitude unmanned aerial
vehicles (UAVs) in shared spectrum forces a critical trade-off between two
operational costs: the UAV's own energy consumption and the occupation of
terrestrial channel resources. The core challenge is to satisfy the aerial data
freshness while finding a Pareto-optimal balance between these costs.
Leveraging predictive channel models and predictive UAV trajectories, we
formulate a bi-objective Pareto optimization problem over a long-term planning
horizon to jointly optimize the sampling timing for aerial traffic and the
power and spectrum allocation for fair coexistence. However, the problem's
non-convex, mixed-integer nature renders classical methods incapable of fully
characterizing the complete Pareto frontier. Notably, we show monotonicity
properties of the frontier, building on which we transform the bi-objective
problem into several single-objective problems. We then propose a new
graph-based algorithm and prove that it can find the complete set of Pareto
optima with low complexity, linear in the horizon and near-quadratic in the
resource block (RB) budget. Numerical comparisons show that our approach meets
the stringent timeliness requirement and achieves a six-fold reduction in RB
utilization or a 6 dB energy saving compared to benchmarks.

</details>


### [93] [Time-Optimal Model Predictive Control for Linear Systems with Multiplicative Uncertainties](https://arxiv.org/abs/2510.26712)
*Renato Quartullo,Andrea Garulli,Mirko Leomanni*

Main category: eess.SY

TL;DR: 提出一种面向线性离散时间系统的时间最优MPC，针对乘法不确定性（区间矩阵）进行鲁棒控制，利用矩阵-zonotope对集合值误差进行界定并离线计算界限；通过自适应终端约束确保递归可行性和有限时间收敛，显著降低在线计算开销，并在卫星轨道会合任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现实系统常受乘法型区间不确定性影响，传统MPC在不确定性下在线求解成本高且难以保障可行性。需要在保证收敛性的同时实现时间最优，并尽量降低在线计算量，便于嵌入式实施。

Method: 在线性离散时间系统中引入乘法不确定性（区间矩阵）并将集合值误差的传播通过矩阵-zonotope界定器进行近似界定；设计时间最优MPC目标与自适应终端约束以保证递归可行性和有限时间收敛；所有界限集合可离线计算，显著减轻在线计算负担；通过一个轨道会合任务的数值案例进行验证。

Result: 给出理论保证：递归可行性与有限时间收敛性；在线计算负担明显降低；数值案例（两颗卫星的轨道会合）表明所提方法在含不确定性系统下的有效性。

Conclusion: 该方法在不确定线性系统的时间最优MPC中提供了一种可行且计算友好的解法，矩阵-zonotope界定与自适应终端约束是关键要点，适用于航天/轨道控制等需要高效鲁棒性能的场景。

Abstract: This paper presents a time-optimal Model Predictive Control (MPC) scheme for
linear discrete-time systems subject to multiplicative uncertainties
represented by interval matrices. To render the uncertainty propagation
computationally tractable, the set-valued error system dynamics are
approximated using a matrix-zonotope-based bounding operator. Recursive
feasibility and finite-time convergence are ensured through an adaptive
terminal constraint mechanism. A key advantage of the proposed approach is that
all the necessary bounding sets can be computed offline, substantially reducing
the online computational burden. The effectiveness of the method is illustrated
via a numerical case study on an orbital rendezvous maneuver between two
satellites.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [94] [Coherence-Aware Distributed Learning under Heterogeneous Downlink Impairments](https://arxiv.org/abs/2510.25917)
*Mehdi Karbalayghareh,David J. Love,Christopher G. Brinton*

Main category: cs.IT

TL;DR: 提出一种面向异质相干时间的无线联邦学习框架，利用产品叠加在下行实现静态与动态设备的资源重用，并将全局更新嵌入移动设备的 Pilot 中，提高通信效率和收敛性。


<details>
  <summary>Details</summary>
Motivation: 无线FL中CSI时效性受设备移动性和环境差异影响，导致相干时间不一致，传统方案因资源浪费而低效。

Method: 提出基于相干感知的资源重用框架：在下行信道中通过产品叠加实现对静态设备与动态设备的同时调度，将静态设备的全局模型更新嵌入到用于移动设备的 Pilot 传输中，以实现联合信道训练与模型更新，并给出收敛性分析。

Result: 给出收敛性分析，并量化在期望通信效率和训练精度上的提升；实验验证在移动性诱发的动态下的有效性，揭示了实际部署的有用见解。

Conclusion: 该框架为无线FL在异质衰落动力学下的实际部署提供了可行路径，强调相干感知与资源复用对提升训练效率的重要性，并指出未来对多天线、鲁棒性和上行链路等扩展方向。

Abstract: The performance of federated learning (FL) over wireless networks critically
depends on accurate and timely channel state information (CSI) across
distributed devices. This requirement is tightly linked to how rapidly the
channel gains vary, i.e., the coherence intervals. In practice, edge devices
often exhibit unequal coherence times due to differences in mobility and
scattering environments, leading to unequal demands for pilot signaling and
channel estimation resources. Conventional FL schemes that overlook this
coherence disparity can suffer from severe communication inefficiencies and
training overhead. This paper proposes a coherence-aware,
communication-efficient framework for joint channel training and model updating
in practical wireless FL systems operating under heterogeneous fading dynamics.
Focusing on downlink impairments, we introduce a resource-reuse strategy based
on product superposition, enabling the parameter server to efficiently schedule
both static and dynamic devices by embedding global model updates for static
devices within pilot transmissions intended for mobile devices. We
theoretically analyze the convergence behavior of the proposed scheme and
quantify its gains in expected communication efficiency and training accuracy.
Experiments demonstrate the effectiveness of the proposed framework under
mobility-induced dynamics and offer useful insights for the practical
deployment of FL over wireless channels.

</details>


### [95] [Duality-Based Fixed Point Iteration Algorithm for Beamforming Design in ISAC Systems](https://arxiv.org/abs/2510.26147)
*Xilai Fan,Ya-Feng Liu*

Main category: cs.IT

TL;DR: ISAC波束成形通过半定松弛(SDR)与对偶固定点法解决，将含不定权的广义下游波束成形(GDB)问题转化为可解形式；提出Dual-FPI算法，含外层子梯度上升与内层FPI，实验显示达到全局最优并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 在ISAC场景中，需同时满足通信SINR和雷达MSE的耦合约束，寻求高效且理论上有保证的波束成形解，以降低总发射功率并实现高效联合感知与通信.

Method: 建立原始ISAC波束成形问题与SDR的等价性，推导拉格朗日对偶并进一步将其重构为带可能不定权矩阵的广义下行波束成形(GDB)问题。给出GDB问题的必要且充分的有界性条件，并提出一个带收敛性保证的定点迭代(FPI)算法解决GDB。基于此，提出Dual-FPI算法：包含外层的子梯度上升循环和内层的FPI循环，以实现近似最优解并降低计算复杂度。

Result: 仿真结果表明，Dual-FPI能够获得全局最优解（在理论及条件允许下），且与现有基线相比显著降低计算复杂度。

Conclusion: 本工作提供了一种可用于ISAC的高效对偶性固定点框架，尤其在带不定权矩阵的广义下行波束成形场景具备理论与计算优势，对未来在更复杂系统中的应用具有潜在价值。

Abstract: In this paper, we investigate the beamforming design problem in an integrated
sensing and communication (ISAC) system, where a multi-antenna base station
simultaneously serves multiple communication users while performing radar
sensing. We formulate the problem as the minimization of the total transmit
power, subject to signal-to-interference-plus-noise ratio (SINR) constraints
for communication users and mean-squared-error (MSE) constraints for radar
sensing. The core challenge arises from the complex coupling between
communication SINR requirements and sensing performance metrics. To efficiently
address this challenge, we first establish the equivalence between the original
ISAC beamforming problem and its semidefinite relaxation (SDR), derive its
Lagrangian dual formulation, and further reformulate it as a generalized
downlink beamforming (GDB) problem with potentially indefinite weighting
matrices. Compared to the classical DB problem, the presence of indefinite
weighting matrices in the GDB problem introduces substantial analytical and
computational challenges. Our key technical contributions include (i) a
necessary and sufficient condition for the boundedness of the GDB problem, and
(ii) a tailored efficient fixed point iteration (FPI) algorithm with a provable
convergence guarantee for solving the GDB problem. Building upon these results,
we develop a duality-based fixed point iteration (Dual-FPI) algorithm, which
integrates an outer subgradient ascent loop with an inner FPI loop. Simulation
results demonstrate that the proposed Dual-FPI algorithm achieves globally
optimal solutions while significantly reducing computational complexity
compared with existing baseline approaches.

</details>


### [96] [Efficient Spectral Efficiency Maximization Design for IRS-aided MIMO Systems](https://arxiv.org/abs/2510.26279)
*Fuying Li,Yajun Wang,Zhuxian Lian,Wen Chen*

Main category: cs.IT

TL;DR: 提出一个 ADMM-APG 算法用于 IRS 辅助 MIMO 的联合传输前馈矩阵与 IRS 相位移的优化，以最大化光谱效率；通过将原问题分解为子问题并给出封闭形式解，达到比基准方法更高的光谱效率和更低的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 为提升无线通信的频谱效率，IRS 能 动态重构传播环境；但是联合优化发射 precoding 与 IRS 相位移的非凸问题难以求解，亟需高效算法。

Method: 将交替方向乘子法（ADMM）与加速投影梯度法（APG）结合，将原问题分解成若干可逐步求解的子问题，每个子问题都具备闭式解，且计算复杂度低。

Result: 仿真结果显示与现有基准相比，ADMM-APG 在光谱效率和计算复杂度上均有显著提升，在不同系统配置下表现稳健。

Conclusion: 所提出的 ADMM-APG 框架有效解决 IRS 辅助 MIMO 的光谱效率最大化问题，具备较高的性能和较低的实现复杂度，为未来在高效IRS系统设计中提供了可行方案。

Abstract: Driven by the growing demand for higher spectral efficiency in wireless
communications, intelligent reflecting sur- faces (IRS) have attracted
considerable attention for their ability to dynamically reconfigure the
propagation environment. This work addresses the spectral efficiency
maximization problem in IRS-assisted multiple-input multiple-output (MIMO)
systems, which involves the joint optimization of the transmit precoding matrix
and the IRS phase shift configuration. This problem is inherently challenging
due to its non-convex nature. To tackle it effectively, we introduce a
computationally efficient algorithm, termed ADMM-APG, which integrates the
alternating direction method of multipliers (ADMM) with the accelerated
projected gradient (APG) method. The proposed framework decomposes the original
problem into tractable subproblems, each admitting a closed-form solution while
maintaining low computational com- plexity. Simulation results demonstrate that
the ADMM-APG algorithm consistently surpasses existing benchmark methods in
terms of spectral efficiency and computational complexity, achieving
significant performance gains across a range of system configurations.

</details>


### [97] [Diffusion-Aided Bandwidth-Efficient Semantic Communication with Adaptive Requests](https://arxiv.org/abs/2510.26442)
*Xuesong Wang,Xinyan Xie,Mo Li,Zhaoqian Liu*

Main category: cs.IT

TL;DR: 通过自适应重传的扩散式语义通信框架，在仅发送简明文本描述和少量关键潜在视觉特征的前提下进行图像重建，通过扩散模型进行修复，并在接收端进行语义一致性检测以触发增量传输，从而在保持语义与视觉保真度的同时显著降低带宽开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有语义通信在视觉内容传输中对语义冗余的高成本问题；文本描述往往无法捕捉空间布局和细粒度外观，且直接传输密集潜在特征会导致显著的语义冗余。需要在保持语义理解与视觉保真度的前提下降低冗余、提升带宽效率。

Method: 提出一个扩散式语义通信框架，发送简洁文本描述和有限的关键潜在视觉特征，利用扩散式修复（inpainting）模型重建图像；接收端引入语义一致性评估，若重建结果与原始文本存在偏差则触发重传，请求少量额外潜在块以细化重建。

Result: 理论与实验指示在保持高语义准确性的前提下显著降低带宽开销，能够在重建质量与传输成本之间实现有效折衷。

Conclusion: 该框架通过自适应重传和扩散重建实现更高的传输效率与语义保真，提出了视觉语义传输的新方向。

Abstract: Semantic communication focuses on conveying the intrinsic meaning of data
rather than its raw symbolic representation. For visual content, this paradigm
shifts from traditional pixel-level transmission toward leveraging the semantic
structure of images to communicate visual meaning. Existing approaches
generally follow one of two paths: transmitting only text descriptions, which
often fail to capture precise spatial layouts and fine-grained appearance
details; or transmitting text alongside dense latent visual features, which
tends to introduce substantial semantic redundancy. A key challenge, therefore,
is to reduce semantic redundancy while preserving semantic understanding and
visual fidelity, thereby improving overall transmission efficiency. This paper
introduces a diffusion-based semantic communication framework with adaptive
retransmission. The system transmits concise text descriptions together with a
limited set of key latent visual features, and employs a diffusion-based
inpainting model to reconstruct the image. A receiver-side semantic consistency
mechanism is designed to evaluate the alignment between the reconstructed image
and the original text description. When a semantic discrepancy is detected, the
receiver triggers a retransmission to request a small set of additional latent
blocks and refine the image reconstruction. This approach significantly reduces
bandwidth usage while preserving high semantic accuracy, achieving an efficient
balance between reconstruction quality and transmission overhead.

</details>


### [98] [PolarZero: A Reinforcement Learning Approach for Low-Complexity Polarization Kernel Design](https://arxiv.org/abs/2510.26452)
*Yi-Ting Hong,Stefano Rini,Luca Barletta*

Main category: cs.IT

TL;DR: 使用基于强化学习的Gumbel AlphaZero框架设计大核极化码的核结构，在递归最大似然解码下实现较低解码复杂度，同时保持或提升误差指数。对于尺寸为16的核，学习设计比手工设计的解码复杂度降低约17%，误差指数达到0.5183（优于Arikan核的0.5）。


<details>
  <summary>Details</summary>
Motivation: 解决大核极化码在提升误差指数的同时实现低解码复杂度的挑战；通过智能搜索设计空间以发现符合目标误差指数且解码复杂度更低的核结构。

Method: 在递归最大似然解码（RMLD）框架下，采用基于Gumbel AlphaZero的强化学习方法对核设计进行搜索与优化，目标是在给定误差指数约束下最小化解码复杂度。

Result: 对于尺寸为16的核，所得到的学习设计在解码复杂度上比手工设计低约17%，且误差指数达到0.5183，超过Arikan核的0.5。

Conclusion: 证明了基于学习的核设计在实际极化码构造中的有效性，能够在性能与实现复杂度之间取得更优的权衡。

Abstract: Polar codes with large kernels can achieve improved error exponents but are
challenging to design with low decoding com- plexity. This work investigates
kernel construction under recursive maximum likelihood decoding (RMLD) using a
reinforcement learning framework based on the Gumbel AlphaZero algorithm. The
proposed method efficiently explores the design space and identifies large-size
kernels that satisfy a given error exponent while minimizing decoding
complexity. For a size-16 kernel, it achieves 17% lower decoding complexity
than handcrafted designs while reaching an error exponent of 0.5183 compared to
0.5 for Arikan's kernel, demonstrating the effectiveness of the learning-based
approach for practical polar code construction.

</details>


### [99] [Entropy Functions on Two-Dimensional Faces of Polymatroidal Region of Degree Four: Part II: Information Theoretic Constraints Breed New Combinatorial Structures](https://arxiv.org/abs/2510.26552)
*Shaocheng Liu,Qi Chen,Minquan Cheng*

Main category: cs.IT

TL;DR: Part II of a two-paper series on entropy functions on 2D faces of the polymatroidal region Γ4. Completes the remaining 10 face types (8 fully characterized, 2 partially). Introduces new combinatorial design structures.


<details>
  <summary>Details</summary>
Motivation: To deepen understanding of entropy structures under the Shannon outer bound by characterizing entropy functions on the faces of the polymatroidal region; extend Part I and enrich combinatorial design theory.

Method: Extend the framework from Part I: enumerate the 2D face types of Γ4 with an algorithm; analyze entropy functions constrained to each face; fully characterize 8 of the 10 remaining face types and partially characterize the other 2; introduce new combinatorial design structures to facilitate the characterization.

Result: From the 59 possible 2D face types of Γ4, Part I fully characterized 49. Part II completes the remaining 10 face types, with 8 fully characterized and 2 partially characterized. Additionally, new combinatorial design structures are introduced to assist the characterization.

Conclusion: A substantial step toward a complete entropy-function picture on 2D faces of Γ4. The newly introduced combinatorial designs not only assist the current classifications but may hold independent interest for related information-theoretic and combinatorial investigations.

Abstract: Characterization of entropy functions is of fundamental importance in
information theory. By imposing constraints on their Shannon outer bound, i.e.,
the polymatroidal region, one obtains the faces of the region and entropy
functions on them with special structures. In this series of two papers, we
characterize entropy functions on the $2$-dimensional faces of the
polymatroidal region $\Gamma_4$. In Part I, we formulated the problem,
enumerated all $59$ types of $2$-dimensional faces of $\Gamma_4$ by a
algorithm, and fully characterized entropy functions on $49$ types of them. In
this paper, i.e., Part II, we will characterize entropy functions on the
remaining $10$ types of faces, among which $8$ types are fully characterized
and $2$ types are partially characterized. To characterize these types of
faces, we introduce some new combinatorial design structures which are
interesting themself.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [100] [Symmetry-Driven Asynchronous Forwarding for Reliable Distributed Coordination in Toroidal Networks](https://arxiv.org/abs/2510.26071)
*Shenshen Luan,Yumo Tian,Xinyu Zhang,Qingwen Zhang,Tianheng Wang,Yan Yang,Shuguo Xie*

Main category: cs.NI

TL;DR: 基于 torus 对称性的异步转发机制，在无需控制平面协作的情况下提升在不可靠链路上的包传递鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在大规模分布式系统（如卫星星座、HPC 集群）中，链路不可靠时需要可靠的通信原语。现有路由在控制平面故障或同步失效时易产生较高包丢失。

Method: 建立以拓扑势梯度为模型的流量框架，揭示对称性破缺会诱发反向流，并以此绕过故障。提出两种局部转发策略：RF-CF（反向流+对向优先）与 RF-LF（反向流+侧向优先）。在不修改协议、无额外包头开销的前提下，确保在前向流相变点实现到达目标。通过渗透分析与在 16×16 torus 上的包级仿真进行评估。

Result: 在 1% 链路故障率下，包丢失率可降低至原始水平的约82.5%（降低幅度最高达17.5%）。RF-LF 策略对成功交付的包贡献约28%。

Conclusion: 将拓扑对称性与通信鲁棒性建立联系，提供轻量、协议无关的基础设施来提升分布式系统的鲁棒性。

Abstract: The proliferation of large-scale distributed systems, such as satellite
constellations and high-performance computing clusters, demands robust
communication primitives that maintain coordination under unreliable links. The
torus topology, with its inherent rotational and reflection symmetries, is a
prevalent architecture in these domains. However, conventional routing schemes
suffer from substantial packet loss during control-plane synchronization after
link failures. This paper introduces a symmetry-driven asynchronous forwarding
mechanism that leverages the torus's geometric properties to achieve reliable
packet delivery without control-plane coordination. We model packet flow using
a topological potential gradient and demonstrate that symmetry-breaking
failures naturally induce a reverse flow, which we harness for fault
circumvention. We propose two local forwarding strategies, Reverse Flow with
Counter-facing Priority (RF-CF) and Lateral-facing Priority (RF-LF), that
guarantee reachability to the destination via forward-flow phase transition
points, without protocol modifications or additional in-packet overhead.
Through percolation analysis and packet-level simulations on a 16 x 16 torus,
we show that our mechanism reduces packet loss by up to 17.5% under a 1% link
failure rate, with the RF-LF strategy contributing to 28% of successfully
delivered packets. This work establishes a foundational link between
topological symmetry and communication resilience, providing a lightweight,
protocol-agnostic substrate for enhancing distributed systems.

</details>


### [101] [From req/res to pub/sub: Exploring Media over QUIC Transport for DNS](https://arxiv.org/abs/2510.26234)
*Mathis Engelbart,Mike Kosek,Lars Eggert,Jörg Ott*

Main category: cs.NI

TL;DR: 提出一种基于Media-over-QUIC的DNS发布-订阅变体，用于推送资源记录更新，原型实现显示在降低更新流量和提高更新时效方面有潜在好处，但也带来额外状态开销与首次查询时的会话建立延迟。


<details>
  <summary>Details</summary>
Motivation: DNS 作为互联网基础设施，长期从静态目录服务逐步扩展到负载均衡、服务发现等用例；在某些场景下，向所有曾对相关记录感兴趣的解析器主动分发更新比请求-缓存模式更合适。

Method: 提出一个草案系统及协议，基于Media-over-QUIC，实现RR更新的推送；给出原型实现并进行评估。

Result: 推送更新能显著降低更新流量并缩短解析器获取最新版本RR的时间，从而支持CDN中的负载均衡等用例；同时带来端点侧额外状态管理开销和首次查询的会话建立延迟，增加查询延迟。

Conclusion: DNS可从发布-订阅变体中受益，提升更新时效性与减少流量，但需解决额外开销和首次查询延迟等挑战，未来工作包括优化会话管理、降低开销等。

Abstract: The DNS is a key component of the Internet. Originally designed to facilitate
the resolution of host names to IP addresses, its scope has continuously
expanded over the years, today covering use cases such as load balancing or
service discovery. While DNS was initially conceived as a rather static
directory service in which resource records (RR) only change rarely, we have
seen a number of use cases over the years where a DNS flavor that isn't purely
based upon requesting and caching RRs, but rather on an active distribution of
updates for all resolvers that showed interest in the respective records in the
past, would be preferable. In this paper, we thus explore a publish-subscribe
variant of DNS based on the Media-over-QUIC architecture, where we devise a
strawman system and protocol proposal to enable pushing RR updates. We provide
a prototype implementation, finding that DNS can benefit from a
publish-subscribe variant: next to limiting update traffic, it can considerably
reduce the time it takes for a resolver to receive the latest version of a
record, thereby supporting use cases such as load balancing in content
distribution networks. The publish-subscribe architecture also brings new
challenges to the DNS, including a higher overhead for endpoints due to
additional state management, and increased query latencies on first lookup, due
to session establishment latencies.

</details>


### [102] [Joint Computing Resource Allocation and Task Offloading in Vehicular Fog Computing Systems Under Asymmetric Information](https://arxiv.org/abs/2510.26256)
*Geng Sun,Siyi Chen,Zemin Sun,Long He,Jiacheng Wang,Dusit Niyato,Zhu Han,Dong In Kim*

Main category: cs.NI

TL;DR: 提出一个分层VFC架构，结合RSU和FV的计算资源，采用凸优化进行RSU资源分配、契约理论激励机制进行FV资源配置，以及双边匹配理论用于任务卸载，以解决信息不对称、资源有限和任务异质性带来的挑战，显著降低延迟并提升完成率、吞吐量与资源利用公平性。


<details>
  <summary>Details</summary>
Motivation: 解决VFC中资源有限、信息不对称和任务异质性带来的挑战；通过分层架构充分利用RSU与FV的计算能力，提升任务完成时间和资源配置效率。

Method: 将问题建模为DMOP（延迟最小化的混合整数非线性规划，NP-hard）。提出JCRATOA：1) 使用凸优化对RSU资源进行分配；2) 以契约理论构建FV资源获取的激励机制；3) 采用双边匹配（匹配博弈）实现任务卸载的高效分配。

Result: 仿真结果表明，JCRATOA在任务完成延迟、完成率、系统吞吐量和资源利用公平性方面均优于基线，且能有效满足约束条件。

Conclusion: 所提出的分层VFC架构与联合资源分配及任务卸载方法能够缓解资源约束和信息不对称问题，提升系统性能，具备应用潜力并为后续工作提供了方向。

Abstract: Vehicular fog computing (VFC) has emerged as a promising paradigm, which
leverages the idle computational resources of nearby fog vehicles (FVs) to
complement the computing capabilities of conventional vehicular edge computing.
However, utilizing VFC to meet the delay-sensitive and computation-intensive
requirements of the FVs poses several challenges. First, the limited resources
of road side units (RSUs) struggle to accommodate the growing and diverse
demands of vehicles. This limitation is further exacerbated by the information
asymmetry between the controller and FVs due to the reluctance of FVs to
disclose private information and to share resources voluntarily. This
information asymmetry hinders the efficient resource allocation and
coordination. Second, the heterogeneity in task requirements and the varying
capabilities of RSUs and FVs complicate efficient task offloading, thereby
resulting in inefficient resource utilization and potential performance
degradation. To address these challenges, we first present a hierarchical VFC
architecture that incorporates the computing capabilities of both RSUs and FVs.
Then, we formulate a delay minimization optimization problem (DMOP), which is
an NP-hard mixed integer nonlinear programming problem. To solve the DMOP, we
propose a joint computing resource allocation and task offloading approach
(JCRATOA). Specifically, we propose a convex optimization-based method for RSU
resource allocation and a contract theory-based incentive mechanism for FV
resource allocation. Moreover, we present a two-sided matching method for task
offloading by employing the matching game. Simulation results demonstrate that
the proposed JCRATOA is able to achieve superior performances in task
completion delay, task completion ratio, system throughput, and resource
utilization fairness, while effectively meeting the satisfying constraints.

</details>


### [103] [Wireless Memory Approximation for Energy-efficient Task-specific IoT Data Retrieval](https://arxiv.org/abs/2510.26473)
*Junya Shiraishi,Shashi Raj Pandey,Israel Leyva-Mayorga,Petar Popovski*

Main category: cs.NI

TL;DR: 提出基于无线内存激活与无线内存近似的DRAM能效优化框架，通过按需管理ML模型存储访问以降低 standby 能耗，同时保持检索准确性。


<details>
  <summary>Details</summary>
Motivation: 在ML推理场景中，DRAM需要周期性刷新，导致 standby 能耗显著，尤其对资源受限的物联网设备。需在不牺牲检索精度的前提下降低能源浪费。

Method: 提出两种新颖方法：1) 无线内存激活：通过无线机制触发DRAM进入活动/待机状态的控制，降低非必要时的能耗；2) 无线内存近似：通过无线通道对存储的ML模型进行近似/压缩访问，结合时序与模型使用相关性来调度访问，从而减少能耗并保持必要的检索精度。

Result: 数值结果显示，与始终开启的方案相比，在满足检索准确性约束的前提下，该方案实现了更低的能耗。

Conclusion: 该方案为无线/物联网场景下的内存管理提供了一种能效提升途径，能够在保证检索准确性的同时降低DRAM相关能耗，适用于ML推理加速的下一代通信系统。

Abstract: The use of Dynamic Random Access Memory (DRAM) for storing Machine Learning
(ML) models plays a critical role in accelerating ML inference tasks in the
next generation of communication systems. However, periodic refreshment of DRAM
results in wasteful energy consumption during standby periods, which is
significant for resource-constrained Internet of Things (IoT) devices. To solve
this problem, this work advocates two novel approaches: 1) wireless memory
activation and 2) wireless memory approximation. These enable the wireless
devices to efficiently manage the available memory by considering the timing
aspects and relevance of ML model usage; hence, reducing the overall energy
consumption. Numerical results show that our proposed scheme can realize
smaller energy consumption than the always-on approach while satisfying the
retrieval accuracy constraint.

</details>


### [104] [Low-Altitude UAV-Carried Movable Antenna for Joint Wireless Power Transfer and Covert Communications](https://arxiv.org/abs/2510.26628)
*Chuang Zhang,Geng Sun,Jiahui Li,Jiacheng Wang,Qingqing Wu,Dusit Niyato,Shiwen Mao,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 提出在低空UAV携带可移动天线进行WPT与隐蔽通信的联合系统，通过能量信号作为天然掩护实现对IoT的能量补充和对隐蔽用户的传输链接；提出MoE-SAC算法解决多目标优化的非凸且时序耦合性问题。


<details>
  <summary>Details</summary>
Motivation: 在物联网密集部署场景中，能量供给与隐蔽通信需求并存，传统方案受限于信道可见性和能量效率，需在能源补给、覆盖和隐蔽性之间进行权衡，且UAV能量与天线位置的动态优化极具挑战。

Method: 提出低空 UAV 携带可移动天线的传输系统，联合无线能量传输与隐蔽通信。建模一个多目标优化问题：最大化IoT节点总能量收获和隐蔽用户的吞吐率之和，同时最小化UAV推进能耗。为求解非凸、时序耦合问题，提出混合专家增强的软演员-评论家（MoE-SAC）算法，采用稀疏Top-K门控的混合浅层专家来表示多模态策略分布，并加入一个动作投影模块强制执行逐时隙的功率预算与天线位置约束。

Result: 仿真结果显示，该方法在多个基线方法及先进深度强化学习算法上显著提升性能，能在能量收获和隐蔽通信效率之间取得平衡并降低 UAV 能耗。

Conclusion: 提出的MoE-SAC 框架结合可移动天线和隐蔽通信策略，提供一种在能量供给与隐蔽传输场景中可行且高效的解决方案，但实际部署需解决信道建模、硬件实现与法规约束等问题。

Abstract: The proliferation of Internet of Things (IoT) networks has created an urgent
need for sustainable energy solutions, particularly for the battery-constrained
spatially distributed IoT nodes. While low-altitude uncrewed aerial vehicles
(UAVs) employed with wireless power transfer (WPT) capabilities offer a
promising solution, the line-of-sight channels that facilitate efficient energy
delivery also expose sensitive operational data to adversaries. This paper
proposes a novel low-altitude UAV-carried movable antenna-enhanced transmission
system joint WPT and covert communications, which simultaneously performs
energy supplements to IoT nodes and establishes transmission links with a
covert user by leveraging wireless energy signals as a natural cover. Then, we
formulate a multi-objective optimization problem that jointly maximizes the
total harvested energy of IoT nodes and sum achievable rate of the covert user,
while minimizing the propulsion energy consumption of the low-altitude UAV. To
address the non-convex and temporally coupled optimization problem, we propose
a mixture-of-experts-augmented soft actor-critic (MoE-SAC) algorithm that
employs a sparse Top-K gated mixture-of-shallow-experts architecture to
represent multimodal policy distributions arising from the conflicting
optimization objectives. We also incorporate an action projection module that
explicitly enforces per-time-slot power budget constraints and antenna position
constraints. Simulation results demonstrate that the proposed approach
significantly outperforms some baseline approaches and other state-of-the-art
deep reinforcement learning algorithms.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [105] [APThreatHunter: An automated planning-based threat hunting framework](https://arxiv.org/abs/2510.25806)
*Mustafa F. Abdelwahed,Ahmed Shafee,Joan Espasa*

Main category: cs.CR

TL;DR: APThreatHunter 自动生成威胁假设，最小化人类干预，降低偏见、时间与成本；在实际 Android 恶意软件数据上评估，显示自动规划在假设生成中的可行性。


<details>
  <summary>Details</summary>
Motivation: 因为手工生成和确认的假设在威胁狩猎中成本高、耗时长，亟需自动化以提升效率和覆盖面。

Method: 提出 APThreatHunter，基于系统当前状态和指示器，使用自动化规划生成攻击假设，并判断风险是否正在发生；以实际 Android 恶意样本进行评估。

Result: 评估结果表明在目标假设生成上的自动规划具有可行性，能够在最小人工干预下产生有效的威胁假设。

Conclusion: APThreatHunter 可降低分析师偏见、减少时间和成本，证明自动化规划在威胁狩猎中的实用性，具备推广潜力。

Abstract: Cyber attacks threaten economic interests, critical infrastructure, and
public health and safety. To counter this, entities adopt cyber threat hunting,
a proactive approach that involves formulating hypotheses and searching for
attack patterns within organisational networks. Automating cyber threat hunting
presents challenges, particularly in generating hypotheses, as it is a manually
created and confirmed process, making it time-consuming. To address these
challenges, we introduce APThreatHunter, an automated threat hunting solution
that generates hypotheses with minimal human intervention, eliminating analyst
bias and reducing time and cost. This is done by presenting possible risks
based on the system's current state and a set of indicators to indicate whether
any of the detected risks are happening or not. We evaluated APThreatHunter
using real-world Android malware samples, and the results revealed the
practicality of using automated planning for goal hypothesis generation in
cyber threat hunting activities.

</details>


### [106] [A Critical Roadmap to Driver Authentication via CAN Bus: Dataset Review, Introduction of the Kidmose CANid Dataset (KCID), and Proof of Concept](https://arxiv.org/abs/2510.25856)
*Brooke Elizabeth Kidmose,Andreas Brasen Kidmose,Cliff C. Zou*

Main category: cs.CR

TL;DR: 综述现有开源驾驶员指纹数据集的优缺点，提出KCID原始CAN数据集并用于驾驶员认证防盗系统原型，证实CAN总线指纹在现实场景的可行性，并探讨其潜在应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有开源驾驶员指纹数据集的关键局限性（依赖解码诊断数据、固定路径设计、采样率低、缺乏人口统计信息），以推动更鲁棒的CAN总线—基于驾驶员认证的防盗系统的发展。

Method: 对现有数据集进行系统性评审并分析其优缺点，提出KCID数据集：包含16位驾驶员、4辆车的原始CAN总线数据、完整人口统计信息，以及日常驾驶与受控固定路线数据。同时提出一个驾驶员认证防盗框架并在单板机上实现概念验证，通过真实道路试验验证系统可行性，并探索KCID在保险、安全评估、机械异常检测、未成年人驾驶监控和疲劳/酒驾检测等场景的应用。

Result: KCID解决了现有数据集的核心不足，展示了基于CAN总线的驾驶员认证防盗系统的实际可行性，并为驱动认证相关应用提供数据和方法学基础。

Conclusion: 本文提供了一个可用于开发鲁棒、可部署驾驶员认证系统的数据集和方法学基础，KCID的潜在应用超出认证，还可用于驾驶行为分析、汽车安全评估等领域。

Abstract: Modern vehicles remain vulnerable to unauthorized use and theft despite
traditional security measures including immobilizers and keyless entry systems.
Criminals exploit vulnerabilities in Controller Area Network (CAN) bus systems
to bypass authentication mechanisms, while social media trends have expanded
auto theft to include recreational joyriding by underage drivers. Driver
authentication via CAN bus data offers a promising additional layer of
defense-in-depth protection, but existing open-access driver fingerprinting
datasets suffer from critical limitations including reliance on decoded
diagnostic data rather than raw CAN traffic, artificial fixed-route
experimental designs, insufficient sampling rates, and lack of demographic
information.
  This paper provides a comprehensive review of existing open-access driver
fingerprinting datasets, analyzing their strengths and limitations to guide
practitioners in dataset selection. We introduce the Kidmose CANid Dataset
(KCID), which addresses these fundamental shortcomings by providing raw CAN bus
data from 16 drivers across four vehicles, including essential demographic
information and both daily driving and controlled fixed-route data. Beyond
dataset contributions, we present a driver authentication anti-theft framework
and implement a proof-of-concept prototype on a single-board computer. Through
live road trials with an unaltered passenger vehicle, we demonstrate the
practical feasibility of CAN bus-based driver authentication anti-theft
systems. Finally, we explore diverse applications of KCID beyond driver
authentication, including driver profiling for insurance and safety
assessments, mechanical anomaly detection, young driver monitoring, and
impaired driving detection. This work provides researchers with both the data
and methodological foundation necessary to develop robust, deployable driver
authentication systems...

</details>


### [107] [Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies](https://arxiv.org/abs/2510.25878)
*Pavel Hubáček,Jan Václavek,Michelle Yeo*

Main category: cs.CR

TL;DR: 提出以比特币等加密资产作为抵押、实现法定货币贷款的有限托管协议，并给出博弈论分析及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 加密货币日益成为金融资产，需要将其作为抵押实现稳定的法币贷款，提升金融实用性，同时降低对信任的依赖。

Method: 设计有限托管的抵押贷款协议，基于受信仲裁的安全机制，进行形式化博弈论分析，并讨论实现细节和安全性。

Result: 提出具体的安全协议设计与理论分析，展示在受信仲裁条件下的可行性及相关权衡，同时指出未来研究方向。

Conclusion: 本文确立了将加密资产作为抵押的法币贷款的研究方向，强调有限托管设计的可行性并提出进一步研究路线。

Abstract: The rising importance of cryptocurrencies as financial assets pushed their
applicability from an object of speculation closer to standard financial
instruments such as loans. In this work, we initiate the study of secure
protocols that enable fiat-denominated loans collateralized by cryptocurrencies
such as Bitcoin. We provide limited-custodial protocols for such loans relying
only on trusted arbitration and provide their game-theoretical analysis. We
also highlight various interesting directions for future research.

</details>


### [108] [FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X](https://arxiv.org/abs/2510.25932)
*Soufiane Essahli,Oussama Sarsar,Imane Fouad,Anas Motii,Ahmed Bentajer*

Main category: cs.CR

TL;DR: 提出FakeZero，一种完全在客户端本地运行的跨平台浏览器扩展，用于在Facebook和X上标记不可靠信息，具备高精度、低延迟和隐私保护特性。


<details>
  <summary>Details</summary>
Motivation: 社交平台信息传播速度空前，错误信息会侵蚀公共话语，需要在用户设备层面提供隐私保护的可信信息标注与检测能力。

Method: 采用三阶段训练课程：基线微调与领域自适应训练，结合焦点损失、对抗增强及后训练量化。核心模型包括DistilBERT-Quant（67.6 MB）和TinyBERT-Quant（14.7 MB），实现全本地推理，延迟在几十毫秒级别。对239,000条帖子数据进行评估。

Result: DistilBERT-Quant在宏F1为97.1%、准确度97.4%、AUROC为0.996，内存与延迟适中；TinyBERT-Quant在宏F1为95.7%、准确度96.1%、体积为14.7 MB、延迟约40 ms，资源更友好。显示在有限资源预算下也能实现高质量假新闻检测。

Conclusion: 内联可信度提示可帮助政策制定者治理跨平台错信息传播；经用户同意后可为研究者提供大规模假新闻数据收集，促进更鲁棒的检测技术的发展。

Abstract: Social platforms distribute information at unprecedented speed, which in turn
accelerates the spread of misinformation and threatens public discourse. We
present FakeZero, a fully client-side, cross-platform browser extension that
flags unreliable posts on Facebook and X (formerly Twitter) while the user
scrolls. All computation, DOM scraping, tokenisation, Transformer inference,
and UI rendering run locally through the Chromium messaging API, so no personal
data leaves the device.FakeZero employs a three-stage training curriculum:
baseline fine-tuning and domain-adaptive training enhanced with focal loss,
adversarial augmentation, and post-training quantisation. Evaluated on a
dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%
macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of
approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant
variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to
14.7 MB and lowering latency to approximately 40 ms, showing that high-quality
fake-news detection is feasible under tight resource budgets with only modest
performance loss.By providing inline credibility cues, the extension can serve
as a valuable tool for policymakers seeking to curb the spread of
misinformation across social networks. With user consent, FakeZero also opens
the door for researchers to collect large-scale datasets of fake news in the
wild, enabling deeper analysis and the development of more robust detection
techniques.

</details>


### [109] [SoK: Honeypots & LLMs, More Than the Sum of Their Parts?](https://arxiv.org/abs/2510.25939)
*Robert A. Bridges,Thomas R. Mitchell,Mauricio Muñoz,Ted Henriksson*

Main category: cs.CR

TL;DR: 本文是一篇系统化综述（SoK），对基于大型语言模型的诱捕器研究进行梳理：提出检测向量的分类学框架，描述LLM诱捕器的典型架构与评估趋势，并追踪从简单数据降维到自动化智能生成的日志分析演化，最终给出面向自治、自我改进欺骗系统的研究路线图。


<details>
  <summary>Details</summary>
Motivation: 解决LLM诱捕器研究中存在的分散、零散进展与缺乏统一框架的问题，旨在提升对高保真欺骗能力与低操作风险之间权衡的理解，并推动形成可落地的对抗性系统设计。

Method: 通过系统性文献梳理与综合，将研究聚焦于三大交叉领域：一) 诱捕器检测向量的分类学；二) LLM诱捕器的典型架构与评估趋势；三) 诱捕日志分析的演化路径，并据此提出面向未来的研究路线图。

Result: 提出一个综合研究谱系：明确的检测向量分类框架、对LLM诱捕器的 canonical 架构描述、评估方法与指标的演化趋势，以及日志分析从数据降维到自动化智能生成的演化，形成对后续自治与自我改进欺骗系统的路线指引。

Conclusion: 强调该领域的潜力在于构建自治、自我改进的欺骗系统，以应对日益智能化、自动化化的攻击者，并给出可操作的研究路线图，引导未来在高保真欺骗与低风险运维之间实现更好的权衡。

Abstract: The advent of Large Language Models (LLMs) promised to resolve the
long-standing paradox in honeypot design: achieving high-fidelity deception
with low operational risk. However, despite a flurry of research since late
2022, progress has been incremental, and the field lacks a cohesive
understanding of the emerging architectural patterns, core challenges, and
evaluation paradigms. To fill this gap, this Systematization of Knowledge (SoK)
paper provides the first comprehensive overview of this new domain. We survey
and systematize three critical, intersecting research areas: first, we provide
a taxonomy of honeypot detection vectors, structuring the core problems that
LLM-based realism must solve; second, we synthesize the emerging literature on
LLM-honeypots, identifying a canonical architecture and key evaluation trends;
and third, we chart the evolutionary path of honeypot log analysis, from simple
data reduction to automated intelligence generation. We synthesize these
findings into a forward-looking research roadmap, arguing that the true
potential of this technology lies in creating autonomous, self-improving
deception systems to counter the emerging threat of intelligent, automated
attackers.

</details>


### [110] [WaveVerif: Acoustic Side-Channel based Verification of Robotic Workflows](https://arxiv.org/abs/2510.25960)
*Zeynep Yasemin Erdogan,Shishir Nagaraja,Chuadhry Mujeeb Ahmed,Ryan Shah*

Main category: cs.CR

TL;DR: 通过声学侧通道分析实现对机器人执行指令的实时、被动验证，基线条件下对单动作及工作流均达到约80%+的分类准确率，且无需硬件修改。


<details>
  <summary>Details</summary>
Motivation: 解决对机器人行为的实时、低成本且非侵入式验证需求，尤其在敏感环境中的安全性与可靠性评估。

Method: 将声学发射作为侧信道，构建基于机器学习的工作流验证系统，评估移动速度、方向、麦克风距离等因素，使用SVM、DNN、RNN、CNN等四种分类器，对单动作及拣放、包装等工作流进行识别与验证。

Result: 在基线条件下，单动作可实现>80%准确率；四种分类器均达到或接近该水平；拣放与包装等工作流也可以同等高置信度被识别。

Conclusion: 声学信号可以在敏感机器人环境中提供实时、低成本、被动的验证能力，无需对硬件进行修改。

Abstract: In this paper, we present a framework that uses acoustic side- channel
analysis (ASCA) to monitor and verify whether a robot correctly executes its
intended commands. We develop and evaluate a machine-learning-based workflow
verification system that uses acoustic emissions generated by robotic
movements. The system can determine whether real-time behavior is consistent
with expected commands. The evaluation takes into account movement speed,
direction, and microphone distance. The results show that individual robot
movements can be validated with over 80% accuracy under baseline conditions
using four different classifiers: Support Vector Machine (SVM), Deep Neural
Network (DNN), Recurrent Neural Network (RNN), and Convolutional Neural Network
(CNN). Additionally, workflows such as pick-and-place and packing could be
identified with similarly high confidence. Our findings demonstrate that
acoustic signals can support real-time, low-cost, passive verification in
sensitive robotic environments without requiring hardware modifications.

</details>


### [111] [Message Recovery Attack in NTRU via Knapsack](https://arxiv.org/abs/2510.26003)
*Eirini Poimenidou,K. A. Draziotis*

Main category: cs.CR

TL;DR: 提出基于模背包问题的消息恢复攻击，针对所有版本的 NTRU-HPS，加上部分信息公开时的可行性分析。对 ε 约等于 0.45 的情况，FLATTER 归约能在商用桌面上几分钟内恢复消息。


<details>
  <summary>Details</summary>
Motivation: 评估在部分已知信息（关于消息 m 和随机向量 r 的比例 ε）下，NTRU-HPS 的解密是否易被仿射攻击或格子问题的方法破解；揭示信息泄露阈值及其实用性，为后续对该密码系统的安全性评估提供实证依据。

Method: 将解密等价问题转化为在格子中寻找短向量，该格子编码一个模背包问题的实例；在此基础上结合 FLATTER 归约，利用对消息与随机向量子集的部分已知信息进行破解；若已知信息比例 ε 约等于 0.45，则可实现实用的消息恢复。

Result: 理论上可行并在实验上得到证据：当 ε 约等于 0.45 时，能够从模背包问题中恢复消息；实现对 m 的恢复时间在普通桌面计算机上几分钟内。

Conclusion: 表明在有相当比例的先验信息时，NTRU-HPS 的安全性存在显著隐患，攻击依赖于将解密问题转化为格子问题并利用 FLATTER 归约。对参数选择、扰动分布以及信息泄露阈值需要进一步研究，以评估在现实世界中的实际风险与防护策略。

Abstract: In the present paper, we introduce a message-recovery attack based on the
Modular Knapsack Problem, applicable to all variants of the NTRU-HPS
cryptosystem. Assuming that a fraction $\epsilon$ of the coefficients of the
message ${\bf{m}}\in\{-1,0,1\}^N$ and of the nonce vector ${\bf
r}\in\{-1,0,1\}^N$ are known in advance at random positions, we reduce message
decryption to finding a short vector in a lattice that encodes an instance of a
modular knapsack system. This allows us to address a key question: how much
information about ${\bf m}$, or about the pair $({\bf m},{\bf r})$, is required
before recovery becomes feasible? A FLATTER reduction successfully recovers the
message, in practice when $\epsilon\approx 0.45$. Our implementation finds
${\bf m}$ within a few minutes on a commodity desktop.

</details>


### [112] [PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy](https://arxiv.org/abs/2510.26102)
*Lisha Shuai,Jiuling Dong,Nan Zhang,Shaofeng Tan,Haokun Zhang,Zilong Song,Gaoya Dong,Xiaolong Yang*

Main category: cs.CR

TL;DR: PEEL: A post-processing framework for Local Differential Privacy to expose poisoning by re-encoding perturbed data; uses sparsification, normalization, and low-rank projection; claims unbiasedness preserved and improves poisoning exposure with lower client-side cost.


<details>
  <summary>Details</summary>
Motivation: LDP is vulnerable to poisoning; existing defenses are resource-intensive or domain-specific; need a light-weight, generalizable defense for IoT-scale deployments.

Method: PEEL is a non-intrusive post-processing module integrated with LDP that re-encodes LDP-perturbed data via sparsification, normalization, low-rank projection; exploits structural consistency to reveal output and rule poisoning attacks.

Result: Theoretical analysis shows unbiasedness and accuracy preserved; empirical evaluation shows higher poisoning exposure accuracy than four state-of-the-art defenses and reduced client-side computational costs.

Conclusion: PEEL enables robust poisoning exposure while maintaining LDP properties and efficiency, suitable for large-scale IoT deployments.

Abstract: Local Differential Privacy (LDP) is a widely adopted privacy-protection model
in the Internet of Things (IoT) due to its lightweight, decentralized, and
scalable nature. However, it is vulnerable to poisoning attacks, and existing
defenses either incur prohibitive resource overheads or rely on domain-specific
prior knowledge, limiting their practical deployment. To address these
limitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical
framework for LDP, which departs from resource- or prior-dependent
countermeasures and instead leverages the inherent structural consistency of
LDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies
stealthy poisoning effects by re-encoding LDP-perturbed data via
sparsification, normalization, and low-rank projection, thereby revealing both
output and rule poisoning attacks through structural inconsistencies in the
reconstructed space. Theoretical analysis proves that PEEL, integrated with
LDP, retains unbiasedness and statistical accuracy, while being robust to
expose both output and rule poisoning attacks. Moreover, evaluation results
show that LDP-integrated PEEL not only outperforms four state-of-the-art
defenses in terms of poisoning exposure accuracy but also significantly reduces
client-side computational costs, making it highly suitable for large-scale IoT
deployments.

</details>


### [113] [Security Vulnerabilities in AI-Generated Code: A Large-Scale Analysis of Public GitHub Repositories](https://arxiv.org/abs/2510.26103)
*Maximilian Schreiber,Pascal Tippe*

Main category: cs.CR

TL;DR: 对公开 GitHub 仓库中 AI 生成代码的安全漏洞进行大规模实证分析，涵盖 7,703 份代码、来自四大 AI 工具的代码，占比：ChatGPT 91.52%、Copilot 7.50%、CodeWhisperer 0.52%、Tabnine 0.46%，通过 CodeQL 静态分析识别 4,241 条 CWE 漏洞，涵盖 77 种类型。结果显示 87.9% 的 AI 生成代码未检测到 CWE 漏洞，但在语言与工具之间存在显著差异；Python 漏洞率最高，Copilot 在 Python/TypeScript 上表现更好，ChatGPT 在 JavaScript 上表现更优；并发现 39% 的文件用于自动化文档生成。


<details>
  <summary>Details</summary>
Motivation: 揭示 AI 生成代码的安全风险，支持面向语言及场景的安全实践，扩展前人研究、提供对 AI 辅助编码的合规与安全建议。

Method: 收集 7,703 份来自四大 AI 工具的文件，使用 CodeQL 静态分析识别 CWE 漏洞，覆盖 77 种漏洞类型，按语言与工具对漏洞密度与分布进行对比分析。

Result: 共识别 4,241 条 CWE 实例，87.9% 的代码未检测到 CWE 漏洞；Python 漏洞率为 16.18%-18.50%，JavaScript 为 8.66%-8.99%，TypeScript 为 2.50%-7.14%；Copilot 在 Python（1,739 LOC/CWE） 与 TypeScript 上具有更好安全密度，ChatGPT 在 JavaScript 上表现更好；同时发现 39% 的文件用于文档生成。

Conclusion: 结果为语言特定、上下文感知的安全实践提供证据，强调在 AI 代码生成的开发工作流中实行负责任的集成，且这份大规模数据集有助于未来的研究与实践。

Abstract: This paper presents a comprehensive empirical analysis of security
vulnerabilities in AI-generated code across public GitHub repositories. We
collected and analyzed 7,703 files explicitly attributed to four major AI
tools: ChatGPT (91.52\%), GitHub Copilot (7.50\%), Amazon CodeWhisperer
(0.52\%), and Tabnine (0.46\%). Using CodeQL static analysis, we identified
4,241 Common Weakness Enumeration (CWE) instances across 77 distinct
vulnerability types. Our findings reveal that while 87.9\% of AI-generated code
does not contain identifiable CWE-mapped vulnerabilities, significant patterns
emerge regarding language-specific vulnerabilities and tool performance. Python
consistently exhibited higher vulnerability rates (16.18\%-18.50\%) compared to
JavaScript (8.66\%-8.99\%) and TypeScript (2.50\%-7.14\%) across all tools. We
observed notable differences in security performance, with GitHub Copilot
achieving better security density for Python (1,739 LOC per CWE) and
TypeScript, while ChatGPT performed better for JavaScript. Additionally, we
discovered widespread use of AI tools for documentation generation (39\% of
collected files), an understudied application with implications for software
maintainability. These findings extend previous work with a significantly
larger dataset and provide valuable insights for developing language-specific
and context-aware security practices for the responsible integration of
AI-generated code into software development workflows.

</details>


### [114] [Who Moved My Transaction? Uncovering Post-Transaction Auditability Vulnerabilities in Modern Super Apps](https://arxiv.org/abs/2510.26210)
*Junlin Liu,Zhaomeng Deng,Ziming Wang,Mengyu Yao,Yifeng Cai,Yutao Hu,Ziqi Zhang,Yao Guo,Ding Li*

Main category: cs.CR

TL;DR: 研究揭示六款超级应用中删除交易记录的能力及对删除操作的认证不足，暴露了后交易审计完整性的重要漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前安全重点在于交易前的身份验证，忽略了交易后对审计痕迹的保护；若用户能永久删除交易记录，可能隐藏未授权或敏感活动。

Method: 对6名志愿者进行横向评估，在6款超级应用上进行实验，测试删除交易记录的可行性及是否需要强认证。

Result: 所有应用均允许删除交易记录；其中5/6（约83%）未对删除行为进行强认证；仅有1款应用要求生物识别验证才可删除。

Conclusion: 这是移动安全领域中一个近乎普遍但被忽视的漏洞，表明需要从根本上强化后交易审计的完整性，推动安全 paradigm 转向对审计轨迹的保护。

Abstract: Super apps are the cornerstones of modern digital life, embedding financial
transactions into nearly every aspect of daily routine. The prevailing security
paradigm for these platforms is overwhelmingly focused on pre-transaction
authentication, preventing unauthorized payments before they occur. We argue
that a critical vulnerability vector has been largely overlooked: the fragility
of post-transaction audit trails. We investigate the ease with which a user can
permanently erase their transaction history from an app's interface, thereby
concealing unauthorized or sensitive activities from the account owner. To
quantify this threat, we conducted an empirical study with 6 volunteers who
performed a cross-evaluation on six super apps. Our findings are alarming: all
six applications studied allow users to delete transaction records, yet a
staggering five out of six (83+\%) fail to protect these records with strong
authentication. Only one app in our study required biometric verification for
deletion. This study provides the first concrete evidence of this
near-ubiquitous vulnerability, demonstrating a critical gap in the current
mobile security landscape and underscoring the urgent need for a paradigm shift
towards ensuring post-transaction audit integrity.

</details>


### [115] [PVMark: Enabling Public Verifiability for LLM Watermarking Schemes](https://arxiv.org/abs/2510.26274)
*Haohua Duan,Liyao Xiang,Xin Zhang*

Main category: cs.CR

TL;DR: PVMark通过零知识证明实现对大型语言模型水印检测的公钥可验证性，使第三方能够在不暴露秘密密钥的情况下验证检测正确性；并在多语言、多水印方案、多哈希函数及多ZKP协议下实现，显示在不影响水印性能的前提下具备有效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有水印方案的检测多依赖秘密密钥，导致非公开检测难以被外部证明其真实性，存在信任缺口；需要一个公开、可验证的检测过程而又不泄露密钥的方案。

Method: 基于零知识证明的“检测正确执行”证明，将水印检测的映射、随机数生成、比较与求和等过程转化为ZKP约束；实现多种组合：三种水印方案、三种哈希函数、四种ZKP协议；在Python、Rust、Circom等实现以覆盖广泛场景。

Result: 通过实验，PVMark实现了对现有最前沿水印方案的公开可验证性，同时未显著降低水印检测性能，证明其在实际部署中的可行性与有效性。

Conclusion: PVMark提供了一种在不泄露密钥的前提下实现水印检测公开可验证性的解决方案，提升了水印系统的透明度与信任度，具备落地应用潜力。

Abstract: Watermarking schemes for large language models (LLMs) have been proposed to
identify the source of the generated text, mitigating the potential threats
emerged from model theft. However, current watermarking solutions hardly
resolve the trust issue: the non-public watermark detection cannot prove itself
faithfully conducting the detection. We observe that it is attributed to the
secret key mostly used in the watermark detection -- it cannot be public, or
the adversary may launch removal attacks provided the key; nor can it be
private, or the watermarking detection is opaque to the public. To resolve the
dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),
enabling the watermark detection process to be publicly verifiable by third
parties without disclosing any secret key. PVMark hinges upon the proof of
`correct execution' of watermark detection on which a set of ZKP constraints
are built, including mapping, random number generation, comparison, and
summation. We implement multiple variants of PVMark in Python, Rust and Circom,
covering combinations of three watermarking schemes, three hash functions, and
four ZKP protocols, to show our approach effectively works under a variety of
circumstances. By experimental results, PVMark efficiently enables public
verifiability on the state-of-the-art LLM watermarking schemes yet without
compromising the watermarking performance, promising to be deployed in
practice.

</details>


### [116] [SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification](https://arxiv.org/abs/2510.26420)
*Yingjia Wang,Ting Qiao,Xing Liu,Chongzuo Li,Sixing Wu,Jianbin Li*

Main category: cs.CR

TL;DR: 提出了一种样本特定的干净标签后门水印（SSCL-BW），通过U-Net生成每个样本的专属水印，使用三项损失的综合目标函数实现有效性、触发性与不可察觉性，并在黑箱测试下进行所有权验证，实验表明对水印去除攻击有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有后门水印在标签不一致导致毒标签水印易被检测、清洗标签水印在高分辨率图像上的实现困难，以及静态水印模式易被检测/移除的问题。提出样本特定水印以提高鲁棒性和不可感知性。

Method: 训练一个U-Net水印样本生成器，为每个样本生成独特水印。设计三组成分的复合损失：目标样本损失确保水印有效，非目标样本损失保证触发可靠性，感知相似性损失保持视觉不可察觉。通过黑箱测试进行所有权验证。

Result: 在基准数据集上进行广泛实验，展示所提方法的有效性以及对潜在水印移除攻击的鲁棒性。

Conclusion: 样本特定的清洁标签水印克服了静态水印的局限性，提高了水印的隐蔽性和鲁棒性，但实现复杂度较高且需在不同数据集上进一步评估稳定性。

Abstract: The rapid advancement of deep neural networks (DNNs) heavily relies on
large-scale, high-quality datasets. However, unauthorized commercial use of
these datasets severely violates the intellectual property rights of dataset
owners. Existing backdoor-based dataset ownership verification methods suffer
from inherent limitations: poison-label watermarks are easily detectable due to
label inconsistencies, while clean-label watermarks face high technical
complexity and failure on high-resolution images. Moreover, both approaches
employ static watermark patterns that are vulnerable to detection and removal.
To address these issues, this paper proposes a sample-specific clean-label
backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked
sample generator, this method generates unique watermarks for each sample,
fundamentally overcoming the vulnerability of static watermark patterns. The
core innovation lies in designing a composite loss function with three
components: target sample loss ensures watermark effectiveness, non-target
sample loss guarantees trigger reliability, and perceptual similarity loss
maintains visual imperceptibility. During ownership verification, black-box
testing is employed to check whether suspicious models exhibit predefined
backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the
effectiveness of the proposed method and its robustness against potential
watermark removal attacks.

</details>


### [117] [Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy Policies](https://arxiv.org/abs/2510.26523)
*Shuaishuai Liu,Gergely Acs,Gergely Biczók*

Main category: cs.CR

TL;DR: 对20款视频门铃/智能摄像头的隐私政策进行聚焦分析，聚焦旁观者隐私（bystander privacy）。结果显示厂商多以免责声明形式承认旁观者存在，但将数据收集的伦理责任转移给设备拥有者，政策与现实之间存在缺口，并给出面向政策语言和系统设计的改进建议。


<details>
  <summary>Details</summary>
Motivation: 在共享物理空间和数据主体不对称的背景下，智能家居设备对旁观者隐私的影响日益突出。现有法规对跨主体、跨设备的数据控制缺乏激励，厂商隐私政策往往未充分覆盖旁观者维度，需通过分析政策与实际案例，提升透明度并为改进提供方向。

Method: 对20款视频门铃与智能摄像头产品的隐私政策进行聚焦分析，评估其中对旁观者隐私的表述与覆盖程度；结合真实世界案例，检视其对非用户的潜在影响；将厂商政策与现有法律框架及技术能力进行对照，并提出面向政策与系统设计的改进建议。

Result: 研究发现：1) 部分厂商承认旁观者存在，但大多仅以免责声明形式提及，将对非用户数据的收集伦理责任转移给设备拥有者；2) 政策覆盖不一致，缺乏对旁观者权利、通知、数据最小化与透明度的明确要求；3) 存在若干真实案例，显示部署会对非用户造成隐私影响；4) 将厂商政策置于现有法律框架与技术能力之下进行评估，提出改进策略与系统设计原则。

Conclusion: 提高透明度与赋权：建议完善政策语言以明示旁观者权利与设备主控方的责任边界，强化数据最小化、通知与同意机制，并在设备设计层面引入旁观者保护机制（如区域标示、访问控制、数据访问审计、可撤销的监控范围等），以实现对旁观者与设备拥有者的双向保护。

Abstract: Smart home devices such as video doorbells and security cameras are becoming
increasingly common in everyday life. While these devices offer convenience and
safety, they also raise new privacy concerns: how these devices affect others,
like neighbors, visitors, or people passing by. This issue is generally known
as interdependent privacy, where one person's actions (or inaction) may impact
the privacy of others, and, specifically, bystander privacy in the context of
smart homes. Given lax data protection regulations in terms of shared physical
spaces and amateur joint data controllers, we expect that the privacy policies
of smart home products reflect the missing regulatory incentives. This paper
presents a focused privacy policy analysis of 20 video doorbell and smart
camera products, concentrating explicitly on the bystander aspect. We show that
although some of the vendors acknowledge bystanders, they address it only to
the extent of including disclaimers, shifting the ethical responsibility for
collecting the data of non-users to the device owner. In addition, we identify
and examine real-world cases related to bystander privacy, demonstrating how
current deployments can impact non-users. Based on our findings, we analyze
vendor privacy policies in light of existing legal frameworks and technical
capabilities, and we provide practical recommendations for both policy language
and system design to enhance transparency and empower both bystanders and
device owners.

</details>


### [118] [A Comprehensive Evaluation and Practice of System Penetration Testing](https://arxiv.org/abs/2510.26555)
*Chunyi Zhang,Jin Zeng,Xiaoqi Li*

Main category: cs.CR

TL;DR: 该论文综述系统化渗透测试的理论与实践，强调流程、工具评估及在实战中的落地应用，通过案例总结经验教训。


<details>
  <summary>Details</summary>
Motivation: 在信息化不断发展、威胁日益复杂的背景下，系统化的渗透测试有助于发现与修复安全漏洞，提升防御能力。

Method: 对渗透测试过程的各阶段进行梳理，评估现有渗透工具的优劣及适用领域，基于流程选择合适的工具，在目标环境中复现实验攻击，并通过案例分析提炼可操作的经验教训。

Result: 明确了工具选择的维度、流程驱动的测试方法、以及在不同场景下的工具适用性。通过实际案例的对比，总结出某些攻击手段的成功要件与防御要点。

Conclusion: 渗透测试是信息安全中的前瞻性防御手段，需在规范流程、工具组合与持续学习方面持续发展；未来研究可聚焦自动化、场景化测试与持续风险评估。

Abstract: With the rapid advancement of information technology, the complexity of
applications continues to increase, and the cybersecurity challenges we face
are also escalating. This paper aims to investigate the methods and practices
of system security penetration testing, exploring how to enhance system
security through systematic penetration testing processes and technical
approaches. It also examines existing penetration tools, analyzing their
strengths, weaknesses, and applicable domains to guide penetration testers in
tool selection. Furthermore, based on the penetration testing process outlined
in this paper, appropriate tools are selected to replicate attack processes
using target ranges and target machines. Finally, through practical case
analysis, lessons learned from successful attacks are summarized to inform
future research.

</details>


### [119] [A DRL-Empowered Multi-Level Jamming Approach for Secure Semantic Communication](https://arxiv.org/abs/2510.26610)
*Weixuan Chen,Qianqian Yang*

Main category: cs.CR

TL;DR: DRL-enabled multi-level jamming for SemCom security over MIMO fading wiretap channels; combines semantic-layer and physical-layer jamming; uses DDPG to optimize precoding; alternating training; comparable security to ESCS/EJ with PSNR gain ~0.6 dB.


<details>
  <summary>Details</summary>
Motivation: Semantic communications improve efficiency but raise privacy risk as semantics can be eavesdropped; need secure SemCom that preserves task performance while protecting semantic information.

Method: Two-level jamming: semantic-layer by encoding task-irrelevant text; physical-layer by encoding Gaussian noise; superposed with task-relevant semantic information. A DDPG agent designs precoding matrices for semantic information and jamming signals. An alternating optimization scheme trains SemCom and DRL modules iteratively.

Result: Experimental results show security comparable to encryption-based ESCS and encoded jammer EJ benchmarks, with the legitimate user’s PSNR improved by up to ~0.6 dB.

Conclusion: The framework delivers effective semantic security with dynamic, DRL-driven precoding and dual-level jamming, preserving or enhancing legitimate performance while hindering eavesdroppers.

Abstract: Semantic communication (SemCom) aims to transmit only task-relevant
information, thereby improving communication efficiency but also exposing
semantic information to potential eavesdropping. In this paper, we propose a
deep reinforcement learning (DRL)-empowered multi-level jamming approach to
enhance the security of SemCom systems over MIMO fading wiretap channels. This
approach combines semantic layer jamming, achieved by encoding task-irrelevant
text, and physical layer jamming, achieved by encoding random Gaussian noise.
These two-level jamming signals are superposed with task-relevant semantic
information to protect the transmitted semantics from eavesdropping. A deep
deterministic policy gradient (DDPG) algorithm is further introduced to
dynamically design and optimize the precoding matrices for both taskrelevant
semantic information and multi-level jamming signals, aiming to enhance the
legitimate user's image reconstruction while degrading the eavesdropper's
performance. To jointly train the SemCom model and the DDPG agent, we propose
an alternating optimization strategy where the two modules are updated
iteratively. Experimental results demonstrate that, compared with both the
encryption-based (ESCS) and encoded jammer-based (EJ) benchmarks, our method
achieves comparable security while improving the legitimate user's peak
signalto-noise ratio (PSNR) by up to approximately 0.6 dB.

</details>


### [120] [Toward Automated Security Risk Detection in Large Software Using Call Graph Analysis](https://arxiv.org/abs/2510.26620)
*Nicholas Pecka,Lotfi Ben Othmane,Renee Bryce*

Main category: cs.CR

TL;DR: 通过对调用图进行密度聚类和社区检测来实现半自动化威胁建模，并在 Splunk Forwarder Operator 的案例中验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 解决手工威胁建模成本高、易出错的问题，需面向现代云原生环境的可扩展自动化方法。

Method: 在软件调用图上结合密度聚类和社区检测算法进行聚类，再对得到的聚类进行威胁分析；以 Splunk Forwarder Operator（SFO）为案例进行评估，应用选定的聚类指标来评估代码密度与潜在的安全薄弱点。

Result: 结果表明该方法可行，能识别与代码密度相关的安全薄弱点，并促进系统化的威胁评估。

Conclusion: 为现代云原生环境提供可扩展的半自动化威胁建模框架的潜力。

Abstract: Threat modeling plays a critical role in the identification and mitigation of
security risks; however, manual approaches are often labor intensive and prone
to error. This paper investigates the automation of software threat modeling
through the clustering of call graphs using density-based and community
detection algorithms, followed by an analysis of the threats associated with
the identified clusters. The proposed method was evaluated through a case study
of the Splunk Forwarder Operator (SFO), wherein selected clustering metrics
were applied to the software's call graph to assess pertinent code-density
security weaknesses. The results demonstrate the viability of the approach and
underscore its potential to facilitate systematic threat assessment. This work
contributes to the advancement of scalable, semi-automated threat modeling
frameworks tailored for modern cloud-native environments.

</details>
