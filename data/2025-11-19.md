<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.NI](#cs.NI) [Total: 2]
- [eess.SY](#eess.SY) [Total: 23]
- [cs.LG](#cs.LG) [Total: 58]
- [eess.SP](#eess.SP) [Total: 5]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.13771)
*Shaowei Guan,Yu Zhai,Zhengyu Zhang,Yanze Wang,Hin Chi Kwok*

Main category: cs.CR

TL;DR: 提出 ExplainableGuard，一种可解释的对抗性防御框架，利用连锁推理(CoT)来检测、消除文本扰动并给出逐步解释；初步在 GLUE 与 IMDB 数据集显示有效性，人工评估显示解释更清晰、具体、可执行，.deployability-trust 为 72.5%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对对抗性攻击日益脆弱，现有防御多数为黑箱，缺乏透明性和可解释性，需要可解释的防御机制来提升信任。

Method: 通过定制的连锁推理(CoT)提示，利用 DeepSeek-Reasoner 进行多维分析（字符、词、结构、语义），在给出净化后输出的同时提供人类可读的防御理由。

Result: 在 GLUE 基准和 IMDB 影评数据集上展示了有希望的防御效果；并且人类评估表明 ExplainableGuard 的解释在清晰度、具体性和可操作性方面优于删除式对比，具备 72.5% 的可部署性信任度。

Conclusion: 提出一种更可信的 LLM 部署路径，通过可解释的对抗防御提升透明度和信任度，建议进一步扩展评估与应用场景。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to adversarial attacks that can subtly manipulate their outputs. While various defense mechanisms have been proposed, many operate as black boxes, lacking transparency in their decision-making. This paper introduces ExplainableGuard, an interpretable adversarial defense framework leveraging the chain-of-thought (CoT) reasoning capabilities of DeepSeek-Reasoner. Our approach not only detects and neutralizes adversarial perturbations in text but also provides step-by-step explanations for each defense action. We demonstrate how tailored CoT prompts guide the LLM to perform a multi-faceted analysis (character, word, structural, and semantic) and generate a purified output along with a human-readable justification. Preliminary results on the GLUE Benchmark and IMDB Movie Reviews dataset show promising defense efficacy. Additionally, a human evaluation study reveals that ExplainableGuard's explanations outperform ablated variants in clarity, specificity, and actionability, with a 72.5% deployability-trust rating, underscoring its potential for more trustworthy LLM deployments.

</details>


### [2] [Hashpower allocation in Pay-per-Share blockchain mining pools](https://arxiv.org/abs/2511.13777)
*Pierre-Olivier Goffard,Hansjoerg Albrecher,Jean-Pierre Fouque*

Main category: cs.CR

TL;DR: 在工作量证明区块链的PPS奖励系统中，研究矿工如何在多个矿池间分配计算资源，权衡把风险转移给矿池管理者与支付管理费之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 解释矿工为何参与矿池及PPS机制下管理费与难度调整对收入风险的影响。

Method: 使用简化的财富模型，分析矿工在不同矿池之间的资源分配决策，考察可调的分享难度和管理费对最优策略的影响。

Result: 揭示在考虑风险转移与管理费的前提下，矿工的资源分配策略如何随参数变化而变化，提供对PPS系统中风险分担的定量直觉。

Conclusion: 对PPS下的风险分担与定价机制有初步认识，指出未来研究应纳入更真实的收益分布和市场动态等因素。

Abstract: Mining blocks in a blockchain using the \textit{Proof-of-Work} consensus protocol involves significant risk, as network participants face continuous operational costs while earning infrequent capital gains upon successfully mining a block. A common risk mitigation strategy is to join a mining pool, which combines the computing resources of multiple miners to provide a more stable income. This article examines a Pay-per-Share (PPS) reward system, where the pool manager can adjust both the share difficulty and the management fee. Using a simplified wealth model for miners, we explore how miners should allocate their computing resources among different mining pools, considering the trade-off between risk transfer to the manager and management fees.

</details>


### [3] [Human-Centered Threat Modeling in Practice: Lessons, Challenges, and Paths Forward](https://arxiv.org/abs/2511.13781)
*Warda Usman,Yixin Zou,Daniel Zappala*

Main category: cs.CR

TL;DR: HCTM 是一种非规定性的、由关系、价值观与制度共同塑造的演进性实践；通过23名研究者的访谈揭示其设计研究、威胁提取和价值取向的做法，以及情感压力与结构性障碍对落地的制约；未来需要共用基础设施、扩大多样性贡献的认可，并建立将研究结果转化为政策、设计和社会变革的机制。


<details>
  <summary>Details</summary>
Motivation: 了解研究者在实践中如何准备和参与以人为中心的威胁建模，梳理研究设计、威胁获取、价值观与长期目标的权衡。

Method: 采用半结构化访谈的定性研究，共23名参与HCTM研究的学者，进行主题分析以揭示做法、挑战与机会。

Result: 发现HCTM并非一个线性、可操作的规程，而是一组随时间演变的实践，受研究对象关系、学科背景与制度结构影响；以关怀、正义、自治等价值引导持续的基础工作与以参与者为中心的探询；同时面临情感压力、伦理困境和结构性障碍，这些因素阻碍研究发现的实际运营与社会影响的转化。

Conclusion: 通过搭建共用基础设施、提升对多样贡献的认可度、以及建立将发现转化为政策、设计与社会变革的机制，推动HCTM向更具系统性和社会性影响的方向发展。

Abstract: Human-centered threat modeling (HCTM) is an emerging area within security and privacy research that focuses on how people define and navigate threats in various social, cultural, and technological contexts. While researchers increasingly approach threat modeling from a human-centered perspective, little is known about how they prepare for and engage with HCTM in practice. In this work, we conduct 23 semi-structured interviews with researchers to examine the state of HCTM, including how researchers design studies, elicit threats, and navigate values, constraints, and long-term goals. We find that HCTM is not a prescriptive process but a set of evolving practices shaped by relationships with participants, disciplinary backgrounds, and institutional structures. Researchers approach threat modeling through sustained groundwork and participant-centered inquiry, guided by values such as care, justice, and autonomy. They also face challenges including emotional strain, ethical dilemmas, and structural barriers that complicate efforts to translate findings into real-world impact. We conclude by identifying opportunities to advance HCTM through shared infrastructure, broader recognition of diverse contributions, and stronger mechanisms for translating findings into policy, design, and societal change.

</details>


### [4] [Uncovering and Aligning Anomalous Attention Heads to Defend Against NLP Backdoor Attacks](https://arxiv.org/abs/2511.13789)
*Haotian Jin,Yang Li,Haihui Fan,Lin Shen,Xiangfang Li,Bo Li*

Main category: cs.CR

TL;DR: 提出一种基于注意力相似性的后门检测与对齐/微调方法，在不需要触发知识的前提下检测并抑制后门攻击，同时保持下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 后门攻击在大模型中日益普遍，触发器从固定化向动态/隐式演变，增加了检测难度；需要无需触发信息的检测与修复手段。

Method: 基于观察到模型在含触发样本时注意力头之间呈现异常高相似性的现象，提出一种注意力安全对齐并通过逐头微调修复被污染的注意力头的方法，达到检测与缓解后门的目的。

Result: 实验结果表明，该方法显著降低后门攻击的成功率，同时基本不影响模型在下游任务上的表现。

Conclusion: 该工作提供了一个触发器无关的后门检测与修复框架，提升了对动态触发的鲁棒性与安全性。

Abstract: Backdoor attacks pose a serious threat to the security of large language models (LLMs), causing them to exhibit anomalous behavior under specific trigger conditions. The design of backdoor triggers has evolved from fixed triggers to dynamic or implicit triggers. This increased flexibility in trigger design makes it challenging for defenders to identify their specific forms accurately. Most existing backdoor defense methods are limited to specific types of triggers or rely on an additional clean model for support. To address this issue, we propose a backdoor detection method based on attention similarity, enabling backdoor detection without prior knowledge of the trigger. Our study reveals that models subjected to backdoor attacks exhibit unusually high similarity among attention heads when exposed to triggers. Based on this observation, we propose an attention safety alignment approach combined with head-wise fine-tuning to rectify potentially contaminated attention heads, thereby effectively mitigating the impact of backdoor attacks. Extensive experimental results demonstrate that our method significantly reduces the success rate of backdoor attacks while preserving the model's performance on downstream tasks.

</details>


### [5] [Zipf-Gramming: Scaling Byte N-Grams Up to Production Sized Malware Corpora](https://arxiv.org/abs/2511.13808)
*Edward Raff,Ryan R. Curtin,Derek Everett,Robert J. Joyce,James Holt*

Main category: cs.CR

TL;DR: 通过 Zipf-Gramming 算法快速提取 top-k n-gram，提升 malware 检测的训练规模和 AUC。


<details>
  <summary>Details</summary>
Motivation: 现有方法在 terabytes 数据量时提取 top-k n-grams 极为慢，无法频繁更新模型，需在大小、速度、延迟之间取得平衡。

Method: 利用 Zipfian 分布特性，提出 Zipf-Gramming 的 top-k 提取算法；理论与经验分析表明误差小；实现与生产训练结合，获得 35 倍的加速。

Result: 最大可在新样本检测中提升 AUC 约 30%；能扩展到更大的训练集。

Conclusion: Zipf-Gramming 提供高效且准确的 top-k 选择，理论与工程实现的结合是关键，使模型能够定期更新并提升检测效果。

Abstract: A classifier using byte n-grams as features is the only approach we have found fast enough to meet requirements in size (sub 2 MB), speed (multiple GB/s), and latency (sub 10 ms) for deployment in numerous malware detection scenarios. However, we've consistently found that 6-8 grams achieve the best accuracy on our production deployments but have been unable to deploy regularly updated models due to the high cost of finding the top-k most frequent n-grams over terabytes of executable programs. Because the Zipfian distribution well models the distribution of n-grams, we exploit its properties to develop a new top-k n-gram extractor that is up to $35\times$ faster than the previous best alternative. Using our new Zipf-Gramming algorithm, we are able to scale up our production training set and obtain up to 30\% improvement in AUC at detecting new malware. We show theoretically and empirically that our approach will select the top-k items with little error and the interplay between theory and engineering required to achieve these results.

</details>


### [6] [The Battle of Metasurfaces: Understanding Security in Smart Radio Environments](https://arxiv.org/abs/2511.13939)
*Paul Staat,Christof Paar,Swarun Kumar*

Main category: cs.CR

TL;DR: 对称性RIS对抗场景的系统性研究，首次揭示相对 RIS 能力时的“军事化对抗”如何在时序、定位、算法策略与硬件尺度等因素影响下直接抵消对方效果，进而影响安全与隐私防护的物理层设计。


<details>
  <summary>Details</summary>
Motivation:  metasurfaces/ RIS 成为可编程无线传播的新前沿，安全问题日益突出；不同于以往单向应用研究，本工作聚焦于攻击方与防守方都具备可比 RIS 能力的对称场景，揭示其在无线安全中的新 threat 与 defense 的相互作用。

Method: 基于理论建模与现实环境实验，结合Wi-Fi 场景的多种案例，分析对称 RIS 的干扰、信道混淆与感知伪装等任务中的互动，考察时序、放置位置、算法策略、硬件尺度等变量的影响。

Result: 对称 RIS 相互作用可显著抵消或完全抵消对方效果；结果受到时序、放置、策略和硬件尺度的综合影响；在多种Wi-Fi 情境下（含无线干扰、感知/通信的信道混淆、感知欺骗）对方 RIS 能力能削弱甚至抵消先前的安全/隐私方案。

Conclusion: 提供了关于 RIS 与 RIS 之间上下文无关交互的首次系统研究，为设计在智能无线环境中更具鲁棒性和高保障性的物理层安全策略指明方向，同时提示在 RIS 部署与对抗中需要考虑对称性与协同防护。

Abstract: Metasurfaces, or Reconfigurable Intelligent Surfaces (RISs), have emerged as a transformative technology for next-generation wireless systems, enabling digitally controlled manipulation of electromagnetic wave propagation. By turning the traditionally passive radio environment into a smart, programmable medium, metasurfaces promise advances in communication and sensing. However, metasurfaces also present a new security frontier: both attackers and defenders can exploit them to alter wireless propagation for their own advantage. While prior security research has primarily explored unilateral metasurface applications - empowering either attackers or defenders - this work investigates symmetric scenarios, where both sides possess comparable metasurface capabilities. Using both theoretical modeling and real-world experiments, we analyze how competing metasurfaces interact for diverse objectives, including signal power and sensing perception. Thereby, we present the first systematic study of context-agnostic metasurface-to-metasurface interactions and their implications for wireless security. Our results reveal that the outcome of metasurface "battles" depends on an interplay of timing, placement, algorithmic strategy, and hardware scale. Across multiple case studies in Wi-Fi environments, including wireless jamming, channel obfuscation for sensing and communication, and sensing spoofing, we demonstrate that opposing metasurfaces can substantially or fully negate each other's effects. By undermining previously proposed security and privacy schemes, our findings open new opportunities for designing resilient and high-assurance physical-layer systems in smart radio environments.

</details>


### [7] [Privis: Towards Content-Aware Secure Volumetric Video Delivery](https://arxiv.org/abs/2511.14005)
*Kaiyuan Hu,Hong Kang,Yili Jin,Junhua Liu,Chengming Hu,Haolun Wu,Xue Liu*

Main category: cs.CR

TL;DR: Privis: a saliency-guided secure delivery framework for volumetric video that partitions assets, uses adaptive key rotation with lightweight authenticated encryption, and selective traffic shaping to balance confidentiality and latency.


<details>
  <summary>Details</summary>
Motivation: Volumetric video in XR requires real-time, latency-constrained secure transport. Existing encryption schemes borrowed from 2D video ignore heterogeneous privacy sensitivity of 3D geometry and the stringent motion-to-photon latency constraints.

Method: Introduce Privis, a saliency-guided transport framework that (i) partitions volumetric assets into independent units, (ii) applies lightweight authenticated encryption with adaptive key rotation, and (iii) employs selective traffic shaping. It defines a generalized transport-layer security architecture for volumetric media with adaptable protection mechanisms. A prototype implementation is explored and initial latency measurements are reported to illustrate feasibility and design tradeoffs.

Result: Preliminary feasibility demonstrated via a prototype and initial latency measurements, providing early empirical guidance on the tradeoffs between confidentiality and latency in saliency-aware secure volumetric delivery.

Conclusion: Privis offers an initial framework and prototype for content-aware, saliency-guided secure delivery of volumetric video, marking a step toward real-time, saliency-conditioned secure streaming and outlining open questions for future work.

Abstract: Volumetric video has emerged as a key paradigm in eXtended Reality (XR) and immersive multimedia because it enables highly interactive, spatially consistent 3D experiences. However, the transport-layer security for such 3D content remains largely unaddressed. Existing volumetric streaming pipelines inherit uniform encryption schemes from 2D video, overlooking the heterogeneous privacy sensitivity of different geometry and the strict motion-to-photon latency constraints of real-time XR.
  We take an initial step toward content-aware secure volumetric video delivery by introducing Privis, a saliency-guided transport framework that (i) partitions volumetric assets into independent units, (ii) applies lightweight authenticated encryption with adaptive key rotation, and (iii) employs selective traffic shaping to balance confidentiality and low latency. Privis specifies a generalized transport-layer security architecture for volumetric media, defining core abstractions and adaptive protection mechanisms. We further explore a prototype implementation and present initial latency measurements to illustrate feasibility and design tradeoffs, providing early empirical guidance toward future work on real-time, saliency-conditioned secure delivery.

</details>


### [8] [Location-Dependent Cryptosystem](https://arxiv.org/abs/2511.14032)
*Kunal Mukherjee*

Main category: cs.CR

TL;DR: 提出一种基于定位的加密系统，通过超宽带（UWB）的精确时序将密钥隐式编码在传输时间戳中，只有在授权物理区域内的接收者才能重建密钥并解密；给出原型实现与评估。


<details>
  <summary>Details</summary>
Motivation: 解决数字内容分发中的版权保护难题：传统加密在密钥泄露后难以阻止在其他地点解密，需将解密能力与物理位置绑定，避免电子/物理传输密钥的风险。

Method: 将 AES 密钥经 SHA-256 哈希后映射到预定的传输时间戳集合，通过超宽带数据包的精确时序传输；在 JMTK 协议下进行时间槽观测，授权区域内的接收端能重建密钥并解密，未授权区域观察到的密钥不正确。实现一个完整的原型，采用音频数据的加密与传输。

Result: 系统省略了电子或物理传输解密口令的需求，避免被窃听者恢复解密钥，且对合法用户提供了非弱化的空间容忍度；原型测试显示在授权区域内能正确解密，区域外无法解密。

Conclusion: 将解密能力绑定到物理位置的定位型加密具有实际可行性，能够缓解密钥分发带来的风险，但依赖高精度时序硬件与可靠的近似空间容忍，对部署环境有要求。

Abstract: Digital content distribution and proprietary research-driven industries face persistent risks from intellectual property theft and unauthorized redistribution. Conventional encryption schemes such as AES, TDES, ECC, and ElGamal provide strong cryptographic guarantees, but they remain fundamentally agnostic to where decryption takes place.In practice, this means that once a decryption key is leaked or intercepted, any adversary can misuse the key to decrypt the protected content from any location. We present a location-dependent cryptosystem in which the decryption key is not transmitted as human- or machine-readable data, but implicitly encoded in precise time-of-flight differences of ultra-wideband (UWB) data transmission packets. The system leverages precise timing hardware and a custom JMTK protocol to map a SHA-256 hashed AES key onto scheduled transmission timestamps. Only receivers located within a predefined spatial region can observe the packet timings that align with the intended "time slot" pattern, enabling them to reconstruct the key and decrypt the secret. Receivers outside the authorized region observe incorrect keys. We implement a complete prototype that encrypts and transmits audio data using our cryptosystem, and only when the receiver is within the authorized data, they are able to decrypt the data. Our evaluation demonstrates that the system (i) removes the need to share decryption passwords electronically or physically, (ii) ensures the decryption key cannot be recovered by the eavesdropper, and (iii) provides a non-trivial spatial tolerance for legitimate users.

</details>


### [9] [GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards](https://arxiv.org/abs/2511.14045)
*Yule Liu,Heyi Zhang,Jinyi Zheng,Zhen Sun,Zifan Peng,Tianshuo Cong,Yilong Yang,Xinlei He,Zhuo Ma*

Main category: cs.CR

TL;DR: 提出DIBA，一种面向RLVR的成员推断攻击，基于行为差异而非记忆，能在多场景中实现高AUC（约0.8）与TPR@0.1%FPR的显著提升，并对常见防御具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: RLVR训练的在策略自生成数据上进行在线优化，导致隐私泄露的新型模式；现有MIA研究多聚焦记忆化推断，缺乏对RLVR情境的审计框架，因此需要一种专门针对行为变化的攻击来评估隐私风险。

Method: 提出DIBA（Divergence-in-Behavior Attack），通过衡量两条行为轴来推断成员身份：优势端改进（如正确性提升）与对数端分歧（如策略漂移）。该框架可在有监督/无监督、分布内外、跨数据集、跨算法、黑盒及扩展到视觉-语言模型等多种 setting 下使用，并对中等防御具有鲁棒性。

Result: 实验显示DIBA显著优于基线，约0.8的AUC，以及TPR@0.1%FPR提升一个量级以上；在多场景（包括分布内、跨数据集、跨算法、黑盒、Vision-Language模型扩展）均展现优势。

Conclusion: 首次系统揭示RLVR场景下的隐私脆弱性，即使缺乏显式监督，也可通过训练行为轨迹推断训练数据是否被用于微调，揭示了对RLVR的隐私风险需要专门评估与防护。

Abstract: Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.

</details>


### [10] [Dynamic Black-box Backdoor Attacks on IoT Sensory Data](https://arxiv.org/abs/2511.14074)
*Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.CR

TL;DR: 对传感器数据基的识别系统的黑箱对抗攻击提出一种动态触发生成技术，能以最小扰动对多数据集和分类模型进行攻击，比较其它中毒/后门技术并讨论防御的影响。


<details>
  <summary>Details</summary>
Motivation: 现代可穿戴设备中的IMU传感器广泛使用，深度学习分类模型易受对抗攻击和后门攻击影响，需理解和评估新型触发策略及其防御。

Method: 提出动态触发生成技术用于对传感器数据基IoT系统的黑箱对抗攻击；在多数据集和模型上进行实验评估，比较其在性能与隐蔽性方面与其他中毒技术的差异，并探讨可用的防御机制及其效果。

Result: 攻击在各种数据集和分类模型上以最小扰动取得成功；在隐蔽性与性能方面优于或等同于其他中毒技术；给出防御机制对攻击有效性的影响分析。

Conclusion: 该动态触发生成技术证实了传感器数据基IoT系统在黑箱对抗攻击上的脆弱性，提示需要加强鲁棒性防御并为未来研究提供方向，如扩展数据集、实地场景评估和防御策略的综合评估。

Abstract: Sensor data-based recognition systems are widely used in various applications, such as gait-based authentication and human activity recognition (HAR). Modern wearable and smart devices feature various built-in Inertial Measurement Unit (IMU) sensors, and such sensor-based measurements can be fed to a machine learning-based model to train and classify human activities. While deep learning-based models have proven successful in classifying human activity and gestures, they pose various security risks. In our paper, we discuss a novel dynamic trigger-generation technique for performing black-box adversarial attacks on sensor data-based IoT systems. Our empirical analysis shows that the attack is successful on various datasets and classifier models with minimal perturbation on the input data. We also provide a detailed comparative analysis of performance and stealthiness to various other poisoning techniques found in backdoor attacks. We also discuss some adversarial defense mechanisms and their impact on the effectiveness of our trigger-generation technique.

</details>


### [11] [Resolving Availability and Run-time Integrity Conflicts in Real-Time Embedded Systems](https://arxiv.org/abs/2511.14088)
*Adam Caulfield,Muhammad Wasif Kamran,N. Asokan*

Main category: cs.CR

TL;DR: 提出 PAIR，一种在实时系统中实现运行时完整性与可用性之间权衡的硬件化监控方法，通过维护可用性区域 AR，在任务违规时通过不可屏蔽中断将其终止，其余任务保持可用性；对低端 MCU 仅增加约 2.3% 的内存与硬件开销。


<details>
  <summary>Details</summary>
Motivation: 实时系统的运行时完整性监控通常增加执行开销；面临在保证系统安全和满足 deadlines 之间的两难选择。需要一个介于两端的方案，实现选择性中止违规任务以维持可用性。

Method: 提出 PAIR，基于硬件实现的运行时完整性监控，维护一个 AR，若任务违反则触发非屏蔽中断终止该任务，允许 AR 中的其他任务继续执行；对正在执行的任务无额外运行时开销，且对低端 MCU 友好，内存和硬件开销约 +2.3%。

Result: 实现机制能够保证只有违规任务被阻止执行，同时维持系统的可用性，且引入的硬件/内存开销较低。

Conclusion: 为实时系统提供一个在安全性与可用性之间的折中方案，易于与 RTOS 集成，适用于资源受限的 MCU。

Abstract: Run-time integrity enforcement in real-time systems presents a fundamental conflict with availability. Existing approaches in real- time systems primarily focus on minimizing the execution-time overhead of monitoring. After a violation is detected, prior works face a trade-off: (1) prioritize availability and allow a compromised system to continue to ensure applications meet their deadlines, or (2) prioritize security by generating a fault to abort all execution. In this work, we propose PAIR, an approach that offers a middle ground between the stark extremes of this trade-off. PAIR monitors real-time tasks for run-time integrity violations and maintains an Availability Region (AR) of all tasks that are safe to continue. When a task causes a violation, PAIR triggers a non-maskable interrupt to kill the task and continue executing a non-violating task within AR. Thus, PAIR ensures only violating tasks are prevented from execution, while granting availability to remaining tasks. With its hardware approach, PAIR does not cause any run-time overhead to the executing tasks, integrates with real-time operating systems (RTOSs), and is affordable to low-end microcontroller units (MCUs) by incurring +2.3% overhead in memory and hardware usage.

</details>


### [12] [A Fuzzy Logic-Based Cryptographic Framework For Real-Time Dynamic Key Generation For Enhanced Data Encryption](https://arxiv.org/abs/2511.14132)
*Kavya Bhand,Payal Khubchandani,Jyoti Khubchandani*

Main category: cs.CR

TL;DR: 基于模糊逻辑的动态密钥生成框架，结合系统熵与硬件信任，通过 TPM 封装后用于 AES-GCM 实现自适应加密。


<details>
  <summary>Details</summary>
Motivation: 解决静态密钥在暴力攻击、密钥泄露和未授权访问中的脆弱性，提供在高安全、零信任和云环境中的实时、可扩展的密钥生成方案。

Method: 使用模糊推理系统评估系统参数（CPU利用率、进程数量、时间戳变化等），基于模糊规则分配熵等级；将此熵与硬件随机性融合，并通过 TPM 封装，形成密钥并嵌入 AES-GCM 以保证数据的机密性和完整性。

Result: 提出设计与实现思路，强调可扩展性和对高安全环境的适用性，但未在摘要中给出实验结果，需后续评估。

Conclusion: 该框架为自适应加密提供可扩展方案，结合硬件信任与系统熵提升密钥安全性，未来工作包括性能评估、密钥管理策略以及在实际部署中的兼容性研究。

Abstract: With the ever-growing demand for cybersecurity, static key encryption mechanisms are increasingly vulnerable to adversarial attacks due to their deterministic and non-adaptive nature. Brute-force attacks, key compromise, and unauthorized access have become highly common cyber threats. This research presents a novel fuzzy logic-based cryptographic framework that dynamically generates encryption keys in real-time by accessing system-level entropy and hardware-bound trust. The proposed system leverages a Fuzzy Inference System (FIS) to evaluate system parameters that include CPU utilization, process count, and timestamp variation. It assigns entropy level based on linguistically defined fuzzy rules which are fused with hardware-generated randomness and then securely sealed using a Trusted Platform Module (TPM). The sealed key is incorporated in an AES-GCM encryption scheme to ensure both confidentiality and integrity of the data. This system introduces a scalable solution for adaptive encryption in high-assurance computing, zero-trust environments, and cloud-based infrastructure.

</details>


### [13] [Beyond Fixed and Dynamic Prompts: Embedded Jailbreak Templates for Advancing LLM Security](https://arxiv.org/abs/2511.14140)
*Hajun Kim,Hyunsik Na,Daeseon Choi*

Main category: cs.CR

TL;DR: 提出嵌入式越狱模板和渐进式提示工程，提升对LLMs的红队演练与政策回归测试的真实性与可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前越狱研究受限于固定模板或由LLM生成整套模板，导致意图不清晰且可复现实践性不足。需要在保持模板结构的同时，将有害查询自然嵌入上下文，以更贴近真实使用情景与攻击意图。

Method: 提出 Embedded Jailbreak Template，保持模板结构并在其中嵌入有害查询；采用渐进式提示工程确保模板质量与一致性；制定标准化的生成与评估 protocol。

Result: 提供一个更贴近现实使用情境的基准，能够真实体现有害意图，有助于红队演练与策略回归测试的有效性。

Conclusion: 该工作丰富了对LLMs安全研究的基准与评估方法，促进更稳健的模型部署和防御策略的发展。

Abstract: As the use of large language models (LLMs) continues to expand, ensuring their safety and robustness has become a critical challenge. In particular, jailbreak attacks that bypass built-in safety mechanisms are increasingly recognized as a tangible threat across industries, driving the need for diverse templates to support red-teaming efforts and strengthen defensive techniques. However, current approaches predominantly rely on two limited strategies: (i) substituting harmful queries into fixed templates, and (ii) having the LLM generate entire templates, which often compromises intent clarity and reproductibility. To address this gap, this paper introduces the Embedded Jailbreak Template, which preserves the structure of existing templates while naturally embedding harmful queries within their context. We further propose a progressive prompt-engineering methodology to ensure template quality and consistency, alongside standardized protocols for generation and evaluation. Together, these contributions provide a benchmark that more accurately reflects real-world usage scenarios and harmful intent, facilitating its application in red-teaming and policy regression testing.

</details>


### [14] [Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion](https://arxiv.org/abs/2511.14301)
*Eric Xue,Ruiyi Zhang,Zijun Zhang,Pengtao Xie*

Main category: cs.CR

TL;DR: SteganoBackdoor is a sequence-level, semantic-backdoor attack that uses natural-language steganography and gradient-guided data optimization to convert semantic triggers (names/entities) into steganographic carriers embedding a backdoor payload, achieving >99% success with much less poisoning and strong defense evasion.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor research focuses on stylized artifacts or token-level triggers, leaving a realistic threat model where semantic triggers (names/entities) can influence outputs. There is a gap between practical defenses and semantic backdoors that this work aims to address by exposing and mitigating a blind spot in current defenses.

Method: Apply gradient-guided data optimization to transform semantic trigger seeds into steganographic carriers. The carriers embed a high backdoor payload while remaining fluent and lacking representational resemblance to the trigger, enabling covert activation. Evaluate across diverse NLP tasks and defenses to demonstrate stealth and efficacy.

Result: Achieves over 99% attack success with an order-of-magnitude lower data-poisoning rate than prior approaches and shows strong evasion against a comprehensive suite of data-level defenses.

Conclusion: Reveals a practical and covert backdoor threat aligned with real-world semantic triggers, underscoring the need for adversarial data defenses and threat modeling that account for semantic backdoors in deployed NLP systems.

Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.

</details>


### [15] [Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection](https://arxiv.org/abs/2511.14422)
*Zhengchunmin Dai,Jiaxiong Tang,Peng Sun,Honglong Chen,Liantao Wu*

Main category: cs.CR

TL;DR: Proposes Sigil, a mandatory watermarking framework for capability-limited servers in Split Federated Learning; embeds a statistical activation-space constraint via gradient injection and uses adaptive gradient clipping to ensure stealthy, mandatory watermarking without data access; demonstrates fidelity, robustness, and stealthiness across datasets and models.


<details>
  <summary>Details</summary>
Motivation: In Split Federated Learning and its variants, servers have limited access to model parameters and data, increasing risk of model theft by malicious clients. Existing client-cooperation-dependent watermarking or server-side approaches that require data access are infeasible in this setting.

Method: Define the watermark as a statistical constraint on the server-visible activation space. Embed the watermark into client models through gradient injection without requiring knowledge of the training data. Introduce adaptive gradient clipping to keep the watermark mandatory and stealthy, resisting gradient anomaly detection and adaptive subspace removal attacks.

Result: Extensive experiments across multiple datasets and models show Sigil achieves high fidelity of watermark, robustness against attacks, and stealthiness.

Conclusion: Sigil offers a practical, data-free, mandatory watermarking framework for protecting intellectual property of capability-limited servers in Split Federated Learning, addressing both reliability and stealthiness in adversarial settings.

Abstract: In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.

</details>


### [16] [SecureSign: Bridging Security and UX in Mobile Web3 through Emulated EIP-6963 Sandboxing](https://arxiv.org/abs/2511.14611)
*Charles Cheng Ji,Brandon Kong*

Main category: cs.CR

TL;DR: 提出 SecureSign，一种基于PWA的移动端Web3钱包架构，通过EIP-6963 provider沙箱化，将dApp在可信父应用的iframe中执行，兼顾高安全性与原生移动能力，且无需改动现有应用代码。


<details>
  <summary>Details</summary>
Motivation: 解决移动端Web3的极低留存率（<5%）和高获取成本（$500-$1,000/留存用户）的矛盾。现有嵌入式钱包易受点击劫持，应用钱包尽管安全却因下载和上下文切换成本导致低留存。

Method: 将桌面浏览器扩展的安全模型移植到移动端，采用EIP-6963提供程序沙箱化，将dApp放入受信任父应用中的iframe内执行，以实现点击劫持免疫、交易完整性，同时支持原生移动能力（推送通知、主屏安装、零上下文切换）。提供即插即用的SDK，无需改动现有代码库。

Result: 在攻击表面上声称对点击劫持、覆盖和 skim 等攻击具有免疫力，保持跨dApp的钱包互操作性；摘要中未给出实证数据，但提出显著提升留存与获取成本的潜力。

Conclusion: SecureSign通过在移动端复制桌面扩展的安全性并结合原生移动能力，解决移动端Web3的安全与易用性矛盾，具备提升留存与降低获取成本的潜力；未来工作包括更全面的实证评估与大规模部署的可行性研究。

Abstract: Mobile Web3 faces catastrophic retention (< 5%) yielding effective acquisition costs of \$500 - \$1,000 per retained user. Existing solutions force an impossible tradeoff: embedded wallets achieve moderate usability but suffer inherent click-jacking vulnerabilities; app wallets maintain security at the cost of 2 - 3% retention due to download friction and context-switching penalties. We present SecureSign, a PWA-based architecture that adapts desktop browser extension security to mobile via EIP-6963 provider sandboxing. SecureSign isolates dApp execution in iframes within a trusted parent application, achieving click-jacking immunity and transaction integrity while enabling native mobile capabilities (push notifications, home screen installation, zero context-switching). Our drop-in SDK requires no codebase changes for existing Web3 applications. Threat model analysis demonstrates immunity to click-jacking, overlay, and skimming attacks while maintaining wallet interoperability across dApps.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [17] [Benchmarking OpenWiFiSync on ESP32: Towards Cost-Effective Wireless Time Synchronization](https://arxiv.org/abs/2511.14457)
*Michael Gundall,Jan Herbst,Robin Müller,Hans D. Schotten*

Main category: cs.NI

TL;DR: 通过参考广播基础设施同步（RBIS）在无线环境中实现高精度时间同步，实现了 +/-30 微秒的同步精度，使用低成本硬件（ESP32）与商业 Wi-Fi 接入点进行测试，并以 OpenWifiSync 项目以 GPLv3 许可证开源。


<details>
  <summary>Details</summary>
Motivation: 解决无线环境中传统有线时钟同步协议在性能方面的不足，满足工业4.0对协调任务与高精度时间戳的需求，同时利用无线通信的广播特性实现非侵入式、标准兼容的同步。

Method: 采用 RBIS 协议，基于无线广播的特性进行时间同步。使用低成本测试台（ESP32 模块）和商用 Wi-Fi 访问点进行实现与验证，并将实现开源发布（GNU GPLv3，OpenWifiSync，GitHub）。

Result: 在能耗友好且成本较低的硬件上实现了 +/-30 微秒的同步精度，证明该方法适用于广泛的应用场景。

Conclusion: RBIS + 开源实现为无线时间同步提供了一种经济高效、易部署的解决方案，满足工业4.0 的多样化时序需求。

Abstract: Wireless time synchronization of mobile devices is a key enabler for numerous Industry 4.0 applications, such as coordinated and synchronized tasks or the generation of high-precision timestamps for machine learning or artificial intelligence algorithms. Traditional wireline clock synchronization protocols, however, cannot achieve the performance in wireless environments without significant modifications. To address this challenge, we make use of the Reference Broadcast Infrastructure Synchronization protocol, which leverages the broadcast nature of wireless communications and remains both non-invasive and standard-compliant. We implement and validate this protocol on a low-cost testbed using ESP32 modules and a commercial Wi-Fi access point. To support further research and development, we release our implementation as open-source software under the GNU General Public License Version 3 license via the OpenWifiSync project on GitHub.
  Our results demonstrate that synchronization accuracies within +/-30 microseconds are achievable using energy-efficient and affordable hardware, making this approach suitable for a wide range of use cases.

</details>


### [18] [From Topology to Behavioral Semantics: Enhancing BGP Security by Understanding BGP's Language with LLMs](https://arxiv.org/abs/2511.14467)
*Heng Zhao,Ruoyu Wang,Tianhang Zheng,Qi Li,Bo Lv,Yuyi Wang,Wenliang Du*

Main category: cs.NI

TL;DR: BGPShield: an anomaly detection framework using LLM embeddings to capture semantic behavior of ASes for BGP anomaly detection; achieves 100% anomaly detection with FDR<5%, fast unseen AS embedding, and outperforms BEAM.


<details>
  <summary>Details</summary>
Motivation: To overcome MDL-based detection methods that over-rely on topological features and suffer from poor precision, limited generalizability, and high retraining costs; aim to incorporate semantic characteristics of Autonomous Systems (ASes) and improve scalability.

Method: Develop BGPShield with segment-wise aggregation to convert AS descriptions into LLM representations; apply a lightweight contrastive reduction network to obtain semantic-consistent embeddings; use AR-DTW to align semantic distances and reveal behavioral inconsistencies; evaluate on 16 real-world datasets.

Result: BGPShield detects 100% of verified anomalies with a false discovery rate below 5%; LLMs used were released before evaluation, supporting generalizability; unseen AS representations can be constructed within 1 second, outperforming BEAM which requires about 65 hours of retraining.

Conclusion: Semantic-aware, LLM-based representations of AS behavior improve BGP anomaly detection, offering fast adaptation to unseen ASes and better scalability, with strong empirical results across real-world datasets.

Abstract: The trust-based nature of Border Gateway Protocol (BGP) makes it vulnerable to disruptions like prefix hijacking and misconfigurations, threatening routing stability. Traditional detection relies on manual inspection with limited scalability. Machine/Deep Learning (M/DL) approaches automate detection but suffer from suboptimal precision, limited generalizability, and high retraining costs. This is because existing methods focus on topological structures rather than comprehensive semantic characteristics of Autonomous Systems (ASes), often misinterpreting functionally similar but topologically distant ASes.
  To address this, we propose BGPShield, an anomaly detection framework built on LLM embeddings that captures the Behavior Portrait and Routing Policy Rationale of each AS beyond topology, such as operational scale and global role. We propose a segment-wise aggregation scheme to transform AS descriptions into LLM representations without information loss, and a lightweight contrastive reduction network to compress them into a semantic-consistent version. Using these representations, our AR-DTW algorithm aligns and accumulates semantic distances to reveal behavioral inconsistencies. Evaluated on 16 real-world datasets, BGPShield detects 100% of verified anomalies with a false discovery rate below 5%. Notably, the employed LLMs were released prior to evaluation events, verifying generalizability. Furthermore, BGPShield constructs representations for unseen ASes within one second, significantly outperforming BEAM which demands costly retraining (averaging 65 hours).

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [19] [Deep reinforcement learning-based spacecraft attitude control with pointing keep-out constraint](https://arxiv.org/abs/2511.13746)
*Juntang Yang,Mohamed Khalil Ben-Larbi*

Main category: eess.SY

TL;DR: 用 Soft Actor-Critic (SAC) 的深度强化学习，结合对抗约束态表示与课程学习，在单一指向保持区的太空飞行器姿态控制中实现受约束的指向控制；在仿真中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在太空飞行器姿态控制中，需同时实现高精度跟踪和避开禁止指向区的约束，且控制输入为连续量，传统控制方法在处理复杂约束与高维状态-动作空间时有难度。深度强化学习，尤其是 SAC，能在无模型或弱模型的情形下学习策略；通过在状态中显式编码约束区域并在奖励中强化约束满足，可实现受约束的姿态控制；级进学习（课程学习）有助于稳定训练与提升收敛性。

Method: 采用 Soft Actor-Critic (SAC) 进行连续状态-动作控制；设计状态表示，使其显式包含姿态约束区的紧凑表示；将奖励函数设计为在完成控制目标的同时强制满足姿态约束；采用课程学习对智能体进行分阶段训练以提高收敛性与鲁棒性；在单一保持区的仿真环境中验证。

Result: 仿真结果表明，该基于 DRL 的方法在实现指向控制目标的同时，能有效满足姿态约束。

Conclusion: 在包含单一指向保持区的太空飞行器姿态控制场景中，基于 SAC 的 DRL 方法，结合约束感知的状态表示与课程学习，是一种可行且有效的无模型/低依赖的控制策略。

Abstract: This paper implements deep reinforcement learning (DRL) for spacecraft reorientation control with a single pointing keep-out zone. The Soft Actor-Critic (SAC) algorithm is adopted to handle continuous state and action space. A new state representation is designed to explicitly include a compact representation of the attitude constraint zone. The reward function is formulated to achieve the control objective while enforcing the attitude constraint. A curriculum learning approach is used for the agent training. Simulation results demonstrate the effectiveness of the proposed DRL-based method for spacecraft pointing-constrained attitude control.

</details>


### [20] [Game-theoretic Decentralized Coordination for Airspace Sector Overload Mitigation](https://arxiv.org/abs/2511.13770)
*Jaehan Im,Daniel Delahaye,David Fridovich-Keil,Ufuk Topcu*

Main category: eess.SY

TL;DR: 提出一种基于最佳响应动态的去中心化空域交通管理机制，允许各区域独立调整航班起飞时间以降低自身拥堵，并引入可调协作系数以衡量对其他区域的让步。通过证明存在潜在博弈结构，最佳响应动作为收敛到纯纳什均衡提供了充分条件，且在给定条件下过载为零解可使潜在函数达到全局最小。数值实验基于欧洲24小时航班数据表明，即使协作极低，算法也能显著降低拥堵，且具可扩展性且与集中求解器的解质量相当。


<details>
  <summary>Details</summary>
Motivation: 在去中心化的空域交通管理中，现实常态下各区域独立优化、缺乏充分协作会导致系统整体拥堵与性能下降。本研究寻求在自利行为驱动的前提下，仍可实现对系统性能的提升。

Method: 建立以最佳响应为基础的自利行为模型，加入一个可调的协作系数来描述各区域在降低自身拥堵的同时对其他区域的让步程度；证明该机制具备潜在博弈结构，最佳响应迭代在温和条件下收敛到纯纳什均衡；给出一个充分条件，确保不存在过载时的解同时是潜在函数的全局最小点。通过对欧洲航班数据的数值实验评估算法在最小协作下的表现。

Result: 实验结果显示，即使仅有有限程度的跨区域合作，所提算法也能显著降低区域拥堵，并保持良好的可扩展性；其解质量与集中求解器相当，接近最优。

Conclusion: 该工作证明了在去中心化的空域交通管理框架内，基于自利行为的最佳响应机制仍可实现接近全局最优的系统性能，且通过潜在博弈结构确保收敛性，为设计具备局部自治与全局性能折中机制的空域管理提供理论与实证支持。

Abstract: Decentralized air traffic management systems offer a scalable alternative to centralized control, but often assume high levels of cooperation. In practice, such assumptions frequently break down since airspace sectors operate independently and prioritize local objectives. We address the problem of sector overload in decentralized air traffic management by proposing a mechanism that models self-interested behaviors based on best response dynamics. Each sector adjusts the departure times of flights under its control to reduce its own congestion, without any shared decision making. A tunable cooperativeness factor models the degree to which each sector is willing to reduce overload in other sectors. We prove that the proposed mechanism satisfies a potential game structure, ensuring that best response dynamics converge to a pure Nash equilibrium, under a mild restriction. In addition, we identify a sufficient condition under which an overload-free solution corresponds to a global minimizer of the potential function. Numerical experiments using 24 hours of European flight data demonstrate that the proposed algorithm substantially reduces overload even with only minimal cooperation between sectors, while maintaining scalability and matching the solution quality of centralized solvers.

</details>


### [21] [Quantifying Distribution Shift in Traffic Signal Control with Histogram-Based GEH Distance](https://arxiv.org/abs/2511.13785)
*Federico Taschin,Ozan K. Tonguz*

Main category: eess.SY

TL;DR: 提出一种以需求直方图与GEH距离衡量分布偏移的无策略依赖方法，可用于预测分布偏移对交通信号控制性能的影响，尤其对学习型控制具有显著解释力。


<details>
  <summary>Details</summary>
Motivation: 分布偏移会导致交通信号控制性能下降，需一种可解释、与交通工程统计量兼容的量化方法来预测并监控这种下降。

Method: 将交通场景表示为需求直方图，使用GEH距离量化分布差异，方法独立于控制策略，易于解释。

Result: 在20个仿真场景中，使用NEMA电控与FRAP++强化学习控制器进行评估。场景距离越大，与旅行时间增加、吞吐量下降相关，且对学习型控制具有更强的解释力；该方法在预测分布偏移引发的性能下降方面优于先前技术。

Conclusion: 该框架可用于基准测试、训练 regime 设计与自适应交通信号控制的监控，帮助在分布偏移条件下更好地预测与缓解性能下降。

Abstract: Traffic signal control algorithms are vulnerable to distribution shift, where performance degrades under traffic conditions that differ from those seen during design or training. This paper introduces a principled approach to quantify distribution shift by representing traffic scenarios as demand histograms and comparing them with a GEH-based distance function. The method is policy-independent, interpretable, and leverages a widely used traffic engineering statistic. We validate the approach on 20 simulated scenarios using both a NEMA actuated controller and a reinforcement learning controller (FRAP++). Results show that larger scenario distances consistently correspond to increased travel time and reduced throughput, with particularly strong explanatory power for learning-based control. Overall, this method can predict performance degradation under distribution shift better than previously published techniques. These findings highlight the utility of the proposed framework for benchmarking, training regime design, and monitoring in adaptive traffic signal control.

</details>


### [22] [Data-Driven EV Charging Load Profile Estimation and Typical EV Daily Load Dataset Generation](https://arxiv.org/abs/2511.13861)
*Linhan Fang,Jesus Silva-Rodriguez,Xingpeng Li*

Main category: eess.SY

TL;DR: 使用两种数据驱动方法基于居住用户表计数据估计家庭充电轮廓；基于最小二乘法的平均充电速率提取和起止时间推断；以及基于KDE的概率充电模型；两种方法均得到夜间充电的u形日充电曲线，便于公用事业进行需求预测与规划。


<details>
  <summary>Details</summary>
Motivation: 随着电动车广泛普及，分配电网面临局部负荷集中、充电行为随机性和数据不足等挑战，需要可扩展、数据驱动的家庭充电轮廓估计方法以支持网规划和需求预测。

Method: 方法一：通过对比聚合的EV与非EV表计数据，使用最小二乘估计提取平均充电速率并推断充电的起始和结束时间。方法二：从表计曲线中分离EV负荷，再对EV负荷应用核密度估计(KDE)以建立概率充电模型。

Result: 两种方法均生成明显的“u形”日充电曲线，充电主要发生在夜间。经验证的轮廓可为公用事业提供可扩展工具，用于更好地预测EV驱动的需求增加并支持主动网规划。

Conclusion: 数据驱动的家庭充电轮廓可帮助公用事业在不同情景下进行需求预测与容量规划，促进EV对电网影响的前瞻性管理。

Abstract: Widespread electric vehicle (EV) adoption introduces new challenges for distribution grids due to large, localized load increases, stochastic charging behavior, and limited data availability. This paper proposes two data-driven methods to estimate residential EV charging profiles using real-world customer meter data from CenterPoint Energy serving the Houston area. The first approach applies a least-squares estimation to extract average charging rates by comparing aggregated EV and non-EV meter data, enabling a statistical method for starting and ending charge times. The second method isolates EV load from meter profiles and applies a kernel density estimation (KDE) to develop a probabilistic charging model. Both methods produce a distinct "u-shaped" daily charging profile, with most charging occurring overnight. The validated profiles offer a scalable tool for utilities to better anticipate EV-driven demand increases and support proactive grid planning.

</details>


### [23] [Just Few States are Enough: Randomized Sparse Feedback for Stability of Dynamical Systems](https://arxiv.org/abs/2511.13870)
*Zaid Hadach,Hajar El Hammouti,El Houcine Bergou,Adnane Saoud*

Main category: eess.SY

TL;DR: 提出一种在随机稀疏测量条件下的线性系统稳定性设计框架，结合AMSS稳定性和LMIs，联合设计反馈增益矩阵与测量稀疏策略，且允许概率随状态分量变化。数值结果显示在仅需0.3%状态分量测量的情况下也能达到接近全状态反馈的性能。


<details>
  <summary>Details</summary>
Motivation: 传统最优控制和鲁棒控制常要求对全部状态有观测或输出测量。本文旨在在每一步仅获得随机选取的状态分量的情况下仍实现闭环系统的平均平方渐近稳定性，降低观测量与通信开销。

Method: 将随机稀疏测量引入闭环系统，通过线性矩阵不等式（LMI）构造一个联合优化问题，设计一个稳健/稳定化的反馈增益矩阵K以及一个随机稀疏策略（包括稀疏概率向量），以在期望意义上最小化测量维度同时保证AMSS。进而推广至稀疏概率随状态分量异质性的情形，并给出一种计算稀疏参数向量及对应K的算法。

Result: 给出系统在何种动态特性下可实现稀疏化以及满足AMSS的充要条件；提出基于LMI的联合设计算法；扩展到可变稀疏概率场景；通过数值仿真，显示在0.3%测量的情况下可获得接近全状态反馈的性能。

Conclusion: 这是首次系统研究在仅依赖随机选择的状态观测下的控制系统稳定性，为减少传感与通信开销提供了一种可行的设计框架，并给出可执行的计算程序来得到稀疏参数向量与对应的反馈增益。

Abstract: While classical control theory assumes that the controller has access to measurements of the entire state (or output) at every time instant, this paper investigates a setting where the feedback controller can only access a randomly selected subset of the state vector at each time step. Due to the random sparsification that selects only a subset of the state components at each step, we analyze the stability of the closed-loop system in terms of Asymptotic Mean-Square Stability (AMSS), which ensures that the system state converges to zero in the mean-square sense. We consider the problem of designing both a feedback gain matrix and a measurement sparsification strategy that minimizes the number of state components required for feedback, while ensuring AMSS of the closed-loop system. Interestingly, (1) we provide conditions on the dynamics of the system under which it is possible to find a sparsification strategy, and (2) we propose a Linear Matrix Inequality (LMI) based algorithm that jointly computes a stabilizing gain matrix, and a randomized sparsification strategy that minimizes the expected number of measured state coordinates while preserving the AMSS. Our approach is then extended to the case where the sparsification probabilities vary across the state components. Based on these theoretical findings, we propose an algorithmic procedure to compute the vector of sparsification parameters, along with the corresponding feedback gain matrix. To the best of our knowledge, this is the first study to investigate the stability properties of control systems that rely solely on randomly selected state measurements. Numerical simulations demonstrate that, in some settings, the system achieves comparable performance to full-state feedback while requiring measurements from only $0.3\%$ of the state coordinates.

</details>


### [24] [Dynamic state estimation of hybrid systems: Inverters that switch between grid-following and grid-forming control schemes](https://arxiv.org/abs/2511.13872)
*Bukunmi G. Odunlami,Marcos Netto*

Main category: eess.SY

TL;DR: Hybrid automata model with mode-switching inverters, embedded in an extended Kalman filter for state estimation; shows stable dynamics and improved estimation across switching.


<details>
  <summary>Details</summary>
Motivation: Address stability and estimation challenges in inverter fleets that switch between grid-following and grid-forming modes, especially near mode transitions.

Method: Model inverters as hybrid automata with guard conditions on voltage/frequency and reset maps for phase, frequency, and droop references; integrate the hybrid model into an extended Kalman filter to evaluate estimation performance during mode switching.

Result: The hybrid framework yields stable, well-behaved dynamics and improved state estimation, particularly at switching instants, compared with purely smooth continuous models.

Conclusion: A hybrid modeling plus EKF framework provides robust performance for mode-switching in grid interfacing inverters and enhances estimation accuracy during transitions.

Abstract: This paper develops a hybrid system modeling framework for inverters that switch between grid-following and grid-forming control schemes. In particular, such inverters are modeled as hybrid automata with guard conditions on voltage and frequency, and reset maps that maintain consistent phase, frequency, and droop references during mode transitions. The hybrid model is embedded within an extended Kalman filter to assess estimation performance under explicit mode switching. Results show that the proposed framework ensures stable, well-behaved dynamics and improves state estimation, especially near switching instants, compared with smooth continuous models.

</details>


### [25] [L-Functions Certify Set Attractivity for Discrete-Time Uncertain Nonlinear Switched Systems](https://arxiv.org/abs/2511.13906)
*Alejandro Anderson,Esteban A. Hernandez-Vargas,Giulia Giordano*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce the class of L-functions to certify the attractivity of sets for uncertain nonlinear switched systems in discrete time. The existence of an L-function associated with a set guarantees the robust local attractivity of that set under the system dynamics. We propose a constructive method for obtaining piecewise-continuous L-functions based on contractive sets for the system, and show that the existence of a robust control contractive set for the dynamics implies the existence of an appropriate L-function, and hence the robust local attractivity of the set itself. We illustrate the proposed framework through examples that elucidate the theoretical concepts, and through the case study of a nonlinear switched system modelling antimicrobial resistance, which highlights the relevance of the approach to the analysis of biological systems.

</details>


### [26] [dkpy: Robust Control with Structured Uncertainty in Python](https://arxiv.org/abs/2511.13927)
*Timothy Everett Adams,Steven Dahdah,James Richard Forbes*

Main category: eess.SY

TL;DR: dkpy: an open-source Python package for robust controller analysis and synthesis under structured uncertainties, including uncertainty characterization from data.


<details>
  <summary>Details</summary>
Motivation: Robust control requires accounting for model uncertainty to ensure stability and performance. μ-analysis and μ-synthesis provide a structured framework for analysis and design under uncertainty.

Method: Implements μ-analysis/synthesis algorithms and tools for uncertainty characterization from perturbed-system data; open-source at GitHub.

Result: A software package enabling robust analysis and synthesis and data-driven uncertainty characterization for systems with structured uncertainty.

Conclusion: dkpy lowers the barrier to robust controller design and uncertainty analysis, benefiting researchers and practitioners.

Abstract: Models used for control design are, to some degree, uncertain. Model uncertainty must be accounted for to ensure the robustness of the closed-loop system. $μ$-analysis and $μ$-synthesis methods allow for the analysis and design of controllers subject to structured uncertainties. Moreover, these tools can be applied to robust performance problems as they are fundamentally robust control problems with structured uncertainty. The contribution of this paper is dkpy, an open-source Python package for performing robust controller analysis and synthesis for systems subject to structured uncertainty. dkpy also provides tools for performing model uncertainty characterization using data from a set of perturbed systems. The open-source project can be found at https://github.com/decargroup/dkpy.

</details>


### [27] [Power Delivery for Cryogenic Scalable Quantum Applications: Challenges and Opportunities](https://arxiv.org/abs/2511.13965)
*Yating Zou,Batuhan Keskin,Gregor G. Taylor,Zenghui Li,Jie Wang,Eduard Alarcon,Fabio Sebastiano,Masoud Babaie,Edoardo Charbon*

Main category: eess.SY

TL;DR: HV非辐射有线供电在可扩展量子系统中最具潜力，权衡热负荷、噪声耦合与扩展性等因素。


<details>
  <summary>Details</summary>
Motivation: 为百万量级量子比特的冷却需求设计高效、可扩展的低温供电方案，解决温差引发的热负荷、线缆数量与噪声问题。

Method: 对HV有线供电、辐射式无线供电、非辐射式无线供电以及混合HV与非辐射方案在稀释制冷机环境中的可行性进行定性/定量比较，评估热负荷、功率损失、加热、耦合噪声、功率密度、可扩展性、可靠性和复杂性。

Result: 通过比较分析揭示各架构的权衡与适用场景，指出HV非辐射供电在总体指标上具备较优平衡，成为可扩展量子系统的有力候选。

Conclusion: 在面向大规模量子系统的供电需求中，优先发展HV非辐射供电路线，以实现低热负荷、低噪声耦合和良好扩展性；但需进一步工程化优化与实验验证。

Abstract: Quantum technologies offer unprecedented capabilities in computation and secure information transfer. Their implementation requires qubits to operate at cryogenic temperatures (CT) while control and readout electronics typically still remains at room temperature (RT). As systems scale to millions of qubits, the electronics should also operate at CT to avoid a wiring bottleneck. However, wired power transfer from RT for such electronics introduces severe challenges, including thermal load between cooling stages, Joule heating, noise coupling, and wiring scalability. This paper addresses those challenges by evaluating several candidate architectures for scalable power transfer in the dilution frige: high-voltage (HV) wired power transfer, radiative wireless transfer, non-radiative wireless transfer, and a hybrid HV and non-radiative transfer. These architectures are analyzed in terms of thermal load, power loss, heating, coupling noise, power density, scalability, reliability, and complexity. Comparative analysis demonstrates the trade-offs among these architectures, while highlighting HV non-radiative transfer as a promising candidate for scalable quantum systems.

</details>


### [28] [Data Center Control Against Sub-Synchronous Resonance: A Data-Driven Approach](https://arxiv.org/abs/2511.14141)
*Grant Ruan,Marija D. Ilic,Le Xie*

Main category: eess.SY

TL;DR: 通过对数据中心与电网耦合的阻抗特性及其随计算工作负载变化的两音扫频分析，提出基于数据驱动阻抗的早期预警与自适应工作负载管理策略，以降低亚同步谐振SSR的风险。


<details>
  <summary>Details</summary>
Motivation: 数据中心通过PFC转换器从电网取电，且其与电网的相互作用对SSR及潜在级联故障的风险认识不足；需要定量化阻抗特征并据此开展预警与控制。

Method: 进行两音扫频以获取数据中心阻抗（幅值、相位）随频率的变化，研究工作负载对阻抗的深刻影响；基于阻抗建立数据驱动的可用性评估与早期警报机制，并给出在给定电网条件下的灵活工作负载调度策略；通过案例研究验证阻抗建模、预警与控制方法的有效性。

Result: 阻抗随工作负载变化存在明显的阻抗谷，能捕捉到工作负载导致的频率-阻抗特征；数据驱动阻抗能有效刻画不同工作负载下的阻抗变化并用于SSR风险评估；早期警报和预防性控制可提高安全裕度且对工作负载重新分配的影响较小。

Conclusion: 为电网运维与数据中心管理者提供了利用阻抗视角理解两者耦合、并在未来大规模数据中心接入情景中提升安全性与鲁棒性的工具与方法。

Abstract: Data centers host a variety of essential services such as cloud computing and artificial intelligence. Electric grid operators, however, have limited knowledge of the reliability risks of data center interconnection due to their unique operational characteristics. An emerging concern is the sub-synchronous resonance (SSR) which refer to unexpected voltage/current oscillations at typical frequencies below 60/50 Hz. It remains unknown whether and how the interactions between data centers and the grid may trigger resonances, equipment damages, and even cascading failures. In this paper, we focus on grid-connected data centers that draw electricity from the grid through power factor correction (PFC) converters. We conduct two-tone frequency sweep to investigate the data centers' impedance characteristics, i.e. magnitude and phase angle variations over frequencies, and showcase their deep dependence on compute workloads. The impedance modeling provides a direct approach to evaluating SSR risks and enable a cooperative mechanism to alarm and avoid resonance-prone situations. Building upon the impedance, a data-driven preventive controller is then established to raise early warnings of risky operation and suggest flexible workload management according to the given grid conditions. Through case study, we demonstrate how to use impedance to understand the unexpected interactions. Data-driven impedance is validated to show decent performance in capturing the unique impedance dips and tracking the impedance variations across a range of workload scenarios. The early warning and preventive control approaches are further effective to improve the safety margins with minimal workload rescheduling. The key findings of this work will provide valuable insights for grid operators and data center managers, and support preparation for future scenarios involving large-scale data center integration.

</details>


### [29] [Accelerating Automatic Differentiation of Direct Form Digital Filters](https://arxiv.org/abs/2511.14390)
*Chin-Yun Yu,György Fazekas*

Main category: eess.SY

TL;DR: 提出了一种通过直接形成滤波器实现自动微分的通用表述，给出包含初始条件梯度的封闭形式反向传播。这一单一表达式同时描述滤波器及其梯度计算，且支持并行。


<details>
  <summary>Details</summary>
Motivation: 解决在信号滤波中进行高效梯度计算的需求，提供一个统一的前向与梯度计算表达式，避免对不同滤波器逐个实现导数，以及提升并行性。

Method: 推导一个直接形式滤波器的封闭形式反向传播表达式，包含初始条件的梯度；实现为C++/CUDA的PyTorch实现，能在GPU上达到明显的加速；将时间域的解析梯度与频域方法进行对比。

Result: 在基于Python的朴素实现下实现不低于1000x的加速，并在GPU上始终是最快实现；对于实战中常用的低阶滤波器，时间域的解析梯度优势于频域方法在速度上；源代码托管在GitHub。

Conclusion: 提出的通用表述使滤波器的正向计算和梯度计算可以并行化、统一化，显著提升微分能力的实现效率，适合高性能应用场景。

Abstract: We introduce a general formulation for automatic differentiation through direct form filters, yielding a closed-form backpropagation that includes initial condition gradients. The result is a single expression that can represent both the filter and its gradients computation while supporting parallelism. C++/CUDA implementations in PyTorch achieve at least 1000x speedup over naive Python implementations and consistently run fastest on the GPU. For the low-order filters commonly used in practice, exact time-domain filtering with analytical gradients outperforms the frequency-domain method in terms of speed. The source code is available at https://github.com/yoyolicoris/philtorch.

</details>


### [30] [Uncertainty Discounting in Deterministic Black Box Price Predictions for Energy Arbitrage](https://arxiv.org/abs/2511.14158)
*Arnab Bhattacharjee*

Main category: eess.SY

TL;DR: 引入基于后验不确定性折扣的简单启发式方法，将其整合到模型预测控制框架中，用于电池能源套利，且不依赖具体预测模型，即可在相同约束下提升经济回报超过20%。


<details>
  <summary>Details</summary>
Motivation: 现实中的电力市场价格存在显著不确定性，传统的确定性预测（如AEMO的现货价格预测）在极端价格波动下易导致套利策略表现下降，因此需要不依赖预测模型内部结构的鲁棒折扣策略来提升收益。

Method: 提出无需访问预测模型结构或输入的简单启发式不确定性折扣策略，并将其嵌入现有的MPC框架来优化电池调度，以实现对价格不确定性的鲁棒性。

Result: 在相同运营约束下，经济回报提升超过20%。

Conclusion: 该方法实用、可扩展，独立于具体的预测模型，能在能源套利决策中显著提升收益。

Abstract: This study examines the economic impact of post-hoc uncertainty discounting in predictive energy management, specifically in battery energy arbitrage. A 2.2 MWh, 1.1 MW Tesla battery, emulating operations at the University of Queensland's St. Lucia campus, is used as a test system. Traditionally, Model Predictive Control (MPC) frameworks rely on deterministic spot price forecasts from the Australian Energy Market Operator (AEMO) to optimize battery scheduling. However, these forecasts lack uncertainty awareness, making arbitrage strategies vulnerable to extreme price volatility. To address this, we propose simple heuristic uncertainty discounting methods, which require no access to the predictive model's architecture or inputs. By integrating these strategies into existing MPC frameworks, we demonstrate a more than 20% improvement in economic returns under identical operational constraints. This approach enhances decision-making in energy arbitrage while remaining practical, scalable, and independent of specific forecasting models

</details>


### [31] [Entirely Transformerless Universal Direct-Injection Power-Flow Controller](https://arxiv.org/abs/2511.14209)
*Davood Keshavarzi,Alexander Koehler,Wolfram H. Wellssow,Stefan M. Goetz*

Main category: eess.SY

TL;DR: 提出一种无变压器的紧凑高电流全功率流控制电路，用硅/碳化硅混合器件实现直流注入，能在低压电网中实现双向馈线之间的无隔离 inverter 连接的功率流控制，且仿真和实验验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 高比例可再生能源、電動車充電和储能对低压电网造成功率管理和稳定性挑战（反向潮流、局部过载、对/欠电压等）。现有潮流和软开点方案笨重、成本高，需要变压器和大磁性元件，尽管有高频变压器方案也难以降低成本与体积。因此需要更紧凑、低成本、无变压器的解决方案。

Method: 提出一种电路拓扑，将并联的并网分流变换器通过非隔离逆变器双向耦合到低压串联模组，模组能与各自相位浮动地连接，在不同馈线之间实现直接注入与全功率流控制。该方案结合硅和碳化硅器件的优势以实现高电流密度的直接注入，并设计比以往方案更少的半导体器件。对电路进行了数学分析，并通过仿真与实验结果对其进行评估。

Result: 电路具备紧凑结构与高电流全功率流控制能力，且无需变压器，所用半导体数量少于前期概念设计；在仿真和实验层面均证明了可行性，显示出在低压电网中实现双向、跨馈线的功率注入与电压/潮流管理的潜力。

Conclusion: 该工作证明了无变压器、紧凑型高电流全功率流控制在低压电网中的可行性，能有效缓解可再生能源渗透带来的逆潮流与电压波动等问题，并具有潜在的成本与体积优势，适合应用于多馈线低压配电场景。

Abstract: An increasing penetration of renewable energy resources, electric vehicle chargers, and energy storage systems into low-voltage power grids causes several power management and stability problems, such as reverse power flow, (local) overload lines, and over- / under-voltage. Previous power-flow and soft-open-point solutions are bulky and expensive. They need transformers and large magnetics, some on grid frequency, others more compact at high frequency. Even suggested circuits with high-frequency transformers still struggle with cost and size. We present a compact partial power-conversion high-current full-power-flow control circuit without a single transformer. We combine silicon and silicon-carbide, each with their specific advantages for current-dense direct injection. The circuit further needs fewer semiconductors than previous concepts. The circuit links a shunt converter through a non-isolated inverter bidirectionally with low-voltage series modules that practically float with their respective phases can serve between different feeders in low-voltage power grids. We analyze the circuit mathematically and evaluate the operation in simulation and experimental results.

</details>


### [32] [Secure parameter identification of ARX systems with CKKS cryptosystem](https://arxiv.org/abs/2511.14267)
*Jialong Chen,Jimin Wang,Ji-Feng Zhang*

Main category: eess.SY

TL;DR: 提出一种基于 CKKS 的云端隐私保护的 ARX 参数辨识算法，并给出在估计量上的投影映射条件，证明统计距离近似为零从而获得与标准 CKKS 相同的安全性；并利用随机逼近方法证明算法在几乎处处收敛和平方均方收敛，通过数值示例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在云端进行 ARX 系统的参数辨识时，需要保护系统输入输出的隐私，传统云端辨识面临数据暴露风险。通过同态加密（CKKS）实现数据加密后端计算，从而在不暴露原始数据的前提下完成辨识，同时保持一定的安全强度。

Method: 采用 CKKS 公钥密码体系进行参数辨识，证明高斯分布与截断离散分布之间的统计距离可忽略，从而得到与标准 CKKS 相同的安全等级；对估计量施以投影映射，给出正确加解密的条件；在此条件下，利用随机逼近（ stochastic approximation）方法来证明算法的几乎必然收敛和均方收敛性。

Result: 该算法在安全性方面达到与标准 CKKS 相同的水平，并且在理论上证明了在给定条件下的收敛性（几乎必然收敛与平方均方收敛），数值示例验证了其有效性。

Conclusion: 在符合条件的前提下，可以实现基于 CKKS 的云端隐私保护的 ARX 参数辨识，既保障数据隐私又保证收敛性与实用性，研究为将辨识任务外包到云端提供了一种可行的安全方案。

Abstract: This paper focuses on the cloud-based parameter identification problem of ARX systems while protecting the system input and output. To do so, a CKKS-cryptosystem-based parameter identification algorithm is proposed. By rigorously proving that the statistical distance between the Gaussian distribution and the truncated discrete one is negligible, the algorithm has the same security level as the standard CKKS cryptosystem. By utilizing the projection mapping on the estimates, the conditions for correct encryption and decryption are given. Based on these conditions, the stochastic approximation method is further employed to achieve the almost sure and mean square convergence of the algorithm. The effectiveness is demonstrated through a numerical example.

</details>


### [33] [Overtourism to Equilibrium: A System Dynamics & Multi-Objective Model for Sustainable Destinations](https://arxiv.org/abs/2511.14288)
*Huanzhu Lyu,Xiao Yang,Xintong Ji*

Main category: eess.SY

TL;DR: Integrated system-dynamics and NSGA-II framework for sustainable tourism; multi-objective optimization across revenue, environment, and social well-being; tested on Juneau and Iceland with sensitivity analyses guiding policy levers.


<details>
  <summary>Details</summary>
Motivation: Overtourism threatens natural environments and communities; need decision-support that balances economic, environmental, and social goals under dynamic, data-rich contexts.

Method: Develop a dynamic system with four modules (tourist behavior, government finance, environmental evolution, social well-being) and collect data from 2008-2024. Use NSGA-II to optimize three objectives: cumulative net revenue, final environmental index, final social satisfaction. Perform Sobol and Morris sensitivity analyses. Conduct scenario simulations on capacity limits, dynamic pricing at crowded attractions, and investment strategies in lesser-known sites.

Result: Juneau: Pareto-optimal solutions yield net revenue up to 1.64B with environmental index 0.93 and social satisfaction 0.86. Iceland extension: revenues 150M-200M, environmental index up to 0.92, social >0.80. Sensitivity: carbon fees and price elasticity explain >60% of environmental outcome variance; capacity limits explain ~90% of net revenue variability. Scenarios show capacity, pricing, marketing/infrastructure strategies can mitigate congestion and enhance sustainability.

Conclusion: An integrated SD-NSGA-II framework is effective for sustainable tourism management and demonstrates portability across diverse destinations; global sensitivity analysis helps identify policy levers for decision-makers.

Abstract: Overtourism poses severe challenges to popular destinations worldwide, threatening natural environments and local communities. This paper develops a decision-making model integrating system dynamics with multi-objective evolutionary algorithms (NSGA-II) to balance economic returns, environmental protection, and social satisfaction. We collect multi-source data from 2008-2024 including visitor arrivals (up to 3.1M), government revenue/expenditure (up to $10.3M), glacier retreat (220-350 ft), CO2 emissions (77K-105K tons), and social satisfaction (0.29-0.48), and establish a dynamic system with four modules: tourist behavior, government finance, environmental evolution, and social well-being.
  We optimize three objectives via NSGA-II: cumulative net revenue, final environmental index, and final social satisfaction. Experiments on Juneau show optimal solutions yield net revenue up to $1.64B with environmental index 0.93 and social satisfaction 0.86. Extending to Iceland reveals Pareto fronts spanning revenues $150M-$200M, environment indices up to 0.92, and social satisfaction above 0.80.
  Sobol and Morris sensitivity analyses indicate carbon fees and price elasticity account for over 60% of environmental outcome variance, while capacity limits explain around 90% of net revenue variability. Scenario simulations demonstrate how capacity limits and dynamic pricing on crowded attractions, combined with marketing and infrastructure investment in lesser-known sites, mitigate congestion and enhance sustainability.
  This work contributes: (i) an integrated system-dynamics and NSGA-II framework for sustainable tourism management; (ii) demonstrated portability via case studies on Juneau and Iceland; and (iii) global sensitivity analysis highlighting influential policy levers for decision makers.

</details>


### [34] [Multi-Timescale Model Predictive Control for Slow-Fast Systems](https://arxiv.org/abs/2511.14311)
*Lukas Schroth,Daniel Morton,Amon Lahr,Daniele Gammelli,Andrea Carron,Marco Pavone*

Main category: eess.SY

TL;DR: 提出一种基于EDS的多时间尺度MPC框架，通过在预测 horizon 上逐步切换到简化的慢动态模型并指数性增加步长，以实现对快/慢动态系统的高效实时控制。实验在仿真中的三个机器人任务上实现了最高约10×的加速。


<details>
  <summary>Details</summary>
Motivation: MPC在约束控制中的广泛应用依赖于高保真模型和长预测 horizon，但实时解算成本高，限制应用。EDS表明误差对预测 horizon 的影响随距离增大而指数衰减，提供了在 horizon 末端降低模型细节的理论依据。

Method: 提出一个多时间尺度的MPC：在预测过程中按距离时间步长把模型分为快/慢两组，并在 horizon 内逐步切换到仅包含慢动态的简化模型，同时将积分步长按 horizon 指数增长，使后段的求解使用更大步长和更粗的模型。

Result: 在三种机器人控制任务的仿真实验中，获得高达一个数量级的加速（接近10×）的计算效率提升，同时保持可接受的控制性能。

Conclusion: 该方法在理论层面利用EDS特性来降低计算需求，适合存在明显时间尺度分离的系统，需在未来工作中验证鲁棒性、稳定性及在不同硬件上的推广性。

Abstract: Model Predictive Control (MPC) has established itself as the primary methodology for constrained control, enabling autonomy across diverse applications. While model fidelity is crucial in MPC, solving the corresponding optimization problem in real time remains challenging when combining long horizons with high-fidelity models that capture both short-term dynamics and long-term behavior. Motivated by results on the Exponential Decay of Sensitivities (EDS), which imply that, under certain conditions, the influence of modeling inaccuracies decreases exponentially along the prediction horizon, this paper proposes a multi-timescale MPC scheme for fast-sampled control. Tailored to systems with both fast and slow dynamics, the proposed approach improves computational efficiency by i) switching to a reduced model that captures only the slow, dominant dynamics and ii) exponentially increasing integration step sizes to progressively reduce model detail along the horizon. We evaluate the method on three practically motivated robotic control problems in simulation and observe speed-ups of up to an order of magnitude.

</details>


### [35] [An adaptive extension to robust data-driven predictive control under parametric uncertainty](https://arxiv.org/abs/2511.14319)
*Ignacio Sanchez,Filiberto Fele,Daniel Limon*

Main category: eess.SY

TL;DR: A robust data-driven control scheme for time-varying linear systems with polytopic uncertainty that fuses offline extreme-variation data and online rolling-window data via data informativity and SDP to achieve stabilization and a bound on the cost-to-go, without requiring full system identification.


<details>
  <summary>Details</summary>
Motivation: Stabilize time-varying linear systems with uncertain parameters using data-driven methods that exploit both historical extremes and current online measurements, reducing reliance on system identification while maintaining guarantees.

Method: Employ the data informativity framework to combine an offline dataset capturing extreme parameter variations with an online rolling window of recent state-input data. Use Lyapunov-based arguments implemented via semidefinite programming to synthesize a state-feedback controller. The approach yields an upper bound on the cost-to-go for systems consistent with the online data and ensures a decreasing cost for systems compatible with the offline data.

Result: A stabilizing data-driven state-feedback controller is obtained via SDP, with an explicit bound on the cost-to-go for online-consistent systems and a guaranteed decrease of cost for offline-consistent systems; numerical experiments illustrate effectiveness.

Conclusion: The proposed robust data-driven framework successfully stabilizes time-varying linear systems under bounded polytopic uncertainty by fusing offline extremes and online data, relaxing excitation requirements while providing performance guarantees and demonstrating practical efficacy.

Abstract: Robust data-driven controllers typically rely on datasets from previous experiments, which embed information on the variability of the system parameters across past operational conditions. Complementarily, data collected online can contribute to improving the feedback performance relative to the current system's conditions, but are unable to account for the overall -- possibly time-varying -- system operation.
  With this in mind, we consider the problem of stabilizing a time-varying linear system, whose parameters are only known to lie within a bounded polytopic set. Taking a robust data-driven approach, we synthesize the control law by simultaneously leveraging two sets of historical state and input measures: an offline dataset -- which covers the extreme variations of the system parameters -- and an online dataset consisting of a rolling window of the latest state and input samples.
  Our approach relies on the data informativity framework: we thus relax persistent excitation requirements (i.e., the collected samples need not be sufficient for system identification), while still allowing for the design of a stabilizing controller. The state feedback law is obtained from standard Lyapunov arguments, implemented via semi-definite optimization: this also yields an upper bound on the cost-to-go for the class of systems that are consistent with the online data, while guaranteeing a decreasing cost for all systems compatible with the offline data. Numerical experiments are presented to illustrate the effectiveness of the proposed controller.

</details>


### [36] [Offset-free Data-Driven Predictive Control for Grid-Connected Power Converters in Weak Grid Faults](https://arxiv.org/abs/2511.14337)
*Ivo Kraayeveld,Thomas de Jong,Mircea Lazar*

Main category: eess.SY

TL;DR: 在弱网故障下，使用偏移自由数据驱动预测控制替代 PI 控制，提升容错与鲁棒性；仿真显示显著提升阻抗容限和减小误差，计算成本接近 PI。


<details>
  <summary>Details</summary>
Motivation: 解决传统 PI 控制在弱网故障中的振荡与故障穿越能力不足的问题，旨在提供无需物理建模且对偏移具鲁棒性的控制方案。

Method: 提出 offset-free 数据驱动预测控制（DPC），利用故障前或故障时数据构建输入输出预测器，替代外环 PI 调节直流环路与 PCC 电压，达到偏移自由控制。

Result: 仿真表明：前故障偏移自由 DPC 可将可处理的临界等效网阻抗翻倍；故障期间 RMSE 降低约 40 倍；计算时间与传统 PI 相当。

Conclusion: 该方法为弱网下变流器提供简单、鲁棒且计算高效的替代控制方案，显著提升故障穿越能力。

Abstract: Grid-connected power converters encounter significant stability challenges during weak grid faults, when conventional PI-based controllers exhibit an oscillatory response and poor fault-ride-through performance. This paper addresses this problem by replacing the conventional outer PI controllers that regulate DC-link and PCC voltages with an offset-free data-driven predictive controller. The developed algorithm leverages either pre-fault or fault-time data to construct input-output predictors, yielding offset-free control without the need for physics-based modelling. Simulation results show that pre-fault offset-free DPC doubles the critical equivalent grid impedance that can be handled and reduces the root mean squared error during faults by a factor of 40, while maintaining computation times comparable to conventional PI control. These findings demonstrate that the developed offset-free data predictive controller offers a simple, robust, and computationally efficient alternative to conventional control, significantly enhancing fault-ride-through capabilities of converters in weak grids.

</details>


### [37] [Identifying Time-varying Costs in Finite-horizon Linear Quadratic Gaussian Games](https://arxiv.org/abs/2511.14358)
*Kai Ren,Maryam Kamgarpour*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We address cost identification in a finite-horizon linear quadratic Gaussian game. We characterize the set of cost parameters that generate a given Nash equilibrium policy. We propose a backpropagation algorithm to identify the time-varying cost parameters. We derive a probabilistic error bound when the cost parameters are identified from finite trajectories. We test our method in numerical and driving simulations. Our algorithm identifies the cost parameters that can reproduce the Nash equilibrium policy and trajectory observations.

</details>


### [38] [Ultra-Low Insertion Loss Stepped Impedance Resonator Topology for HTSC RF Front-End](https://arxiv.org/abs/2511.14447)
*Ilan Kurtser,Yoav Koral,Eldad Holdengreber,Shmuel E. Schacham,Eliyahu Farber*

Main category: eess.SY

TL;DR: 11-pole HTSC SIR S-band filter using YBCO on sapphire achieves ultra-low insertion loss (-0.1 dB), sharp roll-off (~100 MHz), and high rejection (>80 dB); when integrated with a cryogenic LNA, system-level analysis predicts a noise figure of ~0.34 dB at 3.39 GHz, enabling ~20% radar range improvement over copper front ends. Demonstrates feasibility of practical HTSC RF front-ends for future communication and radar systems.


<details>
  <summary>Details</summary>
Motivation: 在低温条件下提高射频前端的灵敏度和带通选择性，以实现更长探测距离和更低噪声；HTSC材料的极低损耗结合SIR拓扑有潜力显著提升S-band前端性能。

Method: 设计、制备并测试11阶（11-pole）YBCO薄膜SIR滤波器，基于蓝宝石衬底，实现S波段应用；测量插入损耗、滚降和抑制；与低温LNA耦合，进行系统级仿真与测量以估算NF及探测距离改进；

Result: 插入损耗约-0.1 dB，滚降约100 MHz，抑制>80 dB；与LNA耦合时预测在3.39 GHz处NF约0.34 dB，Radar探测距离可比铜基前端提升约20%。

Conclusion: 显示基于HTSC的RF前端在下一代通信和雷达系统中的可行性，尤其在低温条件下可显著降低噪声和提升探测距离。

Abstract: We present the design, fabrication, and measurement of a high-temperature superconductor (HTSC) Stepped Impedance Resonator (SIR) band-pass filter for S-band applications, and its incorporation into a cryogenic receiver cascade. The 11-pole filter, implemented in YBa2Cu3O(7-x) (YBCO) thin films on sapphire, exhibits an ultra-low insertion loss (IL) of -0.1~dB, a sharp roll-off of 100~MHz, and a rejection level exceeding --80~dB. These measured results represent, to the best of our knowledge, the lowest reported IL for an S-band filter with this number of poles. When integrated with a cryogenic low-noise amplifier (LNA), system-level simulations and measurements predict a receiver noise figure (NF) of 0.34~dB at 3.39~GHz, enabling a 20% increase in radar detection range compared with conventional copper-based front ends. This work demonstrates the feasibility of practical HTSC-based RF front-ends for next-generation communication and radar systems.

</details>


### [39] [Agentic AI Systems in Electrical Power Systems Engineering: Current State-of-the-Art and Challenges](https://arxiv.org/abs/2511.14478)
*Soham Ghosh,Gaurav Mittal*

Main category: eess.SY

TL;DR: 本文提出对“agentic AI”的定义与分类体系，并以电气工程中的四个前沿用例为支撑，展示其在工程领域的应用潜力，辅以故障模式分析与设计建议，旨在给出安全、可靠、可问责的实现路线。


<details>
  <summary>Details</summary>
Motivation: 随着 agentic AI 系统的发展，其能力与风险并存，亟需清晰的概念框架和分类以引导研究与产业应用，避免混淆与滥用。

Method: 进行系统性综述，提出定义与分类（taxonomy），并结合电气工程中的四个具体用例进行案例分析，同时给出故障模式分析与设计要点。

Result: 建立了关于 agentic AI 的明确定义与分层/分类框架，展示其在电力系统研究、动态定价与电池共享等场景的应用潜力，以及相应的安全与可问责要点。

Conclusion: 为研究者与从业者提供设计与实现 agentic AI 的安全、可靠、可问责的路线图与参考框架。

Abstract: Agentic AI systems have recently emerged as a critical and transformative approach in artificial intelligence, offering capabilities that extend far beyond traditional AI agents and contemporary generative AI models. This rapid evolution necessitates a clear conceptual and taxonomical understanding to differentiate this new paradigm. Our paper addresses this gap by providing a comprehensive review that establishes a precise definition and taxonomy for "agentic AI," with the aim of distinguishing it from previous AI paradigms. The concepts are gradually introduced, starting with a highlight of its diverse applications across the broader field of engineering. The paper then presents four detailed, state-of-the-art use case applications specifically within electrical engineering. These case studies demonstrate practical impact, ranging from an advanced agentic framework for streamlining complex power system studies and benchmarking to a novel system developed for survival analysis of dynamic pricing strategies in battery swapping stations. Finally, to ensure robust deployment, the paper provides detailed failure mode investigations. From these findings, we derive actionable recommendations for the design and implementation of safe, reliable, and accountable agentic AI systems, offering a critical resource for researchers and practitioners.

</details>


### [40] [Robust Offset-free Kernelized Data-Driven Predictive Control for Nonlinear Systems](https://arxiv.org/abs/2511.14652)
*Mahmood Mazare,Hossein Ramezani*

Main category: eess.SY

TL;DR: 提出一种 Kernelized Data-Driven Predictive Control (KDPC)，通过核岭回归学习非线性映射并对核映射进行解析线性化，使预测可转化为标准二次规划（QP），实现鲁棒的、无偏差的跟踪并具备收敛性与可行性保证；在 Van der Pol 振荡器上验证，抗扰且消除稳态误差，性能优于标准模型为基础的控制器。


<details>
  <summary>Details</summary>
Motivation: 解决非线性系统的鲁棒跟踪与实时实现性之间的权衡。传统方法在处理未知扰动和偏置时可能不具备无偏跟踪能力且计算成本高。通过将数据驱动的核方法与线性化策略结合，提供一个在理论上具备可行性与渐近稳定性的高效控制框架。

Method: 把预测过程分成两步：1) 使用核岭回归从历史轨迹学习非线性映射；2) 对核映射进行解析线性化，以近似未来输入的影响，使控制器可写成一个标准的二次规划（QP）形式实现实时计算。偏置跟踪通过输入增量实现无偏差信号。理论上给出递归可行性和渐近稳定性保证。

Result: 给出递归可行性与渐近稳定性的理论保障。仿真/验证在非线性 Van der Pol 振荡器上进行，能够有效抑制未测扰动、消除稳态误差，性能优于标准模型为基础的控制器。

Conclusion: KDPC 提供一个计算高效且具有理论保证的非线性系统鲁棒、无偏跟踪的控制框架，适用于实时控制任务。通过核学习与线性化相结合实现的 QP 求解在实际应用中具有潜在的普适性和扩展性。

Abstract: This paper proposes a novel Kernelized Data-Driven Predictive Control (KDPC) scheme for robust, offset-free tracking of nonlinear systems. Our computationally efficient hybrid approach separates the prediction: (1) kernel ridge regression learns the nonlinear map from past trajectories, and (2) analytical linearization of the kernel map approximates the effect of future inputs. This linearization is key, allowing the controller to be formulated as a standard Quadratic Program (QP) for efficient real-time implementation. Offset-free tracking is inherently achieved by using input increments. We provide theoretical guarantees for recursive feasibility and asymptotic stability. The algorithm is validated on a nonlinear Van der Pol oscillator, where it successfully rejects unmeasured disturbances and eliminates steady-state errors, outperforming a standard model-based controller.

</details>


### [41] [Towards AC Feasibility of DCOPF Dispatch](https://arxiv.org/abs/2511.14725)
*Michael A. Boateng,Russell Bent,Sidhant Misra,Parikshit Pareek,Pascal Van Hentenryck,Daniel Molzahn*

Main category: eess.SY

TL;DR: 提出一个统一的 DCOPF-ACPF 管线，在四种 DCOPF 变体和分布式 slack 分配/ PV/PQ 切换的组合下，从 DCOPF 出发恢复 AC 可行解，显著提升准确性和合规性。


<details>
  <summary>Details</summary>
Motivation: DCOPF 虽然简单高效，但其无损、对无功不考虑的模型在实际 AC 方程下往往不可行。尽管如此，行业仍广泛采用，需要一个从 DCOPF 到 AC 可行解的实用转换方案。

Method: 提出四种 DCOPF 变体，结合基于分布式 slack 分配和 PV/PQ 切换的 AC 可行性恢复，构建一个统一的管线，用以从 DCOPF 产出恢复 AC 可行解，并在多种测试集上评估以选出最有效组合。

Result: 在超过 10,000 个调度场景和多种测试情形下，结构化的 ACPF 模型实现了同时满足 ACPF 方程与工程不等式约束的解；在 13,659 节点大规模案例中，与传统单 slack 总线方法相比，DCOPF-ACPF 的均值绝对误差(或 MAE)与成本差异分别减少约 75% 与 93%；在极端负载条件下，管线对不等式违反的抑制效果提升 3–5 倍。

Conclusion: 以结构化 ACPF 为核心的管线在恢复 AC 可行性方面最有效，且能在大规模系统中稳定运行，具有明显的实际应用潜力，建议优先采用该管线的组合。

Abstract: DC Optimal Power Flow (DCOPF) is widely utilized in power system operations due to its simplicity and computational efficiency. However, its lossless, reactive power-agnostic model often yields dispatches that are infeasible under practical operating scenarios such as the nonlinear AC power flow (ACPF) equations. While theoretical analysis demonstrates that DCOPF solutions are inherently AC-infeasible, their widespread industry adoption suggests substantial practical utility. This paper develops a unified DCOPF-ACPF pipeline to recover AC feasible solutions from DCOPF-based dispatches. The pipeline uses four DCOPF variants and applies AC feasibility recovery using both distributed slack allocation and PV/PQ switching. The main objective is to identify the most effective pipeline for restoring AC feasibility. Evaluation across over 10,000 dispatch scenarios on various test cases demonstrates that the structured ACPF model yields solutions that satisfy both the ACPF equations, and all engineering inequality constraints. In a 13,659 bus case, the mean absolute error and cost differences between DCOPF and ACOPF are reduced by 75% and 93%, respectively, compared to conventional single slack bus methods. Under extreme loading conditions, the pipeline reduces inequality constraint violations by a factor of 3 to 5.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Blurred Encoding for Trajectory Representation Learning](https://arxiv.org/abs/2511.13741)
*Silin Zhou,Yao Chen,Shuo Shang,Lisi Chen,Bingsheng He,Ryosuke Shibasaki*

Main category: cs.LG

TL;DR: BLUE 将 GPS 坐标通过逐步降低精度形成多层分层补丁，在编码器-解码器的金字塔结构中用 Transformer 学习不同层的轨迹嵌入，并通过重构任务进行自监督训练，从而在多项下游任务上超越 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的 SOTA TRL 方法将轨迹简化为网格或道路段，丢失细粒度的时空信息，迫切需要在不同粒度间有效融合并保持细粒度信息以提升下游任务表现。

Method: 引入 BLUE（BLUrred Encoding）。通过逐步降低 GPS 坐标精度生成多层分层补丁；在每个补丁层使用 Transformer 学习当前层的轨迹嵌入。编码器通过池化将信息上采样至高层，解码器通过上采样向低层提供指导。整个模型以轨迹重构为目标，使用 MSE 损失进行训练，且具备 Pyramid 结构以实现多层协同学习。

Result: 在对比的 3 项下游任务中，BLUE 相对于 8 种 SOTA TRL 方法均表现更好，平均领先约 30.90%。代码可公开获取。

Conclusion: BLUE 通过在不同粒度的分层补丁之间进行协同学习，实现了同时保留细粒度细节与捕捉全局模式的轨迹表示，提升了多任务下的性能。

Abstract: Trajectory representation learning (TRL) maps trajectories to vector embeddings and facilitates tasks such as trajectory classification and similarity search. State-of-the-art (SOTA) TRL methods transform raw GPS trajectories to grid or road trajectories to capture high-level travel semantics, i.e., regions and roads. However, they lose fine-grained spatial-temporal details as multiple GPS points are grouped into a single grid cell or road segment. To tackle this problem, we propose the BLUrred Encoding method, dubbed BLUE, which gradually reduces the precision of GPS coordinates to create hierarchical patches with multiple levels. The low-level patches are small and preserve fine-grained spatial-temporal details, while the high-level patches are large and capture overall travel patterns. To complement different patch levels with each other, our BLUE is an encoder-decoder model with a pyramid structure. At each patch level, a Transformer is used to learn the trajectory embedding at the current level, while pooling prepares inputs for the higher level in the encoder, and up-resolution provides guidance for the lower level in the decoder. BLUE is trained using the trajectory reconstruction task with the MSE loss. We compare BLUE with 8 SOTA TRL methods for 3 downstream tasks, the results show that BLUE consistently achieves higher accuracy than all baselines, outperforming the best-performing baselines by an average of 30.90%. Our code is available at https://github.com/slzhou-xy/BLUE.

</details>


### [43] [Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds](https://arxiv.org/abs/2511.14056)
*Marios Papamichals,Regina Ruane*

Main category: cs.LG

TL;DR: 提出 Radial Compensation (RC) 与 Balanced-Exponential (bExp) 的几何图表框架，在曲面上的生成模型中解耦曲率与模型参数，提升稳定性、似然和计算效率，RC-bExp 成为曲面上 Likelihood-trained 模型的默认鲁棒方案。


<details>
  <summary>Details</summary>
Motivation: 曲线曲率与模型参数的耦合会放大梯度方差；指数映射的雅可比随半径波动且难以控；体积保持的图表会扭曲测地距离；高维潜变量流中的包裹指数先验在曲率尺度之外拉伸半径，导致似然下降与求解困难。需要一种曲率不变、参数语义解耦的几何方法。

Method: 从信息几何出发，RC 通过在切空间选取基密度使似然仅依赖于从某极点的测地距离，从而将参数意义与曲率解耦；将 RC 扩展到具备已知测地极地体积的流形；提出 Balanced-Exponential（bExp）图族，平衡体积失真与测地误差；在 RC 条件下，所有 bExp 设置保持相同的流形密度与 Fisher 信息，较小的调节值降低梯度方差与流成本。

Result: 在密度函数、VAE、图像/图上的流、蛋白质模型等场景中实现稳定性提升；提高似然、恢复干净的测地半径、避免高维流中的半径发散；RC 与 bExp 提供曲面上似然训练的稳健默认。

Conclusion: RC 提供曲率不变、参数语义解耦的框架，使径向参数在测地单位下保持直观含义，曲线的数值预条件可通过图表调节；bExp 在同一流形下等价地保留密度与 Fisher 信息，但通过较小的调参实现更低梯度方差与更低计算成本；在图像、图、蛋白质等任务上的实验表明 RC-bExp 为概率建模在曲面上的强健默认。

Abstract: Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.

</details>


### [44] [SCALEX: Scalable Concept and Latent Exploration for Diffusion Models](https://arxiv.org/abs/2511.13750)
*E. Zhixuan Zeng,Yuhao Chen,Alexander Wong*

Main category: cs.LG

TL;DR: SCALEX 通过自然语言提示在扩散模型潜在空间中实现可扩展、零监督的方向探索，揭示性别/职业等偏见及概念结构，且不需要再训练或人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型偏见分析工具要么局限于预定义类别，要么依赖于手工解释的潜在方向，难以扩展并发现隐性或意外模式。需要一个可扩展、自动化且可发现新模式的分析框架。

Method: SCALEX 仅利用自然语言提示从 H-空间抽取语义化方向，实现零监督解释；无需再训练或标注，将提示映射到潜在方向，便于跨任意概念的比较与大规模发现。

Result: SCALEX 能在职业提示中发现性别偏见，按身份描述对语义对齐进行排序，并揭示无监督的簇状概念结构，提升偏见分析的可扩展性、可解释性与可扩展性。

Conclusion: 与现有方法相比，SCALEX 使扩散模型的偏见分析更加可扩展、可解释和可拓展，降低了对先验类别和人工标注的依赖。

Abstract: Image generation models frequently encode social biases, including stereotypes tied to gender, race, and profession. Existing methods for analyzing these biases in diffusion models either focus narrowly on predefined categories or depend on manual interpretation of latent directions. These constraints limit scalability and hinder the discovery of subtle or unanticipated patterns.
  We introduce SCALEX, a framework for scalable and automated exploration of diffusion model latent spaces. SCALEX extracts semantically meaningful directions from H-space using only natural language prompts, enabling zero-shot interpretation without retraining or labelling. This allows systematic comparison across arbitrary concepts and large-scale discovery of internal model associations. We show that SCALEX detects gender bias in profession prompts, ranks semantic alignment across identity descriptors, and reveals clustered conceptual structure without supervision. By linking prompts to latent directions directly, SCALEX makes bias analysis in diffusion models more scalable, interpretable, and extensible than prior approaches.

</details>


### [45] [Robustness of LLM-enabled vehicle trajectory prediction under data security threats](https://arxiv.org/abs/2511.13753)
*Feilong Wang,Fuqiang Liu*

Main category: cs.LG

TL;DR: LLM驱动的车辆轨迹预测存在对抗性脆弱性：提出单一特征差分进化攻击，在黑盒下对输入提示中的单一运动学特征进行扰动，即使微小且物理上可行的扰动也能显著改变输出；首次揭示在自动驾驶场景中LLM预测的安全性与鲁棒性挑战。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型融入自动驾驶以提升推理与决策，但关于其预测鲁棒性和安全性的研究很少。了解对抗攻击对LLM驱动的轨迹预测的影响对于确保安全性至关重要。

Method: 提出一种单特征差分进化攻击（one-feature differential evolution attack），在黑箱设置下对 Surrounding vehicles 的单个运动学特征进行扰动，并将扰动引入LLM输入提示中。基于highD数据集进行实验，分析准确性与鲁棒性之间的权衡，探究失败机制，并探讨潜在缓解方案。

Result: 实验表明即使是微小、物理上可行的扰动也能显著干扰模型输出，揭示LLM驱动的车辆轨迹预测对对抗性操控的易受攻击性。

Conclusion: 首次从系统层面揭示LLM驱动的自动驾驶预测在车辆交互场景中的对抗性脆弱性，强调在未来基于LLM的智能交通系统设计中需要优先考虑鲁棒性与安全性。

Abstract: The integration of large language models (LLMs) into automated driving systems has opened new possibilities for reasoning and decision-making by transforming complex driving contexts into language-understandable representations. Recent studies demonstrate that fine-tuned LLMs can accurately predict vehicle trajectories and lane-change intentions by gathering and transforming data from surrounding vehicles. However, the robustness of such LLM-based prediction models for safety-critical driving systems remains unexplored, despite the increasing concerns about the trustworthiness of LLMs. This study addresses this gap by conducting a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction. We propose a one-feature differential evolution attack that perturbs a single kinematic feature of surrounding vehicles within the LLM's input prompts under a black-box setting. Experiments on the highD dataset reveal that even minor, physically plausible perturbations can significantly disrupt model outputs, underscoring the susceptibility of LLM-based predictors to adversarial manipulation. Further analyses reveal a trade-off between accuracy and robustness, examine the failure mechanism, and explore potential mitigation solutions. The findings provide the very first insights into adversarial vulnerabilities of LLM-driven automated vehicle models in the context of vehicular interactions and highlight the need for robustness-oriented design in future LLM-based intelligent transportation systems.

</details>


### [46] [Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement](https://arxiv.org/abs/2511.13755)
*Zhe Yang,Wenrui Li,Hongtao Chen,Penghong Wang,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.LG

TL;DR: 提出 RedReg 方法，通过监测冗余并对优势模态梯度进行正交投影与门控，实现多模态信息的平衡 refined，提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 在联合多模态训练中，模态偏置导致优势模态主导反向传播，长期会削弱表示-输出耦合并积累冗余信息；现有方法未能解决冗余长期占优和跨模态语义与方向性缺乏考虑的问题。

Method: 提出 Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg)。核心包括：1) 冗余阶段监控器，通过有效增益增长率和冗余的联合标准在冗余高时触发干预；2) 共同信息门控机制，基于跨模态语义估计当前主导模态的贡献；任务若主要依赖单一模态，则自动禁用抑制项以保留模态特有信息；3) 将主导模态梯度投影到联合多模态梯度子空间的正交补上，并依据冗余程度抑制梯度。

Result: 实验表明在大多数场景下优于现有主流方法；消融实验验证了方法有效性，并给出代码链接（GitHub）。

Conclusion: RedReg 将信息瓶颈灵感融入冗余控制与梯度方向性调节，实现对多模态信息的高效平衡与细粒度控制，提升多模态学习的稳定性与性能。

Abstract: Multimodal learning aims to improve performance by leveraging data from multiple sources. During joint multimodal training, due to modality bias, the advantaged modality often dominates backpropagation, leading to imbalanced optimization. Existing methods still face two problems: First, the long-term dominance of the dominant modality weakens representation-output coupling in the late stages of training, resulting in the accumulation of redundant information. Second, previous methods often directly and uniformly adjust the gradients of the advantaged modality, ignoring the semantics and directionality between modalities. To address these limitations, we propose Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg), which is inspired by information bottleneck principle. Specifically, we construct a redundancy phase monitor that uses a joint criterion of effective gain growth rate and redundancy to trigger intervention only when redundancy is high. Furthermore, we design a co-information gating mechanism to estimate the contribution of the current dominant modality based on cross-modal semantics. When the task primarily relies on a single modality, the suppression term is automatically disabled to preserve modality-specific information. Finally, we project the gradient of the dominant modality onto the orthogonal complement of the joint multimodal gradient subspace and suppress the gradient according to redundancy. Experiments show that our method demonstrates superiority among current major methods in most scenarios. Ablation experiments verify the effectiveness of our method. The code is available at https://github.com/xia-zhe/RedReg.git

</details>


### [47] [VitalBench: A Rigorous Multi-Center Benchmark for Long-Term Vital Sign Prediction in Intraoperative Care](https://arxiv.org/abs/2511.13757)
*Xiuding Cai,Xueyao Wang,Sen Wang,Yaoyao Zhu,Jiao Chen,Yu Yao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Intraoperative monitoring and prediction of vital signs are critical for ensuring patient safety and improving surgical outcomes. Despite recent advances in deep learning models for medical time-series forecasting, several challenges persist, including the lack of standardized benchmarks, incomplete data, and limited cross-center validation. To address these challenges, we introduce VitalBench, a novel benchmark specifically designed for intraoperative vital sign prediction. VitalBench includes data from over 4,000 surgeries across two independent medical centers, offering three evaluation tracks: complete data, incomplete data, and cross-center generalization. This framework reflects the real-world complexities of clinical practice, minimizing reliance on extensive preprocessing and incorporating masked loss techniques for robust and unbiased model evaluation. By providing a standardized and unified platform for model development and comparison, VitalBench enables researchers to focus on architectural innovation while ensuring consistency in data handling. This work lays the foundation for advancing predictive models for intraoperative vital sign forecasting, ensuring that these models are not only accurate but also robust and adaptable across diverse clinical environments. Our code and data are available at https://github.com/XiudingCai/VitalBench.

</details>


### [48] [ChemFixer: Correcting Invalid Molecules to Unlock Previously Unseen Chemical Space](https://arxiv.org/abs/2511.13758)
*Jun-Hyoung Park,Ho-Jun Song,Seong-Whan Lee*

Main category: cs.LG

TL;DR: ChemFixer 通过一个基于Transformer的纠错框架，用对照的有效/无效分子对进行掩码预训练和微调，纠正深度学习生成中产生的无效分子，从而在保持分子分布的前提下显著提升有效性，并扩展潜在药物空间，且适用于数据有限场景的下游任务。


<details>
  <summary>Details</summary>
Motivation: 深度学习药物分子生成常产生化学上无效的分子，导致可用化学空间受限、降低实际应用价值。需要一个能修正无效分子、保持化学与生物分布特征的框架，以提升输出质量并扩大潜在药物候选集合。

Method: 提出基于Transformer的ChemFixer框架，利用掩码化预训练，并在一个由大量有效/无效分子对构成的规模化数据集上进行微调。通过对不同生成模型的评估，验证其在纠正无效性同时尽量保留原输出的化学与生物分布。

Result: ChemFixer显著提升了多种生成模型的分子有效性，同时在保持原输出分布特征方面表现良好，能够恢复此前无法生成的分子，扩展潜在药物候选的多样性。此外，在DTI任务中对数据有限的场景也表现出良好效果，提升生成配体的有效性并发现潜在的配体-蛋白对。

Conclusion: ChemFixer作为一个实用工具，能够在药物发现的各阶段提升分子有效性并扩展可达的化学空间，且具备对数据有限场景和下游任务的良好适应性。

Abstract: Deep learning-based molecular generation models have shown great potential in efficiently exploring vast chemical spaces by generating potential drug candidates with desired properties. However, these models often produce chemically invalid molecules, which limits the usable scope of the learned chemical space and poses significant challenges for practical applications. To address this issue, we propose ChemFixer, a framework designed to correct invalid molecules into valid ones. ChemFixer is built on a transformer architecture, pre-trained using masking techniques, and fine-tuned on a large-scale dataset of valid/invalid molecular pairs that we constructed. Through comprehensive evaluations across diverse generative models, ChemFixer improved molecular validity while effectively preserving the chemical and biological distributional properties of the original outputs. This indicates that ChemFixer can recover molecules that could not be previously generated, thereby expanding the diversity of potential drug candidates. Furthermore, ChemFixer was effectively applied to a drug-target interaction (DTI) prediction task using limited data, improving the validity of generated ligands and discovering promising ligand-protein pairs. These results suggest that ChemFixer is not only effective in data-limited scenarios, but also extensible to a wide range of downstream tasks. Taken together, ChemFixer shows promise as a practical tool for various stages of deep learning-based drug discovery, enhancing molecular validity and expanding accessible chemical space.

</details>


### [49] [Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive Content Detection](https://arxiv.org/abs/2511.13759)
*Han Wang,Deyi Ji,Junyu Lu,Lanyun Zhu,Hailong Zhang,Haiyang Wu,Liqun Liu,Peng Shu,Roy Ka-Wei Lee*

Main category: cs.LG

TL;DR: 基于自训练的协作伪标签框架，利用多模态语言模型与文本分类器的共识来提升低资源下的 offensive content 检测，接近大模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于 offensive content 的标注稀缺、低发生率以及高标注成本，迫切需要充分利用海量未标注数据，降低对高成本标签的依赖。

Method: 从一个小型标注集训练的轻量级分类器出发，对未标注数据进行迭代伪标签；分类器与 MA-VLMs 的输出进行比对，将未标注数据分为 Agreed-Unknown（ classifier 与 MA-VLMs 同意的未知标签）和 Disagreed-Unknown（存在分歧的未知样本）。MA-VLMs 通过 moderator 与 user 两个视角来模拟监管和主观观点以提升标签可信度；使用新颖的 Positive-Negative-Unlabeled (PNU) 损失，联合利用有标签数据、Agreed-Unknown 与 Disagreed-Unknown 数据，同时抑制伪标签噪声。

Result: 在基准数据集上的实验表明，与仅有少量监督的基线相比，本框架显著提升性能，并且接近大规模模型的性能。

Conclusion: 通过协作伪标签和 PNU 损失，充分利用未标记数据，提升低资源条件下的 offensive content 检测效果，降低伪标签噪声，具有较强的鲁棒性和实用性。

Abstract: Accurate detection of offensive content on social media demands high-quality labeled data; however, such data is often scarce due to the low prevalence of offensive instances and the high cost of manual annotation. To address this low-resource challenge, we propose a self-training framework that leverages abundant unlabeled data through collaborative pseudo-labeling. Starting with a lightweight classifier trained on limited labeled data, our method iteratively assigns pseudo-labels to unlabeled instances with the support of Multi-Agent Vision-Language Models (MA-VLMs). Un-labeled data on which the classifier and MA-VLMs agree are designated as the Agreed-Unknown set, while conflicting samples form the Disagreed-Unknown set. To enhance label reliability, MA-VLMs simulate dual perspectives, moderator and user, capturing both regulatory and subjective viewpoints. The classifier is optimized using a novel Positive-Negative-Unlabeled (PNU) loss, which jointly exploits labeled, Agreed-Unknown, and Disagreed-Unknown data while mitigating pseudo-label noise. Experiments on benchmark datasets demonstrate that our framework substantially outperforms baselines under limited supervision and approaches the performance of large-scale models

</details>


### [50] [MoETTA: Test-Time Adaptation Under Mixed Distribution Shifts with MoE-LayerNorm](https://arxiv.org/abs/2511.13760)
*Xiao Fan,Jingyan Jiang,Zhaoru Chen,Fanding Huang,Xiao Chen,Qinting Jiang,Bowen Zhang,Xing Tang,Zhi Wang*

Main category: cs.LG

TL;DR: 提出 MoETTA：基于熵的 TTA 框架，结合 Mixture-of-Experts 实现对混合分布偏移的多方向自适应。引入 potpourri 与 potpourri+ 基准以模拟现实部署场景，后者还包含源域数据以评估遗忘鲁棒性。实验证明在三类混合分布偏移设定下，MoETTA 超越强基线，达到 SOTA，并展示通过专家级多样性实现不同方向的自适应更新的优势。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的测试数据往往存在多域且相互冲突的分布偏移，单一路径的梯度更新在不同域上未必最优，现有 TTA 多为统一适应路径，忽略了域间梯度方向的差异，且评估基准仅限于合成或单一偏移，无法反映真实部署的复杂性。

Method: 提出一个熵基 TTA 框架 MoETTA，结合 Mixture-of-Experts（MoE）架构。引入一组结构上解耦的专家以实现对不同域梯度方向的适应，专家间可在不同方向上进行参数更新。通过熵或门控机制实现对不同样本分配到不同专家的策略，提升对异质偏移的适应能力。设计了两套新基准 potpourri 与 potpourri+，覆盖自然、艺术化、对抗性失真等多源偏移；potpourri+ 还包含源域样本以评估灾变遗忘。

Result: 在三类混合分布偏移设定下，MoETTA 持续优于强基线，达到 SOTA 性能，并强调通过专家级多样性建模来获得多方向自适应的优势。

Conclusion: 通过引入 MoE 及熵基门控机制，MoETTA 能更好地应对异质分布偏移并降低灾变遗忘风险，新的 potpourri/potpourri+ 基准提供更接近现实部署的评估框架，实验表明多专家协作的自适应更新在现实场景中具有显著优势。

Abstract: Test-Time adaptation (TTA) has proven effective in mitigating performance drops under single-domain distribution shifts by updating model parameters during inference. However, real-world deployments often involve mixed distribution shifts, where test samples are affected by diverse and potentially conflicting domain factors, posing significant challenges even for SOTA TTA methods. A key limitation in existing approaches is their reliance on a unified adaptation path, which fails to account for the fact that optimal gradient directions can vary significantly across different domains. Moreover, current benchmarks focus only on synthetic or homogeneous shifts, failing to capture the complexity of real-world heterogeneous mixed distribution shifts. To address this, we propose MoETTA, a novel entropy-based TTA framework that integrates the Mixture-of-Experts (MoE) architecture. Rather than enforcing a single parameter update rule for all test samples, MoETTA introduces a set of structurally decoupled experts, enabling adaptation along diverse gradient directions. This design allows the model to better accommodate heterogeneous shifts through flexible and disentangled parameter updates. To simulate realistic deployment conditions, we introduce two new benchmarks: potpourri and potpourri+. While classical settings focus solely on synthetic corruptions, potpourri encompasses a broader range of domain shifts--including natural, artistic, and adversarial distortions--capturing more realistic deployment challenges. Additionally, potpourri+ further includes source-domain samples to evaluate robustness against catastrophic forgetting. Extensive experiments across three mixed distribution shifts settings show that MoETTA consistently outperforms strong baselines, establishing SOTA performance and highlighting the benefit of modeling multiple adaptation directions via expert-level diversity.

</details>


### [51] [Gene Incremental Learning for Single-Cell Transcriptomics](https://arxiv.org/abs/2511.13762)
*Jiaxin Qi,Yan Cui,Jianqiang Huang,Gaogang Xie*

Main category: cs.LG

TL;DR: 提出一个基因增量学习框架，应用于单细胞转录组数据，改编自类增量学习以缓解遗忘，给出完整的基因增量学习基准。


<details>
  <summary>Details</summary>
Motivation: 本研究以往增量学习多关注计算机视觉中的类别与令牌，缺乏对令牌级别增量学习的系统研究。由于语言中的全局性使增量学习设计困难，作者将研究焦点转向一类特殊的令牌——基因，并在大规模生物数据集上提出增量学习管线与评估，以填补方法学空白。

Method: 将现有的类增量学习方法改编用于基因增量学习，研究并缓解遗忘现象，构建增量学习管线并在单细胞转录组数据上进行大量实验以验证框架与适配策略。

Result: 发现基因增量学习中也存在遗忘问题，验证了改编方法的有效性，实验表明框架设计与评估具有合理性，最终提供了完整的基因增量学习基准。

Conclusion: 该工作为基因增量学习在单细胞转录组中的应用提供了可用的框架、评估和基准，推动了该领域的标准化与可重复性。

Abstract: Classes, as fundamental elements of Computer Vision, have been extensively studied within incremental learning frameworks. In contrast, tokens, which play essential roles in many research fields, exhibit similar characteristics of growth, yet investigations into their incremental learning remain significantly scarce. This research gap primarily stems from the holistic nature of tokens in language, which imposes significant challenges on the design of incremental learning frameworks for them. To overcome this obstacle, in this work, we turn to a type of token, gene, for a large-scale biological dataset--single-cell transcriptomics--to formulate a pipeline for gene incremental learning and establish corresponding evaluations. We found that the forgetting problem also exists in gene incremental learning, thus we adapted existing class incremental learning methods to mitigate the forgetting of genes. Through extensive experiments, we demonstrated the soundness of our framework design and evaluations, as well as the effectiveness of our method adaptations. Finally, we provide a complete benchmark for gene incremental learning in single-cell transcriptomics.

</details>


### [52] [Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels](https://arxiv.org/abs/2511.13764)
*Arun Thangamani,Md Asghar Ahmad Shahid,Adam Siemieniuk,Rolf Morel,Renato Golin,Alexander Heinecke*

Main category: cs.LG

TL;DR: 基于 MLIR 的编译框架，自动生成可扩展且高性能的微内核，降低对低级库的依赖，并在目标 CPU 的向量与分块指令上实现近似最优寄存器利用。


<details>
  <summary>Details</summary>
Motivation: AI/ML 工作负载日益复杂，但高效硬件利用仍需深厚硬件知识；手工内核或依赖专用库难以扩展到广泛应用。

Method: 通过 MLIR dialect，设计一种将领域操作映射到处理器能力的编译方案，并从低级 IR 构造纳米内核，实现高效寄存器使用；支持向量与分块 CPU 指令。

Result: 生成的 nanokernel 达到生产级质量，并与现有微内核库相竞争。

Conclusion: 证明了基于 MLIR 的自动微内核生成可提高可扩展性，降低对专用库的依赖，适合广泛的 ML 实践者。

Abstract: The rapidly evolving landscape of AI and machine learning workloads has widened the gap between high-level domain operations and efficient hardware utilization. Achieving near-peak performance still demands deep hardware expertise-experts either handcraft target-specific kernels (e.g., DeepSeek) or rely on specialized libraries (e.g., CUTLASS)-both of which add complexity and limit scalability for most ML practitioners.
  This paper introduces a compilation scheme that automatically generates scalable, high-performance microkernels by leveraging the MLIR dialects to bridge domain-level operations and processor capabilities. Our approach removes dependence on low-level libraries by enabling the compiler to auto-generate near-optimal code directly. At its core is a mechanism for composing nanokernels from low-level IR constructs with near-optimal register utilization, forming efficient microkernels tailored to each target. We implement this technique in an MLIR-based compiler supporting both vector and tile based CPU instructions. Experiments show that the generated nanokernels are of production-quality, and competitive with state-of-the-art microkernel libraries.

</details>


### [53] [PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning](https://arxiv.org/abs/2511.13765)
*Shengjie Sun,Jiafei Lyu,Runze Liu,Mengbei Yan,Bo Liu,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: PROF通过LLMs生成并改进离线IL的可执行奖励函数，并用RPR在不依赖环境交互的情况下对奖励函数进行排序，从而在D4RL上实现强竞争力。


<details>
  <summary>Details</summary>
Motivation: 离线模仿学习需要在缺乏奖励信号的条件下学习策略；现有方法往往假设轨迹与专家演示的相似度与奖励正相关，难以正确建模复杂的奖励结构。本文提出PROF框架，利用大语言模型从自然语言描述和单个专家轨迹生成并改进可执行的奖励函数代码。为避免需要环境交互或RL训练，提出Reward Preference Ranking (RPR)以对奖励函数进行质量评估与排序。

Method: PROF先使用大型语言模型从自然语言描述和单个专家轨迹生成并改进可执行的奖励函数代码；再通过RPR对候选奖励函数进行排序，评估标准为与专家偏好的支配关系。随后交替进行基于文本的梯度优化与RPR排名，自动选取并细化最佳奖励函数用于下游策略学习。

Result: 在D4RL数据集上，PROF能够超越或匹配最近的强基线，展示其在多数据集与领域中的有效性。

Conclusion: PROF实现了离线IL中奖励函数的端到端自动化生成、筛选与改进，配合RPR为奖励函数质量提供无环境交互的评估与排序，提升下游策略学习的效果。

Abstract: Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward function quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains, highlighting the effectiveness of our approach.

</details>


### [54] [Credal Ensemble Distillation for Uncertainty Quantification](https://arxiv.org/abs/2511.13766)
*Kaizheng Wang,Fabio Cuzzolin,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: 提出了 credal ensemble distillation (CED)，将深度集成压缩为单模型 CREDIT，通过输出类别概率区间形成可信集合以进行不确定性量化，在 OOD 检测上与基线相当或更优，同时显著降低推理开销。


<details>
  <summary>Details</summary>
Motivation: 解决深度集合在推理阶段的高计算与内存成本，同时保留对预测不确定性的量化能力，尤其区分并衡量 aleatoric 与 epistemic 不确定性。

Method: 通过将深度集合的知识压缩为单一模型 CREDIT，使其输出每个类别的概率区间，定义一个概率分布的等价集合（credal set），以获得更丰富的不确定性信息并实现高效推理。

Result: 在外部分布检测（OOD）基准上，CED 的不确定性估计优于或等同于若干基线，并在推理成本方面显著优于原始的深度集合。

Conclusion: CED 为在保持不确定性量化能力的前提下实现高效推理提供了一种有效的压缩框架，适用于分类任务中的可信性评估。

Abstract: Deep ensembles (DE) have emerged as a powerful approach for quantifying predictive uncertainty and distinguishing its aleatoric and epistemic components, thereby enhancing model robustness and reliability. However, their high computational and memory costs during inference pose significant challenges for wide practical deployment. To overcome this issue, we propose credal ensemble distillation (CED), a novel framework that compresses a DE into a single model, CREDIT, for classification tasks. Instead of a single softmax probability distribution, CREDIT predicts class-wise probability intervals that define a credal set, a convex set of probability distributions, for uncertainty quantification. Empirical results on out-of-distribution detection benchmarks demonstrate that CED achieves superior or comparable uncertainty estimation compared to several existing baselines, while substantially reducing inference overhead compared to DE.

</details>


### [55] [Dynamic Temperature Scheduler for Knowledge Distillation](https://arxiv.org/abs/2511.13767)
*Sibgat Ul Islam,Jawad Ibn Ahad,Fuad Rahman,Mohammad Ruhul Amin,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: Proposes Dynamic Temperature Scheduler (DTS) for knowledge distillation (KD) that adapts the softmax temperature based on the divergence between teacher and student (via cross-entropy loss gap). Demonstrates consistent gains over fixed-temperature baselines across vision and NLP tasks; code released.


<details>
  <summary>Details</summary>
Motivation: Fixed, static temperature in KD is suboptimal due to early benefits of softer distributions and later need for sharper distributions; architectural mismatches cause logit magnitude differences; a temperature that adapts to teacher-student divergence can improve KD performance.

Method: Introduce Dynamic Temperature Scheduler (DTS) that adjusts the KD temperature dynamically according to the cross-entropy loss gap between teacher and student. DTS integrates with existing KD frameworks and can be applied across different KD strategies. Evaluation on vision tasks (CIFAR-100, Tiny-ImageNet) and NLP tasks (GLUE, Dolly, SelfIns, UnNI, S-NI) showing improvements; code available at provided repository.

Result: DTS consistently outperforms static-temperature baselines across multiple KD strategies and datasets in both vision and NLP domains.

Conclusion: DTS provides a practical, model-agnostic temperature scheduling mechanism for KD that adapts to teacher-student divergence, improving performance and integration with existing KD frameworks; encourages broader adoption and further exploration of dynamic hyperparameters in KD.

Abstract: Knowledge Distillation (KD) trains a smaller student model using a large, pre-trained teacher model, with temperature as a key hyperparameter controlling the softness of output probabilities. Traditional methods use a fixed temperature throughout training, which is suboptimal. Moreover, architectural differences between teacher and student often result in mismatched logit magnitudes. We demonstrate that students benefit from softer probabilities early in training but require sharper probabilities in later stages. We introduce Dynamic Temperature Scheduler (DTS), which adjusts temperature dynamically based on the cross-entropy loss gap between teacher and student. To our knowledge, this is the first temperature scheduling method that adapts based on the divergence between teacher and student distributions. Our method integrates seamlessly with existing KD frameworks. We validate DTS across multiple KD strategies on vision (CIFAR-100, Tiny-ImageNet) and NLP tasks (GLUE, Dolly, SelfIns, UnNI, S-NI), consistently outperforming static-temperature baselines. Code is available at https://github.com/Sibgat-Ul/DTS.

</details>


### [56] [Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture](https://arxiv.org/abs/2511.13780)
*Nihal Mehta*

Main category: cs.LG

TL;DR: 自注意力可从分布式语义学的投影中推导而来，Transformer 的结构是基于共现统计投影的自然结果，而非任意设计。


<details>
  <summary>Details</summary>
Motivation: 揭示自注意力与分布式语义之间的联系，提供对 Transformer 及其成分（Q/K/V、位置编码、多头注意力）的一种 principled 理解。

Method: 以 GloVe 的共现矩阵为出发点，展示如何将其投影到序列上下文中，推导出查询-键-值的非对称扩展以建模方向性关系，并将位置编码与多头注意力视为对这种投影原理的结构化 refinements。

Result: 证明 Transformer 的代数形式来自这些投影原则而非任意选择；建立从共现统计到注意力机制的清晰路径，解释了分布式语义与序列建模的统一。

Conclusion: Transformer 架构的核心算子和组件可以从投影分解的原则推导出，强调统计共现到注意力的自然过渡，增强对模型设计的理论基础理解。

Abstract: This paper presents a mathematical interpretation of self-attention by connecting it to distributional semantics principles. We show that self-attention emerges from projecting corpus-level co-occurrence statistics into sequence context. Starting from the co-occurrence matrix underlying GloVe embeddings, we demonstrate how the projection naturally captures contextual influence, with the query-key-value mechanism arising as the natural asymmetric extension for modeling directional relationships. Positional encodings and multi-head attention then follow as structured refinements of this same projection principle. Our analysis demonstrates that the Transformer architecture's particular algebraic form follows from these projection principles rather than being an arbitrary design choice.

</details>


### [57] [ScoresActivation: A New Activation Function for Model Agnostic Global Explainability by Design](https://arxiv.org/abs/2511.13809)
*Emanuel Covaci,Fabian Galis,Radu Balan,Daniela Zaharie,Darian Onchis*

Main category: cs.LG

TL;DR: 提出一种可微分的全局可解释性方法，将特征重要性融入模型训练，通过 ScoresActivation 实现嵌入式、端到端的特征排序，并显著提升训练效率和分类性能，同时提供与 SHAP 值一致的全局特征排名。


<details>
  <summary>Details</summary>
Motivation: 当前的事后解释方法与模型训练过程分离，导致解释的可信度和实用性受限；需要一种在训练阶段就能提供高保真、可扩展的全局解释机制。

Method: 引入 ScoresActivation 函数，作为特征排序机制嵌入学习管道，实现在梯度可传递的方式下按贡献度排序特征，并在端到端训练中优化，兼顾预测性能和解释一致性。

Result: 在基准数据集上，得到全局可信、稳定的特征排序，与 SHAP 值和真实重要性一致；保持较高预测性能；特征排序比传统 SHAP 快 150 倍：训练中仅需 2 秒完成排序，相较 SHAP 的 300 秒。并且在 10 个特征（其中 5 个相关）时准确率提升 11.24%，在 16 个特征（5 相关、11 不相关）时提升 29.33%；对无关输入鲁棒。

Conclusion: 本方法弥合模型准确性与可解释性之间的鸿沟，提供可扩展、固有可解释的机器学习框架。

Abstract: Understanding the decision of large deep learning models is a critical challenge for building transparent and trustworthy systems. Although the current post hoc explanation methods offer valuable insights into feature importance, they are inherently disconnected from the model training process, limiting their faithfulness and utility. In this work, we introduce a novel differentiable approach to global explainability by design, integrating feature importance estimation directly into model training. Central to our method is the ScoresActivation function, a feature-ranking mechanism embedded within the learning pipeline. This integration enables models to prioritize features according to their contribution to predictive performance in a differentiable and end-to-end trainable manner. Evaluations across benchmark datasets show that our approach yields globally faithful, stable feature rankings aligned with SHAP values and ground-truth feature importance, while maintaining high predictive performance. Moreover, feature scoring is 150 times faster than the classical SHAP method, requiring only 2 seconds during training compared to SHAP's 300 seconds for feature ranking in the same configuration. Our method also improves classification accuracy by 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant, 11 irrelevant), demonstrating robustness to irrelevant inputs. This work bridges the gap between model accuracy and interpretability, offering a scalable framework for inherently explainable machine learning.

</details>


### [58] [Beyond One-Size-Fits-All: Neural Networks for Differentially Private Tabular Data Synthesis](https://arxiv.org/abs/2511.13893)
*Kai Chen,Chen Gong,Tianhao Wang*

Main category: cs.LG

TL;DR: MargNet combines adaptive marginal selection with neural-network-based generation to improve DP tabular data synthesis, achieving near-best performance of statistical methods with 7x faster speed on sparsely correlated data, and state-of-the-art fidelity on densely correlated data (up to 26% improvement). Code released on GitHub.


<details>
  <summary>Details</summary>
Motivation: The prevailing view in DP tabular data synthesis is that statistical models outperform neural-network (NN) approaches. However, densely correlated datasets present complex dependencies that can overwhelm statistical methods. NNs have potential to model such distributions directly from samples but currently suffer limitations. This work argues for a hybrid approach that leverages the strengths of both families to address data with varying correlation structure.

Method: Introduce MargNet, which integrates successful algorithmic designs of statistical models into neural networks. It uses an adaptive marginal selection strategy and trains the NN to generate data that conforms to the selected marginals, effectively combining marginal-based constraints with neural generation.

Result: On sparsely correlated datasets, MargNet achieves utility close to the best statistical method while providing an average 7× speedup. On densely correlated datasets, it establishes new state-of-the-art, reducing fidelity error by up to 26% compared to the previous best.

Conclusion: MargNet bridges the gap between statistical models and neural networks for DP tabular data synthesis, offering high utility across correlation regimes and practical benefits in speed. The authors release code on GitHub.

Abstract: In differentially private (DP) tabular data synthesis, the consensus is that statistical models are better than neural network (NN)-based methods. However, we argue that this conclusion is incomplete and overlooks the challenge of densely correlated datasets, where intricate dependencies can overwhelm statistical models. In such complex scenarios, neural networks are more suitable due to their capacity to fit complex distributions by learning directly from samples. Despite this potential, existing NN-based algorithms still suffer from significant limitations. We therefore propose MargNet, incorporating successful algorithmic designs of statistical models into neural networks. MargNet applies an adaptive marginal selection strategy and trains the neural networks to generate data that conforms to the selected marginals. On sparsely correlated datasets, our approach achieves utility close to the best statistical method while offering an average 7$\times$ speedup over it. More importantly, on densely correlated datasets, MargNet establishes a new state-of-the-art, reducing fidelity error by up to 26\% compared to the previous best. We release our code on GitHub.\footnote{https://github.com/KaiChen9909/margnet}

</details>


### [59] [Beat the long tail: Distribution-Aware Speculative Decoding for RL Training](https://arxiv.org/abs/2511.13841)
*Zelei Shao,Vikranth Srivatsa,Sanjana Srivastava,Qingyang Wu,Alpay Ariyak,Xiaoxia Wu,Ameen Patel,Jue Wang,Percy Liang,Tri Dao,Ce Zhang,Yiying Zhang,Ben Athiwaratkun,Chenfeng Xu,Junxiong Wang*

Main category: cs.LG

TL;DR: DAS: 使用分布感知的推测解码来加速RL对齐中LLMs的 rollout，利用基于历史rollouts的suffix树构建的自适应无参数草拟器和长度感知的推测策略，在不改变模型输出的前提下，最高可提高50%的 rollout速度。


<details>
  <summary>Details</summary>
Motivation: RL后训练在对齐LLMs时需要巨量rollout，且滚动长度呈长尾分布，极少数长生成占用大部分真实时间；同时历史rollouts揭示了跨训练周期的稳定提示层模式，如何在不损失学习质量的前提下提升效率成为关键。

Method: 提出DAS：1) 基于最近rollouts的自适应非参数草拟器，利用增量维护的后缀树进行草拟；2) 结合长度感知的推测策略，给较长轨迹分配更积极的草拟预算，从而降低主时延并平衡基础和逐字解码成本。该设计利用历史rollout来维持接受度，并在解码阶段实现成本的权衡。

Result: 在数学与代码推理任务上，DAS实现了 rollout 时间最高可减少约50%的加速，同时保持等效的训练曲线和学习质量。

Conclusion: 分布感知的推测解码能够显著加速RL后训练过程中的rollout而不损害学习效果，具有对LLM对齐任务的实际应用价值。

Abstract: Reinforcement learning(RL) post-training has become essential for aligning large language models (LLMs), yet its efficiency is increasingly constrained by the rollout phase, where long trajectories are generated token by token. We identify a major bottleneck:the long-tail distribution of rollout lengths, where a small fraction of long generations dominates wall clock time and a complementary opportunity; the availability of historical rollouts that reveal stable prompt level patterns across training epochs. Motivated by these observations, we propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without altering model outputs. DAS integrates two key ideas: an adaptive, nonparametric drafter built from recent rollouts using an incrementally maintained suffix tree, and a length aware speculation policy that allocates more aggressive draft budgets to long trajectories that dominate makespan. This design exploits rollout history to sustain acceptance while balancing base and token level costs during decoding. Experiments on math and code reasoning tasks show that DAS reduces rollout time up to 50% while preserving identical training curves, demonstrating that distribution-aware speculative decoding can significantly accelerate RL post training without compromising learning quality.

</details>


### [60] [AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection](https://arxiv.org/abs/2511.13880)
*Saleh Momeni,Changnan Xiao,Bing Liu*

Main category: cs.LG

TL;DR: 提出 AnaCP 的类增量学习方法，通过分析性投影实现增量特征自适应，同时保持分析性分类器的高效性，在不进行梯度训练的情况下消除记忆灾难，并达到联合训练的水平。


<details>
  <summary>Details</summary>
Motivation: 在持续学习的类增量学习任务中，CF 仍然是核心难题。尽管把预训练模型（PTM）作为固定特征提取器并结合解析分类器的做法取得了进展，但仍无法对特征表示进行增量自适应，导致性能受限。需要一种能在不进行梯度训练的前提下持续调整特征表示的方案。

Method: 提出 AnaCP（Analytic Contrastive Projection），在保持解析分类器高效性的同时，利用分析性投影实现增量特征自适应，避免梯度更新带来的CF。通过对新任务的特征进行对比学习式的分析性投影，使得特征与固定分类器之间的兼容性提升，达到可增量扩展而不依赖梯度优化的特征更新。

Result: 实验结果显示：AnaCP在多项基线方法上实现显著提升，并达到与联合训练相近甚至相同的准确度，接近CIL的上界。

Conclusion: AnaCP 提供了一个高效且可扩展的类增量学习新范式：在保持分析性分类器优点的同时，通过分析性投影实现增量特征自适应，消除了因梯度训练引发的遗忘问题，接近联合训练的性能上限。

Abstract: This paper studies the problem of class-incremental learning (CIL), a core setting within continual learning where a model learns a sequence of tasks, each containing a distinct set of classes. Traditional CIL methods, which do not leverage pre-trained models (PTMs), suffer from catastrophic forgetting (CF) due to the need to incrementally learn both feature representations and the classifier. The integration of PTMs into CIL has recently led to efficient approaches that treat the PTM as a fixed feature extractor combined with analytic classifiers, achieving state-of-the-art performance. However, they still face a major limitation: the inability to continually adapt feature representations to best suit the CIL tasks, leading to suboptimal performance. To address this, we propose AnaCP (Analytic Contrastive Projection), a novel method that preserves the efficiency of analytic classifiers while enabling incremental feature adaptation without gradient-based training, thereby eliminating the CF caused by gradient updates. Our experiments show that AnaCP not only outperforms existing baselines but also achieves the accuracy level of joint training, which is regarded as the upper bound of CIL.

</details>


### [61] [Certified but Fooled! Breaking Certified Defences with Ghost Certificates](https://arxiv.org/abs/2511.14003)
*Quoc Viet Vo,Tashreque M. Haq,Paul Montague,Tamas Abraham,Ehsan Abbasnejad,Damith C. Ranasinghe*

Main category: cs.LG

TL;DR: 该论文研究通过概率认证框架的恶意利用来理解鲁棒性证明的局限，提出区域聚焦的对抗扰动以伪造证书并获得对目标类别的超大认证半径，同时对 ImageNet 上的 DensePure 等方法进行绕过实验。


<details>
  <summary>Details</summary>
Motivation: 鲁棒性认证提供可证明的鲁棒性，但其安全性与可信度面临挑战，攻击者可能通过伪造证书来误导模型，或提升错误类别的认证半径。

Method: 分析并设计在扰动小且不可感知的前提下，使输入产生错误分类并诱导认证系统发出对目标类的伪证书；提出区域聚焦的对抗样本以实现证书伪装；在 ImageNet 上进行大量实验，比较对 DensePure 等最新认证防御的绕过能力。

Result: 证明存在能够绕过最先进的认证防御并产生比源类别证书更大半径的对抗样本；Region-focused对抗样本能实现不可感知的扰动并 spoof 证书。

Conclusion: 认证鲁棒性存在明显局限性，需要对概率证书框架的鲁棒性进行更深入的理论和实践研究，提升对证书欺骗的防御机制。

Abstract: Certified defenses promise provable robustness guarantees. We study the malicious exploitation of probabilistic certification frameworks to better understand the limits of guarantee provisions. Now, the objective is to not only mislead a classifier, but also manipulate the certification process to generate a robustness guarantee for an adversarial input certificate spoofing. A recent study in ICLR demonstrated that crafting large perturbations can shift inputs far into regions capable of generating a certificate for an incorrect class. Our study investigates if perturbations needed to cause a misclassification and yet coax a certified model into issuing a deceptive, large robustness radius for a target class can still be made small and imperceptible. We explore the idea of region-focused adversarial examples to craft imperceptible perturbations, spoof certificates and achieve certification radii larger than the source class ghost certificates. Extensive evaluations with the ImageNet demonstrate the ability to effectively bypass state-of-the-art certified defenses such as Densepure. Our work underscores the need to better understand the limits of robustness certification methods.

</details>


### [62] [Tractable Probabilistic Models for Investment Planning](https://arxiv.org/abs/2511.13888)
*Nicolas M. Cuadrado A.,Mohannad Takrouri,Jiří Němeček,Martin Takáč,Jakub Mareček*

Main category: cs.LG

TL;DR: 使用可切实可推断的概率模型（Sum-Product Networks，SPN）来对电力系统投资规划中的不确定性进行精确推断，从而实现稳健的情景分析和直接嵌入机会约束的优化，结果在一个代表性案例中显示出相较传统情景模型的计算与可靠性优势。


<details>
  <summary>Details</summary>
Motivation: 在几十年尺度上的能源结构与需求预测存在深刻不确定性，传统的有限情景方法难以揭示情景特定的波动性，影响稳健决策。

Method: 采用可推断的概率模型（TPM），特别是 Sum-Product Networks（SPN），对高维不确定性进行高效、可精确推断，求取情景似然、边缘、条件概率等并嵌入到机会约束优化中；与传统情景模型对比展示优势。

Result: 通过一个代表性的电力系统规划案例，证明 TPM（SPN）在计算效率和可靠性评估方面优于传统基于情景的方法，能够直接进行情景分析与波动性量化。

Conclusion: TPMs（尤其是 SPN）为在深度不确定性下的电力系统投资规划提供一个可扩展、可实现的稳健框架，便于实现机会约束与风险评估。

Abstract: Investment planning in power utilities, such as generation and transmission expansion, requires decade-long forecasts under profound uncertainty. Forecasting of energy mix and energy use decades ahead is nontrivial. Classical approaches focus on generating a finite number of scenarios (modeled as a mixture of Diracs in statistical theory terms), which limits insight into scenario-specific volatility and hinders robust decision-making. We propose an alternative using tractable probabilistic models (TPMs), particularly sum-product networks (SPNs). These models enable exact, scalable inference of key quantities such as scenario likelihoods, marginals, and conditional probabilities, supporting robust scenario expansion and risk assessment.
  This framework enables direct embedding of chance-constrained optimization into investment planning, enforcing safety or reliability with prescribed confidence levels. TPMs allow both scenario analysis and volatility quantification by compactly representing high-dimensional uncertainties. We demonstrate the approach's effectiveness through a representative power system planning case study, illustrating computational and reliability advantages over traditional scenario-based models.

</details>


### [63] [Observational Auditing of Label Privacy](https://arxiv.org/abs/2511.14084)
*Iden Kalemaj,Luca Melis,Maxime Boucher,Ilya Mironov,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Differential privacy (DP) auditing is essential for evaluating privacy guarantees in machine learning systems. Existing auditing methods, however, pose a significant challenge for large-scale systems since they require modifying the training dataset -- for instance, by injecting out-of-distribution canaries or removing samples from training. Such interventions on the training data pipeline are resource-intensive and involve considerable engineering overhead. We introduce a novel observational auditing framework that leverages the inherent randomness of data distributions, enabling privacy evaluation without altering the original dataset. Our approach extends privacy auditing beyond traditional membership inference to protected attributes, with labels as a special case, addressing a key gap in existing techniques. We provide theoretical foundations for our method and perform experiments on Criteo and CIFAR-10 datasets that demonstrate its effectiveness in auditing label privacy guarantees. This work opens new avenues for practical privacy auditing in large-scale production environments.

</details>


### [64] [Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation](https://arxiv.org/abs/2511.14406)
*Bastien Vuillod,Pierre-Alain Moellic,Jean-Max Dutertre*

Main category: cs.LG

TL;DR: 本研究首次分析了在联邦学习中LoRA对后门攻击的影响，发现对最佳注入情形，LoRA秩较低时后门持久性更高，并揭示评估后门攻击在FL中的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在分布式、隐私保护场景中的应用日益增多，理解参数高效微调（如LoRA）对安全性的影响，尤其是后门攻击的持续性与评估公正性，具有重要意义。

Method: 在不同LoRA秩设置下，系统评估后门在FL中的寿命（backdoor lifespan），分析攻击场景与攻击者注入能力对持久性的影响，并提供公开可用的实现代码。

Result: 关键发现：对于最优注入的后门，LoRA秩较低时后门的持久性更长；LoRA对后门攻击的影响取决于具体注入条件，且揭示了评估FL中后门攻击的潜在偏差和问题。

Conclusion: 该工作推动对FL环境下后门攻击评估方法的改进，促进更健壮、公平的安全风险评估，并提升对关键FL系统的可靠性认知。

Abstract: Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.

</details>


### [65] [The Impact of Bootstrap Sampling Rate on Random Forest Performance in Regression Tasks](https://arxiv.org/abs/2511.13952)
*Michał Iwaniuk,Mateusz Jarosz,Bartłomiej Borycki,Bartosz Jezierski,Jan Cwalina,Stanisław Kaźmierczak,Jacek Mańdziuk*

Main category: cs.LG

TL;DR: 调参关键信息：随机森林回归中自举样本比例（BR）对性能影响显著，非默认 BR 常常优于 BR=1.0，且最优 BR 取决于数据特征与噪声水平。


<details>
  <summary>Details</summary>
Motivation: BR 是一个常设默认值（BR=1.0），但其对回归性能的影响尚未充分系统研究；揭示 BR 与数据特征之间的关系有助于更好地定制 RF 回归模型。

Method: 在39个异质回归数据集和16种 RF 配置下，系统性地将 BR 从0.2到5.0进行变化，通过重复两折交叉验证和均方误差评估性能；此外在受控噪声的合成数据上研究偏差-方差权衡。

Result: 最佳 BR 在24个数据集为 BR ≤ 1.0，15个数据集 BR > 1.0，只有4个数据集 BR=1.0 最优；数据集特征与BR偏好存在关联：当全局特征-目标关系强时偏好更高 BR；当局部目标方差较高时偏好更低 BR。合成数据表明低噪声下高 BR 能降低偏差，高噪声下低 BR 能降低方差，验证了偏差-方差权衡。

Conclusion: BR 是一个具有显著影响的超参数，应对回归随机森林进行调优；基于数据集的全局关系与噪声水平给出 BR 的选取方向，提升 RF 回归模型的性能。

Abstract: Random Forests (RFs) typically train each tree on a bootstrap sample of the same size as the training set, i.e., bootstrap rate (BR) equals 1.0. We systematically examine how varying BR from 0.2 to 5.0 affects RF performance across 39 heterogeneous regression datasets and 16 RF configurations, evaluating with repeated two-fold cross-validation and mean squared error. Our results demonstrate that tuning the BR can yield significant improvements over the default: the best setup relied on BR \leq 1.0 for 24 datasets, BR > 1.0 for 15, and BR = 1.0 was optimal in 4 cases only. We establish a link between dataset characteristics and the preferred BR: datasets with strong global feature-target relationships favor higher BRs, while those with higher local target variance benefit from lower BRs. To further investigate this relationship, we conducted experiments on synthetic datasets with controlled noise levels. These experiments reproduce the observed bias-variance trade-off: in low-noise scenarios, higher BRs effectively reduce model bias, whereas in high-noise settings, lower BRs help reduce model variance. Overall, BR is an influential hyperparameter that should be tuned to optimize RF regression models.

</details>


### [66] [Data Whitening Improves Sparse Autoencoder Learning](https://arxiv.org/abs/2511.13981)
*Ashwin Saraswatula,David Klindt*

Main category: cs.LG

TL;DR: PCA whitening of input activations improves interpretability and optimization ease for sparse autoencoders, with a slight drop in reconstruction; suggests whitening as default preprocessing when interpretability is priority.


<details>
  <summary>Details</summary>
Motivation: Interpretability of features learned by SAEs is hindered by correlated inputs; improved optimization landscape could enhance interpretability metrics.

Method: Apply PCA whitening to SAE inputs; theoretical analysis showing convexification; empirical evaluation of ReLU and Top-K SAEs across architectures, widths, and sparsity using SAEBench; measure interpretability metrics (sparse probing accuracy, feature disentanglement) and reconstruction quality.

Result: Whitening consistently improves interpretability metrics and makes optimization landscape more convex; minor decrease in reconstruction quality.

Conclusion: PCA whitening should be considered a default preprocessing step for SAE training when interpretability is prioritized over reconstruction fidelity; the link between sparsity-fidelity and interpretability may be non-optimal under standard training assumptions.

Abstract: Sparse autoencoders (SAEs) have emerged as a promising approach for learning interpretable features from neural network activations. However, the optimization landscape for SAE training can be challenging due to correlations in the input data. We demonstrate that applying PCA Whitening to input activations -- a standard preprocessing technique in classical sparse coding -- improves SAE performance across multiple metrics. Through theoretical analysis and simulation, we show that whitening transforms the optimization landscape, making it more convex and easier to navigate. We evaluate both ReLU and Top-K SAEs across diverse model architectures, widths, and sparsity regimes. Empirical evaluation on SAEBench, a comprehensive benchmark for sparse autoencoders, reveals that whitening consistently improves interpretability metrics, including sparse probing accuracy and feature disentanglement, despite minor drops in reconstruction quality. Our results challenge the assumption that interpretability aligns with an optimal sparsity--fidelity trade-off and suggest that whitening should be considered as a default preprocessing step for SAE training, particularly when interpretability is prioritized over perfect reconstruction.

</details>


### [67] [Node-Level Uncertainty Estimation in LLM-Generated SQL](https://arxiv.org/abs/2511.13984)
*Hilaf Hasson,Ruocheng Guo*

Main category: cs.LG

TL;DR: 通过对生成的 SQL 的 AST 节点进行不确定性评估，提出一个两阶段框架以检测 LLM 生成的 SQL 错误：阶段一对节点进行语义化标注，阶段二用丰富的特征训练分类器预测逐节点错误概率，并将概率视为校准的不确定性，以实现精细诊断、定位与修复。


<details>
  <summary>Details</summary>
Motivation: 现有的基于令牌的置信度通常对结构容器、别名等不敏感，难以对生成的 SQL 进行细粒度诊断。需要一种可解释、对跨数据库鲁棒的节点级不确定性度量，以支持定位、修复和人机协同。

Method: 阶段1：提出语义感知的标注算法，在生成 SQL 与金标准之间为每个 AST 节点分配正确性标签，避免过度惩罚结构性容器或别名变化。阶段2：对每个节点构建丰富的特征（模式感知的、词汇/标识符相关、别名解析、类型兼容、作用域歧义、拼写信号等），并训练一个监督分类器来预测逐节点的错误概率。将这些概率解释为校准的不确定性，用于精细诊断、定位错误位置。

Result: 在多数据库与数据集上，该方法的平均 AUC 比基于 token 的对数概率提升约 27.44%，并在跨数据库评估中保持鲁棒性。除了作为准确性信号，节点级不确定性还能支持定向修复、人机参与审核和后续的选择性执行。

Conclusion: 节点中心、语义化的不确定性估计提供了一个强大且可解释的替代方案，优于聚合序列级置信度，并为精细诊断、定位和后续修复提供可操作的信号。

Abstract: We present a practical framework for detecting errors in LLM-generated SQL by estimating uncertainty at the level of individual nodes in the query's abstract syntax tree (AST). Our approach proceeds in two stages. First, we introduce a semantically aware labeling algorithm that, given a generated SQL and a gold reference, assigns node-level correctness without over-penalizing structural containers or alias variation. Second, we represent each node with a rich set of schema-aware and lexical features - capturing identifier validity, alias resolution, type compatibility, ambiguity in scope, and typo signals - and train a supervised classifier to predict per-node error probabilities. We interpret these probabilities as calibrated uncertainty, enabling fine-grained diagnostics that pinpoint exactly where a query is likely to be wrong. Across multiple databases and datasets, our method substantially outperforms token log-probabilities: average AUC improves by +27.44% while maintaining robustness under cross-database evaluation. Beyond serving as an accuracy signal, node-level uncertainty supports targeted repair, human-in-the-loop review, and downstream selective execution. Together, these results establish node-centric, semantically grounded uncertainty estimation as a strong and interpretable alternative to aggregate sequence level confidence measures.

</details>


### [68] [A Machine Learning-Based Multimodal Framework for Wearable Sensor-Based Archery Action Recognition and Stress Estimation](https://arxiv.org/abs/2511.14057)
*Xianghe Liu,Jiajia Liu,Chuxian Xu,Minghan Wang,Hongbo Peng,Tao Sun,Jiaqi Xu*

Main category: cs.LG

TL;DR: 提出一种基于可穿戴传感器的多模态框架，用于同时进行动作识别和压力估计，适用于射箭等精准运动，可实现实时反馈。"


<details>
  <summary>Details</summary>
Motivation: 精准运动需要生物力学稳定性与心理韧性，传统运动分析系统昂贵且具有侵入性，限制实际训练。研究通过便携式传感设备与机器学习实现非侵入、自然场景下的动作分识与压力评估，提升训练效果。

Method: 使用自研手腕装置，装载加速度计与PPG传感器，采集同步的运动与生理数据。提出平滑差分加速度（SmoothDiff）特征用于动作识别，采用LSTM模型进行动作阶段识别。对PPG信号提取心率变异性(HRV)特征，使用多层感知机(MLP)进行压力等级分类。

Result: 动作识别准确度为96.8%，F1-score为95.9%；压力二分类准确率为80%。

Conclusion: 将运动信号与生理信号有效融合，能为射箭及其他精准运动提供有意义的技术与心理状态洞察，具备实现实时智能反馈系统的潜力。

Abstract: In precision sports such as archery, athletes' performance depends on both biomechanical stability and psychological resilience. Traditional motion analysis systems are often expensive and intrusive, limiting their use in natural training environments. To address this limitation, we propose a machine learning-based multimodal framework that integrates wearable sensor data for simultaneous action recognition and stress estimation. Using a self-developed wrist-worn device equipped with an accelerometer and photoplethysmography (PPG) sensor, we collected synchronized motion and physiological data during real archery sessions. For motion recognition, we introduce a novel feature--Smoothed Differential Acceleration (SmoothDiff)--and employ a Long Short-Term Memory (LSTM) model to identify motion phases, achieving 96.8% accuracy and 95.9% F1-score. For stress estimation, we extract heart rate variability (HRV) features from PPG signals and apply a Multi-Layer Perceptron (MLP) classifier, achieving 80% accuracy in distinguishing high- and low-stress levels. The proposed framework demonstrates that integrating motion and physiological sensing can provide meaningful insights into athletes' technical and mental states. This approach offers a foundation for developing intelligent, real-time feedback systems for training optimization in archery and other precision sports.

</details>


### [69] [CafeMed: Causal Attention Fusion Enhanced Medication Recommendation](https://arxiv.org/abs/2511.14064)
*Kelin Ren,Chan-Yang Ju,Dong-Ho Lee*

Main category: cs.LG

TL;DR: CafeMed 将动态因果推理与跨模态注意力结合，用于个性化且安全的药物推荐，通过将静态因果效应转化为以患者状态为条件的动态权重，并通过 CHARM 建模诊断与处置之间的复杂依存关系，在 MIMIC-III/IV 上显著优于最新基线，达到更高的药物预测准确性并降低药物相互作用风险。


<details>
  <summary>Details</summary>
Motivation: 现有方法将医学实体看作独立特征，忽略它们在药物选择中的协同效应；此外，使用静态因果关系，无法适应患者特定情境和健康状态，限制个性化和安全性。需要同时建模实体间的协同作用和随患者状态动态变化的因果关系。

Method: 提出两大模块：CWG（Causal Weight Generator）将静态因果效应转化为基于患者状态的动态调制权重；CHARM（Channel Harmonized Attention Refinement Module）通过跨模态注意力，捕捉诊断与处置之间的复杂互依关系，强化跨模态特征的协同表达。

Result: 在 MIMIC-III/MIMIC-IV 上的实验显示，CafeMed 显著优于最先进基线，在药物预测准确性方面提升，同时降低药物间相互作用的发生率。

Conclusion: 将动态因果关系和跨模态协同作用融入药物推荐，可实现更贴近临床的个性化安全药物决策；研究结果支持该方法的实际应用潜力，代码公开。

Abstract: Medication recommendation systems play a crucial role in assisting clinicians with personalized treatment decisions. While existing approaches have made significant progress in learning medication representations, they suffer from two fundamental limitations: (i) treating medical entities as independent features without modeling their synergistic effects on medication selection; (ii) employing static causal relationships that fail to adapt to patient-specific contexts and health states. To address these challenges, we propose CafeMed, a framework that integrates dynamic causal reasoning with cross-modal attention for safe and accurate medication recommendation. CafeMed introduces two key components: the Causal Weight Generator (CWG) that transforms static causal effects into dynamic modulation weights based on individual patient states, and the Channel Harmonized Attention Refinement Module (CHARM) that captures complex interdependencies between diagnoses and procedures. This design enables CafeMed to model how different medical conditions jointly influence treatment decisions while maintaining medication safety constraints. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate that CafeMed significantly outperforms state-of-the-art baselines, achieving superior accuracy in medication prediction while maintaining the lower drug--drug interaction rates. Our results indicate that incorporating dynamic causal relationships and cross-modal synergies leads to more clinically-aligned and personalized medication recommendations. Our code is released publicly at https://github.com/rkl71/CafeMed.

</details>


### [70] [CFG-EC: Error Correction Classifier-Free Guidance](https://arxiv.org/abs/2511.14075)
*Nakkyu Yang,Yechan Lee,SooJean Han*

Main category: cs.LG

TL;DR: CFG-EC 通过将未条件噪声预测的误差与条件误差正交化来减少采样误差，从而提升基于 CFG 的方法在低引导下的采样质量和提示对齐。


<details>
  <summary>Details</summary>
Motivation: 在分类无引导（CFG）训练阶段，模型在条件提示与空提示之间随机切换；而在采样阶段，CFG 输出同时包含空与条件提示，导致训练与采样过程中噪声估计不一致，进而影响生成质量。

Method: 提出 CFG-EC，一种可扩展到任意 CFG 基方法的纠正方案，通过 refining 未条件的噪声预测来使未条件误差正交于条件误差，从而避免两者之间的干扰，收敛采样误差上界，提升指导轨迹的稳定性。

Result: 数值实验表明，CFG-EC 相较 CFG 与 CFG++ 更有效地处理未条件分量，在低引导采样模式下实现显著性能提升，并在整体上达到更高的提示对齐。

Conclusion: CFG-EC 为 CFG 基方法提供了一种通用且有效的纠正机制，能更可靠地引导生成过程，降低采样误差上限。

Abstract: Classifier-Free Guidance (CFG) has become a mainstream approach for simultaneously improving prompt fidelity and generation quality in conditional generative models. During training, CFG stochastically alternates between conditional and null prompts to enable both conditional and unconditional generation. However, during sampling, CFG outputs both null and conditional prompts simultaneously, leading to inconsistent noise estimates between the training and sampling processes. To reduce this error, we propose CFG-EC, a versatile correction scheme augmentable to any CFG-based method by refining the unconditional noise predictions. CFG-EC actively realigns the unconditional noise error component to be orthogonal to the conditional error component. This corrective maneuver prevents interference between the two guidance components, thereby constraining the sampling error's upper bound and establishing more reliable guidance trajectories for high-fidelity image generation. Our numerical experiments show that CFG-EC handles the unconditional component more effectively than CFG and CFG++, delivering a marked performance increase in the low guidance sampling regime and consistently higher prompt alignment across the board.

</details>


### [71] [MoE-SpeQ: Speculative Quantized Decoding with Proactive Expert Prefetching and Offloading for Mixture-of-Experts](https://arxiv.org/abs/2511.14102)
*Wenfeng Wang,Jiacheng Liu,Xiaofeng Hou,Xinfeng Xia,Peng Tang,Mingxuan Zhang,Chao Li,Minyi Guo*

Main category: cs.LG

TL;DR: MoE-SpeQ introduces an on-device draft model to predict future expert usage and prefetch their weights from host memory, overlapping data transfer with computation to hide I/O latency and surpass traditional offloading. It uses an adaptive governor guided by an Amortization Roofline Model to tune speculation for hardware, achieving up to 2.34x speedup on Phi-MoE compared to state-of-the-art offloading.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art Mixture-of-Experts (MoE) models demand memory beyond a single accelerator. Offloading experts to host memory incurs a severe I/O bottleneck on PCIe due to data-dependent expert selection, which lies on the critical path of inference.

Method: Co-design of speculative execution and expert offloading named MoE-SpeQ. It uses a small on-device draft model to forecast the sequence of required experts for upcoming tokens, enabling a runtime orchestrator to prefetch these experts from host memory. This prefetching overlaps I/O with computation. An adaptive governor, guided by an Amortization Roofline Model, dynamically tunes the speculation strategy to the hardware.

Result: On memory-constrained devices, MoE-SpeQ achieves up to 2.34× speedup over a state-of-the-art offloading framework for the Phi-MoE model.

Conclusion: This work provides a principled approach to managing data-dependent memory access in resource-limited environments, enabling more accessible MoE inference on commodity hardware.

Abstract: The immense memory requirements of state-of-the-art Mixture-of-Experts (MoE) models present a significant challenge for inference, often exceeding the capacity of a single accelerator. While offloading experts to host memory is a common solution, it introduces a severe I/O bottleneck over the PCIe bus, as the data-dependent nature of expert selection places these synchronous transfers directly on the critical path of execution, crippling performance.
  This paper argues that the I/O bottleneck can be overcome by trading a small amount of cheap, on-device computation to hide the immense cost of data movement. We present MoE-SpeQ, a new inference system built on a novel co-design of speculative execution and expert offloading. MoE-SpeQ employs a small, on-device draft model to predict the sequence of required experts for future tokens. This foresight enables a runtime orchestrator to prefetch these experts from host memory, effectively overlapping the expensive I/O with useful computation and hiding the latency from the critical path. To maximize performance, an adaptive governor, guided by an Amortization Roofline Model, dynamically tunes the speculation strategy to the underlying hardware. Our evaluation on memory-constrained devices shows that for the Phi-MoE model, MoE-SpeQ achieves at most 2.34x speedup over the state-of-the-art offloading framework. Our work establishes a new, principled approach for managing data-dependent memory access in resource-limited environments, making MoE inference more accessible on commodity hardware.

</details>


### [72] [Soft-Label Training Preserves Epistemic Uncertainty](https://arxiv.org/abs/2511.14117)
*Agamdeep Singh,Ashish Tiwari,Hosein Hasanbeig,Priyanshu Gupta*

Main category: cs.LG

TL;DR: 把注释分布作为地面真相来训练模型，而不是将它们折叠为单一标签，这能更好地捕捉 epistemic uncertainty，并在视觉与NLP任务上比硬标签训练在与人类注释的一致性方面更好，同时保持准确度。


<details>
  <summary>Details</summary>
Motivation: 注释的主观性导致标签分布多样，传统的只保留一个标签忽略了不确定性，这样的训练错配模型的置信度与人类感知的多样性。

Method: 将注释分布作为训练目标（软标签），在视觉和NLP任务上比较软标签与硬标签训练，评估KL发散和注释熵的一致性与相关性；并比较准确度。

Result: 软标签训练在KL散度相较人类注释下降约32%，模型对注释熵的相关性提升约61%，在准确度上等同于硬标签训练。

Conclusion: 将注释分布从噪声信号转为对 epistemic uncertainty 的 faithful 表征，模型应学习重现这种不确定性；这改变了对注释分布的理解，从可忽略的噪声到关键的知识来源。

Abstract: Many machine learning tasks involve inherent subjectivity, where annotators naturally provide varied labels. Standard practice collapses these label distributions into single labels, aggregating diverse human judgments into point estimates. We argue that this approach is epistemically misaligned for ambiguous data--the annotation distribution itself should be regarded as the ground truth. Training on collapsed single labels forces models to express false confidence on fundamentally ambiguous cases, creating a misalignment between model certainty and the diversity of human perception. We demonstrate empirically that soft-label training, which treats annotation distributions as ground truth, preserves epistemic uncertainty. Across both vision and NLP tasks, soft-label training achieves 32% lower KL divergence from human annotations and 61% stronger correlation between model and annotation entropy, while matching the accuracy of hard-label training. Our work repositions annotation distributions from noisy signals to be aggregated away, to faithful representations of epistemic uncertainty that models should learn to reproduce.

</details>


### [73] [Synthetic Survival Control: Extending Synthetic Controls for "When-If" Decision](https://arxiv.org/abs/2511.14133)
*Jessy Xinyi Han,Devavrat Shah*

Main category: cs.LG

TL;DR: 提出了 Synthetic Survival Control (SSC) 的方法框架，用于在面板数据中估计时间事件的反事实风险轨迹，解决观测数据中的混杂、删失与治疗异质性问题；在低秩结构下对因果性进行识别与有限样本保证，并在多国癌症治疗数据上验证，结果显示获得新治疗有助于降低事件后风险，提升生存结局。


<details>
  <summary>Details</summary>
Motivation: 在观测数据中估计时间-事件结局的因果效应面临删失、样本有限与治疗指派非随机等挑战，且需要回答“如果在特定干预下事件发生的时机会如何变化”的“when-if”问题，尤其在治疗采用存在异质性与混杂时。

Method: 在面板数据和多期治疗场景中，提出 SSC，通过将目标单位的反事实危险轨迹表示为来自其他单位观察轨迹的加权组合；在一个低秩结构的因果生存分析框架下给出 formal 的识别性结果与有限样本保障；并理论上说明该低秩结构在经典生存模型下自然出现。

Result: 在一个多国癌症治疗数据集上进行实证验证，治疗存在阶段性引入导致准实验环境；结果显示获得新治疗与较低的后期危险轨迹相关，相对于合成对照具显著改进。

Conclusion: SSC 为观测数据中的反事实生存推断提供了一般且可解释的工具，具有广泛适用性，适用于医学、经济学与公共政策中的生存分析场景。

Abstract: Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such "when-if" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.

</details>


### [74] [Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation](https://arxiv.org/abs/2511.14135)
*Promise Ekpo,Saesha Agarwal,Felix Grimm,Lekan Molu,Angelique Taylor*

Main category: cs.LG

TL;DR: 提出Fair-GNE：一个基于约束广义纳什均衡的自我强化学习框架，用于在多智能体学习驱动的需求侧医疗工作负载分配中实现可证实的公平性，且在仿真中获得显著改善和高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体协作/竞争情景中，传统MARL通过后置奖励塑形实现公平，但缺乏可自我执行、不可篡改的公平性约束。需要一个自包含且对各agent不可被单方面改动的公平机制。

Method: 将MARL建模为受约束的广义纳什均衡博弈，使用GNE寻求算法实现群体策略收敛到一个局部有效且安全的均衡；设计一个自适应约束以促成公平的工作量分配；在高保真复苏模拟器中验证。

Result: 在实验中，相比固定惩罚基线，Fair-GNE在工作负载均衡方面显著提升（0.89 vs 0.33 JFI，p<0.01），同时任务成功率达到86%。

Conclusion: 提出的Fair-GNE及其自适应约束执行提供了在大规模多智能体学习医疗系统中实现公平的原理、评估指标和均衡寻求创新。

Abstract: Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\ 0.33 JFI, $p < 0.01$) while maintaining 86\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.

</details>


### [75] [A Comprehensive Study of Implicit and Explicit Biases in Large Language Models](https://arxiv.org/abs/2511.14153)
*Fatima Kazi,Alex Young,Yash Inani,Setareh Rafatirad*

Main category: cs.LG

TL;DR: 提出一个自动化的偏见识别框架以检测大语言模型中的显性和隐性社会偏见，并在偏见基准上评估多模型，结合提示技术和数据增强实现约20%的隐性偏见提升。


<details>
  <summary>Details</summary>
Motivation: LLMs在训练数据中继承偏见，影响公平性和安全性；需要系统化的识别和缓解方法，并通过StereoSet、CrowSPairs等基准评估。

Method: 建立两步法来检测显性与隐性偏见；开发自动化Bias-Identification Framework；在文本数据中使用Bag-of-Words分析；对BERT、GPT-3.5等模型进行微调、提示技巧与数据增强；进行跨数据集测试。

Result: 微调模型对性别偏见表现不佳却在种族偏见检测上表现良好；模型过度依赖关键词；Bag-of-Words表明存在隐性定型；通过提示和数据增强的微调提升在隐性偏见基准上的性能，提升幅度可达约20%。

Conclusion: 自动化偏见识别与针对性微调对改进LLM偏见缓解有帮助，但在性别等领域仍存在挑战；需要持续开发以实现更全面且稳健的公平性与安全性。

Abstract: Large Language Models (LLMs) inherit explicit and implicit biases from their training datasets. Identifying and mitigating biases in LLMs is crucial to ensure fair outputs, as they can perpetuate harmful stereotypes and misinformation. This study highlights the need to address biases in LLMs amid growing generative AI. We studied bias-specific benchmarks such as StereoSet and CrowSPairs to evaluate the existence of various biases in multiple generative models such as BERT and GPT 3.5. We proposed an automated Bias-Identification Framework to recognize various social biases in LLMs such as gender, race, profession, and religion. We adopted a two-pronged approach to detect explicit and implicit biases in text data. Results indicated fine-tuned models struggle with gender biases but excelled at identifying and avoiding racial biases. Our findings illustrated that despite having some success, LLMs often over-relied on keywords. To illuminate the capability of the analyzed LLMs in detecting implicit biases, we employed Bag-of-Words analysis and unveiled indications of implicit stereotyping within the vocabulary. To bolster the model performance, we applied an enhancement strategy involving fine-tuning models using prompting techniques and data augmentation of the bias benchmarks. The fine-tuned models exhibited promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.

</details>


### [76] [Bridging the Gap Between Bayesian Deep Learning and Ensemble Weather Forecasts](https://arxiv.org/abs/2511.14218)
*Xinlei Xiong,Wenbo Hu,Shuxun Zhou,Kaifeng Bi,Lingxi Xie,Ying Liu,Richang Hong,Qi Tian*

Main category: cs.LG

TL;DR: 提出一个混合贝叶斯深度学习框架，用于集合天气预报，将预测不确定性分解为知识不确定性（epistemic）和本征不确定性（aleatoric），通过变分推断和物理信息驱动的随机扰动来建模，理论上将BDL与EPS联系起来。基于ERA5数据集进行大尺度验证，结果在准确性、校准和计算效率方面优于现有的概率扩散模型，并计划开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决传统集合预报（EPS）与贝叶斯深度学习（BDL）之间的割裂，提供可扩展且对流场依赖型不确定性友好的概率天气预报方法，结合物理约束与数据驱动来提升预测与不确定性量化的一致性。

Method: 提出一个混合BDL框架：使用变分推断来建模 epistemic 不确定性；通过物理信息驱动的随机扰动来建模 aleatoric（与大气动力学相关的）不确定性；将预测不确定性分解为两部分，并建立与EPS的统一理论框架和定理。对 large-scale ERA5（1979-2019，0.25°）进行评估，测试包括预测准确性、校准性及计算效率，且与概率扩散模型相比具备更高效率。

Result: 实验表明该方法在预测准确性和不确定性校准方面优于对比方法，并在计算效率上优于最先进的概率扩散模型。研究还承诺在论文接受后开源代码。

Conclusion: 提出的混合BDL框架成功将BDL与EPS统一在一个理论与实践体系中，提供对流场驱动的不确定性分解以及更高效的概率天气预报途径，具备良好推广潜力。

Abstract: Weather forecasting is fundamentally challenged by the chaotic nature of the atmosphere, necessitating probabilistic approaches to quantify uncertainty. While traditional ensemble prediction (EPS) addresses this through computationally intensive simulations, recent advances in Bayesian Deep Learning (BDL) offer a promising but often disconnected alternative. We bridge these paradigms through a unified hybrid Bayesian Deep Learning framework for ensemble weather forecasting that explicitly decomposes predictive uncertainty into epistemic and aleatoric components, learned via variational inference and a physics-informed stochastic perturbation scheme modeling flow-dependent atmospheric dynamics, respectively. We further establish a unified theoretical framework that rigorously connects BDL and EPS, providing formal theorems that decompose total predictive uncertainty into epistemic and aleatoric components under the hybrid BDL framework. We validate our framework on the large-scale 40-year ERA5 reanalysis dataset (1979-2019) with 0.25° spatial resolution. Experimental results show that our method not only improves forecast accuracy and yields better-calibrated uncertainty quantification but also achieves superior computational efficiency compared to state-of-the-art probabilistic diffusion models. We commit to making our code open-source upon acceptance of this paper.

</details>


### [77] [EBind: a practical approach to space binding](https://arxiv.org/abs/2511.14229)
*Jim Broadbent,Felix Cohen,Frederik Hvilshøj,Eric Landau,Eren Sasoglu*

Main category: cs.LG

TL;DR: 提出了一种数据中心化、参数高效的跨模态对齐方法 EBind，通过单模态编码器和高质量数据，将1.8B参数的多模态模型推向SOTA，训练成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 降低跨模态对齐模型的训练成本与资源需求，实现在单GPU上数小时完成训练，提升数据效率和可扩展性，同时通过数据驱动的方式提升模型性能。

Method: 提出 EBind（Easy, data-centric, parameter-efficient Bind）方法，绑定多位对比模型的嵌入空间，采用单一编码器覆盖图像/文本/视频/音频/3D 多模态，并以三类数据源（大规模自动化的五模态数据、经过人类标注的1M 三元组及多标签、以及3.4M 现有的标题数据）进行数据驱动训练与对比学习；提供13种评估来衡量各数据源的价值，并首次引入高质量共识注释的音频与计算机视觉之间的零样本分类基准，最后对外开源代码、模型权重与数据集。

Result: 1.8B 参数的图像-文本-视频-音频-3D模型在多模态对齐任务上实现了优于比它大4到17倍规模模型的性能。通过13项评估验证各数据源的贡献，且在对比实验中体现出数据驱动的优势。首次建立高质量共识标注的零-shot 音频-PCs分类基准。

Conclusion: 数据的高质量与选择性使用，以及简化的模型结构，共同实现了高效且强的跨模态对齐能力，证明了数据驱动方法在跨模态学习中的潜力，并承诺开源。

Abstract: We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.

</details>


### [78] [Object-Centric World Models for Causality-Aware Reinforcement Learning](https://arxiv.org/abs/2511.14262)
*Yosuke Nishimoto,Takashi Matsubara*

Main category: cs.LG

TL;DR: STICA introduces an object-centric world model using a Slot Transformer, paired with causality-aware policy and value networks, to handle high-dimensional, non-stationary, object-rich environments with improved sample efficiency and final performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of learning accurate world models for environments that are high-dimensional, non-stationary, and composed of multiple interacting objects is difficult with holistic representations. Emulating human object-centric perception can improve decision-making efficiency.

Method: Represent observations as a set of object-centric tokens plus action and reward tokens. Use a Slot Transformer as the world model to predict token-level dynamics and interactions. Train policy and value networks to infer token-level causal relations and incorporate them into attention mechanisms, guiding decision-making causally.

Result: Experiments on object-rich benchmarks show STICA achieves superior sample efficiency and final performance compared to state-of-the-art agents.

Conclusion: Object-centric world models with causality-aware decision-making enable more sample-efficient and high-performing reinforcement learning in environments with many interacting objects.

Abstract: World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.

</details>


### [79] [Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention](https://arxiv.org/abs/2511.14265)
*Rui Zhang,Chao Li,Kezhong Liu,Chen Wang,Bolong Zheng,Hongbo Jiang*

Main category: cs.LG

TL;DR: 提出一个统一的可解释多模态船舶轨迹预测框架，结合持续与瞬态导航意向：使用历史轨迹构建持续意向树、用CVAE建模瞬态意向、并通过非局部注意力保持全局一致性，在AIS数据上实现ADE/FDE提升与可解释性增强。


<details>
  <summary>Details</summary>
Motivation: 解决现有船舶多模态轨迹预测在场景适用性不足与可解释性不足的问题，提出一个可解释的统一框架来覆盖更多场景并揭示预测背后的导航意向。

Method: 提出持续意向树来从历史轨迹中提取稳定的意向；使用条件变分自编码器(CVAE)建模动态瞬态意向；引入非局部注意力以保持全局场景一致性；并在输出中显式揭示每条预测轨迹的导航意向。

Result: 在真实AIS数据集上，方法对多样场景具有广泛适用性，并在ADE和FDE指标上取得显著改进；同时提升可解释性，明确揭示每条预测轨迹所对应的导航意向。

Conclusion: 该框架实现了可解释性和广泛适用性的平衡，展示了将持续与瞬态意向结合的有效性，并通过全局注意力机制和CVAE实现对复杂海况的鲁棒多模态预测。

Abstract: Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.

</details>


### [80] [Comparing Task-Agnostic Embedding Models for Tabular Data](https://arxiv.org/abs/2511.14276)
*Frederik Hoppe,Lars Kleinemeier,Astrid Franz,Udo Göbel*

Main category: cs.LG

TL;DR: 简洁的发现是：在表格数据任务中，用简单的 TableVectorizer 特征就能达到与或超越基于大规模 TabPFN/TabICL 的表示学习性能，并且速度快得多（可达上千倍加速），表明对表示学习的过度依赖并非必要。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估基于表格数据的基础模型在任务无关的表示学习方面的能力，即获得可迁移的、任务无关的嵌入。作者将 TabPFN、TabICL 等表格基础模型的任务无关表示与经典特征工程方法 TableVectorizer 进行对比，并在不同应用任务上评估其效果。

Method: 系统性地在多任务设定下评估任务无关的表示：对比 TabPFN、TabICL 与 TableVectorizer 在异常检测 ADBench 和监督学习 TabArena Lite 等任务上的表现，并考察速度与效率，以代码实现 TabEmbedBench 的基线评估框架。

Result: 结果表明，TableVectorizer 的特征在大多数任务中达到可比甚至优于基于基础模型的表示学习，且速度比 TabPFN/TabICL 快许多，达到了最多三个数量级的提升。

Conclusion: 结论强调，在表格数据领域，任务无关的高容量嵌入并非必要；经过简单的特征工程（TableVectorizer）便可获得竞争性表现且显著提高效率。并给出开源代码 TabEmbedBench。

Abstract: Recent foundation models for tabular data achieve strong task-specific performance via in-context learning. Nevertheless, they focus on direct prediction by encapsulating both representation learning and task-specific inference inside a single, resource-intensive network. This work specifically focuses on representation learning, i.e., on transferable, task-agnostic embeddings. We systematically evaluate task-agnostic representations from tabular foundation models (TabPFN and TabICL) alongside with classical feature engineering (TableVectorizer) across a variety of application tasks as outlier detection (ADBench) and supervised learning (TabArena Lite). We find that simple TableVectorizer features achieve comparable or superior performance while being up to three orders of magnitude faster than tabular foundation models. The code is available at https://github.com/ContactSoftwareAI/TabEmbedBench.

</details>


### [81] [H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata](https://arxiv.org/abs/2511.14312)
*Chenyang Xu,Siming Li,Hao Wang*

Main category: cs.LG

TL;DR: 提出了一种分层潜在扩散模型H-LDM，用结构化元数据实现对心音信号的可控生成，解决PCG数据标注不足的问题。通过多尺度VAE实现生理解耦的潜在空间、分层文本到生物信号的管线对17种条件进行控制、以及Medical Attention模块引导的可解释扩散过程，在PhysioNet CirCor数据集上达到FAD 9.7、92%属性解耦、87.1%临床有效性，并能提升罕见疾病诊断准确率11.3%。


<details>
  <summary>Details</summary>
Motivation: 解决心血管疾病诊断中标注稀缺的病理PCG数据，提出可控且具有临床解释性的合成数据以增强AI系统性能。

Method: 1) 多尺度VAE学习生理上解耦的潜在空间（节律、心音、杂音）；2) 利用丰富临床元数据的分层文本到生物信号管线，实现对17种条件的细粒度控制；3) 通过新颖的Medical Attention模块引导的可解释扩散过程；整体组成H-LDM。

Result: 在PhysioNet CirCor数据集上实现FAD = 9.7、属性解耦率92%、临床有效性87.1%，使用合成数据提升罕见疾病分类准确率11.3%。

Conclusion: 为心脏诊断中的数据增强开辟新方向，结合可解释性临床洞察与数据稀缺之间的桥梁。

Abstract: Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG signals from structured metadata. Our approach features: (1) a multi-scale VAE that learns a physiologically-disentangled latent space, separating rhythm, heart sounds, and murmurs; (2) a hierarchical text-to-biosignal pipeline that leverages rich clinical metadata for fine-grained control over 17 distinct conditions; and (3) an interpretable diffusion process guided by a novel Medical Attention module. Experiments on the PhysioNet CirCor dataset demonstrate state-of-the-art performance, achieving a Fréchet Audio Distance of 9.7, a 92% attribute disentanglement score, and 87.1% clinical validity confirmed by cardiologists. Augmenting diagnostic models with our synthetic data improves the accuracy of rare disease classification by 11.3\%. H-LDM establishes a new direction for data augmentation in cardiac diagnostics, bridging data scarcity with interpretable clinical insights.

</details>


### [82] [Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect](https://arxiv.org/abs/2511.14317)
*Yuwen Zhang,Viet Tran,Paul Weng*

Main category: cs.LG

TL;DR: 提出两种用于鲁棒模型评估与选择的工具：干预效率（IE）和扰动验证框架（PVF）。IE在有限干预条件下衡量模型识别可行动真阳性（actionable TP）的能力，强调与临床实用性的关系；PVF在数据扰动下评估模型性能的稳定性，帮助识别在验证集噪声或分布偏移中保持最稳定的模型。通过对合成和真实医疗数据的实证，表明这两工具有助于选出泛化更稳健且符合容量约束的模型，从而缓解临床场景中的Rashomon效应。


<details>
  <summary>Details</summary>
Motivation: 在临床机器学习中，存在多种性能相当的模型（Rashomon效应），这给可信部署和评估带来挑战。小样本、不均衡、噪声数据，以及高维、弱识别特征，放大了模型多样性，使传统验证方案不可靠，单纯用F1等指标难以在资源和操作优先级约束下做出选择。

Method: 提出两种互补工具：干预效率（IE）和扰动验证框架（PVF）。IE是一个容量感知的度量，量化在有限干预条件下模型识别“可行动”的真阳性（true positives）效率，强调预测性能与临床实用性之间的联系；PVF提供一个结构化的方法，在带有数据扰动的情形下评估模型的稳定性，识别在噪声或分布偏移的验证集上表现最不易受影响的模型。对合成和真实医疗数据集的实验表明，该工具组合有助于选择具有更强泛化性且符合容量约束的模型。

Result: 实验结果表明，使用IE和PVF可以更有效地分辨出在实际部署中更具鲁棒性和可用性的模型，相较仅凭传统指标更能对Rashomon效应进行缓解。

Conclusion: 两种工具为临床设置中的模型选择提供了新方向：在关注容量约束的前提下，结合性能与臨床实用性进行评估，从而提升鲁棒性和可部署性。

Abstract: In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.

</details>


### [83] [Learning with Statistical Equality Constraints](https://arxiv.org/abs/2511.14320)
*Aneesh Barthakur,Luiz F. O. Chamon*

Main category: cs.LG

TL;DR: Generalization theory for equality-constrained learning; a practical algorithm converting equality-constrained problems into a sequence of unconstrained empirical problems; demonstrated in fair learning, interpolating classifiers, and boundary value problems.


<details>
  <summary>Details</summary>
Motivation: As ML systems become more complex, fixing accuracy is insufficient. Tuning penalties for multiple requirements is hard, particularly when constraints involve equalities (e.g., fairness, boundary values). Existing constrained approaches lack generalization guarantees for equality constraints, creating a gap between theory and practice.

Method: Develop a generalization theory for equality-constrained statistical learning and show that solutions can be approximated from finite samples with rich parameterizations. Propose a practical algorithm that solves a sequence of unconstrained, empirical learning problems to enforce equality constraints.

Result: The theory establishes that equality-constrained solutions can be approximated using finite samples. The proposed algorithm is effective in practice and enables new formulations driven by equality constraints in areas such as fair learning, interpolating classifiers, and boundary value problems.

Conclusion: Equality constraints extend the scope of learnable problems and provide a path to principled generalization guarantees. The proposed approach offers a practical, scalable way to incorporate equality constraints into ML training for broader applications.

Abstract: As machine learning applications grow increasingly ubiquitous and complex, they face an increasing set of requirements beyond accuracy. The prevalent approach to handle this challenge is to aggregate a weighted combination of requirement violation penalties into the training objective. To be effective, this approach requires careful tuning of these hyperparameters (weights), involving trial-and-error and cross-validation, which becomes ineffective even for a moderate number of requirements. These issues are exacerbated when the requirements involve parities or equalities, as is the case in fairness and boundary value problems. An alternative technique uses constrained optimization to formulate these learning problems. Yet, existing approximation and generalization guarantees do not apply to problems involving equality constraints. In this work, we derive a generalization theory for equality-constrained statistical learning problems, showing that their solutions can be approximated using samples and rich parametrizations. Using these results, we propose a practical algorithm based on solving a sequence of unconstrained, empirical learning problems. We showcase its effectiveness and the new formulations enabled by equality constraints in fair learning, interpolating classifiers, and boundary value problems.

</details>


### [84] [Enforcing hidden physics in physics-informed neural networks](https://arxiv.org/abs/2511.14348)
*Nanxi Chen,Sifan Wang,Rujin Ma,Airong Chen,Chuanjie Cui*

Main category: cs.LG

TL;DR: 提出了一种对PINN进行不可逆性正则化的方法，将隐藏的热力学第二定律不可逆性作为软约束加入训练，以确保学习得到的解符合单向的不可逆过程。


<details>
  <summary>Details</summary>
Motivation: 现有PINN在训练过程中往往忽略隐藏的不可逆性（如热力学第二定律），可能导致不物理的解或训练失败。需要在物理先验基础上加入不可逆性约束以提高解的物理一致性和稳定性。

Method: 提出一个简单、广义且鲁棒的不可逆性正则化策略，将其作为软约束融入PINN训练中，强制学习的解遵循不可逆过程的单向性，且对现有PINN框架改动极少。

Result: 在旅行波传播、定常燃烧、冰融解、腐蚀演化和裂纹扩展等多种基准上，正则化显著降低预测误差（超过一个数量级），同时几乎不需要对现有PINN框架做大规模修改。

Conclusion: 所提出的不可逆性正则化框架具有广泛适用性，可应用于广义的PDE驱动物理系统，预计对科研机器学习领域产生重要影响。

Abstract: Physics-informed neural networks (PINNs) represent a new paradigm for solving partial differential equations (PDEs) by integrating physical laws into the learning process of neural networks. However, despite their foundational role, the hidden irreversibility implied by the Second Law of Thermodynamics is often neglected during training, leading to unphysical solutions or even training failures in conventional PINNs. In this paper, we identify this critical gap and introduce a simple, generalized, yet robust irreversibility-regularized strategy that enforces hidden physical laws as soft constraints during training. This approach ensures that the learned solutions consistently respect the intrinsic one-way nature of irreversible physical processes. Across a wide range of benchmarks spanning traveling wave propagation, steady combustion, ice melting, corrosion evolution, and crack propagation, we demonstrate that our regularization scheme reduces predictive errors by more than an order of magnitude, while requiring only minimal modification to existing PINN frameworks. We believe that the proposed framework is broadly applicable to a wide class of PDE-governed physical systems and will have significant impact within the scientific machine learning community.

</details>


### [85] [Toward Robust and Harmonious Adaptation for Cross-modal Retrieval](https://arxiv.org/abs/2511.14416)
*Haobin Li,Mouxing Yang,Xi Peng*

Main category: cs.LG

TL;DR: 提出REST，在跨模态检索中应对在线且多样化的查询漂移问题。通过对查询预测进行在线 refined、设计查询漂移鲁棒的目标函数，以及使用梯度解耦模块防止对通用知识的遗忘，从而实现对QS的鲁棒自适应，且在20个基准、3个跨模态检索任务上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决源模型在目标域数据分布错位、以及在线、 Diverse 查询带来的查询漂移（QS），避免在自适应过程中遗忘通用跨模态检索知识。

Method: 提出REST：1) 在线移客带来的查询预测的再构造与对齐，形成QS鲁棒的目标函数；2) 使用梯度解耦模块在适应阶段操控梯度，防止遗忘一般知识，保持跨模态通用空间。

Result: 在20个基准数据集、3个CMR任务上验证有效性，展现对查询漂移的鲁棒性与对通用知识的保留，优于对比方法。

Conclusion: REST实现了在线且和谐的自适应，缓解查询漂移对跨模态检索的挑战，使模型在实际场景中的适用性显著提升。

Abstract: Recently, the general-to-customized paradigm has emerged as the dominant approach for Cross-Modal Retrieval (CMR), which reconciles the distribution shift problem between the source domain and the target domain. However, existing general-to-customized CMR methods typically assume that the entire target-domain data is available, which is easily violated in real-world scenarios and thus inevitably suffer from the query shift (QS) problem. Specifically, query shift embraces the following two characteristics and thus poses new challenges to CMR. i) Online Shift: real-world queries always arrive in an online manner, rendering it impractical to access the entire query set beforehand for customization approaches; ii) Diverse Shift: even with domain customization, the CMR models struggle to satisfy queries from diverse users or scenarios, leaving an urgent need to accommodate diverse queries. In this paper, we observe that QS would not only undermine the well-structured common space inherited from the source model, but also steer the model toward forgetting the indispensable general knowledge for CMR. Inspired by the observations, we propose a novel method for achieving online and harmonious adaptation against QS, dubbed Robust adaptation with quEry ShifT (REST). To deal with online shift, REST first refines the retrieval results to formulate the query predictions and accordingly designs a QS-robust objective function on these predictions to preserve the well-established common space in an online manner. As for tackling the more challenging diverse shift, REST employs a gradient decoupling module to dexterously manipulate the gradients during the adaptation process, thus preventing the CMR model from forgetting the general knowledge. Extensive experiments on 20 benchmarks across three CMR tasks verify the effectiveness of our method against QS.

</details>


### [86] [FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis](https://arxiv.org/abs/2511.14419)
*Xiaowei Xu,Justin Sonneck,Hongxiao Wang,Roman Burkard,Hendrik Wohrle,Anton Grabmasier,Matthias Gunzer,Jianxu Chen*

Main category: cs.LG

TL;DR: FlowRoI 通过基于光流的 ROI 提取实现高吞吐量免疫细胞迁移视频的 ROI 感知压缩，提供与 JPEG2000 相近的速度并在相同 PSNR 条件下实现更高的数据压缩比。


<details>
  <summary>Details</summary>
Motivation: 应对高吞吐成像平台（如 ComplexEye）带来的数据爆炸式增长，需高效的存储与传输方案，同时保持迁移细胞的定量分析能力。

Method: 利用相邻帧之间的光流估计，推导覆盖几乎所有迁移细胞的 RoI 掩模；将原始图像与 RoI 掩模联合编码为 JPEG2000，从而实现 RoI 感知压缩；在 i7-1255U CPU 的现代笔记本上实现约 30 帧/秒的运行时，且在细胞区域获得更高的 PSNR，并在匹配 PSNR 时实现 2.0–2.2 倍的压缩率。

Result: FlowRoI 的吞吐量接近标准 JPEG2000 的水平，细胞区域的 PSNR 更高；在相同 PSNR 条件下，压缩率提升约 2.0–2.2 倍。

Conclusion: 基于光流的 RoI 提取与联合编码的 FlowRoI 为高吞吐免疫细胞迁移研究提供一种高效的 ROI 感知压缩解决方案，便于数据管理并具备向临床转化的潜力。

Abstract: Autonomous migration is essential for the function of immune cells such as neutrophils and plays a pivotal role in diverse diseases. Recently, we introduced ComplexEye, a multi-lens array microscope comprising 16 independent aberration-corrected glass lenses arranged at the pitch of a 96-well plate, capable of capturing high-resolution movies of migrating cells. This architecture enables high-throughput live-cell video microscopy for migration analysis, supporting routine quantification of autonomous motility with strong potential for clinical translation. However, ComplexEye and similar high-throughput imaging platforms generate data at an exponential rate, imposing substantial burdens on storage and transmission. To address this challenge, we present FlowRoI, a fast optical-flow-based region of interest (RoI) extraction framework designed for high-throughput image compression in immune cell migration studies. FlowRoI estimates optical flow between consecutive frames and derives RoI masks that reliably cover nearly all migrating cells. The raw image and its corresponding RoI mask are then jointly encoded using JPEG2000 to enable RoI-aware compression. FlowRoI operates with high computational efficiency, achieving runtimes comparable to standard JPEG2000 and reaching an average throughput of about 30 frames per second on a modern laptop equipped with an Intel i7-1255U CPU. In terms of image quality, FlowRoI yields higher peak signal-to-noise ratio (PSNR) in cellular regions and achieves 2.0-2.2x higher compression rates at matched PSNR compared to standard JPEG2000.

</details>


### [87] [MiAD: Mirage Atom Diffusion for De Novo Crystal Generation](https://arxiv.org/abs/2511.14426)
*Andrey Okhotin,Maksim Nakhodnov,Nikita Kazeev,Andrey E Ustyuzhanin,Dmitry Vetrov*

Main category: cs.LG

TL;DR:  Mirage infusion 让扩散模型在晶体生成中也能改变原子数量，显著提升质量与 S.U.N. 比例；MiAD 在 MP-20 数据集达到 8.2% 的 S.U.N. 率，且超越现有方法，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的晶体生成往往固定原子数，限制了采样轨迹和多样性；需要让模型在生成过程中动态改变原子数量以探索更大设计空间。

Method: 提出 mirage infusion 技术，使晶体中的原子状态可在存在/非存在之间切换；提出一个等变联合扩散模型 MiAD，能够在生成过程中改变原子数，进行去 novo 晶体生成。

Result: 相较同模型未修改的情况下，质量提升最多约 2.5 倍；在 MP-20 数据集实现 8.2% 的 S.U.N. 率，显著优于现有方法。

Conclusion:  Mirage Atom Diffusion（MiAD）为去 novo 晶体生成提供了有效的原子数动态调整能力，mirage infusion 作为简单而强大的技术手段，提升了生成质量与多样性；代码公开于 GitHub。

Abstract: In recent years, diffusion-based models have demonstrated exceptional performance in searching for simultaneously stable, unique, and novel (S.U.N.) crystalline materials. However, most of these models don't have the ability to change the number of atoms in the crystal during the generation process, which limits the variability of model sampling trajectories. In this paper, we demonstrate the severity of this restriction and introduce a simple yet powerful technique, mirage infusion, which enables diffusion models to change the state of the atoms that make up the crystal from existent to non-existent (mirage) and vice versa. We show that this technique improves model quality by up to $\times2.5$ compared to the same model without this modification. The resulting model, Mirage Atom Diffusion (MiAD), is an equivariant joint diffusion model for de novo crystal generation that is capable of altering the number of atoms during the generation process. MiAD achieves an $8.2\%$ S.U.N. rate on the MP-20 dataset, which substantially exceeds existing state-of-the-art approaches. The source code can be found at \href{https://github.com/andrey-okhotin/miad.git}{\texttt{github.com/andrey-okhotin/miad}}.

</details>


### [88] [Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters](https://arxiv.org/abs/2511.14452)
*Emanuele Palumbo,Sorawit Saengkyongam,Maria R. Cervera,Jens Behrmann,Andrew C. Miller,Guillermo Sapiro,Christina Heinze-Deml,Antoine Wehenkel*

Main category: cs.LG

TL;DR: 提出一种混合方法：通过对PPG-APW配对数据训练的条件变分自编码器（cVAE）和在带标签的模拟APW片段上训练的条件密度估计器，结合仿真与未标注临床数据来从PPG推断心血管关键 biomarkers（SV和CO），并在监测时间变化上优于有监督基线。


<details>
  <summary>Details</summary>
Motivation: 解决从无创PPG预测关键心脏生物标志物（如 SV、CO）所面临的数据稀缺与信号不充分问题，利用物理仿真与未标注数据实现从PPG到血流力学指标的准确估计。

Method: 1) 使用条件变分自编码器（cVAE），在成对的PPG-APW数据上学习PPG到潜在表征的映射；2) 在带标签的模拟APW片段上训练条件密度估计器，以估计心脏生物标志物的条件分布；3) 将两者结合，直接从PPG估计SV和CO的分布并监测其时间变化。

Result: 实验表明该混合模型能够检测CO和SV的波动，并在监测这些生物标志物的时间变化方面优于有监督基线。

Conclusion: 将物理仿真与无标注数据相结合的混合数据驱动方法，可实现非侵入性心血管生物标志物的更优跟踪，与纯监督方法相比具有更强的时间变化检测能力。

Abstract: Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.

</details>


### [89] [nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers](https://arxiv.org/abs/2511.14465)
*Clément Dumas*

Main category: cs.LG

TL;DR: nnterp is a lightweight wrapper around NNsight that unifies transformer analysis across 50+ models and 16 architecture families, preserving HuggingFace behavior while standardizing interfaces and adding validation tests.


<details>
  <summary>Details</summary>
Motivation: There is a fundamental tradeoff in mechanistic interpretability tooling: custom pipelines offer consistency but require architecture-specific adaptations with potential numerical drift, while direct HuggingFace access preserves behavior but lacks cross-model standardization. A unified, validated tool is needed to enable reliable cross-architecture analysis.

Method: nnterp wraps NNsight with automatic module renaming to produce a consistent interface across architectures, includes built-in interpretability methods (logit lens, patchscope, activation steering), exposes attention probabilities where supported, and packages comprehensive validation tests so researchers can verify compatibility with custom models locally across 50+ variants and 16 architecture families.

Result: A unified, compatible interface is produced that preserves original HuggingFace implementations while enabling cross-model analysis, reduces numerical mismatch risk, and provides validation coverage to ensure compatibility with bespoke models.

Conclusion: nnterp bridges correctness and usability in mechanistic interpretability tooling by enabling one-time intervention code to run across a broad spectrum of transformer architectures with validated reliability.

Abstract: Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.

</details>


### [90] [Notes on Kernel Methods in Machine Learning](https://arxiv.org/abs/2511.14485)
*Diego Armando Pérez-Rosero,Danna Valentina Salazar-Dubois,Juan Camilo Lugo-Rojas,Andrés Marino Álvarez-Meza,Germán Castellanos-Dominguez*

Main category: cs.LG

TL;DR: 以Hilbert空间为基础的核方法自洽入门，系统构建正定核、RKHS、Hilbert–Schmidt算子等工具，并将协方差、回归、信息量等经典概念映射到几何框架，涵盖核密度估计、分布嵌入与MMD，为高斯过程、核贝叶斯推断等后续主题奠定基础。


<details>
  <summary>Details</summary>
Motivation: 期望把核方法与几何/函数分析视角统一起来，提供一个自洽的理论框架，便于统计估计和分布表示的分析与实现。

Method: 从构造希尔伯特空间出发，系统发展正定核与RKHS的性质、Hilbert–Schmidt算子及其在估计与分布表示中的作用；通过该几何视角重新审视协方差、回归和信息度量；介绍核密度估计、分布的核嵌入以及最大均值差(MMD)等工具；为后续主题（高斯过程、核贝叶斯推断、函数分析方法在ML中的应用）打基础。

Result: 提供一个自洽的理论与方法论框架，使得核方法的统计推断、分布比较和知识表示能够在RKHS与Hilbert–Schmidt的框架内统一处理。

Conclusion: 该笔记将作为更高级主题的基础性教材，鼓励以几何-分析视角推进核方法在现代机器学习中的应用与理论发展。

Abstract: These notes provide a self-contained introduction to kernel methods and their geometric foundations in machine learning. Starting from the construction of Hilbert spaces, we develop the theory of positive definite kernels, reproducing kernel Hilbert spaces (RKHS), and Hilbert-Schmidt operators, emphasizing their role in statistical estimation and representation of probability measures. Classical concepts such as covariance, regression, and information measures are revisited through the lens of Hilbert space geometry. We also introduce kernel density estimation, kernel embeddings of distributions, and the Maximum Mean Discrepancy (MMD). The exposition is designed to serve as a foundation for more advanced topics, including Gaussian processes, kernel Bayesian inference, and functional analytic approaches to modern machine learning.

</details>


### [91] [Towards Stable and Structured Time Series Generation with Perturbation-Aware Flow Matching](https://arxiv.org/abs/2511.14488)
*Jintao Zhang,Mingyue Cheng,Zirui Liu,Xianquan Wang,Yitong Zhou,Qi Liu*

Main category: cs.LG

TL;DR: PAFM 提出 Perturbation-Aware Flow Matching 框架，通过扰动引导训练、双路径速度场与混合专家解码实现对扰动轨迹的建模，从而提升时间序列生成的结构一致性，且在无条件与有条件生成任务中超越基线。


<details>
  <summary>Details</summary>
Motivation: 时间序列生成需要保持结构一致性，但局部扰动导致的时间异质性使现有的流匹配方法难以捕捉瞬变与非平稳性。

Method: 引入扰动感知的流匹配框架，使用双路径速度场捕捉扰动下的轨迹偏离，采用扰动引导训练来模拟局部扰动，并使用 mixture-of-experts 解码器通过流路由动态分配建模容量以适应不同轨迹动力学。

Result: 在无条件和条件生成任务上，PAFM 相对于强基线具有一致的性能提升，实验表明对扰动的敏感性与表达能力显著增强。

Conclusion: PAFM 能有效建模扰动轨迹、提升结构一致性，并通过更具表达力的解码器实现对扰动的自适应建模；代码已公开。

Abstract: Time series generation is critical for a wide range of applications, which greatly supports downstream analytical and decision-making tasks. However, the inherent temporal heterogeneous induced by localized perturbations present significant challenges for generating structurally consistent time series. While flow matching provides a promising paradigm by modeling temporal dynamics through trajectory-level supervision, it fails to adequately capture abrupt transitions in perturbed time series, as the use of globally shared parameters constrains the velocity field to a unified representation. To address these limitations, we introduce \textbf{PAFM}, a \textbf{P}erturbation-\textbf{A}ware \textbf{F}low \textbf{M}atching framework that models perturbed trajectories to ensure stable and structurally consistent time series generation. The framework incorporates perturbation-guided training to simulate localized disturbances and leverages a dual-path velocity field to capture trajectory deviations under perturbation, enabling refined modeling of perturbed behavior to enhance the structural coherence. In order to further improve sensitivity to trajectory perturbations while enhancing expressiveness, a mixture-of-experts decoder with flow routing dynamically allocates modeling capacity in response to different trajectory dynamics. Extensive experiments on both unconditional and conditional generation tasks demonstrate that PAFM consistently outperforms strong baselines. Code is available at https://anonymous.4open.science/r/PAFM-03B2.

</details>


### [92] [CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design](https://arxiv.org/abs/2511.14510)
*Jiawei Yi,Ping Gong,Youhui Bai,Jiaqi Ruan,Shengnan Wang,Pengcheng Wang,Haibo Wang,Weiguang Wang,Xia Zhu,Feng Wu,Cheng Li*

Main category: cs.LG

TL;DR: CLO是一种CPU轻量级KVCache下沉框架，通过算法-系统协同实现对大语言模型推理的高效内存与传输管理。与现有系统相比，在保持相近精度的前提下显著降低CPU开销，充分利用PCIe带宽，解码吞吐量提升约9.3%~66.6%。


<details>
  <summary>Details</summary>
Motivation: 随着百万级别token的LLM增长，KVCache成为推理中的内存与数据传输瓶颈。现有下沉系统多关注CPU端缓存管理开销、CPU端聚集操作导致的PCIe带宽低效，以及CPU主导的同步造成的GPU端空闲，因此仍未解决CPU瓶颈与传输瓶颈的综合问题。

Method: 提出CLO：1) 粗粒度的逐头(head-wise)近似的GPU端缓存策略，缓存管理开销极低；2) 数据预取与GPU端持久缓存的无缝结合，降低传输开销；3) 零拷贝传输引擎，充分挖掘PCIe带宽；4) 以GPU为中心的同步机制，消除GPU等待。

Result: 在两种广泛使用的LLM上评估，CLO在保持与最先进系统相当的精度的同时，显著降低CPU开销，充分利用PCIe带宽，使解码吞吐量提升在9.3%到66.6%之间。

Conclusion: 算法-系统协同设计对于现代GPU平台上内存受限的LLM推理至关重要，CLO已开源实现，展示了面向大规模KVCache的有效加速思路。

Abstract: The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.

</details>


### [93] [MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation](https://arxiv.org/abs/2511.14543)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 提出一种混合确定性扩散框架用于混合类型表格缺失值填充，通过将连续数值特征用 DDIM 作为确定性去噪通道、将离散/类别特征用离散潜在路径扩散通道分离建模，并在统一条件填充目标下共同训练，以提升精度、稳定性和对 MCAR/MAR/MNAR 的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据常同时包含数值、分类和离散属性，现有扩散模型通常假设同质特征空间且依赖随机去噪，导致条件一致性差、类别变量信息塌陷或数值变量更新不稳定。单一扩散过程难以有效处理混合类型缺失问题，因此需要结构化的扩散框架来提升填充质量与稳定性。

Method: 提出一个混合确定性扩 diffusion 框架，将异质特征分离为两条互补的生成通道：一条基于 DDIM 的连续通道用于数值变量的高效确定性去噪；另一条受 loopholing-based 离散扩散启发的离散潜在路径扩散通道用于类别/离散特征，确保留在有效样本流形。两条通道在统一的条件填充目标下共同训练。

Result: 在多组真实数据集上进行广泛实验，所提框架在填充精度、采样轨迹稳定性以及对 MCAR、MAR、MNAR 的鲁棒性方面优于现有扩散及经典方法。

Conclusion: 结构感知的扩散过程对混合类型缺失表格数据的深度学习填充具有重要意义，混合通道设计能提升填充质量与稳定性。

Abstract: Incomplete data are common in real-world tabular applications, where numerical, categorical, and discrete attributes coexist within a single dataset. This heterogeneous structure presents significant challenges for existing diffusion-based imputation models, which typically assume a homogeneous feature space and rely on stochastic denoising trajectories. Such assumptions make it difficult to maintain conditional consistency, and they often lead to information collapse for categorical variables or instability when numerical variables require deterministic updates. These limitations indicate that a single diffusion process is insufficient for mixed-type tabular imputation.
  We propose a hybrid deterministic diffusion framework that separates heterogeneous features into two complementary generative channels. A continuous DDIM-based channel provides efficient and stable deterministic denoising for numerical variables, while a discrete latent-path diffusion channel, inspired by loopholing-based discrete diffusion, models categorical and discrete features without leaving their valid sample manifolds. The two channels are trained under a unified conditional imputation objective, enabling coherent reconstruction of mixed-type incomplete data.
  Extensive experiments on multiple real-world datasets show that the proposed framework achieves higher imputation accuracy, more stable sampling trajectories, and improved robustness across MCAR, MAR, and MNAR settings compared with existing diffusion-based and classical methods. These results demonstrate the importance of structure-aware diffusion processes for advancing deep learning approaches to incomplete tabular data.

</details>


### [94] [Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction](https://arxiv.org/abs/2511.14544)
*Jaume Ros,Alessio Arleo,Fernando Paulovich*

Main category: cs.LG

TL;DR: 提出 Warping Index（WI）作为新的二维降维投影质量度量，聚焦于保持点之间的空白区域以提升投影的视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的投影质量度量（PQMs）多关注全局或局部结构的保留，而忽略投影图中的视觉失真、异常点或伪影，可能导致误导性的分析结论。需要一个与人类视觉感知更一致的度量，尤其关注点之间的空白区域在投影中的保持。

Method: 在二维投影中基于“正确保留空白区域”的假设构建度量框架 WI，用以量化投影对空白区域的保留程度，从而评估投影的扭曲。摘要未给出具体的计算细节或算法实现，因此这里以概念层面的描述呈现。

Result: 摘要未给出具体实验结果、对比或定量评估，明确提出了 WI 的概念与目标。

Conclusion: WI 作为对二维降维投影可视化质量的补充性度量，旨在通过关注空白区域的保留来改善对投影失真的检测，帮助避免因视觉伪影而导致的误导性分析。

Abstract: Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.

</details>


### [95] [Task Addition and Weight Disentanglement in Closed-Vocabulary Models](https://arxiv.org/abs/2511.14569)
*Adam Hazimeh,Alessandro Favero,Pascal Frossard*

Main category: cs.LG

TL;DR: 在闭词汇视觉模型中应用任务相加，揭示权重解耦为普遍现象，能够实现高效的多任务编辑与部署；线性探测同样具竞争力。


<details>
  <summary>Details</summary>
Motivation: 在开放词汇模型中，任务算术被用来高效地编辑模型；但对未使用语言监督、闭词汇的预训练模型的适用性尚未研究。

Method: 系统地在闭词汇视觉变换器上测试任务相加，比较不同预训练方案，分析权重解耦的出现，并与线性探测基线比较。

Result: 预训练导致的权重解耦是一种通用现象；闭词汇视觉变换器也可以通过任务相加进行有效编辑，获得较高的任务添加性能；线性探测在此任务中也具竞争力。

Conclusion: 扩大任务算术的适用范围，促进多任务高效部署，并强调线性探测是一个强有力的基线。

Abstract: Task arithmetic has recently emerged as a promising method for editing pre-trained \textit{open-vocabulary} models, offering a cost-effective alternative to standard multi-task fine-tuning. However, despite the abundance of \textit{closed-vocabulary} models that are not pre-trained with language supervision, applying task arithmetic to these models remains unexplored. In this paper, we deploy and study task addition in closed-vocabulary image classification models. We consider different pre-training schemes and find that \textit{weight disentanglement} -- the property enabling task arithmetic -- is a general consequence of pre-training, as it appears in different pre-trained closed-vocabulary models. In fact, we find that pre-trained closed-vocabulary vision transformers can also be edited with task arithmetic, achieving high task addition performance and enabling the efficient deployment of multi-task models. Finally, we demonstrate that simple linear probing is a competitive baseline to task addition. Overall, our findings expand the applicability of task arithmetic to a broader class of pre-trained models and open the way for more efficient use of pre-trained models in diverse settings.

</details>


### [96] [ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents](https://arxiv.org/abs/2511.14584)
*Ankush Kadu,Ashwanth Krishnan*

Main category: cs.LG

TL;DR: 提出 ReflexGrad，将LLM的分层任务分解、历史反思的因果分析与梯度优化三种机制耦合，实现零样本泛化并在 ALFWorld 上展现强初次暴露表现和跨任务转移。


<details>
  <summary>Details</summary>
Motivation: 在强化学习与决策任务中实现从经验学习并跨任务泛化仍是核心挑战。现有方法通常独立使用记忆、提示优化或层次化任务分解，缺乏综合协同效应与零样本通用性。

Method: 三大机制耦合：1) 基于LLM的分层TODO分解用于策略性规划；2) 面向历史的因果反思，分析近期行动模式以发现失败根因并实现同场学习；3) 梯度优化以系统性改进。与以往仅依赖少量示例不同，该系统通过纯LLM语义推理实现真正零样本泛化，无需任务特定示例、微调或硬编码相似度度量。

Result: 在 ALFWorld 基准任务上，ReflexGrad 在 Trial 0 的零样本成功率达到 67%，首次暴露就表现出有效能力。通过实证分析，揭示了导致稳定收敛（零动作循环）和有效跨任务转移（提升 67% 到 78%）的架构机制。

Conclusion: 证明三种互补学习机制的协同集成可实现鲁棒的零样本泛化，其性能接近先前工作中的少样本基线。

Abstract: Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.

</details>


### [97] [Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare](https://arxiv.org/abs/2511.14619)
*Marco Locatelli,Arjen Hommersom,Roberto Clemens Cerioli,Daniela Besozzi,Fabio Stella*

Main category: cs.LG

TL;DR: 使用专家知识的模糊伪计数来引导POMDP参数估计，提出Fuzzy MAP EM，在数据不足时对参数学习更稳健，且在医疗场景中实现数据高效建模并产生临床一致的POMDP。


<details>
  <summary>Details</summary>
Motivation: 在数据有限的情况下从POMDP参数中学习是一项挑战；通过将先验知识以模糊信息融入EM框架，转化为MAP估计，以在数据稀缺和噪声较大时提供更可靠的参数估计。

Method: 将基于专家定义的模糊模型产生的模糊伪计数引入到EM框架，形成Fuzzy MAP EM算法，使学习过程自然地成为MAP优化并对参数进行正则化，引导在有限数据下的学习。

Result: 在合成医学仿真中，Fuzzy MAP EM在数据量较少和噪声较高的条件下，稳定地优于标准EM算法；在Myasthenia Gravis的案例研究中，能够恢复出一个临床一致的POMDP，显示其作为医疗领域数据高效建模的潜在实用性。

Conclusion: Fuzzy MAP EM为数据稀缺环境下的POMDP参数学习提供了一种有效的融入专家知识的学习框架，具有在医疗等领域实现数据高效建模的潜在应用与价值。

Abstract: Learning the parameters of Partially Observable Markov Decision Processes (POMDPs) from limited data is a significant challenge. We introduce the Fuzzy MAP EM algorithm, a novel approach that incorporates expert knowledge into the parameter estimation process by enriching the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model. This integration naturally reformulates the problem as a Maximum A Posteriori (MAP) estimation, effectively guiding learning in environments with limited data. In synthetic medical simulations, our method consistently outperforms the standard EM algorithm under both low-data and high-noise conditions. Furthermore, a case study on Myasthenia Gravis illustrates the ability of the Fuzzy MAP EM algorithm to recover a clinically coherent POMDP, demonstrating its potential as a practical tool for data-efficient modeling in healthcare.

</details>


### [98] [Failure to Mix: Large language models struggle to answer according to desired probability distributions](https://arxiv.org/abs/2511.14630)
*Ivy Yuqian Yang,David Yu Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of "1" 49% of the time produces an answer of "0" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.

</details>


### [99] [Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting](https://arxiv.org/abs/2511.14632)
*Yuchen Luo,Xinyu Li,Liuhua Peng,Mingming Gong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \textbf{channel-independent} (CI) or \textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \textbf{A}daptive \textbf{C}hannel \textbf{E}nhancer (\textbf{ACE}) for enriching embedding processes and the \textbf{A}daptive \textbf{C}hannel \textbf{F}orecaster (\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [100] [DualLaguerreNet: A Decoupled Spectral Filter GNN and the Uncovering of the Flexibility-Stability Trade-off](https://arxiv.org/abs/2511.13729)
*Huseyin Goksu*

Main category: eess.SP

TL;DR: DualLaguerreNet decouples spectral filters into low/high frequency components, enabling two independent adaptive Laguerre filters, improving heterophily performance but risking overfitting on homophily due to a flexibility-stability trade-off.


<details>
  <summary>Details</summary>
Motivation: Tackle the compromise of single adaptive spectral filters that average responses across the graph spectrum, aiming to better handle heterophily and over-smoothing while understanding the bias-variance implications.

Method: Split the graph Laplacian into L_low and L_high, learn two separate adaptive Laguerre polynomial filters with parameters alpha_1 and alpha_2, effectively doubling filter and model parameters.

Result: Achieves state-of-the-art results on complex heterophilic tasks (outperforming LaguerreNet) but underperforms on simple homophilic tasks, indicating overfitting and a flexibility-stability trade-off.

Conclusion: Proposes a heterophily-focused SOTA architecture while highlighting the inherent bias-variance trade-off in adaptive GNN filter design; simpler models may act as a useful regularizer for homophily tasks.

Abstract: Graph Neural Networks (GNNs) based on spectral filters, such as the Adaptive Orthogonal Polynomial Filter (AOPF) class (e.g., LaguerreNet), have shown promise in unifying the solutions for heterophily and over-smoothing. However, these single-filter models suffer from a "compromise" problem, as their single adaptive parameter (e.g., alpha) must learn a suboptimal, averaged response across the entire graph spectrum. In this paper, we propose DualLaguerreNet, a novel GNN architecture that solves this by introducing "Decoupled Spectral Flexibility." DualLaguerreNet splits the graph Laplacian into two operators, L_low (low-frequency) and L_high (high-frequency), and learns two independent, adaptive Laguerre polynomial filters, parameterized by alpha_1 and alpha_2, respectively. This work, however, uncovers a deeper finding. While our experiments show DualLaguerreNet's flexibility allows it to achieve state-of-the-art results on complex heterophilic tasks (outperforming LaguerreNet), it simultaneously underperforms on simpler, homophilic tasks. We identify this as a fundamental "Flexibility-Stability Trade-off". The increased parameterization (2x filter parameters and 2x model parameters) leads to overfitting on simple tasks, demonstrating that the "compromise" of simpler models acts as a crucial regularizer. This paper presents a new SOTA architecture for heterophily while providing a critical analysis of the bias-variance trade-off inherent in adaptive GNN filter design.

</details>


### [101] [GegenbauerNet: Finding the Optimal Compromise in the GNN Flexibility-Stability Trade-off](https://arxiv.org/abs/2511.13730)
*Huseyin Goksu*

Main category: eess.SP

TL;DR: GegenbauerNet在[-1,1]域中提出一阶对称、单参量的Gegenbauer滤波器以在灵活性与稳定性之间取得折中；在K=2的局部滤波和异质性图上表现优于0/2参数模型，表明受控的对称自由度在某些场景最优。


<details>
  <summary>Details</summary>
Motivation: 解决在[-1,1]谱域下的灵活性-稳定性权衡问题。前序工作表明2参数的自适应L-JacobiNet易产生高方差，在某些场景甚至劣于0参数、稳定的S-JacobiNet，需寻找合适的折中策略。

Method: 提出GegenbauerNet，利用Gegenbauer多项式构造滤波器，强制对称（alpha=beta），仅学习一个形状参数lambda，从而限制灵活性以降低方差，同时避免完全固定的S-JacobiNet偏置。通过K=2的局部滤波、异质性图的实验，以及跨7个数据集的K消融研究，比较全自适应L-JacobiNet与其他基线。

Result: 在关键局部滤波场景（K=2）和异质性图上，1参数的GegenbauerNet实现了优于其他模型的性能，表明受控的对称自由度在该区域更有利；而在高K滤波任务中，完全自适应的L-JacobiNet仍呈现最高性能，体现最大灵活性在可控正则化下的价值。

Conclusion: 在[-1,1]谱域中，最优滤波器需依据目标局部性K及可接受的设计偏置来选择。GegenbauerNet提供的设计原则是：对称性约束与单自由度的折中，在低至中等K的场景下往往优于高度自适应的方案；而高K场景则可能需要更高的灵活性以提高性能。

Abstract: Spectral Graph Neural Networks (GNNs) operating in the canonical [-1, 1] domain (like ChebyNet and its adaptive generalization, L-JacobiNet) face a fundamental Flexibility-Stability Trade-off. Our previous work revealed a critical puzzle: the 2-parameter adaptive L-JacobiNet often suffered from high variance and was surprisingly outperformed by the 0-parameter, stabilized-static S-JacobiNet. This suggested that stabilization was more critical than adaptation in this domain. In this paper, we propose \textbf{GegenbauerNet}, a novel GNN filter based on the Gegenbauer polynomials, to find the Optimal Compromise in this trade-off. By enforcing symmetry (alpha=beta) but allowing a single shape parameter (lambda) to be learned, GegenbauerNet limits flexibility (variance) while escaping the fixed bias of S-JacobiNet. We demonstrate that GegenbauerNet (1-parameter) achieves superior performance in the key local filtering regime (K=2 on heterophilic graphs) where overfitting is minimal, validating the hypothesis that a controlled, symmetric degree of freedom is optimal. Furthermore, our comprehensive K-ablation study across homophilic and heterophilic graphs, using 7 diverse datasets, clarifies the domain's behavior: the fully adaptive L-JacobiNet maintains the highest performance on high-K filtering tasks, showing the value of maximum flexibility when regularization is managed. This study provides crucial design principles for GNN developers, showing that in the [-1, 1] spectral domain, the optimal filter depends critically on the target locality (K) and the acceptable level of design bias.

</details>


### [102] [Cross-Sparsity-Enabled Multipath Perception via Structured Bayesian Inference for Multi-Target Estimation](https://arxiv.org/abs/2511.14051)
*Xiang Chen,Ming-Min Zhao,An Liu,Min Li,Qingjiang Shi,Min-Jian Zhao*

Main category: eess.SP

TL;DR: 提出三层层次结构的跨稀疏先验（3LHS）并结合结构化快速涡轮变分贝叶斯推断（SF-TVBI），在多径环境中利用第一阶散射路径的结构信息提升多目标角度估计的精度，且计算复杂度低于Turbo-VBI，性能相近。


<details>
  <summary>Details</summary>
Motivation: 在多目标、多径场景中，第一阶散射路径的到达/发射角与直接路径的角度相匹配，携带可用的先验结构信息；通过跨稀疏建模和高效推断来提升角度估计的鲁棒性与精度。

Method: 提出3LHS跨稀疏先验模型来对多目标感知信道进行稀疏表示；设计结构化快速 turbo 变分贝叶斯推断（SF-TVBI），结合消息传递实现跨稀疏内部的概率交换，并引入两时尺度更新以减少高维稀疏向量的更新频率。

Result: 仿真结果显示利用跨稀疏结构可显著提升目标角度估计精度；SF-TVBI在性能上与Turbo-VBI相近但具有更低的计算复杂度。

Conclusion: 基于3LHS跨稀疏先验的SF-TVBI提供了一个高效且精确的推断框架，适用于复杂多径环境中的多目标角度估计与定位任务。

Abstract: In this paper, we investigate a multi-target sensing system in multipath environment, where inter-target scattering gives rise to first-order reflected paths whose angles of departure (AoDs) and angles of arrival (AoAs) coincide with the direct-path angles of different targets. Unlike other multipath components, these first-order paths carry structural information that can be exploited as additional prior knowledge for target direction estimation. To exploit this property, we construct a sparse representation of the multi-target sensing channel and propose a novel cross sparsity structure under a three-layer hierarchical structured (3LHS) prior model, which leverages the first-order paths to enhance the prior probability of the direct paths and thereby improve the estimation accuracy. Building on this model, we propose a structured fast turbo variational Bayesian inference (SF-TVBI) algorithm, which integrates an efficient message-passing strategy to enable tractable probabilistic exchange within the cross sparsity, and a two-timescale update scheme to reduce the update frequency of the high-dimensional sparse vector. Simulation results demonstrate that leveraging the proposed cross sparsity structure is able to improve the target angle estimation accuracy substantially, and the SF-TVBI algorithm achieves estimation performance comparable to that of the Turbo-VBI, but with lower computational complexity.

</details>


### [103] [Lightweight Multi-task CNN for ECG Diagnosis with GRU-Diffusion](https://arxiv.org/abs/2511.14104)
*Lehuai Xu,Zirui Lu,Haoran Yang,Yina Zhou*

Main category: eess.SP

TL;DR: 提出一个轻量级多任务框架 DFNet，用于在边缘设备上对 ECG 进行实时多任务分类，并通过 GRU 融合扩散模型来处理数据不平衡问题，在 MIT-BIH 与 PTB 数据集上均取得高精度且参数量明显降低。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的边缘设备上进行实时 ECG 分类的挑战，特别是在不平衡数据集上的准确性和实时性需求。

Method: 提出 Multi-task DFNet 这一轻量级多任务框架，支持跨任务的知识动态共享；引入 GRU 装入扩散模型以捕捉时序依赖并生成高质量的合成信号用于处理类别不平衡；涉及任务包括心律失常检测、心肌梗死（MI）分类及其他心血管异常；评估在 MIT-BIH 和 PTB 数据库上。

Result: 在 MIT-BIH 与 PTB 数据集上分别实现 99.72% 和 99.89% 的准确率；参数量显著低于传统模型，适合在可穿戴 ECG 监测设备上部署。

Conclusion: 该方法提供一种紧凑高效的多任务 ECG 诊断解决方案，对资源受限设备的边缘医疗应用具有良好前景。

Abstract: With the increasing demand for real-time Electrocardiogram (ECG) classification on edge devices, existing models face challenges of high computational cost and limited accuracy on imbalanced datasets.This paper presents Multi-task DFNet, a lightweight multi-task framework for ECG classification across the MIT-BIH Arrhythmia Database and the PTB Diagnostic ECG Database, enabling efficient task collaboration by dynamically sharing knowledge across tasks, such as arrhythmia detection, myocardial infarction (MI) classification, and other cardiovascular abnormalities. The proposed method integrates GRU-augmented Diffusion, where the GRU is embedded within the diffusion model to capture temporal dependencies better and generate high-quality synthetic signals for imbalanced classes. The experimental results show that Multi-task DFNet achieves 99.72% and 99.89% accuracy on the MIT-BIH dataset and PTB dataset, respectively, with significantly fewer parameters compared to traditional models, making it suitable for deployment on wearable ECG monitors. This work offers a compact and efficient solution for multi-task ECG diagnosis, providing a promising potential for edge healthcare applications on resource-constrained devices.

</details>


### [104] [A Patient-Independent Neonatal Seizure Prediction Model Using Reduced Montage EEG and ECG](https://arxiv.org/abs/2511.14110)
*Sithmini Ranasingha,Agasthi Haputhanthri,Hansa Marasinghe,Nima Wickramasinghe,Kithmin Wickremasinghe,Jithangi Wanigasinghe,Chamira U. S. Edussooriya,Joshua P. Kulasingham*

Main category: eess.SP

TL;DR: 基于CNN的多模态EEG/ECG特征用于早期预测新生儿癫痫，能够在发作前30分钟内给出高准确度警报。


<details>
  <summary>Details</summary>
Motivation: 新生儿癫痫诊断困难和延迟治疗风险高，现有的连续视频脑电图监测成本高且需要专业人员，需开发低成本、可移植且泛化能力强的预测模型。

Method: 提出患者无关的CNN模型，输入特征为多通道EEG和ECG信号的梅尔频率倒谱系数(MFCC)矩阵；结合注意力机制；在Helsinki新生儿EEG数据集上进行10折交叉验证，评估准确率、灵敏度、特异性、F1；对比EEG与EEG+ECG，使用SHAP进行解释并通过头皮图定位癫痫焦点；通过迁移学习实现对未见受试者的泛化。

Result: 平均准确度97.52%，灵敏度98.31%，特异性96.39%，F1 97.95%；EEG+ECG使F1提高1.42%，注意力机制再增0.5%；能在发作前长达30分钟预测；SHAP解释增强模型透明度，头皮图定位癫痫焦点；具备对未见对象的良好泛化能力。

Conclusion: 该方法在新生儿NICU中具有潜在的可解释、低监督部署能力，且通过迁移学习实现对新受试者的泛化，支持早期、可靠的癫痫预测。

Abstract: Neonates are highly susceptible to seizures, often leading to short or long-term neurological impairments. However, clinical manifestations of neonatal seizures are subtle and often lead to misdiagnoses. This increases the risk of prolonged, untreated seizure activity and subsequent brain injury. Continuous video electroencephalogram (cEEG) monitoring is the gold standard for seizure detection. However, this is an expensive evaluation that requires expertise and time. In this study, we propose a convolutional neural network-based model for early prediction of neonatal seizures by distinguishing between interictal and preictal states of the EEG. Our model is patient-independent, enabling generalization across multiple subjects, and utilizes mel-frequency cepstral coefficient matrices extracted from multichannel EEG and electrocardiogram (ECG) signals as input features. Trained and validated on the Helsinki neonatal EEG dataset with 10-fold cross-validation, the proposed model achieved an average accuracy of 97.52%, sensitivity of 98.31%, specificity of 96.39%, and F1-score of 97.95%, enabling accurate seizure prediction up to 30 minutes before onset. The inclusion of ECG alongside EEG improved the F1-score by 1.42%, while the incorporation of an attention mechanism yielded an additional 0.5% improvement. To enhance transparency, we incorporated SHapley Additive exPlanations (SHAP) as an explainable artificial intelligence method to interpret the model and provided localization of seizure focus using scalp plots. The overall results demonstrate the model's potential for minimally supervised deployment in neonatal intensive care units, enabling timely and reliable prediction of neonatal seizures, while demonstrating strong generalization capability across unseen subjects through transfer learning.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [105] [DNA Storage in the Short Molecule Regime](https://arxiv.org/abs/2511.14284)
*Ran Tamir,Nir Weinberger,Albert Guillén i Fàbregas*

Main category: cs.IT

TL;DR: 完成对一个关于在短分子DNA存储系统中可可靠存储信息量随规模的缩放的 conjecture 的证明。通过对每个码字进行量化的随机生成概率质量函数的随机编码方案进行分析，并对最优化最大似然解码器推导出可实现性界，且该界与最近建立的反例界在整个短分子区域都一致。此外，提出第二种低复杂度的编码方案，在大多数情况下也能实现最优缩放，唯一在非常短的分子范围内的某些情形例外。


<details>
  <summary>Details</summary>
Motivation: 阐明在短分子DNA存储系统中可可靠存储信息的极限缩放规律，完成 Shomorony 与 Heckel (2022) 关于该领域的猜想，建立编码-解码策略的可实现性界并与对立界相匹配。

Method: 使用从概率简单形中随机抽取的概率质量函数量化后得到的码字进行随机编码；对最优的最大似然解码器进行分析以推导可实现性界，并提出第二种计算复杂度显著降低的编码方案。

Result: 给出可实现性界，与最近建立的对立界在整個短分子区间相匹配；并提出第二种低复杂度方案，在除了极短分子的特定范围外实现最优缩放。

Conclusion: 论文解决了关于短分子DNA存储中信息量缩放的猜想，证明了两种编码方案在不同复杂度–性能权衡下实现了最优或近似最优的缩放，提供了理论上的极限与可行路径。

Abstract: We study the amount of reliable information that can be stored in a DNA-based storage system composed of short DNA molecules. In this regime, Shomorony and Heckel (2022) put forward a conjecture on the scaling of the number of information bits that can be reliably stored. In this paper, we complete the proof of this conjecture. We analyze a random-coding scheme in which each codeword is obtained by quantizing a randomly generated probability mass function drawn from the probability simplex. By analyzing the optimal maximum-likelihood decoder, we derive an achievability bound that matches a recently established converse bound across the entire short-molecule regime. We also propose a second coding scheme, which operates with significantly lower computational complexity but achieves the optimal scaling, except for a specific range of very short molecules.

</details>


### [106] [The Capacity of Collusion-Resilient Decentralized Secure Aggregation with Groupwise Keys](https://arxiv.org/abs/2511.14444)
*Zhou Li,Xiang Zhang,Yizhou Zhao,Haiqiang Chen,Jihao Fan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 在基于组密钥的去中心化安全聚合（DSA）中，给出可达的广播率与组密钥率的最优区域；当G=1或G≥K−T时不可行；且在2≤G<K−T时，为安全计算每用户至少广播一个符号，且每个组密钥至少含有(K−T−2)/C(K−T−1,G)个独立符号。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中的隐私保护聚合问题，结合信息论安全约束与组密钥结构，研究在实际组密钥设置下的最小通信与密钥开销。

Method: 基于信息论分析，推导最优率区域。通过对恢复性与安全性约束的并行考虑，给出G的分段可行性与下界。

Result: 给出可达到的最优广播率与组密钥率的区域界限；G=1或G≥K−T不可行；对于2≤G<K−T，给出每用户最少广播符号数和每组密钥最少独立符号数的具体下界。

Conclusion: 揭示带组密钥的DSA的基本极限，为在去中心化学习中的安全聚合设计提供通信与密钥效率之间的权衡指南。

Abstract: This paper investigates the information-theoretic decentralized secure aggregation (DSA) problem under practical groupwise secret keys and collusion resilience. In DSA, $K$ users are interconnected through error-free broadcast channels. Each user holds a private input and aims to compute the sum of all other users' inputs, while satisfying the security constraint that no user, even when colluding with up to $T$ other users, can infer any information about the inputs beyond the recovered sum. To ensure security, users are equipped with secret keys to mask their inputs. Motivated by recent advances in efficient group-based key generation protocols, we consider the symmetric groupwise key setting, where every subset of $G$ users shares a group key that is independent of all other group keys. The problem is challenging because the recovery and security constraints must hold simultaneously for all users, and the structural constraints on the secret keys limit the flexibility of key correlations. We characterize the optimal rate region consisting of all achievable pairs of per-user broadcast communication rate and groupwise key rate. In particular, we show that DSA with groupwise keys is infeasible when $G=1$ or $G\ge K-T$. Otherwise, when $2\le G<K-T$, to securely compute one symbol of the desired sum, each user must broadcast at least one symbol, and each group key must contain at least $(K-T-2)/\binom{K-T-1}{G}$ independent symbols. Our results establish the fundamental limits of DSA with groupwise keys and provide design insights for communication- and key-efficient secure aggregation in decentralized learning systems.

</details>


### [107] [Monimial Matrix Analogue of Yoshida's theorem](https://arxiv.org/abs/2511.14480)
*Ananda Chakraborty*

Main category: cs.IT

TL;DR: 本工作将有限域线性码的权枚举量扩展到二者及g重的平均完整联合权枚举量，给出MacWilliams型恒等式，并在单变量/单矩阵框架下获得Yoshida定理的多项式等价以及对g重情形的广义表示。


<details>
  <summary>Details</summary>
Motivation: 旨在将经典的权枚举理论扩展至跨越多个线性码的平均联合分布，统一并推广MacWilliams型恒等式和Yoshida型定理，以便分析多个码之间的联合权分布。

Method: 1) 给出二个F_q线性码的平均完整联合权枚举量的定义；2) 推导其MacWilliams型恒等式；3) 建立该平均量的单项式(Yoshida)定理之同构（monomial analogue）；4) 一般化到对任意整数g的平均g重完整联合权枚举量，给出其广义表示并给出单项矩阵 analogue的Yoshida型定理。

Result: 获得二码及多码的平均完整联合权枚举量的MacWilliams型恒等式，给出单项式_Yoshida_定理的等价形式，并对g重情形给出广义表示与单项矩阵版本的Yoshida定理。

Conclusion: 为多码的权枚举量提供统一的理论框架，扩展了经典MacWilliams与Yoshida定理在平均联合情形下的适用性，并为分析多码的联合权分布提供工具。

Abstract: In this paper, we study variants of weight enumerators of linear codes over $\mathbb{F}_q$. We generalize the concept of average complete joint weight enumerators of two linear codes over $\mathbb{F}_q$. We also give its MacWilliams type identities. Then we establish a monomial analogue of Yoshida's theorem for this average complete joint weight enumerators. Finally, we present the generalized representation for average of $g$-fold complete joint weight enumerators for $\mathbb{F}_q$-linear codes and establish a monomial matrix analogue of Yoshida's theorem for average $g$-fold complete joint weight enumerators.

</details>


### [108] [Compression with Privacy-Preserving Random Access](https://arxiv.org/abs/2511.14524)
*Venkat Chandar,Aslan Tchamkerten,Shashank Vatedka*

Main category: cs.IT

TL;DR: 存在一种无损压缩方案，在码率略高于源熵时，可以对任意 Xi 进行单独解码而不泄露其他位的信息。


<details>
  <summary>Details</summary>
Motivation: 研究无损压缩与逐位隐私的兼容性及极限，满足在不增加外部密钥的前提下对单比特的信息泄露最小化。

Method: 通过随机分箱/随机编码、典型性、独立性约束等信息论技术，构造码字并证明存在满足 I(X_{-i}; Xi decoded) = 0 的方案，在 r > H(X) 时可实现。

Result: 证明存在这样的编码，随着样本量趋近无穷大，逐位隐私属性成立；无损性和隐私性并存。

Conclusion: 该结果揭示无损压缩与逐位隐私之间的兼容性，并为隐私保护的无损数据传输和分布式编码中的信息泄漏控制提供理论基础；但实际实现与扩展需进一步研究。

Abstract: It is shown that an i.i.d. binary source sequence $X_1, \ldots, X_n$ can be losslessly compressed at any rate above entropy such that the individual decoding of any $X_i$ reveals \emph{no} information about the other bits $\{X_j : j \neq i\}$.

</details>
