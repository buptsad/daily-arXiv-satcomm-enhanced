<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 1]
- [eess.SP](#eess.SP) [Total: 13]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.CR](#cs.CR) [Total: 8]
- [eess.SY](#eess.SY) [Total: 5]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Recursive decoding of binary rank Reed-Muller codes and Plotkin construction for matrix codes](https://arxiv.org/abs/2510.19095)
*Alain Couvreur,Rakhi Pratihar*

Main category: cs.IT

TL;DR: 本文提出一种新的秩度量码族，其等价于在 twisted group algebra 上的 L-子空间；以二进制 G=(Z/2)^m 为情形展开研究，借鉴 Reed‑Muller 的 Plotkin 递归结构给出一种递归解码。对于限定子类，解码的渐近复杂度优于基于 Dickson 矩阵的通用解码算法；同时引入与 Plotkin 构造相对应的秩度量解码思路，并提出一个通用的 Plotkin‑风格构造，适用于任意具有效解码器的矩阵秩度量码对。


<details>
  <summary>Details</summary>
Motivation: 旨在为秩度量码提供高效解码策略并建立与二元 Reed‑Muller/Plotkin 结构的联系；通过在特定子类上实现递归解码，提升解码效率并拓展 Plotkin 构造在矩阵秩度量码中的应用。

Method: 将编码定义为 twisted group algebra L[G] 的某些 L-子空间，围绕二进制群 G=(Z/2)^m 构造并实现递归解码；受 Hamming/ Reed‑Muller 的 Plotkin (u|u+v) 结构启发，提出针对特定子类的解码算法，并给出一个通用的 Plotkin‑like 构造及其解码器，适用于任意具有效解码器的矩阵秩度量码对。

Result: 在所限定的子类中，递归解码的渐近复杂度优于基于 Dickson 矩阵的通用解码算法；提出一种与 Plotkin 构造相呼应的秩度量解码器，以及一个适用于广义矩阵秩度量码的 Plotkin‑风格构造。

Conclusion: 为矩阵秩度量码的解码提供了新的方向，建立了与 Reed‑Muller/Plotkin 结构的联系，并给出可扩展的 Plotkin‑风格构造，可能应用于更广范畴的秩度量码的高效解码。

Abstract: In 2021, Augot, Couvreur, Lavauzelle and Neri introduced a new class of rank
metric codes which can be regarded as rank metric counterparts of Reed-Muller
codes. Given a finite Galois extension $\mathbb{L} / \mathbb{K}$, these codes
are defined as some specific $\mathbb{L}$-subspaces of the twisted group
algebra $\mathbb{L} [\textrm{G}]$. We investigate the decoding of such codes in
the "binary" case, \emph{i.e.,} when $\textrm{G} = (\mathbb{Z}/2\mathbb{Z})^m$.
Our approach takes its inspiration from the decoding of Hamming metric binary
Reed-Muller codes using their recursive Plotkin "$(u ~|~ u+v)$" structure. If
our recursive algorithm restricts to a specific subclass of rank metric
Reed-Muller codes, its asymptotic complexity beats that of the recently
proposed decoding algorithm for arbitrary rank metric Reed-Muller codes based
on Dickson matrices. Also, this decoder is of completely different nature and
leads a natural rank metric counterpart of the Plotkin construction. To
illustrate this, we also propose a generic Plotkin-like construction for matrix
rank metric codes with an associate decoder, which can be applied to any pair
of codes equipped with an efficient decoder.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [2] [AI Signal Processing Paradigm for Movable Antenna: From Geometric Optimization to Electromagnetic Reconfigurability](https://arxiv.org/abs/2510.19209)
*Yining Li,Ziwei Wan,Chongjia Sun,Kaijun Feng,Keke Ying,Wenyan Ma,Lipeng Zhu,Xiaodan Shao,Zhenyu Xiao,Zhen Gao*

Main category: eess.SP

TL;DR: 提出了可移动与可重构天线（MARA）的统一建模框架，融合几何优化的GMA与电磁重构的ERA，研究GMA/ERA/MARA的信道建模与频谱效率SE优化，并系统评估AI驱动的信号处理在高维非凸优化中的优势，旨在提升6G系统的频谱效率与灵活性。


<details>
  <summary>Details</summary>
Motivation: 6G朝向智能化与高可重配置，传统固定天线的局限性突出，GMA与ERA分别提供几何与电磁域的自由度，但二者组合的高维混合优化问题亟待解决，需要一个统一的建模与优化框架，以及AI驱动的方法来应对复杂优化。

Method: 提出Movable and Reconfigurable Antenna的统一建模框架MARA；对GMA、ERA、MARA在信道建模与SE优化方面进行分析；系统回顾基于AI的解决方案，比较AI与传统算法在高维非凸优化中的优势。

Result: 首次给出MARA的统一建模框架，并对GMA、ERA、MARA的信道建模与频谱效率优化进行分析；提供AI驱动信号处理的系统性综述，揭示其在高维非凸优化中的潜在优势，为6G系统的高SE与灵活性设计提供理论支撑。

Conclusion: MARA框架有效解决几何-电磁双重重配置带来的高维优化挑战，为6G提供更高的频谱效率与灵活性；AI驱动的方法可在该新范式下展现显著优势。

Abstract: As 6G wireless communication systems evolve toward intelligence and high
reconfigurability, the limitations of traditional fixed antenna (TFA) has
become increasingly prominent, with geometrically movable antenna (GMA) and
electromagnetically reconfigurable antenna (ERA) emerging as key technologies
to break through this bottleneck. GMA activates spatial degrees of freedom
(DoF) by dynamically adjusting antenna positions, ERA regulates radiation
characteristics using tunable metamaterials, thereby introducing DoF in the
electromagnetic domain. However, the ``geometric-electromagnetic dual
reconfiguration" paradigm formed by their integration poses severe challenges
of high-dimensional hybrid optimization to signal processing. To address this
issue, we integrate the geometric optimization of GMA and the electromagnetic
reconfiguration of ERA for the first time, propose a unified modeling framework
for movable and reconfigurable antenna (MARA), investigate the channel modeling
and spectral efficiency (SE) optimization for GMA, ERA, and MARA. Besides, we
systematically review artificial intelligence (AI)-based solutions, focusing on
analyzing the advantages of AI over traditional algorithms in high-dimensional
non-convex optimization computations. This paper fills the gap in existing
literature regarding the lack of a comprehensive review on the AI-driven signal
processing paradigm under geometric-electromagnetic dual reconfiguration and
provides theoretical support for the design and optimization of 6G wireless
systems with high SE and flexibility.

</details>


### [3] [Generalized Modified Blake-Zisserman Robust Spline Adaptive Filter for Generalized Gaussian Noise](https://arxiv.org/abs/2510.19256)
*Haiquan Zhao,Bei Xu*

Main category: eess.SP

TL;DR: 提出 generalized modified Blake-Zisserman 鲁棒样条自适应滤波（GMBZ-SAF）及其在主动降噪中的滤波器-C GMBZ 变体，以增强在广义高斯噪声和冲击噪声环境下的鲁棒性与收敛性。


<details>
  <summary>Details</summary>
Motivation: SAF 在非线性系统辨识中具有良好的收敛性能，但在广义高斯噪声（GGN）环境下性能下降，且在冲击噪声下存在稳态失配；现有 SAF 方法对离群点的鲁棒性不足，因此需要更鲁棒的框架。

Method: 提出 generalized modified Blake-Zisserman 鲁棒样条自适应滤波（GMBZ-SAF）。在此基础上，给出步长的均值收敛区间和稳态均方误差（MSE）的推导；并为主动降噪应用扩展出滤波器-C GMBZ（FcGMBZ）算法以提升对冲击噪声的降噪能力。通过仿真验证稳态MSE 的理论一致性，以及在 GG N 环境下 SAF-GMBZ 相对于传统 SAF 的优越性，并证实 FcGMBZ 在冲击噪声下的 ANC 效果。

Result: 仿真实验表明：1) SAF-GMBZ 在 GGN 下的稳态 MSE 和收敛性优于传统 SAF，且对离群点具有更强鲁棒性；2) 理论推导的步长均值收敛范围与实际观测吻合；3) FcGMBZ 在主动降噪任务中对冲击噪声具显著降噪效果。

Conclusion: SAF-GMBZ 与 FcGMBZ 为在带有广义高斯噪声与冲击噪声的场景下的非线性系统辨识与主动降噪提供了更鲁棒且理论可控的解决方案，提升了收敛性与稳态性能。

Abstract: The spline adaptive filtering (SAF) algorithm-based information-theoretic
learning has exhibited strong convergence performance in nonlinear system
identification (NSI), establishing SAF as a promising framework for adaptive
filtering. However, existing SAF-based methods suffer from performance
degradation under generalized Gaussian noise (GGN) environment and exhibit
significant steady-state misalignment under impulse noise. Moreover, prior
research on SAF algorithms has not effectively addressed the adverse effects
caused by outliers. To overcome these challenges, the generalized modified
Blake-Zisserman robust spline adaptive filtering (SAF-GMBZ) algorithm is
proposed. Compared to conventional SAF algorithms, SAF-GMBZ exhibits superior
learning performance in GGN. Furthermore, the mean convergence ranges of the
step-sizes and the steady-state mean-square error (MSE) are calculated by
introducing the commonly utilized assumptions. To arrive at good convergence
accuracy and noise cancellation capability in active noise control (ANC)
application, the filter-c GMBZ (FcGMBZ) algorithm is further developed based on
SAF-GMBZ. Simulation results confirm the accuracy of the theoretical
steady-state MSE, and the superiority of the SAF-GMBZ algorithm under GGN
environment in NSI, along with the effectiveness of the FcGMBZ algorithm in ANC
application under impulsive noise environment.

</details>


### [4] [A Study on Delay Assessment for Heterogenous Traffic in VANET](https://arxiv.org/abs/2510.19267)
*Shama Siddiqu,Indrakshi Dey*

Main category: eess.SP

TL;DR: FROG-MAC（基于分段传输的MAC协议）通过把普通数据分段传输并在分段之间加入短暂停顿，使高优先级紧急数据能在通道中提前访问，从而改进VANET中高/低优先级数据的时延与吞吐量，对比802.11p的EDCA机制。


<details>
  <summary>Details</summary>
Motivation: 在VANET中存在多优先级、异构且时空不同步的节点，安全信息等紧急数据需要比非紧急数据更低时延的传输。然而，802.11p的EDCA在非紧急数据已经占用信道时，紧急数据需要等待。需要一种在保持现有PHY/MAC结构的前提下提高紧急信息传输的时效性的方法。

Method: 对802.11p（用于VANET的标准PHY/MAC，基于EDCA优先级）与FROG-MAC（一种基于分段传输的MAC协议）进行对比。通过仿真评估高低优先级数据的时延与吞吐量，评估在紧急信道访问方面的改进。

Result: 仿真结果显示，FROG-MAC通过在分段传输之间设置短暂停顿，使紧急流能够在间隙中获得信道访问，从而在时延和吞吐量两个维度上提高了对高低优先级数据的性能，尤其对紧急流的时延下降有显著效果。

Conclusion: 基于分段传输的FROG-MAC能提升VANET中紧急数据的通道访问时效性与整体性能；在实现层面仍需权衡分段带来的额外开销、同步与碎片化管理等问题。

Abstract: Vehicular Ad hoc Networks (VANETs) comprise of multi-priority hetero-genous
nodes, both stationary and/or mobile. The data generated by these nodes may
include messages relating to information, safety, entertainment, traffic
management and emergency alerts. The data in the network needs dif-ferentiated
service based on the priority/urgency. Media Access Control (MAC) protocols
hold a significant value for managing the data priority. This paper studies a
comparison of 802.11p which is a standard PHY and MAC protocol for VANET with a
fragmentation-based protocol, FROG-MAC. The major design principle of 802.11-p
is to allow direct Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I)
communication without associa-tion, using Enhanced Distributed Channel Access
(EDCA) to prioritize safety-critical messages. However, if non-critical
messages already start to transmit, the nodes with critical data have to wait.
FROG-MAC reduces this delay by transmitting normal packets in fragments with
short pauses between them, al-lowing urgent packets to access the channel
during these intervals. Simula-tions have been performed to assess the delay
and throughput for high and low priority data. We report that FROG-MAC improves
both the performance parameters due to offering an early channel access to the
emergency traffic.

</details>


### [5] [IoT-Enabled Sleep Monitoring and Cognitive Assessment for Evaluating Teacher Well-Being](https://arxiv.org/abs/2510.19269)
*Anwar Ahmed Khan,Shama Siddiqui,Mehar Ullah,Indrakshi Dey*

Main category: eess.SP

TL;DR: 通过物联网可穿戴设备监测巴基斯坦高中教师的睡眠质量，并通过CAQ评估认知功能；208名教师样本显示睡眠质量和认知功能普遍偏差，提示工作负荷与教师福祉对教学质量的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 在高压、多任务的教学情境下，睡眠质量与认知功能密切相关；利用 IoT 技术实现对教师睡眠的客观、可扩展监测，以揭示睡眠与认知之间的关系并为政策干预提供证据。

Method: 横断面研究，样本量208名教师，分布在巴基斯坦；使用嵌入脉率和SpO2传感器的智能手表收集睡眠数据，将睡眠质量分为'差'、'一般'、'好'三类；使用认知评估问卷（CAQ）进行自评认知功能；对睡眠等级与CAQ分数进行相关性分析。

Result: 大多数教师睡眠质量与认知功能均偏低；睡眠质量与认知功能存在关联，表明工作负荷等因素需改善，以提升教师福祉，从而有利于教学质量的提升。

Conclusion: 基于 IoT 的睡眠监测结合自评认知评估可以识别高风险教师群体，改善工作条件和工作负荷有望提升教师健康与教学表现。

Abstract: Sleep quality is an important indicator of the efficient cognitive function
for high school teachers. Due to the high work stress and multi-tasking
expectations, the teachers often face issues with their sleep quality and
cognitive function, which has a clearly negative influence on their teaching
abilities. In this work, we propose a unique but simple method of deploying
Internet of Things (IoT) technology to monitor the sleep quality of high school
teachers at Pakistan. Smart watches embedded with pulse rate and SpO2 sensors
were used to collect data and categorize the sleep quality as "poor", "fair" or
"good". Moreover, we used a psychological tool, Cognitive Assessment
Questionnaire (CAQ) for the self-assessment of teachers' cognitive function.
The study was conducted over 208 high school teachers from across Pakistan. It
has been found that most of the teachers had a poor sleep quality and cognitive
function; The link between these two variables indicate that the workload and
other factors must be improved for the teachers to ensure their well-being,
which will in turn have a positive impact on their teaching quality.

</details>


### [6] [Neuromorphic computing for anomaly detection in a laser powder bed fusion process](https://arxiv.org/abs/2510.19309)
*Shreyan Banerjee,Aasifa Rounak,Cathal Hoare,Denis Dowling,Vikram Pakrashi*

Main category: eess.SP

TL;DR: 首次将SNN用于LPBF过程的异常检测，并在CPU、FPGA与英特尔Loihi神经形态芯片上实现原型，显示通过调整Spike延迟提升检测能力，具备边缘端低功耗运行的潜力。


<details>
  <summary>Details</summary>
Motivation: 在激光粉末床熔融制造（LPBF）中需快速、精准地检测层间能量波动导致的异常，以提高部件质量与过程可控性；传统方法在实时性和功耗方面存在挑战，神经形态计算提供低功耗、实时处理的潜在优势。

Method: 对打印过程中的光电探测信号（来自用于监控等离子体和IR辐射的光电二极管）进行采集，构建SNN模型用于异常检测；先在常规硬件（CPU、FPGA）上实现，再迁移到低功耗神经形态芯片Loihi；通过调整脉冲延迟（Spike latency）以减少噪声对信息的掩蔽，从而提高检测性能。

Result: 通过调节Spike延迟，异常检测性能得到提升；证明了在低功耗神经形态芯片上的边缘实现可行性，并为LPBF过程提供了一个可行的监控框架。

Conclusion: 表明神经形态芯片具备在增材制造过程监控中实现实时、低功耗异常检测的潜力，建立了面向LPBF过程的监控框架。

Abstract: This study is the first application of spiking neural networks (SNNs) for
anomaly detection in the Laser Powder Bed Fusion (LPBF) additive manufacturing
process. The neural networks were used to identify print processing anomalies
generated by dropping of laser energy during the printing of individual layers
in a Ti-6Al-4V alloy lattice structures. Associated changes in the laser
generated melt pool were observed using an in-process photodiode monitoring
technique. photodiode sensors capturing plasma and infrared radiations
reflected from the print bed of the metal 3D printer were utilized to detect
sudden changes caused by anomalies during the printing process. The algorithm
is first implemented on non-neuromorphic hardware including a central
processing unit (CPU), on Field Programmable Gate Arrays (FPGA) and then on
neuromorphic Intel's Loihi chip. Improved detection of anomalies is achieved by
adjusting the spike latency of the neural network, which reduces masking of
information by noise within the monitored temporal signal. The work
demonstrates the possibility of using low-power neuromorphic chips within an
edge framework for anomaly detection in additive manufacturing and creates a
framework for the process.

</details>


### [7] [Multi-code rate Task-Oriented Communication for Multi-Edge Cooperative Inference](https://arxiv.org/abs/2510.19360)
*Dongwon Kim,Jiwan Seo,Joonhyuk Kang*

Main category: eess.SP

TL;DR: 提出基于重要性自适应的特征量化与动态规划资源分配的多边缘协同推理框架，在带宽受限条件下实现通信效率与推理性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 在物联网与AI结合下的多边缘协同推理中，所有设备以固定比特率同时传输特征会导致带宽利用率低下且隐私与通信成本高企。需要通过自适应码率和资源分配来提升效率并保护隐私。

Method: 提出速率自适应量化（RAQ）以根据对下游推理任务的重要性动态调整特征的编码率，并使用动态规划（DP）在离散的码率选项中为各边缘设备分配码率，以满足带宽约束。

Result: 在多视数据集上实验表明，该框架显著优于固定速率量化方案，在有限带宽条件下实现更优的通信效率与推理性能权衡。

Conclusion: 所提的RAQ与DP资源分配框架有效提升了多边缘物联网场景中的通信与推理表现，具有良好的带宽受限条件下的适用性。

Abstract: The integration of artificial intelligence (AI) with the internet of things
(IoT) enables task-oriented communication for multi-edge cooperative inference
system, where edge devices transmit extracted features of local sensory data to
an edge server to perform AI-driven tasks. However, the privacy concerns and
limited communication bandwidth pose fundamental challenges, since simultaneous
transmission of extracted features with a single fixed compression ratio from
all devices leads to severe inefficiency in communication resource utilization.
To address this challenge, we propose a framework that dynamically adjusts the
code rate in feature extraction based on its importance to the downstream
inference task by adopting a rate-adaptive quantization (RAQ) scheme.
Furthermore, to select the code rate for each edge device under limited
bandwidth constraint, a dynamic programming (DP) approach is leveraged to
allocate the code rate across discrete code rate options. Experiments on
multi-view datasets demonstrate that the proposed frameworks significantly
outperform the frameworks using fixed-rate quantization, achieving a favorable
balance between communication efficiency and inference performance under
limited bandwidth conditions.

</details>


### [8] [Ray-Tracing Based Narrow-Beam Channel Simulation, Characterization and Performance Evaluation for 5G-R Systems](https://arxiv.org/abs/2510.19401)
*Tao Zhou,Liying Geng,Yiqun Liang,Kaifeng Bao,Tianyun Feng,Liu Liu,Bo Ai*

Main category: eess.SP

TL;DR: 利用射线追踪在高铁场景下对窄波束信道进行建模、分析与评估，揭示信道统计特性、波束宽度影响，以及在Vienna 5G仿真器和硬件在环平台上的系统性能表现。


<details>
  <summary>Details</summary>
Motivation: 在高速移动的铁路场景中，窄波束是5G-R的核心但缺乏基于射线追踪的动态信道建模与波束管理研究，需要系统地评估信道特性与端到端性能。

Method: 建立三种代表性HSR场景（高架桥、切线、站点），通过定制波束跟踪实现持续对准；基于RT进行动态窄波束信道仿真，分析大尺度与小尺度衰落及非平稳性；考察波束宽度对信道属性的影响；使用Vienna 5G仿真器评估5G-R系统性能（BER、吞吐量、光谱效率）；并开发硬件在环平台评估同步信号接收功率、信噪比及参考信号质量。

Result: 给出窄波束信道在三个HSR场景下的路径损耗、阴影衰落、衰落严重性、时-频-空间色散及非平稳性区间等统计特性；验证波束宽度对信道属性的显著影响；在系统层面给出BER、吞吐量、光谱效率等性能趋势；硬件在环实验验证同步信号功率、SINR、RSRQ等指标的可行性与一致性。

Conclusion: 为HSR环境下5G-R窄波束设计与优化提供实证依据，提出基于RT的动态信道建模与波束管理评估框架，支持在高动态场景中的系统设计与优化。

Abstract: This paper investigates narrow-beam channel characterization and performance
evaluation for 5G for railway (5G-R) systems based on ray-tracing (RT)
simulation. Three representative high-speed railway (HSR) scenarios including
viaduct, cutting, and station are established, and RT-based dynamic narrow-beam
channel simulations are conducted using a designed beam tracking scheme that
ensures continuous alignment with the moving train. The channel characteristics
are analyzed in terms of both large-scale and small-scale fading, as well as
non-stationarity, providing statistical insights into path loss, shadow fading,
fading severity, time-frequency-space dispersion, and stationarity interval.
The influence of beamwidth on these channel properties is also examined.
Furthermore, the performance of 5G-R systems operating in such narrow-beam
channels is evaluated using the Vienna 5G simulator, with a focus on block
error rate, throughput, and spectral efficiency. A hardware-in-the-loop
simulation platform is developed to further assess synchronization signal
reference signal received power, signal-to-interference-plus-noise ratio, and
reference signal received quality. The results provide valuable guidance for
the design and optimization of 5G-R systems in HSR environments.

</details>


### [9] [A Novel Delay-Doppler Domain Channel Sounding Method for 6G High-Mobility Scenarios](https://arxiv.org/abs/2510.19402)
*Kaifeng Bao,Tao Zhou,Chaoyi Li,Liu Liu,Bo Ai*

Main category: eess.SP

TL;DR: 提出用于6G高移动场景的延迟-多普勒域通道声测方法，覆盖波形设计、同步、CSF估计、精度提升及实测验证。


<details>
  <summary>Details</summary>
Motivation: 6G高移动环境中，传统的时域/频域声测无法直接获取由高多普勒引起的多普勒信息；延迟-多普勒域的通道展现可自然表征传播环境并提供更完整的通道信息。

Method: 提出用于声测的波形设计并分析其声测能力；给出DD域通道声测的方法学，包括同步与CSF估计；提出提升测量精度的算法；构建适用于6G高移动场景的DD域通道声测系统；进行方法评估并开展城市环境下的V2I场景实测。

Result: 通过实验与仿真评估，建立并验证可用于6G高移动场景的DD域声测系统，获得CSF、功率时延谱、多普勒功率谱密度、MPC数量等特征，结果显示所提方法有效并为6G高移动通信研究提供有价值的见解。

Conclusion: DD域通道声测在6G高移动场景中可行且有效，能够提供丰富的时延-多普勒域通道信息，推动6G高移动通信的建模与系统设计。

Abstract: Channel measurements are the prerequisite for applying emerging transmission
technologies and designing communication systems. In sixth-generation (6G)
system, conventional time or frequency domain channel sounding methods cannot
directly obtain Doppler information induced by high-mobility scenarios. The
channel spreading function (CSF) simultaneously captures delay and Doppler
information, while naturally characterizing the propagation environment in the
delay-Doppler (DD) domain. However, DD domain channel sounding methods remain
underexplored. This paper presents a novel DD domain channel sounding method
for 6G high-mobility scenarios. First, we introduce the waveform design for the
sounding signal and analyze its sounding capability. Next, the methodology of
DD domain channel sounding, including synchronization and CSF estimation, is
thoroughly detailed. Additionally, an algorithm for enhancing measurement
precision is proposed. The performance of the proposed method is rigorously
evaluated. Subsequently, a DD domain channel sounding system competent for 6G
high-mobility scenarios is established. Finally, DD domain channel measurements
are conducted for a vehicle-to-infrastructure scenario in urban environments.
Measurement results, including CSF, power delay profile, Doppler power spectral
density, number of multipath components, and other characteristics, are
derived, which confirm the effectiveness of the proposed method and offer
helpful insights for advancing research on 6G high-mobility communications.

</details>


### [10] [Network-Centric Anomaly Filtering and Spoofer localization for 5G-NR Localization in LAWNs](https://arxiv.org/abs/2510.19521)
*Zexin Fang,Bin Han,Zhu Han,Hans D. Schotten*

Main category: eess.SP

TL;DR: Proposes a security-centric framework for 3GPP 5G-NR TDoA UAV localization in urban A2G settings: optimize node selection, introduce UE-assisted lightweight methods, reveal merged-peak spoofing vulnerabilities, and develop a network-wide anomaly detection plus a robust gradient-descent localization to jointly localize the victim and the spoofer; validated by simulations.


<details>
  <summary>Details</summary>
Motivation: Tackle vulnerabilities in 3GPP NR TDoA-based UAV localization under urban A2G channels, where synchronization errors, geometry, and spoofing can degrade or falsify positioning, posing safety and security risks.

Method: 1) Optimize node selection strategies as a function of UAV altitude and deployment density; 2) propose lightweight UE-assisted localization to reduce overhead while boosting accuracy; 3) analyze and model merged-peak spoofing attacks; 4) design a network-centric anomaly detection framework at the LMF using existing 3GPP parameters; 5) develop a recursive gradient-descent-based robust localization algorithm to filter anomalies and estimate UAV position; 6) validate through extensive simulations.

Result: Characterizes how synchronization quality and geometric factors govern spoofing success probability; demonstrates the unified framework can provide robust victim localization and spoofer localization; shows effectiveness of optimization and security mechanisms in simulations.

Conclusion: The paper presents a unified framework that enhances both robustness of victim localization and the capability to locate spoofers in 3GPP-compliant UAV positioning, contributing to more secure 5G-NR UAV localization; potential avenues include real-world evaluation and integration with broader security architectures.

Abstract: This paper investigates security vulnerabilities and countermeasures for 3rd
Generation Partnership Project (3GPP) Fifth Generation New Radio (5G-NR) Time
Difference of Arrival (TDoA)-based unmanned aerial vehicle (UAV) localization
in low-altitude urban environments. We first optimize node selection strategies
under Air to Ground (A2G) channel conditions, proving that optimal selection
depends on UAV altitude and deployment density. We propose lightweight User
Equipment (UE)-assisted that reduce overhead while enhancing accuracy. We then
expose critical security vulnerabilities by introducing merged-peak spoofing
attacks where rogue UAVs transmit multiple lower-power pulses that merge with
legitimate signals, bypassing existing detection methods. Through theoretical
modeling and sensitivity analysis, we quantify how synchronization quality and
geometric factors determine spoofing success probability, revealing fundamental
weaknesses in current 3GPP positioning frameworks. To address these
vulnerabilities, we design a network-centric anomaly detection framework at the
Localization Management Function (LMF) using existing 3GPP-specified
parameters, coupled with a recursive gradient descent-based robust localization
algorithm that filters anomaly data while estimating UAV position. Our unified
framework simultaneously provides robust victim localization and spoofer
localization-capabilities not integrated in existing literature. Extensive
simulations validate the effectiveness of both optimization and security
mechanisms for 3GPP-compliant UAV positioning.

</details>


### [11] [On the Robustness of AFDM and OTFS Against Passive Eavesdroppers](https://arxiv.org/abs/2510.19525)
*Vincent Savaux,Hyeon Seok Rou,Zeping Sui,Giuseppe Thadeu Freitas de Abreu,Zilong Liu*

Main category: eess.SP

TL;DR: AFDM对抗被动窃听者的暴力穷举demodulation时，复杂度更高（O(N^2)）且BER几乎不可解，与OTFS相比显示出更强的鲁棒性；在相同条件下，OTFS仅能部分恢复信号。


<details>
  <summary>Details</summary>
Motivation: 评估AFDM与OTFS在无知识窃听者下对暴力穷举解调的鲁棒性，量化攻击难度与误码性能，以探讨两种波形的物理层安全性。

Method: 给出对窃听者无 Chirp 参数（AFDM）或延迟-多普勒网格配置（OTFS）知识的假设，推导暴力穷举所需的计算复杂度，并通过BER仿真对比两种波形的窃听可解码程度。N表示子载波数量。

Result: 分析表明：OTFS的 brute-force 复杂度为 O(√N)，AFDM为 O(N^2)，因此AFDM在计算层面对窃听者更具抗性。BER仿真显示，在相同条件下，AFDM下信号在窃听者端几乎不可解码，而OTFS可以部分恢复信号。

Conclusion: 在假设窃听者无法获得 Chirp/网格配置信息时，AFDM相较OTFS对暴力穷举的鲁棒性更强，表现为更高的计算成本和更低的窃听解码能力。然而结果依赖于参数保密性，实际安全性还需结合其他防护措施与攻击模型的全面评估。

Abstract: We investigate the robustness of affine frequency division multiplexing
(AFDM) and orthogonal time frequency space (OTFS) waveforms against passive
eavesdroppers performing brute-force demodulation to intercepted signals, under
the assumption that eavesdroppers have no knowledge of chirp parameters (in
AFDM) or the delay-Doppler grid configuration (in OTFS), such that they must
search exhaustively over possible demodulation matrices. Analytical results
show that the brute-force complexity scales as $\mathcal{O}(\sqrt{N})$ for OTFS
and $\mathcal{O}(N^2)$ for AFDM, where $N$ is the number of subcarriers,
indicating that AFDM has superior resilience over OTFS. Bit error rate (BER)
simulations confirm the analysis by showing that, with AFDM, the signal remains
nearly undecodable at the eavesdropper, while OTFS allows partial signal
recovery under equivalent conditions.

</details>


### [12] [Multilayer Perceptron Neural Network Model: A Novel Approach for LFP Contrast Sensitivity Tuning](https://arxiv.org/abs/2510.19636)
*Sahar Maleki,Reza Lashgari,Mahdi Aliyari Shoorehdeli,Mohammad Komareji*

Main category: eess.SP

TL;DR: MLP-based tuning model for LFP contrast responses in V1 outperforms traditional CRF models, especially for supersaturating CRFs, enabling better tuning across more neural recordings.


<details>
  <summary>Details</summary>
Motivation: LFP responses often show weaker and broader tunings than spikes; optimized tuning methods are needed to accurately evaluate LFPs and compare with spiking activity.

Method: Analyze luminance-evoked LFPs in primate V1; classify CRFs by monotonicity index (MI); identify supersaturating data using static identification methods (MLP, RBF, fuzzy, neuro-fuzzy, LOLIMOT); compare with traditional/modified Naka-Rushton functions.

Result: MLP outperforms other methods, yielding superior tuning of LFP responses and enabling successful tuning of a significantly higher number of recordings across all three types.

Conclusion: MLP-based modeling provides a novel, more accurate approach to estimating contrast sensitivity tuning curves for neuronal populations than existing models.

Abstract: Local field potentials (LFPs) have been demonstrated to be an important
measurement to study the activity of a local population of neurons. The
response tunings of LFPs have been mostly reported as weaker and broader than
spike tunings. Therefore, selecting optimized tuning methods is essential for
appropriately evaluating the LFP responses and comparing them with neighboring
spiking activity. In this paper, new models for tuning of the contrast response
functions (CRFs) are proposed. To this end, luminance contrast-evoked LFP
responses recorded in primate primary visual cortex (V1) are first analyzed.
Then, supersaturating CRFs are distinguished from linear and saturating CRFs by
using monotonicity index (MI). The supersaturated recording data are then
identified through static identification methods including multilayer
perceptron (MLP) neural network, radial basis function (RBF) neural network,
fuzzy model, neuro-fuzzy model, and the local linear model tree (LOLIMOT)
algorithm. Our results demonstrate that the MLP neural network, compared to
traditional and modified hyperbolic Naka-Rushton functions, exhibits superior
performance in tuning the local field potential responses to luminance contrast
stimuli, resulting in successful tuning of a significantly higher number of
neural recordings of all three types. These results suggest that the MLP neural
network model can be used as a novel approach to measure a better fitted
contrast sensitivity tuning curve of a population of neurons than other
currently used models.

</details>


### [13] [Micro-Doppler Energy-Based Robust Multi-Target Vital Signs Monitoring Using 77-GHz FMCW Radar with Spatiotemporal Adaptive Processing](https://arxiv.org/abs/2510.19639)
*Chenxing Tan,Yuguan Hou,Hao Wang,Zhonghao Yuan*

Main category: eess.SP

TL;DR: 基于微多普勒能量的框架，用于在77-GHz FMCW 雷达下对多目标进行鲁棒生命体征监测；通过微多普勒能量变化提取生理信号，结合 STAP、MUSIC 和自适应谱滤波实现目标检测、定位与分离，优于传统基于相位的方法。


<details>
  <summary>Details</summary>
Motivation: 现有相位基方法在环境噪声、随机体动和严格标定要求下对多目标生命体征监测鲁棒性不足，需开发对噪声和运动更鲁棒的能量基提取方法，且能在多目标场景中实现精准分离。

Method: 提出一个整体 processing 流水线：利用空间-时间自适应处理(STAP)进行目标检测与跟踪，使用MUSIC实现高分辨角度估计，并设计自适应谱滤波策略从微多普勒能量中提取心肺信号；建立将微多普勒能量变化与生理活动对应的数理框架，使相邻目标的能量信号可分离。核心在于基于能量的特征提取对相位噪声和运动伪影具鲁棒性。

Result: 在毫米波雷达数据集上进行实验，系统能够在5米内对多达4个目标进行准确检测与生命体征分离，呼吸率与心率的平均绝对误差分别为1.2 bpm和2.3 bpm；相较于传统相位基方法，在复杂场景与环境噪声、体动等条件下表现更优。

Conclusion: 提出的微多普勒能量提取框架在鲁棒多目标生命体征监测方面具有显著优势，特别是在噪声与运动干扰条件下，对多目标的分离和估计更加稳健，具备在实际应用中实现实时监测的潜力。

Abstract: This paper presents a novel micro-Doppler energy-based framework for robust
multi-target vital signs monitoring using 77-GHz Frequency-Modulated
Continuous-Wave (FMCW) radar. Unlike conventional phase-based methods that are
susceptible to environmental noise, random body movements, and stringent
calibration requirements, our approach exploits the energy variations in radar
returns induced by cardiopulmonary activities. The proposed system integrates a
comprehensive processing pipeline including space-time adaptive processing
(STAP) for target detection and tracking, MUSIC algorithm for high-resolution
angle estimation, and an innovative adaptive spectral filtering technique for
vital signs extraction. We establish a rigorous mathematical framework that
formalizes the relationship between micro-Doppler energy variations and
physiological activities, enabling robust separation of closely spaced targets.
The key innovation lies in the micro-Doppler energy extraction methodology that
provides inherent robustness to phase noise and motion artifacts. Experimental
results using millimeter-wave radar datasets demonstrate that the system can
accurately detect and separate vital signs of up to four targets within
\SI{5}{\meter} range, achieving mean absolute errors of \SI{1.2}beats per
minute and \SI{2.3} beats per minute for respiration and heart rates,
respectively. The proposed approach demonstrates superior performance compared
to traditional phase-based methods, particularly in challenging multi-target
scenarios with environmental noise and subject movement.

</details>


### [14] [Interpretable machine learning for cardiogram-based biometrics](https://arxiv.org/abs/2510.19775)
*Ilija Tanasković,Ljiljana B. Lazarević,Goran Knežević,Nikola Milosavljević,Olga Dubljević,Bojana Bjegojević,Nadica Miljković*

Main category: eess.SP

TL;DR: ECG features, especially QRS-centric components, dominate biometric identification and remain robust to emotional variation; ICG features add complementary cues but are less stable; a 14-feature subset achieves ~99% accuracy, with multicollinearity redistributing importance across correlated features; this supports a central role for QRS features in cardiogram-based identity.


<details>
  <summary>Details</summary>
Motivation: 评估ECG与ICG特征在生物识别中的判别力与对情绪变化的鲁棒性，并通过多种可解释性方法揭示特征贡献及其相互关系。

Method: 使用29个跨四个领域(时域、振幅、斜率、形态)的特征，采用随机森林（RF）模型并结合Gini重要性、置换重要性与SHAP等可解释性方法评估特征重要性。进行相关性分析以检验多重共线性；进行统计分析以识别基线与愤怒情绪下显著特征；使用递归特征消除和遗传算法进行特征选择，得到14特征子集，并比较其与全集的识别性能；分析BCX ICG特征与ECG特征在判别力上的互补性与稳定性。

Result: ECG与ICG特征均在Gini、置换与SHAP等方法中进入前十，ECG的QRS相关描述符居于最前；ICG BCX特征更多来自振幅维度，提供互补但稳定性较低。存在显著的多重共线性，RF将重要性在高度相关的特征之间分散。14个特征子集的识别准确率接近全特征集合（99%），与全集相比仅差1个百分点。情绪改变对QRS-centric特征的鲁棒性较高，表明其在个体身份识别中具有核心地位。

Conclusion: 表明个体身份信息主要编码在ECG的QRS相关特征，跨四个领域保持稳定性；ICG BCX特征通过振幅提供辅助性线索但稳定性较弱。未来基于心电图的身份模型可重点关注QRS特征，并在必要时结合ICG信息以提升鲁棒性，但需注意特征间的共线性对独立贡献的稀释作用。

Abstract: This study investigates the role of electrocardiogram (ECG) and impedance
cardiogram (ICG) features in biometric identification, emphasizing their
discriminative capacity and robustness to emotional variability. A total of 29
features spanning four domains (temporal, amplitude, slope, and morphological)
are evaluated using random forest (RF) models combined with multiple
interpretability methods. Feature importance shows that both ECG- and
ICG-derived features are consistently ranked among the top 10 by Gini
importance, permutation importance, and SHAP values, with ECG features,
particularly QRS-centric descriptors, occupying the highest positions. In
parallel, ICG BCX features contribute complementary, however, with lower
cross-method stability. Correlation analysis reveals substantial
multicollinearity, where the RF distributes and diminishes importance across
highly correlated pairs, confirming reduced independent contributions.
Statistical analysis identifies 14 features with significant differences
between baseline and anger, without a clear pattern by domain. Feature
selection with recursive feature elimination and genetic algorithms converges
on a subset (14 features) that attains accuracy within 1% of the full set
(99%), improving efficiency in storage and computation. These complementary
analyses indicate that the individuality required for reliable identification
is primarily encoded in QRS-related ECG features across all four domains.
Meanwhile, BCX-derived ICG features contribute mainly through amplitude,
providing supportive but less stable discriminatory cues. The confirmed
resilience of QRS-centric descriptors to emotional variation, where stable
inter-individual differences in the QRS complex could be traced to variations
in ventricular mass, conduction pathways, and thoracic geometry, may indicate
their central role in future models of cardiogram-based identity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency](https://arxiv.org/abs/2510.18905)
*Minseok Jung,Abhas Ricky,Muhammad Rameez Chatni*

Main category: cs.LG

TL;DR: 提出三维多目标优化框架用于推理缩放，综合准确性、成本和时延，通过膝点优化在不同场景下实现最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有1D/2D优化未充分考虑成本与延迟约束，导致对部署环境的适应性不足。

Method: 提出统一的三维优化框架，基于蒙特卡洛仿真对三个场景、九个LLM，比较四种MOO方法；将推理缩放k置于约束感知的优化决策空间。

Result: 膝点优化在权衡精度与资源之间表现最佳；在以精度为优先时，准确性最大化策略仍然有利；该框架确立了部署感知的推理缩放理论基础。

Conclusion: 在多样化操作背景下，实现部署感知的推理缩放，定义可行解空间并帮助环境自适应选择k。

Abstract: AI inference scaling is often tuned through 1D heuristics (a fixed reasoning
passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail
to consider cost and latency constraints. We introduce a 3D optimization
framework that jointly calibrates accuracy, cost, and latency within a unified
decision space, enabling constraints-aware inference scaling. Using Monte Carlo
simulations across three representative scenarios and nine simulated large
language models, we evaluate four optimization methods to address the 3D
multi-objective optimization (MOO) problem. Framing inference scaling in MOO
shapes a feasible space that 1D and 2D optimizations fail to capture, enabling
environmentadaptive selection of the inference scaling k. Results show that
knee-point optimization achieves the best balance, while accuracy-maximization
remains favorable when precision is prioritized. The framework establishes a
theoretical foundation for deployment-aware inference scaling across diverse
operational contexts.

</details>


### [16] [Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape](https://arxiv.org/abs/2510.18910)
*Ziquan Wei,Tingting Dan,Guorong Wu*

Main category: cs.LG

TL;DR: 提出一个面向功能性神经影像的可扩展基础模型：通过对BEI（脑-环境互动）进行多任务分词化预训练，并以预训练BEI的伪标签进行半监督微调，以应对有限标注数据并对齐脑-结果关系。


<details>
  <summary>Details</summary>
Motivation: 解决自监督训练与脑-结果关系之间的潜在错位，在有限样本下构建对临床任务更有用的基金模型；充分利用大量无标签fMRI及丰富环境/人口变量。

Method: i) 通过将BEI分解为多模态/多任务标记，对脑环境交互进行令牌化以做多任务预训练；ii) 通过对预训练得到的BEI产生伪标签，在半监督框架下对模型进行微调。

Result: 在性别预测、行为识别以及自闭症、帕金森、阿尔茨海默病、精神分裂症等疾病的早期诊断等任务上得到有希望的结果，表明该方法具有在临床常规中放大当前神经影像应用的潜力。

Conclusion: BEI驱动的多任务基础模型为大规模无标签fMRI数据的利用提供了一条可扩展路径，并改善了对下游任务的对齐，从而提升临床神经影像的应用潜力。

Abstract: A reliable foundation model of functional neuroimages is critical to promote
clinical applications where the performance of current AI models is
significantly impeded by a limited sample size. To that end, tremendous efforts
have been made to pretraining large models on extensive unlabeled fMRI data
using scalable self-supervised learning. Since self-supervision is not
necessarily aligned with the brain-to-outcome relationship, most foundation
models are suboptimal to the downstream task, such as predicting disease
outcomes. By capitalizing on rich environmental variables and demographic data
along with an unprecedented amount of functional neuroimages, we form the brain
modeling as a multitask learning and present a scalable model architecture for
(i) multitask pretraining by tokenizing multiple brain-environment interactions
(BEI) and (ii) semi-supervised finetuning by assigning pseudo-labels of
pretrained BEI. We have evaluated our foundation model on a variety of
applications, including sex prediction, human behavior recognition, and disease
early diagnosis of Autism, Parkinson's disease, Alzheimer's disease, and
{Schizophrenia}, where promising results indicate the great potential to
facilitate current neuroimaging applications in clinical routines.

</details>


### [17] [ADPO: Anchored Direct Preference Optimization](https://arxiv.org/abs/2510.18913)
*Wang Zixian*

Main category: cs.LG

TL;DR: ADPO 作为统一框架，将 DPO 的硬标签扩展为软偏好、引入参考策略锚定，以及支持列表式偏好，包含对 DPO、Bradley-Terry 与 Top-1-vs-Rest 等特殊情况的证明与衍生变体；提出用于 pairwise、listwise 以及 KDE 平滑的三种实际版本，并在上下文 bandits 与强化学习任务中展示显著性能提升和对噪声鲁棒性的改进。


<details>
  <summary>Details</summary>
Motivation: 解决传统 DPO 在硬二分类标签和两两比较下的训练不稳定性、梯度漂移以及对噪声敏感的问题，提供一个更鲁棒、可扩展的偏好优化框架，能够通过锚定、软偏好和列表化建模提升学习稳定性和泛化。

Method: 提出一个统一的 Anchored Direct Preference Optimization (ADPO) 框架，加入软偏好概率以编码不确定性、可选的参考策略锚定以实现分组不变性和隐式 KL 正则化，以及用 Plackett-Luce 分布实现列表式偏好建模。给出 DPO、Bradley-Terry、Top-1-vs-Rest 等的特例证明；衍生出三种实用变体：pairwise anchored Soft-DPO、listwise anchored Soft-DPO（带原始奖励）、以及基于 KDE 的列表式平滑以对抗重尾噪声；在上下文 bandits 和顺序强化学习（CartPole、LunarLander）中进行实验，比较锚定对 WinMass 的提升、KDE 平滑对重尾污染的鲁棒性，以及多步任务中的传递效果。

Result: 在上下文 bandits 中，锚定显著提升了相较于标准 DPO 的 WinMass 38-63%；在重尾污染场景下，KDE 平滑达到 0.68 对 0.32（相对提升112%）。在顺序强化学习的实验（CartPole、LunarLander）中，锚定对带噪声的偏好性能提升 15-29%，证实单步到多步设置的迁移有效性。不同规模模型（10-256 参数）给出明确的实操指引：对噪声较清晰或中等的场景，推荐使用 pairwise anchored Soft-DPO；对极端污染，推荐 KDE 基于的 listwise ADPO。

Conclusion: ADPO 提供一个可扩展且鲁棒的偏好优化框架，通过软偏好、锚定与列表式建模的组合，覆盖从二元对比到列表化排序的多样需求，并给出在实际任务中的清晰使用指南：针对不同噪声水平和任务类型，选择相应变体即可实现显著的性能与鲁棒性提升。

Abstract: Anchored Direct Preference Optimization (ADPO) is a unified framework that
generalizes Direct Preference Optimization (DPO) with soft preferences,
reference-policy anchoring, and groupwise extensions. While standard DPO
assumes hard binary labels and pairwise comparisons, ADPO introduces: (i) soft
preference probabilities that encode uncertainty and mitigate gradient drift;
(ii) arbitrary reference-policy anchors that stabilize training via groupwise
shift invariance and implicit KL regularization; and (iii) listwise preference
modeling through Plackett-Luce distributions. We prove that DPO, Bradley-Terry
objectives, and Top-1-vs-Rest formulations emerge as special cases. ADPO yields
three practical variants: pairwise anchored Soft-DPO, listwise anchored
Soft-DPO with raw rewards, and KDE-based listwise smoothing for heavy-tailed
noise. In contextual bandits, anchoring improves WinMass by 38-63% over
standard DPO, while KDE smoothing achieves 0.68 vs 0.32 under heavy-tailed
contamination (112% relative gain). In sequential reinforcement learning
(CartPole, LunarLander), anchoring improves noisy-preference performance by
15-29%, confirming transfer from single-step to multi-step settings.
Experiments with 10-256 parameter models provide clear guidance: use pairwise
anchored Soft-DPO for clean or moderate noise, and KDE-based listwise ADPO for
extreme contamination.

</details>


### [18] [Benchmarking On-Device Machine Learning on Apple Silicon with MLX](https://arxiv.org/abs/2510.18921)
*Oluwaseun A. Ajayi,Ogundepo Odunayo*

Main category: cs.LG

TL;DR: MLX 为在 Apple Silicon 上进行推断优化的框架，聚焦 transformer 模型的端设备推理，并与 PyTorch/CUDA 在两台 Macbook 与一张 NVIDIA GPU 上的延迟进行对比，展示本地部署的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着在笔记本和移动设备上部署大语言模型的需求增加，需要针对特定硬件（如 Apple Silicon）优化的框架，以加速端上推理并降低对云端依赖。

Method: 提出 MLX-transformers，包含多种 transformer 实现；从 Hugging Face 下载并将检查点转换为 MLX 格式，直接在 MLX 上执行推断；在两台 Apple Silicon Macbook 与 NVIDIA CUDA GPU 上对 BERT、RoBERTa、XLM-RoBERTa 等同参数规模模型进行延迟对比。

Result: MLX 在 Apple Silicon 上展示出有竞争力的推理延迟，且在与 PyTorch/CUDA 的对比中证明了端设备上可行性；MLX 能直接加载来自 Hugging Face 的模型，减少端口转换开销。

Conclusion: MLX 显示了在 Apple 生态中实现高效且易用的端上 ML 应用的潜力，未来工作将扩展到更多模型和模态，以提供更全面的性能评估。

Abstract: The recent widespread adoption of Large Language Models (LLMs) and machine
learning in general has sparked research interest in exploring the
possibilities of deploying these models on smaller devices such as laptops and
mobile phones. This creates a need for frameworks and approaches that are
capable of taking advantage of on-device hardware. The MLX framework was
created to address this need. It is a framework optimized for machine learning
(ML) computations on Apple silicon devices, facilitating easier research,
experimentation, and prototyping.
  This paper presents a performance evaluation of MLX, focusing on inference
latency of transformer models. We compare the performance of different
transformer architecture implementations in MLX with their Pytorch
counterparts. For this research we create a framework called MLX-transformers
which includes different transformer implementations in MLX and downloads the
model checkpoints in pytorch and converts it to the MLX format. By leveraging
the advanced architecture and capabilities of Apple Silicon, MLX-Transformers
enables seamless execution of transformer models directly sourced from Hugging
Face, eliminating the need for checkpoint conversion often required when
porting models between frameworks.
  Our study benchmarks different transformer models on two Apple Silicon
macbook devices against an NVIDIA CUDA GPU. Specifically, we compare the
inference latency performance of models with the same parameter sizes and
checkpoints. We evaluate the performance of BERT, RoBERTa, and XLM-RoBERTa
models, with the intention of extending future work to include models of
different modalities, thus providing a more comprehensive assessment of MLX's
capabilities. The results highlight MLX's potential in enabling efficient and
more accessible on-device ML applications within Apple's ecosystem.

</details>


### [19] [Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients](https://arxiv.org/abs/2510.18924)
*Omar El mansouri,Mohamed El Amine Seddik,Salem Lahlou*

Main category: cs.LG

TL;DR: 提出一种对抗奖励信号噪声的鲁棒策略学习框架：GRPO/Dr.GRPO，通过将奖励污染建模为伯努利噪声，在估计奖励翻转概率后进行噪声修正，从而得到无偏梯度估计。理论上，基于分组的策略优化天然缓解个体层面的噪声，修正策略进一步增强了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: RLHF/RLVR 在真实应用中容易受奖励噪声影响，且广泛使用的基于分组的策略优化方法对噪声的鲁棒性尚未充分研究。该工作旨在同时提供理论洞见和实用算法，以提升在嘈杂现实场景中的对齐效果。

Method: 将奖励扰动建模为伯努利噪声；在估计奖励翻转概率后进行噪声修正以去偏学习信号；将该修正应用于群体相关的策略优化（GRPO）及其改进版 Dr.GRPO，理论上证明了无偏梯度估计；并给出对分组方法在噪声存在下的鲁棒性分析。

Result: 在数学和编码任务上，应用该噪声修正后，相对于基线在真实 Reward 模型条件下，准确率提升可达 6.7 个百分点（数学任务）和 1.5 个百分点（编码任务）。

Conclusion: 本研究将监督学习中的标签噪声修正思路引入现代 RLHF 框架，提供了理论分析与实用算法，显著提升了对真实场景中噪声的鲁棒性与部署可行性。

Abstract: Reinforcement learning from human feedback (RLHF) or verifiable rewards
(RLVR), the standard paradigm for aligning LLMs or building recent SOTA
reasoning models, is highly sensitive to noise from inconsistent or erroneous
rewards. Yet, the interaction between such noise and widely used group-based
policy optimization methods remains underexplored. We introduce a noise-robust
Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO)
framework that explicitly models reward corruption as Bernoulli noise. Our
method applies noise correction after estimating reward flip probabilities to
debias the learning signal, yielding provably unbiased gradient estimates.
Theoretical analysis shows that group-based methods inherently mitigate
individual-level noise, and our correction strategy amplifies this robustness.
Empirically, we observe consistent improvements across math and code tasks when
applying our noise correction to standard reward model usage, with particular
gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code
tasks under realistic reward model conditions. This work bridges label-noise
correction from supervised learning with modern RLHF, offering both theoretical
insights and a practical algorithm for noisy real-world deployment.

</details>


### [20] [BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.18927)
*Zhiheng Xi,Xin Guo,Yang Nan,Enyu Zhou,Junrui Shen,Wenxiang Chen,Jiaqi Liu,Jixuan Huang,Zhihao Zhang,Honglin Guo,Xun Deng,Zhikai Lei,Miao Zheng,Guoteng Wang,Shuo Zhang,Peng Sun,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: 提出 BAPO（Balanced Policy Optimization with Adaptive Clipping），通过自适应裁剪来平衡正负梯度贡献，保持策略熵并稳定离策略RL训练，在离线数据和分段回放等场景下实现更快、数据更高效的训练。


<details>
  <summary>Details</summary>
Motivation: 在离策略RL（使用过去策略数据）下训练LLM对齐时，策略熵下降、优化不稳定和梯度爆炸等问题突出。研究发现正向-优势样本在梯度中占主导，抑制有用行为并带来梯度爆炸风险；同时，PPO 等固定裁剪机制系统性地阻断熵增更新，导致策略过度开发，牺牲探索。

Method: 基于两点洞察，提出 BAPO，通过自适应裁剪界限动态调整，重新平衡正负贡献，保持熵并稳定优化。结合理论与经验分析，在样本回放、部分回放等离策略场景下实现更快、稳定和数据高效的训练。

Result: 在 AIME 2024/2025 基准上，7B 的 BAPO 超过开源对手（如 SkyWork-OR1-7B）；32B 的 BAPO 在同规模中达到最优，并超越 o3-mini、Gemini-2.5-Flash-Thinking 等旗舰系统。

Conclusion: BAPO 提供了一个简单但有效的离策略 RL 优化框架，通过自适应裁剪实现对熵的保护和梯度稳定性提升，提升样本效率和训练稳定性，具有提升 LLM 对齐性能的潜力。

Abstract: Reinforcement learning (RL) has recently become the core paradigm for
aligning and strengthening large language models (LLMs). Yet, applying RL in
off-policy settings--where stale data from past policies are used for
training--improves sample efficiency, but remains challenging: policy entropy
declines sharply, optimization often becomes unstable and may even collapse.
Through theoretical and empirical analysis, we identify two key insights: (i)
an imbalance in optimization, where negative-advantage samples dominate the
policy gradient, suppressing useful behaviors and risking gradient explosions;
and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping
mechanism in PPO-like objectives systematically blocks entropy-increasing
updates, thereby driving the policy toward over-exploitation at the expense of
exploration. Building on these insights, we propose BAlanced Policy
Optimization with Adaptive Clipping (BAPO), a simple yet effective method that
dynamically adjusts clipping bounds to adaptively re-balance positive and
negative contributions, preserve entropy, and stabilize RL optimization. Across
diverse off-policy scenarios--including sample replay and partial rollout--BAPO
achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025
benchmarks, our 7B BAPO model surpasses open-source counterparts such as
SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art
results among models of the same scale but also outperforms leading proprietary
systems like o3-mini and Gemini-2.5-Flash-Thinking.

</details>


### [21] [NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.18940)
*Zhi Zhang,Yixian Shen,Congfeng Cao,Ekaterina Shutova*

Main category: cs.LG

TL;DR: NeuroAda introduces a fine-grained PEFT method by identifying important parameters and adding bypass connections; only bypass parameters are trained, achieving state-of-the-art performance with extremely few trainable parameters and substantial memory savings.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off between addition-based PEFT methods (limited capacity) and selective fine-tuning (high memory usage) by enabling fine-grained adaptation with low memory.

Method: Identify important network connections via selective adaptation, insert bypass connections for these parameters, freeze original model, and train only the bypass parameters during fine-tuning.

Result: On 23+ NLP tasks spanning generation and understanding, NeuroAda achieves state-of-the-art performance while using as little as <=0.02% trainable parameters and reducing CUDA memory by up to 60%. Code released at provided URL.

Conclusion: NeuroAda demonstrates that targeted bypass-based fine-tuning can deliver high-performance, memory-efficient adaptation, offering a practical PEFT approach with extreme parameter efficiency.

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into
two categories: addition-based and selective in-situ adaptation. The former,
such as LoRA, introduce additional modules to adapt the model to downstream
tasks, offering strong memory efficiency. However, their representational
capacity is often limited, making them less suitable for fine-grained
adaptation. In contrast, the latter directly fine-tunes a carefully chosen
subset of the original model parameters, allowing for more precise and
effective adaptation, but at the cost of significantly increased memory
consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT
method that enables fine-grained model finetuning while maintaining high memory
efficiency. Our approach first identifies important parameters (i.e.,
connections within the network) as in selective adaptation, and then introduces
bypass connections for these selected parameters. During finetuning, only the
bypass connections are updated, leaving the original model parameters frozen.
Empirical results on 23+ tasks spanning both natural language generation and
understanding demonstrate that NeuroAda achieves state-of-the-art performance
with as little as $\leq \textbf{0.02}\%$ trainable parameters, while reducing
CUDA memory usage by up to 60%. We release our code here:
https://github.com/FightingFighting/NeuroAda.git.

</details>


### [22] [Towards Universal Solvers: Using PGD Attack in Active Learning to Increase Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE Solvers](https://arxiv.org/abs/2510.18989)
*Yifei Sun*

Main category: cs.LG

TL;DR: 通过对抗性教师-学生蒸馏与可微分谱解算子结合的训练框架，显著提升神经算子在出分布之外的鲁棒性，同时保持低参数量和快速推理。


<details>
  <summary>Details</summary>
Motivation: 现有高精度求解器需要巨量内存和昂贵计算，同时基于神经算子的快速推理在OOD泛化方面表现不佳。需一种在保持效率的同时提高鲁棒性的泛化方法。

Method: 提出一个对抗性教师-学生蒸馏框架：1) 使用可微分的数值谱算子作为教师，对紧凑型神经算子进行蒸馏。2) 采用PGD风格的主动采样循环，在平滑性与能量约束下搜索最差输入以扩充训练集。3) 通过可微分谱解算子实现梯度驱动的对抗搜索并稳定采样。

Result: 在 Burgers 方程和 Navier–Stokes 系统上，对抗性蒸馏显著提升了神经算子在OOD下的鲁棒性，同时保持低参数量级和快速推理。

Conclusion: 提出的对抗性教师-学生蒸馏框架有效提升神经PDE求解的鲁棒性与泛化，同时兼顾效率与参数规模，具有推广至更复杂PDE的潜力。

Abstract: Nonlinear PDE solvers require fine space-time discretizations and local
linearizations, leading to high memory cost and slow runtimes. Neural operators
such as FNOs and DeepONets offer fast single-shot inference by learning
function-to-function mappings and truncating high-frequency components, but
they suffer from poor out-of-distribution (OOD) generalization, often failing
on inputs outside the training distribution. We propose an adversarial
teacher-student distillation framework in which a differentiable numerical
solver supervises a compact neural operator while a PGD-style active sampling
loop searches for worst-case inputs under smoothness and energy constraints to
expand the training set. Using differentiable spectral solvers enables
gradient-based adversarial search and stabilizes sample mining. Experiments on
Burgers and Navier-Stokes systems demonstrate that adversarial distillation
substantially improves OOD robustness while preserving the low parameter cost
and fast inference of neural operators.

</details>


### [23] [An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version](https://arxiv.org/abs/2510.18998)
*Buang Zhang,Tung Kieu,Xiangfei Qiu,Chenjuan Guo,Jilin Hu,Aoying Zhou,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: 提出一种编码-解耦的新颖自编码器框架，通过将编码表示分解为稳定与辅助表示，并用基于互信息的度量替代重构误差来检测异常，在多种时间序列数据集上表现竞争力且对污染鲁棒。


<details>
  <summary>Details</summary>
Motivation: 无监督时间序列异常检测在大规模系统中具有重要意义，传统自编码器容易受到训练时序列中异常的污染，导致表示学习和异常分数的性能下降，因此需要更鲁棒的表示学习与检测指标。

Method: 引入encode-then-decompose范式：将编码后的表示分解为稳定表示和辅助表示，以提高对污染的鲁棒性；提出基于互信息的度量，替代常用的重构误差来识别异常。

Result: 在八个常用的多变量和单变量时间序列基准上达到竞争力或state-of-the-art的性能，并对不同污染比率的时间序列具有鲁棒性。

Conclusion: 所提方法通过分离稳定与辅助表示并以互信息为核心的检测度量，提升了对污染数据的鲁棒性及无监督时间序列异常检测的性能。

Abstract: Time series anomaly detection is important in modern large-scale systems and
is applied in a variety of domains to analyze and monitor the operation of
diverse systems. Unsupervised approaches have received widespread interest, as
they do not require anomaly labels during training, thus avoiding potentially
high costs and having wider applications. Among these, autoencoders have
received extensive attention. They use reconstruction errors from compressed
representations to define anomaly scores. However, representations learned by
autoencoders are sensitive to anomalies in training time series, causing
reduced accuracy. We propose a novel encode-then-decompose paradigm, where we
decompose the encoded representation into stable and auxiliary representations,
thereby enhancing the robustness when training with contaminated time series.
In addition, we propose a novel mutual information based metric to replace the
reconstruction errors for identifying anomalies. Our proposal demonstrates
competitive or state-of-the-art performance on eight commonly used multi- and
univariate time series benchmarks and exhibits robustness to time series with
different contamination ratios.

</details>


### [24] [Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records](https://arxiv.org/abs/2510.19014)
*Saman Nessari,Ali Bozorgi-Amiri*

Main category: cs.LG

TL;DR: 提出一个将大语言模型、条件表征GAN、T-学习者和情境赌博相结合的个性化临床决策框架，通过将非结构化文本转化为结构化数据、生成合成病人数据、预测治疗响应，并在线平衡探索与利用，在阶段III结肠癌数据中实现竞争性奖励并缓解冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 标准化的治疗框架忽视个体差异，导致治疗效果不理想，因此需要一个数据驱动、面向患者个体的决策系统以提升临床结局。

Method: 构建一个综合系统：使用大语言模型将非结构化的医疗叙述转化为结构化数据（准确率约93.2%）；用CTGAN生成现实的合成患者数据（两样本检验约55%的准确性）；采用T-learner预测患者的个体治疗响应（准确率约84.3%）；引入基于先验信息的上下文赌博以提升在线治疗选择，通过KernelUCB进行探索与利用的权衡；在阶段III结肠癌数据集上进行测试，进行5,000轮评估。

Result: KernelUCB在5,000轮中实现约0.60–0.61的平均奖励分数，领先于其他参考方法；系统还能缓解在线学习中的冷启动问题且提升计算效率。

Conclusion: 该系统代表个体化医学的重要进步，证实了在数据驱动的在线学习和个性化决策方面的潜力，但仍需在真实临床环境中进一步验证与整合。

Abstract: Current medical practice depends on standardized treatment frameworks and
empirical methodologies that neglect individual patient variations, leading to
suboptimal health outcomes. We develop a comprehensive system integrating Large
Language Models (LLMs), Conditional Tabular Generative Adversarial Networks
(CTGAN), T-learner counterfactual models, and contextual bandit approaches to
provide customized, data-informed clinical recommendations. The approach
utilizes LLMs to process unstructured medical narratives into structured
datasets (93.2% accuracy), uses CTGANs to produce realistic synthetic patient
data (55% accuracy via two-sample verification), deploys T-learners to forecast
patient-specific treatment responses (84.3% accuracy), and integrates
prior-informed contextual bandits to enhance online therapeutic selection by
effectively balancing exploration of new possibilities with exploitation of
existing knowledge. Testing on stage III colon cancer datasets revealed that
our KernelUCB approach obtained 0.60-0.61 average reward scores across 5,000
rounds, exceeding other reference methods. This comprehensive system overcomes
cold-start limitations in online learning environments, improves computational
effectiveness, and constitutes notable progress toward individualized medicine
adapted to specific patient characteristics.

</details>


### [25] [Empowering Decision Trees via Shape Function Branching](https://arxiv.org/abs/2510.19040)
*Nakul Upadhya,Eldan Cohen*

Main category: cs.LG

TL;DR: 提出 Shape Generalized Tree (SGT) 及其学习算法 ShapeCART，扩展到 S^2GT 与 SGT_K，提供可解释的非线性分割并在多数据集上实现更小模型但性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统决策树受限于轴对齐的一维分割，难以捕捉非线性特征效应，导致需要深树且可解释性下降；需要一种在单个分割内实现丰富非线性且易于可视化解释的新树结构。

Method: 引入 Shape Generalized Tree，在每个内部节点应用可学习的轴对齐形状函数对单一特征进行分割；提出 ShapeCART 学习算法；扩展到双变量形状函数 S^2GT 与多路树 SGT_K，并给出 Shape^2CART 与 ShapeCART_K。

Result: 在多组数据集上实验表明与传统轴对齐线性树相比，SGTs 能以更小的模型规模实现更好的性能。

Conclusion: SGTs 提供可解释的非线性分割并具备良好性能，能够通过可视化的节点形状函数直观解释模型决策机制；扩展框架覆盖多变量与多路结构，具备实用性。

Abstract: Decision trees are prized for their interpretability and strong performance
on tabular data. Yet, their reliance on simple axis-aligned linear splits often
forces deep, complex structures to capture non-linear feature effects,
undermining human comprehension of the constructed tree. To address this
limitation, we propose a novel generalization of a decision tree, the Shape
Generalized Tree (SGT), in which each internal node applies a learnable
axis-aligned shape function to a single feature, enabling rich, non-linear
partitioning in one split. As users can easily visualize each node's shape
function, SGTs are inherently interpretable and provide intuitive, visual
explanations of the model's decision mechanisms. To learn SGTs from data, we
propose ShapeCART, an efficient induction algorithm for SGTs. We further extend
the SGT framework to bivariate shape functions (S$^2$GT) and multi-way trees
(SGT$_K$), and present Shape$^2$CART and ShapeCART$_K$, extensions to ShapeCART
for learning S$^2$GTs and SGT$_K$s, respectively. Experiments on various
datasets show that SGTs achieve superior performance with reduced model size
compared to traditional axis-aligned linear trees.

</details>


### [26] [POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2510.19056)
*Kuai Yu,Xiaoyu Wu,Peishen Yan,Qingqian Yang,Linshan Jiang,Hao Wang,Yang Hua,Tao Song,Haibing Guan*

Main category: cs.LG

TL;DR: POLAR is a lightweight RL-based framework that selects backdoor-critical layers in federated learning using Bernoulli sampling and policy gradient, achieving higher backdoor success while restricting the footprint to stay stealthy; outperforms six SOTA defenses by up to 40%.


<details>
  <summary>Details</summary>
Motivation: Federated Learning is vulnerable to backdoor attacks; existing BC-layer methods are rule-based and ignore inter-layer interactions, limiting effectiveness and detectability.

Method: Proposes POLAR: a policy-based, layer-wise reinforcement learning pipeline with Bernoulli sampling. It learns a layer-selection strategy by optimizing backdoor success rate via policy gradient updates, and uses a regularization term to limit the number of modified layers to maintain stealth.

Result: Empirical evaluation shows POLAR outperforms latest attack methods by up to 40% against six SOTA defenses.

Conclusion: POLAR demonstrates that RL-based layer-wise backdoor attacks can be more effective and stealthier than rule-based approaches, highlighting the need for defenses tailored to RL-driven strategies and suggesting future work to mitigate such attacks.

Abstract: Federated Learning (FL) enables decentralized model training across multiple
clients without exposing local data, but its distributed feature makes it
vulnerable to backdoor attacks. Despite early FL backdoor attacks modifying
entire models, recent studies have explored the concept of backdoor-critical
(BC) layers, which poison the chosen influential layers to maintain
stealthiness while achieving high effectiveness. However, existing BC layers
approaches rely on rule-based selection without consideration of the
interrelations between layers, making them ineffective and prone to detection
by advanced defenses. In this paper, we propose POLAR (POlicy-based LAyerwise
Reinforcement learning), the first pipeline to creatively adopt RL to solve the
BC layer selection problem in layer-wise backdoor attack. Different from other
commonly used RL paradigm, POLAR is lightweight with Bernoulli sampling. POLAR
dynamically learns an attack strategy, optimizing layer selection using policy
gradient updates based on backdoor success rate (BSR) improvements. To ensure
stealthiness, we introduce a regularization constraint that limits the number
of modified layers by penalizing large attack footprints. Extensive experiments
demonstrate that POLAR outperforms the latest attack methods by up to 40%
against six state-of-the-art (SOTA) defenses.

</details>


### [27] [Weight Decay may matter more than muP for Learning Rate Transfer in Practice](https://arxiv.org/abs/2510.19093)
*Atli Kosson,Jeremy Welborn,Yang Liu,Martin Jaggi,Xi Chen*

Main category: cs.LG

TL;DR: 在大规模训练中，muP 的学习率缩放并非普遍有效，后续阶段由权重衰减主导跨宽度的更新稳定性，muP 主要相当于隐式学习率预热，可以用改进的预热方案替代。


<details>
  <summary>Details</summary>
Motivation: 量化评估在大模型训练中学习率迁移的有效性，检验 muP 的几何对齐假设在实际训练中的成立时间，以及为何需要独立权重衰减等实践。

Method: 进行大规模、跨宽度的经验研究，比较 muP 缩放与常规模型在更新动态、输入-权重-梯度对齐等方面的行为，考察从初始阶段到训练中后期的变化，并评估改良的预热方案。

Result: muP 的关键假设仅在训练初期成立；训练后期跨宽度的更新稳定性主要由权重衰减实现，muP 更像隐式预热。可通过修改的预热计划替代 muP；这解释了为何 muP 需要独立权重衰减来实现迁移。

Conclusion: 这项工作挑战了关于学习率迁移的主流观点，强调权重衰减在跨宽度稳定性中的核心作用，并给出在实际大模型训练中优先采用改良的预热策略的建议，同时解释了为何 muP 的独立权重衰减要求是必要的。

Abstract: Transferring the optimal learning rate from small to large neural networks
can enable efficient training at scales where hyperparameter tuning is
otherwise prohibitively expensive. To this end, the Maximal Update
Parameterization (muP) proposes a learning rate scaling designed to keep the
update dynamics of internal representations stable across different model
widths. However, the scaling rules of muP rely on strong assumptions,
particularly about the geometric alignment of a layer's inputs with both its
weights and gradient updates. In this large-scale empirical investigation, we
show that these assumptions hold only briefly at the start of training in the
practical setups where learning rate transfer is most valuable, such as LLM
training. For the remainder of training it is weight decay rather than muP that
correctly stabilizes the update dynamics of internal representations across
widths, facilitating learning rate transfer. This suggests muP's scaling
primarily acts as a form of implicit learning rate warmup, allowing us to
largely replace it with modified warmup schedules. Together these findings
fundamentally challenge prevailing beliefs about learning rate transfer and can
explain empirical practice such as why muP requires the independent weight
decay variant for successful transfer.

</details>


### [28] [What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning](https://arxiv.org/abs/2510.19099)
*Yaning Jia,Chunhui Zhang,Xingjian Diao,Xiangchi Yuan,Zhongyu Ouyang,soroush vosoughi*

Main category: cs.LG

TL;DR: 提出一个五维难度的统一离线评估框架来评估 CL 对 LLMs 的影响，在多种模型和数学推理任务上对前向与反向 CL 进行对比，发现没有普适策略；样本难度不同对收益有显著差异；任务对齐的课程偏向影响最终表征与泛化，内部状态导向的课程则影响信心与不确定性。


<details>
  <summary>Details</summary>
Motivation: 尽管课程学习在提升LLMs推理方面有潜力，但现有研究使用的困难度量和训练设置不统一，使得何时、哪种方向以及何种度量能带来收益仍不清楚。本文提出一个统一、可复现的框架，用五个互补维度来分解课程难度，并系统回答这些核心问题。

Method: 构建离线评价框架，将课程难度分解为 Problem Difficulty、Model Surprisal、Confidence Margin、Predictive Uncertainty、Decision Variability。对数学推理基准在 Llama3.1-8B、Mistral-7B、Gemma3-4B 上进行受控的后训练实验，比较前向（easy→hard）与反向（hard→easy）CL，并分析在不同维度下样本难度对学习效果的影响。

Result: - 没有普适的前向/反向CL策略，效果取决于模型能力与任务复杂性。- 即使在同一度量下，不同难度级别的样本对任务需求产生不同的收益。- 任务对齐的课程主要影响模型的最终表示和泛化，而内部状态（如自信、预测不确定性）则受内在状态导向的课程影响。

Conclusion: 结论是：不存在一刀切的课程策略；需要结合模型能力、任务复杂性和评估指标来设计课程；在某些指标上，优先选择决策不确定性样本可能进一步提升学习效果。

Abstract: Curriculum learning (CL) - ordering training data from easy to hard - has
become a popular strategy for improving reasoning in large language models
(LLMs). Yet prior work employs disparate difficulty metrics and training
setups, leaving open fundamental questions: When does curriculum help? Which
direction - forward or reverse - is better? And does the answer depend on what
we measure? We address these questions through a unified offline evaluation
framework that decomposes curriculum difficulty into five complementary
dimensions: Problem Difficulty, Model Surprisal, Confidence Margin, Predictive
Uncertainty, and Decision Variability. Through controlled post-training
experiments on mathematical reasoning benchmarks with Llama3.1-8B, Mistral-7B,
and Gemma3-4B, we find that (i) no curriculum strategy dominates universally -
the relative effectiveness of forward versus reverse CL depends jointly on
model capability and task complexity; (ii) even within a single metric, samples
at different difficulty levels produce distinct gains depending on task
demands; and (iii) task-aligned curricula focus on shaping the model's final
representations and generalization, whereas inner-state curricula modulate
internal states such as confidence and uncertainty. Our findings challenge the
notion of a universal curriculum strategy and offer actionable guidance across
model and task regimes, with some metrics indicating that prioritizing
decision-uncertain samples can further enhance learning outcomes.

</details>


### [29] [The Tail Tells All: Estimating Model-Level Membership Inference Vulnerability Without Reference Models](https://arxiv.org/abs/2510.19773)
*Euodia Dodd,Nataša Krčo,Igor Shilov,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: 提出一种无参考模型的模型级隐私风险估计方法，通过高损失区域缺失信号来预测对成员推断攻击的易感性，能够在低FPR下以TPR衡量，并有效预测对SOTA攻击LiRA的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的成员推断攻击评估依赖大量参考模型、成本高，难以在实际场景中进行低成本、可扩展的模型级隐私风险评估。需要基于分布的信号来替代昂贵的参考模型。

Method: 利用训练与测试分布的损失分布的非对称性与重尾特性，提出一个无需参考模型的风险估计方法：通过高损失区域缺失(outliers absence)来预测风险；以一个简单的损失攻击的真阴性率（TPR at低FPR）衡量模型级易感性；在广泛的架构和数据集上评估，与LiRA等SOTA攻击对齐，并与RMIA及其他分布差异度量比较，同时探讨用非线性函数评估风险在大语言模型中的潜力。

Result: 实证表明，该无参方法能在多种架构和数据集上准确估计对LiRA的易感性；相比低成本攻击RMIA及其他分布差异度量，表现更优；对非线性风险评估在大语言模型上的探索展现出潜力。

Conclusion: 提出一个基于高损失区域缺失信号的无参考MIA脆弱性估计框架，成本低、可扩展，适用于广泛模型，尤其是大语言模型的隐私风险评估。

Abstract: Membership inference attacks (MIAs) have emerged as the standard tool for
evaluating the privacy risks of AI models. However, state-of-the-art attacks
require training numerous, often computationally expensive, reference models,
limiting their practicality. We present a novel approach for estimating
model-level vulnerability, the TPR at low FPR, to membership inference attacks
without requiring reference models. Empirical analysis shows loss distributions
to be asymmetric and heavy-tailed and suggests that most points at risk from
MIAs have moved from the tail (high-loss region) to the head (low-loss region)
of the distribution after training. We leverage this insight to propose a
method to estimate model-level vulnerability from the training and testing
distribution alone: using the absence of outliers from the high-loss region as
a predictor of the risk. We evaluate our method, the TNR of a simple loss
attack, across a wide range of architectures and datasets and show it to
accurately estimate model-level vulnerability to the SOTA MIA attack (LiRA). We
also show our method to outperform both low-cost (few reference models) attacks
such as RMIA and other measures of distribution difference. We finally evaluate
the use of non-linear functions to evaluate risk and show the approach to be
promising to evaluate the risk in large-language models.

</details>


### [30] [Steering Autoregressive Music Generation with Recursive Feature Machines](https://arxiv.org/abs/2510.19127)
*Daniel Zhao,Daniel Beaglehole,Taylor Berg-Kirkpatrick,Julian McAuley,Zachary Novack*

Main category: cs.LG

TL;DR: 提出 MusicRFM 框架，通过对冻结的预训练音乐模型内部激活进行可解释的控制，利用递归特征机器（RFM）探针在隐藏状态中发现“概念方向”，以实时、可控地引导音乐生成，无需逐步优化或重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决可控音乐生成的挑战：现有方法常需模型重训练或引入可听的伪影，本文提供一个轻量、可解释且可以在推理时动态控制的方案。

Method: 训练轻量级的 RFM 探针以在音乐模型的隐藏状态中发现与音乐属性（如音符、和弦）对应的方向；在推理阶段将这些方向注入模型以引导生成；引入动态时间调度和多属性约束等机制实现对多属性的同时控制，且不进行逐步优化。

Result: 在目标音符生成上，准确性从 0.23 提升到 0.82；文本提示遵循度仅受约0.02 的轻微下降，显示在提升可控性的同时对提示忠实度影响很小。

Conclusion: 展示了将 RFMs 应用于冻结音乐模型以实现可控性的可行性，提出了可扩展的时间调度与多属性控制策略，并公开代码，未来可在音乐领域及跨模态模型中扩展应用。

Abstract: Controllable music generation remains a significant challenge, with existing
methods often requiring model retraining or introducing audible artifacts. We
introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs)
to enable fine-grained, interpretable control over frozen, pre-trained music
models by directly steering their internal activations. RFMs analyze a model's
internal gradients to produce interpretable "concept directions", or specific
axes in the activation space that correspond to musical attributes like notes
or chords. We first train lightweight RFM probes to discover these directions
within MusicGen's hidden states; then, during inference, we inject them back
into the model to guide the generation process in real-time without per-step
optimization. We present advanced mechanisms for this control, including
dynamic, time-varying schedules and methods for the simultaneous enforcement of
multiple musical properties. Our method successfully navigates the trade-off
between control and generation quality: we can increase the accuracy of
generating a target musical note from 0.23 to 0.82, while text prompt adherence
remains within approximately 0.02 of the unsteered baseline, demonstrating
effective control with minimal impact on prompt fidelity. We release code to
encourage further exploration on RFMs in the music domain.

</details>


### [31] [Subliminal Corruption: Mechanisms, Thresholds, and Interpretability](https://arxiv.org/abs/2510.19152)
*Reya Vir,Sarvesh Bhatnagar*

Main category: cs.LG

TL;DR: Subliminal corruption in synthetic data can cause abrupt, phase-transition-like degradation in model alignment, with three key findings and implications for safety protocols.


<details>
  <summary>Details</summary>
Motivation: As ML systems rely increasingly on synthetic data, latent misalignments can propagate through interconnected models. A quantitative understanding of subliminal corruption’s scaling, thresholds, and mechanisms is essential for robust safety.

Method: A teacher-student experimental setup using GPT-2 to study subliminal corruption, examining scaling laws, critical thresholds, and underlying mechanisms. Includes behavioral crossover analysis, a sharp phase-transition observation, and interpretability studies.

Result: Three main findings: (1) Subliminal corruption degrades overall alignment, not just the targeted trait; (2) Alignment fails via a sharp phase transition at a critical poisoned-data threshold; (3) The mechanism mimics the model’s natural fine-tuning, hindering detection.

Conclusion: Demonstrates a critical vulnerability in AI systems that rely on synthetic data and underscores the need for safety protocols accounting for latent, hard-to-detect threats.

Abstract: As machine learning models are increasingly fine-tuned on synthetic data,
there is a critical risk of subtle misalignments spreading through
interconnected AI systems. This paper investigates subliminal corruption, which
we define as undesirable traits are transmitted through semantically neutral
data, bypassing standard safety checks. While this phenomenon has been
identified, a quantitative understanding of its dynamics is missing. To address
this gap, we present a systematic study of the scaling laws, thresholds, and
mechanisms of subliminal corruption using a teacher-student setup with GPT-2.
Our experiments reveal three key findings: (1) subliminal corruption causes
behavioral crossover, degrading the model's overall alignment, not just the
targeted trait; (2) alignment fails in a sharp phase transition at a critical
threshold of poisoned data, rather than degrading gradually; and (3)
interpretability analysis shows the corruption mechanism mimics the model's
natural fine-tuning process, making it difficult to detect. These results
demonstrate a critical vulnerability in AI systems that rely on synthetic data
and highlight the need for new safety protocols that can account for latent
threats.

</details>


### [32] [Feature Space Adaptation for Robust Model Fine-Tuning](https://arxiv.org/abs/2510.19155)
*Peng Wang,Minghao Gu,Qiang Huang*

Main category: cs.LG

TL;DR: 提出在特征空间进行微调的两种新方法（LoRFA与VeFA），以缓解灾难性遗忘并提高鲁棒性，在图像分类、自然语言理解与生成任务上与LoRA相比具有相当的微调性能但鲁棒性更强。


<details>
  <summary>Details</summary>
Motivation: 在微调中，尤其是目标领域数据有限或与预训练分布差异较大时，容易发生灾难性遗忘。现有的参数高效微调多在权重空间修改模型参数，容易使模型对下游数据过拟合且覆盖到的知识受损。需要一种能在不显著干扰预训练知识的条件下提升对分布偏移的鲁棒性的微调方式。

Method: 提出在特征空间进行微调的两种方法：LoRFA（低秩特征自适应）和VeFA（基于向量的特征自适应）。受效应等价建模（EEM）思想启发，通过对观测特征上等价因子进行轻量级的特征级变换，补偿下游潜在变量的影响，从而在保持预训练表示的同时提升泛化能力。与LoRA等权重空间方法对比，在图像分类、自然语言理解与生成任务中评估标准微调指标及鲁棒性。

Result: 与LoRA相比，特征空间自适应在微调结果上具有可比性，但在鲁棒性方面表现更强，且在多模态任务中对分布偏移的抵抗力更突出。整体上，LoRFA与VeFA实现了对预训练知识的较好保留与更稳定的鲁棒性提升。

Conclusion: 特征空间微调为高效的微调策略提供了有效替代，能在保持预训练知识的同时提升对分布偏移的鲁棒性，适用于图像、NLU与NLG等多模态任务，且与权重空间方法（如LoRA）具有互补性。

Abstract: Catastrophic forgetting is a common issue in model fine-tuning, especially
when the downstream domain contains limited labeled data or differs greatly
from the pre-training distribution. Existing parameter-efficient fine-tuning
methods operate in the weight space by modifying or augmenting the pre-trained
model's parameters, which can yield models overly specialized to the available
downstream data. To mitigate the risk of overwriting pre-trained knowledge and
enhance robustness, we propose to fine-tune the pre-trained model in the
feature space. Two new fine-tuning methods are proposed: LoRFA (Low-Rank
Feature Adaptation) and VeFA (Vector-Based Feature Adaptation). Feature space
adaptation is inspired by the idea of effect equivalence modeling (EEM) of
downstream lurking variables causing distribution shifts, which posits that
unobserved factors can be represented as the total equivalent amount on
observed features. By compensating for the effects of downstream lurking
variables via a lightweight feature-level transformation, the pre-trained
representations can be preserved, which improves model generalization under
distribution shift. We evaluate LoRFA and VeFA versus LoRA on image
classification, NLU, and NLG, covering both standard fine-tuning metrics and
robustness. Feature space adaptation achieves comparable fine-tuning results
and consistently stronger robustness.

</details>


### [33] [Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring](https://arxiv.org/abs/2510.19158)
*Federico Di Gennaro,Khaled Eldowa,Nicolò Cesa-Bianchi*

Main category: cs.LG

TL;DR: 在对经典部分监控的线性推广下，研究了对抗性有限行动设置中的探索-优化方法，给出与博弈结构相关的实例化后悔界限，在线性可观测性下达到 sqrt(T)，在全局可观测性下达到 T^{2/3}，并展示这些界限在若干设置中的紧性。


<details>
  <summary>Details</summary>
Motivation: 将部分监控扩展到可观测空间无限、但损失和观察均线性结构；作为线性带宽的推广，打通损失与反馈的解耦，目标是在对抗性环境中获得透明、实例依赖的后悔界限。

Method: 采用简单且高效实现的探索-通过-优化（exploration-by-optimization）框架，推导出依赖于游戏结构的后悔界限；引入表征观测-损失对齐程度的实例特定量，并在若干旧/新部分信息设定下进行实例化。

Result: 得到对抗性有限动作情形下的后悔界限，含实例相关量；在易（局部可观测）游戏中达到 sqrt(T)，在难（全局可观测）游戏中达到 T^{2/3}；并指出这些结构依赖在某些情形下是紧的。

Conclusion: 该框架将多种旧/新部分信息设定统一在线性部分监控的框架中，强调博弈结构对后悔的影响，并在易/难情形下给出相应的理论界限，揭示了对复杂信息结构的可行性和界限。

Abstract: In contrast to the classic formulation of partial monitoring, linear partial
monitoring can model infinite outcome spaces, while imposing a linear structure
on both the losses and the observations. This setting can be viewed as a
generalization of linear bandits where loss and feedback are decoupled in a
flexible manner. In this work, we address a nonstochastic (adversarial),
finite-actions version of the problem through a simple instance of the
exploration-by-optimization method that is amenable to efficient
implementation. We derive regret bounds that depend on the game structure in a
more transparent manner than previous theoretical guarantees for this paradigm.
Our bounds feature instance-specific quantities that reflect the degree of
alignment between observations and losses, and resemble known guarantees in the
stochastic setting. Notably, they achieve the standard $\sqrt{T}$ rate in easy
(locally observable) games and $T^{2/3}$ in hard (globally observable) games,
where $T$ is the time horizon. We instantiate these bounds in a selection of
old and new partial information settings subsumed by this model, and illustrate
that the achieved dependence on the game structure can be tight in interesting
cases.

</details>


### [34] [Preliminary Use of Vision Language Model Driven Extraction of Mouse Behavior Towards Understanding Fear Expression](https://arxiv.org/abs/2510.19160)
*Paimon Goulart,Jordan Steinhauser,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 通过视觉-语言模型将视频与文本信息结合，直接对小鼠在不同环境中的行为进行时序分类，输出每个体的行为向量，且无需微调模型即可在多类行为，甚至罕见行为（如冻结、逃跑）上取得较高的F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决跨学科研究需要将多源数据（视频与文本）整合以提升对动物行为的理解，目标是建立可生成高准确度、低人工输入的可扩展数据集，覆盖时间维度与多环境条件。

Method: 采用开源的Qwen2.5-VL模型，通过提示工程、上下文学习（ICL）与逐帧预处理来增强性能，并在不对模型进行微调的前提下实现行为分类，输出每个被试在每个会话中的时序行为向量。

Result: 在所有行为类别上提升了分类性能，且对罕见类别（如冻结、逃跑）也表现出强F1分数；不同方法的结合达到最佳效果，显示该方法在跨时间与环境的综合数据集构建中具有较高的实用性。

Conclusion: 该工作将有助于跨学科研究者将多时点、跨环境的行为特征整合为一个全面的数据集，进而支持复杂研究问题的解答。

Abstract: Integration of diverse data will be a pivotal step towards improving
scientific explorations in many disciplines. This work establishes a
vision-language model (VLM) that encodes videos with text input in order to
classify various behaviors of a mouse existing in and engaging with their
environment. Importantly, this model produces a behavioral vector over time for
each subject and for each session the subject undergoes. The output is a
valuable dataset that few programs are able to produce with as high accuracy
and with minimal user input. Specifically, we use the open-source Qwen2.5-VL
model and enhance its performance through prompts, in-context learning (ICL)
with labeled examples, and frame-level preprocessing. We found that each of
these methods contributes to improved classification, and that combining them
results in strong F1 scores across all behaviors, including rare classes like
freezing and fleeing, without any model fine-tuning. Overall, this model will
support interdisciplinary researchers studying mouse behavior by enabling them
to integrate diverse behavioral features, measured across multiple time points
and environments, into a comprehensive dataset that can address complex
research questions.

</details>


### [35] [Natural Gradient VI: Guarantees for Non-Conjugate Models](https://arxiv.org/abs/2510.19163)
*Fangyuan Sun,Ilyas Fatkhullin,Niao He*

Main category: cs.LG

TL;DR: 在非共轭场景下对NGVI的几何与收敛进行理论分析：给出相对光滑性条件、提出含非欧几里得投影的改进算法并证明全局收敛，以及在额外结构假设下实现快速全局收敛。


<details>
  <summary>Details</summary>
Motivation: 弥补对净化式(NGVI)理论的空白，尤其在非共轭似然下，变分损失非凸且分析困难；揭示NGVI在广义几何下的收敛性质。

Method: 证明变分损失相对于镜像映射的相对光滑性条件；提出结合非欧几里得投影的改进NGVI算法，并证明其全局非渐近收敛到驻点；在对数似然等额外结构假设下揭示隐藏的凸性并实现快速全局收敛。

Result: 给出满足相对光滑性的充分条件；提出并分析改进的NGVI算法，证明其全局收敛到驻点；在额外的 likelihood 结构下发现隐藏凸性，确保NGVI的快速全局收敛。

Conclusion: 加强对NGVI在非共轭推断中的几何与收敛理解，为在复杂模型中的理论支撑提供新视角。

Abstract: Stochastic Natural Gradient Variational Inference (NGVI) is a widely used
method for approximating posterior distribution in probabilistic models.
Despite its empirical success and foundational role in variational inference,
its theoretical underpinnings remain limited, particularly in the case of
non-conjugate likelihoods. While NGVI has been shown to be a special instance
of Stochastic Mirror Descent, and recent work has provided convergence
guarantees using relative smoothness and strong convexity for conjugate models,
these results do not extend to the non-conjugate setting, where the variational
loss becomes non-convex and harder to analyze. In this work, we focus on
mean-field parameterization and advance the theoretical understanding of NGVI
in three key directions. First, we derive sufficient conditions under which the
variational loss satisfies relative smoothness with respect to a suitable
mirror map. Second, leveraging this structure, we propose a modified NGVI
algorithm incorporating non-Euclidean projections and prove its global
non-asymptotic convergence to a stationary point. Finally, under additional
structural assumptions about the likelihood, we uncover hidden convexity
properties of the variational loss and establish fast global convergence of
NGVI to a global optimum. These results provide new insights into the geometry
and convergence behavior of NGVI in challenging inference settings.

</details>


### [36] [Imbalanced Gradients in RL Post-Training of Multi-Task LLMs](https://arxiv.org/abs/2510.19178)
*Runzhe Wu,Ankur Samanta,Ayush Jain,Scott Fujimoto,Jeongyeol Kwon,Ben Kretzu,Youliang Yu,Kaveh Hassani,Boris Vidolov,Yonathan Efroni*

Main category: cs.LG

TL;DR: RL后训练中跨任务梯度不平衡会导致对任务的偏置更新；更大的梯度并不一定带来更大的学习收益，因此需要在梯度层面进行校正的研究。


<details>
  <summary>Details</summary>
Motivation: 多任务后训练往往混合不同任务的数据集并联合优化，默认假设各任务梯度幅值相近；但在强化学习信号下，这一假设可能失效，某些任务产生显著更大的梯度，从而主导更新过程。

Method: 通过对RL后训练阶段的多任务进行系统分析，比较各任务的梯度大小与对应的学习收益，进一步考察其与常规训练统计（如奖励、优势）的关系，评估梯度不平衡的成因是否来自任务间固有差异，并讨论需要的梯度层面校正策略。

Result: 发现存在显著的梯度不平衡：部分任务的梯度远大于其他任务，但它们并不必然带来更高的学习收益，甚至可能收益较小。该不平衡不能用训练统计如奖励或优势来解释，暗示源于任务间的固有差异。

Conclusion: 对 naive 的数据集混合需保持谨慎，未来需在梯度层面提出更 principled 的校正方法，以实现更均衡且有效的跨任务学习。

Abstract: Multi-task post-training of large language models (LLMs) is typically
performed by mixing datasets from different tasks and optimizing them jointly.
This approach implicitly assumes that all tasks contribute gradients of similar
magnitudes; when this assumption fails, optimization becomes biased toward
large-gradient tasks. In this paper, however, we show that this assumption
fails in RL post-training: certain tasks produce significantly larger
gradients, thus biasing updates toward those tasks. Such gradient imbalance
would be justified only if larger gradients implied larger learning gains on
the tasks (i.e., larger performance improvements) -- but we find this is not
true. Large-gradient tasks can achieve similar or even much lower learning
gains than small-gradient ones. Further analyses reveal that these gradient
imbalances cannot be explained by typical training statistics such as training
rewards or advantages, suggesting that they arise from the inherent differences
between tasks. This cautions against naive dataset mixing and calls for future
work on principled gradient-level corrections for LLMs.

</details>


### [37] [Enhancing Graph Neural Networks: A Mutual Learning Approach](https://arxiv.org/abs/2510.19223)
*Paul Agbaje,Akajyoti Mitra,Afia Anjum,Pranali Khose,Ebelechukwu Nwafor,Habeeb Olufowobi*

Main category: cs.LG

TL;DR: Collaborative mutual learning among GNNs without a pre-trained teacher improves performance via ensemble-based mutual teaching, adaptive logit weighting, and entropy enhancement.


<details>
  <summary>Details</summary>
Motivation: Overcome reliance on pre-trained teacher models in knowledge distillation; enable lightweight GNNs to collaborate and improve performance across multiple tasks.

Method: An ensemble of student GNNs mutually teach each other during training, with an adaptive logit weighting unit to regulate knowledge exchange and an entropy enhancement technique to boost mutual learning.

Result: Experiments on node and graph classification datasets (three datasets per task) show improved performance and robustness of the collaborative framework compared with baselines.

Conclusion: Collaborative learning among shallow GNNs with dynamic knowledge exchange effectively improves downstream task performance, reducing dependence on teacher models.

Abstract: Knowledge distillation (KD) techniques have emerged as a powerful tool for
transferring expertise from complex teacher models to lightweight student
models, particularly beneficial for deploying high-performance models in
resource-constrained devices. This approach has been successfully applied to
graph neural networks (GNNs), harnessing their expressive capabilities to
generate node embeddings that capture structural and feature-related
information. In this study, we depart from the conventional KD approach by
exploring the potential of collaborative learning among GNNs. In the absence of
a pre-trained teacher model, we show that relatively simple and shallow GNN
architectures can synergetically learn efficient models capable of performing
better during inference, particularly in tackling multiple tasks. We propose a
collaborative learning framework where ensembles of student GNNs mutually teach
each other throughout the training process. We introduce an adaptive logit
weighting unit to facilitate efficient knowledge exchange among models and an
entropy enhancement technique to improve mutual learning. These components
dynamically empower the models to adapt their learning strategies during
training, optimizing their performance for downstream tasks. Extensive
experiments conducted on three datasets each for node and graph classification
demonstrate the effectiveness of our approach.

</details>


### [38] [Controllable Machine Unlearning via Gradient Pivoting](https://arxiv.org/abs/2510.19226)
*Youngsik Hwang,Dong-Young Lim*

Main category: cs.LG

TL;DR: 将机器忘记问题从单目标优化转化为多目标优化，提出Pivoting Gradient的CUP算法，通过单一超参数“unlearning intensity”在Pareto前沿上可控导航，使用超体积指标评估全局权衡，实验在多种视觉任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决SOO框架下的三大挑战：对忘除力度的过度，缺乏对忘除过程的细粒度控制，以及难以用单一指标评估权衡质量与多样性，需提供可控且综合的去学习解决方案。

Method: 将MU建模为多目标优化问题，提出CUP算法及其独特的Pivoting Gradient机制，使算法不收敛到单一解，而是能够沿着Pareto前沿全局探索。通过单一超参数“unlearning intensity”来控制权衡强度，使用超体积指标来度量整个解集的质量与多样性，评估探索能力与权衡效果。

Result: 实验结果显示CUP生成的Pareto最优解集合在多种视觉任务上显著优于现有方法，展现出更好的权衡与多样性。

Conclusion: 提供了一种更可控、全面评估的机器忘记框架，能够在隐私保护和模型性能之间实现更灵活的权衡，具备较强的泛化潜力与扩展性。

Abstract: Machine unlearning (MU) aims to remove the influence of specific data from a
trained model. However, approximate unlearning methods, often formulated as a
single-objective optimization (SOO) problem, face a critical trade-off between
unlearning efficacy and model fidelity. This leads to three primary challenges:
the risk of over-forgetting, a lack of fine-grained control over the unlearning
process, and the absence of metrics to holistically evaluate the trade-off. To
address these issues, we reframe MU as a multi-objective optimization (MOO)
problem. We then introduce a novel algorithm, Controllable Unlearning by
Pivoting Gradient (CUP), which features a unique pivoting mechanism. Unlike
traditional MOO methods that converge to a single solution, CUP's mechanism is
designed to controllably navigate the entire Pareto frontier. This navigation
is governed by a single intuitive hyperparameter, the `unlearning intensity',
which allows for precise selection of a desired trade-off. To evaluate this
capability, we adopt the hypervolume indicator, a metric that captures both the
quality and diversity of the entire set of solutions an algorithm can generate.
Our experimental results demonstrate that CUP produces a superior set of
Pareto-optimal solutions, consistently outperforming existing methods across
various vision tasks.

</details>


### [39] [Brain-Inspired Perspective on Configurations: Unsupervised Similarity and Early Cognition](https://arxiv.org/abs/2510.19229)
*Juntang Wang,Yihan Wang,Hao Wu,Dongmian Zou,Shixin Xu*

Main category: cs.LG

TL;DR: 提出一种脑启发的有限分辨率聚类框架（配置），通过吸引-排斥动力实现层级、对新颖性敏感和灵活适应，并引入 mheatmap 进行公平多分辨率评估；在多数据集上表现优越，尤其在新颖性检测和动态类别演化稳定性方面。


<details>
  <summary>Details</summary>
Motivation: 解决在无监督情境下婴儿式的类别发现、对新颖性的敏感性以及对新情境的自适应能力的挑战，提供一个认知启发的聚类机制，弥补现有机器学习在这些方面的不足。

Method: 提出一个单一分辨率参数的有限分辨率聚类框架，利用 attraction-repulsion（吸引-排斥）动力实现层级结构、对新颖性的敏感性以及对动态环境的自适应。为评估多分辨率和动态行为，开发了 mheatmap及其重新分配算法，以公平比较不同配置的表现。

Result: 在多数据集上，配置方法在标准聚类指标上具有竞争力；新颖性检测的 AUC 达到 87%；在动态类别演化过程中的稳定性提升约 35%。

Conclusion: 将配置视为早期认知分类的原理性计算模型，以及朝向脑启发AI的重要一步；同时提供一个可扩展的多分辨率评估框架，为未来在认知与AI领域的探索提供工具与基准。

Abstract: Infants discover categories, detect novelty, and adapt to new contexts
without supervision -- a challenge for current machine learning. We present a
brain-inspired perspective on configurations, a finite-resolution clustering
framework that uses a single resolution parameter and attraction-repulsion
dynamics to yield hierarchical organization, novelty sensitivity, and flexible
adaptation. To evaluate these properties, we introduce mheatmap, which provides
proportional heatmaps and a reassignment algorithm to fairly assess
multi-resolution and dynamic behavior. Across datasets, configurations are
competitive on standard clustering metrics, achieve 87% AUC in novelty
detection, and show 35% better stability during dynamic category evolution.
These results position configurations as a principled computational model of
early cognitive categorization and a step toward brain-inspired AI.

</details>


### [40] [Understanding the Implicit Biases of Design Choices for Time Series Foundation Models](https://arxiv.org/abs/2510.19236)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 通过理论+实验，系统揭秘TSFM设计参数（补丁大小、嵌入选择、训练目标等）如何引入隐性偏差，影响时间行为、几何结构、对均值回归的强度，并揭示偏置之间的复杂交互及对构建TSFMs的启示。


<details>
  <summary>Details</summary>
Motivation: 旨在理解训练过程中的设计选择如何塑造时序 foundation 模型的行为，而非仅仅在现有基准上取得胜利。

Method: 结合理论分析与受控实证评估，考察多种设计 knob（如补丁大小、嵌入方式、训练目标等）对模型基本属性的影响，并通过离群点处理等案例研究揭示偏置的相互作用。

Result: 识别并量化这些设计选择导致的隐性偏差，发现偏置可能直观或反直觉，且在不同数据与模型属性下有复杂交互；案例研究展示偏置如何在实际任务中叠加影响。

Conclusion: 模型设计参数会影响基本属性与泛化能力，需结合数据特征审慎选择训练目标与嵌入等设计，以避免负面隐性效应并提升TSFM的鲁棒性与可解释性。

Abstract: Time series foundation models (TSFMs) are a class of potentially powerful,
general-purpose tools for time series forecasting and related temporal tasks,
but their behavior is strongly shaped by subtle inductive biases in their
design. Rather than developing a new model and claiming that it is better than
existing TSFMs, e.g., by winning on existing well-established benchmarks, our
objective is to understand how the various ``knobs'' of the training process
affect model quality. Using a mix of theory and controlled empirical
evaluation, we identify several design choices (patch size, embedding choice,
training objective, etc.) and show how they lead to implicit biases in
fundamental model properties (temporal behavior, geometric structure, how
aggressively or not the model regresses to the mean, etc.); and we show how
these biases can be intuitive or very counterintuitive, depending on properties
of the model and data. We also illustrate in a case study on outlier handling
how multiple biases can interact in complex ways; and we discuss implications
of our results for learning the bitter lesson and building TSFMs.

</details>


### [41] [SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes](https://arxiv.org/abs/2510.19241)
*Xuyuan Xiong,Pedro Chumpitaz-Flores,Kaixun Hua,Cheng Hua*

Main category: cs.LG

TL;DR: SPOT提出了一种基于MILP的可解释强化学习决策树策略，结合减小搜索空间的分支界定方法，在并行化搜索中实现显著的速度提升和可扩展性，同时保持可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 在高风险决策场景需要可解释的策略，同时现有方法在MDP中的决策树优化难以扩展，存在计算成本高、可扩展性差的问题。

Method: 将决策树策略的优化建模为MILP，并通过减少空间的分支界定法（decoupled dynamics vs tree约束）实现并行搜索，每次迭代保证全局最优，针对MDP动态与树结构约束分离以提升效率。

Result: 在标准基准上，SPOT显著加速，能处理更大状态数的MDP，得到可解释、紧凑的决策树策略，表现与现有方法相比速度至少提升一个数量级，具备更好的可扩展性。

Conclusion: 该方法实现了可解释性与可扩展性并存的高质量策略，展示了用MILP和分区并行搜索可在可解释策略学习中达到显著性能提升。

Abstract: Interpretable reinforcement learning policies are essential for high-stakes
decision-making, yet optimizing decision tree policies in Markov Decision
Processes (MDPs) remains challenging. We propose SPOT, a novel method for
computing decision tree policies, which formulates the optimization problem as
a mixed-integer linear program (MILP). To enhance efficiency, we employ a
reduced-space branch-and-bound approach that decouples the MDP dynamics from
tree-structure constraints, enabling efficient parallel search. This
significantly improves runtime and scalability compared to previous methods.
Our approach ensures that each iteration yields the optimal decision tree.
Experimental results on standard benchmarks demonstrate that SPOT achieves
substantial speedup and scales to larger MDPs with a significantly higher
number of states. The resulting decision tree policies are interpretable and
compact, maintaining transparency without compromising performance. These
results demonstrate that our approach simultaneously achieves interpretability
and scalability, delivering high-quality policies an order of magnitude faster
than existing approaches.

</details>


### [42] [Mixing Configurations for Downstream Prediction](https://arxiv.org/abs/2510.19248)
*Juntang Wang,Hao Wu,Runkun Guo,Yihan Wang,Dongmian Zou,Shixin Xu*

Main category: cs.LG

TL;DR: 提出 GraMixC，一种可插拔模块，用于从多分辨率聚类配置中提取并对齐这些结构，通过注意力融合提升下游任务的预测性能；在 DSN1 16S rRNA 预测任务中将 R2 从 0.6 提升至 0.9，并在标准表格数据集上优于单分辨率 baselines。


<details>
  <summary>Details</summary>
Motivation: 人类具备将对象按相似性分组的天生能力，而聚类算法的最新进展使得在多分辨率尺度上发现有效层次配置成为可能，且无需标注数据。本文正式刻画这些配置，并在 Vision Transformers 的寄存器 tokens 中发现相似的 emergent 结构。配置相比寄存器 tokens 具有更低冗余且无需 ad hoc 选择，且可通过无监督/self-supervised 学习，但其选择与组成仍受下游任务和输入影响。

Method: 提出 GraMixC：一个插件模块，负责从模型中提取配置，使用 Reverse Merge/Split (RMS) 进行对齐，再通过注意力头进行融合，最后将融合结果传给任意下游预测器。模块可作为下游任务的通用组件使用。

Result: 在 DSN1 16S rRNA 培养基预测任务上，GraMixC 将 R2 从 0.6 提升至 0.9，达到新的最优状态。并且在标准表格基准上，持续超越单分辨率与静态特征基线。

Conclusion: GraMixC 作为一种可插拔的配置提取/对齐与融合方案，展示了在无监督或自监督条件下对多分辨率层次结构的有效利用，并具有良好的跨任务泛化性。

Abstract: Humans possess an innate ability to group objects by similarity, a cognitive
mechanism that clustering algorithms aim to emulate. Recent advances in
community detection have enabled the discovery of configurations -- valid
hierarchical clusterings across multiple resolution scales -- without requiring
labeled data. In this paper, we formally characterize these configurations and
identify similar emergent structures in register tokens within Vision
Transformers. Unlike register tokens, configurations exhibit lower redundancy
and eliminate the need for ad hoc selection. They can be learned through
unsupervised or self-supervised methods, yet their selection or composition
remains specific to the downstream task and input. Building on these insights,
we introduce GraMixC, a plug-and-play module that extracts configurations,
aligns them using our Reverse Merge/Split (RMS) technique, and fuses them via
attention heads before forwarding them to any downstream predictor. On the DSN1
16S rRNA cultivation-media prediction task, GraMixC improves the R2 score from
0.6 to 0.9 across multiple methods, setting a new state of the art. We further
validate GraMixC on standard tabular benchmarks, where it consistently
outperforms single-resolution and static-feature baselines.

</details>


### [43] [FnRGNN: Distribution-aware Fairness in Graph Neural Network](https://arxiv.org/abs/2510.19257)
*Soyoung Park,Sungsu Lim*

Main category: cs.LG

TL;DR: 将GNN用于节点回归的公平性问题引入FnRGNN框架，通过三层次干预实现对齐与正则化：结构层边重加权、表示层MMD对齐、预测层通过Sinkhorn分布匹配实现均衡，达到降低群体差异同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注分类任务或表示级别的去偏，未充分解决节点级回归问题中的持续性偏差，且在复杂图拓扑上鲁棒性不足。因此需要一个在处理中就进行多层次公平性控制的GNN回归框架。

Method: 提出FnRGNN，在三个层次进行干预：(i) 结构层的边权重重分配以削弱敏感群体的传播影响；(ii) 表示层通过最大均值差异（MMD）实现跨群体表示的对齐；(iii) 预测层通过基于Sinkhorn的分布匹配对预测输出进行归一化与正则化。综合优化目标实现多层次公平性约束与回归性能保留。

Result: 在四个真实数据集上实验，FnRGNN显著降低群体差异，同时不牺牲回归性能，证实方法在复杂图结构下的鲁棒性与有效性；并给出开源代码.

Conclusion: 多层次干预的GNN回归公平性框架能有效提升群体公平性，同时维持或提升预测性能，适用于需要节点级回归且拓扑结构复杂的场景，未来工作可拓展至其他任务或进一步优化各层的权衡机制。

Abstract: Graph Neural Networks (GNNs) excel at learning from structured data, yet
fairness in regression tasks remains underexplored. Existing approaches mainly
target classification and representation-level debiasing, which cannot fully
address the continuous nature of node-level regression. We propose FnRGNN, a
fairness-aware in-processing framework for GNN-based node regression that
applies interventions at three levels: (i) structure-level edge reweighting,
(ii) representation-level alignment via MMD, and (iii) prediction-level
normalization through Sinkhorn-based distribution matching. This multi-level
strategy ensures robust fairness under complex graph topologies. Experiments on
four real-world datasets demonstrate that FnRGNN reduces group disparities
without sacrificing performance. Code is available at
https://github.com/sybeam27/FnRGNN.

</details>


### [44] [Knowledge Distillation of Uncertainty using Deep Latent Factor Model](https://arxiv.org/abs/2510.19290)
*Sehyun Park,Jongjin Lee,Yunseop Shin,Ilsang Ohn,Yongdai Kim*

Main category: cs.LG

TL;DR: 提出高斯蒸馏（Gaussian distillation），通过深潜在因子模型（DLF）将教师集合的分布蒸馏为一个单一的学生分布，并用EM估计均值与协方差。与现有基线相比，在多项基准数据集上表现更优，且适用于语言模型微调和分布变化场景，在不显著牺牲不确定性估计的前提下实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 核心难点在于直接蒸馏成学生模型往往丧失不确定性信息，因为模型规模缩小导致方差下降；深度集成虽强但代价高，限制了在设备上的应用。

Method: 将教师集成的输出分布看作由一个深潜在因子模型（DLF）生成的高斯过程，通过对教师成员的观测实现对 DLf 的均值和协方差函数的 EM 估计，从而得到一个学生分布；使用分布蒸馏替代传统的学生教师蒸馏。

Result: 在多个基准数据集上证实优于现有基线；对语言模型微调和分布偏移问题也有良好表现。

Conclusion: 通过对教师集合的分布建模并蒸馏为单一学生分布，Gaussian distillation 能在保留不确定性的同时显著减少模型体积，便于在设备端部署，同时具备对语言模型和分布变化鲁棒性。

Abstract: Deep ensembles deliver state-of-the-art, reliable uncertainty quantification,
but their heavy computational and memory requirements hinder their practical
deployments to real applications such as on-device AI. Knowledge distillation
compresses an ensemble into small student models, but existing techniques
struggle to preserve uncertainty partly because reducing the size of DNNs
typically results in variation reduction. To resolve this limitation, we
introduce a new method of distribution distillation (i.e. compressing a teacher
ensemble into a student distribution instead of a student ensemble) called
Gaussian distillation, which estimates the distribution of a teacher ensemble
through a special Gaussian process called the deep latent factor model (DLF) by
treating each member of the teacher ensemble as a realization of a certain
stochastic process. The mean and covariance functions in the DLF model are
estimated stably by using the expectation-maximization (EM) algorithm. By using
multiple benchmark datasets, we demonstrate that the proposed Gaussian
distillation outperforms existing baselines. In addition, we illustrate that
Gaussian distillation works well for fine-tuning of language models and
distribution shift problems.

</details>


### [45] [QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation](https://arxiv.org/abs/2510.19296)
*Yang Zhang,Rui Zhang,Jiaming Guo,Lei Huang,Di Huang,Yunpu Zhao,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 提出 QiMeng-SALV，通过信号级别的正确性来优化 RL 在 Verilog 代码生成中的奖励信号，利用 AST 找到部分正确模块中的信号相关实现，进行信号感知的 DPO，以实现微观的功能导向优化，达到对 VerilogEval/RTLLM 的 state-of-the-art。


<details>
  <summary>Details</summary>
Motivation: LLMs 在 Verilog 代码生成中缺乏有意义的功能性奖励，导致强化学习偏好优化效果受限。希望通过信号层面的细粒度奖励提升正确性和鲁棒性。

Method: 用参考模块对生成模块的信号功能正确性进行对比，借助抽象语法树定位在部分错误模块中也可提供有意义奖励的信号相关代码段；提出信号感知的 DPO，在这些信号级正确片段上进行优化，抑制错误信号的噪声。

Result: 在 VerilogEval 与 RTLLM 基准上实现了最先进性能，7B 模型达到接近 DeepSeek v3 671B 的水平，并显著优于同数据集训练的开源模型 CodeV。

Conclusion: 将优化焦点从模块级转向信号级的微观优化，解决功能奖励不足的问题，推动 Verilog 代码生成的 RL 训练进入信号感知的新范式。

Abstract: The remarkable progress of Large Language Models (LLMs) presents promising
opportunities for Verilog code generation which is significantly important for
automated circuit design. The lacking of meaningful functional rewards hinders
the preference optimization based on Reinforcement Learning (RL) for producing
functionally correct Verilog code. In this paper, we propose Signal-Aware
Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments
of functionally correct output signal to optimize RL training. Considering
Verilog code specifies the structural interconnection of hardware gates and
wires so that different output signals are independent, the key insight of
QiMeng-SALV is to extract verified signal-aware implementations in partially
incorrect modules, so as to enhance the extraction of meaningful functional
rewards. Roughly, we verify the functional correctness of signals in generated
module by comparing with that of reference module in the training data. Then
abstract syntax tree (AST) is employed to identify signal-aware code segments
which can provide meaningful functional rewards from erroneous modules.
Finally, we introduce signal-aware DPO which is optimized on the correct
signal-level code segments, thereby preventing noise and interference from
incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from
conventional module-level to fine-grained signal-level optimization in Verilog
code generation, addressing the issue of insufficient functional rewards.
Experiments demonstrate that our method achieves state-of-the-art performance
on VerilogEval and RTLLM, with a 7B parameter model matching the performance of
the DeepSeek v3 671B model and significantly outperforming the leading
open-source model CodeV trained on the same dataset. Our code is available at
https://github.com/zy1xxx/SALV.

</details>


### [46] [Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall](https://arxiv.org/abs/2510.19304)
*Mingyu Jo,Jaesik Yoon,Justin Deschenaux,Caglar Gulcehre,Sungjin Ahn*

Main category: cs.LG

TL;DR: 提出了一种名为Loopholing的离散扩散模型，利用确定性潜在通道在采样阶段保留信息，解决离散采样导致信息断裂的问题。通过自条件化训练，LDDMs显著提升生成性能，缩小与自回归模型的差距，在推理任务（如 Countdown、Game of 24）上也有提升。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在并行解码中存在“采样屏障”问题，即一旦进行类别采样，信息难以跨步传播，导致后续步骤信息受限，影响文本连贯性与推理能力。

Method: 引入Loopholing机制，通过一个确定性潜在路径保留采样过程中的丰富分布信息，使多步之间能共享信息；采用自条件化训练策略提升学习效果。

Result: 在生成困惑度（perplexity）方面相对基线提升显著，最高可下降约61%，缩小甚至超过与自回归模型的差距，文本更连贯；在推理任务的算术基准（Countdown、Game of 24）上也表现出更好性能；还显示Loopholing可缓解闲置步数与振荡，具备可扩展性用于高质量非自回归文本生成。

Conclusion: Loopholing为离散扩散模型提供了一条可扩展的路径，通过确定性潜在通道与自条件化训练实现信息在多步中的有效传播，显著提升非自回归文本生成质量并提升推理任务表现。

Abstract: Discrete diffusion models offer a promising alternative to autoregressive
generation through parallel decoding, but they suffer from a sampling wall:
once categorical sampling occurs, rich distributional information collapses
into one-hot vectors and cannot be propagated across steps, forcing subsequent
steps to operate with limited information. To mitigate this problem, we
introduce Loopholing, a novel and simple mechanism that preserves this
information via a deterministic latent pathway, leading to Loopholing Discrete
Diffusion Models (LDDMs). Trained efficiently with a self-conditioning
strategy, LDDMs achieve substantial gains-reducing generative perplexity by up
to 61% over prior baselines, closing (and in some cases surpassing) the gap
with autoregressive models, and producing more coherent text. Applied to
reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such
as Countdown and Game of 24. These results also indicate that loopholing
mitigates idle steps and oscillations, providing a scalable path toward
high-quality non-autoregressive text generation.

</details>


### [47] [FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation](https://arxiv.org/abs/2510.19305)
*Chirag Padubidri,Pranesh Velmurugan,Andreas Lanitis,Andreas Kamilaris*

Main category: cs.LG

TL;DR: 多模态深度学习结合数据平衡与缺失值插补，显著提升蛙类分布预测在EY-2022挑战数据上的表现：MAE由189降至29；多模态融合（地表覆盖、NDVI等）在未见地区具备良好泛化， frog counting准确度84.9%、AUC 0.90，显示在数据稀缺情境下的预测能力。


<details>
  <summary>Details</summary>
Motivation: 保护性生物多样性需要大尺度且高准确性的物种分布预测；传统采集覆盖有限，民众科学数据虽有价值却常不完整。通过深度学习、数据插补和数据平衡等方法，提升SDM在稀疏数据条件下的预测性能，扩大监测的时空覆盖。

Method: 利用深度学习和数据插补来增强SDM，结合数据平衡、特征选择与多模态融合。以EY-2022 Biodiversity Challenge数据为来源，将图像（如土地覆盖/遥感）与表格环境变量统一输入，构建多模态集成模型；进行特征选择以筛选关键环境因素，评估在未见区域的泛化能力。

Result: 数据平衡将MAE从189降低到29；特征选择识别出影响物种出现的关键环境因素；多模态集成模型优于单模态，具备对未见区域的稳健推广；图像与表格数据的融合提升蛙类计数与栖息地分类的性能，准确率达84.9%，AUC为0.90。

Conclusion: 证明在数据稀缺或不完整情形下，多模态学习与数据预处理（平衡、插补）能显著提升预测生态建模的准确性与可扩展性，有助于实现更精准且可扩展的生物多样性监测。

Abstract: Monitoring species distribution is vital for conservation efforts, enabling
the assessment of environmental impacts and the development of effective
preservation strategies. Traditional data collection methods, including citizen
science, offer valuable insights but remain limited in coverage and
completeness. Species Distribution Modelling (SDM) helps address these gaps by
using occurrence data and environmental variables to predict species presence
across large regions. In this study, we enhance SDM accuracy for frogs (Anura)
by applying deep learning and data imputation techniques using data from the
"EY - 2022 Biodiversity Challenge." Our experiments show that data balancing
significantly improved model performance, reducing the Mean Absolute Error
(MAE) from 189 to 29 in frog counting tasks. Feature selection identified key
environmental factors influencing occurrence, optimizing inputs while
maintaining predictive accuracy. The multimodal ensemble model, integrating
land cover, NDVI, and other environmental inputs, outperformed individual
models and showed robust generalization across unseen regions. The fusion of
image and tabular data improved both frog counting and habitat classification,
achieving 84.9% accuracy with an AUC of 0.90. This study highlights the
potential of multimodal learning and data preprocessing techniques such as
balancing and imputation to improve predictive ecological modeling when data
are sparse or incomplete, contributing to more precise and scalable
biodiversity monitoring.

</details>


### [48] [Calibration and Discrimination Optimization Using Clusters of Learned Representation](https://arxiv.org/abs/2510.19328)
*Tomer Lavi,Bracha Shapira,Nadav Rappoport*

Main category: cs.LG

TL;DR: 提出一个基于对学习表示聚类的校准函数集成管线，显著提升模型校准并实现辨别-校准的联合优化。


<details>
  <summary>Details</summary>
Motivation: 在高风险决策中，校准的可靠性与辨别一样重要，但往往被忽视；因此需要一个通用、可扩展的校准框架。

Method: 通过对输入样本的学习表示进行聚类，训练一个校准函数的集合；引入一种匹配度量来选取在辨别与校准之间实现良好折中和优化的模型；该方案对底层表示、聚类方法、校准方法和评测指标均具备通用性。

Result: 在多种校准方法上，校准分数从82.28%提升至100%；同时引入的匹配度量确保模型选择在辨别与校准之间实现更优权衡，整体表现优于常用方法。

Conclusion: 该通用校准管线提供了一个灵活、可扩展的框架，能够显著提升校准性能并实现辨别与校准的协同优化，适用于不同表示与校准技术。

Abstract: Machine learning models are essential for decision-making and risk
assessment, requiring highly reliable predictions in terms of both
discrimination and calibration. While calibration often receives less
attention, it is crucial for critical decisions, such as those in clinical
predictions. We introduce a novel calibration pipeline that leverages an
ensemble of calibration functions trained on clusters of learned
representations of the input samples to enhance overall calibration. This
approach not only improves the calibration score of various methods from 82.28%
up to 100% but also introduces a unique matching metric that ensures model
selection optimizes both discrimination and calibration. Our generic scheme
adapts to any underlying representation, clustering, calibration methods and
metric, offering flexibility and superior performance across commonly used
calibration methods.

</details>


### [49] [Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning](https://arxiv.org/abs/2510.19338)
*Ling Team,Bin Han,Caizhi Tang,Chen Liang,Donghao Zhang,Fan Yuan,Feng Zhu,Jie Gao,Jingyu Hu,Longfei Li,Meng Li,Mingyang Zhang,Peijie Jiang,Peng Jiao,Qian Zhao,Qingyuan Yang,Wenbo Shen,Xinxing Yang,Yalin Zhang,Yankun Ren,Yao Zhao,Yibo Cao,Yixuan Sun,Yue Zhang,Yuchen Fang,Zibin Lin,Zixuan Cheng,Jun Zhou*

Main category: cs.LG

TL;DR: Ring-linear系列引入Ring-mini-linear-2.0和Ring-flash-linear-2.0，通过混合注意力机制显著降低长上下文推理的I/O和计算开销，达成与32B密集模型相比1/10成本，与原Ring系列相比超过50%成本降低。依托自研FP8算子库linghe与训练-推理引擎对齐，在强化学习阶段实现稳定高效优化，达到多项复杂推理基准的SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文推理中的计算与I/O瓶颈，提升训练和推理效率，同时在强化学习阶段保持或提升SOTA性能。

Method: 在混合架构中集成线性注意力与softmax注意力，并系统地探索两种注意力比重以找出当前最优结构。利用自研FP8算子库linghe提升训练效率，并确保训练与推理引擎算子高度对齐以实现长期稳定优化，特别是在强化学习阶段。

Result: Ring-mini-linear-2.0为16B参数、957M激活；Ring-flash-linear-2.0为104B参数、6.1B激活。与32B密集模型相比推理成本降至1/10；与原Ring系列相比成本降低>50%。在对两种注意力比例的系统性探索后，确定了当前最优模型结构。通过linghe FP8算子库，整体训练效率提高约50%。

Conclusion: Ring-linear系列在长上下文推理任务中实现高效、可扩展的推理-训练流程，训练与推理引擎高度对齐使强化学习阶段能够稳定高效优化，并在多项复杂推理基准上保持SOTA水平。

Abstract: In this technical report, we present the Ring-linear model series,
specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.
Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while
Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both
models adopt a hybrid architecture that effectively integrates linear attention
and softmax attention, significantly reducing I/O and computational overhead in
long-context inference scenarios. Compared to a 32 billion parameter dense
model, this series reduces inference cost to 1/10, and compared to the original
Ring series, the cost is also reduced by over 50%. Furthermore, through
systematic exploration of the ratio between different attention mechanisms in
the hybrid architecture, we have identified the currently optimal model
structure. Additionally, by leveraging our self-developed high-performance FP8
operator library-linghe, overall training efficiency has been improved by 50%.
Benefiting from the high alignment between the training and inference engine
operators, the models can undergo long-term, stable, and highly efficient
optimization during the reinforcement learning phase, consistently maintaining
SOTA performance across multiple challenging complex reasoning benchmarks.

</details>


### [50] [Foundation Model Forecasts: Form and Function](https://arxiv.org/abs/2510.19345)
*Alvaro Perez-Diaz,James C. Loach,Danielle E. Toutoungi,Lee Middleton*

Main category: cs.LG

TL;DR: 本文系统分析时间序列基础模型的预测形式对实际任务的影响，发现仅追求预测准确度不足以保证实用性；并给出在不同预测形式之间的相互转换条件、以及将六类任务映射到最小充足的预测形式和评估框架。


<details>
  <summary>Details</summary>
Motivation: 解释为何在实际应用中，预测形式比单纯的预测准确率更决定性，强调需要将预测输出的形式与具体运营任务对齐。

Method: 通过对最新TSFM的综述，建立关于预测形式之间转换的理论分析（包含边际化、联合分布、Copulas、 conformal 等方法），证明边际不能唯一决定路径相关事件概率；并将六个基本预测任务映射到最小充足的预测类型，提出任务对齐的评估框架。

Result: 发现约三分之二的模型仅产生点或参数化预测；许多任务需要保持时间相关性的轨迹序列；轨迹序列可以通过边际化转化为简单形式，但逆转需要引入时间依赖性；边际信息无法确定路径相关事件概率；提出六类任务的最小充足预测类型及其评估框架。

Conclusion: 预测形式决定实用性而非单纯的预测准确度；未来研究应聚焦于针对具体运营任务选择合适的预测形式，并建立以任务为导向的评估标准。

Abstract: Time-series foundation models (TSFMs) achieve strong forecast accuracy, yet
accuracy alone does not determine practical value. The form of a forecast --
point, quantile, parametric, or trajectory ensemble -- fundamentally constrains
which operational tasks it can support. We survey recent TSFMs and find that
two-thirds produce only point or parametric forecasts, while many operational
tasks require trajectory ensembles that preserve temporal dependence. We
establish when forecast types can be converted and when they cannot: trajectory
ensembles convert to simpler forms via marginalization without additional
assumptions, but the reverse requires imposing temporal dependence through
copulas or conformal methods. We prove that marginals cannot determine
path-dependent event probabilities -- infinitely many joint distributions share
identical marginals but yield different answers to operational questions. We
map six fundamental forecasting tasks to minimal sufficient forecast types and
provide a task-aligned evaluation framework. Our analysis clarifies when
forecast type, not accuracy, differentiates practical utility.

</details>


### [51] [Scalable LinUCB: Low-Rank Design Matrix Updates for Recommenders with Large Action Spaces](https://arxiv.org/abs/2510.19349)
*Evgenia Shustova,Marina Sheshukova,Sergey Samsonov,Evgeny Frolov*

Main category: cs.LG

TL;DR: 提出 Scalable LinUCB，通过对逆正则化设计矩阵的低秩动力学参数化，实现对设计矩阵的高效更新、低内存占用和可扩展推理。复杂度为每步 O(d r)，内存 O(d r)，避免直接构造或逆转全矩阵，在高维特征和大动作空间情境下具有实际可行性。


<details>
  <summary>Details</summary>
Motivation: 线性上下文 bandit 的 LinUCB 在特征维度和动作空间增大时，训练、推断和记忆成本急剧上升，核心瓶颈在于需要更新、求逆并存储吸收历史信息的设计矩阵。需一种可扩展且内存友好的更新机制来支撑实时推荐场景。

Method: 提出逆正则化设计矩阵的动态低秩参数化（类似 Cholesky 风格的分解）并设计数值稳定的单位秩(rank-1)与批量更新，以在不直接形成全矩阵的前提下保持逆矩阵。通过投影分裂积分器实现动力学低秩近似以控制记忆增长，得到平均每步更新成本为 O(d r)、近似秩为 r 时的内存为 O(d r)。推理复杂度为每个动作评估 O(d r)。实验验证在推荐系统数据集上的有效性。

Result: 实验证明该算法在高维特征和大动作空间场景中具有良好的可扩展性与记忆效率，同时保持与传统 LinUCB 相近的推荐性能，显著降低了更新和推理的计算成本。

Conclusion: Scalable LinUCB 为线性上下文 bandits 提供了一种高效的记忆/时间复杂度解决方案，通过动态低秩逆设计矩阵和投影分裂积分实现稳定的低秩更新，适用于大规模推荐系统等应用，未来可在自适应秩选择和进一步的数值稳定性方面拓展。

Abstract: Linear contextual bandits, especially LinUCB, are widely used in recommender
systems. However, its training, inference, and memory costs grow with feature
dimensionality and the size of the action space. The key bottleneck becomes the
need to update, invert and store a design matrix that absorbs contextual
information from interaction history. In this paper, we introduce Scalable
LinUCB, the algorithm that enables fast and memory efficient operations with
the inverse regularized design matrix. We achieve this through a dynamical
low-rank parametrization of its inverse Cholesky-style factors. We derive
numerically stable rank-1 and batched updates that maintain the inverse without
directly forming the entire matrix. To control memory growth, we employ a
projector-splitting integrator for dynamical low-rank approximation, yielding
average per-step update cost $O(dr)$ and memory $O(dr)$ for approximation rank
$r$. Inference complexity of the suggested algorithm is $O(dr)$ per action
evaluation. Experiments on recommender system datasets demonstrate the
effectiveness of our algorithm.

</details>


### [52] [Optimization Benchmark for Diffusion Models on Dynamical Systems](https://arxiv.org/abs/2510.19376)
*Fabian Schaipp*

Main category: cs.LG

TL;DR: 本研究在扩散模型训练中对最新优化算法进行基准评估，发现 Muon 和 SOAP 相对 AdamW 更高效，能显著降低最终损失；并讨论学习率策略和 Adam 与 SGD 的性能差异在扩散模型训练中的体现。


<details>
  <summary>Details</summary>
Motivation: 阐明在扩散模型训练场景下对优化算法的真实效果，以揭示常见在文本/图像任务中观察到的现象是否同样适用于扩散建模。

Method: 在一个用于去噪流轨迹的扩散模型训练任务中，系统比较 Muon、SOAP、AdamW 等优化器的训练效率与最终损失，并分析学习率调度、Adam 与 SGD 的性能差距等因素。

Result: Muon 与 SOAP 相较于 AdamW 提高训练效率、最终损失降低约 18%；学习率调度影响训练动态；Adam 与 SGD 在这个任务中的表现差异显著，提出对扩散模型训练的一些可复现的观察。

Conclusion: 为扩散模型训练领域提供可重复的优化基准，提示在该领域应关注替代优化器及学习率策略，且某些在文本/图像任务中的现象在扩散模型训练中也具有一定的适用性。

Abstract: The training of diffusion models is often absent in the evaluation of new
optimization techniques. In this work, we benchmark recent optimization
algorithms for training a diffusion model for denoising flow trajectories. We
observe that Muon and SOAP are highly efficient alternatives to AdamW (18%
lower final loss). We also revisit several recent phenomena related to the
training of models for text or image applications in the context of diffusion
model training. This includes the impact of the learning-rate schedule on the
training dynamics, and the performance gap between Adam and SGD.

</details>


### [53] [CPSVD: Enhancing Large Language Model Compression via Column-Preserving Singular Value Decomposition](https://arxiv.org/abs/2510.19385)
*Lin Xv,Jingsheng Gao,Xian Gao,Ting Li,Yuzhuo Fu*

Main category: cs.LG

TL;DR: CPSVD 提出了一种列保留的 SVD 压缩方法，通过区分列的重构误差，针对高误差列直接保留、低误差列采用 SVD，并在层内自适应分配压缩率，显著提升压缩性能。


<details>
  <summary>Details</summary>
Motivation: LLM 压缩需要比传统 SVD 更细粒度的处理，因为参数矩阵的重构误差在不同部分差异很大，统一处理往往导致次优结果。

Method: 识别出具有高分解误差的列并直接保留；对低误差列应用 SVD；确定两者之间的最优平衡点以最小化误差；在同一层内对不同模块自适应分配非均匀的压缩率，同时满足目标层级压缩比。

Result: 大量实验证明 CPSVD 在 SVD 基压缩方法中性能领先，表现为更低的困惑度和更高的零-shot 任务准确性。

Conclusion: CPSVD 通过误差感知的列保留和自适应模块级别压缩，提供比以往方法更有效的 LLM 压缩方案，能够在压缩约束下实现更佳性能。

Abstract: The rapid advancement of Large Language Models (LLMs) faces a critical
bottleneck in their immense size, necessitating efficient compression
techniques. While Singular Value Decomposition (SVD) is a promising approach,
existing SVD-based methods treat the entire parameter matrix uniformly,
overlooking that SVD approximation errors vary significantly across different
matrix parts, which often leads to suboptimal compression. To address this, we
propose \textbf{C}olumn-\textbf{P}reserving \textbf{S}ingular \textbf{V}alue
\textbf{D}ecomposition (CPSVD), a novel method that refines SVD-based LLM
compression by intelligently segmenting the parameter matrix. Unlike
traditional SVD, CPSVD identifies and directly preserves matrix columns with
high decomposition errors, applying SVD only to columns with low decomposition
errors, while precisely determining the optimal balance point between these two
strategies to minimize error. Furthermore, leveraging the inherent
heterogeneity in decomposition errors across different matrices within an LLM,
CPSVD adaptively allocates non-uniform compression rates to modules within that
layer, while adhering to a target layer-wise compression ratio, thereby further
enhancing compression performance. Extensive experiments demonstrate that CPSVD
consistently outperforms state-of-the-art SVD-based LLM compression methods,
achieving lower perplexity and higher accuracy on zero-shot tasks.

</details>


### [54] [ARA: Adaptive Rank Allocation for Efficient Large Language Model SVD Compression](https://arxiv.org/abs/2510.19389)
*Lin Xv,Jingsheng Gao,Xian Gao,Ting Liu,Yuzhuo Fu*

Main category: cs.LG

TL;DR: Adaptive Rank Allocation (ARA) for SVD-based LLM compression that jointly learns which singular values to retain via a dedicated mask and an extra loss term, achieving state-of-the-art results under high compression.


<details>
  <summary>Details</summary>
Motivation: SVD can only be applied to linear modules; under a global compression constraint, choosing ranks per module is critical. Existing heuristics and mask-based methods have limited exploration, poor relation to trainable parameters, and ignore non-smooth gain at compression ratio 1, leading to suboptimal minima.

Method: ARA introduces a specialized mask design to map and update retained ranks to trainable parameters, and adds an auxiliary loss to steer optimization toward globally optimal solutions, enabling adaptive rank allocation across modules.

Result: On LLaMA2-7B with 80% compression, ARA reduces WikiText2 perplexity from 8.38 to 6.42 and improves average zero-shot task accuracy by 9.72 percentage points versus uniform compression, indicating state-of-the-art performance for rank allocation in SVD-based LLM compression.

Conclusion: ARA provides effective adaptive rank allocation for SVD-based LLM compression, achieving significant performance gains, and offering a viable approach to balance compression ratio with model quality.

Abstract: In the field of large language model (LLM) compression, singular value
decomposition (SVD) is a widely studied and adopted low-rank decomposition
technique. Since SVD operates exclusively on linear modules, and these modules
in LLMs are separated by nonlinear components, SVD can only be applied
independently to each linear module. Under a global compression ratio
constraint, determining the appropriate rank for different linear modules
becomes a critical problem. Existing approaches, such as heuristic algorithms
and mask-based training, have made progress in addressing this challenge.
However, these methods still suffer from several limitations: heuristic
algorithms explore the solution space within restricted regions, while
mask-based training struggles to efficiently capture the relationship between
singular value spectra and trainable parameters. More importantly, current
methods overlook the key property that the gain function is non-smooth at a
compression ratio of 1, which often leads the training process to suboptimal
local minima. To address these issues, we propose an Adaptive Rank Allocation
(ARA) method. Specifically, (1) ARA introduces a dedicated mask design that
enables efficient mapping and updating between retained ranks and trainable
parameters; and (2) it employs an additional loss function to guide parameter
selection toward globally optimal solutions. Experimental results demonstrate
that ARA achieves state-of-the-art performance. On the LLaMA2-7B model with a
80\% compression ratio, ARA reduces perplexity on WikiText2 from 8.38 to 6.42
and improves average zero-shot task accuracy by 9.72 percentage points compared
with uniform compression. These results highlight the effectiveness of our
method for rank allocation in SVD-based LLM compression.

</details>


### [55] [FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA](https://arxiv.org/abs/2510.19421)
*Songqi Zhou,Zeyuan Liu,Benben Jiang*

Main category: cs.LG

TL;DR: 提出 FairNet，通过 bias detector + LoRA 实现动态、实例级公平纠正，针对有偏样本激活纠正，配合对比学习损失缓解 minority underfitting，支持有标签、部分标签、无标签场景，理论与经验均显示改进最差群体表现且不损害整体性能。


<details>
  <summary>Details</summary>
Motivation: 在当前偏差矫正方法中，常牺牲性能、依赖静态策略、在数据稀缺与少数群体时表现不佳；需要在全局与个体之间平衡，同时充分利用敏感属性信息（无论是否完整标注）来提升公平性。

Method: 将偏置检测器与条件低秩适应 LoRA 结合，仅对被判定有偏的实例激活公平纠正；提出对比学习损失以最小化同一类别内不同敏感组的表征差异，缓解少数群体欠拟合；框架可处理完全/部分/缺失敏感标签场景。

Result: 理论分析表明在中等TPR/FPR下，FairNet能提升最差群体表现且不降低总体表现，甚至略有提升；实验在视觉与语言任务上广泛验证有效性。

Conclusion: FairNet提供一种灵活、实例级的公平纠正框架，结合偏置检测器和 LoRA，若标签充足/部分/缺失均可，兼顾公平与性能且对少数群体有显著提升潜力。

Abstract: Ensuring fairness in machine learning models is a critical challenge.
Existing debiasing methods often compromise performance, rely on static
correction strategies, and struggle with data sparsity, particularly within
minority groups. Furthermore, their utilization of sensitive attributes is
often suboptimal, either depending excessively on complete attribute labeling
or disregarding these attributes entirely. To overcome these limitations, we
propose FairNet, a novel framework for dynamic, instance-level fairness
correction. FairNet integrates a bias detector with conditional low-rank
adaptation (LoRA), which enables selective activation of the fairness
correction mechanism exclusively for instances identified as biased, and
thereby preserve performance on unbiased instances. A key contribution is a new
contrastive loss function for training the LoRA module, specifically designed
to minimize intra-class representation disparities across different sensitive
groups and effectively address underfitting in minority groups. The FairNet
framework can flexibly handle scenarios with complete, partial, or entirely
absent sensitive attribute labels. Theoretical analysis confirms that, under
moderate TPR/FPR for the bias detector, FairNet can enhance the performance of
the worst group without diminishing overall model performance, and potentially
yield slight performance improvements. Comprehensive empirical evaluations
across diverse vision and language benchmarks validate the effectiveness of
FairNet.

</details>


### [56] [LLM Unlearning with LLM Beliefs](https://arxiv.org/abs/2510.19422)
*Kemou Li,Qizhou Wang,Yue Wang,Fengpeng Li,Jun Liu,Bo Han,Jiantao Zhou*

Main category: cs.LG

TL;DR: BS框架通过联合抑制目标输出与模型信念来对抗“挤压效应”，实现更彻底的遗忘并尽量保留实用性；BS-T（逐词）与BS-S（序列）共同作用。


<details>
  <summary>Details</summary>
Motivation: 现有的遗忘方法多依赖梯度上升，易造成概率质量重新分配到目标相关的高概率重述，产生挤压效应；评估指标易误导，亟需利用模型自身的高置信区域来引导遗忘。

Method: 提出Bootstrapping（BS）框架，将模型信念（高概率区域）与挤压效应相联系；在遗忘目标上同时抑制目标输出和模型信念。BS-T在逐词层面抑制高概率标记，BS-S在序列层面抑制整段高置信输出；两者结合实现更彻底的遗忘，同时尽量保持模型效用。

Result: 在多种基准和不同模型家族上进行广泛实验证明，该方法显著优于基线，在减少记忆泄露方面更有效，且对下游任务绩效的影响更小。

Conclusion: BS框架为解决挤压效应提供了一种系统化方法，通过直接对齐模型信念来提升遗忘的彻底性与鲁棒性，具备跨模型的泛化性与潜在的应用价值。

Abstract: Large language models trained on vast corpora inherently risk memorizing
sensitive or harmful content, which may later resurface in their outputs.
Prevailing unlearning methods generally rely on gradient ascent and its
variants to lower the probability of specific target responses. However, we
find that this strategy induces a critical side effect: probability mass is
redistributed into high-likelihood regions, often corresponding to semantically
related rephrasings of the targets. We refer to this as the squeezing effect,
which explains why many methods yield merely spurious unlearning, a problem
further obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport
actual success. To address this, we propose a bootstrapping (BS) framework that
explicitly links the squeezing effect with the model's own high-confidence
generations, namely its model beliefs. Since model beliefs inherently capture
the very high-likelihood regions where probability mass is squeezed,
incorporating them into the unlearning objective directly counters the
squeezing effect. By jointly suppressing both target responses and model
beliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S
(sequence) removes entire high-confidence generations, together achieving more
thorough forgetting while preserving utility. Extensive experiments across
diverse benchmarks with various model families confirm the effectiveness of our
approach.

</details>


### [57] [Neural Variational Dropout Processes](https://arxiv.org/abs/2510.19425)
*Insu Jeon,Youngjin Park,Gunhee Kim*

Main category: cs.LG

TL;DR: 提出 NVDPs，一种基于贝叶斯元学习的任务特定 dropout 方法，通过低秩的 Bernoulli 专家模型实现 dropout 率的高效映射，并在全任务数据条件下的先验引导下进行变分推断，从而实现对新任务的快速而鲁棒的多任务小样本重配置，实验显示在1D回归、图像修复和分类等任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 解决元学习中需要推断条件后验分布的挑战，提升对任务间不确定性和模糊性的鲁棒性，同时实现内存高效映射以便快速适应新任务。

Method: NVDPs 使用任务特定 dropout 来建模条件后验；通过一个低秩的 Bernoulli 专家组合实现 dropout 率的高效映射；提出以整个任务数据为条件的先验，在变分推断中对条件 dropout 后验进行优化。

Result: 与其他元学习方法在少样本任务（1D 回归、图像修复、分类）上的对比实验显示 NVDPs 具有优秀性能；对任务特定 dropout 率的近似鲁棒且能处理广泛的函数不确定性。

Conclusion: NVDPs 能够鲁棒地近似任务特定 dropout 率，适用于多任务少样本学习中对广泛不确定性的处理，且具有内存高效的映射和快速的任务重新配置能力。

Abstract: Learning to infer the conditional posterior model is a key step for robust
meta-learning. This paper presents a new Bayesian meta-learning approach called
Neural Variational Dropout Processes (NVDPs). NVDPs model the conditional
posterior distribution based on a task-specific dropout; a low-rank product of
Bernoulli experts meta-model is utilized for a memory-efficient mapping of
dropout rates from a few observed contexts. It allows for a quick
reconfiguration of a globally learned and shared neural network for new tasks
in multi-task few-shot learning. In addition, NVDPs utilize a novel prior
conditioned on the whole task data to optimize the conditional \textit{dropout}
posterior in the amortized variational inference. Surprisingly, this enables
the robust approximation of task-specific dropout rates that can deal with a
wide range of functional ambiguities and uncertainties. We compared the
proposed method with other meta-learning approaches in the few-shot learning
tasks such as 1D stochastic regression, image inpainting, and classification.
The results show the excellent performance of NVDPs.

</details>


### [58] [g-DPO: Scalable Preference Optimization for Protein Language Models](https://arxiv.org/abs/2510.19474)
*Constance Ferragu,Jonathan D. Ziegler,Nicolas Deutschmann,Arthur Lindoulsi,Eli Bixby,Cradle ML Team*

Main category: cs.LG

TL;DR: g-DPO: a scalable enhancement to Direct Preference Optimization for protein language models that prunes training pairs via sequence-space clustering and amortizes computations via group-based approximations, achieving similar performance to standard DPO but 1.8–3.7x faster across three protein engineering tasks.


<details>
  <summary>Details</summary>
Motivation: Direct Preference Optimization (DPO) effectively aligns protein language models but scales poorly because the number of training pairs grows quadratically with the number of labeled sequences, making training times prohibitive for even modest datasets.

Method: Introduce g-DPO with (i) sequence-space clustering to prune redundant training pairs while preserving the training signal, and (ii) group-based approximations to amortize likelihood computations.

Result: Across three protein engineering tasks, g-DPO achieves performance in-silico and in-vitro statistically indistinguishable from standard DPO while converging 1.8 to 3.7 times faster; larger gains are expected as dataset size increases.

Conclusion: g-DPO provides a scalable and efficient framework for aligning protein language models, maintaining performance while significantly reducing training time, with the potential for greater speedups on larger datasets.

Abstract: Direct Preference Optimization (DPO) is an effective approach for aligning
protein language models with experimental design goals. However, DPO faces a
scalability bottleneck: the number of possible training pairs grows
quadratically with the number of labeled sequences, leading to prohibitive
training times even for modestly sized datasets. We introduce g-DPO, a
framework that (i) uses sequence space clustering to prune redundant pairs
while preserving training signal, and (ii) amortizes likelihood computations
with group-based approximations. Across three protein engineering tasks, g-DPO
maintains in-silico and in-vitro performance that is statistically
indistinguishable from standard DPO, while converging 1.8 to 3.7 times faster,
with greater gains expected as the size of the dataset increases.

</details>


### [59] [A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring](https://arxiv.org/abs/2510.19476)
*Julian Schulz*

Main category: cs.LG

TL;DR: 提出基于链路推断（CoT）监测的安全性证据体系，用于在推理模型达到危险能力水平时提供控制与可信度保障的两段式安全性框架，并将监测可验证性作为核心问题进行系统化分析，同时引入预测市场来评估实现可行性的关键里程碑。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力接近危险阈值，传统“无能力”安全性证据不足以覆盖潜在风险，亟需新的安全保障思路。CoT监测被视为在推理过程层面提升可控性与可信度的潜在途径。

Method: 提出一个两部分安全性证据：1) 证明在没有CoT时模型不具备危险能力；2) 证明任何因CoT引发的危险能力都能被CoT监测检测到。系统性地检视对监测性的两大威胁（神经语言学“neuralese”和编码推理），并将其分为语言漂移、隐写和外星推理三种形式及其驱动因素；评估现有与新颖的保持CoTfaithfulness的技术；探讨在不可监测推理的情况下提取可监测的CoT的可能性；并提出通过预测市场汇聚关键技术里程碑的预测，以评估CoT监测安全性证据的可行性。

Result: 提出一个概念性框架与研究路线图，明确了监测性威胁、应对策略及评估路径；提出了潜在的证据收集与市场化评估机制，但尚未给出实证结果，只在理论层面给出方案与研究议题。

Conclusion: 若能确保CoT监测的可监测性并有效识别/提取监测信号，CoT监测安全性证据可同时支撑控制性与可信性框架；其可行性取决于对神经语言与编码推理等威胁的缓解程度，以及对未来里程碑的预测性市场工具的有效性。

Abstract: As AI systems approach dangerous capability levels where inability safety
cases become insufficient, we need alternative approaches to ensure safety.
This paper presents a roadmap for constructing safety cases based on
chain-of-thought (CoT) monitoring in reasoning models and outlines our research
agenda. We argue that CoT monitoring might support both control and
trustworthiness safety cases. We propose a two-part safety case: (1)
establishing that models lack dangerous capabilities when operating without
their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT
are detectable by CoT monitoring. We systematically examine two threats to
monitorability: neuralese and encoded reasoning, which we categorize into three
forms (linguistic drift, steganography, and alien reasoning) and analyze their
potential drivers. We evaluate existing and novel techniques for maintaining
CoT faithfulness. For cases where models produce non-monitorable reasoning, we
explore the possibility of extracting a monitorable CoT from a non-monitorable
CoT. To assess the viability of CoT monitoring safety cases, we establish
prediction markets to aggregate forecasts on key technical milestones
influencing their feasibility.

</details>


### [60] [Graph Unlearning Meets Influence-aware Negative Preference Optimization](https://arxiv.org/abs/2510.19479)
*Qiang Chen,Zhongze Wu,Ang He,Xi Lin,Shuo Jiang,Shan You,Chang Xu,Yi Chen,Xiu Su*

Main category: cs.LG

TL;DR: INPO通过影响感知的负偏好优化在图删除中放慢发散、提升遗忘质量，同时在保持模型效用的前提下提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度上升的图删除方法在忘记集合上容易导致模型效用快速下降，急速发散。需要降低发散速度、缓解忘记集合与保留集合之间的强耦合，并提高对高影响边的删除鲁棒性。

Method: 提出INPO框架：分析NPO的发散速度较慢但仍有不足，理论上高影响边的遗忘能减小整体影响；设计影响感知信息传递函数以放大待删除边的影响，缓解拓扑耦合；用基于移除的办法快速估计边的影响；提出拓扑熵损失以从拓扑角度避免局部结构信息过度丢失；在五个真实数据集上进行广泛实验。

Result: 实验结果显示INPO在所有忘记质量指标上达到SOTA，同时保持模型的原始效用。代码公开。

Conclusion: INPO通过关注高影响边和拓扑熵约束，提升图删除过程的鲁棒性与遗忘质量，降低对模型效用的损害。

Abstract: Recent advancements in graph unlearning models have enhanced model utility by
preserving the node representation essentially invariant, while using gradient
ascent on the forget set to achieve unlearning. However, this approach causes a
drastic degradation in model utility during the unlearning process due to the
rapid divergence speed of gradient ascent. In this paper, we introduce
\textbf{INPO}, an \textbf{I}nfluence-aware \textbf{N}egative
\textbf{P}reference \textbf{O}ptimization framework that focuses on slowing the
divergence speed and improving the robustness of the model utility to the
unlearning process. Specifically, we first analyze that NPO has slower
divergence speed and theoretically propose that unlearning high-influence edges
can reduce impact of unlearning. We design an influence-aware message function
to amplify the influence of unlearned edges and mitigate the tight topological
coupling between the forget set and the retain set. The influence of each edge
is quickly estimated by a removal-based method. Additionally, we propose a
topological entropy loss from the perspective of topology to avoid excessive
information loss in the local structure during unlearning. Extensive
experiments conducted on five real-world datasets demonstrate that INPO-based
model achieves state-of-the-art performance on all forget quality metrics while
maintaining the model's utility. Codes are available at
\href{https://github.com/sh-qiangchen/INPO}{https://github.com/sh-qiangchen/INPO}.

</details>


### [61] [ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices](https://arxiv.org/abs/2510.19482)
*Xin Nie,Liang Dong,HaiCheng Zhang,JiaWang Xiao,G. Sun*

Main category: cs.LG

TL;DR: ELUTQ提出一种层次线性量化HLQ的高效量化框架，用于在CPU边缘设备上对LLMs进行低比特量化以降低内存和延迟，同时消除去量化开销。该方法与现有量化算法正交，可无缝融入现有量化流水线，并提供优化的CPU推理内核。实验显示，在LLaMA3-8B上，HLQ在3比特和2比特下的后量化（PTQ）分别实现约8%与85%的困惑度改善；在2比特下可在一小时内完成量化，若进行高效微调，则2比特表现进一步提升。2比特的LLaMA2-7B在Apple M2（4线程，批量1）上推理速率超过25个token/s。


<details>
  <summary>Details</summary>
Motivation: 在CPU边缘设备上部署大语言模型时，受限于内存和计算资源，推理时的内存占用和延迟成为主要瓶颈。现有的硬件友好量化多采用统一量化，难以很好拟合权重分布，且在低比特宽度下会产生较高的去量化开销。

Method: 提出层次线性量化HLQ的量化格式，作为ELUTQ框架的一部分。这种量化方式能更好地拥抱权重的统计特征，同时不增加基于位序列的LUT GEMM运算的计算成本，从而消除去量化开销。HLQ与现有量化算法正交且可无缝接入各种量化管线。为端设备部署给出优化的CPU内核以支撑端到端推理。

Result: 在实验中，HLQ对LLaMA3-8B在3-bit和2-bit下的PTQ分别带来约8%和约85%的困惑度改善，并且在一小时内完成量化。配合高效微调，2-bit的表现进一步提升。就推理效率而言，2-bit的LLaMA2-7B在Apple M2（4线程，批量1）上达到>25 tokens/s。

Conclusion: HLQ为在边缘设备上部署LLMs提供了一种实用且可扩展的量化方案，具备显著的困惑度改进与推理吞吐提升，并且与现有量化流程正交、易于集成，适用于多种量化管线与硬件。

Abstract: The deployment of Large Language Models (LLMs) on CPU-based edge devices is
crucial for enabling on-device intelligence and expanding AI accessibility.
However, it remains challenging due to limited memory and computational
resources. During edge inference, memory usage and latency are the primary
bottlenecks. Although weight quantization can effectively reduce memory
consumption, existing hardware-friendly approaches often rely on uniform
quantization, which poorly fits weight distributions and incurs high
dequantization overhead at low bit widths. To address these limitations, we
propose ELUTQ, an efficient quantization framework introducing a novel
quantization format, Hierarchical Linear Quantization (HLQ). HLQ better
captures the statistical characteristics of weights without increasing the
computational cost of Bit-serial LUT-based GEMM operations, thereby eliminating
dequantization overhead. It is orthogonal to existing quantization algorithms
and can be seamlessly integrated into various quantization pipelines. For
efficient on-device deployment, ELUTQ provides optimized CPU kernels for
end-to-end inference. Experiments show that for LLaMA3-8B, HLQ reduces
perplexity by about 8% at 3-bit and 85% at 2-bit precision under post-training
quantization, completing quantization within one hour. With efficient
finetuning, HLQ further improves 2-bit performance within two hours. In terms
of inference efficiency, our 2-bit LLaMA2-7B achieves over 25 tokens/s on an
Apple M2 chip (4 threads, batch size = 1).

</details>


### [62] [Teaming LLMs to Detect and Mitigate Hallucinations](https://arxiv.org/abs/2510.19507)
*Demian Till,John Smeaton,Peter Haubrick,Gouse Saheb,Florian Graef,David Berman*

Main category: cs.LG

TL;DR: 将单模型的一致性方法扩展到多模型联盟，称为“consortium consistency”，在15个LLMs上评估，取得比单模型更强的检测与缓解能力，并在某些情形下降低推理成本。


<details>
  <summary>Details</summary>
Motivation: LLM在训练数据偏差与信息覆盖不足等因素下易产生幻觉，对抗幻觉需要稳健的检测与缓解。单模型一致性方法受限于数据与模型范围，跨模型整合可能提升鲁棒性和覆盖面。

Method: 构建跨多模型的响应聚合框架，将不同训练数据、训练策略和模型架构的LLMs联合起来；在来自多个模型团队的15个LLMs上进行评估，比较单模型一致性、跨模型一致性以及混合设置的效果，分析在何种条件下受益并评估推理成本。

Result: 在多模型联盟下，幻觉检测与缓解性能显著优于单模型一致性；收益受条件影响，但在多数场景中表现更好；并且推理成本常明显下降，抵消单模型一致性中的成本问题。

Conclusion: 跨模型一致性是一种有效的幻觉治理策略，能够提升鲁棒性与检测/缓解能力，同时在成本上具备潜在优势，值得在大规模部署中考虑。

Abstract: Recent work has demonstrated state-of-the-art results in large language model
(LLM) hallucination detection and mitigation through consistency-based
approaches which involve aggregating multiple responses sampled from a single
LLM for a given prompt. These approaches help offset limitations stemming from
the imperfect data on which LLMs are trained, which includes biases and
under-representation of information required at deployment time among other
limitations which can lead to hallucinations. We show that extending these
single-model consistency methods to combine responses from multiple LLMs with
different training data, training schemes and model architectures can result in
substantial further improvements in hallucination detection and mitigation
capabilities beyond their single-model consistency counterparts. We evaluate
this \emph{consortium consistency} approach across many model teams from a pool
of 15 LLMs and explore under what conditions it is beneficial to team together
different LLMs in this manner. Further, we show that these performance
improvements often come with reduced inference costs, offsetting a significant
drawback with single-model consistency methods.

</details>


### [63] [From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification](https://arxiv.org/abs/2510.19514)
*Maciej Mozolewski,Betül Bayrak,Kerstin Bach,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: 提出一种基于原型驱动的稀疏对照解释框架，用于12导联ECG时间序列的原型驱动对照因果解释，结合SHAP、DTW、原型聚类和R峰对齐实现近实时、可解释且生理上连贯的诊断解释。


<details>
  <summary>Details</summary>
Motivation: 在时序数据的可解释性AI中，需提供更加可操作、对临床有用的实例级解释；现有模型往往缺乏生理一致性、稳定性和可控性，难以直接用于临床决策。通过原型驱动的对照解释与生理特征对齐，提升ECG诊断的可解释性和可信度。

Method: 1) 使用SHAP阈值识别关键信号段并转化为区间规则；2) 通过DTW和Medoid聚类提取代表性原型；3) 将原型与查询样本的R峰对齐以确保解释的时间一致性；4) 生成对照解释，允许对原始信号进行局部修改（约78%信号被修改），并在所有类别上保持81.3%的有效性，时序稳定性提升43%；5) 评估Original、Sparse、Aligned Sparse三种变体，MI等类别的性能从高效性到挑战性不等（MI接近98.9%有效性，HYP约13.2%）并实现小于1秒的近实时生成。

Result: 对照解释在时间维度上更稳定，能在近实时内生成并具备临床有效性；原型化策略使不同类别的解释具有差异化表现，MI等疾病实现高有效性，而HYP等类别存在挑战；总体上，方法提供了稳健的可解释性解决方案并可扩展到交互式解释平台。

Conclusion: 本工作建立了生理感知的对照解释设计原则，推动可控、临床友好的解释界面与交互平台的开发，为AI诊断系统的临床部署铺平道路。

Abstract: In eXplainable Artificial Intelligence (XAI), instance-based explanations for
time series have gained increasing attention due to their potential for
actionable and interpretable insights in domains such as healthcare. Addressing
the challenges of explainability of state-of-the-art models, we propose a
prototype-driven framework for generating sparse counterfactual explanations
tailored to 12-lead ECG classification models. Our method employs SHAP-based
thresholds to identify critical signal segments and convert them into interval
rules, uses Dynamic Time Warping (DTW) and medoid clustering to extract
representative prototypes, and aligns these prototypes to query R-peaks for
coherence with the sample being explained. The framework generates
counterfactuals that modify only 78% of the original signal while maintaining
81.3% validity across all classes and achieving 43% improvement in temporal
stability. We evaluate three variants of our approach, Original, Sparse, and
Aligned Sparse, with class-specific performance ranging from 98.9% validity for
myocardial infarction (MI) to challenges with hypertrophy (HYP) detection
(13.2%). This approach supports near realtime generation (< 1 second) of
clinically valid counterfactuals and provides a foundation for interactive
explanation platforms. Our findings establish design principles for
physiologically-aware counterfactual explanations in AI-based diagnosis systems
and outline pathways toward user-controlled explanation interfaces for clinical
deployment.

</details>


### [64] [Bi-Level Decision-Focused Causal Learning for Large-Scale Marketing Optimization: Bridging Observational and Experimental Data](https://arxiv.org/abs/2510.19517)
*Shuli Zhang,Hao Zhou,Jiaqi Zheng,Guibin Jiang,Bing Cheng,Wei Lin,Guihai Chen*

Main category: cs.LG

TL;DR: Bi-DFCL 通过一个双层决策聚焦的因果学习框架，利用实验数据中的无偏决策质量估计来引导模型训练，并用隐式微分的 bi-level 优化整合观测数据与实验数据，从而在提升下游决策质量的同时降低偏差方差，已在美团等场景落地并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线平台的营销策略需要在预测与决策之间对齐，同时应对观测数据中的偏差与实验数据的高方差与稀缺性带来的挑战。传统的预测-决策管线在预测精度与决策效果之间存在错配，且受样本偏差影响导致学习方向不佳。

Method: 提出无偏的 OR 决策质量估计器，基于实验数据构建梯度可传播的替代损失，连接离散优化梯度；构建双层优化框架，联合利用观测数据与实验数据，通过隐式微分求解，使无偏的决策质量估计器纠正来自 biased 的观测数据的学习方向，同时实现偏差-方差权衡的最优。

Result: 在公开基准、工业营销数据以及大规模在线 A/B 测试上均展现对比方法的统计显著提升；已在美团等全球规模的在线餐饮平台部署。

Conclusion: Bi-DFCL 成功解决了预测-决策错配与偏差-方差权衡问题，提供了一种将因果学习与优化耦合的通用框架，并具备良好的实证与落地效能。

Abstract: Online Internet platforms require sophisticated marketing strategies to
optimize user retention and platform revenue -- a classical resource allocation
problem. Traditional solutions adopt a two-stage pipeline: machine learning
(ML) for predicting individual treatment effects to marketing actions, followed
by operations research (OR) optimization for decision-making. This paradigm
presents two fundamental technical challenges. First, the prediction-decision
misalignment: Conventional ML methods focus solely on prediction accuracy
without considering downstream optimization objectives, leading to improved
predictive metrics that fail to translate to better decisions. Second, the
bias-variance dilemma: Observational data suffers from multiple biases (e.g.,
selection bias, position bias), while experimental data (e.g., randomized
controlled trials), though unbiased, is typically scarce and costly --
resulting in high-variance estimates. We propose Bi-level Decision-Focused
Causal Learning (Bi-DFCL) that systematically addresses these challenges.
First, we develop an unbiased estimator of OR decision quality using
experimental data, which guides ML model training through surrogate loss
functions that bridge discrete optimization gradients. Second, we establish a
bi-level optimization framework that jointly leverages observational and
experimental data, solved via implicit differentiation. This novel formulation
enables our unbiased OR estimator to correct learning directions from biased
observational data, achieving optimal bias-variance tradeoff. Extensive
evaluations on public benchmarks, industrial marketing datasets, and
large-scale online A/B tests demonstrate the effectiveness of Bi-DFCL, showing
statistically significant improvements over state-of-the-art. Currently,
Bi-DFCL has been deployed at Meituan, one of the largest online food delivery
platforms in the world.

</details>


### [65] [Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning](https://arxiv.org/abs/2510.19530)
*Ruiyao Miao,Junren Xiao,Shiya Tsang,Hui Xiong,Yingnian Wu*

Main category: cs.LG

TL;DR: 提出 REBMBO，一种将高斯过程用于局部引导与能量基模型结合的贝叶斯优化框架，通过将每次 BO 迭代建模为马尔可夫决策过程并使用 PPO 进行多步前瞻，动态调整探索深度与方向，以克服传统 BO 的一步偏差，并在合成与真实基准上展现优越性。


<details>
  <summary>Details</summary>
Motivation: 现有 BO 方法在成本高、维度较高的任务中容易陷入一阶偏差，导致收敛到局部最优。Black-Box Optimization (BBO) 在梯度不可用的场景下表现良好，但需要更强的全局信息建模与策略性探索。

Method: 将每次优化迭代视为一个马尔可夫决策过程，使用高斯过程提供局部引导，能量基模型用于捕捉全局结构信息；通过近端策略优化（PPO）训练策略，实现自适应多步前瞻，动态调整探索深度与方向，从而克服传统 BO 的局部偏差。

Result: 在合成和真实基准上进行的广泛实验显示 REBMBO 的性能优于传统贝叶斯优化方法；对不同 GP 配置的额外分析展示了其适应性与鲁棒性。

Conclusion: REBMBO 能有效平衡探索与利用，利用自适应多步前瞻策略，克服一阶偏差，对成本高的目标函数评估具有实际潜力，且对不同 GP 配置具有良好的鲁棒性。

Abstract: Existing Bayesian Optimization (BO) methods typically balance exploration and
exploitation to optimize costly objective functions. However, these methods
often suffer from a significant one-step bias, which may lead to convergence
towards local optima and poor performance in complex or high-dimensional tasks.
Recently, Black-Box Optimization (BBO) has achieved success across various
scientific and engineering domains, particularly when function evaluations are
costly and gradients are unavailable. Motivated by this, we propose the
Reinforced Energy-Based Model for Bayesian Optimization (REBMBO), which
integrates Gaussian Processes (GP) for local guidance with an Energy-Based
Model (EBM) to capture global structural information. Notably, we define each
Bayesian Optimization iteration as a Markov Decision Process (MDP) and use
Proximal Policy Optimization (PPO) for adaptive multi-step lookahead,
dynamically adjusting the depth and direction of exploration to effectively
overcome the limitations of traditional BO methods. We conduct extensive
experiments on synthetic and real-world benchmarks, confirming the superior
performance of REBMBO. Additional analyses across various GP configurations
further highlight its adaptability and robustness.

</details>


### [66] [The Confusing Instance Principle for Online Linear Quadratic Control](https://arxiv.org/abs/2510.19531)
*Waris Radji,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 提出 MED-LQ，通过 Confusing Instance 原理和 MED 框架，对未知动态的线性-二次系统进行模型驱动强化学习控制，在 LQR 结构下实现鲁棒且有扩展潜力的控制，表现出与现有方法的竞争力。


<details>
  <summary>Details</summary>
Motivation: 弥补基于多臂赌博机的 Optimism/C.Thompson 等方法在大规模或连续状态-动作系统中的实际局限，寻求对未知动力学的可扩展且理论有支撑的控制策略。

Method: 将 Confusing Instance/ Minimum Empirical Divergence 的原理应用于线性二次控制，结合 LQR 策略的结构特征、敏感性分析与稳定性分析，提出 MED-LQ，超越小规模设置的适用性。

Result: 在全面的控制基准上，MED-LQ 展现出与多场景下的竞争性表现，验证了其在大规模 MDP 和复杂控制任务中的潜力与可扩展性。

Conclusion: CI 与 MED 框架为未知动力学的线性-二次控制提供了有效的模型驱动 RL 路线，且具备向大规模 MDP 应用扩展的前景。

Abstract: We revisit the problem of controlling linear systems with quadratic cost
under unknown dynamics with model-based reinforcement learning. Traditional
methods like Optimism in the Face of Uncertainty and Thompson Sampling, rooted
in multi-armed bandits (MABs), face practical limitations. In contrast, we
propose an alternative based on the Confusing Instance (CI) principle, which
underpins regret lower bounds in MABs and discrete Markov Decision Processes
(MDPs) and is central to the Minimum Empirical Divergence (MED) family of
algorithms, known for their asymptotic optimality in various settings. By
leveraging the structure of LQR policies along with sensitivity and stability
analysis, we develop MED-LQ. This novel control strategy extends the principles
of CI and MED beyond small-scale settings. Our benchmarks on a comprehensive
control suite demonstrate that MED-LQ achieves competitive performance in
various scenarios while highlighting its potential for broader applications in
large-scale MDPs.

</details>


### [67] [Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data](https://arxiv.org/abs/2510.19535)
*Markus Bujotzek,Evelyn Trautmann,Calum Hand,Ian Hales*

Main category: cs.LG

TL;DR: 本研究评估在分布式分子数据上进行聚类的联邦学习方法，比较 Fed-kMeans、Fed-PCA+Fed-kMeans、Fed-LSH 与集中式基线，在八个多样化分子数据集上进行评估；引入面向化学领域的 SF-ICF 指标；结果强调领域知识对联邦聚类评估的重要性以及在客户端进行可解释性分析的必要性。


<details>
  <summary>Details</summary>
Motivation: 私有制药数据在公开数据集上缺乏规模与多样性。联邦学习提供一种隐私保护的跨数据孤岛协作训练方案，但联邦数据访问带来数据中心任务（如衡量数据多样性、制定数据划分、理解联合化学空间结构）的挑战。本研究旨在探索联邦聚类在分布式分子数据中的分离与表示能力。

Method: 对三种联邦聚类方法进行基准评估：Fed-kMeans、Fed-PCA+Fed-kMeans 和 Fed-LSH，并与集中化方法在八个分子数据集上进行对比。评估同时使用标准度量和本研究引入的化学信息化度量 SF-ICF；并结合对客户端的可解释性分析，进行对数据多样性与联合化学空间的解释与分析。

Result: 联邦聚类方法能够在一定程度上逼近集中化结果，表现受方法与数据集差异影响显著。引入 SF-ICF 等化学领域指标揭示了标准度量难以捕捉的分子数据多样性与结构特征，强调领域知识在评估中的作用；客户端级别的可解释性分析有助于理解联邦环境下的多样性分布与数据划分影响。

Conclusion: 将领域知识融入评估是化学领域联邦聚类研究的关键。SF-ICF 及相关的客户端可解释性分析为隐私保护下的分子数据多样性评估提供了有效工具。联邦聚类有望支持药物发现中的隐私保护数据整合、数据划分优化与分子空间探索，但在异质性数据与可解释性方面仍需进一步研究。

Abstract: AI methods are increasingly shaping pharmaceutical drug discovery. However,
their translation to industrial applications remains limited due to their
reliance on public datasets, lacking scale and diversity of proprietary
pharmaceutical data. Federated learning (FL) offers a promising approach to
integrate private data into privacy-preserving, collaborative model training
across data silos. This federated data access complicates important
data-centric tasks such as estimating dataset diversity, performing informed
data splits, and understanding the structure of the combined chemical space. To
address this gap, we investigate how well federated clustering methods can
disentangle and represent distributed molecular data. We benchmark three
approaches, Federated kMeans (Fed-kMeans), Federated Principal Component
Analysis combined with Fed-kMeans (Fed-PCA+Fed-kMeans), and Federated
Locality-Sensitive Hashing (Fed-LSH), against their centralized counterparts on
eight diverse molecular datasets. Our evaluation utilizes both, standard
mathematical and a chemistry-informed evaluation metrics, SF-ICF, that we
introduce in this work. The large-scale benchmarking combined with an in-depth
explainability analysis shows the importance of incorporating domain knowledge
through chemistry-informed metrics, and on-client explainability analyses for
federated diversity analysis on molecular data.

</details>


### [68] [Learning and Simulating Building Evacuation Patterns for Enhanced Safety Design Using Generative Models](https://arxiv.org/abs/2510.19623)
*Jin Han,Zhe Zheng,Yi Gu,Jia-Rui Lin,Xin-Zheng Lu*

Main category: cs.LG

TL;DR: DiffEvac 使用扩散模型从 evacuation 热力图学习建筑疏散模式，提出解耦特征表示以嵌入布局与人口密度等物理特征，实现快速且可扩展的疏散仿真，显著提升图像质量度量与仿真速度。


<details>
  <summary>Details</summary>
Motivation: 在早期设计阶段，传统疏散仿真参数繁多且难以快速迭代，迫切需要一种数据驱动、可扩展且高效的仿真方法来支持快速设计迭代与多目标优化。

Method: 建立399个多样化功能布局及对应的建筑疏散热力图数据集；提出解耦特征表示以嵌入布局、 occupant density 等物理特征；基于图像提示的扩散模型从仿真热力图中学习疏散模式。

Result: 相较于使用 RGB 表示的条件 GAN，DiffEvac 在 SSIM 上提升可达 37.6%，在 PSNR 上提升 142%，仿真时间提升约 16 倍，最终实现约 2 分钟的仿真时间。案例研究显示可显著加速设计迭代并拓展安全优化的技术路径。

Conclusion: 该方法降低建模负担，使大规模“若干设计”探索成为可能，并能与多目标设计工具耦合，为智能建筑设计中的安全优化提供新路径。

Abstract: Evacuation simulation is essential for building safety design, ensuring
properly planned evacuation routes. However, traditional evacuation simulation
relies heavily on refined modeling with extensive parameters, making it
challenging to adopt such methods in a rapid iteration process in early design
stages. Thus, this study proposes DiffEvac, a novel method to learn building
evacuation patterns based on Generative Models (GMs), for efficient evacuation
simulation and enhanced safety design. Initially, a dataset of 399 diverse
functional layouts and corresponding evacuation heatmaps of buildings was
established. Then, a decoupled feature representation is proposed to embed
physical features like layouts and occupant density for GMs. Finally, a
diffusion model based on image prompts is proposed to learn evacuation patterns
from simulated evacuation heatmaps. Compared to existing research using
Conditional GANs with RGB representation, DiffEvac achieves up to a 37.6%
improvement in SSIM, 142% in PSNR, and delivers results 16 times faster,
thereby cutting simulation time to 2 minutes. Case studies further demonstrate
that the proposed method not only significantly enhances the rapid design
iteration and adjustment process with efficient evacuation simulation but also
offers new insights and technical pathways for future safety optimization in
intelligent building design. The research implication is that the approach
lowers the modeling burden, enables large-scale what-if exploration, and
facilitates coupling with multi-objective design tools.

</details>


### [69] [Latent Space Factorization in LoRA](https://arxiv.org/abs/2510.19640)
*Shashi Kumar,Yacouba Kaloga,John Mitros,Petr Motlicek,Ina Kodrasi*

Main category: cs.LG

TL;DR: FVAE-LoRA 通过一个分解的变分自编码器把 LoRA 的低秩子空间分成任务相关和残差信息两个潜在空间，并通过新的基于证据下界的因式分解约束实现两者的分离，从而在文本、音频和图像任务上普遍优于标准 LoRA，并在分布偏移下表现出更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的 LoRA 缺乏在学习的低秩子空间内显式区分任务相关信息的机制，可能限制下游性能与对分布偏移的鲁棒性。

Method: 提出 FVAE-LoRA：使用变分自编码器学习两个独立的潜在空间；引入新的证据下界（ELBO）以显式促进两个潜在空间的因式分解，一组专用于任务显著特征，另一组用于残差信息；将该框架与 LoRA 结合以实现高效微调。

Result: 在文本、音频和图像任务上，FVAE-LoRA 持续优于标准 LoRA；对虚假相关性（spurious correlations）的评估表明其能更好地分离任务相关信号，从而在分布偏移下具有更强的鲁棒性。

Conclusion: FVAE-LoRA 能在低秩自适应场景中有效地解耦任务相关信息，具备跨模态的推广性，并带来鲁棒性提升；代码已开源。

Abstract: Low-rank adaptation (LoRA) is a widely used method for parameter-efficient
finetuning. However, existing LoRA variants lack mechanisms to explicitly
disambiguate task-relevant information within the learned low-rank subspace,
potentially limiting downstream performance. We propose Factorized Variational
Autoencoder LoRA (FVAE-LoRA), which leverages a VAE to learn two distinct
latent spaces. Our novel Evidence Lower Bound formulation explicitly promotes
factorization between the latent spaces, dedicating one latent space to
task-salient features and the other to residual information. Extensive
experiments on text, audio, and image tasks demonstrate that FVAE-LoRA
consistently outperforms standard LoRA. Moreover, spurious correlation
evaluations confirm that FVAE-LoRA better isolates task-relevant signals,
leading to improved robustness under distribution shifts. Our code is publicly
available at: https://github.com/idiap/FVAE-LoRA

</details>


### [70] [Overlap-weighted orthogonal meta-learner for treatment effect estimation over time](https://arxiv.org/abs/2510.19643)
*Konstantin Hess,Dennis Frauen,Mihaela van der Schaar,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出一种时变设置下的时间异质性处理效应（HTE）估计的新方法：重叠加权正交WO元学习器（WO-learner），通过聚焦于高观测到干预序列的区域来减小因低重叠导致的方差并提高估计稳定性；方法具Neyman正交性、模型无关性，适用于任意机器学习模型。通过Transformer和LSTM骨架的广泛实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在时变治疗设置中，随着预测 horizon 增长，观测到特定治疗序列的概率呈指数级下降，导致数据对许多合理治疗序列的支持不足，产生严重重叠问题。现有元学习者通常假设足够的治疗重叠，在重叠不足时估计方差急剧放大。需要一种鲁棒、数据驱动的方法来缓解重叠不足带来的不稳定性。

Method: 提出重叠加权的总体风险函数，并结合Neyman正交性以最小化重叠加权的oracle风险。WO-learner 将重点放在高概率接受干预序列的样本区域，具备数据驱动性且对 nuisance 函数的误设具有鲁棒性。该方法完全模型无关，可与任意机器学习模型集成（如 Transformer、LSTM 等骨架）。

Result: 理论上，该方法具备 Neyman-正交性，对 nuisance 函数的错误指定具有鲁棒性。实证结果通过 Transformer 与 LSTM 两种骨架的广泛实验，展示了WO-learner 在处理重叠不足情形下提升HTE估计的稳定性与可靠性。

Conclusion: WO-learner 提供了一种鲁棒、数据驱动且模型无关的解决方案，用于时变环境中的HTE估计，能有效缓解重叠不足带来的估计不稳定性，并可灵活地与各种机器学习模型结合以提升表现。

Abstract: Estimating heterogeneous treatment effects (HTEs) in time-varying settings is
particularly challenging, as the probability of observing certain treatment
sequences decreases exponentially with longer prediction horizons. Thus, the
observed data contain little support for many plausible treatment sequences,
which creates severe overlap problems. Existing meta-learners for the
time-varying setting typically assume adequate treatment overlap, and thus
suffer from exploding estimation variance when the overlap is low. To address
this problem, we introduce a novel overlap-weighted orthogonal (WO)
meta-learner for estimating HTEs that targets regions in the observed data with
high probability of receiving the interventional treatment sequences. This
offers a fully data-driven approach through which our WO-learner can counteract
instabilities as in existing meta-learners and thus obtain more reliable HTE
estimates. Methodologically, we develop a novel Neyman-orthogonal population
risk function that minimizes the overlap-weighted oracle risk. We show that our
WO-learner has the favorable property of Neyman-orthogonality, meaning that it
is robust against misspecification in the nuisance functions. Further, our
WO-learner is fully model-agnostic and can be applied to any machine learning
model. Through extensive experiments with both transformer and LSTM backbones,
we demonstrate the benefits of our novel WO-learner.

</details>


### [71] [Policy Learning with Abstention](https://arxiv.org/abs/2510.19672)
*Ayush Sawarni,Jikai Jin,Justin Whitehouse,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: 提出在策略学习中引入 abstention（弃权）机制，并给出两阶段学习器：先筛选近似最优策略集合，再基于分歧构建弃权规则；在已知与未知 propensities 下分别给出快速的 O(1/n) 及对偶鲁棒扩展，并展示弃权在多方面的应用与理论保障。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景下，强制在预测不确定时做出决策具有较高风险。弃权允许在不确定时转向安全的默认策略或专家，同时对弃权行为附带小额奖励，提高整体策略的鲁棒性与安全性。

Method: 提出一个两阶段学习框架：第一阶段确定近似最优策略的集合；第二阶段基于这些策略的分歧构建弃权规则，使在不确定情形下能够安然弃权并获得额外奖励。对于已知倾向率，推导出 O(1/n) 的快速遗憾界；对于未知倾向率，通过引入一个对偶鲁棒（DR）目标扩展上述界限。

Result: 在已知倾向率情境下获得 O(1/n) 遗憾界；在未知倾向率情境下通过 DR 对象实现等价的鲁棒性界限。弃权作为一种通用工具，能够在边际条件（margin conditions）下提供更强的理论保证；与分布鲁棒策略学习相关，能够对小数据偏移做出防护；并且在安全策略改进方面，能以高概率确保对基线策略的改进。

Conclusion: 弃权为策略学习提供一种多用途且理论上强大的工具，不仅提升在高风险场景的安全性与鲁棒性，也在多种核心问题中具备实用的理论与方法论联系与推广潜力。

Abstract: Policy learning algorithms are widely used in areas such as personalized
medicine and advertising to develop individualized treatment regimes. However,
most methods force a decision even when predictions are uncertain, which is
risky in high-stakes settings. We study policy learning with abstention, where
a policy may defer to a safe default or an expert. When a policy abstains, it
receives a small additive reward on top of the value of a random guess. We
propose a two-stage learner that first identifies a set of near-optimal
policies and then constructs an abstention rule from their disagreements. We
establish fast O(1/n)-type regret guarantees when propensities are known, and
extend these guarantees to the unknown-propensity case via a doubly robust (DR)
objective. We further show that abstention is a versatile tool with direct
applications to other core problems in policy learning: it yields improved
guarantees under margin conditions without the common realizability assumption,
connects to distributionally robust policy learning by hedging against small
data shifts, and supports safe policy improvement by ensuring improvement over
a baseline policy with high probability.

</details>


### [72] [Study of Training Dynamics for Memory-Constrained Fine-Tuning](https://arxiv.org/abs/2510.19675)
*Aël Quélennec,Nour Hezbri,Pavlo Mozharovskyi,Van-Tam Nguyen,Enzo Tartaglione*

Main category: cs.LG

TL;DR: Memory-efficient transfer learning with TraDy leverages dynamic stochastic channel selection and layer-importance-aware updates to achieve high sparsity and large FLOP reductions while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address memory bottlenecks in training large models under strict resource constraints; exploit architecture-dependent layer importance and stochastic channel selection to improve gradient approximation.

Method: Propose TraDy: treat layer importance as architecture-dependent and computable a priori; apply dynamic channel selection by stochastically resampling channels between epochs within preselected layers; evaluate on various downstream tasks and architectures under memory constraints.

Result: Achieves state-of-the-art performance across downstream tasks and architectures under memory constraints; reports up to 99% activation sparsity, 95% weight-derivative sparsity, and 97% reduction in FLOPs for weight-derivative computation.

Conclusion: TraDy enables memory-efficient transfer learning by combining a priori layer-importance with dynamic stochastic channel sampling, delivering substantial compute and memory savings with competitive performance.

Abstract: Memory-efficient training of deep neural networks has become increasingly
important as models grow larger while deployment environments impose strict
resource constraints. We propose TraDy, a novel transfer learning scheme
leveraging two key insights: layer importance for updates is
architecture-dependent and determinable a priori, while dynamic stochastic
channel selection provides superior gradient approximation compared to static
approaches. We introduce a dynamic channel selection approach that
stochastically resamples channels between epochs within preselected layers.
Extensive experiments demonstrate TraDy achieves state-of-the-art performance
across various downstream tasks and architectures while maintaining strict
memory constraints, achieving up to 99% activation sparsity, 95% weight
derivative sparsity, and 97% reduction in FLOPs for weight derivative
computation.

</details>


### [73] [Fast Inference via Hierarchical Speculative Decoding](https://arxiv.org/abs/2510.19705)
*Amir Globerson,Haim Kaplan,Yishay Mansour,Clara Mohri,Tal Schuster*

Main category: cs.LG

TL;DR: Hierarchical Speculative Decoding (HSD) stack-drafts models to reduce autoregressive generation latency: draft models propose tokens, larger models verify in sequence until the target verifies. The latency-optimal hierarchy can be found in polynomial time, and empirically yields up to 1.2x speed-up over the best single-draft baseline.


<details>
  <summary>Details</summary>
Motivation: Speculative decoding uses draft models to propose tokens for the main model in parallel, reducing latency, but real systems may have a spectrum of draft models with different speed-accuracy trade-offs. The study seeks to exploit this by organizing draft models into a hierarchy to minimize latency while maintaining output quality.

Method: Construct a hierarchy of draft models where each model proposes tokens that the next larger model verifies in a single forward pass, culminating in the target model verification. Derive an expression for the expected latency of any hierarchy and prove that the latency-optimal hierarchy can be found in polynomial time.

Result: The hierarchical scheme achieves up to 1.2x speed-up over the best single-draft baseline in experiments, demonstrating practical latency reductions beyond prior speculative decoding methods.

Conclusion: Hierarchical Speculative Decoding effectively reduces generation latency by leveraging a sequence of draft models, with a tractable method to identify the optimal hierarchy and substantial empirical speed-ups over single-draft approaches.

Abstract: Transformer language models generate text autoregressively, making inference
latency proportional to the number of tokens generated. Speculative decoding
reduces this latency without sacrificing output quality, by leveraging a small
draft model to propose tokens that the larger target model verifies in
parallel. In practice, however, there may exist a set of potential draft
models- ranging from faster but less inaccurate, to slower yet more reliable.
We introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacks
these draft models into a hierarchy, where each model proposes tokens, and the
next larger model verifies them in a single forward pass, until finally the
target model verifies tokens. We derive an expression for the expected latency
of any such hierarchy and show that selecting the latency-optimal hierarchy can
be done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over the
best single-draft baseline, demonstrating the practicality of our algorithm in
reducing generation latency beyond previous techniques.

</details>


### [74] [SEMPO: Lightweight Foundation Models for Time Series Forecasting](https://arxiv.org/abs/2510.19710)
*Hui He,Kun Yi,Yuanchi Ma,Qi Zhang,Zhendong Niu,Guansong Pang*

Main category: cs.LG

TL;DR: 提出SEMPO，一种轻量级时序基础模型，通过能量感知的谱分解和混合提示的Transformer，在较小数据和模型规模下实现良好的一般化能力，在零-shot和少量-shot场景中优于现有方法，且在16个数据集、两大基准上有验证，开源代码与数据。


<details>
  <summary>Details</summary>
Motivation: 在大规模时序基础模型普及的背景下，存在模型规模庞大、需要大量预训练数据、难以部署于资源受限环境的问题；需要更轻量、数据友好且具备广泛泛化能力的模型。

Method: 提出能量感知的谱分解模块，强调对高能且低能但信息量大的频段的利用；以及混合提示的Transformer，通过小数据集特定提示来学习异质时间模式，并将时间序列token动态路由到提示专家以实现参数高效的跨数据域适配。

Result: 通过在两个大规模基准、覆盖16个数据集的实验，展示SEMPO在零-shot和少-shot forecast任务中的优越性，显著降低预训练数据规模和模型规模，同时保持强泛化能力，相较于最先进方法具有竞争力。

Conclusion: SEMPO证明在资源受限环境中也能实现强通用的时序预测，降低数据和计算需求，同时保持良好性能；代码和数据已公开，便于复现与进一步研究。

Abstract: The recent boom of large pre-trained models witnesses remarkable success in
developing foundation models (FMs) for time series forecasting. Despite
impressive performance across diverse downstream forecasting tasks, existing
time series FMs possess massive network architectures and require substantial
pre-training on large-scale datasets, which significantly hinders their
deployment in resource-constrained environments. In response to this growing
tension between versatility and affordability, we propose SEMPO, a novel
lightweight foundation model that requires pretraining on relatively
small-scale data, yet exhibits strong general time series forecasting.
Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctral
decomposition module, that substantially improves the utilization of
pre-training data by modeling not only the high-energy frequency signals but
also the low-energy yet informative frequency signals that are ignored in
current methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns
heterogeneous temporal patterns through small dataset-specific prompts and
adaptively routes time series tokens to prompt-based experts for
parameter-efficient model adaptation across different datasets and domains.
Equipped with these modules, SEMPO significantly reduces both pre-training data
scale and model size, while achieving strong generalization. Extensive
experiments on two large-scale benchmarks covering 16 datasets demonstrate the
superior performance of SEMPO in both zero-shot and few-shot forecasting
scenarios compared with state-of-the-art methods. Code and data are available
at https://github.com/mala-lab/SEMPO.

</details>


### [75] [Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series](https://arxiv.org/abs/2510.19728)
*Mahmoud Ibrahim,Bart Elen,Chang Sun,Gökhan Ertaylan,Michel Dumontier*

Main category: cs.LG

TL;DR: 提出一种增强的 TimeAutoDiff 框架，通过分布对齐惩罚提升合成 ICU 时间序列的评估可信度，显著缩小真实-对比评估差距并提高亚群估计的准确性，同时实现隐私保护的评估路径。


<details>
  <summary>Details</summary>
Motivation: 在临床预测中，使用合成数据不仅需要具备训练价值，还需具备可信且可验证的评估能力，尤其是在隐私保护和少数据的亚群情境下。本研究在前代生成模型基础上通过增强目标函数来提升评估可信度。

Method: 在 TimeDiff、HealthGen、TimeAutoDiff 等基础上提出 Enhanced TimeAutoDiff，通过在潜在扩散目标中加入分布对齐惩罚来改善合成数据的分布匹配；在 MIMIC-III 与 eICU 数据集上针对 24 小时死亡率和二元住院时长任务进行实验，评估覆盖 32 个交叉子群及大规模合成队列对亚群 AUROC 的估计误差。

Result: Enhanced TimeAutoDiff 将真实-on合成（TRTS）差距缩小超过 70%，ΔTRTS ≤ 0.014 的 AUROC；保持训练效用（ΔTSTR ≈ 0.01）。对于 32 交叉子群，大规模合成队列将亚群层面的 AUROC 估计误差相比小真实测试集降低至多 50%，且在 72–84% 的子群中优于小测试集。

Conclusion: 研究提供了一条可实践的、隐私保护的可trustworthy、粒度化模型评估路线，支持在不暴露敏感 EHR 数据的前提下，对多样化人群的模型性能进行稳健分析，提升医学 AI 的可信度。

Abstract: We present a novel framework for leveraging synthetic ICU time-series data
not only to train but also to rigorously and trustworthily evaluate predictive
models, both at the population level and within fine-grained demographic
subgroups. Building on prior diffusion and VAE-based generators (TimeDiff,
HealthGen, TimeAutoDiff), we introduce \textit{Enhanced TimeAutoDiff}, which
augments the latent diffusion objective with distribution-alignment penalties.
We extensively benchmark all models on MIMIC-III and eICU, on 24-hour mortality
and binary length-of-stay tasks. Our results show that Enhanced TimeAutoDiff
reduces the gap between real-on-synthetic and real-on-real evaluation (``TRTS
gap'') by over 70\%, achieving $\Delta_{TRTS} \leq 0.014$ AUROC, while
preserving training utility ($\Delta_{TSTR} \approx 0.01$). Crucially, for 32
intersectional subgroups, large synthetic cohorts cut subgroup-level AUROC
estimation error by up to 50\% relative to small real test sets, and outperform
them in 72--84\% of subgroups. This work provides a practical,
privacy-preserving roadmap for trustworthy, granular model evaluation in
critical care, enabling robust and reliable performance analysis across diverse
patient populations without exposing sensitive EHR data, contributing to the
overall trustworthiness of Medical AI.

</details>


### [76] [Statistical Inference for Linear Functionals of Online Least-squares SGD when $t \gtrsim d^{1+δ}$](https://arxiv.org/abs/2510.19734)
*Bhavya Agrawalla,Krishnakumar Balasubramanian,Promit Ghosal*

Main category: cs.LG

TL;DR: Non-asymptotic Berry–Esseen bounds for online least-squares SGD yield a Gaussian CLT in growing dimension, enabling online, data-driven confidence intervals with near-optimal scaling.


<details>
  <summary>Details</summary>
Motivation: Quantify uncertainty for SGD in high-dimensional, online settings; enable finite-sample inference with theoretical guarantees.

Method: Derive non-asymptotic Berry–Esseen bounds for linear functionals of online least-squares SGD, establishing a Gaussian CLT in t growing as d^{1+δ}. Propose an online variance estimator and high-probability deviation bounds to enable online CI construction.

Result: CLT for SGD iterates in growing dimension (t ≥ d^{1+δ}); computational benefits: O(td) time, O(d) memory; online variance estimator with deviation bounds; first fully online, data-driven CI framework for SGD in near-optimal scaling.

Conclusion: Presents the first fully online and data-driven confidence interval framework for SGD iterates in the near-optimal scaling regime, significantly extending dimensionality and efficiency over covariance-inversion based methods.

Abstract: Stochastic Gradient Descent (SGD) has become a cornerstone method in modern
data science. However, deploying SGD in high-stakes applications necessitates
rigorous quantification of its inherent uncertainty. In this work, we establish
\emph{non-asymptotic Berry--Esseen bounds} for linear functionals of online
least-squares SGD, thereby providing a Gaussian Central Limit Theorem (CLT) in
a \emph{growing-dimensional regime}. Existing approaches to high-dimensional
inference for projection parameters, such as~\cite{chang2023inference}, rely on
inverting empirical covariance matrices and require at least $t \gtrsim
d^{3/2}$ iterations to achieve finite-sample Berry--Esseen guarantees,
rendering them computationally expensive and restrictive in the allowable
dimensional scaling. In contrast, we show that a CLT holds for SGD iterates
when the number of iterations grows as $t \gtrsim d^{1+\delta}$ for any $\delta
> 0$, significantly extending the dimensional regime permitted by prior works
while improving computational efficiency. The proposed online SGD-based
procedure operates in $\mathcal{O}(td)$ time and requires only $\mathcal{O}(d)$
memory, in contrast to the $\mathcal{O}(td^2 + d^3)$ runtime of
covariance-inversion methods. To render the theory practically applicable, we
further develop an \emph{online variance estimator} for the asymptotic variance
appearing in the CLT and establish \emph{high-probability deviation bounds} for
this estimator. Collectively, these results yield the first fully online and
data-driven framework for constructing confidence intervals for SGD iterates in
the near-optimal scaling regime $t \gtrsim d^{1+\delta}$.

</details>


### [77] [BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models](https://arxiv.org/abs/2510.19749)
*Catherine Villeneuve,Benjamin Akera,Mélisande Teng,David Rolnick*

Main category: cs.LG

TL;DR: 建立在贝叶斯深度学习之上的SDM框架BATIS，通过在有限观测数据下迭代更新先验预测，结合解释性不确定性（aleatoric/epistemic）以提高数据匮乏区域的预测可靠性。并在一个包含eBird公民科学数据的新数据集上对不确定性量化方法进行基准评估。结果表明贝叶斯深度学习显著提升了数据稀缺情境下SDM的可靠性，利于生态理解与保护。


<details>
  <summary>Details</summary>
Motivation: 现有物种分布模型受空间偏倚和数据稀缺的制约，深度SDM在复杂与异质数据上表现较好但对不确定性和局部信息的整合能力不足。需要一个能够将局部细粒度洞察与全局生态模式结合，并合理量化并更新不确定性的框架。

Method: 提出BATIS框架：在初始先验基础上，使用有限观测数据迭代更新先验预测；强调同时捕获 aleatoric 与 epistemic 不确定性以实现局部与全局模式的融合。对一组不确定性量化方法进行基准测试，数据集包括来自eBird等公民科学观测数据的新增数据。

Result: 实验表明，贝叶斯深度学习方法在数据稀缺的位置显著提高SDM的可靠性，改善对环境变量与物种分布关系的不确定性估计，并促进对生态模式的理解及保护工作。

Conclusion: 将贝叶斯深度学习引入SDM并通过BATIS进行迭代更新，可在数据不足区域提升预测可靠性和不确定性处理能力，对生态学研究和保育决策具有实际意义。

Abstract: Species distribution models (SDMs), which aim to predict species occurrence
based on environmental variables, are widely used to monitor and respond to
biodiversity change. Recent deep learning advances for SDMs have been shown to
perform well on complex and heterogeneous datasets, but their effectiveness
remains limited by spatial biases in the data. In this paper, we revisit deep
SDMs from a Bayesian perspective and introduce BATIS, a novel and practical
framework wherein prior predictions are updated iteratively using limited
observational data. Models must appropriately capture both aleatoric and
epistemic uncertainty to effectively combine fine-grained local insights with
broader ecological patterns. We benchmark an extensive set of uncertainty
quantification approaches on a novel dataset including citizen science
observations from the eBird platform. Our empirical study shows how Bayesian
deep learning approaches can greatly improve the reliability of SDMs in
data-scarce locations, which can contribute to ecological understanding and
conservation efforts.

</details>


### [78] [When Do Transformers Learn Heuristics for Graph Connectivity?](https://arxiv.org/abs/2510.19753)
*Qilin Ye,Deqing Fu,Robin Jia,Vatsal Sharan*

Main category: cs.LG

TL;DR: 在图连通性任务中，L层解耦Transformer的容量等同于解决直径最多为3^L的图，训练数据的分布决定模型学习到的策略：在容量内可以学到正确的算法，在超出容量时会偏向度数启发式。


<details>
  <summary>Details</summary>
Motivation: 解释为什么Transformer难以学习可泛化的算法，并揭示模型容量与训练数据分布如何影响学习到的策略；以图的连通性作为测试床来界定算法学习的边界。

Method: 分析并证明简化的解耦Transformer在理论上具备解决直径为3^L的图的能力，等价于对邻接矩阵幂的计算；研究训练动态，考察在容量内外数据分布下模型学习到的策略差异；通过实证实验检验在容量受限的数据情形下两种Transformer是否能学习到真正的算法。

Result: 理论结果：L层模型的容量上界为直径3^L，可实现的算法等价于计算邻接矩阵幂。训练动态结果：容量内的图推动学习正确的算法，容量外的图推动学习基于节点度的启发式。实证结果：将训练数据限制在模型容量内时，标准和解耦Transformer都能学习到精确的算法，而非度数启发式。

Conclusion: 模型容量与训练数据分布共同决定是否学到普适算法。限定在容量内的训练数据有助于模型学习并泛化到该算法，而超出容量则易被度数等简单特征所支配，从而影响泛化能力。

Abstract: Transformers often fail to learn generalizable algorithms, instead relying on
brittle heuristics. Using graph connectivity as a testbed, we explain this
phenomenon both theoretically and empirically. We consider a simplified
Transformer architecture, the disentangled Transformer, and prove that an
$L$-layer model has capacity to solve for graphs with diameters up to exactly
$3^L$, implementing an algorithm equivalent to computing powers of the
adjacency matrix. We analyze the training-dynamics, and show that the learned
strategy hinges on whether most training instances are within this model
capacity. Within-capacity graphs (diameter $\leq 3^L$) drive the learning of a
correct algorithmic solution while beyond-capacity graphs drive the learning of
a simple heuristic based on node degrees. Finally, we empirically demonstrate
that restricting training data within a model's capacity leads to both standard
and disentangled transformers learning the exact algorithm rather than the
degree-based heuristic.

</details>


### [79] [CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees](https://arxiv.org/abs/2510.19754)
*Aman Bilkhoo,Milad Kazemi,Nicola Paoletti,Mehran Hosseini*

Main category: cs.LG

TL;DR: 提出 CONFEX，一种结合 Conformal Prediction 与 MILP 的不确定性感知逆向解释方法，提供局部覆盖保证和高效的 MILP 编码，通过离线树划分实现局部化 CP，以在预测不确定性与最优性之间提供形式保证。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗/逆向解释方法往往忽视预测不确定性，缺乏带有严格保证的机制，因此难以提供可靠且可操作的解释。

Method: 在局部区域内应用 Conformal Prediction 来给出置信覆盖，利用 MILP 对逆向解释进行全局优化，并通过离线树划分将输入空间划分成局部区域以提升计算效率；提出新的局部化 CP 流程，确保解释在不确定性下的可靠性，并给出严格的覆盖保证。

Result: 与现有方法在多个基准上比较，CONFEX 在不确定性感知方面更鲁棒，生成的解释更可信且具有更好的现实可操作性。

Conclusion: 提出了一种可在预测不确定性范围内提供形式化保证的逆向解释生成框架，结合 CP 与 MILP，实现局部覆盖与最优性之间的平衡，具备广泛适用性和稳健性。

Abstract: Counterfactual explanations (CFXs) provide human-understandable
justifications for model predictions, enabling actionable recourse and
enhancing interpretability. To be reliable, CFXs must avoid regions of high
predictive uncertainty, where explanations may be misleading or inapplicable.
However, existing methods often neglect uncertainty or lack principled
mechanisms for incorporating it with formal guarantees. We propose CONFEX, a
novel method for generating uncertainty-aware counterfactual explanations using
Conformal Prediction (CP) and Mixed-Integer Linear Programming (MILP). CONFEX
explanations are designed to provide local coverage guarantees, addressing the
issue that CFX generation violates exchangeability. To do so, we develop a
novel localised CP procedure that enjoys an efficient MILP encoding by
leveraging an offline tree-based partitioning of the input space. This way,
CONFEX generates CFXs with rigorous guarantees on both predictive uncertainty
and optimality. We evaluate CONFEX against state-of-the-art methods across
diverse benchmarks and metrics, demonstrating that our uncertainty-aware
approach yields robust and plausible explanations.

</details>


### [80] [GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters](https://arxiv.org/abs/2510.19778)
*Anand Choudhary,Yasser Sulaıman,Lukas Mauch,Ghouthi Boukli Hacene,Fabien Cardinaux,Antoine Bosselut*

Main category: cs.LG

TL;DR: GaLLoP 通过对梯度大小最大、且预训练参数大小最小的参数进行选择，仅对这些稀疏子集进行微调，以实现任务适应，同时尽量保护预训练知识。


<details>
  <summary>Details</summary>
Motivation: 稀疏微调的核心在于如何挑选要微调的参数。现有方法在选择上常导致对预训练知识的干扰或忘记式现象。提出一种在下游任务中优先调整对任务敏感但对原始知识干扰最小的参数的方法。

Method: 提出 GaLLoP（Gradient-based Sparse Learning on Low-Magnitude Parameters），在下游任务上计算参数的梯度幅值并结合参数的预训练大小，筛选梯度幅值最大且预训练大小最小的参数子集进行微调；仅对该稀疏子集进行微调。

Result: 在 LLaMA3 8B 与 Gemma 2B 基模型上，GaLLoP 在有监督和分布外任务上均优于或等价于 LoRA、DoRA、SAFT 等主流参数高效微调方法；同时显著缓解灾难性遗忘与对任务数据的记忆化，表现更稳定，对不同随机种子具有鲁棒性。

Conclusion: 通过聚焦对任务相关但对预训练知识干扰最小的参数，GaLLoP 提供了一种高效且稳健的稀疏微调策略，兼顾性能与知识保留。

Abstract: Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a
sparse subset of model parameters. However, the effectiveness of sparse
adaptation depends on optimally selecting the model parameters to be
fine-tuned. In this work, we introduce a novel sparse fine-tuning technique
named GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which
fine-tunes only those model parameters which have the largest gradient
magnitudes on downstream tasks and the smallest pre-trained magnitudes,
intuitively prioritizing parameters that are highly task-relevant, but
minimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3
8B and Gemma 2B as base models shows that GaLLoP consistently improves or
matches the in-distribution as well as out-of-distribution performance obtained
via the usage of other leading parameter-efficient fine-tuning techniques,
including LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates
catastrophic forgetting and memorization of task data, as important pre-trained
parameters remain unchanged, and stabilizes performance relative to other
fine-tuning techniques, robustly generalizing across most random seeds.

</details>


### [81] [Environment Inference for Learning Generalizable Dynamical System](https://arxiv.org/abs/2510.19784)
*Shixuan Liu,Yue He,Haotian Wang,Wenjing Yang,Yunfei Wang,Peng Cui,Zhong Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data-driven methods offer efficient and robust solutions for analyzing
complex dynamical systems but rely on the assumption of I.I.D. data, driving
the development of generalization techniques for handling environmental
differences. These techniques, however, are limited by their dependence on
environment labels, which are often unavailable during training due to data
acquisition challenges, privacy concerns, and environmental variability,
particularly in large public datasets and privacy-sensitive domains. In
response, we propose DynaInfer, a novel method that infers environment
specifications by analyzing prediction errors from fixed neural networks within
each training round, enabling environment assignments directly from data. We
prove our algorithm effectively solves the alternating optimization problem in
unlabeled scenarios and validate it through extensive experiments across
diverse dynamical systems. Results show that DynaInfer outperforms existing
environment assignment techniques, converges rapidly to true labels, and even
achieves superior performance when environment labels are available.

</details>


### [82] [Blackbox Model Provenance via Palimpsestic Membership Inference](https://arxiv.org/abs/2510.19796)
*Rohith Kuditipudi,Jing Huang,Sally Zhu,Diyi Yang,Christopher Potts,Percy Liang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Suppose Alice trains an open-weight language model and Bob uses a blackbox
derivative of Alice's model to produce text. Can Alice prove that Bob is using
her model, either by querying Bob's derivative model (query setting) or from
the text alone (observational setting)? We formulate this question as an
independence testing problem--in which the null hypothesis is that Bob's model
or text is independent of Alice's randomized training run--and investigate it
through the lens of palimpsestic memorization in language models: models are
more likely to memorize data seen later in training, so we can test whether Bob
is using Alice's model using test statistics that capture correlation between
Bob's model or text and the ordering of training examples in Alice's training
run. If Alice has randomly shuffled her training data, then any significant
correlation amounts to exactly quantifiable statistical evidence against the
null hypothesis, regardless of the composition of Alice's training data. In the
query setting, we directly estimate (via prompting) the likelihood Bob's model
gives to Alice's training examples and order; we correlate the likelihoods of
over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to
12B parameters with the base model's training data order, achieving a p-value
on the order of at most 1e-8 in all but six cases. In the observational
setting, we try two approaches based on estimating 1) the likelihood of Bob's
text overlapping with spans of Alice's training examples and 2) the likelihood
of Bob's text with respect to different versions of Alice's model we obtain by
repeating the last phase (e.g., 1%) of her training run on reshuffled data. The
second approach can reliably distinguish Bob's text from as little as a few
hundred tokens; the first does not involve any retraining but requires many
more tokens (several hundred thousand) to achieve high power.

</details>


### [83] [Transformers are almost optimal metalearners for linear classification](https://arxiv.org/abs/2510.19797)
*Roey Magen,Gal Vardi*

Main category: cs.LG

TL;DR: 在一个线性分类的高斯混合任务设置中，经过梯度下降训练的简化 Transformer 能作为近似最优的元学习器，利用少量的 in-context 示例就能泛化到新任务，且训练任务和任务样本量的上界与维度无关。


<details>
  <summary>Details</summary>
Motivation: 理解 Transformer 的 in-context learning 能否形成真正的元学习能力，即在相关任务集合上比单独解决每个任务更高效地学习；给出在线性 Gaussian 任务中的理论上界与分析，填补现有理论对正式元学习框架缺失的空白。

Method: 考虑一个简化的 Transformer 架构，由梯度下降训练，目标是在 class-conditional Gaussian 混合模型的任务上学习；每个任务的均值向量在 R^d 的一个共享的 k 维子空间中；在训练足够多的任务后，证明该 Transformer 能以只需 O(k / R^4) 个 in-context 示例就能在新任务上泛化；将该表现与一个已知共享子空间的最优学习者进行对比，并指出相比仅依赖 in-context 数据的学习者（需要 Ω(d / R^4) 的示例），Transformer 更有效；并给出训练任务数量和每个任务样本量的界限，且与 d 无关。

Result: 理论上证明了一个经过梯度下降训练的简化 Transformer 能作为近似最优元学习器，在给定的线性分类高斯混合任务中实现良好泛化；新任务的 in-context 泛化所需示例数为 O(k / R^4)，显著优于仅依赖 in-context 数据的基线 Ω(d / R^4)；且这些界限对环境维度 d 的影响可忽略，需的训练任务数与每任务的样本量上界与 d 无关。

Conclusion: 该工作首次在一个线性元学习框架中证明，经过训练的 Transformer 架构可以近似元学习者，且其样本复杂度对维度无关，揭示了 ICL 的理论潜力及其在参数高效学习中的作用，尽管分析限定在特定任务族和线性情形。

Abstract: Transformers have demonstrated impressive in-context learning (ICL)
capabilities, raising the question of whether they can serve as metalearners
that adapt to new tasks using only a small number of in-context examples,
without any further training. While recent theoretical work has studied
transformers' ability to perform ICL, most of these analyses do not address the
formal metalearning setting, where the objective is to solve a collection of
related tasks more efficiently than would be possible by solving each task
individually. In this paper, we provide the first theoretical analysis showing
that a simplified transformer architecture trained via gradient descent can act
as a near-optimal metalearner in a linear classification setting. We consider a
natural family of tasks where each task corresponds to a class-conditional
Gaussian mixture model, with the mean vectors lying in a shared $k$-dimensional
subspace of $R^d$. After training on a sufficient number of such tasks, we show
that the transformer can generalize to a new task using only $O(k / R^4)$
in-context examples, where $R$ denotes the signal strength at test time. This
performance (almost) matches that of an optimal learner that knows exactly the
shared subspace and significantly outperforms any learner that only has access
to the in-context data, which requires $\Omega(d / R^4)$ examples to
generalize. Importantly, our bounds on the number of training tasks and
examples per task needed to achieve this result are independent of the ambient
dimension $d$.

</details>


### [84] [The Feasibility of Training Sovereign Language Models in the Global South: A Study of Brazil and Mexico](https://arxiv.org/abs/2510.19801)
*Sandra Malagon,Monica A. Ulloa Ruiz,Tatiana Elizabeth Sandoval Plaza,Gabriel Rafael Rosario Bolívar,Valentina García Mesa,Ivanna Alvarado Morales*

Main category: cs.LG

TL;DR: Sovereign-scale training feasibility for Brazil and Mexico under hardware, energy, and fiscal constraints.


<details>
  <summary>Details</summary>
Motivation: Assess whether middle-income countries can develop auditable, locally aligned LLMs without competing at the global frontier.

Method: Dual-axis experimental design varying accelerator generation (NVIDIA H100 vs A100) and training duration (90 vs 150 days) to model a 10-trillion-token LLM; estimate compute, energy, capex, regulatory compatibility.

Result: H100-based configurations are financially feasible (8–14 million USD); A100 configurations require 19–32 million USD; energy efficiency and hardware costs dominate; extending training duration can mitigate hardware constraints.

Conclusion: Context-sensitive governance strategies can support sustainable, sovereign AI capabilities in middle-income countries without full frontier-scale investments.

Abstract: The rapid escalation of computational requirements for training large-scale
language models has reinforced structural asymmetries between high-capacity
jurisdictions and countries in the Global South. This paper examines the
technical and fiscal feasibility of sovereign-scale language model training in
Brazil and Mexico under conditions of constrained hardware access, energy
availability, and fiscal ceilings. Using a dual-axis design that varies
accelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150
days), we estimate compute demand, energy consumption, capital expenditures,
and regulatory compatibility for the training of a 10-trillion-token model. Our
findings show that while all configurations remain below export-control and
electrical infrastructure thresholds, fiscal viability is determined by
hardware efficiency. H100-based scenarios achieve training feasibility at a
total cost of 8-14 million USD, while A100 deployments require 19-32 million
USD due to higher energy and hardware demand. We argue that extending training
timelines should be treated as a policy lever to mitigate hardware constraints,
enabling the production of usable, auditable, and locally aligned models
without competing at the global frontier. This study contributes to the
discourse on AI compute governance and technological sovereignty by
highlighting context-sensitive strategies that allow middle-income countries to
establish sustainable and strategically sufficient AI capabilities.

</details>


### [85] [Semantic World Models](https://arxiv.org/abs/2510.19818)
*Jacob Berg,Chuning Zhu,Yanda Bao,Ishan Durugkar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 将世界模型从逐帧像素重建转为对未来的语义信息预测，通过将世界建模视为对未来帧的视觉问答（QA）来实现，并以视觉语言模型为基础进行微调，以实现更好的规划和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 像素级未来重建目标往往与实际规划目标不对齐，强像素重建并不必然带来良好的规划决策。通过预测与任务相关的语义信息，可以直接提升决策和鲁棒性，且可利用预训练的视觉-语言模型的泛化能力。

Method: 将世界建模问题转化为对未来帧中的语义信息进行视觉问答的形式；以图像-动作-文本数据对视觉语言模型进行有监督微调，使其成为语义世界模型；利用该模型在决策与规划中提升策略。

Result: 在开放式机器人任务上实现了策略改进，相较于基于像素重建的行动条件世界模型，展现出显著的泛化提升，并受益于预训练视觉语言模型的鲁棒性与泛化属性。

Conclusion: 语义化的世界建模—通过基于视觉语言模型的QA式预测—更契合规划目标，具有更强的泛化和鲁棒性，是未来机器人学习与规划的有希望方向。

Abstract: Planning with world models offers a powerful paradigm for robotic control.
Conventional approaches train a model to predict future frames conditioned on
current frames and actions, which can then be used for planning. However, the
objective of predicting future pixels is often at odds with the actual planning
objective; strong pixel reconstruction does not always correlate with good
planning decisions. This paper posits that instead of reconstructing future
frames as pixels, world models only need to predict task-relevant semantic
information about the future. For such prediction the paper poses world
modeling as a visual question answering problem about semantic information in
future frames. This perspective allows world modeling to be approached with the
same tools underlying vision language models. Thus vision language models can
be trained as "semantic" world models through a supervised finetuning process
on image-action-text data, enabling planning for decision-making while
inheriting many of the generalization and robustness properties from the
pretrained vision-language models. The paper demonstrates how such a semantic
world model can be used for policy improvement on open-ended robotics tasks,
leading to significant generalization improvements over typical paradigms of
reconstruction-based action-conditional world modeling. Website available at
https://weirdlabuw.github.io/swm.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [86] [Fusion of Machine Learning and Blockchain-based Privacy-Preserving Approach for Health Care Data in the Internet of Things](https://arxiv.org/abs/2510.19026)
*Behnam Rezaei Bezanjani,Seyyed Hamid Ghafouri,Reza Gholamrezaei*

Main category: cs.CR

TL;DR: Three-phase IoT healthcare security framework leveragingBlockchain-Enabled Request/Transaction Encryption, Request Pattern Recognition, and BiLSTM-based intrusion detection with feature selection; outperforms three recent baselines in simulations.


<details>
  <summary>Details</summary>
Motivation: To address the rising security and privacy risks in IoT-enabled healthcare systems amid widespread deployment, ensuring confidentiality, integrity, and availability of medical data.

Method: Phase 1: Blockchain-Enabled Request and Transaction Encryption to provide immutable, transparent data transactions. Phase 2: Request Pattern Recognition Check using diverse data sources to detect and block unauthorized access. Phase 3: Feature Selection and a BiLSTM network to enhance intrusion detection accuracy and efficiency. Comparative evaluation against AIBPSF-IoMT, OMLIDS-PBIoT, and AIMMFIDS using metrics such as detection rate, false alarm rate, precision, recall, and accuracy.

Result: Simulation results show that the proposed three-phase method outperforms the three reference methods across all evaluated metrics.

Conclusion: The proposed framework effectively enhances security for IoT-based healthcare systems and demonstrates strong potential for practical deployment; further work could address real-world deployment challenges and scalability.

Abstract: In recent years, the rapid integration of Internet of Things (IoT) devices
into the healthcare sector has brought about revolutionary advancements in
patient care and data management. While these technological innovations hold
immense promise, they concurrently raise critical security concerns,
particularly in safeguarding medical data against potential cyber threats. The
sensitive nature of health-related information requires robust measures to
ensure the confidentiality, integrity, and availability of patient data in
IoT-enabled medical environments. Addressing the imperative need for enhanced
security in IoT-based healthcare systems, we propose a comprehensive method
encompassing three distinct phases. In the first phase, we implement
Blockchain-Enabled Request and Transaction Encryption to strengthen data
transaction security, providing an immutable and transparent framework. In the
second phase, we introduce a Request Pattern Recognition Check that leverages
diverse data sources to identify and block potential unauthorized access
attempts. Finally, the third phase incorporates Feature Selection and a BiLSTM
network to enhance the accuracy and efficiency of intrusion detection using
advanced machine learning techniques. We compared the simulation results of the
proposed method with three recent related methods: AIBPSF-IoMT, OMLIDS-PBIoT,
and AIMMFIDS. The evaluation criteria include detection rate, false alarm rate,
precision, recall, and accuracy - crucial benchmarks for assessing the overall
performance of intrusion detection systems. Our findings show that the proposed
method outperforms existing approaches across all evaluated criteria,
demonstrating its effectiveness in improving the security of IoT-based
healthcare systems.

</details>


### [87] [OpenGuardrails: An Open-Source Context-Aware AI Guardrails Platform](https://arxiv.org/abs/2510.19169)
*Thomas Wang,Haowen Li*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, safeguarding them against unsafe, malicious, or
privacy-violating content is critically important. We present OpenGuardrails,
the first open-source project to provide both a context-aware safety and
manipulation detection model and a deployable platform for comprehensive AI
guardrails. OpenGuardrails protects against content-safety risks,
model-manipulation attacks (e.g., prompt injection, jailbreaking,
code-interpreter abuse, and the generation/execution of malicious code), and
data leakage. Content-safety and model-manipulation detection are implemented
by a unified large model, while data-leakage identification and redaction are
performed by a separate lightweight NER pipeline (e.g., Presidio-style models
or regex-based detectors). The system can be deployed as a security gateway or
an API-based service, with enterprise-grade, fully private deployment options.
OpenGuardrails achieves state-of-the-art (SOTA) performance on safety
benchmarks, excelling in both prompt and response classification across
English, Chinese, and multilingual tasks. All models are released under the
Apache 2.0 license for public use.

</details>


### [88] [LAPRAD: LLM-Assisted PRotocol Attack Discovery](https://arxiv.org/abs/2510.19264)
*R. Can Aygun,Yehuda Afek,Anat Bremler-Barr,Leonard Kleinrock*

Main category: cs.CR

TL;DR: 提出了一种 LLM 辅助的协议攻击发现方法（LAPRAD），以提升对 DNS 等协议漏洞的发现效率。通过三阶段流程：阶段1 借助在 DNS 与历史 DDoS 数据上训练的大语言模型识别潜在攻击点；阶段2 使用 ReACT/LangChain 自动生成攻击配置（DNS 区域文件生成）；阶段3 验证攻击的功能与效果。结果显示发现三种新型 DNS DDoS 攻击并复现两起近期报道的攻击，涉及 SigCacheFlush、使用 RSA-4096 的多密钥绕过默认 RRSet 限制、以及 ANY 类型响应等变体，均可显著降低解析器处理能力。


<details>
  <summary>Details</summary>
Motivation: 提升互联网协议安全研究的效率，降低漏洞发现的门槛，结合大语言模型与自动化配置生成来推进对 DNS、BGP 等协议的安全分析。

Method: 三阶段流程：1) 以在 DNS 资料与历史 DDoS 攻击上训练的 LLM 初步识别潜在利用点；2) 通过 ReACT/LangChain 自动构建相应的攻击配置，包含 DNS 区域文件生成等；3) 对攻击的功能性与有效性进行验证。

Result: 使用 LAPRAD 发现三种新颖的 DNS DDoS 攻击，并复现两起近期报道的攻击未包含在 LLM 的训练数据中。第一种为诱骗缓存大量伪 DNSSEC RRSIG 的 bait-and-switch 变体，使解析容量降至约 6%；第二种利用 RSA-4096 加密算法及多密钥绕过近期实现的默认 RRSet 限制；第三种通过 ANY 类型响应实现类似效果。这些变体构成的 SigCacheFlush 缓存刷新型 DDoS 攻击，绕过现有修补，严重削弱解析器的查询能力，影响主流解析器实现。

Conclusion: LAPRAD 展现出半自动化漏洞发现的潜力，能在较短时间内在 DNS 领域发现和复现攻击。但其双用途性质需谨慎管理，须加强伦理约束、评估现有修补的稳健性，以及扩展至其他协议的可行性与风险控制。

Abstract: With the goal of improving the security of Internet protocols, we seek
faster, semi-automatic methods to discover new vulnerabilities in protocols
such as DNS, BGP, and others. To this end, we introduce the LLM-Assisted
Protocol Attack Discovery (LAPRAD) methodology, enabling security researchers
with some DNS knowledge to efficiently uncover vulnerabilities that would
otherwise be hard to detect.
  LAPRAD follows a three-stage process. In the first, we consult an LLM
(GPT-o1) that has been trained on a broad corpus of DNS-related sources and
previous DDoS attacks to identify potential exploits. In the second stage, a
different LLM automatically constructs the corresponding attack configurations
using the ReACT approach implemented via LangChain (DNS zone file generation).
Finally, in the third stage, we validate the attack's functionality and
effectiveness.
  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and
rediscovered two recently reported ones that were not included in the LLM's
training data. The first new attack employs a bait-and-switch technique to
trick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving
capacity to as little as 6%. The second exploits large DNSSEC encryption
algorithms (RSA-4096) with multiple keys, thereby bypassing a recently
implemented default RRSet limit. The third leverages ANY-type responses to
produce a similar effect.
  These variations of a cache-flushing DDoS attack, called SigCacheFlush,
circumvent existing patches, severely degrade resolver query capacity, and
impact the latest versions of major DNS resolver implementations.

</details>


### [89] [Reliability and Resilience of AI-Driven Critical Network Infrastructure under Cyber-Physical Threats](https://arxiv.org/abs/2510.19295)
*Konstantinos A. Lizos,Leandros Maglaras,Elena Petrovik,Saied M. Abd El-atty,Georgios Tsachtsiris,Mohamed Amine Ferrag*

Main category: cs.CR

TL;DR: A fault-tolerant, resilience-aware framework for AI-driven 5G/6G networks that combines anomaly detection, adaptive routing, and redundancy to mitigate cyber-physical attacks; validated with NS-3 simulations showing improved reliability, latency, resilience, and reduced packet loss.


<details>
  <summary>Details</summary>
Motivation: Growing dependence on AI-driven 5G/6G networks for mission-critical services and increasing cyber-physical threats, which cause cascading failures in distributed, virtualized, cross-domain networks; need for robust fault tolerance and resilience.

Method: Propose an integrated framework combining AI-based anomaly detection, adaptive routing, and redundancy mechanisms; validate via NS-3 simulations across various attack scenarios; compare performance against baseline approaches using KPIs like reliability, latency, resilience index, and packet loss.

Result: The framework enhances fault recovery, stabilizes packet delivery, and reduces service disruption relative to baselines, as evidenced by improved KPIs under attack scenarios.

Conclusion: Integrated AI anomaly detection, adaptive routing, and redundancy can significantly improve resilience and fault tolerance in cyber-physical AI-driven networks, mitigating cascading failures during cyber-physical attacks.

Abstract: The increasing reliance on AI-driven 5G/6G network infrastructures for
mission-critical services highlights the need for reliability and resilience
against sophisticated cyber-physical threats. These networks are highly exposed
to novel attack surfaces due to their distributed intelligence, virtualized
resources, and cross-domain integration. This paper proposes a fault-tolerant
and resilience-aware framework that integrates AI-driven anomaly detection,
adaptive routing, and redundancy mechanisms to mitigate cascading failures
under cyber-physical attack conditions. A comprehensive validation is carried
out using NS-3 simulations, where key performance indicators such as
reliability, latency, resilience index, and packet loss rate are analyzed under
various attack scenarios. The deduced results demonstrate that the proposed
framework significantly improves fault recovery, stabilizes packet delivery,
and reduces service disruption compared to baseline approaches.

</details>


### [90] [A Probabilistic Computing Approach to the Closest Vector Problem for Lattice-Based Factoring](https://arxiv.org/abs/2510.19390)
*Max O. Al-Hasso,Marko von der Leyen*

Main category: cs.CR

TL;DR: 提出将概率计算用于CVP近似 refinements 在格基因 factoring 中的应用；主张线性时间的 CVP refinement，并能在给定格参数下使半素数分解所需的格实例数比现有量子/经典方法少至 100 倍。


<details>
  <summary>Details</summary>
Motivation: CVP 的最近向量问题是基于格的密码系统安全性的核心，且 Schnorr 的格基因分解算法将整数分解归约到 CVP；近年来在格基 factoring 中引入 CVP 近似的启发式 refinement，并尝试用量子变分算法实现该优化；同时 probabilistic computing 作为对随机化算法的硬件加速，成为新兴方向。本文旨在将 probabilistic computing 应用于 CVP 近似 refinement，以提高 factoring 的效率。

Method: 设计一种用于 CVP 近似 refinement 的 probabilistic 计算算法，讨论“质数格”（prime lattice）参数，并通过实验评估该方法在求解 CVP 以及作为格基 factoring 子过程中的有效性。与现有量子与经典方法进行对比。

Result: 结果显示： (a) 该方法在问题规模线性时间内即可找到最大可用的 CVP 近似 refinement；(b) 在给定的格参数下，利用 probabilistic computing 进行格基 factoring 时，能够以高达 100 倍的减少格实例数量完成半素数分解，相比于同类方法。

Conclusion: 证明了 probabilistic computing 作为 CVP refinement 的有效性，以及作为格基 factoring 的子程序时的潜在加速作用；对密码系统的安全性有潜在影响，需进一步在鲁棒性、误差控制、可重复性及广泛适用性方面进行研究。

Abstract: The closest vector problem (CVP) is a fundamental optimization problem in
lattice-based cryptography and its conjectured hardness underpins the security
of lattice-based cryptosystems. Furthermore, Schnorr's lattice-based factoring
algorithm reduces integer factoring (the foundation of current cryptosystems,
including RSA) to the CVP. Recent work has investigated the inclusion of a
heuristic CVP approximation `refinement' step in the lattice-based factoring
algorithm, using quantum variational algorithms to perform the heuristic
optimization. This coincides with the emergence of probabilistic computing as a
hardware accelerator for randomized algorithms including tasks in combinatorial
optimization. In this work we investigate the application of probabilistic
computing to the heuristic optimization task of CVP approximation refinement in
lattice-based factoring. We present the design of a probabilistic computing
algorithm for this task, a discussion of `prime lattice' parameters, and
experimental results showing the efficacy of probabilistic computing for
solving the CVP as well as its efficacy as a subroutine for lattice-based
factoring. The main results found that (a) this approach is capable of finding
the maximal available CVP approximation refinement in time linear in problem
size and (b) probabilistic computing used in conjunction with the lattice
parameters presented can find the composite prime factors of a semiprime number
using up to 100x fewer lattice instances than similar quantum and classical
methods.

</details>


### [91] [From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data](https://arxiv.org/abs/2510.19418)
*Mete Harun Akcay,Buse Gul Atli,Siddharth Prakash Rao,Alexandros Bakas*

Main category: cs.CR

TL;DR: 提出一个面向策略驱动访问控制的可信数据共享架构，通过自动检测敏感区域、后处理、密钥管理和访问控制实现对敏感区域的选择性保护，兼具可扩展性与效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据量增长，跨多用户/角色的数据共享环境中保护敏感信息成为挑战，需在确保可扩展性的同时实现细粒度的访问控制。

Method: 提出四大模块：敏感区域自动检测、后处理、密钥管理、访问控制；采用对称加密提升效率，并结合属性基加密（ABE）实现策略执行；实现高效的密钥分发与密钥存储隔离；在视觉数据集上评估，包含自动检测、再评估与选择性加密。

Result: 实验表明系统实现了有效的PSO检测，宏F1提升5%，平均精度提升10%，且策略性解密平均耗时<1秒/图像，体现出系统的有效性、效率和可扩展性。

Conclusion: 该体系为大规模数据共享场景提供了细粒度访问控制的可行解决方案，兼具安全性、效率与扩展性，具有实际应用潜力。

Abstract: As the volume of stored data continues to grow, identifying and protecting
sensitive information within large repositories becomes increasingly
challenging, especially when shared with multiple users with different roles
and permissions. This work presents a system architecture for trusted data
sharing with policy-driven access control, enabling selective protection of
sensitive regions while maintaining scalability. The proposed architecture
integrates four core modules that combine automated detection of sensitive
regions, post-correction, key management, and access control. Sensitive regions
are secured using a hybrid scheme that employs symmetric encryption for
efficiency and Attribute-Based Encryption for policy enforcement. The system
supports efficient key distribution and isolates key storage to strengthen
overall security. To demonstrate its applicability, we evaluate the system on
visual datasets, where Privacy-Sensitive Objects in images are automatically
detected, reassessed, and selectively encrypted prior to sharing in a data
repository. Experimental results show that our system provides effective PSO
detection, increases macro-averaged F1 score (5%) and mean Average Precision
(10%), and maintains an average policy-enforced decryption time of less than 1
second per image. These results demonstrate the effectiveness, efficiency and
scalability of our proposed solution for fine-grained access control.

</details>


### [92] [Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains](https://arxiv.org/abs/2510.19491)
*Jonas Gebele,Timm Mutzel,Burak Oez,Florian Matthes*

Main category: cs.CR

TL;DR: 将TEE支持的机密计算与公开区块链结算结合的密封竞拍协议，兼顾隐私、可验证性与可扩展性，避免信任中介。


<details>
  <summary>Details</summary>
Motivation: 解决在公链环境中实现密封竞拍的隐私性、可验证性与可扩展性之间的矛盾，避免多轮协议或昂贵密码学的同时，且不依赖受信任的中介。

Method: 在TEE背书的机密计算区块链上执行敏感竞拍逻辑，拍卖资金锁定于 enclave 生成的托管地址；到截止后任意方触发分辨，机密区块链通过离线可验证计算确定赢家并对公链签署结算交易，完成在公链上的执行。

Result: 证明了在不依赖可信第三方或协议改动的情况下实现安全、隐私与可扩展性；在 SUAVE 上实现并以以太坊进行结算，评估可扩展性与信任前提，并在现有基础设施上实现最小集成。

Conclusion: 该设计在隐私、可验证性和可扩展性之间提供了折中方案，适用于在公共区块链环境中进行密封竞拍，具备落地潜力。

Abstract: Sealed-bid auctions ensure fair competition and efficient allocation but are
often deployed on centralized infrastructure, enabling opaque manipulation.
Public blockchains eliminate central control, yet their inherent transparency
conflicts with the confidentiality required for sealed bidding. Prior attempts
struggle to reconcile privacy, verifiability, and scalability without relying
on trusted intermediaries, multi-round protocols, or expensive cryptography. We
present a sealed-bid auction protocol that executes sensitive bidding logic on
a Trusted Execution Environment (TEE)-backed confidential compute blockchain
while retaining settlement and enforcement on a public chain. Bidders commit
funds to enclave-generated escrow addresses, ensuring confidentiality and
binding commitments. After the deadline, any party can trigger resolution: the
confidential blockchain determines the winner through verifiable off-chain
computation and issues signed settlement transactions for execution on the
public chain. Our design provides security, privacy, and scalability without
trusted third parties or protocol modifications. We implement it on SUAVE with
Ethereum settlement, evaluate its scalability and trust assumptions, and
demonstrate deployment with minimal integration on existing infrastructure

</details>


### [93] [CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against IP Leakage](https://arxiv.org/abs/2510.19676)
*Nowfel Mashnoor,Mohammad Akyash,Hadi Kamali,Kimia Azar*

Main category: cs.CR

TL;DR: CircuitGuard 提供一个面向 RTL 代码生成的记忆防护框架，通过 RTL 感知的相似性度量和激活层级引导，显著降低对专有模式的语义相似性，同时保持生成质量，并具备跨领域迁移能力。


<details>
  <summary>Details</summary>
Motivation: LLMs 在 RTL 代码生成中容易记住或泄露专有/安全敏感的硬件设计。RTL 的等效实现可在结构上不同但功能相同，导致传统去重不足，且小的语法变动也可能改变电路正确性，因此需要专门的防护机制。

Method: 提出 RTL-aware 的相似性度量，结合结构性与功能性等效性，并开发激活级别引导方法，定位并抑制对记忆影响最大的变换器组件。实验在 Llama 3.1-8B 的 18-28 层中识别出275个记忆关键特征；评估显示显著降低对专有模式的语义相似性，同时保留生成质量，并实现跨领域的 78-85% 迁移效果。

Result: 在不重新训练的前提下，CircuitGuard 能实现对记忆特征的识别与隔离，达到最高约 80% 的语义相似性降低，以及 78-85% 的跨域迁移效果，覆盖不同电路类别的记忆防护需求。

Conclusion: RTL 感知防护是可行且具有普适性的路线，CircuitGuard 可在保持正确性的前提下显著降低记忆泄露风险，并具备跨域适应性，适用于多种电路实现。

Abstract: Large Language Models (LLMs) have achieved remarkable success in generative
tasks, including register-transfer level (RTL) hardware synthesis. However,
their tendency to memorize training data poses critical risks when proprietary
or security-sensitive designs are unintentionally exposed during inference.
While prior work has examined memorization in natural language, RTL introduces
unique challenges: In RTL, structurally different implementations (e.g.,
behavioral vs. gate-level descriptions) can realize the same hardware, leading
to intellectual property (IP) leakage (full or partial) even without verbatim
overlap. Conversely, even small syntactic variations (e.g., operator precedence
or blocking vs. non-blocking assignments) can drastically alter circuit
behavior, making correctness preservation especially challenging. In this work,
we systematically study memorization in RTL code generation and propose
CircuitGuard, a defense strategy that balances leakage reduction with
correctness preservation. CircuitGuard (1) introduces a novel RTL-aware
similarity metric that captures both structural and functional equivalence
beyond surface-level overlap, and (2) develops an activation-level steering
method that identifies and attenuates transformer components most responsible
for memorization. Our empirical evaluation demonstrates that CircuitGuard
identifies (and isolates) 275 memorization-critical features across layers
18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic
similarity to proprietary patterns while maintaining generation quality.
CircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling
robust memorization mitigation across circuit categories without retraining.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [94] [Active Cooling Device: A Flexible, Lab-Scale Experimental Unit to Develop Spatio-Temporal Temperature Control Strategies](https://arxiv.org/abs/2510.18987)
*Victor Oliveira Ferreira,Wiebke Mainville,Vincent Raymond,Jean-Michel Lamarre,Antoine Hamel,Mikael Vaillant,Moncef Chioua,Bruno Blais*

Main category: eess.SY

TL;DR: 提出并实现一个可重构的多输入多输出热管理单元，利用可控喷射冷却对表面进行时空温度分布控制，提供CAD/PCB/G UI等完整开放资源用于系统表征与控制策略评估。


<details>
  <summary>Details</summary>
Motivation: 解决实验室尺度对表面温度场的精确时空控制需求，提供一个可重复、可扩展的热管理平台，支持对新型温控策略的评估与对比。

Method: 通过一个可重构的通道流体喷射网格（manifold），通过改变各通道在输入/输出/关闭之间的角色来实现流体方向控制；配套提供STEP CAD文件、PCB Gerber、Python GUI，并给出组装步骤、实时温度与流量跟踪的GUI接口；并通过脉冲响应、PID跟踪以及对扰动的耦合系统鲁棒性等例子对系统进行表征。

Result: 证明设计具备安全、灵活、完整的活跃冷却能力，能在实验室规模评估自定义温控策略的性能，并提供用于扩展应用的开放资源。

Conclusion: 该工作提供了一个可重复实现的实验平台，用于研究围绕喷涌式封闭腔体的温度控制，且资源开放以促进方法的比较与扩展。

Abstract: We present an experimental unit that realizes the ``multi-input, multi-output
manifold'' thermal management technology proposed by Lamarre & Raymond (2023).
The proposed setup can be used for experiments aimed at controlling
spatiotemporal temperature distribution. Temperature control is achieved by
impinging coolant fluid jets, leveraging a manifold of channels targeted to the
surface. The direction of the fluid is controlled by shifting the role of
channels between inputs, outputs, or closing them. Files associated with this
work include Computer-Aided Design (CAD) STEP files, Gerber files to
manufacture a Printed Circuit Board (PCB), and a Graphical User Interface (GUI)
written in Python. We provide a step-by-step guide to assemble the experimental
setup. We also provide instructions to interact with the setup through the GUI,
which allows for real-time tracking of sample temperature and flow rates per
flow control device. Additionally, we provide examples of usage of the setup,
including system characterization with step response,
Proportional-Integral-Derivative performance tracking, and disturbance
rejection in a coupled system. Extending the application is accessible through
the files provided in the open repository associated with this work. The active
cooling device presents a safe, flexible, and complete design, allowing for
lab-scale assessment of the performance of custom temperature control
strategies using enclosed impinging jets.

</details>


### [95] [Extreme value distributions of peak loads for non-residential customer segments](https://arxiv.org/abs/2510.19052)
*Shaohong Shi,Eric A. Cator,Jacco Heres,Simon H. Tindemans*

Main category: eess.SY

TL;DR: 将极端值理论用于峰负荷分布，得到四参数 Fréchet 重尾的分布模型；通过多分位回归实现分位 Velander's formula，且预测性能保持不变。


<details>
  <summary>Details</summary>
Motivation: 解决非时序峰负荷预测的简洁性与对大客户峰值准确性的需求，并为 Velander的公式提供统计分布基础。

Method: 基于极值理论构建四参数分布，使用最大似然估计和似然比检验；结合多分位回归实现分位 VF；将模型表示简化为四个参数。

Result: 证明分析对象的峰负荷分布属于Fréchet重尾；四参数化的分位VF在预测上与原方法等价或更优。

Conclusion: 提供一个统计解释的、简化的峰负荷预测框架，便于对极端峰值进行概率估计。

Abstract: Electrical grid congestion is a growing challenge in Europe, driving the need
for accurate prediction of load, particularly of peak load. Non-time-resolved
models of peak load offer the advantages of simplicity and compactness, and
among them, Velander's formula (VF) is a traditional method that has been used
for decades. Moreover, VF can be adapted into a quantile VF, which learns a
truncated cumulative distribution function of peak load based on electricity
consumption. This paper proposes a mathematical model based on extreme value
theory to characterize the probability distribution of peak load for large
non-residential customers. The model underpins the quantile VF as demonstrated
through multiple quantile regression and reduces its representation to just
four parameters without sacrificing predictive performance. Moreover, using
maximum likelihood estimation and the likelihood ratio test, we validate that
the probability distribution of peak load of analysed groups belongs to the
heavy-tailed Fr\'echet class.

</details>


### [96] [Managing Charging Induced Grid Stress and Battery Degradation in Electric Taxi Fleets](https://arxiv.org/abs/2510.19293)
*Michael Yuhas,Rajesh K. Ahir,Laksamana Vixell Tanjaya Hartono,Muhammad Dzaki Dwi Putranto,Arvind Easwaran,Suhono Harso Supangkat*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Operating fleets of electric vehicles (EVs) introduces several challenges,
some of which are borne by the fleet operator, and some of which are borne by
the power grid. To maximize short-term profit a fleet operator could always
charge EVs at the maximum rate to ensure vehicles are ready to service ride
demand. However, due to the stochastic nature of electricity demand, charging
EVs at their maximum rate may potentially increase the grid stress and lead to
overall instability. Furthermore, high-rate charging of EVs can accelerate
battery degradation, thereby reducing the service lifespan of the fleet. This
study aims to reconcile the conflicting incentives of fleet longevity,
short-term profitability, and grid stability by simulating a taxi fleet
throughout its lifespan in relation to its charging policies and service
conditions. We develop an EV fleet simulator to evaluate the battery
degradation due to unpredictable charging and ride demand. Consequently, the
impact on the power grid through the charging infrastructure is assessed due to
these activities. This simulation utilizes publicly accessible real-world
travel data from the NYC taxi dataset. We compare a baseline 80-20 fleet
charging policy with a reinforcement learning-based policy designed to prolong
the fleet's service life and alleviate grid stress. We monitor grid stress,
battery degradation, and profitability over five years and find that our
learned policy outperforms the baseline. This simulator enables fleet operators
to assess the impact of different charging policies on these indicators to make
informed decisions in the future.

</details>


### [97] [Multi-UAV Flood Monitoring via CVT with Gaussian Mixture of Density Functions for Coverage Control](https://arxiv.org/abs/2510.19548)
*Jie Song,Yang Bai,Mikhail Svinin,Naoki Wakamiya*

Main category: eess.SY

TL;DR: GMDF-based density modeling within a CVT framework improves UAV coverage for unknown flood monitoring compared to axis-aligned Gaussian models.


<details>
  <summary>Details</summary>
Motivation: Unknown flood regions require accurate estimation of inundation extent and efficient coverage by multiple UAVs; a density-driven CVT approach can guide UAV distribution to improve monitoring.

Method: Employ Centroidal Voronoi Tessellation (CVT) with a density function modeled by Gaussian Mixture of Density Functions (GMDF). Compare GMDF to conventional axis-aligned Gaussian models. Evaluate performance across UAV fleets of 16, 20, and 24 in ROS/Gazebo with multiple simulation trials.

Result: GMDF-based formulation consistently achieves higher coverage rates and better UAV spatial distribution than the axis-aligned Gaussian model across tested fleet sizes in simulation.

Conclusion: GMDF provides a more accurate density representation of inundated areas and enhances flood monitoring performance when used within a CVT-based coverage framework for multi-UAV systems.

Abstract: This study presents a control strategy for coordinating multiple unmanned
aerial vehicles (UAVs) to monitor unknown flood regions and estimate the extent
of inundation. The proposed method adopts a density-driven coverage framework
based on Centroidal Voronoi Tessellation (CVT), in which the density function
is modeled using a Gaussian Mixture of Density Functions (GMDF). This
formulation provides a more accurate characterization of inundated areas
compared to conventional axis-aligned Gaussian models. The performance of the
two density modeling approaches is systematically evaluated under different UAV
fleet sizes (16, 20, and 24), with multiple simulation trials conducted in the
ROS/Gazebo environment. The results show that the GMDF-based formulation
consistently achieves higher coverage rates, demonstrating its effectiveness in
enhancing flood monitoring and improving UAV spatial distribution.

</details>


### [98] [Control Barrier Functions for the Full Class of Signal Temporal Logic Tasks using Spatiotemporal Tubes](https://arxiv.org/abs/2510.19595)
*Ratnangshu Das,Subhodeep Choudhury,Pushpak Jagtap*

Main category: eess.SY

TL;DR: 提出一个基于时空管道（STT）的TV-CBF框架，通过鲁棒优化（ROP）与情景优化（SOP）求解STT，从而构造对通用信号时序逻辑（STL）规格可覆盖且在任意控制律下保持不变量的时间变控制屏障函数，并在差动驱动移动机器人和四旋翼上的案例中展示效率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决对通用STL任务的鲁棒满足问题；将时空管道（STT）与时间变控制屏障函数（TV-CBF）相结合，以提供可验证且高效的STL执行框架，降低对系统不确定性和对控制律依赖的敏感性。

Method: 将STT合成问题建模为鲁棒优化问题（ROP），通过情景优化（SOP）求解以获得覆盖给定STL规格的管道；再基于所得到的STTs构造TV-CBF，确保在任意使TV-CBF不变的控制律下系统能够满足STL任务；通过对差分驱动移动机器人和四旋翼的案例研究进行对比分析以验证框架的有效性与效率。

Result: 给出形式化保证：所构造的STTs能够捕捉并强制满足给定的STL规格；所构造的TV-CBF在不变性下能确保STL任务的实现；实验结果显示该框架在计算效率与扩展性方面优于现有方法。

Conclusion: 该工作提供了一种鲁棒、可验证且高效的新途径，用于合成面向通用STL任务的TV-CBF，具备良好的应用潜力与跨系统适用性。

Abstract: This paper introduces a new framework for synthesizing time-varying control
barrier functions (TV-CBFs) for general Signal Temporal Logic (STL)
specifications using spatiotemporal tubes (STT). We first formulate the STT
synthesis as a robust optimization problem (ROP) and solve it through a
scenario optimization problem (SOP), providing formal guarantees that the
resulting tubes capture the given STL specifications. These STTs are then used
to construct TV-CBFs, ensuring that under any control law rendering them
invariant, the system satisfies the STL tasks. We demonstrate the framework
through case studies on a differential-drive mobile robot and a quadrotor, and
provide a comparative analysis showing improved efficiency over existing
approaches.

</details>
