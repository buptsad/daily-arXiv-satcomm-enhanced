<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.IT](#cs.IT) [Total: 5]
- [eess.SY](#eess.SY) [Total: 5]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 65]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Compensation of Coarse Quantization Effects on Channel Estimation and BER in Massive MIMO](https://arxiv.org/abs/2512.14893)
*Reza Mohammadkhani,Azad Azizzadeh,Seyed Vahab Al-Din Makki,John Thompson,Maziar Nekovee*

Main category: eess.SP

TL;DR: 在不完美CSI下，给出带ZF检测的uncoded M-QAM的BER紧密近似，并提出联合优化量化分辨率、发射功率与导频长度的设计框架，以提升低分辨率量化下的大规模MIMO能效。


<details>
  <summary>Details</summary>
Motivation: 降低大规模MIMO的实现成本与功耗，考虑粗量化噪声对信道估计与数据传输的影响，给出在现实量化约束下的系统性能评估与优化手段。

Method: 基于LMMSE信道估计，推导 uncoded M-QAM+ZF 的BER近似，并将量化噪声纳入分析；构建一个快速的优化框架，在不同用户数和基站天线下，联合优化量化分辨率、发射功率与导频长度。

Result: 给出BER的紧密近似，可作为替代Monte Carlo仿真的设计工具；在16-QAM示例中，通过导频延长2.5倍、发射功率降低0.5dB，可以使3比特量化系统达到全分辨率的BER；显示在不同参数设定下的能效提升潜力。

Conclusion: 该框架提供快速且准确的设计方法，使在量化约束下的上行链路系统参数优化成为可行，并有助于比较不同量化策略对能效的影响。

Abstract: Low-resolution quantization is essential to reduce implementation cost and power consumption in massive multiple-input multiple-output (MIMO) systems for 5G and 6G. While most existing studies assume perfect channel state information (CSI), we model the impact of coarse quantization noise on both channel estimation and data transmission, yielding a more realistic assessment of system performance under imperfect CSI conditions in the uplink. We develop a tight approximation for the bit-error ratio (BER) of uncoded M-QAM with zero-forcing detection, based on the linear minimum mean-square error (LMMSE) channel estimate. These analytical results enable compensation strategies that jointly optimize quantization resolution, transmit power, and pilot length across different numbers of users and base station antennas. We further demonstrate the applicability of the proposed framework through several design scenarios that highlight its effectiveness in optimizing system parameters and improving energy efficiency under quantization constraints. For example, in a 16-QAM system, extending the pilot sequence by 2.5 times and lowering transmit power by 0.5 dB enables a 3-bit quantized system to match the BER of the full-resolution case. The proposed framework offers a fast and accurate alternative to Monte Carlo simulations, enabling practical system optimization under realistic quantization constraints.

</details>


### [2] [Janus Metasurface Breaking Polarization Symmetry: Surface-Modulated Electromagnetic Wave Radiation with Coexistent Linear and Circular Polarization](https://arxiv.org/abs/2512.15045)
*Aparna Parameswaran,Hoyoung Kim,Sangkil Kim*

Main category: eess.SP

TL;DR: 提出一种 Janus 相位面张量阻抗全息天线（JHA），在单一馈源/单开口下同时辐射线性偏振和圆偏振束，通过改进的张量阻抗方程显著降低高辐射角度的交叉极化，实现0.5 GHz 带宽的宽带工作并保持高圆偏振纯度，理论与 aperture 场积分验证，且有三种变体原型验证。


<details>
  <summary>Details</summary>
Motivation: 需要在单馈、单开口条件下实现多极化（LP 与 CP）辐射，同时抑制高角度干扰的交叉极化，降低系统复杂性并提升带宽，适用于面向宽带通信的高性能天线。

Method: 提出 Janus 指向的张量阻抗全息设计（JHA），通过修改张量阻抗方程实现对 LP/CP 辐射的控制；利用 aperture field integration 理论来推导和验证阻抗分布对远场的影响；设计并制造三种变体原型并进行实验验证。

Result: 实现双极化辐射（LP 与 CP）从单口径天线，显著降低高角度的交叉极化；实现约0.5 GHz 的带宽并保持高 CP纯度；理论推导与数值-实验对照均一致，原型验证了可制造性与性能。

Conclusion: 提出的 JHA 提供一种在宽带通信场景下的单馈单口、多极化高性能天线设计思路，具有良好的理论基础、实现可行性及实验验证，适用于未来低复杂度的宽带通信系统。

Abstract: In this work, a Janus metasurface based tensor impedance holographic antenna (JHA) is proposed that simultaneously radiates linearly polarized (LP) and circularly polarized (CP) beams from a single aperture excited by a single feed. The proposed design introduces modified tensor impedance equations to significantly reduce cross-polarization at higher radiation angles. It demonstrates broadband operation bandwidth of 0.5 GHz while maintaining high circular polarization purity. The design methodology is verified using aperture field integration theory, ensuring that the impedance distribution produces the desired far-field radiation patterns. Prototypes of three variations of the holographic antenna are fabricated, validating its performance. The radiation characteristics of the proposed antenna make it an attractive choice for advanced broadband communication applications.

</details>


### [3] [Deep Reinforcement Learning for Joint Time and Power Management in SWIPT-EH CIoT](https://arxiv.org/abs/2512.15062)
*Nadia Abdolkhani,Nada Abdel Khalek,Walaa Hamouda,Iyad Dayoub*

Main category: eess.SP

TL;DR: 基于深度强化学习的认知物联网SWIPT系统中联合时分配和功率控制的双重DDQN方法及其性能提升


<details>
  <summary>Details</summary>
Motivation: 在认知物联网环境中，需要在能量获取与信息传输之间实现高效权衡，同时需应对小尺度衰落、现实能量收集模型以及干扰约束，提升吞吐和系统寿命是关键挑战

Method: 将联合时间分配与功率控制建模为马尔可夫决策过程；提出带上界置信边界的双DQN（DDQN-UB）以改进探索与利用的平衡；在SWIPT背景下学习时间切换因子、能量管理和传输策略，并评估在小尺度衰落与干扰约束下的性能

Result: 仿真结果表明该方法在性能上优于现有DRL方法，展现出更高吞吐量与更长系统寿命的潜力

Conclusion: 将上置信边界结合进DDQN以应对CIoT-SWIPT中的联合时分配与功率控制，是一个有效的研究方向，能显著提升能量效率与系统稳定性

Abstract: This letter presents a novel deep reinforcement learning (DRL) approach for joint time allocation and power control in a cognitive Internet of Things (CIoT) system with simultaneous wireless information and power transfer (SWIPT). The CIoT transmitter autonomously manages energy harvesting (EH) and transmissions using a learnable time switching factor while optimizing power to enhance throughput and lifetime. The joint optimization is modeled as a Markov decision process under small-scale fading, realistic EH, and interference constraints. We develop a double deep Q-network (DDQN) enhanced with an upper confidence bound. Simulations benchmark our approach, showing superior performance over existing DRL methods.

</details>


### [4] [CF-Net: A Cross-Feature Reconstruction Network for High-Accuracy 1-Bit Target Classification](https://arxiv.org/abs/2512.15105)
*Jundong Qi,Weize Sun,Shaowu Chen,Lei Huang,Qiuchen Liu*

Main category: eess.SP

TL;DR: Two-stage CF-Net using self-supervised pretraining to enable high-precision 1-bit radar target classification without oversampling; demonstrates competitive performance with 16-bit methods.


<details>
  <summary>Details</summary>
Motivation: 1-bit quantization causes heavy information loss; oversampling at high frequencies is impractical; aim to achieve high-accuracy classification directly from 1-bit data.

Method: Stage1: dual-branch U-Net self-supervised pretraining with cross-feature reconstruction to convert 1-bit inputs to high-fidelity 16-bit-like representations. Stage2: fine-tune the pre-trained encoder for multi-class target classification.

Result: On two radar datasets, CF-Net learns robust 1-bit features and attains accuracy comparable or superior to some 16-bit methods without oversampling.

Conclusion: CF-Net demonstrates that accurate 1-bit radar classification is achievable at the same sampling rate via self-supervised feature learning and encoder reuse, reducing the need for oversampling.

Abstract: Target classification is a fundamental task in radar systems, and its performance critically depends on the quantization precision of the signal. While high-precision quantization (e.g. 16-bit) is well established, 1-bit quantization offers distinct advantages by enabling direct sampling at high frequencies and eliminating complex intermediate stages. However, its extreme quantization leads to significant information loss. Although higher sampling rates can compensate for this loss, such oversampling is impractical at the high frequencies targeted for direct sampling. To achieve high-accuracy classification directly from 1-bit radar data under the same sampling rate, this paper proposes a novel two-stage deep learning framework, CF-Net. First, we introduce a self-supervised pre-training strategy based on a dual-branch U-Net architecture. This network learns to restore high-fidelity 16-bit images from their 1-bit counterparts via a cross-feature reconstruction task, forcing the 1-bit encoder to learn robust features despite extreme quantization. Subsequently, this pre-trained encoder is repurposed and fine-tuned for the downstream multi-class target classification task. Experiments on two radar target datasets demonstrate that CF-Net can effectively extract discriminative features from 1-bit imagery, achieving comparable and even superior accuracy to some 16-bit methods without oversampling.

</details>


### [5] [Learning-Based Phase Shift Optimization of Liquid Crystal RIS in Dynamic mmWave Networks](https://arxiv.org/abs/2512.15279)
*Le Hao,Robin Neuder,Mohamadreza Delbari,Alejandro Jiménez-Sáez,Vahid Jamali,Arash Asadi,Andrea Ortiz*

Main category: eess.SP

TL;DR: 提出面向动态场景的LC-RIS自适应控制框架，采用DDPG进行相位控制以提升移动用户下的毫米波数据速率，同时在不完美CSI条件下权衡信噪比与配置时间，利用高保真射线追踪与LC原型数据进行验证。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信需要更高覆盖和信号质量；LC-RIS具能效和降低成本的潜力，但重新配置时间长（几十毫秒），在动态场景中应用受限。现有工作多数聚焦硬件设计或静态场景，缺乏对动态环境下的优化方法。

Method: 提出基于深度确定性策略梯度（DDPG）的强化学习优化框架，动态调整LC-RIS相位以最大化数据率；不依赖完美的信道状态信息，权衡信噪比与配置时间；在高保真射线追踪仿真和LC-RIS原型测量数据上进行验证。

Result: 实验结果表明该强化学习策略可在动态场景中对LC-RIS相位进行自适应控制，提升动态mmWave系统的性能潜力，且证明在缺少完美CSI的情况下仍能获得有效数据率提升。

Conclusion: 本文提出的RL驱动的LC-RIS动态控制框架为解决慢重新配置问题提供一条可行路径，结合仿真与实测数据，显示出在动态场景下实现自适应控制的潜力与应用前景。

Abstract: To enhance coverage and signal quality in millimeter-wave (mmWave) frequencies, reconfigurable intelligent surfaces (RISs) have emerged as a game-changing solution to manipulate the wireless environment. Traditional semiconductor-based RISs face scalability issues due to high power consumption. Meanwhile, liquid crystal-based RISs (LC-RISs) offer energy-efficient and cost-effective operation even for large arrays. However, this promise has a caveat. LC-RISs suffer from long reconfiguration times, on the order of tens of milliseconds, which limits their applicability in dynamic scenarios. To date, prior works have focused on hardware design aspects or static scenarios to address this limitation, but little attention has been paid to optimization solutions for dynamic settings. Our paper fills this gap by proposing a reinforcement learning-based optimization framework to dynamically control the phase shifts of LC-RISs and maximize the data rate of a moving user. Specifically, we propose a Deep Deterministic Policy Gradient (DDPG) algorithm that adapts the LC-RIS phase shifts without requiring perfect channel state information and balances the tradeoff between signal-to-noise ratio (SNR) and configuration time. We validate our approach through high-fidelity ray tracing simulations, leveraging measurement data from an LC-RIS prototype. Our results demonstrate the potential of our solution to bring adaptive control to dynamic LC-RIS-assisted mmWave systems.

</details>


### [6] [Moment-Matching Array Processing Technique for diffuse source estimation](https://arxiv.org/abs/2512.15283)
*Colin Cros,Laurent Ferro-Famil*

Main category: eess.SP

TL;DR: MoMET：一种低复杂度的基于矩量匹配的 diffuse 源 DOA 估计方法，能够在无需对源分布做强先验假设的前提下，同时估计平均到达角、分布扩展（第一中心矩）与能量，通过协方差匹配实现估计，鲁棒且高效，并推导了渐近偏差与协方差，且通过仿真验证性能。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散源 DOA 估计往往依赖于对功率密度的先验分布假设；若假设错误会导致显著估计偏差。因此需要一种对先验分布依赖较弱、参数可直接估计的鲁棒方法。

Method: 将未知的源密度用其均值 DOA 与一阶中心矩来刻画；通过对观测量的经验协方差与由这些矩量所建模的协方差进行匹配来估计参数（协方差匹配）。该过程实现低复杂度估计，并推导出 MoMET 的渐近偏差与协方差。

Result: 提出的 MoMET 具有鲁棒性强、数值效率高的特性，且给出渐近性质（偏差、协方差）的推导；通过仿真验证了其性能。

Conclusion: MoMET 提供了一种无需强先验分布假设即可对窄带扩散源进行有效的 DOA、 dispersion（扩展）和功率估计的方法，具有理论和数值上的优越性，适用于需低复杂度且对先验依赖较弱的场景。

Abstract: Direction of Arrival (DOA) estimation is a fundamental problem in signal processing. Diffuse sources, whose power density cannot be represented with a single angular coordinate, are usually characterized based on prior assumptions, which associate the source angular density with a specific set of functions. However, these assumptions can lead to significant estimation biases when they are incorrect. This paper introduces the Moment-Matching Estimation Technique (MoMET), a low-complexity method for estimating the mean DOA, spread, and power of a narrow diffuse source without requiring prior knowledge on the source distribution. The unknown source density is characterized by its mean DOA and its first central moments, which are estimated through covariance matching techniques which fit the empirical covariance of the measurements to that modeled from the moments. The MoMET parameterization is robust to incorrect model assumptions, and numerically efficient. The asymptotic bias and covariance of the new estimator are derived and its performance is demonstrated through simulations.

</details>


### [7] [On the Asymptotic Performance of Diagonally Loaded Detectors for Large Arrays: To Achieve CFAR and Optimality](https://arxiv.org/abs/2512.15290)
*Jie Zhou,Junhao Xie*

Main category: eess.SP

TL;DR: 在大维度极限下，提出两种CFAR DL检测器（CFAR-DL-SCMF、CFAR-DL-AMF），通过对 |a|^2 的归一化实现对协方差矩阵、目标方向以及加载因子的CFAR性，推导最优加载 λ_opt 及其一致估计，给出最优 CFAR-DL-SCMF/AMF，并在多种场景下优于 EL-AMF 与 persymmetric AMF。


<details>
  <summary>Details</summary>
Motivation: 解决 DL-AMF 在任意协方差矩阵下缺乏 CFAR 性以及缺少选取最佳加载因子的准则的问题；在大维度极限下分析性能并给出可实施的加载因子估计。

Method: 在大样本极限下，证明 DL 检测器的 |a|^2 在归一化后等价于确定量收敛，从而设计两种 CFAR DL 检测器，并推导 λ_opt 的解析表达及一致估计。

Result: CFAR-DL-SCMF 与 CFAR-DL-AMF 提供对协方差矩阵、目标方向和加载因子的CFAR性；给出 λ_opt 的显式表达及一致估计；实验显示两者优于 EL-AMF 与 persymmetric AMF，在全秩与低秩环境下均有优势。

Conclusion: 通过大维度分析实现 CFAR-DL 检测，提出两种可实施的最优 CFAR-DL 检测器，提升鲁棒性与检测性能。

Abstract: This paper addresses two critical limitations in diagonally loaded (DL) adaptive matched filter (AMF) detector: (1) the lack of CFAR property with respect to arbitrary covariance matrices, and (2) the absence of selection criteria for optimal loading factor from the perspective of maximizing the detection probability (Pd). We provide solutions to both challenges through a comprehensive analysis for the asymptotic performance of DL-AMF under large dimensional regime (LDR) where the dimension N and sample size K tend to infinity whereas their ratio N/K converges to a constant c\in(0,1). The analytical results show that any DL detectors constructed by normalizing the random variable |a|2=|sH(R+λIN)-1y0|2 with a deterministic quantity or a random variable that converges almost surely to a deterministic value will exhibit equivalent performance under LDR. Following this idea, we derive two CFAR DL detectors: CFAR DL semi-clairvoyant matched filter (CFAR-DL-SCMF) detector and CFAR DL adaptive matched filter (CFAR-DL-AMF) detector, by normalizing |a|2 with an appropriate deterministic quantity and its consistent estimate, respectively. The theoretical analysis and simulations show that both CFAR-DL-SCMF and CFAR-DL-AMF achieve CFAR with respect to covariance matrix, target steering vector and loading factor. Furthermore, we derive the asymptotically optimal loading factor λ_opt by maximizing the explicit expression of asymptotic Pd. For practical implementation, we provide a consistent estimator for λ_opt under LDR. Based on λ_opt and its consistent estimate, we establish the optimal CFAR-DL-SCMF (opt-CFAR-DL-SCMF) and the optimal CFAR-DL-AMF (opt-CFAR-DL-AMF). Numerical examples demonstrate that the proposed opt-CFAR-DL-SCMF and opt-CFAR-DL-AMF consistently outperform EL-AMF and persymmetric AMF in both full-rank and low-rank clutter plus noise environments.

</details>


### [8] [Semi-Blind Joint Channel and Symbol Estimation for Beyond Diagonal Reconfigurable Surfaces](https://arxiv.org/abs/2512.15441)
*Gilderlan Tavares de Araújo,André L. F. de Almeida Buno Sokal,Gabor Fodor,Paulo R. B. Gomes*

Main category: eess.SP

TL;DR: 提出了一种半盲张量方法用于BD-RIS上联合通道与符号估计，省略训练序列，给出两种实现（两阶段PARATUCK转PARAFAC及第四阶TUCKER单阶段）和辨识性条件，并通过数值结果比较现有方案的性能与权衡。


<details>
  <summary>Details</summary>
Motivation: 解决BD-RIS中因复杂连通性带来的通道估计开销与对训练序列依赖，同时在移动场景下实现对时变信道的无训练估计。

Method: 将接收信号重构为高阶张量并应用半盲张量分解；提出两种半盲接收机：1) 两阶段方法，将四阶PARATUCK模型转化为三阶PARAFAC以实现分解；2) 基于第四阶TUCKER分解的单阶段迭代算法。并给出对可靠联合恢复的辨识性条件。

Result: 仿真表明在无训练的条件下，所提半盲方案可在鲁棒性与精度之间实现有竞争力的权衡，优于部分基于训练的传统方案，且两种实现各自具有不同的复杂度与收敛性特征。

Conclusion: 半盲张量方法为BD-RIS场景提供可行且高效的通道与符号估计方案，在降低训练开销的同时实现较好估计性能；两种实现提供不同的复杂度与鲁棒性权衡，适用于移动性较高的应用。

Abstract: The beyond-diagonal reconfigurable intelligent surface (BD-RIS) is a recent architecture in which scattering elements are interconnected to enhance the degrees of freedom for wave control, yielding performance gains over traditional single-connected RISs. For BD-RIS, channel estimation - well-studied for conventional RIS - becomes more challenging due to the complex connections and a larger number of coefficients. Prior works rely on pilot-assisted estimation followed by data decoding. This paper introduces a semi-blind tensor-based approach for joint channel and symbol estimation that eliminates the need for training sequences by leveraging data symbols directly. A practical scenario with time-varying user terminal-RIS channels under mobility is considered. By reformulating the received signal from a tensor decomposition perspective, we develop two semi-blind receivers: a two-stage method transforming the fourth-order PARATUCK model into a third-order PARAFAC model, and a single-stage iterative process based on fourth-order TUCKER decomposition. Identifiability conditions for reliable joint recovery are derived, and numerical results demonstrate the performance advantages and trade-offs of the proposed schemes over existing solutions.

</details>


### [9] [Optimum Discrete Beamforming via Minkowski Sum of Polygons](https://arxiv.org/abs/2512.15546)
*Heedong Do,Angel Lozano*

Main category: eess.SP

TL;DR: 将离散波束成形问题转化为计算凸多边形的Minkowski和，其结果仍为凸多边形；新结构保证目标多边形的顶点数不超过原多边形顶点数之和，从而实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 解决离散波束成形中的离散性/组合性难题，提供一个几何、凸性视角，使优化可以更高效地进行。

Method: 将问题表述为两个或多个凸多边形的Minkowski和的计算问题；利用该和的顶点数上界（不超过输入顶点数之和）实现高效的多边形运算与求解。

Result: 给出一种高效求解离散波束成形的理论框架，证明在凸几何视角下可获得最优解且计算复杂度可控。

Conclusion: Minkowski和的凸几何表述提供直观且计算上高效的路径来求解最优离散波束成形。

Abstract: This letter casts the problem of optimum discrete beamforming as the computation of the Minkowski sum of convex polygons, which is itself a convex polygon. The number of vertices of the latter is at most the sum of the number of vertices of the original polygons, enabling its efficient computation. This original and intuitive formulation confirms that the optimum beamforming solution can be found efficiently.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs](https://arxiv.org/abs/2512.14741)
*Jing Cui,Yufei Han,Jianbin Jiao,Junge Zhang*

Main category: cs.CR

TL;DR: P-Trojan 在多阶段持续微调中实现高持久性后门，超过99% 的持久性且保持清洁任务精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少考察后门在用户驱动的持续微调下的持久性，且朴素注入的后门在更新后往往衰减。本研究系统地评估并强化持续微调中的后门持久性。

Method: 提出基于触发的攻击算法 P-Trojan，通过对嵌入层的污染梯度与清洁任务梯度对齐，使后门映射在后续更新中不易被抑制或遗忘；提供理论分析，并在 Qwen2.5 与 LLaMA3 等模型及多任务序列上进行实验。

Result: 在多任务序列设定下，P-Trojan 实现超过 99% 的后门持久性，同时保持清洁任务的准确度。

Conclusion: 需在现实模型自适应流水线中进行持久性评估并强化防御，以应对持续微调环境下的后门攻击。

Abstract: Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), enabling adversaries to trigger harmful outputs or bypass safety controls. However, the persistence of the implanted backdoors under user-driven post-deployment continual fine-tuning has been rarely examined. Most prior works evaluate the effectiveness and generalization of implanted backdoors only at releasing and empirical evidence shows that naively injected backdoor persistence degrades after updates. In this work, we study whether and how implanted backdoors persist through a multi-stage post-deployment fine-tuning. We propose P-Trojan, a trigger-based attack algorithm that explicitly optimizes for backdoor persistence across repeated updates. By aligning poisoned gradients with those of clean tasks on token embeddings, the implanted backdoor mapping is less likely to be suppressed or forgotten during subsequent updates. Theoretical analysis shows the feasibility of such persistent backdoor attacks after continual fine-tuning. And experiments conducted on the Qwen2.5 and LLaMA3 families of LLMs, as well as diverse task sequences, demonstrate that P-Trojan achieves over 99% persistence while preserving clean-task accuracy. Our findings highlight the need for persistence-aware evaluation and stronger defenses in realistic model adaptation pipelines.

</details>


### [11] [Factor(U,T): Controlling Untrusted AI by Monitoring their Plans](https://arxiv.org/abs/2512.14745)
*Edward Lue Chee Lip,Anthony Channg,Diana Kim,Aaron Sandoval,Kevin Zhu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As AI capabilities advance, we increasingly rely on powerful models to decompose complex tasks $\unicode{x2013}$ but what if the decomposer itself is malicious? Factored cognition protocols decompose complex tasks into simpler child tasks: one model creates the decomposition, while other models implement the child tasks in isolation. Prior work uses trusted (weaker but reliable) models for decomposition, which limits usefulness for tasks where decomposition itself is challenging. We introduce Factor($U$,$T$), in which an untrusted (stronger but potentially malicious) model decomposes while trusted models implement child tasks. Can monitors detect malicious activity when observing only natural language task instructions, rather than complete solutions? We baseline and red team Factor($U$,$T$) in control evaluations on BigCodeBench, a dataset of Python coding tasks. Monitors distinguishing malicious from honest decompositions perform poorly (AUROC 0.52) compared to monitors evaluating complete Python solutions (AUROC 0.96). Furthermore, Factor($D$,$U$), which uses a trusted decomposer and monitors concrete child solutions, achieves excellent discrimination (AUROC 0.96) and strong safety (1.2% ASR), demonstrating that implementation-context monitoring succeeds where decomposition-only monitoring fails.

</details>


### [12] [BLINDSPOT: Enabling Bystander-Controlled Privacy Signaling for Camera-Enabled Devices](https://arxiv.org/abs/2512.14746)
*Jad Al Aaraj,Athina Markopoulou*

Main category: cs.CR

TL;DR: BlindSpot enables bystanders to signal privacy preferences to camera devices in real-time using three on-device modalities (hand gesture, VLC, UWB), with anti-impersonation validation, implemented on smartphones.


<details>
  <summary>Details</summary>
Motivation: Protect bystander privacy from pervasive camera capture; current bystander controls are lacking; need real-time, device-local solutions without sharing sensitive data.

Method: Design and compare three signaling modalities; implement on smartphone; add geometric consistency-based validation to confirm signal origin and guard impersonation; evaluate accuracy and latency under varying distances, illumination, and motion.

Result: Modalities are feasible; trade-offs observed: VLC/hand gesture vs UWB in latency, range, robustness; validation mechanism works to prevent impersonation; system demonstrated on commodity device.

Conclusion: BlindSpot shows practical, on-device privacy signaling with multiple options; enables real-time bystander control with security considerations; scalable across environments.

Abstract: Camera-equipped mobile devices, such as phones, smart glasses, and AR headsets, pose a privacy challenge for bystanders, who currently lack effective real-time mechanisms to control the capture of their picture, video, including their face. We present BlindSpot, an on-device system that enables bystanders to manage their own privacy by signaling their privacy preferences in real-time without previously sharing any sensitive information. Our main contribution is the design and comparative evaluation of three distinct signaling modalities: a hand gesture mechanism, a significantly improved visible light communication (VLC) protocol, and a novel ultra-wideband (UWB) communication protocol. For all these modalities, we also design a validation mechanism that uses geometric consistency checks to verify the origin of a signal relative to the sending bystander, and defend against impersonation attacks. We implement the complete system (BlindSpot) on a commodity smartphone and conduct a comprehensive evaluation of each modality's accuracy and latency across various distances, lighting conditions, and user movements. Our results demonstrate the feasibility of these novel bystander signaling techniques and their trade-offs in terms of system performance and convenience.

</details>


### [13] [Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection](https://arxiv.org/abs/2512.15503)
*Konstantinos Kalogiannis,Ahmed Mohamed Hussain,Hexu Li,Panos Papadimitratos*

Main category: cs.CR

TL;DR: Transformer-based AIMformer for real-time misbehavior detection in vehicular platoons; achieves high accuracy (≥0.93) with sub-ms edge inference.


<details>
  <summary>Details</summary>
Motivation: Traditional misbehavior detectors rely on plausibility checks and statistics, suffering high FP and failing to capture complex temporal-spatial dynamics in multi-vehicle platoons; a robust real-time solution is needed.

Method: AIMformer employs multi-head self-attention to model intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It uses global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. Introduces a Precision-Focused BCE loss to penalize false positives. Edge-friendly deployment via TensorFlow Lite, ONNX, and TensorRT for sub-millisecond inference.

Result: Extensive evaluation over 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios shows performance ≥0.93. Deployment analysis demonstrates sub-millisecond latency, suitable for real-time edge operation.

Conclusion: AIMformer is viable for both in-vehicle and roadside deployment, offering improved safety-critical misbehavior detection in vehicular platoons.

Abstract: Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.

</details>


### [14] [One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs](https://arxiv.org/abs/2512.14751)
*Yixin Tan,Zhe Yu,Jun Sakuma*

Main category: cs.CR

TL;DR: Finetuning pretrained LLMs preserves jailbreak vulnerabilities from the pretrained stage, with prompts optimized on the pretrained model transferring effectively to finetuned derivatives. The authors reveal this transferability via representation-level analysis and introduce a Probe-Guided Projection (PGP) attack that exploits transferability directions, showing strong cross-family transfer across tasks.


<details>
  <summary>Details</summary>
Motivation: Understand whether finetuning introduces or preserves jailbreak vulnerabilities and quantify transferability of adversarial prompts from pretrained to finetuned LLMs in a realistic pretrain-to-finetune threat model.

Method: Empirical evaluation with white-box access to the pretrained model and black-box access to finetuned derivatives; adversarial prompts optimized on pretrained models; representation-level probing to assess linear separability of transferable prompts; development of Probe-Guided Projection (PGP) attack to steer optimization toward transferability directions.

Result: Adversarial prompts crafted on the pretrained model transfer most effectively to finetuned variants; transferable prompts form linear separations in pretrained hidden states; the PGP attack achieves strong transfer success across multiple LLM families and finetuned tasks.

Conclusion: The pretrain-to-finetune paradigm harbors security risks as vulnerabilities are inherited via transferability; the PGP attack demonstrates a practical and robust method to exploit these vulnerabilities, highlighting the need for defense strategies at both pretraining and fine-tuning stages.

Abstract: Finetuning pretrained large language models (LLMs) has become the standard paradigm for developing downstream applications. However, its security implications remain unclear, particularly regarding whether finetuned LLMs inherit jailbreak vulnerabilities from their pretrained sources. We investigate this question in a realistic pretrain-to-finetune threat model, where the attacker has white-box access to the pretrained LLM and only black-box access to its finetuned derivatives. Empirical analysis shows that adversarial prompts optimized on the pretrained model transfer most effectively to its finetuned variants, revealing inherited vulnerabilities from pretrained to finetuned LLMs. To further examine this inheritance, we conduct representation-level probing, which shows that transferable prompts are linearly separable within the pretrained hidden states, suggesting that universal transferability is encoded in pretrained representations. Building on this insight, we propose the Probe-Guided Projection (PGP) attack, which steers optimization toward transferability-relevant directions. Experiments across multiple LLM families and diverse finetuned tasks confirm PGP's strong transfer success, underscoring the security risks inherent in the pretrain-to-finetune paradigm.

</details>


### [15] [CODE ACROSTIC: Robust Watermarking for Code Generation](https://arxiv.org/abs/2512.14753)
*Li Lin,Siyuan Xin,Yang Cao,Xiaochun Cao*

Main category: cs.CR

TL;DR: 通过Cue List对代码的低熵/高熵部分进行区分，在注入水印时遵循该 Cue List，从而在面对注释删除攻击时仍具备较高的可检测性与可用性。与现有三种代码水印技术相比，在HumanEval上取得更好结果。


<details>
  <summary>Details</summary>
Motivation: 防止LLMs被滥用的代码水印需求日益重要，尤其是对代码的知识产权保护。现有的代码水印方法无法应对注释删除攻击，导致水印失效，需开发对抗低熵代码、注释删除等攻击的新方法。

Method: 利用先验知识构建一个 Cue List，用以区分低熵与高熵的代码片段，然后在高信息熵区域注入水印，并遵循 Cue List 指导的策略。

Result: 在 HumanEval 数据集上对比三种先进的代码水印方法，所提出的方法在可检测性和可用性上具有更优表现，证明了其有效性。

Conclusion: 通过基于 Cue List 的水印注入策略，可以提升对注释删除等攻击的鲁棒性，从而提高对 LLM 代码输出的水印检测与实用性。

Abstract: Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.

</details>


### [16] [Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation](https://arxiv.org/abs/2512.14767)
*Unai Laskurain,Aitor Aguirre-Ortuzar,Urko Zurutuza*

Main category: cs.CR

TL;DR: 提出了一种基于私有集合交集的Shapley-CMI隐私实现，用于Vertical Federated Learning，在不交换原始数据的前提下计算特征贡献的Shapley-CMI值。


<details>
  <summary>Details</summary>
Motivation: 在Vertical FL中，在模型尚未训练时，如何评估各方特征贡献是关键挑战，该工作目标是在早期阶段实现无模型、无原始数据共享的特征估值。

Method: 引入私有集合交集(PSI)服务器，执行特征置换和交集大小的加密计算，针对离散化和加密的ID组，在不暴露原始数据的情况下得到Shapley-CMI所需的边际效用；各方据此计算自己的Shapley-CMI值。

Result: 初步实验验证正确性和隐私性，证明该系统在VFL中实现安全且高效的特征贡献估计的可行性。

Conclusion: 该方法可扩展到多方，保护数据机密性，无需共享原始数据或训练模型，即可实现公平的数据估值。

Abstract: Federated Learning (FL) is an emerging machine learning paradigm that enables multiple parties to collaboratively train models without sharing raw data, ensuring data privacy. In Vertical FL (VFL), where each party holds different features for the same users, a key challenge is to evaluate the feature contribution of each party before any model is trained, particularly in the early stages when no model exists. To address this, the Shapley-CMI method was recently proposed as a model-free, information-theoretic approach to feature valuation using Conditional Mutual Information (CMI). However, its original formulation did not provide a practical implementation capable of computing the required permutations and intersections securely. This paper presents a novel privacy-preserving implementation of Shapley-CMI for VFL. Our system introduces a private set intersection (PSI) server that performs all necessary feature permutations and computes encrypted intersection sizes across discretized and encrypted ID groups, without the need for raw data exchange. Each party then uses these intersection results to compute Shapley-CMI values, computing the marginal utility of their features. Initial experiments confirm the correctness and privacy of the proposed system, demonstrating its viability for secure and efficient feature contribution estimation in VFL. This approach ensures data confidentiality, scales across multiple parties, and enables fair data valuation without requiring the sharing of raw data or training models.

</details>


### [17] [MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber](https://arxiv.org/abs/2512.14846)
*Arth Bhardwaj,Sia Godika,Yuvam Loonker*

Main category: cs.CR

TL;DR: A practical Multi-Agent LLM Cyber Defense Framework (MALCDF) coordinates four LLM agents—Detection, Intelligence, Response, and Analysis—via a secure, ontology-aligned messaging layer to achieve real-time cyber defense, outperforming a lightweight ML-IDS baseline on a 50-record CICIDS2017-based stream.


<details>
  <summary>Details</summary>
Motivation: Traditional, centralized security tools often miss adaptive, multi-vector attacks; there is a need for real-time coordination among specialized LLM agents with auditable outputs.

Method: Four LLM agents (Detection, Intelligence, Response, Analysis) operate in real time and communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages. Outputs include audit-friendly artifacts like MITRE ATT&CK mappings. Evaluation uses a 50-record live stream derived from CICIDS2017, with training on CICIDS2017 fields/schema. Baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the same 50-record stream, with no training/test overlap.

Result: MALCDF achieves 90.0% detection accuracy, 85.7% F1-score, 9.1% false-positive rate, and 6.8 seconds average per-event latency, outperforming the LRF-IDS baseline and a single-LLM setup in accuracy while delivering consistent end-to-end outputs.

Conclusion: Coordinating simple LLM agents with secure, ontology-aligned messaging can enhance practical, real-time cyber defense.

Abstract: Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&CK mappings).
  For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records.
  In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.

</details>


### [18] [Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection](https://arxiv.org/abs/2512.14935)
*Nnamdi Philip Okonkwo,Lubna Luxmi Dhirani*

Main category: cs.CR

TL;DR: 在AWS上实现的AI增强云SOC，结合云原生监控与ML检测，通过三台EC2实例（Attacker、Defender、Monitoring）实现日志收集、恶意软件检测和日志异常检测的多模态威胁情报融合，从而对活动进行NORMAL/SUSPICIOUS/HIGH_CONFIDENCE_ATTACK的三级分级；在受控测试中宏F1最高可达1.00，但在嘈杂环境下性能可能下降。


<details>
  <summary>Details</summary>
Motivation: 云治理、风险与合规模块需要在高并发、成本敏感的环境中实现高效的态势感知与快速分级处置；现有单模态检测在云环境中的可用性与成本效益仍是挑战。

Method: 设计三台EC2（Attacker、Defender、Monitoring），在AWS上进行仿真入侵（利用Metasploit的反向shell），Filebeat将 Defender 日志送往Elasticsearch/Kibana；训练两个分类器：一个基于公开数据集的恶意软件检测器；一个基于合成增强日志（含对抗变体）的日志异常检测器；对分数进行校准与融合，输出多模态威胁情报并分级为 NORMAL、SUSPICIOUS、HIGH_CONFIDENCE_ATTACK。

Result: 在 held-out 测试中，融合策略实现了极高的宏观F1（最高达到1.00）在受控条件，但在噪声与多样化环境中性能可能波动。

Conclusion: 简单且经过校准的融合策略可以在受限、成本敏感的云SOC场景下提升检测能力与响应效率。

Abstract: Cloud Security Operations Center (SOC) enable cloud governance, risk and compliance by providing insights visibility and control. Cloud SOC triages high-volume, heterogeneous telemetry from elastic, short-lived resources while staying within tight budgets. In this research, we implement an AI-Augmented Security Operations Center (AISOC) on AWS that combines cloud-native instrumentation with ML-based detection. The architecture uses three Amazon EC2 instances: Attacker, Defender, and Monitoring. We simulate a reverse-shell intrusion with Metasploit, and Filebeat forwards Defender logs to an Elasticsearch and Kibana stack for analysis. We train two classifiers, a malware detector built on a public dataset and a log-anomaly detector trained on synthetically augmented logs that include adversarial variants. We calibrate and fuse the scores to produce multi-modal threat intelligence and triage activity into NORMAL, SUSPICIOUS, and HIGH\_CONFIDENCE\_ATTACK. On held-out tests the fusion achieves strong macro-F1 (up to 1.00) under controlled conditions, though performance will vary in noisier and more diverse environments. These results indicate that simple, calibrated fusion can enhance cloud SOC capabilities in constrained, cost-sensitive setups.

</details>


### [19] [SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports](https://arxiv.org/abs/2512.15003)
*Sogol Masoumzadeh,Yufei Li,Shane McIntosh,Dániel Varró,Lili Wei*

Main category: cs.CR

TL;DR: 提出SEBERTIS框架，通过对语义替代词汇的掩码语言模型微调，实现对未见安全相关问题的高效检测，在GitHub问题报告数据集上达到0.9880 F1，显著优于ML和LLM基线。


<details>
  <summary>Details</summary>
Motivation: 监控问题跟踪中的高风险、安全相关缺陷的优先级排序需要在早期识别；现有ML/LLM方法往往记忆词汇线索，难以对复杂提交实现可靠检测，现实中对实时检测有更高要求。

Method: 将双向变换器架构作为掩码语言模型进行微调，使用一系列语义等价的预测标签词汇（语义替代品，Semantic Surrogates）在被替换为掩码时进行预测，从而训练出不依赖词汇线索的分类器，能检测完全未见的安全相关问题。

Result: 在包含1万条GitHub issue报告的语料库上，SEBERTIS达到0.9880的F1分数；相比ML基线，精度/召回/ F1分数提升约14.44%-96.98%、15.40%-93.07%、14.90%-94.72%；相比LLM基线，提升约23.20%-63.71%、36.68%-85.63%、39.49%-74.53%。

Conclusion: SEBERTIS展示了一个不依赖词汇线索的DNN分类框架，能够在未见安全相关问题上实现高检测性能，显著优于现有的ML和LLM基线。

Abstract: Monitoring issue tracker submissions is a crucial software maintenance activity. A key goal is the prioritization of high risk, security-related bugs. If such bugs can be recognized early, the risk of propagation to dependent products and endangerment of stakeholder benefits can be mitigated. To assist triage engineers with this task, several automatic detection techniques, from Machine Learning (ML) models to prompting Large Language Models (LLMs), have been proposed. Although promising to some extent, prior techniques often memorize lexical cues as decision shortcuts, yielding low detection rate specifically for more complex submissions. As such, these classifiers do not yet reach the practical expectations of a real-time detector of security-related issues. To address these limitations, we propose SEBERTIS, a framework to train Deep Neural Networks (DNNs) as classifiers independent of lexical cues, so that they can confidently detect fully unseen security-related issues. SEBERTIS capitalizes on fine-tuning bidirectional transformer architectures as Masked Language Models (MLMs) on a series of semantically equivalent vocabulary to prediction labels (which we call Semantic Surrogates) when they have been replaced with a mask. Our SEBERTIS-trained classifier achieves a 0.9880 F1-score in detecting security-related issues of a curated corpus of 10,000 GitHub issue reports, substantially outperforming state-of-the-art issue classifiers, with 14.44%-96.98%, 15.40%-93.07%, and 14.90%-94.72% higher detection precision, recall, and F1-score over ML-based baselines. Our classifier also substantially surpasses LLM baselines, with an improvement of 23.20%-63.71%, 36.68%-85.63%, and 39.49%-74.53% for precision, recall, and F1-score.

</details>


### [20] [APT-ClaritySet: A Large-Scale, High-Fidelity Labeled Dataset for APT Malware with Alias Normalization and Graph-Based Deduplication](https://arxiv.org/abs/2512.15039)
*Zhenhao Yin,Hanbing Yan,Huishu Lu,Jing Xiong,Xiangyu Li,Rui Mei,Tianning Zang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large-scale, standardized datasets for Advanced Persistent Threat (APT) research are scarce, and inconsistent actor aliases and redundant samples hinder reproducibility. This paper presents APT-ClaritySet and its construction pipeline that normalizes threat actor aliases (reconciling approximately 11.22\% of inconsistent names) and applies graph-feature deduplication -- reducing the subset of statically analyzable executables by 47.55\% while retaining behaviorally distinct variants. APT-ClaritySet comprises: (i) APT-ClaritySet-Full, the complete pre-deduplication collection with 34{,}363 malware samples attributed to 305 APT groups (2006 - early 2025); (ii) APT-ClaritySet-Unique, the deduplicated release with 25{,}923 unique samples spanning 303 groups and standardized attributions; and (iii) APT-ClaritySet-FuncReuse, a function-level resource that includes 324{,}538 function-reuse clusters (FRCs) enabling measurement of inter-/intra-group sharing, evolution, and tooling lineage. By releasing these components and detailing the alias normalization and scalable deduplication pipeline, this work provides a high-fidelity, reproducible foundation for quantitative studies of APT patterns, evolution, and attribution.

</details>


### [21] [Quantifying Return on Security Controls in LLM Systems](https://arxiv.org/abs/2512.15081)
*Richard Helder Moulton,Austin O'Brien,John D. Hastings*

Main category: cs.CR

TL;DR: 为LLM安全工作流提出一个以决策为导向的框架，量化剩余风险、将对抗性探测转化为财务风险与控制收益率（RoC），并可对分层防护进行货币化比较。结果显示，在给定RAG系统与攻击场景下，基线风险极高，ABAC和NER redaction显著降低损失并提升RoC，NeMo Guardrails 效果有限。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏定量化的防护评估指南来帮助实践者在LLM安全工作流中做出性价比决策。本研究提出一个可复现的方法学，通过把攻击成败转化为经济损失，结合蒙特卡洛仿真与公开 breach-cost 数据，来比较不同防护的性价比。

Method: 建立一个以决策为导向的风险量化框架，使用Laplace的成功定理估计攻击成功概率，结合损失三角形分布，进行10,000次蒙特卡洛仿真，得到损失分布和期望损失。以DeepSeek-R1上的RAG服务对包含合成PII的语料进行自洽攻击，覆盖PII泄露、潜在上下文注入、提示注入、对抗性生成和发散等五类漏洞。比较三种防护：ABAC、Microsoft Presidio的NER红act、以及NeMo Guardrails，相对于基线RAG配置。

Result: 基线系统在PII、潜在注入、提示注入等攻击上攻击成功率很高（≥0.98），每次攻击场景的总期望损失约为313,000美元。ABAC将PII和提示相关攻击的成功概率降至近零，总损失降低约94%，RoC为9.83。NER redaction同样消除了PII泄露，RoC为5.97；而NeMo Guardrails收益有限，RoC仅为0.05。

Conclusion: 在LLM基础设施中，结合ABAC和NER redaction等防护，可以显著降低财务风险并提升RoC，使货币化对比成为可行的风险管理工具；不同防护的性价比差异明显，NeMo Guardrails的投资回报率较低。

Abstract: Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).

</details>


### [22] [MCPZoo: A Large-Scale Dataset of Runnable Model Context Protocol Servers for AI Agent](https://arxiv.org/abs/2512.15144)
*Mengying Wu,Pei Chen,Geng Hong,Aichao An,Jinsong Chen,Binwang Wan,Xudong Pan,Jiarun Dai,Min Yang*

Main category: cs.CR

TL;DR: MCPZoo 是一个规模化的 MCP 服务器数据集，包含约 90,146 台服务器和超过 1 万个可运行实例，开放获取以支持 MCP 的安全分析研究。


<details>
  <summary>Details</summary>
Motivation: 现有的 MCP 研究因缺乏大规模、易获得的服务器数据集而难以进行现实世界的交互和基准评估。

Method: 从多处公开来源收集 MCP 服务器，筛选并验证其可运行性与可交互性，整理统一的元数据并提供统一的访问接口；最终形成包含 90,146 台服务器的集合，且有超过一万套可运行实例。

Result: 公布 MCPZoo 作为开源、可访问的资源，允许在无需本地部署的情况下进行系统性探索和互动，提升对 MCP 的安全分析研究的可重复性和可扩展性。

Conclusion: 该数据集将显著促进基于 MCP 的安全分析研究，推动评估、对比与复现性的提升，并为 MCP 生态系统的研究提供了重要基准资源。

Abstract: Model Context Protocol (MCP) enables agents to interact with external tools, yet empirical research on MCP is hindered by the lack of large-scale, accessible datasets. We present MCPZoo, the largest and most comprehensive dataset of MCP servers collected from multiple public sources, comprising 90,146 servers. MCPZoo includes over ten thousand server instances that have been deployed and verified as runnable and interactable, supporting realistic experimentation beyond static analysis. The dataset provides unified metadata and access interfaces, enabling systematic exploration and interaction without manual deployment effort. MCPZoo is released as an open and accessible resource to support research on MCP-based security analysis.

</details>


### [23] [No More Hidden Pitfalls? Exposing Smart Contract Bad Practices with LLM-Powered Hybrid Analysis](https://arxiv.org/abs/2512.15179)
*Xiaoqi Li,Zongwei Li,Wenkai Li,Yuqing Zhang,Xin Wang*

Main category: cs.CR

TL;DR: SCALM：一个基于LLM的智能合约不良实践检测框架，结合分层推理与向量化模式匹配，在多数据集和多模型实验中优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 以太坊等区块链平台日益成熟，需在开发实践上维持高标准。尽管不良实践不必然导致安全问题，但会显著增大风险。本论文首次对智能合约中的不良开发实践进行系统化研究，覆盖47个具体问题。

Method: 提出SCALM，一个LLM驱动的框架，具有两项方法创新：(1) 将上下文感知的函数级切片与知识增强的语义推理通过可扩展的向量化模式匹配相结合的混合架构；(2) 一个多层推理核验系统，将底层代码模式与高层安全原则通过语法、设计模式与体系结构分析关联起来。

Result: 在使用多种LLM与数据集进行的大规模实验中，SCALM在检测智能合约不良实践方面的表现优于现有工具。

Conclusion: SCALM为智能合约不良实践的系统性研究与检测提供新的基线，验证了基于LLM的混合推理框架在安全相关代码分析中的有效性，并为后续工具开发奠定基础。

Abstract: As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they elevate the risk of encountering problems. Therefore, to understand and avoid these bad practices, this paper introduces the first systematic study of bad practices in smart contracts, delving into over 47 specific issues. Specifically, we propose SCALM, an LLM-powered framework featuring two methodological innovations: (1) A hybrid architecture that combines context-aware function-level slicing with knowledge-enhanced semantic reasoning via extensible vectorized pattern matching. (2) A multi-layer reasoning verification system connects low-level code patterns with high-level security principles through syntax, design patterns, and architecture analysis. Our extensive experiments using multiple LLMs and datasets have shown that SCALM outperforms existing tools in detecting bad practices in smart contracts.

</details>


### [24] [Talking to the Airgap: Exploiting Radio-Less Embedded Devices as Radio Receivers](https://arxiv.org/abs/2512.15387)
*Paul Staat,Daniel Davidovich,Christof Paar*

Main category: cs.CR

TL;DR: 研究表明未修改的嵌入式设备可作为无线接收器，在无硬件修改的情况下实现对气隙系统的无线渗透，工作在300–1000 MHz，数据速率可达100 kbps，覆盖数十米，挑战气隙的安全假设。


<details>
  <summary>Details</summary>
Motivation: 确保关键基础设施的可靠性与完整性，抵御远程攻击；传统的气隙防护依赖物理隔离，但对潜在的隐蔽射频接收能力尚未充分评估，存在被动式侵入风险。这项工作揭示了未修改设备的射频敏感性及其对气隙安全的潜在威胁。

Method: 在12款商用嵌入式设备和2个定制原型上，系统性地识别促成射频敏感性的配置，利用PCB走线和片上模数转换器的寄生RF敏感性，通过纯软件解码实现对外部信号的接收与解码；开展跨距离、非视距条件下的数据接收实验，评估不同配置对接收能力的影响，给出一套评估设备易受RF入侵的系统性方法。

Result: 在300–1000 MHz范围内可重复观测到接收，最小探测信号功率约1 mW；实现数据接收距离达数十米，非线性遮挡条件下也可工作，数据速率最高可达约100 kbps；不依赖额外传感器，设备的寄生RF敏感性即可被软解码利用。

Conclusion: 揭示了一个前所未有的指令与控制向量，即使在严格的物理隔离下，气隙系统也可能被有意或无意的软件控制的射频接收能力所突破；提出系统化的设备配置识别框架以评估风险，并强调需要重新审视嵌入式设备的物理层防护与气隙安全策略。

Abstract: Intelligent electronics are deeply embedded in critical infrastructures and must remain reliable, particularly against deliberate attacks. To minimize risks and impede remote compromise, sensitive systems can be physically isolated from external networks, forming an airgap. Yet, airgaps can still be infiltrated by capable adversaries gaining code execution. Prior research has shown that attackers can then attempt to wirelessly exfiltrate data across the airgap by exploiting unintended radio emissions. In this work, we demonstrate reversal of this link: malicious code execution on embedded devices can enable wireless infiltration of airgapped systems without any hardware modification. In contrast to previous infiltration methods that depend on dedicated sensors (e.g., microphones, LEDs, or temperature sensors) or require strict line-of-sight, we show that unmodified, sensor-less embedded devices can inadvertently act as radio receivers. This phenomenon stems from parasitic RF sensitivity in PCB traces and on-chip analog-to-digital converters (ADCs), allowing external transmissions to be received and decoded entirely in software.
  Across twelve commercially available embedded devices and two custom prototypes, we observe repeatable reception in the 300-1000 MHz range, with detectable signal power as low as 1 mW. To this end, we propose a systematic methodology to identify device configurations that foster such radio sensitivities and comprehensively evaluate their feasibility for wireless data reception. Exploiting these sensitivities, we demonstrate successful data reception over tens of meters, even in non-line-of-sight conditions and show that the reception sensitivities accommodate data rates of up to 100 kbps. Our findings reveal a previously unexplored command-and-control vector for air-gapped systems while challenging assumptions about their inherent isolation. [shortened]

</details>


### [25] [ComMark: Covert and Robust Black-Box Model Watermarking with Compressed Samples](https://arxiv.org/abs/2512.15641)
*Yunfei Yang,Xiaojun Chen,Zhendong Zhao,Yu Zhou,Xiaoyan Gu,Juan Cao*

Main category: cs.CR

TL;DR: 提出 ComMark，一种基于频域变换的黑盒模型水印框架。通过压缩与去高频实现隐蔽且抗攻击的水印样本，结合仿真攻击与相似性损失训练，达到 covertness 与 robustness 的 state-of-the-art，并扩展至多模态任务。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型成为宝贵资产，因数据与训练成本高，模型易被窃取或泄露，因此需要强有力的知识产权保护手段。现有黑盒水印在隐蔽性与鲁棒性之间难以兼顾，亟需更稳健、隐蔽的水印方法。

Method: 提出基于频域变换的水印生成框架 ComMark，通过对输入/模型输出进行高效的频域过滤，生成压缩、隐蔽的水印样本；在训练阶段引入仿真攻击场景和相似性损失以提升鲁棒性；属于黑盒水印方法。

Result: 在多数据集与多种体系结构上实现了在隐蔽性与鲁棒性上的新基线，达到 state-of-the-art 水平；并且将水印方法扩展至语音识别、情感分析、图像生成、图像描述与视频识别等多模态任务，展示了广泛的适用性。

Conclusion: ComMark 提供了一种高隐蔽性、强鲁棒性的黑盒水印解决方案，适用于广泛的应用场景，能够有效用于版权验证与模型保护。

Abstract: The rapid advancement of deep learning has turned models into highly valuable assets due to their reliance on massive data and costly training processes. However, these models are increasingly vulnerable to leakage and theft, highlighting the critical need for robust intellectual property protection. Model watermarking has emerged as an effective solution, with black-box watermarking gaining significant attention for its practicality and flexibility. Nonetheless, existing black-box methods often fail to better balance covertness (hiding the watermark to prevent detection and forgery) and robustness (ensuring the watermark resists removal)-two essential properties for real-world copyright verification. In this paper, we propose ComMark, a novel black-box model watermarking framework that leverages frequency-domain transformations to generate compressed, covert, and attack-resistant watermark samples by filtering out high-frequency information. To further enhance watermark robustness, our method incorporates simulated attack scenarios and a similarity loss during training. Comprehensive evaluations across diverse datasets and architectures demonstrate that ComMark achieves state-of-the-art performance in both covertness and robustness. Furthermore, we extend its applicability beyond image recognition to tasks including speech recognition, sentiment analysis, image generation, image captioning, and video recognition, underscoring its versatility and broad applicability.

</details>


### [26] [Distributed HDMM: Scalable, Distributed, Accurate, and Differentially Private Query Workloads without a Trusted Curator](https://arxiv.org/abs/2512.15648)
*Ratang Sedimo,Ivoline C. Ngong,Jami Lashua,Joseph P. Near*

Main category: cs.CR

TL;DR: 提出分布式高维矩阵机制（Distributed HDMM），在没有可信托管者的情况下，通过安全聚合实现中心模型HDMM的准确性，用于分布式数据的线性查询工作负载。对恶意聚合者和恶意客户端在多数诚实的前提下提供安全性，初步实验在现实数据与数千客户端下耗时不足1分钟。


<details>
  <summary>Details</summary>
Motivation: 中央模型HDMM需要信任的托管者来聚合数据并添加噪声以保护隐私，但在现实场景中难以获得或不希望有单一可信实体。分布式环境需要在无托管者的前提下实现等效的准确性，同时具备对恶意参与者的鲁棒性和可扩展性。

Method: 整合一个安全聚合协议，将分布在不同客户端的数据在不暴露原始数据的前提下进行HDMM计算；在假设诚实多数的前提下，抵御恶意聚合者和恶意客户端对结果的操控；通过分布式噪声管理和对线性查询工作负载的优化实现HDMM在分布式环境中的等效性。

Result: 初步的经验评估表明，在现实数据集与包含数千客户端的工作负载下，该协议能够在不到1分钟的时间内完成计算，且达到与中心化HDMM相近的准确性。

Conclusion: Distributed HDMM在没有可信托管者的情况下实现了中心化HDMM的准确性，具备良好的可扩展性和对恶意参与者的鲁棒性，适合大规模分布式数据的隐私保护分析。

Abstract: We present the Distributed High-Dimensional Matrix Mechanism (Distributed HDMM), a protocol for answering workloads of linear queries on distributed data that provides the accuracy of central-model HDMM without a trusted curator. Distributed HDMM leverages a secure aggregation protocol to evaluate HDMM on distributed data, and is secure in the context of a malicious aggregator and malicious clients (assuming an honest majority). Our preliminary empirical evaluation shows that Distributed HDMM can run on realistic datasets and workloads with thousands of clients in less than one minute.

</details>


### [27] [BashArena: A Control Setting for Highly Privileged AI Agents](https://arxiv.org/abs/2512.15688)
*Adam Kaufman,James Lucassen,Tyler Tracy,Cody Rushing,Aryan Bhatt*

Main category: cs.CR

TL;DR: 提出 BashArena 作为安全关键环境中 AI 控制研究的基准，包含637项Linux任务及四类破坏目标，用于评估前沿语言模型在任务完成、隐蔽破坏和监控规避能力，给出基线并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 随着智能体可能获得提升权限，若未对齐，可能滥用权力造成严重损害，因此需要开发控制技术来降低风险同时保留有用性；BashArena 旨在现实场景中系统地研究这类控制方法。

Method: 构建 BashArena：637个复杂的Linux系统管理与基础设施任务，分布在复杂、现实化环境中，另外设定四个破坏目标（执行恶意软件、窃取机密、提升权限、禁用防火墙）供红队挑战，评估多家前沿大模型在完成任务、未被侦测地进行破坏以及侦测破坏方面的能力。通过轨迹式评估并报告任务完成率、破坏成功率与监控检测的假阳性率（FPR）。

Result: 研究发现 Claude Sonnet 4.5 能在一定条件下成功执行破坏并规避监控，在 GPT-4.1 mini 下有约 26% 的规避率；轨迹相关假阳性率约为 4%。研究为设计更有效的控制协议提供基线，并将数据集以 ControlArena 形式发布，同时公布任务生成管线。

Conclusion: BashArena 为未来在安全关键环境中设计和评估 AI 控制技术提供基线，推动对抗性与监控能力的研究，并通过数据集与管线的公开，促进后续工作。

Abstract: Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [28] [Low-Complexity Channel Estimation for Internet of Vehicles AFDM Communications With Sparse Bayesian Learning](https://arxiv.org/abs/2512.14776)
*Xiangxiang Li,Haiyan Wang,Yao Ge,Xiaohong Shen,Miaowen Wen,Shun Zhang,Yong Liang Guan*

Main category: cs.IT

TL;DR: 提出基于稀疏贝叶斯学习（SBL）的 AFDM 通道估计框架，并引入两种离网(off-grid)估计方法：GR-SBL（网格细化）和 GE-SBL（网格演化），并提出分布式变体 D-GR-SBL 和 D-GE-SBL 以降低计算复杂度。仿真结果显示所提方法优于现有方案，GR-SBL 精度最高但代价较高，GE-SBL 在性能与复杂度之间提供更好折中，分布式方案在保持可比性能的同时显著降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 在双多普勒/色散信道下，AFDM 的通道估计需要高精度以满足高可靠性通信的要求，但现有方法在估计偏移与计算复杂度方面面临挑战。引入 off-grid 的 SBL 框架，以提高对多径与时频漂移的建模能力，并通过分布式实现提升可扩展性和实时性。

Method: 提出一个基于稀疏贝叶斯学习的 AFDM 通道估计框架，并设计两种 off-grid 方案：GR-SBL 通过局部网格细化实现高精度；GE-SBL 通过一阶线性近似实现网格向前演化，兼顾性能与复杂度。并提出分布式实现 D-GR-SBL/ D-GE-SBL，将大维度模型分解为若干小模型以支持并行处理，降低计算延迟。

Result: 仿真显示提议的通道估计方案在性能上优于现有竞争方案。GR-SBL 在网格细化下实现高精度估计，但计算复杂度较高；GE-SBL 提供更优的复杂度-性能折中。分布式版本 D-GR-SBL 与 D-GE-SBL 能显著降低复杂度，同时保持与原始 GR-SBL/GE-SBL 相当的性能。

Conclusion: 提出的 SBL-AFDM 框架有效解决了双多普勒通道下的离网估计问题，GR-SBL 提供最高精度但成本较高，GE-SBL 提供更佳的折中，分布式实现实现了可扩展性与实时性，适合在高要求的应用场景如物联网/车联网中部署。

Abstract: Affine frequency division multiplexing (AFDM) has been considered as a promising waveform to enable high-reliable connectivity in the internet of vehicles. However, accurate channel estimation is critical and challenging to achieve the expected performance of the AFDM systems in doubly-dispersive channels. In this paper, we propose a sparse Bayesian learning (SBL) framework for AFDM systems and develop a dynamic grid update strategy with two off-grid channel estimation methods, i.e., grid-refinement SBL (GR-SBL) and grid-evolution SBL (GE-SBL) estimators. Specifically, the GR-SBL employs a localized grid refinement method and dynamically updates grid for a high-precision estimation. The GE-SBL estimator approximates the off-grid components via first-order linear approximation and enables gradual grid evolution for estimation accuracy enhancement. Furthermore, we develop a distributed computing scheme to decompose the large-dimensional channel estimation model into multiple manageable small-dimensional sub-models for complexity reduction of GR-SBL and GE-SBL, denoted as distributed GR-SBL (D-GR-SBL) and distributed GE-SBL (D-GE-SBL) estimators, which also support parallel processing to reduce the computational latency. Finally, simulation results demonstrate that the proposed channel estimators outperform existing competitive schemes. The GR-SBL estimator achieves high-precision estimation with fine step sizes at the cost of high complexity, while the GE-SBL estimator provides a better trade-off between performance and complexity. The proposed D-GR-SBL and D-GE-SBL estimators effectively reduce complexity and maintain comparable performance to GR-SBL and GE-SBL estimators, respectively.

</details>


### [29] [Rotatable IRS-Assisted 6DMA Communications: A Two-timescale Design](https://arxiv.org/abs/2512.15092)
*Chao Zhou,Changsheng You,Cong Zhou,Liujia Yao,Weijie Yuan,Beixiong Zheng,Nan Wu*

Main category: cs.IT

TL;DR: 提出在六维移动天线（6DMA）基站和可旋转IRS（R-IRS）的多功能天线/表面系统，采用两时相传输协议进行联合优化，在单/多用户场景下显著提升平均总吞吐量（sum-rate）并降低通道估计复杂度。


<details>
  <summary>Details</summary>
Motivation: IRS与移动天线各自存在实际限制，难以单独实现理想性能。通过结合6DMA的灵活波束与R-IRS对环境的可控反射，可在统计CSI和瞬时CSI分离的两时尺度下提升链路质量与覆盖范围，降低实时通道估计与波束赋形的开销。

Method: 建立一个两时尺度（TTS）优化框架：外层基于统计CSI（S-CSI）对基站天线配置（位置/旋转）与IRS的旋转与反射进行优化，内层基于瞬时CSI（I-CSI）进行BS发射波束赋形。首先在单用户情形分析6DMA应形成稀疏阵列以实现对IRS和用户的多波束传输并实现直接/反射信道的高效聚焦；针对多用户的非凸优化，提出结合加权MMSE（WMMSE）与随机一致(convex)近似（SSCA）的高效算法，并给出低复杂度版本。

Result: 数值仿真验证了所提系统的有效性，显示在TTS协议下通过联合利用6DMA-BS与R-IRS的空间自由度，可以获得显著的吞吐量增益。

Conclusion: 通过在空间域对6DMA-BS与R-IRS进行协同优化，并利用两时尺度协同估计，本文实现了更高的下行性能与更优的资源分配，同时降低了实时通道估计难度，适合未来低延迟和高效能的无线系统。

Abstract: Intelligent reflecting surface (IRS) and movable antenna (MA) are promising technologies to enhance wireless communication by reconfiguring channels at the environment and transceiver sides. However, their performance is constrained by practical limitations. To address this, we propose a multi-functional antenna/surface system that leverages their complementary advantages. A rotatable IRS (R-IRS) is deployed to enhance downlink communications from a six-dimensional MA (6DMA)-equipped base station (BS) to multiple single-antenna users. To reduce the complexity of real-time channel estimation and beamforming, we formulate an optimization problem to maximize the average sum-rate using a two-timescale (TTS) transmission protocol. Specifically, the BS antenna configuration (including position and rotation) and IRS rotation and reflection are optimized based on statistical channel state information (S-CSI), while BS transmit beamforming is designed using instantaneous CSI (I-CSI) in the short timescale. We first consider a single-user case and show that the 6DMA at the BS should form a sparse array for multi-beam transmission towards both the IRS and the user, allowing efficient coordination of direct and reflected channels, while the IRS rotation achieves effective multi-path alignment. For the general multi-user case, the optimization problem is non-convex and challenging to solve. To tackle this, we propose an efficient algorithm combining weighted minimum mean-square error (WMMSE) and stochastic successive convex approximation (SSCA) techniques. A low-complexity algorithm is also proposed to reduce computational complexity. Numerical results validate the proposed system, showing significant performance gains by jointly exploiting the spatial degrees of freedom of the 6DMA-BS and R-IRS under the TTS protocol.

</details>


### [30] [Sparse Principal Component Analysis with Energy Profile Dependent Sample Complexity](https://arxiv.org/abs/2512.15191)
*Mengchu Xu,Jian Wang,Yonina C. Eldar*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study sparse principal component analysis in the high-dimensional, sample-limited regime, aiming to recover a leading component supported on a few coordinates. Despite extensive progress, most methods and analyses are tailored to the flat-spike case, offering little guidance when spike energy is unevenly distributed across the support. Motivated by this, we propose Spectral Energy Pursuit (SEP), an effective iterative scheme that repeatedly screens and reselects coordinates, with a sample complexity that adapts to the energy profile. We develop our framework around a structure function \(s(p)\) that quantifies how spike energy accumulates over its top \(p\) entries. We establish that SEP succeeds with a sample size of order \(\max_{1\le p\le k} p\,s^2(p)\,\log n\), which matches the classical \(k^2\log n\) sample complexity for flat spikes and improves toward the \(k\log n\) regime as the profile becomes more concentrated. As a lightweight post-processing, a single truncated power iteration is proven to enable the final estimator to attain a uniform statistical error bound. Empirical simulations across flat, power-law, and exponential signals validate that SEP adapts to profile structure without tuning and outperforms existing algorithms.

</details>


### [31] [Variational Robust Kalman Filters: A Unified Framework](https://arxiv.org/abs/2512.15419)
*Shilei Li,Dawei Shi,Hao Yu,Ling Shi*

Main category: cs.IT

TL;DR: 提出一个统一的鲁棒卡尔曼滤波器，基于Student's t分布损失与变分推断，在固定点迭代下实现，兼容普通KF、鲁棒KF与自适应KF，并在复杂噪声环境中表现优越。


<details>
  <summary>Details</summary>
Motivation: 鲁棒性和自适应性在卡尔曼滤波中通常相互矛盾。现实应用中的过程与测量噪声可能因离群值、时变性而复杂化，需在同一框架下兼顾两者并处理复杂噪声。

Method: 提出一个统一的变分鲁棒卡尔曼滤波器，利用基于Student's t分布的损失函数并通过变分推断进行估计，采用固定点迭代实现高效计算。通过切换规则将鲁棒性作为自适应的前提，从而在同一框架内切换实现普通KF、鲁棒KF与自适应KF。该滤波器能够抑制不理想的过程和测量噪声，使在复杂噪声环境中具有更强的鲁棒性与适应性。

Result: 理论与仿真实验表明，鲁棒性是实现自适应的前提，所提出的框架可通过调节参数在普通KF、鲁棒KF、自适应KF之间切换，且在含离群点和时变噪声的场景下表现优于传统滤波方法。

Conclusion: 给出一个统一的滤波框架，将鲁棒性和自适应性有机结合，通过切换规则和变分推断实现对复杂噪声环境的鲁棒处理与自适应调整，适用于需要在噪声特性随时间变化的应用场景。

Abstract: Robustness and adaptivity are two competing objectives in Kalman filters (KF). Robustness involves temporarily inflating prior estimates of noise covariances, while adaptivity updates prior beliefs using real-time information. In practical applications, both process and measurement noise can be influenced by outliers, be time-varying, or both. Existing works may not effectively address the above complex noise scenarios, as there is an intrinsic incompatibility between robust filters and adaptive filters. In this work, we propose a unified variational robust Kalman filter, built on a Student's t-distribution induced loss function and variational inference, and solved through fixed-point iteration in a computationally efficient manner. We demonstrate that robustness can be understood as a prerequisite for adaptivity, making it possible to merge the above two competing goals into a single framework through switching rules. Additionally, our proposed filter can recover conventional KF, robust KF, and adaptive KF by adjusting parameters, and can suppress both the imperfect process and measurement noise, enabling it to perform superiorly in complex noise environments. Simulations verify the effectiveness of the proposed method.

</details>


### [32] [An Anti-Interference AFDM System: Interference Impacts Analyses and Parameter Optimization](https://arxiv.org/abs/2512.15425)
*Peng Yuan,Zulin Wang,Tao Luo,Yuanhan Ni*

Main category: cs.IT

TL;DR: 提出一种抗干扰AFDM系统，在高移动场景下对来自对手设备的高功率干扰实现可靠性与资源效率。通过DAFT域的干扰闭式表达式、干扰分类（定态/非定态）以及散射谱与编码参数对吞吐的影响关系，设计参数优化算法以最大化分组吞吐；并提出线性复杂度的DAFT域相关检测器（CDD），利用自相关和AFDM输入输出的循环移位特性实现无矩阵求逆的相关等化，实验验证了理论表达式的准确性与高吞吐。


<details>
  <summary>Details</summary>
Motivation: 在高移动场景下，面对来自对手设备的高功率干扰，需提升系统的可靠性与资源利用率；通过在DAFT域对干扰进行刻画并结合扩频与纠错编码优化参数，提升吞吐并设计低复杂度检测方案以实现全通道分集增益。

Method: 利用静态相位原理和Affine傅里叶变换卷积定理，在离散DAFT域推导干扰的闭式表达，将干扰分为定态与非定态；建立分组吞吐率与扩频参数、编码参数之间的解析关系，并提出一个参数优化算法以最大化吞吐。接收端通过利用扩频序列的自相关性和AFDM输入输出关系的循环移位性质，设计线性复杂度的DAFT域相关检测器（CDD），实现相关性等化以避免矩阵求逆并获得全分集增益。

Result: 数值结果验证了推导出的闭式表达式的正确性，并证明所提抗干扰AFDM在高移动场景下能够在干扰存在时实现较高的分组吞吐率；检测器实现了线性复杂度并达到全分集增益，实验支持理论分析。

Conclusion: 本文提出的抗干扰AFDM系统通过理论推导、参数优化和低复杂度检测，实现在高移动且存在高功率干扰的场景下的高吞吐与可靠性；该框架为在对手干扰环境中提升AFDM系统性能提供了有效的分析与设计工具。

Abstract: This paper proposes an anti-interference affine frequency division multiplexing (AFDM) system to ensure reliability and resource efficiency under malicious high-power interference originating from adversarial devices in high-mobility scenarios. Closed-form expressions of interferences in the discrete affine Fourier transform (DAFT) domain are derived by utilizing the stationary phase principle and the Affine Fourier transform convolution theorem, which indicates that interference impacts can be classified into stationary and non-stationary categories. On this basis, we reveal the analytical relationship between packet throughput and the paramerters of spread spectrum and error correction coding in our proposed anti-interference system, which enables the design of a parameter optimization algorithm that maximizes packet throughput. For reception, by jointly utilizing the autocorrelation function of spreading sequence and the cyclic-shift property of AFDM input-output relation, we design a linear-complexity correlation-based DAFT domain detector (CDD) capable of achieving full diversity gain, which performs correlation-based equalization to avoid matrix inversion. Numerical results validate the accuracy of the derived closed-form expressions and verify that the proposed anti-interference AFDM system could achieve high packet throughput under interference in high-mobility scenarios.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [33] [Remote Magnetic Levitation Using Reduced Attitude Control and Parametric Field Models](https://arxiv.org/abs/2512.15207)
*Neelaksh Singh,Jasan Zughaibi,Denis von Arx,Bradley J. Nelson,Michael Muehlebach*

Main category: eess.SY

TL;DR: 通过 OctoMag 系统实现对一个刚体在较大空气间隙中的远程悬浮与控制，提出基于参数化解析模型的 eMNS，去除对仿真或查找表的依赖，并结合线性二次调控与非线性时不变控制实现多自由度控制。


<details>
  <summary>Details</summary>
Motivation: 在微创治疗场景中，需要快速、精准的磁场控制来实现远程导航与定位；现有方法对仿真复杂性和实时性存在挑战，且需在大空气间隙中保持稳定性。

Method: 构建紧凑的参数化解析模型，将线圈电流映射到作用于悬浮对象的力和力矩； translational 采用线性二次调控器（LQR）实现稳定；降维后使用非线性时不变控制器调控降维的姿态，处理自转轴的不可控性并稳定五自由度中的可控位姿子空间；分析设计瓶颈并通过轨迹跟踪实验验证。

Result: 证明了在大空气间隙中的远程悬浮与控制的动态能力；通过轨迹跟踪实验验证所提出模型与控制策略的有效性，并讨论了关键设计局限性。

Conclusion: 该工作展示了 eMNS 的动态能力与潜力，基于反馈控制的方法有望开启新的医学应用场景。

Abstract: Electromagnetic navigation systems (eMNS) are increasingly used in minimally invasive procedures such as endovascular interventions and targeted drug delivery due to their ability to generate fast and precise magnetic fields. In this paper, we utilize the OctoMag eMNS to achieve remote levitation and control of a rigid body across large air gaps which showcases the dynamic capabilities of clinical eMNS. A compact parametric analytical model maps coil currents to the forces and torques acting on the levitating object, eliminating the need for computationally expensive simulations or lookup tables and leading to a levitator agnostic modeling approach. Translational motion is stabilized using linear quadratic regulators. A nonlinear time-invariant controller is used to regulate the reduced attitude accounting for the inherent uncontrollability of rotations about the dipole axis and stabilizing the full five degrees of freedom controllable pose subspace. We analyze key design limitations and evaluate the approach through trajectory tracking experiments. This work demonstrates the dynamic capabilities and potential of feedback control in electromagnetic navigation, which is likely to open up new medical applications.

</details>


### [34] [Ising Machines for Model Predictive Path Integral-Based Optimal Control](https://arxiv.org/abs/2512.15533)
*Lorin Werthen-Brabants,Pieter Simoens*

Main category: eess.SY

TL;DR: 将 MPC 转换为 QUBO 并通过 Ising 机器实现，利用 Gibbs 采样从能量景观中进行近似最优控制轨迹搜索，提升实时性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 提高实时控制中高维系统的计算效率与鲁棒性，通过将 MPPI 融入 Ising/概率计算框架，以实现更高效的轨迹采样与优化。

Method: 将模型预测控制（MPC）问题表达为二次无约束二进制优化（QUBO），映射到 Ising 模型的能量函数，利用 Gibbs 采样在 Ising 体系中探索轨迹；以 MPPI 为框架，进行随机采样、轨迹评估与控制决策。

Result: 与参考的 MPPI 实现相比，轨迹跟踪更准确，证明了 Ising 基础的 MPPI 在实时控制中的潜力。

Conclusion: 将 MPPI 与 Ising 机器结合，提供一种适用于新型概率计算的控制方法，通过能量景观中的采样实现高效搜索近似最优轨迹，适用于机器人和自主系统的实时控制。

Abstract: We present a sampling-based Model Predictive Control (MPC) method that implements Model Predictive Path Integral (MPPI) as an \emph{Ising machine}, suitable for novel forms of probabilistic computing. By expressing the control problem as a Quadratic Unconstrained Binary Optimization (QUBO) problem, we map MPC onto an energy landscape suitable for Gibbs sampling from an Ising model. This formulation enables efficient exploration of (near-)optimal control trajectories. We demonstrate that the approach achieves accurate trajectory tracking compared to a reference MPPI implementation, highlighting the potential of Ising-based MPPI for real-time control in robotics and autonomous systems.

</details>


### [35] [Scheduling the Charge of Temporally Flexible Electric Vehicles: a Market-based Approach](https://arxiv.org/abs/2512.15583)
*Sabri El Amrani,Thibaut Horel,Saurabh Vaishampayan,Maryam Kamgarpour,Munther A. Dahleh*

Main category: eess.SY

TL;DR: 提出以电动汽车电池作为储能手段，通过充电调度和激励机制提升车对电网服务，并缓解oversubscribed充电站拥堵；通过混合整数二次规划求解，采用ADMM近似，并用Vickrey–Clarke–Groves机制确保 truthful reporting，利用真实数据进行仿真实验评估收益。


<details>
  <summary>Details</summary>
Motivation: 随着人类活动的电气化加速和可再生能源波动性增加，电网对储能需求上升；电动汽车具备充电弹性，可作为后备容量与需求响应来源。然而，司机在需要其电池时可能中断放电/充电，且充电站易拥堵，因此需要一个能鼓励司机延迟断开的机制并缓解拥堵的调度策略，同时确保信息披露的真实性。

Method: 将最佳可变时间表的计算建模为混合整数二次规划（MIQP），通过交替方向乘子法（ADMM）对求解进行分解性近似；考虑策略性司机可能虚报偏好，提出Vickrey–Clarke–Groves（VCG）机制以激励诚实申报；通过基于真实世界数据的仿真案例研究评估方法与激励在提升车网协同效应方面的量级。

Result: 仿真结果表明，司机时间弹性对于提升车对电网服务（如峰谷波动缓解、容量提供）和降低充电站拥堵具有可观的价值；所提出的调度与激励组合能在一定程度上实现资源的有效利用与需求约束的缓解。

Conclusion:  temporal flexibility（时间弹性）结合激励机制可以显著提升车网协同效应，增强对电网的支撑能力并缓解充电站拥堵；未来工作可聚焦现实部署、鲁棒性分析以及不同充电市场的制度设计。

Abstract: The increasing electrification of human activities and the rapid integration of variable renewable energy sources strain the power grid. A solution to address the need for more grid storage is to use the battery of electric vehicles as a back-up capacity. However, drivers tend to disconnect their electric vehicle when its battery is needed the most. We propose a charge scheduler that incentivizes drivers to delay their disconnection to improve vehicle-to-grid services. We also leverage drivers' temporal flexibility to alleviate congestion in oversubscribed charging stations. We formulate the computation of an optimal flexible schedule as a mixed-integer quadratic problem. We tractably approximate its solution using the Alternating Direction Method of Multipliers. Considering the possibility that strategic drivers misreport their charging preferences to the station coordinator, we then propose a Vickrey-Clarke-Groves mechanism that incentivizes truthful reporting. We conclude with a simulated case study using real-world data to quantitatively assess the added value of drivers' temporal flexibility for enhancing vehicle-to-grid services and reducing station congestion.

</details>


### [36] [Enhancing industrial microalgae production through Economic Model Predictive Control](https://arxiv.org/abs/2512.15668)
*Pablo Otálora,Sigurd Skogestad,José Luis Guzmán,Manuel Berenguel*

Main category: eess.SY

TL;DR: 通过经济模型预测控制（EMPC）框架优化微藻产业过程，在气候变化情景下实现经济最优与动态稳定，优于传统运行。


<details>
  <summary>Details</summary>
Motivation: 微藻工业生产高度非线性、随时间变化，建模、控制和优化困难，直接影响成本、产出和竞争力。需要一种统一的决策框架来提升经济性与稳定性。

Method: 提出一个经济模型预测控制器，集中化决策，并在不同气候情景下与常规工业运行进行对比，验证其理论最优操作与动态稳定性。

Result: 实现了经济优化和过程的动态稳定，提供了工业层面的优先级洞察，支持在实际操作中优先采用最优控制策略而非传统运营。

Conclusion: 在微藻工业生产中，EMPC可提升经济性与鲁棒性，特别是在气候变化条件下，展示了比传统操作更优的控制效果与决策支持。

Abstract: The industrial production of microalgae is an important and sustainable process, but its actual competitiveness is closely related to its optimization. The biological nature of the process hinders this task, mainly due to the high nonlinearity of the process along with its changing nature, features that make its modeling, control and optimization remarkably challenging. This paper presents an economic optimization framework aiming to enhance the operation of such systems. An Economic Model Predictive Controller is proposed, centralizing the decision making and achieving the theoretical optimal operation. Different scenarios with changing climate conditions are presented, and a comparison with the typical, non-optimized industrial process operation is established. The obtained results achieve economic optimization and dynamic stability of the process, while providing some insight into the priorities during process operation at industrial level, and justifying the use of optimal controllers over traditional operation.

</details>


### [37] [Service-Oriented Fast Frequency Response from Flexible Loads and Energy Storage in Low-Inertia Power Systems](https://arxiv.org/abs/2512.15677)
*Xiaojie Tao,Rajit Gadh*

Main category: eess.SY

TL;DR: Proposes a service-oriented framework to coordinate fast frequency response (FFR) from flexible loads and energy storage across multiple time scales, prioritizing ultra-fast resources for initial frequency arrest and slower, energy-rich resources for sustainment.


<details>
  <summary>Details</summary>
Motivation: Growing penetration of inverter-based generation reduces system inertia, making grids more vulnerable to rapid frequency deviations; existing work focuses on individual resources or controller-level designs, lacking a system-level deployment framework.

Method: Decomposes FFR into layered, time-critical services based on response speed, power capacity, and energy sustainability; dynamically allocates responsibilities among heterogeneous resources; explicitly accounts for latency, saturation limits, and energy constraints to enable coordinated dispatch.

Result: Conceptual framework proposing layered service allocation and prioritization to translate heterogeneous FFR capabilities into deployable grid services; no empirical validation reported in the abstract.

Conclusion: The framework provides a systematic bridge between capability assessment and grid operation, enabling coordinated FFR across heterogeneous resources and potentially improving frequency stability.

Abstract: The increasing penetration of inverter-based renewable generation has significantly reduced system inertia, making modern power grids more vulnerable to rapid frequency deviations following disturbances. While a wide range of flexible resources-including electric vehicles (EVs), data centers, and battery energy storage systems (BESS)-have demonstrated the physical capability to provide fast frequency response (FFR), existing studies primarily focus on individual resource performance or controller-level designs. A systematic framework that translates heterogeneous FFR capabilities into deployable, system-level frequency services remains largely unexplored. This paper proposes a service-oriented coordination framework for fast frequency response from flexible loads and energy storage, bridging the gap between physical capability assessment and grid-operational utilization. The framework decomposes frequency support into multiple time-critical service layers based on response speed, power capacity, and energy sustainability, and dynamically allocates FFR responsibilities among heterogeneous resources accordingly. By explicitly accounting for response latency, saturation limits, and energy constraints, the proposed approach enables coordinated dispatch that prioritizes ultra-fast resources for initial frequency arrest while leveraging slower but energy-rich resources to sustain recovery.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [38] [GenAI-enabled Residual Motion Estimation for Energy-Efficient Semantic Video Communication](https://arxiv.org/abs/2512.15481)
*Shavbo Salehi,Pedro Enrique Iturria-Rivera,Medhat Elsayed,Majid Bavand,Yigit Ozcan,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 提出PENME框架：在语义视频传输中通过可预测性与熵自适应的运动估计与选择性扩散来实现低时延、低数据量与高语义保真度的传输。


<details>
  <summary>Details</summary>
Motivation: 香农范式在高带宽和高功耗场景下的资源浪费与视频传输的高要求，促使研究转向以“意义”为中心的语义通信以降低资源开销，同时保持质量。

Method: PENME在逐帧层面根据五步策略（运动强度、全局运动一致性、峰值清晰、异质性、残差误差）在残差运动提取模型、CNN、ViT或光流之间进行选择；将残差运动发送给接收端并通过运动补偿更新重建；对低可预测或残差大的帧应用Latent Consistency Model (LCM-4)进行选择性扩 refinement；并基于残差运动和信道状态进行无线资源块分配以降低能耗与带宽；在Vimeo90K数据集对比传统、混合和自适应比特率语义通信，显示显著改进。

Result: 在多类型视频上实现了显著的时延降低、传输数据量下降和吞吐提升，并在语义指标方面优于基线：约40%时延降低、90%数据量减少、35%吞吐提升；PSNR提升约40%、MS-SSIM提升约19%、LPIPS下降约35%。

Conclusion: 通过PENME实现高效的语义视频传输，证明基于可预测性和残差自适应策略在资源受限和高延迟场景下能显著提升资源利用率与语义质量。

Abstract: Semantic communication addresses the limitations of the Shannon paradigm by focusing on transmitting meaning rather than exact representations, thereby reducing unnecessary resource consumption. This is particularly beneficial for video, which dominates network traffic and demands high bandwidth and power, making semantic approaches ideal for conserving resources while maintaining quality. In this paper, we propose a Predictability-aware and Entropy-adaptive Neural Motion Estimation (PENME) method to address challenges related to high latency, high bitrate, and power consumption in video transmission. PENME makes per-frame decisions to select a residual motion extraction model, convolutional neural network, vision transformer, or optical flow, using a five-step policy based on motion strength, global motion consistency, peak sharpness, heterogeneity, and residual error. The residual motions are then transmitted to the receiver, where the frames are reconstructed via motion-compensated updates. Next, a selective diffusion-based refinement, the Latent Consistency Model (LCM-4), is applied on frames that trigger refinement due to low predictability or large residuals, while predictable frames skip refinement. PENME also allocates radio resource blocks with awareness of residual motion and channel state, reducing power consumption and bandwidth usage while maintaining high semantic similarity. Our simulation results on the Vimeo90K dataset demonstrate that the proposed PENME method handles various types of video, outperforming traditional communication, hybrid, and adaptive bitrate semantic communication techniques, achieving 40% lower latency, 90% less transmitted data, and 35% higher throughput. For semantic communication metrics, PENME improves PSNR by about 40%, increases MS-SSIM by roughly 19%, and reduces LPIPS by nearly 35%, compared with the baseline methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts](https://arxiv.org/abs/2512.14706)
*Krunal Jesani,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TL;DR: LLM-guided NAS pipeline NN-Caption生成可运行的图像描述模型，组合LEMUR骨干的CNN编码器与序列解码器（LSTM/GRU/Transformer），在严格的Net API约束下实现。以DeepSeek-R1-0528-Qwen3-8B为主生成器，并给出提示模板与示例。对MS COCO数据集采用BLEU-4评估，结果显示生成了多种模型，超过一半可训练并产出有意义的描述；对提示中的输入组件数量（5 vs. 10）的影响进行分析，发现提供更多候选组件时成功率略有下降；给出训练过程曲线与最高BLEU-4值。研究表明LLM-guided NAS具有潜力，LLM不仅提出架构，还给出超参数与训练策略，同时指出代码幻觉和API合规等挑战，并通过提示规则与迭代代码修正来解决。该工作将提示生成与自动评估结合成一个可复现的管线，并向开源LEMUR数据集添加大量新模型以促进基准和AutoML研究。


<details>
  <summary>Details</summary>
Motivation: 降低神经架构搜索的成本与专业知识门槛，利用大型语言模型引导架构设计与超参数选择，提高可复现性与AutoML研究的可扩展性。

Method: 使用基于深度提示的代码生成，采用DeepSeek-R1-0528-Qwen3-8B作为主生成器，结合严格的Net API约束从LEMUR分类骨干中挑选CNN编码器并缀以序列解码器（LSTM/GRU/Transformer）实现图像描述模型；设计提示模板和示例，做5或10个输入组件的比较，训练并在MS COCO上以BLEU-4评估，记录训练动力学与最高BLEU-4，并报告对API合规与代码幻觉的处理策略。

Result: 生成多份图像描述模型，超过一半可成功训练并产出有意义的描述；5 vs 10输入组件对成功率有影响，更多候选组件略降；报告了训练动态与最高BLEU-4；对开源LEMUR数据集添加大量新模型，促进可复现基准与AutoML研究。

Conclusion: LLM-guided NAS展现出潜在价值，LLM能提出架构、超参数与训练实践；需解决代码幻觉与API合规等挑战，借助提示规则与迭代修正可缓解。该管线将提示生成与自动评估结合，提升可复现性并扩展AutoML研究。

Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.

</details>


### [40] [Autonomous Source Knowledge Selection in Multi-Domain Adaptation](https://arxiv.org/abs/2512.14710)
*Keqiuyin Li,Jie Lu,Hua Zuo,Guangquan Zhang*

Main category: cs.LG

TL;DR: AutoS automates source sample and model selection for unsupervised multi-domain adaptation using density-driven strategy and pseudo-label enhancement to improve target task performance.


<details>
  <summary>Details</summary>
Motivation: In massive-source domain settings, many source domains contain redundant or unrelated information that can harm transfer performance, necessitating identification and selection of transferable knowledge.

Method: Density-driven selection to choose source samples during training and to decide which source models should contribute to target prediction; a pseudo-label enhancement module built on a pre-trained multimodal model to mitigate target label noise and strengthen self-supervision.

Result: Experiments on real-world datasets demonstrate the superiority of AutoS over baselines.

Conclusion: Autonomous source knowledge selection improves transfer learning efficiency and effectiveness in unsupervised multi-domain adaptation.

Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.

</details>


### [41] [SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI](https://arxiv.org/abs/2512.14712)
*Ryan Cartularo*

Main category: cs.LG

TL;DR: 对脓毒症多模态预测的端到端深度融合与上下文感知堆叠进行系统比较，揭示在小样本下端到端注意力易过拟合；以上下文感知MoE为核心的SepsisLateFusion实现SOTA并发布SepsisSuite。


<details>
  <summary>Details</summary>
Motivation: 脓毒症预测需整合生理数据、文本信息和影像等异质数据；现有方法往往在模态间融合上要么单一模态，要么使用脆弱的早期融合，缺乏对跨模态交互的鲁棒性研究。因此有必要比较不同融合架构，并探索在现实数据规模下的可扩展、可部署解。

Method: 在MIMIC-IV数据集上比较两类架构：1) End-to-End Deep Fusion（SepsisFusionFormer）——Quad-Modal Hierarchical Gated Attention Network，处理生命体征、文本与影像的跨模态交互；2) Context-Aware Stacking（SepsisLateFusion）——将模态视为三位专家（Historian/Static、Monitor/Temporal、Reader/NLP），通过CatBoost元学习器进行动态门控的上下文感知Mixture-of-Experts；并在4小时预测任务与多类抗生素选择任务中评估。

Result: SepsisFusionFormer在小样本抗生素子集出现attention starvation，过拟合，AUC约0.66；SepsisLateFusion达到0.915的AUC，提前4小时预测，并通过阈值校准将漏警率降低约48%；抗生素多类选择任务的Quad-Modal Ensemble达到0.72 AUC；所有模型集成到SepsisSuite，提供免费部署框架。

Conclusion: 在小样本、多模态环境下，基于上下文感知的MoE门控架构对脓毒症预测更鲁棒且具可部署性；端到端注意力驱动的深度融合易受样本规模限制影响。研究结果支持将复杂模态交互以模块化专家和元学习器的方式进行组合，并以SepsisSuite实现落地应用。

Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info

</details>


### [42] [A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour](https://arxiv.org/abs/2512.14713)
*Georges Sfeir,Stephane Hess,Thomas O. Hancock,Filipe Rodrigues,Jamal Amani Rad,Michiel Bliemer,Matthew Beck,Fayyaz Khan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.

</details>


### [43] [Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms](https://arxiv.org/abs/2512.14714)
*Lucas Cesar Ferreira Domingos,Russell Brinkworth,Paulo Eduardo Santos,Karl Sammut*

Main category: cs.LG

TL;DR: 提出 GSE ResNeXt：将可学习的 Gabor 卷积与 ResNeXt+SE 结构结合，以提升水下声学目标分类的鲁棒性与泛化，在数据受限场景下优于基线并显著降低训练时间。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂、船舶辐射噪声与环境噪声叠加，且公开数据集稀缺，导致特征提取和泛化性能不足；缺乏标准化评测制约方法比较与推广。

Method: 在 ResNeXt 主干中嵌入可学习的 Gabor 卷积层，作为二维自适应带通滤波器扩展特征通道；引入 squeeze-and-excitation 通道注意力以提升训练稳定性与收敛性；在三个渐进复杂度的分类任务上评估并分析训练与测试数据之间的时间差（船舶与传感器距离）的影响。

Result: GSE ResNeXt 在所有三个任务上优于 Xception、ResNet、MobileNetV2 等基线；初层添加 Gabor 卷积可约 28% 降低训练时间；研究表明环境因素对输入信号的影响显著影响模型性能。

Conclusion: 强调信号处理策略在提高水下声学分类的可靠性与泛化性中的作用，特别是在数据受限场景；未来工作应致力于减缓环境因素对输入信号的影响以提升模型稳健性。

Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.

</details>


### [44] [How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection](https://arxiv.org/abs/2512.14715)
*Zafaryab Haider,Md Hafizur Rahman,Shane Moeykens,Vijay Devabhaktuni,Prabuddha Chakraborty*

Main category: cs.LG

TL;DR: 提出BLADE框架，通过可微分的梯度敏感性估计定位对语义影响最大的比特位置，并用caption-level的语义-流畅性目标细化选择，用于研究在图像描述生成中比特翻转如何改变语义而保持语法。


<details>
  <summary>Details</summary>
Motivation: 硬件中难以检测的比特翻转可能导致生成式视觉-语言模型在语义层面的崩坏；现有故障分析多聚焦分类准确性，忽略语言生成的语义与语言学维度，需要研究意义是如何编码、可操控的。

Method: 提出BLADE：Bit-level Fault Analysis via Differentiable Estimation，利用梯度基的敏感性估计定位关键信息比特；再通过一个用于优化的caption级语义-流畅性目标对所选比特进行细化筛选。

Result: 作为方法论的论文摘要，给出概念性证据：低级比特翻转即可影响生成的语义并保持语法，与仅崩溃或降低准确率的故障分析不同；框架可用于鲁棒性测试、对抗防御与可解释性研究，揭示了比特层面对生成模型语义输出的潜在操控路径。

Conclusion: BLADE为理解意义在编码层面的分布与可变性提供新视角，有助于评估与提升生成模型对比特级故障的鲁棒性、开发防御以及推进可解释性AI。

Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.

</details>


### [45] [Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox](https://arxiv.org/abs/2512.14717)
*Ziqian Bi,Danyang Zhang,Junhao Song,Chiung-Yi Tseng*

Main category: cs.LG

TL;DR: 本研究表明，在金融NLP任务中，较小的GPT-OSS-20B模型在精度上与更大模型接近，并且在计算效率方面显著优于对手，挑战“模型越大越好”的普遍观点。


<details>
  <summary>Details</summary>
Motivation: 在金融服务领域对大语言模型进行全面评估，并引入新的效率度量，帮助在准确性与资源开销之间实现更优的部署折中。

Method: 基于GPT-OSS家族的120B和20B参数变体，以及其他前沿LLM，在十个金融NLP任务上进行严格基准测试；数据集包括Financial PhraseBank、FiQA-SA、FLARE FINERORD；提出并使用Token Efficiency Score等新效率指标；与Qwen3-235B等大模型对比。

Result: 20B GPT-OSS在准确率方面接近甚至略低于120B版本（65.1% vs 66.5%），但在效率方面明显更优，Token Efficiency Score为198.4，处理速度为每秒159.80个token；GPT-OSS整体优于包括Qwen3-235B在内的更大对手。

Conclusion: 通过架构创新与训练策略，小型GPT-OSS模型可在保持竞争性性能的同时显著降低计算开销，为金融应用的可持续、成本效益高的部署提供路径，并提供有助于部署决策的新基准与指标。

Abstract: The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model family alongside contemporary LLMs across ten diverse financial NLP tasks. Through extensive experimentation on 120B and 20B parameter variants of GPT-OSS, we reveal a counterintuitive finding: the smaller GPT-OSS-20B model achieves comparable accuracy (65.1% vs 66.5%) while demonstrating superior computational efficiency with 198.4 Token Efficiency Score and 159.80 tokens per second processing speed [1]. Our evaluation encompasses sentiment analysis, question answering, and entity recognition tasks using real-world financial datasets including Financial PhraseBank, FiQA-SA, and FLARE FINERORD. We introduce novel efficiency metrics that capture the trade-off between model performance and resource utilization, providing critical insights for deployment decisions in production environments. The benchmark reveals that GPT-OSS models consistently outperform larger competitors including Qwen3-235B, challenging the prevailing assumption that model scale directly correlates with task performance [2]. Our findings demonstrate that architectural innovations and training strategies in GPT-OSS enable smaller models to achieve competitive performance with significantly reduced computational overhead, offering a pathway toward sustainable and cost-effective deployment of LLMs in financial applications.

</details>


### [46] [SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting](https://arxiv.org/abs/2512.14718)
*Feng Xiong,Zongxia Xie,Yanru Sun,Haoyu Wang,Jianhong Lin*

Main category: cs.LG

TL;DR: SEED通过光谱熵引导的评估框架，提出 Dependency Evaluator、Spectral Entropy-based Fuser、Signed Graph Constructor、Context Spatial Extractor，解决多变量时间序列预测中的依赖建模难题，并在12个数据集上达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力/图结构的方法在提取变量间依赖时存在三大问题：强时序自相关被无关变量干扰、softmax处理导致忽略或抵消负相关、变量难以感知时序位置信息，因此需要一个更鲁棒的依赖建模框架。

Method: SEED框架核心包括：1) Dependency Evaluator：利用谱熵动态评估每个变量的时空依赖，智能平衡Channel Independence与Channel Dependence；2) Spectral Entropy-based Fuser：进一步消除由其他变量引起的时序正则化影响，分离这部分依赖；3) Signed Graph Constructor：允许边权带符号，保留负相关；4) Context Spatial Extractor：通过局部上下文窗口提取空间特征，帮助变量感知时序位置。

Result: 在12个真实数据集上进行广泛实验，SEED达到或超过当前最优水平，验证了方法的有效性和通用性。

Conclusion: SEED提供了一种通用、有效的时空依赖建模方案，能够更精准地捕捉变量间关系、保留负相关并感知时序位置，对多变量时间序列预测具有广泛的适用性。

Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.

</details>


### [47] [EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks](https://arxiv.org/abs/2512.15067)
*Zijiang Yan,Yixiang Huang,Jianhua Pei,Hina Tabassum,Luca Chiaraviglio*

Main category: cs.LG

TL;DR: EMFusion is a conditional multivariate diffusion-based forecasting framework for frequency-selective EMF data that provides calibrated probabilistic forecasts with explicit uncertainty, using a residual U‑Net with cross-attention and imputation-based sampling to handle irregular measurements and contextual factors (time, season, holidays).


<details>
  <summary>Details</summary>
Motivation: There is a need to move beyond univariate, wideband predictions to frequency-selective multivariate forecasting that captures inter-operator and inter-frequency variations, while also quantifying uncertainty for compliant and proactive network planning; existing methods lack probabilistic forecasting and temporal coherence with irregular data.

Method: A conditional multivariate diffusion model (EMFusion) with a residual U-Net backbone and cross-attention mechanism to incorporate external contextual factors. Forecasting is cast as a structural inpainting task via imputation-based sampling to maintain temporal coherence when data are irregular. Outputs are probabilistic, yielding calibrated prediction intervals.

Result: On frequency-selective EMF datasets, incorporating working-hours context improves performance; EMFusion outperforms baselines by 23.85% in CRPS, 13.93% in NRMSE, and reduces CRPS error by 22.47%.

Conclusion: EMFusion provides calibrated probabilistic forecasts with explicit uncertainty suitable for trustworthy decision-making in network planning and compliance; leveraging context improves accuracy, and the method robustly handles irregular data through imputation-based sampling.

Abstract: The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.

</details>


### [48] [Hybrid Attribution Priors for Explainable and Robust Model Training](https://arxiv.org/abs/2512.14719)
*Zhuoran Zhang,Feng Zhang,Shangyuan Li,Yang Shi,Yuanxing Zhang,Wei Chen,Tengjiao Wang,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 提出 Class-Aware Attribution Prior (CAP) 及 CAP Hybrid，通过更具区分性的归因先验来提升小语言模型在解释性和鲁棒性上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法虽然能定位到类别相关的标记，但往往聚焦于语义相似类之间的共性关键词，导致区分性不足，难以提供强有力的判别性监督。需要产生更细粒度且具差异性的归因先验以提高模型区分能力。

Method: 分析主流分类任务中的 attribution 方法，提出 CAP，提取面向类别差异的归因先验以引导模型学习更细粒度的决策相关特征；在此基础上提出 CAP Hybrid，将 CAP 的先验与现有归因技术的先验融合，形成更全面、平衡的监督信号，并通过对齐模型自注意力的归因与这些丰富先验来促进多样化特征学习。

Result: 在全数据、少样本以及对抗情景下的大规模实验中，CAP 与 CAP Hybrid 显著提升了解释性与鲁棒性。

Conclusion: CAP 与 CAP Hybrid 能提升小语言模型的可解释性和鲁棒性，且通过对齐自归因与丰富先验实现更具区分性的学习。

Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.

</details>


### [49] [Multi-Modal Semantic Communication](https://arxiv.org/abs/2512.15691)
*Matin Mortaheb,Erciyes Karakaya,Sennur Ulukus*

Main category: cs.LG

TL;DR: 提出一个多模态语义通信框架，结合文本查询引导视觉信息抽取，通过跨模态注意力筛选信息，按信道容量自适应传输图像片段并独立训练的编解码器对实现重构。


<details>
  <summary>Details</summary>
Motivation: 在复杂场景和带宽受限条件下，传统仅依赖视觉信号的语义传输难以高效捕捉任务关键信息，需引入语言指令来提供明确的任务目标与上下文。

Method: 引入文本-视觉的跨模态注意力来产生对视觉数据的软相关性分数；基于这些分数和即时信道带宽，使用独立训练的编码器-解码器对以自适应分辨率传输图像块，确保总比特率匹配信道容量；接收端重构并拼接块以保留任务相关信息。

Result: 未给出具体实验数据，主要提出框架、算法设计和实现思路。

Conclusion: 通过文本查询引导的多模态语义传输实现对任务关键信息的高效传输，适用于复杂场景与带宽受限环境，显示出灵活性和目标驱动性。

Abstract: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.

</details>


### [50] [Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example](https://arxiv.org/abs/2512.14721)
*Arno Appenzeller,Nick Terzer,André Hohmeyer,Jan-Philipp Redlich,Sabine Luttmann,Friedrich Feuerhake,Nadine S. Schaadt,Timm Intemann,Sarah Teuber-Hanselmann,Stefan Nikolin,Joachim Weis,Klaus Kraywinkel,Pascal Birnstill*

Main category: cs.LG

TL;DR: 提出基于统计信息自动生成 Synthea 规则以生成隐私保护的合成医疗数据，使用真实世界癌症数据构建 glioblastoma 模块并生成合成数据，对比原始数据评估统计属性保留。


<details>
  <summary>Details</summary>
Motivation: 解决合成医疗数据的规则设计复杂性与对现实样本数据的依赖，提升隐私保护研究中的数据可用性与可重复性。

Method: 从表格数据中提取统计信息，自动生成 Synthea 规则；以胶质母细胞瘤 glioblastoma 为例，基于真实世界数据创建模块并生成合成数据；评估合成数据在疾病进程和统计属性上的保留程度。

Result: 合成数据在再现已知疾病进程和大多数统计特性方面表现良好；表明对隐私保护研究具有潜在价值，但需要考虑与现有方法相同的医学解释与应用限制。

Conclusion: 基于统计信息自动生成规则的 Synthea 方法具有提升合成数据可用性与隐私保护研究潜力的前景，但需进一步评估方法的局限性、适用性及对非目标疾病的泛化能力。

Abstract: The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates data based on rules describing the lifetime of a synthetic patient. These rules typically express the probability of a condition occurring, such as a disease, depending on factors like age. Since they only contain statistical information, rules usually have no specific data protection requirements. However, creating meaningful rules can be a very complex process that requires expert knowledge and realistic sample data. In this paper, we introduce and evaluate an approach to automatically generate Synthea rules based on statistics from tabular data, which we extracted from cancer reports. As an example use case, we created a Synthea module for glioblastoma from a real-world dataset and used it to generate a synthetic dataset. Compared to the original dataset, the synthetic data reproduced known disease courses and mostly retained the statistical properties. Overall, synthetic patient data holds great potential for privacy-preserving research. The data can be used to formulate hypotheses and to develop prototypes, but medical interpretation should consider the specific limitations as with any currently available approach.

</details>


### [51] [HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers](https://arxiv.org/abs/2512.14722)
*Mohamed Malhou,Ludovic Perret,Kristin Lauter*

Main category: cs.LG

TL;DR: 使用层次注意力变换器(HAT)通过树状结构偏置来计算Groebner基，提高多变量多项式方程组求解的效率；扩展到任意深度，给出成本分析，并通过课程学习实现比先前工作更大规模的实例求解。


<details>
  <summary>Details</summary>
Motivation: Groebner基是多项式方程求解中的核心工具，但传统计算成本高、难以扩展到高维/大规模。基于NeurIPS 2024的Kera等工作，本文通过引入层次结构的注意力模型来提升学习求解Groebner基的效率和可扩展性。

Method: 采用Hierarchical Attention Transformers（HATs），在Transformer中引入树状结构的归纳偏置以建模数据中的层级关系；将其应用于Groebner基的求解过程并推广到任意深度，提供详细的计算成本分析；结合课程学习策略以逐步增大难度进行训练，提升对大规模实例的求解能力。

Result: 与普通平坦注意力模型相比，显著降低计算成本；在比Kera等2024年的工作更大规模的实例上实现求解，验证了方法的可扩展性和效率提升，同时给出系统的成本分析。

Conclusion: 在Groebner基计算中引入HAT可显著提升效率与可扩展性，且结合课程学习进一步扩展了适用规模，为多项式方程组求解提供了更强的学习驱动工具。

Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)

</details>


### [52] [A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications](https://arxiv.org/abs/2512.14727)
*Klaus-Rudolf Kladny,Bernhard Schölkopf,Lisa Koch,Christian F. Baumgartner,Michael Muehlebach*

Main category: cs.LG

TL;DR: 理论保证对任意大小的校准集成立，但在实际应用中，校准集大小决定了置信区间的实用性，尤其在数据稀缺的医学领域，此保证的实际效用可能受限；经验研究在医疗图像分类中支持这一点。


<details>
  <summary>Details</summary>
Motivation: 在医疗决策需要可靠的不确定性估计的背景下，区位预测（Conformal Prediction, CP）被广泛应用，因其为预测集合提供统计保证。尽管理论上校准集可以任意大小，但在数据稀缺情景下，需检验规模对实际效用的影响。

Method: 对 CP 保证进行理论分析，揭示随校准集规模变化的实际效用依赖性；并在一个医疗图像分类任务中进行经验证明，比较不同校准集规模下 CP 预测集合的表现。

Result: 尽管理论保证对任意大小的校准集成立，但实际有用性高度依赖于校准集的大小；在数据稀缺的医学场景中，小规模校准集会显著削弱 CP 的实用性，且实证结果支持该结论。

Conclusion: 研究提醒在医疗应用中不要只依赖 CP 的理论不确定性保证，应关注校准集规模对实际效用的影响，必要时结合其他不确定性估计方法或增大校准数据以提升预测集合的实用性。

Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.

</details>


### [53] [A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems](https://arxiv.org/abs/2512.14728)
*Jie He,Yong Qin,Jianyuan Guo,Xuan Sun,Xuanchuan Zheng*

Main category: cs.LG

TL;DR: 本研究提出一个基于数据驱动的城市轨道旅客轨迹推断框架，利用AFC与AVL数据推断单个乘客轨迹，并通过KLEM（基于KL散度的EM参数估计）实现自适应参数化，使用真实数据验证，在高峰时段达到>90%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能依赖外部数据或调查问卷，缺乏无外部数据的鲁棒性；需要将多源交通数据融合以精确重构个人旅行轨迹，并以真实数据进行验证以提升可信度。

Method: 1) 基于AFC与AVL数据建立时空约束下的列车替选集；2) 数据驱动的自适应轨迹推断与旅程构造；3) 提出KLEM方法（基于KL散度的参数估计，结合EM算法）以无外部数据进行参数拟合；4) 使用真实个人旅行轨迹数据进行结果验证。

Result: 实验结果表明所提方法可以实现高精度的乘客轨迹推断，在高峰时段城市轨道旅行轨迹推断的准确率超过90%。

Conclusion: 该方法具有良好的鲁棒性和广泛适用性，省去了对外部或调查数据的依赖，能提升轨道运营的决策支撑能力。

Abstract: Refined trajectory inference of urban rail transit is of great significance to the operation organization. In this paper, we develop a fully data-driven approach to inferring individual travel trajectories in urban rail transit systems. It utilizes data from the Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems to infer key trajectory elements, such as selected train, access/egress time, and transfer time. The approach includes establishing train alternative sets based on spatio-temporal constraints, data-driven adaptive trajectory inference, and trave l trajectory construction. To realize data-driven adaptive trajectory inference, a data-driven parameter estimation method based on KL divergence combined with EM algorithm (KLEM) was proposed. This method eliminates the reliance on external or survey data for parameter fitting, enhancing the robustness and applicability of the model. Furthermore, to overcome the limitations of using synthetic data to validate the result, this paper employs real individual travel trajectory data for verification. The results show that the approach developed in this paper can achieve high-precision passenger trajectory inference, with an accuracy rate of over 90% in urban rail transit travel trajectory inference during peak hours.

</details>


### [54] [O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization](https://arxiv.org/abs/2512.15229)
*Elio Gruttadauria,Mathieu Fontaine,Jonathan Le Roux,Slim Essid*

Main category: cs.LG

TL;DR: 一个端到端的在线说话人分离/对话者识别系统（O-EENC-SD），将 EEND-EDA 与基于RNN的在线拼接模块和质心细化解码器结合在一起，实现无需超参数的高效在线预测，在CallHome数据集上实现与现有方法相近的两说话人对话语音分离性能，同时在复杂度和推理速度上具有更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 在线说话人分离通常依赖聚类等离线策略或代价高昂的在线端到端方法。需要一个无需超参数、计算高效且能单-pass 的在线说话人分离系统，特别是在两说话人对话电话语音场景中。

Method: 提出基于 EEND-EDA 的端到端在线说话人分离系统 O-EENC-SD，新增基于RNN的在线拼接机制以实现在线预测；引入质心细化解码器，并通过消融实验评估其有效性；与聚类基线及现有在线端到端方法进行对比，验证在两说话人通讯电话数据上的性能与效率。

Result: 在CallHome两说话人对话电话语音数据集上，O-EENC-SD 的 DER 与当前方法的性能相当；在DER与复杂度之间取得良好权衡，即使在没有重叠的独立片段下也表现出极高的推理效率，显示出相比传统方法更优的效率与实用性。

Conclusion: 本文提出的 O-EENC-SD 提供了一种高效的在线说话人分离方案，质心细化解码器和在线拼接机制使在线预测高效且性能竞争力强；相比需大量超参数的聚类方法，该系统具备明显的超参数自由优势，并在实际场景中展现出优异的DER与资源开销之间的折衷。

Abstract: We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.

</details>


### [55] [Semantic Geometry for policy-constrained interpretation](https://arxiv.org/abs/2512.14731)
*Nikit Phadke*

Main category: cs.LG

TL;DR: 提出一个以单位球面为几何表达的策略约束语义解释框架，通过将意义表示为方向、证据为向量集合、可接受解释为球面凸域，政策约束作为先验定义，与证据几何分离；解释通过带约束的优化实现，若出现矛盾或政策排除则自然而然拒绝；并与信息理论、贝叶斯推断和层叠语义相关联，且在大规模受监管金融数据上实现零幻觉批准。


<details>
  <summary>Details</summary>
Motivation: 在高风险/高代价场景中提供可证明的安全性，避免模型产生幻觉性承诺；通过将信息理论、贝叶斯推断与层叠语义结合，建立严格的界限与可解释性。

Method: 将语义编码为单位球面的方向，证据以 witness 向量集合表示，可接受解释对应球面凸域。政策约束作为同一流形上的显式先验，与证据几何分离。解释问题转化为对可接受区域的带约束优化；若出现矛盾或政策排除，拒绝成为拓扑上必然结果。进一步给出与信息理论、贝叶斯推断和层叠语义的联系，并证明复杂度边界在信息论意义上是最优的。

Result: 在大规模受监管金融数据上的实验结果显示对多种政策制度实现零幻觉批准，标志着这一领域的大规模成功。

Conclusion: 该框架通过将政策先验与证据几何分离，提供可证明的无幻觉解释能力，并且其复杂度达到信息论上的最优，拓展了高风险领域语义解释的安全性与稳健性研究。

Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.

</details>


### [56] [Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery](https://arxiv.org/abs/2512.15344)
*Hiroyoshi Nagahama,Katsufumi Inoue,Masayoshi Todorokihara,Michifumi Yoshioka*

Main category: cs.LG

TL;DR: 提出两种相位感知预处理策略以对多轴振动数据中的随机相位变化进行对齐，并在两阶段学习框架和六种深度模型上验证，单轴参考相位策略下保留轴间关系，达到最高准确率提升。


<details>
  <summary>Details</summary>
Motivation: 在旋转机械的预测性维护中，传统特征提取常忽略/错用相位信息，可能影响分类/诊断性能。通过引入相位对齐来利用相位信息以提高鲁棒性和可扩展性。

Method: 提出三轴独立相位调整（逐轴对齐到零相位）和单轴参考相位调整（对整个三轴统一时间偏移以保留相位关系）；在新构建的具同步三轴传感器的转子数据集上，采用两阶段学习框架并对六种深度学习架构进行评估。

Result: 三轴独立相位方法对Transformer等模型带来稳定提升（+2.7%），单轴参考相位方法在保留空间相位关系的前提下实现最好性能，最高准确率提升达96.2%（+5.4%）。

Conclusion: 两种相位对齐策略均为预测性维护系统提供实用且可扩展的改进，适用于需要在多轴振动分析中敏感相位信息的场景。

Abstract: Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.

</details>


### [57] [Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection](https://arxiv.org/abs/2512.15385)
*Julian Oelhaf,Mehran Pashaei,Georg Kordowich,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.
  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.

</details>


### [58] [Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness](https://arxiv.org/abs/2512.14734)
*Qiang Chen,Venkatesh Ganapati Hegde,Hongfei Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.

</details>


### [59] [NoveltyRank: Estimating Conceptual Novelty of AI Papers](https://arxiv.org/abs/2512.14738)
*Zhengxu Yan,Han Li,Yuming Feng*

Main category: cs.LG

TL;DR: 提出一个基于文本特征的AI论文概念新颖度评估与排序模型，利用论文题/摘要及与已有文献的语义相似度，聚焦二元分类与成对比较两种任务，微调Qwen3-4B-Instruct-2507与SciBERT并与GPT-5.1对比，公开代码。


<details>
  <summary>Details</summary>
Motivation: 应对AI领域论文数量剧增与快速迭代，人工新颖性评估缺乏一致性且耗时；需要可扩展、数据驱动的量化新颖性信号，辅助作者与评审。

Method: 以论文题目和摘要及其与先前文献的语义相似度为核心特征，设计两类任务：1) 二元分类，预测论文的绝对新颖性；2) 成对比较，判断相对新颖性。对Qwen3-4B-Instruct-2507与SciBERT进行微调，在这两种任务上进行评估，并与GPT-5.1进行对比。实现和数据可公开获取，代码公开在GitHub。

Result: 原文摘要未给出具体实验结果；报告了对两种任务设定及建模选择对性能的影响的基线分析，并对不同模型的表现进行了对比，但未在摘要中给出量化指标。完整结果及实验细节将在全文中给出。

Conclusion: 该工作提出一种数据驱动的论文新颖性信号来源，能帮助提升投稿筛选与评审中的新颖性判断的一致性与效率；并且提供开源实现以便复现与进一步研究。

Abstract: With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.

</details>


### [60] [Guided Discrete Diffusion for Constraint Satisfaction Problems](https://arxiv.org/abs/2512.14765)
*Justin Jung*

Main category: cs.LG

TL;DR: 离散扩散引导用于 CSPs，能够在无监督条件下解决数独。


<details>
  <summary>Details</summary>
Motivation: CSP求解常受限于显式约束和监督信号；引入离散扩散过程可能提供更灵活的搜索和去噪能力。

Method: 构建离散扩散引导框架，将扩散/去噪过程应用于 CSP 的解空间，以数独为例进行无监督求解演示。

Result: 在数独等 CSP 实例上，展示无监督求解能力，并达到有竞争力的成功率与收敛性。

Conclusion: 该方法为 CSP 提供一种无监督的新的求解路线，数独实验表明潜力，后续可扩展到其他离散约束问题。

Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.

</details>


### [61] [Evaluating Weather Forecasts from a Decision Maker's Perspective](https://arxiv.org/abs/2512.14779)
*Kornelius Raeth,Nicole Ludwig*

Main category: cs.LG

TL;DR: 决策层校准揭示了预测准确性未必等同于决策性能；不同任务下机器学习和经典数值天气预报模型的排名可能不同，需以决策影响为导向进行模型选择。


<details>
  <summary>Details</summary>
Motivation: 将 forecast 评估从单纯预测准确性转向对决策结果的影响评估，解决“好预测不一定有好决策”这一问题。

Method: 使用决策校准框架，在多种天气相关决策任务中对机器学习模型与经典数值天气预报模型进行比较，评估两者在实际决策中的表现，分析预测层面的改进是否转化为决策层面的收益。

Result: 结果显示：预测阶段的性能提升并不总是转化为决策层面的性能提升；某些差异仅在决策层显现，且不同决策任务下的模型排名可能发生变化。

Conclusion: 单纯的预测等级评估不足以在特定决策任务中选择最佳模型，应在模型评估中纳入决策级别的指标与校准，考虑任务特定性来指导模型选择与部署。

Abstract: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.

</details>


### [62] [How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal](https://arxiv.org/abs/2512.14873)
*Sam Jeong,Hae Yong Kim*

Main category: cs.LG

TL;DR: FAN 通过在部分 ReLU 上引入正弦/余弦激活来提升性能，但其机制来自正弦在 x=0 附近的局部导数特性而非周期性；FAN 主要缓解 dying-ReLU，提出更高效的收敛加速器 Dual-Activation Layer (DAL)，在三个任务上实现更快收敛且达到相同或更高的验证准确率。


<details>
  <summary>Details</summary>
Motivation: 揭示 FAN 改进的真实机制，推动从单纯的频谱解释转向对训练动力学的分析，并据此提出更高效的激活组合以加速收敛。

Method: 系统分析正弦/余弦激活对梯度与学习 dynamics 的影响，比较它们对性能的贡献，诊断 FAN 对 dying-ReLU 的缓解，进而提出 DAL 并在多任务上进行评估。

Result: 实验结果表明：1) 仅正弦激活对性能有正向贡献，余弦通常有害；2) 改进源于正弦在 x≈0 的局部梯度行为，而非其周期性；3) FAN 主要缓解 dying-ReLU 的问题；4) DAL 在三项任务上实现更快的收敛并达到等于或更高的验证准确率。

Conclusion: 将 FAN 的效应解释为训练动力学层面的改进，并提出 DAL 作为更高效的收敛加速器，在多任务中表现稳定。

Abstract: Fourier Analysis Network (FAN) was recently proposed as a simple way to improve neural network performance by replacing part of ReLU activations with sine and cosine functions. Although several studies have reported small but consistent gains across tasks, the underlying mechanism behind these improvements has remained unclear. In this work, we show that only the sine activation contributes positively to performance, whereas the cosine activation tends to be detrimental. Our analysis reveals that the improvement is not a consequence of the sine function's periodic nature; instead, it stems from the function's local behavior near x = 0, where its non-zero derivative mitigates the vanishing-gradient problem. We further show that FAN primarily alleviates the dying-ReLU problem, in which a neuron consistently receives negative inputs, produces zero gradients, and stops learning. Although modern ReLU-like activations, such as Leaky ReLU, GELU, and Swish, reduce ReLU's zero-gradient region, they still contain input domains where gradients remain significantly diminished, contributing to slower optimization and hindering rapid convergence. FAN addresses this limitation by introducing a more stable gradient pathway. This analysis shifts the understanding of FAN's benefits from a spectral interpretation to a concrete analysis of training dynamics, leading to the development of the Dual-Activation Layer (DAL), a more efficient convergence accelerator. We evaluate DAL on three tasks: classification of noisy sinusoidal signals versus pure noise, MNIST digit classification, and ECG-based biometric recognition. In all cases, DAL models converge faster and achieve equal or higher validation accuracy compared to models with conventional activations.

</details>


### [63] [Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse](https://arxiv.org/abs/2512.14879)
*Jingwei Chen*

Main category: cs.LG

TL;DR: 提出 Entropy-Reservoir Bregman Projection (ERBP) 作为一个信息几何框架，统一自监督学习中的崩溃现象（模型崩溃、模式坍塌、策略过度利用等）及其修正策略，通过引入高熵“熵水库”来维持熵水平，并给出收敛/发散的必要与充分条件及依赖样本量与光滑常数的闭式速率。


<details>
  <summary>Details</summary>
Motivation: 自监督/自我训练中，模型容易因有限样本噪声在分布空间的投影序列中收敛到逐渐缩小的经验支持而导致熵指数下降、文本重复、模式丢失等崩溃现象。尽管存在多种经验性修复手段，但缺乏统一、可预测的原理来解释成功/失败的共性与作用机制。

Method: 将闭环建模为分布空间中的随机Bregman投影序列；在无外部耦合时，有限样本噪声导致熵的指数衰减与崩溃。引入熵水库，将高熵分布混入每次投影中，给予可控的熵通量以稳定动力学。给出崩溃的必要条件、确保非平凡熵下界的充分条件，以及仅依赖样本量和Bregman生成函数的强凸/ Lipschitz常数的解析速率。通过大语言模型自训练、Soft Actor-Critic、GAN优化等实验验证理论预测，且证明不同的稳定化启发式对应该水库的具体选取和耦合系数。

Result: ERBP 提供了一种统一且可操作的设计规则：通过监控和预算熵通量，将自监督学习中的不稳定性转化为可控参数。理论上给出崩溃的必要条件、非平凡熵下界的充分条件和与样本量及光滑性相关的闭式速率；实验上证实不同稳态策略对应不同的水库配置与耦合，从而将经验性修正统一成一个定量的设计原则。

Conclusion: ERBP 将多种看似分散的稳定化技巧统一成一个定量框架，提供“监控并预算熵通量”的实用设计法则，帮助研究者对自监督学习中的信息循环进行可控的熵管理，进而提升稳定性和样本利用效率。

Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.

</details>


### [64] [Task Matrices: Linear Maps for Cross-Model Finetuning Transfer](https://arxiv.org/abs/2512.14880)
*Darrin O' Brien,Dhikshith Gajulapalli,Eric Xia*

Main category: cs.LG

TL;DR: 提出任务矩阵以在预训练-微调嵌入之间建立线性跨层编码，在多数据集上实现优于线性探针的表现，並提供数据驱动的近似与跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 回答是否存在更广泛适应场景中的线性表示，以及如何解释模型的线性适配性，进而降低微调成本。

Method: 提出任务矩阵作为基嵌入到微调嵌入的线性变换；在视觉与文本模型及十个数据集上进行评估，比较基线、线性探针与任务矩阵；给出基于数据的近似方法并检验跨域泛化；实现公开可用。

Result: 在加入任务矩阵的基模型上，性能超越线性探针，在某些场景接近微调水平；验证了跨层线性编码的存在性；提出高效且可泛化的数据近似方法；实现公开。

Conclusion: 跨域的预训练到微调嵌入的线性编码存在，任务矩阵提供了一种高效的近似与理解模型适应性的途径，潜在降低微调成本并推动迁移学习的理解。

Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.

</details>


### [65] [OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams](https://arxiv.org/abs/2512.14892)
*Mohammad Abu-Shaira,Alejandro Rodriguez,Greg Speegle,Victor Sheng,Ishfaq Ahmad*

Main category: cs.LG

TL;DR: 提出OLR-WA（OnLine Regression with Weighted Average），一种新颖且多变量的在线线性回归模型，能够增量更新、处理漂移、快速收敛，并在与现有在线模型的对比中表现优异，且在极少初始数据时（仅1%-10%总数据量）也能实现较高的R^2值。


<details>
  <summary>Details</summary>
Motivation: 解决在不大量存储数据或重算模型的前提下进行高效在线回归的问题，并能处理数据随时间演变的漂移现象（时间漂移与置信度驱动的漂移）。

Method: 提出OLR-WA：带权重平均的在线回归方法；通过更新规则引入对旧数据（尤其是高置信度数据）的保守处理，适应漂移场景；进行收敛性分析；与现有在线回归模型比较。

Result: OLR-WA在性能上接近批回归，且与其他最先进的在线模型相比具有相当或更优的表现；从第一次迭代到最后一次迭代均能实现快速收敛，且在极低初始样本量（1%-10%）时也能维持高R^2；能够有效处理时间基漂移以及置信度驱动的挑战性场景，且在此方面表现为唯一能够有效处理的模型。

Conclusion: OLR-WA在在线线性回归任务中展现出高度的灵活性与实用性，结合快速收敛、竞争性准确性以及对不同漂移情景的鲁棒性，是一个有价值的在线回归解决方案。

Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.

</details>


### [66] [Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective](https://arxiv.org/abs/2512.14932)
*Daniel Gomes de Pinho Zanco,Leszek Szczecinski,Jacob Benesty,Eduardo Vinicius Kuhn*

Main category: cs.LG

TL;DR: 提出一种基于Kronecker表示的低秩MMSE滤波器正则化参数高效估计方法，揭示正则化参数与秩选择之间的紧密关系，并在低秩设置中对性能至关重要。通过仿真验证，方法相较于常用方法取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 正则化参数的取值在低秩MMSE滤波器中与秩选择密切相关，错误选择会显著降低性能，因此需要一种高效、稳健的参数估计方法。

Method: 提出基于Kronecker-product表示的正则化参数估计方法，探索其与秩选择之间的联系，并设计在低秩设置中高效地确定该参数的策略。通过数值仿真验证其有效性。

Result: 仿真实验显示该方法在与常用参数选择方法相比时获得显著性能提升。

Conclusion: 利用Kronecker表示揭示正则化参数与秩选择之间的联系，并给出高效的参数估计策略，提升了低秩MMSE滤波的性能。

Abstract: In this work, we propose a method to efficiently find the regularization parameter for low-rank MMSE filters based on a Kronecker-product representation. We show that the regularization parameter is surprisingly linked to the problem of rank selection and, thus, properly choosing it, is crucial for low-rank settings. The proposed method is validated through simulations, showing significant gains over commonly used methods.

</details>


### [67] [An Efficient Gradient-Based Inference Attack for Federated Learning](https://arxiv.org/abs/2512.15143)
*Pablo Montaña-Fernández,Ines Ortega-Fernandez*

Main category: cs.LG

TL;DR: 提出一种基于梯度的成员推断攻击，针对联邦学习（FL）中最后一层梯度随轮次的时序演化进行分析。利用影子模型学习轮次级梯度模式，无需私有数据，适用于半诚实与恶意参与者（聚合方或数据拥有者）。可扩展至离散属性推断；对任意梯度模型及分类/回归均具适用性。实验表明在多轮FL中泄漏增强，且聚合方威胁更大；高维数据泄漏更强，计算/内存开销与现有攻击相当。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习框架下，虽然降低了直接数据暴露，但模型更新仍可能泄露敏感信息。随着多轮训练的进行，梯度的时序特征可能揭示训练记录的隐私。需要评估不同威胁模型（聚合方与数据拥有者）的攻击能力，以及在不同数据特性下的风险程度。

Method: 提出基于梯度的攻击，观察多轮训练中最后一层梯度的演化，利用影子技术学习轮次级梯度模式；无需访问私有数据，即可对训练记录进行成员推断，且可推广到离散属性推断：通过在不同属性假设下对比梯度响应。该攻击与模型无关，适用于任意梯度基模型，且可用于分类与回归情景。

Result: 实验在CIFAR-100和Purchase100数据集上进行成员推断，在Breast Cancer Wisconsin数据集上进行属性推断，显示攻击性能强且与文献中的另一攻击在计算与内存开销方面相当。结论上，多轮联邦学习提升了推断攻击的脆弱性，聚合方威胁程度高于数据拥有者，且数据的性质（尤其高维数据）显著影响攻击效果。

Conclusion: 该工作揭示了多轮FL环境下的隐私脆弱性及威胁模型的重要性，强调需要结合更强的防护策略（如差分隐私、安全聚合、梯度裁剪/噪声注入等）来降低梯度泄露风险，并对高维、结构化数据场景下的防护策略提出未来研究方向。

Abstract: Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.

</details>


### [68] [Softly Constrained Denoisers for Diffusion Models](https://arxiv.org/abs/2512.14980)
*Victor M. Yeom Song,Severi Rissanen,Arno Solin,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: Softly guided denoisers for constrained diffusion without altering loss or sampling loop.


<details>
  <summary>Details</summary>
Motivation: 在科学应用中，约束条件常常需要被遵守，但把约束引入损失函数或采样过程可能会让模型偏离真实数据分布，尤其在约束存在错设时。

Method: 将 guidance 式调整直接嵌入到去噪器本身，给予其对约束一致性的软性偏置，而非修改损失或采样循环。

Result: 具有约束知识的软约束去噪器相比标准去噪器在遵守约束方面更优，同时保持在数据与约束错配时的灵活性，不会过度强制遵循错误约束。

Conclusion: 在去噪器层面整合约束信息提供了对外部正则化或采样阶段引导的鲁棒替代方案，适用于需要优雅处理约束错设的科学数据。

Abstract: Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.

</details>


### [69] [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982)
*Yaniv Leviathan,Matan Kalman,Yossi Matias*

Main category: cs.LG

TL;DR: 在不使用推理（不进行链式思考）的条件下，重复输入提示可以提升多种模型的性能（Gemini、GPT、Claude、Deepseek），且不增加生成的 token 数量或延迟。


<details>
  <summary>Details</summary>
Motivation: 探索简单的提示设计是否能在不增加推理成本的前提下提升模型性能，并评估跨多模型的普适性。

Method: 在多模型上对比实验：在不启用推理的条件下，对同一任务用重复提示与原始提示进行对比，测量性能指标、生成 token 数量和端到端 latency。

Result: 重复输入提示在不增加生成 token 数量或延迟的前提下提升了模型在所测试任务上的性能。

Conclusion: 提示重复（输入再写）是一种简单且成本低的方式，可在不进行推理时提高多种模型的性能，具有一定的跨模型适用性。

Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.

</details>


### [70] [Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes](https://arxiv.org/abs/2512.14991)
*Hanqing Jin,Renyuan Xu,Yanzhao Yang*

Main category: cs.LG

TL;DR: Adaptive model-based RL for controlled diffusion processes with unbounded state spaces using adaptive partitioning of joint state-action space; derives regret bounds via a zooming-dimension concept; validated on high-dimensional problems like multi-asset mean-variance portfolio selection.


<details>
  <summary>Details</summary>
Motivation: Address learning in continuous, high-dimensional diffusion settings with unbounded state spaces and polynomially growing rewards, common in finance, economics, and operations research.

Method: An adaptive partitioning scheme that maintains estimators of drift, volatility, and rewards within each partition, refining partitions when estimation bias exceeds statistical confidence; model-based RL with regret analysis using a new zooming-dimension notion applicable to unbounded diffusion processes.

Result: Regret bounds that depend on horizon, state dimension, reward growth, and zooming dimension; bounds recover those for bounded settings and extend guarantees to diffusion-type problems; numerical experiments, including high-dimensional multi-asset mean-variance portfolio selection.

Conclusion: The work extends theoretical guarantees to unbounded diffusion processes and demonstrates practical effectiveness in high-dimensional problems via numerical experiments, bridging theory and applications in finance and related fields.

Abstract: We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.

</details>


### [71] [DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding](https://arxiv.org/abs/2512.15000)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: A coding-focused Reward Model using chain-of-function prompting and meta-learning label correction to improve code generation; achieves 80.9 pass@1 on LiveCodeBench, surpassing o4-mini.


<details>
  <summary>Details</summary>
Motivation: PRMs help LLMs at test time but struggle in coding due to lack of meaningful step decomposition and noisy partial labels. A coding-specific PRM with modular reasoning and robust label correction can improve code generation.

Method: DreamPRM-Code treats functions as reasoning steps via a Chain-of-Function prompting strategy to induce modular code generation. It includes a meta-learning-based correction mechanism that uses clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels.

Result: State-of-the-art performance on LiveCodeBench with 80.9 pass@1, surpassing OpenAI o4-mini.

Conclusion: Demonstrates the effectiveness of modular, function-level reasoning in coding PRMs and shows that meta-learning label correction with bi-level optimization can mitigate label noise to improve test-time performance.

Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.

</details>


### [72] [Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets](https://arxiv.org/abs/2512.15008)
*Sandeep Neela*

Main category: cs.LG

TL;DR: 提出 Stock Pattern Assistant (SPA)——一个确定性框架，利用日线OHLCV数据和事件流，提取单调价格运行段、对齐相关事件并生成事实性、历史性且可审计的解释。已在AAPL、NVDA、SCHW、PGR等股票上演示，展现稳定的结构分解与上下文叙事，且通过消融实验确认分段、事件对齐和受限解释对可解释性的贡献。


<details>
  <summary>Details</summary>
Motivation: 市场价格往往被噪声所遮蔽，现有技术指标、图表启发式和预测模型在透明性与可审计性方面存在不足。需要一个可重复、可解释的价格结构分析框架，以支持分析师工作流、风险评估与可解释AI管线。

Method: SPA是一个确定性框架：（1）从日线OHLCV数据中提取单调价格运行段；（2）通过对称相关窗口将公开事件附着到价格序列；（3）生成事实性、历史性且受约束的解释。核心组件包括确定性分段、事件对齐与受限解释；并通过消融实验评估各组成部分对可解释性的贡献。

Result: 在四只股票（AAPL、NVDA、SCHW、PGR）上展示出稳定的结构分解和上下文叙事，覆盖不同波动性和行业特征；消融实验表明确定性分段、事件对齐与受限解释对可解释性的贡献显著。

Conclusion: SPA不是一个预测系统，也不构成交易信号；它提供一个透明、可重复的历史价格结构视图，能够补充分析师工作流、风险评估以及可解释AI的整合。

Abstract: Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.

</details>


### [73] [Epistemic diversity across language models mitigates knowledge collapse](https://arxiv.org/abs/2512.15011)
*Damian Hodel,Jevin D. West*

Main category: cs.LG

TL;DR: 在自我训练的模型生态系统中， epistemic diversity（知识多样性）可在达到最佳水平前缓解“知识崩塌”；过少模型无法表达完整分布，过多模型又降低了单模型近似能力，导致第一步就表现差。政策上应关注并促进领域与社区层面的多样性，以避免单一化风险。


<details>
  <summary>Details</summary>
Motivation: 探究AI系统中的知识崩塌现象是否可通过跨模型的生态系统多样性来缓解，以及模型生态多样性对长期性能的影响。

Method: 在多语言模型中将训练数据分割给不同模型，构建模型生态系统，进行十轮自我训练迭代，评估不同数量和数据分割方式下的性能与稳定性；分析多样性水平对崩塌现象的缓解效应。

Result: 提升认知/知识层面的 epistemic diversity 能在一定程度上减缓崩塌，但存在最优点；过少的多样性无法覆盖真实分布的丰富性，过多则削弱单模型对真实分布的逼近能力，第一轮即表现不佳。总体呈现“多样性-性能-崩塌”的倒U形关系。

Conclusion: 在AI monoculture背景下，应监控系统间的多样性并推动域内、社区特定模型的发展，以维持有益的生态多样性并降低知识崩塌风险。

Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.

</details>


### [74] [The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems](https://arxiv.org/abs/2512.15068)
*Debu Sinha*

Main category: cs.LG

TL;DR: 对话式检索增强生成系统中的“语义幻觉”问题：外推和嵌入式检测的局限性被揭示，提出采用 conformal prediction 的检测框架以提供有限样本覆盖率，但实际的真实数据集上嵌入式检测仍然存在高假阳性。


<details>
  <summary>Details</summary>
Motivation: 揭示现有基于嵌入和NLI的幻觉检测在实际应用中的局限性，尤其是在生产级别的 RAG 系统中对高可靠性检测的需求。通过引入具有限样本覆盖保证的 conformal prediction，评估检测在合成与真实幻觉数据上的表现差异。

Method: 对齐并应用 conformal prediction 给幻觉检测，在约 600 条例校准样本上构建覆盖率保证；在 Natural Questions 的合成幻觉数据上达到 94% 覆盖且 0% 假阳性；在三组真实幻觉基准（HaluEval、RAGTruth、WikiBio）上对比嵌入式方法（包括 OpenAI text-embedding-3-large、跨编码器模型）以及 GPT-4 作为判定者的表现。

Result: 合成幻觉数据上，85%~100% 的置信覆盖实现；真实幻觉数据上，嵌入式检测的假阳性率极高，HaluEval 100%、RAGTruth 88%、WikiBio 50%；而将 GPT-4 作为判定者时 FPR 降至 7%（95% CI：3.4%–13.7%），表明通过推理可解决该任务；整体结论认为“语义幻觉”使嵌入式检测在生产环境中不可靠。

Conclusion: 嵌入式检测在多任务、多模型场景下并不足以解决生产级 RAG 的幻觉检测问题；需要采纳以 conformal prediction 为基础的检测框架或其他非嵌入式/推理驱动的方法来实现稳定的幻觉检测与可控误报率。该现象被命名为“语义错觉”并在不同模型、嵌入架构和任务类型上普遍存在。

Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.

</details>


### [75] [The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks](https://arxiv.org/abs/2512.15082)
*Wanfu Gao,Zebin He,Jun Gao*

Main category: cs.LG

TL;DR: 提出 FEAML，在多标签学习中利用大语言模型进行自动特征工程，通过元数据与标签共现矩阵引导 LLM 进行代码生成，生成高质量特征并以模型准确度评估及 Pearson 相关性检测冗余性，形成带反馈的自我改进循环，实验显示优于其他特征工程方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的特征工程尚未应用于多标签学习，难以建模复杂标签依赖，且未针对多标签任务特征进行专门设计。需实现高效、可解释且能捕捉标签间关系的特征工程。

Method: 提出 FEAML：利用 LLM 的代码生成功能，结合数据元数据和标签共现矩阵，引导 LLM 以任务目标为导向生成特征代码；对生成的特征进行模型准确度评估以衡量效用，同时用 Pearson 系数检测冗余性；将评估结果反馈给 LLM 以迭代优化代码生成，形成自我改进的特征工程循环；结合 LLM 和反馈机制实现高效、可解释的特征工程。

Result: 在多种多标签数据集上证明 FEAML 的表现优于其他特征工程方法。

Conclusion: FEAML 为多标签学习提供一种高效、可解释且自我提升的特征工程范式，能够更好地捕捉标签依赖并提升分类性能。

Abstract: Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.

</details>


### [76] [PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network](https://arxiv.org/abs/2512.15086)
*Hongjin Mi,Huiqiang Lun,Changhong Mou,Yeyu Zhang*

Main category: cs.LG

TL;DR: A Physics-informed Partition Penalty Deep Operator Network (PIP^2 Net) that uses partition-of-unity based penalty to regularize trunk outputs, improving expressiveness and stability; shows improved accuracy and robustness over DeepONet, PI-DeepONet, POU-DeepONet on Burgers, Allen–Cahn, diffusion–reaction PDEs.


<details>
  <summary>Details</summary>
Motivation: To address instability and mode imbalance in trunk networks of operator-learning architectures and to leverage the locality and stability of partition-of-unity (PoU) methods for better generalization with potentially smaller training datasets.

Method: Introduce PIP^2 Net, a revised PoU-based regularization and a simplified partition penalty integrated into the DeepONet/PI-DeepONet framework. The partition penalty coordinates trunk outputs to enhance expressiveness while preserving flexibility.

Result: Empirical evaluation on three nonlinear PDEs (viscous Burgers, Allen–Cahn, and diffusion–reaction system) shows that PIP^2 Net consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.

Conclusion: PoU-based partition penalties provide a principled, stable regularization for operator learning, enabling more expressive yet flexible trunk outputs; PIP^2 Net advances SOTA in data-efficient operator learning for parameterized PDEs.

Abstract: Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \emph{P}hysics-\emph{i}nformed \emph{P}artition \emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.

</details>


### [77] [SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs](https://arxiv.org/abs/2512.15088)
*Xianglin Wu,Chiheb Ben Hammouda,Cornelis W. Oosterlee*

Main category: cs.LG

TL;DR: 提出 SigMA（Signature Multi-head Attention）——将路径签名与多头自注意力结合的架构，用于fBm驱动的SDE参数推断，在合成与真实数据上优于 CNN/LSTM/Transformer/Deep Signature，且模型更紧凑、鲁棒。


<details>
  <summary>Details</summary>
Motivation: 非马尔可夫、缺乏半鞘过程结构的 fBm 驱动 SDE 给参数估计带来挑战。路径签名提供高效、紧凑的特征表示，注意力机制可捕捉时间序列中的长程相关性与结构信息。建立一个可扩展且鲁棒的推断框架，以提高估计精度与模型复杂度之间的权衡。

Method: 提出 SigMA，将路径签名与多头自注意力相结合，辅以卷积预处理层和多层感知机以实现有效特征编码。以合成路径（fBm 驱动的 SDE、fractional OU、rough Heston）为训练数据，目标包括赫斯特指数估计与联合多参数推断，并具备对未知轨迹的泛化能力。

Result: 在合成数据与两组真实数据（股指 realized volatility、锂离子电池退化）上的实验中，SigMA 在准确性、鲁棒性与模型紧凑性方面持续优于 CNN、LSTM、原生 Transformer 与 Deep Signature 基线。

Conclusion: 将路径签名与注意力机制结合，提供一种对具有粗糽或强持续时间结构的随机系统参数推断的有效、可扩展框架。

Abstract: Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.

</details>


### [78] [FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation](https://arxiv.org/abs/2512.15116)
*Runze Li,Hanchen Wang,Wenjie Zhang,Binghao Li,Yu Zhang,Xuemin Lin,Ying Zhang*

Main category: cs.LG

TL;DR: 提出 FADTI，一种基于扩散的多变量时间序列插补框架，通过可学习的傅里叶偏置投影在频域中引入特征调制的先验，并结合自注意力与门控卷积进行时序建模，在多项基准数据集（含新生物时间序列数据集）上，尤其在高缺失率下，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多变量时间序列中，传感器故障和不规则采样经常导致大量缺失值。现有基于 Transformer 和扩散的插补模型缺乏显式的先验（inductive bias）和频率感知，导致对结构化缺失模式和分布漂移的泛化能力有限。

Method: 提出 FADTI：一个扩散式生成插补框架，加入可学习的傅里叶偏置投影（FBP）模块，通过多谱基实现对稳定与非稳定模式的自适应编码，将频域先验注入到生成插补过程；同时用自注意力和门控卷积进行时序建模。FBP 支持多种谱基，能够在频域引入对不同时间特性的偏置。

Result: 在多项基准数据集（含新引入的生物时间序列数据集）上，FADTI 在缺失率较高时对比现有最优方法表现更优，显示出对结构化缺失和分布变化的鲁棒性和泛化能力。

Conclusion: 引入频域可学习偏置的特征调制与扩散式插补，提升了多变量时间序列插补的性能与鲁棒性，尤其在高缺失率场景下。代码开放可得。

Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF

</details>


### [79] [Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany](https://arxiv.org/abs/2512.15140)
*Roland Baatz*

Main category: cs.LG

TL;DR: 对德国NUTS-3区域作物产量预测的ML模型存在时间维度上的泛化缺口；在空间分割的测试集上表现良好并不能保证对未来年份的预测能力，且SHAP等解释方法在模型泛化性差时仍可能给出看似可信的特征重要性。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习在农业/环境数据中的跨时空泛化能力，并检视后验解释在真实广义性不足时的可信度。

Method: 对比评估：集成树模型（XGBoost、Random Forest）与深度学习模型（LSTM、TCN），使用高质量、长期的德国NUTS-3作物产量数据；进行空间分割与时间独立年度的验证，分析SHAP特征重要性，讨论 domain-aware验证及混合建模策略。

Result: 所有模型在空间分割的测试集上表现可观，但在时间独立验证年上显著下降；高测试集准确性并不等同于稳健的时间泛化能力；SHAP在泛化差的模型上仍可能给出看似可信的解释。强调需要基于验证的解释，倡导领域感知的验证、混合建模与对解释性方法的更严格审查。

Conclusion: 强调在环境数据科学中对泛化与解释的评价需要更健壮的验证框架；呼吁领域导向的验证、混合建模和对解释性方法的谨慎使用，以提升对数据驱动农业预测的信任度。

Abstract: This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).
  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.
  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?

</details>


### [80] [Understanding NTK Variance in Implicit Neural Representations](https://arxiv.org/abs/2512.15169)
*Chengguang Ou,Yixin Zhuang*

Main category: cs.LG

TL;DR: 提出一个统一视角，将 INR 的谱偏差与 NTK 条件数联系起来，揭示位置编码、球面归一化、Hadamard 调制等组件如何通过改变输入相似性与方差分量来降低 NTK 的特征值方差，从而提升收敛速度与重建质量。


<details>
  <summary>Details</summary>
Motivation: 解释为何 INR 常规以有限输入交互导致谱偏差，以及现有工作如何仅在NTK层面解释而未系统解耦各种架构影响。

Method: 推导常见 INR 组件的方差分解，给出闭式解，分析输入相似性、尺度因子对NTK本征值方差的影响；通过数值和实验验证。

Result: 给出位置编码 reshapes input similarity、球面归一化通过层内缩放降低方差、Hadamard 调制引入下限的相似性因子从而实现乘法降方差；实验显示更快更稳定收敛，重建质量提升。

Conclusion: 统一视角解释了多种 INR 架构如何通过改善NTK 条件来缓解谱偏差；建议在设计 INR 时关注小集合的相似性因素和尺度项以提高性能。

Abstract: Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.

</details>


### [81] [Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT](https://arxiv.org/abs/2512.15206)
*Liyu Zhang,Yejia Liu,Kwun Ho Liu,Runxi Huang,Xiaomin Ouyang*

Main category: cs.LG

TL;DR: 提出Chorus，一种面向IoT的上下文感知、数据无关的模型定制方法，能够在无目标域数据的情况下适应未见部署环境中的上下文漂移。


<details>
  <summary>Details</summary>
Motivation: 现实世界的物联网应用中传感器因放置位置、环境等上下文因素导致数据模式和性能显著变化，现有领域自适应方法往往忽略上下文或难以处理 unseen context shifts。

Method: Chorus通过无监督的跨模态重建，将未标注的传感器数据和语言上下文嵌入进行对齐，同时对上下文嵌入空间进行正则化以学习鲁棒、泛化的上下文表达。随后在有限标注样本上训练一个轻量级门控头，以动态平衡传感器信号和上下文对模型的贡献。为降低推理延迟，引入上下文缓存机制，在检测到上下文漂移时才更新缓存。

Result: 在IMU、语音与WiFi传感任务上出现多种上下文漂移的情景，Chorus相较于最先进基线在 unseen contexts 中性能提升最多达11.3%，且在智能手机和边缘设备上的推理延迟保持可比。

Conclusion: 面向未见部署上下文的传感数据自适应可以显著提升性能，Chorus通过上下文重建、鲁棒嵌入、门控融合与缓存机制实现高效、数据无需求的定制化模型适配。

Abstract: In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.

</details>


### [82] [Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis](https://arxiv.org/abs/2512.15250)
*Youssef Ghallab,Omar Iraqy,Mohamed Kandil,Mohamed Ashraf,Saadeldine Eletter,Morougue Ghazal,Ayman Khalafallah,Nagwa El-Makky*

Main category: cs.LG

TL;DR: 提出了一种基于 foundation-model 思路的多模态生理信号分析框架。通过对 ECG 使用 CBraMod 进行大规模自监督预训练并引入双重 masking 捕捉同导联之间与跨导联关系，同时对 EEG 使用预训练的 CBraMod，构建对称的 ECG 编码器。将两种模态的表示通过简单的嵌入拼接进行融合，分类头学习跨模态交互，在情感识别任务中接近状态-of-the-art，展示了设计良好的生理信号编码器在标签受限下的有效性。


<details>
  <summary>Details</summary>
Motivation: 生理信号（ECG/EEG）提供互补信息，但多模态标注稀缺、模态间差异大，难以发挥联合信息。需要基于 foundation-model 的通用、可扩展的编码器来获得丰富且可迁移的表示，并实现高效的跨模态融合。

Method: 对 ECG 使用 CBraMod 编码器进行大规模自监督预训练，提出双 masking 策略以捕捉峰间和导联内的依赖关系；对 EEG 使用预训练的 CBraMod，并对 ECG 训练一个对称的编码器；通过简单的嵌入拼接将两模态表示融合，训练分类头以学习跨模态交互。

Result: 在情感识别任务上实现接近/近似 state-of-the-art 的性能，表明精心设计的生理信号编码器即使采用简单的融合方式也能显著提升下游性能；凸显 foundation-model 风格在生理信号整体理解、标签高效及泛化能力方面的潜力。

Conclusion: 以基础模型思路设计的 ECG/EEG 编码器结合简易融合方法，能够实现可扩展、标注友好、对健康与情感计算等应用具有良好泛化能力的解决方案。

Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.

</details>


### [83] [Topological Metric for Unsupervised Embedding Quality Evaluation](https://arxiv.org/abs/2512.15285)
*Aleksei Shestov,Anton Klenitskiy,Daria Denisova,Amurkhan Dzagkoev,Daniil Petrovich,Andrey Savchenko,Maksim Makarenko*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.

</details>


### [84] [A Regime-Aware Fusion Framework for Time Series Classification](https://arxiv.org/abs/2512.15378)
*Honey Singh Chauhan,Zahraa S. Abdallah*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.

</details>


### [85] [EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning](https://arxiv.org/abs/2512.15405)
*Jianfei Ma,Wee Sun Lee*

Main category: cs.LG

TL;DR: 提出 EUBRL：一种利用 epistemic 指导的贝叶斯强化学习算法，在无限-horizon 折扣 MDP 中实现近最小极大回报与样本复杂度的理论保证，并在稀疏奖励、长时程和随机性任务中展现良好样本效率、可扩展性与一致性。


<details>
  <summary>Details</summary>
Motivation: 在已知与未知的边界，智能体不可避免地面临探索与利用的抉择。 epistemic 不确定性反映了因知识有限而导致的系统性不确定性。需要一个能在此类边界条件下进行 principled 探索的算法，以提高学习效率和稳健性。

Method: 提出基于贝叶斯强化学习的算法 EUBRL，通过利用后验中的 epistemic 指导来进行探索；在估计误差引发的逐步后悔(per-step regret)上自适应减少，从而实现 principled 探索。对一类表达力充分的先验，在无限-horizon 折扣 MDP 下给出 nearly minimax-optimal 的回报与样本复杂度保证。

Result: 理论方面，给出近似最优的回报上界（近似 minimax-optimal 的回报与相应的样本复杂度）。实验方面，在稀疏奖励、长时间步长且带随机性的任务中，EUBRL 展现出更高的样本效率、 scalable 能力及结果的一致性。

Conclusion: EUBRL 提供了一个结合理论 guarantees 与经验表现的贝叶斯 RL 框架，能够在知识边界处进行更高效且更稳健的探索。未来工作可拓展到更广的先验设定、降低计算开销、以及在非平稳或对抗性环境中的鲁棒性研究。

Abstract: At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.

</details>


### [86] [FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows](https://arxiv.org/abs/2512.15420)
*Yeonwoo Cha,Semin Kim,Jinhyeon Kwon,Seunghoon Hong*

Main category: cs.LG

TL;DR: FlowBind 提出一个高效的任意模态到任意模态生成框架：以共享潜在空间为核心，使用模态特异的可逆流将潜在映射到各模态；单一目标函数训练，推理时直接翻译，显著降低数据和计算要求，达到与现有方法相近的生成质量但参数更少、训练更快。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的方法在效率方面受限：需要大规模带标签/配对数据、联合分布建模成本高、训练通常是多阶段且复杂。需要更高效、可扩展的任意模态生成。

Method: 学习一个共享潜在空间，模态特异的可逆流连接潜在与各模态；通过单一flow-matching目标进行联合优化；推理阶段可逆流作为编码器/解码器实现跨模态翻译；通过潜在因子化交互，能够利用任意子模态集合进行训练。

Result: 在文本、图像、音频上实验，生成质量与对比方法相当，同时参数量可减少最多6倍，训练速度提升至原方法的10倍；项目页有代码。

Conclusion: FlowBind 提供一种高效、灵活的任意模态生成框架，显著降低数据需求与计算成本，具备良好扩展性与可复现性。

Abstract: Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.

</details>


### [87] [Statistics of Min-max Normalized Eigenvalues in Random Matrices](https://arxiv.org/abs/2512.15427)
*Hyakka Nakada,Shu Tanaka*

Main category: cs.LG

TL;DR: 研究 min-max 归一化后随机矩阵的特征值分布及其累积分布的缩放规律，并推导矩阵分解中的残差误差，辅以数值验证。


<details>
  <summary>Details</summary>
Motivation: 数据科学中广泛使用数据归一化，理解归一化后的特征值统计性质有助于理论分析与实际算法的性能评估。

Method: 利用已提出的 min-max 归一化特征值分布，评估其累积分布的缩放规律；推导矩阵分解过程中的残差误差；通过数值实验验证理论。

Result: 验证了归一化分布在累积分布尺度上的缩放规律，给出残差误差的理论表达并通过数值实验得到一致结果。

Conclusion: 为随机矩阵分析与相关算法提供了关于归一化后特征值分布及其对矩阵分解的影响的理论与实证支持。

Abstract: Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.

</details>


### [88] [FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments](https://arxiv.org/abs/2512.15430)
*Quanxi Zhou,Wencan Mao,Manabu Tsukada,John C. S. Lui,Yusheng Ji*

Main category: cs.LG

TL;DR: A generalized Feature Model-Based Enhanced Actor-Critic (FM-EAC) framework that fuses planning, acting, and learning for multi-task RL, leveraging feature-based models to improve transferability; demonstrated superior performance and configurable sub-networks.


<details>
  <summary>Details</summary>
Motivation: Transferability across tasks and scenarios remains a major challenge in RL; MBRL and MFRL follow different paths but both underperform in generalization; a unified framework is needed to generalize to multiple tasks with modular components.

Method: Proposes FM-EAC, a hybrid architecture combining model-based planning with an enhanced actor-critic, using feature-based models; allows customization of sub-networks per user requirements; integrates planning, acting, learning for multi-task control in dynamic environments.

Result: Empirical simulations in urban and agricultural settings show FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods.

Conclusion: FM-EAC provides improved generalizability and flexibility for multi-task control, and its modular sub-networks enable user-specific customization.

Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.

</details>


### [89] [Double Horizon Model-Based Policy Optimization](https://arxiv.org/abs/2512.15439)
*Akihiro Kubo,Paavo Parmas,Shin Ishii*

Main category: cs.LG

TL;DR: 双视角模型基强化学习（DHMBPO）：将展开过程分为长期分布回放（DR）与短期训练回放（TR）两种目标区间，以平衡分布漂移、模型偏差与梯度方差，从而在连续控制任务上提升样本效率与运行时间效率。


<details>
  <summary>Details</summary>
Motivation: 在模型-based RL 中，展开回放的长度影响分布漂移、模型偏差和梯度估计的方差，存在互相矛盾的最优 horizon。单一 horizon 难以同时优化样本效率和计算成本。

Method: 提出双视角回放框架：长的分布回放（DR）用于获取近似 on-policy 的状态分布、缓解分布漂移；短的训练回放（TR）使用可导转移实现更精确的值梯度估计、稳定梯度更新，减少更新次数和总体运行时间；两者协同训练实现折中。

Result: 在连续控制基准上，双视角方法在样本效率和运行时间方面超过现有的MBRL方法。

Conclusion: 通过将回放过程分成长期的分布回放和短期的训练回放，DHMBPO 成功平衡了分布漂移、模型偏差与梯度不稳定性，提供了更优的综合性能。

Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.

</details>


### [90] [Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting](https://arxiv.org/abs/2512.15442)
*Neeraj Sarna,Yuanyuan Li,Michael von Gablenz*

Main category: cs.LG

TL;DR: 研究将链式推理提示、任务指令提示、负提示和重写提示等方法联合用于降低文本到图像模型生成受版权保护内容的风险，并在多模型上进行数值实验评估。


<details>
  <summary>Details</summary>
Motivation: 解决大规模文本到图像模型在训练数据中记忆并再现受版权保护材料而引发的版权侵权风险，以及由此对AI用户和开发者带来的法律与经济压力。

Method: 提出一种将链式推理提示、任务指令提示、负提示、提示重写四种技术结合的公式；通过评估生成图像与受版权保护图像的相似性以及与输入文本的相关性来衡量效果；在多种模型上进行数值实验，比较模型复杂度对效果的影响。

Result: 实验显示这些技巧的有效性随模型复杂度变化而变化；在某些模型上能降低对版权材料的再现程度并保持对输入的相关性，但并非对所有模型均同样有效，需进一步量化与改进。

Conclusion: 将链式推理、任务指令、负提示与提示重写相结合的策略在版权风险缓解方面具有潜力，模型复杂度是关键因素；未来需要更系统的评估与通用化方法以实现更稳健的版权风险控制。

Abstract: Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.

</details>


### [91] [From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2512.15460)
*Xiangrui Xu,Zhize Li,Yufei Han,Bin Wang,Jiqiang Liu,Wei Wang*

Main category: cs.LG

TL;DR: A framework InvLoss for quantifying maximum DRA effectiveness in FL; derives upper bound, connects risk to Jacobian spectral properties, provides InvRE risk estimator, and proposes two adaptive noise defenses; validated experimentally.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a theoretically-grounded risk quantification framework for data reconstruction attacks in federated learning.

Method: Introduce invertibility loss (InvLoss), derive tight upper bound, analyze Jacobian spectral properties; develop InvRE estimator; propose two adaptive noise defenses; experimental validation.

Result: Empirical validation on real-world datasets showing InvLoss provides systematic risk evaluation and enables effective defense strategies.

Conclusion: InvLoss framework enables principled risk quantification and mitigation of DRAs in FL, guiding defense design and evaluation.

Abstract: Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.

</details>


### [92] [Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier](https://arxiv.org/abs/2512.15492)
*Adrián Detavernier,Jasper De Bock*

Main category: cs.LG

TL;DR: 对分类器单个预测的可靠性评估存在两条独立路径：鲁棒性量化（RQ）与不确定性量化（UQ）。在若干基准数据集上的比较显示两者无明确胜者，彼此互补且可组合成混合方法，显著优于任一方法；并给出每个数据集不确定性与鲁棒性对不可置信性的相对贡献。


<details>
  <summary>Details</summary>
Motivation: 解决单一预测可靠性评估的两种主流思路之间的差异和潜在局限，探索它们的互补性以及将二者结合以提升整体预测可靠性的可能性。

Method: 在多个基准数据集上分别应用RQ与UQ两种方法进行对比分析，并提出一个将两者结合的混合方法；通过实验评估混合方法在可靠性评估上的性能是否超越单独使用的RQ或UQ，并对每个数据集量化不确定性与鲁棒性作为不可靠性来源的相对重要性。

Result: 没有明确的优胜者；RQ与UQ互为补充，混合方法在多数数据集中优于任一单独方法；并为每个数据集给出不确定性和鲁棒性对不可置信性的相对贡献评估。

Conclusion: 两种方法具备互补性，耦合使用可提升预测单例的可靠性评估性能；在不同数据集上，可以依据相对重要性权衡使用，以获得对不可靠性来源的定量理解。

Abstract: We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.

</details>


### [93] [Soft Geometric Inductive Bias for Object Centric Dynamics](https://arxiv.org/abs/2512.15493)
*Hampus Linander,Conor Heins,Alexander Tschantz,Marco Perin,Christopher Buckley*

Main category: cs.LG

TL;DR: Soft geometric inductive bias via geometric algebra neural networks in object-centric world models improves long-horizon physical fidelity in 2D rigid-body dynamics with obstacles, offering robust generalization and sample efficiency and serving as a middle ground between hand-crafted physics and unstructured nets.


<details>
  <summary>Details</summary>
Motivation: Exact group equivariance can hurt when symmetries are imperfect or broken. A milder, soft inductive bias could provide robustness and better generalization for learning physical dynamics.

Method: Develop object-centric world models that employ geometric algebra neural networks to introduce a soft geometric inductive bias. Train to predict the next step autoregressively in simulated 2D rigid-body environments with static obstacles.

Result: Long-horizon rollouts show improved physical fidelity with the soft inductive bias compared to non-equivariant baselines. The approach aligns with soft-equivariance ideas and demonstrates robust generalization and sample efficiency.

Conclusion: Geometric algebra offers a viable middle ground between handcrafted physics and unstructured deep nets, enabling efficient, robust dynamics models for multi-object scenes.

Abstract: Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.

</details>


### [94] [Tracking Temporal Dynamics of Vector Sets with Gaussian Process](https://arxiv.org/abs/2512.15538)
*Taichi Aida,Mamoru Komachi,Toshinobu Ogiso,Hiroya Takamura,Daichi Mochihashi*

Main category: cs.LG

TL;DR: 提出一种基于无限维高斯过程并结合随机傅里叶特征的方法，用于建模随时间变化的向量集合的潜在分布，从而得到紧凑且可比较的时间表征，并可视化其演变，应用于犯罪分布与词嵌入数据，表现出可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解跨域的时间性向量集合的演化及其结构性变化；现有方法在高维、随时间变化的集合上难以得到稳定、可比较的表示。

Method: 将每个时间点向量集合的潜在分布建模为无限维高斯过程；用随机傅里叶特征近似潜在函数，得到紧凑的、可跨时间对比的向量表示；可在低维空间跟踪和可视化时间转变。

Result: 在社会学数据（犯罪分布）与语言数据（词嵌入）上验证，方法能够捕捉时间动态，得到可解释且鲁棒的表征。

Conclusion: 为跨域的时序向量集的结构变化分析提供一个强有力的框架，便于在低维中可视化和比较。

Abstract: Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.

</details>


### [95] [Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation](https://arxiv.org/abs/2512.15574)
*Yuxin Cai,Yanyong Huang,Jinyuan Chang,Dongjie Wang,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出 JUICE，在未标注的多视图不完整数据上实现特征与实例的联合无监督共选并通过跨视图补全提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将缺失数据填补与视图拼接作为两个独立过程，忽略了共选与补全的潜在互相促进关系；此外，简单合并多视图无法充分利用不同视图的互补信息。需要一个统一框架，将重建与共选耦合，并利用跨视图邻域关系改善缺失值推断。

Method: 提出 JUICE：在一个统一框架中对不完整的多视图数据进行重建，联合学习特征与实例的无监督共选；通过跨视图邻域信息学习样本间关系并在重建过程中迭代地 refine 缺失值；实现对更具代表性的特征和样本的选取。

Result: 大量实验证明 JUICE 在与现有方法的比较中实现更优的特征与实例选择，提升整体性能。

Conclusion: JUICE 作为一个统一且无监督的框架，成功将重建、跨视图协作与共选耦合，针对未标注的多视图不完整数据实现更有效的特征与实例选择。

Abstract: Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.

</details>


### [96] [Corrective Diffusion Language Models](https://arxiv.org/abs/2512.15596)
*Shuibai Zhang,Fred Zhangzhi Peng,Yiheng Zhang,Jin Pan,Grigorios G. Chrysos*

Main category: cs.LG

TL;DR: 提出一种面向纠错的后训练原则，通过显式监督可见的错误标记，提升扩散语言模型在错误定位与就地纠错中的能力；并引入 Code Revision Benchmark (CRB) 评估纠错与就地改写能力；实验证明该方法在代码修订任务和受控设置中显著优于标准 MDLM，并提升纯完成任务性能。


<details>
  <summary>Details</summary>
Motivation: 标准的掩码扩散语言模型难以自然地体现纠错能力，因为模型往往无法在完整输入中识别不可靠的标记，使得基于置信度的改写无效。需要一种明确的监督信号来引导模型识别并纠正错误，从而实现可控的错误定位与就地修订。

Method: 提出一种纠错导向的后训练原则，在训练阶段显式监督可见的错误标记，鼓励对错误标记的置信度降低并进行就地修订；结合新的评测基准 CRB，用以衡量模型在错误定位、就地纠错和保持原有正确内容方面的能力。

Result: 在代码修订任务和受控设置中，采用该后训练原则的模型在纠错场景中显著优于标准 MDLM，同时纯完成任务性能也有所提升。公开代码库可复现相关结果。

Conclusion: 纠错导向的后训练提供了一种有效的机制，使扩散语言模型具备误差感知与就地修订的能力，并通过 CRB 等基准实现可控评估与比较，具有推广到实际纠错与改写任务的潜力。

Abstract: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.

</details>


### [97] [How Smoothing is N-simplicial Attention?](https://arxiv.org/abs/2512.15600)
*Alexandre Dussolle,Pietro Liò*

Main category: cs.LG

TL;DR: 引入N- simplex注意力，将高阶交互从成对相似性扩展到更高阶关系，并将其与Rotary位置嵌入RoPE结合；通过成本有效的单纯形选择聚焦更具任务敏感性的交互，提升表达能力与计算效率；同时对平滑性进行理论分析，给出 Lipschitz 上界并指出单独的高阶注意力也会导致过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 旨在突破现有点对点或层内注意力的局限，捕捉更丰富的高阶关系以提升模型表达力，同时在计算开销可控的前提下扩展到高阶信息的传递。

Method: 提出N- simplex注意力机制，将高阶单纯形中的节点关系用于信息聚合，并与旋转位置嵌入RoPE结合；引入成本有效的单纯形选择策略，将计算聚焦于对任务最敏感的交互；对平滑性进行理论分析，推导Lipschitz上界，并通过实验证明单独的高阶注意力易产生过平滑，同时扩展了attention的传递域。

Result: 在理论上揭示了高阶注意力在表达力上的潜在收益及其对平滑性的挑战；通过简化的单纯形选择实现可控的计算负担；实验上可能展示在任务相关性和收敛性方面的改进与权衡，但也强调了需要对平滑性进行约束来避免性能下降。

Conclusion: 高阶的N- simplex注意力提供了新的建模维度，能够捕捉更丰富的交互信息，但其计算复杂度与平滑性问题需通过有效的选择策略和理论约束来平衡，才具备实际应用潜力。

Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.

</details>


### [98] [Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction](https://arxiv.org/abs/2512.15605)
*Mathieu Blondel,Michael E. Sander,Germain Vivier-Ardisson,Tianlin Liu,Vincent Roulet*

Main category: cs.LG

TL;DR: 论文提出了自回归模型（ARM）与能量基模型（EBM）之间的统一视角，通过在函数空间建立显式双射，将两类模型联系起来，并将其与最大熵强化学习中的软Bellman方程联系起来，进而证明ARM与EBM的有监督学习等价性，并给出EBM蒸馏为ARM的误差界，解释ARM的前瞻性规划能力。


<details>
  <summary>Details</summary>
Motivation: 澄清ARM与EBM之间的深层关系，解释为何ARM在后训练对齐中具有“规划能力”的本质，并为跨模型蒸馏和训练范式的统一提供理论基础。

Method: 以概率链式法则为起点，在函数空间构建ARM与EBM之间的双射；将该双射与最大熵强化学习中的软Bellman方程联系起来，推导出ARM与EBM在有监督学习中的等价性；给出EBM蒸馏到ARM的误差界，并讨论其对前瞻性推理的含义。

Result: 证明了ARM与EBM在函数空间的双射关系及其与软Bellman方程的对应性；给出了ARM和EBM的有监督学习等价性；给出EBM蒸馏至ARM的理论误差界；对ARM具备前瞻性规划能力提供了理论解释。

Conclusion: 为理解ARM的规划能力提供了EBM视角的统一框架，量化了蒸馏误差，并为两类模型的跨域训练与应用提供理论支撑，未来可在强化学习和序列建模中进一步推动这两种范式的互补与融合。

Abstract: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.

</details>


### [99] [Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary](https://arxiv.org/abs/2512.15614)
*Xinshun Feng,Mingzhe Liu,Yi Qiao,Tongyu Zhu,Leilei Sun,Shuai Wang*

Main category: cs.LG

TL;DR: BEAT 提出一种统一可转移的框架，将用户和物品行为离散化为可解释的序列，并通过向量量化自编码器构建行为词汇表，结合多层语义监督和语义对齐，将行为标记嵌入到冻结语言模型的输入中，以实现零-shot 个性化推荐并生成连贯解释。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法对 ID 表示的依赖及对语言模型结构的限制，以及真实场景中多样化意图与协同信号不易对齐的问题。

Method: 通过向量量化自编码对图形表示进行拆解，构建行为词汇表；引入多层语义监督以连接行为信号和语言空间；设计语义对齐正则，将行为令牌嵌入到冻结语言模型的输入空间。

Result: 在三个公开数据集上，BEAT 提升了零-shot 推荐表现，并生成连贯且信息丰富的解释；行为令牌能够捕捉细粒度语义，且具备与大型语言模型的插拔式对接能力。

Conclusion: BEAT 提供一个统一、可转移的框架，通过可解释的行为令牌实现对复杂行为模式的便捷嵌入，促进语言模型在推荐场景的应用及解释能力提升。

Abstract: Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.

</details>


### [100] [SoFlow: Solution Flow Models for One-Step Generative Modeling](https://arxiv.org/abs/2512.15657)
*Tianze Luo,Haotian Yuan,Zhuang Liu*

Main category: cs.LG

TL;DR: 提出 Solution Flow Models (SoFlow)，实现从零开始的一步生成。通过分析速度场的 ODE 的解函数与速度函数的关系，提出 Flow Matching 损失和解一致性损失，用于训练模型；Flow Matching 损失在训练时可提供估计的速度场以用于 CFG，提升生成性能；解一致性损失不需要 JVP 的计算，降低实现难度。实验证明，在相同 DiT 架构和相同训练轮数下，SoFlow 在 ImageNet 256x256 上的 FID-50K 比 MeanFlow 更好。


<details>
  <summary>Details</summary>
Motivation: 降低扩散/流量匹配等多步生成的效率瓶颈，推动从零开始的一步生成研究。

Method: 提出 Solution Flow Models（SoFlow），分析速度方程的速度函数与解函数之间的关系，设计 Flow Matching 损失和解一致性损失。Flow Matching 损失在训练中允许模型给出估计的速度场以进行无条件/条件引导（CFG）。解一致性损失不需要 JVP 的计算，提升实现效率。使用相同的 Diffusion Transformer（DiT）架构并在同等训练轮数下进行训练。

Result: 实验结果显示，在 ImageNet 256x256 数据集上，SoFlow 在相同架构与训练轮数条件下，FID-50K 指标优于 MeanFlow。

Conclusion: 提出的 SoFlow 框架实现从零开始的一步生成，并通过 Flow Matching 与解一致性损失提升生成效果，同时避免 JVP 的计算开销，显示出在高分辨率图像生成任务上的竞争优势。

Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.

</details>


### [101] [Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2512.15687)
*Zhenwen Liang,Sidi Lu,Wenhao Yu,Kishan Panaganti,Yujun Zhou,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: G2RL提出一种基于模型自身更新几何的梯度引导强化学习框架，通过对轨迹的最终层敏感性进行序列级特征比较，在采样组内给予引入新梯度方向的轨迹以有界的乘法奖励，以提高大语言模型在推理任务上的探索效率，优于基于熵的GRPO与外部嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习探索机制（如熵奖金或外部语义比较器）倾向于表层的行为变异，未能保证采样轨迹在优化方向上的差异性；需设计与模型实际学习方向对齐的探索信号。

Method: 对每个响应构造来自模型最终层敏感性的序列级特征；在一个采样组内比较这些特征，评估每条轨迹对策略的潜在更新方向影响。对引入新梯度方向的轨迹给予有界乘法奖励缩放，弱化冗余或越界的更新，形成自指向的探索信号；保持与PPO风格的稳定性和KL约束的一致性。

Result: 在数学与一般推理基准（MATH500、AMC、AIME24、AIME25、GPQA、MMLUpro）上，使用Qwen3 base 1.7B与4B模型，G2RL在pass@1、maj@16、pass@k等指标上持续优于基于熵的GRPO与外部嵌入方法。

Conclusion: 对比分析显示，通过将探索放在政策的自身更新空间，G2RL可以获得更多正交甚至相反方向的梯度探索，同时保持语义连贯性，揭示了更新方向空间作为引导探索的更为真实与有效的基础。

Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.

</details>


### [102] [FrontierCS: Evolving Challenges for Evolving Intelligence](https://arxiv.org/abs/2512.15699)
*Qiuyang Mang,Wenhao Chai,Zhifei Li,Huanzhi Mao,Shang Zhou,Alexander Du,Hanchen Li,Shu Liu,Edwin Chen,Yichuan Wang,Xieting Chu,Zerui Cheng,Yuan Xu,Tian Xia,Zirui Wang,Tianneng Shi,Jianzhu Yao,Yilong Zhao,Qizheng Zhang,Charlie Ruan,Zeyu Shen,Kaiyuan Liu,Runyuan He,Dong Xing,Zerui Li,Zirong Zeng,Yige Jiang,Lufeng Cheng,Ziyi Zhao,Youran Sun,Wesley Zheng,Meiyuwang Zhang,Ruyi Ji,Xuechang Tu,Zihan Zheng,Zexing Chen,Kangyang Zhou,Zhaozi Wang,Jingbang Chen,Aleksandra Korolova,Peter Henderson,Pramod Viswanath,Vijay Ganesh,Saining Xie,Zhuang Liu,Dawn Song,Sewon Min,Ion Stoica,Joseph E. Gonzalez,Jingbo Shang,Alvin Cheung*

Main category: cs.LG

TL;DR:  FrontierCS 是一个包含156个开放式问题的基准，强调未知最优解但可客观评估的任务，通过实现可执行程序来解决，覆盖算法与研究问题，提供专家参考解和自动评测器。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注已知最优解或简单任务，难以衡量模型在前沿难度上的推理与系统设计能力， FrontierCS 旨在在未知最优解的领域提供可衡量的进展。

Method: 设计、同行评审与多领域覆盖的开放式问题集；每题提供专家参考解和自动评测器；问题分为算法问题（含 NP-hard 的变体并带有客观部分分数）与研究问题；模型需实现可执行程序来解决而非直接给出答案。

Result: 经验性评估显示，前沿推理模型在算法与研究两条线都显著落后于人类专家；单纯增加推理预算并不能显著缩小差距；模型往往过度追求生成可工作的代码，而非发现高质量的算法与系统设计。

Conclusion:  FrontierCS 构成了一个前沿难度的基准，揭示当前模型在开放式问题上的局限性，强调需要提升对高层次算法与系统设计能力的研究。

Abstract: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.

</details>


### [103] [Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data](https://arxiv.org/abs/2512.15706)
*Kayode Olumoyin,Lamees El Naqa,Katarzyna Rejniak*

Main category: cs.LG

TL;DR: Proposes learning time-varying cell interactions under treatment using physics-informed neural networks (PINNs) in sparse oncological data.


<details>
  <summary>Details</summary>
Motivation: Biological populations interact with evolving effects of external interventions; fixed-parameter models fail to capture dynamic changes, especially with limited tumor data.

Method: Use physics-informed neural networks to learn time-varying interactions between tumor cells and immune cells and predict subpopulation trajectories at unobserved times, under combination therapies.

Result: The approach yields plausible subpopulation trajectories consistent with biological explanations and demonstrates the ability to infer evolving interactions from limited data.

Conclusion: Provides a framework for learning dynamic, intervention-influenced interactions among biological organisms when data are scarce, potentially guiding treatment design and understanding tumor-immune dynamics.

Abstract: In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics. In oncology, this is further exacerbated by the fact that experimental data are often sparse and sometimes are composed of a few time points of tumor volume. In this paper, we propose to learn time-varying interactions between cells, such as those of bladder cancer tumors and immune cells, and their response to a combination of anticancer treatments in a limited data scenario. We employ the physics-informed neural network (PINN) approach to predict possible subpopulation trajectories at time points where no observed data are available. We demonstrate that our approach is consistent with the biological explanation of subpopulation trajectories. Our method provides a framework for learning evolving interactions among biological organisms when external interventions are applied to their environment.

</details>
