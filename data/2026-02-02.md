<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 10]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 145]
- [eess.SP](#eess.SP) [Total: 7]
- [cs.NI](#cs.NI) [Total: 3]
- [eess.SY](#eess.SY) [Total: 2]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Capacity of Two-User Wireless Systems Aided by Movable Signals](https://arxiv.org/abs/2601.22358)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 本文研究可移动信号在两用户系统中的功效：通过动态调频实现频道正交，显著扩大容量区域，并在受限频率范围内获得高达45%的速率提升。


<details>
  <summary>Details</summary>
Motivation: 在智能无线环境中，现有的RIS和柔性天线已无法充分利用频率维度的灵活性；考察可移动信号是否能开启更高效的频谱利用与用户分离。

Method: 通过针对两用户多址与广播通道的容量区域分析，利用可移动信号动态调频以正交化用户信道，并对受限频率范围内的调频问题进行优化。

Result: 可移动信号将MAC与BC的容量区域扩大，且在频率受限下实现多达45%的总速率提升。

Conclusion: 论文阐明可移动信号能显著扩展多用户无线系统的容量区域，并在频率受限的环境下相较于固定信号实现最多45%的总速率提升。

Abstract: Movable signals have emerged as a third approach to enable smart radio environments (SREs), complementing reconfigurable intelligent surfaces (RISs) and flexible antennas. This paper investigates their potential to enhance multi-user wireless systems. Focusing on two-user systems, we characterize the capacity regions of the multiple access channel (MAC) and broadcast channel (BC). Interestingly, movable signals can dynamically adjust the operating frequency to orthogonalize the user channels, thereby significantly expanding the capacity regions. We also study frequency optimization, constraining it in a limited frequency range, and show that movable signals provide up to 45% sum rate gain over fixed signals.

</details>


### [2] [5G LDPC Codes as Root LDPC Codes via Diversity Alignment](https://arxiv.org/abs/2601.22470)
*Hyuntae Ahn,Inki Kim,Hee-Youl Kwak,Yongjune Kim,Chanki Kim,Sang-Hyo Kim*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper studies the diversity of protographbased quasi-cyclic low-density parity-check (QC-LDPC) codes over nonergodic block-fading channels under iterative beliefpropagation decoding. We introduce diversity evolution (DivE), a Boolean-function-based analysis method that tracks how the fading dependence of belief-propagation messages evolves across decoding iterations. Under a Boolean approximation of block fading, DivE derives a Boolean fading function for each variable node (VN) output (i.e., the a-posteriori reliability after iterative decoding), from which the VN diversity order can be directly determined. Building on this insight, we develop a greedy blockmapping search that assigns protograph VNs to fading blocks so that all information VNs achieve full diversity, while including the minimum additional parity VNs when full diversity is infeasible at the nominal rate. Numerical results on the 5G New Radio LDPC codes show that the proposed search finds block mappings that guarantee full diversity for all information bits without modifying the base-graph structure, yielding a markedly steeper high-SNR slope and lower BLER than random mappings.

</details>


### [3] [Successive Cancellation List Decoding of Extended Reed-Solomon Codes](https://arxiv.org/abs/2601.22482)
*Xiaoqian Ye,Jingyu Lin,Junjie Huang,Li Chen,Chang-An Zhao*

Main category: cs.IT

TL;DR: 改进RS码列表解码：把eRS码映射为极化码，使用SC/SCL，结合预变换矩阵特性，理论分析+数值验证表明性能提升


<details>
  <summary>Details</summary>
Motivation: 提高扩展RS码在二元有限域上的列表解码性能

Method: 将eRS码转化为n个二进制极化码，利用SC/SCL解码，研究预变换矩阵列线性独立性

Result: 通过理论分析和数值验证，证明了SC和SCL性能提升

Conclusion: 提出的新方法在满足Singleton界的eRS码上实现了可行的列表解码，证明了预变换矩阵在性能中的关键作用

Abstract: Reed-Solomon (RS) codes are an important class of non-binary error-correction codes. They are particularly competent in correcting burst errors, being widely applied in modern communications and data storage systems. This also thanks to their distance property of reaching the Singleton bound, being the maximum distance separable (MDS) codes. This paper proposes a new list decoding for extended RS (eRS) codes defined over a finite field of characteristic two, i.e., F_{2^n}. It is developed based on transforming an eRS code into n binary polar codes. Consequently, it can be decoded by the successive cancellation (SC) decoding and further their list decoding, i.e., the SCL decoding. A pre-transformed matrix is required for reinterpretating the eRS codes, which also determines their SC and SCL decoding performances. Its column linear independence property is studied, leading to theoretical characterization of their SC decoding performance. Our proposed decoding and analysis are validated numerically.

</details>


### [4] [Flexible FTN-OTFS for High-Mobility LEO Satellite-to-Ground Communication](https://arxiv.org/abs/2601.22526)
*Chaorong Zhang,Hui Xu,Benjamin K. Ng,Yue Liu,Chan-Tong Lam,Halim Yanikomeroglu*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, a lightweight LEO satellite-assisted flexible faster-than-Nyquist (FTN)-orthogonal time frequency space (OTFS) (LEO-FFTN-OTFS) scheme is proposed to address the stringent constraints on onboard power consumption and the severe impact of fast time-varying channels in non-terrestrial networks. A rigorous system framework incorporating realistic 3GPP Tapped Delay Line (TDL) channel models is established to accurately capture high-mobility propagation characteristics. To counteract channel aging effects while maintaining low computational complexity, an SNR-aware flexible FTN strategy is introduced, wherein a low-complexity Look-Up Table (LUT) is utilized to adaptively optimize the time-domain compression factor based on instantaneous channel responses. Through this mechanism, the trade-off between rate acceleration and interference penalty is effectively resolved, ensuring that spectral efficiency is maximized while strict reliability constraints are satisfied with minimal processing overhead. Moreover, a comprehensive theoretical analysis is provided, in which analytical expressions for effective throughput, energy efficiency, and bit error rate are derived. Finally, it is demonstrated by extensive simulations that the proposed scheme significantly outperforms static FTN benchmarks, offering a superior balance of high throughput and robustness for next-generation LEO communications.

</details>


### [5] [Quantum $(r,δ)$-Locally Recoverable BCH and Homothetic-BCH Codes](https://arxiv.org/abs/2601.22567)
*Carlos Galindo,Fernando Hernando,Ryutaroh Matsumoto*

Main category: cs.IT

TL;DR: 用BCH同类码构造实现量子多重失效恢复的最优局部可恢复码


<details>
  <summary>Details</summary>
Motivation: 构建可容忍多重失效的分布式量子纠错码

Method: 利用BCH与同类BCH码的欧几里得或厄米对偶包含性构造量子$(r,δ)$-局部可恢复码

Result: 得到最优的纯量子$(r,δ)$-LRC，并满足Singleton-like界限

Conclusion: 提供一种从经典BCH码生成最优量子局部可恢复码的方法

Abstract: Quantum $(r,δ)$-locally recoverable codes ($(r,δ)$-LRCs) are the quantum version of classical $(r,δ)$-LRCs designed to recover multiple failures in large-scale distributed and cloud storage systems. A quantum $(r,δ)$-LRC, $Q(C)$, can be constructed from an $(r,δ)$-LRC, $C$, which is Euclidean or Hermitian dual-containing.
  This article is devoted to studying how to get quantum $(r,δ)$-LRCs from BCH and homothetic-BCH codes. As a consequence, we give pure quantum $(r,δ)$-LRCs which are optimal for the Singleton-like bound.

</details>


### [6] [Multi-target DoA estimation with a single Rydberg atomic receiver by spectral analysis of spatially-resolved fluorescence](https://arxiv.org/abs/2601.22704)
*Liangcheng Han,Haifan Yin,Mérouane Debbah*

Main category: cs.IT

TL;DR: 将Rydberg聚光细胞内的荧光映射到空间频率，通过与强振荡器叠加实现正弦波线性化，再用Prony谱估计实现多目标、宽带方向定位。


<details>
  <summary>Details</summary>
Motivation: 本文试图突破Rydberg基波束方向定位（DoA）技术在接收阵列复杂性和单目标、窄带限制方面的瓶颈，致力于实现多目标、宽带、高分辨率的定位与传感。

Method: 利用蒸汽细胞内的荧光分布进行空间分辨，将入射信号与强本地振荡器相叠加，将复杂的原子吸收模式线性化为正弦波叠加；在此表示下，空间频率直接映射到目标方向，然后用Prony方法进行谱估计，形成名为Imaging-based Spectral Estimation（ISE）的方案。

Result: ISE方法能有效实现多目标检测，恢复并放宽了对细胞长度的依赖，恢复了传感器的全宽带能力；仿真验证其对多目标的分辨优势，理论分析给出CRLB作为性能基准，并指出对多通道Rydberg接收机与全息MIMO的潜在应用。

Conclusion: 通过空间荧光分辨与LO叠加，将Rydberg传感器的多目标、宽带DoA定位限制降为谱估计问题，实现了更高效、真实、连续的方向估计，为多通道、全息MIMO感知奠定了基础。

Abstract: Rydberg-based Direction-of-Arrival (DoA) estimation has been hampered by the complexity of receiver arrays and the single-target, narrow-band limitations of existing single-receiver methods. This paper introduces a novel approach that addresses these limitations. We demonstrate that by spatially resolving the fluorescence profile along the vapor cell, the multi-target problem can be effectively solved. Our approach hinges on the insight that by superimposing incoming signals with a strong local oscillator (LO), the complex atomic absorption pattern is linearized into a simple superposition of sinusoids. In this new representation, each spatial frequency uniquely and directly maps to the DoA of a target. This reduces the multi-target challenge into a spectral estimation problem, which we address using Prony's method. Our approach, termed Imaging-based Spectral Estimation (ISE), inherently supports multi-target detection and restores the full broadband capability of the sensor by removing the restrictive cell-length dependency. This development also shows potential for realizing multi-channel Rydberg receivers and the continuous-aperture sensing required for holographic multiple-input multiple-output (MIMO). We develop a comprehensive theoretical model, derive the Cramer-Rao Lower Bound (CRLB) as a performance benchmark, and present simulations validating the effectiveness of the approach to resolve multiple targets.

</details>


### [7] [Status Updating via Integrated Sensing and Communication: Freshness Optimisation](https://arxiv.org/abs/2601.22901)
*Touraj Soleymani,Mohamad Assaad,John S. Baras*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper studies strategic design in an integrated sensing and communication (ISAC) architecture for status updating of remotely navigating agents. We consider an ISAC-enabled base station that can sense the state of a remote source and communicate this information back to the source. Both sensing and communication succeed with given probabilities and incur distinct costs. The objective is to optimise a long-term cost that captures information freshness, measured by the age of information (AoI), at the source together with sensing and communication overheads. The resulting sequential decision problem is formulated as a discounted infinite-horizon Markov decision process with a two-dimensional AoI state, representing information freshness at the source and at the base station. We prove that the optimal stationary policy admits a monotone threshold structure characterised by a nondecreasing switching curve in the AoI state space. Our numerical analysis illustrates the structures of the value function and the optimal decision map. These results demonstrate that freshness-based objectives can be naturally integrated into ISAC design, while yielding interpretable and implementable strategies.

</details>


### [8] [Feedback Control via Integrated Sensing and Communication: Uncertainty Optimisation](https://arxiv.org/abs/2601.22912)
*Touraj Soleymani,Mohamad Assaad,John S. Baras*

Main category: cs.IT

TL;DR: 本文在ISAC架构下对对源的感知与控制信息传输进行阈值切换最优设计，阈值与源、基站估计协方差相关，为提升机电系统反馈控制提供理论依据。


<details>
  <summary>Details</summary>
Motivation: 研究 ISAC 架构在机电系统反馈控制中的战略设计，以解决感知与通信在源状态跟踪与控制信息传输间的权衡问题。

Method: 在高斯马尔科夫源、IID 伯努利感知及通信链路、有限时域线性二次高斯成本下，利用不确定性感知合成严谨推导最优策略。

Result: 证明基站的最优切换策略为估计协方差阈值策略；源端的最优控制策略为线性。阈值域随源/基站不确定性变化而单调改变。

Conclusion: 基站实现的阈值切换策略和源估计线性控制策略在 ISAC 架构中实现了最优控制，阈值域随源不确定性增大而扩展，随基站不确定性增大而收缩。

Abstract: This paper studies strategic design in an integrated sensing and communication (ISAC) architecture for feedback control of cyber-physical systems. We focus on a setting in which the regulation of a physical process (i.e., remote source) is performed via an ISAC-enabled base station. The base station can alternate between tracking the state of the source and delivering control-relevant information back to the source. For a Gauss-Markov source subject to i.i.d. Bernoulli sensing and communication links, under a finite-horizon linear-quadratic-Gaussian cost, we rigorously characterise the optimal policies through an uncertainty-aware synthesis. We establish that the optimal switching policy, for the ISAC system at the base station, is threshold-based in terms of the source and base-station estimation covariances, while the optimal control policy, for the actuator at the source, is linear in the source state estimate. We show that the threshold region$\unicode{x2014}$defined as the set of estimation covariance pairs for which communication is preferred over sensing$\unicode{x2014}$expands with increasing source uncertainty and contracts with increasing base-station uncertainty.

</details>


### [9] [A complete characterisation of conditional entropies](https://arxiv.org/abs/2601.23213)
*Roberto Rubboli,Erkka Haapasalo,Marco Tomamichel*

Main category: cs.IT

TL;DR: 本文用三条直觉公理完全划分了条件熵，证明它们都是瑞奈熵的指数平均，并揭示了在量子热力学中侧信息的“第二定律”。


<details>
  <summary>Details</summary>
Motivation: 尽管条件熵在信息论与统计学中至关重要，但缺乏对满足自然可操作性公理的所有条件熵量度的完整表征。本文旨在填补该空白，并探讨其与量子热力学第二定律的关联。

Method: 定义一组旨在捕捉“可操作意义”的公理（可加性、重标签不变性、单调性），利用这些公理推导条件熵的一般表达式。通过数学证明，将满足公理的函数证明为瑞奈熵的指数平均形式，并进一步联系到条件混合通道下的转化速率。

Result: 得到满足三条公理的条件熵的最一般形式：指数平均瑞奈熵，参数为实数和正实数上的概率测度。证明此类量度决定条件混合下的转化速率，并推导出带侧信息的量子热力学第二定律集合。

Conclusion: 本文通过三条自然可操作性公理——独立随机变量的可加性、重标签不变性以及在条件混合通道下的单调性，完成了对条件熵的完整表征，证明所有满足这三条公理的条件熵均为瑞奈熵的指数平均，参数化为实数和正实数上的概率测度。此结果不仅阐明了条件熵的本质，也揭示了其在带侧信息的热力学第二定律中的角色。

Abstract: Entropies are fundamental measures of uncertainty with central importance in information theory and statistics and applications across all the quantitative sciences. Under a natural set of operational axioms, the most general form of entropy is captured by the family of Rényi entropies, parameterized by a real number $α$. Conditional entropy extends the notion of entropy by quantifying uncertainty from the viewpoint of an observer with access to potentially correlated side information. However, despite their significance and the emergence of various useful definitions, a complete characterization of measures of conditional entropy that satisfy a natural set of operational axioms has remained elusive. In this work, we provide a complete characterization of conditional entropy, defined through a set of axioms that are essential for any operationally meaningful definition: additivity for independent random variables, invariance under relabeling, and monotonicity under conditional mixing channels. We prove that the most general form of conditional entropy is captured by a family of measures that are exponential averages of Rényi entropies of the conditioned distribution and parameterized by a real parameter and a probability measure on the positive reals. Finally, we show that these quantities determine the rate of transformation under conditional mixing and provide a set of second laws of quantum thermodynamics with side information for states diagonal in the energy eigenbasis.

</details>


### [10] [Secure Integrated Sensing and Communication against Communication and Sensing Eavesdropping](https://arxiv.org/abs/2601.23216)
*Sidong Guo,Matthieu R. Bloch*

Main category: cs.IT

TL;DR: 本文研究单波形ISAC系统的安全问题，证明通过反馈键提取与码隐藏能平衡通信保密和感知隐私，并给出可实现的性能区间。


<details>
  <summary>Details</summary>
Motivation: 在敌对无线环境中，感知隐私和通信保密虽然相互独立却又相互交织，传统方法难以统一处理。

Method: 构建物理层安全框架，分析密钥提取、可解析码与代码结构隐藏的影响；提出可达到的性能区域并给出数值示例。

Result: 得到包含发射机保密速率、检测指数和对手检测指数的可行区域，并通过数值示例展示其设计权衡。

Conclusion: 在单波形ISAC系统中，发送者可以通过反馈获得密钥、使用窃听和可解析码隐藏信息，实现可信隐私和通信之间的权衡。

Abstract: Sensing privacy and communication confidentiality play fundamentally different but interconnected roles in adversarial wireless environments. Capturing this interplay within a single physical-layer framework is particularly challenging in integrated sensing and communication (ISAC) systems, where the same waveform simultaneously serves dual purposes. We study a secure ISAC system in which a monostatic transmitter simultaneously sends a confidential message to a legitimate receiver and senses an environmental state, while a passive adversary attempts both message decoding and state estimation. We partially characterize the fundamental trade-offs among three performance measures: the transmitter's secrecy rate, its detection exponent, and the adversary's detection exponent. Beyond the joint input distribution that governs overall performance, the trade-offs are further shaped by the transmitter's ability to extract keys via feedback and hide both the content and structure of the codewords via wiretap and resolvability codes. We derive an achievable region, and illustrate the resulting design trade-offs through a numerical example.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [11] [ShellForge: Adversarial Co-Evolution of Webshell Generation and Multi-View Detection for Robust Webshell Defense](https://arxiv.org/abs/2601.22182)
*Yizhong Ding*

Main category: cs.CR

TL;DR: ShellForge uses generator‑detector co‑training with semantic, structural, and entropy fusion, plus LLM‑based hard negatives, achieving 0.981 F1 and 93.9% evasion on VirusTotal.


<details>
  <summary>Details</summary>
Motivation: Webshell detection is challenged by rapid evolution, sophisticated obfuscation, and high false‑alarm rates when benign obfuscated scripts are present.

Method: ShellForge employs an adversarial co‑evolution framework: a generator produces highly evasive webshells via supervised fine‑tuning plus reinforcement learning, while a multi‑view detector fuses semantic (long‑string compression), structural (AST pruning), and global (entropy) features. Hard samples are exchanged in a co‑training loop and the detector is hardened by LLM‑generated de‑malicious hard negatives.

Result: On the FWOID benchmark, after convergence the detector reaches 0.981 F1‑score, and the generator attains 0.939 evasion against commercial engines on VirusTotal.

Conclusion: The co‑evolution strategy substantially reduces false positives and enhances detection robustness against evolving webshell threats.

Abstract: Webshells remain a primary foothold for attackers to compromise servers, particularly within PHP ecosystems. However, existing detection mechanisms often struggle to keep pace with rapid variant evolution and sophisticated obfuscation techniques that camouflage malicious intent. Furthermore, many current defenses suffer from high false-alarm rates when encountering benign administrative scripts that employ heavy obfuscation for intellectual property protection. To address these challenges, we present ShellForge, an adversarial co-evolution framework that couples automated webshell generation with multi-view detection to continuously harden defensive boundaries. The framework operates through an iterative co-training loop where a generator and a detector mutually reinforce each other via the exchange of hard samples. The generator is optimized through supervised fine-tuning and preference-based reinforcement learning to synthesize functional, highly evasive variants. Simultaneously, we develop a multi-view fusion detector that integrates semantic features from long-string compression, structural features from pruned abstract syntax trees, and global statistical indicators such as Shannon entropy. To minimize false positives, ShellForge utilizes a LLM-based transformation to create de-malicious samples--scripts that retain complex obfuscation patterns but lack harmful payloads--serving as high-quality hard negatives during training. Evaluations on the public FWOID benchmark demonstrate that ShellForge significantly enhances defensive robustness. Upon convergence, the detector maintains a 0.981 F1-score while the generator achieves a 0.939 evasion rate against commercial engines on VirusTotal.

</details>


### [12] [MemeChain: A Multimodal Cross-Chain Dataset for Meme Coin Forensics and Risk Analysis](https://arxiv.org/abs/2601.22185)
*Alberto Maria Mongardini,Alessandro Mei*

Main category: cs.CR

TL;DR: MemeChain 数据集提供跨链 34,988 代币的链上及链下数据，用于研究市场波动、诈骗检测与多模态异常分析。


<details>
  <summary>Details</summary>
Motivation: Meme 代币生态活跃但缺少可观测性，现有数据集多为单链、缺乏多模态信息，导致风险评估受限，亟需整合链上与链下证据的全链路数据资源。

Method: 通过抓取以太坊、BNB 智能链、Solana 与 Base 上的 34,988 个 meme 代币的链上信息，结合其网站 HTML 源码、代币图标及对应社交媒体账号，构建包含链上与链下多模态特征的数据集。

Result: 视觉品牌常被省略，许多项目缺失功能性网站；数 1,801 个（5.15%）币在上线后 24 小时内全停交易；MemeChain 覆盖四条主链，含 34,988 代币及其链下资产。

Conclusion: MemeChain 为碎片化且缺乏可观测性的新型项目提供了统一、跨链的全面数据来源，支持金融取证、多模态异常检测与反欺诈研究。

Abstract: The meme coin ecosystem has grown into one of the most active yet least observable segments of the cryptocurrency market, characterized by extreme churn, minimal project commitment, and widespread fraudulent behavior. While countless meme coins are deployed across multiple blockchains, they rely heavily on off-chain web and social infrastructure to signal legitimacy. These very signals are largely absent from existing datasets, which are often limited to single-chain data or lack the multimodal artifacts required for comprehensive risk modeling.
  To address this gap, we introduce MemeChain, a large-scale, open-source, cross-chain dataset comprising 34,988 meme coins across Ethereum, BNB Smart Chain, Solana, and Base. MemeChain integrates on-chain data with off-chain artifacts, including website HTML source code, token logos, and linked social media accounts, enabling multimodal and forensic study of meme coin projects. Analysis of the dataset shows that visual branding is frequently omitted in low-effort deployments, and many projects lack a functional website. Moreover, we quantify the ecosystem's extreme volatility, identifying 1,801 tokens (5.15%) that cease all trading activity within just 24 hours of launch. By providing unified cross-chain coverage and rich off-chain context, MemeChain serves as a foundational resource for research in financial forensics, multimodal anomaly detection, and automated scam prevention in the meme coin ecosystem.

</details>


### [13] [A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy](https://arxiv.org/abs/2601.22240)
*Pedro H. Barcha Correia,Ryan W. Achjian,Diego E. G. Caetano de Oliveira,Ygor Acacio Maria,Victor Takashi Hayashi,Marcos Lopes,Charles Christian Miers,Marcos A. Simplicio*

Main category: cs.CR

TL;DR: 本文综述了88篇提示注入防御研究，扩展了NIST分类体系，并提供了评估效果与开源状态的完整清单，帮助研究者和开发者快速落地防御方案。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和LLM的快速发展催生了新的安全漏洞（如提示注入、越狱等），而攻击与防御技术迭代快速，缺乏统一、系统的防御知识框架，阻碍了安全对策的深入研究与实战部署。

Method: 采用系统性文献综述方法，对NIST报告和已有学术综述之外的研究进行收集与筛选；提出额外防御类别以扩充NIST词汇表；对每项防御在不同LLM与攻击数据集上的定量效果进行归档，并标注开源与模型无关属性。

Result: 在文献综述中识别出除NIST外的研究并扩充了防御分类；创建了包含防御类别、效果评估、开源与模型无关标注的综合目录；为后续研究提供了标准化命名及评估基准。

Conclusion: 本文系统梳理了88篇关于大语言模型提示注入防御的研究，扩展并统一了NIST分类体系，并基于评估数据创建了可公开获取且模型无关的防御清单，旨在为研究者和实践者提供一套标准化、可复现的参考资源。

Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (GenAI) and large language models (LLMs) has been accompanied by the emergence of new security vulnerabilities and challenges, such as jailbreaking and other prompt injection attacks. These maliciously crafted inputs can exploit LLMs, causing data leaks, unauthorized actions, or compromised outputs, for instance. As both offensive and defensive prompt injection techniques evolve quickly, a structured understanding of mitigation strategies becomes increasingly important. To address that, this work presents the first systematic literature review on prompt injection mitigation strategies, comprehending 88 studies. Building upon NIST's report on adversarial machine learning, this work contributes to the field through several avenues. First, it identifies studies beyond those documented in NIST's report and other academic reviews and surveys. Second, we propose an extension to NIST taxonomy by introducing additional categories of defenses. Third, by adopting NIST's established terminology and taxonomy as a foundation, we promote consistency and enable future researchers to build upon the standardized taxonomy proposed in this work. Finally, we provide a comprehensive catalog of the reviewed prompt injection defenses, documenting their reported quantitative effectiveness across specific LLMs and attack datasets, while also indicating which solutions are open-source and model-agnostic. This catalog, together with the guidelines presented herein, aims to serve as a practical resource for researchers advancing the field of adversarial machine learning and for developers seeking to implement effective defenses in production systems.

</details>


### [14] [MirrorMark: A Distortion-Free Multi-Bit Watermark for Large Language Models](https://arxiv.org/abs/2601.22246)
*Ya Jiang,Massieh Kordi Boroujeny,Surender Suresh Kumar,Kai Zeng*

Main category: cs.CR

TL;DR: MirrorMark利用测度保持的镜像采样实现多比特无失真水印，实验表明可检测性提升显著，文本质量保持不变。


<details>
  <summary>Details</summary>
Motivation: 需要在大语言模型生成中实现可靠内容归属，同时避免传统方法带来的文本质量下降或检测难度低的问题。

Method: 通过在采样随机性中镜像映射维持测度不变性，随后采用基于上下文的调度器平衡不同位位置的标记，从而在保持分布完整的同时实现多比特嵌入。

Result: 在54比特嵌入300个token实验中，MirrorMark提升了8–12%的比特准确率，并在1%误报率下可识别额外约11%的水印文本，且与非水印文本的质量几乎无差。

Conclusion: MirrorMark在保持文本质量不变的前提下实现了多比特、无失真水印，具备明显提升的可检测性和鲁棒性。

Abstract: As large language models (LLMs) become integral to applications such as question answering and content creation, reliable content attribution has become increasingly important. Watermarking is a promising approach, but existing methods either provide only binary signals or distort the sampling distribution, degrading text quality; distortion-free approaches, in turn, often suffer from weak detectability or robustness. We propose MirrorMark, a multi-bit and distortion-free watermark for LLMs. By mirroring sampling randomness in a measure-preserving manner, MirrorMark embeds multi-bit messages without altering the token probability distribution, preserving text quality by design. To improve robustness, we introduce a context-based scheduler that balances token assignments across message positions while remaining resilient to insertions and deletions. We further provide a theoretical analysis of the equal error rate to interpret empirical performance. Experiments show that MirrorMark matches the text quality of non-watermarked generation while achieving substantially stronger detectability: with 54 bits embedded in 300 tokens, it improves bit accuracy by 8-12% and correctly identifies up to 11% more watermarked texts at 1% false positive rate.

</details>


### [15] [Rethinking Anonymity Claims in Synthetic Data Generation: A Model-Centric Privacy Attack Perspective](https://arxiv.org/abs/2601.22434)
*Georgi Ganev,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 本文重构合成数据隐私评估视角，强调模型中心论断；指出差分隐私优于相似度度量；提供与GDPR对应的可识别风险映射与评估框架


<details>
  <summary>Details</summary>
Motivation: 评估合成数据与生成模型在隐私保护中的真实性效能，尤其在GDPR框架下对可识别风险的认知与防护

Method: 从模型中心出发，将GDPR关于个人数据与匿名化的定义与实际模型可交互情境结合，映射至各种攻击范式；随后对差分隐私(DP)与基于相似度的隐私度量(SBPMs)进行对比分析

Result: 合成数据技术单独无法充分消除可识别风险；DP在多种攻击场景下提供可靠保护，而SBPMs缺乏足够防护；本研究构建了基于法规与攻击的评估框架，促进科研、实践与政策制定的协同创新

Conclusion: 合成数据的安全性评估必须以模型能力为核心，结合现行法规与最新攻击方法；仅依赖合成技巧不可行，应采用差分隐私等稳健机制以满足监管与实际需求

Abstract: Training generative machine learning models to produce synthetic tabular data has become a popular approach for enhancing privacy in data sharing. As this typically involves processing sensitive personal information, releasing either the trained model or generated synthetic datasets can still pose privacy risks. Yet, recent research, commercial deployments, and privacy regulations like the General Data Protection Regulation (GDPR) largely assess anonymity at the level of an individual dataset.
  In this paper, we rethink anonymity claims about synthetic data from a model-centric perspective and argue that meaningful assessments must account for the capabilities and properties of the underlying generative model and be grounded in state-of-the-art privacy attacks. This perspective better reflects real-world products and deployments, where trained models are often readily accessible for interaction or querying. We interpret the GDPR's definitions of personal data and anonymization under such access assumptions to identify the types of identifiability risks that must be mitigated and map them to privacy attacks across different threat settings. We then argue that synthetic data techniques alone do not ensure sufficient anonymization. Finally, we compare the two mechanisms most commonly used alongside synthetic data -- Differential Privacy (DP) and Similarity-based Privacy Metrics (SBPMs) -- and argue that while DP can offer robust protections against identifiability risks, SBPMs lack adequate safeguards. Overall, our work connects regulatory notions of identifiability with model-centric privacy attacks, enabling more responsible and trustworthy regulatory assessment of synthetic data systems by researchers, practitioners, and policymakers.

</details>


### [16] [FraudShield: Knowledge Graph Empowered Defense for LLMs against Fraud Attacks](https://arxiv.org/abs/2601.22485)
*Naen Xu,Jinghuai Zhang,Ping He,Chunyi Zhou,Jun Wang,Zhihui Fu,Tianyu Du,Zhaoxiang Wang,Shouling Ji*

Main category: cs.CR

TL;DR: FraudShield builds a knowledge graph linking fraud tactics to keywords, enhances LLM inputs, and surpasses existing defenses in accuracy and explainability across multiple models and fraud scenarios.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to manipulation by fraudulent information, and current defenses lack effectiveness, interpretability, and generalizability for LLM-based applications.

Method: Constructing and refining a fraud tactic—keyword knowledge graph that augments LLM inputs with highlighted keywords and evidence.

Result: Consistent state‑of‑the‑art performance across four mainstream LLMs and five fraud types, with interpretable clues for generated outputs.

Conclusion: FraudShield effectively protects LLMs from fraudulent content, outperforming existing defenses and offering interpretability.

Abstract: Large language models (LLMs) have been widely integrated into critical automated workflows, including contract review and job application processes. However, LLMs are susceptible to manipulation by fraudulent information, which can lead to harmful outcomes. Although advanced defense methods have been developed to address this issue, they often exhibit limitations in effectiveness, interpretability, and generalizability, particularly when applied to LLM-based applications. To address these challenges, we introduce FraudShield, a novel framework designed to protect LLMs from fraudulent content by leveraging a comprehensive analysis of fraud tactics. Specifically, FraudShield constructs and refines a fraud tactic-keyword knowledge graph to capture high-confidence associations between suspicious text and fraud techniques. The structured knowledge graph augments the original input by highlighting keywords and providing supporting evidence, guiding the LLM toward more secure responses. Extensive experiments show that FraudShield consistently outperforms state-of-the-art defenses across four mainstream LLMs and five representative fraud types, while also offering interpretable clues for the model's generations.

</details>


### [17] [VocBulwark: Towards Practical Generative Speech Watermarking via Additional-Parameter Injection](https://arxiv.org/abs/2601.22556)
*Weizhi Liu,Yue Li,Zhaoxia Yin*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generated speech achieves human-level naturalness but escalates security risks of misuse. However, existing watermarking methods fail to reconcile fidelity with robustness, as they rely either on simple superposition in the noise space or on intrusive alterations to model weights. To bridge this gap, we propose VocBulwark, an additional-parameter injection framework that freezes generative model parameters to preserve perceptual quality. Specifically, we design a Temporal Adapter to deeply entangle watermarks with acoustic attributes, synergizing with a Coarse-to-Fine Gated Extractor to resist advanced attacks. Furthermore, we develop an Accuracy-Guided Optimization Curriculum that dynamically orchestrates gradient flow to resolve the optimization conflict between fidelity and robustness. Comprehensive experiments demonstrate that VocBulwark achieves high-capacity and high-fidelity watermarking, offering robust defense against complex practical scenarios, with resilience to Codec regenerations and variable-length manipulations.

</details>


### [18] [The Semantic Trap: Do Fine-tuned LLMs Learn Vulnerability Root Cause or Just Functional Pattern?](https://arxiv.org/abs/2601.22655)
*Feiyang Huang,Yuqiang Sun,Fan Zhang,Ziqi Yang,Han Liu,Yang Liu*

Main category: cs.CR

TL;DR: LLM在漏洞检测中常陷入功能模式“语义陷阱”，评估工具TrapEval揭示其缺乏真正安全推理能力。


<details>
  <summary>Details</summary>
Motivation: 探究LLM微调提升的检测分数是否源于对功能模式的记忆，而非对漏洞根因的真正推理。

Method: 提出TrapEval评估框架，构造V2N（漏洞代码与无关安全代码对）和V2P（漏洞代码与补丁代码对）两类数据集，采用跨数据集测试、语义保留扰动与CodeBLEU度量语义差距。

Result: 五大类LLM在TrapEval上无法有效区分补丁与原始代码，对细微语义改变鲁棒性差，且在语义差距小的情况下大量依赖功能上下文短路。

Conclusion: LLM微调后在漏洞检测上表现良好，但实际上难以真正理解安全根因，容易陷入“语义陷阱”；现有评测数据集合与细微差别能揭示此缺陷。

Abstract: LLMs demonstrate promising performance in software vulnerability detection after fine-tuning. However, it remains unclear whether these gains reflect a genuine understanding of vulnerability root causes or merely an exploitation of functional patterns. In this paper, we identify a critical failure mode termed the "semantic trap," where fine-tuned LLMs achieve high detection scores by associating certain functional domains with vulnerability likelihood rather than reasoning about the underlying security semantics.To systematically evaluate this phenomenon, we propose TrapEval, a comprehensive evaluation framework designed to disentangle vulnerability root cause from functional pattern. TrapEval introduces two complementary datasets derived from real-world open-source projects: V2N, which pairs vulnerable code with unrelated benign code, and V2P, which pairs vulnerable code with its corresponding patched version, forcing models to distinguish near-identical code that differs only in subtle security-critical logic. Using TrapEval, we fine-tune five representative state-of-the-art LLMs across three model families and evaluate them under cross-dataset testing, semantic-preserving perturbations, and varying degrees of semantic gap measured by CodeBLEU.Our empirical results reveal that, despite improvements in metrics, fine-tuned LLMs consistently struggle to distinguish vulnerable code from its patched counterpart, exhibit severe robustness degradation under minor semantic-preserving transformations, and rely heavily on functional-context shortcuts when the semantic gap is small. These findings provide strong evidence that current fine-tuning practices often fail to impart true vulnerability reasoning. Our findings serve as a wake-up call: high benchmark scores on traditional datasets may be illusory, masking the model's inability to understand the true causal logic of vulnerabilities.

</details>


### [19] [RealSec-bench: A Benchmark for Evaluating Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2601.22706)
*Yanlin Wang,Ziyao Zhang,Chong Wang,Xinyi Xu,Mingwei Liu,Yong Wang,Jiachi Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: RealSec-bench 通过真实 Java 漏洞构建基准，发现现有 LLM 在功能上可被优化，但安全性提升有限，提示策略反而可能降低功能正确性。


<details>
  <summary>Details</summary>
Motivation: 缺乏能够同时衡量功能与安全性的真实世界评测基准，导致现有研究难以准确反映 LLM 在安全编码中的表现。

Method: 构建 RealSec-bench 基准：从高风险 Java 仓库抽取 105 个真实漏洞实例，结合 CodeQL 静态分析、LLM 误报过滤及人工专家验证完成实例处理；使用 SecurePass@K 指标同时评估功能正确性与安全性，并对 5 大 LLM 进行实证实验。

Result: RealSec-bench 覆盖 19 类 CWE，数据流复杂度最高可达 34 跳；5 个 LLM 在功能正确性方面受 RAG 技术提升，安全性提升极小；强制安全提示会导致编译错误并未有效抑制漏洞。

Conclusion: 当前主流大型语言模型在功能正确性方面表现不错，但在安全代码生成上仍显不足；虽有 Retrieval-Augmented Generation 等技术提升功能表现，安全性却几乎无改进，说明功能与安全性之间存在显著差距。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their proficiency in producing secure code remains a critical, under-explored area. Existing benchmarks often fall short by relying on synthetic vulnerabilities or evaluating functional correctness in isolation, failing to capture the complex interplay between functionality and security found in real-world software. To address this gap, we introduce RealSec-bench, a new benchmark for secure code generation meticulously constructed from real-world, high-risk Java repositories. Our methodology employs a multi-stage pipeline that combines systematic SAST scanning with CodeQL, LLM-based false positive elimination, and rigorous human expert validation. The resulting benchmark contains 105 instances grounded in real-word repository contexts, spanning 19 Common Weakness Enumeration (CWE) types and exhibiting a wide diversity of data flow complexities, including vulnerabilities with up to 34-hop inter-procedural dependencies. Using RealSec-bench, we conduct an extensive empirical study on 5 popular LLMs. We introduce a novel composite metric, SecurePass@K, to assess both functional correctness and security simultaneously. We find that while Retrieval-Augmented Generation (RAG) techniques can improve functional correctness, they provide negligible benefits to security. Furthermore, explicitly prompting models with general security guidelines often leads to compilation failures, harming functional correctness without reliably preventing vulnerabilities. Our work highlights the gap between functional and secure code generation in current LLMs.

</details>


### [20] [AlienLM: Alienization of Language for API-Boundary Privacy in Black-Box LLMs](https://arxiv.org/abs/2601.22710)
*Jaehee Kim,Pilsung Kang*

Main category: cs.CR

TL;DR: AlienLM通过词表双射把输入转成外星语言，客户端能无损恢复，训练后模型能直接处理这种输入。实验显示在多种LLM和任务上损失低于19%，而攻击即使掌握模型权重也几乎无恢复。


<details>
  <summary>Details</summary>
Motivation: 在现代LLM通过黑盒API使用时，用户需要提交敏感提示与输出，导致在API界面出现严重隐私泄露风险，亟需可部署的、仅API层级的隐私保护技术。

Method: 构建词表规模的双射将文本转译为外星语言；利用仅API可调用的Fine‑Tuning接口进行Alien Adaptation Training，让目标模型直接处理外星化输入；在四种LLM基准上评估。

Result: 在七个基准上平均保持81%以上的完整表现；与随机双射和字符级基线相比显著优越；即便攻击者拥有模型权重、语料统计和学习逆译，也仅能恢复不到0.22%的外星化令牌。

Conclusion: AlienLM提供了一种在仅使用API访问时的隐私保护方案，通过将文本映射为“外星语言”实现无损翻译并在客户端恢复，同时保持高性能并抵抗逆向攻击。

Abstract: Modern LLMs are increasingly accessed via black-box APIs, requiring users to transmit sensitive prompts, outputs, and fine-tuning data to external providers, creating a critical privacy risk at the API boundary. We introduce AlienLM, a deployable API-only privacy layer that protects text by translating it into an Alien Language via a vocabulary-scale bijection, enabling lossless recovery on the client side. Using only standard fine-tuning APIs, Alien Adaptation Training (AAT) adapts target models to operate directly on alienized inputs. Across four LLM backbones and seven benchmarks, AlienLM retains over 81\% of plaintext-oracle performance on average, substantially outperforming random-bijection and character-level baselines. Under adversaries with access to model weights, corpus statistics, and learning-based inverse translation, recovery attacks reconstruct fewer than 0.22\% of alienized tokens. Our results demonstrate a practical pathway for privacy-preserving LLM deployment under API-only access, substantially reducing plaintext exposure while maintaining task performance.

</details>


### [21] [Rust and Go directed fuzzing with LibAFL-DiFuzz](https://arxiv.org/abs/2601.22772)
*Timofey Mezhuev,Darya Parygina,Daniil Kuts*

Main category: cs.CR

TL;DR: 基于 LibAFL-DiFuzz 的 Rust/Go 定向模糊工具通过自定义编译器预处理和图监测技术，在实验中获得了更快、更多的漏洞发现速度。


<details>
  <summary>Details</summary>
Motivation: 传统的覆盖率引导模糊难以在特定位置验证静态分析报告或重现崩溃，且现代语言（Rust、Go）缺乏高效的定向模糊支持。

Method: 在 rustc 编译器中实施自定义预处理、扩展图构造和监测技术，并基于 LibAFL-DiFuzz 后端开发了 Rust‑LibAFL‑DiFuzz 与 Go‑LibAFL‑DiFuzz 两款工具，采用高级接近度度量实现定向模糊。

Result: TTE 实验中，Rust‑LibAFL‑DiFuzz 及 Go‑LibAFL‑DiFuzz 的平均和最佳 TTE 均优于 afl.rs、cargo‑fuzz、go‑fuzz 等主流工具，且在多数实验中表现出数十倍甚至数百倍的优势。

Conclusion: 本文提出了针对 Rust 与 Go 语言的定向灰盒模糊测试方法，并通过实验验证了其比现有模糊器更高的发现效率和准确性。

Abstract: In modern SSDLC, program analysis and automated testing are essential for minimizing vulnerabilities before software release, with fuzzing being a fast and widely used dynamic testing method. However, traditional coverage-guided fuzzing may be less effective in specific tasks like verifying static analysis reports or reproducing crashes, while directed fuzzing, focusing on targeted program locations using proximity metrics, proves to be more effective. Some of the earliest directed fuzzers are, for example, AFLGo and BEACON, which use different proximity metric approaches. Although most automated testing tools focus on C/C++ code, the growing popularity of Rust and Go causes the need for precise and efficient testing solutions for these languages. This work expands the applicability of directed fuzzing beyond traditional analysis of C/C++ software. We present a novel approach to directed greybox fuzzing tailored specifically for Rust and Go applications. We introduce advanced preprocessing techniques, rustc compiler customizations, and elaborate graph construction and instrumentation methods to enable effective targeting of specific program locations. Our implemented fuzzing tools, based on LibAFL-DiFuzz backend, demonstrate competitive advantages compared to popular existing fuzzers like afl.rs, cargo-fuzz, and go-fuzz. According to TTE (Time to Exposure) experiments, Rust-LibAFL-DiFuzz outperforms other tools by the best TTE result. Some stability issues can be explained by different mutation approaches. Go-LibAFL-DiFuzz outperforms its opponent by the best and, in the majority of cases, by average result, having two cases with orders of magnitude difference. These results prove better efficiency and accuracy of our approach.

</details>


### [22] [A Real-Time Privacy-Preserving Behavior Recognition System via Edge-Cloud Collaboration](https://arxiv.org/abs/2601.22938)
*Huan Song,Shuyu Tian,Junyi Hao,Cheng Yuan,Zhenyu Jia,Jiawei Shao,Xuelong Li*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As intelligent sensing expands into high-privacy environments such as restrooms and changing rooms, the field faces a critical privacy-security paradox. Traditional RGB surveillance raises significant concerns regarding visual recording and storage, while existing privacy-preserving methods-ranging from physical desensitization to traditional cryptographic or obfuscation techniques-often compromise semantic understanding capabilities or fail to guarantee mathematical irreversibility against reconstruction attacks. To address these challenges, this study presents a novel privacy-preserving perception technology based on the AI Flow theoretical framework and an edge-cloud collaborative architecture. The proposed methodology integrates source desensitization with irreversible feature mapping. Leveraging Information Bottleneck theory, the edge device performs millisecond-level processing to transform raw imagery into abstract feature vectors via non-linear mapping and stochastic noise injection. This process constructs a unidirectional information flow that strips identity-sensitive attributes, rendering the reconstruction of original images impossible. Subsequently, the cloud platform utilizes multimodal family models to perform joint inference solely on these abstract vectors to detect abnormal behaviors. This approach fundamentally severs the path to privacy leakage at the architectural level, achieving a breakthrough from video surveillance to de-identified behavior perception and offering a robust solution for risk management in high-sensitivity public spaces.

</details>


### [23] [Trojan-Resilient NTT: Protecting Against Control Flow and Timing Faults on Reconfigurable Platforms](https://arxiv.org/abs/2601.22804)
*Rourab Paul,Krishnendu Guha,Amlan Chakrabarti*

Main category: cs.CR

TL;DR: 在FPGA上实现的安全NTT能有效抵御硬件木马与SASCA，提供高成功率的错误检测与纠正，且开销低。


<details>
  <summary>Details</summary>
Motivation: 在基于格的后量子密码算法中，NTT是核心模块；硬件木马可低成本破坏控制信号，导致计算完整性失效，进而被用于侧信道攻击。

Method: 利用时间延迟检测、控制流完整性检查以及针对SASCA的探测模块结合自适应容错纠错单元，在Artix‑7 FPGA上实现了该安全NTT。

Result: 仿真与实现表明，该安全NTT能够高概率探测并纠正由硬件木马或意外故障产生的错误，且面积与时序开销仅为常规实现的轻微提升。

Conclusion: 本工作提出了一种可检测并纠正硬件木马及软解析侧信道攻击（SASCA）引入的异常延迟和控制流中断的安全整数变换（NTT）体系结构。

Abstract: Number Theoretic Transform (NTT) is the most essential component for polynomial multiplications used in lattice-based Post-Quantum Cryptography (PQC) algorithms such as Kyber, Dilithium, NTRU etc. However, side-channel attacks (SCA) and hardware vulnerabilities in the form of hardware Trojans may alter control signals to disrupt the circuit's control flow and introduce unconventional delays in the critical hardware of PQC. Hardware Trojans, especially on control signals, are more low cost and impactful than data signals because a single corrupted control signal can disrupt or bypass entire computation sequences, whereas data faults usually cause only localized errors. On the other hand, adversaries can perform Soft Analytical Side Channel Attacks (SASCA) on the design using the inserted hardware Trojan. In this paper, we present a secure NTT architecture capable of detecting unconventional delays, control-flow disruptions, and SASCA, while providing an adaptive fault-correction methodology for their mitigation. Extensive simulations and implementations of our Secure NTT on Artix-7 FPGA with different Kyber variants show that our fault detection and correction modules can efficiently detect and correct faults whether caused unintentionally or intentionally by hardware Trojans with a high success rate, while introducing only modest area and time overheads.

</details>


### [24] [Hide and Seek in Embedding Space: Geometry-based Steganography and Detection in Large Language Models](https://arxiv.org/abs/2601.22818)
*Charles Westphal,Keivan Navaie,Fernando E. Rosas*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fine-tuned LLMs can covertly encode prompt secrets into outputs via steganographic channels. Prior work demonstrated this threat but relied on trivially recoverable encodings. We formalize payload recoverability via classifier accuracy and show previous schemes achieve 100\% recoverability. In response, we introduce low-recoverability steganography, replacing arbitrary mappings with embedding-space-derived ones. For Llama-8B (LoRA) and Ministral-8B (LoRA) trained on TrojanStego prompts, exact secret recovery rises from 17$\rightarrow$30\% (+78\%) and 24$\rightarrow$43\% (+80\%) respectively, while on Llama-70B (LoRA) trained on Wiki prompts, it climbs from 9$\rightarrow$19\% (+123\%), all while reducing payload recoverability. We then discuss detection. We argue that detecting fine-tuning-based steganographic attacks requires approaches beyond traditional steganalysis. Standard approaches measure distributional shift, which is an expected side-effect of fine-tuning. Instead, we propose a mechanistic interpretability approach: linear probes trained on later-layer activations detect the secret with up to 33\% higher accuracy in fine-tuned models compared to base models, even for low-recoverability schemes. This suggests that malicious fine-tuning leaves actionable internal signatures amenable to interpretability-based defenses.

</details>


### [25] [Evaluating Large Language Models for Security Bug Report Prediction](https://arxiv.org/abs/2601.22921)
*Farnaz Soltaniani,Shoaib Razzaq,Mohammad Ghafari*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Early detection of security bug reports (SBRs) is critical for timely vulnerability mitigation. We present an evaluation of prompt-based engineering and fine-tuning approaches for predicting SBRs using Large Language Models (LLMs). Our findings reveal a distinct trade-off between the two approaches. Prompted proprietary models demonstrate the highest sensitivity to SBRs, achieving a G-measure of 77% and a recall of 74% on average across all the datasets, albeit at the cost of a higher false-positive rate, resulting in an average precision of only 22%. Fine-tuned models, by contrast, exhibit the opposite behavior, attaining a lower overall G-measure of 51% but substantially higher precision of 75% at the cost of reduced recall of 36%. Though a one-time investment in building fine-tuned models is necessary, the inference on the largest dataset is up to 50 times faster than that of proprietary models. These findings suggest that further investigations to harness the power of LLMs for SBR prediction are necessary.

</details>


### [26] [Protecting Private Code in IDE Autocomplete using Differential Privacy](https://arxiv.org/abs/2601.22935)
*Evgeny Grigorenko,David Stanojević,David Ilić,Egor Bogomolov,Kostadin Cvejoski*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern Integrated Development Environments (IDEs) increasingly leverage Large Language Models (LLMs) to provide advanced features like code autocomplete. While powerful, training these models on user-written code introduces significant privacy risks, making the models themselves a new type of data vulnerability. Malicious actors can exploit this by launching attacks to reconstruct sensitive training data or infer whether a specific code snippet was used for training. This paper investigates the use of Differential Privacy (DP) as a robust defense mechanism for training an LLM for Kotlin code completion. We fine-tune a \texttt{Mellum} model using DP and conduct a comprehensive evaluation of its privacy and utility. Our results demonstrate that DP provides a strong defense against Membership Inference Attacks (MIAs), reducing the attack's success rate close to a random guess (AUC from 0.901 to 0.606). Furthermore, we show that this privacy guarantee comes at a minimal cost to model performance, with the DP-trained model achieving utility scores comparable to its non-private counterpart, even when trained on 100x less data. Our findings suggest that DP is a practical and effective solution for building private and trustworthy AI-powered IDE features.

</details>


### [27] [From Data Leak to Secret Misses: The Impact of Data Leakage on Secret Detection Models](https://arxiv.org/abs/2601.22946)
*Farnaz Soltaniani,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 重复样本造成的泄漏会夸大AI秘密检测器的性能，误导实践有效性评估。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在软件安全领域的应用普遍，但训练和评估的数据来源多为互联网，常见重复或相似样本。

Method: 对流行的硬编码秘密基准数据集进行重复样本检测，并评估数据泄漏对AI秘密检测器性能的影响。

Result: 发现数据泄漏显著提升了模型的表面表现，导致对实际效果的误导。

Conclusion: 重复样本导致的泄漏显著扭曲了AI秘密检测器的评估结果。

Abstract: Machine learning models are increasingly used for software security tasks. These models are commonly trained and evaluated on large Internet-derived datasets, which often contain duplicated or highly similar samples. When such samples are split across training and test sets, data leakage may occur, allowing models to memorize patterns instead of learning to generalize. We investigate duplication in a widely used benchmark dataset of hard coded secrets and show how data leakage can substantially inflate the reported performance of AI-based secret detectors, resulting in a misleading picture of their real-world effectiveness.

</details>


### [28] [WiFiPenTester: Advancing Wireless Ethical Hacking with Governed GenAI](https://arxiv.org/abs/2601.23092)
*Haitham S. Al-Sinani,Chris J. Mitchell*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Wireless ethical hacking relies heavily on skilled practitioners manually interpreting reconnaissance results and executing complex, time-sensitive sequences of commands to identify vulnerable targets, capture authentication handshakes, and assess password resilience; a process that is inherently labour-intensive, difficult to scale, and prone to subjective judgement and human error. To help address these limitations, we propose WiFiPenTester, an experimental, governed, and reproducible system for GenAI-enabled wireless ethical hacking. The system integrates large language models into the reconnaissance and decision-support phases of wireless security assessment, enabling intelligent target ranking, attack feasibility estimation, and strategy recommendation, while preserving strict human-in-the-loop control and budget-aware execution. We describe the system architecture, threat model, governance mechanisms, and prompt-engineering methodology, and empirical experiments conducted across multiple wireless environments. The results demonstrate that GenAI assistance improves target selection accuracy and overall assessment efficiency, while maintaining auditability and ethical safeguards. This indicates that WiFiPenTester is a meaningful step toward practical, safe, and scalable GenAI-assisted wireless penetration testing, while reinforcing the necessity of bounded autonomy, human oversight, and rigorous governance mechanisms when deploying GenAI in ethical hacking.

</details>


### [29] [Secure Tool Manifest and Digital Signing Solution for Verifiable MCP and LLM Pipelines](https://arxiv.org/abs/2601.23132)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Foutse Khomh,Amin Nikanjam,Mohammad Adnan Hamdaqa*

Main category: cs.CR

TL;DR: 提出加签工具清单+透明日志的LLM执行安全框架，验证性能优异、可线性扩展、双重合法性校验。


<details>
  <summary>Details</summary>
Motivation: 传统的合规机制缺乏可验证执法和透明验证，导致LLM在医疗、金融等敏感场景中的执行易受操控，亟需安全可靠的对策。

Method: 在Model Context Protocol基础上引入加密签名的工具清单、透明验证日志，并将模型内部执行元数据与用户可见组件隔离。

Result: 实验表明框架在可扩展性上近线性（R²=0.998），对合法执行的几乎100%接受率，对非法执行保持一致拒绝率，并在执行管道中保持模型使用平衡。

Conclusion: 本文提出一种安全工具清单与数字签名框架，能够在敏感领域内保证LLM执行流程的可验证性与完整性。

Abstract: Large Language Models (LLMs) are increasingly adopted in sensitive domains such as healthcare and financial institutions' data analytics; however, their execution pipelines remain vulnerable to manipulation and unverifiable behavior. Existing control mechanisms, such as the Model Context Protocol (MCP), define compliance policies for tool invocation but lack verifiable enforcement and transparent validation of model actions. To address this gap, we propose a novel Secure Tool Manifest and Digital Signing Framework, a structured and security-aware extension of Model Context Protocols. The framework enforces cryptographically signed manifests, integrates transparent verification logs, and isolates model-internal execution metadata from user-visible components to ensure verifiable execution integrity. Furthermore, the evaluation demonstrates that the framework scales nearly linearly (R-squared = 0.998), achieves near-perfect acceptance of valid executions while consistently rejecting invalid ones, and maintains balanced model utilization across execution pipelines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [31] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 提出首个 EEG-to-Text foundation model CELM，利用 9922 条报告与 11000 小时 EEG 大数据集训练，显著提升多尺度报道质量，零-shot 也表现优于传统方法，代码与数据公开。


<details>
  <summary>Details</summary>
Motivation: 当前从长期 EEG 生成临床报告既耗时又人工，缺乏大型数据和可扩展的自动化方法；

Method: 使用大规模 9922 条 EEG 报告及约 11000 小时记录构建数据集，训练一个多模态 foundation model（CELM），可从长时段 EEG 中提取录音描述、背景活动、癫痫性异常、事件/癫痫发作与印象等层面生成文本；

Result: 在患者历史监督下，CELM 在 ROUGE‑1/METEOR 上平均提升 70–95%；在无历史的零-shot 情境下，得分 0.43–0.52，远超基线 0.17–0.26；

Conclusion: CELM 通过整合预训练的脑电基础模型和语言模型，实现了对长时序 EEG 信号的高质量、可扩展的临床报告生成，显著提升了多尺度摘要性能；

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [32] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: FedAdaVR 通过自适应优化和方差降低处理间歇性客户端参与，实现理论收敛保证和实验优于基线；量化版本 FedAdaVR-Quant 降低内存至 50%–87.5% 同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 处理在联邦学习中由间歇性客户端参与导致的异质性问题，降低梯度噪声、客户端漂移和参与错误等负面影响。

Method: 提出 FedAdaVR，该算法使用自适应优化器和方差降低技术，利用最近已存储的客户端更新（即使该客户端当前缺席）来模拟其存在。进一步设计 FedAdaVR-Quant，采用量化存储更新，以显著降低内存需求。对 FedAdaVR 在一般非凸条件下的收敛性进行理论分析，证明能消除部分客户端参与错误。

Result: 实验表明，在 IID 和非 IID 数据集上，FedAdaVR 与 FedAdaVR-Quant 均优于现有最先进基础方法；FedAdaVR-Quant 能将内存需求降低 50%–87.5%，且模型性能保持一致。

Conclusion: FedAdaVR 通过自适应优化与方差降低有效缓解间歇性客户端参与导致的异质性问题，并在理论与实验上证明其优越性。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [33] [Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions](https://arxiv.org/abs/2601.22211)
*Lingkai Kong,Anagha Satish,Hezi Jiang,Akseli Kangaslahti,Andrew Ma,Wenbo Chen,Mingxiao Song,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\% across a range of challenging combinatorial RL tasks.

</details>


### [34] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [35] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 利用可穿戴传感器与支持张量机的低成本框架，可准确识别老年人居家活动，优势超过传统机器学习模型，具有远程医疗和智能家居健身应用潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗设施有限导致老年人等弱势群体需依赖居家护理，影响康复运动的执行和监测。通过低成本智能框架填补居家康复监测的空白。

Method: 使用加速度计/陀螺仪采集行走、上下楼梯、坐、站、仰卧等动作数据，并将时空信息构成张量，利用支持张量机进行训练与分类，完成低成本、自动化的动作识别。

Result: SVM、LR、RF、k-NN的准确率分别为93.33%、91.11%；STM在测试集上达96.67%，交叉验证准确率最高达98.50%。

Conclusion: 本文提出的支持张量机 (STM) 在低成本可穿戴传感器上实现的自动人体动作识别框架，在低资源、农村医疗环境下表现出显著优于传统机器学习模型的分类性能，可广泛应用于远程健康监测与老年人护理。

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [36] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: FunPRM 结合函数式提示与奖励校正，提升 LLM 的代码生成表现，实现最先进的准确率与代码质量。


<details>
  <summary>Details</summary>
Motivation: 现有 PRM 在代码生成中因缺少有效的步骤分解和部分奖励噪声而效果受限，而复杂编程任务对 LLM 的准确性要求更高，需要一种更可靠的测试时缩放方法。

Method: 首先设计提示让 LLM 组织代码为函数，视函数为 PRM 推理步骤；随后引入基于元学习的奖励校正机制，利用单元测试得到的最终解决方案奖励，对 Monte Carlo 推算的部分解决方案奖励进行去噪。

Result: 在 LiveCodeBench 与 BigCodeBench 上，FunPRM 在五种主流 LLM 上均优于现有测试时缩放方法，在与 O4-mini 结合时达到 LiveCodeBench 的 1.00‑6.09% 最高准确率，并生成更易读、可复用的代码。

Conclusion: FunPRM 通过让 LLM 生成模块化函数代码并采用奖励校正机制，显著提升了代码生成的准确性和可读性，取得了 LiveCodeBench 和 BigCodeBench 的最佳结果，证明了其在复杂编程任务中的有效性。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [37] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 消除注意力机制中的无效旋转自由度：使用固定偏置改善优化器表现并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制携带了冗余的旋转自由度，这些自由度在计算中被传递，却不影响激活或输出。通过消除这些冗余，可改善优化效率并提供新的可解释性维度。

Method: 在标准 Transformer 中加入固定的、随机批量采样的、不可学习的查询和值偏置，实现对旋转空间的对称破坏；随后在 124M 参数模型上使用四种优化器（AdamW, SOAP, SGDM, ECD）进行预训练，并评估验证损失与下游逻辑推理性能。

Result: 实验表明，偏置方法能显著缩小甚至消除简单优化器与复杂自适应方法之间的性能差距，同时在注意力头中可选择性放大语义含义明确的 token 类别，从而提升解释性。

Conclusion: 通过在注意力机制中引入固定的查询和值偏置，消除了多余的旋转自由度，既显著提升了简单优化器的性能，也解锁了旋转空间的可解释性，让模型能在不同注意力头中突出语义相关的 token 类别。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [38] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [39] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: SPECIAL为联邦域增量学习提供了简单、无记忆且理论guaranteed的解决方案，兼顾向后知识迁移和收敛性能。


<details>
  <summary>Details</summary>
Motivation: 现实联邦系统需在异构客户端、连续出现的域漂移和固定标签空间下学习，缺乏向后知识迁移保证与跨任务收敛率。

Method: 在FedAvg基础上加入轻量级的服务器端proximal锚点，倾向已学习模型以抑制漂移。

Result: 证明了BKT界限与收敛率O((E/NT)^1/2)，实验验证了方法有效。

Conclusion: SPECIAL通过服务器端锚点实现了无记忆的FDIL版本，保证了向后知识迁移且具备理论收敛性。

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [40] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: 出现多样性提升、图像质量强化和临床数据源定位的高效归因
- 论文提出 SurrogateSHAP，其核心：无重训、树模型近似 Shapley 值；
- 在三种任务上评估：CIFAR‑20 图像质量、Stable Diffusion 审美、FLUX.1 产品多样性； 
- 成果：相较旧方法显著降低计算成本，且在临床图像上能定位数据源异常。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像扩散模型日益被实际创作工作流程使用的背景下，需要一个基于原则的框架来对提供数据集的贡献者进行价值评估，以实现公平补偿和可持续的数据市场。

Method: 提出 SurrogateSHAP：一种无重训的框架，通过预训练模型的推断近似昂贵的重训游戏；进一步使用梯度提升树逼近效用函数，并从树模型解析得到 Shapley 值。

Result: 在图像质量、审美和产品多样性三种不同的归因任务上，SurrogateSHAP 的表现优于之前的方法，同时显著降低计算开销；能够定位临床图像中导致异常相关的数据源，为安全关键生成模型的审计提供可扩展路径。

Conclusion: SurrogateSHAP 通过无重训和树模型近似，为文本到图像模型的数据贡献者归因提供了高效、理论上合理的解决方案，促进公平补偿和数据市场的可持续发展。

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [41] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: RLOs将优化映射为黎曼几何上的受控动力系统，利用NAIM和李雅普诺夫函数构建新的优化器，既能归纳经典方法，又能在大规模任务上获得最优表现。


<details>
  <summary>Details</summary>
Motivation: 传统优化器多依赖经验改进，缺乏统一理论支持；作者旨在用几何和控制论统一优化器，提供可验证的稳定性保证，并探索更优的优化器设计。

Method: 利用控制理论框架，将优化过程建模为扩展状态的离散时间受控动力系统，识别一个正常吸引不变流形（NAIM），并构造严格的李雅普诺夫函数以保证收敛；通过这一方法生成优化器，既可恢复已有算法，又能设计新的高性能优化器。

Result: 在大规模基准测试中，基于RLO框架设计的优化器实现了先进性能，并通过几何诊断验证其理论预测。

Conclusion: Riemannian Lyapunov Optimizers（RLOs）通过将优化视为在黎曼几何背景下的受控动力系统，统一并系统地推导了经典优化器，提供了一种可靠且可拓展的优化器设计框架。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [42] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 合并成功受方法和任务影响，关键因素是子空间重叠与梯度对齐，提出诊断框架与未来微调策略。


<details>
  <summary>Details</summary>
Motivation: 探究模型合并成功的根本因素，弥补以往仅把可合并性视为内在属性的研究；为微调策略提供更科学的依据。

Method: 使用与架构无关的框架，对四种合并方法进行线性优化，计算可解释的两两指标（如梯度L2距离），并分析其与合并后性能的相关性。

Result: 发现不同方法成功驱动因素差异显著（46.7%指标重叠；55.3%符号一致），体现方法特异性“指纹”。但子空间重叠和梯度对齐始终是兼容性的通用先决条件。

Conclusion: 模型合并的成功取决于合并方法和合作任务。研究发现，子空间重叠与梯度对齐是方法无关的基础前提，为理解合并兼容性提供诊断基础，并激励未来在微调时显式关注这些属性。

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [43] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 两通道sEMG深度学习框架，使用CSAE即时特征提取，结合少样本迁移与增量学习，实现在多种姿态下高精度、低成本假肢控制。


<details>
  <summary>Details</summary>
Motivation: 提高肌电假肢的可靠控制，削减受试者差异影响，并实现低成本、低传感器密度的可行方案。

Method: 使用卷积稀疏自编码器（CSAE）从原始信号提取时序特征，配合少样本迁移学习提升跨受试者性能，并支持增量学习扩展至更多姿态。

Result: 在6类姿态上多受试者F1分数94.3%±0.3%；少样本迁移将未见受试者性能从35.1%±3.1%提升至92.3%±0.9%；增量学习扩展到10类时F1为90.0%±0.2%。

Conclusion: 本研究通过两通道sEMG、卷积稀疏自编码器以及少量样本迁移与增量学习，达到了高精度、低成本、可扩展的我的肌电假肢控制方案。

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [44] [ParalESN: Enabling parallel information processing in Reservoir Computing](https://arxiv.org/abs/2601.22296)
*Matteo Pinna,Giacomo Lagomarsini,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: ParalESN 用对角线复杂递归并行化 RC，保持准确度的同时显著节省计算资源


<details>
  <summary>Details</summary>
Motivation: 解决传统 Reservoir Computing 在处理时序数据时的串行计算和高维水体内存占用昂贵的限制

Method: 提出基于复杂对角线线性递归的 Parallel Echo State Network（ParalESN），实现并行时序数据处理与高维存储结构

Result: 理论证明 Par ಕಾರ್ಯ保留 Echo State Property 和传统 ESN 的通用性，等价表示任意线性水体；实验表明在时间序列预测和 1-D 像素分类任务中保持类似精度，同时大幅降低计算量、能耗

Conclusion: ParalESN 为将RC与深度学习结合提供了可扩展、原则性的管道，兼顾性能与资源效率

Abstract: Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.

</details>


### [45] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: 提出CP4Gen——一种聚类密度估计的条件生成模型置信预测方法，取得更小预测集体积、更简洁结构，提升不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 条件生成模型缺乏校准的不确定性估计，阻碍了其在需要高风险决策的领域的应用。

Method: 基于密度估计的聚类技术，利用模型生成的样本构建预测集；采用系统化的可信度校准来保证预估置信水平。

Result: 在合成与实际气候模拟任务中，CP4Gen相较于现有方法在预测集体积和结构复杂度两方面均表现更佳。

Conclusion: CP4Gen是一种针对条件生成模型的自适应置信预测方法，能够提供校准的、不受异常点干扰、结构更简单的预测集，从而提升高风险应用中的可解释性与信任度。

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [46] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [47] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [48] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 提出隐蔽攻击与BayesClean防御，验证其在工业回归中的有效性能。


<details>
  <summary>Details</summary>
Motivation: 研究回归模型在工业与科学领域的鲁棒性，聚焦对抗性中毒攻击；现有研究多采用不现实的威胁模型，缺乏实践价值。

Method: 提出最优隐蔽攻击框架，考虑不同检测阈值；通过归一化目标法评估攻击效果与可检测性权衡；设计BayesClean防御算法。

Result: 新的隐蔽攻击能突破现有防御；BayesClean在攻击隐蔽且中毒点数较多时，比以往防御更有效。

Conclusion: 提供更具可行性的攻击评估与对策，提升回归模型对中毒的整体安全性。

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [49] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: SCALAR基准通过多尺度材料结构测试，发现显式链式推理能降低幻觉误差但可能损坏一致性，说明几何泛化不能仅靠准确率判断。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型在材料科学推理中效果逐步提升，但其在物理结构化分布变换下的表现尚不清楚。

Method: 提出SCALAR基准，构建从原型晶体到通过超胞扩展与几何截断得到的纳米结构（覆盖数原子到上万原子，总计约10万结构）相互映射的任务，并设计三类评估任务：(i) CIF→属性预测；(ii) 基于链式思考的物理定理驱动推理；(iii) 给定目标属性的逆检索。使用结构化指标评估数值误差、结构幻觉、跨提示一致性、单调推理、输出有效性与检索误差。

Result: 对多种基础模型的实验表明，在显式链式思考的条件下模型表现出现显著但模型依赖的分散变化；虽能减少幻觉与误差，却往往破坏一致性或有效性。结果表明，仅凭准确率无法判断几何尺度泛化能力。

Conclusion: SCALAR基准揭示大语言模型在不同尺度的材料结构推理中存在模型相关的变化，强调需要综合多维指标评估其可解释性与稳健性。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [50] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [51] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: PA-GP-UCB将低保真预测与离线数据融入GP-UCB，用控制变差校正偏差，提升oracle查询效率，理论与实验均验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 真实世界中昂贵oracle与廉价低保真oracle共存，以及大量离线数据可用，传统GP-UCB在采样效率上受限。

Method: 采用控制变差估计器与联合高斯过程后验校正预测偏差，并在GP-UCB框架下实现。

Result: 理论证明后，PA-GP-UCB保持基准GP-UCB的调和四阶regret上界，但常数更小；实验显示在合成及基于人类行为数据的真实场景中收敛更快。

Conclusion: PA-GP-UCB通过结合低成本预测模型与昂贵真实oracle，并利用离线数据，提升了贝叶斯优化的样本效率。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [52] [FlowSymm: Physics Aware, Symmetry Preserving Graph Attention for Network Flow Completion](https://arxiv.org/abs/2601.22317)
*Ege Demirci,Francesco Bullo,Ananthram Swami,Ambuj Singh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recovering missing flows on the edges of a network, while exactly respecting local conservation laws, is a fundamental inverse problem that arises in many systems such as transportation, energy, and mobility. We introduce FlowSymm, a novel architecture that combines (i) a group-action on divergence-free flows, (ii) a graph-attention encoder to learn feature-conditioned weights over these symmetry-preserving actions, and (iii) a lightweight Tikhonov refinement solved via implicit bilevel optimization. The method first anchors the given observation on a minimum-norm divergence-free completion. We then compute an orthonormal basis for all admissible group actions that leave the observed flows invariant and parameterize the valid solution subspace, which shows an Abelian group structure under vector addition. A stack of GATv2 layers then encodes the graph and its edge features into per-edge embeddings, which are pooled over the missing edges and produce per-basis attention weights. This attention-guided process selects a set of physics-aware group actions that preserve the observed flows. Finally, a scalar Tikhonov penalty refines the missing entries via a convex least-squares solver, with gradients propagated implicitly through Cholesky factorization. Across three real-world flow benchmarks (traffic, power, bike), FlowSymm outperforms state-of-the-art baselines in RMSE, MAE and correlation metrics.

</details>


### [53] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 通过联邦学习，多个设备在不共享数据的情况下共同学习更好的LLM路由器，在准确性与成本之间取得更佳权衡。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM路由需要中心化的查询-模型评估数据，难以获取；且每个客户端本地数据有限，难以训练有效路由器。

Method: 提出首个联邦学习框架，使多个客户端在保持数据本地的前提下共同学习共享路由策略；支持多层感知机和非参数K-means两种路由器；支持异构查询分布与非均匀模型覆盖。

Result: 在两个基准上，联邦协作提升了准确-成本前沿：有效模型覆盖增加，查询泛化更好；理论上证明联邦训练降低路由子最优。

Conclusion: 联邦路由框架能够在隐私与异构环境下实现更优的LLM路由，突破传统集中式方法的局限。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [54] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 本文将持续均值估计迁移到近似差分隐私，提出新的矩阵分解方法，得到更低均方误差的更优算法。


<details>
  <summary>Details</summary>
Motivation: 研究持续均值估计时的数据向量是依次到达的，目标是保持对运行均值的准确估计，同时满足用户级差分隐私（protects entire dataset per user）。先前工作集中在纯差分隐私，导致在实际应用中噪声过大，限制了可行性。

Method: 采用近似差分隐私，利用Matrix Factorization机制的最新进展，并提出了一种针对均值估计的专用因子化方法，提升效率与精度。

Result: 实现了在用户级差分隐私下持续均值估计的均方误差界面上的渐近改进，效率更高、准确率更好。

Conclusion: 利用近似差分隐私和专用矩阵分解可显著降低持续均值估计的噪声，提高实用性。

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [55] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: SCOPE是一种基于RL的可扩展路由框架，通过预测模型成本和性能，在不同需求下实现高准确率或低成本。


<details>
  <summary>Details</summary>
Motivation: 现有路由器受限于固定模型集合，难以适应新模型或预算变化，需要一种可扩展、可控制的路由框架。

Method: 使用强化学习训练SCOPE，使其基于检索相似任务中模型表现的推理预测，实现对未见模型的支持，并将路由视为动态决策问题。

Result: 实验表明，SCOPE在性能优先时提升准确率达25.7%，在效率优先时降低成本达95.1%。

Conclusion: SCOPE通过预测模型的成本和性能，实现了对大规模多模型环境下的高效路由，能够在保持高准确度的前提下显著降低推理成本。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [56] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: AgentScore通过LLM生成规则并用数据验证筛选，提出了可落地且性能优异的临床评分生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在临床实践中难以落地，主要因为它们未与工作流程限制（如可记忆性、可审计性、床边执行）对齐；因此需要在可部署的解析规则框架内寻找高性能模型。

Method: 利用大型语言模型提供候选规则，并通过确定性、数据驱动的验证与选择循环，确保统计有效性与可部署性，从而在指数级离散规则空间中进行语义引导优化。

Result: 在八个临床预测任务中，AgentScore超越现有评分生成方法，AUC与更灵活的可解释模型相当；在两项外部验证任务中，其区分度甚至超过传统基于指南的评分。

Conclusion: AgentScore在满足可部署性约束的前提下，生成的临床评分系统在多项预测任务中优于现有方法，并在两项外部验证任务中表现出更高的区分度。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [57] [Label-Efficient Monitoring of Classification Models via Stratified Importance Sampling](https://arxiv.org/abs/2601.22326)
*Lupo Marsigli,Angel Lopez de Haro*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Monitoring the performance of classification models in production is critical yet challenging due to strict labeling budgets, one-shot batch acquisition of labels and extremely low error rates. We propose a general framework based on Stratified Importance Sampling (SIS) that directly addresses these constraints in model monitoring. While SIS has previously been applied in specialized domains, our theoretical analysis establishes its broad applicability to the monitoring of classification models. Under mild conditions, SIS yields unbiased estimators with strict finite-sample mean squared error (MSE) improvements over both importance sampling (IS) and stratified random sampling (SRS). The framework does not rely on optimally defined proposal distributions or strata: even with noisy proxies and sub-optimal stratification, SIS can improve estimator efficiency compared to IS or SRS individually, though extreme proposal mismatch may limit these gains. Experiments across binary and multiclass tasks demonstrate consistent efficiency improvements under fixed label budgets, underscoring SIS as a principled, label-efficient, and operationally lightweight methodology for post-deployment model monitoring.

</details>


### [58] [Molecular Representations in Implicit Functional Space via Hyper-Networks](https://arxiv.org/abs/2601.22327)
*Zehong Wang,Xiaolong Han,Qi Yang,Xiangru Tang,Fang Wu,Xiaoguang Guo,Weixiang Sun,Tianyi Ma,Pietro Lio,Le Cong,Sheng Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: MolField models molecules as continuous 3D fields using a hyper‑network, improving generalization and stability over conventional discrete methods.


<details>
  <summary>Details</summary>
Motivation: Molecular representations often treat molecules as discrete objects, ignoring their continuous, field-like nature.

Method: Treat every molecule as a continuous 3D function and use a hyper‑network (MolField) with structured weight tokenization to learn distributions over these molecular fields, ensuring SE(3) invariance via canonical coordinates.

Result: MolField demonstrably improves performance on molecular dynamics and property prediction tasks, yielding stable and generalizable results that are robust to discretization choices.

Conclusion: Formulating molecular learning in function space fundamentally changes representation generalization and yields more stable downstream behavior.

Abstract: Molecular representations fundamentally shape how machine learning systems reason about molecular structure and physical properties. Most existing approaches adopt a discrete pipeline: molecules are encoded as sequences, graphs, or point clouds, mapped to fixed-dimensional embeddings, and then used for task-specific prediction. This paradigm treats molecules as discrete objects, despite their intrinsically continuous and field-like physical nature. We argue that molecular learning can instead be formulated as learning in function space. Specifically, we model each molecule as a continuous function over three-dimensional (3D) space and treat this molecular field as the primary object of representation. From this perspective, conventional molecular representations arise as particular sampling schemes of an underlying continuous object. We instantiate this formulation with MolField, a hyper-network-based framework that learns distributions over molecular fields. To ensure physical consistency, these functions are defined over canonicalized coordinates, yielding invariance to global SE(3) transformations. To enable learning directly over functions, we introduce a structured weight tokenization and train a sequence-based hyper-network to model a shared prior over molecular fields. We evaluate MolField on molecular dynamics and property prediction. Our results show that treating molecules as continuous functions fundamentally changes how molecular representations generalize across tasks and yields downstream behavior that is stable to how molecules are discretized or queried.

</details>


### [59] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: MAAT利用知识驱动核重构和先验约束，实现更鲁棒的符号动力学发现。


<details>
  <summary>Details</summary>
Motivation: 传统方法在受噪声、观测不完整或仅使用黑盒潜在动力学时往往失效，难以恢复物理可解释的支配方程。

Method: MAAT框架在再生核希尔伯特空间中构建状态重构，直接将非负性、守恒定律和领域特定观测模型等结构与语义先验融入目标函数，并兼容异构采样与测量粒度，输出光滑、物理一致的状态估计和解析时间导数。

Result: 在十二个多样科学基准和多种噪声环境下，MAAT相较强基线显著降低轨迹及其导数的均方误差，为下游符号回归提供更可靠的特征。

Conclusion: MAAT提供了从碎片化传感数据到符号回归的严格接口，显著提升状态估计质量与物理一致性。

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [60] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS通过局部尺度高斯核和自适应稀疏采样，高效校正Cell Painting数据中的批次效应，具备近线性时间、良好精度。


<details>
  <summary>Details</summary>
Motivation: Cell Painting数据受实验室、仪器和协议差异导致的批次效应影响显著，难以保留生物学信号。

Method: BALANS通过为每对样本i、j设定与j所在批次的k近邻距离相对应的局部尺度，用高斯核计算稀疏关联矩阵A；随后采用自适应采样保留每行最强关联，显著降低计算量。

Result: 在多种真实Cell Painting数据集和大规模合成基准上，BALANS实现了近线性时间复杂度，显著缩短跑时并保持批次校正质量。

Conclusion: BALANS是可扩展、高效的批次校正方法，为大规模细胞形态学数据分析提供实用工具。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [61] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 新噪声相关策略：仅与前一迭代相关、部分抵消，去除存储需求，轻量化且精度提升。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵因式分解噪声相关策略准确率提高，但需存储大量噪声，造成显著的存储开销。

Method: 使用伪随机噪声生成器重建噪声，仅与前一迭代相关，并控制地抵消一部分噪声，无需存储历史噪声。

Result: 实验表明，该方法在保持与DP‑SGD相同的内存需求下，计算开销极小，并实现了比标准DP‑SGD更优的精度。

Conclusion: 通过仅对相邻迭代生成并部分抵消噪声，使DP‑SGD在不增加额外存储的前提下提升了精度。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [62] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 本文推导了偏好型贝叶斯优化的解析知识梯度，实验证明其显著优于现有方法，但在某些极端场景下仍有限制。


<details>
  <summary>Details</summary>
Motivation: 现有的知识梯度很难直接应用于只能通过成对比较获得信息的偏好型BO问题，计算昂贵且近似方法效果有限；需要一种可解析且可扩展的知识梯度。

Method: 构造厌恶高斯过程的后验分布，对偏好型BO的look‑ahead步骤进行精确推导，得到闭合式解析表达式；随后将该表达式用于多任务实验和案例研究。

Result: 实验显示EGKG在大部分基准测试中收敛速度快、最终目标值优；在案例研究中亦揭示其在某些稀疏或噪声极大的情况下表现不佳。

Conclusion: 本文提出了精确解析的偏好型知识梯度（EGKG）获取函数，并证明其在多组基准实验中总体优于现有方法；同时指出在某些情形下仍然存在局限。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [63] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 语言模型在有限交互预算下对环境探索表现糟糕；拆分并行与总结历史可显著改善，并提示更好的探索策略仍需研究。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在受限交互预算下探索交互式环境的能力，发现它们往往存在欠探索和次优解，需寻找更有效的探索策略。

Method: 设计三种可调节探索难度的参数化任务（覆盖连续与离散环境），并在此基础上测试两种轻量化介入：将固定预算拆分为并行执行，以及周期性总结交互历史以保留关键发现。

Result: 实验表明：现有顶尖模型普遍欠探索，性能明显落后于简单的探索–利用启发式基线；预算增大时扩展性弱。拆分预算的并行执行意外提升表现；周期性历史总结进一步加速探索。

Conclusion: 语言模型在资源受限的探索任务中表现不佳，简单的预算并行与历史摘要干预可提升性能，说明需开发更系统的探索机制。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [64] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [65] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出将策略表示为占用度量期望，使用集合架构与变分对比的隐空间进行梯度优化，从而在测试时无额外训练即可满足新约束。


<details>
  <summary>Details</summary>
Motivation: 在MDP下，使得多策略的表示可统一学习，以便在测试时通过隐空间优化实现新的行为约束，而不需额外训练。

Method: 将状态-动作样本映射至隐空间，解码出策略与多奖励价值函数，利用变分生成和对比学习塑造隐空间几何，实现梯度优化和行为合成。

Result: 构建的模型能够在隐空间中对政策进行梯度优化，实现对未见价值函数约束的行为合成。

Conclusion: 提出一种基于占用度量的策略表示方法，并通过集合架构、变分生成模型与对比学习构建平滑隐空间，实现在隐空间中梯度优化，从而在测试时引导策略满足未见约束。

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [66] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 提出新几何量化非高斯性，证明锥平坦性，提供一维闭式与高维优化实现，实验验证更好高斯近似。


<details>
  <summary>Details</summary>
Motivation: 研究在最优传输框架下量化经验分布与高斯分布偏离的精确测度；传统矩匹配方法在W_2距离下并非最优近似。

Method: 构造二次Wasserstein空间的锥几何，定义相对Wasserstein角度与正交投影距离，推导角度平坦性证明；在一维给出闭式表达；在高维采用基于半离散对偶形式的随机流形优化算法进行近似计算。

Result: 在一维示例如均匀、拉普拉斯、逻辑分布等得到闭式结果；在高维通过算法有效计算；实验表明相对Wasserstein角度对噪声更鲁棒，且新近似的高斯在FID评价中优于矩匹配。

Conclusion: 该研究提出相对Wasserstein角度与正交投影距离两种度量，用以量化经验分布偏离高斯分布的程度，证明在相对平移不变的二次Wasserstein空间中，任意两射线生成的填充锥是平坦的，从而角度、投影和内积可严格定义；此外发现传统的矩匹配高斯并非W_2-最近高斯。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [67] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: 提出 PoSafeNet，一个基于偏序安全约束的可微分安全层，在多任务实验中提升了安全学习的可行性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的机器人控制中，现有的安全学习方法往往统一或固定优先级地约束多重安全限制，导致不可行或脆弱行为；实际安全需求往往呈异质与部分可比的结构，亟需新的安全建模与实现方法。

Method: 本文将安全约束建模为偏序集合，并将安全组合视为策略类的结构性属性；随后设计 PoSafeNet——一种可微分神经安全层，通过在 poset‑一致的约束顺序下进行序列闭合式投影，实现对安全约束的在线强制执行。

Result: 在多障碍导航、受限机器人操作以及基于视觉的自动驾驶等任务中，PoSafeNet 在可行性、鲁棒性以及可扩展性方面优于传统的无结构和可微分二次规划安全层。

Conclusion: PoSafeNet能够在具有部分可比较安全约束的环境下实现更高的可行性、稳健性和可扩展性，验证了基于 poset 的安全层设计的有效性。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [68] [The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples](https://arxiv.org/abs/2601.22359)
*Hsiang Hsu,Pradeep Niroula,Zichang He,Ivan Brugere,Freddy Lecue,Chun-Fu Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model outputs when inputs are adversarially perturbed. In particular, slight perturbations of forget samples may still be correctly recognized by the unlearned model - even when a re-trained model fails to do so - revealing a novel privacy risk: information about the forget samples may persist in their local neighborhood. In this work, we formalize this vulnerability as residual knowledge and show that it is inevitable in high-dimensional settings. To mitigate this risk, we propose a fine-tuning strategy, named RURK, that penalizes the model's ability to re-recognize perturbed forget samples. Experiments on vision benchmarks with deep neural networks demonstrate that residual knowledge is prevalent across existing unlearning methods and that our approach effectively prevents residual knowledge.

</details>


### [69] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [70] [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)
*Rosen Ting-Ying Yu,Nicholas Sung,Faez Ahmed*

Main category: cs.LG

TL;DR: FIRE利用预训练表格模型无训练即可完成多保真度回归，提升效率与准确性，适用于数据不平衡场景。


<details>
  <summary>Details</summary>
Motivation: 现有GP方法在高保真样本稀缺时会过拟合且计算量巨大，迫切需要一种快速且稳健的方案。

Method: 通过将表格基础模型与高保真度校正模型结合，利用低保真度后验预测分布实现零样本贝叶斯推断。

Result: 在31个基准任务中，FIRE在精度、不确定度量化和运行时间上均优于七种最先进的GP和深度学习多保真度方法。

Conclusion: FIRE实现了在多保真度回归中的高效、无训练需求的方法，显著提升了在极端数据不平衡场景下的性能与效率。

Abstract: Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.

</details>


### [71] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [72] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR用LLM控制多阶段ML流水线，提升50% P99、降低成本97%，无需离线训练。


<details>
  <summary>Details</summary>
Motivation: 解决多阶段ML推理流水线在异构资源、跨阶段耦合和动态瓶颈迁移下的自动扩容挑战。

Method: SAIR框架利用LLM作为情境强化学习控制器，采用基于Pareto支配的奖励塑形、独立边际和惊奇引导的经验检索、以及用户空间CUDA拦截实现细粒度GPU速率控制。

Result: 在四条ML服务流水线及三种工作负载模式下，相较部署基线，SAIR最高可提升P99延迟50%，有效成本降低97%，并实现86%的瓶颈检测准确率。

Conclusion: SAIR通过无梯度更新的LLM控制器，实现了高效且稳健的ML流水线自动扩容，显著提升性能与成本效益。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [73] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: SIREN利用分数函数和积分梯度在高维环境下高效、可不确定性地识别异常根因，性能超过传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的启发式或反事实方法在不确定性和高维依赖下难以有效识别异常值根本原因。

Method: 基于分数函数的归因方法，使用积分梯度沿从异常值到正常分布的路径累积贡献，满足Shapley值四个公设中三个与非对称性公设。

Result: 在合成随机图和真实云服务及供应链数据上，SIREN在归因准确率和计算效率上均优于现有最先进基线。

Conclusion: SIREN通过估计数据似然的分数函数并利用积分梯度对异常值根本原因进行解释，从而在高维非线性数据中实现可靠、高效的根因归因。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [74] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [75] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: MetaLead 是一个完整标注的 ML 结果数据集，记录所有实验和元信息，提升榜单透明度与评估细粒度。


<details>
  <summary>Details</summary>
Motivation: 传统榜单需要大量人工工作；现有数据集仅包含最佳结果且缺乏丰富元信息，难以满足透明和细粒度评估需求。

Method: 构建了一个全面、人工标注的 ML Leaderboard 数据集，记录了每篇论文的所有实验结果，并添加了实验类型（基线、提出方法、方法变体）和训练/测试集划分等元信息，以支持实验类型引导的比较和跨域评估。

Result: MetaLead 为机器学习研究提供了一个功能强大的资源，支持更透明、更细致的评估；同时为自动生成榜单奠定了基础。

Conclusion: MetaLead 通过提供完整的实验结果和详尽元数据，显著提高了机器学习榜单的透明度与可比性，使研究者能够进行更细粒度、跨域的评估。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [76] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 对GRPO进行改进，采用对比学习划分正负结果，最大化正样本概率，验证在数学测验上具竞争力。


<details>
  <summary>Details</summary>
Motivation: GRPO目前需要大量经验技巧来提升性能，例如渐进裁剪和零方差数据过滤，难以针对性改进。

Method: 将K个推理输出拆分为正负两组，采用对比学习（噪声对比估计）最大化正样本似然，而非估计优势值并做软阈值。

Result: 在一系列严苛的数学基准测试上，与DAPO和在线DPO等强基线相比，取得了竞争性甚至更优的表现。

Conclusion: 对比学习方法为LLM推理提供了一种简洁有效的替代GRPO的方案，减少了对经验参数的依赖并提高了性能。

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [77] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 弱扩散先验在信息量足够时依旧能成功逆求，理论解释了其背后的贝叶斯收敛机制。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中常需使用低质量或不匹配的扩散先验，探究其在逆问题中的鲁棒性与适用范围。

Method: 对比实验验证在信息量高的测量下弱先验的表现，并结合贝叶斯一致性理论分析后验收敛条件。

Result: 实验表明弱先验在多数信息量充足场景下与强域内先验相近；理论给出高维测量导致后验聚焦真实信号的条件。

Conclusion: 无论训练数据与未知信号不匹配，若测量信息足够丰富，弱扩散模型仍能有效恢复原始信号，提供可接受的成像质量。

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [78] [Automating Forecasting Question Generation and Resolution for AI Evaluation](https://arxiv.org/abs/2601.22444)
*Nikos I. Bosse,Peter Mühlbacher,Jack Wildman,Lawrence Phillips,Dan Schwarz*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141).

</details>


### [79] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 新权重基解释显示25%特征直接影响输出，深度相关注意力参与，语义侧重分布不同


<details>
  <summary>Details</summary>
Motivation: 当前解释方法忽略了特征为前向传播的计算角色而训练的重构激活，而未能充分理解自编码器的功能影响

Method: 提出基于权重交互的解释框架，直接衡量功能效应，无需激活数据

Result: 在Gemma-2和Llama-3.1模型上三项实验显示：1/4特征直接预测输出token；特征在深度上参与注意力机制；语义与非语义特征在注意力电路中分布不同

Conclusion: 提供了自编码器特征解释的失之“缺失”的外部上下文视角

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [80] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA 通过可扩展的提示池和前沿采样，提升 RLVR 训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统提示池静态或与模型进展不紧密耦合，导致卷盘浪费；需要支持稳定在线策略池增长而不增加额外成本或延迟。

Method: 使用基于堆的边界采样维护可扩展池，结合异步验证的轻量级在线增强，以拓展池并通过拓扑感知重估来稳定查询。

Result: 在两套训练语料、两种训练方案和七个基准上，HeaPA 在准确率上持续领先，且计算量更低，时间保持相当。

Conclusion: 通过前沿聚焦采样和在线策略池增长，HeaPA 在 RLVR 训练中显著提升准确率，并以更少计算达到目标性能，且保持时间便宜。

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [81] [Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity](https://arxiv.org/abs/2601.22450)
*Jianhao Huang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 本文从k-帕里蒂任务出发，阐释并证明Mask Diffusion目标可避免grokking，并通过掩码概率优化提升大模型性能


<details>
  <summary>Details</summary>
Motivation: 研究Masked Diffusion Language Models在k-帕里蒂问题上的泛化性能及其相较于自回归模型的优势

Method: 将MD目标理论分解为信号与噪声两类，对nanoGPT进行训练，并优化掩码概率分布

Result: MD目标避免了grokking，快速且同步泛化；对50M参数模型提升困惑度，对8B模型提升8.8%、5.8%性能，并在预训练与微调中表现优异

Conclusion: MD目标显著改善大规模掩码扩散语言模型的学习曲线与泛化，可作为高效训练与微调的框架

Abstract: Masked Diffusion Language Models have recently emerged as a powerful generative paradigm, yet their generalization properties remain understudied compared to their auto-regressive counterparts. In this work, we investigate these properties within the setting of the $k$-parity problem (computing the XOR sum of $k$ relevant bits), where neural networks typically exhibit grokking -- a prolonged plateau of chance-level performance followed by sudden generalization. We theoretically decompose the Masked Diffusion (MD) objective into a Signal regime which drives feature learning, and a Noise regime which serves as an implicit regularizer. By training nanoGPT using MD objective on the $k$-parity problem, we demonstrate that MD objective fundamentally alters the learning landscape, enabling rapid and simultaneous generalization without experiencing grokking. Furthermore, we leverage our theoretical insights to optimize the distribution of the mask probability in the MD objective. Our method significantly improves perplexity for 50M-parameter models and achieves superior results across both pre-training from scratch and supervised fine-tuning. Specifically, we observe performance gains peaking at $8.8\%$ and $5.8\%$, respectively, on 8B-parameter models, confirming the scalability and effectiveness of our framework in large-scale masked diffusion language model regimes.

</details>


### [82] [Machine Unlearning in Low-Dimensional Feature Subspace](https://arxiv.org/abs/2601.22456)
*Kun Fang,Qinghua Tao,Junxu Liu,Yaxin Xiao,Qingqing Ye,Jian Sun,Haibo Hu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remaining and forgetting data herein. This separability motivates our LOFT, a method that proceeds unlearning in a LOw-dimensional FeaTure subspace from the pretrained model skithrough principal projections, which are optimized to maximally capture the information of the remaining data and meanwhile diminish that of the forgetting data. In training, LOFT simply optimizes a small-size projection matrix flexibly plugged into the pretrained model, and only requires one-shot feature fetching from the pretrained backbone instead of repetitively accessing the raw data. Hence, LOFT mitigates two critical issues in mainstream MU methods, i.e., the privacy leakage risk from massive data reload and the inefficiency of updates to the entire pretrained model. Extensive experiments validate the significantly lower computational overhead and superior unlearning performance of LOFT across diverse models, datasets, tasks, and applications. Code is anonymously available at https://anonymous.4open.science/r/4352/.

</details>


### [83] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: EvoEGF‑Mol利用指数族与Fisher‐Rao几何，将分子生成从分离的欧氏和概率路径整合为稳定的连续流，显著提升生成分子的结构准确性与药物活性。


<details>
  <summary>Details</summary>
Motivation: 传统SBDD在欧氏与概率空间分别构造路径，导致与分子统计流形不匹配，进而影响生成精度与结构一致性。需要一种统一的信息几何方法来弥补这一缺陷。

Method: 构建分子为指数族分布模型，沿Fisher‐Rao度量的指数测地线定义生成流；为防止测地线直接收缩至Dirac点，采用动态集中分布与渐进参数细化的EvoEGF‑Mol框架；并在训练中使用Pro‐Ref架构保持学习稳定。

Result: 在CrossDock上达到93.4%的PoseBusters通过率，超过基线；在Real‑World MolGenBench任务中恢复生物活性骨架，并生成满足MedChem过滤器的候选分子。

Conclusion: EvoEGF-Mol通过在信息几何框架下利用指数族分布和Fisher‐Rao度量，提供了一种稳定的通用化生成流程，显著提升了分子生成的几何精度与化学活性保持。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [84] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: LLM可通过无奖励探索获得潜在学习效果，随后奖励训练能进一步放大；两阶段训练比单纯奖励RL表现更好，证明LLM也能在无奖励环境中构建任务相关知识，提升适应与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统RL对外部奖励过度依赖，限制灵活性与泛化；潜在学习在生物体中已证明能在无奖励的情况下构建内部表征；探讨潜在学习是否存在于LLM中，是否能提升其训练效果。

Method: 先让LLM在无奖励探索阶段进行任务相关知识组织，再进行奖励驱动的强化学习；对多种模型族和任务领域做广泛实验，并提供理论分析解释无奖励探索为何带来性能提升。

Result: 实验显示LLM在无奖励探索阶段有小幅性能提升；采用两阶段无奖励+奖励训练的LLM在最终性能上明显优于仅用奖励跑完整RL过程的LLM。

Conclusion: LLM在未奖励探索阶段表现出类似心理学意义下的潜在学习（latent learning），在后续有奖励的训练中提升表现，最终超越纯奖励驱动的强化学习；此发现揭示LLM在训练中可利用无奖励探索获取任务相关知识，提高泛化与适应性。

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [85] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 教师-学生蒸馏结合MoE与回放策略，提供高效且稳定的持续RL，达到性能保留率>85%，遗忘率<10%.


<details>
  <summary>Details</summary>
Motivation: 强化学习在单任务上表现出色，但难以直接用于顺序任务流；需要高效管理稳定性-可塑性矛盾，利用先前经验实现对新任务的快速泛化。

Method: 将持续学习拆分为两步：先用分布式强化学习训练单任务教师模型，再通过混合专家（MoE）架构和基于回放的策略蒸馏将其持续迁移到中央通才模型。

Result: 在Meta-World基准上，所提出框架实现了超过85%的教师性能恢复，且任务级遗忘率控制在10%以内。

Conclusion: 该训练教师-学生蒸馏框架成功实现了可扩展的持续强化学习，在保持高性能的同时控制灾难性遗忘。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [86] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [87] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: STARS：监测隐藏状态，用几何分析识别认知转折点，注入语言提示实时修正，显著提升大型推理模型性能。


<details>
  <summary>Details</summary>
Motivation: 在等级推理模型中，认知惯性导致过度推理或推理僵化，但现有检测方法无法捕捉内部无声冲突，需要一种无需训练的新框架。

Method: STARS通过监测隐藏层的L2距离突变来捕捉认知转折点，利用几何轨迹分析诊断转向结构，并实时注入状态感知语言提示以引导模型。

Result: 实验表明，STARS在多个基准上显著减少了多余循环并提升模型精度。

Conclusion: STARS框架在不需要额外微调的情况下有效地缓解了大型推理模型的认知惯性，减少了冗余循环并提升了推理准确率。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [88] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: GFT通过温度渐进式微调流模型，在保持预训练优势的同时，理论保证收敛并在有限数据与分布迁移场景下实现更快推理与稳定训练。


<details>
  <summary>Details</summary>
Motivation: 传统的无约束微调易导致预训练获得的精度与效率被破坏；现有奖励驱动方法受限于漂移或训练技术，难以在有限数据与分布迁移中取得均衡。

Method: 在GFT中引入温度控制的中间目标序列，通过逐步插值预训练流和目标分布的漂移；利用最优传输等耦合方法，证明了边际和条件目标的收敛。

Result: 实验表明GFT显著提升收敛稳定性，缩短概率路径，从而加快推理并保持生成质量与标准微调相当。

Conclusion: Gradual Fine‑Tuning（GFT）为在数据有限或分布漂移场景下的流匹配模型提供了稳健高效的微调框架，理论上保证收敛并在实践中提升稳定性与推理速度。

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [89] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 本文提出「动作足够性」概念，证明其优于传统价值推理，展示对数损失训练能自然获取更佳目标表示，并在实验中取得优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过价值函数学习得到的目标表示常会“折叠”需要区分的目标状态，导致低层动作学习失败；需要更合适的目标表示。

Method: 构建信息论框架定义动作足够性；证明价值足够性不蕴含动作足够性；使用低层策略的标准对数损失训练自然而然地产生动作足够的表示；在离线目标条件强化学习环境中进行验证。

Result: 在离散环境中验证动作足够性比价值足够性更能预测控制成功；在流行基准上，基于演员的表示始终优于基于价值估计的表示。

Conclusion: 动作足够性（action sufficiency）是确定最佳动作选择的必要条件，基于演员（actor）训练得到的目标表示比基于价值估计的表示在控制任务中表现更佳。

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [90] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: LLM 学习顺序与人类不符，导致算法错误在新分布中放大；该“碎片化组合”问题无论模型规模或记忆板均难以解决。


<details>
  <summary>Details</summary>
Motivation: LLM 在高规模下常出现意外错误，双方在技能组合方面存在明显差异，导致非人类行为成因尚不明晰，需系统性研究其学习机制。

Method: 在合成算术任务上训练 Transformer，结合广泛消融和细粒度诊断指标，探究学习动力学与技能组合的形成过程。

Result: 发现 Transformer 并未可靠地按人类顺序构建技能，而是往往逆序或并行获取，导致分布偏移下的“碎片化组合”错误；这种现象由对训练数据的相关匹配驱动，而非因果/程序性组合，并且在现代 LLM 中仍然存在，纯模型扩展或记忆板推理并未缓解。

Conclusion: LLM的学习行为与人类期望的技能组合显著不匹配，导致推理可靠性与分布外鲁棒性受限，需要对模型学习机制进行根本性改进。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [91] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 本文提出闭式最优飞行高度与基于信息素奖励的DDPG算法，提升无人机VLC系统的轨迹规划效率。


<details>
  <summary>Details</summary>
Motivation: 开发无人机在可见光通信环境下的三维轨迹规划，提升数据收集效率并减少飞行距离

Method: 先推导VLC信道增益阈值下的闭式最优飞行高度，然后使用带有信息素奖励机制的双延迟深度确定性策略梯度算法优化无人机水平轨迹

Result: 闭式最优高度使飞行距离比基线方法缩短35%，奖励机制将收敛步骤缩短≈50%

Conclusion: 采用最优高度与信息素奖励机制可显著提高无人机辅助VLC数据收集的效率与收敛速度

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [92] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: 提出SCOPE-PD框架，结合主观客观评估，用随机森林预测PD，准确率98.66%，并通过SHAP给出可解释的特征贡献。


<details>
  <summary>Details</summary>
Motivation: 传统帕金森病诊断主观且延迟，需要客观、可解释的早期预测工具。

Method: 收集PPMI数据，构建多模态预测框架，应用多种机器学习算法（如随机森林）并以SHAP进行模型解释。

Result: 随机森林在主客观特征组合下取得最高准确率98.66%，识别出震颤、运动迟缓和面部表情为主要贡献特征。

Conclusion: SCOPE-PD通过融合主观与客观评估，利用随机森林模型实现了98.66%的预测准确率，并通过SHAP解释了关键特征。

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [93] [Learn from A Rationalist: Distilling Intermediate Interpretable Rationales](https://arxiv.org/abs/2601.22531)
*Jiayi Dai,Randy Goebel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Because of the pervasive use of deep neural networks (DNNs), especially in high-stakes domains, the interpretability of DNNs has received increased attention. The general idea of rationale extraction (RE) is to provide an interpretable-by-design framework for DNNs via a select-predict architecture where two neural networks learn jointly to perform feature selection and prediction, respectively. Given only the remote supervision from the final task prediction, the process of learning to select subsets of features (or \emph{rationales}) requires searching in the space of all possible feature combinations, which is computationally challenging and even harder when the base neural networks are not sufficiently capable. To improve the predictive performance of RE models that are based on less capable or smaller neural networks (i.e., the students), we propose \textbf{REKD} (\textbf{R}ationale \textbf{E}xtraction with \textbf{K}nowledge \textbf{D}istillation) where a student RE model learns from the rationales and predictions of a teacher (i.e., a \emph{rationalist}) in addition to the student's own RE optimization. This structural adjustment to RE aligns well with how humans could learn effectively from interpretable and verifiable knowledge. Because of the neural-model agnostic nature of the method, any black-box neural network could be integrated as a backbone model. To demonstrate the viability of REKD, we conduct experiments with multiple variants of BERT and vision transformer (ViT) models. Our experiments across language and vision classification datasets (i.e., IMDB movie reviews, CIFAR 10 and CIFAR 100) show that REKD significantly improves the predictive performance of the student RE models.

</details>


### [94] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [95] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 在专家可变的非平稳时间序列场景下，提出 L2D‑SLDS 模型并配合 IDS 路由；实验验证了其相对 bandit 方法的优势，并强调共享因子的重要作用。


<details>
  <summary>Details</summary>
Motivation: 非平稳时间序列中存在部分反馈以及专家可用性随时间变化的问题，需要一种在此环境下进行灵活决策的策略。

Method: 将专家残差建模为带上下文依赖的范式转换的 L2D-SLDS（Factorised Switching Linear-Gaussian State‑Space Model），并通过 IDS（Information‑Driven Sampling）启发式路由策略在预测成本与信息增益之间做权衡。

Result: 实验表明，与基于上下文的 bandit 方案相比，新方法在预测精度上有显著提升，并且去掉共享全球因子后的消融实验验证了共享因子机制的必要性。

Conclusion: 在非平稳、部分反馈、多变专家可用的环境中，结合 L2D‑SLDS 和 IDS 路由能够有效提升预测性能；共享因子为跨专家信息传递提供了关键机制。

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [96] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 融合模型基、模型免费和记忆控制三模块，提出一种高效贝叶斯采样器，在贝叶斯深度学习中提供快速推断和更佳不确定性估计


<details>
  <summary>Details</summary>
Motivation: 探究大脑多模式控制机制如何实现高效学习，并将其原理应用于可扩展的贝叶斯推断与探索算法

Method: 设计三元采样器：1) 模型基础模块利用目标分布指引慢速采样；2) 模型免费模块基于先前样本学习参数空间模式，快速无评估采样；3) 记忆控制模块通过回忆特定过去事件实现快速采样

Result: 该算法在探索后验分布时显著加速，并在贝叶斯深度学习任务中实现了更精准的系统性不确定性量化

Conclusion: 将模型基、模型免费与记忆控制三种决策机制映射到采样过程，实现了高效、可扩展的贝叶斯推断，适用于大规模机器学习

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [97] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 深度学习PDE模拟长期预测受累积误差和守恒失效影响；保守量校正技术能提高稳定性，且频域研究指出需要关注高频以改进湍流建模。


<details>
  <summary>Details</summary>
Motivation: 深度学习PDE近似模型在长期预测时易累积自回归误差，且往往无法保持物理守恒，导致数值不稳定。

Method: 提出一种模型无关的保守量校正技术，在深度学习PDE近似模型中嵌入物理守恒准则；对多种算子架构进行实验，评估其对长期稳定性的影响；同时从频域分析神经算子的性能，识别对高频成分处理的局限。

Result: 在多种模型架构下，应用保守量校正均实现长期预测稳定性的持续提升；频域分析显示现有架构对高频成分的捕捉能力有限。

Conclusion: 本研究通过保守量校正方法显著提升了自回归神经算子模型在长期预测中的稳定性，表明在深度学习PDE近似中加入物理守恒约束是必要的。同时，还揭示了现有架构在频域处理中对高频成分的不足，提示未来模型设计需更强调高频信息以更好模拟湍流。

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [98] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: FedDis使用因果解耦的双分支（本地/全局）架构和互信息最小化，显著提升联邦时空预测在异质交通数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 非均匀分布的交通数据导致传统联邦模型难以分离全局共性与局部动态，需从生成源层面进行解耦以提升模型鲁棒性。

Method: 设计双支路架构：Personalized Bank提取客户端特有因素，Global Pattern Bank提取共性时空模式，并通过互信息最小化的正则化实现两者信息正交的因果解耦。

Result: 在四个真实数据集上的实验表明，FedDis在性能、效率及可扩展性方面均优于现有最先进方法。

Conclusion: FedDis实现了在跨客户端数据异质性场景下的联邦时空预测的最优性能，且具备高效率与良好可扩展性。

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [99] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: 用中位数基线替代均值，去掉中位数那条样本，解决小回合时优势符号翻转问题，显著提升语言模型小样本训练效果。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的情境下，传统的 GRPO 仅使用 G 条rollout，均值基线会因噪声导致优势符号翻转，进而反向更新模型，降低准确率。

Method: 在每个prompt生成 G+1 条rollout，用中位数作为基线来计算优势；将正好为中位数的那条rollout设为零优势并排除在梯度更新之外，从而保持每个prompt的梯度样本数为 G。

Result: MC‑GRPO 在多种 GRPO 案例、不同模型及规模下均表现出更高的稳定性与最终准确率；对小样本回合的性价比显著提升，且 G=2 与 G=8 的gap缩小到 1%。

Conclusion: Median-Centered Group Relative Policy Optimization (MC‑GRPO) 通过用中位数替代均值基线，大幅提升了在小样本回合（G）下语言模型的稳定性和最终准确率；在 G=2 与 G=8 之间的性能差距缩小到 1% 以内。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [100] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: MGMT：先图内Transformer再meta-graph联合推理，适用于异构多图，表现优异且可解释。


<details>
  <summary>Details</summary>
Motivation: 多图学习在从异构图集合中提取有意义信号方面至关重要，但不同拓扑、尺度和语义的图在缺乏共享节点身份的情况下有效融合信息仍面临挑战。

Method: 提出了Multi-Graph Meta-Transformer（MGMT）框架：先对每个图使用Graph Transformer编码器，将结构和属性映射到共享潜在空间；随后通过注意力机制挑选任务相关的超节点，构建meta-graph，将不同图中功能对齐的超节点通过潜在空间相似性连接；在meta-graph上再加层Graph Transformer实现跨图与内图的联合推理；meta-graph提供内建可解释性。

Result: 在合成数据与真实神经科学应用上，MGMT 在图层级预测任务中持续优于现有最先进模型，并通过超节点/超边实现可解释表示，便于科学发现。

Conclusion: MGMT 成为统一、可扩展、可解释的多图学习框架，在图数据占主导的领域中提升表示技术，为结构化多图学习迈出重要一步。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [101] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: Lethe通过解耦去学习与保留知识，并在继续训练中保持持久去学习，解决了知识复发问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设去学习操作后协作终止，忽略了训练继续的情况，导致知识复发（Knowledge resurfacing）。

Method: Lethe采用‘Reshape–Rectify–Restore’三步流程：先用梯度上升训练临时适配器以放大退学习数据的更新；随后将该适配器作为校正信号，进行层级分离的修正；最后移除适配器，并对保留数据进行短期恢复，以保持去学习内容与保留内容的解耦。

Result: 实验验证 Lethe 在所有层级下均优于传统方法，后续训练后知识复发率落在1%以下，显示出显著的持久去学习效果。

Conclusion: Lethe在统一的脱中心化系统中实现了跨级别（客户端、类别、样本）的持续去学习，并在后续多轮训练中保持了极低的知识复发率（大多数情况下<1%）。

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [102] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 用共识机制代替注意力，可在更宽的学习率范围内稳定训练 transformer，且混合框架兼顾性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统基于注意力的transformer在学习率过大时训练不稳定，现有方法主要改进优化过程，缺乏针对架构的根本创新。

Method: 引入共识机制（Consensus）作为注意力的直接替代，并将其建模为图模型；进一步提出混合共识-注意力框架以兼顾性能与稳定性。

Result: 在文本、DNA 与蛋白质数据集的学习率扫描实验中，Consensus 展示了更宽的稳定学习率区间，并且混合框架在保持性能的同时提升了训练稳定性。

Conclusion: 共识机制通过架构层级显著提升 transformer 在高学习率下的鲁棒性，为未来设计更稳健模型提供了理论与实证支持。

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [103] [TTCS: Test-Time Curriculum Synthesis for Self-Evolving](https://arxiv.org/abs/2601.22628)
*Chengyi Yang,Zhishang Xiang,Yunbo Tang,Zongpei Teng,Chengsong Huang,Fei Long,Yuhan Liu,Jinsong Su*

Main category: cs.LG

TL;DR: 提出TTCS，采用自我一致性奖励驱动的共进化试题生成与求解者更新，显著提升LLM在困难推理和通用任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有test-time训练方法在处理复杂推理问题时受限：原始测试问题难以产生高质量伪标签；测试集规模有限，持续在线更新容易不稳定。

Method: TTCS框架在同一预训练模型上初始化问题合成器和推理求解器两份策略，并通过迭代优化协同演化：合成器根据当前测试问题生成挑战性变体，构建适配求解器能力的结构化课程；求解器利用从原始与合成问题采样的多条答案计算自一致性奖励进行更新；求解器的反馈进一步引导合成器生成更贴合当前能力的问题，整个循环实现了动态、稳定的test-time训练。

Result: 在数学推理基准任务中，TTCS始终提升推理能力；在不同LLM骨干的大规模通用域任务中亦展现显著效果，证明其可扩展性与通用性。

Conclusion: TTCS通过共同进化问题合成器与推理求解器的方式，有效提升LLM在困难推理任务上的性能，并在多模型测试中表现出显著、稳定的改进。

Abstract: Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.

</details>


### [104] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 引入实时形式逻辑验证与强化学习相结合的神经符号框架，显著提升 LLM 逻辑推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的随机 next-token 预测导致逻辑不一致和奖励劫持，传统神经符号方法仅被动后验验证，难以即时纠错。

Method: 使用两阶段训练管线：① 以形式逻辑验证为指导的监督微调；② 基于策略优化的强化学习，实时反馈机制对中间推理错误进行处罚。

Result: 在六个包含数学、逻辑和通用推理的基准测试中，7B 模型平均提升 10.4% ，14B 模型提升 14.2%，均超过现有最先进基线。

Conclusion: 该论文证明通过将形式逻辑验证引入LLM生成过程，可以显著提升推理质量；验证在多个推理基准上带来显著性能提升。

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [105] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: GUDA通过对完整训练模型进行按组无学习，利用ELBO差值做似然归因，快速且准确地完成视觉生成模型的组级训练数据归因，显著快于LOGO retraining。


<details>
  <summary>Details</summary>
Motivation: 传统的训练数据归因方法以样本级别为主，缺乏对艺术风格、类别等组级归因的能力；同时，以LOGO retraining 为基础的组级归因在组数较多时计算成本极高，需要一种更高效的方案。

Method: 利用机器无学习（machine unlearning）技术，对完整训练模型进行按组“去学习”，从而得到每个组缺失的对抗模型；通过比较完整模型与各组去学习后模型的ELBO差值，构造基于似然的评分规则，量化每组对生成结果的影响。

Result: 在CIFAR-10 及 Stable Diffusion 艺术风格归因实验中，GUDA能更可靠地识别主要贡献组，且相较于语义相似度、梯度归因及实例级无学习方法表现更优；在CIFAR-10 上缩短训练时间约两百倍。

Conclusion: GUDA提供了一种高效、可扩展的方法来进行视觉生成模型的组级训练数据归因，能够准确识别对生成结果有显著影响的训练组，并在速度上显著优于传统LOGO retraining 方法。

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [106] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出一种利用服务器端任务向量增长率的零验证早停策略，在FL中显著减省轮数并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统FL早停依赖固定全局轮数或验证数据，导致高计算成本与隐私风险，限制了在实际部署中的应用。

Method: 通过监测服务器端参数中的任务向量增长率，采用数据无关的方式确定最佳停止点，无需额外收集或传输验证数据。

Result: 在皮肤病变与血细胞分类任务中，所提框架平均分别节省47/20轮，并比基于验证数据的早停方法提升12.5%/10.3%的性能。

Conclusion: 本工作首次提出不使用验证数据的联邦学习早停框架，能够在保持甚至提升性能的同时显著减少所需通信轮数。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [107] [Full-Graph vs. Mini-Batch Training: Comprehensive Analysis from a Batch Size and Fan-Out Size Perspective](https://arxiv.org/abs/2601.22678)
*Mengfan Liu,Da Zheng,Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 论文系统比较全图与小批量 GNN 训练，提出 Wasserstein 泛化分析，揭示 batch/fan-out 对性能的异向影响，建议在有限资源时慎选训练策略。


<details>
  <summary>Details</summary>
Motivation: 探究 batch 与 fan-out 对 GNN 收敛和泛化的影响，以及两种训练方式的系统设计需求和效率差异。

Method: 对比全图与小批量 GNN 训练，结合经验与理论，从 batch 与 fan-out 角度进行分析，并使用 Wasserstein 距离进行泛化分析。

Result: 发现 batch 与 fan-out 对收敛与泛化具有非同质影响；为资源受限环境提供调优指南；全图训练并不必然更优。

Conclusion: 全图训练并非始终优于小批量训练，需根据 batch 与 fan-out 尺寸及资源约束调优。

Abstract: Full-graph and mini-batch Graph Neural Network (GNN) training approaches have distinct system design demands, making it crucial to choose the appropriate approach to develop. A core challenge in comparing these two GNN training approaches lies in characterizing their model performance (i.e., convergence and generalization) and computational efficiency. While a batch size has been an effective lens in analyzing such behaviors in deep neural networks (DNNs), GNNs extend this lens by introducing a fan-out size, as full-graph training can be viewed as mini-batch training with the largest possible batch size and fan-out size. However, the impact of the batch and fan-out size for GNNs remains insufficiently explored. To this end, this paper systematically compares full-graph vs. mini-batch training of GNNs through empirical and theoretical analyses from the view points of the batch size and fan-out size. Our key contributions include: 1) We provide a novel generalization analysis using the Wasserstein distance to study the impact of the graph structure, especially the fan-out size. 2) We uncover the non-isotropic effects of the batch size and the fan-out size in GNN convergence and generalization, providing practical guidance for tuning these hyperparameters under resource constraints. Finally, full-graph training does not always yield better model performance or computational efficiency than well-tuned smaller mini-batch settings. The implementation can be found in the github link: https://github.com/LIUMENGFAN-gif/GNN_fullgraph_minibatch_training.

</details>


### [108] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 本文从流图理论解释一致性模型的稳定性问题，提出优化自蒸馏方法，验证了其在图像生成和策略学习中的有效性。


<details>
  <summary>Details</summary>
Motivation: 一致性模型快速生成但训练不稳且可重复性差，现有阐释零散，缺乏系统理论链接。

Method: 基于流映射的理论分析，重构自蒸馏以抑制过大梯度，采用稳定优化训练而无需预训练扩散模型。

Result: 说明训练过程中的不稳定导致退化解，改进自蒸馏在图像生成和基于扩散的策略学习中均显著提升性能，无需预训练模型。

Conclusion: 通过流图视角阐明一致性模型的稳定性与退化解，提出改进的自蒸馏策略实现更稳定收敛，并扩展到非预训练扩散策略学习。

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [109] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: LLM 在周期性 OOD 泛化方面存在显著局限，提出的 Coper 基准验证了 Transformers 在记忆与泛化上的区别。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型相较于人类在周期性 OOD 场景下的泛化差距，阐明 Transformer 对周期性模式的捕捉与泛化瓶颈。

Method: 在抽象代数与推理视角下统一解释周期性，构造 Coper 基准（含 Hollow 与 Extrapolation 两种 OOD 设置），并通过实验评估模型表现。

Result: 实验显示，Transformers 能在训练期间记忆周期数据，却无法将复合周期泛化到 OOD 场景。

Conclusion: Transformers 在周期性 OOD 泛化上表现受限，能够记忆训练数据但难以泛化至未见的复合周期模式。

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [110] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 研究提出了可操作的METRIC指标库，用于评估医学机器学习数据质量，并在心电图数据集上验证其效果，为可信医疗AI奠定基础。


<details>
  <summary>Details</summary>
Motivation: 本研究意识到医学机器学习在临床应用中，数据质量的评估缺乏系统方法，导致算法可信性难以保证，亟需量化数据质量的手段来提升可信度。

Method: 提出METRIC框架并实现为实用指标库，配备指标卡（定义、适用性、例子、陷阱、建议），并给出决策树帮助使用者根据具体任务挑选合适指标。

Result: 在PTB-XL ECG 数据集上验证了该指标库的实效性，展示了不同指标组合对模型性能与可靠性的影响。

Conclusion: 首次为医学ML训练与测试数据提供“适用性评估”工具，为建立可信AI提供可操作的基础，为临床与监管应用奠定了重要基础。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [111] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [112] [A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation](https://arxiv.org/abs/2601.22708)
*Haonan He,Jingqi Ye,Minglei Li,Zhengbo Wang,Tao Chen,Lei Bai,Peng Ye*

Main category: cs.LG

TL;DR: 首次对 LoRA 变体进行系统化分类、统一理论复盘、代码整合和标准化评测，发现其对学习率敏感，且优于大多数变体。


<details>
  <summary>Details</summary>
Motivation: LoRA 变体众多导致方法、理论、代码与评估碎片化，缺乏统一研究与系统化评测。

Method: 对 LoRA 变体进行四轴分类（秩、优化动态、初始化、与 Mixture-of-Experts 的整合），统一理论框架下审视其更新动力学，开发 LoRAFactory 模块化代码库，支持统一接口实验与细粒度分析。

Result: 大规模跨任务评测揭示 LoRA 对学习率极为敏感，在适当配置下可与并超过其多数变体。

Conclusion: LoRA 基本方法在与其变体的系统研究后显示出在多任务中可匹配或超越多数变体的性能，且其关键超参数主要为学习率。

Abstract: Low-Rank Adaptation (LoRA) is a fundamental parameter-efficient fine-tuning method that balances efficiency and performance in large-scale neural networks. However, the proliferation of LoRA variants has led to fragmentation in methodology, theory, code, and evaluation. To this end, this work presents the first unified study of LoRA variants, offering a systematic taxonomy, unified theoretical review, structured codebase, and standardized empirical assessment. First, we categorize LoRA variants along four principal axes: rank, optimization dynamics, initialization, and integration with Mixture-of-Experts. Then, we review their relationships and evolution within a common theoretical framework focused on low-rank update dynamics. Further, we introduce LoRAFactory, a modular codebase that implements variants through a unified interface, supporting plug-and-play experimentation and fine-grained analysis. Last, using this codebase, we conduct a large-scale evaluation across natural language generation, natural language understanding, and image classification tasks, systematically exploring key hyperparameters. Our results uncover several findings, notably: LoRA and its variants exhibit pronounced sensitivity to the choices of learning rate compared to other hyperparameters; moreover, with proper hyperparameter configurations, LoRA consistently matches or surpasses the performance of most of its variants.

</details>


### [113] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 论文证明，使用VLM的可提示嵌入作为LAM训练目标，可消除动作相关噪声并大幅提升下游任务性能（最高六倍），提示干扰物是关键。旧版VLM有时更优。


<details>
  <summary>Details</summary>
Motivation: Latent Action Models（LAMs）在预训练视觉语言动作模型中非常重要，但在观察中包含动作相关干扰物时会失效。研究旨在利用现有视觉语言模型（VLM）的共识推理能力，提供可提示的表征，从而在无监督方式下分离可控变化与噪声。

Method: 将VLM产生的可提示表征作为LAM训练的目标，对多种主流VLM进行基准测试，考察其对提示词和超参数的鲁棒性，并尝试用“忽略干扰物”的方式引导VLM从而提升LAM质量。

Result: 发现不同VLM在可提示表征质量上存在显著差异；更近期的VLM表现不一定优于较旧模型；通过简单提示忽略干扰物，能够显著提升LAM质量，在Distracting MetaWorld上成功率提升约六倍。

Conclusion: 利用VLM的可提示表征能有效引导LAM对动作进行无监督学习，减少干扰；提示策略对提升性能至关重要；VLM的版本并非越新越好。

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [114] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: LoRDS利用低秩矩阵拆解实现元素级量化，兼顾效率与精度，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统块级量化在保持效率的同时牺牲了表示自由度；迫切需要一种兼顾效率与表达性的量化方法。

Method: 通过将缩放张量拆解为连续低秩矩阵（S=BA）实现元素级量化，并在此基础上构建LoRDS框架，支持高效的初始化、迭代优化、联合QAT以及高秩PEFT。

Result: 在Llama3-8B等模型上，LoRDS在3位量化下相较NormalFloat提升多达27.0%准确率；在RTX 4090上实现1.5倍推理加速，并在下游任务中以4bit QLoRA提升9.6%的PEFT性能。

Conclusion: LoRDS提供了一种高效且更具表达力的LLM量化方案，兼具低开销、可集成的PTQ/QAT以及高秩权重更新，显著提升模型精度与推理速度。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [115] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文把 Softmax 损失放进 Fenchel‑Young 框架中，评估其一致性；用偏差-方差拆解分析近似方法的收敛；给出每轮复杂度和效率-有效性权衡；实验验证理论与实践的一致性。


<details>
  <summary>Details</summary>
Motivation: 希望厘清 Softmax 损失的基本理论特性，解决在极大类别数下的可扩展性问题；为在高类别维场景中选取合适的损失函数提供科学依据。

Method: 对 Softmax 族损失进行一致性、梯度动态和收敛性分析；引入结构化的偏差-方差分解，给出近似方法的收敛保证；对每轮迭代的时间复杂度进行解析，揭示有效性-效率的明确折中，并通过实验验证。

Result: 验证了 Softmax 族损失在分类与排序指标上的一致性，并揭示了不同代价函数对应的梯度收敛差异；偏差-方差分解为近似方法的收敛性提供了保证；实验表明一致性、收敛与性能高度相关，为实际应用带来明确改进。

Conclusion: 本文为大类机器学习任务中的 Softmax族损失提供了理论与实践两方面的统一框架，证明其一致性与梯度行为，并给出近似方法的偏差-方差分析与复杂度权衡，为损失选择提供了系统且可操作的理论依据。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [116] [Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks](https://arxiv.org/abs/2601.22751)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|μ- α|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.

</details>


### [117] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [118] [Understanding Generalization from Embedding Dimension and Distributional Convergence](https://arxiv.org/abs/2601.22756)
*Junjie Yu,Zhuoli Ouyang,Haotian Deng,Chen Wei,Wenxiao Ma,Jianyu Zhang,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep neural networks often generalize well despite heavy over-parameterization, challenging classical parameter-based analyses. We study generalization from a representation-centric perspective and analyze how the geometry of learned embeddings controls predictive performance for a fixed trained model. We show that population risk can be bounded by two factors: (i) the intrinsic dimension of the embedding distribution, which determines the convergence rate of empirical embedding distribution to the population distribution in Wasserstein distance, and (ii) the sensitivity of the downstream mapping from embeddings to predictions, characterized by Lipschitz constants. Together, these yield an embedding-dependent error bound that does not rely on parameter counts or hypothesis class complexity. At the final embedding layer, architectural sensitivity vanishes and the bound is dominated by embedding dimension, explaining its strong empirical correlation with generalization performance. Experiments across architectures and datasets validate the theory and demonstrate the utility of embedding-based diagnostics.

</details>


### [119] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 论文系统分析分子语言模型在固定计算预算下的尺度规律，揭示模型规模、数据量和分子表示方式对性能的影响，并公开最大规模模型库。


<details>
  <summary>Details</summary>
Motivation: 大规模分子生成模型已表现出潜力，但尚不清楚其是否在固定资源下遵循尺度规律，理解这一点对于正确分配模型规模、数据量与表示方法至关重要。

Method: 在300个模型上进行10,000+实验，严格控制计算预算，独立变动模型规模、训练token数及分子表示方式，分析预训练与下游转移任务的尺度行为。

Result: 发现预训练和下游转移任务均存在清晰的尺度规律；分子表示方式对性能有显著影响；解释了此前关于分子生成模型尺度行为的矛盾观察。

Conclusion: 本文证明了在固定计算预算下，分子语言模型的性能遵循可预测的尺度规律，并强调分子表示方式在性能中的显著作用。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [120] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [121] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO 用总变差二次惩罚替代裁剪，保持全局可微，解决大模型训练不稳问题，并在多任务上保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 裁剪机制在大模型后训练中引入梯度失效、奖励劫持、训练不稳等问题

Method: 采用基于总变差(Total Variation)的凸二次惩罚替代传统裁剪机制

Result: 在推理与对齐场景下，CFPO 与裁剪法同等性能且提升了稳定期，降低了冗余使用、能力退化。实现只需一行代码，且不增加额外超参

Conclusion: CFPO 是一种可直接替代裁剪方法的高效稳定的后训练策略优化技术

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [122] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: 提出 B 指标衡量边界质量，基于此开发 Sombrero 算法，通过置信度引导边界放置，提高分层模型对难预测字节的关注，显著提升准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统的固定分词导致计算在均衡位置分配，尽管端到端方法可学习边界，但难以定量评估并控制计算投入在文本中难预测的位置。

Method: 提出基于边界增强度 B 的无路由器边界质量度量，并设计 Sombrero：使用置信度对齐的边界损失以及在输入层应用置信度加权平滑，以引导边界向预测难度高的位置移动，稳定边界学习。

Result: 在覆盖英语、德语、代码及数学内容的 1B 规模 UTF-8 语料库中，Sombrero 在准确率与效率的权衡上均有提升，且生成的边界更一致地聚焦于高突发率位置。

Conclusion: Sombrero通过更合理地将词段边界放在难以预测的位置，提升了分层序列模型的准确性-效率平衡，使得计算资源更集中于高信息量的字节上。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [123] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 改进NVFP4量化训练：使用MS-EDEN降低量化误差，Quartet II方案提升梯度估计与速度，已在大模型训练中验证并公开代码。


<details>
  <summary>Details</summary>
Motivation: NVFP4支持全低精度训练但现有SR方法在梯度估计中损失表示能力；需改进量化精度以兼顾训练效率与精度。

Method: 提出微尺度无偏量化原理MS-EDEN，并将其嵌入Quartet II线性层全NVFP4量化方案，实现正向及反向矩阵乘法梯度估计改进；并提供配套CUDA核实现。

Result: 实验证明Quartet II在所有主要矩阵乘法上梯度估计更佳；在38B token 1.9B 参数LLM训练中保持接近FP16精度；Blackwell GPU上实现4.2×BF16速度提升。

Conclusion: 通过MS-EDEN量化和Quartet II方案，NVFP4完全量化训练在误差、梯度估计及吞吐上均优于现有SR方法，可在Blackwell GPU上实现4.2倍BF16加速，并在1.9B参数LLM训练中保持高精度。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [124] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 本研究提出级联生成架构，结合低分辨率分类表示与高分辨率流匹配，显著提升混合型特征生成质量、检测分数提升40%。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型难以精准生成包含离散状态与连续分布混合的单一特征，需改进方法以处理离散化/缺失等数值特征的混合型特征。

Method: 先生成低分辨率表格行，包括纯分类特征和数值特征的粗分类表示；随后利用该低分辨率信息指导高分辨率流匹配模型，应用新颖的引导条件概率路径和数据相关耦合。

Result: 模型生成的样本在分布细节捕捉上显著优于现有方法，检测分数提升约40%，证明指标上效果显著。

Conclusion: 通过级联生成和引导条件概率路径，模型显著提升了混合型特征的生成质量，并证明了平均运输成本的收紧；实验结果显示生成样本更真实，分布细节更精准，检测分数提升约40%。

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [125] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: MetaDrug 通过双层自适应与同伴适应，并结合不确定性筛选，在新患者冷启动情况下提升药物推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 高维电子健康记录数据库是临床决策支持的核心，但现有药物推荐方法在面对新患者（用户冷启动）时表现不佳，主要因缺乏足够的处方历史来构建个性化模型。

Method: MetaDrug 提出一种双层元学习框架：1）自适应（self‑adaptation）：利用新患者自身的时间序列事件作支持集来捕获时序依赖；2）同伴适应（peer‑adaptation）：借助与新患者相似访视的同伴数据丰富表示；3）不确定性量化模块对支持集进行排序与筛选，剔除无关信息。

Result: 在 MIMIC‑III 与 AKI 数据集上，MetaDrug 在冷启动患者情境下，一直优于现有最优药物推荐方法。

Conclusion: MetaDrug 通过多级元学习与不确定性处理，显著缓解了患者冷启动问题，为临床药物推荐提供了更可靠、个性化的解决方案。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [126] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [127] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 一种基于 LoRA 的稀疏专家池与 AGO 正交损失的轻量级持续学习框架，显著降低参数与推理成本，取得领先的泛化与性能升级


<details>
  <summary>Details</summary>
Motivation: 持续学习在视觉-语言模型中仍面临任务自适应与灾难性遗忘的双重挑战，传统方法往往伴随高计算成本或对外部知识的依赖，本工作旨在通过参数高效调优减少这些问题

Method: 构建可分解的 Rank‑1 专家池，将单一 LoRA 模块拆解为稀疏任务专用更新；以 [CLS] 语义引导动态选取专家；提出 Activation‑Guided Orthogonal（AGO）损失在任务间正交化关键 LoRA 权重，从而实现低参数更新和最小化任务干扰

Result: 在多种基准上实现所有指标的 state‑of‑the‑art 结果，突破零样本上界；训练参数比基线下降 96.7%；方法不需要外部数据或任务 ID，推理无额外延迟，计算轻量

Conclusion: 通过稀疏专家组合和正交约束，提出的框架有效缓解灾难性遗忘，同时保持下游任务性能，为视觉‑语言模型的持续学习提供了高效、轻量级的解决方案

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [128] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [129] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OptiMAG 用 Fused Gromov-Wasserstein 距离和 KL 散度生成正则化，解决多模态图中语义架构与显式图结构不一致的问题，显著提升各类节点学习与多模态生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 对多模态图进行建模时，节点在不同模态下的嵌入所诱导出的隐式语义结构与显式图结构往往存在不一致，导致在使用显式结构进行消息传递时会聚合相似度较低的特征，引入模态特异噪声，阻碍有效的节点表征学习。

Method: 设计 OptiMAG：一个基于不平衡最优传输的正则化框架。核心技术为使用 Fused Gromov-Wasserstein 距离显式引导局部邻域内的跨模态结构一致性，缓解结构-语义冲突；同时通过 KL 散度惩罚实现跨模态不一致的自适应处理。该框架可无缝集成进现有多模态图模型，作为 Drop‑in 正则化器使用。

Result: 在节点分类、链路预测等图中心任务以及图2文字、图2图像等多模态生成任务上，OptiMAG 与基线方法相比持续表现更优。

Conclusion: OptiMAG 通过将隐式语义与显式图结构对齐，抑制模态噪声，提高多模态图学习的效果，为多模态图建模提供了系统且易用的正则化方案。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [130] [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)
*Zhanglu Yan,Kaiwen Tang,Zixuan Zhu,Zhenyu Bai,Qianhui Liu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Matterhorn 通过新编码与 memristive synapse 单元降低能耗，在 GLUE 上平均准确率提升 1.42%，能效提升 2.31 倍。


<details>
  <summary>Details</summary>
Motivation: 当前 SNN 能耗评估仅关注 accumulate 操作，忽略了数据移动占总能耗近 80% 的事实，亟需更全面的节能方案。

Method: 结合 Masked Time-To-First-Spike (M‑TTFS) 编码、死区稀疏策略以及 memristive synapse unit (MSU) 的 compute‑in‑memory 技术，减少了 spike 移动和权重访问成本。

Result: 在 GLUE 基准上，Matterhorn 在平均准确率上超过现有 SNN 1.42%，并实现了 2.31 倍的能效提升。

Conclusion: Matterhorn 在大规模语言模型推理中实现了能效显著提升，并在 GLUE 基准上取得了超过现有 SNN 的 1.42% 平均准确率提升。

Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.

</details>


### [131] [Synthetic Time Series Generation via Complex Networks](https://arxiv.org/abs/2601.22879)
*Jaime Vale,Vanessa Freitas Silva,Maria Eduarda Silva,Fernando Silva*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.

</details>


### [132] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL通过将命题映射为可参数化的谓词，解决了LTL任务在未见命题词汇上的泛化难题，在实验中实现了零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 在多任务强化学习中，需要训练通用策略以处理未训练过的任务；现有的线性时序逻辑（LTL）方法虽能在LTL规范上泛化，却无法泛化到未见的命题词汇。该论文旨在解决命题词汇泛化问题。

Method: 将命题视为参数化谓词，而非离散符号，训练可共享结构的策略；提出将谓词嵌入并组合以表示LTL规范的新架构。

Result: 通过实验，在挑战性环境中实现了对新命题词汇和任务的零样本泛化，展示了在LTL结构和命题参数化方面的成功。

Conclusion: PlatoLTL能够让策略在LTL公式结构和命题参数上实现组合型与参数型的零样本泛化，为通用多任务强化学习提供了可靠方法。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [133] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出一种正则化校准方法，使用预秩函数在训练阶段实现多变量校准，并新增PCA预秩以发现结构误差。


<details>
  <summary>Details</summary>
Motivation: 多变量校准难以实现，现有预秩诊断仅用于后验评估，缺乏训练时校准手段。

Method: 基于预秩函数的正则化校准方法，并引入PCA预秩投影到预测分布的主成分方向。

Result: 在仿真和18个真实多输出回归数据集上，该方法显著提高了预秩校准，且PCA预秩能检测到传统预秩未发现的依赖结构缺陷。

Conclusion: 利用预秩函数进行正则化校准，可在多变量分布回归训练过程中实现更好的校准而不损失预测精度，同时PCA预秩能揭示依赖结构失配。

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [134] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [135] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: FlexLoRA利用谱能量熵报告矩阵重要性，支持秩裁剪扩展并以零影响初始化，实现更灵活、更高效的低秩微调。


<details>
  <summary>Details</summary>
Motivation: 全量微调成本高；LoRA固定秩限制灵活性；现有动态秩方法缺乏矩阵级评估且无法在需要时扩展容量。

Method: 通过谱能量熵评估矩阵重要性，在全局预算下支持秩裁剪与扩展，并采用零影响初始化以保证新加入奇异方向的稳定性。

Result: 在多个基准任务上，FlexLoRA始终优于最先进的基线，性能提升显著。

Conclusion: FlexLoRA提供了一种更具原则性和灵活性的低秩适配框架，显著提升了参数高效微调的性能与稳定性。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [136] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [137] [Scalable Topology-Preserving Graph Coarsening with Graph Collapse](https://arxiv.org/abs/2601.22943)
*Xiang Wu,Rong-Hua Li,Xunkai Li,Kangfei Zhao,Hongchao Qin,Guoren Wang*

Main category: cs.LG

TL;DR: STPGC uses algebraic‑topology concepts to coarsen graphs efficiently, preserving topology and GNN receptive fields, leading to better performance in node classification.


<details>
  <summary>Details</summary>
Motivation: Graph coarsening aims to reduce graph size while preserving predictive performance of GNNs. Existing methods either preserve spectral or spatial characteristics, but preserving topological features retains GNN performance yet incurs exponential complexity.

Method: Introduce graph strong collapse and graph edge collapse from algebraic topology, leading to GStrongCollapse, GEdgeCollapse, and NeighborhoodConing algorithms to remove dominated nodes/edges while strictly preserving topological features and GNN receptive fields.

Result: Experiments on node‑classification tasks show that STPGC is efficient and effective, with accelerated training compared to prior approaches.

Conclusion: The proposed STPGC framework provides a scalable, topology‑preserving coarsening method that maintains GNN performance with reduced computational cost.

Abstract: Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.

</details>


### [138] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: ECTR 将环境条件尾部重加权与总变差 invariant 学习结合，在有无环境标签情况都能显著提升多种场景下的 OOD 性能。


<details>
  <summary>Details</summary>
Motivation: 提升在多环境和稀有样本导致的分布偏移下的泛化性能，解决现有IRL方法忽略样本层异质性的问题。

Method: 提出环境条件下的尾部重加权（Environment‑Conditioned Tail Reweighting）结合总变差 (Total Variation) 的 invariant 学习框架 (ECTR)，同时可通过对抗式最小化推断潜在环境，从而实现环境层级和样本层级的协同鲁棒。

Result: 在回归、表格、时序与图像分类等多样的 OOD 基准实验中，ECTR 在最差环境及平均 OOD 性能均持续优于现有方法。

Conclusion: 该框架能够在混合分布偏移条件下，通过联合环境级不变性与样本级鲁棒性，显著提升 OOD 泛化。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [139] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR: 文章通过严谨的Transformer理论证明困惑度可能误导模型选择，即：如果模型能准确预测一序列，它必然会在另一序列上出现低困惑度但错误预测，导致困惑度不一定反映模型真实性能。


<details>
  <summary>Details</summary>
Motivation: 先前实验表明困惑度存在局限性，但缺乏严谨理论证明，本文针对这一缺口展开研究。

Method: 使用Transformer连续性理论推导困惑度与模型准确率的关系，并对等困惑度曲线进行解析研究。

Result: 证明困惑度在某些情况下无法准确识别更优秀模型，并提出模型置信度提升需伴随准确率提升才能被困惑度选中。

Conclusion: 本文证明，在假设存在可压缩的解码器Transformer能够准确自信预测某序列的前提下，其余可预见到存在另一个序列在该模型下低困惑度但却预测错误，从而揭示常用困惑度指标在模型选择时的潜在不稳定性。

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [140] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [141] [Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic](https://arxiv.org/abs/2601.22970)
*Jeong Woon Lee,Kyoleen Kwak,Daeho Kim,Hyoseok Hwang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rather than the cause. In this work, we theoretically establish that policy non-smoothness is fundamentally governed by the differential geometry of the critic. By applying implicit differentiation to the actor-critic objective, we prove that the sensitivity of the optimal policy is bounded by the ratio of the Q-function's mixed-partial derivative (noise sensitivity) to its action-space curvature (signal distinctness). To empirically validate this theoretical insight, we introduce PAVE (Policy-Aware Value-field Equalization), a critic-centric regularization framework that treats the critic as a scalar field and stabilizes its induced action-gradient field. PAVE rectifies the learning signal by minimizing the Q-gradient volatility while preserving local curvature. Experimental results demonstrate that PAVE achieves smoothness and robustness comparable to policy-side smoothness regularization methods, while maintaining competitive task performance, without modifying the actor.

</details>


### [142] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [143] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: dgMARK为离散扩散语言模型提供了一种无需改动模型的解码引导水印方案，可与流行解码策略结合，显著提升水印检测能力并在编辑后保持可靠性。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型与自回归模型不同，可任意顺序生成标记，故其在未遮盖顺序上的强烈依赖为创建新型水印通道提供可能。

Method: 通过利用扩散模型在不同解码顺序下的敏感性，制定基于二进制哈希的奇偶校验约束，引导解码顺序；采用滑动窗口统计检测水印，并可与各种解码策略结合，提供一阶前瞻增强。

Result: dgMARK在未显式重新加权模型概率的情况下，成功通过奇偶匹配统计检测水印，并在插入、删除、替换及改写等后处理操作下保持鲁棒性。

Conclusion: 提出了一种针对离散扩散语言模型的解码引导水印方法dgMARK，能够高效识别模型输出并保持后处理鲁棒性。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [144] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [145] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 作者提出CC1框架统一三类离线RL约束，并提出自适应的ACPO算法，实验验证其在多域数据集上的性能优势。


<details>
  <summary>Details</summary>
Motivation: 离线RL中不同约束形式各自受限，缺少统一理论解释其相互联系与权衡，导致方法选择繁琐且难以优化。

Method: ①构建连续约束插值（CCI）优化框架，统一三类约束为同一光谱；②引入单一插值参数，通过闭式解析实现约束类型的平滑过渡与组合；③基于CCI设计ACPO算法，采用原始-对偶方法通过拉格朗日对偶更新自适应插值参数；④在最大熵下推导性能差异引理，并给出闭式最优策略及其参数化投射的性能下界。

Result: 在D4RL与NeoRL2基准上，ACPO相较于现有基线显著提升性能，整体达到最先进水平。

Conclusion: 该论文通过连续约束插值（CCI）框架揭示了离线强化学习中三类约束（加权行为克隆、密度正则化、支持约束）的统一关系，并提出了通过拉格朗日双重更新实现插值参数自适应的自动约束策略优化（ACPO）算法。实验表明，ACPO在D4RL与NeoRL2数据集上均实现了稳健的性能提升，达到了领先水平。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [146] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [147] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [148] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [149] [Adaptive Edge Learning for Density-Aware Graph Generation](https://arxiv.org/abs/2601.23052)
*Seyedeh Ava Razi Razavi,James Sargant,Sheridan Houghten,Renata Dividino*

Main category: cs.LG

TL;DR: 新版Wasserstein图生成框架通过可学习的距离型边预测和密度自适应选择，显著提高了生成图的结构真实性和类别一致性。


<details>
  <summary>Details</summary>
Motivation: 传统基于GAN的图生成往往使用固定概率的随机采样，难以捕捉复杂节点间的结构依赖；因此需提出更灵活的生成策略来逼近真实图的复杂连边模式。

Method: 采用WGAN+梯度惩罚，生成器通过把节点嵌入潜在空间并基于距离学习边的概率，随后由可微边预测器直接给出节点间连边；通过密度感知选择机制自适应匹配目标类的稀疏度；判别器使用GCN来评估生成图的拓扑与类分布。

Result: 在基准数据集上实验显示，该框架生成的图在结构连贯度、类一致连边与真实图的密度分布上均优于现有基线，并且训练更稳定、可实现可控合成。

Conclusion: 本研究通过结合可学习的距离基边预测器和密度感知选择机制，在Wasserstein GAN框架下实现了更具结构一致性和类别自洽性的图生成，显著提升了生成图的真实性与可控性。

Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.

</details>


### [150] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: RLRR通过使用相对排名奖励，解决了传统基于绝对奖励的稀疏与不稳定问题，提升了RL在大型语言模型推理与生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统组基RL方法依赖绝对奖励，导致在可验证任务中监督稀疏，或在开放任务中奖励范围不稳定，影响优势估计与学习效果。

Method: 采用相对奖励框架RLRR，并配合基于列表的Ranking Reward Model来直接生成组内相对排名，从而实现对奖励的重塑与优化。

Result: 在推理基准与开放式生成任务上，RLRR均优于基线组学方法，表现出更稳健、更持续的提升。

Conclusion: RLRR通过将奖励从绝对分数转化为相对排名，显著减轻了奖励稀疏与不稳定问题，在推理和开放式生成任务上均实现了相对稳定的性能提升。

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [151] [ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations](https://arxiv.org/abs/2601.23068)
*Joao Fonseca,Julia Stoyanovich*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Computing the importance of features in supervised classification tasks is critical for model interpretability. Shapley values are a widely used approach for explaining model predictions, but require direct access to the underlying model, an assumption frequently violated in real-world deployments. Further, even when model access is possible, their exact computation may be prohibitively expensive. We investigate whether meaningful Shapley value estimations can be obtained in a zero-shot setting, using only the input data distribution and no evaluations of the target model. To this end, we introduce ExplainerPFN, a tabular foundation model built on TabPFN that is pretrained on synthetic datasets generated from random structural causal models and supervised using exact or near-exact Shapley values. Once trained, ExplainerPFN predicts feature attributions for unseen tabular datasets without model access, gradients, or example explanations.
  Our contributions are fourfold: (1) we show that few-shot learning-based explanations can achieve high fidelity to SHAP values with as few as two reference observations; (2) we propose ExplainerPFN, the first zero-shot method for estimating Shapley values without access to the underlying model or reference explanations; (3) we provide an open-source implementation of ExplainerPFN, including the full training pipeline and synthetic data generator; and (4) through extensive experiments on real and synthetic datasets, we show that ExplainerPFN achieves performance competitive with few-shot surrogate explainers that rely on 2-10 SHAP examples.

</details>


### [152] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: SplineFlow 通过 B‑spline 插值改进流匹配算法，更准确地逼近非线性动力系统，并在多任务实验中超过传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前最先进方法在建模动力系统时使用线性插补来构造条件路径，无法充分捕捉状态演化，尤其是从不规则采样观测中学习高阶动力学时会出现不足。需要满足多边缘约束；传统高阶多项式易不稳定、振荡。

Method: 提出 SplineFlow，利用 B‑spline 基的光滑与稳定性，联合建模跨观测的条件路径。B‑spline 逼近捕获复杂底层动力学，并保证多边缘需求。

Result: 在多种确定性与随机动力系统及细胞轨迹推断任务上，SplineFlow 相较现有基线显著提升。

Conclusion: SplineFlow 为连续归一化流提供稳健的高阶动力学建模框架，在多边缘约束下展现理论和实验上的优势。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [153] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: 修改训练目标使置信度与真值匹配，提升ECL 2-8%，保持或提升准确率，并引入 Confidence@k 做贝叶斯最优输出选择。


<details>
  <summary>Details</summary>
Motivation: 大模型虽擅长下一个词预测，但置信度校准往往失衡，偏好对齐增加了这一问题。

Method: 提出Calibration Aware Token-level Training Objective (CATTO)，在保留原偏好优化目标的同时将预测置信度与实际正确率对齐。

Result: 与直接偏好优化(DPO)相比，CATTO 在分布内降低了2.22%-7.61% 的ECL，分布外降低 1.46%-10.44%；相较最强 DPO 基线，提升更为细微。准确率保持不变或略有提升。引入 Confidence@k 进行测试时的基尼优化。

Conclusion: CATTO 通过对齐置信度与真实性质，显著提升了对齐模型的校准表现且未牺牲任务准确率，为下一步应用提供更可靠不确定性估计。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [154] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 开发进化预测框架，利用短期训练和进化推理实现长期预测，获得比传统DF更优性能且更稳定的结果。


<details>
  <summary>Details</summary>
Motivation: 传统的直接预测（DF）方法要求针对每个预测时长重新训练模型，导致计算成本高昂，并且在长周期内存在梯度冲突导致局部动态学习受阻。

Method: 提出进化预测（EF）框架，将DF视为其退化特例，使用短期训练并在推理阶段通过进化推理实现长期预测，从而缓解梯度冲突。

Result: 在多项标准基准上，单一EF模型优于针对不同时间跨度的DF模型集成，并在极端外推时表现出稳健的渐近稳定性。

Conclusion: EF实现了从被动静态映射向主动进化推理的范式转变，重新定义了长期时间序列预测的研究方向。

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [155] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [156] [Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures](https://arxiv.org/abs/2601.23147)
*Saeid Jamshidi,Omar Abdul Wahab,Rolando Herrero,Foutse Khomh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies.

</details>


### [157] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 提出了一种低成本、理论可信、灵活的分布修正策略，针对等式约束生成模型，理论与实验均证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 等式约束导致生成模型在科学应用中面临分布建模的数学限制，需要一种贴合流形几何且计算成本低的解决方案。

Method: 通过在约束感知的方式下对原始数据分布进行扰动，构建新分布，随后在扩散模型与归一化流中验证其有效性。

Result: 理论分析与实验表明，该方法在多种代表任务中均能实现数据分布恢复与稳定采样。

Conclusion: 本研究提出了一种针对等式约束生成模型的分布修正方法，能够在保持分布支撑维度的同时隐式融入流形几何，实现数据分布恢复与稳定采样。

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [158] [Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data](https://arxiv.org/abs/2601.23153)
*Eugenia Iofinova,Dan Alistarh*

Main category: cs.LG

TL;DR: Behemoth simulates training data for neural nets, letting researchers analyze model edits accurately. Experiments show that constraining update rank can enhance editing, matching real‑world patterns.


<details>
  <summary>Details</summary>
Motivation: Understanding how training data distribution and embedding storage affect model editing, which is difficult with real-world large language models due to complexity and lack of transparency.

Method: Synthetic generation of tabular data and controlled model editing experiments on artificial neural networks.

Result: Demonstrated that in some scenarios, limiting the update rank improves editing performance, and these findings align with real-world observations.

Conclusion: Behemoth provides a synthetic data generation framework that enables systematic study of model editing, revealing insights such as the counterintuitive effect of rank restriction on edit efficacy.

Abstract: As artificial neural networks, and specifically large language models, have improved rapidly in capabilities and quality, they have increasingly been deployed in real-world applications, from customer service to Google search, despite the fact that they frequently make factually incorrect or undesirable statements. This trend has inspired practical and academic interest in model editing, that is, in adjusting the weights of the model to modify its likely outputs for queries relating to a specific fact or set of facts. This may be done either to amend a fact or set of facts, for instance, to fix a frequent error in the training data, or to suppress a fact or set of facts entirely, for instance, in case of dangerous knowledge. Multiple methods have been proposed to do such edits. However, at the same time, it has been shown that such model editing can be brittle and incomplete. Moreover the effectiveness of any model editing method necessarily depends on the data on which the model is trained, and, therefore, a good understanding of the interaction of the training data distribution and the way it is stored in the network is necessary and helpful to reliably perform model editing. However, working with large language models trained on real-world data does not allow us to understand this relationship or fully measure the effects of model editing. We therefore propose Behemoth, a fully synthetic data generation framework. To demonstrate the practical insights from the framework, we explore model editing in the context of simple tabular data, demonstrating surprising findings that, in some cases, echo real-world results, for instance, that in some cases restricting the update rank results in a more effective update. The code is available at https://github.com/IST-DASLab/behemoth.git.

</details>


### [159] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 通过在ICU镇痛剂量决策中引入死亡率目标，强化学习可制定更安全、更有效的治疗方案。


<details>
  <summary>Details</summary>
Motivation: 传统医用强化学习研究在镇痛和镇静方面往往忽视患者生存价值，并使用不适合不完全信息环境的算法，导致潜在的安全隐患。

Method: 采用深度强化学习框架，在部分可观测环境下，利用MIMIC-IV数据库的47144例ICU停留数据，训练针对阿片类药物、丙泊酚、苯二氮卓类和dexmedetomidine的小时剂量策略，并设计两种目标：单独降低疼痛与同时降低疼痛及死亡率两种预算。

Result: 两种策略均能降低疼痛水平，但第一种仅关注疼痛的策略与死亡率呈正相关，而包含死亡率目标的策略与死亡率呈负相关。

Conclusion: 本研究表明，在重症监护疼痛管理中，将长期结果（如死亡率）纳入强化学习目标显著提高安全性，单纯追求短期疼痛缓解可能会增加死亡风险。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [160] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [161] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: LLM推理轨迹越长，准确率和决策承诺越强，核心因相关信息而非长度或风格。轨迹探测可用于优化模型部署与可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过“推理痕迹”解决复杂问题，但尚不清楚推理轨迹中准确性和决策承诺如何变化，且中间段落是否提供超越长度或风格的答案相关信息。

Method: ①生成模型的完整推理痕迹；②按固定 token 百分位截断；③将每个截断后的片段注入同一或不同模型，通过 next‑token 概率获取答案分布并评估。

Result: 准确率与决策承诺随提供的推理 token 比例提升；提升主要源于模型生成中的相关内容，而非单纯的上下文长度或通用风格；强模型能从错误片段回溯，但即时答案常被弱模型错误答案绑架。

Conclusion: 轨迹探测为推理模型的高效、安全部署提供诊断工具，测得的分布可指导轨迹处理与监控策略，提升可靠性而不需假设中间 token 本质上为可信解释。

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [162] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [163] [Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning](https://arxiv.org/abs/2601.23169)
*İlker Işık,Wenchao Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current neural architectures lack a principled way to handle interchangeable tokens, i.e., symbols that are semantically equivalent yet distinguishable, such as bound variables. As a result, models trained on fixed vocabularies often struggle to generalize to unseen symbols, even when the underlying semantics remain unchanged. We propose a novel Transformer-based mechanism that is provably invariant to the renaming of interchangeable tokens. Our approach employs parallel embedding streams to isolate the contribution of each interchangeable token in the input, combined with an aggregated attention mechanism that enables structured information sharing across streams. Experimental results confirm the theoretical guarantees of our method and demonstrate substantial performance gains on open-vocabulary tasks that require generalization to novel symbols.

</details>


### [164] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: DyCAST提供可变帧率的字符对齐音频编码，减少token数量，实现高效且质量不降的语音重建。


<details>
  <summary>Details</summary>
Motivation: 传统音频编码器固定帧率导致token序列冗余、效率低下；需要通过可变帧率精准对齐字符单位来降低token数量并提升下游性能。

Method: 利用软字符级对齐与明确时长建模进行动态token化，并在解码时通过检索增强机制提升低帧率下的重建效果。

Result: 实验表明DyCAST在单语音重建和下游任务上与固定帧率编码器相媲美，但token量显著降低，且低帧率下重构质量通过检索增强得到提振。

Conclusion: DyCAST通过动态帧率音频分割和检索增强解码，实现更少tokens且不降低重构质量，从而在保持话语生成质量的同时提升效率。

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [165] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: MGN-T 将 Transformer 与 MeshGraphNet 结合，解决了传统模型在大网格上的长距传播瓶颈，实现高效、精确的工业级网格学习。


<details>
  <summary>Details</summary>
Motivation: 标准 MeshGraphNet 由于在大尺寸、高分辨率网格上使用递归消息传递导致信息传播受限，无法有效建模长距离相互作用。

Method: 采用 physics-attention Transformer 作为全局处理器，在保持网格图表示的同时，对所有节点状态进行同步更新，并显式保留节点与边属性，从而直接捕捉长程物理交互。

Result: MGN-T 成功处理工业规模冲击动力学网格，准确建模自接触、塑性与多变量输出，并在经典基准上超越现有最优模型，显著提升精度并降低参数量。

Conclusion: MeshGraphNet-Transformer (MGN-T) 在集成 Transformers 的全局建模能力与 MeshGraphNets 的几何归纳偏差的基础上，克服了传统 MGN 在高分辨率网格上长距离信息传播效率低的问题。通过 physics-attention Transformer 的全局更新机制，MGN-T 能同时更新所有节点状态，显式保留节点与边属性，在工业规模网格上实现高效学习，并在冲击动力学任务中准确模拟自接触、塑性与多变量输出。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [166] [Ensuring Semantics in Weights of Implicit Neural Representations through the Implicit Function Theorem](https://arxiv.org/abs/2601.23181)
*Tianming Qiu,Christos Sonis,Hao Shen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Weight Space Learning (WSL), which frames neural network weights as a data modality, is an emerging field with potential for tasks like meta-learning or transfer learning. Particularly, Implicit Neural Representations (INRs) provide a convenient testbed, where each set of weights determines the corresponding individual data sample as a mapping from coordinates to contextual values. So far, a precise theoretical explanation for the mechanism of encoding semantics of data into network weights is still missing. In this work, we deploy the Implicit Function Theorem (IFT) to establish a rigorous mapping between the data space and its latent weight representation space. We analyze a framework that maps instance-specific embeddings to INR weights via a shared hypernetwork, achieving performance competitive with existing baselines on downstream classification tasks across 2D and 3D datasets. These findings offer a theoretical lens for future investigations into network weights.

</details>


### [167] [Learning to Execute Graph Algorithms Exactly with Graph Neural Networks](https://arxiv.org/abs/2601.23207)
*Muhammad Fetrat Qharabagh,Artur Back de Luca,George Giapitzakis,Kimon Fountoulakis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a two-step process. First, we train an ensemble of multi-layer perceptrons (MLPs) to execute the local instructions of a single node. Second, during inference, we use the trained MLP ensemble as the update function within a graph neural network (GNN). Leveraging Neural Tangent Kernel (NTK) theory, we show that local instructions can be learned from a small training set, enabling the complete graph algorithm to be executed during inference without error and with high probability. To illustrate the learning power of our setting, we establish a rigorous learnability result for the LOCAL model of distributed computation. We further demonstrate positive learnability results for widely studied algorithms such as message flooding, breadth-first and depth-first search, and Bellman-Ford.

</details>


### [168] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 通过环形化交通密度描述与偏最小二乘回归，构建了墨西哥城空气质量动态预测模型，效果良好且易推广。


<details>
  <summary>Details</summary>
Motivation: 随着空气污染对健康影响的认识加深，揭示交通强度与空气质量的关系对于提供超局部、动态预测具有重要意义。

Method: 首先将颜色编码的交通密集度地图转换为同心环的定量描述，然后使用偏最小二乘回归对这些环形描述进行建模，并通过多组训练样本进行优化以提升预测性能。

Result: 模型在墨西哥城的数据集上实现了较高的预测精度，并揭示了不同污染物与交通密度之间的关联，可迁移至其它城市。

Conclusion: 本研究通过将交通密集度转换为同心环形描述并使用偏最小二乘回归，建立了能实时预测空气质量的模型，可在墨西哥城及其他城市广泛应用。

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [169] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 研究分析了多数投票和贝叶斯聚合在\(\varepsilon\)-公平性框架下的公平性特性，给出多数投票上界并证明其指数收敛；将多分类后处理算法推广到离散域，实现严格人群平等约束；实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 获取可靠标注耗资高昂或不可行，众包与噪声聚合是常见方案，但聚合过程中个人偏见会被放大，尤其涉及敏感特征，导致公平性风险；当前对众包聚合公平性的理论研究极其稀少，缺乏收敛保证和有限的后处理方式。

Method: 1) 在\(\varepsilon\)-公平性框架下对Majority Vote和Optimal Bayesian Aggregation进行公平性分析；2) 推导多数投票的公平性上界，关联个体注释者的公平性差距；3) 在小规模众包情境下证明聚合公平性差距指数收敛；4) 将现有多分类公平性后处理算法从连续转为离散，强化严格人群平等约束；5) 在合成与真实数据集上实验验证。

Result: 在小众包规模下，推导出多数投票公平性差距上界，并证明其指数快速收敛至真实标签公平性差距；把多分类公平性后处理算法扩展到离散域，能够对任意聚合规则施加严格的人群平等约束；实验验证了上述方法的有效性，支持理论分析。

Conclusion: 本研究提出了在\(\varepsilon\)-公平性框架下对众包聚合方法（多数投票和贝叶斯最优聚合）的公平性分析，并给出了多数投票的公平性上界与个体标注者公平性差距相关的公式；同时证明了聚合结果的公平性差距在满足可解释条件时指数式收敛至真实标签的公平性差距。此外，作者将现有的多分类公平性后处理算法从连续域推广到离散域，实现了对任何聚合规则的严格人群平等约束。实验验证了理论与方法的有效性。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [170] [YuriiFormer: A Suite of Nesterov-Accelerated Transformers](https://arxiv.org/abs/2601.23236)
*Aleksandr Zimin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 用优化算法视角重新诠释transformer，提出加速版本，实验验证效果更佳。


<details>
  <summary>Details</summary>
Motivation: 阐明transformer层可以看作优化算法的迭代，实现对序列嵌入的能量最小化，从而为架构设计提供理论基础。

Method: 提出变分框架，将自注意力视为交互能量梯度步，MLP视为势能梯度更新；利用Lie–Trotter分裂得到标准GPT；在此基础上设计Nesterov加速变体。

Result: 该加速Transformer在TinyStories和OpenWebText上持续优于nanoGPT基准。

Conclusion: 优化理论视角能够指导变压器的有效架构改进，并带来实测性能提升。

Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.

</details>


### [171] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [172] [TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training](https://arxiv.org/abs/2601.23261)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Dongyang Li,Yupeng Su,Sijia Liu,Zheng Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization beyond individual layers by modeling the gradients of a neural network as a structured higher-order tensor. We present TEON's improved convergence guarantee over layer-wise Muon, and further develop a practical instantiation of TEON based on the theoretical analysis with corresponding ablation. We evaluate our approach on two widely adopted architectures: GPT-style models, ranging from 130M to 774M parameters, and LLaMA-style models, ranging from 60M to 1B parameters. Experimental results show that TEON consistently improves training and validation perplexity across model scales and exhibits strong robustness under various approximate SVD schemes.

</details>


### [173] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: FOCUS通过聚焦计算于可解码token，提升了Diffusion LLM的推理速度，最快可达3.5倍吞吐改进。


<details>
  <summary>Details</summary>
Motivation: DLLM在解码时仅有很小一部分token可解码，使得并行计算大部分时间被浪费在不可解码token上，导致计算成本高昂，限制了大规模部署。

Method: 设计了FOCUS推理系统，在DLLM解码过程中根据token重要性和解码概率动态调整计算资源，仅对当前可解码的token进行前向推理，将不可解码的token从批处理中释放。

Result: 在多个基准测试中，FOCUS相对于生产级引擎LMDeploy提升了多达3.52倍的推理吞吐量，并且生成质量得以保持或提升。

Conclusion: FOCUS通过动态聚焦可解码token的计算并实时驱逐不可解码token，显著降低了DLLM解码中的计算浪费，从而提升了推理吞吐量并保持或提升了生成质量。

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [174] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: DDIS通过解耦系数先验与PDE前向模型，结合DAPS提升了低样本逆PDE的精度，显著降低误差并避免过平滑。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散后验采样方法通过联合建模索引系数和解，隐式体现物理规律，需大量配对监督。

Method: 提出解耦扩散逆解器(DDIS)：无条件扩散学习系数先验；神经算子显式建模前向PDE以指导；并结合解耦退火后验采样(DAPS)避免扩散后验采样的过平滑。

Result: 理论证明在数据稀缺时DDIS避免联合模型的引导衰减失效；实验表明在稀疏观测下l2误差提升11%、谱误差提升54%；当数据仅1%时，l2误差保持较联合模型40%的优势。

Conclusion: DDIS在低样本、稀疏观测情境下提供更高效、精确的逆PDE求解方案，充分利用物理信息和解耦策略实现显著性能提升。

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [175] [Compressive Beam-Pattern-Aware Near-field Beam Training via Total Variation Denoising](https://arxiv.org/abs/2601.22243)
*Zijun Wang,Maria Nivetha A,Ye Hu,Rui Zhang*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Extremely large antenna arrays envisioned for 6G incurs near-field effect, where steering vector depends on angles and range simultaneously. Polar-domain near-field codebooks can focus energy accurately but incur extra two-dimensional sweeping overhead; compressed-sensing (CS) approaches with Gaussian-masked DFT sensing offer a lower-overhead alternative. This letter revisits near-field beam training using conventional DFT codebooks. Unlike far-field responses that concentrate energy on a few isolated DFT beams, near-field responses produce contiguous, plateau-like energy segments with sharp transitions in the DFT beamspace. Pure LASSO denoising, therefore, tends to over-shrink magnitudes and fragment plateaus. We propose a beam-pattern-preserving beam training scheme for multiple-path scenarios that combines LASSO with a lightweight denoising pipeline: LASSO to suppress small-amplitude noise, followed by total variation (TV) to maintain plateau levels and edge sharpness. The two proximal steps require no near-field codebook design. Simulations with Gaussian pilots show consistent NMSE and cosine-similarity gains over least squares and LASSO at the same pilot budget.

</details>


### [176] [Dual-Diode Unified SWIPT for High Data Rates with Adaptive Detection](https://arxiv.org/abs/2601.22270)
*Zulqarnain Bin Ashraf,Triantafyllos Mavrovoltsos,Constantinos Psomas,Ioannis Krikidis,Besma Smida*

Main category: eess.SP

TL;DR: 作者构建了考虑二极管非线性与电容记忆的U‑SWIPT双二极管模型，并开发了低复杂度的自适应检测器，证明整流器记忆能显著提高低功耗IoT接收机的性能。


<details>
  <summary>Details</summary>
Motivation: 低功耗、低复杂度的U‑SWIPT接收机适合IoT应用，现有模型忽略直流电容记忆，需更准确捕捉实际运行条件。

Method: 提出统一瞬态框架，联合考虑二极管非线性与电容诱导记忆；基于此记忆感知模型设计线性复杂度自适应检测器，学习非线性状态转移并采用决策导向检测。

Result: 该检测方案在记忆主导情形下可逼近MLSD性能，且无需传统序列检测的指数搜索，显著提升能量采集与信息解码的权衡。

Conclusion: 通过充分利用整流器记忆效应，能够在数据速率与可靠性之间达到更优平衡，提升统一双二极U‑SWIPT接收机在低功耗IoT环境下的整体性能。

Abstract: Due to their low-complexity and energy-efficiency, unified simultaneous wireless information and power transfer (U-SWIPT) receivers are especially suitable for low-power Internet of Things (IoT) applications. Towards accurately modeling practical operating conditions, in this study, we provide a unified transient framework for a dual-diode U-SWIPT that jointly accounts for diode nonlinearity and capacitor-induced memory effects. The proposed model accurately describes the inherent time dependence of the rectifier, highlighting its fundamental impact on both energy harvesting (EH) and information decoding (ID) processes. Based on the provided memory-aware model, we design a low-complexity adaptive detector that learns the nonlinear state transition dynamics and performs decision-directed detection with linear complexity. The proposed detection scheme approaches maximum likelihood sequence detection (MLSD) performance in memory-dominated regimes, while avoiding the exponential search required by classical sequence detection. Overall, these results demonstrate that properly exploiting rectifier memory provides a better tradeoff between data rate and reliability for U-SWIPT receivers.

</details>


### [177] [On the Optimality of Rate Balancing for Max-Min Fair Multicasting](https://arxiv.org/abs/2601.22415)
*Sadaf Syed,Wolfgang Utschick,Michael Joham*

Main category: eess.SP

TL;DR: 本文推导MMF多播最优解并指出其与速率平衡等价，基于此提出低复杂度闭式算法，实验验证优于现有方案，并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决多播中最大-最小公平性（MMF）问题的NP-hard性与计算复杂度

Method: 对MMF多播问题进行理论推导，证明在特定条件下速率平衡与MMF最优解等价，并基于此提出低复杂度闭式解算法

Result: 所提算法在仿真中表现出优于现有方法的性能，并具有更高的计算效率

Conclusion: 提供了MMF多播问题的最优解理论依据及实用高效算法，为相应应用提供了理论与方法双重贡献

Abstract: The max-min fair (MMF) multicasting problem is known to be NP-hard. In this work, we analytically derive the optimal solution to this NP-hard problem and establish the equivalence between rate balancing and the optimal MMF multicasting solution under certain conditions. Based on this theoretical insight, we propose a low-complexity algorithm for MMF multicasting that yields closed-form solutions. Simulation results validate our analysis and demonstrate that the proposed algorithm outperforms the state-of-the-art methods while being computationally more efficient.

</details>


### [178] [SORIS: A Self-Organized Reconfigurable Intelligent Surface Architecture for Wireless Communications](https://arxiv.org/abs/2601.22724)
*Evangelos Koutsonas,Alexandros-Apostolos A. Boulogeorgos,Stylianos E. Trevlakis,George C. Alexandropoulos,Theodoros A. Tsiftsis,Rui Zhang*

Main category: eess.SP

TL;DR: A RIS system using a microcontroller and selective transmitting elements for quick channel estimation, combined with a neural network to infer remaining channels, enabling self-configured communication without external links. Simulations confirm its efficacy.


<details>
  <summary>Details</summary>
Motivation: Address efficient channel state acquisition in RIS without external pilot links.

Method: Proposed self-organized RIS (SORIS) architecture with a microcontroller and single-antenna receiver. Uses a subset of RIS elements in transmission mode for initial channel estimation, followed by a low-complexity neural network exploiting spatial correlation to predict remaining channel coefficients.

Result: Demonstrated via Monte Carlo simulations that SORIS achieves accurate channel state information, enabling dynamic metamaterial reconfiguration for data transmission while maintaining low wiring density and control signaling overhead.

Conclusion: The self-organized RIS architecture effectively removes the need for separate channel estimation connections, offering feasible hardware configuration, reduced complexity, and practical guidelines for selecting transmitting elements.

Abstract: In this paper, a new reconfigurable intelligent surface (RIS) hardware architecture, called self-organized RIS (SORIS), is proposed. The architecture incorporates a microcontroller connected to a single-antenna receiver operating at the same frequency as the RIS unit elements, operating either in transmission or reflection mode. The transmitting RIS elements enable the low latency estimation of both the incoming and outcoming channels at the microcontroller's side. In addition, a machine learning approach for estimating the incoming and outcoming channels involving the remaining RIS elements operating in reflection mode is devised. Specifically, by appropriately selecting a small number of elements in transmission mode, and based on the channel reciprocity principle, the respective channel coefficients are first estimated, which are then fed to a low-complexity neural network that, leveraging spatial channel correlation over RIS elements, returns predictions of the channel coefficients referring to the rest of elements. In this way, the SORIS microcontroller acquires channel state information, and accordingly reconfigures the panel's metamaterials to assist data communication between a transmitter and a receiver, without the need for separate connections with them. Moreover, the impact of channel estimation on the proposed solution, and a detailed complexity analysis for the used model, as well as a wiring density and control signaling analysis, is performed. The feasibility and efficacy of the proposed self-organized RIS design and operation are verified by Monte Carlo simulations, providing useful guidelines on the selection of the RIS elements for operating in transmission mode for initial channel estimation.

</details>


### [179] [Interpolation Techniques for Fast Channel Estimation in Ray Tracing](https://arxiv.org/abs/2601.23119)
*Ruibin Chen,Jayadev Joy,Yaqi Hu,Mingsheng Yin,Marco Mezzavilla,Sundeep Rangan*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ray tracing is increasingly utilized in wireless system simulations to estimate channel paths. In large-scale simulations with complex environments, ray tracing at high resolution can be computationally demanding. To reduce the computation, this paper presents a novel method for conducting ray tracing at a coarse set of reference points and interpolating the channels at other locations. The key insight is to interpolate the images of reflected points. In addition to the computational savings, the method directly captures the spherical nature of each wavefront enabling fast and accurate computation of channels using line-of-sight MIMO and other wide aperture techniques. Through empirical validation and comparison with exhaustive ray tracing, we demonstrate the efficacy and practicality of our approach in achieving high-fidelity channel predictions with reduced computational resources.

</details>


### [180] [Bayesian Matrix Completion Under Geometric Constraints](https://arxiv.org/abs/2601.22765)
*Rohit Varma Chiluvuri,Santosh Nannuru*

Main category: eess.SP

TL;DR: 引入分层贝叶斯层次结构，直接对潜在点集设Prior，天然实现几何约束，采用M-H + Gibbs 采样推断，实验显示在稀疏嘈杂环境下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的秩约束优化和半正定规划在稀疏或噪声条件下难以满足几何约束，导致重建效果不佳；

Method: 通过在潜在点集上施加分层结构先验，天然嵌入几何约束，利用 Metropolis‑Hastings 在 Gibbs 采样器中对耦合的潜在点后验进行推断；

Result: 在合成数据稀疏场景下，本方法相较基准算法表现出更高的重建精度；

Conclusion: 本研究开发了一个分层贝叶斯框架，证明在稀疏和噪声观测下可以更准确地完成欧氏距离矩阵的恢复，并在实验中优于传统确定性方法；

Abstract: The completion of a Euclidean distance matrix (EDM) from sparse and noisy observations is a fundamental challenge in signal processing, with applications in sensor network localization, acoustic room reconstruction, molecular conformation, and manifold learning. Traditional approaches, such as rank-constrained optimization and semidefinite programming, enforce geometric constraints but often struggle under sparse or noisy conditions. This paper introduces a hierarchical Bayesian framework that places structured priors directly on the latent point set generating the EDM, naturally embedding geometric constraints. By incorporating a hierarchical prior on latent point set, the model enables automatic regularization and robust noise handling. Posterior inference is performed using a Metropolis-Hastings within Gibbs sampler to handle coupled latent point posterior. Experiments on synthetic data demonstrate improved reconstruction accuracy compared to deterministic baselines in sparse regimes.

</details>


### [181] [Intrinsic MIMO Particle Communication Channel with Random Advection](https://arxiv.org/abs/2601.22915)
*Fatih Merdan,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 对流保持脉冲完整，空间分布的接收器加权合并能在低信噪比下提升分子通信的检测性能。


<details>
  <summary>Details</summary>
Motivation: 拯救仅靠扩散的分子通信在实际流体环境中易失调的脉冲信息以及可靠性不足的问题；通过利用对流效应，可实现更复杂的调制及提升系统鲁棒性，为后续多分子MIMO等技术奠定基础。

Method: 在单一发射器、单一信息分子类型下，对对流支配的扩散-对流信道进行理论建模，分析对流对脉冲的保留特性；随后提出多种接收器合并策略（例如最大比合并、基于信噪比加权等），并通过仿真评估其在不同信噪比下的误检率。

Result: 实验结果表明，多接收器多样性合并在低至中等信噪比区间内可显著降低误检率，且相较单接收器操作具有明显优势；对流实现的脉冲保留为基于脉冲的调制提供了理论支持。

Conclusion: 本文表明，流体流动在扩散-对流通道中能保持信号脉冲的时序与形状，为基于脉冲的以及更高阶的调制提供了可能；通过在空间上分布多接收器并采用多样性合并，可在低至中等信噪比环境中显著提升检测性能，验证了多接收器在分子通信中的优势。

Abstract: In this work, receiver diversity in advection-dominated diffusion-advection channels is investigated. Strong directed flow fundamentally alters the communication-theoretic properties of molecular communication systems (MC). Specifically, advection preserves the temporal ordering and shape of transmitted pulses, enabling pulse-based and higher-order modulation schemes that are typically infeasible in purely diffusive environments. Focusing on a single transmitter and a single type of information molecule, it is demonstrated that spatially distributed receivers can observe distinct realizations of the same transmitted signal, giving rise to diversity gain. Several receiver combining strategies are evaluated and shown to improve detection performance compared to single-receiver operation, particularly in low-to-moderate signal-to-noise ratio (SNR) regimes. The results provide a structured framework for understanding receiver-side diversity in molecular communication, highlighting the role of advection as a key enabler for reliable pulse-based signaling. This perspective establishes a foundation for future studies on advanced modulation, joint equalization and detection, and multi-molecule MIMO extensions that can further enhance the performance and physical applicability of MC systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [182] [Toward Non-Expert Customized Congestion Control](https://arxiv.org/abs/2601.22461)
*Mingrui Zhang,Hamid Bagheri,Lisong Xu*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: General-purpose congestion control algorithms (CCAs) are designed to achieve general congestion control goals, but they may not meet the specific requirements of certain users. Customized CCAs can meet certain users' specific requirements; however, non-expert users often lack the expertise to implement them. In this paper, we present an exploratory non-expert customized CCA framework, named NECC, which enables non-expert users to easily model, implement, and deploy their customized CCAs by leveraging Large Language Models and the Berkeley Packet Filter (BPF) interface. To the best of our knowledge, we are the first to address the customized CCA implementation problem. Our evaluations using real-world CCAs show that the performance of NECC is very promising, and we discuss the insights that we find and possible future research directions.

</details>


### [183] [Chance-Constrained Secrecy Optimization in Hybrid RIS-Empowered and UAV-Assisted Networks](https://arxiv.org/abs/2601.22499)
*Elhadj Moustapha Diallo,Mamadou Aliou Diallo,Abusaeed B. M. Adam,Muhammad Naeem Shah*

Main category: cs.NI

TL;DR: 采用UAV+STAR+H-RIS混合可重构环境，并通过Bernstein约束和SCA交替优化，可显著提升下行安全性并在多变环境中保持鲁棒。


<details>
  <summary>Details</summary>
Motivation: 为了解决在用户移动、动态阻塞以及协同内外部窃听者的影响下，室内外安全下行通信中服从统计CSI不确定性和硬件失真约束的复杂系统设计需求，提出利用多平台RIS协同应用的混合环境。

Method: 基于3GPP/ITU的随机通道模型，采用Bernstein型确定性逼近将机会约束转化为分布鲁棒凸约束；随后构建交替优化框架，利用SCA将每个子问题凸化，交替求解基站天线波束、不同表面相位系数及无人机位置，直至收敛到稳态点。

Result: 仿真结果（以3GPP TR 38.901、TR 36.873及ITU-R P.2109为基准）表明，UAV-RIS+STAR-RIS+H-RIS的组合显著降低了保密性失效概率，并对通道不确定性、阻塞、协同窃听和硬件失真表现出强鲁棒性。

Conclusion: 结合UAV-mounted RIS、空中STAR-RIS和室内全息RIS的混合可重构环境显著降低了保密性失效概率，并在多用户动态移动、阻塞、协同窃听及硬件失真等复杂环境中展现出卓越的鲁棒性能。

Abstract: This paper considers a hybrid reconfigurable environment comprising a UAV-mounted reflecting RIS, an outdoor STAR-RIS enabling simultaneous transmission and reflection, and an indoor holographic RIS (H-RIS), jointly enhancing secure downlink communication for indoor and outdoor users. The system operates under user mobility, dynamic blockages, colluding idle and active eavesdroppers, and transceiver and surface hardware impairments. A 3GPP and ITU-compliant stochastic channel model is developed, capturing mobility-induced covariance evolution, outdoor-indoor penetration losses, and distortion-aware noise due to practical EVM-based impairments. We aim to minimize the aggregate secrecy-outage probability subject to secrecy-rate constraints, QoS requirements, power limitations, and statistical CSI uncertainty. The resulting problem contains coupled secrecy and QoS chance constraints and nonlinear interactions among the BS beamforming vectors, multi-surface phase coefficients, and UAV position. To handle these difficulties, we derive rigorous Bernstein-type deterministic approximations for all chance constraints, yielding a distributionally robust reformulation. Building on this, we propose an alternating optimization framework that employs successive convex approximation (SCA) to convexify each block and solve the BS beamforming, RIS, STAR-RIS, H-RIS configuration, and UAV placement subproblems efficiently. The proposed algorithm is shown to monotonically decrease a smooth surrogate of the secrecy-outage cost and converge to a stationary point of the robustified problem. Simulations based on 3GPP TR 38.901, TR 36.873, and ITU-R P.2109 demonstrate that integrating UAV-RIS, STAR-RIS, and H-RIS significantly reduces secrecy-outage probability compared with benchmark schemes and provides strong robustness to channel uncertainty, blockages, colluding eavesdroppers, and hardware impairments.

</details>


### [184] [Digital Twin Synchronization: towards a data-centric architecture](https://arxiv.org/abs/2601.23051)
*Eduardo Freitas,Assis T. de Oliveira Filho,Pedro R. X. do Carmo,Djamel Sadok,Judith Kelner*

Main category: cs.NI

TL;DR: 本文评估现有DT同步方案，发现仍然存在延迟与安全等难题，随后提出一种可实现多行业统一部署、兼顾安全与互操作性的同步架构。


<details>
  <summary>Details</summary>
Motivation: 在工业4.0背景下，数字孪生（DT）技术为工业流程带来巨大变革，但同步与真实物理实体的精确映射仍面临挑战。

Method: 本文对现行同步技术与体系结构进行综述，识别关键技术难点，并设计了一套统一的同步架构，兼顾安全与互操作性需求。

Result: 提出的统一同步架构能够在多种工业应用场景中实现实时、可信的数字孪生同步，并为后续系统升级提供可扩展的基础。

Conclusion: 该研究填补了数字孪生同步的技术空白，推动行业向标准化、持续改进的方向发展。

Abstract: Digital Twin (DT) technology revolutionizes industrial processes by enabling the representation of physical entities and their dynamics to enhance productivity and operational efficiency. It has emerged as a vital enabling technology in the Industry 4.0 context. The present article examines the particular issue of synchronizing a digital twin while ensuring an accurate reflection of its physical counterpart. Despite the reported recent advances in the design of middleware and low delay communication technologies, effective synchronization between both worlds remains challenging. This paper reviews currently adopted synchronization technologies and architectures, identifies vital outstanding technical challenges, and proposes a unified synchronization architecture for use by various industrial applications while addressing security and interoperability requirements. As such, this study aims to bridges gaps and advance robust synchronization in DT environments, emphasizing the need for a standardized architecture to ensure seamless operation and continuous improvement of industrial systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [185] [Modeling of Non-linear Dynamics of Lithium-ion Batteries via Delay-Embedded Dynamic Mode Decomposition](https://arxiv.org/abs/2601.22403)
*Khalid Mahmud Labib,Shabbir Ahmed*

Main category: eess.SY

TL;DR: 使用HPPC实验获得的电压电流数据，构建时间延迟嵌入状态矩阵，训练DMD和DMDc数据驱动模型。通过最小化RSS确定最佳嵌入维度（DMDc1810，输入6）。模型在健康状态下得到的系统矩阵可用于老化电池，仅需更新控制输入即可预测不同老化水平下的终端电压，准确捕捉非线性转瞬态动力学，且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池在实际使用中呈现复杂非线性行为，传统基于电化学方程的模型计算成本高且需要详细材料信息。开发一种仅依赖可获得的电压电流数据、计算效率高且可捕获非线性动力学的数据驱动模型，以实现更好的管理与控制。

Method: 基于电压和电流HPPC实验数据构建时间延迟嵌入状态矩阵，以多维度嵌入（40–2000）生成数据快照；利用部分实验数据对DMD和DMDc模型进行训练，并通过最小化残差平方和（RSS）确定最佳延迟维度；采用DMDc时，在输入电流上也加入延迟嵌入（1–12），进一步减小RSS。

Result: 最优embedding维度为1810的DMDc模型在训练集上RSS为3.86；对应的标准DMD模型RSS为30。再进一步，在DMDc输入矩阵中embedding=6时，RSS降至1.74。模型在健康状态下识别的系统矩阵A、B在老化细胞中保持不变，仅更新控制输入即可模拟老化过程，并且能够捕捉电压下降、瞬态响应等关键动态。

Conclusion: 该研究证明动态模态分解（DMD）及其带控制的变体DMDc能够在仅使用电压电流数据的基础上，准确且高效地捕捉锂电池在不同SoC及老化状态下的非线性动力学，并且仅通过更新输入控制即可模拟老化过程，保持模型稳健性。

Abstract: The complex electrochemical behavior of lithium-ion batteries results in non-linear dynamics and appropriate modeling of this non-linear dynamical system is of interest for better management and control. In this work, we proposed a family of dynamic mode decomposition (DMD)-based data-driven models that do not require detailed knowledge of the composition of the battery materials but can essentially capture the non-linear dynamics with higher computational efficiency. Only voltage and current data obtained from hybrid pulse power characterization (HPPC) tests were utilized to form the state space matrices and subsequently used for predicting the future terminal voltage at different state of charge (SoC) and aging levels. To construct the system model, 60\% of the data from a single HPPC test was utilized to generate time-delay embedded snapshots, with embedding dimension ranging from 40 to 2000. Among these, an embedding dimension of 1810 resulted in the least residual sum of squares (RSS) error of 3.86 for the dynamic mode decomposition with control (DMDc) model and 30 for the standard DMD model. For DMDc model, delay embeddings (ranging from 1 to 12) were also incorporated into the input current signals. For the input matrix, an embedding dimension of 6 resulted in a minimum RSS error of 1.74. Furthermore, the system matrices A and B, identified from the HPPC test when the cell is in its healthy state, were held fixed and used to simulate the system dynamics for aged batteries by updating only the control input. Despite the presence of nonlinear degradation effects in later cycles, the DMDc model effectively captured key inner dynamics such as voltage dips and transient responses for subsequent charge and discharge cycles.

</details>


### [186] [Robust Control of Constrained Linear Systems using Online Convex Optimization and a Reference Governor](https://arxiv.org/abs/2601.23160)
*Marko Nonhoff,Mohammad Taher Al Torshan,Matthias A. Müller*

Main category: eess.SY

TL;DR: 用于线性时不变系统的控制器设计，结合在线凸优化、参考统御与约束收缩，保证约束满足与鲁棒性；闭环动态 regret 与成本变化率、扰动大小呈线性关系，数值案例验证性能。


<details>
  <summary>Details</summary>
Motivation: 在控制线性时不变系统时，成本函数往往随时间变化且未知，传统方法难以同时满足约束、保持鲁棒性并提供性能评价。

Method: 将在线凸优化框架、参考统御器（reference governor）及约束收缩（constraint tightening）技术相结合，以构建兼顾约束满足与鲁棒性的控制器。

Result: 闭环动态 regret 被证明被成本函数变化率和扰动大小线性上界；通过数值案例验证了该方法在跟踪控制任务中的有效性。

Conclusion: 本研究提出了一种适用于线性时不变系统的控制方法，能够处理时变且先验未知的成本函数，满足状态与输入约束，并对外部干扰具有鲁棒性；该方法通过递归可行性保证实现约束满足，而闭环性能则通过动态 regret 进行评估。

Abstract: This article develops a control method for linear time-invariant systems subject to time-varying and a priori unknown cost functions, that satisfies state and input constraints, and is robust to exogenous disturbances. To this end, we combine the online convex optimization framework with a reference governor and a constraint tightening approach. The proposed framework guarantees recursive feasibility and robust constraint satisfaction. Its closed-loop performance is studied in terms of its dynamic regret, which is bounded linearly by the variation of the cost functions and the magnitude of the disturbances. The proposed method is illustrated by a numerical case study of a tracking control problem.

</details>
