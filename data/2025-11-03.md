<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 55]
- [cs.IT](#cs.IT) [Total: 6]
- [eess.SY](#eess.SY) [Total: 10]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.CR](#cs.CR) [Total: 8]
- [eess.SP](#eess.SP) [Total: 11]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning](https://arxiv.org/abs/2510.26829)
*Svetlana Churina,Niranjan Chebrolu,Kokil Jaidka*

Main category: cs.LG

TL;DR: 在持续预训练阶段对LLM注入少量带有误导性的但自信陈述的数据，可能导致模型内部对事实的信念发生持久的偏移；Layer of Truth 框架可量化不同层和模型规模的信念漂移。


<details>
  <summary>Details</summary>
Motivation: 将人类的illusory truth效应概念引入LLMs，研究持续更新过程中的误信息污染风险，填补静态预训练数据污染研究在持续学习场景的空缺。

Method: 构建 Layer of Truth 框架与数据集，注入可控比例的 poisoned 数据，在不同检查点、不同模型规模、以及不同问题类型下对中间表示进行探测，量化事实信念的变化。

Result: 即使极少量暴露也能引起持久的表征偏离，对事实的信念产生漂移；对不同层和模型规模的易感性存在差异。

Conclusion: 凸显持续更新的LLMs 的潜在脆弱性，即可能将错误信息内化，强调在模型更新过程中对事实完整性的监控与防护需求。

Abstract: Large language models (LLMs) continually evolve through pre-training on
ever-expanding web data, but this adaptive process also exposes them to subtle
forms of misinformation. While prior work has explored data poisoning during
static pre-training, the effects of such manipulations under continual
pre-training remain largely unexplored. Drawing inspiration from the illusory
truth effect in human cognition - where repeated exposure to falsehoods
increases belief in their accuracy - we ask whether LLMs exhibit a similar
vulnerability. We investigate whether repeated exposure to false but
confidently stated facts can shift a model's internal representation away from
the truth.
  We introduce Layer of Truth, a framework and dataset for probing belief
dynamics in continually trained LLMs. By injecting controlled amounts of
poisoned data and probing intermediate representations across checkpoints,
model scales, and question types, we quantify when and how factual beliefs
shift. Our findings reveal that even minimal exposure can induce persistent
representational drift in well-established facts, with susceptibility varying
across layers and model sizes. These results highlight an overlooked
vulnerability of continually updated LLMs: their capacity to internalize
misinformation analogously to humans, underscoring the need for robust
monitoring of factual integrity during model updates.

</details>


### [2] [Towards a Measure of Algorithm Similarity](https://arxiv.org/abs/2510.27063)
*Shairoz Sohail,Taher Ali*

Main category: cs.LG

TL;DR: EMOC是一种将算法实现嵌入到特征空间的框架，用于衡量算法相似性与差异性，并在PACD数据集上验证其有效性，支持聚类、分类、近重复检测以及对LLM生成程序的多样性量化，且提供代码和数据以促进复现。


<details>
  <summary>Details</summary>
Motivation: 在通用层面上判定两算法是否本质不同是不可计算的，实用上又存在多种相似性定义的矛盾。因此需要一个务实、可重复的相似性度量来支持应用场景如克隆检测和程序合成。

Method: 提出Evaluation-Memory-Operations-Complexity（EMOC）框架，将算法实现映射到可用于下游任务的特征空间；构建PACD数据集，收集在三类问题上的经验证的Python实现；利用EMOC特征进行下游任务的聚类、分类、近重复检测及对LLM生成程序多样性的量化。

Result: EMOC特征能够支持对算法类型的聚类与分类、近重复检测，以及对LLM生成程序的多样性进行量化；提供了代码、数据和计算EMOC嵌入的工具以促进复现与未来工作。

Conclusion: EMOC嵌入在评估算法相似性方面具有可行性和有效性，能够帮助研究者和开发者在实际应用中对算法进行更一致的比较与分析，并推动进一步的研究与数据集建设。

Abstract: Given two algorithms for the same problem, can we determine whether they are
meaningfully different? In full generality, the question is uncomputable, and
empirically it is muddied by competing notions of similarity. Yet, in many
applications (such as clone detection or program synthesis) a pragmatic and
consistent similarity metric is necessary. We review existing equivalence and
similarity notions and introduce EMOC: An
Evaluation-Memory-Operations-Complexity framework that embeds algorithm
implementations into a feature space suitable for downstream tasks. We compile
PACD, a curated dataset of verified Python implementations across three
problems, and show that EMOC features support clustering and classification of
algorithm types, detection of near-duplicates, and quantification of diversity
in LLM-generated programs. Code, data, and utilities for computing EMOC
embeddings are released to facilitate reproducibility and future work on
algorithm similarity.

</details>


### [3] [SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation and Clustering Aggregation](https://arxiv.org/abs/2510.26830)
*Guangzhi Su,Shuchang Huang,Yutong Ke,Zhuohang Liu,Long Qian,Kaizhu Huang*

Main category: cs.LG

TL;DR: 提出SmoothGuard，一种轻量且模型无关的防御框架，通过对连续模态注入高斯噪声、生成多组候选输出并进行嵌入聚类筛选，最终从多数簇中选取答案，以提升多模态大语言模型的对抗鲁棒性，同时尽量保持效用。


<details>
  <summary>Details</summary>
Motivation: MLLM在对抗扰动下易产生错答，亟需高效、通用的防御机制来提升安全性和鲁棒性。

Method: 对图像等连续模态进行高斯噪声扰动，生成多组候选输出；将输出嵌入到统一向量空间并进行聚类，筛选被对抗影响较小的预测；最终从多数簇中选取答案。

Result: 在POPE、LLaVA-Bench(In-the-Wild)、MM-SafetyBench上证明SmoothGuard提升对抗鲁棒性，同时保持竞争力的任务效用。中短期ablation表明噪声范围0.1-0.2具最佳平衡效果。

Conclusion: SmoothGuard提供一种轻量、模型无关的防御方案，兼顾鲁棒性与实用性，适用于多模态大语言模型在潜在对抗环境中的部署。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across diverse tasks by jointly reasoning over textual and visual inputs.
Despite their success, these models remain highly vulnerable to adversarial
manipulations, raising concerns about their safety and reliability in
deployment. In this work, we first generalize an approach for generating
adversarial images within the HuggingFace ecosystem and then introduce
SmoothGuard, a lightweight and model-agnostic defense framework that enhances
the robustness of MLLMs through randomized noise injection and clustering-based
prediction aggregation. Our method perturbs continuous modalities (e.g., images
and audio) with Gaussian noise, generates multiple candidate outputs, and
applies embedding-based clustering to filter out adversarially influenced
predictions. The final answer is selected from the majority cluster, ensuring
stable responses even under malicious perturbations. Extensive experiments on
POPE, LLaVA-Bench (In-the-Wild), and MM-SafetyBench demonstrate that
SmoothGuard improves resilience to adversarial attacks while maintaining
competitive utility. Ablation studies further identify an optimal noise range
(0.1-0.2) that balances robustness and utility.

</details>


### [4] [Accurate Target Privacy Preserving Federated Learning Balancing Fairness and Utility](https://arxiv.org/abs/2510.26841)
*Kangkang Sun,Jun Wu,Minyi Guo,Jianhua Li,Jianwei Huang*

Main category: cs.LG

TL;DR: 提出 FedPF，通过将公平与隐私约束转化为零和博弈，揭示隐私-公平冲突并呈现非单调的公平-泛化关系，实验在多个数据集显著降低歧视且保持竞争性准确率，强调两者需权衡的不可避免性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景下，个人隐私保护与对多样群体的公平性需求可能相互矛盾，如何在不共享原始数据的前提下同时实现高效、公平且隐私保护强的模型，是一个关键且具挑战性的研究问题。

Method: 提出差分隐私的公平联邦学习算法 FedPF，将多目标优化问题转化为一个零和博弈框架，在模型效用、隐私保护与公平性约束之间进行对抗性权衡；理论分析揭示隐私约束越严格越削弱对 demographic biases 的检测与纠正能力，导致隐私-公平之间的固有张力；在中等程度的公平约束下，模型泛化在初期可能提升后又下降，呈现非单调的公平-效用关系；通过实验证明在三个数据集上可实现最高约 42.9% 的歧视降低且保持竞争性准确率，并提供公开代码。

Result: 理论上揭示隐私保护与公平性之间的固有张力，以及公平约束对泛化的非单调影响；实证方面实现显著的歧视降低（最高约 42.9%）且保持准确性，源代码公开。

Conclusion: 要同时实现隐私保护与公平性需进行精心的权衡和折衷，而不是单纯优化任一目标；隐私-公平张力在联邦学习场景中具有较强的普遍性，未来工作应探索自适应权衡策略、在更广数据集上的鲁棒性与扩展性，以及对不同公平定义和隐私预算的敏感性分析。

Abstract: Federated Learning (FL) enables collaborative model training without data
sharing, yet participants face a fundamental challenge, e.g., simultaneously
ensuring fairness across demographic groups while protecting sensitive client
data. We introduce a differentially private fair FL algorithm (\textit{FedPF})
that transforms this multi-objective optimization into a zero-sum game where
fairness and privacy constraints compete against model utility. Our theoretical
analysis reveals a surprising inverse relationship, i.e., stricter privacy
protection fundamentally limits the system's ability to detect and correct
demographic biases, creating an inherent tension between privacy and fairness.
Counterintuitively, we prove that moderate fairness constraints initially
improve model generalization before causing performance degradation, where a
non-monotonic relationship that challenges conventional wisdom about
fairness-utility tradeoffs. Experimental validation demonstrates up to 42.9 %
discrimination reduction across three datasets while maintaining competitive
accuracy, but more importantly, reveals that the privacy-fairness tension is
unavoidable, i.e., achieving both objectives simultaneously requires carefully
balanced compromises rather than optimization of either in isolation. The
source code for our proposed algorithm is publicly accessible at
https://github.com/szpsunkk/FedPF.

</details>


### [5] [CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs](https://arxiv.org/abs/2510.26843)
*Zhiyuan Ning,Jiawei Shao,Ruge Xu,Xinfei Guo,Jun Zhang,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 提出级联自我 Speculative Decoding 的 CAS-Spec 框架，通过动态开关加速策略 DSIA（层稀疏化与激活量化）构建草拟模型，并引入动态树级联 DyTC 来自适应路由多层草拟模型与草拟长度，显著提高自我推断的加速比。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的无损推断场景下，提升自我推测解码的速度，降低训练成本，弥补对高效 cascading 的需求。

Method: 提出 CAS-Spec，结合 DSIA（层稀疏化、激活量化）等推理加速技术，构建可切换的草拟模型；同时提出 Dynamic Tree Cascade（DyTC）算法，基于接受率与延迟预测自适应路由与分配草拟长度，改进传统的垂直/水平级联策略。

Result: 在多种LLM与数据集上，与现有自我推测解码方法相比实现平均 1.1x-2.3x 的速度提升；DyTC 相较级联基线与树基线的平均提升分别达到约 47% 与 48%。

Conclusion: CAS-Spec 易于嵌入现有大模型，具备进一步提升潜力，随着自我推测解码技术的发展将带来更高的加速空间。

Abstract: Speculative decoding has become a widely adopted as an effective technique
for lossless inference acceleration when deploying large language models
(LLMs). While on-the-fly self-speculative methods offer seamless integration
and broad utility, they often fall short of the speed gains achieved by methods
relying on specialized training. Cascading a hierarchy of draft models promises
further acceleration and flexibility, but the high cost of training multiple
models has limited its practical application. In this paper, we propose a novel
Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs
speculative draft models by leveraging dynamically switchable inference
acceleration (DSIA) strategies, including layer sparsity and activation
quantization. Furthermore, traditional vertical and horizontal cascade
algorithms are inefficient when applied to self-speculative decoding methods.
We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the
multi-level draft models and assigns the draft lengths, based on the heuristics
of acceptance rates and latency prediction. Our CAS-Spec method achieves
state-of-the-art acceleration compared to existing on-the-fly speculative
decoding methods, with an average speedup from $1.1\times$ to $2.3\times$ over
autoregressive decoding across various LLMs and datasets. DyTC improves the
average speedup by $47$\% and $48$\% over cascade-based baseline and tree-based
baseline algorithms, respectively. CAS-Spec can be easily integrated into most
existing LLMs and holds promising potential for further acceleration as
self-speculative decoding techniques continue to evolve.

</details>


### [6] [BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs](https://arxiv.org/abs/2510.26892)
*Mahsa Valizadeh,Rui Tuo,James Caverlee*

Main category: cs.LG

TL;DR: BI-DCGAN: a Bayesian extension of DCGAN that uses Bayes by Backprop and mean-field variational inference to model weight uncertainty, improving sample diversity and robustness with a covariance-based theoretical justification, while maintaining training efficiency.


<details>
  <summary>Details</summary>
Motivation: GANs suffer from mode collapse and lack of diversity and uncertainty awareness. A Bayesian treatment of GAN weights aims to quantify uncertainty and encourage diverse outputs, addressing limitations of standard DCGANs, while remaining computationally efficient compared to heavier methods like diffusion models.

Method: Introduce Bayes by Backprop to learn a distribution over DCGAN weights and apply mean-field variational inference for efficient posterior approximation during training. Prove, via covariance matrix analysis, that Bayesian modeling enhances diversity in GAN samples. Validate with experiments on standard benchmarks showing improved diversity and robustness without sacrificing training efficiency.

Result: BI-DCGAN yields more diverse and robust samples than DCGAN, with maintained training efficiency and scalable training. Empirical results on standard generative benchmarks support the theoretical covariance-based claim that Bayesian modeling enhances diversity.

Conclusion: BI-DCGAN is a scalable and timely solution for applications needing both diversity and uncertainty awareness, and it offers a more resource-efficient alternative to diffusion models while improving GAN performance.

Abstract: Generative Adversarial Networks (GANs) are proficient at generating synthetic
data but continue to suffer from mode collapse, where the generator produces a
narrow range of outputs that fool the discriminator but fail to capture the
full data distribution. This limitation is particularly problematic, as
generative models are increasingly deployed in real-world applications that
demand both diversity and uncertainty awareness. In response, we introduce
BI-DCGAN, a Bayesian extension of DCGAN that incorporates model uncertainty
into the generative process while maintaining computational efficiency.
BI-DCGAN integrates Bayes by Backprop to learn a distribution over network
weights and employs mean-field variational inference to efficiently approximate
the posterior distribution during GAN training. We establishes the first
theoretical proof, based on covariance matrix analysis, that Bayesian modeling
enhances sample diversity in GANs. We validate this theoretical result through
extensive experiments on standard generative benchmarks, demonstrating that
BI-DCGAN produces more diverse and robust outputs than conventional DCGANs,
while maintaining training efficiency. These findings position BI-DCGAN as a
scalable and timely solution for applications where both diversity and
uncertainty are critical, and where modern alternatives like diffusion models
remain too resource-intensive.

</details>


### [7] [Functional embeddings enable Aggregation of multi-area SEEG recordings over subjects and sessions](https://arxiv.org/abs/2510.27090)
*Sina Javadzadeh,Rahil Soroushmojdehi,S. Alireza Seyyed Mousavi,Mehrnaz Asadi,Sumiko Abe,Terence D. Sanger*

Main category: cs.LG

TL;DR: 提出一种跨受试者的颅内记录表征框架：通过对抗性学习得到电极的主体无关功能嵌入，并用 Transformer 在跨区域层级建模以实现零-shot 通道转移与掩码重建。


<details>
  <summary>Details</summary>
Motivation: 跨受试者的电极数量、放置位置和覆盖区域差异显著，传统的 MNI 等解剖归一化往往无法捕捉真正的功能相似性，且个体间虽在解剖坐标附近，神经动力学差异显著，因此需要一个无主体特异头、可扩展的功能表示和跨区域关系建模方法。

Method: (i) 使用 Siamese 编码器对多区域局部场电位学习主体无关的电极功能身份，通过对比学习塑形嵌入使其对区域特征具有局部敏感性；(ii) 将这些嵌入 token 化，输入 Transformer，处理可变通道数的跨区域关系，且无需主体特异的监督头。

Result: 在包含 20 名受试者的 basal ganglia–thalamic 数据集上，学习的功能空间在同一受试者内实现高鉴别性并形成区域一致的聚类，且具备对未见通道的零-shot 转移能力；Transformer 在功能 token 上捕获跨区域依赖并实现被掩盖通道的重建。

Conclusion: 为在缺乏统一任务结构和传感器放置的情形下，实现跨受试者大规模聚合与自监督/预训练提供了可扩展的功能骨架，并推动 intracranial 神经数据的跨区域、跨个体分析。

Abstract: Aggregating intracranial recordings across subjects is challenging since
electrode count, placement, and covered regions vary widely. Spatial
normalization methods like MNI coordinates offer a shared anatomical reference,
but often fail to capture true functional similarity, particularly when
localization is imprecise; even at matched anatomical coordinates, the targeted
brain region and underlying neural dynamics can differ substantially between
individuals. We propose a scalable representation-learning framework that (i)
learns a subject-agnostic functional identity for each electrode from
multi-region local field potentials using a Siamese encoder with contrastive
objectives, inducing an embedding geometry that is locality-sensitive to
region-specific neural signatures, and (ii) tokenizes these embeddings for a
transformer that models inter-regional relationships with a variable number of
channels. We evaluate this framework on a 20-subject dataset spanning basal
ganglia-thalamic regions collected during flexible rest/movement recording
sessions with heterogeneous electrode layouts. The learned functional space
supports accurate within-subject discrimination and forms clear,
region-consistent clusters; it transfers zero-shot to unseen channels. The
transformer, operating on functional tokens without subject-specific heads or
supervision, captures cross-region dependencies and enables reconstruction of
masked channels, providing a subject-agnostic backbone for downstream decoding.
Together, these results indicate a path toward large-scale, cross-subject
aggregation and pretraining for intracranial neural data where strict task
structure and uniform sensor placement are unavailable.

</details>


### [8] [Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering](https://arxiv.org/abs/2510.26898)
*Crystal Su,Kuai Yu,Jingrui Zhang,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 将本体论与大语言模型相结合，提出面向化学工程的可解释、可审计框架。通过与COPE本体对齐的管线、模板化问答、受控解码和引用门控实现 grounding，并以语言质量与本体准确性双重评估。


<details>
  <summary>Details</summary>
Motivation: 在需要高信赖度与领域知识一致性的安全关键工程任务中，单纯神经生成易产生幻觉和不一致输出，因此需要显式的符号化知识结构来提升可解释性与可审计性。

Method: 提出数据获取—语义预处理—信息提取—本体映射的管线，与COPE本体对齐；生成模板化问答以引导微调；引入控制解码与引用门约束输出在本体相关术语内；提出评估指标以同时评估语言质量和本体准确性；讨论语义检索与迭代验证等未来扩展。

Result: 给出一个可 grounding 的框架与评估体系，能够在过程控制、安全分析等关键工程场景实现更具解释性和可审计性的LLM应用；尚未给出具体的实验数据。

Conclusion: 符号结构与神经生成的耦合为高风险工程领域的LLM应用提供透明、可审计的途径，并为后续的语义检索与迭代验证等扩展奠定基础。

Abstract: This work presents an ontology-integrated large language model (LLM)
framework for chemical engineering that unites structured domain knowledge with
generative reasoning. The proposed pipeline aligns model training and inference
with the COPE ontology through a sequence of data acquisition, semantic
preprocessing, information extraction, and ontology mapping steps, producing
templated question-answer pairs that guide fine-tuning. A control-focused
decoding stage and citation gate enforce syntactic and factual grounding by
constraining outputs to ontology-linked terms, while evaluation metrics
quantify both linguistic quality and ontological accuracy. Feedback and future
extensions, including semantic retrieval and iterative validation, further
enhance the system's interpretability and reliability. This integration of
symbolic structure and neural generation provides a transparent, auditable
approach for applying LLMs to process control, safety analysis, and other
critical engineering contexts.

</details>


### [9] [Discovering EV Charging Site Archetypes Through Few Shot Forecasting: The First U.S.-Wide Study](https://arxiv.org/abs/2510.26910)
*Kshitij Nikhal,Luke Ackerknecht,Benjamin S. Riggan,Phil Stahlfeld*

Main category: cs.LG

TL;DR: Proposes a framework that clusters charging sites to identify archetypes and uses few-shot forecasting; archetype-specific models outperform global baselines for unseen sites, enabling better infrastructure segmentation and grid resilience.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of small datasets and weak generalization in EV charging demand forecasting, enabling scalable, site-level planning for decarbonization.

Method: Integrates clustering with few-shot forecasting on a large-scale charging-demand dataset; develops archetype-specific expert models and benchmarks against global baselines, to forecast demand at unseen sites.

Result: Archetype-specific models outperform global baselines in forecasting demand at unseen sites; demonstrates improved accuracy and practical value for infrastructure decisions such as energy costs and pricing strategies.

Conclusion: Forecast-based archetype segmentation offers actionable insights for operators to optimize investment, energy use, pricing, and grid resilience, supporting climate goals.

Abstract: The decarbonization of transportation relies on the widespread adoption of
electric vehicles (EVs), which requires an accurate understanding of charging
behavior to ensure cost-effective, grid-resilient infrastructure. Existing work
is constrained by small-scale datasets, simple proximity-based modeling of
temporal dependencies, and weak generalization to sites with limited
operational history. To overcome these limitations, this work proposes a
framework that integrates clustering with few-shot forecasting to uncover site
archetypes using a novel large-scale dataset of charging demand. The results
demonstrate that archetype-specific expert models outperform global baselines
in forecasting demand at unseen sites. By establishing forecast performance as
a basis for infrastructure segmentation, we generate actionable insights that
enable operators to lower costs, optimize energy and pricing strategies, and
support grid resilience critical to climate goals.

</details>


### [10] [MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models](https://arxiv.org/abs/2510.26937)
*Zimeng Huang,Jinxin Ke,Xiaoxuan Fan,Yufeng Yang,Yang Liu,Liu Zhonghan,Zedi Wang,Junteng Dai,Haoyi Jiang,Yuyu Zhou,Keze Wang,Ziliang Chen*

Main category: cs.LG

TL;DR: MM-OPERA 构建了一个用于评估开放式联想推理的基准，包含11,497个样本，覆盖远程-item 关联（RIA）与上下文关联（ICA）两个任务，采用LLM评审对自由回答和推理路径进行过程性评估。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉-语言模型在关联推理方面仍落后于人类认知，现有闭合式任务难以捕捉开放式、创造性和跨域的联想能力。需要一个结合心理测量学原则的开放式评估框架来更全面地评估LVLM的关联推理。

Method: 设计并实现 MM-OPERA 基准，包含 11,497 条实例，覆盖两类开放式任务：Remote-Item Association (RIA) 与 In-Context Association (ICA)。输出为自由形式答案并给出显式推理路径，利用定制化的 LLM 作为评审，对输出进行过程-奖励驱动的判断，分析跨能力、领域、语言、文化等维度的多样性与有效性。

Result: 在多家前沿 LVLMs 上进行了广泛实证研究，包括对任务实例敏感性分析、LLM 评审策略有效性分析以及跨能力、领域、语言、文化等维度的多样性分析，揭示现有模型在开放式联想推理方面的局限性，并验证了基准的诊断能力及评估体系的可行性。

Conclusion: MM-OPERA 提供了一种更接近人类心理测量学原则的开放式联想评估框架，为推动更具人类类比性和通用性的 AI 研究提供数据集与工具，促进对 LVLMs 的进一步改进与对比分析。

Abstract: Large Vision-Language Models (LVLMs) have exhibited remarkable progress.
However, deficiencies remain compared to human intelligence, such as
hallucination and shallow pattern matching. In this work, we aim to evaluate a
fundamental yet underexplored intelligence: association, a cornerstone of human
cognition for creative thinking and knowledge integration. Current benchmarks,
often limited to closed-ended tasks, fail to capture the complexity of
open-ended association reasoning vital for real-world applications. To address
this, we present MM-OPERA, a systematic benchmark with 11,497 instances across
two open-ended tasks: Remote-Item Association (RIA) and In-Context Association
(ICA), aligning association intelligence evaluation with human psychometric
principles. It challenges LVLMs to resemble the spirit of divergent thinking
and convergent associative reasoning through free-form responses and explicit
reasoning paths. We deploy tailored LLM-as-a-Judge strategies to evaluate
open-ended outputs, applying process-reward-informed judgment to dissect
reasoning with precision. Extensive empirical studies on state-of-the-art
LVLMs, including sensitivity analysis of task instances, validity analysis of
LLM-as-a-Judge strategies, and diversity analysis across abilities, domains,
languages, cultures, etc., provide a comprehensive and nuanced understanding of
the limitations of current LVLMs in associative reasoning, paving the way for
more human-like and general-purpose AI. The dataset and code are available at
https://github.com/MM-OPERA-Bench/MM-OPERA.

</details>


### [11] [Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction](https://arxiv.org/abs/2510.26940)
*Ashwin Kumar,Hanyu Zhang,David A. Schweidel,William Yeoh*

Main category: cs.LG

TL;DR: Audit state-of-the-art mobility prediction models for demographic disparities and propose FGIS with SAKM to reduce fairness gaps in incremental data collection while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Address societal impacts of mobility prediction and uncover fairness gaps caused by dataset biases; develop data-centric, scalable interventions that do not require individual demographic labels.

Method: Audit large-scale models; introduce Size-Aware K-Means (SAKM) to infer proxy group labels from census proportions; design Fairness-Guided Incremental Sampling (FGIS) to prioritize samples by expected performance gains across groups; evaluate on MetaPath2Vec and a transformer-encoder model.

Result: Demographic-based disparities in predictive performance; FGIS reduces group disparity by up to ~40% with minimal accuracy trade-offs; gains are most pronounced during early sampling; demonstrates viability of lightweight, data-centric fairness interventions.

Conclusion: Structural inequities exist in mobility prediction pipelines; simple, scalable sampling strategies can meaningfully improve fairness in low-resource settings without heavy modeling changes.

Abstract: Next location prediction underpins a growing number of mobility, retail, and
public-health applications, yet its societal impacts remain largely unexplored.
In this paper, we audit state-of-the-art mobility prediction models trained on
a large-scale dataset, highlighting hidden disparities based on user
demographics. Drawing from aggregate census data, we compute the difference in
predictive performance on racial and ethnic user groups and show a systematic
disparity resulting from the underlying dataset, resulting in large differences
in accuracy based on location and user groups. To address this, we propose
Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy
designed for incremental data collection settings. Because individual-level
demographic labels are unavailable, we introduce Size-Aware K-Means (SAKM), a
clustering method that partitions users in latent mobility space while
enforcing census-derived group proportions. This yields proxy racial labels for
the four largest groups in the state: Asian, Black, Hispanic, and White. Built
on these labels, our sampling algorithm prioritizes users based on expected
performance gains and current group representation. This method incrementally
constructs training datasets that reduce demographic performance gaps while
preserving overall accuracy. Our method reduces total disparity between groups
by up to 40\% with minimal accuracy trade-offs, as evaluated on a state-of-art
MetaPath2Vec model and a transformer-encoder model. Improvements are most
significant in early sampling stages, highlighting the potential for
fairness-aware strategies to deliver meaningful gains even in low-resource
settings. Our findings expose structural inequities in mobility prediction
pipelines and demonstrate how lightweight, data-centric interventions can
improve fairness with little added complexity, especially for low-data
applications.

</details>


### [12] [Can machines think efficiently?](https://arxiv.org/abs/2510.26954)
*Adam Winchell*

Main category: cs.LG

TL;DR: 提出在图灵测试中增加能耗约束的能效测试框架，通过能耗来评估AI的效率与资源成本，并给出可衡量的结束线。


<details>
  <summary>Details</summary>
Motivation: 原始图灵测试对如今已能通过测试的AI以及相关伦理与环境问题不足以应对，需要一个资源敏感的新标准。

Method: 在模仿游戏基础上增加能量消耗约束，量化回答所消耗的能量，构建以能效为核心的智能评估方法，确保评估具有实际可达的结束线。

Result: 本文提出的是概念性框架与方法论，尚无实证结果。

Conclusion: 将能耗约束引入图灵测试可以获得可衡量的结束线，促使社会在采用AI时权衡时间收益与资源成本。

Abstract: The Turing Test is no longer adequate for distinguishing human and machine
intelligence. With advanced artificial intelligence systems already passing the
original Turing Test and contributing to serious ethical and environmental
concerns, we urgently need to update the test. This work expands upon the
original imitation game by accounting for an additional factor: the energy
spent answering the questions. By adding the constraint of energy, the new test
forces us to evaluate intelligence through the lens of efficiency, connecting
the abstract problem of thinking to the concrete reality of finite resources.
Further, this proposed new test ensures the evaluation of intelligence has a
measurable, practical finish line that the original test lacks. This additional
constraint compels society to weigh the time savings of using artificial
intelligence against its total resource cost.

</details>


### [13] [Gradient Descent as Loss Landscape Navigation: a Normative Framework for Deriving Learning Rules](https://arxiv.org/abs/2510.26997)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 将学习规则视为在部分可观测损失景观中导航的策略，寻找通过最优控制问题得到的最优学习规则；从 SGD/动量、自然梯度、非梯度规则到 Adam 等再到权重重置的 continual 学习，统一在一个目标下。


<details>
  <summary>Details</summary>
Motivation: 解释为什么不同学习规则效果差异、在何种假设下某规则是最优，并为自适应算法设计提供统一理论基础。

Method: 把学习规则建模为在（部分可观测的）损失景观上实现的策略，构造一个相关的最优控制问题；在不同的时域/可控性假设下推导出不同的规则（梯度下降、动量、自然梯度、非梯度、Adam；以及权重重置）。

Result: 在同一理论框架内自然出现多种知名规则；揭示学习过程的计算结构；为设计自适应算法提供原理性基础。

Conclusion: 通过单一目标统一学习规则，为持续学习和自适应方法提供理论基础，未来可据此设计更优的学习算法。

Abstract: Learning rules -- prescriptions for updating model parameters to improve
performance -- are typically assumed rather than derived. Why do some learning
rules work better than others, and under what assumptions can a given rule be
considered optimal? We propose a theoretical framework that casts learning
rules as policies for navigating (partially observable) loss landscapes, and
identifies optimal rules as solutions to an associated optimal control problem.
A range of well-known rules emerge naturally within this framework under
different assumptions: gradient descent from short-horizon optimization,
momentum from longer-horizon planning, natural gradients from accounting for
parameter space geometry, non-gradient rules from partial controllability, and
adaptive optimizers like Adam from online Bayesian inference of loss landscape
shape. We further show that continual learning strategies like weight resetting
can be understood as optimal responses to task uncertainty. By unifying these
phenomena under a single objective, our framework clarifies the computational
structure of learning and offers a principled foundation for designing adaptive
algorithms.

</details>


### [14] [A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms](https://arxiv.org/abs/2510.27001)
*Elise Wolf*

Main category: cs.LG

TL;DR: 提出一个可重复的MAB评估框架Bandit Playground，对八种经典及方差感知算法进行系统比较，揭示方差感知方法在高不确定性和难以区分的情形下可能优于经典方法；强调标准化评估对可重复性的关键性。


<details>
  <summary>Details</summary>
Motivation: 解决多臂带权（MAB）算法评估缺乏统一标准与可复现性的问题，特别是方差感知扩展对环境敏感性较高。

Method: 设计一个可重复的评估框架Bandit Playground，明确实验设置、提供多种性能指标（奖励、后悔、奖励分布、风险指标如价值在险、行动最优性），并提供交互式评估界面，以系统比较八种经典与方差感知的MAB算法。

Result: 在高不确定性、奖励差异微妙时，方差感知算法可能具备优势；在更易分离的情形或经过细致调参时，经典算法往往表现同等或更好；并提供可复现的代码库。

Conclusion: 提出一个系统化的MAB算法评估框架，并给出在何种条件下方差感知方法优于传统方法的洞察，强调评估的可复现性的重要性。

Abstract: Multi-armed bandit (MAB) problems serve as a fundamental building block for
more complex reinforcement learning algorithms. However, evaluating and
comparing MAB algorithms remains challenging due to the lack of standardized
conditions and replicability. This is particularly problematic for
variance-aware extensions of classical methods like UCB, whose performance can
heavily depend on the underlying environment. In this study, we address how
performance differences between bandit algorithms can be reliably observed, and
under what conditions variance-aware algorithms outperform classical ones. We
present a reproducible evaluation designed to systematically compare eight
classical and variance-aware MAB algorithms. The evaluation framework,
implemented in our Bandit Playground codebase, features clearly defined
experimental setups, multiple performance metrics (reward, regret, reward
distribution, value-at-risk, and action optimality), and an interactive
evaluation interface that supports consistent and transparent analysis. We show
that variance-aware algorithms can offer advantages in settings with high
uncertainty where the difficulty arises from subtle differences between arm
rewards. In contrast, classical algorithms often perform equally well or better
in more separable scenarios or if fine-tuned extensively. Our contributions are
twofold: (1) a framework for systematic evaluation of MAB algorithms, and (2)
insights into the conditions under which variance-aware approaches outperform
their classical counterparts.

</details>


### [15] [Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase](https://arxiv.org/abs/2510.27002)
*Mihir Mahajan,Alfred Nguyen,Franz Srambical,Stefan Bauer*

Main category: cs.LG

TL;DR: Jasmine is a scalable, high-performance JAX-based world modeling codebase that enables fast, reproducible training and benchmarking across models with scalable data loading, training, and sharding.


<details>
  <summary>Details</summary>
Motivation: The field faces data scarcity in robotics and lacks open, scalable infrastructure for world modeling; there is a need for reproducible benchmarks and faster reproduction of existing studies (e.g., CoinRun).

Method: Jasmine is implemented in JAX and scales from a single host to hundreds of accelerators with minimal code changes. It includes performance optimizations across data loading, training, and checkpointing, guarantees fully reproducible training, and supports diverse sharding configurations. It is paired with curated large-scale datasets to establish benchmarking pipelines across model families and architectural ablations.

Result: Achieves an order-of-magnitude faster reproduction of the CoinRun case study compared with prior open implementations, due to the described performance optimizations. The codebase enables fully reproducible training, supports diverse sharding, and provides infrastructure for rigorous benchmarking pipelines when paired with curated large-scale datasets.

Conclusion: Jasmine offers a performant, scalable, and reproducible world-modeling infrastructure and benchmarking platform, enabling rigorous evaluation of model families and architectural ablations on large-scale datasets.

Abstract: While world models are increasingly positioned as a pathway to overcoming
data scarcity in domains such as robotics, open training infrastructure for
world modeling remains nascent. We introduce Jasmine, a performant JAX-based
world modeling codebase that scales from single hosts to hundreds of
accelerators with minimal code changes. Jasmine achieves an order-of-magnitude
faster reproduction of the CoinRun case study compared to prior open
implementations, enabled by performance optimizations across data loading,
training and checkpointing. The codebase guarantees fully reproducible training
and supports diverse sharding configurations. By pairing Jasmine with curated
large-scale datasets, we establish infrastructure for rigorous benchmarking
pipelines across model families and architectural ablations.

</details>


### [16] [Enhancing Sentiment Classification with Machine Learning and Combinatorial Fusion](https://arxiv.org/abs/2510.27014)
*Sean Patten,Pin-Yu Chen,Christina Schweikert,D. Frank Hsu*

Main category: cs.LG

TL;DR: CFA-based ensemble of RoBERTa + RF/SVM/XGBoost achieves 97.072% on IMDB, outperforming traditional ensembles with diversity-driven fusion.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of simply scaling individual models; leverage cognitive diversity to improve ensemble accuracy and efficiency in sentiment classification.

Method: Apply Combinatorial Fusion Analysis (CFA) to fuse predictions from a RoBERTa transformer and traditional models (Random Forest, SVM, XGBoost) by computing rank-score characteristic functions to quantify model dissimilarity and strategically combine predictions.

Result: Achieves state-of-the-art IMDB sentiment accuracy of 97.072% and outperforms conventional ensemble methods while requiring less computational scaling of individual models.

Conclusion: CFA effectively exploits model diversity to enhance performance in sentiment classification, offering a resource-efficient alternative to large-scale model scaling and potentially generalizing to other tasks.

Abstract: This paper presents a novel approach to sentiment classification using the
application of Combinatorial Fusion Analysis (CFA) to integrate an ensemble of
diverse machine learning models, achieving state-of-the-art accuracy on the
IMDB sentiment analysis dataset of 97.072\%. CFA leverages the concept of
cognitive diversity, which utilizes rank-score characteristic functions to
quantify the dissimilarity between models and strategically combine their
predictions. This is in contrast to the common process of scaling the size of
individual models, and thus is comparatively efficient in computing resource
use. Experimental results also indicate that CFA outperforms traditional
ensemble methods by effectively computing and employing model diversity. The
approach in this paper implements the combination of a transformer-based model
of the RoBERTa architecture with traditional machine learning models, including
Random Forest, SVM, and XGBoost.

</details>


### [17] [Quantitative Bounds for Length Generalization in Transformers](https://arxiv.org/abs/2510.27015)
*Zachary Izzo,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 给出变换器长度外推的首个定量训练长度界限，并在多种设置下给出条件性证据，验证了“更丰富的训练数据”有助于在更长输入上的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有工作表明只要训练序列长度超过某个阈值，变换器可以实现长度外推，但未给出如何大到什么程度才可以。本文尝试给出关于训练长度与外推可达性的首个定量界限，并覆盖多种场景以提高理论理解。

Method: 在若干场景下推导长度外推的条件界限：1) 对于l∞误差控制与对输入分布的平均误差控制；2) 无穷精度的softmax注意力与有限精度注意力（简化为argmax）的对比；3) 单层与两层变换器。通过分析长序列行为是否可由训练中看到的短序列行为“仿真”来界定LG发生的必要条件，并给出训练数据长度的定性估计，同时给出实验验证。

Result: 得出在多种设定下的LG发生条件：当变换器在较长序列上的内部行为可由较短序列的行为仿真时，LG会发生。给出训练长度的定性界限与相应场景下的表现证据，并通过实验支持这些定性结论。

Conclusion: 该工作深化了对变换器外推机制的理论理解，形式化了需要更丰富训练数据以实现对更复杂任务的泛化的直觉，并提供对训练数据规模的定性指南，有助于解释和设计更具外推能力的模型。

Abstract: We study the problem of length generalization (LG) in transformers: the
ability of a model trained on shorter sequences to maintain performance when
evaluated on much longer, previously unseen inputs. Prior work by Huang et al.
(2025) established that transformers eventually achieve length generalization
once the training sequence length exceeds some finite threshold, but left open
the question of how large it must be. In this work, we provide the first
quantitative bounds on the required training length for length generalization
to occur. Motivated by previous empirical and theoretical work, we analyze LG
in several distinct problem settings: $\ell_\infty$ error control vs. average
error control over an input distribution, infinite-precision softmax attention
vs. finite-precision attention (which reduces to an argmax) in the transformer,
and one- vs. two-layer transformers. In all scenarios, we prove that LG occurs
when the internal behavior of the transformer on longer sequences can be
"simulated" by its behavior on shorter sequences seen during training. Our
bounds give qualitative estimates for the length of training data required for
a transformer to generalize, and we verify these insights empirically. These
results sharpen our theoretical understanding of the mechanisms underlying
extrapolation in transformers, and formalize the intuition that richer training
data is required for generalization on more complex tasks.

</details>


### [18] [Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning](https://arxiv.org/abs/2510.27044)
*Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: RLVR 能提升评估指标，但往往强化表面启发式而非真正的数学推理；需要更精确的基准以区分推理与捷径，评估应关注“真正在推理中的进步”。


<details>
  <summary>Details</summary>
Motivation: 评估 RLVR 是否真正在大语言模型中促进可验证的数学推理能力，特别是在可验证解的组合问题上，探究其普适性与局限性。

Method: 在两个可完全验证解的组合问题（活动调度和最长递增子序列）上，使用经过精心筛选且解唯一的数据集，比较多种奖励设计下的 RLVR 效果；通过多种评估指标分析 RLVR 是否获得真实推理能力而非捷径。

Result: 在多种奖励设计下，RLVR 提高了评估指标，但往往强化了表面性启发式而非新颖的推理策略，存在泛化受限的问题；需要能够区分“真正的推理能力”与“捷径利用”的基准来衡量进展。

Conclusion: RLVR 的改进受限，亟需设计能揭示真推理能力的基准与评测指标，并提供可复现的证明性评估方法；论文亦提供代码以便复现实验。

Abstract: Mathematical reasoning is a central challenge for large language models
(LLMs), requiring not only correct answers but also faithful reasoning
processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as
a promising approach for enhancing such capabilities; however, its ability to
foster genuine reasoning remains unclear. We investigate RLVR on two
combinatorial problems with fully verifiable solutions: \emph{Activity
Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully
curated datasets with unique optima. Across multiple reward designs, we find
that RLVR improves evaluation metrics but often by reinforcing superficial
heuristics rather than acquiring new reasoning strategies. These findings
highlight the limits of RLVR generalization, emphasizing the importance of
benchmarks that disentangle genuine mathematical reasoning from shortcut
exploitation and provide faithful measures of progress. Code available at
https://github.com/xashru/rlvr-seq-generalization.

</details>


### [19] [Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062)
*Alex Irpan,Alexander Matt Turner,Mark Kurzeja,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 提出并比较两种一致性训练（BCT与ACT）来提升大模型对提示扰动的鲁棒性，降低sycophancy与jailbreaking现象，同时避免使用静态数据集。


<details>
  <summary>Details</summary>
Motivation: 当前LLM易受提示中的无关线索与嵌入式禁令影响，导致事实性下降、拒绝策略被绕过。需要一种自监督、鲁棒的训练范式来提高对无关提示的不变性。

Method: 提出两种一致性训练：BCT在输出层约束模型输出的一致性，ACT在内部激活层实现一致性。通过对提示进行数据增强（如前置问题、 jailbreaking 文本等）来生成自监督训练信号，使用模型自身的响应来构建训练数据，避免依赖静态数据集。

Result: 两种方法都降低Gemini 2.5 Flash对无关提示的敏感性；BCT在抑制 jailbreaking 方面效果更好，两者在抑制 sycophancy 方面相当。与静态数据集相比，方法避免了能力下降或指引过时的问题。

Conclusion: 将某些对齐问题视为一致性问题更具普遍性，BCT可简化训练流程，具备提高鲁棒性的潜力。

Abstract: An LLM's factuality and refusal training can be compromised by simple changes
to a prompt. Models often adopt user beliefs (sycophancy) or satisfy
inappropriate requests which are wrapped within special text (jailbreaking). We
explore \emph{consistency training}, a self-supervised paradigm that teaches a
model to be invariant to certain irrelevant cues in the prompt. Instead of
teaching the model what exact response to give on a particular prompt, we aim
to teach the model to behave identically across prompt data augmentations (like
adding leading questions or jailbreak text). We try enforcing this invariance
in two ways: over the model's external outputs (\emph{Bias-augmented
Consistency Training} (BCT) from Chua et al. [2025]) and over its internal
activations (\emph{Activation Consistency Training} (ACT), a method we
introduce). Both methods reduce Gemini 2.5 Flash's susceptibility to irrelevant
cues. Because consistency training uses responses from the model itself as
training data, it avoids issues that arise from stale training data, such as
degrading model capabilities or enforcing outdated response guidelines. While
BCT and ACT reduce sycophancy equally well, BCT does better at jailbreak
reduction. We think that BCT can simplify training pipelines by removing
reliance on static datasets. We argue that some alignment problems are better
viewed not in terms of optimal responses, but rather as consistency issues.

</details>


### [20] [MLPerf Automotive](https://arxiv.org/abs/2510.27065)
*Radoyeh Shojaei,Predrag Djurdjevic,Mostafa El-Khamy,James Goel,Kasper Mecklenburg,John Owens,Pınar Muyan-Özçelik,Tom St. John,Jinho Suh,Arjun Suresh*

Main category: cs.LG

TL;DR: MLPerf Automotive：首个用于汽车AI加速系统的标准化公开基准，聚焦延迟与准确率，并提供可重复的评测协议；初始任务覆盖2D目标检测、2D语义分割与3D目标检测。


<details>
  <summary>Details</summary>
Motivation: 汽车领域的ML工作负载具备独特的安全性与实时性约束，现有基准无法提供可比性、可重复的评测。

Method: 由MLCommons与自动驾驶计算联盟合作开发，确立基准的任务集合、参考模型与提交规则，制定评测协议与指标（包括延迟与准确性），并讨论数据获取和参考实现的工程工作；第一轮基准提交及面临的挑战。

Result: 确定了基准设计框架，定义了初始任务、参考实现与评测流程；发布了代码库（GitHub）以便公开使用，并记录了首轮提交与数据获取方面的挑战。

Conclusion: 该框架有助于在不同硬件平台和软件实现之间实现一致、可重复的性能对比，未来将扩展任务集与数据集以覆盖更多自动驾驶场景。

Abstract: We present MLPerf Automotive, the first standardized public benchmark for
evaluating Machine Learning systems that are deployed for AI acceleration in
automotive systems. Developed through a collaborative partnership between
MLCommons and the Autonomous Vehicle Computing Consortium, this benchmark
addresses the need for standardized performance evaluation methodologies in
automotive machine learning systems. Existing benchmark suites cannot be
utilized for these systems since automotive workloads have unique constraints
including safety and real-time processing that distinguish them from the
domains that previously introduced benchmarks target. Our benchmarking
framework provides latency and accuracy metrics along with evaluation protocols
that enable consistent and reproducible performance comparisons across
different hardware platforms and software implementations. The first iteration
of the benchmark consists of automotive perception tasks in 2D object
detection, 2D semantic segmentation, and 3D object detection. We describe the
methodology behind the benchmark design including the task selection, reference
models, and submission rules. We also discuss the first round of benchmark
submissions and the challenges involved in acquiring the datasets and the
engineering efforts to develop the reference implementations. Our benchmark
code is available at https://github.com/mlcommons/mlperf_automotive.

</details>


### [21] [Towards Understanding Self-play for LLM Reasoning](https://arxiv.org/abs/2510.27072)
*Justin Yang Chae,Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 自我对弈（self-play）在大语言模型推理中的训练动态有别于 RLVR 与 SFT，通过分析参数更新稀疏性、词分布熵变化和提议者奖励等因素，揭示机制并指明局限性，为提高数学推理能力提供方向。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示自我对弈如何推动大语言模型推理能力的提升及其与 RLVR、SFT 的差异机制，通过对训练动态的实证分析来理解其有效性与局限性。

Method: 对 Absolute Zero Reasoner 的自我对弈与 RLVR、SFT 进行比较，关注参数更新的稀疏性、 token 分布的熵动态，以及替代的提议者奖励函数；并使用 pass@k 评估来关联推理性能。

Result: 揭示自我对弈在训练动态上的独特性（如稀疏性与熵模式的差异）以及对不同奖励信号的敏感性；在领域内外的性能提升部分来源于这些动态，但也暴露出对数学推理等方面的局限性；为改进自我对弈提供了方向，如优化奖励设计与训练过程的控制变量。

Conclusion: 自我对弈与其他后训练策略存在本质差异，具有固有的局限性，但通过更精细的奖励设计与训练动态调控，仍具备提升 LLM 数学推理的潜力，未来工作应聚焦于机制性分析与方法改进。

Abstract: Recent advances in large language model (LLM) reasoning, led by reinforcement
learning with verifiable rewards (RLVR), have inspired self-play post-training,
where models improve by generating and solving their own problems. While
self-play has shown strong in-domain and out-of-domain gains, the mechanisms
behind these improvements remain poorly understood. In this work, we analyze
the training dynamics of self-play through the lens of the Absolute Zero
Reasoner, comparing it against RLVR and supervised fine-tuning (SFT). Our study
examines parameter update sparsity, entropy dynamics of token distributions,
and alternative proposer reward functions. We further connect these dynamics to
reasoning performance using pass@k evaluations. Together, our findings clarify
how self-play differs from other post-training strategies, highlight its
inherent limitations, and point toward future directions for improving LLM math
reasoning through self-play.

</details>


### [22] [Hierarchical Bayesian Model for Gene Deconvolution and Functional Analysis in Human Endometrium Across the Menstrual Cycle](https://arxiv.org/abs/2510.27097)
*Crystal Su,Kuai Yu,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 基于贝叶斯层次模型的 bulk RNA-seq 去卷积框架，利用单细胞参考推断细胞类型比例及其表达变化，揭示月经周期中细胞组成的动态与细胞类型特异性差异表达，对生殖生理与疾病具潜在临床意义。


<details>
  <summary>Details</summary>
Motivation:  bulk RNA-seq 在异质样品中提供的平均表达掩盖了细胞类型特异性动态；需要利用高分辨率单细胞参考进行去卷积；同时，内膜在月经周期中受激素驱动，细胞组成发生显著变化，因此需同时推断比例和细胞特异表达变化。

Method: 提出一个概率性层次贝叶斯模型，将 bulk 表达分解为各细胞类型的表达与比例，利用高分辨率单细胞参考作为先验信息与参照；扩展框架以同时推断细胞类型比例和细胞特异的基因表达变化，并详细描述模型结构、先验和推断策略；通过仿真与与现有方法比较进行验证；进行稳健性测试以评估对参考不匹配与噪声的鲁棒性。

Result: 在月经周期各阶段观察到上皮、间充质/基质以及免疫细胞分馏比例的动态变化；识别出与内膜功能相关的细胞类型特异性差异表达，如分泌期基质细胞的蜕膜化标志物表达；模型在仿真和与现有方法的比较中表现良好，并对参考不匹配与噪声具有鲁棒性。

Conclusion: 该贝叶斯去卷积方法提供对 bulk 数据的原理性推断，揭示细胞组成在生殖周期中的动态及细胞类型特异表达变化，具潜在临床意义（生育力、内膜疾病），未来可与空间转录组等技术整合以进一步提升解析度。

Abstract: Bulk tissue RNA sequencing of heterogeneous samples provides averaged gene
expression profiles, obscuring cell type-specific dynamics. To address this, we
present a probabilistic hierarchical Bayesian model that deconvolves bulk
RNA-seq data into constituent cell-type expression profiles and proportions,
leveraging a high-resolution single-cell reference. We apply our model to human
endometrial tissue across the menstrual cycle, a context characterized by
dramatic hormone-driven cellular composition changes. Our extended framework
provides a principled inference of cell type proportions and cell-specific gene
expression changes across cycle phases. We demonstrate the model's structure,
priors, and inference strategy in detail, and we validate its performance with
simulations and comparisons to existing methods. The results reveal dynamic
shifts in epithelial, stromal, and immune cell fractions between menstrual
phases, and identify cell-type-specific differential gene expression associated
with endometrial function (e.g., decidualization markers in stromal cells
during the secretory phase). We further conduct robustness tests and show that
our Bayesian approach is resilient to reference mismatches and noise. Finally,
we discuss the biological significance of our findings, potential clinical
implications for fertility and endometrial disorders, and future directions,
including integration of spatial transcriptomics.

</details>


### [23] [Group-Sensitive Offline Contextual Bandits](https://arxiv.org/abs/2510.27123)
*Yihong Guo,Junjie Luo,Guodong Gao,Ritu Agarwal,Anqi Liu*

Main category: cs.LG

TL;DR: 提出了一种离线策略优化的约束框架，通过引入按组的奖励差距约束来减小离线情境带点学习中可能产生的群体间奖励差距，同时保持总体性能，并给出收敛性保证与经验验证。


<details>
  <summary>Details</summary>
Motivation: 在离线情境带点学习中，优化策略可能会放大不同群体的奖励差距，导致资源有限时的不公平问题。需要在训练过程中对组间奖励不平等进行约束或最小化。

Method: 在离线策略评估/优化的梯度更新中加入组间奖励差距约束，采用双重鲁棒估计来更准确地估计差距，并给出收敛性保障。通过离线数据进行对策略的训练，要求同时控制群体间差异与整体表现。

Result: 在合成数据与真实数据集上，方法能够有效降低组间奖励差异，同时保持竞争性的总奖励表现。

Conclusion: 所提出的带组别差异约束的离线策略优化框架在理论上具备收敛性保证，实验上实现了对群体公平性的改进，且不显著牺牲整体性能。

Abstract: Offline contextual bandits allow one to learn policies from
historical/offline data without requiring online interaction. However, offline
policy optimization that maximizes overall expected rewards can unintentionally
amplify the reward disparities across groups. As a result, some groups might
benefit more than others from the learned policy, raising concerns about
fairness, especially when the resources are limited. In this paper, we study a
group-sensitive fairness constraint in offline contextual bandits, reducing
group-wise reward disparities that may arise during policy learning. We tackle
the following common-parity requirements: the reward disparity is constrained
within some user-defined threshold or the reward disparity should be minimized
during policy optimization. We propose a constrained offline policy
optimization framework by introducing group-wise reward disparity constraints
into an off-policy gradient-based optimization procedure. To improve the
estimation of the group-wise reward disparity during training, we employ a
doubly robust estimator and further provide a convergence guarantee for policy
optimization. Empirical results in synthetic and real-world datasets
demonstrate that our method effectively reduces reward disparities while
maintaining competitive overall performance.

</details>


### [24] [AI Agents in Drug Discovery](https://arxiv.org/abs/2510.27130)
*Srijit Seal,Dinh Long Huynh,Moudather Chelbi,Sara Khosravi,Ankur Kumar,Mattson Thieme,Isaac Wilks,Mark Davies,Jessica Mustali,Yannick Sun,Nick Edwards,Daniil Boiko,Andrei Tyrin,Douglas W. Selinger,Ayaan Parikh,Rahul Vijayan,Shoman Kasbekar,Dylan Reid,Andreas Bender,Ola Spjuth*

Main category: cs.LG

TL;DR: 综述agentic AI在药物发现中的应用与实现，涵盖从ReAct/Reflection等架构到端到端流程的真实案例与量化影响。


<details>
  <summary>Details</summary>
Motivation: 提升药物发现的速度、可重复性与可扩展性，通过把LLMs与感知、计算、行动与记忆工具结合，建立闭环的研究工作流。

Method: 系统梳理agentic AI的架构与应用，结合真实世界实现和量化评估，覆盖文献综述、毒性预测、自动化协议、合成、药物再定位和端到端决策。

Result: 早期实现显示显著提速与可重复性提升，将原本需要数月的工作压缩到数小时，且保持科学追溯性；同时揭示数据异质性、系统可靠性、隐私和基准测试等挑战。

Conclusion: 未来将进一步解决数据与系统挑战，推动agentic AI在药物发现中的广泛落地与转化。

Abstract: Artificial intelligence (AI) agents are emerging as transformative tools in
drug discovery, with the ability to autonomously reason, act, and learn through
complicated research workflows. Building on large language models (LLMs)
coupled with perception, computation, action, and memory tools, these agentic
AI systems could integrate diverse biomedical data, execute tasks, carry out
experiments via robotic platforms, and iteratively refine hypotheses in closed
loops. We provide a conceptual and technical overview of agentic AI
architectures, ranging from ReAct and Reflection to Supervisor and Swarm
systems, and illustrate their applications across key stages of drug discovery,
including literature synthesis, toxicity prediction, automated protocol
generation, small-molecule synthesis, drug repurposing, and end-to-end
decision-making. To our knowledge, this represents the first comprehensive work
to present real-world implementations and quantifiable impacts of agentic AI
systems deployed in operational drug discovery settings. Early implementations
demonstrate substantial gains in speed, reproducibility, and scalability,
compressing workflows that once took months into hours while maintaining
scientific traceability. We discuss the current challenges related to data
heterogeneity, system reliability, privacy, and benchmarking, and outline
future directions towards technology in support of science and translation.

</details>


### [25] [Exploring the Utilities of the Rationales from Large Language Models to Enhance Automated Essay Scoring](https://arxiv.org/abs/2510.27131)
*Hong Jiao,Hanna Choi,Haowei Hua*

Main category: cs.LG

TL;DR: 对GPT-4.1/GPT-5生成的推理过程在自动作文评分中的作用进行比较：总体而言，单纯作文评分优于基于推理的评分，但在类别不平衡下，推理评分在分数0的F1上更具优势；对作文评分和推理评分进行集成能提升整体评分准确性，最佳QWK达到0.870，超越文献0.848。


<details>
  <summary>Details</summary>
Motivation: 评估大模型推理的实际效用，探索推理自解释对评分性能的增益或损害，以及如何通过集成来提升鲁棒性和对少数类的敏感度；使用真实数据集（2012 Kaggle ASAP）和多模型对比。

Method: 使用GPT-4.1和GPT-5对Prompt 6作文进行推理生成，分别进行作文基线评分和推理评分；评估指标为Quadratic Weighted Kappa（QWK）和分数0的F1；构建单模型和多模型集成：仅作文评分，作文+各自推理评分，以及作文+两种推理评分的组合；与文献基线QWK对比。

Result: 总体上作文评分优于推理评分；在类别不平衡下，推理评分在分数0的F1更高；作文评分模型的集成提高了不同分数水平特征和总的准确性；作文评分与任一推理评分的集成表现相近；作文评分与两种推理评分全部结合的集成获得最佳绩效，QWK 0.870，超过文献0.848。

Conclusion: 推理生成的解释对整体评分贡献有限，但在少数类上具有潜在价值；通过恰当的集成，作文评分与推理信号的结合可显著提升鲁棒性和总分一致性；未来工作可关注推理质量、权重学习和对不同分布的自适应校准。

Abstract: This study explored the utilities of rationales generated by GPT-4.1 and
GPT-5 in automated scoring using Prompt 6 essays from the 2012 Kaggle ASAP
data. Essay-based scoring was compared with rationale-based scoring. The study
found in general essay-based scoring performed better than rationale-based
scoring with higher Quadratic Weighted Kappa (QWK). However, rationale-based
scoring led to higher scoring accuracy in terms of F1 scores for score 0 which
had less representation due to class imbalance issues. The ensemble modeling of
essay-based scoring models increased the scoring accuracy at both specific
score levels and across all score levels. The ensemble modeling of essay-based
scoring and each of the rationale-based scoring performed about the same.
Further ensemble of essay-based scoring and both rationale-based scoring
yielded the best scoring accuracy with QWK of 0.870 compared with 0.848
reported in literature.

</details>


### [26] [FairAD: Computationally Efficient Fair Graph Clustering via Algebraic Distance](https://arxiv.org/abs/2510.27136)
*Minh Phu Vuong,Young-Ju Lee,Iván Ojeda-Ruiz,Chul-Ho Lee*

Main category: cs.LG

TL;DR: 提出了一种高效的公平图聚类算法FairAD，通过基于代数距离的亲和矩阵、图粗化和受约束的最小化来在大规模图上实现“公平”分组，且相较于现有方法速度提升可达40倍。


<details>
  <summary>Details</summary>
Motivation: 在机器学习模型对特定受保护群体表现出不公平行为的背景下，研究对象是将公平性引入图聚类，即使得每个簇中各受保护群体的比例与整个数据集中的比例尽可能一致，同时还能在大规模图上保持高效计算。

Method: 1) 构造基于代数距离的新亲和矩阵，使聚类过程嵌入公平约束；2) 对该亲和矩阵进行图粗化，找到可代表k个簇的代表节点；3) 通过受约束的最小化问题求解最终的公平聚类解。

Result: 在改进的随机块模型和六个公开数据集上进行实验，FairAD在实现公平聚类的同时，速度比现有最优公平图聚类算法快多达40倍。

Conclusion: FairAD提供了一种高效的公平图聚类方案，能够在大规模图上实现公平性约束下的高效聚类，具有良好的实验效果与显著的计算优势，适用于需要大规模、带公平性约束的聚类任务。

Abstract: Due to the growing concern about unsavory behaviors of machine learning
models toward certain demographic groups, the notion of 'fairness' has recently
drawn much attention from the community, thereby motivating the study of
fairness in graph clustering. Fair graph clustering aims to partition the set
of nodes in a graph into $k$ disjoint clusters such that the proportion of each
protected group within each cluster is consistent with the proportion of that
group in the entire dataset. It is, however, computationally challenging to
incorporate fairness constraints into existing graph clustering algorithms,
particularly for large graphs. To address this problem, we propose FairAD, a
computationally efficient fair graph clustering method. It first constructs a
new affinity matrix based on the notion of algebraic distance such that
fairness constraints are imposed. A graph coarsening process is then performed
on this affinity matrix to find representative nodes that correspond to $k$
clusters. Finally, a constrained minimization problem is solved to obtain the
solution of fair clustering. Experiment results on the modified stochastic
block model and six public datasets show that FairAD can achieve fair
clustering while being up to 40 times faster compared to state-of-the-art fair
graph clustering algorithms.

</details>


### [27] [Exploring Landscapes for Better Minima along Valleys](https://arxiv.org/abs/2510.27153)
*Tong Zhao,Jiacheng Li,Yuanchang Zhou,Guangming Tan,Weile Jia*

Main category: cs.LG

TL;DR: 提出将梯度优化器适配器 E，使优化过程在达到局部最小后仍沿低损失峡谷搜索，以寻求更低且更平坦的局部最小值，从而提升泛化。给出凸/非凸情形的收敛性证明，并在大批量训练中以 Lamb 为基准验证，平均提升测试准确率约 2.5%。


<details>
  <summary>Details</summary>
Motivation: 深度学习中，单纯达到局部最小并不保证全局最优或最佳泛化。损失面几何复杂，需新的优化策略在“可接受误差附近”继续探索以提高泛化能力。

Method: 设计一个梯度基优化器的适配器 E，使优化在达到局部最小后仍沿着低损失的峡谷继续探索，旨在寻找更低且更平坦的局部最小值。并提出在此框架下的改进版本（ALTO，基于 Lamb 的优化器），附带对凸/非凸情形的收敛性证明，以及在大批量训练任务中的实验验证。

Result: 在大批量训练场景中，ALTO 相对于 Lamb 提升了测试准确率，平均约 2.5%。

Conclusion: 该方法为优化算法设计打开了新的方向，尤其在追求更好泛化的局部最小值方面提供了有效途径。

Abstract: Finding lower and better-generalizing minima is crucial for deep learning.
However, most existing optimizers stop searching the parameter space once they
reach a local minimum. Given the complex geometric properties of the loss
landscape, it is difficult to guarantee that such a point is the lowest or
provides the best generalization. To address this, we propose an adaptor "E"
for gradient-based optimizers. The adapted optimizer tends to continue
exploring along landscape valleys (areas with low and nearly identical losses)
in order to search for potentially better local minima even after reaching a
local minimum. This approach increases the likelihood of finding a lower and
flatter local minimum, which is often associated with better generalization. We
also provide a proof of convergence for the adapted optimizers in both convex
and non-convex scenarios for completeness. Finally, we demonstrate their
effectiveness in an important but notoriously difficult training scenario,
large-batch training, where Lamb is the benchmark optimizer. Our testing
results show that the adapted Lamb, ALTO, increases the test accuracy
(generalization) of the current state-of-the-art optimizer by an average of
2.5% across a variety of large-batch training tasks. This work potentially
opens a new research direction in the design of optimization algorithms.

</details>


### [28] [Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler](https://arxiv.org/abs/2510.27172)
*Zixuan Hu,Li Shen,Zhenyi Wang,Yongxian Wei,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出 Bayesion Data Scheduler (BDS)，通过对每条训练数据的安全属性进行贝叶斯后验推断并对微调数据进行加权，从而在不依赖攻击仿真的情况下实现自适应且可迁移的有害微调防护。


<details>
  <summary>Details</summary>
Motivation: 现有对有害微调的防护多依赖攻击仿真，但这存在攻击范围受限、难以覆盖未知攻击且对不同设定缺乏自适应性的问题。需一种能针对具体数据集自适应、且不依赖事先攻击仿真的防护策略。

Method: 将防护问题建模为贝叶斯推断，学习每个样本的安全属性的后验分布（条件于微调与对齐数据集）。在微调阶段用从后验采样得到的安全属性对数据加权，从而抑制有害数据的影响。后验依赖于具体微调数据集，具备自适应性。此外，提出基于渐进贝叶斯学习的神经调度器，实现对新数据的高效迁移，无需重新训练。

Result: 在多样化的攻击与防御设置下，方法达到当前最先进的性能水平，且代码公开。

Conclusion: BDS提供一种无需攻击仿真的自适应防护框架，且通过可迁移的神经调度器提升对有害微调的鲁棒性与实用性，具有较强的实用潜力与扩展性。

Abstract: Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service
for large language models. Existing defense strategies preemptively build
robustness via attack simulation but suffer from fundamental limitations: (i)
the infeasibility of extending attack simulations beyond bounded threat models
due to the inherent difficulty of anticipating unknown attacks, and (ii)
limited adaptability to varying attack settings, as simulation fails to capture
their variability and complexity. To address these challenges, we propose
Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with
no need for attack simulation. BDS formulates harmful fine-tuning defense as a
Bayesian inference problem, learning the posterior distribution of each data
point's safety attribute, conditioned on the fine-tuning and alignment
datasets. The fine-tuning process is then constrained by weighting data with
their safety attributes sampled from the posterior, thus mitigating the
influence of harmful data. By leveraging the post hoc nature of Bayesian
inference, the posterior is conditioned on the fine-tuning dataset, enabling
BDS to tailor its defense to the specific dataset, thereby achieving adaptive
defense. Furthermore, we introduce a neural scheduler based on amortized
Bayesian learning, enabling efficient transfer to new data without retraining.
Comprehensive results across diverse attack and defense settings demonstrate
the state-of-the-art performance of our approach. Code is available at
https://github.com/Egg-Hu/Bayesian-Data-Scheduler.

</details>


### [29] [A Polynomial-time Algorithm for Online Sparse Linear Regression with Improved Regret Bound under Weaker Conditions](https://arxiv.org/abs/2510.27177)
*Junfan Li,Shizhong Liao,Zenglin Xu,Liqiang Nie*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we study the problem of online sparse linear regression (OSLR)
where the algorithms are restricted to accessing only $k$ out of $d$ attributes
per instance for prediction, which was proved to be NP-hard. Previous work gave
polynomial-time algorithms assuming the data matrix satisfies the linear
independence of features, the compatibility condition, or the restricted
isometry property. We introduce a new polynomial-time algorithm, which
significantly improves previous regret bounds (Ito et al., 2017) under the
compatibility condition that is weaker than the other two assumptions. The
improvements benefit from a tighter convergence rate of the $\ell_1$-norm error
of our estimators. Our algorithm leverages the well-studied Dantzig Selector,
but importantly with several novel techniques, including an algorithm-dependent
sampling scheme for estimating the covariance matrix, an adaptive parameter
tuning scheme, and a batching online Newton step with careful initializations.
We also give novel and non-trivial analyses, including an induction method for
analyzing the $\ell_1$-norm error, careful analyses on the covariance of
non-independent random variables, and a decomposition on the regret. We further
extend our algorithm to OSLR with additional observations where the algorithms
can observe additional $k_0$ attributes after each prediction, and improve
previous regret bounds (Kale et al., 2017; Ito et al., 2017).

</details>


### [30] [SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference](https://arxiv.org/abs/2510.27182)
*Zongshun Zhang,Ibrahim Matta*

Main category: cs.LG

TL;DR: SERFLOW通过在FaaS与IaaS之间进行动态分区与资源调度来优化自适应ML推理的延迟与成本，考虑阶段退出率与VM冷启动等现实因素。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界因素（如VM冷启动、长尾分布等）导致的传统单一资源配置在延迟与成本之间难以兼顾的问题，提升自适应推理应用的性价比与鲁棒性。

Method: 将每个ML查询建模为无环的阶段序列，每个阶段是稀疏参数的连续块，末端为分类器；引入阶段级资源 provisioning，依据各阶段的退出率调整资源；结合基于请求进入的负载均衡，在VM与无服务器函数之间动态分配工作负载；通过FaaS容器实现弹性计算与低成本扩展，兼顾冷启动与高并发场景。

Result: 相比纯IaaS或纯FaaS方案，SERFLOW在动态 workloads 下显著降低云成本 (>23%)，并且在不同阶段退出分布下保持高效的自适应性。

Conclusion: 通过阶段感知的混合云资源供给和自适应负载均衡，SERFLOW实现更低成本且对输入分布鲁棒的自适应推理框架，具备良好扩展性与现实部署潜力。

Abstract: Dynamic offloading of Machine Learning (ML) model partitions across different
resource orchestration services, such as Function-as-a-Service (FaaS) and
Infrastructure-as-a-Service (IaaS), can balance processing and transmission
delays while minimizing costs of adaptive inference applications. However,
prior work often overlooks real-world factors, such as Virtual Machine (VM)
cold starts, requests under long-tail service time distributions, etc. To
tackle these limitations, we model each ML query (request) as traversing an
acyclic sequence of stages, wherein each stage constitutes a contiguous block
of sparse model parameters ending in an internal or final classifier where
requests may exit. Since input-dependent exit rates vary, no single resource
configuration suits all query distributions. IaaS-based VMs become
underutilized when many requests exit early, yet rapidly scaling to handle
request bursts reaching deep layers is impractical. SERFLOW addresses this
challenge by leveraging FaaS-based serverless functions (containers) and using
stage-specific resource provisioning that accounts for the fraction of requests
exiting at each stage. By integrating this provisioning with adaptive load
balancing across VMs and serverless functions based on request ingestion,
SERFLOW reduces cloud costs by over $23\%$ while efficiently adapting to
dynamic workloads.

</details>


### [31] [Feature-Function Curvature Analysis: A Geometric Framework for Explaining Differentiable Models](https://arxiv.org/abs/2510.27207)
*Hamed Najafi,Dongsheng Luo,Jason Liu*

Main category: cs.LG

TL;DR: FFCA introduces a geometry-aware explainability framework that quantifies per-feature signatures (Impact, Volatility, Non-linearity, Interaction) and extends to Dynamic Archetype Analysis to track how these signatures evolve during training, revealing hierarchical learning and enabling diagnostics for capacity and overfitting.


<details>
  <summary>Details</summary>
Motivation: Mainstream attribution methods provide static, single-score explanations that conflate non-linearity and interactions; there is a need for a geometry-aware and temporally dynamic understanding of how models learn to trust explanations and diagnose model behavior.

Method: Define a four-dimensional feature signature via Feature-Function Curvature Analysis (Impact, Volatility, Non-linearity, Interaction); compute model function geometry to quantify these dimensions; extend to Dynamic Archetype Analysis to monitor the evolution of signatures over training epochs; use empirical experiments to validate hierarchical learning and diagnostic capabilities.

Result: Empirical evidence that models learn simple linear effects before complex interactions; FFCA provides a geometric context for explanations and reveals dynamics that static methods miss; dynamic analysis offers practical diagnostics for insufficient capacity and onset of overfitting; static and dynamic components together enhance trustworthy explanations of the learning process.

Conclusion: FFCA and Dynamic Archetype Analysis provide a comprehensive, geometry-informed, and temporally evolving framework for understanding and explaining neural networks, moving beyond static feature attribution to capture how models learn throughout training.

Abstract: Explainable AI (XAI) is critical for building trust in complex machine
learning models, yet mainstream attribution methods often provide an
incomplete, static picture of a model's final state. By collapsing a feature's
role into a single score, they are confounded by non-linearity and
interactions. To address this, we introduce Feature-Function Curvature Analysis
(FFCA), a novel framework that analyzes the geometry of a model's learned
function. FFCA produces a 4-dimensional signature for each feature, quantifying
its: (1) Impact, (2) Volatility, (3) Non-linearity, and (4) Interaction.
Crucially, we extend this framework into Dynamic Archetype Analysis, which
tracks the evolution of these signatures throughout the training process. This
temporal view moves beyond explaining what a model learned to revealing how it
learns. We provide the first direct, empirical evidence of hierarchical
learning, showing that models consistently learn simple linear effects before
complex interactions. Furthermore, this dynamic analysis provides novel,
practical diagnostics for identifying insufficient model capacity and
predicting the onset of overfitting. Our comprehensive experiments demonstrate
that FFCA, through its static and dynamic components, provides the essential
geometric context that transforms model explanation from simple quantification
to a nuanced, trustworthy analysis of the entire learning process.

</details>


### [32] [Soft Task-Aware Routing of Experts for Equivariant Representation Learning](https://arxiv.org/abs/2510.27222)
*Jaebyeong Jeon,Hyeonseo Jang,Jy-yong Sohn,Kibok Lee*

Main category: cs.LG

TL;DR: 引入 Soft Task-Aware Routing (STAR)，将投影头视为专家，通过软路由让专家分别专注于共享信息或任务特定信息，减少冗余特征并提升跨任务迁移性能。


<details>
  <summary>Details</summary>
Motivation: 同时学习不变表示与等变表示常有收益，但现有做法将两个学习分支分离的投影头可能忽略它们之间的共享信息，造成冗余与容量浪费。

Method: 提出 STAR 路由策略，将投影头建模为专家，通过软路由/门控机制使专家在共享信息和任务特定信息之间分工，降低 invariant 与 equivariant 表示之间的相关性，实验证明其有效性。

Result: 在多种迁移学习任务上获得一致性提升；观察到不变量和等变嵌入的典型相关性下降；代码公开。

Conclusion: STAR 促进不同表示之间的协同利用，减少冗余特征，提升整体表示学习的效率与迁移性能。

Abstract: Equivariant representation learning aims to capture variations induced by
input transformations in the representation space, whereas invariant
representation learning encodes semantic information by disregarding such
transformations. Recent studies have shown that jointly learning both types of
representations is often beneficial for downstream tasks, typically by
employing separate projection heads. However, this design overlooks information
shared between invariant and equivariant learning, which leads to redundant
feature learning and inefficient use of model capacity. To address this, we
introduce Soft Task-Aware Routing (STAR), a routing strategy for projection
heads that models them as experts. STAR induces the experts to specialize in
capturing either shared or task-specific information, thereby reducing
redundant feature learning. We validate this effect by observing lower
canonical correlations between invariant and equivariant embeddings.
Experimental results show consistent improvements across diverse transfer
learning tasks. The code is available at https://github.com/YonseiML/star.

</details>


### [33] [Not All Instances Are Equally Valuable: Towards Influence-Weighted Dataset Distillation](https://arxiv.org/abs/2510.27253)
*Qiyan Deng,Changqian Zheng,Lianpeng Qiao,Yuping Wang,Chengliang Chai,Lei Cao*

Main category: cs.LG

TL;DR: 提出一种影响函数驱动的数据集蒸馏（IWD）框架，通过对样本分配自适应权重来区分有信息性、冗余与有害的数据，从而提高蒸馏数据的质量与模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中存在数据质量差异（有信息性、冗余或有害样本），直接蒸馏全量数据可能削弱模型表现。利用影响函数将每个样本对蒸馏目标的影响度量并加权，旨在提升蒸馏效果。

Method: 在蒸馏框架中引入影响函数，给每个样本分配基于其对蒸馏目标的影响的自适应权重；该设计是模块化的，可以无缝集成到不同的蒸馏方法中。

Result: 经验结果显示，嵌入IWD后，蒸馏数据质量提升，模型性能提升可达7.8%的准确率。

Conclusion: 通过显式考虑数据质量，IWD为数据集蒸馏提供了一个更稳健的权重化机制，具有良好的可扩展性和与其他蒸馏框架的兼容性。

Abstract: Dataset distillation condenses large datasets into synthetic subsets,
achieving performance comparable to training on the full dataset while
substantially reducing storage and computation costs. Most existing dataset
distillation methods assume that all real instances contribute equally to the
process. In practice, real-world datasets contain both informative and
redundant or even harmful instances, and directly distilling the full dataset
without considering data quality can degrade model performance. In this work,
we present Influence-Weighted Distillation IWD, a principled framework that
leverages influence functions to explicitly account for data quality in the
distillation process. IWD assigns adaptive weights to each instance based on
its estimated impact on the distillation objective, prioritizing beneficial
data while downweighting less useful or harmful ones. Owing to its modular
design, IWD can be seamlessly integrated into diverse dataset distillation
frameworks. Our empirical results suggest that integrating IWD tends to improve
the quality of distilled datasets and enhance model performance, with accuracy
gains of up to 7.8%.

</details>


### [34] [ECVL-ROUTER: Scenario-Aware Routing for Vision-Language Models](https://arxiv.org/abs/2510.27256)
*Xin Tang,Youfang Han,Fangfei Gou,Wei Zhao,Xin Meng,Yang Yu,Jinguo Zhang,Yuanchun Shi,Yuntao Wang,Tengxiang Zhang*

Main category: cs.LG

TL;DR: 提出了 ECVL-ROUTER，这是一种面向场景的路由框架，用于在大型云端模型和小型边缘模型之间，根据用户对快速响应、高质量输出与低能耗等需求动态分配查询，以提升总体效用。并构建了面向路由训练的多模态响应质量数据集，通过实验验证路由效果。


<details>
  <summary>Details</summary>
Motivation: 在视觉-语言模型（VLM）场景中，不同任务对延迟、质量和能耗的要求各不相同。单纯依赖云端大模型会带来较高的时延和能耗，而边缘小模型虽然低延迟、能耗低，但能力受限。需要一个能同时发挥大模型的高质量和小模型的效率的系统来分配查询。

Method: 提出新的路由策略和评估指标，动态为每个查询选取合适的模型；构建并使用一个面向路由训练的多模态响应质量数据集；通过广泛实验验证框架的有效性。

Result: 该方法实现将超过80%的查询路由到小模型，同时问题求解概率的下降小于10%。

Conclusion: 场景感知路由框架能够有效结合大模型与小模型的优势，在提升总体效用的同时显著提升资源利用率，为高效的VLM部署提供了可行路径。

Abstract: Vision-Language Models (VLMs) excel in diverse multimodal tasks. However,
user requirements vary across scenarios, which can be categorized into fast
response, high-quality output, and low energy consumption. Relying solely on
large models deployed in the cloud for all queries often leads to high latency
and energy cost, while small models deployed on edge devices are capable of
handling simpler tasks with low latency and energy cost. To fully leverage the
strengths of both large and small models, we propose ECVL-ROUTER, the first
scenario-aware routing framework for VLMs. Our approach introduces a new
routing strategy and evaluation metrics that dynamically select the appropriate
model for each query based on user requirements, maximizing overall utility. We
also construct a multimodal response-quality dataset tailored for router
training and validate the approach through extensive experiments. Results show
that our approach successfully routes over 80\% of queries to the small model
while incurring less than 10\% drop in problem solving probability.

</details>


### [35] [ODP-Bench: Benchmarking Out-of-Distribution Performance Prediction](https://arxiv.org/abs/2510.27263)
*Han Yu,Kehan Li,Dongbai Li,Yue He,Xingxuan Zhang,Peng Cui*

Main category: cs.LG

TL;DR: 提出 ODP-Bench，建立一个面向 OOD 性能预测的综合基准，整合常用的 OOD 数据集与性能预测算法，并提供经过训练的模型以确保评测的一致性与可复现性。


<details>
  <summary>Details</summary>
Motivation: 现有文献的评测协议不一致，数据集有限且覆盖面不足，难以进行公平比较；需要一个便捷且公平的基准来评估模型在未标注 OOD 测试集上的性能，以支持风险敏感场景的部署。

Method: 提出 ODP-Bench 基准，收集并整理常用的 OOD 数据集及现有实用的性能预测算法，构建统一的评测框架；提供经过训练的模型作为测试基准以避免重复训练的开销；并进行深入实验分析以界定方法的能力边界。

Result: ODP-Bench 汇聚了多样化的 OOD 数据集与基线算法，公开了训练好的模型作为测试基准，开展了系统化的实验以评估方法的有效性与边界，提升评测的一致性、可重复性与可比性。

Conclusion: 该基准有望标准化 OOD 性能预测的评测，从而加速领域进展，促进不同算法之间的公平比较并降低重复工作量。

Abstract: Recently, there has been gradually more attention paid to Out-of-Distribution
(OOD) performance prediction, whose goal is to predict the performance of
trained models on unlabeled OOD test datasets, so that we could better leverage
and deploy off-the-shelf trained models in risk-sensitive scenarios. Although
progress has been made in this area, evaluation protocols in previous
literature are inconsistent, and most works cover only a limited number of
real-world OOD datasets and types of distribution shifts. To provide convenient
and fair comparisons for various algorithms, we propose Out-of-Distribution
Performance Prediction Benchmark (ODP-Bench), a comprehensive benchmark that
includes most commonly used OOD datasets and existing practical performance
prediction algorithms. We provide our trained models as a testbench for future
researchers, thus guaranteeing the consistency of comparison and avoiding the
burden of repeating the model training process. Furthermore, we also conduct
in-depth experimental analyses to better understand their capability boundary.

</details>


### [36] [Temporal Cardiovascular Dynamics for Improved PPG-Based Heart Rate Estimation](https://arxiv.org/abs/2510.27297)
*Berken Utku Demirel,Christian Holz*

Main category: cs.LG

TL;DR: 提出基于互信息的非线性心率分析，结合深度学习提升真实场景下的心率估计，性能显著提升并简化传感与后处理。


<details>
  <summary>Details</summary>
Motivation: 解决真实生活中心率估计受到非线性/混沌时序影响的问题，提升鲁棒性与准确性，同时减少对多模态传感和后处理的依赖。

Method: 以互信息研究心率序列的非线性时序复杂性，提出一种新颖的心率估计增强方法；将该方法与深度学习框架结合以提升估计效果；在四个真实数据集上进行广泛消融实验。

Result: 相比传统方法和现有机器学习技术，心率估计性能提升约40%；同时降低对多模态传感的依赖，并消除了后处理步骤。

Conclusion: 该方法有效解释和处理非线性时间序列的复杂性，提升真实场景下的心率估计，具有实用潜力和推广价值。

Abstract: The oscillations of the human heart rate are inherently complex and
non-linear -- they are best described by mathematical chaos, and they present a
challenge when applied to the practical domain of cardiovascular health
monitoring in everyday life. In this work, we study the non-linear chaotic
behavior of heart rate through mutual information and introduce a novel
approach for enhancing heart rate estimation in real-life conditions. Our
proposed approach not only explains and handles the non-linear temporal
complexity from a mathematical perspective but also improves the deep learning
solutions when combined with them. We validate our proposed method on four
established datasets from real-life scenarios and compare its performance with
existing algorithms thoroughly with extensive ablation experiments. Our results
demonstrate a substantial improvement, up to 40\%, of the proposed approach in
estimating heart rate compared to traditional methods and existing
machine-learning techniques while reducing the reliance on multiple sensing
modalities and eliminating the need for post-processing steps.

</details>


### [37] [Un-Attributability: Computing Novelty From Retrieval & Semantic Similarity](https://arxiv.org/abs/2510.27313)
*Philipp Davydov,Ameya Prabhu,Matthias Bethge,Elisa Nguyen,Seong Joon Oh*

Main category: cs.LG

TL;DR: 提出 un-attributability 作为语义新颖性的可操作度量；通过两阶段检索（GIST 索引 + ColBERTv2 重排序）评估输出与预训练语料的可归因性。结果表明模型在更长的跨域范围内使用预训练数据，某些领域促/抑制新颖性，指令微调提高新颖性，并释放约20 TB 的语料块及索引以便复现。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型输出与预训练语料之间的关系，改进传统基于留一法的训练数据归因的局限性，提出可操作的、可扩展的新颖性评估。

Method: 定义 un-attributability，将输出是否可归因于任何预训练样本的判定转化为新颖性判断；采用两阶段检索：先用轻量级 GIST 嵌入对语料进行索引并检索前 n 条候选，再用 ColBERTv2 重新排序，输出若最近的语料项比人类文本参考的可归因性更低，则视为新颖。

Result: 在 SmolLM 与 SmolLM2 上评估，得到三点结论：1) 模型在比以往报道的范围更长的跨度上调用预训练数据；2) 不同领域系统性地促进或抑制新颖性；3) 指令微调不仅改变风格，还提升新颖性。

Conclusion: 将新颖性评估从归因转向 un-attributability 的框架，可以在大规模预训练数据下实现更高效的分析；同时公开 ~20 TB 的语料块和索引以支持复现实验和跨模型扩展。

Abstract: Understanding how language-model outputs relate to the pretraining corpus is
central to studying model behavior. Most training data attribution (TDA)
methods ask which training examples causally influence a given output, often
using leave-one-out tests. We invert the question: which outputs cannot be
attributed to any pretraining example? We introduce un-attributability as an
operational measure of semantic novelty: an output is novel if the pretraining
corpus contains no semantically similar context. We approximate this with a
simple two-stage retrieval pipeline: index the corpus with lightweight GIST
embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the
nearest corpus item is less attributable than a human-generated text reference,
we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2
and report three findings: (1) models draw on pretraining data across much
longer spans than previously reported; (2) some domains systematically promote
or suppress novelty; and (3) instruction tuning not only alters style but also
increases novelty. Reframing novelty assessment around un-attributability
enables efficient analysis at pretraining scale. We release ~20 TB of corpus
chunks and index artifacts to support replication and large-scale extension of
our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm

</details>


### [38] [MedM2T: A MultiModal Framework for Time-Aware Modeling with Electronic Health Record and Electrocardiogram Data](https://arxiv.org/abs/2510.27321)
*Yu-Chen Kuo,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: MedM2T 是一个面向医疗时间序列的多模态、时间感知框架，通过稀疏时间序列编码器、分层时间感知融合和双模态注意力来处理不规则数据和跨模态交互，在 MIMIC 数据集上实现对心血管疾病预测、院内死亡和 ICU LOS 的高性能预测，且公开实现代码。


<details>
  <summary>Details</summary>
Motivation: 医疗数据具有多模态性和异质的时间结构，存在数据稀疏、不规则时间间隔和不同模态粒度等挑战，需要统一的跨模态建模框架来捕捉微观与宏观时间模式并对跨模态进行交互分析。

Method: 提出 MedM2T，包含：1) 稀疏时间序列编码器以处理不规则/稀疏序列；2) 分层时间感知融合以捕捉来自多条密集时间序列（如 ECG）的微观和宏观时间模式；3) 双模态注意力以提取跨模态互动，可扩展到多模态。通过模态特定的预训练编码器并在共享编码器对齐特征来弥合模态粒度差异。

Result: 在 MIMIC-IV 与 MIMIC-IV-ECG 上完成三类任务：90 天心血管疾病预测、院内死亡预测、ICU LOS 回归。相比领先的多模态学习框架和现有时间序列模型，MedM2T 获得：CVD AUROC 0.947、AUPRC 0.706；死亡 AUROC 0.901、AUPRC 0.558；LOS MAE 2.31。

Conclusion: 结果表明 MedM2T 具有稳健性和广泛适用性，具成为临床预测的有前景工具，且提供代码实现。

Abstract: The inherent multimodality and heterogeneous temporal structures of medical
data pose significant challenges for modeling. We propose MedM2T, a time-aware
multimodal framework designed to address these complexities. MedM2T integrates:
(i) Sparse Time Series Encoder to flexibly handle irregular and sparse time
series, (ii) Hierarchical Time-Aware Fusion to capture both micro- and
macro-temporal patterns from multiple dense time series, such as ECGs, and
(iii) Bi-Modal Attention to extract cross-modal interactions, which can be
extended to any number of modalities. To mitigate granularity gaps between
modalities, MedM2T uses modality-specific pre-trained encoders and aligns
resulting features within a shared encoder. We evaluated MedM2T on MIMIC-IV and
MIMIC-IV-ECG datasets for three tasks that encompass chronic and acute disease
dynamics: 90-day cardiovascular disease (CVD) prediction, in-hospital mortality
prediction, and ICU length-of-stay (LOS) regression. MedM2T outperformed
state-of-the-art multimodal learning frameworks and existing time series
models, achieving an AUROC of 0.947 and an AUPRC of 0.706 for CVD prediction;
an AUROC of 0.901 and an AUPRC of 0.558 for mortality prediction; and Mean
Absolute Error (MAE) of 2.31 for LOS regression. These results highlight the
robustness and broad applicability of MedM2T, positioning it as a promising
tool in clinical prediction. We provide the implementation of MedM2T at
https://github.com/DHLab-TSENG/MedM2T.

</details>


### [39] [Reasoning Models Sometimes Output Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)
*Arun Jose*

Main category: cs.LG

TL;DR: 基于结果导向的强化学习（outcome-based RL）使链式推理（CoT）变得不透明，监控难以读懂；强制仅采用可读部分显著降低正确性（约53%），但可读性与性能关系在重采样中并不简单。结论：若不显式优化可读性， outcome-based RL 可能产生更隐蔽的推理，削弱监控效果。


<details>
  <summary>Details</summary>
Motivation: 探究CoT可读性对监控与安全的影响，以及为何基于结果的RL会促成更隐蔽的推理过程，从而影响对模型意图的理解与潜在恶意行为的检测。

Method: 对14个推理模型进行实证评估，比较人类与AI监控者对CoT的可读性评估与最终答案的可读性；在强制条件下仅使用可读部分、进行重采样以及难度对比等实验以揭示可读性与性能的关系与潜在机制；分析如隐写、训练伪影、残留标记等可能原因。

Result: 除了少数模型（如 Claude）外，大多数模型的CoT不可读，即使最终答案可读；若强制仅使用“可读”部分，准确性下降约53%；在重采样条件下，CoT的可读性与性能并无简单相关性；难度提升时CoT的可读性进一步下降；提出的解释包括隐写、训练伪影、 vestigial tokens等。

Conclusion: 在未对可读性进行显式优化的情况下，基于结果的RL倾向使推理过程变得越来越不透明，可能削弱对模型意图与潜在恶意行为的监控，需要探索显式优化目标、监控友好的训练策略与更强的检测方法。

Abstract: Language models trained via outcome-based reinforcement learning (RL) to
reason using chain-of-thought (CoT) have shown remarkable performance.
Monitoring such a model's CoT may allow us to understand its intentions and
detect potential malicious behavior. However, to be effective, this requires
that CoTs are legible and faithful. We study CoT legibility across 14 reasoning
models, finding that RL often causes reasoning to become illegible to both
humans and AI monitors, with reasoning models (except Claude) generating
illegible CoTs while returning to perfectly readable final answers. We show
that models use illegible reasoning to reach correct answers (accuracy dropping
by 53\% when forced to use only legible portions), yet find no correlation
between legibility and performance when resampling - suggesting the
relationship is more nuanced. We also find that legibility degrades on harder
questions. We discuss potential hypotheses for these results, including
steganography, training artifacts, and vestigial tokens. These results suggest
that without explicit optimization for legibility, outcome-based RL naturally
produces models with increasingly opaque reasoning processes, potentially
undermining monitoring approaches.

</details>


### [40] [FedMuon: Accelerating Federated Learning with Matrix Orthogonalization](https://arxiv.org/abs/2510.27403)
*Junkang Liu,Fanhua Shang,Junchao Zhou,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: 提出 FedMuon，结合局部矩阵正交化与全局对齐，解决非IID 环境下的客户端漂移问题，以实现更快的收敛和更少的通信轮次。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，通信开销是核心瓶颈，现有以元素级优化器（如 Adam/SGD）为主的局部更新难以考虑权重矩阵的几何结构，易放大病理方向，降低条件数并拖慢收敛。需要引入对矩阵结构有感知的优化方法以提升通信效率。

Method: 提出 Muon 局部优化器，具备矩阵正交化以优化矩阵结构化参数。为非 IID 场景提出 FedMuon，核心在于：1) 动量聚合用于本地初始化，2) 局部–全局对齐使局部梯度与全局更新方向对齐以缓解客户端漂移。理论上证明在没有异质性假设时，FedMuon 具线性加速收敛率；经验上在语言和视觉模型上验证，述比较基线显著减少通信轮次并提升测试精度。

Result: 在 IID 设置下，Local Muon 能显著加速 FL 收敛并减少相对 Local SGD/Local AdamW 的通信轮次；在非 IID 设置中，基于本地分布的独立矩阵正交化会引发强漂移。为此提出 FedMuon，通过动量聚合实现本地初始化与局部–全局对齐显著降低漂移，理论上实现无异质性假设下的线性速度提升，并在语言与视觉任务上实证验证，相较多基线显著降低通信轮次并提升测试准确性。

Conclusion: FedMuon 为联邦学习提供一组有效策略以处理矩阵结构参数的局部更新问题，能够在理论和实证层面实现更快的收敛和更少的通信开销，且通过局部–全局对齐有效缓解非 IID 情况下的漂移。

Abstract: The core bottleneck of Federated Learning (FL) lies in the communication
rounds. That is, how to achieve more effective local updates is crucial for
reducing communication rounds. Existing FL methods still primarily use
element-wise local optimizers (Adam/SGD), neglecting the geometric structure of
the weight matrices. This often leads to the amplification of pathological
directions in the weights during local updates, leading deterioration in the
condition number and slow convergence. Therefore, we introduce the Muon
optimizer in local, which has matrix orthogonalization to optimize
matrix-structured parameters. Experimental results show that, in IID setting,
Local Muon significantly accelerates the convergence of FL and reduces
communication rounds compared to Local SGD and Local AdamW. However, in non-IID
setting, independent matrix orthogonalization based on the local distributions
of each client induces strong client drift. Applying Muon in non-IID FL poses
significant challenges: (1) client preconditioner leading to client drift; (2)
moment reinitialization. To address these challenges, we propose a novel
Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1)
momentum aggregation, where clients use the aggregated momentum for local
initialization; (2) local-global alignment, where the local gradients are
aligned with the global update direction to significantly reduce client drift.
Theoretically, we prove that \texttt{FedMuon} achieves a linear speedup
convergence rate without the heterogeneity assumption, where $S$ is the number
of participating clients per round, $K$ is the number of local iterations, and
$R$ is the total number of communication rounds. Empirically, we validate the
effectiveness of FedMuon on language and vision models. Compared to several
baselines, FedMuon significantly reduces communication rounds and improves test
accuracy.

</details>


### [41] [Atlas-Alignment: Making Interpretability Transferable Across Language Models](https://arxiv.org/abs/2510.27413)
*Bruno Puri,Jim Berend,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Atlas-Alignment通过将未知潜在空间对齐到一个标注好的概念图谱（Concept Atlas），实现跨语言模型的可解释性迁移，利用有限的共享输入和轻量级的表示对齐方法，不需要带标签的概念数据，即可进行语义特征检索和受人类可控的生成。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释性管线成本高、难以扩展。为新模型实现可解释性往往需要训练特定的稀疏自编码器、标注及验证，难以规模化。希望通过一次性的高质量概念图谱，实现对多模型的可解释性迁移和可控性。

Method: 提出Atlas-Alignment框架，通过与Concept Atlas进行表示层对齐，将未知潜在空间映射到标注的人类可解释潜在空间。利用共享输入和轻量级的表示对齐技术，且不需要有标签的概念数据。

Result: 简单的表示对齐方法即可实现稳健的语义检索和可控的生成，且在量化与质性评估中表现良好，跨模型适用性强。

Conclusion: 因此可以摊薄可解释AI的成本：通过投资一个高质量的Concept Atlas，可以在最小边际成本下使多种模型变得透明和可控。

Abstract: Interpretability is crucial for building safe, reliable, and controllable
language models, yet existing interpretability pipelines remain costly and
difficult to scale. Interpreting a new model typically requires costly training
of model-specific sparse autoencoders, manual or semi-automated labeling of SAE
components, and their subsequent validation. We introduce Atlas-Alignment, a
framework for transferring interpretability across language models by aligning
unknown latent spaces to a Concept Atlas - a labeled, human-interpretable
latent space - using only shared inputs and lightweight representational
alignment techniques. Once aligned, this enables two key capabilities in
previously opaque models: (1) semantic feature search and retrieval, and (2)
steering generation along human-interpretable atlas concepts. Through
quantitative and qualitative evaluations, we show that simple representational
alignment methods enable robust semantic retrieval and steerable generation
without the need for labeled concept data. Atlas-Alignment thus amortizes the
cost of explainable AI and mechanistic interpretability: by investing in one
high-quality Concept Atlas, we can make many new models transparent and
controllable at minimal marginal cost.

</details>


### [42] [MVeLMA: Multimodal Vegetation Loss Modeling Architecture for Predicting Post-fire Vegetation Loss](https://arxiv.org/abs/2510.27443)
*Meenu Ravi,Shailik Sarkar,Yanshen Sun,Vaishnavi Singh,Chang-Tien Lu*

Main category: cs.LG

TL;DR: 提出了 MVeLMA，多模态融合的堆叠集成架构，用于预测县级野火后植被损失，包含不确定性估计和置信度地图，实验显示优于SOTA，结果可用于灾后恢复和政策决策。


<details>
  <summary>Details</summary>
Motivation: 解决野火后植被损失随时间演化难以捕捉的问题；当前研究在因素、模态及其交互方面未充分探索，且缺乏可解释性和不确定性估计，限制实际应用。

Method: 提出端到端的多模态植被损失建模架构 MVeLMA，使用多模态特征融合、堆叠集成、概率建模实现不确定性，并生成置信度/风险地图；具体实现细节包括模态集成、基于不确定性的输出、以及县级预测。

Result: 实验表明 MVeLMA 在预测野火后植被损失方面优于多种SOTA与基线方法；生成的置信度地图用于识别高风险县，支持有针对性的恢复工作。

Conclusion: 该方法有潜力为灾害救助规划、生态政策制定与野生动物恢复管理提供可解释且带不确定性的信息，推动生态预测向实用化方向发展。

Abstract: Understanding post-wildfire vegetation loss is critical for developing
effective ecological recovery strategies and is often challenging due to the
extended time and effort required to capture the evolving ecosystem features.
Recent works in this area have not fully explored all the contributing factors,
their modalities, and interactions with each other. Furthermore, most research
in this domain is limited by a lack of interpretability in predictive modeling,
making it less useful in real-world settings. In this work, we propose a novel
end-to-end ML pipeline called MVeLMA (\textbf{M}ultimodal \textbf{Ve}getation
\textbf{L}oss \textbf{M}odeling \textbf{A}rchitecture) to predict county-wise
vegetation loss from fire events. MVeLMA uses a multimodal feature integration
pipeline and a stacked ensemble-based architecture to capture different
modalities while also incorporating uncertainty estimation through
probabilistic modeling. Through comprehensive experiments, we show that our
model outperforms several state-of-the-art (SOTA) and baseline models in
predicting post-wildfire vegetation loss. Furthermore, we generate vegetation
loss confidence maps to identify high-risk counties, thereby helping targeted
recovery efforts. The findings of this work have the potential to inform future
disaster relief planning, ecological policy development, and wildlife recovery
management.

</details>


### [43] [Simplex-to-Euclidean Bijections for Categorical Flow Matching](https://arxiv.org/abs/2510.27480)
*Bernardo Williams,Victor M. Yeom-Song,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 提出一种在简单锥体上学习与采样的方法，通过将开锥面映射到欧几里得空间的平滑双射，结合Aitchison几何并用Dirichlet插值将离散观测去量化为连续数据，在保持Aitchison几何的同时在欧氏空间建模密度，并能对原始离散分布进行精确恢复。


<details>
  <summary>Details</summary>
Motivation: 在简单锥体（simplex）上进行密度估计和采样时，传统方法往往依赖于黎曼几何或自定义噪声过程，计算复杂且可能失去几何正确性。本工作旨在提供一个在欧几里得空间中进行建模的替代方案，同时尊重Aitchison几何的结构，以提高方法的可实现性与性能。

Method: 构造从开简单锥面到欧几里得空间的光滑双射，利用Aitchison几何定义映射；通过Dirichlet插值实现对离散观测的去量化，得到连续的观测；在欧几里得空间进行密度建模并通过双射映射回简单锥面，从而实现对原始离散分布的精确恢复。

Result: 在仿真与真实数据集上，与在简单锥面上使用Riemann几何或自定义噪声过程的方法相比，该方法在保持Aitchison几何的前提下在欧几里得空间进行建模，表现具竞争力。并且实现了对离散分布的精确恢复。

Conclusion: 提供一种在保持Aitchison几何的同时，在欧几里得空间进行密度学习和采样的实用替代方案，简化实现并在多种数据集上显示出强健的性能。

Abstract: We propose a method for learning and sampling from probability distributions
supported on the simplex. Our approach maps the open simplex to Euclidean space
via smooth bijections, leveraging the Aitchison geometry to define the
mappings, and supports modeling categorical data by a Dirichlet interpolation
that dequantizes discrete observations into continuous ones. This enables
density modeling in Euclidean space through the bijection while still allowing
exact recovery of the original discrete distribution. Compared to previous
methods that operate on the simplex using Riemannian geometry or custom noise
processes, our approach works in Euclidean space while respecting the Aitchison
geometry, and achieves competitive performance on both synthetic and real-world
data sets.

</details>


### [44] [Thought Branches: Interpreting LLM Reasoning Requires Resampling](https://arxiv.org/abs/2510.27484)
*Uzay Macar,Paul C. Bogdan,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 通过对多样化推理路径进行重采样（resampling）来分析模型的因果推理，从而获得对CoT分布的理解、可控干预及更清晰的决策叙事。


<details>
  <summary>Details</summary>
Motivation: 单一CoT样本无法充分揭示推理的因果影响与内部计算；把推理分布系统性理解需要对其进行采样与分析。

Method: 通过重采样来评估句子级原因对行动的因果影响、比较离策略编辑与在策略内（on-policy）干预、引入韧性指标以防止内容重复、结合因果中介分析来探索未被明确提及的线索对输出的影响。

Result: 自我保护句子对行动的因果影响小；离策略干预效果小且不稳定；韧性评估显示关键规划语句在去除时抵抗力强，但完全移除时影响显著；隐性提示对输出有因果效应且持续影响CoT，即使被移除也有残留效应。

Conclusion: 通过重采样研究分布可以实现更可靠的因果分析、清晰的推理叙事及更有原则的CoT干预。

Abstract: Most work interpreting reasoning models studies only a single
chain-of-thought (CoT), yet these models define distributions over many
possible CoTs. We argue that studying a single sample is inadequate for
understanding causal influence and the underlying computation. Though fully
specifying this distribution is intractable, it can be understood by sampling.
We present case studies using resampling to investigate model decisions. First,
when a model states a reason for its action, does that reason actually cause
the action? In "agentic misalignment" scenarios, we resample specific sentences
to measure their downstream effects. Self-preservation sentences have small
causal impact, suggesting they do not meaningfully drive blackmail. Second, are
artificial edits to CoT sufficient for steering reasoning? These are common in
literature, yet take the model off-policy. Resampling and selecting a
completion with the desired property is a principled on-policy alternative. We
find off-policy interventions yield small and unstable effects compared to
resampling in decision-making tasks. Third, how do we understand the effect of
removing a reasoning step when the model may repeat it post-edit? We introduce
a resilience metric that repeatedly resamples to prevent similar content from
reappearing downstream. Critical planning statements resist removal but have
large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can
our methods teach us anything in these settings? Adapting causal mediation
analysis, we find that hints that have a causal effect on the output without
being explicitly mentioned exert a subtle and cumulative influence on the CoT
that persists even if the hint is removed. Overall, studying distributions via
resampling enables reliable causal analysis, clearer narratives of model
reasoning, and principled CoT interventions.

</details>


### [45] [FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models](https://arxiv.org/abs/2510.27486)
*Junkang Liu,Fanhua Shang,Kewen Zhu,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: 提出联邦AdamW变体FedAdamW，结合本地校正与解耦权重衰减，以降低方差、对齐本地与全局更新，并在无异质性假设下实现线性加速的收敛，同时显著减少通信轮次并提升测试精度；代码公开。


<details>
  <summary>Details</summary>
Motivation: 直接在联邦学习中使用AdamW面临挑战：数据异质性导致二阶矩估计v的方差较大；局部过拟合造成客户端漂移；每轮重新初始化动量估计（m,v）会拖慢收敛。

Method: 提出FedAdamW：通过本地校正机制使本地更新与全局更新对齐，采用解耦权重衰减以缓解局部过拟合；高效聚合第二矩估计均值以降低方差并重新初始化v。给出理论收敛性：在不依赖异质性假设下，收敛速率为O(sqrt((L Δ σ_l^2)/(S K R ε^2))+(L Δ)/R)，并利用PAC-Bayesian分析解释本地训练中解耦权重衰减的有效性。

Result: 在语言与视觉Transformer模型上对比基线证明了FedAdamW能显著减少通信轮次并提升测试准确率。

Conclusion: FedAdamW为在FL中训练与微调大模型提供了有效解决方案，解决了三大挑战并给出理论与经验支持，代码在GitHub公开。

Abstract: AdamW has become one of the most effective optimizers for training
large-scale models. We have also observed its effectiveness in the context of
federated learning (FL). However, directly applying AdamW in federated learning
settings poses significant challenges: (1) due to data heterogeneity, AdamW
often yields high variance in the second-moment estimate $\boldsymbol{v}$; (2)
the local overfitting of AdamW may cause client drift; and (3) Reinitializing
moment estimates ($\boldsymbol{v}$, $\boldsymbol{m}$) at each round slows down
convergence. To address these challenges, we propose the first
\underline{Fed}erated \underline{AdamW} algorithm, called \texttt{FedAdamW},
for training and fine-tuning various large models. \texttt{FedAdamW} aligns
local updates with the global update using both a \textbf{local correction
mechanism} and decoupled weight decay to mitigate local overfitting.
\texttt{FedAdamW} efficiently aggregates the \texttt{mean} of the second-moment
estimates to reduce their variance and reinitialize them. Theoretically, we
prove that \texttt{FedAdamW} achieves a linear speedup convergence rate of
$\mathcal{O}(\sqrt{(L \Delta \sigma_l^2)/(S K R \epsilon^2)}+(L \Delta)/R)$
without \textbf{heterogeneity assumption}, where $S$ is the number of
participating clients per round, $K$ is the number of local iterations, and $R$
is the total number of communication rounds. We also employ PAC-Bayesian
generalization analysis to explain the effectiveness of decoupled weight decay
in local training. Empirically, we validate the effectiveness of
\texttt{FedAdamW} on language and vision Transformer models. Compared to
several baselines, \texttt{FedAdamW} significantly reduces communication rounds
and improves test accuracy. The code is available in
https://github.com/junkangLiu0/FedAdamW.

</details>


### [46] [DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm](https://arxiv.org/abs/2510.27504)
*Junkang Liu,Yuxuan Tian,Fanhua Shang,Yuanyuan Liu,Hongying Liu,Junchao Zhou,Daorui Ding*

Main category: cs.LG

TL;DR: 提出 DP-FedPGN，在本地损失中加入全局梯度范数惩罚以寻找全局扁平极小点，缓解差分隐私下的性能下降并降低梯度裁剪误差，同时对数据异质性具鲁棒性、收敛更快，在 ResNet 与 Transformer 的六项视觉和 NLP 任务上显著优于现有方法，代码公开。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中应用差分隐私保护时，局部平整性不等于全局平整性，单纯依赖局部的 Sharpness Aware Minimization 可能无法充分缓解 DP 造成的泛化下降；需要一种能追求全局扁平极小点的训练目标，以提升鲁棒性和泛化。

Method: 提出 DP-FedPGN 算法，在局部损失中引入全局梯度范数惩罚，旨在引导优化找到全局扁平极小点；通过该惩罚同时降低局部更新范数，减小梯度裁剪误差；结合 Rényi DP 提供严格隐私保证，并进行局部更新的敏感性分析；理论分析表明方法缓解 DP 的性能损失、对数据异质性不敏感且收敛更快。

Result: 在 ResNet 与 Transformer 上的六个视觉与 NLP 任务中，DP-FedPGN 相比现有方法取得显著提升。

Conclusion: DP-FedPGN 能有效缓解差分隐私保护下的性能下降，提升全局扁平性、鲁棒性和收敛速度，具有理论隐私保障和广泛适用性。

Abstract: To prevent inference attacks in Federated Learning (FL) and reduce the
leakage of sensitive information, Client-level Differentially Private Federated
Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually
result in sharper loss landscapes, which leads to a decrease in model
generalization after differential privacy protection. By using Sharpness Aware
Minimization (SAM), the current popular federated learning methods are to find
a local flat minimum value to alleviate this problem. However, the local
flatness may not reflect the global flatness in CL-DPFL. Therefore, to address
this issue and seek global flat minima of models, we propose a new CL-DPFL
algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to
the local loss to find the global flat minimum. Moreover, by using our global
gradient norm penalty, we not only find a flatter global minimum but also
reduce the locally updated norm, which means that we further reduce the error
of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN
mitigates the performance degradation caused by DP. Meanwhile, the proposed
DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves
fast convergence. We also use R\'enyi DP to provide strict privacy guarantees
and provide sensitivity analysis for local updates. Finally, we conduct
effectiveness tests on both ResNet and Transformer models, and achieve
significant improvements in six visual and natural language processing tasks
compared to existing state-of-the-art algorithms. The code is available at
https://github.com/junkangLiu0/DP-FedPGN

</details>


### [47] [Leveraging Generic Time Series Foundation Models for EEG Classification](https://arxiv.org/abs/2510.27522)
*Théo Gnassounou,Yessin Moakher,Shifeng Xie,Vasilii Feofanov,Ievgen Redko*

Main category: cs.LG

TL;DR: Time series foundation models pretrained on real or synthetic data can transfer to EEG tasks (motor imagery, sleep stage), outperforming EEGNet and CBraMod.


<details>
  <summary>Details</summary>
Motivation: Explore whether generalist time series foundation models pretrained on non-EEG data can transfer to EEG domains, addressing potential cross-domain benefits for brain signal analysis.

Method: Evaluate a recently proposed time series classification foundation model on EEG tasks (motor imagery classification and sleep stage prediction). Compare two pretraining regimes: (a) heterogeneous real-world time series pretraining from multiple domains, (b) purely synthetic data pretraining. Baselines include EEGNet and CBraMod.

Result: Both pretraining variants yield strong performance and consistently outperform EEGNet and CBraMod, indicating effective cross-domain transfer to EEG.

Conclusion: Generalist time series foundation models pretrained across domains can transfer to EEG, supporting the potential to leverage broader time series advances for brain signal analysis.

Abstract: Foundation models for time series are emerging as powerful general-purpose
backbones, yet their potential for domain-specific biomedical signals such as
electroencephalography (EEG) remains rather unexplored. In this work, we
investigate the applicability a recently proposed time series classification
foundation model, to a different EEG tasks such as motor imagery classification
and sleep stage prediction. We test two pretraining regimes: (a) pretraining on
heterogeneous real-world time series from multiple domains, and (b) pretraining
on purely synthetic data. We find that both variants yield strong performance,
consistently outperforming EEGNet, a widely used convolutional baseline, and
CBraMod, the most recent EEG-specific foundation model. These results suggest
that generalist time series foundation models, even when pretrained on data of
non-neural origin or on synthetic signals, can transfer effectively to EEG. Our
findings highlight the promise of leveraging cross-domain pretrained models for
brain signal analysis, suggesting that EEG may benefit from advances in the
broader time series literature.

</details>


### [48] [Active transfer learning for structural health monitoring](https://arxiv.org/abs/2510.27525)
*J. Poole,N. Dervilis,K. Worden,P. Gardner,V. Giglioni,R. S. Mills,A. J. Hughes*

Main category: cs.LG

TL;DR: 在 PBSHM 中引入一个贝叶斯域适应框架，并结合主动学习，将传输学习与在线标注结合起来，以提高数据效率、并在在线更新目标分类器时减少标注需求和检查次数。


<details>
  <summary>Details</summary>
Motivation: SHM 数据标注成本高且难以获取；不同结构的数据分布差异会导致泛化误差增大；现有大多无监督域适应且缺乏在线更新能力，因此需要一种数据高效的在线学习方法来支撑多结构健康监测。

Method: 提出一个用于 PBSHM 的贝叶斯域适应模型，利用有限的目标标注数据来提升无监督映射的效果；将该模型嵌入一个主动采样策略中，以选择最具信息量的观测值进行标注，从而在监测过程中实现在线更新与自适应。

Result: 在包含多种损伤状态和环境条件的实验桥梁群上进行评估，发现结合传输学习和主动学习能够在标注数据稀缺的情形下提升数据效率，并减少对目标分类器的标注需求。

Conclusion: 将贝叶斯域适应与主动学习结合应用于 PBSHM，可显著提升数据效率、降低现场检查次数与运营成本，并实现对结构群体的数据驱动运行维护。

Abstract: Data for training structural health monitoring (SHM) systems are often
expensive and/or impractical to obtain, particularly for labelled data.
Population-based SHM (PBSHM) aims to address this limitation by leveraging data
from multiple structures. However, data from different structures will follow
distinct distributions, potentially leading to large generalisation errors for
models learnt via conventional machine learning methods. To address this issue,
transfer learning -- in the form of domain adaptation (DA) -- can be used to
align the data distributions. Most previous approaches have only considered
\emph{unsupervised} DA, where no labelled target data are available; they do
not consider how to incorporate these technologies in an online framework --
updating as labels are obtained throughout the monitoring campaign. This paper
proposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DA
mappings using a limited quantity of labelled target data. In addition, this
model is integrated into an active sampling strategy to guide inspections to
select the most informative observations to label -- leading to further
reductions in the required labelled data to learn a target classifier. The
effectiveness of this methodology is evaluated on a population of experimental
bridges. Specifically, this population includes data corresponding to several
damage states, as well as, a comprehensive set of environmental conditions. It
is found that combining transfer learning and active learning can improve data
efficiency when learning classification models in label-scarce scenarios. This
result has implications for data-informed operation and maintenance of
structures, suggesting a reduction in inspections over the operational lifetime
of a structure -- and therefore a reduction in operational costs -- can be
achieved.

</details>


### [49] [TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control](https://arxiv.org/abs/2510.27527)
*Yuxiang Chen,Xiaoming Xu,Pengle Zhang,Michael Beyer,Martin Rapp,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: TetraJet-v2提出一个端到端4位FQT方法，基于NVFP4实现线性层的激活、权重和梯度全量化训练。通过引入双区块量化的无偏化方法、OsciReset抑制权值振荡、OutControl保持离群值精度来解决低精度训练中的权重振荡和离群值问题。在370M参数、200B token规模的LLM预训练任务上，显著优于早期FP4方法，平均缩小与全精度训练的性能差距约51.3%。


<details>
  <summary>Details</summary>
Motivation: LLM训练代价极高，迫切需要低精度训练方案。尽管4位格式（如NVFP4）带来显著效率提升，但实现近损失无关的训练仍面临权重振荡和离群值等挑战，因此需要端到端的4位全量化训练方法.

Method: 提出端到端的NVFP4 4-bit全量化训练框架TetraJet-v2，覆盖激活、权重和梯度在所有线性层；引入无偏双区块量化以提升线性层的量化稳定性；设计OsciReset以抑制权重振荡；设计OutControl以保持离群值精度。

Result: 在多种模型规模（最高370M参数）和数据规模（最高200B tokens）的预训练任务中，TetraJet-v2优于此前的FP4训练方法，平均将与全精度训练的性能差距缩小约51.3%。

Conclusion: 端到端4位全量化训练在LLM预训练中具备可行性，并通过新的量化与正则策略显著改善了低精度训练的稳定性与精度接近性。

Abstract: Large Language Models (LLMs) training is prohibitively expensive, driving
interest in low-precision fully-quantized training (FQT). While novel 4-bit
formats like NVFP4 offer substantial efficiency gains, achieving near-lossless
training at such low precision remains challenging. We introduce TetraJet-v2,
an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,
and gradients in all linear layers. We identify two critical issues hindering
low-precision LLM training: weight oscillation and outliers. To address these,
we propose: 1) an unbiased double-block quantization method for NVFP4 linear
layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)
OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently
outperforms prior FP4 training methods on pre-training LLMs across varying
model sizes up to 370M and data sizes up to 200B tokens, reducing the
performance gap to full-precision training by an average of 51.3%.

</details>


### [50] [AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for Proprietary Data Challenges in Financial Question Answering](https://arxiv.org/abs/2510.27537)
*Mohammad Zahangir Alam,Khandoker Ashik Uz Zaman,Mahdi H. Miraz*

Main category: cs.LG

TL;DR: 提出AstuteRAG-FQA，一种面向金融问答的自适应RAG框架，结合混合检索、任务感知提示、四层级任务分类与严格的安全合规机制，并评估三种数据集成技术的可行性。


<details>
  <summary>Details</summary>
Motivation: 在知识密集型任务中，RAG 能提升领域专用性和时效相关性、降低幻觉，但金融领域面临对专有数据的受限访问、检索精度、监管合规和敏感数据处理等挑战。

Method: 提出自适应提示框架与混合检索策略（整合公开与专有金融数据，确保安全合规）、四层级任务分类（显性事实、隐性事实、可解释的推理、隐藏推理）、多层安全机制（差分隐私、数据匿名化、基于角色的访问控制）、实时合规监控与自动监管验证系统、以及对三种数据集成技术（上下文嵌入、小模型增强、定向微调）的评估。

Result: 提出评估框架，分析三种数据集成技术在不同金融环境中的效率与可行性；未给出具体量化结果。

Conclusion: 框架具备提升FQA的精确性、情境相关性与合规性的潜力，但需在实际应用中进一步实验验证与实证数据支持。

Abstract: Retrieval-Augmented Generation (RAG) shows significant promise in
knowledge-intensive tasks by improving domain specificity, enhancing temporal
relevance, and reducing hallucinations. However, applying RAG to finance
encounters critical challenges: restricted access to proprietary datasets,
limited retrieval accuracy, regulatory constraints, and sensitive data
interpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored
for Financial Question Answering (FQA), leveraging task-aware prompt
engineering to address these challenges. The framework uses a hybrid retrieval
strategy integrating both open-source and proprietary financial data while
maintaining strict security protocols and regulatory compliance. A dynamic
prompt framework adapts in real time to query complexity, improving precision
and contextual relevance. To systematically address diverse financial queries,
we propose a four-tier task classification: explicit factual, implicit factual,
interpretable rationale, and hidden rationale involving implicit causal
reasoning. For each category, we identify key challenges, datasets, and
optimization techniques within the retrieval and generation process. The
framework incorporates multi-layered security mechanisms including differential
privacy, data anonymization, and role-based access controls to protect
sensitive financial information. Additionally, AstuteRAG-FQA implements
real-time compliance monitoring through automated regulatory validation systems
that verify responses against industry standards and legal obligations. We
evaluate three data integration techniques - contextual embedding, small model
augmentation, and targeted fine-tuning - analyzing their efficiency and
feasibility across varied financial environments.

</details>


### [51] [ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling](https://arxiv.org/abs/2510.27610)
*Zhuohan Wang,Ziwei Zhu,Ziniu Li,Congliang Chen,Yizhou Han,Yufeng Lin,Zhihang Lin,Angyang Gu,Xinglin Hu,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Formulating optimization problems for industrial applications demands
significant manual effort and domain expertise. While Large Language Models
(LLMs) show promise in automating this process, evaluating their performance
remains difficult due to the absence of robust metrics. Existing solver-based
approaches often face inconsistency, infeasibility issues, and high
computational costs. To address these issues, we propose ORGEval, a
graph-theoretic evaluation framework for assessing LLMs' capabilities in
formulating linear and mixed-integer linear programs. ORGEval represents
optimization models as graphs, reducing equivalence detection to graph
isomorphism testing. We identify and prove a sufficient condition, when the
tested graphs are symmetric decomposable (SD), under which the
Weisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.
Building on this, ORGEval integrates a tailored variant of the WL-test with an
SD detection algorithm to evaluate model equivalence. By focusing on structural
equivalence rather than instance-level configurations, ORGEval is robust to
numerical variations. Experimental results show that our method can
successfully detect model equivalence and produce 100\% consistent results
across random parameter configurations, while significantly outperforming
solver-based methods in runtime, especially on difficult problems. Leveraging
ORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs
on optimization modeling. Our results reveal that although optimization
modeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4
achieve the highest accuracies under direct prompting, outperforming even
leading reasoning models.

</details>


### [52] [Panprediction: Optimal Predictions for Any Downstream Task and Loss](https://arxiv.org/abs/2510.27638)
*Sivaraman Balakrishnan,Nika Haghtalab,Daniel Hsu,Brian Lee,Eric Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Supervised learning is classically formulated as training a model to minimize
a fixed loss function over a fixed distribution, or task. However, an emerging
paradigm instead views model training as extracting enough information from
data so that the model can be used to minimize many losses on many downstream
tasks. We formalize a mathematical framework for this paradigm, which we call
panprediction, and study its statistical complexity. Formally, panprediction
generalizes omniprediction and sits upstream from multi-group learning, which
respectively focus on predictions that generalize to many downstream losses or
many downstream tasks, but not both. Concretely, we design algorithms that
learn deterministic and randomized panpredictors with
$\tilde{O}(1/\varepsilon^3)$ and $\tilde{O}(1/\varepsilon^2)$ samples,
respectively. Our results demonstrate that under mild assumptions,
simultaneously minimizing infinitely many losses on infinitely many tasks can
be as statistically easy as minimizing one loss on one task. Along the way, we
improve the best known sample complexity guarantee of deterministic
omniprediction by a factor of $1/\varepsilon$, and match all other known sample
complexity guarantees of omniprediction and multi-group learning. Our key
technical ingredient is a nearly lossless reduction from panprediction to a
statistically efficient notion of calibration, called step calibration.

</details>


### [53] [Imbalanced Classification through the Lens of Spurious Correlations](https://arxiv.org/abs/2510.27650)
*Jakob Hackstein,Sidney Bender*

Main category: cs.LG

TL;DR: 在数据不平衡条件下，通过对比性解释识别并消除Clever Hans效应，从而提升分类鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡不仅削弱少数类性能，也可能通过欠拟合/过拟合引入可解释性偏差的Clever Hans效应。现有数据重加权常忽略解释层面的不一致。

Method: 提出基于对照性（counterfactual）解释的可解释AI框架，联合识别并消除在不平衡条件下出现的CH效应。通过生成与目标分布对照的解释来纠正模型决策。

Result: 在三个数据集上获得与基线相当甚至有竞争力的分类性能，并揭示不平衡条件下CH效应的出现机制，提供新的解释性视角。

Conclusion: 方法不仅改善鲁棒性，还揭示不平衡下CH效应的根源与可解释性问题，为改进不平衡学习提供新的分析框架。

Abstract: Class imbalance poses a fundamental challenge in machine learning, frequently
leading to unreliable classification performance. While prior methods focus on
data- or loss-reweighting schemes, we view imbalance as a data condition that
amplifies Clever Hans (CH) effects by underspecification of minority classes.
In a counterfactual explanations-based approach, we propose to leverage
Explainable AI to jointly identify and eliminate CH effects emerging under
imbalance. Our method achieves competitive classification performance on three
datasets and demonstrates how CH effects emerge under imbalance, a perspective
largely overlooked by existing approaches.

</details>


### [54] [Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition](https://arxiv.org/abs/2510.27651)
*Shuyan Lyu,Zhanzimo Wu,Junliang Du*

Main category: cs.LG

TL;DR: 通过信息论分析与DIB层级训练，提出基于Deterministic Information Bottleneck的分层训练，并辅以辅助分类器，在CIFAR-10/100及交通标志识别上实现与SGD相当的性能，且优于现有分层训练基线。


<details>
  <summary>Details</summary>
Motivation: 受限于全局端到端训练的生物可观性、内存与梯度问题，层级化训练提供更高效、可扩展的替代方案；通过信息瓶颈理解CNN在层级间的信息流与学习动力学，促使设计更少依赖中间梯度的训练策略。

Method: 首先用信息理论视角分析用SGD训练的CNN的学习动力学，发现从底层到顶层逐层收敛，信息流遵循马尔可夫信息瓶颈。基于此提出基于DIB和矩阵基 Rényi 熵的层级训练方法：每一层与直接连到输出层的辅助分类器共同训练，目标是学习最小充分的任务相关表征。

Result: 在CIFAR-10/100上使用现代深度CNN进行验证，并在交通标志识别等实际任务中证明可行性。该方法不仅在现有分层训练基线中具备优越性，还达到与SGD相当的性能。

Conclusion: 以DIB为核心的分层训练在理论与实证上提供了可行的替代端到端训练路径，能在保持竞争力的同时降低对中间梯度与存储的需求，具有良好的扩展性。

Abstract: Modern deep neural networks (DNNs) are typically trained with a global
cross-entropy loss in a supervised end-to-end manner: neurons need to store
their outgoing weights; training alternates between a forward pass
(computation) and a top-down backward pass (learning) which is biologically
implausible. Alternatively, greedy layer-wise training eliminates the need for
cross-entropy loss and backpropagation. By avoiding the computation of
intermediate gradients and the storage of intermediate outputs, it reduces
memory usage and helps mitigate issues such as vanishing or exploding
gradients. However, most existing layer-wise training approaches have been
evaluated only on relatively small datasets with simple deep architectures. In
this paper, we first systematically analyze the training dynamics of popular
convolutional neural networks (CNNs) trained by stochastic gradient descent
(SGD) through an information-theoretic lens. Our findings reveal that networks
converge layer-by-layer from bottom to top and that the flow of information
adheres to a Markov information bottleneck principle. Building on these
observations, we propose a novel layer-wise training approach based on the
recently developed deterministic information bottleneck (DIB) and the
matrix-based R\'enyi's $\alpha$-order entropy functional. Specifically, each
layer is trained jointly with an auxiliary classifier that connects directly to
the output layer, enabling the learning of minimal sufficient task-relevant
representations. We empirically validate the effectiveness of our training
procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further
demonstrate its applicability to a practical task involving traffic sign
recognition. Our approach not only outperforms existing layer-wise training
baselines but also achieves performance comparable to SGD.

</details>


### [55] [Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems](https://arxiv.org/abs/2510.27659)
*Alireza Saleh Abadi,Leen-Kiat Soh*

Main category: cs.LG

TL;DR: 提出在开放性MARL中对信用分配问题的概念与经验分析，指出开放性会导致信用错配并削弱性能，需要新型CA/CAP方法。


<details>
  <summary>Details</summary>
Motivation: 随着个体进入/退出、任务变化以及能力变化，开放性破坏静态假设，对分配个体贡献的难度增加。

Method: 先进行概念分析，提出新亚类别的开放性；随后在开放环境中对代表性时序与结构性算法进行实证研究。

Result: 开放性直接导致信用错配，表现为损失函数不稳定、性能显著下降。

Conclusion: 需要将开放性因素纳入CAP/CA方法设计，发展鲁棒的信用分配机制和自适应的环境建模。

Abstract: In the rapidly evolving field of multi-agent reinforcement learning (MARL),
understanding the dynamics of open systems is crucial. Openness in MARL refers
to the dynam-ic nature of agent populations, tasks, and agent types with-in a
system. Specifically, there are three types of openness as reported in (Eck et
al. 2023) [2]: agent openness, where agents can enter or leave the system at
any time; task openness, where new tasks emerge, and existing ones evolve or
disappear; and type openness, where the capabil-ities and behaviors of agents
change over time. This report provides a conceptual and empirical review,
focusing on the interplay between openness and the credit assignment problem
(CAP). CAP involves determining the contribution of individual agents to the
overall system performance, a task that becomes increasingly complex in open
environ-ments. Traditional credit assignment (CA) methods often assume static
agent populations, fixed and pre-defined tasks, and stationary types, making
them inadequate for open systems. We first conduct a conceptual analysis,
in-troducing new sub-categories of openness to detail how events like agent
turnover or task cancellation break the assumptions of environmental
stationarity and fixed team composition that underpin existing CAP methods. We
then present an empirical study using representative temporal and structural
algorithms in an open environment. The results demonstrate that openness
directly causes credit misattribution, evidenced by unstable loss functions and
significant performance degradation.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [56] [Inferring the Chemotaxis Distortion Function from Cellular Decision Strategies](https://arxiv.org/abs/2510.26988)
*Fardad Vakilipoor,Johannes Konrad,Maximilian Schäfer*

Main category: cs.IT

TL;DR: 本文提出基于信息理论的框架，利用速率失真理论及反向Blahut-Arimoto算法来研究细胞在不确定环境下的决策策略，并在凋亡与趋化模型中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究在嘈杂信号与资源约束下，细胞如何进行高效的信息处理和决策。将信息论工具应用于生物系统，以揭示 sensing 的权衡与鲁棒性。

Method: 基于速率失真理论（RDT），利用Blahut-Arimoto算法（BAA）求解在给定失真约束下最小互信息的最优决策策略；提出反向Blahut-Arimoto算法（IBAA）以计算失真函数，量化从输入信号到输出的决策映射标准；通过LEGI模型模拟趋化响应并在细胞凋亡场景中验证失真函数的估计。

Result: 获得对失真函数的准确估计，揭示细胞的状态依赖性决策标准；通过LEGI模型推导出从细胞角度的失真函数，指示不同状态下的决策准则存在差异。

Conclusion: 该通用框架不仅适用于趋化，还可扩展到需要在不确定性下进行高效信息处理的生物与工程系统；IBAA为从系统层面推导和量化决策准则提供新工具。

Abstract: Cellular intelligence enables cells to process environmental signals and make
context-dependent decisions, as exemplified by chemotaxis, where cells navigate
chemical gradients despite noisy signaling pathways. To investigate how cells
deal with uncertainty, we apply an information-theoretic framework based on
rate distortion theory (RDT). The Blahut-Arimoto algorithm (BAA) computes
optimal decision strategies that minimize mutual information while satisfying
distortion constraints, balancing sensing accuracy with distortion constraint
equivalent to resource cost. We propose the inverse Blahut-Arimoto algorithm
(IBAA) to compute the distortion function, which quantifies the system's
decision-making criteria for realizing a decision strategy to map input signals
to outputs. This general framework extends beyond chemotaxis to biological and
engineered systems requiring efficient information processing under
uncertainty. We validate the proposed IBAA by accurately estimating theoretical
distortion functions in a cellular apoptosis scenario. Additionally, using the
local excitation global inhibition (LEGI) model to simulate chemotactic
responses, we compute the distortion functions from the cell's perspective. Our
finding reveals a state-dependent decision criteria by the cell.

</details>


### [57] [Secure Communication in the Presence of an RIS-Enhanced Eavesdropper in MIMO Networks](https://arxiv.org/abs/2510.27147)
*Gaoyuan Zhang,Ruisong Si,Boyuan Li,Zijian Li,Baofeng Ji,Chenqi Zhu,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 提出 RIS 支持的统一安全 MIMO 框架，针对可移动且可主动优化信道的被动窃听者，给出一款无需完整瞬时信道信息的轻量级信息论安全方案，并结合 SVD 预编码与 RIS 相位设计与检测策略在多场景下的仿真验证。


<details>
  <summary>Details</summary>
Motivation: 在 RIS 增强的无线环境中，窃听者可能移动且主动调整接收信号，传统安全方案在此情形下效果不足，因此需要无需全量瞬时信道信息的低复杂度保护机制。

Method: 构建一个通用的智能窃听模型，将数据处理视作一个传输通道；引入随机比特翻转以最小化秘密信息对窃听者的互信息；使用基于奇异值分解(SVD)的预编码优化功率分配，确保合法用户不受随机翻转影响；从窃听者角度设计 RIS 相位以应对不同攻击情景；给出合法用户和窃听者的最优检测方案；并通过大量仿真验证理论分析。

Result: 在不需要全局瞬时 CSI 的条件下，该方案有效降低信息泄露并具鲁棒性；在多种攻击场景下给出 RIS 设计策略且仿真结果与理论一致，验证了方案的有效性与鲁棒性。

Conclusion: 所提出的轻量级安全方案在 RIS 辅助的 MIMO 系统中具有实际可行性，且无需完整 ICSI 即可实现对高阶攻击的鲁棒防护。研究同时提供了对抗智能窃听的新思路及相关的 RIS 相位设计和检测策略。

Abstract: We pay our attention towards secure and robust communication in the presence
of a Reconfigurable Intelligent Surface (RIS)-enhanced mobile eavesdropping
attacker in Multiple-Input Multiple-Output (MIMO)wireless
networks.Specifically,we first provide a unifying framework that generalizes
specific intelligent wiretap model wherein the passive eavesdropper configured
with any number of antennas is potentially mobile and can actively optimize its
received signal strength with the help of RIS by intelligently manipulating
wiretap channel characteristics.To effectively mitigate this intractable
threat,we then propose a novel and lightweight secure communication scheme from
the perspective of information theory.The main idea is that the data processing
can in some cases be observed as communication channel,and a random
bit-flipping scheme is then carefully involved for the legitimate transmitter
to minimize the mutual information between the secret message and the passive
eavesdropper's received data.The Singular Value Decomposition (SVD)-based
precoding strategy is also implemented to optimize power allocation,and thus
ensure that the legitimate receiver is not subject to interference from this
random bit-flipping.The corresponding results depict that our secure
communication scheme is practically desired, which does not require any a prior
knowledge of the eavesdropper's full instantaneous Channel State Information
(ICSI). Furthermore,we consider the RIS optimization problem from the
eavesdropper's perspective,and provide RIS phase shift design solutions under
different attacking scenarios.Finally,the optimal detection schemes
respectively for the legitimate user and the eavesdropper are provided,and
comprehensive simulations are presented to verify our theoretical analysis and
show the effectiveness and robustness of our secure communication scheme across
a wide range of attacking scenarios.

</details>


### [58] [Byzantine Attacks in RIS-Enhanced Cooperative Spectrum Sensing: A Decision Fusion Perspective](https://arxiv.org/abs/2510.27175)
*Gaoyuan Zhang,Gaolei Song,Boyuan Li,Zijian Li,Baofeng Ji,Ruijuan Zheng,Guoqiang Zheng,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: The paper analyzes Byzantine (malicious) attacks in RIS-enhanced, decode-and-forward relayed cooperative spectrum sensing for mobile cognitive radio networks, develops channel- and attack-aware hard decision fusion rules, characterizes optimal channel-aware Byzantine attack strategies, and shows attack effectiveness hinges more on the fraction of Byzantine nodes than on channel dynamics, with validation by simulations.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate security risks in advanced CSS systems that leverage RIS and relay nodes under mobility, where attackers may exploit limited instantaneous channel state information (ICSI) to degrade sensing performance.

Method: 1) Construct a RIS-enhanced decode-and-forward CSS system under dynamic channel conditions due to user mobility. 2) Develop channel- and attack-aware hard decision fusion rules. 3) Derive optimal channel-aware Byzantine attack strategies for both small-scale and large-scale attacking scenarios. 4) Provide theoretical analysis and extensive simulations for various attack scenarios.

Result: The optimal Byzantine attack does not require full a priori knowledge of global instantaneous CSI (e.g., global false alarm and detection probabilities); RIS and relay nodes, plus mobility-induced fast fading, complicate attacks. The optimal attack can be described by a unifying framework; the explicit attack strategy may not be unique. Attack effectiveness is primarily determined by the Byzantine node fraction rather than channel dynamics, enabling a relaxation of heavy reliance on ICSI and fusion rules in practical designs. Simulations validate the theory across diverse scenarios.

Conclusion: Channel-aware Byzantine attacks are practical threats in RIS-enhanced CSS, but defense design should focus on robustness to Byzantine fraction rather than exact channel knowledge. The results suggest the possibility of unified attack frameworks and highlight the need for fusion-rule designs that are less dependent on global ICSI, with empirical results supporting these insights.

Abstract: From the perspective of hard decision fusion, we investigate Byzantine
attacks in Reconfigurable Intelligent Surface (RIS)-enhanced and
decode-and-forward relay-assisted Cooperative Spectrum Sensing (CSS) for mobile
Cognitive Radio Networks (CRNs) in this paper. Specially, a RIS-enhanced and
decode-and-forward relay-assisted CSS configuration is first constructed under
dynamic channel scenarios due to user mobility. Subsequently, the channel- and
attack-aware hard decision fusion rules are developed, and the optimal
channel-aware Byzantine attack strategies are then developed under both
small-scale and large-scale attacking scenarios. The corresponding results
depict that the optimal attack strategy does not require any a prior knowledge
of the global instantaneous Channel State Information (ICSI) (e.g. false alarm
probability and detection probability of all the secondary users), although
perfect acquisition of ICSI is clearly always not affordable from the attacker
perspective, which is further exacerbated by the RIS and decode-and-forward
relays involved in CSS and the potential high mobility of secondary users that
leads to fast fading channels. Furthermore, our counterintuitive results also
indicate that, regardless of the attacker's awareness of the decision fusion
rule, the optimal Byzantine attack can be achieved through a unifying
framework, the explicit attack strategy may be not unique, and the attacking
effectiveness is primarily determined by the fraction of the Byzantine nodes
rather than the channel dynamics. That is, to make the channel-aware approach
more practical, the challenge that the heavy reliance on the global ICSI and
decision fusion rule in obtaining the Byzantine attacks is successfully
relaxed. Finally, we empirically validate our theoretical analysis through
extensive simulations across a wide range of attacking scenarios.

</details>


### [59] [Dual-Scale Antenna Deployment for Pinching Antenna Systems](https://arxiv.org/abs/2510.27185)
*Xu Gan,Zhaolin Wang,Yuanwei Liu*

Main category: cs.IT

TL;DR: 提出一种双尺度部署（DSD）框架用于 pinching antenna 系统（PASS），通过大尺度波导层次的粗略位移和小尺度的高精度细化位移实现 PA 的两阶段部署，并给出四种部署协议。基于能耗模型推导 PASS 的理论能量效率公式，并在此基础上联合优化发射波束成形、PA 辐射功率和 PA 部署以最大化能量效率，提出一个低复杂度的惩罚项交替优化算法。仿真结果表明 PASS 在能量效率方面显著优于传统蜂窝无关架构和 MIMO。


<details>
  <summary>Details</summary>
Motivation: 在无线系统中提升能量效率的需求日益突出，尤其需要对 PA 的部署与资源分配进行耦合优化。现有方法往往在尺度与精度上缺乏分层设计，且求解非凸、耦合强的问题时计算复杂度高，因此需要一个能分阶段实现的框架及低复杂度的求解算法。

Method: 1) 提出双尺度部署框架：粗略阶段在波导层实现大尺度位移；细化阶段进行小尺度、高精度定位。2) 给出四种 PA 部署协议。3) 构建基于 PA 部署的功耗模型，推导 PASS 的能量效率公式。4) 联合优化发射 precoding、PA 辐射功率与 PA 部署，在给定部署协议下最大化能量效率。5) 为求解非凸、强耦合问题，提出低复杂度的惩罚项基于交替优化的算法。

Result: 仿真验证了理论推导与算法的有效性与收敛性；PASS 的能量效率比传统蜂窝无关架构约高 70%，相较于 MIMO 系统接近两倍；同时指出必须明确 DSD 的分辨率与部署协议才能实现最大能量效率。

Conclusion: DSD 框架通过分层部署和耦合优化实现显著的能量效率提升，提供了部署分辨率、部署协议选择及低复杂度优化算法等关键设计要点，为面向高效能源传输的 Pinching Antenna 系统提供了有力的实现路径。

Abstract: A dual-scale deployment (DSD) framework for pinching antenna systems (PASS)
is proposed. 1) In the first coarse stage, the pinching antenna (PA) is
transferred over a large-scale range at the waveguide level. 2) The refinement
stage performs small-scale relocation of the PA with high precision. Four PA
deployment protocols are provided in the proposed DSD framework. Then, a
practical power consumption model is proposed, based on which the theoretical
energy efficiency formulas for PASS are derived. The transmit precoding, PA
radiation power, and PA deployment are jointly optimized to maximize the energy
efficiency under the provided PA deployment protocols. To solve this
non-convex, highly coupled problem, a low-complexity penalty-based alternating
optimization algorithm is proposed. Simulation results validate the accuracy of
theoretical results and the convergence of the proposed algorithm. It is
demonstrated that: 1) PASS delivers about 70% higher energy efficiency than the
conventional cell-free architecture and nearly twofold improvement relative to
MIMO systems; 2) it is essential to specify the DSD resolution and deployment
protocol to achieve the maximum energy efficiency for PASS.

</details>


### [60] [Cross-Band Channel Impulse Response Prediction: Leveraging 3.5 GHz Channels for Upper Mid-Band](https://arxiv.org/abs/2510.27349)
*Fan-Hao Lin,Chi-Jui Sung,Chu-Hsiang Huang,Hui Chen,Chao-Kai Wen,Henk Wymeersch*

Main category: cs.IT

TL;DR: 提出 CIR-UNext 框架用于在 6G 的高频段跨带预测，通过利用 3.5 GHz 的 CIR 数据和 RT 生成数据集，使用 AU-Net 对增益/相位进行预测，并扩展为 Channel2ComMap 的基石模型以进行 MIMO-OFDM 的吞吐量预测，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在 FR3 频段（7–24 GHz）进行高保真跨带信道预测，面临高频数据获取成本高、射线追踪计算量大等挑战；需要一种低成本、可扩展的跨带预测解决方案，以支持定位、波束管理、数字孪生和资源调度等6G应用。

Method: 构建一个以 RT 为基础的数据集流水线的 CIR-UNext 框架，利用注意力 U-Net (AU-Net) 及其变体对增益和相位进行预测；提出 AU-Net-Aux 模型实现中位增益误差 0.58 dB、相位误差 0.27 rad 的未见复杂环境预测；并将 CIR-UNext 扩展为 Channel2ComMap 基础模型，用于 MIMO-OFDM 系统的吞吐量预测，显示优越性。

Result: AU-Net-Aux 在未见复杂环境上的中位增益误差 0.58 dB、相位预测误差 0.27 rad；Channel2ComMap 在吞吐量预测上优于现有方法，证实跨带预测的有效性与可扩展性。

Conclusion: CIR-UNext 提供了一种高效、可扩展的跨带预测解决方案，支撑6G 时代的本地化、波束管理、数字孪生和资源调度等应用，并可通过基础模型 Channel2ComMap 推动更广泛的下游任务。

Abstract: Accurate cross-band channel prediction is essential for 6G networks,
particularly in the upper mid-band (FR3, 7--24 GHz), where penetration loss and
blockage are severe. Although ray tracing (RT) provides high-fidelity modeling,
it remains computationally intensive, and high-frequency data acquisition is
costly. To address these challenges, we propose CIR-UNext, a deep learning
framework designed to predict 7 GHz channel impulse responses (CIRs) by
leveraging abundant 3.5 GHz CIRs. The framework integrates an RT-based dataset
pipeline with attention U-Net (AU-Net) variants for gain and phase prediction.
The proposed AU-Net-Aux model achieves a median gain error of 0.58 dB and a
phase prediction error of 0.27 rad on unseen complex environments. Furthermore,
we extend CIR-UNext into a foundation model, Channel2ComMap, for throughput
prediction in MIMO-OFDM systems, demonstrating superior performance compared
with existing approaches. Overall, CIR-UNext provides an efficient and scalable
solution for cross-band prediction, enabling applications such as localization,
beam management, digital twins, and intelligent resource allocation in 6G
networks.

</details>


### [61] [Weight Enumerators From Equivalence Relations and MacWilliams Identities](https://arxiv.org/abs/2510.27358)
*S. T. Dougherty,C. Fernández-Córdoba*

Main category: cs.IT

TL;DR: 论文将权重枚举器的概念扩展到在有限场、有限交换群和有限Frobenius环上的码，给出等价关系下的权重枚举器定义，并研究何时满足MacWilliams关系，且对特定等价关系给出具体的权重枚举研究。


<details>
  <summary>Details</summary>
Motivation: 在经典的MacWilliams对偶性基础上，探索更广泛的权重枚举器（包括等价关系下的权重）以适用于更一般的代数结构（有限域、有限群、Frobenius环），以统一和拓展码的对偶性工具。

Method: 定义在字母表上的等价关系下的权重枚举器，并推导在该定义下的MacWilliams关系的存在条件；对某些具体的等价关系（如按某些分区、等价类或符号变换）给出权重枚举的计算或性质结果；可能利用特征函数、表征理论或代数-组合技巧来建立对偶性条件。

Result: 给出在不同代数结构上，等价关系权重枚举器的MacWilliams关系成立的具体情形，并对若干具体等价关系给出权重枚举的研究结果，形成一个可用于分析不同模域和群结构码的统一框架。

Conclusion: 工作扩展了MacWilliams对偶性的适用性，提供了一个分析码在更广泛代数结构中权重性质的通用工具，并对特定等价关系的权重枚举提供了具体分析范式。

Abstract: In this paper, we consider codes over finite fields, finite abelian groups,
and finite Frobenius rings. For such codes, the complete weight enumerator and
the Hamming weight enumerator serve as powerful tools. These two types of
weight enumerators satisfy the MacWilliams relations. We define the weight
enumerator of a code with respect to an equivalence relation and determine in
which cases the MacWilliams relations hold for this weight enumerator. We also
study some weight enumerators for specific equivalence relations.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [62] [Cooperative Integrated Estimation-Guidance for Simultaneous Interception of Moving Targets](https://arxiv.org/abs/2510.26948)
*Lohitvel Gopikannan,Shashi Ranjan Kumar,Abhinav Sinha*

Main category: eess.SY

TL;DR: 提出一种多无人车协同的估计-制导框架，针对传感器异质性目标观测，在有向通信下使用 prescribed-time observer 与 TPNG 实现状态收敛与时间到导的一致性。


<details>
  <summary>Details</summary>
Motivation: 解决单一设计难以同时处理估计与制导的问题，以及传感器分布不均导致的观测困难，提升拦截非机动目标的鲁棒性与效率。

Method: 结合带传感器的与无传感器的无人车，利用有向图通信实现信息共享；对传感器车辆建立体现目标状态的 prescribed-time observer；无传感器车辆通过与邻居交换信息来估计目标状态；采用 true proportional navigation 指导，利用精确的时间-到-go（time-to-go）估计；通过 prescribed-time 控制器实现目标状态与时间到达的收敛/一致性。

Result: 仿真覆盖多种交战情景，验证在设定时间内实现状态收敛和时间到导的一致性，且对传感器异质性具有鲁棒性。

Conclusion: 将观测与制导统一在一个协同框架内，适用于异质传感器分布的多无人系统环境，扩展了估计-制导耦合设计的应用范围。

Abstract: This paper proposes a cooperative integrated estimation-guidance framework
for simultaneous interception of a non-maneuvering target using a team of
unmanned autonomous vehicles, assuming only a subset of vehicles are equipped
with dedicated sensors to measure the target's states. Unlike earlier
approaches that focus solely on either estimation or guidance design, the
proposed framework unifies both within a cooperative architecture. To
circumvent the limitation posed by heterogeneity in target observability,
sensorless vehicles estimate the target's state by leveraging information
exchanged with neighboring agents over a directed communication topology
through a prescribed-time observer. The proposed approach employs true
proportional navigation guidance (TPNG), which uses an exact time-to-go
formulation and is applicable across a wide spectrum of target motions.
Furthermore, prescribed-time observer and controller are employed to achieve
convergence to true target's state and consensus in time-to-go within set
predefined times, respectively. Simulations demonstrate the effectiveness of
the proposed framework under various engagement scenarios.

</details>


### [63] [Ferrohydrodynamic Microfluidics for Bioparticle Separation and Single-Cell Phenotyping: Principles, Applications, and Emerging Directions](https://arxiv.org/abs/2510.26950)
*Yuhao Zhang,Yong Teng,Kenan Song,Xianqiao Wang,Xianyan Chen,Yuhua Liu,Yiping Zhao,He Li,Leidong Mao,Yang Liu*

Main category: eess.SY

TL;DR: 本文是一篇综述性工作，概述了基于铁磁流体的微流控在多尺度生物样本分离中的进展、关键物理原理、应用场景、面临挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 需要一类无标记、可扩展且在微观到亚微观尺度上对生物粒子进行分离与表型分析的方法。铁磁流体微流控通过磁场梯度实现对 Diamagnetic 粒子的定向操控，为生物样品从细胞到外泌体等提供了新颖的分离与富集途径。

Method: 通过梳理近期理论与实验研究，解析铁磁液体与颗粒相互作用产生的主导磁浮力及其在微流控中的应用原理；系统性总结高分辨率的基于尺寸的分离、亚细胞富集与基于物理特征的表型筛选，并评估设计要点和可行的实验条件。

Result: 总结了以 ferrofluid 为介质的微流控在多尺度生物粒子分离、富集与表型分析方面的进展，明确了生物相容性、通量、纳米颗粒耗竭等关键挑战，并提出将机器学习、3D 打印和多重检测等技术融入的未来研究路线。

Conclusion: 铁磁流体微流控具备在精准生物医学、诊断与细胞工程领域发挥重要作用的潜力，但需解决生物相容性、提高系统通量、防止纳米粒子耗竭等挑战；未来方向包括跨学科方法的融合以实现更高效的无标记分离与表型筛选。

Abstract: Ferrohydrodynamic microfluidics relies on magnetic field gradients to
manipulate diamagnetic particles in ferrofluid-filled microenvironments. It has
emerged as a promising tool for label-free manipulation of bioparticles,
including their separation and phenotyping. This perspective reviews recent
progress in the development and applications of ferrofluid-based microfluidic
platforms for multiscale bioparticle separation, ranging from micron-scale
cells to submicron extracellular vesicles. We highlight the fundamental
physical principles for ferrohydrodynamic manipulation, including the dominant
magnetic buoyancy force resulting from the interaction of ferrofluids and
particles. We then describe how these principles enable high-resolution
size-based bioparticle separation, subcellular bioparticle enrichment, and
phenotypic screening based on physical traits. We also discuss key challenges
in ferrohydrodynamic microfluidics from the aspects of ferrofluid
biocompatibility, system throughput, and nanoparticle depletion. Finally, we
outline future research directions involving machine learning, 3D printing, and
multiplexed detection. These insights chart a path for advancing
ferrofluid-based technologies in precision biomedicine, diagnostics, and
cellular engineering.

</details>


### [64] [Quantifying Grid-Forming Behavior: Bridging Device-level Dynamics and System-Level Strength](https://arxiv.org/abs/2510.26953)
*Kehao Zhuang,Huanhai Xin,Verena Häberle,Xiuqiang He,Linbin Huang,Florian Dörfler*

Main category: eess.SY

TL;DR: 提出一个从设备到系统的统一框架，用以量化GFM转换器的响应、定义FI等指标，并证明GFM提升系统强度。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏精确的量化方法和统一定义，设备层与系统层之间存在脱节，且对GFM对系统稳定性的影响缺乏量化，需建立可比较的基准以设计、放置GFM并评估稳定性。

Method: 在设备层引入Forming Index (FI)以量化转换器对网格电压波动的响应；在系统层提出多母线电压刚度的量化度量，扩展为网格强度与母线强度以识别薄弱区域；并证明GFM转换器能提升系统强度，从而实现设备与系统的统一基准。

Result: FI量化了对网格变化的敏感性，系统层引入的多母线电压刚度、网格强度与母线强度量化框架可识别薄弱区域并桥接设备与系统层，最终提供一个用于GFM设计、放置和稳定性评估的统一基准。

Conclusion: 该框架为GFM设计、最优放置与系统稳定性评估提供统一的分析基准，明确揭示GFM如何提升系统强度。

Abstract: Grid-forming (GFM) technology is widely regarded as a promising solution for
future power systems dominated by power electronics. However, a precise method
for quantifying GFM converter behavior and a universally accepted GFM
definition remain elusive. Moreover, the impact of GFM on system stability is
not precisely quantified, creating a significant disconnect between device and
system levels. To address these gaps from a small-signal perspective, at the
device level, we introduce a novel metric, the Forming Index (FI) to quantify a
converter's response to grid voltage fluctuations. Rather than enumerating
various control architectures, the FI provides a metric for the converter's GFM
ability by quantifying its sensitivity to grid variations. At the system level,
we propose a new quantitative measure of system strength that captures the
multi-bus voltage stiffness, which quantifies the voltage and phase angle
responses of multiple buses to current or power disturbances. We further extend
this concept to grid strength and bus strength to identify weak areas within
the system. Finally, we bridge the device and system levels by formally proving
that GFM converters enhance system strength. Our proposed framework provides a
unified benchmark for GFM converter design, optimal placement, and system
stability assessment.

</details>


### [65] [Adaptive Control for a Physics-Informed Model of a Thermal Energy Distribution System: Qualitative Analysis](https://arxiv.org/abs/2510.26959)
*Paul Seurin,Auradha Annaswamy,Linyu Lin*

Main category: eess.SY

TL;DR: 提出一个面向线性系统的自适应控制（AC）框架，并在综合能源系统（IES）中将其应用于GHX控制，针对不确定性情形实现显著误差下降。


<details>
  <summary>Details</summary>
Motivation: IES 具有多种能量耦合与不确定性，现有方法缺乏统一、实时自适应能力来确保稳定可靠运行。

Method: 推导一个对全部状态可观测的线性系统的自适应控制（AC）方案，并将其应用于IES中的糖醇换热器（GHX）控制。基于先前对GHX系统动力学不确定性的量化，在名义模型中引入50%的误差；在以线性二次调节器（LQR）作为名义控制的基线下，比较AC在该情形下的性能表现，评价指标包括平均绝对误差（MAE）与积分时间绝对误差（ITAE），并指出AC在计算开销方面的低额外负担。

Result: AC显著降低MAE与ITAE，幅度介于30%到75%之间，且计算开销较低，体现了AC的鲁棒性与实用性；但控制努力显著增加，需要进一步研究其对物理系统的影响。

Conclusion: 未来工作将扩展到部分可观测与非线性动力学情形，开发线性框架的增强版本以提高稳健性并降低对实际装置的冲击。

Abstract: Integrated energy systems (IES) are complex heterogeneous architectures that
typically encompass power sources, hydrogen electrolyzers, energy storage, and
heat exchangers. This integration is achieved through operating control
strategy optimization. However, the lack of physical understanding as to how
these systems evolve over time introduces uncertainties that hinder reliable
application thereof. Techniques that can accommodate such uncertainties are
fundamental for ensuring proper operation of these systems. Unfortunately, no
unifying methodology exists for accommodating uncertainties in this regard.
That being said, adaptive control (AC) is a discipline that may allow for
accommodating such uncertainties in real-time. In the present work, we derive
an AC formulation for linear systems in which all states are observable and
apply it to the control of a glycol heat exchanger (GHX) in an IES. Based on
prior research in which we quantified the uncertainties of the GHXs system
dynamics, we introduced an error of 50% on four terms of the nominal model. In
the case where a linear quadratic regulator is used as the nominal control for
the reference system, we found that employing AC can reduce the mean absolute
error and integral time absolute error by a factor of 30%-75%. This reduction
is achieved with minimal computing overhead and control infrastructure, thus
underscoring the strength of AC. However, the control effort induced is
significant, therefore warranting further study in order to estimate its impact
on a physical system. To address further challenges, including partially
observable and non-linear dynamics, enhancements of the linear formulation are
currently being developed.

</details>


### [66] [Dispatchable Current Source Virtual Oscillator Control Achieving Global Stability](https://arxiv.org/abs/2510.26977)
*Kehao Zhuang,Linbin Huang,Huanhai Xin,Xiuqiang He,Verena Häberle,Florian Dörfler*

Main category: eess.SY

TL;DR: 提出一种用于GFL转换器的派发型电流源虚拟振荡控制(dCVOC)。在两方面与dVOC存在对偶性：电流频率通过无功功率控制（类似PLL）产生，电流幅值通过有功功率控制产生；给出在考虑LVRT与电流饱和约束下的稳态平衡性与全局稳定性的形式化证明，并通过高保真电磁暂态仿真验证效果，相比传统GFL能避免低电压瞬态与弱网不稳定。


<details>
  <summary>Details</summary>
Motivation: 现有网联并网型GFL控制在弱网和低电压情况下易失稳，需确保在LVRT、饱和约束下仍具备稳态存在性和全局稳定性，并提高对低电压瞬态与弱网的鲁棒性。

Method: 提出派 dispatchable current source虚拟振荡控制(dCVOC)方案，使电流频率由无功控制产生、幅值参考由有功控制产生，与dVOC存在对偶性。给出在给定网-转换器参数条件下的稳态存在性与全局稳定性的形式化证明，考虑LVRT与电流饱和约束，并通过高保真EM仿真验证。

Result: 理论上证明在合理的网与转换器参数下，dCVOC保证稳态平衡存在且全局稳定；引入LVRT与电流饱和约束的鲁棒性分析；仿真结果显示该控制在避免低电压瞬态与弱网不稳定方面优于传统GFL。

Conclusion: dCVOC提供了一种可派发的电流源式控制框架，与dVOC存在双重对偶性，提升了GFL在LVRT与电流饱和约束下的鲁棒性与稳定性，并通过高保真仿真得到验证。

Abstract: This work introduces a novel dispatchable current source virtual oscillator
control (dCVOC) scheme for grid-following (GFL) converters, which exhibits
duality with dispatchable virtual oscillator control (dVOC) in two ways: a) the
current frequency is generated through reactive power control, similar to a PLL
; b) the current magnitude reference is generated through active power control.
We formally prove that our proposed control always admits a steady-state
equilibrium and ensures global stability under reasonable conditions on grid
and converter parameters, even when considering LVRT and current saturation
constraints. Our approach avoids low-voltage transients and weak grid
instability, which is not the case for conventional GFL control. The
effectiveness of our proposed control is verified through high-fidelity
electromagnetic transient simulations.

</details>


### [67] [Solving Infinite-Horizon Optimal Control Problems using the Extreme Theory of Functional Connections](https://arxiv.org/abs/2510.27187)
*Tanay Raghunandan Srinivasa,Suraj Kumar*

Main category: eess.SY

TL;DR: 提出一种结合物理信息学习的无限时域最优控制求解框架，利用HJB PDE并通过X-TFC+ELM近似值函数，解析表达最优控制，边界条件解析满足，训练成本低于PINNs，在线性/非线性系统及航天任务中有验证。


<details>
  <summary>Details</summary>
Motivation: 解决无限时域HJB PDE的高成本和边界条件难以严格满足的问题，寻求高效、可解释且可推广的物理信息学习方法来获得最优控制策略。

Method: 对仿射动力学系统，给出严格凸且可分离的控制成本时，最优控制可解析地以值函数梯度表示。采用X-TFC与ELM的混合方法近似值函数，确保边界条件解析满足并显著降低训练成本，相比PINNs。

Result: 在具有解析解的线性与非线性系统以及太空任务（如航天器去摆动控制）中的基准测试显示有效性与可行性。

Conclusion: 提供一种高效可扩展的物理信息机器学习框架，用于无限时域最优控制的HJB PDE求解，边界条件可解析满足、训练成本低，适用于更复杂系统的推广与应用。

Abstract: This paper presents a physics-informed machine learning approach for
synthesizing optimal feedback control policy for infinite-horizon optimal
control problems by solving the Hamilton-Jacobi-Bellman (HJB) partial
differential equation(PDE). The optimal control policy is derived analytically
for affine dynamical systems with separable and strictly convex control costs,
expressed as a function of the gradient of the value function. The resulting
HJB-PDE is then solved by approximating the value function using the Extreme
Theory of Functional Connections (X-TFC) - a hybrid approach that combines the
Theory of Functional Connections (TFC) with the Extreme Learning Machine (ELM)
algorithm. This approach ensures analytical satisfaction of boundary conditions
and significantly reduces training cost compared to traditional
Physics-Informed Neural Networks (PINNs). We benchmark the method on linear and
non-linear systems with known analytical solutions as well as demonstrate its
effectiveness on control tasks such as spacecraft optimal de-tumbling control.

</details>


### [68] [Simplifying Preference Elicitation in Local Energy Markets: Combinatorial Clock Exchange](https://arxiv.org/abs/2510.27306)
*Shobhit Singhal,Lesia Mitridati*

Main category: eess.SY

TL;DR: 提出一种面向产消者的多产品市场设计，将组合时钟交换与机器学习相结合以表达复杂偏好，允许产消者在挂牌价格下仅报告偏好组合，利用ML辅助定价以加速收敛，采用线性定价规则，仿真表明约15轮时钟迭代可达到清算价格。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源资源（DER）的快速增长，需要新型市场平台使产消者能够交易多种电力及支撑网格的产品；但产消者往往具有复杂且相互依赖的偏好，并且认知和计算资源有限，难以参与复杂的竞价格式。

Method: 在一个多产品市场中，将组合时钟交换（combinatorial clock exchange）与机器学习（ML）相融合，提出迭代机制，产消者只需在挂牌价格下报告期望的产品组合；无需预测产品价格或遵循复杂的竞价格式；通过ML辅助的价格发现实现更快收敛；采用线性定价以提升透明度和可解释性。

Result: 通过数值仿真，市场在大约15轮时钟迭代后收敛到清算价格。

Conclusion: 该设计在提升可用性、收敛速度和价格透明度方面具有潜力，尤其适用于规模化DER市场，但需进一步研究在实际部署中的激励兼容性、鲁棒性和动态环境适应性。

Abstract: As distributed energy resources (DERs) proliferate, future power system will
need new market platforms enabling prosumers to trade various electricity and
grid-support products. However, prosumers often exhibit complex, product
interdependent preferences and face limited cognitive and computational
resources, hindering engagement with complex market structures and bid formats.
We address this challenge by introducing a multi-product market that allows
prosumers to express complex preferences through an intuitive format, by fusing
combinatorial clock exchange and machine learning (ML) techniques. The
iterative mechanism only requires prosumers to report their preferred package
of products at posted prices, eliminating the need for forecasting product
prices or adhering to complex bid formats, while the ML-aided price discovery
speeds up convergence. The linear pricing rule further enhances transparency
and interpretability. Finally, numerical simulations demonstrate convergence to
clearing prices in approximately 15 clock iterations.

</details>


### [69] [A Switching Strategy for Event-Trigger Control of Spacecraft Rendezvous](https://arxiv.org/abs/2510.27414)
*Tommaso Del Carro,Gerson Portilla,Alexandre Seuret,Rafael Vazquez*

Main category: eess.SY

TL;DR: 提出一种基于 Hill-Clohessy-Wiltshire 方程的冲动式状态反馈控制律，用状态依赖切换策略来确定推进器激活时机与输入幅值，结合 Lyapunov 稳定性和 LMIs，确保闭环稳定并最小化驱动事件数量，数值对比 MPC 展示优劣与权衡。


<details>
  <summary>Details</summary>
Motivation: 在轨道成对会合中寻求更低推进器开关频次和燃料消耗的同时，保证在 CW 方程描述的线性化近似下的系统稳定性与鲁棒性。

Method: 建立状态反馈的冲动控制框架，采用状态依赖切换逻辑确定推进脉冲的输入幅值与触发条件；基于 Lyapunov 稳定性分析和线性矩阵不等式（LMIs）推导非线性控制律，得到闭环稳定性并实现事件最小化。

Result: 证明了闭环系统的稳定性，且实现了尽可能少的驱动事件；通过数值算例与标准 Model Predictive Control（MPC）进行对比，展示了所提方法的优势与权衡。

Conclusion: 该框架在 CW 动力学下提供了稳定且低触发的会合控制解，具备理论可证明性和数值验证的支持，并提供与 MPC 的直接对比以帮助权衡实际应用中的性能与开销。

Abstract: This paper presents the design of a state-feedback control law for spacecraft
rendezvous, formulated using the Hill-Clohessy-Wiltshire equations. The
proposed method introduces an impulsive control strategy to regulate thruster
operations. Specifically, a state-dependent switching framework is developed to
determine both the control input magnitudes and the precise state conditions
that trigger thruster activation. The nonlinear control law is derived using
principles from automatic control theory, particularly Lyapunov stability
analysis and the Linear Matrix Inequality framework. The resulting closed-loop
system is proven to be stable, while simultaneously minimizing the total number
of actuation events. The effectiveness of the proposed method is demonstrated
through a numerical case study, which includes a comparative analysis with a
standard Model Predictive Control scheme, highlighting the advantages and
trade-offs of the developed control structure.

</details>


### [70] [Context-Aware Stochastic Modeling of Consumer Energy Resource Aggregators in Electricity Markets](https://arxiv.org/abs/2510.27478)
*Chatum Sankalpa,Ghulam Mohy-ud-din,Erik Weyer,Maria Vrakopoulou*

Main category: eess.SY

TL;DR: 本文提出三种两阶段随机优化方法（风险中性、鲁棒、机会约束）来管理聚合器在澳大利亚国家电力市场中对CER不确定性的影响，聚焦屋顶太阳能与电池储能等CER，利用精确的荷电态动态和互补性约束，形成可扩展的两阶段混合整数线性规划，通过情景方法和仿射回归实现可处理的 reformulations，并在多种用例下评估其对不确定性的缓解、盈利能力提升以及方法选择的情境化指导。


<details>
  <summary>Details</summary>
Motivation: CER（如屋顶光伏与储能）的不确定性会导致可行性问题或收益损失。需要可扩展的随机优化方法来有效管理这些不确定性，并为聚合器在能源和调控市场中的决策提供有据可依的指引。

Method: 提出三种两阶段随机优化策略：风险中性、鲁棒、及机会约束。问题建模为两阶段随机混合整数线性规划，包含BES的精确状态-电荷动力学与互补性约束，利用情景法与仿射回归策略实现可处理的 reformulations；对大规模情景集进行松弛处理；以情景为基础的求解流程。

Result: 通过不同用例对运营设置、市场特征及决策偏好的多样性进行评估，结果表明这些方法能够缓解不确定性、提升盈利性，并为聚合器在选择最合适的随机优化方法时提供情境化的指导。

Conclusion: 本文提供一个面向CER聚合器的可扩展随机优化工具箱，能够在不确定性环境中实现稳健收益并给予市场参与者关于方法选择的实用建议。

Abstract: Aggregators of consumer energy resources (CERs) like rooftop solar and
battery energy storage (BES) face challenges due to their inherent
uncertainties. A sensible approach is to use stochastic optimization to handle
such uncertainties, which can lead to infeasible problems or loss in revenues
if not chosen appropriately. This paper presents three efficient two-stage
stochastic optimization methods: risk-neutral, robust, and chance-constrained,
to address the impact of CER uncertainties for aggregators who participate in
energy and regulation services markets in the Australian National Electricity
Market. Furthermore, these methods utilize the flexibility of BES, considering
precise state-of-charge dynamics and complementarity constraints, aiming for
scalable performance while managing uncertainty. The problems are formed as
two-stage stochastic mixed-integer linear programs, with relaxations adopted
for large scenario sets. The solution approach employs scenario-based
methodologies and affine recourse policies to obtain tractable reformulations.
These methods are evaluated across use cases reflecting diverse operational and
market settings, uncertainty characteristics, and decision-making preferences,
demonstrating their ability to mitigate uncertainty, enhance profitability, and
provide context-aware guidance for aggregators in choosing the most appropriate
stochastic optimization method.

</details>


### [71] [Technical Report for Dissipativity Learning in Reproducing Kernel Hilbert Space](https://arxiv.org/abs/2510.27669)
*Xiuzhen Ye,Wentao Tang*

Main category: eess.SY

TL;DR: A nonparametric RKHS-based framework for learning dissipativity of unknown nonlinear systems from input-output data, using Hilbert-Schmidt operators, reduced to a finite convex program via representer theorem, with generalization guarantees and data-driven stability/performance certification.


<details>
  <summary>Details</summary>
Motivation: To enable data-driven certification of stability and performance for unknown nonlinear systems without explicit dynamics, extending dissipativity to a nonparametric setting and avoiding fixed quadratic matrices.

Method: Model storage and supply as Hilbert-Schmidt operators on kernel features in RKHS. Formulate dissipativity as an energy-balance inequality. Convert to a one-class SVM problem; apply the representer theorem to reduce to a finite-dimensional convex program using kernel Gram matrices.

Result: Obtains generalization guarantees from statistical learning theory, including confidence bounds on dissipation rate and L2 gain. Numerical results show effective identification of nonlinear dissipative behavior from input-output data, enabling model-free control analysis and synthesis.

Conclusion: The proposed RKHS-based dissipativity learning provides a powerful, interpretable, and tractable framework for data-driven analysis and synthesis of nonlinear systems, avoiding explicit dynamic modeling.

Abstract: This work presents a nonparametric framework for dissipativity learning in
reproducing kernel Hilbert spaces, which enables data-driven certification of
stability and performance properties for unknown nonlinear systems without
requiring an explicit dynamic model. Dissipativity is a fundamental system
property that generalizes Lyapunov stability, passivity, and finite L2 gain
conditions through an energy balance inequality between a storage function and
a supply rate. Unlike prior parametric formulations that approximate these
functions using quadratic forms with fixed matrices, the proposed method
represents them as Hilbert Schmidt operators acting on canonical kernel
features, thereby capturing nonlinearities implicitly while preserving
convexity and analytic tractability. The resulting operator optimization
problem is formulated in the form of a one-class support vector machine and
reduced, via the representer theorem, to a finite dimensional convex program
expressed through kernel Gram matrices. Furthermore, statistical learning
theory is applied to establish generalization guarantees, including confidence
bounds on the dissipation rate and the L2 gain. Numerical results demonstrate
that the proposed RKHS based dissipativity learning method effectively
identifies nonlinear dissipative behavior directly from input output data,
providing a powerful and interpretable framework for model free control
analysis and synthesis.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [72] [Analytical Model of NR-V2X Mode 2 with Re-Evaluation Mechanism](https://arxiv.org/abs/2510.27108)
*Shuo Zhu,Siyu Lin*

Main category: cs.NI

TL;DR: 提出基于DTMC的 NR-V2X Mode 2分析模型与消息生成器，用以评估重评机制在变 traffic 下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决仿真中对变 traffic 的依赖，提供可分析的框架来评估重评对可靠性与时延的影响。

Method: 使用离散时间马尔可夫链构建消息发生器，模拟3GPP V2X服务的 traffic，并建立 NR-V2X Mode 2 的分析 model，评估资源 sensing 与重评对性能的影响。

Result: 重评机制提升可靠性，但时延方面仍有局部改进空间。

Conclusion: 重评机制对可靠性有效，但需进一步优化时延；所建模型为在变 traffic 下评估 V2X MAC 性能的分析工具。

Abstract: Massive message transmissions, unpredictable aperiodic messages, and
high-speed moving vehicles contribute to the complex wireless environment,
resulting in inefficient resource collisions in Vehicle to Everything (V2X). In
order to achieve better medium access control (MAC) layer performance, 3GPP
introduced several new features in NR-V2X. One of the most important is the
re-evaluation mechanism. It allows the vehicle to continuously sense resources
before message transmission to avoid resource collisions. So far, only a few
articles have studied the re-evaluation mechanism of NR-V2X, and they mainly
focus on network simulator that do not consider variable traffic, which makes
analysis and comparison difficult. In this paper, an analytical model of NR-V2X
Mode 2 is established, and a message generator is constructed by using discrete
time Markov chain (DTMC) to simulate the traffic pattern recommended by 3GPP
advanced V2X services. Our study shows that the re-evaluation mechanism
improves the reliability of NR-V2X transmission, but there are still local
improvements needed to reduce latency.

</details>


### [73] [Stochastic Geometry of Cylinders: Characterizing Inter-Nodal Distances for 3D UAV Networks](https://arxiv.org/abs/2510.27111)
*Yunfeng Jiang,Zhiming Huang,Jianping Pan*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The analytical characterization of coverage probability in finite
three-dimensional wireless networks has long remained an open problem, hindered
by the loss of spatial independence in finite-node settings and the coupling
between link distances and interference in bounded geometries. This paper
closes this gap by presenting the first exact analytical framework for coverage
probability in finite 3D networks modeled by a binomial point process within a
cylindrical region. To bypass the intractability that has long hindered such
analyses, we leverage the independence structure, convolution geometry, and
derivative properties of Laplace transforms, yielding a formulation that is
both mathematically exact and computationally efficient. Extensive Monte Carlo
simulations verify the analysis and demonstrate significant accuracy gains over
conventional Poisson-based models. The results generalize to any confined 3D
wireless system, including aerial, underwater, and robotic networks.

</details>


### [74] [Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO Satellite Networks](https://arxiv.org/abs/2510.27506)
*Ke He,Thang X. Vu,Le He,Lisheng Fan,Symeon Chatzinotas,Bjorn Ottersten*

Main category: cs.NI

TL;DR: PRIMAL: an asynchronous, risk-aware multi-agent routing framework for ultra-dense LEO constellations that learns full QoS cost distributions and uses primal-dual optimization to bound tail risks, achieving lower latency and balanced load.


<details>
  <summary>Details</summary>
Motivation: Traditional synchronous, risk-oblivious routing fails in asynchronous, dynamic LEO networks with large scale and delays; need decentralized algorithms that optimize multiple QoS objectives while managing worst-case performance.

Method: Event-driven multi-agent system where each satellite acts on its own timeline; agents learn cost distributions of QoS targets and constrain tail risks via a primal-dual formulation; routing decisions are decoupled and risk-aware.

Result: Extensive simulations on a 1584-satellite LEO constellation show significant latency/load improvements; compared to risk-oblivious baselines, ~70% reduction in queuing delay and ~12 ms end-to-end delay reduction in loaded scenarios.

Conclusion: Autonomous risk-awareness and the explicit handling of cost distributions are essential for robust routing in highly dynamic, asynchronous satellite networks.

Abstract: The rise of ultra-dense LEO constellations creates a complex and asynchronous
network environment, driven by their massive scale, dynamic topologies, and
significant delays. This unique complexity demands an adaptive packet routing
algorithm that is asynchronous, risk-aware, and capable of balancing diverse
and often conflicting QoS objectives in a decentralized manner. However,
existing methods fail to address this need, as they typically rely on
impractical synchronous decision-making and/or risk-oblivious approaches. To
tackle this gap, we introduce PRIMAL, an event-driven multi-agent routing
framework designed specifically to allow each satellite to act independently on
its own event-driven timeline, while managing the risk of worst-case
performance degradation via a principled primal-dual approach. This is achieved
by enabling agents to learn the full cost distribution of the targeted QoS
objectives and constrain tail-end risks. Extensive simulations on a LEO
constellation with 1584 satellites validate its superiority in effectively
optimizing latency and balancing load. Compared to a recent risk-oblivious
baseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms
end-to-end delay reduction in loaded scenarios. This is accomplished by
resolving the core conflict between naive shortest-path finding and congestion
avoidance, highlighting such autonomous risk-awareness as a key to robust
routing.

</details>


### [75] [Rethinking Telemetry Design for Fine-Grained Anomaly Detection in 5G User Planes](https://arxiv.org/abs/2510.27664)
*Niloy Saha,Noura Limam,Yang Xiao,Raouf Boutaba*

Main category: cs.NI

TL;DR: Kestrel: a sketch-based telemetry system for 5G user planes that provides fine-grained anomaly detection at a fraction of per-packet cost, by extending Count-Min Sketch with histogram-augmented buckets and per-queue partitioning; it offers detectability guarantees and shows improved accuracy (10%) and 10x bandwidth reduction on a 5G testbed with Intel Tofino switches.


<details>
  <summary>Details</summary>
Motivation: Need for fine-grained, low-overhead QoS anomaly detection in 5G user planes. Coarse per-class counters miss transient/per-flow anomalies; per-packet schemes are too costly; selective postcards miss smaller or brief anomalies. A scalable, accurate telemetry mechanism is required.

Method: Extend Count-Min Sketch with histogram-augmented buckets and per-queue partitioning to compress per-packet measurements into compact summaries while preserving anomaly signals. Develop formal detectability guarantees that account for sketch collisions, and derive sizing rules and binning strategies to maximize anomaly separability.

Result: Empirical evaluation on a 5G testbed with Intel Tofino switches shows Kestrel achieves about 10% better detection accuracy than existing selective postcard schemes and reduces export bandwidth by about 10x.

Conclusion: Kestrel provides fine-grained visibility into key metric distributions (e.g., latency tails, inter-arrival times) at low overhead, with principled guarantees to guide configuration, making it a promising telemetry solution for 5G U-plane anomaly detection.

Abstract: Detecting QoS anomalies in 5G user planes requires fine-grained per-flow
visibility, but existing telemetry approaches face a fundamental trade-off.
Coarse per-class counters are lightweight but mask transient and per-flow
anomalies, while per-packet telemetry postcards provide full visibility at
prohibitive cost that grows linearly with line rate. Selective postcard schemes
reduce overhead but miss anomalies that fall below configured thresholds or
occur during brief intervals. We present Kestrel, a sketch-based telemetry
system for 5G user planes that provides fine-grained visibility into key metric
distributions such as latency tails and inter-arrival times at a fraction of
the cost of per-packet postcards. Kestrel extends Count-Min Sketch with
histogram-augmented buckets and per-queue partitioning, which compress
per-packet measurements into compact summaries while preserving
anomaly-relevant signals. We develop formal detectability guarantees that
account for sketch collisions, yielding principled sizing rules and binning
strategies that maximize anomaly separability. Our evaluations on a 5G testbed
with Intel Tofino switches show that Kestrel achieves 10% better detection
accuracy than existing selective postcard schemes while reducing export
bandwidth by 10x.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [76] [VISAT: Benchmarking Adversarial and Distribution Shift Robustness in Traffic Sign Recognition with Visual Attributes](https://arxiv.org/abs/2510.26833)
*Simon Yu,Peilin Yu,Hongbo Zheng,Huajie Shao,Han Zhao,Lui Sha*

Main category: cs.CR

TL;DR: VISAT introduces a dataset and benchmarking suite for evaluating robustness in traffic sign recognition under adversarial attacks and distribution shifts, leveraging MTSD attributes and multi-task learning (MTL) networks.


<details>
  <summary>Details</summary>
Motivation: Address the vulnerability of traffic sign recognition to adversarial inputs and distribution shifts, and investigate spurious correlations in attribute-based MTL networks.

Method: Build VISAT on MTSD with two benchmarks: (1) adversarial robustness using PGD attacks; (2) distribution shift using ImageNet-C-like corruptions and natural variations. Annotate each sign with visual attributes (color, shape, symbol, text) and train base (ResNet-152) and MTL (ViT-B/32) models, including synthetic color-quantization alterations to study correlations.

Result: MTL networks reveal spurious correlations among tasks; robustness degrades under both adversarial attacks and corruptions; performance differences observed between base and MTL models; synthetic color alterations help diagnose correlations; VISAT provides a framework to measure and analyze robustness.

Conclusion: VISAT facilitates understanding and improving robustness of traffic sign recognition in real-world autonomous systems, highlighting challenges from adversarial attacks, distribution shifts, and attribute-based spurious correlations, and guiding development of more resilient models.

Abstract: We present VISAT, a novel open dataset and benchmarking suite for evaluating
model robustness in the task of traffic sign recognition with the presence of
visual attributes. Built upon the Mapillary Traffic Sign Dataset (MTSD), our
dataset introduces two benchmarks that respectively emphasize robustness
against adversarial attacks and distribution shifts. For our adversarial attack
benchmark, we employ the state-of-the-art Projected Gradient Descent (PGD)
method to generate adversarial inputs and evaluate their impact on popular
models. Additionally, we investigate the effect of adversarial attacks on
attribute-specific multi-task learning (MTL) networks, revealing spurious
correlations among MTL tasks. The MTL networks leverage visual attributes
(color, shape, symbol, and text) that we have created for each traffic sign in
our dataset. For our distribution shift benchmark, we utilize ImageNet-C's
realistic data corruption and natural variation techniques to perform
evaluations on the robustness of both base and MTL models. Moreover, we further
explore spurious correlations among MTL tasks through synthetic alterations of
traffic sign colors using color quantization techniques. Our experiments focus
on two major backbones, ResNet-152 and ViT-B/32, and compare the performance
between base and MTL models. The VISAT dataset and benchmarking framework
contribute to the understanding of model robustness for traffic sign
recognition, shedding light on the challenges posed by adversarial attacks and
distribution shifts. We believe this work will facilitate advancements in
developing more robust models for real-world applications in autonomous driving
and cyber-physical systems.

</details>


### [77] [Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token](https://arxiv.org/abs/2510.26847)
*Shaked Zychlinski,Yuval Kainan*

Main category: cs.CR

TL;DR: CPT-Filtering 提出了一种轻量级、模型无关的防护方法，通过检测文本的每个 token 的平均字符数（CPT）来识别经过编码的对话/输入，从而抵御通过 cipher 和字符级编码绕过安全守则的 jailbreak 攻击，实验表明在多个分词器和多种编码方案下具有高准确性，且计算开销极低，可用于实时过滤与离线数据清洗。


<details>
  <summary>Details</summary>
Motivation: 抵御通过加密、编码等手段绕过安全守则的 jailbreak 攻击；现有对策往往计算成本高、模型依赖强，亟需一种轻量、通用的防护机制。

Method: 基于 BPE 分词器的特性，利用文本的平均字符数/token（CPT）作为对自然语言与编码文本之间分布差异的信号。通过在不同分词器和多种编码方案上对超过10万条提示进行评估，设定 CPT 阈值来区分自然文本与编码文本，实现模型无关的实时过滤。

Result: 简单的 CPT 阈值就能在多种编码方案和分词器下对编码文本进行高精度识别，且对较短输入也有效，显示出低开销且鲁棒的效果。

Conclusion: CPT-Filtering 提供了一种实用、可立即部署的防御层，适用于实时文本过滤和离线数据整理，具备模型无关性与低成本特征。

Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where
malicious prompts are disguised using ciphers and character-level encodings to
bypass safety guardrails. While these guardrails often fail to interpret the
encoded content, the underlying models can still process the harmful
instructions. We introduce CPT-Filtering, a novel, model-agnostic with
negligible-costs and near-perfect accuracy guardrail technique that aims to
mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair
Encoding (BPE) tokenizers. Our method is based on the principle that
tokenizers, trained on natural language, represent out-of-distribution text,
such as ciphers, using a significantly higher number of shorter tokens. Our
technique uses a simple yet powerful artifact of using language models: the
average number of Characters Per Token (CPT) in the text. This approach is
motivated by the high compute cost of modern methods - relying on added modules
such as dedicated LLMs or perplexity models. We validate our approach across a
large dataset of over 100,000 prompts, testing numerous encoding schemes with
several popular tokenizers. Our experiments demonstrate that a simple CPT
threshold robustly identifies encoded text with high accuracy, even for very
short inputs. CPT-Filtering provides a practical defense layer that can be
immediately deployed for real-time text filtering and offline data curation.

</details>


### [78] [LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks](https://arxiv.org/abs/2510.26941)
*Seif Ikbarieh,Maanak Gupta,Elmahedi Mahalal*

Main category: cs.CR

TL;DR: 提出一个混合框架，将机器学习用于多类攻击检测，与大语言模型用于攻击行为分析和缓解建议，并引入标准化、量化评估指标与多模模型评审，以提升对IoT安全的分析与应对能力。


<details>
  <summary>Details</summary>
Motivation: IoT快速扩展带来更大攻击面，现有评估多为定性，缺乏客观、可比较的量化基准，亟需统一的评估框架来衡量AI在攻击检测、分析和缓解中的效果。

Method: 在Edge-IIoTset和CICIoT2023上对多种ML/DL分类器进行基准测试；通过结构化角色扮演提示工程与检索增强生成(RAG)引导ChatGPT-o3和DeepSeek-R1进行攻击行为分析和缓解建议；提出新的量化评估指标；建立由多种评估主体（ChatGPT-4o、DeepSeek-V3、Mixtral、Gemini等）组成的独立评审。

Result: 随机森林表现最佳的检测模型；ChatGPT-o3在攻击分析与缓解方面优于DeepSeek-R1；通过多模型评审实现对输出的独立评价。

Conclusion: 提出可重复、可比较的量化基准与混合方法，显示该框架在IoT安全威胁检测与缓解分析中的潜力，但具体性能需在更多场景下验证。

Abstract: The Internet of Things has expanded rapidly, transforming communication and
operations across industries but also increasing the attack surface and
security breaches. Artificial Intelligence plays a key role in securing IoT,
enabling attack detection, attack behavior analysis, and mitigation suggestion.
Despite advancements, evaluations remain purely qualitative, and the lack of a
standardized, objective benchmark for quantitatively measuring AI-based attack
analysis and mitigation hinders consistent assessment of model effectiveness.
In this work, we propose a hybrid framework combining Machine Learning (ML) for
multi-class attack detection with Large Language Models (LLMs) for attack
behavior analysis and mitigation suggestion. After benchmarking several ML and
Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we
applied structured role-play prompt engineering with Retrieval-Augmented
Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed,
context-aware responses. We introduce novel evaluation metrics for quantitative
assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o,
DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon
H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the
responses. Results show that Random Forest has the best detection model, and
ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.

</details>


### [79] [Unvalidated Trust: Cross-Stage Vulnerabilities in Large Language Model Architectures](https://arxiv.org/abs/2510.27190)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As Large Language Models (LLMs) are increasingly integrated into automated,
multi-stage pipelines, risk patterns that arise from unvalidated trust between
processing stages become a practical concern. This paper presents a
mechanism-centered taxonomy of 41 recurring risk patterns in commercial LLMs.
The analysis shows that inputs are often interpreted non-neutrally and can
trigger implementation-shaped responses or unintended state changes even
without explicit commands. We argue that these behaviors constitute
architectural failure modes and that string-level filtering alone is
insufficient. To mitigate such cross-stage vulnerabilities, we recommend
zero-trust architectural principles, including provenance enforcement, context
sealing, and plan revalidation, and we introduce "Countermind" as a conceptual
blueprint for implementing these defenses.

</details>


### [80] [Prevalence of Security and Privacy Risk-Inducing Usage of AI-based Conversational Agents](https://arxiv.org/abs/2510.27275)
*Kathrin Grosse,Nico Ebert*

Main category: cs.CR

TL;DR: 英国样本调查显示，常用对话型AI的用户存在潜在被利用的风险行为，约三分之一的“常用用户”可能执行攻击相关行为，四分之一尝试过越狱；数据清洗普遍但对数据用途和可选退出缺乏认知，需加强防护与透明度。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs日益普及，关于真实世界用户行为及其对系统安全的影响仍不充分。本研究通过大规模调查揭示普通用户的风险行为与认知空缺，为安全治理提供实证基础。

Method: 通过在2024年使用Prolific招募的3270名英国成年人进行横断面调查，聚焦“常用用户”（每周至少一次使用CA），评估越狱尝试、上传恶意文件、数据清洗、敏感信息披露、对数据用法的知情与退出权等行为及认知。

Result: 在“常用用户”中，约有三分之一的行为可能帮助攻击；四分之一尝试过越狱；约一半进行数据清洗，大多数不分享敏感数据，极少分享如密码等非常敏感信息；多数参与者不清楚数据可用于训练模型且可选择退出；研究发现现有威胁模型在现实世界有体现，需开发安全使用的指南與防护机制。

Conclusion: 应强化AI守护机制与安全使用准则，防止敏感信息外泄。厂商需加强对敏感数据的入口控制，并提高数据使用政策与设置的透明度，以提升用户教育与系统安全性。

Abstract: Recent improvement gains in large language models (LLMs) have lead to
everyday usage of AI-based Conversational Agents (CAs). At the same time, LLMs
are vulnerable to an array of threats, including jailbreaks and, for example,
causing remote code execution when fed specific inputs. As a result, users may
unintentionally introduce risks, for example, by uploading malicious files or
disclosing sensitive information. However, the extent to which such user
behaviors occur and thus potentially facilitate exploits remains largely
unclear. To shed light on this issue, we surveyed a representative sample of
3,270 UK adults in 2024 using Prolific. A third of these use CA services such
as ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a
third exhibited behaviors that may enable attacks, and a fourth have tried
jailbreaking (often out of understandable reasons such as curiosity, fun or
information seeking). Half state that they sanitize data and most participants
report not sharing sensitive data. However, few share very sensitive data such
as passwords. The majority are unaware that their data can be used to train
models and that they can opt-out. Our findings suggest that current academic
threat models manifest in the wild, and mitigations or guidelines for the
secure usage of CAs should be developed. In areas critical to security and
privacy, CAs must be equipped with effective AI guardrails to prevent, for
example, revealing sensitive information to curious employees. Vendors need to
increase efforts to prevent the entry of sensitive data, and to create
transparency with regard to data usage policies and settings.

</details>


### [81] [Coordinated Position Falsification Attacks and Countermeasures for Location-Based Services](https://arxiv.org/abs/2510.27346)
*Wenjie Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 提出一种基于接收机自校验RAIM的多源信息融合方法，通过整合GNSS、机载传感器与地面基础设施信号，对抗低成本的定位欺骗和干扰，实验显示检测精度最高提升约62%。


<details>
  <summary>Details</summary>
Motivation: LBS对定位数据的完整性依赖日益增加，低成本攻击（<50美元）易导致欺骗和服务滥用，亟需鲁棒的检测与防护机制。

Method: 在RAIM框架上引入机会性信息，将机载传感器数据、地面信号信息与GNSS等异构信号进行信息融合，理论分析表明融合提升对抗有针对性的对手的鲁棒性；并通过实验评估验证检测精度的提升以及定位的恢复效果。

Result: 理论分析表明异构信号融合能够提高对抗能力；实验结果显示相比基线，检测准确度提升最高约62%，且定位误差得到显著改善。

Conclusion: 通过扩展RAIM并利用现成的多源信息，提升LBS定位完整性与鲁棒性，能够应对复杂攻击场景，具备实际应用潜力。

Abstract: With the rise of location-based service (LBS) applications that rely on
terrestrial and satellite infrastructures (e.g., GNSS and crowd-sourced Wi-Fi,
Bluetooth, cellular, and IP databases) for positioning, ensuring their
integrity and security is paramount. However, we demonstrate that these
applications are susceptible to low-cost attacks (less than $50), including
Wi-Fi spoofing combined with GNSS jamming, as well as more sophisticated
coordinated location spoofing. These attacks manipulate position data to
control or undermine LBS functionality, leading to user scams or service
manipulation. Therefore, we propose a countermeasure to detect and thwart such
attacks by utilizing readily available, redundant positioning information from
off-the-shelf platforms. Our method extends the receiver autonomous integrity
monitoring (RAIM) framework by incorporating opportunistic information,
including data from onboard sensors and terrestrial infrastructure signals,
and, naturally, GNSS. We theoretically show that the fusion of heterogeneous
signals improves resilience against sophisticated adversaries on multiple
fronts. Experimental evaluations show the effectiveness of the proposed scheme
in improving detection accuracy by 62% at most compared to baseline schemes and
restoring accurate positioning.

</details>


### [82] [Sockeye: a language for analyzing hardware documentation](https://arxiv.org/abs/2510.27485)
*Ben Fiedler,Samuel Gruetter,Timothy Roscoe*

Main category: cs.CR

TL;DR: 提出了一种领域专用语言，用于描述SoC硬件语义、软件行为假设和安全属性，并对8个SoC进行了机器可读规格化与形式化证明，揭示文档错误并发现实际漏洞，提供可扩展的系统级安全分析工具。


<details>
  <summary>Details</summary>
Motivation: System-on-Chip的复杂性和英语描述的歧义导致对平台安全的形式化陈述困难。需要一个可机器可读的、可验证的框架来描述硬件语义、软件假设和安全属性，并对整个SoC进行形式化证明。

Method: 设计一个DSL描述硬件语义、软件行为假设和安全目标；基于公开参考手册，将8个SoC转化为机器可读规范；进行形式化证明，覆盖内存保密性和完整性；在分析过程中发现若干文档错误，并在分析一个真实服务器芯片时发现漏洞；工具链支持系统集成商用以描述并证明安全属性或生成反例。

Result: 对8个SoC的安全性进行了形式化证明（内存机密性/完整性），发现文档错误，揭示一个实际服务器芯片的漏洞；构建的工具链能对整个SoC的安全属性进行证明或给出反例。

Conclusion: 该方法为系统级的安全属性描述与证明提供了可扩展的框架，帮助系统集成商在设计阶段及早发现安全问题，提升SoC安全性；未来工作包括提升DSL表达力、证明可扩展性、提升自动化程度和对不同硬件模块的适配性等。

Abstract: Systems programmers have to consolidate the ever growing hardware mess
present on modern System-on-Chips (SoCs). Correctly programming a multitude of
components, providing functionality but also security, is a difficult problem:
semantics of individual units are described in English prose, descriptions are
often underspecified, and prone to inaccuracies. Rigorous statements about
platform security are often impossible.
  We introduce a domain-specific language to describe hardware semantics,
assumptions about software behavior, and desired security properties. We then
create machine-readable specifications for a diverse set of eight SoCs from
their reference manuals, and formally prove their (in-)security. In addition to
security proofs about memory confidentiality and integrity, we discover a
handful of documentation errors. Finally, our analysis also revealed a
vulnerability on a real-world server chip. Our tooling offers system
integrators a way of formally describing security properties for entire SoCs,
and means to prove them or find counterexamples to them.

</details>


### [83] [Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models](https://arxiv.org/abs/2510.27629)
*Boyi Wei,Zora Che,Nathaniel Li,Udari Madhushani Sehwag,Jasper Götting,Samira Nedungadi,Julian Michael,Summer Yue,Dan Hendrycks,Peter Henderson,Zifan Wang,Seth Donoughe,Mantas Mazeika*

Main category: cs.CR

TL;DR: 该论文提出 eval 框架，用以评估用于降低生物基础模型的双重用途能力的鲁棒性，发现当前数据过滤可能不足以阻止潜在滥用。


<details>
  <summary>Details</summary>
Motivation: Open-weight bio-foundation models 具有显著的潜在双重用途风险，需要评估现有防护措施的有效性，以及在微调阶段的鲁棒性。

Method: 提出 eval 框架，围绕三个方面评估：序列建模、突变效应预测、致病性预测；通过对比过滤数据的可替代性、在预训练后的表示中的信号等，评估过滤策略的局限性；使用线性探测等方法从预训练表征中提取信息。

Result: 结果显示：1) 现有过滤可能被微调快速恢复，被用于生成被排除知识的能力，且在序列建模方面具有更广的泛化性；2) 双重用途信号已存在于预训练表征中，可通过简单线性探测提取；3) 仅靠数据过滤难以成为单一安全策略。

Conclusion: 需要进一步研究更鲁棒的安全与安全策略来管理开放权重生物基础模型的双重用途风险，数据过滤不能单独作为唯一防护措施。

Abstract: Open-weight bio-foundation models present a dual-use dilemma. While holding
great promise for accelerating scientific research and drug development, they
could also enable bad actors to develop more deadly bioweapons. To mitigate the
risk posed by these models, current approaches focus on filtering biohazardous
data during pre-training. However, the effectiveness of such an approach
remains unclear, particularly against determined actors who might fine-tune
these models for malicious use. To address this gap, we propose \eval, a
framework to evaluate the robustness of procedures that are intended to reduce
the dual-use capabilities of bio-foundation models. \eval assesses models'
virus understanding through three lenses, including sequence modeling,
mutational effects prediction, and virulence prediction. Our results show that
current filtering practices may not be particularly effective: Excluded
knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits
broader generalizability in sequence modeling. Furthermore, dual-use signals
may already reside in the pretrained representations, and can be elicited via
simple linear probing. These findings highlight the challenges of data
filtering as a standalone procedure, underscoring the need for further research
into robust safety and security strategies for open-weight bio-foundation
models.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [84] [Investigation of Superdirectivity in Planar Holographic Arrays](https://arxiv.org/abs/2510.26803)
*Hang Lin,Liuxun Xue,Shu Sun,Ruifeng Gao,Jue Wang,Tengjiao Wang*

Main category: eess.SP

TL;DR: 推导了统一矩形阵列（URA）在全息MIMO中的最大直接度上限，并给出其解析表达式；通过耦合效应的合理利用，可显著提升直接度，但在深次波长间距时收益递减。


<details>
  <summary>Details</summary>
Motivation: 研究URAs的超指向性以提升全息阵列在5G/6G中的容量与方向性控制，填补对耦合影响在高密度阵列中的理论理解。

Method: 建立URAs的直接度数学模型，推导最大直接度的解析表达式，并结合数值仿真分析耦合效应对直接度的影响。

Result: 通过耦合的合理利用可显著提高直接度；但当天线间距趋向深次波长时，收益趋于饱和或下降。在数值仿真中得到的结果与理论分析一致，验证了耦合对超指向性的关键作用及其上限。

Conclusion: 为全息URAs的设计提供理论基础，并为5G/6G中的全息阵列优化提供洞见。

Abstract: This paper studies the superdirectivity characteristics of uniform
rectangular arrays (URAs) for holographic multiple-input multiple-output
systems. By establishing a mathematical directivity model for the URA, an
analytical expression for the maximum directivity is derived. Accordingly,
systematic analysis is performed in conjunction with numerical simulations.
Results show that the directivity can be significantly enhanced via rational
utilization of coupling effects. However, this enhancement yields diminishing
returns when antenna spacings transition to deep sub-wavelength scales. This
study provides a theoretical basis for the design of superdirective URAs and
offers valuable insights for holographic array optimization in 5G/6G
communication systems.

</details>


### [85] [Joint optimization of microphone array geometry, sensor directivity pattern, and beamforming parameters for linear superarrays](https://arxiv.org/abs/2510.26822)
*Yuanhang Qian,Xueqin Luo,Jilu Jin,Gongping Huang,Jingdong Chen,Jacob Benesty*

Main category: eess.SP

TL;DR: 提出一种通用的线性超阵列(LSA)优化框架，联合优化阵列几何、单元指向性和波束形成滤波器，以在全频带和感兴趣区域内最小化设计波束形与理想指向性(IDP)的近似误差；通过Jacobi-Anger展开近似IDP，利用遗传算法优化几何与指向性；仿真表明相较传统LSA，误差更小且方向性增益和白噪声增益更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有线性差分麦克风阵列（LDMAs）和LSA在波束形成性能上受限，未能对阵列几何和单元指向性进行联合优化，从而难以在宽频带和广视角内实现理想波束响应。

Method: 提出广义LSA优化框架：同时优化阵列几何、单元指向性与波束形成滤波器；使用Jacobi-Anger级数展开来近似理想指向性(IDP)；通过遗传算法进行几何和指向性的全局优化，以最小化所设计波束图与IDP在目标频带和感兴趣区域内的误差。

Result: 仿真结果显示，该优化阵列在目标频带和转向范围内的近似误差低于传统LSA；并且其指向因子和白噪声增益在不同频率和转向角度上表现更稳定、性能更好。

Conclusion: 将阵列几何、单元指向性与波束形成滤波的耦合优化，提升了波束形近似度与鲁棒性，展现出在LSA框架下实现更灵活、稳健波束形成的潜力。

Abstract: Linear superarrays (LSAs) have been proposed to address the limited steering
capability of conventional linear differential microphone arrays (LDMAs) by
integrating omnidirectional and directional microphones, enabling more flexible
beamformer designs. However, existing approaches remain limited because array
geometry and element directivity, both critical to beamforming performance, are
not jointly optimized. This paper presents a generalized LSA optimization
framework that simultaneously optimizes array geometry, element directivity,
and the beamforming filter to minimize the approximation error between the
designed beampattern and an ideal directivity pattern (IDP) over the full
frequency band and all steering directions within the region of interest. The
beamformer is derived by approximating the IDP using a Jacobi-Anger series
expansion, while the array geometry and element directivity are optimized via a
genetic algorithm. Simulation results show that the proposed optimized array
achieves lower approximation error than conventional LSAs across the target
frequency band and steering range. Additionally, its directivity factor and
white noise gain demonstrate more stable and improved performance across
frequencies and steering angles.

</details>


### [86] [RFI Detection and Identification at OVRO Using Pseudonymetry](https://arxiv.org/abs/2510.27078)
*Meles Weldegebriel,Zihan Li,Greg Hellbourg,Ning Zhang,Neal Patwari*

Main category: eess.SP

TL;DR: 首次在OVRO进行的现场演示表明伪名化可让被动接收机检测并识别干扰源，即使在低SNR下也能触发关闭干扰传输，保护射电天文观测。


<details>
  <summary>Details</summary>
Motivation: 现有的数据库协调和射电静默区无法快速识别或抑制特定干扰源，特别是在低信噪比场景下。

Method: 在实验中，窄带次级发射机在信号中嵌入伪名水印，广带射电望远镜通过谱图数据被动提取水印；实现异构系统的协同频谱共享；这是对Over-the-Air field demonstration of Pseudonymetry的现场应用演示。

Result: 干扰在低SNR下可被可靠检测，干扰设备可被唯一识别，传统解调不可行时仍有效。

Conclusion: 被动科学接收机可参与轻量化反馈回路以触发关闭有害传输，Pseudonymetry可作为保护射电天文环境的补充执法工具。

Abstract: Protecting radio astronomy observatories from unintended interference is
critical as wireless transmissions increases near protected bands. While
database-driven coordination frameworks and radio quiet zones exist, they
cannot rapidly identify or suppress specific interfering transmitters,
especially at low signal-to-noise ratio (SNR) levels. This paper presents the
first over-the-air field demonstration of Pseudonymetry at the Owens Valley
Radio Observatory (OVRO), illustrating cooperative spectrum sharing between
heterogeneous wireless systems. In our experiment, a narrow-band secondary
transmitter embeds a pseudonym watermark into its signal, while the wide-band
radio telescope passively extracts the watermark from spectrogram data. Results
show that interference can be reliably detected and the interfering device
uniquely identified even at low SNR where conventional demodulation is
infeasible. These findings validate that passive scientific receivers can
participate in a lightweight feedback loop to trigger shutdown of harmful
transmissions, demonstrating the potential of Pseudonymetry as a complementary
enforcement tool for protecting radio astronomy environments.

</details>


### [87] [Unlimited Sampling of Multiband Signals: Single-Channel Acquisition and Recovery](https://arxiv.org/abs/2510.27110)
*Gal Shtendel,Ayush Bhandari*

Main category: eess.SP

TL;DR: 在USF框架下，提出从模制折叠（modulo）采样重构多带信号的低复杂度单通道方法，实现子Nyquist采样并提升动态范围；理论与硬件实验表明可在6个谱带内达到约13x的动态范围提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统多带信号采样在高动态范围下的高采样率与设备复杂性问题；通过模制采样与USF实现低复杂度、单通道的子Nyquist重构，并收紧带通信号的采样定理。

Method: 在Unlimited Sensing Framework下，利用模折叠采样构造重建算法并给出理论保证，针对单通道低复杂度实现；包含理论分析、算法设计，以及对硬件的实验验证。

Result: 给出在USF下的子Nyquist重建保证；对带通信号的采样定理进行了收紧；实现的重建算法在硬件实验中对多达6个谱带的情形，动态范围提升约13倍。

Conclusion: 证明USF与模折叠采样相结合可实现高动态范围的实时多带信号采集，降低动态范围瓶颈与过采样需求，具有实际应用潜力。

Abstract: In this paper, we address the problem of reconstructing multiband signals
from modulo-folded, pointwise samples within the Unlimited Sensing Framework
(USF). Focusing on a low-complexity, single-channel acquisition setup, we
establish recovery guarantees demonstrating that sub-Nyquist sampling is
achievable under the USF paradigm. In doing so, we also tighten the previous
sampling theorem for bandpass signals. Our recovery algorithm demonstrates up
to a 13x dynamic range improvement in hardware experiments with up to 6
spectral bands. These results enable practical high-dynamic-range multiband
acquisition in scenarios previously limited by dynamic range and excessive
oversampling.

</details>


### [88] [From OFDM to AFDM: Enabling Adaptive Integrated Sensing and Communication in High-Mobility Scenarios](https://arxiv.org/abs/2510.27192)
*Haoran Yin,Yanqun Tang,Jun Xiong,Fan Liu,Yuanhan Ni,Qu Luo,Roberto Bomfin,Marwa Chafii,Marios Kountouris,Christos Masouros*

Main category: eess.SP

TL;DR: 本工作对 Doppler-鲁棒的 AFDM-ISAC 进行全面综述，强调在高移动场景中以 AFDM 替代 OFDM 来提升 ISAC 性能；通过分析多样性阶数、歧义函数与 CRB 等性能界限，并提出感知算法与未来研究机会。


<details>
  <summary>Details</summary>
Motivation: 在车联网、无人机等高移动应用中，信道往往存在显著的延迟和多普勒扩展，导致 OFDM 的性能下降。需要一种 doppler 鲁棒的波形来提升 ISAC 的联合感知与通信能力，AFDM 作为候选波形显示出潜力。

Method: 系统性介绍 AFDM-ISAC 的基本原理，强调其具有频率调制的连续波 (FMCW) 风格特征；通过分析 AFDM 的多样性阶数、歧义函数（AF）与科西CRB（CRB）来界定性能极限；并给出若干有效的 sensing 算法与未来研究机会。

Result: 揭示 AFDM-ISAC 的潜力及在高移动场景中的应用前景，给出其在多样性、AF 与 CRB 等方面的性能界限，以及可行的感知算法框架，但需进一步实证分析与实现细化。

Conclusion: AFDM-ISAC 作为高移动场景下的有力候选波形，具备对 OFDM 的潜在显著优势与灵活性，但仍需更深入的算法优化和实际系统验证，以充分发挥其感知与通信的协同能力。

Abstract: Integrated sensing and communication (ISAC) is a key feature of
next-generation wireless networks, enabling a wide range of emerging
applications such as vehicle-to-everything (V2X) and unmanned aerial vehicles
(UAVs), which operate in high-mobility scenarios. Notably, the wireless
channels within these applications typically exhibit severe delay and Doppler
spreads. The latter causes serious communication performance degradation in the
Orthogonal Frequency-Division Multiplexing (OFDM) waveform that is widely
adopted in current wireless networks. To address this challenge, the recently
proposed Doppler-resilient affine frequency division multiplexing (AFDM)
waveform, which uses flexible chirp signals as subcarriers, shows great
potential for achieving adaptive ISAC in high-mobility scenarios. This article
provides a comprehensive overview of AFDM-ISAC. We begin by presenting the
fundamentals of AFDM-ISAC, highlighting its inherent frequency-modulated
continuous-wave (FMCW)-like characteristics. Then, we explore its ISAC
performance limits by analyzing its diversity order, ambiguity function (AF),
and Cramer-Rao Bound (CRB). Finally, we present several effective sensing
algorithms and opportunities for AFDM-ISAC, with the aim of sparking new ideas
in this emerging field.

</details>


### [89] [Joint Visible Light and Backscatter Communications for Proximity-Based Indoor Asset Tracking Enabled by Energy-Neutral Devices](https://arxiv.org/abs/2510.27217)
*Boxuan Xie,Lauri Mela,Alexis A. Dowhuszko,Yu Bai,Zehui Xiong,Zhu Han,Dusit Niyato,Riku Jäntti*

Main category: eess.SP

TL;DR: 提出在SLIPT框架下，将可见光定位与背散通信结合的能源中性室内定位系统(BD在LED端供电、反射RF载波指示位置)，以多灯互干扰抑制的四色调度和边缘RF读取器的粒子滤波融合定位，实现亚米级定位且无需复杂传感器。


<details>
  <summary>Details</summary>
Motivation: 针对物联网对能源中性和可持续性的需求，利用普适的照明基础设施实现对能量中立设备的定位与跟踪；VLP与RF互补，降低传感器成本和功耗。

Method: 提出低复杂度的背散设备(BD)，从LED接入点获取能量并调制反射RF载波来指示在特定VLC小区的位置；在多小区VLC部署中使用FDM并通过四色映射调度分配LED载波频段以降低干扰；在边缘RF阅读器上提出轻量级粒子滤波跟踪算法，将邻近报告与接收回传信号强度融合以实现BD跟踪。

Result: 实验结果显示中位定位误差0.318 m，90百分位误差0.634 m；BD无需复杂光探测器与主动RF合成器，表现对多条室内轨迹具有鲁棒性，并具备可扩展性、低成本及能源中性特征。

Conclusion: 该工作证明了在VLC-BC-SLIPT框架下的能源中立室内定位是可行且具扩展性的，适合普及化的物联网边缘协助应用。

Abstract: In next-generation wireless systems, providing location-based mobile
computing services for energy-neutral devices has become a crucial objective
for the provision of sustainable Internet of Things (IoT). Visible light
positioning (VLP) has gained great research attention as a complementary method
to radio frequency (RF) solutions since it can leverage ubiquitous lighting
infrastructure. However, conventional VLP receivers often rely on
photodetectors or cameras that are power-hungry, complex, and expensive. To
address this challenge, we propose a hybrid indoor asset tracking system that
integrates visible light communication (VLC) and backscatter communication (BC)
within a simultaneous lightwave information and power transfer (SLIPT)
framework. We design a low-complexity and energy-neutral IoT node, namely
backscatter device (BD) which harvests energy from light-emitting diode (LED)
access points, and then modulates and reflects ambient RF carriers to indicate
its location within particular VLC cells. We present a multi-cell VLC
deployment with frequency division multiplexing (FDM) method that mitigates
interference among LED access points by assigning them distinct frequency pairs
based on a four-color map scheduling principle. We develop a lightweight
particle filter (PF) tracking algorithm at an edge RF reader, where the fusion
of proximity reports and the received backscatter signal strength are employed
to track the BD. Experimental results show that this approach achieves the
positioning error of 0.318 m at 50th percentile and 0.634 m at 90th percentile,
while avoiding the use of complex photodetectors and active RF synthesizing
components at the energy-neutral IoT node. By demonstrating robust performance
in multiple indoor trajectories, the proposed solution enables scalable,
cost-effective, and energy-neutral indoor tracking for pervasive and
edge-assisted IoT applications.

</details>


### [90] [Variational Bayesian Estimation of Low Earth Orbits for Satellite Communication](https://arxiv.org/abs/2510.27345)
*Anders Malthe Westerkam,Amélia Struyf,Dimitri Lederer,Troels Pedersen,François Quitin*

Main category: eess.SP

TL;DR: 提出了一种基于变分消息传递的联合定位与波束跟踪算法，用于地面站具备混合模拟-数字架构的LEO卫星。假设轨道为圆形，通过轨道参数估计来推导角度，实验结果表明对探测丢失有较强鲁棒性，在视线边缘也能实现稳定跟踪，并降低混合架构的歧义性。


<details>
  <summary>Details</summary>
Motivation: LEO卫星的高轨道速度导致观测时间短且需要快速波束对准；毫米波大阵列在混合模拟-数字架构下的波束跟踪存在歧义和鲁棒性挑战。该工作旨在通过概率推断在尽可能少的观测信息下实现定位与跟踪，从而提高信号覆盖和跟踪可靠性。

Method: 提出一个变分消息传递（Variational Message Passing, VMP）算法来联合估计卫星轨道参数（在圆形轨道模型下），并据此从轨道推导出到 ground station 的观测角度。该算法在混合模拟-数字波束成形体系中运行，利用轨道参数的贝叶斯形式进行推断以提升鲁棒性。

Result: 仿真结果表明该方法对探测丢失高度鲁棒；能够在视界边缘附近实现可靠跟踪；并有效缓解混合架构固有的歧义性。

Conclusion: 在圆形轨道模型假设下，变分消息传递框架能实现面向LEO毫米波系统的联合定位与波束跟踪，具有对探测失败的鲁棒性及在混合架构下的歧义缓解能力，后续工作可拓展到非圆形轨道与更复杂的轨道模型。

Abstract: Low-earth-orbit (LEO) satellite communication systems that use
millimeter-wave frequencies rely on large antenna arrays with hybrid
analog-digital architectures for rapid beam steering. LEO satellites are only
visible from the ground for short periods of times (a few tens of minutes) due
to their high orbital speeds. This paper presents a variational message passing
algorithm for joint localization and beam tracking of a LEO satellite from a
ground station equipped with a hybrid transceiver architecture. The algorithm
relies on estimating the parameters of the orbit, which is modelled as
circular. Angles are then obtained from the orbit in a straightforward manner.
Simulation results show that the proposed method is highly resilient to missed
detections, enables reliable satellite tracking even near the horizon, and
effectively alleviates the ambiguities inherent in hybrid architectures.

</details>


### [91] [UNILocPro: Unified Localization Integrating Model-Based Geometry and Channel Charting](https://arxiv.org/abs/2510.27394)
*Yuhao Zhang,Guangjin Pan,Musa Furkan Keskin,Ossi Kaltiokallio,Mikko Valkama,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出UNILocPro：在混合LoS/NLoS场景下联合模型基定位和通道图(CC)的统一定位框架，基于LoS/NLoS识别进行自适应切换；通过无监督学习利用模型端信息训练CC模型，设计成对距离、三元组、LoS与最优传输（OT）损失等以保留全局几何；并给出低复杂度UNILoc版本，通过自生成标签和单次OT转换避免Sinkhorn迭代。


<details>
  <summary>Details</summary>
Motivation: 解决混合LoS/NLoS场景下定位精度与训练复杂度的挑战；充分利用模型基定位与CC的互补性，提供无监督且低复杂度的解决方案。

Method: 基于LoS/NLoS识别的自适应模型激活；用模型端信息训练CC模型；联合使用成对距离损失（新型不相似度度量）、若有时间戳则的三元组损失、LoS损失和OT损失，以保持全局几何。提出UNILocPro的可行性并实现低复杂度UNILoc（自生成标签、单次OT转化，避免Sinkhorn迭代）。

Result: 实验数据显示，所提统一框架在定位精度上显著优于单独的模型基与CC方法；UNILocPro在有时间戳时达到与全监督指纹化相当的性能；低复杂度UNILoc显著降低训练复杂度，且性能降幅有限。

Conclusion: 给出了一种在混合LoS/NLoS场景下高效、无监督、低复杂度的统一定位框架，证明了模型基与CC的互补性以及自生成标签与OT转换在保持全局几何中的有效性。

Abstract: In this paper, we propose a unified localization framework (called UNILocPro)
that integrates model-based localization and channel charting (CC) for mixed
line-of-sight (LoS)/non-line-of-sight (NLoS) scenarios. Specifically, based on
LoS/NLoS identification, an adaptive activation between the model-based and
CC-based methods is conducted. Aiming for unsupervised learning, information
obtained from the model-based method is utilized to train the CC model, where a
pairwise distance loss (involving a new dissimilarity metric design), a triplet
loss (if timestamps are available), a LoS-based loss, and an optimal transport
(OT)-based loss are jointly employed such that the global geometry can be well
preserved. To reduce the training complexity of UNILocPro, we propose a
low-complexity implementation (called UNILoc), where the CC model is trained
with self-generated labels produced by a single pre-training OT transformation,
which avoids iterative Sinkhorn updates involved in the OT-based loss
computation. Extensive numerical experiments demonstrate that the proposed
unified frameworks achieve significantly improved positioning accuracy compared
to both model-based and CC-based methods. Notably, UNILocPro with timestamps
attains performance on par with fully-supervised fingerprinting despite
operating without labelled training data. It is also shown that the
low-complexity UNILoc can substantially reduce training complexity with only
marginal performance degradation.

</details>


### [92] [Classification of Induction Motor Fault and Imbalance Based on Vibration Signal Using Single Antenna's Reactive Near Field](https://arxiv.org/abs/2510.27382)
*Sagar Dutta,Banani Basu,Fazal Ahmed Talukdar*

Main category: eess.SP

TL;DR: Antenna-based, noninvasive bearing fault and imbalance detection in induction motors using S11 with deep learning, achieving high classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Reduce economic losses from motor failures by enabling early fault diagnosis with a low-cost, noninvasive sensing approach instead of expensive sensor deployments requiring expert installation.

Method: Measure time-varying S11 with an omnidirectional antenna, analyze spectrograms and Validate fault frequencies via FFT on S11 data; evaluate average power under normal and fault conditions; train a deep learning model to classify faults using magnitude and phase of S11 across varying operating conditions.

Result: High classification accuracies: 98.2% (magnitude+phase), 96% (magnitude), 92.1% (phase); additionally studied effects of operating frequency, antenna location, and time window on accuracy.

Conclusion: Antenna-based S11 reflection-coefficient with deep learning provides accurate, noninvasive fault/imbalance detection in induction motors, robust to operational variabilities and cost-effective.

Abstract: Early fault diagnosis is imperative for the proper functioning of rotating
machines. It can reduce economic losses in the industry due to unexpected
failures. Existing fault analysis methods are either expensive or demand
expertise for the installation of the sensors. This article proposes a novel
method for the detection of bearing faults and imbalance in induction motors
using an antenna as the sensor, which is noninvasive and cost-efficient.
Time-varying S11 is measured using an omnidirectional antenna, and it is seen
that the spectrogram of S11 shows unique characteristics for different fault
conditions. The experimental setup has analytically evaluated the vibration
frequencies due to fault and validated the characteristic fault frequency by
applying FFT analysis on the captured S11 data. This article has evaluated the
average power content of the detected signals at normal and different fault
conditions. A deep learning model is used to classify the faults based on the
reflection coefficient ( S11). It is found that classification accuracy of
98.2% is achieved using both magnitude and phase of S11, 96% using the
magnitude of S11 and 92.1% using the phase of S11. The classification accuracy
for different operating frequencies, antenna location, and time windows are
also investigated.

</details>


### [93] [Estimation of aboveground biomass in a tropical dry forest: An intercomparison of airborne, unmanned, and space laser scanning](https://arxiv.org/abs/2510.27408)
*Nelson Mattié,Arturo Sanchez-Azofeifa,Pablo Crespo-Peremarch,Juan-Ygnacio López-Hernández*

Main category: eess.SP

TL;DR: 本研究通过多源激光扫描数据与机器学习方法，提升热带干燥林地上生物量（AGB）的估算精度；发现六个人为变量与高度相关，叶面积指数、冠层覆盖、地形高程及全波形信号能量等为关键变量，SVM在多源数据上的误差约为17 Mg/ha。


<details>
  <summary>Details</summary>
Motivation: 响应巴黎协定对全球森林碳数据的高质量需求，提升热带干燥林的地上生物量估算方法及数据可用性。

Method: 比较离散与全波形激光扫描数据（如 ALS、ULS、SLS）作为自变量，采用普通最小二乘回归、贝叶斯回归以及支持向量机（SVM）回归；进行变量选择、SVM调参，并通过交叉验证控制过拟合与欠拟合。

Result: 六个与树高相关的变量（如 Elev.minimum、Elev.L3、lev.MAD.mode、Elev.mode、Elev.MAD.median、Elev.skewness）对 AGB 的估算具有重要性；叶面积指数、冠层覆盖与高度、地形高程、全波形信号能量成为最关键变量。十个哥斯达黎加 Guanacaste 的永久样地中 AGB 范围为 26.02–175.43 Mg/ha。SVM 回归在所有激光扫描系统上总体误差为 17.89，且 SLSF W 的误差最低，为 17.07。

Conclusion: 多源激光扫描数据结合机器学习方法可提升热带干燥林的 AGB 估算精度与稳定性，对森林碳监测与巴黎协定相关数据需求具有实际应用价值。

Abstract: According to the Paris Climate Change Agreement, all nations are required to
submit reports on their greenhouse gas emissions and absorption every two years
by 2024. Consequently, forests play a crucial role in reducing carbon
emissions, which is essential for meeting these obligations. Recognizing the
significance of forest conservation in the global battle against climate
change, Article 5 of the Paris Agreement emphasizes the need for high-quality
forest data. This study focuses on enhancing methods for mapping aboveground
biomass in tropical dry forests. Tropical dry forests are considered one of the
least understood tropical forest environments; therefore, there is a need for
accurate approaches to estimate carbon pools. We employ a comparative analysis
of AGB estimates, utilizing different discrete and full-waveform laser scanning
datasets in conjunction with Ordinary Least Squares and Bayesian approaches
SVM. Airborne Laser Scanning, Unmanned Laser Scanning, and Space Laser Scanning
were used as independent variables for extracting forest metrics. Variable
selection, SVM regression tuning, and cross-validation via a machine-learning
approach were applied to account for overfitting and underfitting. The results
indicate that six key variables primarily related to tree height: Elev.minimum,
Elev.L3, lev.MAD.mode, Elev.mode, Elev.MAD.median, and Elev.skewness, are
important for AGB estimation using ALSD and ULSD , while Leaf Area Index,
canopy coverage and height, terrain elevation, and full-waveform signal energy
emerged as the most vital variables. AGB values estimated from ten permanent
tropical dry forest plots in Costa Rica Guanacaste province ranged from 26.02
Mg/ha to 175.43 Mg/ha . The SVM regressions demonstrated a 17.89 error across
all laser scanning systems, with SLSF W exhibiting the lowest error 17.07 in
estimating total biomass per plot.

</details>


### [94] [Trends and Challenges in Next-Generation GNSS Interference Management](https://arxiv.org/abs/2510.27576)
*Leatile Marata,Mariona Jaramillo-Civill,Tales Imbiriba,Petri Välisuo,Heidi Kuusniemi,Elena Simona Lohan,Pau Closas*

Main category: eess.SP

TL;DR: 对GNSS干扰挑战的综述，提出以AI辅以物理模型的鲁棒干扰管理框架与研究路线，旨在提升基于GNSS的定位在干扰环境中的可靠性与安全性。


<details>
  <summary>Details</summary>
Motivation: GNSS在自动驾驶与环境监测等新兴应用中持续发展，但干扰威胁日益增加，影响可靠性与安全性。传统以物理模型为基础的信号处理难以充分捕捉干扰的复杂形态，需要AI驱动的数据辅助方法来增强鲁棒性。

Method: 对现有干扰问题、挑战及需完成的任务进行系统讨论，提出将AI与物理模型结合的研究愿景，并给出未来工作的路线图。

Result: 提供一个综合分析、任务划分与研究愿景，明确AI在GNSS干扰管理中的潜在作用、数据需求、评估指标及实现路径。

Conclusion: AI驱动的干扰管理有望显著提升GNSS在现实干扰环境中的鲁棒性和安全性，但需在数据、模型融合、评估框架等方面开展系统性研究与验证。

Abstract: The global navigation satellite system (GNSS) continues to evolve in order to
meet the demands of emerging applications such as autonomous driving and smart
environmental monitoring. However, these advancements are accompanied by a rise
in interference threats, which can significantly compromise the reliability and
safety of GNSS. Such interference problems are typically addressed through
signal-processing techniques that rely on physics-based mathematical models.
Unfortunately, solutions of this nature can often fail to fully capture the
complex forms of interference. To address this, artificial intelligence
(AI)-inspired solutions are expected to play a key role in future interference
management solutions, thanks to their ability to exploit data in addition to
physics-based models. This magazine paper discusses the main challenges and
tasks required to secure GNSS and present a research vision on how AI can be
leveraged towards achieving more robust GNSS-based positioning.

</details>
