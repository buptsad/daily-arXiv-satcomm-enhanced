<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 11]
- [cs.CR](#cs.CR) [Total: 15]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.NI](#cs.NI) [Total: 5]
- [eess.SY](#eess.SY) [Total: 20]
- [cs.LG](#cs.LG) [Total: 68]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Fractional Programming and Manifold Optimization for Reciprocal BD-RIS Scattering Matrix Design](https://arxiv.org/abs/2511.07683)
*Marko Fidanovski,Iván Alexander Morales Sandoval,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Emil Björnson,Bruno Clerckx*

Main category: eess.SP

TL;DR: 在BD-RIS辅助的MU-MISO系统中，利用LDT与QT的FP框架，对散射矩阵进行迭代优化，结合流形优化实现求解，获得更高的sum-rate并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 于多用户场景下，BD-RIS带来额外自由度，但优化复杂度高；FP方法有潜力简化非凸优化并提升性能。

Method: 通过LDT和QT将原始目标转换成等价形式；在流形优化框架下对散射矩阵进行迭代求解；与SotA方法比较。

Result: 理论分析与仿真表明，所提方法降低优化复杂度并在相同系统条件下获得显著的sum-rate提升。

Conclusion: 提出的方法有效且可提升BD-RIS-aided MU-MISO的系统性能，具有实际潜力。

Abstract: We investigate the problem of maximizing the sum-rate performance of a beyond-diagonal reconfigurable intelligent surface (BD-RIS)-aided multi-user (MU)-multiple-input single-output (MISO) system using fractional programming (FP) techniques. More specifically, we leverage the Lagrangian Dual Transform (LDT) and Quadratic Transform (QT) to derive an equivalent objective function which is then solved iteratively via a manifold optimization framework. It is shown that these techniques reduce the complexity of the optimization problem for the scattering matrix solution, while also providing notable performance gains compared to state-of-the-art (SotA) methods under the same system conditions. Simulation results confirm the effectiveness of the proposed method in improving sum-rate performance.

</details>


### [2] [Generative Decoding of Compressed CSI for MIMO Precoding Design](https://arxiv.org/abs/2511.07783)
*Hao Luo,Saeed R. Khosravirad,Ahmed Alkhateeb*

Main category: eess.SP

TL;DR: 提出了一种解码端为主的压缩CSI方案，用户端使用标准化编码器进行CSI压缩，基站使用场站特定的生成解码器对压缩CSI进行 refined。引入两种生成解码器的训练方案（端到端与两阶段），并通过面向目标的损失函数优化。通过建立站点特定的数字孪生体生成合成CSI数据以降低数据采集成本。仿真在不同反馈开销下显示出优势。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO中对CSI的高成本获取与传输，以及ML自编码器在标准化、互操作性、向后兼容性和数据采集开销方面的挑战，提出解码端为主、标准化编码器、站点特定生成解码器和数字孪生数据以提升可部署性和效率。

Method: 在用户端使用标准化的CSI编码器进行压缩；在基站端部署站点特定的生成解码器以利用环境知识对压缩CSI进行 refined。提出两种训练方案：1) 端到端训练；2) 两阶段训练；两者均采用面向目标的损失函数。为降低数据采样成本，利用站点特定的数字孪生体生成用于训练的合成CSI数据。

Result: 仿真结果显示该解码端为主的架构在不同反馈开销条件下具有较好的性能提升，证明了方法的有效性。

Conclusion: 提出的解码端为主、标准化编码器的架构结合数字孪生数据，能够在降低数据采集成本的同时提升CSI重建与反馈的效率，为大规模MIMO的实际部署提供更易标准化和可扩展的路径。

Abstract: Massive MIMO systems can enhance spectral and energy efficiency, but they require accurate channel state information (CSI), which becomes costly as the number of antennas increases. While machine learning (ML) autoencoders show promise for CSI reconstruction and reducing feedback overhead, they introduce new challenges with standardization, interoperability, and backward compatibility. Also, the significant data collection needed for training makes real-world deployment difficult. To overcome these drawbacks, we propose an ML-based, decoder-only solution for compressed CSI. Our approach uses a standardized encoder for CSI compression on the user side and a site-specific generative decoder at the base station to refine the compressed CSI using environmental knowledge. We introduce two training schemes for the generative decoder: An end-to-end method and a two-stage method, both utilizing a goal-oriented loss function. Furthermore, we reduce the data collection overhead by using a site-specific digital twin to generate synthetic CSI data for training. Our simulations highlight the effectiveness of this solution across various feedback overhead regimes.

</details>


### [3] [Toward Adaptive BCIs: Enhancing Decoding Stability via User State-Aware EEG Filtering](https://arxiv.org/abs/2511.07891)
*Yeon-Woo Choi,Hye-Bin Shin,Dan Li*

Main category: eess.SP

TL;DR: 提出一种基于用户状态的EEG过滤框架，通过从EEG特征中持续估计注意力水平并对不可靠片段进行自适应加权，提升BCI解码的鲁棒性、准确性和跨会话稳定性，且无需额外标注。


<details>
  <summary>Details</summary>
Motivation: BCI在鲁棒性和长期自适应方面存在瓶颈：用户注意力波动、脑状态随时间变化以及出现伪影时，性能迅速下降。需要在解码前对信号进行更智能的过滤。

Method: 在实时阶段从EEG特征中估计认知状态（如专注/分心），并基于估计的注意力水平对片段进行自适应加权过滤，抑制嘈杂或聚焦不清的epochs，作为解码前的过滤阶段以减少分布漂移。

Result: 在模拟真实BCI场景的多份EEG数据集上，状态感知过滤相较传统预处理管线提升了分类准确性和跨状态/跨会话的稳定性。

Conclusion: 利用脑源的状态信息，即使无需额外用户标签，也能显著提高基于EEG的BCI的可靠性，为实际应用提供更稳健的解码基础。

Abstract: Brain-computer interfaces (BCIs) often suffer from limited robustness and poor long-term adaptability. Model performance rapidly degrades when user attention fluctuates, brain states shift over time, or irregular artifacts appear during interaction. To mitigate these issues, we introduce a user state-aware electroencephalogram (EEG) filtering framework that refines neural representations before decoding user intentions. The proposed method continuously estimates the user's cognitive state (e.g., focus or distraction) from EEG features and filters unreliable segments by applying adaptive weighting based on the estimated attention level. This filtering stage suppresses noisy or out-of-focus epochs, thereby reducing distributional drift and improving the consistency of subsequent decoding. Experiments on multiple EEG datasets that emulate real BCI scenarios demonstrate that the proposed state-aware filtering enhances classification accuracy and stability across different user states and sessions compared with conventional preprocessing pipelines. These findings highlight that leveraging brain-derived state information--even without additional user labels--can substantially improve the reliability of practical EEG-based BCIs.

</details>


### [4] [DMA-aided MU-MISO Systems for Power Splitting SWIPT via Lorentzian-Constrained Holography](https://arxiv.org/abs/2511.08125)
*Askin Altinoklu,Leila Musavian*

Main category: eess.SP

TL;DR: 本研究提出在DMA辅助的多用户 MISO 系统中，对共址 SWIPT 用户进行最优功率分配与波束成形设计，目标在满足 SINR 与能量收集约束下最小化发射功率，采用基于 SDP 的交替优化框架，并结合 Lorentzian-constrained holography（LCH），其中包括自适应半径 LCH（ARLCH）等方案，且评估非线性EH模型与电路噪声对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 在避免大量 RF 链与相控元件的前提下提升 DMA 架构下的 SWIPT 能效与可控性；同时解决 Lorentzian 约束下的幅度/相位优化难题，以实现高效的功率传输与信息传输。

Method: 建立基于 SDP 的交替优化框架，考虑 DMA 的 Lorentzian 约束与波束成形；引入多种 LCH 策略（包括 ARLCH）以提升可调性；在模型中加入非线性能量收集（EH）模型与电路噪声，并与基线方法进行对比评估。

Result: 仿真表明所提方法在保持 SINR 与 EH 要求的前提下显著降低发射功率，相比基线方法具有更高的能效；ARLCH 的引入提升了优化空间的利用率，并且对非线性 EH 与电路噪声具有鲁棒性表现。

Conclusion: 在 DMA 辅助的 SWIPT 系统中，结合 Lorentzian 约束下的波束成形与自适应半径 LCH 能实现更低的发射功率和更好的人机平衡，优于传统架构，显示出 DMA+LCH 的潜力与应用前景。

Abstract: This paper presents an optimal power splitting and beamforming design for co-located simultaneous wireless information and power transfer (SWIPT) users in Dynamic Metasurface Antenna (DMA)-aided multiuser multiple-input single-output (MISO) systems. The objective is to minimize transmit power while meeting users signal-to-interference-plus-noise ratio (SINR) and energy harvesting (EH) requirements. The problem is solved via an alternating optimization framework based on semidefinite programming (SDP), where metasurface tunability follows Lorentzian-constrained holography (LCH). In contrast to traditional beamforming architectures, DMA-assisted architectures reduce the need for RF chains and phase shifters but require optimization under the Lorentzian constraint limiting the amplitude and phase optimizations. Hence, the proposed method integrates several LCH schemes, including the recently proposed adaptive-radius LCH (ARLCH), and evaluates nonlinear EH models and circuit noise effects. Simulation results show that the proposed design significantly reduces transmit power compared with baseline methods, highlighting the efficiency of ARLCH and optimal power splitting in DMA-assisted SWIPT systems.

</details>


### [5] [MA-enhanced Mixed Near-field and Far-field Covert Communications](https://arxiv.org/abs/2511.08107)
*Chao Zhou,Changsheng You,Cong Zhou,Hai Lin,Yi Gong*

Main category: eess.SP

TL;DR: Movable XL-array-assisted covert communications in mixed near-/far-field scenarios improves sum-rate and relaxes covertness constraints; introduces an optimization framework with inner beamforming and outer subarray movement, solved by successive convex approximation and differential evolution; numerical results validate effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing work largely assumes either near-field or far-field and fixed-position XL-arrays. In a realistic mixed-field setting with multiple Bobs and Willies, energy-spread and correlation control become critical for both rate and covertness. A movable, modular XL-array can dynamically reconfigure and mitigate energy spread, enabling better trade-offs.

Method: Propose a modular-based movable XL-array at Alice for mixed-field covert communications. Analyze a two-Bob-one-Willie system to illustrate energy-spread mitigation via subarray movement and relaxed covertness requirements. Formulate a general optimization to maximize sum-rate under a covertness constraint, decomposed into inner beamforming optimization given subarray positions and outer subarray movement optimization. Solve the inner problem via successive convex approximation; solve the outer problem with a customized differential evolution algorithm. Provide numerical results to demonstrate performance gains.

Result: Movable XL-arrays effectively balance sum-rate and covert requirements in mixed-field scenarios. Subarray movement mitigates energy-spread and enhances channel controllability, enabling better secrecy-like constraints. The proposed optimization framework yields high-quality solutions with significant performance gains over fixed-position configurations.

Conclusion: A movable modular XL-array offers substantial advantages for mixed-field covert communications, enabling improved rate-covertness trade-offs. The two-tier optimization framework (beamforming with fixed subarray positions and subarray movement) provides a practical approach, validated by numerical results.

Abstract: In this paper, we propose to employ a modular-based movable extremely large-scale array (XL-array) at Alice for enhancing covert communication performance. Compared with existing work that mostly considered either far-field or near-field covert communications, we consider in this paper a more general and practical mixed-field scenario, where multiple Bobs are located in either the near-field or far-field of Alice, in the presence of multiple near-field Willies. Specifically, we first consider a two-Bob-one-Willie system and show that conventional fixed-position XL-arrays suffer degraded sum-rate performance due to the energy-spread effect in mixed-field systems, which, however, can be greatly improved by subarray movement. On the other hand, for transmission covertness, it is revealed that sufficient angle difference between far-field Bob and Willie as well as adequate range difference between near-field Bob and Willie are necessary for ensuring covertness in fixed-position XL-array systems, while this requirement can be relaxed in movable XL-array systems thanks to flexible channel correlation control between Bobs and Willie. Next, for general system setups, we formulate an optimization problem to maximize the achievable sum-rate under covertness constraint. To solve this non-convex optimization problem, we first decompose it into two subproblems, corresponding to an inner problem for beamforming optimization given positions of subarrays and an outer problem for subarray movement optimization. Although these two subproblems are still non-convex, we obtain their high-quality solutions by using the successive convex approximation technique and devising a customized differential evolution algorithm, respectively. Last, numerical results demonstrate the effectiveness of proposed movable XL-array in balancing sum-rate and covert communication requirements.

</details>


### [6] [Mutual Coupling Aware Channel Estimation for RIS-Aided Multi-User mmWave Systems](https://arxiv.org/abs/2511.08112)
*Tian Qiu,Ruidong Li,Cunhua Pan,Taihaon Zhang,Dongnan Xia,Changhong Wang,Hong Ren*

Main category: eess.SP

TL;DR: 提出一种三阶段上行信道估计方案，用于RIS辅助的多用户毫米波MISO系统，显式考虑互耦MC效应。Stage I利用降维子空间法估计BS端公共AoA；Stage II在MC感知下对典型用户进行级联信道估计，提取等效观测向量并用CS重建参考列，进而重排估计RIS端AoA并获得AoD；Stage III利用已知结构估计其他用户的级联信道。RIS相位训练矩阵在MC存在时设计以提升性能。仿真表明在估计精度和导频效率上优于MC不可知和现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决RIS辅助的MU mmWave系统中的互耦（MC）效应对级联信道估计的影响，同时降低对齐导频开销与计算复杂度，提高估计准确性。

Method: 提出三阶段估计框架：Stage I 使用降维子空间方法估计BS端的公共AoA；Stage II 针对某一典型用户进行MC感知的级联信道估计，提取等效测量向量并用CS重建参考列，重排以估计RIS端AoA并获取AoD；Stage III 基于前述结构估计其他用户的级联信道并降低开销；设计在MC下鲁棒的RIS相位训练矩阵以提升性能。

Result: 仿真结果显示所提方法在估计精度和导频效率方面优于MC不可知与现有方案。

Conclusion: 提出的MC感知三阶段方案可有效应对MC问题，降低级联信道估计的开销和计算量，同时提升估计性能，适用于RIS-MU mmWave系统。

Abstract: This paper proposes a three-stage uplink channel estimation protocol for reconfigurable intelligent surface (RIS)-aided multi-user (MU) millimeter-wave (mmWave) multiple-input single-output (MISO) systems, where both the base station (BS) and the RIS are equipped with uniform planar arrays (UPAs). The proposed approach explicitly accounts for the mutual coupling (MC) effect, modeled via scattering parameter multiport network theory. In Stage~I, a dimension-reduced subspace-based method is proposed to estimate the common angle of arrival (AoA) at the BS using the received signals across all users. In Stage~II, MC-aware cascaded channel estimation is performed for a typical user. The equivalent measurement vectors for each cascaded path are extracted and the reference column is reconstructed using a compressed sensing (CS)-based approach. By leveraging the structure of the cascaded channel, the reference column is rearranged to estimate the AoA at the RIS, thereby reducing the computational complexity associated with estimating other columns. Additionally, the common angle of departure (AoD) at the RIS is also obtained in this stage, which significantly reduces the pilot overhead for estimating the cascaded channels of other users in Stage~III. The RIS phase shift training matrix is designed to optimize performance in the presence of MC and outperforms random phase scheme. Simulation results validate that the proposed method yields better performance than the MC-unaware and existing approaches in terms of estimation accuracy and pilot efficiency.

</details>


### [7] [Effective Capacity Analysis of Joint Near and Far-Field Communication in 6G URLLC Networks](https://arxiv.org/abs/2511.08237)
*Humera Hameed,Waqas Aman,Muhammad Mahboob Ur Rahman,Ali Arshad Nasir*

Main category: eess.SP

TL;DR: 提出一种将近场与远场共存的有效容量（EC）分析框架，在距离不确定性下给出闭式EC表达式，并分析估计方差、QoS指数、远近场边界对延迟性能的影响。


<details>
  <summary>Details</summary>
Motivation: 6G时代通过超大阵列和高载波频率实现近场与远场并存，增强容量的同时带来QoS保障的新挑战，亟需定量评估延迟性能的工具。

Method: 建立联合近场/远场传播模型，在距离估计不确定性下将用户位置建模为跨越两种传播区域的随机变量，推导出可解析的EC闭式表达式，并通过数值仿真评估估计方差、QoS指数、Far-field/near-field边界对EC的影响。

Result: 获得了可解析的EC表达式，数值结果显示估计方差、QoS指数、远近场边界对EC有显著影响，能够定量刻画延迟性能的变化。

Conclusion: 提出的EC分析框架为6G极大阵列下近场-远场共存环境提供量化延迟性能的理论工具，便于QoS保障设计与参数选取。

Abstract: The emergence of 6G networks enables simultaneous near-field and far-field communications through extremely large antenna arrays and high carrier frequencies. While these regimes enhance spatial multiplexing and link capacity, their coexistence poses new challenges in ensuring quality-of-service (QoS) guarantees for delay-sensitive applications. This paper presents an effective capacity (EC) analysis framework that jointly models near- and far-field communication regimes under distance estimation uncertainty. The user location is modeled as a random variable spanning both propagation regions, and tractable closed-form expression for the EC is derived to quantify delay performance. Numerical results illustrate the impact of estimation variance, QoS exponent, far-field boundary and near- field boundary (Fraunhofer distance) on EC performance.

</details>


### [8] [Wide Tuning Range and Low Noise Voltage Control Oscillators for 5G Technology](https://arxiv.org/abs/2511.08273)
*Minh Xuan Bui,Nguyen Thien Dat,Van Hong Lam,Tran Le Anh Quan,Pham Hung Anh,Mai Dong Xuan,Ke Wang*

Main category: eess.SP

TL;DR: 提出了一种新型 cascode cross-couple LC VCO 拓扑，实现毫米波5G所需的宽调谐范围与低噪声；理论分析与仿真结果显示相比传统跨耦合 LC VCO，在调谐范围、VCO增益和相噪方面存在显著改进。


<details>
  <summary>Details</summary>
Motivation: 5G 对毫米波VCO提出对宽调谐范围和低相位噪声的高要求，现有的跨耦合 LC VCO 在调谐范围与噪声之间存在权衡。

Method: 给出小信号模型，分析启动条件、振荡频率及影响相位噪声的因素；提出 cascode cross-couple LC VCO 拓扑；通过理论分析和仿真比较新旧拓扑在调谐范围、VCO增益与相噪声方面的性能。

Result: 理论与仿真结果表明新拓扑在调谐范围、VCO增益与相噪方面优于传统结构。

Conclusion: 该拓扑为毫米波5G VCO 的高性能设计提供一种有效途径，适用于需要宽调谐与低噪声的场景；未来工作可在实现细化与工艺适应性方面展开。

Abstract: This paper presents the analytical design of a new wide tuning range and low-noise millimeter-wave voltage control oscillators (VCO) for 5G technology. The small signal model analysis and phase noise of the VCOs will be presented to evaluate the start-up oscillation condition, oscillation frequency, and phase noise affecting factors. Theoretical analysis and simulation results show the outperformance of the proposed cascode cross-couple LC VCO topology compared to the conventional cross-coupled LC VCO in terms of frequency tuning range, VCO gain and phase noise level.

</details>


### [9] [Waveform-domain NOMA: An Enabler for ISAC in Uplink Transmission](https://arxiv.org/abs/2511.08474)
*Jialiang Zhu,Hamza Haif,Abdelali Arous,Huseyin Arslan,Arman Farhang*

Main category: eess.SP

TL;DR: WD-NOMA ISAC 使用 AFDM/OTFS 上行与 OFDM 下行，在对比 PD-NOMA OFDM 的情况下，BER 行为更优，且 NPE 与 2D-OMP 提升了数据检测与 sensing 性能。


<details>
  <summary>Details</summary>
Motivation: 在 6GISAC 的背景下，集成感知与通信成为关键目标，OFDM 基波与上行波形的选取引发功率域与波形域的 NOMA 竞争，且 AFDM/OTFS 的 DFT-s-OFDM 基础为 WD-NOMA 提供可行性。需要设计有效的上行波形、噪声估计及感知算法来提升整体性能。

Method: 提出 WD-NOMA ISAC 架构，其中上行采用 AFDM/OTFS，下行采用 OFDM 进行通信与感知；将 OFDM 在仿射域近似为白噪声以简化符号检测；设计 AFDM 帧结构与噪声功率估计（NPE）方法；应用二维正交匹配追踪（2D-OMP）对目标的时-多普勒分量进行迭代识别；并通过仿真比较 WD-NOMA 与 PD-NOMA 在 BER 的性能。

Result: 仿真结果显示，采用 AFDM/OTFS 的 WD-NOMA ISAC 在 BER 方面优于仅使用 OFDM 的 PD-NOMA ISAC；NPE 能进一步提升 BER；2D-OMP 有效实现对目标时-多普勒组件的检测。

Conclusion: WD-NOMA（基于 AFDM/OTFS）的 ISAC 表现出比 PD-NOMA 更优的数据检测与 sensing 能力，且在 AFDM 与 OTFS 间均具备可行性，结合 NPE 与 2D-OMP 的算法设计带来显著性能提升。

Abstract: According to the recent 3GPP decisions on 6G air interface, orthogonal frequency-division multiplexing (OFDM)-based waveforms are the primary candidates for future integrated sensing and communication (ISAC) systems. In this paper, we consider a monostatic sensing scenario in which OFDM is used for the downlink and its reflected echo signal is used for sensing. OFDM and discrete Fourier transform-spread OFDM (DFT-s-OFDM) are the options for uplink transmission. When OFDM is used in the uplink, the power difference between this signal and the echo signal leads to a power-domain non-orthogonal multiple access (PD-NOMA) scenario. In contrast, adopting DFT-s-OFDM as uplink signal enables a waveform-domain NOMA(WD-NOMA). Affine frequency-division multiplexing (AFDM) and orthogonal time frequency space (OTFS) have been proven to be DFT-s-OFDM based waveforms. This work focuses on such a WD-NOMA system, where AFDM or OTFS is used as uplink waveform and OFDM is employed for downlink transmission and sensing. We show that the OFDM signal exhibits additive white Gaussian noise (AWGN)-like behavior in the affine domain, allowing it to be modeled as white noise in uplink symbol detection. To enable accurate data detection performance, an AFDM frame design and a noise power estimation (NPE) method are developed. Furthermore, a two-dimensional orthogonal matching pursuit (2D-OMP) algorithm is applied for sensing by iteratively identifying delay-Doppler components of each target. Simulation results demonstrate that the WD-NOMA ISAC system, employing either AFDM or OTFS, outperforms the PD-NOMA ISAC system that uses only the OFDM waveform in terms of bit error rate (BER) performance. Furthermore, the proposed NPE method yields additional improvements in BER.

</details>


### [10] [Low Overhead Channel Estimation in MIMO OTFS Wireless Communication Systems](https://arxiv.org/abs/2511.08504)
*Kailong Wang,Athina Petropulu*

Main category: eess.SP

TL;DR: 提出一种用于MIMO OTFS的低开销、可扩展的 pilot 辅助信道估计方法，通过在时频域嵌入 pilot、利用时频与多普勒-时延防护带来正交性，并借助虚拟阵列与稀疏信号恢复实现高分辨率信道参数估计。


<details>
  <summary>Details</summary>
Motivation: 在高多普勒环境下的 OTFS 系统中，精确高效的信道状态信息(CSI)估计是关键，但现有方法要么需要跨天线的非覆盖式 DD 域 pilot 与守护区域，从而显著降低通信速率；要么需要复杂算法处理重叠 pilot，提升接收端成本与复杂度。本研究旨在实现低开销且高性能的CSI估计，且能随发射天线数量增长保持良好可扩展性。

Method: 在 OTFS bursts 的时频域中嵌入 pilot；提出利用TF和DD防护带以在 pilot 区维护波形正交性与 DD 数据完整性。接收端先获得低复杂度的粗略信道参数估计；利用正交性构建虚拟阵列(VA)，据此将粗略估计用于构建低维字典矩阵，形成稀疏信号恢复(SSR)问题，从而获得高分辨率信道参数。仿真表明在较少 pilot 和防护带下实现良好性能，且开销与发射天线数无关，具备良好扩展性，并考虑了矩形脉冲成形、匹配滤波及分数多普勒。

Result: 仿真结果显示该方法在较少数量的 pilot 与防护带下即可实现较好性能；开销与发射天线数无关，具有良好可扩展性，且对矩形脉冲成形、接收端匹配滤波以及分数多普勒均有实际考虑。

Conclusion: 提出的基于 TF-DD 防护带、虚拟阵列与 SSR 的低开销高性能 CSI 估计方法，适用于大规模 MIMO OTFS 系统，在高移动性场景下具备良好鲁棒性与可扩展性。

Abstract: Orthogonal Time Frequency Space (OTFS) modulation has recently garnered attention due to its robustness in high-mobility wireless communication environments. In OTFS, the data symbols are mapped to the Doppler-Delay (DD) domain. In this paper, we address bandwidth-efficient estimation of channel state information (CSI) for MIMO OTFS systems. Existing channel estimation techniques either require non-overlapped DD-domain pilots and associated guard regions across multiple antennas, sacrificing significant communication rate as the number of transmit antennas increases, or sophisticated algorithms to handle overlapped pilots, escalating the cost and complexity of receivers. We introduce a novel pilot-aided channel estimation method that enjoys low overhead while achieving high performance. Our approach embeds pilots within each OTFS burst in the Time-Frequency (TF) domain. We propose a novel use of TF and DD guard bins, aiming to preserve waveform orthogonality on the pilot bins and DD data integrity, respectively. The receiver first obtains low-complexity coarse estimates of the channel parameters. Leveraging the orthogonality, a virtual array (VA) is constructed. This enables the formulation of a sparse signal recovery (SSR) problem, in which the coarse estimates are used to build a low-dimensional dictionary matrix. The SSR solution yields high-resolution estimates of channel parameters. Simulation results show that the proposed approach achieves good performance with only a small number of pilots and guard bins. Furthermore, the required overhead is independent of the number of transmit antennas, ensuring good scalability of the proposed method for large MIMO arrays. The proposed approach considers practical rectangular transmit pulse-shaping and receiver matched filtering, and also accounts for fractional Doppler effects.

</details>


### [11] [MIMO Communications with 1-bit RIS: Asymptotic Analysis and Over-the-Air Channel Diagonalization](https://arxiv.org/abs/2511.08534)
*Panagiotis Gavriilidis,Kyriakos Stylianopoulos,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 在 Ricean 衰落下的 1 位元 RIS 的 MIMO 系统中，通过随机矩阵理论证明，主特征向量/特征值趋向确定的 LoS 分量，使得仅用 LoS 信息即可实现 SA 相位配置并在 RIS 宽度远大于收发阵列时使端到端信道近似对角化，从而实现无 CSIT 的 OTA 空间复用；并给出基于水位分配的 SA 算法，仿真验证与传统优化相比具有更低复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决在受 Ricean 衰落的多输入多输出系统中如何低复杂度、鲁棒地配置 1-bit RIS，充分利用 LoS 信息实现高效信道增益与干扰抑制。

Method: 采用随机矩阵理论分析渐近极限，证明主奇值/特征向量收敛于确定性 LoS 分量；提出基于 LoS 的 Sign Alignment (SA) 相位配置规则，使通道增益最大化；在 RIS 相较于收发阵列变得更大时，端到端信道在容量公式下呈对角化，消除跨流干扰；提出基于水位分配思想的 SA 算法在统计信道参数下分配 RIS 元件；通过仿真与传统 Riemannian 场优化比较验证。

Result: 渐近意义上，主奇值和主特征向量收敛到 LoS 成分，RIS 配置可仅利用 LoS 信息实现最优或近最优增益；若 RIS 足够大，则端到端 MIMO 信道在容量表示下趋于对角化，支持不需要发射端信道信息的 OTA 空间多路复用；提出的水位分配 SA 算法在资源分配上表现良好；仿真显示与常见优化相比，运行时间量级下降。

Conclusion: 在大规模 RIS 情况下，利用 LoS 为基础的 SA 与水位分配策略可提供接近最优的性能，同时显著降低实现复杂度，验证了在 Ricean 环境中的鲁棒性和可扩展性。

Abstract: This paper presents an asymptotic analysis of Multiple-Input Multiple-Output (MIMO) systems assisted by a 1-bit Reconfigurable Intelligent Surface (RIS) under Ricean fading conditions. Using random matrix theory, we show that, in the asymptotic regime, the dominant singular values and vectors of the transmitter-RIS and RIS-receiver channels converge to their deterministic Line-of-Sight (LoS) components, almost irrespective of the Ricean factors. This enables RIS phase configuration using only LoS information through a closed-form Sign Alignment (SA) rule that maximizes the channel gain. Furthermore, when the RIS is asymptotically larger than the transceiver arrays, proper RIS configuration can render the end-to-end MIMO channel in the capacity formula asymptotically diagonal, thereby eliminating inter-stream interference and enabling Over-The-Air (OTA) spatial multiplexing without channel knowledge at the transmitter. Building on this result, a waterfilling-inspired SA algorithm that allocates RIS elements to spatial streams, based on the asymptotic singular values and statistical channel parameters, is proposed. Simulation results validate the theoretical analyses, demonstrating that the proposed schemes achieve performance comparable to conventional Riemannian manifold optimization, but with orders of magnitude lower runtime.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [12] [KG-DF: A Black-box Defense Framework against Jailbreak Attacks Based on Knowledge Graphs](https://arxiv.org/abs/2511.07480)
*Shuyuan Liu,Jiawei Chen,Xiao Yang,Hang Su,Zhaoxia Yin*

Main category: cs.CR

TL;DR: 提出一种知识图谱防御框架KG-DF，用知识图谱进行安全知识检索和匹配，通过可扩展的语义解析模块将输入转化为结构化的安全概念表示，从而提升对多种越狱 attack 的防御效果，同时在提高一般问答场景下的回答质量方面也有提升。


<details>
  <summary>Details</summary>
Motivation: 面向大语言模型在现实场景中的安全挑战，尤其是越狱攻击带来的输出偏差和风险，现有防御往往在泛化能力和安全性之间权衡不足，需兼顾模型的可用性与鲁棒性。

Method: 提出KG-DF框架：以知识图谱在结构化知识表示和语义关联方面的优势，通过安全知识库的检索与匹配识别潜在有害意图并提供安全推理路径；引入可扩展的语义解析模块，将输入查询转化为结构化、可安全匹配的概念表示，以提升匹配相关性；在实验中对多种越狱攻击方法进行防御评估，并结合领域通用知识提升LLM的一般问答质量。

Result: 实验结果表明，该框架在对抗多种越狱攻击方法方面提升了防御性能；并通过引入领域一般知识，提升了LLM在一般问答场景下的回答质量。

Conclusion: KG-DF通过将知识图谱的结构化知识和可扩展的语义解析相结合，实现在保持模型可用性与提升安全性的同时，提供对多样化攻击的鲁棒防御；语义解析提升了对输入的概念级理解，从而增强防御的泛化能力与匹配效果。

Abstract: With the widespread application of large language models (LLMs) in various fields, the security challenges they face have become increasingly prominent, especially the issue of jailbreak. These attacks induce the model to generate erroneous or uncontrolled outputs through crafted inputs, threatening the generality and security of the model. Although existing defense methods have shown some effectiveness, they often struggle to strike a balance between model generality and security. Excessive defense may limit the normal use of the model, while insufficient defense may lead to security vulnerabilities. In response to this problem, we propose a Knowledge Graph Defense Framework (KG-DF). Specifically, because of its structured knowledge representation and semantic association capabilities, Knowledge Graph(KG) can be searched by associating input content with safe knowledge in the knowledge base, thus identifying potentially harmful intentions and providing safe reasoning paths. However, traditional KG methods encounter significant challenges in keyword extraction, particularly when confronted with diverse and evolving attack strategies. To address this issue, we introduce an extensible semantic parsing module, whose core task is to transform the input query into a set of structured and secure concept representations, thereby enhancing the relevance of the matching process. Experimental results show that our framework enhances defense performance against various jailbreak attack methods, while also improving the response quality of the LLM in general QA scenarios by incorporating domain-general knowledge.

</details>


### [13] [Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models](https://arxiv.org/abs/2511.07503)
*Asia Belfiore,Jonathan Passerat-Palmbach,Dmitrii Usynin*

Main category: cs.CR

TL;DR: 本研究使用具差分隐私的语言模型生成合成的基因变异谱，并提出Biologically-Informed Hybrid Membership Inference Attack（biHMIA）来提升对隐私泄露的攻击能力。结果显示小型与大型GPT类模型都可用于小规模基因组的合成变异谱生成，且混合攻击在平均上比传统的基于度量的MIAs更具攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 基因数据具有高度敏感性，公开或共享需强隐私保护；合成数据的可用性和隐私保护并存的问题需要通过更强的隐私评估方法来验证；需要通过新的对手攻击来充分评估差分隐私保护的稳健性。

Method: 使用GPT样式的变换器模型在差分隐私约束下生成合成的变异谱；引入Biologically-Informed Hybrid Membership Inference Attack（biHMIA），将传统的黑盒MIA与上下文基因组度量结合以增强攻击能力；在小规模基因组数据集上对不同规模的模型进行实验评估，比较合成数据质量与隐私攻击的强度。

Result: 实验表明小型与大型GPT类模型均可作为小规模基因组的合成变异谱生成器；biHMIA相较于传统基于度量的MIAs具有更高的攻击成功率，平均水平提高。

Conclusion: 带差分隐私的语言模型在小规模基因组合成数据领域具备潜力，但隐私保护仍需更严格评估与改进；攻击方法的提升也提示需加强对DP机制的稳健性研究及防御性对抗措施。

Abstract: The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.

</details>


### [14] [A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain](https://arxiv.org/abs/2511.07577)
*Yining Lu,Wenyi Tang,Max Johnson,Taeho Jung,Meng Jiang*

Main category: cs.CR

TL;DR: 提出一个去中心化的RAG系统，利用区块链可信记录的可靠性评分机制动态评估数据源并在检索中优先高质量源，在嘈杂数据环境下相较中心化系统有显著性能提升和显著成本节省，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决集中式RAG在数据收集/集成/隐私方面的高成本和数据控制权问题，同时应对多源数据源在可靠性上的显著异质性带来的检索质量下降。

Method: 提出基于可验证的可靠性评分机制，该评分根据各源对生成回答的质量贡献动态评估；通过区块链智能合约记录与管理，确保透明和不可篡改的可靠性记录；在两种Llama模型（3B和8B）与两种模拟环境、六个可靠性不同的数据源上进行评估；在更新操作上采用批量化更新以实现成本降低。

Result: +10.7% 相对于中心化系统在不可靠数据环境中的性能提升；在理想的可靠数据环境下接近中心化的上限性能；约56%的边际成本节省；代码开源。

Conclusion: 去中心化RAG在保持可验证性与信任的前提下，能以接近中心化的性能运行并显著降低成本，适合在多源环境中应用；实验设置和代码有利于重复验证。

Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.

</details>


### [15] [SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought](https://arxiv.org/abs/2511.07772)
*Shourya Batra,Pierce Tillman,Samarth Gaggar,Shashank Kesineni,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.CR

TL;DR: SALT introduces a lightweight test-time intervention that injects steering vectors into hidden states to curb privacy leakage in a model's Chain of Thought without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: As LLMs become personal assistants with access to sensitive data, private information can leak through reasoning traces, not just outputs. There is a need to protect contextual privacy without degrading reasoning capabilities.

Method: Identify high-leakage layers in LLMs and apply SALT—a test-time technique that injects targeted steering vectors into hidden states to steer reasoning away from leaking private details, preserving utility.

Result: SALT reduces privacy leakage (CPL reductions: 18.2% on QwQ-32B, 17.9% on Llama-3.1-8B, 31.2% on Deepseek AirGapAgent-R) while maintaining comparable task performance.

Conclusion: SALT offers a practical, test-time privacy-protection approach for reasoning-enabled LLMs, facilitating safer deployment of LLM-based personal agents.

Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.

</details>


### [16] [PRISM: Privacy-preserving Inference System with Homomorphic Encryption and Modular Activation](https://arxiv.org/abs/2511.07807)
*Zeinab Elkhatib,Ali Sekmen,Kamrul Hasan*

Main category: cs.CR

TL;DR: 在同态加密下实现可行的CNN推理：用同态友好的多项式/激活近似替代非线性函数，重设计CNN结构，以在CKKS框架下对CIFAR-10实现高准确率与可接受推理时间。


<details>
  <summary>Details</summary>
Motivation: 数据隐私在关键基础设施中的 ML 部署面临挑战；同态加密使数据在密文中计算，但CNN 的非线性激活对密文计算不友好，需开发兼容的近似和结构优化以实现实用性。

Method: 提出一个优化框架，将标准非线性函数替换为同态可计算的近似，重构 CNN 架构，并引入高效的激活近似方法；在 CKKS 下使用度为4的多项式及 Softplus 近似实现激活，权衡精度与加密开销。

Result: 在 CIFAR-10 上实现 94.4% 的准确率；每个密文样本推理约 2.42 秒；10,000个密文样本约 24,000 秒；展示了以多项式近似实现激活在隐私保护与性能之间的可行性。

Conclusion: 通过对 CNN 的结构和激活函数进行同态可计算的优化，达到在保护数据隐私前提下的实用推理效果，且实验在现实数据集上验证了有效的性能权衡。

Abstract: With the rapid advancements in machine learning, models have become increasingly capable of learning and making predictions in various industries. However, deploying these models in critical infrastructures presents a major challenge, as concerns about data privacy prevent unrestricted data sharing. Homomor- phic encryption (HE) offers a solution by enabling computations on encrypted data, but it remains incompatible with machine learning models like convolutional neural networks (CNNs), due to their reliance on non-linear activation functions. To bridge this gap, this work proposes an optimized framework that replaces standard non-linear functions with homomorphically compatible approximations, ensuring secure computations while minimizing computational overhead. The proposed approach restructures the CNN architecture and introduces an efficient activation function approximation method to mitigate the performance trade-offs in- troduced by encryption. Experiments on CIFAR-10 achieve 94.4% accuracy with 2.42 s per single encrypted sample and 24,000 s per 10,000 encrypted samples, using a degree-4 polynomial and Softplus activation under CKKS, balancing accuracy and privacy.

</details>


### [17] [Blockchain-Integrated Privacy-Preserving Medical Insurance Claim Processing Using Homomorphic Encryption](https://arxiv.org/abs/2511.07818)
*Diya Mamoria,Harshit Jain,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: 提出一个去中心化且具密码学安全性的框架，用于医疗保险理赔处理的隐私、数据安全与保护，通过区块链记录不可篡改的理赔交易，并使用同态加密让授权方在不解密的情况下对加密记录进行理赔等操作，辅以智能合约自动化理赔流程；旨在在透明性与隐私之间实现共存，提升信任、效率和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决医疗保险理赔处理中隐私、数据安全和信任不足的问题；传统系统难以在透明度、隐私保护和效率之间取得平衡，需要一种结合区块链与同态加密的新范式。

Method: 在区块链上构建不可篡改、可审计的理赔交易账本；使用同态加密保护患者敏感信息，使授权的保险公司在不解密数据的情况下执行理赔相关操作；通过智能合约实现理赔流程的自动化与合规性。

Result: 提出的框架可实现透明性与隐私的兼容性，降低第三方处理隐私风险，提升理赔流程的自动化与可信度，理论上可改造医疗理赔提交系统生态。

Conclusion: 区块链与同态加密的结合为医疗理赔提供一种可同时保障隐私与透明度的架构，具有现实潜力与应用前景。

Abstract: This research proposes a decentralized and cryptographically secure framework to address the most acute issues of privacy, data security, and protection in the ecosystem of medical insurance claim processing. The scope of this study focuses on enabling the management of insurance claims in a transparent, privacy-protecting manner while maintaining the efficiency and trust level needed by the patients, healthcare providers, and insurers. To accomplish this, the proposed system adds blockchain technology to provide an unchangeable, decentralized, and auditable claim transactions ledger which enhances overall claim-related processes and trust among all stakeholders. To protect critical patient information, the framework employs homomorphic encryption a modern form of cryptography to allow authorized insurance providers to perform necessary operations like claim adjudication and reimbursement on encrypted medical records without any decryption during the process. This method significantly reduces the third-party processing privacy risk because patient data can be kept secret even when third-party processing is done. In addition, smart contracts improve automation of the most important procedures in the claim processing pipeline, which decreases manual, operational, and susceptibility towards human blunders or deceitful acts. The integration of these two transformative technologiesblockchain and homomorphic encryption represents the core contribution of this work, enabling the coexistence of transparency and privacy which are usually viewed as competing objectives in traditional systems. As a result, these technologies are expected to foster the creation of a reliable, effective, and privacy safeguarding architecture that could transform the medical claim submission systems paradigm.

</details>


### [18] [CAHICHA: Computer Automated Hardware Interaction test to tell Computer and Humans Apart](https://arxiv.org/abs/2511.07841)
*Aditya Mitra,Sibi Chakkaravarthy Sethuraman,Devi Priya V S*

Main category: cs.CR

TL;DR: 提出基于硬件交互信号和可信硬件的用户存在性验证方法，以区分真实用户与高级机器人，作为传统人机验证的替代方案。


<details>
  <summary>Details</summary>
Motivation: 随着自动化机器人和AI的快速发展，传统的语音验证码、知识验证等方法易被破解，迫切需要一种能够在硬件层面证明真实人类参与的安全且可用的解决方案。

Method: 利用受信任硬件中的用户存在(UP)标志进行加密证明，并结合硬件交互信号（如输入模式、响应时间、触控行为等）来验证真实物理交互，从而区分人类与自动化脚本。

Result: 在性能、可用性与安全性方面进行了系统评估，系统在高并发条件下实现稳定吞吐并且零请求失败，显示出良好的可扩展性和鲁棒性。

Conclusion: 所提出的系统提供比传统人机验证方法更安全、有效且易用的替代方案，能够更可靠地识别真实用户。

Abstract: As automation bot technology and Artificial Intelligence is evolving rapidly, conventional human verification techniques like voice CAPTCHAs and knowledge-based authentication are becoming less effective. Bots and scrapers with Artificial Intelligence (AI) capabilities can now detect and solve visual challenges, emulate human like typing patterns, and avoid most security tests, leading to high-volume threats like credential stuffing, account abuse, ad fraud, and automated scalping. This leaves a vital gap in identifying real human users versus advanced bots. We present a novel technique for distinguishing real human users based on hardware interaction signals to address this issue. In contrast to conventional approaches, our method leverages human interactions and a cryptographically attested User Presence (UP) flag from trusted hardware to verify genuine physical user engagement providing a secure and reliable way to distinguish authentic users from automated bots or scripted routines. The suggested approach was thoroughly assessed in terms of performance, usability, and security. The system demonstrated consistent throughput and zero request failures under prolonged concurrent user demand, indicating good operational reliability, efficient load handling, and the underlying architecture's robustness. These thorough analyses support the conclusion that the suggested system provides a safer, more effective, and easier-to-use substitute for current human verification methods.

</details>


### [19] [LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation](https://arxiv.org/abs/2511.07876)
*Xingyu Li,Xiaolei Liu,Cheng Liu,Yixiao Xu,Kangyi Ding,Bangzhou Xin,Jia-Li Yin*

Main category: cs.CR

TL;DR: LoopLLM 提出一种基于重复生成的能量-延迟攻击框架，通过重复诱导提示优化和跨模型梯度聚合实现对大语言模型的高能耗/高延迟输出控制，能够在多模型上实现接近输出上限的长文本生成，并显著提升跨模型转移性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模和推理成本迅速提升，能量-延迟攻击成为可行威胁。现有攻击多通过延迟终止符来延长输出，但随着输出长度增加，对输入端控制终止符的难度也随之上升，使该类方法的效果降低。需要一种对输出长度更稳定且跨模型具备转移性的攻击手段。

Method: LoopLLM 结合两点设计：(1) 重复诱导的提示优化，利用自回归生成中的脆弱性来促使模型产生重复输出，形成低熵解码循环；(2) token 对齐的 ensemble 优化，通过汇聚不同梯度信号提升跨模型的转移性，使攻击在多模型场景中更具鲁棒性。

Result: 在 12 个开源模型与 2 个商业模型上进行广泛实验，LoopLLM 显著优于现有方法，在达到最大输出长度方面达到约 90% 以上，而基线约为 20%；在对 DeepSeek-V3 与 Gemini 2.5 Flash 的跨模型转移性方面提升约 40%。

Conclusion: LoopLLM 通过触发低熵的重复解码循环来稳定地推动模型输出直至输出极限，显著提升能量-延迟攻击的有效性与跨模型转移性，为能耗相关安全问题提供新思路与防御方向。

Abstract: As large language models (LLMs) scale, their inference incurs substantial computational resources, exposing them to energy-latency attacks, where crafted prompts induce high energy and latency cost. Existing attack methods aim to prolong output by delaying the generation of termination symbols. However, as the output grows longer, controlling the termination symbols through input becomes difficult, making these methods less effective. Therefore, we propose LoopLLM, an energy-latency attack framework based on the observation that repetitive generation can trigger low-entropy decoding loops, reliably compelling LLMs to generate until their output limits. LoopLLM introduces (1) a repetition-inducing prompt optimization that exploits autoregressive vulnerabilities to induce repetitive generation, and (2) a token-aligned ensemble optimization that aggregates gradients to improve cross-model transferability. Extensive experiments on 12 open-source and 2 commercial LLMs show that LoopLLM significantly outperforms existing methods, achieving over 90% of the maximum output length, compared to 20% for baselines, and improving transferability by around 40% to DeepSeek-V3 and Gemini 2.5 Flash.

</details>


### [20] [Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks](https://arxiv.org/abs/2511.07947)
*Yaxin Xiao,Qingqing Ye,Zi Liang,Haoyang Li,RongHua Li,Huadi Zheng,Haibo Hu*

Main category: cs.CR

TL;DR: 提出了对抗模型提取攻击的水印策略研究。通过揭示基于表示纠缠的水印在应对顺序性MEA和移除攻击时的薄弱性，提出WRK（Watermark Removal attacK）攻击以规避纠缠约束，实证显示能降低水印在已有水印 benchmarks 上的成功率至少88.79%。为提升鲁棒性，提出CFW（Class-Feature Watermarks），通过利用类层面的伪造样本构建.synthetic class，消除原始域样本及其水印样本之间的脆弱决策边界，并同时优化MEA的可转移性与后MEA稳定性。多域实验表明，CFW在提取模型中对抗MEA+WRK扭曲的情况下，水印成功率仍保持至少70.15%，同时保持Protected模型的效用。


<details>
  <summary>Details</summary>
Motivation: 随着黑箱模式的被广泛部署，模型提权攻击（MEA）对知识产权构成威胁；现有水印多通过表示纠缠来提升对MEA的存活性，但对顺序MEA和移除攻击的鲁棒性研究不足，存在被低估的风险。

Method: 1) WRK：通过利用由水印伪影塑造的总体样本层面的决策边界，规避表示纠缠的限制，实现对水印的移除或降低水印可检测性。2) CFW：构造一个由域外样本组成的合成类别，消除原始域样本及其水印变体之间的脆弱决策边界，同时在ME A可转移性与后ME A稳定性之间进行联合优化。

Result: WRK在现有水印基准上至少降低水印成功率88.79%；CFW在多域实验中，即使在ME A+WRK的联合扭曲下，也能保持水印成功率≥70.15%，且不显著损害模型效用。

Conclusion: 提出了更鲁棒的水印策略CFW，克服了以往基于表示纠缠的局限性，使水印对抗MEA和水印移除攻击更具韧性，同时揭示了对抗性水印研究中应关注的决策边界层面。

Abstract: Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks.
  For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.

</details>


### [21] [FedPoP: Federated Learning Meets Proof of Participation](https://arxiv.org/abs/2511.08207)
*Devriş İşler,Elina van Kempen,Seoyeon Hwang,Nikolaos Laoutaris*

Main category: cs.CR

TL;DR: FedPoP提出一种在联邦学习中提供非可链接的参与证明的框架，能够在不暴露隐私或使用公有账本的前提下证明客户端对模型的贡献，且与现有安全聚合协议兼容，适用于真实部署。


<details>
  <summary>Details</summary>
Motivation: 随着模型逐渐成为可货币化的数字资产，证明参与训练以证明所有权的需求日益凸显；现有方案往往依赖公有账本或高昂计算成本，难以在保护隐私的同时实现可审计。

Method: 提出FedPoP框架，与现有安全聚合无缝集成，提供非可链接的参与证明，同时保持客户端匿名与隐私；无需大量计算或公有账本；给出概念实现并在现实下的客户端掉线情形中进行实验评估。

Result: 原型实现显示FedPoP在安全聚合之上每轮额外0.97秒开销，并能在0.0612秒内让客户端向第三方证明其对模型的参与/贡献，经实证评估表明在可观测的掉线情境下仍具现实部署可行性。

Conclusion: FedPoP实现了在不牵涉公有账本的前提下，对参与者提供不可关联的参与证明，同时保持隐私和安全聚合兼容，适合在真实世界的可审计需求中用于声样化拥有权证明等场景。

Abstract: Federated learning (FL) offers privacy preserving, distributed machine learning, allowing clients to contribute to a global model without revealing their local data. As models increasingly serve as monetizable digital assets, the ability to prove participation in their training becomes essential for establishing ownership. In this paper, we address this emerging need by introducing FedPoP, a novel FL framework that allows nonlinkable proof of participation while preserving client anonymity and privacy without requiring either extensive computations or a public ledger. FedPoP is designed to seamlessly integrate with existing secure aggregation protocols to ensure compatibility with real-world FL deployments. We provide a proof of concept implementation and an empirical evaluation under realistic client dropouts. In our prototype, FedPoP introduces 0.97 seconds of per-round overhead atop securely aggregated FL and enables a client to prove its participation/contribution to a model held by a third party in 0.0612 seconds. These results indicate FedPoP is practical for real-world deployments that require auditable participation without sacrificing privacy.

</details>


### [22] [Publish Your Threat Models! The benefits far outweigh the dangers](https://arxiv.org/abs/2511.08295)
*Loren Kohnfelder,Adam Shostack*

Main category: cs.CR

TL;DR: 提出将公开威胁模型（PTM）作为软件开发的透明信息源，主张将现有的威胁建模成果转化为公开可审查的PTM，以提升供应链各方的安全可验证性，并给出红action及发布时机的指南，呼吁行业广泛采用并公开共享。


<details>
  <summary>Details</summary>
Motivation: 提高对软件系统安全属性的透明度，使客户、监管者和生态系统上下游更好地理解和评估安全性；通过公开威胁模型获得竞争优势，同时推动行业标准化与合规驱动的实践。

Method: 理论性综述与论证：列举早期采用案例、讨论PTM的好处与潜在异议、分析监管驱动因素，提出对内部威胁模型的红action与审阅流程，以及何时发布或不发布的时机，最终提出公开共享PTM的行动号召与实现路径。

Result: 提出将PTM作为一种新常态纳入行业实践的路径；强调通过公开披露提升供应链各方对安全性认识，并提供红action、审阅与更新机制以平衡透明性与保密需求。

Conclusion: 呼吁科技社区公开分享PTM，以便上游到下游的安全属性可被理解与评估；企业可以公开工作提升信誉与竞争力，客户可据此评估安全性，而非仅仅被告知“它是安全的”。

Abstract: Threat modeling has long guided software development work, and we consider how Public Threat Models (PTM) can convey useful security information to others. We list some early adopter precedents, explain the many benefits, address potential objections, and cite regulatory drivers. Internal threat models may not be directly suitable for disclosure so we provide guidance for redaction and review, as well as when to update models (published or not). In a concluding call to action, we encourage the technology community to openly share their PTMs so the security properties of each component are known up and down the supply chain. Technology providers proud of their security efforts can show their work for competitive advantage, and customers can ask for and evaluate PTMs rather than be told "it's secure" but little more. Many great products already have fine threat models, and turning those into PTMs is a relatively minor task, so we argue this should (and easily could) become the new norm.

</details>


### [23] [Plaintext Structure Vulnerability: Robust Cipher Identification via a Distributional Randomness Fingerprint Feature Extractor](https://arxiv.org/abs/2511.08296)
*Xiwen Ren,Min Luo,Cong Peng,Debiao He*

Main category: cs.CR

TL;DR: 提出基于 ciphertext 随机性特征的指纹识别方法，避免端到端学习，能对加密算法进行高鲁棒性识别，跨域情况下表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决分类器在训练数据的明文分布与测试数据分布不同时性能下降的问题，以及对哪些具体算法被使用的识别困难。

Method: 利用一组统计检验来计算密文的随机性特征，并基于该特征的频率分布模式构建各算法指纹，方法不直接从密文字节端到端学习。

Result: 在 Canterbury Corpus 数据集上实现高判别性能（AUC > 0.98）。跨域评估中，基线方法在结构化明文比例降低时性能显著下降，而本方法鲁棒性较高，迁移时的性能下降最小，且在最具挑战性的纯随机数据上仍保持较高的排序能力（AUC > 0.90）。

Conclusion: 基于随机性特征的指纹识别对密文分析的鲁棒性强，能在不同明文分布下稳定识别加密算法。

Abstract: Modern encryption algorithms form the foundation of digital security. However, the widespread use of encryption algorithms results in significant challenges for network defenders in identifying which specific algorithms are being employed. More importantly, we find that when the plaintext distribution of test data departs from the training data, the performance of classifiers often declines significantly. This issue exposes the feature extractor's hidden dependency on plaintext features. To reduce this dependency, we adopt a method that does not learn end-to-end from ciphertext bytes. Specifically, this method is based on a set of statistical tests to compute the randomness feature of the ciphertext, and then uses the frequency distribution pattern of this feature to construct the algorithms' respective fingerprints. The experimental results demonstrate that our method achieves high discriminative performance (e.g., AUC > 0.98) in the Canterbury Corpus dataset, which contains a diverse set of data types. Furthermore, in our cross-domain evaluation, baseline models' performance degrades significantly when tested on data with a reduced proportion of structured plaintext. In sharp contrast, our method demonstrates high robustness: performance degradation is minimal when transferring between different structured domains, and even on the most challenging purely random dataset, it maintains a high level of ranking ability (AUC > 0.90).

</details>


### [24] [Why does weak-OOD help? A Further Step Towards Understanding Jailbreaking VLMs](https://arxiv.org/abs/2511.08367)
*Yuxuan Zhou,Yuzhao Peng,Yang Bai,Kuofeng Gao,Yihao Zhang,Yechao Zhang,Xun Chen,Tao Yu,Tao Dai,Shu-Tao Xia*

Main category: cs.CR

TL;DR: 本论文研究大规模视觉-语言模型（VLM）的OOD基越狱方法，提出“弱-OOD”现象，揭示输入意图感知与模型拒绝触发之间的权衡导致该现象；以SI-Attack为研究对象进行理论与实证分析，给出由预训练与对齐阶段失配引发不一致性的理论论证；基于OCR能力提升的思路，设计一种简单高效的VLM越狱方法，性能超越SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 揭示OOD-Based越狱机制的本质及其对VLM安全性的影响，解释为何温和的ODD策略反而更易绕过安全约束，并为构建更稳健的安全对齐提供理论与方法论基础。

Method: 对典型OOD基越狱方法SI-Attack进行系统研究，考察输入意图感知与模型拒绝触发这两大因素在OOD扰动下的响应差异，给出两者不一致性的理论解释；从对齐与预训练的差异出发给出不可避免性论证；借鉴OCR能力提升思路，设计一个简单但有效的VLM越狱方法并与SOTA基线比较。

Result: 发现温和OOD样本在绕过安全约束方面表现更强，证实“弱-OOD”现象；揭示两大驱动因素对OOD扰动的非一致性及其理论根源；提出OCR启发的越狱策略，实验上优于现有SOTA基线。

Conclusion: 深入理解OOD越狱的内在机制与不一致性根源，并给出一个在安全对齐角度具有攻击性但设计简洁的越狱方案；同时强调预训练与对齐之间的差异可能是安全性脆弱性的根源，需要在未来工作中进一步加强对齐鲁棒性。

Abstract: Large Vision-Language Models (VLMs) are susceptible to jailbreak attacks: researchers have developed a variety of attack strategies that can successfully bypass the safety mechanisms of VLMs. Among these approaches, jailbreak methods based on the Out-of-Distribution (OOD) strategy have garnered widespread attention due to their simplicity and effectiveness. This paper further advances the in-depth understanding of OOD-based VLM jailbreak methods. Experimental results demonstrate that jailbreak samples generated via mild OOD strategies exhibit superior performance in circumventing the safety constraints of VLMs--a phenomenon we define as ''weak-OOD''. To unravel the underlying causes of this phenomenon, this study takes SI-Attack, a typical OOD-based jailbreak method, as the research object. We attribute this phenomenon to a trade-off between two dominant factors: input intent perception and model refusal triggering. The inconsistency in how these two factors respond to OOD manipulations gives rise to this phenomenon. Furthermore, we provide a theoretical argument for the inevitability of such inconsistency from the perspective of discrepancies between model pre-training and alignment processes. Building on the above insights, we draw inspiration from optical character recognition (OCR) capability enhancement--a core task in the pre-training phase of mainstream VLMs. Leveraging this capability, we design a simple yet highly effective VLM jailbreak method, whose performance outperforms that of SOTA baselines.

</details>


### [25] [Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly](https://arxiv.org/abs/2511.08403)
*Lucian Trestioreanu,Wazen Shbair,Flaviene Scheidt de Cristo,Radu State*

Main category: cs.CR

TL;DR: Blockly2Hooks 通过 Blockly 的可视化编程降低学习和使用智能合约的门槛，面向非专家用户，利用 XRP Ledger 的实际场景进行教学。


<details>
  <summary>Details</summary>
Motivation: 当前智能合约的普及受限于安全、可用性和成本等因素，通常由专业开发者主导，普通用户缺乏工具和教育资源，因此需要更易上手的开发工具来扩展用户群。

Method: 设计、开发并测试 Blockly2Hooks 平台，结合 Google 的 Blockly Visually Programmable Library，目标在复杂语言（如 C）的智能合约在 XRP Ledger 的应用场景中提升学习与操作的可达性。

Result: 平台初步实现，经过测试显示能降低学习曲线，帮助非专业人员学习和采用智能合约，具有积极的教育与采用潜力。

Conclusion: Blockly2Hooks 具备成为降低非专业入门门槛的重要工具的潜力，但需在不同语言和区块链上的可移植性、安全性等方面进一步验证。

Abstract: Recent technologies such as inter-ledger payments, non-fungible tokens, and smart contracts are all fruited from the ongoing development of Distributed Ledger Technologies. The foreseen trend is that they will play an increasingly visible role in daily life, which will have to be backed by appropriate operational resources. For example, due to increasing demand, smart contracts could soon face a shortage of knowledgeable users and tools to handle them in practice. Widespread smart contract adoption is currently limited by security, usability and costs aspects. Because of a steep learning curve, the handling of smart contracts is currently performed by specialised developers mainly, and most of the research effort is focusing on smart contract security, while other aspects like usability being somewhat neglected. Specific tools would lower the entry barrier, enabling interested non-experts to create smart contracts.
  In this paper we designed, developed and tested Blockly2Hooks, a solution towards filling this gap even in challenging scenarios such as when the smart contracts are written in an advanced language like C. With the XRP Ledger as a concrete working case, Blockly2Hooks helps interested non-experts from the community to learn smart contracts easily and adopt the technology, through leveraging well-proven teaching methodologies like Visual Programming Languages, and more specifically, the Blockly Visual Programming library from Google. The platform was developed and tested and the results are promising to make learning smart contract development smoother.

</details>


### [26] [Coverage-Guided Pre-Silicon Fuzzing of Open-Source Processors based on Leakage Contracts](https://arxiv.org/abs/2511.08443)
*Gideon Geier,Pariya Hajipour,Jan Reineke*

Main category: cs.CR

TL;DR: 提出一种覆盖引导的硬件-软件泄露契约模糊测试方法，利用自组合框架将信息泄露观测为微架构状态分歧，并以 SCD 指标驱动探索，在 Rocket Core 与 BOOM 上验证其可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的硬件验证难以对复杂、工业规模的设计在泄露契约方面进行可扩展的验证；主流硬件模糊测试偏向功能正确性，往往对信息泄露（如 Spectre）缺乏感知，需新的方法来高效发现泄露。

Method: 提出一个自组成框架的覆盖导向模糊测试流程，将信息泄露以微架构状态分歧的形式直接观测；引入 Self-Composition Deviation (SCD) 作为安全导向的覆盖度量，驱动模糊测试探索可能违反泄露契约的执行路径；在开源 RISC-V 核心 Rocket Core（单阶段顺序执行）与 BOOM（复杂的乱序核）上实现并评估。

Result: 覆盖导向的模糊测试在两种核上均优于无引导方法，尤其在 BOOM 核上，随着微架构覆盖率的增加，发现安全漏洞的速度显著提升。

Conclusion: 该方法为大规模工业设计的硬件泄漏契约验证提供可扩展的解决路径，SCD 指标有效地引导模糊测试发现更早、更广的潜在泄漏路径。

Abstract: Hardware-software leakage contracts have emerged as a formalism for specifying side-channel security guarantees of modern processors, yet verifying that a complex hardware design complies with its contract remains a major challenge. While verification provides strong guarantees, current verification approaches struggle to scale to industrial-sized designs. Conversely, prevalent hardware fuzzing approaches are designed to find functional correctness bugs, but are blind to information leaks like Spectre.
  To bridge this gap, we introduce a novel and scalable approach: coverage-guided hardware-software contract fuzzing. Our methodology leverages a self-compositional framework to make information leakage directly observable as microarchitectural state divergence. The core of our contribution is a new, security-oriented coverage metric, Self-Composition Deviation (SCD), which guides the fuzzer to explore execution paths that violate the leakage contract. We implemented this approach and performed an extensive evaluation on two open-source RISC-V cores: the in-order Rocket Core and the complex out-of-order BOOM core. Our results demonstrate that coverage-guided strategies outperform unguided fuzzing and that increased microarchitectural coverage leads to a faster discovery of security vulnerabilities in the BOOM core.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [27] [Robust Dynamic Coded Distributed Storage with Partially Storage Constrained Servers](https://arxiv.org/abs/2511.08278)
*Chen Zhao,Haobo Jia,Zhuqing Jia*

Main category: cs.IT

TL;DR: 提出并完全刻画了部分存储受限服务器的鲁棒动态编码分布式存储（RDCDS）的基本极限，包括可用服务器的最小数量与读/更新的最小通信成本，并给出新对立证明与编码设计，扩展了Jia等人的工作。


<details>
  <summary>Details</summary>
Motivation: 在服务器可能掉线和存储容量受限的情境下，寻求既鲁棒又高效的RDCDS解决方案，明确系统在极限条件下的性能边界，为实际部署提供理论基线。

Method: 给出新的对立证明（converse arguments）和编码设计（coding designs），在Jia等人已有的基础上，系统地分析并覆盖多种服务器掉线情形，最终完整刻画基本极限。

Result: 对RDCDS在部分存储约束下的基本极限给出完整表征：可行更新所需的最小可用服务器数量以及在不同掉线场景下读与更新操作的最小通信成本。

Conclusion: 建立了受部分存储约束的RDCDS的完整理论框架，指明在鲁棒性与通信成本之间的最优权衡，以及未来在实际编码设计和系统实现中的研究方向。

Abstract: We consider the problem of Robust Dynamic Coded Distributed Storage (RDCDS) with partially storage constrained servers where the goal is to enable robust (resilient to server dropouts) and efficient (as measured by the communication costs) read and update operations, subject to the constraint that the storage at $S$ out of $N$ servers is limited by $1/K_c$ the size of the message. Building upon previously established converse arguments and achievability schemes by Jia et al., in this work we develop a set of new converse arguments and coding designs that enable us to completely characterize the fundamental limits of RDCDS with partially storage constrained servers, i.e., the minimum number of available servers for feasible update operation and the minimum communication costs for read and update operations across various server dropout scenarios.

</details>


### [28] [A General Ziv-Zakai Bound for DoA Estimation in MIMO Radar Systems](https://arxiv.org/abs/2511.08326)
*Mohammadreza Bakhshizadeh Mohajer,Daniela Tuninetti,Luca Barletta*

Main category: cs.IT

TL;DR: 提出针对共轭阵列雷达多目标 DoA 估计的 Ziv–Zakai 下界（ZZB），在多目标、任意输入协方差及多快照条件下给出闭式表达，显示边界对发射天线数、目标个数、信噪比和传输协方差的依赖，并在数值上证实在低信噪比下 ZZB 比 CRB 更紧且与转向 CRB 的阈值受参数影响。


<details>
  <summary>Details</summary>
Motivation: 在多目标、低信噪比环境下，单输入单输出或简单高斯输入假设的 CRB 往往对 DoA 估计的极限界限不足以描述真实误差。本文旨在推广 ZZB 到共定位 MIMO 雷达的 DoA 估计，且能覆盖多目标、通用输入协方差、以及快照效应，提供对系统参数影响的可解析洞见。

Method: 推导多目标共定位 MIMO 雷达 DoA 的 ZZB，给出考虑一般输入协方差矩阵、目标雷达截面统计和多快照影响的闭式表达，并得到能揭示发射天线数、目标数、SNR 与传输协方差之间依赖关系的简洁形式。通过数值仿真验证在先验支配区域 ZZB 的紧性，并分析增加发射天线如何压缩转向到 CRB 的阈值，以及目标数对边界在不同 SNR 区间的影响。

Result: 得到适用于多目标场景的 ZZB，不仅在低 SNR 区域比 CRB 更紧，还给出随天线数和目标数变化的系统性规律：提高发射天线数可降低阈值 SNR，使系统更早进入 CRB 受限的区间；目标数的增加改变边界的 SNR 行为曲线。仿真也证实 ZZB 对输入协方差和快照效应的敏感性被恰当地刻画。

Conclusion: ZZB 为多目标共定位 MIMO 雷达 DoA 估计提供了比 CRB 更紧的下界，尤其在低 SNR 情况下，且给出参数对边界形状与阈值的明确影响，为系统设计在性能下界分析与参数优化提供了理论工具。

Abstract: This paper derives a Ziv-Zakai Bound (ZZB) on the Mean Squared Error (MSE) for Direction-of-Arrival (DoA) estimation in co-located Multiple-Input Multiple-Output (MIMO) radar systems and provides closed-form expressions that hold for multi-target scenarios. Unlike classical results that address single-input multiple-output systems with complex Gaussian input signals, the developed ZZB in this paper explicitly accounts for a general input covariance matrix, target radar cross-section statistics and multiple snapshot effects, and admits a compact expression that reveals the dependence of the MSE on the number of transmit antennas, number of targets, Signal-to-Noise Ratio (SNR) and the transmit covariance matrix. Numerical simulations validate the tightness of the ZZB in the a priori dominated region and show how the increase of the number of transmit antennas compresses the threshold SNR for the transition to the Cramer-Rao bound (CRB) while the variation of the number of targets shifts the bound's behavior across SNR regimes. The analytical results and numerical simulations demonstrate that the ZZB is tighter than the CRB, particularly in the low SNR regime.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [29] [Resource Allocation in Hybrid Radio-Optical IoT Networks using GNN with Multi-task Learning](https://arxiv.org/abs/2511.07428)
*Aymen Hamrouni,Sofie Pollin,Hazem Sallouha*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper addresses the problem of dual-technology scheduling in hybrid Internet of Things (IoT) networks that integrate Optical Wireless Communication (OWC) alongside Radio Frequency (RF). We begin by formulating a Mixed-Integer Nonlinear Programming (MINLP) model that jointly considers throughput maximization and delay minimization between access points and IoT nodes under energy and link availability constraints. However, given the intractability of solving such NP-hard problems at scale and the impractical assumption of full channel observability, we propose the Dual-Graph Embedding with Transformer (DGET) framework, a supervised multi-task learning architecture combining a two-stage Graph Neural Networks (GNNs) with a Transformer-based encoder. The first stage employs a transductive GNN that encodes the known graph topology and initial node and link states. The second stage introduces an inductive GNN for temporal refinement, which learns to generalize these embeddings to the evolved states of the same network, capturing changes in energy and queue dynamics over time, by aligning them with ground-truth scheduling decisions through a consistency loss. These enriched embeddings are then processed by a classifier for the communication links with a Transformer encoder that captures cross-link dependencies through multi-head self-attention via classification loss. Simulation results show that hybrid RF-OWC networks outperform standalone RF systems by handling higher traffic loads more efficiently and reducing the Age of Information (AoI) by up to 20%, all while maintaining comparable energy consumption. The proposed DGET framework, compared to traditional optimization-based methods, achieves near-optimal scheduling with over 90% classification accuracy, reduces computational complexity, and demonstrates higher robustness under partial channel observability.

</details>


### [30] [Optimal Multi-Constrained Workflow Scheduling for Cyber-Physical Systems in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.07466)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.NI

TL;DR: 提出基于连续时间混合整数规划的最优调度方法，用于在边缘-集线器-云架构中最小化工作流延迟，并在多种系统配置下优于改进的启发式方法。


<details>
  <summary>Details</summary>
Motivation: 应对边缘设备的异构性、资源受限、传感/执行能力差异，降低端到端延迟以满足实时性要求。

Method: 建立连续时间混合整数线性规划的全面模型，覆盖多设备协作、 hub 与云端、异构多核处理、以及多种传感与执行能力；与改进的启发式算法进行对比并通过实验评估。

Result: 相较于启发式方法，本方法在实际用例中平均延迟提升13.54%；在合成工作负载下实现33.03%的平均延迟降低，体现良好可扩展性。

Conclusion: 该优化调度方法可显著降低端到端延迟并具备对大规模工作负载的可扩展性，适用于边缘-集线器-云协同的实时应用。

Abstract: The emerging edge-hub-cloud paradigm has enabled the development of innovative latency-critical cyber-physical applications in the edge-cloud continuum. However, this paradigm poses multiple challenges due to the heterogeneity of the devices at the edge of the network, their limited computational, communication, and energy capacities, as well as their different sensing and actuating capabilities. To address these issues, we propose an optimal scheduling approach to minimize the overall latency of a workflow application in an edge-hub-cloud cyber-physical system. We consider multiple edge devices cooperating with a hub device and a cloud server. All devices feature heterogeneous multicore processors and various sensing, actuating, or other specialized capabilities. We present a comprehensive formulation based on continuous-time mixed integer linear programming, encapsulating multiple constraints often overlooked by existing approaches. We conduct a comparative experimental evaluation between our method and a well-established and effective scheduling heuristic, which we enhanced to consider the constraints of the specific problem. The results reveal that our technique outperforms the heuristic, achieving an average latency improvement of 13.54% in a relevant real-world use case, under varied system configurations. In addition, the results demonstrate the scalability of our method under synthetic workflows of varying sizes, attaining a 33.03% average latency decrease compared to the heuristic.

</details>


### [31] [A Large-Scale Dataset and Reproducible Framework for RF Fingerprinting on IEEE 802.11g Same-Model Devices](https://arxiv.org/abs/2511.07770)
*Zewei Guo,Zhen Jia,JinXiao Zhu,Wenhao Huang,Yin Chen*

Main category: cs.NI

TL;DR: 提出一个同型号设备的大规模RF指纹数据集和开源评估框架，展示基于随机森林的识别在该数据集上的高性能。


<details>
  <summary>Details</summary>
Motivation: 现有RF指纹数据集受设备规模和模型异质性限制，难以实现鲁棒训练和公平评估；同型号设备差异极小，需大规模、可重复的实验资源。

Method: 构建包含123台同型号IEEE 802.11g设备、共35.42百万原始I/Q样本和1.85百万RF特征的数据集，并开发一个可复现实验框架；在框架中使用基于随机森林的算法进行设备识别。

Result: 在该数据集上识别准确率达到89.06%，并通过广泛实验确认特征之间的关系。

Conclusion: 提出的数据集与开源框架提升了研究可重复性，并为同型号设备指纹识别提供可扩展、公平的基准。

Abstract: Radio frequency (RF) fingerprinting exploits hardware imperfections for device identification, but distinguishing between same-model devices remains challenging due to their minimal hardware variations. Existing datasets for RF fingerprinting are constrained by small device scales and heterogeneous models, which hinders robust training and fair evaluation for machine learning models. To address this gap, we introduce a large-scale dataset of same-model devices along with a fully reproducible, open-source experimental framework. The dataset is built using 123 identical commercial IEEE 802.11g devices and contains 35.42 million raw I/Q samples from the preambles and corresponding 1.85 million RF features. The open-source framework further ensures full reproducibility from data collection to final evaluation. Within this framework, a Random Forest-based algorithm is proposed to achieve 89.06% identification accuracy on this dataset. Extensive experimental evaluations further confirm the relationships between the extracted features.

</details>


### [32] [Demystifying QUIC from the Specifications](https://arxiv.org/abs/2511.08375)
*Darius Saif,Ashraf Matrawy*

Main category: cs.NI

TL;DR: 本文旨在以完整而易懂的方式介绍QUIC，梳理其快速演化、RFC体系的复杂性，以及跨层和隐私特性，使读者能从规范到实现全面理解QUIC及其与HTTP/3的关系。


<details>
  <summary>Details</summary>
Motivation: QUIC发展迅速且存在多种实现与RFC文档，组织结构复杂，学习门槛高；需要一份易于访问的综述来降低理解难度。

Method: 系统性地讲解QUIC的演变、区分标准QUIC与Google QUIC、梳理相关RFC的组织与语言、讨论跨层和隐私实现的要点，提供一个完整且易于接近的入门框架。

Result: 提出一个完整且易于接近的QUIC介绍框架，帮助读者跨越快速演化、RFC结构与隐私/跨层实现的障碍，建立对QUIC及HTTP/3的系统理解。

Conclusion: 通过结构化、易读的讲解，证明QUIC可以被有效简化为清晰的知识体系，读者能够从规范到实现建立对QUIC的全面理解。

Abstract: QUIC is an advanced transport layer protocol whose ubiquity on the Internet is now very apparent. Importantly, QUIC fuels the next generation of web browsing: HTTP/3. QUIC is a stateful and connection oriented protocol which offers similar features (and more) to the combination of TCP and TLS. There are several difficulties which readers may encounter when learning about QUIC: i.) its rapid evolution (particularly, differentiation between the QUIC standard and the now deprecated Google QUIC), ii.) numerous RFCs whose organization, language, and detail may be challenging to the casual reader, and iii.) the nature of QUIC's cross-layer and privacy-centric implementation, making it impossible to understand or debug by looking at packets alone. For these reasons, the aim of this paper is to present QUIC in a complete yet approachable fashion, thereby demystifying the protocol from its specifications.

</details>


### [33] [SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services](https://arxiv.org/abs/2511.08282)
*Eranga Bandara,Safdar H. Bouk,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Peter Foytik,Ross Gore,Xueping Liang,Ng Wee Keong,Kasun De Zoysa*

Main category: cs.NI

TL;DR: 提出一个名为 SRE-Llama 的综合性 SRE 平台，利用 Federated Learning、生成式 AI、区块链与 NFT 自动化 SLA/SLO 及告警管理，并把 SLA/SLO 编码为 NFT 以实现不可变审计，原型基于 Open5GS 5G Core。


<details>
  <summary>Details</summary>
Motivation: 桥接开发者对 SRE 工具（如 Prometheus/Grafana）和 SLIs/SLOs 的理解不足，提升云原生环境中的服务可靠性和可观测性；同时解决数据隐私、可审计性和自动化需求。

Method: 采集云原生服务指标并存储在时序数据库（Prometheus/Mimir）；通过联邦学习找出最相关的 SLI 指标；使用微调的 Llama-3 生成 SLIs/SLOs/错误预算和告警；将生成的 SLI/SLO 编码为 NFT 并存储在区块链，智能合约控制自动化流程；原型在定制化 Open5GS 5G Core 上实现。

Result: 实现了一个原型系统，展示从指标采集到 AI 生成与 NFT 编码的端到端流程，强调在隐私保护、可审计性和可用性方面的优势；但尚未给出量化评估结果。

Conclusion: 该平台将 SRE 自动化向 AI+区块链方向扩展，具有潜在显著提高开发者可访问性和运营效率的前景，同时需要系统性评估其 AI 产出质量、区块链成本、治理和安全性等方面的挑战。

Abstract: Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [34] [ARGUS: A Framework for Risk-Aware Path Planning in Tactical UGV Operations](https://arxiv.org/abs/2511.07565)
*Nuno Soares,António Grilo*

Main category: eess.SY

TL;DR: ARGUS 是一个适用于地面无人车辆的任务规划框架，能够将战场信息与指挥意图转化为可执行路径，具备动态调整能力，并在葡萄牙军队实战演练中验证其可互操作性，提供决策支持和提高安全性。


<details>
  <summary>Details</summary>
Motivation: 在现代战场快速变化的环境中，需要将地形、威胁情报和指挥意图整合以生成安全、有效且可执行的行动轨迹，并能够实时适应未预见的事件，以提升无人地面系统的作战效能与安全性。

Method: 建立一个处理管线，输入地理空间数据、威胁情报及任务优先级，经过一系列集成模块生成优化的轨迹，兼具动态实时调整能力；并验证其互操作性，通过与UGV控制系统的集成在葡萄牙军队的实际演练中进行验证，形成一个面向决策的工具。

Result: 理论与实践均显示生成的轨迹可与UGV控制系统集成；系统不仅输出最优轨迹，还提供执行所需的洞见，提升自主地面系统的有效性和安全性。

Conclusion: ARGUS 能将复杂战场信息转化为可执行行动，增强对无人地面系统的适应性、安全性与效用，体现了信息到行动的端到端决策支持能力。

Abstract: This thesis presents the development of ARGUS, a framework for mission planning for Unmanned Ground Vehicles (UGVs) in tactical environments. The system is designed to translate battlefield complexity and the commander's intent into executable action plans. To this end, ARGUS employs a processing pipeline that takes as input geospatial terrain data, military intelligence on existing threats and their probable locations, and mission priorities defined by the commander. Through a set of integrated modules, the framework processes this information to generate optimized trajectories that balance mission objectives against the risks posed by threats and terrain characteristics. A fundamental capability of ARGUS is its dynamic nature, which allows it to adapt plans in real-time in response to unforeseen events, reflecting the fluid nature of the modern battlefield. The system's interoperability were validated in a practical exercise with the Portuguese Army, where it was successfully demonstrated that the routes generated by the model can be integrated and utilized by UGV control systems. The result is a decision support tool that not only produces an optimal trajectory but also provides the necessary insights for its execution, thereby contributing to greater effectiveness and safety in the employment of autonomous ground systems.

</details>


### [35] [Evolutionary Analysis of Continuous-time Finite-state Mean Field Games with Discounted Payoffs](https://arxiv.org/abs/2511.07655)
*Leonardo Pedroso,Andrea Agazzi,W. P. M. H. Heemels,Mauro Salazar*

Main category: eess.SY

TL;DR: 提出一种面向大规模玩家的动态博弈的平均场进化框架，提出混合稳态纳什均衡（MSNE），并证明其与平均场进化模型的 rest point 等价，同时给出近似保证与演化稳定性条件。


<details>
  <summary>Details</summary>
Motivation: 现有的进化博弈多聚焦静态环境，缺乏状态动态的纳入；需要将个体状态的演化纳入进化框架以描述现实应用（如交通拥塞、资源竞争等），从而实现对大规模动态博弈的近似分析与稳定性判断。

Method: 构造一类连续时间/离散时间的大规模玩家动态博弈，玩家在有限动作集和有限状态集之间转移，状态转移受所选动作影响；单阶段收益取决于个人状态、动作及整体状态-动作分布，体现拥堵等群体效应；提出平均场近似，定义混合稳态纳什均衡（MSNE）；将 MSNE 与所提出的平均场进化模型的 rest points 建立等价关系，并给出 MSNE 的演化稳定性条件。

Result: 给出有限人口博弈到平均场模型的近似保证；证明 MSNE 与平均场进化模型的 rest points 等价；给出实现 MSNE 的演化稳定性条件。

Conclusion: MSNE 为包含状态演化的动态博弈提供了可解释的进化框架，适用于大规模系统的分析与设计，弥合了进化博弈与动态博弈之间的理论空白，并为后续在拥堵、资源分配等场景中的应用奠定基础。

Abstract: We consider a class of continuous-time dynamic games involving a large number of players. Each player selects actions from a finite set and evolves through a finite set of states. State transitions occur stochastically and depend on the player's chosen action. A player's single-stage reward depends on their state, action, and the population-wide distribution of states and actions, capturing aggregate effects such as congestion in traffic networks. Each player seeks to maximize a discounted infinite-horizon reward. Existing evolutionary game-theoretic approaches introduce a model for the way individual players update their decisions in static environments without individual state dynamics. In contrast, this work develops an evolutionary framework for dynamic games with explicit state evolution, which is necessary to model many applications. We introduce a mean field approximation of the finite-population game and establish approximation guarantees. Since state-of-the-art solution concepts for dynamic games lack an evolutionary interpretation, we propose a new concept - the Mixed Stationary Nash Equilibrium (MSNE) - which admits one. We characterize an equivalence between MSNE and the rest points of the proposed mean field evolutionary model and we give conditions for the evolutionary stability of MSNE.

</details>


### [36] [AURORA: Autonomous Updating of ROM and Controller via Recursive Adaptation](https://arxiv.org/abs/2511.07768)
*Jiachen Li,Shihao Li,Dongmei Chen*

Main category: eess.SY

TL;DR: AURORA is a multi-agent LLM framework that autonomously updates reduced-order models and controllers online, enabling autonomous ROM-based control with iterative generation-judge-revision cycles.


<details>
  <summary>Details</summary>
Motivation: Real-time model-based control of high-dimensional nonlinear systems is computationally intractable; traditional ROM control relies on manual tuning and lacks online adaptation. There is a need for automated, adaptive design of ROMs and controllers.

Method: Five specialized agents collaborate in iterative generation-judge-revision cycles to design and update ROMs and controllers. An Evaluation Agent diagnoses degradation sources and routes corrections. The system is validated on eight benchmarks across mechanical assemblies, thermal PDEs, and robots, with comparative evaluation across five LLMs.

Result: The framework shows high autonomy with minimal human intervention and demonstrates practical viability for autonomous control design across diverse domains.

Conclusion: AURORA enables autonomous ROM-based controller design with online adaptation through a cooperative multi-agent LLM architecture, reducing human input and improving adaptability for real-time control.

Abstract: Real-time model-based control of high-dimensional nonlinear systems faces computational intractability, while traditional reduced-order model (ROM) control requires manual expert tuning without online adaptation. We propose AURORA (\textbf{A}utonomous \textbf{U}pdating of \textbf{RO}M and Controller via \textbf{R}ecursive \textbf{A}daptation), a multi-agent LLM framework automating ROM-based controller design with online adaptation. AURORA employs five specialized agents collaborating through iterative generation-judge-revision cycles, with an Evaluation Agent diagnosing degradation sources and routing corrections appropriately. Validated on eight benchmark systems spanning mechanical assemblies, thermal PDEs, and robots. Comparative evaluation across five state-of-the-art LLMs demonstrates high autonomy with minimal intervention, establishing practical viability for autonomous control design.

</details>


### [37] [Experimental Evaluation of Fuzzy-Integral and Classical controls for Power Management in a 24 GHz mmWave 5G Transceiver](https://arxiv.org/abs/2511.07815)
*Karel Walter Gomez Orellana,Berthyn Rodrigo Tiñini Chuquimia,Juan Carlos Paredes Condori,Rodrigo Apaza Huanca,Hugo Orlando Condori Quispe*

Main category: eess.SY

TL;DR: 比较三种控制策略（PID、纯积分、模糊-积分FI）在24 GHz mmWave 收发系统的自适应功率管理中的性能；模糊-积分控制器在结态时间、稳定性和EVM最小化方面优于其他两者。


<details>
  <summary>Details</summary>
Motivation: 在5G毫米波系统中，功率放大器的线性度和效率需在温度引起的增益波动下维持，温升导致的EVM下降成为关键挑战。

Method: 实现并比较三种控制策略（PID、纯积分、FI）用于24 GHz mmWave 收发系统的自适应功率管理；FI控制器将模糊逻辑用于处理非线性，并结合积分作用以实现零稳态误差。

Result: 实验结果显示，FI控制器在结态时间、系统稳定性以及EVM最小化方面超过PID和纯积分控制器。

Conclusion: 在24 GHz mmWave 收发系统的自适应功率管理中，模糊-积分控制器提供更优的性能平衡，兼顾非线性处理与零稳态误差，是更具潜力的解决方案。

Abstract: The deployment of 5G millimeter-wave (mmWave) systems poses significant challenges in maintaining power amplifier linearity and efficiency under varying conditions, such as temperature-induced gain variations that degrade error vector magnitude (EVM). This paper presents a comparative study of three control strategies-PID, pure integral, and fuzzy-integral (FI)-for adaptive power management in a 24 GHz mmWave transceiver. The FI controller integrates fuzzy logic for handling nonlinearities with integral action for zero steady-state error. Experimental results show the FI controller outperforms others in settling time, stability, and EVM minimization.

</details>


### [38] [Comparative Study of Q-Learning for State-Feedback LQG Control with an Unknown Model](https://arxiv.org/abs/2511.07870)
*Mingxiang Liu,Damián Marelli,Minyue Fu,Qianqian Cai*

Main category: eess.SY

TL;DR: 在未知参数的状态反馈LQG设计问题中，基于辨识的SF-LQG方法在渐近意义上是高效的，几乎达到最优；提出的Q-learning方法在渐近意义上也能达到最优，但计算复杂度通常高于经典方法，因此在未知参数情形下，经典的辨识+设计方法仍是更优选择。


<details>
  <summary>Details</summary>
Motivation: 研究在系统矩阵和过程噪声协方差未知的情况下，如何设计LQG控制器，并比较辨识驱动的传统方法与强化学习方法在复杂度与精度上的差异，寻找最具实用性的路径。

Method: 对两种方案进行严格对比：1) 经典路径——对系统进行辨识估计参数，然后用估计参数设计SF-LQG控制器；2) RL路径——提出一种基于Q-learning的控制设计方法，并给出一个算法实现以提高数值计算效率；对两者在渐近情况下的性能和复杂度进行分析。

Result: 经典方法在渐近意义上达到高效，几乎没有改进空间；所提Q-learning方法在渐近意义上可达到最优控制，但在实际计算中通常不如辨识派方法高效；总体上，经典方法在数值实现上更具优势。

Conclusion: 在未知参数的SF-LQG设计中，经典的辨识+设计方法仍然是更优的选择；尽管RL方法具备理论上渐近最优的可能，但代价较高，实际应用中应优先考虑经典方法。

Abstract: We study the problem of designing a state feedback linear quadratic Gaussian (LQG) con- troller for a system in which the system matrices as well as the process noise covariance are unknown. We do a rigorous comparison between two approaches. The first is the classic one in which a system identification stage is used to estimate the unknown parameters, which are then used in a state-feedback LQG (SF-LQG) controller design. The second approach is a recently proposed one using a reinforcement learning paradigm called Q-learning. We do the comparison in terms of complexity and accuracy of the resulting controller. We show that the classic approach asymptotically efficient, giving virtually no room for improvement in terms of accuracy. We also propose a novel Q-learning-based method which we show asymptotically achieves the optimal controller design. We complement our proposed method with a numerically efficient algorithmic implementation aiming at making it competitive in terms of computations. Nevertheless, our complexity analysis shows that the classic approach is still numerically more efficient than this Q-learning-based alternative. We then conclude that the classic approach remains being the best choice for addressing the SF-LQG design in the case of unknown parameters.

</details>


### [39] [Optimisation of Power Modulation for Hall-Héroult Cells: Process Operability and Constraints as Virtual Energy Storage](https://arxiv.org/abs/2511.07893)
*Choon-Jie Wong,Adam A. Larkin,Jie Bao,Maria Skyllas-Kazacos,Barry J. Welch,Nadia Ahli,Maitha Faraj,Mohamed Mahmoud*

Main category: eess.SY

TL;DR: 通过对铝电解槽的功率调制进行优化，在热平衡约束下最大化盈利，采用降阶与详细模型的混合方法，分析不同电价情景下的最优线电流和阳极-阴极距离（ACD）轨迹，为在线控制策略奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 铝生产极度耗能，推行可再生能源需要高效的需求侧调控。铝冶炼槽可作为大规模虚拟储能来平衡电网，本研究旨在在多尺度、空间分布的动力学约束下，找出在功率调制条件下提升经济性与热平衡的最优运行条件。

Method: 提出一个结合降阶模型与细化模型的优化框架，处理时变线电流和ACD的联合优化，考虑热平衡与热耦合等约束；在包含用电价与现货价的场景下分析不同功率调制策略的最优解。

Result: 揭示在不同功率调制情景下的最优线电流和ACD曲线的特性与趋势，为后续在线控制策略提供定量依据与启发。

Conclusion: 该方法为铝电解槽的在线功率调制提供了可执行的优化框架，能够在分布式、多尺度的动力学下导出可落地的电流与ACD轨迹，未来工作可拓展到实时控制策略的实现与应对更复杂市场条件。

Abstract: Aluminium is manufactured through the Hall-Héroult process, which is very energy intensive. Power modulation, as an industrial-scale demand-side power management approach, allows aluminium smelters to operate with variable power consumption rates and as such be powered by renewable energy sources. In this way, aluminium smelting cells can be used as a large virtual energy storage to balance power demand-supply and stabilise electrical grids. This paper studies the potential optimal power modulation operating conditions, including time-varying line current and anode-cathode distance (ACD) profiles to maximise the aluminium reduction cell profitability subject to constraints on the cell thermal balance. To deal with the complex cell dynamics which are spatially distributed and multi-timescale, a novel optimisation approach that utilises both reduced-order and detailed models is developed. The results yield insight into the optimal line current and ACD profiles for different power modulation scenarios including the time of use electricity tariff and spot price. These results can form the foundation for further studies into online control policies of aluminium reduction cells.

</details>


### [40] [An Innovations-Based Data-Driven Kalman Predictor for Predictive Control](https://arxiv.org/abs/2511.07907)
*Mohamed Abdalmoaty,Roy S. Smith*

Main category: eess.SY

TL;DR: 提出一种仅用输入输出数据参数化卡尔曼滤波器的新方案，通过创新过程实现对过程扰动和测量噪声的统一建模，并通过投影步估计创新过程，从而避免离线测量扰动的要求；在基准仿真中验证。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动卡尔曼滤波器需要离线测量过程扰动，这在实际应用中往往不可行，因此需要一个只依赖输入输出数据的参数化方法。

Method: 将卡尔曼滤波器以创新形式参数化，将过程扰动和测量噪声合成为一个正交随机过程（创新过程），并通过一个数值高效的投影步骤从输入输出数据估计创新过程。

Result: 在基准仿真中演示了该方法的性能，表明无需离线扰动信息即可实现有效状态估计。

Conclusion: 创新形式提供了一种数据驱动的参数化路径，降低对离线干预的依赖，并可在实际应用中通过投影自估计创新过程实现卡尔曼滤波的鲁棒性与可实现性。

Abstract: A recently developed data-driven Kalman filter requires offline measurement of the process disturbance; a requirement that is often unmet for many practical applications. We propose a solution that parametrizes the Kalman filter exclusively using measured input and output data. The key idea is to use the innovations form which naturally accounts for the process disturbance and measurement noise into a single orthogonal stochastic process. Unlike process disturbances, the innovations process can be estimated directly from input-output data via a numerically efficient projection step. The performance of the method is demonstrated using a benchmark simulation.

</details>


### [41] [Spacecraft Angular Rate Estimation via Event-Based Camera Sensing](https://arxiv.org/abs/2511.08041)
*Vittorio Franzese,Matteo El Hariry*

Main category: eess.SY

TL;DR: Using event-based cameras to estimate spacecraft angular velocity by analyzing star-induced brightness events; validated via simulations.


<details>
  <summary>Details</summary>
Motivation: Develop a fast, high-temporal-resolution angular-rate estimator for spacecraft that could complement or replace traditional gyros, leveraging asynchronous event cameras and star-field motion.

Method: Analyze the temporal distribution and polarity of brightness events caused by apparent star motion to recover the star-field motion field in the image plane, from which the observer's angular velocity in the camera frame is inferred and converted to spacecraft angular rates given an attitude reference.

Result: Numerical simulations on synthetic event streams with random spacecraft pointing and rate conditions show the approach can accurately estimate angular rates, indicating potential to complement or replace conventional rate sensors.

Conclusion: Event-based vision can provide accurate angular-rate estimates for spacecraft, offering a promising alternative or supplement to traditional rate sensors in attitude determination.

Abstract: This paper presents a method for determining spacecraft angular rates using event-based camera sensing. This is achieved by analyzing the temporal distribution of brightness events triggered by the apparent motion of stars. The location and polarity of the events are used to infer the apparent motion field of the stars, which is, in turn, employed to estimate the observer angular velocity in the camera frame. This can be converted to the spacecraft angular rates provided an attitude reference. The method is validated through numerical simulation for a synthetic dataset of event streams generated on random spacecraft pointing and rates conditions. The accuracy of the method is assessed, demonstrating its potential to complement or replace conventional rate sensors in spacecraft systems using event camera sensing.

</details>


### [42] [Multi-layer barrier function-based adaptive super-twisting controller](https://arxiv.org/abs/2511.08106)
*Antoine Thibault Vié,Leonid Fridman,Roberto Galeazzi,Dimitrios Papageorgiou*

Main category: eess.SY

TL;DR: 提出了一种自适应的超扭曲Sliding模式控制框架，用于存在未知速率界限的扰动的一阶不确定系统；引入嵌套 barrier 函数以保证离散实现下的有界性，且在扰动与采样时间比不利时仍保持有界解。通过Lyapunov分析验证稳定性，并给出数值仿真以展示有效性。


<details>
  <summary>Details</summary>
Motivation: 解决自适应超扭曲控制中 barrier 函数引入的保守性，以及离散时间实现下在扰动突变发生于采样点之间时可能导致的闭环轨迹不可界的问题；通过扩展 barrier 的应用范围和引入嵌套 barrier 机制提升鲁棒性与有界性。

Method: 提出一种自适应超扭曲 Sliding 模态控制框架，利用正定 barrier 函数实现对扰动界的自整定；发展正半定 barrier 的扩展以适用于控制器自适应；提出嵌套 barrier 方案，在扰动-采样时间比不利情形下仍能保证解的有界性；基于Lyapunov分析给出稳定性结论，并通过仿真验证。

Result: 给出 Lyapunov 稳定性的理论证明；数值仿真表明所提框架在存在未知速率界限的扰动下具有良好鲁棒性、能抑制发散并确保有界轨迹，同时降低对初始估计的依赖带来的保守性。

Conclusion: 嵌套 barrier 的自适应超扭曲控制拓展了在不确定速率界限和不利采样时间比下的鲁棒性与有界性保障，理论稳定性得到证实，仿真结果验证了框架的有效性。

Abstract: This article presents an adaptive Super-Twisting Sliding Mode Control framework for uncertain first-order systems, with rate-bounded perturbations, where the bound is constant but unknown. Positive definite barrier functions, when used in self-tuning super-twisting controllers may introduce some conservatism in relation to initial estimations of the perturbation rate bound. Moreover, discrete time implementation of the algorithm does not necessarily guarantee the boundedness of the closed-loop trajectories when sudden changes in the perturbation occur in between two time samples. The salient features of the proposed methodology pertain to extending the use of positive semidefinite barrier functions to Super-Twisting controller adaptation and the employment of a "nested barriers" scheme that ensures boundedness of the solutions even for "unfavourable" perturbations-to-sampling time ratios. The stability of the closed-loop system is assessed via Lyapunov analysis and simulations demonstrate the efficacy of the proposed framework.

</details>


### [43] [A Unified Theory for Transient Synchronization Stability Analysis of Renewable Dominated Power Systems](https://arxiv.org/abs/2511.08165)
*Meng Zhan,Miao Han,Yayao Zhang,Hongsheng Xu,Jiabing Hu,Shijie Cheng,Jürgen Kurths*

Main category: eess.SY

TL;DR: 提出以广义摆动方程(GSE)为核心的RDPS瞬态稳定性统一理论，统一建模切换驱动的风光设备，并通过改进的等面积准则准确评估临界切除时间。


<details>
  <summary>Details</summary>
Motivation: 因为从同步发电机（SG）向以逆变器为主的分布式能源系统（RDPS）的转变，传统的摆动方程无法充分描述切换瞬态和相锁环（PLL）动力学，因此需要一个统一的切换模型与稳定性分析工具。

Method: 建立基于风光设备的瞬态切换机理模型（LVRT约束下），以机械等效和能量守恒为基础，聚焦主导的相锁环（PLL）动力学，将其归结为广义摆动方程（GSE），并提出改进的等面积准则用于稳定性判据。

Result: 即使在大规模新能源场景，临界切除时间的计算误差约为1%。

Conclusion: 基于GSE的非线性动力学方法提供了RDPS瞬态动力学的统一建模与分析框架，有助于理解切换机制并评估RDPS的瞬态稳定性。

Abstract: The change of electric power generation - from synchronous generator (SG) to converter - is generally regarded as the second revolution of power system. Different from rotor swing of SG in traditional grids mainly described by the swing equation (SE), the converter dynamics plays an indispensable role in modern renewable dominated power systems (RDPS). The high complexity of the RDPS, including spatial large-scale, nonlinearity, multi-time-scale, and even sequential switching, prevents us from fully understanding its dynamics and assessing its transient stability under large disturbance. Here, a variety of transient switching mechanism models of renewable devices relying on wind or solar energies under low-voltage ride-through are established and unified, which can be perfectly described by a generalized swing equation (GSE) under parameter changes for switching dynamics. The GSE focusing on the dominant phase-locking loop dynamics is similar to the SE. Mainly relying on the mechanical equivalence and the energy conservative principle, a substantially improved equal-area criterion method is proposed. Based on this method, even for large-scale renewable fields, the calculation errors for the critical clearing time are only about 1%. This elegant nonlinear-dynamics-based approach establishes a unified theory including modelling and analysis for the RDPS transient dynamics.

</details>


### [44] [Stability of Certainty-Equivalent Adaptive LQR for Linear Systems with Unknown Time-Varying Parameters](https://arxiv.org/abs/2511.08236)
*Marcell Bartos,Johannes Köhler,Florian Dörfler,Melanie N. Zeilinger*

Main category: eess.SY

TL;DR: 提出一个基于离线最小均方差滤波与确定等效线性二次调节器的离散时间线性系统自适应控制管道，在未知时变参数下实现有限增益的ℓ2稳定性，并通过在平面四旋翼仿真验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 在系统动力学随时间改变时，基于模型的控制效果会下降，因此需要在线/自适应控制方法来处理未知且时变的参数。

Method: 将 LMS 滤波作为在线参数估计器，与确定等效 LQR 控制器（CE-LQR）组成一个简单、模块化且计算友好的管道；对系统、估计器和控制器的闭环互连给出有限增益 ℓ2 稳定性证明，能容忍未知扰动及时变不确定性。

Result: 理论上证明了闭环系统在 ℓ2 的有限增益稳定性；在一个非线性平面四旋翼的仿真实验中展示了算法的实际可用性。

Conclusion: 所提出的模块化两大经典组件的组合为自适应控制提供一个简单、可扩展且带有稳定性保证的实现框架，具备良好的实际应用前景。

Abstract: Standard model-based control design deteriorates when the system dynamics change during operation. To overcome this challenge, online and adaptive methods have been proposed in the literature. In this work, we consider the class of discrete-time linear systems with unknown time-varying parameters. We propose a simple, modular, and computationally tractable approach by combining two classical and well-known building blocks from estimation and control: the least mean square filter and the certainty-equivalent linear quadratic regulator. Despite both building blocks being simple and off-the-shelf, our analysis shows that they can be seamlessly combined to a powerful pipeline with stability guarantees. Namely, finite-gain $\ell^2$-stability of the closed-loop interconnection of the unknown system, the parameter estimator, and the controller is proven, despite the presence of unknown disturbances and time-varying parametric uncertainties. Real-world applicability of the proposed algorithm is showcased by simulations carried out on a nonlinear planar quadrotor.

</details>


### [45] [PE-TSFM: Self-Supervised Time-Series Learning for Generalizable Power Converter Health Monitoring under Unseen Conditions](https://arxiv.org/abs/2511.08250)
*Xinyuan Liao,Xinyue Zhang,Xing Wei,Junwei Liu,Shuai Zhao,Siqi Bu,Yi Zhang*

Main category: eess.SY

TL;DR: Domain-specific time-series foundation model for power converters (PE-TSFM) pre-trained on large unlabeled domain data with a dual-attention mechanism achieving strong OOD generalization (92% on unseen conditions) compared to generic TSFMs (~60%) and conventional models (~40%).


<details>
  <summary>Details</summary>
Motivation: Address poor generalization of data-driven health monitoring under unseen operating conditions by leveraging domain-specific pretraining to capture physical relationships in power electronics.

Method: Pre-train a domain-specific time-series foundation model on 141 million unlabeled timestamps from operating power converters. Introduce a dual-attention mechanism that combines temporal patterns with inter-channel dependencies to capture sensor relationships and degradation indicators.

Result: PE-TSFM achieves 92% accuracy under unseen conditions, outperforming generic TSFMs (~60%) and conventional models (~40%). Ablation shows channel attention improves performance. Additional studies cover scalability, hyperparameter sensitivity, and interpretability.

Conclusion: A domain-focused TSFM with channel-aware temporal modeling provides strong OOD generalization for converter health monitoring and offers insight through ablation and interpretability studies.

Abstract: Data-driven health monitoring of power converters remains limited by poor generalization to unseen operating conditions. This work addresses this out-of-distribution (OOD) challenge by building a domain-specific time-series foundation model (PE-TSFM) that learns representations directly from large-scale unlabeled converter data. Unlike generic TSFMs trained on broad time-series datasets, the proposed PE-TSFM is pre-trained entirely on domain data, enabling it to learn the physical relationships unique to power electronics. To further tailor the model to this domain, we introduce a dual-attention mechanism that captures both temporal patterns and inter-channel dependencies. While generic TSFMs primarily model temporal dependencies, the added channel attention captures inter-sensor physical relationships essential for converter degradation analysis. A dataset containing 141 million unlabeled timestamps from an operating power converter is used for pre-training. Experiments show that PE-TSFM achieves 92% accuracy under unseen operating conditions. In contrast, generic TSFMs achieve around 60% and conventional time-series models achieve around 40% accuracy. This result confirms the strong OOD generalization of the proposed PE-TSFM. Ablation studies further verify that the introduced channel attention mechanism significantly improves model performance. In addition, we conduct detailed studies on model scalability, hyperparameter sensitivity, and interpretability to provide a comprehensive understanding of the proposed approach.

</details>


### [46] [Extended Time Varying Multi-Cluster Fluctuating Two-Ray Fading Model for Maritime Environment](https://arxiv.org/abs/2511.08338)
*Antoine Thibault Vié,Roberto Galeazzi,Dimitrios Papagergiou*

Main category: eess.SY

TL;DR: 扩展的 MFTR 多簇波动衰落模型，利用随机微分方程驱动时间变化，结合大尺度衰落、时变参数、多普勒效应与延迟损耗，用仿真评估其在海事高带宽通信中的适用性。


<details>
  <summary>Details</summary>
Motivation: 海事通信需比传统 Rayleigh/Rician 更贴近现实的信道建模，尤其是高速船舶在沿海环境中的动态特性、时间变化和多普勒效应。现有 MFTR 已能捕捉多簇波动，但缺乏对大尺度 fading、时变参数和延迟损耗等关键现象的建模，因此需要一个更完整的时间演化信道模型以支持高带宽应用。

Method: 在 MFTR 框架中引入随机微分方程来驱动相位、时延等的时间演化，并结合大尺度衰落、路径损耗以及延迟引起的功率损失等物理因素；同时考虑多普勒效应以体现船速带来的谱移。通过数值仿真对所提模型的准确性进行评估。

Result: 基于仿真的评估表明所扩展的 MFTR 能更好地体现信道的时间变化特性、相位与时延的演化以及大尺度效应；与传统模型相比，能够提供更贴近海事高带宽场景的统计描述（具体数值需在文中结果段给出）。

Conclusion: 将 MFTR 扩展到含有时间演化的随机微分方程驱动的多簇衰落模型，并结合大尺度 fading、延迟损耗和多普勒效应，能够为海事自控与远程操作船舶的高带宽通信提供更现实的信道描述，有助于设计与评估相关传输系统。

Abstract: The recent advancements in autonomous and remote operation of maritime vessels necessitates the development of robust and reliable communication systems to support high-bandwidth applications such as real-time monitoring, navigation, and control. Existing communication channel models, including Rayleigh and Rician fading, are inadequate to accurately describe the dynamic and complex nature of maritime communication, particularly for high-speed vessels in coastal environments. This paper proposes an extension to the Multi-Cluster Fluctuating Two-Ray Fading (MFTR) model that also accounts for key phenomena such as large-scale fading, time-varying parameters and Doppler shifts. The extended MFTR model integrates Stochastic Differential Equations (SDEs) to capture the time-varying characteristics of the channel, such as phase shifts and delays, while considering physical factors like delay-induced power loss and path loss. The accuracy of the proposed model is assessed in simulation.

</details>


### [47] [Power Hardware-in-the-loop Interfacing via $\mathcal{H}_\infty$ Model Matching](https://arxiv.org/abs/2511.08370)
*Jonathan Eid,Ashley Meagher,Dmitry Rimorov,Anil Kumar Bonala,Rajendra Thike,James Richard Forbes*

Main category: eess.SY

TL;DR: 提出一种基于H∞模型匹配的PHIL接口设计，利用透明性作为频域目标，在保留全部信息的前提下实现稳定且高精度耦合，实验验证优于或等同于ITM方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（ITM及其阻抗变体）在准确性和稳定性之间折中，且部分H∞方法未充分利用互连的全部动态信息。设计以透明性为目标，期望同时实现高精度和稳定性，且充分利用理想化网-器互连的动态信息。

Method: 将PHIL问题表述为H∞控制问题，通过模型匹配（model matching）以透明性为显式频域控制目标，利用理想互连的动态信息进行优化。并在实时阻性负载PHIL系统上进行实验验证。

Result: 实验结果显示该透明性导向的H∞模型匹配方法在实时PHIL设置中，精度达到或优于基于ITM的接口。

Conclusion: 通过以透明性为导向的H∞模型匹配，能够充分利用理想化互连的动态信息，在保持稳定性的同时提升PHIL接口的准确性。

Abstract: This paper presents an $\mathcal{H}_\infty$ model matching control-based approach to the problem of power hardware-in-the-loop (PHIL) interfacing. The objective is to interconnect a grid simulation and a physical device via an interface in a way that is stable and accurate. Conventional approaches include the ideal transformer method (ITM) and its impedance-based variants, which trade accuracy for stability, as well as some $\mathcal{H}_\infty$ control-based approaches, which do not make use of all the available information in their optimization for accuracy. Designing for transparency, as opposed to accuracy as existing approaches do, would achieve both accuracy and stability, while making use of all the dynamical information present in the idealized interconnection of the grid and device. The approach proposed in this paper employs model matching to formulate the PHIL problem as an $\mathcal{H}_\infty$ control problem using transparency as the explicit frequency-domain control objective. The approach is experimentally validated in a real-time resistive-load PHIL setup, and is found to achieve accuracy levels that are comparable or superior to those of an ITM-based interface.

</details>


### [48] [Active Short Circuit and Safe Discharge Mechanisms in Multi-Phase Inverters During Critical Failures](https://arxiv.org/abs/2511.08405)
*Siddhesh Pimpale,Sagar Mahadik*

Main category: eess.SY

TL;DR: 提出一种用于多相逆变器的集中短路检测、主动相短路和安全放电的故障保护架构，提升在故障情况下的检测、响应和可靠性，适用于高功率、高安全性需求的 EV 应用中的 SiC 模块。


<details>
  <summary>Details</summary>
Motivation: 多相逆变器在 EV 功率传动中易受短路故障，若不抑制将导致级联故障和永久性损坏，亟需鲁棒的故障保护机制以提高安全性与可靠性。

Method: 提出集中短路检测、主动相短路和受控放电的组合方案；在芯片级实现主动短路，快速隔离故障相，防止故障扩散；放电机制对故障情景下的能量进行受控释放，降低关键部件的热应力。

Result: 实验结果显示，提出的机制显著提升了故障检测性能、故障时的系统响应速度以及整体运行的安全性，与现有方法相比具有显著改进。对于需要高安全性的多相逆变器（如 EV）具有重要意义。

Conclusion: 该方法可增强多相逆变器在故障条件下的安全性与可靠性，建议在高功率应用中进一步验证其鲁棒性并推广至实际设计。

Abstract: The multi-phase inverter has become more complicated, particularly in an Electric Vehicle (EV)'s power train, which requires a robust fault protection system. The proposed active short circuit and safe discharge mechanisms are also included in this work, dedicated to multi-phase converters in failure conditions. With silicon carbide (SiC) power modules increasingly used in high efficiency and high-power applications, the reliability under fault conditions is an extremely important factor. Cascading failures and permanent damage will occur in multi phase inverter systems if short circuit faults are not prevented. The proposed method combines one centralized short circuit detection, active phase shorting and controlled discharge to make these structures more robust. The on chip active short circuit mechanism isolates the affected phases quickly preventing faults from spreading to other areas of the inverter and the safe discharge mechanism controls energy discharged in fault scenarios, which reduces the thermal stress placed on essential components. The experimental results show that the proposed mechanisms can effectively enhance a fault detection performance, system response during faults, and the operation as whole at faults over the several existing methods. These mechanisms are demonstrated to be very important for enhancing the safety and reliability of multiphase inverters, especially for critical applications of such inverters as EV where high operational security is required.

</details>


### [49] [Probabilistic Safety Guarantee for Stochastic Control Systems Using Average Reward MDPs](https://arxiv.org/abs/2511.08419)
*Saber Omidi,Marek Petrik,Se Young Yoon,Momotaz Begum*

Main category: eess.SY

TL;DR: 提出将安全目标化为平均奖励MDP的问题，提供一种新算法在有限状态集上计算安全策略，并通过线性规划求解；在双积分器与倒立摆等系统上验证，结果显示平均奖励解更全面、收敛更快、质量更高，相较最小折扣奖励解。


<details>
  <summary>Details</summary>
Motivation: 在带有已知噪声分布的随机控制系统中，需在不确定演化过程中以高置信度满足约束，但现有方法难以兼顾长期安全性，故需要一个能直接利用MDP框架的安全性分析与优化工具。

Method: 将安全性目标降维到平均奖励MDP，建立在有限状态集上的安全水平，通过求解线性规划等标准MDP技术来得到安全策略；并进行数值验证与分析。

Result: 在双积分器和倒立摆等系统上进行数值验证，平均奖励MDP解法在收敛速度、覆盖范围和解质量上均优于最小折扣奖励解。

Conclusion: 将安全目标转化为平均奖励MDP并用线性规划求解，是一种有效、可扩展的安全控制方法，适用于有限状态集的随机系统。

Abstract: Safety in stochastic control systems, which are subject to random noise with a known probability distribution, aims to compute policies that satisfy predefined operational constraints with high confidence throughout the uncertain evolution of the state variables. The unpredictable evolution of state variables poses a significant challenge for meeting predefined constraints using various control methods. To address this, we present a new algorithm that computes safe policies to determine the safety level across a finite state set. This algorithm reduces the safety objective to the standard average reward Markov Decision Process (MDP) objective. This reduction enables us to use standard techniques, such as linear programs, to compute and analyze safe policies. We validate the proposed method numerically on the Double Integrator and the Inverted Pendulum systems. Results indicate that the average-reward MDPs solution is more comprehensive, converges faster, and offers higher quality compared to the minimum discounted-reward solution.

</details>


### [50] [Computable Characterisations of Scaled Relative Graphs of Closed Operators](https://arxiv.org/abs/2511.08420)
*Talitha Nauta,Richard Pates*

Main category: eess.SY

TL;DR: 论文提出针对闭合线性算子的精确且可计算的SRG构造方法，基于最大/最小增益计算，适用于有界和无界算子；并给出如何将SRG应用于常见的线性时不变（LTI）系统的算子；对于状态空间模型，利用有界实时引理（Bounded Real Lemma）来构建SRG。


<details>
  <summary>Details</summary>
Motivation: 在多输入多输出系统的稳定性与鲁棒性分析中，SRG作为一个有前景的工具。本工作将SRG扩展到更广泛的算子范畴（包括有界与无界算子），并为LTI系统提供可操作的SRG构建方法。

Method: 提出基于最大增益和最小增益的精确可计算的SRG构造步骤，覆盖有界与无界算子，并给出如何将这些构造应用于用于建模线性时不变系统的典型算子。对于状态空间模型，利用有界实引理（Bounded Real Lemma）将SRG与系统矩阵联系起来以实现SRG的构建。

Result: 提供一套适用于有界与无界算子的SRG构造工具，并演示如何为LTI系统中的典型算子绘制SRG；在状态空间情形，通过有界实引理将SRG的构建落地。

Conclusion: SRG成为分析线性系统稳定性与鲁棒性的一个可操作框架，尤其扩展到无界算子及状态空间模型，通过最大/最小增益策略与有界实引理实现可计算的SRG。

Abstract: Scaled Relative Graphs (SRGs) provide a promising tool for stability and robustness analysis of multi-input-multi-output systems. In this paper, we provide tools for exact and computable constructions of the SRG for closed linear operators, based on maximum and minimum gain computations. The results are suitable for bounded and unbounded operators, and we specify how they can be used to draw SRGs for the typical operators that are used to model linear-time-invariant dynamical systems. Furthermore, for the special case of state-space models, we show how the Bounded Real Lemma can be used to construct the SRG.

</details>


### [51] [Toward a Safety Argumentation Lifecycle for Automated Vehicles: Promoting Communication and Interdependency with System Lifecycle Processes](https://arxiv.org/abs/2511.08499)
*Marvin Loba,Robert Graubohm,Niklas Braun,Nayel Fabian Salem,Markus Maurer*

Main category: eess.SY

TL;DR: 提出一个以需求驱动的安全论证生命周期，并将其与系统生命周期对齐，同时引入面向利益相关者的表示驱动沟通（representation-supported communication）以提升透明度和前置风险沟通效果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在开放环境中的残余风险不可完全消除，需建立可辩护的安全论证及其生命周期；现有过程规范在可验证性、沟通透明度和跨阶段协同方面存在不足。

Method: 通过对现有文献的状态分析，提炼安全论证生命周期的需求，并研究安全论证与系统生命周期之间的耦合关系；提出表示驱动的沟通概念以针对不同目标受众设计的论证表示；并给出将该框架整合到系统生命周期中的路径与机制，探讨透明的前置风险沟通的空白。

Result: 确立了一个以需求为驱动的安全论证生命周期设计框架；揭示当前过程规范的局限性及隐性知识；提出面向目标利益相关者的表示驱动沟通框架；给出将该框架嵌入系统生命周期的整合路径；并指出文献在透明前置风险沟通方面的不足。

Conclusion: 该方法有助于在自动驾驶系统部署阶段实现更充分的利益相关者沟通与可辩护的安全论证；未来工作将聚焦于在实际场景中的应用、评估与框架的进一步完善。

Abstract: Despite the growing number of automated vehicles on public roads, operating such systems in open contexts will inevitably involve incidents. This results from an inherent risk in road traffic, which arises from multiple sources of complexity and can never be fully eliminated. One central challenge lies in developing a defensible case that the residual risk has been reduced to a reasonable level. While a safety argumentation is a common means to represent this case, there is a need to guide its creation and maintenance by adequate processes. In this paper, we derive requirements for a safety argumentation lifecycle based on examining the current state of the art. In particular, the requirement-driven process design accounts for both identified limitations of current process specifications and implicit knowledge contained in related work. Subsequently, we reflect on interdependencies between the resulting safety argumentation lifecycle and the system lifecycle. Moreover, we discuss a gap in literature regarding transparent ex ante risk communication. Correspondingly, we introduce the concept of representation-supported communication that is based on deriving representations from the argumentation with respect to target stakeholders and communication purposes. Finally, we demonstrate how this approach can be integrated into the system lifecycle to facilitate stakeholder communication.

</details>


### [52] [A bioreactor-based architecture for in vivo model-based and sim-to-real learning control of microbial consortium composition](https://arxiv.org/abs/2511.08554)
*Sara Maria Brancato,Davide Salzano,Davide Fiore,Francesco De Lellis,Giovanni Russo,Mario di Bernardo*

Main category: eess.SY

TL;DR: 提出一种非基因改造、非环境剧变的两菌共养调控架构，通过混合腔与储罐实现对密度与组成的精准、鲁棒调控；在大肠杆菌两菌系中实现了模型驱动和仿真到现实的学习控制器，能够跟踪时变目标并对扰动具备恢复能力。


<details>
  <summary>Details</summary>
Motivation: 实现对微生物共生体系的可扩展、稳态共存控制，以利工业化生产；现有策略要么带来代谢负担（遗传修饰），要么通过环境改变但可能降低产量或稳定性。需要无遗传工程和最小环境干扰的可控方案。

Method: 提出包含混合腔（协同培养）和储罐（维持较慢生长菌株）的两腔生物反应器控制架构；为两腔开发基于模型的控制器和从仿真到现实的学习控制器；在体外/体内对两菌株大肠杆菌共养进行验证。

Result: 实现对共生密度与组成的精准且鲁棒调控，能够跟踪时间变化的参考信号，并在扰动下实现快速恢复。

Conclusion: 提供一种通用、非遗传工程与非剧烈环境干预的共生调控框架，具备潜在的工业化扩展性，用于稳定维持两菌系共生与产物产出。

Abstract: Microbial consortia offer significant biotechnological advantages over monocultures for bioproduction. However, industrial deployment is hampered by the lack of scalable architectures to ensure stable coexistence between populations. Existing strategies rely on genetic modifications, which impose metabolic load, or environmental changes, which can reduce production. We present a versatile control architecture to regulate density and composition of a two-strain consortium without genetic engineering or drastic environmental changes. Our bioreactor-based control architecture comprises a mixing chamber where both strains are co-cultured and a reservoir sustaining the slower-growing strain. For both chambers we develop model-based and sim-to-real learning controllers. The control architecture is then validated in vivo on a two-strain Escherichia coli consortium, achieving precise and robust regulation of consortium density and composition, including tracking of time-varying references and recovery from perturbations.

</details>


### [53] [The curse of dimensionality: what lies beyond the capabilities of physics-informed neural networks](https://arxiv.org/abs/2511.08561)
*J. Penuela,H. Ouerdane*

Main category: eess.SY

TL;DR: PINNs robust for forward dynamics but struggle with inverse parameter identifiability; with RC低通滤波器，超过两个参数时无法得到唯一解。


<details>
  <summary>Details</summary>
Motivation: 评估PINN在病态逆问题中的可靠性，揭示参数发现的边界。

Method: 以RC低通滤波器为简单示例，比较前向求解与逆向参数估计，当对超过两个参数进行拟合时，PINN难以实现唯一性。

Result: 在前向问题中，PINN能准确预测系统动力学；在求解逆问题且拟合超过两个参数时，无法获得唯一的物理参数解。

Conclusion: 揭示PINN在物理系统参数发现中的局限性，提示需引入正则化、先验信息或参数约束以提升识别性。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising framework for solving forward and inverse problems governed by differential equations. However, their reliability when used in ill-posed inverse problems remains poorly understood. In this study, we explore the fundamental limitations of PINNs using a simple illustrative case: RC low-pass filters. Showing that while PINNs can accurately predict system dynamics in forward problems, they fail to recover unique physical parameters when solving inverse problems when more than two parameters are approximated. Our findings provide grounds to understand the boundaries of PINNs applicability for parameter discovery in physical systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Optimizing Classification of Infrequent Labels by Reducing Variability in Label Distribution](https://arxiv.org/abs/2511.07459)
*Ashutosh Agarwal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a novel solution, LEVER, designed to address the challenges posed by underperforming infrequent categories in Extreme Classification (XC) tasks. Infrequent categories, often characterized by sparse samples, suffer from high label inconsistency, which undermines classification performance. LEVER mitigates this problem by adopting a robust Siamese-style architecture, leveraging knowledge transfer to reduce label inconsistency and enhance the performance of One-vs-All classifiers. Comprehensive testing across multiple XC datasets reveals substantial improvements in the handling of infrequent categories, setting a new benchmark for the field. Additionally, the paper introduces two newly created multi-intent datasets, offering essential resources for future XC research.

</details>


### [55] [Slimmable NAM: Neural Amp Models with adjustable runtime computational cost](https://arxiv.org/abs/2511.07470)
*Steven Atkinson*

Main category: cs.LG

TL;DR: 提出可在不重新训练的情况下在线调整大小和计算成本的 slimmable Neural Amp Models，并在音频插件中实现实时演示。


<details>
  <summary>Details</summary>
Motivation: 为音乐家提供一个在精度与计算资源之间自由权衡的实时可部署神经放大器模型，同时降低训练和部署成本。

Method: 提出并评估一种可伸缩宽度的神经放大模型（slimmable neural amp models），使模型在运行时可动态改变容量且几乎无额外开销；对比基线并实现音频效果插件的实时演示。

Result: 在与常用基线的比较中给出性能量化，并实现了模型在音频效果插件中的实时演示。

Conclusion: 该方法实现了可变计算需求的实时可部署性，便于音乐创作和演出中对资源的动态适配。

Abstract: This work demonstrates "slimmable Neural Amp Models", whose size and computational cost can be changed without additional training and with negligible computational overhead, enabling musicians to easily trade off between the accuracy and compute of the models they are using. The method's performance is quantified against commonly-used baselines, and a real-time demonstration of the model in an audio effect plug-in is developed.

</details>


### [56] [Towards Personalized Quantum Federated Learning for Anomaly Detection](https://arxiv.org/abs/2511.07471)
*Ratun Rahman,Sina Shaham,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 提出了一种名为PQFL的个性化量子联邦学习框架，用于异常检测。通过在量子客户端实现参数化量子电路与经典优化器的本地训练，并以量子为中心的个性化策略来适应各自的硬件特性与数据表示。相较于现有方法，PQFL在真实条件下显著提升检测准确性，降低误报。


<details>
  <summary>Details</summary>
Motivation: 在量子联邦学习场景中，客户端在硬件能力、量子电路设计、噪声水平以及数据编码等方面存在异质性，导致全局模型在非独立同分布（non-IID）数据和不均衡数据下效果不佳；需要一种能够自适应不同客户端特性的个性化方法来提升异常检测性能。

Method: 提出PQFL框架：1) 在每个量子客户端使用参数化量子电路和经典优化器进行本地模型训练；2) 引入以量子为中心的个性化策略，针对每个客户端的硬件特性与数据表示进行模型自适应；3) 通过联邦式聚合结合局部个性化更新以实现整体协同学习。

Result: 大量实验表明，PQFL在多样化且现实的条件下显著提升异常检测准确性。相较于最先进方法，PQFL将误报降低最多23%，在AUROC上提升24.2%，在AUPR上提升20.5%，显示其在实际量子联邦设置中的有效性与可扩展性。

Conclusion: PQFL能够有效应对量子客户端的异质性和非IID数据，提升异常检测的综合表现，具有广泛的应用前景和扩展性。

Abstract: Anomaly detection has a significant impact on applications such as video surveillance, medical diagnostics, and industrial monitoring, where anomalies frequently depend on context and anomaly-labeled data are limited. Quantum federated learning (QFL) overcomes these concerns by distributing model training among several quantum clients, consequently eliminating the requirement for centralized quantum storage and processing. However, in real-life quantum networks, clients frequently differ in terms of hardware capabilities, circuit designs, noise levels, and how classical data is encoded or preprocessed into quantum states. These differences create inherent heterogeneity across clients - not just in their data distributions, but also in their quantum processing behaviors. As a result, training a single global model becomes ineffective, especially when clients handle imbalanced or non-identically distributed (non-IID) data. To address this, we propose a new framework called personalized quantum federated learning (PQFL) for anomaly detection. PQFL enhances local model training at quantum clients using parameterized quantum circuits and classical optimizers, while introducing a quantum-centric personalization strategy that adapts each client's model to its own hardware characteristics and data representation. Extensive experiments show that PQFL significantly improves anomaly detection accuracy under diverse and realistic conditions. Compared to state-of-the-art methods, PQFL reduces false errors by up to 23%, and achieves gains of 24.2% in AUROC and 20.5% in AUPR, highlighting its effectiveness and scalability in practical quantum federated settings.

</details>


### [57] [Multivariate Variational Autoencoder](https://arxiv.org/abs/2511.07472)
*Mehmet Can Yavuz*

Main category: cs.LG

TL;DR: MVAE: a Gaussian-tractable VAE with a global coupling matrix to induce cross-dataset latent correlations, plus per-sample diagonal scales, enabling full-covariance posterior with analytic KL and efficient reparameterization; shows competitive reconstruction and notably better calibration and unsupervised structure metrics across MNIST, Fashion-MNIST, CIFAR-10/100; latent traversals are smoother; code is released for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Improve posterior expressivity in VAEs without sacrificing tractable KL and reparameterization, addressing limitations of diagonal-covariance VAEs such as poor calibration and limited modeling of latent correlations; provide a robust benchmark across standard datasets and enable fair comparisons via released code.

Method: Posterior covariance is factorized as Cov(z|x) = diag(sigma) * C * diag(sigma), where C is a global coupling matrix inducing dataset-wide latent correlations and diag(sigma) is a per-sample diagonal scaling for local uncertainty. This yields a full-covariance Gaussian with analytic KL to a standard Gaussian prior; reparameterization uses L = C diag(sigma). Trained and evaluated on Larochelle-style MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, with comparisons to diagonal-covariance VAEs at matched capacity. Evaluation metrics include reconstruction MSE, NLL, Brier score, ECE, and unsupervised structure metrics NMI/ARI; latent-plane traversals visualize smoother factor interactions. A reproducible implementation with training/evaluation scripts and sweep utilities is released.

Result: MVAE consistently matches or improves reconstruction (lower MSE) and delivers robust gains in calibration (lower NLL/Brier/ECE) and unsupervised structure (higher NMI/ARI) relative to diagonal-covariance VAEs with matched capacity, particularly at mid-range latent sizes. Latent-plane visualizations show smoother, more coherent traversals and sharper local detail.

Conclusion: Introducing a globally coupled full-covariance posterior with per-sample diagonal modulation retains Gaussian tractability and reparameterization efficiency while enhancing calibration and discovery of latent structure. The approach yields practical benefits across multiple datasets and latent dimensionalities, and the authors provide reproducible code to facilitate adoption and fair benchmarking.

Abstract: We present the Multivariate Variational Autoencoder (MVAE), a VAE variant that preserves Gaussian tractability while lifting the diagonal posterior restriction. MVAE factorizes each posterior covariance, where a \emph{global} coupling matrix $\mathbf{C}$ induces dataset-wide latent correlations and \emph{per-sample} diagonal scales modulate local uncertainty. This yields a full-covariance family with analytic KL and an efficient reparameterization via $\mathbf{L}=\mathbf{C}\mathrm{diag}(\boldsymbolσ)$. Across Larochelle-style MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, MVAE consistently matches or improves reconstruction (MSE~$\downarrow$) and delivers robust gains in calibration (NLL/Brier/ECE~$\downarrow$) and unsupervised structure (NMI/ARI~$\uparrow$) relative to diagonal-covariance VAEs with matched capacity, especially at mid-range latent sizes. Latent-plane visualizations further indicate smoother, more coherent factor traversals and sharper local detail. We release a fully reproducible implementation with training/evaluation scripts and sweep utilities to facilitate fair comparison and reuse.

</details>


### [58] [Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data](https://arxiv.org/abs/2511.07481)
*Reem Al-Saidi,Erman Ayday,Ziad Kobti*

Main category: cs.LG

TL;DR: 在基因组序列上对预训练与微调的语言模型嵌入进行重建攻击分析，发现微调能显著提升对重建攻击的抵抗性，并通过为 DNA 序列定制的标记化与对嵌入的分维度比较，揭示隐私风险的分布与变化。


<details>
  <summary>Details</summary>
Motivation: 基于 Pan 等人关于语言模型嵌入可能泄露敏感信息的发现，本文在未明确嵌入类型且针对基因组数据的情境下，系统评估微调对隐私保护的影响，填补方法学空白并扩展数据集到 HS3D。

Method: 1) 将 Pan 等的重建攻击管线应用于预训练与微调的嵌入；2) 为 DNA 序列设计专门的标记化；3) 进行对比分析，比较预训练与微调嵌入在位置-特异性、碱基类型及隐私变化方面的表现，覆盖不同嵌入类型与维度。

Result: 微调在多种架构上提升对重建攻击的抵抗力：XLNet +19.8%、GPT-2 +9.8%、BERT +7.8%，显示任务特定优化可作为隐私增强机制。

Conclusion: 强调在处理敏感基因数据的语言模型中需要更强的隐私保护措施，并将微调视为潜在的隐私提升方向，值得进一步研究与实践。

Abstract: This study investigates embedding reconstruction attacks in large language models (LLMs) applied to genomic sequences, with a specific focus on how fine-tuning affects vulnerability to these attacks. Building upon Pan et al.'s seminal work demonstrating that embeddings from pretrained language models can leak sensitive information, we conduct a comprehensive analysis using the HS3D genomic dataset to determine whether task-specific optimization strengthens or weakens privacy protections. Our research extends Pan et al.'s work in three significant dimensions. First, we apply their reconstruction attack pipeline to pretrained and fine-tuned model embeddings, addressing a critical gap in their methodology that did not specify embedding types. Second, we implement specialized tokenization mechanisms tailored specifically for DNA sequences, enhancing the model's ability to process genomic data, as these models are pretrained on natural language and not DNA. Third, we perform a detailed comparative analysis examining position-specific, nucleotide-type, and privacy changes between pretrained and fine-tuned embeddings. We assess embeddings vulnerabilities across different types and dimensions, providing deeper insights into how task adaptation shifts privacy risks throughout genomic sequences. Our findings show a clear distinction in reconstruction vulnerability between pretrained and fine-tuned embeddings. Notably, fine-tuning strengthens resistance to reconstruction attacks in multiple architectures -- XLNet (+19.8\%), GPT-2 (+9.8\%), and BERT (+7.8\%) -- pointing to task-specific optimization as a potential privacy enhancement mechanism. These results highlight the need for advanced protective mechanisms for language models processing sensitive genomic data, while highlighting fine-tuning as a potential privacy-enhancing technique worth further exploration.

</details>


### [59] [Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits](https://arxiv.org/abs/2511.07482)
*Dev Patel,Gabrielle Gervacio,Diekola Raimi,Kevin Zhu,Ryan Lagasse,Gabriel Grand,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 提出 Alignment-Aware Probe Pruning (AAPP)，通过在推理阶段自适应保留对齐相关电路的动态结构裁剪，在提升效率的同时增强安全性；在 LLaMA 2-7B、Qwen2.5-14B-Instruct、Gemma-3-12B-IT 上实现等效计算下拒绝率提升约50%。


<details>
  <summary>Details</summary>
Motivation: 动态裁剪在提升推理效率方面表现通常优于静态方法，但因仅保留输入相关的安全关键裁剪，可能放大对齐脆弱性，需提出对齐感知的裁剪策略以兼顾效率与安全。

Method: 基于 Probe Pruning 的动态结构裁剪框架，提出 Alignment-Aware Probe Pruning（AAPP），在推理过程中自适应识别并保留与对齐相关的电路结构，进而实现对齐安全的裁剪。

Result: 在 LLaMA 2-7B、Qwen2.5-14B-Instruct、Gemma-3-12B-IT 上的实验表明，AAPP 在等效计算下将拒绝率提升约50%，实现了高效且安全的 LLM 部署。

Conclusion: AAPP 将对齐感知融入动态裁剪，显著提升在保证安全性的前提下的推理效率，为大规模语言模型的安全部署提供有效路径。

Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.

</details>


### [60] [Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs](https://arxiv.org/abs/2511.07484)
*Dharmateja Priyadarshi Uddandarao,Ravi Kiran Vadlamani*

Main category: cs.LG

TL;DR: 在结构因果模型与基于Transformer的生成式AI的结合下，提出一种用于对抗因果用户行为预测的框架，能够在条件因果变量的驱动下生成更真实的行为轨迹，提升可解释性和预测/uplift性能。


<details>
  <summary>Details</summary>
Motivation: 面临在不同干预情景下预测用户行为的挑战，以及现有方法在可解释性和干预评估方面的局限性。通过将结构化因果推断与强大生成模型结合，提供对干预的可控、可解释评估。

Method: 建立连接用户交互、采用指标与产品特征之间关系的因果图；在因果变量条件下，使用生成模型（以Transformer为核心）生成满足因果约束的 counterfactual 行为轨迹；通过因果路径可视化提升可解释性；在Web、移动应用和电商数据集上进行验证。

Result: 所提出的方法在跨领域数据集上优于传统预测与提升建模方法，展示了在干预情景下的更准确性与更高的可解释性。

Conclusion: 该框架使产品团队能够在上线前对干预方案进行高保真仿真与评估，依赖因果路径的可视化提升决策透明度与信度。

Abstract: This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that map the connections between user interactions, adoption metrics, and product features. The framework generates realistic behavioral trajectories under counterfactual conditions by using generative models that are conditioned on causal variables. Tested on datasets from web interactions, mobile applications, and e-commerce, the methodology outperforms conventional forecasting and uplift modeling techniques. Product teams can effectively simulate and assess possible interventions prior to deployment thanks to the framework improved interpretability through causal path visualization.

</details>


### [61] [When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift](https://arxiv.org/abs/2511.07485)
*Sushant Mehta*

Main category: cs.LG

TL;DR: 提出一个统一理论框架，将偏差（虚假相关、子群分布偏移、类别不均衡、公平性违背）在信息论层面统一为条件独立性的违反，给出等价性条件并通过实验验证，预测一个强虚假相关性α等价于一个不均衡比率r≈(1+α)/(1−α) 在特征重叠假设下对 Worst-group 的准确率影响，六个数据集、三种模型验证，误差在 worst group 的准确率上差异约3%。


<details>
  <summary>Details</summary>
Motivation: 解决在不同研究领域对偏差的孤立研究所造成的碎片化问题，提供一个统一的理论框架，以便跨领域转移 debiasing 方法和理解偏差对模型在不同场景下的影响。

Method: 将偏差形式化为对条件独立性的违反，并采用信息论量纲进行度量；推导出等价性条件，建立虚假相关强度α与子群不平衡比率r之间的关系，并给出在特征重叠条件下的近似公式 r≈(1+α)/(1−α)。通过六个数据集、三种架构的实验验证所提出的等价性。

Result: 给出严格的等价性条件以及一个可操作的关系式 r≈(1+α)/(1−α)；在六个数据集和三种架构上验证了该等价性， Worst-group 的准确率损失与预测的等价性在约3%的范围内。

Conclusion: 建立了公平性、鲁棒性和分布漂移研究之间的桥梁，为跨领域 debiasing 的方法迁移提供理论支撑。

Abstract: Machine learning systems exhibit diverse failure modes: unfairness toward protected groups, brittleness to spurious correlations, poor performance on minority sub-populations, which are typically studied in isolation by distinct research communities. We propose a unifying theoretical framework that characterizes when different bias mechanisms produce quantitatively equivalent effects on model performance. By formalizing biases as violations of conditional independence through information-theoretic measures, we prove formal equivalence conditions relating spurious correlations, subpopulation shift, class imbalance, and fairness violations. Our theory predicts that a spurious correlation of strength $α$ produces equivalent worst-group accuracy degradation as a sub-population imbalance ratio $r \approx (1+α)/(1-α)$ under feature overlap assumptions. Empirical validation in six datasets and three architectures confirms that predicted equivalences hold within the accuracy of the worst group 3\%, enabling the principled transfer of debiasing methods across problem domains. This work bridges the literature on fairness, robustness, and distribution shifts under a common perspective.

</details>


### [62] [Provably Efficient Sample Complexity for Robust CMDP](https://arxiv.org/abs/2511.07486)
*Sourav Ganguly,Arnob Ghosh*

Main category: cs.LG

TL;DR: 提出一个鲁棒约束马尔可夫决策过程的样本复杂度分析。通过引入扩展状态来表示剩余效用预算，提出鲁棒约束值迭代（RCVI）算法，在生成模型下实现近似εviolations的策略，获得 Õ(|S||A|H^5/ε^2) 的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 在真实环境与仿真模型存在差异时，学习需同时优化累积回报和满足安全约束。现有RCMDP的迭代复杂度有保障，但缺乏样本复杂度分析；且马尔可夫策略在矩形不确定集下可能非最优。

Method: 扩展状态以包含剩余效用预算，提出鲁棒约束值迭代（RCVI）算法以对待最坏情形的转移，同时在可生成模型中通过采样获得近似最优策略。算法核心是解决在不确定性集合下的最优性和约束满足性。

Result: 给出首个RCMDP的样本复杂度界， Õ(|S||A|H^5/ε^2)，在保证ε内的违规量的前提下实现鲁棒性；并通过实验验证该方法的有效性。

Conclusion: 首次给出RCMDP的样本复杂度保证，并提出RCVI作为有效学习框架，用于在仿真与真实环境差异条件下学习满足安全约束的鲁棒策略。

Abstract: We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs), where the agent must maximize reward while ensuring cumulative utility exceeds a threshold under the worst-case dynamics within an uncertainty set. While recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, their sample complexity guarantees remain largely unexplored. In this paper, we first show that Markovian policies may fail to be optimal even under rectangular uncertainty sets unlike the {\em unconstrained} robust MDP. To address this, we introduce an augmented state space that incorporates the remaining utility budget into the state representation. Building on this formulation, we propose a novel Robust constrained Value iteration (RCVI) algorithm with a sample complexity of $\mathcal{\tilde{O}}(|S||A|H^5/ε^2)$ achieving at most $ε$ violation using a generative model where $|S|$ and $|A|$ denote the sizes of the state and action spaces, respectively, and $H$ is the episode length. To the best of our knowledge, this is the {\em first sample complexity guarantee} for RCMDP. Empirical results further validate the effectiveness of our approach.

</details>


### [63] [FMMI: Flow Matching Mutual Information Estimation](https://arxiv.org/abs/2511.08552)
*Ivan Butakov,Alexander Semenenko,Alexey Frolov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 通过学习一个正则化流，将联合分布和边际分布互相转换，从而估计互信息；不再使用分类判别方法，提升高维场景的效率与精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于判别器的互信息估计在高维数据和精度方面的局限性，寻求一个更高效、可扩展的MI估计框架。

Method: 设计并训练一个可逆的正则化流，将一个分布映射到另一个分布，使得两者的密度比可由流的变换直接推导，从而得到互信息的估计。与基于分类器的方法不同，这种方法通过密度变换来实现MI计算。

Result: 该方法在高维数据上实现了更高的计算效率与更高的精度，且对广泛的真实MI值范围具有鲁棒性。

Conclusion: 提出了一种基于正则化流的新型互信息估计框架，提供了比传统判别式方法更高效、可扩展的MI估计策略。

Abstract: We introduce a novel Mutual Information (MI) estimator that fundamentally reframes the discriminative approach. Instead of training a classifier to discriminate between joint and marginal distributions, we learn a normalizing flow that transforms one into the other. This technique produces a computationally efficient and precise MI estimate that scales well to high dimensions and across a wide range of ground-truth MI values.

</details>


### [64] [Methodological Precedence in Health Tech: Why ML/Big Data Analysis Must Follow Basic Epidemiological Consistency. A Case Study](https://arxiv.org/abs/2511.07500)
*Marco Roccetti*

Main category: cs.LG

TL;DR: 该论文强调：若基本设计和数据质量存在缺陷，复杂的 ML/大数据分析只能放大偏差，导致不可信的结果；通过对一篇队列研究的简易统计比对揭示设计问题与选择偏倚的数学伪影，主张在无随机化情况下需使用稳健方法（如倾向评分匹配）来实现因果推断。


<details>
  <summary>Details</summary>
Motivation: 在健康研究领域，尽管 ML 与大数据有潜在提升诊断和风险预测的能力，但前提是数据质量、抽样与统计设计要严谨，否则复杂分析无法纠正基本缺陷。

Method: 以一篇关于疫苗结果与精神病事件的最近发表队列研究为对象，应用简单描述统计和国家级流行病学基准进行对照分析，揭示HR与发病率比较之间的矛盾，并指出这些悖论源于未校正的选择偏倚。

Result: 揭示多处统计悖论：在高风险人群中出现不合理的慢性疾病风险下降、发病率比较矛盾等，导致所报告的HR不可信，结果被视为数学伪影。

Conclusion: 强调在建立因果结论前必须通过基本流行病学一致性检验；在缺乏随机化的行政数据中，需采用稳健方法（如倾向评分匹配）以获得更可靠的因果推断。

Abstract: The integration of advanced analytical tools, including Machine Learning (ML) and massive data processing, has revolutionized health research, promising unprecedented accuracy in diagnosis and risk prediction. However, the rigor of these complex methods is fundamentally dependent on the quality and integrity of the underlying datasets and the validity of their statistical design. We propose an emblematic case where advanced analysis (ML/Big Data) must necessarily be subsequent to the verification of basic methodological coherence. This study highlights a crucial cautionary principle: sophisticated analyses amplify, rather than correct, severe methodological flaws rooted in basic design choices, leading to misleading or contradictory findings. By applying simple, standard descriptive statistical methods and established national epidemiological benchmarks to a recently published cohort study on vaccine outcomes and psychiatric events, we expose multiple, statistically irreconcilable paradoxes. These paradoxes, including an implausible risk reduction for a chronic disorder in a high-risk group and contradictory incidence rate comparisons, definitively invalidate the reported hazard ratios (HRs). We demonstrate that the observed effects are mathematical artifacts stemming from an uncorrected selection bias in the cohort construction. This analysis serves as a robust reminder that even the most complex health studies must first pass the test of basic epidemiological consistency before any conclusion drawn from subsequent advanced ML or statistical modeling can be considered valid or publishable. We conclude that robust methods, such as Propensity Score Matching, are essential for achieving valid causal inference from administrative data in the absence of randomization

</details>


### [65] [Private-RAG: Answering Multiple Queries with LLMs while Keeping Your Data Private](https://arxiv.org/abs/2511.07637)
*Ruihan Wu,Erchi Wang,Zhiyuan Zhang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出两种多查询DP-RAG算法，MURAG与MURAG-ADA，在私有文档的多次检索场景下维持可用性，逐文档检索频次而非总查询次数累积隐私损失，并通过私有化发布查询阈值提高文档选择精度，实验证明在ε≈10的预算下数百次查询仍具意义效用。


<details>
  <summary>Details</summary>
Motivation: 在RAG中使用外部语料库时，若包含敏感信息，未保护的RAG易泄露私人数据。现有DP-RAG多为单查询，难以应对现实的多查询场景，因此需要扩展到多查询情形并提升实用性。

Method: 提出两种算法：MURAG引入单独的隐私过滤器，使累计隐私损失仅与每个文档被检索的频率相关；MURAG-ADA在此基础上私下释放与查询相关的阈值，以更精确地筛选相关文档，从而提升效用。

Result: 在多种LLM与数据集上实验，方法可扩展到数百次查询，在可观的隐私预算ε≈10下仍保留有意义的效用。

Conclusion: 将DP-RAG扩展到多查询场景，提供实用的隐私预算和改进的效用，支持在包含敏感信息的外部资源上的RAG系统的安全部署。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving documents from an external corpus at inference time. When this corpus contains sensitive information, however, unprotected RAG systems are at risk of leaking private information. Prior work has introduced differential privacy (DP) guarantees for RAG, but only in single-query settings, which fall short of realistic usage. In this paper, we study the more practical multi-query setting and propose two DP-RAG algorithms. The first, MURAG, leverages an individual privacy filter so that the accumulated privacy loss only depends on how frequently each document is retrieved rather than the total number of queries. The second, MURAG-ADA, further improves utility by privately releasing query-specific thresholds, enabling more precise selection of relevant documents. Our experiments across multiple LLMs and datasets demonstrate that the proposed methods scale to hundreds of queries within a practical DP budget ($\varepsilon\approx10$), while preserving meaningful utility.

</details>


### [66] [Enhancing Binary Encoded Crime Linkage Analysis Using Siamese Network](https://arxiv.org/abs/2511.07651)
*Yicheng Zhan,Fahim Ahmed,Amy Burrell,Matthew J. Tonkin,Sarah Galambos,Jessica Woodhams,Dalal Alrajeh*

Main category: cs.LG

TL;DR: Siamese Autoencoder-based crime linkage using ViCLAS; decoder-integrated geographic-temporal features mitigate sparse data effects; yields up to 9% AUC improvement and insights on preprocessing.


<details>
  <summary>Details</summary>
Motivation: Traditional crime linkage methods struggle with high-dimensional, sparse, and heterogeneous data; a robust latent representation and domain-informed preprocessing are needed to improve accuracy and interpretability.

Method: A Siamese Autoencoder framework learns latent representations; geographic-temporal features are incorporated at the decoder stage to amplify behavioral signals; trained and evaluated on ViCLAS data from the UK National Crime Agency; systematic exploration of data reduction strategies.

Result: Consistent improvements across evaluation metrics; up to 9% AUC improvement over traditional methods; practical guidance on preprocessing and interpretable insights for investigation.

Conclusion: ML-based crime linkage can substantially improve linkage accuracy and provide actionable, interpretable insights; the proposed architecture offers a practical approach to handle sparse, high-dimensional crime data.

Abstract: Effective crime linkage analysis is crucial for identifying serial offenders and enhancing public safety. To address limitations of traditional crime linkage methods in handling high-dimensional, sparse, and heterogeneous data, we propose a Siamese Autoencoder framework that learns meaningful latent representations and uncovers correlations in complex crime data. Using data from the Violent Crime Linkage Analysis System (ViCLAS), maintained by the Serious Crime Analysis Section of the UK's National Crime Agency, our approach mitigates signal dilution in sparse feature spaces by integrating geographic-temporal features at the decoder stage. This design amplifies behavioral representations rather than allowing them to be overshadowed at the input level, yielding consistent improvements across multiple evaluation metrics. We further analyze how different domain-informed data reduction strategies influence model performance, providing practical guidance for preprocessing in crime linkage contexts. Our results show that advanced machine learning approaches can substantially enhance linkage accuracy, improving AUC by up to 9% over traditional methods while offering interpretable insights to support investigative decision-making.

</details>


### [67] [ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings](https://arxiv.org/abs/2511.07658)
*Xiaomeng Yang,Jian Gao,Yanzhi Wang,Xuan Zhang*

Main category: cs.LG

TL;DR: ZeroSim 是一种基于变换器的性能建模框架，能在训练拓扑集合上实现对新参数配置的稳健分布内泛化与对未见拓扑的零样本泛化，且在参数优化中实现显著加速（约13×）相对于 SPICE。


<details>
  <summary>Details</summary>
Motivation: 解决传统 SPICE 仿真耗时与现有 ML 方法需要针对特定拓扑重新训练或手动子结构分割的问题，提升放大器设计的可扩展性与效率。

Method: 使用变换器为核心的性能建模框架；通过多达360万条实例、覆盖60+拓扑的多样化训练语料；引入统一的拓扑嵌入，结合全局感知令牌和分层注意力以泛化到新电路；提出拓扑条件的参数映射，保持结构表示在参数变动下的一致性。

Result: 在零样本泛化和跨拓扑预测方面显著优于MLP、GNN和纯Transformer等基线；在基于强化学习的参数优化中对比SPICE实现约13倍加速。

Conclusion: ZeroSim 展示了在不对新拓扑进行微调的情况下实现跨拓扑的鲁棒泛化的潜力，并能显著提升模拟-设计循环的效率，为模拟驱动的放大器设计自动化任务提供实用价值。

Abstract: Although recent advancements in learning-based analog circuit design automation have tackled tasks such as topology generation, device sizing, and layout synthesis, efficient performance evaluation remains a major bottleneck. Traditional SPICE simulations are time-consuming, while existing machine learning methods often require topology-specific retraining or manual substructure segmentation for fine-tuning, hindering scalability and adaptability. In this work, we propose ZeroSim, a transformer-based performance modeling framework designed to achieve robust in-distribution generalization across trained topologies under novel parameter configurations and zero-shot generalization to unseen topologies without any fine-tuning. We apply three key enabling strategies: (1) a diverse training corpus of 3.6 million instances covering over 60 amplifier topologies, (2) unified topology embeddings leveraging global-aware tokens and hierarchical attention to robustly generalize to novel circuits, and (3) a topology-conditioned parameter mapping approach that maintains consistent structural representations independent of parameter variations. Our experimental results demonstrate that ZeroSim significantly outperforms baseline models such as multilayer perceptrons, graph neural networks and transformers, delivering accurate zero-shot predictions across different amplifier topologies. Additionally, when integrated into a reinforcement learning-based parameter optimization pipeline, ZeroSim achieves a remarkable speedup (13x) compared to conventional SPICE simulations, underscoring its practical value for a wide range of analog circuit design automation tasks.

</details>


### [68] [Probabilities Are All You Need: A Probability-Only Approach to Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2511.07694)
*Manh Nguyen,Sunil Gupta,Hung Le*

Main category: cs.LG

TL;DR: 提出一种训练-free的不确定性估计方法，通过响应的前K概率近似预测熵，并通过自适应K选择过滤低置信度概率，在三个自由形式问答数据集上对比多家LLM，优于昂贵的最先进基线。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多任务中展现出强大能力，但幻觉现象仍然普遍，因此需要有效的不确定性估计来提升可信度。现有方法常需多次采样或额外计算以估计语义熵，成本高且不易部署。本工作旨在提出一种训练无关、计算高效的预测熵近似方法以提升鲁棒性。

Method: 利用回答的顶K概率分布近似预测熵，并设计自适应机制动态确定K值，以提高灵活性、过滤低置信度概率且无需额外训练或推理开销。

Result: 在三个自由形式问答数据集、跨多种LLM的实验中，该方法在性能上优于成本更高的最先进基线。

Conclusion: 该方法为提升LLM可信度提供了一种高效、训练-free的工具，可降低计算成本并有望广泛提升LLM的可靠性与可用性。

Abstract: Large Language Models (LLMs) exhibit strong performance across various natural language processing (NLP) tasks but remain vulnerable to hallucinations, generating factually incorrect or misleading outputs. Uncertainty estimation, often using predictive entropy estimation, is key to addressing this issue. However, existing methods often require multiple samples or extra computation to assess semantic entropy. This paper proposes an efficient, training-free uncertainty estimation method that approximates predictive entropy using the responses' top-$K$ probabilities. Moreover, we employ an adaptive mechanism to determine $K$ to enhance flexibility and filter out low-confidence probabilities. Experimental results on three free-form question-answering datasets across several LLMs demonstrate that our method outperforms expensive state-of-the-art baselines, contributing to the broader goal of enhancing LLM trustworthiness.

</details>


### [69] [On the Role of Calibration in Benchmarking Algorithmic Fairness for Skin Cancer Detection](https://arxiv.org/abs/2511.07700)
*Brandon Dominique,Prudence Lam,Nicholas Kurtansky,Jochen Weber,Kivanc Kose,Veronica Rotemberg,Jennifer Dy*

Main category: cs.LG

TL;DR:  Calibration-based auditing reveals that melanoma-detection AI models, while improving discrimination, often have poor calibration and over-diagnose risk across subgroups defined by sex, Fitzpatrick skin tone, and age; highlighting the need for comprehensive auditing and metadata for equitable AI in dermatology.


<details>
  <summary>Details</summary>
Motivation:  Address the limitation of AUROC-focused fairness evaluations by incorporating calibration to better reflect the clinical utility and subgroup equity of melanoma-detection models.

Method:  Evaluate the top ISIC 2020 Challenge melanoma detection algorithm on the ISIC 2020 and PROVE-AI datasets, comparing it with the 2nd and 3rd place models. Subgroups are defined by sex, race (Fitzpatrick Skin Tone), and age. Use calibration-oriented metrics alongside AUROC-based fairness metrics to assess subgroup performance.

Result:  Discriminative accuracy improves across models, but calibration issues emerge on new datasets; models tend to over-diagnose risk and exhibit miscalibration in subgroups.

Conclusion:  Emphasizes the need for comprehensive model auditing strategies and extensive metadata collection to achieve equitable AI-driven dermatological care; code for the study is publicly available.

Abstract: Artificial Intelligence (AI) models have demonstrated expert-level performance in melanoma detection, yet their clinical adoption is hindered by performance disparities across demographic subgroups such as gender, race, and age. Previous efforts to benchmark the performance of AI models have primarily focused on assessing model performance using group fairness metrics that rely on the Area Under the Receiver Operating Characteristic curve (AUROC), which does not provide insights into a model's ability to provide accurate estimates. In line with clinical assessments, this paper addresses this gap by incorporating calibration as a complementary benchmarking metric to AUROC-based fairness metrics. Calibration evaluates the alignment between predicted probabilities and observed event rates, offering deeper insights into subgroup biases. We assess the performance of the leading skin cancer detection algorithm of the ISIC 2020 Challenge on the ISIC 2020 Challenge dataset and the PROVE-AI dataset, and compare it with the second and third place models, focusing on subgroups defined by sex, race (Fitzpatrick Skin Tone), and age. Our findings reveal that while existing models enhance discriminative accuracy, they often over-diagnose risk and exhibit calibration issues when applied to new datasets. This study underscores the necessity for comprehensive model auditing strategies and extensive metadata collection to achieve equitable AI-driven healthcare solutions. All code is publicly available at https://github.com/bdominique/testing_strong_calibration.

</details>


### [70] [A Ranking-Based Optimization Algorithm for the Vehicle Relocation Problem in Car Sharing Services](https://arxiv.org/abs/2511.07724)
*Piotr Szwed,Paweł Skrzynski,Jarosław Wąs*

Main category: cs.LG

TL;DR: 提出一种基于区域划分和快速排序的车辆再调度方法，用于自由浮动车共享，结合区域车辆数量、需求密度的预测以及预计行程时长进行决策；在波兰某运营商真实数据上，与无优化基线及精确MIP解的对比显示显著改善，但MIP包含当前业务规则不允许的行程选择。


<details>
  <summary>Details</summary>
Motivation: 解决自由浮动车共享中的车辆再定位问题，降低总行驶时间、提升资源利用效率；通过区域化划分和快速启发式排序实现可扩展的离散优化调度。

Method: 提出两阶段方法：第一阶段将服务区域划分成若干区域以捕捉在车模式和需求的时空特征，从而应用离散优化；第二阶段给出基于区域在车数量、需求密度的预测分布以及预计行程时长的快速排序算法用于决策；在真实数据上与无优化基线以及求解MIP的精确解进行比较。

Result: 在相同的车辆、员工和需求分布条件下，算法相对基线的平均改进为8.44%，而MIP求解的改进为19.6%；MIP还包含了当前服务规则禁止的“行程选择”决策。结果还显示， workforce规模对性能提升有显著影响，提升范围约3–10%。

Conclusion: 区域化+排序的再调度方法可实现在降低总行驶时间方面的有效性和可扩展性；相较MIP，虽潜在收益更高但需遵循实际业务约束。

Abstract: The paper addresses the Vehicle Relocation Problem in free-floating car-sharing services by presenting a solution focused on strategies for repositioning vehicles and transferring personnel with the use of scooters. Our method begins by dividing the service area into zones that group regions with similar temporal patterns of vehicle presence and service demand, allowing the application of discrete optimization methods. In the next stage, we propose a fast ranking-based algorithm that makes its decisions on the basis of the number of cars available in each zone, the projected probability density of demand, and estimated trip durations. The experiments were carried out on the basis of real-world data originating from a major car-sharing service operator in Poland. The results of this algorithm are evaluated against scenarios without optimization that constitute a baseline and compared with the results of an exact algorithm to solve the Mixed Integer Programming (MIP) model. As performance metrics, the total travel time was used. Under identical conditions (number of vehicles, staff, and demand distribution), the average improvements with respect to the baseline of our algorithm and MIP solver were equal to 8.44\% and 19.6\% correspondingly. However, it should be noted that the MIP model also mimicked decisions on trip selection, which are excluded by current services business rules. The analysis of results suggests that, depending on the size of the workforce, the application of the proposed solution allows for improving performance metrics by roughly 3%-10%.

</details>


### [71] [Algorithm-Relative Trajectory Valuation in Policy Gradient Control](https://arxiv.org/abs/2511.07878)
*Shihao Li,Jiachen Li,Jiamin Xu,Christopher Martin,Wei Li,Dongmei Chen*

Main category: cs.LG

TL;DR: 轨迹价值是算法相关的，未稳定化时PE与边际价值呈负相关，经过状态 whitening/Fisher 预条件化后相关性转为正，方差在不同设置下驱动效应，信息内容才是主导。


<details>
  <summary>Details</summary>
Motivation: 理解策略梯度控制中轨迹价值对学习算法的敏感性，以及方差-信息之间的竞争关系。

Method: 在不确定的LQR中引入 Trajectory Shapley，分析 PE 与梯度方差的关系及其对边际贡献的影响；提出并验证一个方差中介机制；通过状态 whitening 和 Fisher 预条件化进行对照；结合 Leave-One-Out 与 Shapley 的剪枝分析。

Result: 未稳定化REINFORCE下 PE 与边际价值的相关性约为 -0.38；稳定化后相关性约为 +0.29；提出方差中介机制：固定能量时高 PE 降低梯度方差；接近鞍点时高方差提升逃逸概率并提高边际贡献；Shapley 识别有害子集，LOO 与 Shapley 互为补充用于剪枝。

Conclusion: 轨迹价值受算法影响显著，方差与信息内容之间的竞争决定在不同学习设置下的价值分布；Shapley 为子集分析提供有效工具，LOO 提供额外的剪枝信息。

Abstract: We study how trajectory value depends on the learning algorithm in policy-gradient control. Using Trajectory Shapley in an uncertain LQR, we find a negative correlation between Persistence of Excitation (PE) and marginal value under vanilla REINFORCE ($r\approx-0.38$). We prove a variance-mediated mechanism: (i) for fixed energy, higher PE yields lower gradient variance; (ii) near saddles, higher variance increases escape probability, raising marginal contribution. When stabilized (state whitening or Fisher preconditioning), this variance channel is neutralized and information content dominates, flipping the correlation positive ($r\approx+0.29$). Hence, trajectory value is algorithm-relative. Experiments validate the mechanism and show decision-aligned scores (Leave-One-Out) complement Shapley for pruning, while Shapley identifies toxic subsets.

</details>


### [72] [Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning](https://arxiv.org/abs/2511.07730)
*Bill Chunyuan Zheng,Vivek Myers,Benjamin Eysenbach,Sergey Levine*

Main category: cs.LG

TL;DR: 将 TD 与多步蒙特卡洛回报结合，学习一个准距离（准度量）以实现长时目标推理的无监督/离线 GCRL，能够在长时任务和现实世界机器人中实现多步拼接。


<details>
  <summary>Details</summary>
Motivation: 长距离目标导向推理的核心是观测对之间的时间距离估计。时序差分方法虽有局部更新带来的理论保障，但对长时依赖的性能通常不如全局更新的蒙特卡洛多步回报；两者之间的结合能同时获得稳定性与全局信息。

Method: 提出一个端到端的GCRL框架，通过多步蒙特卡洛回报拟合一个准度量距离，并结合近似的时距学习，实现对观测点之间时间距离的估计与多步拼接（stitching）的能力。该方法在无标签离线视觉数据上训练，具备端到端特性。

Result: 在最长可达4000步的长时任务上优于现有的GCRL方法，且对视觉观测也有效；在Bridge现实世界机器人操作中实现多步拼接，首次在无标注离线视觉数据上实现端到端GCRL的多步拼接。

Conclusion: 证明了端到端GCRL结合多步回报的可行性与实际效用，提升长时推理能力并具备现实世界机器人应用潜力。

Abstract: Learning how to reach goals in an environment is a longstanding challenge in AI, yet reasoning over long horizons remains a challenge for modern methods. The key question is how to estimate the temporal distance between pairs of observations. While temporal difference methods leverage local updates to provide optimality guarantees, they often perform worse than Monte Carlo methods that perform global updates (e.g., with multi-step returns), which lack such guarantees. We show how these approaches can be integrated into a practical GCRL method that fits a quasimetric distance using a multistep Monte-Carlo return. We show our method outperforms existing GCRL methods on long-horizon simulated tasks with up to 4000 steps, even with visual observations. We also demonstrate that our method can enable stitching in the real-world robotic manipulation domain (Bridge setup). Our approach is the first end-to-end GCRL method that enables multistep stitching in this real-world manipulation domain from an unlabeled offline dataset of visual observations.

</details>


### [73] [Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction](https://arxiv.org/abs/2511.07899)
*Ihab Tabbara,Yuxuan Yang,Hussein Sibai*

Main category: cs.LG

TL;DR: 提出使用保守的 conformal prediction 框架来提供学习的 HJ 值函数和策略的概率性安全保证，并比较单一值函数与值函数集合作为安全滤波器，同时研究在失效集合前的策略切换。


<details>
  <summary>Details</summary>
Motivation: 解决高维系统中 Hamilton-Jacobi 值函数的计算成本与学习方法的不确定性之间的矛盾，旨在为学习驱动的安全控制提供可证的安全保证。

Method: 将 conformal prediction 集成到学习的 HJ 值函数和策略的安全评估中，标定在 unsafe nominal 控制器与学习的 HJ 安全策略之间切换的条件，并给出在该切换下的安全保证。还探讨使用独立训练的 HJ 值函数组成的集合作为安全滤波器，并与单值函数进行对比。

Result: 提出的 CP 框架能够为给定状态下的安全性提供概率区间，并在切换策略下给出安全保证；使用值函数集合作为滤波器有望提升鲁棒性，相比单一值函数可能具备更好的抗误差性，但需要通过实验验证以量化收益。

Conclusion: 将 CP 与 HJ 及学习方法相结合，提升学习驱动的安全控制的可证性和鲁棒性；集合安全滤波器有潜在优势，但需权衡计算开销和一致性挑战。

Abstract: Safety assurance is a fundamental requirement for deploying learning-enabled autonomous systems. Hamilton-Jacobi (HJ) reachability analysis is a fundamental method for formally verifying safety and generating safe controllers. However, computing the HJ value function that characterizes the backward reachable set (BRS) of a set of user-defined failure states is computationally expensive, especially for high-dimensional systems, motivating the use of reinforcement learning approaches to approximate the value function. Unfortunately, a learned value function and its corresponding safe policy are not guaranteed to be correct. The learned value function evaluated at a given state may not be equal to the actual safety return achieved by following the learned safe policy. To address this challenge, we introduce a conformal prediction-based (CP) framework that bounds such uncertainty. We leverage CP to provide probabilistic safety guarantees when using learned HJ value functions and policies to prevent control systems from reaching failure states. Specifically, we use CP to calibrate the switching between the unsafe nominal controller and the learned HJ-based safe policy and to derive safety guarantees under this switched policy. We also investigate using an ensemble of independently trained HJ value functions as a safety filter and compare this ensemble approach to using individual value functions alone.

</details>


### [74] [From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training](https://arxiv.org/abs/2511.07738)
*Donglai Xu,Hongzheng Yang,Yuzhi Zhao,Pingping Zhang,Jinpeng Chen,Wenao Ma,Zhijian Hou,Mengyang Wu,Xiaolei Li,Senkang Hu,Ziyi Guan,Jason Chun Lok Li,Lai Man Po*

Main category: cs.LG

TL;DR: 提出一个两阶段的令牌级熵优化策略用于强化学习可验证奖励（RLVR）的多模态大语言模型（MLLM）。通过在训练初期进行熵最大化实现探索、后期进行熵最小化实现开发，提升对噪声标注的鲁棒性并改善奖励信号的估计，且在三种后端模型（Qwen2-VL-2B、Qwen2-VL-7B、Qwen2.5-VL-3B）上实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在RLVR框架中，优质标注数据稀缺且容易受到噪声干扰，现有无监督RLVR方法（如纯熵最小化）易对错误标签过拟合，削弱组内策略优化（GRPO）的奖励排序信号。因此需要一种能提高噪声容忍度且稳定学习的训练策略。

Method: 提出两阶段、令牌级熵优化的RLVR方法：训练初期执行熵最大化以促进输出的多样性与随机性，作为对噪声标签的正则化并提升在GRPO中的奖励梯度估计；训练后期转为熵最小化以获得更自信的输出、巩固知识并提升预测准确性。该策略可在GRPO框架中结合外部、内部及基于熵的方法，并在三种Qwen变体上验证。

Result: 在Qwen2-VL-2B、Qwen2-VL-7B、Qwen2.5-VL-3B这三种后端模型及多种噪声设置和任务上，所提出的阶段性熵优化策略普遍优于先前的方法，展现出更稳健的性能提升。

Conclusion: 阶段性、令牌级熵优化是提升MLLMs在RLVR场景下面对噪声标注时的鲁棒性与奖励信号利用效率的有效策略，且具备跨模型与多任务的泛化能力，能够统一并增强外部、内部及熵基方法的效果。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs) is highly dependent on high-quality labeled data, which is often scarce and prone to substantial annotation noise in real-world scenarios. Existing unsupervised RLVR methods, including pure entropy minimization, can overfit to incorrect labels and limit the crucial reward ranking signal for Group-Relative Policy Optimization (GRPO). To address these challenges and enhance noise tolerance, we propose a novel two-stage, token-level entropy optimization method for RLVR. This approach dynamically guides the model from exploration to exploitation during training. In the initial exploration phase, token-level entropy maximization promotes diverse and stochastic output generation, serving as a strong regularizer that prevents premature convergence to noisy labels and ensures sufficient intra-group variation, which enables more reliable reward gradient estimation in GRPO. As training progresses, the method transitions into the exploitation phase, where token-level entropy minimization encourages the model to produce confident and deterministic outputs, thereby consolidating acquired knowledge and refining prediction accuracy. Empirically, across three MLLM backbones - Qwen2-VL-2B, Qwen2-VL-7B, and Qwen2.5-VL-3B - spanning diverse noise settings and multiple tasks, our phased strategy consistently outperforms prior approaches by unifying and enhancing external, internal, and entropy-based methods, delivering robust and superior performance across the board.

</details>


### [75] [Schedulers for Schedule-free: Theoretically inspired hyperparameters](https://arxiv.org/abs/2511.07767)
*Yuen-Man Pun,Matthew Buchholz,Robert M. Gower*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The recently proposed schedule-free method has been shown to achieve strong performance when hyperparameter tuning is limited. The current theory for schedule-free only supports a constant learning rate, where-as the implementation used in practice uses a warm-up schedule. We show how to extend the last-iterate convergence theory of schedule-free to allow for any scheduler, and how the averaging parameter has to be updated as a function of the learning rate. We then perform experiments showing how our convergence theory has some predictive power with regards to practical executions on deep neural networks, despite that this theory relies on assuming convexity. When applied to the warmup-stable-decay (wsd) schedule, our theory shows the optimal convergence rate of $\mathcal{O}(1/\sqrt{T})$. We then use convexity to design a new adaptive Polyak learning rate schedule for schedule-free. We prove an optimal anytime last-iterate convergence for our new Polyak schedule, and show that it performs well compared to a number of baselines on a black-box model distillation task.

</details>


### [76] [HardFlow: Hard-Constrained Sampling for Flow-Matching Models via Trajectory Optimization](https://arxiv.org/abs/2511.08425)
*Zeyang Li,Kaveh Alim,Navid Azizan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion and flow-matching have emerged as powerful methodologies for generative modeling, with remarkable success in capturing complex data distributions and enabling flexible guidance at inference time. Many downstream applications, however, demand enforcing hard constraints on generated samples (for example, robot trajectories must avoid obstacles), a requirement that goes beyond simple guidance. Prevailing projection-based approaches constrain the entire sampling path to the constraint manifold, which is overly restrictive and degrades sample quality. In this paper, we introduce a novel framework that reformulates hard-constrained sampling as a trajectory optimization problem. Our key insight is to leverage numerical optimal control to steer the sampling trajectory so that constraints are satisfied precisely at the terminal time. By exploiting the underlying structure of flow-matching models and adopting techniques from model predictive control, we transform this otherwise complex constrained optimization problem into a tractable surrogate that can be solved efficiently and effectively. Furthermore, this trajectory optimization perspective offers significant flexibility beyond mere constraint satisfaction, allowing for the inclusion of integral costs to minimize distribution shift and terminal objectives to further enhance sample quality, all within a unified framework. We provide a control-theoretic analysis of our method, establishing bounds on the approximation error between our tractable surrogate and the ideal formulation. Extensive experiments across diverse domains, including robotics (planning), partial differential equations (boundary control), and vision (text-guided image editing), demonstrate that our algorithm, which we name $\textit{HardFlow}$, substantially outperforms existing methods in both constraint satisfaction and sample quality.

</details>


### [77] [Physical Consistency of Aurora's Encoder: A Quantitative Study](https://arxiv.org/abs/2511.07787)
*Benjamin Richards,Pushpa Kumar Balan*

Main category: cs.LG

TL;DR: Aurora 的潜在嵌入体现物理一致性，线性探针可识别地貌边界、极端温度事件和大气不稳定性；对罕见事件的捕捉能力有限，强调可解释性的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模天气预报模型的“黑箱”问题，验证其嵌入是否与已知物理和气象概念一致，以提升可解释性和信任。

Method: 使用Aurora编码器的海量嵌入数据，训练线性分类器（线性探针）来识别三个概念：陆-海边界、高影响极端温度事件、以及大气不稳定性。

Result: 实证表明模型学习到物理一致的特征；在某些方面不完全捕捉稀有事件，存在局限性。

Conclusion: 可解释性方法对验证并建立对下一代AI驱动天气模型的信任至关重要；应进一步发展探针和解释性工具。

Abstract: The high accuracy of large-scale weather forecasting models like Aurora is often accompanied by a lack of transparency, as their internal representations remain largely opaque. This "black box" nature hinders their adoption in high-stakes operational settings. In this work, we probe the physical consistency of Aurora's encoder by investigating whether its latent representations align with known physical and meteorological concepts. Using a large-scale dataset of embeddings, we train linear classifiers to identify three distinct concepts: the fundamental land-sea boundary, high-impact extreme temperature events, and atmospheric instability. Our findings provide quantitative evidence that Aurora learns physically consistent features, while also highlighting its limitations in capturing the rarest events. This work underscores the critical need for interpretability methods to validate and build trust in the next generation of Al-driven weather models.

</details>


### [78] [Analyzing Political Text at Scale with Online Tensor LDA](https://arxiv.org/abs/2511.07809)
*Sara Kangaslahti,Danny Ebanks,Jean Kossaifi,Anqi Liu,R. Michael Alvarez,Animashree Anandkumar*

Main category: cs.LG

TL;DR: A scalable topic modeling method TLDA with linear scalability to billions of documents, offering identifiability and sample complexity guarantees, GPU-based open-source implementation, and enabling large-scale social science analyses (e.g., #MeToo evolution and election fraud discussions) in near real-time.


<details>
  <summary>Details</summary>
Motivation: To enable rigorous, scalable analysis of extremely large text corpora for social science questions, addressing limitations of traditional LDA in terms of scalability, identifiability, and practical deployment.

Method: Tensor Latent Dirichlet Allocation (TLDA), a topic modeling approach with identifiable and recoverable parameters and theoretical sample complexity guarantees; a GPU-accelerated, memory-efficient implementation; designed for linear scaling with dataset size; parallelized to achieve substantial speedups over prior LDA methods.

Result: Demonstrates 3-4x faster runtimes than prior parallelized LDA methods; scales linearly to over a billion documents; provides an open-source GPU-based implementation; enables two large-scale real-world studies (MeToo evolution and election-fraud discussions) on social media.

Conclusion: TLDA provides researchers, especially social scientists, with a scalable, provably sound, and practically efficient tool to study very large text corpora in near real-time, unlocking analyses previously deemed computationally prohibitive.

Abstract: This paper proposes a topic modeling method that scales linearly to billions of documents. We make three core contributions: i) we present a topic modeling method, Tensor Latent Dirichlet Allocation (TLDA), that has identifiable and recoverable parameter guarantees and sample complexity guarantees for large data; ii) we show that this method is computationally and memory efficient (achieving speeds over 3-4x those of prior parallelized Latent Dirichlet Allocation (LDA) methods), and that it scales linearly to text datasets with over a billion documents; iii) we provide an open-source, GPU-based implementation, of this method. This scaling enables previously prohibitive analyses, and we perform two real-world, large-scale new studies of interest to political scientists: we provide the first thorough analysis of the evolution of the #MeToo movement through the lens of over two years of Twitter conversation and a detailed study of social media conversations about election fraud in the 2020 presidential election. Thus this method provides social scientists with the ability to study very large corpora at scale and to answer important theoretically-relevant questions about salient issues in near real-time.

</details>


### [79] [Multi-Objective Bilevel Learning](https://arxiv.org/abs/2511.07824)
*Zhiyao Zhang,Zhuqing Liu,Xin Zhang,Wen-Yen Chen,Jiyan Yang,Jia Liu*

Main category: cs.LG

TL;DR: 提出了多目标双层学习（MOBL）的统一框架WC-MHGD，能够在确定性和随机场景下实现有限时间的Pareto-停机收敛，并支持系统性Pareto前沿探索，同时具备较低的oracle复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用的日益复杂，需在不同层次的耦合决策变量下处理多目标且目标之间可能相互冲突的问题。当前MOBL研究尚不成熟，亟需理论和算法基础。

Method: 提出一个统一的算法框架 weighted-Chebyshev multi-hyper-gradient-descent (WC-MHGD)，可用于确定性与随机设置，具备有限时间的Pareto-停机收敛率保证，且能实现Pareto前沿的系统性探索。

Result: 理论方面给出有限时间的Pareto-stationarity收敛率，并指出低oracle复杂度；在实验上验证理论结果，展示框架在MOBL中的有效性与可扩展性。

Conclusion: WC-MHGD为MOBL问题提供了一个统一、高效的优化框架，既能实现目标间的折衷与停机准则，又能系统性地探索Pareto前沿，具有理论与实验上的双重保障。

Abstract: As machine learning (ML) applications grow increasingly complex in recent years, modern ML frameworks often need to address multiple potentially conflicting objectives with coupled decision variables across different layers. This creates a compelling need for multi-objective bilevel learning (MOBL). So far, however, the field of MOBL remains in its infancy and many important problems remain under-explored. This motivates us to fill this gap and systematically investigate the theoretical and algorithmic foundation of MOBL. Specifically, we consider MOBL problems with multiple conflicting objectives guided by preferences at the upper-level subproblem, where part of the inputs depend on the optimal solution of the lower-level subproblem. Our goal is to develop efficient MOBL optimization algorithms to (1) identify a preference-guided Pareto-stationary solution with low oracle complexity; and (2) enable systematic Pareto front exploration. To this end, we propose a unifying algorithmic framework called weighted-Chebyshev multi-hyper-gradient-descent (WC-MHGD) for both deterministic and stochastic settings with finite-time Pareto-stationarity convergence rate guarantees, which not only implies low oracle complexity but also induces systematic Pareto front exploration. We further conduct extensive experiments to confirm our theoretical results.

</details>


### [80] [MURPHY: Multi-Turn GRPO for Self Correcting Code Generation](https://arxiv.org/abs/2511.07833)
*Chanakya Ekbote,Vijay Lingam,Behrooz Omidvar-Tehrani,Jun Huan,Sujay Sanghavi,Anoop Deoras,Stefano Soatto*

Main category: cs.LG

TL;DR: Murphy extends GRPO with multi-turn reflective optimization, enabling iterative self-correction for better agentic reasoning in LLMs; achieves up to 8% relative gain in pass@1 on code generation benchmarks.


<details>
  <summary>Details</summary>
Motivation: GRPO and similar RLVR methods improve reasoning but struggle on agentic, multi-turn decision tasks. There is a need for a framework that supports iterative self-correction and feedback-driven refinement to enhance long-horizon reasoning.

Method: Murphy integrates iterative self-correction into training by extending GRPO with multi-turn reflective optimization. It leverages both quantitative execution feedback (e.g., pass rates, scores) and qualitative feedback to progressively refine reasoning across turns during training.

Result: On code generation benchmarks using model families such as Qwen and OLMo, Murphy consistently improves performance, achieving up to an 8% relative gain in pass@1 over GRPO while maintaining similar compute budgets.

Conclusion: Murphy demonstrates the value of multi-turn reflective optimization and iterative feedback in RLVR for agentic tasks, offering a stronger improvement over GRPO and suggesting broader applicability to complex reasoning tasks in LLMs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful framework for enhancing the reasoning capabilities of large language models (LLMs). However, existing approaches such as Group Relative Policy Optimization (GRPO) and its variants, while effective on reasoning benchmarks, struggle with agentic tasks that require iterative decision-making. We introduce Murphy, a multi-turn reflective optimization framework that extends GRPO by incorporating iterative self-correction during training. By leveraging both quantitative and qualitative execution feedback, Murphy enables models to progressively refine their reasoning across multiple turns. Evaluations on code generation benchmarks with model families such as Qwen and OLMo show that Murphy consistently improves performance, achieving up to a 8% relative gain in pass@1 over GRPO, on similar compute budgets.

</details>


### [81] [DP-AdamW: Investigating Decoupled Weight Decay and Bias Correction in Private Deep Learning](https://arxiv.org/abs/2511.07843)
*Jay Chooi,Kevin Cong,Russell Li,Lillian Sun*

Main category: cs.LG

TL;DR: 提出 DP-AdamW 及带 DP 偏置修正的 DP-AdamW-BC，在隐私与收敛性方面给出理论保证，并通过跨隐私预算的实验表明 DP-AdamW 优于现有的 DP 优化器，而 DP-AdamW-BC 在偏置修正下并不提升性能。


<details>
  <summary>Details</summary>
Motivation: 在深度学习广泛使用敏感数据的背景下，现有的 DP 优化器在性能上常常受限。AdamW 以其卓越的经验性能广受欢迎，本文旨在把差分隐私约束引入到 AdamW，同时研究带偏置修正的第二矩估计对性能的影响。

Method: 给出 DP-AdamW 和 DP-AdamW-BC 的隐私与收敛性理论分析；设计带 DP 偏置修正的第二矩估计的 AdamW 变体；在 ε=1、3、7 的多种隐私预算下对文本分类、图像分类、图节点分类等任务进行跨域评估。

Result: 与现有的 DP 优化器（如 DP-SGD、DP-Adam、DP-AdamBC）相比，DP-AdamW 的性能提升在文本分类中超过 15%，在图像分类中最高达 5%，在图节点分类中稳定达到 1%；引入偏置修正的 DP-AdamW-BC 在实验中始终降低精度（与 DP-AdamBC 对比的提升不存在或为负）。

Conclusion: DP-AdamW 显示出在差分隐私设定下的优越性，是一个有前景的 DP 优化器选择；而 DP-AdamW-BC 的偏置修正并未带来性能提升，甚至带来负效应，提示未来工作应聚焦于对尺度、梯度方差偏置的更有效处理或寻找其他在隐私约束下的优化改进。

Abstract: As deep learning methods increasingly utilize sensitive data on a widespread scale, differential privacy (DP) offers formal guarantees to protect against information leakage during model training. A significant challenge remains in implementing DP optimizers that retain strong performance while preserving privacy. Recent advances introduced ever more efficient optimizers, with AdamW being a popular choice for training deep learning models because of strong empirical performance. We study \emph{DP-AdamW} and introduce \emph{DP-AdamW-BC}, a differentially private variant of the AdamW optimizer with DP bias correction for the second moment estimator. We start by showing theoretical results for privacy and convergence guarantees of DP-AdamW and DP-AdamW-BC. Then, we empirically analyze the behavior of both optimizers across multiple privacy budgets ($ε= 1, 3, 7$). We find that DP-AdamW outperforms existing state-of-the-art differentially private optimizers like DP-SGD, DP-Adam, and DP-AdamBC, scoring over 15\% higher on text classification, up to 5\% higher on image classification, and consistently 1\% higher on graph node classification. Moreover, we empirically show that incorporating bias correction in DP-AdamW (DP-AdamW-BC) consistently decreases accuracy, in contrast to the improvement of DP-AdamBC improvement over DP-Adam.

</details>


### [82] [A Generalized Spectral Framework to Expain Neural Scaling and Compression Dynamics](https://arxiv.org/abs/2511.07892)
*Yizhou Zhang*

Main category: cs.LG

TL;DR: 提出一个广义谱框架，通过一个渐近多项式谱演化函数 g(λ,t;β) 来统一学习动力学与压缩现象，参数 β 控制光谱-时间弹性 ρ(β)，可将现有 lazy 与 feature-learning 理论作为特例，并推出学习与压缩的不变量关系。


<details>
  <summary>Details</summary>
Motivation: 现有经验缩放定律在不同场域（如模型压缩）表现出不同的行为，但缺乏统一解释。借助谱分析的进展，期望把学习动力学与压缩现象放到同一框架中，用一个通用的谱演化函数描述其动力学。

Method: 将谱演化函数从线性核形式 g(λt)=λt 扩展为渐近多项式形式 g(λ,t;β)，并引入有效谱-时间弹性 ρ(β)。通过该函数在不同 β 下的行为，展示如何退化回已知的 lazy（延迟学习）与 feature-learning（特征学习）理论，及证明学习与压缩之间存在不变量关系。

Result: 得到一个一般化谱框架，其核心是 g(λ,t;β) 和 ρ(β)，在特定极限下重现已有理论；提出学习与压缩之间的守恒/不变量关系，暗示谱特性决定了两者的耦合。

Conclusion: 该框架提供一个统一视角解释经验缩放律在学习与压缩之间的一致性，未来可用于预测不同设置下的学习曲线，并为设计更高效的模型与压缩策略提供理论支撑。

Abstract: Empirical scaling laws describe how test loss and other performance metrics depend on model size, dataset size, and compute. While such laws are consistent within specific regimes, apparently distinct scaling behaviors have been reported for related settings such as model compression. Motivated by recent progress in spectral analyses of neural representations, this paper develops a \emph{generalized spectral framework} that unifies learning dynamics and compression phenomena under a common functional ansatz. We generalize the spectral evolution function from the linear kernel form $g(λt)=λt$ to an asymptotically polynomial function $g(λ,t;β)$, characterized by an effective spectral--temporal elasticity $ρ(β)$. This framework recovers existing lazy and feature-learning theories as special cases and yields an invariant relation between learning and compression

</details>


### [83] [Test-driven Reinforcement Learning](https://arxiv.org/abs/2511.07904)
*Zhao Yu,Xiuping Wu,Liangjun Ke*

Main category: cs.LG

TL;DR: 提出一种基于测试驱动的强化学习TdRL框架，使用多测试函数来定义任务目标并引导学习，以替代单一手工设计的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 奖励函数往往同时定义目标与学习过程，设计困难且可能导致子优化的任务表示。

Method: 引入两类测试函数（通过准入/通过-失败测试和指示性测试）来分别定义最优目标与指导学习；证明若轨迹回报函数使更接近最优轨迹集的轨迹获得更高回报，则最大熵策略优化能产生更接近最优策略的结果；利用词序层次的启发式（lexicographic）比较轨迹与最优集合的距离以学习轨迹回报函数；实现TdRL算法，并在DeepMind Control Suite上评估。

Result: 在DeepMind Control Suite上，TdRL与手工奖励方法在策略训练上表现相当或更好，且设计更简洁，天然支持多目标优化。

Conclusion: TdRL提供了表示任务目标的新视角，有助于缓解RL中的奖励设计难题，并具备扩展到多目标等应用的潜力。

Abstract: Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.

</details>


### [84] [Rectified Noise: A Generative Model Using Positive-incentive Noise](https://arxiv.org/abs/2511.07911)
*Zhenyu Gu,Yanchen Xu,Sida Huang,Yubin Guo,Hongyuan Zhang*

Main category: cs.LG

TL;DR: 提出 Rectified Noise（ΔRN）以改进 Rectified Flow（RF），通过将 π-noise 注入到 RF 的速度场来训练 π-noise 生成器；可将预训练 RF 转换为 π-noise 生成器，显著提升生成性能（如 ImageNet-1k FID 从 10.16 降至 9.05；仅增加约0.39% 的额外参数）


<details>
  <summary>Details</summary>
Motivation: 解决 RF 在采样阶段的性能受限问题，结合 π-noise 的正向激励特性，通过在速度场中注入噪声来提升表达能力和采样质量；希望在不大量修改现有 RF 架构的前提下提升性能。

Method: 训练 π-noise 生成器，将 π-noise 注入到预训练 RF 的速度场，提出 Rectified Noise 流程，使得 RF 可高效转换成 π-noise 生成器；在多种模型和数据集上进行广泛实验。

Result: 在实验中，RF + Rectified Noise 显著提升性能：如 ImageNet-1k 的 FID 从 10.16 提升到 9.05，同时 π-noise 生成器仅增加约 0.39% 的训练参数；在多种模型架构和数据集上验证有效性。

Conclusion: Rectified Noise 提供一种简单而有效的方式来提升 RF 的生成性能，且对已有 RF 的改动极小，具备良好的可扩展性和实用性；未来可进一步探索更广泛的 π-noise 形式及其在其他概率流模型中的应用。

Abstract: Rectified Flow (RF) has been widely used as an effective generative model. Although RF is primarily based on probability flow Ordinary Differential Equations (ODE), recent studies have shown that injecting noise through reverse-time Stochastic Differential Equations (SDE) for sampling can achieve superior generative performance. Inspired by Positive-incentive Noise ($π$-noise), we propose an innovative generative algorithm to train $π$-noise generators, namely Rectified Noise ($Δ$RN), which improves the generative performance by injecting $π$-noise into the velocity field of pre-trained RF models. After introducing the Rectified Noise pipeline, pre-trained RF models can be efficiently transformed into $π$-noise generators. We validate Rectified Noise by conducting extensive experiments across various model architectures on different datasets. Notably, we find that: (1) RF models using Rectified Noise reduce FID from \textbf{10.16 to 9.05} on ImageNet-1k. (2) The models of $π$-noise generators achieve improved performance with only \textbf{0.39\%} additional training parameters.

</details>


### [85] [Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison](https://arxiv.org/abs/2511.07919)
*Yoonho Lee,Joseph Boen,Chelsea Finn*

Main category: cs.LG

TL;DR: 提出 Feedback Descent，通过结构化文本反馈在文本空间进行优化，而非仅依赖二元偏好；支持对提示、代码和分子进行无权重更新的推理时间迭代，在多领域实现超越现有方法的性能，且在 DOCKSTRING 的药物分子发现任务中达到极高阈值。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习往往将复杂评估压缩为二元偏好，导致信息损失；通过保留详细评述，能在文本空间提供更高带宽且导向性更强的梯度信号，提升对文本结构的有效优化。

Method: 提出一个框架 Feedback Descent，利用结构化文本反馈来指导文本产出（提示、代码、分子设计）的迭代优化；在上下文中将结构化反馈转化为梯度样信息，利用模型的在-context学习能力实现方向性编辑；迭代在推理时间完成，模型权重不变，且适用于不同任务域；对三个领域进行评估，与 GEPA、GRPO、REINVENT 等方法比较。

Result: 在多领域实验中，Feedback Descent 超越了现有的提示优化、强化学习与分子优化方法；在 DOCKSTRING 药物分子发现基准上，找到了新颖的、药物性分子的分布，超过包含超26万分子的数据库的 99.9 百分位，覆盖六个蛋白靶点。

Conclusion: 文本级结构化反馈作为高带宽监督，结合在-context 学习实现梯度样信息，可在不修改模型权重的情况下进行高效、领域无关的迭代优化，适用于提示、代码与分子等多种文本产出任务。

Abstract: We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.

</details>


### [86] [SERL: Self-Examining Reinforcement Learning on Open-Domain](https://arxiv.org/abs/2511.07922)
*Weixuan Ou,Yanzhao Zheng,Shuoshuo Sun,Wei Zhang,Baohua Dong,Hangcheng Zhu,Ruohui Huang,Gang Yu,Pengwei Yan,Yifan Qiao*

Main category: cs.LG

TL;DR: Self-Examining Reinforcement Learning (SERL) enables LLM self-improvement without external rewards by using Copeland-style pairwise judgments as actor rewards and a self-consistency reward for the Judge, improving performance on open-domain tasks. It achieves state-of-the-art results among self-improvement methods and is competitive with larger models.


<details>
  <summary>Details</summary>
Motivation: RL for open-domain tasks faces subjectivity in rewards and reliance on external reward mechanisms. A self-contained, self-improving framework could provide robust, scalable improvement for LLMs without external supervision.

Method: LLM serves as both Actor and Judge. The Actor receives rewards derived from Copeland-style pairwise comparison judgments across multiple generated responses. The Judge uses a self-consistency reward to improve its reliability, leading to better judgments that in turn provide a more robust reward signal for the Actor.

Result: SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%; it achieves state-of-the-art performance among self-improving approaches and is competitive with larger models like Qwen3-32B.

Conclusion: SERL demonstrates effective self-improvement for LLMs without external rewards, offering robust performance gains on open-domain tasks and suggesting a promising direction for scalable, self-contained RL for LLMs.

Abstract: Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor's capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge's reliability. This process refines the Judge's capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.

</details>


### [87] [IBMA: An Imputation-Based Mixup Augmentation Using Self-Supervised Learning for Time Series Data](https://arxiv.org/abs/2511.07930)
*Dang Nha Nguyen,Hai Dang Nguyen,Khoa Tho Anh Nguyen*

Main category: cs.LG

TL;DR: 提出了一种基于插补的混合增广IBMA，用于时间序列预测，结合插补增强和Mixup以提升泛化与预测性能，在多模型和多数据集上表现出显著提升。


<details>
  <summary>Details</summary>
Motivation: 相较于图像/文本领域，时间序列的增广策略受限，现有Mixup等高级方法使用有限；需要更有效的增广来提升时间序列预测的泛化能力。

Method: 设计IBMA，将插补增强的数据与Mixup相结合，对DLinear、TimesNet、iTrainformer等模型进行评估，数据集包含ETTh1/ETTh2/ETTm1/ETTm2，比较8种增广技术。

Result: IBMA在四个数据集上的24个对比实验中，22次获得改进，并有其中10次为最佳，尤其在iTrainformer插补设置下表现突出。

Conclusion: IBMA是一种有效的时间序列增广方法，能在多模型与数据集上提升预测性能与泛化能力。

Abstract: Data augmentation in time series forecasting plays a crucial role in enhancing model performance by introducing variability while maintaining the underlying temporal patterns. However, time series data offers fewer augmentation strategies compared to fields such as image or text, with advanced techniques like Mixup rarely being used. In this work, we propose a novel approach, Imputation-Based Mixup Augmentation (IBMA), which combines Imputation-Augmented data with Mixup augmentation to bolster model generalization and improve forecasting performance. We evaluate the effectiveness of this method across several forecasting models, including DLinear (MLP), TimesNet (CNN), and iTrainformer (Transformer), these models represent some of the most recent advances in time series forecasting. Our experiments, conducted on four datasets (ETTh1, ETTh2, ETTm1, ETTm2) and compared against eight other augmentation techniques, demonstrate that IBMA consistently enhances performance, achieving 22 improvements out of 24 instances, with 10 of those being the best performances, particularly with iTrainformer imputation.

</details>


### [88] [Balance Equation-based Distributionally Robust Offline Imitation Learning](https://arxiv.org/abs/2511.07942)
*Rishabh Agrawal,Yusuf Alvi,Rahul Jain,Ashutosh Nayyar*

Main category: cs.LG

TL;DR: 提出一种基于平衡方程的分布鲁棒离线模仿学习框架，在不能改变环境的情况下，利用对转移模型的不确定性集合进行最坏情形优化，目标只基于名义数据分布实现可解的离线学习。


<details>
  <summary>Details</summary>
Motivation: 现实中 IL 经常假设环境动力学在训练和部署之间保持不变，但建模误差、参数变化与对抗扰动会导致转移动力学的偏移，造成性能下降，需在离线阶段对动力学不确定性进行鲁棒性处理。

Method: 提出Balance Equation-based Distributionally Robust Offline Imitation Learning（BR-DR-OIL）框架，围绕一个转移模型的不确定性集合进行分布鲁棒优化，最坏情形下最小化模仿损失。该鲁棒目标可完全转化为以名义数据分布为基础的等价形式，从而实现可行的离线学习。

Result: 在连续控制基准上的实验显示，与现有离线 IL baselines 相比，该方法在面对扰动或系统动态偏移时具有更强的鲁棒性与泛化能力。

Conclusion: 所提出的方法在离线模仿学习中提供了一种对动力学偏移的鲁棒解决方案，能够在不额外环境交互的前提下实现更稳健的策略泛化。

Abstract: Imitation Learning (IL) has proven highly effective for robotic and control tasks where manually designing reward functions or explicit controllers is infeasible. However, standard IL methods implicitly assume that the environment dynamics remain fixed between training and deployment. In practice, this assumption rarely holds where modeling inaccuracies, real-world parameter variations, and adversarial perturbations can all induce shifts in transition dynamics, leading to severe performance degradation. We address this challenge through Balance Equation-based Distributionally Robust Offline Imitation Learning, a framework that learns robust policies solely from expert demonstrations collected under nominal dynamics, without requiring further environment interaction. We formulate the problem as a distributionally robust optimization over an uncertainty set of transition models, seeking a policy that minimizes the imitation loss under the worst-case transition distribution. Importantly, we show that this robust objective can be reformulated entirely in terms of the nominal data distribution, enabling tractable offline learning. Empirical evaluations on continuous-control benchmarks demonstrate that our approach achieves superior robustness and generalization compared to state-of-the-art offline IL baselines, particularly under perturbed or shifted environments.

</details>


### [89] [Continual Unlearning for Text-to-Image Diffusion Models: A Regularization Perspective](https://arxiv.org/abs/2511.07970)
*Justin Lee,Zheda Mai,Jinsu Yoo,Chongyu Fan,Cheng Zhang,Wei-Lun Chao*

Main category: cs.LG

TL;DR: 本论文系统性研究文本到图像扩散模型中的连续撤回（continual unlearning），发现按序请求的撤回会导致参数漂移，快速降低保留知识的有效性和图像质量；提出正则化策略以缓解漂移，并与现有撤回方法兼容；引入面向语义的梯度投影方法，将漂移约束在与未撤回目标正交的子空间中，从而显著提升连续撤回性能，补充了通用正则化并给出基线、洞见与未来方向。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，撤回请求往往是逐步到来而非一次性；若直接对逐次请求进行处理，模型容易发生参数漂移，导致已保留的知识逐步流失，生成的图像质量下降。需要新的方法来实现连续的、对保留知识友好的撤回。

Method: 在现有撤回方法的基础上，增加一组附加正则化项，既能抑制参数漂移又能与现有方法兼容；提出面向语义的正则化，强调保持与撤回目标相关的概念；提出梯度投影方法，将参数漂移约束在与未撤回目标正交的子空间中。与其他正则化结合使用，进一步提高连续撤回的性能。

Result: 正则化项能缓解参数漂移，提升连续撤回的稳定性；梯度投影方法显著改善对被保留概念的保留程度，且与其他正则化互补，带来进一步的提升。

Conclusion: 将连续撤回确立为文本到图像生成中的基本挑战，提供了基线、洞见与未来研究方向，推动更安全、可问责的生成式AI发展。

Abstract: Machine unlearning--the ability to remove designated concepts from a pre-trained model--has advanced rapidly, particularly for text-to-image diffusion models. However, existing methods typically assume that unlearning requests arrive all at once, whereas in practice they often arrive sequentially. We present the first systematic study of continual unlearning in text-to-image diffusion models and show that popular unlearning methods suffer from rapid utility collapse: after only a few requests, models forget retained knowledge and generate degraded images. We trace this failure to cumulative parameter drift from the pre-training weights and argue that regularization is crucial to addressing it. To this end, we study a suite of add-on regularizers that (1) mitigate drift and (2) remain compatible with existing unlearning methods. Beyond generic regularizers, we show that semantic awareness is essential for preserving concepts close to the unlearning target, and propose a gradient-projection method that constrains parameter drift orthogonal to their subspace. This substantially improves continual unlearning performance and is complementary to other regularizers for further gains. Taken together, our study establishes continual unlearning as a fundamental challenge in text-to-image generation and provides insights, baselines, and open directions for advancing safe and accountable generative AI.

</details>


### [90] [Low-Rank Curvature for Zeroth-Order Optimization in LLM Fine-Tuning](https://arxiv.org/abs/2511.07971)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: LOREN 是一种针对大语言模型微调的曲率感知零阶优化方法。通过自适应估计各向异性的扰动分布来实现梯度预条件化，利用低秩块对角预条件子来捕捉曲率（基于自然进化策略框架），并采用 REINFORCE 留一法梯度估计以降低方差。实验证明在标准LLM基准上优于最先进的零阶方法，达到更高准确性与更快收敛，同时相比 MeZO-Adam 峰值内存下降最多 27.3%。


<details>
  <summary>Details</summary>
Motivation: 解决现有零阶优化在估计梯度时的高方差和子最优搜索方向问题，特别是在大语言模型微调场景中。需要引入曲率感知的预条件化和更高效的方差控制来提升性能与内存效率。

Method: (i) 将梯度预条件化问题重构为自适应估计梯度估计的各向异性扰动分布；(ii) 通过自然进化策略框架，使用低秩块对角预条件子来捕捉曲率信息；(iii) 采用 REINFORCE 留一梯度估计（RLOO）以降低估计方差。

Result: 在标准LLM基准上，该方法优于现有的零阶方法，表现为更高准确性与更快收敛，并且相较于 MeZO-Adam 峰值内存下降高达 27.3%。

Conclusion: LOREN 提供了一种更高效的曲率感知零阶优化方案，结合自适应扰动分布、低秩曲率建模与方差降低策略，适合大规模语言模型的微调。

Abstract: We introduce LOREN, a curvature-aware zeroth-order (ZO) optimization method for fine-tuning large language models (LLMs). Existing ZO methods, which estimate gradients via finite differences using random perturbations, often suffer from high variance and suboptimal search directions. Our approach addresses these challenges by: (i) reformulating the problem of gradient preconditioning as that of adaptively estimating an anisotropic perturbation distribution for gradient estimation, (ii) capturing curvature through a low-rank block diagonal preconditioner using the framework of natural evolution strategies, and (iii) applying a REINFORCE leave-one-out (RLOO) gradient estimator to reduce variance. Experiments on standard LLM benchmarks show that our method outperforms state-of-the-art ZO methods by achieving higher accuracy and faster convergence, while cutting peak memory usage by up to 27.3% compared with MeZO-Adam.

</details>


### [91] [Generalizable Insights for Graph Transformers in Theory and Practice](https://arxiv.org/abs/2511.08028)
*Timo Stoll,Luis Müller,Christopher Morris*

Main category: cs.LG

TL;DR: 提出并评估 Generalized-Distance Transformer (GDT)，在标准注意力框架下整合近期 GT 的多项进展，并在大规模跨域数据上系统性分析其表示能力与实用性。


<details>
  <summary>Details</summary>
Motivation: 填补理论性表达力研究与实际大规模应用之间的断层，缺乏对广域场景、跨任务及大模型尺度的综合验证与可推广洞见。

Method: 提出 GDT，基于标准注意力，融合近年 GT 的多项改进；对 GDT 的注意力与位置嵌入（PEs）的表示能力进行细粒度分析；进行覆盖超过 800 万张图、约 2.7 亿 token 的大规模跨域实验，涵盖图像对象检测、分子性质预测、代码摘要与 OOD 算法推理，并提炼出普适性设计要点。

Result: 发现若干在多任务/多模型尺度上稳定有效的设计选择；在少样本迁移且无需微调的情形下表现突出；提供跨域、跨任务的实证证据（覆盖 ~800 万图、约 2.7 亿 token 等规模）。

Conclusion: 将理论与实践的发现凝练为关于高效 GT 设计、训练与推理的可推广洞见，便于未来研究与应用采纳与推广。

Abstract: Graph Transformers (GTs) have shown strong empirical performance, yet current architectures vary widely in their use of attention mechanisms, positional embeddings (PEs), and expressivity. Existing expressivity results are often tied to specific design choices and lack comprehensive empirical validation on large-scale data. This leaves a gap between theory and practice, preventing generalizable insights that exceed particular application domains. Here, we propose the Generalized-Distance Transformer (GDT), a GT architecture using standard attention that incorporates many advancements for GTs from recent years, and develop a fine-grained understanding of the GDT's representation power in terms of attention and PEs. Through extensive experiments, we identify design choices that consistently perform well across various applications, tasks, and model scales, demonstrating strong performance in a few-shot transfer setting without fine-tuning. Our evaluation covers over eight million graphs with roughly 270M tokens across diverse domains, including image-based object detection, molecular property prediction, code summarization, and out-of-distribution algorithmic reasoning. We distill our theoretical and practical findings into several generalizable insights about effective GT design, training, and inference.

</details>


### [92] [From Sequential to Recursive: Enhancing Decision-Focused Learning with Bidirectional Feedback](https://arxiv.org/abs/2511.08035)
*Xinyu Wang,Jinxiao Du,Yiyang Peng,Wei Ma*

Main category: cs.LG

TL;DR: Introducing recursive decision-focused learning (R-DFL) to capture bidirectional prediction–optimization feedback, with two gradient methods (explicit unrolling and implicit differentiation). It shows comparable gradient accuracy and better efficiency, and yields improved decision quality on synthetic and real datasets (e.g., newsvendor, bipartite matching) over sequential baselines.


<details>
  <summary>Details</summary>
Motivation: S-DFL assumes a strictly sequential pipeline (predict→optimize) and misses bidirectional interactions between forecasting and optimization in complex decision problems. There is a need for a framework that allows feedback from downstream decisions back to predictions to improve end-task performance.

Method: Propose R-DFL with bidirectional feedback between downstream optimization and upstream prediction. Develop two gradient propagation methods: (1) explicit unrolling via automatic differentiation, and (2) implicit differentiation based on fixed-point methods. Provide theoretical analysis showing comparable gradient accuracy and highlight superior computational efficiency of the implicit method. Validate with experiments on synthetic and real-world tasks (e.g., newsvendor, bipartite matching).

Result: R-DFL yields substantially better final decision quality than sequential baselines and demonstrates robust adaptability across various closed-loop decision-making scenarios. Explicit and implicit gradient methods achieve similar gradient accuracy, with implicit differentiation being more computationally efficient.

Conclusion: R-DFL effectively models bidirectional prediction–decision feedback, enabling efficient gradient propagation and improved decision quality, supported by theoretical and empirical results.

Abstract: Decision-focused learning (DFL) has emerged as a powerful end-to-end alternative to conventional predict-then-optimize (PTO) pipelines by directly optimizing predictive models through downstream decision losses. Existing DFL frameworks are limited by their strictly sequential structure, referred to as sequential DFL (S-DFL). However, S-DFL fails to capture the bidirectional feedback between prediction and optimization in complex interaction scenarios. In view of this, we first time propose recursive decision-focused learning (R-DFL), a novel framework that introduces bidirectional feedback between downstream optimization and upstream prediction. We further extend two distinct differentiation methods: explicit unrolling via automatic differentiation and implicit differentiation based on fixed-point methods, to facilitate efficient gradient propagation in R-DFL. We rigorously prove that both methods achieve comparable gradient accuracy, with the implicit method offering superior computational efficiency. Extensive experiments on both synthetic and real-world datasets, including the newsvendor problem and the bipartite matching problem, demonstrate that R-DFL not only substantially enhances the final decision quality over sequential baselines but also exhibits robust adaptability across diverse scenarios in closed-loop decision-making problems.

</details>


### [93] [Online Linear Regression with Paid Stochastic Features](https://arxiv.org/abs/2511.08073)
*Nadav Merlis,Kyoungseok Jang,Nicolò Cesa-Bianchi*

Main category: cs.LG

TL;DR: 在可支付降噪的在线线性回归中，噪声协方差可控时， regret 为 sqrt(T)；协方差未知时，为 T^{2/3}（忽略对数因子）；并且通过矩阵马尔科夫不等式证明损失的统一收敛。


<details>
  <summary>Details</summary>
Motivation: 研究在噪声受控的在线回归情景下，如何通过支付降低观测噪声，从而在未知分布假设下实现最优表现在损失-支付权衡中。

Method: 提出模型，假设输入特征独立同分布，给出可支付降噪从而改变噪声协方差的机制；在已知支付-噪声映射时，给出 sqrt(T) 速率的下界上界；在未知映射时，推导 T^{2/3} 的速率；通过矩阵马尔科夫不等式证明经验损失均匀收敛到期望损失。

Result: 给出两种场景下的最优（或近似最优） regrets 速率：已知映射下为 sqrt(T)，未知映射下为 T^{2/3}，并且提供收敛性分析。

Conclusion: 本研究揭示了支付成本与提取噪声之间的权衡对在线线性回归性能的影响，及其对 regret 的直接影响，同时展示了矩阵 martingale 技术在统一下注释中的应用。

Abstract: We study an online linear regression setting in which the observed feature vectors are corrupted by noise and the learner can pay to reduce the noise level. In practice, this may happen for several reasons: for example, because features can be measured more accurately using more expensive equipment, or because data providers can be incentivized to release less private features. Assuming feature vectors are drawn i.i.d. from a fixed but unknown distribution, we measure the learner's regret against the linear predictor minimizing a notion of loss that combines the prediction error and payment. When the mapping between payments and noise covariance is known, we prove that the rate $\sqrt{T}$ is optimal for regret if logarithmic factors are ignored. When the noise covariance is unknown, we show that the optimal regret rate becomes of order $T^{2/3}$ (ignoring log factors). Our analysis leverages matrix martingale concentration, showing that the empirical loss uniformly converges to the expected one for all payments and linear predictors.

</details>


### [94] [An Integrated Fusion Framework for Ensemble Learning Leveraging Gradient Boosting and Fuzzy Rule-Based Models](https://arxiv.org/abs/2511.08077)
*Jinbo Li,Peng Liu,Long Chen,Witold Pedrycz,Weiping Ding*

Main category: cs.LG

TL;DR: 将梯度提升与模糊规则基础模型融合的集成框架，通过动态控制因子和基于样本的校正提高性能、降低过拟合并保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 结合梯度提升的高预测能力与模糊规则模型的可解释性，克服各自的局限性，如可扩展性、复杂设计和过拟合风险。

Method: 提出一个集成融合框架，在每次迭代中构建模糊规则基础模型，并由一个动态因子控制其对整体模型的贡献。该因子用于防止单模型支配、鼓励多样性、作为正则化参数，并基于模型表现实现动态调优以缓解过拟合。此外，框架引入基于样本的纠错机制，利用验证集的反馈进行自适应调整。

Result: 实验结果表明，该梯度提升融合框架在提升性能的同时，显著缓解了多数规则的复杂性和过拟合问题，保持了可解释性并简化了模型的维护与更新。

Conclusion: 通过最优的控制因子来管理各子模型的贡献，框架能够提升整体性能、维持可解释性并降低模型维护成本，适用于基于模糊规则的可解释模型的提升。

Abstract: The integration of different learning paradigms has long been a focus of machine learning research, aimed at overcoming the inherent limitations of individual methods. Fuzzy rule-based models excel in interpretability and have seen widespread application across diverse fields. However, they face challenges such as complex design specifications and scalability issues with large datasets. The fusion of different techniques and strategies, particularly Gradient Boosting, with Fuzzy Rule-Based Models offers a robust solution to these challenges. This paper proposes an Integrated Fusion Framework that merges the strengths of both paradigms to enhance model performance and interpretability. At each iteration, a Fuzzy Rule-Based Model is constructed and controlled by a dynamic factor to optimize its contribution to the overall ensemble. This control factor serves multiple purposes: it prevents model dominance, encourages diversity, acts as a regularization parameter, and provides a mechanism for dynamic tuning based on model performance, thus mitigating the risk of overfitting. Additionally, the framework incorporates a sample-based correction mechanism that allows for adaptive adjustments based on feedback from a validation set. Experimental results substantiate the efficacy of the presented gradient boosting framework for fuzzy rule-based models, demonstrating performance enhancement, especially in terms of mitigating overfitting and complexity typically associated with many rules. By leveraging an optimal factor to govern the contribution of each model, the framework improves performance, maintains interpretability, and simplifies the maintenance and update of the models.

</details>


### [95] [HipKittens: Fast and Furious AMD Kernels](https://arxiv.org/abs/2511.08083)
*William Hu,Drew Wadsworth,Sean Siddens,Stanley Winata,Daniel Y. Fu,Ryann Swann,Muhammad Osama,Christopher Ré,Simran Arora*

Main category: cs.LG

TL;DR: 提出 HipKittens (HK) 框架，将基于 tile 的编程原语从 NVIDIA 生态推广到 AMD GPU，并在 CDNA3/4 上验证其可行性与通用性，性能接近或超过手写汇编及编译器基线，在某些场景超越 1.2–2.4x，展示跨厂商的潜在统一层。


<details>
  <summary>Details</summary>
Motivation: 把 AI 算法高效映射到不同厂商的 GPU 架构具有挑战性；现有以 NVIDIA 为中心的 DSL（如 ThunderKittens）并不直接覆盖 AMD；需要一种跨厂商的 tile-based 软件层来简化高性能 AI 内核开发。

Method: 设计并实现 HK 框架，重新实现适用于 AMD 的 tile 基编程原语；在 AMD 的 CDNA3/4 平台上进行实验评估；将 HK 与 AMD 的手写汇编内核和编译器基线进行对比，覆盖 GEMMs、注意力、内存带宽等工作负载，验证可泛化性。

Result: tile 基本抽象可从 NVIDIA 泛化到 AMD，但 AMD 需要重新设计算法以适配其硬件；HK 在 CDNA3/4 上与手写汇编相当甚至有优势；在某些设置下，HK 超越所有可用基线 1.2–2.4 倍（如 d=64 的注意力、GQA 反向传播、内存绑定内核）；并在 宏观范围内验证了跨厂商的通用性。

Conclusion: 证实基于 tile 的编软件层有望成为跨 GPU 厂商的通用入口，HK 已开源（GitHub），为跨厂商高性能 AI 内核的开发铺平道路。

Abstract: AMD GPUs offer state-of-the-art compute and memory bandwidth; however, peak performance AMD kernels are written in raw assembly. To address the difficulty of mapping AI algorithms to hardware, recent work proposes C++ embedded and PyTorch-inspired domain-specific languages like ThunderKittens (TK) to simplify high performance AI kernel development on NVIDIA hardware. We explore the extent to which such primitives -- for explicit tile-based programming with optimized memory accesses and fine-grained asynchronous execution across workers -- are NVIDIA-specific or general. We provide the first detailed study of the programming primitives that lead to performant AMD AI kernels, and we encapsulate these insights in the HipKittens (HK) programming framework. We find that tile-based abstractions used in prior DSLs generalize to AMD GPUs, however we need to rethink the algorithms that instantiate these abstractions for AMD. We validate the HK primitives across CDNA3 and CDNA4 AMD platforms. In evaluations, HK kernels compete with AMD's hand-optimized assembly kernels for GEMMs and attention, and consistently outperform compiler baselines. Moreover, assembly is difficult to scale to the breadth of AI workloads; reflecting this, in some settings HK outperforms all available kernel baselines by $1.2-2.4\times$ (e.g., $d=64$ attention, GQA backwards, memory-bound kernels). These findings help pave the way for a single, tile-based software layer for high-performance AI kernels that translates across GPU vendors. HipKittens is released at: https://github.com/HazyResearch/HipKittens.

</details>


### [96] [Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks](https://arxiv.org/abs/2511.08086)
*Muthukumar Pandaram,Jakob Hollenstein,David Drexel,Samuele Tosatto,Antonio Rodríguez-Sánchez,Justus Piater*

Main category: cs.LG

TL;DR: 在机器人强化学习任务中，全球级别的稀疏性很少见，但存在局部、状态相关且在时间上局部化的稀疏性，挑战了普遍采用的稀疏性先验。


<details>
  <summary>Details</summary>
Motivation: 检验关于世界模型的稀疏性假设，尤其是因果图稀疏性、状态依赖性和局部性随时间的变化，以评估其是否符合真实任务。

Method: 从 MuJoCo Playground 基准的真实动力学数据出发，分析环境动力学的因果图稀疏性、状态依赖性以及局部动态的时间变化，关注在接触事件等时刻的局部结构和对特定维度的影响。

Result: 全局稀疏性罕见；局部、状态相关的稀疏性存在，呈现成簇状的时间分布，尤其在接触事件等关键时刻，影响特定状态维度子集。

Conclusion: 应优先考虑反映状态依赖性和时序局部性的有据可依的归纳偏置，避免过度依赖全局稀疏性假设，提高动态学习模型的真实性和样本效率。

Abstract: The use of learned dynamics models, also known as world models, can improve the sample efficiency of reinforcement learning. Recent work suggests that the underlying causal graphs of such dynamics models are sparsely connected, with each of the future state variables depending only on a small subset of the current state variables, and that learning may therefore benefit from sparsity priors. Similarly, temporal sparsity, i.e. sparsely and abruptly changing local dynamics, has also been proposed as a useful inductive bias.
  In this work, we critically examine these assumptions by analyzing ground-truth dynamics from a set of robotic reinforcement learning environments in the MuJoCo Playground benchmark suite, aiming to determine whether the proposed notions of state and temporal sparsity actually tend to hold in typical reinforcement learning tasks.
  We study (i) whether the causal graphs of environment dynamics are sparse, (ii) whether such sparsity is state-dependent, and (iii) whether local system dynamics change sparsely.
  Our results indicate that global sparsity is rare, but instead the tasks show local, state-dependent sparsity in their dynamics and this sparsity exhibits distinct structures, appearing in temporally localized clusters (e.g., during contact events) and affecting specific subsets of state dimensions. These findings challenge common sparsity prior assumptions in dynamics learning, emphasizing the need for grounded inductive biases that reflect the state-dependent sparsity structure of real-world dynamics.

</details>


### [97] [A robust methodology for long-term sustainability evaluation of Machine Learning models](https://arxiv.org/abs/2511.08120)
*Jorge Paz-Ruza,João Gama,Amparo Alonso-Betanzos,Bertha Guijarro-Berdiñas*

Main category: cs.LG

TL;DR: 提出一个面向长期可持续性的ML评估协议，适用于批训练与流式学习，揭示静态评估往往低估现实场景中的环境成本与性能收益，并在多模型、多任务上显示长期可持续性差异明显，且有时更高成本并未带来显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 弥补现有监管与报告实践中缺乏标准化、模型无关的长期可持续性评估协议的不足，真实反映AI系统在长期生命周期中的资源与环境影响。

Method: 提出一套适用于批处理和流式学习的综合评估协议，结合多任务分类任务与多种模型类型的长期实验，评估在数据分布演变和重复模型更新条件下的资源消耗与性能表现。对比静态训练—测试评估与长期更新情景的可持续性表现。

Result: 发现传统的静态训练-测试评估不能可靠捕捉在数据演变与重复更新下的长期可持续性；不同模型的长期可持续性差异显著，且在许多情形下较高的环境成本并未带来显著的性能提升。

Conclusion: 提出的评估协议填补监管与报告中的方法学空白，为制定更可持续的AI系统设计与部署策略提供证据基础。

Abstract: Sustainability and efficiency have become essential considerations in the development and deployment of Artificial Intelligence systems, yet existing regulatory and reporting practices lack standardized, model-agnostic evaluation protocols. Current assessments often measure only short-term experimental resource usage and disproportionately emphasize batch learning settings, failing to reflect real-world, long-term AI lifecycles. In this work, we propose a comprehensive evaluation protocol for assessing the long-term sustainability of ML models, applicable to both batch and streaming learning scenarios. Through experiments on diverse classification tasks using a range of model types, we demonstrate that traditional static train-test evaluations do not reliably capture sustainability under evolving data and repeated model updates. Our results show that long-term sustainability varies significantly across models, and in many cases, higher environmental cost yields little performance benefit.

</details>


### [98] [SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories](https://arxiv.org/abs/2511.08136)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出 SafeMIL 的离线安全模仿学习框架，通过多实例学习从非偏好轨迹中学习风险成本，提升策略的安全性同时保持奖励性能。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，在线交互风险高，难以明确给出每个时刻的奖励与安全代价，但可以收集反映不良行为的轨迹。目标不仅模仿偏好 demonstrations，还需避免风险行为。

Method: 使用多实例学习来参数化一个状态-动作对的风险成本预测函数，SafeMIL 以此成本来规避非偏好行为，并训练得到满足成本约束且高奖励的策略。

Result: 实验表明，学得的安全策略在不降低奖励性能的前提下满足成本约束，相对于若干基线表现更好。

Conclusion: SafeMIL 提供了一种将多实例学习整合到离线安全模仿学习中的新思路，有效提升策略的安全性和鲁棒性。

Abstract: In this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via \textit{Multiple Instance Learning}. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.

</details>


### [99] [BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services](https://arxiv.org/abs/2511.08142)
*Anna Lackinger,Andrea Morichetta,Pantelis A. Frangoudis,Schahram Dustdar*

Main category: cs.LG

TL;DR: 提出BIPPO，一种预算感知的独立PPO多智能体强化学习方法，用于在资源受限的IoT-FL环境中高效进行客户端选择，强调能耗效率、稳定性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 面向大规模物联网中的联邦学习场景，现有方法常忽视基础设施效率与资源约束；RL用于客户端选择的研究存在能耗、可泛化和适用性不足等问题，因此需要在预算约束下实现高效、鲁棒的客户端选择。

Method: 提出BIPPO（Budget-aware Independent Proximal Policy Optimization），一种能源高效的多智能体强化学习解决方案，设计改进的采样器以提升在预算受限条件下的客户端选择，并对非IID数据的图像分类任务进行评估。

Result: 在两项非IID图像分类任务中，BIPPO的平均准确率高于非RL机制、传统PPO和IPPO；且预算消耗仅为极小比例且在客户端数量增加时保持稳定，显示出良好的性能、稳定性、可扩展性及可持续性。

Conclusion: BIPPO为物联网-联邦学习中的客户端选择提供一个高效、稳定、可扩展且节能的解决方案。

Abstract: Federated Learning (FL) is a promising machine learning solution in large-scale IoT systems, guaranteeing load distribution and privacy. However, FL does not natively consider infrastructure efficiency, a critical concern for systems operating in resource-constrained environments. Several Reinforcement Learning (RL) based solutions offer improved client selection for FL; however, they do not consider infrastructure challenges, such as resource limitations and device churn. Furthermore, the training of RL methods is often not designed for practical application, as these approaches frequently do not consider generalizability and are not optimized for energy efficiency. To fill this gap, we propose BIPPO (Budget-aware Independent Proximal Policy Optimization), which is an energy-efficient multi-agent RL solution that improves performance. We evaluate BIPPO on two image classification tasks run in a highly budget-constrained setting, with FL clients training on non-IID data, a challenging context for vanilla FL. The improved sampler of BIPPO enables it to increase the mean accuracy compared to non-RL mechanisms, traditional PPO, and IPPO. In addition, BIPPO only consumes a negligible proportion of the budget, which stays consistent even if the number of clients increases. Overall, BIPPO delivers a performant, stable, scalable, and sustainable solution for client selection in IoT-FL.

</details>


### [100] [Improving Long-Range Interactions in Graph Neural Simulators via Hamiltonian Dynamics](https://arxiv.org/abs/2511.08185)
*Tai Hoang,Alessandro Trenta,Alessio Gravina,Niklas Freymuth,Philipp Becker,Davide Bacciu,Gerhard Neumann*

Main category: cs.LG

TL;DR: IGNS提出一种信息保持的图神经模拟器，基于哈密顿/端哈密顿动力学，解决长程耦合与自回归误差累积等GNS瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有GNS在捕捉远程交互和非保守力学方面能力不足，容易在自回归滚动中累积误差，需更强的信息保持与更广的动力学表示。

Method: 在图上实现信息保持的哈密顿/端哈密顿结构，扩展到端哈密顿系统以覆盖非保守效应；加入暖启动阶段以初始化全局上下文、几何编码以处理不规则网格，以及多步训练目标以减少滚动误差。并提出针对长程依赖和外部强迫的新基准来评估。

Result: 在所有任务中，IGNS在准确性和稳定性方面持续优于最新的GNS方法，尤其在长程依赖和复杂动力系统上表现更佳。

Conclusion: 信息保持的物理驱动GNS可显著提升保真度与鲁棒性，端哈密顿扩展使模型覆盖更广的动力学；新基准揭示其优势与潜在应用前景。

Abstract: Learning to simulate complex physical systems from data has emerged as a promising way to overcome the limitations of traditional numerical solvers, which often require prohibitive computational costs for high-fidelity solutions. Recent Graph Neural Simulators (GNSs) accelerate simulations by learning dynamics on graph-structured data, yet often struggle to capture long-range interactions and suffer from error accumulation under autoregressive rollouts. To address these challenges, we propose Information-preserving Graph Neural Simulators (IGNS), a graph-based neural simulator built on the principles of Hamiltonian dynamics. This structure guarantees preservation of information across the graph, while extending to port-Hamiltonian systems allows the model to capture a broader class of dynamics, including non-conservative effects. IGNS further incorporates a warmup phase to initialize global context, geometric encoding to handle irregular meshes, and a multi-step training objective to reduce rollout error. To evaluate these properties systematically, we introduce new benchmarks that target long-range dependencies and challenging external forcing scenarios. Across all tasks, IGNS consistently outperforms state-of-the-art GNSs, achieving higher accuracy and stability under challenging and complex dynamical systems.

</details>


### [101] [The Online Patch Redundancy Eliminator (OPRE): A novel approach to online agnostic continual learning using dataset compression](https://arxiv.org/abs/2511.08226)
*Raphaël Bayle,Martial Mermillod,Robert M. French*

Main category: cs.LG

TL;DR: 本研究同意在持续学习中，现有方法易受先验信息影响，提出在线数据集压缩算法OPRE，结合测试时的分类器训练，在CIFAR-10/100上表现优于多种在线持续学习方法，且对未来的完全不可知性（agnostic）更友好。


<details>
  <summary>Details</summary>
Motivation: 持续学习面临灾难性遗忘；现有评估通常将数据分割成任务并阶段性学习，且许多方法引入先验信息（如预训练特征提取器），从而降低对未来数据的普适性与不可知性。本文旨在揭示并降低这类先验信息的影响，推动更“不可知”的在线持续学习。

Method: 提出在线数据集压缩算法OPRE（Online Patch Redundancy Eliminator），在训练阶段使用压缩后的数据流，且在测试时训练分类器；对CIFAR-10/100进行实验比较，强调OPRE只需对未来数据做极小且可解释的假设。

Result: 在CIFAR-10与CIFAR-100上，结合OPRE的在线学习流程实现的性能优于多种当前的在线持续学习方法；OPRE对数据假设的依赖较低，具有更好的泛化潜力。

Conclusion: 在线数据集压缩可能是实现真正不可知（agnostic）持续学习的必要条件；通过在测试时进行分类器训练并配合OPRE，可以在保持低先验信息的同时提升性能。

Abstract: In order to achieve Continual Learning (CL), the problem of catastrophic forgetting, one that has plagued neural networks since their inception, must be overcome. The evaluation of continual learning methods relies on splitting a known homogeneous dataset and learning the associated tasks one after the other. We argue that most CL methods introduce a priori information about the data to come and cannot be considered agnostic. We exemplify this point with the case of methods relying on pretrained feature extractors, which are still used in CL. After showing that pretrained feature extractors imply a loss of generality with respect to the data that can be learned by the model, we then discuss other kinds of a priori information introduced in other CL methods. We then present the Online Patch Redundancy Eliminator (OPRE), an online dataset compression algorithm, which, along with the training of a classifier at test time, yields performance on CIFAR-10 and CIFAR-100 superior to a number of other state-of-the-art online continual learning methods. Additionally, OPRE requires only minimal and interpretable hypothesis on the data to come. We suggest that online dataset compression could well be necessary to achieve fully agnostic CL.

</details>


### [102] [Towards Non-Stationary Time Series Forecasting with Temporal Stabilization and Frequency Differencing](https://arxiv.org/abs/2511.08229)
*Junkai Lu,Peng Chen,Chenjuan Guo,Yang Shu,Meng Wang,Bin Yang*

Main category: cs.LG

TL;DR: 提出DTAF，一种在时域与频域同时建模非平稳性的双分支框架，以提升长序列预测的鲁棒性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列普遍存在非平稳性（包括时间分布漂移和谱变动），对长期预测造成显著挑战，需在时域与频域同时进行建模以实现鲁棒性。

Method: 提出Dual-branch框架DTAF：其一是Temporal Stabilizing Fusion（TFS），使用非平稳的专家混合（MOE）过滤器来分离并抑制时域的非平稳模式，同时保留长期依赖；其二是Frequency Wave Modeling（FWM），通过频率差分动态突出具有显著光谱漂移的分量；两者输出再进行融合以产生鲁棒预测。

Result: 在真实基准数据上，DTAF显著优于现有基线，提升预测准确性，且代码公开（https://github.com/PandaJunk/DTAF）。

Conclusion: DTAF通过同时建模时域与频域的非平稳性，提供更强的鲁棒性与预测性能，适用于多领域的长时序预测。

Abstract: Time series forecasting is critical for decision-making across dynamic domains such as energy, finance, transportation, and cloud computing. However, real-world time series often exhibit non-stationarity, including temporal distribution shifts and spectral variability, which pose significant challenges for long-term time series forecasting. In this paper, we propose DTAF, a dual-branch framework that addresses non-stationarity in both the temporal and frequency domains. For the temporal domain, the Temporal Stabilizing Fusion (TFS) module employs a non-stationary mix of experts (MOE) filter to disentangle and suppress temporal non-stationary patterns while preserving long-term dependencies. For the frequency domain, the Frequency Wave Modeling (FWM) module applies frequency differencing to dynamically highlight components with significant spectral shifts. By fusing the complementary outputs of TFS and FWM, DTAF generates robust forecasts that adapt to both temporal and frequency domain non-stationarity. Extensive experiments on real-world benchmarks demonstrate that DTAF outperforms state-of-the-art baselines, yielding significant improvements in forecasting accuracy under non-stationary conditions. All codes are available at https://github.com/PandaJunk/DTAF.

</details>


### [103] [A Unified Geometric Field Theory Framework for Transformers: From Manifold Embeddings to Kernel Modulation](https://arxiv.org/abs/2511.08243)
*Xianshuai Shi,Jianfeng Zhu,Leibo Liu*

Main category: cs.LG

TL;DR: 提出一个统一的结构性理论框架，将位置编码、核积分算子和注意力机制整合，并将离散位置信息映射到连续流形上的场论视角，使 Transformer 层可被理解为对嵌入流形的核调制算子。


<details>
  <summary>Details</summary>
Motivation: 解决 Transformer 的核心组成部分（位置编码和注意力）缺乏统一的物理/数学解释的问题，并提供一个在离散序列与连续几何之间的统一理论桥梁，以便进行更深的理论分析。

Method: 提出一种将位置编码、核积分算子和注意力机制整合的结构性理论框架；将离散位置映射到连续流形上的空间函数，使 Transformer 层被解释为作用在嵌入流形上的核调制算子。

Result: 给出了一种新的理论框架和映射关系，提供对 Transformer 的几何/场论性质的统一解释，便于理论分析与潜在的新设计思路，但未在此摘要中陈述具体实验结果。

Conclusion: 该工作为 Transformer 的位置编码与注意力机制提供了一个场论层面的解释，促进对其结构与行为的深入理论研究，并可能推动新型注意力/编码机制的理论设计。

Abstract: The Transformer architecture has achieved tremendous success in natural language processing, computer vision, and scientific computing through its self-attention mechanism. However, its core components-positional encoding and attention mechanisms-have lacked a unified physical or mathematical interpretation. This paper proposes a structural theoretical framework that integrates positional encoding, kernel integral operators, and attention mechanisms for in-depth theoretical investigation. We map discrete positions (such as text token indices and image pixel coordinates) to spatial functions on continuous manifolds, enabling a field-theoretic interpretation of Transformer layers as kernel-modulated operators acting over embedded manifolds.

</details>


### [104] [Data-Driven Discovery of Feature Groups in Clinical Time Series](https://arxiv.org/abs/2511.08260)
*Fedor Sergeev,Manuel Burger,Polina Leshetkina,Vincent Fortuin,Gunnar Rätsch,Rita Kuznetsova*

Main category: cs.LG

TL;DR: 提出一种通过聚类特征嵌入层的权重来学习特征分组的方法，并将其整合到常规监督训练中，以提升临床时间序列预测性能并获得可解释的分组。


<details>
  <summary>Details</summary>
Motivation: 临床时间序列通常多变量且特征来自不同数据源，合理分组特征可提升深度学习性能；但仅凭语义知识定义分组困难且缺乏数据驱动性与可解释性。

Method: 在特征级嵌入层的权重上进行聚类以学习分组，并与标准监督训练无缝集成；所获分组直接提升下游任务表现，同时具备临床可解释性。

Result: 在合成数据上优于静态聚类方法，在真实医学数据上表现接近专家定义的分组；所学分组具临床可解释性，可用于数据驱动的变量之间关系发现。

Conclusion: 该方法为临床时间序列的特征分组提供了一种可训练、可解释且能提升预测性能的解决方案，便于与现有模型结合并促进临床洞察。

Abstract: Clinical time series data are critical for patient monitoring and predictive modeling. These time series are typically multivariate and often comprise hundreds of heterogeneous features from different data sources. The grouping of features based on similarity and relevance to the prediction task has been shown to enhance the performance of deep learning architectures. However, defining these groups a priori using only semantic knowledge is challenging, even for domain experts. To address this, we propose a novel method that learns feature groups by clustering weights of feature-wise embedding layers. This approach seamlessly integrates into standard supervised training and discovers the groups that directly improve downstream performance on clinically relevant tasks. We demonstrate that our method outperforms static clustering approaches on synthetic data and achieves performance comparable to expert-defined groups on real-world medical data. Moreover, the learned feature groups are clinically interpretable, enabling data-driven discovery of task-relevant relationships between variables.

</details>


### [105] [Rethinking Explanation Evaluation under the Retraining Scheme](https://arxiv.org/abs/2511.08281)
*Yi Cai,Thibaud Ardoin,Mayank Gulati,Gerhard Wunder*

Main category: cs.LG

TL;DR: 通过重新框架解释评估，解决推理-基础评估中的符号/残留信息问题；提出新的变体以提高评估效率与可靠性，并在多数据尺度上的实验给出对解释器选择与基准的深入见解。


<details>
  <summary>Details</summary>
Motivation: 解释方法的评估因缺乏地面真实解释而困难；推理-based评估会导致分布偏移，ROAR等重训练方案虽解决偏移但与理论假设常常矛盾。因此需要诊断并修正评估过程中的偏差，提升评估的一致性与实用性。

Method: 分析解释性评估中的“符号问题”（sign issue）作为残留信息的关键原因；给出评估过程的简单重构以消除该问题；在现有框架基础上提出新的变体，以系统地构築对解释评估的综合视角，并坚持提升效率。

Result: 重新框架有效解决了所识别的问题；提出的变体显著提升了相较标准重训练协议的评估效率，提升 exploainer 选择与基准测试的实用性；跨多数据尺度的实证结果提供对解释器性能的更深见解与未来方向。

Conclusion: 评估框架的重构与新变体能提高解释评估的可靠性和实用性，为 explainer 的选择与 benchmark 提供更强的实践指引，同时揭示了仍待解决的开放挑战。

Abstract: Feature attribution has gained prominence as a tool for explaining model decisions, yet evaluating explanation quality remains challenging due to the absence of ground-truth explanations. To circumvent this, explanation-guided input manipulation has emerged as an indirect evaluation strategy, measuring explanation effectiveness through the impact of input modifications on model outcomes during inference. Despite the widespread use, a major concern with inference-based schemes is the distribution shift caused by such manipulations, which undermines the reliability of their assessments. The retraining-based scheme ROAR overcomes this issue by adapting the model to the altered data distribution. However, its evaluation results often contradict the theoretical foundations of widely accepted explainers. This work investigates this misalignment between empirical observations and theoretical expectations. In particular, we identify the sign issue as a key factor responsible for residual information that ultimately distorts retraining-based evaluation. Based on the analysis, we show that a straightforward reframing of the evaluation process can effectively resolve the identified issue. Building on the existing framework, we further propose novel variants that jointly structure a comprehensive perspective on explanation evaluation. These variants largely improve evaluation efficiency over the standard retraining protocol, thereby enhancing practical applicability for explainer selection and benchmarking. Following our proposed schemes, empirical results across various data scales provide deeper insights into the performance of carefully selected explainers, revealing open challenges and future directions in explainability research.

</details>


### [106] [Dual-Kernel Graph Community Contrastive Learning](https://arxiv.org/abs/2511.08287)
*Xiang Chen,Kun Yue,Wenjie Liu,Zhenyu Zhang,Liang Duan*

Main category: cs.LG

TL;DR: Efficient GCL via kernelized community-level loss with linear complexity and distillation-enabled decoupled GNN for scalable large graphs.


<details>
  <summary>Details</summary>
Motivation: GCL face scalability challenges due to costly message passing and quadratic contrastive loss over node pairs; need scalable representation learning that preserves structural information.

Method: Transform the input graph into a compact network of interconnected node sets (communities); introduce kernelized graph community contrastive loss with linear complexity to enable information transfer among node sets and capture hierarchical graph structure; incorporate knowledge distillation into a decoupled GNN architecture to accelerate inference while maintaining generalization.

Result: Empirical evaluation on sixteen real-world datasets across scales shows the proposed method outperforms state-of-the-art GCL baselines in both effectiveness and scalability.

Conclusion: The framework achieves scalable, effective graph contrastive learning by combining a kernelized, community-level contrastive objective with a distillation-enhanced, decoupled GNN, enabling efficient learning on large graphs.

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful paradigm for training Graph Neural Networks (GNNs) in the absence of task-specific labels. However, its scalability on large-scale graphs is hindered by the intensive message passing mechanism of GNN and the quadratic computational complexity of contrastive loss over positive and negative node pairs. To address these issues, we propose an efficient GCL framework that transforms the input graph into a compact network of interconnected node sets while preserving structural information across communities. We firstly introduce a kernelized graph community contrastive loss with linear complexity, enabling effective information transfer among node sets to capture hierarchical structural information of the graph. We then incorporate a knowledge distillation technique into the decoupled GNN architecture to accelerate inference while maintaining strong generalization performance. Extensive experiments on sixteen real-world datasets of varying scales demonstrate that our method outperforms state-of-the-art GCL baselines in both effectiveness and scalability.

</details>


### [107] [Test-time Diverse Reasoning by Riemannian Activation Steering](https://arxiv.org/abs/2511.08305)
*Ly Tran Ho Khanh,Dongxuan Zhu,Man-Chung Yue,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 提出了一种无监督的测试时激活指引策略，通过并行优化多条推理路径的 steering 向量，在同步锚点处最大化多条潜在激活子集所张成的体积，以提升多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决 Best-of-N 推理中的输出多样性不足问题，避免同质化的错误复现，同时在测试时提升推理路径的有效探索性。

Method: 在批量生成过程的任意同步锚点，优化多条推理轨迹的 steering 向量，使其最大化所有可能经干预的激活子集所张成的体积。将该优化转化为在积的球面上具有对数行列式目标函数的黎曼优化问题，使用黎曼坐标下降算法求取停稳点，并在下一个同步锚点继续应用这些向量。

Result: 在常用数学基准上，测试时黎曼激活指引策略在多样性和解的准确性方面优于基础的随机采样（vanilla sampling）方法。

Conclusion: 该工作提出了一种无监督、测试时可执行的激活指引策略，利用黎曼优化获得稳定的多路径激活向量，从而提升推理过程的多样性与解题准确性。

Abstract: Best-of-$N$ reasoning improves the accuracy of language models in solving complex tasks by sampling multiple candidate solutions and then selecting the best one based on some criteria. A critical bottleneck for this strategy is the output diversity limit, which occurs when the model generates similar outputs despite stochastic sampling, and hence recites the same error. To address this lack of variance in reasoning paths, we propose a novel unsupervised activation steering strategy that simultaneously optimizes the steering vectors for multiple reasoning trajectories at test time. At any synchronization anchor along the batch generation process, we find the steering vectors that maximize the total volume spanned by all possible intervened activation subsets. We demonstrate that these steering vectors can be determined by solving a Riemannian optimization problem over the product of spheres with a log-determinant objective function. We then use a Riemannian block-coordinate descent algorithm with a well-tuned learning rate to obtain a stationary point of the problem, and we apply these steering vectors until the generation process reaches the subsequent synchronization anchor. Empirical evaluations on popular mathematical benchmarks demonstrate that our test-time Riemannian activation steering strategy outperforms vanilla sampling techniques in terms of generative diversity and solution accuracy.

</details>


### [108] [Improving the accuracy and generalizability of molecular property regression models with a substructure-substitution-rule-informed framework](https://arxiv.org/abs/2511.08314)
*Xiaoyu Fan,Lin Guo,Ruizhen Jia,Yang Tian,Zhihao Yang,Boxue Tian*

Main category: cs.LG

TL;DR: MolRuleLoss is a substructure-substitution-rule-informed loss that enhances accuracy and generalizability of molecular property regression models (MPRMs) by enforcing partial derivative constraints for SSRs; shows consistent improvements across lipophilicity, solubility, solvation energy, melting point, and molecular weight tasks, including OOD and activity-cliff scenarios.


<details>
  <summary>Details</summary>
Motivation: AI-driven drug discovery relies on accurate regression of molecular properties, but existing models struggle with regression accuracy and generalization, especially for out-of-distribution molecules. Incorporating chemical knowledge via SSR-informed loss could improve performance and robustness.

Method: MolRuleLoss introduces partial derivative constraints derived from substructure substitution rules (SSRs) into the loss function of MPRMs (e.g., GEM, UniMol). It is evaluated on multiple properties and datasets (lipophilicity, ESOL, FreeSolv from MoleculeNet) and extended to activity cliffs and OOD scenarios, with dramatic reductions in RMSE for some tasks (e.g., MW prediction).

Result: Quantitative improvements in RMSE when using MolRuleLoss: lipophilicity (0.587 vs 0.660), ESOL (0.777 vs 0.798), FreeSolv (1.252 vs 1.877) indicating 2.6–33.3% gains. Enhanced generalizability for activity cliffs in lipophilicity and OOD cases in melting point prediction. For OOD molecular weight prediction, RMSE dropped from 29.507 to 0.007. Also shows that more and higher-quality SSRs correlate with larger gains; the upper bound of SSR-induced property-change variation is positively correlated with model error.

Conclusion: MolRuleLoss can be added to existing MPRMs to boost accuracy and generalization across diverse molecular-property tasks, with potential wide application in cheminformatics and AI-aided drug discovery.

Abstract: Artificial Intelligence (AI)-aided drug discovery is an active research field, yet AI models often exhibit poor accuracy in regression tasks for molecular property prediction, and perform catastrophically poorly for out-of-distribution (OOD) molecules. Here, we present MolRuleLoss, a substructure-substitution-rule-informed framework that improves the accuracy and generalizability of multiple molecular property regression models (MPRMs) such as GEM and UniMol for diverse molecular property prediction tasks. MolRuleLoss incorporates partial derivative constraints for substructure substitution rules (SSRs) into an MPRM's loss function. When using GEM models for predicting lipophilicity, water solubility, and solvation-free energy (using lipophilicity, ESOL, and freeSolv datasets from MoleculeNet), the root mean squared error (RMSE) values with and without MolRuleLoss were 0.587 vs. 0.660, 0.777 vs. 0.798, and 1.252 vs. 1.877, respectively, representing 2.6-33.3% performance improvements. We show that both the number and the quality of SSRs contribute to the magnitude of prediction accuracy gains obtained upon adding MolRuleLoss to an MPRM. MolRuleLoss improved the generalizability of MPRMs for "activity cliff" molecules in a lipophilicity prediction task and improved the generalizability of MPRMs for OOD molecules in a melting point prediction task. In a molecular weight prediction task for OOD molecules, MolRuleLoss reduced the RMSE value of a GEM model from 29.507 to 0.007. We also provide a formal demonstration that the upper bound of the variation for property change of SSRs is positively correlated with an MPRM's error. Together, we show that using the MolRuleLoss framework as a bolt-on boosts the prediction accuracy and generalizability of multiple MPRMs, supporting diverse applications in areas like cheminformatics and AI-aided drug discovery.

</details>


### [109] [Adversarial Bias: Data Poisoning Attacks on Fairness](https://arxiv.org/abs/2511.08331)
*Eunice Chan,Hanghang Tong*

Main category: cs.LG

TL;DR: Adversarial data poisoning can maximally degrade fairness of naive Bayes classifiers while preserving accuracy, demonstrated across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Ensure fairness in AI/ML and understand potential vulnerabilities where fairness can be intentionally compromised; fill gap in literature on fairness vulnerability and attack surfaces.

Method: Theoretical analysis shows a simple poisoning strategy suffices to induce maximal unfairness. Inject a small fraction of carefully crafted adversarial data into the training set to bias the decision boundary, disproportionately harming a protected group while maintaining generalization. Empirical validation on benchmark datasets and models.

Result: Attack significantly degrades fairness metrics across multiple models and datasets, outperforming existing methods. Achieves higher unfairness with comparable or slightly worse impact on accuracy and works across a wide range of models.

Conclusion: Highlights a potent and robust threat to ML fairness via data poisoning; underlines need for defenses and monitoring against poisoning attacks to preserve fairness across diverse models and datasets.

Abstract: With the growing adoption of AI and machine learning systems in real-world applications, ensuring their fairness has become increasingly critical. The majority of the work in algorithmic fairness focus on assessing and improving the fairness of machine learning systems. There is relatively little research on fairness vulnerability, i.e., how an AI system's fairness can be intentionally compromised. In this work, we first provide a theoretical analysis demonstrating that a simple adversarial poisoning strategy is sufficient to induce maximally unfair behavior in naive Bayes classifiers. Our key idea is to strategically inject a small fraction of carefully crafted adversarial data points into the training set, biasing the model's decision boundary to disproportionately affect a protected group while preserving generalizable performance. To illustrate the practical effectiveness of our method, we conduct experiments across several benchmark datasets and models. We find that our attack significantly outperforms existing methods in degrading fairness metrics across multiple models and datasets, often achieving substantially higher levels of unfairness with a comparable or only slightly worse impact on accuracy. Notably, our method proves effective on a wide range of models, in contrast to prior work, demonstrating a robust and potent approach to compromising the fairness of machine learning systems.

</details>


### [110] [LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration](https://arxiv.org/abs/2511.08339)
*Ruiyu Qiu,Rui Wang,Guanghui Yang,Xiang Li,Zhijiang Shao*

Main category: cs.LG

TL;DR: 提出了一个新的连续LMORL框架LPPG-RL，通过序列梯度投影来实现对优先级的字典多目标优化，使用Dykstra投影和子问题探索，理论保证收敛并在2D导航任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多目标强化学习中，现实情形通常存在明确的优先级排序，且现有LMORL要么依赖手工阈值要么仅适用于离散域，难以在连续空间高效且无先验知识地强制优先关系。需要一种能与任意连续策略梯度算法兼容且高效地实现优先级约束的方法。

Method: 提出LPPG-RL框架，通过逐步的梯度投影来确定可行的策略更新方向。将投影步改写为优化问题，并使用Dykstra投影代替通用求解器以提升速度。引入子问题探索（SE）以防止梯度消失、加速收敛并提高稳定性。理论上给出收敛性保证与对策略改进的下界。与现有连续LMORL方法在2D导航环境的实验结果表明其有效性。

Result: 给出收敛性证明和策略改进下界；在2D导航任务中，LPPG-RL在连续LMORL领域优于现有方法。

Conclusion: LPPG-RL为连续空间中的LMORL提供了一种通用、高效且稳定的实现路径，具备良好的理论保证与实际性能提升，尤其在小到中等规模实例上有显著速度优势。

Abstract: Lexicographic multi-objective problems, which consist of multiple conflicting subtasks with explicit priorities, are common in real-world applications. Despite the advantages of Reinforcement Learning (RL) in single tasks, extending conventional RL methods to prioritized multiple objectives remains challenging. In particular, traditional Safe RL and Multi-Objective RL (MORL) methods have difficulty enforcing priority orderings efficiently. Therefore, Lexicographic Multi-Objective RL (LMORL) methods have been developed to address these challenges. However, existing LMORL methods either rely on heuristic threshold tuning with prior knowledge or are restricted to discrete domains. To overcome these limitations, we propose Lexicographically Projected Policy Gradient RL (LPPG-RL), a novel LMORL framework which leverages sequential gradient projections to identify feasible policy update directions, thereby enabling LPPG-RL broadly compatible with all policy gradient algorithms in continuous spaces. LPPG-RL reformulates the projection step as an optimization problem, and utilizes Dykstra's projection rather than generic solvers to deliver great speedups, especially for small- to medium-scale instances. In addition, LPPG-RL introduces Subproblem Exploration (SE) to prevent gradient vanishing, accelerate convergence and enhance stability. We provide theoretical guarantees for convergence and establish a lower bound on policy improvement. Finally, through extensive experiments in a 2D navigation environment, we demonstrate the effectiveness of LPPG-RL, showing that it outperforms existing state-of-the-art continuous LMORL methods.

</details>


### [111] [From Confusion to Clarity: ProtoScore - A Framework for Evaluating Prototype-Based XAI](https://arxiv.org/abs/2511.08361)
*Helena Monke,Benjamin Sae-Chew,Benjamin Fresz,Marco F. Huber*

Main category: cs.LG

TL;DR: 提出 ProtoScore 框架用于评估基于原型的 XAI 方法，聚焦时间序列数据，并结合 Co-12 属性以实现跨数据类型的公平比较。


<details>
  <summary>Details</summary>
Motivation: 当前原型型 XAI 缺乏标准化基准，尤其在时间序列领域，导致评估主观，难以比较方法。需要统一的、可复用的评价框架以帮助用户选择方法并降低用户研究成本。

Method: 将 ProtoScore 框架整合 Co-12 属性，评估原型方法与彼此及其它 XAI 方法的表现；提供跨数据类型的评估路径并公开代码。

Result: 提出一个可操作的基准框架以及评估流程，理论上实现对原型方法及其他 XAI 的公平比较，促进领域发展，减少对用户研究的依赖。

Conclusion: ProtoScore 有望标准化原型型 XAI 的评估，提高可重复性和可比性，尤其适用于时间序列数据场景；代码公开促使社区采纳和扩展。

Abstract: The complexity and opacity of neural networks (NNs) pose significant challenges, particularly in high-stakes fields such as healthcare, finance, and law, where understanding decision-making processes is crucial. To address these issues, the field of explainable artificial intelligence (XAI) has developed various methods aimed at clarifying AI decision-making, thereby facilitating appropriate trust and validating the fairness of outcomes. Among these methods, prototype-based explanations offer a promising approach that uses representative examples to elucidate model behavior. However, a critical gap exists regarding standardized benchmarks to objectively compare prototype-based XAI methods, especially in the context of time series data. This lack of reliable benchmarks results in subjective evaluations, hindering progress in the field. We aim to establish a robust framework, ProtoScore, for assessing prototype-based XAI methods across different data types with a focus on time series data, facilitating fair and comprehensive evaluations. By integrating the Co-12 properties of Nauta et al., this framework allows for effectively comparing prototype methods against each other and against other XAI methods, ultimately assisting practitioners in selecting appropriate explanation methods while minimizing the costs associated with user studies. All code is publicly available at https://github.com/HelenaM23/ProtoScore .

</details>


### [112] [Multi-objective Hyperparameter Optimization in the Age of Deep Learning](https://arxiv.org/abs/2511.08371)
*Soham Basu,Frank Hutter,Danny Stoll*

Main category: cs.LG

TL;DR: 提出 PriMO，一种能整合多目标用户先验的超参数优化算法，在8个深度学习基准上达到多目标和单目标的最优性能，成为 DL 实践者的首选 HPO 工具。


<details>
  <summary>Details</summary>
Motivation: DL 专家通常对哪些超参数设置表现良好有先验认识，但现有 HPO 算法很少能利用这种先验信息，且没有对多目标的先验进行建模。缺少能同时处理多目标先验的 HPO 方法；因此需要一种能够将多目标用户信念融入优化过程的算法。

Method: 提出 PriMO，首个能够整合多目标用户信念的 HPO 算法。通过引入对多目标的用户先验来引导超参数搜索，结合适当的建模与优化策略实现多目标与单目标的高效优化。

Result: 在8个深度学习基准上，PriMO在多目标和单目标设置下均达到或接近SOTA，优于现有 HPO 方法，显著提升 DL 任务的性能和效率。

Conclusion: PriMO 为深度学习领域的 HPO 提供了一个新范式：通过显式编码多目标用户信念来提升优化效果，成为 DL 实践者的首选 HPO 工具。

Abstract: While Deep Learning (DL) experts often have prior knowledge about which hyperparameter settings yield strong performance, only few Hyperparameter Optimization (HPO) algorithms can leverage such prior knowledge and none incorporate priors over multiple objectives. As DL practitioners often need to optimize not just one but many objectives, this is a blind spot in the algorithmic landscape of HPO. To address this shortcoming, we introduce PriMO, the first HPO algorithm that can integrate multi-objective user beliefs. We show PriMO achieves state-of-the-art performance across 8 DL benchmarks in the multi-objective and single-objective setting, clearly positioning itself as the new go-to HPO algorithm for DL practitioners.

</details>


### [113] [EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting](https://arxiv.org/abs/2511.08396)
*Zhiwei Zhang,Xinyi Du,Xuanchi Guo,Weihao Wang,Wenjuan Han*

Main category: cs.LG

TL;DR: EMAformer：在Transformer基础上加入辅助嵌入组件，通过全球稳定性、相位敏感性和跨轴特异性三大诱导偏置，提升多变量时间序列预测，达到12个真实数据集的SOTA，平均MSE降幅2.73%、MAE降幅5.15%。


<details>
  <summary>Details</summary>
Motivation: 现有的iTransformer在与MLP模型的对比中仍落后，原因在于通道间关系不稳定，难以有效建模跨通道信息交互，因此需要通过额外的嵌入和诱导偏置来稳健化 Transformer 对多变量时间序列的建模能力。

Method: 提出 EMAformer，在 Transformer 上加入一个辅助嵌入套件，形成“铠甲”式增强；通过引入全球稳定性、相位敏感性和跨轴特异性三大 inductive biases，提升跨通道关系建模的稳健性与表达能力。

Result: 在12个真实世界基准数据集上实现了最先进的性能，平均在MSE上降幅2.73%、在MAE上降幅5.15%，并公开代码。

Conclusion: 该方法显著提升了 Transformer 在多变量时间序列预测中的实用性，缩小了与MLP等模型的性能差距。

Abstract: Multivariate time series forecasting is crucial across a wide range of domains. While presenting notable progress for the Transformer architecture, iTransformer still lags behind the latest MLP-based models. We attribute this performance gap to unstable inter-channel relationships. To bridge this gap, we propose EMAformer, a simple yet effective model that enhances the Transformer with an auxiliary embedding suite, akin to armor that reinforces its ability. By introducing three key inductive biases, i.e., \textit{global stability}, \textit{phase sensitivity}, and \textit{cross-axis specificity}, EMAformer unlocks the further potential of the Transformer architecture, achieving state-of-the-art performance on 12 real-world benchmarks and reducing forecasting errors by an average of 2.73\% in MSE and 5.15\% in MAE. This significantly advances the practical applicability of Transformer-based approaches for multivariate time series forecasting. The code is available on https://github.com/PlanckChang/EMAformer.

</details>


### [114] [Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment](https://arxiv.org/abs/2511.08399)
*Hua Ye,Hang Ding,Siyuan Chen,Yiyang Jiang,Changyuan Zhang,Xuan Zhang*

Main category: cs.LG

TL;DR: Introducing Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on for dual-encoder models that uses boundary-aware negative sampling and local attention to convert borderline negatives into a curriculum signal, yielding faster training and SOTA without extra labels.


<details>
  <summary>Details</summary>
Motivation: In contrastive multimodal learning, not all negatives are equal; ambiguous negatives near the positive can mislead learning. Uniform negative sampling ignores these nuances. A curriculum that progressively hardens negatives and a local attention mechanism to emphasize actual mismatches can improve alignment more efficiently.

Method: Two differentiable modules: (1) Boundary-Aware Negative Sampler that gradually increases negative difficulty as training progresses; (2) Contrastive Local Attention loss that highlights where the image-text pair mismatches locally. Compatible with any off-the-shelf dual encoder; no extra labels.

Result: Theoretically predicts O(1/n) error rate. Empirically achieves up to 32% relative R@1 gain over CLIP and sets new state-of-the-art on four large-scale benchmarks, all without additional labels.

Conclusion: BACL provides a simple, effective add-on to existing dual-encoder frameworks, improving robustness to near-miss negatives and enhancing cross-modal alignment through a differentiable curriculum and local attention without requiring extra supervision.

Abstract: Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.

</details>


### [115] [ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games](https://arxiv.org/abs/2511.08412)
*Ruochuan Shi,Runyu Lu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.LG

TL;DR: ARAC combines an attention-based graph neural network (GNN) with adaptive divergence regularization in a multi-agent soft actor-critic framework to address sparse rewards in graph-structured MARL tasks like pursuit and confrontation, yielding faster convergence and better scalability.


<details>
  <summary>Details</summary>
Motivation: Graph-structured MARL tasks involve highly dynamic interactions among agents, where sparse rewards hinder efficient policy learning; there is a need to leverage prior policies or guidance without entrenching suboptimal biases.

Method: Proposes Adaptive Regularized Multi-Agent Soft Actor-Critic (ARAC) that uses an attention-based GNN to model inter-agent dependencies and an adaptive divergence regularization mechanism that initially exploits a reference policy to guide exploration and gradually reduces reliance as training progresses.

Result: Empirical experiments in pursuit and confrontation domains show ARAC achieves faster convergence, higher final success rates, and better scalability with varying numbers of agents compared to MARL baselines.

Conclusion: Adaptive divergence regularization paired with a graph-based representation enhances learning efficiency and policy quality in complex, multi-agent, graph-structured environments, particularly under sparse reward settings.

Abstract: In graph-structured multi-agent reinforcement learning (MARL) adversarial tasks such as pursuit and confrontation, agents must coordinate under highly dynamic interactions, where sparse rewards hinder efficient policy learning. We propose Adaptive Regularized Multi-Agent Soft Actor-Critic (ARAC), which integrates an attention-based graph neural network (GNN) for modeling agent dependencies with an adaptive divergence regularization mechanism. The GNN enables expressive representation of spatial relations and state features in graph environments. Divergence regularization can serve as policy guidance to alleviate the sparse reward problem, but it may lead to suboptimal convergence when the reference policy itself is imperfect. The adaptive divergence regularization mechanism enables the framework to exploit reference policies for efficient exploration in the early stages, while gradually reducing reliance on them as training progresses to avoid inheriting their limitations. Experiments in pursuit and confrontation scenarios demonstrate that ARAC achieves faster convergence, higher final success rates, and stronger scalability across varying numbers of agents compared with MARL baselines, highlighting its effectiveness in complex graph-structured environments.

</details>


### [116] [Physics-Informed Neural Operators for Cardiac Electrophysiology](https://arxiv.org/abs/2511.08418)
*Hannah Lydon,Milad Kazemi,Martin Bishop,Nicola Paoletti*

Main category: cs.LG

TL;DR: PINO（ Physics-Informed Neural Operator ）通过在函数空间之间学习映射来求解心肌电生理相关的PDE，比传统数值求解更具网格无关性、可扩展性和对长期预测的稳定性，且实现了对多分辨率和不同初始条件的泛化。


<details>
  <summary>Details</summary>
Motivation:  cardiac electrophysiology (EP) 的PDE建模计算成本高、对离散化敏感；PINNs虽缓解部分问题，但仍受网格和长期稳定性的限制，需要更高效、可扩展的求解器。

Method: 提出基于物理信息的神经算子（PINO），在函数空间之间学习映射，使模型可在多分辨率网格和不同初始条件上泛化。通过零-shot评估对训练中未见的情景进行推断，并在长时间回滚预测（长时展望）和提高分辨率方面保持高预测质量，分辨率可扩展至训练分辨率的10倍。

Result: PINO能够在较长时间内准确重现心肌EP动力学，覆盖多种传播情景；在长时间回滚中保持高预测质量；具备对新情景的零-shot泛化能力；相比数值PDE求解，显著降低仿真时间，并能将预测分辨率提升至训练分辨率的10倍。

Conclusion: PINO为心肌电生理仿真提供了一种高效、可扩展且泛化性强的求解框架，适合多场景快速仿真与潜在的实时应用。

Abstract: Accurately simulating systems governed by PDEs, such as voltage fields in cardiac electrophysiology (EP) modelling, remains a significant modelling challenge. Traditional numerical solvers are computationally expensive and sensitive to discretisation, while canonical deep learning methods are data-hungry and struggle with chaotic dynamics and long-term predictions. Physics-Informed Neural Networks (PINNs) mitigate some of these issues by incorporating physical constraints in the learning process, yet they remain limited by mesh resolution and long-term predictive stability. In this work, we propose a Physics-Informed Neural Operator (PINO) approach to solve PDE problems in cardiac EP. Unlike PINNs, PINO models learn mappings between function spaces, allowing them to generalise to multiple mesh resolutions and initial conditions. Our results show that PINO models can accurately reproduce cardiac EP dynamics over extended time horizons and across multiple propagation scenarios, including zero-shot evaluations on scenarios unseen during training. Additionally, our PINO models maintain high predictive quality in long roll-outs (where predictions are recursively fed back as inputs), and can scale their predictive resolution by up to 10x the training resolution. These advantages come with a significant reduction in simulation time compared to numerical PDE solvers, highlighting the potential of PINO-based approaches for efficient and scalable cardiac EP simulations.

</details>


### [117] [Coherence Mechanisms for Provable Self-Improvement](https://arxiv.org/abs/2511.08440)
*Mehryar Mohri,Jon Schneider,Yifan Wu*

Main category: cs.LG

TL;DR: 提出一个基于一致性（coherence）的自我改进框架，通过投影机制把基线模型更新为在保持原行为尽可能接近的前提下实现输出的一致性；对直接投影和两步投影给出单调改进保证（期望Bregman散度下降），并在非可实现、有限样本分布与放松一致性约束下扩展，提出普遍性表征定理与刚性结论，表明一致性是可证明自我改进的必要基础。


<details>
  <summary>Details</summary>
Motivation: 当前自我改进方案多依赖经验启发，缺乏形式化保证。本文以一致性为核心，提出在任务保持不变性变换下仍能保持输出一致性并可获得理论上的改进保证，且在更宽的设定下实现鲁棒性与普遍性。

Method: 提出基于投影的更新机制：在基线模型上进行投影以达到一致性，同时尽量保持与原模型的距离。系统地分析直接投影与两步投影两种路径的单调改进性质；将分析扩展到非可实现情形、有限样本分布以及放松的一致性约束。

Result: 在给定框架下证明了单调改进：期望的Bregman散度下降；对直接投影与两步投影的全面理论分析；在非可实现、有限样本与放松约束条件下的鲁棒性扩展；提出普遍性表征定理，表明任何具有类似改进保证的机制必然遵循一致性结构，获得刚性结论。

Conclusion: 一致性是可证明自我改进的基本且必要原则，构成对投影式自我改进机制的关键理论支撑，未来研究应以此为核心准则与框架。

Abstract: Self-improvement is a critical capability for large language models and other intelligent systems, enabling them to refine their behavior and internal consistency without external supervision. Despite its importance, prior approaches largely rely on empirical heuristics and lack formal guarantees. In this paper, we propose a principled framework for self-improvement based on the concept of \emph{coherence}, which requires that a model's outputs remain consistent under task-preserving transformations of the input.
  We formalize this concept using projection-based mechanisms that update a baseline model to be coherent while remaining as close as possible to its original behavior. We provide rigorous theoretical guarantees that these mechanisms achieve \emph{monotonic improvement}, measured by a reduction in expected Bregman divergence. Our analysis is comprehensive, covering both \emph{direct} and \emph{two-step} projection methods, and robustly extends these guarantees to non-realizable settings, empirical (finite-sample) distributions, and relaxed coherence constraints.
  Furthermore, we establish a general \emph{characterization theorem}, showing that any mechanism with similar provable improvement guarantees must inherently conform to a coherence-based structure. This culminates in rigidity results under the demand for universal improvement, establishing coherence as a fundamental and, in a formal sense, necessary principle for provable self-improvement.

</details>


### [118] [One Model for All: Universal Pre-training for EEG based Emotion Recognition across Heterogeneous Datasets and Paradigms](https://arxiv.org/abs/2511.08444)
*Xiang Li,You Li,Yazhou Zhang*

Main category: cs.LG

TL;DR: 提出一个面向多数据集的 EEG 统一预训练框架 One Model for All，通过通道自监督预训练与 ART-GAT 的多变量微调，实现跨数据集的泛化和跨域迁移，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: EEG 数据集存在显著异质性（通道、受试者差异），导致模型难以泛化；需要一个统一、可迁移的预训练范式来缓解分布偏移。

Method: 分两阶段学习：1) 在单通道层面进行自监督对比学习的单变量预训练，结合统一通道模式 UCS，覆盖不同数据集的通道集合；2) 进行多通道时空建模的微调，采用 Adaptive Resampling Transformer (ART) 与 Graph Attention Network (GAT) 的联合结构以捕捉时空依赖。在 SEED 等数据集上进行统一的预训练以稳定训练，随后在 DEAP、DREAMER 等数据集上进行跨数据集微调并评估。

Result: 实验表明统一预训练是稳定器；在 SEED 上避免从零开始崩溃并带来提升；在 DEAP/ DREAMER 上分别提升 +7.65% 与 +3.55%；在单数据集上达到新 SOTA：SEED 99.27%、DEAP 93.69%、DREAMER 93.93%；跨数据集迁移在未见 DREAMER 达到 94.08%（intersection）和 93.05%（UCS），前者超过同域预训练基线。消融研究显示 GAT 模块关键性，相较 GCN 在 DEAP 上提升 22.19%，移除会导致 -16.44% 的性能下降。

Conclusion: 该框架推动 EEG 分析向更通用、可扩展的预训练模型发展，为多任务和跨数据集的应用提供更强的迁移能力。

Abstract: EEG-based emotion recognition is hampered by profound dataset heterogeneity (channel/subject variability), hindering generalizable models. Existing approaches struggle to transfer knowledge effectively. We propose 'One Model for All', a universal pre-training framework for EEG analysis across disparate datasets. Our paradigm decouples learning into two stages: (1) Univariate pre-training via self-supervised contrastive learning on individual channels, enabled by a Unified Channel Schema (UCS) that leverages the channel union (e.g., SEED-62ch, DEAP-32ch); (2) Multivariate fine-tuning with a novel 'ART' (Adaptive Resampling Transformer) and 'GAT' (Graph Attention Network) architecture to capture complex spatio-temporal dependencies. Experiments show universal pre-training is an essential stabilizer, preventing collapse on SEED (vs. scratch) and yielding substantial gains on DEAP (+7.65%) and DREAMER (+3.55%). Our framework achieves new SOTA performance on all within-subject benchmarks: SEED (99.27%), DEAP (93.69%), and DREAMER (93.93%). We also show SOTA cross-dataset transfer, achieving 94.08% (intersection) and 93.05% (UCS) on the unseen DREAMER dataset, with the former surpassing the within-domain pre-training benchmark. Ablation studies validate our architecture: the GAT module is critical, yielding a +22.19% gain over GCN on the high-noise DEAP dataset, and its removal causes a catastrophic -16.44% performance drop. This work paves the way for more universal, scalable, and effective pre-trained models for diverse EEG analysis tasks.

</details>


### [119] [Binary Split Categorical feature with Mean Absolute Error Criteria in CART](https://arxiv.org/abs/2511.08470)
*Peng Yu,Yike Chen,Chao Xu,Albert Bifet,Jesse Read*

Main category: cs.LG

TL;DR: 提出一种针对分类特征的基于MAE的高效分裂算法，并指出用于MAE的无监督数值编码方法不可行；对现有方法的局限性提供了证据。


<details>
  <summary>Details</summary>
Motivation: 在CART中对分类特征使用MAE作为分裂准则时，传统的数值编码方法需要先将类别编码为数值，导致信息损失和计算复杂度。作者认为无监督编码不能直接适用于MAE且需直接处理类别值以实现高效分裂。

Method: 首先论证无监督数值编码在MAE准则下不可行；随后提出一种新颖且高效的分裂算法，专门用于在MAE下处理分类特征；分析算法的复杂度与实现要点，可能包括对类别分组、排序或贪心选择的改进。

Result: 证明现有基于编码的MAE方法存在局限性，提供一种直接且高效处理分类变量的MAE分裂算法，显示对CART在分类数据上的处理能力的潜在提升。

Conclusion: MAE在分类特征上的分裂是可行的，但需合适的分裂策略；无监督编码在MAE场景下不可行；提出的算法可成为提升CART处理分类数据能力的可行方向。

Abstract: In the context of the Classification and Regression Trees (CART) algorithm, the efficient splitting of categorical features using standard criteria like GINI and Entropy is well-established. However, using the Mean Absolute Error (MAE) criterion for categorical features has traditionally relied on various numerical encoding methods. This paper demonstrates that unsupervised numerical encoding methods are not viable for the MAE criteria. Furthermore, we present a novel and efficient splitting algorithm that addresses the challenges of handling categorical features with the MAE criterion. Our findings underscore the limitations of existing approaches and offer a promising solution to enhance the handling of categorical data in CART algorithms.

</details>


### [120] [The Path Not Taken: RLVR Provably Learns Off the Principals](https://arxiv.org/abs/2511.08567)
*Hanqing Zhu,Zhenyu Zhang,Hanxian Huang,DiJia Su,Zechun Liu,Jiawei Zhao,Igor Fedorov,Hamed Pirsiavash,Zhizhou Sha,Jinwon Lee,David Z. Pan,Zhangyang Wang,Yuandong Tian,Kai Sheng Tai*

Main category: cs.LG

TL;DR: RLVR通过模型条件偏置，使参数更新在权重空间内局部化且沿非主方向进行，三门理论解释了这一现象，并指出RLVR与SFT存在本质差异，需几何感知的学习算法。


<details>
  <summary>Details</summary>
Motivation: 提供参数空间层面的因果理解，解释为何少量参数更新可以显著提升推理能力，并揭示稀疏性为何是表面现象。

Method: 提出Three-Gate Theory：Gate I KL Anchor 限制更新、Gate II Model Geometry 将步长引导至低曲率、保持谱的子空间、Gate III Precision 将微更新隐藏在非首选区域；结合实证验证、谱分析，以及与SFT/LoRA等PEFT方法的对比，给出参数空间的学习动力学定义。

Result: 更新集中在特定参数区域且跨实验一致；Off-principal更新带来收益，谱漂移极小，主子空间旋转受控；与SFT相比，RLVR更有效，且SFT易扭曲谱与主方向；首次给出参数空间的学习动力学表征。

Conclusion: RLVR走的是与SFT不同的优化范式，需设计几何感知且适用于RLVR的原生学习算法，而非简单迁移SFT时代的PEFT方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.
  Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.

</details>


### [121] [Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms](https://arxiv.org/abs/2511.08570)
*Jamison Moody,James Usevitch*

Main category: cs.LG

TL;DR: AdaptKAN introduces a data-driven, histogram-based domain adaptation for Kolmogorov-Arnold Networks (KANs), enabling autonomous domain grid updates during training. It aims to reduce manual overhead while preserving or improving accuracy and interpretability, with added OOD detection capabilities.


<details>
  <summary>Details</summary>
Motivation: KANs offer interpretable, symbolic-function learning via trainable activations, but require manual and potentially costly domain grid adjustments. An autonomous, data-driven domain adaptation could reduce overhead and improve performance, including robustness to out-of-distribution inputs.

Method: AdaptKAN employs a histogram-based algorithm to adjust the domain grid (activation input ranges) during training, guided by the changing output ranges of earlier layers, in conjunction with parameterized, trainable activation functions to maintain interpretability and facilitating learning of symbolic equations.

Result: AdaptKAN matches or exceeds the performance of prior KANs and MLPs across four tasks: learning scientific equations from the Feynman dataset, image classification from frozen features, learning a control Lyapunov function, and detecting OOD inputs on the OpenOOD v1.5 benchmark.

Conclusion: AdaptKAN provides autonomous domain adaptation for KANs, reducing training overhead while maintaining or improving accuracy and interpretability, and offering effective OOD detection capabilities.

Abstract: Kolmogorov-Arnold Networks (KANs) are a class of neural networks that have received increased attention in recent literature. In contrast to MLPs, KANs leverage parameterized, trainable activation functions and offer several benefits including improved interpretability and higher accuracy on learning symbolic equations. However, the original KAN architecture requires adjustments to the domain discretization of the network (called the "domain grid") during training, creating extra overhead for the user in the training process. Typical KAN layers are not designed with the ability to autonomously update their domains in a data-driven manner informed by the changing output ranges of previous layers. As an added benefit, this histogram algorithm may also be applied towards detecting out-of-distribution (OOD) inputs in a variety of settings. We demonstrate that AdaptKAN exceeds or matches the performance of prior KAN architectures and MLPs on four different tasks: learning scientific equations from the Feynman dataset, image classification from frozen features, learning a control Lyapunov function, and detecting OOD inputs on the OpenOOD v1.5 benchmark.

</details>
