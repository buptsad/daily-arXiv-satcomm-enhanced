<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.CR](#cs.CR) [Total: 15]
- [eess.SP](#eess.SP) [Total: 9]
- [cs.IT](#cs.IT) [Total: 4]
- [eess.SY](#eess.SY) [Total: 13]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Millimeter-Wave UAV Channel Model with Height-Dependent Path Loss and Shadowing in Urban Scenarios](https://arxiv.org/abs/2511.10763)
*Abdul Saboor,Evgenii Vinogradov*

Main category: cs.NI

TL;DR: 提出一个高度相关的mmWave ABS通道模型，用于在城市环境中评估高度与城市几何对LoS概率、大尺度衰落和路径损耗的影响，且几何结构对PLE有小但一致的影响。通过26 GHz的射线追踪和大规模城市 realizations进行验证，给出一个实用的高度依赖LSF模型。


<details>
  <summary>Details</summary>
Motivation: 在6G mmWave A2G场景下，ABS高度和城市几何影响对LoS概率与衰落的敏感性尚未被充分量化。本研究旨在将高度与城市几何纳入统一的通道模型，以提升ABS规划与覆盖评估的准确性。

Method: 使用MATLAB射线追踪在26 GHz对约1e4个城市 realizations的四种布局进行仿真，这些布局在建筑体量分布相同的前提下，其空间组织不同。通过对提升角度的LoS概率使用sigmoid模型提取，并对高度相关的路径损耗指数(PLE)及阴影衰落趋势进行指数拟合以得到随高度变化的参数。

Result: NLoS的PLE在高海拔处趋向2.5-3，LoS的PLE保持在约2，阴影衰减随高度降低。几何布局对PLE的影响虽小却稳定存在（大约±0.2的变化），即使建筑总量参数相同。所提出的统一模型与射线追踪统计吻合良好，提供了一个适用于复杂城市场景的高度相关LSF模型。

Conclusion: 所提出的高度相关LSF模型可用于ABS在复杂城市环境中的规划与评估，强调几何布局对通道参数有可辨识但有限的影响，并且模型与射线追踪结果一致。

Abstract: Uncrewed Aerial Vehicles (UAVs) serving as Aerial Base Stations (ABSs) are expected to extend 6G millimeter-Wave (mmWave) coverage and improve link reliability in urban areas. However, UAV-based Air-to-Ground (A2G) channels are highly dependent on height and urban geometry. This paper proposes an ABS height-dependent mmWave channel model and investigates whether urban geometry, beyond the standard built-up parameters, significantly affects LoS probability (PLoS) and Large-Scale Fading (LSF). Using MATLAB ray tracing at 26 GHz, we simulate approximately 10K city realizations for four urban layouts that share identical built-up parameters but differ in their spatial organization. We extract elevation-based PLoS using a sigmoid model and derive height-dependent Path-Loss Exponents (PLEs) and shadow-fading trends using exponential fits. Results show that PLE for Non-Line-of-Sight (NLoS) decreases toward 2.5-3 at high altitudes, Line-of-Sight (LoS) PLE remains near 2, and shadow fading reduces with height. We also find that geometric layout introduces a modest but consistent change in PLE (+/- 0.2), even when built-up parameters are fixed. The proposed unified model aligns well with ray-tracing statistics and offers a practical, height-dependent LSF model suitable for ABS planning in complex urban scenarios.

</details>


### [2] [Advancing IoT System Dependability: A Deep Dive into Management and Operation Plane Separation](https://arxiv.org/abs/2511.11204)
*Luoyao Hao,Shuo Zhang,Henning Schulzrinne*

Main category: cs.NI

TL;DR: 提出通过分离管理平面与操作平面的IoT体系结构，并以身份无关的灵活描述符策略框架实现对安全、标准、能耗等全局策略的部署与执行，且整合监管机构与制造商等多方实体；在三个数据集上实现近似最优的策略表达能力与稳健执行。


<details>
  <summary>Details</summary>
Motivation: 为提升大规模IoT系统的可依赖性，解决现有操作平面难以统一全局策略、资源约束与跨机构协同的问题，提出独立的管理平面来统一策略并对系统变动具备适应性。

Method: 设计一个管理平面架构，核心是身份无关的策略框架，采用灵活的描述符替代固定标识符，以便在系统变化时仍能前瞻性地部署和执行策略；并将监管机构、制造商等多方实体纳入管理生态，保持现有IoT运营工作流不变；在三个数据集上评估策略表达力与执行稳健性。

Result: 实验结果显示该框架在表达力方面接近最优，并能实现稳健的策略执行，在三个数据集上均有良好表现。

Conclusion: 通过分离管理与运营、并采用身份无关、描述符驱动的策略框架，提升了系统的可依赖性与适应性；未来可拓展跨域协同与对动态环境的自适应能力。

Abstract: We propose to enhance the dependability of large-scale IoT systems by separating the management and operation plane. We innovate the management plane to enforce overarching policies, such as safety norms, operation standards, and energy restrictions, and integrate multi-faceted management entities, including regulatory agencies and manufacturers, while the current IoT operational workflow remains unchanged. Central to the management plane is a meticulously designed, identity-independent policy framework that employs flexible descriptors rather than fixed identifiers, allowing for proactive deployment of overarching policies with adaptability to system changes. Our evaluation across three datasets indicates that the proposed framework can achieve near-optimal expressiveness and dependable policy enforcement.

</details>


### [3] [Use Cases, Metrics, and Challenges of Nomadic Non-Public Networks for the 6G Standardization](https://arxiv.org/abs/2511.11217)
*Daniel Lindenschmitt,Michael Gundall,Ainur Daurembekova,Marcos Rates Crippa,Mohammad Asif Habibi,Bin Han,Philipp Rosemann,Dennis Krummacker,Benedikt Veith,Hans D. Schotten*

Main category: cs.NI

TL;DR: NNPNs extend non-public networks (NPNs) with nomadic, mobile, self-organizing capabilities to support 6G. The paper reviews architecture, dynamic resource allocation, and backhaul, presents use cases, defines KPIs, and proposes a framework to categorize NNPNs by mobility and requirements. It also discusses architectural, regulatory, and security challenges and emphasizes standardization implications.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional static infrastructures by enabling dynamic, portable, and self-organizing private networks for diverse 6G scenarios (emergency, transport, agriculture, etc.), and to align NNPN development with ongoing standardization efforts.

Method: Literature and conceptual analysis of NNPN concepts; proposal of KPIs and a framework for categorization; examination of use cases and technical/regulatory challenges; discussion aimed at informing standardization.

Result: A structured view of NNPNs outlining their architecture, dynamic resource allocation, and backhauling; a set of KPIs for evaluation; a mobility-based categorization framework; identified challenges (handover, spectrum policy, cross-border, trust) and potential applications demonstrating improved connectivity.

Conclusion: NNPNs offer significant benefits for 6G generation but require adaptable architectures and regulatory policies. The paper contributes to standardization efforts by outlining evaluation criteria, use cases, and challenges to guide integration of NNPNs into future networks.

Abstract: Wireless communication is evolving with the adoption of dynamic and self-organizing networks. They are expected to play a crucial role in shaping sixth-generation (6G) systems and the ongoing standardization process. The concept of non-public networks (NPNs) introduced in fifth-generation (5G) will be enhanced by nomadic non-public networks (NNPNs), extending mobility and adaptability beyond fixed locations. These networks help overcome the limitations of traditional static infrastructures, making them applicable to areas such as emergency response, transportation, agriculture, and others. This paper examines the transition from NPNs to NNPNs, highlighting key technical aspects such as network architecture, dynamic resource allocation, and wireless backhauling. Several use cases illustrate how NNPNs improve connectivity in environments where traditional networks are limited. Additionally, the study defines Key Performance Indicators (KPIs) to evaluate NNPN applications and establishes a framework for categorizing them based on mobility and operational requirements. Despite their advantages, NNPNs introduce architectural, regulatory, and security challenges such as new approaches for handovers, spectrum policies or cross-border functionality, and trust mechanisms to maintain reliable operations. By identifying use cases, defining evaluation criteria, and addressing technical and regulatory challenges, this paper provides insights into integrating NNPNs into future 6G networks. These findings contribute to ongoing standardization efforts and emphasize the need for adaptable policies and network architectures to maximize the benefits of NNPNs in next-generation communication systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups](https://arxiv.org/abs/2511.10683)
*Masih Aminbeidokhti,Subhankar Roy,Eric Granger,Elisa Ricci,Marco Pedersoli*

Main category: cs.LG

TL;DR: LT-Soups: 一种两阶段模型汤合方法，在长尾分布下通过在 balanced 子集上进行模型平均来降低头分类偏置，再仅对分类器在完整数据集上微调以恢复头部准确率，从而在多种 LT 情况下超越 PEFT 和传统模型汤合的权衡。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据通常呈现长尾分布，少数头部类别占主导，尾部类别严重不足。尽管 PEFT（如 LoRA、AdaptFormer）能在细调大模型（如 CLIP）时保持尾部性能，但往往以牺牲头部准确率为代价。头尾比（head-tail ratio，η）作为一个关键但被忽视的因素，影响这类权衡。

Method: 在 CIFAR100 上通过控制不平衡比ρ和头尾比η进行系统实验；提出 LT-Soups：第一阶段在平衡子集上微调的模型进行平均以降低头部偏置；第二阶段在完整数据集上仅对分类器进行微调以恢复头部准确率。并在六个基准数据集上评估，与 PEFT 和传统模型汤合方法比较。

Result: 在尾部占优势的情形，PEFT 能表现出色，但在更平衡或头部占优的分布中表现下降。LT-Soups 在多种 LT 场景下实现更优的头尾权衡，并在六个数据集上优于 PEFT 和传统模型汤合方法。

Conclusion: LT-Soups 提供了一种对长尾分布鲁棒且可泛化的两阶段汤合框架，通过先减弱头部偏置、再在全量数据上恢复头部准确性，获得比现有方法更优的权衡。

Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.

</details>


### [5] [Differentiable Sparse Identification of Lagrangian Dynamics](https://arxiv.org/abs/2511.10706)
*Zitong Zhang,Hao Sun*

Main category: cs.LG

TL;DR: 提出一种可微分的稀疏Lagrangian系统识别框架，结合三次B样条近似，提升在噪声数据下对复杂力学系统的非线性规律的发现能力。


<details>
  <summary>Details</summary>
Motivation: 解决基于稀疏回归的方法在有理函数形式和噪声敏感性方面的局限；Lagrangian形式因为通常避免有理表达式且对系统动力学具有简洁性，但现有Lagrangian识别在测量噪声和数据有限条件下表现不佳；需要一种能鲁棒利用观测并融入物理约束的方法。

Method: 将三次B样条近似引入Lagrangian系统识别；提出鲁棒方程发现机制，结合已知物理约束；基于B样条基函数的递归导数计算方案，约束高阶导数并降低二阶动力系统中的噪声敏感性。

Result: 在复杂力学系统上的实验/仿真展示，相较于基线方法取得更优性能，能够在有噪声数据下更准确可靠地提取物理规律。

Conclusion: 该可微分稀疏识别框架提升了在噪声和数据受限条件下的Lagrangian识别能力，使其在非线性力学系统中更有效地发现 governing equations。

Abstract: Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.

</details>


### [6] [Towards Uncertainty Quantification in Generative Model Learning](https://arxiv.org/abs/2511.10710)
*Giorgio Morales,Frederic Jurie,Jalal Fadili*

Main category: cs.LG

TL;DR: 本文探讨生成模型的不确定性量化，提出聚合精度-召回曲线来评估分布逼近的不确定性，并通过合成数据实验初步验证该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前对生成模型的评估多聚焦于目标分布与学习分布之间的距离，忽视对这些测量本身的不确定性。需要形式化不确定性量化以便更公平地比较不同模型架构。

Method: 提出在生成模型学习中引入不确定性量化的框架，讨论使用基于集成的精度-召回曲线来描述分布逼近的不确定性；在合成数据上进行初步实验，验证聚合的精度-召回曲线在捕捉模型近似不确定性方面的有效性。

Result: 初步实验表明聚合的精度-召回曲线能够捕捉模型逼近的不确定性，提供了一种可用于系统比较不同模型架构不确定性的工具。

Conclusion: 本文为生成模型的可靠性评估提供新方向，强调不确定性量化的重要性，未来工作包括在真实数据和更多模型族上扩展验证。

Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.

</details>


### [7] [Movement-Specific Analysis for FIM Score Classification Using Spatio-Temporal Deep Learning](https://arxiv.org/abs/2511.10713)
*Jun Masaki,Ariaki Higashi,Naoko Shinagawa,Kazuhiko Hirata,Yuichi Kurita,Akira Furui*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The functional independence measure (FIM) is widely used to evaluate patients' physical independence in activities of daily living. However, traditional FIM assessment imposes a significant burden on both patients and healthcare professionals. To address this challenge, we propose an automated FIM score estimation method that utilizes simple exercises different from the designated FIM assessment actions. Our approach employs a deep neural network architecture integrating a spatial-temporal graph convolutional network (ST-GCN), bidirectional long short-term memory (BiLSTM), and an attention mechanism to estimate FIM motor item scores. The model effectively captures long-term temporal dependencies and identifies key body-joint contributions through learned attention weights. We evaluated our method in a study of 277 rehabilitation patients, focusing on FIM transfer and locomotion items. Our approach successfully distinguishes between completely independent patients and those requiring assistance, achieving balanced accuracies of 70.09-78.79 % across different FIM items. Additionally, our analysis reveals specific movement patterns that serve as reliable predictors for particular FIM evaluation items.

</details>


### [8] [Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions](https://arxiv.org/abs/2511.10809)
*Jiazhou Liang,Hassan Khurram,Scott Sanner*

Main category: cs.LG

TL;DR: 提出两种近似全局优化LPC的新方法：利用可分性的理论属性实现近似且有误差边界的MIP简化，以及将LPC近似为QPBO以提升计算效率，在合成与真实数据集上表现出接近全局最优的回归效果和更好的可扩展性，相较贪婪优化和现有MIP具有更低回归误差。


<details>
  <summary>Details</summary>
Motivation: 解决线性预测聚类在全局最优性与可扩展性之间的权衡。现有贪婪方法在可分情形有效但对非可分/簇重叠的情形表现不佳；而MIP尽管保证全局最优，但计算复杂度严重限制其应用。

Method: 基于可分性理论属性提出两类近似：一方面在保持全局性约束的前提下降低MIP的复杂度，并给出可证明的误差界；另一方面将LPC近似为二次布尔优化（QPBO）问题以获得显著的计算提升。

Result: 在合成数据和真实数据集上，该方法能够持续得到接近全局最优的解，回归误差明显低于贪婪优化方法；相较于现有MIP实现，方法在可扩展性方面具有显著提升。

Conclusion: 通过利用可分性等理论特性，本文为LPC提供了高效且具有理论保证的近似全局优化框架，提升了实用性与可扩展性。

Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.

</details>


### [9] [Transformers know more than they can tell -- Learning the Collatz sequence](https://arxiv.org/abs/2511.10811)
*François Charton,Ashvni Narayanan*

Main category: cs.LG

TL;DR: Transformer 在预测长 Collatz 步骤方面表现出显著的基序差异：输入输出的表示基数对准确率有显著影响，但总体呈现一个共同的学习模式，即模型把输入按与二进制表示相关的模 2^p 划分成类别，在这些类别上几乎达到完美正确，而对其他输入的准确率则较低。失败分析显示大多数错误是对循环长度的估计不准确（而非幻觉），幻觉几乎不发生。这一发现揭示了模型在学习复杂算术函数时对控制结构的依赖性。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示大型语言模型在处理复杂算术问题时的学习对象与结构，特别是循环长度等控制结构如何影响学习难度，并通过数学问题来理解模型内部的学习算法，以便解释和可能改进语言模型的能力。

Method: 系统地评估变换器在不同进制下对长 Collatz 步骤的预测准确性，比较不同基编码对输入/输出的影响；分析输入的二进制结构与模 2^p 的分类关系；对失败案例进行模式分析，区分计算正确性与循环长度估计的错位；统计幻觉的发生率并观察错因模式。

Result: 准确率随基数而显著波动：在基数 24 和 32 下最高可达 99.7%，在基数 11 和 3 下最低可达 37% 与 25% 左右。另外，模型对属于某些模 2^p 的输入类别几乎能实现近乎完美的准确性，而对其他输入的准确率均低于 1%；这与 Collatz 序列的一个几何特性相关：长步骤中循环长度能从输入的二进制表示中推断出来。学习模式表现为对具有更长循环长度的输入类别的能力的提升。失败分析显示几乎所有错误都遵循可预测的模式；幻觉几乎从不发生，在超过 90% 的失败中，模型完成了正确的计算但错误估计了循环长度。

Conclusion: 该研究用数学问题作为工具，揭示了模型在学习复杂算术函数时对控制结构的依赖性，尤其是循环长度的推断能力成为学习难点。该方法可用于理解、解释乃至改进语言模型，并有望推广到其他复杂问题的研究中。

Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.

</details>


### [10] [Towards Universal Neural Operators through Multiphysics Pretraining](https://arxiv.org/abs/2511.10829)
*Mikhail Masliaev,Dmitry Gusarov,Ilya Markov,Alexander Hvatov*

Main category: cs.LG

TL;DR: Transformer-based neural operators can transfer知识 across diverse PDE problems and adapt to unseen parameters and new variables through downstream fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Neural operators are computationally expensive to train; exploring transfer learning with transformers across PDEs could reduce cost and improve generalization.

Method: Evaluate transformer-based neural operators on a suite of PDE tasks, testing extrapolation to unseen parameters, adding new variables, and transfer from multi-equation datasets, using downstream fine-tuning.

Result: Empirical results show that advanced neural operator architectures can effectively transfer knowledge across PDE problems, enabling adaptation to varied settings and datasets.

Conclusion: Transformer-based neural operators offer promising generalizable transfer learning for PDEs, enabling efficient learning across related problems.

Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.

</details>


### [11] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: 提出了一种变分量子核框架，使用资源高效的量子化简并结合参数缩放，在八个真实高维数据集上实现基于量子核的分类，并在仿真中显示出对经典核（如RBF）的性能优势；为量子核在现实世界数据上的应用奠定基础，需进一步研究实际量子优势。


<details>
  <summary>Details</summary>
Motivation: 当前研究多局限于低维或合成数据，难以评估量子核在高维现实数据上的潜力，需对多模态、真实世界数据的分类任务进行系统评估。

Method: 提出一个变分量子核框架，采用资源高效的 Ansatz，并引入参数缩放以加速收敛；在八个真实高维数据集（表格、图像、时间序列、图数据）上进行广泛基准测试，所有实验为经典仿真。

Result: 所提出的量子核在仿真实验中对标准经典核（如RBF）表现出明显的性能优势。

Conclusion: 设计良好的量子核可以成为多功能的高性能工具，为现实世界的量子增强机器学习奠定基础；但需要更多研究来充分评估实际的量子优势。

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [12] [SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?](https://arxiv.org/abs/2511.10833)
*Sanchit Kabra,Shobhnik Kriplani,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.LG

TL;DR: 提出 SurfaceBench：首个覆盖表面级符号发现的综合基准，包含183个任务、15个类别，涵盖显式/隐式/参数方程，提供地面真值、变量语义和三维数据；引入几何感知评估（Chamfer、Hausdorff），挑战现有大语言模型在跨表示类型与表面复杂度上的泛化能力；并对科学领域的表面重建和符号推理提出基准，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归基准多聚焦标量函数，忽视表面结构与物理几何意义，且评估多依赖脆弱的字符串匹配。需要一个能覆盖显式/隐式/参数表面表示、具备领域 grounding 的综合基准，以推动对组合泛化和几何重建的评估。

Method: 设计并实现 SurfaceBench：183个任务，覆盖15个符号复杂度类别，涵盖显式、隐式和参数方程的表示形式。每个任务给出地面真值方程、变量语义和合成的三维数据。通过引入几何感知指标（Chamfer、Hausdorff）与符号正确性评估，来评估方程的代数保真与空间重建准确性。任务设计旨抵抗 LLM 的记忆化倾向，通过新颖的符号组合提升对分布外的鲁棒性。

Result: 实验显示最先进框架在特定族群上偶有成功，但在跨表示类型与表面复杂度的泛化上显著不足，SurfaceBench 具有挑战性和诊断性，揭示了符号推理与几何重建耦合的难题。该基准为评估组合泛化和几何感知推理提供了更全面的诊断工具。

Conclusion: SurfaceBench 为符号发现研究提供一个具有挑战性和诊断性的测试床，帮助量化进展在组合泛化、数据驱动的科学归纳以及面向几何感知的推理方面，同时促进 LLM 在符号推理与几何重建中的进展，代码已开源。

Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench

</details>


### [13] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 提出一个区分外在（extrinsic）和内在（intrinsic）幻觉的评估框架，并结合基于注意力的、不确定性量化算法提出新的注意力聚合策略以提升幻觉检测与可解释性。对于外在幻觉，采样式方法（如语义熵）效果较好；对内在幻觉，利用注意力聚合的模型更具优势。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全关键领域的应用中易产生幻觉，需要区分幻觉类型并提高检测与可解释性，以便更可靠地部署。现有方法要么计算成本高，要么未区分幻觉类型，降低了检测的针对性和效率。

Method: 提出一个区分 Extrinsic 与 Intrinsic 的基准评估框架，整合基于注意力的不确定性量化算法，并提出新的注意力聚合策略对输入标记进行聚合以提升可解释性和幻觉检测性能。对比分析了基于采样的语义熵等方法在不同幻觉类型上的表现，验证新方法在内在幻觉上的优势。

Result: 实验表明：采样式方法如语义熵在检测 Extrinsic 幻觉方面效果良好，但在 Intrinsic 幻觉上通常失效；而基于注意力聚合的检测方法在 Intrinsic 幻觉上表现更佳，整体上将注意力信号视为不确定性的重要来源并提升了对幻觉的识别与解释能力。

Conclusion: 区分幻觉类型并针对性的设计检测策略能显著提升在安全关键场景中的幻觉检测效果和可解释性；注意力信号是量化模型不确定性的重要资源，未来可进一步优化注意力聚合策略以覆盖更广的幻觉类型。

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [14] [FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification](https://arxiv.org/abs/2511.10841)
*YongKyung Oh,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: FlowPath learns an invertible, data-adaptive control path for neural controlled differential equations, outperforming fixed interpolants on irregular time series.


<details>
  <summary>Details</summary>
Motivation: Modeling continuous-time dynamics from sparse, irregular observations is hard. Fixed interpolation imposes simplistic geometry that can misrepresent the data manifold, especially with high missingness. A learnable, invertible path geometry can preserve information and better capture dynamics.

Method: Use an invertible neural flow to model the control path geometry, constructing a continuous, data-adaptive manifold. Invertibility enforces information-preserving transformations, differentiable with respect to observations, and provides a principled inductive bias beyond unconstrained path learning.

Result: Empirical evaluation on 18 benchmark datasets and a real-world case study shows FlowPath achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures.

Conclusion: Modeling the geometry of the control path is crucial for learning from irregular time series; FlowPath offers a robust, generalizable solution by learning an invertible, data-adaptive path.

Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.

</details>


### [15] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 通过使用精心设计的行为策略收集数据以获得更低方差的回报估计，并将其扩展到在线强化学习中的策略梯度方法，提升样本效率和表现。


<details>
  <summary>Details</summary>
Motivation: 高方差回报估计导致样本效率低和训练不稳定；离策略评估的最新结果表明，合适的行为策略能在离策略数据中获得更低方差的回报估计；希望将这一洞见应用于在线RL。

Method: 在单一工作者设定下，将离策略数据用于策略评估与改进的交错过程；将这一 regime 引入两种策略梯度方法，结合对方差进行理论控制（包括使用重要性加权的去偏和修剪以管理方差）；实验在多种环境中验证样本效率提升。

Result: 在多种环境中，扩展后的两种策略梯度方法显示更好的样本效率和性能。

Conclusion: 离策略数据来自精心设计的行为策略可实现更低方差的回报估计，从而改善在线RL中的学习效率和稳定性；单工作者设定也能实现这一优势。

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [16] [Multistability of Self-Attention Dynamics in Transformers](https://arxiv.org/abs/2511.11553)
*Claudio Altafini*

Main category: cs.LG

TL;DR: Self-attention dynamics act like a multiagent Oja flow; equilibria include consensus, bipartite consensus, clustering, and polygonal states; stable equilibria often align with eigenvectors of the value matrix, especially the principal one.


<details>
  <summary>Details</summary>
Motivation: 揭示自注意力在连续时间下的动力学性质，连接注意力机制与特征值分解，理解稳定性和均衡结构。

Method: 将自注意力建模为一个连续时间的多智能体系统，将其与Oja流联系起来；在单头情形下对平衡态进行分类并分析稳定性与对特征向量的对齐。

Result: 得到四类平衡态的分类；第一类和第二类平衡态往往存在多重共存且具有局部稳定性；前两类平衡态总是与值矩阵的特征向量对齐，且常常与主特征向量对齐，但并非总是。

Conclusion: 自注意力的动力学揭示了与值矩阵特征结构相关的主要成分解释，以及丰富的稳定平衡态集合，为理解变换器的行为提供几何与动力学视角。

Abstract: In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.

</details>


### [17] [Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics](https://arxiv.org/abs/2511.10878)
*Shuhao Ma,Zeyi Huang,Yu Cao,Wesley Doorsamy,Chaoyang Shi,Jun Li,Zhi-Qiang Zhang*

Main category: cs.LG

TL;DR: 提出一个物理信息驱动的多关节跨注意力BiGRU框架 PI-MJCA-BiGRU，直接从运动学估计肌肉激活与肌肉力，在无需标注数据的情况下实现时间高效推断，且 MJCA 提升关节间协调建模。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在多关节系统中的高计算成本和缺乏高质量标注数据的问题，需高效且无需标注数据的肌肉激活与力估计方法。

Method: 提出多关节跨注意力（MJCA）模块与双向门控循环单元（BiGRU）的组合，在损失函数中嵌入多关节动力学、关节耦合和外力相互作用的物理约束，实现物理信息驱动的无标注学习以估计肌肉激活与力。

Result: 在两组数据集上进行实验，PI-MJCA-BiGRU 的预测与传统有监督方法相当；MJCA 模块显著提升了对关节间协调的建模能力，相比基线表现更优。

Conclusion: 该框架实现了时间高效的推断并在无标注数据条件下给出生理一致的预测，MJCA 的跨关节协同建模效果显著优于其他方法。

Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.

</details>


### [18] [STAMP: Spatial-Temporal Adapter with Multi-Head Pooling](https://arxiv.org/abs/2511.10848)
*Brad Shook,Abby Turner,Jieshi Chen,Michał Wiliński,Mononito Goswami,Jonathan Elmer,Artur Dubrawski*

Main category: cs.LG

TL;DR: STAMP 通过在通用时间序列 foundation model 的单变量嵌入基础上，提出一种空间-时序适配器并结合多头池化，以隐式建模 EEG 的空间-时间特征，从而在 8 个 EEG 临床任务数据集上的评估中，与最先进 EEG 专用模型相当，同时具备参数量小、输入灵活的优点。


<details>
  <summary>Details</summary>
Motivation: 尽管已有面向 EEG 的基础模型，但尚缺乏对 EEG 专用 foundation 模型与通用 TSFM 在 EEG 任务上的系统对比分析。本研究旨在评估通用 TSFM 在 EEG 上的可迁移性，并提出高效的适配策略。

Method: 提出 STAMP：一个基于通用 TSFM 的空间-时序适配器，结合多头池化来捕捉 EEG 的空间-时间结构。它利用通用 TSFM 输出的单变量嵌入，隐式建模 EEG 的空间-时间模式，且参数量小、输入灵活，便于将 TSFM 应用于 EEG 数据。

Result: 对 8 个 EEG 临床任务数据集进行系统分析和消融研究，结果显示 STAMP 的性能可与当前状态的 EEG 专用模型相当。

Conclusion: 该适配器为在不训练大型 EEG 专用模型的前提下，使用广域 TSFM 来处理 EEG 提供了高效且可扩展的解决方案，具有较好的灵活性与轻量性。

Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.

</details>


### [19] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: ExPairT-LLM提出一种用于代码选择的精确学习算法，通过对LLM提出两种新类型的查询（成对成员性和成对等价性），并结合竞赛式筛选过程，在不同数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的代码选择算法要么无法区分等价与非等价的候选程序，要么过度依赖LLM给出每个输入的正确输出，导致错误的程序被选中，因此需要一种对LLM友好且更鲁棒的学习框架。

Method: 提出两种新的对LLM的成对查询：成对成员性查询和成对等价性查询。通过一个竞赛（tournament）机制对候选程序进行筛选，使得算法对LLM的错误具有鲁棒性，达到一个精确学习的目标。

Result: 在四个流行的代码数据集上，pass@1（成功率）平均提升 +13.0%，最大提升达到 +27.1%。此外，对进行复杂推理的LLM，其pass@1也提升了 +24.0%。

Conclusion: 通过更简化的LLM查询和一个基于精确学习的竞赛框架，ExPairT-LLM实现了更鲁棒且更高效的代码选择，显著优于现有最先进方法。

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [20] [Private Zeroth-Order Optimization with Public Data](https://arxiv.org/abs/2511.10859)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: PAZO introduces public-data-assisted zeroth-order optimizers to enhance private zeroth-order training by leveraging public information for gradient approximation, yielding better privacy/utility tradeoffs than private DP-SGD with public data and up to 16x speedup.


<details>
  <summary>Details</summary>
Motivation: Zeroth-order DP methods are attractive due to lower computational and memory demands, but their utilities lag behind first-order DP methods like DP-SGD. Public data offers a valuable signal to improve gradient estimates while maintaining privacy, potentially closing the performance gap.

Method: Propose PAZO framework that uses public data to guide and refine gradient approximation in zeroth-order algorithms with minimal overhead. Provide theoretical analysis under an assumption of similarity between public and private data distributions. Validate empirically on vision and text tasks in both pre-training and fine-tuning, comparing against first-order baselines (with public data) and private baselines, and report privacy-utility tradeoffs and runtime speedups.

Result: The PAZO framework achieves superior privacy/utility tradeoffs on vision and text tasks for both pre-training and fine-tuning. It outperforms the best first-order baselines that utilize public data in highly private settings and delivers up to 16× runtime speedup over traditional methods.

Conclusion: Leveraging public information to guide gradient estimation in zeroth-order private optimization is an effective strategy. PAZO demonstrates substantial gains in privacy-utility efficiency and computational speed, suggesting broader applicability to DP training where gradient information is expensive to compute.

Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.

</details>


### [21] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: Fine-tuning with GO UT Bench improves unit test generation for Go LLMs, addressing data imbalance; 75% tasks improved across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Code LLMs suffer from data imbalance: heavy representation of raw code, underrepresentation of software engineering tasks like unit test generation, especially in low-resource languages such as Go; need benchmark and fine-tuning opportunities to align models with real developer workflows.

Method: Propose GO UT Bench: 5264 code-unit test pairs from 10 permissively licensed Go repositories across domains. Evaluate fine-tuning across two LLM families (mixture-of-experts and dense decoders) and compare fine-tuned vs base models on Go unit-test generation tasks.

Result: Finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

Conclusion: GO UT Bench effectively facilitates fine-tuning to improve Go unit test generation, helping bridge the gap between code autocompletion and real-world software engineering workflows; demonstrates dataset utility across LLM families.

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [22] [GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning](https://arxiv.org/abs/2511.10936)
*Ying Song,Balaji Palanisamy*

Main category: cs.LG

TL;DR: 提出 GraphToxin，这是首个针对图去学习的图重构攻击；通过曲率匹配模块实现对被删除数据的精细化恢复，扩展到对多节点删除的白盒/黑盒攻击，并证明现有防御措施大多无效，强调需要更鲁棒的防御。


<details>
  <summary>Details</summary>
Motivation: 随着隐私法规对“忘记权”的要求，图数据的删除后仍可能留存痕迹，导致未删除信息可被恢复，损害去学习的隐私保证。

Method: 提出曲率匹配模块引导对去学习后图的全量恢复；在白盒与黑盒场景下扩展到多节点删除，建立 worst-case 与随机删除的评估框架。

Result: 实验表明 GraphToxin 能恢复被删除个体的信息、个人关系甚至连接中的敏感内容；现有防御对其几乎无效，某些情况下还放大攻击效果；攻击在随机及最坏-case 删除下具有良好表现。

Conclusion: 图去学习的隐私保护需要更强的防御机制，需开展更系统的 worst-case 评估框架，推动开发更鲁棒的防御策略。

Abstract: Graph unlearning has emerged as a promising solution for complying with "the right to be forgotten" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.

</details>


### [23] [Neural Network-Powered Finger-Drawn Biometric Authentication](https://arxiv.org/abs/2511.11235)
*Maan Al Balkhi,Kordian Gontarska,Marko Harasic,Adrian Paschke*

Main category: cs.LG

TL;DR: Using finger-drawn digits on touchscreens as a biometric, evaluating CNNs (modified Inception-V1 and lightweight CNN) and autoencoders for authentication. Both CNNs achieved ~89% accuracy; autoencoders ~75%. Data from 20 participants, 2000 samples each.


<details>
  <summary>Details</summary>
Motivation: To explore a user-friendly, secure biometric signal on mobile devices by leveraging natural finger-drawn digit patterns as an alternative or augmentation to PIN/pattern-based authentication.

Method: Collecting 2,000 finger-drawn digits per 20 participants for digits 0-9. Evaluating two CNN architectures (a modified Inception-V1 and a lightweight CNN) for user authentication. Also evaluating Convolutional and Fully Connected autoencoders for anomaly detection.

Result: CNNs achieved ~89% authentication accuracy with the shallow CNN using fewer parameters. Autoencoders achieved ~75% accuracy. The approach supports multi-layered security by combining with existing pattern-based methods.

Conclusion: Finger-drawn symbol authentication can be a viable, secure, and user-friendly biometric for touchscreen devices and can augment traditional pattern-based authentication to form multi-layer security for mobile apps.

Abstract: This paper investigates neural network-based biometric authentication using finger-drawn digits on touchscreen devices. We evaluated CNN and autoencoder architectures for user authentication through simple digit patterns (0-9) traced with finger input. Twenty participants contributed 2,000 finger-drawn digits each on personal touchscreen devices. We compared two CNN architectures: a modified Inception-V1 network and a lightweight shallow CNN for mobile environments. Additionally, we examined Convolutional and Fully Connected autoencoders for anomaly detection. Both CNN architectures achieved ~89% authentication accuracy, with the shallow CNN requiring fewer parameters. Autoencoder approaches achieved ~75% accuracy. The results demonstrate that finger-drawn symbol authentication provides a viable, secure, and user-friendly biometric solution for touchscreen devices. This approach can be integrated with existing pattern-based authentication methods to create multi-layered security systems for mobile applications.

</details>


### [24] [Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework](https://arxiv.org/abs/2511.10915)
*Guanxiong He,Jie Wang,Liaoyuan Tang,Zheng Wang,Rong Wang,Feiping Nie*

Main category: cs.LG

TL;DR: 提出结构化隐私保护联邦图聚类（SPP-FGC），通过本地结构图实现隐私共享，提供一Shot与迭代两种模式，实验显示在NMI上比联邦基线提升最多约10%，并具有可证明的隐私保证。


<details>
  <summary>Details</summary>
Motivation: 在分布式、无标签数据场景中，现有联邦聚类需在性能与隐私之间妥协：传输嵌入表示易泄露隐私，而仅分享原型虽保护隐私却降损失模型精度。

Method: 客户端构建私有结构图以捕捉数据内在关系，服务器对其进行安全聚合和对齐，形成全局图并推导统一的聚类结构。提供两种模式：一是一次性方法SPP-FGC，在单轮通信内完成；二是SPP-FGC+，在图像等非结构化数据上采用迭代式的特征对齐与增强，以提升下游性能。

Result: 大量实验表明该框架在与联邦基线比较时获得了领先的聚类效果，NMI提升可达约10%，并且给出可证明的隐私保证。

Conclusion: 通过以本地结构图为信息载体，SPP-FGC在保持隐私保护的同时提升聚类精度，且提供适用于不同数据复杂度的两种工作模式，具有良好的应用前景与可扩展性。

Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.

</details>


### [25] [Cascading Bandits With Feedback](https://arxiv.org/abs/2511.10938)
*R Sri Prakash,Nikhil Karamchandani,Sharayu Moharir*

Main category: cs.LG

TL;DR: 在边缘推断场景下，将每个手臂视为一个带有准确率和错误概率的推断模型，本文比较四种决策策略的理论和实验表现：Explore-then-Commit、Action Elimination、LCB、以及Thompson Sampling。


<details>
  <summary>Details</summary>
Motivation: 解决边缘推断中的探索-利用权衡问题，尤其是在模型间存在隐含不确定性与错误率时，评估不同策略在收敛速度和稳定性方面的表现。

Method: 对四种策略给出理论上的渐近或非渐近的 regret 上界，并通过仿真验证其在实际边缘推断任务中的行为，重点比较策略的可适应性。

Result: Explore-then-Commit 与 Action Elimination 在固定排序的探索后难以适应环境的变化，导致更高的 regret；LCB 与 Thompson Sampling 通过持续更新决策，达到常数级 O(1) 的 regret；仿真结果支持理论结论。

Conclusion: 在带不确定性和误差概率的边缘推断问题中，策略的可适应性是实现低 regret 的关键，基于贝叶斯或置信区间的方法在此类问题上更具鲁棒性。

Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.

</details>


### [26] [From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging](https://arxiv.org/abs/2511.10943)
*Jialin Wu,Jian Yang,Handing Wang,Jiajun Wen,Zhiyong Yu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.

</details>


### [27] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>


### [28] [Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm](https://arxiv.org/abs/2511.11009)
*Fuxiang Huang,Xiaowei Fu,Shiyu Ye,Lina Ma,Wen Li,Xinbo Gao,David Zhang,Lei Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.

</details>


### [29] [Enhancing Graph Representations with Neighborhood-Contextualized Message-Passing](https://arxiv.org/abs/2511.11046)
*Brian Godwin Lim*

Main category: cs.LG

TL;DR: 提出邻域上下文化消息传递（NCMP）框架，通过对注意力变体关键性质的 formalization，将经典GNN的消息传递扩展至整 neighborhood 的上下文；并给出 Soft-Isomorphic Neighborhood-Contextualized GCN（SINC-GCN）作为实现，凭借对称性/软同构的设计，在合成二元节点分类任务中展示出良好表达力与效率，奠定 NCMP 的可用性。


<details>
  <summary>Details</summary>
Motivation: 经典的消息传递GNN（尤其是简化的中心-邻居一对一信息传递）往往只利用中心节点和单个邻居的特征信息，忽略了邻域内更丰富的上下文，从而限制了对复杂关系的建模能力。注意力变体揭示了邻域信息的可变性与重要性，因此需要对邻域层面的上下文进行正式化以扩展消息传递的表达能力。

Method: 提出 neighborhood-contextualized message-passing (NCMP) 框架，将邻域上下文信息融入消息聚合过程；给出一个可参数化、易于实现的具体实现 SINC-GCN，通过软同构（soft-isomorphism）策略实现高效的参数化与计算。

Result: 在一个合成的二元节点分类任务上进行了初步分析，实验表明该架构在表达力和计算效率之间取得良好平衡，优于传统的中心-邻居一对一消息传递的基线。

Conclusion: NCMP 为提升经典GNN的表示能力提供一个可操作的框架和路径，后续工作可在更广泛的数据集和任务上进行理论分析和扩展。

Abstract: Graph neural networks (GNNs) have become an indispensable tool for analyzing relational data. In the literature, classical GNNs may be classified into three variants: convolutional, attentional, and message-passing. While the standard message-passing variant is highly expressive, its typical pair-wise messages nevertheless only consider the features of the center node and each neighboring node individually. This design fails to incorporate the rich contextual information contained within the broader local neighborhood, potentially hindering its ability to learn complex relationships within the entire set of neighboring nodes. To address this limitation, this work first formalizes the concept of neighborhood-contextualization, rooted in a key property of the attentional variant. This then serves as the foundation for generalizing the message-passing variant to the proposed neighborhood-contextualized message-passing (NCMP) framework. To demonstrate its utility, a simple, practical, and efficient method to parametrize and operationalize NCMP is presented, leading to the development of the proposed Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN). A preliminary analysis on a synthetic binary node classification problem then underscores both the expressivity and efficiency of the proposed GNN architecture. Overall, the paper lays the foundation for the novel NCMP framework as a practical path toward further enhancing the graph representational power of classical GNNs.

</details>


### [30] [Echoless Label-Based Pre-computation for Memory-Efficient Heterogeneous Graph Learning](https://arxiv.org/abs/2511.11081)
*Jun Hu,Shangheng Chen,Yufei He,Yuan Li,Bryan Hooi,Bingsheng He*

Main category: cs.LG

TL;DR: 提出 Echoless-LP，通过 PFEP 在预计算 HGNN 中实现无回声的标签传播，分区式传播避免自我标签泄露，同时保持内存高效并兼容任意消息传递方法。


<details>
  <summary>Details</summary>
Motivation: 端到端 HGNN 在大规模图上训练成本高；预计算方法通过一次传播将邻居信息压缩成张量提升训练效率；然而标签的多跳传播导致训练时的自回声泄露，现有缓解方法要么内存开销大，要么与先进的消息传递模式不兼容。

Method: PFEP 将目标节点分区，在跨分区的邻居中进行 echoless 传播；每个分区仅从其他分区收集标签信息，避免回声。引入异步分区方案 APS 以及 PostAdjust 以减小分区带来的信息损失和分布漂移。

Result: 在公开数据集上，Echoless-LP 在保持内存效率的同时实现更高的预测性能，相较基线表现更优。

Conclusion: Echoless-LP 提供一种高效、可扩展的无回声预计算解决方案，与常用消息传递方法兼容，适用于大规模异质图的学习任务。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are widely used for deep learning on heterogeneous graphs. Typical end-to-end HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Pre-computation-based HGNNs address this by performing message passing only once during preprocessing, collecting neighbor information into regular-shaped tensors, which enables efficient mini-batch training. Label-based pre-computation methods collect neighbors' label information but suffer from training label leakage, where a node's own label information propagates back to itself during multi-hop message passing - the echo effect. Existing mitigation strategies are memory-inefficient on large graphs or suffer from compatibility issues with advanced message passing methods. We propose Echoless Label-based Pre-computation (Echoless-LP), which eliminates training label leakage with Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and performs echoless propagation, where nodes in each partition collect label information only from neighbors in other partitions, avoiding echo while remaining memory-efficient and compatible with any message passing method. We also introduce an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to address information loss from partitioning and distributional shifts across partitions. Experiments on public datasets demonstrate that Echoless-LP achieves superior performance and maintains memory efficiency compared to baselines.

</details>


### [31] [Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization](https://arxiv.org/abs/2511.11118)
*Gerard Pons,Besim Bilalli,Anna Queralt*

Main category: cs.LG

TL;DR: 提出了一种信息化的嵌入初始化策略，用于KG连续学习，利用实体类别和已有嵌入为新实体初始化，提升性能与知识保留，同时加速学习，且适用于多种KGE模型。


<details>
  <summary>Details</summary>
Motivation: KG经常更新，需将新实体嵌入并保留旧知识；初始化步骤对最终嵌入质量和训练时间影响显著，尤其在小型、频繁更新场景。

Method: 基于KG模式(schema)和已有嵌入，结合新实体所属类别为其生成初始表示，并可无缝集成到现有KGE的持续学习框架中。

Result: 实验表明该初始化提升KGE的预测性能，增强知识保留，并缩短训练所需的epochs，降低时间成本；对不同类型的KGE学习模型均有收益。

Conclusion: 所提策略具通用性，能提升连续学习中KGEs的学习效率与效果，适用于多种模型。

Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.

</details>


### [32] [Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods](https://arxiv.org/abs/2511.11143)
*Federico Maddanu,Tommaso Proietti,Riccardo Crupi*

Main category: cs.LG

TL;DR: 提出并经验性评估若干鲁棒方法，在中高维数据中实现高破坏点和低计算时间的异常检测，应用于约260万日记录的匿名银行账户余额。


<details>
  <summary>Details</summary>
Motivation: 在存在污染观测的数据中，鲁棒统计有助于发现异常并稳健估计参数，但在高维和大规模数据下通常计算成本高、效率低；需要更高效的鲁棒方法以支持金融监控。

Method: 对若干鲁棒方法进行经验评估，强调在中高维数据中兼具高鲁棒性(高破坏点)与较低计算成本；将其应用于约260万日常记录的匿名银行账户余额数据。

Result: 实验表明所考察的方法在中高维场景中能够在保持鲁棒性的同时实现更低的计算时间，且在大规模数据集上具有可接受的性能。

Conclusion: 鲁棒且可扩展的方法可用于金融领域的异常检测，帮助发现潜在欺诈、运维问题等异常。

Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.

</details>


### [33] [Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI](https://arxiv.org/abs/2511.11152)
*Tanmay Ghosh,Shaurabh Anand,Rakesh Gomaji Nannewar,Nithin Nagaraj*

Main category: cs.LG

TL;DR: 解释性深度学习框架用于四城短时降水预测，结合Time-Distributed CNN-ConvLSTM，在ERA5数据上训练，并通过多种解释性方法揭示模型决策模式，城市特异性变量和1–5天预测深度差异明显。


<details>
  <summary>Details</summary>
Motivation: 解决降水预测中的“黑箱”问题，提升透明度与可解释性，同时保持准确性，覆盖不同气候区的城市环境。

Method: 提出混合架构Time-Distributed CNN-ConvLSTM，针对四城市采用不同卷积核数量（Bengaluru 32；Mumbai 64；Delhi 64；Kolkata 128），在ERA5逐日再分析数据上训练，借助 permutation importance、Grad-CAM、时间遮挡和对照 Perturbation等解释性工具，评估模型行为及其预测景深（1–5天）。

Result: RMSE：Bengaluru 0.21 mm/day；Mumbai 0.52；Delhi 0.48；Kolkata 1.80。模型表现随城市而异，且解释性分析揭示城市特异变量驱动的预测，并指出不同预测 horizons 对模型如何学习。

Conclusion: 此研究表明在城市降水预测中实现可解释性AI是可行且有意义的，可在实际运用中提供透明的决策依据和对降水模式的洞察，但需关注跨城市泛化、数据源局限及方法在复杂时空数据上的鲁棒性。

Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.

</details>


### [34] [Training Neural Networks at Any Scale](https://arxiv.org/abs/2511.11163)
*Thomas Pethick,Kimon Antonakopoulos,Antonio Silveti-Falls,Leena Chennuru Vankadara,Volkan Cevher*

Main category: cs.LG

TL;DR: The article surveys scalable optimization methods for training neural networks, presenting a unified, structure-aware framework and discussing how to make these methods robust to problem size, aimed at practitioners and researchers.


<details>
  <summary>Details</summary>
Motivation: As neural networks grow in size and data, efficient and scalable optimization becomes essential. The goal is to unify diverse algorithms under a template that exploits problem structure and remains effective across scales.

Method: The paper reviews state-of-the-art optimization algorithms within a common algorithmic template, highlighting design principles for adapting to problem structures and for achieving scale-agnostic performance, and provides guidance for implementation and application.

Result: A cohesive survey that clarifies how modern optimization methods can be organized, compared, and applied in practice, including guidelines and insights for developing scalable deep-learning optimizers.

Conclusion: The exposition serves as an introductory bridge between theory and practice, equipping practitioners and researchers with a unified view and actionable principles for scalable neural-network optimization.

Abstract: This article reviews modern optimization methods for training neural networks with an emphasis on efficiency and scale. We present state-of-the-art optimization algorithms under a unified algorithmic template that highlights the importance of adapting to the structures in the problem. We then cover how to make these algorithms agnostic to the scale of the problem. Our exposition is intended as an introduction for both practitioners and researchers who wish to be involved in these exciting new developments.

</details>


### [35] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: 对极端高温事件的预测中，使用幂均值对集合预测进行非线性聚合，显著提升分类性能；对阈值随 q-th 本地分位数变化的鲁棒性及对高百分位的优势显现。


<details>
  <summary>Details</summary>
Motivation: 极端气象事件预测对风险管理至关重要；传统等权或简单平均聚合的集合预测在极端情形中往往表现不足，需要更灵活的聚合方法。

Method: 将问题形式化为二分类：在给定时间窗口内，预测地表气温是否超过局部 q 分位数；使用基于机器学习的天气预报模型生成集合输出，并对其应用幂均值（power mean）进行聚合；探索不同 q 值对最优幂指数的影响；与常规均值进行比较。

Result: 幂均值聚合显著提升了分类准确性，相较于同一模型的平均预测；最优幂指数随分位阈值变化，且在更高极端预测中效果更好。

Conclusion: 非线性聚合（幂均值）对集合预测在极端事件预测中具有潜在优势，且可根据目标分位数自适应调整；该策略可推广至其他极端事件预测任务。

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [36] [On-line learning of dynamic systems: sparse regression meets Kalman filtering](https://arxiv.org/abs/2511.11178)
*Gianluigi Pillonetto,Akram Yazdani,Aleksandr Aravkin*

Main category: cs.LG

TL;DR: 提出了 Sindy-Kalman Filter (SKF)，在保持稀疏性前提下实现对非线性系统的实时学习，将参数视为状态变量并结合卡尔曼滤波进行在线推断。


<details>
  <summary>Details</summary>
Motivation: 需要在实时场景中对具有时变或切换参数的非线性系统进行模型识别，同时保持稀疎、简洁的模型表示以增强可解释性与鲁棒性。

Method: 将 Sindy 的稀疎驱动建模与卡尔曼滤波结合：把未知系统参数作为状态变量，引入含时变与切换的参数，利用观测更新进行自适应推断，并通过看 ahead 距误等策略简化对稀疏度、方差及切换时刻的估计。

Result: 在混沌洛伦兹系统（参数漂移/切换）以及真实飞行数据构建的稀疏非线性飞机模型上验证，展示了 SKF 的实时识别能力和对切换时刻及稀疎性参数的有效估计。

Conclusion: SKF 将稀疎建模与卡尔曼滤波结合，提升对时间变化的非线性系统的实时识别能力，且简化了对稀疎度、噪声方差及切换时点的估计。

Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.

</details>


### [37] [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)
*Zhenghao Zhang,Jun Xie,Xingchen Chen,Tao Yu,Hongzhu Yi,Kaixin Xu,Yuanxiang Wang,Tianyu Zong,Xinming Wang,Jiahuan Chen,Guoqing Chao,Feng Chen,Zhepeng Wang,Jungang Xu*

Main category: cs.LG

TL;DR: 提出DGIMVCM，一体化动态图学习框架用于不完整多视图聚类，通过全局鲁棒图、动态视图-特征图、对比学习、自注意力编码和掩蔽图重构损失实现鲁棒聚类。


<details>
  <summary>Details</summary>
Motivation: 面临两大挑战：1) 以KNN构建的静态图易引入噪声，降低鲁棒性；2) 直接以重构图的MSE损失存在显著梯度噪声，影响优化。

Method: 首先构建缺失鲁棒的全局图；设计图卷积嵌入层提取初级特征并 refined 动态视图图结构，利用全局图对缺失视图进行补全；通过图结构对比学习发现视图特征间的一致性；引入图自注意力编码器以基于初级特征和视图图提取高层表征，并以掩蔽图重构损失替代传统的重构损失以降低梯度噪声；最后构建聚类模块并通过伪标签自监督训练进行优化。

Result: 在多个数据集上进行了广泛实验，结果显示方法在不完整多视图聚类任务上具有显著的有效性和优越性。

Conclusion: DGIMVCM 能有效解决 IMVC 的图结构鲁棒性及梯度噪声问题，通过动态图学习与掩蔽重构损失提升聚类性能，具有良好的泛化能力。

Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.

</details>


### [38] [LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag](https://arxiv.org/abs/2511.11190)
*Tianlang He,Zhongming Lin,Tianrui Jiang,S. -H. Gary Chan*

Main category: cs.LG

TL;DR: LoRaCompass enables robust, efficient localization of LoRa tags under RSSI fluctuations and domain shifts, achieving high success and 40% improvement over baselines with hop-efficient search.


<details>
  <summary>Details</summary>
Motivation: Address the brittleness of RL-based search under signal variability and domain shift in real-world search-and-rescue scenarios, where locating a periodically broadcasting LoRa tag must be done with few moves.

Method: Propose LoRaCompass, a reinforcement learning model that learns robust RSSI-based spatial representations via a spatially-aware feature extractor and policy distillation loss. It includes a UCB-inspired exploration function to guide movement toward the tag with increasing confidence, enabling efficient search in unseen environments.

Result: Validated in ground-based and drone-assisted experiments over unseen environments spanning >80 km^2; achieved >90% success rate to locate the tag within 100 m, about 40% better than existing methods, and a path length in hops that scales linearly with initial distance.

Conclusion: LoRaCompass demonstrates robust and efficient LoRa-based search under domain shift and signal fluctuations, generalizing to unseen environments and offering substantial improvements over prior RL-based approaches.

Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.

</details>


### [39] [When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping](https://arxiv.org/abs/2511.11208)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出一个零-shot合成验证框架，在联邦学习中用生成式AI监控模型性能并实现提前停止，从而近似最优轮次、节省计算。多标签胸部X线分类实验表明可将全局轮数减少约74%，且准确率维持在最优值的±1%。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习通常在固定的全局轮数下训练，易造成不必要的计算开销；若模型已达较好表现或表现不佳时仍持续训练，则资源浪费严重，需高效的早停机制与快速超参数 tuning。

Method: 提出一个零-shot合成验证框架，利用生成式AI对训练过程进行无须额外标注的性能监测，动态评估并在接近最佳轮次时停止训练；通过自适应策略在不牺牲准确性的前提下减少计算资源消耗，并便于快速超参数调整。

Result: 在多标签胸部X线分类任务上，方法可将全局训练轮次最多减少74%，同时保持准确率在最优值的1%之内。

Conclusion: 该框架提升FL训练效率与资源利用率，降低计算成本，且保留近似最优性能，便于在需要大量分布式计算的场景（如医疗影像）快速迭代。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.

</details>


### [40] [A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates](https://arxiv.org/abs/2511.11211)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: 对 Tsallis-INF 的多臂赌博算法在“最佳-两端世界”情形的推导给出简化版本，避免使用共轭函数，利用在线凸优化的现代工具，尽管牺牲了一些常数优化以换取更清晰的证明。


<details>
  <summary>Details</summary>
Motivation: 提供一个直观且简洁的推导框架，统一理解在随机与对抗性环境下的最佳-两端世界保证，并展示无需共轭函数的证明技巧。

Method: 在不使用对偶/共轭函数的前提下，结合在线凸优化的现代工具，给出 Tsallis-INF 的最佳-两端世界保证的推导，并以简洁的证明结构取代繁复推导。

Result: 得到一个更短、易懂的推导过程，证明 Tsallis-INF 在随机与对抗性情形下具备最佳-两端世界的保证。

Conclusion: 提供一个 Slim 的证明框架，降低对传统工具的依赖，便于未来在相关算法中的类似推导与扩展。

Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.

</details>


### [41] [HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning](https://arxiv.org/abs/2511.11240)
*Yuhan Xie,Chen Lyu*

Main category: cs.LG

TL;DR: HealSplit introduces a unified defense framework for Split Federated Learning (SFL) that detects and recovers from multiple poisoning attacks using topology-aware detection, a generative recovery pipeline, and adversarial multi-teacher distillation; it achieves superior robustness across four datasets compared to ten baselines.


<details>
  <summary>Details</summary>
Motivation: SFL exposes new attack surfaces due to limited access to full model updates, making traditional FL defenses less effective. There is a need for end-to-end, SFL-tailored defenses that can detect poisoned data, recover semantically consistent substitutes, and robustly train models.

Method: HealSplit combines three components: (1) topology-aware detection (TAS) that builds graphs over smashed data to identify poisoned samples via topological anomaly scoring; (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for anomalies, validated by a consistency validation student; (3) an adversarial multi-teacher distillation framework that trains the student with semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices.

Result: Extensive experiments on four benchmark datasets show HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.

Conclusion: HealSplit provides end-to-end defense in SFL by integrating topology-based detection, generative recovery, and adversarial multi-teacher distillation, delivering strong robustness against multiple poisoning attacks and outperforming existing defenses.

Abstract: Split Federated Learning (SFL) is an emerging paradigm for privacy-preserving distributed learning. However, it remains vulnerable to sophisticated data poisoning attacks targeting local features, labels, smashed data, and model weights. Existing defenses, primarily adapted from traditional Federated Learning (FL), are less effective under SFL due to limited access to complete model updates. This paper presents HealSplit, the first unified defense framework tailored for SFL, offering end-to-end detection and recovery against five sophisticated types of poisoning attacks. HealSplit comprises three key components: (1) a topology-aware detection module that constructs graphs over smashed data to identify poisoned samples via topological anomaly scoring (TAS); (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for detected anomalies, validated by a consistency validation student; and (3) an adversarial multi-teacher distillation framework trains the student using semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices. Extensive experiments on four benchmark datasets demonstrate that HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.

</details>


### [42] [Heterogeneous Attributed Graph Learning via Neighborhood-Aware Star Kernels](https://arxiv.org/abs/2511.11245)
*Hong Huang,Chengyu Yao,Haiming Chen,Hang Gao*

Main category: cs.LG

TL;DR: NASK is a positive-definite graph kernel that combines exponential Gower similarity with star substructures and WL iterations to jointly model heterogeneous attributes and multi-scale neighborhood information, achieving state-of-the-art results on diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Attributed graphs have irregular topology and mixed attribute types; existing kernels struggle to integrate heterogeneous semantics and neighborhood information simultaneously; a kernel that handles both could improve graph similarity measures for learning tasks.

Method: Compute numerical and categorical similarity via an exponential transform of Gower similarity; build star-structured kernels enhanced by iterative Weisfeiler-Lehman (WL) updates to capture multi-scale neighborhood structure; prove kernel is positive definite; evaluate on multiple benchmarks.

Result: NASK consistently outperforms 16 baselines (9 graph kernels, 7 GNNs) across eleven attributed and four large-scale real-world graphs.

Conclusion: NASK provides a principled, positive-definite kernel that effectively fuses attribute semantics and graph topology for attributed graph learning, offering strong empirical gains and compatibility with kernel-based learners.

Abstract: Attributed graphs, typically characterized by irregular topologies and a mix of numerical and categorical attributes, are ubiquitous in diverse domains such as social networks, bioinformatics, and cheminformatics. While graph kernels provide a principled framework for measuring graph similarity, existing kernel methods often struggle to simultaneously capture heterogeneous attribute semantics and neighborhood information in attributed graphs. In this work, we propose the Neighborhood-Aware Star Kernel (NASK), a novel graph kernel designed for attributed graph learning. NASK leverages an exponential transformation of the Gower similarity coefficient to jointly model numerical and categorical features efficiently, and employs star substructures enhanced by Weisfeiler-Lehman iterations to integrate multi-scale neighborhood structural information. We theoretically prove that NASK is positive definite, ensuring compatibility with kernel-based learning frameworks such as SVMs. Extensive experiments are conducted on eleven attributed and four large-scale real-world graph benchmarks. The results demonstrate that NASK consistently achieves superior performance over sixteen state-of-the-art baselines, including nine graph kernels and seven Graph Neural Networks.

</details>


### [43] [Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria](https://arxiv.org/abs/2511.11293)
*Jiheum Park,Chao Pang,Tristan Y. Lee,Jeong Yun Yang,Jacob Berkowitz,Alexander Z. Wei,Nicholas Tatonetti*

Main category: cs.LG

TL;DR: EHR-based predictive models significantly outperform traditional risk factors in identifying high-risk individuals for cancer, with promise across many cancer types when using large-scale data like All of Us.


<details>
  <summary>Details</summary>
Motivation: Improve cancer screening by leveraging comprehensive, longitudinal EHR data and advanced AI models to detect subtle prediagnostic signals beyond age or single risk factors.

Method: Systematic evaluation of EHR-based predictive models against traditional risk factors (including gene mutations and family history) across eight major cancers using All of Us data (超过86.5万参与者). Baseline models and an EHR foundation model trained on full patient trajectories were assessed for 26 cancer types.

Result: EHR-based models achieved 3- to 6-fold higher enrichment of true cancer cases among those identified as high risk compared with traditional risk factors. The EHR foundation model further improved predictive performance across 26 cancer types, indicating strong clinical potential for early detection.

Conclusion: EHR-based predictive modeling, especially foundation models trained on rich patient trajectories, can enhance precision and scalability of cancer screening beyond conventional risk factors and may inform more effective, broad-spectrum screening strategies.

Abstract: Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.

</details>


### [44] [Fast and Expressive Multi-Token Prediction with Probabilistic Circuits](https://arxiv.org/abs/2511.11346)
*Andreas Grivas,Lorenzo Loconte,Emile van Krieken,Piotr Nawrot,Yu Zhao,Euan Wielewski,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: MTPC introduces probabilistic circuit-based multi-token prediction to balance expressiveness and latency in byte-level LLMs, unifying models like mixtures, HMMs, and tensor networks. It retrofits EvaByte, achieves speedups with speculative decoding while preserving verifier performance, and analyzes expressiveness-latency trade-offs via PC architectures and partial layer sharing.


<details>
  <summary>Details</summary>
Motivation: Existing multi-token prediction (MTP) methods speed up generation but often assume independence between future tokens, reducing expressiveness. There is a need to strike a principled balance between expressiveness and latency in MTP for byte-level LLMs.

Method: Propose MTPC, a framework that encodes joint future-token distributions using probabilistic circuits (PCs). Explore various PC architectures (e.g., hierarchical mixtures, HMMs, tensor networks) to generalize classical models. Retrofit MTPC into byte-level LLMs like EvaByte and combine with speculative decoding. Systematically study parameterizations (PC architectures, partial layer sharing between verifier and draft LLMs) to characterize the expressiveness-latency trade-off.

Result: MTPC enables faster generation than independence-based MTP when used with speculative decoding, while preserving the performance of the original verifier LLM. Experimental results demonstrate meaningful speedups and a tunable trade-off between expressiveness and latency.

Conclusion: MTPC provides a flexible, tunable framework for joint token modeling via probabilistic circuits, enabling principled control over expressiveness and latency. Through architecture choices and layer sharing strategies, one can tailor MTP to achieve speedups without sacrificing verifier performance, and map the optimal expressiveness-latency frontier.

Abstract: Multi-token prediction (MTP) is a prominent strategy to significantly speed up generation in large language models (LLMs), including byte-level LLMs, which are tokeniser-free but prohibitively slow. However, existing MTP methods often sacrifice expressiveness by assuming independence between future tokens. In this work, we investigate the trade-off between expressiveness and latency in MTP within the framework of probabilistic circuits (PCs). Our framework, named MTPC, allows one to explore different ways to encode the joint distributions over future tokens by selecting different circuit architectures, generalising classical models such as (hierarchical) mixture models, hidden Markov models and tensor networks. We show the efficacy of MTPC by retrofitting existing byte-level LLMs, such as EvaByte. Our experiments show that, when combined with speculative decoding, MTPC significantly speeds up generation compared to MTP with independence assumptions, while guaranteeing to retain the performance of the original verifier LLM. We also rigorously study the optimal trade-off between expressiveness and latency when exploring the possible parameterisations of MTPC, such as PC architectures and partial layer sharing between the verifier and draft LLMs.

</details>


### [45] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 在内存受限的边缘设备上，基于零阶优化的MeZO通过仅用前向评估估计梯度，省去对中间激活和优化器状态的存储，因此可在同等设备内存下训练更大模型；代价是可能需要更长的实际训练时间；理论与实验均表明，在充足的墙钟时间条件下，MeZO相比BP具有更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 边缘AI需要在严格内存约束下进行自适应微调；常规反向传播需要大量激活和优化器状态存储，且在设备内存受限时限制模型规模。

Method: 提出对BP和MeZO下可容纳的相对模型大小的理论估计；通过数值仿真验证分析，比较在内存受限条件下的精度表现，并展示MeZO在有充足墙钟时间时的稳定性和优势。

Result: 理论分析和数值结果都显示，在内存约束下，MeZO能容纳更大模型并保持或提高精度，前提是有足够的实际训练时间。

Conclusion: 对于内存受限的边缘设备，MeZO提供一种可行的高效微调路径，使得更大模型可以部署在设备内存中，权衡是需要更长的训练时间；未来工作可能聚焦在提高MeZO的样本效率和时间效率。

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [46] [Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning](https://arxiv.org/abs/2511.11402)
*Amit Jain,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的强化学习框架，在单一策略下统一多阶段轨迹优化，利用Gated Transformer-XL与PPO实现跨阶段记忆和无手动阶段切换的控制。


<details>
  <summary>Details</summary>
Motivation: 在任务阶段高度动态、需跨阶段自适应的航天自主控制中，现有RL方法通常需要为不同阶段设计独立策略，导致适配性不足与运维复杂度高。

Method: 以PPO为基础，用Transformer编码器-解码器替代传统RNN，结合GTrXL实现跨阶段长时记忆，并实现对包括对阶段切换的自动化处理，应用于从单阶段基准（双积分器、Van der Pol振荡器）到多阶段航迹定位（多阶段航点导航）再到复杂火箭升空任务的渐进验证。

Result: 在简单基准中实现接近解析解的性能，在多阶段任务中学习出前后一致、跨阶段的控制策略，且框架能在复杂升空问题中处理气动飞行、级间分离与真空段，显示出与阶段特定控制器相比的竞争力及对安全性验证的兼容性。

Conclusion: Transformer-based RL框架为可扩展的多阶段任务规划奠定基础，降低对阶段定制控制器的依赖，并具备与安全性验证协议兼容的潜力。

Abstract: Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.

</details>


### [47] [Multicalibration yields better matchings](https://arxiv.org/abs/2511.11413)
*Riccardo Colini Baldeschi,Simone Di Gregorio,Simone Fioravanti,Federico Fusco,Ido Guy,Daniel Haimovich,Stefano Leonardi,Fridolin Linder,Lorenzo Perini,Matteo Russo,Niek Tax*

Main category: cs.LG

TL;DR: 在带上下文的随机权重图匹配问题中，利用具偏差校准的预测器可以在对抗预测误差的情况下实现接近最佳的匹配性能，并给出样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 当预测的边权存在误差且信息不完美时，传统基于预测的最优匹配可能表现不佳。引入多校准（multicalibration）以在保护集上使预测无偏，从而对抗预测误差并提升决策鲁棒性。

Method: 给定边权预测器γ和匹配算法族C，构造一个多校准预测器γ̂，使其在一个保护集族的每个元素上都无偏。用γ̂ 选择最佳匹配，得到的决策在竞争意义上可比拟于对γ应用族C中的最佳规则。并给出样本复杂度界限。

Result: 基于γ̂所选的最佳匹配在与γ相关的族C中达到竞争性，接近最优的决策规则；同时给出对样本复杂度的界限。

Conclusion: 多校准预测器提供一种对不完美预测鲁棒的匹配决策工具，具备理论上的竞争性保证和可控的学习成本，弥合预测不确定性与组合优化之间的差距。

Abstract: Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.
  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat γ$, with the following property. Picking the best matching based on the output of $\hat γ$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.

</details>


### [48] [Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization](https://arxiv.org/abs/2511.11415)
*Nikolas Borrel-Jensen,Josiah Bjorgaard*

Main category: cs.LG

TL;DR: 通过可微分编程解决声学反问题的实用方法：用 JAX-FEM 的自动微分直接估计边界导纳并使用随机有限差分进行几何形状优化，显著提升效率与精度。


<details>
  <summary>Details</summary>
Motivation: 解决声学反问题中的参数估计和形状优化难题，降低推导摆动方程的成本，利用现代可微分软件栈实现从前向仿真到反演设计的快速原型化。

Method: （1）对边界导纳的估计：基于 JAX-FEM 的自动微分，从稀疏压力测量出发，进行梯度法参数估计，避免手动推导伴随场方程；（2）形状优化：对声学形状采用随机有限差分，前向仿真用 JAX-FEM，网格操作用 PyTorch3D 的 AD， physics-边界优化与几何网格自适应分离进行。

Result: 在边界导纳估计中实现三位数精度无需推导对偶方程；在形状优化中实现目标频段能量下降 48.1%，相较完整网格的标准有限差分仅需约 1/30 的 FEM 求解。

Conclusion: 展示了使用现代可微分软件栈快速原型化物理反问题优化工作流的潜力：通过 AD 进行参数估计，并结合 FD 与 AD 实现几何设计的可行性与高效性。

Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.

</details>


### [49] [Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching](https://arxiv.org/abs/2511.11418)
*Dara Varam,Diaa A. Abuhani,Imran Zualkernan,Raghad AlDamani,Lujain Khalil*

Main category: cs.LG

TL;DR: OT-based post-training quantization reduces weight precision of Flow Matching models to 2-3 bits per parameter while preserving generation quality, outperforming uniform, piecewise, and logarithmic quantizers.


<details>
  <summary>Details</summary>
Motivation: Enable efficient deployment of Flow Matching generative models on edge/embedded devices by reducing parameter precision without compromising generation quality and latent-space stability.

Method: Post-training quantization guided by optimal transport, optimizing the 2-Wasserstein distance between quantized and full-precision weights; comparative study against uniform, piecewise, and logarithmic quantization; theoretical upper bounds on degradation; empirical evaluation on five benchmark datasets.

Result: OT-based quantization preserves visual generation quality and latent-space stability down to 2-3 bits per parameter; alternative quantization schemes fail at such low bitwidths.

Conclusion: OT-based quantization is a principled, effective approach to compress FM generative models for edge and embedded AI applications.

Abstract: Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.

</details>


### [50] [Retrofit: Continual Learning with Bounded Forgetting for Security Applications](https://arxiv.org/abs/2511.11439)
*Yiling He,Junchi Lei,Hongyu She,Shuo Shao,Xinran Zheng,Yiping Liu,Zhan Qin,Lorenzo Cavallaro*

Main category: cs.LG

TL;DR: RETROFIT提出了一种无数据回顾的连续学习方法，通过参数级合并和低秩/稀疏更新来在不保留历史数据的情况下实现知识保留和迁移，在安全任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 安全分析中的持续学习需要在不使用历史数据的前提下防止知识遗忘，同时应对数据分布漂移和新知识整合。

Method: 通过将已训练模型和新微调模型进行参数层面的合并，形成教师-学生式的知识传递；使用低秩和稀疏更新将参数变动限制在独立子空间；引入知识仲裁器根据模型置信度动态平衡教师贡献。

Result: 在两项应用上验证：在恶意软件检测的时序漂移场景中，保留分数从 20.2% 提升到 38.6%，并超越新数据的 oracle 上限；在跨反汇编级别的二进制摘要任务中，BLEU 分数约为先前工作的两倍，并在跨表示泛化方面领先所有基线。

Conclusion: 证明RETROFIT在数据敏感的安全场景中能够有效减少遗忘并保持适应性，为无数据回顾的持续学习提供可行性。

Abstract: Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.
  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.

</details>


### [51] [DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference](https://arxiv.org/abs/2511.11446)
*Farhana Amin,Sabiha Afroz,Kanchon Gharami,Mona Moghadampanah,Dimitrios S. Nikolopoulos*

Main category: cs.LG

TL;DR: DiffPro is a post-training, hardware-faithful framework that jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without training, using exact integer kernels.


<details>
  <summary>Details</summary>
Motivation: Diffusion models achieve high-quality images but are costly to run due to many denoising steps and heavy matrix operations; there is a need for hardware-compatible post-training optimization that lowers latency and memory without retraining.

Method: DiffPro comprises three components: (1) a manifold-aware sensitivity metric to allocate weight bits, (2) dynamic activation quantization to stabilize activations across timesteps, and (3) a budgeted timestep selector guided by teacher-student drift; it operates with exact integer kernels used in deployment and jointly optimizes timesteps and per-layer precision in Diffusion Transformers (DiTs).

Result: The framework achieves up to 6.25x model compression, ~50% fewer timesteps, and 2.8x faster inference with Delta FID ≤ 10 on standard benchmarks.

Conclusion: DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.

Abstract: Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.

</details>


### [52] [FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression](https://arxiv.org/abs/2511.11459)
*Xiaoyin Xi,Zhe Yu*

Main category: cs.LG

TL;DR: 本文提出 FairReweighing，一种基于互信息的公平性度量与预处理方法，用于分类与回归任务，能够处理二元及连续敏感属性，旨在降低分离性违例并提升公平性，同时保留高准确率。


<details>
  <summary>Details</summary>
Motivation: 在高风险的公共部门和工业场景中，AI 决策的透明性与公平性日益成为关注点。当前的大部分公平性研究聚焦于二元分类任务，回归公平性研究相对不足，且对数据驱动决策的分离性问题尚缺乏有效解法。

Method: 提出基于互信息的分离性度量，并扩展以直接适用于分类和回归任务，且兼容二元与连续敏感属性。以 Reweighing 为灵感，设计基于密度估计的 FairReweighing 预处理算法，确保学习的模型在训练数据上满足分离性约束；理论上在数据独立性假设下保证训练数据的分离性。

Result: 在合成数据与真实数据集上的实验表明，FairReweighing 相较于现有最先进的回归公平性方法，在提升分离性方面效果更好，同时保持较高的准确性。

Conclusion: FairReweighing 为分类与回归场景下的分离性公平性提供了一种有效的预处理解决方案，且可扩展至包含连续敏感属性的情形，推动高风险领域中对公平性的实现。

Abstract: There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.

</details>


### [53] [Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies](https://arxiv.org/abs/2511.11461)
*Riku Green,Huw Day,Zahraa S. Abdallah,Telmo M. Silva Filho*

Main category: cs.LG

TL;DR: 对多步预测，递归策略的偏差-方差直觉并非普适；对于线性预测器，结构缺口为零；对于非线性预测器，重复组合可能提升表达能力并影响结构缺口；递归预测的估计方差等于一步方差乘以雅可比放大系数；实验（MLP在ETTm1数据集）支持在非线性程度和噪声特性下选择策略的实用指引。


<details>
  <summary>Details</summary>
Motivation: 挑战传统的递归与直接策略的偏差-方差直觉，提出更细致的误差分解框架，以理解在何种条件下递归策略能优于直接策略。

Method: 将多步预测误差分解为不可约噪声、结构性差距和估计方差三部分；对线性预测器证明结构缺口为零；分析非线性情形中递归的重复组合如何影响结构缺口；给出递归估计方差等于一步方差乘以雅可比放大因子；通过在ETTm1数据集上使用多层感知机（MLP）进行实验来验证理论。

Result: 线性情形下结构缺口为零；非线性情形中递归可因提升模型表达能力而改变结构缺口，且在某些情形下同时带来更低偏差和更高方差；雅可比放大因子揭示了递归策略误差的放大机制；实验结果与理论一致，给出实际的策略选择指引。

Conclusion: 在多步预测中，应基于模型的非线性程度和数据的噪声特征来权衡递归与直接策略，而非简单依赖传统的偏差-方差直觉，提供一个用于策略选择的可操作框架。

Abstract: Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.

</details>


### [54] [MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture](https://arxiv.org/abs/2511.11462)
*Kevin Chen,Kenneth W. Parker,Anish Arora*

Main category: cs.LG

TL;DR: 通过将 MoCap 数据转换为雷达多普勒谱，提出一个窗口化的序列到序列 Transformer 模型，联合建模空间关系与时间动态，实现高保真谱图生成与良好泛化。


<details>
  <summary>Details</summary>
Motivation: 在雷达数据稀缺且体量大的情况下，利用更易获取的 MoCap 数据来训练可以生成雷达谱的模型，降低物理建模计算成本，并提升边缘计算/物联网雷达的数据生成能力。

Method: 将 MoCap 转换为谱图的问题视为窗口化的序列到序列任务，使用 Transformer 以联合方式捕捉人体各部位之间的空间关系与帧间的时间演化，通过对多部位运动的解码产生 Doppler 谱。并进行消融实验以验证空间关系与多部位信息的重要性。

Result: 在真实世界实验中，所生成的 Doppler 雷达谱在视觉上与定量指标上均具有较高的可信度，并展现出良好的泛化能力。消融实验表明模型同时具备将多部位运动映射到 Doppler 指征的能力以及对人体部位空间关系的理解。

Conclusion: 结果表明 Transformer 在时间序列信号处理中的潜力，尤其适用于边缘计算和 IoT 雷达。该方法还暗示可用更丰富的 MoCap 数据扩充稀缺雷达数据集以支撑更高层应用，并且相比物理基础方法，计算需求更低。

Abstract: We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.
  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.

</details>


### [55] [Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations](https://arxiv.org/abs/2511.11472)
*Sooyong Jang,Insup Lee*

Main category: cs.LG

TL;DR: 提出一种以输入变换排序难度且等质量分箱的自适应概率预测方法，并提出新的评估度量和基于分组条件的自适应预测集算法，在图像分类和医疗任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的自适应区间（预测集）评估往往因分箱不均衡导致对覆盖率与平均集合大小的估计不准确。需要更可靠的自适应性评估指标来衡量难度感知下的预测集表现。

Method: 引入基于输入变换的难度排序和等质量分箱（uniform-mass binning）。在此基础上提出两种新评估指标，用以更稳健地衡量覆盖率违背与平均集合大小的自适应性。并提出一种新的自适应预测集算法：按估计难度对样本分组，实施组条件的 conformal 预测，针对每组设定阈值。

Result: 通过在ImageNet和视觉 acuity预测等任务上的实验，新指标对自适应性具有更强相关性，且新算法在新指标下表现优于现有方法。

Conclusion: 提出的难度分组+组条件预测集框架可提供更可靠的自适应性评估与更优的预测集构造，且具有跨任务的适用性。

Abstract: Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.

</details>


### [56] [Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys](https://arxiv.org/abs/2511.11485)
*Alinda Ezgi Gerçek,Till Korten,Paul Chekhonin,Maleeha Hassan,Peter Steinbach*

Main category: cs.LG

TL;DR: 在仅用10张带标注的扫描电子显微镜图像上，使用轻量级U-Net实现数据高效分割，提取碳化物与基体界限，Dice-Sørensen系数达到0.98，显著优于传统金属学方法，并将标注工作量缩减一个数量级。


<details>
  <summary>Details</summary>
Motivation: 理解反应堆压壳钢微观结构对力学性能的影响尤为关键。碳化物的沉淀既能强化合金又可能引发裂纹；在SEM图像中碳化物与基体的灰度叠加使简单阈值分割失效，因此需要在极少标注数据下实现高效、鲁棒的分割以便快速定量分析。

Method: 提出一个轻量级U-Net（约30.7M参数），仅使用10张带标注的SEM图像进行训练，达到高质量的碳化物分割。所提出的方法在 metallurgy领域超过传统图像分析等方法，并相比现有的数据高效分割模型减少了标注工作量。

Result: 模型在分割任务上获得Dice系数0.98，显著高于传统的0.85，同时将注释劳动量相比最新数据高效分割模型降低约一个数量级，具备对其他钢种的泛化能力。

Conclusion: 数据高效的深度学习分割在RPV钢分析中具有潜力，可实现快速、自动化的碳化物定量并辅助合金设计，且有望推广到其他钢种。

Abstract: Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \textbf{Dice-Sørensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.

</details>


### [57] [Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models](https://arxiv.org/abs/2511.11490)
*Joan Font-Quer Roset,Devina Mohan,Anna Scaife*

Main category: cs.LG

TL;DR: 本研究用扩散模型估计RGZ数据的intrinsic dimension，并分析其随BNN能量分数、FR形态及SNR的变化，发现离分布源iD较高，RGZ总体iD高于自然图像；FR I/II无显著差异但高SNR伴随低iD，提出将iD–能量分数关联用于改进自监督学习表示。


<details>
  <summary>Details</summary>
Motivation: 理解大规模天文影像数据的几何结构以及自监督学习在异分布数据上的表现。

Method: 利用基于扩散模型的分数估计来推断数据的intrinsic dimension，并将其与BNN能源分数和观测标签（FR类、SNR）相关分析。

Result: 离分布源具有更高的iD；RGZ总体iD高于自然图像数据集；FR I/II间无显著关系，但在较高SNR时iD偏低；iD–能源分数关系提示可用于评估和改进自监督表示。

Conclusion: 未来工作可利用iD与能源分数的关系对RGZ数据集中的自监督学习算法进行定量研究与改进。

Abstract: In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.

</details>


### [58] [FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models](https://arxiv.org/abs/2511.11505)
*Yonatan Dukler,Guihong Li,Deval Shah,Vikram Appia,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出 FarSkip-Collective，通过修改模型架构实现通信与计算的重叠，覆盖从 16B 到 109B 的大语言模型，在保持与原始开源发布版本相近的精度的同时，显著提升训练与推理速度。


<details>
  <summary>Details</summary>
Motivation: 阻塞式通信在分布式大模型训练与推理中成为关键瓶颈。现有方法难以在大规模模型上普遍实现全层修改后的可行性与高效性，因此需要评估是否可在保持模型能力的前提下通过结构改造实现重叠计算与通信。

Method: 通过对现代模型架构进行改造，添加跳跃连接以实现计算与通信的重叠；并采用自蒸馏等策略确保改造后模型在性能上仍接近原始模型。对 16B–109B 参数规模的多种模型进行改造，且对 Llama 4 Scout（109B）进行自蒸馏以达到与指令调优版本接近的性能，同时实现针对现有框架的优化实现以显式重叠通信与计算。

Result: 在广泛参数规模的模型上实现了架构改造且维持与原模型相当的精度；以 Llama 4 Scout（109B）为例，经过自蒸馏后，平均在多项下游评测中接近原指令调优版本，误差约 1%；同时，优化实现显著提升训练与推理时的通信-计算重叠效率。

Conclusion: 证明了可通过架构改造实现通信与计算的有效重叠，适用于极大规模模型，能在不牺牲显著精度的前提下提升分布式训练与推理的效率，为 MoE 和大规模并行部署提供新的优化方向。

Abstract: Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.

</details>


### [59] [Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications](https://arxiv.org/abs/2511.11539)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本研究将 closest fair clustering 的研究从两组扩展到多组保护属性，并给出近线性时间近似算法及多项相关问题的改进解，与两组情形相比解决了更一般的情况。


<details>
  <summary>Details</summary>
Motivation: 解决聚类在多属性保护下的公平性问题，弥补现有工作仅聚焦于两组的局限，目标是在尽量小的修改代价（可能作为后处理步骤）下提升聚类公平性。

Method: 首先给出在等大小分组情形下的 NP-hard 证据，随后给出可处理任意大小多组的近线性时间近似算法；再利用 closest fair clustering 的结果提升公平相关聚类（fair correlation clustering）的近似保証，并首次给出多组 (>2) 的公平一致性聚类（fair consensus clustering）的近似算法。

Result: 证明了多组等大小情形下的 NP-hard；提出近线性时间的 closest fair clustering 近似算法；在此基础上改善了 fair correlation clustering 的近似保証；首次给出多组的 fair consensus clustering 的近似算法，回应了 COLT'25 的开放问题。

Conclusion: 把 closest fair clustering 推广到多组设定，给出可实际实现的近似算法并具备理论保真度，推进了多属性公平聚类的理论与应用，解决了相关开放方向。

Abstract: Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.
  In this work, we generalize the study of the \emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].
  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].

</details>


### [60] [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)
*Guangxuan Xiao,Junxian Guo,Kasra Mazaheri,Song Han*

Main category: cs.LG

TL;DR: MoBA的性能取决于路由器将查询键对的相关块进行正确区分的能力，提出了理论化的信噪比来连接架构参数与检索精度。通过小块尺寸和对键进行短卷积来提升路由准确性，但小块尺寸在GPU上效率低下，因此提出了面向硬件的CUDA实现FlashMoBA，在小块情境下也能高效运行。经从头训练的LLM验证，改进的MoBA可达到与密集注意力相当的性能，且在小块情形下相对于FlashAttention-2可实现高达14.7x的加速。代码公开。


<details>
  <summary>Details</summary>
Motivation: 理解MoBA背后的工作原理并提升其在GPU上的实际效率，使其在长上下文场景中的应用更具实用性。

Method: 建立MoBA的统计模型，推导出信噪比以量化架构参数对路由检索准确性的影响；提出两条改进路径：使用更小的块尺寸，以及在键上做短卷积以聚集相关信号；实现面向硬件的CUDA内核FlashMoBA以在小块尺寸下仍能高效执行；通过从头训练LLM来验证理论与实现的有效性并与密集注意力基线比较；给出性能对比与代码。

Result: 理论上揭示了架构参数与路由检索精度之间的联系（信噪比），并通过两条改进路径提升路由准确性；实验结果表明改进后的MoBA可以与密集注意力在性能上等效，且FlashMoBA在小块条件下相比现有内核（FlashAttention-2）达到最高14.7x的加速；代码开放。

Conclusion: 基于理论分析驱动的硬件感知实现将MoBA从理论方法落地为实际可用的长上下文注意力方案，路由准确性的提升与高效的CUDA实现共同推动在GPU场景中的实用性，使其在成本与性能之间实现折中更优的平衡。

Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [61] [Do Not Merge My Model! Safeguarding Open-Source LLMs Against Unauthorized Model Merging](https://arxiv.org/abs/2511.10712)
*Qinfeng Li,Miao Pan,Jintao Chen,Fu Teng,Zhiqiang Shen,Ge Su,Hao Peng,Xuhong Zhang*

Main category: cs.CR

TL;DR: 提出 MergeBarrier，一种可插拔防御，通过破坏受保护模型与同源模型之间的线性模态连通性（LMC），阻断低损失路径，从而防止未授权的模型合并与剽窃，且对准确率影响极小。


<details>
  <summary>Details</summary>
Motivation: 应对模型合并剽窃这一新威胁；现有防御在主动性、开放性兼容性和高安全性方面存在不足。

Method: 通过干扰 LMC，使受保护模型与同源模型之间难以找到低损失合并路径，从而主动阻止未授权的模型合并；实现即插即用，兼容开源设置。

Result: 实验表明 MergeBarrier 能有效防止模型合并剽窃，且对模型精度影响很小。

Conclusion: MergeBarrier 提供主动化、开源友好且低成本的防护方案，提升对模型合并剽窃的防护水平。

Abstract: Model merging has emerged as an efficient technique for expanding large language models (LLMs) by integrating specialized expert models. However, it also introduces a new threat: model merging stealing, where free-riders exploit models through unauthorized model merging. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify three critical protection properties that existing methods fail to simultaneously satisfy: (1) proactively preventing unauthorized merging; (2) ensuring compatibility with general open-source settings; (3) achieving high security with negligible performance loss. To address the above issues, we propose MergeBarrier, a plug-and-play defense that proactively prevents unauthorized merging. The core design of MergeBarrier is to disrupt the Linear Mode Connectivity (LMC) between the protected model and its homologous counterparts, thereby eliminating the low-loss path required for effective model merging. Extensive experiments show that MergeBarrier effectively prevents model merging stealing with negligible accuracy loss.

</details>


### [62] [BadThink: Triggered Overthinking Attacks on Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2511.10714)
*Shuaitong Liu,Renjue Li,Lijia Yu,Lijun Zhang,Zhiming Liu,Gaojie Jin*

Main category: cs.CR

TL;DR: BadThink 是一种 stealthy 的后门攻击，通过触发提示诱导大语言模型在链式推理（CoT）过程中“过度思考”，显著增加推理轨迹长度和计算成本，同时保持最终输出的一致性。实验在多种模型和任务上均实现了推理轨迹长度的大幅增加（在 MATH-500 上超过 17 倍），揭示了 CoT 系统中一个新的效率攻击面。


<details>
  <summary>Details</summary>
Motivation: 在提升推理能力的同时，链式推理对计算资源的需求成为新的攻击面。研究提出了一个能够悄无声息地降低系统效率的攻击向量，强调需要关注推理过程的成本与可检测性，而不仅仅是最终答案的正确性。

Method: 通过一种基于投毒的微调策略实现攻击；使用基于大型语言模型的迭代优化过程，生成高度自然的被污染数据，使模型在触发条件下输出膨胀的推理过程（但最终输出保持一致）。在多种最先进模型和推理任务上进行评估，验证了推理轨迹长度显著增加且具备隐蔽性和鲁棒性。

Result: 在多项任务和模型上，BadThink 不仅显著拉长推理轨迹长度（如 MATH-500 数据集上超过 17x），且保持输出的一致性与正确性，攻击对推理成本和延迟有实质性影响，但输出结果看似正常。攻击对现有评估方法在输出层面的检测具有一定的规避性。

Conclusion: 揭示了一个之前未被充分关注的安全隐患：通过操控推理效率来对 CoT 系统进行攻击，形成一种新的复杂攻击类型。该工作推动了对 CoT 安全性的研究，需发展检测推理轨迹异常、对抗惩罚性微调以及增强训练过程中的鲁棒性防御机制。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially improved the reasoning capabilities of large language models (LLMs), but have also introduced their computational efficiency as a new attack surface. In this paper, we propose BadThink, the first backdoor attack designed to deliberately induce "overthinking" behavior in CoT-enabled LLMs while ensuring stealth. When activated by carefully crafted trigger prompts, BadThink manipulates the model to generate inflated reasoning traces - producing unnecessarily redundant thought processes while preserving the consistency of final outputs. This subtle attack vector creates a covert form of performance degradation that significantly increases computational costs and inference time while remaining difficult to detect through conventional output evaluation methods. We implement this attack through a sophisticated poisoning-based fine-tuning strategy, employing a novel LLM-based iterative optimization process to embed the behavior by generating highly naturalistic poisoned data. Our experiments on multiple state-of-the-art models and reasoning tasks show that BadThink consistently increases reasoning trace lengths - achieving an over 17x increase on the MATH-500 dataset - while remaining stealthy and robust. This work reveals a critical, previously unexplored vulnerability where reasoning efficiency can be covertly manipulated, demonstrating a new class of sophisticated attacks against CoT-enabled systems.

</details>


### [63] [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](https://arxiv.org/abs/2511.10720)
*Runpeng Geng,Yanting Wang,Chenlong Yin,Minhao Cheng,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: 提出 PISanitizer：在长上下文中先定位并净化潜在被注入的 token，再让后端 LLM 生成回答，从而阻断注入指令的影响。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文情境下的提示注入风险，现有防御在长上下文中效果有限，因为注入指令在海量上下文中占比微小。

Method: 基于两点观察：注入攻击本质是让 LLM 遵循某条指令；LLM 的注意力机制决定了输出所关注的关键输入。通过有意让 LLM 遵循任意指令并随后对高注意力 token 进行净化，来消除注入影响，并在此过程中让注入者难以提升效果。

Result: 通过实验，PISanitizer 能有效防止提示注入，保持任务性能，优于现有防御，具备高效性和对优化型及强自适应攻击的鲁棒性。代码公开。

Conclusion: 在长上下文的提示注入防御方面引入以注意力为导向的净化策略，兼顾效用与安全性，具有实用潜力与可扩展性。

Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.

</details>


### [64] [SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses](https://arxiv.org/abs/2511.11381)
*Gioliano de Oliveira Braga,Pedro Henrique dos Santos Rocha,Rafael Pimenta de Mattos Paixão,Giovani Hoff da Costa,Gustavo Cavalcanti Morais,Lourenço Alves Pereira Júnior*

Main category: cs.CR

TL;DR: 对Wi-Fi CSI生物识别的系统化综述，揭示评估与安全方面的系统性不一致，提出统一评估框架、可复现性指南，以及面向攻击面的安全研究方向。


<details>
  <summary>Details</summary>
Motivation: 澄清CSI生物识别在安全性、鲁棒性与方法学上的不足，推动标准化、可重复的评估。


Method: 对现有工作在感知基础设施、信号表示、特征管线、学习模型与评估方法等维度进行系统综述；构建统一评估框架并通过实验揭示传统报道的隐患；分析可行的攻击面（重放、几何仿真、环境扰动）及其对风险分布的影响。

Result: 发现枢纽性问题包括总体准确率的依赖、对FAR/FRR/EER等指标的报告不足、缺乏按用户的风险分析、对威胁模型与对抗可行性的关注不足。提出统一评估框架、强调使用按类别EER、FCS、Gini系数等安全相关指标来暴露风险集中度。明确了潜在攻击面并展示方法学选择如何改变脆弱性画像。

Conclusion: CSI生物识别具有潜力但当前评估存在明显安全性不足，需通过规范化评估、可复现的实验设计以及面向未来的研究方向来提升其作为身份验证原语的可信度。

Abstract: Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security perspective, analyzing how existing work differs across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility. We construct a unified evaluation framework to empirically expose these issues and demonstrate how security-relevant metrics, such as per-class EER, FCS, and the Gini Coefficient, uncover risk concentration that remains hidden under traditional reporting practices. Our analysis highlights concrete attack surfaces and shows how methodological choices materially influence vulnerability profiles, which include replay, geometric mimicry, and environmental perturbation. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.

</details>


### [65] [AFLGopher: Accelerating Directed Fuzzing via Feasibility-Aware Guidance](https://arxiv.org/abs/2511.10828)
*Weiheng Bai,Kefu Wu,Qiushi Wu,Kangjie Lu*

Main category: cs.CR

TL;DR: 提出可行性感知的定向模糊测试方法 AFLGopher，通过可行性预测和动态更新提高到达目标和触发漏洞的速度，显著超越多款基线工具。


<details>
  <summary>Details</summary>
Motivation: 现有的定向模糊测试多将距离度量设为可行性无关的信号，无法准确反映分支执行的可行性，导致搜索效率下降。需要一种能在有限 traces 下预测分支可行性并通过运行时更新提升精度的 guiding 机制。

Method: 提出可行性感知的距离计算，并通过新型分类方法基于有限追踪预测所有分支的可行性，辅以运行时可行性更新机制逐步提高预测精度。实现工具 AFLGopher，并与 AFLGo、增强版 AFLGo、WindRanger、BEACON、SelectFuzz 等进行对比。

Result: 在到达目标方面，AFLGopher 相对于 AFLGo、BEACON、WindRanger、SelectFuzz、增强 AFLGo 的加速比分别为 3.76x、2.57x、3.30x、2.52x、2.86x；在触发已知漏洞方面的加速分别为 5.60x、5.20x、4.98x、4.52x、5.07x。

Conclusion: 可行性感知的距离计算显著提升了定向模糊测试的效率，AFLGopher 在到达目标和触发漏洞方面均获得对比基线的显著性能提升。

Abstract: Directed fuzzing is a useful testing technique that aims to efficiently reach target code sites in a program. The core of directed fuzzing is the guiding mechanism that directs the fuzzing to the specified target. A general guiding mechanism adopted in existing directed fuzzers is to calculate the control-flow distance between the current progress and the target, and use that as feedback to guide the directed fuzzing. A fundamental problem with the existing guiding mechanism is that the distance calculation is \emph{feasibility-unaware}.
  In this work, we propose feasibility-aware directed fuzzing named AFLGopher. Our new feasibility-aware distance calculation provides pragmatic feedback to guide directed fuzzing to reach targets efficiently. We propose new techniques to address the challenges of feasibility prediction. Our new classification method allows us to predict the feasibility of all branches based on limited traces, and our runtime feasibility-updating mechanism gradually and efficiently improves the prediction precision. We implemented AFLGopher and compared AFLGopher with state-of-the-art directed fuzzers including AFLGo, enhanced AFLGo, WindRanger, BEACON and SelectFuzz. AFLGopher is 3.76x, 2.57x, 3.30x, 2.52x and 2.86x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in reaching targets. AFLGopher is 5.60x, 5.20x, 4.98x, 4.52x, and 5.07x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in triggering known vulnerabilities.

</details>


### [66] [Armadillo: Robust Single-Server Secure Aggregation for Federated Learning with Input Validation](https://arxiv.org/abs/2511.10863)
*Yiping Ma,Yue Guo,Harish Karthikeyan,Antigoni Polychroniadou*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a secure aggregation system Armadillo that has disruptive resistance against adversarial clients, such that any coalition of malicious clients (within the tolerated threshold) can affect the aggregation result only by misreporting their private inputs in a pre-defined legitimate range. Armadillo is designed for federated learning setting, where a single powerful server interacts with many weak clients iteratively to train models on client's private data. While a few prior works consider disruption resistance under such setting, they either incur high per-client cost (Chowdhury et al. CCS '22) or require many rounds (Bell et al. USENIX Security '23). Although disruption resistance can be achieved generically with zero-knowledge proof techniques (which we also use in this paper), we realize an efficient system with two new designs: 1) a simple two-layer secure aggregation protocol that requires only simple arithmetic computation; 2) an agreement protocol that removes the effect of malicious clients from the aggregation with low round complexity. With these techniques, Armadillo completes each secure aggregation in 3 rounds while keeping the server and clients computationally lightweight.

</details>


### [67] [On the Information-Theoretic Fragility of Robust Watermarking under Diffusion Editing](https://arxiv.org/abs/2511.10933)
*Yunyi Ni,Ziyu Yang,Ze Niu,Emily Davis,Finn Carter*

Main category: cs.CR

TL;DR: 随着扩散模型的迭代编辑，鲁棒水印会被显著削弱甚至被移除；提出引导扩散攻击，水印恢复几乎为零且图像保留高视觉保真；给出对策与设计准则。


<details>
  <summary>Details</summary>
Motivation: 扩散驱动的图像编辑日益强大，成为鲁棒水印的新威胁；需要理论与实验评估水印在此情境下的鲁棒性并提出防御策略。

Method: 对迭代扩散变换下水印信息的互信息进行理论分析；提出一个明确针对水印信号并在生成过程中的 erasure 的引导扩散攻击算法；在近期深度学习水印方案上进行实验评估，量化水印恢复率与图像保真度。

Result: 在迭代扩散后，水印与载荷之间的互信息趋近于零，解码失败；攻击实现水印几乎不可恢复，但 regenerated 图像保持较高的视觉保真度；对多种深度学习水印方案均有显著效果。

Conclusion: 水印设计需要在对抗扩散生成与编辑的鲁棒性方面作出改进，提出设计要点与伦理讨论，指明未来方向。

Abstract: Robust invisible watermarking embeds hidden information in images such that the watermark can survive various manipulations. However, the emergence of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we investigate the intersection of diffusion-based image editing and robust image watermarking. We analyze how diffusion-driven image edits can significantly degrade or even fully remove embedded watermarks from state-of-the-art robust watermarking systems. Both theoretical formulations and empirical experiments are provided. We prove that as a image undergoes iterative diffusion transformations, the mutual information between the watermarked image and the embedded payload approaches zero, causing watermark decoding to fail. We further propose a guided diffusion attack algorithm that explicitly targets and erases watermark signals during generation. We evaluate our approach on recent deep learning-based watermarking schemes and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Finally, we discuss ethical implications of such watermark removal capablities and provide design guidelines for future watermarking strategies to be more resilient in the era of generative AI.

</details>


### [68] [Gynopticon: Consensus-Based Cheating Detection System for Competitive Games](https://arxiv.org/abs/2511.10992)
*Jeuk Kang,Jungheum Park*

Main category: cs.CR

TL;DR: 提出 GYNOPTICON，一个基于用户共识的隐私保护反作弊框架，通过轻量级客户端检测与服务器投票实现作弊判定，已在仿真和真实FPS环境中验证可行性与有效性。


<details>
  <summary>Details</summary>
Motivation: 在线游戏中的作弊对竞争性游戏和产业造成重大威胁；现有研究偏重 MMORPG，竞争性类型缺乏有效检测手段，且 kernel-level 反作弊带来隐私与安全担忧，因此需要一种透明、可扩展且保护隐私的替代方案。

Method: 结合轻量级客户端检测与服务器投票的分布式框架：当检测到可疑行为时，客户端投票给服务器，服务器聚合投票以达成共识，区分作弊者与合法玩家；设计强调透明性、较低的监控侵入性和隐私保护。

Result: 在受控仿真中验证可行性与需求；在真实FPS环境中实验表明对作弊者的检测具有可靠性；并通过公开数据集演示系统在长期游戏管理中的适用性与可持续性。

Conclusion: GYNOPTICON 提供了一种以用户为驱动、基于共识的反作弊替代方案，兼具实用性与隐私保护，适用于竞争性在线游戏的长期运营。

Abstract: Cheating in online games poses significant threats to the gaming industry, yet most prior research has concentrated on Massively Multiplayer Online Role-Playing Games (MMORPGs). Competitive genres-such as Multiplayer Online Battle Arena (MOBA), First Person Shooter (FPS), Real Time Strategy (RTS), and Action games-remain underexplored due to the difficulty of detecting cheating users and the demand for complex data and techniques. To address this gap, many game companies rely on kernel-level anti-cheat solutions, which, while effective, raise serious concerns regarding user privacy and system security. In this paper, we propose GYNOPTICON, a novel cheating detection framework that leverages user consensus to identify abnormal behavior. GYNOPTICON integrates a lightweight client-side detection mechanism with a server-side voting system: when suspicious activity is identified, clients cast votes to the server, which aggregates them to establish consensus and distinguish cheaters from legitimate players. This architecture enables transparency, reduces reliance on intrusive monitoring, and mitigates privacy risks. We evaluate GYNOPTICON in both a controlled simulation and a real-world FPS environment. Simulation results verify its feasibility and requirements, while real-world experiments confirm its effectiveness in reliably detecting cheating users. Furthermore, we demonstrate the system's applicability and sustainability for long-term game management using public datasets. GYNOPTICON represents a user-driven, consensus-based alternative to conventional anti-cheat systems, offering a practical and privacy-preserving solution for competitive online games.

</details>


### [69] [Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis](https://arxiv.org/abs/2511.11020)
*Farhad Abtahi,Fernando Seoane,Iván Pau,Mario Vega-Barbas*

Main category: cs.CR

TL;DR: Healthcare AI对数据污染极为脆弱，跨多种攻击向量且小样本即可侵害，检测常显著延迟甚至缺失；需多层防御和可解释模型等对策。


<details>
  <summary>Details</summary>
Motivation: 揭示医疗AI数据污染的风险、评估现有防御与法规的不足，并为构建更安全的医疗AI体系提供依据。

Method: 分析八种攻击场景，涵盖架构、基础设施、资源分配与供应链等四大类，给出关键发现与风险点。

Result: 仅需100-500个样本就可破坏AI系统；多数情况下成功率>60%；检测需6-12个月，甚至可能不发生；联邦学习与隐私保护等可隐藏攻击轨迹；供应链可使1个厂商污染50-200家机构；提出多层防御与对可解释性的新方向。

Conclusion: 建议强制对抗攻击性测试、基于集合的检测、隐私保护的安全机制及国际AI安全标准协调；提倡向可解释、具有可验证安全保证的模型转型，改进监管框架。

Abstract: Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 100-500 samples can compromise healthcare AI regardless of dataset size, often achieving over 60 percent success, with detection taking an estimated 6 to 12 months or sometimes not occurring at all. The distributed nature of healthcare infrastructure creates many entry points where insiders with routine access can launch attacks with limited technical skill. Privacy laws such as HIPAA and GDPR can unintentionally shield attackers by restricting the analyses needed for detection. Supply chain weaknesses allow a single compromised vendor to poison models across 50 to 200 institutions. The Medical Scribe Sybil scenario shows how coordinated fake patient visits can poison data through legitimate clinical workflows without requiring a system breach. Current regulations lack mandatory adversarial robustness testing, and federated learning can worsen risks by obscuring attribution. We recommend multilayer defenses including required adversarial testing, ensemble-based detection, privacy-preserving security mechanisms, and international coordination on AI security standards. We also question whether opaque black-box models are suitable for high-stakes clinical decisions, suggesting a shift toward interpretable systems with verifiable safety guarantees.

</details>


### [70] [SALT-V: Lightweight Authentication for 5G V2X Broadcasting](https://arxiv.org/abs/2511.11028)
*Liu Cao,Weizheng Wang,Qipeng Xie,Dongyu Wei,Lyutianyang Zhang*

Main category: cs.CR

TL;DR: SALT-V is a hybrid V2X authentication framework that blends ECDSA bootstrapping with GMAC data-frame authentication using an EST whitelist and Bloom filter revocation to achieve immediate verification with low latency and scalable performance.


<details>
  <summary>Details</summary>
Motivation: Traditional public-key schemes (ECDSA) incur ~2 ms verification, while TESLA-style key disclosures incur 20–100 ms; NR-V2X demands both rapid authentication and computational efficiency, creating a fundamental trade-off.

Method: Protocol stratification: 10% of frames (BOOT) signed by ECDSA to establish sender trust; remaining 90% (DATA) authenticated with lightweight GMAC leveraging the trust anchor. Ephemeral Session Tag (EST) whitelist enables 95% of messages to verify immediately without key disclosure; Bloom filters enable O(1) revocation checks (~1 μs). Evaluation shows microsecond computation and low latency.

Result: 0.035 ms average computation time (57× faster than pure ECDSA); 1 ms end-to-end latency; 41-byte overhead; scalable to 2000 vehicles; first practical solution to meet stringent real-time V2X safety requirements.

Conclusion: SALT-V convincingly demonstrates that a hybrid, protocol-stratified approach with EST whitelisting and Bloom-filter revocation can satisfy all safety-critical NR-V2X requirements for real-time operation.

Abstract: Vehicle-to-Everything (V2X) communication faces a critical authentication dilemma: traditional public-key schemes like ECDSA provide strong security but impose 2 ms verification delays unsuitable for collision avoidance, while symmetric approaches like TESLA achieve microsecond-level efficiency at the cost of 20-100 ms key disclosure latency. Neither meets 5G New Radio (NR)-V2X's stringent requirements for both immediate authentication and computational efficiency. This paper presents SALT-V, a novel hybrid authentication framework that reconciles this fundamental trade-off through intelligent protocol stratification. SALT-V employs ECDSA signatures for 10% of traffic (BOOT frames) to establish sender trust, then leverages this trust anchor to authenticate 90% of messages (DATA frames) using lightweight GMAC operations. The core innovation - an Ephemeral Session Tag (EST) whitelist mechanism - enables 95% of messages to achieve immediate verification without waiting for key disclosure, while Bloom filter integration provides O(1) revocation checking in 1 us. Comprehensive evaluation demonstrates that SALT-V achieves 0.035 ms average computation time (57x faster than pure ECDSA), 1 ms end-to-end latency, 41-byte overhead, and linear scalability to 2000 vehicles, making it the first practical solution to satisfy all safety-critical requirements for real-time V2X deployment.

</details>


### [71] [Bridging Local and Federated Data Normalization in Federated Learning: A Privacy-Preserving Approach](https://arxiv.org/abs/2511.11249)
*Melih Coşğun,Mert Gençtürk,Sinem Sav*

Main category: cs.CR

TL;DR: 提出在联邦学习中适配数据归一化，定义并实现联邦归一化（federated normalization），通过在参与方之间协作交换归一化参数来模拟池化归一化，同时通过同态加密保护参数以降低隐私风险。


<details>
  <summary>Details</summary>
Motivation: 在FL场景中数据分布异质且分散，局部归一化对非独立同分布数据的鲁棒性有限，池化归一化违背数据本地性，因此需要在保护隐私前提下实现接近池化归一化的归一化策略。

Method: 将常用的归一化技术改造为联邦版本，通过协同交换归一化参数实现协作归一化；提出一种面向联邦设置的同态加密下的k-th序排名元素（以及中位数）计算算法，作为安全高效的归一化支撑；给出基于多方全同态加密的隐私保护实现方案。

Result: 系统性评估不同联邦与局部归一化技术在异质FL场景中的影响，提出可替代池化归一化的联邦归一化框架，并给出安全高效的k-th及中位数计算方法，提升归一化在FL中的效果与隐私保护水平。

Conclusion: 联邦归一化在保持数据本地性的前提下接近或达到池化归一化的性能，同时通过隐私保护机制降低敏感参数泄露的风险，为FL中的数据预处理提供可行的隐私友好方案。

Abstract: Data normalization is a crucial preprocessing step for enhancing model performance and training stability. In federated learning (FL), where data remains distributed across multiple parties during collaborative model training, normalization presents unique challenges due to the decentralized and often heterogeneous nature of the data. Traditional methods rely on either independent client-side processing, i.e., local normalization, or normalizing the entire dataset before distributing it to parties, i.e., pooled normalization. Local normalization can be problematic when data distributions across parties are non-IID, while the pooled normalization approach conflicts with the decentralized nature of FL. In this paper, we explore the adaptation of widely used normalization techniques to FL and define the term federated normalization. Federated normalization simulates pooled normalization by enabling the collaborative exchange of normalization parameters among parties. Thus, it achieves performance on par with pooled normalization without compromising data locality. However, sharing normalization parameters such as the mean introduces potential privacy risks, which we further mitigate through a robust privacy-preserving solution. Our contributions include: (i) We systematically evaluate the impact of various federated and local normalization techniques in heterogeneous FL scenarios, (ii) We propose a novel homomorphically encrypted $k$-th ranked element (and median) calculation tailored for the federated setting, enabling secure and efficient federated normalization, (iii) We propose privacy-preserving implementations of widely used normalization techniques for FL, leveraging multiparty fully homomorphic encryption (MHE).

</details>


### [72] [Prompt Engineering vs. Fine-Tuning for LLM-Based Vulnerability Detection in Solana and Algorand Smart Contracts](https://arxiv.org/abs/2511.11250)
*Biagio Boi,Christian Esposito*

Main category: cs.CR

TL;DR: 本研究探讨大语言模型（LLMs）在非EVM生态（Solana 的 Rust、Algorand 的 PyTeal）中检测 OWASP 风险的可行性，设计合成数据集并评估提示工程、微调及两者结合三种配置的效果，发现提示工程在一般性鲁棒性方面表现良好，微调在 TEAL 等语义较弱语言上的精确性与召回率提升明显；同时分析平台架构对漏洞表现及检测难度的影响，提出平台特定映射与安全工具的局限性。结论认为在结合领域数据与分类体系时，基于LLMs的方法可用于智能合约的静态漏洞检测。


<details>
  <summary>Details</summary>
Motivation: 现有面向智能合约的安全研究多集中在 Ethereum EVM 生态，缺少对 Solana、Algorand 等非 EVM 平台的系统性研究。由于缺乏标注数据，亟需构建跨平台的漏洞分類体系与数据集，以评估 LLM 在跨语言、跨平台的漏洞检测能力。

Method: 构建基于 OWASP 漏洞分类的合成、带标注的 Rust（Solana）与 PyTeal（Algorand）智能合约片段数据集；设计并比较三种配置：提示工程、微调、两者结合；在不同漏洞类别上评估 LLM 的检测性能；分析 Solana/Algorand 的体系结构差异对漏洞的表现及可检测性的影响，给出平台特定映射与工具局限性分析。

Result: 实验结果表明：提示工程在提高鲁棒性方面具有良好的一般性表现；对语义不丰富的 TEAL 等语言，微调用以提升精确性与召回率更有效；并且平台架构差异影响漏洞的表现与可检测性，作者给出面向 Solana 与 Algorand 的平台特定映射，指出现有安全工具的局限性。

Conclusion: 在结合领域数据与分类体系的前提下，LLM 基于静态漏洞检测的可行性得到验证，未来可通过更丰富的领域数据和跨平台的培训流程进一步提升性能与覆盖范围。

Abstract: Smart contracts have emerged as key components within decentralized environments, enabling the automation of transactions through self-executing programs. While these innovations offer significant advantages, they also present potential drawbacks if the smart contract code is not carefully designed and implemented. This paper investigates the capability of large language models (LLMs) to detect OWASP-inspired vulnerabilities in smart contracts beyond the Ethereum Virtual Machine (EVM) ecosystem, focusing specifically on Solana and Algorand. Given the lack of labeled datasets for non-EVM platforms, we design a synthetic dataset of annotated smart contract snippets in Rust (for Solana) and PyTeal (for Algorand), structured around a vulnerability taxonomy derived from OWASP. We evaluate LLMs under three configurations: prompt engineering, fine-tuning, and a hybrid of both, comparing their performance on different vulnerability categories. Experimental results show that prompt engineering achieves general robustness, while fine-tuning improves precision and recall on less semantically rich languages such as TEAL. Additionally, we analyze how the architectural differences of Solana and Algorand influence the manifestation and detectability of vulnerabilities, offering platform-specific mappings that highlight limitations in existing security tooling. Our findings suggest that LLM-based approaches are viable for static vulnerability detection in smart contracts, provided domain-specific data and categorization are integrated into training pipelines.

</details>


### [73] [Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions](https://arxiv.org/abs/2511.11347)
*Shaowei Guan,Hin Chi Kwok,Ngai Fong Law,Gregor Stiglic,Vivian Hui*

Main category: cs.CR

TL;DR: RAG在医疗中的应用迅速扩展，但隐私风险治理不足，需建立标准评估框架、自动化工具及隐私保护路线图。


<details>
  <summary>Details</summary>
Motivation: 随着RAG在临床流程中的潜在效益，PHI暴露等隐私风险成为关键瓶颈。当前文献对隐私挑战的系统梳理不足。

Method: 对23篇医疗RAG应用文章和17篇隐私保护策略文章进行系统综述，提出面向数据存储、传输、检索、生成的管线化分析框架，解析潜在故障模式和威胁模型，并评估现有保护机制。

Result: 揭示关键空白与挑战，包括临床验证不足、缺乏统一评估框架与自动化评估工具；提出基于这些发现的可操作方向和优先级。

Conclusion: 提供面向研究者和实践者的隐私漏洞理解框架和实现临床有效性与隐私保护并重的RAG系统的路线图。

Abstract: Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.

</details>


### [74] [SEAL: Subspace-Anchored Watermarks for LLM Ownership](https://arxiv.org/abs/2511.11356)
*Yanbo Dai,Zongjie Li,Zhenlan Ji,Shuai Wang*

Main category: cs.CR

TL;DR: 提出 SEAL，一种子空间锚定水印框架，在模型潜在表示中嵌入多比特签名，支持白盒与黑盒验证；通过对锚样本的隐藏表示与正交比特向量对齐实现水印，同时尽量保持原始预测，实验在多数据集与六大LLMs上优于11种方法，展现鲁棒性与隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型等大型模型的知识产权得到有效保护。现有指纹/水印方法存在无法确证具体实例所有权、易被微调或蒸馏等后处理移除、隐蔽性与鲁棒性不足等问题，因此需要一种对模型性能影响极小、难以移除且可在白盒和黑盒条件下验证的水印方案。

Method: 通过模型编辑技术在潜在空间中将选定锚样本的隐含表示对齐到预定义的正交比特向量，从而在潜在子空间嵌入多比特签名；实现水印的同时尽量保持模型原始事实性预测不变，具备白盒与黑盒两种验证路径；并与11种现有指纹/水印方法在六个LLMs和多数据集上进行对比评估。

Result: 实验显示SEAL在有效性、保真度、效率与鲁棒性方面优于对比方法；在潜在攻击下，若攻击者具备对水印机制和签名的了解，仍能维持较强的验证性能，证实了方法的鲁棒性与实用性。

Conclusion: SEAL提供一种 stealthy、鲁棒且对原模型性能友好的水印方案，适用于大语言模型的所有权证明，在白盒与黑盒场景均具备强验签能力并能抵抗常见后处理攻击。

Abstract: Large language models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, demonstrating human-level performance in text generation, reasoning, and question answering. However, training such models requires substantial computational resources, large curated datasets, and sophisticated alignment procedures. As a result, they constitute highly valuable intellectual property (IP) assets that warrant robust protection mechanisms. Existing IP protection approaches suffer from critical limitations. Model fingerprinting techniques can identify model architectures but fail to establish ownership of specific model instances. In contrast, traditional backdoor-based watermarking methods embed behavioral anomalies that can be easily removed through common post-processing operations such as fine-tuning or knowledge distillation.
  We propose SEAL, a subspace-anchored watermarking framework that embeds multi-bit signatures directly into the model's latent representational space, supporting both white-box and black-box verification scenarios. Our approach leverages model editing techniques to align the hidden representations of selected anchor samples with predefined orthogonal bit vectors. This alignment embeds the watermark while preserving the model's original factual predictions, rendering the watermark functionally harmless and stealthy. We conduct comprehensive experiments on multiple benchmark datasets and six prominent LLMs, comparing SEAL with 11 existing fingerprinting and watermarking methods to demonstrate its superior effectiveness, fidelity, efficiency, and robustness. Furthermore, we evaluate SEAL under potential knowledgeable attacks and show that it maintains strong verification performance even when adversaries possess knowledge of the watermarking mechanism and the embedded signatures.

</details>


### [75] [Automated Side-Channel Analysis of Cryptographic Protocol Implementations](https://arxiv.org/abs/2511.11385)
*Faezeh Nasrabadi,Robert Künnemann,Hamed Nemati*

Main category: cs.CR

TL;DR: 首个来源于 WhatsApp 二进制实现的形式化模型；将侧信道泄露契约整合到协议模型并通过 DeepSec 验证；揭示前向保密性、克隆攻击、实现与规范差异及隐私侧信道攻击等关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 为一个大型闭源应用 WhatsApp 提供可验证的形式化安全分析，弥补对其实现与规范之间的差距，并评估对微体系结构层面的侧信道攻击的鲁棒性。

Method: 将 CryptoBap 的二进制级分析与 Ghidra 的逆向工程相结合，提取 WhatsApp 的首个正式模型；扩展 CryptoBap 以集成硬件泄漏契约到协议模型，再交给 DeepSec 做形式化证明与分析。

Result: 得到 WhatsApp 的首个二进制源自的形式化模型；证明了前向保密性、发现对抗后妥协的克隆攻击、指出实现与规范之间的功能性差距；提出分析侧信道攻击的新框架；通过该方法在 WhatsApp 中发现隐私攻击并在 BAC 协议的 unlinkability 上提出确认。

Conclusion: 展示仅依赖规范的分析可能遗漏关键漏洞，提出将侧信道泄漏契约引入协议建模以提升安全验证的可行性，并为大规模闭源应用的安全分析提供可重复的方法論。

Abstract: We extract the first formal model of WhatsApp from its implementation by combining binary-level analysis (via CryptoBap) with reverse engineering (via Ghidra) to handle this large closed-source application. Using this model, we prove forward secrecy, identify a known clone-attack against post-compromise security and discover functional gaps between WhatsApp's implementation and its specification. We further introduce a methodology to analyze cryptographic protocol implementations for their resilience to side-channel attacks. This is achieved by extending the CryptoBap framework to integrate hardware leakage contracts into the protocol model, which we then pass to the state-of-the-art protocol prover, DeepSec. This enables a detailed security analysis against both functional bugs and microarchitectural side-channel attacks. Using this methodology, we identify a privacy attack in WhatsApp that allows a side-channel attacker to learn the victim's contacts and confirm a known unlinkability attack on the BAC protocol used in electronic passports.
  Key contributions include (1) the first formal model of WhatsApp, extracted from its binary, (2) a framework to integrate side-channel leakage contracts into protocol models for the first time, and (3) revealing critical vulnerabilities invisible to specification-based methods.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [76] [Detection in Bistatic ISAC with Deterministic Sensing and Gaussian Information Signals](https://arxiv.org/abs/2511.10897)
*Xianxin Song,Xianghao Yu,Jie Xu,Derrick Wing Kwan Ng*

Main category: eess.SP

TL;DR: 在双基ISAC系统中，利用高斯信息信号与确定性感知信号共同提升目标检测性能，并通过SDR/SCA进行发射束形成优化以同时兼顾通信与探测。


<details>
  <summary>Details</summary>
Motivation: 解决ISAC系统中探测性能受限的问题，提出同时利用确定性感知信号与随机信息信号来提高目标检测的概率，并在满足CU最低SINR与总功率约束下优化发射波束。

Method: 提出基于NP检验的检测器，充分利用确定性感知信号与随机信息信号；推导闭式分析结果，证明两类信号均提升检测性能；将发射波束形成问题建模为在通信SINR约束和功率约束下最大化探测概率的非凸优化，通过半正定松弛（SDR）与交替凸逼近（SCA）求解。

Result: 仿真结果表明，基于NP检验的检测器明显优于将信息信号视为干扰的基准方案；利用两类信号的检测能力提升探测性能；更高的通信速率门限会将更多功率分配给高斯信息信号，导致确定性信号功率下降，进而削弱探测性能。

Conclusion: 将确定性感知信号与随机信息信号的协同利用用于探测可以显著提升ISAC双基系统的检测性能；但通信性能与探测性能存在权衡，需在CUSINR约束与总功率预算下优化波束以实现更优的综合性能。

Abstract: Integrated sensing and communications (ISAC) is a disruptive technology enabling future sixth-generation (6G) networks. This paper investigates target detection in a bistatic ISAC system, in which the base station (BS) transmits superimposed ISAC signals comprising both Gaussian information-bearing and deterministic sensing components to simultaneously provide communication and sensing functionalities. First, we develop a Neyman-Pearson (NP)-based detector that effectively utilizes both the deterministic sensing and random communication signals. Closed-form analysis reveals that both signal components contribute to improving the overall detection performance. Subsequently, we optimize the BS transmit beamforming to maximize the detection probability, subject to a minimum signal-to-interference-plus-noise ratio (SINR) constraint for the communication user (CU) and a total transmit power budget at the BS. The resulting non-convex beamforming optimization problem is addressed via semi-definite relaxation (SDR) and successive convex approximation (SCA) techniques. Simulation results demonstrate the superiority of the proposed NP-based detector, which leverages both types of signals, over benchmark schemes that treat information signals as interference. They also reveal that a higher communication-rate threshold directs more transmit power to Gaussian information-bearing signals, thereby diminishing deterministic-signal power and weakening detection performance.

</details>


### [77] [A Novel Partitioning Scheme for RIS Identification and Beamforming](https://arxiv.org/abs/2511.11335)
*Yarkın Gevez,Aymen Khaleel,Ertugrul Basar*

Main category: eess.SP

TL;DR: 提出一种对可重构智能表面(RIS)的动态分区方案，能够在识别与波束成形之间 jointly 分配 RIS 元件，提升信噪比（SNR）并保证识别性能，同时给出理论分析与仿真验证。


<details>
  <summary>Details</summary>
Motivation: 在 RIS 的应用中，识别（identification）和波束成形（beamforming）对RIS的资源有不同的性能需求，两者的资源分配存在权衡。需要一种动态、高效的分区机制，在给定的资源约束下同时优化两类任务的指标。

Method: 提出一种动态分区算法，将 RIS 元件在识别用户与波束成形用户之间进行分配。算法在考虑各自的性能度量（如识别可靠性与 SNR）的前提下，实时调整分区以提高系统整体性能。给出理论分析，并通过计算机仿真验证算法有效性。

Result: 在保持识别性能的前提下，显著提升了系统的信噪比（SNR），仿真结果与理论分析一致，证明了动态分区方案的有效性。

Conclusion: 动态 RIS 分区方案为联合识别与波束成形的资源分配提供了一种有效框架，能够在不同任务需求之间进行权衡并提升整体系统性能，且可扩展至多用户或动态场景。

Abstract: This letter introduces a novel partitioning scheme for reconfigurable intelligent surfaces (RISs) that simultaneously consider RIS identification and beamforming. The proposed scheme dynamicly and efficiently allocates RIS elements between identification and beamforming users, considering the different performance metrics associated with each of them. By employing a dynamic partitioning algorithm that efficiently manage the RIS resources (elements), the scheme significantly enhances the signal-to-noise ratio (SNR) while maintaining reliable identification performance. Finally, theoretical analysis and computer simulations are provided to demonstrate the validity of the proposed scheme.

</details>


### [78] [3D-HQAM Constellation Design and Performance Evaluation under AWGN](https://arxiv.org/abs/2511.11224)
*Sukhsagar,Nagendra Kumar,Ambuj Kumar Mishra,Vimal Bhatia,Ondrej Krejcar*

Main category: eess.SP

TL;DR: 3D-HQAM 构建与降维提升信号星座，显著提升 MED 与误码性能


<details>
  <summary>Details</summary>
Motivation: 在高阶星座下提高可靠性并降低决策复杂度，扩展二维 HQAM 到三维以获得更高的能量效率与距离谱

Method: 将二维六角 HQAM 系统扩展到三维信号空间，形成结构化晶格，提出降维（DR）技术以简化决策并得到闭式 SEP 表达式，在 AWGN 条件下推导

Result: 得到与仿真吻合的 SEP 解析表达式，3D HQAM 的最小欧氏距离相对于 2D 提升从 12.14%（8-HQAM）到 160.81%（1024-HQAM）不等，显著提升误码性能

Conclusion: 3D-HQAM 星座在高质量与可靠的下一代数字通信系统中具有潜在应用价值

Abstract: This paper proposes a simple and effective method for constructing higher-order three-dimensional (3D) signal constellations, aiming to enhance the reliability of digital communication systems. The approach systematically extends the conventional two-dimensional hexagonal quadrature amplitude modulation (2D-HQAM) constellation into a 3D-HQAM signal space, forming structured lattice configurations. To address the increased decision complexity resulting from a larger number of constellation points, a dimension reduction (DR) technique is introduced, allowing the derivation of closed-form symbol error probability (SEP) expressions under additive white Gaussian noise (AWGN) conditions. Theoretical SEPs closely match simulation results, validating the accuracy of the proposed method. The minimum Euclidean distance (MED) of the 3D constellations shows a minimum increase of 12.14% over 2D constellation for 8-HQAM, reaching up to 160.81% for 1024-HQAM constellations. This significant improvement in MED leads to enhanced error performance. Therefore, the proposed 3D constellations are promising candidates for high-quality and reliable next-generation digital communication systems.

</details>


### [79] [Testbed Evaluation of AI-based Precoding in Distributed MIMO Systems](https://arxiv.org/abs/2511.11251)
*Tianzheng Miao,Thomas Feys,Gilles Callebaut,Jarne Van Mulders,Md Arifur Rahman,François Rottenberg*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Distributed MIMO (D-MIMO) has emerged as a key architecture for future sixth-generation (6G) networks, enabling cooperative transmission across spatially distributed access points (APs). However, most existing studies rely on idealized channel models and lack hardware validation, leaving a gap between algorithmic design and practical deployment. Meanwhile, recent advances in artificial intelligence (AI)-driven precoding have shown strong potential for learning nonlinear channel-to-precoder mappings, but their real-world deployment remains limited due to challenges in data collection and model generalization. This work presents a framework for implementing and validating an AI-based precoder on a D-MIMO testbed with hardware reciprocity calibration. A pre-trained graph neural network (GNN)-based model is fine-tuned using real-world channel state information (CSI) collected from the Techtile platform and evaluated under both interpolation and extrapolation scenarios before end-to-end validation. Experimental results demonstrate a 15.7% performance gain over the pre-trained model in the multi-user case after fine-tuning, while in the single-user scenario the model achieves near-maximum ratio transmission (MRT) performance with less than 0.7 bits/channel use degradation out of a total throughput of 5.19 bits/channel use on unseen positions. Further analysis confirms the data efficiency of real-world measurements, showing consistent gains with increasing training samples, and end-to-end validation verifies coherent power focusing comparable to MRT.

</details>


### [80] [RadAround: A Field-Expedient Direction Finder for Contested IoT Sensing & EM Situational Awareness](https://arxiv.org/abs/2511.11392)
*Owen A. Maute,Blake A. Roberts,Berker Peköz*

Main category: eess.SP

TL;DR: 提出RadAround，一种低成本、模块化的被动2-D方位探测系统，利用机械偏转窄波天线与SCADA/ SDR协同，在现场生成高分辨率EM热图，适用于灾害应急 EMC 测试、战场频谱监测、电子入侵检测和战术EM态势感知等场景，实验显示可透墙检测计算设备、评估利用率并定位Faraday屏蔽箱的 EMI 漏漏源。


<details>
  <summary>Details</summary>
Motivation: 在对抗环境中需要被动、鲁棒、可快速部署的电磁场探测系统，用以态势感知、干扰源定位和合规性测试，同时成本需低、可在现场组装/定制。

Method: 通过机械驱动的窄波天线实现2-D方向探测；使用现场可部署的SCADA软件协调天线定位与射频采样（通过软件定义无线电 SDR），实现实时数据采集和EM热图生成；系统模块化设计便于快速适配EMC测试、战场监测、电子入侵检测、EMS A等应用；实验评估。

Result: 实验表明RadAround能够在透墙检测计算设备、评估设备利用率、并从Faraday屏蔽箱中定位EMI泄漏源，且能生成高分辨率的EM热图，成本低（使用COTS或3D打印件）。

Conclusion: RadAround提供了一套可在现场快速部署的被动EM探测平台，具有良好可扩展性和适应性，适用于EMC测试、应急和战场监测等多种场景，进一步研究可提升分辨率、抗干扰和系统集成。

Abstract: This paper presents RadAround, a passive 2-D direction-finding system designed for adversarial IoT sensing in contested environments. Using mechanically steered narrow-beam antennas and field-deployable SCADA software, it generates high-resolution electromagnetic (EM) heatmaps using low-cost COTS or 3D-printed components. The microcontroller-deployable SCADA coordinates antenna positioning and SDR sampling in real time for resilient, on-site operation. Its modular design enables rapid adaptation for applications such as EMC testing in disaster-response deployments, battlefield spectrum monitoring, electronic intrusion detection, and tactical EM situational awareness (EMSA). Experiments show RadAround detecting computing machinery through walls, assessing utilization, and pinpointing EM interference (EMI) leakage sources from Faraday enclosures.

</details>


### [81] [Physiological Measures of the Mental Workload in Users of a Lower Limb Exosuit: A Comparison of Subjective and Objective Metrics](https://arxiv.org/abs/2511.11414)
*Giulia Mariani,Chiara Lambranzi,Nicholas Cartocci,Giacinto Barresi,Christian Di Natali,Elena De Momi,Jesus Ortiz*

Main category: eess.SP

TL;DR: 该研究评估下肢软性外骨骼XoSoft在步行条件下的心理工作负荷（MWL），通过NASA-TLX、平均瞳孔直径变化百分比(APCPS)和Baevsky应激指数(SI)进行主客观对比，18名健康受试者在单任务/双任务条件下进行测试。结果显示任务难度、外骨骼启用状态和瞳孔动力学之间存在复杂耦合，可能在高MWL下达到饱和；瞳孔直径与NASA-TLX存在相关性，SI指标在不同刺激下显示不一致。研究还发现眼部不对称性，右眼对认知负荷更敏感。


<details>
  <summary>Details</summary>
Motivation: 在外骨骼辅助下评估并比较多种MWL测量指标的有效性，探索主观与客观指标的对应关系，以支持更好的人机交互设计和控制策略。

Method: 在18名健康受试者中进行步行实验，使用XoSoft下肢软性外骨骼，并引入数学任务形成双任务条件。评估工具包括NASA-TLX问卷、APCPS（平均瞳孔直径变化百分比）以及Baevsky应激指数（SI），同时分析瞳孔直径和心率等生理信号以及眼部是否对称。

Result: 研究发现任务难度、外骨骼激活状态与瞳孔动力学之间存在复杂交互，受试者在高MWL下可能达到饱和。瞳孔直径与NASA-TLX呈相关趋势，表明瞳孔可作为客观MWL指标。SI与眼科/心血管信号在不同刺激下表现出不一致的响应。还发现眼部存在不对称性，右眼对认知负荷更为敏感。

Conclusion: 瞳孔直径有潜力作为客观MWL指标用于评估外骨骼使用者的认知负荷；但需结合多种指标（包括主观量表与其他生理信号）以获得全面评估。不同MWL指标对不同刺激的反应差异提示在人机交互设计中应考虑多模态监测，以及眼部不对称性对评估结果的影响。

Abstract: Lower-limb exosuits are particularly relevant for individuals with some degree of mobility impairment, such as post-stroke patients or older adults with reduced movement capabilities. This study aims to investigate the mental workload (MWL) assessment of XoSoft, a lower-limb soft exoskeleton, using and comparing subjective and objective physiological metrics. The NASA-TLX questionnaire, the average percentage change in pupil size (APCPS), and the Baevsky stress index (SI) are compared. The experiments were conducted on 18 healthy subjects while walking and involved mathematical tasks to create a double-task condition. The results show a complex interaction between task difficulty, exoskeleton activation, and pupillary dynamics, suggesting that the subject might reach a saturated condition under a high mental load. Besides, the data indicate that pupil diameter may be an objective mental workload indicator that correlates with subjective NASA-TLX questionnaires. The discordant indications from the stress index suggest how different metrics of the ocular and cardiac levels respond differently to various stimuli and dynamics. Research has also revealed ocular asymmetry, with the right eye more sensitive to cognitive load.

</details>


### [82] [A Scalable and Exact Relaxation for Densest $k$-Subgraph via Error Bounds](https://arxiv.org/abs/2511.11451)
*Ya Liu,Junbin Liu,Wing-Kin Ma,Aritra Konar*

Main category: eess.SP

TL;DR: 提出一种可扩展且精确的连续罚化方法用于 Densest k-Subgraph (DkS)，基于误差界原理设计罚函数，证明罚化问题的全局/局部最优等价于原问题，并给出收敛性分析的非凸近端梯度算法，及在大规模实例上的实验结果。


<details>
  <summary>Details</summary>
Motivation: DkS NP-hard 且难以近似，尽管基于罚化的连续放松在实际中表现良好，但需要具备可扩展性、理论保证与收敛性分析的方案，以在大规模数据上保持解的质量。

Method: 基于误差界原理设计罚函数并构造罚化问题；发展非凸近端梯度算法，近端算子可闭式计算，降低每步复杂度；给出算法收敛性分析。

Result: 证明罚化问题的全局和局部最优解与原始 DkS 问题的最优解一致；实现了具有低每步复杂度的迭代方法。实验证明在大规模 DkS 及其变体 Dk1k2BS 上，算法在计算成本与解质量之间取得良好平衡。

Conclusion: 提供了一种可扩展且具有理论保证的连续罚化框架，适用于 DkS 及相关变体，便于在大规模实例上应用。

Abstract: Given an undirected graph and a size parameter $k$, the Densest $k$-Subgraph (D$k$S) problem extracts the subgraph on $k$ vertices with the largest number of induced edges. While D$k$S is NP--hard and difficult to approximate, penalty-based continuous relaxations of the problem have recently enjoyed practical success for real-world instances of D$k$S. In this work, we propose a scalable and exact continuous penalization approach for D$k$S using the error bound principle, which enables the design of suitable penalty functions. Notably, we develop new theoretical guarantees ensuring that both the global and local optima of the penalized problem match those of the original problem. The proposed penalized reformulation enables the use of first-order continuous optimization methods. In particular, we develop a non-convex proximal gradient algorithm, where the non-convex proximal operator can be computed in closed form, resulting in low per-iteration complexity. We also provide convergence analysis of the algorithm. Experiments on large-scale instances of the D$k$S problem and one of its variants, the Densest ($k_1, k_2$) Bipartite Subgraph (D$k_1k_2$BS) problem, demonstrate that our method achieves a favorable balance between computation cost and solution quality.

</details>


### [83] [Enabling Wireless Power Transfer (WPT) in Pinching Antenna Systems (PASS)](https://arxiv.org/abs/2511.11465)
*Deqiao Gan,Xiaoxia Xu,Xiaohu Ge,Yue Liu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出一种新型的PINching Antenna System (PASS) 结合无线电能传输框架，允许EHR与IDR共存，通过在接收端附近激活PINching天线并灵活调整辐射功率比，提升能量收集效率和通信质量。对变量强耦合问题给出双层优化，上层联合优化波束成形、PA位置及功率辐射比区间以最大化功率转换效率（PCE）并满足速率约束，下层在功率辐射比上 refine 以最大化总速率。两用户和多用户场景均给出高效解法；两用户采用AO-WMMSE求解，三者达到驻点解；多用户提出QT-LDT算法迭代更新PCE和总速率，且对PA位置和功率辐射比分步优化，推导出闭式解。数值结果显示与传统MIMO和固定辐射的PASS相比，PASS-WPT在EHR的PCE提升约81.45%/43.19%，在IDR的总速率提升约77.81%/31.91%。


<details>
  <summary>Details</summary>
Motivation: 在同时存在能量传输和信息传输的无线系统中，变量之间强耦合导致优化困难；需要新颖的结构和算法以同时提升能量收集效率和通信性能，并能在多用户场景下保持良好可扩展性。

Method: 提出 PASS-WPT 框架，在接收端两边激活 PINching 天线并可调辐射功率比。上层优化：联合波束成形、PA 位置与功率辐射比区间以最大化 PCE，满足速率约束。下层对功率辐射比进行细化以最大化总速率。两用户场景：AO 基于和 WMMSE 的算法获得波束成形、PA 位置与功率辐射比的驻点解。多用户场景：QT-LDT 算法迭代更新 PCE 与总速率，分步优化 PA 位置与功率辐射比，给出闭式解。

Result: 在两用户场景中，提出的 AO-WMMSE 算法可获得各变量的驻点解；在多用户场景中，QT-LDT 算法提供了对 PCE 与总速率的有效迭代更新策略，且对两类问题均给出闭式解。数值仿真显示相较于传统MIMO与固定辐射的 PASS，EHR 的 PCE 提升约 81.45%/43.19%，IDR 的总速率提升约 77.81%/31.91%。

Conclusion: PASS-WPT 框架可显著提升能量采集和通信性能，且在两用户和多用户场景均展现出较强的性能提升，证实了通过活化靠近接收端的 PINching 天线并灵活分配功率辐射比的策略在 WPT 条件下的有效性。

Abstract: A novel pinching antenna system (PASS) enabled wireless power transfer (WPT) framework is proposed, where energy harvesting receivers (EHRs) and information decoding receivers (IDRs) coexist. By activating pinching antennas (PAs) near both receivers and flexibly adjusting PAs' power radiation ratios, both energy harvesting efficiency and communication quality can be enhanced. A bi-level optimization problem is formulated to overcome the strong coupling between optimization variables. The upper level jointly optimizes transmit beamforming, PA positions, and feasible interval of power radiation ratios for power conversion efficiency (PCE) maximization under rate requirements, while the lower level refines power radiation ratio for the sum rate maximization. Efficient solutions are developed for both two-user and multi-user scenarios. 1) For the two-user case, where an EHR and an IDR coexist, the alternating optimization (AO)-based and weighted minimum mean square error (WMMSE)-based algorithms are developed to achieve the stationary solutions of transmit beamforming, PA positions, and power radiation ratios. 2) For the multi-user case, a quadratic transform-Lagrangian dual transform (QT-LDT) algorithm is proposed to iteratively update PCE and sum rate by optimizing PA positions and power radiation ratios individually. Closed-form solutions are derived for both maximization problems. Numerical simulation results demonstrate that the proposed PASS-WPT framework significantly outperforms conventional MIMO and the baseline PASS with fixed power radiation, which demonstrates that: i) Compared to the conventional MIMO and baseline PASS, the proposed PASS-WPT framework achieves 81.45% and 43.19% improvements in PCE of EHRs, and ii) also increases the sum rate by 77.81% and 31.91% for IDRs.

</details>


### [84] [SynthSoM-Twin: A Multi-Modal Sensing-Communication Digital-Twin Dataset for Sim2Real Transfer via Synesthesia of Machines](https://arxiv.org/abs/2511.11503)
*Junlong Chen,Ziwei Huang,Xuesong Cai,Xiang Cheng,Liuqing Yang*

Main category: eess.SP

TL;DR: 提出 SynthSoM-Twin，一个与现实世界时空一致的多模态合成数据集，用于实现感知-通信领域的 Sim2Real 迁移；并发现少量真实数据注入（<15%）即可达到甚至超过全面真实数据的效果。


<details>
  <summary>Details</summary>
Motivation: 解决多模态感知与通信任务在现实与仿真之间的迁移挑战；需要时空一致性与跨模态对齐，传统纯仿真或真实数据不足以覆盖场景；以SoM框架提升数据扩展与跨模态对齐。

Method: 提出一个框架，扩展现有真实数据集的模态数量和缺失模态处理；通过多模态感知辅助的目标检测和跟踪，确保静态与动态对象在现实和仿真中的时空一致性；将场景导入 AirSim、WaveFarer、Sionna RT 三个高保真仿真器，生成 66,868 张快照，包括 RGB、深度、LiDAR、mmWave 雷达点云，以及大尺度和小尺度的信道衰落数据；以两种跨模态下游任务（跨模态信道生成模型和多模态感知辅助的波束生成模型）验证，并探索真实数据注入阈值。

Result: 在 SynthSoM-Twin 上训练的模型实现了可观的实际性能；引入真实数据进一步提升 Sim2Real 迁移性；在真实数据比例低于 15% 时，达到与使用全部真实数据相当甚至更优的效果。

Conclusion: SynthSoM-Twin 为多模态感知-通信的 Sim2Real 提供了有效数据基础，降低对真实数据的依赖；所提出的跨模态生成模型及感知-辅助波束模型具备良好迁移潜力，提示未来在更大规模场景中可用少量真实数据实现良好迁移。

Abstract: This paper constructs a novel multi-modal sensing-communication digital-twin dataset, named SynthSoM-Twin, which is spatio-temporally consistent with the real world, for Sim2Real transfer via Synesthesia of Machines (SoM). To construct the SynthSoM-Twin dataset, we propose a new framework that can extend the quantity and missing modality of existing real-world multi-modal sensing-communication dataset. Specifically, we exploit multi-modal sensing-assisted object detection and tracking algorithms to ensure spatio-temporal consistency of static objects and dynamic objects across real world and simulation environments. The constructed scenario is imported into three high-fidelity simulators, i.e., AirSim, WaveFarer, and Sionna RT. The SynthSoM-Twin dataset contains spatio-temporally consistent data with the real world, including 66,868 snapshots of synthetic RGB images, depth maps, light detection and ranging (LiDAR) point clouds, millimeter wave (mmWave) radar point clouds, and large-scale and small-scale channel fading data. To validate the utility of SynthSoM-Twin dataset, we conduct Sim2Real transfer investigation by implementing two cross-modal downstream tasks via cross-modal generative models (CMGMs), i.e., cross-modal channel generation model and multi-modal sensing-assisted beam generation model. Based on the downstream tasks, we explore the threshold of real-world data injection that can achieve a decent trade-off between real-world data usage and models' practical performance. Experimental results show that the model training on the SynthSoM-Twin dataset achieves a decent practical performance, and the injection of real-world data further facilitates Sim2Real transferability. Based on the SynthSoM-Twin dataset, injecting less than 15% of real-world data can achieve similar and even better performance compared to that trained with all the real-world data only.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [85] [Support Recovery in One-bit Compressed Sensing with Near-Optimal Measurements and Sublinear Time](https://arxiv.org/abs/2511.10777)
*Xiaxin Li,Arya Mazumdar*

Main category: cs.IT

TL;DR: 提出在一位压缩感知中实现子线性时间的支持恢复算法，给出三类方案： universal 精确/近似以及概率性子线性运行时的精确恢复，显著降低运行时并给出对应的测量数。


<details>
  <summary>Details</summary>
Motivation: 降低传统 Ω(n) 运行时瓶颈，提供对不同需求的 universal 与 probabilistic 场景的子线性时间方案，提升 one-bit CS 的实用性。

Method: 设计两类方案：1) universal 案例，覆盖精确与 ε-近似两种需求，通过分组/编码与测量构造实现子线性读取与恢复，给出测量数 m 与运行时 D 的表达。2) probabilistic 子线性案列，在较低测量下实现高概率的精确恢复，结合结构化随机矩阵和稀疏支撑探索，改进前人工作。

Result: universaI exact: m=O(k^2 log(n/k) log n)，D=O(km)；universal ε-approx: m=O(k ε^{-1} log(n/k) log n)，D=O(ε^{-1} m)；probabilistic exact: m=O(k (log k)/(log log k) log n)，运行时间 O(m)，错误概率趋近零。

Conclusion: 给出三类子线性时间 1bCS 支持恢复方案，显著降低运行时与/或测量数，超越了 Matsumoto et al. 2023 与 Yang et al. 2025 等工作，扩展了 1bCS 的实际应用潜力。

Abstract: The problem of support recovery in one-bit compressed sensing (1bCS) aim to recover the support of a signal $x\in \mathbb{R}^n$, denoted as supp$(x)$, from the observation $y=\text{sign}(Ax)$, where $A\in \mathbb{R}^{m\times n}$ is a sensing matrix and $|\text{supp}(x)|\leq k, k \ll n$. Under this setting, most preexisting works have a recovery runtime $Ω(n)$. In this paper, we propose two schemes that have sublinear $o(n)$ runtime. (1.i): For the universal exact support recovery, a scheme of $m=O(k^2\log(n/k)\log n)$ measurements and runtime $D=O(km)$. (1.ii): For the universal $ε$-approximate support recovery, the same scheme with $m=O(kε^{-1}\log(n/k)\log n)$ and runtime $D=O(ε^{-1}m)$, improving the runtime significantly with an extra $O(\log n)$ factor in the number of measurements compared to the current optimal (Matsumoto et al., 2023). (2): For the probabilistic exact support recovery in the sublinear regime, a scheme of $m:=O(k\frac{\log k}{\log\log k}\log n)$ measurements and runtime $O(m)$, with vanishing error probability, improving the recent result of Yang et al., 2025.

</details>


### [86] [Joint Beamforming and Position Optimization for IRS-Aided SWIPT with Movable Antennas](https://arxiv.org/abs/2511.11148)
*Yanze Zhu,Qingqing Wu,Xinrong Guan,Ziyuan Zheng,Honghao Wang,Wen Chen,Yang Liu,Yuan Guo*

Main category: cs.IT

TL;DR: 提出了在SWIPT系统中结合IRS与可移动天线，联合优化BS与IRS的波束形成以及MA位置，以提升IDR速率与EHR功率，并通过WMMSE-BCD-MM-PDD等算法求解，给出可行性判定方法，仿真显示IRS在该场景下具有显著优势。


<details>
  <summary>Details</summary>
Motivation: SWIPT在物联网中实现信息传输与能量传输，但远距离传播导致能量传输效率低，需通过新兴的IRS与MA提升传播效率与可调性，从而兼顾IDR与EHR的需求。

Method: 将优化问题设定为在保证EHR约束的前提下，最大化IDR的加权和速率，变量包括基站（BS）的有源波束形成、IRS的无源相位移、以及MA的位置。为求解这类非凸问题，提出将WMMSE、BCD、MM和PDD等方法耦合的高效算法，并给出EHR达成性的可行性判定方法。

Result: 算法显著提高系统性能，仿真结果表明在所考场景下，IRS的优化配置可能带来比MA更高的性能增益。

Conclusion: 所提出的联合设计与优化框架有效提升了SWIPT系统的信息与能量传输性能，验证了IRS在SWIPT中的潜力，并揭示在该场景中IRS相较MA的优势；未来工作可在更广场景与实现复杂度方面扩展。

Abstract: Simultaneous wireless information and power transfer (SWIPT) has been envisioned as a promising technology to support ubiquitous connectivity and reliable sustainability in Internet-of-Things (IoT) networks, which, however, generally suffers from severe attenuation caused by long distance propagation, leading to inefficient wireless power transfer (WPT) for energy harvesting receivers (EHRs). This paper proposes to introduce emerging intelligent reflecting surface (IRS) and movable antenna (MA) technologies into SWIPT systems aiming at enhancing information transmission for information decoding receivers (IDRs) and improving receive power of EHRs. We consider to maximize the weighted sum-rate of IDRs via jointly optimizing the active and passive beamforming at the base station (BS) and IRS, respectively, as well as the positions of MAs, while guaranteeing the requirements of all EHRs. To tackle this challenging task due to the non-convexity of associated optimization, we develop an efficient algorithm combining weighted minimal mean square error (WMMSE), block coordinate descent (BCD), majorization-minimization (MM), and penalty duality decomposition (PDD) frameworks. Besides, we present a feasibility characterization method to examine the achievability of EHRs' requirements. Simulation results demonstrate the significant benefits of our proposed solutions. Particularly, the optimized IRS configuration may exhibit higher performance gain than MA counterpart under our considered scenario.

</details>


### [87] [Mutual Coupling in Continuous Aperture Arrays: Physical Modeling and Beamforming Design](https://arxiv.org/abs/2511.11225)
*Zhaolin Wang,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Yuanwei Liu*

Main category: cs.IT

TL;DR: 提出并分析连续孔径阵列中的相互耦合，给出耦合核的一般物理模型、无极化耦合核特性、以及对CAPA与SPDA的耦合影响，给出两种求解最优波束形成结构的方法，并在大孔径极限下分析阵列增益和波束形状，结果显示耦合导致各向异性和更高指向性。


<details>
  <summary>Details</summary>
Motivation: 解决连续孔径阵列中耦合的物理效应及其对波束形成的影响，弥补忽略耦合的传统无耦合模型，提供能够考虑极化、表面耗散与弓曲等因素的完整框架，并统一CAPA与SPDA的分析。

Method: 建立包含极化和表面耗散损失的通用物理模型；刻画单极化耦合核及其各向异性；将CAPA的波束形成问题表述为泛函优化，利用变分法推导最优波束形成结构；为求解耦合核的逆问题提出两种方法：kernel approximation method通过波数域变换和高斯-勒让德求积得到闭式解；以及共轭梯度法逐步迭代求解等效二次泛函优化；在大孔径极限分析最优阵列增益与波束形状；将耦合模型推广到离散空间阵列。

Result: 给出数值结果：耦合的SPDA收敛到CAPA极限，且不耦合模型存在物理不一致；极化导致阵列增益呈各向异性；耦合波束图比无耦合更具指向性。

Conclusion: 提出的连续耦合模型在理论上统一了CAPA与SPDA的分析，揭示耦合对方向性和增益的改进与复杂性，并提供两种实际可实现的求解策略；为设计考虑耦合效应的CAPA与SPDA提供了一个完整框架。

Abstract: The phenomenon of mutual coupling in continuous aperture arrays (CAPAs) is studied. First, a general physical model for the phenomenon that accounts for both polarization and surface dissipation losses is developed. Then, the unipolarized coupling kernel is characterized, revealing that polarization induces anisotropic coupling and invalidates the conventional half-wavelength spacing rule for coupling elimination. Next, the beamforming design problem for CAPAs with coupling is formulated as a functional optimization problem, leading to the derivation of optimal beamforming structures via the calculus of variations. To address the challenge of inverting the coupling kernel in the optimal structure, two methods are proposed: 1) the kernel approximation method, which yields a closed-form solution via wavenumber-domain transformation and GaussLegendre quadrature, and 2) the conjugate gradient method, which addresses an equivalent quadratic functional optimization problem iteratively. Furthermore, the optimal array gain and beampattern are analyzed at the large-aperture limit. Finally, the proposed continuous mutual coupling model is extended to spatially discrete arrays (SPDAs), and comprehensive numerical results are provided, demonstrating that: 1) coupled SPDA performance correctly converges to the CAPA limit, while uncoupled models are shown to violate physics, 2) polarization results in anisotropic array gain behavior, and 3) the coupled beampattern exhibits higher directivity than the uncoupled beampattern.

</details>


### [88] [SCL Decoding of Non-Binary Linear Block Codes](https://arxiv.org/abs/2511.11256)
*Jingyu Lin,Li Chen,Xiaoqian Ye*

Main category: cs.IT

TL;DR: 提出了一种针对二进制特征场F_{2^r}的非二进制线性块码NB-LBCs的逐步取消列表(SCL)解码。通过建立每个非二进制码字的二进制组成与r个二进制极化码码字的一对一映射，使对这r个极化码进行SCL解码的复杂度为子平方级，并提出一个r步解码路径排序策略以提升解码效率。仿真结果显示在扩展RS(eRS)与非二进制扩展BCH NB-eBCH码上，SCL解码优于现有软判决解码且需要更少的有限场代数运算。对于长度为16的eRS码，ML解码性能在中等列表大小下即可接近。


<details>
  <summary>Details</summary>
Motivation: NB-LBCs在纠错、抗 Burst 错误方面具有优势，然而软判决解码的实现存在较高复杂度和资源开销，因此需要更高效的解码方案。将非二进制码映射到二进制极化编码并进行SCL解码，是在保持性能的同时降低复杂度的一种新思路。

Method: 建立每个非二进制码字二进制组成到r个二进制极化码的逐字映射；对这r个极化码进行SCL解码，并引入一个r步解码路径排序策略以提升解码效率和分支管理；通过对扩展RS与NB-eBCH码的仿真评估其性能与算力开销。

Result: 仿真结果显示SCL解码可超越现有软判决解码，且在有限场代数运算方面更为高效；对于长度为16的eRS码，借助中等大小的列表尺寸即可实现接近ML的性能。

Conclusion: 将NB-LBCs的解码问题转化为对r个二进制极化码的SCL解码问题，并通过r步路径排序策略提升效率，能够在保持或提升解码性能的同时降低复杂度，尤其在eRS和NB-eBCH码上表现突出，且有望在更长码长或不同码型中实现接近ML的性能。

Abstract: Non-binary linear block codes (NB-LBCs) are an important class of error-correcting codes that are especially competent in correcting burst errors. They have broad applications in modern communications and storage systems. However, efficient soft-decision decoding of these codes remains challenging. This paper proposes successive cancellation list (SCL) decoding for NB-LBCs that are defined over a finite field of characteristic two, i.e., F_{2^r}, where r is the extension degree. By establishing a one-to-r mapping between the binary composition of each non-binary codeword and r binary polar codewords, SCL decoding of the r polar codes can be performed with a complexity that is sub-quadratic in the codeword length. An r-step decoding path sorting strategy is further proposed to facilitate the decoding. Simulation results on extended Reed-Solomon (eRS) and non-binary extended BCH (NB-eBCH) codes show that SCL decoding can outperform their state-of-the-art soft-decision decoding with fewer finite field arithmetic operations. For length-16 eRS codes, their maximum-likelihood (ML) decoding performances can be approached with a moderate list size.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [89] [Privacy protection under the exposure of systems' prior information](https://arxiv.org/abs/2511.10771)
*Le Liu,Yu Kawano,Ming Cao*

Main category: eess.SY

TL;DR: 提出基于点对点最大泄漏（PML）隐私的系统隐私保护框架，针对对手具备系统先验信息（如稳态）的情形，给出PML隐私的必要充分条件与可行设计流程；揭示PML、差分隐私（DP）和互信息隐私（MI）之间的关系；从PML角度重新分析卡尔曼滤波并给出稳态估计误差协方差的下界；并通过分布式传感的案例验证。


<details>
  <summary>Details</summary>
Motivation: 在涉及敏感信息的动态系统中， adversary可能具备系统先验信息（如稳态），使隐私保护任务更加严苛。现有隐私度量（如差分隐私、互信息隐私）在考虑该先验信息时可能不足以描述最坏情形下的隐私泄露，因此需要一个针对具备先验知识场景的系统性隐私保护框架。

Method: 建立面向离散时间线性时不变系统的点对点最大泄漏（PML）隐私约束，推导其必要且充分条件，并给出可行的设计流程与实现方法；分析PML隐私与DP、MI之间的关系；从PML视角重新解析卡尔曼滤波，推导在PML参数下的稳态估计误差协方差下界；通过分布式传感在智慧建筑中的案例进行验证。

Result: 得到PML隐私的必要充分条件和可行设计程序；揭示PML、DP、MI之间的关系；给出在给定PML参数下的卡尔曼滤波稳态误差协方差下界；案例显示该方法在分布式传感中的隐私保护效果。

Conclusion: PML提供对抗具备先验信息的对手的鲁棒隐私度量，与DP和MI之间的关系得到清晰阐明，可通过系统设计在分布式感知等场景实现隐私保护，并且可结合卡尔曼滤波的稳态性能分析。

Abstract: For systems whose states implicate sensitive information, their privacy is of great concern. While notions like differential privacy have been successfully introduced to dynamical systems, it is still unclear how a system's privacy can be properly protected when facing the challenging yet frequently-encountered scenario where an adversary possesses prior knowledge, e.g., the steady state, of the system. This paper presents a new systematic approach to protect the privacy of a discrete-time linear time-invariant system against adversaries knowledgeable of the system's prior information. We employ a tailored \emph{pointwise maximal leakage (PML) privacy} criterion. PML characterizes the worst-case privacy performance, which is sharply different from that of the better-known mutual-information privacy. We derive necessary and sufficient conditions for PML privacy and construct tractable design procedures. Furthermore, our analysis leads to insight into how PML privacy, differential privacy, and mutual-information privacy are related. We then revisit Kalman filters from the perspective of PML privacy and derive a lower bound on the steady-state estimation-error covariance in terms of the PML parameters. Finally, the derived results are illustrated in a case study of privacy protection for distributed sensing in smart buildings.

</details>


### [90] [Retail electricity costs and emissions incentives are misaligned for commercial and industrial power consumers](https://arxiv.org/abs/2511.10775)
*Fletcher T. Chapin,Akshay K. Rao,Adhithyan Sakthivelu,Carson I. Tucker,Eres David,Casey S. Chen,Erin Musabandesu,Meagan S. Mauter*

Main category: eess.SY

TL;DR: 构建并统一时空分辨的美国电力成本与排放数据集，发现大用户的成本与排放激励错配显著，且地理/时间上变异性大；站点选择与现有 tariff 结构限制电气化带来碳减排潜力。


<details>
  <summary>Details</summary>
Motivation: 随着电气化在美国商业与工业领域增长，Scope 2 排放和成本信息对决策者日益关键；然而现有数据碎片化，缺乏对成本激励与排放激励之间对齐程度的定量评估，因此需要统一数据与分析方法。

Method: 建立时空分辨的价格型需求响应和激励型需求响应数据集；开发离散数据的时间序列近似以解决结构性不兼容；统一地理异质数据；比较与MEF、AEF、DAM相关的数据，分析成本与排放激励的一致性。

Result: 在成本与碳排放激励上存在显著的时空异质性；站点选择成为影响电力成本和Scope 2排放的关键因素；现有电力 tariff 架构对经济激励与排放激励错配，鼓励消费碳密集电力；存在提高电气化碳减排的潜在障碍。

Conclusion: 需要重新设计 tariff 以对齐经济与排放激励，从而释放电气化的碳减排潜力；研究数据集可用于更有效的需求响应部署和电气化策略优化。

Abstract: Electrification is contributing to substantial growth in U.S. commercial and industrial loads, but the cost and Scope 2 carbon emission implications of this load growth are opaque for both power consumers and utilities. This work describes a unique spatiotemporally resolved data set of U.S. electricity costs and emissions and applies time series approximation methods to quantify the alignment of electricity cost and emission incentives for large commercial and industrial consumers. We present a comprehensive spatiotemporal dataset of U.S. price-based demand response (i.e., tariff) and incentive-based demand response (IBDR) programs, enabling direct comparison to previously published marginal emission factor (MEF), average emission factor (AEF), and day-ahead market (DAM) prices. We resolved the structural incompatibility and fragmentation of these datasets by developing time series approximations of discrete data and unifying geospatially heterogeneous datasets. Analysis of these datasets reveals significant spatial and temporal heterogeneity in cost and carbon emissions incentives for demand-side energy flexibility, underscoring the importance of site selection as a key factor influencing power costs and scope 2 emissions. Analysis also reveals broad misalignment of economic and emissions incentives under existing electricity tariff structures, meaning tariffs are incentivizing consumption of more carbon-intensive electricity, and highlighting potential barriers to electrification delivering carbon savings.

</details>


### [91] [Semantic Property Maps for Driving Applications](https://arxiv.org/abs/2511.10798)
*Marcus Greiff,Ray Zhang,Takeru Shirasawa,John Subosits*

Main category: eess.SY

TL;DR: 提出基于贝叶斯矩匹配的空间参数地图用于预测控制的车辆动力学建模；结合摄像机信息在线更新参数，并在与路径坐标一致的地图中外部化参数不确定性。


<details>
  <summary>Details</summary>
Motivation: 为预测控制提供在线、空间变化的参数信息，提高车辆在不同位置的动力学模型准确性与鲁棒性。

Method: 用摄像机观测与瞬时参数估计结合，应用贝叶斯矩匹配更新一个在路径坐标中的参数概率地图；地图使用与测量似然的共轭先验，参数的空间分布可外部化，利于控制系统访问；对相关参数的空间平滑性给出理论保证。

Result: 实现一个在线自适应的参数空间地图，提供局部的参数似然以及随空间变化的平滑性证明。

Conclusion: 该方法为基于参数地图的预测控制提供理论和实践基础，能在驾驶场景中提高控制性能与鲁棒性，尤其在参数随位置变化时。

Abstract: We consider the problem of estimating the parameters of a vehicle dynamics model for predictive control in driving applications. Instead of solely using the instantaneous parameters estimated from the vehicle signals, we combine this with cameras and update a probabilistic map with parameter estimates and semantic information using Bayesian moment matching. Key to this approach is the map representation, which is constructed with conjugate priors to the measurement likelihoods and defined in the same path coordinates as the vehicle controller, such that the map can be externalized to provide a local representation of the parameter likelihoods that vary in space. The result is a spatial map of vehicle parameters adapted online to enhance the driving control system. We provide theoretical guarantees on the smoothness of relevant parameter likelihood statistics as a function of space, which is critical for their use in predictive control.

</details>


### [92] [Tissue Activation Calculation in Dual-lead Deep Brain Stimulation](https://arxiv.org/abs/2511.10844)
*Anna Franziska Frigge,Alexander Medvedev*

Main category: eess.SY

TL;DR: 对于近距离多通道深脑刺激(DBS)电极的全局双通道模型显示，单纯的VTAs叠加在电极间相互作用显著时会低估激活范围；将电场叠加或激活函数叠加则易高估。结论是，在电极彼此距离较近的情况下，不能将VTAs独立计算，需考虑全局双通道耦合模型。该结论在MS震颤患者和偏内侧布设的强迫症患者中得到一致性验证。


<details>
  <summary>Details</summary>
Motivation: 解决近距离 DBS 电极之间的电场相互作用对刺激范围估计的影响，评估多通道/双通道模型在临床 DBS 编程中的必要性。

Method: 建立并比较全局双通道模型与基于单通道近似的多种叠加方法，应用于两组患者数据：一组是12例多发性硬化症（MS）震颤患者，另一组是偏内侧布放的强迫症（OCD）患者。对比三种近似：VTAs 的简单叠加、电场叠加、激活函数叠加，以及全局双通道模型的实际结果，评估激活的估计差异。

Result: 在通道近距布放下，单纯的VTAs叠加低估了激活区域的扩展；电场叠加或激活函数叠加则倾向于高估激活。全局双通道模型揭示了明显且复杂的相互作用，单独计算VTAs不能准确反映真实激活。MS组与OCD组中均观察到类似的VTAs差异。

Conclusion: 在近距离多通道 DBS 条件下，需避免单通道独立VTAs的近似，而应采用全局双通道或耦合模型以捕捉相互作用，否则可能导致刺激设计上的显著偏差。结果具有跨疾病数据的一致性。

Abstract: Deep Brain Stimulation (DBS) is a well-established neurosurgical treatment aiming at symptom alleviation in a range of neurological and psychiatric diseases. Computational models of DBS are widely used to investigate the effects of stimulation on neural tissue, to explore stimulation targets and sweetspots, and ultimately, to aid clinicians in the DBS programming by calculating the stimulation parameters. Commonly, DBS is performed bilaterally, i.e. with one lead in each brain hemisphere, where computational models are solved independently for one lead at a time. This paper treats scenarios where multiple DBS leads are implanted in close proximity to one another, resulting in interacting electrical fields and, therefore, potentially overlapping stimulation spreads. In particular, a global dual-lead model is compared to approximations derived from single-lead approaches in a cohort of twelve multiple sclerosis (MS) tremor patients. It is concluded that simple superposition of volumes of tissue activated (VTAs) underestimates activation, while superposition of electric fields or activating functions leads to overestimation. It is concluded that given close proximity of DBS leads, the VTA cannot be computed individually as stimulation fields exhibit significant and complex interaction. The approach is extended to modeling two obsessive compulsive disorder patients with medially placed leads, where similar VTA discrepancies as in the MS patient cohort are observed.

</details>


### [93] [Adaptive Digital Twin of Sheet Metal Forming via Proper Orthogonal Decomposition-Based Koopman Operator with Model Predictive Control](https://arxiv.org/abs/2511.10852)
*Yi-Ping Chen,Derick Suarez,Ying-Kuan Tsai,Vispi Karkaria,Guanzhong Hu,Zihan Chen,Ping Guo,Jian Cao,Wei Chen*

Main category: eess.SY

TL;DR: Adaptive digital twin for deformation-based metal forming using POD, Koopman operator, MPC, and online RLS; demonstrated on robotic English Wheel to adapt to varying toolpaths and achieve target shapes.


<details>
  <summary>Details</summary>
Motivation: Deformation-based metal forming presents strongly coupled spatial-temporal dynamics and nonlinear toolpath–material response, complicating DT application; artisan processes like English wheel lack autonomous digital counterparts.

Method: Combine POD for reduced-order physics, Koopman operator for linearized nonlinear dynamics in lifted space, MPC for real-time control, and online Recursive Least Squares to update operator coefficients as new data arrives.

Result: Experimental validation on a robotic English Wheel; deformation fields are modeled under varying toolpaths; the adaptive DT achieves the target shape and captures non-stationary behaviors.

Conclusion: The framework offers a generalizable, interpretable, adaptive, and computationally efficient DT approach that bridges reduced-order physics with data-driven adaptability for autonomous process control and optimization in nonlinear manufacturing.

Abstract: Digital Twin (DT) technologies are transforming manufacturing by enabling real-time prediction, monitoring, and control of complex processes. Yet, applying DT to deformation-based metal forming remains challenging because of the strongly coupled spatial-temporal behavior and the nonlinear relationship between toolpath and material response. For instance, sheet-metal forming by the English wheel, a highly flexible but artisan-dependent process, still lacks digital counterparts that can autonomously plan and adapt forming strategies. This study presents an adaptive DT framework that integrates Proper Orthogonal Decomposition (POD) for physics-aware dimensionality reduction with a Koopman operator for representing nonlinear system in a linear lifted space for the real-time decision-making via model predictive control (MPC). To accommodate evolving process conditions or material states, an online Recursive Least Squares (RLS) algorithm is introduced to update the operator coefficients in real time, enabling continuous adaptation of the DT model as new deformation data become available. The proposed framework is experimentally demonstrated on a robotic English Wheel sheet metal forming system, where deformation fields are measured and modeled under varying toolpaths. Results show that the adaptive DT is capable of controlling the forming process to achieve the given target shape by effectively capturing non-stationary process behaviors. Beyond this case study, the proposed framework establishes a generalizable approach for interpretable, adaptive, and computationally-efficient DT of nonlinear manufacturing systems, bridging reduced-order physics representations with data-driven adaptability to support autonomous process control and optimization.

</details>


### [94] [Heterogeneous CACC Coexistence: Simulation, Analysis, and Modeling](https://arxiv.org/abs/2511.11429)
*Lorenzo Ghiro,Marco Franceschini,Renato Lo Cigno,Michele Segata*

Main category: eess.SY

TL;DR: 混合CACCs车队的存在性和性能取决于具体组合；某些CACCs的混合运行可安全且高效，但也有组合导致安全、舒适性或吞吐量显著下降，需要系统设计与理论框架来支撑混合 platoons 的分析与设计。


<details>
  <summary>Details</summary>
Motivation: 市场竞争下，来自不同厂商的CACCs可能共存于同一编队，现有文献多基于同质编队，缺乏对异质编队的安全性与性能评估。本研究提出混合编队的概念并通过仿真探究其特性。

Method: 基于三种已有CACCs的混合仿真实验，分两部分：第一部分在孤立场景中研究单一混合编队在速度变化和紧急制动场景下的微观安全性；第二部分在高密度环路场景评估安全性、舒适性和交通吞吐量，并与标准自适应巡航（ACC）及人类驾驶进行对比。

Result: 研究发现某些CACCs混合组合能稳健且安全地运行；而其他组合在安全、舒适或效率方面存在显著局限性。在高密度环路场景中，混合编队对吞吐量的影响与单独CACCs及人类驾驶相比具有差异化表现，强调异质性对系统性能的非平凡影响。

Conclusion: 异质化引入的复杂互动需要谨慎的系统设计，并推动建立用于异质编队建模的理论框架，以界定安全工作包线并指导不同CACCs的互操作性和兼容性设计。

Abstract: The design of Cooperative Adaptive Cruise Control (CACC) algorithms for vehicle platooning has been extensively investigated, leading to a wide range of approaches with different requirements and performance. Most existing studies evaluate these algorithms under the assumption of homogeneous platoons, i.e., when all platoon members adopt the same CACC. However, market competition is likely to result in vehicles from different manufacturers implementing distinct CACCs. This raises fundamental questions about whether heterogeneous vehicles can safely cooperate within a platoon and what performance can be achieved. To date, these questions have received little attention, as heterogeneous platoons are difficult to model and analyze. In this work, we introduce the concept of mixed platoons, i.e., platoons made of vehicles running heterogeneous CACCs, and we study their performance through simulation-based experiments. We consider mixtures of three well-established CACCs from the literature. In the first part of the paper, we study a single mixed platoon in isolation to understand the microscopic effects on safety: we evaluate the performance of various CACC-mixtures across speed change and emergency braking scenarios. In the second part, we examine a high-density ring-road scenario to assess macroscopic impacts on safety, comfort, and traffic throughput, especially comparing throughput results with those obtained from vehicles controlled by a standard Adaptive Cruise Control (ACC) or by human drivers. Our findings highlight that some combinations of CACCs can operate robustly and safely, while others exhibit critical limitations in safety, comfort, or efficiency. These results emphasize the need for careful system design and the development of theoretical frameworks for modeling heterogeneous platoons.

</details>


### [95] [Convergence of Flow-Policy Gradient Learning for Linear Quadratic Regulator Problems](https://arxiv.org/abs/2511.11131)
*Farnaz Adib Yaghmaie,Arunava Naha*

Main category: eess.SY

TL;DR: 在离线的线性二次系统中，Flow Q-learning 的 one-step policy 通过与行为克隆正则化结合，提升策略表达并实现收敛性与稳定性分析，理论基于平均成本损失与策略梯度定理，仿真以线性化倒立摆验证。


<details>
  <summary>Details</summary>
Motivation: 将专家示范融入 actor-critic 框架，利用流式生成模型和行为克隆正则化来提升策略表达，同时在离线设置下研究 one-step policy 的收敛性与稳定性。

Method: 给出基于平均预期成本的 one-step policy 损失的新表述，并将行为克隆正则化引入，使得分析可以借助现有策略梯度定理的结果来研究收敛性与稳定性。

Result: 给出理论结果：在所考虑的离线线性二次系统中，one-step policy 具有收敛性与稳定性；通过对线性化倒立摆的仿真验证了理论结论。

Conclusion: 离线学习场景下，结合行为克隆的 one-step policy 能实现更稳定的学习过程并具备收敛性，为基于流模型的策略表示提供理论支撑。

Abstract: Flow $Q$-learning has recently been introduced to integrate learning from expert demonstrations into an actor-critic structure. Central to this innovation is the ``the one-step policy'' network, which is optimized through a $Q$-function that is regularized with the behavioral cloning from expert trajectories, allowing learning more expressive policies using flow-based generative models. In this paper, we studied the convergence property and stabilizablity of the one-step policy during learning for linear quadratic problems under the offline settings. Our theoretical results are based on a new formulation of the one-step policy loss based on the average expected cost, and regularized with the behavioral cloning loss. Such a formulation allows us to tap into existing strong theoretical results from the policy gradient theorem to study the convergence properties of the one-step policy. We verify our theoretical finding with simulation results on a linearized inverted pendulum.

</details>


### [96] [Prognostics and Health Management in Polymer Electrolyte Fuel Cells: Current Trends, Challenges, and Future Directions](https://arxiv.org/abs/2511.11180)
*Farideh Abdollahi,Kourosh Malek,Thomas Kadyk,Nadiia Kulyk,Christophe Gerling,Michael H. Eikerling*

Main category: eess.SY

TL;DR: 综述聚焦PEFC的PHM，梳理关键退化机理与物理/数据/混合建模方法，分析在从诊断/剩余使用寿命估计到实时系统响应的行动阶段的挑战，并提出以传感、AI、数字孪生及材料创新为核心的未来方向与研究路线。


<details>
  <summary>Details</summary>
Motivation: 提升聚合物电解质燃料电池的可靠性与寿命评估能力，降低故障风险；将诊断与RUL等信息无缝转化为动态负载管理、热管理与维护触发等可执行的决策，促进PEFC在实际应用中的可持续性。

Method: 通过系统性文献综述，梳理并比较物理感知、数据驱动与混合建模在PHM中的应用，总结退化机制、诊断与寿命估计的进展，识别关键挑战与未来研究方向。

Result: 构建了PEFC PHM领域的当前进展与挑战的综合地图，强调多堆系统管理、行动阶段的可执行性、以及数据稀缺等方面的待解决问题。

Conclusion: 要将PHM有效嵌入PEFC实际应用，需要解决数据不足、实现诊断/预测到实时行动的无缝衔接，并在传感、人工智能、数字孪生与材料创新等方面推进跨学科研究与协作。

Abstract: Prognostics and Health Management is crucial for the reliability and lifetime assessment of Polymer Electrolyte Fuel Cells (PEFCs). Here, we review the current advances on this topic, focusing mainly on key degradation mechanisms and methodologies such as physics-aware, data-driven, and hybrid modeling approaches. Key open challenges are analyzed, including the need for more accurate degradation modeling, effective management of multi-stack systems, and advancements in the currently underdeveloped action phase, in which diagnostic and prognostic insights are translated into real-time system responses, such as dynamic load derating, thermal-management adjustments, or automated maintenance triggers, to prevent failures and extend PEFC life. While notable strides have been made in recent years in diagnostics and remaining useful life estimation, it remains challenging to seamlessly integrate these insights into actionable strategies. Future directions highlight the need to address data scarcity and advance interdisciplinary research. Key focus areas include sensor integration, artificial intelligence, and digital twins. Additionally material innovations play a crucial role in bridging existing gaps. This work, therefore, intends to map the further development of Prognostics and Health Management systems toward ensuring the viability of PEFCs in practical applications.

</details>


### [97] [Numerical Discretization Schemes that Preserve Flatness](https://arxiv.org/abs/2511.11183)
*Ashutosh Jindal,Florentina Nicolau,David Martin Diego,Ravi Banavar*

Main category: eess.SY

TL;DR: 提出一种通过离散化映射构造的数值方案，以在离散时间系统中保持平整性（flatness）/差分平整性。


<details>
  <summary>Details</summary>
Motivation: 虽有连续时间系统的平整性在控制中有效，离散化往往破坏平整性，迫切需要保持平整性的离散化方法。

Method: 基于离散化映射的概念，借鉴作者的前期工作，设计并构造能在离散时间实现平整性的数值离散化方案。

Result: 给出能够在离散化过程中保持平整性的数值方案的构造方法（理论框架/证明），并可能给出示例。

Conclusion: 为实现数字控制中的平整化控制提供可保持平整性的离散化工具，提升离散时间实现的可行性和鲁棒性。

Abstract: Differential flatness serves as a powerful tool for controlling continuous time nonlinear systems in problems such as motion planning and trajectory tracking. A similar notion, called difference flatness, exists for discrete-time systems. Although many control systems evolve in continuous time, control implementation is performed digitally, requiring discretization. It is well known in the literature that discretization does not necessarily preserve structural properties, and it has been established that, in general, flatness is not preserved under discretization (whether exact or approximate). In this paper, inspired by our previous work [1] and based on the notion of discretization maps, we construct numerical schemes that preserve flatness.

</details>


### [98] [Modeling and Physics-Enhanced Fault Detection in Wastewater Pump Stations](https://arxiv.org/abs/2511.11304)
*Katayoun Eshkofti,Henrik Sandberg,Mikael Nilsson,Matthieu Barreau*

Main category: eess.SY

TL;DR: 提出高保真物理增强三泵污水站仿真器，用于生成数据、支持数据驱动分析与故障数据平衡，并给出嵌套模型F检验与切线残差法等故障检测框架，辅助早期诊断与基于条件的维护。


<details>
  <summary>Details</summary>
Motivation: 污水泵站监测仍以人工方式，缺乏可扩展的算法与丰富数据。现有数据稀缺，难以进行故障诊断、状态评估及数据驱动分析，因此需要一个参数驱动、可泛化的仿真平台来生成高质量数据并验证检测框架。

Method: 开发一个一秒分辨率的物理增强仿真器，耦合水力与机械动力学，覆盖三泵站的瞬态行为，参数驱动、可迁移至其他站点；将仿真数据与高频SCADA数据进行对比验证；提出嵌套模型F检验以检测泵退化或系统故障，及切线残差法用于将泵故障与系统故障区分；支持 what-if 情景分析并生成平衡故障数据集。

Result: 与市政站点高频SCADA数据对比显示关键运行指标高度一致；可以生成平衡的故障数据集；所提框架在故障检测与诊断方面表现鲁棒，便于基于流量和水头的早期诊断与维护决策。

Conclusion: 该框架为污水泵站的监测、诊断与条件维护提供可操作的工具和方法，仿真器具备可迁移性，能促进数据驱动分析与研究。

Abstract: Monitoring wastewater pump stations is essential because they are critical infrastructure. However, monitoring is still often performed manually due to the lack of suitable algorithmic methods and data. This paper introduces a high-fidelity, physics-enhanced simulator of a three-pump wastewater station that captures transient hydro-mechanical dynamics at a one-second resolution. The simulator is fully parameter-driven, adaptable to other wastewater stations, and capable of generating datasets for data-driven analytics. It can also generate balanced faulty datasets when real failures are scarce or confidential. A comparison with high-frequency SCADA data from a municipal station shows strong agreement across key operational metrics. Furthermore, the paper proposes robust statistical and mathematical frameworks for fault detection and isolation, including a nested-model F-test to detect pump degradation or system faults, and a tangent residual approach to distinguish pump faults from system faults using operating-point kinematics. This framework enables what-if studies, facilitates early fault diagnosis based on flow rate and head, and provides actionable insights for condition-based maintenance in wastewater pumping infrastructure.

</details>


### [99] [Policy Optimization for Unknown Systems using Differentiable Model Predictive Control](https://arxiv.org/abs/2511.11308)
*Riccardo Zuliani,Efe C. Balta,John Lygeros*

Main category: eess.SY

TL;DR: Hybrid MPC policy optimization using differentiable optimization plus zeroth-order gradient estimates to handle model uncertainty; improves transient performance with convergence guarantees on a 12D quadcopter.


<details>
  <summary>Details</summary>
Motivation: Model-based policy optimization degrades when system dynamics are inaccurate, especially for MPC that relies on the model for real-time planning; need a method that fuses model-based and model-free gradients with guarantees under uncertainty.

Method: A novel framework integrating differentiable optimization (for MPC) with zeroth-order (model-free) gradient estimation, yielding a hybrid gradient estimator and optimized policy under uncertainty; validated on nonlinear control task.

Result: Faster transient performance compared to fully data-driven approaches; convergence guarantees maintained even under model uncertainty; demonstrated on a 12-dimensional quadcopter model.

Conclusion: The proposed framework provides a robust approach for MPC-based policy optimization in the presence of model errors, achieving better transient performance while preserving convergence, as shown on a high-dimensional quadcopter system.

Abstract: Model-based policy optimization often struggles with inaccurate system dynamics models, leading to suboptimal closed-loop performance. This challenge is especially evident in Model Predictive Control (MPC) policies, which rely on the model for real-time trajectory planning and optimization. We introduce a novel policy optimization framework for MPC-based policies combining differentiable optimization with zeroth-order optimization. Our method combines model-based and model-free gradient estimation approaches, achieving faster transient performance compared to fully data-driven approaches while maintaining convergence guarantees, even under model uncertainty. We demonstrate the effectiveness of the proposed approach on a nonlinear control task involving a 12-dimensional quadcopter model.

</details>


### [100] [Data-Driven Stabilization of Continuous-Time LTI Systems from Noisy Input-Output Data](https://arxiv.org/abs/2511.11417)
*Alessandro Bosso,Marco Borghesi,Andrea Iannelli,Bowen Yi,Giuseppe Notarstefano*

Main category: eess.SY

TL;DR: 在带噪声的输入–输出轨迹条件下，通过基于非最小实现观测器和仅依赖数据的LMI反馈律实现对连续时间LTI系统的输出反馈稳定化控制；给出LMI可行性等价于对所有与数据一致的非最小实现的稳定化的必要充分条件、与信噪比相关的可行性条件、噪声能量界限的估计指南以及数值仿真。


<details>
  <summary>Details</summary>
Motivation: 在缺乏精确系统模型时，利用带噪声的输入–输出轨迹进行直接的数据驱动稳定化设计，是控制领域的核心挑战之一。

Method: 设计一个针对植物的非最小实现的观测器，并构造一个仅依赖可获得数据的线性矩阵不等式(LMI)反馈律；在满足区间激励条件和知晓噪声能量界限的前提下，证明LMI的可行性等价于稳定化所有与数据一致的非最小实现；给出与信噪比相关的可行性条件、噪声能量界限的计算/估计指南，并通过数值仿真验证方法有效性。

Result: 给出LMI可行性与稳定化之间的必要充分条件的理论联系；提出与信噪比相关的可行性判据、噪声能量界限的计算指南；并通过数值仿真展示所提方法的有效性。

Conclusion: 提出了一种具有理论保证的直接数据驱动稳定化框架：在含噪数据下，通过非最小实现观测器与LMI反馈律实现输出反馈稳定化，并给出可操作的可行性条件与实现指南。

Abstract: We present an approach to compute stabilizing controllers for continuous-time linear time-invariant systems directly from an input-output trajectory affected by process and measurement noise. The proposed output-feedback design combines (i) an observer of a non-minimal realization of the plant and (ii) a feedback law obtained from a linear matrix inequality (LMI) that depends solely on the available data. Under a suitable interval excitation condition and knowledge of a noise energy bound, the feasibility of the LMI is shown to be necessary and sufficient for stabilizing all non-minimal realizations consistent with the data. We further provide a condition for the feasibility of the LMI related to the signal-to-noise ratio, guidelines to compute the noise energy bound, and numerical simulations that illustrate the effectiveness of the approach.

</details>


### [101] [Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems](https://arxiv.org/abs/2511.11567)
*Allen Emmanuel Binny,Anushri Dixit*

Main category: eess.SY

TL;DR: An iterative conformal prediction framework adapts the ego-agent to endogenous distribution shift caused by reactive agents, delivering probabilistic safety guarantees and improved success rates in multi-agent simulations.


<details>
  <summary>Details</summary>
Motivation: Safe, uncertainty-aware motion planning in settings where surrounding agents are interactive and respond to predictions, causing distribution shifts that traditional conformal methods cannot handle.

Method: Introduce an iterative conformal prediction pipeline that (i) models endogenous distribution shift, (ii) iteratively updates the ego-agent controller's uncertainty predictions, and (iii) establishes convergence conditions for the pipeline; validated in simulation for 2- and 3-agent interactions.

Result: The framework achieves collision avoidance without excessive conservatism and yields up to 9.6% higher success rates compared with other conformal-prediction baselines.

Conclusion: The proposed iterative conformal prediction approach provides probabilistic safety guarantees in interactive, closed-loop scenarios and effectively adapts to evolving behaviors of reactive agents.

Abstract: Uncertainty-aware prediction is essential for safe motion planning, especially when using learned models to forecast the behavior of surrounding agents. Conformal prediction is a statistical tool often used to produce uncertainty-aware prediction regions for machine learning models. Most existing frameworks utilizing conformal prediction-based uncertainty predictions assume that the surrounding agents are non-interactive. This is because in closed-loop, as uncertainty-aware agents change their behavior to account for prediction uncertainty, the surrounding agents respond to this change, leading to a distribution shift which we call endogenous distribution shift. To address this challenge, we introduce an iterative conformal prediction framework that systematically adapts the uncertainty-aware ego-agent controller to the endogenous distribution shift. The proposed method provides probabilistic safety guarantees while adapting to the evolving behavior of reactive, non-ego agents. We establish a model for the endogenous distribution shift and provide the conditions for the iterative conformal prediction pipeline to converge under such a distribution shift. We validate our framework in simulation for 2- and 3- agent interaction scenarios, demonstrating collision avoidance without resulting in overly conservative behavior and an overall improvement in success rates of up to 9.6% compared to other conformal prediction-based baselines.

</details>
