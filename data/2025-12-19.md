<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 7]
- [eess.SY](#eess.SY) [Total: 13]
- [cs.CR](#cs.CR) [Total: 21]
- [eess.SP](#eess.SP) [Total: 9]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 62]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Information theory and discriminative sampling for model discovery](https://arxiv.org/abs/2512.16000)
*Yuxuan Bao,J. Nathan Kutz*

Main category: cs.IT

TL;DR: 本文将Fisher信息矩阵（FIM）与SINDy相结合，用信息度量指导数据采样，提升混沌与非混沌动力学系统的模型发现效率与数据利用率。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动的动力学建模中，合理选择采样位置和时间能显著降低数据需求。Fisher信息与香农熵能量化观测对参数与动力学学习的贡献，提供理论性采样准则。

Method: 在SINDy框架中计算FIM并可视化单条轨迹及多初始条件下的信息分布；通过谱分析解释bagging的统计增益；使用信息量和熵指标提出三类场景（单轨迹、可调控制参数、多初始条件）下的样本优先级策略。

Result: 展示了在若干混沌与非混沌示例中，基于FIM的优先采样能提升学习效率、减少所需样本，并解释了bagging在提升鲁棒性与信息覆盖面的机制。

Conclusion: 以量化信息为驱动的采样与实验设计可在SINDy等模型发现任务中显著提高数据效率，为进一步的主动学习与实验规划提供了可操作的理论和实践依据。

Abstract: Fisher information and Shannon entropy are fundamental tools for understanding and analyzing dynamical systems from complementary perspectives. They can characterize unknown parameters by quantifying the information contained in variables, or measure how different initial trajectories or temporal segments of a trajectory contribute to learning or inferring system dynamics. In this work, we leverage the Fisher Information Matrix (FIM) within the data-driven framework of {\em sparse identification of nonlinear dynamics} (SINDy). We visualize information patterns in chaotic and non-chaotic systems for both single trajectories and multiple initial conditions, demonstrating how information-based analysis can improve sampling efficiency and enhance model performance by prioritizing more informative data. The benefits of statistical bagging are further elucidated through spectral analysis of the FIM. We also illustrate how Fisher information and entropy metrics can promote data efficiency in three scenarios: when only a single trajectory is available, when a tunable control parameter exists, and when multiple trajectories can be freely initialized. As data-driven model discovery continues to gain prominence, principled sampling strategies guided by quantifiable information metrics offer a powerful approach for improving learning efficiency and reducing data requirements.

</details>


### [2] [Optimal Key Rates for Decentralized Secure Aggregation with Arbitrary Collusion and Heterogeneous Security Constraints](https://arxiv.org/abs/2512.16112)
*Zhou Li,Xiang Zhang,Giuseppe Caire*

Main category: cs.IT

TL;DR: This paper studies decentralized secure aggregation (DSA) under arbitrary collusion and heterogeneous security constraints, characterizing optimal communication and source key rates; the minimal source key rate is given by solving a linear program.


<details>
  <summary>Details</summary>
Motivation: Traditional DSA requires large shared keys to protect all non-sum information against collusion. The paper aims to reduce key overhead by allowing different security requirements for different subsets of users (security set) and different possible collusion sets.

Method: Define collections of security sets and collusion sets; for each pair (security set, collusion set) require secrecy of inputs in the security set from the collusion set. Analyze feasibility and derive information-theoretic bounds. Characterize optimal communication and source key rates; reduce the minimal key-rate characterization to a linear program.

Result: Exact characterization of optimal communication and source key rates for arbitrary collections of security and collusion sets. Key contribution: minimal number of key bits per input bit (optimal source key rate) necessary and sufficient for feasibility, computable by solving a linear program.

Conclusion: The framework generalizes traditional DSA by supporting heterogeneous security constraints and arbitrary collusion; it substantially clarifies the tradeoff between communication and key resources and offers a computable criterion (LP) to determine minimal key sharing needed for secure aggregation.

Abstract: Decentralized secure aggregation (DSA) considers a fully-connected network of $K$ users, where each pair of users can communicate bidirectionally over an error-free channel. Each user holds a private input, and the goal is for each user to compute the sum of all inputs without revealing any additional information, even in the presence of collusion among up to $T$ users. Traditional DSA typically requires large key sizes to protect all information except for the input sum and the information of colluding users. To mitigate the source keys overhead, we study decentralized secure aggregation with arbitrary collusion and heterogeneous security constraints. In this setting, the inputs of a predefined collection of user subsets, called the \emph{security set} $\bm{\mathcal{S}}$, must be protected from another predefined collection, the \emph{collusion set} $\bm{\mathcal{T}}$. For an arbitrary security set $\mathcal{S}\in \bm{\mathcal{S}}$ and an arbitrary collusion set $\mathcal{T}\in \bm{\mathcal{T}}$, we characterize the optimal communication and source key rates. A key contribution of this work is the characterization of the optimal source key rate, i.e., the minimum number of key bits per input bit that must be shared among users for decentralized secure aggregation with arbitrary collusion and heterogeneous security constraints to be feasible. In general, this characterization reduces to solving a linear program.

</details>


### [3] [New Quantum Stabilizer Codes from generalized Monomial-Cartesian Codes constructed using two different generalized Reed-Solomon codes](https://arxiv.org/abs/2512.16482)
*Oisin Campion,Fernando Hernando,Gary McGuire*

Main category: cs.IT

TL;DR: 提出并研究了广义单项式笛卡尔码（GMCC），将两种广义Reed–Solomon码结合构造GMCC，并给出其Hermitian自正交的充分条件，从而构造出新的量子码。


<details>
  <summary>Details</summary>
Motivation: 将经典的广义Reed–Solomon（GRS）码推广到更一般的单项式笛卡尔码结构，以获得更丰富的代数码族，并为构造量子码提供新的自正交码源。

Method: 定义GMCC的代数与编码结构，说明如何通过组合两种不同的GRS码来构造GMCC，推导出能够保证GMCC在Hermitian内积下自正交的充分代数条件。

Result: 给出GMCC的具体构造方法和自正交判据，从而得到一类新的Hermitian自正交线性码，并基于此构造出若干新的量子码。

Conclusion: GMCC是GRS的自然推广，所给构造和自正交条件为量子码的设计提供了新的途径，扩展了可用于量子编码的经典码族。

Abstract: In this work, we define Generalized Monomial Cartesian Codes (GMCC), which constitute a natural extension of generalized Reed-Solomon codes. We describe how two different generalized Reed-Solomon codes can be combined to construct one GMCC. We further establish sufficient conditions ensuring that the GMCC are Hermitian self-orthogonal, thus leading to new constructions of quantum codes.

</details>


### [4] [Novel Inconsistency Results for Partial Information Decomposition](https://arxiv.org/abs/2512.16662)
*Philip Hendrik Matthias,Abdullah Makkeh,Michael Wibral,Aaron J. Gutknecht*

Main category: cs.IT

TL;DR: The paper proves that three fundamental properties of classical information theory—non-negativity, the chain rule, and invariance under invertible transformations—cannot all hold in any Partial Information Decomposition (PID) framework, and strengthens Rauh et al.'s inconsistency result.


<details>
  <summary>Details</summary>
Motivation: PID aims to decompose information about a target into redundant, unique, and synergistic parts but many competing proposals exist; proving incompatibility results clarifies which axioms cannot coexist and guides principled trade-offs.

Method: Use the mereological approach to PID to formalize axioms and derive logical contradictions; construct proofs showing mutual inconsistency of the targeted properties and extend Rauh et al.'s classical impossibility by tightening assumptions or constructions.

Result: Main theorem: non-negativity, chain rule, and invariance under invertible transformations are mutually incompatible in PID. Also a strengthened impossibility result relative to Rauh et al.'s earlier work.

Conclusion: Any PID must give up at least one of these seemingly fundamental properties; inconsistency results help map the space of viable PID definitions and force explicit choices about which axioms to abandon or relax.

Abstract: Partial Information Decomposition (PID) seeks to disentangle how information about a target variable is distributed across multiple sources, separating redundant, unique, and synergistic contributions. Despite extensive theoretical development and applications across diverse fields, the search for a unique, universally accepted solution remains elusive, with numerous competing proposals offering different decompositions. A promising but underutilized strategy for making progress is to establish inconsistency results, proofs that certain combinations of intuitively appealing axioms cannot be simultaneously satisfied. Such results clarify the landscape of possibilities and force us to recognize where fundamental choices must be made. In this work, we leverage the recently developed mereological approach to PID to establish novel inconsistency results with far-reaching implications. Our main theorem demonstrates that three cornerstone properties of classical information theory, namely non-negativity, the chain rule, and invariance under invertible transformations, become mutually incompatible when extended to the PID setting. This result reveals that any PID framework must sacrifice at least one property that seems fundamental to information theory itself. Additionally, we strengthen the classical result of Rauh et al., which showed that non-negativity, the identity property, and the Williams and Beer axioms cannot coexist.

</details>


### [5] [Secure Event-triggered MolecularvCommunication - Information Theoretic Perspective and Optimal Performance](https://arxiv.org/abs/2512.16761)
*Wafa Labidi,Vida Gholamian,Yaning Zhao,Christian Deppe,Holger Boche*

Main category: cs.IT

TL;DR: 本文把离散时间泊松信道(DTPC)放在识别(Identification)框架下研究，分别给出了随机化识别(RI)与安全随机化识别(SRI)的容量公式，证明了ID范式在能耗与硬件复杂度上的优势，并讨论了体内通信的安全性问题。


<details>
  <summary>Details</summary>
Motivation: 分子通信事件驱动、信息以分子释放强度传递，常用泊松模型描述；传统Shannon传输不适合作为事件驱动系统的性能指标，因此采用Ahlswede–Dueck提出的识别范式，同时考虑体内通信的安全威胁。

Method: 建立离散时间泊松信道模型，使用随机化编码分析识别容量。首先推导RI容量公式，再在存在窃听者的模型下分析SRI，给出安全识别容量的表达式与证明思路。

Result: 得到了DTPC下RI与SRI的容量公式，显示在随机化编码下识别码的规模随区块长度呈双指数增长，从理论上说明ID在能效与硬件要求上的潜在优势，并量化了安全约束对识别容量的影响。

Conclusion: 工作将识别理论引入分子通信并扩展到安全场景，填补了理论空白。后续可关注有限区块长度构造、具有记忆性的通道、实际调制/检测方案以及实现与生物兼容性评估等问题。

Abstract: Molecular Communication (MC) is an emerging field of research focused on understanding how cells in the human body communicate and exploring potential medical applications. In theoretical analysis, the goal is to investigate cellular communication mechanisms and develop nanomachine-assisted therapies to combat diseases. Since cells transmit information by releasing molecules at varying intensities, this process is commonly modeled using Poisson channels. In our study, we consider a discrete-time Poisson channel (DTPC). MC is often event-driven, making traditional Shannon communication an unsuitable performance metric. Instead, we adopt the identification framework introduced by Ahlswede and Dueck. In this approach, the receiver is only concerned with detecting whether a specific message of interest has been transmitted. Unlike Shannon transmission codes, the size of identification (ID) codes for a discrete memoryless channel (DMC) increases doubly exponentially with blocklength when using randomized encoding. This remarkable property makes the ID paradigm significantly more efficient than classical Shannon transmission in terms of energy consumption and hardware requirements. Another critical aspect of MC, influenced by the concept of the Internet of Bio-NanoThings, is security. In-body communication must be protected against potential eavesdroppers. To address this, we first analyze the DTPC for randomized identification (RI) and then extend our study to secure randomized identification (SRI). We derive capacity formulas for both RI and SRI, providing a comprehensive understanding of their performance and security implications.

</details>


### [6] [An Extension of Enumerative Sphere Shaping for Arbitrary Channel Input Distributions](https://arxiv.org/abs/2512.16808)
*Frederik Ritter,Andrej Rode,Laurent Schmalen*

Main category: cs.IT

TL;DR: Generalizes enumerative sphere shaping (ESS) to produce arbitrary discrete input distributions, enabling its use with non-Gaussian channels; shows gains vs CCDM in optical link simulations.


<details>
  <summary>Details</summary>
Motivation: ESS achieves low rate loss but only for fixed (Gaussian-like) input distributions; need a shaping method that can realize arbitrary discrete distributions to approach capacity on general channels.

Method: Replace ESS's fixed internal weights with distribution-dependent weights to shape sequences to any target discrete distribution; integrate generalized ESS with probabilistic amplitude shaping (PAS) and evaluate on a simplified unamplified coherent optical channel.

Result: Simulations for 256-symbol frames show generalized ESS increases maximum transmission rate by 0.0425 bit/symbol at FER < 1e-4 versus CCDM.

Conclusion: Generalized ESS extends ESS applicability to non-Gaussian channels and yields measurable performance gains over CCDM in the tested optical scenario.

Abstract: A non-uniform channel input distribution is key for achieving the capacity of arbitrary channels. However, message bits are generally assumed to follow a uniform distribution which must first be transformed to a non-uniform distribution by using a distribution matching algorithm. One such algorithm is enumerative sphere shaping (ESS). Compared to algorithms such as constant composition distribution matching (CCDM), ESS can utilize more channel input symbol sequences, allowing it to achieve a comparably low rate loss. However, the distribution of channel input symbols produced by ESS is fixed, restricting the utility of ESS to channels with Gaussian-like capacity-achieving input distributions. In this paper, we generalize ESS to produce arbitrary discrete channel input distributions, making it usable on most channels. Crucially, our generalization replaces fixed weights used internally by ESS with weights depending on the desired channel input distribution. We present numerical simulations using generalized ESS with probabilistic amplitude shaping (PAS) to transmit sequences of 256 symbols over a simplified model of an unamplified coherent optical link, a channel with a distinctly non-Gaussian capacity-achieving input distribution. In these simulations, we found that generalized ESS improves the maximum transmission rate by 0.0425 bit/symbol at a frame error rate below 10^{-4} compared to CCDM.

</details>


### [7] [Toward 6G Downlink NOMA: CRC-Aided GRAND for Noise-Resilient NOMA Decoding in Beyond-5G Networks](https://arxiv.org/abs/2512.16860)
*Emirhan Zor,Bora Bozkurt,Ferkan Yilmaz*

Main category: cs.IT

TL;DR: This paper proposes integrating CRC-aided GRAND with SIC in a two-user downlink power-domain NOMA system to reduce error propagation and remove separate FEC, improving BER under AWGN and Rayleigh channels.


<details>
  <summary>Details</summary>
Motivation: Conventional SIC in NOMA suffers from error propagation especially when power disparity between users is small; universal, noise-centric decoding like GRAND can potentially identify error patterns and aid SIC to improve reliability and reduce overhead.

Method: Design a CRC-aided GRAND decoding pipeline: treat CRC as both error detector and implicit code check, apply GRAND to decode weaker user's signal (no separate FEC), then perform SIC at strong user using GRAND-reinforced decoding to mitigate error propagation; evaluate under AWGN and Rayleigh fading with varying power allocations and distances.

Result: Simulations show substantial BER gains over existing NOMA decoding techniques across channel models and power allocations; GRAND-assisted SIC reduces error propagation and increases throughput while removing separate FEC.

Conclusion: Integrating universal decoding (CRC-aided GRAND) into NOMA is promising for interference-limited multiuser systems, offering improved robustness and reduced overhead; further work needed on complexity, latency, and broader multiuser scenarios.

Abstract: Non-Orthogonal Multiple Access (NOMA) technology has emerged as a promising technology to enable massive connectivity and enhanced spectral efficiency in next-generation wireless networks. In this study, we propose a novel two-user downlink power-domain NOMA framework that integrates a Cyclic Redundancy Check (CRC)-aided Guessing Random Additive Noise Decoding (GRAND) with successive interference cancellation (SIC). Unlike conventional SIC methods, which are susceptible to error propagation when there is low power disparity between users, the proposed scheme leverages GRAND's noise-centric strategy to systematically rank and test candidate error patterns until the correct codeword is identified. In this architecture, CRC is utilized not only to detect errors but also to aid the decoding process, effectively eliminating the need for separate Forward Error Correction (FEC) codes and reducing overall system overhead. Furthermore, the strong user enhances its decoding performance by applying SIC that is reinforced by GRAND-based decoding of the weaker user's signals, thereby minimizing error propagation and increasing throughput. Comprehensive simulation results over both Additive White Gaussian Noise (AWGN) and Rayleigh fading channels, under varying power allocations and user distances, show that the CRC-aided GRAND-NOMA approach significantly improves the Bit Error Rate (BER) performance compared to state-of-the-art NOMA decoding techniques. These findings underscore the potential of integrating universal decoding methods like GRAND into interference-limited multiuser environments for robust future wireless networks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [8] [Generative design of stabilizing controllers with diffusion models: the Youla approach](https://arxiv.org/abs/2512.15725)
*Matteo Cercola,Donatello Materassi,Simone Formentin*

Main category: eess.SY

TL;DR: Proposes a diffusion-model-based method to generate stabilizing linear controllers via Youla parameterization, trained on synthetic SISO plants to meet sensitivity and settling-time specs; claims first demonstration of diffusion-generated stabilizing controllers.


<details>
  <summary>Details</summary>
Motivation: Bridge gap between high-performance controller design and provable closed-loop stability by leveraging generative models that can produce controllers conditioned on plant and performance targets while guaranteeing stability by construction.

Method: Use Youla–Kučera parameterization to represent all stabilizing controllers; train a conditional diffusion generative model that maps plant dynamics and desired performance metrics to feasible Youla parameters of fixed order. At sampling time, generate Youla parameters that yield internally stable controllers satisfying user-specified targets.

Result: On synthetic stable SISO plants, the trained diffusion model reliably synthesizes controllers that meet prescribed sensitivity and settling-time specifications on unseen systems. Demonstrates that diffusion models can be used to generate stabilizing controllers with control-theoretic guarantees.

Conclusion: Combines rigorous stability guarantees (via Youla parameterization) with the expressiveness of diffusion generative models for controller synthesis; offers a novel data-driven yet provably stable approach, though evaluated on limited settings (synthetic SISO, fixed-order Youla).

Abstract: Designing controllers that simultaneously achieve strong performance and provable closed-loop stability remains a central challenge in control engineering. This work introduces a diffusion-based generative framework for linear controller synthesis grounded in the Youla-Kucera parameterization, enabling the construction of stabilizing controllers by design. The diffusion model learns a conditional mapping from plant dynamics and desired performance metrics to feasible Youla parameters, guaranteeing internal stability while flexibly accommodating user-specified targets. Trained on synthetically generated stable SISO plants with fixed-order Youla parameters, the proposed approach reliably synthesizes controllers that meet prescribed sensitivity and settling-time specifications on previously unseen systems. To the best of our knowledge, this work provides the first demonstration that diffusion models can generate stabilizing controllers, combining rigorous control-theoretic guarantees with the versatility of modern generative modeling.

</details>


### [9] [Run-to-Run Indirect Trajectory Tracking Control of Electromechanical Systems Based on Identifiable and Flat Models](https://arxiv.org/abs/2512.15734)
*Eloy Serrano-Seco,Edgar Ramirez-Laboreo,Eduardo Moya-Lasheras*

Main category: eess.SY

TL;DR: 提出一种在无法直接测量被控输出（如位置）情况下，仅利用可测信号（线圈电流）进行闭环位置跟踪的控制方案：基于可辨识模型的前馈/预测器与迭代参数更新环路相结合，在电机开关设备仿真中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 微分平坦模型常用于电机类系统的前馈控制，但控制性能强依赖模型精度；实际中位置传感器不可用或不可行时，需要一种只依赖可测量量（如电流）来实现反馈与补偿模型误差的方案。

Method: 构建可辨识的系统模型并设计基于该模型的控制器与预测器；通过比较一个可测量输出（线圈电流）与其预测值，采用迭代参数更新（在线辨识）来修正模型参数；控制器利用更新后的模型执行轨迹跟踪，整个流程在闭环中运行。

Result: 数值仿真（电磁开关类器件）表明，在仅测量线圈电流的条件下，系统能够有效跟踪期望位置轨迹；参数估计量随迭代收敛，预测误差和跟踪误差显著降低。

Conclusion: 该方法证明了在缺乏位置测量时，借助可辨识模型与基于可测量量的在线参数更新，可实现可靠的位置跟踪，减少对位置传感器的依赖；但限于仿真验证，需后续实验验证和对辨识性、收敛性条件的理论分析。

Abstract: Differentially flat models are frequently used to design feedforward controllers for electromechanical systems. However, control performance depends on model accuracy, which makes feedback imperative. This paper presents a control scheme for electromechanical systems in which measuring or estimating the output to be controlled -- typically the position -- is not feasible. It employs an identifiable-model-based controller and predictor, coupled with an iterative loop that updates model parameters using the error between a measurable output and its prediction. Simulations on electromechanical switching devices show effective tracking of the desired position trajectory using only coil current measurements.

</details>


### [10] [A Comprehensive Benchmark Platform for Process Control Research of Outdoor Microalgae Raceway Reactors](https://arxiv.org/abs/2512.15916)
*Enrique Rodríguez-Miranda,Pablo Otálora,José González-Hernández,José Luis Guzmán,Manuel Berenguel*

Main category: eess.SY

TL;DR: 该论文提出了一个用于室外微藻赛道反应器过程控制的基准框架，集成了pH、溶解氧、培养物体积及温度四项调节任务；基于高保真实验标定动力学模型和闭环仿真环境，提供约束、时延与多日扰动场景，并包含On/Off、PI/PID与经济型MPC基线控制器与统一性能指标，方便定量比较控制策略。


<details>
  <summary>Details</summary>
Motivation: 工业规模开放式赛道池的热、物化与生物过程强耦合且受日变化扰动，缺乏可重复、规范化的基准工具来评估多变量控制方法在真实约束与扰动下的性能，阻碍控制研究向户外藻类工程的转化。

Method: 构建一个经实验标定的高保真动态模型，模拟热传输、气体传质与生物生长；实现闭环仿真平台，包含执行器约束、气体传输时延和刚性积分器；定义四个可替换控制器接口（CO2注入、鼓泡、收获/稀释、换热器）；设计基于跟踪误差、气体/能耗与生物产量的统一全局性能指标与各子问题评价指标；提供On/Off、PI/PID与EMPC基线实现以示范用法。

Result: 生成了一个可重现、计算可行的基准平台，演示了不同调节方法在多日户外扰动下的性能差异；基线控制器给出参考性能，统一指标便于量化比较与多目标权衡。

Conclusion: 该基准有助于把先进控制方法（如多变量MPC、经济型与鲁棒/自适应控制）与户外藻类工程问题对接，推动控制算法在扰动丰富的环境系统中的应用与比较。

Abstract: This paper presents a benchmarking framework to evaluate process control strategies in outdoor microalgae raceway reactors, integrating four key control regulation tasks: pH, dissolved oxygen (DO), culture volume through coordinated harvest-dilution actions, and temperature via a sump-mounted spiral heat exchanger. The benchmark is built upon a high-fidelity, experimentally calibrated dynamic model that captures the strongly coupled thermal, physicochemical, and biological processes governing industrial-scale open raceway ponds. A closed-loop simulation environment is provided, featuring realistic actuator constraints, gas transport delays, stiff integration, and a fully specified scenario based on multi-day outdoor disturbances (irradiance, temperature, wind, and humidity). Four user-replaceable controllers define the manipulation of CO2 injection, air bubbling, harvest/dilution sequencing, and heat-exchanger operation. The platform computes a unified global performance index, in addition to individual metrics for each control problem, combining tracking error, gas and energy usage, and biomass productivity, enabling consistent and quantitative comparison of alternative control strategies. Baseline regulatory architectures (On/Off, PI/PID, and Economic Model Predictive Control (EMPC)) are included to illustrate the benchmark use for classical and advanced control methods. By providing an openly specified, reproducible, and computationally tractable benchmark with well-defined function interfaces, this work aims to bridge control methodology and outdoor algal bioprocess engineering, and to support the development of multivariable control strategies for disturbance-rich environmental systems.

</details>


### [11] [Lyapunov-based Adaptive Transformer (LyAT) for Control of Stochastic Nonlinear Systems](https://arxiv.org/abs/2512.15996)
*Saiedeh Akbari,Xuehui Shen,Wenqian Xue,Jordan C. Insinger,Warren E. Dixon*

Main category: eess.SY

TL;DR: 提出了一种基于Lyapunov的自适应Transformer（LyAT）控制器，用于随机非线性系统。该方法在无需离线预训练的情况下，实时估计系统漂移和扩散不确定性，并通过解析推导的自适应律保证跟踪与参数估计误差的概率一致的最终有界性。仿真/实验证明了四旋翼上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的控制方法多依赖离线训练和固定权重，导致在实际控制中缺乏实时自适应能力与稳定性保证；需要一种能与自适应控制理论结合、提供稳定性证明且无需离线预训练的Transformer型控制器。

Method: 设计连续时间LyAT控制器，利用Transformer结构在线估计随机动力学中的漂移与扩散不确定性；从Lyapunov稳定性分析出发，解析推导出权重的自适应更新律，使得权重可在运行时实时调整并嵌入控制律中；证明在该更新律下跟踪误差和参数估计误差以概率方式满足一致最终有界性。

Result: 解析性自适应律使得Transformer权重可实时更新，理论证明了基于Lyapunov的稳定性性质；在四旋翼实验中，LyAT展示了良好的跟踪性能和对随机不确定性的鲁棒性，验证了方法的有效性。

Conclusion: 将Transformer引入有解析稳定性保证的自适应控制框架是可行的。LyAT在无需离线预训练的情况下实现了实时自适应与概率性稳定性保证，为基于注意力的控制器在实际闭环系统中的应用提供了一条可行路径。

Abstract: This paper presents a novel Lyapunov-based Adaptive Transformer (LyAT) controller for stochastic nonlinear systems. While transformers have shown promise in various control applications due to sequential modeling through self-attention mechanisms, they have not been used within adaptive control architectures that provide stability guarantees. Existing transformer-based approaches for control rely on offline training with fixed weights, resulting in open-loop implementations that lack real-time adaptation capabilities and stability assurances. To address these limitations, a continuous LyAT controller is developed that adaptively estimates drift and diffusion uncertainties in stochastic dynamical systems without requiring offline pre-training. A key innovation is the analytically derived adaptation law constructed from a Lyapunov-based stability analysis, which enables real-time weight updates while guaranteeing probabilistic uniform ultimate boundedness of tracking and parameter estimation errors. Experimental validation on a quadrotor demonstrates the performance of the developed controller.

</details>


### [12] [Synchronization, Identification, and Signal Detection for Underwater Photon-Counting Communications With Input-Dependent Shot Noise](https://arxiv.org/abs/2512.16102)
*Fanghua Li,Xiaolin Zhou,Yongkang Chen,Wei Ni,Xin Wang,Dusit Niyato,Ekram Hossain*

Main category: eess.SY

TL;DR: 本文针对信号依赖泊松散粒噪声和异步多用户干扰的水下光子计数光无线通信系统，提出了基于新帧结构的同步算法与非线性迭代多用户检测方法，在复杂干扰和未知延迟条件下实现了接近理想同步情形的误码性能。


<details>
  <summary>Details</summary>
Motivation: 水下PhC OWC系统受到信号依赖泊松噪声与异步多用户干扰影响，传统检测与同步方法在复杂噪声与异步场景下性能下降且计算复杂度高，亟需低延迟、低复杂度且鲁棒的同步与多用户检测方案。

Method: 设计新的帧结构并提出分组先验的同步算法，实现先按用户组估计活动用户与传输延时，再在用户层面精细化估计以降低复杂度与延迟；提出基于用户检测窗的非线性迭代MUD，在每时隙识别干扰符号并逐时隙估计MUI，随后采用最大后验概率（MAP）对用户符号进行判决。

Result: 通过仿真验证，所提方案在误码率（BER）上可达到与已知传输延时且检测完美同步情形相当的性能，且在计算复杂度与延迟方面具备优势。

Conclusion: 提出的帧结构、分层同步与基于检测窗的迭代MUD能够有效应对信号依赖泊松噪声与异步MUI，显著提升水下PhC OWC系统的同步与检测性能，同时降低复杂度和时延。

Abstract: Photon counting (PhC) is an effective detection technology for underwater optical wireless communication (OWC) systems. The presence of signal-dependent Poisson shot noise and asynchronous multi-user interference (MUI) complicates the processing of received data signals, hindering the effective signal detection of PhC OWC systems. This paper proposes a novel iterative signal detection method in grant-free, multi-user, underwater PhC OWC systems with signal-dependent Poisson shot noise. We first introduce a new synchronization algorithm with a unique frame structure design.The algorithm performs active user identification and transmission delay estimation. Specifically, the estimation is performed first on a user group basis and then at the individual user level with reduced complexity and latency.We also develop a nonlinear iterative multi-user detection (MUD) algorithm that utilizes a detection window for each user to identify interfering symbols and estimate MUI on a slot-by-slot basis, followed by maximum \textit{a-posteriori} probability detection of user signals.Simulations demonstrate that our scheme achieves bit error rates comparable to scenarios with transmission delays known and signal detection perfectly synchronized.

</details>


### [13] [Black-Start Power Capacity Sizing and Control Strategy for an Islanded DFIG Wind-to-Hydrogen System](https://arxiv.org/abs/2512.16263)
*Bosen Yang,Kang Ma,Jin Lin,Yonghua Song*

Main category: eess.SY

TL;DR: 提出一种用于离网风氢系统的黑启动方法，通过对安装在制氢厂内的质子交换膜燃料电池(PEMFC)进行额定容量计算，并设计风-氢协同与氢储能协同控制，在保持DFIG风电机组常规并网型运行的前提下，实现可靠的黑启动与风功率波动下的频压稳定。


<details>
  <summary>Details</summary>
Motivation: 离网W2H系统在无电网支撑时需具备自举能力，需为风机励磁、无源元件充电及电解槽等辅件供电，合理确定PEMFC容量并设计协调控制策略以保证黑启动与运行稳定性。

Method: 基于微电网拓扑与黑启动流程，通过潮流分析确定PEMFC额定容量；提出风-氢协同控制(WHCC)和氢储能协同控制(HSCC)，在风机MPPT下由电解槽吸纳波动，并采用固定频率控制使DFIG保持网随特性以降低换流器开发成本；为两种控制模式建立黑启动序列并在MATLAB/Simulink中仿真验证。

Result: 仿真结果表明：计算得到的PEMFC容量能够满足自举期间对无源元件、励磁及辅助负载的需求；所提出的黑启动控制策略能实现平稳自启动；协调控制在风功率波动下维持了电压与频率稳定，验证了方案的实用性与鲁棒性。

Conclusion: 该研究给出了一种可实施的离网风氢系统黑启动与运行协调控制框架，但需在更高保真度的器件模型、故障情形、经济性分析与实体试验上进一步验证与优化。

Abstract: This paper proposes a black-start method for an off-grid wind-to-hydrogen (W2H) system comprising a wind farm based on Doubly-Fed Induction Generators (DFIGs), proton exchange membrane fuel cells (PEMFCs) serving as the black-start power source, and a hydrogen production industry. The PEMFC is installed within the hydrogen industry to facilitate direct access to hydrogen fuel. Based on the microgrid topology and black-start scheme, this study innovatively sizes the rated capacity of the PEMFC through power flow analysis. The capacity must be sufficient to charge passive components such as transmission lines and transformers, provide rotor excitation, and supply wind turbine (WT) and electrolyzer (ELZ) auxiliaries during startup. The proposed system integrates wind-hydrogen coordinated control (WHCC) and hydrogen-storage coordinated control (HSCC). Under maximum power point tracking (MPPT) of the WTs, the ELZ follows power fluctuations to absorb wind output, ensuring stable voltage and frequency. Fixed-frequency control applied to either the DFIG or PEMFC converters enables DFIGs to retain conventional grid-following (GFL) operation, reducing converter development costs. For both control modes, this paper establishes the black-start sequence and formulates a comprehensive coordinated control strategy for the entire system. The entire control system is validated through simulations in MATLAB/Simulink. Results confirm that the calculated PEMFC capacity supports reliable black-start, while the black-start control strategy ensures smooth system self-startup. Furthermore, the coordinated control strategy maintains stable frequency and voltage under fluctuating wind power, demonstrating the practicality and robustness of the proposed approach.

</details>


### [14] [Closed Loop Reference Optimization for Extrusion Additive Manufacturing](https://arxiv.org/abs/2512.16333)
*Rawan Hoteit,Andrea Balestra,Nathan Mingard,Efe C. Balta,John Lygeros*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Various defects occur during material extrusion additive manufacturing processes that degrade the quality of the 3D printed parts and lead to significant material waste. This motivates feedback control of the extrusion process to mitigate defects and prevent print failure. We propose a linear quadratic regulator (LQR) for closed-loop control with force feedback to provide accurate width tracking of the extruded filament. Furthermore, we propose preemptive optimization of the reference force given to the LQR that accounts for the performance of the LQR and generates the optimal reference for the closed loop extrusion dynamics and machine constraints. Simulation results demonstrate the improved tracking performance and response time. Experiments on a Fused Filament Fabrication 3D printer showcase a root mean square error improvement of 39.57% compared to tracking the unmodified reference as well as an 83.7% shorter settling time.

</details>


### [15] [Using Seminorms To Analyze Contraction of Switched Systems With Only Non-Contracting Modes](https://arxiv.org/abs/2512.16338)
*Edwin Baum,Zonglin Liu,Yuzhen Qin,Olaf Stursberg*

Main category: eess.SY

TL;DR: 研究在所有子系统都非收缩时，如何通过子空间上的半范数与半收缩性质，保证切换系统整体收缩性；给出基于子空间族与模式激活时间的充分条件，并用数值例子验证。


<details>
  <summary>Details</summary>
Motivation: 现有切换系统收缩性结果通常要求至少有一个模式为收缩，然而许多实际系统的单模式均非收缩但在某些不变或显著子空间内可能表现为收缩。该工作旨在放宽收缩模式的要求，利用子空间上的局部收缩性质推导整体收缩性条件。

Method: 引入半范数与半收缩系统概念来刻画子空间内的收缩行为；选择能分离状态空间的子空间族，使得对应的半范数构成分离族；对每个模式在每个子空间上检验是否收缩，并利用模式在各子空间上的累积激活时间给出整体收缩的充分条件。

Result: 证明了在合适的子空间分解与半范数选择下，即便所有模式均非全局收缩，通过约束各模式在不同子空间上的激活时间，也可保证系统在整个状态空间的收缩性。提供了若干数值例子以说明理论条件的可行性与保守性。

Conclusion: 本文扩展了切换系统收缩性分析的适用范围，为没有全局收缩模式的系统提供了基于子空间半收缩性质的整体收缩保证，并指出了半范数选择与子空间覆盖对结果保守性的影响。

Abstract: This paper investigates contraction properties of switched dynamical systems for the case that all modes are non-contracting, thereby extending existing results that require at least one mode to be contracting. Leveraging the property that unstable systems may still exhibit stable behavior within certain subspaces, conditions are provided which ensure contracting evolution within a given subspace of the state space of the switched system. These conditions are derived using the concepts of seminorms and semi-contracting systems. Then, by selecting a set of subspaces whose corresponding seminorms form a separating family of the state space, and by verifying whether a given mode is contracting in each subspace, conditions on the activation time of each mode are provided by which contraction on the complete state space is guaranteed. Numerical examples are presented for illustration.

</details>


### [16] [Contraction Analysis of Filippov Solutions in Multi-Modal Piecewise Smooth Systems](https://arxiv.org/abs/2512.16345)
*Zonglin Liu,Kyra Borchhardt,Olaf Stursberg*

Main category: eess.SY

TL;DR: 本文将双模PWS系统的收缩分析扩展到多模情形，提出对各模式流和切换流的收缩条件，并分别处理Rn中互不相交的多切换流形和R2中两条相交流形的情形，辅以数值验证。


<details>
  <summary>Details</summary>
Motivation: 已有针对双模PWS系统的收缩性判据可保证模式内流与切换流的收缩，从而推导解的渐近性质。作者希望把该思想推广到更多模式和更复杂切换结构，以分析更广泛PWS系统的渐近行为。

Method: 基于Filippov解的框架，给出每一模式及在切换流形上的滑动动力学满足收缩映射的充分条件。先处理Rn中多条互不相交的切换流形的构造与判据，再具体研究R2中两条相交流形时的特殊处理与条件，并讨论向Rn中一般相交情况的推广。

Result: 得到了一组可检验的充分条件，保证相应PWS系统的Filippov解具有收缩性，从而推导出收敛性结论。通过数值例子验证了理论条件的有效性，并对更一般相交情况给出讨论性结论。

Conclusion: 工作把双模收缩分析的原则成功推广到更复杂的PWS结构，为分析多模系统的渐近行为提供了可操作的条件，但对高维相交流形的全面处理仍需进一步研究。

Abstract: This paper provides conditions to ensure contractive behavior of Filippov solutions generated by multi-modal piecewise smooth (PWS) systems. These conditions are instrumental in analyzing the asymptotic behavior of PWS systems, such as convergence towards an equilibrium point or a limit cycle. The work is motivated by a known principle for contraction analysis of bimodal PWS systems which ensures that the flow dynamics of each mode and the sliding dynamics on the switching manifold are contracting. This approach is extended first to PWS systems with multiple non-intersecting switching manifolds in Rn, and then to two intersecting switching manifolds in R2. Numerical examples are provided to validate the theoretical findings, along with a discussion on extensions to more general intersecting switching manifolds in Rn.

</details>


### [17] [Optimal chiller loading including transients](https://arxiv.org/abs/2512.16356)
*Manuel R. Arahal,Manuel G. Satué,Manuel G. Ortega*

Main category: eess.SY

TL;DR: 提出一个在多冷机组厂中进行负荷调度和载荷分配的新框架，扩展决策变量集合并考虑冷机启停瞬态动态，用同时扰动随机逼近(SPSA)求解，较静态方法在能耗上表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统最优冷机负荷/排列研究多限于静态模型，忽略冷机启停产生的瞬态效应，且决策变量较少限制了优化空间。本工作旨在通过增加独立决策变量并显式考虑瞬态动态，提升冷却厂运行的能效。

Method: 引入扩展的决策变量集以提供额外自由度；以可管理的方式对启停瞬态进行建模使问题仍可求解；采用同时扰动随机逼近(SPSA)算法对能耗最优化问题进行求解，并与一个类似的静态方法进行对比实验。

Result: 包含瞬态动态和更多决策变量的优化框架在仿真对比中比静态方法能进一步降低能耗，证明考虑启停瞬态和扩展决策空间带来实际节能优势。

Conclusion: 在多冷机组优化中，引入更多独立变量并考虑启停瞬态是可行且有益的。使用SPSA可在可接受的复杂度下求解该动态问题，显示出比传统静态优化更低的能耗，但实际应用仍需权衡复杂度、收敛性与设备磨损等因素。

Abstract: Scheduling and loading of chillers in a multi-chiller plant is considered. A new framework is introduced considering an extended set of independent variables for the optimization problem of energy consumption. In this way the number of decision variables is increased, providing extra degrees of freedom to optimize cooling plant operation. The dynamic effects due to transients arising from switching on and off of units are usually not considered in the literature dealing with Optimal Chiller Loading/Sequencing which is restricted to the static case. In this paper, these effects are treated in a way that results in a manageable optimization problem. A Simultaneous Perturbation Stochastic Approximation solution is deployed for the problem and the proposed method is compared with a similar but static approach showing the benefits in terms of reduced energy consumption.

</details>


### [18] [Economic versus energetic model predictive control of a cold production plant with thermal energy storage](https://arxiv.org/abs/2512.16379)
*Manuel G. Satué,Manuel R. Arahal,Luis F. Acedo,Manuel G. Ortega*

Main category: eess.SY

TL;DR: 本文首次比较了面向能耗（energetic）和面向经济（economic）的模型预测控制（MPC）目标在多机组水冷机组与蓄冷系统组合的冷却厂中的性能差异。研究使用Simscape建模并采用非凸混合优化求解，结果显示在高电价季节，经济目标相比能耗目标能降低约2.94%的电费，但会增加约2.15%的能耗。


<details>
  <summary>Details</summary>
Motivation: 当前文献多以电费最小化（economic MPC）作为控制目标，忽视了直接以能耗最小化为目标的比较。作者旨在评估两种目标在实际工程系统中的差异，以指导冷却厂运行决策。

Method: 建立包含风冷水冷机组与蓄冷系统的物理模型并集成至Simscape；分别以能耗最小化和电费最小化为目标，使用非凸混合（mixed）优化算法求解MPC，验证在多种情景和不同季节（电价季节性）下的运行结果。

Result: 在多场景、不同季节实验中，能耗目标在总能耗上优于经济目标；但在高电价季节、按代表性电价计，经济目标能将运行成本降低约2.94%，而能耗增加约2.15%。结果受电价结构和季节影响显著。

Conclusion: 尽管经济目标能在部分季节降低费用，但能耗目标在整体能耗效率上更优。选择哪种目标应依据电价结构与运营侧重（节能或节费）来决定；作者建议在实际应用中重视能耗优化。

Abstract: Economic model predictive control has been proposed as a means for solving the unit loading and unit allocation problem in multi-chiller cooling plants. The adjective economic stems from the use of financial cost due to electricity consumption in a time horizon, such is the loss function minimized at each sampling period. The energetic approach is rarely encountered. This article presents for the first time a comparison between the energetic optimization objective and the economic one. The comparison is made on a cooling plant using air-cooled water chillers and a cold storage system. Models developed have been integrated into Simscape, and non-convex mixed optimization methods used to achieve optimal control trajectories for both energetic and economic goals considered separately. The results over several scenarios, and in different seasons, support the consideration of the energetic approach despite the current prevalence of the economic one. The results are dependent on the electric season and the available tariffs. In particular, for the high electric season and considering a representative tariff, the results show that an increment of about 2.15% in energy consumption takes place when using the economic approach instead of the energetic one. On the other hand, a reduction in cost of 2.94% is achieved.

</details>


### [19] [From Liability to Asset: A Three-Mode Grid-Forming Control Framework for Centralized Data Center UPS Systems](https://arxiv.org/abs/2512.16497)
*Mohamed Shamseldein*

Main category: eess.SY

TL;DR: 该论文提出用于大型数据中心的集中式中压UPS控制架构，包含三种工作模式以在正常运行、故障限流和频率响应间切换，仿真表明在弱电网条件下能降低逆变器电流冲击、避免IT能量中断并改善PCC电压响应，同时缓冲脉动负载。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载使数据中心成为高度动态的电力电子负载，故障期间的行为和工负载脉动会对弱网接入点造成应力。需要一种控制策略以在保护电网的同时保证IT负载连续性和减小对电网的冲击。

Method: 提出集中式MV-UPS控制架构及三种模式：Mode1——稳固直流总线并在正常运行下整形对电网的拉动；Mode2——故障时执行电流限制下的P-Q优先，利用UPS-BESS缓冲并进行速率限制的软返回；Mode3（可选）——基于下垂的快速频率响应。采用基频平均化dq仿真，包含50MW模块、SCR=1.5及150ms三相0.5 p.u.短路情形，并考察1Hz脉冲负载工况。

Result: 与基线(momentary-cessation和SRF-PLL LVRT)相比，提出方法在故障工况下实现零未供电IT能量、显著降低峰值逆变器电流（0.57 vs 1.02 p.u.）、在故障窗口保持非零均值网侧拉动（0.20 p.u.）并使PCC电压在一周期后更高（0.79 vs 0.66 p.u.）。在脉动负载案例中，正常运行下的整形滤除了对电网的振荡，而UPS-BESS缓冲了脉动成分。

Conclusion: 该控制架构能在弱网接入下同时保护电网稳定性和保证IT负载连续性，通过分模式设计与UPS-BESS协同，实现故障限流、平滑网侧功率以及可选频率响应，仿真结果验证了其有效性。

Abstract: AI workloads are turning large data centers into highly dynamic power-electronic loads; fault-time behavior and workload pulsing can stress weak-grid points of interconnection. This paper proposes a centralized medium-voltage (MV) uninterruptible power supply (UPS) control architecture implemented as three operating modes: Mode 1 regulates a DC stiff bus and shapes normal-operation grid draw, Mode 2 enforces current-limited fault-mode P--Q priority with UPS battery energy storage system (UPS-BESS) buffering and a rate-limited post-fault "soft return," and Mode 3 optionally provides droop-based fast frequency response via grid-draw modulation. Fundamental-frequency averaged dq simulations (50 MW block, short-circuit ratio (SCR) = 1.5, 0.5 p.u. three-phase dip for 150~ms) show zero unserved information-technology (IT) energy (0.00000 MWh vs.0.00208 MWh for a momentary-cessation benchmark), a 0.57 p.u. peak inverter current (vs. 1.02 p.u. for a synchronous-reference-frame phase-locked loop (SRF-PLL) low-voltage ride-through (LVRT) baseline), a nonzero mean fault-window grid draw of 0.20~p.u. (vs.approx 0 for momentary cessation), and an improved settled point-of-common-coupling (PCC) voltage minimum of 0.79 p.u. after one cycle (vs. 0.66 p.u.). A forced-oscillation case study applies a 1 Hz pulsed load (+/- 0.25 p.u.) and shows that the normal-operation shaping filters the oscillation seen by the grid while the UPS-BESS buffers the pulsing component.

</details>


### [20] [Observer-based Differentially Private Consensus for Linear Multi-agent Systems](https://arxiv.org/abs/2512.16736)
*Xiaofeng Zong,Ming-Yu Wang,Jimin Wang,Ji-Feng Zhang*

Main category: eess.SY

TL;DR: 研究在输出反馈协定协议下，线性多智能体系统引入（衰减）拉普拉斯噪声以保护输出隐私，建立了均方和几乎必然收敛的充分条件，证明分离原理仍成立，给出收敛速率并提出估计器/控制/噪声的联合设计以保证ε-差分隐私，包含全阶与降阶观测器，仿真验证理论结果。


<details>
  <summary>Details</summary>
Motivation: 在多智能体协同中，智能体的输出信息可能包含私有数据，直接交换会带来隐私泄露风险。需在保证协同（共识）性能的同时，保护每个智能体在每个时刻的输出信息不被暴露。

Method: 对交换的输出信息叠加拉普拉斯噪声（且噪声强度随时间衰减）；基于观测器的输出反馈协议，采用回溯法和非负几乎超鞅的收敛理论推导均方与几乎必然收敛条件；证明对具有衰减噪声的线性MAS，分离原理依然适用；给出收敛速率；建立状态估计增益、反馈增益和噪声强度的联合设计框架以满足ε-差分隐私；分别讨论全阶和降阶观测器情形并给出充分条件。

Result: 给出了使观测器驱动的线性MAS在加噪的通信下达成均方与几乎必然共识的充分条件和收敛速率；证明在每个时刻均可保持ε-差分隐私；提出联合设计方法能在满足隐私级别的同时确保共识（并可达到指定的ε*）；仿真实例支持理论分析。

Conclusion: 本文在理论上把差分隐私机制（衰减拉普拉斯噪声）与线性多智能体的观测器/控制设计结合，既保证了收敛性与收敛速率，又能在每个时刻保护输出隐私，提供了可行的联合设计框架和充分条件，具有较好的理论与数值验证。

Abstract: This paper investigates the differentially private consensus problem for general linear multi-agent systems (MASs) based on output feedback protocols. To protect the output information, which is considered private data and may be at high risk of exposure, Laplace noise is added to the information exchange. The conditions for achieving mean square and almost sure consensus in observer-based MASs are established using the backstepping method and the convergence theory for nonnegative almost supermartingales. It is shown that the separation principle remains valid for the consensus problem of linear MASs with decaying Laplace noise. Furthermore, the convergence rate is provided. Then, a joint design framework is developed for state estimation gain, feedback control gain, and noise to ensure the preservation of ε-differential privacy. The output information of each agent is shown to be protected at every time step. Finally, sufficient conditions are established for simultaneously achieving consensus and preserving differential privacy for linear MASs utilizing both full-order and reduced-order observers. Meanwhile, an ε*-differentially private consensus is achieved to meet the desired privacy level. Two simulation examples are provided to validate the theoretical results.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [21] [A Survey on Reconfigurable Intelligent Surfaces in Practical Systems: Security and Privacy Perspectives](https://arxiv.org/abs/2512.15754)
*Ziyu Chen,Yitong Shen,Jingzhe Zhang,Yao Zheng,Yili Ren,Xuyu Wang,Shiwen Mao,Hanqing Guo*

Main category: cs.CR

TL;DR: 本文综述了可重构智能表面（RIS）在实际智能环境中的应用，重点分析了从安全和隐私角度出现的威胁、脆弱性与防御策略，并汇总了相关开源资源。


<details>
  <summary>Details</summary>
Motivation: 尽管理论研究大量证明了RIS在通信与感知上的潜力，但在家庭、车载和工业等实际场景中的部署、以及由此带来的安全与隐私问题缺乏系统性研究和实证资源，因而需要一篇面向实践的安全隐私综述来引导研究与工程实现。

Method: 对已有文献进行系统性梳理：构建两类系统（有/无合法RIS）与两类攻击者（有/无恶意RIS）的情景分类，列举RIS带来的新型攻击（窃听、干扰、欺骗等），并总结相应防御（额外安全算法、干扰攻击者、早期检测、合法RIS防御等）；同时整理开源工具、数据集与示例链接以支持后续研究。

Result: 揭示RIS在实际系统中可被滥用以实现物理层与感知层的攻击，现有防御方法覆盖算法层、物理干预和检测三大方向，但多数方案仍缺乏现实部署验证与统一评估基准；提供的开源集合为研究复现与比较提供基础。

Conclusion: 本文为RIS在实际环境中的安全与隐私问题提供了全面视角，指出关键研究缺口并通过资源汇总促进未来工作，旨在推动更安全、鲁棒与隐私保护的RIS工程实践。

Abstract: Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative technology capable of reshaping wireless environments through dynamic manipulation of electromagnetic waves. While extensive research has explored their theoretical benefits for communication and sensing, practical deployments in smart environments such as homes, vehicles, and industrial settings remain limited and under-examined, particularly from security and privacy perspectives. This survey provides a comprehensive examination of RIS applications in real-world systems, with a focus on the security and privacy threats, vulnerabilities, and defensive strategies relevant to practical use. We analyze scenarios with two types of systems (with and without legitimate RIS) and two types of attackers (with and without malicious RIS), and demonstrate how RIS may introduce new attacks to practical systems, including eavesdropping, jamming, and spoofing attacks. In response, we review defenses against RIS-related attacks in these systems, such as applying additional security algorithms, disrupting attackers, and early detection of unauthorized RIS. We also discuss scenarios in which the legitimate user applies an additional RIS to defend against attacks. To support future research, we also provide a collection of open-source tools, datasets, demos, and papers at: https://awesome-ris-security.github.io/. By highlighting RIS's functionality and its security/privacy challenges and opportunities, this survey aims to guide researchers and engineers toward the development of secure, resilient, and privacy-preserving RIS-enabled practical wireless systems and environments.

</details>


### [22] [Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?](https://arxiv.org/abs/2512.15769)
*Junchi Lu,Xinke Li,Yuheng Liu,Qi Alfred Chen*

Main category: cs.CR

TL;DR: 本文发现开源扩散模型可能作为“数据链后门”（DCB）的隐蔽载体：它们会记忆并在生成样本中再现后门触发器，导致下游模型继承后门。尤其在干净标签攻击下该风险隐蔽且有效；并揭示了早期阶段触发器显现（ESTM）现象，触发模式在逆扩散的高噪声早期更明显。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型被广泛用于合成数据增强以降低标注成本，新的数据源链带来了尚未充分研究的安全风险——开源生成模型可能把恶意后门嵌入到合成数据中，从而传递给依赖这些数据训练的下游感知模型。本文旨在识别并分析这一“数据链后门”威胁路径及其特性。

Method: 基于实验验证与生成过程分析：对开源扩散模型进行后门注入/测评，观察其在生成样本中再现触发器的能力；在干净标签设置下训练下游模型以评估继承效果；分析逆扩散生成流程各阶段样本以定位触发器显现时机，发现并命名早期阶段触发器显现（ESTM）现象。

Result: 实验证实开源扩散模型会记忆并在生成样本中重现后门触发器，导致下游模型被感染且攻击在干净标签场景下仍然高效且对合成数据实用性影响微弱。进一步发现触发模式在逆扩散的早期高噪声阶段更为显著（ESTM），之后被逐步掩饰进最终样本。

Conclusion: 揭示并定义了生成式数据供应链中的新型安全威胁（DCB）及其早期显现特性（ESTM）。呼吁关注合成数据来源的可信性与审计，提出初步缓解方向（如模型/样本审计、早期阶段检测、数据溯源与鲁棒训练等），并指出需进一步研究更有效的检测与防御机制。

Abstract: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.

</details>


### [23] [Variable Record Table: A Unified Hardware-Assisted Framework for Runtime Security](https://arxiv.org/abs/2512.15777)
*Suraj Kumar Sah,Love Kumar Sah*

Main category: cs.CR

TL;DR: 提出了一种名为可变记录表（VRT）的硬件辅助统一框架，在单一硬件结构中同时实现空间内存安全（防止缓冲区溢出）、后边缘控制流完整性（back-edge CFI）以及检测投机执行攻击，作者宣称在MiBench和SPEC基准上检测到所有测试攻击变体且无需额外指令开销，并将面积/功耗开销控制在可接受范围内。


<details>
  <summary>Details</summary>
Motivation: 当前系统面临多类攻击（内存污染、控制流劫持、投机执行漏洞），现有方案通常各自解决单一威胁，引入性能或实现成本，并留下安全盲区。论文目标是用单一硬件结构同时覆盖三类威胁，减少运行时开销与硬件资源占用。

Method: 在运行时对指令进行插装以提取内存地址、边界元数据和控制流签名，动态构造一个保护表（VRT），由硬件辅助检测越界访问、后边缘控制流违规与投机执行相关异常。VRT 保持固定条目数（示例为512条），配合硬件逻辑进行检测与响应。

Result: 在MiBench和SPEC基准上，作者报告对所测试的所有攻击变体均被检测到，且“零额外指令开销”。资源占用方面，VRT在512条目配置下内存需求低于25KB，面积/功耗开销分别低于8%和11.65 μW（文中声明）。

Conclusion: 将三类安全机制整合到单一硬件结构可以提供较全面的保护并将性能影响最小化，VRT被提出为一种轻量级、低资源占用的实用方案。

Abstract: Modern computing systems face security threats, including memory corruption attacks, speculative execution vul- nerabilities, and control-flow hijacking. Although existing solu- tions address these threats individually, they frequently introduce performance overhead and leave security gaps. This paper presents a Variable Record Table (VRT) with a unified hardware- assisted framework that simultaneously enforces spatial memory safety against buffer overflows, back-edge control-flow integrity (CFI), and speculative execution attack detection. The VRT dynamically constructs a protection table by instrumenting run- time instructions to extract memory addresses, bounds metadata, and control-flow signatures. Our evaluation across MiBench and SPEC benchmarks shows that VRT successfully detects all attack variants tested with zero additional instruction overhead. Fur- thermore, it maintains memory requirements below 25KB (for 512 entries) and maintains area / power overhead under 8% and 11.65 μW, respectively. By consolidating three essential security mechanisms into a single hardware structure, VRT provides comprehensive protection while minimizing performance impact.

</details>


### [24] [Detecting Malicious Entra OAuth Apps with LLM-Based Permission Risk Scoring](https://arxiv.org/abs/2512.15781)
*Ashim Mahara*

Main category: cs.CR

TL;DR: 提出一个统一检测框架：构建完整的 Microsoft Graph 权限语料库，使用 LLM 生成一致的权限风险评分，并将评分嵌入实时检测引擎以识别恶意 OAuth 同意活动。


<details>
  <summary>Details</summary>
Motivation: 恶意应用通过诱导用户授权 OAuth 权限来获取公司数据与能力；现有方法缺乏对 Microsoft Graph 权限的全面语义理解及统一的风险量化，且难以在实时场景中稳定识别异常同意行为。

Method: （1）收集并规范化 Microsoft Graph 的全部权限，建立结构化语料库；（2）基于大语言模型对每个权限生成一致的风险评分（设计提示、评分尺度与校准流程）；（3）将权限与评分映射到实时检测引擎，对 OAuth 同意事件进行特征化、评分聚合与阈值/规则判断以触发告警。

Result: 框架能为每个权限提供可解释且一致的风险评分，并在实时引擎中基于这些评分有效识别出异常或恶意的 OAuth 同意活动，降低漏报同时控制误报率（文章应包含定量评估指标与具体检测示例）。

Conclusion: 通过将权权限语义化与 LLM 风险量化相结合，能显著增强对恶意 OAuth 同意的检测能力；方案具有可扩展性，可用于持续更新权限库和迁移到其他 API 生态，后续可改进评分校准与模型可解释性。

Abstract: This project presents a unified detection framework that constructs a complete corpus of Microsoft Graph permissions, generates consistent LLM-based risk scores, and integrates them into a real-time detection engine to identify malicious OAuth consent activity.

</details>


### [25] [Auto-Tuning Safety Guardrails for Black-Box Large Language Models](https://arxiv.org/abs/2512.15782)
*Perry Abdulkadir*

Main category: cs.CR

TL;DR: 将安全护栏视为在冻结的大模型上需要调优的超参数。作者在Mistral-7B-Instruct外部组合模块化的系统提示和基于ModernBERT的有害性分类器，在三套公开基准上对配置进行评估：恶意样例生成、绕过提示和良性查询。以48点网格搜索建立基线，再用黑盒Optuna搜索，结果表明Optuna能以少得多的评估次数和约8倍更短的实际时间重现最佳网格配置，表明该思路在计算和时间受限的黑盒部署中可行。


<details>
  <summary>Details</summary>
Motivation: 实际产品中经常无法改动模型权重，安全护栏常由人工手动调参，易碎且难复现。作者提出将护栏设计作为超参数优化问题，以提高可复现性和效率。

Method: 在冻结的Mistral-7B-Instruct外层，组合模块化系统提示（包括用于测试的绕过和恶意诱导提示）与ModernBERT有害性分类器，定义评价指标（恶意/绕过成功率、对良性请求的有害响应率、端到端延迟）。先做48点网格搜索建立基线，再用Optuna做黑盒优化，比较评估次数和耗时。

Result: Optuna在相同搜索空间中能稳定重现网格搜索的最佳配置，但只需约十分之一的评估次数和约8倍更少的墙钟时间，证明黑盒超参搜索在此问题上高效且实用。

Conclusion: 将安全护栏视为可调的超参数是一个切实可行的策略，尤其适用于无法修改模型权重的黑盒部署场景，有助于在有限的计算和时间预算下提升安全性和可复现性。

Abstract: Large language models (LLMs) are increasingly deployed behind safety guardrails such as system prompts and content filters, especially in settings where product teams cannot modify model weights. In practice these guardrails are typically hand-tuned, brittle, and difficult to reproduce. This paper studies a simple but practical alternative: treat safety guardrail design itself as a hyperparameter optimization problem over a frozen base model. Concretely, I wrap Mistral-7B-Instruct with modular jailbreak and malware system prompts plus a ModernBERT-based harmfulness classifier, then evaluate candidate configurations on three public benchmarks covering malware generation, classic jailbreak prompts, and benign user queries. Each configuration is scored using malware and jailbreak attack success rate, benign harmful-response rate, and end-to-end latency. A 48-point grid search over prompt combinations and filter modes establishes a baseline. I then run a black-box Optuna study over the same space and show that it reliably rediscovers the best grid configurations while requiring an order of magnitude fewer evaluations and roughly 8x less wall-clock time. The results suggest that viewing safety guardrails as tunable hyperparameters is a feasible way to harden black-box LLM deployments under compute and time constraints.

</details>


### [26] [Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India](https://arxiv.org/abs/2512.15799)
*Sahibpreet Singh,Shikha Dhiman*

Main category: cs.CR

TL;DR: 研究审视生成式AI对印度刑事司法中计算取证完整性的影响，指出《数字个人数据保护法，2023》在应对对抗性AI（反取证、深度伪造）方面存在空白，提出以可解释AI为核心的人本取证模型以协调隐私与取证需求。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在提升证据提取效率的同时带来对抗性威胁，需评估现行印度隐私法与国际伦理标准对这些风险的适配性。

Method: 采用学理法方法，结合《DPDP法》条文分析与IEEE、EU等全球伦理框架进行比较评估。

Result: 机器学习在模式识别上表现出高准确率，但存在数据投毒与算法偏见等脆弱性；法案中的数据最小化原则与取证保留需求存在冲突，且法律定义未充分涵盖AI驱动的工具型与目标型犯罪。

Conclusion: 建议通过将可解释AI纳入取证流程、人本优先的监管框架以及与国际取证标准的同步，来修订法律与技术标准以缓解合成媒体风险，并为立法与标准化提供路线图。

Abstract: The integration of generative Artificial Intelligence into the digital ecosystem necessitates a critical re-evaluation of Indian criminal jurisprudence regarding computational forensics integrity. While algorithmic efficiency enhances evidence extraction, a research gap exists regarding the Digital Personal Data Protection Act, 2023's compatibility with adversarial AI threats, specifically anti-forensics and deepfakes. This study scrutinizes the AI "dual-use" dilemma, functioning as both a cyber-threat vector and forensic automation mechanism, to delineate privacy boundaries in high-stakes investigations. Employing a doctrinal legal methodology, the research synthesizes statutory analysis of the DPDP Act with global ethical frameworks (IEEE, EU) to evaluate regulatory efficacy. Preliminary results indicate that while Machine Learning offers high accuracy in pattern recognition, it introduces vulnerabilities regarding data poisoning and algorithmic bias. Findings highlight a critical tension between the Act's data minimization principles and forensic data retention requirements. Furthermore, the paper identifies that existing legal definitions inadequately encompass AI-driven "tool crimes" and "target crimes." Consequently, the research proposes a "human-centric" forensic model prioritizing explainable AI (XAI) to ensure evidence admissibility. These implications suggest that synchronizing Indian privacy statutes with international forensic standards is imperative to mitigate synthetic media risks, establishing a roadmap for future legislative amendments and technical standardization.

</details>


### [27] [An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative](https://arxiv.org/abs/2512.15803)
*Apurva Shet,Izzat Alsmadi*

Main category: cs.CR

TL;DR: 分析了2024年1–4月ZDI披露的415个零日漏洞，研究目的为识别披露趋势、供应商间严重度分布、以及预测高严重度漏洞的特征，比较传统机器学习与深度学习在基于结构化元数据与文本描述的严重度分类上的表现。


<details>
  <summary>Details</summary>
Motivation: 零日漏洞在补丁发布前造成高度暴露，企业需要通过优先级策略和自动化手段快速识别高危漏洞以降低风险；因此需要系统性分析披露特征并评估自动化严重度预测方法的可行性。

Method: 构建包含CVE、CVSSv3得分、发布日期与文本描述的数据集；进行描述性统计（时间、供应商、漏洞类型、CVSS分布）；提取结构化特征（供应商、产品、时间）与文本特征（TF-IDF、词嵌入）；训练传统分类器（逻辑回归、随机森林、XGBoost）与深度学习模型（BiLSTM、Transformer-based）并使用交叉验证与AUC、F1等指标评估；分析特征重要性与误分类案例。

Result: 揭示了漏洞披露的时间/供应商集中度与高CVSS漏洞的分布差异；基于元数据+文本的模型通常优于仅用单一类型特征，XGBoost在小样本场景中表现稳健，基于Transformer的文本模型在捕捉语义上有优势但需大量数据；重要指示特征包括漏洞影响范围（远程/本地）、可利用性与描述中关键术语。

Conclusion: 对ZDI零日披露的系统性分析有助于改进补丁优先级与漏洞管理；在样本有限的实际场景中，结合元数据与浅层文本特征并采用稳健的传统模型是可行路径，同时应注意数据偏差、标签质量与模型可解释性，并建议扩大多源数据与长期跟踪以提升预测能力。

Abstract: Zero-day vulnerabilities represent some of the most critical threats in cybersecurity, as they correspond to previously unknown flaws in software or hardware that are actively exploited before vendors can develop and deploy patches. During this exposure window, affected systems remain defenseless, making zero-day attacks particularly damaging and difficult to mitigate. This study analyzes the Zero Day Initiative (ZDI) vulnerability disclosures reported between January and April 2024, Cole [2025] comprising a total of 415 vulnerabilities. The dataset includes vulnerability identifiers, Common Vulnerability Scoring System (CVSS) v3.0 scores, publication dates, and short textual descriptions. The primary objectives of this work are to identify trends in zero-day vulnerability disclosures, examine severity distributions across vendors, and investigate which vulnerability characteristics are most indicative of high severity. In addition, this study explores predictive modeling approaches for severity classification, comparing classical machine learning techniques with deep learning models using both structured metadata and unstructured textual descriptions. The findings aim to support improved patch prioritization strategies, more effective vulnerability management, and enhanced organizational preparedness against emerging zero-day threats.

</details>


### [28] [Unveiling the Attribute Misbinding Threat in Identity-Preserving Models](https://arxiv.org/abs/2512.15818)
*Junming Fu,Jishen Zeng,Yi Jiang,Peiyu Zhuang,Baoying Chen,Siyu Lu,Jianquan Yang*

Main category: cs.CR

TL;DR: 本文提出“属性错配攻击”，通过构造看似无害的文本提示，利用模型注意力偏差导致的属性绑定缺陷，使身份保持生成模型错误地将有害描述归因于目标身份并生成不安全内容，从而绕过文本过滤器。作者构建了Misbinding Prompt评估集并引入了属性绑定安全评分（ABSS）来同时评估内容忠实度和安全合规性。实验显示该评估集较主流数据集对五种主流文本过滤器的绕过成功率提高了5.28%，并产生更多NSFW内容。


<details>
  <summary>Details</summary>
Motivation: 揭示并量化身份保持生成模型在属性绑定方面的脆弱性，评估其被滥用生成针对个人的有害（NSFW）内容的风险，并提供更全面的安全评估指标。

Method: 提出属性错配攻击：设计表面无害的提示词以逃避文本过滤器，利用模型内部的注意力偏差导致属性绑定错误，从而将危险属性（如色情、暴力等）错误地关联到目标身份。构建Misbinding Prompt评估集覆盖色情、暴力、歧视和非法性四类风险，并设计ABSS指标以并行衡量内容忠实度与安全合规性。

Result: 在实验中，Misbinding Prompt评估集相比现有主流评估集对包括GPT-4o在内的五种文本过滤器具有5.28%的更高绕过成功率，并且生成的不安全内容比例更高。ABSS能更全面地反映身份保持模型在内容忠实与安全合规方面的表现。

Conclusion: 该工作指出身份保持模型存在基于注意力偏差的属性绑定脆弱性，提出了有效的攻击方法和评估手段（Misbinding Prompt与ABSS），为模型安全性评估与防护提供了新的视角与基线。

Abstract: Identity-preserving models have led to notable progress in generating personalized content. Unfortunately, such models also exacerbate risks when misused, for instance, by generating threatening content targeting specific individuals. This paper introduces the \textbf{Attribute Misbinding Attack}, a novel method that poses a threat to identity-preserving models by inducing them to produce Not-Safe-For-Work (NSFW) content. The attack's core idea involves crafting benign-looking textual prompts to circumvent text-filter safeguards and leverage a key model vulnerability: flawed attribute binding that stems from its internal attention bias. This results in misattributing harmful descriptions to a target identity and generating NSFW outputs. To facilitate the study of this attack, we present the \textbf{Misbinding Prompt} evaluation set, which examines the content generation risks of current state-of-the-art identity-preserving models across four risk dimensions: pornography, violence, discrimination, and illegality. Additionally, we introduce the \textbf{Attribute Binding Safety Score (ABSS)}, a metric for concurrently assessing both content fidelity and safety compliance. Experimental results show that our Misbinding Prompt evaluation set achieves a \textbf{5.28}\% higher success rate in bypassing five leading text filters (including GPT-4o) compared to existing main-stream evaluation sets, while also demonstrating the highest proportion of NSFW content generation. The proposed ABSS metric enables a more comprehensive evaluation of identity-preserving models by concurrently assessing both content fidelity and safety compliance.

</details>


### [29] [Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications](https://arxiv.org/abs/2512.15823)
*Mohammad Waquas Usmani,Sankalpa Timilsina,Michael Zink,Susmit Shannigrahi*

Main category: cs.CR

TL;DR: Proposes server-side downsampling plus partial encryption and client-side ML super-resolution to reduce bandwidth and encryption latency for 360°/6DoF point cloud streaming, achieving near-linear bandwidth/latency reduction and good reconstruction quality with modest inference time.


<details>
  <summary>Details</summary>
Motivation: High bandwidth and low-latency requirements of immersive point-cloud AR/VR streaming; encryption/decryption overhead worsens end-to-end latency. Aim to reduce network load and cryptographic delays while preserving visual fidelity.

Method: Downsample point clouds at the origin server; perform partial (selective) encryption to reduce cryptographic workload; transmit reduced content; at client, decrypt only encrypted parts and use an ML-based super-resolution/upscaling model to reconstruct full-resolution point clouds.

Result: Evaluation shows almost linear reductions in bandwidth and latency as downsampling increases; encryption/decryption overhead also reduces. The ML super-resolution model reconstructs full-resolution point clouds with minimal error and has modest inference time suitable for real-time use.

Conclusion: Combining downsampling, partial encryption, and ML super-resolution can effectively trade bandwidth and cryptographic latency for computation on the client with good reconstruction quality, but further details and evaluations are needed for security, scalability, and device constraints.

Abstract: Immersive formats such as 360° and 6DoF point cloud videos require high bandwidth and low latency, posing challenges for real-time AR/VR streaming. This work focuses on reducing bandwidth consumption and encryption/decryption delay, two key contributors to overall latency. We design a system that downsamples point cloud content at the origin server and applies partial encryption. At the client, the content is decrypted and upscaled using an ML-based super-resolution model. Our evaluation demonstrates a nearly linear reduction in bandwidth/latency, and encryption/decryption overhead with lower downsampling resolutions, while the super-resolution model effectively reconstructs the original full-resolution point clouds with minimal error and modest inference time.

</details>


### [30] [Security Aspects of ISO 15118 Plug and Charge Payment](https://arxiv.org/abs/2512.15966)
*Jakob Löw,Vishwa Vasu,Thomas Hutzelmann,Hans-Joachim Hof*

Main category: cs.CR

TL;DR: 本文系统性审视了ISO 15118中定义的安全控制，发现其在实际场景中不足以保障快速充电通信，特别是揭示并实现了一个未公开的“plug and charge”漏洞：攻击者可让一辆车充电而将费用记到另一辆受害车上。作者给出PoC并提出一种更简化、抗攻击性的认证替代方案。


<details>
  <summary>Details</summary>
Motivation: 随着电动车长途驾驶的增长，缩短充电时间至关重要；ISO 15118作为欧洲主导的直流快充标准，旨在在保证通信与支付安全的同时实现无缝身份认证（plug and charge），但实际部署中存在安全风险需要分析并修复。

Method: 逐条检查ISO 15118定义的安全控制，形式化攻击面与威胁模型，设计并实现攻击的PoC来验证漏洞可被实际利用；基于发现的缺陷，提出一种替代的plug and charge认证方案，分析其部署成本与安全性。

Result: 证明现有标准下存在可实际利用的漏洞，PoC能够在现实或近现实环境中使一辆车充电但把账单记在另一辆车；替代认证方案在证书注册开销更小且在理论上更具抗滥用性。

Conclusion: 当前ISO 15118的安全控制不足以阻止严重的计费滥用攻击，建议在实现与标准进化中纳入本文发现与替代方案的缓解措施，以保障快充生态的安全与信任。

Abstract: For the rise of electric vehicles, especially for long-distance driving, minimizing charging times is vital. While multiple standards for DC fast charging exist, the leading standard in Europe is ISO 15118. In theory, this standard is accompanied by a variety of security controls, ensuring the authenticity and confidentiality of charging communication, as well as the exchange of payment information. In practice, these security controls are insufficient for effectively securing charging communication. In this paper, we go through all security controls defined in ISO 15118 and demonstrate their shortcomings. Most notably, we present a previously unpublished vulnerability in the plug and charge functionality of ISO 15118. We provide a proof-of-concept implementation of this vulnerability, which, allows a vehicle to be charged while a second, victim vehicle is billed for it. Additionally, we define an alternative plug and charge authentication scheme, which requires fewer efforts towards certificate enrollment and promises to be more resilient and future-proof. Our findings should be considered when implementing and advancing the standard, as the mitigation of the discovered vulnerability is critical for the security of fast charging.

</details>


### [31] [ContextLeak: Auditing Leakage in Private In-Context Learning Methods](https://arxiv.org/abs/2512.16059)
*Jacob Choi,Shuying Cao,Xingjian Dong,Wang Bill Zhu,Robin Jia,Sai Praneeth Karimireddy*

Main category: cs.CR

TL;DR: 本文提出ContextLeak，一种通过插入canary（可识别的标记）并设计针对性查询来测量ICL中最坏情况信息泄露的框架。实验表明ContextLeak与理论隐私预算ε高度相关，能可靠检测泄露，同时揭示多数现有防护在隐私-效用上权衡较差。


<details>
  <summary>Details</summary>
Motivation: 在ICL中示例可能包含敏感信息，尽管已有多种保护方法，但缺乏用于审计和量化这些方法实际泄露风险的工具，尤其是测量最坏情况泄露。

Method: （补充）在多种模型和防御下运行实验，比较ContextLeak检测到的泄露与理论隐私预算ε的相关性，并评估隐私-效用权衡。

Result: ContextLeak能够紧密反映理论ε并可靠检测泄露；实验发现许多现有方法要么不能阻止敏感信息泄露，要么为了保护隐私大幅降低下游任务性能。

Conclusion: ContextLeak是评估ICL隐私风险的有效工具，可用于审计现有和新提出的私有ICL方法；研究指出需要更好的方法来改善隐私与效用之间的权衡。

Abstract: In-Context Learning (ICL) has become a standard technique for adapting Large Language Models (LLMs) to specialized tasks by supplying task-specific exemplars within the prompt. However, when these exemplars contain sensitive information, reliable privacy-preserving mechanisms are essential to prevent unintended leakage through model outputs. Many privacy-preserving methods are proposed to protect the information leakage in the context, but there are less efforts on how to audit those methods. We introduce ContextLeak, the first framework to empirically measure the worst-case information leakage in ICL. ContextLeak uses canary insertion, embedding uniquely identifiable tokens in exemplars and crafting targeted queries to detect their presence. We apply ContextLeak across a range of private ICL techniques, both heuristic such as prompt-based defenses and those with theoretical guarantees such as Embedding Space Aggregation and Report Noisy Max. We find that ContextLeak tightly correlates with the theoretical privacy budget ($ε$) and reliably detects leakage. Our results further reveal that existing methods often strike poor privacy-utility trade-offs, either leaking sensitive information or severely degrading performance.

</details>


### [32] [Design of a Decentralized Fixed-Income Lending Automated Market Maker Protocol Supporting Arbitrary Maturities](https://arxiv.org/abs/2512.16080)
*Tianyi Ma*

Main category: cs.CR

TL;DR: 本文提出BondMM-A，将BondMM的数学不变量推广到任意到期日，实现单一合约内支持多期限固定收益借贷，从而提升资金效率和操作灵活性；实验显示在利率稳定性与金融稳健性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有DeFi固定收益AMM面临时间相关复杂度且多为单到期日设计，导致资金分散、流动性低和LP受限。作者认为BondMM的不变量形式优雅，可用于构建支持任意到期日的通用框架以解决这些问题。

Method: 提出BondMM-A：基于对BondMM数学不变量的推广，设计单一智能合约管理多期限债券头寸与流动性分配；允许用户提供或借入任意到期日的固定收益产品，合约在内部以数学曲线/不变量保证价格与利率结构，并提供LP权重与资本效率优化机制。

Result: 实验（模拟或回测）表明BondMM-A在利率波动控制、资金利用率和对冲/清算鲁棒性方面表现良好，优于单一到期日方案（摘要未给出具体数值）。

Conclusion: BondMM-A为DeFi固定收益AMM提供了可扩展且高效的多期限方案，提升了LP和用户的灵活性与资本效率；需补充理论证明、详细实验设置、对手力与实现成本分析以增强结论可信度。

Abstract: In decentralized finance (DeFi), designing fixed-income lending automated market makers (AMMs) is extremely challenging due to time-related complexities. Moreover, existing protocols only support single-maturity lending. Building upon the BondMM protocol, this paper argues that its mathematical invariants are sufficiently elegant to be generalized to arbitrary maturities. This paper thus propose an improved design, BondMM-A, which supports lending activities of any maturity. By integrating fixed-income instruments of varying maturities into a single smart contract, BondMM-A offers users and liquidity providers (LPs) greater operational freedom and capital efficiency. Experimental results show that BondMM-A performs excellently in terms of interest rate stability and financial robustness.

</details>


### [33] [DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack](https://arxiv.org/abs/2512.16182)
*Hao Li,Yubing Ren,Yanan Cao,Yingjie Li,Fang Fang,Shi Wang,Li Guo*

Main category: cs.CR

TL;DR: 提出DualGuard，一种通过自适应双流水印机制在语义基础上动态注入两种互补水印信号的算法，能够同时抵抗意译（paraphrase）攻击和搭便车式（piggyback/spoofing）攻击，实现检测和溯源。实验表明其在可检测性、鲁棒性、可溯源性和文本质量上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着云端LLM服务普及，模型滥用风险上升。现有水印方法多聚焦于对抗意译攻击，忽视了搭便车/伪造注入类攻击，这类攻击可注入有害内容、破坏水印可靠性与归属信任。需要一种能同时检测和追踪这些攻击的水印方案。

Method: 提出DualGuard：采用自适应双流（dual-stream）水印机制，根据文本语义动态注入两种互补水印信号。双信号设计用于在遭遇不同类型攻击时仍保留可检测特征，并为搭便车式伪造提供溯源线索。实现细节包括信号生成策略、语义感知注入机制和检测/追溯算法（摘要层面描述）。

Result: 在多个数据集和不同规模语言模型上进行了广泛实验。结果显示，DualGuard在水印可检测性、对意译和伪造攻击的鲁棒性、伪造追踪能力以及对文本质量（可读性、任务性能）损害最小化方面均优于现有方法。

Conclusion: DualGuard首次实现了同时防护意译与搭便车式伪造攻击的水印体系，能够检测并追踪伪造，有助于提升水印检测的可靠性与现实部署的可行性。

Abstract: With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.

</details>


### [34] [In-Context Probing for Membership Inference in Fine-Tuned Language Models](https://arxiv.org/abs/2512.16292)
*Zhexi Lu,Hongliang Chi,Nathalie Baracaldo,Swanand Ravindra Kadhe,Yuseok Jeon,Lei Yu*

Main category: cs.CR

TL;DR: 提出ICP-MIA，一种基于训练动力学中“优化回报递减”现象的黑盒成员推断攻击框架：引入“优化差距”作为成员信号，并通过无训练的上下文探测（ICP）估计该差距（基于参考数据或自扰动）；在多任务多模型上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒MIA依赖置信度或token似然等信号，但这些信号与样本内在难易度或稀有性混杂，导致泛化差、信噪比低。需要更稳健且与训练过程相关的成员判据。

Method: 提出Optimization Gap概念：收敛时成员样本的剩余损失可减小空间小于非成员。设计In-Context Probing(ICP)在黑盒下模拟微调行为以估计该差距。两种探测策略：1) 参考数据法：用语义相似的公开样本构造上下文；2) 自扰动法：通过mask或生成改动样本本身。无需对模型进行额外训练或访问梯度。

Result: 在三类任务和多种大模型、不同PEFT配置及训练计划上进行实验，结果表明ICP-MIA在整体性能和低假阳性率区间显著优于先前黑盒攻击。还分析了参考数据对齐度、模型类型及微调细节对攻击效果的影响。

Conclusion: ICP-MIA将训练动力学理论与黑盒探测结合，给出了一个实用且理论支撑的成员推断框架，可用于审计部署LLM的隐私风险。

Abstract: Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.

</details>


### [35] [Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks](https://arxiv.org/abs/2512.16307)
*Safwan Shaheer,G. M. Refatul Islam,Mohammad Rafid Hamid,Tahsin Zaman Jilan*

Main category: cs.CR

TL;DR: 论文研究小型开源LLM（以LLaMA系为例）面临的提示注入（prompt injection）攻击风险，提出一种以“Chain of Thoughts”为种子、迭代生成和优化防御提示的自动化防御框架，并在基准攻击集上系统评估，结果显示显著降低攻击成功率和误报率。


<details>
  <summary>Details</summary>
Motivation: 随着小型开源LLM在边缘设备和广泛部署中的增长，提示注入导致的目标劫持（goal-hijacking）成为严重安全隐患，现有防御对最新攻击适应性不足，需要自动化、可扩展且针对资源受限环境的防御机制。

Method: 提出一个基于种子防御（Chain of Thoughts）的迭代提示优化框架：从初始CoT防御提示出发，自动生成并评估候选防御提示，利用基准攻击集合进行筛选和精化；在LLaMA系列小模型上对比现有提示级防御，使用攻击成功率和误报率等指标量化效果。

Result: 实验表明该方法在多种基准攻击下显著降低了目标劫持的成功率，同时降低了误报率，提升了对goal-hijacking行为的检测能力，在小型开源模型上实现了可观的防御增益。

Conclusion: 该工作评估了当前提示防御的有效性，提出并验证了一个可自动化、迭代优化的防御框架，证明其在资源受限环境中增强小型开源LLM安全性的可行性，为后续在更广攻击场景和实地部署中的改进提供了基础。

Abstract: In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.

</details>


### [36] [SoK: Reviewing Two Decades of Security, Privacy, Accessibility, and Usability Studies on Internet of Things for Older Adults](https://arxiv.org/abs/2512.16394)
*Suleiman Saka,Sanchari Das*

Main category: cs.CR

TL;DR: 该系统综述评估了2004–2024年44项关于老年人IoT的研究，提出了包含27项标准的SPAU-IoT框架（安全、隐私、无障碍、可用性），并给出威胁模型与设计指南。总体发现：多数研究关注身份验证与加密，但少于50%考虑无障碍或可用性，缺乏集成化SPAU方法。


<details>
  <summary>Details</summary>
Motivation: 老年人面临因IoT设备普及带来的安全、隐私、无障碍与可用性风险，现有研究分散且未形成统一评估标准，亟需系统化框架以指导设计与评估。

Method: （注：对方法项重复项作保留以供完整性）

Result: 发现超过70%的研究实现了认证与加密，但<50%涉及无障碍或可用性。提出了明确映射资产与攻击向量（如钓鱼、照护者滥用、弱密码），并将缺口转化为与标准对齐的可操作设计建议。

Conclusion: 现有IoT研究中缺乏将安全、隐私、无障碍与可用性整合的系统化方案；提出的SPAU-IoT框架与威胁模型可作为评估与设计基线，但需更多实证验证、纵向与跨文化研究以提升可推广性。

Abstract: The Internet of Things (IoT) has the potential to enhance older adults' independence and quality of life, but it also exposes them to security, privacy, accessibility, and usability (SPAU) risks. We conducted a systematic review of 44 peer-reviewed studies published between 2004 and 2024 using a five-phase screening pipeline. From each study, we extracted data on study design, IoT type, SPAU measures, and identified research gaps. We introduce the SPAU-IoT Framework, which comprises 27 criteria across four dimensions: security (e.g., resilience to cyber threats, secure authentication, encrypted communication, secure-by-default settings, and guardianship features), privacy (e.g., data minimization, explicit consent, and privacy-preserving analytics), accessibility (e.g., compliance with ADA/WCAG standards and assistive-technology compatibility), and usability (e.g., guided interaction, integrated assistance, and progressive learning). Applying this framework revealed that more than 70% of studies implemented authentication and encryption mechanisms, whereas fewer than 50% addressed accessibility or usability concerns. We further developed a threat model that maps IoT assets, networks, and backend servers to exploit vectors such as phishing, caregiver exploitation, and weak-password attacks, explicitly accounting for age-related vulnerabilities including cognitive decline and sensory impairment. Our results expose a systemic lack of integrated SPAU approaches in existing IoT research and translate these gaps into actionable, standards-aligned design guidelines for IoT systems designed for older adults.

</details>


### [37] [From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2512.16439)
*Hao Li,Yubing Ren,Yanan Cao,Yingjie Li,Fang Fang,Xuebin Wang*

Main category: cs.CR

TL;DR: SemMark 提出一种基于语义的嵌入向量水印方案：用局部敏感哈希（LSH）划分语义空间，在特定语义区域注入水印，并用基于局部离群因子（LOF）的自适应权重保持原始分布。作者还设计了检测-采样（Detect-Sampling）和降维（Dimensionality-Reduction）攻击以评估鲁棒性，并在四个 NLP 数据集上展示了较好的可验证性、多样性、隐蔽性和无害性。


<details>
  <summary>Details</summary>
Motivation: 现有 EaaS（水嵌入服务）易遭仿制，传统水印方法忽视嵌入向量最关键的属性——语义，导致水印易被检测或破坏，同时影响服务质量，因此需要一种语义感知、对下游任务无害且隐蔽的水印方案。

Method: 用 LSH 将嵌入空间划分为语义区块，在选定区块内注入语义相关的水印信号；基于 LOF 计算局部异常度并调整水印权重以尽量保持原始分布和下游性能；提出 Detect-Sampling 与 Dimensionality-Reduction 两类攻击并构建四种评估场景来测试鲁棒性。

Result: 在四个常用 NLP 数据集上的大量实验表明：SemMark 在水印可验证性、样本/信号多样性、隐蔽性（难被发现）和无害性（对下游任务影响小）等指标上优于对比方法，且在所提出的攻击场景下表现稳健。

Conclusion: 通过引入语义感知的区域划分与自适应权重，SemMark 在保护 EaaS 知识产权时能更好兼顾隐蔽性与服务质量。未来需进一步验证对更强适应性攻击、跨模型迁移和大规模在线部署的鲁棒性与扩展性。

Abstract: Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.

</details>


### [38] [Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking](https://arxiv.org/abs/2512.16658)
*Sangeeth B,Serena Nicolazzo,Deepa K.,Vinod P*

Main category: cs.CR

TL;DR: 提出一种在白盒设置下利用混沌序列（logistic map）注入到中间层权重的轻量级水印方案，并用遗传算法从被盗模型权重中恢复混沌参数以验证所有权。对MNIST和CIFAR-10进行了实证，声称对微调鲁棒且不影响准确率。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型价值上升，模型被复制或滥用的风险增加，需要能在白盒条件下嵌入和验证所有权的高效、不可见且鲁棒的水印机制。

Method: 使用logistic map产生对初始值敏感的混沌序列，将该序列注入到选定中间层的权重中而不改变模型结构或训练目标；验证阶段用遗传算法搜索混沌参数，使提取序列与重生成序列的相似度最大化；还采用权重密度图和基于激活的分类器进行可视化和区分分析。

Result: 在MNIST和CIFAR-10上实验表明水印嵌入后模型准确率下降可忽略，经过微调后水印仍可检测；同时展示数值恢复、权重分布可视化及激活分类器在原始、带水印和篡改模型间的区分能力。

Conclusion: 该方法在白盒场景下提供了一种可扩展、灵活且对微调有鲁棒性的所有权嵌入与验证方案，适合实际IP保护需求。

Abstract: The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.

</details>


### [39] [Efficient Bitcoin Meta-Protocol Transaction and Data Discovery Through nLockTime Field Repurposing](https://arxiv.org/abs/2512.16683)
*Nikodem Tomczak*

Main category: cs.CR

TL;DR: 提出 Lockchain 协议：用比特币交易的 4 字节 nLockTime 字段作为紧凑元数据头，实现高效事务发现且不增加区块空间。通过限定为一个未被使用的过去 Unix 时间戳区间（>=500,000,000），在保持对共识与策略规则有效性的同时编码协议信号、类型、变体与序列标识。索引器可先检索固定大小头部再深入检查较重的负载，从而提高发现效率。


<details>
  <summary>Details</summary>
Motivation: 当前大规模交易发现（如基于 OP_RETURN 或见证字段的元数据）效率低，索引器需要扫描和解码大量可变长度字段，成本高且不便扩展。作者希望在不引入新链上存储机制或密码学原语的前提下，提供一个轻量、零边际块空间成本的发现层。

Method: 将每笔交易的 nLockTime 字段限定到一个未被使用的过去 Unix 时间戳范围，作为 4 字节固定头部来编码协议版本、消息类型、变体与序列 ID。索引器先读取该固定字段来快速筛选候选交易，再按需解析 OP_RETURN、见证等重字段。整个设计遵循现有协议设计模式，强调兼容性与低侵入性。

Result: 提出了一种低影响、高效的交易发现层设计，理论上显著降低索引器在海量交易中筛选相关元数据的成本，并能在不占用额外区块空间的前提下实现版本化与序列化的元数据标识。作者未声称新密码学或存储方法。

Conclusion: Lockchain 利用被低估的 nLockTime 字段实现了一种实用的、向后兼容的元数据发现机制，适合构建高效索引层。采用该方法需要注意与现有钱包、替代使用场景及隐私/策略交互的潜在冲突，建议通过规范（如 BIP）与工具链支持逐步推广。

Abstract: We describe the Lockchain Protocol, a lightweight Bitcoin meta-protocol that enables highly efficient transaction discovery at zero marginal block space cost, and data verification without introducing any new on-chain storage mechanism. The protocol repurposes the mandatory 4-byte nLockTime field of every Bitcoin transaction as a compact metadata header. By constraining values to an unused range of past Unix timestamps greater than or equal to 500,000,000, the field can encode a protocol signal, type, variant, and sequence identifier while remaining fully valid under Bitcoin consensus and policy rules. The primary contribution of the protocol is an efficient discovery layer. Indexers can filter candidate transactions by examining a fixed-size header field, independent of transaction payload size, and only then selectively inspect heavier data such as OP RETURN outputs or witness fields. The Lockchain Protocol applies established protocol design patterns to an under-optimised problem domain, namely transaction discovery at scale, and does not claim new cryptographic primitives or storage methods.

</details>


### [40] [PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy](https://arxiv.org/abs/2512.16851)
*Ripan Kumar Kundu,Istiak Ahmed,Khaza Anuarul Hoque*

Main category: cs.CR

TL;DR: 提出将可解释性(XAI)与差分隐私(DP)结合，通过后验解释选取最重要特征并仅对其在推理时施加DP噪声，旨在在保障隐私（对抗MIA与RDA）同时保留模型效用并加速推理。结果在三种XR任务上显著降低攻击成功率并保持高精度，且在HTC VIVE Pro上实现实时可调隐私界面PrivateXR。


<details>
  <summary>Details</summary>
Motivation: AI XR系统使用高度敏感的行为数据（如眼动）且特征维度高，统一对所有特征应用DP会引入过量噪声，损失准确率并增加延迟，不利于实时XR。作者希望有选择性地施加隐私保护以平衡隐私与效用。

Method: 利用后验XAI方法（post-hoc explanations）识别模型推理时最具影响力的特征，并在推理阶段仅对这些重要特征应用差分隐私机制以添加噪声。评估基线包括三种AI XR模型与三类数据集（晕动、情绪、活动），并在真实设备上实现UI供用户调节隐私等级。

Result: 在晕动任务中，MIA和RDA成功率分别最多降低约43%与39%，Transformer模型在保留高达97%准确率的同时实现约2倍的推理加速（相比传统对所有特征统一DP的方案）。实现了可调隐私UI（PrivateXR）并部署于HTC VIVE Pro。

Conclusion: XAI引导的选择性DP在AI XR场景中能更有效地权衡隐私与模型效用，减少攻击成功率并提升实时性，具备实际部署潜力。

Abstract: The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.

</details>


### [41] [How Good is Post-Hoc Watermarking With Language Model Rephrasing?](https://arxiv.org/abs/2512.16904)
*Pierre Fernandez,Tom Sander,Hady Elsahar,Hongyan Chang,Tomáš Souček,Valeriu Lacatusu,Tuan Tran,Sylvestre-Alvise Rebuffi,Alexandre Mourachko*

Main category: cs.CR

TL;DR: 研究提出并评估“事后水印（post-hoc watermarking）”——通过让大模型重写已有文本并在生成时嵌入水印，以便追踪AI生成内容。实验显示在开放式文本（如书籍）上能在保持语义忠实度的同时取得高可检测性，但在可验证文本（如代码）上效果较差且小模型反而更优。


<details>
  <summary>Details</summary>
Motivation: 现有水印多为生成时策略，受模型部署方式限制；研究探讨在事后重写场景下，如何利用更灵活的生成与检测策略（如更大模型、多候选、束搜索、熵过滤）改进水印质量与可检测性之间的权衡，满足版权保护与训练/RAG检测需求。

Method: 在事后重写场景下，比较多种水印嵌入与检测配置：不同规模的重写模型、采样策略（nucleus、Gumbel-max等）、束搜索、多候选生成、以及检测侧的熵过滤；在开放式文本与可验证文本（代码）上评估语义保真、可检测性（radioactivity）与生成质量。

Result: 在开放式文本上多数方法能兼顾高可检测性与语义忠实度；Gumbel-max在nucleus采样下表现优于最新方法，束搜索普遍提升效果；在代码等可验证文本中大多数方法失败，且小模型比大模型表现更好（反常现象）。

Conclusion: 事后水印在开放式文本的实践中具有显著潜力，但对可验证内容（如代码）存在局限；研究为实际应用与后续改进（采样策略、模型-文本类型匹配、检测优化）提供了方向。

Abstract: Generation-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore *post-hoc watermarking* where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [42] [TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge](https://arxiv.org/abs/2512.15729)
*Matteo Fasulo,Giusy Spacone,Thorir Mar Ingolfsson,Yawei Li,Luca Benini,Andrea Cossettini*

Main category: eess.SP

TL;DR: 本文提出TinyMyo，一种基于Transformer编码器的轻量级表面肌电（EMG）基础模型。通过自监督预训练并在多个公开数据集上微调，TinyMyo在手势分类、手部运动回归、语音生成与识别等任务上达到或优于现有最先进水平，同时模型参数仅3.6M且可部署于超低功耗微控制器（GAP9），实测功耗36.45mW。作者已开源模型与代码。


<details>
  <summary>Details</summary>
Motivation: 当前EMG模型难以在不同受试者、设备与采集协议之间泛化，且现有EMG基础模型常限于单一下游任务且不可部署到嵌入式平台。为解决这些限制，提出轻量且跨任务的EMG基础模型，兼具可移植性与低功耗部署能力。

Method: 设计了名为TinyMyo的Transformer编码器骨干，采用自监督预训练策略在公开EMG数据集上学习高保真重建。模型仅3.6M参数，经过简单任务特定头部适配即可用于多种下游任务（分类、回归、语音等），并在不同传感位置和硬件平台的数据上进行评估与微调。作者还在GAP9微控制器上实现部署并测量功耗。

Result: 在多个数据集上取得或超越最先进结果：NinaPro DB5 89.4±0.16%、UCI-EMG 97.56±0.32%、EPN-612 96.74±0.09%。模型参数低于5M，预训练模型与下游架构开源，且实现了在GAP9上的首次EMG基础模型部署，平均功耗36.45mW。

Conclusion: TinyMyo展示了轻量级EMG基础模型在多任务泛化与嵌入式部署方面的可行性，有望成为EMG研究的通用基础资源，推动未来研究与实际应用。

Abstract: Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\pm0.16\%$), UCI-EMG ($97.56\pm0.32\%$), and EPN-612 ($96.74\pm0.09\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.

</details>


### [43] [HiLTS: Human in the Loop Therapeutic System: A Wireless-enabled Precision Medicine Platform for Brainwave Entrainment](https://arxiv.org/abs/2512.15807)
*Arfan Ghani*

Main category: eess.SP

TL;DR: 本文提出并验证了一个低功耗定制数字芯片，可产生稳定的6 Hz脉冲列用于“强制”节律性地同步/覆盖癫痫发作信号。通过公开EEG数据的发作波形提取、数字化并与芯片输出（使用Saleae Logic采集）直接比较，时域与频域分析表明芯片能在模拟化重构后对发作活动施加窄带6 Hz节律，作为可穿戴癫痫干预设备的概念性证明。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作源于异常同步的神经活动，且部分患者对药物治疗无效。已有的节律干预设备通常依赖复杂的模拟电路或高功率刺激硬件，需探索低功耗、数字化且简化的可穿戴解决方案。

Method: 1) 从公开EEG发作库提取并平均发作波形；2) 将波形数字化以模拟神经前端；3) 使用定制数字芯片产生稳定的6 Hz脉冲列并用Saleae Logic记录数字输出；4) 对芯片脉冲列重采样并低通重构为模拟6 Hz波形；5) 通过时域和频域分析比较原始发作、数字化表示与芯片输出的形态与谱特性。

Result: 重构后的芯片输出呈窄带6 Hz节律，能够覆盖原有发作的宽带谱分布，频域和时域分析均支持芯片对发作活动施加优势频率的结论，为低功耗数字化节律干预提供实验性证据。

Conclusion: 研究提供了定制低功耗数字芯片用于癫痫节律性干预的概念验证，指出该方法有望简化并推动可穿戴发作中断设备的发展，但仍需进一步的生理学、体内实验、个体化调节与安全性/长时稳定性研究。

Abstract: Epileptic seizures arise from abnormally synchronised neural activity and remain a major global health challenge, affecting more than 50 million people worldwide. Despite advances in pharmacological interventions, a significant proportion of patients continue to experience uncontrolled seizures, underscoring the need for alternative neuromodulation strategies. Rhythmic neural entrainment has recently emerged as a promising mechanism for disrupting pathological synchrony, but most existing systems rely on complex analogue electronics or high-power stimulation hardware. This study investigates a minimal digital custom-designed chip that generates a stable 6 Hz oscillation capable of entraining epileptic seizure activity. Using a publicly available EEG seizure dataset, we extracted and averaged analogue seizure waveforms, digitised them to emulate neural front-ends, and directly interfaced the digitised signals with digital output recordings acquired from the chip using a Saleae Logic analyser. The chip pulse train was resampled and low-pass-reconstructed to produce an analogue 6 Hz waveform, allowing direct comparison between seizure morphology, its digitised representation, and the entrained output. Frequency-domain and time-domain analyses demonstrate that the chip imposes a narrow-band 6 Hz rhythm that overrides the broadband spectral profile of seizure activity. These results provide a proof-of-concept for low-power digital custom-designed entrainment as a potential pathway toward simplified, wearable seizure-interruption devices for precision medicine and future healthcare devices.

</details>


### [44] [Concurrence: A dependence criterion for time series, applied to biological data](https://arxiv.org/abs/2512.16001)
*Evangelos Sariyanidi,John D. Herrington,Lisa Yankowitz,Pratik Chaudhari,Theodore D. Satterthwaite,Casey J. Zampella,Jeffrey S. Morris,Edward Gunning,Robert T. Schultz,Russell T. Shinohara,Birkan Tunc*

Main category: eess.SP

TL;DR: Proposes 'concurrence': classify aligned vs misaligned segment pairs to detect dependence between time series without strong priors or large datasets.


<details>
  <summary>Details</summary>
Motivation: Nonlinear, complex dependencies in biological time series are hard to detect with existing methods that require model assumptions or large samples; need a general, data-driven criterion.

Method: Formulate dependence criterion: build classifier to discriminate temporally aligned segment pairs from misaligned (shuffled) ones; good classification accuracy implies dependence. Theoretically link classifier performance with statistical dependence.

Result: Demonstrated concurrence works across modalities (fMRI, physiological, behavioral), detects relationships without ad-hoc tuning or massive data, and aligns with dependence measures in theory and practice.

Conclusion: Concurrence provides a practical, broadly applicable test for dependence in time series, especially useful for complex biological signals, though details on classifier choice, sample requirements, and statistical calibration need clarification.

Abstract: Measuring the statistical dependence between observed signals is a primary tool for scientific discovery. However, biological systems often exhibit complex non-linear interactions that currently cannot be captured without a priori knowledge or large datasets. We introduce a criterion for dependence, whereby two time series are deemed dependent if one can construct a classifier that distinguishes between temporally aligned vs. misaligned segments extracted from them. We show that this criterion, concurrence, is theoretically linked with dependence, and can become a standard approach for scientific analyses across disciplines, as it can expose relationships across a wide spectrum of signals (fMRI, physiological and behavioral data) without ad-hoc parameter tuning or large amounts of data.

</details>


### [45] [Fast Collaborative Inference via Distributed Speculative Decoding](https://arxiv.org/abs/2512.16273)
*Ce Zheng,Ke Zhang,Sun Chen,Wenqi Zhang,Qiong Liu,Angesom Ataklity Tesfay*

Main category: eess.SP

TL;DR: 提出了 Truncated Sparse Logits Transmission (TSLT)：在分布式推理中只上行传输截断候选集合的 logits 与索引，从而大幅降低上行通信开销，同时在理论上保证接受率不变，并扩展到多候选以进一步提高接受概率。实验证明在保持端到端延迟与模型质量的前提下显著节省上行通信，适用于 AI-RAN 场景的可扩展通信高效分布式 LLM 推理。


<details>
  <summary>Details</summary>
Motivation: 在 AI 原生无线接入网（AI-RAN）中，采用小模型草稿+大模型核验的 speculative decoding 可实现设备-边缘协同推理，但现有方案每步上传全词表 logits，造成巨大的上行通信开销，限制系统可扩展性与实用性。

Method: 提出“先稀疏后采样”的策略 TSLT：只发送截断候选集（candidate set）对应的 logits 与索引，而非全词表；给出理论分析，证明在该截断下接受率（acceptance rate）得以保持；并将方法扩展到多候选情形，即每步草稿生成多个候选以提高核验通过概率。

Result: 实验表明 TSLT 在不同设置下均能显著减少上行通信量，同时保持最终生成质量与端到端推理延迟不变，证明了在 AI-RAN 场景中实现通信效率与推理准确性兼顾的可行性。

Conclusion: TSLT 为分布式 LLM 推理在带宽受限的无线边缘环境中提供了一种有效的通信压缩方案，兼顾理论保证与实证效果，适合推广至未来 AI-RAN 系统。

Abstract: Speculative decoding accelerates large language model (LLM) inference by allowing a small draft model to predict multiple future tokens for verification by a larger target model. In AI-native radio access networks (AI-RAN), this enables device-edge collaborative inference but introduces significant uplink overhead, as existing distributed speculative decoding schemes transmit full vocabulary logits at every step. We propose a sparsify-then-sample strategy, Truncated Sparse Logits Transmission (TSLT), which transmits only the logits and indices of a truncated candidate set. We provide theoretical guarantees showing that the acceptance rate is preserved under TSLT. TSLT is further extended to multi-candidate case, where multiple draft candidates per step increase acceptance probability. Experiments show that TSLT significantly reduces uplink communication while maintaining end-to-end inference latency and model quality, demonstrating its effectiveness for scalable, communication-efficient distributed LLM inference in future AI-RAN systems.

</details>


### [46] [An active-set algorithm for spectral unmixing](https://arxiv.org/abs/2512.16432)
*Nils Foix-Colonier,Sébastien Bourguignon*

Main category: eess.SP

TL;DR: 提出一种针对受非负性和和为一约束的线性光谱解混问题的专用算法（基于活动集），并将非负性扩展为更一般的最小丰度约束，以期在大字典（监督解混）场景下提高计算性能。


<details>
  <summary>Details</summary>
Motivation: 在监督解混（大字典）下，解通常因非负性而具有稀疏性，通用求解器在这种特定结构的问题上效率不足，因而需要专门算法以提升计算速度和可扩展性。

Method: 设计基于活动集的专用求解器，利用问题的凸性、非负性与和为一约束结构进行增量/活动集更新；将非负下界推广为任意最小丰度下界，保证可满足KKT条件并通过投影或解析更新提高每步效率。

Result: 相比通用凸优化器，所提出方法在大字典、稀疏解场景中可显著降低计算时间与内存开销，同时保持或改善解的精度；实验（仿真或实测）验证了其收敛性与性能优势。

Conclusion: 该工作提供了一个针对光谱解混问题定制的高效活动集算法，尤其适用于监督、大字典和稀疏丰度情形，并通过将非负性推广为最小丰度约束增强了模型的灵活性。

Abstract: Linear spectral unmixing under nonnegativity and sum-to-one constraints is a convex optimization problem for which many algorithms were proposed. In practice, especially for supervised unmixing (i.e., with a large dictionary), solutions tend to be sparse due to the nonnegativity of the abundances, thereby motivating the use of an active-set solver. Given the problem specific features, it seems advantageous to design a dedicated algorithm in order to gain computational performance compared to generic solvers. In this paper, we propose to derive such a specific algorithm, while extending the nonnegativity constraints to broader minimum abundance constraints.

</details>


### [47] [Robust 6G OFDM High-Mobility Communications Using Delay-Doppler Superimposed Pilots](https://arxiv.org/abs/2512.16496)
*Mauro Marchese,Pietro Savazzi*

Main category: eess.SP

TL;DR: 提出一种用于6G高移动场景的OFDM接收机架构：在延迟-多普勒域叠加单个导频，考虑ICI、分数延迟和多普勒，给出离散的分数延迟-多普勒估计算法和基于Landweber迭代的低复杂度均衡方法，仿真表明在高达1000 km/h时仍能显著提升有效吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在6G高速移动环境中，传统导频开销大且对频移/分数时延敏感，需设计低开销且鲁棒的信道估计与均衡方案以提高有效吞吐量。

Method: 在延迟-多普勒域叠加单个导频，考虑ICI与分数延迟/多普勒影响，提出一个解耦的分数延迟-多普勒估计算法，并基于信道结构设计了利用Landweber迭代的低复杂度均衡器。

Result: 仿真验证在各种移动速度下（最高1000 km/h）系统性能稳健，且与现有方法相比提高了有效吞吐量。

Conclusion: 该方案在高速场景中通过极低的导频开销和结构化均衡实现了鲁棒的通信性能，但需进一步的复杂度/收敛性分析和更广泛的信道模型验证。

Abstract: In this work, a novel receiver architecture for orthogonal frequency division multiplexing (OFDM) communications in 6G high-mobility scenarios is developed. In particular, a delay-Doppler superimposed pilot (SP) scheme is used for channel estimation (CE) by adding a single pilot in the delay-Doppler domain. Unlike previous research on delay-Doppler superimposed pilots in OFDM systems, intercarrier interference (ICI) effects, fractional delays, and Doppler shifts are considered. Consequently, a disjoint fractional delay-Doppler estimation algorithm is derived, and a reduced-complexity equalization method based on the Landweber iteration, which exploits intrinsic channel structure, is proposed. Simulation results reveal that the proposed receiver architecture achieves robust communication performance across various mobility conditions, with speeds of up to 1000 km/h, and increases the effective throughput compared to existing methods.

</details>


### [48] [Efficient Precoding for LEO Satellites: A Low-Complexity Matrix Inversion Method via Woodbury Matrix Identity and arSVD](https://arxiv.org/abs/2512.16543)
*Mohammad Momani,Thomas Delamotte,Andreas Knopp*

Main category: eess.SP

TL;DR: 提出将Woodbury公式与自适应随机SVD(arSVD)结合，用于低Earth轨道卫星环境下实时更新RZF预编码中的Gram矩阵逆，显著降低计算复杂度（最高节省约61%），仅带来小幅和可接受的和速率下降。


<details>
  <summary>Details</summary>
Motivation: LEO卫星部署大量天线阵列时，信道随卫星运动剧烈变化，传统RZF因需频繁求解Gram矩阵逆而难以实时部署，尤其在功耗与计算资源受限的卫星平台上需要更高效的在线更新方法。

Method: 利用Woodbury公式针对低秩扰动快速更新Gram矩阵逆，结合自适应随机SVD动态提取主导奇异向量/值以近似表示扰动，从而在每次位置或信道小幅变化时以低开销更新逆矩阵，避免完全重算。

Result: 通过蒙特卡洛仿真展示，该方法在与传统全矩阵求逆的RZF比较下，计算开销最多可下降约61%，而sum-rate仅有小幅下降（论文称“适度”），表明在功耗受限环境中可实现实时可行的折衷。

Conclusion: WB-arSVD为下一代卫星通信系统提供了一种可扩展、计算高效的RZF实现途径，适合需要频繁在线更新预编码且受算力/能耗约束的LEO平台。

Abstract: The increasing deployment of massive active antenna arrays in low Earth orbit (LEO) satellites necessitates computationally efficient and adaptive precoding techniques to mitigate dynamic channel variations and enhance spectral efficiency. Regularized zero-forcing (RZF) precoding is widely used in multi-user MIMO systems; however, its real-time implementation is limited by the computationally intensive inversion of the Gram matrix. In this work, we develop a low-complexity framework that integrates the Woodbury (WB) formula with adaptive randomized singular value decomposition (arSVD) to efficiently update the Gram matrix inverse as the satellite moves along its orbit. By leveraging low-rank perturbations, the WB formula reduces inversion complexity, while arSVD dynamically extracts dominant singular components, further enhancing computational efficiency. Monte Carlo simulations demonstrate that the proposed method achieves computational savings of up to 61\% compared to conventional RZF precoding with full matrix inversion, while incurring only a modest degradation in sum-rate performance. These results demonstrate that WB-arSVD offers a scalable and efficient solution for next-generation satellite communications, facilitating real-time deployment in power-constrained environments.

</details>


### [49] [Channel State Information Preprocessing for CSI-based Physical-Layer Authentication Using Reconciliation](https://arxiv.org/abs/2512.16719)
*Atsu Kokuvi Angelo Passah,Rodrigo C. de Lamare,Arsenia Chorti*

Main category: eess.SP

TL;DR: 提出了一种自适应RPCA预处理（A-RPCA），用于缓解时域CSI波动以提升基于CSI的物理层认证性能，并结合极化码的高斯近似用于短码长的Slepian-Wolf信息纠正；仿真与真实数据中相较若干基线方法表现更优。


<details>
  <summary>Details</summary>
Motivation: CSI在时域上存在变化与不一致，直接用于物理层认证会降低鉴别性能；需要一种鲁棒且自适应的预处理，减小时域噪声/干扰影响，从而提升后续认证与纠错的效果。

Method: 基于鲁棒主成分分析（RPCA）提出自适应RPCA（A-RPCA）预处理以分离低秩通道结构与稀疏扰动；将处理后CSI用于基于信息纠正的PLA框架，采用极化码的高斯近似（GA）设计短码长Slepian-Wolf译码器；并对A-RPCA的方法学进行分析与对比实验。

Result: A-RPCA在仿真与真实数据集上，比无预处理/无纠错基线显著降低纠错后误码率，并在LOS与NLOS场景中均显著提升检测概率（论文声称达到1）；在PCA、RPCA、自编码器和ReProCS等方案上也显示更好性能。

Conclusion: A-RPCA为CSI-PLA提供了一种有效的时域预处理手段，配合短码长Slepian-Wolf译码能显著改善认证可靠性，且在多种数据集上优于现有方法。

Abstract: This paper introduces an adaptive preprocessing technique to enhance the accuracy of channel state information-based physical layer authentication (CSI-PLA) alleviating CSI variations and inconsistencies in the time domain. To this end, we develop an adaptive robust principal component analysis (A-RPCA) preprocessing method based on robust principal component analysis (RPCA). The performance evaluation is then conducted using a PLA framework based on information reconciliation, in which Gaussian approximation (GA) for Polar codes is leveraged for the design of short codelength Slepian Wolf decoders. Furthermore, an analysis of the proposed A-RPCA methods is carried out. Simulation results show that compared to a baseline scheme without preprocessing and without reconciliation, the proposed A-RPCA method substantially reduces the error probability after reconciliation and also substantially increases the detection probabilities that is also 1 in both line-of-sight (LOS) and non-line-of-sight (NLOS) scenarios. We have compared against state-of the-art preprocessing schemes in both synthetic and real datasets, including principal component analysis (PCA) and robust PCA, autoencoders and the recursive projected compressive sensing (ReProCS) framework and we have validated the superior performance of the proposed approach.

</details>


### [50] [Misspecified Crame-Rao Bound for AoA Estimation at a ULA under a Spoofing Attack](https://arxiv.org/abs/2512.16735)
*Sotiris Skaperas,Arsenia Chorti*

Main category: eess.SP

TL;DR: 论文提出使用误设Cramér--Rao下界（MCRB）分析主动攻击对基于到达角（AoA）物理层认证的影响，得到AoA估计MCRB的闭式表达，发现攻击引入了与SNR无关的惩罚项，取决于攻击者位置、阵列几何和预编码向量。


<details>
  <summary>Details</summary>
Motivation: 量化主动欺骗攻击对基于位置的物理层认证中AoA估计精度的损害，提供一个理论下界以评估和设计抗攻击的认证机制。

Method: 在确定性导频信号假设下，构建单天线合法用户、验证者为M阵元均匀线阵（ULA）、攻击者拥有任意L天线的系统模型；将攻击视为导致的模型偏差，利用误设CRB理论推导AoA估计的MCRB并给出闭式解，分析该界中新增惩罚项的参数依赖性。

Result: 得到MCRB的闭式表达式，证明了相对于经典CRB存在一个额外惩罚项；该惩罚项独立于SNR，但与攻击者位置、阵列几何和攻击者的预编码向量有关；在无攻击或特殊条件下退化为经典CRB。

Conclusion: MCRB能够刻画主动欺骗对AoA估计的根本性性能损失，为物理层认证的鲁棒性评估和阵列/预编码设计提供理论依据；建议在更实际情形（随机导频、多天线用户、信道衰落等）下扩展分析。

Abstract: A framework is presented for analyzing the impact of active attacks to location-based physical layer authentication (PLA) using the machinery of misspecified Cramér--Rao bound (MCRB). In this work, we focus on the MCRB in the angle-of-arrival (AoA) based authentication of a single antenna user when the verifier posseses an $M$ antenna element uniform linear array (ULA), assuming deterministic pilot signals; in our system model the presence of a spoofing adversary with an arbitrary number $L$ of antenna elements is assumed. We obtain a closed-form expression for the MCRB and demonstrate that the attack introduces in it a penalty term compared to the classic CRB, which does not depend on the signal-to-noise ratio (SNR) but on the adversary's location, the array geometry and the attacker precoding vector.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [51] [Acoustic RIS for Massive Spatial Multiplexing: Unleashing Degrees of Freedom and Capacity in Underwater Communications](https://arxiv.org/abs/2512.16470)
*Longfei Zhao,Jingbo Tan,Jintao Wang,Ian F Akyildiz,Zhi Sun*

Main category: cs.NI

TL;DR: 本文提出在水下声学通信中引入声学可重构智能表面（aRIS），通过主动生成正交可区分的虚拟路径来增强空间自由度（DoF）和信道容量。建立了海洋特定的DoF-信道耦合模型，解析给出空间秩提升条件，并推导出使单个aRIS最大化DoF的最优几何位置“光点”。在深海和浅海分别可分别引入两条和三条可分辨路径；提出了支持主动发射与反射的ASTAR aRIS架构及结合UUV与声强梯度感知的自适应波束跟踪机制；仿真表明在浅海/深海场景下信道容量分别提升高达265%和170%。


<details>
  <summary>Details</summary>
Motivation: 传统水下声学MIMO受限于带宽窄、传播损耗大和稀疏多径，且阵列分辨率有限导致角度模糊和空间DoF不足。需要新的手段生成可区分路径以提升空间DoF与容量。

Method: 提出将aRIS部署在海洋中以主动生成虚拟路径，建立海洋场景下的DoF-信道耦合模型并解析空间秩提升条件；解析推导出最优部署几何位置“光点”；设计ASTAR aRIS硬件架构支持同时发射与反射并实现独立波束控制；提出基于UUV移动与声强梯度传感的自适应波束跟踪机制；通过仿真实验评估联合部署与波束形成策略的性能。

Result: 理论及仿真结果表明：在浅海环境单个aRIS可额外引入三条可分辨路径，在深海为两条；在最优“光点”部署可最大化空间秩；ASTAR与自适应跟踪机制使得系统在浅海/深海场景下的信道容量分别提升最高约265%和170%。

Conclusion: aRIS能显著增强水下声学MIMO系统的空间DoF与信道容量；通过解析模型与最优部署（光点）指导，结合ASTAR硬件与UUV感知辅助的自适应波束跟踪，可在实际海洋环境中实现显著性能提升。

Abstract: Underwater acoustic (UWA) communications are essential for high-speed marine data transmission but remain severely constrained by limited bandwidth, significant propagation loss, and sparse multipath structures. Conventional underwater acoustic multiple-input multiple-output (MIMO) systems primarily utilize spatial diversity but suffer from limited array resolution, causing angular ambiguity and insufficient spatial degrees of freedom (DoFs). This paper addresses these limitations through acoustic Reconfigurable Intelligent Surfaces (aRIS) to actively generate orthogonally distinguishable virtual paths, significantly enhancing spatial DoFs and channel capacity. An ocean-specific DoF-channel coupling model is established, explicitly deriving conditions for spatial rank enhancement. Subsequently, the optimal geometric locus, termed the Light-Point, is analytically identified, where deploying a single aRIS maximizes DoFs by introducing two and three additional resolvable paths in deep-sea and shallow-sea environments, respectively. Furthermore, an active simultaneous transmitting and reflecting (ASTAR) aRIS architecture with independent beam control and adaptive beam-tracking mechanism integrating unmanned underwater vehicles (UUVs) and acoustic intensity gradient sensing is proposed. Extensive simulations validate the proposed joint aRIS deployment and beamforming framework, demonstrating substantial UWA channel capacity improvements-up to 265% and 170% in shallow-sea and deep-sea scenarios, respectively.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Hybrid Quantum-Classical Ensemble Learning for S\&P 500 Directional Prediction](https://arxiv.org/abs/2512.15738)
*Abraham Itzhak Weinberg*

Main category: cs.LG

TL;DR: 本文提出了一个混合集成框架，结合量子情感分析、Decision Transformer 和智能模型选择，在S&P 500方向性预测上达到60.14%的准确率，比单模型提升约3.10%。


<details>
  <summary>Details</summary>
Motivation: 金融市场预测噪声大、非平稳且效率高，单一模型难以突破约55–57%的准确率，需要通过架构多样性、情感增强和筛选弱模型来提升预测性能。

Method: 构建由LSTM、Decision Transformer、XGBoost、随机森林、逻辑回归等多种模型组成的集成；在情感分析中使用4量子比特可变量子电路增强特征；通过剔除准确率低于52%的模型并只保留Top-7模型形成最终集成；在2020–2023年多资产数据上评估并用McNemar检验显著性。

Result: 在S&P 500方向性预测上集成模型达60.14%准确率，优于单模型和同架构多数据训练（60.14% vs 52.80%）；量子情感分析为每个模型带来0.8%–1.5%的提升；Top-7模型集成优于全体35模型（60.14% vs 51.2%）；置信度过滤下回测Sharpe比1.2，高于买入持有的0.8。

Conclusion: 通过架构多样性、量子情感增强与智能筛选，可以显著改善方向性预测精度并在初步回测中展现可交易价值，但需要进一步验证鲁棒性、交易成本影响和更严格的稳健性检验。

Abstract: Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\% directional accuracy on S\&P 500 prediction, a 3.10\% improvement over individual models.
  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\% vs.\ 52.80\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\% to +1.5\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\%$), improving ensemble performance (Top-7 models: 60.14\% vs.\ all 35 models: 51.2\%).
  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.

</details>


### [53] [How Do Graph Signals Affect Recommendation: Unveiling the Mystery of Low and High-Frequency Graph Signals](https://arxiv.org/abs/2512.15744)
*Feng Liu,Hao Cang,Huanhuan Yuan,Jiaqing Fan,Yongjing Hao,Fuzhen Zhuang,Guanfeng Liu,Pengpeng Zhao*

Main category: cs.LG

TL;DR: 本文研究了图信号频率（低频与高频）在推荐任务中的作用，理论证明二者在平滑用户-物品相似性方面等效。提出了频率信号缩放器（可插拔模块）用于调节滤波器的平滑程度，并提出空间翻转方法恢复图嵌入的表达能力。实验在四个公开数据集上验证了方法有效性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有光谱GNN在推荐中常被认为依赖低通滤波，但近期工作指出高频信号也重要；低频与高频信号在推荐中的真实作用尚不清晰，且图嵌入方法可能无法完整表达图信号特征。

Method: 理论分析表明低频与高频信号对推荐的影响等价，二者都通过平滑用户-物品相似性起作用。基于此设计“频率信号缩放器”作为可插拔模块，调节图信号滤波函数以控制平滑强度；进一步证明图嵌入方法存在表达缺陷，提出“空间翻转”技术恢复嵌入的表达能力。

Result: 给出理论证明与实验验证：在四个公共数据集上，使用频率信号缩放器和空间翻转能提升推荐性能；还验证了仅使用低频或仅使用高频即可获得良好效果。代码已发布于作者仓库。

Conclusion: 通过调节图信号的频率与空间结构，可有效控制用户-物品相似性的平滑性，从而提升基于GNN的推荐系统性能；此外，图嵌入需要额外处理（如空间翻转）以恢复完整的信号表达能力。

Abstract: Spectral graph neural networks (GNNs) are highly effective in modeling graph signals, with their success in recommendation often attributed to low-pass filtering. However, recent studies highlight the importance of high-frequency signals. The role of low-frequency and high-frequency graph signals in recommendation remains unclear. This paper aims to bridge this gap by investigating the influence of graph signals on recommendation performance. We theoretically prove that the effects of low-frequency and high-frequency graph signals are equivalent in recommendation tasks, as both contribute by smoothing the similarities between user-item pairs. To leverage this insight, we propose a frequency signal scaler, a plug-and-play module that adjusts the graph signal filter function to fine-tune the smoothness between user-item pairs, making it compatible with any GNN model. Additionally, we identify and prove that graph embedding-based methods cannot fully capture the characteristics of graph signals. To address this limitation, a space flip method is introduced to restore the expressive power of graph embeddings. Remarkably, we demonstrate that either low-frequency or high-frequency graph signals alone are sufficient for effective recommendations. Extensive experiments on four public datasets validate the effectiveness of our proposed methods. Code is avaliable at https://github.com/mojosey/SimGCF.

</details>


### [54] [LLaDA2.0: Scaling Up Diffusion Language Models to 100B](https://arxiv.org/abs/2512.15745)
*Tiwei Bie,Maosong Cao,Kun Chen,Lun Du,Mingliang Gong,Zhuochen Gong,Yanmei Gu,Jiaqi Hu,Zenan Huang,Zhenzhong Lan,Chengxi Li,Chongxuan Li,Jianguo Li,Zehuan Li,Huabin Liu,Ling Liu,Guoshan Lu,Xiaocheng Lu,Yuxin Ma,Jianfeng Tan,Lanning Wei,Ji-Rong Wen,Yipeng Xing,Xiaolu Zhang,Junbo Zhao,Da Zheng,Jun Zhou,Junlin Zhou,Zhanchao Zhou,Liwang Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: Proposes LLaDA2.0: convert autoregressive LLMs into discrete diffusion LLMs (dLLM) up to 100B via a 3-phase block-level WSD training (warm-up, stable, decay) and post-training alignment (SFT, DPO); yields instruction-tuned MoE models LLaDA2.0-mini (16B) and flash (100B) with parallel decoding advantages.


<details>
  <summary>Details</summary>
Motivation: Avoid costly from-scratch training of diffusion LLMs at frontier scale by reusing pre-trained AR models, keeping knowledge, improving efficiency and enabling practical deployment.

Method: Introduce block-level WSD training: progressively increase block size (warm-up), run full-sequence diffusion at scale (stable), then revert to compact blocks (decay). Convert AR checkpoints to dLLM and apply post-training alignment (SFT and DPO). Build MoE instruction-tuned variants and leverage parallel decoding.

Result: Successfully scaled dLLM up to 100B parameters, producing two open-source instruction-tuned MoE models (16B and 100B) claiming superior efficiency and performance through parallel decoding and conversion strategy.

Conclusion: A practical paradigm to obtain frontier-scale diffusion LLMs cheaply by systematic conversion from AR models with a 3-phase block-level training schedule plus alignment; shows feasibility of large-scale dLLMs for deployment.

Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.

</details>


### [55] [A Unified Generative-Predictive Framework for Deterministic Inverse Design](https://arxiv.org/abs/2512.15746)
*Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: 提出 Janus：一种将生成与预测统一在同一隐空间的框架，用于解决异质材料微观结构的逆设计问题。通过编码器-解码器与可分离的预测头（KHRONOS）耦合，训练出既适合确定性反演又保留物理预测能力的隐流形。MNIST 与热导率标注微观结构上验证，获得高保真重建、良好预测（R^2=0.98）和满足目标物性（1% 误差）的快速反演。


<details>
  <summary>Details</summary>
Motivation: 逆向设计高维异质材料微观结构是病态且计算代价高的问题。现有生成模型虽能模拟复杂前向行为，但缺乏对确定性、物理感知反演的内在支持。需要一种能同时兼顾快速反演与物理先验的统一表征。

Method: 提出 Janus：一个深度编码器-解码器与可分离的 KHRONOS 预测头联合训练的框架。训练目标使隐空间在拓扑上对生成反演保持等距，同时对物性预测进行剪枝，从而实现隐变量的‘解耦’。通过联合生成与预测损失促进隐表示对两类任务同时友好。

Result: 在 MNIST 上展示了高保真重建、准确分类与面向十类目标的多样化生成反演。在微观结构热导率数据集上，前向预测 R^2=0.98（约 2% 相对误差），像素级重建误差 <5%，反演解满足目标物性到 1% 相对误差内。属性扫掠显示隐流形平滑可遍历，UMAP 可视化显示低维解耦流形结构。

Conclusion: Janus 通过在单一隐空间中统一生成与预测，实现了实时、物理感知的微结构逆设计，计算成本低于传统基于优化的方法，同时产生了可解释且解耦的低维隐流形。

Abstract: Inverse design of heterogeneous material microstructures is a fundamentally ill-posed and famously computationally expensive problem. This is exacerbated by the high-dimensional design spaces associated with finely resolved images, multimodal input property streams, and a highly nonlinear forward physics. Whilst modern generative models excel at accurately modeling such complex forward behavior, most of them are not intrinsically structured to support fast, stable \emph{deterministic} inversion with a physics-informed bias. This work introduces Janus, a unified generative-predictive framework to address this problem. Janus couples a deep encoder-decoder architecture with a predictive KHRONOS head, a separable neural architecture. Topologically speaking, Janus learns a latent manifold simultaneously isometric for generative inversion and pruned for physical prediction; the joint objective inducing \emph{disentanglement} of the latent space. Janus is first validated on the MNIST dataset, demonstrating high-fidelity reconstruction, accurate classification and diverse generative inversion of all ten target classes. It is then applied to the inverse design of heterogeneous microstructures labeled with thermal conductivity. It achieves a forward prediction accuracy $R^2=0.98$ (2\% relative error) and sub-5\% pixelwise reconstruction error. Inverse solutions satisfy target properties to within $1\%$ relative error. Inverting a sweep through properties reveal smooth traversal of the latent manifold, and UMAP visualization confirms the emergence of a low-dimensional, disentangled manifold. By unifying prediction and generation within a single latent space, Janus enables real-time, physics-informed inverse microstructure generation at a lower computational cost typically associated with classical optimization-based approaches.

</details>


### [56] [D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models](https://arxiv.org/abs/2512.15747)
*Javon Hickmon*

Main category: cs.LG

TL;DR: 提出D3G：一种训练自由的零样本方法，利用Stable Diffusion XL在推理阶段生成具有多样人口属性的图像，结合CLIP以提高分类准确率并降低人口学偏差。


<details>
  <summary>Details</summary>
Motivation: 低容量模型在细粒度分类上易欠拟合，且数据集人口分布不平衡会导致预测偏向过度表示的群体。需要无需再训练就能提升跨模态表示质量和减少有害偏差的方案。

Method: 以CLIP为基础模型，使用Stable Diffusion XL在推理时为每个类别生成多元人口学变体样本（例如不同性别、年龄、肤色等），将这些生成样本的跨模态相似性整合进零样本决策流程，从而改善语义覆盖和类内多样性。该方法不改变模型参数，属于训练自由的零样本增强。

Result: 在论文中作者展示了通过在推理时提供多样人口学数据可以提升CLIP的分类性能并降低人口学偏差。同时分析了单独人口学因子对最终准确率的影响。

Conclusion: D3G证明了生成模型可作为零样本公平性与性能增强的有效工具，为在不重新训练大模型的情况下缓解数据偏差提供了一条实用路径。进一步工作可关注生成质量、评估指标和潜在的伦理风险。

Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.

</details>


### [57] [Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?](https://arxiv.org/abs/2512.15748)
*Tian Liu,Anwesha Basu,James Caverlee,Shu Kong*

Main category: cs.LG

TL;DR: 在物种视觉识别任务中，少样本训练的“专家”模型整体优于大多模态大模型（LMM），但LMM能在事后纠正专家模型的错误。基于此，作者提出Post-hoc Correction（POC）：将专家模型的top-k预测、softmax置信度和少样本视觉示例作为提示输入LMM，让LMM对top-k重新排序。该方法无需额外训练或验证，在五个VSR基准上平均提高约+6.4%的准确率，并能通用于不同预训练骨干与LMM。


<details>
  <summary>Details</summary>
Motivation: 物种识别标注成本高、每类样本少，实用的解决方案需要在少样本条件下达到高精度。近年LMM在通用识别能力上表现优异，值得探究其在专业VSR任务中是否能替代或补强少样本专家模型。

Method: 首先比较LMM与常规模型在VSR少样本设置下的表现，发现LMM表现不佳。但在错误分析中发现：当将专家模型的top-k预测作为候选并以合适提示（包含softmax分数与少样本图像示例）输入LMM时，LMM能显著提升最终决策。据此设计POC：对每个测试图像，取专家模型的top-k预测和对应置信度，构造富含视觉示例的提示，调用LMM对候选进行重排序，输出最终预测。无需额外训练或人工干预。

Result: 在五个具有挑战性的VSR基准上，POC比先前的少样本方法平均提升约+6.4%准确率；并在不同预训练视觉骨干和不同LMM上都保持增益，体现出良好的泛化性和可插拔性。

Conclusion: 将专家模型与LMM的互补能力结合，通过事后提示式重排序可以显著提升少样本物种识别性能，是一种无需再训练即可增强现有FSL方法的实用策略。

Abstract: Visual Species Recognition (VSR) is pivotal to biodiversity assessment and conservation, evolution research, and ecology and ecosystem management. Training a machine-learned model for VSR typically requires vast amounts of annotated images. Yet, species-level annotation demands domain expertise, making it realistic for domain experts to annotate only a few examples. These limited labeled data motivate training an ''expert'' model via few-shot learning (FSL). Meanwhile, advanced Large Multimodal Models (LMMs) have demonstrated prominent performance on general recognition tasks. It is straightforward to ask whether LMMs excel in the highly specialized VSR task and whether they outshine FSL expert models. Somewhat surprisingly, we find that LMMs struggle in this task, despite using various established prompting techniques. LMMs even significantly underperform FSL expert models, which are as simple as finetuning a pretrained visual encoder on the few-shot images. However, our in-depth analysis reveals that LMMs can effectively post-hoc correct the expert models' incorrect predictions. Briefly, given a test image, when prompted with the top predictions from an FSL expert model, LMMs can recover the ground-truth label. Building on this insight, we derive a simple method called Post-hoc Correction (POC), which prompts an LMM to re-rank the expert model's top predictions using enriched prompts that include softmax confidence scores and few-shot visual examples. Across five challenging VSR benchmarks, POC outperforms prior art of FSL by +6.4% in accuracy without extra training, validation, or manual intervention. Importantly, POC generalizes to different pretrained backbones and LMMs, serving as a plug-and-play module to significantly enhance existing FSL methods.

</details>


### [58] [KAN-Matrix: Visualizing Nonlinear Pairwise and Multivariate Contributions for Physical Insight](https://arxiv.org/abs/2512.15755)
*Luis A. De la Fuente,Hernan A. Moreno,Laura V. Alvarez,Hoshin V. Gupta*

Main category: cs.LG

TL;DR: 本文提出将Kolmogorov-Arnold Networks(KANs)用于解释性分析，给出两种可视化矩阵：PKAN（刻画变量间非线性关联）和MKAN（多元特征贡献排序），并声明较Pearson与互信息更鲁棒、有助于发现物理模式。


<details>
  <summary>Details</summary>
Motivation: 高维与变量共线性使复杂数据解释困难，传统相关分析（如Pearson）无法捕捉非线性或功能形态，需更具可解释性与简洁性的工具支持特征选择与模型解释。

Method: 利用KANs学习输入到输出的潜在非线性映射，构建Pairwise KAN Matrix(PKAN)评估两变量间的非线性关系，并用Multivariate KAN Contribution Matrix(MKAN)量化每个输入对目标的相对贡献，配以色码可视化。通过与Pearson相关和互信息方法比较，评估矩阵在识别关系强度与函数形态上的能力。

Result: 作者报告在实验比较中PKAN与MKAN比Pearson Correlation与Mutual Information更稳健且信息量更大，能够同时揭示关系强度与函数形式，有助于特征选择、冗余分析、模型解释及物理洞见发现。

Conclusion: KAN驱动的PKAN与MKAN为数据预处理与后处理提供了有力工具，通过捕捉非线性强度与形式，促进领域知识驱动的模型开发与隐藏物理模式的发现。

Abstract: Interpreting complex datasets remains a major challenge for scientists, particularly due to high dimensionality and collinearity among variables. We introduce a novel application of Kolmogorov-Arnold Networks (KANs) to enhance interpretability and parsimony beyond what traditional correlation analyses offer. We present two interpretable, color-coded visualization tools: the Pairwise KAN Matrix (PKAN) and the Multivariate KAN Contribution Matrix (MKAN). PKAN characterizes nonlinear associations between pairs of variables, while MKAN serves as a nonlinear feature-ranking tool that quantifies the relative contributions of inputs in predicting a target variable. These tools support pre-processing (e.g., feature selection, redundancy analysis) and post-processing (e.g., model explanation, physical insights) in model development workflows. Through experimental comparisons, we demonstrate that PKAN and MKAN yield more robust and informative results than Pearson Correlation and Mutual Information. By capturing the strength and functional forms of relationships, these matrices facilitate the discovery of hidden physical patterns and promote domain-informed model development.

</details>


### [59] [ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning](https://arxiv.org/abs/2512.15756)
*Yoonpyo Lee*

Main category: cs.LG

TL;DR: 提出ReactorFold，将燃料组件设计表述为语言模型的序列建模问题，使用蒙特卡洛数据、参数高效微调和直接偏好优化(DPO)生成布局，模型在训练数据限制下表现出自发扩展设计空间和发现非常规非对称配置的能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法受限于固定、人工定义的配置空间，难以探索根本不同的新拓扑；作者希望用生成模型打破这些限制，自动化发现高性能、可能出乎直觉的设计。

Method: 用蒙特卡洛模拟生成组件样本；对大规模语言模型进行参数高效微调；用DPO对模型进行对齐以捕捉偏好（比如功率峰值约束）；模型一次前向传递生成候选布局。

Result: DPO对齐的模型在严格功率峰值约束下自动调整镓(Gd)库存（尽管训练样本中Gd数量固定），并发现高性能的非对称配置，进入传统方法难以到达的设计空间。

Conclusion: 语言模型可以内化并利用物理因果关系，超越人工施加的设计约束，提供一种高效生成核燃料组件新拓扑的途径。

Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.

</details>


### [60] [Twin Restricted Kernel Machines for Multiview Classification](https://arxiv.org/abs/2512.15757)
*A. Quadir,M. Sajid,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种名为TMvRKM的多视图双重受限核机，通过正则化最小二乘替代大规模二次规划，加入耦合项平衡多视图误差，并结合早期与晚期融合，声称在UCI、KEEL、AwA基准上优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前多视图学习中，基于SVM的模型在高维核空间中构造决策边界时计算量大且对视图不一致敏感；需要一种既高效又能改善泛化性能的核方法。

Method: 提出TMvRKM——将核机与多视图框架结合，采用正则化最小二乘解代替传统QPP求解，原始目标包含用于平衡各视图误差的耦合项；训练时融合早期与晚期信息以兼顾整体与视图特异性。

Result: 在UCI、KEEL和AwA数据集上进行实验，结果与统计检验表明TMvRKM在所有场景中均优于基线模型，泛化性能显著提升。

Conclusion: TMvRKM在计算效率和分类性能上对传统基于核的多视图方法提供了改进，且对视图不一致有一定鲁棒性；建议进一步验证可扩展性、理论保证和对比更多现代多视图方法的性能。

Abstract: Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.

</details>


### [61] [Yantra AI -- An intelligence platform which interacts with manufacturing operations](https://arxiv.org/abs/2512.15758)
*Varshini Krishnamurthy*

Main category: cs.LG

TL;DR: 本文提出面向XRIT的智能生产系统，结合随机森林、Isolation Forest、Streamlit可视化与GPT‑4虚拟助手，以改善能耗管理、预测性维护与决策支持；用合成数据进行了可扩展性测试并报告了性能提升趋势。


<details>
  <summary>Details</summary>
Motivation: 受工业4.0推动，生产系统需要实时跟踪、预测性维护和AI决策支持以降低停机时间、优化能耗并提高生产效率，针对XRIT的具体需求设计一套智能化解决方案。

Method: 构建数据采集与预处理管道；使用Random Forest Classifier进行预测性维护，Isolation Forest用于异常检测；采用Streamlit构建实时交互式仪表盘并展示监控指标；集成基于GPT‑4的虚拟助手用于自然语言查询与决策辅助；在合成数据上验证系统可扩展性和功能。

Result: 在合成数据测试下，系统在工作效率、能耗管理和维修计划方面显示出改进（作者陈述的性能提升），同时系统证明了可扩展部署的可行性，并通过仪表盘和虚拟助理提升了用户交互体验。

Conclusion: 该系统是一套端到端的工业智能原型，证明了把传统生产监控与现代机器学习、可视化和大语言模型结合的可行性。下一步应着重将系统对接真实生产数据、展开严格的定量评估并改进模型与部署细节，以满足实际工业环境需求。

Abstract: Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.

</details>


### [62] [BUILD with Precision: Bottom-Up Inference of Linear DAGs](https://arxiv.org/abs/2512.16111)
*Hamed Ajorlou,Samuel Rey,Gonzalo Mateos,Geert Leus,Antonio G. Marques*

Main category: cs.LG

TL;DR: 提出了BUILD算法：在线性高斯SEM且噪声方差相等的条件下，利用观测精度矩阵（precision matrix）的特殊结构，从下而上识别叶节点及其父节点并逐步修剪，从真实精度矩阵能精确重构DAG；在有限样本下通过周期性重估精度矩阵来缓解误差累积。实验证明在合成基准上可与最先进方法竞争，并可显式控制复杂度与运行时的权衡。


<details>
  <summary>Details</summary>
Motivation: 在等噪声方差的线性高斯SEM下DAG可被识别，作者观察到观测精度矩阵存在可供利用的特殊结构，因此希望据此设计一个确定性、逐步的图结构恢复方法，既可精确重构又便于控制复杂度和稳健性。

Method: 提出BUILD（Bottom-Up Inference of Linear DAGs）算法：从精度矩阵出发以确定性规则识别叶节点及其父集；将识别出的叶及其连接边“修剪”后在剩余变量上重复该过程；在有限样本场景下引入了周期性重估精度矩阵的策略以缓和由病态条件数和估计误差带来的累积错误，代价是更高的运行时间。

Result: 理论上：使用真实精度矩阵时，BUILD能精确重构原始DAG；实证上：在挑战性的合成数据基准上，BUILD在性能上可与现有最先进的DAG学习算法媲美，并且提供关于算法复杂度与鲁棒性的明确调节手段。

Conclusion: BUILD为在等方差线性高斯假设下的可识别DAG学习提供了一种高可解释性、可控复杂度且在样本估计上有鲁棒性机制的实用方案；但其效能依赖于模型假设与精度矩阵估计质量。

Abstract: Learning the structure of directed acyclic graphs (DAGs) from observational data is a central problem in causal discovery, statistical signal processing, and machine learning. Under a linear Gaussian structural equation model (SEM) with equal noise variances, the problem is identifiable and we show that the ensemble precision matrix of the observations exhibits a distinctive structure that facilitates DAG recovery. Exploiting this property, we propose BUILD (Bottom-Up Inference of Linear DAGs), a deterministic stepwise algorithm that identifies leaf nodes and their parents, then prunes the leaves by removing incident edges to proceed to the next step, exactly reconstructing the DAG from the true precision matrix. In practice, precision matrices must be estimated from finite data, and ill-conditioning may lead to error accumulation across BUILD steps. As a mitigation strategy, we periodically re-estimate the precision matrix (with less variables as leaves are pruned), trading off runtime for enhanced robustness. Reproducible results on challenging synthetic benchmarks demonstrate that BUILD compares favorably to state-of-the-art DAG learning algorithms, while offering an explicit handle on complexity.

</details>


### [63] [Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning](https://arxiv.org/abs/2512.15759)
*Jahidul Arafat*

Main category: cs.LG

TL;DR: 提出了带语义约束的联邦聚合框架SCFA，通过将领域知识作为优化约束减小数据异构性并改善隐私-效用权衡，给出O(1/√T + ρ)收敛率证明；在Bosch维修预测上验证了更快收敛、降低模型漂移和更好的差分隐私表现。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习在非IID情形下收敛慢且把所有客户端更新一视同仁，忽视了语义/领域有效性约束，导致性能和隐私-效用受损。

Method: 提出在分布式优化中引入领域知识约束的SCFA框架，形式化约束违例率ρ并在理论上分析其对收敛率和有效数据异构性的影响；同时结合差分隐私机制评估隐私-效用关系。

Result: 理论上证明收敛率为O(1/√T + ρ)，并量化约束使有效异构性降低41%、假设空间缩小因子θ=0.37。在ε=10的差分隐私下，SCFA将效用降幅控制在3.7%（标准联邦学习为12.1%）。实证在Bosch数据上显示22%加速收敛、41.3%漂移减少；ρ<0.05时维持90%最优性能，ρ>0.18导致失败；理论与实验拟合R^2>0.90。

Conclusion: 将显式领域知识作为约束引入联邦聚合可以同时改善收敛性、鲁棒性和隐私-效用权衡，且理论与实证结果一致。

Abstract: Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.

</details>


### [64] [Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario](https://arxiv.org/abs/2512.16648)
*Liu Yang,Qiang Li,Luxiong Wen,Jian Yang*

Main category: cs.LG

TL;DR: 提出并解决源端无数据的跨接收机RFFI自适应问题（SCRFFI），通过约束伪标签框架与MS-SHOT方法在目标域无标签且可能存在标签分布变化的情形下，实现稳健适配并显著提升识别精度。


<details>
  <summary>Details</summary>
Motivation: 边缘计算与RFFI的普及要求可信的设备认证，但不同接收机硬件引入分布偏移，导致以往深度模型跨接收机部署表现下降；在实际场景中源域数据常不可用（隐私或传输限制），因此需要源端无数据的跨接收机适配方法。

Method: 构建约束伪标签的源端无数据适配框架，并从理论上分析其泛化性，指出目标域性能对伪标签质量高度敏感。在此基础上提出MS-SHOT：结合动量中心（momentum-center）引导的软伪标签与全局结构约束，促进预测既自信又多样，能处理目标域标签偏移及未知非均匀类分布。

Result: 在多组真实数据集和跨接收机迁移任务上广泛实验，MS-SHOT在准确率与鲁棒性上持续优于现有方法，尤其在目标域存在标签分布变化时优势明显。

Conclusion: MS-SHOT为源端无数据的跨接收机RFFI适配提供了实用、可扩展的解决方案；理论与实证均表明提升伪标签质量与施加全局结构约束是提高目标域性能的关键。

Abstract: With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.

</details>


### [65] [Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps](https://arxiv.org/abs/2512.15761)
*Christopher Blum,Michael Neidlin*

Main category: cs.LG

TL;DR: 提出了一个基于CFD流场特征的可解释机器学习框架（基于逻辑回归与结构化特征选择）用于旋转血泵内局部血栓风险评估，能从宏观尺度血栓模型标注的空间风险中学习，计算高效且具物理可解释性，可用于快速筛查和辅助器械设计。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型难以将复杂流动特征可靠且可解释地映射为局部血栓风险，且昂贵的物理仿真限制了在设计流程中的快速筛查。需要一个既能保持机制透明又能高效评估局部血栓危险性的工具。

Method: 从CFD提取局部流场特征，构建结构化特征选择管线并结合逻辑回归模型（包括非线性特征组合），以宏观尺度、已验证血栓模型的空间风险标注作为训练目标。训练在两种代表性场景上进行，并将训练得到的模型应用到不同类型泵中进行验证。

Result: 模型能重现标注的风险分布，识别与风险增加相关的不同流场特征集；在离心泵上推广时也能预测出合理的血栓易发区域，即使仅用单一轴流泵运行工况训练。计算成本低，适合快速筛查。

Conclusion: 可解释机器学习可作为对基于物理的血栓建模的有力补充，在保证可解释性和机制透明性的同时显著降低计算开销，为CFD驱动的血栓分析与装置设计工作流提供方法学基础。

Abstract: Thrombosis in rotary blood pumps arises from complex flow conditions that remain difficult to translate into reliable and interpretable risk predictions using existing computational models. This limitation reflects an incomplete understanding of how specific flow features contribute to thrombus initiation and growth. This study introduces an interpretable machine learning framework for spatial thrombosis assessment based directly on computational fluid dynamics-derived flow features. A logistic regression (LR) model combined with a structured feature-selection pipeline is used to derive a compact and physically interpretable feature set, including nonlinear feature combinations. The framework is trained using spatial risk patterns from a validated, macro-scale thrombosis model for two representative scenarios. The model reproduces the labeled risk distributions and identifies distinct sets of flow features associated with increased thrombosis risk. When applied to a centrifugal pump, despite training on a single axial pump operating point, the model predicts plausible thrombosis-prone regions. These results show that interpretable machine learning can link local flow features to thrombosis risk while remaining computationally efficient and mechanistically transparent. The low computational cost enables rapid thrombogenicity screening without repeated or costly simulations. The proposed framework complements physics-based thrombosis modeling and provides a methodological basis for integrating interpretable machine learning into CFD-driven thrombosis analysis and device design workflows.

</details>


### [66] [Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction](https://arxiv.org/abs/2512.15762)
*Kanxue Li,Yibing Zhan,Hua Jin,Chongchong Qi,Xu Lin,Baosheng Yu*

Main category: cs.LG

TL;DR: 提出了CSA-TTA：一种跨样本增强的测试时适应框架，通过从其他病人引入低血压样本来缓解IOH事件稀少导致的测试时训练不可靠问题；结合K-Shape聚类和基于相似性的top-K检索，并融合掩码自监督重建与回顾性序列预测信号，能在VitalDB和真实住院数据上稳定提升Recall和F1，尤其在零样本场景提升显著。


<details>
  <summary>Details</summary>
Motivation: 术中低血压（IOH）对手术风险大，但预测困难，原因在于个体间高变异性和IOH事件稀少导致的测试时训练不稳定。标准的测试时适应（TTA）受限于罕见正例，需引入跨个体信息以提升个性化预测可靠性。

Method: 提出CSA-TTA：先将历史时序按低血压与非低血压分片构建跨样本库；采用粗到细检索（先用K-Shape聚类得到代表性簇心，再根据当前患者信号检索top-K语义相似样本）构建测试时训练集；训练时同时使用掩码重构的自监督信号和回顾性序列预测信号，以增强模型对快速、微妙术中动态的适应性；在TimesFM和UniTS等时间序列预测模型上进行集成评估。

Result: 在VitalDB和真实住院数据集上均有一致提升。示例：在VitalDB上，微调时Recall和F1分别提升+1.33%与+1.13%，零样本情形下分别提升+7.46%与+5.07%；与SOTA模型结合表现稳健，说明方法具备良好泛化性。

Conclusion: CSA-TTA通过跨样本增强与粗到细检索策略，解决了IOH事件稀少带来的测试时训练样本不足问题；结合双重自监督信号提高了模型对术中快速变化的适应力，显著提升了个性化IOH预测性能，尤其在零样本场景中效果明显。

Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.

</details>


### [67] [Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic](https://arxiv.org/abs/2512.15765)
*Mélissa Tamine,Otmane Sakhi,Benjamin Heymann*

Main category: cs.LG

TL;DR: 本文提出利用Direct Preference Optimization（DPO）训练的大型语言模型特有的数学结构，大幅简化计算数据的Shapley值，从而在可扩展性上克服传统需大量重训练的难题，使得数据估值在LLM场景下变得可行。


<details>
  <summary>Details</summary>
Motivation: 数据为训练LLM的重要资源，获取优质或专有数据成本高，数据所有者需要在数据采集、标注和合作共享上做出投资决策，并在合作时公平分配成果；现有通过Shapley值解决数据估值的问题，因需大量模型重训练而在规模化LLM上不可行。

Method: 作者利用DPO的特定数学结构（DPO的目标和参数更新形式）推导出能够高效计算每个数据源边际贡献的办法，避免针对每个子集重复训练模型，从而把Shapley值的计算复杂度大幅降低。

Result: 理论上和/或实证上展示了在DPO训练下计算Shapley值的可扩展算法或公式，使得在现实规模的LLM与数据所有者数量下能实际运行数据价值分配。

Conclusion: DPO为LLM数据估值提供了一个实用路径，开启了数据策划决策支持和多方合作训练中公平分配收益的多种应用可能性。

Abstract: Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.

</details>


### [68] [TENG++: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets under General Boundary Conditions](https://arxiv.org/abs/2512.15771)
*Xinjie He,Chenggong Zhang*

Main category: cs.LG

TL;DR: Extends Time-Evolving Natural Gradient (TENG) to handle Dirichlet boundary conditions in PINN-style PDE solvers by adding boundary-penalty terms and combining natural-gradient optimization with Euler/Heun time-stepping; Heun gives higher accuracy, Euler is cheaper.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical PDE solvers struggle with high-dimensional/complex problems; PINNs help but face accuracy and boundary-enforcement issues. The work aims to improve PINN training stability and accuracy under Dirichlet conditions using TENG.

Method: Integrate Dirichlet boundary-penalty terms into the loss and apply the TENG optimizer coupled with numerical time-stepping schemes (Euler and Heun). Implement natural-gradient updates that evolve over time steps, ensuring stability; compare first-order (Euler) and second-order (Heun) integration for parameter dynamics.

Result: On the heat equation benchmarks, the Heun-based TENG achieved superior accuracy due to second-order corrections, while the Euler-based TENG was computationally cheaper and suitable for simpler scenarios. Boundary conditions were enforced precisely via penalty terms.

Conclusion: The extended TENG framework effectively handles Dirichlet boundary conditions in PINN PDE solvers, balancing accuracy and efficiency. It paves the way to support Neumann/mixed conditions and broader PDE classes for practical applications.

Abstract: Partial Differential Equations (PDEs) are central to modeling complex systems across physical, biological, and engineering domains, yet traditional numerical methods often struggle with high-dimensional or complex problems. Physics-Informed Neural Networks (PINNs) have emerged as an efficient alternative by embedding physics-based constraints into deep learning frameworks, but they face challenges in achieving high accuracy and handling complex boundary conditions. In this work, we extend the Time-Evolving Natural Gradient (TENG) framework to address Dirichlet boundary conditions, integrating natural gradient optimization with numerical time-stepping schemes, including Euler and Heun methods, to ensure both stability and accuracy. By incorporating boundary condition penalty terms into the loss function, the proposed approach enables precise enforcement of Dirichlet constraints. Experiments on the heat equation demonstrate the superior accuracy of the Heun method due to its second-order corrections and the computational efficiency of the Euler method for simpler scenarios. This work establishes a foundation for extending the framework to Neumann and mixed boundary conditions, as well as broader classes of PDEs, advancing the applicability of neural network-based solvers for real-world problems.

</details>


### [69] [TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration](https://arxiv.org/abs/2512.15773)
*Ye Li,Jiahe Feng,Yuan Meng,Kangye Ji,Chen Tang,Xinwan Wen,Shutao Xia,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: 提出TS-DP，一种用于扩散策略（Diffusion Policy, DP）的带时间感知的强化学习推测解码框架，通过训练Transformer“草案器”和基于强化学习的调度器，在动态困难的体感控制任务中实现自适应、无损的加速，实验显示推理加速最高达4.17倍，接受率超过94%，可达25Hz实时控制且性能不下降。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在体感控制中效果好但推理延迟和计算量大；传统静态加速方法（如量化）不能适应任务难度随时间变化的问题，而推测解码作为一种自适应且无损的加速手段尚未在DP中充分研究。作者希望解决在时变任务难度下以更低代价匹配基模型去噪质量，以及如何动态交互式地调整计算以适应任务难度。

Method: 设计TS-DP框架：一是蒸馏一个基于Transformer的草案器（drafter），模仿原始扩散模型的去噪输出以替代昂贵的去噪调用；二是设计一个基于强化学习的调度器，根据时间变化的任务难度自适应地调整推测参数（如草案接受阈值、推测步数等），以在保证准确率的同时提高效率。整体实现推测解码并具备时间感知能力。

Result: 在多种体感环境中广泛实验，TS-DP在不降低性能的情况下实现最高4.17×的推理加速，草案接受率超过94%，推理频率可达25Hz，从而实现实时的扩散策略控制。

Conclusion: TS-DP首次在扩散策略中引入时间感知的推测解码，通过Transformer草案器与RL调度器的结合，实现了在时变环境下自适应且无损的推理加速，为实时扩散策略控制提供了有效思路和实践验证。

Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.

</details>


### [70] [Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence](https://arxiv.org/abs/2512.15780)
*Samruddhi Baviskar*

Main category: cs.LG

TL;DR: 该研究评估了面向金融决策的表格化机器学习模型在对抗样本下的鲁棒性，发现小幅扰动即可导致性能和公平性显著下降，且对抗训练能部分恢复但无法完全抵消损失。


<details>
  <summary>Details</summary>
Motivation: 金融决策（信用评分、欺诈检测）对模型可靠性、风险控制与合规性要求高，但关于表格数据模型在对抗攻击下的鲁棒性和对风险/公平性影响的研究不足。作者旨在填补这一空白，评估攻击如何改变预测性能、校准与歧视性指标，并探讨防御效果。

Method: 使用信用评分与欺诈检测数据集，针对训练好的表格模型实施基于梯度的对抗攻击（构造小幅输入扰动），度量在不同攻击强度下模型的AUC、校准误差、歧视指标和金融相关风险度量的变化；同时对比普通训练与对抗训练（adversarial training）的性能。

Result: 在小幅扰动下模型的预测性能（如AUC）和校准显著恶化，歧视性指标与金融损失相关度量均有明显上升；应用对抗训练后部分指标得到恢复，但仍存在残余脆弱性，且防御代价（如泛化性能下降）不可忽视。

Conclusion: 表格化金融模型对对抗扰动敏感，攻击不仅损害分类性能，也会影响校准与公平性；需要在金融部署中将鲁棒性评估与防御策略纳入常规审查，未来应研究更多攻击种类、验证性防御与监管测评框架。

Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.

</details>


### [71] [Boosting t-SNE Efficiency for Sequencing Data: Insights from Kernel Selection](https://arxiv.org/abs/2512.15900)
*Avais Jan,Prakash Chourasia,Sarwan Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 该论文比较了九种核函数在t-SNE用于分子序列降维时的表现，发现余弦相似性核在保邻域结构、运算效率和下游任务（分类、聚类）上普遍优于高斯核与隔离核，适用于大规模生物序列分析。


<details>
  <summary>Details</summary>
Motivation: 传统t-SNE使用高斯核计算相似度，但高斯核对序列数据缺乏依赖性且计算成本高，已有的隔离核替代方案也未必能最优捕获序列相似性。因此需要系统评估多种核函数在序列数据上的效果。

Method: 在三种序列嵌入方法（One-Hot、Spike2Vec、minimizers）上，将九种核函数（包括余弦、高斯、隔离核等）用于t-SNE降维；通过主观可视化、客观指标（如邻域保持分数）以及在六个生物数据集（Spike7k、Host、ShortRead、Rabies、Genome、Breast Cancer）上进行分类与聚类实验来评估性能。

Result: 实验表明余弦相似性核在保留高维点对关系、运行时间和下游分类/聚类任务上总体优于其他核函数，高斯核和隔离核在部分场景下表现较差。结果在多数据集、多嵌入策略和多模型下具有一致性。

Conclusion: 核函数的选择显著影响t-SNE可视化质量和下游分析结果；对于分子序列数据，余弦相似性核是更稳健且高效的选择，尤其适合大规模序列分析。

Abstract: Dimensionality reduction techniques are essential for visualizing and analyzing high-dimensional biological sequencing data. t-distributed Stochastic Neighbor Embedding (t-SNE) is widely used for this purpose, traditionally employing the Gaussian kernel to compute pairwise similarities. However, the Gaussian kernel's lack of data-dependence and computational overhead limit its scalability and effectiveness for categorical biological sequences. Recent work proposed the isolation kernel as an alternative, yet it may not optimally capture sequence similarities. In this study, we comprehensively evaluate nine different kernel functions for t-SNE applied to molecular sequences, using three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Through both subjective visualization and objective metrics (including neighborhood preservation scores), we demonstrate that the cosine similarity kernel in general outperforms other kernels, including Gaussian and isolation kernels, achieving superior runtime efficiency and better preservation of pairwise distances in low-dimensional space. We further validate our findings through extensive classification and clustering experiments across six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer), employing multiple machine learning algorithms and evaluation metrics. Our results show that kernel selection significantly impacts not only visualization quality but also downstream analytical tasks, with the cosine similarity kernel providing the most robust performance across different data types and embedding strategies, making it particularly suitable for large-scale biological sequence analysis.

</details>


### [72] [Introduction to Symbolic Regression in the Physical Sciences](https://arxiv.org/abs/2512.15920)
*Deaglan J. Bartlett,Harry Desmond,Pedro G. Ferreira,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 本文介绍了面向物理科学的符号回归（SR）专刊，回顾了SR的概念基础、方法要点与应用场景，并汇集了从自动方程发现到紧凑仿真替代模型的最新进展。


<details>
  <summary>Details</summary>
Motivation: 受2025年4月英国皇家学会讨论会启发，致力于推动在物理科学中利用SR实现可解释的数学关系发现与高效经验建模，解决复杂模拟与理论归纳的需求。

Method: 综述了SR的概念框架并与传统回归对比，讨论了搜索空间与算子选择、复杂度控制、特征选择、与现代人工智能方法的整合，以及将对称性和渐近行为等理论信息纳入的策略。

Result: 专刊文章展示了SR在自动方程发现、涌现现象建模和构建用于昂贵计算模拟的紧凑替代器（emulator）等方面的实际应用与案例，体现了方法学与实践的加速发展。

Conclusion: SR在物理科学中的重要性与适用性正在增长，但仍面临可扩展性、抗噪性、过拟合和计算复杂度等挑战；融合物理约束（如对称性与渐近行为）被认为是未来的关键方向。

Abstract: Symbolic regression (SR) has emerged as a powerful method for uncovering interpretable mathematical relationships from data, offering a novel route to both scientific discovery and efficient empirical modelling. This article introduces the Special Issue on Symbolic Regression for the Physical Sciences, motivated by the Royal Society discussion meeting held in April 2025. The contributions collected here span applications from automated equation discovery and emergent-phenomena modelling to the construction of compact emulators for computationally expensive simulations.
  The introductory review outlines the conceptual foundations of SR, contrasts it with conventional regression approaches, and surveys its main use cases in the physical sciences, including the derivation of effective theories, empirical functional forms and surrogate models. We summarise methodological considerations such as search-space design, operator selection, complexity control, feature selection, and integration with modern AI approaches. We also highlight ongoing challenges, including scalability, robustness to noise, overfitting and computational complexity. Finally we emphasise emerging directions, particularly the incorporation of symmetry constraints, asymptotic behaviour and other theoretical information. Taken together, the papers in this Special Issue illustrate the accelerating progress of SR and its growing relevance across the physical sciences.

</details>


### [73] [A Unification of Discrete, Gaussian, and Simplicial Diffusion](https://arxiv.org/abs/2512.15923)
*Nuria Alina Chandra,Yucen Lily Li,Alan N. Amin,Alex Ali,Joshua Rollins,Sebastian W. Ober,Aniruddh Raghu,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文提出以Wright–Fisher族群遺傳模型為基礎的統一理論，將離散擴散（離散空間、歐氏高斯與單純形上的擴散）視為同一過程的不同參數化，並在大族群極限下導出單純形與高斯擴散。基於此理論修正了單純形擴散的數值不穩定性，實驗表明在條件DNA生成上性能更好，且可訓練單一模型在三種域間切換並得到有競爭力的結果。


<details>
  <summary>Details</summary>
Motivation: 當前處理離散序列（如DNA、蛋白質、語言）的擴散方法分為三類，分別在域的自然性、算法成熟度與數值穩定性上各有長短，缺乏一個統一框架方便比較、轉換與綜合優勢。

Method: 提出以Wright–Fisher（WF）族群遺傳模型為統一基礎，證明單純形與高斯擴散可由WF在大族群極限導出；形式上掛鉤不同模型的似然與超參數，並借用數學遺傳學結果設計數值穩定的單純形擴散過程；最後訓練同一模型以在測試時在三種域間切換。

Result: 理論上連接了三類擴散方法並給出穩定化的單純形擴散算法；實驗上WF單純形擴散在條件DNA生成上優於先前單純形方法，且單一多域模型在各域表現可與專門訓練的模型相比。

Conclusion: 本文建立了一個統一的理論與實用算法，既能解釋三類離散擴散方法的關係，也提供更穩定的單純形擴散實現，並展示了多域通用模型的可行性，對離散序列的擴散模型研究與應用具有實際價值。

Abstract: To model discrete sequences such as DNA, proteins, and language using diffusion, practitioners must choose between three major methods: diffusion in discrete space, Gaussian diffusion in Euclidean space, or diffusion on the simplex. Despite their shared goal, these models have disparate algorithms, theoretical structures, and tradeoffs: discrete diffusion has the most natural domain, Gaussian diffusion has more mature algorithms, and diffusion on the simplex in principle combines the strengths of the other two but in practice suffers from a numerically unstable stochastic processes. Ideally we could see each of these models as instances of the same underlying framework, and enable practitioners to switch between models for downstream applications. However previous theories have only considered connections in special cases. Here we build a theory unifying all three methods of discrete diffusion as different parameterizations of the same underlying process: the Wright-Fisher population genetics model. In particular, we find simplicial and Gaussian diffusion as two large-population limits. Our theory formally connects the likelihoods and hyperparameters of these models and leverages decades of mathematical genetics literature to unlock stable simplicial diffusion. Finally, we relieve the practitioner of balancing model trade-offs by demonstrating it is possible to train a single model that can perform diffusion in any of these three domains at test time. Our experiments show that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial diffusion models on conditional DNA generation. We also show that we can train models on multiple domains at once that are competitive with models trained on any individual domain.

</details>


### [74] [DSO: Direct Steering Optimization for Bias Mitigation](https://arxiv.org/abs/2512.15926)
*Lucas Monteiro Paes,Nivedha Sivakumar,Yinong Oliver Wang,Masha Fedzechkina Donaldson,Luca Zappella,Nicholas Apostoloff*

Main category: cs.LG

TL;DR: 本文提出直接驱动优化（DSO），使用强化学习在推理时学习线性激活变换，以在减少生成模型（VLM/LLM）偏见的同时尽量保持模型能力，实现可控的公平-性能折中。作者声称在VLM和LLM上达成了最先进的折中效果，并提供推理时可控性。


<details>
  <summary>Details</summary>
Motivation: 生成模型的决策会受输入中被感知的人口学属性影响，导致对某些群体的识别或决策不公；既有的激活驱动方法并不能可靠地实现各群体间的等概率输出，因此需要一种可在推理时直接可控且对偏见具有矫正能力的方法。

Method: 提出Direct Steering Optimization (DSO)：用强化学习来搜索用于激活层的线性变换（steering transforms），优化目标同时考虑公平性指标与模型性能，从而在推理时通过线性变换调节输出分布以减少偏差并保持能力。

Result: 在视觉-语言模型和大语言模型上，DSO比现有激活驱动或启发式方法提供更好的公平-能力折中，能在推理时给用户控制权以调整偏见减缓与模型性能间的权衡。

Conclusion: 面向行为控制目标直接优化的激活驱动策略比依赖预定义启发式的可控方法更有效；DSO为偏见干预提供了更实际的推理时控制手段。

Abstract: Generative models are often deployed to make decisions on behalf of users, such as vision-language models (VLMs) identifying which person in a room is a doctor to help visually impaired individuals. Yet, VLM decisions are influenced by the perceived demographic attributes of people in the input, which can lead to biased outcomes like failing to identify women as doctors. Moreover, when reducing bias leads to performance loss, users may have varying needs for balancing bias mitigation with overall model capabilities, highlighting the demand for methods that enable controllable bias reduction during inference. Activation steering is a popular approach for inference-time controllability that has shown potential in inducing safer behavior in large language models (LLMs). However, we observe that current steering methods struggle to correct biases, where equiprobable outcomes across demographic groups are required. To address this, we propose Direct Steering Optimization (DSO) which uses reinforcement learning to find linear transformations for steering activations, tailored to mitigate bias while maintaining control over model performance. We demonstrate that DSO achieves state-of-the-art trade-off between fairness and capabilities on both VLMs and LLMs, while offering practitioners inference-time control over the trade-off. Overall, our work highlights the benefit of designing steering strategies that are directly optimized to control model behavior, providing more effective bias intervention than methods that rely on pre-defined heuristics for controllability.

</details>


### [75] [BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research](https://arxiv.org/abs/2512.15931)
*Tiancheng Gao,Scott C. Lowe,Brendan Furneaux,Angel X Chang,Graham W. Taylor*

Main category: cs.LG

TL;DR: 提出BarcodeMamba+，基于高效的状态空间模型的真菌条形码基础模型，采用预训练+微调并利用部分标注数据，结合层级标签平滑、加权损失和多头输出等增强策略，在具有分布漂移的真菌分类基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 真菌物种分类受限于标注稀疏和长尾类分布，常规模型难以推广到未见物种并捕捉层级分类结构，需要一种能利用部分标注数据、处理层级关系并具有良好泛化能力的方案。

Method: 构建基于状态空间模型的基础模型（BarcodeMamba+），使用预训练‑微调范式，在预训练阶段利用部分标注数据学习表征，微调时加入层级标签平滑、加权损失函数和来自MycoAI的多头输出层等技术；进行了系统性组件消融与评估。

Result: 在含有与训练集有显著分类分布差异的严格真菌基准上，模型在所有分类层级上均优于多种现有方法，各个改进组件均带来显著性能提升。

Conclusion: 提出了一种对稀疏标签和层级关系更健壮的可扩展训练范式，为基于基因组的生物多样性监测提供了有力工具；且代码开源，便于复现与扩展。

Abstract: Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba.

</details>


### [76] [In-Context Semi-Supervised Learning](https://arxiv.org/abs/2512.15934)
*Jiashuo Fan,Paul Rosu,Aaron T. Wang,Michael Li,Lawrence Carin,Xiang Cheng*

Main category: cs.LG

TL;DR: 该论文提出并分析了“in-context 半监督学习（IC-SSL）”问题：在少量带标签样本加大量无标签上下文下，证明Transformer能利用无标签上下文学习稳健的上下文依赖表示，从而在低标签数据情形显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有关于Transformer的理论多关注带显式标签的监督ICL，但实际应用中上下文往往标签稀缺或缺失，且Transformer仍表现良好，说明无标签上下文中存在可利用的结构；需要理论化地理解Transformer如何从无标签示例中获益。

Method: 提出IC-SSL范式：少量带标签样本与大量无标签示例共同作为上下文输入，分析Transformer在该设置下如何构建上下文相关的表示与学习策略；通过理论与/或构造性的分析展示模型如何从无标签点中提取有用信息以改进表示学习。

Result: 证明或展示Transformer确实能够利用无标签上下文学习出对预测有用的、鲁棒的上下文依赖表示，从而在低标签率场景下显著提高性能；结果表明IC-SSL可作为解释Transformer在无标签或半监督上下文中良好表现的基础。

Conclusion: IC-SSL为理解Transformer如何从无标签上下文中学习表示提供了理论与实证基础，表明在ICL框架下无标签示例可被有效利用以提升低标签情形下的泛化能力，并为未来半监督ICL研究指明方向。

Abstract: There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.

</details>


### [77] [Governance by Evidence: Regulated Predictors in Decision-Tree Models](https://arxiv.org/abs/2512.15955)
*Alexios Veskoukis,Dimitris Kalles*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Decision-tree methods are widely used on structured tabular data and are valued for interpretability across many sectors. However, published studies often list the predictors they use (for example age, diagnosis codes, location). Privacy laws increasingly regulate such data types. We use published decision-tree papers as a proxy for real-world use of legally governed data. We compile a corpus of decision-tree studies and assign each reported predictor to a regulated data category (for example health data, biometric identifiers, children's data, financial attributes, location traces, and government IDs). We then link each category to specific excerpts in European Union and United States privacy laws. We find that many reported predictors fall into regulated categories, with the largest shares in healthcare and clear differences across industries. We analyze prevalence, industry composition, and temporal patterns, and summarize regulation-aligned timing using each framework's reference year. Our evidence supports privacy-preserving methods and governance checks, and can inform ML practice beyond decision trees.

</details>


### [78] [Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling](https://arxiv.org/abs/2512.15956)
*John Hateley,Sriram Narasimhan,Omid Abari*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a novel, cost-effective, and scalable approach to track numerous assets distributed in forested environments using commodity Radio Frequency Identification (RFID) targeting wildfire response applications. Commodity RFID systems suffer from poor tag localization when dispersed in forested environments due to signal attenuation, multi-path effects and environmental variability. Current methods to address this issue via fingerprinting rely on dispersing tags at known locations {\em a priori}. In this paper, we address the case when it is not possible to tag known locations and show that it is possible to localize tags to accuracies comparable to global positioning systems (GPS) without such a constraint. For this, we propose Gaussian Process to model various environments solely based on RF signal response signatures and without the aid of additional sensors such as global positioning GPS or cameras, and match an unknown RF to the closest match in a model dictionary. We utilize a new weighted log-likelihood method to associate an unknown environment with the closest environment in a dictionary of previously modeled environments, which is a crucial step in being able to use our approach. Our results show that it is possible to achieve localization accuracies of the order of GPS, but with passive commodity RFID, which will allow the tracking of dozens of wildfire assets within the vicinity of mobile readers at-a-time simultaneously, does not require known positions to be tagged {\em a priori}, and can achieve localization at a fraction of the cost compared to GPS.

</details>


### [79] [Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering](https://arxiv.org/abs/2512.16717)
*Rudra Dubey,Arpit Mani Tripathi,Archit Srivastava,Sarvpal Singh*

Main category: cs.LG

TL;DR: 提出了一个集成字符级CNN与LightGBM（结合36个人工特征）的URL钓鱼检测系统，在19,873条URL测试集上报告了 ~99.82% 的准确率与接近完美的精度/ROC-AUC，并通过FastAPI提供在线服务。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击持续演化，单一模型难以同时捕捉URL的顺序信息与人工设计的词法/结构/域特征，故提出融合深度学习与梯度提升树的集成方法以提升检测性能与实用性。

Method: 提取36个词法、结构与域相关特征；用字符级CNN抽取序列特征；用LightGBM基于人工特征训练；通过加权/集成（文中宣称LightGBM贡献40%、字符CNN贡献60%）合并预测，部署为FastAPI服务并提供UI。

Result: 在19,873条URL测试集上，报告准确率99.819%、精度100%、召回99.635%、ROC-AUC99.947%。集成在性能上优于单模型，且假阳性率极低。

Conclusion: 结合字符级深度特征与工程特征的集成方法能高效检测当前钓鱼技巧，并能以在线服务形式提供实时检测。

Abstract: In actuality, phishing attacks remain one of the most prevalent cybersecurity risks in existence today, with malevolent actors constantly changing their strategies to successfully trick users. This paper presents an AI model for a phishing detection system that uses an ensemble approach to combine character-level Convolutional Neural Networks (CNN) and LightGBM with engineered features. Our system uses a character-level CNN to extract sequential features after extracting 36 lexical, structural, and domain-based features from the URLs. On a test dataset of 19,873 URLs, the ensemble model achieves an accuracy of 99.819 percent, precision of 100 percent, recall of 99.635 percent, and ROC-AUC of 99.947 percent. Through a FastAPI-based service with an intuitive user interface, the suggested system has been utilised to offer real-time detection. In contrast, the results demonstrate that the suggested solution performs better than individual models; LightGBM contributes 40 percent and character-CNN contributes 60 percent to the final prediction. The suggested method maintains extremely low false positive rates while doing a good job of identifying contemporary phishing techniques. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis

</details>


### [80] [Provably Extracting the Features from a General Superposition](https://arxiv.org/abs/2512.15987)
*Allen Liu*

Main category: cs.LG

TL;DR: 提出了一种高效查询算法：在黑盒查询模型 f(x)=∑ a_i σ_i(v_i^T x) 下，从有噪声的oracle恢复非退化响应的所有特征方向 v_i 并重构 f；算法允许超完备（n>d）且响应函数任意，只要求不同 v_i 不近似相同。


<details>
  <summary>Details</summary>
Motivation: 研究复杂机器学习模型中以线性表示编码但处于叠加（superposition/超完备）状态的特征，理解并设计算法从仅有的黑盒查询中分离并恢复这些特征，特别是当特征数量超过维度时的困难情形。

Method: 引入一种在傅里叶域中搜索的迭代细化方法：通过查询oracle 在频域信息中定位隐藏方向 v_i，逐步收缩候选空间，识别并分离非退化的响应分量；算法对响应函数 σ_i 几乎没有限制，仅需方向间非重合性。

Result: 给出一个在有噪声查询下高效运行的算法，证明能识别所有具有非退化响应的 v_i 并重构对应函数 f；该结果在允许任意响应函数与超完备设置方面超越了以往相关工作。

Conclusion: 本文推进了对超完备特征学习的理论与算法理解，展示了通过频域迭代搜索可在较弱假设下从黑盒查询中分离特征；主要限制为对“非退化”响应和方向分离的需求，以及对查询复杂度和实际实现细节需在全文中进一步考察。

Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,σ_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $σ_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $σ_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.

</details>


### [81] [Higher-Order LaSDI: Reduced Order Modeling with Multiple Time Derivatives](https://arxiv.org/abs/2512.15997)
*Robert Stephany,William Michael Anderson,Youngsoo Choi*

Main category: cs.LG

TL;DR: 本文提出两项改进：一种灵活且高阶但代价低的有限差分格式，以及一种名为Rollout loss的训练方法，旨在提高降阶模型（ROM）对参数化PDE在长时间预测中的鲁棒性；在2D Burgers方程上展示效果。


<details>
  <summary>Details</summary>
Motivation: 求解复杂偏微分方程计算昂贵，ROM通过降维快速近似但在长时间预测时精度下降，需要改进数值格式和训练目标以提升长期可预测性。

Method: 1) 设计一种高阶且低成本的有限差分格式以提高时间和空间离散精度；2) 引入Rollout loss：在训练时模拟任意时间长度的多步演化（rollout），直接对长期轨迹误差进行最小化，从而使ROM学到稳定且可累积预测误差控制的动态映射。

Result: 在二维Burgers方程上的数值实验表明：结合高阶差分格式和Rollout loss训练的ROM在长时间步预测中具有显著更好的准确性和稳定性，相较于传统逐步误差最小化训练方法表现更优。

Conclusion: 通过改进数值格式与引入面向长期轨迹的Rollout loss，可以显著增强ROM对参数化PDE的长期预测能力，为工程和科学中高效求解时变问题提供可行路径。

Abstract: Solving complex partial differential equations is vital in the physical sciences, but often requires computationally expensive numerical methods. Reduced-order models (ROMs) address this by exploiting dimensionality reduction to create fast approximations. While modern ROMs can solve parameterized families of PDEs, their predictive power degrades over long time horizons. We address this by (1) introducing a flexible, high-order, yet inexpensive finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to make accurate predictions over arbitrary time horizons. We demonstrate our approach on the 2D Burgers equation.

</details>


### [82] [Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results](https://arxiv.org/abs/2512.16013)
*Ruolei Zeng,Arun Sharma,Shuai An,Mingzhou Yang,Shengya Zhang,Licheng Liu,David Mulla,Shashi Shekhar*

Main category: cs.LG

TL;DR: 提出FTBSC-KGML：在KGML-ag基础上加入预训练-微调与站点特定参数，利用遥感GPP、气候与土壤数据通过空间异质性感知的迁移学习，提高不同州/站点下碳排放估计的局部精度与可解释性。


<details>
  <summary>Details</summary>
Motivation: 在决策相关尺度上准确且经济地量化农业生态系统碳循环对气候缓解与可持续农业至关重要，但异构数据与跨尺度依赖使迁移学习与空间可变性利用困难，传统方法忽视地点特定异质性，限制了在高度空间变异区域的适用性。

Method: 构建FTBSC-KGML：在KGML-ag框架上加入预训练-微调流程与站点特定参数。首先用跨多个中西部站点的遥感GPP、气候和土壤协变量进行全局预训练；然后在州或站点级别微调以学习地点感知的表征。引入空间异质性感知的迁移学习方案，在保持可解释性的同时增强小样本下的本地性能。

Result: 实证显示，FTBSC-KGML相比纯全局模型在验证误差上更低，对解释力（可解释性指标）更一致，能更好地捕捉州际/站点间的空间差异，从而提升陆地排放估计的精度与稳定性。

Conclusion: FTBSC-KGML扩展了先前的SDSA-KGML框架，通过预训练-微调和站点参数化有效融合迁移学习与空间异质性，提升了在数据受限环境下的局部预测能力与可解释性，对区域尺度碳循环量化具有实用价值。

Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.

</details>


### [83] [CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting](https://arxiv.org/abs/2512.16046)
*Shu Wan,Reepal Shah,John Sabo,Huan Liu,K. Selçuk Candan*

Main category: cs.LG

TL;DR: 提出CauStream：一个联合学习径流因果图与站点间路由图的时空流量预测框架，在非参数设置下给出因果结构可识别性证明，并在三个美国产流域和多个预测时段上优于现有方法，同时能恢复与水文学知识一致的可解释结构。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽能取得高预测精度，但忽视物理因果机制，影响可解释性与泛化；现有因果学习方法依赖固定因果图、不能随数据自适应，亟需一个同时可学习因果结构与时空依赖且具可识别性的统一框架。

Method: CauStream联合学习两类图：①描述气象驱动下径流生成的因果图；②描述站点间动态依赖的路由图。方法在非参数假设下推导出因果结构的可识别性条件，并将图结构与时序预测模块耦合训练以进行多时距流量预报。

Result: 在三大美国流域、三种预测时窗上的实验证明CauStream持续优于现有最优方法，且随预测窗口延长性能优势增大。模型学习到的因果与路由结构与水文学领域知识高度一致，提升了解释性。

Conclusion: CauStream为因果时空建模提供了原则性框架，兼顾精度与可解释性，具较好泛化能力，可推广到其他科学与环境问题。

Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.

</details>


### [84] [In-Context Multi-Operator Learning with DeepOSets](https://arxiv.org/abs/2512.16074)
*Shao-Ting Chiu,Aditya Nambiar,Ali Syed,Jonathan W. Siegel,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: 提出并证明DeepOSets在适当修改下可作为一种“多算子”的in-context学习器：在prompt中给出参数-解的示例对，无需权重更新即可恢复新的、训练时未见过的PDE的解算子；并给出普适一致逼近（universal uniform approximation）理论结果，证明对一类连续算子可达到任意精度；实验证明在Poisson和反应-扩散正/逆问题上有效。


<details>
  <summary>Details</summary>
Motivation: 填补关于in-context learning（ICL）产生机制的知识空白：现有ICL研究集中在自回归transformer/自注意力机制上，作者展示非自回归、无注意力的DeepOSets也能实现ICL，且将ICL的范畴推广到算子学习和PDE求解领域。

Method: 基于DeepSets（集合学习）与DeepONets（算子学习）融合的DeepOSets架构，做出适当修改使其成为“多算子”模型：模型以prompt形式接受若干参数-解示例对并在测试时给出新参数的解，且不更新权重。理论上证明该架构对一类连续算子具有统一的逼近能力（存在单一结构和足够多的示例可达任意精度）。实验在Poisson与reaction-diffusion正/逆边值问题上验证了方法的泛化与ICL能力。

Result: 理论成果：证明DeepOSets可作为对某类连续算子的普适一致逼近器（首次在科学机器学习文献中给出此类结果）。实证成果：在若干PDE问题上，模型能利用prompt中的示例对准确预测训练外的PDE的解，支持其作为多算子in-context学习器的主张。

Conclusion: 工作拓展了ICL的实现机制与适用领域，表明非注意力、非自回归架构也能完成复杂算子级别的in-context泛化，并给出严谨的泛函逼近保证和实验验证，具有理论与应用双重贡献。

Abstract: In-context Learning (ICL) is the remarkable capability displayed by some machine learning models to learn from examples in a prompt, without any further weight updates. ICL had originally been thought to emerge from the self-attention mechanism in autoregressive transformer architectures. DeepOSets is a non-autoregressive, non-attention based neural architecture that combines set learning via the DeepSets architecture with operator learning via Deep Operator Networks (DeepONets). In a previous study, DeepOSets was shown to display ICL capabilities in supervised learning problems. In this paper, we show that the DeepOSets architecture, with the appropriate modifications, is a multi-operator in-context learner that can recover the solution operator of a new PDE, not seen during training, from example pairs of parameter and solution placed in a user prompt, without any weight updates. Furthermore, we show that DeepOSets is a universal uniform approximator over a class of continuous operators, which we believe is the first result of its kind in the literature of scientific machine learning. This means that a single DeepOSets architecture exists that approximates in-context any continuous operator in the class to any fixed desired degree accuracy, given an appropriate number of examples in the prompt. Experiments with Poisson and reaction-diffusion forward and inverse boundary-value problems demonstrate the ability of the proposed model to use in-context examples to predict accurately the solutions corresponding to parameter queries for PDEs not seen during training.

</details>


### [85] [Privacy Blur: Quantifying Privacy and Utility for Image Data Release](https://arxiv.org/abs/2512.16086)
*Saeed Mahloujifar,Narine Kokhlikyan,Chuan Guo,Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: 本文评估了用于图像中隐私信息（如人脸、车牌）遮蔽的几种方法，发现常用的高斯模糊在实际低精度实现下容易被逆向，而像素化与像素化+噪声（DP-Pix）在合理粒度下兼具隐私与可用性。作者提供了名为 Privacy Blur 的软件包与推荐参数。


<details>
  <summary>Details</summary>
Motivation: 现实世界采集的图像包含敏感信息，需要在发布时既保护隐私又保留模型训练所需的有用信号。现行工业做法（如高斯模糊）是否足够安全值得检验。

Method: 比较四种遮蔽算法：高斯模糊、像素化、像素化加噪声（DP-Pix）、裁剪。通过逆向攻击与判别攻击评估隐私性，通过在遮蔽脸部的情况下训练模型并评估学得表征质量评估可用性。还提供推荐参数并发布工具包。

Result: 实验表明：在实际低精度实现中，高斯模糊可被逆向，隐私性最差；像素化与 DP-Pix 在适当粒度下在多个计算机视觉任务上兼顾隐私与效用；裁剪的表现依情形而异。

Conclusion: 不要依赖默认的高斯模糊；考虑使用像素化或像素化+噪声并按照建议参数设置；作者提供了 Privacy Blur 软件以便部署与复现。

Abstract: Image data collected in the wild often contains private information such as faces and license plates, and responsible data release must ensure that this information stays hidden. At the same time, released data should retain its usefulness for model-training. The standard method for private information obfuscation in images is Gaussian blurring. In this work, we show that practical implementations of Gaussian blurring are reversible enough to break privacy. We then take a closer look at the privacy-utility tradeoffs offered by three other obfuscation algorithms -- pixelization, pixelization and noise addition (DP-Pix), and cropping. Privacy is evaluated by reversal and discrimination attacks, while utility by the quality of the learnt representations when the model is trained on data with obfuscated faces. We show that the most popular industry-standard method, Gaussian blur is the least private of the four -- being susceptible to reversal attacks in its practical low-precision implementations. In contrast, pixelization and pixelization plus noise addition, when used at the right level of granularity, offer both privacy and utility for a number of computer vision tasks. We make our proposed methods together with suggested parameters available in a software package called Privacy Blur.

</details>


### [86] [AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation](https://arxiv.org/abs/2512.16103)
*Sandeep Neela*

Main category: cs.LG

TL;DR: AIMM combines Reddit-derived (synthetic) social signals, bot/coordination indicators and OHLCV into a daily manipulation-risk score, provides a parquet-native pipeline + dashboard, releases a small labeled AIMM-GT (33 ticker-days, 3 positives), uses forward-walk evaluation and claims early warning (GME flagged 22 days pre-peak).


<details>
  <summary>Details</summary>
Motivation: Coordination-driven market manipulation increasingly originates from social-media campaigns; stakeholders need tools linking online narratives/coordination patterns to market moves for early detection and surveillance.

Method: Parquet-native data pipeline + Streamlit dashboard; social features are synthetic but calibrated to documented events (due to Reddit API limits); real OHLCV from Yahoo Finance; models output daily manipulation risk scores per ticker; evaluation via forward-walk and prospective logging.

Result: Preliminary discriminative signal on a very small labeled set; AIMM flagged GME 22 days before the January 2021 squeeze peak; code, schema and dashboard design released to support further research.

Conclusion: AIMM is a promising proof-of-concept with good engineering practice (pipeline, prospective logging, dashboard) but is constrained by a tiny, imbalanced labeled set and synthetic social features—further labeling, validation, ablation and real-world testing are required.

Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.

</details>


### [87] [Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure](https://arxiv.org/abs/2512.16126)
*Lulu Xue,Shengshan Hu,Linqiang Qian,Peijin Guo,Yechao Zhang,Minghui Li,Yanjun Zhang,Dayong Ye,Leo Yu Zhang*

Main category: cs.LG

TL;DR: Machine unlearning 在删除训练数据的同时，会在“dual-view”（对未删前后模型都能查询）场景下放大对保留数据的隐私泄露。作者提出“privacy knowledge gain”从信息论解释这一风险，并设计DVIA——一种基于黑盒查询和似然比推断、无需训练攻击模型的双视图成员推断攻击。实验表明 DVIA 在不同数据集和模型上有效。


<details>
  <summary>Details</summary>
Motivation: 填补现有工作对“被保留数据”在机器去学习过程中的隐私风险研究空白，特别是在攻击者能同时访问原始模型和去学习后模型的双视图场景下。

Method: 从信息论引入“privacy knowledge gain”概念，表明双视图查询能带来比单模型更多的信息。提出 DVIA：通过对原始与去学习后模型的黑盒查询，计算输出的似然比进行高效成员推断，避免训练专用攻击模型。

Result: 在多种数据集和模型结构上，DVIA 有效提高了对保留数据的成员推断成功率，验证了双视图能显著放大隐私泄露。

Conclusion: 机器去学习在双视图访问下会增加对保留数据的隐私风险，应关注此类威胁并开发相应防护机制。

Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.

</details>


### [88] [INTELLECT-3: Technical Report](https://arxiv.org/abs/2512.16144)
*Prime Intellect Team,Mika Senghaas,Fares Obeid,Sami Jaghouar,William Brown,Jack Min Ong,Daniel Auras,Matej Sirovatka,Jannik Straube,Andrew Baker,Sebastian Müller,Justus Mattern,Manveer Basra,Aiman Ismail,Dominik Scherm,Cooper Miller,Ameen Patel,Simon Kirsten,Mario Sieg,Christian Reetz,Kemal Erdem,Vincent Weisser,Johannes Hagemann*

Main category: cs.LG

TL;DR: INTELLECT-3 是一个 106B 参数的 MoE（12B 活跃）模型，使用大规模强化学习训练并开源了模型与完整训练/评估基础设施；在数学、代码、科学和推理基准上对同体积模型表现出色，并优于许多更大模型。


<details>
  <summary>Details</summary>
Motivation: 通过端到端强化学习架构与可扩展异步 RL 框架，提升中大规模模型在复杂推理、代码与科学任务上的综合能力，同时开放训练与评估栈以提高可复现性与社区协作。

Method: 在 GLM-4.5-Air-Base 基础上进行 SFT 与大规模异步 RL（prime-rl）训练，采用 MoE 结构（106B 总参数、12B 活跃），使用 verifiers 库构建的大量环境并通过 Environments Hub 分享，训练规模可扩展至 512 块 H200 GPU。

Result: 在数学、代码、科学和推理多个基准上达到了同等规模的最先进性能，优于不少更大规模的前沿模型；展示了高训练效率与可扩展性。

Conclusion: 通过同时发布模型与完整 RL 基础设施（包括 prime-rl 和环境集合），该工作推动了大规模 agentic 强化学习的可复现实践，展示了 MoE+RL 在提升模型能力与资源利用上的潜力。

Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.

</details>


### [89] [Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions](https://arxiv.org/abs/2512.16200)
*Haishan Ye*

Main category: cs.LG

TL;DR: 本文为基于秩的零阶优化（只依赖函数值排序）给出首个显式、非渐近的查询复杂度界；在强凸和非凸光滑目标下分别给出接近线性依赖维度d和L/μ或L/ε的复杂度。


<details>
  <summary>Details</summary>
Motivation: 秩基零阶方法（如CMA-ES、NES）在实际中对噪声与单调变换鲁棒，但理论结果主要为渐近性质，缺乏对选择top-k方向算法的显式收敛率分析。

Method: 分析一个简单的秩基零阶算法，采用新的分析框架（避免传统的漂移与信息几何技术），推导高概率下的非渐近查询复杂度界。

Result: 在d维、L-光滑、μ-强凸情形，查询复杂度为 ~O((dL/μ) log(dL/(μδ)) log(1/ε))；在光滑非凸情形为 O((dL/ε) log(1/ε))，均以概率至少1-δ成立（0<δ<1）。符号O和~O分别忽略常数和额外的log log(1/ε)项。

Conclusion: 本文填补了秩基零阶优化理论上的空白，提供了可用的非渐近复杂度界，并给出为何秩基启发式能高效的洞见，尽管常数因子与top-k具体依赖仍需进一步明确。

Abstract: Rank-based zeroth-order (ZO) optimization -- which relies only on the ordering of function evaluations -- offers strong robustness to noise and monotone transformations, and underlies many successful algorithms such as CMA-ES, natural evolution strategies, and rank-based genetic algorithms. Despite its widespread use, the theoretical understanding of rank-based ZO methods remains limited: existing analyses provide only asymptotic insights and do not yield explicit convergence rates for algorithms selecting the top-$k$ directions.
  This work closes this gap by analyzing a simple rank-based ZO algorithm and establishing the first \emph{explicit}, and \emph{non-asymptotic} query complexities. For a $d$-dimension problem, if the function is $L$-smooth and $μ$-strongly convex, the algorithm achieves $\widetilde{\mathcal O}\!\left(\frac{dL}μ\log\!\frac{dL}{μδ}\log\!\frac{1}{\varepsilon}\right)$ to find an $\varepsilon$-suboptimal solution, and for smooth nonconvex objectives it reaches $\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$. Notation $\cO(\cdot)$ hides constant terms and $\widetilde{\mathcal O}(\cdot)$ hides extra $\log\log\frac{1}{\varepsilon}$ term. These query complexities hold with a probability at least $1-δ$ with $0<δ<1$. The analysis in this paper is novel and avoids classical drift and information-geometric techniques. Our analysis offers new insight into why rank-based heuristics lead to efficient ZO optimization.

</details>


### [90] [Neural emulation of gravity-driven geohazard runout](https://arxiv.org/abs/2512.16221)
*Lorenzo Nava,Ye Chen,Maximillian Van Wyk de Vries*

Main category: cs.LG

TL;DR: Train neural emulator to predict geohazard runout (flow extent and deposit thickness) across real terrains using 100k+ numerical simulations; matches key physics and is 100–10,000× faster than solvers.


<details>
  <summary>Details</summary>
Motivation: Runout prediction is critical for saving lives and infrastructure but existing methods force a trade-off between physical realism and computational speed, limiting large-scale or real-time applications like early warning.

Method: Generate >100,000 numerical simulations on >10,000 real-world DEM chips; train a machine-learning model to predict spatially resolved flow extent and deposit thickness; validate against numerical solvers and test generalization across flow types, sizes, and landscapes.

Result: Model achieves high accuracy in extent and thickness prediction, reproduces physical behaviors such as avulsion and deposition patterns, generalizes across diverse terrains, and runs 100–10,000× faster than traditional numerical solvers.

Conclusion: Neural emulation offers a promising route to scale physically realistic geohazard runout modelling to spatial and temporal domains needed for operational early warning and impact-based forecasting.

Abstract: Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.

</details>


### [91] [Sharpness-aware Federated Graph Learning](https://arxiv.org/abs/2512.16247)
*Ruiyu Li,Peige Zhao,Guangxia Li,Pengcheng Wu,Xingyu Gao,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 提出SEAL：在联邦图学习中同时最小化局部损失与其“锐度”（曲率），并引入基于表示相关矩阵的正则项以缓解维度塌缩，从而提升异构数据下的泛化与分类性能。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习中各客户端数据分布差异较大，常规基于经验风险最小化的优化器容易让局部模型陷入“尖锐”极值，降低对分布外图数据的泛化，且局部表示常出现维度塌缩，削弱分类能力。

Method: 设计一个感知“锐度”的优化目标，联合最小化损失与其曲率以寻求平缓低损失区域；同时引入基于局部表示相关矩阵的正则项以放松样本间表示的相关性，缓解维度塌缩。将上述思想整合为联邦算法SEAL，并在分布式GNN训练框架中实现。

Result: 在若干图分类基准上进行实验，SEAL在分类精度和泛化能力方面持续优于现有SOTA FGL基线，并对更多参与方带来性能提升。

Conclusion: SEAL通过同时优化锐度和表示相关性，有效缓解了联邦图学习中的数据异构带来的泛化与表示问题，实验证明了方法的可行性和优势。

Abstract: One of many impediments to applying graph neural networks (GNNs) to large-scale real-world graph data is the challenge of centralized training, which requires aggregating data from different organizations, raising privacy concerns. Federated graph learning (FGL) addresses this by enabling collaborative GNN model training without sharing private data. However, a core challenge in FGL systems is the variation in local training data distributions among clients, known as the data heterogeneity problem. Most existing solutions suffer from two problems: (1) The typical optimizer based on empirical risk minimization tends to cause local models to fall into sharp valleys and weakens their generalization to out-of-distribution graph data. (2) The prevalent dimensional collapse in the learned representations of local graph data has an adverse impact on the classification capacity of the GNN model. To this end, we formulate a novel optimization objective that is aware of the sharpness (i.e., the curvature of the loss surface) of local GNN models. By minimizing the loss function and its sharpness simultaneously, we seek out model parameters in a flat region with uniformly low loss values, thus improving the generalization over heterogeneous data. By introducing a regularizer based on the correlation matrix of local representations, we relax the correlations of representations generated by individual local graph samples, so as to alleviate the dimensional collapse of the learned model. The proposed \textbf{S}harpness-aware f\textbf{E}derated gr\textbf{A}ph \textbf{L}earning (SEAL) algorithm can enhance the classification accuracy and generalization ability of local GNN models in federated graph learning. Experimental studies on several graph classification benchmarks show that SEAL consistently outperforms SOTA FGL baselines and provides gains for more participants.

</details>


### [92] [Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data](https://arxiv.org/abs/2512.16277)
*Jialiang Wang,Xueyan Bao,Hao Wu*

Main category: cs.LG

TL;DR: 提出一种将Sharpness-aware Minimization思想引入Second-order Latent Factor（SLF）学习的模型（SSLF），通过Hessian-向量积获得二阶信息并在曲率中注入sharpness项，以改善非凸双线性SLF模型的泛化性能。实验证明在多套工业数据集上优于SOTA基线。


<details>
  <summary>Details</summary>
Motivation: SLF在从高维且不完全的数据中学习低秩节点交互模式方面有效，但因目标函数双线性且非凸导致优化困难；SAM能找到更平坦的局部极小值以提高泛化，因而将两者结合有望改进SLF的训练稳定性与泛化能力。

Method: 设计SSLF：利用Hessian-向量积（HVP）来获取二阶信息，并通过特定构造的HVP将sharpness项注入到曲率中；训练过程在参数更新时显式考虑该曲率/锐度修正以偏向平坦最小值。

Result: 在多个工业数据集上的实验显示，SSLF在若干评估指标上持续优于现有最先进方法，表明引入二阶sharpness信息有助于提升性能。

Conclusion: 将SAM思想与SLF结合是可行且有效的，但论文需补充更多实现细节、复杂度分析、消融实验与理论支撑以增强结论可信度。

Abstract: Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.

</details>


### [93] [CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity](https://arxiv.org/abs/2512.16282)
*Jinhao Zhang,Yunquan Zhang,Daning Chen*

Main category: cs.LG

TL;DR: 提出了一种无需微调、按层选择量化算法的框架（CKA Guided Modular Quantization），用线性CKA度量自动为每层挑选最合适的PTQ方法，并将这些策略拼接成混合量化模型，在LLaMA、Qwen等模型上在困惑度和下游任务上优于统一量化和现有混精度方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的后训练量化对所有层采用统一策略，忽略了不同层在算法适配性上的显著差异，导致量化性能受限。需要一种能按层选择最优量化算法且无需微调的自动化方法。

Method: 对每一层独立应用多种PTQ算法并生成量化结果，使用线性中心化核对齐（Linear CKA）作为层级表示相似性/信息保持度量，自动判断哪种PTQ算法对该层损失最小并择优选择；将各层最优策略组合为混合量化模型。该框架为即插即用、无需微调。

Result: 在主流大模型（如LLaMA、Qwen）上进行实验，按困惑度（PPL）和若干下游任务评估，方法稳定优于统一量化基线与现有混精度方法，显示出更好的保真性和任务性能。

Conclusion: 基于CKA的模块化按层算法选择为PTQ提供了有效的自动化路径，在无需微调的前提下提升量化后模型质量；但该方法的开销（每层多算法评估）、对CKA指标的依赖以及在推理效率和硬件实现上的影响仍需进一步量化与验证。

Abstract: Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.

</details>


### [94] [Feature-Selective Representation Misdirection for Machine Unlearning](https://arxiv.org/abs/2512.16297)
*Taozhao Chen,Linghan Huang,Kim-Kwang Raymond Choo,Huaming Chen*

Main category: cs.LG

TL;DR: 提出一种称为SRMU的激活编辑方法，通过结构化误导向量和激活重要性映射，选择性抑制有害表示以实现面向表示的“忘记/去除”而尽量保留良性效用，在高度交织（overlap）场景下优于基线。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在安全/受监管领域的应用，模型必须能删除敏感或被禁止的知识以满足隐私、合规与安全需求；现有去学习方法假设可干净分离忘记与保留数据，但实际分布高度交织时会导致实用性下降或安全失败。

Method: SRMU在激活层面进行方向受控的扰动：构造结构化的误导向量并配合激活重要性图，允许只在与有害表征相关的激活上施加抑制性扰动，从而避免全局权重扰动带来的效用损失。

Result: 在WMDP基准的低/高交织配置下，SRMU在保留任务效用的同时实现了更好的去学习效果，在20–30%重叠区域仍保持有效，而现有基线方法性能崩溃。

Conclusion: SRMU为在高交织分布下的安全驱动模型治理与受控知识移除提供了鲁棒方案，并具备较好的实证性能与可复现性（给出了复现包）。

Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.

</details>


### [95] [Multivariate Uncertainty Quantification with Tomographic Quantile Forests](https://arxiv.org/abs/2512.16383)
*Takuya Kanazawa*

Main category: cs.LG

TL;DR: 提出了一种用于多变量目标的不参数化、不确定性感知的树基回归模型——Tomographic Quantile Forests（TQF），通过学习各方向投影的条件分位数并在推断时通过最小化切片Wasserstein距离聚合，还原多变量条件分布。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，量化预测不确定性对安全和可信赖的AI至关重要，但对多变量目标进行完全非参数化的条件分布估计仍具有挑战性。现有方向分位数方法通常仅给出凸的分位区域，且需为不同方向训练独立模型。

Method: TQF 在单个模型中学习关于输入 x 和单位方向 n 的方向投影 n^T y 的条件分位数，覆盖所有方向；推断时在多方向上聚合分位数，通过一个高效的交替方案（子问题为凸优化）最小化切片Wasserstein距离从而重建多元条件分布。模型基于树结构（quantile forests），无需对分位区域作凸性假设。

Result: 在合成数据和真实世界数据集上进行了评估，结果显示TQF可以在无需凸性限制的前提下有效建模多元条件分布，并公开了GitHub源码。

Conclusion: TQF 提供了一种灵活的、非参数化且高效的树基方法，用于多变量目标的不确定性估计，克服了传统方向分位数方法的若干限制。

Abstract: Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\mathbf{n}^{\top}\mathbf{y}$ as functions of the input $\mathbf{x}$ and the unit direction $\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.

</details>


### [96] [Quantitative Verification of Fairness in Tree Ensembles](https://arxiv.org/abs/2512.16386)
*Zhenjiang Zhao,Takahisa Toda,Takashi Kitamura*

Main category: cs.LG

TL;DR: 提出了一种面向树模型集成的定量公平性验证方法，能够在运行中给出上/下界（any-time bounds），比现有模型不可知且仅能给下界的方法更精确且更高效。


<details>
  <summary>Details</summary>
Motivation: 需要不仅指出单个反例，而是估计所有反例的比例并定位其分布区域以便诊断和缓解偏差。现有定量验证几乎只针对DNN，且基于CGAR的泛化方法存在只能给下界和可扩展性差的问题。

Method: 先将CGAR扩展为模型无关形式，识别其局限性后，利用树集成的离散结构设计高效的计量化技术：通过对输入空间和叶节点结构的组合分析，构造能够在任意时刻输出可行的上/下界的算法（any-time upper/lower bounds），并能定位反例发生的区域。

Result: 在五个常用数据集上的实验证明了方法的有效性与效率。在公平性测试场景中，本方法显著优于现有最先进的测试技术，能更快更准确地量化不公平性并定位问题区域。

Conclusion: 该工作把定量公平性验证从主要针对DNN扩展到树集成，提供可用的上/下界和可扩展的实现，为偏差诊断与缓解提供了实用工具。

Abstract: This work focuses on quantitative verification of fairness in tree ensembles. Unlike traditional verification approaches that merely return a single counterexample when the fairness is violated, quantitative verification estimates the ratio of all counterexamples and characterizes the regions where they occur, which is important information for diagnosing and mitigating bias. To date, quantitative verification has been explored almost exclusively for deep neural networks (DNNs). Representative methods, such as DeepGemini and FairQuant, all build on the core idea of Counterexample-Guided Abstraction Refinement, a generic framework that could be adapted to other model classes. We extended the framework into a model-agnostic form, but discovered two limitations: (i) it can provide only lower bounds, and (ii) its performance scales poorly. Exploiting the discrete structure of tree ensembles, our work proposes an efficient quantification technique that delivers any-time upper and lower bounds. Experiments on five widely used datasets demonstrate its effectiveness and efficiency. When applied to fairness testing, our quantification method significantly outperforms state-of-the-art testing techniques.

</details>


### [97] [Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference](https://arxiv.org/abs/2512.16391)
*Dhruv Deshmukh,Saurabh Goyal,Nipun Kwatra,Ramachandran Ramjee*

Main category: cs.LG

TL;DR: Kascade是一个无需训练的稀疏注意力方法：在少数“anchor”层上精确算出Top-k索引，并在中间层重用这些索引；anchor通过动态规划在开发集上按跨层相似性自动选择。方法基于注意力后softmax稀疏性和高权重键在相邻层间的稳定性，支持head-aware选择、tile级实现，适用于prefill和decode。与FlashAttention-3在H100上相比，在长上下文基准上能在decode/prefill分别实现约4.1×/2.2×加速，同时保持接近密集注意力的精度。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理中注意力计算成为主要延迟来源，且希望在不重新训练模型的前提下减少计算并保持精度，便于在RAG与推理场景中部署。

Method: 在少数算法选出的anchor层中精确计算每个head的Top-k key索引；通过动态规划在开发集上选取最大化跨层相似性的anchor集合；在其他“reuse”层直接复用这些索引以避免重复计算。实现上考虑tile级操作约束并同时覆盖prefill与decode路径，且Top-k选择为head-aware以保留准确率。

Result: 在LongBench与AIME-24等长上下文基准上，Kascade与FlashAttention-3基线比较，在H100上decode注意力可达最高4.1×加速，prefill约2.2×加速，同时在任务上精度接近密集注意力。

Conclusion: Kascade提供了一种易部署的、训练自由的稀疏注意力加速方案，通过跨层重用Top-k索引在保持精度的同时显著降低长上下文推理延迟。

Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.

</details>


### [98] [NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning](https://arxiv.org/abs/2512.16408)
*Ruifeng Xu,Liang He*

Main category: cs.LG

TL;DR: 提出嵌套双智能体强化学习（NDRL）用于灌溉与氮肥动态调控：父智能体负责宏观决策筛选，子智能体基于水分与氮营养胁迫因子并采用混合概率分布进行日常细调；用DSSAT经2023–2024田间资料标定与交互仿真，较基线在产量和资源利用效率上有小幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有工作难以同时解决水氮组合优化复杂性与对轻微胁迫信号的量化和滞后反馈问题，导致产量优化受限与资源利用效率低下。作者旨在通过分层强化学习提高决策效率与精细化控制能力。

Method: 设计NDRL：父智能体基于预测的累计产量效益筛选有前景的宏观灌溉/施肥动作以减少低效探索；子智能体在奖励中引入水分胁迫因子（WSF）和氮胁迫因子（NSF），并使用混合概率分布对每日策略进行动态优化。使用经2023、2024年田间数据标定的DSSAT作为环境进行训练与评估。

Result: 仿真结果显示：与最佳基线相比，产量在两年均提高4.7%；灌溉水生产率分别提高5.6%和5.1%；氮部分要素生产率分别提高6.3%和1.0%。

Conclusion: NDRL在仿真条件下能在兼顾产量与资源效率方面取得稳定提升，为棉花灌溉与氮肥精细化管理提供了一条可行思路，推动农业资源管理的复杂性与精确性问题向可解决方向发展。

Abstract: Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.

</details>


### [99] [Topic Modelling Black Box Optimization](https://arxiv.org/abs/2512.16445)
*Roman Akramov,Artem Khamatullin,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 将LDA的主题数T选择视为离散的黑盒优化问题，在固定评估预算下比较两类传统进化算法（GA、ES）与两种学习得到的摊销式优化器（PABBO、SABBO）；结果显示摊销式方法在样本和时间效率上明显优于进化算法，SABBO常在几乎一次评估内给出近优解，PABBO也很快，而GA/ES通常要用尽预算才能接近同一区域。


<details>
  <summary>Details</summary>
Motivation: 主题数T对主题模型的统计拟合与可解释性影响显著，但穷举或网格搜索代价高；因此将T选择问题建模为黑盒优化以在有限评估预算内更高效地找到合适的T。

Method: 把每次函数评估定义为训练一个LDA并测量验证集perplexity。比较四类优化器：两种手工设计的进化方法（遗传算法GA与进化策略ES）和两种学习得到的摊销式方法（PABBO与SABBO），在相同评估预算下比较收敛速度、样本效率与最终perplexity。

Result: 最终四种方法收敛到相近的perplexity区间，但摊销式优化器显著更快：SABBO通常在几乎一次评估内找到近优T，PABBO在少数评估内给出有竞争力的配置，而GA与ES需要接近满预算才能达到类似性能。

Conclusion: 在有限评估预算下，摊销式黑盒优化器（尤其是SABBO）在为LDA选择主题数T时更为高效；但应注意摊销方法依赖于训练/先验数据和泛化能力，其预训练成本与在不同数据集上的稳健性需在实际应用中评估。

Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.

</details>


### [100] [Persistent Multiscale Density-based Clustering](https://arxiv.org/abs/2512.16558)
*Daniël Bot,Leland McInnes,Jan Aerts*

Main category: cs.LG

TL;DR: The paper proposes PLSCAN, a density-based clustering algorithm that identifies all minimum cluster sizes for which HDBSCAN* yields stable leaf clusters, using scale-space clustering and an equivalence to persistent homology on a novel metric. It shows improved ARI and robustness compared to HDBSCAN* and competitive runtimes to k-Means in low dimensions.


<details>
  <summary>Details</summary>
Motivation: Density-based clustering is useful for exploratory data analysis but requires hard-to-tune hyperparameters (e.g., DBSCAN density threshold, HDBSCAN* min cluster size). The work aims to remove or reduce hyperparameter sensitivity by automatically finding cluster-relevant scales.

Method: Introduce Persistent Leaves Spatial Clustering (PLSCAN), which applies scale-space clustering principles and constructs a novel metric space where persistent homology identifies stable leaf clusters across minimum cluster sizes. PLSCAN efficiently enumerates all minimum cluster sizes that produce stable clusters, leveraging mutual reachability concepts.

Result: On several real-world datasets, PLSCAN achieves higher average Adjusted Rand Index (ARI) than HDBSCAN* and shows lower sensitivity to the number of mutual reachability neighbours. Runtime is competitive with k-Means for low-dimensional data; at higher dimensions, runtime behavior becomes similar to HDBSCAN*.

Conclusion: PLSCAN provides a principled, persistence-based way to detect density-based clusters with reduced hyperparameter tuning, improving clustering stability and accuracy in experiments while maintaining reasonable computational cost in low dimensions.

Abstract: Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.

</details>


### [101] [Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling](https://arxiv.org/abs/2512.16581)
*Sullivan Castro,Artem Betlei,Thomas Di Martino,Nadir El Manouzi*

Main category: cs.LG

TL;DR: 提出Abacus：一种自监督预训练方法，通过预测用户事件的经验频率分布来增强深度序列模型，并将该目标与序列学习目标混合，改善广告点击/购买预测的稀疏性与时序性问题。


<details>
  <summary>Details</summary>
Motivation: 展示在展示广告实时竞价场景中，用户购买事件稀疏且时序不规则；现有系统依赖人工统计特征或直接序列模型，分别忽略时序细节或计数统计的稳定性。提出预训练以结合两者优点。

Method: 设计Abacus任务：预测用户事件的经验频率分布（即在某时间窗口/历史中事件计数的分布），对序列模型进行自监督预训练；并提出一个混合目标，将Abacus的聚合统计稳定性与序列学习的敏感性统一到下游监督任务中。

Result: 在两个真实业务数据集上，Abacus预训练比已有方法更快收敛；混合目标在AUC上相较基线最多提升+6.1%。

Conclusion: 通过用事件计数分布作为自监督信号，Abacus为序列模型提供了稳定的全局统计信息，混合训练在保持时序敏感性的同时提升预测性能和收敛速度。

Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.

</details>


### [102] [Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game](https://arxiv.org/abs/2512.16626)
*Barna Pásztor,Thomas Kleine Buening,Andreas Krause*

Main category: cs.LG

TL;DR: 提出SLHF，将偏好优化建模为领导者-追随者的序贯博弈，利用追随者的推理期细化实现推理时改进，声称在一致性、对数据敏感性和处理非传递偏好上优于RLHF和NLHF，在多尺度模型和数据集上有实证结果并能迁移细化策略。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF把人类偏好压缩为标量奖励，或NLHF寻求同时移动平衡，难以捕捉更复杂的偏好结构和利用推理时细化的机会；SLHF借助序贯不对称性分解问题，旨在更好地建模和优化人类偏好。

Method: 定义Leader先行动并承诺，Follower在观测Leader动作后有条件响应；将Follower训练为‘细化器’，Leader在对抗性的目标上优化；通过迭代采样在推理时利用Follower的细化能力。

Result: 在多种偏好数据集和从0.5B到8B参数的模型上，SLHF取得较强的对齐效果，且推理期细化能在不同模型家族间迁移，无需额外微调。

Conclusion: SLHF是一种能够捕捉更丰富偏好的序贯框架，兼具训练与推理期优势，并在实验中展示了优越性与可迁移性。

Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.

</details>


### [103] [Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification](https://arxiv.org/abs/2512.16687)
*Natnael Tilahun Sinshaw,Mengmei He,Tadesse K. Bahiru,Sudhir Kumar Mohapatra*

Main category: cs.LG

TL;DR: 本文比较了SVM、NB、LR、AdaBoost、XGBoost、SVM_R等传统机器学习与神经符号AI（NeSy）及深度学习模型在文本分类（如博客性别分类）上的表现，并考察了TF-IDF、USE、RoBERTa等表示与若干特征选择方法。结果表明在小数据集下NeSy能达到与强MLP相当的效果。


<details>
  <summary>Details</summary>
Motivation: 文本分类是成熟且应用广泛的任务；研究者希望评估神经符号AI在资源受限场景与传统/深度模型之间的相对优势与局限。

Method: （重复项已合并）实验包括模型训练、不同特征与表示的比较、以及对NeSy方法性能的测量。

Result: NeSy在所用的小规模数据集上性能接近强MLP，证明NeSy在数据不足时仍具一定竞争力；但论文未明确给出详细指标、统计显著性或数据集规模的完整细节。

Conclusion: NeSy为小样本文本分类提供了可行路径。未来工作应扩展知识库、更多嵌入类型与超参搜索以更全面评估NeSy有效性。

Abstract: Text classification problems, such as gender classification from a blog, have been a well-matured research area that has been well studied using machine learning algorithms. It has several application domains in market analysis, customer recommendation, and recommendation systems. This study presents a comparative analysis of the widely used machine learning algorithms, namely Support Vector Machines (SVM), Naive Bayes (NB), Logistic Regression (LR), AdaBoost, XGBoost, and an SVM variant (SVM_R) with neuro-symbolic AI (NeSy). The paper also explores the effect of text representations such as TF-IDF, the Universal Sentence Encoder (USE), and RoBERTa. Additionally, various feature extraction techniques, including Chi-Square, Mutual Information, and Principal Component Analysis, are explored. Building on these, we introduce a comparative analysis of the machine learning and deep learning approaches in comparison to the NeSy. The experimental results show that the use of the NeSy approach matched strong MLP results despite a limited dataset. Future work on this research will expand the knowledge base, the scope of embedding types, and the hyperparameter configuration to further study the effectiveness of the NeSy approach.

</details>


### [104] [Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation](https://arxiv.org/abs/2512.16718)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 提出一种由多段（packages）多调和样条（polyharmonic splines）构成的级联（cascade）架构，用以克服先前基于随机函数理论的回归解在计算复杂度（O(N^3)）和高维输入时理论假设失效的问题，同时对未知的低维流形有理论保证，并给出高效的矩阵前向计算与端到端求导方法。


<details>
  <summary>Details</summary>
Motivation: 先前工作在随机函数理论框架下给出了回归问题的解析最优核（多调和样条），但直接应用受限于三次方级别的计算开销以及当输入空间维度过高时原始理论假设失效。需要一个既可扩展又能在高维或本征低维情况下保持理论合理性的方案。

Method: 提出把多调和样条按包（packages）组织成级联架构：通过将样本/基函数分组并串联多个层次的局部拟合来降低复杂度与提高表达能力。同时给出对前向计算的高效矩阵操作与用于梯度传递的端到端微分流程，保证可训练性并适用于现代自动微分框架。该设计在理论上对具有未知本征低维度（intrinsic low dimensionality）的问题有正当性。

Result: 理论上证明或论证该级联结构同时改善了可扩展性并维持了基于多调和样条的最优核近似，对于本征低维问题有保证。工程上给出了高效的矩阵化实现细节，使得前向推断和反向梯度计算都可在较低成本下完成（避免直接O(N^3)开销）。

Conclusion: 级联的多调和样条包为将随机函数理论得出的解析核方法推广到大规模与高维问题提供了可行路径。该方法在理论与实现上都具有优势，适合需要保持解析核性质同时追求可扩展性的回归任务。

Abstract: In a previous paper it was shown that a machine learning regression problem can be solved within the framework of random function theory, with the optimal kernel analytically derived from symmetry and indifference principles and coinciding with a polyharmonic spline. However, a direct application of that solution is limited by O(N^3) computational cost and by a breakdown of the original theoretical assumptions when the input space has excessive dimensionality. This paper proposes a cascade architecture built from packages of polyharmonic splines that simultaneously addresses scalability and is theoretically justified for problems with unknown intrinsic low dimensionality. Efficient matrix procedures are presented for forward computation and end-to-end differentiation through the cascade.

</details>


### [105] [Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis](https://arxiv.org/abs/2512.16742)
*Wisnu Uriawan,Muhamad Veva Ramadhan,Firman Adi Nugraha,Hasbi Nur Wahid,M Dantha Arianvasya,Muhammad Zaki Alghifari*

Main category: cs.LG

TL;DR: 提出并评估基于机器学习的系统以自动验证印度尼西亚朝觐（Hajj 与 Umrah）相关移动应用真伪，结合应用描述的 TF-IDF 文本特征与敏感权限元数据，比较 SVM、随机森林与朴素贝叶斯三种分类器的表现。


<details>
  <summary>Details</summary>
Motivation: 移动化服务加速普及的同时带来伪造应用的金融与隐私风险，需要自动化、可扩展的方法来鉴别官方与非官方宗教旅游应用以保护朝圣者数据与财产安全。

Method: 构建包含官方（由宗教事务部注册）与非官方应用的综合数据集，提取文本（应用描述 TF-IDF）与元数据（敏感访问权限）混合特征，训练并比较 SVM、随机森林、朴素贝叶斯三种分类器，进行特征重要性分析以识别关键区分因素。

Result: SVM 表现最佳：准确率 92.3%，精确率 91.5%，F1-score 92.0%。关键区分特征包括与合法性相关的关键词与高风险权限（如 READ_PHONE_STATE）。

Conclusion: 该系统可作为提高宗教旅游数字信任的前瞻性、可扩展解决方案，有潜力作为国家级应用验证系统的原型，建议进一步的实时检测与模型更新机制以应对应用生态演化。

Abstract: The rapid digitalization of Hajj and Umrah services in Indonesia has significantly facilitated pilgrims but has concurrently opened avenues for digital fraud through counterfeit mobile applications. These fraudulent applications not only inflict financial losses but also pose severe privacy risks by harvesting sensitive personal data. This research aims to address this critical issue by implementing and evaluating machine learning algorithms to verify application authenticity automatically. Using a comprehensive dataset comprising both official applications registered with the Ministry of Religious Affairs and unofficial applications circulating on app stores, we compare the performance of three robust classifiers: Support Vector Machine (SVM), Random Forest (RF), and Na"ive Bayes (NB). The study utilizes a hybrid feature extraction methodology that combines Textual Analysis (TF-IDF) of application descriptions with Metadata Analysis of sensitive access permissions. The experimental results indicate that the SVM algorithm achieves the highest performance with an accuracy of 92.3%, a precision of 91.5%, and an F1-score of 92.0%. Detailed feature analysis reveals that specific keywords related to legality and high-risk permissions (e.g., READ PHONE STATE) are the most significant discriminators. This system is proposed as a proactive, scalable solution to enhance digital trust in the religious tourism sector, potentially serving as a prototype for a national verification system.

</details>


### [106] [NRGPT: An Energy-based Alternative for GPT](https://arxiv.org/abs/2512.16762)
*Nima Dehmamy,Benjamin Hoover,Bishwajit Saha,Leo Kozachkov,Jean-Jacques Slotine,Dmitry Krotov*

Main category: cs.LG

TL;DR: 提出将GPT最小修改以融入能量模型（EBM）框架，形成eNeRgy-GPT（NRGPT）；推导并实证在某些条件下推断等价于梯度下降；在多种语言/合成任务上取得良好表现，并显示更强的过拟合耐受性。


<details>
  <summary>Details</summary>
Motivation: 统一自回归Transformer与能量模型的思想，使推断被解释为在能量景观上的动力学探索，从而获得更多推断灵活性与潜在的泛化/稳健性优势。

Method: 对GPT做最小改动以构造能量函数，将推断视为在离散token能量景观上的探索（NRGPT）；证明在特定条件下该探索过程可被近似为梯度下降，并在多项任务上通过训练/推断验证该方法。

Result: 在Shakespeare、ListOPS和OpenWebText等任务上表现良好；理论推导与实验结果一致；观察到训练过程中对过拟合更为耐受（仅在很长训练后才过拟合）。

Conclusion: NRGPT为将自回归Transformer与能量模型连接提供了一种可行的、简单的方案，具有理论可解释性和一定的实证优势，但在最优性能和计算复杂性方面存在权衡。

Abstract: Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.

</details>


### [107] [Pattern recognition in complex systems via vector-field representations of spatio-temporal data](https://arxiv.org/abs/2512.16763)
*Ingrid Amaranta Membrillo Solis,Maria van Rossem,Tristan Madeleine,Tetiana Orlova,Nina Podoliak,Giampaolo D'Alessandro,Jacek Brodzki,Malgosia Kaczmarek*

Main category: cs.LG

TL;DR: 提出一种基于离散测度空间上向量场理论的几何框架，并给出一族二参数度量，用于分析复杂系统的时空数据；结合多维尺度分析（MDS），在数值模拟数据上验证能实现降维、模态分解、相空间重构与吸引子表征。


<details>
  <summary>Details</summary>
Motivation: 复杂系统具有高维非线性动力学，传统降维、相空间重构和吸引子刻画在海量时空数据面前受限。需要一种统一且可扩展的几何度量工具，能处理时间相关图像、梯度以及定义在图或单纯形复形上的标量/向量场，从而支持数据驱动的分析与机器学习应用。

Method: 建立在离散测度空间向量场理论上的几何框架，构造一族含两个参数的度量，适用于时变图像、图/单纯形上的实值或向量值函数及其梯度。将该度量与多维尺度分析等降维方法结合，用于模态分解、相空间构建和吸引子分析。

Result: 在平面与曲面域上的生物和物理系统数值模拟数据上进行验证。结果表明所提度量与MDS能有效地降维、提取主导模态、重构相空间并刻画吸引子结构，克服了传统方法在处理复杂时空数据时的若干局限。

Conclusion: 该方法为在难以建立精确解析模型但数据丰富的复杂动力学系统提供了一条稳健的数据驱动分析路径，具有广泛的适用性与潜在的机器学习集成价值。

Abstract: A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth's climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.

</details>


### [108] [MEPIC: Memory Efficient Position Independent Caching for LLM Serving](https://arxiv.org/abs/2512.16822)
*Qian Wang,Zahra Yousefijamarani,Morgan Lindsay Heisler,Rongzhi Gu,Bai Xiaolong,Shan Yizhou,Wei Zhang,Wang Lan,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: MEPIC通过页对齐KV布局、块级重算和在注意力核中融合RoPE，实现跨位置、请求和批次的块级KV共享，从而在不改模型的前提下将HBM占用在常见PIC上减少约2倍（长提示下可达5倍），延迟与精度可比。


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用在长提示历史中重复处理相同文档/代码片段，KV缓存受限于HBM内存且需保持高吞吐和低延迟；现有前缀缓存/位置无关缓存（PIC）因位置编码、按请求重算和内存页未对齐导致大量KV重复，难以充分节省内存。

Method: MEPIC对齐块级KV到页边界，改为块（block）级重算使仅首块与请求相关，在注意力核内将RoPE与KV融合以消除显式位置编码，剩余块可在请求间共享；总体避免不同请求产生的KV内存布局分歧，支持跨批次页共享。

Result: 在无需更改模型的条件下，MEPIC比现有PIC在HBM占用上节省约2倍，长提示场景下可达5倍，且在延迟和输出精度上与基线可比。

Conclusion: MEPIC提出了工程性强且可部署的方案，通过页对齐、块级重算与RoPE融合解决了PIC的重复KV与页共享问题，显著降低HBM使用，适合RAG、代码助手等长提示密集型应用。

Abstract: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.

</details>


### [109] [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866)
*Jiabin Xue*

Main category: cs.LG

TL;DR: 本文提出Knowledge Transformation (KT)，将知识蒸馏、主动学习与因果推理结合，在在线边缘机器学习场景中由教师模型生成伪标签以训练学生模型，从而应对未来未见样本的标注问题。模拟实验表明：当教师模型稳定时，学生模型可达到预期最大性能。


<details>
  <summary>Details</summary>
Motivation: 在线边缘ML需在边缘设备上持续更新模型，但难点在于如何为真正未来的、未见的数据点确定标签，尤其当人工标注昂贵或任务标签难以获得时。作者提出用已有教师模型作为“活性学习中的oracle”以生成伪标签来缓解这一问题。

Method: 提出Knowledge Transformation (KT)：以教师模型输出作为知识源，通过知识蒸馏将其转移给学生模型；结合主动学习策略选择有信息量的数据进行标注（此处由教师生成伪标签）；并引入因果推理成分以提升标签的可靠性与鲁棒性。进行了两类模拟实验——不稳定教师与相对稳定教师，比较学生模型随时间的表现。

Result: 实验结果显示：在给定相对稳定的教师模型时，学生模型最终能接近教师或达到其预期最大性能；而在教师不稳定时，效果受限，说明KT对教师质量敏感。

Conclusion: KT为在线边缘ML中无法直接获得真实标签的情形提供了可行方案，尤其适合教师任务通用且可用预训练模型，或学生任务标注昂贵的场景。但方法依赖教师稳定性，且需更多真实世界实验验证其因果推理模块与主动学习策略的实际效益。

Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.

</details>


### [110] [Sequencing to Mitigate Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2512.16871)
*Hesham G. Moussa,Aroosa Hameed,Arashmid Akhavain*

Main category: cs.LG

TL;DR: 本文提出通过智能任务排序（使用受NAS启发的零样本评分算法）来缓解连续学习中的灾难性遗忘，结果显示排序能明显降低遗忘并与传统方法结合提升性能。


<details>
  <summary>Details</summary>
Motivation: 连续学习中灾难性遗忘严重影响长期学习能力。现有方法多分类集中在重放、正则化、优化、表示和架构层面，本文从任务呈现顺序角度出发，探究是否能通过优化任务序列减轻遗忘。

Method: 提出一种基于零样本任务评分的任务排序方法，受到神经结构搜索（NAS）启发，用评分机制估计不同任务顺序对模型遗忘的影响并选择最优序列。方法可与现有连续学习策略结合。

Result: 实验表明智能任务排序能显著降低灾难性遗忘；与传统连续学习方法结合时，整体表现和鲁棒性进一步提升。作者还指出该方法可扩展到课程学习等其他应用。

Conclusion: 任务序列本身是影响持续学习性能的重要因素。通过自动化、零样本的排序策略，可以在不改动模型结构或记忆机制的情况下减少遗忘，并补强现有方法。该思路为持续学习和课程学习提供了新的研究方向。

Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.

</details>


### [111] [Impacts of Racial Bias in Historical Training Data for News AI](https://arxiv.org/abs/2512.16901)
*Rahul Bhargava,Malene Hornstrup Jespersen,Emily Boardman Ndulue,Vivica Dsouza*

Main category: cs.LG

TL;DR: 本文分析了基于《纽约时报注释语料库》训练的多标签分类器中出现的“blacks”主题标签，发现该标签在模型中部分充当广义“种族主义检测器”，但对现代反亚裔仇恨案件和黑人的民权运动报道表现不佳，揭示在新闻室使用 AI 工具时可能再现历史偏见的风险。


<details>
  <summary>Details</summary>
Motivation: AI 模型训练于历史文本，可能编码过时或偏见的观念；研究动机是探究这些偏见如何在新闻相关的多标签分类任务中体现，及其对新闻工作流程（如故事发现、受众定位、摘要等）的影响。

Method: 对训练语料库中“blacks”标签的出现情况进行定量分析，并结合定性语料审查；使用可解释 AI 方法（如特征重要性、注意力/激活可视化或类激活映射）分析模型在不同新时代案例（COVID-19 反亚裔仇恨、Black Lives Matter 等）上的行为。

Result: 发现“blacks”标签在训练数据中既包含对黑人的直接报道也包含带有种族主义话语的内容。模型学到的概念部分泛化为对多种少数群体的“种族主义指示器”，但对现代、语境化的反亚裔或 BLM 报道识别不足，导致错误或遗漏。

Conclusion: 在新闻室部署 AI 工具前需进行细致的偏见审查和可解释性分析；建议结合多样化语料、标签细化、后处理校正和人工审查以降低历史偏见的再现风险。

Abstract: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning "blacks" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the "blacks" label operates partially as a general "racism detector" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.

</details>


### [112] [Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning](https://arxiv.org/abs/2512.16911)
*Andrew Wagenmaker,Perry Dong,Raymond Tsao,Chelsea Finn,Sergey Levine*

Main category: cs.LG

TL;DR: 提出Posterior Behavioral Cloning（PostBC），通过学习示范者行为的后验分布以保证对示范动作的覆盖，从而作为强化学习微调的更好初始化。理论证明BC可能不保证覆盖，PostBC能保证覆盖且不降低预训练表现；实验证明在机器人控制与真实操控任务中显著提升微调效果。


<details>
  <summary>Details</summary>
Motivation: 当前常用先基于示范数据预训练策略再用RL微调的方法，微调性能依赖预训练策略作为初始化的质量。但现有工作多关注改进微调算法，忽视如何构造更适合微调的预训练策略，且BC可能无法满足微调所需的动作覆盖性。

Method: 理论分析表明BC可能失败保证对示范动作的覆盖。提出PostBC：不是精确拟合示范动作，而是学习给定示范数据下示范者行为的后验分布，从而保证覆盖性。PostBC可用现代生成模型并通过标准监督学习实现。

Result: 理论证明PostBC能确保对示范动作的覆盖且预训练表现不弱于BC。实验在现实机器人控制基准和真实机器人操作任务上表明，使用PostBC预训练的策略在RL微调时性能显著优于标准BC预训练策略。

Conclusion: 通过学习示范者行为后验分布获得的预训练策略能作为更有效的RL微调初始化，提升下游微调性能；PostBC兼具理论保证与实际可实现性，在机器人控制领域表现优异。

Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

</details>


### [113] [Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward](https://arxiv.org/abs/2512.16912)
*Peter Chen,Xiaopeng Li,Ziniu Li,Wotao Yin,Xi Chen,Tianyi Lin*

Main category: cs.LG

TL;DR: The paper studies why both spurious rewards and entropy minimization can improve LLM reasoning under RL with verifiable rewards. It finds that spurious rewards induce clipping bias that reduces policy entropy (making outputs more deterministic), while entropy minimization alone does not explain the performance gains. A reward-misalignment model is proposed to account for spurious-reward benefits beyond contaminated settings.


<details>
  <summary>Details</summary>
Motivation: Resolve the apparent paradox that both discouraging exploitation (via spurious rewards) and discouraging exploration (via entropy minimization) improve reasoning in RLVR, and understand how policy entropy and spurious rewards affect performance.

Method: Analyze policy entropy–performance relationships theoretically and empirically; investigate clipping bias and model contamination as mechanisms; develop a reward-misalignment model to explain gains from spurious rewards; run experiments to test entropy effects and spurious-reward behavior.

Result: Clipping bias from spurious rewards reduces policy entropy and produces more confident/deterministic outputs. Entropy minimization by itself does not yield the observed performance improvements. The reward-misalignment model explains why spurious rewards can help even when contamination is not present.

Conclusion: Spurious-reward benefits arise partly through clipping-induced entropy reduction and through reward misalignment effects; these insights clarify when and how to use spurious rewards in RLVR and guide better RLVR training strategies.

Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.

</details>
