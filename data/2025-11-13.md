<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [cs.IT](#cs.IT) [Total: 5]
- [cs.CR](#cs.CR) [Total: 14]
- [eess.SY](#eess.SY) [Total: 14]
- [cs.LG](#cs.LG) [Total: 65]
- [cs.NI](#cs.NI) [Total: 3]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Dynamic and Static Energy Efficient Design of Pinching Antenna Systems](https://arxiv.org/abs/2511.08720)
*Saba Asaad,Chongjun Ouyang,Ali Bereyhi,Zhiguo Ding*

Main category: eess.SP

TL;DR: 提出一个一致的功率分布框架来评估PASS（pinching-antenna systems）的能效，通过调节pinching位置和耦合长度实现能量分配控制，并提出一类混合动态-静态算法以在不同速率下最大化能效；实验表明动态调节pinching位置能显著提升能效。


<details>
  <summary>Details</summary>
Motivation: PASS的每天功率分布并非通过显式的功率分配策略直接控制，而是通过可调的pinching耦合和位置隐式控制。pinching位置的移动调节具有较高的时变性，但耦合长度的调节更新速率较低，需在不同更新速率下设计控制策略以提升能效。

Method: 建立一套一致的功率分布表述，分析两类可调参数（1）可移动元件实现的pinching位置、（2）通过pinching元件的有效耦合长度实现的耦合调节，提出一类在不同速率下工作 的混合动态-静态算法以最大化能效。通过实验评估动态调节位移对能效的影响。

Result: 提出的混合动态-静态算法能够在不同更新速率下实现能效最大化；实验结果显示，动态调节pinching位置对提升PASS的能效具有显著作用。

Conclusion: 对PASS而言，动态调节pinching位置是提升能效的关键因素；耦合长度的快速调节受限，需要结合静态与动态策略以实现更优的能效提升。未来工作可进一步优化调参机制并扩展到更复杂的场景。

Abstract: We study the energy efficiency of pinching-antenna systems (PASSs) by developing a consistent formulation for power distribution in these systems. The per-antenna power distribution in PASSs is not controlled explicitly by a power allocation policy, but rather implicitly through tuning of pinching couplings and locations. Both these factors are tunable: (i) pinching locations are tuned using movable elements, and (ii) couplings can be tuned by varying the effective coupling length of the pinching elements. While the former is feasible to be addressed dynamically in settings with low user mobility, the latter cannot be addressed at a high rate. We thus develop a class of hybrid dynamic-static algorithms, which maximize the energy efficiency by updating the system parameters at different rates. Our experimental results depict that dynamic tuning of pinching locations can significantly boost energy efficiency of PASSs.

</details>


### [2] [Linear-Bias Time Encoding for Low-Rate Quantized Representation of Bandlimited Signals](https://arxiv.org/abs/2511.09007)
*Anshu Arora,Kaluguri Yashaswini,Satish Mulleti*

Main category: eess.SP

TL;DR: 提出一种线性偏置IF-TEM（LB-IF-TEM），通过动态跟踪输入来维持积分器输入的大致恒定，从而集中发射间隔，减少过采样并实现低比特率下的高保真重构。


<details>
  <summary>Details</summary>
Motivation: 传统的IF-TEM在编码带限信号时往往发生过度采样，导致信息分布平滑时的编码效率低下；需要一种更高效、功耗低且可分析的时基编码框架。

Method: 提出LB-IF-TEM，其中偏置随输入动态跟踪，以保持积分器输入近似恒定，从而将发射间隔聚集为局部化分布，并实现对非均匀量化的有效支持。理论分析给出可达到的过采样范围界限，实验结果表明在显著降低比特率的情况下仍可获得与现有IF-TEM变体相当的重构精度。

Result: 给出了关于过采样范围的显式界限；实验验证了在较低比特率下仍能达到与现有方法相近的重构误差。

Conclusion: LB-IF-TEM提供了一种低功耗、通信高效且解析性强的时基信号编码与重构框架。

Abstract: Integrate-and-fire time encoding machines (IF-TEMs) provide an efficient framework for asynchronous sampling of bandlimited signals through discrete firing times. However, conventional IF-TEMs often exhibit excessive oversampling, leading to inefficient encoding for signals with smoothly distributed information. This letter introduces a linear-bias IF-TEM (LB-IF-TEM), where the bias dynamically tracks the input signal to maintain a nearly constant integrator input, thereby localizing the firing intervals. The resulting concentrated distribution enables effective non-uniform quantization with reduced distortion. Theoretical analysis establishes explicit bounds on the achievable oversampling range, while experimental results demonstrate that the proposed method attains comparable reconstruction accuracy at significantly lower bitrate than existing IF-TEM variants. The LB-IF-TEM thus provides a low-power, communication-efficient, and analytically tractable framework for time-based signal encoding and reconstruction.

</details>


### [3] [Equivalence of Several 6G Modulation Schemes for Doubly-Selective Channels](https://arxiv.org/abs/2511.09418)
*Nishant Mehrotra,Sandesh Rao Mattu,Robert Calderbank*

Main category: eess.SP

TL;DR: 提出一个以非选择性与可预测性为核心的框架，比较在大延迟-多普勒扩展的双选性信道中的调制方案，发现延迟-多普勒、啁啶和时间-序列域的调制彼此等价且非选择性、可预测性高，而时频域调制则具备选择性且不可预测性低，暗示后者在此场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 面对具有大延迟和多普勒扩展的 doubly-selective 信道，传统基于时频表示的调制在性能上受限，需建立一个直接关联多样性与频谱效率的分析框架来评估不同表示下的调制。

Method: 提出以非选择性和可预测性两个特性来分析调制，在延迟-多普勒、啁啶、时间-序列等表示下进行比较，证明它们之间的等价性和对比于时频域的差异。

Result: 延迟-多普勒、啁啶、时间-序列域的调制在该框架下表现为非选择性、可预测且彼此等价；相比之下，时频域调制表现为选择性、不可预测。

Conclusion: 建立了一个统一的分析视角，提示在双选性信道条件下应优先采用非选择性、可预测的表示（如延迟-多普勒、啁啶、时间-序列），以提升多样性利用与频谱效率，避免传统时频表示的局限。

Abstract: There is significant recent interest in designing new modulation schemes for doubly-selective channels with large delay and Doppler spreads, where legacy modulation schemes based on time-frequency signal representations do not perform well. In this paper, we develop a framework for analyzing such modulations using two characteristics -- non-selectivity and predictability -- which directly relate to the diversity and spectral efficiency that the modulations achieve. We show that modulations in the delay-Doppler, chirp and time-sequency domains are non-selective, predictable and equivalent to one another, whereas time-frequency modulations are selective and non-predictable.

</details>


### [4] [VAE-Based Synthetic EMG Generation with Mix-Consistency Loss for Recognizing Unseen Motion Combinations](https://arxiv.org/abs/2511.09060)
*Itsuki Yazawa,Akira Furui*

Main category: eess.SP

TL;DR: 提出了一种基于变分自编码器(VAE)的EMG混合动作合成方法，利用混合一致性损失在结构化的潜在空间中将组合动作嵌入于基本动作之间，并在该潜在空间中生成合成数据以训练分类器，从而提升对未见组合动作的识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决以往将组合动作简化为基本动作线性组合的假设在肌肉共收缩等现象下导致合成信号低保真、分类性能下降的问题；需要在潜在空间中生成更高保真、具有效结构的合成数据以提升泛化能力。

Method: 使用变分自编码器将EMG信号编码到低维潜在空间，并引入混合一致性(mixconsistency)损失以将组合动作嵌入于其组成基本动作之间。然后在该结构化潜在空间内进行合成，并使用合成数据训练分类器以识别未见的组合动作。

Result: 在八名健康参与者的上肢动作分类实验中，该方法相比输入空间合成方法实现了约30%的准确率提升。

Conclusion: 在结构化潜在空间中进行混合式动作的合成可获得更高保真度的训练数据，显著提升EMG基于动作分类在未见组合动作上的泛化性能。

Abstract: Electromyogram (EMG)-based motion classification using machine learning has been widely employed in applications such as prosthesis control. While previous studies have explored generating synthetic patterns of combined motions to reduce training data requirements, these methods assume that combined motions can be represented as linear combinations of basic motions. However, this assumption often fails due to complex neuromuscular phenomena such as muscle co-contraction, resulting in low-fidelity synthetic signals and degraded classification performance. To address this limitation, we propose a novel method that learns to synthesize combined motion patterns in a structured latent space. Specifically, we employ a variational autoencoder (VAE) to encode EMG signals into a low-dimensional representation and introduce a mixconsistency loss that structures the latent space such that combined motions are embedded between their constituent basic motions. Synthetic patterns are then generated within this structured latent space and used to train classifiers for recognizing unseen combined motions. We validated our approach through upper-limb motion classification experiments with eight healthy participants. The results demonstrate that our method outperforms input-space synthesis approaches, achieving approximately 30% improvement in accuracy.

</details>


### [5] [LMMSE-Optimal Pilot Pattern Design Based on Covariance Matrix Approximation for OFDM Channel Estimation in Doubly Dispersive Channel](https://arxiv.org/abs/2511.09140)
*Xuyao Yu,Zijun Gong,Zhilu Lai*

Main category: eess.SP

TL;DR: 提出在双多径OFDM信道中，基于Kronecker-Toeplitz协方差与Szegö极限定理实现DFT对角化的近似，给出LMMSE信道估计的闭式下界及可达到该下界的格点导频模式，并通过数值验证近似误差微小。


<details>
  <summary>Details</summary>
Motivation: 在双多径信道下设计导频模式以优化LMMSE估计，同时保持解析可行性与计算简单性。

Method: 将信道协方差表示为延迟和多普勒域的Hermitian Toeplitz矩阵的Kronecker积；利用Szegö极限定理近似由DFT矩阵对角化；据此推导LMMSE估计误差的紧凑解析形式，得到闭式下界；给出在格点导频模式下实现该下界的充要条件。

Result: 得到一个闭式下界；证明格点导频模式可实现该下界；数值结果表明矩阵近似造成的误差可忽略，给出若干导频格的示例。

Conclusion: 所提出的结构化设计为双向分散信道的导频设计提供分析工具与实用模式，格点化设计可在理论下界附近实现接近最优的性能，数值验证支持近似的有效性。

Abstract: This paper investigates the optimal pilot pattern design, in the linear minimum mean square error (LMMSE) estimator sense, for OFDM systems in doubly dispersive channels. To enable analytical tractability, the channel covariance matrix is decomposed into the Kronecker product of two Hermitian Toeplitz matrices corresponding to the delay and Doppler domains. By invoking the Szegö limit theorem, these matrices are shown to be approximately diagonalizable by discrete Fourier transform (DFT) matrices. Based on this structure, the LMMSE channel estimation error is reformulated into a compact analytical form, from which a closed-form lower bound is derived. Furthermore, we establish the condition under which this bound is achieved by a lattice-based pilot pattern. Numerical results verify that the proposed matrix approximation introduces negligible error and examples of the proposed lattice design are given.

</details>


### [6] [Mip-NeWRF: Enhanced Wireless Radiance Field with Hybrid Encoding for Channel Prediction](https://arxiv.org/abs/2511.09150)
*Yulin Fu,Jiancun Fan,Shiyu Zhai,Zhibo Duan,Jie Luo*

Main category: eess.SP

TL;DR: 提出 Mip-NeWRF：一个基于物理信息的神经框架，用于室内信道预测，采用分层采样、尺度一致的混合位置编码、课程学习等，在仿真中显著超越基线（NMSE 提升约 14.3 dB）。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中无线辐射场预测鲁棒性不足、收敛慢、精度有限；需要结合物理先验与高效的采样与编码来提高预测准确性与收敛速度。

Method: 基于射线管线的粗到细重要性采样：对锥台截取样本进行编码，由共享 MLP 处理，再合成为 CFR；事前进行 conical-frustum sampling，使用尺度一致混合位置编码；训练采用课程学习；在信道合成阶段，MLP 输出的虚拟发射器存在概率和幅度与路径损耗、表面衰减等物理量结合以增强保物理性。

Result: 在典型场景中，NMSE 相比最先进基线下降约 14.3 dB。

Conclusion: 证明了所提框架在室内信道预测方面的有效性，兼具鲁棒性、加速收敛和更高的精度。

Abstract: Recent work on wireless radiance fields represents a promising deep learning approach for channel prediction, however, in complex environments these methods still exhibit limited robustness, slow convergence, and modest accuracy due to insufficiently refined modeling. To address this issue, we propose Mip-NeWRF, a physics-informed neural framework for accurate indoor channel prediction based on sparse channel measurements. The framework operates in a ray-based pipeline with coarse-to-fine importance sampling: frustum samples are encoded, processed by a shared multilayer perceptron (MLP), and the outputs are synthesized into the channel frequency response (CFR). Prior to MLP input, Mip-NeWRF performs conical-frustum sampling and applies a scale-consistent hybrid positional encoding to each frustum. The scale-consistent normalization aligns positional encodings across scene scales, while the hybrid encoding supplies both scale-robust, low-frequency stability to accelerate convergence and fine spatial detail to improve accuracy. During training, a curriculum learning schedule is applied to stabilize and accelerate convergence of the shared MLP. During channel synthesis, the MLP outputs, including predicted virtual transmitter presence probabilities and amplitudes, are combined with modeled pathloss and surface interaction attenuation to enhance physical fidelity and further improve accuracy. Simulation results demonstrate the effectiveness of the proposed approach: in typical scenarios, the normalized mean square error (NMSE) is reduced by 14.3 dB versus state-of-the-art baselines.

</details>


### [7] [Characterizing ISCI in Multi-carrier ISAC Systems over Doubly Dispersive Channel: Joint Sensing and Communication Performance Analysis](https://arxiv.org/abs/2511.09163)
*Xuyao Yu,Zijun Gong,Zhilu Lai*

Main category: eess.SP

TL;DR: 给出一句话概述：提出基于广义 OFDM 的 Weyl–Heisenberg 框架，系统评估四种 ISCI 处理策略，在双多径信道下给出 LMMSE 估计、感知与容量指标，并揭示性能与复杂度的权衡，为 ISAC 波形与接收机设计提供指导。


<details>
  <summary>Details</summary>
Motivation: ISAC 系统在双分散信道中需要同时完成感知与通信任务，ISCI 的建模和评估缺乏统一的框架，需比较不同处理策略对感知与通信性能的影响。

Method: 建立通过 Weyl–Heisenberg OFDM 框架的延迟-多普勒信道模型；系统评估四种 ISCI 处理路径：显式估计与补偿、完全 ignorance、非相关彩色噪声近似、相关彩色噪声建模；推导 Pilot‑assisted 与全符号已知情形下的 LMMSE 信道估计器及其误差（作 sensing 指标）；在 CSI 不完美条件下给出 ergodic capacity 边界；结合理论推导与数值仿真分析。

Result: 给出可解析的估计误差和感知指标表达，以及在 CSI 不完美条件下的容量边界；对四种处理策略在不同场景下的性能与复杂度进行对比，揭示关键的权衡关系。

Conclusion: 提供一个统一的分析框架，为实际 ISAC 波形与接收机设计提供定量指导，帮助在双分散信道中在感知精度、通信速率与实现复杂度之间做出权衡。

Abstract: This paper presents a systematic analysis of inter-symbol and inter-carrier interference (ISCI) modeling in doubly dispersive channels for integrated sensing and communication (ISAC) systems. We propose a generalized OFDM (Weyl-Heisenberg) framework to evaluate four ISCI treatment approaches: (1) explicit estimation and compensation, (2) complete ignorance, (3) uncorrelated colored noise approximation, and (4) correlated colored noise modeling. Through continuous delay-Doppler channel characterization, we derive LMMSE channel estimators and corresponding estimation errors (as sensing metrics) for both pilot-assisted and fully-known symbol scenarios. The communication performance is quantified via ergodic capacity bounds under imperfect CSI. Our theoretical analysis and numerical results reveal fundamental performance-complexity trade-offs, providing insights for practical ISAC waveform and receiver design in doubly dispersive channels.

</details>


### [8] [Delay-Multiply-And-Sum Beamforming for Real-Time In-Air Acoustic Imaging](https://arxiv.org/abs/2511.09165)
*Wouter Jansen,Walter Daems,Jan Steckel*

Main category: eess.SP

TL;DR: 提出了一种基于 Delay-Multiply-and-Sum 与相干因子权重的高阶非线性波束形成方法，用于在空气中声学成像。通过 GPU 加速实现嵌入式实时性能，显著提升对比度并改善空间分辨率，相较 DAS 基线在仿真和实测数据中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统的 DAS 波束形成存在较高副瓣、较宽主瓣、对比度低等问题，难以在高动态范围和高空间分辨率要求下实时应用。现有自适应方法通常计算成本高且受单快照限制。需要一种高性能、低延迟的波束形成方案，适用于在空气中使用的麦克风阵列。

Method: 将高阶非线性波束形成引入 Delay-Multiply-and-Sum（DMAS）技术，结合 Coherence Factor（CF）加权，对超声在空气中的麦克风阵列进行定制化实现，并通过 GPU 加速以实现嵌入式实时处理。该方法结合 DMAS 的非线性增强与 CF 的相干性权衡以提升对比度与分辨率，同时保持可行的计算开销。

Result: 在与 DAS 基线的对比中通过仿真与真实声学数据验证，提出方法显著提高成像对比度，显示出高阶非线性波束形成在空气声学成像中的实用性与性能提升。

Conclusion: 高阶非线性波束形成结合 DMAS 与 CF 加权是一种可行且高性能的在空气声学成像方案，能够在嵌入式平台通过 GPU 实现实时处理，显著提升图像对比度和空间分辨率。

Abstract: In-air acoustic imaging systems demand beamforming techniques that offer a high dynamic range and spatial resolution while also remaining robust. Conventional Delay-and-Sum (DAS) beamforming fails to meet these quality demands due to high sidelobes, a wide main lobe and the resulting low contrast, whereas advanced adaptive methods are typically precluded by the computational cost and the single-snapshot constraint of real-time field operation. To overcome this trade-off, we propose and detail the implementation of higher-order non-linear beamforming methods using the Delay-Multiply-and-Sum technique, coupled with Coherence Factor weighting, specifically adapted for ultrasonic in-air microphone arrays. Our efficient implementation allows for enabling GPU-accelerated, real-time performance on embedded computing platforms. Through validation against the DAS baseline using simulated and real-world acoustic data, we demonstrate that the proposed method provides significant improvements in image contrast, establishing higher-order non-linear beamforming as a practical, high-performance solution for in-air acoustic imaging.

</details>


### [9] [Two-Dimensional Pinching-Antenna Systems: Modeling and Beamforming Design](https://arxiv.org/abs/2511.09207)
*Yuan Zhong,Yue Xiao,Yijia Li,Hao Chen,Xianfu Lei,Pingzhi Fan*

Main category: eess.SP

TL;DR: 提出了二维PINCHING-Antenna System (2D-PASS)，将传统线形PASS扩展到连贯的介质波导平面，实现二维空间的可重构辐射平面，并通过粒子群优化（PSO）在连续及离散放置约束下优化PA布置，以最大化最小SNR；实验仿真表明相比线形PASS和固定天线，在多用户和室内场景中提高了鲁棒性与覆盖性能。


<details>
  <summary>Details</summary>
Motivation: 现有线形PASS在空间灵活性方面受限，难以在多用户和室内环境实现高效的波束控制和跨二维空间的可重构辐射，需提供覆盖广、鲁棒性强的2D可控辐射平面。

Method: 提出将PASS扩展至连续的介质波导平面，形成可重构的辐射平面；通过自适应调整PAs的放置来实现模拟波束控制。对连续位置场景采用粒子群优化（PSO）以高效搜索非凸优化空间，对有限放置分辨率的离散场景提供离散版本以适应硬件约束；目标是最大化所有用户中的最小SNR。

Result: 仿真结果显示，与传统线形PASS和固定天线基准相比，2D-PASS显著提升最小SNR，并对用户分布和距离变化具有鲁棒性。

Conclusion: 2D-PASS在二维空间实现了可重构辐射平面和动态波束自适应，结合PSO优化方法在连续和离散情形均有效，适合室内多用户场景的鲁棒性提升。

Abstract: Recently, the pinching-antenna system (PASS) has emerged as a promising architecture owing to its ability to reconfigure large-scale path loss and signal phase by activating radiation points along a dielectric waveguide. However, existing studies mainly focus on line-shaped PASS architectures, whose limited spatial flexibility constrains their applicability in multiuser and indoor scenarios. In this paper, we propose a novel two-dimensional (2D) pinching-antenna system (2D-PASS) that extends the conventional line-shaped structure into a continuous dielectric waveguide plane, thereby forming a reconfigurable radiating plane capable of dynamic beam adaptation across a 2D spatial domain. An optimization framework is developed to maximize the minimum received signal-to-noise ratio (SNR) among user equipments (UEs) by adaptively adjusting the spatial configuration of pinching antennas (PAs), serving as an analog beamforming mechanism for dynamic spatial control. For the continuous-position scenario, a particle swarm optimization (PSO)-based algorithm is proposed to efficiently explore the nonconvex search space, while a discrete variant is introduced to accommodate practical hardware constraints with limited PA placement resolution. Simulation results demonstrate that the proposed 2D-PASS substantially improves the minimum SNR compared with conventional line-shaped PASS and fixed-position antenna (FPA) benchmarks, while maintaining robustness under varying user distributions and distances.

</details>


### [10] [Positioning via Digital-Twin-Aided Channel Charting with Large-Scale CSI Features](https://arxiv.org/abs/2511.09227)
*José Miguel Mateos-Ramos,Frederik Zumegen,Henk Wymeersch,Christian Häger,Christoph Studer*

Main category: eess.SP

TL;DR: 提出一个DT辅助的无监督通道图定位框架，将估计CSI与数字孪生特征对齐，并通过余弦相似度损失与传统CC损失相结合，实现真实坐标系中的定位；在室内仿真中相对均方距离误差下降约29%，且对DT建模不匹配与测试分布漂移具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Channel charting在定位时输出的坐标系与真实世界坐标系未对齐，限制了实用性。本研究通过引入数字孪生来提供额外的物理语义信息，目标是在无需标注数据的情况下将CC结果对齐到真实坐标系，并提升鲁棒性。

Method: 提出一个新框架：(i) 从估计CSI和DT中提取大尺度特征；(ii) 使用余弦相似度损失将来自CSI与DT的特征进行匹配，并将DT损失与传统的CC损失结合，学习一个输出真实坐标的定位函数，使其不依赖标注数据。

Result: 在一个仿真的室内场景中，所提框架相比现有方法在相对距离误差方面显著下降约29%。此外，方法对DT建模失配与测试数据的分布漂移表现出鲁棒性。

Conclusion: DT辅助的CC框架能够将CC定位映射到真实坐标系，降低对标注数据的依赖，并提高对模型误差与数据分布变化的鲁棒性，显示出在室内场景的实用潜力。

Abstract: Channel charting (CC) is a self-supervised positioning technique whose main limitation is that the estimated positions lie in an arbitrary coordinate system that is not aligned with true spatial coordinates. In this work, we propose a novel method to produce CC locations in true spatial coordinates with the aid of a digital twin (DT). Our main contribution is a new framework that (i) extracts large-scale channel-state information (CSI) features from estimated CSI and the DT and (ii) matches these features with a cosine-similarity loss function. The DT-aided loss function is then combined with a conventional CC loss to learn a positioning function that provides true spatial coordinates without relying on labeled data. Our results for a simulated indoor scenario demonstrate that the proposed framework reduces the relative mean distance error by 29% compared to the state of the art. We also show that the proposed approach is robust to DT modeling mismatches and a distribution shift in the testing data.

</details>


### [11] [Constellation Design and Detection under Generalized Hardware Impairments](https://arxiv.org/abs/2511.09234)
*Thrassos K. Oikonomou,Dimitrios Tyrovolas,Sotiris A. Tegos,Panagiotis D. Diamantoulakis,Panagiotis Sarigiannidis,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 提出一种联合考虑幅度与相位失真的最大似然检测框架（PAD-D），在极坐标域工作，通过失真感知加权有效抑制硬件非线性噪声，对比欧氏距离检测(EUC-D)与高斯相位噪声检测(GAP-D)具有显著改进；给出高信噪比的通用符号误差概率(SEP)解析近似，并对常见和自定义星座进行优化以进一步降低误错率。


<details>
  <summary>Details</summary>
Motivation: 现实发射端存在幅度与相位的硬件失真（残余幅度噪声和相位噪声），现有检测器多聚焦于单一失真或在理想假设下工作，无法在广义HW impairments下实现鲁棒检测与高效星座设计，因此需要一个统一、可实际应用的检测框架并兼容常用星座的优化。

Method: 建立在极坐标域的最大似然检测框架，把传输端的失真建模为残余幅度噪声和相位噪声；提出约简的相幅失真检测器（PAD-D），通过失真感知加权来同时抑制两种失真；推导高信噪比情况下的闭式SEP近似，适用于任意星座；进一步给出在该框架下的最优星座设计（在复平面优化符号位置以最小化SEP）。

Result: PAD-D相较于EUC-D和GAP-D在多种场景下呈现显著性能提升，误差地板大幅降低，特别是在高阶QAM与SAPSK星座中可达数量级级别的改进；给出的SEP解析近似在优化星座时也保持较高的精确度。

Conclusion: 提供了一个统一、实用且可扩展的检测框架，显式考虑幅度与相位失真，且可据此设计出更鲁棒的最佳化星座；解析 SEP 的闭式表达和对复杂星座的有效性在仿真中得到验证，具备在实际系统中应用的潜力。

Abstract: This paper presents a maximum-likelihood detection framework that jointly mitigates hardware (HW) impairments in both amplitude and phase. By modeling transceiver distortions as residual amplitude and phase noise, we introduce the approximate phase-and-amplitude distortion detector (PAD-D), which operates in the polar domain and effectively mitigates both distortion components through distortion-aware weighting. The proposed detector performs reliable detection under generalized HW impairment conditions, achieving substantial performance gains over the conventional Euclidean detector (EUC-D) and the Gaussian-assumption phase noise detector (GAP-D), which is primarily designed to address phase distortions. In addition, we derive a closed-form high-SNR symbol error probability (SEP) approximation, which offers a generic analytical expression applicable to arbitrary constellations. Simulation results demonstrate that the PAD-D achieves up to an order-of-magnitude reduction in the error floor relative to EUC-D and GAP-D for both high-order quadrature amplitude modulation (QAM) and super amplitude phase-shift keying (SAPSK) constellations, establishing a unified and practical framework for detection under realistic transceiver impairments. Building on this framework, we further develop optimized constellations tailored to PAD-D, where the symbol positions are optimized in the complex plane to minimize SEP. The optimality of these constellations is confirmed through extensive simulations, which also verify the accuracy of the proposed analytical SEP approximation, even for the optimized designs.

</details>


### [12] [Flexible Continuous Aperture Arrays](https://arxiv.org/abs/2511.09244)
*Kuranage Roche Rayan Ranasinghe,Zhaolin Wang,Giuseppe Thadeu Freitas de Abreu,Emil Björnson*

Main category: eess.SP

TL;DR: 提出并评估一种柔性连续孔径阵列（FCAPA），通过对下行多用户波束成形的优化提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统刚性CAPA在几何和自由度方面受限，难以在多用户MIMO场景中充分利用可变形/可 morphable 表面带来的额外自由度，从而限制系统吞吐与覆盖。引入柔性结构以增加可控自由度（DoF），潜在提升多用户下行通信性能。

Method: 建立FCAPA的建模框架，将表面柔性引入CAPA模型；对下行多用户波束成形问题进行优化，目标函数为加权和速率（WSR），在考虑FCAPA的可 morphability 的前提下求解最优波束与形变配置；通过理论分析与数值仿真比较与典型CAPA系统的性能差异。

Result: 仿真结果显示，FCAPA在相同资源约束下明显优于典型CAPA，WSR和系统吞吐提升显著；且性能提升随 morphability 增强而增大，表明可变形能力是提升多用户MIMO性能的关键因素。

Conclusion: 柔性连续孔径阵列能够显著提升下行多用户波束成形的灵活性和性能，为未来自适应天线阵列设计提供新思路；但也应关注实现复杂度、结构可靠性与控制算法的可扩展性等挑战。

Abstract: A novel electromagnetic (EM) structure termed flexible continuous aperture array (FCAPA) is proposed, which incorporates inherent surface flexibility into typical continuous aperture array (CAPA) systems, thereby enhancing the degrees-of-freedom (DoF) of multiple-input multiple-output (MIMO) systems equipped with this technology. By formulating and solving a downlink multi-user beamforming optimization problem to maximize the weighted sum rate (WSR) of the multiple users with FCAPA, it is shown that the proposed structure outperforms typical CAPA systems by a wide margin, with performance increasing with increasing morphability.

</details>


### [13] [2D Waveguide-Fed Metasurface Antenna Arrays: Modeling and Optimization for Bistatic Sensing](https://arxiv.org/abs/2511.09254)
*Ioannis Gavras,Panagiotis Gavriilidis,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 提出一个物理一致的双基XL MIMO感知框架，结合2D波导馈电的超表面阵列，建立耦合的磁极化率–被动性约束模型，利用Neumann-series近似和CRB导出多目标参数估计的优化配置，结果表明元件放置在辐射近场中的强耦合元材料阵列对定位精度有关键作用。


<details>
  <summary>Details</summary>
Motivation: 在高度耦合、元材料驱动的XL MIMO双基探测场景中，需同时考虑波导与自由空间耦合、阵列互耦以及被动性约束，建立物理自洽的建模与优化框架以提升参数估计性能。

Method: 1) 提出耦合dipole模型，捕捉波导与自由空间的互耦；2) 引入磁极化率的被动性约束；3) 在双基场景下对阵列响应采用Neumann-series近似；4) 推导多目标参数的Cramer–Rao界（CRB）；5) 将CRB纳入关于每元共振强度配置的优化问题以设计元件。

Result: 仿真结果在辐射近场条件下给出位置误差界的量化，显示所提设计中的元件放置对强耦合元材料阵列的XL MIMO双基感知系统性能具有决定性作用。

Conclusion: 给出一个物理自洽的XL MIMO bistatic sensing框架，验证了元件放置对性能的关键影响；未来工作可扩展至更广场景、进行实验验证，并进一步研究耦合和共振配置的优化。

Abstract: This paper presents a physics-consistent framework for bistatic sensing incorporating a 2-Dimensional (2D) waveguide-fed metasurface antenna array capable of realizing eXtremely-Large Multiple-Input Multiple-Output (XL MIMO) apertures. A coupled-dipole model is presented that captures the array's mutual coupling due to both waveguide and free-space interactions, and a novel passivity constraint on the corresponding magnetic polarizabilities is proposed. Focusing on a bistatic sensing setup, we leverage a Neumann-series approximation of the array response model and derive the Cramer-Rao bound for multi-target parameter estimation, which is then incorporated into a sensing optimization formulation with respect to the metasurface's per-element resonance strength configuration. Simulation results on the position error bound in the radiative near field with the proposed design quantify the critical role of metamaterial placement in strongly coupled metasurface-based XL MIMO bistatic sensing systems.

</details>


### [14] [End-to-End Hardware Modeling and Sensitivity Optimization of Photoacoustic Signal Readout Chains](https://arxiv.org/abs/2511.09341)
*Weiran Yang,Yiqi Cai,Handi Deng,Cheng Ma*

Main category: eess.SP

TL;DR: 提出基于KLM的端到端声学传感系统模型，定量分析换能元件面积、线缆长度和接收端阻抗对系统灵敏度的耦合影响，并给出误差小于5%的实验验证。


<details>
  <summary>Details</summary>
Motivation: PAI系统中传感子系统的灵敏度直接影响图像质量，但现有研究多聚焦前端或后端，缺乏传感器–线缆–接收端的端到端耦合分析。本研究旨在建立完整的系统级灵敏度优化模型。

Method: 基于KLM模型从线性压电本构、1D波方程和传输线理论出发，将声学部件抽象为受控电压源，并引入电缆和接收端的等效参数，构建端到端等效电路。推导系统传递函数的解析表达式，揭示EA、CL、RI等关键参数的耦合及其对灵敏度的影响。

Result: 模型与实验结果一致，平均误差<5%；发现并分析了由超出1D振动假设引起的低频尾部现象，指出其对模型适用性及对伪影抑制的启示。

Conclusion: 提供一个完整框架用于优化检测灵敏度与提升PAI成像保真度，强调端到端耦合在传感系统设计中的重要性及适用条件。

Abstract: The sensitivity of the acoustic detection subsystem in photoacoustic imaging (PAI) critically affects image quality. However, previous studies often focused only on front-end acoustic components or back-end electronic components, overlooking end-to-end coupling among the transducer, cable, and receiver. This work develops a complete analytical model for system-level sensitivity optimization based on the Krimholtz, Leedom, and Matthaei (KLM) model. The KLM model is rederived from first principles of linear piezoelectric constitutive equations, 1D wave equations and transmission line theory to clarify its physical basis and applicable conditions. By encapsulating the acoustic components into a controlled voltage source and extending the model to include lumped-parameter representations of cable and receiver, an end-to-end equivalent circuit is established. Analytical expressions for the system transfer functions are derived, revealing the coupling effects among key parameters such as transducer element area (EA), cable length (CL), and receiver impedance (RI). Experimental results validate the model with an average error below 5%. Additionally, a low-frequency tailing phenomenon arising from exceeding the 1D vibration assumption is identified and analyzed, illustrating the importance of understanding the model's applicable conditions and providing a potential pathway for artifact suppression. This work offers a comprehensive framework for optimizing detection sensitivity and improving image fidelity in PAI systems.

</details>


### [15] [A cross-modal pre-training framework with video data for improving performance and generalization of distributed acoustic sensing](https://arxiv.org/abs/2511.09342)
*Junyi Duan,Jiageng Chen,Zuyuan He*

Main category: eess.SP

TL;DR: A novel enhanced DAS framework that fuses STFT-based temporal-frequency features with video-to-DAS cross-modal pre-training to address data scarcity and improve frequency analysis, achieving strong few-shot and external-damage recognition performance while enabling cross-modal learning between computer vision and DAS sensing.


<details>
  <summary>Details</summary>
Motivation: DAS signals exhibit complex 2D spatio-temporal morphology. Standard methods are suboptimal for analysis, while deep learning thrives with rich data. DAS-MAE, though strong in label-free reconstruction, underperforms in temporal-frequency analysis and is data-hungry for transformers. There is a need for explicit time-frequency features and methods to alleviate data constraints, potentially through cross-modal pre-training with CV.

Method: Augment DAS with short-time Fourier transform (STFT) to extract explicit temporal-frequency features. Introduce video-to-DAS cross-modal pre-training to leverage unlabeled data and cross-domain representations, training on reconstruction tasks to learn high-level representations (e.g., event classification) without labels.

Result: Achieves 0.1% error rate in few-shot classification (90.9% relative improvement over DAS-MAE) and 4.7% recognition error in external damage prevention (75.4% improvement over from-scratch). Claims to pioneer video-to-DAS cross-modal pre-training, expanding training resources by bridging CV and DAS sensing, with improved generalization.

Conclusion: The approach broadens DAS deployment across diverse industrial scenarios and advances cross-modal representation learning for industrial IoT sensing by combining explicit temporal-frequency features with cross-modal pre-training.

Abstract: Fiber-optic distributed acoustic sensing (DAS) has emerged as a critical Internet-of-Things (IoT) sensing technology with broad industrial applications. However, the two-dimensional spatial-temporal morphology of DAS signals presents analytical challenges where conventional methods prove suboptimal, while being well-suited for deep learning approaches. Although our previous work, DAS Masked Autoencoder (DAS-MAE), established state-of-the-art performance and generalization without labels, it is not satisfactory in frequency analysis in temporal-dominated DAS data. Moreover, the limitation of effective training data fails to address the substantial data requirements inherent to Transformer architectures in DAS-MAE. To overcome these limitations, we present an enhanced framework incorporating short-time Fourier transform (STFT) for explicit temporal-frequency feature extraction and pioneering video-to-DAS cross-modal pre-training to mitigate data constraints. This approach learns high-level representations (e.g., event classification) through label-free reconstruction tasks. Experimental results demonstrate transformative improvements: 0.1% error rate in few-shot classification (90.9% relative improvement over DAS-MAE) and 4.7% recognition error in external damage prevention applications (75.4% improvement over from-scratch training). As the first work to pioneer video-to-DAS cross-modal pre-training, available training resources are expanded by bridging computer vision and distributed sensing areas. The enhanced performance and generalization facilitate DAS deployment across diverse industrial scenarios while advancing cross-modal representation learning for industrial IoT sensing.

</details>


### [16] [Reduced-Complexity Model Selection and Rate Allocation for Multiple-Model Electrical Signal Compression](https://arxiv.org/abs/2511.09370)
*Corentin Presvôts,Michel Kieffer,Thibault Prevost*

Main category: eess.SP

TL;DR: 提出一种降低复杂度的多模型编码（MMC）框架，用于采样电信号波形的VCS，协同优化第一阶段模型速率与第二阶段残差压缩的速率，以在给定失真约束下降低比特率。通过穷举基线、黄金分割搜索和基于速率失真模型的模型筛选实现降复杂度，仿真实验表明在等效复杂度下可实现相对更低的传输率。


<details>
  <summary>Details</summary>
Motivation: 需在满足重建信号质量约束的前提下，降低MMC在采样电信号波形（VCS）处理中的比特率和计算复杂性，改善现有常数大小向量与变质量比特流的两阶段MMC性能。

Method: 提出三种方法：1) 穷举搜索作为基线，2) 使用黄金分割搜索来确定第一阶段的速率以降低复杂度，3) 结合第一阶段各模型的速率失真（RD）模型，从而在第一阶段筛选出有前景的模型并对两阶段的速率搜索区间进行收缩。实现第一阶段模型的参数化与速率分配，以及第二阶段残差的压缩方法及其速率的联合优化，使目标失真得到满足。

Result: 仿真结果显示，在等效复杂度下，所提降低复杂度的MMC方案在给定失真约束的情况下可实现更低的比特率，相比现有最先进的VCS编码方案具有优势。

Conclusion: 通过将第一阶段的参数化模型与第二阶段残差压缩的速率进行联合优化，并辅以穷举、黄金分割和基于RD模型的筛选，所提出的降低复杂度的MMC框架能够在满足失真约束的同时降低比特率，证明了该方法在VCS场景下的有效性和可扩展性。

Abstract: This paper adapts a Multiple-Model Coding (MMC) approach for sampled electrical signal waveforms to satisfy reconstructed signal quality constraints. The baseline MMC approach consists of two stages processing vectors of Voltage and Current Signal (VCS) of constant size and producing bitstreams of constant rate but varying quality. In the proposed approach, the parametric model and the rate allocated to the first stage, as well as the residual compression method of the second stage and its associated rate, are jointly optimized to achieve a target distortion of the reconstructed signal. Three approaches are proposed. An exhaustive search serves as a baseline for comparison. Then, an approach involving a Golden Section search is exploited to determine the rate of the first stage with reduced complexity. Finally, rate-distortion models of the compression efficiency for each model in the first stage are employed to obtain a subset of promising models in the first stage and reduced-size search intervals for the rate selection in both stages. Simulation results demonstrate that the proposed reduced-complexity MMC approach reduces the rate for a given distortion constraint compared to state-of-the-art solutions for VCS with equivalent complexity.

</details>


### [17] [LLM Enabled Beam Training for Pinching Antenna Systems (PASS)](https://arxiv.org/abs/2511.09453)
*Deqiao Gan,Xiaoxia Xu,Xiaohu Ge,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出面向PASS的LLM驱动波束训练框架，利用端到端GPT训练实现单用户与多用户MIMO场景下的代码本生成与波束选择，显著提升波束增益与系统吞吐。


<details>
  <summary>Details</summary>
Motivation: 降低下行PASS在波束训练中的开销，提升探测的上下文感知与环境自适应能力。

Method: 单用户：构建以最大化波束增益为目标的发射代码本，采用MRT获得最优波束。多用户：在MMSE传输下进行联合代码本生成与波束选择，标签通过从各用户Top-S候选波束中选取最大化系统性能的组合构造；基于预训练GPT的端到端训练，最小化交叉熵损失。

Result: 单用户：Top-1准确率95%，波束形成增益提升51.92%。多用户：在和基于LLM的 Massive MIMO、以及传统PASS相比，和率提升分别达到57.14%与33.33%。

Conclusion: LLM驱动的PASS在单用户和多用户场景中显著提高波束训练效率与系统性能，展示了LLM在环境自适应探测与波束选择中的潜力。

Abstract: To enable intelligent beam training, a large language model (LLM)-enabled beam training framework is proposed for the pinching antenna system (PASS) in downlink multi-user multiple-input multiple-output (MIMO) communications. A novel LLM-based beam training supervised learning mechanism is developed, allowing context-aware and environment-adaptive probing for PASS to reduce overheads. Both single-user and multi-user cases are considered. 1) For single-user case, the LLM-based pinching beamforming codebook generation problem is formulated to maximize the beamforming gain. Then, the optimal transmit beamforming is obtained by maximum ratio transmission (MRT). 2) For multi-user case, a joint codebook generation and beam selection problem is formulated based on the system sum rate under the minimum mean square error (MMSE) transmit beamforming. The training labels for pinching beamforming are constructed by selecting the beam combination that maximizes system performance from each user's Top-S candidate beams. Based on pretrained Generative Pre-trained Transformers (GPTs), the LLM is trained in an end-to-end fashion to minimize the cross-entropy loss. Simulation results demonstrate that: i) For single-user case, the proposed LLM-enabled PASS attains over 95% Top-1 accuracy in beam selection and achieves 51.92% improvements in beamforming gains compared to conventional method. ii) For multi-user case, the proposed LLM-enabled PASS framework significantly outperforms both the LLM-based massive MIMO and conventional PASS beam training, achieving up to 57.14% and 33.33% improvements in sum rate, respectively.

</details>


### [18] [Scalable Long-Term Beamforming for Massive Multi-User MIMO](https://arxiv.org/abs/2511.09464)
*Ali Rasteh,Amirreza Kiani,Marco Mezzavilla,Sundeep Rangan*

Main category: eess.SP

TL;DR: 提出一种基于长期波束形成的低开销数字接收机，针对大规模MIMO（1000+天线），通过从空间协方差估计中提取低秩投影并结合快速多项式矩阵求逆，在仿真中接近全瞬时波束形成的性能，同时显著降低开销与计算量。


<details>
  <summary>Details</summary>
Motivation: 在极大规模数字MIMO中，面对极高的天线数带来的通道估计开销和数字计算负担，需要可扩展且高效的接收机设计。

Method: 采用从时空协方差估计得到的低秩投影作为投影/前馈的核心，并结合快速多项式矩阵逆运算实现实时处理；以长期（LTD）波束形成思想降低对实时精确通道估计的依赖，并通过射线追踪/仿真评估。

Result: 与完整的瞬时波束形成相比，损失很小；在开销和计算方面获得显著提升。

Conclusion: 给出一种可扩展的、计算效率高的大规模数字MIMO接收机设计，使用长期波束形成在1000+天线系统中实现近似最优性能与显著资源节省。

Abstract: Fully digital massive MIMO systems with large numbers (1000+) of antennas offer dramatically increased capacity gains from spatial multiplexing and beamforming. Designing digital receivers that can scale to these array dimensions presents significant challenges regarding both channel estimation overhead and digital computation. This paper presents a computationally efficient and low-overhead receiver design based on long-term beamforming. The method combines finding a low-rank projection from the spatial covariance estimate with a fast polynomial matrix inverse. Ray tracing simulations show minimal loss relative to complete instantaneous beamforming while offering significant overhead and computational gains.

</details>


### [19] [Outage Probability Analysis of MRC-Based Fluid Antenna Systems under Rician Fading](https://arxiv.org/abs/2511.09474)
*Tummi Ganesh,Soumya P. Dash,Italo Atzeni*

Main category: eess.SP

TL;DR: Analytical outage performance of a MRC-based fluid antenna system (FAS) with best-K port selection under Rician fading; Laplace-transform method yields OP, a lower bound, and high-SNR asymptotics.


<details>
  <summary>Details</summary>
Motivation: Understand and quantify the reliability (outage probability) of FAS when multiple FA ports are available at the receiver, with selection of the top-K ports and MRC combining.

Method: Model the FA ports under Rician fading, select K ports with the highest instantaneous SNR, apply maximum ratio combining across the selected ports. Derive the post-combining SNR statistics using a Laplace transform-based approach, and obtain exact OP expressions along with a closed-form lower bound and high-SNR asymptotic results.

Result: Closed-form lower bound on the outage probability and asymptotic OP expressions at high SNR; numerical results validate the analysis and illustrate parameter effects (M, K, Rician factor, etc.).

Conclusion: The paper provides a tractable analytical framework for evaluating and designing FAS systems with port selection and MRC under fading, showing how system parameters influence reliability and offering practical insights for design.

Abstract: This paper investigates a fluid antenna system (FAS) where a single-antenna transmitter communicates with a receiver equipped with a fluid antenna (FA) over a Rician fading channel. Considering that multiple ports among the M available FA ports can be activated, the receiver selects the best K with the highest instantaneous signal-to-noise ratio (SNR) and combines the received signals at the selected ports using maximum ratio combining. The statistics of the post-combining SNR are derived using a Laplace transform-based approach, which allows to analyze the outage probability (OP) of the FAS. Additional closed-form expressions for a lower bound on the OP and the asymptotic OP at high SNR are presented. Numerical results validate the analytical framework and demonstrate the interplay of key system parameters on the performance of the considered MRC-based FAS.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [20] [Tracing AG Codes: Toward Meeting the Gilbert-Varshamov Bound](https://arxiv.org/abs/2511.08788)
*Gil Cohen,Dean Doron,Noam Goldgraber,Tomer Manket*

Main category: cs.IT

TL;DR: 研究对 AG 码取 traces 来构造的 TAG 码，以分析其在 Gilbert–Varshamov 界限下的表现；给出一个定制的 Hasse–Weil 型定理及其对指数和估计的扩展，并比较 TAG 与串联编码在高距离下的极限。


<details>
  <summary>Details</summary>
Motivation:  GV 界限与显式编码的匹配是编码理论的长期难题；在非二进制但常数大小域上，代数几何码已超越 GV；通过对 AG 码取 traces 构成 TAG 码，期望在降维到二进制域时仍保持性能；不同于常见的字母表降维（如串联/级联），本研究试图在保持代数结构的同时进行分析。

Method: 提出并证明一个面向 TAG 码的 Hasse–Weil 型定理，适用于 TAG 的分析；在此框架内使用新的指数和估计来评估 TAG 的参数，且在字母表降维步骤中保留 AG 码的代数结构，超越简单的降维/串联方法；对相关该定理进行一般化扩展。

Result: 未给出改进的显式构造；若对界的常数因子略作加强即可实现改进；在高距离区域，TAG 码的性能不及编码串联；该定理具有更广泛的适用性，并给出对指数和的新估计。

Conclusion: TAG 提供了在保持代数结构的前提下分析字母表降维的新框架；但当前界限尚不足以超越串联的极限。所建立的 Hasse–Weil 型定理及相关指数和估计具普遍意义，可能为今后的改进提供关键工具。

Abstract: One of the oldest problems in coding theory is to match the Gilbert-Varshamov bound with explicit binary codes. Over larger-yet still constant-sized-fields, algebraic-geometry codes are known to beat the GV bound. In this work, we leverage this phenomenon by taking traces of AG codes. Our hope is that the margin by which AG codes exceed the GV bound will withstand the parameter loss incurred by taking the trace from a constant field extension to the binary field. In contrast to concatenation, the usual alphabet-reduction method, our analysis of trace-of-AG (TAG) codes uses the AG codes' algebraic structure throughout - including in the alphabet-reduction step.
  Our main technical contribution is a Hasse-Weil-type theorem that is well-suited for the analysis of TAG codes. The classical theorem (and its Grothendieck trace-formula extension) are inadequate in this setting. Although we do not obtain improved constructions, we show that a constant-factor strengthening of our bound would suffice. We also analyze the limitations of TAG codes under our bound and prove that, in the high-distance regime, they are inferior to code concatenation. Our Hasse-Weil-type theorem holds in far greater generality than is needed for analyzing TAG codes. In particular, we derive new estimates for exponential sums.

</details>


### [21] [Learning Binary Autoencoder-Based Codes with Progressive Training](https://arxiv.org/abs/2511.09221)
*Vukan Ninkovic,Dejan Vukobratovic*

Main category: cs.IT

TL;DR: 通过两阶段训练的自编码器端到端设计来学习二进制纠错码。先进行连续预训练，再直接二值化并微调，避免梯度近似。在(7,4)对称二元信道(BSC)上，学习到的编码器-解码器对形成Hamming码的一个coset码，具备线性与距离性质，在最大似然解码下达到同样的BLER，表明紧凑的自编码器架构可以稳定地学习结构化、代数最优的二进制码。


<details>
  <summary>Details</summary>
Motivation: 在可微分自编码器框架中强制二进制码字存在困难，因为离散化会中断梯度流并导致不稳定的收敛。本文提出一个简化的两阶段训练流程，旨在在不使用梯度近似技术的情况下实现二值化与微调，从而将二进制纠错码的结构性与最优性结合到端到端学习中。

Method: 提出两阶段训练：1) 连续（float）前训练以学习编码器/解码器的初始参数；2) 直接二值化并在不使用梯度近似的情况下进行微调。实验以(7,4)块码在二元对称信道BSC上进行，学习出一个旋转版本（coset码）的最优Hamming码，保持其线性与距离性质。

Result: 所学的编码-解码对在BLER上与最大似然（ML）解码等价，且实现了对Hamming码的coset表示，隐含地学到了结构化、代数最优的二进制码。

Conclusion: 证明紧凑的AE结构在稳定且直接的训练下，能够学习到结构化的、代数意义上的最优二进制码，为端到端学习在编码理论中的应用提供了有力证据。

Abstract: Error correcting codes play a central role in digital communication, ensuring that transmitted information can be accurately reconstructed despite channel impairments. Recently, autoencoder (AE) based approaches have gained attention for the end-to-end design of communication systems, offering a data driven alternative to conventional coding schemes. However, enforcing binary codewords within differentiable AE architectures remains difficult, as discretization breaks gradient flow and often leads to unstable convergence. To overcome this limitation, a simplified two stage training procedure is proposed, consisting of a continuous pretraining phase followed by direct binarization and fine tuning without gradient approximation techniques. For the (7,4) block configuration over a binary symmetric channel (BSC), the learned encoder-decoder pair learns a rotated version (coset code) of the optimal Hamming code, naturally recovering its linear and distance properties and thereby achieving the same block error rate (BLER) with maximum likelihood (ML) decoding. These results indicate that compact AE architectures can effectively learn structured, algebraically optimal binary codes through stable and straightforward training.

</details>


### [22] [Generic Construction of Optimal-Access Binary MDS Array Codes with Smaller Sub-packetization](https://arxiv.org/abs/2511.09251)
*Lan Ma,Qifu Tyler Sun,Shaoteng Liu,Liyang Zhou*

Main category: cs.IT

TL;DR: 提出两种通用构造（C1 与 C2）来从基二进制 MDS 数组码构造具有单节点最优访问带宽的二进制 MDS 数组码。C1 给出 (k+r, k, m s^{ceil((k+r)/s)}) 码，修复需连接 d=k+s−1 个辅助节点；C2 在特定参数下给出 (k+r, k, m s^{(k+r)/(s+1)}) 码，且拥有更小的子分区化，且达到最优访问带宽。文中 claim C2 的子分区化在现有公开结果中最小。


<details>
  <summary>Details</summary>
Motivation: 针对二进制 MDS 数组码在单节点故障下的修复带宽瓶颈，寻求在子分区化和可访问性之间的折中，提供可泛化的构造方法以实现最优修复带宽。

Method: 以任意二进制 MDS 数组码（子分区化为 m）为基码，给出两种泛化构造：Construction I 对任意 s ≤ r 构造 (k+r, k, m s^{ceil((k+r)/s)})，修复时需要与 k+s−1 个节点通信，其中 s−1 个被指定，其余 k 个可自由选择；Construction II 仅在 r 为偶且 s=r/2、且 s+1 整除 k+r 时成立，构造 (k+r, k, m s^{(k+r)/(s+1)})，修复带宽最优，且有 (s/(s+1))(k+r) 个节点具备最优访问性质。

Result: C1 提供一个通用、灵活的框架，子分区化较大但易于实现；C2 在特定参数下获得更小的子分区化且具有最优修复带宽，同时实现更多节点具备最优访问性质。文献声称在已知二进制 MDS 数组码中，C2 的子分区化是最小的。

Conclusion: 两种泛化构造为二进制 MDS 数组码的修复带宽优化提供了新的手段，特别是 C2 在子分区化方面达到目前的最低水平，提升了实际可用性与实现灵活性。

Abstract: A $(k+r,k,l)$ binary array code of length $k+r$, dimension $k$, and sub-packetization $l$ is composed of $l\times(k+r)$ matrices over $\mathbb{F}_2$, with every column of the matrix stored on a separate node in the distributed storage system and viewed as a coordinate of the codeword. It is said to be maximum distance separable (MDS) if any $k$ out of $k+r$ coordinates suffice to reconstruct the whole codeword. The repair problem of binary MDS array codes has drawn much attention, particularly for single-node failures. In this paper, given an arbitrary binary MDS array code with sub-packetization $m$ as the base code, we propose two generic approaches (Generic Construction I and II) for constructing binary MDS array codes with optimal access (or repair) bandwidth for single-node failures. For every $s\leq r$, a $(k+r,k,ms^{\lceil \frac{k+r}{s}\rceil})$ code $\mathcal{C}_1$ with optimal access bandwidth can be constructed by Generic Construction I. Repairing a failed node of $\mathcal{C}_1$ requires connecting to $d = k+s-1$ helper nodes, in which $s-1$ helper nodes are designated and $k$ are free to select. $\mathcal{C}_1$ generally achieves smaller sub-packetization and provides greater flexibility in the selection of its coefficient matrices. For even $r\geq4$ and $s=\frac{r}{2}$ such that $s+1$ divides $k+r$, a $(k+r, k,ms^{\frac{k+r}{s+1}})$ code $\mathcal{C}_2$ with optimal repair bandwidth can be constructed by Generic Construction II, with $\frac{s}{s+1}(k+r)$ out of $k+r$ nodes having the optimal access property. To the best of our knowledge, $\mathcal{C}_2$ possesses the smallest sub-packetization among existing binary MDS array codes with optimal repair bandwidth known to date.

</details>


### [23] [Enabling Smart Radio Environments in the Frequency Domain With Movable Signals](https://arxiv.org/abs/2511.09384)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 提出了一种在频域实现的可移动信号来实现智能射频环境（SREs）的新范畴，避免对电子重构组件的依赖。系统分析表明，在MISO/LoS条件下，移动信号可实现高于量化等增益传输的平均接收功率；在NLoS条件下，借助具有固定电磁特性的均匀间距表面的固定智能表面（FIS）也能有效。理论结果显示，FIS辅助的移动信号系统的接收功率可达到RIS使用固定频率信号系统的4倍左右。


<details>
  <summary>Details</summary>
Motivation: 现有RIS和柔性天线需要电子可重构或可移动部件，存在实现难点和商业化困难。需寻找更低复杂度、易于实现的SREs方案，并将域扩展到频域以利用可移动信号的优势。

Method: 对MISO系统在LoS条件下的移动信号进行理论分析，比较其与量化等增益传输（EGT）的平均接收功率；在NLoS条件下研究移动信号，利用由固定EM属性且均匀间距元件构成的表面（FIS）实现反射以增强信道，并给出解析结果和功率提升。

Result: 在LoS条件下，移动信号相比量化EGT可获得更高的平均接收功率；在NLoS条件下，移动信号通过FIS反射仍具有效性；理论分析表明，FIS辅助的移动信号系统的接收功率可达到使用固定频率信号的RIS系统的约4倍。

Conclusion: 将移动信号引入频域作为实现SRE的新途径，具有潜在的硬件简化和较高性能提升的前景。FIS与移动信号的组合可在NLoS场景下实现显著增益，接近或超越基于RIS的固定频率实现。

Abstract: Smart radio environments (SREs) enhance wireless communications by allowing control over the channel. They have been enabled through surfaces with reconfigurable electromagnetic (EM) properties, known as reconfigurable intelligent surfaces (RISs), and through flexible antennas, which can be viewed as realizations of SREs in the EM domain and space domain, respectively. However, these technologies rely on electronically reconfigurable or movable components, introducing implementation challenges that could hinder commercialization. To overcome these challenges, we propose a new domain to enable SREs, the frequency domain, through the concept of movable signals, where the signal spectrum can be dynamically moved along the frequency axis. We first analyze movable signals in multiple-input single-output (MISO) systems under line-of-sight (LoS) conditions, showing that they can achieve higher average received power than quantized equal gain transmission (EGT). We then study movable signals under non-line-of-sight (NLoS) conditions, showing that they remain effective by leveraging reflections from surfaces made of uniformly spaced elements with fixed EM properties, denoted as fixed intelligent surfaces (FISs). Analytical results reveal that a FIS-aided system using movable signals can achieve up to four times the received power of a RIS-aided system using fixed-frequency signals.

</details>


### [24] [Computability of the Optimizer for Rate Distortion Functions](https://arxiv.org/abs/2511.09412)
*Jonathan E. W. Huffmann,Holger Boche*

Main category: cs.IT

TL;DR: 本文研究率失真理论中优化器的可计算性问题，结果是尽管率失真函数通常可计算，但该优化器在一般情形下不可计算，甚至对简单失真度量亦然。


<details>
  <summary>Details</summary>
Motivation: 揭示信息理论中优化对象的可计算性，影响码本构造和测试信道的可实现性，并将理论与算法实现联系起来。

Method: 通过将可计算性分析应用到率失真优化问题，给出对测试信道最优解不可计算性的证明，利用互信息的凸优化结构以及对简单失真度量的构造性分析。

Result: 证明率失真函数通常可计算，但优化器或测试信道的最优解在一般情况下不可计算，即使在简单的失真度量下也成立。

Conclusion: 该结果揭示编码理论中的根本性可计算性边界，提示在实际算法设计中需依赖近似、受限模型或可计算的子类以获得可实现的构造。

Abstract: Rate distortion theory treats the problem of encoding a source with minimum codebook size while at the same time allowing for a certain amount of errors in the reconstruction measured by a fidelity criterion and distortion level. Similar to the channel coding problem the optimal rate of the codebook with respect to the blocklength is given by a convex optimization problem involving information theoretic quantities like mutual information. The value of the rate in dependence of the distortion level as well as the optimizer used in the codebook construction are of theoretical and practical importance in communication and information theory. In this paper the behavior of the rate distortion function regarding the computability of the optimizing test channel is investigated. We find that comparable with known results about the optimizer for other information theoretic problems a similar result is found to be true also regarding the computability of the optimizer for rate distortion functions.
  It turns out that while the rate distortion function is usually computable the optimizer for this problem is in general non-computable even for simple distortion measures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [25] [Automated Hardware Trojan Insertion in Industrial-Scale Designs](https://arxiv.org/abs/2511.08703)
*Yaroslav Popryho,Debjit Pal,Inna Partin-Vaisband*

Main category: cs.CR

TL;DR: 提出一个自动化、可扩展的HT样本生成框架，用来在工业级网表上生成与功能保持的触发-载荷对，挑战现有检测方法；实验表明现有图学习模型难以检测此类Trojan。


<details>
  <summary>Details</summary>
Motivation: 工业SoCs规模极大，公开基准往往规模小且多为手工设计； release真实恶意RTL带来伦理与安全风险；需要可重复、可共享但不泄露攻击细节的评估基准，以促进检测工具的鲁棒性评估。

Method: 建立一个三步管线： (i) 将大规模门级设计解析为连通性图； (ii) 使用SCOAP测试性度量来挖掘罕见区域； (iii) 应用参数化、保持功能不变的图变换，合成触发-载荷对，使其在统计上与隐蔽HT的特征相符。

Result: 在本文工作所生成的基准上，代表性的最先进图学习模型无法检测Trojan。该框架弥合学术电路与现代SoCs之间的评估鸿沟，提供可重复的挑战实例以推动安全研究。

Conclusion: 该工作提供了一个可扩展、可复现的HT评估框架，帮助研究者在不公开逐步攻击细节的前提下测试和比较检测工具，同时促进对抗性HT的理解与检测能力的提升。

Abstract: Industrial Systems-on-Chips (SoCs) often comprise hundreds of thousands to millions of nets and millions to tens of millions of connectivity edges, making empirical evaluation of hardware-Trojan (HT) detectors on realistic designs both necessary and difficult. Public benchmarks remain significantly smaller and hand-crafted, while releasing truly malicious RTL raises ethical and operational risks. This work presents an automated and scalable methodology for generating HT-like patterns in industry-scale netlists whose purpose is to stress-test detection tools without altering user-visible functionality. The pipeline (i) parses large gate-level designs into connectivity graphs, (ii) explores rare regions using SCOAP testability metrics, and (iii) applies parameterized, function-preserving graph transformations to synthesize trigger-payload pairs that mimic the statistical footprint of stealthy HTs. When evaluated on the benchmarks generated in this work, representative state-of-the-art graph-learning models fail to detect Trojans. The framework closes the evaluation gap between academic circuits and modern SoCs by providing reproducible challenge instances that advance security research without sharing step-by-step attack instructions.

</details>


### [26] [Channel-Robust RFF for Low-Latency 5G Device Identification in SIMO Scenarios](https://arxiv.org/abs/2511.08902)
*Yingjie Sun,Guyue Li,Hongfu Chou,Aiqun Hu*

Main category: cs.CR

TL;DR: 提出基于多收发天线的LLDR(RFF)方法，利用共时 CFR 的对数线性增量比来抑制多径影响，且在子带内独立计算以提升鲁棒性，避免时域多采样，从而实现低延迟的设备指纹识别。


<details>
  <summary>Details</summary>
Motivation: 5G 的极低时延（URLLC）对设备识别提出严格要求；传统密码学方案增加计算开销，造成识别延迟；RFF 能从物理层识别设备并阻止冒充，但多径会降低准确率，且现有抗通道方法需额外信令或多时点处理，导致额外延迟，因此需要一种快速、鲁棒且低延迟的RFF方法。

Method: 利用来自多天线的共时 CFR 信号，计算各天线的对数线性增量比（LLDR）来提取辨识特征；不同子带内独立计算 LLDR 以适应信道随频带的变化；避免任何时域多采样，消除额外采样延迟；提出以子带分割来提升在有限信道变化条件下的鲁棒性。

Result: 仿真实验在20路径信道、SNR=20 dB下，对30个UE的识别准确率达到96.13%；基于 Roofline 模型论证的空气接口端到端延迟为0.491 ms，满足URLLC 的时延要求。

Conclusion: 多天线协同的 LLDR RFF 能有效缓解多径对指纹识别的影响且不增加时延，具有较高的识别精度和极低的端到端延迟，适用于5G/URLLC 场景下的设备鉴别。

Abstract: Ultra-low latency, the hallmark of fifth-generation mobile communications (5G), imposes exacting timing demands on identification as well. Current cryptographic solutions introduce additional computational overhead, which results in heightened identification delays. Radio frequency fingerprint (RFF) identifies devices at the physical layer, blocking impersonation attacks while significantly reducing latency. Unfortunately, multipath channels compromise RFF accuracy, and existing channel-resilient methods demand feedback or processing across multiple time points, incurring extra signaling latency. To address this problem, the paper introduces a new RFF extraction technique that employs signals from multiple receiving antennas to address multipath issues without adding latency. Unlike single-domain methods, the Log-Linear Delta Ratio (LLDR) of co-temporal channel frequency responses (CFRs) from multiple antennas is employed to preserve discriminative RFF features, eliminating multi-time sampling and reducing acquisition time. To overcome the challenge of the reliance on minimal channel variation, the frequency band is segmented into sub-bands, and the LLDR is computed within each sub-band individually. Simulation results indicate that the proposed scheme attains a 96.13% identification accuracy for 30 user equipments (UEs) within a 20-path channel under a signal-to-noise ratio (SNR) of 20 dB. Furthermore, we evaluate the theoretical latency using the Roofline model, resulting in the air interface latency of 0.491 ms, which satisfies ultra-reliable and low-latency communications (URLLC) latency requirements.

</details>


### [27] [iSeal: Encrypted Fingerprinting for Reliable LLM Ownership Verification](https://arxiv.org/abs/2511.08905)
*Zixun Xiong,Gaoyi Wu,Qingyang Yu,Mingyu Derek Ma,Lingfeng Yao,Miao Pan,Xiaojiang Du,Hao Wang*

Main category: cs.CR

TL;DR: iSeal 提供一种面向端到端对抗“模型窃贼控制”的 LLM 指纹鉴定方法，通过在模型及外部模块中嵌入独特特征，并辅以误差纠正与基于相似度的验证策略，在验证时对抗联合撤销指纹与输出操控等攻击，达到对 12 个 LLM 的 10 余种攻击实现 100% 指纹成功率。


<details>
  <summary>Details</summary>
Motivation: 在大模型高昂的训练成本背景下，保护 LLM 的知识产权成为关键，但现有指纹鉴定多依赖模型特征，易被窃贼在推理阶段完全控制来进行指纹撤销或回避；因此需要一种对攻击具有强鲁棒性的鉴定方法。

Method: 提出 iSeal，通过在模型和外部模块中注入独特指纹特征，辅以误差纠正机制与基于相似度的验证策略，使指纹在推理过程被完全控制的攻击情境下仍然可检出，理论分析与大规模实验支持其鲁棒性。

Result: 在 12 种 LLM 上对超过 10 种攻击情形，iSeal 实现 100% 指纹成功率，而基线方法在撤销与输出操控场景中失效。

Conclusion: iSeal 提供面向端到端攻击的指纹验证新范式，显著提升在攻击者掌控推理过程时的指纹鲁棒性与可验证性，并优于现有基线。

Abstract: Given the high cost of large language model (LLM) training from scratch, safeguarding LLM intellectual property (IP) has become increasingly crucial. As the standard paradigm for IP ownership verification, LLM fingerprinting thus plays a vital role in addressing this challenge. Existing LLM fingerprinting methods verify ownership by extracting or injecting model-specific features. However, they overlook potential attacks during the verification process, leaving them ineffective when the model thief fully controls the LLM's inference process. In such settings, attackers may share prompt-response pairs to enable fingerprint unlearning or manipulate outputs to evade exact-match verification. We propose iSeal, the first fingerprinting method designed for reliable verification when the model thief controls the suspected LLM in an end-to-end manner. It injects unique features into both the model and an external module, reinforced by an error-correction mechanism and a similarity-based verification strategy. These components are resistant to verification-time attacks, including collusion-based fingerprint unlearning and response manipulation, backed by both theoretical analysis and empirical results. iSeal achieves 100 percent Fingerprint Success Rate (FSR) on 12 LLMs against more than 10 attacks, while baselines fail under unlearning and response manipulations.

</details>


### [28] [DeepTracer: Tracing Stolen Model via Deep Coupled Watermarks](https://arxiv.org/abs/2511.08985)
*Yunfei Yang,Xiaojun Chen,Yuexin Xuan,Zhendong Zhao,Xin Zhao,He Li*

Main category: cs.CR

TL;DR: 提出了一个鲁棒的模型水印框架DeepTracer，通过同类耦合损失和特定水印样本构造，以及筛选机制，在模型窃取攻击下提升水印的可验证性和鲁棒性，达到对现有水印方法的超越。


<details>
  <summary>Details</summary>
Motivation: 现有水印在面对模型窃取攻击时容易被移除或失效，导致水印难以在被窃取的模型中证实所有权，因此需要分析失败原因并提出更鲁棒的水印方案。

Method: 提出DeepTracer框架：1) 新颖的水印样本构造方法；2) 同类耦合损失约束，使水印任务与主任务高度耦合，窃取者在盗用主任务功能时不可避免地学习水印任务；3) 水印样本筛选机制，谨慎选择用于所有权验证的关键样本以提升水印可靠性。

Result: 在多数据集和多模型上进行广泛实验，结果表明该方法在防御多种模型窃取攻击以及水印攻击方面优于现有方法，达到新的状态-艺术的有效性与鲁棒性。

Conclusion: 通过高耦合的水印任务与主任务，以及精心筛选的水印样本，DeepTracer实现了对模型所有权的更可靠验证，显著提升对窃取与水印攻击的鲁棒性与有效性。

Abstract: Model watermarking techniques can embed watermark information into the protected model for ownership declaration by constructing specific input-output pairs. However, existing watermarks are easily removed when facing model stealing attacks, and make it difficult for model owners to effectively verify the copyright of stolen models. In this paper, we analyze the root cause of the failure of current watermarking methods under model stealing scenarios and then explore potential solutions. Specifically, we introduce a robust watermarking framework, DeepTracer, which leverages a novel watermark samples construction method and a same-class coupling loss constraint. DeepTracer can incur a high-coupling model between watermark task and primary task that makes adversaries inevitably learn the hidden watermark task when stealing the primary task functionality. Furthermore, we propose an effective watermark samples filtering mechanism that elaborately select watermark key samples used in model ownership verification to enhance the reliability of watermarks. Extensive experiments across multiple datasets and models demonstrate that our method surpasses existing approaches in defending against various model stealing attacks, as well as watermark attacks, and achieves new state-of-the-art effectiveness and robustness.

</details>


### [29] [MedHE: Communication-Efficient Privacy-Preserving Federated Learning with Adaptive Gradient Sparsification for Healthcare](https://arxiv.org/abs/2511.09043)
*Farjana Yesmin*

Main category: cs.CR

TL;DR: MedHE 是一个面向医疗保健的隐私保护联邦学习框架，将自适应梯度稀疏化与 CKKS 同态加密结合，实现对敏感医疗数据的隐私协作学习，同时大幅降低通信量并提供形式化安全与差分隐私保证。


<details>
  <summary>Details</summary>
Motivation: 解决医疗场景下联邦学习的隐私与通信瓶颈，使得资源受限的医疗机构能够在不暴露敏感数据的前提下进行协作学习，同时保持模型效用。

Method: 引入带有误差补偿的自适应阈值顶-k 梯度选择的动态阈值机制，并与 CKKS 同态加密结合；对 RLWE 假设下的形式化安全性进行分析，并给出 epsilon ≤ 1.0 的差分隐私保证；在 5 次独立试验中评估准确率与通信成本；可扩展至 100+ 机构，符合 HIPAA 要求。

Result: 实现了 97.5% 的通信量降低（从 1277 MB 降至 32 MB/轮），准确率为 89.5% ± 0.8%，与标准联邦学习性能相当（p=0.32）；显著降低计算与通信开销，具备在真实医疗部署中的可行性并符合 HIPAA 合规与百机构扩展性。

Conclusion: MedHE 提供一个在医疗场景中可行的隐私保护联邦学习解决方案，在强隐私保障与高模型效用、低通信成本之间取得平衡，适合在临床环境中大规模部署。

Abstract: Healthcare federated learning requires strong privacy guarantees while maintaining computational efficiency across resource-constrained medical institutions. This paper presents MedHE, a novel framework combining adaptive gradient sparsification with CKKS homomorphic encryption to enable privacy-preserving collaborative learning on sensitive medical data. Our approach introduces a dynamic threshold mechanism with error compensation for top-k gradient selection, achieving 97.5 percent communication reduction while preserving model utility. We provide formal security analysis under Ring Learning with Errors assumptions and demonstrate differential privacy guarantees with epsilon less than or equal to 1.0. Statistical testing across 5 independent trials shows MedHE achieves 89.5 percent plus or minus 0.8 percent accuracy, maintaining comparable performance to standard federated learning (p=0.32) while reducing communication from 1277 MB to 32 MB per training round. Comprehensive evaluation demonstrates practical feasibility for real-world medical deployments with HIPAA compliance and scalability to 100 plus institutions.

</details>


### [30] [Attack-Centric by Design: A Program-Structure Taxonomy of Smart Contract Vulnerabilities](https://arxiv.org/abs/2511.09051)
*Parsa Hedayatnia,Tina Tavakkoli,Hadi Amini,Mohammad Allahbakhsh,Haleh Amintoosi*

Main category: cs.CR

TL;DR: 提出一个面向攻击的程序结构化分类法，将Solidity漏洞统一归纳为八大根本原因，涵盖控制流、对外调用、状态完整性、算术安全、环境依赖、访问控制、输入校验和跨域协议假设，并通过静态/动态/学习型工具进行检测信号映射与数据集对比，提供一致的术语和可落地的核对清单。


<details>
  <summary>Details</summary>
Motivation: 现有的漏洞分类与工具多以症状（如重入）分散，难以形成可对比、可复用的审计与教育体系。通过以攻击原因为导向的结构化 taxonomy，提升可解释性、复现性与检测覆盖度，并促进研究者与实践者之间的知识共享。

Method: 提出以攻击为中心的程序结构 taxonomy，将Solidity漏洞分为八个根本原因家族，覆盖控制流、对外调用、状态完整性、算术安全、环境依赖、访问控制、输入校验、以及跨域协议假设。每个家族配以简短的Solidity示例、利用机制与缓解措施，并链接到可被静态、动态和学习型工具检测到的信号。对Legacy数据集（SmartBugs、SolidiFI）进行跨映射，以揭示标签漂移和覆盖不足。

Result: 建立了一个统一、可操作的漏洞分类体系，提供一致的术语、可执行的检查清单、对检测工具的映射能力，以及对历史数据集的评估，提升可解释性、审计可重复性与安全教育的系统性。

Conclusion: 该 taxonomy 为研究与实践提供了稳定的知识框架，能够更好地组织检测、 audits 与教育资源，并促使未来工作聚焦于根本原因而非单一症状，从而提高Solidity生态的安全性与可维护性。

Abstract: Smart contracts concentrate high value assets and complex logic in small, immutable programs, where even minor bugs can cause major losses. Existing taxonomies and tools remain fragmented, organized around symptoms such as reentrancy rather than structural causes. This paper introduces an attack-centric, program-structure taxonomy that unifies Solidity vulnerabilities into eight root-cause families covering control flow, external calls, state integrity, arithmetic safety, environmental dependencies, access control, input validation, and cross-domain protocol assumptions. Each family is illustrated through concise Solidity examples, exploit mechanics, and mitigations, and linked to the detection signals observable by static, dynamic, and learning-based tools. We further cross-map legacy datasets (SmartBugs, SolidiFI) to this taxonomy to reveal label drift and coverage gaps. The taxonomy provides a consistent vocabulary and practical checklist that enable more interpretable detection, reproducible audits, and structured security education for both researchers and practitioners.

</details>


### [31] [Toward an Intrusion Detection System for a Virtualization Framework in Edge Computing](https://arxiv.org/abs/2511.09068)
*Everton de Matos,Hazaa Alameri,Willian Tessaro Lunardi,Martin Andreoni,Eduardo Viegas*

Main category: cs.CR

TL;DR: LDPI as an isolated DL-based anomaly detector on edge, evaluated against signature-based IDS, showing near-perfect AUC and favorable overhead for edge security.


<details>
  <summary>Details</summary>
Motivation: Enhance security at the edge by combining deep learning anomaly detection with virtualization-based isolation to minimize attack surface and compare performance/overhead with traditional signature-based IDS.

Method: Deploy LDPI as an isolated service within a virtualization framework on a laptop-class edge node. Evaluate under identical workloads against Suricata and Snort, and assess detection of network flooding attacks. Report metrics including AUC (5-fold mean) across packet-window settings (n, l) and F1 at conservative thresholds.

Result: LDPI achieved AUC 0.999 (5-fold mean) across evaluated (n, l) settings with high F1 at conservative operating points. In overhead/performance tests, it was analyzed against Suricata/Snort under identical workloads and evaluated for network flooding scenarios, demonstrating strong detection while maintaining manageable overhead in the isolated environment.

Conclusion: Integrating LDPI as an isolated service within a virtualization framework provides effective edge-based anomaly detection with strong accuracy and acceptable overhead, offering a viable alternative or complement to signature-based IDS for gateway/edge security.

Abstract: Edge computing pushes computation closer to data sources, but it also expands the attack surface on resource-constrained devices. This work explores the deployment of the Lightweight Deep Anomaly Detection for Network Traffic (LDPI) integrated as an isolated service within a virtualization framework that provides security by separation. LDPI, adopting a Deep Learning approach, achieved strong training performance, reaching AUC 0.999 (5-fold mean) across the evaluated packet-window settings (n, l), with high F1 at conservative operating points. We deploy LDPI on a laptop-class edge node and evaluate its overhead and performance in two scenarios: (i) comparing it with representative signature-based IDSes (Suricata and Snort) deployed on the same framework under identical workloads, and (ii) while detecting network flooding attacks.

</details>


### [32] [Improving Sustainability of Adversarial Examples in Class-Incremental Learning](https://arxiv.org/abs/2511.09088)
*Taifeng Liu,Xinjing Liu,Liangqiu Dong,Yang Liu,Yilong Yang,Zhuo Ma*

Main category: cs.CR

TL;DR: 提出 SAE，通过语义修正模块和过滤增强模块提升对 CIL 下对抗样本的鲁棒性，使其在模型更新后仍然有效；在 9 倍类增加的设置下，平均提升 31.28%。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本多针对静态模型设计，随着类别增量学习（CIL）的引入，模型将不断更新，导致对抗样本的语义在域迁移中发生漂移，难以持续有效。仅依赖初始模型优化对抗样本易产生过拟合，且在新数据分布下对抗样本易失效。因此，需在跨域条件下提升对抗样本的语义鲁棒性与泛化能力。

Method: 提出 SAE：1) 语义修正模块，结合视觉-语言模型产生的普适语义，及利用 CIL 模型纠正优化方向，使对抗样本语义更接近目标类别且与其他类别区分开；2) 过滤与增强模块，先在潜在空间中识别具有目标类别语义的非目标样本，再通过数据增强促使语义更稳定。该框架旨在减少对初始模型依赖带来的过拟合，并提升在后续模型更新中的鲁棒性。

Result: 在将类别数目增加 9 倍的实验设置中，SAE 相较基线平均提升 31.28%。

Conclusion: SAE 能显著提升对抗样本在类别增量更新过程中的可持续性与鲁棒性，通过将语义泛化、方向校正与筛选-增强机制相结合，有效缓解域漂移导致的语义退化。

Abstract: Current adversarial examples (AEs) are typically designed for static models. However, with the wide application of Class-Incremental Learning (CIL), models are no longer static and need to be updated with new data distributed and labeled differently from the old ones. As a result, existing AEs often fail after CIL updates due to significant domain drift. In this paper, we propose SAE to enhance the sustainability of AEs against CIL. The core idea of SAE is to enhance the robustness of AE semantics against domain drift by making them more similar to the target class while distinguishing them from all other classes. Achieving this is challenging, as relying solely on the initial CIL model to optimize AE semantics often leads to overfitting. To resolve the problem, we propose a Semantic Correction Module. This module encourages the AE semantics to be generalized, based on a visual-language model capable of producing universal semantics. Additionally, it incorporates the CIL model to correct the optimization direction of the AE semantics, guiding them closer to the target class. To further reduce fluctuations in AE semantics, we propose a Filtering-and-Augmentation Module, which first identifies non-target examples with target-class semantics in the latent space and then augments them to foster more stable semantics. Comprehensive experiments demonstrate that SAE outperforms baselines by an average of 31.28% when updated with a 9-fold increase in the number of classes.

</details>


### [33] [Differentially Private Rankings via Outranking Methods and Performance Data Aggregation](https://arxiv.org/abs/2511.09120)
*Luis Del Vasto-Terrientes*

Main category: cs.CR

TL;DR: 将多准则决策方法（MCDM）与差分隐私（DP）结合的隐私保护排序框架，通过对用户评估的聚合实现对个体贡献的隐私保护；实验显示真排序与匿名化排序具有高相关性，提供稳健的隐私保证。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统等数据驱动场景中，个人和敏感数据在决策中的作用日益重要，但将隐私机制与MCDM方法耦合的研究仍相对不足。

Method: 提出将MCDM的Outranking方法与差分隐私相结合的综合框架。核心包括一个预处理步骤：将多个用户评估聚合为一个综合性能矩阵；在该矩阵上应用差分隐私噪声以保护个体信息；在噪声化的性能矩阵上执行 outranking 以得到排序结果。

Result: 评估结果显示真排名与匿名化排名之间存在从强到非常强的统计相关性，且实现了对隐私参数的稳健保证。

Conclusion: 证明将DP与MCDM outranking方法结合的可行性与有效性，适用于需要保护个人隐私的排序与建议场景，并为后续在动态数据驱动环境中应用提供方向。

Abstract: Multiple-Criteria Decision Making (MCDM) is a sub-discipline of Operations Research that helps decision-makers in choosing, ranking, or sorting alternatives based on conflicting criteria. Over time, its application has been expanded into dynamic and data-driven domains, such as recommender systems. In these contexts, the availability and handling of personal and sensitive data can play a critical role in the decision-making process. Despite this increased reliance on sensitive data, the integration of privacy mechanisms with MCDM methods is underdeveloped. This paper introduces an integrated approach that combines MCDM outranking methods with Differential Privacy (DP), safeguarding individual contributions' privacy in ranking problems. This approach relies on a pre-processing step to aggregate multiple user evaluations into a comprehensive performance matrix. The evaluation results show a strong to very strong statistical correlation between the true rankings and their anonymized counterparts, ensuring robust privacy parameter guarantees.

</details>


### [34] [One Signature, Multiple Payments: Demystifying and Detecting Signature Replay Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2511.09134)
*Zexu Wang,Jiachi Chen,Zewei Lin,Wenqing Chen,Kaiwen Ning,Jianxing Yu,Yuming Feng,Yu Zhang,Weizhe Zhang,Zibin Zheng*

Main category: cs.CR

TL;DR: 提出并实证研究签名重放漏洞SRV，提出LASiR工具基于LLMs与符号执行的静态分析检测，在多链环境中广泛存在，相关资产可能达到数百万美元。


<details>
  <summary>Details</summary>
Motivation: 随着智能合约对数字签名的广泛依赖，未对签名使用条件进行充分约束可能导致重复验签、权限滥用和合约资产风险，因此需要系统性识别与检测SRV。

Method: 对1,419份审计报告中108份有详细SRV描述的案例进行分类，提出五类SRV。设计LASiR，利用大型语言模型的语义理解能力辅助静态污点分析以识别签名重用，并通过符号执行的路径可达性验证提升检测的有效性与可靠性。对15,383个包含签名验证的合约进行大规模评估，涵盖Ethereum、Binance Smart Chain、Polygon与Arbitrum四条链。

Result: SRV普遍存在，相关合约共持有约476万美元的活跃资产；在以太坊上，使用签名的合约中有SRV的比例为19.63%。人工验证表明LASiR的F1分数为87.90%，消融研究和对比实验显示LLMs提供的语义信息显著提升静态污点分析的检测性能。

Conclusion: 这是首次对SRV进行系统性实证研究并提出可自动检测的LASiR方法，表明需要在设计与审计阶段加强对签名使用条件的约束，以及结合LLMs与符号执行的检测框架以提升智能合约的安全性与可扩展性。

Abstract: Smart contracts have significantly advanced blockchain technology, and digital signatures are crucial for reliable verification of contract authority. Through signature verification, smart contracts can ensure that signers possess the required permissions, thus enhancing security and scalability. However, lacking checks on signature usage conditions can lead to repeated verifications, increasing the risk of permission abuse and threatening contract assets. We define this issue as the Signature Replay Vulnerability (SRV). In this paper, we conducted the first empirical study to investigate the causes and characteristics of the SRVs. From 1,419 audit reports across 37 blockchain security companies, we identified 108 with detailed SRV descriptions and classified five types of SRVs. To detect these vulnerabilities automatically, we designed LASiR, which utilizes the general semantic understanding ability of Large Language Models (LLMs) to assist in the static taint analysis of the signature state and identify the signature reuse behavior. It also employs path reachability verification via symbolic execution to ensure effective and reliable detection. To evaluate the performance of LASiR, we conducted large-scale experiments on 15,383 contracts involving signature verification, selected from the initial dataset of 918,964 contracts across four blockchains: Ethereum, Binance Smart Chain, Polygon, and Arbitrum. The results indicate that SRVs are widespread, with affected contracts holding $4.76 million in active assets. Among these, 19.63% of contracts that use signatures on Ethereum contain SRVs. Furthermore, manual verification demonstrates that LASiR achieves an F1-score of 87.90% for detection. Ablation studies and comparative experiments reveal that the semantic information provided by LLMs aids static taint analysis, significantly enhancing LASiR's detection performance.

</details>


### [35] [Unveiling Hidden Threats: Using Fractal Triggers to Boost Stealthiness of Distributed Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2511.09252)
*Jian Wang,Hong Shen,Chan-Tong Lam*

Main category: cs.CR

TL;DR: 提出了一种基于分形自相似性的分布式后门攻击（FTDBA），通过增强子触发器的特征强度显著降低污染数据需求并提高隐蔽性，同时引入动态角度扰动以在训练过程中平衡效率与隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 传统分布式后门攻击需要较大比例的污染数据以维持攻击强度，且易被检测到。本研究旨在通过分形特征提升触发器强度并降低污染量，同时通过动态扰动缓解在频域和梯度域的可检测性。

Method: 利用分形的自相似性放大子触发器的特征强度，以减少所需污染数据量；引入动态角度扰动，在训练阶段自适应调整扰动强度以平衡攻击效率与隐蔽性。

Result: 在实验中，FTDBA达到92.3%的攻击成功率，仅需传统DBA 62.4%的污染体积；检测率降低22.8%，KL散度下降41.2%。

Conclusion: FTDBA提供了一种低暴露、高效率的联邦学习后门攻击范式，并拓展了分形特征在对抗样本生成中的应用。

Abstract: Traditional distributed backdoor attacks (DBA) in federated learning improve stealthiness by decomposing global triggers into sub-triggers, which however requires more poisoned data to maintian the attck strength and hence increases the exposure risk. To overcome this defect, This paper proposes a novel method, namely Fractal-Triggerred Distributed Backdoor Attack (FTDBA), which leverages the self-similarity of fractals to enhance the feature strength of sub-triggers and hence significantly reduce the required poisoning volume for the same attack strength. To address the detectability of fractal structures in the frequency and gradient domains, we introduce a dynamic angular perturbation mechanism that adaptively adjusts perturbation intensity across the training phases to balance efficiency and stealthiness. Experiments show that FTDBA achieves a 92.3\% attack success rate with only 62.4\% of the poisoning volume required by traditional DBA methods, while reducing the detection rate by 22.8\% and KL divergence by 41.2\%. This study presents a low-exposure, high-efficiency paradigm for federated backdoor attacks and expands the application of fractal features in adversarial sample generation.

</details>


### [36] [SecTracer: A Framework for Uncovering the Root Causes of Network Intrusions via Security Provenance](https://arxiv.org/abs/2511.09266)
*Seunghyeon Lee,Hyunmin Seo,Hwanjo Heo,Anduo Wang,Seungwon Shin,Jinwoo Kim*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern enterprise networks comprise diverse and heterogeneous systems that support a wide range of services, making it challenging for administrators to track and analyze sophisticated attacks such as advanced persistent threats (APTs), which often exploit multiple vectors. To address this challenge, we introduce the concept of network-level security provenance, which enables the systematic establishment of causal relationships across hosts at the network level, facilitating the accurate identification of the root causes of security incidents. Building on this concept, we present SecTracer as a framework for a network-wide provenance analysis. SecTracer offers three main contributions: (i) comprehensive and efficient forensic data collection in enterprise networks via software-defined networking (SDN), (ii) reconstruction of attack histories through provenance graphs to provide a clear and interpretable view of intrusions, and (iii) proactive attack prediction using probabilistic models. We evaluated the effectiveness and efficiency of SecTracer through a real-world APT simulation, demonstrating its capability to enhance threat mitigation while introducing less than 1% network throughput overhead and negligible latency impact.

</details>


### [37] [Quantum Meet-in-the-Middle Attacks on Key-Length Extension Constructions](https://arxiv.org/abs/2511.09351)
*Min Liang,Ruihao Gao,Jiali Wu*

Main category: cs.CR

TL;DR: 提出并分析多种量子MITM攻击用于KLE构造，表明在资源充足的情况下2kTE的安全性在Q2模型下下降，3XCE在Q1模型下具有量子加速，且攻击框架可扩展到SITM。


<details>
  <summary>Details</summary>
Motivation: 评估密钥长度扩展在量子时代的安全性，并为KLE构造提供量子对抗下的攻击与防御思路。

Method: 提出两类量子MITM攻击：对2kTE的量子MITM（QCF算法与Grover-based）以及面向3XCE的Q1模型无QRAM攻击；并构建通用量子SITM框架 ELE=E^2∘L∘E^1，给出不同中间层L的具体攻击。

Result: 2kTE：QCF攻击时间/资源为O(2^{2κ/3})；Grover攻击为O(2^{κ/2})，QRAM分别为O(2^{2κ/3})与O(2^{κ})。3XCE在Q1模型无QRAM攻击，时间为O(2^{(κ+n)/2})，较经典MITM有平方级加速。SITM框架被扩展到更广泛的构造。

Conclusion: 量子MITM与SITM方法揭示KLE构造在量子时代的潜在脆弱性，提供了系统化的攻击框架，同时提示需要在设计KLE时强化量子安全性考虑。

Abstract: Key-length extension (KLE) techniques provide a general approach to enhancing the security of block ciphers by using longer keys. There are mainly two classes of KLE techniques, cascade encryption and XOR-cascade encryption. This paper presents several quantum meet-in-the-middle (MITM) attacks against two specific KLE constructions.
  For the two-key triple encryption (2kTE), we propose two quantum MITM attacks under the Q2 model. The first attack, leveraging the quantum claw-finding (QCF) algorithm, achieves a time complexity of $O(2^{2κ/3})$ with $O(2^{2κ/3})$ quantum random access memory (QRAM). The second attack, based on Grover's algorithm, achieves a time complexity of $O(2^{κ/2})$ with $O(2^κ)$ QRAM. The latter complexity is nearly identical to Grover-based brute-force attack on the underlying block cipher, indicating that 2kTE does not enhance security under the Q2 model when sufficient QRAM resources are available.
  For the 3XOR-cascade encryption (3XCE), we propose a quantum MITM attack applicable to the Q1 model. This attack requires no QRAM and has a time complexity of $O(2^{(κ+n)/2})$ ($κ$ and $n$ are the key length and block length of the underlying block cipher, respectively.), achieving a quadratic speedup over classical MITM attack.
  Furthermore, we extend the quantum MITM attack to quantum sieve-in-the-middle (SITM) attack, which is applicable for more constructions. We present a general quantum SITM framework for the construction $ELE=E^2\circ L\circ E^1$ and provide specific attack schemes for three different forms of the middle layer $L$. The quantum SITM attack technique can be further applied to a broader range of quantum cryptanalysis scenarios.

</details>


### [38] [Enhancing Password Security Through a High-Accuracy Scoring Framework Using Random Forests](https://arxiv.org/abs/2511.09492)
*Muhammed El Mustaqeem Mazelan,Noor Hazlina Abdul,Nouar AlDahoul*

Main category: cs.CR

TL;DR: 提出了一种基于混合特征的密码强度评分系统，比较随机森林、SVM、CNN、逻辑回归四种模型，随机森林表现最佳且具有良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的密码强度度量往往依赖静态规则（如字符类型要求），无法有效抵抗常见的密码模式，导致用户产生安全的错觉；需要更贴近真实攻击特征的评估方法。

Method: 基于超过66万条真实密码的数据集，提出混合特征工程：leetspeak归一化的香农熵以衡量真随机性、键盘走步与序列模式检测以捕捉常见输入规律、字符级TF-IDF n-grams以识别泄露密码中的重复子串；并对RF、SVM、CNN、LR四种模型进行比较，强调RF的可解释性。

Result: 随机森林在独立测试集上达到99.12%的准确率；各模型间存在性能差异，RF的特征重要性分析可为实际安全工具提供改进线索。

Conclusion: 提出的高性能混合特征密码强度评分系统在提升预测准确性的同时，增强了可解释性，能够提供具体、可操作的用户反馈，帮助降低基于密码的漏洞风险。

Abstract: Password security plays a crucial role in cybersecurity, yet traditional password strength meters, which rely on static rules like character-type requirements, often fail. Such methods are easily bypassed by common password patterns (e.g., 'P@ssw0rd1!'), giving users a false sense of security. To address this, we implement and evaluate a password strength scoring system by comparing four machine learning models: Random Forest (RF), Support Vector Machine (SVM), a Convolutional Neural Network (CNN), and Logistic Regression with a dataset of over 660,000 real-world passwords. Our primary contribution is a novel hybrid feature engineering approach that captures nuanced vulnerabilities missed by standard metrics. We introduce features like leetspeak-normalized Shannon entropy to assess true randomness, pattern detection for keyboard walks and sequences, and character-level TF-IDF n-grams to identify frequently reused substrings from breached password datasets. our RF model achieved superior performance, achieving 99.12% accuracy on a held-out test set. Crucially, the interpretability of the Random Forest model allows for feature importance analysis, providing a clear pathway to developing security tools that offer specific, actionable feedback to users. This study bridges the gap between predictive accuracy and practical usability, resulting in a high-performance scoring system that not only reduces password-based vulnerabilities but also empowers users to make more informed security decisions.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [39] [Energy-Workload Coupled Migration Optimization Strategy for Virtual Power Plants with Data Centers Considering Fuzzy Chance Constraints](https://arxiv.org/abs/2511.08619)
*Jia-Kai Wu,Zhi-Wei Liu,Yong Zhao,Yan-Wu Wang,Fan-Rong Qu,Chaojie Li*

Main category: eess.SY

TL;DR: 提出一个与数据中心工作负载耦合的虚拟电厂能耗-工作负载迁移优化框架，通过博弈-多方协调、模糊机会约束的确定性等效、以及ADMM解耦实现高维非凸优化，显著提升DR曲线跟踪精度并降低成本。


<details>
  <summary>Details</summary>
Motivation: 数据中心工作负载存在稀疏性，传统概率建模在DR场景中受限；需要跨区域资源协同与公平分配，同时保持解的可计算性。

Method: 建立以反对称矩阵表征的跨区域资源协调的博弈耦合迁移框架；利用模糊集理论将模糊概率约束转化为确定性等效，非凸随机问题转化为可求解的二阶锥规划；提出改进的Shapley值分配方法以提升公平性与计算可行性；采用ADMM带共识变量分割，将耦合的反对称约束分解为带解析解的子问题。

Result: 基于谷歌多数据中心的真实数据仿真表明，所提框架在提高DR曲线跟踪精度和降低运营成本方面有效。

Conclusion: 本文构建的耦合迁移博弈+确定性等效+ADMM求解的整合框架，提供了实现DR曲线精确跟踪与成本优化的可行路径，并在真实数据上验证了方法的有效性和实用性。

Abstract: This paper proposes an energy-workload coupled migration optimization strategy for virtual power plants (VPPs) with data centers (DCs) to enhance resource scheduling flexibility and achieve precise demand response (DR) curve tracking. A game-based coupled migration framework characterized by antisymmetric matrices is first established to facilitate the coordination of cross-regional resource allocation between VPPs. To address the challenge posed to conventional probabilistic modeling by the inherent data sparsity of DC workloads, deterministic equivalent transformations of fuzzy chance constraints are derived based on fuzzy set theory, and non-convex stochastic problems are transformed into a solvable second-order cone program. To address the multi-player interest coordination problem in cooperative games, an improved Shapley value profit allocation method with the VPP operator as intermediary is proposed to achieve a balance between theoretical fairness and computational feasibility. In addition, the alternating direction method of multipliers with consensus-based variable splitting is introduced to solve the high-dimensional non-convex optimization problem, transforming coupled antisymmetric constraints into separable subproblems with analytical solutions. Simulations based on real data from Google's multiple DCs demonstrate the effectiveness of the proposed method in improving DR curve tracking precision and reducing operational costs.

</details>


### [40] [Dynamic Modeling and Control of Phosphate-Pebble Drying Systems - A Comprehensive Approach](https://arxiv.org/abs/2511.08623)
*Jose M. Campos-Salazar,Felipe Santander,Eduardo Keim*

Main category: eess.SY

TL;DR: 提出基于第一性原理的磷矿石球粒旋转干燥过程的非线性动态模型，聚焦热质传递耦合、蒸发动力学及子系统耦合。


<details>
  <summary>Details</summary>
Motivation: 干燥过程的非线性、多变量特性使得准确建模与控制困难，亟需统一的理论框架以提升能效与处理性。

Method: 从第一原理出发，建立耦合热传导、质量传递、蒸发动力学的非线性微分方程模型，并纳入不同子系统的相互作用。

Result: 为磷酸岩球粒干燥过程提供一个可描述关键动力学的模型框架，便于仿真、参数辨识和控制设计；在摘要中未给出具体数值结果。

Conclusion: 该模型为后续的控制策略开发与工艺优化提供理论基础，需结合实验数据进行参数辨识与模型验证。

Abstract: Dryers play a central role in the processing of phosphate rock, where moisture removal is essential for downstream handling and energy efficiency. Due to the inherently nonlinear and multivariable nature of these systems, accurate modeling and control remain industrial challenges. This article presents a comprehensive nonlinear dynamic model of a phosphate-pebble rotary drying process, built from first principles to capture coupled heat and mass transfer, evaporation kinetics, and subsystem interactions.

</details>


### [41] [Recursive Binary Identification under Data Tampering and Non-Persistent Excitation with Application to Emission Control](https://arxiv.org/abs/2511.08629)
*Jian Guo,Lihong Pei,Wenchao Xue,Yanlong Zhao,Ji-Feng Zhang*

Main category: eess.SY

TL;DR: 提出了面向带二进制输出且可能被对手篡改的数据的在线参数估计方法。给出两种算法：一阶梯度递推与二阶准牛顿，能处理已知/未知篡改并在几乎必然收敛；放宽经典最小二乘所需的激励条件，且可扩展到自适应控制，给出误差上界。通过数值仿真和车辆排放控制应用验证鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 在控制-物理系统（CPS）中需要实时在线参数估计，但现有方法多为离线，无法实时适应对数据进行篡改的威胁。需要在二进制输出和潜在对手篡改场景下实现鲁棒、实时的学习与控制。

Method: 提出两类在线估计算法：1) 一阶梯度基算法，递归地利用新数据更新参数；2) 二阶准牛顿算法，在不依赖持续激励（PE）条件的情况下实现更快收敛。两者均考虑已知/未知篡改策略的版本，并给出收敛性证明（几乎 surely 收敛）。二阶算法的收敛条件匹配随机回归中的最小激励要求。并将二阶算法扩展至自适应控制框架，给出对二进制输出FIR系统在未知篡改下的跟踪误差上界。

Result: 理论上证明参数估计收敛（几乎必然收敛）。二阶算法在弱化的激励条件下也能收敛，且提供实现上的更快收敛。给出明确的二阶算法在自适应控制中的跟踪误差界，数值仿真验证鲁棒性。车辆排放控制应用中，该方法增强对超标排放事件的检测准确性。

Conclusion: 所提在线算法可在带二进制输出且存在未知/已知篡改的情形下实现鲁棒在线参数学习，具有理论保证和实际效用，且经数值验证和应用示例证明其有效性。

Abstract: This paper studies the problem of online parameter estimation for cyber-physical systems with binary outputs that may be subject to adversarial data tampering. Existing methods are primarily offline and unsuitable for real-time learning. To address this issue, we first develop a first-order gradient-based algorithm that updates parameter estimates recursively using incoming data. Considering that persistent excitation (PE) conditions are difficult to satisfy in feedback control scenarios, a second-order quasi-Newton algorithm is proposed to achieve faster convergence without requiring the PE condition. For both algorithms, corresponding versions are developed to handle known and unknown tampering strategies, and their parameter estimates are proven to converge almost surely over time. In particular, the second-order algorithm ensures convergence under a signal condition that matches the minimal excitation required by classical least-squares estimation in stochastic regression models. The second-order algorithm is also extended to an adaptive control framework, providing an explicit upper bound on the tracking error for binary-output FIR systems under unknown tampering. Three numerical simulations verify the theoretical results and show that the proposed methods are robust against data tampering. Finally, the approach is validated via a vehicle emission control problem, where it effectively improves the detection accuracy of excess-emission events.

</details>


### [42] [RIS-based Communication Enhancement and Location Privacy Protection in UAV Networks](https://arxiv.org/abs/2511.09094)
*Ziqi Chen,Jun Du,Chunxiao Jiang,Tony Q. S. Quek,Zhu Han*

Main category: eess.SY

TL;DR: 提出一种基于主动可重构智能表面（ARIS）的隐蔽通信方案，通过虚拟分区和人工噪声（AN）保护源UAV（SU）的位置隐私，同时保持与合法接收 UAV（RU）的高效通信。通过对RSS定位的Cramér-Rao下界（CRLB）进行推导，建立通信增益与定位干扰的联合优化框架，给出分区与功率分配的最优解及其在平均信道条件下的实现方法，最后提出针对两个分区的反射前向设计与AN生成策略。仿真表明在提升MU对SU定位误差的同时不显著牺牲SU-RU的通信性能。


<details>
  <summary>Details</summary>
Motivation: 在开放的通信环境下，恶意无人机（MU）可能通过分析接收信号的RSS等信息来推测源UAV的位置，从而威胁SU的位置隐私。提出ARIS+AN的隐蔽通信方案，旨在提升隐私保护的同时确保通信效率。

Method: 将ARIS的反射元件分成两子区域：一组优化以提升SU与RU之间的通信速率，另一组产生AN以干扰MU对SU的定位。基于RSS的CRLB推导定位精度，构建联合优化问题以同时提升通信和干扰定位能力，推导最优分区和在平均信道下的功率分配，并设计两区域的反射相控与AN设计算法。

Result: 仿真结果显示，与基线方案相比，所提方案显著提高MU对SU的定位误差，同时维持SU-RU的高效通信性能。

Conclusion: ARIS+AN的分区式设计能够在不牺牲通信性能的前提下有效提升SU位置隐私，提供一种可行的隐蔽通信解决方案，并具有在鲁棒性和实现性方面的潜在优势。

Abstract: With the explosive advancement of unmanned aerial vehicles (UAVs), the security of efficient UAV networks has become increasingly critical. Owing to the open nature of its communication environment, illegitimate malicious UAVs (MUs) can infer the position of the source UAV (SU) by analyzing received signals, thus compromising the SU location privacy. To protect the SU location privacy while ensuring efficient communication with legitimate receiving UAVs (RUs), we propose an Active Reconfigurable Intelligent Surface (ARIS)-assisted covert communication scheme based on virtual partitioning and artificial noise (AN). Specifically, we design a novel ARIS architecture integrated with an AN module. This architecture dynamically partitions its reflecting elements into multiple sub-regions: one subset is optimized to enhance the communication rate between the SU and RUs, while the other subset generates AN to interfere with the localization of the SU by MUs. We first derive the Cramér-Rao Lower Bound (CRLB) for the localization with received signal strength (RSS), based on which, we establish a joint optimization framework for communication enhancement and localization interference. Subsequently, we derive and validate the optimal ARIS partitioning and power allocation under average channel conditions. Finally, tailored optimization methods are proposed for the reflection precoding and AN design of the two partitions. Simulation results validate that, compared to baseline schemes, the proposed scheme significantly increases the localization error of the SU by MUs while maintaining efficient communication between the SU and RUs, thereby effectively protecting the SU location privacy.

</details>


### [43] [Information-Driven Fault Detection and Identification for Multi-Agent Spacecraft Systems: Collaborative On-Orbit Inspection Mission](https://arxiv.org/abs/2511.08752)
*Akshita Gupta,Arna Bhardwaj,Yashwanth Kumar Nakka,Changrak Choi,Amir Rahmani*

Main category: eess.SY

TL;DR: 一个全球到局部的容错检测与定位（FDI）框架，用于低地球轨道多航天器协同检查任务，利用统一的信息驱动成本函数连接任务分配、传感与运动控制，具自适应阈值，且在仿真中证实对不确定条件下的故障定位与分类的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在自治多航天器协同检查任务中，需要对传感器、执行器与状态估计器的故障进行高鲁棒性检测与定位，同时将任务分配、感知和控制统一在一个信息驱动框架内以提高系统的韧性。

Method: 提出一个全局信息驱动的成本泛函，整合传感器模型、航天器姿态与任务级信息增益目标；通过同一成本函数驱动全局任务分配和局部感知/运动决策；通过对期望任务度量与观测度量的比较实现故障检测，利用高阶成本梯度实现对传感器、执行器和状态估计的故障定位；引入自适应阈值以捕捉时间变化的检查几何和动态任务条件；在多航天器检查场景的仿真中验证鲁棒性。

Result: 仿真结果在代表性多航天器检查场景中显示出对不确定性条件下的故障定位与分类的可靠性，为面向自律检查任务的韧性架构提供了一个统一的、信息驱动的基础。

Conclusion: 该框架将指导、控制和FDI有机整合，形成一个适用于自适应、信息驱动的多航天器自主检查体系的基础。

Abstract: This work presents a global-to-local, task-aware fault detection and identification (FDI) framework for multi-spacecraft systems conducting collaborative inspection missions in low Earth orbit. The inspection task is represented by a global information-driven cost functional that integrates the sensor model, spacecraft poses, and mission-level information-gain objectives. This formulation links guidance, control, and FDI by using the same cost function to drive both global task allocation and local sensing or motion decisions. Fault detection is achieved through comparisons between expected and observed task metrics, while higher-order cost-gradient measures enable the identification of faults among sensors, actuators, and state estimators. An adaptive thresholding mechanism captures the time-varying inspection geometry and dynamic mission conditions. Simulation results for representative multi-spacecraft inspection scenarios demonstrate the reliability of fault localization and classification under uncertainty, providing a unified, information-driven foundation for resilient autonomous inspection architectures.

</details>


### [44] [Grid Operational Benefit Analysis of Data Center Spatial Flexibility: Congestion Relief, Renewable Energy Curtailment Reduction, and Cost Saving](https://arxiv.org/abs/2511.08759)
*Haoxiang Wan,Linhan Fang,Xingpeng Li*

Main category: eess.SY

TL;DR: Spatial flexibility of data centers—migrating workloads geographically—can alleviate transmission constraints and renewable curtailment, potentially deferring grid upgrades.


<details>
  <summary>Details</summary>
Motivation: Rapid growth of data centers increases concentrated electricity demand, raising transmission congestion risks and compromising grid reliability amid intermittent solar/wind generation.

Method: Develop an optimal power flow model that co-optimizes generation, security reserves, and flexible data-center loads; validate with case studies on a modified IEEE 73-bus system to assess transmission constraints and solar curtailment.

Result: Inflexible data-center placement can cause significant line overloads (up to 30.1%). Enabling spatial flexibility reduces violations and restores feasibility, and can cut solar curtailment by up to 61% by reallocating load to solar-rich areas.

Conclusion: Spatial data-center flexibility is a viable mechanism to defer transmission upgrades and improve renewable utilization while maintaining grid reliability.

Abstract: Data centers are facilities housing computing infrastructure for processing and storing digital information. The rapid expansion of artificial intelligence is driving unprecedented growth in data center capacity, with global electricity demand from data centers projected to double by 2026. This growth creates substantial challenges for power transmission networks, as large concentrated loads can cause congestion and threaten grid reliability. Meanwhile, the intermittent nature of solar and wind generation requires flexible resources to maintain grid reliability and minimize curtailment. This paper assesses whether data center spatial flexibility-the ability to migrate computational workloads geographically-can serve as a grid resource to address these challenges. An optimal power flow model is developed to co-optimize generation dispatch, security reserves, and flexible data center loads. Case studies on a modified IEEE 73-bus system show that inflexible data center placement can lead to severe transmission violations, with line overloads reaching 30.1%. Enabling spatial flexibility mitigates these violations in the studied scenarios and restores system feasibility. This flexibility also reduces solar curtailment by up to 61.0% by strategically reallocating load to solar-rich areas. The results suggest that spatial flexibility offers a viable approach to defer transmission upgrades and enhance renewable utilization.

</details>


### [45] [Discovering and exploiting active sensing motifs for estimation](https://arxiv.org/abs/2511.08766)
*Benjamin Cellini,Burak Boyacioglu,Austin Lopez,Floris van Breugel*

Main category: eess.SY

TL;DR: 提出并实现了 BOUNDS 框架及 pybounds 包，用于在部分可观测的非线性动态系统中通过自适应传感器运动提升信息并改进状态估计，同时提出 AI-KF 将数据驱动估计与模型基估计结合。


<details>
  <summary>Details</summary>
Motivation: 缺乏可以量化传感器运动对估计性能贡献的工具，且在非线性、部分观测、带噪声的传感环境中，主动感知能够提升信息获取，需要数学严谨且实用的分析与实现。

Method: 提出 BOUNDS 框架和 Python 包 pybounds，能发现提升特定状态变量信息的传感器运动模式，并可应用于仿真或观测轨迹，同时考虑传感器噪声。通过一个以受限传感器的飞行体为案例，展示如何利用主动感知估计地速、高度和风向等变量。并提出 Augmented Information Kalman Filter (AI-KF)，将数据驱动状态与观测模型相结合，对散乱的主动感知估计进行融合与修正。

Result: 在飞行器案例中验证了 BOUNDS 能识别提高信息量的传感器轨迹模式，提升关键变量的可估计性，并通过 GPS-denied 的外部四旋翼飞行数据验证 AI-KF 的信息增强和融合能力。

Conclusion: 该框架有助于解码主动感知策略，指导传感器系统的状态估计算法设计，并为在传感器运动控制中实现更鲁棒的估计提供方法论。

Abstract: From organisms to machines, autonomous systems rely on measured sensory cues to estimate unknown information about themselves or their environment. For nonlinear systems, carefully selected sensor motion can be exploited to extract information that is otherwise unavailable, i.e. active sensing. Empirical, yet mathematically rigorous, tools are needed to (1) quantify how sensor movement can contribute to estimation performance, and (2) leverage this knowledge to improve state estimates. Here, we introduce "BOUNDS: Bounding Observability for Uncertain Nonlinear Dynamic Systems", and Python package pybounds, which can discover patterns of sensor motion that increase information for individual state variables. Crucially, it is suitable for partially observable nonlinear systems, accounts for sensor noise, and can be applied to either simulated or observed trajectories. We demonstrate BOUNDS through a case study on a flying agent with limited sensors, showing how active sensing can be leveraged to estimate key variables such as ground speed, altitude, and ambient wind direction. Finally, we present a framework to refine sporadic estimates from bouts of active sensing that combines data-driven state and observability estimation from artificial neural networks with model-based estimation, which we call the Augmented Information Kalman Filter (AI-KF). We validate our framework using altitude estimation given GPS-denied data from an outdoor quadcopter flight. Collectively, our work will help decode active sensing strategies and inform the design of estimation algorithms in sensorimotor systems.

</details>


### [46] [Incorporating the nonlinearity index into adaptive-mesh sequential convex optimization for minimum-fuel low-thrust trajectory design](https://arxiv.org/abs/2511.08837)
*Saeid Tafazzol,Ehsan Taheri*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Successive convex programming (SCP) is a powerful class of direct optimization methods, known for its polynomial complexity and computational efficiency, making it particularly suitable for autonomous applications. Direct methods are also referred to as ``discretize-then-optimize'' with discretization being a fundamental solution step. A key step in all practical direct methods is mesh refinement, which aims to refine the solution resolution by enhancing the precision and quality of discretization techniques through strategic distribution and placement of mesh/grid points. We propose a novel method to enhance adaptive mesh refinement stability by integrating it with a nonlinearity-index-based trust-region strategy within the SCP framework for spacecraft trajectory design. The effectiveness of the proposed method is demonstrated through solving minimum-fuel, low-thrust missions, including a benchmark Earth-to-Asteroid rendezvous and an Earth-Moon L2 Halo-to-Halo transfer using the Circular Restricted Three-Body (CR3BP) model.

</details>


### [47] [An Improved Dual-Attention Transformer-LSTM for Small-Sample Prediction of Modal Frequency and Actual Anchor Radius in Micro Hemispherical Resonator Design](https://arxiv.org/abs/2511.08900)
*Yuyi Yao,Gongliu Yang,Runzhuo Xu,Yongqiang Tu,Haozhou Mo*

Main category: eess.SY

TL;DR: 提出基于改进的Transformer-LSTM 的快速预测方法，用于在高温软化下的微型半球谐振器（MHR）设计中预测模态频率与实际锚点半径，从而显著提升设计效率。


<details>
  <summary>Details</summary>
Motivation: 解决MHR多配置设计评估的耗时仿真问题；在有限数据条件下实现高精度回归以降低工艺与材料试错成本。

Method: 在锚点半径、谐振器高度、边缘厚度等参数取值下进行有限元仿真与模态分析，获得前六个模态频率和实际锚点半径；将Transformer替换为双MHSA机制以提升隐藏信息捕获能力，构建改进的Transformer-LSTM模型进行回归预测。

Result: 预测准确度达到约96.35%，与传统有限元方法相比，计算时间缩短至1/48000；通过消融与对比实验验证方法优越性，支持快速评估MHR模态特性与制造可行性。

Conclusion: 为在复杂工艺条件下的MEMS设计提供新的智能范式，显著提升设计效率并可扩展到其他类似高温或复杂工艺的微系统设计。

Abstract: The high-temperature glassblowing-fabricated micro hemispherical resonator (MHR) exhibits high symmetry and high Q-value for precision inertial navigation. However, MHR design entails a comprehensive evaluation of multiple possible configurations and demands extremely time-consuming simulation of key parameters combination. To address this problem, this paper proposed a rapid prediction method of modal frequency and actual anchor radius of designed MHR using an improved Transformer-LSTM (Long Short-Term Memory) model for rapid design sizing. High-temperature-induced softening deformation at the anchor point reduces the actual anchor radius below the designed value. By varying key parameters such as resonator height, anchor radius and edge thickness, finite element glassblowing simulation and modal analyse were conducted to obtain the first six modal frequencies and actual anchor radius. To address regression prediction challenges with limited data, dual multi-head self-attention (MHSA) mechanisms replaced the transformer's standard Feed Forward Network, to improve hidden information capture for high-accuracy predictions of modal frequencies and anchor radius. By checking fabricating feasibility of anchor radius and allowing rapid modal characteristics evaluation without interference, ablation and comparative experiments validated the method's superiority, as an effective support of MHR design. Design optimization experiments demonstrate a prediction accuracy of 96.35%, with computational time reduced to 1/48,000 of traditional finite element methods, significantly improving design efficiency. This study offers a new paradigm for intelligent Micro-Electro-Mechanical System (MEMS) device design under complex process conditions.

</details>


### [48] [Validating Warehouse Picking Strategies Using Simulation: Case Study of a Plumbing Equipment Firm](https://arxiv.org/abs/2511.08928)
*Phattara Khumprom,Wanatchapong Kongkaew,Antoun Yaacoub,Nattakit Thanawitsatien*

Main category: eess.SY

TL;DR: 通过仿真与ABC分析优化仓库拣货，提升泰国一家管道设备分销商的拣货循环时间，比较固定、随机及组合存储策略在分区拣货中的效果。


<details>
  <summary>Details</summary>
Motivation: 在竞争激烈的物流环境中，拣货时间是关键绩效指标。存储混乱和高频物品放置不当会降低拣货效率；需要通过科学的布局与库存放置以提升整体性能。

Method: 以泰国一家领先的管道设备分销商为案例，采用仿真分析。通过ABC分析对存放进行优先级排序，将高频商品放置在靠近入口；在分区拣货策略下，比较三种存储策略（固定、随机、组合（Fixed Zone））的拣货路线与性能表现。

Result: 仿真结果揭示存储组织和高频物品放置对拣货效率的显著影响；在三种策略中不同配置对拣货时间和吞吐量有不同程度的提升潜力，指向在仓储布局与库存放置上的改进空间。

Conclusion: 该研究为仓储布局与库存放置提供有操作性的洞察，能够提升整体绩效与拣货效率。

Abstract: In today competitive business environment, efficient logistics are essential, especially in industries where timely delivery matters. This research aims to improve warehouse picking cycle time through simulation-based analysis, using a leading plumbing equipment distributor in Thailand as a case study. The study identifies inefficiencies such as disorganized storage and poor placement of high-frequency items that slow down picking. To address this, an optimized storage approach using ABC analysis is proposed, prioritizing high-demand items near the entrance. Three storage policies-Fixed, Random, and Combination (Fixed Zone)-are tested with a Zone Picking strategy through simulation to identify the most efficient picking routes. The findings provide insights for improving warehouse layout and inventory placement to enhance overall performance.

</details>


### [49] [Unifying Sequential Quadratic Programming and Linear-Parameter-Varying Algorithms for Real-Time Model Predictive Control](https://arxiv.org/abs/2511.09106)
*Kristóf Floch,Amon Lahr,Roland Tóth,Melanie N. Zeilinger*

Main category: eess.SY

TL;DR: 提出一个统一框架，将SQP与LPV-MPC结合，通过LPV-MPC的微分表述、特定的调度变量选择以及2nd FTC嵌入技术实现两者的统一，并比较收敛性；在对零阶SQP与LPV-MPC调度的统一性下提升随机与鲁棒MPC的计算效率；通过仿真对比与基于高斯过程GP的MPC在自主赛车中的实时性与实测性能进行验证。


<details>
  <summary>Details</summary>
Motivation: 解决SQP与LPV-MPC在优化与控制中的分离问题，提升鲁棒及随机MPC的计算效率与实时性

Method: 使用LPV-MPC的微分形式、选取合适的调度变量、应用2nd FTC嵌入，将SQP的零阶近似与LPV-MPC调度联系起来，并对两者的收敛性进行比较。通过仿真实验和GP-MPC在自主赛车中的实时实验来验证。

Result: 实现两者的 unified，展示收敛性对比、在某些场景下零阶SQP可借助LPV调度提高效率；仿真与实际实验表明GP-MPC基于零阶LPV-MPC在实时性与性能方面可行。

Conclusion: 提供理论与实践层面的统一框架，使零阶SQP能够受益于LPV调度，提升随机/鲁棒MPC的计算效率，并在真实系统中得到验证。

Abstract: This paper presents a unified framework that connects sequential quadratic programming (SQP) and the iterative linear-parameter-varying model predictive control (LPV-MPC) technique. Using the differential formulation of the LPV-MPC, we demonstrate how SQP and LPV-MPC can be unified through a specific choice of scheduling variable and the 2nd Fundamental Theorem of Calculus (FTC) embedding technique and compare their convergence properties. This enables the unification of the zero-order approach of SQP with the LPV-MPC scheduling technique to enhance the computational efficiency of stochastic and robust MPC problems. To demonstrate our findings, we compare the two schemes in a simulation example. Finally, we present real-time feasibility and performance of the zeroorder LPV-MPC approach by applying it to Gaussian process (GP)-based MPC for autonomous racing with real-world experiments.

</details>


### [50] [Runtime Safety and Reach-avoid Prediction of Stochastic Systems via Observation-aware Barrier Functions](https://arxiv.org/abs/2511.09192)
*Shenghua Feng,Jie An,Fanjiang Xu*

Main category: eess.SY

TL;DR: 提出了一种基于观测感知的障碍函数框架，用于对离散时间随机系统在在线观测下的安全性与reach-avoid概率进行预测，结合离线计算与在线反向迭代以动态更新边界，并给出理论保证与在基准系统上的实验验证。


<details>
  <summary>Details</summary>
Motivation: 在存在不确定性的情况下，需要在运行时对系统是否满足安全或reach-avoid规范的概率进行预测与更新；与仅依赖离线模型的方法相比，能够利用实时观测来动态 refine 概率界，提升可靠性与响应性。

Method: 提出观测感知的 barrier 函数，通过在线观测不断更新概率界，并将高效的离线计算与在线反向迭代相结合，构建一个能够在获取新观测时自适应调整的概率边界框架。

Result: 给出理论保证；在基准系统上的实验结果表明该方法在实际场景中具有显著的预测准确性和计算可行性。

Conclusion: 该框架实现了对不确定性下的安全与 reach-avoid 概率的严格且具响应性的预测，并通过观测驱动的更新机制提升了运行时的可靠性。

Abstract: Stochastic dynamical systems have emerged as fundamental models across numerous application domains, providing powerful mathematical representations for capturing uncertain system behavior. In this paper, we address the problem of runtime safety and reach-avoid probability prediction for discrete-time stochastic systems with online observations, i.e., estimating the probability that the system satisfies a given safety or reach-avoid specification. Unlike traditional approaches that rely solely on offline models, we propose a framework that incorporates real-time observations to dynamically refine probability estimates for safety and reach-avoid events. By introducing observation-aware barrier functions, our method adaptively updates probability bounds as new observations are collected, combining efficient offline computation with online backward iteration. This approach enables rigorous and responsive prediction of safety and reach-avoid probabilities under uncertainty. In addition to the theoretical guarantees, experimental results on benchmark systems demonstrate the practical effectiveness of the proposed method.

</details>


### [51] [Investigation of resonance between HVDC-MMC link and AC network](https://arxiv.org/abs/2511.09235)
*Iva Radecic,Bozidar Filipovic-Grcic,Paul Akiki,Alain Xemard,Bruno Jurisic*

Main category: eess.SY

TL;DR: Electrical resonance in HVDC networks is mainly pronounced in weak networks with long cables; damping exists and resonant harmonics dissipate quickly; strong networks show resonance linked with converter protection interactions; resonance can be activated by network changes and varies with converter parameters.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify electrical resonances in HVDC stations to ensure stable operation and mitigate oscillations as HVDC links and renewable integration expand.

Method: Numerical EMT simulations; Nyquist criterion applied to frequency responses; time-domain simulations of two real cases by introducing network changes (temporary faults, changes in network strength) to trigger resonances; use of available MMC converter model to study parameter sensitivity.

Result: Electrical resonance is most pronounced in weak networks with long cables; confirmed by Nyquist criterion. Two real-case time-domain simulations show that resonance can be activated by network changes. In strong networks with short cables, resonance occurs in conjunction with interactions between the network and the converter's protection system. The resonant harmonic amplitudes dissipate quickly, indicating sufficient damping in the studied configurations. The network is sensitive to converter parameter changes modeled with the MMC model.

Conclusion: HVDC networks require adequate damping and careful consideration of network strength and converter parameters to avoid persistent resonances. Protection-system interactions in strong networks can influence resonance. The study confirms damping behavior and suggests parameter-sensitivity analyses as a tool for robust HVDC design.

Abstract: HVDC networks offer several advantages over traditional HVAC systems, particularly for long-distance power transmission and integration of renewable energy sources, such as reduced losses and enhanced stability and control, but also increase the risk of oscillations. This study investigates electrical resonant phenomena associated with HVDC stations through numerical EMT simulations. The findings indicate that electrical resonance is primarily pronounced in weak networks with long cables, as confirmed by the Nyquist criterion applied to frequency responses. Two real cases were successfully simulated in the time domain by introducing network changes, such as temporary faults and alterations in network's power strength, to activate the identified resonances. Notably, in a strong network with short cables, electrical resonance occurred alongside interactions between the network and the converter's protection system. The analysis of voltage waveforms revealed that the amplitude of the induced resonant harmonic dissipates quickly, indicating sufficient damping in the network configuration. Furthermore, the study confirmed the network's sensitivity to changes in converter parameters modeled using available MMC model.

</details>


### [52] [Security Index from Input/Output Data: Theory and Computation](https://arxiv.org/abs/2511.09524)
*Takumi Shinohara,Karl H. Johansson,Henrik Sandberg*

Main category: eess.SY

TL;DR: 提出一种在未知系统模型下仅用输入/输出数据即可计算的数据驱动安全指标；给出在满足条件时该数据驱动指标等价于模型驱动指标的充分条件，并给出计算算法及其多项式上界，实验于车队编队场景以展示方法的有效性与局限性。


<details>
  <summary>Details</summary>
Motivation: 安全指标用于量化组件被妥协而产生的最低需被攻破的组件数，从而评估系统的安全风险并设计对策。在系统模型未知时，如何从数据中直接推断每个组件的风险，是一个实际且具挑战性的问题。

Method: 提出数据驱动的安全指标定义，给出在模型未知的前提下通过输入/输出数据计算该指标的算法；给出一个充分条件，使数据驱动指标与模型驱动指标等价，从而可仅依赖数据识别出每个分量的确切风险；给出该指标的计算算法，并证明该问题NP-hard，同时给出一个多项式时间可计算的上界；通过车队编队的数值实验评估方法的有效性与局限性。

Result: 在满足充分条件时，数据驱动安全指标与模型驱动安全指标一致，可以仅用数据识别出各组件的确切风险；给出计算数据驱动指标的算法；该问题在一般情况下是NP-hard，但存在一个多项式时间可计算的上界；数值实验表明该方法在车队编队场景下具有一定效果但也存在局限性。

Conclusion: 数据驱动安全索引在模型未知时具有可行性，且在特定条件下可获得与模型驱动相同的风险评估；尽管存在计算困难，但通过上界和数值例子可以提供实用的近似与洞察，为在数据驱动的安全分析提供一种可操作框架。

Abstract: The concept of a security index quantifies the minimum number of components that must be compromised to carry out an undetectable attack. This metric enables system operators to quantify each component's security risk and implement countermeasures. In this paper, we introduce a data-driven security index that can be computed solely from input/output data when the system model is unknown. We show a sufficient condition under which the data-driven security index coincides with the model-based security index, which implies that the exact risk level of each component can be identified solely from the data. We provide an algorithm for computing the data-driven security index. Although computing this index is NP-hard, we derive a polynomial-time computable upper bound. Numerical examples on vehicle platooning illustrate the efficacy and limitations of the proposed index and algorithms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [A Lightweight CNN-Attention-BiLSTM Architecture for Multi-Class Arrhythmia Classification on Standard and Wearable ECGs](https://arxiv.org/abs/2511.08650)
*Vamsikrishna Thota,Hardik Prajapati,Yuvraj Joshi,Shubhangi Rathi*

Main category: cs.LG

TL;DR: A lightweight 1D CNN–attention–BiLSTM model for arrhythmia classification from ECGs, achieving high accuracy and F1-scores on CPSC 2018 with about 0.95M parameters, outperforming baselines and suitable for real-time wearable deployment.


<details>
  <summary>Details</summary>
Motivation: Early and accurate detection of cardiac arrhythmias is crucial for timely diagnosis and intervention, especially in wearable health monitoring. The dataset exhibits class imbalance, necessitating robust learning with limited resources.

Method: A compact architecture combining 1D convolutional feature extraction, attention mechanisms to highlight informative temporal regions, and Bidirectional LSTM to capture long-range dependencies, applied to both 12-lead and single-lead ECGs. Class weightings are used in the loss to address class imbalance. Evaluated on the CPSC 2018 dataset with a model size of 0.945 million parameters.

Result: The proposed model achieves superior accuracy and F1-scores relative to baseline models on CPSC 2018, despite its small size, indicating effective learning and handling of class imbalance.

Conclusion: The approach demonstrates that highly efficient models can deliver competitive performance for arrhythmia detection and is well-suited for real-time deployment in wearable health monitoring systems.

Abstract: Early and accurate detection of cardiac arrhythmias is vital for timely diagnosis and intervention. We propose a lightweight deep learning model combining 1D Convolutional Neural Networks (CNN), attention mechanisms, and Bidirectional Long Short-Term Memory (BiLSTM) for classifying arrhythmias from both 12-lead and single-lead ECGs. Evaluated on the CPSC 2018 dataset, the model addresses class imbalance using a class-weighted loss and demonstrates superior accuracy and F1- scores over baseline models. With only 0.945 million parameters, our model is well-suited for real-time deployment in wearable health monitoring systems.

</details>


### [54] [Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion](https://arxiv.org/abs/2511.08653)
*Kaleem Ullah Qasim,Jiashu Zhang*

Main category: cs.LG

TL;DR: 提出 CGAR 训练方法，通过对架构深度进行 curriculum 学习（Progressive Depth Curriculum）与分层监督权重（Hierarchical Supervision Weighting），在递归推理模型训练中实现显著的训练加速，同时保持近乎相同的准确率，并提升推理阶段的效率。


<details>
  <summary>Details</summary>
Motivation: 递归推理模型在复杂推理任务中通过迭代细化获得强大性能，但训练成本高，需大规模算力，限制了广泛应用和研究。现有工作在单数据集上的训练成本高昂，例如约36 GPU 小时/数据集。

Method: 提出两大耦合组件：1) Progressive Depth Curriculum：在训练过程中动态将递归深度从浅到深进行调整，降低早期过拟合与计算开销；2) Hierarchical Supervision Weighting：对监督步骤的损失权重进行指数衰减，使损失加权与梯度幅值衰减一致。此外在 Sudoku-Extreme（423,168 道测试题）上评估，展示在训练时间、成本和准确率方面的显著改进。

Result: CGAR 实现了 1.71x 的训练加速（从 10.93 小时降至 6.38 小时，成本下降约 42%），准确率仅下降 0.63%（从 86.65% 降至 86.02%）。系统性消融实验显示仅使用 Progressive Depth Curriculum 即可达到 2.26x 的加速、且准确率为 85.47%，呈现出在训练效率和解题质量上的 Pareto 改进。CGAR 模型在推理阶段具备更高的推理效率，达到 100% 停止准确性，推理步数减少约 11%。此外提供代码与模型的公开链接。

Conclusion: 基于对架构深度的有原则的 curriculum，可以在 modest 硬件条件下高效训练递归推理模型，CGAR 通过在深度与监督权重上的组合设计实现了显著的训练加速与推理效率提升，同时对解题质量几乎无损。

Abstract: Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku

</details>


### [55] [TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models](https://arxiv.org/abs/2511.08667)
*Léo Grinsztajn,Klemens Flöge,Oscar Key,Felix Birkel,Philipp Jund,Brendan Roof,Benjamin Jäger,Dominik Safaric,Simone Alessi,Adrian Hayler,Mihir Manium,Rosen Yu,Felix Jablonski,Shi Bin Hoo,Anurag Garg,Jake Robertson,Magnus Bühler,Vladyslav Moroshan,Lennart Purucker,Clara Cornu,Lilly Charlotte Wehrhahn,Alessandro Bonetto,Bernhard Schölkopf,Sauraj Gambhir,Noah Hollmann,Frank Hutter*

Main category: cs.LG

TL;DR: TabPFN-2.5: TabPFN系列升级版，支持最多5万数据点和2000个特征，TabArena性能领先；可通过 distillation 转换为高效的 MLP 或树 ensemble，实现低延迟部署，同时保持接近原模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 将 TabPFN 的优势扩展到更大规模的表格数据集，并提升生产环境的部署效率与延迟表现，同时提升在行业基准上的竞争力。

Method: 提出 TabPFN-2.5 架构，扩大可处理的数据量与特征数，评估在 TabArena 上的表现（数据量最高到 100k，特征可达 2k），与经过调参的树模型和 AutoGluon 1.4 进行对比；引入蒸馏引擎将 TabPFN-2.5 转换为紧凑的 MLP 或树集成模型以降低延迟并实现即插即用部署。

Result: 在 TabArena 等基准上，TabPFN-2.5 表现领先；默认情况下对 XGBoost 在小/中等数据集上实现 100% 胜率，在较大数据集（最多 100k 点、2k 特征）上达到 87% 的胜率（回归为 85%），并且对比 AutoGluon 1.4 能达到相同的准确性；蒸馏后模型在保持大部分准确性的同时显著降低延迟，便于生产部署。

Conclusion: 该版本显著增强 TabPFN 生态系统的生产力和适用性，能够更快地将现有方法和应用落地到实际场景中。

Abstract: The first tabular foundation model, TabPFN, and its successor TabPFNv2 have impacted tabular AI substantially, with dozens of methods building on it and hundreds of applications across different use cases. This report introduces TabPFN-2.5, the next generation of our tabular foundation model, built for datasets with up to 50,000 data points and 2,000 features, a 20x increase in data cells compared to TabPFNv2. TabPFN-2.5 is now the leading method for the industry standard benchmark TabArena (which contains datasets with up to 100,000 training data points), substantially outperforming tuned tree-based models and matching the accuracy of AutoGluon 1.4, a complex four-hour tuned ensemble that even includes the previous TabPFNv2. Remarkably, default TabPFN-2.5 has a 100% win rate against default XGBoost on small to medium-sized classification datasets (<=10,000 data points, 500 features) and a 87% win rate on larger datasets up to 100K samples and 2K features (85% for regression). For production use cases, we introduce a new distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, preserving most of its accuracy while delivering orders-of-magnitude lower latency and plug-and-play deployment. This new release will immediately strengthen the performance of the many applications and methods already built on the TabPFN ecosystem.

</details>


### [56] [Gromov-Wasserstein Graph Coarsening](https://arxiv.org/abs/2511.08733)
*Carlos A. Taveras,Santiago Segarra,César A. Uribe*

Main category: cs.LG

TL;DR: Two greedy coarsening algorithms within Gromov-Wasserstein geometry (GPC and KGPC) for graph coarsening; provide optimality conditions and empirical validation on six datasets.


<details>
  <summary>Details</summary>
Motivation: To address graph coarsening under GW framework by a distortion-based merging criterion, enabling scalable clustering and downstream tasks.

Method: Greedy Pair Coarsening merges locally minimal distortion pairs; KGPC clusters nodes via pairwise distortion metrics and merges clusters; theoretical conditions for optimality.

Result: Empirical evaluation on six large-scale datasets shows outperforming existing methods across parameter regimes; improved clustering downstream task.

Conclusion: Proposes effective distortion-based coarsening schemes within GW geometry with provable optimality conditions and practical scalability.

Abstract: We study the problem of graph coarsening within the Gromov-Wasserstein geometry. Specifically, we propose two algorithms that leverage a novel representation of the distortion induced by merging pairs of nodes. The first method, termed Greedy Pair Coarsening (GPC), iteratively merges pairs of nodes that locally minimize a measure of distortion until the desired size is achieved. The second method, termed $k$-means Greedy Pair Coarsening (KGPC), leverages clustering based on pairwise distortion metrics to directly merge clusters of nodes. We provide conditions guaranteeing optimal coarsening for our methods and validate their performance on six large-scale datasets and a downstream clustering task. Results show that the proposed methods outperform existing approaches on a wide range of parameters and scenarios.

</details>


### [57] [A Generalized Bias-Variance Decomposition for Bregman Divergences](https://arxiv.org/abs/2511.08789)
*David Pfau*

Main category: cs.LG

TL;DR: 在平方误差之外，给出基于 Bregman 发散的偏差-方差分解及其在指数族最大似然中的意义，并提供清晰的推导。


<details>
  <summary>Details</summary>
Motivation: 偏差-方差分解是统计与机器学习的核心结果，通常针对平方误差；需要一个通用、可推广的分解来覆盖更广的损失，如 Bregman 发散，特别是在指数族模型的极大似然估计中。

Method: 给出一个独立、通俗易懂的推导，基于 Bregman 发散的性质以及对预测误差的期望分解；明确地列出偏差、方差对总误差的贡献；并将其与平方误差情形对比，说明等价性与差异；提供与先前文献的关系。

Result: 得到针对任意 Bregman 发散的偏差-方差分解公式，便于在指数族模型的最大似然场景应用；提供了可教学的推导结构。

Conclusion: 这篇简注澄清了相关推导并补充了文献引用，便于教学与应用，尽管结果在先前已有，但此为独立、清晰的推导和综述。

Abstract: The bias-variance decomposition is a central result in statistics and machine learning, but is typically presented only for the squared error. We present a generalization of the bias-variance decomposition where the prediction error is a Bregman divergence, which is relevant to maximum likelihood estimation with exponential families. While the result is already known, there was not previously a clear, standalone derivation, so we provide one for pedagogical purposes. A version of this note previously appeared on the author's personal website without context. Here we provide additional discussion and references to the relevant prior literature.

</details>


### [58] [BayesQ: Uncertainty-Guided Bayesian Quantization](https://arxiv.org/abs/2511.08821)
*Ismail Lamaakal,Chaymae Yahyati,Yassine Maleh,Khalid El Makkaoui,Ibrahim Ouahbi*

Main category: cs.LG

TL;DR: BayesQ: an uncertainty-guided post-training quantization framework that optimizes quantization under the posterior expected loss using a lightweight Gaussian weight posterior, posterior-whitening, codebook design, and mixed-precision allocation via a greedy knapsack.


<details>
  <summary>Details</summary>
Motivation: To improve post-training quantization by explicitly accounting for model uncertainty, enabling principled distortion minimization and bit-allocation under a global budget.

Method: Fit a lightweight Gaussian posterior over weights (diagonal Laplace by default; optional K-FAC/low-rank); whiten by the posterior covariance; design codebooks to minimize posterior-expected distortion; allocate mixed precision via a greedy knapsack that maximizes marginal expected-loss reduction per bit under a global budget; closed-form tables for scalar quantizers; Monte Carlo on a small calibration set for task-aware proxies; optional calibration-only distillation to align with the posterior predictive teacher.

Result: At matched average bits/weight of 3.0/3.5/4.0, BayesQ improves over strong PTQ baselines (e.g., GPTQ) on ResNet-50 (ImageNet) and BERT-base (GLUE): about +1.5/ +0.7/ +0.3 top-1 points on RN50 and +1.1/ +0.4/ +0.2 GLUE points on BERT, with one-time preprocessing comparable to a GPTQ pass.

Conclusion: BayesQ reframes low-bit quantization as uncertainty-aware risk minimization in a practical, post-training pipeline.

Abstract: We present BayesQ, an uncertainty-guided post-training quantization framework that is the first to optimize quantization under the posterior expected loss. BayesQ fits a lightweight Gaussian posterior over weights (diagonal Laplace by default; optional K-FAC/low-rank), whitens by the posterior covariance, designs codebooks to minimize posterior-expected distortion, and allocates mixed precision via a greedy knapsack that maximizes marginal expected-loss reduction per bit under a global budget. For scalar quantizers, posterior-expected MSE yields closed-form tables; task-aware proxies are handled by short Monte Carlo on a small calibration set. An optional calibration-only distillation aligns the quantized model with the posterior predictive teacher. At matched average bits/weight of 3.0/3.5/4.0, BayesQ improves over strong PTQ baselines on ResNet-50 (ImageNet) and BERT-base (GLUE) e.g., vs. GPTQ by $+1.5/+0.7/+0.3$ top-1 percentage points on RN50 and $+1.1/+0.4/+0.2$ GLUE points on BERT, while requiring one-time preprocessing comparable to a GPTQ pass. BayesQ reframes low-bit quantization as uncertainty-aware risk minimization in a practical, post-training pipeline.

</details>


### [59] [Physics-Informed Machine Learning for Characterizing System Stability](https://arxiv.org/abs/2511.08831)
*Tomoki Koike,Elizabeth Qian*

Main category: cs.LG

TL;DR: 提出LyapInf，一种基于轨迹数据的Lyapunov函数推断方法，在不知道系统方程的情况下，通过最小化Zubov方程残差来拟合二次形式的Lyapunov算符，从而给出近似最大椭球形的稳定区域估计。


<details>
  <summary>Details</summary>
Motivation: 在复杂动力系统中，稳定域往往难以先验确定且难以计算；传统的Lyapunov方法依赖显式的系统方程，限制了对黑箱系统的适用性。

Method: 将Lyapunov函数设为未知的二次形式，通过对系统轨迹数据进行拟合，最小化Zubov方程的一阶偏导数残差，以不需要系统方程的方式推断Lyapunov函数；由推断出的二次Lyapunov函数得到一个椭圆形的稳定区域近似。

Result: 在基准示例上验证，所推断的Lyapunov函数能够产生接近最大稳定区域的椭圆估计，且不需要显式的系统方程。

Conclusion: 该物理信息学习方法为无方程信息的稳定域估计提供了有效途径，利用Zubov方程与二次Lyapunov假设实现对复杂系统的近似稳定域推断。

Abstract: In the design and operation of complex dynamical systems, it is essential to ensure that all state trajectories of the dynamical system converge to a desired equilibrium within a guaranteed stability region. Yet, for many practical systems -- especially in aerospace -- this region cannot be determined a priori and is often challenging to compute. One of the most common methods for computing the stability region is to identify a Lyapunov function. A Lyapunov function is a positive function whose time derivative along system trajectories is non-positive, which provides a sufficient condition for stability and characterizes an estimated stability region. However, existing methods of characterizing a stability region via a Lyapunov function often rely on explicit knowledge of the system governing equations. In this work, we present a new physics-informed machine learning method of characterizing an estimated stability region by inferring a Lyapunov function from system trajectory data that treats the dynamical system as a black box and does not require explicit knowledge of the system governing equations. In our presented Lyapunov function Inference method (LyapInf), we propose a quadratic form for the unknown Lyapunov function and fit the unknown quadratic operator to system trajectory data by minimizing the average residual of the Zubov equation, a first-order partial differential equation whose solution yields a Lyapunov function. The inferred quadratic Lyapunov function can then characterize an ellipsoidal estimate of the stability region. Numerical results on benchmark examples demonstrate that our physics-informed stability analysis method successfully characterizes a near-maximal ellipsoid of the system stability region associated with the inferred Lyapunov function without requiring knowledge of the system governing equations.

</details>


### [60] [TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations](https://arxiv.org/abs/2511.08832)
*Nikunj Gupta,Ludwika Twardecka,James Zachary Hare,Jesse Milzman,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we propose capturing and utilizing \textit{Temporal Information through Graph-based Embeddings and Representations} or \textbf{TIGER} to enhance multi-agent reinforcement learning (MARL). We explicitly model how inter-agent coordination structures evolve over time. While most MARL approaches rely on static or per-step relational graphs, they overlook the temporal evolution of interactions that naturally arise as agents adapt, move, or reorganize cooperation strategies. Capturing such evolving dependencies is key to achieving robust and adaptive coordination. To this end, TIGER constructs dynamic temporal graphs of MARL agents, connecting their current and historical interactions. It then employs a temporal attention-based encoder to aggregate information across these structural and temporal neighborhoods, yielding time-aware agent embeddings that guide cooperative policy learning. Through extensive experiments on two coordination-intensive benchmarks, we show that TIGER consistently outperforms diverse value-decomposition and graph-based MARL baselines in task performance and sample efficiency. Furthermore, we conduct comprehensive ablation studies to isolate the impact of key design parameters in TIGER, revealing how structural and temporal factors can jointly shape effective policy learning in MARL. All codes can be found here: https://github.com/Nikunj-Gupta/tiger-marl.

</details>


### [61] [Enhancing DPSGD via Per-Sample Momentum and Low-Pass Filtering](https://arxiv.org/abs/2511.08841)
*Xincheng Xu,Thilina Ranbaduge,Qing Wang,Thierry Rakotoarivelo,David Smith*

Main category: cs.LG

TL;DR: DP-PMLF: 结合逐样本动量与低通滤波的 DPSGD 改进，旨在同时减小 DP 噪声与裁剪偏差，提升隐私-效用平衡。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私随机梯度下降中，噪声与裁剪带来的偏差共同降低模型精度，现有方法往往只优化其中之一。

Method: 使用逐样本动量平滑梯度后进行裁剪，并在后续使用一个不额外隐私预算的低通滤波器抑制高频 DP 噪声，从而同时减小采样方差与裁剪偏差。

Result: 理论上给出在严格 DP 下的收敛性提升；实验上相较多种 DPSGD 变体，在隐私-效用权衡方面有显著提升。

Conclusion: DP-PMLF 提供一种同时缓解噪声和裁剪偏差的新方法，提升在具有隐私保护要求的深度学习场景中的性能。

Abstract: Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to train deep neural networks with formal privacy guarantees. However, the addition of differential privacy (DP) often degrades model accuracy by introducing both noise and bias. Existing techniques typically address only one of these issues, as reducing DP noise can exacerbate clipping bias and vice-versa. In this paper, we propose a novel method, \emph{DP-PMLF}, which integrates per-sample momentum with a low-pass filtering strategy to simultaneously mitigate DP noise and clipping bias. Our approach uses per-sample momentum to smooth gradient estimates prior to clipping, thereby reducing sampling variance. It further employs a post-processing low-pass filter to attenuate high-frequency DP noise without consuming additional privacy budget. We provide a theoretical analysis demonstrating an improved convergence rate under rigorous DP guarantees, and our empirical evaluations reveal that DP-PMLF significantly enhances the privacy-utility trade-off compared to several state-of-the-art DPSGD variants.

</details>


### [62] [On topological descriptors for graph products](https://arxiv.org/abs/2511.08846)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: 研究图的积在滤波下的拓扑描述符表达力：EC在颜色基滤波下的表达力得到完整表征；PH在图产品上信息量大于单图，给出计算算法并用实验验证。


<details>
  <summary>Details</summary>
Motivation: 在关系数据中，拓扑描述符（EC和PH）用于捕捉多尺度结构信息。本文考察（盒子）图积在不同颜色基滤波下的表现，探索积对描述能力的影响，以及与直接对单图描述的比较。

Method: 系统分析一般颜色基滤波下EC的表达力，证明其完全刻画；证明（虚拟）图产品的PH描述符包含比单图更丰富的信息，而EC则不；提出针对图积在顶点-边级过滤下的PH图的计算算法；通过实验评估运行时间、表达力和图分类性能。

Result: PH描述符在图产品中提供比对单图更丰富的信息，EC没有这种提升；给出实现PH图的算法并在多组实验中验证效率与分类性能提升。

Conclusion: 通过引入图积滤波，拓展图持久描述符的表达能力，为基于滤波的图分析提供更强工具，并提供开源实现。

Abstract: Topological descriptors have been increasingly utilized for capturing multiscale structural information in relational data. In this work, we consider various filtrations on the (box) product of graphs and the effect on their outputs on the topological descriptors - the Euler characteristic (EC) and persistent homology (PH). In particular, we establish a complete characterization of the expressive power of EC on general color-based filtrations. We also show that the PH descriptors of (virtual) graph products contain strictly more information than the computation on individual graphs, whereas EC does not. Additionally, we provide algorithms to compute the PH diagrams of the product of vertex- and edge-level filtrations on the graph product. We also substantiate our theoretical analysis with empirical investigations on runtime analysis, expressivity, and graph classification performance. Overall, this work paves way for powerful graph persistent descriptors via product filtrations. Code is available at https://github.com/Aalto-QuML/tda_graph_product.

</details>


### [63] [Rethinking Graph Super-resolution: Dual Frameworks for Topological Fidelity](https://arxiv.org/abs/2511.08853)
*Pragya Singh,Islem Rekik*

Main category: cs.LG

TL;DR: 提出两种GNN无关框架来实现图超分辨率：Bi-SR通过LR-HR节点的二分图实现结构感知的节点超分；DEFEND通过将HR边映射到对偶图的节点实现边推断；在真实脑连接组和12个模拟数据集上实现优异表现并提供用于综合评测的数据集。


<details>
  <summary>Details</summary>
Motivation: 在资源受限场景（如医疗领域）中，图超分辨率尚未被充分研究且具有潜在价值；现有GNN方法面临两大局限：矩阵化的节点超分忽略结构且缺乏置换不变性；依赖节点表征推断边权限制可扩展性与表达力。需要结构感知的节点超分及边推断的新范式。

Method: Bi-SR 构建 LR 与 HR 节点之间的二分图以实现结构感知的节点超分并保持拓扑与置换不变性；DEFEND 将 HR 边映射到对偶图的节点，允许通过标准基于节点的GNN进行边推断；评估在真实脑连接组和12个新模拟数据集上。

Result: 在真实脑连接组数据集上实现七个拓扑指标上的SOTA；提出12个模拟数据集以促进方法的泛化性评测；两种框架显著提升图超分辨率性能并提供基准数据集。

Conclusion: 提出的两种GNN无关框架解决了结构感知和边推断的核心挑战，具有良好泛化性，且为医学生物等资源受限场景的图超分辨率研究提供了可用数据资源与基准评测。

Abstract: Graph super-resolution, the task of inferring high-resolution (HR) graphs from low-resolution (LR) counterparts, is an underexplored yet crucial research direction that circumvents the need for costly data acquisition. This makes it especially desirable for resource-constrained fields such as the medical domain. While recent GNN-based approaches show promise, they suffer from two key limitations: (1) matrix-based node super-resolution that disregards graph structure and lacks permutation invariance; and (2) reliance on node representations to infer edge weights, which limits scalability and expressivity. In this work, we propose two GNN-agnostic frameworks to address these issues. First, Bi-SR introduces a bipartite graph connecting LR and HR nodes to enable structure-aware node super-resolution that preserves topology and permutation invariance. Second, DEFEND learns edge representations by mapping HR edges to nodes of a dual graph, allowing edge inference via standard node-based GNNs. We evaluate both frameworks on a real-world brain connectome dataset, where they achieve state-of-the-art performance across seven topological measures. To support generalization, we introduce twelve new simulated datasets that capture diverse topologies and LR-HR relationships. These enable comprehensive benchmarking of graph super-resolution methods.

</details>


### [64] [Decomposition of Small Transformer Models](https://arxiv.org/abs/2511.08854)
*Casper L. Christensen,Logan Riggs*

Main category: cs.LG

TL;DR: 将随机参数分解（SPD）扩展到 Transformer 模型，提出面向序列数据的更新因果重要性函数与新损失函数。将 SPD 应用于 toy induction-head 模型与 GPT-2-small，分别定位到可解释概念对应的参数子组件，初步表明 SPD 可用于现代模型的参数空间机制分析。


<details>
  <summary>Details</summary>
Motivation: 弥合“玩具模型”与“真实模型”之间的差距，推进对 Transformer 等现代模型在参数空间层面的可解释性研究；通过对 SPD 的扩展，发现参数级别的因果结构与可解释子组件。

Method: 将随机参数分解（SPD）扩展至 Transformer，提出适用于序列数据的更新因果重要性函数和新的损失函数；在 toy induction-head 上验证可还原两步电路，在 GPT-2-small 上定位与“高尔夫球”、“篮球”等可解释概念相关的子组件。

Result: 在 toy induction-head 模型中，SPD 能成功分解并恢复预期的 2 步电路；在 GPT-2-small 中，SPD 能定位出与可解释概念相关的子组件。

Conclusion: 这是将 SPD 推向现代大模型的第一步，表明该方法可用于揭示参数空间中的可解释机制。

Abstract: Recent work in mechanistic interpretability has shown that decomposing models in parameter space may yield clean handles for analysis and intervention. Previous methods have demonstrated successful applications on a wide range of toy models, but the gap to "real models" has not yet been bridged. In this work, we extend Stochastic Parameter Decomposition (SPD) to Transformer models, proposing an updated causal importance function suited for sequential data and a new loss function. We demonstrate that SPD can successfully decompose a toy induction-head model and recover the expected 2-step circuit. We also show that applying SPD to GPT-2-small can successfully locate subcomponents corresponding to interpretable concepts like "golf" and "basketball". These results take the first step in the direction of extending SPD to modern models, and show that we can use the method to surface interpretable parameter-space mechanisms.

</details>


### [65] [ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model](https://arxiv.org/abs/2511.08856)
*Krishu K Thapa,Supriya Savalkar,Bhupinderjeet Singh,Trong Nghia Hoang,Kirti Rajagopalan,Ananth Kalyanaraman*

Main category: cs.LG

TL;DR: ForeSWE: a probabilistic spatio-temporal forecasting model for Snow Water Equivalent that fuses attention-based deep learning with Gaussian processes to produce both accurate forecasts and principled uncertainty estimates, validated on 512 SNOTEL stations across the Western US; showing improved accuracy and well-calibrated prediction intervals, with deployment potential for water management.


<details>
  <summary>Details</summary>
Motivation: Forecasting SWE is difficult due to strong spatio-temporal variability influenced by topography and environmental conditions; classical methods often underutilize spatial/temporal correlations and lack uncertainty quantification, limiting decision support.

Method: ForeSWE combines an attention-based deep learning module to capture spatiotemporal patterns and interactions with a Gaussian process component to quantify predictive uncertainty, evaluated on a network of SNOTEL stations.

Result: The model achieves significant improvements in forecasting accuracy and prediction interval quality compared to existing approaches; uncertainty estimates are shown to be more reliable, leveraging the probabilistic framework.

Conclusion: ForeSWE provides a practically deployable platform for water management communities, delivering accurate SWE forecasts with quantified uncertainty to support decision making and feedback.

Abstract: Various complex water management decisions are made in snow-dominant watersheds with the knowledge of Snow-Water Equivalent (SWE) -- a key measure widely used to estimate the water content of a snowpack. However, forecasting SWE is challenging because SWE is influenced by various factors including topography and an array of environmental conditions, and has therefore been observed to be spatio-temporally variable. Classical approaches to SWE forecasting have not adequately utilized these spatial/temporal correlations, nor do they provide uncertainty estimates -- which can be of significant value to the decision maker. In this paper, we present ForeSWE, a new probabilistic spatio-temporal forecasting model that integrates deep learning and classical probabilistic techniques. The resulting model features a combination of an attention mechanism to integrate spatiotemporal features and interactions, alongside a Gaussian process module that provides principled quantification of prediction uncertainty. We evaluate the model on data from 512 Snow Telemetry (SNOTEL) stations in the Western US. The results show significant improvements in both forecasting accuracy and prediction interval compared to existing approaches. The results also serve to highlight the efficacy in uncertainty estimates between different approaches. Collectively, these findings have provided a platform for deployment and feedback by the water management community.

</details>


### [66] [EEG-X: Device-Agnostic and Noise-Robust Foundation Model for EEG](https://arxiv.org/abs/2511.08861)
*Navid Mohammadi Foumani,Soheila Ghane,Nam Nguyen,Mahsa Salehi,Geoffrey I. Webb,Geoffrey Mackellar*

Main category: cs.LG

TL;DR: 提出 EEG-X，一种设备无关且鲁棒于噪声的 EEG 表征预训练模型，通过位置化通道嵌入、噪声感知掩蔽/重构、以及字典启发的卷积变换 DiCT 层实现跨域泛化与去噪鲁棒性，并在多设备数据集上优于 SOTA。


<details>
  <summary>Details</summary>
Motivation: EEG 受到不同设备/配置带来的数据变异以及低信噪比的挑战，现有模型难以在跨域任务中泛化。

Method: 提出 EEG-X：包含位置基的通道嵌入以编码空间信息并支持变通道/变长度；噪声感知掩蔽和重构，在原始和潜在空间对噪声进行鲁棒处理；以去噪信号进行重构以聚焦神经活动；引入 DiCT 层，将信号投影到结构化特征空间后再计算重构损失（MSE），提高稳定性和对频率/形状信息的捕捉能力。

Result: 在来自多设备的数据集上，EEG-X 在多项下游任务和跨域场景中超越了现有方法，且对不同电极布局的训练-测试集具较强的跨域鲁棒性。

Conclusion: EEG-X 的设计有效提升跨域泛化与降噪鲁棒性，且相关代码与模型可在 GitHub 获取，促进 EEG 基础模态的表示学习与跨域应用。

Abstract: Foundation models for EEG analysis are still in their infancy, limited by two key challenges: (1) variability across datasets caused by differences in recording devices and configurations, and (2) the low signal-to-noise ratio (SNR) of EEG, where brain signals are often buried under artifacts and non-brain sources. To address these challenges, we present EEG-X, a device-agnostic and noise-robust foundation model for EEG representation learning. EEG-X introduces a novel location-based channel embedding that encodes spatial information and improves generalization across domains and tasks by allowing the model to handle varying channel numbers, combinations, and recording lengths. To enhance robustness against noise, EEG-X employs a noise-aware masking and reconstruction strategy in both raw and latent spaces. Unlike previous models that mask and reconstruct raw noisy EEG signals, EEG-X is trained to reconstruct denoised signals obtained through an artifact removal process, ensuring that the learned representations focus on neural activity rather than noise. To further enhance reconstruction-based pretraining, EEG-X introduces a dictionary-inspired convolutional transformation (DiCT) layer that projects signals into a structured feature space before computing reconstruction (MSE) loss, reducing noise sensitivity and capturing frequency- and shape-aware similarities. Experiments on datasets collected from diverse devices show that EEG-X outperforms state-of-the-art methods across multiple downstream EEG tasks and excels in cross-domain settings where pre-trained and downstream datasets differ in electrode layouts. The models and code are available at: https://github.com/Emotiv/EEG-X

</details>


### [67] [Transformer-Based Sleep Stage Classification Enhanced by Clinical Information](https://arxiv.org/abs/2511.08864)
*Woosuk Chung,Seokwoo Hong,Wonhyeok Lee,Sangyoon Bae*

Main category: cs.LG

TL;DR: 提出一个两阶段模型（Transformer per-epoch编码器 + 1D-CNN聚合器）并引入上下文信息（人口统计元数据和逐时刻事件标注）来提升睡眠分期的准确性，在SHHS数据集上显著优于仅用原始PSG信号的基线。


<details>
  <summary>Details</summary>
Motivation: 人工睡眠分期耗时且存在评定者间差异；现有深度模型多依赖原始PSG信号，忽略专家使用的上下文线索；引入临床元数据和事件标注可能提升性能和可解释性。

Method: 两阶段架构：一个基于Transformer的逐时段编码器对每个睡眠分时期进行表征，后接1D-CNN聚合器；对比PSG-only基线以及包含上下文特征的模型；在Sleep Heart Health Study (SHHS) 队列（n=8,357）上评估；比较特征融合与多任务学习。

Result: 相对于PSG-only基线（macro-F1 0.7745，micro-F1 0.8774），最终模型在引入上下文后达到 macro-F1 0.8031、micro-F1 0.9051；事件注释贡献最大；特征融合优于预测相同辅助标签的多任务方法；无需改动PSG接纳、无需额外传感器；提升了性能和可解释性。

Conclusion: 将上下文信息与学习表示相结合可实现更实用、可扩展的上下文感知睡眠分期系统，具备现实部署潜力。

Abstract: Manual sleep staging from polysomnography (PSG) is labor-intensive and prone to inter-scorer variability. While recent deep learning models have advanced automated staging, most rely solely on raw PSG signals and neglect contextual cues used by human experts. We propose a two-stage architecture that combines a Transformer-based per-epoch encoder with a 1D CNN aggregator, and systematically investigates the effect of incorporating explicit context: subject-level clinical metadata (age, sex, BMI) and per-epoch expert event annotations (apneas, desaturations, arousals, periodic breathing). Using the Sleep Heart Health Study (SHHS) cohort (n=8,357), we demonstrate that contextual fusion substantially improves staging accuracy. Compared to a PSG-only baseline (macro-F1 0.7745, micro-F1 0.8774), our final model achieves macro-F1 0.8031 and micro-F1 0.9051, with event annotations contributing the largest gains. Notably, feature fusion outperforms multi-task alternatives that predict the same auxiliary labels. These results highlight that augmenting learned representations with clinically meaningful features enhances both performance and interpretability, without modifying the PSG montage or requiring additional sensors. Our findings support a practical and scalable path toward context-aware, expert-aligned sleep staging systems.

</details>


### [68] [Covariance Scattering Transforms](https://arxiv.org/abs/2511.08878)
*Andrea Cavallo,Ayushman Raghuvanshi,Sundeep Prabhakar Chepuri,Elvin Isufi*

Main category: cs.LG

TL;DR: Covariance Scattering Transform (CST) is an untrained deep network that uses covariance-spectrum localized filters (covariance wavelets) to produce hierarchical data representations, offering stability in low-sample regimes and competitive performance without training, blending PCA's unsupervised nature with VNNs' spectral expressiveness.


<details>
  <summary>Details</summary>
Motivation: Address PCA's two main drawbacks—loss of information in low-variance directions and instability with close covariance eigenvalues—while avoiding the need for supervised training inherent to covariance neural networks (VNNs). The goal is to combine the benefits of PCA and VNNs in an unsupervised, training-free framework.

Method: Propose Covariance Scattering Transforms (CSTs), a deep untrained network that applies filters localized in the covariance spectrum to the input data. Define filters as covariance wavelets capturing detailed spectral patterns. Improve CSTs with a pruning mechanism for computational and memory efficiency. Prove that finite-sample covariance estimation errors are less sensitive to close eigenvalues than PCA. Validate on neuroimaging age-prediction tasks across four datasets.

Result: CSTs yield stable representations in low-data settings similar to VNNs but without training and achieve comparable or better predictive performance than more complex learning models; pruning enhances efficiency.

Conclusion: CSTs offer a practical, training-free framework that leverages covariance-spectrum filtering to achieve stable, expressive representations, bridging PCA’s unsupervised approach and VNNs’ spectral expressiveness for low-sample neuroimaging tasks.

Abstract: Machine learning and data processing techniques relying on covariance information are widespread as they identify meaningful patterns in unsupervised and unlabeled settings. As a prominent example, Principal Component Analysis (PCA) projects data points onto the eigenvectors of their covariance matrix, capturing the directions of maximum variance. This mapping, however, falls short in two directions: it fails to capture information in low-variance directions, relevant when, e.g., the data contains high-variance noise; and it provides unstable results in low-sample regimes, especially when covariance eigenvalues are close. CoVariance Neural Networks (VNNs), i.e., graph neural networks using the covariance matrix as a graph, show improved stability to estimation errors and learn more expressive functions in the covariance spectrum than PCA, but require training and operate in a labeled setup. To get the benefits of both worlds, we propose Covariance Scattering Transforms (CSTs), deep untrained networks that sequentially apply filters localized in the covariance spectrum to the input data and produce expressive hierarchical representations via nonlinearities. We define the filters as covariance wavelets that capture specific and detailed covariance spectral patterns. We improve CSTs' computational and memory efficiency via a pruning mechanism, and we prove that their error due to finite-sample covariance estimations is less sensitive to close covariance eigenvalues compared to PCA, improving their stability. Our experiments on age prediction from cortical thickness measurements on 4 datasets collecting patients with neurodegenerative diseases show that CSTs produce stable representations in low-data settings, as VNNs but without any training, and lead to comparable or better predictions w.r.t. more complex learning models.

</details>


### [69] [Spectral Predictability as a Fast Reliability Indicator for Time Series Forecasting Model Selection](https://arxiv.org/abs/2511.08884)
*Oliver Wang,Pengrui Quan,Kang Yang,Mani Srivastava*

Main category: cs.LG

TL;DR: Spectral predictability Ω enables fast, predictive stratification of time-series model performance, allowing a quick first-pass model selection and reducing validation cost.


<details>
  <summary>Details</summary>
Motivation: Practitioners face computationally expensive exhaustive validation; choosing a poor model degrades performance.

Method: Define and compute Ω; conduct controlled experiments across domains; expand to 51 models and 28 datasets; analyze performance stratification.

Result: Ω correlates with performance; TSFMs outperform when Ω is high; advantage fades as Ω decreases; computing Ω takes seconds per dataset; provides a fast screening tool.

Conclusion: Ω offers a practical first-pass filter to guide model choice, focusing attention on low-Ω problems that require stronger models; helps decide when TSFMs are worth it.

Abstract: Practitioners deploying time series forecasting models face a dilemma: exhaustively validating dozens of models is computationally prohibitive, yet choosing the wrong model risks poor performance. We show that spectral predictability~$Ω$ -- a simple signal processing metric -- systematically stratifies model family performance, enabling fast model selection. We conduct controlled experiments in four different domains, then further expand our analysis to 51 models and 28 datasets from the GIFT-Eval benchmark. We find that large time series foundation models (TSFMs) systematically outperform lightweight task-trained baselines when $Ω$ is high, while their advantage vanishes as $Ω$ drops. Computing $Ω$ takes seconds per dataset, enabling practitioners to quickly assess whether their data suits TSFM approaches or whether simpler, cheaper models suffice. We demonstrate that $Ω$ stratifies model performance predictably, offering a practical first-pass filter that reduces validation costs while highlighting the need for models that excel on genuinely difficult (low-$Ω$) problems rather than merely optimizing easy ones.

</details>


### [70] [FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis](https://arxiv.org/abs/2511.08887)
*Tianming Sha,Zechuan Chen,Zhan Cheng,Haotian Zhai,Xuwei Ding,Junnan Li,Haixiang Tang,Zaoting Sun,Yanchuan Tang,Yongzhe Yi,Yanjie Huang,Anhao Li,Yuan Gao,Keze Wang*

Main category: cs.LG

TL;DR: 提出一种结合域对抗训练与分组分布鲁棒优化的新框架FAST-CAD，用于公平且高效的非接触性中风诊断，具理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有自动诊断方法在不同人口子群中的公平性不足，可能加剧医疗不平等；需要在保持准确性的同时实现跨人群公平的非接触诊断。

Method: 将域对抗训练(DAT)与分组分布鲁棒优化(Group-DRO)结合；使用自监督编码器与对抗域鉴别学习学习人口变量不变表征，Group-DRO优化最差子群风险；理论上给出收敛性和公平性界限。

Result: 在多模态数据集上对12个年龄、性别、姿态子群进行评估，FAST-CAD在诊断性能上优于基线并保持对各子群的公平性，理论分析支持 DAT + Group-DRO 的统一框架有效性。

Conclusion: 为公平医疗AI系统提供实际性进展与理论见解，提出统一的 DAT + Group-DRO 框架以实现公平且稳健的中风诊断。

Abstract: Stroke is an acute cerebrovascular disease, and timely diagnosis significantly improves patient survival. However, existing automated diagnosis methods suffer from fairness issues across demographic groups, potentially exacerbating healthcare disparities. In this work we propose FAST-CAD, a theoretically grounded framework that combines domain-adversarial training (DAT) with group distributionally robust optimization (Group-DRO) for fair and accurate non-contact stroke diagnosis. Our approach is built on domain adaptation and minimax fairness theory and provides convergence guarantees and fairness bounds. We curate a multimodal dataset covering 12 demographic subgroups defined by age, gender, and posture. FAST-CAD employs self-supervised encoders with adversarial domain discrimination to learn demographic-invariant representations, while Group-DRO optimizes worst-group risk to ensure robust performance across all subgroups. Extensive experiments show that our method achieves superior diagnostic performance while maintaining fairness across demographic groups, and our theoretical analysis supports the effectiveness of the unified DAT + Group-DRO framework. This work provides both practical advances and theoretical insights for fair medical AI systems.

</details>


### [71] [Quasi-Newton Compatible Actor-Critic for Deterministic Policies](https://arxiv.org/abs/2511.09509)
*Arash Bahari Kordabad,Dean Brandner,Sebastien Gros,Sergio Lucia,Sadegh Soudjani*

Main category: cs.LG

TL;DR: 提出一种二阶确定性actor-critic框架，通过二次critic捕捉策略性能的曲率信息，并采用最小二乘时序差分学习估计参数，支持准牛顿风格的actor更新，从而比一阶方法具备更快的收敛与更好性能。


<details>
  <summary>Details</summary>
Motivation: 在确定性策略梯度的基础上，利用性能函数的曲率信息以提升学习效率和收敛性，方法对任意可微策略类具普适性。

Method: 引入兼容函数逼近下的二次critic，既保留真实策略梯度，又近似性能Hessian；通过最小二乘时序差分学习估计二次critic参数；基于critic信息实现准牛顿型的actor更新。

Result: 数值实验表明，与标准的一阶确定性actor-critic基线相比，所提方法在收敛速度和最终性能上具有显著提升。

Conclusion: 该框架对可微策略族具有普适性，能提升样本利用效率与收敛性，适合扩展到更复杂的强化学习场景。

Abstract: In this paper, we propose a second-order deterministic actor-critic framework in reinforcement learning that extends the classical deterministic policy gradient method to exploit curvature information of the performance function. Building on the concept of compatible function approximation for the critic, we introduce a quadratic critic that simultaneously preserves the true policy gradient and an approximation of the performance Hessian. A least-squares temporal difference learning scheme is then developed to estimate the quadratic critic parameters efficiently. This construction enables a quasi-Newton actor update using information learned by the critic, yielding faster convergence compared to first-order methods. The proposed approach is general and applicable to any differentiable policy class. Numerical examples demonstrate that the method achieves improved convergence and performance over standard deterministic actor-critic baselines.

</details>


### [72] [Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2511.08922)
*Yunchang Ma,Tenglong Liu,Yixing Lan,Xin Yin,Changxin Zhang,Xinglong Zhang,Xin Xu*

Main category: cs.LG

TL;DR: DIVO uses value-conditioned diffusion with a binary-weighted mechanism to focus training on high-advantage actions from offline data and to filter high-return candidates during policy optimization, achieving a better conservatism–explorability balance in offline RL.


<details>
  <summary>Details</summary>
Motivation: Address value overestimation and excessive conservatism caused by applying diffusion-based regularization to out-of-distribution (OOD) actions and low-quality datasets.

Method: Introduce a binary-weighted mechanism that uses action advantages from the offline dataset to guide diffusion model training, enabling selective expansion of high-advantage actions; during policy improvement, dynamically filter high-return-potential actions from the diffusion model to guide learning.

Result: On D4RL benchmarks, DIVO achieves superior performance, with notable gains in average returns across locomotion tasks and outperforming baselines in the challenging AntMaze domain with sparse rewards.

Conclusion: DIVO achieves a critical balance between conservatism and explorability in offline RL by leveraging value-conditioned diffusion for targeted coverage and selective action expansion.

Abstract: In offline reinforcement learning, value overestimation caused by out-of-distribution (OOD) actions significantly limits policy performance. Recently, diffusion models have been leveraged for their strong distribution-matching capabilities, enforcing conservatism through behavior policy constraints. However, existing methods often apply indiscriminate regularization to redundant actions in low-quality datasets, resulting in excessive conservatism and an imbalance between the expressiveness and efficiency of diffusion modeling. To address these issues, we propose DIffusion policies with Value-conditional Optimization (DIVO), a novel approach that leverages diffusion models to generate high-quality, broadly covered in-distribution state-action samples while facilitating efficient policy improvement. Specifically, DIVO introduces a binary-weighted mechanism that utilizes the advantage values of actions in the offline dataset to guide diffusion model training. This enables a more precise alignment with the dataset's distribution while selectively expanding the boundaries of high-advantage actions. During policy improvement, DIVO dynamically filters high-return-potential actions from the diffusion model, effectively guiding the learned policy toward better performance. This approach achieves a critical balance between conservatism and explorability in offline RL. We evaluate DIVO on the D4RL benchmark and compare it against state-of-the-art baselines. Empirical results demonstrate that DIVO achieves superior performance, delivering significant improvements in average returns across locomotion tasks and outperforming existing methods in the challenging AntMaze domain, where sparse rewards pose a major difficulty.

</details>


### [73] [TransactionGPT](https://arxiv.org/abs/2511.08939)
*Yingtong Dou,Zhimeng Jiang,Tianyi Zhang,Mingzhi Hu,Zhichao Xu,Shubham Jain,Uday Singh Saini,Xiran Fan,Jiarui Sun,Menghai Pan,Junpeng Wang,Xin Dai,Liang Wang,Chin-Chia Michael Yeh,Yujie Fan,Vineeth Rakesh,Huiyuan Chen,Mangesh Bendre,Zhongfang Zhuang,Xiaoting Li,Prince Aboagye,Vivian Lai,Minghua Xu,Hao Yang,Yiwei Cai,Mahashweta Das,Yuzhong Chen*

Main category: cs.LG

TL;DR: TGPT 是一个面向消费交易数据的基础模型，采用新颖的三维 Transformer 架构实现交易轨迹建模與多任务下游预测/分类的联合优化。训练于十亿级真实交易数据，显著提升下游分类与未来交易生成表现，并对比基线与生产模型显示优势；并探索将大语言模型嵌入整合进 TGPT 的效果与效率，提出面向交易数据的架构创新与实用指南。


<details>
  <summary>Details</summary>
Motivation: 在海量且复杂的支付交易数据中，需统一建模交易轨迹以支持多种下游任务（预测、分类、生成）。现有方法在多模态融合、长程依赖与计算效率方面存在局限，且难以直接从大规模数据中获益。

Method: 提出面向交易数据的 3D-Transformer 架构，专门设计用于捕捉支付交易的动态特征并实现模态融合与高效计算。支持与下游目标的联合优化，并在十亿级真实交易数据上进行训练。引入基于 LLM 的嵌入以对比评估，并在多个多样数据集上对 TGPT 进行广泛评估，与成熟方法比较其预测准确性、训练与推理速度等指标。

Result: 对下游分类性能显著优于有竞争力的生产模型，在生成未来交易方面也优于基线。结合 LLM 嵌入的 TGPT 表现更佳的预测精度以及更快的训练和推理。跨数据集评估验证了方法的有效性与效率相对优势。

Conclusion: 本工作在 Transaction-like 数据上提出的架构创新与实用指南，具备促进交易数据领域基础模型研究和应用的潜力，且可为未来相关研究提供范式与参考。

Abstract: We present TransactionGPT (TGPT), a foundation model for consumer transaction data within one of world's largest payment networks. TGPT is designed to understand and generate transaction trajectories while simultaneously supporting a variety of downstream prediction and classification tasks. We introduce a novel 3D-Transformer architecture specifically tailored for capturing the complex dynamics in payment transaction data. This architecture incorporates design innovations that enhance modality fusion and computational efficiency, while seamlessly enabling joint optimization with downstream objectives. Trained on billion-scale real-world transactions, TGPT significantly improves downstream classification performance against a competitive production model and exhibits advantages over baselines in generating future transactions. We conduct extensive empirical evaluations utilizing a diverse collection of company transaction datasets spanning multiple downstream tasks, thereby enabling a thorough assessment of TGPT's effectiveness and efficiency in comparison to established methodologies. Furthermore, we examine the incorporation of LLM-derived embeddings within TGPT and benchmark its performance against fine-tuned LLMs, demonstrating that TGPT achieves superior predictive accuracy as well as faster training and inference. We anticipate that the architectural innovations and practical guidelines from this work will advance foundation models for transaction-like data and catalyze future research in this emerging field.

</details>


### [74] [Improving Conditional VAE with approximation using Normalizing Flows](https://arxiv.org/abs/2511.08946)
*Tuhin Subhra De*

Main category: cs.LG

TL;DR: 通过在CVAE中引入可学习的高斯解码方差并用正则化流估计q(z|y)，提升条件CVAE的图像生成质量和建模能力。


<details>
  <summary>Details</summary>
Motivation: CVAE在CV领域的典型缺陷是输出模糊、样本多样性不足，以及若把条件分布q(z|y)与先验p(z)简化为一致，会导致不匹配和性能下降。希望通过更真实的条件潜变量分布来提升生成质量与对数似然。

Method: 在CVAE框架中引入高斯解码方差作为可学习参数以缓解模糊，同时对条件潜变量分布q(z|y)使用正则化流进行参数化，学习比先验p(z)更贴近的分布，实验对比标准CVAE结果。

Result: 相较于传统CVAE，所提方法将 FID 降低约5%，对数似然提升约7.7%，显示通过流式建模的条件潜变量分布能显著提升生成质量与概率建模。

Conclusion: 使用流式建模来近似条件潜变量分布q(z|y)并对解码方差进行可学习化，可有效提升CVAE在条件图像生成任务中的性能，缩小模糊与多样性不足的问题。

Abstract: Variational Autoencoders and Generative Adversarial Networks remained the state-of-the-art (SOTA) generative models until 2022. Now they are superseded by diffusion based models. Efforts to improve traditional models have stagnated as a result. In old-school fashion, we explore image generation with conditional Variational Autoencoders (CVAE) to incorporate desired attributes within the images. VAEs are known to produce blurry images with less diversity, we refer a method that solve this issue by leveraging the variance of the gaussian decoder as a learnable parameter during training. Previous works on CVAEs assumed that the conditional distribution of the latent space given the labels is equal to the prior distribution, which is not the case in reality. We show that estimating it using normalizing flows results in better image generation than existing methods by reducing the FID by 5% and increasing log likelihood by 7.7% than the previous case.

</details>


### [75] [Bayesian Mixture of Experts For Large Language Models](https://arxiv.org/abs/2511.08968)
*Maryam Dialameh,Hossein Rajabzadeh,Weiwei Zhang,Walid Ahmed,Hyock Ju Kwon*

Main category: cs.LG

TL;DR: 提出一种基于贝叶斯专家混合模型的后验不确定性估计框架Bayesian-MoE，面向对微调后的大语言模型（LLMs）在Mixture-of-Experts结构下的可校准不确定性。通过对每个专家的第二个线性层应用结构化拉普拉斯近似，在不改变训练程序或引入新参数的前提下实现可校准的不确定性估计，同时直接在MoE的专家通道上进行分块后验估计，避免对新增适配器模块进行贝叶斯推断。利用Kronecker-factored低秩近似建模曲率，推导可扩展的预测不确定性和边缘似然估计。实验在Qwen1.5-MoE与DeepSeek-MoE等常识推理基准上，显示Bayesian-MoE在平均校准误差（ECE）和负对数似然（NLL）方面优于基线，证实其对下游决策的可靠性贡献。


<details>
  <summary>Details</summary>
Motivation: 在不重新训练、也不添加新参数的前提下，为微调后的LLMs提供可校准、可靠的不确定性估计。通过直接针对MoE的专家通道进行后验推断，利用MoE的模块化结构实现可控、可扩展的后验近似。

Method: 对每个专家的第二个线性层应用结构化拉普拉斯近似，进行分块后验估计；采用Kronecker-factored低秩近似来建模曲率；推导出可扩展的预测不确定性和边际似然估计；不修改原始训练流程，也不引入额外参数。

Result: 在Qwen1.5-MoE与DeepSeek-MoE的常识推理基准上，Bayesian-MoE在ECE和NLL方面优于基线，提升了不确定性校准与预测可靠性。

Conclusion: Bayesian-MoE通过对MoE中的专家路径进行后验推断与结构化近似，实现了不修改训练流程的前提下的可校准不确定性估计，适用于提升下游任务的决策可靠性。

Abstract: We present Bayesian Mixture of Experts (Bayesian-MoE), a post-hoc uncertainty estimation framework for fine-tuned large language models (LLMs) based on Mixture-of-Experts architectures. Our method applies a structured Laplace approximation to the second linear layer of each expert, enabling calibrated uncertainty estimation without modifying the original training procedure or introducing new parameters. Unlike prior approaches, which apply Bayesian inference to added adapter modules, Bayesian-MoE directly targets the expert pathways already present in MoE models, leveraging their modular design for tractable block-wise posterior estimation. We use Kronecker-factored low-rank approximations to model curvature and derive scalable estimates of predictive uncertainty and marginal likelihood. Experiments on common-sense reasoning benchmarks with Qwen1.5-MoE and DeepSeek-MoE demonstrate that Bayesian-MoE improves both expected calibration error (ECE) and negative log-likelihood (NLL) over baselines, confirming its effectiveness for reliable downstream decision-making.

</details>


### [76] [Data reuse enables cost-efficient randomized trials of medical AI models](https://arxiv.org/abs/2511.08986)
*Michael Nercessian,Wenxin Zhang,Alexander Schubert,Daphne Yang,Maggie Chung,Ahmed Alaa,Adam Yala*

Main category: cs.LG

TL;DR: BRIDGE是一种数据复用的AI风险模型RCT设计，通过在后续模型预测与先前模型预测一致时重复利用已完成试验的参与者级数据，显著降低 enrollment 需求与成本，同时保持统计功效与类型I错误控制。


<details>
  <summary>Details</summary>
Motivation: 医学人工智能工具的RCT常因成本与时间过长，难以及时评估快速迭代的模型。需要一个可对新模型迭代进行高效、可控的证据生成路径。

Method: 提出BRIDGE设计：在历史试验成功预测达到一致时复用参与者级数据；提供判断再利用数据是否可兼容因果推断及控制类型I错误的可操作清单。通过真实世界数据集（乳腺癌、心血管疾病、败血症）评估 successive AI模型的预测一致性（如前后模型在高风险队列的前5%重叠率）并进行乳腺癌筛查情景的仿真，以量化 enrollment 的节省与统计功效。

Result: 在多数据集上观察到 successive 模型之间存在显著的一致性：最高可达到前5%高风险队列的重叠率64.8%。乳腺癌筛查的BRIDGE仿真显示 enrollment 下降46.6%，节省超280万美元，同时保持80% 的统计功效。

Conclusion: 通过将试验转化为自适应、模块化的研究，BRIDGE使每次模型迭代都可以获得一线证据（Level I），从而加速AI在临床的成本效益转化。

Abstract: Randomized controlled trials (RCTs) are indispensable for establishing the clinical value of medical artificial-intelligence (AI) tools, yet their high cost and long timelines hinder timely validation as new models emerge rapidly. Here, we propose BRIDGE, a data-reuse RCT design for AI-based risk models. AI risk models support a broad range of interventions, including screening, treatment selection, and clinical alerts. BRIDGE trials recycle participant-level data from completed trials of AI models when legacy and updated models make concordant predictions, thereby reducing the enrollment requirement for subsequent trials. We provide a practical checklist for investigators to assess whether reusing data from previous trials allows for valid causal inference and preserves type I error. Using real-world datasets across breast cancer, cardiovascular disease, and sepsis, we demonstrate concordance between successive AI models, with up to 64.8% overlap in top 5% high-risk cohorts. We then simulate a series of breast cancer screening studies, where our design reduced required enrollment by 46.6%--saving over US$2.8 million--while maintaining 80% power. By transforming trials into adaptive, modular studies, our proposed design makes Level I evidence generation feasible for every model iteration, thereby accelerating cost-effective translation of AI into routine care.

</details>


### [77] [Fast $k$-means clustering in Riemannian manifolds via Fréchet maps: Applications to large-dimensional SPD matrices](https://arxiv.org/abs/2511.08993)
*Ji Shi,Nicolas Charon,Andreas Mang,Demetrio Labate,Robert Azencott*

Main category: cs.LG

TL;DR: 提出一种基于 p-Fréchet 映射的非欧几里得流形数据高效聚类框架，将数据从任意度量空间嵌入到低维欧几里得空间以应用标准聚类。


<details>
  <summary>Details</summary>
Motivation: 解决高维非欧几里得流形聚类中原生（intrinsic）方法计算成本高、可扩展性差的问题，特别是在 SPD(n) 等复杂流形上。

Method: 定义 p-Fréchet 映射 F^p: M -> R^l，通过一组参考点 r_i ∈ M，计算点到参考点的距离并作为坐标，形成嵌入；在嵌入后的欧几里得空间中应用 k-means 等常规聚类算法。对 F^p 在欧几里得空间及 SPD(n) 等流形上给出性质分析（如界、稳定性、近似保序等）。

Result: 在合成数据和真实 SPD(n) 数据上进行广泛数值实验，结果显示比内在流形方法快数十至数百倍（高达两个数量级），且聚类精度高，在现有方法表现不佳的场景也表现良好。

Conclusion: 该嵌入式聚类框架为在复杂流形上的大规模聚类提供了可扩展、鲁棒的解决方案，未来可扩展到更多流形及优化参考点选取策略。

Abstract: We introduce a novel, efficient framework for clustering data on high-dimensional, non-Euclidean manifolds that overcomes the computational challenges associated with standard intrinsic methods. The key innovation is the use of the $p$-Fréchet map $F^p : \mathcal{M} \to \mathbb{R}^\ell$ -- defined on a generic metric space $\mathcal{M}$ -- which embeds the manifold data into a lower-dimensional Euclidean space $\mathbb{R}^\ell$ using a set of reference points $\{r_i\}_{i=1}^\ell$, $r_i \in \mathcal{M}$. Once embedded, we can efficiently and accurately apply standard Euclidean clustering techniques such as k-means. We rigorously analyze the mathematical properties of $F^p$ in the Euclidean space and the challenging manifold of $n \times n$ symmetric positive definite matrices $\mathit{SPD}(n)$. Extensive numerical experiments using synthetic and real $\mathit{SPD}(n)$ data demonstrate significant performance gains: our method reduces runtime by up to two orders of magnitude compared to intrinsic manifold-based approaches, all while maintaining high clustering accuracy, including scenarios where existing alternative methods struggle or fail.

</details>


### [78] [FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks](https://arxiv.org/abs/2511.09025)
*Tianao Xiang,Mingjian Zhi,Yuanguo Bi,Lin Cai,Yuhao Chen*

Main category: cs.LG

TL;DR: 提出基于联邦学习的大语言模型在自动驾驶中的协作框架FLAD，利用云-边-车架构与并行训练和知识蒸馏实现隐私保护、降低通信成本并提升个性化性能，已在NVIDIA Jetson测试床原型验证。


<details>
  <summary>Details</summary>
Motivation: LLMs在自动驾驶中的数据融合与推理能力强，但训练成本高、传输开销大且涉及敏感驾驶数据的隐私问题。联邦学习可使多辆车辆在不共享原始数据的前提下协同训练模型；需在异构环境下高效整合分布式多模态数据、解决资源受限设备的参与效率、并实现个性化。

Method: 提出三大创新：(1) 云-边-车协同架构，降低通信延迟并保护数据隐私；(2) 智能并行化协同训练与通信调度机制，利用资源受限的终端设备参与训练以提升效率；(3) 基于知识蒸馏的个性化方法，针对异质边缘数据微调LLM。并在NVIDIA Jetson测试床原型实现，解决CPU/GPU内存共享、动态模型分区与容错训练等实际挑战。

Result: 大量实验表明FLAD在端到端自动驾驶性能上优于对比方法，同时高效利用分布式车辆资源，体现隐私保护、降低通信成本并提升个性化适应性的综合优势。

Conclusion: 为协同自动驾驶模型训练和知识共享提供可行路径，展示了云-边-车协同与分布式LLM在未来实际部署中的潜力与研究方向。

Abstract: Large Language Models (LLMs) have impressive data fusion and reasoning capabilities for autonomous driving (AD). However, training LLMs for AD faces significant challenges including high computation transmission costs, and privacy concerns associated with sensitive driving data. Federated Learning (FL) is promising for enabling autonomous vehicles (AVs) to collaboratively train models without sharing raw data. We present Federated LLM-based Autonomous Driving (FLAD), an FL framework that leverages distributed multimodal sensory data across AVs in heterogeneous environment. FLAD has three key innovations: (1) a cloud-edge-vehicle collaborative architecture that reduces communication delay and preserving data privacy; (2) an intelligent parallelized collaborative training with a communication scheduling mechanism that optimizes training efficiency, leveraging end-devices otherwise having insufficient resources for model training; and (3) a knowledge distillation method that personalizes LLM according to heterogeneous edge data. In addition, we prototype FLAD in a testbed with NVIDIA Jetsons, overcoming practical implementation challenges including CPU/GPU memory sharing in resource-constrained devices, dynamic model partitions, and fault-tolerant training.Extensive experimental evaluation demonstrates that FLAD achieves superior end-to-end AD performance while efficiently utilizing distributed vehicular resources, opening up new possibilities for future collaborative AD model training and knowledge sharing.

</details>


### [79] [FedSDWC: Federated Synergistic Dual-Representation Weak Causal Learning for OOD](https://arxiv.org/abs/2511.09036)
*Zhenyuan Huang,Hui Zhang,Wenzhong Tang,Haijun Yang*

Main category: cs.LG

TL;DR: FedSDWC is a causal-inference-based federated learning framework that combines invariant and variant features to handle covariate and semantic shifts, improving generalization and out-of-distribution detection; it surpasses baselines such as FedICON on CIFAR-10 and CIFAR-100.


<details>
  <summary>Details</summary>
Motivation: In federated learning, data privacy and non-iid distribution (including covariate and semantic shifts) undermine reliability. Existing invariant-learning methods struggle to accurately capture invariant features and construct causal representations. A method that integrates invariant and variant features and models their causal relationship can enhance generalization and OOD detection, with theoretical guarantees linked to client priors.

Method: FedSDWC infers causal semantic representations by modeling the weak causal influence between invariant and variant features and integrates them to form causal representations. This approach overcomes limitations of existing invariant-learning methods, directly constructing causal representations. Theoretical generalization error bounds are derived under certain conditions and connected to client prior distributions.

Result: Empirical evaluations on multiple benchmark datasets demonstrate the effectiveness of FedSDWC in handling covariate and semantic shifts, outperforming FedICON by an average of 3.04% on CIFAR-10 and 8.11% on CIFAR-100.

Conclusion: FedSDWC enhances federated learning’s generalization and OOD detection by building causal representations from invariant and variant features, supported by theoretical bounds and empirical gains, offering a principled approach to robust FL under distribution shifts.

Abstract: Amid growing demands for data privacy and advances in computational infrastructure, federated learning (FL) has emerged as a prominent distributed learning paradigm. Nevertheless, differences in data distribution (such as covariate and semantic shifts) severely affect its reliability in real-world deployments. To address this issue, we propose FedSDWC, a causal inference method that integrates both invariant and variant features. FedSDWC infers causal semantic representations by modeling the weak causal influence between invariant and variant features, effectively overcoming the limitations of existing invariant learning methods in accurately capturing invariant features and directly constructing causal representations. This approach significantly enhances FL's ability to generalize and detect OOD data. Theoretically, we derive FedSDWC's generalization error bound under specific conditions and, for the first time, establish its relationship with client prior distributions. Moreover, extensive experiments conducted on multiple benchmark datasets validate the superior performance of FedSDWC in handling covariate and semantic shifts. For example, FedSDWC outperforms FedICON, the next best baseline, by an average of 3.04% on CIFAR-10 and 8.11% on CIFAR-100.

</details>


### [80] [Fairness-Aware Few-Shot Learning for Audio-Visual Stress Detection](https://arxiv.org/abs/2511.09039)
*Anushka Sanjay Shelke,Aditya Sneh,Arya Adyasha,Haroon R. Lone*

Main category: cs.LG

TL;DR: 提出 FairM2S 的公平性元学习框架，用于音视频数据的压力检测，解决数据匮乏情境中的性别偏见问题。通过在元训练和自适应阶段引入 Equalized Odds 约束，结合对抗梯度屏蔽和公平性约束的元更新，在与五个基线对比中实现 78.1% 的准确率与 Equal Opportunity 0.06 的显著公平性提升，且发布了手机采集数据集 SAVSD。


<details>
  <summary>Details</summary>
Motivation: 在基于 AI 的压力检测中，存在性别偏见，尤其在数据稀缺场景下更为显著，影响公平性和可推广性。

Method: 提出 FairM2S，在元学习框架中加入 Equalized Odds 公平约束，利用对抗梯度屏蔽和公平性约束的元更新来缓解偏见。数据模态为音视频，具备少样本学习能力。

Result: 与五个前沿基线相比，FairM2S 达到 78.1% 的准确率，并将 Equal Opportunity 降至 0.06，显示出显著的公平性提升。

Conclusion: FairM2S 可视为在可扩展的少样本压力检测场景中，面向公平与性能的前沿方法；并发布 SAVSD 数据集，以支持在真实世界低资源条件下的公平性研究与应用。

Abstract: Fairness in AI-driven stress detection is critical for equitable mental healthcare, yet existing models frequently exhibit gender bias, particularly in data-scarce scenarios. To address this, we propose FairM2S, a fairness-aware meta-learning framework for stress detection leveraging audio-visual data. FairM2S integrates Equalized Odds constraints during both meta-training and adaptation phases, employing adversarial gradient masking and fairness-constrained meta-updates to effectively mitigate bias. Evaluated against five state-of-the-art baselines, FairM2S achieves 78.1% accuracy while reducing the Equal Opportunity to 0.06, demonstrating substantial fairness gains. We also release SAVSD, a smartphone-captured dataset with gender annotations, designed to support fairness research in low-resource, real-world contexts. Together, these contributions position FairM2S as a state-of-the-art approach for equitable and scalable few-shot stress detection in mental health AI. We release our dataset and FairM2S publicly with this paper.

</details>


### [81] [GeoGNN: Quantifying and Mitigating Semantic Drift in Text-Attributed Graphs](https://arxiv.org/abs/2511.09042)
*Liangwei Yang,Jing Ma,Jianguo Zhang,Zhiwei Liu,Jielin Qiu,Shirley Kokane,Shiyu Wang,Haolin Chen,Rithesh Murthy,Ming Zhu,Huan Wang,Weiran Yao,Caiming Xiong,Shelby Heinecke*

Main category: cs.LG

TL;DR: 在文本--属性图的GNN中，沿几何曲面的线性聚合会引起语义漂移，本文提出基于测地线的聚合GeoGNN，并用本地PCA度量语义漂移，实证表明其优于强基线。


<details>
  <summary>Details</summary>
Motivation: 文本属性图（TAG）中的预训练语言模型（PLM）嵌入在非线性、曲面化的语义流形上，线性聚合会扭曲几何结构，导致语义漂移和表达能力下降，需要设计流形感知的聚合机制。

Method: 提出在单位球上的对数映射（log-exp）结合测地线进行聚合的Geodesic Aggregation；并实现GeoGNN，将球面注意力与流形插值结合以实现实际应用。

Result: 在四个基准数据集和多种文本编码器上，GeoGNN显著缓解语义漂移，且持续超越强基线，表现稳定提升。

Conclusion: 强调流形几何在文本属性图学习中的重要性，提出的流形感知聚合是提升GNN性能的关键路径，GeoGNN为此提供了可操作的实现与实证支持。

Abstract: Graph neural networks (GNNs) on text--attributed graphs (TAGs) typically encode node texts using pretrained language models (PLMs) and propagate these embeddings through linear neighborhood aggregation. However, the representation spaces of modern PLMs are highly non--linear and geometrically structured, where textual embeddings reside on curved semantic manifolds rather than flat Euclidean spaces. Linear aggregation on such manifolds inevitably distorts geometry and causes semantic drift--a phenomenon where aggregated representations deviate from the intrinsic manifold, losing semantic fidelity and expressive power. To quantitatively investigate this problem, this work introduces a local PCA--based metric that measures the degree of semantic drift and provides the first quantitative framework to analyze how different aggregation mechanisms affect manifold structure. Building upon these insights, we propose Geodesic Aggregation, a manifold--aware mechanism that aggregates neighbor information along geodesics via log--exp mappings on the unit sphere, ensuring that representations remain faithful to the semantic manifold during message passing. We further develop GeoGNN, a practical instantiation that integrates spherical attention with manifold interpolation. Extensive experiments across four benchmark datasets and multiple text encoders show that GeoGNN substantially mitigates semantic drift and consistently outperforms strong baselines, establishing the importance of manifold--aware aggregation in text--attributed graph learning.

</details>


### [82] [Preference is More Than Comparisons: Rethinking Dueling Bandits with Augmented Human Feedback](https://arxiv.org/abs/2511.09047)
*Shengbo Wang,Hong Sun,Ke Li*

Main category: cs.LG

TL;DR: 在无模型的对打赌框架中，通过增强信心界与反馈增强策略处理稀疏人类反馈的交互偏好学习，并给出理论与实证结果。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏的人类反馈导致的学习低效，降低对参数化奖励模型的依赖，提升交互偏好学习的鲁棒性与泛化性。

Method: 提出基于增强信心界的模型无关对打赌框架，整合增强的人类反馈，且在广义集中性假设下进行理论分析与 regret（遗憾）界，给出原型算法并在多领域基准上验证。

Result: 原型算法在推荐、 多目标优化、以及大语言模型响应优化等IPE基准上展现竞争性性能，并给出理论遗憾界/收敛性分析，支持方法的有效性。

Conclusion: 通过引入反馈增强的增强信心界，实现对稀疏偏好反馈的高效学习，且具备广泛应用潜力与理论保障，未来可扩展至更多实时偏好学习场景。

Abstract: Interactive preference elicitation (IPE) aims to substantially reduce human effort while acquiring human preferences in wide personalization systems. Dueling bandit (DB) algorithms enable optimal decision-making in IPE building on pairwise comparisons. However, they remain inefficient when human feedback is sparse. Existing methods address sparsity by heavily relying on parametric reward models, whose rigid assumptions are vulnerable to misspecification. In contrast, we explore an alternative perspective based on feedback augmentation, and introduce critical improvements to the model-free DB framework. Specifically, we introduce augmented confidence bounds to integrate augmented human feedback under generalized concentration properties, and analyze the multi-factored performance trade-off via regret analysis. Our prototype algorithm achieves competitive performance across several IPE benchmarks, including recommendation, multi-objective optimization, and response optimization for large language models, demonstrating the potential of our approach for provably efficient IPE in broader applications.

</details>


### [83] [Guaranteeing Conservation of Integrals with Projection in Physics-Informed Neural Networks](https://arxiv.org/abs/2511.09048)
*Anthony Baez,Wang Zhang,Ziwen Ma,Lam Nguyen,Subhro Das,Luca Daniel*

Main category: cs.LG

TL;DR: Introduces PINN-Proj, a projection method that exactly conserves linear and/or quadratic integrals in PINNs by solving constrained optimization; reduces conservation error by 3–4 orders with only marginal PDE error impact; may improve optimization conditioning and generalize to other integral quantities.


<details>
  <summary>Details</summary>
Motivation: Soft constraints in PINNs allow violation of physical laws; a projection-based approach is needed to enforce exact conservation of integral quantities without compromising training flexibility.

Method: Derive projection formulas by solving constrained nonlinear optimization problems to enforce separate and joint conservation of linear and quadratic integrals; apply these projections to PINNs, yielding PINN-Proj.

Result: Conservation error reduced by 3–4 orders of magnitude compared to soft constraints; PDE solution error is marginally reduced; evidence that projection improves convergence by conditioning the loss landscape.

Conclusion: PINN-Proj offers a general framework to guarantee the conservation of integral quantities in PINNs if a tractable projection is available, with potential broad applicability.

Abstract: We propose a novel projection method that guarantees the conservation of integral quantities in Physics-Informed Neural Networks (PINNs). While the soft constraint that PINNs use to enforce the structure of partial differential equations (PDEs) enables necessary flexibility during training, it also permits the discovered solution to violate physical laws. To address this, we introduce a projection method that guarantees the conservation of the linear and quadratic integrals, both separately and jointly. We derived the projection formulae by solving constrained non-linear optimization problems and found that our PINN modified with the projection, which we call PINN-Proj, reduced the error in the conservation of these quantities by three to four orders of magnitude compared to the soft constraint and marginally reduced the PDE solution error. We also found evidence that the projection improved convergence through improving the conditioning of the loss landscape. Our method holds promise as a general framework to guarantee the conservation of any integral quantity in a PINN if a tractable solution exists.

</details>


### [84] [Break the Tie: Learning Cluster-Customized Category Relationships for Categorical Data Clustering](https://arxiv.org/abs/2511.09049)
*Mingjie Zhao,Zhanpei Huang,Yang Lu,Mengke Li,Yiqun Zhang,Weifeng Su,Yiu-ming Cheung*

Main category: cs.LG

TL;DR: 提出可学习的分类属性距离度量以灵活揭示不同数据簇结构；实现对混合数据集的扩展并在12个基准数据集上显著提升聚类准确性。


<details>
  <summary>Details</summary>
Motivation: 在聚类分析中，分类属性的离散类别缺乏明确的相互关系，传统距离度量通常假设固定的类别拓扑关系，导致对不同簇分布的适应性不足和聚类性能受限。需要通过学习可自定义的类别关系来更准确地揭示簇结构。

Method: 提出一种学习可定制的类别距离度量的方法，打破类别之间的固定关系约束，使得所学习的类别关系与数据簇分布更契合，并且使所得到的类别关系与欧氏距离兼容，从而可无缝扩展到含数值属性的混合数据集。

Result: 在12个真实基准数据集上进行比较实验并进行显著性检验，所提方法的聚类准确性显著优于当前最佳方法，平均排名为1.25，而对比方法为5.21。

Conclusion: 通过可学习的类别关系实现更强的拟合能力与簇发现效果，并且所学习的关系与欧氏距离兼容，便于在混合数据集上应用，实验结果支持其优越性。

Abstract: Categorical attributes with qualitative values are ubiquitous in cluster analysis of real datasets. Unlike the Euclidean distance of numerical attributes, the categorical attributes lack well-defined relationships of their possible values (also called categories interchangeably), which hampers the exploration of compact categorical data clusters. Although most attempts are made for developing appropriate distance metrics, they typically assume a fixed topological relationship between categories when learning distance metrics, which limits their adaptability to varying cluster structures and often leads to suboptimal clustering performance. This paper, therefore, breaks the intrinsic relationship tie of attribute categories and learns customized distance metrics suitable for flexibly and accurately revealing various cluster distributions. As a result, the fitting ability of the clustering algorithm is significantly enhanced, benefiting from the learnable category relationships. Moreover, the learned category relationships are proved to be Euclidean distance metric-compatible, enabling a seamless extension to mixed datasets that include both numerical and categorical attributes. Comparative experiments on 12 real benchmark datasets with significance tests show the superior clustering accuracy of the proposed method with an average ranking of 1.25, which is significantly higher than the 5.21 ranking of the current best-performing method.

</details>


### [85] [Human-Corrected Labels Learning: Enhancing Labels Quality via Human Correction of VLMs Discrepancies](https://arxiv.org/abs/2511.09063)
*Zhongnian Li,Lan Chen,Yixin Xu,Shi Xu,Xinzheng Xu*

Main category: cs.LG

TL;DR: 提出了人类纠错标签(HCL)框架，通过对VLM生成的噪声标签进行有选择的人工纠错，并结合VLM输出与模型预测，提出风险一致估计器和条件概率标签分布估计方法，显著提升分类性能并对标签噪声具有鲁棒性；附带开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决VLM在数据标注中的标签质量低和缺乏纠错机制的问题，提升标注质量并降低人工成本。

Method: 1) HCL：仅对VLM存在不一致的样本进行人工纠错；2) 提出风险一致估计器，将人工纠错标签与VLM预测结合用于训练分类器；3) 提出条件概率方法，结合VLM输出和模型预测估计标签分布。

Result: 在分类任务上实现更高准确率，对噪声标签具鲁棒性；实验证明方法有效，且代码开放。

Conclusion: HCL在实际弱监督场景中有效，提供了一种高效的标签改进路径和可复现的训练框架。

Abstract: Vision-Language Models (VLMs), with their powerful content generation capabilities, have been successfully applied to data annotation processes. However, the VLM-generated labels exhibit dual limitations: low quality (i.e., label noise) and absence of error correction mechanisms. To enhance label quality, we propose Human-Corrected Labels (HCLs), a novel setting that efficient human correction for VLM-generated noisy labels. As shown in Figure 1(b), HCL strategically deploys human correction only for instances with VLM discrepancies, achieving both higher-quality annotations and reduced labor costs. Specifically, we theoretically derive a risk-consistent estimator that incorporates both human-corrected labels and VLM predictions to train classifiers. Besides, we further propose a conditional probability method to estimate the label distribution using a combination of VLM outputs and model predictions. Extensive experiments demonstrate that our approach achieves superior classification performance and is robust to label noise, validating the effectiveness of HCL in practical weak supervision scenarios. Code https://github.com/Lilianach24/HCL.git

</details>


### [86] [FedPM: Federated Learning Using Second-order Optimization with Preconditioned Mixing of Local Parameters](https://arxiv.org/abs/2511.09100)
*Hiro Ishii,Kenta Niwa,Hiroshi Sawada,Akinori Fujino,Noboru Harada,Rio Yokota*

Main category: cs.LG

TL;DR: FedPM提出一种联邦学习的新方法，在服务端对局部参数进行预条件混合，结合二阶优化来提升收敛性，缓解本地预条件器漂移；在强凸场景单次本地更新下可实现超线性收敛，并在实验中显著提升测试准确率。


<details>
  <summary>Details</summary>
Motivation: 现有二阶FL方法（LocalNewton、LTDA、FedSophia）在客户端进行局部迭代后在服务器上简单混合参数，易导致本地预条件器漂移，严重影响收敛，尤其在数据异质性下。

Method: 将理想的二阶更新分解为服务器端的参数混合和客户端的本地参数更新，并在服务器端引入预条件混合本地参数的机制，以抑制漂移并实现更稳健的更新。

Result: 给出对收敛性的理论分析，在强凸且仅一次本地更新的情形下获得超线性收敛；在大量实验中，FedPM在测试准确率上显著优于仅进行简单混合的传统方法。

Conclusion: FedPM通过在服务器端实现预条件混合，有效缓解本地预条件器漂移，充分发挥二阶优化潜力，提升收敛速度和模型性能。

Abstract: We propose Federated Preconditioned Mixing (FedPM), a novel Federated Learning (FL) method that leverages second-order optimization. Prior methods--such as LocalNewton, LTDA, and FedSophia--have incorporated second-order optimization in FL by performing iterative local updates on clients and applying simple mixing of local parameters on the server. However, these methods often suffer from drift in local preconditioners, which significantly disrupts the convergence of parameter training, particularly in heterogeneous data settings. To overcome this issue, we refine the update rules by decomposing the ideal second-order update--computed using globally preconditioned global gradients--into parameter mixing on the server and local parameter updates on clients. As a result, our FedPM introduces preconditioned mixing of local parameters on the server, effectively mitigating drift in local preconditioners.
  We provide a theoretical convergence analysis demonstrating a superlinear rate for strongly convex objectives in scenarios involving a single local update. To demonstrate the practical benefits of FedPM, we conducted extensive experiments. The results showed significant improvements with FedPM in the test accuracy compared to conventional methods incorporating simple mixing, fully leveraging the potential of second-order optimization.

</details>


### [87] [Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment](https://arxiv.org/abs/2511.09105)
*Shigeki Kusaka,Keita Saito,Mikoto Kudo,Takumi Tanabe,Akifumi Wachi,Youhei Akimoto*

Main category: cs.LG

TL;DR: 提出了一种最小成本数据投毒攻击框架，用于通过在 RLHF/DPO 的偏好标签翻转来引导 LLM 策略。将攻击问题建模为带线性约束的凸优化，给出最小成本的上下界，并提出一种对现有标签翻转攻击的后处理方法以减少翻转次数但保持攻击效果。实证表明在奖励模型特征维度相对数据集规模较小时，该后处理显著降低投毒成本。


<details>
  <summary>Details</summary>
Motivation: 在真实系统中，LLMs 通过 RLHF/DPO 进行对齐，数据投毒攻击的理论基础尚不充分。需要量化达到目标的最小成本，以及评估鲁棒性的方法。

Method: 将投毒问题建模为凸优化问题，带线性不等式约束，目标是最小化攻击成本（如翻转标签的成本），在不改变被比较输出的前提下将策略指向攻击者目标。推导最小成本的下界与上界，并给出一种对任意标签翻转攻击的后处理步骤，使其转化为成本更低的等效攻击，保留同等污染效果。

Result: 理论上给出最小攻击成本的界限；方法上提供一个后处理过程以减少翻转数量且不损失效果；实证结果显示与基线相比，该后处理在降低投毒成本方面具有显著优势，尤其当奖励模型的特征维度较小于数据集规模时。

Conclusion: 该工作揭示 RLHF/DPO 流水线的潜在脆弱性，提供评估鲁棒性的新工具，帮助衡量和降低低成本投毒对 LLM 对齐的影响。

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems, making it critical to understand their vulnerabilities. While data poisoning attacks during RLHF/DPO alignment have been studied empirically, their theoretical foundations remain unclear. We investigate the minimum-cost poisoning attack required to steer an LLM's policy toward an attacker's target by flipping preference labels during RLHF/DPO, without altering the compared outputs. We formulate this as a convex optimization problem with linear constraints, deriving lower and upper bounds on the minimum attack cost. As a byproduct of this theoretical analysis, we show that any existing label-flipping attack can be post-processed via our proposed method to reduce the number of label flips required while preserving the intended poisoning effect. Empirical results demonstrate that this cost-minimization post-processing can significantly reduce poisoning costs over baselines, particularly when the reward model's feature dimension is small relative to the dataset size. These findings highlight fundamental vulnerabilities in RLHF/DPO pipelines and provide tools to evaluate their robustness against low-cost poisoning attacks.

</details>


### [88] [Practical Global and Local Bounds in Gaussian Process Regression via Chaining](https://arxiv.org/abs/2511.09144)
*Junyi Liu,Stanley Kok*

Main category: cs.LG

TL;DR: 提出基于 chaining 的框架，用于在未见数据上估计高/低极端值的期望界，并给出全局与局部不确定性量化；对常用核（RBF、Matérn）给出核特定的收紧界，避免解析松弛以提高数值紧致度，且不依赖具体输入位置。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性界限往往依赖对特定输入特征的访问、后验均值/方差或调参，导致鲁棒性不足且难以刻画模型在全局层面的期望行为。需要在不依赖输入定位信息的前提下，获得全局以及局部的不确定性界和更紧的界限。

Method: 提出一个基于 chaining 的框架来估计未见数据上的极端值的期望上下界，理论上给出全局界的收敛性与核相关的优化。对 RBF、Matérn 等常用核给出核特定的界限改进，且通过避免解析松弛提升数值紧致度。另提出面向给定输入的局部不确定性量化方法，利用 chaining 几何和分区直径来适应局部结构，不依赖后验方差的尺度。

Result: 理论上给出全局极值期望界的收敛性和核相关的紧界；核特定改进使得 RBF、Matérn 的界更紧且数值更稳定；局部量化方法能在指定输入处实现对不确定性的自适应估计，实验结果在合成与真实数据集上优于现有方法。

Conclusion: 提供一种面向高可信度应用的新框架，结合 chaining 结构的全局不确定性界估计和局部不确定性量化，且对常用核具备核特定的收紧和数值稳定性提升。

Abstract: Gaussian process regression (GPR) is a popular nonparametric Bayesian method that provides predictive uncertainty estimates and is widely used in safety-critical applications. While prior research has introduced various uncertainty bounds, most existing approaches require access to specific input features and rely on posterior mean and variance estimates or tuning hyperparameters. These limitations hinder robustness and fail to capture the model's global behavior in expectation. To address these limitations, we propose a chaining-based framework for estimating upper and lower bounds on the expected extreme values over unseen data, without requiring access to specific input locations. We provide kernel-specific refinements for commonly used kernels such as RBF and Matérn, in which our bounds are tighter than generic constructions. We further improve numerical tightness by avoiding analytical relaxations. In addition to global estimation, we also develop a novel method for local uncertainty quantification at specified inputs. This approach leverages chaining geometry through partition diameters, adapting to local structure without relying on posterior variance scaling. Our experimental results validate the theoretical findings and demonstrate that our method outperforms existing approaches on both synthetic and real-world datasets.

</details>


### [89] [Unsupervised Feature Selection Through Group Discovery](https://arxiv.org/abs/2511.09166)
*Shira Lifshitz,Ofir Lindenbaum,Gal Mishne,Ron Meir,Hadas Benisty*

Main category: cs.LG

TL;DR: 提出 GroupFS：一个端到端、可微分的无监督特征选择框架，能够同时发现潜在的特征组并选择最具信息量的组，利用特征与样本图的拉普拉斯平滑以及组稀疏正则，在九项基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督特征选择多在独立评估特征，难以捕捉特征之间的群组效应。部分方法虽尝试组结构，但依赖预定义分组或标签监督，限制了通用性。需要不依赖固定分组、端到端学习的组级特征选择。

Method: 提出 GroupFS：端到端、可微分的框架，联合发现潜在特征组并在组内进行信息量选择；对特征图和样本图都施加拉普拉斯平滑；引入组稀疏正则以学习紧凑的结构化表示。

Result: 在覆盖图像、表格和生物数据的九个基准上，GroupFS 在聚类等任务上持续优于最先进的无监督FS，并选择出与有意义模式相一致的特征组。

Conclusion: GroupFS 展示了无监督分组特征选择的可行性和有效性，为高维数据的解释性和泛化能力提供了新思路，且具有跨领域的适用性。

Abstract: Unsupervised feature selection (FS) is essential for high-dimensional learning tasks where labels are not available. It helps reduce noise, improve generalization, and enhance interpretability. However, most existing unsupervised FS methods evaluate features in isolation, even though informative signals often emerge from groups of related features. For example, adjacent pixels, functionally connected brain regions, or correlated financial indicators tend to act together, making independent evaluation suboptimal. Although some methods attempt to capture group structure, they typically rely on predefined partitions or label supervision, limiting their applicability. We propose GroupFS, an end-to-end, fully differentiable framework that jointly discovers latent feature groups and selects the most informative groups among them, without relying on fixed a priori groups or label supervision. GroupFS enforces Laplacian smoothness on both feature and sample graphs and applies a group sparsity regularizer to learn a compact, structured representation. Across nine benchmarks spanning images, tabular data, and biological datasets, GroupFS consistently outperforms state-of-the-art unsupervised FS in clustering and selects groups of features that align with meaningful patterns.

</details>


### [90] [Data Fusion-Enhanced Decision Transformer for Stable Cross-Domain Generalization](https://arxiv.org/abs/2511.09173)
*Guojian Wang,Quinson Hon,Xuyang Chen,Lin Zhao*

Main category: cs.LG

TL;DR: DFDT 提出一种数据融合强化的决策变换器，用于跨域转移问题。通过两级数据过滤、MMD 对齐状态结构、OT 偏差确保动作可行性，将目标数据与可信源片段融合成一个可行的分布，并用优势条件化的令牌替代 RTG，辅以 Q 引导正则化以抑制接点跳跃，从而提升跨域任务的回报与稳定性。理论给出与 MMD 与 OT 相关的性能上界，实验在 D4RL 风格任务上优于强基线。


<details>
  <summary>Details</summary>
Motivation: 跨域分布转移使决策变换器难以拼接不同来源的轨迹，现有方法通常只用单一过滤准则，导致状态结构错位、RTG 比较性下降、以及接点动作跳跃，进而削弱模型推理能力。需要提升跨域片段的拼接性与序列语义连续性。

Method: 提出 DFDT：1) 两级数据过滤：挑选并加权可信源片段与目标数据；2) MMD-mismatch 用于状态结构对齐；3) OT-deviation 用于动作可行性评估；4) 以可行性加权的融合分布训练；5) 用优势条件化令牌替代 RTG；6) 引入 Q 指导正则化以抑制接点处的值与动作跳跃。理论推导状态价值与策略性能间的界限，受 MMD 与 OT 的影响。

Result: 在 D4RL 风格的控制任务中，DFDT 相较于强力的离线 RL 与序列模型基线在回报与稳定性上表现更优，并通过 token-stitching 与序列语义稳定性分析得到进一步验证。

Conclusion: DFDT 通过数据融合与语义稳定性设计显著提高跨域 DT 的拼接性与决策稳定性，并给出理论界限证明其收敛性与鲁棒性在 MMD 与 OT 收缩时提升。未来工作可进一步分析计算成本、对更多跨域情景的适配性，以及对 RTG 替换策略的普适性。

Abstract: Cross-domain shifts present a significant challenge for decision transformer (DT) policies. Existing cross-domain policy adaptation methods typically rely on a single simple filtering criterion to select source trajectory fragments and stitch them together. They match either state structure or action feasibility. However, the selected fragments still have poor stitchability: state structures can misalign, the return-to-go (RTG) becomes incomparable when the reward or horizon changes, and actions may jump at trajectory junctions. As a result, RTG tokens lose continuity, which compromises DT's inference ability. To tackle these challenges, we propose Data Fusion-Enhanced Decision Transformer (DFDT), a compact pipeline that restores stitchability. Particularly, DFDT fuses scarce target data with selectively trusted source fragments via a two-level data filter, maximum mean discrepancy (MMD) mismatch for state-structure alignment, and optimal transport (OT) deviation for action feasibility. It then trains on a feasibility-weighted fusion distribution. Furthermore, DFDT replaces RTG tokens with advantage-conditioned tokens, which improves the continuity of the semantics in the token sequence. It also applies a $Q$-guided regularizer to suppress junction value and action jumps. Theoretically, we provide bounds that tie state value and policy performance gaps to the MMD-mismatch and OT-deviation measures, and show that the bounds tighten as these two measures shrink. We show that DFDT improves return and stability over strong offline RL and sequence-model baselines across gravity, kinematic, and morphology shifts on D4RL-style control tasks, and further corroborate these gains with token-stitching and sequence-semantics stability analyses.

</details>


### [91] [FSampler: Training Free Acceleration of Diffusion Sampling via Epsilon Extrapolation](https://arxiv.org/abs/2511.09180)
*Michael A. Vladimir*

Main category: cs.LG

TL;DR: FSampler 通过基于有限差分的高阶外推，在保留原有采样器更新规则的前提下，用历史epsilon预测替代部分模型调用，从而在不改变采样公式的情况下显著加速扩散采样。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在推理阶段的高昂成本，通过尽量减少对模型前向的调用次数（NFE），提升推理效率，同时兼容现有的采样器（Euler/DDIM, DPM++, LMS/AB2, RES 等）。

Method: 在最近的真实模型调用中维护一个简短的epsilon历史，使用二/三/四阶有限差分预测来推断下一个epsilon；在某些步长用预测的epsilon替代模型调用，保持采样器更新规则不变；对预测进行有限性、幅度检验，并引入学习稳定器对 skipped 步的预测重新尺度，必要时可加入梯度估计稳定器；引入保护窗口、周期锚点和连续跳步上限以控制轨迹偏差；在采样器层集成多种方法（Euler/DDIM、DPM++ 2M/2S、LMS/AB2、RES），并提供可选自适应门控以获得更大加速。

Result: 在 FP/FLUX.1 dev、Qwen Image、Wan 2.2 等环境中，所谓的高保真度下（SSIM 0.95–0.99）实现时间缩减 8–22%、模型调用缩减 15–25%；在低保真度（SSIM 0.73–0.74）且使用更激进的自适应门控时，模型调用可减少 45–50%。

Conclusion: FSampler 提供一种训练自由、与采样器无关的加速层，在不改变采样公式的前提下，通过对 epsilons 的预测替代部分模型调用，显著降低推理成本，同时保持较高的图像/生成质量。

Abstract: FSampler is a training free, sampler agnostic execution layer that accelerates diffusion sampling by reducing the number of function evaluations (NFE). FSampler maintains a short history of denoising signals (epsilon) from recent real model calls and extrapolates the next epsilon using finite difference predictors at second order, third order, or fourth order, falling back to lower order when history is insufficient. On selected steps the predicted epsilon substitutes the model call while keeping each sampler's update rule unchanged. Predicted epsilons are validated for finiteness and magnitude; a learning stabilizer rescales predictions on skipped steps to correct drift, and an optional gradient estimation stabilizer compensates local curvature. Protected windows, periodic anchors, and a cap on consecutive skips bound deviation over the trajectory. Operating at the sampler level, FSampler integrates with Euler/DDIM, DPM++ 2M/2S, LMS/AB2, and RES family exponential multistep methods and drops into standard workflows. FLUX.1 dev, Qwen Image, and Wan 2.2, FSampler reduces time by 8 to 22% and model calls by 15 to 25% at high fidelity (Structural Similarity Index (SSIM) 0.95 to 0.99), without altering sampler formulas. With an aggressive adaptive gate, reductions can reach 45 to 50% fewer model calls at lower fidelity (SSIM 0.73 to 0.74).

</details>


### [92] [Sure! Here's a short and concise title for your paper: "Contamination in Generated Text Detection Benchmarks"](https://arxiv.org/abs/2511.09200)
*Philipp Dingfelder,Christian Riess*

Main category: cs.LG

TL;DR: 通过对 DetectRL 数据集的分析与清洗，揭示并消除可被检测器利用的模式化特征，从而提升检测鲁棒性并提供一个更清洁的公开数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的 AI 生成文本检测数据集存在易被利用的简单模式，使得检测器可能通过捷径而非真正理解文本来进行判别，影响鲁棒性与可重复性，因此需要系统性的数据清洗与高质量基准集。

Method: 分析 DetectRL 数据集，识别其中的模式化特征（如特定开头词、任务拒绝等），设计并执行多轮数据清洗流程以去除这些捷径，然后在清洗前后对检测器鲁棒性进行实证比较，最终公开再处理的数据集。

Result: 数据清洗后，检测器对基于模式的攻击的易受性下降、鲁棒性提升，且重新处理的数据集公开可用。

Conclusion: 数据集质量对检测系统的鲁棒性至关重要，系统化的数据清洗能显著减小模式化特征带来的偏差，提升对抗性检测研究的可靠性和可重复性。

Abstract: Large language models are increasingly used for many applications. To prevent illicit use, it is desirable to be able to detect AI-generated text. Training and evaluation of such detectors critically depend on suitable benchmark datasets. Several groups took on the tedious work of collecting, curating, and publishing large and diverse datasets for this task. However, it remains an open challenge to ensure high quality in all relevant aspects of such a dataset. For example, the DetectRL benchmark exhibits relatively simple patterns of AI-generation in 98.5% of the Claude-LLM data. These patterns may include introductory words such as "Sure! Here is the academic article abstract:", or instances where the LLM rejects the prompted task. In this work, we demonstrate that detectors trained on such data use such patterns as shortcuts, which facilitates spoofing attacks on the trained detectors. We consequently reprocessed the DetectRL dataset with several cleansing operations. Experiments show that such data cleansing makes direct attacks more difficult. The reprocessed dataset is publicly available.

</details>


### [93] [Stochastic Mean-Shift Clustering](https://arxiv.org/abs/2511.09202)
*Itshak Lapidot,Yann Sepulcre,Tom Trigano*

Main category: cs.LG

TL;DR: 提出了基于随机梯度上升的随机均值偏移聚类，实验证明在多数情况下优于标准均值移位，并应用于说话人聚类。


<details>
  <summary>Details</summary>
Motivation: 在高维数据聚类中，标准均值移位算法计算成本高且对大规模数据集缺乏鲁棒性，采用随机梯度更新的变体有望提升收敛速度与可扩展性，同时保持聚类性能。

Method: 对数据点序列进行随机选择，按部分梯度上升步骤优化目标函数，提出随机化的均值移位过程；给出收敛性理论分析；在合成二维高斯混合数据与实际说话人聚类任务上进行对比实验。

Result: 在合成二维高斯混合数据上，随机均值移位在多数情形优于标准均值移位及部分对比方法；在说话人聚类任务中也展现出有效性。

Conclusion: 随机化的梯度更新为均值移位带来可扩展性与稳定性，适合大规模和分布变化的数据集，提供了一种可扩展的均值移位变体。

Abstract: We present a stochastic version of the mean-shift clustering algorithm. In this stochastic version a randomly chosen sequence of data points move according to partial gradient ascent steps of the objective function. Theoretical results illustrating the convergence of the proposed approach, and its relative performances is evaluated on synthesized 2-dimensional samples generated by a Gaussian mixture distribution and compared with state-of-the-art methods. It can be observed that in most cases the stochastic mean-shift clustering outperforms the standard mean-shift. We also illustrate as a practical application the use of the presented method for speaker clustering.

</details>


### [94] [CoCo-MILP: Inter-Variable Contrastive and Intra-Constraint Competitive MILP Solution Prediction](https://arxiv.org/abs/2511.09209)
*Tianle Pu,Jianing Li,Yingying Gao,Shixuan Liu,Zijie Geng,Haoyang Liu,Chao Chen,Changjun Fan*

Main category: cs.LG

TL;DR: CoCo-MILP 提出 inter-variable contrastive loss 与 intra-constraint competitive GNN，显式建模变量之间的对比和约束内的竞争关系，从而提升 MILP 的解预测质量；在标准基准上显著优于现有学习方法，解决差距的提升高达 68.12%；并开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有基于 GNN 的 MILP 求解方法在目标学习层面将变量独立处理，忽略变量之间的相对优先级；在模型结构层面，常规的消息传递会对变量表示产生平滑，未能捕捉到约束内变量的竞争与排斥关系。

Method: 提出 Inter-Variable Contrastive Loss (VCL)，通过最大化赋值为 1 与 0 的变量嵌入间的边界来增强变量间对比；设计 Intra-Constraint Competitive GNN 层，使同一约束内的竞争变量的特征学习呈现出排斥性差异，而非同质化。

Result: 在标准基准上，CoCo-MILP 显著优于现有学习方法，减少解的差距（gap）高达 68.12%；并给出公开代码：https://github.com/happypu326/CoCo-MILP。

Conclusion: 通过在目标与结构两端对齐 MILP 的固有结构，CoCo-MILP 能更准确地预测 MILP 的解，展示了学习型 MILP 求解的潜力与可扩展性。

Abstract: Mixed-Integer Linear Programming (MILP) is a cornerstone of combinatorial optimization, yet solving large-scale instances remains a significant computational challenge. Recently, Graph Neural Networks (GNNs) have shown promise in accelerating MILP solvers by predicting high-quality solutions. However, we identify that existing methods misalign with the intrinsic structure of MILP problems at two levels. At the leaning objective level, the Binary Cross-Entropy (BCE) loss treats variables independently, neglecting their relative priority and yielding plausible logits. At the model architecture level, standard GNN message passing inherently smooths the representations across variables, missing the natural competitive relationships within constraints. To address these challenges, we propose CoCo-MILP, which explicitly models inter-variable Contrast and intra-constraint Competition for advanced MILP solution prediction. At the objective level, CoCo-MILP introduces the Inter-Variable Contrastive Loss (VCL), which explicitly maximizes the embedding margin between variables assigned one versus zero. At the architectural level, we design an Intra-Constraint Competitive GNN layer that, instead of homogenizing features, learns to differentiate representations of competing variables within a constraint, capturing their exclusionary nature. Experimental results on standard benchmarks demonstrate that CoCo-MILP significantly outperforms existing learning-based approaches, reducing the solution gap by up to 68.12% compared to traditional solvers. Our code is available at https://github.com/happypu326/CoCo-MILP.

</details>


### [95] [Parameter-Free Clustering via Self-Supervised Consensus Maximization (Extended Version)](https://arxiv.org/abs/2511.09211)
*Lijun Zhang,Suyuan Liu,Siwei Wang,Shengju Yu,Xueling Zhu,Miaomiao Li,Xinwang Liu*

Main category: cs.LG

TL;DR: 提出一个无参数的聚类框架SCMax，通过自监督的一致性最大化在层次聚合聚类中进行表示学习与聚类评估的统一过程，在每一步聚合时生成结构感知表示，并使用最近邻一致性分数来确定最优簇数。实验显示在未知簇数场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法往往依赖大量超参数，尤其是簇数的设定，限制了其实际应用。需要一个完全无参数的、能自适应确定簇数并同时学习表示的框架。

Method: 采用层次凝聚聚类，在每次合并时通过自监督学习任务产生新的结构感知表示；提出最近邻一致性分数，衡量原始表示与自监督表示在最近邻合并决策上的一致性；在一致性最大化发生的时刻，用作确定最优聚类数的准则。

Result: 在多个数据集上进行广泛实验，SCMax在未知簇数设置下的聚类效果优于设计用于无簇数信息场景的现有方法。

Conclusion: SCMax提供了一个完全参数无关的聚类框架，将表征学习与聚类结构在一个统一过程中进行，基于一致性最大化的判定可自动确定簇数，对不同数据集具有鲁棒性。

Abstract: Clustering is a fundamental task in unsupervised learning, but most existing methods heavily rely on hyperparameters such as the number of clusters or other sensitive settings, limiting their applicability in real-world scenarios. To address this long-standing challenge, we propose a novel and fully parameter-free clustering framework via Self-supervised Consensus Maximization, named SCMax. Our framework performs hierarchical agglomerative clustering and cluster evaluation in a single, integrated process. At each step of agglomeration, it creates a new, structure-aware data representation through a self-supervised learning task guided by the current clustering structure. We then introduce a nearest neighbor consensus score, which measures the agreement between the nearest neighbor-based merge decisions suggested by the original representation and the self-supervised one. The moment at which consensus maximization occurs can serve as a criterion for determining the optimal number of clusters. Extensive experiments on multiple datasets demonstrate that the proposed framework outperforms existing clustering approaches designed for scenarios with an unknown number of clusters.

</details>


### [96] [Controllable protein design through Feynman-Kac steering](https://arxiv.org/abs/2511.09216)
*Erik Hartman,Jonas Wallin,Johan Malmström,Jimmy Olsson*

Main category: cs.LG

TL;DR: 扩展Feynman-Kac (FK) 引导框架至扩散式蛋白设计，作为推理时控制策略，结合结构生成引导采样朝向期望的结构/能量特征，同时维持扩散过程的多样性。通过ProteinMPNN和全原子弛豫 refined 模型，对序列与结构属性进行并行生成的奖励。应用于结合体设计，FK引导在多目标上提升预测界面能量，开销极小。总体上，这一方法证明FK控制可对任意非微分、非特定奖励的目标进行泛化，提供一个模型无关的引导分子生成框架。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在蛋白设计中难以定向优化功能性/生物化学目标的问题，提升结合能、序列组成等目标的可控性，同时保持结果多样性。

Method: 将FK引导扩展到扩散式蛋白设计，通过与结构生成耦合，在采样阶段对结果进行奖励引导，使采样朝着结构或能量层面的期望特征前进；为实现同时生成序列与结构属性，奖励在通过 ProteinMPNN 与全原子弛豫细化的模型上计算；针对结合体设计应用，观察到界面能量等指标得到改善，且计算开销低。

Result: 在多样目标下，应用FK引导可一致提升预测的界面能量等性能指标，且计算开销较低，验证了分布式目标下的稳健性与效率；结果显示FK控制可适用于非微分、非特定奖励的广义目标任务。

Conclusion: 推理时的FK控制可将扩散式蛋白设计泛化至任意非微分、奖励不可知的目标，提供一个统一、模型无关的引导分子生成框架。

Abstract: Diffusion-based models have recently enabled the generation of realistic and diverse protein structures, yet they remain limited in their ability to steer outcomes toward specific functional or biochemical objectives, such as binding affinity or sequence composition. Here we extend the Feynman-Kac (FK) steering framework, an inference-time control approach, to diffusion-based protein design. By coupling FK steering with structure generation, the method guides sampling toward desirable structural or energetic features while maintaining the diversity of the underlying diffusion process. To enable simultaneous generation of both sequence and structure properties, rewards are computed on models refined through ProteinMPNN and all-atom relaxation. Applied to binder design, FK steering consistently improves predicted interface energetics across diverse targets with minimal computational overhead. More broadly, this work demonstrates that inference-time FK control generalizes diffusion-based protein design to arbitrary, non-differentiable, and reward-agnostic objectives, providing a unified and model-independent framework for guided molecular generation.

</details>


### [97] [GuardFed: A Trustworthy Federated Learning Framework Against Dual-Facet Attacks](https://arxiv.org/abs/2511.09294)
*Yanli Li,Yanan Zhou,Zhongliang Guo,Nan Yang,Yuning Zhang,Huaming Chen,Dong Yuan,Weiping Ding,Witold Pedrycz*

Main category: cs.LG

TL;DR: 提出并评估一种同时削弱模型准确性与公平性的对抗攻击DFA及其两种变体，并提出自适应防御GuardFed，在公开数据集上表现优于现有鲁棒FL方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有联邦学习在同时关注准确性与公平性场景的脆弱性，填补对双目标攻击的研究空缺，确保在非IID和对抗条件下的鲁棒性和公平性。

Method: 提出Dual-Facet Attack (DFA)，包含Synchronous DFA (S-DFA)与Split DFA (Sp-DFA)两种变体以模拟不同的协同攻击场景；提出GuardFed防御框架，通过在服务器端维护一个公平性参考模型并引入双重信任评分，进行可信更新的选择性聚合。

Result: 实验表明现有鲁棒FL防御对DFA无效；GuardFed在多种非IID和对抗条件下能有效维持准确性和群体公平性，表现优于现有方法。

Conclusion: DFA揭示了多目标对抗对联邦学习系统的挑战，GuardFed提供一种自适应、兼顾公平性的聚合策略，具有一定的理论与实际应用价值。

Abstract: Federated learning (FL) enables privacy-preserving collaborative model training but remains vulnerable to adversarial behaviors that compromise model utility or fairness across sensitive groups. While extensive studies have examined attacks targeting either objective, strategies that simultaneously degrade both utility and fairness remain largely unexplored. To bridge this gap, we introduce the Dual-Facet Attack (DFA), a novel threat model that concurrently undermines predictive accuracy and group fairness. Two variants, Synchronous DFA (S-DFA) and Split DFA (Sp-DFA), are further proposed to capture distinct real-world collusion scenarios. Experimental results show that existing robust FL defenses, including hybrid aggregation schemes, fail to resist DFAs effectively. To counter these threats, we propose GuardFed, a self-adaptive defense framework that maintains a fairness-aware reference model using a small amount of clean server data augmented with synthetic samples. In each training round, GuardFed computes a dual-perspective trust score for every client by jointly evaluating its utility deviation and fairness degradation, thereby enabling selective aggregation of trustworthy updates. Extensive experiments on real-world datasets demonstrate that GuardFed consistently preserves both accuracy and fairness under diverse non-IID and adversarial conditions, achieving state-of-the-art performance compared with existing robust FL methods.

</details>


### [98] [Mixture-of-Channels: Exploiting Sparse FFNs for Efficient LLMs Pre-Training and Inference](https://arxiv.org/abs/2511.09323)
*Tong Wu,Yutong He,Bin Wang,Kun Yuan*

Main category: cs.LG

TL;DR: Mixture-of-Channels (MoC) is a novel FFN architecture for LLMs that activates only the Top-K most relevant channels per token, determined by SwiGLU gating, to reduce activation memory and improve inference efficiency via partial weight loading into GPU SRAM, validated by memory profiling and experiments.


<details>
  <summary>Details</summary>
Motivation: Activation memory, especially FFN activations, is the dominant bottleneck in activation memory for large language models, particularly when FlashAttention is used. Existing memory reductions focus on parameter and optimizer state, but they do not adequately address FFN activation memory.

Method: Introduce MoC: a FFN variant that uses SwiGLU's gating to select the Top-K channels activated per token, reducing activation memory. Also leverage partial weight loading into GPU SRAM to decrease memory accesses. Conduct detailed memory profiling to identify FFN activations as the primary overhead, followed by extensive experiments to measure memory savings, throughput, and model performance.

Result: MoC achieves substantial activation memory savings and throughput gains during pre-training and inference, with competitive model performance, validating its effectiveness in reducing memory bottlenecks without sacrificing accuracy.

Conclusion: MoC effectively mitigates the FFN activation memory bottleneck in LLMs, enabling more memory-efficient pre-training and inference. The approach reduces memory traffic and activation memory, leading to improved throughput while maintaining competitive performance.

Abstract: Large language models (LLMs) have demonstrated remarkable success across diverse artificial intelligence tasks, driven by scaling laws that correlate model size and training data with performance improvements. However, this scaling paradigm incurs substantial memory overhead, creating significant challenges for both training and inference. While existing research has primarily addressed parameter and optimizer state memory reduction, activation memory-particularly from feed-forward networks (FFNs)-has become the critical bottleneck, especially when FlashAttention is implemented. In this work, we conduct a detailed memory profiling of LLMs and identify FFN activations as the predominant source to activation memory overhead. Motivated by this, we introduce Mixture-of-Channels (MoC), a novel FFN architecture that selectively activates only the Top-K most relevant channels per token determined by SwiGLU's native gating mechanism. MoC substantially reduces activation memory during pre-training and improves inference efficiency by reducing memory access through partial weight loading into GPU SRAM. Extensive experiments validate that MoC delivers significant memory savings and throughput gains while maintaining competitive model performance.

</details>


### [99] [MARBLE: Multi-Armed Restless Bandits in Latent Markovian Environment](https://arxiv.org/abs/2511.09324)
*Mohsen Amiri,Konstantin Avrachenkov,Ibtihal El Mimouni,Sindri Magnússon*

Main category: cs.LG

TL;DR: 提出 MARBLE：在隐含马尔可夫环境下的 RMAB，加入潜在环境状态以引入非平稳性；引入 Markov-Averaged Indexability (MAI) 作为放宽的指数性假设；在 MAI 下，即使存在未观测的 regime 切换，同步的 Q-learning 与 Whittle 指数（QWI）几乎必然收敛到最优 Q 函数及相应的 Whittle 指数；在数字孪生推荐系统中验证，QWI 能适应潜在状态的变化并收敛。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中，经典 RMAB 常假设固定动力学，难以应对潜在的 regime 切换导致的非平稳性。引入 MARBLE 以建模潜在马尔可夫环境并提供在此情况下的学习收敛性保障。

Method: 把每个臂置于一个随时间切换的潜在环境状态中，提出 MAI 作为放宽的指数性条件。给出同步 Q 学习结合 Whittle 指数的收敛性分析，证明在 MAI 下即使观察不到 regime 的切换，QWI 可几乎必然收敛至最优 Q 函数与相应的 Whittle 指数。并在一个经过校准的数字孪生推荐系统仿真上进行验证。

Result: 理论上证明了在 MAI 条件下，存在对未观测 regime 切换的鲁棒性，Synchronous QWI 收敛到最优策略及指数组。实验结果表明 QWI 能持续适应潜在状态变化并收敛到最优策略。

Conclusion: MAI 提供了在非平稳 RMAB 的放宽指数性框架下的收敛性保障；MARBLE 模型与 QWI 在数字孪生场景中有效，理论与实证相互印证。

Abstract: Restless Multi-Armed Bandits (RMABs) are powerful models for decision-making under uncertainty, yet classical formulations typically assume fixed dynamics, an assumption often violated in nonstationary environments. We introduce MARBLE (Multi-Armed Restless Bandits in a Latent Markovian Environment), which augments RMABs with a latent Markov state that induces nonstationary behavior. In MARBLE, each arm evolves according to a latent environment state that switches over time, making policy learning substantially more challenging. We further introduce the Markov-Averaged Indexability (MAI) criterion as a relaxed indexability assumption and prove that, despite unobserved regime switches, under the MAI criterion, synchronous Q-learning with Whittle Indices (QWI) converges almost surely to the optimal Q-function and the corresponding Whittle indices. We validate MARBLE on a calibrated simulator-embedded (digital twin) recommender system, where QWI consistently adapts to a shifting latent state and converges to an optimal policy, empirically corroborating our theoretical findings.

</details>


### [100] [GAMMA_FLOW: Guided Analysis of Multi-label spectra by MAtrix Factorization for Lightweight Operational Workflows](https://arxiv.org/abs/2511.09326)
*Viola Rädle,Tilman Hartwig,Benjamin Oesen,Emily Alice Kröger,Julius Vogt,Eike Gericke,Martin Baron*

Main category: cs.LG

TL;DR: GAMMA_FLOW：一个开源的Python包，用于对1D光谱数据的实时、轻量级分析，利用监督NMF实现分类、降噪、分解和离群检测，达到>90%分类准确率，适用于伽马射线谱等任意1D光谱数据。


<details>
  <summary>Details</summary>
Motivation: 需要一种快速、成本低、可扩展的光谱分析方法，避免依赖计算资源密集的模型，支持实时处理与自动解释。

Method: 基于监督的非负矩阵分解用于降维和特征提取，支持单/多分量光谱，作为开放源代码的Python实现，强调快速、灵活和减少计算成本。

Result: 分类准确率超过90%，实现可靠的自动光谱解释，适用于除了伽马射线谱之外的任意1D光谱数据；作为对专有软件的开放替代方案。

Conclusion: GAMMA_FLOW提供快速、有效、可适应的分析框架，降低计算成本，广泛应用于研究和工业领域，适合任何一维光谱数据。

Abstract: GAMMA_FLOW is an open-source Python package for real-time analysis of spectral data. It supports classification, denoising, decomposition, and outlier detection of both single- and multi-component spectra. Instead of relying on large, computationally intensive models, it employs a supervised approach to non-negative matrix factorization (NMF) for dimensionality reduction. This ensures a fast, efficient, and adaptable analysis while reducing computational costs. gamma_flow achieves classification accuracies above 90% and enables reliable automated spectral interpretation. Originally developed for gamma-ray spectra, it is applicable to any type of one-dimensional spectral data. As an open and flexible alternative to proprietary software, it supports various applications in research and industry.

</details>


### [101] [Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier](https://arxiv.org/abs/2511.09332)
*Xinpeng Li,Kai Ming Ting*

Main category: cs.LG

TL;DR: 提出 Distributional Feature Attribution eXplanations (DFAX)，一种基于数据分布的模型无关特征归因方法，首次直接以数据分布为依据解释分类器预测，实验结果显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前特征归因研究缺乏正式的问题定义；许多方法未与数据的概率分布保持一致，导致解释的可信度和普适性受限；需要将解释建立在给定数据分布之上以提升理论和实践的合理性。

Method: 提出 DFAX，这是一种模型无关的特征归因方法，直接基于数据分布来解释分类器的预测。文中先给出特征归因问题的形式化定义，然后设计基于分布的归因策略，克服传统方法的局限性。

Result: 在大量实验中，DFAX在效果和效率上均优于当前最先进的基线方法。

Conclusion: DFAX首次实现以数据分布为核心的特征归因，填补研究中的 formalization 缺口，提供更可信且高效的分布一致性解释。

Abstract: The proliferation of complex, black-box AI models has intensified the need for techniques that can explain their decisions. Feature attribution methods have become a popular solution for providing post-hoc explanations, yet the field has historically lacked a formal problem definition. This paper addresses this gap by introducing a formal definition for the problem of feature attribution, which stipulates that explanations be supported by an underlying probability distribution represented by the given dataset. Our analysis reveals that many existing model-agnostic methods fail to meet this criterion, while even those that do often possess other limitations. To overcome these challenges, we propose Distributional Feature Attribution eXplanations (DFAX), a novel, model-agnostic method for feature attribution. DFAX is the first feature attribution method to explain classifier predictions directly based on the data distribution. We show through extensive experiments that DFAX is more effective and efficient than state-of-the-art baselines.

</details>


### [102] [From Decision Trees to Boolean Logic: A Fast and Unified SHAP Algorithm](https://arxiv.org/abs/2511.09376)
*Alexander Nadel,Ron Wettenstein*

Main category: cs.LG

TL;DR: WOODELF 将 SHAP 计算与布尔逻辑和伪布尔公式结合，提供背景 SHAP 的线性时间计算并支持多种 SHAP 变体，且在 CPU/GPU 上实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 解释树模型的需求日益增长；背景分布下的 SHAP 计算通常代价高昂，迫切需要在大规模数据上高效、可集成的方法。

Method: 将决策树集成、博弈论与布尔逻辑整合，构建伪布尔公式以表示样本特征、树结构和背景数据；据此实现背景 SHAP 的线性时间计算，并可计算 Path-Dependent SHAP、Shapley 互动值、Banzhaf 值及其互动值。

Result: 在包含 3,000,000 行、5,000,000 背景样本、127 特征的数据集上，CPU 162 秒、GPU 16 秒即可计算全部 Background SHAP，与最佳方法相比实现 16x/165x 的加速；实现基于 NumPy、SciPy、CuPy，无需自定义 C++/CUDA。

Conclusion: WOODELF 提供一种高效、可扩展的 SHAP 与博弈价值计算框架，支持大规模数据，易于与现有框架集成，适合实际应用。

Abstract: SHapley Additive exPlanations (SHAP) is a key tool for interpreting decision tree ensembles by assigning contribution values to features. It is widely used in finance, advertising, medicine, and other domains. Two main approaches to SHAP calculation exist: Path-Dependent SHAP, which leverages the tree structure for efficiency, and Background SHAP, which uses a background dataset to estimate feature distributions.
  We introduce WOODELF, a SHAP algorithm that integrates decision trees, game theory, and Boolean logic into a unified framework. For each consumer, WOODELF constructs a pseudo-Boolean formula that captures their feature values, the structure of the decision tree ensemble, and the entire background dataset. It then leverages this representation to compute Background SHAP in linear time. WOODELF can also compute Path-Dependent SHAP, Shapley interaction values, Banzhaf values, and Banzhaf interaction values.
  WOODELF is designed to run efficiently on CPU and GPU hardware alike. Available via the WOODELF Python package, it is implemented using NumPy, SciPy, and CuPy without relying on custom C++ or CUDA code. This design enables fast performance and seamless integration into existing frameworks, supporting large-scale computation of SHAP and other game-theoretic values in practice.
  For example, on a dataset with 3,000,000 rows, 5,000,000 background samples, and 127 features, WOODELF computed all Background Shapley values in 162 seconds on CPU and 16 seconds on GPU - compared to 44 minutes required by the best method on any hardware platform, representing 16x and 165x speedups, respectively.

</details>


### [103] [Diffusion-based Sinogram Interpolation for Limited Angle PET](https://arxiv.org/abs/2511.09383)
*Rüveyda Yilmaz,Julian Thull,Johannes Stegmaier,Volkmar Schulz*

Main category: cs.LG

TL;DR: 使用条件扩散模型对稀疏采样的PET正位成像投影数据进行插值，以支持非圆柱形、缺口的探测几何。


<details>
  <summary>Details</summary>
Motivation: 面对 walk-through 设计、长轴环、间隙导致 severely undersampled sinograms；传统硬件受限，需可学习的先验来填补缺失信息；数据驱动方法，尤其生成模型，能在临床场景中实现更经济、患者友好的几何配置。

Method: 将缺失的线对作为可学习的先验，通过条件扩散模型对稀疏的 sinogram 进行插值，给出可训练的端到端框架，潜在地生成完整的 sinogram，以便后续重建。

Result: 原文摘要未给出具体实验结果，强调方法论和潜在应用。

Conclusion: 提出基于条件扩散模型的缺失 LOR 插值策略，为新型、低成本、友好患者的 PET 几何设计提供路径；需在真实临床数据上进行评估与验证。

Abstract: Accurate PET imaging increasingly requires methods that support unconstrained detector layouts from walk-through designs to long-axial rings where gaps and open sides lead to severely undersampled sinograms. Instead of constraining the hardware to form complete cylinders, we propose treating the missing lines-of-responses as a learnable prior. Data-driven approaches, particularly generative models, offer a promising pathway to recover this missing information. In this work, we explore the use of conditional diffusion models to interpolate sparsely sampled sinograms, paving the way for novel, cost-efficient, and patient-friendly PET geometries in real clinical settings.

</details>


### [104] [Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm](https://arxiv.org/abs/2511.09392)
*Jiajie Su,Zihan Nan,Yunshan Ma,Xiaobo Xia,Xiaohua Feng,Weiming Liu,Xiaolin Zheng,Chaochao Chen*

Main category: cs.LG

TL;DR: 提出 CREAT，一种受限强化驱动的 Profile Pollution Attack（PPA）框架，通过双层优化与多奖励强化学习实现对序列推荐系统的定向污染，同时保持低可检测性和较高隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 序列推荐系统容易成为对抗攻击的目标；现有基于数据投毒的攻击要么需要大规模账号或伪造档案，难以落地。现有的PPA方法在两方面存在不足：对序列视界影响过度依赖，导致对项转移的微调能力不足；全局性修改容易引发分布漂移，被检测到。

Method: 提出 Pattern Balanced Rewarding Policy：整合模式反转奖励以扰动关键模式，同时通过不平衡最优传输实现分布一致性奖励，降低可检测的分布漂移。随后采用 Constrained Group Relative Reinforcement Learning：借助动态屏障约束与组共享经验回放实现逐步扰动，进行有目标的污染，提升隐蔽性。

Result: 通过广泛实验，CREAT 在定向污染任务上相较基线表现更优，能够在保持攻击效果的同时降低被检测的风险。

Conclusion: 将双层优化框架与多奖励强化学习结合，给出一个可扩展且较难被检测的序列推荐定向污染解决方案，具有潜在的现实应用价值与研究启示。

Abstract: Sequential Recommenders, which exploit dynamic user intents through interaction sequences, is vulnerable to adversarial attacks. While existing attacks primarily rely on data poisoning, they require large-scale user access or fake profiles thus lacking practicality. In this paper, we focus on the Profile Pollution Attack that subtly contaminates partial user interactions to induce targeted mispredictions. Previous PPA methods suffer from two limitations, i.e., i) over-reliance on sequence horizon impact restricts fine-grained perturbations on item transitions, and ii) holistic modifications cause detectable distribution shifts. To address these challenges, we propose a constrained reinforcement driven attack CREAT that synergizes a bi-level optimization framework with multi-reward reinforcement learning to balance adversarial efficacy and stealthiness. We first develop a Pattern Balanced Rewarding Policy, which integrates pattern inversion rewards to invert critical patterns and distribution consistency rewards to minimize detectable shifts via unbalanced co-optimal transport. Then we employ a Constrained Group Relative Reinforcement Learning paradigm, enabling step-wise perturbations through dynamic barrier constraints and group-shared experience replay, achieving targeted pollution with minimal detectability. Extensive experiments demonstrate the effectiveness of CREAT.

</details>


### [105] [Abstract Gradient Training: A Unified Certification Framework for Data Poisoning, Unlearning, and Differential Privacy](https://arxiv.org/abs/2511.09400)
*Philip Sosnin,Matthew Wicker,Josh Collyer,Calvin Tsay*

Main category: cs.LG

TL;DR: 提出 Abstract Gradient Training (AGT)：一个统一框架，用于在训练数据扰动（有界扰动、数据点 removal、添加新样本）下对模型及其训练过程进行鲁棒性认证，通过界定可达到的参数集合来给出参数空间的证明界限。


<details>
  <summary>Details</summary>
Motivation: 尽管推理阶段的对抗扰动已被广泛研究，但训练阶段的数据扰动（数据投毒、数据删除/机器学习去训练、以及与差分隐私相关的替换）尚缺乏系统的可证性分析，需要一个统一框架来对这些扰动下的模型行为进行理论认证。

Method: 引入 Abstract Gradient Training (AGT) 作为统一框架，结合一阶优化方法，在参数的可达集合上建立界限，覆盖有界扰动、数据点删除、添加新样本等场景，给出可证明的参数空间界限，以实现对训练过程鲁棒性的认证。

Result: 提供理论层面的参数空间界限与鲁棒性证明框架，能够在训练阶段对训练数据扰动进行可证性分析，适用于通过一阶优化方法训练的模型。

Conclusion: AGT 为认证训练数据扰动鲁棒性提供了形式化工具，并为在对抗性数据扰动下的稳健训练设计提供理论基础。

Abstract: The impact of inference-time data perturbation (e.g., adversarial attacks) has been extensively studied in machine learning, leading to well-established certification techniques for adversarial robustness. In contrast, certifying models against training data perturbations remains a relatively under-explored area. These perturbations can arise in three critical contexts: adversarial data poisoning, where an adversary manipulates training samples to corrupt model performance; machine unlearning, which requires certifying model behavior under the removal of specific training data; and differential privacy, where guarantees must be given with respect to substituting individual data points. This work introduces Abstract Gradient Training (AGT), a unified framework for certifying robustness of a given model and training procedure to training data perturbations, including bounded perturbations, the removal of data points, and the addition of new samples. By bounding the reachable set of parameters, i.e., establishing provable parameter-space bounds, AGT provides a formal approach to analyzing the behavior of models trained via first-order optimization methods.

</details>


### [106] [Spatio-Temporal Graph Unlearning](https://arxiv.org/abs/2511.09404)
*Qiming Guo,Wenbo Sun,Wenlu Wang*

Main category: cs.LG

TL;DR: 提出 CallosumNet：一个面向时空图的完全消除(unlearning)框架，采用ECS和GGB的分治策略，通过局部子图重构全球依赖，实现对单节点数据的高效“删除”且性能损失极小（1-2% 相对MAE）并优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在GDPR/CCPA等隐私法规约束下，需对被授权或删除请求的数据进行完全消除。时空图因信息在时空维度上的扩散特性，使得仅依赖静态图的局部去除难以达到高效且完全的消除，因此需要面向时空图的高效消除方法。

Method: 提出 CallosumNet，受脑部胼胝体结构启发，采用两大技术：1) Enhanced Subgraph Construction (ESC) 动态构建多个局部子图，基于多因素（包括类脑的虚拟神经节）确定子图边界；2) Global Ganglion Bridging (GGB) 从这些局部子图中重建全局时空依赖，恢复完整图表示。整体采用分治策略实现完全消除并尽量减少性能损失。

Result: 在四个真实数据集上的实验表明，CallosumNet 能实现完全消除，仅在相对 MAE 上与金标准模型下降 1%-2%，显著优于现有基线。消融研究验证了两种新技术的有效性。

Conclusion: 提出了一种可扩展且高效的时空图完全消除框架，利用 ESC 和 GGB 实现分治-重建策略，在隐私合规场景下对单节点数据进行高效清除且代价较低。

Abstract: Spatio-temporal graphs are widely used in modeling complex dynamic processes such as traffic forecasting, molecular dynamics, and healthcare monitoring. Recently, stringent privacy regulations such as GDPR and CCPA have introduced significant new challenges for existing spatio-temporal graph models, requiring complete unlearning of unauthorized data. Since each node in a spatio-temporal graph diffuses information globally across both spatial and temporal dimensions, existing unlearning methods primarily designed for static graphs and localized data removal cannot efficiently erase a single node without incurring costs nearly equivalent to full model retraining. Therefore, an effective approach for complete spatio-temporal graph unlearning is a pressing need. To address this, we propose CallosumNet, a divide-and-conquer spatio-temporal graph unlearning framework inspired by the corpus callosum structure that facilitates communication between the brain's two hemispheres. CallosumNet incorporates two novel techniques: (1) Enhanced Subgraph Construction (ESC), which adaptively constructs multiple localized subgraphs based on several factors, including biologically-inspired virtual ganglions; and (2) Global Ganglion Bridging (GGB), which reconstructs global spatio-temporal dependencies from these localized subgraphs, effectively restoring the full graph representation. Empirical results on four diverse real-world datasets show that CallosumNet achieves complete unlearning with only 1%-2% relative MAE loss compared to the gold model, significantly outperforming state-of-the-art baselines. Ablation studies verify the effectiveness of both proposed techniques.

</details>


### [107] [Probing then Editing: A Push-Pull Framework for Retain-Free Machine Unlearning in Industrial IoT](https://arxiv.org/abs/2511.09414)
*Jiao Chen,Weihua Li,Jianhua Tang*

Main category: cs.LG

TL;DR: 提出了一种无保留学习的知识清除框架PTE（Probing then Editing），通过探测目标类别的决策边界并据此编辑，再通过推送-拉取的协同优化实现对该类别的忘记，同时以被保留的类别保持原有知识，且仅使用待忘记数据和原模型即可实现有效且平衡的忘记与模型保持。


<details>
  <summary>Details</summary>
Motivation: 在动态IIoT环境中，模型需具备有选择性忘记过时或错误知识的能力；现有方法往往依赖保留数据来约束模型行为，带来计算与能耗负担、数据孤岛与隐私合规等挑战。

Method: 提出Probing then Editing (PTE)框架，将忘记视为探测-编辑过程：首先通过梯度上升探测待忘记类别在模型决策边界的邻域，并利用模型自身预测生成对应的编辑指令；随后进行Push-Pull协同优化：Push分支利用编辑指令主动瓦解目标类别的决策区域；Pull分支通过掩码知识蒸馏，将保留类别的知识锚定回原始状态。该机制仅依赖待忘记数据与原模型进行知识编辑。

Result: PTE在多种通用与工业基准（如CWRU、SCUT-FD）上实现了忘记效果与模型效用之间的良好平衡，表现出高效且可控的知识编辑能力。

Conclusion: PTE提供了一种 retain-free 的学习忘记新范式，通过探测-编辑与推拉协同，使得在不保留原始数据的前提下对指定类别进行高效且受控的忘记，同时尽量保持对其他类别的原有知识。

Abstract: In dynamic Industrial Internet of Things (IIoT) environments, models need the ability to selectively forget outdated or erroneous knowledge. However, existing methods typically rely on retain data to constrain model behavior, which increases computational and energy burdens and conflicts with industrial data silos and privacy compliance requirements. To address this, we propose a novel retain-free unlearning framework, referred to as Probing then Editing (PTE). PTE frames unlearning as a probe-edit process: first, it probes the decision boundary neighborhood of the model on the to-be-forgotten class via gradient ascent and generates corresponding editing instructions using the model's own predictions. Subsequently, a push-pull collaborative optimization is performed: the push branch actively dismantles the decision region of the target class using the editing instructions, while the pull branch applies masked knowledge distillation to anchor the model's knowledge on retained classes to their original states. Benefiting from this mechanism, PTE achieves efficient and balanced knowledge editing using only the to-be-forgotten data and the original model. Experimental results demonstrate that PTE achieves an excellent balance between unlearning effectiveness and model utility across multiple general and industrial benchmarks such as CWRU and SCUT-FD.

</details>


### [108] [Transformer Semantic Genetic Programming for d-dimensional Symbolic Regression Problems](https://arxiv.org/abs/2511.09416)
*Philipp Anthes,Dominik Sobania,Franz Rothlauf*

Main category: cs.LG

TL;DR: TSGP 使用预训练的 Transformer 作为变异算子，通过语义相似性的控制来生成后代表达式，能够在不同维度的符号回归问题上泛化。相较于标准 GP 等方法，TSGP 在24个数据集上显著提升平均排名，且解的紧凑性更高。通过目标语义距离 SD_t 来控制语义空间步长，平衡探索与开发。


<details>
  <summary>Details</summary>
Motivation: 解决现有语义遗传程序设计中对固定合成变换的依赖问题，希望通过从大规模程序数据中学习得到的语义变异实现更丰富且有效的结构变异，以提升搜索效率和解的质量。

Method: 在数百万程序上训练一个 Transformer，使其作为变异算子对父程序进行变换，生成语义相似的后代；对符号回归问题进行评估，跨24个真实和合成数据集，与GP、SLIM_GSGP、Deep Symbolic Regression、Denoising Autoencoder GP等方法比较；分析目标语义距离 SD_t 对搜索过程的控制作用。

Result: 在大多数基准上表现优于对比方法，平均排名为1.58；生成的解比 SLIM_GSGP 更紧凑且在精度方面也更高；Transformer 能在不同维度的符号回归问题上泛化。

Conclusion: SD_t 能有效控制语义空间中的步长，实现对探索与利用的平衡；单个预训练 Transformer 就能在跨问题域的情形下提供有效的语义变异操作，显示出基于学习的变异在符号回归的潜力与实用性。

Abstract: Transformer Semantic Genetic Programming (TSGP) is a semantic search approach that uses a pre-trained transformer model as a variation operator to generate offspring programs with controlled semantic similarity to a given parent. Unlike other semantic GP approaches that rely on fixed syntactic transformations, TSGP aims to learn diverse structural variations that lead to solutions with similar semantics. We find that a single transformer model trained on millions of programs is able to generalize across symbolic regression problems of varying dimension. Evaluated on 24 real-world and synthetic datasets, TSGP significantly outperforms standard GP, SLIM_GSGP, Deep Symbolic Regression, and Denoising Autoencoder GP, achieving an average rank of 1.58 across all benchmarks. Moreover, TSGP produces more compact solutions than SLIM_GSGP, despite its higher accuracy. In addition, the target semantic distance $\mathrm{SD}_t$ is able to control the step size in the semantic space: small values of $\mathrm{SD}_t$ enable consistent improvement in fitness but often lead to larger programs, while larger values promote faster convergence and compactness. Thus, $\mathrm{SD}_t$ provides an effective mechanism for balancing exploration and exploitation.

</details>


### [109] [Group Equivariance Meets Mechanistic Interpretability: Equivariant Sparse Autoencoders](https://arxiv.org/abs/2511.09432)
*Ege Erdogan,Ana Lucic*

Main category: cs.LG

TL;DR: 将稀疏自编码器（SAE）与群对称性结合起来以提高可解释性和下游任务表现；提出自适应等变SAE，能够匹配基础模型的等变性水平，并在对表征的可解释性与探测任务性能方面优于常规SAE。


<details>
  <summary>Details</summary>
Motivation: 旨在将SAE从语言领域扩展到具备对称性（如旋转）的科学数据领域，以提升可解释性和对下游任务的帮助，同时解决忽略对称性结构导致的效果不足的问题。

Method: 在合成图像上训练SAE，发现一个单一矩阵即可解释图像旋转时激活的变换；在此基础上提出自适应等变SAE，使其能够根据基模型的等变性水平进行自适应；评估探测任务中的特征可用性与性能。

Result: 自适应等变SAE发现的特征在探测任务上优于常规SAE，且用以解释激活随旋转的变化的矩阵具有简洁性；将对称性纳入机制性可解释性工具可提升下游任务的表现。

Conclusion: 将对称性整合进机制性可解释性工具具有价值，通过自适应等变SAE可以更好地发现特征并提升下游性能，拓展了SAE在非语言领域的应用。

Abstract: Sparse autoencoders (SAEs) have proven useful in disentangling the opaque activations of neural networks, primarily large language models, into sets of interpretable features. However, adapting them to domains beyond language, such as scientific data with group symmetries, introduces challenges that can hinder their effectiveness. We show that incorporating such group symmetries into the SAEs yields features more useful in downstream tasks. More specifically, we train autoencoders on synthetic images and find that a single matrix can explain how their activations transform as the images are rotated. Building on this, we develop adaptively equivariant SAEs that can adapt to the base model's level of equivariance. These adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs, demonstrating the value of incorporating symmetries in mechanistic interpretability tools.

</details>


### [110] [LLM-Guided Dynamic-UMAP for Personalized Federated Graph Learning](https://arxiv.org/abs/2511.09438)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Tanzim Ahad,Sajedul Talukder*

Main category: cs.LG

TL;DR: LLM-assisted graph ML with personalization and privacy: data augmentation for sparse graphs, prompt/instruction tuning, and in-context learning to derive few-shot graph reasoning signals; these signals define a Dynamic UMAP manifold of client embeddings used in a Bayesian variational objective for personalized federated learning, enabling node classification and link prediction in low-resource settings with a cross-modal regularizer aligning LLM latent space to graph structure.


<details>
  <summary>Details</summary>
Motivation: 在数据稀疏与隐私/个性化需求并存的场景下，利用大型语言模型提升图学习的能力，尤其在节点分类、链路预测等任务中，结合联邦学习和跨模态对齐以提高性能与隐私保护。

Method: 1) 针对稀疏图的数据增强；2) 使用提示工程/指令微调（prompt/instruction tuning）使基础模型适应图任务；3) 通过在-context learning 提供少样本图推理信号；4) 将这些信号参数化为客户端特异嵌入的 Dynamic UMAP 流形，在贝叶斯变分目标中实现个性化联邦学习；5) 通过跨模态正则化器将语言模型潜在表示与图结构对齐；6) 提出对变分聚合过程的收敛性论证与基于矩量账户的差分隐私威胁模型；7) 应用场景包括知识图完成、类推荐的链路预测、以及论文/产品图等。

Result: 摘要主要提出一个理论性与方法论并重的框架，强调框架的可实现性及潜在应用，但未给出具体实验结果或量化评估细节。

Conclusion: 该工作提出一个统一的、以 LLM 为驱动的图学习框架，结合个性化与隐私保护，包含收敛性分析、DP 威胁建模与多场景应用，强调对基准评测的仔细设计与未来的实验验证需求。

Abstract: We propose a method that uses large language models to assist graph machine learning under personalization and privacy constraints. The approach combines data augmentation for sparse graphs, prompt and instruction tuning to adapt foundation models to graph tasks, and in-context learning to supply few-shot graph reasoning signals. These signals parameterize a Dynamic UMAP manifold of client-specific graph embeddings inside a Bayesian variational objective for personalized federated learning. The method supports node classification and link prediction in low-resource settings and aligns language model latent representations with graph structure via a cross-modal regularizer. We outline a convergence argument for the variational aggregation procedure, describe a differential privacy threat model based on a moments accountant, and present applications to knowledge graph completion, recommendation-style link prediction, and citation and product graphs. We also discuss evaluation considerations for benchmarking LLM-assisted graph machine learning.

</details>


### [111] [How does the Performance of the Data-driven Traffic Flow Forecasting Models deteriorate with Increasing Forecasting Horizon? An Extensive Approach Considering Statistical, Machine Learning and Deep Learning Models](https://arxiv.org/abs/2511.09450)
*Amanta Sherfenaz,Nazmul Haque,Protiva Sadhukhan Prova,Md Asif Raihan,Md. Hadiuzzaman*

Main category: cs.LG

TL;DR: 本研究对比统计、机器学习与深度学习模型在交通速度与交通流预测上的性能，基于加州 Harbor Freeway 的 PeMS 数据。结果显示：ANFIS-GP 在短时预测表现最佳；Bi-LSTM 在中期预测更具鲁棒性；通过对数变换量化模型衰减，Bi-LSTM 拥有最平滑的斜率，提示未来可探索混合模型。


<details>
  <summary>Details</summary>
Motivation: 随着快速城市化，交通拥堵加剧， ITS 需要高效的交通预测来支撑诸如匝道控制、信号控制与动态路径规划等管理策略。

Method: 比较统计、机器学习与深度学习三类模型；在 20 个预测窗口内（最长 1 小时 40 分钟）对交通速度与流量进行预测；采用 RMSE、MAE、R^2 等指标评估；通过对数变换量化预测时间延长带来的性能衰减；数据源为 Caltrans PeMS 的 Harbor Freeway。

Result: 在早期预测窗口中，ANFIS-GP 表现最佳，RMSE 0.038、MAE 0.0276、R^2 0.9983；Bi-LSTM 在中期预测更具鲁棒性，在 predicting 20 时刻的 RMSE 0.1863、MAE 0.0833、R^2 0.987。对数变换的斜率用于衡量鲁棒性，Bi-LSTM 斜率最小（0.0454 的 RMSE、0.0545 的 MAE（流量）），而 ANFIS-GP 的斜率分别为 0.1058（RMSE）和 0.1037（流量 MAE）。总体结果提示混合模型具有较大潜力。

Conclusion: 作者认为混合模型是未来的有前景方向。

Abstract: With rapid urbanization in recent decades, traffic congestion has intensified due to increased movement of people and goods. As planning shifts from demand-based to supply-oriented strategies, Intelligent Transportation Systems (ITS) have become essential for managing traffic within existing infrastructure. A core ITS function is traffic forecasting, enabling proactive measures like ramp metering, signal control, and dynamic routing through platforms such as Google Maps. This study assesses the performance of statistical, machine learning (ML), and deep learning (DL) models in forecasting traffic speed and flow using real-world data from California's Harbor Freeway, sourced from the Caltrans Performance Measurement System (PeMS). Each model was evaluated over 20 forecasting windows (up to 1 hour 40 minutes) using RMSE, MAE, and R-Square metrics. Results show ANFIS-GP performs best at early windows with RMSE of 0.038, MAE of 0.0276, and R-Square of 0.9983, while Bi-LSTM is more robust for medium-term prediction due to its capacity to model long-range temporal dependencies, achieving RMSE of 0.1863, MAE of 0.0833, and R-Square of 0.987 at a forecasting of 20. The degradation in model performance was quantified using logarithmic transformation, with slope values used to measure robustness. Among DL models, Bi-LSTM had the flattest slope (0.0454 RMSE, 0.0545 MAE for flow), whereas ANFIS-GP had 0.1058 for RMSE and 0.1037 for flow MAE. The study concludes by identifying hybrid models as a promising future direction.

</details>


### [112] [Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach](https://arxiv.org/abs/2511.09475)
*Anli Ji,Pranjal Patil,Chetraj Pandey,Manolis K. Georgoulis,Berkay Aydin*

Main category: cs.LG

TL;DR: 提出一个数据驱动的日面高能粒子（SEP）预测框架，结合全局解释性与按需特征映射以提升模型透明度，且对341个SEP事件（包括244个≥10 MeV阳质子事件，超越Space Weather Prediction Center S1阈值）在太阳循环22-24年的数据集进行了验证，并通过一个以解释性为焦点的案例研究展示了物理学驱动的理解的增强。


<details>
  <summary>Details</summary>
Motivation: 现有的以数据驱动的SEP预测方法多为黑箱模型，难以解释结果的物理原因。需要可解释性强、便于太阳物理学家理解的预测框架，以便不仅获得预测结果，也理解驱动机制。

Method: 提出一个将全局解释方法与按需特征映射相结合的框架，用以提升预测模型的透明度。数据集包含341组SEP事件（其中244组≥10 MeV的质子事件，超过S1阈值），覆盖太阳循环22-24年。并辅以一个解释性导向的重大SEP事件案例研究，展示该方法在解释性和物理意义层面的提升。

Result: 该框架实现了对SEP预测的可解释性增强，并通过案例研究展示了在物理驱动理解方面的改进。对341个事件的数据集验证表明方法具有可操作性，并能提供比传统黑箱模型更具物理可解释性的洞察。

Conclusion: 将全局解释与按需特征映射结合的SEP预测框架，有望提高预测的物理可解释性，帮助太阳物理学家从机制层面理解事件发生原因，并推进SEP预测的可解释性研究。

Abstract: Solar energetic particle (SEP) events, as one of the most prominent manifestations of solar activity, can generate severe hazardous radiation when accelerated by solar flares or shock waves formed aside from coronal mass ejections (CMEs). However, most existing data-driven methods used for SEP predictions are operated as black-box models, making it challenging for solar physicists to interpret the results and understand the underlying physical causes of such events rather than just obtain a prediction. To address this challenge, we propose a novel framework that integrates global explanations and ad-hoc feature mapping to enhance model transparency and provide deeper insights into the decision-making process. We validate our approach using a dataset of 341 SEP events, including 244 significant (>=10 MeV) proton events exceeding the Space Weather Prediction Center S1 threshold, spanning solar cycles 22, 23, and 24. Furthermore, we present an explainability-focused case study of major SEP events, demonstrating how our method improves explainability and facilitates a more physics-informed understanding of SEP event prediction.

</details>


### [113] [Latent Planning via Embedding Arithmetic: A Contrastive Approach to Strategic Reasoning](https://arxiv.org/abs/2511.09477)
*Andrew Hamara,Greg Hamerly,Pablo Rivas,Andrew C. Freeman*

Main category: cs.LG

TL;DR: SOLIS 引入一个评估对齐的潜在空间，通过监督对比学习在高维决策空间中进行规划；通过一个全局优势向量来指引方向，在潜在空间中将动作排序，使用浅层搜索实现棋类任务，作为对传统动力学模型或策略学习的轻量替代。


<details>
  <summary>Details</summary>
Motivation: 在高维决策空间中直接在评估对齐的嵌入空间进行规划，避免或减少对显式动力学模型/策略头的依赖，提升规划效率与可解释性。

Method: 用监督对比学习学习嵌入空间，使结果相似性以距离表示；引入一个全局优势向量，表示从失利到胜利的方向；对候选动作进行投影/对齐评分，下降到向该方向的对齐程度，规划等同于向量运算；在棋类任务中用浅层搜索在该嵌入中实现竞争力。

Result: 在棋类任务中，SOLIS 仅使用浅层搜索结合学习的嵌入就达到有竞争力的强度，在受限条件下表现良好；结果表明评估对齐的潜在规划可作为对传统动力学模型或策略学习的轻量替代。

Conclusion: 评估对齐的潜在规划提供了一种轻量替代传统动力学/策略学习的方法，具有潜在的广泛适用性和对高维决策空间的规划效率提升。

Abstract: Planning in high-dimensional decision spaces is increasingly being studied through the lens of learned representations. Rather than training policies or value heads, we investigate whether planning can be carried out directly in an evaluation-aligned embedding space. We introduce SOLIS, which learns such a space using supervised contrastive learning. In this representation, outcome similarity is captured by proximity, and a single global advantage vector orients the space from losing to winning regions. Candidate actions are then ranked according to their alignment with this direction, reducing planning to vector operations in latent space. We demonstrate this approach in chess, where SOLIS uses only a shallow search guided by the learned embedding to reach competitive strength under constrained conditions. More broadly, our results suggest that evaluation-aligned latent planning offers a lightweight alternative to traditional dynamics models or policy learning.

</details>


### [114] [AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting](https://arxiv.org/abs/2511.09478)
*Renda Li,Hailang Huang,Fei Wei,Feng Xiong,Yong Wang,Xiangxiang Chu*

Main category: cs.LG

TL;DR: 提出 AdaCuRL，自适应难度与课程调度的强化学习框架，结合数据重访和稀疏 KL，缓解梯度饥錀、策略退化与遗忘，显著提升LLMs/MLLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有 RL-LLM 训练在混合难度样本上容易出现梯度 Starvation、策略退化等问题；CoT 注释昂贵；课程学习需要繁琐设计且易忘记。需要一个自适应、低人工成本的课程策略来提升推理能力。

Method: AdaCuRL 核心包括：1) 粗到细难度自适应估计；2) 自适应课程调度，动态匹配模型能力与数据难度；3) 数据重访机制，防止灾难性遗忘；4) 自适应参考和稀疏 KL 策略，防止策略退化。并在多种推理基准上验证。

Result: 在多样化推理基准上，对LLMs和MLLMs均实现显著性能提升。

Conclusion: AdaCuRL 提供了一个可扩展的自适应课程强化学习框架，缓解现有不足，具有良好推广性和鲁棒性。

Abstract: Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.

</details>


### [115] [PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness](https://arxiv.org/abs/2511.09487)
*Junqi Gao,Zhichang Guo,Dazhi Zhang,Yao Li,Yi Ran,Biqing Qi*

Main category: cs.LG

TL;DR: 提出PDAC：基于投影高斯混合的密度感知核心集合方法，结合流式EM实现SPDAC，在持续学习中以高概率密度样本优先构造记忆缓冲，提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 回放式持续学习依赖于缓冲样本的质量，而现有核心集合方法通常为双层最优化，代价高且难以扩展。需要一个更高效、适应流数据的样本选择方案。

Method: 构建投影高斯混合模型来估计每个样本的联合密度，进行密度优先的缓冲区选择；提出流式EM算法以在线估计PGM参数，形成SPDAC。

Result: 在多种CL设置下相较基线具有更优的性能和效率，尤其在计算成本与内存利用方面表现更佳。

Conclusion: 密度导向的样本选择及其流式学习机制为回放式CL提供了更高效的缓冲策略，适用于大规模和流数据场景。

Abstract: Rehearsal-based Continual Learning (CL) maintains a limited memory buffer to store replay samples for knowledge retention, making these approaches heavily reliant on the quality of the stored samples. Current Rehearsal-based CL methods typically construct the memory buffer by selecting a representative subset (referred to as coresets), aiming to approximate the training efficacy of the full dataset with minimal storage overhead. However, mainstream Coreset Selection (CS) methods generally formulate the CS problem as a bi-level optimization problem that relies on numerous inner and outer iterations to solve, leading to substantial computational cost thus limiting their practical efficiency. In this paper, we aim to provide a more efficient selection logic and scheme for coreset construction. To this end, we first analyze the Mean Squared Error (MSE) between the buffer-trained model and the Bayes-optimal model through the perspective of localized error decomposition to investigate the contribution of samples from different regions to MSE suppression. Further theoretical and experimental analyses demonstrate that samples with high probability density play a dominant role in error suppression. Inspired by this, we propose the Probability Density-Aware Coreset (PDAC) method. PDAC leverages the Projected Gaussian Mixture (PGM) model to estimate each sample's joint density, enabling efficient density-prioritized buffer selection. Finally, we introduce the streaming Expectation Maximization (EM) algorithm to enhance the adaptability of PGM parameters to streaming data, yielding Streaming PDAC (SPDAC) for streaming scenarios. Extensive comparative experiments show that our methods outperforms other baselines across various CL settings while ensuring favorable efficiency.

</details>


### [116] [AutoSynth: Automated Workflow Optimization for High-Quality Synthetic Dataset Generation via Monte Carlo Tree Search](https://arxiv.org/abs/2511.09488)
*Shuzhen Bi,Chang Song,Siyu Song,Jinze Lv,Jian Chen,Xinyun Wang,Aimin Zhou,Hao Hao*

Main category: cs.LG

TL;DR: AutoSynth自动化发现和优化数据生成工作流，用于无参考数据的LLM微调任务，通过基于蒙特卡洛树搜索的策略在数据集自由条件下进行工作流和数据生成的联合优化。


<details>
  <summary>Details</summary>
Motivation: 翻译成中文：高质量数据集是SFT的关键，但手工标注成本高且难以扩展；对主观、开放性任务，缺乏客观地面真相，传统的奖励建模方法存在“冷启动”问题，因此需要无需参考数据的自动化方法来推动数据生成与工作流优化。

Method: 将问题重塑为蒙特卡洛树搜索（MCTS）问题，在数据集自由（dataset-free）的情境下通过一种新颖的混合奖励来引导搜索。引入两种LLM作为评审组件：一份评估样本质量（基于动态生成的任务特定指标），另一份评估工作流代码和提示质量。通过这两种 judge 进行元学习，以实现工作流和数据的联合优化与筛选。

Result: 在主观教育任务上，专家设计的工作流在人工偏好测试中占优（胜率约96-99%），而AutoSynth的工作流仅获得40-51%的胜率；但基于AutoSynth生成的数据训练的模型明显优于基线（相对胜率提升到40-51%对比2-5%），且在某些指标上可达到甚至超过专家工作流的表现；同时人工投入从5-7小时降到约30分钟，提升超过90%的效率。

Conclusion: AutoSynth解决了数据中心化AI中的冷启动问题，为主观性LLM任务提供 scalable、成本效益高的工作流发现与数据生成方法。给出了不开启人工标注的情况下仍能实现有效SFT数据生成的路径，并且揭示了部分品质维度可能超越人类直觉的发现。

Abstract: Supervised fine-tuning (SFT) of large language models (LLMs) for specialized tasks requires high-quality datasets, but manual curation is prohibitively expensive. Synthetic data generation offers scalability, but its effectiveness relies on complex, multi-stage workflows, integrating prompt engineering and model orchestration. Existing automated workflow methods face a cold start problem: they require labeled datasets for reward modeling, which is especially problematic for subjective, open-ended tasks with no objective ground truth. We introduce AutoSynth, a framework that automates workflow discovery and optimization without reference datasets by reframing the problem as a Monte Carlo Tree Search guided by a novel dataset-free hybrid reward. This reward enables meta-learning through two LLM-as-judge components: one evaluates sample quality using dynamically generated task-specific metrics, and another assesses workflow code and prompt quality. Experiments on subjective educational tasks show that while expert-designed workflows achieve higher human preference rates (96-99% win rates vs. AutoSynth's 40-51%), models trained on AutoSynth-generated data dramatically outperform baselines (40-51% vs. 2-5%) and match or surpass expert workflows on certain metrics, suggesting discovery of quality dimensions beyond human intuition. These results are achieved while reducing human effort from 5-7 hours to just 30 minutes (>90% reduction). AutoSynth tackles the cold start issue in data-centric AI, offering a scalable, cost-effective method for subjective LLM tasks. Code: https://github.com/bisz9918-maker/AutoSynth.

</details>


### [117] [GenePheno: Interpretable Gene Knockout-Induced Phenotype Abnormality Prediction from Gene Sequences](https://arxiv.org/abs/2511.09512)
*Jingquan Yan,Yuwei Miao,Lei Yu,Yuzhi Guo,Xue Xiao,Lin Xu,Junzhou Huang*

Main category: cs.LG

TL;DR: 提出 GenePheno，一种可解释的多标签预测框架，可从基因序列预测基因敲除引发的表型异常，具可解释性和竞争性性能，提供四个数据集以促进研究。


<details>
  <summary>Details</summary>
Motivation: 解决序列与表型之间的模态差距、基因-表型关系的多效性和现有方法对先验知识依赖带来的可扩展性/泛化性局限，推动从序列直接预测敲除表型异常的可行性。

Method: 提出对比式多标签学习目标以捕捉表型之间的相关性，额外的专属正则化以保证生物学一致性，加入基因功能瓶颈层以提供可解释的功能概念；基于四个数据集进行评估。

Result: 在四个数据集上，GenePheno达到先进的基因中心 Fmax 与表型中心 AUC 指标，且案例研究显示其揭示基因功能机制的能力。

Conclusion: 首次提出可解释的多标签框架用于预测基因敲除引发的表型异常，配套数据集以推动领域研究，具备良好的解释性与竞争性表现。

Abstract: Exploring how genetic sequences shape phenotypes is a fundamental challenge in biology and a key step toward scalable, hypothesis-driven experimentation. The task is complicated by the large modality gap between sequences and phenotypes, as well as the pleiotropic nature of gene-phenotype relationships. Existing sequence-based efforts focus on the degree to which variants of specific genes alter a limited set of phenotypes, while general gene knockout induced phenotype abnormality prediction methods heavily rely on curated genetic information as inputs, which limits scalability and generalizability. As a result, the task of broadly predicting the presence of multiple phenotype abnormalities under gene knockout directly from gene sequences remains underexplored. We introduce GenePheno, the first interpretable multi-label prediction framework that predicts knockout induced phenotypic abnormalities from gene sequences. GenePheno employs a contrastive multi-label learning objective that captures inter-phenotype correlations, complemented by an exclusive regularization that enforces biological consistency. It further incorporates a gene function bottleneck layer, offering human interpretable concepts that reflect functional mechanisms behind phenotype formation. To support progress in this area, we curate four datasets with canonical gene sequences as input and multi-label phenotypic abnormalities induced by gene knockouts as targets. Across these datasets, GenePheno achieves state-of-the-art gene-centric Fmax and phenotype-centric AUC, and case studies demonstrate its ability to reveal gene functional mechanisms.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [118] [Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments](https://arxiv.org/abs/2511.08851)
*Po-Heng Chou,Da-Chih Lin,Hung-Yu Wei,Walid Saad,Yu Tsao*

Main category: cs.NI

TL;DR: 提出一个基于测量驱动框架的早期RLF预测，在5G NSA铁路场景中，利用10 Hz地铁轨迹数据，对六种模型在不同观察窗口和预测时长下进行基准评估。 TimesNet在3秒观察窗口+3秒预测时长达到最高F1，CNN在2秒预测时长下具有更佳的准确性与延迟权衡，表明深度时序模型可利用商用设备的轻量特征提前数秒预测可靠性下降，为5G铁路系统提供早期告警路径。


<details>
  <summary>Details</summary>
Motivation: 在5G铁路场景中提升对无线链路中断（RLF）的预测能力，从而实现提前的冗余与自适应切换等控制策略。

Method: 基于10 Hz metro-train traces，使用服务小区与邻区指标，对CNN、LSTM、XGBoost、Anomaly Transformer、PatchTST、TimesNet六种模型，在不同观测窗口（如2s、3s等）和预测时长（如2s、3s等）下进行基准评估。

Result: 观测窗口为3秒时，TimesNet在3秒预测时长获得最高F1。CNN在2秒预测时长下提供较优的准确性-延迟权衡。多模型结果显示，利用商业设备的轻量特征，深度时序模型可在数秒前预测出可靠性下降，为早期预警控制提供可行路径。

Conclusion: 该测量驱动框架为在5G基础铁路系统中实现早期告警控制提供了一个实用的路径，强调深度时序模型在可部署性与对轻量特征的有效利用方面的潜力。

Abstract: In this paper, a measurement-driven framework is proposed for early radio link failure (RLF) prediction in 5G non-standalone (NSA) railway environments. Using 10 Hz metro-train traces with serving and neighbor-cell indicators, we benchmark six models, namely CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under varied observation windows and prediction horizons. When the observation window is three seconds, TimesNet attains the highest F1 score with a three-second prediction horizon, while CNN provides a favorable accuracy-latency tradeoff with a two-second horizon, enabling proactive actions such as redundancy and adaptive handovers. The results indicate that deep temporal models can anticipate reliability degradations several seconds in advance using lightweight features available on commercial devices, offering a practical path to early-warning control in 5G-based railway systems.

</details>


### [119] [Hierarchical Reinforcement Learning for Integrated Cloud-Fog-Edge Computing in IoT Systems](https://arxiv.org/abs/2511.09006)
*Ameneh Zarei,Mahmood Ahmadi,Farhad Mardukhi*

Main category: cs.NI

TL;DR: 提出Hierarchical IoT Processing Architecture (HIPA)，在云、雾、边缘三层动态分配计算任务，通过机器学习优化以提升IoT系统的延迟、可扩展性与数据隐私。


<details>
  <summary>Details</summary>
Motivation: 解决物联网场景中的海量数据和实时性需求对传统云计算架构的挑战，促进端到端的高效与安全协同。

Method: 提出基于机器学习的分层任务分配框架HIPA，结合云、雾、边缘计算的层级结构，动态调度计算、存储与通信资源，并在文献综述基础上进行综合分析。

Result: 通过层级协同，理论上可显著降低端到端延迟，提升系统扩展性和数据隐私保护能力，构建更高效、安全的IoT生态系统。

Conclusion: 云–雾–边缘的协同是实现高效IoT的关键，HIPA提供了一种可执行的任务分配架构，并指明了未来研究方向与应用前景。

Abstract: The Internet of Things (IoT) is transforming industries by connecting billions of devices to collect, process, and share data. However, the massive data volumes and real-time demands of IoT applications strain traditional cloud computing architectures. This paper explores the complementary roles of cloud, fog, and edge computing in enhancing IoT performance, focusing on their ability to reduce latency, improve scalability, and ensure data privacy. We propose a novel framework, the Hierarchical IoT Processing Architecture (HIPA), which dynamically allocates computational tasks across cloud, fog, and edge layers using machine learning. By synthesizing current research and introducing HIPA, this paper highlights how these paradigms can create efficient, secure, and scalable IoT ecosystems.

</details>


### [120] [Experimenting with Energy-Awareness in Edge-Cloud Containerized Application Orchestration](https://arxiv.org/abs/2511.09116)
*Dalal Ali,Rute C. Sofia*

Main category: cs.NI

TL;DR: Energy-aware scheduling for edge-cloud deployments reduces energy consumption by injecting energy metrics into scheduling decisions; validated on ARM-based testbed showing energy efficiency gains over standard Kubernetes, especially under high load.


<details>
  <summary>Details</summary>
Motivation: Mitigate the growing energy footprint of heterogeneous edge-cloud infrastructures by integrating energy metrics into resource management and orchestration.

Method: Proposes methods to inject energy metrics at both computation and network levels into existing scheduling approaches; conducts experimental evaluation on a real-world ARM-based testbed; compares against standard Kubernetes scheduling.

Result: Demonstrates consistent improvements in energy efficiency, particularly under high-load scenarios, indicating energy-awareness can improve workload distribution and resource allocation.

Conclusion: Incorporating energy-awareness into orchestration processes can lead to more sustainable cloud-native computing in heterogeneous edge-cloud environments.

Abstract: This paper explores the role of energy-awareness strategies into the deployment of applications across heterogeneous Edge-Cloud infrastructures. It proposes methods to inject into existing scheduling approaches energy metrics at a computational and network level, to optimize resource allocation and reduce energy consumption. The pro- posed approach is experimentally evaluated using a real-world testbed based on ARM devices, comparing energy consumption and workload distribution against standard Kubernetes scheduling. Results demon- strate consistent improvements in energy efficiency, particularly under high-load scenarios, highlighting the potential of incorporating energy- awareness into orchestration processes for more sustainable cloud- native computing.

</details>
