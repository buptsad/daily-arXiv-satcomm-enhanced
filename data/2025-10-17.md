<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 10]
- [eess.SP](#eess.SP) [Total: 7]
- [cs.LG](#cs.LG) [Total: 68]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 16]
- [eess.SY](#eess.SY) [Total: 7]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Information flow in multilayer perceptrons: an in-depth analysis](https://arxiv.org/abs/2510.13846)
*Giuliano Armano*

Main category: cs.IT

TL;DR: 本论文将信息理论用于分析多层感知机的信息流，提出信息矩阵作为分析框架，并将其与信息瓶颈（IB）框架的优化策略进行对比，认为 MLP 像一个适配器，在实现学习目标的同时处理输入信息。


<details>
  <summary>Details</summary>
Motivation: 揭示在监督学习条件下，信息在 MLP 各层的流动机制，建立一个统一的分析工具以理解优化策略的起源与信息处理过程。

Method: 提出信息矩阵概念作为正式框架，分析并定义一个参数化的优化策略，比较该策略与信息瓶颈框架的优化路径，研究 MLP 如何作为将输入映射以实现目标的适配器。

Result: i) 定义了一个参数化优化策略；ii) 发现信息矩阵框架的优化策略与信息瓶颈框架的策略存在显著相似之处；iii) 指出多层感知机可视为一个“适配器”，用于按给定目标处理输入。

Conclusion: 信息矩阵与信息瓶颈框架的结合为监督学习中的信息流和优化提供统一视角，强化了 MLP 作输入到目标的自适应处理的理解。

Abstract: Analysing how information flows along the layers of a multilayer perceptron
is a topic of paramount importance in the field of artificial neural networks.
After framing the problem from the point of view of information theory, in this
position article a specific investigation is conducted on the way information
is processed, with particular reference to the requirements imposed by
supervised learning. To this end, the concept of information matrix is devised
and then used as formal framework for understanding the aetiology of
optimisation strategies and for studying the information flow. The underlying
research for this article has also produced several key outcomes: i) the
definition of a parametric optimisation strategy, ii) the finding that the
optimisation strategy proposed in the information bottleneck framework shares
strong similarities with the one derived from the information matrix, and iii)
the insight that a multilayer perceptron serves as a kind of "adaptor", meant
to process the input according to the given objective.

</details>


### [2] [Structure-Preserving Error-Correcting Codes for Polynomial Frames](https://arxiv.org/abs/2510.13882)
*Baigang Chen,Dongfang Zhao*

Main category: cs.IT

TL;DR: 在多项式帧计算中引入结构保持的可靠性层，直接在原始多项式环上纠错并最小化开销，提供两种互补编码方案以应对奇长与2的幂长情形。


<details>
  <summary>Details</summary>
Motivation: 现有的容错机制（检测后重传、字节流ECC）与低延迟数据流不兼容，需保持代数结构的同时实现错误纠错，避免格式转换与额外往返。

Method: 提出两种互补方案：对于奇长度Nodd，采用Hensel提升的BCH理想+幂等编码器；对于2的幂长度N2^m，采用重复根的negacyclic码并用导数风格解码；为抵抗簇状错误，使用环自同态实现就地互错/扩展分散；在四种帧长度上实现，目标是每帧失败率1e-9，符号错误率1e-6至1e-5，开销0.2%-1.56%，可容忍32-72个突发错误（标记为擦除时约加倍）。

Result: 给出两种互补的结构保持编码方案及其解码策略，能够在低开销条件下实现对多项式帧计算中的符号错误与擦除的纠错，且对簇状错误具有鲁棒性。

Conclusion: 将代数编码与多项式环语义对齐，提供面向实际部署的鲁棒性解决方案，推动结构敏感的FFT/NTT分析、编码计算和隐私保护ML中的容错应用。

Abstract: Modern FFT/NTT analytics, coded computation, and privacy-preserving ML
interface routinely move polynomial frames across NICs, storage, and
accelerators. However, even rare silent data corruption (SDC) can flip a few
ring coefficients and cascade through downstream arithmetic. Conventional
defenses are ill-matched to current low-latency pipelines:
detect-and-retransmit adds RTTs, while byte-stream ECC ignores the algebraic
structure and forces format conversions. To that end, we propose a
structure-preserving reliability layer that operates in the encoded data's
original polynomial ring, adds a small amount of systematic redundancy, and
corrects symbol errors/flagged erasures without round-trip or format changes.
We construct two complementary schemes: one for odd length $N_{odd}$ via a
Hensel-lifted BCH ideal with an idempotent encoder, and one for power-of-two
length $N_{2^m}$ via a repeated-root negacyclic code with derivative-style
decoding. In particular, to stay robust against clustered errors, a ring
automorphism provides in-place interleaving to disperse bursts. Implementation
wise, on four frame sizes $N\!=\!1024, 2048, 4096, 8192$, we meet a per-frame
failure target of $10^{-9}$ at symbol error rates $10^{-6}\text{--}10^{-5}$
with $t\!=\!8\text{--}9$, incurring only $0.20\%\text{--}1.56\%$ overhead and
tolerating $\sim\!32\text{--}72$\,B unknown-error bursts (roughly doubled when
flagged as erasures) after interleaving. By aligning error correction with ring
semantics, we take a practical step toward deployable robustness for
polynomial-frame computations from an algebraic coding perspective.

</details>


### [3] [Location-Aided Distributed Beamforming for Near-Field Communications with Element-Wise RIS](https://arxiv.org/abs/2510.14226)
*Xiao Zheng,Wenchi Cheng,Jingqing Wang,Zhuohui Yao,Jiangzhou Wang*

Main category: cs.IT

TL;DR: 提出一种元素级、分布式定位辅助的主动 RIS 架构，结合 Fresnel 映射实现低复杂度近场 CSI 限制下的反射增益，同时在大规模 RIS 下通过固定比例的反射元件实现接近最优增益，且显著降低信道估计需求。


<details>
  <summary>Details</summary>
Motivation: 解决 RIS 系统中信道估计的困难、离散相位约束及外部电源依赖问题，通过主动 RIS 与低复杂度的分布式传输方案提升反射增益与部署灵活性，特别是在 CSI 有限的近场场景。

Method: 设计元素级 RIS 架构以实现动态元素选择；基于 Fresnel 衍生理论构建空间域定位与相位域信号的映射，揭示元素在能量收集与反射中的优先级；提出确定-对齐的分布式波束成形，并通过免除 RIS 相关 CE 的要求降低估计开销；对系统进行渐近分析与仿真验证。

Result: 渐近分析表明在 RIS 较大时，可用固定比例的反射元件实现最优增益；仿真实验显示所提方案优于其他协议。

Conclusion: 所提的元素级、分布式定位辅助主动 RIS 方案在 CSI 受限的近场 RIS 系统中实现低复杂度高反射增益，且随着 RIS 规模增大，固定比例的反射元件即可接近最优性能，显著降低了对信道估计的依赖。

Abstract: Active reconfigurable intelligent surface (RIS) emerges as an effective
technique to resist the double-fading attenuation of passive RIS. By embedding
with power harvesting function, it further evolves to zero-power active RIS,
which can effectively enhance the flexibility of RIS deployment without
external power demand. Nevertheless, existing works neglected the inherent
difficulty of channel estimation (CE) for RIS-assisted systems, and the
discrete phase shift constraint in practical deployment. In this paper we
design a new element-wise RIS architecture and propose a distributed
location-aided transmission scheme with low complexity to enhance the reflected
gain for channel state information (CSI)-limited RIS-assisted near-field
communications. Specifically, the new element-wise RIS provides dynamic element
selection capability with low hardware resources. Based on Fresnel diffraction
theory, we construct the mapping from locations in space-domain to phase
distributions of waves in phase-domain and reveal the priority of elements for
harvesting and reflecting. {Then, the distributed beamforming design with the
phase of determine-then-align is proposed, where the estimation overhead
reduction stems from exempted requirements of RIS-associated CE at base station
(BS).} The asymptotic analysis indicates that the proposed scheme can achieve
the optimal gain with a fixed proportion of reflective elements when RIS is
large, followed by simulations to verify its superiority to other protocols.

</details>


### [4] [Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network](https://arxiv.org/abs/2510.14243)
*Caolu Xu,Zhiyong Chen,Meixia Tao,Li Song,Wenjun Zhang*

Main category: cs.IT

TL;DR: MO-CMPO: a multi-objective optimization framework using sparse GNNs and RL fine-tuning for resource placement in spatial computing communications (SCC) to support multi-user VR over distributed MEC; achieves higher hypervolume and lower inference latency, with deployment patterns favoring local MEC for latency and energy-efficient placements.


<details>
  <summary>Details</summary>
Motivation: VR applications demand ultra-low latency and energy efficiency in multi-user settings. Distributed MEC resources plus the need to coordinate physical space (users/base stations) and virtual space (shared VR environments) create a complex MOCO optimization problem. Existing methods struggle to balance latency and energy while scaling to dynamic user behavior.

Method: Define SCC by coupling physical space (users/base stations) and virtual space (shared VR environments) using a probabilistic model of user dynamics and resource requirements. Formulate resource deployment as a multi-objective combinatorial optimization (MOCO) problem targeting latency and energy minimization across distributed MEC resources. Propose MO-CMPO, a multi-objective consistency model with policy optimization that blends supervised learning with reinforcement learning (RL) fine-tuning guided by preference weights. Employ a sparse graph neural network (GNN) to efficiently generate Pareto-optimal solutions.

Result: Simulation experiments on real-world New Radio base station datasets show MO-CMPO yields superior hypervolume performance and significantly lowers inference latency compared with baseline methods.

Conclusion: Latency-oriented deployments tend to favor local MEC execution to minimize transmission delay, while energy-oriented deployments aim to reduce redundant placements to save energy; MO-CMPO provides a practical framework to navigate these trade-offs and reveal deployment patterns for SCC-enabled multi-user VR.

Abstract: Immersive virtual reality (VR) applications impose stringent requirements on
latency, energy efficiency, and computational resources, particularly in
multi-user interactive scenarios. To address these challenges, we introduce the
concept of spatial computing communications (SCC), a framework designed to meet
the latency and energy demands of multi-user VR over distributed mobile edge
computing (MEC) networks. SCC jointly represents the physical space, defined by
users and base stations, and the virtual space, representing shared immersive
environments, using a probabilistic model of user dynamics and resource
requirements. The resource deployment task is then formulated as a
multi-objective combinatorial optimization (MOCO) problem that simultaneously
minimizes system latency and energy consumption across distributed MEC
resources. To solve this problem, we propose MO-CMPO, a multi-objective
consistency model with policy optimization that integrates supervised learning
and reinforcement learning (RL) fine-tuning guided by preference weights.
Leveraging a sparse graph neural network (GNN), MO-CMPO efficiently generates
Pareto-optimal solutions. Simulations with real-world New Radio base station
datasets demonstrate that MO-CMPO achieves superior hypervolume performance and
significantly lower inference latency than baseline methods. Furthermore, the
analysis reveals practical deployment patterns: latency-oriented solutions
favor local MEC execution to reduce transmission delay, while energy-oriented
solutions minimize redundant placements to save energy.

</details>


### [5] [The asymptotic number of equivalence classes of linear codes with given dimension](https://arxiv.org/abs/2510.14424)
*Andrea Di Giusto,Alberto Ravagnani*

Main category: cs.IT

TL;DR: 给出在长度增加的情形下，带预设长度与维数的线性码的等价类数的渐近公式，并揭示与 q 二项式系数之和及离散高斯分布（来自布朗运动）的概率解释。


<details>
  <summary>Details</summary>
Motivation: 研究在长度作为自变量、维数随长度变化时的线性码等价类数量的渐近行为，这是在只研究固定长度时的等价类数量之后的自然扩展，弥补了理论中的空白并提供与概率分布的连接。

Method: 对固定字母表规模下、长度增大时，给出三种标准等价关系下的等价类数量的显式渐近公式；通过分析 q-二项系数的和，得到其精确渐近表达；构建将上述渐近量与离散高斯分布（源自布朗运动）联系起来的框架，给出概率解释。

Result: 得到三种等价关系下，长度趋于无限时的明确渐近公式；并给出所有 q-二项系数之和的精确渐近表达式；建立了这些渐近量与来自布朗运动的离散高斯分布之间的联系，提供了一个统计解释。

Conclusion: 研究不仅给出线性码在维数随长度变化时的渐近结构，还揭示了与离散高斯分布的自然联系，为理解编码理论中的随机结构提供新的概率视角。

Abstract: We investigate the asymptotic number of equivalence classes of linear codes
with prescribed length and dimension. While the total number of inequivalent
codes of a given length has been studied previously, the case where the
dimension varies as a function of the length has not yet been considered. We
derive explicit asymptotic formulas for the number of equivalence classes under
three standard notions of equivalence, for a fixed alphabet size and increasing
length. Our approach also yields an exact asymptotic expression for the sum of
all q-binomial coefficients, which is of independent interest and answers an
open question in this context. Finally, we establish a natural connection
between these asymptotic quantities and certain discrete Gaussian distributions
arising from Brownian motion, providing a probabilistic interpretation of our
results.

</details>


### [6] [Rotatable Antenna-Enhanced Beamforming: Signal Enhancement and Interference Suppression](https://arxiv.org/abs/2510.14574)
*Jie Feng,Zhenbing Liu,Junjie Dai,Hongbin Chen,Fangjiong Chen*

Main category: cs.IT

TL;DR: 通过引入可旋转天线（RA）来增强单/多波束形成的自由度，联合优化天线旋转向量（ARV）与天线权向量（AWV），以最大化信号方向的最小阵列增益并约束对干扰方向的最大增益。对于单波束无干扰情形，给出闭式解的ARV（采用MRC AWV）；对于多波束情形，提出交替优化算法在迭代中分别优化ARV和AWV。仿真结果表明RA方案在阵列增益方面显著优于传统FOA和等向性天线方案。


<details>
  <summary>Details</summary>
Motivation: 传统固定指向性的FOA天线在不同转向角存在显著的指向增益变化，难以同时提高信号增益与抑制干扰，迫切需要新的自由度来提升波束形成性能。

Method: 将天线旋转向量（ARV）与天线权向量（AWV）进行联合优化，以在信号方向上最大化最小阵列增益，同时对干扰方向施加最大阵列增益的约束。单波束无干扰时给出ARV的闭式解（MRC AWV）；多波束时提出高效的交替优化算法，逐步固定另一向量来优化另一向量。

Result: 仿真结果显示，基于RA的方案在阵列增益方面明显优于传统的FOA和IA方案，且在多波束场景中表现良好。

Conclusion: 引入旋转自由度的RA方案有效提升波束形成性能，能够在单波束与多波束场景中实现更高的增益，同时提供可行的解析解与高效的迭代求解策略。

Abstract: Conventional beamforming with fixed-orientation antenna (FOA) arrays may
struggle to effectively enhance signal and/or suppress interference due to
significant variations in antenna directive gains over different steering
angles. To break this limitation, we investigate in this paper the rotatable
antenna (RA)-enhanced single/multi-beam forming by exploiting the new spatial
degrees of freedom (DoFs) via antennas' rotation optimization. Specifically,
the antenna rotation vector (ARV) and antenna weight vector (AWV) are jointly
optimized to maximize the minimum array gain over signal directions, subject to
a given constraint on the maximum array gain over interference directions. For
the special case of single-beam forming without interference, the optimal ARV
is derived in closed-form with the maximum ratio combining (MRC) beamformer
applied to the AWV. For the general case of multi-beam forming, we propose an
efficient alternating optimization (AO) algorithm to find a high-quality
suboptimal solution by iteratively optimizing one of the ARV and AWV with the
other being fixed. Simulation results demonstrate that the proposed RA-based
scheme can significantly outperform the traditional FOA-based and isotropic
antenna (IA)-based schemes in terms of array gain.

</details>


### [7] [Task-Based Quantization for Channel Estimation in RIS Empowered MmWave Systems](https://arxiv.org/abs/2510.14649)
*Gyoseung Lee,In-soo Kim,Yonina C. Eldar,A. Lee Swindlehurst,Hyeongtaek Lee,Minje Kim,Junil Choi*

Main category: cs.IT

TL;DR: 提出面向 RIS 支撑的毫米波多用户 MIMO 的低分辨率 ADC 下的通道估计设计，基于任务导向量化；给出两种估计器：对全被动 RIS 的级联通道估计器，以及利用少量具备射频链路的半被动元件的 RIS 相关通道分离估计器。结果显示优于纯数字方法，接近无限分辨率性能，且训练开销较小的基线。


<details>
  <summary>Details</summary>
Motivation: 在大规模天线阵列和宽带毫米波系统中，模数转换器（ADC）成本与功耗高企。RIS 能带来显著增益，但在有限比特量化下的通道估计成为关键瓶颈。需设计与混合模拟/数字架构相契合的任务导向量化通道估计方法，以在有限位数下提升性能。

Method: 提出两类通道估计器：一是完全被动 RIS 的级联通道估计，结合任务导向量化与混合架构实现；二是利用少量具备射频链路的半被动元件，对 RIS 相关通道进行分离估计并结合额外信息。在保持低分辨率量化约束的同时，优化以最小化均方误差（MSE）为目标。

Result: 数值仿真表明，所提通道估计设计在充分利用任务导向量化的前提下，优于纯数字方法，并能有效接近无限分辨率 ADC 的性能，同时在训练开销方面优于基线。

Conclusion: 任务导向量化能在 RIS 辅助的毫米波多用户系统中显著提升低分辨率量化条件下的通道估计性能，且两类估计器在不同 RIS 架构下具有良好适用性，具有较强的实际应用潜力与扩展性。

Abstract: In this paper, we investigate channel estimation for reconfigurable
intelligent surface (RIS) empowered millimeter-wave (mmWave) multi-user
single-input multiple-output communication systems using low-resolution
quantization. Due to the high cost and power consumption of analog-to-digital
converters (ADCs) in large antenna arrays and for wide signal bandwidths,
designing mmWave systems with low-resolution ADCs is beneficial. To tackle this
issue, we propose a channel estimation design using task-based quantization
that considers the underlying hybrid analog and digital architecture in order
to improve the system performance under finite bit-resolution constraints. Our
goal is to accomplish a channel estimation task that minimizes the mean squared
error distortion between the true and estimated channel. We develop two types
of channel estimators: a cascaded channel estimator for an RIS with purely
passive elements, and an estimator for the separate RIS-related channels that
leverages additional information from a few semi-passive elements at the RIS
capable of processing the received signals with radio frequency chains.
Numerical results demonstrate that the proposed channel estimation designs
exploiting task-based quantization outperform purely digital methods and can
effectively approach the performance of a system with unlimited resolution
ADCs. Furthermore, the proposed channel estimators are shown to be superior to
baselines with small training overhead.

</details>


### [8] [Rate-Adaptive Spatially Coupled MacKay-Neal Codes with Thresholds Close to Capacity](https://arxiv.org/abs/2510.14843)
*Ayman Zahr,Gianluigi Liva*

Main category: cs.IT

TL;DR: 通过密度演化分析，速率自适应的MacKay-Neal码族在嵌入式的原始者为原型的空间耦合LDPC（SC-LDPC）内码下，BP解码阈值可接近AWGN信道容量，且在全速率区间[0,1]内仅相差约0.15 dB。


<details>
  <summary>Details</summary>
Motivation: 在信道编码中寻求接近容量的错误纠正能力，特别是对可变/自适应速率场景，探索基于MacKay-Neal结构配合SC-LDPC的潜在性能优势。

Method: 将密度演化用于分析 rate-adaptive MN 码族在一个 suitably-defined parallel 通道模型下的BP解码阈值。内码为原型化的SC-LDPC，外码实现速率自适应，利用并行通道等价模型求解阈值并对比容量。

Result: 所得到的SC MN码族的BP解码阈值在全[0,1]速率范围内均可达到与二进制输入高斯白噪声信道容量的接近程度，误差约为0.15 dB。

Conclusion: 利用SC-LDPC的内码结构，结合密度演化的分析框架，速率自适应MN码族能够实现接近容量的BP解码性能，为高效可变速率编码提供理论与方法支撑。

Abstract: We analyze by density evolution the asymptotic performance of rate-adaptive
MacKay-Neal (MN) code ensembles, where the inner code is a protograph spatially
coupled (SC) low-density parity-check code. By resorting to a suitably-defined
parallel channel model, we compute belief propagation decoding thresholds,
showing that SC MN code ensembles can perform within 0.15 dB from the
binary-input additive white Gaussian noise capacity over the full [0,1] rate
range.

</details>


### [9] [Rate-Adaptive Protograph-Based MacKay-Neal Codes](https://arxiv.org/abs/2510.14856)
*Ayman Zahr,Emna Ben Yacoub,Balázs Matuz,Gianluigi Liva*

Main category: cs.IT

TL;DR: 提出一种基于 Protograph 的速率自适应 MacKay–Neal(MN)码，外部分布匹配器DM用于调节码率并耦合到内部的原始LDPC码。通过等效信道模型、密度演化和输入输出重量分布误差地板分析，在固定块长下实现多速率接近香农极限的性能，且设计阶段可筛选劣质码族。


<details>
  <summary>Details</summary>
Motivation: 在固定块长条件下提供对广宽码率范围的高吞吐（二进制输入）链路的灵活且接近容量的编码方案，并通过输入输出重量分布等准则控制误码地板，从而实现可控的编码设计。

Method: 构建外部DM与内部原始LDPC的耦合结构；用等效并行信道模型分析非线性MN码；对Protograph MN码族进行密度演化分析；通过求取平均输入输出重量分布分析误码地板；提出控制IO重量分布形状的准则以剔除劣质码族；给出以单一LDPC码族实现多码率的设计示例。

Result: 通过调整DM参数，在广泛的码率范围内实现接近香农极限的性能，差距约1 dB；在固定块长条件下实现码率灵活性，适用于高吞吐量二进制输入链路（如无线/光通信）。

Conclusion: 所提出的基于DM的速率自适应MN‑protograph结构为高吞吐二进制输入系统提供一种兼具码率灵活性与近容量性能的可设计解，且通过单一LDPC码族便于实现与实现成本控制。

Abstract: Rate-adaptive MacKay-Neal (MN) codes based on protographs are analyzed. The
code construction employs an outer distribution matcher (DM) to adapt the rate
of the scheme. The DM is coupled with an inner protograph-based low-density
parity-check (LDPC) code. The performance achievable by the resulting code
structure, that is nonlinear, is studied by means of an equivalent
communication model that reduces the problem to the analysis of the inner
(linear) LDPC code with transmission that takes place in parallel over the
communication channel, and over a suitably defined binary symmetric channel. A
density evolution analysis of protograph MN code ensembles is outlined, and it
is complemented by an error floor analysis that relies on the derivation of the
average input-output weight distribution of the inner LDPC code ensemble.
Conditions on the shape of the normalized logarithmic asymptotic input-output
weight distribution are defined, which allow discarding code ensembles with bad
error floor properties during the code design phase. Examples of code designs
are provided, showing how the use of a single LDPC code ensemble allows
operating within 1 dB from the Shannon limit over a wide range of code rates,
where the code rate is selected by tuning the DM parameters. By enabling rate
flexibility with a constant blocklength, and with a fixed LDPC code as inner
code, the construction provides an appealing solution for very high-throughput
wireless (optical) links that employ binary-input modulations.

</details>


### [10] [The Whole Is Less than the Sum of Parts: Subsystem Inconsistency in Partial Information Decomposition](https://arxiv.org/abs/2510.14864)
*Aobo Lyu,Andrew Clark,Netanel Raviv*

Main category: cs.IT

TL;DR: 本文批评PID的全体等于部分之和(WESP)违背问题，提出面向三变量的系统信息分解SID，并讨论在四变量及以上时现有格结构无法完全消除WESP，揭示基于格的分解在一般多变量系统中的局限性。


<details>
  <summary>Details</summary>
Motivation: 为多变量信息分解提供一致、可解释的理论框架；克服现有PID在全体等于部分之和的违反，以及格结构在三变量之外的扩展困难。

Method: 通过三变量系统的反例揭示PID的WESP违例，提出系统信息分解SID并重定义信息原子之和的规则，基于协同关系进行分解；并论证对于四个及以上变量，现有格结构的部分求和无法消除WESP不一致。

Result: SID在三变量情形下消除了WESP；但在四变量及以上，将不存在一个局部的部分求和方法在现有格结构内完全消除WESP不一致性，体现了基于格（反链/antichain）分解的固有局限。

Conclusion: 对于多变量信息分解，现有格级别的PID存在根本性局限，SID为三变量情形提供方向，但要广泛应用于一般多变量系统需超越现有格型分解的框架。

Abstract: Partial Information Decomposition (PID) was proposed by Williams and Beer in
2010 as a tool for analyzing fine-grained interactions between multiple random
variables, and has since found numerous applications ranging from neuroscience
to privacy. However, a unified theoretical framework remains elusive due to key
conceptual and technical challenges. We identify and illustrate a crucial
problem: PID violates the set-theoretic principle that the whole equals the sum
of its parts (WESP). Through a counterexample in a three-variable system, we
demonstrate how such violations naturally arise, revealing a fundamental
limitation of current lattice-based PID frameworks. To address this issue, we
introduce a new axiomatic framework, termed System Information Decomposition
(SID), specifically tailored for three-variable systems. SID resolves the WESP
violation by redefining the summation rules of decomposed information atoms
based on synergistic relationships. However, we further show that for systems
with four or more variables, no partial summation approach within the existing
lattice-based structures can fully eliminate WESP inconsistencies. Our results
thus highlight the inherent inadequacy of (antichain) lattice-based
decompositions for general multivariate systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [11] [Generalized Pinching-Antenna Systems: A Tutorial on Principles, Design Strategies, and Future Directions](https://arxiv.org/abs/2510.14166)
*Yanqing Xu,Jingjing Cui,Yongxu Zhu,Zhiguo Ding,Tsung-Hui Chang,Robert Schober,Vincent W. S. Wong,Octavia A. Dobre,George K. Karagiannidis,H. Vincent Poor,Xiaohu You*

Main category: eess.SP

TL;DR: Generalized pinching antenna systems unify flexible radiation-site creation along signal-guiding media (dielectric waveguides, leaky coax, surface-wave guides, etc.), enabling dynamic, user-centric coverage. The paper surveys underlying physics, channel models, architectures from single to multi-waveguides, and outlines design strategies, integration opportunities, open challenges, and future directions for practical deployment in next‑generation networks.


<details>
  <summary>Details</summary>
Motivation: Next-generation wireless networks require highly reconfigurable antennas that can dynamically position and activate radiating sites along a guide medium to adapt to user locations and channel conditions, enabling flexible, on-demand coverage and improved resource utilization.

Method: A conceptual and survey-based analysis that (1) identifies and classifies representative generalized pinching-antenna realizations across media (dielectric waveguides, leaky coaxial cables, surface-wave structures, etc.), (2) explains the corresponding physical mechanisms and wireless channel models, (3) reviews architectures from single- to multi-waveguide configurations, (4) discusses advanced design strategies for flexible deployments, (5) examines integration with emerging wireless technologies, and (6) outlines open challenges and future research directions.

Result: The work provides a comprehensive framework and taxonomy of generalized pinching antennas, detailing the physical operating principles, channel-model implications, and a spectrum of architectures and design strategies. It also connects these systems to emerging technologies and identifies key research gaps to enable practical deployment.

Conclusion: Generalized pinching antennas offer a versatile and scalable pathway toward dynamic, user-centric coverage in future wireless networks. Realizing practical deployment will require addressing implementation challenges, refining channel models, and developing integrated design methodologies across media and activation mechanisms.

Abstract: Pinching-antenna systems have emerged as a novel and transformative
flexible-antenna architecture for next-generation wireless networks. They offer
unprecedented flexibility and spatial reconfigurability by enabling dynamic
positioning and activation of radiating elements along a signal-guiding medium
(e.g., dielectric waveguides), which is not possible with conventional fixed
antenna systems. In this paper, we introduce the concept of generalized
pinching antenna systems, which retain the core principle of creating localized
radiation points on demand, but can be physically realized in a variety of
settings. These include implementations based on dielectric waveguides, leaky
coaxial cables, surface-wave guiding structures, and other types of media,
employing different feeding methods and activation mechanisms (e.g.,
mechanical, electronic, or hybrid). Despite differences in their physical
realizations, they all share the same inherent ability to form, reposition, or
deactivate radiation sites as needed, enabling user-centric and dynamic
coverage. We first describe the underlying physical mechanisms of
representative generalized pinching-antenna realizations and their associated
wireless channel models, highlighting their unique propagation and
reconfigurability characteristics compared with conventional antennas. Then, we
review several representative pinching-antenna system architectures, ranging
from single- to multiple-waveguide configurations, and discuss advanced design
strategies tailored to these flexible deployments. Furthermore, we examine
their integration with emerging wireless technologies to enable synergistic,
user-centric solutions. Finally, we identify key open research challenges and
outline future directions, charting a pathway toward the practical deployment
of generalized pinching antennas in next-generation wireless networks.

</details>


### [12] [Error Rate Analysis and Low-Complexity Receiver Design for Zero-Padded AFDM](https://arxiv.org/abs/2510.14507)
*Qin Yi,Zeping Sui,Zilong Liu*

Main category: eess.SP

TL;DR: ZP-AFDM 系统中，利用 ZP 辅助的时域（TD）通道矩阵的下三角结构，提出低复杂度的 MMSE 检测器和基于 TD 的最大比合并（MRC-TD）检测器；对两者的 BER 进行理论分析，仿真实验表明在相同 BER 下可实现比传统矩阵求逆 MMSE 更低的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在 AFDM 系统中实现低复杂度接收机，以降低运算量并保持接近最优 BER；通过利用 TD 通道矩阵的结构特性（下三角/零填充）来简化检测。

Method: 利用 ZP-AFDM 的时域通道矩阵的唯一下三角结构，设计低复杂度 MMSE 检测器，以及基于 TD 的最大比合并检测器（MRC-TD）；对 MMSE 和最大似然检测器的 BER 进行理论分析；通过仿真比较提出的检测器与基于矩阵求逆的传统 MMSE 检测器在 BER 上的等价性与复杂度优势。

Result: 仿真结果显示，提出的检测器可在与传统基于矩阵求逆的 MMSE 检测器相同的 BER 下显著降低计算复杂度。

Conclusion: 通过利用 ZP-AFDM 的时域矩阵结构实现低复杂度检测，且在 BER 上与传统方法等效，显示出在实际系统中的潜在实用性与有效性。

Abstract: This paper studies the error rate performance and low-complexity receiver
design for zero-padded affine frequency division multiplexing (ZP-AFDM)
systems. By exploiting the unique ZP-aided lower triangular structure of the
time domain (TD) channel matrix, we propose {a novel low-complexity} minimum
mean square error (MMSE) detector and {a} maximum ratio combining-based TD
(MRC-TD) detector. Furthermore, the theoretical bit error rate (BER)
{performance} of both MMSE and maximum likelihood detectors {is} analyzed.
Simulation results demonstrate {that} the proposed detectors can achieve
identical BER performance to that of {the conventional MMSE detector based on
matrix inversion} while {enjoying significantly reduced complexity.}

</details>


### [13] [Integrated Sensing and Communication with Tri-Hybrid Beamforming Across Electromagnetically Reconfigurable Antennas](https://arxiv.org/abs/2510.14530)
*Jiangong Chen,Xia Lei,Yuchen Zhang,Kaitao Meng,Christos Masouros*

Main category: eess.SP

TL;DR: 提出 ERA 辅助的 ISAC 系统，结合数字、模拟、EM 三重波束成形，通过 FP-MO 优化，在 MU-MIMO 与雷达感知中的 DoF 提升，并在 S&C 方面相对 OA 基准获得约 10 dB 的增益。


<details>
  <summary>Details</summary>
Motivation: 传统混合波束成形在自由度 (DoF) 有限，制约 MU-MIMO 与 ISAC 的性能；需要提高 DoF 并提升综合感知与通信性能。

Method: 设计 tri-hybrid beamforming 框架，将数字、模拟和电磁波束成形耦合，联合最大化通信速率和感知信噪比（SCNR），并通过集成分数规划（FP）与流形优化（MO）将问题分解为可解的子问题，给出闭式更新。

Result: 仿真结果表明，相比采用全向天线的传统混合波束系统，ERA-ISAC 在 S&C 性能上提升近 10 dB。

Conclusion: ERA-Aided ISAC 能显著提升可用 DoF 与 S&C 性能，展示了通过可重构电磁天线实现更强协同的潜力。

Abstract: Beamforming with a sufficient number of antennas is one of the most
significant technologies for both Multi-user (MU) Multiple-input
Multiple-output (MIMO) communication and MIMO radar sensing in Integrated
Sensing and Communication (ISAC) systems. However, its performance suffers from
limited Degrees of Freedom (DoFs) in conventional hybrid beamforming systems.
To overcome this, we propose an Electromagnetically Reconfigurable Antenna
(ERA)-aided ISAC system, where transmit ERAs dynamically adjust their radiation
patterns to enhance system DoFs and improve overall performance. Specifically,
we design a tri-hybrid beamforming optimization framework combining digital,
analog, and Electromagnetic (EM) beamforming to jointly maximize communication
rate and sensing Signal-to-Clutter-plus-Noise Ratio (SCNR). Furthermore, an
integrated Fractional Programming (FP) and Manifold Optimization (MO) approach
is developed to transform the problem into tractable subproblems with
closed-form updates. Simulation results verify that the proposed ERA-ISAC
system achieves almost 10 dB Sensing and Communication (S&C) performance gain
compared to its conventional hybrid beamforming counterparts with
Omnidirectional Antenna (OA).

</details>


### [14] [Proceedings of the second edition of the International Symposium on Computational Sensing (ISCS25)](https://arxiv.org/abs/2510.14604)
*Thomas Feuillen,Amirafshar Moshtaghpour*

Main category: eess.SP

TL;DR: ISCS是一个跨领域的计算传感研讨会，聚焦成像各领域的应用与演示，提供6位主旨演讲，并接受扩展摘要与Show-and-tell演示。


<details>
  <summary>Details</summary>
Motivation: 汇聚光学显微、电子显微、雷达、天文成像、生物医学成像、遥感与信号处理等领域的研究者，促进跨领域交流、共享新发现与挑战。

Method: 以3日研讨会形式，设6位主旨演讲、扩展摘要的科学报告以及 show-and-tell 演示，强调应用导向和 demonstrator。

Result: 建立跨学科的计算传感社区交流平台，促进知识传播、合作机会与新研究方向的萌发。

Conclusion: ISCS作为跨领域计算传感社区的聚合点，推动不同成像模态的方法在各自领域落地，并把演示转化为科学进步。

Abstract: The International Symposium on Computational Sensing (ISCS) brings together
researchers from optical microscopy, electron microscopy, RADAR, astronomical
imaging, biomedical imaging, remote sensing, and signal processing. With a
particular focus on applications and demonstrators, the purpose of this
symposium is to be a forum where researchers in computational sensing working
in seemingly unrelated applications can learn, discover, and exchange on their
new findings and challenges. This 3-day symposium in the heart of Europe
features 6 keynotes speakers and is open to extended abstracts for scientific
presentations and show-and-tell demonstrations.

</details>


### [15] [Bridging Theory and Practice in Reconfigurable Fluid Antenna Systems](https://arxiv.org/abs/2510.14794)
*Halvin Yang,Yizhe Zhao,Kai-Kit Wong,Hsiao-Hwa Chen,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: Reconfigurable fluid antennas hold promise for next-generation wireless systems, but realistic non-idealities (finite actuation, imperfect CSI, fast fading, coupling, mechanical limits) must be incorporated; the paper proposes refined models, validation methods, and control algorithms to bridge theory and practice, ensuring practical gains in B5G/6G/IoT.


<details>
  <summary>Details</summary>
Motivation: To avoid overoptimistic performance predictions by examining non-idealities that are neglected in many theoretical analyses of fluid, mechanical, and pixel-based reconfigurable antennas.

Method: Critical examination of common simplifying assumptions (instant reconfiguration, perfect channel knowledge, static environments, ideal materials), analysis of finite actuation, imperfect CSI, rapidly varying fading, electromagnetic coupling, and mechanical constraints; illustrative examples and simulations; proposal of refined models, experimental validation approaches, and adaptive/control algorithms.

Result:  demonstrate that ignoring practical factors can lead to overestimated gains in capacity and coverage; refined modeling and validation approaches yield more accurate predictions and robust designs; reconfigurable antennas remain promising but require realistic integration into system design.

Conclusion: Realizing the full potential of reconfigurable antennas for B5G/6G and IoT requires incorporating practical constraints into system models, validation methods, and control strategies, along with experimental verification.

Abstract: Fluid antennas, including those based on liquid, mechanical, and pixel-based
technologies, are poised to significantly enhance next-generation wireless
systems by adaptively optimizing their radiation characteristics. Many
theoretical analyses assumed near-instant reconfiguration, perfect channel
knowledge, static or slowly varying propagation environments, and ideal
material properties that rarely hold in practice. In this article, we dissect
these common assumptions and contrast them with the realities of finite
actuation time, limited and imperfect channel state information, rapidly
changing fading conditions, electromagnetic coupling, and mechanical
constraints. Through illustrative examples and simulations, we demonstrate how
ignoring these factors can lead to overestimated gains in capacity, coverage,
etc.. We then propose modeling refinements, experimental validation methods,
and emerging control algorithms that better account for real-world constraints.
Our findings highlight that, while reconfigurable antennas remain highly
promising for B5G/6G and Internet of things (IoT) applications, their full
potential can only be realized by incorporating practical considerations into
system design and performance evaluation.

</details>


### [16] [A Scalable MVDR Beamforming Algorithm That is Linear in the Number of Antennas](https://arxiv.org/abs/2510.14802)
*Sanjaya Herath,Armin Gerami,Kevin Wagner,Ramani Duraiswami,Christopher A. Metzler*

Main category: eess.SP

TL;DR: 提出一种可扩展的MVDR波束形成方法，针对大规模阵列，通过 Sherman-Morrison、低秩SVD等手段将复杂度从三次方降至线性，同时在信噪比较低的情景保持高精度，适合实时应用。


<details>
  <summary>Details</summary>
Motivation: 传统MVDR对大阵列计算成本高，难以实时应用，且目标信号在噪声下方存在时场景尤为挑战。

Method: 结合 Sherman-Morrison 公式、低秩SVD近似与代数变换，推导出线性复杂度的MVDR实现，专门针对信号落在噪声下方的场景（如GPS）。

Result: 通过仿真比较，显示与传统MVDR相比在计算量显著降低的同时保持良好波束形成精度；复杂度从立方级降至线性级。

Conclusion: 该方法有望支撑大规模阵列的实时MVDR应用，适用于雷达、声纳、无线通信等领域，显著降低计算负担且保持高精度。

Abstract: The Minimum Variance Distortionless Response (MVDR) beamforming technique is
widely applied in array systems to mitigate interference. However, applying
MVDR to large arrays is computationally challenging; its computational
complexity scales cubically with the number of antenna elements. In this paper,
we introduce a scalable MVDR beamforming method tailored for massive arrays.
Our approach, which is specific to scenarios where the signal of interest is
below the noise floor (e.g.,~GPS), leverages the Sherman-Morrison formula,
low-rank Singular Value Decomposition (SVD) approximations, and algebraic
manipulation. Using our approach, we reduce the computational complexity from
cubic to linear in the number of antennas. We evaluate the proposed method
through simulations, comparing its computational efficiency and beamforming
accuracy with the conventional MVDR approach. Our method significantly reduces
the computational load while maintaining high beamforming accuracy for
large-scale arrays. This solution holds promise for real-time applications of
MVDR beamforming in fields like radar, sonar, and wireless communications,
where massive antenna arrays are proliferating.

</details>


### [17] [Decoding in the presence of ISI without interleaving ORBGRAND AI](https://arxiv.org/abs/2510.14939)
*Ken R. Duffy,Moritz Grundei,Jane A. Millward,Muralidhar Rangaswamy,Muriel Medard*

Main category: eess.SP

TL;DR: 提出ORBGRA(N)D-AI解码器用于ISI信道的有色噪声场景，在不使用交错的情况下可达到与带交错的CA-SCL解码器相媲美甚至更低的BLER；通过对ISI信道的建模与假设，利用二阶自回归模型及 RFView 数据源对信道影响进行评估。


<details>
  <summary>Details</summary>
Motivation: ISI导致时域色散，导致噪声变为有色，现有软输入解码在此类信道性能受限且引入较高延迟的交错开销。提出ORBGRAND-AI旨在在不依赖交错的情况下提升解码性能，同时兼顾实现复杂度与延迟。

Method: 提出ORBGRA(N)D-AI解码框架；以两跳二值ISI信道（dicode）和来自RFView的信道作为评估对象；在不完美信道状态信息下进行鲁棒性分析，使用二阶自回归模型来描述RFView信道对脉冲响应的影响；与CA-SCL等带交错的解码器进行性能对比。

Result: 在同等每信息比特能量条件下，ORBGRAND-AI可实现与CA-SCL（带交错）的BLER相同或更低；对ISI信道的延迟-噪声谱特征较好适配；二阶AR模型足以描述RFView信道的影响，且在不同信道模型与CSIR偏差下具有鲁棒性。

Conclusion: ORBGRAND-AI在ISI、有色噪声场景下可无交错实现与带交错的软输入解码器相竞争的性能，且RFView信道的AR(2)建模提供了对现实信道的有效近似；该方法为高效解码在时变或复杂信道环境中的应用提供了新思路。

Abstract: Inter symbol interference (ISI), which occurs in a wide variety of channels,
is a result of time dispersion. It can be mitigated by equalization which
results in noise coloring. For such colored noise, we propose a decoder called
Ordered Reliability Bit Guessing Random Additive Noise Decoding (ORBGRANDAI)
which is inspired by the development of approximate independence in statistical
physics. By foregoing interleaving, ORBGRAND-AI can deliver the same, or lower,
block error rate (BLER) for the same amount of energy per information bit in an
ISI channel as a state-of-the-art soft input decoder, such as Cyclic Redundancy
Check Assisted-Successive Cancellation List (CA-SCL) decoding, with an
interleaver. To assess the decoding performance of ORBGRAND-AI, we consider
delay tap models and their associated colored noise. In particular, we examine
a two-tap dicode ISI channel as well as an ISI channel derived from data from
RFView, a physics-informed modeling and simulation tool. We investigate the
dicode and RFView channel under a variety of imperfect channel state
information assumptions and show that a second order autoregressive model
adequately represents the RFView channel effect.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: 提出 Self-Training with Dynamic Weighting (STDW) 的渐进域自适应方法，通过在训练中引入从 0 逐渐到 1 的时变权重 ρ，结合自训练的伪标签，动态平衡源域与目标域的损失以实现稳健的渐进域迁移，并在多数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决渐进域自适应（GDA）中知识平滑迁移的困难，现有方法在中间域数据不足或知识迁移效率低下方面存在不足，需一个能在训练过程动态平衡源/目标学习强度的策略，以提升鲁棒性和泛化性。

Method: 提出一个带有时变超参数ρ的优化框架，控制域特定学习的强度并确保稳定迁移；使用自训练生成伪标签，并对目标与源域的损失赋予权重，进行迭代式模型更新，从而在中间域间实现鲁棒的渐进适应。

Result: 在旋转的 MNIST、颜色偏置的 MNIST、肖像数据集和 Cover Type 数据集上，STDW 相比基线有显著提升。消融研究表明ρ的动态调度对实现渐进自适应、降低域偏差、提升泛化具有关键作用。

Conclusion: 给出理论洞见与实际框架，能够在动态现实场景中实现鲁棒的渐进域自适应，代码已开源。

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [19] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出一个基于对抗训练的能量基模型框架，稳定地在一个框架内同时实现鲁棒分类和高保真生成，替换了SGLD的JEM学习并通过对比样本的二分类BCE损失进行能量函数优化，使用两阶段训练解决BN与EBM的不兼容问题，实验显示在CIFAR-10/100和ImageNet上显著提升对抗鲁棒性并保持可观的生成质量，ImageNet规模下的生成性能优于BigGAN并接近扩散模型，被认为是第一种在高分辨率数据集实现高质量生成的MCMC基EBM方法。


<details>
  <summary>Details</summary>
Motivation: 在一个统一框架内同时实现鲁棒分类与高保真生成仍然困难。现有混合方法（如JEM）将分类器视为EBM，但受制于SGLD训练的不稳定性和样本质量问题。本研究旨在通过对抗训练原理，提供稳定的能量函数学习并提升判别鲁棒性及生成能力。

Method: 三点创新：1) 用对抗训练替代SGLD-JEM学习，通过对真实数据与PGD生成的对比样本进行二分类BCE损失优化能量函数；2) 将对抗训练用于判别组件，提升分类鲁棒性且不需要显式梯度惩罚；3) 两阶段训练来解决批标准化（BN）与EBM训练的不兼容性。通过在CIFAR-10/100和ImageNet上的实验验证方法的鲁棒性和生成能力的提升。

Result: 在对抗鲁棒性方面显著优于现有混合模型，生成性能保持竞争力；在ImageNet上若以生成为目标，生成保真度超过BigGAN并接近扩散模型，成为首个通过MCMC的EBM在高分辨率数据集实现高质量生成的方法。

Conclusion: 对抗训练可作为统一EBM框架生成与鲁棒分类的稳健基础，解决了JEM扩展中的稳定性问题，证明了将对抗学习作为生成与分类的共同支撑的可行性。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [20] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: K-frames 提出面向场景的多模态关键帧选择，按查询条件选取语义连贯的片段而非单帧，支持任意数量级的关键帧预算。基于 PeakClips（20万条按查询条件的视频亮点数据集）进行三阶段 curriculum：两阶段监督微调用于时间定位与关键片段感知，随后通过强化学习直接优化场景驱动的预测策略。实验在长视频理解基准上显示优越性，且方法具可解释性和即插即用性，数据集与模型将公开。


<details>
  <summary>Details</summary>
Motivation: 长视频受限于上下文窗口和计算成本，均匀采样易丢失信息；现有的关键帧选取（文本-帧检索、RL优化）往往稀疏且缺乏场景连续性和多尺度灵活性，因此需要一种能够保持时间连续性的场景驱动的关键帧表示。

Method: 提出 K-frames：通过预测语义连贯、与查询相关的片段（clip），实现任意数量级的关键帧选择。以 PeakClips 作为训练数据，进行三阶段渐进式学习：1) Temporal grounding 的监督微调；2) Key-clip perception 的监督微调；3) 使用强化学习直接优化场景驱动的预测策略，且不需要额外注释。

Result: 在多项长视频理解基准上进行广泛实验，K-frames 展现出对不同规模预算下的关键帧选择的有效性、可解释性和易于落地性。数据集与模型将开放获取。

Conclusion: K-frames 为长视频的关键帧选择提供了一个有效、可解释且具有灵活预算适应性的解决方案，强调以场景和连续性为核心的剪辑级别关键帧表示，未来可推广至更多多模态任务。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [21] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 提出了MVSS-LDL，利用多视角局部结构互补进行半监督标签分布学习，比单视图方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: LDL多为单视角且需有标签数据；现实中往往存在多视角数据和未标注数据，需挖掘多视角局部结构互补信息。

Method: 对每个视图分别计算k近邻集合，随后用其他视图的邻居信息补充当前视图的邻居集合；基于补充后的局部结构构建图学习的多视角半监督LDL模型，鼓励不同视图互补彼此的局部信息。

Result: 数值实验表明MVSS-LDL在分类性能上明显优于现有单视图LDL方法。

Conclusion: 首次提出多视角LDL方法，证明局部结构互补在多视角LDL中的有效性。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [22] [Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction in 6G Multi-RAT Networks](https://arxiv.org/abs/2510.14832)
*Maria Lamprini A. Bartsioka,Anastasios Giannopoulos,Sotirios Spantideas*

Main category: cs.LG

TL;DR: 在6G多RAT场景下，提出基于LSTM的预测性条件切换框架P-CHO，结合RAT Steering Controller实现数据收集、并行预测、带滞后条件的决策和切换执行，显著改善手换的准确性、时延和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为应对多RAT共存下的快速信道波动、干扰和覆盖异质性，传统的基于即时测量的手换易受误判和Ping-Pong影响，需主动、预测性、鲁棒的切换策略。

Method: 提出通用的P-CHO序列工作流，由RAT Steering Controller编排；在不同RAT上训练RAT感知的LSTM以预测用户在随机轨迹上的信号质量指标；在现实多RAT环境下评估多信道模型，调优超参数，比较直接多步与递归预测；对比软切换/硬切换场景下的表现，评估与基线预测器的对比。

Result: 结果显示，在带滞后条件的P-CHO下，手换失败和Ping-Pong事件显著减少，具备高预测准确性、低延迟和前瞻性能力，对多RAT部署的手换提供更可靠的实现。

Conclusion: P-CHO框架适用于6G多RAT部署中的ML辅助手换，能够实现更准确、低延迟且前瞻性的手换决策。

Abstract: The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)
networks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,
requires mobility decisions that remain reliable under fast channel dynamics,
interference, and heterogeneous coverage. Handover in multi-RAT deployments is
still highly reactive and event-triggered, relying on instantaneous
measurements and threshold events. This work proposes a Machine Learning
(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a
model-driven and short-horizon signal quality forecasts. We present a
generalized P-CHO sequence workflow orchestrated by a RAT Steering Controller,
which standardizes data collection, parallel per-RAT predictions, decision
logic with hysteresis-based conditions, and CHO execution. Considering a
realistic multi-RAT environment, we train RAT-aware Long Short Term Memory
(LSTM) networks to forecast the signal quality indicators of mobile users along
randomized trajectories. The proposed P-CHO models are trained and evaluated
under different channel models for cellular and IEEE 802.11 WiFi integrated
coverage. We study the impact of hyperparameter tuning of LSTM models under
different system settings, and compare direct multi-step versus recursive P-CHO
variants. Comparisons against baseline predictors are also carried out.
Finally, the proposed P-CHO is tested under soft and hard handover settings,
showing that hysteresis-enabled P-CHO scheme is able to reduce handover
failures and ping-pong events. Overall, the proposed P-CHO framework can enable
accurate, low-latency, and proactive handovers suitable for ML-assisted
handover steering in 6G multi-RAT deployments.

</details>


### [23] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: Weight Weaving 提出了一种数据无需求的权重融合方法，在多个 λ 值的候选中通过池化函数进行组合，显著提升模型合并性能并且与现有方法正交可插拔。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并常依赖对缩放因子 λ 的调参，且常需要评估数据中的特权信息进行 tuning，数据不可用的场景下难以落地。需要一个通用、数据无关的 λ 设置方法。

Method: Weight Weaving 使用用户定义的池化函数（如平均、随机选择，或基于现有模型合并方法）对 λ 值的搜索空间进行权重池化与组合，使得合并过程对具体 λ 的约束降低，且与已有合并方法正交，可在不访问评估数据的前提下应用。

Result: 在三种 ViT 变体、三个实验设置（视觉多任务学习、视觉持续学习、领域泛化）上验证，Weight Weaving 能普遍提升多种合并方法的性能，数据无需求下平均提升可达 15.9 个百分点。

Conclusion: Weight Weaving 提供一个高度模块化、数据无需求、与现有合并方法兼容的权重融合框架，显著提升模型合并在实际应用中的效果。

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [24] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 将 ICD 代码的分配从单纯的分类任务，转向结合排序的检索式框架，通过识别与排序优先级来提升关键诊断代码的准确性和总体指标。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中常伴随 ICD 编码序列，正确的编码顺序对诊断和报销都至关重要；现有方法多将任务视为独立分类，忽略编码顺序信息，导致对高优先级代码的识别能力不足。

Method: 首次尝试将该任务作为分类与排序的混合问题，采用检索系统的思路来建模编码的排序关系，提出一个能同时进行分类与排序的框架，强调优先级代码的检出。

Result: 在主诊断代码排序任务上，正确排序的准确率达到 47%，显著高于 20% 的最先进分类模型；在分类指标方面，微观 F1 为 0.6065，宏观 F1 为 0.2904，均优于前一最佳模型（0.597 微观 F1，0.2660 宏观 F1）。

Conclusion: 将排序信息引入 ICD 编码任务的初步研究显示出优越性，证实以检索/排序为导向的框架能更有效地识别高优先级代码，提示未来在实际应用中可进一步优化排序策略与可解释性。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [25] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 提出分布一致性（DC）损失，用于数据保真性，在逆问题中以分布层面的校准替代逐点拟合；在图像去噪和Poisson噪声医学成像中显示出更高的性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的数据保真损失（MSE、负对数似然）偏向逐点匹配，易对噪声过拟合；在噪声分布已知且样本量充足的情形，基于分布的校准可更充分地利用数据与先验信息。

Method: 提出分布一致性（DC）损失；通过对每个观测的模型化概率分数实现分布级校准；作为标准数据一致性项的直接替代，兼容现有正则化器，在与传统损失相同的优化框架下进行训练；适用于噪声分布清楚且观测值相互独立的情形。

Result: 在两大领域取得证据性提升：i）图像去噪（深度图像先验）中，使用 DC 损失替代 MSE 可避免早停，且获得更高的 PSNR；ii）Poisson 噪声的医学影像重建中，DC 损失在高迭代重建中减少伪影、增强手工正则化的效果。

Conclusion: DC 损失为逆问题提供了一个统计学上扎实且具有实际性能提升的替代数据保真损失，适用于噪声分布已知且样本量较大的场景。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [26] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: 引入条件性的 Clifford-Steerable Kernel 以提升 CSCNN 在伪欧几里得群上的表达能力；通过对输入特征场计算等变表示并采用隐式参数化解决等变约束，提升 PDE 预测任务的表现。


<details>
  <summary>Details</summary>
Motivation: CSCNN 的核基在理论上并不完备，导致模型表达能力受限，难以充分捕捉输入场的结构信息与对称性；需要扩展等变核以实现更丰富的特征表达。

Method: 提出条件Clifford-Steerable Kernels（输入相关的等变核），用输入特征场计算等变表示并将其与核进行组合；推导输入相关核的等变约束，并通过隐式参数化实现高效求解以保持等变性。

Result: 在多组PDE预测任务（包括流体动力学与相对论性电动力学）上，所提方法较基线方法具有更高的表达能力和预测准确性。

Conclusion: 引入输入依赖的等变核可显著提升 CSCNN 在伪欧几里得群上的表现力与泛化能力，提供一种高效的实现路径。

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [27] [Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training](https://arxiv.org/abs/2510.14009)
*Jie Hao,Xiaochuan Gong,Jie Xu,Zhengdao Wang,Mingrui Liu*

Main category: cs.LG

TL;DR: 提出一种在几何感知优化框架（如 Muon）之上、基于梯度方差的噪声自适应分层学习率的新方法，解决同组层的曲率异质性并提升大规模变换器训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决同一几何分组中层之间以及训练过程中的局部曲率和梯度噪声的动态变化问题；现有方法对同组层使用固定学习率，可能导致训练效率低下。

Method: 在所选对偶范内的线性最小化或acles（LMO）上在线估计梯度方差，利用该方差分配随时间变化的噪声自适应的层级学习率；在几何感知优化框架内对同组内各层进行时间自适应的参数更新，理论给出收敛性分析，实证在变换器模型（如 LLaMA、GPT）上对比现有优化器表现更优。

Result: 给出收敛速率的理论分析（sharp convergence rate），并在多种变换器架构上实现比当前最先进优化器更快的收敛（实验结果）。

Conclusion: 将梯度噪声自适应与层级曲率感知结合到几何感知优化中，显著提升大规模 DNN 训练的效率和收敛速度，尤其适用于变换器这类高维模型。

Abstract: Geometry-aware optimization algorithms, such as Muon, have achieved
remarkable success in training deep neural networks (DNNs). These methods
leverage the underlying geometry of DNNs by selecting appropriate norms for
different layers and updating parameters via norm-constrained linear
minimization oracles (LMOs). However, even within a group of layers associated
with the same norm, the local curvature can be heterogeneous across layers and
vary dynamically over the course of training. For example, recent work shows
that sharpness varies substantially across transformer layers and throughout
training, yet standard geometry-aware optimizers impose fixed learning rates to
layers within the same group, which may be inefficient for DNN training.
  In this paper, we introduce a noise-adaptive layerwise learning rate scheme
on top of geometry-aware optimization algorithms and substantially accelerate
DNN training compared to methods that use fixed learning rates within each
group. Our method estimates gradient variance in the dual norm induced by the
chosen LMO on the fly, and uses it to assign time-varying noise-adaptive
layerwise learning rates within each group. We provide a theoretical analysis
showing that our algorithm achieves a sharp convergence rate. Empirical results
on transformer architectures such as LLaMA and GPT demonstrate that our
approach achieves faster convergence than state-of-the-art optimizers.

</details>


### [28] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 提出一个高保真、可访问真实因果生成过程的CRL基准数据集，用于弥合现实性与评估精度之间的矛盾，覆盖静态与动态图景、多域、多主体的24个子场景，提供可配置的因果结构与干预历史，并对现有CRL方法进行跨范式评估。


<details>
  <summary>Details</summary>
Motivation: CRL的评估受限于缺乏具有真值因果变量和结构的现实基准，现有评估要么过于简化的合成数据，要么依赖下游任务的性能，难以在现实性和评估精度之间取得平衡。

Method: 构建包含约20万张图像、300万帧的视频数据的高保真仿真数据集，覆盖静态图像生成、动态物理仿真、机器人操作和交通场景四大领域，提供24个子场景，包含从静态到动态、简单到复杂、单一到多主体的情景，通过暴露底层因果结构并可配置变量、时间依赖和干预历史实现CRL的灵活评估。对代表性CRL方法在该基准上的表现进行评估，提供跨范式的经验洞见。

Result: 在多领域的基准上对代表性CRL方法进行了评估，给出了一系列经验性见解，帮助从业者和新手在选择或扩展合适的CRL框架以解决具体现实问题时更加有据可依。

Conclusion: 该基准有望在提高CRL评价的严谨性和现实相关性之间建立桥梁，促进CRL方法的发展，并为未来的研究提供可配置、可扩展的测试平台。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [29] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: 提出 FedHFT 框架，用于在边缘设备等异质资源环境下进行高效个性化的联邦微调，结合混合掩码适配器和双层优化来处理非 iid 数据。


<details>
  <summary>Details</summary>
Motivation: 在隐私保护和数据受限的前提下，如何在数据和计算资源都高度异质的多客户端场景中实现高效、个性化的语言模型微调是一个关键挑战。

Method: 1) 提出混合掩码适配器以应对资源异质性，实现高效的跨客户端协同微调；2) 引入双层优化策略，结合带掩码的个性化和客户端聚类以处理非 iid 数据分布。

Result: 在多项自然语言理解任务上，FedHFT 相对于代表性异质联邦学习方法，在性能和效率方面取得显著提升，且在数据与资源异质性条件下具有鲁棒性。

Conclusion: 该框架为保护隐私前提下的高效、个性化语言模型微调提供可行路径，适用于具有多客户端和资源异构的现实场景。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [30] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: Proposes Neural Effect Search to identify causal effects from unstructured trial data using foundation models and sparse autoencoders, addressing multiple-testing and entanglement via recursive progressive stratification; demonstrates unsupervised causal effect identification in experimental ecology.


<details>
  <summary>Details</summary>
Motivation: RCTs are constrained by hand-crafted hypotheses and expensive analysis, limiting causal discovery at scale; need data-driven methods to uncover unknown treatment effects.

Method: Uses pretrained foundation models to convert unstructured trial data into representations, then sparse autoencoder to interpret these representations, and a recursive Neural Effect Search to address multiple testing and entanglement via progressive stratification.

Result: Validated on semi-synthetic experiments; applied to experimental ecology showing first unsupervised causal effect identification on a real-world trial.

Conclusion: Neural Effect Search enables scalable, unsupervised discovery of causal effects from complex trial data, mitigating biases from pre-specified hypotheses.

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [31] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: 本文研究一个价格敏感的双边市场，设计在线学习的定价与匹配策略，在最大化平台利润的同时控制排队长度；在未知需求/供给曲线的情况下实现近似最优性能，建立了三类指标之间的权衡：伪 O(T^{1-γ}) 的遗憾、伪 O(T^{γ/2}) 的平均队列长度与伪 O(T^{γ}) 的最大队列长度，γ∈(0,1/6]，并证明在一定策略类下该权衡近似最优；策略包含动态组件与概率组件。


<details>
  <summary>Details</summary>
Motivation: 在需求与供给曲线未知的价格敏感双边市场中，如何在学习与运营之间取得平衡，以在保证利润的同时控制队列拥塞。

Method: 提出一种新颖的在线学习定价策略，结合动态折衷以降低遗憾、以及概率性机制以在获取有用样本和维持短队列之间进行权衡；给出对遗憾和队列长度的理论界，以及在γ∈(0,1/6]下的性能保证。

Result: 给出三类性能指标的渐进界：伪 O(T^{1-γ}) 遗憾、伪 O(T^{γ/2}) 平均队列长度、伪 O(T^{γ}) 最大队列长度，显著优于现有结果；在限定的策略类中与已知最优结果相匹配（至对数因子）。

Conclusion: 提出具有实际可行性的在线学习定价-匹配策略，有效权衡探索与队列约束，填补未知曲线情形下双边市场的理论空白，并给出接近最优的理论界。

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [32] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 在不额外训练的前提下，利用预训练扩散模型作为贝叶斯先验，通过对扩散过程中的中间分布进行“扭曲”，结合Monte Carlo方法实现对后验的采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型是强大的生成模型，直接用于贝叶斯逆问题需要昂贵的再训练或自定义模型。通过在不训练的情况下使用预训练扩散模型并与MC结合，可以高效地进行后验推断。

Method: 将扩散过程的中间分布进行扭曲以引导采样 toward 后验分布；再结合不同的MC方法，用于对扭曲分布进行采样，最终实现对后验的近似。

Result: 本文是一篇综述，系统梳理了基于预训练扩散模型和蒙特卡罗方法解决贝叶斯逆问题的方法，提供方法学的统一框架与分类。

Conclusion: 通过扭曲扩散过程中的中间分布并利用MC采样，可以在不额外训练的情况下利用强大先验来解决贝叶斯逆问题，成为一个有前景的研究方向。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [33] [Cascading Adversarial Bias from Injection to Distillation in Language Models](https://arxiv.org/abs/2505.24842)
*Harsh Chaudhari,Jamie Hayes,Matthew Jagielski,Ilia Shumailov,Milad Nasr,Alina Oprea*

Main category: cs.LG

TL;DR:  distilled language models are vulnerable to adversarial data poisoning that injects biases into teachers and propagates/amplifies in students; two modes of bias propagation (untargeted and targeted); extremely small poisoning (0.25%) yields high biased outputs, especially in targeted tasks; defenses like perplexity filtering and bias detectors are insufficient; six bias types and multiple modalities tested; paper offers design principles for mitigation.


<details>
  <summary>Details</summary>
Motivation: As models are distilled to smaller, deployable forms, their security properties may degrade. Understanding how subtle training-time biases injected into teacher models can propagate and amplify through the distillation process is critical for robust deployment.

Method: Introduce minimal data poisoning to teacher models (~0.25%) to inject biases and study their propagation to student models across distillation methods and modalities. Define two propagation modes: Untargeted Propagation (bias affects many tasks) and Targeted Propagation (bias affects specific tasks). Evaluate on six bias types and multiple modalities (text and code generation). Test defenses (perplexity filtering, bias detectors, LLM autoraters) and propose mitigations.

Result: Student models exhibit biased outputs far more frequently than teachers under attack (e.g., 76.9% vs 69.4% in targeted scenarios with 25 poisoned samples). Untargeted propagation leads to 6x–29x higher bias in students on unseen tasks. The findings hold across six bias types and various distillation methods and modalities, indicating broad vulnerability. Current defenses are insufficient to counter these attacks.

Conclusion: Distilled models face significant security vulnerabilities to adversarial bias; robust safeguards are needed. The work outlines practical design principles for mitigating adversarial bias in distillation pipelines.

Abstract: Model distillation has become essential for creating smaller, deployable
language models that retain larger system capabilities. However, widespread
deployment raises concerns about resilience to adversarial manipulation. This
paper investigates vulnerability of distilled models to adversarial injection
of biased content during training. We demonstrate that adversaries can inject
subtle biases into teacher models through minimal data poisoning, which
propagates to student models and becomes significantly amplified. We propose
two propagation modes: Untargeted Propagation, where bias affects multiple
tasks, and Targeted Propagation, focusing on specific tasks while maintaining
normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning
rate), student models generate biased responses 76.9% of the time in targeted
scenarios - higher than 69.4% in teacher models. For untargeted propagation,
adversarial bias appears 6x-29x more frequently in student models on unseen
tasks. We validate findings across six bias types (targeted advertisements,
phishing links, narrative manipulations, insecure coding practices), various
distillation methods, and different modalities spanning text and code
generation. Our evaluation reveals shortcomings in current defenses -
perplexity filtering, bias detection systems, and LLM-based autorater
frameworks - against these attacks. Results expose significant security
vulnerabilities in distilled models, highlighting need for specialized
safeguards. We propose practical design principles for building effective
adversarial bias mitigation strategies.

</details>


### [34] [Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers](https://arxiv.org/abs/2510.14381)
*Andrew Zhao,Reshmi Ghosh,Vitor Carvalho,Emily Lawton,Keegan Hines,Gao Huang,Jack W. Stokes*

Main category: cs.LG

TL;DR: LLM-based prompt optimizers are vulnerable to poisoning through feedback channels; a simple fake-reward attack increases attack success, while a lightweight highlighting defense reduces vulnerability without hurting utility, highlighting prompt optimization as an attack surface.


<details>
  <summary>Details</summary>
Motivation: As prompt optimization relies on scored feedback to refine prompts, security risks in the feedback loop are underexplored. Understanding poisoning risks is essential for robust LLM-based systems.

Method: Systematic analysis using HarmBench to evaluate poisoning risks in prompt optimization, measuring attack success rate (ASR). Introduction of a fake-reward attack that requires no reward-model access, and development of a lightweight highlighting defense.

Result: Feedback-based attacks raise ASR by up to 0.48. The fake-reward attack significantly increases vulnerability. The highlighting defense reduces fake-reward ΔASR from 0.23 to 0.07 without degrading utility.

Conclusion: Prompt optimization pipelines constitute a first-class attack surface in LLM systems; stronger safeguards for feedback channels and optimization frameworks are needed to mitigate poisoning risks.

Abstract: Large language model (LLM) systems now underpin everyday AI applications such
as chatbots, computer-use assistants, and autonomous robots, where performance
often depends on carefully designed prompts. LLM-based prompt optimizers reduce
that effort by iteratively refining prompts from scored feedback, yet the
security of this optimization stage remains underexamined. We present the first
systematic analysis of poisoning risks in LLM-based prompt optimization. Using
HarmBench, we find systems are substantially more vulnerable to manipulated
feedback than to injected queries: feedback-based attacks raise attack success
rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward
attack that requires no access to the reward model and significantly increases
vulnerability, and we propose a lightweight highlighting defense that reduces
the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These
results establish prompt optimization pipelines as a first-class attack surface
and motivate stronger safeguards for feedback channels and optimization
frameworks.

</details>


### [35] [Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks](https://arxiv.org/abs/2510.14139)
*Islam Akef Ebeid,Haoteng Tang,Pengfei Gu*

Main category: cs.LG

TL;DR: ProtGram-DirectGCN 框架通过全局 n-gram 蛋白质图和定向图卷积实现低成本、鲁棒的PPI预测，在小样本条件下也表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖大规模的蛋白质语言模型，要么使用3D结构GNN，计算成本高；需要高效的替代方案来进行PPI预测。

Method: 两阶段图表示学习：第一阶段 ProtGram 构建蛋白质一级结构的全局 n-gram 有向图，边权为残基转移概率，边连接残基；第二阶段 DirectGCN 使用定制的有向图卷积层，分别处理进入、离开、无向路径及共享变换，并通过门控机制融合，得到残基嵌入后再通过注意力池化得到蛋白质级嵌入用于PPI预测。

Result: DirectGCN 在标准节点分类基准上与主流方法等价，且在复杂有向、密集异构图上表现突出；将 DirectGCN 应用于 ProtGram 的PPI预测任务时，框架展现出强鲁棒性，且在数据有限时仍保持良好性能。

Conclusion: 该框架为PPI预测提供高效且稳健的替代方案，适合资源受限场景并具良好可扩展性。

Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is
crucial for understanding cellular functions and advancing drug development.
Existing in-silico methods use direct sequence embeddings from Protein Language
Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein
structures. This study explores less computationally intensive alternatives. We
introduce a novel framework for downstream PPI prediction through link
prediction. Methods We introduce a two-stage graph representation learning
framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach
models a protein's primary structure as a hierarchy of globally inferred n-gram
graphs. In these graphs, residue transition probabilities define edge weights.
Each edge connects a pair of residues in a directed graph. The probabilities
are aggregated from a large corpus of sequences. Second, we propose DirectGCN,
a custom directed graph convolutional neural network. This model features a
unique convolutional layer. It processes information through separate
path-specific transformations: incoming, outgoing, and undirected. A shared
transformation is also applied. These paths are combined via a learnable gating
mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level
embeddings. These embeddings are pooled via attention to generate protein-level
embeddings for prediction. Results We first established the efficacy of
DirectGCN on standard node classification benchmarks. Its performance matches
established methods on general datasets. The model excels at complex, directed
graphs with dense, heterophilic structures. When applied to PPI prediction, the
full ProtGram-DirectGCN framework delivers robust predictive power. This strong
performance holds even with limited training data.

</details>


### [36] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 本文系统评估了多种先进的排序损失（点对点、成对、列表）在基于 Transformer 的日度股票回报预测中的排行能力，以及对基于排序的投资组合选择的影响，提供了实践中的损失函数选择指南。


<details>
  <summary>Details</summary>
Motivation: 金融市场的非平稳性和股票间复杂关系使得仅追求预测准确性的标准损失不足以训练出能正确排序未来回报的模型。尽管信息检索领域存在多种排序损失，但在用现代 Transformer 进行股票选择时缺乏系统比较。

Method: 在 S&P 500 数据上，系统地评估并比较包括点对点、成对、列表等在内的多种排序损失，对日度回报进行预测，评估模型在学习跨资产相对顺序及其随时间的变化方面的能力，并通过排名质量指标和基于排序的投资组合绩效进行对比。

Result: 综合基准显示，各损失函数对排序能力的影响显著不同，部分点对点、成对、列表损失在学习跨横截面和时间依赖方面更有利，提升了排序型投资策略的潜在效果。

Conclusion: 该工作填补了金融时间序列中的 Transformer 训练损失对排序性能影响的空缺，向实务提供如何在排序型股票投资策略中选择合适损失函数的可操作指南。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [37] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 本工作回顾并比较传统数据分析与张量方法在数据集表征中的应用，强调张量方法在可解释性与鲁棒性方面的潜在优势。


<details>
  <summary>Details</summary>
Motivation: 现有统计、结构和模型分析难以提供深层洞察与可解释性，需要更强的多维数据表征方式来支撑数据驱动的创新与解释。

Method: 系统性综述当前的常规数据分析技术，聚焦张量方法及其在数据集表征中的应用，辅以示例以展示其揭示数据特征的能力。

Result: 示例表明张量方法能够揭示细微的数据特征，提升可解释性与可行动性，并展示相较传统方法的潜在改进空间。

Conclusion: 提倡在数据表征中广泛采用张量方法，以推动对复杂数据的理解、智能化和可解释的数据驱动发现。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [38] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出可逆模型合并（RMM）以解决低秩压缩Adapter的合并问题，通过构建可重建的模型基底使任务可线性组合恢复各自模型，数据无依赖，提供闭式解，实验显示优于现有合并方法，保留低秩模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的模型合并在对低秩权重（LoRA/SVD）进行合并时会显著降性能，因此需要一个能够在不损失任务专用性能的前提下保留可回溯性的合并框架。

Method: 提出一个基底模型空间的表示：从中线性组合得到各任务专用模型的可恢复表示；给出选择基底和任务系数的闭式解；无需访问训练数据；可逆(可回退到任一任务模型)；实现并验证在不同数据集和模型规模上的有效性。

Result: 在多数据集与不同模型规模上，RMM持续优于现有合并方法，显著保留低秩压缩模型的性能。

Conclusion: RMM提供一个高效、灵活、数据无关的可逆合并框架，避免单一合并模型对任务的落后，同时保留对各任务模型的可回退能力。

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [39] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA是一个面向企业的多智能体注释框架，支持配置驱动的注释任务、在金融场景中有效消除积压、并实现与人类标注的高一致性与高效协作。


<details>
  <summary>Details</summary>
Motivation: 在金融服务等行业存在大规模客户 utterance 注释的积压，需要可扩展、可配置且可实际部署的系统来提升标注效率、降低人工成本，并允许通过配置而非代码变更来定义新的注释类型。

Method: MAFA通过多智能体协作、结构化推理和基于评审的共识机制实现注释任务的动态自适应。系统以配置驱动注释类型（FAQ、意图、实体等），支持跨语言和多数据集的应用，并在JPMorgan Chase等场景落地，形成生产级能力。

Result: 实现了对1百万条utterance积压的消除；与人工标注者的一致性约95%？（原文为86%），年节省超过5,000小时人工标注工作。系统在输入数据中对注释的置信度分布为高85%、中10%、低5%，有利于将模糊任务交给人类处理。相较基线，内部意图识别Top-1提升13.8%、Top-5提升15.1%、F1提升16.9%，且在公开基准上也表现出类似的改进。

Conclusion: 该工作将多智能体理论与企业落地应用结合，提供了一套可配置、可部署的注释解决方案蓝图，适用于面临类似大规模注释挑战的组织。

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [40] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA通过在扩散嵌入中引入对比学习来对齐潜在几何与系统动力学，从而在潜在空间实现非线性轨迹遍历与可控生成，在多领域数据集上优于线性遍历和基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的潜在空间尚未对动力学具有显式的可解释结构，难以实现精确且可控的生成。最近的对比学习研究表明，目标对比可以获得更解耦和结构化的表示。因此，本文提出在扩散潜变量层面通过对比学习组织潜在结构，使遍历方向能够反映基础动力学因素。

Method: 在扩散嵌入内应用对比学习，构建一个对比目标以对齐不同潜在表示与相应的动力学因素；通过对潜在流形进行非线性遍历，支持忠实的插值、外推和可控生成；并在流体动力学、神经钙成像、神经调控和人脸表情等基准数据集上评估。

Result: ConDA在多领域基准上实现了可解释的潜在表示，并相较线性遍历与基线的条件化方法具有更好的可控性和对动力学结构的保留，提升了插值与外推的忠实性。

Conclusion: Diffusion潜在空间确实编码了与动力学相关的结构，但要充分利用这一结构需在潜在层面对其进行组织并在潜在流形上进行遍历，ConDA提供了一种有效的方法来实现这一点。

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [41] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 激励机制对联邦学习的实际落地至关重要；本文/章节给出面向中心化与去中心化架构的完整分类法，结合经济学、博弈论、区块链和深度强化学习等技术，评估在医疗、智慧基础设施、车联网和区块链去中心化系统等领域的应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 解决参与方参与意愿不足、虚假参与（搭便车）等问题，确保参与者在隐私保护前提下愿意贡献数据和算力，从而实现可持续、鲁棒的联邦学习生态。

Method: 提出一个综合 taxonomy，覆盖基于理论（经济学、博弈论）与技术手段（区块链、深度强化学习）的激励设计；同时从应用视角分析集中式与去中心化架构及实际产业场景。

Result: 梳理出一系列有希望的激励解决方案及显著挑战，强调激励机制不仅是可选特征，而是联邦学习成功的关键组成部分，指向更可持续、公平、鲁棒的生态系统。

Conclusion: 未来需要进一步将理论与应用深化，完善可持续、公平与鲁棒的激励设计框架，并在现实场景中验证其有效性与可扩展性。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [42] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 首次对 QM9 数据集上多种分子表示的核岭回归进行系统谱分析，结果显示谱丰富性并不等同于更好性能，领先特征值主导信息，保留顶2%特征值即可几近恢复全部性能。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量关于核谱的研究，分子核谱的系统分析仍缺乏。本研究旨在揭示谱结构、表征方式与预测性能之间的关系，以及在数据有限情境下核方法的表现。

Method: 使用核岭回归在 QM9 上，对分子指纹、预训练 Transformer 表征、全局/局部 3D 表征等七种分子属性进行谱分析，比较四种谱度量；通过相关性分析（Pearson）研究谱丰富性与性能的关联；实现截断核，仅保留前2%特征值来考察谱与预测的关系。

Result: 发现比谱丰富性更重要的是前几特征值的贡献；在 transformer-based 和局部 3D 表征中，谱丰富性与性能甚至呈负相关；截断到顶2%特征值常常可几乎恢复所有预测性能，表明信息最丰富的特征来自少量主成分。

Conclusion: 这些结果挑战“谱越丰富越好”的直觉，强调表征与核特征之间的依赖关系，对评估核方法与自监督学习在数据受限任务中的有效性具有启示。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [43] [When Flatness Does (Not) Guarantee Adversarial Robustness](https://arxiv.org/abs/2510.14231)
*Nils Philipp Walter,Linara Adilova,Jilles Vreeken,Michael Kamp*

Main category: cs.LG

TL;DR: Flatness implies local but not global adversarial robustness; robust behavior requires curvature away from the data manifold; empirical evidence across architectures confirms that adversarial regions are large flat pockets with high-confidence misclassifications.


<details>
  <summary>Details</summary>
Motivation: Solve the incomplete, informal link between flat minima and robustness by formalizing how flatness relates to loss variation and adversarial vulnerability, distinguishing local vs. global robustness.

Method: Derive a closed-form expression for relative flatness in the penultimate layer; use it to bound loss variation in input space; extend the analysis to the whole network; show that maintaining global robustness requires the loss to curve sharply away from the data manifold.

Result: Theoretical predictions align with empirical results across architectures and datasets; reveal a geometric structure where flat regions can host confidently wrong predictions; establishes a nuanced connection between flatness and robustness, beyond simplistic interpretations.

Conclusion: Flatness alone does not guarantee global adversarial robustness; achieving nonlocal robustness requires curvature away from the data manifold. The work provides a refined understanding of how geometry governs adversarial vulnerability and links flatness to model confidence.

Abstract: Despite their empirical success, neural networks remain vulnerable to small,
adversarial perturbations. A longstanding hypothesis suggests that flat minima,
regions of low curvature in the loss landscape, offer increased robustness.
While intuitive, this connection has remained largely informal and incomplete.
By rigorously formalizing the relationship, we show this intuition is only
partially correct: flatness implies local but not global adversarial
robustness. To arrive at this result, we first derive a closed-form expression
for relative flatness in the penultimate layer, and then show we can use this
to constrain the variation of the loss in input space. This allows us to
formally analyze the adversarial robustness of the entire network. We then show
that to maintain robustness beyond a local neighborhood, the loss needs to
curve sharply away from the data manifold. We validate our theoretical
predictions empirically across architectures and datasets, uncovering the
geometric structure that governs adversarial vulnerability, and linking
flatness to model confidence: adversarial examples often lie in large, flat
regions where the model is confidently wrong. Our results challenge simplified
views of flatness and provide a nuanced understanding of its role in
robustness.

</details>


### [44] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: GenCluster是一个可扩展且可重复的测试时计算框架，通过大规模生成、行为聚类、排序和轮询提交，在有限验证预算下高效探索多样解空间，使开源模型达到IOI金牌级别并缩小与闭源系统的差距。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的推理和解决问题能力的一个关键场景是竞赛编程，IOI作为最具声望的竞赛之一，成为衡量人类与AI编程水平的基准。尽管一些专有模型声称达到IOI金牌水平，通常方法未公开，且用开源权重模型达到同等水平仍具挑战性，因此需要一个可扩展、可重复且公开的评测框架来推动进步。

Method: 在测试阶段通过GenCluster框架执行：生成大量尝试、基于行为特征进行聚类以覆盖不同解法、对解法进行排序以筛选潜在最佳方案，并采取轮询式提交策略在有限的验证预算内进行多轮尝试，系统性地探索多样化的解题空间。该框架结合大规模生成、行为聚类、候选解排序和轮询提交的组合，以提高用开源权重模型达到高水平解题能力的可能性。

Result: 实验结果显示，随着可用计算资源的增加，GenCluster的性能呈现稳定的扩展性，显著缩小了开源模型与闭源系统之间的差距；在理论与模拟评估中，显示有可能用开源模型在IOI达到金牌水平，尤其是采用gpt-oss-120b等大模型时，首次展现出在IOI 2025上实现金牌的潜力。

Conclusion: GenCluster为可重复、透明的推理评测提供了一个新范式，证明在开放模型下也可通过系统化的测试时计算和多轮尝试达到接近顶尖竞赛水平的表现，有助于推动竞赛编程与LLM推理研究的公开评测与对比。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [45] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 提出在线鲁棒策略优化算法 DR-RPO，通过参考策略正则化实现可 tractable 的鲁棒策略优化，结合 d-矩形线性 MDP 与上置信界探索，给出理论保证并在多域上验证。


<details>
  <summary>Details</summary>
Motivation: 在训练与部署环境分布不同时的决策问题中，鲁棒性与样本效率并重的在线 RL 尚缺乏充分理论支持与实用方法。

Method: 提出 DR-RPO，模型无关的在线策略优化方法；通过参考策略正则化实现对转移与策略的双重约束；在大规模状态-动作空间使用 d-矩形线性 MDP、线性函数逼近与上置信度探索实现乐观性探索。

Result: 给出理论保障：策略优化可实现多项式级别的次优界与样本效率，与值函数方法相当；通过多领域实验印证理论与鲁棒性。

Conclusion: DR-RPO 在鲁棒 RL 的在线设定下证明了策略优化的可行性与有效性，并具备可扩展性以应对大规模问题。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [46] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 对比一般化时序基金模与专用模在PPG信号上的表现，基于51项任务的综合基准，结果显示在全调优场景下，专用模型的胜率提高27%，同时对泛化、可迁移性、鲁棒性等作了分析。


<details>
  <summary>Details</summary>
Motivation: 探索通用时序基金模型是否能在跨域、跨任务场景中接近或超越专用模型，尤其是在PPG等生理信号领域，解决数据同质性问题与标注成本。

Method: 构建51项任务的评测集，涵盖心态/状态评估、实验室值估计、跨模态推断等七个维度；比较通用时序基金模型（如 MOMENT 等）与专用模型在不同微调策略下的表现，评估维度包括胜率、平均性能、特征质量、调优增益、性能方差、迁移性和可扩展性，并给出注意力可视化、训练数据选择对结果的影响等分析。

Result: 总体结果表明，专用模型在关键指标上仍具优势，特别是在全调优情况下；在跨域迁移、数据效率和鲁棒性方面，通用模型展现潜力，且可通过训练数据选择和微调策略提升表现。

Conclusion: 该研究提供一个较全面的比较框架，揭示通用模型与专用模型各自的优势与局限，强调训练数据质量与任务适配对性能的重要性，为设计时间序列基础模型提供参考。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [47] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: CAST是一个无探针的变换矩阵与谱分析框架，用以解释Transformer各层的功能。通过 Moore-Penrose伪逆估计每层的实际转换矩阵，并进行六维谱分析来刻画层行为；揭示编码器与解码器在层级上的差异。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型内部机制的多角度需求推动对可解释性工具的研究，现有方法各具优点但存在信息孤岛，因此需要一个综合且探针自由的视角来揭示层级功能。

Method: 计算每层的实际转换矩阵（使用 Moore-Penrose 伪逆），对矩阵进行谱分析并用六个可解释性指标刻画层行为；结合核分析/CKA评估层间功能关系，揭示层的阶段性模式。

Result: 发现解码器模型呈现压缩-扩张的循环行为，而编码器模型保持高秩的稳定处理；核分析显示层之间存在可重复的功能关系模式，CKA相似性矩阵将层划分为三个阶段：特征提取、压缩和专门化。

Conclusion: 提供了一种基于谱分析的探针自由框架来理解Transformer的层功能，与现有探针方法互补，能够揭示编码器-解码器在层级行为上的差异与共性。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [48] [Nonparametric Data Attribution for Diffusion Models](https://arxiv.org/abs/2510.14269)
*Yutian Zhao,Chao Du,Xiaosen Zheng,Tianyu Pang,Min Lin*

Main category: cs.LG

TL;DR: 提出一种非参数的数据归因方法，仅基于数据进行推断，通过生成图像的patch级相似性测量训练样本对输出的影响，适用于扩散模型，且无需梯度或重训练。


<details>
  <summary>Details</summary>
Motivation: 在专有或大规模场景中，往往无法访问模型梯度或重训练能力，迫切需要一种模型无关且基于数据的归因方法；同时希望得到可在空间上直观解读的归因，并揭示训练数据与输出之间的内在关系。

Method: 基于最优分数函数的解析形式，将归因扩展到多尺度表示，并通过卷积加速实现高效计算。方法完全基于数据，不依赖模型梯度或训练过程，输出具有空间可解释性的归因图，揭示训练数据与模型输出之间的内在关系。

Result: 实验表明该非参数方法的归因性能与梯度方法高度接近，且显著优于现有的非参数基线。代码公开（GitHub）。

Conclusion: 该方法提供了一种高效、模型无关的数据归因工具，适用于生成模型，且可扩展至多尺度表示，便于在无法获取梯度的场景中理解训练数据与输出之间的关系。

Abstract: Data attribution for generative models seeks to quantify the influence of
individual training examples on model outputs. Existing methods for diffusion
models typically require access to model gradients or retraining, limiting
their applicability in proprietary or large-scale settings. We propose a
nonparametric attribution method that operates entirely on data, measuring
influence via patch-level similarity between generated and training images. Our
approach is grounded in the analytical form of the optimal score function and
naturally extends to multiscale representations, while remaining
computationally efficient through convolution-based acceleration. In addition
to producing spatially interpretable attributions, our framework uncovers
patterns that reflect intrinsic relationships between training data and
outputs, independent of any specific model. Experiments demonstrate that our
method achieves strong attribution performance, closely matching gradient-based
approaches and substantially outperforming existing nonparametric baselines.
Code is available at https://github.com/sail-sg/NDA.

</details>


### [49] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: 提出 CAREBench——一个评估 EEP 的多模态基准，重点考量预测准确性与时间稳定性，覆盖表格 EHR、ECG 波形和文本输入，並引入基于局部 Lipschitz 常数的稳定性度量，揭示现有方法在追求高精度时的稳定性不足，尤其是大语言模型表现较差。


<details>
  <summary>Details</summary>
Motivation:  bedside trust 需求要求风险轨迹既精准又平滑且能随新证据更新；现有基准往往忽视稳定性并主要测试表格数据，无法反映真实的连续监测场景。

Method: 提出 CAREBench，包含六个预测任务（如败血症发作），比较传统学习器、深度序列模型与零-shot 大语言模型，使用多模态输入（表格 EHR、ECG 波形、临床文本），并引入基于局部 Lipschitz 常数的稳定性指标来惩罚短期波动。

Result:  across tasks, 现有方法，尤其是大语言模型，在同时优化准确性与稳定性方面表现欠佳；在高精度操作点的召回率也不理想。结果表明需要产生证据对齐、稳定的轨迹以提升临床信任度。

Conclusion: 稳定性应成为评估可部署性的重要维度，CAREBench 提供一个评估框架，推动开发能生成证据化且稳定的连续监测风险轨迹的模型。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [50] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 提出将光谱残余（SR）注意机制与Reservoir Computing（RC）结合的SR-RC，用于边缘设备的时序异常检测，提升精度且保持学习效率，具硬件友好性。


<details>
  <summary>Details</summary>
Motivation: RC在资源受限的边缘设备上需要小型的时序处理器，但单纯RC往往性能不足；注意机制若复杂会增加计算开销。SR方法是一种无学习成本的自下而上的注意机制，可能在不扩大 reservoir 的情况下提升性能并易于硬件实现。

Method: 将光谱残余（SR）方法作为注意机制嵌入到RC框架中，形成SR-RC，并在基准任务和真实世界时序数据集上进行评估比较。

Result: SR-RC在对比中优于传统RC与基于SR特征的逻辑回归模型，且与SR方法一样适合硬件实现，因此具有在边缘AI中部署RC进行时序异常检测的实际意义。

Conclusion: SR-RC为边缘时间序列异常检测提供一种实用路径，结合了无学习成本的SR注意机制与RC的学习效率，便于在硬件实现的边缘设备上部署。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>


### [51] [LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search](https://arxiv.org/abs/2510.14331)
*Shivam Singhal,Eran Malach,Tomaso Poggio,Tomer Galanti*

Main category: cs.LG

TL;DR: 通过语言模型引导的提议与验证框架 LLM-ERM，在不使用梯度更新的情况下，先用 LLM 生成若干候选短程序并逐一验证，达到接近有限类 ERM 的统计效率且具计算可行性。


<details>
  <summary>Details</summary>
Motivation: 存在的矛盾：短程序可通过长度优先枚举学习，但搜索随描述长度指数级增长；而基于梯度的训练减少了搜索成本但在某些短程序族上需要大量样本甚至过拟合。因此需要兼具样本效率与计算可行性的学习方案。

Method: 提出 LLM-ERM：不进行穷举枚举，而是使用预训练的推理增强型大语言模型来产生 k 个候选程序，然后对每个候选在数据上进行编译与验证，返回最佳 verified 假设，整个过程没有反馈、递归自适应或梯度更新。

Result: 理论上，坐标式在线小批量 SGD 需要大量样本才能学习某些短程序。实证方面，LLM-ERM 能以大约 200 个样本解决奇偶性变体、模式匹配和素性测试等任务，而以 SGD 训练的变换器在同等条件下即使达到 10^5 样本也易过拟合。

Conclusion: 语言引导的程序合成能恢复接近有限类 ERM 的统计效率，同时保持计算可行，为学习简洁假设提供实用路径，超越梯度训练的局限。

Abstract: We seek algorithms for program learning that are both sample-efficient and
computationally feasible. Classical results show that targets admitting short
program descriptions (e.g., with short ``python code'') can be learned with a
``small'' number of examples (scaling with the size of the code) via
length-first program enumeration, but the search is exponential in description
length. Consequently, Gradient-based training avoids this cost yet can require
exponentially many samples on certain short-program families.
  To address this gap, we introduce LLM-ERM, a propose-and-verify framework
that replaces exhaustive enumeration with an LLM-guided search over candidate
programs while retaining ERM-style selection on held-out data. Specifically, we
draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and
check each on the data, and return the best verified hypothesis, with no
feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise
online mini-batch SGD requires many samples to learn certain short programs.
{\em Empirically, LLM-ERM solves tasks such as parity variants, pattern
matching, and primality testing with as few as 200 samples, while SGD-trained
transformers overfit even with 100,000 samples}. These results indicate that
language-guided program synthesis recovers much of the statistical efficiency
of finite-class ERM while remaining computationally tractable, offering a
practical route to learning succinct hypotheses beyond the reach of
gradient-based training.

</details>


### [52] [Stop-RAG: Value-Based Retrieval Control for Iterative RAG](https://arxiv.org/abs/2510.14337)
*Jaewan Park,Solbee Cho,Jay-Yoon Lee*

Main category: cs.LG

TL;DR: Stop-RAG is a value-based stopping controller for iterative retrieval-augmented generation that adaptively decides when to stop retrieval, using full-width forward-view Q(λ) targets; it improves multi-hop QA by outperforming fixed-iteration baselines and prompting-based stopping.


<details>
  <summary>Details</summary>
Motivation: Iterative RAG improves reasoning for multi-hop questions but increases latency, cost, and the risk of introducing distracting evidence. There is a need for an adaptive stopping decision that truly reflects whether further retrieval will help.

Method: Formulate iterative RAG as a finite-horizon MDP and train a value-based controller Stop-RAG using full-width forward-view Q(λ) targets from complete trajectories. The approach is designed to be compatible with black-box APIs and existing pipelines.

Result: Stop-RAG consistently outperforms fixed-iteration baselines and prompting-based stopping on multi-hop QA benchmarks.

Conclusion: Adaptive stopping is a crucial component in agentic systems; value-based control can boost the accuracy of retrieval-augmented generation.

Abstract: Iterative retrieval-augmented generation (RAG) enables large language models
to answer complex multi-hop questions, but each additional loop increases
latency, costs, and the risk of introducing distracting evidence, motivating
the need for an efficient stopping strategy. Existing methods either use a
predetermined number of iterations or rely on confidence proxies that poorly
reflect whether more retrieval will actually help. We cast iterative RAG as a
finite-horizon Markov decision process and introduce Stop-RAG, a value-based
controller that adaptively decides when to stop retrieving. Trained with
full-width forward-view Q($\lambda$) targets from complete trajectories,
Stop-RAG learns effective stopping policies while remaining compatible with
black-box APIs and existing pipelines. On multi-hop question-answering
benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines
and prompting-based stopping with LLMs. These results highlight adaptive
stopping as a key missing component in current agentic systems, and demonstrate
that value-based control can improve the accuracy of RAG systems.

</details>


### [53] [Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis](https://arxiv.org/abs/2510.14342)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 提出一个几何化的自动微分框架，基于喷射束与 Weil 代数。逆向模式视为切张量的回推，Taylor 模式等价于在 Weil 代数中的评估；给出正确性、稳定性与复杂度的简洁表述，并指出张量化 Weil 代数可实现所有混合偏导的一次通过计算，成本与代数维度线性相关，避免嵌套 JVP/VJP 的组合爆炸；为深度学习与科学计算中的结构保持微分提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 建立一个统一的、几何化的自动微分理论框架，以分析和提升正确性、稳定性与效率，并解决高阶导数计算中的维度与组合爆炸问题，促使在深度学习和科学计算中发展结构保持的微分方法。

Method: 以微分几何中的喷射束（jet bundles）和 Weil 代数为工具，将逆向模式解释为 cotangent 回推，将 Taylor-mode 解释为在 Weil 代数上的评估；给出与逆向模式相关的函子恒等、以及高阶导数的代数精确性；利用张量化的 Weil 代数实现对所有混合导数的一次通过计算，成本与代数维度线性相关；从而避免多层 JVP/VJP 调度的组合爆炸。

Result: 提出的框架给出：1) 逆向模式的函子性恒等；2) 高阶导数的代数精确性；3) 截断误差的显式界；4) 通过张量化 Weil 代数实现混合导数的一次通过、成本与代数维度线性相关；5) 估计论文附带实现代码与示例，指明可在深度学习与科学计算中用于结构保持的微分方法的可能性。

Conclusion: 该几何化 AD 框架为 AD 理论提供了 differential geometry 视角与可操作的结构保持方法论基础，有望促进在深度学习与科学计算中开发更稳定、可解释且高效的微分算法。

Abstract: We present a geometric formulation of automatic differentiation (AD) using
jet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback,
while Taylor-mode corresponds to evaluation in a Weil algebra. From these
principles, we derive concise statements on correctness, stability, and
complexity: a functorial identity for reverse-mode, algebraic exactness of
higher-order derivatives, and explicit bounds on truncation error. We further
show that tensorized Weil algebras permit one-pass computation of all mixed
derivatives with cost linear in the algebra dimension, avoiding the
combinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD
theory through the lens of differential geometry and offers a foundation for
developing structure-preserving differentiation methods in deep learning and
scientific computing. Code and examples are available at
https://git.nilu.no/geometric-ad/jet-weil-ad.

</details>


### [54] [Revisit Modality Imbalance at the Decision Layer](https://arxiv.org/abs/2510.14411)
*Xiaoyu Ma,Hao Chen*

Main category: cs.LG

TL;DR: 多模态学习中的模态不平衡不仅在表示层存在，也在决策层显著体现；简单的输出融合会放大弱模态的不足，需在决策层引入自适应权重分配以实现相对平衡。


<details>
  <summary>Details</summary>
Motivation: 即使经过大量预训练和对模态进行平衡优化，模型在音频等主导模态上仍表现偏好，揭示模态不平衡的源头不仅来自优化动力学，还与特征空间和决策权重的固有差异有关。

Method: 在CREMAD和Kinetic-Sounds等音视频数据集上进行系统实验，分析表示层与决策层的模态权重分布，揭示不平衡的来源，并验证若将未经过标定的模态输出在融合阶段进行简单汇总会导致决策层权重偏差。

Result: 发现偏差根源在于特征空间和决策权重分布的固有差异，且在多模态融合时弱模态难以贡献；即使经过预训练和对模态的平衡优化，仍存在显著的模态偏好，尤其偏向音频。

Conclusion: 应在决策层引入自适应权重分配机制，使各模态的权重与其能力相匹配，提升弱模态的贡献。

Abstract: Multimodal learning integrates information from different modalities to
enhance model performance, yet it often suffers from modality imbalance, where
dominant modalities overshadow weaker ones during joint optimization. This
paper reveals that such an imbalance not only occurs during representation
learning but also manifests significantly at the decision layer. Experiments on
audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after
extensive pretraining and balanced optimization, models still exhibit
systematic bias toward certain modalities, such as audio. Further analysis
demonstrates that this bias originates from intrinsic disparities in
feature-space and decision-weight distributions rather than from optimization
dynamics alone. We argue that aggregating uncalibrated modality outputs at the
fusion stage leads to biased decision-layer weighting, hindering weaker
modalities from contributing effectively. To address this, we propose that
future multimodal systems should focus more on incorporate adaptive weight
allocation mechanisms at the decision layer, enabling relative balanced
according to the capabilities of each modality.

</details>


### [55] [Interaction Concordance Index: Performance Evaluation for Interaction Prediction Methods](https://arxiv.org/abs/2510.14419)
*Tapio Pahikkala,Riikka Numminen,Parisa Movahedi,Napsu Karmitsa,Antti Airola*

Main category: cs.LG

TL;DR: 提出了交互方向一致性指数 IC-index，用以评估预测模型在药物-靶标相互作用方向上的正确性，与传统 DTA 预测指标互补。


<details>
  <summary>Details</summary>
Motivation: 在药物-靶标相互作用中，单纯预测结合强度不足以捕捉交互效应的方向性。IC-index 旨在衡量预测是否正确地指出互动效应的方向，从而提升资源分配等决策。

Method: 定义 IC-index，研究其在无法捕捉交互的预测器上的不变性；分析学习算法对药物/靶标身份的置换等变性(permutation equivariance)如何导致在训练集中未出现的实体时无法捕捉交互；通过引入对药物和靶标的辅助信息来弥补；在多组生物互作数据集和多种算法上进行系统评估。

Result: IC-index 能补充现有预测性能评估，量化方向预测的正确性；理论结果表明若模型未能捕捉交互则 IC-index 不变；若模型对药物/靶标置换等变性，尤其在未见实体时难以捕捉交互；利用侧信息可缓解此问题；实验表明不同强度预测方法在 IC-index 上的表现差异，提供新的评估维度。

Conclusion: IC-index 为 DTA 预测的评估框架提供有价值的补充，强调在建模中关注交互方向性、并通过引入侧信息解决置换等变性；未来工作可进一步完善该指标及其与传统指标的结合。

Abstract: Consider two sets of entities and their members' mutual affinity values, say
drug-target affinities (DTA). Drugs and targets are said to interact in their
effects on DTAs if drug's effect on it depends on the target. Presence of
interaction implies that assigning a drug to a target and another drug to
another target does not provide the same aggregate DTA as the reversed
assignment would provide. Accordingly, correctly capturing interactions enables
better decision-making, for example, in allocation of limited numbers of drug
doses to their best matching targets. Learning to predict DTAs is popularly
done from either solely from known DTAs or together with side information on
the entities, such as chemical structures of drugs and targets. In this paper,
we introduce interaction directions' prediction performance estimator we call
interaction concordance index (IC-index), for both fixed predictors and machine
learning algorithms aimed for inferring them. IC-index complements the
popularly used DTA prediction performance estimators by evaluating the ratio of
correctly predicted directions of interaction effects in data. First, we show
the invariance of IC-index on predictors unable to capture interactions.
Secondly, we show that learning algorithm's permutation equivariance regarding
drug and target identities implies its inability to capture interactions when
either drug, target or both are unseen during training. In practical
applications, this equivariance is remedied via incorporation of appropriate
side information on drugs and targets. We make a comprehensive empirical
evaluation over several biomedical interaction data sets with various
state-of-the-art machine learning algorithms. The experiments demonstrate how
different types of affinity strength prediction methods perform in terms of
IC-index complementing existing prediction performance estimators.

</details>


### [56] [MergeMoE: Efficient Compression of MoE Models via Expert Output Merging](https://arxiv.org/abs/2510.14436)
*Ruijie Miao,Yilun Yao,Zihan Wang,Zhiming Wang,Bairen Yi,LingJun Liu,Yikai Zhao,Tong Yang*

Main category: cs.LG

TL;DR: MergeMoE提出一种基于优化的问题，将MoE的专家合并看作在前向计算中插入额外矩阵的过程，从而构造压缩矩阵，实现对MoE的有效压缩，在同等压缩比下实现对比基线的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决大规模Mixture-of-Experts (MoE) 模型的显存开销问题；尽管专家合并是一种压缩思路，但此前多从参数聚合角度理解，本工作从输出合并的角度给出理论分析，并提出一个可通过优化求解的压缩矩阵构造方法。

Method: 对专家合并进行理论分析，将合并等价为在前向过程插入额外矩阵，并据此建立一个优化框架以设计压缩矩阵。基于该分析提出MergeMoE算法，通过数学优化来确定压缩矩阵以实现MoE的高效压缩。并在多种MoE模型上进行实验评估。

Result: 在多种MoE模型上，MergeMoE在相同压缩比下持续优于基线方法，显示出稳定的压缩效果和性能提升。

Conclusion: 提供了一种基于优化的系统性方法来压缩MoE，利用输出级合并的等价性来构造压缩矩阵，MergeMoE在实验中实现了对同等压缩比基线的领先，具有良好的推广潜力。

Abstract: The Mixture-of-Experts (MoE) technique has proven to be a promising solution
to efficiently scale the model size, which has been widely applied in recent
LLM advancements. However, the substantial memory overhead of MoE models has
made their compression an important research direction. In this work, we
provide a theoretical analysis of expert merging, a recently proposed technique
for compressing MoE models. Rather than interpreting expert merging from the
conventional perspective of parameter aggregation, we approach it from the
perspective of merging experts' outputs. Our key insight is that the merging
process can be interpreted as inserting additional matrices into the forward
computation, which naturally leads to an optimization formulation. Building on
this analysis, we introduce MergeMoE, a method that leverages mathematical
optimization to construct the compression matrices. We evaluate MergeMoE on
multiple MoE models and show that our algorithm consistently outperforms the
baselines with the same compression ratios.

</details>


### [57] [A Free Lunch in LLM Compression: Revisiting Retraining after Pruning](https://arxiv.org/abs/2510.14444)
*Moritz Wagner,Christophe Roux,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 在每个Transformer块内对注意力和MLP等子组件进行独立重构，在资源有限的情况下实现近乎最佳的性能与最优的资源权衡，甚至优于全量再训练。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型裁剪后需要大量再训练的计算成本问题；探究块级重构是否能有效恢复裁剪带来的性能损失，并评估其对效率的影响。

Method: 对最新GPT架构进行大规模计算研究，比较不同裁剪后的重构粒度（单矩阵重构 vs 整个Transformer块重构），评估简单与复杂裁剪准则（如 Wanda）的效果，关注 perplexity、内存占用和 Pareto 前沿。

Result: 提出 Pareto 最优的重构策略：在每个Transformer块中分开重构注意力与MLP，能以较低内存需求达到甚至优于全量再训练的 perplexity，且相对简单的 Wanda 在正确执行的前提下往往优于更复杂的方法。

Conclusion: 打破“应完全避免再训练”的直觉：通过恰当的重构策略，裁剪后仍能获得更好的性能与效率平衡，为LLMs的裁剪后性能恢复提供新设计思路。

Abstract: While Neural Network pruning typically requires retraining the model to
recover pruning-induced performance degradation, state-of-the-art Large
Language Models (LLMs) pruning methods instead solve a layer-wise mask
selection and reconstruction problem on a small set of calibration data to
avoid full retraining, as it is considered computationally infeasible for LLMs.
Reconstructing single matrices in isolation has favorable properties, such as
convexity of the objective and significantly reduced memory requirements
compared to full retraining. In practice, however, reconstruction is often
implemented at coarser granularities, e.g., reconstructing a whole transformer
block against its dense activations instead of a single matrix. In this work,
we study the key design choices when reconstructing or retraining the remaining
weights after pruning. We conduct an extensive computational study on
state-of-the-art GPT architectures, and report several surprising findings that
challenge common intuitions about retraining after pruning. In particular, we
observe a free lunch scenario: reconstructing attention and MLP components
separately within each transformer block is nearly the most resource-efficient
yet achieves the best perplexity. Most importantly, this Pareto-optimal setup
achieves better performance than full retraining, despite requiring only a
fraction of the memory. Furthermore, we demonstrate that simple and efficient
pruning criteria such as Wanda can outperform much more complex approaches when
the reconstruction step is properly executed, highlighting its importance. Our
findings challenge the narrative that retraining should be avoided at all costs
and provide important insights into post-pruning performance recovery for LLMs.

</details>


### [58] [Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits](https://arxiv.org/abs/2510.14445)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: GANs trained on process-based 3D fluvial deposits can reproduce non-stationary geological structures, remain stable, and respect geological principles; robustness promising but needs testing on larger 3D and multimodal data.


<details>
  <summary>Details</summary>
Motivation: To generate realistic subsurface geological structures with uncertainty, overcoming difficulties in reproducing continuity in fluvial deposits, by leveraging GANs trained on data from a process-based geological model.

Method: An ablation study transferring generation techniques from large 2D images to 3D fluvial deposit images; training GANs on samples produced by a process-based model; using deposition time to validate that samples honor the law of superposition; evaluating stability and avoidance of mode collapse; exploring inclusion of properties beyond usual physical properties.

Result: Training remains stable; generated 3D samples capture non-stationarity and fine details without mode collapse or memorization; process-based data enables including additional properties; validation via superposition supports fidelity of the samples;GANs demonstrate robustness for targeted geological structures.

Conclusion: GANs are promising for 3D geological structure generation (e.g., fluvial deposits); robustness appears higher than commonly credited for this targeted task; future work needed to extend to larger 3D images and multimodal datasets and to further exploit geological principles.

Abstract: The distribution of resources in the subsurface is deeply linked to the
variations of its physical properties. Generative modeling has long been used
to predict those physical properties while quantifying the associated
uncertainty. But current approaches struggle to properly reproduce geological
structures, and fluvial deposits in particular, because of their continuity.
This study explores whether a generative adversarial network (GAN) - a type of
deep-learning algorithm for generative modeling - can be trained to reproduce
fluvial deposits simulated by a process-based model - a more expensive model
that mimics geological processes. An ablation study shows that developments
from the deep-learning community to generate large 2D images are directly
transferable to 3D images of fluvial deposits. Training remains stable, and the
generated samples reproduce the non-stationarity and details of the deposits
without mode collapse or pure memorization of the training data. Using a
process-based model to generate those training data allows us to include
valuable properties other than the usual physical properties. We show how the
deposition time let us monitor and validate the performance of a GAN by
checking that its samples honor the law of superposition. Our work joins a
series of previous studies suggesting that GANs are more robust that given
credit for, at least for training datasets targeting specific geological
structures. Whether this robustness transfers to larger 3D images and
multimodal datasets remains to be seen. Exploring how deep generative models
can leverage geological principles like the law of superposition shows a lot of
promise.

</details>


### [59] [Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints](https://arxiv.org/abs/2510.14449)
*Jahidul Arafat,Fariha Tasmin,Md Kaosar Uddin,Sanjaya Poudel,Eftakhar Ahmed Arnob*

Main category: cs.LG

TL;DR: 在Wine数据集上比较自实现的One-vs-Rest逻辑回归与scikit-learn优化器，揭示了训练速度与分类准确性的权衡，以及L1正则化带来的特征稀疏与解释性提升，同时提出了高效、低成本的5特征子集用于实时质量控制。


<details>
  <summary>Details</summary>
Motivation: 研究在分析化学生产场景中，需在准确性、特征维度与可解释性之间权衡，并探索在资源受限环境中的可部署性与成本效益。

Method: 对UCI Wine数据集（178样本，3个葡萄品种，13个化学特征）应用One-vs-Rest逻辑回归，比较自定义梯度下降实现与scikit-learn的优化求解器；研究L1正则化对特征稀疏性的影响；进行特征子集搜索，评估总体 accuracy、训练/推理时间和成本节约，分析不同品种的化学特征签名及预测延迟。

Result: 手工梯度下降实现达到平均测试准确率92.59%，收敛平滑；scikit-learn实现达到98.15%准确率，并实现约24倍的训练速度提升。L1正则化实现54–69%的特征减少，同时仅造成约4.63%的准确率下降。提出一个5特征子集，能实现约62%的复杂度降低，预测准确率估计在92–94%之间，同时实现每样本80美元的成本节省和约56%的时间缩短，预测时延低于2毫秒，利于实时质量控制。

Conclusion: 研究为实践者提供在全面化学分析与目标化特征测量之间的权衡思路，提出在资源受限环境中实现成本效益与可解释性的可部署策略，并强调在真实时间场景中的稳健泛化与低延迟。

Abstract: Multi-class wine classification presents fundamental trade-offs between model
accuracy, feature dimensionality, and interpretability - critical factors for
production deployment in analytical chemistry. This paper presents a
comprehensive empirical study of One-vs-Rest logistic regression on the UCI
Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing
from-scratch gradient descent implementation against scikit-learn's optimized
solvers and quantifying L1 regularization effects on feature sparsity. Manual
gradient descent achieves 92.59 percent mean test accuracy with smooth
convergence, validating theoretical foundations, though scikit-learn provides
24x training speedup and 98.15 percent accuracy. Class-specific analysis
reveals distinct chemical signatures with heterogeneous patterns where color
intensity varies dramatically (0.31 to 16.50) across cultivars. L1
regularization produces 54-69 percent feature reduction with only 4.63 percent
accuracy decrease, demonstrating favorable interpretability-performance
trade-offs. We propose an optimal 5-feature subset achieving 62 percent
complexity reduction with estimated 92-94 percent accuracy, enabling
cost-effective deployment with 80 dollars savings per sample and 56 percent
time reduction. Statistical validation confirms robust generalization with
sub-2ms prediction latency suitable for real-time quality control. Our findings
provide actionable guidelines for practitioners balancing comprehensive
chemical analysis against targeted feature measurement in resource-constrained
environments.

</details>


### [60] [From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?](https://arxiv.org/abs/2510.14488)
*Sujai Hiremath,Dominik Janzing,Philipp Faller,Patrick Blöbaum,Elke Kirschbaum,Shiva Prasad Kasiviswanathan,Kyra Gan*

Main category: cs.LG

TL;DR: G2G 框架通过让专家猜测引导统计检验序列，同时保持统计一致性；提供 PC-Guess 和 gPC-Guess，理论上对专家误差鲁棒并在有质量输入时提升有限样本性能，实证呈现随专家准确度上升的单调改进，gPC-Guess 表现更强。


<details>
  <summary>Details</summary>
Motivation: 在样本有限的情况下，因果发现算法性能受限。尽管将专家知识（包括来自大语言模型）作为约束有望提升，但现有方法的正确性保证往往要求预测或不确定性估计完全正确，现实中难以实现。因此需要一种在保持严谨性的前提下利用专家输入的办法。

Method: G2G 将专家猜测作为引导，决定统计检验的顺序和取舍，而不是直接用来替代检验。提出两个实现：PC-Guess，扩展 PC 算法；gPC-Guess，加入学习组件以更好利用高质量专家输入。

Result: 理论上，两种方法在专家出错的情况下仍然正确；在有限样本情形下，若专家输入优于随机，gPC-Guess 相对于非增强版本具有更好性能。经验上，随着专家准确度提高，性能单调提升，且 gPC-Guess 的提升显著。

Conclusion: G2G 提供一种在不牺牲正确性的前提下利用专家知识的途径，尤其是高质量输入时，gPC-Guess 能带来更大收益，鼓励将 LLMs 等专家知识融入因果发现的测试策略中。

Abstract: Causal discovery algorithms often perform poorly with limited samples. While
integrating expert knowledge (including from LLMs) as constraints promises to
improve performance, guarantees for existing methods require perfect
predictions or uncertainty estimates, making them unreliable for practical use.
We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide
the sequence of statistical tests rather than replacing them. This maintains
statistical consistency while enabling performance improvements. We develop two
instantiations of G2G: PC-Guess, which augments the PC algorithm, and
gPC-Guess, a learning-augmented variant designed to better leverage
high-quality expert input. Theoretically, both preserve correctness regardless
of expert error, with gPC-Guess provably outperforming its non-augmented
counterpart in finite samples when experts are "better than random."
Empirically, both show monotonic improvement with expert accuracy, with
gPC-Guess achieving significantly stronger gains.

</details>


### [61] [Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals](https://arxiv.org/abs/2510.14503)
*Andrejs Sorstkins,Omer Tariq,Muhammad Bilal*

Main category: cs.LG

TL;DR: 提出可逆学习框架，通过 Phi(s,a) 衡量状态—行动的可逆性并引入选择性回滚，在 TD 更新中动态调整惩罚，从而提升值基 RL 的安全性、鲁棒性和效率；在 CliffWalking v0 与 Taxi v3 上显著提升性能并显著降低风险。


<details>
  <summary>Details</summary>
Motivation: 解决值过高估计和在部分不可逆环境中的不稳定性问题，提升价值基强化学习的安全性、稳定性和效率。

Method: 引入 per-state action estimator Phi(s,a) 以在固定 horizon K 内估计回到先前状态的可能性；在 TD 更新中据此动态调整惩罚项；设计选择性回滚操作：当某行动的预期回报显著低于当前估计价值且超过阈值时，对前一状态进行回退，中断低风险的高风险轨迹；通过可逆性感知评估与回滚相结合改进学习。

Result: 在 CliffWalking v0 中，框架将灾难性坠落降低超过 99.8%，平均回报提升约 55%；在 Taxi v3 中，非法动作抑制率达 99.9% 以上，累计奖励提升约 65.7%，同时方差显著下降。消融实验表明回滚是关键贡献。

Conclusion: 将可逆性感知评估与有针对性的回滚结合，显著提升安全性、性能和稳定性，为安全可靠的序列决策提供鲁棒路径。

Abstract: This paper proposes a reversible learning framework to improve the robustness
and efficiency of value based Reinforcement Learning agents, addressing
vulnerability to value overestimation and instability in partially irreversible
environments. The framework has two complementary core mechanisms: an
empirically derived transition reversibility measure called Phi of s and a, and
a selective state rollback operation. We introduce an online per state action
estimator called Phi that quantifies the likelihood of returning to a prior
state within a fixed horizon K. This measure is used to adjust the penalty term
during temporal difference updates dynamically, integrating reversibility
awareness directly into the value function. The system also includes a
selective rollback operator. When an action yields an expected return markedly
lower than its instantaneous estimated value and violates a predefined
threshold, the agent is penalized and returns to the preceding state rather
than progressing. This interrupts sub optimal high risk trajectories and avoids
catastrophic steps. By combining reversibility aware evaluation with targeted
rollback, the method improves safety, performance, and stability. In the
CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8
percent and yielded a 55 percent increase in mean episode return. In the Taxi
v3 domain, it suppressed illegal actions by greater than or equal to 99.9
percent and achieved a 65.7 percent improvement in cumulative reward, while
also sharply reducing reward variance in both environments. Ablation studies
confirm that the rollback mechanism is the critical component underlying these
safety and performance gains, marking a robust step toward safe and reliable
sequential decision making.

</details>


### [62] [Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective](https://arxiv.org/abs/2510.14510)
*Xingjian Wu,Xiangfei Qiu,Hanyin Cheng,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 提出 Selective Representation Space (SRS) 模块，通过可学习的 Selective Patching 与 Dynamic Reassembly，从上下文时间序列中自适应选择并打乱最具信息量的补丁，以提升基于补丁的时间序列预测性能；以 SRSNet（SRS + MLP 头）实现了 state-of-the-art，并且具备插件化能力。


<details>
  <summary>Details</summary>
Motivation: 传统补丁化将时间序列分割为相邻补丁，导致表示空间固定且表达力不足，难以充分捕获长程依赖。需构建更灵活的表示空间，筛选最具信息量的补丁以提升预测性能。

Method: 提出 Selective Representation Space (SRS) 模块，结合可学习的 Selective Patching 与 Dynamic Reassembly，自适应地从上下文中选择并重排补丁；将 SRS 嵌入现有补丁化模型，形成 SRSNet（SRS + MLP 头），并作为插件应用于其他模型。

Result: 在来自多个领域的真实世界数据集上，SRSNet 取得了 state-of-the-art 的性能；SRS 还能作为插件提升现有补丁化模型的表现。

Conclusion: SRS 模块有效提升基于补丁的时间序列预测性能，具备良好的通用性和插件性，适合作为未来时间序列建模中的灵活表示空间构建方案。

Abstract: Time Series Forecasting has made significant progress with the help of
Patching technique, which partitions time series into multiple patches to
effectively retain contextual semantic information into a representation space
beneficial for modeling long-term dependencies. However, conventional patching
partitions a time series into adjacent patches, which causes a fixed
representation space, thus resulting in insufficiently expressful
representations. In this paper, we pioneer the exploration of constructing a
selective representation space to flexibly include the most informative patches
for forecasting. Specifically, we propose the Selective Representation Space
(SRS) module, which utilizes the learnable Selective Patching and Dynamic
Reassembly techniques to adaptively select and shuffle the patches from the
contextual time series, aiming at fully exploiting the information of
contextual time series to enhance the forecasting performance of patch-based
models. To demonstrate the effectiveness of SRS module, we propose a simple yet
effective SRSNet consisting of SRS and an MLP head, which achieves
state-of-the-art performance on real-world datasets from multiple domains.
Furthermore, as a novel plugin-and-play module, SRS can also enhance the
performance of existing patch-based models. The resources are available at
https://github.com/decisionintelligence/SRSNet.

</details>


### [63] [On the Identifiability of Tensor Ranks via Prior Predictive Matching](https://arxiv.org/abs/2510.14523)
*Eliezer da Silva,Arto Klami,Diego Mesquita,Iñigo Urteaga*

Main category: cs.LG

TL;DR: 提出一个将先验预测的矩条件转化为关于边际矩、先验超参数和秩的对数线性系统的方法，从而把秩可 identifiability 转化为求解该系统的可解性。对四种基础张量模型进行应用：CP/PARAFAC、Tensor Train、Tensor Ring 的线性/链状/闭环结构使系统可解，因此秩可识别；而 Tucker 的对称拓扑导致系统欠定，从而秩不可由此方法辨识。对于可识别模型，给出基于观测数据矩的闭式秩估计量，并在数据上进行经验验证和鲁棒性评估。


<details>
  <summary>Details</summary>
Motivation: 在张量分解中，如何为潜在秩（维度）选择一个合理的值一直是一个核心但通常依赖启发式的方法的问题。缺乏严格的可辨识性准则可能导致过拟合或欠拟合。本文旨在提供一个以概率张量模型为背景、基于先验预测的矩匹配的严格可辨识性框架。

Method: 将一组矩匹配条件转化为关于边际矩、先验超参数与秩的对数线性方程组，秩可辨识性等价于该系统的可解性。并分别对 CP、Tensor Train、Tensor Ring、以及 Tucker 四种模型进行分析：前者的线性、后两者的链状/闭环结构使系统可解，从而秩可辨识；而 Tucker 的对称拓扑导致系统欠定。对于可辨识模型，给出基于观测数据矩的闭式秩估计量。

Result: 对 CP、Tensor Train、Tensor Ring 模型，系统可解，秩可辨识，且给出观测矩的闭式估计量；对 Tucker 模型，系统为欠定，秩不可由此法辨识。实验部分验证了可辨识模型的秩估计量在数据上的有效性与鲁棒性。

Conclusion: 提供了一个通用、严格的秩可辨识性框架，能从观测数据的矩出发，仅利用矩的分布信息进行秩的识别与估计，并对四种基础张量模型给出明确的结论与闭式估计量，具有理论和实践意义。

Abstract: Selecting the latent dimensions (ranks) in tensor factorization is a central
challenge that often relies on heuristic methods. This paper introduces a
rigorous approach to determine rank identifiability in probabilistic tensor
models, based on prior predictive moment matching. We transform a set of moment
matching conditions into a log-linear system of equations in terms of marginal
moments, prior hyperparameters, and ranks; establishing an equivalence between
rank identifiability and the solvability of such system. We apply this
framework to four foundational tensor-models, demonstrating that the linear
structure of the PARAFAC/CP model, the chain structure of the Tensor Train
model, and the closed-loop structure of the Tensor Ring model yield solvable
systems, making their ranks identifiable. In contrast, we prove that the
symmetric topology of the Tucker model leads to an underdetermined system,
rendering the ranks unidentifiable by this method. For the identifiable models,
we derive explicit closed-form rank estimators based on the moments of observed
data only. We empirically validate these estimators and evaluate the robustness
of the proposal.

</details>


### [64] [MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving](https://arxiv.org/abs/2510.14557)
*Jungi Lee,Junyong Park,Soohyun Cha,Jaehoon Cho,Jaewoong Sim*

Main category: cs.LG

TL;DR: MX+是一种针对块浮点(BFP)的非侵入式扩展，通过将离群值块中的指数位改造为扩展尾数，提升极低比特宽度下的语言模型推理精度，同时保持存储与计算开销极小，优于MXFP4/MXFP6。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型对低精度数据格式的需求日益增长，但现有超低比特BFP变体在块内离群值上易导致性能下降，且需要对软件框架或硬件厂商进行较大修改。本文以行业主导的BFP变体为对象，分析其局限性，提出非侵入且易于落地的扩展，以提升LLM推理的性价比。

Method: 分析现有工业驱动的BFP变体及其在离群值处理上的不足；设计MX+扩展：在不改变元素数据类型的前提下，将块中离群元素的指数字段改造为扩展尾数以提高精度；将MX+集成到微缩放(mx)格式家族中；通过在4-bit MX格式(MXFP4)和其他MX变体上对比评估，考察在不同离群分布下的模型精度、存储开销和运行时开销。

Result: 实验结果显示，MX+在语言模型性能方面显著高于MXFP4，同时几乎不增加存储开销和运行时延迟，提供了比MXFP4或MXFP6更具吸引力的高效推理方案。

Conclusion: MX+为低比特宽度的大型语言模型推理提供了一种成本效益高且非侵入的BFP扩展，易于与MX格式并人、在实际部署中可替代MXFP4/6以实现更高的推理效率。

Abstract: Reduced-precision data formats are crucial for cost-effective serving of
large language models (LLMs). While numerous reduced-precision formats have
been introduced thus far, they often require intrusive modifications to the
software frameworks or are rather unconventional for widespread adoption across
hardware vendors. In this paper, we instead focus on recent industry-driven
variants of block floating-point (BFP) formats and conduct a comprehensive
analysis to push their limits for efficient LLM serving. Our analysis shows
that existing ultra low-bit BFP variants struggle to provide reasonable
language model performance due to outlier values in blocks. To address the
outliers with BFPs, we propose MX+, a cost-effective and non-intrusive
extension designed for seamless integration into the microscaling (MX) formats.
MX+ builds on the key insight that the outlier does not need to use its
exponent field in the element data type, which allows us to repurpose the
exponent field as an extended mantissa to increase the precision of the outlier
element. Our evaluation shows that MX+ achieves significantly higher model
performance compared to the 4-bit MX format (MXFP4) with negligible storage
overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6
for efficient LLM inference.

</details>


### [65] [Redundancy-Aware Test-Time Graph Out-of-Distribution Detection](https://arxiv.org/abs/2510.14562)
*Yue Hou,He Zhu,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: 提出 RedOUT：一种无监督的图数据测试时 OOD 检测框架，通过 ReGIB 将结构熵纳入信息瓶颈，降低冗余并提升 OOD 探测性能。


<details>
  <summary>Details</summary>
Motivation: 训练与测试分布差异导致 OOD 下预测不准确；现有图 OOD 检测受结构冗余影响导致语义偏移，需改进对本质信息的提取与冗余信息的抑制。

Method: 引入 Redundancy-aware Graph Information Bottleneck (ReGIB)，将目标函数拆解为本质信息与冗余信息，最小化结构熵以降低冗余，并给出优化的上确界与下确界。

Result: 在真实数据集上验证了 RedOUT 在 OOD 检测上的优越性，平均提升约 6.7%，在 ClinTox/LIPO 数据集对上领先最佳对手 17.3%。

Conclusion: 通过抑制结构冗余并在测试时结合结构熵，RedOUT 能提升图分类的 OOD 检测性能，并给出理论界限与实证支持。

Abstract: Distributional discrepancy between training and test data can lead models to
make inaccurate predictions when encountering out-of-distribution (OOD) samples
in real-world applications. Although existing graph OOD detection methods
leverage data-centric techniques to extract effective representations, their
performance remains compromised by structural redundancy that induces semantic
shifts. To address this dilemma, we propose RedOUT, an unsupervised framework
that integrates structural entropy into test-time OOD detection for graph
classification. Concretely, we introduce the Redundancy-aware Graph Information
Bottleneck (ReGIB) and decompose the objective into essential information and
irrelevant redundancy. By minimizing structural entropy, the decoupled
redundancy is reduced, and theoretically grounded upper and lower bounds are
proposed for optimization. Extensive experiments on real-world datasets
demonstrate the superior performance of RedOUT on OOD detection. Specifically,
our method achieves an average improvement of 6.7%, significantly surpassing
the best competitor by 17.3% on the ClinTox/LIPO dataset pair.

</details>


### [66] [Selective Labeling with False Discovery Rate Control](https://arxiv.org/abs/2510.14581)
*Huipeng Huang,Wenbo Liao,Huajun Xi,Hao Zeng,Mengchen Zhao,Hongxin Wei*

Main category: cs.LG

TL;DR: Conformal Labeling 提供基于FAIR（FDR）控制的 AI 标签信任度识别，通过对比 AI 预测置信度与经校准数据的错误标注，给出每个实例的 conformal p 值，并据此据数据自适应阈值实现 FDR 控制，具有高效性和适用于图像、文本及 LLM QA 的证据支持。


<details>
  <summary>Details</summary>
Motivation: 获得大规模数据的高质量标签成本高昂，现有的基于选择性人工标注的方法缺乏对 AI 标签质量的理论保证，导致 AI 标签子集的错误率不可控。需要一种能对 AI 预测进行可信性认证并提供理论保障的标注方法。

Method: 为每个测试实例构造 conformal p 值，比较 AI 模型的预测置信度与由 AI 出错的校准实例的置信度。然后选取 p 值低于数据自适应阈值的实例，证明这些 AI 预测在所选子集内的错误率受控（FDR 控制在名义水平以下）。提供对 FDR 的理论保证，并在多任务（图像、文本标注、LLM QA）上进行广泛实验以评估功效。

Result: 方法在多种任务中实现了严格的 FDR 控制并具有较高的检出能力（power），显著提升 AI 标签子集的可信度，相较于传统的选择性标注方法具有更强的理论保证与实用性。

Conclusion: Conformal Labeling 提供了一种可验证、可控的 AI 标签信任框架，通过构建个体化的 conformal p 值并实现 FDR 控制，达到在保持成本效益的同时确保较低错误率的目标。

Abstract: Obtaining high-quality labels for large datasets is expensive, requiring
massive annotations from human experts. While AI models offer a cost-effective
alternative by predicting labels, their label quality is compromised by the
unavoidable labeling errors. Existing methods mitigate this issue through
selective labeling, where AI labels a subset and human labels the remainder.
However, these methods lack theoretical guarantees on the quality of
AI-assigned labels, often resulting in unacceptably high labeling error within
the AI-labeled subset. To address this, we introduce \textbf{Conformal
Labeling}, a novel method to identify instances where AI predictions can be
provably trusted. This is achieved by controlling the false discovery rate
(FDR), the proportion of incorrect labels within the selected subset. In
particular, we construct a conformal $p$-value for each test instance by
comparing AI models' predicted confidence to those of calibration instances
mislabeled by AI models. Then, we select test instances whose $p$-values are
below a data-dependent threshold, certifying AI models' predictions as
trustworthy. We provide theoretical guarantees that Conformal Labeling controls
the FDR below the nominal level, ensuring that a predefined fraction of
AI-assigned labels is correct on average. Extensive experiments demonstrate
that our method achieves tight FDR control with high power across various
tasks, including image and text labeling, and LLM QA.

</details>


### [67] [Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically Valid Molecular Docking](https://arxiv.org/abs/2510.14586)
*Daria Frolova,Talgat Daulbaev,Egor Sevryugov,Sergei A. Nikolenko,Dmitry N. Ivankov,Ivan Oseledets,Marina A. Pak*

Main category: cs.LG

TL;DR: Matcha is a three-stage docking pipeline that uses flow matching across geometric spaces (R^3, SO(3), SO(2)) with learned scoring and unsupervised physical validity filters, achieving higher docking success and plausibility and about 25× faster than large-scale co-folding models; code is open-source.


<details>
  <summary>Details</summary>
Motivation: Accurate protein–ligand docking requires speed, accuracy, and physical plausibility. Existing methods often trade off these goals or struggle with geometric consistency and post-docking filtering.

Method: Three sequential stages implemented as flow matching models operating on R^3, SO(3), and SO(2). A dedicated scoring model assesses docking quality, followed by unsupervised physical validity filters to remove unrealistic poses. The pipeline refines docking predictions stepwise and integrates all components in a single inference flow.

Result: Matcha achieves superior docking success rate and physical plausibility on Astex and PDBbind test sets compared with baseline methods; approx 25× faster than modern large-scale co-folding models.

Conclusion: The combination of multi-stage flow matching, learned scoring, and physics-aware filtering yields fast, accurate, and physically plausible docking, with open-source code and weights available for reproducibility.

Abstract: Accurate prediction of protein-ligand binding poses is crucial for
structure-based drug design, yet existing methods struggle to balance speed,
accuracy, and physical plausibility. We introduce Matcha, a novel molecular
docking pipeline that combines multi-stage flow matching with learned scoring
and physical validity filtering. Our approach consists of three sequential
stages applied consecutively to refine docking predictions, each implemented as
a flow matching model operating on appropriate geometric spaces
($\mathbb{R}^3$, $\mathrm{SO}(3)$, and $\mathrm{SO}(2)$). We enhance the
prediction quality through a dedicated scoring model and apply unsupervised
physical validity filters to eliminate unrealistic poses. Compared to various
approaches, Matcha demonstrates superior performance on Astex and PDBbind test
sets in terms of docking success rate and physical plausibility. Moreover, our
method works approximately 25 times faster than modern large-scale co-folding
models. The model weights and inference code to reproduce our results are
available at https://github.com/LigandPro/Matcha.

</details>


### [68] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: 提出一种面向模态的混合检索架构 MAHA，用于多模态问答，结合密集向量检索与结构化知识图，处理非结构化多模态文档并实现跨模态推理，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）多为单模态文本处理，难以对包含文本、图像、表格、公式、图形等多模态信息的非结构化文档进行高效检索与推理，需要结合嵌入式语义检索和显式文档结构以实现跨模态理解。

Method: 提出 MAHA 架构，将密集向量检索与结构化图遍历相结合；通过模态感知的知识图对跨模态语义和关系进行编码，实现对不同模态的语义检索和上下文感知检索。

Result: 在多个基准数据集上，MAHA 显著优于基线方法，取得 ROUGE-L 0.486 的成绩并实现对所有模态的覆盖，表明将嵌入与显式文档结构相结合的检索策略在多模态检索中具有优势。

Conclusion: MAHA 提供了一个可扩展且可解释的检索框架，通过模态感知的推理能力提升对非结构化多模态数据的检索与问答表现，推动了RAG 系统在多模态检索方面的发展。

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [69] [First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training](https://arxiv.org/abs/2510.14614)
*Gyudong Kim,Hyukju Na,Jin Hyeon Kim,Hyunsung Jang,Jaemin Park,Jaegi Hwang,Namkoo Ha,Seungryong Kim,Young Geun Kim*

Main category: cs.LG

TL;DR: 提出 FAL 与 FAL+ 以降低 Transformer 的通信开销，通过重路由第一层 MHA 的输出到下一层 MLP 的输入，取消跨块 MHA-MLP 连接，从而实现 MHA 和 MLP 的并行执行；FAL 在多 GPU 训练中显著提速，FAL+ 进一步提升模型质量。


<details>
  <summary>Details</summary>
Motivation: 在亿级规模变换器并行训练中，多 GPU 的分布式训练存在高通信开销，尤其是在张量并行 TP 下，每个块的 MHA-MLP 连接需要进行 all-reduce，成为性能瓶颈；需要降低这种通信成本并提升吞吐。

Method: 通过将第一层的 MHA 输出重定向并作为下一层 MLP 输入，绕过每个块的 MHA-MLP 连接，并使 MHA 与 MLP 在同一 GPU 上并行执行；FAL+ 在 MHA 输出中增加归一化的第一注意力输出以增强 MLP 输入，从而提高模型质量。

Result: 在多 GPU 场景下，FAL 将训练时间缩短最多 44%，单 GPU 吞吐量提升最多 1.18×，相比基线 GPT 拥有更好 perplexity；FAL+ 即使在不增加训练时间的情况下，获得更低 perplexity。

Conclusion: FAL 提供了一种高效的变换器架构来减少通信开销，适用于大规模并行训练；FAL+ 在不增加训练代价的前提下进一步提升模型质量。

Abstract: As training billion-scale transformers becomes increasingly common, employing
multiple distributed GPUs along with parallel training methods has become a
standard practice. However, existing transformer designs suffer from
significant communication overhead, especially in Tensor Parallelism (TP),
where each block's MHA-MLP connection requires an all-reduce communication.
Through our investigation, we show that the MHA-MLP connections can be bypassed
for efficiency, while the attention output of the first layer can serve as an
alternative signal for the bypassed connection. Motivated by the observations,
we propose FAL (First Attentions Last), an efficient transformer architecture
that redirects the first MHA output to the MLP inputs of the following layers,
eliminating the per-block MHA-MLP connections. This removes the all-reduce
communication and enables parallel execution of MHA and MLP on a single GPU. We
also introduce FAL+, which adds the normalized first attention output to the
MHA outputs of the following layers to augment the MLP input for the model
quality. Our evaluation shows that FAL reduces multi-GPU training time by up to
44%, improves single-GPU throughput by up to 1.18x, and achieves better
perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity
without increasing the training time than the baseline.

</details>


### [70] [LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching](https://arxiv.org/abs/2510.14623)
*Zhuo Cao,Xuan Zhao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: LeapFactual 是一种基于条件流匹配的模型无关对比度量解释方法，能在真实与学习边界分离时生成可靠且分布内的对照样本，适用于人机协同场景并可用于数据增强。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域需要可解释且可操作的解释方法。现有对比解释存在梯度消失、潜在空间不连续以及过度依赖真实与学习决策边界对齐的问题，限制了对比解释的可靠性与适用性。

Method: LeapFactual 通过条件流量匹配来生成对比样本，属于模型无关的方法，能够在真实与学习边界分离时仍产生可靠的分布内对比样本。它支持人机交互的系统，并能将对比样本用于新的训练数据来增强模型性能。

Result: 在基准与真实世界数据集上进行了广泛实验，证明所生成的对比样本在分布内、标签与 ground truth 对齐且具有可操作性洞见；可用于数据增强以提升模型。

Conclusion: 方法具有广泛适用性，提升了科学知识发现与非专家的可解释性，同时扩展了对比解释的应用范围，尤其适用于需要人机协同的情景。

Abstract: The growing integration of machine learning (ML) and artificial intelligence
(AI) models into high-stakes domains such as healthcare and scientific research
calls for models that are not only accurate but also interpretable. Among the
existing explainable methods, counterfactual explanations offer
interpretability by identifying minimal changes to inputs that would alter a
model's prediction, thus providing deeper insights. However, current
counterfactual generation methods suffer from critical limitations, including
gradient vanishing, discontinuous latent spaces, and an overreliance on the
alignment between learned and true decision boundaries. To overcome these
limitations, we propose LeapFactual, a novel counterfactual explanation
algorithm based on conditional flow matching. LeapFactual generates reliable
and informative counterfactuals, even when true and learned decision boundaries
diverge. Following a model-agnostic approach, LeapFactual is not limited to
models with differentiable loss functions. It can even handle human-in-the-loop
systems, expanding the scope of counterfactual explanations to domains that
require the participation of human annotators, such as citizen science. We
provide extensive experiments on benchmark and real-world datasets showing that
LeapFactual generates accurate and in-distribution counterfactual explanations
that offer actionable insights. We observe, for instance, that our reliable
counterfactual samples with labels aligning to ground truth can be beneficially
used as new training data to enhance the model. The proposed method is broadly
applicable and enhances both scientific knowledge discovery and non-expert
interpretability.

</details>


### [71] [Galaxy Morphology Classification with Counterfactual Explanation](https://arxiv.org/abs/2510.14655)
*Zhuo Cao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: 提出在传统的编码-解码架构上加入可逆流（invertible flow）的改进，以实现高预测性能的同时，提供关于决策过程的反事实解释，从而提升对星系形态分类的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前星系形态识别的ML方法多为黑盒，难以解释模型决策，且人工标注工作量大；需要在保持预测性能的同时，获得对模型决策过程的可解释信息，提升科学可解释性。

Method: 在经典的编码-解码架构中加入可逆流（invertible flow），使模型能够生成反事实解释，从而在保持良好预测性能的同时，提供关于模型决策的可解释信息。

Result: 所提出的模型能够在保持良好预测性能的同时，提供关于决策过程的反事实解释，提升模型的可解释性。

Conclusion: 将可逆流整合进编码-解码框架，为天文学中的星系形态分析提供一种兼具准确性与可解释性的建模范式，能够给出有意义的反事实解释。

Abstract: Galaxy morphologies play an essential role in the study of the evolution of
galaxies. The determination of morphologies is laborious for a large amount of
data giving rise to machine learning-based approaches. Unfortunately, most of
these approaches offer no insight into how the model works and make the results
difficult to understand and explain. We here propose to extend a classical
encoder-decoder architecture with invertible flow, allowing us to not only
obtain a good predictive performance but also provide additional information
about the decision process with counterfactual explanations.

</details>


### [72] [Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings](https://arxiv.org/abs/2510.14666)
*Shayan Gharib,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 利用Siegel嵌入将一阶与二阶矩整合为对称正定矩阵，并在SPD流形上的Riemannian距离进行跨域对齐；通过统一的几何距离提高无监督领域适应的分布移位处理，理论与经验均表明优于传统低阶矩对齐；代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有UDA多依赖低阶矩对齐和任意相似度度量，忽略分布的几何结构。需要一个几何上更自然的距离来对齐源/目标的均值和协方差，从而提高跨域泛化。

Method: 将一阶和二阶矩通过Siegel嵌入转化为一个SPD矩阵；在SPD矩阵的流形上使用Riemannian距离进行对齐；将这种距离与目标域误差界相联系；在图像去噪和分类任务中验证。

Result: 在图像去噪和分类基准上验证有效性，方法表现出对跨域对齐的改进；并给出公开代码。

Conclusion: 通过在共享SPD流形上统一对齐一阶/二阶矩，保留均值和协方差结构，提供更自然的跨域度量，并具有理论联系与实验验证的潜力。

Abstract: We address the problem of distribution shift in unsupervised domain
adaptation with a moment-matching approach. Existing methods typically align
low-order statistical moments of the source and target distributions in an
embedding space using ad-hoc similarity measures. We propose a principled
alternative that instead leverages the intrinsic geometry of these
distributions by adopting a Riemannian distance for this alignment. Our key
novelty lies in expressing the first- and second-order moments as a single
symmetric positive definite (SPD) matrix through Siegel embeddings. This
enables simultaneous adaptation of both moments using the natural geometric
distance on the shared manifold of SPD matrices, preserving the mean and
covariance structure of the source and target distributions and yielding a more
faithful metric for cross-domain comparison. We connect the Riemannian manifold
distance to the target-domain error bound, and validate the method on image
denoising and image classification benchmarks. Our code is publicly available
at https://github.com/shayangharib/GeoAdapt.

</details>


### [73] [FedPPA: Progressive Parameter Alignment for Personalized Federated Learning](https://arxiv.org/abs/2510.14698)
*Maulidi Adi Prasetia,Muhamad Risqi U. Saputra,Guntur Dharma Putra*

Main category: cs.LG

TL;DR: 提出 FedPPA，与全球模型的权重逐步对齐来实现更强的个性化，并在框架中结合熵加权平均以提升全局模型性能，适用于非IID数据和异构计算资源场景。


<details>
  <summary>Details</summary>
Motivation: 在现实中，客户端存在不同的计算能力且数据非IID，现有个性化联邦学习方法往往忽略模型和数据异质性的共存，导致个性化效果不稳定。

Method: 提出渐进式参数对齐机制：在训练过程中逐步使公共层的权重与全局模型对齐，同时保护和保留客户端的本地知识；并在 FedPPA 框架中引入基于熵的加权平均，以兼顾全局与个性化目标。

Result: 在 MNIST、FMNIST、CIFAR-10 等数据集上，FedPPA 显著优于若干现有 FL 算法，在个性化适配方面具有更强鲁棒性和性能。

Conclusion: FedPPA 有效缓解模型与数据在异质环境下的不一致性问题，提升个性化鲁棒性及全局性能，具备良好的扩展性。

Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving
machine learning paradigm that enables multiple clients to collaboratively
train a model without sharing their data. In real-world scenarios, however,
clients often have heterogeneous computational resources and hold
non-independent and identically distributed data (non-IID), which poses
significant challenges during training. Personalized Federated Learning (PFL)
has emerged to address these issues by customizing models for each client based
on their unique data distribution. Despite its potential, existing PFL
approaches typically overlook the coexistence of model and data heterogeneity
arising from clients with diverse computational capabilities. To overcome this
limitation, we propose a novel method, called Progressive Parameter Alignment
(FedPPA), which progressively aligns the weights of common layers across
clients with the global model's weights. Our approach not only mitigates
inconsistencies between global and local models during client updates, but also
preserves client's local knowledge, thereby enhancing personalization
robustness in non-IID settings. To further enhance the global model performance
while retaining strong personalization, we also integrate entropy-based
weighted averaging into the FedPPA framework. Experiments on three image
classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate
that FedPPA consistently outperforms existing FL algorithms, achieving superior
performance in personalized adaptation.

</details>


### [74] [Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References](https://arxiv.org/abs/2510.14719)
*Hongzheng Chen,Bin Fan,Alexander Collins,Bastian Hagedorn,Evghenii Gaburov,Masahiro Masuda,Matthew Brookhart,Chris Sullivan,Jason Knight,Zhiru Zhang,Vinod Grover*

Main category: cs.LG

TL;DR: Tiwa（Tawa）是一款自动编译器，能从高层 tile 程序自动生成 warp 专用代码，使用异步引用（aref）IR 表达 warp 级通信，在 NVIDIA H100 上对多类工作负载实现高效数据流，达到对比 cuBLAS、Triton 等的显著加速并降低编程量。


<details>
  <summary>Details</summary>
Motivation: 现有 SIMT 编程模型与任务并行的 GPU 硬件之间存在明显不匹配，warp 专门化若要高效需要大量低层实现，造成高成本、易错且不可维护。需要一个能自动化生成 warp 专用代码的编译框架来缩短开发周期并提升可移植性。

Method: 提出异步引用（aref）作为 warp 级通信的中间表示；Tawa 将程序自动划分成生产者-消费者角色，自动管理数据流管线，基于 tile 结构从高层输入生成高性能的 warp 专用代码，减少对内核的手动改写。评估对象包括 NVIDIA H100 上的代表性 LLM 内核、GEMM、注意力工作负载等。

Result: 在 GEMM/ cuBLAS 基线下，最高实现约 1.1x 加速；在注意力工作负载上，达 1.2x 优于 Triton，并与手工优化的 CUTLASS FlashAttention-3 内核相当，同时显著降低编程工作量。

Conclusion: 通过 aref IR 与自动分区的组合，Tawa 有效提升硬件利用率并降低编程复杂性，填补从高层程序到 warp 专用实现之间的空白，尤其在大模型推理/训练场景具备潜在价值。

Abstract: Modern GPUs feature specialized hardware units that enable high-performance,
asynchronous dataflow execution. However, the conventional SIMT programming
model is fundamentally misaligned with this task-parallel hardware, creating a
significant programmability gap. While hardware-level warp specialization is
the key to unlocking peak performance, it forces developers to manually
orchestrate complex, low-level communication and software pipelines--a process
that is labor-intensive, error-prone, and unsustainable. To address this
challenge, we present Tawa, an automated compiler that systematically generates
high-performance, warp-specialized code from a high-level, tile-based program.
Central to our approach is a novel IR abstraction, asynchronous references
(aref), which expresses warp-level communication without exposing low-level
hardware details. Using this abstraction, Tawa automatically partitions
programs into producer-consumer roles and manages the intricate dataflow
pipeline, relieving developers of invasive kernel rewriting. Evaluation on
NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers
high hardware utilization, achieving up to 1.1$\times$ speedup over highly
optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains
1.2$\times$ speedup over Triton and matches the performance of the
hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming
effort.

</details>


### [75] [The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents](https://arxiv.org/abs/2510.14727)
*Antony Bartlett,Cynthia Liem,Annibale Panichella*

Main category: cs.LG

TL;DR: A multi-objective search approach called INDAGO-Nexus improves DRL safety testing by jointly optimizing failure likelihood and test scenario diversity, outperforming the prior INDAGO in unique failure discovery and time-to-failure across three agents.


<details>
  <summary>Details</summary>
Motivation: Existing reliance on single-objective optimization that maximizes failure counts often yields limited diversity and misses distinct error types. There is a need for diversified failure scenarios and diversity-aware optimization in safety-critical DRL testing.

Method: INDAGO-Nexus uses multi-objective evolutionary algorithms with multiple diversity metrics and Pareto front selection to simultaneously optimize failure likelihood and scenario diversity. It is evaluated on three DRL agents (humanoid walker, self-driving car, parking agent).

Result: INDAGO-Nexus discovers up to 83% more unique failures than INDAGO in the self-driving car (SDC) scenario and up to 40% more in the parking scenario, while reducing time-to-failure by up to 67% across all evaluated agents.

Conclusion: A diversity-aware, multi-objective search framework (INDAGO-Nexus) enhances test effectiveness by increasing failure diversity and reducing time-to-failure, providing more efficient and diverse failure discovery for safety-critical DRL systems.

Abstract: Testing deep reinforcement learning (DRL) agents in safety-critical domains
requires discovering diverse failure scenarios. Existing tools such as INDAGO
rely on single-objective optimization focused solely on maximizing failure
counts, but this does not ensure discovered scenarios are diverse or reveal
distinct error types. We introduce INDAGO-Nexus, a multi-objective search
approach that jointly optimizes for failure likelihood and test scenario
diversity using multi-objective evolutionary algorithms with multiple diversity
metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on
three DRL agents: humanoid walker, self-driving car, and parking agent. On
average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test
effectiveness) than INDAGO in the SDC and Parking scenarios, respectively,
while reducing time-to-failure by up to 67% across all agents.

</details>


### [76] [Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries](https://arxiv.org/abs/2510.14751)
*Divyat Mahajan,Sachin Goyal,Badr Youbi Idrissi,Mohammad Pezeshki,Ioannis Mitliagkas,David Lopez-Paz,Kartik Ahuja*

Main category: cs.LG

TL;DR: 提出未来摘要预测（FSP）来改进长程推理与长篇生成，通过训练一个辅助头预测未来的紧凑表示，分为手工摘要（如未来词袋）和学习摘要（由从右到左的反向语言模型得到的嵌入）。在3B与8B参数模型上，FSP相较于NTP和MTP在数学、推理与编码等基准上表现出改进。


<details>
  <summary>Details</summary>
Motivation: NTP在长程推理、规划和创作方面存在局限，部分原因在于教师级训练。MTP能缓解部分问题但主要捕捉短程依赖，提升有限。提出FSP以保留对长远未来有用的信息。

Method: 在模型中增加一个辅助头，用以预测未来的紧凑表示。两种变体：1) 手工摘要，如未来序列的词袋摘要；2) 学习摘要，使用由从右向左训练的反向语言模型生成的嵌入。进行了大规模预训练实验（3B和8B参数模型），以比较NTP、MTP和FSP。

Result: 实验结果表明，FSP在数学、推理和编码等基准上相较NTP和MTP具显著改进，且在3B与8B参数规模下均有收益。

Conclusion: 通过学习或设计的紧凑未来表示，FSP有效保留对长篇生成有用的信息，成为改进长程推理和长篇文本生成的可行途径，优于传统的NTP与MTP。

Abstract: Next-token prediction (NTP) has driven the success of large language models
(LLMs), but it struggles with long-horizon reasoning, planning, and creative
writing, with these limitations largely attributed to teacher-forced training.
Multi-token prediction (MTP) partially mitigates these issues by predicting
several future tokens at once, but it mostly captures short-range dependencies
and offers limited improvement. We propose future summary prediction (FSP),
which trains an auxiliary head to predict a compact representation of the
long-term future, preserving information relevant for long-form generations. We
explore two variants of FSP: handcrafted summaries, for example, a bag of words
summary of the future of the sequence, and learned summaries, which use
embeddings produced by a reverse language model trained from right to left.
Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate
that FSP provides improvements over both NTP and MTP across math, reasoning,
and coding benchmarks.

</details>


### [77] [Causal Discovery for Linear DAGs with Dependent Latent Variables via Higher-order Cumulants](https://arxiv.org/abs/2510.14780)
*Ming Cai,Penggang Gao,Hisayuki Hara*

Main category: cs.LG

TL;DR: 提出了一种 LvLiNGAM 算法，在存在潜在混淆变量的线性非高斯有向无环模型中，允许潜在变量与观测变量、以及两者之间存在因果关系，并利用高阶累积量来识别因果结构；通过大量仿真和真实数据验证其有效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么假设潜在混淆变量两两独立，要么在观测变量之间存在因果关系时表现有限；需要一种同时处理潜在混淆、观测变量及其之间因果关系的结构识别方法。

Method: 提出一种新算法，利用观测数据的高阶累积量来识别因果结构，允许潜在变量彼此之间、观测变量之间，以及潜在变量与观测变量之间存在因果关系，适用于线性非高斯的有向无环模型（LvLiNGAM）。

Result: 在广泛的仿真和真实数据实验中，所提出的算法展示出良好的识别准确性与实际可用性，验证了方法的有效性。

Conclusion: 该算法能够在 LvLiNGAM 框架下识别包含潜在混淆变量且潜在与观测变量之间存在复杂因果关系的 DAG，为实际应用提供可行的因果结构识别工具。

Abstract: This paper addresses the problem of estimating causal directed acyclic graphs
in linear non-Gaussian acyclic models with latent confounders (LvLiNGAM).
Existing methods assume mutually independent latent confounders or cannot
properly handle models with causal relationships among observed variables.
  We propose a novel algorithm that identifies causal DAGs in LvLiNGAM,
allowing causal structures among latent variables, among observed variables,
and between the two. The proposed method leverages higher-order cumulants of
observed data to identify the causal structure. Extensive simulations and
experiments with real-world data demonstrate the validity and practical utility
of the proposed algorithm.

</details>


### [78] [Efficient Dynamic Structured Sparse Training with Learned Shuffles](https://arxiv.org/abs/2510.14812)
*Abhishek Tyagi,Arjun Iyer,Liam Young,William H Renninger,Christopher Kanan,Yuhao Zhu*

Main category: cs.LG

TL;DR: Permutation-augmented dynamic sparse training (PA-DST) learns a permutation matrix for each layer to extend the expressivity of structured sparsity (block, N:M, diagonals), achieving unstructured DST accuracy at high sparsity with speedups.


<details>
  <summary>Details</summary>
Motivation: Structured sparsity typically speeds up compute but sacrifices expressivity compared to unstructured DST, since fixed layouts sample only a subset of possible sparse masks. The goal is to recover expressivity and bridge accuracy-efficiency gaps.

Method: Jointly learn a permutation matrix per layer together with the structured weight matrix. Apply this to three canonical structures—block, N:M, and diagonals—and evaluate on ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2).

Result: Permutation-augmented DST (PA-DST) matches unstructured baselines (RigL, SET) at 90–95% sparsity on ImageNet-1K and WikiText-103, while enabling faster training (up to 1.21×) and faster inference (up to 2.9×).

Conclusion: Incorporating a learned permutation with structured sparsity yields a favorable trade-off between accuracy and efficiency, positioning structure plus permutation as a strong middle ground.

Abstract: Structured sparsity accelerates training and inference on modern GPUs, yet it
still trails unstructured dynamic sparse training (DST) in accuracy. The
shortfall stems from a loss of expressivity: whereas a dense layer can realize
every possible mask obtained by choosing any $w$ active weights out of $n$, a
fixed block or N:M layout explores only a subset of those possibilities. We
propose to close this gap by learning, for each layer, a single permutation
matrix jointly with the structured weight matrix. Applied to three canonical
structures -- block, N:M, and diagonals -- we show that permutation-augmented
DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\% sparsity on
ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\times$
and infers up to $2.9\times$ faster. The results position structure + learned
permutation as a sweet spot between accuracy and efficiency.

</details>


### [79] [Tackling Time-Series Forecasting Generalization via Mitigating Concept Drift](https://arxiv.org/abs/2510.14814)
*Zhiyuan Zhao,Haoxin Liu,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 论文聚焦时间序列中的分布漂移，区分概念漂移与时间漂移，提出ShifTS框架，在先处理时间漂移的前提下利用软注意力从回看和未来序列中挖掘不变模式，方法对模型无特定假设（agnostic），实验显示对多数据集均能提升预测精度并优于已有基线。


<details>
  <summary>Details</summary>
Motivation: 现实时间序列经常发生分布变化，现有研究多关注时间漂移，忽略概念漂移在时间序列预测中的影响；需要一个统一框架同时处理两类漂移，且在现实中保持方法的通用性。

Method: 提出软注意力机制用于在回看（lookback）与 horizon（未来）时间序列中发现不变模式；提出ShifTS，一个方法无关的框架，先处理时间漂移再处理概念漂移，二者在一个统一框架内协同工作。

Result: 大量实验显示ShifTS能持续提升不同无偏（agnostic）模型的预测准确性，且优于现有的概念漂移、时间漂移及二者结合的基线。

Conclusion: 将时间漂移作为概念漂移处理的前提，ShifTS提供了一种统一且通用的解决思路，验证了对多数据集的有效性与鲁棒性。

Abstract: Time-series forecasting finds broad applications in real-world scenarios. Due
to the dynamic nature of time series data, it is important for time-series
forecasting models to handle potential distribution shifts over time. In this
paper, we initially identify two types of distribution shifts in time series:
concept drift and temporal shift. We acknowledge that while existing studies
primarily focus on addressing temporal shift issues in time series forecasting,
designing proper concept drift methods for time series forecasting has received
comparatively less attention.
  Motivated by the need to address potential concept drift, while conventional
concept drift methods via invariant learning face certain challenges in
time-series forecasting, we propose a soft attention mechanism that finds
invariant patterns from both lookback and horizon time series. Additionally, we
emphasize the critical importance of mitigating temporal shifts as a
preliminary to addressing concept drift. In this context, we introduce ShifTS,
a method-agnostic framework designed to tackle temporal shift first and then
concept drift within a unified approach. Extensive experiments demonstrate the
efficacy of ShifTS in consistently enhancing the forecasting accuracy of
agnostic models across multiple datasets, and outperforming existing concept
drift, temporal shift, and combined baselines.

</details>


### [80] [Programmatic Representation Learning with Language Models](https://arxiv.org/abs/2510.14825)
*Gabriel Poesia,Georgia Gabriela Sampaio*

Main category: cs.LG

TL;DR: LeaPRs: Learned Programmatic Representations combine code-based features with decision trees; uses LLMs to synthesize features; two learning algorithms; competitive with neural networks; interpretable, neural-network-free predictors.


<details>
  <summary>Details</summary>
Motivation:  brid ge the interpretability of classical models with the representational power of neural nets by learning features as code and using decision trees; leveraging LLMs to generate domain-relevant features.

Method: Two algorithms: (1) FunSearch adaptation to learn features instead of predictors; (2) a novel ID3-like algorithm that generates new features on-demand at leaf splits; features are functions from data to scalars synthesized by LLMs; experiments across chess, image, text.

Result: LeaPRs produce high-quality predictors, often competitive with neural networks; can be neural-network-free; demonstrate end-to-end interpretable representation learning.

Conclusion: Proposes a flexible paradigm for learning interpretable representations where both features and predictions are inspectable; leveraging LLM-synthesized features integrated with decision-tree predictors; broad applicability.

Abstract: Classical models for supervised machine learning, such as decision trees, are
efficient and interpretable predictors, but their quality is highly dependent
on the particular choice of input features. Although neural networks can learn
useful representations directly from raw data (e.g., images or text), this
comes at the expense of interpretability and the need for specialized hardware
to run them efficiently. In this paper, we explore a hypothesis class we call
Learned Programmatic Representations (LeaPR) models, which stack arbitrary
features represented as code (functions from data points to scalars) and
decision tree predictors. We synthesize feature functions using Large Language
Models (LLMs), which have rich prior knowledge in a wide range of domains and a
remarkable ability to write code using existing domain-specific libraries. We
propose two algorithms to learn LeaPR models from supervised data. First, we
design an adaptation of FunSearch to learn features rather than directly
generate predictors. Then, we develop a novel variant of the classical ID3
algorithm for decision tree learning, where new features are generated on
demand when splitting leaf nodes. In experiments from chess position evaluation
to image and text classification, our methods learn high-quality, neural
network-free predictors often competitive with neural networks. Our work
suggests a flexible paradigm for learning interpretable representations
end-to-end where features and predictions can be readily inspected and
understood.

</details>


### [81] [Backdoor Unlearning by Linear Task Decomposition](https://arxiv.org/abs/2510.14845)
*Amel Abdelraheem,Alessandro Favero,Gerome Bovet,Pascal Frossard*

Main category: cs.LG

TL;DR: 提出一种基于权重空间解耦的反向学习方法，用于从大规模基础模型中“洗劫”回门触发（backdoor），在给定攻击信息时实现近乎完善的去后门，同时保持高比例的干净准确率（约96%），并且在攻击未知时也可通过反向触发估计实现去后门，通常优于现有防御。


<details>
  <summary>Details</summary>
Motivation:  foundation 模型在面对对抗性扰动和定向后门攻击时容易被攻击，且大规模模型不能进行大量再训练以保证安全，因此需要在不损害通用能力的前提下去除后门。

Method: 基于观察到后门在模型权重空间中与其他无害任务“解耦”，从而可以分离并消除后门的影响，提出一种简单的去学习（unlearning）方法。该方法在 CLIP 等模型及常见触发上验证；若能获得攻击信息，则实现几乎完美的去后门；在攻击信息未知时，可通过反向触发估计来进行去后门。

Result: 在多组实验中实现近乎完美的去后门，平均保留约96%的干净准确率；相比现有防御，在去后门效果与干净准确率之间表现出更优的权衡；即便攻击信息未知也能通过估计进行有效去后门。

Conclusion: 基于权重空间的解耦性，提出的去学习方法能够在大型基础模型中高效去除后门，同时最小化对无害能力的影响，为后门防御提供可扩展且性能友好的新路线。

Abstract: Foundation models have revolutionized computer vision by enabling broad
generalization across diverse tasks. Yet, they remain highly susceptible to
adversarial perturbations and targeted backdoor attacks. Mitigating such
vulnerabilities remains an open challenge, especially given that the
large-scale nature of the models prohibits retraining to ensure safety.
Existing backdoor removal approaches rely on costly fine-tuning to override the
harmful behavior, and can often degrade performance on other unrelated tasks.
This raises the question of whether backdoors can be removed without
compromising the general capabilities of the models. In this work, we address
this question and study how backdoors are encoded in the model weight space,
finding that they are disentangled from other benign tasks. Specifically, this
separation enables the isolation and erasure of the backdoor's influence on the
model with minimal impact on clean performance. Building on this insight, we
introduce a simple unlearning method that leverages such disentanglement.
Through extensive experiments with CLIP-based models and common adversarial
triggers, we show that, given the knowledge of the attack, our method achieves
approximately perfect unlearning, while retaining, on average, 96% of clean
accuracy. Additionally, we demonstrate that even when the attack and its
presence are unknown, our method successfully unlearns backdoors by proper
estimation using reverse-engineered triggers. Overall, our method consistently
yields better unlearning and clean accuracy tradeoffs when compared to present
state-of-the-art defenses.

</details>


### [82] [Predicting kernel regression learning curves from only raw data statistics](https://arxiv.org/abs/2510.14878)
*Dhruva Karkada,Joseph Turnbull,Yuxi Liu,James B. Simon*

Main category: cs.LG

TL;DR: 提出一种通过 Hermite 徽結（HEA）来预测核回归在真实数据集上的学习曲线的新框架：以数据协方差矩阵和目标函数的经验多项式分解为输入，近似核特征的本征值与本征函数，尤其在各向异性数据分布下。


<details>
  <summary>Details</summary>
Motivation: 在只用极少量的观测（数据协方差与目标函数的经验多项式分解）即可预测测试误差对样本量的学习曲线时，建立一个连结数据结构与模型性能的理论框架，涵盖真实数据集（如 CIFAR-5m、SVHN、ImageNet）中的核回归及实证意义。

Method: 提出 Hermite eigenstructure ansatz (HEA)，对核的特征在各向异性数据分布下的近似本征值/本征函数进行解析，给出高斯数据下的严格证明并在真实图像数据上验证其合理性；依据核特征结构与数据分布关系，结合已有结果推导学习曲线。扩展到特征学习阶段的多层感知机（MLP），观察其在 HEA 预测阶数上学习 Hermite 多项式。

Result: 给出一个可从两项观测（协方差矩阵与目标函数的经验多项式分解）预测测试风险随样本量的学习曲线的理论框架；在高斯数据上证明 HEA，并在真实图像数据上经验性地显示其“Gaussian enough”的有效性；将核特征结构与学习曲线的关系用于预测，并发现 MLP 在特征学习阶段按 HEA 预测的阶数学习 Hermite 多项式。

Conclusion: 该工作提供一个端到端的学习理论雏形，展示在现实数据与非平凡学习算法下，数据结构能映射到模型性能的可能性，输出为可操作的学习曲线预测框架。

Abstract: We study kernel regression with common rotation-invariant kernels on real
datasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical
framework that predicts learning curves (test risk vs. sample size) from only
two measurements: the empirical data covariance matrix and an empirical
polynomial decomposition of the target function $f_*$. The key new idea is an
analytical approximation of a kernel's eigenvalues and eigenfunctions with
respect to an anisotropic data distribution. The eigenfunctions resemble
Hermite polynomials of the data, so we call this approximation the Hermite
eigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find
that real image data is often "Gaussian enough" for the HEA to hold well in
practice, enabling us to predict learning curves by applying prior results
relating kernel eigenstructure to test risk. Extending beyond kernel
regression, we empirically find that MLPs in the feature-learning regime learn
Hermite polynomials in the order predicted by the HEA. Our HEA framework is a
proof of concept that an end-to-end theory of learning which maps dataset
structure all the way to model performance is possible for nontrivial learning
algorithms on real datasets.

</details>


### [83] [Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards](https://arxiv.org/abs/2510.14884)
*Sarah Liaw,Benjamin Plaut*

Main category: cs.LG

TL;DR: 提出在高风险环境中使用 abstain 选项的两类情境 bandit，通过谨慎探索实现次线性 regret，在无导师情形下学习“何时不学习”。


<details>
  <summary>Details</summary>
Motivation: 在高风险、不可修复的错误可能导致不可逆后果的场景中，传统探索可能造成不可逆损害。若没有导师辅助，需研究如何在不造成严重损害的前提下进行学习。

Method: 将问题建模为具有 abstain 选项的两动作情境带宽披露：每轮观测输入后选择 abstain（奖励为0）或 commit（执行既有策略，奖励上有界但可为任意为负，且与输入 Lipschitz 相关）。提出一种谨慎型算法，选取可信区域，仅在证据未证明可能伤害时才进行提交。基于独立同分布的输入假设。

Result: 在 i.i.d. 输入下，得到 sublinear regret 保证，理论性地证明在高风险环境中通过谨慎探索实现安全部署的有效性。

Conclusion: 为无导师情境下的高风险环境提供了一个学习何时不学习的框架，验证了谨慎探索在确保安全的同时仍能实现有效学习的可能性。

Abstract: In high-stakes AI applications, even a single action can cause irreparable
damage. However, nearly all of sequential decision-making theory assumes that
all errors are recoverable (e.g., by bounding rewards). Standard bandit
algorithms that explore aggressively may cause irreparable damage when this
assumption fails. Some prior work avoids irreparable errors by asking for help
from a mentor, but a mentor may not always be available. In this work, we
formalize a model of learning with unbounded rewards without a mentor as a
two-action contextual bandit with an abstain option: at each round the agent
observes an input and chooses either to abstain (always 0 reward) or to commit
(execute a preexisting task policy). Committing yields rewards that are
upper-bounded but can be arbitrarily negative, and the commit reward is assumed
Lipschitz in the input. We propose a caution-based algorithm that learns when
not to learn: it chooses a trusted region and commits only where the available
evidence does not already certify harm. Under these conditions and i.i.d.
inputs, we establish sublinear regret guarantees, theoretically demonstrating
the effectiveness of cautious exploration for deploying learning agents safely
in high-stakes environments.

</details>


### [84] [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/abs/2510.14901)
*Aayush Karan,Yilun Du*

Main category: cs.LG

TL;DR: 通过一个基于MCMC的迭代采样方法，在不进行任何额外训练的情况下，从基础大模型的概率分布中提取/提升推理能力，显著接近或超越RL的效果，且避免样本多样性下降，适用于多领域任务。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于：以往的工作多聚焦于通过强化学习对大模型进行后训练以显现新颖推理行为，但本文从另一角度出发，探讨是否仅在推理阶段的纯采样即可让基础模型具备相当的推理能力，且无需额外训练，从而降低成本并扩大适用范围。

Method: 提出一个简单的迭代采样算法，借鉴MCMC用于从 sharpened 分布采样，利用基础模型的自有似然进行采样；不需要训练、定制数据集或验证器；在不同基模型上均显示显著效果。

Result: 在多任务评测（如 MATH500、HumanEval、GPQA）中，采样方法显著提升推理能力，接近甚至超越RL后训练的效果；且避免RL后训练中常见的样本多样性崩溃，具备较强的跨模型和广泛适用性。

Conclusion: 纯粹依赖推理阶段的采样就能实现接近RL的推理能力，且无需额外训练，方法具有广泛适用性，适用于难以验证领域和需要高效推理的场景。

Abstract: Frontier reasoning models have exhibited incredible capabilities across a
wide array of disciplines, driven by posttraining large language models (LLMs)
with reinforcement learning (RL). However, despite the widespread success of
this paradigm, much of the literature has been devoted to disentangling truly
novel behaviors that emerge during RL but are not present in the base models.
In our work, we approach this question from a different angle, instead asking
whether comparable reasoning capabilites can be elicited from base models at
inference time by pure sampling, without any additional training. Inspired by
Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened
distributions, we propose a simple iterative sampling algorithm leveraging the
base models' own likelihoods. Over different base models, we show that our
algorithm offers substantial boosts in reasoning that nearly match and even
outperform those from RL on a wide variety of single-shot tasks, including
MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in
diversity over multiple samples that is characteristic of RL-posttraining.
Crucially, our method does not require training, curated datasets, or a
verifier, suggesting broad applicability beyond easily verifiable domains.

</details>


### [85] [Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models](https://arxiv.org/abs/2510.14961)
*Jonas Geiping,Xinyu Yang,Guinan Su*

Main category: cs.LG

TL;DR: 提出一种针对带循环深度的扩散语言模型的扩散强制采样器，通过在前向传播的每一步解码新标记并通过递归并行细化潜在状态，实现对传统自回归生成的表达性提升，并在相同时间预算下对3.5B级模型可实现最高约5x加速，兼具推理并行化与连续扩散视角的理论与实践意义。


<details>
  <summary>Details</summary>
Motivation: 旨在提升带循环深度的变换器模型在推理阶段的推理效率与表达能力；并探索将循环深度语言模型与扩散语言模型的关系进行统一建模，以促进大规模模型的高效生成。

Method: 提出一种新的扩散强制采样器，用于此类模型：在模型的每个前向传播步骤解码新的token，同时通过递归在并行中进一步精炼这些token的潜在状态。理论上，在同等时间预算和现代硬件条件下，该采样器的生成表达性严格大于基线自回归生成；并且该采样器可以直接应用到现有的3.5B规模的循环深度变换器上，且无需额外微调。

Result: 在理论层面，给出该采样器在时间预算下的表达性提升并优于自回归基线；在应用层面，展示了对于3.5B参数的循环深度变换器可实现最高约5倍的推理加速。

Conclusion: 研究将带循环深度的模型自然地看作强大的连续但具因果性的扩散语言模型的一个实例，提供了一种在推理阶段并行化额外计算的高效机制，并为未来在此类模型上的扩散式生成提供了理论与实践框架。

Abstract: Language models with recurrent depth, also referred to as universal or looped
when considering transformers, are defined by the capacity to increase their
computation through the repetition of layers. Recent efforts in pretraining
have demonstrated that these architectures can scale to modern language
modeling tasks while exhibiting advantages in reasoning tasks. In this work, we
examine the relationship between recurrent-depth models and diffusion language
models. Building on their similarities, we develop a new diffusion forcing
sampler for these models to accelerate generation. The sampler advances by
decoding new tokens at every forward pass of the model, while the latent states
of these tokens can be further refined in parallel through recurrence.
Theoretically, generation with our sampler is strictly more expressive than the
baseline autoregressive generation using the same time budget on modern
hardware. Moreover, this sampler, based on principles from diffusion
literature, can be directly applied to existing 3.5B recurrent-depth
transformers without any tuning, leading to up to a 5x speedup. Consequently,
our findings not only provide an efficient mechanism for parallelizing the
extra computation in recurrent-depth models at inference, but also suggest that
such models can be naturally viewed as strong continuous, though causal,
diffusion language models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [86] [Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach](https://arxiv.org/abs/2510.13819)
*George Stamatelis,Hui Chen,Henk Wymeersch,George C. Alexandropoulos*

Main category: cs.NI

TL;DR: A neuroevolution–supervised learning-based multi-agent scheme jointly optimizes RIS phase shifts and uplink power for RIS-assisted localization, using single-bit feedback and discrete RIS elements, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Improve RIS-assisted localization under limited feedback and discrete RIS constraints by enabling dynamic uplink power control via a feedback link.

Method: Hybrid NeuroEvolution and supervised learning-based multi-agent algorithm that jointly optimizes RIS configurations and user transmit power; supports single-bit feedback and discrete RIS elements.

Result: Numerical results show it outperforms fingerprinting, deep RL baselines, and backpropagation-based position estimators.

Conclusion: Demonstrates effectiveness of the proposed NE+SL hybrid approach for RIS-aided localization with limited feedback and discrete RIS elements.

Abstract: This paper studies user localization aided by a Reconfigurable Intelligent
Surface (RIS). A feedback link from the Base Station (BS) to the user is
adopted to enable dynamic power control of the user pilot transmissions in the
uplink. A novel multi-agent algorithm for the joint control of the RIS phase
configuration and the user transmit power is presented, which is based on a
hybrid approach integrating NeuroEvolution (NE) and supervised learning. The
proposed scheme requires only single-bit feedback messages for the uplink power
control, supports RIS elements with discrete responses, and is numerically
shown to outperform fingerprinting, deep reinforcement learning baselines and
backpropagation-based position estimators.

</details>


### [87] [DiffLoc: Diffusion Model-Based High-Precision Positioning for 6G Networks](https://arxiv.org/abs/2510.14111)
*Taekyun Lee,Tommaso Balercia,Heasung Kim,Hyeji Kim,Jeffrey G. Andrews*

Main category: cs.NI

TL;DR: 通过条件生成扩散模型将原始上行SRS指纹直接映射到连续地理坐标的室外定位框架，显著超越传统指纹定位和网格融合，在高速度和未见轨迹下实现近似厘米级时空定位。


<details>
  <summary>Details</summary>
Motivation: 解决大规模、动态室外环境中指纹定位的可扩展性和数据覆盖问题，避免需要密集且不可行的数据调查；实现从原始CSI指纹到地理坐标的端到端定位。

Method: 将条件生成扩散模型直接应用于高维 Massive MIMO 通道状态信息（CSI），以原始上行SRS指纹为输入，学习从指纹到连续坐标的直接映射，并通过一致性训练将推理步数从200步降到2步，以支持实时定位。

Result: 在真实的 ray-traced 东京城域网场景中，DiffLoc 的最佳模型 DiffLoc-CT 达到0.5 cm 融合精度和1-2 cm 单基站精度；相比传统有监督回归（误差>10 m）和网格融合（约3 m），精度提升显著；推理步数显著减少，适用于15-25 m/s 的高速用户和未见轨迹。

Conclusion: 提出的 DiffLoc 框架在室外高精度定位方面展现出可行性，适用于实时6G应用，并具备在大规模、动态场景中扩展的潜力。

Abstract: This paper introduces a novel framework for high-accuracy outdoor user
equipment (UE) positioning that applies a conditional generative diffusion
model directly to high-dimensional massive MIMO channel state information
(CSI). Traditional fingerprinting methods struggle to scale to large, dynamic
outdoor environments and require dense, impractical data surveys. To overcome
these limitations, our approach learns a direct mapping from raw uplink
Sounding Reference Signal (SRS) fingerprints to continuous geographic
coordinates. We demonstrate that our DiffLoc framework achieves unprecedented
sub-centimeter precision, with our best model (DiffLoc-CT) delivering 0.5 cm
fusion accuracy and 1-2 cm single base station (BS) accuracy in a realistic,
ray-traced Tokyo urban macro-cell environment. This represents an
order-of-magnitude improvement over existing methods, including supervised
regression approaches (over 10 m error) and grid-based fusion (3 m error). Our
consistency training approach reduces inference time from 200 steps to just 2
steps while maintaining exceptional accuracy even for high-speed users (15-25
m/s) and unseen user trajectories, demonstrating the practical feasibility of
our framework for real-time 6G applications.

</details>


### [88] [Energy-Latency Optimization for Dynamic 5G Mobile Radio Access Networks](https://arxiv.org/abs/2510.14214)
*Gabriela N. Caspa H.,Carlos A. Astudillo,Nelson L. S. da Fonseca*

Main category: cs.NI

TL;DR: 提出一个三目标MILP模型用于5G RAN配置与功能分解，并辅以启发式算法，在考虑TSN的延迟约束下，平衡端到端时延与能耗，支持eMBB/URLLC/mMTC切片的FS选择、RAN功能放置与路由。


<details>
  <summary>Details</summary>
Motivation: 随着5G BS分拆和新的服务对带宽和低时延的要求，需在保持性能的同时降低能耗；现有研究多聚焦单一目标，缺乏同时考虑延迟与能耗的现实场景优化。

Method: 建立一个MILP模型，包含三个目标函数：最小化FH延迟、最小化能耗，以及同时权衡延迟与能耗的双目标优化；确定FS选项、RAN功能放置与路由，覆盖eMBB/URLLC/mMTC切片；引入TSN建模以分析延迟；在MILP计算量较大时，给出可行的启发式算法以符合RAN约束。

Result: 实验在多种拓扑和不同gNB需求下验证，揭示了延迟与能耗之间的权衡关系，并强调需要动态RAN重配置。提出的框架可用于优化现有与未来的RAN部署。

Conclusion: 多目标优化方法可在RAN配置中同时考量性能与能耗，TSN建模有助于更准确的时延评估；尽管MILP求解复杂，但在结合启发式策略后具备实用性，提供了对未来RAN部署的改进路径。

Abstract: In 5G networks, base station (BS) disaggregation and new services present
challenges in radio access network (RAN) configuration, particularly in meeting
their bandwidth and latency constraints. The BS disaggregation is enabled by
functional splitting (FS), which distributes the RAN functions in processing
nodes and alleviates latency and bandwidth requirements in the fronthaul (FH).
Besides network performance, energy consumption is a critical concern for
mobile network operators (MNO), since RAN operation constitutes a major portion
of their operational expenses (OPEX). RAN configuration optimization is
essential to balance service performance with cost-effective energy
consumption. In this paper, we propose a mixed-integer linear programming
(MILP) model formulated with three objective functions: (i) minimizing
fronthaul (FH) latency, (ii) minimizing energy consumption, and (iii) a
bi-objective optimization that jointly balances both latency and energy
consumption. The model determines the optimal FS option, RAN function
placement, and routing for eMBB, URLLC, and mMTC slices. Although prior studies
have addressed RAN configuration either from an energy minimization or latency
reduction perspective, few have considered both aspects in realistic scenarios.
Our evaluation spans different topologies, accounts for variations in
aggregated gNB demand, explores diverse FS combinations, and incorporates Time
Sensitive Networking (TSN) modeling for latency analysis, as it is also crucial
in RAN performance. Given that MILP's execution time can be significant, we
propose a heuristic algorithm that adheres to RAN constraints. Our results
reveal a trade-off between latency and energy consumption, highlighting the
need for dynamic RAN reconfiguration. These insights provide a foundation to
optimize existing and future RAN deployments.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [89] [Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic](https://arxiv.org/abs/2510.13822)
*Bartosz Burgiel*

Main category: cs.CR

TL;DR: 被动分析智能家居无线流量，即使数据被加密，也能从流量模式推断隐私信息（设备、活动状态、位置与布局），暴露隐私风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示智能家居场景下元数据泄露的风险，即使加密，旁观者（如墙后邻居）也可通过流量特征推断用户隐私行为。

Method: 采用被动分析方法，分析原始802.11数据包与BLE广播，进行设备识别、活动状态推断，并基于RSSI实现三边定位，实验设置模拟紧邻公寓的窃听者场景。

Result: 证明在加密数据下仍可检测多媒体设备的活跃时段，推断睡眠/工作/观看媒体等常见活动，甚至近似推导邻居公寓的布局，显示隐私风险不限于传统数据泄露的内容。

Conclusion: 强调智能家居的隐私风险不仅来自明文数据，带墙后监听的被动流量分析也能获得敏感信息，需考虑相应的缓解对策与安全设计。

Abstract: This thesis explores the extent to which passive observation of wireless
traffic in a smart home environment can be used to infer privacy-invasive
information about its inhabitants. Using a setup that mimics the capabilities
of a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and
Bluetooth Low Energy advertisemets. From this data, we identify devices, infer
their activity states and approximate their location using RSSI-based
trilateration. Despite the encrypted nature of the data, we demonstrate that it
is possible to detect active periods of multimedia devices, infer common
activities such as sleeping, working and consuming media, and even approximate
the layout of the neighbor's apartment. Our results show that privacy risks in
smart homes extend beyond traditional data breaches: a nosy neighbor behind the
wall can gain privacy-invasive insights into the lives of their neighbors
purely from encrypted network traffic.

</details>


### [90] [PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)
*Wei Zou,Yupei Liu,Yanting Wang,Ying Chen,Neil Gong,Jinyuan Jia*

Main category: cs.CR

TL;DR: 提出 PIShield，一种高效的提示注入检测方法。通过在 LLM 的特定层的最终令牌内部表示中提取特征，使用线性分类器对清洁与污染提示进行区分。在 5 个基准数据集和 8 种攻击场景下，相对于 11 个基线方法，PIShield 表现出显著优势并能抵抗自适应攻击。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击对依赖 LLM 的应用构成安全威胁，现有检测方法要么性能不足，要么计算开销高，需要一个高效且有效的解决方案。

Method: 识别注入关键层的内部表示，提取最后一个令牌在该层的向量特征；对带标签的清洁/污染提示数据集训练一个简单的线性分类器来区分两者。

Result: 在 5 个基准数据集和 8 种攻击上，与 11 种基线方法对比，PIShield 表现出高效且有效的对比优势，显著超越现有方法；还能抵抗强自适应攻击。

Conclusion: PIShield 提供了一个实用且高效的提示注入防御方案，所用特征与模型可扩展并对自适应攻击具鲁棒性。

Abstract: LLM-integrated applications are vulnerable to prompt injection attacks, where
an attacker contaminates the input to inject malicious prompts, causing the LLM
to follow the attacker's intent instead of the original user's. Existing prompt
injection detection methods often have sub-optimal performance and/or high
computational overhead. In this work, we propose PIShield, a detection method
that is both effective and efficient. Our key observation is that the internal
representation of the final token in a prompt-extracted from a specific layer
of the LLM, which we term the injection-critical layer-captures distinguishing
features between clean and contaminated prompts. Leveraging this insight, we
train a simple linear classifier on these internal representations using a
labeled set of clean and contaminated prompts. We compare PIShield against 11
baselines across 5 diverse benchmark datasets and 8 prompt injection attacks.
The results demonstrate that PIShield is both highly effective and efficient,
substantially outperforming existing methods. Additionally, we show that
PIShield resists strong adaptive attacks.

</details>


### [91] [Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments](https://arxiv.org/abs/2510.14066)
*Rajendra Upadhyay,Al Nahian Bin Emran,Rajendra Paudyal,Lisa Donnan,Duminda Wijesekera*

Main category: cs.CR

TL;DR: 端到端仿真显示，在混合地面-LEO卫星的5G系统中，回退/local控制对限定恶意无人机入侵的时延至关重要；额外切换影响有限；卫星中断若无回退可能造成不可界定的缓解延迟。


<details>
  <summary>Details</summary>
Motivation: 未协作的无人机对关键基础设施构成威胁，需要理解检测到缓解的时延及在5G+LEO卫星场景下的鲁棒性机制。

Method: 构建端到端仿真框架，模型包含地面gNB、卫星回传链路的随机中断、基于切换不稳定性与信号质量波动触发的检测逻辑；在检测后触发封锁/限速机制，并提供可选的本地回退以降低缓解延迟；通过蒙特卡洛试验在无人机高度、速度和卫星 outage 率上做扫掠。

Result: 结果显示，卫星回传中断可导致缓解延迟呈任意增大趋势；为满足回退期限，需对延迟进行有效界定/限制；切换不稳定性在所考参数范围内影响甚微；回退的鲁棒性主要体现在延迟限制上，能防止在中断期间无人机在受限区域内逗留；在压力情景下，回退对避免控制面和物理安全漏洞至关重要；巡逻UE对周边产生的副作用很小，切换率接近地面基线。

Conclusion: 应将非地面链路与本地控制结合起来，以确保对未合作无人机入侵的鲁棒、及时响应。

Abstract: Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to
critical infrastructure and border protection by operating as rogue user
equipment (UE) within cellular networks, consuming resources, creating
interference, and potentially violating restricted airspaces. This paper
presents minimal features of the operating space, yet an end-to-end simulation
framework to analyze detect-to-mitigate latency of such intrusions in a hybrid
terrestrial-non-terrestrial (LEO satellite) 5G system. The system model
includes terrestrial gNBs, satellite backhaul (with stochastic outages), and a
detection logic (triggered by handover instability and signal quality
variance). A lockdown mechanism is invoked upon detection, with optional local
fallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes,
speeds, and satellite outage rates yield several insights. First, satellite
backhaul outages can cause arbitrarily long mitigation delays, yet, to meet
fallback deadlines, they need to be effectively bounded. Second, while handover
instability was hypothesized, our results show that extra handovers have a
negligible effect within the range of parameters we considered. The main
benefit of resilience from fallback comes from the delay in limiting
mitigation. Third, patrol UEs experience negligible collateral impact, with
handover rates close to terrestrial baselines. Stress scenarios further
highlight that fallback is indispensable in preventing extreme control-plane
and physical security vulnerabilities: Without fallback, prolonged outages in
the satellite backhaul delay lockdown commands, allowing rogue UAVs to linger
inside restricted corridors for several seconds longer. These results
underscore the importance of complementing non-terrestrial links with local
control to ensure robust and timely response against uncooperative UAV
intrusions.

</details>


### [92] [Every Language Model Has a Forgery-Resistant Signature](https://arxiv.org/abs/2510.14086)
*Matthew Finlayson,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CR

TL;DR: Ellipse-based signatures in logprobs can identify the source language model, are hard to forge, naturally occurring, self-contained, and compact; they enable a cryptographic-like output verification protocol, though scaling to production models poses challenges.


<details>
  <summary>Details</summary>
Motivation: With widespread closed-weight LMs and public APIs, there is a need for forensic methods to infer hidden model details and to identify the originating model from outputs. The paper argues that a geometric ellipse constraint on model outputs serves as a robust, detectable signature.

Method: Demonstrate that LM outputs lie on a high-dimensional ellipse surface, treat this ellipse as a model signature, develop techniques to extract the ellipse from small models, analyze its distinctive properties compared to existing fingerprints, and propose a protocol for LM output verification analogous to cryptographic MACs.

Result: The ellipse signature is hard to forge without access to model parameters; it is naturally occurring for all LMs, self-contained (detectable without inputs or full weights), and compact/redundant (detectable per-output). The paper evaluates extraction of the ellipse from small models and discusses practical hurdles for production-scale models, and proposes a language-model output verification protocol inspired by symmetric-key MACs.

Conclusion: Ellipse-based signatures offer a promising, robust mechanism for model identification and output verification that does not require access to inputs or weights; however, scaling to large production models and operational deployment present challenges that need addressing.

Abstract: The ubiquity of closed-weight language models with public-facing APIs has
generated interest in forensic methods, both for extracting hidden model
details (e.g., parameters) and for identifying models by their outputs. One
successful approach to these goals has been to exploit the geometric
constraints imposed by the language model architecture and parameters. In this
work, we show that a lesser-known geometric constraint--namely, that language
model outputs lie on the surface of a high-dimensional ellipse--functions as a
signature for the model and can be used to identify the source model of a given
output. This ellipse signature has unique properties that distinguish it from
existing model-output association methods like language model fingerprints. In
particular, the signature is hard to forge: without direct access to model
parameters, it is practically infeasible to produce log-probabilities
(logprobs) on the ellipse. Secondly, the signature is naturally occurring,
since all language models have these elliptical constraints. Thirdly, the
signature is self-contained, in that it is detectable without access to the
model inputs or the full weights. Finally, the signature is compact and
redundant, as it is independently detectable in each logprob output from the
model. We evaluate a novel technique for extracting the ellipse from small
models and discuss the practical hurdles that make it infeasible for
production-scale models. Finally, we use ellipse signatures to propose a
protocol for language model output verification, analogous to cryptographic
symmetric-key message authentication systems.

</details>


### [93] [RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models](https://arxiv.org/abs/2510.14233)
*Fanchao Meng,Jiaping Gui,Yunbo Li,Yue Wu*

Main category: cs.CR

TL;DR: RHINO 将基于LLM的攻击分析分解为三阶段推理：行为抽象、多人协同推断、验证，以提升可解释性、鲁棒性及对MITRE ATT&CK的对齐。


<details>
  <summary>Details</summary>
Motivation: 解决低级日志告警的语义碎片化问题，克服规则基和单步分类的局限，减少模型幻觉，使LLM在威胁分析中的推理更可解释且可扩展。

Method: 提出三阶段框架：1) 行为抽象：将原始日志转化为情境化叙事；2) 多角色协同推断：基于MITRE ATT&CK进行候选技术推断；3) 验证：与官方MITRE定义比对以纠正幻觉，并跨模型评估。

Result: 在三个基准数据集、四个骨干模型上，RHINO实现86.38%到88.45%的准确率，模型间相对提升介于24.25%到76.50%。

Conclusion: RHINO提高了威胁分析的可解释性和可扩展性，为在运营安全场景中部署LLMs提供了一个可行蓝图。

Abstract: Modern Network Intrusion Detection Systems generate vast volumes of low-level
alerts, yet these outputs remain semantically fragmented, requiring
labor-intensive manual correlation with high-level adversarial behaviors.
Existing solutions for automating this mapping-rule-based systems and machine
learning classifiers-suffer from critical limitations: rule-based approaches
fail to adapt to novel attack variations, while machine learning methods lack
contextual awareness and treat tactic-technique mapping as a syntactic matching
problem rather than a reasoning task. Although Large Language Models have shown
promise in cybersecurity tasks, preliminary experiments reveal that existing
LLM-based methods frequently hallucinate technique names or produce
decontextualized mappings due to their single-step classification approach.
  To address these challenges, we introduce RHINO, a novel framework that
decomposes LLM-based attack analysis into three interpretable phases mirroring
human reasoning: (1) behavioral abstraction, where raw logs are translated into
contextualized narratives; (2) multi-role collaborative inference, generating
candidate techniques by evaluating behavioral evidence against MITRE ATT&CK
knowledge; and (3) validation, cross-referencing predictions with official
MITRE definitions to rectify hallucinations. RHINO bridges the semantic gap
between low-level observations and adversarial intent while improving output
reliability through structured reasoning.
  We evaluate RHINO on three benchmarks across four backbone models. RHINO
achieved high accuracy, with model performance ranging from 86.38% to 88.45%,
resulting in relative gains from 24.25% to 76.50% across different models. Our
results demonstrate that RHINO significantly enhances the interpretability and
scalability of threat analysis, offering a blueprint for deploying LLMs in
operational security settings.

</details>


### [94] [Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks](https://arxiv.org/abs/2510.14283)
*Xinhao Deng,Jingyou Chen,Linxiao Yu,Yixiang Zhang,Zhongyi Gu,Changhao Qiu,Xiyuan Zhao,Ke Xu,Qi Li*

Main category: cs.CR

TL;DR: 提出一个多维评估框架，对现有网站指纹攻击在现实世界条件下进行系统性、全面评估，发现多数攻击在多种现实挑战下性能显著下降，强调需要更健壮、实用的攻击模型。


<details>
  <summary>Details</summary>
Motivation: 当前大多数 WF 研究只在单一场景下评估，难以反映现实世界的复杂性，导致攻防研究的外推性不足。

Method: 在包括防御、流量漂移、多标签浏览、早期检测、开放世界和小样本场景等多种现实条件下，对现有 WF 攻击进行系统性评估，并提出一个多维评估框架。

Result: 实验结果表明，许多在孤立场景下表现较好的 WF 技术在面对其他条件时性能显著下降；真实世界环境往往同时组合多种挑战，导致目前的攻击难以直接落地。

Conclusion: 该研究揭示了 WF 攻击的局限性，并提供一个多维评估框架，为开发更稳健、实用的 WF 攻击提供关键洞察。

Abstract: Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to
infer the websites visited by users, posing a serious threat to anonymous
communication systems. Although recent WF techniques achieve over 90% accuracy
in controlled experimental settings, most studies remain confined to single
scenarios, overlooking the complexity of real-world environments. This paper
presents the first systematic and comprehensive evaluation of existing WF
attacks under diverse realistic conditions, including defense mechanisms,
traffic drift, multi-tab browsing, early-stage detection, open-world settings,
and few-shot scenarios. Experimental results show that many WF techniques with
strong performance in isolated settings degrade significantly when facing other
conditions. Since real-world environments often combine multiple challenges,
current WF attacks are difficult to apply directly in practice. This study
highlights the limitations of WF attacks and introduces a multidimensional
evaluation framework, offering critical insights for developing more robust and
practical WF attacks.

</details>


### [95] [BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection](https://arxiv.org/abs/2510.14344)
*Zichen Liu,Shao Yang,Xusheng Xiao*

Main category: cs.CR

TL;DR: 提出 BINCTX 的多模态学习框架，通过三种视图（字节码图像、上下文视图、第三方库使用路径）对应用进行行为检测，融合特征以训练上下文感知分类器。对现实恶意软件与正常应用实现高宏 F1 ~94.7%，对混淆具有鲁棒性，抗对抗样本能力优于基于字节码的方法。


<details>
  <summary>Details</summary>
Motivation: 移动应用市场中存在大量应用，仍有大量不良行为难以捕捉，因为它们往往不依赖于权限受保护的 API，且可通过 UI/元数据编辑进行伪装，因此需要跨模态的表示来揭示潜在行为

Method: 构建三类视图： (i) 全局字节码作为图像的视图，捕捉代码级语义与家族化模式；(ii) 上下文视图（清单行为、组件、声明的权限、URL/IP 常量），指示行为触发方式；(iii) 第三方库使用视图，汇总沿跨组件调用路径的调用频率。对三视图进行嵌入并融合，训练一个具备上下文感知能力的分类器

Result: 在真实世界的恶意软件与正常应用数据集上，BINCTX 实现宏 F1 94.73%，较强基线至少提升 14.92%；在商业混淆下保持鲁棒性（混淆后 F1 约 84%）；对对抗样本相较于仅基于字节码的系统更具抵抗力

Conclusion: 多模态特征融合显著提升对不良行为的检测性能，并对混淆和对抗攻击具有较好的鲁棒性，显示跨模态表示在移动应用安全领域的潜力。

Abstract: Mobile app markets host millions of apps, yet undesired behaviors (e.g.,
disruptive ads, illegal redirection, payment deception) remain hard to catch
because they often do not rely on permission-protected APIs and can be easily
camouflaged via UI or metadata edits. We present BINCTX, a learning approach
that builds multi-modal representations of an app from (i) a global
bytecode-as-image view that captures code-level semantics and family-style
patterns, (ii) a contextual view (manifested actions, components, declared
permissions, URL/IP constants) indicating how behaviors are triggered, and
(iii) a third-party-library usage view summarizing invocation frequencies along
inter-component call paths. The three views are embedded and fused to train a
contextual-aware classifier. On real-world malware and benign apps, BINCTX
attains a macro F1 of 94.73%, outperforming strong baselines by at least
14.92%. It remains robust under commercial obfuscation (F1 84%
post-obfuscation) and is more resistant to adversarial samples than
state-of-the-art bytecode-only systems.

</details>


### [96] [Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries](https://arxiv.org/abs/2510.14384)
*Sebastian Jänich,Merlin Sievers,Johannes Kinder*

Main category: cs.CR

TL;DR: 提出在二进制层对物联网固件进行最小侵入式修补，通过局部重组自动修补已知漏洞；在 MAGMA 和 KARONTE 数据集上取得较高修补率（83%/96%）。


<details>
  <summary>Details</summary>
Motivation: 物联网设备普遍存在更新困难，易部署过时且存在漏洞的软件；需要一种低成本、无需厂商支持的修补方法，降低副作用和破坏性。

Method: 引入最小侵入的本地重组(local reassembly)技术，在不改变系统总体架构的前提下，自动对已知漏洞进行二进制级修补，尽量减少副作用和破坏性。对108个 MAGMA 基准测试二进制和30个 KARONTE 实际固件进行评估。

Result: 在 MAGMA 中成功修补83%的目标漏洞，在 KARONTE 固件数据集中修补成功率达96%。

Conclusion: 证明在物联网固件层进行二进制级修补的可行性与有效性，能够提升弱势设备的安全性，同时尽量避免引入破坏性变更；未来工作可聚焦于处理更复杂二进制与混淆代码，以及扩展修补场景的覆盖范围。

Abstract: Low-cost Internet of Things (IoT) devices are increasingly popular but often
insecure due to poor update regimes. As a result, many devices run outdated and
known-vulnerable versions of open-source software. We address this problem by
proposing to patch IoT firmware at the binary level, without requiring vendor
support. In particular, we introduce minimally invasive local reassembly, a new
technique for automatically patching known (n-day) vulnerabilities in IoT
firmware. Our approach is designed to minimize side effects and reduce the risk
of introducing breaking changes. We systematically evaluate our approach both
on 108 binaries within the controlled environment of the MAGMA benchmarks, as
well as on 30 real-world Linux-based IoT firmware images from the KARONTE
dataset. Our prototype successfully patches 83% of targeted vulnerabilities in
MAGMA and 96% in the firmware dataset.

</details>


### [97] [Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models](https://arxiv.org/abs/2510.14470)
*Xiaoyu Xue,Yuni Lai,Chenxi Huang,Yulin Zhu,Gaolei Li,Xiaoge Zhang,Kai Zhou*

Main category: cs.CR

TL;DR: 提出一种面向文本与结构的双触发后门框架，能在文本属性不可访问的 TAG 场景下，通过预设文本池实现文本级和结构级触发，从而在保持高清洁精度的同时实现高攻击成功率，揭示基于语言模型的图 foundation 模型在开源环境中的后门风险。


<details>
  <summary>Details</summary>
Motivation: 随着图数据的广泛应用与语言模型的融合，LM-增强的图学习模型在文本属性图上的表现突出，但在提示微调阶段暴露安全隐患。现有的图后门研究多针对传统 GNN，缺乏对 LM-驱动 GFMs 在属性不可访问情境下的攻击与鲁棒性研究，因此有必要系统性分析并提出对抗视角的攻击-防护。

Method: 提出一个双触发后门攻击框架，分别在文本层级和结构层级触发。该框架无需对触发节点文本属性进行显式优化，依赖一个预先建立的文本池来实现触发文本的选择与替换，并且能够在属性不可访问/受限的 TAG 场景下执行。

Result: 通过大量实验，攻击在保持高的干净准确率的同时，获得显著的高攻击成功率，即使在极度隐蔽的单触发节点场景也能达到有效攻击。不同于传统的单触发后门，该方法在 LM-驱动的 GFMs 上展现出更强的鲁棒性与隐蔽性。

Conclusion: 该工作揭示了网页端部署的 LM-增强 GFMs 在开放源代码平台上的后门风险，强调需要在基础模型时代加强监督与防护机制，促进更鲁棒的评估与防御研究，以应对面向文本-属性图的未来攻击。

Abstract: The emergence of graph foundation models (GFMs), particularly those
incorporating language models (LMs), has revolutionized graph learning and
demonstrated remarkable performance on text-attributed graphs (TAGs). However,
compared to traditional GNNs, these LM-empowered GFMs introduce unique security
vulnerabilities during the unsecured prompt tuning phase that remain
understudied in current research. Through empirical investigation, we reveal a
significant performance degradation in traditional graph backdoor attacks when
operating in attribute-inaccessible constrained TAG systems without explicit
trigger node attribute optimization. To address this, we propose a novel
dual-trigger backdoor attack framework that operates at both text-level and
struct-level, enabling effective attacks without explicit optimization of
trigger node text attributes through the strategic utilization of a
pre-established text pool. Extensive experimental evaluations demonstrate that
our attack maintains superior clean accuracy while achieving outstanding attack
success rates, including scenarios with highly concealed single-trigger nodes.
Our work highlights critical backdoor risks in web-deployed LM-empowered GFMs
and contributes to the development of more robust supervision mechanisms for
open-source platforms in the era of foundation models.

</details>


### [98] [Certifying optimal MEV strategies with Lean](https://arxiv.org/abs/2510.14480)
*Massimo Bartoletti,Riccardo Marchesin,Roberto Zunino*

Main category: cs.CR

TL;DR: 提出对 MEV 的机器可检查形式化与证明框架，在 Lean 上构建可证明的 MEV 上界，并对两种典型 DeFi 协议建模分析，尤有在 Automated Market Makers 中给出 sandwich 攻击最优性的机器可检证明。


<details>
  <summary>Details</summary>
Motivation: MEV 攻击对区块链安全具有系统性影响，现有基于经验和逐条分析的方法难以提供严格的上界，需要可验证的证明方法来确保协议设计的正确性与鲁棒性。

Method: 在 Lean theorem prover 上建立通用的 MEV 上界证明方法；对两类典型 DeFi 协议进行形式化建模与分析；首次给出 sandwich 攻击在 AMM 中的最优性机器可检证明。

Result: 实现了 MEV 的首个机器可检查形式化，提供可重复的上界证明方法；完成对两种协议的建模分析，特别是 AMM 的 sandwich 攻击最优性的机器证明。

Conclusion: 将可验证性方法引入 MEV 研究，提升对 DeFi 安全性分析的正确性和鲁棒性，展示形式化方法在高风险金融协议中的应用潜力。

Abstract: Maximal Extractable Value (MEV) refers to a class of attacks to decentralized
applications where the adversary profits by manipulating the ordering,
inclusion, or exclusion of transactions in a blockchain. Decentralized Finance
(DeFi) protocols are a primary target of these attacks, as their logic depends
critically on transaction sequencing. To date, MEV attacks have already
extracted billions of dollars in value, underscoring their systemic impact on
blockchain security. Verifying the absence of MEV attacks requires determining
suitable upper bounds, i.e. proving that no adversarial strategy can extract
more value (if any) than expected by protocol designers. This problem is
notoriously difficult: the space of adversarial strategies is extremely vast,
making empirical studies and pen-and-paper reasoning insufficiently rigorous.
In this paper, we present the first mechanized formalization of MEV in the Lean
theorem prover. We introduce a methodology to construct machine-checked proofs
of MEV bounds, providing correctness guarantees beyond what is possible with
existing techniques. To demonstrate the generality of our approach, we model
and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the
first machine-checked proof of the optimality of sandwich attacks in Automated
Market Makers, a fundamental DeFi primitive.

</details>


### [99] [Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration](https://arxiv.org/abs/2510.14522)
*Evangelos Lamprou,Julian Dai,Grigoris Ntousakis,Martin C. Rinard,Nikos Vasilakis*

Main category: cs.CR

TL;DR: Lexo automatically learns a component's observable behavior and synthesizes a vulnerability-free version that preserves original functionality, using multiple guarded LLMs; validated on 100+ real-world packages and high-profile attacks, enabling fast, compatible removal of malicious code.


<details>
  <summary>Details</summary>
Motivation: Open-source software supply-chain attacks can embed stealthy malicious functionality; defenders need automated, scalable methods to regenerate safe versions without breaking behavior.

Method: Lexo models full observable behavior by generating input-output pairs, then synthesizes a new component implementing the original functionality while avoiding malicious behavior. It uses several LLM instances with correctness/coverage-guided prompts and guardrails to supervise outputs.

Result: Demonstrates scalability across domains; regeneration time under 100 seconds on average; maintains compatibility; eliminates malicious code in several real-world supply-chain attacks, even when a state-of-the-art LLM fails to remove it when prompted.

Conclusion: Automated regeneration with multi-LLM guardrails is a promising approach to mitigating stealthy OSS supply-chain threats, enabling safer reuse of components without sacrificing functionality.

Abstract: Software supply-chain attacks are an important and ongoing concern in the
open source software ecosystem. These attacks maintain the standard
functionality that a component implements, but additionally hide malicious
functionality activated only when the component reaches its target environment.
Lexo addresses such stealthy attacks by automatically learning and regenerating
vulnerability-free versions of potentially malicious components. Lexo first
generates a set of input-output pairs to model a component's full observable
behavior, which it then uses to synthesize a new version of the original
component. The new component implements the original functionality but avoids
stealthy malicious behavior. Throughout this regeneration process, Lexo
consults several distinct instances of Large Language Models (LLMs), uses
correctness and coverage metrics to shepherd these instances, and guardrails
their results. Our evaluation on 100+ real-world packages, including high
profile stealthy supply-chain attacks, indicates that Lexo scales across
multiple domains, regenerates code efficiently (<100s on average), maintains
compatibility, and succeeds in eliminating malicious code in several real-world
supply-chain-attacks, even in cases when a state-of-the-art LLM fails to
eliminate malicious code when prompted to do so.

</details>


### [100] [Symbolic verification of Apple's Find My location-tracking protocol](https://arxiv.org/abs/2510.14589)
*Vaishnavi Sundararajan,Rithwik*

Main category: cs.CR

TL;DR: 本文提出对 Apple Find My 等众包定位系统的符号模型，并给出可验证的性质规格，在 Tamarin 验证器中给出自动化、可机器检查的证明。


<details>
  <summary>Details</summary>
Motivation: 在众包定位环境中，用户隐私和监控问题突出，且即便依赖强加密也可能存在协议层的逻辑缺陷。对 Find My 之类系统进行形式化建模与证明，有助于在设计阶段发现漏洞并提升隐私保障对等性。

Method: 建立 Find My 协议的符号模型，并给出可明确的、理想化的性质规范；使用 Tamarin 验证器对这些性质进行自动化、机器可检查的证明。

Result: 给出针对所定性质的机器可检验证明，验证了在给定模型下的隐私与安全属性的一致性。

Conclusion: 该工作通过形式化建模与自动化证明提升对大规模众包定位系统隐私保护的信心，但结果依赖于对现实系统的正确建模与性质定义，且需警惕模型之外的侧信道与实现层缺陷。

Abstract: Tracking devices, while designed to help users find their belongings in case
of loss/theft, bring in new questions about privacy and surveillance of not
just their own users, but in the case of crowd-sourced location tracking, even
that of others even orthogonally associated with these platforms. Apple's Find
My is perhaps the most ubiquitous such system which can even locate devices
which do not possess any cellular support or GPS, running on millions of
devices worldwide. Apple claims that this system is private and secure, but the
code is proprietary, and such claims have to be taken on faith. It is well
known that even with perfect cryptographic guarantees, logical flaws might
creep into protocols, and allow undesirable attacks. In this paper, we present
a symbolic model of the Find My protocol, as well as a precise formal
specification of desirable properties, and provide automated, machine-checkable
proofs of these properties in the Tamarin prover.

</details>


### [101] [AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX](https://arxiv.org/abs/2510.14675)
*Nicolas Dutly,Friederike Groschupp,Ivan Puddu,Kari Kostiainen,Srdjan Capkun*

Main category: cs.CR

TL;DR: AEX-Notify对抗中断基于逐步执行的攻击并非全面有效，提出AEX-NStep及两种概率性中断计数攻击，能在AEX-Notify保护的SGX enclave上实现ECDSA密钥泄漏。这扩展了对AEX-Notify的安全分析，并指向需要更强的缓解措施。


<details>
  <summary>Details</summary>
Motivation: 评估AEX-Notify在现实攻击场景中的防护强度，特别是对抗基于中断的攻击（如SGX-Step）的能力，以及是否还存在可行的概率性攻击路径与密钥泄漏风险。

Method: 提出AEX-NStep作为对AEX-Notify的新攻击向量，证明对中断计数攻击不必依赖确定的单步执行；设计两种概率性中断计数攻击用于评估安全性，并用其构建在AEX-Notify启用的SGX enclave上的ECDSA密钥泄漏攻击。

Result: 证明确定性单步并非必需条件，AEX-Notify的“前进加密/模糊”安全性承诺不成立；提出并验证两种概率性攻击路径；给出在AEX-Notify保护的 enclave上实现ECDSA密钥泄漏的可行性。

Conclusion: 扩展了AEX-Notify的安全分析，表明需要更强的缓解措施来实现对中断计数攻击的全面防护，并为未来设计提供改进方向与安全评估基准。

Abstract: To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel
introduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent
deterministic single-stepping. In this work, we introduce AEX-NStep, the first
interrupt counting attack on AEX-Notify-enabled Enclaves. We show that
deterministic single-stepping is not required for interrupt counting attacks to
be practical and that, therefore, AEX-Notify does not entirely prevent such
attacks. We specifically show that one of AEX-Notify's security guarantees,
obfuscated forward progress, does not hold, and we introduce two new
probabilistic interrupt counting attacks. We use these attacks to construct a
practical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our
results extend the original security analysis of AEX-Notify and inform the
design of future mitigations.

</details>


### [102] [FibRace: a large-scale benchmark of client-side proving on mobile devices](https://arxiv.org/abs/2510.14693)
*Simon Malatrait,Alex Sirac*

Main category: cs.CR

TL;DR: FibRace是首次在手机端进行大规模零知识证明生成的实证测试，显示现代智能手机能在不依赖远程证明者或专用硬件的情况下快速地产生证明，并提供全球设备的基线数据。


<details>
  <summary>Details</summary>
Motivation: 验证移动设备在本地生成零知识证明的可行性与性能，为轻量化证明、隐私保护应用及链上验证的扩展性提供实证基线。

Method: 通过将 Cairo M 的证明生成实现包装成一个移动游戏，吸引玩家参与，在三周内收集设备、RAM、SoC 等参数与证明耗时等数据，覆盖 99 个国家的 6,047 名玩家和 1,420 种设备。

Result: 累计生成 2,195,488 个证明，覆盖 1,420 种设备模型；多数现代手机在 5 秒内完成证明；RAM ≥3 GB 与 SoC 性能是关键因素；Apple 的 A19 Pro 与 M 系列芯片最优；Hyli 的区块链在链上逐一验证每个证明且无拥堵；提供迄今为止最全面的移动证明性能数据集。

Conclusion: FibRace 为移动端证明能力提供实证基线，推动面向轻量证明、隐私保护移动应用与基于证明的基础设施研究；未来可扩展到更多设备、改进证明引擎、优化移动端用户体验并考量能源消耗等因素。

Abstract: FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale
experiment to test client-side proof generation on smartphones using Cairo M.
Presented as a mobile game in which players proved Fibonacci numbers and
climbed a leaderboard, FibRace served a dual purpose: to engage the public and
to provide empirical benchmarking. Over a three-week campaign (September 11-30,
2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420
unique device models. The results show that most modern smartphones can
complete a proof in under 5 seconds, confirming that *mobile devices are now
capable of producing zero-knowledge proofs reliably*, without the need for
remote provers or specialized hardware. Performance was correlated primarily
with RAM capacity and SoC (System on Chip) performance: devices with at least 3
GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the
fastest proving times. Hyli's blockchain natively verified every proof onchain
without congestion. FibRace provides the most comprehensive dataset to date on
mobile proving performance, establishing a practical baseline for future
research in lightweight provers, proof-powered infrastructure, and
privacy-preserving mobile applications.

</details>


### [103] [SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services](https://arxiv.org/abs/2510.14708)
*Ha Xuan Son,Nguyen Quoc Anh,Phat T. Tran-Truong,Le Thanh Tuan,Pham Thanh Nghiem*

Main category: cs.CR

TL;DR: 提出SLIE，一种基于WKD-IBE的轻量级身份加密，面向IoMT的端到端加密、分层访问控制与轻量化密钥管理，具对侧信道攻击防护与基于到期的密钥撤销机制；在性能上显著优于RSA。


<details>
  <summary>Details</summary>
Motivation: IoMT在设备管理与通信中存在显著安全漏洞，数据敏感性要求高安全、可扩展的信任与访问控制；资源受限的医疗设备需要高效、轻量的加密方案以符合HIPAA/GDPR等合规要求。

Method: 设计并实现基于Wildcard Key Derivation Identity-Based Encryption（WKD-IBE）的SLIE系统，包含常量时间操作、内存混淆和到期撤销机制的侧信道防护；实现端到端加密、层次化访问控制与轻量化密钥管理。

Result: 对比RSA，1KB数据的加密时间0.936ms、解密时间0.217ms，分别提升约84.54%与99.70%；能效0.014 J/KB。

Conclusion: SLIE在IoMT场景中满足端到端安全、可扩展信任与合规性需求，并在性能与能效方面优于RSA，但需结合实际部署评估对侧信道鲁棒性及密钥管理开销等实际因素。

Abstract: The Internet of Medical Things (IoMT) has revolutionized healthcare by
transforming medical operations into standardized, interoperable services.
However, this service-oriented model introduces significant security
vulnerabilities in device management and communication, which are especially
critical given the sensitivity of medical data. To address these risks, this
paper proposes SLIE (Secure and Lightweight Identity Encryption), a novel
cryptosystem based on Wildcard Key Derivation Identity-Based Encryption
(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication
through end-to-end encryption, hierarchical access control, and a lightweight
key management system designed for resource-constrained devices. It
incorporates constant-time operations, memory obfuscation, and expiry-based key
revocation to counter side-channel, man-in-the-middle, and unauthorized access
attacks, thereby ensuring compliance with standards like HIPAA and GDPR.
Evaluations show that SLIE significantly outperforms RSA, with encryption and
decryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement
in encryption speed, a 99.70% improvement in decryption speed, and an energy
efficiency of 0.014 J/KB.

</details>


### [104] [Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning](https://arxiv.org/abs/2510.14894)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 提出用于秘密稀疏矩阵乘法的MPC算法，降低内存与通信成本，适用于高维稀疏数据的ML场景，并提出在稀疏矩阵的非零分布上采用安全上界的新方法。


<details>
  <summary>Details</summary>
Motivation: 在隐私保护下对稀疏数据进行分布式机器学习，但现有MPC框架对稀疏数据效率低下，难以处理高维稀疏数据，内存成本高，限制了在推荐系统、基因组学等场景的应用。

Method: 提出针对秘密稀疏矩阵的MPC乘法算法，避免将数据表示为密集矩阵以缓解内存瓶颈，并通过优化通信量实现显著的降低（在现实规模下某些实验达到1000x的通信节省）。算法还引入对非零分布的安全上界假设，以兼容不同数据集的幂律分布特征，提升实际可用性。对两种ML应用进行了验证，表明现有协议难以应用且所提方法更具可行性。

Result: 在实际问题规模下显著降低通信成本，某些实验显示通信开销可降至千分之一级；在两种ML应用中验证了稀疏MPC乘法的可行性和有效性，且比基线密集实现具显著优势。

Conclusion: 为稀疏数据下的隐私保护机器学习提供可扩展的MPC矩阵乘法方案，通过对非零分布的安全上界减少对先验信息的依赖，使稀疏数据在隐私保护前提下的ML应用更具现实性。

Abstract: To preserve privacy, multi-party computation (MPC) enables executing Machine
Learning (ML) algorithms on secret-shared or encrypted data. However, existing
MPC frameworks are not optimized for sparse data. This makes them unsuitable
for ML applications involving sparse data, e.g., recommender systems or
genomics. Even in plaintext, such applications involve high-dimensional sparse
data, that cannot be processed without sparsity-related optimizations due to
prohibitively large memory requirements.
  Since matrix multiplication is central in ML algorithms, we propose MPC
algorithms to multiply secret sparse matrices. On the one hand, our algorithms
avoid the memory issues of the "dense" data representation of classic secure
matrix multiplication algorithms. On the other hand, our algorithms can
significantly reduce communication costs (some experiments show a factor 1000)
for realistic problem sizes. We validate our algorithms in two ML applications
in which existing protocols are impractical.
  An important question when developing MPC algorithms is what assumptions can
be made. In our case, if the number of non-zeros in a row is a sensitive piece
of information then a short runtime may reveal that the number of non-zeros is
small. Existing approaches make relatively simple assumptions, e.g., that there
is a universal upper bound to the number of non-zeros in a row. This often
doesn't align with statistical reality, in a lot of sparse datasets the amount
of data per instance satisfies a power law. We propose an approach which allows
adopting a safe upper bound on the distribution of non-zeros in rows/columns of
sparse matrices.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [105] [Multi-Period Sparse Optimization for Proactive Grid Blackout Diagnosis](https://arxiv.org/abs/2510.14045)
*Qinghua Ma,Reetam Sen Biswas,Denis Osipov,Guannan Qu,Soummya Kar,Shimiao Li*

Main category: eess.SY

TL;DR: A multi-period sparse optimization approach identifies persistent vulnerability locations in power grids under escalating stress, enabling early warning of potential collapses, with good scalability.


<details>
  <summary>Details</summary>
Motivation: Extreme events in power grids often share failure locations but differ in severity; there is a need to discover persistent vulnerabilities across sequences of stressed systems to improve resilience.

Method: Introduce a multi-period sparse optimization framework that enforces persistency constraints across scenarios; utilize circuit-theory based power-flow formulations and circuit-inspired optimization heuristics to scale to large grids.

Result: Experiments on benchmark systems demonstrate reliable tracking of persistent vulnerability locations as load increases, with scalability to large systems (around 200 s per scenario for 2000+ bus systems).

Conclusion: The proposed approach provides proactive vulnerability identification across stressed conditions and is scalable for large-scale power grids.

Abstract: Existing or planned power grids need to evaluate survivability under extreme
events, like a number of peak load overloading conditions, which could possibly
cause system collapses (i.e. blackouts). For realistic extreme events that are
correlated or share similar patterns, it is reasonable to expect that the
dominant vulnerability or failure sources behind them share the same locations
but with different severity. Early warning diagnosis that proactively
identifies the key vulnerabilities responsible for a number of system collapses
of interest can significantly enhance resilience. This paper proposes a
multi-period sparse optimization method, enabling the discovery of {persistent
failure sources} across a sequence of collapsed systems with increasing system
stress, such as rising demand or worsening contingencies. This work defines
persistency and efficiently integrates persistency constraints to capture the
``hidden'' evolving vulnerabilities. Circuit-theory based power flow
formulations and circuit-inspired optimization heuristics are used to
facilitate the scalability of the method. Experiments on benchmark systems show
that the method reliably tracks persistent vulnerability locations under
increasing load stress, and solves with scalability to large systems ({on
average} taking {around} 200 s per scenario on 2000+ bus systems).

</details>


### [106] [DiffOPF: Diffusion Solver for Optimal Power Flow](https://arxiv.org/abs/2510.14075)
*Milad Hoseinpour,Vladimir Dvorkin*

Main category: eess.SY

TL;DR: DiffOPF 将 OPF 问题视为条件采样问题，通过扩散模型学习负载与调度点的联合分布，并给出条件于负载的边际调度分布，从而解决多值、非凸 OPF 及系统参数变化带来的多样性；并在功率系统基准上验证样本复杂性与实验性能。


<details>
  <summary>Details</summary>
Motivation: OPF 具有多值性和非凸性，且系统参数（如阻抗、拓扑）变化会使给定负载对应多种可能的调度点。现有深度学习 OPF 求解器通常是单值的，若不将所有系统参数作为特征全面表示，难以捕捉该多样性，需求一种能建模条件分布的解决方案。

Method: 将 OPF 问题视为条件采样问题，使用扩散模型从历史运行数据中学习负载与调度点的联合分布，并输出在给定负载条件下的边际调度分布。该方法可生成统计上可信的 warm starts，并在成本和约束满足之间取得更有利的权衡。文中还分析了 DiffOPF 的样本复杂度，确保所得到的解在与优化解的距离上受控，并在功率系统基准上进行实验验证。

Result: 与单值求解器相比，DiffOPF 通过采样实现多解空间的探索，能够提供更灵活的成本与约束权衡。论文探讨了在给定距离容限内逼近优化解所需的样本复杂度，并在多组功率系统基准上验证了方法的有效性。

Conclusion: DiffOPF 为处理多值且受系统参数变化影响的 OPF 提供了一种可行的学习驱动解决方案；通过条件分布采样实现更可信的 warm-start 和更好的约束/成本权衡，且在基准系统上得到实证支持。

Abstract: The optimal power flow (OPF) is a multi-valued, non-convex mapping from loads
to dispatch setpoints. The variability of system parameters (e.g., admittances,
topology) further contributes to the multiplicity of dispatch setpoints for a
given load. Existing deep learning OPF solvers are single-valued and thus fail
to capture the variability of system parameters unless fully represented in the
feature space, which is prohibitive. To solve this problem, we introduce a
diffusion-based OPF solver, termed \textit{DiffOPF}, that treats OPF as a
conditional sampling problem. The solver learns the joint distribution of loads
and dispatch setpoints from operational history, and returns the marginal
dispatch distributions conditioned on loads. Unlike single-valued solvers,
DiffOPF enables sampling statistically credible warm starts with favorable cost
and constraint satisfaction trade-offs. We explore the sample complexity of
DiffOPF to ensure the OPF solution within a prescribed distance from the
optimization-based solution, and verify this experimentally on power system
benchmarks.

</details>


### [107] [Belief Space Control of Safety-Critical Systems Under State-Dependent Measurement Noise](https://arxiv.org/abs/2510.14100)
*Rohan Walia,Mitchell Black,Andrew Schoer,Kevin Leahy*

Main category: eess.SY

TL;DR: BCBF-GEKF extends Belief Control Barrier Functions to handle state-dependent measurement noise using GEKF, reducing conservatism while preserving safety; validated in 1D integrator and 2D unicycle simulations.


<details>
  <summary>Details</summary>
Motivation: Existing CBF-based safety methods assume fixed additive noise, which is often overly conservative for complex sensors with state-dependent errors. There is a need to adapt CBFs to account for state-dependent uncertainty.

Method: Integrate Generalized Extended Kalman Filter (GEKF) into the Belief CBF framework to model measurement noise as a linear function of the state; compare against baseline BCBF in simulation on a 1D integrator and a 2D unicycle trajectory tracking task.

Result: The BCBF-GEKF approach yields less conservative control while maintaining safety, with improved performance in the simulated scenarios compared to the baseline.

Conclusion: Incorporating state-dependent noise modeling via GEKF into the BCBF framework is effective for reducing conservatism and preserving safety in safety-critical control under sensor uncertainty.

Abstract: Safety-critical control is imperative for deploying autonomous systems in the
real world. Control Barrier Functions (CBFs) offer strong safety guarantees
when accurate system and sensor models are available. However, widely used
additive, fixed-noise models are not representative of complex sensor
modalities with state-dependent error characteristics. Although CBFs have been
designed to mitigate uncertainty using fixed worst-case bounds on measurement
noise, this approach can lead to overly-conservative control. To solve this
problem, we extend the Belief Control Barrier Function (BCBF) framework to
accommodate state-dependent measurement noise via the Generalized Extended
Kalman Filter (GEKF) algorithm, which models measurement noise as a linear
function of the state. Using the original BCBF framework as baseline, we
demonstrate the performance of the BCBF-GEKF approach through simulation
results on a 1D single integrator setpoint tracking scenario and 2D unicycle
kinematics trajectory tracking scenario. Our results confirm that the BCBF-GEKF
approach offers less conservative control with greater safety.

</details>


### [108] [Resource-Aware Stealthy Attacks in Vehicle Platoons](https://arxiv.org/abs/2510.14119)
*Ali Eslami,Mohammad Pirani*

Main category: eess.SY

TL;DR: 提出隐蔽攻击设计框架：攻击者在不被检测情况下操控车队轨迹，分析可行性条件、对通信拓扑与控制协议的依赖，以及所需资源，并指出现有防御的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着连接与自动驾驶车辆(CAVs)的普及，车队协同高度依赖通信，致使潜在隐蔽攻击可能带来严重现实后果；现有研究多聚焦防御，亟需系统性研究隐蔽攻击以提升鲁棒性。

Method: 提出攻击设计的分析框架，给出攻击在何种条件下可行；分析攻击对不同通信拓扑和控制协议的敏感性；评估攻击者所需的资源规模与约束。

Result: 揭示车队架构与异常检测机制中的关键薄弱点，量化攻击所需资源，并证实现有系统的易受攻击性与脆弱性。

Conclusion: 强调需要更具韧性的车队设计与防御机制，以提升CAV系统的安全性和可信度，并为设计更安全、可核验的CAV系统提供指引。

Abstract: Connected and Autonomous Vehicles (CAVs) are transforming modern
transportation by enabling cooperative applications such as vehicle platooning,
where multiple vehicles travel in close formation to improve efficiency and
safety. However, the heavy reliance on inter-vehicle communication makes
platoons highly susceptible to attacks, where even subtle manipulations can
escalate into severe physical consequences. While existing research has largely
focused on defending against attacks, far less attention has been given to
stealthy adversaries that aim to covertly manipulate platoon behavior. This
paper introduces a new perspective on the attack design problem by
demonstrating how attackers can guide platoons toward their own desired
trajectories while remaining undetected. We outline conditions under which such
attacks are feasible, analyze their dependence on communication topologies and
control protocols, and investigate the resources required by the attacker. By
characterizing the resources needed to launch stealthy attacks, we address
system vulnerabilities and informing the design of resilient countermeasures.
Our findings reveal critical weaknesses in current platoon architectures and
anomaly detection mechanisms and provide methods to develop more secure and
trustworthy CAV systems.

</details>


### [109] [A Human-Vector Susceptible--Infected--Susceptible Model for Analyzing and Controlling the Spread of Vector-Borne Diseases](https://arxiv.org/abs/2510.14787)
*Lorenzo Zino,Alessandro Casu,Alessandro Rizzo*

Main category: eess.SY

TL;DR: A two-population vector-borne disease model with cross-species infection, analyzed via monotone systems theory to identify global dynamics and optimal control of vector control and protection adoption.


<details>
  <summary>Details</summary>
Motivation: To understand how interactions between humans and vectors drive disease spread and to design effective control policies combining vector management and protective behavior incentives.

Method: Formulate a compartmental ODE model extending SI to two populations with cross-contagion, analyze using monotone systems theory to establish global stability results (disease-free vs endemic equilibria), and incorporate two control actions; formulate an optimal control problem and derive policy implications.

Result: Characterizes global asymptotic behavior; criteria for eradication or unique endemic equilibrium; quantify impact of vector control and protection incentives; derive optimal policy possibly in closed form or via numerical optimization.

Conclusion: Demonstrates that monotone systems provide rigorous insights into threshold dynamics and control effectiveness; identifies conditions under which rapid disease elimination is achievable and how best to allocate resources between vector control and protective measure incentives.

Abstract: We propose an epidemic model for the spread of vector-borne diseases. The
model, which is built extending the classical susceptible-infected-susceptible
model, accounts for two populations -- humans and vectors -- and for
cross-contagion between the two species, whereby humans become infected upon
interaction with carrier vectors, and vectors become carriers after interaction
with infected humans. We formulate the model as a system of ordinary
differential equations and leverage monotone systems theory to rigorously
characterize the epidemic dynamics. Specifically, we characterize the global
asymptotic behavior of the disease, determining conditions for quick
eradication of the disease (i.e., for which all trajectories converge to a
disease-free equilibrium), or convergence to a (unique) endemic equilibrium.
Then, we incorporate two control actions: namely, vector control and incentives
to adopt protection measures. Using the derived mathematical tools, we assess
the impact of these two control actions and determine the optimal control
policy.

</details>


### [110] [Improved Voltage Regulation with Optimal Design of Decentralized Volt-VAr Control](https://arxiv.org/abs/2510.14834)
*Daniel Russell,Dakota Hamilton,Mads R. Almassalkhi,Hamid R. Ossareh*

Main category: eess.SY

TL;DR: 以谱半径为约束的非凸稳定性条件用于去中心化 Volt-VAr 控制（VVC）的斜率设计，显著提升电压调控效果；在真实馈线仿真中优于现有凸约束方法。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源资源（DER）的接入，需实现自适应、动态的电压调控。若VVC设计不当，反馈与电网耦合可能引发不稳定。因此，需在保证系统稳定的同时优化电压调控能力，探索非凸谱半径约束的可行性。

Method: 采用线性化潮流模型建立网- VVC 闭环动力学，结合历史数据提升模型准确性；在此模型下，通过最小化稳态电压偏离标称值来设计 VVC 斜率，并将一个非凸的谱半径稳定性约束纳入优化，与常见的凸约束进行对比；在真实馈线的仿真中评估性能。

Result: 采用谱半径约束的设计在电压调控效果上优于基于凸约束的方案，仿真结果显示在真实馈线场景中实现更有效的电压调控。

Conclusion: 非凸的谱半径稳定性约束在 VVC 斜率设计中具有潜在优势，能在保持稳定性的前提下提升电压调控性能，证明了该约束在该应用中的可行性与潜在收益。

Abstract: Integration of distributed energy resources has created a need for
autonomous, dynamic voltage regulation. Decentralized Volt-VAr Control (VVC) of
grid-connected inverters presents a unique opportunity for voltage management
but, if designed poorly, can lead to unstable behavior when in feedback with
the grid. We model the grid-VVC closed-loop dynamics with a linearized power
flow approach, leveraging historical data, which shows improvement over the
commonly used LinDistFlow model. This model is used to design VVC slopes by
minimizing steady-state voltage deviation from the nominal value, subject to a
non-convex spectral radius stability constraint, which has not been previously
implemented within this context. We compare this constraint to existing convex
restrictions and demonstrate, through simulations on a realistic feeder, that
using the spectral radius results in more effective voltage regulation.

</details>


### [111] [Further Results on Safety-Critical Stabilization of Force-Controlled Nonholonomic Mobile Robots](https://arxiv.org/abs/2510.14931)
*Bo Wang,Tianyu Han,Guangwei Wang*

Main category: eess.SY

TL;DR: 提出基于gamma m-QP的统一CLF-CBF控制框架，用以在力控非holonomic移动机器人中同时保证稳定性与安全性，并通过全局严格Lyapunov函数和级联系统设计实现；并给出仿真和实验验证。


<details>
  <summary>Details</summary>
Motivation: 在安全约束条件下实现力控非holonomic移动机器人的稳定控制是一项挑战，需同时确保系统安全性和稳定性。

Method: 设计一个连续、时间不变的控制律，利用gamma m-QP框架将CLF与CBF统一在一个优化问题中；首次给出全局、时间不变、严格Lyapunov函数用于闭环系统（极坐标下的名义稳定控制器），作为CLF；基于车辆动力学的级联结构，通过积分式回推(backstepping)构造CBF；并在此基础上得到闭环系统的稳定性和安全性结果。

Result: 理论上证明闭环系统的渐近稳定和安全性；通过仿真和实际实验验证控制策略的有效性与性能。

Conclusion: 该框架有效地将稳定性与安全性耦合在一起，适用于力控非holonomic移动机器人，具有理论与实验的支持。

Abstract: In this paper, we address the stabilization problem for force-controlled
nonholonomic mobile robots under safety-critical constraints. We propose a
continuous, time-invariant control law based on the gamma m-quadratic
programming (gamma m-QP) framework, which unifies control Lyapunov functions
(CLFs) and control barrier functions (CBFs) to enforce both stability and
safety in the closed-loop system. For the first time, we construct a global,
time-invariant, strict Lyapunov function for the closed-loop nonholonomic
mobile robot system with a nominal stabilization controller in polar
coordinates; this strict Lyapunov function then serves as the CLF in the QP
design. Next, by exploiting the inherent cascaded structure of the vehicle
dynamics, we develop a CBF for the mobile robot via an integrator backstepping
procedure. Our main results guarantee both asymptotic stability and safety for
the closed-loop system. Both the simulation and experimental results are
presented to illustrate the effectiveness and performance of our approach.

</details>
