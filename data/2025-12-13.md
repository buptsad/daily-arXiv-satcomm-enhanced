<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 线性模型在长时序外生仅使用室内温度过去值的预测中优于 Transformer 系列，DLinear表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估在仅利用历史室内温度数据的单变量、长 horizon 预测任务中，线性与 Transformer 架构的相对强弱及基线地位。

Method: 比较 Linear, NLinear, DLinear, Transformer, Informer, Autoformer 在标准化的 train/val/test 划分上的预测性能。

Result: 线性基线稳定优于 Transformer 家族，DLinear 在所有划分上取得最佳总体准确性。

Conclusion: 在此类外生-only 的时间序列预测中，设计良好的线性模型仍是强有力的基线，提示需谨慎对比新模型。

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [2] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: GTL通过引导采样实现离散扩散模型的迁移学习，无需微调预训练的去噪器；统一处理离散时间和连续时间的扩展，且引入高效的采样策略，使大词汇表和长序列下的实际应用成为可能；在序列数据上进行评估并给出行为分析。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在语言等离散领域表现出色，但往往需要大规模训练数据，迁移到新领域成本高、微调不可行；需要一种不微调、有效的迁移方法来适配新分布。

Method: 提出基于比率的离散扩散迁移学习框架GTL，在不修改预训练去噪器的前提下进行引导采样；统一适用于离散时间扩散与连续时间得分型离散扩散；并设计高效的引导采样器，只对规划位置和候选token进行集中计算以降低开销。

Result: 在合成马尔可夫链和语言建模等序列任务上评估，给出对GTL行为的经验分析，验证其能够在不微调的情况下从目标分布进行采样，并显著降低采样计算成本。

Conclusion: GTL提供了可扩展的离散扩散模型迁移学习解决方案，通过引导采样实现无需微调即可对目标分布采样；统一处理离散/连续时间形式，并通过高效采样策略使大词汇表和长序列的应用成为现实。

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [3] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 在水下环境中使用深度强化学习进行自主导航的论文，基于 PPO 在 BlueROV2 平台上进行训练与评估；提出将目标导航信息、虚拟占据网格和边界射线投射作为观测，且与基准的 DWA 进行比较；通过现实仿真和 3D 数字孪生实现仿真到现实的转移验证；结果显示 PPO 在高拥挤环境中优于 DWA，且具备良好的仿真到现实的迁移性。


<details>
  <summary>Details</summary>
Motivation: 水下无线环境缺乏 GPS、能见度差且有 submerged obstacles，使得传统导航难以可靠实现。通过在常用实验平台 BlueROV2 上应用深度强化学习，探索能在复杂水下场景中实现鲁棒自主导航的方法，并验证其在仿真与真实世界中的可转移性。

Method: 采用近端策略优化(PPO)作为学习算法，观测空间结合目标导向导航信息、虚拟占据栅格以及在作业区域边界上的射线投射（ray-casting）。将学习策略与基线的确定性运动规划器 Dynamic Window Approach (DWA) 相比。评估在现实的仿真环境中进行，并通过对测试场地的 3D 数字孪生进行实际 BlueROV2 的验证来降低真实世界试验风险。

Result: PPO 策略在高拥挤环境中持续优于 DWA，显示出更强的局部适应能力与更少的碰撞。并且实验证明了从仿真到现实世界的行为转移性，验证了深度 RL 在水下自主导航中的相关性。

Conclusion: 在水下自主导航中，深度强化学习（特别是基于 PPO 的方法）具有潜在优势，能够在复杂、GPS 受限的环境中实现更鲁棒的导航，并具备较好的仿真到现实的迁移性。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [4] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了一种解耦 chunk length 的方法，使策略使用较短的行动块，而价值评估使用较长的块，来自优化对部分行动块的蒸馏评估，从而在保留多步价值传播的同时避免开环失效与长块策略学习难题，在离线长时目标条件任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: TD 方法中的自我回溯会产生回溯偏差， chunked critics 能加速值回传，但必须输出整个行动块的开环策略，导致在需要策略反应性的环境中不理想，且随着块长度增大学习困难。通过将 critic 的块长度与策略的块长度解耦，能够让策略专注于较短的行动块，同时仍然享受多步价值传递的好处。

Method: 提出一种新算法：对部分行动块的蒸馏 critic 进行对比性背回(back-up)构造，即从原始 chunked critic 的信息中向前乐观地回推，估计当部分行动块扩展为完整块时所能达到的最大价值。用这个蒸馏 critic 来训练策略，使其输出较短的行动块、同时维持对完整块的价值信息的利用。该设计在保持多步价值传播的同时，规避了长行动块的开环次优与对长块策略学习的困难。

Result: 在具有挑战性的长时离线目标条件任务上进行评估，所提出的方法显示出相对于先前方法的显著改进，表现出更高的稳定性和性能。

Conclusion: 通过将 critic 的 chunk length 与 policy 的 chunk length 解耦，该方法在保留多步价值传播的同时降低了学习较长行动块策略的难度，且有效缓解了开环带来的劣势，适用于长时离线任务。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [5] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 利用旋转嵌入实现思考与聆听并行的实时推理，从而使LLM在无需额外训练的情况下实现实时交互。


<details>
  <summary>Details</summary>
Motivation: 现实世界的语音/嵌入式助手需要LLM能够实时响应并在交互中持续接收新信息。传统的逐步推理使模型必须先“停下来思考”再回答，导致交互变慢且不适合实时场景；人类在阅读时就开始思考，回答时仍在继续推理。

Method: 基于旋转嵌入的特性，使为序列交互设计的LLM在不进行额外训练的前提下，能够同时进行“思考-聆听-输出”的并行推理，从而实现实时互动。

Result: 在数学、常识和安全推理任务上实现了实时的思考增强输出，首次非思考符号的时间从分钟缩短至≤5秒，整体实时延迟提升了6到11倍。

Conclusion: 证明通过嵌入设计和模型架构的改进，可以在不额外训练的情况下实现思考与输出的并行，从而提升真实场景中的交互性与响应速度；但在长时依赖和复杂推理场景中的鲁棒性还需进一步评估与验证。

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [6] [Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks](https://arxiv.org/abs/2512.10936)
*Kristina Korotkova,Aleksandr Katrutsa*

Main category: cs.LG

TL;DR: 利用改进的无投影投射方法（修改的Frank-Wolfe）构造白盒对抗样本，以评估执行效率和效果，与基于投影的对比，使用MNIST/CIFAR-10上的多类逻辑回归、CNN、ViT模型进行理论与数值评估。


<details>
  <summary>Details</summary>
Motivation: 在对抗鲁棒性评估中，需要快速高效地生成对抗样本，投影步骤往往成为瓶颈；通过投影自由的Frank-Wolfe方法可能提高效率并保持攻击效果。

Method: 采用修改后的Frank-Wolfe等投影自由优化算法来构造在给定扰动约束内的对抗扰动；对比基于投影的标准攻击方法；进行理论分析与数值实验。

Result: 理论与数值结果表明，投影自由的修改Frank-Wolfe方法在某些设置下可生成有效的白盒对抗样本，与传统投影方法具有竞争性；在MNIST与CIFAR-10上对多种模型（逻辑回归、CNN、ViT）进行了实验验证。

Conclusion: 投影自由的改进Frank-Wolfe方法为对抗攻击构造提供了一个有力的替代途径，在鲁棒性评估中可实现更高的计算效率与可扩展性，尤其在大模型场景中具有潜在优势。

Abstract: The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).

</details>


### [7] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: Proposes a new point-wise function Derf(x)=erf(αx+s) as a normalization-like alternative that outperforms LayerNorm, RMSNorm and DyT across vision, speech, and DNA tasks; gains stem from better generalization; enables normalization-free Transformers.


<details>
  <summary>Details</summary>
Motivation: Normalization layers are ubiquitous, but recent DyT shows alternatives are possible. By understanding how intrinsic properties of point-wise functions affect training, the authors perform a large-scale search for a more effective design to push beyond DyT.

Method: First analyze how point-wise function properties influence training; then conduct a large-scale search for a more effective function design. The result is Derf(x)=erf(αx+s), using the erf as a rescaled Gaussian CDF, identified as the best-performing design and tested across multiple domains.

Result: Derf outperforms LayerNorm, RMSNorm, and DyT across vision (image recognition and generation), speech representation, and DNA sequence modeling. The gains largely come from improved generalization rather than increased fitting capacity. Derf is simple and effective enough to be used in normalization-free Transformer architectures.

Conclusion: Derf is a practical and superior normalization-like function for normalization-free Transformers. Its strong generalization and simplicity make it a compelling alternative to conventional normalization layers.

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [8] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: 提出 DaSH，通过在数据集与其来源分组的层级上建模效用，从大异质池中选择整套数据集，在受限资源下提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中数据按数据集和机构分组，数据源差异导致样本级选择效率低且泛化性不足；需要能够在有限观测下对不同数据源的效用进行跨层级估计与泛化的选择方法。

Method: 提出 Dataset Selection via Hierarchies (DaSH)，在数据集层级与组划分层级上建模效用，通过层级信息共享来实现高效探索与泛化，适用于从大规模异质数据池中选择整套数据集以提升下游任务性能，且满足资源约束。

Result: 在两个公开基准 Digit-Five 与 DomainNet 上，DaSH 相较于最先进的数据选择基线在准确率上提升最高可达 26.2%，并显著减少探索步骤。

Conclusion: DaSH 对低资源场景具有鲁棒性，适用于可扩展、可自适应的多源学习工作流中的数据集选择，提升下游模型性能的同时降低探索成本。

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>
