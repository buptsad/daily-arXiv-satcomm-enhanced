<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 8]
- [cs.CR](#cs.CR) [Total: 12]
- [eess.SY](#eess.SY) [Total: 18]
- [eess.SP](#eess.SP) [Total: 14]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 43]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Constructing Low-Redundancy Codes via Distributed Graph Coloring](https://arxiv.org/abs/2512.04197)
*Yuting Li,Ryan Gabrys,Farzad Farnoud*

Main category: cs.IT

TL;DR: 用分布式图着色框架构造纠错码，在LOCAL模型下实现高效编码/解码，提出独立可解、列表解码、增量同步和爆发位错的高效码，冗余与GV界限比较有竞争力。


<details>
  <summary>Details</summary>
Motivation: 在分布式计算和局部模型下，寻求一种通用的纠错码构造框架，避免依赖单一基码，提高编码/解码效率与冗余性能。

Method: 将码字与混淆图中的独立集建立对应；通过修改的Linial着色算法在多项式时间内求解单个顶点的颜色，使全局着色一致；由此推导出编码/解码过程，并扩展为：i) 常数数量错误的唯一可解码码，冗余为 GV界限的两倍；ii) 通过超图标记实现的列表解码；iii) 增量同步方案，在编辑距离未知时降低平均通信量；iv) 对任意长度爆发错误的渐近最优码（在8的因子内）。并与Syndrome压缩比较。

Result: 给出具体码的存在性和性能结果：在不同误差类型和参数下实现唯一解码、列表解码、增量同步和爆发错误的高效码，且编码/解码在多项式时间内；冗余在接近 GV 下界的两倍等。

Conclusion: 该框架比传统的基码依赖更灵活、可泛化，且在多参数区间实现冗余提升；为纠错码设计提供新的图论/分布式计算视角。

Abstract: We present a general framework for constructing error-correcting codes using distributed graph coloring under the LOCAL model. Building on the correspondence between independent sets in the confusion graph and valid codes, we show that the color of a single vertex - consistent with a global proper coloring - can be computed in polynomial time using a modified version of Linial's coloring algorithm, leading to efficient encoding and decoding. Our results include: i) uniquely decodable code constructions for a constant number of errors of any type with redundancy twice the Gilbert-Varshamov bound; ii) list-decodable codes via a proposed extension of graph coloring, namely, hypergraph labeling; iii) an incremental synchronization scheme with reduced average-case communication when the edit distance is not precisely known; and iv) the first asymptotically optimal codes (up to a factor of 8) for correcting bursts of unbounded-length edits. Compared to syndrome compression, our approach is more flexible and generalizable, does not rely on a good base code, and achieves improved redundancy across a range of parameters.

</details>


### [2] [Joint Low-Rank and Sparse Bayesian Channel Estimation for Ultra-Massive MIMO Communications](https://arxiv.org/abs/2512.04470)
*Jianghan Ji,Cheng-Xiang Wang,Shuaifei Chen,Chen Huang,Xiping Wu,Emil Björnson*

Main category: cs.IT

TL;DR: 提出了一种联合低秩和稀疏贝叶斯估计的算法（LRSBE），用于超大规模MIMO的空间非平稳通道估计，利用波束域的低秩性与稀疏性，在EM框架中结合稀疏贝叶斯学习和软阈梯度下降，显著提升估计精度并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 在超大规模MIMO场景下，通道估计面临高维、非平稳性和复杂度挑战。通过在波束域利用通道的低秩性与稀疏性，可以提高估计效率和准确性。

Method: 提出LRSBE算法，在EM框架内将稀疏贝叶斯学习与软阈梯度下降相结合，利用低秩与稀疏先验实现对空间非平稳 ultra-massive 通道的高效估计。算法核心在于将通道表示在波束域并同时捕捉其低秩与稀疏性质。

Result: 仿真结果表明，在不同信噪比条件下，所提算法在估计精度和总体复杂度方面显著优于现有先进算法。

Conclusion: 所提出的LRSBE方法有效应对超大规模MIMO的空间非平稳通道估计问题，能够在保持较低复杂度的同时提升估计准确性，相较于对比方法具有显著优势。

Abstract: This letter investigates channel estimation for ultra-massive multiple-input multiple-output (MIMO) communications. We propose a joint low-rank and sparse Bayesian estimation (LRSBE) algorithm for spatial non-stationary ultra-massive channels by exploiting the low-rankness and sparsity in the beam domain. Specifically, the channel estimation integrates sparse Bayesian learning and soft-threshold gradient descent within the expectation-maximization framework. Simulation results show that the proposed algorithm significantly outperforms the state-of-the-art alternatives under different signal-to-noise ratio conditions in terms of estimation accuracy and overall complexity.

</details>


### [3] [Timely Information for Strategic Persuasion](https://arxiv.org/abs/2512.04679)
*Ahmet Bugra Gundogan,Melih Bastopcu*

Main category: cs.IT

TL;DR: 在资源受限的情况下，研究动态贝叶斯劝说中信息披露时序对接收者信念的影响。通过将信息披露时序化，并在单源与多源情境下求解最优采样策略，发现最优策略是在状态0上分配最小采样率以满足IC约束，其余分配给状态1，从而提升接收者估计为1的长期时间比例。


<details>
  <summary>Details</summary>
Motivation: 强调发送者与接收者在时间维度上的目标错位，以及通过控制信息揭示的时机来影响信念的潜在收益；在CTMC驱动的二元信息源上讨论可行的策略与结构。

Method: 将问题建模为发送者的领导者-跟随者博弈（Stackelberg），引入信息披露策略和IC约束，在总采样约束下最大化接收者估计为1的长期平均时间；单源问题中给出最优解：对状态0仅提供足够满足IC的最小采样，对状态1提供剩余采样；扩展到多源时各源有各自的最小采样，推导策略对多源的收益。

Result: 证明了在资源约束下，通过把握信息揭示的时效性，发送者可以显著提高目标估计的实现概率；单源策略具有简洁阐释性，扩展到多源情形仍保留最优分配结构，整体上提升了发送者的效用。

Conclusion: 信息揭示的时序性与采样约束共同决定了动态贝叶斯劝说的效用，上述最优策略揭示了在不完全信息与IC约束下的有效设计原则，并指出多源情形的潜在收益。

Abstract: This work investigates a dynamic variant of Bayesian persuasion, in which a strategic sender seeks to influence a receiver's belief over time through controlling the timing of the information disclosure, under resource constraints. We consider a binary information source (i.e., taking values 0 or 1), where the source's state evolve according to a continuous-time Markov chain (CTMC). In this setting, the receiver aims to estimate the source's state as accurately as possible. In contrast, the sender seeks to persuade the receiver to estimate the state to be 1, regardless of whether this estimate reflects the true state. This misalignment between their objectives naturally leads to a Stackelberg game formulation where the sender, acting as the leader, chooses an information-revelation policy, and the receiver, as the follower, decides whether to follow the sender's messages. As a result, the sender's objective is to maximize the long-term average time that the receiver's estimate equals 1, subject to a total sampling constraint and a constraint for the receiver to follow the sender's messages called incentive compatibility (IC) constraint. We first consider the single-source problem and show that the sender's optimal policy is to allocate a minimal sampling rate to the undesired state 0 (just enough to satisfy the IC constraint) and assign the remaining sampling rate to the desired state 1. Next, we extend the analysis to the multi-source case, where each source has a different minimal sampling rate. Our results show that the sender can leverage the timeliness of the revealed information to influence the receiver, thereby achieving a higher utility.

</details>


### [4] [One-Step Generative Channel Estimation via Average Velocity Field](https://arxiv.org/abs/2512.04501)
*Zehua Jiang,Fenghao Zhu,Siming Jiang,Chongwen Huang,Zhaohui Yang,Richeng Jin,Zhaoyang Zhang,Merouane Debbah*

Main category: cs.IT

TL;DR: 提出一种一步生成式通道估计方法，通过学习平均速度场直接估计通道，跳过迭代去噪，显著降低时延并在仿真中实现最大约2.65 dB的归一化MSE提升，相较扩散方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型可学习信道分布，但扩散模型的迭代去噪带来高时延，对时延敏感的无线场景提出低时延的通道估计方案需求。

Method: 直接学习平均速度场，以一步法完成通道估计，绕过多步迭代去噪，与基线扩散方法进行对比。

Result: 仿真结果显示，与扩散法相比，归一化均方误差提升最多约2.65 dB；同时时延约降低90%。

Conclusion: 方法在低时延无线场景中具有潜力，验证了学习平均速度场的一步法在通道估计中的有效性。

Abstract: Generative models have shown immense potential for wireless communication by learning complex channel data distributions. However, the iterative denoising process associated with these models imposes a significant challenge in latency-sensitive wireless communication scenarios, particularly in channel estimation. To address this challenge, we propose a novel solution for one-step generative channel estimation. Our approach bypasses the time-consuming iterative steps of conventional models by directly learning the average velocity field. Through extensive simulations, we validate the effectiveness of our proposed method over existing state-of-the-art diffusion-based approach. Specifically, our scheme achieves a normalized mean squared error up to 2.65 dB lower than the diffusion method and reduces latency by around 90%, demonstrating the potential of our method to enhance channel estimation performance.

</details>


### [5] [Rotatable Antenna-Enhanced Cell-Free Communication](https://arxiv.org/abs/2512.04742)
*Kecheng Pan,Beixiong Zheng,Yanhua Tan,Emil Björnson,Robert Schober,Rui Zhang*

Main category: cs.IT

TL;DR: 提出了一种基于可旋转天线（RA）的无中心化（cell-free）下行系统，多个RA配备的接入点（AP）协同为多个单天线用户服务在同一时频资源上；通过两阶段的AP-用户关联以及FP和SCA相结合的算法对RA指向方向进行优化，以最大化系统总速率。


<details>
  <summary>Details</summary>
Motivation: RA通过灵活调整3D法线方向来挖掘新的空间自由度，结合无中心化大量AP协作的场景，可以显著提升下行传输的频谱效率；然而需要对AP-用户关联和RA指向进行联合优化以充分利用RA带来的新自由度。

Method: 提出两阶段的AP-用户关联过程以确定合作关系；随后结合分数编程（FP）和顺序凸逼近（SCA）技术，对RA的仰角/方位等指向参数进行优化，以最大化系统总速率。

Result: 数值结果显示，与多种基准方案相比，RA增强的cell-free系统在下行传输中具有显著的性能提升。

Conclusion: 引入RA并通过FP与SCA对RA指向进行优化，能够有效提升cell-free系统的系统吞吐，证明了RA技术在多AP协同下行传输中的潜力与有效性。

Abstract: Rotatable antenna (RA) is a promising technology that can exploit new spatial degrees-of-freedom (DoFs) by flexibly adjusting the three-dimensional (3D) boresight direction of antennas. In this letter, we investigate an RA-enhanced cell-free system for downlink transmission, where multiple RA-equipped access points (APs) cooperatively serve multiple single-antenna users over the same time-frequency resource. Specifically, we aim to maximize the sum rate of all users by jointly optimizing the AP-user associations and the RA boresight directions. Accordingly, we propose a two-stage strategy to solve the AP-user association problem, and then employ fractional programming (FP) and successive convex approximation (SCA) techniques to optimize the RA boresight directions. Numerical results demonstrate that the proposed RA-enhanced cell-free system significantly outperforms various benchmark schemes.

</details>


### [6] [Exact 3-D Channel Impulse Response for Spherical Receivers with Arbitrary Drift Directions](https://arxiv.org/abs/2512.04858)
*Yen-Chi Lee,Ping-Cheng Yeh,Chia-Han Lee*

Main category: cs.IT

TL;DR: 给出3D球形接收球的有漂移的分子通信CIR的首个精确解析解，并提供对非对准入射的改进。


<details>
  <summary>Details</summary>
Motivation: 在分子MIMO系统中，球形吸收接收器的信道用于现实建模，但现有解析解仅适用于1D/无漂移或平面几何。需要在任意漂移方向下获得精确CIR以减少建模误差。

Method: 将吉尔森-格布之变换用于把静态介质中的达到时间分布（hitting-time）转化为有漂移介质中的分布，推导出球形接收器在3D空间、均匀漂移条件下的精确闭式CIR；相对于以往近似忽略漂移方向与传输轴夹角的做法，该方法给出无偏差的解析表达。

Result: 得到一个闭式且可计算的CIR表达式，避免了以往对off-axis接收器的建模误差，便于高效探索系统指标（如峰值时间、峰值幅度）并减少纯仿真成本。

Conclusion: 首次在三维中给出球形接收器有漂移的MIMO分子通信CIR的精确解，为后续系统设计与参数优化提供可靠基础，优于以往的近似方法。

Abstract: Accurate channel modeling for spherical absorbing receivers is fundamental to the design of realistic molecular multiple-input multiple-output (MIMO) systems. While advanced modulation schemes have been proposed to mitigate interference, determining the channel impulse response (CIR) under arbitrary flow directions remains a challenge; existing exact solutions are restricted to either 1-D/no-drift scenarios or planar receiver geometries. Addressing this gap, we derive the first exact analytical CIR for a spherical receiver in a 3-D molecular communication system with uniform drift in an arbitrary direction. Unlike prior approximations that ignore the angle between the drift and the transmission axis, our approach utilizes the Girsanov theorem to analytically transform the hitting-time distribution from a stationary medium to a drifted one. The proposed closed-form expression not only eliminates modeling errors inherent in previous approximations for off-axis receivers but also enables efficient parameter-space exploration of critical system metrics (e.g., peak time and amplitude), a task that would be computationally costly with pure simulation-based approaches.

</details>


### [7] [Environment-Aware Channel Inference via Cross-Modal Flow: From Multimodal Sensing to Wireless Channels](https://arxiv.org/abs/2512.04966)
*Guangming Liang,Mingjie Yang,Dongzhu Liu,Paul Henderson,Lajos Hanzo*

Main category: cs.IT

TL;DR: 基于多模态感知的无导频CSI推断框架，通过跨模态流匹配将感知数据映射到信道分布，实现实时CSI估计并提升beamforming性能。


<details>
  <summary>Details</summary>
Motivation: 在大规模MIMO与高多普勒环境中，导频开销高，迫切需要利用环境感知数据降低或消除导频以提升效率和系统容量。

Method: 提出一个数据驱动的跨模态流匹配框架，将摄像头、LiDAR、GPS等模态特征融入潜在信道分布，并学习一个速度场将该分布变换为信道分布；把问题重构为条件流匹配并引入模态对齐损失，采用低延迟推理；使用Sionna与Blender生成仿真场景与传播；进行系统级评估。

Result: 与导频基线和感知基线相比，在信道估计精度和下游 beamforming 的谱效率方面获得显著提升。

Conclusion: 所提出的方法有效地利用多模态感知实现无导频的CSI推断，具备实时性和对高Doppler场景的适用性，具有潜在的扩展性和对其他模态的适用性。

Abstract: Accurate channel state information (CSI) underpins reliable and efficient wireless communication. However, acquiring CSI via pilot estimation incurs substantial overhead, especially in massive multiple-input multiple-output (MIMO) systems operating in high-Doppler environments. By leveraging the growing availability of environmental sensing data, this treatise investigates pilot-free channel inference that estimates complete CSI directly from multimodal observations, including camera images, LiDAR point clouds, and GPS coordinates. In contrast to prior studies that rely on predefined channel models, we develop a data-driven framework that formulates the sensing-to-channel mapping as a cross-modal flow matching problem. The framework fuses multimodal features into a latent distribution within the channel domain, and learns a velocity field that continuously transforms the latent distribution toward the channel distribution. To make this formulation tractable and efficient, we reformulate the problem as an equivalent conditional flow matching objective and incorporate a modality alignment loss, while adopting low-latency inference mechanisms to enable real-time CSI estimation. In experiments, we build a procedural data generator based on Sionna and Blender to support realistic modeling of sensing scenes and wireless propagation. System-level evaluations demonstrate significant improvements over pilot- and sensing-based benchmarks in both channel estimation accuracy and spectral efficiency for the downstream beamforming task.

</details>


### [8] [Performance Analysis of Fluid Reconfigurable Intelligent Surface over Covert Communications](https://arxiv.org/abs/2512.05085)
*Farshad Rostami Ghadi,Masoud Kaveh,Hanjiang Hong,Kai-Kit Wong,Riku Jantti,F. Javier Lopez-Martinez*

Main category: cs.IT

TL;DR: FRIS 在隐蔽通信中对抗侦测的影响：在低至中等功率下，流体可重构智能表面(FRIS) 相较固定 RIS 能提升可靠性与隐蔽性；在极高功率下，固定 RIS 可能因减少对对手的泄露而略优。


<details>
  <summary>Details</summary>
Motivation: 探究流体可重构智能表面对隐蔽通信的潜在收益，量化误报警与漏检概率，并给出盖度中断概率的闭式表达，以揭示隐蔽性与可靠性之间的权衡。

Method: 建立传输场景，分析 FA/MD 概率，推导覆盖率中断概率 COP 的闭式表达，给出在最优检测阈值下的成功概率，以及对 FRIS 与固定位置 RIS 的对比验证；附数值仿真。

Result: 得到 COP 的闭式表达；在最优阈值下给出成功概率的解析或仿真结果；FRIS 在低-中功率区域优于固定 RIS，提升隐蔽性和可靠性；高功率时固定 RIS 可能因对抗对手泄露较小而有略高成功概率。

Conclusion: FRIS 为隐蔽通信提供有利条件，尤其在中低功率范围；还有对功率和系统设计的折衷，需在 FRIS 与固定 RIS 之间权衡，以实现最佳隐蔽性-可靠性权衡。

Abstract: This paper investigates the impact of the recently proposed concept of fluid reconfigurable intelligent surfaces (FRIS) on covert communications. Specifically, we consider a communication scenario where a legitimate transmitter aims to covertly deliver information to its intended receiver through a planar FRIS, while an adversary attempts to detect whether any transmission is occurring. In this context, we analyze the false alarm (FA) and missed detection (MD) probabilities, and derive a closed-form expression for the covertness outage probability (COP). Furthermore, the success probability is characterized under the optimal detection threshold, providing new insights into the trade-off between covertness and reliable transmission. Numerical results reveal that FRIS provides a clear advantage over fixed-position RIS at low-to-moderate transmit powers by improving reliability and enhancing covertness, while at very high power levels, fixed-position RIS may sustain slightly higher success probability due to reduced leakage toward the adversary.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [9] [Towards Contextual Sensitive Data Detection](https://arxiv.org/abs/2512.04120)
*Liang Telkamp,Madelon Hulsebos*

Main category: cs.CR

TL;DR: 提出一种上下文感知的敏感数据检测框架，结合类型上下文和领域上下文，提升对数据敏感性的识别效果，基于LLM验证并开源数据集与实现。


<details>
  <summary>Details</summary>
Motivation: 开源数据门户日益增多，如何在发布前有效保护敏感数据成为关键挑战。现有抑制方法多聚焦个人数据，但对数据在特定情境下的敏感性定义不足，需要更广义、情境化的检测。

Method: 提出两种上下文敏感数据检测机制：1) 类型上下文化：先识别数据值的语义类型，再结合数据在数据集/文档中的整体上下文进行敏感性判断；2) 领域上下文化：通过检索指明数据敏感性的规则（如数据主题、地理来源等）来判断数据集在更广域的敏感性。实验在大语言模型辅助下进行，比较与商业工具的性能，并对人道主义数据等非常规领域进行验证。

Result: 类型上下文化显著降低误报，召回率达到94%，高于商业工具的63%；领域上下文化在非标准领域（如人道主义数据集）有效；人道主义数据专家的评估显示，基于上下文的LLM解释有助于手动数据审计的一致性。

Conclusion: 提供开源的上下文敏感数据检测机制与标注数据集，地址为GitHub链接，强调扩展对敏感性的定义并在实际数据治理中具有可用性。

Abstract: The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that con- sider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.

</details>


### [10] [Primitive Vector Cipher(PVC): A Hybrid Encryption Scheme based on the Vector Computational Diffie-Hellman (V-CDH) Problem](https://arxiv.org/abs/2512.04237)
*Gülçin ÇİVİ BİLİR*

Main category: cs.CR

TL;DR: 提出 Primitive Vector Cipher (PVC)，一种基于矩阵的混合加密，结合 Diffie-Hellman 密钥交换，建立在 Vector Computational Diffie-Hellman (V-CDH) 的困难性之上。通过两层设计实现明文掩码和块级随机化，具备高并行性和对线性/已知明文攻击的抗性，并在 STS 协议下有望达到 IND-CCA 安全。


<details>
  <summary>Details</summary>
Motivation: 解决高并行性、非确定性与对线性/已知明文攻击的兼顾难题；通过将矩阵密码学与 DH 密钥交换结合，提升安全性和可扩展性。

Method: 两层设计：采用经过 DH 认证的共享原始向量，并利用 HKDF 对明文进行掩码；为每个数据块引入偏移量以实现随机化，避免重复性与提高对攻击的抵抗性。基于矩阵密码学实现，支持块级并行处理。并结合 STS 协议以提升至 IND-CCA 安全性。

Result: 给出对 PVC 的形式化安全分析，证明在 V-CDH 假设下实现 IND-CPA；通过 STS 集成可向 IND-CCA 安全性进一步靠拢；显示出显著的并行性和线性扩展性。

Conclusion: PVC 提供一种在 V-CDH 框架下具有强并行性和抗攻击性的混合加密方案，核心安全性基于 V-CDH，STS 的引入进一步增强到 IND-CCA，具备潜在的实际应用前景。

Abstract: This work introduces the Primitive Vector Cipher (PVC), a novel hybrid encryption scheme integrating matrix-based cryptography with advanced Diffie-Hellman key exchange. PVC's security is grounded on the established hardness of the Vector Computational Diffie- Hellman (V-CDH) problem. The two-layered design uses HKDF to mask plaintext via a DH-authenticated shared primitive vector and randomize cipher blocks with a per-block offset. This approach eliminates deterministic repetitions and provides strong resistance against linear and known-plaintext attacks. PVC's block-wise structure allows for massive parallelism and excellent linear scaling. Security is formally analyzed, demonstrating INDCPA security under V-CDH. STS protocol integration elevates security toward IND-CCA guarantees.

</details>


### [11] [Hey GPT-OSS, Looks Like You Got It - Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics](https://arxiv.org/abs/2512.04254)
*Gaëtan Michelet,Janine Schneider,Aruna Withanage,Frank Breitinger*

Main category: cs.CR

TL;DR: 探索将推理语言模型用于数字取证的潜力，评估其内部推理机制的可解释性对可操作性和法律可用性的影响。结果显示，中等推理水平有助于解释与验证，但高推理水平未提升输出质量，仍需权衡。


<details>
  <summary>Details</summary>
Motivation: 提升数字取证场景中模型输出的可解释性与可用性，探究可本地部署的推理语言模型在取证任务中的潜力与限制。

Method: 以四个测试用例评估推理组件对结果解释性的贡献，结合一种新定量指标与定性分析，比较不同推理水平对结果质量与可解释性的影响。

Result: 推理组件在中等推理水平下有助于解释和验证语言模型输出，但这种帮助通常有限；更高的推理水平并未提升回答质量。

Conclusion: 推理机制对数字取证具有潜在价值，但当前应用需在可解释性和输出质量之间权衡；高推理水平的收益有限，需改进以提升实用性。

Abstract: The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.

</details>


### [12] [WildCode: An Empirical Analysis of Code Generated by ChatGPT](https://arxiv.org/abs/2512.04259)
*Kobra Khanmohammadi,Pooria Roy,Raphael Khoury,Abdelwahab Hamou-Lhadj,Wilfried Patrick Konan*

Main category: cs.CR

TL;DR: 对真实代码生成的ChatGPT输出进行大规模的实证分析，发现安全性常被忽视，且用户对安全特性关注度低；与以往以合成查询的研究结果一致。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估现实情境中LLM生成代码的正确性与安全性，以及理解用户在请求代码时的意图，以解决以往以合成数据为基础的结论可能不具现实性的局限。

Method: 对ChatGPT生成的现实代码进行大规模实证分析；在代码正确性和安全性方面进行评估；分析请求代码用户的意图与安全关注度。

Result: 证实以合成查询为基础的研究的结论，即LLM生成的代码在安全方面常常不充分；同时发现用户在请求代码时对安全特性缺乏关注，缺少相关查询。

Conclusion: 强调在评估与应用LLM生成代码时，需关注安全性并考虑真实世界的用户行为；实验设计应避免仅使用合成数据，以提高现实意义；需要提升对用户和代码安全的教育与工具支持。

Abstract: LLM models are increasingly used to generate code, but the quality and security of this code are often uncertain. Several recent studies have raised alarm bells, indicating that such AI-generated code may be particularly vulnerable to cyberattacks. However, most of these studies rely on code that is generated specifically for the study, which raises questions about the realism of such experiments. In this study, we perform a large-scale empirical analysis of real-life code generated by ChatGPT. We evaluate code generated by ChatGPT both with respect to correctness and security and delve into the intentions of users who request code from the model. Our research confirms previous studies that used synthetic queries and yielded evidence that LLM-generated code is often inadequate with respect to security. We also find that users exhibit little curiosity about the security features of the code they ask LLMs to generate, as evidenced by their lack of queries on this topic.

</details>


### [13] [Breaking Isolation: A New Perspective on Hypervisor Exploitation via Cross-Domain Attacks](https://arxiv.org/abs/2512.04260)
*Gaoning Pan,Yiming Tao,Qinying Wang,Chunming Wu,Mingde Hu,Yizhi Ren,Shouling Ji*

Main category: cs.CR

TL;DR: 提出了跨域攻击(CDA)的系统化分析与自动化框架，利用 guest 内存控制与跨域内存重用实现对 hypervisor 的能力提升利用。


<details>
  <summary>Details</summary>
Motivation: 在虚拟化环境中，传统利用依赖受限结构和确定地址的框架难以生效；由于 guest 内存对宿主可控且暴露，提供了新的攻击基元。

Method: 提出跨域 gadget 识别、指针损坏匹配、触发输入合成和利用链组装的自动化系统；在 QEMU/VirtualBox 上对 15 个漏洞进行评估。

Result: CDA 在现实场景中广泛适用且有效，能够将 guest 内存作为攻击手段实现跨域能力提升。

Conclusion: 首次对跨域攻击进行系统性表征与分类，给出自动化工具，推动对 hypervisor 安全性的新方向和应急对策。

Abstract: Hypervisors are under threat by critical memory safety vulnerabilities, with pointer corruption being one of the most prevalent and severe forms. Existing exploitation frameworks depend on identifying highly-constrained structures in the host machine and accurately determining their runtime addresses, which is ineffective in hypervisor environments where such structures are rare and further obfuscated by Address Space Layout Randomization (ASLR). We instead observe that modern virtualization environments exhibit weak memory isolation -- guest memory is fully attacker-controlled yet accessible from the host, providing a reliable primitive for exploitation. Based on this observation, we present the first systematic characterization and taxonomy of Cross-Domain Attacks (CDA), a class of exploitation techniques that enable capability escalation through guest memory reuse. To automate this process, we develop a system that identifies cross-domain gadgets, matches them with corrupted pointers, synthesizes triggering inputs, and assembles complete exploit chains. Our evaluation on 15 real-world vulnerabilities across QEMU and VirtualBox shows that CDA is widely applicable and effective.

</details>


### [14] [One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises](https://arxiv.org/abs/2512.04338)
*Biagio Montaruli,Luca Compagna,Serena Elisa Ponta,Davide Balzarotti*

Main category: cs.CR

TL;DR: 提出一种对抗性训练驱动的鲁棒检测框架，用于在 PyPI 与企业生态中检测恶意 Python 包，具备对抗性变换鲁棒性并可按不同受众的 FPR 要求进行自适应配置。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击日益猖獗，恶意 Python 包通过公共仓库和企业环境传播；现有检测在面对代码混淆等对抗性变换时鲁棒性不足，且难以在不同 FPR 需求下灵活调整。

Method: 提出一种可无缝集成的鲁棒检测器，结合针对性的小粒度代码混淆生成对抗样本的方法以及对抗性训练（AT），以提升对混淆文本的鲁棒性和检测能力。对 122,398 个每日从 PyPI 收集的包在 80 天的级别数据进行评估，AT 将鲁棒性提升约 2.5 倍，且在发现更多被混淆包的同时，对非混淨包性能略有下降。通过两项用例研究展示系统的生产适配性： (i) PyPI 维护者场景，FPR 调整为 0.1%，每日检测到 2.48 个恶意包，误报 2.18 个； (ii) 面向企业团队场景，FPR 调整为 10%，每日误报 1.24 个。整体实现对公私仓库的无缝接入，且误报审核时间仅需几分钟。

Result: 引入的对抗训练显著提升系统鲁棒性和探测覆盖，能在不同 FPR 需求下实现可控的检测性能，且在实际数据集上达成可观的每日检测率和较低的误报。共发现并报告 346 个恶意包。

Conclusion: 该检测框架具备良好的可部署性与适配性，适用于公开仓库与企业生态，呈现出对抗性样本下的稳健性和可配置的误报阈值，推动供应链安全检测的落地应用。

Abstract: The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).
  We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages.
  We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives.
  Overall, we uncovered 346 malicious packages, now reported to the community.

</details>


### [15] [AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning](https://arxiv.org/abs/2512.04368)
*Praveen Anugula,Avdhesh Kumar Bhardwaj,Navin Chhibber,Rohit Tewari,Sunil Khemka,Piyush Ranjan*

Main category: cs.CR

TL;DR: 提出一个基于强化学习的自愈安全框架AutoGuard用于DevSecOps，提升威胁检测、缩短MTTR并增强韧性。


<details>
  <summary>Details</summary>
Motivation: 现有的规则基检测与静态扫描对系统演化的适应性差，响应时间长，面对新兴攻击向量的能力不足。

Method: 构建RL驱动的自愈安全环境，持续监控流水线活动，基于动态学习的策略进行预防、检测和快速响应；在模拟CI/CD环境中进行评估。

Result: 在模拟CI/CD环境中，AutoGuard将威胁检测准确性提升约22%，MTTR降低约38%，相较传统方法，整体韧性提升。

Conclusion: AutoGuard实现DevSecOps中的自适应安全防护，提升实时响应能力和鲁棒性，适于需要快速演化安全策略的持续集成/部署场景。

Abstract: Contemporary DevSecOps pipelines have to deal with the evolution of security in an ever-continuously integrated and deployed environment. Existing methods,such as rule-based intrusion detection and static vulnerability scanning, are inadequate and unreceptive to changes in the system, causing longer response times and organization needs exposure to emerging attack vectors. In light of the previous constraints, we introduce AutoGuard to the DevSecOps ecosystem, a reinforcement learning (RL)-powered self-healing security framework built to pre-emptively protect DevSecOps environments. AutoGuard is a self-securing security environment that continuously observes pipeline activities for potential anomalies while preemptively remediating the environment. The model observes and reacts based on a policy that is continually learned dynamically over time. The RL agent improves each action over time through reward-based learning aimed at improving the agent's ability to prevent, detect and respond to a security incident in real-time. Testing using simulated ContinuousIntegration / Continuous Deployment (CI/CD) environments showed AutoGuard to successfully improve threat detection accuracy by 22%, reduce mean time torecovery (MTTR) for incidents by 38% and increase overall resilience to incidents as compared to traditional methods.
  Keywords- DevSecOps, Reinforcement Learning, Self- Healing Security, Continuous Integration, Automated Threat Mitigation

</details>


### [16] [ReFuzz: Reusing Tests for Processor Fuzzing with Contextual Bandits](https://arxiv.org/abs/2512.04436)
*Chen Chen,Zaiyan Xu,Mohamadreza Rostami,David Liu,Dileep Kalathil,Ahmad-Reza Sadeghi,Jeyavijayan,Rajendran*

Main category: cs.CR

TL;DR: ReFuzz: 基于上下文赌博的自适应硬件模糊测试框架，能够复用先前处理器中的高效测试来对目标处理器进行模糊测试，发现新变种漏洞和功能性缺陷，并显著提升覆盖率效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有处理器模糊测试仅对单一设计进行测试、无法迁移已知漏洞知识以发现相似/新漏洞的问题，提高漏洞发现效率和覆盖率。

Method: 使用上下文赌博框架来选择并变异来自先前处理器的测试用例，以对目标指令集架构（PUT within an ISA）进行模糊测试；通过重新利用对先前处理器有用的测试来引导测试，发现相似及新变种的漏洞；评估在多个处理器上的漏洞和功能缺陷，以及覆盖率提升。

Result: 在实验中发现三项新安全漏洞和两项新功能性缺陷；其中一项漏洞通过复用在先前处理器中的测试触发；一项功能缺陷在三个共享设计模块的处理器上存在；第二个缺陷具有两种变体；并且相对于现有模糊测试工具，ReFuzz在覆盖速度上平均提升约511.23倍，总覆盖率提升至多9.33%。

Conclusion: 通过跨处理器知识迁移的自适应模糊测试，可提高漏洞发现效率与覆盖率，证明在硬件设计领域将先前测试经验迁移应用的可行性。

Abstract: Processor designs rely on iterative modifications and reuse well-established designs. However, this reuse of prior designs also leads to similar vulnerabilities across multiple processors. As processors grow increasingly complex with iterative modifications, efficiently detecting vulnerabilities from modern processors is critical. Inspired by software fuzzing, hardware fuzzing has recently demonstrated its effectiveness in detecting processor vulnerabilities. Yet, to our best knowledge, existing processor fuzzers fuzz each design individually, lacking the capability to understand known vulnerabilities in prior processors to fine-tune fuzzing to identify similar or new variants of vulnerabilities.
  To address this gap, we present ReFuzz, an adaptive fuzzing framework that leverages contextual bandit to reuse highly effective tests from prior processors to fuzz a processor-under-test (PUT) within a given ISA. By intelligently mutating tests that trigger vulnerabilities in prior processors, ReFuzz effectively detects similar and new variants of vulnerabilities in PUTs. ReFuzz uncovered three new security vulnerabilities and two new functional bugs. ReFuzz detected one vulnerability by reusing a test that triggers a known vulnerability in a prior processor. One functional bug exists across three processors that share design modules. The second bug has two variants. Additionally, ReFuzz reuses highly effective tests to enhance efficiency in coverage, achieving an average 511.23x coverage speedup and up to 9.33% more total coverage, compared to existing fuzzers.

</details>


### [17] [A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution](https://arxiv.org/abs/2512.04580)
*Huifeng Zhu,Shijie Li,Qinfeng Li,Yier Jin*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.
  In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.

</details>


### [18] [Cryptanalysis of Gleeok-128](https://arxiv.org/abs/2512.04675)
*Siwei Chen,Peipei Xie,Shengyuan Xu,Xiutao Feng,Zejun Xiang,Xiangyong Zeng*

Main category: cs.CR

TL;DR: 首次对 Gleeok-128 进行第三方全面密码分析，提出两阶段 MILP 框架用于分支级和整体 DL 区分、以及针对多分支设计的积分型密钥恢复框架；给出新的区分器与密钥恢复界限，揭示线性安全评估中的缺陷，并提出改进的线性层参数，提供多分支对称设计的通用分析方法。


<details>
  <summary>Details</summary>
Motivation: 在多分支 SPN 结构的密钥伪随机函数 (PRF) 中，评估安全裕度与进行有效的密钥恢复具有非平凡挑战。本工作旨在超越设计文档中的结论，给出对 Gleeok-128 的独立分析并建立可推广的方法论。

Method: 提出两阶段 MILP 框架，用于构建分支级与全体密码的 DL（Differential-Linear）区分器；结合针对多分支设计的积分型密钥恢复框架；通过收紧代数次数界，获得分支的整数性区分器与对全 PRF 的区分器，并给出用于线性层优化的参数。

Result: 得到 Branch 1/2/3 与 Gleeok-128 的 DL 区分器覆盖 7/7/8/4 筹轮，平方相关性约为 2^-88.12、2^-88.12、2^-38.73、2^-49.04；通过整数量化的边界得到三处分支和全 PRF 的7轮及分支 7 轮的整数性区分器，VK 在非全码本与全码本设定下实现 7 轮和 8 轮的密钥恢复攻击；发现 Branch 3 的线性安全评估存在缺陷，在任意 12 轮下的数据复杂度约为 2^48 即可区分；并提出优化的线性层参数以显著提升线性抗性而不牺牲扩散性。

Conclusion: 这项工作深化了对 Gleeok-128 的理解，并提供了可应用于多分支对称设计的一般分析方法，推动了多分支 SPN 设计的安全评估与对比分析的发展。

Abstract: Gleeok is a family of low latency keyed pseudorandom functions (PRFs) consisting of three parallel SPN based permutations whose outputs are XORed to form the final value. Both Gleeok-128 and Gleeok-256 use a 256 bit key, with block sizes of 128 and 256 bits, respectively. Owing to its multi branch structure, evaluating security margins and mounting effective key recovery attacks present nontrivial challenges. This paper provides the first comprehensive third party cryptanalysis of Gleeok-128. We introduce a two stage MILP based framework for constructing branch wise and full cipher differential linear (DL) distinguishers, together with an integral based key recovery framework tailored to multi branch designs. Our DL analysis yields 7, 7, 8, and 4 round distinguishers for Branch 1, Branch 2, Branch 3, and Gleeok-128, respectively, with squared correlations approximately 2 to the power minus 88.12, 2 to the power minus 88.12, 2 to the power minus 38.73, and 2 to the power minus 49.04, outperforming those in the design document except for the full PRF case. By tightening algebraic degree bounds, we further derive 9, 9, and 7 round integral distinguishers for the three branches and a 7 round distinguisher for the full PRF, extending the designers results by 3, 3, and 2 rounds and by 2 rounds, respectively. These integral properties enable 7 round and 8 round key recovery attacks in the non full codebook and full codebook settings. In addition, we identify a flaw in the original linear security evaluation of Branch 3, showing that it can be distinguished over all 12 rounds with data complexity about 2 to the power 48. We also propose optimized linear layer parameters that significantly improve linear resistance without sacrificing diffusion. Our results advance the understanding of Gleeok-128 and provide general methods for analyzing multi branch symmetric designs.

</details>


### [19] [SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://arxiv.org/abs/2512.04841)
*Wei Zhao,Zhe Li,Jun Sun*

Main category: cs.CR

TL;DR: 提出一个统一的因果分析框架用于大语言模型，覆盖从token/神经元/层到表征级的干预与分析；并对多模型与安全基准进行实证评估，发现安全机制局部化且因果特征检测准确率超过95%；代码开源。


<details>
  <summary>Details</summary>
Motivation: 理解对抗性操控（如越狱攻击）背后的因果因素，建立可重复、可比较的因果攻击与防御研究框架，并提升LLMs的安全性与可解释性。

Method: 提出一个覆盖token-level、neuron-level、layer-level干预以及表征层面的统一因果分析框架；补充给出关于因果驱动越狱研究的综合综述；在多种开源权重模型和安全基准上进行实证评估（越狱、幻觉检测、后门识别、公平性评估等）。

Result: (1) 针对因果关键组成部分的定向干预能够可靠地改变安全行为；(2) 安全相关机制高度局部化，集中在早中层，只有1-2%的神经元具有因果影响；(3) 从框架中提取的因果特征在多种威胁类型上达到>95%的检测准确率。

Conclusion: 将理论因果分析与实际模型安全相结合，提供一个可重复的因果攻击、解释性研究与鲁棒检测/缓解的基础，代码开源以便复现与扩展。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.
  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\% detection accuracy across multiple threat types.
  By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.

</details>


### [20] [Opacity problems in multi-energy timed automata](https://arxiv.org/abs/2512.04950)
*Étienne André,Lydia Bakiri*

Main category: cs.CR

TL;DR: Guarded multi-energy timed automata extend timed automata with multiple energy variables and guards; opacity verification is undecidable in general but becomes decidable under several attacker observation constraints, providing practical verification for CPS with timing and energy leaks.


<details>
  <summary>Details</summary>
Motivation: Cyber-physical systems can leak information through timing and energy signals. There is a need to formally verify opacity in models that incorporate both time and energy dimensions, which standard timed automata cannot capture.

Method: Introduce guarded multi-energy timed automata (GMETA) with multiple energy variables and guards on these variables. Formalize the opacity problem under various attacker observation models. Prove undecidability for the general formalism and establish decidable results for specific subclasses, particularly when the attacker observes only final energy and/or execution time, and when they observe energy values at every time unit.

Result: General GMETA opacity is undecidable, but there are positive decidability results for practical attacker models: (1) attacker observes final energy and/or total execution time; (2) attacker observes energy variables at each time unit. These subclasses admit decision procedures for opacity verification.

Conclusion: The framework delineates the boundary between decidable and undecidable opacity problems in energy-aware CPS. While the fully general model is intractable, the identified subclasses offer workable verification methods for common security scenarios involving timing and energy leaks.

Abstract: Cyber-physical systems can be subject to information leakage; in the presence of continuous variables such as time and energy, these leaks can be subtle to detect. We study here the verification of opacity problems over systems with observation over both timing and energy information. We introduce guarded multi-energy timed automata as an extension of timed automata with multiple energy variables and guards over such variables. Despite undecidability of this general formalism, we establish positive results over a number of subclasses, notably when the attacker observes the final energy and/or the execution time, but also when they have access to the value of the energy variables every time unit.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [21] [Quantum-Embedded Dynamic Security Control using Hybrid Deep Reinforcement Learning](https://arxiv.org/abs/2512.04095)
*Amin Masoumi,Mert Korkali*

Main category: eess.SY

TL;DR: 提出一种将量子嵌入式、模型无关深度强化学习用于动态安全控制（DSC）的框架，并在IEEE 39母线系统上评估，显示潜在性能提升但存在待解决的不足。


<details>
  <summary>Details</summary>
Motivation: DSC在高比例并网、尤其是基于反向变流资源的电网中，因需要实时求解复杂的微分代数方程而面临计算成本与精度的挑战，需更高效的控制策略。

Method: 将模型无关的深度强化学习与近似量子计算（NISQ）理念结合，提出量子嵌入的算法框架，并在IEEE 39-bus测试系统上评估其在DSC中的性能与可靠性。

Result: 实验结果显示该量子嵌入的模型无关算法在DSC场景下具有潜在应用前景，表现出一定的改进空间，但也暴露出若干不足之处，需要进一步改进与发展。

Conclusion: 量子增强的模型无关DSC框架对未来电网安全控制具有前景，但仍需系统性研究以解决实现复杂性、鲁棒性及在实际大规模系统上的推广问题。

Abstract: Dynamic security control (DSC) is considered a pivotal step for the future power grid, which is increasingly penetrated by inverter-based resources. However, the efficiency of such practices, whether governed by automatic generation control or virtual inertia scheduling, can be intractable due to the complexity of the problem and the need to solve the differentialalgebraic equation in a timely manner with the required accuracy. In this regard, the model-free deep reinforcement learning algorithm demonstrates reliable performance. In addition, the introduction of fault-tolerant and near-term quantum computing terminologies, i.e., noisy intermediate-scale quantum, opens avenues for improving the performance of model-free algorithms leveraging quantum capabilities. This paper provides an organized framework and assesses its dependability by evaluating the performance of a quantum-embedded algorithm on the DSC of the IEEE 39-bus test system. Hence, the obtained results demonstrate promising applications, along with shortcomings that can be addressed and further developed later.

</details>


### [22] [PINN vs LSTM: A Comparative Study for Steam Temperature Control in Heat Recovery Steam Generators](https://arxiv.org/abs/2512.04183)
*Mojtaba Fanoodi,Farzaneh Abdollahi,Mahdi Aliyari Shoorehdeli*

Main category: eess.SY

TL;DR: PINN-based控制在HRSG蒸汽温度自适应控制中优于LSTM，且对 valve leakage 故障具有更好鲁棒性，在故障情形下比LSTM的积分绝对误差少54%。


<details>
  <summary>Details</summary>
Motivation: 在HRSG中维持精确的蒸汽温度对效率和安全至关重要，但传统控制在非线性和故障引发的动态下表现有限。将物理规律直接融入数据驱动控制，可提升鲁棒性与可复用性，尤其在未知故障场景下。

Method: 将PINN与LSTM用于实时调参的PI-前馈控制律的增益自适应调节；LSTM为离线历史数据训练的纯数据驱动控制器，PINN将热力学定律嵌入在线学习的损失函数中实现物理约束。通过以包含正常负载变化和阀门泄漏故障的组合循环发电厂数据进行模型验证与评估，比较两者在不同工况下的表现。

Result: LSTM显著优于传统方法，但在未见故障情形下性能下降；PINN在鲁棒性和性能上始终优于LSTM，且在故障条件下实现了54%的积分绝对误差（IAE）下降相对于LSTM的改进。

Conclusion: 将物理知识嵌入数据驱动控制中对开发在复杂工业场景中更可靠、容错性更高的自治控制系统具有关键作用。

Abstract: This paper introduces a direct comparative study of Physics-Informed Neural Networks (PINNs) and Long Short-Term Memory (LSTM) networks for adaptive steam temperature control in Heat Recovery Steam Generators (HRSGs), particularly under valve leakage faults. Maintaining precise steam temperature in HRSGs is critical for efficiency and safety, yet traditional control strategies struggle with nonlinear, fault-induced dynamics. Both architectures are designed to adaptively tune the gains of a PI-plus-feedforward control law in real-time. The LSTM controller, a purely data-driven approach, was trained offline on historical operational data, while the PINN controller integrates fundamental thermodynamic laws directly into its online learning process through a physics-based loss function. Their performance was evaluated using a model validated with data from a combined cycle power plant, under normal load changes and a challenging valve leakage fault scenario. Results demonstrate that while the LSTM controller offers significant improvement over conventional methods, its performance degrades under the unseen fault. The PINN controller consistently delivered superior robustness and performance, achieving a 54\% reduction in integral absolute error compared to the LSTM under fault conditions. This study concludes that embedding physical knowledge into data-driven control is essential for developing reliable, fault-tolerant autonomous control systems in complex industrial applications.

</details>


### [23] [Configuration-Constrained Tube MPC for Periodic Operation](https://arxiv.org/abs/2512.04239)
*Filippo Badalamenti,Jose A. Borja-Conde,Sampath Kumar Mulagaleti,Boris Houska,Alberto Bemporad,Mario Eduardo Villanueva*

Main category: eess.SY

TL;DR: 提出一种基于鲁棒模型预测控制的周期性轨迹优化框架，适用于多面体线性差分包含模型，通过对配置约束的多面体管进行优化实现鲁棒性与递归可行性，并给出周期成本的二次上界以构成QP高效求解。


<details>
  <summary>Details</summary>
Motivation: 在经济与环境条件变化下，工业过程的周期性操作往往更具经济性。本研究旨在在不确定性环境中实现鲁棒且高效的周期性最优控制。

Method: 将鲁棒控制问题转化为凸优化问题，优化配置受约束的多面体管与周期轨迹；通过添加人工变量确保在线更新经济准则时的递归可行性与鲁棒约束满足，并在准则不变时保证收敛到相应的最优周期管；在 Lipschitz 连续性假设下，对周期成本进行二次上界，得到一个保持理论性质的二次规划（QP）形式。

Result: 在基准示例和一个具有八个状态的球-板系统上验证了方法的有效性与可扩展性。

Conclusion: 该鲁棒MPC框架在鲁棒性、可行性与计算效率之间提供理论保证并具备实证有效性，适用于以经济准则驱动的周期性操作场景。

Abstract: Periodic operation often emerges as the economically optimal mode in industrial processes, particularly under varying economic or environmental conditions. This paper proposes a robust model predictive control (MPC) framework for uncertain systems modeled as polytopic linear differential inclusions (LDIs), where the dynamics evolve as convex combinations of finitely many affine control systems with additive disturbances. The robust control problem is reformulated as a convex optimization program by optimizing over configuration-constrained polytopic tubes and tracks a periodic trajectory that is optimal for a given economic criterion. Artificial variables embedded in the formulation ensure recursive feasibility and robust constraint satisfaction when the economic criterion is updated online, while guaranteeing convergence to the corresponding optimal periodic tube when the criterion remains constant. To improve computational efficiency, we introduce a quadratic over-approximation of the periodic cost under a Lipschitz continuity assumption, yielding a Quadratic Program (QP) formulation that preserves the above theoretical guarantees. The effectiveness and scalability of the approach are demonstrated on a benchmark example and a ball-plate system with eight states.

</details>


### [24] [Stability of Lyapunov redesign trajectory tracking control with unbounded perturbations -- A tube-based stability analysis](https://arxiv.org/abs/2512.04247)
*Niclas Titze,Kai Wulff,Johann Reger*

Main category: eess.SY

TL;DR: 在含有无界扰动的 Byrnes-Isidori 形式非线性系统中，利用带反馈线性化的李雅普诺夫重新设计实现轨迹跟踪，并通过管道化几何不变性对闭环轨迹进行收缩约束，从而把经典的稳定判据从常数参考推广到非恒定参考。


<details>
  <summary>Details</summary>
Motivation: 在存在无界扰动时，传统的轨迹跟踪稳定性分析通常以常数参考或有界扰动为前提，难以直接推广到随时间变化的参考轨迹。需要一个能够结合李雅普诺夫重新设计、反馈线性化及管道不变性的方法来实现对随时间变化参考轨迹的鲁棒跟踪。

Method: 在 Byrnes-Isidori 形的一类非线性系统上，采用带反馈线性化的李雅普诺夫重新设计用于轨迹跟踪；引入管道化几何特征的闭环不变性来描述系统在参考轨迹附近的收缩性质；把参考轨迹显式引入控制设计，并利用跟踪误差的李雅普诺夫函数满足微分不等式，证明闭环解在相对于参考轨迹的收缩管内运动。

Result: 证明了在存在无界扰动的情况下，闭环解沿参考轨迹保持在一个收缩管中，从而实现对非恒定参考的稳定跟踪；把经典的稳定性判据从恒定参考推广到非恒定参考。

Conclusion: 该工作将李雅普诺夫重新设计、反馈线性化与管道化不变性结合，提供了一种鲁棒轨迹跟踪框架，适用于 Byrnes-Isidori 形式的非线性系统并能处理无界扰动，同时扩展了对时间变化参考的稳定性分析边界。

Abstract: Considering a nonlinear system in Byrnes-Isidori form that is subject to unbounded perturbations, we apply Lyapunov redesign via feedback linearisation for trajectory tracking. Leveraging the ideas of tube-based geometric characterisation of the invariance properties of the closed loop, we generalise the classical stability criterion from the~literature from constant to nonconstant reference trajectories. The proposed analysis is tailored to the Lyapunov redesign and the tracking problem insofar as we incorporate the reference trajectory and the transient decrease of the tracking error enforced by the controller. In particular, we exploit that the Lyapunov function of the tracking error satisfies a differential inequality, thereby guaranteeing that the solution of the closed loop remains in a contracting tube along the reference trajectory.

</details>


### [25] [Optimizing DER Aggregate Flexibility via Network Reconfiguration](https://arxiv.org/abs/2512.04472)
*Feixiang Zhang,Hongyi Li,Bai Cui,Zhaoyu Wang*

Main category: eess.SY

TL;DR: The paper develops a two-stage adaptive robust optimization framework to optimize the aggregated DER (distributed energy resources) flexibility region via distribution network reconfiguration, employing an exact convex reformulation with many second-order cone constraints and a scalable Benders decomposition algorithm, demonstrated on the IEEE 123-bus system with notable flexibility gains.


<details>
  <summary>Details</summary>
Motivation: Quantify and enlarge the aggregated flexibility region of DERs to enable wholesale market participation and grid services, recognizing that network topology constrains DER flexibility; need scalable, robust methods to optimize this region under uncertainty.

Method: Model the ellipsoidal aggregate flexibility region; formulate as a two-stage adaptive robust optimization problem; derive an exact convex reformulation with a large number of SOC constraints; develop a scalable Benders decomposition algorithm with finite convergence; formulate an optimal distribution network reconfiguration problem to optimize the aggregate flexibility region and solve it using the custom Benders decomposition.

Result: Numerical experiments on the IEEE 123-bus test feeder show substantial improvements in the aggregate flexibility region across multiple scenarios when using the optimized topology, compared to existing approaches; the proposed method is scalable.

Conclusion: Distribution network reconfiguration, combined with robust optimization and Benders-based solution, can significantly expand the aggregate flexibility region of DERs, enabling greater market participation and grid services; the approach provides a tractable framework for topology optimization to maximize flexibility.

Abstract: The aggregate flexibility region of distributed energy resources (DERs) quantifies the aggregate power shaping capabilities of DERs. It characterizes the distribution network's potential for wholesale market participation and grid service provision at the transmission level. To enhance flexibility and fully exploit the potential of DERs, this paper proposes a method to optimize the aggregate flexibility region through distribution network reconfiguration. First, we formulate the ellipsoidal aggregate flexibility region characterization problem as a two-stage adaptive robust optimization problem and derive an exact convex reformulation with a large number of second-order cone constraints. By exploiting the problem structure, we propose a scalable Benders decomposition algorithm with provable finite convergence to the optimal solution. Finally, we propose an optimal reconfiguration problem for aggregate flexibility region optimization and solve it using the custom Benders decomposition. Numerical simulations on the IEEE 123-bus test feeder demonstrate that, compared to existing approaches, substantial improvements in the aggregate flexibility region can be achieved over multiple scenarios with the optimized topology.

</details>


### [26] [Adaptive Time-Domain Harmonic Control for Noise-Vibration-Harshness Reduction of Electric Drives](https://arxiv.org/abs/2512.04512)
*Klaus Herburger,Fabian Jakob,David Gänzle,Maximilian Manderla,Andrea Iannelli*

Main category: eess.SY

TL;DR: 提出了三种时间域自适应谐波控制架构以降NVH，结合降低计算量的参数估计和delta-learning实现快速收敛，经过仿真与试验验证，适合嵌入式实时实现。


<details>
  <summary>Details</summary>
Motivation: 在电驱动中，NVH直接影响用户体验和部件寿命；对电动车传动、热泵压缩机等应用，严格的NVH要求需要实时、鲁棒的控制解决方案；现有方法往往计算复杂或对快速工况变化响应不足。

Method: 提出三种控制结构的时间域自适应谐波控制框架；给出改进的参数估计方案以降低计算量并保持精度；引入delta-learning，将自适应控制与基于查找表的前馈估计结合，实现快速收敛与鲁棒性；在永磁同步电机驱动上进行仿真与试验验证。

Result: 在仿真与试验台上实现显著的NVH降低，在各种工作条件下均表现出良好的鲁棒性和实时可实现性。

Conclusion: 时间域自适应谐波控制为电驱动中实时NVH抑制提供了一个具有理论基础的、实用的解决方案。

Abstract: Reducing Noise, Vibration, and Harshness (NVH) in electric drives is crucial for applications such as electric vehicle drivetrains and heat-pump compressors, where strict NVH requirements directly affect user satisfaction and component longevity. This work presents the integration of an adaptive time-domain harmonic controller into an existing electric-drive control loop to attenuate harmonic disturbances. Three control structures are proposed and analyzed, along with a modified parameter-estimation scheme that reduces computational effort while preserving estimation accuracy, making the method suitable for embedded real-time implementation. To cope with fast operating-point changes, a delta-learning approach combines adaptive control with a lookup-table-based feedforward estimator, ensuring fast convergence and robustness. The proposed controller architectures are validated through simulation and testbench experiments on a permanent-magnet synchronous machine drive, demonstrating substantial NVH reductions across operating conditions. The results confirm that time-domain adaptive harmonic control offers a practical and theoretically grounded solution for real-time NVH mitigation in electric drives.

</details>


### [27] [Adapt and Stabilize, Then Learn and Optimize: A New Approach to Adaptive LQR](https://arxiv.org/abs/2512.04565)
*Peter A. Fisher,Anuradha M. Annaswamy*

Main category: eess.SY

TL;DR: 提出了一种结合直接MRAC和分段(epoch-based)策略的新型离散时间LQR自适应控制算法，旨在克服现有自适应LQR在初始稳定控制器、探索需求和计算复杂性方面的不足，给出高概率的后悔界，并在特定系统类上表现出更低的后悔。


<details>
  <summary>Details</summary>
Motivation: 解决离散时间自适应LQR在实际实现中的三大挑战：需要初始稳定控制器、对探索的依赖以保证闭环稳定，以及计算复杂度高。希望在不牺牲理论保证的前提下提升可实用性和性能。

Method: 将直接MRAC与分段(epoch-based)策略结合，提出一种新算法以实现对特定离散时间系统的自适应控制，推导出在高概率意义下的后悔界，并通过对比实验展示在不同条件下的性能表现。

Result: 在满足条件(i)和(ii)时，新算法的后悔与现有方法相当；当不满足这两个条件中的任意一个时，算法的后悔显著降低，表明对实际限制的鲁棒性提升。

Conclusion: 对于特定类的离散时间系统，直接MRAC与分段策略的结合可在给出高概率后悔界的同时，减少对初始稳定性和探索的依赖，提升实际可用性。

Abstract: This paper focuses on adaptive control of the discrete-time linear quadratic regulator (adaptive LQR). Recent literature has made significant contributions in proving non-asymptotic convergence rates, but existing approaches have a few drawbacks that pose barriers for practical implementation. These drawbacks include (i) a requirement of an initial stabilizing controller, (ii) a reliance on exploration for closed-loop stability, and/or (iii) computationally intensive algorithms. This paper proposes a new algorithm that overcomes these drawbacks for a particular class of discrete-time systems. This algorithm leverages direct Model-Reference Adaptive Control (direct MRAC) and combines it with an epoch-based approach in order to address the drawbacks (i)-(iii) with a provable high-probability regret bound comparable to existing literature. Simulations demonstrate that the proposed approach yields regrets that are comparable to those from existing methods when the conditions (i) and (ii) are met, and yields regrets that are significantly smaller when either of these two conditions is not met.

</details>


### [28] [Gauss-Newton accelerated MPPI Control](https://arxiv.org/abs/2512.04579)
*Hannes Homburger,Katrin Baumgärtner,Moritz Diehl,Johannes Reuter*

Main category: eess.SY

TL;DR: 将高斯-牛顿加速融入MPPI，通过雅可比重构提升高维场景下的可扩展性与计算效率。


<details>
  <summary>Details</summary>
Motivation: MPPI在随机采样的蒙特卡洛优化中，面对高维问题性能下降，需要更高效的优化框架来保留灵活性与鲁棒性。

Method: 在原始MPPI框架中引入雅可比重构技术和二阶广义高斯-牛顿优化策略，形成 Gauss-Newton accelerated MPPI，并在GPU上实现加速以实现更高效的随机搜索。

Result: 数值结果显示，该方法显著提升MPPI在高维问题上的可扩展性与计算效率，同时保留经典 MPPI 的优点。

Conclusion: Gauss-Newton 加速的 MPPI 为高维模型预测控制提供了一种有前景的改进路径，兼具灵活性和效率。

Abstract: Model Predictive Path Integral (MPPI) control is a sampling-based optimization method that has recently attracted attention, particularly in the robotics and reinforcement learning communities. MPPI has been widely applied as a GPU-accelerated random search method to deterministic direct single-shooting optimal control problems arising in model predictive control (MPC) formulations. MPPI offers several key advantages, including flexibility, robustness, ease of implementation, and inherent parallelizability. However, its performance can deteriorate in high-dimensional settings since the optimal control problem is solved via Monte Carlo sampling. To address this limitation, this paper proposes an enhanced MPPI method that incorporates a Jacobian reconstruction technique and the second-order Generalized Gauss-Newton method. This novel approach is called \textit{Gauss-Newton accelerated MPPI}. The numerical results show that the Gauss-Newton accelerated MPPI approach substantially improves MPPI scalability and computational efficiency while preserving the key benefits of the classical MPPI framework, making it a promising approach even for high-dimensional problems.

</details>


### [29] [Strategies for zero boil-off liquid hydrogen transfer: an export terminal case-study](https://arxiv.org/abs/2512.04609)
*Halvor Aarnes Krog,David Berstad*

Main category: eess.SY

TL;DR: 两种策略将LH2出口码头的沸损降到零。第一种：采用带变速驱动的泵并用分区控制实现接近零至0.24 wt% 的损失，泵效率接近70%是关键；固定速泵则损失较高，0.76–1.06 wt%。第二种策略通过提高海上储罐最大压力（从1.15 bara提高到1.35 bara，对固定速泵；对VSD泵在60%效率下需1.22 bara）来实现零损失。


<details>
  <summary>Details</summary>
Motivation: LH2出口码头要具备经济可行性，沸损需尽量降低以减轻运输与储存成本；通过对比变速驱动泵和固定速泵，以及不同储罐压力条件，寻找降低沸损的有效操作策略。

Method: 对两种策略进行对比分析：1) 使用变速驱动（VSD）和分区流量控制以实现可控流量，进行不确定性分析以评估损失（0–0.24 wt%）。2) 对固定速泵在不同储罐最大压力下的损失进行分析，确定在何种压力下可以达到零损失；同样评估VSD在60%效率条件下需要的压力。

Result: 在第一策略中，泵效率接近70%是实现极低损失的关键，VSD配合分区控制的损失范围为0至0.24 wt%，固定速泵则损失为0.76–1.06 wt%（相当于每艘船119吨）。“零损失”可通过将第二策略中的储罐最大压力提高来实现：固定速泵在1.35 bara时可达零损失；VSD泵在60%效率下需将最大压力提升至1.22 bara。

Conclusion: 要实现LH2出口 transfer 的零沸损，需要在泵设计和操作策略之间做出权衡：若追求低损失，应优先采用高效率的VSD泵并利用分区控制，同时通过适当提升储罐压力作为辅助手段；在固定速泵情境下，提升压力是可行但需要更高的压力等级。

Abstract: To ensure economic viability, LH2 export terminals must minimize boil-off losses. We show two strategies to achieve zero boil-off losses for the transfer of 160 000 m3 LH2 (11 248 tons) using a centrifugal pump. In the first strategy, a pump with variable speed drive (VSD) and split-range control for the flow rate achieves losses from 0 wt% to 0.24 wt% in an uncertainty analysis. A pump efficiency approaching 70% is the most important factor to minimize losses. In contrast, a fixed-speed pump has unacceptably high losses ranging from 0.76 wt% to 1.06 wt% (119 tons per ship). The second strategy is to increase the maximum pressure in the seaborne tank (base case is 1.15 bara). Zero loss is achieved for the fixed speed pump if the maximum pressure is increased to 1.35 bara, while 1.22 bara is required for the pump with VSD assuming an efficiency of 60%.

</details>


### [30] [Auto-Optimization with Active Learning in Uncertain Environment: A Predictive Control Approach](https://arxiv.org/abs/2512.04647)
*Yuan Tan,Jun Yang,Zhongguo Li,Wen-Hua Chen,Shihua Li*

Main category: eess.SY

TL;DR: 提出一种自适应的自动最优MPC框架，结合主动学习，能够在未知且会随环境变化的条件下追踪最优运行点。


<details>
  <summary>Details</summary>
Motivation: 在参数辨识受限且传统MPC难以在未知环境中实现稳定跟踪的背景下，提升对未知环境的自适应性与鲁棒性，尤其是解决持续激励不足与参数识别的挑战。

Method: 首先提出EO-MPC，将实时采样数据与鲁棒集合参数估计相结合，通过在终端约束中引入虚拟激励信号并建立持续激励条件的验证机制，解决参数辨识中持续激励不足的问题。基于此，提出AL-MPC，将可用数据与虚拟未来数据结合，解决跟踪未知最优条件与参数辨识之间的矛盾；并给出递归可行性与收敛性证明。

Result: 给出严格的理论证明：递归可行性与收敛性；大量实例证明在实际应用中的可靠性与有效性。

Conclusion: 该框架可在未知、可变环境中实现自适应、稳定的最优运行跟踪，提升MPC对环境变化的鲁棒性与学习能力。

Abstract: This paper presents an auto-optimal model predictive control (MPC) framework enhanced with active learning, designed to autonomously track optimal operational conditions in an unknown environment,where the conditions may dynamically adjust to environmental changes. First, an exploitation-oriented MPC (EO-MPC) is proposed, integrating real-time sampling data with robust set-based parameter estimation techniques to address the critical challenge of parameter identification. By introducing virtual excitation signals into the terminal constraint and establishing a validation mechanism for persistent excitation condition, the EO-MPC effectively resolves the issue of insufficient persistent excitation in parameter identification. Building upon this foundation, an active learning MPC (AL-MPC) approach is developed to integrate both available and virtual future data to resolve the fundamental conflict between tracking an unknown optimal operational condition and parameter identification. The recursive feasibility and convergence of the proposed methods are rigorously established, and numerous examples substantiate the reliability and effectiveness of the approach in practical applications.

</details>


### [31] [A Unified Low-rank ADI Framework with Shared Linear Solves for Simultaneously Solving Multiple Lyapunov, Sylvester, and Riccati Equations](https://arxiv.org/abs/2512.04676)
*Umair Zulfiqar,Zhong-Yi Huang*

Main category: eess.SY

TL;DR: 提出一个统一的低秩ADI框架，通过共享两次移位的线性求解来同时解六个Lyapunov、一个Sylvester和十个Riccati方程，并原生地实现模型降阶插值，同时保持稳定性、最小相位、正实、有界实、无耗等系统性质。


<details>
  <summary>Details</summary>
Motivation: 降低求解多类矩阵方程所需的计算成本，并保持关键系统性质；揭示低秩ADI在Lyapunov/Sylvester/Riccati问题中的插值本质以及共同的极点放置特性。

Method: 将Lyapunov、Sylvester、Riccati的ADIs仅在极点放置上有所差异，提出一个在每次迭代仅需两次移位线性求解的统一框架；可共享大部分线性求解，极点放置仅涉及小规模操作；可并行求解多种方程并从共享解中提取各自解。并推导降阶模型作为附带产物，具有插值性质且保持多种系统属性。

Result: 显著降低了计算成本，能够在两次移位线性求解下同时解决多达六个Lyapunov、一个Sylvester和十个Riccati方程；提取的降阶模型在镜像移位处插值，并保留稳定性、最小相位、正实、无界实和可让的被动性等属性。

Conclusion: 所提出的统一ADi框架不仅提升了求解效率，还作为递归、插值驱动的模型降阶方法，能够在降阶模型中保留原系统的若干关键性质。

Abstract: It is known in the literature that the low-rank ADI method for Lyapunov equations is a Petrov-Galerkin projection algorithm that implicitly performs model order reduction. In this paper, we show that the low-rank ADI methods for Sylvester and Riccati equations are also Petrov-Galerkin projection algorithms that implicitly perform model order reduction. By observing that the ADI methods for Lyapunov, Sylvester, and Riccati equations differ only in pole placement and not in their interpolatory nature, we show that the shifted linear solves-which constitute the bulk of the computational cost-can be shared. The pole-placement step involves only small-scale operations and is therefore inexpensive. We propose a unified ADI framework that requires only two shifted linear solves per iteration to simultaneously solve six Lyapunov equations, one Sylvester equation, and ten Riccati equations, thus substantially increasing the return on investment for the computational cost spent on the linear solves. All operations needed to extract the individual solutions from these shared linear solves are small-scale and inexpensive.
  Since all ADI methods implicitly perform model order reduction when solving these linear matrix equations, we show that the resulting reduced-order models can be obtained as an additional byproduct. These models not only interpolate the original transfer function at the mirror images of the ADI shifts but also preserve important system properties such as stability, minimum-phase property, positive-realness, bounded-realness, and passivity. Consequently, the proposed unified ADI framework also serves as a recursive, interpolation-based model order reduction method, which can preserve several important properties of the original model in the reduced-order model.

</details>


### [32] [Pick-to-Learn for Systems and Control: Data-driven Synthesis with State-of-the-art Safety Guarantees](https://arxiv.org/abs/2512.04781)
*Dario Paccagnan,Daniel Marks,Marco C. Campi,Simone Garatti*

Main category: eess.SY

TL;DR: P2L 是一种将数据驱动控制方法与最先进的安全性与性能保证相结合的框架，通过充分利用现有数据来联合设计与证书化，避免单独保留数据做校准或验证。它在最优控制、可达性分析、安全合成和鲁棒控制等核心问题上均有应用，且在多种应用中实现优于常用方法的设计与证书。


<details>
  <summary>Details</summary>
Motivation: 在安全关键环境中，对数据驱动控制方法需要严格的安全与性能保证，但现有方法往往牺牲数据用于测试/校准或限制学习算法，导致性能受限。

Method: 提出 Pick-to-Learn (P2L) 框架，使任意数据驱动控制方法具备状态最先进的安全性与性能保证。P2L 使用所有可用数据进行联合设计与证书化，省去为校准或验证而单独保留数据的需求。并通过对一系列系统与控制问题的综合版本进行验证。

Result: 在核心问题（包括最优控制、可达性分析、安全合成、鲁棒控制）上，P2L 能提供比常用方法更优的设计和证书，且在广泛的实际场景中显示出强潜力。

Conclusion: P2L 为数据驱动控制引入了一种数据高效且可证书化的框架，提升了在安全关键场景中的可部署性和性能潜力。

Abstract: Data-driven methods have become paramount in modern systems and control problems characterized by growing levels of complexity. In safety-critical environments, deploying these methods requires rigorous guarantees, a need that has motivated much recent work at the interface of statistical learning and control. However, many existing approaches achieve this goal at the cost of sacrificing valuable data for testing and calibration, or by constraining the choice of learning algorithm, thus leading to suboptimal performances. In this paper, we describe Pick-to-Learn (P2L) for Systems and Control, a framework that allows any data-driven control method to be equipped with state-of-the-art safety and performance guarantees. P2L enables the use of all available data to jointly synthesize and certify the design, eliminating the need to set aside data for calibration or validation purposes. In presenting a comprehensive version of P2L for systems and control, this paper demonstrates its effectiveness across a range of core problems, including optimal control, reachability analysis, safe synthesis, and robust control. In many of these applications, P2L delivers designs and certificates that outperform commonly employed methods, and shows strong potential for broad applicability in diverse practical settings.

</details>


### [33] [Constrained Control of PDE Traffic Flow via Spatial Control Barrier Functions](https://arxiv.org/abs/2512.04823)
*Brian Block,Stephanie Stockar*

Main category: eess.SY

TL;DR: 通过将控制李雅普诺夫函数（CLF）与空间可变控制屏障函数（sCBF）相结合，提出了一种对变速限制（VSL）控制的约束控制框架，用于一维泊松型LWR交通模型的域内控制，在确保密度不超过安全上限的前提下实现密度向期望分布收敛。


<details>
  <summary>Details</summary>
Motivation: 解决在时空变化环境下实现交通密度稳定控制与安全约束的问题；将CLF理论从常微分方程扩展到带有空间和时间变动的状态与控制输入，并通过sCBF将安全性统一纳入控制设计，提升VSL在PDE交通模型中的实用性与鲁棒性。

Method: 把ODE的CLF扩展到对包含空间维度的PDE系统，提出在域内VSL控制中应用的控制设计框架；引入空间可变的CBF（sCBF）以描述安全域并确保密度处在定义的安全集合内；通过一个在每个时间步可解的优化问题（结合CLF约束与CBF约束）实现受限控制输入，保持对目标密度的收敛同时满足安全上限。

Result: 结果显示，在不显著牺牲稳定化控制影响的前提下，约束控制能够有效维持交通密度在期望分布和安全边界之间，实现对密度的稳定控制与安全性保障。

Conclusion: 将CLF与sCBF结合用于PDE型交通模型的VSL控制是可行且有效的，能够在域内实现对目标密度的跟踪，同时满足安全约束，具有潜在的实时实现前景及对复杂时空变化的鲁棒性。

Abstract: In this paper, a constrained control approach to variable speed limit (VSL) control for macroscopic partial differential equations (PDE) traffic models is developed. Control Lyapunov function (CLF) theory for ordinary differential equations (ODE) is extended to account for spatially and temporally varying states and control inputs. The stabilizing CLF is then unified with safety constraints through the introduction of spatially varying control barrier functions (sCBF). These methods are applied to in-domain VSL control of the Lighthill-Whitham-Richards (LWR) model to regulate traffic density to a desired profile while ensuring the density remains below prescribed limits enforced by the sCBF. Results show that incorporating constrained control minimally affects the stabilizing control input while successfully maintaining the density with the defined safe set.

</details>


### [34] [Counterfactual Explanations for Power System Optimisation](https://arxiv.org/abs/2512.04833)
*Benjamin Fritz,Waqquas Bukhsh*

Main category: eess.SY

TL;DR: 提出基于反事实解释的电力市场可解释性框架，通过双层优化找出最小需求变动以改变最优解，在 DCOPF 与单元承诺问题中应用，结合历史数据提升计算效率与解释准确性。


<details>
  <summary>Details</summary>
Motivation: 在复杂电力市场中，调度决策的透明性与公平性要求对结果的可解释性；现有方法在大规模现实场景下往往计算成本高或解释性不足。

Method: 构建一个反事实解释框架，寻求使原始最优解发生改变的最小输入变动（最小化输入扰动），通过双层优化实现。研究对象包括 DCOPF（空间需求分布）与 Unit Commitment（时间需求分布）。比较前沿解释方法与数据驱动启发式，利用历史已解决实例数据加速求解并提高解释可行性。

Result: 实验结果表明，利用历史数据可以显著提升求解速度，且在传统可行性受限的场景下仍能给出有效解释；数据驱动启发式在解释准确性和计算效率之间取得良好平衡。

Conclusion: 提出可扩展的反事实解释框架，提升电力市场调度的透明性与可追溯性；未来工作可聚焦于进一步优化启发式、扩展至更多市场模型及约束。

Abstract: Enhanced computational capabilities of modern decision-making software have allowed us to solve increasingly sophisticated optimisation problems. But in complex socio-economic, technical environments such as electricity markets, transparent operation is key to ensure a fair treatment of all parties involved, particularly regarding dispatch decisions. We address this issue by building on the concept of counterfactual explanations, answering questions such as "Why was this generator not dispatched?" by identifying minimum changes in the input parameters that would have changed the optimal solution. Both DC Optimal Power Flow and Unit Commitment problems are considered, wherein the variable parameters are the spatial and temporal demand profiles, respectively. The thereby obtained explanations allow users to identify the most important differences between the real and expected market outcomes and observe which constraints have led to the solution. The framework uses a bilevel optimisation problem to find the counterfactual demand scenarios. State-of-the-art methods are compared with data-driven heuristics on the basis of computational efficiency and explanation accuracy. Results show that leveraging historical data from previously solved instances can provide significant speed benefits and allows us to derive explanations in cases where conventional methods would not be tractable.

</details>


### [35] [Stability-Guaranteed Dual Kalman Filtering for Electrochemical Battery State Estimation](https://arxiv.org/abs/2512.04885)
*Feng Guo,Guangdi Hu,Keyi Liao,Luis D. Couto,Khiem Trad,Ru Hong,Hamid Hamed,Mohammadhosein Safari*

Main category: eess.SY

TL;DR: 提出一种稳定性保障的双卡尔曼滤波（SG-DKF），通过李雅普诺夫分析给出稳定性下界并引入自适应死区规则，在创新超过稳定界限时暂停参数更新，提升电池状态/参数估计的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 双卡尔曼滤波在耦合较强时易在大初始误差或模型不匹配下发散，亟需稳定性保障以在电池管理系统中实现可靠状态估计。

Method: 基于李雅普诺夫分析推导出充分稳定性条件，进而得到自适应死区规则，使当创新超出稳定界限时暂停参数更新；将SG-DKF应用于电化学电池模型以验证稳定性与鲁棒性。

Result: 与双EKF的精度相当；在较大初始状态误差下，SOC RMSE降低超过45%，显著提升鲁棒性。

Conclusion: 所提出的SG-DKF在保证稳定性的前提下实现了对状态与参数的稳健估计，且在电池管理场景中对大Init误差情形具有显著的性能提升。

Abstract: Accurate and stable state estimation is critical for battery management. Although dual Kalman filtering can jointly estimate states and parameters, the strong coupling between filters may cause divergence under large initialization errors or model mismatch. This paper proposes a Stability Guaranteed Dual Kalman Filtering (SG-DKF) method. A Lyapunov-based analysis yields a sufficient stability condition, leading to an adaptive dead-zone rule that suspends parameter updates when the innovation exceeds a stability bound. Applied to an electrochemical battery model, SG-DKF achieves accuracy comparable to a dual EKF and reduces state of charge RMSE by over 45% under large initial state errors.

</details>


### [36] [Small-Signal Stability Oriented Real-Time Operation of Power Systems with a High Penetration of Inverter-Based Resources](https://arxiv.org/abs/2512.04892)
*Francesca Rossi,Juan Carlos Olives-Camps,Eduardo Prieto-Araujo,Oriol Gomis-Bellmunt*

Main category: eess.SY

TL;DR: A two-stage framework combining offline data-driven damping-based stability assessment with Online Feedback Optimization (OFO) to ensure safe, optimal operation in power systems with high inverter-based resources (IBRs); validated on a modified IEEE 9-bus test case with grid-following and grid-forming IBRs.


<details>
  <summary>Details</summary>
Motivation: Stability risk and performance trade-offs in grids with high penetration of IBRs require guarantees of small-signal stability while achieving optimal operating points; a data-driven stability index can guide online control to avoid the instability region.

Method: Stage 1 offline: generate operating-point data and fit a regression model that maps operating conditions to a damping-based stability index. Stage 2 online: deploy an Online Feedback Optimization (OFO) controller to reach optimal operating points while maintaining a secure distance from the instability region. Evaluation on a modified IEEE 9-bus system where synchronous generators are replaced by IBRs operating in grid-following and grid-forming modes.

Result: The approach is shown to be effective: the data-driven stability index guides the OFO to operate away from the instability region while achieving near-optimal operation; results are analyzed in detail for both GF and GFM IBR cases on the test system.

Conclusion: The proposed two-stage strategy offers a viable path to safe and optimal operation in future grids with high IBR penetration, leveraging data-driven stability assessment and online optimization; applicability is demonstrated on a standard test case, with discussion on performance and potential limitations.

Abstract: This study proposes a control strategy to ensure the safe operation of modern power systems with high penetration of inverter-based resources (IBRs) within an optimal operation framework. The objective is to obtain operating points that satisfy the optimality conditions of a predefined problem while guaranteeing small-signal stability. The methodology consists of two stages. First, an offline analysis of a set of operating points is performed to derive a data-driven regression-based expression that captures a damping-based stability index as a function of the operating conditions. Second, an Online Feedback Optimization (OFO) controller is employed to drive the system toward an optimal operating point while maintaining a secure distance from the instability region. The proposed strategy is evaluated on an academic test case based on a modified version of the IEEE 9-bus system, in which synchronous generators are replaced by IBRs operating under both grid-following and grid-forming control modes. The results demonstrate the effectiveness of the method and are discussed in detail.

</details>


### [37] [A Randomized Scheduling Framework for Privacy-Preserving Multi-robot Rendezvous given Prior Information](https://arxiv.org/abs/2512.05053)
*Le Liu,Yu Kawano,Ming Cao*

Main category: eess.SY

TL;DR: 提出一种基于随机调度的隐私保护多机器人 rendezvous 方法，在更低通信率下仍能实现目标并提升隐私，隐私以点wise最大泄漏量量化。


<details>
  <summary>Details</summary>
Motivation: 随着对伦理与操作约束的日益关注，多机器人系统中的隐私保护成为关键问题，需要在协同与信息披露之间取得折中。

Method: 引入随机调度机制，通过降低通信率并对隐私用点wise最大泄漏量进行量化，证明在该机制下仍可实现 rendezvous；结合理论推导与分析，并辅以数值仿真。

Result: 提出的机制在较低传输率下获得更强隐私保障；证明 rendezvous 在随机调度下成立；提供数值仿真实验以验证方法的有效性。

Conclusion: 该工作证明了在降低通信成本的同时还能提升隐私保护并实现 rendezvous，为隐私感知的多机器人协同提供新思路。

Abstract: Privacy has become a critical concern in modern multi-robot systems, driven by both ethical considerations and operational constraints. As a result, growing attention has been directed toward privacy-preserving coordination in dynamical multi-robot systems. This work introduces a randomized scheduling mechanism for privacy-preserving robot rendezvous. The proposed approach achieves improved privacy even at lower communication rates, where privacy is quantified via pointwise maximal leakage. We show that lower transmission rates provide stronger privacy guarantees and prove that rendezvous is still achieved under the randomized scheduling mechanism. Numerical simulations are provided to demonstrate the effectiveness of the method.

</details>


### [38] [The Evolving Landscape of Interactive Surface Sensing Technologies](https://arxiv.org/abs/2512.05071)
*David Wang,Wilson Chen,Tianju Wang,Jiale Zhang*

Main category: eess.SY

TL;DR: 这是一篇关于交互式表面传感技术的综述，梳理从红外视觉到电容触控的发展，并扩展至视觉、声学、毫米波雷达和振动等新兴模态；比较各模态的工作原理、分辨率、可扩展性与应用，讨论多模态融合、设计权衡，以及面临的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 揭示历史演进、帮助设计者理解不同模态之优劣及适用场景，提供评估框架以推动无处不在、智能化的交互环境研究。

Method: 基于系统性文献综述的对比分析，围绕工作原理、分辨率、可扩展性与应用场景构建比较表，分析多模态融合的潜力和 trade-offs。

Result: 总结各传感模态的优缺点与典型应用，提出用于评估交互表面性能的综合框架，并识别感知精度、功耗、隐私等关键挑战及潜在解决方向。

Conclusion: 新兴模态（如毫米波雷达、振动感知）有望提升环境的无处不在性与智能性，但需解决精度、功耗、隐私等问题，未来研究应聚焦提升准确性、能效及模态融合的可扩展部署。

Abstract: Interactive surfaces have evolved from capacitive touch and IR based systems into a diverse ecosystem of sensing technologies that support rich and expressive human computer interaction. This survey traces that progression, beginning with infrared vision based approaches, such as FTIR and diffuse illumination, and the rise of capacitive touch as the dominant technology in modern devices, to focusing on contemporary modalities including vision and acoustic sensing. New technologies under development are also discussed, including mmWave radar, and vibration based techniques. Each sensing technique is examined in terms of its operating principles, resolution, scalability, and applications, along with discussions of multimodal integration. By comparing tradeoffs between sensing modalities, the survey highlights the technical and design factors that shape interactive surface performance and user experience. The review concludes by identifying persistent challenges, including sensing accuracy, power constraints, and privacy concerns, and outlines how emerging sensing modalities can enable future interactive environments to be ubiquitous and intelligent.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [39] [A Spatial Array for Spectrally Agile Wireless Processing](https://arxiv.org/abs/2512.04182)
*Ali Rasteh,Andrew Hennessee,Ishaan Shivhare,Siddharth Garg,Sundeep Rangan,Brandon Reagen*

Main category: eess.SP

TL;DR: 提出一种可重构的空间阵列核心，用于无线场核的通用处理，与专用核在32nm工艺下通过高层次综合评估对比，在特定工作负载下接近专用核心的效率，推动可扩展和灵活的系统实现。


<details>
  <summary>Details</summary>
Motivation: 面向未来无线系统的规模化、频谱灵活性、集成感知、抗干扰能力等需求，现有专用核难以同时兼顾高效、可重用和快速适配。

Method: 设计并实现一种自定义的可重构空间阵列架构，针对一类常见的无线场核，作为通用处理核心；与针对每个核的专业核进行对比，通过高层次综合（HLS）实现并在32nm工艺下合成，评估延迟、吞吐、面积与功耗。

Result: 在某些工作负载/条件下，通用的时序列阵列架构能够逼近专用核的效率，显示出在可扩展性与灵活性方面的潜力；给出具体条件与权衡。

Conclusion: 可重构的通用空间阵列为未来大规模MIMO与感知任务提供了更具适应性的实现路径，在特定约束下可达到接近专用核的性能，同时需要对工作负载与实现成本进行细化分析。

Abstract: Massive MIMO is a cornerstone of next-generation wireless communication, offering significant gains in capacity, reliability, and energy efficiency. However, to meet emerging demands such as high-frequency operation, wide bandwidths, co-existence, integrated sensing, and resilience to dynamic interference, future systems must exhibit both scalability and spectral agility. These requirements place increasing pressure on the underlying processing hardware to be both efficient and reconfigurable. This paper proposes a custom-designed spatial array architecture that serves as a reconfigurable, general-purpose core optimized for a class of wireless kernels that commonly arise in diverse communications and sensing tasks. The proposed spatial array is evaluated against specialized cores for each kernel using High-Level Synthesis (HLS). Both the reconfigurable and specialized designs are synthesized in a 32 nm process to assess latency, throughput, area, and power in realistic processes. The results identify conditions under which general-purpose systolic architectures can approach the efficiency of specialized cores, thereby paving the way toward more scalable and agile systems.

</details>


### [40] [A Mixed Precision FFT with applications in MRI](https://arxiv.org/abs/2512.04317)
*Nikhil Deveshwar,Abhejit Rajagopal,Peder E. Z. Larson*

Main category: eess.SP

TL;DR: 提出一种混合精度快速傅立叶变换（FFT）实现，采用分块微缩放（MX）、全局二的幂次前置缩放和预量化的低比特位绕数。对两组公开的MRI数据集进行前向与来回FFT的保真度评估，比较不同低精度格式、图像尺寸与MX分块大小对图像质量的影响。结果显示在MX缩放下，尾数精度是主要限制因素；图像尺寸对数值性能影响较弱，但分块大小对性能有显著权衡，较大分块尺寸可实现更好的数值表现。


<details>
  <summary>Details</summary>
Motivation: 在MRI数据处理中，需要在保持数值稳定性的同时提升FFT的计算效率，因此需要研究在低精度数字格式下的混合精度FFT，尤其关注分块策略对数值稳定性和保真度的影响。

Method: 提出MX：分块微缩放策略；使用全局二的幂次前置缩放；以及预量化的低比特绕数。对两组公开MRI数据集进行前向FFT和循环/来回FFT的保真度评估，比较不同低精度格式、图像尺寸和MX分块大小对图像质量的影响。

Result: 结果表明，在MX缩放下， mantissa（尾数）精度是主要的限制因素；对比消融实验显示图像尺寸对数值性能影响较弱，但分块大小存在明显权衡：较大分块尺寸通常带来更好的数值表现。

Conclusion: 该方法在混合精度FFT中表现出可控的数值行为，适当选择尾数精度和分块大小可在保持保真度的同时提升计算效率；未来工作可进一步细化对不同低精度格式的稳定性分析及在实际MRI工作流中的硬件实现考量。

Abstract: A mixed precision Fast Fourier transform (FFT) implementation is presented. The procedure uses per-block microscaling (MX), a global power-of-two prescale, and prequantized low bit twiddles. We evaluate forward and round-trip FFT fidelity on two public MRI datasets and compare the effect of various low precision formats, image sizes, and MX block sizes on image quality. Results show that mantissa precision is the primary limiter under MX scaling while ablations suggest weak dependence on image size but a clear block-size trade-off with larger block sizes resulting in better numerical performance.

</details>


### [41] [RRAM-Based Analog Matrix Computing for Massive MIMO Signal Processing: A Review](https://arxiv.org/abs/2512.04365)
*Pushen Zuo,Zhong Sun*

Main category: eess.SP

TL;DR: RRAM 基于 AMC 为大规模 MIMO 信号处理提供高效的 in-memory 计算框架，覆盖 MVM、矩阵求逆、以及压缩感知等应用；综述总结进展、机会与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着 6G 下的超大规模 MIMO 对高吞吐、低功耗的矩阵运算需求日益增长，RRAM-AMC 提供就地计算以缓解数据搬运瓶颈，并在 OFDM、检测、预编码、信号恢复等关键任务中带来潜在加速。

Method: 综述近来把基于 RRAM 的 AMC 应用于大规模 MIMO 的若干方面：使用 MVM 电路完成 DFT/IDFT 的 OFDM 调制/解调；基于 MVM 的迭代算法进行检测与预编码；利用 INV 和 GINV 电路实现快速矩阵求逆与广义逆；以及压缩感知的通道估计和特征矩阵的本征电路等；并讨论端到端的设备–电路–算法协同设计，以及面临的挑战与解决路径。

Result: 研究表明 AMC 能在一定程度上实现 OFDM 的高效变换、MIMO 检测与预编码的加速，以及“单步”矩阵求逆等潜在优势；提出了用于通道估计的压缩感知和用于泄漏型预编码的特征电路，同时明确了设备可靠性、模数精度、阵列扩展性和数据转换瓶颈等关键挑战。

Conclusion: 在设备-电路-算法协同设计的持续推进下，基于 RRAM 的 AMC 有望为 6G 时代的（超）大规模 MIMO 信号处理提供高效、可靠的解决方案。

Abstract: Resistive random-access memory (RRAM) provides an excellent platform for analog matrix computing (AMC), enabling both matrix-vector multiplication (MVM) and the solution of matrix equations through open-loop and closed-loop circuit architectures. While RRAM-based AMC has been widely explored for accelerating neural networks, its application to signal processing in massive multiple-input multiple-output (MIMO) wireless communication is rapidly emerging as a promising direction. In this Review, we summarize recent advances in applying AMC to massive MIMO, including DFT/IDFT computation for OFDM modulation and demodulation using MVM circuits; MIMO detection and precoding using MVM-based iterative algorithms; and rapid one-step solutions enabled by matrix inversion (INV) and generalized inverse (GINV) circuits. We also highlight additional opportunities, such as AMC-based compressed-sensing recovery for channel estimation and eigenvalue circuits for leakage-based precoding. Finally, we outline key challenges, including RRAM device reliability, analog circuit precision, array scalability, and data conversion bottlenecks, and discuss the opportunities for overcoming these barriers. With continued progress in device-circuit-algorithm co-design, RRAM-based AMC holds strong promise for delivering high-efficiency, high-reliability solutions to (ultra)massive MIMO signal processing in the 6G era.

</details>


### [42] [Enabling Fast Polar SC Decoding with IR-HARQ](https://arxiv.org/abs/2512.04418)
*Marwan Jalaleddine,Mohamad Ali Jarkas,Jiajie Li,Warren J. Gross*

Main category: eess.SP

TL;DR: 提出一种将极化码与增量冗余（IR） HARQ 结合的改进 SC 解码器，通过引入特殊节点来加速解码，在长度2048时减少节点遍历72%，且 FER 性能不下降。


<details>
  <summary>Details</summary>
Motivation: 在下一代无线通信系统中扩展极化码的应用，需支持 IR-HARQ；同时在高吞吐场景下，SC 解码具备高面积效率，因此需要通过结构化修改来提升解码速度以适应高吞吐。

Method: 对 SC 解码器进行修改，加入“特殊节点”以加速解码过程，并确保与 IR-HARQ 的兼容性，从而实现更高吞吐与低复杂度。

Result: 与未修改的 SC IR-HARQ 相比，长度2048时节点遍历减少约72%；仿真结果显示 FER 性能未下降。

Conclusion: 提出的特殊节点修改在不损害 FER 的前提下提高了解码效率和吞吐潜力，使 SC 解码更适用于高吞吐 IR-HARQ 场景。

Abstract: To extend the applications of polar codes within next-generation wireless communication systems, it is essential to incorporate support for Incremental Redundancy (IR) Hybrid Automatic Repeat Request (HARQ) schemes. For very high- throughput applications, Successive Cancellation (SC) decoding is particularly appealing for polar codes owing to its high area efficiency. In this paper, we propose modifications to SC decoders that employ special nodes to accelerate decoding. Our modifications enable the use of polar IR-HARQ with SC decoding for high throughput applications. Compared to the unmodified SC IR-HARQ scheme, our proposed approach allows us to achieve a 72% reduction in node traversals with a polar code of length 2048. Simulation results confirm that the proposed special node modifications do not cause any degradation in FER performance.

</details>


### [43] [Nonlinear EM-based Signal Processing](https://arxiv.org/abs/2512.04595)
*Mattia Fabiani,Giulia Torcolacci,Davide Dardari*

Main category: eess.SP

TL;DR: In XL-MIMO systems at high frequencies, the paper proposes over-the-air electromagnetic (EM) processing using reconfigurable passive linear and nonlinear scattering elements placed in the reactive near field to reduce hardware complexity and RF chains while maintaining essential performance metrics.


<details>
  <summary>Details</summary>
Motivation: To address hardware complexity, latency, and power consumption in ultra-large antenna arrays operating at high-frequency bands by moving part of the signal processing into the EM domain.

Method: Propose architectures that use reconfigurable passive linear and nonlinear scattering elements in the reactive near field of signal sources to enable multifunctional linear and nonlinear EM signal processing directly over the air.

Result: Numerical analyses indicate substantial reductions in system complexity and the number of RF chains while still achieving key performance goals such as direction-of-arrival and position estimation without requiring additional analog or digital processing.

Conclusion: Over-the-air EM processing leveraging near-field interactions and reconfigurable scattering elements can enable scalable, sustainable XL-MIMO systems, reducing hardware demands while preserving essential sensing/estimation capabilities.

Abstract: The use of high-frequency bands, combined with antenna arrays containing an extremely large number of elements (XL-MIMO), is pushing current technology to its limits in terms of hardware complexity, latency, and power consumption. A promising approach to achieving scalable and sustainable solutions is to shift part of the signal processing directly into the electromagnetic (EM) domain. In this paper, we investigate novel architectures that harness the interaction of reconfigurable passive linear and nonlinear (NL) scattering elements positioned in the reactive near field of signal sources. The objective is to enable multifunctional linear and NL EM signal processing to occur directly "over-the-air." Numerical results highlight the potential to significantly reduce both system complexity and the number of RF chains, while still achieving key performance metrics in applications such as direction-of-arrival and position estimation, without the need for additional analog or digital processing.

</details>


### [44] [Pinching-Antenna System Design under Random LoS and NLoS Channels](https://arxiv.org/abs/2512.04719)
*Yanqing Xu,Yang Lu,Zhiguo Ding,Tsung-Hui Chang*

Main category: eess.SP

TL;DR: 提出了一种多用户可调节辐射元件的pinching-antenna系统，针对距离相关的LoS阻塞与NLoS散射的组合概率信道，设计两种优化目标并给出全局最优的分支-二分解法，显著优于固定天线设计。


<details>
  <summary>Details</summary>
Motivation: 现实无线传播具有随机性，传统仅考虑LoS主导的理想化场景不足以描述实际环境。需要同时兼顾系统吞吐与可靠性，因此引入距离相关的LoS阻塞与NLoS散射的组合信道模型，并通过两类设计指标来实现性能与可靠性的权衡。

Method: 将问题建模为两类优化：1) 在多用户场景下最大化最小平均SNR以提升公平性和长期吞吐；2) 在每个用户 outage约束下最大化保证SNR阈值。尽管问题非凸，但利用潜在的单调结构，提出基于二分法的低复杂度全局最优算法，仅需简单标量评估即可收敛。

Result: 通过大量仿真验证，所提方法在随机LoS与NLoS通道下显著优于传统固定天线方案，且针插天线系统能够有效提升系统性能。

Conclusion: 针插天线系统在现实多路径、不确定信道环境中具有明显优势，能够实现更高的吞吐与可靠性，适合在大规模系统中灵活重构信道状态。

Abstract: Pinching antennas, realized through position-adjustable radiating elements along dielectric waveguides, have emerged as a promising flexible-antenna technology thanks to their ability to dynamically reshape large-scale channel conditions. However, most existing studies focus on idealized LoS-dominated environments, overlooking the stochastic nature of realistic wireless propagation. This paper investigates a more practical multiuser pinching-antenna system under a composite probabilistic channel model that captures distance-dependent LoS blockage and NLoS scattering. To account for both efficiency and reliability aspects of communication, two complementary design metrics are considered: an average signal-to-noise ratio (SNR) metric characterizing long-term throughput and fairness, and an outage-constrained metric ensuring a prescribed reliability level. Based on these metrics, we formulate two optimization problems: the first maximizes the max-min average SNR across users, while the second maximizes a guaranteed SNR threshold under per-user outage constraints. Although both problems are inherently nonconvex, we exploit their underlying monotonic structures and develop low-complexity, bisection-based algorithms that achieve globally optimal solutions using only simple scalar evaluations. Extensive simulations validate the effectiveness of the proposed methods and demonstrate that pinching-antenna systems significantly outperform conventional fixed-antenna designs even under random LoS and NLoS channels.

</details>


### [45] [CIG-MAE: Cross-Modal Information-Guided Masked Autoencoder for Self-Supervised WiFi Sensing](https://arxiv.org/abs/2512.04723)
*Gang Liu,Yanling Hao,Yixuan Zou*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human Action Recognition using WiFi Channel State Information (CSI) has emerged as an attractive alternative to vision-based methods due to its ubiquity, device-agnostic nature, and inherent privacy-preserving capabilities. However, the high cost of manual annotation and the limited scale of publicly available CSI datasets restrict the performance of supervised approaches. Self-supervised learning (SSL) offers a promising avenue, but existing contrastive paradigms rely on data augmentations that conflict with the physical semantics of radio signals and require large-batch training, making them poorly suited for CSI. To overcome these challenges, we introduce CIG-MAE -- a Cross-modal Information-Guided Masked Autoencoder -- that reconstructs both the amplitude and phase of CSI using a symmetric dual-stream architecture with a high masking ratio. Specifically, we propose an Adaptive Information-Guided Masking strategy that dynamically allocates attention to time-frequency regions with high information density to improve learning efficiency, and incorporate a Barlow Twins regularizer to align cross-modal representations without negative samples. Experiments on three public datasets show that CIG-MAE consistently outperforms SOTA SSL methods and even surpasses a fully supervised baseline, demonstrating superior data efficiency, robustness, and representation generalization.

</details>


### [46] [Beampattern Synthesis for Discrete Phase RIS in Communication and Sensing Systems](https://arxiv.org/abs/2512.04881)
*Xiao Cai,Hei Victor Cheng,Daniel E. Lucani*

Main category: eess.SP

TL;DR: 提出一种适用于离散相位约束的 RIS 阵列模式综合方法，结合惩罚项和 MM 技术将优化问题转化为凸优化，能够生成宽波束以覆盖目标区域，与传统波束扫描相比显著提升 AOA 估计精度与探测概率。


<details>
  <summary>Details</summary>
Motivation: 在目标尚未被检测到前，需对可能的目标区域进行宽覆盖以提升方向估计；RIS 的相位受限（离散或连续）给阵列模式综合带来挑战，需在幅度和相位自由度受限的条件下实现宽波束。

Method: 引入惩罚法，将离散相位约束推向凸包边界；再通过 Minorization-Maximization (MM) 将问题重新表述为凸优化问题；允许幅度和相位共同调节以实现离散相位下的宽辐射模式，并与传统波束扫描进行对比。

Result: 数值结果表明，该算法能够在离散相位约束下生成与具有逐功率约束的情形相近的宽波束；在低到中等信噪比下，AOA 的均方误差（MSE）相比传统方法降低数个数量级；在探测概率方面，相较于传统波束扫描，约在所需的信噪比上实现约8 dB 的提升。

Conclusion: 该方法有效处理 RIS 的离散相位约束，实现宽波束模式，并显著提升方向估计精度与探测性能，优于传统波束扫掠方法。

Abstract: Extensive research on Reconfigurable Intelligent Surfaces (RIS) has primarily focused on optimizing reflective coefficients for passive beamforming in specific target directions. This optimization typically assumes prior knowledge of the target direction, which is unavailable before the target is detected. To enhance direction estimation, it is critical to develop array pattern synthesis techniques that yield a wider beam by maximizing the received power over the entire target area. Although this challenge has been addressed with active antennas, RIS systems pose a unique challenge due to their inherent phase constraints, which can be continuous or discrete.
  This work addresses this challenge through a novel array pattern synthesis method tailored for discrete phase constraints in RIS. We introduce a penalty method that pushes these constraints to the boundary of the convex hull. Then, the Minorization-Maximization (MM) method is utilized to reformulate the problem into a convex one. Our numerical results show that our algorithm can generate a wide beam pattern comparable to that achievable with per-power constraints, with both the amplitudes and phases being adjustable. We compare our method with a traditional beam sweeping technique, showing a) several orders of magnitude reduction of the MSE of Angle of Arrival (AOA) at low to medium Signal-to-Noise Ratio (SNR)s; and b) $8$~dB SNR reduction to achieve a high probability of detection.

</details>


### [47] [Channel-Aware Multi-Domain Feature Extraction for Automatic Modulation Recognition in MIMO Systems](https://arxiv.org/abs/2512.04899)
*Yunpeng Qu,Yazhou Sun,Bingyu Hui,Jintao Wang,Jian Wang*

Main category: eess.SP

TL;DR: 提出了 CAMD 框架以在 MIMO 场景下进行 AMR，通过通道补偿模块与多域特征提取实现对信道干扰的鲁棒识别。并在 MIMOSig-Ref 数据集上验证，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在非合作通信中，AMR 的挑战在于多天线信道会改变信号的统计特性，导致现有 DL-AMR 多集中在 SISO 设置且对 MIMO 的鲁棒性不足。需要一种通道感知且能整合多域特征的方案以提升识别性能。

Method: 提出 Channel-Aware Multi-Domain feature extraction (CAMD) 框架。包含一个高效的信道补偿模块以重建发射信号，并通过提取与整合跨域特征（包括天线内的时域相关性和天线间的信道相关性），实现对信道干扰更鲁棒的表征。

Result: 在广泛使用的 MIMOSig-Ref 数据集及复杂移动信道环境下，实验结果显示 CAMD 相对于以往的最先进方法具有性能优势。

Conclusion: 通过通道感知的多域特征提取和信道补偿，CAMD 能提升 MIMO 场景下的自动调制识别鲁棒性和准确性，适用于复杂移动信道中的非合作通信。

Abstract: Automatic modulation recognition (AMR) is a key technology in non-cooperative communication systems, aiming to identify the modulation scheme from signals without prior information. Deep learning (DL)-based methods have gained wide attention due to their excellent performance, but research mainly focuses on single-input single-output (SISO) systems, with limited exploration for multiple-input multiple-output (MIMO) systems. The confounding effects of multi-antenna channels can interfere with the statistical properties of MIMO signals, making identification particularly challenging. To overcome these limitations, we propose a Channel-Aware Multi-Domain feature extraction (CAMD) framework for AMR in MIMO systems. Our CAMD framework reconstructs the transmitted signal through an efficient channel compensation module and achieves a more robust representation capability against channel interference by extracting and integrating multi-domain features, including intra-antenna temporal correlations and inter-antenna channel correlations. We have verified our method on the widely-used dataset, MIMOSig-Ref, with complex mobile channel environments. Extensive experiments confirm the performance advantages of CAMD over previous state-of-the-art methods.

</details>


### [48] [Analytical and Cross-Sectional Clinical Validity of a Smartphone-Based U-Turn Test in Multiple Sclerosis](https://arxiv.org/abs/2512.04914)
*Marta Płonka,Rafał Klimas,Dimitar Stanev,Lorenza Angelini,Natan Napiórkowski,Gabriela González Chan,Lisa Bunn,Paul S Glazier,Richard Hosking,Jenny Freeman,Jeremy Hobart,Mattia Zanon,Jonathan Marsden,Licinio Craveiro,Mike D Rinderknecht*

Main category: eess.SP

TL;DR: 便携智能手机在MS患者中的UTT测试可在实验室和居家环境中以不同部位佩戴实现高准确度的转向检测和可重复性，且与传统 mocap 结果高度一致，具备在临床试验中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 为MS患者提供低成本、可远程监测的动态平衡和步态转向评估工具，克服传统运动捕捉系统的局限性，评估 smartphone-based U-Turn Test（UTT）在不同佩戴位置和设置下的可行性与可靠性。

Method: 纳入96名EMD评分在0–6.5之间的MS患者。让受试者在体外实验室（有监督）使用6个不同佩戴部位的智能手机完成UTT，在为期2周的无监督居家阶段使用1部智能手机完成UTT。对监督设置中用智能手机检测的转向与 mocap 结果进行F1分数比较；使用ICC(3,1)评估手机转向速度与 mocap 的一致性及偏差。无监督设置评估ICC(2,1)的重测可靠性，并用斯皮尔曼相关分析与临床指标及患者报告量表相关性。

Result: 监督条件下，转向检测准确性高，F1>95%（各佩戴位置一致）。手机转向速度在监督（1.44 rad/s）与无监督（1.47 rad/s）以及与mocap（1.47 rad/s）之间相近。ICC(3,1)在0.87–0.92之间，表明手机与mocap的转向速度具有高一致性，偏差在-0.04至0.11 rad/s之间为最小。无监督条件下，若将测试次数聚合≥2次，ICC(2,1) >0.90，表明良好的重测可靠性。UTT转向速度与临床指标（如Timed 25-Foot Walk、EDSS、Ambulation Score、MS Walking Scale、Activities-specific Balance Confidence）呈显著负相关，r介于-0.79至-0.61。

Conclusion: UTT在不同佩戴部位和不同设置下均能准确且可重复地测量转向速度，具有作为MS试验工具的潜力，便于大规模远程监测与跨场景的评估。

Abstract: The observational GaitLab study (ISRCTN15993728) enrolled adult people with multiple sclerosis (PwMS) with Expanded Disability Status Scale (EDSS) <=6.5. PwMS performed the U-Turn Test (UTT), a smartphone-based assessment of dynamic balance, in a gait laboratory (supervised setting) using 6 smartphones at different body locations and daily during a 2-week remote period (unsupervised setting) using 1 smartphone. In the supervised setting, the accuracy of detecting turns with smartphones was compared against turns detected with a motion capture system (mocap) using F1 scores. Agreement between turn speed measured with smartphones and mocap was assessed by intraclass correlation coefficient (ICC[3,1]) and bias. In the unsupervised setting, test-retest reliability was assessed by ICC(2,1), and correlations with clinical and patient-reported measures by Spearman rank correlation. Ninety-six PwMS were included. In the supervised setting, turns were detected with high accuracy (F1 scores >95% across smartphone wear locations). Smartphone-derived turn speed was comparable across the supervised (1.44 rad/s) and unsupervised settings (1.47 rad/s), and with mocap-derived turn speed (1.47 rad/s). ICC(3,1) revealed high agreement between smartphone- and mocap-derived turn speed (ICC[3,1]: 0.87-0.92 across smartphone wear locations). Bias was minimal (-0.04 to 0.11 rad/s). In the unsupervised setting, test-retest reliability (ICC[2,1]) was >0.90 when aggregating >=2 tests. The UTT correlated with Timed 25-Foot Walk gait speed, EDSS, Ambulation score, 12-item Multiple Sclerosis Walking Scale, and Activities-specific Balance Confidence scale (r=-0.79 to -0.61). The UTT measures turn speed accurately and reproducibly irrespective of smartphone wear location and settings. These findings affirm its potential as a valuable tool in multiple sclerosis trials.

</details>


### [49] [Distributed Riemannian Optimization in Geodesically Non-convex Environments](https://arxiv.org/abs/2512.04915)
*Xiuheng Wang,Ricardo Borsoi,Cédric Richard,Ali H. Sayed*

Main category: eess.SP

TL;DR: Proposes a distributed Riemannian diffusion adaptation algorithm for optimizing a sum of geodesically smooth non-convex costs on manifolds with bounded curvature. It shows network agreement in Fréchet variance, convergence to first-order stationary points, and linear convergence under a Riemannian PL condition, with application to decentralized robust PCA on Grassmann manifolds.


<details>
  <summary>Details</summary>
Motivation: To extend diffusion adaptation, a well-known distributed optimization strategy, to Riemannian manifolds in order to solve large-scale, networked optimization problems where the objective is non-convex along geodesics.

Method: Develop a Riemannian diffusion adaptation algorithm for aggregating local geodesically smooth non-convex costs across a network. Analyze Fréchet variance to quantify network agreement and prove convergence to first-order stationary points. Establish linear convergence under a constant step size when the global cost satisfies the Riemannian Polyak-Lojasiewicz (PL) condition. Demonstrate the approach on a decentralized robust PCA problem on the Grassmann manifold with numerical simulations.

Result: The algorithm achieves approximate network agreement via small Fréchet variance among agents and converges to first-order stationary points for general geodesically non-convex costs. When the global cost satisfies the Riemannian PL condition, it converges linearly (up to a steady-state error) with a constant step size. Numerical experiments on decentralized robust PCA on the Grassmann manifold illustrate convergence and performance.

Conclusion: The study provides theoretical guarantees for distributed non-convex optimization on manifolds using a diffusion-based approach, linking network agreement and convergence properties under geometric conditions, and validating the framework with practical demonstrations on Grassmann manifolds.

Abstract: This paper studies the problem of distributed Riemannian optimization over a network of agents whose cost functions are geodesically smooth but possibly geodesically non-convex. Extending a well-known distributed optimization strategy called diffusion adaptation to Riemannian manifolds, we show that the resulting algorithm, the Riemannian diffusion adaptation, provably exhibits several desirable behaviors when minimizing a sum of geodesically smooth non-convex functions over manifolds of bounded curvature. More specifically, we establish that the algorithm can approximately achieve network agreement in the sense that Fréchet variance of the iterates among the agents is small. Moreover, the algorithm is guaranteed to converge to a first-order stationary point for general geodesically non-convex cost functions. When the global cost function additionally satisfies the Riemannian Polyak-Lojasiewicz (PL) condition, we also show that it converges linearly under a constant step size up to a steady-state error. Finally, we apply this algorithm to a decentralized robust principal component analysis (PCA) problem formulated on the Grassmann manifold and illustrate its convergence and performance through numerical simulations.

</details>


### [50] [Markov-Renewal Single-Photon LiDAR Simulator](https://arxiv.org/abs/2512.04924)
*Weijian Zhang,Prateek Chennuri,Hashan K. Weerasooriya,Bole Ma,Stanley H. Chan*

Main category: eess.SP

TL;DR: 提出一种基于马尔科夫-再生过程（MRP）的SP-LiDAR光子统计仿真，结合谱截断与移位不变性，实现既高保真又高效的3D直方图立方生成，显著加速数据生成以用于学习型SP-LiDAR重建。


<details>
  <summary>Details</summary>
Motivation: 解决SP-LiDAR仿真在速度和保真度之间的权衡问题，迫切需要在物理真实度与计算效率之间取得平衡，以支持大规模、学到的重建方法的发展。

Method: 提出MRP模型来解析死时间下的注册光子计数的均值与方差，使用谱截断规则高效计算复杂协方差。证明该过程具备 shift-invariance，并据此通过预计算查找表实现像素级到整个平台直方图体积的扩展。

Result: 在保持逐序列金标准相同保真度的前提下，显著提高仿真速度，能生成与真实数据不可区分的3D数据立方，便于大规模数据生成用于学习基的SP-LiDAR重建。

Conclusion: 为SP-LiDAR仿真提供了一种兼具保真度与速度的新范式，通过MRP与谱截断实现高效、物理可信的数据生成，推动学习驱动的SP-LiDAR研究与应用。

Abstract: Single-photon LiDAR (SP-LiDAR) simulators face a dilemma: fast but inaccurate Poisson models or accurate but prohibitively slow sequential models. This paper breaks that compromise. We present a simulator that achieves both fidelity and speed by focusing on the critical, yet overlooked, component of simulation: the photon count statistics. Our key contribution is a Markov-renewal process (MRP) formulation that, for the first time, analytically predicts the mean and variance of registered photon counts under dead time. To make this MRP model computationally tractable, we introduce a spectral truncation rule that efficiently computes the complex covariance statistics. By proving the shift-invariance of the process, we extend this per-pixel model to full histogram cube generation via a precomputed lookup table. Our method generates 3D cubes indistinguishable from the sequential gold-standard, yet is orders of magnitude faster. This finally enables large-scale, physically-faithful data generation for learning-based SP-LiDAR reconstruction.

</details>


### [51] [Generalized Pinching-Antenna Systems: A Leaky-Coaxial-Cable Perspective](https://arxiv.org/abs/2512.04979)
*Kaidi Wang,Zhiguo Ding,Lajos Hanzo*

Main category: eess.SP

TL;DR: 提出了一种基于漏泄同轴电缆LCX的低频可重构 pinching 天线系统，用于6G下行链路，实现可控辐射槽并模拟两个阶段的传播，结合博弈论联合优化和凸优化功率分配以最大化吞吐量，结果显示相较于固定天线基准有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 把波导式 pinching 天线在高频场景中的优点扩展到低频应用，解决低频下行链路对LoS/NLoS环境的适应性、路径损耗以及灵活部署的问题。

Method: 提出使用带可控辐射槽的漏泄同轴电缆（LCX）实现下行通道的 pinching 天线。构建双阶段传播模型以描述引导传输与无线辐射在LoS与NLoS路径上的特性，并设计匹配的联合优化框架：通过博弈论实现链路选择/关联，以及通过凸优化实现功率分配，从而最大化系统吞吐量。

Result: 理论分析揭示局部增益显著且随距离快速衰减；仿真结果显示相比于传统固定天线基准，系统具备明显的吞吐量提升。

Conclusion: 提出的低频LCX实现的 pinching 天线概念与双阶段传播模型结合，提供可实际部署的6G相关伪装性与吞吐优化能力，显示出对灵活、可重新配置的天线架构的潜在价值。

Abstract: The evolution toward the sixth-generation (6G) wireless networks has flexible reconfigurable antenna architectures capable of adapting their radiation characteristics to the surrounding environment. At the center-stage, while waveguide based pinching antennas have been shown to beneficially ameliorate wireless propagation environments, their applications have remained confined to high-frequency scenarios. As a remedy, we propose a downlink generalized pinching-antenna system that adapts this compelling concept to low-frequency operation through a leaky-coaxial-cable (LCX) implementation. By endowing LCX structures with controllable radiation slots, the system inherits the key capabilities of waveguide based pinching antennas. Explicitly, these include reconfigurable line-of-sight (LoS) links, reduced path loss, and flexible deployment, while supporting a practical implementation of the pinching-antenna concept at low frequencies. A twin-stage propagation model is developed for characterizing both the guided transmission and wireless radiation encountered over LoS and non-line-of-sight (NLoS) paths. Analytical results reveal strong local gain, complemented by rapid distance-dependent decay. Hence, we conceive a matching joint optimization framework, which maximizes throughput by harnessing game-theoretic association and convex power allocation. Simulation results demonstrate substantial performance gains over conventional fixed-antenna benchmarks.

</details>


### [52] [Efficient Decoders for Sensing Subspace Code](https://arxiv.org/abs/2512.05028)
*Siva Aditya Gooty,Hessam Mahdavifar*

Main category: eess.SP

TL;DR: 提出对 sensing subspace codes 的高效解码算法，将复杂度从立方降至平方，并在仿真中渐近 MAP 性能。


<details>
  <summary>Details</summary>
Motivation: 在6G通信与感知的 JCAS 框架中，稀疏阵列的 DoA 估计与子空间编码有理论联系，构建更高效的感知子空间编码以提升 DoA 分辨率与系统性能。

Method: 提出新的解码算法，用以对 Bose-Chowla sensing subspace code 进行高效解码，将复杂度从 O(N^3) 降至 O(N^2)，并提供可调权衡的参数以折衷复杂度与误差性能。通过蒙特卡洛仿真在不同信噪比下评估性能。

Result: 在仿真中，新的解码算法的误差性能逐步逼近 MAP，当阵元数增加时，复杂度从平方提升至立方，性能表现接近 MAP。

Conclusion: 提出的解码方案在保持可控失效率的前提下显著降低复杂度，并提供了在 6G JCAS 场景下高效 DoA 估计的潜力。

Abstract: Sparse antenna array sensing of source/target via direction of arrival (DoA) estimation motivates design of the sensing framework in joint communication and sensing (JCAS) systems for sixth generation (6G) communication systems. Recently, it is established by Mahdavifar, Rajamäki, and Pal that array geometry of sparse arrays has fundamental connections with the design of subspace codes in coding theory. This was then utilized to design efficient \textit{sensing subspace codes} that estimate the DoA with good resolution. Specifically, the Bose-Chowla sensing subspace code provides near optimal code design for unique DoA estimation with tight theoretical upper bound on the error performance. However, the currently known decoder for these codes, to estimate the DoA, is a traditional \textit{Maximum-a-Posterior (MAP) decoder} with complexity that is cubic with the number of antennas. In this work, we propose novel efficient decoding algorithms for sensing subspace codes, that reduce the complexity down to quadratic while providing new knobs to tune in order to tradeoff complexity with error performance. The decoders are further evaluated for their performance via Monte Carlo simulations for a range of SNRs demonstrating promising performance that smoothly approaches the MAP performance as the complexity grows from quadratic to cubic in the number of antennas.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [53] [Making Cellular Networks Crisis-Proof: Towards Island-Ready, Resilient-By-Design 6G Communication Network](https://arxiv.org/abs/2512.04346)
*Leon Janzen,Matthias Hollick*

Main category: cs.NI

TL;DR: 提出面向6G的岛屿就绪、以本地为先的韧性设计，旨在灾害情境下通过本地化连接维持区域应用与服务，评估5G/5G-Advanced的易碎性并指出实现路径与挑战。


<details>
  <summary>Details</summary>
Motivation: 5G/5G-Advanced过度依赖中央核心网，灾害或攻击导致局部区域与核心网断连，危及紧急通信、广播、消息与新闻等关键服务；需要在本地实现连接与服务的可用性。

Method: 基于概念性分析和场景设想，提出岛屿就绪的6G愿景；采用全社会协作视角，分析需要哪些参与方并评估当前体系的岛屿就绪程度；提出去中心化核心网与本地优先架构的设计方向；梳理实现路径中的关键难点与研究问题。

Result: 给出一个概念性框架，描述本地化连接到区域应用服务器的设想，在现阶段不可实现；明确需要的系统要素与工作方向，如核心网去中心化、本地化应用架构、以及与应急/公共服务的协同。

Conclusion: 要实现岛屿就绪的6G，需要多方协作、去中心化核心网及本地化应用生态；未来需要解决的开放挑战包括核心网去中心化、本地化信令与路由、数据隐私与安全、应急通信标准、以及法规与运营模型等。

Abstract: 5G and 5G-Advanced cellular networks are vulnerable to regional outages resulting from disasters or targeted attacks. This fragility stems from the reliance on the central core network involved for most 5G connectivity use cases. Crisis-struck regions isolated from the cellular core network form islands, where crisis response is hindered by the unavailability of recovery-relevant services, such as emergency calls, cell broadcasts, messengers, and news apps. Our concept of island-ready, resilient-by-design 6G communication networks envisions local cellular connectivity allowing users to connect to regional application servers, which is currently impossible. In our conceptualization, we follow an all-society approach, as realizing island connectivity requires the cooperation of multiple actors, including users, operators, developers, providers, and authorities. We evaluate how island-ready 5G and 5G-Advanced systems are and outline the open challenges stakeholders must address for full island readiness, such as decentralizing the 6G core network and designing local-first application architectures.

</details>


### [54] [Vision and Causal Learning Based Channel Estimation for THz Communications](https://arxiv.org/abs/2512.04380)
*Kitae Kim,Yan Kyaw Tun,Md. Shirajum Munir,Chirsto Kurisummoottil Thomas,Walid Saad,Choong Seon Hong*

Main category: cs.NI

TL;DR: 提出一种基于视觉信息与因果推理的THz城市MIMO信道估计方法，在NLoS场景中显著优于传统方法，且具强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在6G THz通信中，城市环境中高损耗、障碍物和大气吸收导致信道估计困难，传统方法在复杂NLoS场景下表现不足，需利用环境信息提升估计。

Method: 将计算机视觉算法与变分因果动力学(VCD)相结合，利用实时城市场景图像分析物理因素对THz传播的影响，并据此预测信道。

Result: 仿真结果表明，该视觉-因果方法在多种动态城市场景下显著优于传统AI估计技术，信道预测的准确性最高可达传统方法的约两倍，且对未见环境具有更强泛化能力，尤在NLoS条件下通过建模反射与绕射等间接路径获得提升。

Conclusion: 基于视觉与因果推理的城市THz信道估计方法在NLoS场景下表现出色，提升了THz MIMO系统的可靠性与鲁棒性，并具备良好的泛化潜力。

Abstract: The use of terahertz (THz) communications with massive multiple input multiple output (MIMO) systems in 6G can potentially provide high data rates and low latency communications. However, accurate channel estimation in THz frequencies presents significant challenges due to factors such as high propagation losses, sensitivity to environmental obstructions, and strong atmospheric absorption. These challenges are par- ticularly pronounced in urban environments, where traditional channel estimation methods often fail to deliver reliable results, particularly in complex non-line-of-sight (NLoS) scenarios. This paper introduces a novel vision-based channel estimation tech- nique that integrates causal reasoning into urban THz communi- cation systems. The proposed method combines computer vision algorithms with variational causal dynamics (VCD) to analyze real-time images of the urban environment, allowing for a deeper understanding of the physical factors that influence THz signal propagation. By capturing the complex, dynamic interactions between physical objects (such as buildings, trees, and vehicles) and the transmitted signals, the model can predict the channel with up to twice the accuracy of conventional methods. This model improves estimation accuracy and demonstrates supe- rior generalization performance. Hence, it can provide reliable predictions even in previously unseen urban environments. The effectiveness of the proposed method is particularly evident in NLoS conditions, where it significantly outperforms traditional methods such as by accounting for indirect signal paths, such as reflections and diffractions. Simulation results confirm that the proposed vision-based approach surpasses conventional artificial intelligence (AI)-based estimation techniques in accuracy and robustness, showing a substantial improvement across various dynamic urban scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [ASCIIBench: Evaluating Language-Model-Based Understanding of Visually-Oriented Text](https://arxiv.org/abs/2512.04125)
*Kerry Luo,Michael Fu,Joshua Peguero,Husnain Malik,Anvay Patil,Joyce Lin,Megan Van Overborg,Ryan Sarmiento,Kevin Zhu*

Main category: cs.LG

TL;DR: ASCIIBench introduces a 5,315‑image ASCII-art benchmark and a fine‑tuned CLIP variant to assess generation and classification; it reveals a representation bottleneck in ASCII categories, not merely generation variance.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit emergent reasoning but struggle with precise spatial/positional reasoning. ASCII art, as a symbolic visual modality, provides a stress test to probe multimodal representations and benchmark generation/classification of structured text images.

Method: Construct a filtered dataset of 5,315 class-labeled ASCII images and release weights for a CLIP model adapted to capture ASCII structure. Evaluate LLM-generated ASCII art via cosine similarity in CLIP embedding space and analyze discriminability across categories.

Result: Cosine similarity over CLIP embeddings yields chance-level performance for most ASCII categories; high internal mean similarity classes show clear discriminability, indicating the bottleneck is representation rather than generational variance.

Conclusion: ASCII art serves as a stress test for multimodal embeddings and motivates new embedding methods or evaluation metrics tailored to symbolic visual modalities; resources are publicly available at the provided GitHub repository.

Abstract: Large language models (LLMs) have demonstrated several emergent behaviors with scale, including reasoning and fluency in long-form text generation. However, they continue to struggle with tasks requiring precise spatial and positional reasoning. ASCII art, a symbolic medium where characters encode structure and form, provides a unique probe of this limitation. We introduce ASCIIBench, a novel benchmark for evaluating both the generation and classification of ASCII-text images. ASCIIBench consists of a filtered dataset of 5,315 class-labeled ASCII images and is, to our knowledge, the first publicly available benchmark of its kind. Alongside the dataset, we release weights for a fine-tuned CLIP model adapted to capture ASCII structure, enabling the evaluation of LLM-generated ASCII art. Our analysis shows that cosine similarity over CLIP embeddings fails to separate most ASCII categories, yielding chance-level performance even for low-variance classes. In contrast, classes with high internal mean similarity exhibit clear discriminability, revealing that the bottleneck lies in representation rather than generational variance. These findings position ASCII art as a stress test for multimodal representations and motivate the development of new embedding methods or evaluation metrics tailored to symbolic visual modalities. All resources are available at https://github.com/ASCIIBench/ASCIIBench.

</details>


### [56] [Decoding Large Language Diffusion Models with Foreseeing Movement](https://arxiv.org/abs/2512.04135)
*Yichuan Mo,Quan Chen,Mingjie Li,Zeming Wei,Yisen Wang*

Main category: cs.LG

TL;DR: 提出 FDM 和 FDM-A 解码方法，用搜索式优化考虑局部和全局影响，提升 LLDM 的解码效率与可控性；FDM-A 在关键步骤聚焦深入探索，实验显示良好的效率-性能权衡和可扩展性。


<details>
  <summary>Details</summary>
Motivation: LLDM 的并行解码对解码顺序高度敏感，现有启发式多关注局部效应，忽视长期影响，缺乏全局视角的优化方法。

Method: 引入 Foreseeing Decoding Method (FDM)，利用一个基于搜索的策略在离散空间中同时考虑局部与全局的解码影响以实现有效优化。通过分析完整解码过程中的选词一致性，提出 FDM-A，通过将深度探索限制在被识别为探索与平衡的关键步骤来提升效率。

Result: 在多种基准和模型体系结构上的大量实验验证了 FDM 的可扩展性；FDM-A 在效率与性能之间实现更优的权衡。

Conclusion: 工作为 LLDM 的解码方法提供一个原理性步骤，可能推动更强大的解码方法的发展。

Abstract: Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance becomes highly sensitive to the decoding order of tokens. Existing heuristic methods, however, focus mainly on local effects while overlooking long-term impacts. To address this limitation, we propose the Foreseeing Decoding Method (FDM), a novel approach that integrates both local and global considerations to unlock the full potential, employing a search-based strategy to enable effective optimization in discrete spaces. Furthermore, by analyzing the consistency of chosen tokens in the full decoding process, we develop a variant, FDM with Acceleration (FDM-A), which restricts deep exploration to critical steps identified as the exploration and balance circumantences. Extensive experiments across diverse benchmarks and model architectures validate the scalability of FDM and demonstrate the superior efficiency-performance trade-off achieved by FDM-A. Our work might potentially provide a principled step toward more powerful decoding methods for LLDMs.

</details>


### [57] [MechDetect: Detecting Data-Dependent Errors](https://arxiv.org/abs/2512.04138)
*Philipp Jung,Nicholas Chandler,Sebastian Jäger,Felix Biessmann*

Main category: cs.LG

TL;DR: MechDetect提出一个基于错误掩码的简单算法，利用机器学习判定数据特征是否影响错误，从而揭示误差生成机制，且可扩展到其他错误类型。


<details>
  <summary>Details</summary>
Motivation: 数据质量监控领域普遍关注错误检测与变化量，但对错误是如何产生的机制研究不足。理解生成机制有助于追踪根源、定位修复点，提升数据清洗和维护效果。

Method: 给定一个表格数据集及其对应的错误掩码，算法通过训练机器学习模型来估计错误是否依赖数据特征，从而推断错误生成的机制。该思路借鉴缺失值统计研究的机制检测方法，并在此基础上扩展到其他错误类型，只要有可用的错误掩码。

Result: 在基准数据集上的实验表明MechDetect能够有效识别并区分依赖数据的错误生成机制，验证了该方法的可行性与有效性。

Conclusion: 该框架有助于理解并追踪数据错误的产生过程，便于定位与修复，且具备对其他错误类型的通用适用性，但前提是可获得准确的错误掩码。

Abstract: Data quality monitoring is a core challenge in modern information processing systems. While many approaches to detect data errors or shifts have been proposed, few studies investigate the mechanisms governing error generation. We argue that knowing how errors were generated can be key to tracing and fixing them. In this study, we build on existing work in the statistics literature on missing values and propose MechDetect, a simple algorithm to investigate error generation mechanisms. Given a tabular data set and a corresponding error mask, the algorithm estimates whether or not the errors depend on the data using machine learning models. Our work extends established approaches to detect mechanisms underlying missing values and can be readily applied to other error types, provided that an error mask is available. We demonstrate the effectiveness of MechDetect in experiments on established benchmark datasets.

</details>


### [58] [Network of Theseus (like the ship)](https://arxiv.org/abs/2512.04198)
*Vighnesh Subramaniam,Colin Conwell,Boris Katz,Andrei Barbu,Brian Cheung*

Main category: cs.LG

TL;DR: NoT introduces a progressive, part-by-part substitution method to transform a trained or untrained guide network into a different target architecture while preserving performance, enabling deployment-time architectural flexibility.


<details>
  <summary>Details</summary>
Motivation: Standard deep learning practice assumes the trained architecture's inductive biases persist during deployment, which restricts exploration of more efficient or differently-biased architectures. Decoupling optimization from deployment could unlock a broader design space.

Method: Iteratively replace components of the guide network with modules from the target architecture. At each stage, align representations via similarity metrics to preserve functionality. The method can convert architectures across very different families (e.g., CNN to MLP, GPT-2 to RNN).

Result: The approach largely preserves the guide network's performance despite substantial architectural changes, demonstrating that optimization need not be tightly coupled to deployment architecture.

Conclusion: Decoupling optimization from deployment expands viable inference-time architectures, enabling better accuracy-efficiency tradeoffs and more directed exploration of architectural design space.

Abstract: A standard assumption in deep learning is that the inductive bias introduced by a neural network architecture must persist from training through inference. The architecture you train with is the architecture you deploy. This assumption constrains the community from selecting architectures that may have desirable efficiency or design properties due to difficulties with optimization. We challenge this assumption with Network of Theseus (NoT), a method for progressively converting a trained, or even untrained, guide network architecture part-by-part into an entirely different target network architecture while preserving the performance of the guide network. At each stage, components in the guide network architecture are incrementally replaced with target architecture modules and aligned via representational similarity metrics. This procedure largely preserves the functionality of the guide network even under substantial architectural changes-for example, converting a convolutional network into a multilayer perceptron, or GPT-2 into a recurrent neural network. By decoupling optimization from deployment, NoT expands the space of viable inference-time architectures, opening opportunities for better accuracy-efficiency tradeoffs and enabling more directed exploration of the architectural design space.

</details>


### [59] [ActVAE: Modelling human activity schedules with a deep conditional generative approach](https://arxiv.org/abs/2512.04223)
*Fred Shone,Tim Hillel*

Main category: cs.LG

TL;DR: 提出一种条件变分自编码器（Conditional VAE）用于在给定输入标签（如年龄、就业状态等）的条件下，生成现实且多样的人类活动日程。与纯生成或纯条件模型相比，该方法更准确地捕捉随机性和多样性，并可在需求建模框架中实际部署。


<details>
  <summary>Details</summary>
Motivation: 建模人类活动日程的复杂性与多样性，传统方法难以同时考虑条件因素与随机性。需要一种可在不同标签下快速生成现实日程、且具有可部署性的数据驱动方法。

Method: 提出一个结构化潜在生成框架与条件建模的结合，即一种新的条件VAE架构，用于在潜在变量和条件标签之间建立联系。通过联合密度估计框架和若干案例研究进行评估，并将其与纯生成和纯条件的 baselines 进行对比。

Result: 该模型能够在给定输入标签时快速生成精确且现实的日程，并且在联合密度估计和案例研究中表现良好。与仅生成或仅条件的方法相比，显式建模复杂行为中的随机性具有显著优势，且具备实际数据和计算需求的可部署性。

Conclusion: 将条件性与生成性结合的深度学习方法在建模人类复杂且多样的行为方面具有显著优势，适合整合入新旧需求建模框架，提升对随机性和多样性的捕捉能力。

Abstract: Modelling the complexity and diversity of human activity scheduling behaviour is inherently challenging. We demonstrate a deep conditional-generative machine learning approach for the modelling of realistic activity schedules depending on input labels such as an individual's age, employment status, or other information relevant to their scheduling. We combine (i) a structured latent generative approach, with (ii) a conditional approach, through a novel Conditional VAE architecture. This allows for the rapid generation of precise and realistic schedules for different input labels. We extensively evaluate model capabilities using a joint density estimation framework and several case studies. We additionally show that our approach has practical data and computational requirements, and can be deployed within new and existing demand modelling frameworks. We evaluate the importance of generative capability more generally, by comparing our combined approach to (i) a purely generative model without conditionality, and (ii) a purely conditional model which outputs the most likely schedule given the input labels. This comparison highlights the usefulness of explicitly modelling the randomness of complex and diverse human behaviours using deep generative approaches.

</details>


### [60] [The Initialization Determines Whether In-Context Learning Is Gradient Descent](https://arxiv.org/abs/2512.04268)
*Shifeng Xie,Rui Yuan,Simone Rossi,Thomas Hannagan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions with zero-mean Gaussian priors and zero initialization for GD. However, subsequent studies have challenged this simplified view by highlighting its overly restrictive assumptions, demonstrating instead that under conditions such as multi-layer or nonlinear attention, self-attention performs optimization-like inference, akin to but distinct from GD. We investigate how multi-head LSA approximates GD under more realistic conditions specifically when incorporating non-zero Gaussian prior means in linear regression formulations of ICL. We first extend multi-head LSA embedding matrix by introducing an initial estimation of the query, referred to as the initial guess. We prove an upper bound on the number of heads needed for ICL linear regression setup. Our experiments confirm this result and further observe that a performance gap between one-step GD and multi-head LSA persists. To address this gap, we introduce yq-LSA, a simple generalization of single-head LSA with a trainable initial guess yq. We theoretically establish the capabilities of yq-LSA and provide experimental validation on linear regression tasks, thereby extending the theory that bridges ICL and GD. Finally, inspired by our findings in the case of linear regression, we consider widespread LLMs augmented with initial guess capabilities, and show that their performance is improved on a semantic similarity task.

</details>


### [61] [Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order](https://arxiv.org/abs/2512.04277)
*Prakhar Gupta,Vaibhav Gupta*

Main category: cs.LG

TL;DR: 在单一标量目标下的强化学习后训练可能忽略了解题顺序的结构信息；引入一个粗粒度的解题顺序信号并进行混合奖励，可以在不改变数据或模型的情况下显著提升求解准确率。


<details>
  <summary>Details</summary>
Motivation: 研究在后训练阶段仅用标量奖励优化是否会错过解题过程的结构信息，尤其是求解器的排序顺序对最终解法提出的引导作用，以及是否通过引入排序信号来改进鲁棒性和性能。

Method: 在数独任务中，先对 Transformer 进行标准微调并在随机解题顺序下继续训练；再使用组相对策略优化（GRPO）进行后训练，采用两类奖励：单元格准确度和一个解题顺序对齐的排序奖励。通过固定混合权重将两者合并，并用自举缩放使两项奖励在初始化时量级相当。

Result: 混合奖励通常优于仅使用单元格准确度的优化；最佳混合在测试集上显著超越仅在随机顺序上训练的模型，且接近在求解器顺序序列上训练的模型的性能。

Conclusion: 粗粒度的排序信号能够引导强化学习后训练朝向求解器排序轨迹，而无需修改监督数据或模型结构。

Abstract: Post-training with reinforcement learning (RL) typically optimizes a single scalar objective and ignores structure in how solutions are produced. We ask whether a scalar hint toward a canonical solver ordering, used only during RL post-training, improves performance even when fine-tuned on randomized solution sequences. On Sudoku, we train a Transformer with standard fine-tuning on randomized solving orders, then post-train it with Group Relative Policy Optimization (GRPO) with two rewards: cell accuracy and an ordering reward that increases when the model's emission order aligns with the solver order. To compare signals cleanly, we combine them via fixed mixtures and use a simple bootstrapped scaling to equalize component magnitudes at initialization. Mixed rewards generally outperform cell-only optimization--the best mixture yields substantially higher test accuracy than the fine-tuned-only model trained on random-order and approaches the fine-tuned-only model trained on solver-order sequences in accuracy. These results suggest that coarse ordering signals can steer RL post-training toward solver-order trajectories without modifying supervised data or architecture.

</details>


### [62] [GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers](https://arxiv.org/abs/2512.04296)
*Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: GRASP is a light-weight parameter-efficient fine-tuning (PEFT) framework that partitions token representations into K groups and learns shared per-group scale/shift, drastically reducing trainable parameters; StochGRASP extends GRASP with Gaussian perturbations to weights and a noise-aware loss to model hardware variability, improving robustness. Empirically, GRASP matches or surpasses LoRA/BitFit on GLUE and E2E NLG with far fewer trainable params, while StochGRASP consistently outperforms deterministic variants under noise, suggesting suitability for edge hardware.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational and memory burden of fine-tuning large pre-trained models while maintaining or improving task performance, and to address hardware-induced variability in edge/Ai hardware for robust deployment.

Method: - GRASP: divide D-dimensional token representations in selected layers into K groups (K << D); learn a shared scaling and shifting vector for each group to modulate activations, reducing trainable parameters. - StochGRASP: introduce Gaussian perturbations to pre-trained weights during training (probabilistic parameterization) and employ a noise-aware loss to account for hardware variability.

Result: GRASP achieves performance on GLUE (RoBERTa-base/large) and E2E NLG (GPT-2 Medium) that matches or exceeds established PEFT methods (e.g., LoRA, BitFit) with an order-of-magnitude fewer trainable parameters than LoRA/BitFit. StochGRASP consistently outperforms deterministic GRASP under various noise levels in edge-like scenarios.

Conclusion: Grouped activation sharing is an effective and parameter-efficient form of modulation for PEFT, enabling strong performance with far fewer trainable parameters. Introducing stochastic weight perturbations further enhances robustness to hardware noise, making the approach well-suited for resource-constrained and noisy deployment environments.

Abstract: Parameter-efficient fine-tuning (PEFT) provides a scalable alternative to full-model adaptation by updating only a small subset of parameters in large pre-trained models. We introduce GRASP - GRouped Activation Shared Parameterization - a lightweight PEFT framework that partitions the D-dimensional token representations of selected layers into K << D groups and learns a shared scaling and shifting vector for each group. This grouped modulation reduces the number of trainable parameters significantly while preserving the ability of the model to learn task-specific features. Building on this formulation, we further propose StochGRASP, which learns Gaussian distributions as perturbations to the pre-trained weights rather than deterministic values. This probabilistic parameterization along with a noise-aware loss function formulation enables modelling hardware-level variability in programmed weights and significantly improves robustness under non-ideal inference conditions-an important requirement for deployment on edge-based emerging AI hardware. Across GLUE (RoBERTa-base & RoBERTa-large) and E2E NLG (GPT-2 Medium), GRASP matches or exceeds the performance of established PEFT methods while achieving an order of magnitude reduction in trainable parameters compared to LoRA and BitFit. Under varying levels of noise, StochGRASP consistently outperforms deterministic variants, demonstrating its suitability for energy-efficient and noise-prone hardware platforms.

</details>


### [63] [Exploiting \texttt{ftrace}'s \texttt{function\_graph} Tracer Features for Machine Learning: A Case Study on Encryption Detection](https://arxiv.org/abs/2512.04590)
*Kenan Begovic,Abdulaziz Al-Ali,Qutaibah Malluhi*

Main category: cs.LG

TL;DR: 通过 Linux 内核 ftrace 的函数图追踪器提取基于图的特征，将系统调用跟踪用于机器学习的加密检测与程序识别等任务；实验在加密检测任务中达到 99.28% 的高精度，并在多标签分类任务中得到验证；提供了跟踪数据预处理与图特征提取的方法论，推动系统行为分析、异常检测与安全分析的 ML 应用。


<details>
  <summary>Details</summary>
Motivation: 弥合系统跟踪数据与机器学习之间的鸿沟，提升对系统行为的建模能力，以实现更高效的性能监控和安全分析；通过从跟踪数据中提取图结构特征，提升对加密活动和正在运行的程序的检测与识别能力。

Method: 利用 Linux 内核 ftrace 框架中的函数图追踪器收集函数调用跟踪；从跟踪数据中提取基于图的特征并进行预处理；在加密检测任务和多标签分类任务中应用多种学习算法进行评估。

Result: 在加密检测任务中实现 99.28% 的高准确率；在多标签分类实验中成功识别正在运行的程序，验证了图基特征的有效性与通用性；给出完整的跟踪数据预处理和特征提取流程。

Conclusion: 该工作将系统跟踪与机器学习紧密结合，显著推动了基于系统行为的 ML 应用的发展，尤其在性能监控、程序识别与异常检测方面具有潜在的实用价值。

Abstract: This paper proposes using the Linux kernel ftrace framework, particularly the function graph tracer, to generate informative system level data for machine learning (ML) applications. Experiments on a real world encryption detection task demonstrate the efficacy of the proposed features across several learning algorithms. The learner faces the problem of detecting encryption activities across a large dataset of files, using function call traces and graph based features. Empirical results highlight an outstanding accuracy of 99.28 on the task at hand, underscoring the efficacy of features derived from the function graph tracer. The results were further validated in an additional experiment targeting a multilabel classification problem, in which running programs were identified from trace data. This work provides comprehensive methodologies for preprocessing raw trace data and extracting graph based features, offering significant advancements in applying ML to system behavior analysis, program identification, and anomaly detection. By bridging the gap between system tracing and ML, this paper paves the way for innovative solutions in performance monitoring and security analytics.

</details>


### [64] [Data-regularized Reinforcement Learning for Diffusion Models at Scale](https://arxiv.org/abs/2512.04332)
*Haotian Ye,Kaiwen Zheng,Jiashu Xu,Puheng Li,Huayu Chen,Jiaqi Han,Sheng Liu,Qinsheng Zhang,Hanzi Mao,Zekun Hao,Prithvijit Chattopadhyay,Dinghao Yang,Liang Feng,Maosheng Liao,Junjie Bai,Ming-Yu Liu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: DDRL通过前向KL对齐策略到离线数据分布，解决扩散模型强化学习中的奖励操纵问题；在高分辨率视频生成任务上，显著提升奖励并提升人类偏好，同时为扩散后训练提供稳健、可扩展范式。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型与人类偏好对齐的强化学习容易被奖励操纵攻击，正则化的局限性导致惩罚信号不可靠，难以实现鲁棒的偏好对齐。需要一种能够鲁棒地将RL与扩散训练结合的方法。

Method: 提出Data-regularized Diffusion Reinforcement Learning (DDRL)框架，使用前向KL散度将策略锚定在离线数据分布上；理论上实现鲁棒、无偏地将RL与扩散训练结合；在实践中将奖励最大化与扩散损失最小化结合，形成简单但有效的训练流程。

Result: 经过超百万GPU小时的实验与一万次双盲人类评估，DDRL在高分辨率视频生成任务中显著提升奖励并缓解基线的奖励操纵，提高了人类偏好，建立了扩散后训练的鲁棒且可扩展范式。

Conclusion: DDRL为扩散模型的RL对齐提供鲁棒、无偏的集成框架，缓解奖励操纵问题，具备良好的可扩展性，适用于高分辨率视频等复杂任务的扩散后训练。

Abstract: Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.

</details>


### [65] [Distance Is All You Need: Radial Dispersion for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2512.04351)
*Manh Nguyen,Sunil Gupta,Hung Le*

Main category: cs.LG

TL;DR: 提出了径向离散度分数（RDS），一种简单、无参数、与模型无关的不确定性度量，基于嵌入空间的采样生成的径向离散度。可选的加权版本结合模型的逐词概率。在四个挑战性自由文本问答数据集和多种大模型上，RDS及其加权变体在错觉检测和答案选择上达到最先进的表现，且对采样数量和嵌入选择具有鲁棒性，同时支持逐样本评分以实现Best-of-N选择和置信度过滤。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在检测LLM不确定性方面过于复杂，依赖脆弱的语义聚类或内部状态。需要一种简单、稳健、可扩展且与模型无关的不确定性度量。

Method: 构建一个基于嵌入空间的径向离散度指标，衡量在对模型的多次采样中生成文本在嵌入空间的径向聚散程度。引入一个轻量级的带权变体，在可获得时结合模型对每个词的概率，以提升区分度。指标完全参数化，且对模型架构和嵌入选择不敏感。

Result: 在四个挑战性自由文本问答数据集和多种LLM上，RDS及其带权变体实现了对九个基线方法的超越，在错觉检测和答案选择任务中达到最新研究水平，且对样本量和嵌入选择具有鲁棒性和可扩展性。

Conclusion: RDS提供了一种简单、参数无关、完全模型无关的不确定性度量，能够按样本进行评分，支持Best-of-N选择和置信过滤，且在实际应用中表现出良好的鲁棒性和扩展性。

Abstract: Detecting when large language models (LLMs) are uncertain is critical for building reliable systems, yet existing methods are overly complicated, relying on brittle semantic clustering or internal states. We introduce \textbf{Radial Dispersion Score (RDS)}, a simple, parameter-free, fully model-agnostic uncertainty metric that measures the radial dispersion of sampled generations in embedding space. A lightweight probability-weighted variant further incorporates the model's own token probabilities when available, outperforming different nine strong baselines. Moroever, RDS naturally extends to per-sample scoring, enabling applications such as best-of-$N$ selection and confidence-based filtering. Across four challenging free-form QA datasets and multiple LLMs, our metrics achieve state-of-the-art hallucination detection and answer selection performance, while remaining robust and scalable with respect to sample size and embedding choice.

</details>


### [66] [SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction](https://arxiv.org/abs/2512.04354)
*April S. Liang,Fatemeh Amrollahi,Yixing Jiang,Conor K. Corbin,Grace Y. E. Kim,David Mui,Trevor Crowell,Aakash Acharya,Sreedevi Mony,Soumya Punnathanam,Jack McKeown,Margaret Smith,Steven Lin,Arnold Milstein,Kevin Schulman,Jason Hom,Michael A. Pfeffer,Tho D. Pham,David Svec,Weihan Chu,Lisa Shieh,Christopher Sharp,Stephen P. Ma,Jonathan H. Chen*

Main category: cs.LG

TL;DR: ML驱动的临床决策支持系统SmartAlert在多中心住院环境中通过预测稳定CBC结果，显著减少无效重复检测，且安全有效。


<details>
  <summary>Details</summary>
Motivation: 重复实验室检测造成患者负担和成本上升；现有教育或限制措施效果有限，需要更智能、可控的解决方案来降低不必要的重复CBC检测。

Method: 在两个医院、八个急性病区的9270次住院中进行随机对照试点，将SmartAlert嵌入电子健康记录，关注CBC检测；比较SmartAlert显示后52小时内CBC检测次数，评估安全性。结果与实施中的治理、界面设计及用户反馈等要点一并讨论。

Result: 52小时内CBC检测次数显著下降（1.54 vs 1.82，p<0.01），实现相对下降约15%，对二级安全结局无不良影响。

Conclusion: 基于机器学习的 CDS若辅以周密的实施和治理过程，能够在住院情境中就实验室检测提供精准指引，安全地减少不必要的重复检测。

Abstract: Repetitive laboratory testing unlikely to yield clinically useful information is a common practice that burdens patients and increases healthcare costs. Education and feedback interventions have limited success, while general test ordering restrictions and electronic alerts impede appropriate clinical care. We introduce and evaluate SmartAlert, a machine learning (ML)-driven clinical decision support (CDS) system integrated into the electronic health record that predicts stable laboratory results to reduce unnecessary repeat testing. This case study describes the implementation process, challenges, and lessons learned from deploying SmartAlert targeting complete blood count (CBC) utilization in a randomized controlled pilot across 9270 admissions in eight acute care units across two hospitals between August 15, 2024, and March 15, 2025. Results show significant decrease in number of CBC results within 52 hours of SmartAlert display (1.54 vs 1.82, p <0.01) without adverse effect on secondary safety outcomes, representing a 15% relative reduction in repetitive testing. Implementation lessons learned include interpretation of probabilistic model predictions in clinical contexts, stakeholder engagement to define acceptable model behavior, governance processes for deploying a complex model in a clinical environment, user interface design considerations, alignment with clinical operational priorities, and the value of qualitative feedback from end users. In conclusion, a machine learning-driven CDS system backed by a deliberate implementation and governance process can provide precision guidance on inpatient laboratory testing to safely reduce unnecessary repetitive testing.

</details>


### [67] [STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting](https://arxiv.org/abs/2512.04385)
*Nan Zhou,Weijie Hong,Huandong Wang,Jianfeng Zheng,Qiuhua Wang,Yali Song,Xiao-Ping Zhang,Yong Li,Xinlei Chen*

Main category: cs.LG

TL;DR: A physics-informed diffusion framework (STeP-Diff) using DeepONet to forecast fine-grained air pollution from incomplete mobile-sensor data, enforcing convection-diffusion physics to improve spatio-temporal predictions; validated with field deployments showing substantial accuracy gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Urban air quality management requires high-resolution forecasts, but data from non-dedicated mobile sensors are incomplete and temporally inconsistent. Incorporating physical laws can regularize learning and improve generalization.

Method: STeP-Diff combines DeepONet to model the spatial sequence of measurements with a PDE-informed diffusion process. A PDE-constrained regularization guides the denoising in the reverse diffusion process, ensuring convergence toward convection-diffusion dynamics. Training explores reverse-process patterns to handle missing/time-varying data. Field validation deployed 59 portable sensors across two cities for 14 days.

Result: Compared to the second-best algorithm, STeP-Diff achieves up to 89.12% reduction in MAE, 82.30% in RMSE, and 25.00% in MAPE, demonstrating strong capability to capture spatio-temporal dependencies in air pollution fields.

Conclusion: STeP-Diff provides physics-grounded, accurate spatio-temporal forecasts from incomplete mobile-sensor data, with promising potential for urban air quality monitoring and management.

Abstract: Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.

</details>


### [68] [Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems](https://arxiv.org/abs/2512.04476)
*Zehao Fan,Zhenyu Liu,Yunzhen Liu,Yayue Hou,Hadjer Benmeziane,Kaoutar El Maghraoui,Liu Liu*

Main category: cs.LG

TL;DR: Context-aware MoE inference using CXL-NDP to offload cold experts, dynamic GPU pinning of hot experts, and per-expert mixed-precision quantization, achieving high throughput with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) scale LLMs by increasing the number of experts, but the accompanying expert weights grow memory-bound; offloading weights to external memory introduces costly transfers. The paper seeks to convert parameter movement into cheaper activation movement via near-data processing and context-aware placement to accelerate decoding.

Method: Employ CXL-attached near-data processing (CXL-NDP) to host cold experts in external memory while keeping hot experts in GPU memory; use prefill-stage activation statistics to guide decoding-stage expert placement, pin hot experts to GPU HBM, and map the rest to CXL-NDP. Introduce context-aware mixed-precision quantization (1-4 bit per-expert) based on prefill to meet NDP compute limits. Overlap GPU and NDP execution to hide movement costs.

Result: Demonstrates up to 8.7× decoding throughput improvement over state-of-the-art methods with an average accuracy drop of only 0.13%. Evaluation conducted on a GPU-NDP system.

Conclusion: A context-aware, multi-device MoE inference framework combining adaptive quantization and smart data/compute placement can substantially increase decoding throughput while maintaining negligible accuracy loss.

Abstract: Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.

</details>


### [69] [Dual-Path Region-Guided Attention Network for Ground Reaction Force and Moment Regression](https://arxiv.org/abs/2512.05030)
*Xuan Li,Samuel Bello*

Main category: cs.LG

TL;DR: Dual-Path Region-Guided Attention Network for insole-based 3D GRF/GRM estimation; integrates anatomy-inspired spatial priors and temporal priors with a region-level attention mechanism, plus a complementary path for full sensor context; jointly trained and fused; outperforms CNN/CNN-LSTM baselines with 5.78% six-component NRMSE on an insole dataset and 1.42% vertical GRF on a public dataset.


<details>
  <summary>Details</summary>
Motivation: Accurate, noninvasive estimation of three-dimensional ground reaction forces and moments is essential for biomechanics and rehabilitation. Insole sensors offer a convenient data source, but extracting accurate GRF/GRM requires effective integration of spatial-temporal information and anatomical priors. The paper aims to improve estimation accuracy by leveraging a dual-path architecture that fuses region-guided attention with global context.

Method: A Dual-Path Region-Guided Attention Network consisting of: (1) a region-level attention path that incorporates anatomy-inspired spatial priors and temporal priors to focus on informative sensor regions; (2) a complementary path that captures context from the entire sensor field. The two paths are trained jointly, and their outputs are fused to predict the six-component GRF/GRM at each time point.

Result: The model outperforms strong baselines (CNN and CNN-LSTM) on two datasets, achieving the lowest six-component average NRMSE of 5.78% on the insole dataset and 1.42% for the vertical GRF on the public dataset.

Conclusion: The proposed dual-path, region-guided attention framework provides robust GRF/GRM estimation from insole data, outperforming traditional CNN-based architectures and demonstrating strong generalization across datasets.

Abstract: Accurate estimation of three-dimensional ground reaction forces and moments (GRFs/GRMs) is crucial for both biomechanics research and clinical rehabilitation evaluation. In this study, we focus on insole-based GRF/GRM estimation and further validate our approach on a public walking dataset. We propose a Dual-Path Region-Guided Attention Network that integrates anatomy-inspired spatial priors and temporal priors into a region-level attention mechanism, while a complementary path captures context from the full sensor field. The two paths are trained jointly and their outputs are combined to produce the final GRF/GRM predictions. Conclusions: Our model outperforms strong baseline models, including CNN and CNN-LSTM architectures on two datasets, achieving the lowest six-component average NRMSE of 5.78% on the insole dataset and 1.42% for the vertical ground reaction force on the public dataset. This demonstrates robust performance for ground reaction force and moment estimation.

</details>


### [70] [Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval](https://arxiv.org/abs/2512.04524)
*Tianle Hu,Weijun Lv,Na Han,Xiaozhao Fang,Jie Wen,Jiaxing Li,Guoxu Zhou*

Main category: cs.LG

TL;DR: PSCA 是一个两阶段的领域自适应检索框架，通过原型对齐实现类级语义一致性与互相近似的量化约束，解决现有方法在类级对齐、伪标签可靠性与几何信息利用、以及原始特征量化带来的问题。


<details>
  <summary>Details</summary>
Motivation: 现有领域自适应检索在类级语义对齐不足、过度追求样本级对齐、缺乏伪标签可靠性与几何引导、以及直接对原始特征进行量化导致哈希码质量下降等方面存在不足。

Method: 提出 Prototype-Based Semantic Consistency Alignment (PSCA)，分两阶段实现：第一阶段通过正交原型直接建立类级语义关联，最大化类间可分性并聚集同类样本；在原型学习过程中利用几何接近性为伪标签一致性对齐提供可靠性指示，采用自适应权重调整伪标签置信度；得到的成员矩阵和原型用于特征重构，确保对重构特征进行量化，从而提升哈希编码质量并连接两阶段。第二阶段通过领域特定的量化函数，在重构特征上的互相近似约束下，生成跨领域统一的二值哈希码。

Result: 大量实验表明，PSCA在多数据集上实现了优越的检索性能。

Conclusion: PSCA 通过结合类级语义对齐、伪标签可靠性与几何信息引导，以及重构后量化，提供了一种高效的领域自适应检索哈希方法，能在跨域场景中产生更一致且高质量的哈希编码。

Abstract: Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.

</details>


### [71] [Explainable Graph Representation Learning via Graph Pattern Analysis](https://arxiv.org/abs/2512.04530)
*Xudong Wang,Ziheng Sun,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: 提出PXGL-GNN框架，通过图模式分析实现可解释的图表示学习，将模式计数向量的可解释性与特征信息相结合，提供鲁棒性与泛化性理论分析，并在有监督/无监督任务中优于基线。


<details>
  <summary>Details</summary>
Motivation: 解释性在图表示学习中的不足：现有工作多聚焦于模型层面或实例级解释，缺乏对图表示层面所捕捉信息的清晰揭示。受图核思路启发，通过子结构模式计数来构建可解释的表示，并解决忽略节点特征和维度高的问题。

Method: 从图中采样不同模式的子结构，学习这些模式的表示；通过加权和将模式表示组合，权重表示各模式对最终图表示的贡献；提供理论分析（鲁棒性与泛化性）；在真实数据上通过模式分析学习并解释图表示，且与多组基线在监督与无监督任务上比较。

Result: 实验展示了在真实数据上通过模式分析学习与解释图表示的可行性，并在有监督和无监督任务中对比多基线表现出优势，验证了方法的有效性。

Conclusion: 通过图模式分析实现表示级别的可解释图表示学习，揭示图结构信息的具体形式，具备鲁棒性与泛化性的理论支撑，且在实际任务中具有良好的解释性和性能。

Abstract: Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.

</details>


### [72] [On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference](https://arxiv.org/abs/2512.04558)
*Yue Yu,Qiwei Di,Quanquan Gu,Dongruo Zhou*

Main category: cs.LG

TL;DR: Reward-filtered sequential inference (RFSI) 提升测试时计算（TTC）的效率与性能，克服 BoN 的子最优并提供更强的理论保障。


<details>
  <summary>Details</summary>
Motivation: 揭示 LLM TTC 方法的基本极限，寻找比传统的 BoN、顺序修正更接近最优的前沿方法。

Method: 提出混合参考策略模型，分析 BoN 的局限，提出 RFSI，使仅纳入高奖励生成进入上下文；给出严格的理论保证并在多个基准上评估。

Result: 理论上，RFSI 提供比标准 TTC 更强的保证；实验上，在多样基准上持续优于广泛采用的方法，验证其实用性。

Conclusion: 通过聚焦于高质量候选，RFSI 将计算资源更有效地分配到更优解上，显著推进 TTC 的理论和实践前沿。

Abstract: Test-time compute (TTC) has become an increasingly prominent paradigm for enhancing large language models (LLMs). Despite the empirical success of methods such as best-of-$n$ (BoN) sampling and sequential revision, their fundamental limits remain unclear. We address this gap by analyzing a mixture-of-reference policy model and proving that standard BoN is inherently suboptimal. To move closer to the optimal frontier, we study reward-filtered sequential inference, a simple procedure that selectively incorporates only high-reward generations into the context. This mechanism concentrates computation on superior policy candidates and suppresses inferior ones. On the theoretical side, we show that reward-filtered sequential inference yields strictly stronger guarantees than standard TTC paradigms. On the empirical side, we evaluate such an inference strategy across diverse benchmarks and observe consistent improvements over widely used approaches, demonstrating the practical effectiveness of our framework.

</details>


### [73] [Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function](https://arxiv.org/abs/2512.04559)
*Hyeongyu Kang,Jaewoo Lee,Woocheol Shin,Kiyoung Om,Jinkyoo Park*

Main category: cs.LG

TL;DR: 提出Soft Q-based Diffusion Finetuning (SQDF)，通过KL正则化的强化学习对扩散模型进行对齐训练，利用训练-free但可微的软Q估计，辅以折扣因子、一致性模型和离线回放以提高模式覆盖与奖励–多样性权衡；实验显示在文本到图像对齐中实现更高目标奖励且保持多样性，在在线黑盒优化中具备高样本效率并维持自然性与多样性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高概率样本的同时需要对齐下游目标；现有微调易导致奖励过度优化，产生高奖励但不自然且多样性下降，需一种稳定且高效的对齐方法。

Method: 提出Soft Q-based Diffusion Finetuning（SQDF），采用KL正则化的RL对扩散对齐进行训练；使用一个训练无但可微的软Q函数字估计来进行再参数化策略梯度更新。三大创新：引入折扣因子以在去噪过程中的正确归因；将一致性模型用于 refinement Q 函数估计；使用离线回放缓冲提升模式覆盖并平衡奖励-多样性。

Result: 实验表明SQDF在文本到图像对齐任务中获得更高的目标奖励，同时保持多样性；在在线黑盒优化场景下具有较高的样本效率，同时保持自然性与多样性。

Conclusion: SQDF有效缓解奖励过度优化的问题，同时提升对齐质量与样本效率，并在高多样性场景中维持自然感。

Abstract: Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.

</details>


### [74] [LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models](https://arxiv.org/abs/2512.04562)
*Siddharth Betala,Samuel P. Gleason,Ali Ramlaoui,Andy Xu,Georgia Channing,Daniel Levy,Clémentine Fourrier,Nikita Kazeev,Chaitanya K. Joshi,Sékou-Oumar Kaba,Félix Therrien,Alex Hernandez-Garcia,Rocío Mercado,N. M. Anoop Krishnan,Alexandre Duval*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.

</details>


### [75] [Reliable Statistical Guarantees for Conformal Predictors with Small Datasets](https://arxiv.org/abs/2512.04566)
*Miguel Sánchez-Domínguez,Lucas Lacasa,Javier de Vicente,Gonzalo Rubio,Eusebio Valero*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Surrogate models (including deep neural networks and other machine learning algorithms in supervised learning) are capable of approximating arbitrarily complex, high-dimensional input-output problems in science and engineering, but require a thorough data-agnostic uncertainty quantification analysis before these can be deployed for any safety-critical application. The standard approach for data-agnostic uncertainty quantification is to use conformal prediction (CP), a well-established framework to build uncertainty models with proven statistical guarantees that do not assume any shape for the error distribution of the surrogate model. However, since the classic statistical guarantee offered by CP is given in terms of bounds for the marginal coverage, for small calibration set sizes (which are frequent in realistic surrogate modelling that aims to quantify error at different regions), the potentially strong dispersion of the coverage distribution around its average negatively impacts the reliability of the uncertainty model, often obtaining coverages below the expected value, resulting in a less applicable framework. After providing a gentle presentation of uncertainty quantification for surrogate models for machine learning practitioners, in this paper we bridge the gap by proposing a new statistical guarantee that offers probabilistic information for the coverage of a single conformal predictor. We show that the proposed framework converges to the standard solution offered by CP for large calibration set sizes and, unlike the classic guarantee, still offers reliable information about the coverage of a conformal predictor for small data sizes. We illustrate and validate the methodology in a suite of examples, and implement an open access software solution that can be used alongside common conformal prediction libraries to obtain uncertainty models that fulfil the new guarantee.

</details>


### [76] [Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift](https://arxiv.org/abs/2512.04571)
*Aditi Naiknaware,Sanchit Singh,Hajar Homayouni,Salimeh Sekeh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.

</details>


### [77] [QoSDiff: An Implicit Topological Embedding Learning Framework Leveraging Denoising Diffusion and Adversarial Attention for Robust QoS Prediction](https://arxiv.org/abs/2512.04596)
*Guanchen Du,Jianlong Xu,Wei Wei*

Main category: cs.LG

TL;DR: QoSDiff通过扩散模型实现无图学习的QoS预测，并引入对抗交互模块与双向混合注意力，以捕捉高阶关系，显著优于基线且具备良好的跨数据集泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有QoS预测多依赖显式的用户–服务交互图，造成可扩展性瓶颈且在连接稀疏或受噪声干扰时性能下降；需要一种无需显式图且能鲁棒地捕获高阶关系的新框架.

Method: 提出QoSDiff：使用去噪扩散概率模型从噪声初始化中恢复潜在结构；再通过对抗交互模块结合双向混合注意力机制，动态筛选信息模式，实现对用户–服务关联的双视角建模。

Result: 在两个大规模真实数据集上，QoSDiff显著优于SOTA基线；展现出良好的跨数据集泛化能力，以及对数据稀疏性与观测噪声的鲁棒性。

Conclusion: QoSDiff实现了无图QoS预测的新范式，具备良好可扩展性、鲁棒性与泛化性，能有效提升服务质量预测的效果。

Abstract: Accurate Quality of Service (QoS) prediction is fundamental to service computing, providing essential data-driven guidance for service selection and ensuring superior user experiences. However, prevalent approaches, particularly Graph Neural Networks (GNNs), heavily rely on constructing explicit user--service interaction graphs. This dependency introduces severe scalability bottlenecks and limits performance when explicit connections are sparse or corrupted by noise. To address these challenges, this paper introduces \emph{QoSDiff}, a novel embedding learning framework that bypasses the prerequisite of explicit graph construction. Specifically, it leverages a denoising diffusion probabilistic model to recover intrinsic latent structures from noisy initializations. To further capture high-order interactions, we propose an adversarial interaction module that integrates a bidirectional hybrid attention mechanism. This adversarial paradigm dynamically distinguishes informative patterns from noise, enabling a dual-perspective modeling of intricate user--service associations. Extensive experiments on two large-scale real-world datasets demonstrate that QoSDiff significantly outperforms state-of-the-art baselines. Notably, the results highlight the framework's superior cross-dataset generalization capability and exceptional robustness against data sparsity and observational noise.

</details>


### [78] [Score Matching for Estimating Finite Point Processes](https://arxiv.org/abs/2512.04617)
*Haoqun Cao,Yixuan Zhang,Feng Zhou*

Main category: cs.LG

TL;DR: 针对有限点过程的分数匹配提出严格框架，并给出自回归加权分数匹配与生存-分类扩充，实验显示与MLE相当且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有点过程的分数匹配缺乏对有限点过程的严格分析，导致在有限域上的归一化与识别问题。需要在有限点过程上建立理论框架并提供可训练的无积分目标。

Method: 通过 Janossy 测度建立有限点过程的分数匹配形式；提出自回归加权分数匹配估计量，并在参数设定下分析其统计性质；对非参数模型揭示不能唯一识别真分布的原因，提出生存-分类扩充以获得完整、无积分的训练目标。

Result: 在仿真与真实数据的时空数据集上，方法能准确重建强度并达到与MLE相当的性能，同时具有更高的计算效率。

Conclusion: 将分数匹配引入有限点过程的理论框架，解决非识别和归一化问题；提出的扩充方法实现了对任意强度基础模型的完整训练目标，并在实验中证实了其有效性。

Abstract: Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estimators for point processes, this work highlights the limitations of these existing methods, which stem primarily from the lack of a mathematically rigorous analysis of how score matching behaves on finite point processes -- special random configurations on bounded spaces where many of the usual assumptions and properties of score matching no longer hold. To this end, we develop a formal framework for score matching on finite point processes via Janossy measures and, within this framework, introduce an (autoregressive) weighted score-matching estimator, whose statistical properties we analyze in classical parametric settings. For general nonparametric (e.g., deep) point process models, we show that score matching alone does not uniquely identify the ground-truth distribution due to subtle normalization issues, and we propose a simple survival-classification augmentation that yields a complete, integration-free training objective for any intensity-based point process model for spatio-temporal case. Experiments on synthetic and real-world temporal and spatio-temporal datasets, demonstrate that our method accurately recovers intensities and achieves performance comparable to MLE with better efficiency.

</details>


### [79] [Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective](https://arxiv.org/abs/2512.04625)
*Bowen Zheng,Ran Cheng*

Main category: cs.LG

TL;DR: 提出通用解耦知识蒸馏GDKD，通过对预测分布的重新理解，提高对顶位与非顶 logits 的解耦与蒸馏效果，并提出高效分区策略，在多数据集上优于DKD与其他方法，且开源代码。


<details>
  <summary>Details</summary>
Motivation: 重新审视DKD的机制，利用预测分布视角深入理解顶位和非顶 logits 的关系，解决多模态教师分布的挑战，提升知识蒸馏效果与效率。

Method: 提出Generalized Decoupled Knowledge Distillation (GDKD)损失，对 logits 进行更灵活的解耦；分析教师预测分布对GDKD梯度的影响，提出两条关键洞见；基于分区策略设计简化的GDKD算法以处理教师分布的多模态性；在CIFAR-100、ImageNet、Tiny-ImageNet、CUB-200-2011、Cityscapes等数据集上进行大规模实验并给出代码。

Result: GDKD在多个基准上优于原始DKD及其他流行蒸馏方法，提升性能并具备更好的鲁棒性与效率。

Conclusion: 从预测分布角度对知识蒸馏的解耦形式进行扩展，顶位分区和非顶 logits 的聚焦提供了关键的改进点，所提出的分区策略有效应对教师分布的多模态性，推动KD研究向更通用和高效方向发展。

Abstract: In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.

</details>


### [80] [Federated Learning for Anomaly Detection in Maritime Movement Data](https://arxiv.org/abs/2512.04635)
*Anita Graser,Axel Weißenfeld,Clemens Heistracher,Melitta Dragaschnig,Peter Widhalm*

Main category: cs.LG

TL;DR: 提出 M3fed，一种用于移动异常检测的联邦学习新解，能提升数据隐私并降低通信成本；通过海事 AIS 数据的示例实验来评估，与集中式 M3 的对比用于评估模型质量与通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决在移动异常检测任务中对数据隐私的需求，以及在分布式数据场景中降低通信成本的挑战；在海上环境中应用FL以实现更高的数据利用率与隐私保护。

Method: 设计了用于训练 M3fed 的新型联邦学习策略，使用海事 AIS 数据进行示例实验，并通过与经典集中式 M3 的对比，评估通信成本与 FL 模型质量。

Result: 通过与集中式 M3 的对比，评估显示在通信成本与模型质量方面对比分析的结果用于衡量 M3fed 的性能，但具体数值在摘要中未给出，因此结论性结果待实验数据确认。

Conclusion: M3fed 展现出在隐私保护与通信效率方面的潜力，并可在移动异常检测任务中与集中式方法竞争；未来工作包括进一步完善 FL 策略、扩展至更多数据源及大规模部署的可扩展性评估。

Abstract: This paper introduces M3fed, a novel solution for federated learning of movement anomaly detection models. This innovation has the potential to improve data privacy and reduce communication costs in machine learning for movement anomaly detection. We present the novel federated learning (FL) strategies employed to train M3fed, perform an example experiment with maritime AIS data, and evaluate the results with respect to communication costs and FL model quality by comparing classic centralized M3 and the new federated M3fed.

</details>


### [81] [Contract-Governed Training for Earth Observation: Observed Service Agreement Graphs and Coverage-Accuracy Trade-offs](https://arxiv.org/abs/2512.04644)
*Wenzhang Du*

Main category: cs.LG

TL;DR: OSAG通过契约治理的地球观测模型训练，将样本分组为服务契约并设定目标覆盖份额；通过契约归一化采样权重、以及可选的混合系数alpha和契约正则化权重lambda_C，在训练中实现对覆盖与准确性的显式治理。理论与 toy 场景给出覆盖收敛与服务风险边界；在 AVIRIS、EuroSAT 数据集上实验表明能降低优先覆盖误差、保持全局精度并提升高优先级准确性，粗/细契约设计展示了治理成本的降低与效率提升。


<details>
  <summary>Details</summary>
Motivation: 当前 EO 模型多在全局准确性框架下训练，缺乏对地区、类别或关键子群的显式服务保障，导致某些高优先区域或类别可能被忽视。需引入契约化、可解释的治理机制以确保在训练过程中的覆盖与权益。

Method: 提出 Observed Service Agreement Graph（OSAG）作为轻量化治理层，包含服务契约、目标服务份额、契约归一化采样权重，以及可选的采样混合系数alpha和契约正则化权重lambda_C。理论上证明：OSAG 采样会向目标覆盖收敛，覆盖偏差可上界地约束服务风险；契约设计的越精细，治理成本越低。通过 toy 场景的简化理论和对 AVIRIS（Indian Pines、Salinas）与 Sentinel-2 EuroSAT 的实验验证，比较 coarse 与 fine 合同对治理效果与成本的影响。

Result: OSAG 能显著降低优先覆盖误差，同时在保持全局准确度的前提下提升高优先级准确性；理论分析显示覆盖聚焦于目标、覆盖偏差与服务风险之间存在界限，粗/细契约的设计影响治理成本与效果，欧盟 EuroSAT 的 coarse-vs-fine 实验进一步证实语义更细的契约能以更低的单位治理成本实现更高的准确性增益。

Conclusion: OSAG 提供明确的准确性-治理权衡钩子，面向语义化的契约设计能够在提升治理效率的同时减少对总体准确性的代价。

Abstract: Earth observation (EO) models are frequently trained under implicit sampling policies that optimize global accuracy but provide no explicit guarantees on who (which regions, classes, or mission-critical strata) is being served throughout training. This paper introduces a contract-governed training paradigm for EO in which training samples are grouped into service contracts -- semantically meaningful units such as (dataset, region, rare-crop indicator) -- and each contract is assigned a target service share. We instantiate this paradigm as an Observed Service Agreement Graph (OSAG), a lightweight governance layer that (i) monitors contract-level exposure (coverage) during optimization, (ii) drives empirical coverage toward target shares via contract-normalized sampling weights, and (iii) exposes explicit accuracy-governance trade-offs through two knobs: a sampling mixture coefficient alpha and a contract-regularization weight lambda_C. We provide a compact theory in a toy setting: OSAG sampling concentrates empirical coverage to targets; coverage deviations upper-bound service-risk deviations; and contract design (coarse vs. fine) modulates governance cost. Experiments on AVIRIS hyperspectral scenes (Indian Pines plus Salinas) and multispectral Sentinel-2 EuroSAT demonstrate that OSAG can substantially reduce priority coverage error while maintaining global accuracy and improving high-priority accuracy. A EuroSAT coarse-vs-fine contract ablation further evidences how semantically refined contracts can reduce the accuracy cost per unit of governance improvement.

</details>


### [82] [TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation](https://arxiv.org/abs/2512.04694)
*Baris Yilmaz,Bevan Deniz Cilgin,Erdem Akagündüz,Salih Tileylioglu*

Main category: cs.LG

TL;DR: TimesNet-Gen: a time-domain conditional generator for station-specific strong-motion synthesis using a station latent bottleneck, achieving strong station-wise alignment via HVSR and f0 distributions, outperforming a spectrogram-based conditional VAE baseline; code released.


<details>
  <summary>Details</summary>
Motivation: Accurate site-specific ground-motion models are essential for effective earthquake risk reduction. Data-driven approaches that extract site-controlled signatures from recorded motions can capture local site effects.

Method: A time-domain conditional generator (TimesNet-Gen) with a station-specific latent bottleneck. Evaluation uses HVSR curves and fundamental site-frequency f0 distribution per station, plus a confusion-matrix-based station specificity score. Compared against a spectrogram-based conditional VAE baseline.

Result: TimesNet-Gen achieves strong station-wise alignment and compares favorably to the conditional VAE baseline in site-specific strong motion synthesis.

Conclusion: Demonstrates the feasibility of time-domain, station-conditioned generation of strong ground motions and provides code for replication.

Abstract: Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency $f_0$ distributions between real and generated records per station, and summarize station specificity with a score based on the $f_0$ distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.

</details>


### [83] [TRINITY: An Evolved LLM Coordinator](https://arxiv.org/abs/2512.04695)
*Jinglue Xu,Qi Sun,Peter Schwendeman,Stefan Nielsen,Edoardo Cetin,Yujin Tang*

Main category: cs.LG

TL;DR: Trinity通过一个轻量级协调器实现跨模态模型协作，利用将近0.6B参数的小型语言模型和约10K参数的头部，采用一种进化策略来高效分配任务角色（Thinker/Worker/Verifier），在多轮对话中提升各类任务表现，达到标准基准的SOTA并对OOD具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决权重合并在不同架构和封闭API下的局限性，避免对齐或再训练的成本，提供一种可扩展的多模型协作框架。

Method: 设计一个由极小型语言模型和极简头部组成的协调器，通过进化策略对协调器进行优化；在多轮查询中，协调器将LLMs分配为Thinker、Worker、Verifier三种角色，以分工协作完成复杂技能获取，减少协调器本体的学习负担。

Result: Trinity在编码、数学、推理和领域知识任务上持续优于单一模型与现有方法，并具备对分布外任务的稳健泛化；在LiveCodeBench等标准基准上达到SOTA，如86.2%的得分。

Conclusion: 隐藏状态表征为输入提供丰富上下文；在高维与严格预算下，分离型Covariance Matrix Adaptation Evolution Strategy（分离型 CMA-ES）相较强化学习、模仿学习和随机搜索在利用潜在的块状可分性方面具有优势，从而提升总体性能与效率。

Abstract: Combining diverse foundation models is promising, but weight-merging is limited by mismatched architectures and closed APIs. Trinity addresses this with a lightweight coordinator that orchestrates collaboration among large language models (LLMs). The coordinator, comprising a compact language model (approximately $0.6$B parameters) and a lightweight head (approximately $10$K parameters), is optimized with an evolutionary strategy for efficient and adaptive delegation. Trinity processes queries over multiple turns, where at each turn the coordinator assigns one of three roles (Thinker, Worker, or Verifier) to a selected LLM, effectively offloading complex skill acquisition from the coordinator itself. Experiments show that Trinity consistently outperforms individual models and existing methods across coding, math, reasoning, and domain knowledge tasks, and generalizes robustly to out-of-distribution tasks. On standard benchmarks, Trinity achieves state-of-the-art results, including a score of 86.2% on LiveCodeBench. Theoretical and empirical analyses identify two main factors behind this performance: (1) the coordinator's hidden-state representations provide rich contextualization of inputs, and (2) under high dimensionality and strict budget constraints, the separable Covariance Matrix Adaptation Evolution Strategy offers advantages over reinforcement learning, imitation learning, and random search by exploiting potential block-epsilon-separability.

</details>


### [84] [Towards Continuous-Time Approximations for Stochastic Gradient Descent without Replacement](https://arxiv.org/abs/2512.04703)
*Stefan Perko*

Main category: cs.LG

TL;DR: 提出一个基于Young积分的随机连续时间近似来刻画SGDo（逐块无放回的随机梯度下降），并给出强凸目标在学习率u_t=1/(1+t)^β（β∈(0,1)）下的几乎必然收敛性及渐近收敛速率界限，与现有SGDo结果相比不劣。


<details>
  <summary>Details</summary>
Motivation: 尽管SGDo在实际应用中表现良好，但其理论研究相较于带放回和单遍（one-pass）方法仍较薄弱。需要一种严格的连续时间框架来理解SGDo的动力学与收敛性，并弥合无放回与传统随机梯度方法之间的理论空缺。

Method: 构造以Young积分驱动的随机微分方程近似，噪声来自一种称为“分时期布朗运动”的过程，捕捉SGDo的离散性与无放回特性；在强凸目标和学习率形式u_t=(1+t)^(-β)的条件下，分析该系统的几乎必然收敛性，并推导渐近收敛速率的上界。

Result: 证明了上述连续时间近似在强凸目标下对所设学习率的情形具有几乎必然收敛性；并给出渐近收敛速率的上界，该上界与现有的SGDo结果相当甚至更优。

Conclusion: 该连续时间近似为理解SGDo及其无放回版本提供了有力的理论工具，表明利用Young积分的随机微分方程框架可以获得可靠的收敛性与速率界限，具有推广潜力。

Abstract: Gradient optimization algorithms using epochs, that is those based on stochastic gradient descent without replacement (SGDo), are predominantly used to train machine learning models in practice. However, the mathematical theory of SGDo and related algorithms remain underexplored compared to their "with replacement" and "one-pass" counterparts. In this article, we propose a stochastic, continuous-time approximation to SGDo with additive noise based on a Young differential equation driven by a stochastic process we call an "epoched Brownian motion". We show its usefulness by proving the almost sure convergence of the continuous-time approximation for strongly convex objectives and learning rate schedules of the form $u_t = \frac{1}{(1+t)^β}, β\in (0,1)$. Moreover, we compute an upper bound on the asymptotic rate of almost sure convergence, which is as good or better than previous results for SGDo.

</details>


### [85] [RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting](https://arxiv.org/abs/2512.04752)
*Siqi Wang,Hailong Yang,Junjie Zhu,Xuezhu Wang,Yufan Xu,Depei Qian*

Main category: cs.LG

TL;DR: 提出 RLHFSpec，通过自适应推测解码、工作负载感知的 drafting 策略和样本重新分配，显著提升 RLHF 的生成阶段吞吐量并带来整体训练加速。


<details>
  <summary>Details</summary>
Motivation: RLHF 的生成阶段是整个流程的瓶颈，需要加速以提升整体验证。

Method: 将 speculative decoding 融入 RLHF 生成阶段，提出基于工作负载的 drafting 策略选择机制，联合考虑验证成本与接受 token 数，同时提出样本重新分配和高效的迁移机制以充分利用 GPU 资源。

Result: 在生成阶段实现比现有方法更高的吞吐量；由于缓解瓶颈，整个 RLHF 执行也获得显著加速。

Conclusion: 通过在生成阶段引入自适应推测解码和资源调度，RLHFSpec 能有效提升 RLHF 的性能与效率。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is an important fine-tuning technique for large language models (LLMs) and comprises three stages: generation, inference, and training. The generation stage generates samples that are then used to infer learnable experiences for training. We observe that the generation stage is the bottleneck of the entire execution process and consider it a key point for optimization. Specifically, we realize the first attempt to integrate speculative decoding into the RLHF generation stage and propose RLHFSpec, an RLHF system that accelerates generation execution with adaptive speculative decoding and sample reallocation. To fully exploit the performance potential provided by speculative decoding, especially dealing with the dynamic workload of the generation stage, RLHFSpec proposes a workload-aware drafting strategy selection mechanism, which selects the near-optimal strategy by jointly considering the verification cost and the number of accepted tokens. Moreover, RLHFSpec also proposes sample reallocation to fully utilize the GPU resources, and optimizes it with an efficient sample migration mechanism. The experimental results show that the RLHFSpec can achieve higher throughput in the generation stage compared to state-of-the-art works. Moreover, due to the effective alleviation of the generation bottleneck, RLHFSpec also shows significant performance speedup in the entire RLHF execution.

</details>


### [86] [MemLoRA: Distilling Expert Adapters for On-Device Memory Systems](https://arxiv.org/abs/2512.04763)
*Massimo Bini,Ondrej Bohdal,Umberto Michieli,Zeynep Akata,Mete Ozay,Taha Ceritli*

Main category: cs.LG

TL;DR: 提出 MemLoRA 和 MemLoRA-V，一种在本地设备部署的记忆增强框架。通过为小型语言模型设计专用记忆适配器，分别完成知识提取、记忆更新和记忆增强生成等记忆操作，并在视觉扩展中结合小型视觉-语言模型实现原生的视觉理解。


<details>
  <summary>Details</summary>
Motivation: 在本地设备上实现隐私友好、低成本且高效的记忆增强对话系统；解决现有基于大规模语言模型的记忆系统在本地部署成本高、缺乏视觉能力、且对设备要求高的问题；利用小型模型和专用记忆适配器实现高性能的记忆操作与多模态能力。

Method: 提出 MemLoRA：为小型语言模型设计专用内存适配器，分步训练以实现知识提取、记忆更新、记忆增强生成等记忆操作，并通过知识蒸馏等原则实现高效微调。 extending 这一思路，MemLoRA-V 将视觉语言能力引入，通过集成小型视觉-语言模型实现原生视觉理解。

Result: 文本任务方面，MemLoRA 的性能优于 LoCoMo 基线中规模大10倍的模型（如 Gemma2-27B），并在 LoCoMo 基准上达到相当于规模大60倍模型（如 GPT-OSS-120B）的表现。视觉任务方面，将 MemLoRA 与视觉语言模型结合的 MemLoRA-V 在视觉问答任务中显著优于基于 caption 的方法（81.3 对比 23.7 的准确率），同时在文本任务上保持较强性能。

Conclusion: MemLoRA 与 MemLoRA-V 展现了在边缘设备上实现高效、隐私友好的记忆操作的可行性，并通过多模态扩展证明了在视觉推理场景中的有效性与潜力。

Abstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.

</details>


### [87] [Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows](https://arxiv.org/abs/2512.04954)
*Rajneil Baruah*

Main category: cs.LG

TL;DR: A novel amortized posterior estimation method using Normalizing Flows trained with likelihood-weighted importance sampling for efficient inference in high-dimensional inverse problems; it reveals that the topology of base distributions critically affects multimodal posteriors and shows that initializing with a Gaussian Mixture Model matching the number of target modes improves reconstruction fidelity.


<details>
  <summary>Details</summary>
Motivation: To enable fast, accurate posterior inference in complex, high-dimensional inverse problems where traditional posterior sampling is expensive, and to understand how base distribution topology affects the ability of Normalizing Flows to capture multimodal posteriors.

Method: Train Normalizing Flows using likelihood-weighted importance sampling to amortize posterior estimation without requiring posterior training samples. Evaluate on multi-modal 2D and 3D benchmark tasks. Investigate the impact of base distribution topology and mitigate issues by initializing the flow with a Gaussian Mixture Model that matches the target modal cardinality.

Result: Unimodal base distributions fail to capture disconnected posterior supports, yielding spurious bridges between modes. Initializing the flow with a Gaussian Mixture Model that matches the number of target modes significantly improves reconstruction fidelity, as quantified by standard distance/divergence metrics on the benchmarks.

Conclusion: Modeling the topology of the base distribution in Normalizing Flows is crucial for accurate multimodal posterior estimation in high-dimensional inverse problems. A matched-GMM initialization provides a simple, effective fix to capture multiple disconnected modes, though further work is needed to assess robustness and scalability across broader problems.

Abstract: We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.

</details>


### [88] [Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning](https://arxiv.org/abs/2512.04958)
*Roberto Cipollone,Luca Iocchi,Matteo Leonetti*

Main category: cs.LG

TL;DR: 提出 Realizable Abstractions 的新概念来为层级强化学习提供更强的表达能力与理论保障，并给出 RARL 算法，通过对抽象策略的合成来实现近似最优的低层策略，具有PAC 性能保证、多项式样本复杂度与对抽象误差的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有 HRL 抽象在表达能力和理论保证方面的不足；现有抽象往往导致非马克夫性或缺乏近似最优性保证，因此需要一个在低层 MDP 与高层决策过程之间具有可 realizable 的关系的框架，并能将抽象策略转化为低层策略。

Method: 提出 Realizable Abstractions 的关系，并证明任一抽象策略都能通过合适的选项组合转化为低层近似最优策略；这些选项可由约束 MDPs 的解来表达。基于此，提出 RARL 阿尔gorithm，它在给定的 Realizable Abstraction 下，返回可组合且近似最优的低层策略。理论分析表明 RARL 是 Probably Approximately Correct，样本复杂度多项式，且对抽象中的不精确性具有鲁棒性。

Result: 理论上建立了 Realizable Abstractions 与低层策略之间的可转化性，并设计了可实现的算法 RARL；R算法在理论意义上具有 PAC 性能、多项式样本复杂度、对抽象误差鲁棒性。

Conclusion: Realizable Abstractions 为 HRL 提供了具备表达能力与理论保障的抽象框架，RARL 作为利用该框架的算法，能产出 compositional、近似最优的低层策略，展现了在存在抽象误差时的鲁棒性与高效性。

Abstract: The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.

</details>


### [89] [Efficient Generative Transformer Operators For Million-Point PDEs](https://arxiv.org/abs/2512.04974)
*Armand Kassaï Koupaï,Lise Le Boudec,Patrick Gallinari*

Main category: cs.LG

TL;DR: ECHO 是一个 transformer-operator 框架，通过分层卷积编码-解码实现百万点 PDE 轨迹生成的高效性，结合从稀疏网格到高分辨率解的训练适配策略，以及学习完整轨迹段的生成建模，显著降低长期误差并支持多任务。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在密集网格的可扩展性、动态展开过程中的误差积累以及针对特定任务的设计方面受到限制，亟需一个高效、通用且可扩展的 PDE 求解/推断框架。

Method: 提出三大创新： (i) 分层卷积编码-解码架构实现 100x 的时空压缩，同时保持网格点的保真。

Result: 在百万点级别的仿真中，针对具有复杂几何、高频动态与长时域的多种 PDE 系统，达到或超过当前状态的性能。

Conclusion: 通过解决可扩展性、长期漂移及多任务能力等关键挑战，ECHO 为高分辨率 PDE 轨迹的生成提供了统一且高效的框架，并支持条件/无条件生成与多任务应用。

Abstract: We introduce ECHO, a transformer-operator framework for generating million-point PDE trajectories. While existing neural operators (NOs) have shown promise for solving partial differential equations, they remain limited in practice due to poor scalability on dense grids, error accumulation during dynamic unrolling, and task-specific design. ECHO addresses these challenges through three key innovations. (i) It employs a hierarchical convolutional encode-decode architecture that achieves a 100 $\times$ spatio-temporal compression while preserving fidelity on mesh points. (ii) It incorporates a training and adaptation strategy that enables high-resolution PDE solution generation from sparse input grids. (iii) It adopts a generative modeling paradigm that learns complete trajectory segments, mitigating long-horizon error drift. The training strategy decouples representation learning from downstream task supervision, allowing the model to tackle multiple tasks such as trajectory generation, forward and inverse problems, and interpolation. The generative model further supports both conditional and unconditional generation. We demonstrate state-of-the-art performance on million-point simulations across diverse PDE systems featuring complex geometries, high-frequency dynamics, and long-term horizons.

</details>


### [90] [SuperActivators: Only the Tail of the Distribution Contains Reliable Concept Signals](https://arxiv.org/abs/2512.05038)
*Cassandra Goldberg,Chaehyeon Kim,Adam Stein,Eric Wong*

Main category: cs.LG

TL;DR: 提出并验证了 SuperActivator 机制，通过极端高尾的 in-concept 激活信号来指示概念存在，相较标准向量或 prompting 检测方法在多模态和多架构设置下提升最高 14% 的 F1，并用于改进特征归因。


<details>
  <summary>Details</summary>
Motivation: 提高概念向量在解释性中的可靠性，解决激活噪声和不一致性问题。

Method: 识别在概念内、概念外激活的重叠模式，提出 SuperActivator tokens 的概念，系统地在图像和文本模态、不同模型架构、不同层和提取技术上比较，与标准向量/ prompting 方法对比，评估 F1；并将该 token 用于改进概念的特征归因。

Result: SuperActivator tokens 在多种设置下显著优于基线，平均或最高提升达 14% 的 F1；该模式在模型、层、模态和提取技术上呈现普遍性。

Conclusion: 该机制具有一般性和可迁移性，可用于提升解释性工具中的概念检测与特征归因。

Abstract: Concept vectors aim to enhance model interpretability by linking internal representations with human-understandable semantics, but their utility is often limited by noisy and inconsistent activations. In this work, we uncover a clear pattern within the noise, which we term the SuperActivator Mechanism: while in-concept and out-of-concept activations overlap considerably, the token activations in the extreme high tail of the in-concept distribution provide a reliable signal of concept presence. We demonstrate the generality of this mechanism by showing that SuperActivator tokens consistently outperform standard vector-based and prompting concept detection approaches, achieving up to a 14% higher F1 score across image and text modalities, model architectures, model layers, and concept extraction techniques. Finally, we leverage SuperActivator tokens to improve feature attributions for concepts.

</details>


### [91] [Multi-LLM Collaboration for Medication Recommendation](https://arxiv.org/abs/2512.05066)
*Huascar Sanchez,Briland Hitaj,Jules Bergmann,Linda Briesemeister*

Main category: cs.LG

TL;DR: 将多LLM协作与化学性互动建模结合的策略用于药物推荐，旨在提高稳定性、可校准性及可信性，但初步结果需进一步验证。


<details>
  <summary>Details</summary>
Motivation: 应对大语言模型在临床决策中易产生幻觉和不一致的问题，单模型或简单集成往往难以提供稳定且可信的药物推荐。基于先前的LLM Chemistry工作，利用模型之间的兼容性评估来提升多模型协作的可靠性。

Method: 在多LLM框架中引入化学-inspired的相互作用建模来引导模型间的协作，构建具有互补性、稳定性和校准性的集合。以真实临床情景进行评估，观察该交互感知的集成是否能产出可信、以患者为中心的用药建议。

Result: 初步结果显示该化学引导的多LLM协作具有潜力，能够提高药物推荐的可信度和一致性，且在利用模型互补优势方面表现积极，但仍需在更大规模和临床实证中验证。

Conclusion: 基于化学互动建模的多LLM协作为构建可靠、可信的临床AI助手提供了有前景的路径，未来需要扩展评估范围、深入分析失败模式并优化校准机制。

Abstract: As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.

</details>


### [92] [OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design](https://arxiv.org/abs/2512.05080)
*Ian Dunn,Liv Toft,Tyler Katz,Juhi Gupta,Riya Shah,Ramith Hettiarachchi,David R. Koes*

Main category: cs.LG

TL;DR: 提出 OMTRA，一种多模态流匹配模型，用于结构基础药物设计（SBDD）的统一生成框架，能够执行多种相关任务，并构建了一个 5 亿个 3D 分子构象的数据集。对口袋条件下的 de novo 设计和对接任务达到最新水平，但大规模预训练和多任务训练的收益有限。


<details>
  <summary>Details</summary>
Motivation: 解决 SBDD 中不同任务（如对接、药效团搜索、从零生成等）在本质上具有相同的生成结构的多样性问题，提供一个统一、灵活的生成框架，并通过大规模 3D 库来提升模型的化学多样性与训练能力。

Method: 提出 OMTRA，一种多模态的流式匹配模型，能够以多模态输入（如口袋几何、配体特征、3D 构象等）进行条件生成和任务切换。通过对 5 亿 3D 构象数据集的 curate，扩展训练材料并整合蛋白-配体数据，实现多任务学习（包括但不限于口袋条件下的 de novo 设计、对接等）。

Result: 在口袋条件下的 de novo 设计和对接任务上达到当前最先进水平；但大规模预训练和多任务训练的效应有限，说明需要更有效的训练策略与数据效率提升。

Conclusion: 提供了一个统一、灵活的 SBDD 生成框架，以及可重复获取的代码、模型和数据集，尽管规模化前沿带来优势的边际效益有限，仍为将来多任务 SBDD 的研究与应用提供了有价值的方向与资源。

Abstract: Structure-based drug design (SBDD) focuses on designing small-molecule ligands that bind to specific protein pockets. Computational methods are integral in modern SBDD workflows and often make use of virtual screening methods via docking or pharmacophore search. Modern generative modeling approaches have focused on improving novel ligand discovery by enabling de novo design. In this work, we recognize that these tasks share a common structure and can therefore be represented as different instantiations of a consistent generative modeling framework. We propose a unified approach in OMTRA, a multi-modal flow matching model that flexibly performs many tasks relevant to SBDD, including some with no analogue in conventional workflows. Additionally, we curate a dataset of 500M 3D molecular conformers, complementing protein-ligand data and expanding the chemical diversity available for training. OMTRA obtains state of the art performance on pocket-conditioned de novo design and docking; however, the effects of large-scale pretraining and multi-task training are modest. All code, trained models, and dataset for reproducing this work are available at https://github.com/gnina/OMTRA

</details>


### [93] [Gradient Descent with Provably Tuned Learning-rate Schedules](https://arxiv.org/abs/2512.05084)
*Dravyansh Sharma*

Main category: cs.LG

TL;DR: A framework for provable hyperparameter tuning in gradient-based optimization that extends to non-convex/non-smooth settings and neural networks, achieving matching sample complexity bounds for learning step-sizes and extending to multiple hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter tuning (e.g., learning rate, momentum) is critical in gradient methods yet lacks near-optimal guarantees in non-convex settings. Extending guarantees beyond convex/smooth functions is highly desirable for modern ML.

Method: Develop analytical tools to bound the sample complexity of learning hyperparameters (like step-size) for gradient descent. Apply to non-convex/non-smooth functions, including neural networks with ReLU, sigmoid, tanh. Extend to tuning multiple hyperparameters (learning rate schedules, momentum, initialization) and to optimizing both validation loss and iteration counts.

Result: Derives matching (up to logarithmic factors) sample complexity bounds for learning step-sizes on a broad function class, extending prior results from smooth convex functions. The framework also handles tuning multiple hyperparameters and applies to common neural network activations.

Conclusion: The work broadens theoretical guarantees for hyperparameter learning in gradient-based optimization, with potential practical impact on tuning, scheduling, and initialization. Future work could include empirical validation, broader algorithm families, and deeper exploration of assumptions and constants.

Abstract: Gradient-based iterative optimization methods are the workhorse of modern machine learning. They crucially rely on careful tuning of parameters like learning rate and momentum. However, one typically sets them using heuristic approaches without formal near-optimality guarantees. Recent work by Gupta and Roughgarden studies how to learn a good step-size in gradient descent. However, like most of the literature with theoretical guarantees for gradient-based optimization, their results rely on strong assumptions on the function class including convexity and smoothness which do not hold in typical applications. In this work, we develop novel analytical tools for provably tuning hyperparameters in gradient-based algorithms that apply to non-convex and non-smooth functions. We obtain matching sample complexity bounds for learning the step-size in gradient descent shown for smooth, convex functions in prior work (up to logarithmic factors) but for a much broader class of functions. Our analysis applies to gradient descent on neural networks with commonly used activation functions (including ReLU, sigmoid and tanh). We extend our framework to tuning multiple hyperparameters, including tuning the learning rate schedule, simultaneously tuning momentum and step-size, and pre-training the initialization vector. Our approach can be used to bound the sample complexity for minimizing both the validation loss as well as the number of gradient descent iterations.

</details>


### [94] [The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception](https://arxiv.org/abs/2512.05089)
*Eduardo Di Santi*

Main category: cs.LG

TL;DR: 该论文提出一个确定性的函数-拓扑框架，认为真实世界的信号聚焦于紧致、低变异的感知流形上，并通过自监督的蒙特卡洛采样在未知规律方程的情况下发现边界，从而实现快速泛化。


<details>
  <summary>Details</summary>
Motivation: 解释生物学习者和自监督AI在有限观测下为何能够泛化，以及为何现实物理过程的信号具有低维、可观测的几何结构，能够支撑感知、表征和世界模型的统一。

Method: 建立一个确定性的函数拓扑框架，将物理现象的有效实现集合视为具有稳定不变量和有限Hausdorff半径的紧致感知流形；通过完全自监督的蒙特卡洛采样来发现流形边界；给出理论保证、可操作的知识边界估计量，并在三个领域进行实证验证。

Result: 在 electromechanical railway point machines、 electrochemical battery discharge curves、以及生理ECG信号等三个领域给出理论与实证验证，显示边界可在无外部监督下通过采样获得，且理论结果与经验数据相符。

Conclusion: 确定性函数拓扑为感知、表征和世界模型构建提供了统一的数学基础，解释了生物学习者和自监督AI为何能从有限观测中泛化。

Abstract: Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.
  This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.
  We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.
  Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.

</details>


### [95] [TV2TV: A Unified Framework for Interleaved Language and Video Generation](https://arxiv.org/abs/2512.05103)
*Xiaochuang Han,Youssef Emad,Melissa Hall,John Nguyen,Karthik Padthe,Liam Robbins,Amir Bar,Delong Chen,Michal Drozdzal,Maha Elbayad,Yushi Hu,Shang-Wen Li,Sreya Dutta Roy,Jakob Verbeek,XuDong Wang,Marjan Ghazvininejad,Luke Zettlemoyer,Emily Dinan*

Main category: cs.LG

TL;DR: TV2TV 将文本和视频生成整合在一个 Mixture-of-Transformers 框架中，通过交替生成文本与视频帧来实现“先用语言再用像素行动”的推理过程，从而提升视频质量和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在需要复杂语义分支或长时间推理的情境下表现不足。通过引入语言建模对规划的能力，结合视频流匹配，提升生成质量与对指令的对齐。

Method: 提出 TV2TV 框架，采用 interleaved text and video generation，利用 Mixture-of-Transformers 同时学习下一文本 token 的预测和下一帧的生成。推理时在文本与视频帧之间交替，文本塔用于思考后续情节，视频塔执行像素生成。通过在文本层面进行干预实现细粒度可控性。对视频游戏数据进行对照实验，并扩展到自然视频（用 Vision-Language 模型对体育视频进行文本描述的嵌入），训练后表现出良好的视觉质量与指令对齐。

Result: 在视频游戏数据的对照实验中，TV2TV 在视觉质量和可控性方面均有显著提升；也可扩展到自然视频，取得强烈的视觉质量与指令对齐，证明模型具备对复杂真实动作序列进行推理与生成的能力。

Conclusion: TV2TV 为具备开放式文本推理与控制能力的视频生成提供了一个有希望的方向，即通过将语言建模与视频生成耦合、并在推理过程中有意识地在文本与像素之间切换。

Abstract: Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.

</details>


### [96] [Deep infant brain segmentation from multi-contrast MRI](https://arxiv.org/abs/2512.05114)
*Malte Hoffmann,Lilla Zöllei,Adrian V. Dalca*

Main category: cs.LG

TL;DR: A deep-learning framework BabySeg for infant/child brain MRI segmentation that leverages domain randomization and flexible multi-scan feature pooling to handle diverse MRI protocols and age ranges, enabling a single model to perform robust segmentation with improved runtime efficiency.


<details>
  <summary>Details</summary>
Motivation: Pediatric brain MRI poses unique segmentation challenges due to ongoing development, variable imaging modalities, motion artifacts, and limited generalization of existing methods across age groups and clinical conditions.

Method: Extends domain randomization to synthesize training images beyond realistic bounds to promote dataset shift invariance. Introduces a mechanism to flexibly pool and interact features from arbitrary numbers of input scans, enabling a single model to work with diverse input configurations and modalities.

Result: Achieves state-of-the-art or competitive accuracy across multiple age cohorts and input configurations using one model, while significantly reducing runtime compared to many existing tools.

Conclusion: BabySeg provides a unified, robust segmentation framework for pediatric brain MRI that generalizes across protocols and ages, addressing fragmentation in existing methods and supporting practical clinical deployment.

Abstract: Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.

</details>


### [97] [Value Gradient Guidance for Flow Matching Alignment](https://arxiv.org/abs/2512.05116)
*Zhen Liu,Tim Z. Xiao,Carles Domingo-Enrich,Weiyang Liu,Dinghuai Zhang*

Main category: cs.LG

TL;DR: 提出 VGG-Flow：基于最优控制的梯度匹配微调方法，用于将预训练的流匹配模型与人类偏好对齐，兼顾适配效率与先验保持。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法在适配效率和_probs前的先验保持之间难以兼顾；需要高效微调并保持先验分布，以实现稳健的人机对齐。

Method: 基于最优控制理论，提出 VGG-Flow，一种梯度匹配的微调方法。核心思想是微调的最优速度场与预训练速度场之间的差异，应与值函数的梯度场相匹配，并通过对价值函数的启发式初始化提升收敛速度。该方法同时利用奖励模型的一阶信息。

Result: 在文本到图像流匹配模型 Stable Diffusion 3 下进行实验，显示在有限计算预算下可实现有效且先验保持的对齐微调。

Conclusion: 将第一阶信息与最优控制框架相结合，为流式/流式对齐模型的微调提供高效、稳健且保留先验的解决方案。

Abstract: While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.

</details>
