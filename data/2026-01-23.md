<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 9]
- [cs.IT](#cs.IT) [Total: 13]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 44]
- [eess.SY](#eess.SY) [Total: 7]
- [eess.SP](#eess.SP) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Multi-Input Ciphertext Multiplication for Homomorphic Encryption](https://arxiv.org/abs/2601.15401)
*Sajjad Akherati,Xinmiao Zhang*

Main category: cs.CR

TL;DR: 改进多输入同态加密乘法框架：实现多输入组合、维持噪声不升高、有效缩放与重线化，硬件实现显著降低面积与延迟。


<details>
  <summary>Details</summary>
Motivation: 多输入乘法在同态加密应用（ML、医疗、金融分析）中必需，但现有方案仅支持两输入，需扩展以提升效率。

Method: 重构三输入乘法实现多输入组合；引入额外评估键实现多项式乘积重线化；理论分析重定位缩放，提出多层缩放方案；给出输入划分指南与硬件架构实现。

Result: 三输入乘法器逻辑面积下降15%，时延降低50%；多输入(4-12)乘法器面积平均节省32%，时延缩短45%。

Conclusion: 针对多输入同态加密乘法，提出了可兼合并计算以减少复杂度的三输入乘法框架，并扩展至任意多输入，保持噪声开销不升高。

Abstract: Homomorphic encryption (HE) enables arithmetic operations to be performed directly on encrypted data. It is essential for privacy-preserving applications such as machine learning, medical diagnosis, and financial data analysis. In popular HE schemes, ciphertext multiplication is only defined for two inputs. However, the multiplication of multiple inputs is needed in many HE applications. In our previous work, a three-input ciphertext multiplication method for the CKKS HE scheme was developed. This paper first reformulates the three-input ciphertext multiplication to enable the combination of computations in order to further reduce the complexity. The second contribution is extending the multiplication to multiple inputs without compromising the noise overhead. Additional evaluation keys are introduced to achieve relinearization of polynomial multiplication results. To minimize the complexity of the large number of rescaling units in the multiplier, a theoretical analysis is developed to relocate the rescaling, and a multi-level rescaling approach is proposed to implement combined rescaling with complexity similar to that of a single rescaling unit. Guidelines and examples are provided on the input partition to enable the combination of more rescaling. Additionally, efficient hardware architectures are designed to implement our proposed multipliers. The improved three-input ciphertext multiplier reduces the logic area and latency by 15% and 50%, respectively, compared to the best prior design. For multipliers with more inputs, ranging from 4 to 12, the architectural analysis reveals 32% savings in area and 45% shorter latency, on average, compared to prior work.

</details>


### [2] [DCeption: Real-world Wireless Man-in-the-Middle Attacks Against CCS EV Charging](https://arxiv.org/abs/2601.15515)
*Marcell Szakály,Martin Strohmeier,Ivan Martinovic,Sebastian Köhler*

Main category: cs.CR

TL;DR: 使用实时SDR对CCS HPGP通信进行攻击，验证车辆对电力信息缺乏防护并提出兼容降级防护的协议扩展。


<details>
  <summary>Details</summary>
Motivation: HPGP因无线泄露可能让近距离攻击者窃听或操纵充电数据，缺乏实测评估，迫切需要对CCS的安全性作深入研究与对策提出。

Method: 构建实时SDR HPGP实现，收集并分析2750次充电会话，采用改进的MitM框架，执行TLS剥离与协议降级攻击，针对车辆安全功能进行实时监测。

Result: 实现对TLS及CCS协议版本的完全操控，造成车辆显示超额功率（>900kw）与实际仅40kw，并通过短暂过充触发紧急停机；表明车辆对安全关键信息高度宽容。

Conclusion: 本文通过实时SDR实现分析了CCS的HPGP通信，可演示完整的T​LS中间人攻击，并针对真实车辆测试发现其对电力信息极其宽容，故建议加入向后兼容的降级预防扩展。

Abstract: The adoption of Electric Vehicles (EVs) is happening at a rapid pace. To ensure fast and safe charging, complex communication is required between the vehicle and the charging station. In the globally used Combined Charging System (CCS), this communication is carried over the HomePlug Green PHY (HPGP) physical layer. However, HPGP is known to suffer from wireless leakage, which may expose this data link to nearby attackers.
  In this paper, we examine active wireless attacks against CCS, and study the impact they can have. We present the first real-time Software-Defined Radio (SDR) implementation of HPGP, granting unprecedented access to the communications within the charging cables. We analyze the characteristics of 2,750 real-world charging sessions to understand the timing constraints for hijacking. Using novel techniques to increase the attacks' reliability, we design a robust wireless Man-in-the-Middle evaluation framework for CCS.
  We demonstrate full control over TLS usage and CCS protocol version negotiation, including TLS stripping attacks. We investigate how real devices respond to safety-critical MitM attacks, which modify power delivery information, and found target vehicles to be highly permissive. First, we caused a vehicle to display charging power exceeding 900 kW on the dashboard, while receiving only 40 kW. Second, we remotely overcharged a vehicle, at twice the requested current for 17 seconds before the vehicle triggered the emergency shutdown. Finally, we propose a backwards-compatible, downgrade-proof protocol extension to mitigate the underlying vulnerabilities.

</details>


### [3] [Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning](https://arxiv.org/abs/2601.15595)
*Xinjie Zhou,Zhihui Yang,Lechao Cheng,Sai Wu,Gang Chen*

Main category: cs.CR

TL;DR: 开发一种无训练数据的Data-Free Selective Unlearning框架，先通过语言模型反演生成伪PII，再构造词级掩码并在LoRA子空间用对比掩码损失实现选择性消融，显著去除LLM中的PII并保留模型实用性


<details>
  <summary>Details</summary>
Motivation: LLM在训练时可能泄露敏感PII，但现有机器遗忘方法需访问训练数据，实际部署往往不具备；需无训练数据的解决方案

Method: 合成伪PII → 构建词级隐私掩码 → 在LoRA子空间内使用对比掩码损失进行词级选择性消融

Result: 在AI4Privacy PII-Masking数据集上对Pythia模型实验表明，该方法能有效去除目标PII且不显著损失模型性能

Conclusion: 数据无训练需求的选择性消融有效抵消LLM对PII信息的记忆，并保持模型实用性

Abstract: Large language models (LLMs) exhibit powerful capabilities but risk memorizing sensitive personally identifiable information (PII) from their training data, posing significant privacy concerns. While machine unlearning techniques aim to remove such data, they predominantly depend on access to the training data. This requirement is often impractical, as training data in real-world deployments is commonly proprietary or inaccessible. To address this limitation, we propose Data-Free Selective Unlearning (DFSU), a novel privacy-preserving framework that removes sensitive PII from an LLM without requiring its training data. Our approach first synthesizes pseudo-PII through language model inversion, then constructs token-level privacy masks for these synthetic samples, and finally performs token-level selective unlearning via a contrastive mask loss within a low-rank adaptation (LoRA) subspace. Extensive experiments on the AI4Privacy PII-Masking dataset using Pythia models demonstrate that our method effectively removes target PII while maintaining model utility.

</details>


### [4] [Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2601.15678)
*Mengyu Yao,Ziqi Zhang,Ning Luo,Shaofei Li,Yifeng Cai,Xiangqun Chen,Yao Guo,Ding Li*

Main category: cs.CR

TL;DR: RAGCRAWLER通过ASCP与全局状态相结合，实现了对RAG系统的高效长程信息提取，显著超越现有攻破技术，揭示RAG安全缺陷并呼吁加强防护。


<details>
  <summary>Details</summary>
Motivation: 在采用检索增强生成（RAG）的隐私场景中，攻击者可能通过精心设计的多轮查询逐步泄露敏感信息；但现有攻击方法缺乏系统的长程规划能力，难以在不确定环境下高效提取完整内容。

Method: 将提取攻破任务建模为ASCP，采用全局状态维护与知识图谱相结合的方式来估算条件边际增益（CMG），在语义空间中规划查询以覆盖尚未检索的文档区域，并通过查询不断更新全局状态以驱动下一轮攻击。

Result: 在多种RAG架构及数据集上的实验表明，RAGCRAWLER在固定查询预算内可达84.4%的语料覆盖率，比最佳基线提升约20.7%；且在语义保真度、内容重构准确性与攻击成本等指标上均保持优异表现，并在逆向改写、联合检索等高级RAG系统中保持高效。

Conclusion: RAGCRAWLER通过在RAG系统中实现自适应随机覆盖问题（ASCP）并利用全局攻击者状态，能够在保密词库中实现高效的长程信息提取。其卓越性能凸显了RAG技术在隐私安全方面的严重漏洞，提示迫切需要更强的安全防护措施。

Abstract: Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.

</details>


### [5] [Balancing Security and Privacy: The Pivotal Role of AI in Modern Healthcare Systems](https://arxiv.org/abs/2601.15697)
*Binu V P,Deepthy K Bhaskar,Minimol B*

Main category: cs.CR

TL;DR: 本文探讨AI在医疗安全与隐私保护间的平衡，提供实践示例与合规建议。


<details>
  <summary>Details</summary>
Motivation: 应对数字威胁增长，同时保护患者隐私与合规需求。

Method: 阐述AI技术在威胁检测、监控、自动响应与透明化设计中的应用场景。

Result: 基于真实医疗案例，证明AI方案能强化安全而不泄露医疗数据。

Conclusion: AI可协助提升医疗信息安全且遵守隐私法规。

Abstract: As digital threats continue to grow, organizations must find ways to enhance security while protecting user privacy. This paper explores how artificial intelligence (AI) plays a crucial role in achieving this balance. AI technologies can improve security by detecting threats, monitoring systems, and automating responses. However, using AI also raises privacy concerns that need careful consideration.We examine real-world examples from the healthcare sector to illustrate how organizations can implement AI solutions that strengthen security without compromising patient privacy. Additionally, we discuss the importance of creating transparent AI systems and adhering to privacy regulations.Ultimately, this paper provides insights and recommendations for integrating AI into healthcare security practices, helping organizations navigate the challenges of modern management while keeping patient data safe.

</details>


### [6] [CAFE-GB: Scalable and Stable Feature Selection for Malware Detection via Chunk-wise Aggregated Gradient Boosting](https://arxiv.org/abs/2601.15754)
*Ajvad Haneef K,Karan Kuwar Singh,Madhu Kumar S D*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High-dimensional malware datasets often exhibit feature redundancy, instability, and scalability limitations, which hinder the effectiveness and interpretability of machine learning-based malware detection systems. Although feature selection is commonly employed to mitigate these issues, many existing approaches lack robustness when applied to large-scale and heterogeneous malware data. To address this gap, this paper proposes CAFE-GB (Chunk-wise Aggregated Feature Estimation using Gradient Boosting), a scalable feature selection framework designed to produce stable and globally consistent feature rankings for high-dimensional malware detection. CAFE-GB partitions training data into overlapping chunks, estimates local feature importance using gradient boosting models, and aggregates these estimates to derive a robust global ranking. Feature budget selection is performed separately through a systematic k-selection and stability analysis to balance detection performance and robustness. The proposed framework is evaluated on two large-scale malware datasets: BODMAS and CIC-AndMal2020, representing large and diverse malware feature spaces. Experimental results show that classifiers trained on CAFE-GB -selected features achieve performance parity with full-feature baselines across multiple metrics, including Accuracy, F1-score, MCC, ROC-AUC, and PR-AUC, while reducing feature dimensionality by more than 95\%. Paired Wilcoxon signed-rank tests confirm that this reduction does not introduce statistically significant performance degradation. Additional analyses demonstrate low inter-feature redundancy and improved interpretability through SHAP-based explanations. Runtime and memory profiling further indicate reduced downstream classification overhead. Overall, CAFE-GB provides a stable, interpretable, and scalable feature selection strategy for large-scale malware detection.

</details>


### [7] [FirmReBugger: A Benchmark Framework for Monolithic Firmware Fuzzers](https://arxiv.org/abs/2601.15774)
*Mathew Duong,Michael Chesser,Guy Farrelly,Surya Nepal,Damith C. Ranasinghe*

Main category: cs.CR

TL;DR: 推出FirmReBugger框架和FirmBench平台，用漏洞判据评估固件模糊器，完成了对9种模糊器的可重复性实验，揭示了技术现状与改进空间。


<details>
  <summary>Details</summary>
Motivation: 单片固件模糊测试缺乏可靠的基准与衡量指标，现有方法依赖代码覆盖和独特崩溃难以客观评估进步，亟需以漏洞为中心的评测体系。

Method: 设计并实现了FirmReBugger框架，利用bug或acles（C语法表达式）与解释器自动分析，仪式化重播种子且不改动目标二进制；构建了包含313个真实漏洞的FirmBench平台；在10 CPU‑年投入下，对9个主流固件模糊器进行可重复性实验。

Result: 实现了FirmReBugger并构建了FirmBench；使用该框架对9种主流固件模糊器进行实验，发现了不同模糊器在处理真实漏洞时的表现差异，并指出了未来改进方向。

Conclusion: FirmReBugger提供了一个可靠的漏洞基准框架，使单片固件模糊测试的评估更公平、可重复，9种先进模糊器在FirmBench上被系统评估，结果揭示了当前技术的优势与不足。

Abstract: Monolithic Firmware is widespread. Unsurprisingly, fuzz testing firmware is an active research field with new advances addressing the unique challenges in the domain. However, understanding and evaluating improvements by deriving metrics such as code coverage and unique crashes are problematic, leading to a desire for a reliable bug-based benchmark. To address the need, we design and build FirmReBugger, a holistic framework for fairly assessing monolithic firmware fuzzers with a realistic, diverse, bug-based benchmark. FirmReBugger proposes using bug oracles--C syntax expressions of bug descriptors--with an interpreter to automate analysis and accurately report on bugs discovered, discriminating between states of detected, triggered, reached and not reached. Importantly, our idea of benchmarking does not modify the target binary and simply replays fuzzing seeds to isolate the benchmark implementation from the fuzzer while providing a simple means to extend with new bug oracles. Further, analyzing fuzzing roadblocks, we created FirmBench, a set of diverse, real-world binary targets with 313 software bug oracles. Incorporating our analysis of roadblocks challenging monolithic firmware fuzzing, the bench provides for rapid evaluation of future advances. We implement FirmReBugger in a FuzzBench-for-Firmware type service and use FirmBench to evaluate 9 state-of-the art monolithic firmware fuzzers in the style of a reproducibility study, using a 10 CPU-year effort, to report our findings.

</details>


### [8] [CONTEX-T: Contextual Privacy Exploitation via Transformer Spectral Analysis for IoT Device Fingerprinting](https://arxiv.org/abs/2601.16160)
*Nazmul Islam,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid expansion of internet of things (IoT) devices have created a pervasive ecosystem where encrypted wireless communications serve as the primary privacy and security protection mechanism. While encryption effectively protects message content, packet metadata and statistics inadvertently expose device identities and user contexts. Various studies have exploited raw packet statistics and their visual representations for device fingerprinting and identification. However, these approaches remain confined to the spatial domain with limited feature representation. Therefore, this paper presents CONTEX-T, a novel framework that exploits contextual privacy vulnerabilities using spectral representation of encrypted wireless traffic for IoT device characterization. The experiments show that spectral analysis provides new and rich feature representation for covert reconnaissance attacks, revealing a complex and expanding threat landscape that would require robust countermeasures for IoT security management. CONTEXT-T first transforms raw packet length sequences into time-frequency spectral representations and then utilizes transformer-based spectral analysis for the device identification. We systematically evaluated multiple spectral representation techniques and transformer-based models across encrypted traffic samples from various IoT devices. CONTEXT-T effectively exploited privacy vulnerabilities and achieved device classification accuracy exceeding 99% across all devices while remaining completely passive and undetectable.

</details>


### [9] [PAL*M: Property Attestation for Large Generative Models](https://arxiv.org/abs/2601.16199)
*Prach Chantasantitam,Adam Ilyas Caulfield,Vasisht Duddu,Lachlan J. Gunn,N. Asokan*

Main category: cs.CR

TL;DR: PAL*M：为大型生成模型提供受可信硬件支撑的属性证明框架，实现了训练、推理阶段的可追溯性与完整性验证。


<details>
  <summary>Details</summary>
Motivation: 现有属性证明方法不支持生成模型或大数据集，亟需对训练与推理全过程进行可证明的安全与合规性保障。

Method: 利用Intel TDX和NVIDIA H100的可信虚拟机与安全GPU实现CPU‑GPU操作可追踪；使用增量多集合哈希及内存映射数据集来高效保持完整性。

Result: 实验表明PAL*M在效率、可扩展性、多功能性和安全性方面均可满足大型生成模型的需求。

Conclusion: PAL*M为大型生成模型提供了可验证属性的框架，使模型提供者能够向监管者和客户证明模型与数据集的属性，并支持大语言模型的训练和推理阶段。

Abstract: Machine learning property attestations allow provers (e.g., model providers or owners) to attest properties of their models/datasets to verifiers (e.g., regulators, customers), enabling accountability towards regulations and policies. But, current approaches do not support generative models or large datasets. We present PAL*M, a property attestation framework for large generative models, illustrated using large language models. PAL*M defines properties across training and inference, leverages confidential virtual machines with security-aware GPUs for coverage of CPU-GPU operations, and proposes using incremental multiset hashing over memory-mapped datasets to efficiently track their integrity. We implement PAL*M on Intel TDX and NVIDIA H100, showing it is efficient, scalable, versatile, and secure.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [10] [Partially Polarized Polar Codes: A New Design for 6G Control Channels](https://arxiv.org/abs/2601.15404)
*Arman Fazeli,Mohammad M. Mansour,Ziyuan Zhu,Louay Jalloul*

Main category: cs.IT

TL;DR: PPP 通过剪枝极化核提升早期非冻结比特数量，从而实现更快的盲解码终止，提升性能且不增加硬件成本。


<details>
  <summary>Details</summary>
Motivation: 在下行控制信道盲解码场景下，需要快速终止无效候选，提升处理效率并降低硬件资源。

Method: 在标准极化码结构中选择性剪枝极化核，调整合成比特通道容量，保证前期有足够非冻结比特以实现早期终止。制定与 PPP 代码匹配的冻结位图设计方案。

Result: 实验表明 PPP 代码在更大块长度下性能显著优于传统极化码，且相较聚合或分段方法不需额外硬件支持，取得更高效率。

Conclusion: PPP 代码在低误码率性能和快速终止方面优于传统极化码，尤其在大块长度下保持高效且不需额外硬件。

Abstract: We introduce a new family of polar-like codes, called Partially Polarized Polar (PPP) codes. PPP codes are constructed from conventional polar codes by selectively pruning polarization kernels, thereby modifying the synthesized bit-channel capacities to ensure a guaranteed number of non-frozen bits available early in decoding. These early-access information bits enable more effective early termination, which is particularly valuable for blind decoding in downlink control channels, where user equipment (UE) must process multiple candidates, many of which carry no valid control information. Our results show that PPP codes offer substantial performance gains over conventional polar codes, particularly at larger block lengths where hardware limitations restrict straightforward scaling. Compared with existing methods such as aggregation or segmentation, PPP codes achieve higher efficiency without the need for additional hardware support. Finally, we propose several frozen-bitmap design strategies tailored to PPP codes.

</details>


### [11] [Stabilizer-Code Channel Transforms Beyond Repetition Codes for Improved Hashing Bounds](https://arxiv.org/abs/2601.15505)
*Tyler Kann,Matthieu R. Bloch,Shrinivas Kudekar,Ruediger Urbanke*

Main category: cs.IT

TL;DR: 本文提出了一种利用任意稳定器码作为通道变换的通用方法，计算诱导的逻辑错误分布，并结合哈希界+侧信息提升量子通信速率；在某些偏斜误差的 Pauli 通道中已验证率能显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统量子哈希界是记忆无关 Pauli 通道实现率的上限，但并不紧密；已知使用小型内部稳定器码可在某些非对称通道上提升速率。该工作试图将此“诱导通道”视角推广到任意稳定器码，并检验其对可达率的提升。

Method: ① 构建给定 [[n,k]] 稳定器生成集合的完整辛对称化表；② 在物理 Pauli 通道作用下求解逻辑 Pauli 错误与 syndrome 的联合概率分布；③ 将此诱导通道视为新的 Pauli 通道，应用哈希界并加入 decoder 侧信息计算可达率；④ 对小规模变换做结构化搜索寻找改进示例。

Result: 在对先前研究中考察的不对称独立误差 Pauli 通道族上，通过搜索得到的若干小尺寸变换实例取得了超过基准哈希界的可达率。

Conclusion: 通过构造任意稳定器代码的完整辛对称化表并计算其诱导的逻辑错误与 syndrome 的联合分布，利用哈希界与 decoder 侧信息，可在某些 Pauli 通道上得到可达率，且在特定带偏斜且独立错误的通道族中实现率超过传统哈希界。

Abstract: The quantum hashing bound guarantees that rates up to $1-H(p_I, p_X, p_Y, p_Z)$ are achievable for memoryless Pauli channels, but it is not generally tight. A known way to improve achievable rates for certain asymmetric Pauli channels is to apply a small inner stabilizer code to a few channel uses, decode, and treat the resulting logical noise as an induced Pauli channel; reapplying the hashing argument to this induced channel can beat the baseline hashing bound. We generalize this induced-channel viewpoint to arbitrary stabilizer codes used purely as channel transforms. Given any $ [\![ n, k ]\!] $ stabilizer generator set, we construct a full symplectic tableau, compute the induced joint distribution of logical Pauli errors and syndromes under the physical Pauli channel, and obtain an achievable rate via a hashing bound with decoder side information. We perform a structured search over small transforms and report instances that improve the baseline hashing bound for a family of Pauli channels with skewed and independent errors studied in prior work.

</details>


### [12] [A Class of Subadditive Information Measures and their Applications](https://arxiv.org/abs/2601.15639)
*Hamidreza Abin,Mahdi Zinati,Amin Gohari,Mohammad Hossein Yassaee,Mohammad Mahdi Mojahedian*

Main category: cs.IT

TL;DR: 提出 $(G,f)$-散度并证明其子加性，应用于信息理论中的极限分析和错误概率下界。


<details>
  <summary>Details</summary>
Motivation: 为更灵活地利用 f-散度捕获信息与错误概率之间的关系，弥补传统互信息及散度在子加性上的不足。

Method: 构造两参数 $(G,f)$-散度，利用 Csiszár 的互信息泛化，提出减少原理，将子加性验证归约到二元字母表；对特定 G 函数推导出 f 的充要条件；应用于有限块长极限和硬判别测试。

Result: 给出一系列实用的子加性条件，涵盖常见 f-散度；在信道编码、二元假设检验和香农-加拉格-伯莱坎普球面-包装指数框架中实现新的界限。

Conclusion: 证明了在广泛条件下，采用非递减变换 G 的 f-散度可满足子加性，并将其推广至信息测度与二元离散系统的研究。

Abstract: We introduce a two-parameter family of discrepancy measures, termed \emph{$(G,f)$-divergences}, obtained by applying a non-decreasing function $G$ to an $f$-divergence $D_f$. Building on Csiszár's formulation of mutual $f$-information, we define a corresponding $(G,f)$-information measure $
I_{G,f}(X;Y)$. A central theme of the paper is subadditivity over product distributions and product channels. We develop reduction principles showing that, for broad classes of $G$, it suffices to verify divergence subadditivity on binary alphabets. Specializing to the functions $G(x)\in\{x,\log(1+x),-\log(1-x)\}$, we derive tractable sufficient conditions on $f$ that guarantee subadditivity, covering many standard $f$-divergences. Finally, we present applications to finite-blocklength converses for channel coding, bounds in binary hypothesis testing, and an extension of the Shannon--Gallager--Berlekamp sphere-packing exponent framework to subadditive $(G,f)$-divergences.

</details>


### [13] [Generative AI-Empowered Semantic Twin Channel Model for ISAC](https://arxiv.org/abs/2601.15642)
*Yi Chen,Yatao Hu,Ming Li,Chong Han*

Main category: cs.IT

TL;DR: 本文提出基于环境语义的ISAC通道建模原则，并通过生成式AI实现语义双胞胎通道模型，成功生成符合语义条件的物理可行通道样本，验证了模型在多视角下的语义一致性，为ISAC系统仿真与标准化提供了可控、可复现的方案。


<details>
  <summary>Details</summary>
Motivation: 现有统计或确定性通道模型要么忽略感知所需的细粒度多径特征，要么计算量过大，难以实现ISAC系统级评估，亟需一种兼顾准确与效率、兼顾通信与感知需求的统一建模框架。

Method: 通过分析环境语义与通道结构的多级关联，提出语义导向的通道建模原则，并利用生成式AI构建STCM，生成在各语义条件下物理上可行的通道样本；随后在多视角场景中验证模型的语义一致性。

Result: STCM在多视角挑战下保持语义一致，并通过案例研究展示了其在可控仿真、数据集生成和可重复ISAC基准测试方面的实用价值。

Conclusion: 本研究提出并验证了一种基于环境语义的ISAC通道建模原则及生成式AI驱动的语义双胞胎通道模型（STCM），有效弥合了传统通信与感知模型在多路径细节处理上的差距，为ISAC系统级评估提供了可控、可复现的仿真与数据集生成手段。

Abstract: Integrated sensing and communication (ISAC) increasingly exposes a gap in today's channel modeling. Efficient statistical models focus on coarse communication-centric metrics, and therefore miss the weak but critical multipath signatures for sensing, whereas deterministic models are computationally inefficient to scale for system-level ISAC evaluation. This gap calls for a unifying abstraction that can couple what the environment means for sensing with how the channel behaves for communication, namely, environmental semantics. This article clarifies the meaning and essentiality of environmental semantics in ISAC channel modeling and establishes how semantics is connected to observable channel structures across multiple semantic levels. Based on this perspective, a semantics-oriented channel modeling principle was advocated, which preserves environmental semantics while abstracting unnecessary detail to balance accuracy and complexity. Then, a generative AI-empowered semantic twin channel model (STCM) was introduced to generate a family of physically plausible channel realizations representative of a semantic condition. Case studies further show semantic consistency under challenging multi-view settings, suggesting a practical path to controllable simulation, dataset generation, and reproducible ISAC benchmarking toward future design and standardization.

</details>


### [14] [Generalized Information Inequalities via Submodularity, and Two Combinatorial Problems](https://arxiv.org/abs/2601.15723)
*Gunank Jakhar,Gowtham R. Kurri,Suryajith Chillara,Vinod M. Prabhakaran*

Main category: cs.IT

TL;DR: 用凸函数通用方法推广Madiman‑Tetali不等式，得到更强的Loomis‑Whitney投影不等式及新的极值图论结果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在在已存在的熵不等式与子模函数统一框架上进一步推广，构造更强的凸函数形式不等式，并探讨其在投影与图论等应用中的新结果。

Method: 结合Madiman–Tetali的强弱不等式与Sason的凸功能框架，构造子模函数的凸功能推广；利用特定情形推导Loomis‑Whitney投影不等式；在极值图论问题中采用Shearer's lemma进行分析。

Result: 给出了子模函数的凸功能强弱Madiman‑Tetali不等式；推出了一条改进的Loomis‑Whitney投影不等式，能够利用切片层次结构信息；在极值图论问题上得到的结果既恢复又超越了Sason（2022）与Boucheron等人的先前结论。

Conclusion: 本文通过凸功能推广进一步完善了熵与子模函数不等式的理论框架，并将其扩展到投影与极值图论，从而提供了更广阔的应用视角。

Abstract: It is well known that there is a strong connection between entropy inequalities and submodularity, since the entropy of a collection of random variables is a submodular function. Unifying frameworks for information inequalities arising from submodularity were developed by Madiman and Tetali (2010) and Sason (2022). Madiman and Tetali (2010) established strong and weak fractional inequalities that subsume classical results such as Han's inequality and Shearer's lemma. Sason (2022) introduced a convex-functional framework for generalizing Han's inequality, and derived unified inequalities for submodular and supermodular functions. In this work, we build on these frameworks and make three contributions. First, we establish convex-functional generalizations of the strong and weak Madiman and Tetali inequalities for submodular functions. Second, using a special case of the strong Madiman-Tetali inequality, we derive a new Loomis-Whitney-type projection inequality for finite point sets in $\mathbb{R}^d$, which improves upon the classical Loomis-Whitney bound by incorporating slice-level structural information. Finally, we study an extremal graph theory problem that recovers and extends the previously known results of Sason (2022) and Boucheron et al., employing Shearer's lemma in contrast to the use of Han's inequality in those works.

</details>


### [15] [Recursive Flow: A Generative Framework for MIMO Channel Estimation](https://arxiv.org/abs/2601.15767)
*Zehua Jiang,Fenghao Zhu,Chongwen Huang,Richeng Jin,Zhaohui Yang,Xiaoming Chen,Zhaoyang Zhang,Mérouane Debbah*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Channel estimation is a fundamental challenge in massive multiple-input multiple-output systems, where estimation accuracy governs the spectral efficiency and link reliability. In this work, we introduce Recursive Flow (RC-Flow), a novel solver that leverages pre-trained flow matching priors to robustly recover channel state information from noisy, under-determined measurements. Different from conventional open-loop generative models, our approach establishes a closed-loop refinement framework via a serial restart mechanism and anchored trajectory rectification. By synergizing flow-consistent prior directions with data-fidelity proximal projections, the proposed RC-Flow achieves robust channel reconstruction and delivers state-of-the-art performance across diverse noise levels, particularly in noise-dominated scenarios. The framework is further augmented by an adaptive dual-scheduling strategy, offering flexible management of the trade-off between convergence speed and reconstruction accuracy. Theoretically, we analyze the Jacobian spectral radius of the recursive operator to prove its global asymptotic stability. Numerical results demonstrate that RC-Flow reduces inference latency by two orders of magnitude while achieving a 2.7 dB performance gain in low signal-to-noise ratio regimes compared to the score-based baseline.

</details>


### [16] [Practical applications of Set Shaping Theory to Non-Uniform Sequences](https://arxiv.org/abs/2601.15853)
*A. Schmidt,A. Vdberg,A. Petit*

Main category: cs.IT

TL;DR: 研究证明SST对非均匀序列有效，提出近似排序方案并开放代码，可复现实验。


<details>
  <summary>Details</summary>
Motivation: 面对非均匀序列时，需要对原始与变换后的集合按信息量排序，但精确排序难度指数级，因此需要方法。

Method: 通过近似但信息保留的排序方法，保留SST结构要求，从而避免指数复杂度。

Result: 近似排序实现了SST预测的编码增益，验证了在非均匀序列中依旧存在造型优势。

Conclusion: Set Shaping Theory的变形优势在非均匀序列中依然成立。

Abstract: Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work

</details>


### [17] [Blind Identification of Channel Codes: A Subspace-Coding Approach](https://arxiv.org/abs/2601.15903)
*Pramod Singh,Prasad Krishnan,Arti Yardi*

Main category: cs.IT

TL;DR: 提出一种新译码器，用于在二进制对称信道上盲识别随机线性码，提供理论上限与实验证据，性能优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有盲识别方法往往需要特殊结构且缺乏严谨性能保证，亟需对通用码族在典型信道上提供可分析且高效的识别方案

Method: 结合海明度量与子空间度量的译码策略，推出最小去噪子空间差异译码器，并给出其在有限重量误差及 BSC 下的错误概率上界

Result: 理论上给出了有限错误重量下的成功识别保证，并给出了 BSC 下的错误概率上界；仿真表明该译码器在多种信道条件下优于现有通用技术，且只需要有限数量接收向量即可实现可靠识别

Conclusion: 通过最小去噪子空间差异译码器实现了在 BSC 上对随机线性码进行盲识别的理论保障与较优性能

Abstract: The problem of blind identification of channel codes at a receiver involves identifying a code chosen by a transmitter from a known code-family, by observing the transmitted codewords through the channel. Most existing approaches for code-identification are contingent upon the codes in the family having some special structure, and are often computationally expensive otherwise. Further, rigorous analytical guarantees on the performance of these existing techniques are largely absent. This work presents a new method for code-identification on the binary symmetric channel (BSC), inspired by the framework of subspace codes for operator channels, carefully combining principles of hamming-metric and subspace-metric decoding. We refer to this method as the minimum denoised subspace discrepancy decoder. We present theoretical guarantees for code-identification using this decoder, for bounded-weight errors, and also present a bound on the probability of error when used on the BSC. Simulations demonstrate the improved performance of our decoder for random linear codes beyond existing general-purpose techniques, across most channel conditions and even with a limited number of received vectors.

</details>


### [18] [A Remark on Downlink Massive Random Access](https://arxiv.org/abs/2601.15928)
*Yuchen Liao,Wenyi Zhang*

Main category: cs.IT

TL;DR: 作者提供了确定性可变长度码，降低DMRA的身份编码开销至常数级别。


<details>
  <summary>Details</summary>
Motivation: 避免显式编码设备身份导致的以$\log_2$总用户数为比例的开销。

Method: 利用组合学中的覆蓋数组，对DMRA的码设计进行确定性构造。

Result: 构造得到的码在总用户数不变的情况下，开销上限为$1+\log_2 e$位。

Conclusion: 存在可变长度码实现下行大规模随机接入的开销不大于$1+\log_2 e$位，优于以往上界。

Abstract: In downlink massive random access (DMRA), a base station transmits messages to a typically small subset of active users, selected randomly from a massive number of total users. Explicitly encoding the identities of active users would incur a significant overhead scaling logarithmically with the number of total users. Recently, via a random coding argument, Song, Attiah and Yu have shown that the overhead can be reduced to within some upper bound irrespective of the number of total users. In this remark, recognizing that the code design for DMRA is an instance of covering arrays in combinatorics, we show that there exists deterministic construction of variable-length codes that incur an overhead no greater than $1 + log_2 e$ bits.

</details>


### [19] [RIS-Aided Cooperative ISAC Network for Imaging-Based Low-Altitude Surveillance](https://arxiv.org/abs/2601.16033)
*Zhixin Chen,Yixuan Huang,Zhengze Ji,Jie Yang,Shi Jin*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The low-altitude economy is integral to the advancement of numerous sectors, necessitating the development of advanced low-altitude surveillance techniques. Nevertheless, conventional methods encounter limitations of high deployment costs and low signal strength. This study proposes a reconfigurable intelligent surface (RIS)-aided cooperative integrated sensing and communication (ISAC) network for low-altitude surveillance. This network employs RISs to reflect ISAC signals into low-altitude space for sensing. To enhance signal strength, we employ active RIS (ARIS) to amplify the signals. Moreover, in order to avoid error propagation and data association in traditional sensing methods, we model low-altitude surveillance as an imaging problem based on compressed sensing theory, which can be solved through the subspace pursuit algorithm. We derive the Cramer-Rao lower bound (CRLB) of the proposed RIS-aided low-altitude imaging system and analyze the impacts of various system parameters on sensing performance, providing guidance for ISAC system configuration. Numerical results show that ARIS outperforms passive RIS under identical power constraints, achieving effective imaging and target detection at altitudes up to 300 meters.

</details>


### [20] [Tri-Hybrid Beamforming Design for integrated Sensing and Communications](https://arxiv.org/abs/2601.16036)
*Tianyu Fang,Mengyuan Ma,Markku Juntti,Nhan Thanh Nguyen*

Main category: cs.IT

TL;DR: 提出闭式迭代三合体波束成形算法，能提升能效与空间增益，但波束对齐下降。


<details>
  <summary>Details</summary>
Motivation: 在额外大型天线阵列中采用低成本可编程超材料天线实现能效通信系统的必要性；同时提升集成感知与通信（ISAC）性能。

Method: 通过将通信信噪比和目标方向的感知功率作为多目标优化目标，并在总功率消耗和三合体波束成形架构的物理限制内约束，提出了一种闭式迭代算法，实现了低复杂度与快速执行。

Result: 数值仿真表明，三合体架构在空间增益和能源效率方面优于传统混合波束成形，但在波束对齐方面略逊。

Conclusion: 三合体波束成形架构在大规模天线阵列中显著提升了空间增益和能源效率，尽管与传统混合波束成形相比，波束对齐能力略有下降。

Abstract: Tri-hybrid beamforming architectures have been proposed to enable energy-efficient communications systems in extra-largescale antenna arrays using low-cost programmable metasurface antennas. We study the tri-hybrid beamforming design for integrated sensing and communications (ISAC) to improve both communications and sensing performances. Specifically, we formulate a multi-objective optimization problem that balances communications signal-to-noise ratio (SNR) and the sensing power at a target direction, subject to constraints on the total power consumption and physical limitations inherent to the trihybrid beamforming architecture. We develop an efficient iterative algorithm in which the variables are updated in a closed form at each iteration, leading to a low-complexity and fast-execution design. Numerical results show that the tri-hybrid architecture improves spatial gain and energy efficiency, though with reduced beam alignment capability compared to conventional hybrid beamforming architectures.

</details>


### [21] [Tensor Reed-Muller Codes: Achieving Capacity with Quasilinear Decoding Time](https://arxiv.org/abs/2601.16164)
*Emmanuel Abbe,Colin Sandon,Oscar Sprumont*

Main category: cs.IT

TL;DR: 提出两套张量Reed-Muller码的准线性时间解码实现，并提供通用张量码多项式时间解码算法，显著提升接近容量码的实用性。


<details>
  <summary>Details</summary>
Motivation: 寻求在近容量下具备高效解码的块码结构；TRM码拥有优良参数，却缺乏既高效又能处理大量错误的解码方法。

Method: 构造 t=3 与 t≥4 的两种TRM码，并提出一种多项式时间的张量码（C=C1⊗⋯⊗Ct）解码算法，能够在不要求各子码可多项式解码的前提下，从至少\frac{dmin(C)}{2\max\{dmin(C1),…,dmin(Ct)}\}-1 个替代错误中恢复信息。

Result: \begin{itemize}\item t=3 时，误码概率为 n^{-ω(log n)}，解码时间为 O(n\log\log n)。\item t\ge4 时，误码概率为 2^{-n^{\frac12-\frac1{2(t-2)}-o(1)}}，解码时间为 O(n\log n)。\item 上述解码算法可处理 \frac{dmin(C)}{2\max\{dmin(C1),…,dmin(Ct)\}}-1 个误码。\end{itemize}

Conclusion: 本文证明存在常数速率、接近容量的张量Reed-Muller码，可在准线性时间内解码。

Abstract: Define the codewords of the Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;r_2,m_2;\dots;r_t,m_t)$ to be the evaluation vectors of all multivariate polynomials in the variables $\left\{x_{ij}\right\}_{i=1,\dots,t}^{j=1,\dots m_i}$ with degree at most $r_i$ in the variables $x_{i1},x_{i2},\dots,x_{im_i}$. The generator matrix of $\mathsf{TRM}(r_1,m_1;\dots;r_t,m_t)$ is thus the tensor product of the generator matrices of the Reed-Muller codes $\mathsf{RM}(r_1,m_1),\dots, \mathsf{RM}(r_t,m_t)$.
  We show that for any constant rate $R$ below capacity, one can construct a Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;\dotsc;r_t,m_t)$ of rate $R$ that is decodable in quasilinear time. For any blocklength $n$, we provide two constructions of such codes:
  1) Our first construction (with $t=3$) has error probability $n^{-ω(\log n)}$ and decoding time $O(n\log\log n)$.
  2) Our second construction, for any $t\geq 4$, has error probability $2^{-n^{\frac{1}{2}-\frac{1}{2(t-2)}-o(1)}}$ and decoding time $O(n\log n)$.
  One of our main tools is a polynomial-time algorithm for decoding an arbitrary tensor code $C=C_1\otimes\dotsc\otimes C_t$ from $\frac{d_{\min}(C)}{2\max\{d_{\min}(C_1),\dotsc,d_{\min}(C_t) \}}-1$ adversarial errors. Crucially, this algorithm does not require the codes $C_1,\dotsc,C_t$ to themselves be decodable in polynomial time.

</details>


### [22] [Non-Linearly Separable Distributed Computing: A Sparse Tensor Factorization Approach](https://arxiv.org/abs/2601.16171)
*Ali Khalesi,Ahmad Tanha,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: ...


<details>
  <summary>Details</summary>
Motivation: ...

Method: ...

Result: ...

Conclusion: ...

Abstract: The work considers the $N$-server distributed computing setting with $K$ users requesting functions that are arbitrary multi-variable polynomial evaluations of $L$ real (potentially non-linear) basis subfunctions. Our aim is to seek efficient task-allocation and data-communication techniques that reduce computation and communication costs. Towards this, we take a tensor-theoretic approach, in which we represent the requested non-linearly decomposable functions using a properly designed tensor $\bar{\mathcal{F}}$, whose sparse decomposition into a tensor $\bar{\mathcal{E}}$ and matrix $\mathbf{D}$ directly defines the task assignment, connectivity, and communication patterns. We here design an achievable scheme, employing novel fixed-support SVD-based tensor factorization methods and careful multi-dimensional tiling of subtensors, yielding computation and communication protocols whose costs are derived here, and which are shown to perform substantially better than the state of art.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [23] [MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments](https://arxiv.org/abs/2601.15578)
*Cyril Shih-Huan Hsu,Xi Li,Lanfranco Zanzi,Zhiheng Yang,Chrysa Papagianni,Xavier Costa Pérez*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference](https://arxiv.org/abs/2601.15333)
*Xuanning Hu,Anchen Li,Qianli Xing,Jinglong Ji,Hao Tuo,Bo Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.

</details>


### [25] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: 研究发现语言差异导致 LLM 在低资源语言上的回答质量低，且语言影响模型的文化背景选择，进而影响答案。


<details>
  <summary>Details</summary>
Motivation: 确保不同语言用户在与 LLM 交互时不受系统性劣势，探究语言与文化对回答质量的影响。

Method: 基于 WildChat 的实测开放式问题集，评估不同语言的回复质量；使用 LLM-as-a-Judge 检测回复中的文化上下文；对 CulturalBench 进行多语言翻译后评估。

Result: LLM 在低资源语言上均表现出更低的回答质量，语言显著影响模型使用的文化语境，进而影响答案质量。

Conclusion: LLM 在低资源语言上的回答质量显著低于高资源语言，且语言选择会改变模型使用的文化背景，从而影响答案质量。

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [26] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: 作者通过在Mixture-of-Experts中加入null专家，实现了在自回归模型中兼顾权重与数据稀疏性，从而在视觉语言任务中提升计算效率、训练表现和模态感知。


<details>
  <summary>Details</summary>
Motivation: 令每个token仅激活少数专家的权重稀疏性已见效；但在自回归模型中，直接的专家选择会破坏因果性，导致训练-推理不匹配。如何在保持因果性的前提下实现数据稀疏性是研究的关键。

Method: 在自回归MoE中引入null专家作为零计算槽，采用标准负载平衡目标，使模型在期望上均匀使用所有专家（真实和null），从而在保持因果性的同时实现数据稀疏性。

Result: 在与视觉-语言混合输入的训练任务中，结合权重稀疏性和数据稀疏性，在匹配期望FLOPs的情况下，比单独使用权重稀疏性更高效的计算前沿，并且训练损失和下游性能均得到提升；同时模型学会了隐式的模态感知分配策略。

Conclusion: 通过结合权重稀疏性和数据稀疏性，Causal Token-Choice MoE实现了更高的计算效率，并在视觉语言模型训练中取得更好的训练损失和下游性能；模型能自适应地为视觉token更积极地分配null专家，从而实现隐式的模态感知。

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [27] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: 把注意力看成熵正则化最优传输，提出 GOAT：可学习连续先验，解决注意力沉降，兼容高效核，学习可外推的空间先验。


<details>
  <summary>Details</summary>
Motivation: 标准注意力假设均匀先验导致代表性权衡，且难以解释注意力沉降；需要更灵活、更具泛化性的注意力机制。

Method: 将注意力视为熵正则化最优传输问题，提出 GOAT 用可学习先验替代默认均匀先验，并融合空间信息得到可外推先验。

Result: GOAT 与 FlashAttention 等核兼容；通过EOT解释并解决注意力沉降；学习的先验既有可学习位置嵌入的灵活性，又保留固定编码的长度泛化能力。

Conclusion: GOAT 能在保持与传统注意力兼容的同时，用可学习的连续先验提升泛化与效率，并通过EOT解释注意力沉降。

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [28] [FedUMM: A General Framework for Federated Learning with Unified Multimodal Models](https://arxiv.org/abs/2601.15390)
*Zhaolong Su,Leheng Zhao,Xiaoying Wu,Ziyue Xu,Jindong Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.

</details>


### [29] [Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC](https://arxiv.org/abs/2601.15399)
*Ashna Nawar Ahmed,Banooqa Banday,Terry Jones,Tanzima Z. Islam*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.

</details>


### [30] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: 通过自我迭代增强数据质量，利用Ambient Diffusion鲁棒学习，显著提升扩散模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据集质量不均，直接训练导致模型欠佳；需要一种逐步提升数据质量的方法以优化扩散模型。

Method: 采用迭代式数据模型共进步骤，每轮将待生成样本视为轻度噪声，利用Ambient Diffusion学习抗噪，避免自我消耗循环。

Result: 在无条件与文本条件图像生成以及新型蛋白设计任务上，Ambient Dataloops实现了行业最先进的性能。

Conclusion: 本研究提出Ambient Dataloops迭代框架，通过数据集与模型共同演化提高数据质量，显著提升扩散模型性能。

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [31] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: Lattice 通过置信门控在需要时激活行为原型，既提升预测效果，又在不适用时安全退回，体现了管理不确定性的有效架构。


<details>
  <summary>Details</summary>
Motivation: 在时间序列预测中需处理分布漂移与被激活结构的适用性，传统模型难以平衡提升与误激活风险；本文旨在提出一种在安全关键环境下可控使用结构的机制。

Method: Lattice 采用行为窗口聚类为行为原型，利用二元置信门控判定是否激活原型基评分；置信度低时退回基线预测；模型可与 LSTM 或 transformer 结合。


Result: 在 MovieLens+LSTM，HR@10 提升 31.9%；对比 SASRec & BERT4Rec 分别提升 109.4% 与 218.6%。
在 LIGO 与金融数据中，在分布漂移时成功拒绝原型激活，表明门控避免误激活。
在 transformer 背骨上，Lattice 维持 0.0% 改善，表明在已具备结构时仅退回基线而不降级。


Conclusion: 本研究提出的 Lattice 通过二元置信门控动态激活学习到的行为结构，在不同领域的序列预测任务中实现了显著提升，尤其在基准模型尚未包含结构时表现突出；而在已具备深度结构的模型上保持性能不变，证明了置信门控在安全关键应用中管理表观不确定性的有效性。

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [32] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: CASL 将扩散模型的稀疏潜在维度通过 SAE 与线性映射与语义概念对齐；提供 CASL‑Steer 探查工具和 EPR 指标；实现更精准、更可解释的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 内部激活含丰富语义信息，但缺乏可解释性和对齐现有方法仅为无监督，无法与人类概念对齐，限制了对生成图像的可靠语义控制。

Method: 先在冻结的 U‑Net 激活上训练稀疏自编码器获取可解耦的潜在表示；随后学习轻量级线性映射，将每个语义概念对应到一小组相关潜在维度；通过 CASL‑Steer 对齐方向进行因果探查，评估对生成内容的影响；并提出 Editing Precision Ratio (EPR) 指标衡量概念专一性与属性保持。

Result: 相比现有方法，CASL 在编辑精度和可解释性上取得显著提升；EPR 指标验证了概念特定性与属性保留；实验表明为首个实现监督对齐的工作。

Conclusion: 本文提出 CASL 框架，实现了扩散模型稀疏潜在维度与人类可理解语义概念的监督对齐，显著提升了编辑精度和可解释性，成为首个此类监督对齐方法。

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [33] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: LLM生成文本污染导致经验风险最小化在混合数据上表现不佳；通过非均匀加权或其他鲁棒学习策略，可克服数据源不可知的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及，合成文本大量污染了真实数据源，传统学习算法在不知道数据来源的情况下可能产生失真结果。本研究旨在系统探讨在此普遍存在的数据污染场景下，学习理论的基本假设和方法是否仍成立，以及如何改进算法以提高鲁棒性。

Method: 本文将混合自然/合成数据的学习任务视为一系列学习任务，假设学习算法对每个样本的来源一无所知。作者针对均值估计问题，比较了传统ERM与对不同生成批次样本分配非均匀权重的算法；随后在PAC学习框架下检验ERM的收敛性，并设计了可处理任意VC类、任意污染量的鲁棒算法。

Result: 1. 对均值估计：ERM最终收敛至真实均值，但性能被一种对不同生成批次样本赋予非均匀权重的算法超越。2. 在PAC学习中，ERM不一定收敛到真概念；然而，作者证明了存在能够在任意VC类和任意污染水平下学习正确假设的算法。

Conclusion: 本文指出在大规模语言模型生成文本普及的背景下，传统的经验风险最小化（ERM）方法在混合自然与合成数据场景中存在显著局限。通过对均值估计和PAC学习两个典型问题的分析，发现尽管ERM在均值估计上能收敛，但在更复杂的VC类函数族上往往失效；相对而言，给不同来源样本分配非统一权重或采用其他模型可在任意污染水平下得到正确假设。该研究强调了在数据源不可知的学习任务中，设计对合成数据具有鲁棒性的学习策略的必要性。

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [34] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther: a PyTorch-compatible library implementing RandNLA compression techniques. Replacing standard layers yields ~75% memory savings on BERT with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Training large DL models limited by GPU memory & compute, need compressions

Method: Panther library reimplements RandNLA techniques as efficient PyTorch-compatible layers (sketched linear, conv, attention, randomized decompositions) with C++/CUDA backend 

Result: Replacing PyTorch linear layers reduces memory by ~75% on BERT while keeping similar loss

Conclusion: Panther enables easy adoption of RandNLA compression to improve memory efficiency in deep learning models

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [35] [Early predicting of hospital admission using machine learning algorithms: Priority queues approach](https://arxiv.org/abs/2601.15481)
*Jakub Antczak,James Montgomery,Małgorzata O'Reilly,Zbigniew Palmowski,Richard Turner*

Main category: cs.LG

TL;DR: 对ED每日到院量使用SARIMAX、XGBoost、LSTM进行七日预测；XGBoost最精准，SARIMAX在高复杂病例上更好，三者均在突发高峰时低估数据。


<details>
  <summary>Details</summary>
Motivation: ED拥挤直接危及病人安全和运营效率，准确预测每日到院量是资源合理配置的关键；因此需要评估多种时间序列与机器学习模型在此特定场景的表现。

Method: 使用SARIMAX、XGBoost和LSTM三种模型分别对澳大利亚三级医院2017-2021年的每日ED到院量进行为期七天的预测；对需求进行八类病房细分并按临床复杂度分层；利用Prophet模型为COVID‑19期间异常数据生成对照的合成值，保障模型训练与评估的稳健性。

Result: 与季节性基线相比，三种模型均有提升；XGBoost在总到院量预测中MAE为6.63，取得最佳精度；SARIMAX在预测高复杂度病例时略优，MAE为3.77。所有模型均倾向于低估短时骤增的患者量。

Conclusion: 这些预测模型能够准确重现日常患者流量模式，但在预测突发且高峰的病人量时普遍出现低估，显示需进一步改进模型以适应极端波动。

Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.

</details>


### [36] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: MFS利用马尔可夫理论替代 heuristics，实现更优推理路径与高效计算。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型的自回归解码由于逐词生成，无法全局最优地搜索推理路径；现有推理时策略如foresight sampling依赖经验启发式，缺乏理论依据。

Method: 提出Martingale Foresight Sampling(MFS)，将推理路径质量视为随机过程，利用马尔可夫理论：Doob分解定理给出步长评估、可选停机理论用于子路径剪枝、马尔可夫收敛定理用以自适应终止。

Result: 在六个推理基准上，MFS在准确率超过现有最优方案，同时计算效率显著提升。

Conclusion: 本工作证明基于马尔可夫随机过程的精确理论框架能显著提升大型语言模型的推理性能与效率，为未来推理算法提供可复制、可解释的基础。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [37] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: 提出Margin‑Aware Speculative Verification：无需额外训练、可兼容现有框架，依据logits稳定度动态放宽拒绝，显著提升多规模LLM推理速度并保持质量。


<details>
  <summary>Details</summary>
Motivation: 现有SD方案严格基于token‑级拒绝抽样，在模型低边际区间（top候选间差距小）下对合格候选的拒绝提供极小信息增益，却大幅增加回滚成本，导致验证效率低下。

Method: 该方法在不需要额外训练的前提下，只根据目标模型logits的决策稳定性来动态调整拒绝规则：当严格拒绝收益低时放宽拒绝。

Result: 在8B至235B模型规模的大规模实验中，该方法在多种基准上相较最先进基线保持生成质量的前提下，实现了一致且显著的推理速度提升。

Conclusion: Margin‑Aware Speculative Verification通过在目标模型局部决策不确定时放宽拒绝，提升了归约成本与信息收益的平衡，从而持续获得显著的推理加速，且不降低生成质量。

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [38] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [39] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 本研究通过整合更丰富的脑区 MRI 特征，并使用 Boruta 选取重要地区，提升了非典型 AD（atAD）诊断召回率。


<details>
  <summary>Details</summary>
Motivation: 传统的核磁共振（MRI）仅依靠海马体积和认知评估在识别非典型阿尔茨海默症（atAD）方面表现不足，导致误诊率高，需寻找更精准的诊断方法。

Method: 基于机器学习的特征选择与分类，使用临床测试外加广泛的全脑MRI特征，采用Boruta算法挑选显著脑区，再用多模型对 atAD 与非AD 进行二分类。

Result: 在NACC数据集中召回率提升至69%（从52%），在ADNI中提升至77%（从34%），同时保持高精度。

Conclusion: 通过扩大MRI特征维度并结合Boruta统计方法，机理模型显著提升了非典型阿尔茨海默症患者的诊断召回率，验证了在临床常规检测与MRI数据下可有效区分 atAD 与非 AD 病例。

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [40] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: 低位量化会让遗忘无效。作者通过在 logits 空间引入 hinge 损失，确保遗忘信息在 4‑bit 量化后不再被重写，实验验证了效果。


<details>
  <summary>Details</summary>
Motivation: 在模型部署时常采用低位量化压缩模型，但发现量化会恢复已遗忘的数据，破坏隐私与版权保护需求。

Method: 先分析权重变化统计与量化桶重叠，发现传统遗忘更新幅度不足以跨越量化阈值；随后在 logits 空间对每个被遗忘样本强制输出与原模型至少相差半个量化步长的 hinge 损失，以保持遗忘信息不被重写。

Result: 在多语言和分类任务（含推特谣言数据集）上实验显示，提出方法在 4‑bit 量化环境下成功保持遗忘效果，而现有遗忘技术则几乎完全恢复被遗忘信息。

Conclusion: 低位量化会破坏机器学习中的遗忘效果，导致模型在被量化后恢复被遗忘信息。该研究提出的量化感知遗忘方法通过在对数空间引入 hinge 损失，保证遗忘样本在量化后仍能被区分，从而实现 4‑bit 量化下的有效数据遗忘。

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [41] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [42] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++ 针对持续测试时适应中极端漂移场景，引入熵/KL 漂移检测和自适应重置，实验证明其在 CCC 流上显著提升准确率并稳定运行。


<details>
  <summary>Details</summary>
Motivation: 在持续测试时适应（CTTA）领域，现有方法（如 Tent、EATA）虽在短期分布漂移上表现良好，但在面对持续快速或极长时序的分布演变（如 CCC 基准）时性能下降，导致模型误判甚至崩溃。该研究旨在解决此类长时间/高频漂移环境下的可靠适应问题。

Method: 提出 RDumb++，在 RDumb 的基础上加入两种漂移检测机制（熵基漂移评分与 KL 散度漂移评分）及自适应重置策略。通过漂移检测判断适应累积是否有害，并在预测崩溃前进行恢复。对漂移阈值和重置力度进行消融分析。

Result: 在 CCC-medium 的三种速度、三种种子（九个跑，包含 100 万样本）的实验中，RDumb++ 比 RDumb 高约 3% 的绝对准确率，并在整个流中保持稳定适应；消融实验表明漂移感知重置是防止崩溃并实现可靠长程 CTTA 的关键。

Conclusion: RDumb++ 通过引入漂移检测与自适应重置，可在极大规模、持续变化的测试流中实现更稳健、更可靠的持续测试时适应，为处理真实长时序分布漂移提供有效方法。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [43] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: 医疗领域的模型优化应采用临床导向的评估指标，而非单纯依赖验证损失。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习仅关注损失函数的优化，忽视了医疗场景中对模型准确性、鲁棒性等临床指标的严格要求，导致模型在临床部署时表现不佳。

Method: 通过设计两个对照实验，先用普通验证损失作为优化目标，再用临床特定指标（非可导）进行优化，并比较实验结果。

Result: 实验显示，使用临床定制指标优化的模型在相关临床任务指标上取得更优表现，展示了该方法的实用价值。

Conclusion: 本文确认在医疗健康机器学习中，使用针对临床需求量身定制的评估指标进行模型优化，能显著提高模型在临床任务上的表现，优于传统基于验证损失的优化策略。

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [44] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 本研究引入了面向不完整观测数据的神经算子学习框架，借助掩码训练和物理感知的潜在传播器，显著降低误差并适用于高缺失率场景，验证了其在三类PDE任务及气候预测中的优越性


<details>
  <summary>Details</summary>
Motivation: 充分利用不完整观测数据解决实际科学应用中的PDE求解挑战

Method: 提出Latent Autoregressive Neural Operator，包含mask-to-predict训练策略和Physics-Aware Latent Propagator两大创新组件

Result: 在POBench-PDE基准上相较现有方法实现18–69%的L2误差下降，能够应对多达75%缺失率，涵盖三类PDE任务及真实气候预测

Conclusion: 成功构建可从部分观测数据学习神经算子框架，显著提升在真实世界情境下的性能，桥接理论与实践的差距

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [45] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [46] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 论文针对1识别问题，若至少存在一臂符合阈值 μ0，利用优化方法得到拉取次数下界；随后设计的算法在保证成功率的前提下，以几乎最优的拉取次数执行。


<details>
  <summary>Details</summary>
Motivation: 解决多臂赌博机中1-identification问题中，已知阈值 μ0，至少有一臂满足条件的情况下，如何在保持高置信度的同时最小化拉取次数；此前对多臂情形的研究缺乏完善的上界分析。

Method: 作者通过构造优化问题来推导下界，并基于这一下界设计了一种自适应拉拽策略，从而得到相应的上界。

Result: 提出了新的下界并给出了一种几乎匹配的算法，上界与下界仅差多项式对数因子，完整覆盖所有问题实例；同时为多臂的情形提供了新的参考。

Conclusion: 本文在纯探索的1识别问题中针对存在至少一个合格臂的情形推导了一条新的预期拉取次数下界，并设计了一个满足置信度约束的算法，其上界与下界差距仅为多项式对数因子，实现了对所有实例的最优或近似最优性能。

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [47] [An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types](https://arxiv.org/abs/2601.15640)
*Natasha Trinkle,Huong Ha,Jeffrey Chan*

Main category: cs.LG

TL;DR: 本文在贝叶斯优化中探索迁移学习效果，提出正权重约束加权与温启动初始化两大改进策略，并在真实基准上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 利用历史相关数据提升贝叶斯优化的样本效率，使得在昂贵黑盒目标函数上能更快达到全局最优。

Method: 对比分析多种集成迁移学习贝叶斯优化方法，设计并测试了正权重约束的加权策略、温启动初始化和无效迁移处理等管道组件，并在三套实时基准上评估其效果。

Result: 实验表明，温启动初始化和正权重约束是提升性能的两大关键组件。

Conclusion: 通过引入正权重约束的加权策略、温启动初始化以及负向转移学习处理组件，本文显著提升了基于集成迁移学习的贝叶斯优化结果。

Abstract: Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.

</details>


### [48] [Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting](https://arxiv.org/abs/2601.15669)
*Jingjing Bai,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.

</details>


### [49] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [50] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 提出ZEH评估框架，揭露先进LLM的错误边界，探讨其对安全应用的启示，并给出成本优化方案。


<details>
  <summary>Details</summary>
Motivation: 指导LLM在安全关键领域的可靠部署，防止模型在看似简单任务上出现不可忽视的错误。

Method: 通过构造极简错误检测任务，逐步扩展输入规模，测算模型在无误区间上的最大阈值；随后对比不同模型的ZEH与准确率、推理能力。

Result: 发现GPT‑5.2在小字符串偶数判断和括号平衡测试中均失效；Qwen2.5的ZEH与精度相关但细节行为差异；采用树结构与在线softmax显著降低ZEH计算开销。

Conclusion: 本文提出了零错误视野（ZEH）概念，验证其对现代LLM的评估能够揭示模型在安全关键任务中的局限性，并为降低评估成本提供方法。

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [51] [Communication-efficient Federated Graph Classification via Generative Diffusion Modeling](https://arxiv.org/abs/2601.15722)
*Xiuling Wang,Xin Huang,Haibo Hu,Jianliang Xu*

Main category: cs.LG

TL;DR: CeFGC通过生成模型实现只需三轮通信即可有效训练FGNN，在非IID图数据上优于现有方案，通信成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 传统FGNN面临高通信开销和数据分布不均匀的挑战，亟需降低通信负载并提升分布式学习在非IID数据上的表现。

Method: 1) 每个客户端训练一个生成扩散模型捕获其本地图分布并上傳；2) 服务器收集模型后再广播回所有客户端；3) 客户端使用生成模型合成图与本地数据联合训练本地GNN；4) 上传本地模型权重进行聚合得到全局GNN；5) 仅进行三轮通信。

Result: 实验显示CeFGC在多种真实图数据集上明显优于现有最优方法，既减少了通信轮次，又通过多样化图生成使模型鲁棒性更强，显著提升非IID场景下的性能。

Conclusion: CeFGC通过限制服务器与客户端之间的通信轮数为三轮，并利用生成扩散模型生成高质量合成图来提升FGNN的效率和鲁棒性，显著减少通信开销并在非IID图数据上实现优越性能。

Abstract: Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.

</details>


### [52] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: GenRel-DDI 通过关系层抽象学习药物相互作用，去掉药物身份约束，显著提升在未知药物和新配对上的预测性能。


<details>
  <summary>Details</summary>
Motivation: 药物-药物相互作用（DDI）预测在药物发现和临床开发中至关重要，但现有模型多依赖药物身份，导致在未见药物和有限交互验证的真实部署场景中泛化能力差。

Method: 提出 GenRel-DDI：将 DDI 预测转化为关系中心学习，通过学习与药物身份无关的相互作用表示，捕捉可迁移的交互模式，从而实现对未知药物和新药物对的泛化。

Result: 在多项基准上进行广泛实验，GenRel-DDI 在严格实体分离评估中明显优于现有最优方法，尤其提升显著。

Conclusion: 关系学习能够显著提升 DDI 预测的稳健性与实际应用价值，证明了去除药物身份依赖的有效性。

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [53] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [54] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: 新框架GOSV通过全局优化与激活补丁识别LLM安全向量，揭示安全功能的分离路径，并开发出效果更佳的白盒逃逸攻击。


<details>
  <summary>Details</summary>
Motivation: 现有安全分析方法仅局部贪婪归因，忽视跨组件协作效应，导致对安全机制的理解不足。

Method: 通过对所有注意力头的全局优化，结合有害补丁(Harmful Patching)和零消融(Zero Ablation)两种激活补丁策略，抽取安全向量并对其进行空间区分。

Result: 发现约30%注意力头被补丁即可完全破坏安全；基于安全向量的白盒攻击在所有测试模型上显著优于现有攻击。

Conclusion: GOSV能够精准识别LLM中对安全至关重要的注意力头，并展示了安全机制在模型中的分离功能通路，验证了该框架在安全可解释性上的有效性。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [55] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 对大型AI模型进行后训练结构分析，发现大规模参数构成可分解子结构，采用结构退火方法可显著降低推理成本，实现高效并行推理。


<details>
  <summary>Details</summary>
Motivation: 当前大型AI模型的推理成本与模型规模呈指数增长，主要因推理过程被视作整体而忽略了学习过程中形成的内在结构。

Method: 通过统计后训练阶段的梯度更新事件，发现大多数参数依赖保持与初始化分布相似。随后提出后训练统计准则与结构退火方法，去除不受支持的依赖，提炼出稳定且独立的子结构。

Result: 得到一种后训练、面向模型无关的结构化视图，能够在不改动模型功能和接口的前提下，实现结构化并行推理。

Conclusion: 大型模型的推理体系本质上并非统一整体，可通过结构分解显著降低成本并提升系统可扩展性。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [56] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: IA-HVAE通过线性解码器与混合推理加速推断，提升速度和重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统HVAE在迭代推理时速度慢，解码器非线性导致深度受限；需要兼顾速度与精度以应用于实时逆问题。

Method: 结合初始经验型推断与解码梯度迭代，在变换域中构造线性可分解解码器，形成IA-HVAE。

Result: IA-HVAE实现35倍推理速度提升；在准确率和速度上均优于完全经验型和完全迭代算法，且在去模糊与去噪任务中重建质量提升。

Conclusion: IA-HVAE在速度和重建质量上均优于传统HVAE；其混合推理方案在大模型深度下实现实时推理并在逆问题中提升性能。

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [57] [Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data](https://arxiv.org/abs/2601.15977)
*Binbin Lin,Lei Zou,Hao Tian,Heng Cai,Yifan Yang,Bing Zhou*

Main category: cs.LG

TL;DR: 本研究综合医院特征、社会经济与空间因素，用五种模型预测休斯敦医院访问流量，发现Deep Gravity最优；访问受容量、ICU占用率、评分、受欢迎度及族裔、教育水平等多因素共同影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究将医院属性、人口社会经济和空间因素分离研究，缺乏整体整合以揭示访问模式的多维驱动机制。

Method: 构建并比较了五种流量预测模型（Naive Regression、Gradient Boosting、MLP、Deep Gravity、HGNN），并利用SHAP和PDP分析不同因素的联合影响。

Result: Deep Gravity模型取得最高预测准确率；发现短距离访问受便利性主导，长距离受医院评分驱动；不同族裔与教育群体对评分敏感度不同；高比例西班牙裔、黑人、未成年和老年人地区访问频率较高。

Conclusion: 深度重力模型在预测医院访问流量方面表现最佳，且医院容量、ICU占用率、评分和受欢迎程度对访问模式具有显著影响，影响随旅行距离和人口结构不同而变化。

Abstract: Healthcare visitation patterns are influenced by a complex interplay of hospital attributes, population socioeconomics, and spatial factors. However, existing research often adopts a fragmented approach, examining these determinants in isolation. This study addresses this gap by integrating hospital capacities, occupancy rates, reputation, and popularity with population SES and spatial mobility patterns to predict visitation flows and analyze influencing factors. Utilizing four years of SafeGraph mobility data and user experience data from Google Maps Reviews, five flow prediction models, Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and applied to simulate visitation flows in Houston, Texas, U.S. The Shapley additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP) method were employed to examine the combined impacts of different factors on visitation patterns. The findings reveal that Deep Gravity outperformed other models. Hospital capacities, ICU occupancy rates, ratings, and popularity significantly influence visitation patterns, with their effects varying across different travel distances. Short-distance visits are primarily driven by convenience, whereas long-distance visits are influenced by hospital ratings. White-majority areas exhibited lower sensitivity to hospital ratings for short-distance visits, while Asian populations and those with higher education levels prioritized hospital rating in their visitation decisions. SES further influence these patterns, as areas with higher proportions of Hispanic, Black, under-18, and over-65 populations tend to have more frequent hospital visits, potentially reflecting greater healthcare needs or limited access to alternative medical services.

</details>


### [58] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [59] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 本文提出条件灵活性指数（CFI）通过学习历史数据的可接受不确定性集合并利用上下文信息对集合进行条件化，利用正则化流把潜在空间中的超球体映射到数据空间，从而提升对调度决策灵活性的评估，实验证明了其在机组委派问题中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有灵活性指数仅使用简单几何不确定性集合，未考虑可用的上下文信息（如预测），导致对真正安全可行的调度决策估计不足。

Method: 采用正则化流（normalizing flow）从高斯基分布到数据分布建立双射映射，构造潜在空间中的超球体作为可接受不确定性集，然后映射回数据空间。通过引入上下文特征，使可接受集合对当前条件产生条件化，并利用历史数据学习参数化映射。

Result: 在示例研究中表明，数据驱动与条件化不确定性集合在依赖区域只包含实际实现的参数空间，但无法得出普遍优于简单集合或条件集合优于无条件集合的结论；在安全约束机组委派实验中指出，结合时间信息的CFI可提升调度质量。

Conclusion: 条件灵活性指数（CFI）通过从历史数据学习参数化的可接受不确定性集合并利用情境信息使其条件化，提供了比传统基于基本几何形状（如超立方体）的灵活性指数更具信息量的灵活性评估，并在安全约束机组委派问题中通过整合时间上下文提高了调度质量。

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [60] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: CLASP通过革新的凸投影分析，在约束在线学习中实现了损失与约束违约的对数阶最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在约束惩罚项处理上缺乏对平方罚的理论上限，尤其在强凸场景下尚未获得对数阶表现。

Method: 利用凸投影的粘几乎收缩特性，构造CLASP算法并给出对应的收敛分析。

Result: 对一般凸损失，CLASP取得归约 \(O(T^{\max\{β,1-β\}})\) 与平方罚 \(O(T^{1-β})\)；在强凸情形下，两者均满足对数阶 \(O(\log T)\) 上界。

Conclusion: CLASP算法在约束在线凸优化框架中既能最小化累计损失，又能控制约束违约的平方罚，尤其在强凸情形下实现对数阶上界。

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [61] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [62] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: 用PAC算法改进MAP推断，信息论可判定可行性，结合概率电路和随机化技术，实现高效且有理论保证的解法。


<details>
  <summary>Details</summary>
Motivation: MAP估计在概率推断中至关重要，但普遍难以求解，需在多种结构和近似约束下寻找可计算方法。

Method: 采用信息论指标描述PAC-MAP可解性，并利用概率电路进行高效实现；随机化技术既可独立使用，也能增强传统启发式方法。

Result: 实验在多种基准数据集上验证了方法的有效性，显示出性能提高和理论保证双重收益。

Conclusion: 提出了基于PAC框架的MAP推断算法，为在可变与固定计算预算下提供可证明最优解，架构及随机化策略显著提升了推断效率与鲁棒性。

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [63] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: 本文系统比较了三种以上专为Raman光谱设计的深度学习分类器，在三大公开数据集上采用统一训练协议，给出准确率与宏F1评分，为社区提供可复现的性能基准。


<details>
  <summary>Details</summary>
Motivation: 由于以往深度学习评估多为孤立或与传统机器学习方法、直观修改后的视觉模型对比，缺乏对特定Raman光谱分析模型的直接跨数据集比较，导致难以客观评估其性能。

Method: 选择三份公开的Raman数据集，设计统一的训练与超参数调优流程，对五种代表性深度学习架构进行评估，并在标准评估、微调和分布偏移测试上进行测量。

Result: 报告了各模型在不同数据集上的分类准确率和宏观平均F1分数，展示了不同架构在Raman光谱分类任务中的相对优劣。

Conclusion: 本研究提供了首批针对Raman光谱专门设计的深度学习分类器的系统基准，展现了不同模型在统一训练与超参数调优协议下的表现，为后续研究提供公平可重复的性能评估框架。

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [64] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [65] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: Beat-SSL双重上下文对比学习与软目标改进，达成多标签分类93%基线、分割任务+4%提升。


<details>
  <summary>Details</summary>
Motivation: 在标签稀缺的ECG分析领域，现有对比学习方法要么只关注全局语境，或未充分利用ECG特有特征，且采用硬对比标签无法体现信号连续相似性。

Method: 使用双层对比框架对ECG数据进行节律级与心跳级对比训练，目标采用软标签而非硬标签；随后在两类下游任务中评估模型。

Result: 在多标签分类任务上，Beat-SSL实现了基础模型93%的性能；在分割任务上，比其他三种方法均高出4%。

Conclusion: Beat-SSL通过双重上下文（节律级和心跳级）对比学习，并采用软目标，显著提升了在多标签心律评估与ECG分割任务中的表现。

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [66] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: TTT‑Discover是测试时持续强化学习框架，在多领域连续奖励任务上实现最低重叠、最快GPU核、最高算法竞赛和最优降噪性能。


<details>
  <summary>Details</summary>
Motivation: 提升科学问题求解的状态，由于传统方法在测试时不允许模型继续学习，难以针对单一问题产生最优解。

Method: 在测试时对冻结模型进行强化学习，形成专注于单一最佳解的持续学习框架，称为Test‑Time Training to Discover (TTT‑Discover)。

Result: 在数学、GPU核工程、算法设计与生物学四类连续奖励问题上广泛应用，分别在Erdős最小重叠问题、GPUMode核竞赛、AtCoder算法竞赛及单细胞降噪问题取得近乎全部领先效果，取得新工艺最快性能达2倍提升。

Conclusion: TTT‑Discover通过在测试阶段让模型持续学习并针对单一任务优化，能够在开放模型框架下得到高质量结果，验证其在多种科学任务的广泛可用性。

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [67] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: 对抗训练提升模型可解释性，并增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法侧重后置生成对抗解释，难以确保解释符合实际可行性；本研究旨在让模型直接承担解释责任，以提升决策系统的可信度和鲁棒性。

Method: 在训练过程中加入对抗解释监督，最小化模型学习表征与合乎事实、可行动的对抗解释之间的差异，从而让模型自身产生可解释且可靠的对抗解释。

Result: 实证与理论分析显示，采用对抗训练能显著提升生成解释的可行性、可操作性，并在对应的对抗攻击场景中表现出优越的鲁棒性能。

Conclusion: 该方法通过在训练阶段使用可解释的对抗例子，使模型能够自然而且高效地产生符合可操作性与可行性要求的对抗解释，并提升模型的抗干扰鲁棒性。

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [68] [Semantics in Actuation Systems: From Age of Actuation to Age of Actuated Information](https://arxiv.org/abs/2601.15496)
*Ali Nikkhah,Anthony Ephremides,Nikolaos Pappas*

Main category: eess.SY

TL;DR: 这篇论文提出Age of Actuated Information指标，解析了不同缓冲策略下的及时性，揭示了更新频率与行动及时性之间的非直线关系，并提供Geo/Geo/1队列的闭式稳态分布。


<details>
  <summary>Details</summary>
Motivation: 衡量行动及时性需要同时考虑数据老化与动作机会，AoAI专注于动作时刻的包年龄。

Method: 离散时间模型，考虑单包或无限缓冲，求平均AoA、AoAI及Geo/Geo/1队列稳态分布。

Result: 得到AoA/AoAI闭式表达式，发现冲突行为，并给出Geo/Geo/1队列稳态分布的新解析。

Conclusion: AoAI与AoA在有存储的系统中表现不同，且在某些速率下提升更新反而降低行动及时性。

Abstract: In this paper, we study the timeliness of actions in communication systems where actuation is constrained by control permissions or energy availability. Building on the Age of Actuation (AoA) metric, which quantifies the timeliness of actions independently of data freshness, we introduce a new metric, the \emph{Age of Actuated Information (AoAI)}. AoAI captures the end-to-end timeliness of actions by explicitly accounting for the age of the data packet at the moment it is actuated. We analyze and characterize both AoA and AoAI in discrete-time systems with data storage capabilities under multiple actuation scenarios. The actuator requires both a data packet and an actuation opportunity, which may be provided by a controller or enabled by harvested energy. Data packets may be stored either in a single-packet buffer or an infinite-capacity queue for future actuation. For these settings, we derive closed-form expressions for the average AoA and AoAI and investigate their structural differences. While AoA and AoAI coincide in instantaneous actuation systems, they differentiate when data buffering is present. Our results reveal counterintuitive regimes in which increasing update or actuation rates degrade action timeliness for both AoA and AoAI. Moreover, as part of the analysis, we obtain a novel closed-form characterization of the steady-state distribution of a Geo/Geo/1 queue operating under the FCFS discipline, expressed solely in terms of the queue length and the age of the head-of-line packet. The proposed metrics and analytical results provide new insights into the semantics of timeliness in systems where information ultimately serves the purpose of actuation.

</details>


### [69] [Design, Modelling, and Control of Magnetic Ball Suspension System](https://arxiv.org/abs/2601.15622)
*Sampson E. Nwachukwu*

Main category: eess.SY

TL;DR: 本文通过模型建立与三种控制策略的仿真验证，证明在磁球悬浮系统中，LQR 控制可实现稳健且低能耗的悬浮性能。


<details>
  <summary>Details</summary>
Motivation: 磁球悬浮系统是典型的非线性不稳定主动电机系统，能够实现无摩擦悬浮，广泛用于精密技术，但其不稳定性和非线性特点使得传统控制难以满足性能要求。

Method: 首先建立机械与电气耦合的状态空间模型，利用雅可比矩阵进行反馈线性化，随后进行可控性与可观测性分析。接着设计三种控制方案：极点配置状态反馈、全阶观测器、线性二次调节器（LQR），分别在线性化与非线性模型上通过 Simulink 仿真验证其效果。

Result: 仿真显示，线性化系统在极点配置与 LQR 下能保持低振动；非线性系统在启动阶段出现显著瞬态振荡；全阶观测器提升了状态估计精度，便于不直接测量状态时的控制；LQR 在多种情形下提供了更佳的鲁棒性与最小控制量。

Conclusion: 论文表明，通过状态空间建模、雅可比矩阵线性化及反馈线性化，可对磁球悬浮系统实现稳定控制；三种控制方案（极点配置、全阶观测器、LQR）均能在仿真中实现预期性能，其中LQR在鲁棒性和能耗方面表现更佳。

Abstract: This paper presents the modeling, control design, and performance analysis of a Magnetic Ball Suspension System (MBSS), a nonlinear and inherently unstable electromechanical system used in various precision applications. The system's primary objective is to levitate a steel ball using electromagnetic force without physical contact, thereby eliminating frictional losses. A comprehensive state-space model was developed, capturing both the mechanical and electrical dynamics. The equilibrium points of the system were determined through feedback linearization using the Jacobian matrix. To ensure system stability, controllability and observability analyses were conducted, confirming that state feedback and observer-based control strategies could be effectively implemented. Three distinct control methods were explored: pole placement-based state feedback control, full-order observer design, and optimal state feedback control using the Linear Quadratic Regulator (LQR). Each control strategy was validated through Simulink simulations for both linearized and nonlinear models. Simulation results demonstrated that the linearized system consistently achieved desired performance with minimal oscillations, whereas the nonlinear system exhibited significant transient oscillations before stabilization. The full-order observer enhanced estimation accuracy, enabling effective control where direct state measurement was impractical. The LQR-based control offered improved robustness and minimized control effort, though its performance was comparable to standard state feedback in some cases.

</details>


### [70] [Bridging Qualitative Rubrics and AI: A Binary Question Framework for Criterion-Referenced Grading in Engineering](https://arxiv.org/abs/2601.15626)
*Lili Chen,Winn Wing-Yiu Chow,Stella Peng,Bencheng Fan,Sachitha Bandara*

Main category: eess.SY

TL;DR: 研究将GenAI与二元评分框架结合，验证其准确率与人类相近，显著提升形成性反馈，但需进一步优化以适用于多样化解法，并探讨学生态度。


<details>
  <summary>Details</summary>
Motivation: 解决人工评分耗时且易漏失细小错误的问题，探索GenAI如何提升评分效率与反馈质量。

Method: 基于人为示范者与人工分类的实验，利用GenAI进行二元问题评分与错误检测，并比较两位经验丰富人类评分者的准确率及对工具的感知。

Result: GenAI评分准确率92.5%，与两名专业评分人相当；工具被视为有助于第二次审核、改善错误检测和填补反馈缺口，但在自主使用方面尚不可靠，尤其针对非传统解法。

Conclusion: 本研究证明，当GenAI与结构化的基准式评分框架和二元问题相结合时，可在工程数学评估中实现与人类专家相当的评分准确率，并嵌入高质量可扩展的形成性反馈；但仍需改进以适用于非传统解法并考察学生态度。

Abstract: PURPOSE OR GOAL: This study investigates how GenAI can be integrated with a criterion-referenced grading framework to improve the efficiency and quality of grading for mathematical assessments in engineering. It specifically explores the challenges demonstrators face with manual, model solution-based grading and how a GenAI-supported system can be designed to reliably identify student errors, provide high-quality feedback, and support human graders. The research also examines human graders' perceptions of the effectiveness of this GenAI-assisted approach. ACTUAL OR ANTICIPATED OUTCOMES: The study found that GenAI achieved an overall grading accuracy of 92.5%, comparable to two experienced human graders. The two researchers, who also served as subject demonstrators, perceived the GenAI as a helpful second reviewer that improved accuracy by catching small errors and provided more complete feedback than they could manually. A central outcome was the significant enhancement of formative feedback. However, they noted the GenAI tool is not yet reliable enough for autonomous use, especially with unconventional solutions. CONCLUSIONS/RECOMMENDATIONS/SUMMARY: This study demonstrates that GenAI, when paired with a structured, criterion-referenced framework using binary questions, can grade engineering mathematical assessments with an accuracy comparable to human experts. Its primary contribution is a novel methodological approach that embeds the generation of high-quality, scalable formative feedback directly into the assessment workflow. Future work should investigate student perceptions of GenAI grading and feedback.

</details>


### [71] [Stability Analysis of Power-Electronics-Dominated Grids Using Scaled Relative Graphs](https://arxiv.org/abs/2601.16014)
*Eder Baron-Prada,Adolfo Anta,Florian Dörfler*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a novel approach to stability analysis for grid-connected converters utilizing Scaled Relative Graphs (SRG). Our method effectively decouples grid and converter dynamics, thereby establishing a comprehensive and efficient framework for evaluating closed-loop stability. Our analysis accommodates both linear and non-linear loads, enhancing its practical applicability. Furthermore, we demonstrate that our stability assessment remains unaffected by angular variations resulting from dq-frame transformations, significantly increasing the method's robustness and versatility. The effectiveness of our approach is validated in several simulation case studies, which illustrate its broad applicability in modern power systems.

</details>


### [72] [Dynamic Tactile Sensing System and Soft Actor Critic Reinforcement Learning for Inclusion Characterization](https://arxiv.org/abs/2601.16061)
*John Bannan,Nazia Rahman,Chang-Hee Won*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents the Dynamic Tactile Sensing System that utilizes robotic tactile sensing in conjunction with reinforcement learning to locate and characterize embedded inclusions. A dual arm robot is integrated with an optical Tactile Imaging Sensor that utilizes the Soft Actor Critic Algorithm to acquire tactile data based on a pixel intensity reward. A Dynamic Interrogation procedure for tactile exploration is developed that enables the robot to first localize inclusion and refine their positions for precise imaging. Experimental validation conducted on Polydimethylsiloxane phantoms demonstrates that the robot using the Tactile Soft Actor Critic Model was able to achieve size estimation errors of 2.61% and 5.29% for soft and hard inclusions compared to 7.84% and 6.87% for expert human operators. Results also show that Dynamic Tactile Sensing System was able to locate embedded inclusions and autonomously determine their mechanical properties, useful in applications such as breast tumor characterization.

</details>


### [73] [Interconnection-based Model Reduction for Linear Hybrid Systems](https://arxiv.org/abs/2601.16149)
*Zirui Niu,Giordano Scarciotti,Alessandro Astolfi*

Main category: eess.SY

TL;DR: 提出利用瞬变匹配的互联系统技术，对线性混合系统进行降阶，以同时满足直接与交换互连，并在周期跳跃下可简化实现。


<details>
  <summary>Details</summary>
Motivation: 在混合系统模型中常需降低模型维度以便仿真与控制设计，传统方法缺少针对混合系统双互连匹配的理论，因此提出新的降阶方法以满足瞬变匹配需求。

Method: 利用基于互联系统的刻度匹配技术，分别处理直接和交换互连两种经典互连；通过对稳态响应的混合表征构建降阶模型，并将两种互连结果综合得到统一模型。

Result: 给出了两类互连的降阶模型族，并证明在跳跃周期性条件下可进一步简化；通过数值仿真验证了方法的有效性。

Conclusion: 设计出一组可同时满足直接与交换互连瞬变匹配的线性混合系统降阶模型，且在跳跃周期性时可简化实现。

Abstract: In this paper, we address the model reduction problem for linear hybrid systems via the interconnection-based technique called moment matching. We consider two classical interconnections, namely the direct and swapped interconnections, in the hybrid setting, and we present families of reduced-order models for each interconnection via a hybrid characterisation of the steady-state responses. By combining the results for each interconnection, the design of a reduced-order model that achieves moment matching simultaneously for both interconnections is studied. In addition, we show that the presented results have simplified counterparts when the jumps of the hybrid system are periodic. A numerical simulation is finally given to illustrate the results.

</details>


### [74] [Stochastic Control Barrier Functions under State Estimation: From Euclidean Space to Lie Groups](https://arxiv.org/abs/2601.16198)
*Ruoyu Lin,Magnus Egerstedt*

Main category: eess.SY

TL;DR: 该工作为在存在噪声状态估计的随机无人系统中提供可证明安全性的CBF方法。


<details>
  <summary>Details</summary>
Motivation: 在无人系统安全保障中，传统CBF未考虑不确定性与随机系统在流形上的演化，导致安全保证不足

Method: 将状态估计不确定性（过程噪声与测量噪声）显式引入控制障碍函数，设计可适应不确定度级别的控制器，并在线性系统中给出闭式解

Result: 成功推导出安全概率上界，并在从欧氏空间到李群的多种系统上实现实验验证，证明框架有效

Conclusion: 提供了可证明的有限时间安全概率上界，并展示了在噪声状态信息下实现安全控制的可行性

Abstract: Ensuring safety for autonomous systems under uncertainty remains challenging, particularly when safety of the true state is required despite the true state not being fully known. Control barrier functions (CBFs) have become widely adopted as safety filters. However, standard CBF formulations do not explicitly account for state estimation uncertainty and its propagation, especially for stochastic systems evolving on manifolds. In this paper, we propose a safety-critical control framework with a provable bound on the finite-time safety probability for stochastic systems under noisy state information. The proposed framework explicitly incorporates the uncertainty arising from both process and measurement noise, and synthesizes controllers that adapt to the level of uncertainty. The framework admits closed-form solutions in linear settings, and experimental results demonstrate its effectiveness on systems whose state spaces range from Euclidean space to Lie groups.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [75] [ISAC-over-NTN: HAPS-UAV Framework for Post-Disaster Responsive 6G Networks](https://arxiv.org/abs/2601.15422)
*Berk Ciloglu,Ozgun Ersoy,Metin Ozturk,Ali Gorcin*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In disaster scenarios, ensuring both reliable communication and situational awareness becomes a critical challenge due to the partial or complete collapse of terrestrial networks. This paper proposes an integrated sensing and communication (ISAC) over non-terrestrial networks (NTN) architecture referred to as ISAC-over-NTN that integrates multiple uncrewed aerial vehicles (UAVs) and a high-altitude platform station (HAPS) to maintain resilient and reliable network operations in post-disaster conditions. We aim to achieve two main objectives: i) provide a reliable communication infrastructure, thereby ensuring the continuity of search-and-rescue activities and connecting people to their loved ones, and ii) detect users, such as those trapped under rubble or those who are mobile, using a Doppler-based mobility detection model. We employ an innovative beamforming method that simultaneously transmits data and detects Doppler-based mobility by integrating multi-user multiple-input multiple-output (MU-MIMO) communication and monostatic sensing within the same transmission chain. The results show that the proposed framework maintains reliable connectivity and achieves high detection accuracy of users in critical locations, reaching 90% motion detection sensitivity and 88% detection accuracy.

</details>


### [76] [Applicability and Limitation Analysis of PMU Data and Phasor Concept for Low- and High- Frequency Oscillations](https://arxiv.org/abs/2601.15529)
*Bowen Ou,Bin Wang,Slava Maslennikov,Hanchao Liu,Jim Follum*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Phasor Measurement Units (PMUs) convert high-speed waveform data into low-speed phasor data, which are fundamental to wide-area monitoring and control in power systems, with oscillation detection and localization among their most prominent applications. However, representing electrical waveform signals with oscillations using PMU phasors is effective only for low-frequency oscillations. This paper investigates the root causes of this limitation, focusing on errors introduced by Discrete Fourier Transform (DFT)-based signal processing, in addition to the attenuation effects of anti-aliasing filters, and the impact of low reporting rates. To better represent and estimate waveform signals with oscillations, we propose a more general signal model and a multi-step estimation method that leverages one-cycle DFT, the Matrix Pencil Method, and the Least Squares Method. Numerical experiments demonstrate the superior performance of the proposed signal model and estimation method. Furthermore, this paper reveals that the phasor concept, let alone PMU phasors, can become invalid for waveform signals with high-frequency oscillations characterized by asymmetric sub- and super-synchronous components. These findings highlight the fundamental limitations of PMU data and phasor concept, and emphasize the need to rely on waveform data for analyzing high-frequency oscillations in modern power systems.

</details>


### [77] [An Iterated Hybrid Fast Parallel FIR Filter](https://arxiv.org/abs/2601.15582)
*Keshab K. Parhi*

Main category: eess.SP

TL;DR: 本文设计了一种混合结构的快速并行FIR滤波器，利用迭代快速FIR算法减少硬件复杂度，改进加法量低于先前方案。


<details>
  <summary>Details</summary>
Motivation: 在DSP应用中并行FIR滤波器能提升计算效率与吞吐量，但现有设计在硬件复杂度与运算量上仍有改进空间，需寻找更低成本的实现方式。

Method: 采用多相分解技术与迭代快速FIR算法相结合，内层层使用转置型2-并行快速FIR滤波器，外层使用直接型2-并行快速FIR滤波器，形成混合迭代结构。

Result: 实验表明，所提出的混合快速并行滤波器在硬件资源消耗和加法次数方面均优于之前的设计。

Conclusion: 该论文提出一种新的迭代快速并行FIR滤波器——快速混合滤波器，在保留性能的同时降低了硬件复杂度并减少了加法次数。

Abstract: This paper revisits the design and optimization of parallel fast finite impulse response (FIR) filters using polyphase decomposition and iterated fast FIR algorithms (FFAs). Parallel FIR filtering enhances computational efficiency and throughput in digital signal processing (DSP) applications by enabling the simultaneous processing of multiple input samples. We revisit a prior approach to design of fast parallel filter architectures by using the iterated FFA approach where the same primitive filter, such as 2-parallel, is iterated to design the fast parallel filter. In this paper, we present yet another novel iterated fast parallel FIR filter, referred to as the fast hybrid filter. The hybrid filter iterates a transposed 2-parallel fast FIR filter in all the inner layers and a direct-form 2-parallel fast FIR filter in the outermost layer, resulting in reduced hardware complexity. Such an iterated hybrid approach has not been presented before. We show that the hybrid fast parallel filters require less number of additions compared to prior approaches.

</details>


### [78] [Amalgamated CHIRP and OFDM for ISAC](https://arxiv.org/abs/2601.15584)
*Pankaj Kumar,Mohammed El-Hajjar,Ibrahim A. Hemadeh,Yasser Mestrah,Suraj Srivastava,Aditya K. Jagannatham,Lajos Hanzo*

Main category: eess.SP

TL;DR: 提出OFDM+啁啾仿射波形，降低PAPR，提升感知精度，无需分配感知资源，整体性能优于传统方案。


<details>
  <summary>Details</summary>
Motivation: 传统OFDM在感知与通信双重需求中需划分资源，导致通信性能下降；同时OFDM高PAPR限制了发射功率。故需开发一种兼顾高效通信和低PAPR、同时支持感知的波形。

Method: 采用仿射加法将OFDM与啁啾波形叠加，形成常数包络OFDM信号；在时隙级别将啁啾信号嵌入OFDM调制，以实现无资源占用的感知；并通过该波形的自相关特性提升距离估计与速度估计。

Result: 仿射复合波形实现了更小的PAPR、更优的自相关峰值、更低的距离与速度RMSE，并在通信与感知之间实现了更佳的权衡。

Conclusion: 该论文提出的基于OFDM与啁啾波形的仿射复合波形能在统一感知与通信框架下同时提升通信性能与雷达检测精度，并通过降低PAPR保持信号发射效率。

Abstract: Integrated Sensing and Communication (ISAC) requires the development of a waveform capable of efficiently supporting both communication and sensing functionalities. This paper proposes a novel waveform that combines the benefits of both the orthogonal frequency division multiplexing (OFDM) and the chirp waveforms to improve both the communication and sensing performance within an ISAC framework. Hence, a new architecture is proposed that utilizes the conventional communication framework while leveraging the parameters sensed at the receiver (Rx) for enhancing the communication performance. We demonstrate that the affine addition of OFDM and chirp signals results in a near constant-envelope OFDM waveform, which effectively reduces the peak-to-average power ratio (PAPR), a key limitation of traditional OFDM systems. Using the OFDM framework for sensing in the conventional fashion requires the allocation of some resources for sensing, which in turn reduces communication performance. As a remedy, the proposed affine amalgam facilitates sensing through the chirp waveform without consuming communication resources, thereby preserving communication efficiency. Furthermore, a novel technique of integrating the chirp signal into the OFDM framework at the slot-level is proposed to enhance the accuracy of range estimation. The results show that the OFDM signal incorporated with chirp has better autocorrelation properties, improved root mean square error (RMSE) of range and velocity, and lower PAPR. Finally, we characterize the trade-off between communications and sensing performance.

</details>


### [79] [Does 6G Need a New Waveform: Comparing Zak-OTFS with CP-OFDM](https://arxiv.org/abs/2601.15602)
*Imran Ali Khan,Saif Khan Mohammed,Ronny Hadani,Ananthanarayanan Chockalingam,Robert Calderbank,Anton Monk,Shachar Kons,Shlomo Rakib,Yoav Hebron*

Main category: eess.SP

TL;DR: 本文对CP-OFDM和Zak-OTFS在6G不同传播条件下的性能进行综合比较，揭示在高延迟/多普勒场景中Zak-OTFS更优，但最终选择取决于业务场景。


<details>
  <summary>Details</summary>
Motivation: 随着6G出现多普勒/延迟扩展大、移动性强的场景，传统OFDM受ICI影响显著，需要评估替代波形Zak-OTFS的可行性与性能优势。

Method: 在理论分析与仿真模拟基础上，对两种波形在不同传播环境下的ICI、ISI以及I/O关系进行对比评估。

Result: 在高移动性、宽延迟和大单元的场景中，Zak-OTFS相较于CP-OFDM在误码率、时频资源利用率等指标上更具优势；在低延迟/低多普勒环境下两者表现相近。

Conclusion: 本文比较了CP-OFDM与Zak-OTFS在完整6G传播环境中的性能，指出在高延迟/多普勒传播环境下Zak-OTFS表现优越，但架构选择仍需根据典型使用场景决定。

Abstract: Across the world, there is growing interest in new waveforms, Zak-OTFS in particular, and over-the-air implementations are starting to appear. The choice between OFDM and Zak-OTFS is not so much a choice between waveforms as it is an architectural choice between preventing inter-carrier interference (ICI) and embracing ICI. In OFDM, once the Input-Output (I/O) relation is known, equalization is relatively simple, at least when there is no ICI. However, in the presence of ICI the I/O relation is non-predictable and its acquisition is non-trivial. In contrast, equalization is more involved in Zak-OTFS due to inter-symbol-interference (ISI), however the I/O relation is predictable and its acquisition is simple. {Zak-OTFS exhibits superior performance in doubly-spread 6G use cases with high delay/Doppler channel spreads (i.e., high mobility and/or large cells), but architectural choice is governed by the typical use case, today and in the future. What is typical depends to some degree on geography, since large delay spread is a characteristic of large cells which are the rule rather than the exception in many important wireless markets.} This paper provides a comprehensive performance comparison of cyclic prefix OFDM (CP-OFDM) and Zak-OTFS across the full range of 6G propagation environments. The performance results provide insights into the fundamental architectural choice.

</details>


### [80] [Joint Pilot and Unknown Data-based Localization for OFDM Opportunistic Radar Systems](https://arxiv.org/abs/2601.15785)
*Mathieu Reniers,Martin Willame,Jérôme Louveaux,Luc Vandendorpe*

Main category: eess.SP

TL;DR: 通过FFT快速估计，无需解码即可利用通信数据符号提升位置信息提取。


<details>
  <summary>Details</summary>
Motivation: 当前方案要么仅利用已知导频忽略数据符号，要么依赖数据判决受限于通信性能，缺乏充分利用数据信息的策略。

Method: 构建使用均匀线性阵列的旁观雷达，利用通信信号的随机数据符号，通过FFT实现高效定位估计。

Result: 数值仿真表明该方法在多用户环境下比现有方案取得更优的定位性能。

Conclusion: 提出一种在不解码数据包的情况下提取位置信息的新方法，显著提升了定位精度。

Abstract: Integrated Sensing and Communications (ISAC) has emerged as a promising paradigm for Sixth Generation (6G) and Wi-Fi 7 networks, with the communication-centric approach being particularly attractive due to its compatibility with current standards. Typical communication signals comprise both deterministic known pilot signals and random unknown data payloads. Most existing approaches either rely solely on pilots for positioning, thereby ignoring the radar information present in the received data symbols that constitute the majority of each frame, or rely on data decisions, which bounds positioning performance to that of the communication system. To overcome these limitations, we propose a novel method that extracts positioning information from data payloads without decoding them. We consider an opportunistic scenario in which communication signals from a user are captured by an opportunistic radar equipped with a Uniform Linear Arrays of antennas. We show that, in this setting, the estimation can be efficiently implemented using Fast Fourier Transforms. Finally, we demonstrate superior localization performance compared to existing methods in the literature through numerical simulations.

</details>


### [81] [Dual-Mapping Sparse Vector Transmission for Short Packet URLLC](https://arxiv.org/abs/2601.15819)
*Yanfeng Zhang,Xu Zhu,Jinkai Zheng,Weiwei Yang,Xianhua Yu,Haiyong Zeng,Yujie Liu,Yong Liang Guan*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sparse vector coding (SVC) is a promising short-packet transmission method for ultra reliable low latency communication (URLLC) in next generation communication systems. In this paper, a dual-mapping SVC (DM-SVC) based short packet transmission scheme is proposed to further enhance the transmission performance of SVC. The core idea behind the proposed scheme lies in mapping the transmitted information bits onto sparse vectors via block and single-element sparse mappings. The block sparse mapping pattern is able to concentrate the transmit power in a small number of non-zero blocks thus improving the decoding accuracy, while the single-element sparse mapping pattern ensures that the code length does not increase dramatically with the number of transmitted information bits. At the receiver, a two-stage decoding algorithm is proposed to sequentially identify non-zero block indexes and single-element non-zero indexes. Extensive simulation results verify that proposed DM-SVC scheme outperforms the existing SVC schemes in terms of block error rate and spectral efficiency.

</details>


### [82] [Separable Delay And Doppler Estimation In Passive Radar](https://arxiv.org/abs/2601.15821)
*Mats Viberg,Daniele Gerosa,Tomas McKelvey,Patrik Dammert,Thomas Eriksson*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In passive radar, a network of distributed sensors exploit signals from so-called Illuminators-of-Opportunity to detect and localize targets. We consider the case where the IO signal is available at each receiver node through a reference channel, whereas target returns corrupted by interference are collected in a separate surveillance channel. The problem formulation is similar to an active radar that uses a noise-like waveform, or an integrated sensing and communication application. The available data is first split into batches of manageable size. In the direct approach, the target's time-delay and Doppler parameters are estimated jointly by incoherently combining the batch-wise data. We propose a new method to estimate the time-delay separately, thus avoiding a costly 2-D search. Our approach is designed for slowly moving targets, and the accuracy of the time-delay estimate is similar to that of the full batch-wise 2-D method. Given the time-delay, the coherency between batches can be restored when estimating the Doppler parameter. Thereby, the separable approach is found to yield superior Doppler estimates over a wide parameter range. In addition to reducing computational complexity, the proposed separable estimation technique also significantly reduces the communication overhead in a distributed radar setting.

</details>


### [83] [Performance Analysis of Digital Beamforming mmWave MIMO with Low-Resolution DACs/ADCs](https://arxiv.org/abs/2601.15831)
*Faruk Pasic,Mariam Mussbah,Stefan Schwarz,Markus Rupp,Fredrik Tufvesson,Christoph F. Mecklenbräuker*

Main category: eess.SP

TL;DR: 4 位 DAC/ADC 在全数字毫米波 MIMO 波束成形中能实现能量与速率的最佳折中，值得考虑。


<details>
  <summary>Details</summary>
Motivation: 为满足毫米波 MIMO 频段下高数据率与低延迟应用对全数字波束成形的需求，同时提高能源效率，需要在系统中采用低分辨率 ADC/DAC，但量化失真会削弱性能，亟需评估其对信道估计及整体系统性能的影响。

Method: 通过在实际系统约束下对低分辨率量化的毫米波 MIMO 系统进行信道估计性能分析，并以光谱效率（SE）和能量效率（EE）作为评估指标，利用仿真验证不同分辨率对系统性能的影响。

Result: 仿真表明，4 位量化提供了能量消耗较低且可实现的数据速率之间的最佳平衡，低于此分辨率时，能量优势不足；高于此分辨率时，能量消耗显著增加但性能提升有限。

Conclusion: 本研究表明，在完全数字的毫米波 MIMO 波束成形系统中，采用4位低分辨率的 DAC/ADC 能够实现能量消耗与可达数据速率之间的良好折中，从而兼顾能源效率与光谱效率。

Abstract: Future wireless communications will rely on multiple-input multiple-output (MIMO) beamforming operating at millimeter wave (mmWave) frequency bands to deliver high data rates. To support flexible spatial processing and meet the demands of latency critical applications, it is essential to use fully digital mmWave MIMO beamforming, which relies on accurate channel estimation. However, ensuring power efficiency in fully digital mmWave MIMO systems requires the use of low-resolution digital-to-analog converters (DACs) and analog-to-digital converters (ADCs). The reduced resolution of these quantizers introduces distortion in both transmitted and received signals, ultimately degrading system performance. In this paper, we investigate the channel estimation performance of mmWave MIMO systems employing fully digital beamforming with low-resolution quantization, under practical system constraints. We evaluate the system performance in terms of spectral efficiency (SE) and energy efficiency (EE). Simulation results demonstrate that a moderate quantization resolutions of 4-bit per DAC/ADC offers a favorable trade-off between energy consumption and achievable data rate.

</details>


### [84] [Time-Varying Rician K-factor in Measured Vehicular Channels at cmWave and mmWave Bands](https://arxiv.org/abs/2601.15863)
*Faruk Pasic,Markus Hofer,Thomas Zemen,Andreas F. Molisch,Christoph F. Mecklenbräuker*

Main category: eess.SP

TL;DR: 多频段V2I测量显示，3.2 GHz、34.3 GHz及62.35 GHz的K‑因子相似，并与RMS时延扩展相关。


<details>
  <summary>Details</summary>
Motivation: 未来车联网需要在毫米波与常规厘米波频段之间研究传播效应与小尺度衰落差异，以提升数据传输速率。

Method: 在城市街道环境下，使用155.5 MHz带宽、31.25 µs声测重复率的多频段通道测量，获取3.2 GHz、34.3 GHz和62.35 GHz三组频段数据，分析其时间变动的Rician K‑因子及与RMS时延扩展的关系。

Result: Rician K‑因子在不同频段（3.2 GHz、34.3 GHz、62.35 GHz）上相近，并与RMS时延扩展呈相关性。

Conclusion: K‑因子与频率无显著差异，且可由时延扩展预测，体现了毫米波与厘米波在V2I小尺度衰落特性上的相似性。

Abstract: Future vehicular communication systems will integrate millimeter wave (mmWave) technology to enhance data transmission rates. To investigate the propagation effects and small-scale fading differences between mmWave and conventional centimeter wave (cmWave) bands, multi-band channel measurements have to be conducted. One key parameter to characterize small-scale fading is the Rician K-factor. In this paper, we analyze the time-varying K-factor of vehicle-to-infrastructure (V2I) channels across multiple frequency bands, measured in an urban street environment. Specifically, we investigate three frequency bands with center frequencies of 3.2 GHz, 34.3 GHz and 62.35 GHz using measurement data with 155.5 MHz bandwidth and a sounding repetition rate of 31.25 μs. Furthermore, we analyze the relationship between K-factor and root-mean-square (RMS) delay spread. We show that the Ricean K-factor is similar at different frequency bands and that is correlated with the RMS delay spread.

</details>


### [85] [Reconstructing Patched or Partial Holograms to allow for Whole Slide Imaging with a Self-Referencing Holographic Microscope](https://arxiv.org/abs/2601.15952)
*Philip Groult,Julia D. Sistermanns,Ellen Emken,Oliver Hayden,Wolfgang Utschick*

Main category: eess.SP

TL;DR: 论文研发了一种基于三波自参数字全息的重构算法，能通过单次全息图像完成宫颈细胞全切片成像，已在实验中验证成功。


<details>
  <summary>Details</summary>
Motivation: 结合全切片成像（WSI）与定量相位成像（QPI）的优势，克服两者尚未整合的问题，以更丰富的细胞信息实现无损细胞检测。

Method: 使用自参三波数字全息显微镜获取单次全息图像，并设计适配多种输入（部分全息图和拼接全息图）的重构算法，依据全切片成像需要进行自适应重建。

Result: 实验结果表明，该重构算法在测试的上皮细胞样本上具有良好的重建质量，能够实现有效的全切片成像。

Conclusion: 该论文提出了一种自参三波数字全息显微镜的重构算法，使得能够通过单图像实现宫颈细胞涂片的全切片成像，并成功在实验中验证了该算法对上皮细胞的有效性。

Abstract: The last decade has seen significant advances in computer-aided diagnostics for cytological screening, mainly through the improvement and integration of scanning techniques such as whole slide imaging (WSI) and the combination with deep learning. Simultaneously, new imaging techniques such as quantitative phase imaging (QPI) are being developed to capture richer cell information with less sample preparation. So far, the two worlds of WSI and QPI have not been combined. In this work, we present a reconstruction algorithm which makes whole slide imaging of cervical smears possible by using a self-referencing three-wave digital holographic microscope. Since a WSI is constructed by combining multiple patches, the algorithm is adaptive and can be used on partial holograms and patched holograms. We present the algorithm for a single shot hologram, the adaptations to make it flexible to various inputs and show that the algorithm performs well for the tested epithelial cells. This is a preprint of our paper, which has been accepted for publication in 2026 IEEE International Symposium on Biomedical Imaging (ISBI).

</details>


### [86] [Performance Scaling Laws for PD Array-based Receivers in IM/DD Optical Wireless Communication Systems](https://arxiv.org/abs/2601.15973)
*Aravindh Krishnamoorthy,Robert Schober,Harald Haas*

Main category: eess.SP

TL;DR: PD阵列只有在束宽窄且SNR足够高时才有性能提升；增加PD数量不够，必须综合优化波束、模式、功率和部署位置。


<details>
  <summary>Details</summary>
Motivation: 为下一代高带宽PD阵列接收机提供实用设计准则，解决在多PD系统中功率、噪声与波束匹配的关键问题。

Method: 基于光电功率的平方律，构建PD阵列与单一PD参考接收机的SNR与可达率模型；通过解析推导和数值仿真验证性能扩展规律，并探索设计参数的联合优化。

Result: PD阵列在窄束和高SNR场景下可显著提升SNR和可达率，但单纯扩增PD数目无效；系统性能取决于波束模式、电磁场模式、接收功率与PD部署的综合优化。

Conclusion: 仅在束宽足够窄且信噪比超过阈值时，光电探测器（PD）阵列才可获得性能提升；单纯增加PD数量并不能提高性能，需要联合优化波束角度、模式、接收功率和PD位置。

Abstract: We study the performance scaling laws for electrical-domain combining in photodetector (PD) array-based receivers employing intensity modulation and direct detection, taking into account the inherent square-law relationship between the optical and electrical received powers. The performance of PD array-based systems is compared, in terms of signal-to-noise ratio (SNR) and achievable rate, to that of a reference receiver employing a single PD. Analytical and numerical results show that PD arrays provide performance gains for sufficiently narrow beams and above an SNR threshold. Furthermore, increasing the number of PDs alone does not enhance performance, and joint optimization of beam pattern, transverse electromagnetic mode, received power, and PD positions is necessary. Our model and derived insights provide practical guidelines and highlight the trade-offs for the design of next-generation high-bandwidth PD array receivers.

</details>


### [87] [Graph Topology Identification Based on Covariance Matching](https://arxiv.org/abs/2601.15999)
*Yongsheng Han,Raj Thilak Rajan,Geert Leus*

Main category: eess.SP

TL;DR: 提出 CovMatch：利用协方差匹配进行图拓扑识别；适用于无向/有向稀疏图；转化为整数锥规划或正交矩阵优化；实验验证性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统 GTI 方法往往依赖概率模型或复杂非凸优化，限制了其应用；寻找更通用、更高效且假设更少的拓扑识别框架成为迫切需求。

Method: 将经验协方差与理论协方差对齐，并通过重参数化把图学习转化为无向图的混合整数锥规划或有向图的正交矩阵优化。

Result: 实验表明，CovMatch 在相对较大的图上能够高效恢复真实拓扑，并在准确率上优于常规基准。

Conclusion: CovMatch 通过协方差匹配提供了一种统一且可扩展的图拓扑识别方法，可在无需显式假设无环性或正权重的前提下，准确恢复线性结构方程模型中的无向或有向稀疏图。

Abstract: Graph topology identification (GTI) is a central challenge in networked systems, where the underlying structure is often hidden, yet nodal data are available. Conventional solutions to address these challenges rely on probabilistic models or complex optimization formulations, commonly suffering from non-convexity or requiring restrictive assumptions on acyclicity or positivity. In this paper, we propose a novel covariance matching (CovMatch) framework that directly aligns the empirical covariance of the observed data with the theoretical covariance implied by an underlying graph. We show that as long as the data-generating process permits an explicit covariance expression, CovMatch offers a unified route to topology inference.
  We showcase our methodology on linear structural equation models (SEMs), showing that CovMatch naturally handles both undirected and general sparse directed graphs - whether acyclic or positively weighted - without explicit knowledge of these structural constraints. Through appropriate reparameterizations, CovMatch simplifies the graph learning problem to either a conic mixed integer program for undirected graphs or an orthogonal matrix optimization for directed graphs. Numerical results confirm that, even for relatively large graphs, our approach efficiently recovers the true topology and outperforms standard baselines in accuracy. These findings highlight CovMatch as a powerful alternative to log-determinant or Bayesian methods for GTI, paving the way for broader research on learning complex network topologies with minimal assumptions.

</details>


### [88] [Low-Complexity Sparse Superimposed Coding for Ultra Reliable Low Latency Communications](https://arxiv.org/abs/2601.16012)
*Yanfeng Zhang,Xi'an Fan,Xu Zhu,Jinkai Zheng,Hui Liang,Weiwei Yang,Tom H. Luan*

Main category: eess.SP

TL;DR: 稀疏码本+MPMP的SSC方案兼顾BLER与复杂度，鲁棒性佳，适合短包传输。


<details>
  <summary>Details</summary>
Motivation: 在超可靠低时延通信场景下，传统稀疏叠加编码（SSC）方案由于使用密集码本矩阵导致编码与解码复杂度高昂，亟需低复杂度解决方案。

Method: 通过设计仅含少量非零元素的稀疏码本结构，并采用传统多路径匹配追踪(MPMP)算法进行解码，充分利用码本稀疏性显著降低总体复杂度。

Result: 仿真结果表明，该方案在块错误率（BLER）表现与计算复杂度之间实现了良好折中，并在不同传输块长度下保持了强鲁棒性。

Conclusion: 提出的低复杂度SSC方案在保持优异BLER性能的同时，大幅降低了编码与解码开销，适用于超可靠低时延通信需求。

Abstract: Sparse superimposed coding (SSC) has emerged as a promising technique for short-packet transmission in ultra-reliable low-latency communication scenarios. However, conventional SSC schemes often suffer from high encoding and decoding complexity due to the use of dense codebook matrices. In this paper, we propose a low-complexity SSC scheme by designing a sparse codebook structure, where each codeword contains only a small number of non-zero elements. The decoding is performed using the traditional multipath matching pursuit algorithm, and the overall complexity is significantly reduced by exploiting the sparsity of the codebook. Simulation results show that the proposed scheme achieves a favorable trade-off between BLER performance and computational complexity, and exhibits strong robustness across different transmission block lengths.

</details>


### [89] [Hybrid Channel Estimation with Quantized Phase Feedback for Over-the-Air Computation](https://arxiv.org/abs/2601.16054)
*Martin Dahl,Erik G. Larsson*

Main category: eess.SP

TL;DR: 提出一种混合式渠道估计方案，利用互惠和相位反馈的优势，第二个变体在仿真中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 简化空中计算的信号开销

Method: 结合基于互惠的通道估计与反馈通道估计，并分别量化相位反馈

Result: 第二种变体在相位的互惠估计与最优相量化下，能优于仅通过反馈估计相位的第一种变体

Conclusion: 提供了分别调节幅度与相位估计精度的混合方案，显著降低了信号开销并提升性能

Abstract: To reduce the signaling overhead of over-the-air computation, a hybrid channel estimation scheme is proposed, where reciprocity-based and feedback-based channel estimation are combined. In particular, the impact of quantized phase-feedback is studied while the amplitude is assumed estimated exactly. The scheme enables selecting the estimation precision of amplitude and phase separately, depending on the importance of each. Two variants of the scheme are proposed: As shown through simulations and theory, the second variant with reciprocity-based estimation of the channel phase, and optimal quantization of phase feedback, can outperform the first variant estimating the phase by feedback only.

</details>
