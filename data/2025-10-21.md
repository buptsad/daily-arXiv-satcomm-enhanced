<div id=toc></div>

# Table of Contents

- [eess.SY](#eess.SY) [Total: 14]
- [cs.LG](#cs.LG) [Total: 153]
- [cs.IT](#cs.IT) [Total: 10]
- [cs.CR](#cs.CR) [Total: 39]
- [cs.NI](#cs.NI) [Total: 5]
- [eess.SP](#eess.SP) [Total: 12]


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [1] [A Motivational Driver Steering Model: Task Difficulty Homeostasis From Control Theory Perspective](https://arxiv.org/abs/2510.16247)
*H. Mozaffari,A. Nahvi*

Main category: eess.SY

TL;DR: 将任务难度稳态理论与Lyapunov稳定性方法相结合，构建一个通用且具心理学合理性的司机 steering 模型用于碰撞避免，并通过仿真与驾驶模拟器验证，在20–170 km/h范围内的两种场景下平均误差为7%。


<details>
  <summary>Details</summary>
Motivation: 当前大量计算驱动模型仅基于控制理论，缺乏心理学理论支撑，可能难以真实再现人类驾驶行为及其动机。需要一个将心理学动机与控制理论稳定性相结合的统一模型，以提升碰撞避免情境下的行为 plausibility 与安全性预测。

Method: 将“任务难度稳态”作为驱动行为的动机来源，与Lyapunov稳定性方法相融合，构建一个一般性、心理学合理的驾驶员转向模型，用于碰撞避免场景的仿真分析。通过在20–170 km/h的广泛速度区间内对两种碰撞避免场景进行仿真来评估模型表现，并在驾驶模拟器上进行实验以验证模型。

Result: 该模型在与人类行为的对比中表现接近人类，平均误差约7%。

Conclusion: 将心理学动机理论与控制理论稳定性工具结合，能够生成更具心理学合理性的通用驾驶员模型，并通过仿真与模拟实验得到较高的人类行为再现性。

Abstract: A general and psychologically plausible collision avoidance driver model can
improve transportation safety significantly. Most computational driver models
found in the literature have used control theory methods only, and they are not
established based on psychological theories. In this paper, a unified approach
is presented based on concepts taken from psychology and control theory. The
"task difficulty homeostasis theory", a prominent motivational theory, is
combined with the "Lyapunov stability method" in control theory to present a
general and psychologically plausible model. This approach is used to model
driver steering behavior for collision avoidance. The performance of this model
is measured by simulation of two collision avoidance scenarios at a wide range
of speeds from 20 km/h to 170 km/h. The model is validated by experiments on a
driving simulator. The results demonstrate that the model follows human
behavior accurately with a mean error of 7 percent.

</details>


### [2] [Spatial-to-Spectral Harmonic-Modulated Arrays for 6G Multi-Beam MIMO](https://arxiv.org/abs/2510.16262)
*Jose Guajardo,Ali Niknejad*

Main category: eess.SY

TL;DR: 提出并分析一种空间到频谱的谐波调制阵列（SHA），通过频域多路复用实现并行多波束成形，显著降低硬件复制需求，并给出可实现三个空间到频谱自由度的新型架构。


<details>
  <summary>Details</summary>
Motivation: 在6G场景中需要大规模多用户通信、联合通信与感知、以及干扰抑制，同时希望减少硬件冗余以降低成本与能耗。

Method: 对SHA的谐波调制波形进行分析，提出一种用于降低谱效率损失的梳状（comb-like）调制波形，定量评估其增益、噪声、带宽影响；系统地分析独立成束多波束能力并以SHA的空间到频谱自由度来度量；提出一种实现三自由度、硬件复制最小化的新型SHA架构。

Result: 理论与分析表明SHA可实现并行多波束并且通过频域复用替代硬件复制；所提出的梳状波形降低谱效率损失，且在独立波束控制方面以空间-频谱自由度进行量化；新架构实现三自由度且硬件投入最小化。

Conclusion: SHA为6G的可扩展多用户通信和联合通信与感知提供一种低硬件开销的技术路线，频域多路复用与三自由度设计是实现多波束与干扰抑制的关键。

Abstract: This article presents an overview and analysis of spatial-to-spectral
harmonic-modulated arrays (SHAs). Compared to traditional analog or digital
beamforming arrays, SHAs enable concurrent multi-beamforming without requiring
substantial hardware replication. SHAs replace the need for hardware
replication with frequency-domain multiplexing. Furthermore, SHAs have the
potential to become key contributors to future 6G networks by enabling scalable
multi-user communications, joint communication and sensing, and spatial
interference mitigation. In addition, an analysis of the SHA's
harmonic-modulation waveform and its effects on gain, noise and bandwidth is
presented. A comb-like modulation waveform for SHAs that minimizes spectral
inefficiency is proposed. Further, an analysis of the SHA's capability to
independently steer multiple beams is presented. This capability is quantified
in terms of the SHA's spatial-to-spectral degrees of freedom. Lastly, this work
introduces a novel SHA architecture that provides three spatial-to-spectral
degrees of freedom with minimal hardware replication.

</details>


### [3] [Supervisory Control of Hybrid Power Plants Using Online Feedback Optimization: Designs and Validations with a Hybrid Co-Simulation Engine](https://arxiv.org/abs/2510.16352)
*Sayak Mukherjee,Himanshu Sharma,Wenceslao Shaw Cortez,Genevieve Starke,Michael Sinner,Brooke J. Stanislawski,Zachary Tully,Paul Fleming,Sonja Glavaski*

Main category: eess.SY

TL;DR: 提出一种用于混合风力/太阳能/储能电站的监督反馈优化控制框架，通过梯度信息进行在线控制以满足电网功率需求，属于对模型知识需求较低的模型无关方法，具鲁棒性以应对天气不确定性，并在Hercules耦合仿真平台中验证。


<details>
  <summary>Details</summary>
Motivation: 在风光储协同运行的场景下，给出一种无需详尽系统建模即可在线优化的控制方案，以实现对电网出力的精准跟踪并提高对天气不确定性的鲁棒性。

Method: 采用在线反馈优化，通过对成本和输出对输入控制的梯度信息更新控制输入，调整风电、光伏和储能的有功参考功率以满足电网对功率的要求；控制框架具模型无关或对模型知识要求较低，并包含对各元件的建模以用于控制-系统耦合与仿真，与Hercules耦合仿真引擎集成。

Result: 在较为真实的仿真场景中验证了该监督控制的有效性，显示出对天气不确定性具有鲁棒性并能与Hercules耦合仿真平台无缝集成。

Conclusion: 所提出的监督反馈优化框架为混合风/光伏/储能电站在实时条件下满足电网功率需求提供了一种可行、鲁棒且易于集成的控制策略，具有较强的现实应用潜力。

Abstract: This research investigates designing a supervisory feedback controller for a
hybrid power plant that coordinates the wind, solar, and battery energy storage
plants to meet the desired power demands. We have explored an online feedback
control design that does not require detailed knowledge about the models, known
as feedback optimization. The control inputs are updated using the gradient
information of the cost and the outputs with respect to the input control
commands. This enables us to adjust the active power references of wind, solar,
and storage plants to meet the power generation requirements set by grid
operators. The methodology also ensures robust control performance in the
presence of uncertainties in the weather. In this paper, we focus on describing
the supervisory feedback optimization formulation and control-oriented modeling
for individual renewable and storage components of the hybrid power plant. The
proposed supervisory control has been integrated with the hybrid plant
co-simulation engine, Hercules, demonstrating its effectiveness in more
realistic simulation scenarios.

</details>


### [4] [Stabilization of Nonlinear Systems with State-Dependent Representation: From Model-Based to Direct Data-Driven Control](https://arxiv.org/abs/2510.16451)
*Lidong Li,Rui Huang,Lin Zhao*

Main category: eess.SY

TL;DR: 提出一种将非线性系统表示为状态相关的参数变化模型的稳定化框架，Offline通过可解的LMIs设计稳定控制器并给出局部指数稳定性、对扰动的鲁棒性和在输入饱和下的吸引域估计；扩展至数据驱动情形，利用basis函数库拟合未知系数并借助 Petersen 引理推导数据相关LMIs，确保基于有限数据的稳定性、鲁棒性和安全性，而无需显式的模型辨识。


<details>
  <summary>Details</summary>
Motivation: 在实际控制中，需要对非线性系统在扰动和输入饱和下的稳定性、鲁棒性和安全性提供严格的保证。结合模型驱动的可行性（LMIs）与数据驱动的普适性，提出一个从数据到稳定性保证的闭环框架。

Method: 将非线性动力学重写为状态相关的参数变化模型；离线通过可解的线性矩阵不等式（LMIs）综合稳定控制器，给出局部指数稳定性、鲁棒性及在输入饱和下的吸引域估计。随后推广到数据驱动情形：用一个已知的基函数库来表示动力学，系数在噪声数据下保持一致；借助 Petersen 引理得到数据相关的 LMIs，确保所有与数据相容的系统的稳定性与鲁棒性。

Result: 数值和物理实验结果表明，该方法在有限数据条件下直接从数据出发即可获得对稳定性、鲁棒性和安全性的严格保障，且实现端到端的保证。

Conclusion: 该框架将模型驱动的稳定性证明与数据驱动的不确定性处理结合起来，提供了一种在有限数据下也能获得稳定性、鲁棒性与安全性保证的系统设计途径，并在仿真与物理实验中得到验证。

Abstract: This paper presents a novel framework for stabilizing nonlinear systems
represented in state-dependent form. We first reformulate the nonlinear
dynamics as a state-dependent parameter-varying model and synthesize a
stabilizing controller offline via tractable linear matrix inequalities (LMIs).
The resulting controller guarantees local exponential stability, maintains
robustness against disturbances, and provides an estimate of the region of
attraction under input saturation. We then extend the formulation to the direct
data-driven setting, where a known library of basis functions represents the
dynamics with unknown coefficients consistent with noisy experimental data. By
leveraging Petersen's lemma, we derive data-dependent LMIs that ensure
stability and robustness for all systems compatible with the data. Numerical
and physical experimental results validate that our approach achieves rigorous
end-to-end guarantees on stability, robustness, and safety directly from finite
data without explicit model identification.

</details>


### [5] [SMP-RCR: A Sparse Multipoint Moment Matching Method for RC Reduction](https://arxiv.org/abs/2510.16550)
*Siyuan Yin,Yuncheng Xu,Lin Liu,Fan Yang,Xuan Zeng,Chengtao An,Yangfeng Su*

Main category: eess.SY

TL;DR: 提出一种稀疏多点矩量匹配方法用于后布局多端RC电路的模型排序降低（MOR），在高频点相对SIP精度提升超过2个数量级、相对TurboMOR速度提升超过1/2倍，同时保持相同精度。


<details>
  <summary>Details</summary>
Motivation: 解决后布局多端RC电路MOR中高阶矩匹配导致的大规模密集ROM与SIP在高阶矩匹配方面的局限，以及在端口数增多时的效率与精度权衡问题；寻找在精度与计算成本之间更优平衡的方法。

Method: 提出稀疏多点矩量匹配方法，并对其多频高阶矩匹配性质给出理论分析；引入稀疏控制与消去（deflation）策略以提升算法效率。

Result: 数值实验显示：相较SIP，在高频点上精度提升超过2个数量级；相较TurboMOR，速度提升超过2倍，且保持相同精度；未显著增加额外线性分量。

Conclusion: 所提出的方法显著提升了后布局多端RC电路的MOR在精度与效率之间的权衡，尤其适用于端口数较多的场景，缓解了现有高阶矩匹配与基于消去的方法的局限性。

Abstract: In post--layout circuit simulation, efficient model order reduction (MOR) for
many--port resistor--capacitor (RC) circuits remains a crucial issue. The
current mainstream MOR methods for such circuits include high--order moment
matching methods and elimination methods. High-order moment matching
methods--characterized by high accuracy, such as PRIMA and TurboMOR--tend to
generate large dense reduced-order systems when the number of ports is large,
which impairs the efficiency of MOR. Another common type of MOR method for
many--port circuits is based on Gaussian elimination, with the SIP method as a
representative. The main limitation of this method lies in the inadequate
matching of high--order moments. In this paper, we propose a sparse multipoint
moment matching method and present comprehensive theoretical analysis results
regarding the multi--frequency high--order moment matching property. Meanwhile,
to enhance the algorithm's efficiency, sparse control and deflation techniques
are introduced to further optimize the algorithm. Numerical experiments
demonstrated that, compared to SIP, the accuracy is improved by more than two
orders of magnitude at high frequency points without adding many extra linear
components. Compared to TurboMOR methods, our method achieves a speed
improvement of more than twice while maintaining the same level of precision.

</details>


### [6] [Linear State Estimation in Presence of Bounded Uncertainties: A Comparative Analysis](https://arxiv.org/abs/2510.16693)
*Ayan Das,Anushka Sharma,Anamitra Pal*

Main category: eess.SY

TL;DR: 针对线性状态估计在模型扰动（线参数变化）下的鲁棒求解，比较区间算术、凸优化和广义线性分式规划三种方法在IEEE测试系统上的性能；结论是前两者快速可靠，第三者在扩展性上存在问题，不适合LSE。


<details>
  <summary>Details</summary>
Motivation: 现实中线参数可能与数据库中的值不同，导致模型不确定性，需在数据和模型不确定性下进行状态估计。本文在LSE情景下探讨对不确定数据与模型的处理，聚焦界定有界扰动下的鲁棒估计。

Method: 将LSE问题分别用三种框架实现：1) 区间算术用于处理数据和模型的区间不确定性；2) 凸优化构造鲁棒最小二乘或等价的优化问题以鲁棒估计；3) 广义线性分式规划处理含有参数不确定性的目标/约束。对多个IEEE测试系统进行实验，比较其计算速度与估计精度。

Result: 前两种算法在速度和精度上表现出色且符合预期；第三种方法存在明显的可扩展性问题，难以适用于LSE。此外，三种方法在不同系统上的表现存在差异，区间算术和凸优化更具稳健性。

Conclusion: 在受限不确定性下的线性状态估计中，区间算术与凸优化是可行且高效的鲁棒方案；广义线性分式规划因扩展性不足在本工作中不适用，需进一步改进或探索其他鲁棒建模与求解策略。

Abstract: A variety of algorithms have been proposed to address the power system state
estimation problem in the presence of uncertainties in the data. However, less
emphasis has been given to handling perturbations in the model. In the context
of linear state estimation (LSE), which is the focus of this paper,
perturbations in the model come from variations in the line parameters. Since
the actual values of the line parameters can be different from the values
stored in a power utility's database, we investigate three approaches in this
paper to estimate the states in the presence of bounded uncertainties in the
data and the model. The first approach is based on interval arithmetic, the
second is based on convex optimization, and the third is based on generalized
linear fractional programming. The three algorithms are applied to multiple
IEEE test systems and compared in terms of their speed and accuracy. The
results indicate that the first two algorithms are extremely fast and give
expected results, while the third suffers from scalability issues and is
unsuitable for LSE.

</details>


### [7] [A Control-Theoretic Approach to Dynamic Payment Routing for Success Rate Optimization](https://arxiv.org/abs/2510.16735)
*Aniket Agrawal,Harsharanga Patil*

Main category: eess.SY

TL;DR: 将控制理论与自适应决策系统结合的动态支付路由框架，在JUSPAY支付编排器中通过闭环反馈提高交易成功率，融合控制、强化学习和多臂老虎机优化。


<details>
  <summary>Details</summary>
Motivation: 改善支付路由的鲁棒性与交易成功率，降低路由不稳定带来的损失，追求短期响应与长期稳定性。

Method: 将路由建模为闭环控制器，持续感知网关性能，计算纠正动作，动态在网关之间分发交易；结合控制理论、强化学习和多臂老虎机优化，采用广义反馈自适应而非显式PID，网关评分趋向成功率收敛。

Result: 上线生产结果显示在传统基于规则的路由基础上，成功率提升约1.15%。

Conclusion: 混合式方法实现自我调节的交易路由，降低系统不稳定性，提高可靠性，验证了反馈控制在支付系统中的有效性。

Abstract: This paper introduces a control-theoretic framework for dynamic payment
routing, implemented within JUSPAY's Payment Orchestrator to maximize
transaction success rate. The routing system is modeled as a closed-loop
feedback controller continuously sensing gateway performance, computing
corrective actions, and dynamically routes transactions across gateway to
ensure operational resilience. The system leverages concepts from control
theory, reinforcement learning, and multi-armed bandit optimization to achieve
both short-term responsiveness and long-term stability. Rather than relying on
explicit PID regulation, the framework applies generalized feedback-based
adaptation, ensuring that corrective actions remain proportional to observed
performance deviations and the computed gateway score gradually converges
toward the success rate. This hybrid approach unifies control theory and
adaptive decision systems, enabling self-regulating transaction routing that
dampens instability, and improves reliability. Live production results show an
improvement of up to 1.15% in success rate over traditional rule-based routing,
demonstrating the effectiveness of feedback-based control in payment systems.

</details>


### [8] [Safe Payload Transfer with Ship-Mounted Cranes: A Robust Model Predictive Control Approach](https://arxiv.org/abs/2510.16953)
*Ersin Das,William A. Welch,Patrick Spieler,Keenan Albee,Aurelio Noca,Jeffrey Edlund,Jonathan Becktor,Thomas Touma,Jessica Todd,Sriramya Bhamidipati,Stella Kombo,Maira Saboia,Anna Sabel,Grace Lim,Rohan Thakker,Amir Rahmani,Joel W. Burdick*

Main category: eess.SY

TL;DR: 提出一个基于鲁棒模型预测控制（MPC）+ 鲁棒零阶控制屏障函数（R-ZOCBF）的安全控制框架，用于船载起重机在海况扰动下的实时控制，结合Stewart平台模拟环境扰动，并通过时变边界框实现避障，且引入在线鲁棒性自适应，实现在受扰动的5-DOF起重机上安全高效的载荷放置。


<details>
  <summary>Details</summary>
Motivation: 船舶运输中的船载起重机在海况恶劣时受到显著外部扰动，导致动力学不稳定、鲁棒性不足和安全风险上升。需要在保障安全的同时，确保载荷传输性能并实现对目标区域的准确放置。

Method: 在5-DOF起重机系统上建立鲁棒MPC框架，并使用Stewart平台模拟海面扰动带来的外部干扰；通过R-ZOCBF构造非线性MPC中的安全约束以确保载荷定位的安全性，并引入时变边界框进行碰撞避免；提出一种在线鲁棒性参数自适应机制以降低R-ZOCBF的保守性；在一个起重机原型上进行实验验证以评估鲁棒性和安全性。

Result: 实验表明在显著扰动下，所提出的方法能够实现安全的载荷定位与避障，同时维持良好的传输性能，鲁棒性得到提升，且框架具有一定的普适性可推广至其他机器人协作任务。

Conclusion: 尽管研究聚焦于起重机搬运任务，但所提的方法和框架具有更广的应用潜力，适用于机器人辅助的部件配合、插入等需要在不确定环境中保证安全性的任务。

Abstract: Ensuring safe real-time control of ship-mounted cranes in unstructured
transportation environments requires handling multiple safety constraints while
maintaining effective payload transfer performance. Unlike traditional crane
systems, ship-mounted cranes are consistently subjected to significant external
disturbances affecting underactuated crane dynamics due to the ship's dynamic
motion response to harsh sea conditions, which can lead to robustness issues.
To tackle these challenges, we propose a robust and safe model predictive
control (MPC) framework and demonstrate it on a 5-DOF crane system, where a
Stewart platform simulates the external disturbances that ocean surface motions
would have on the supporting ship. The crane payload transfer operation must
avoid obstacles and accurately place the payload within a designated target
area. We use a robust zero-order control barrier function (R-ZOCBF)-based
safety constraint in the nonlinear MPC to ensure safe payload positioning,
while time-varying bounding boxes are utilized for collision avoidance. We
introduce a new optimization-based online robustness parameter adaptation
scheme to reduce the conservativeness of R-ZOCBFs. Experimental trials on a
crane prototype demonstrate the overall performance of our safe control
approach under significant perturbing motions of the crane base. While our
focus is on crane-facilitated transfer, the methods more generally apply to
safe robotically-assisted parts mating and parts insertion.

</details>


### [9] [Generalized Group Selection Strategies for Self-sustainable RIS-aided Communication](https://arxiv.org/abs/2510.17176)
*Lakshmikanta Sau,Priyadarshi Mukherjee,Sasthi C. Ghosh*

Main category: eess.SY

TL;DR: 提出面向自供能 RIS-D2D 的分组选择策略，针对 PS/TS 能量收集配置，结合高阶统计与极值理论推导多种选择策略的中断概率解析表达式与渐近特性，并在数值结果中验证数据吞吐量与中断概率方面的收益。


<details>
  <summary>Details</summary>
Motivation: RIS 为跨五代的前沿通信技术，需在自供能、参数受限的场景下通过分组层面的选择来优化端到端信噪比和能量收集，从而提升系统的中断概率、吞吐量等关键指标；对简单线性与更贴近现实的非线性能量收集模型均给出分析与界限，具备大尺度智能表面应用的理论支撑。

Method: 在自供能 RIS 的 PS 与 TS 配置下，提出多种 RIS 分组选择策略；每种策略基于端到端 SNR 与 RIS 某组的能量收集量来调度第 k 大可用组；利用高阶统计推导出各策略的中断概率表达式，并应用极值理论研究组数趋于无穷时的渐近行为，辅以数值仿真验证。

Result: 给出各策略的中断概率解析表达式及其渐近性质，揭示在大规模分组情境中策略的性能趋势；数值结果显示在数据吞吐量与数据/能量中断概率方面的显著收益。

Conclusion: 基于分组选择的 RIS 自供能 D2D 系统可通过有效的策略设计获得显著性能提升，且大组数情形下的 EVT 提供了对未来大尺度智能表面的理论洞察。

Abstract: Reconfigurable intelligent surface (RIS) is a cutting-edge communication
technology that has been proposed as aviable option for beyond fifth-generation
wireless communication networks. This paper investigates various group
selection strategies in the context of grouping-based self-sustainable
RIS-aided device-to-device (D2D) communication with spatially correlated
wireless channels. Specifically, we consider both power splitting (PS) and time
switching (TS) configurations, of the self-sustainable RIS to analyze the
system performance and propose appropriate bounds on the choice of system
parameters. The analysis takes into account a simplified linear energy
harvesting (EH) model as well as a practical non-linear EH model. Based on the
application requirements, we propose various group selection strategies at the
RIS. Notably, each strategy schedules the k-th best available group at the RIS
based on the end-to-end signal-to-noise ratio (SNR) and also the energy
harvested at a particular group of the RIS. Accordingly, by using tools from
high order statistics, we derive analytical expressions for the outage
probability of each selection strategy. Moreover, by applying the tools from
extreme value theory, we also investigate an asymptotic scenario, where the
number of groups available for selection at an RIS approaches infinity. The
nontrivial insights obtained from this approach is especially beneficial in
applications like large intelligent surface-aided wireless communication.
Finally, the numerical results demonstrate the importance and benefits of the
proposed approaches in terms of metrics such as the data throughput and the
outage (both data and energy) performance.

</details>


### [10] [Enhanced Ground-Satellite Direct Access via Onboard Rydberg Atomic Quantum Receivers](https://arxiv.org/abs/2510.17290)
*Qihao Peng,Tierui Gong,Zihang Song,Qu Luo,Zihuai Lin,Pei Xiao,Chau Yuen*

Main category: eess.SY

TL;DR: 提出一种基于原子EIT的RAQR前端用于航天器，解决6G地-卫星链路中的高损耗、严苛资源限制与拥挤频谱问题，提升前端灵敏度与选择性。


<details>
  <summary>Details</summary>
Motivation: 地-卫星链路在6G场景下面临严重路径损耗、有限的尺寸/重量/功耗、以及拥挤/受限的频谱，传统RF前端难以满足性能需求；需要更高灵敏度、频率选择性以及更小体积的前端。

Method: 提出Rydberg原子量子接收机RAQR，通过原子电磁诱导透明性(EIT)将射频场转换为光信号；实现毫米级前端；采用混合原子-电子设计并建立相应的信号模型以评估性能。

Result: 与传统RF接收机相比，RAQR实现了更高的数据速率、覆盖范围与感知精度等提升。

Conclusion: 给出RAQR在航天载荷中的集成策略、分布式卫星概念以及未来的研究问题，指向将RAQR载荷投入实际服务的路径。

Abstract: Ground-satellite links for 6G networks face critical challenges, including
severe path loss, tight size-weight-power limits, and congested spectrum, all
of which significantly hinder the performance of traditional radio frequency
(RF) front ends. This article introduces the Rydberg Atomic Quantum Receiver
(RAQR) for onboard satellite systems, a millimeter-scale front end that
converts radio fields to optical signals through atomic electromagnetically
induced transparency. RAQR's high sensitivity and high frequency selectivity
address link budget, payload, and interference challenges while fitting within
space constraints. A hybrid atomic-electronic design and supporting signal
model demonstrate enhanced data rate, coverage, and sensing accuracy relative
to conventional RF receivers. The article concludes with integration
strategies, distributed-satellite concepts, and open research problems for
bringing RAQR-enabled satellite payloads into service.

</details>


### [11] [Comparison and performance analysis of dynamic encrypted control approaches](https://arxiv.org/abs/2510.17333)
*Sebastian Schlor,Frank Allgöwer*

Main category: eess.SY

TL;DR: 对动态加密控制的最新方法进行综述与性能分析，比较 bootstrapping、周期性状态重置、整数重构、FIR 控制器等策略，并在基准系统上给出数值对比。


<details>
  <summary>Details</summary>
Motivation: 在保护测量、控制信号及控制器参数隐私的同时，保持系统按预期运行；动态控制器的噪声与溢出问题使其成为一个挑战，需要稳定性与性能分析来评估方法的适用性。

Method: 系统性回顾和分析近年动态加密控制的方法，重点关注 bootstrapping、周期性状态重置、整数重构、FIR 控制器等技术，建立稳定性与性能分析框架，并在基准系统上进行数值比较。

Result: 提出各方法的稳定性与性能要点，给出对噪声增长、溢出风险、加密开销的权衡；提供基准系统上的数值结果以比较不同策略的实际表现。

Conclusion: 在不同应用场景下，一些方法（如 bootstrapping、整数重构）能在可接受的开销下控制噪声并提供稳定性保障；FIR 控制器等结构简化的方案在资源受限场景具有优势；未来工作包括进一步的稳健性分析和实际系统验证。

Abstract: Encrypted controllers using homomorphic encryption have proven to guarantee
the privacy of measurement and control signals, as well as system and
controller parameters, while regulating the system as intended. However,
encrypting dynamic controllers has remained a challenge due to growing noise
and overflow issues in the encoding. In this paper, we review recent approaches
to dynamic encrypted control, such as bootstrapping, periodic resets of the
controller state, integer reformulations, and FIR controllers, and equip them
with a stability and performance analysis to evaluate their suitability. We
complement the analysis with a numerical performance comparison on a benchmark
system.

</details>


### [12] [Accelerating Adaptive Systems via Normalized Parameter Estimation Laws](https://arxiv.org/abs/2510.17371)
*Mohammad Boveiri,Mohammad Khosravi,Peyman Mohajerin Esfahan*

Main category: eess.SY

TL;DR: 提出一种归一化的参数估计律，用以自适应系统的参数估计，显著加速状态收敛至原点。证明状态范数的2/r次方的平方和的积分为有限（||x(t)||^2)^{1/r} ∈ L1，r≥1 可选且可较大），相较传统Lyapunov型估计律强调更强的时域稀疏性，促进更快的收敛。方法还给出高阶扩展（带动量）与广泛适用性，且不依赖时间变化增益、持久激励，适用于匹配和不匹配不确定性，且与任意基于CLF的确定性等效控制兼容。实验证明性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统Lyapunov型参数估计律在收敛速度和信号持续时间方面的局限，提出一种能在不依赖高自适应增益或持久激励的前提下，加速收敛并引入时域稀疏性的估计框架。

Method: 提出归一化参数估计律，通过对估计误差进行归一化处理，获得对 r 的任意大取值下的收敛保证。给出对 ∥x(t)∥^2 的 r 次幂的积分性质的理论证明，且扩展到高阶含动量的形式。此外，方法兼容任意存在的CLF，以及不需要系统结构的强假设。

Result: 理论上证明：对于广泛的一类系统，∥x(t)∥_2^{2/r} ∈ L1，与传统 ∥x(t)∥_2^2 的L1相比提供更强的时间域收敛信号稀疏性；数值实验显示与推导一致，且在不依赖持久激励和高增益的前提下提升了收敛速率。

Conclusion: 该方法在不要求时变增益、持久激励、且对CLF存在性仅有基本要求的条件下，提供了一个通用的、与CLF控制兼容的参数估计框架，并可扩展到高阶带动量形式，实验验证了其性能提升。

Abstract: In this paper, we propose a new class of parameter estimation laws for
adaptive systems, called \emph{normalized parameter estimation laws}. A key
feature of these estimation laws is that they accelerate the convergence of the
system state, $\mathit{x(t)}$, to the origin. We quantify this improvement by
showing that our estimation laws guarantee finite integrability of the
$\mathit{r}$-th root of the squared norm of the system state, i.e., \(
\mathit{\|x(t)\|}_2^{2/\mathit{r}} \in \mathcal{L}_1, \) where $\mathit{r} \geq
1$ is a pre-specified parameter that, for a broad class of systems, can be
chosen arbitrarily large. In contrast, standard Lyapunov-based estimation laws
only guarantee integrability of $\mathit{\|x(t)\|}_2^2$ (i.e., $\mathit{r} =
1$). We motivate our method by showing that, for large values of $r$, this
guarantee serves as a sparsity-promoting mechanism in the time domain, meaning
that it penalizes prolonged signal duration and slow decay, thereby promoting
faster convergence of $\mathit{x(t)}$. The proposed estimation laws do not rely
on time-varying or high adaptation gains and do not require persistent
excitation. Moreover, they can be applied to systems with matched and unmatched
uncertainties, regardless of their dynamic structure, as long as a control
Lyapunov function (CLF) exists. Finally, they are compatible with any CLF-based
certainty equivalence controllers. We further develop higher-order extensions
of our estimation laws by incorporating momentum into the estimation dynamics.
We illustrate the performance improvements achieved with the proposed scheme
through various numerical experiments.

</details>


### [13] [Artificial magnetic conductor backed dual-mode sectoral cylindrical DRA for off-body biomedical telemetry](https://arxiv.org/abs/2510.17619)
*Nayab Gogosh,Sohail Khalid,Bilal Tariq Malik,Slawomir Koziel*

Main category: eess.SY

TL;DR: 提出了扇区型圆柱介电谐振器天线(CDRA)的双模设计用于生物医学遥测，通过四分扇区PEC边界实现尺寸减小，结合AMC表面降低SAR，获得5.2-5.9 GHz带宽0.7 GHz、峰值增益7.9 dBi、在手臂SAR 1.24 W/kg。


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴设备中CDRA的带宽和尺寸限制，以及对生物组织的安全性（SAR），通过双模工作和尺寸削减来提升可穿戴生物医疗传输的可行性。

Method: 将CDRA设计为四分扇区、PEC边界；推导EH110与TE210两种模态的场分量；在天线背面引入人工磁性导体AMC实现SAR降低并提升TE模的兼容性；实验测量带宽、增益与SAR。

Result: 实现0.7 GHz带宽（5.2-5.9 GHz）、测得峰值增益7.9 dBi，以及在手臂上的SAR为1.24 W/kg。

Conclusion: 所提出的扇 sector CDRA设计在带宽、增益与尺寸方面对生物医学应用具备可行性，AMC的引入有效降低SAR，适合可穿戴生物医疗遥测。

Abstract: This research investigates the potential of a sectoral Cylindrical Dielectric
Resonator Antenna (CDRA) for biomedical telemetry. CDRAs are known for their
low loss, ruggedness, and stability, but their limited bandwidth and size make
them unsuitable for wearable devices. The research addresses these limitations
by proposing a dual mode antenna that operates in EH110 and TE210 modes. The
sectoral CDRA is a quarter segment with Perfect Electric Conductor boundaries,
reducing its size by a factor of four. Mathematical derivations of the field
components for both modes are derived to support the design. To minimize
specific absorption rate (SAR), an Artificial Magnetic Conductor (AMC) surface
is applied to the antennas backside, enhancing compatibility with the
transverse electric modes. The antenna achieves a bandwidth of 0.7 GHz (5.2-5.9
GHz), suitable for biomedical applications, with a measured peak gain of 7.9
dBi and a SAR of 1.24 W/kg when applied to a human arm.

</details>


### [14] [Data-driven Communication and Control Design for Distributed Frequency Regulation with Black-box Inverters](https://arxiv.org/abs/2510.17769)
*Michael Nestor,Jiaxin Wang,Ning Zhang,Fei Teng*

Main category: eess.SY

TL;DR: 提出一种基于分布式数据驱动的二级频率控制框架，通过点对点通信实现多逆变器协同，设计与通信拓扑相关的控制器并给出闭环稳定性保证，在IEEE 39总线系统上验证通信需求与控制性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着并网的逆变器资源占比提升且常只能获得黑盒模型，传统频率控制面临挑战；需要在没有中心控制的情况下利用局部信息和通信实现高效的频率调节。

Method: 建立分布式数据驱动框架，基于点对点通信进行信息交换；设计与通信拓扑相匹配的控制器，并给出闭环稳定性保障；提供设计框架以在通信开销与控制性能之间做权衡。

Result: 在IEEE 39-节点系统的案例研究中验证框架，展示了通信拓扑设计对控制性能的影响以及在给定通信资源下的性能提升与稳定性。

Conclusion: 该框架为二级频率调控的拓扑设计提供了实用方法，尤其适用于高比例逆变器电网，在受限通信条件下仍能实现稳定且可控的频率调节。

Abstract: The increasing penetration of inverter-based resources into the power grid,
with often only black-box models available, challenges long-standing frequency
control methods. Most recent works take a decentralized approach without online
device coordination via communication. This paper considers both dynamic
behavior and communication within secondary frequency control on an
intermediate timescale. We develop a distributed data-driven approach that
utilizes peer-to-peer communication between inverters to avoid the need for a
central control center. To enable a trade off between communication network
requirements and control performance, we present a framework to guide
communication topology design for secondary frequency regulation. Following
design of the inter-agent information exchange scheme, we design a controller
that is structured according to the communication topology with a closed-loop
stability guarantee. Case studies on the IEEE 39-bus system validate the
framework and illustrate the trade-off between communication requirements and
control performance that is enabled by our approach.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder 是一个面向 Lean 和 mathlib 的语义检索引擎，能够理解并对齐数学家的意图，通过分析公开的 Lean 讨论语义、在合成查询上微调文本嵌入，并结合多元反馈信号，比先前的检索引擎和 GPT-4o 提升超过 30%，并与基于大语言模型的定理证明器兼容。其目标是把检索与形式推理连接起来，公开地址为 leanfinder.github.io。


<details>
  <summary>Details</summary>
Motivation: 在正式化定理证明的进程中，寻找相关定理和 Lean 4 语言的学习成本成为主要障碍；现有 Lean 检索引擎依赖非正式化表达且与实际用户查询存在错配，需要一个以数学家需求为中心的语义检索解决方案。

Method: 通过分析并聚类公开 Lean 讨论的语义，针对用户意图的合成查询对文本嵌入进行微调，并利用多样的反馈信号对 Lean Finder 进行对齐；在多角度编码目标，确保与数学家目标的一致性；与 LLM 基于定理证明器的桥接，使检索与形式推理相连接。

Result: 在真实世界查询、非正式化陈述以及证明状态的评估中，Lean Finder 相对于先前的检索引擎和 GPT-4o 实现超过 30% 的相对改进。

Conclusion: Lean Finder 为 Lean 与 mathlib 提供面向用户的语义检索，能够与基于大模型的定理证明系统协同工作，将检索与形式推理有效结合；并且已对外提供服务，地址为 leanfinder.github.io。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [16] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: LS-OGD: 一个面向概念漂移的在线自适应控制框架，通过在线控制器在检测漂移和预测误差时动态调整学习率及模态融合权重。理论上，在有界漂移条件下，预测误差为统一最终有界；若漂移停止，误差收敛到零。自适应融合策略还能隔离并减轻强模态特异漂移，提升鲁棒性与容错性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习在非平稳环境中的概念漂移问题，尤其是模态特异漂移和缺乏持续稳定自适应机制导致的性能下降。

Method: 提出在线控制器，依据检测到的漂移和不断变化的预测误差，动态调整模型的学习率以及不同模态之间的融合权重；对漂移条件下的误差进行理论分析，证明系统具有统一最终有界性并在漂移停止时收敛；通过自适应融合实现对强模态漂移的隔离与缓解。

Result: 在有界漂移条件下，LS-OGD的预测误差有统一的最终有界性，且当漂移停止时收敛到零；自适应融合策略有效分离并缓解严重的模态特异漂移，提升了系统的鲁棒性与容错性。

Conclusion: 为持续自适应的多模态学习系统提供了原理性基础和理论保证，表明在漂移环境下可通过在线控制的学习率与模态融合策略实现稳定且可持续的学习。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [17] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一种自适应停止采样的贝叶斯序列决策框架，通过在实时无再训练的情况下更新后验并权衡边际收益与成本，显著减少采样次数且保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 在LLM生成多条回应以提升质量的同时，计算成本显著上升，缺乏一个 principled 的停止准则来在准确性和效率之间取舍。

Method: 基于带有贝叶斯学习的序列搜索，逐步从策略LLM生成回应并更新奖励分布的后验，在计算边际收益与成本后决定是否继续采样；若边际探索的效用小于成本则终止。理论上具备最优性保证，且在实践上可行。

Result: 在实验中，BEACON将平均采样次数最多减少80%，同时保持回应质量；并展示了在成本受控的数据偏好生成方面的应用价值。

Conclusion: BEACON提供一个原理性且高效的自适应采样框架，具备理论与实用性，并有可扩展性以支持未来研究。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [18] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: PatMD introduces a misjudgment-risk-guided framework for harmful meme detection. It builds a knowledge base of misjudgment patterns, retrieves relevant patterns for a target meme, and uses them to dynamically guide multi-modal LLMs, outperforming baselines on 6,626 memes across 5 tasks (8.30% F1, 7.71% accuracy).


<details>
  <summary>Details</summary>
Motivation: Internet memes often encode harmful ideas through implicit expressions (irony, metaphor). Conventional detection methods, including MLLMs, struggle with these subtleties, leading to false negatives and false positives. A proactive mitigation of misjudgments is needed.

Method: Construct a knowledge base that decomposes each meme into misjudgment risk patterns explaining potential misjudgments (false negatives or false positives). For a target meme, retrieve relevant patterns and dynamically guide the MLLM's reasoning to avoid known misjudgment pitfalls.

Result: Empirical evaluation on a benchmark of 6,626 memes across 5 harmful-detection tasks shows PatMD outperforms state-of-the-art baselines, with an average improvement of 8.30% in F1-score and 7.71% in accuracy.

Conclusion: PatMD demonstrates strong generalizability and improved detection capability for harmful memes by explicitly modeling and mitigating misjudgment risks during multimodal reasoning.

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [19] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 提出 WaveNet 基于的 EEG 分类模型，自动分四类，显著优于 CNN/LSTM，且与 TCN 基线对比。


<details>
  <summary>Details</summary>
Motivation: 解决临床 EEG 需要专家人工审阅的低效问题，提升对大量 EEG 数据的自动化分类与噪声/伪迹识别能力。

Method: 基于 WaveNet 架构，采用 dilated causal 卷积和残差连接；在 Mayo Clinic 与 St. Anne's 数据集上进行 70/20/10 的训练/验证/测试划分，包含 209,232 个样本；进行了预处理、归一化、动态数据划分等以提升泛化，并与 CNN、LSTM、TCN 等基线模型比较。

Result: 模型在四类标签上实现高精度分类，显著优于 CNN/LSTM，并与 TCN 基线相比具有更好表现；对噪声与伪迹的识别尤为精准，但生理与病理之间存在可解释的混淆，反映临床重叠。数据集规模大（209,232 样本），切分比例为 70/20/10。

Conclusion: WaveNet 的因果卷积与残差结构使其能够捕捉短期与长程时间依赖，适合原始 EEG 数据；配合完善的预处理与数据划分策略，提升了跨样本的泛化能力，且对噪声与伪迹的区分具实用性。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [20] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 使用键击动力学作为帕金森病数字生物标志物，提出三阶段深度学习流水线，进行跨数据集的外部验证，AUC-ROC>90%且F1>70%，其中时序卷积模型达到AUC-ROC 91.14%。


<details>
  <summary>Details</summary>
Motivation: 全球帕金森病负担日益加重，早期诊断困难，传统临床评估存在局限性；键击动力学作为非侵入、可扩展的远程筛查与远程监测生物标志物具潜力。

Method: 1) 整理四个数据集，预处理并提取四种时序信号，比较三种方法以应对类别不平衡；2) 在两大数据集上对八种最先进深度学习架构进行预训练，优化时间窗口、步长及其他超参数；3) 在中等规模数据集上进行微调，并在第四个独立队列上进行外部验证。

Result: 外部验证中混合卷积-递归与Transformer模型表现出色，AUC-ROC通常超过0.90，F1-score超过0.70；其中时序卷积模型达到AUC-ROC 91.14%，优于仅使用内部验证的方法。

Conclusion: 键击动力学可作为可靠的数字生物标志物用于帕金森病的早期识别与持续监测，具有远程筛查与监控的现实潜力。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [21] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 采用扩散模型的 Ensemble Score Filter (EnSF) 进行野火数据同化，在高维非线性滤波中提升预测精度、稳定性与计算效率，适用于实时野火蔓延预测，且代码公开。


<details>
  <summary>Details</summary>
Motivation: 野火日益破坏性强，需要实时、准确的蔓延预测。数据同化通过整合观测和数值模型来提高预测准确性。最近提出的基于扩散模型的滤波算法 EnSF 在高维非线性滤波问题上展现出优越性，因此对野火蔓延模型的滤波问题具有潜在价值。

Method: 基于分数/生成式扩散模型的 EnSF，用于野火蔓延的实时数据同化。提供实现细节并通过数值实验验证其在野火数据同化中的性能。

Result: 数值研究表明 EnSF 在精度、稳定性和计算效率方面优于传统方法，显示出在野火数据同化中的鲁棒性和可行性；并公开了相关代码。

Conclusion: EnSF 是一种鲁棒且实用的野火数据同化方法，适用于实时野火蔓延预测，且代码已公开。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [22] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 利用食品生物质（废弃咖啡渣与枣核）经热解制氢，结合AI提升过程建模与优化，比较不同混合比的产氢潜力，并用LSTM对TGA曲线进行高精度预测。


<details>
  <summary>Details</summary>
Motivation: 应对可持续能源与废物管理挑战，开发低价值生物质资源实现清洁氢气生产；通过AI与动力学分析提升热解过程的理解、预测与优化效率。

Method: 对纯DS、SCG及其混合物进行 proximate、ultimate、纤维、TGA/DTG、动力学、热力学、以及 Py-Micro GC 分析；采用等温转化法（KAS、FWO、Friedman）建立动力学模型；使用基于LSTM的AI模型对 lignocellulosic 数据预测TGA曲线；评估混合对比、Ea 及产氢潜力。

Result:  Blend 3 具有最高的产氢潜力，但其活化能 Ea 为 313.24 kJ/mol；Blend 1 的 Ea 最低，为 161.75 kJ/mol；在等转化法中，KAS 被认定为最准确的模型；LSTM 对 TGA 曲线预测表现极佳，R^2 介于 0.9996—0.9998。

Conclusion: 将人工智能整合到热解过程的建模与优化中具有显著潜力；通过混合不同生物质以提升产氢潜力，但需权衡动力学障碍与热力学制约，未来可通过AI进一步提升预测与优化效率。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [23] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出 LAMI：一种结合图结构学习与大语言模型的联合建模框架，用于检测青少年和年轻人群体中的非法药物使用，并从调查数据中给出可解释的行为风险因子。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常对调查变量逐一建模，忽略变量之间潜在的耦合关系和隐含结构；需要可解释、能揭示行为风险途径的分析框架以提升早期检测与干预。

Method: 将个体答卷表示为关系图，使用专门的图结构学习层学习潜在连接；结合大语言模型生成基于图结构与调查语义的自然语言解释。数据源为 YRBS 与 NSDUH，评估包括预测准确性与解释性分析。

Result: 在 YRBS 与 NSDUH 上，LAMI 在预测准确性方面优于竞争基线；解释性分析揭示出如家庭动力、同伴影响、学业压力等与物质使用风险因素一致的行为子结构与心理社会通路。

Conclusion: LAMI 展示了联合图-语言建模在公共卫生调查数据中的潜力，既提升预测性能，又提供对行为风险因子的可解释洞察，有助于理解与干预青少年与青年人的药物使用行为。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [24] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 提出 Shadowy Sparsity 与 Long Exposure，用以加速大语言模型的参数高效微调（PEFT），通过更充分地利用稀疏性和动态稀疏计算实现端到端微调的最高约2.49×加速，展现了对 PEFT 的有效加速潜力。


<details>
  <summary>Details</summary>
Motivation: PEFT 在大规模预训练语言模型中的效率瓶颈阻碍实际部署，需要在微调阶段更高效地利用稀疏性和动态计算特性，以降低时间和成本。

Method: 提出 Shadowy Sparsity 的新型稀疏性概念；设计 Long Exposure 系统以加速 PEFT：包含 Shadowy-sparsity Exposer（扩展感知范围以捕捉更多 Shadowy Sparsity 细节）、Sequence-oriented Predictor（对长序列输入和动态参数进行高效且准确的预测）、Dynamic-aware Operator（面向动态稀疏操作的结构化计算和合并内存访问），以实现更高的计算效率与内存利用率。

Result: 在端到端微调任务中实现相对于现有方法的最高约 2.49× 的加速，证明在 LLM 的 PEFT 场景下的有效性和潜力。

Conclusion: 将 Shadowy Sparsity 作为优化方向之一，结合 Long Exposure 的多组件设计，显示出在加速 PEFT 方面的可行性和前景，可能推动对大模型微调的高效化进一步发展。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [25] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: Deadlock Attack: an adversarial embedding induces perpetual reasoning loops in large reasoning models, causing resource exhaustion and blocking conclusions via backdoor triggers; achieves 100% success on four LRMs and three math benchmarks; stealthy and robust to mitigations.


<details>
  <summary>Details</summary>
Motivation: LRMs' multi-step reasoning (CoT) can be exploited by adversarial inputs that cause endless reasoning, revealing a security vulnerability in reasoning efficiency and resource usage. The work also addresses the continuous-to-discrete projection gap in embedding-to-token translation and demonstrates a backdoor strategy for reliable activation.

Method: Train malicious adversarial embeddings that bias the model to generate transitional tokens (e.g., 'Wait', 'But') after reasoning steps, creating perpetual loops. Introduce a backdoor implantation strategy to ensure reliable activation via trigger tokens. Address continuous-to-discrete projection gap by maintaining loop-inducing embeddings that survive projection. Evaluate on four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) across three math reasoning benchmarks, forcing models to reach maximum token limits.

Result: Achieves a 100% attack success rate across tested models and benchmarks, inducing loops that exhaust model tokens up to the maximum, while maintaining negligible performance drop on benign inputs and showing robustness against existing mitigation strategies.

Conclusion: Reveals a critical and underexplored security vulnerability in LRMs related to inference efficiency and control-flow manipulation; underscores the need for defenses against backdoor-induced CoT loop attacks and improved robustness of reasoning pipelines.

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [26] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 在开集联邦学习场景下，提出Gains，通过对编码器与分类器的细粒度区分实现知识发现与知识整合，并含抗遗忘机制，显著提升源域与目标域性能，同时给出开源代码。


<details>
  <summary>Details</summary>
Motivation: 现实场景中客户端持续增加，需发现新知识并将其整合到全局模型，同时保持源域性能与适应效率。现有方法多关注粗粒度知识发现，往往牺牲源域性能与适配效率。

Method: 将模型分为编码器和分类器并进行细粒度分析：编码器特征对域偏移敏感，分类器对类别增量敏感；据此提出细粒度知识发现与贡献驱动聚合，以识别并整合新知识；引入抗遗忘机制以保持源域性能，确保平衡适应；在多域数据集与三种数据漂移场景下验证。

Result: 实验结果显示Gains在源域和目标域客户端的性能均显著优于其他基线。

Conclusion: 提出在开集情形下的高效知识发现与平衡域适应的方法，通过对编码器/分类器的细粒度处理实现更好的知识发现、整合与保源性能，代码已开源。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [27] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: SAU-FNO 将自注意力、U-Net 与傅里叶神经算子结合，用于3D集成电路热管理的快速高保真预测，能在保持准确性的同时比传统FEM快约842倍。


<details>
  <summary>Details</summary>
Motivation: 3D ICs 的高功率密度使热管理成为关键挑战；基于PDE的FEM虽然准确，但对迭代设计速度太慢；传统FNO等ML方法更快但信息保真性不足且需要大量高保真数据；需要一个在精度、速度与数据需求之间取得良好折中的新型 surrogate 模型。

Method: 提出 SAU-FNO，将自注意力机制与U-Net结构融入傅里叶神经算子框架，建立可捕捉长程依赖和局部高频特征的模型；通过迁移学习对低保真数据进行微调，以减少对大量高保真数据的依赖并提升训练效率。

Result: 在实验中，SAU-FNO 达到最先进的热预测精度，并实现相对于有限元法(FEM)约842x的加速，显示出在3D IC热仿真中的高效性与实用性。

Conclusion: SAU-FNO 提供了一种高效且精确的3D IC热仿真解耦框架，结合了强大建模能力与数据高效利用，迁移学习进一步降低数据需求。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [28] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: LinearizeLLM uses LLM-based agents to automatically linearize nonlinear optimization patterns into a solver-ready linear model, benchmarked on 20 problems derived from ComplexOR, showing promise for automated, conversational modeling of nonlinear optimization.


<details>
  <summary>Details</summary>
Motivation: Nonlinear reformulation is largely manual and expertise-intensive. Automating this process with LLMs could streamline solving nonlinear problems by converting them into linear models suitable for linear solvers or specialized algorithms.

Method: An agent-based framework where each nonlinear pattern is assigned a reformulation agent tasked with deriving exact linear reformulations. Agents coordinate to assemble a solver-ready linear model. Evaluation uses a dataset of 20 real-world nonlinear problems derived from the ComplexOR linear optimization dataset, tested across several LLMs.

Result: Specialized LLM agents can automate the linearization task, enabling automated construction of linear reformulations and suggesting a path toward fully conversational modeling pipelines for nonlinear optimization.

Conclusion: LLM-powered reformulation agents can automate nonlinear problem linearization, offering a foundation for end-to-end conversational modeling pipelines in nonlinear optimization and potential extensions to broader nonlinearities.

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [29] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 通过持久同调分析数据的拓扑结构来衡量数据多样性并揭示冗余，进而提升训练数据质量对模型学习的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管已有关于训练数据类型的研究，但数据的几何/拓扑结构对学习的影响尚未充分研究。

Method: 在度量空间中使用持久同调提取拓扑特征，量化数据多样性与冗余；并将其作为超越基于熵的多样性度量的分析工具。

Result: 证实持久同调能作为分析和改进训练数据的有力工具，揭示拓扑特征与模型性能之间的关系。

Conclusion: 将拓扑特征引入训练数据分析与数据增强流程，促使AI系统在学习过程中的表现提升。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [30] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: PALE uses prompt-guided data augmentation to generate truthful and hallucinated data for robust hallucination detection; introduces CM Score for evaluating intermediate embeddings; achieves notable gains without extra annotations.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs undermines reliability; lack of labeled data hinders detection; need scalable, annotation-free data generation and robust evaluation.

Method: Generate augmented data via LLM prompts; train detectors on augmented data; introduce Contrastive Mahalanobis Score to assess truthfulness in embedding space using matrix decomposition.

Result: PALE outperforms competitive baselines by 6.55% on hallucination detection tasks in experiments.

Conclusion: PALE provides a scalable, annotation-free framework with strong generalization for practical detection of LLM hallucinations.

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [31] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: 提出 DAWP 框架，通过 AIDA 初始化与在观测空间学习的 AI天气预测，使得利用不规则观测数据进行全球级观测预测成为可能，并提升 rollout 与效率。


<details>
  <summary>Details</summary>
Motivation: 为了克服对再分析数据的依赖及其引入的偏差与时间错配，推动观测预测在不完美的观测数据环境中实现稳定有效的天气预测。

Method: 引入 AIDA 模块：对不规则卫星观测 token 进行 MMAE 组装与 MMAE 自编码，使用 mask ViT-VAEs 对观测数据进行编码与同化；提出带 CBC 的时空解耦 Transformer，在观测空间学习动力学，支撑基于子图的全球观测预测。

Result: 实验结果表明 AIDA 初始化显著提升推演 rollout 的稳定性与效率；DAWP 展现出在全球降水预测等场景的潜力。

Conclusion: DAWP 为在观测空间执行 AI 天气预报提供新的范式，降低对再分析数据的依赖，并在全球降水预测等领域具有较大应用前景。

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [32] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: 提出Cog-Rethinker：一个分层元认知强化学习框架，通过两阶段推理与监督微调来提升大型语言模型的推理效率和准确性，尤其在零样本RL训练中通过问题分解与纠错来提高样本利用率。


<details>
  <summary>Details</summary>
Motivation: 解决零RL在弱LLMs中由于直接rollout产生大量无效输出而导致的样本浪费问题；通过借鉴人类认知过程的分解与纠错，提高推理任务的样本利用率与收敛速度。

Method: 在RL训练的rollout基础上，提出分层元认知两阶段框架。阶段一：让策略将零准确性的问题分解为子问题，以产生可行的最终推理结果。阶段二：对前阶段中仍然存在零/低准确的问题，参考之前的错误解来进行改进。为实现模板的一致性和冷启动，使用基于直接rollout模板的两阶段正确样本对策略进行监督微调。

Result: 实验证明Cog-Rethinker在多组数学推理基准上显著优于对比方法，并分析出提高的样本效率和更快的收敛速度。

Conclusion: 该框架通过元认知级别的任务分解与纠错推理，提升了基于RL的LLM推理的样本效率和性能，具有将分解-改进的元认知思路推广到其他推理任务的潜力。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [33] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出 alpha-混合助手分布与 AMiD 框架，将助手分布设计扩展为带有连续参数 alpha 的族，并统一并扩展蒸馏中的散度选择，实验显示在性能与训练稳定性上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 解决自回归大语言模型的高计算/内存成本和知识蒸馏中的容量差距、近零概率导致的训练不稳定性，以及现有助手分布的碎片化问题，迫切需要一个系统、理论扎实的统一框架。

Method: 定义 alpha-混合助手分布，引入可调的参数 alpha 构成连续的助手分布族；在此基础上将与助手分布相关的散度进行一般化，形成 AMiD（alpha-mixture distillation）框架，并通过理论分析与大规模实验验证其有效性与稳定性。

Result: 在广泛实验中，AMiD 显示出更优的性能与更好的训练稳定性，归因于利用了更广泛且理论上有支撑的助手分布空间。

Conclusion: AMiD 为知识蒸馏提供一个统一、理论扎实且表达力更强的助手分布框架，能够提升小模型的学习效果与训练稳定性。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [34] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 提出MEET-Sepsis框架，通过MERE多端表示增强与CDTA时序注意机制，在仅需20% ICU监测时间的条件下，与SOTA方法相当甚至更优地进行早期脓毒症预测，并公开代码。


<details>
  <summary>Details</summary>
Motivation: 脓毒症早期征象微弱且易被忽视，现有AI方法难以捕捉早期信号；需要 richer feature views 与多尺度时序表示以提升预测能力；减少监测时间以便早期干预。

Method: 提出Multi-Endogenous-view Representation Enhancement (MERE) 架构来构建丰富的特征视图，结合 Cascaded Dual-convolution Time-series Attention (CDTA) 做多尺度时序表示学习。整合为MEET-Sepsis框架，且实现仅用20%监测时间的预测。

Result: 在实验中实现与SOTA方法相近或更优的预测准确度，同时显著降低所需监测时间；进行广泛的验证，证实有效性。

Conclusion: MEET-Sepsis展示了在减少监测时间同时保持预测性能的潜力，并且代码可复现，便于进一步研究与临床转化。

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [35] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出一个基于聚类的可解释性AI框架，用于对睡眠障碍患者进行分型并识别关键影响因素；在去标识化真实数据上实验，证明方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍对健康和生活质量有显著影响，但由于症状多样性，诊断复杂；需要可解释的AI以提升临床决策的透明度与信任度。

Method: 构建一个聚类驱动的患者分型方法，将可解释性方法整合以识别影响睡眠障碍的关键因素；在去标识化真实数据上进行实验评估。

Result: 实验结果显示该方法在分组与解释性方面具有有效性和相关性，成功识别出影响路径的关键因素。

Conclusion: 基于可解释性AI的聚类式睡眠障碍分型框架具有潜在的临床应用价值，有助于更好地理解患者谱系并辅助诊断。

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [36] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 通过追踪和操纵潜在的算法原语来理解大语言模型的多步推理，发现原语向量在激活空间呈现可组合的几何结构，具备跨任务和跨模型的转移性；推理微调增强了算法泛化。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型在多步推理中的内部机制，厘清推理过程是否能被可辨识的“算法原语”所支配，以及这些原语是否具有跨任务、跨模型的可转移性。

Method: 将推理过程与内部激活模式联系起来，通过对残差流注入原语向量并测量对推理步骤和任务表现的影响来操作化原语；通过聚类神经激活、标注匹配的推理轨迹、构建可重复使用的原语向量；使用向量运算（加减、标量乘法）构建组合原语；在四个基准任务（TSP、3SAT、AIME、图遍历）和跨模型（Phi-4、Phi-4-Reasoning、Llama-3-8B）上进行跨任务/跨模型评估。

Result: 发现存在共享与任务特异的原语，并且finetuning后在可验证性与路径生成等原语上表现出更系统的使用；将相关原语向量注入基线模型可复现Phi-4-Reasoning的行为特征；原语在激活空间呈现几何组合关系，显示推理具有可组合的算法原语结构并具有跨任务与跨模型的迁移性。

Conclusion: 推理在LLMs中可能由算法原语的可组合几何来支撑，原语具备跨任务/跨模型的传递性，且推理微调能加强跨领域的算法泛化。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [37] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: GRPO is a conservative reweighting method bounded by the base model distribution; OOD gains occur only when the task aligns with pretrained biases, while ID gains saturate; thus GRPO sharpens pretrained biases rather than universally enhancing reasoning.


<details>
  <summary>Details</summary>
Motivation: To explain the inconsistent improvements of GRPO across reasoning domains and establish conditions under which it improves reasoning and generalizes out-of-distribution.

Method: Theoretically prove that GRPO is a conservative reweighting scheme limited by the base model distribution. Conduct controlled experiments by training transformers from scratch and evaluating generalization across reasoning depth, input length, token representation, and compositionality.

Result: GRPO cannot discover solutions beyond the base distribution. Empirically, OOD improvements emerge only when target tasks align with pretrained biases; in-distribution gains diminish as performance saturates.

Conclusion: GRPO should not be viewed as a universal reasoning enhancer; it sharpens pretraining biases. Future work should develop algorithms that expand capabilities beyond pretraining origin.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [38] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: Stratos is an end-to-end LLM distillation pipeline that automates server and model selection, knowledge distillation, and deployment in distributed cloud environments under user-defined performance and budget constraints; it claims substantial accuracy gains and efficiency on vertical-domain tasks.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for customized, cost-efficient LLMs for vertical-domain tasks, especially under latency and budget constraints. Existing distillation frameworks require manual intervention and struggle to adapt to complex user-defined requirements.

Method: Stratos automates: (1) selecting Pareto-optimal servers, (2) dynamically matching teacher-student pairs, and (3) adapting distillation strategies based on task complexity, all within an end-to-end pipeline that deploys in distributed cloud environments.

Result: The approach yields a student model with four times the accuracy of its GPT-4o teacher baseline on a domain-specific Mahjong reasoning task using reverse synthetic data and knowledge injection, along with reduced latency and cost without sacrificing accuracy.

Conclusion: Stratos shows strong potential for automating vertical-domain LLM deployment by optimizing distillation under user-defined constraints and resource budgets.

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [39] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: Explore-then-commit for nonstationary bandits with action-dependent latent linear dynamics; achieves tilde O(T^{2/3}) regret.


<details>
  <summary>Details</summary>
Motivation: Address nonstationary rewards where latent states follow unknown linear dynamics driven by actions, creating a tension between short-term and long-term rewards.

Method: Exploration phase uses random Rademacher actions to estimate Markov parameters of the linear dynamics; commit phase optimizes a long-term action sequence using the estimated parameters; analyzes learning from temporally correlated rewards and connects optimization to indefinite quadratic programming over a hypercube; proposes SDP relaxation with Goemans-Williamson rounding.

Result: Regret bound of tilde O(T^{2/3}); near-optimal sample complexity and error bounds for system identification with bilinear rewards; NP-hardness of the long-term optimization via equivalence to indefinite quadratic optimization; provides suboptimality guarantee and a practical SDP-based rounding approach.

Conclusion: Presents a principled learn-and-optimize framework for nonstationary bandits with action-dependent latent dynamics, including theoretical regret guarantees and a practical relaxation-based solution for the long-horizon optimization.

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [40] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: 本文提出 S4ECG，一种结合结构化状态空间模型的多时间窗ECG分析架构，通过跨多个 epoch 的联合预测显著提升心律失常的检测性能，并给出最佳依赖时间窗为 10-20 分钟。


<details>
  <summary>Details</summary>
Motivation: ECG 信号具有全局趋势与局部波形特征的耦合关系，需同时高时间分辨率分析与跨时间尺度建模；现有方法在单一时间窗内难以捕获全局-局部互作用，导致诊断性能受限。

Method: 引入 S4ECG，基于结构化状态空间模型的深度学习架构，进行多时段/多 epoch 的联合预测；对比单 epoch 的基线；系统性分析最佳时间窗。

Result: 在宏 AUROC 上相比单 epoch 提升 1.0-11.6%；房颤特异性从 0.718-0.979 提升到 0.967-0.998；表现更好且对分布外样本具有鲁棒性；最佳时序依赖窗为 10-20 分钟。

Conclusion: 推动心电图解释向 temporally-aware 的心律失常检测转变，特别是对复杂心律如 AF 和房扑等，提供新的分析范式和应用前景。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [41] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: Proposes STAR, a plug-and-play module that makes Time Series Foundation Models aware of discrete state variables for multivariate anomaly detection, via an identity-guided state encoder with a state memory, a conditional bottleneck adapter, and a numeral-state matching component; leads to improved detection performance on real-world data.


<details>
  <summary>Details</summary>
Motivation: Existing Time Series Foundation Models largely treat state (categorical) variables the same as numerical ones, disregarding their semantic role as system conditions. This mismatch can underutilize state information and even degrade anomaly detection when state features are present.

Method: STAR comprises three components: 1) Identity-guided State Encoder with a learnable State Memory to capture complex categorical semantics; 2) Conditional Bottleneck Adapter that generates state-conditioned low-rank adaptation parameters to inject state influence into the backbone model; 3) Numeral-State Matching module to detect anomalies related to state variables. It is designed as a plug-and-play module for fine-tuning existing TSFMs.

Result: Extensive experiments on real-world datasets show that STAR improves the performance of existing TSFMs on Multivariate Time Series Anomaly Detection (MTSAD).

Conclusion: STAR provides a practical, state-aware enhancement to TSFMs for MTSAD, enabling better exploitation of discrete state information during fine-tuning and improving anomaly detection performance.

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [42] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出一种边缘-云级级联方法 CAb，通过 conformal alignment 将边缘预测集的“条件覆盖率”与云模型的预测分布对齐，能在保留云级覆盖概率的前提下显著降低边端外传请求。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备部署紧凑模型以实现低延迟推断的同时，如何在用户设定的风险水平下保证预测集合的条件覆盖（edge decision 的正确率等同于云模型的覆盖分布）仍然具有挑战性。需要一种能对边缘决策进行统计保证并提供覆盖—延迟—集合大小之间的可控权衡的方法。

Method: 将边缘到云的升级/推送过程建模为多重假设检验问题，提出 conformal alignment for cascade (CAb) 尺度的级联机制，通过对 CA 的改造实现对输入样本的“安全落地”边缘判定选择。该方法可应用于任意边缘预测集合，包含卷积预测的 CP 等变体，提供对边缘预测集合在平均层面上满足云级条件覆盖的统计保证，并可调节覆盖率、交付延迟（deferral rate）和集合大小之间的权衡。

Result: 在 CIFAR-100 图像分类和 TeleQnA 问答基准上，CAb 级联在保持目标边缘预测的条件覆盖的同时，显著降低边缘到云的外传比率，且预测集合规模仅有适度增加。

Conclusion: CAb 提供了一个具有统计保障的边云级级联框架，能够将边缘决策的风险控制与云级分布的一致性结合起来，具备对覆盖、延迟和集合大小的可控权衡能力，并具有对任意边缘预测集合的广泛适用性。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [43] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出一种基于特征驱动的强化学习（RL）方法，结合PPO优化，针对光伏（PV） intraday交易在序列决策框架内学习买卖策略。通过将数据驱动特征融入状态、以平衡利润与偏差罚则的奖励函数来实现，具备线性、可解释的策略，且在历史数据上训练、在样本外评估，显示优于基线并具备快速收敛、实时推理与透明决策规则的特性。


<details>
  <summary>Details</summary>
Motivation: 光伏发电存在显著不确定性，日内交易市场提供实时调整机会以提升收益并降低偏差成本。需要一个数据高效、便于部署的强化学习方法来利用市场微结构信息和历史特征实现主动参与。

Method: 将问题建模为马尔可夫决策过程（MDP），在状态中融入数据驱动特征。奖励函数在交易利润与偏差惩罚之间权衡。使用近端策略优化（PPO）进行策略优化，策略以线性、可解释的形式呈现。模型在历史市场数据上训练，进行样本外评估。

Result: 策略在多种场景下持续优于基准基线，展现快速收敛、可实现的实时推理和透明的决策规则。学习得到的权重强调市场微结构与历史特征的重要作用。

Conclusion: 特征驱动的强化学习为光伏生产商在日内市场中的主动参与提供了一种实用、数据高效且可部署的路线。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [44] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: 本文揭示在多智能体系统中，基于对齐的防御（如 LlamaFirewall）不足以防御控制流劫持攻击，提出并评估一种新的控制流完整性防御 ControlValve，通过生成并强制执行许可的控制流图以及针对每个智能体调用的情境规则来提升安全性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的安全性与功能性目标存在根本冲突；对齐性定义的脆弱性，以及执行上下文可见性不足，导致现有对齐检查难以防范攻击。

Method: 1) 演示通过绕过对齐检查的控制流劫持攻击，even when advanced LLMs perform checks; 2) 提出 ControlValve，受控制流完整性和最小权限原则启发； (a) 生成多智能体系统的允许控制流图； (b) 强制执行所有执行符合该图以及为每个智能体调用生成的上下文规则（零-shot）

Result: 实现并评估 ControlValve；在实验环境中展示其对许可控制流图和上下文规则的强制执行能力，验证其在避免攻击方面的有效性和可行性

Conclusion: 安全与功能性在多智能体系统中存在内在冲突；仅靠对齐性检查不足以提供稳健防护；通过将控制流完整性和最小权限原则结合的 ControlValve 提出了一种更具鲁棒性的防护方向。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [45] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 提出一种基于信息瓶颈的微调方法 IB-FT，旨在解决代码领域大模型在有监督微调中的 memorization barrier（记忆屏障）。通过在代码数据的隐藏表示上施加 IB 惩罚，压缩次要、易 memorized 的特征，同时保留与任务相关的信息，从而提升一般化的代码知识学习。实验证明在 OriGen 与 Evol-CodeAlpaca-V1 上，IB-FT 能显著缓解记忆屏障，提升 top-1 的 Pass@$1，并在更严格的多样本评测 Pass@$k^(m) 下具有更稳定的提升。


<details>
  <summary>Details</summary>
Motivation: 在对预训练大语言模型进行代码域微调以进行代码生成时，模型可能过度 memorizes 下游代码数据，导致优化被记忆驱动，难以学习新的、具有普适性的代码知识。需要一种方法来抑制过拟合到具体数据的记忆，使模型能学习通用的代码能力。

Method: 在对代码数据进行微调时，对模型隐藏层的表征施加信息瓶颈（IB）惩罚，以压缩与任务不相关或易 memorized 的特征，同时保留对完成代码任务所必需的信息，形成 IB-FT。

Result: 在两个代码基准（OriGen 与 Evol-CodeAlpaca-V1）上，IB-FT 显著缓解记忆屏障，提升 top-1 Pass@$1，并在更严格的 Pass@$k^(m) 指标下表现出更稳定的改进，相较于传统的 FT。

Conclusion: IB-FT 通过在隐藏表示上应用信息瓶颈，抑制 memorized 特征并保留任务相关信息，从而提升对代码的通用学习能力，提供对代码域微调的更稳健效果。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [46] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: PolyConFM 提出了一种面向聚合物构象的基础模型，通过对局部重现构象进行掩蔽自回归建模及其取向变换生成来实现聚合物的全局构象表示，并使用分子动力学模拟数据集进行预训练，达到跨任务的优越性与通用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多仅以单体级描述表示聚合物，忽略聚合物构象中的全局结构信息，导致性能受限；缺乏能够支撑多样下游任务的统一基础模型，限制科研进展；需要通过以构象为中心的预训练来统一建模和设计。

Method: 提出 PolyConFM，采用以局部构象为单位的条件生成框架，对局部构象进行掩蔽自回归建模，并生成它们的取向变换以恢复聚合物整体构象。同时通过分子动力学仿真构建高质量聚合物构象数据集以缓解数据稀疏性，并在此数据集上进行构象中心的预训练。

Result: 实验表明 PolyConFM 在多项下游任务上持续优于代表性任务特定方法，展现出较强的通用性和性能提升。

Conclusion: 该工作为聚合物科学提供一个统一且强大的基础工具，连接建模与设计，并以构象信息为核心推动领域进步。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [47] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [48] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [49] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: 提出了一种基于网格状格编码的记忆压缩方法（GCQ），在行动条件化的码本中通过离散化的网格状吸引子动力学实现时空联合压缩，构建统一世界模型以支持长时预测与规划。


<details>
  <summary>Details</summary>
Motivation: 解决传统向量量化在序列化观察-行动数据上的局限性，寻找一个能够同时对空间与时间进行压缩、并具备可扩展的世界模型能力的方法。

Method: 引入网格状吸引子动力学产生的连续码字，并建立行动条件化的码本，通过动作动态选择代码词，实现对观测-行动序列在时空维度上的联合离散化和编码。

Result: 在多任务场景中证明GCQ能实现紧凑编码并提升后续任务性能，支持长期预测、目标导向规划与反向建模。

Conclusion: GCQ提供了一种将网格状编码用于序列建模的统一框架，既作为高效序列建模的计算工具，也为神经系统中网格状编码的形成提供理论视角。

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [50] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: AMS-Quant 将浮点量化扩展到非整数位宽，提出 Mantissa-bit Sharing 与 Adaptive Searching，通过 CUDA 实现，能在解码阶段把模型量化至 FP-5.33-e2m3 与 FP4.25-e2m2，在比 FP16 的解码中获得约2.8x–3.2x 的加速，且精度损失微小。


<details>
  <summary>Details</summary>
Motivation: LLMs 参数规模越来越大，传统整数量化在存储/带宽与精度之间的权衡有限，探索非整数位宽的浮点量化以更接近量化甜点。

Method: 提出 Mantissa-bit Sharing：将 k 个权重分组，共享最低有效尾数位，接近最小位宽；Adaptive Searching：离线优化策略，最小化共享带来的精度下降；并将其原型实现为高效 CUDA Linear kernels，以降低内存访问成本。

Result: 在大规模数据集和模型上，AMS-Quant 能将模型量化到 FP-5.33-e2m3 和 FP4.25-e2m2；与 FP16 相比，解码速度提高约 2.8x 与 3.2x，且精度损失可忽略。

Conclusion: 证明非整数位宽浮点量化在保持精度的同时，显著提升推理效率与内存效率，且离线搜索与分组共享机制是实现近似下界的关键，CUDA 内核实现实现了实际的性能收益。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [51] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 对五种时间序列基础模型及两种基线的校准性进行系统评估，结果显示基础模型通常具有更良好的校准性，且在长期自回归预测中不呈现明显的过度自信或欠自信现象。


<details>
  <summary>Details</summary>
Motivation: 尽管 foundation 模型在预测性能上达到 SOTA，但其在校准性方面的研究相对不足，这在需要概率输出可靠性的实际应用中尤为重要。

Method: 通过一系列系统评估，检测模型的校准（过度/欠置信）、预测头的影响，以及在长期自回归预测情境下的校准表现。比较五种最新的时间序列基础模型与两种竞争基线。

Result: 时间序列基础模型在校准性方面通常优于基线，且不易出现系统性过度自信或欠自信，与在其他深度学习模型中常见的过度自信现象形成对比。

Conclusion: 时间序列基础模型在校准性方面展现优势，适用于需要可靠概率输出的应用，特别是长期预测场景；未来研究可进一步优化校准方法并扩展到更广的应用领域。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [52] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: FedPURIN uses an integer-programming-based parameter selection and sparse aggregation to cut communication in Federated Learning with Personalization, while preserving accuracy under non-IID data.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity and high communication costs in PFL hinder practical deployment; there is a need for efficient, parameter-level transmission control.

Method: Formulate a 'programmed Update' as an integer programming problem to identify critical parameters to transmit, and integrate this with a sparse aggregation scheme within a PFL framework (edge-oriented).

Result: Empirical evaluations on standard image classification benchmarks under varied non-IID partitions show competitive accuracy relative to state-of-the-art methods, coupled with substantial communication reductions via sparse aggregation.

Conclusion: Proposes a new paradigm for communication-efficient PFL, offering benefits for edge intelligence systems dealing with heterogeneous data sources.

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [53] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出了一种面向3D非结构点云的多尺度神经算子(MNO)，通过全局降维注意、局部图注意和微观点级注意三尺度信息分解，在4个基准上对CFD问题表现优于SOTA，能处理高达30万点。


<details>
  <summary>Details</summary>
Motivation: 解决神经算子在不规则域上的精度与可扩展性不足的问题，尤其在多尺度的三维流场中，需要同时建模跨尺度的依赖关系。

Method: 提出MNO架构，包含三层注意机制：全局维度压缩注意用于远程依赖，局部图注意用于邻域交互，微观点级注意用于细粒度细节；在3D非结构点云上执行，保持多尺度归纳偏置，同时保持计算效率；在4个基准的稳态与非稳态流场上评估，点数达300k。

Result: 在所有任务中均优于SOTA基线，预测误差降低5%-40%，在挑战性3D CFD问题上表现出更高鲁棒性，验证显式多尺度设计的有效性。

Conclusion: 明确的多尺度设计对神经算子学习复杂流动具有重要意义，MNO为在不规则域上学习复杂流体动力学提供一个可扩展框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [54] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 基于随机矩阵理论的 Transformer 训练动力学分析框架，提出基于谱分布的三阶段训练划分及两项无验证的早停准则。


<details>
  <summary>Details</summary>
Motivation: 理解 Transformer 训练中的性能提升机制，建立一个可解释的早停准则体系。

Method: 建立基于随机矩阵理论的理论框架；观测浅层自注意力矩阵 V 的谱密度收敛到重尾分布；以幂律拟合作为探针；将训练过程划分为结构探索、重尾结构稳定化、收敛饱和三个阶段；提出两项一致、无验证的准则：重尾动态的定量度量和收敛的谱特征。

Result: 实现了三阶段训练划分并对早停提供指导；两项准则高度一致且无需额外验证，证明了 RMT 在监控 Transformer 训练进程中的实用性。

Conclusion: RMT 为监控与诊断 Transformer 训练提供有力工具，揭示训练动力学并支持有效的训练管理。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [55] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出 BPL（Bias-adaptive Preference Distillation Learning），通过双蒸馏策略在事实与反事实测试环境中对偏置反馈进行适应性蒸馏，提升推荐准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实中的用户反馈存在偏差， debiasing 学习多聚焦于对抗性（counterfactual）测试环境，导致在真实（factual）测试中的性能下降；需要一个在两种测试环境都表现良好的模型。

Method: 引入偏置自适应偏好蒸馏框架 BPL，包含两种蒸馏策略：1) 来自有偏见模型的教师-学生蒸馏，保留与已收集反馈一致的偏好知识以提升 factual 测试；2) 通过带可靠性筛选的自蒸馏，在训练过程中迭代细化知识，从而覆盖更广的用户-物品组合，提升 counterfactual 测试。

Result: 在事实和反事实测试中进行了全面实验，验证了 BPL 的有效性；实现可在 GitHub 公开获取，链接为 https://github.com/SeongKu-Kang/BPL。

Conclusion: BPL 通过结合偏见知识与自蒸馏的迭代更新，在两种测试环境中达到更高的推荐性能与鲁棒性。

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [56] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: Equilibrium Propagation (EP) is extended to discrete and continuous complex-valued wave systems, working in weakly dissipative regimes and enabling in-situ, locally-driven learning in physical networks; demonstrated via exciton-polariton condensates and standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Tackle the difficulty of implementing backpropagation-like learning in physical neural networks by developing an EP-based training scheme applicable to wave-based dynamics, including regimes without well-defined nodes.

Method: Generalize EP to discrete/continuous complex-valued wave systems governed by driven-dissipative dynamics (e.g., generalized Gross-Pitaevskii). Replace trainable inter-node connections with trainable local potentials; validate in weakly dissipative regime and apply to exciton-polariton condensates; evaluate on benchmarks such as a logical task and handwritten-digit recognition.

Result: Numerical studies show stable convergence and successful learning, indicating EP-based in-situ training is feasible in physical systems with local control.

Conclusion: EP can be generalized to a wide range of wave-based physical systems, offering a practical path to in-situ learning without global connectivity, suitable for programmable photonic/condensed-mmatter hardware.

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [57] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出一种基于因式分解的语义恢复框架FSRF，用以缓解多模态情感分析中的模态缺失问题，结合去冗余的同质-异质-噪声因式分解与分布对齐自蒸馏，在两数据集上对比缺失模态的情况下显著优于前人方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中模态缺失导致泛化能力下降，传统MSA多在完整模态上交互融合，亟需对缺失模态的鲁棒处理。

Method: 提出去冗余的同质-异质-噪声三元分解的模态因式分解模块，结合对模态表征的约束学习；并设计一个分布对齐的自蒸馏模块，通过双向知识传递实现对缺失语义的全面恢复。

Result: 在两个数据集的实验中，FSRF在模态缺失场景下对比基线显示出显著的性能提升。

Conclusion: FSRF有效缓解缺失模态问题，提升MSA的鲁棒性和泛化能力。

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [58] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: STABLE 是一个基于门控的连续自我编辑框架，利用LoRA实现参数高效微调，通过对候选编辑使用稳定性预算（Exact Match 丢失、信心增量、KL散度）进行筛选，超出阈值则进行截断或拒绝。实验在 Qwen-2.5-7B 上证明门控能在保护已学知识的同时维持适应性，EM门控在短序列中达到最高的累计性能，代码已发布。


<details>
  <summary>Details</summary>
Motivation: 避免对大语言模型进行全量再训练时出现的灾难性遗忘问题，提出门控式连续自我编辑以实现高效且受控的逐步知识更新。

Method: 提出 STABLE 框架，通过门控的连续自我编辑，利用低秩适应 LoRA 进行参数高效微调。对每个候选编辑在稳定性预算内评估，使用 Exact Match（EM）丢失、比特增量和 KL 散度等三种指标；若超出阈值则对 LoRA 更新进行裁剪或拒绝。对 Qwen-2.5-7B 进行实验，验证门控在抑制遗忘的同时保持可适应性。

Result: 实验表明门控能有效降低遗忘并保持适应性；基于 EM 的门控在短期连续学习序列中获得最高的累计性能；不同的门控策略在分布偏移（KL 散度）方面表现相近，但对精度有显著差异，强调门控设计对持续适应的重要性；给出开源代码。

Conclusion: 提供一个有理论与实践支持的连续模型编辑范式，使大模型在引入新知识的同时保持可靠性。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [59] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: MemCom 提出一种分层的提示压缩方法，用于将多-shot In-Context Learning 的输入长度从 t tokens 缩减到 m 软令牌，同时在不同 transformer 层提供单独的压缩表示。


<details>
  <summary>Details</summary>
Motivation: 在多-shot ICL 中，示例数量的增加提升性能但显著增加内存与计算成本；需要一种高效的压缩策略以降低推理成本且尽量保留准确性。

Method: 提出 MemCom，一种层级（Layer-wise）压缩框架，在每个 Transformer 层对多-shot表示进行独立压缩，并比较多种压缩器模型与训练策略；在不同模型尺寸（2B、7B）、架构（Gemma、Mistral）、多-shot长度（3k-6k tokens）和压缩比（3x-8x）上进行系统评估。

Result: MemCom 在所有压缩比下均优于强基线，且在高压缩比下的性能下降显著低于基线；在多个具有大标签集的分类任务上，基线通常下降 20-30% 以上，而 MemCom 的下降通常小于 10%，显示出更好的鲁棒性与效率。

Conclusion: 层级化的压缩 coupled with 更强的压缩器参数规模，是实现高效且鲁棒的多-shot ICL 的关键，MemCom 提供了一实用且具潜在泛化性的解决方案。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [60] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 在不进行训练的情况下，基于相似性搜索的世界模型可与训练型模型（如 PlaNet/Dreamer 家族）相媲美，且在长时预测上具有优势。


<details>
  <summary>Details</summary>
Motivation: 探究是否可以通过无训练过程的搜索型世界模型来近似或超越训练型模型在潜在变量重建质量与长时序动态预测上的性能，并与 PlaNet 进行对比。

Method: 利用相似性搜索与随机表示来近似世界模型，未执行传统训练过程；与 PlaNet 进行比较。评估维度包括潜在变量的重建质量、重建图像的感知相似性，以及对下一步和长时序动力学的预测。

Result: 所提出的搜索型世界模型在潜在变量重建和图像重建的感知相似性方面与训练型模型相当；在下一步和长时序预测均表现接近，且在跨多种视觉差异环境中，长时序预测表现优于基线。

Conclusion: 搜索型世界模型可以成为训练型模型的有力替代，在长时序动态预测方面尤其具备优势，为图像基强化学习提供了另一种可行路径。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [61] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: First finite-time analysis of on-policy Q-learning with time-varying policies under a minimal irreducibility assumption; obtains last-iterate convergence for E||Q_k−Q*||^2_inf and O(1/ε^2) sample complexity, plus an explicit rate for E||Q^{π_k}−Q*||^2_inf. Shows weaker exploration but faster exploitation (policy converges to optimum).


<details>
  <summary>Details</summary>
Motivation: Fill the gap in finite-time analysis for on-policy, time-varying policies in Q-learning under minimal exploration assumptions, where the sampling is driven by a possibly rapidly changing policy but still induces an irreducible Markov chain.

Method: Use Poisson equation to decompose the time-inhomogeneous Markovian noise from the lazy transition matrix into a martingale-difference term and residual terms. Perform sensitivity analysis of the Poisson equation solution with respect to both Q-function estimates and the evolving policy to control residuals. Develop last-iterate convergence results and explicit rates.

Result: Establish a last-iterate convergence rate for E[||Q_k−Q*||^2_inf] with sample complexity O(1/ε^2). Obtain an explicit rate for E[||Q^{π_k}−Q*||^2_inf]. The results align with off-policy Q-learning in sample complexity but reveal weaker exploration and an exploitation advantage for on-policy learning. Numerical simulations corroborate the theory.

Conclusion: On-policy Q-learning with time-varying policies can achieve optimality with finite-time guarantees, albeit with different exploration-exploitation trade-offs compared to off-policy methods. The developed Poisson-equation-based analysis and sensitivity toolbox may extend to other rapidly time-varying RL algorithms, such as single-timescale actor–critic and learning-in-games methods.

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [62] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出通过指数倾斜的软 SAM 框架，将零阶优化（对扰动参数集合的平均损失）与 SAM 的最大损失导向联系起来，得到一个可调的平滑目标。并给出基于该目标的无梯度、低内存的新算法，在下游任务上实现比传统零阶基线更好的泛化。


<details>
  <summary>Details</summary>
Motivation: 解决传统零阶优化偏向最小化平均损失与 SAM 偏向优化最大损失之间的矛盾，提供一个统一的、可调的尖锐性（sharpness）定义与优化目标，以提高模型的泛化能力。

Method: 引入带有 tilting 参数 t 的软 SAM 目标，通过指数倾斜在平均损失与最大损失之间实现平滑过渡；提出新的零阶算法以求解该目标，并对 tilted SAM 的尖锐性概念进行严格表征。

Result: 在分类、多项选择问答、语言生成等多种下游任务上，所提出的方法比 vanilla 零阶基线具有更好的泛化表现。

Conclusion: 通过 tilt 的 SAM 框架将零阶优化与尖锐性优化整合，提供一种高效的无梯度优化替代，并为 sharpness 的新定义提供清晰刻画。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [63] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 本文对三种生成模型在材料数据集上的生成能力进行系统基准评测，发现 CDVAE 在 KL 散度和 MAE 指标上表现最佳，其次是 AtomGPT，FlowMM 最差，相关代码将公开。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对不同生成模型在材料数据集上的严格、可重复比较，需要统一的数据集和评估来客观比较模型性能。

Method: 将 AtomGPT、CDVAE、FlowMM 在两个公开的超导数据子集上进行训练，任务为从子集重构晶体结构。评估指标包括预测分布与参考分布的 lattice parameters 的 Kullback–Leibler 散度(KL) 和单个晶格常数的平均绝对误差(MAE)。数据集来自 JARVIS Supercon 3D 和 Alexandria 数据库的 DS A/B。

Result: 在所选的 KLD 和 MAE 指标上，CDVAE 表现最好，其次是 AtomGPT，FlowMM 最差。

Conclusion: CDVAE 在该基准中的优势指向其更有效地建模晶格参数分布；研究团队计划将基准代码和模型配置在 GitHub 上公开，以促进重复性和对比性分析。

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [64] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 提出了一种多轮梯度攻击方法，通过随机初始化和样本混合，在尽量小的扰动下实现高欺骗成功率，获得竞赛第一名。


<details>
  <summary>Details</summary>
Motivation: 评估并提升对抗鲁棒性，挑战在最小扰动约束下最大化分类错误的能力。

Method: 在模型可微结构上执行多轮梯度攻击，结合随机起始点和样本混合技术，以提高攻击的覆盖度和稳定性。

Result: 在扰动规模和欺骗成功率上达到最佳表现，夺得比赛第一名。

Conclusion: 该方法证实了将梯度对抗与随机化策略相结合的有效性，适用于高能物理场景的对抗鲁棒性评估，并为未来对抗样本生成提供思路。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [65] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 通过对 Llama-3.2-1B 的层级范围进行因果补丁分析，发现对奖励一致性行为的对齐主要来自中层子空间，早、晚层影响较小；使用 LASSO 发现只有少数层与激活距离的奖励增益相关，表明人类偏好微调的对齐是低秩且方向性强的。


<details>
  <summary>Details</summary>
Motivation: 尽管 RLHF 在对齐语言模型方面广泛应用，但其内部工作机制尚不清楚。本研究通过层级级别的因果分析，揭示偏好优化如何在模型内部实现。

Method: 在基模型和微调模型之间，对跨越人类偏好对进行层级范围的因果补丁，定位影响奖励的中间层子空间；在 Llama-3.2-1B 上实施；使用 LASSO 回归，将激活距离与奖励增益相关联。

Result: 对齐是局部的：中层激活组成的子空间决定奖励一致行为；早、晚层基本不受影响；少数层有非零回归系数；对齐表现为向量方向性、低秩结构。

Conclusion: 对于某些语言模型，基于人类偏好的对齐并非全局分布，而是来自中层的有限子空间，因此可解释性更高，潜在的剪裁或更高效的微调路径。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [66] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本论文主张：RL研究应从单纯追求性能转向理解学习过程和数学基础，并需要更清晰地将基准与正式形式对应。


<details>
  <summary>Details</summary>
Motivation: 作者指出当前RL研究过度聚焦于在学术基准上的性能，可能导致对学习动力学的理解不足、对新问题的迁移性不利，并且基准与理论形式的映射往往不明确。

Method: 以 Arcade Learning Environment (ALE) 为案例，论证如何在看似“饱和”的基准上开展理解性研究，展示在理论与实际部署之间建立更紧密联系的路径。

Result: 提出两点核心主张并对其可行性进行讨论：一是把研究重点从单纯能力展示转向科学理解与方法论；二是对基准的数学形式映射提出更高的精确性要求，借以促进 RL 技术的可迁移性与应用落地。

Conclusion: 通过以 ALE 为例，展示在饱和基准上进行理解性研究的可行性，以及如何辅助 RL 技术在真实世界问题中的部署。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [67] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 提出了一种基于运行时监控语言(RML)的语言型Reward Machines，能够表达非正则、非马尔可夫的奖励函数，超越传统仅限正则语言的Reward Machines；并在实验中展示了更强的表达力与对事件处理和任务规范的优势。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励设定若定义不当，会导致非期望甚至有害的行为；传统 Reward Machines 受限于正则语言，难以表达计数、参数化条件等复杂非马尔可夫奖励；需要更丰富、可解释的奖励表示来提升学习效果与可控性.

Method: 在Reward Machines中引入运行时监控语言(RML)的记忆能力，构建语言型Reward Machines，使其能够指定非正则、非马尔可夫的奖励函数；通过实验评估表达力和对事件处理/任务规范的改进。

Result: 实验表明，该方法在表达力上超越基于传统Reward Machines的方法，并在灵活的事件处理与任务规范方面展现明显优势。

Conclusion: 通过利用RML的内置记忆，语言型Reward Machines扩展了Reward Machines的表达能力，能够覆盖更复杂的非马尔可夫奖励任务，提升可解释性与灵活性，为RL中的奖励设计提供新的研究路径。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [68] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出一个结合关系强化学习（RRL）与面向对象表示的框架，并通过对策略不确定性的显式建模实现对专家的主动查询，以处理结构化和非结构化数据；实验证明该方法有效且学习效率高。


<details>
  <summary>Details</summary>
Motivation: 强化学习在图像/视频等无结构任务上表现出色，但在具有结构信息的任务中往往受限，现有的关系强化学习（RRL）对问题结构有较强假设，难以泛化到任意数量的对象并兼容非结构化数据，因此需要一个更灵活的表示与学习框架。

Method: 将关系强化学习与面向对象对象中心表示相结合，建立一个统一的方法来处理结构化和非结构化数据；引入对策略不确定性的显式建模，使系统能够在不确定性下主动向人类专家寻求指导（主动查询/人机互动），以提升学习效率与策略质量。

Result: 通过实验评估，所提框架在有效性和学习效率上表现良好，能在结构化与非结构化数据场景中展示出优势。

Conclusion: 该工作提出了一种统一处理结构化和非结构化数据的RL框架，结合对象中心表示和对策略不确定性的主动查询机制，具备良好的泛化性和学习效率，并得到实验上的支持。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [69] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 提出一个关于标签噪声检测的综合基准，提出将检测方法分解为标签一致性函数、聚合方法和信息获取方式（样本内/样本外），并在固定噪声比上进行统一评测；在合成与真实噪声的视觉与表格数据集上，样本内信息获取、平均概率聚合以及对数边缘（logit margin）作为标签一致性函数的组合表现最佳，为设计新方法和应用选型提供实际指导。


<details>
  <summary>Details</summary>
Motivation: 现实数据中的标签噪声普遍存在，影响模型训练与评估，因此需要一个公平、可复现的基准来系统性地比较检测方法，并帮助设计更有效的检测策略。

Method: 将检测方法分解为三个核心组件：标签一致性函数（如何衡量标签与模型预测的一致性）、聚合方法（如平均概率）以及信息获取途径（在样本内 vs 在样本外）。提出统一的基准任务：检测的样本量等于数据集的噪声率，并引入在该固定工作点的假阴性率作为新指标。对视觉和表格数据集，在合成和真实噪声条件下进行评估。

Result: 在多数情境中，使用样本内信息获取、平均概率聚合以及以对数边缘（logit margin）作为标签一致性函数的组合表现最佳，提供了对噪声检测方法的实用设计与选型指南。

Conclusion: 研究提出的分解框架可广泛应用于现有检测方法的改进与比较，统一的基准任务有助于在不同应用场景中做出更可靠的选择，并为未来方法的开发提供方向。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [70] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: A machine learning study analyzes climate policy progression within the European Green Deal using 165 policies, comparing text representations (TF-IDF, BERT, ClimateBERT) with and without metadata; ClimateBERT excels on text alone, while adding metadata improves BERT performance; explainable AI identifies wording and metadata as key drivers; demonstrates ML's potential in climate policy analysis and decision-making.


<details>
  <summary>Details</summary>
Motivation: To understand and forecast how climate policies progress from announcement to adoption, addressing the complexity of policy dynamics and the need for data-driven insights.

Method: Assemble a dataset of 165 European Green Deal policies with text and metadata. Evaluate text representations (TF-IDF, BERT, ClimateBERT) for predicting policy progression status. Include metadata features and employ explainable AI to interpret influential factors such as wording, party representation, and country representation.

Result: ClimateBERT outperforms on text-only features (RMSE 0.17, R^2 0.29). With metadata, BERT achieves better performance (RMSE 0.16, R^2 0.38). Explainable AI highlights policy wording and metadata (e.g., political party, country representation) as influential factors.

Conclusion: ML tools hold promise for climate policy analysis and decision-making; integrating text and metadata improves predictive power, and explainability provides actionable insights into drivers of policy progression.

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [71] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出基于神经ODE的连续深度 Evoformer，以替代原本 48 层离散块，实现常量内存和可调运行时的蛋白质结构预测模型。


<details>
  <summary>Details</summary>
Motivation: 解决 Evoformer 深度带来的高计算成本和层级离散化限制，同时提升资源效率与可解释性。

Method: 将 Evoformer 的离散块替换为 Neural ODE 参数化，保留核心自注意力操作；通过 adjoint 方法实现恒定深度内存成本，并利用自适应求解器在运行时与精度之间权衡。

Result: 在蛋白质结构预测任务中获得结构合理的预测，能捕捉如 α-螺旋等二级结构元素，但精度尚未完全达到原始架构；训练成本显著降低，单GPU训练约 17.5 小时。

Conclusion: 连续深度模型为生物大分子建模提供高效、可解释的替代方案，开启在蛋白质结构预测中的新方向。

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [72] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 在联合的 QKV 权重矩阵上应用 SVD，以减少 KV 缓存大小和计算开销；引入动态秩分配以根据对 VLM 精度的影响自适应调整秩，并对权重和激活进行量化，获得显著的性能与效率提升。


<details>
  <summary>Details</summary>
Motivation: VLMs 的高计算成本与内存占用限制了规模化和实时应用，需要更高效的压缩/近似策略来提升推理效率。

Method: 对 Q、K、V 的联合权重矩阵应用奇异值分解（SVD）进行压缩；提出动态秩分配策略，在对准确率影响可控的前提下自适应调整秩；进一步对权重与激活进行量化，形成端到端高效推理方案。

Result: 该方法在保持或提升准确性的同时显著降低硬件开销，相较仅量化或仅 SVD 的方法有超过 10% 的准确性提升，并更适合在资源受限设备上实现实时部署。

Conclusion: 将 SVD、动态秩分配与量化结合，提供了一个更高效的 VLM 压缩与推理框架，显著提升性能与效率，并提供开源实现以便复现。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [73] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: ScaffAug 提出一个基于 scaffold 的 VS 框架，包括：使用图扩散的 scaffold 条件化数据增强；基于 scaffold 的采样以缓解类别和结构不平衡；自训练安全整合合成数据；再排序以提升 top 结果的 scaffold 多样性，同时维持整体性能。


<details>
  <summary>Details</summary>
Motivation: 药物发现中的配体基虚拟筛选面临三个挑战：活性样本极少导致类别不平衡、活性分子结构骨架分布不均导致结构不平衡，以及需要发现具有结构多样性的活性化合物以实现新药开发。现有方法往往在这三方面存在局限。

Method: 提出三个模块的 ScaffAug：1) augmentation 模块：利用图扩散在以真实命中骨架为条件下生成合成数据，缓解类别与结构不平衡；2) scaffold-aware sampling：对 underrepresented 的骨架样本进行更高比例采样，从而改善结构不平衡；3) self-training：对生成数据与原始标注数据进行安全融合；4) reranking 模块：在保持整体性能的前提下，通过提高 top 结果的骨架多样性来改善虚拟筛选的新颖性与多样性。

Result: 在五个靶标类别上进行了全面的计算实验，并与基线方法进行对比，报告多种评估指标并进行消融研究，验证了基于生成增强、 scaffold 感知以及再排序策略的有效性。

Conclusion: ScaffAug 从生成增强、 scaffold 感知以及再排序三个方面，给出了一种新颖的 VS 框架，能够提升活性分子的发掘效率与骨架多样性，同时保持或提升对新颖活性化合物的总体识别能力。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [74] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: 提出 MGTS-Net，通过多模态特征提取、异构图建模与多尺度预测，提升时间序列预测精度与效率。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态信息有潜力提升预测，但现有方法在细粒度时序模式提取、跨模态信息整合和对动态多尺度特征的自适应方面存在瓶颈。

Method: 设计 MGTS-Net，包含 MFE、MFF、MSP 三个核心组件，其中 MFE 依据时序、可视化、文本模态特性优化特征编码，MFF 构建异构图建模模态内时序依赖与跨模态对齐关系并实现动态聚合，MSP 根据短/中/长期预测器输出动态加权融合以适应多尺度特征。

Result: 实验结果表明 MGTS-Net 在轻量高效前提下具备优异性能，相较于最先进基线模型达到更好效果，验证所提方法的有效性。

Conclusion: MGTS-Net 通过三大模块实现对时序多模态信息的高效整合和多尺度预测，具有潜力应用于实时高效的时间序列预测任务。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [75] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出一种稀疏 Transformer 架构，将数据分布先验融入模型，并通过正则化 Wasserstein proximal 操作实现闭式解，提升样本稀疏性与收敛性。


<details>
  <summary>Details</summary>
Motivation: 希望将数据分布的先验信息直接融入模型结构，以改善优化的凸性、增强生成样本的稀疏性，并提升对目标分布的逼近速度。

Method: 以正则化 Wasserstein proximal operator 的闭式解为核心，将其作为 Transformer 的特殊表示形式，设计稀疏 Transformer 架构，并在生成建模和贝叶斯逆问题中进行理论分析与数值验证。

Result: 理论分析与数值实验表明，稀疏 Transformer 在逼近目标分布方面具有更高的准确性和更快的收敛速度，且生成样本呈现更明显的稀疏性，相较于经典流模型与神经微分方程方法有优越性。

Conclusion: 将先验信息直接嵌入 Transformer 架构是可行且有效的，尤其在生成建模和贝叶斯逆问题等应用中，能够提升性能并加速收敛；同时揭示了 Transformer 作为一种特殊的最优传输表示的潜力。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [76] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 通过将路由概率建模为分布混合，提出一种输入域感知的稀疏专家路由方法（Input Domain Aware MoE），以在不依赖特定任务目标的情况下实现专家的清晰专门化与负载均衡，从而提升 vision-language 任务中的性能与资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似度评分的路由在捕捉输入结构方面受限，导致专家专门化与计算负载平衡之间存在权衡，限制了可扩展性与性能。

Method: 提出一种基于概率混合模型的路由机制，将路由概率视为不同分布的混合，允许专家在更明确的输入域边界上专门化，并实现负载均衡。该路由机制在训练阶段与具体任务目标解耦，提升稳定性并获取明确的专家分配。

Result: 在视觉-语言任务上，所提方法持续超过现有的 sMoE 方法，在任务性能和专家利用率的平衡方面均有改进。

Conclusion: 提出的输入域感知路由框架实现了更清晰的专家专门化与更平衡的计算分配，具有良好的可扩展性与稳定性，适用于大规模 Vision-Language 模型。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [77] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 一项序贯强化学习的模仿学习框架，用以建模蜜蜂等传粉者的异质认知策略，通过学习有效记忆视野来预测行为，具可解释性，并发布80只蜂的跟踪数据集，提升现有模仿学习在多策略情境下的表现。


<details>
  <summary>Details</summary>
Motivation: 解释性与灵活性不足的现有模仿学习在多策略和记忆窗口变化情境下难以捕捉关键决策模式；需要揭示记忆、环境因素对策略影响的模型，以支持生物学解读与生态管理。

Method: 提出一个序贯强化学习的模仿学习框架，利用轨迹相似性在具有不同策略的个体之间对齐；在最小化预测损失的同时识别与数据最一致的有效记忆时界；强调可解释性，以便生物学家分析决策策略；构建一个将蜂群策略搜索与带有探索-利用权衡的博弈（bandit）形式联系起来的数学框架；并发布80只在不同天气条件下跟踪的蜂的新的数据集。

Result: 与最先进的模仿学习方法相比，当专家策略在记忆窗口之间转移或偏离最优时，现有方法往往无法捕捉快速和缓慢的学习行为，也难以解释关键决策模式；提出的方法在预测性能、记忆窗口识别和可解释性方面表现更好，提供了有用的生物学洞见和数据集基准。

Conclusion: 该框架推进了对传粉动物认知与记忆交互的理解，提升对植物授粉者学习策略的认识；支持更真实的蜜蜂行为仿真，助力生态治理；为未来在传粉动物认知研究中探索学习策略与记忆机制提供新的研究路径。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [78] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: OracleAD是一种简单、可解释的无监督多变量时间序列异常检测框架。通过将各变量的过去序列编码为因果嵌入，联合预测当前时点并重构输入窗口；嵌入通过自注意力投射到共享潜在空间以捕获时空关系，并通过稳定潜在结构（SLS）对齐以定义正常状态。基于预测误差和对SLS的偏离进行双重打分，实现逐时点、逐变量的细粒度异常诊断；能够在嵌入层定位根源变量，且在多个真实数据集上达到最新之优。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多变量时间序列异常稀少且往往未标注。现有方法往往依赖更复杂的架构并在基准上调优，易出现对异常段的局部检测和对性能的过度声称。因此需要一个简单、可解释且无标注数据下有效的框架来建模时间和变量之间的因果关系，提供可解释的根因诊断。

Method: 为每个变量将过去序列编码成单一因果嵌入；合并嵌入以预测当前时间点并重建输入窗口，从而建模时间动态。通过自注意力将这些嵌入投射到共享潜在空间，捕获空间关系。这些关系并非静态，而是由每个变量的时间动态衍生的属性所驱动。投射得到的嵌入对齐到一个表示正常状态关系的稳定潜在结构SLS。异常通过基于预测误差与对SLS偏离的双重打分来检测，并实现对每个时间点和每个变量的细粒度诊断。此外，任何SLS偏离都源于违反正常数据的时间因果性，因此能在嵌入层定位根因变量。

Result: 在多个真实世界数据集和评估协议上达到最先进的结果（state-of-the-art），并保持可解释性（通过SLS）。

Conclusion: OracleAD提供了一种简单且可解释的无监督框架，用于多变量时间序列的异常检测，能在时间和变量层面实现根因诊断，并在现实数据集上展现优越性能。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [79] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 提出了 eDCF，一种基于局部连通性指标 Connectivity Factor 的跨尺度内在维度估计方法，具可扩展性和并行性，在含噪数据上与领先方法相当，且在准确性上提升，能检测分形几何的决策边界，适用于结构化数据分析。


<details>
  <summary>Details</summary>
Motivation: 估计数据集的内在维度（id）在不同尺度下具有显著偏差：微尺度噪声膨胀 id，粗尺度趋向尺度不变的较低值，因此需要一个鲁棒且可扩展的多尺度估计方法来稳定地测量数据复杂度。

Method: 提出基于 Connectivity Factor（CF）的本地连通性指标，形成一个可扩展、并行化的 eDCF 流程，用于在不同尺度上鲁棒估计 id。

Result: 在带噪声的合成基准数据上，eDCF 的 MAE 与主流估计器相当；在 exact id 匹配率方面达到最高 25.0%，高于 MLE 的16.7% 和 TWO-NN 的12.5%。在中到高噪声水平和大规模数据集下表现尤为突出，还有能力检测决策边界中的分形几何，证明其在分析结构化数据中的实用性。

Conclusion: 该方法提供了一种鲁棒、可并行实现的跨尺度内在维度估计解决方案，适用于大规模、结构化数据的分析，并具备检测分形几何的能力。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [80] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: 本文质疑大语言模型在因果发现中的能力，指出对其评估须防止数据泄露和记忆化，主张构建泄露抵抗的评估基准并结合LLM知识与数据驱动统计的混合方法；将LLM作为先验信息用于PC算法可显著提升准确性，推动真实世界科学发现的研究方向。


<details>
  <summary>Details</summary>
Motivation: 挑战对LLMs在因果发现中的过度乐观结论，强调需要稳健的评估以避免记忆泄露，并推动将LLM知识与统计方法融合以实现可靠的因果发现。

Method: 提出两点方向：P.1 基于真实科学研究的新颖文献的泄露抵抗评估协议，避免训练数据泄露，提取最近发表的因果图以涵盖既有与新颖关系；P.2 将LLM预测作为先验信息融入传统PC算法等统计方法，并在BNLearn等基准上对比，评估混合方法的提升。给出从最新研究中提取因果图的实用流程，确保相关性与防记忆。

Result: 在基准BNLearn上，LLMs的正确率接近完美；但在精心 curate 的真实因果图上表现显著下降，表明仅靠LLM缺乏统计 grounding；使用LLM先验的PC算法显著提高了准确性，优于仅LLM和纯统计方法。

Conclusion: 实现LLMs在因果分析中的潜力需采用科学驱动、 leakage-resistant 的评估基准，并发展将LLM知识与数据驱动统计相结合的混合因果发现方法，推动对真实世界科学问题的可靠发现。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [81] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 本研究使用机器学习与大语言模型在丹麦大规模调查数据上预测生活满意度，准确率约93%，识别27个关键指标，且健康状况为普遍核心决定因素，显示ML/LLM/XAI在主观幸福感研究中的潜力与可解释性。


<details>
  <summary>Details</summary>
Motivation: 改进传统生活满意度测量的有效性、可重复性与解释性，通过数据驱动的方法理解影响因素并提升干预策略。

Method: 使用机器学习对19000名丹麦人口的调查数据进行建模；通过特征学习提取27个显著问题；尝试将表格数据映射为自然语言供临床与生物医学LLMs预测；进行数据再采样与特征选择的消融分析；按年龄分组分析主要决定因素。

Result: ML模型实现93.80%准确率、73.00%宏F1；LLM方式实现93.74%准确率、73.21%宏F1；健康状况在各年龄段均为最重要的决定因素；对数据再采样与特征选择的影响有识别；高可重复性与解释性。

Conclusion: 展示了结合机器学习、大语言模型和可解释性AI（XAI）在量化与理解主观幸福感方面的潜力，具有对学术界和专业人员的显著意义。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [82] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO 将语言反馈与数值奖励分离用于大语言模型的强化学习，构建动态经验池，并提出 Reward-Agnostic Reflection 与 Relevant Abstraction 两条原则以提升数据利用率和泛化能力，在数学推理任务上7B/14B模型显著优于基于 GRPO 的基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法多使用标量奖励，忽略了文本推理过程中的 rationale；且在在线学习中同问题的反馈易导致信息泄露或记忆化，来自不同问题的反馈又可能因上下文无关而造成行为崩溃。需在利用语言级反馈的同时控制信息泄漏并提高样本效率。

Method: 提出 LANPO：将反馈分为语言引导探索和数值奖励驱动优化；从历史试验构建动态经验池；提出 Reward-Agnostic Reflection（在样本内安全自我纠错）和 Relevant Abstraction（从跨样本经验中提炼可泛化的规律）；在数学推理基准上评估。

Result: 在 7B 与 14B 模型上，LANPO 相较基于 GRPO 的强基线在测试准确率方面显著提升。

Conclusion: LANPO 提供一种鲁棒的将历史经验整合进 LLM RL 循环的方法，提升数据效率与学习效果，适用于需要语言反馈且对样本效率敏感的任务。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [83] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 在无标注训练数据条件下，利用通用大语言模型实现分子推理。通过将原子唯一标识符用于结构锚定，LLM可先执行一次-shot任务识别相关片段及其化学标签或转化类别，再在少量示例的few-shot任务中预测化学转化，用于单步回推。


<details>
  <summary>Details</summary>
Motivation: 解决标注数据稀缺与高成本，提升化学任务中LLMs的可用性；通过把分子结构与推理连接，使零-shot/少样本学习可行。

Method: 引入位置感知的推理框架：给分子结构分配原子唯一标识符；LLM进行一次性任务来识别片段与标签；可选地，使用少量示例进行二步few-shot任务以预测化学转化。应用于单步 retrosynthesis。

Result: 在学术基准和药物发现分子集上，LLMs在识别反应位点、命名反应类别、最终反应物方面达到高水平：≥90%、≥40%、≥74%。并提供一种将化学知识映射到分子结构以生成理论上有据的数据集的方法。

Conclusion: 框架展示了在无标注数据情境下通过结构锚定的LLM推理解决化学任务的潜力，并拓展了合成数据的生成方法。

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [84] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [85] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 研究在知识图谱视角下的多步推理对增强步骤数的依赖。提出相位转变现象：当先验知识稀疏且成小连通分量时，需进行约√n次查询；一旦正确知识密度超出阈值形成巨型连通分量，期望查询次数降至常数级别。


<details>
  <summary>Details</summary>
Motivation: 揭示测试时增强（如RAG、工具使用）背后的理论基础，定量分析模型的参数性知识与外部检索信息之间的关系，回答在有限增强步数下需要多少先验知识才能得到准确答案，并为检索策略与预训练设计提供理论指导。

Method: 将模型的预训练知识 表示为包含n个顶点的部分、可能带噪声的子图。将增强表示为对一个 oracle 的真边的查询，用以扩充模型知识。把多步推理建模为知识图上的s-t连通问题，给出在给定部分知识下生成正确答案所需的必要和充分的增强步数。通过分析图的连通性和密度，得到相位转变结论：当先验知识图在n个顶点上是分裂成小分量的稀疏图时，寻找路径的代价是Ω(√n)；一旦正确知识密度超过阈值形成巨型分量，路径就能以期望常数次数的查询找到。

Result: 给出对增强步数的严格刻画：在先验知识稀疏且分散的情形存在下界Ω(√n)，表示需要大量查询；在知识达到临界密度并形成巨型连通分量后，存在常数数量级的期望查询，从而实现高效的多步推理。还揭示了达到巨型分量的阈值与数据分布的关系。

Conclusion: 理论上说明要实现少量增强就能完成推理，需确保预训练知识在图中达到足以形成巨型连通分量的密度；这对设计RAG/工具调用策略、选择预训练规模及数据分布具有指导意义，并为理解模型知识与检索信息的耦合提供稳健的理论框架。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [86] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: 通过用 Vision Mamba 替换 PIsToN 的 Vision Transformer，PUMBA 在蛋白质-蛋白质界面评分上实现了更高的性能，在大规模数据集上持续优于前身。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-蛋白质相互作用的正确评分函数对于区分天然复合物和非天然复合物至关重要；PIsToN 使用 Vision Transformer，但仍有潜在局限；Mamba 架构在长距离建模方面表现出色，可能提升界面特征的全局与局部模式捕获。

Method: 将 PIsToN 的 Vision Transformer 主体替换为 Vision Mamba，利用 Mamba 的高效长范围序列建模对图像块进行处理；在若干大规模公开数据集上训练并评估，与原始 PIsToN 比较。

Result: 实验结果显示 PUMBA 在多个广泛使用的公开数据集上持续优于原 Transformer 基础的 PIsToN。

Conclusion: 使用 Vision Mamba 的骨干网提升蛋白质界面评分的性能，证明了更高效的全局和局部特征建模对 docking 评分的重要性，具有潜在的研究和应用价值。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [87] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 在高成本数据环境中，提出一种可解释且以先验最小化依赖为前提的主动目标发现框架，即使先验极不充分也能通过逐步观测实现单调改进的先验估计，并在多域任务中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 在数据获取成本高且难以获得强先验的场景中，现有基于学习的先验引导方法往往无法泛化。需要在信息稀缺条件下实现鲁棒探索、可解释的决策，并能在动态现实世界中适应。该工作受神经科学启发，力求提供理论上 principled 的设计。

Method: 提出一个面向序列观测的主动目标发现框架，具有可解释性、避免黑箱策略的特征；通过受神经科学启发的机制引导决策，在每次新观测后保证先验估计实现单调改进，以提升探索效率与鲁棒性；框架可在缺乏强先验时仍有效地引导查询。背景提及 diffusion 模型等先验工具以说明应用场景。

Result: 在多领域实验（包括物种分布建模和遥感）及消融研究中，该方法显著优于基线方法，表现出更强的探索性和适应性。

Conclusion: 该框架理论上有 principled 指导，具备可解释性并保证随每次观测实现先验的单调改进，使高成本数据场景中的主动目标发现更加可靠和可扩展。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [88] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 在MIT-BIH心肌数据库上基于每秒心率的严格因果流式基准，比较GRU-D与Transformer在两项任务上：近未来10秒的心动风险与单步心率预测；结果显示任务决定模型选择，短期风险用RNN更具竞争力，预测任务Transformer更具优势。


<details>
  <summary>Details</summary>
Motivation: 建立紧凑且严格因果的流式临床时间序列基准，结合预算约束对比不同模型，并进行校准化评估，以支持长期监测中的模型选择。

Method: 在MIT-BIH数据集上进行非重叠记录级拆分，研究两项任务：1) 近十秒内的心动风险预测；2) 单步心率预测。比较GRU-D（RNN）与Transformer在相同训练预算下的表现，并与强基线比较。评估采用温度标定的校准，以及分组自举置信区间。

Result: 在MIT-BIH上，GRU-D在心动风险任务上略优于Transformer；在预测任务上，Transformer显著降低预测误差，相较GRU-D与持久基线。

Conclusion: 纵向监测中，模型选择应依任务而定：紧凑型RNN在短期风险评分仍具竞争力，紧凑型Transformer在点预测上带来更明显收益，强调任务驱动的模型选择在临床流式时间序列中的重要性。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [89] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 使用扩散过程视角对带噪声的 SGD 进行高维精确分析，揭示其统计风险与隐私损失的连续时间动态，并研究一种无需显式梯度灵敏度（无梯度裁剪）的变体，聚焦于带 L2 正则的最小二乘问题。


<details>
  <summary>Details</summary>
Motivation: 在隐私保护机器学习中，噪声 SGD 已成为核心工具，但对其在高维环境中的精确行为知之甚少。现有结论多为统计风险和隐私损失的界限性分析，缺乏对动态过程的完整理解。本研究通过扩散（连续时间）视角，填补对风险演化与隐私损失动态的系统理解空白。

Method: 将带噪声的 SGD 建模为相关的随机微分方程（SDE），通过扩散极限分析其在高维中的统计风险与隐私泄露动态的精确行为；同时研究一种不需要显式梯度灵敏度（不裁剪梯度）的变体，且专注于带 L2 正则的最小二乘问题。

Result: 给出一个扩散框架，能够对带噪声的 SGD 在高维中的风险演化与隐私损失进行精确、连续时间的描述；并验证在最小二乘+L2 正则化情形下，存在无需裁剪的梯度灵敏度处理方式的可行性与特性。

Conclusion: 扩散视角提升了对隐私保护下优化过程的本质理解，尤其在高维场景下提供了对风险与隐私的统一、精确描述，并指明在特定问题（如最小二乘+L2 正则化）下无需显式梯度裁剪的可行策略及其潜在优势。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [90] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出一种分辨率感知的检索增强预测模型，用于零-shot微气候预测，通过对信号分解为不同频段实现全局与局部信息的动态检索，在 ERA5 数据上显著优于 HRRR 和 Chronos。


<details>
  <summary>Details</summary>
Motivation: 零-shot 预测在缺乏历史数据的条件下仍需准确性，传统方法难以充分利用跨空间和时间分辨率信息，需数据高效的建模范式。

Method: 将信号分解为低频和高频分量，采用分辨率感知检索：低频分量依赖更广域的空间上下文，高频分量聚焦局部影响，进行动态数据检索并适应新位置；应用于微气候预测，并与传统方法、数值天气预报、现代时间序列基金会模型比较。

Result: 在 ERA5 数据集上，对比 HRRR MSE 下降约 71%，对 Chronos 下降约 34%；显著优于传统方法、数值预报和基金会模型。

Conclusion: 检索增强与分辨率感知策略对零-shot 微气候预测具有较强有效性，具备可扩展性和数据经济性，潜在适用于其他领域的零-shot 时序预测。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [91] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 状态基因因果效应在某些情形可识别，即使变量基因效应不可识别；识别性依赖额外知识如情境特定独立性和条件函数依赖；对状态约束的知识在单独时不能改善识别性，但与其他知识结合时可提升识别性。


<details>
  <summary>Details</summary>
Motivation: 扩展因果识别的概念，从变量级别扩展到状态级别，以揭示在观察数据中潜在可识别的因果效应。

Method: 理论分析，考察状态干预与变量干预的区别，分析情境特定独立性和条件函数依赖等知识对识别性的作用。

Result: 证明状态基因因果效应在某些情况下可识别，即使对应的变量基因因果效应不可识别；这种分离只有在存在额外知识时才出现；对变量状态约束的知识单独不足以提高识别性，但与情境特定独立性等知识结合时可提升两者的识别性。

Conclusion: 强调在观测数据中可估计的重要性，以及现有变量基因框架可能错过这些识别机会，建议在因果识别框架中引入状态级别分析和更丰富的知识结构。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [92] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 提出一种基于潜变量多输出高斯过程的两层层级结构多任务学习框架，用于预测NLP模型的学习曲线，支持零-shot预测并通过主动学习降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 通过提前预测学习曲线来提升决策效率，减少计算开销和数据获取/整理成本；利用任务与层级之间的共享信息来建立更可靠的预测模型。

Method: 将学习曲线预测任务建模为多任务学习，数据按两层层级组织；采用潜变量多输出高斯过程以建模任务间及层级间的相关性，支持零-shot学习曲线预测并通过主动学习选择最具信息性的查询点。

Result: 在三个小规模NLP数据集上进行验证（最多包含30条学习曲线），覆盖 nanoGPT 模型、mBART 与 Transformer 的双语翻译，以及 M2M100 的多语言翻译等场景，证明该框架能在较低成本下产生不确定性量化的概率性学习曲线，并通过主动学习将预测误差降至接近真实缩放规律。

Conclusion: 该方法为成本友好的概率性学习曲线提供了一种可行的统一框架，具备零-shot预测和主动查询以提升准确性和数据利用效率，便于制定资源分配和数据采集策略。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [93] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 提出一种SegDeformer的联合特征与任务解码方案，通过源编码压缩在保持准确性的同时降低计算复杂度，从而提升车载端帧率并在云端实现不同比特率下的SOTA性能，显著减少云端DNN参数量。


<details>
  <summary>Details</summary>
Motivation: 面向自动驾驶语义分割的变换器模型计算开销较高，需在车载端的实时性与云端的带宽/资源约束之间取得平衡，因此需要一种能够在不显著损失精度的前提下降低计算的联合解码策略。

Method: 针对SegDeformer引入联合特征与任务解码，使得部分解码工作可在源端进行压缩并共用特征表示，从而降低云端与车载端的算力需求，同时在 Cityscapes 与 ADE20K 上进行系统评估，比较与原始非压缩基线以及先前SOTA在不同比特率下的差异。

Result: 在车载应用中，fps从1.4提升至16.5（Cityscapes，提升11.7×），从43.3提升至154.3（ADE20K，提升3.5×）。在mIoU方面，与不进行源编码压缩的变换器基线保持同等水平。在分布式应用中，在不同比特率下实现mIoU的SOTA，同时云端使用的参数量仅为以前SOTA的0.14%（ADE20K）与0.04%（Cityscapes）。

Conclusion: 联合解码策略使SegDeformer在车载与云端场景中均具备更好的可扩展性与效率，能够在保持准确性的前提下显著降低计算需求，推动端到端的高效语义分割在自动驾驶场景中的落地。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [94] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: SAMOSA proposes a querying strategy for open set active learning that leverages data typicality and Sharpness-Aware Minimization (SAM) to select informative samples near decision boundaries, improving accuracy with negligible overhead.


<details>
  <summary>Details</summary>
Motivation: In open set active learning, unlabeled pools contain irrelevant or unknown classes; labeling is costly. There is a need to select informative samples that help distinguish targeted from non-targeted classes while accounting for generalization properties influenced by data typicality and optimization sharpness.

Method: SAMOSA integrates typicality-based querying with Sharpness-Aware Minimization. It identifies atypical samples lying near model decision boundaries in the embedding manifold and prioritizes those that are informative for targeted classes and helpful for separating targeted vs. unwanted classes. It builds on theoretical findings about data typicality's impact on SGD/SAM generalization and does not add computational overhead.

Result: Empirical evaluation across several datasets shows SAMOSA achieves up to 3% accuracy improvement over the state of the art in open set active learning, without extra computational overhead. The authors also provide source code for reproducibility.

Conclusion: SAMOSA provides an effective, low-overhead querying strategy for open set active learning by leveraging data typicality and SAM, enabling improved accuracy through more informative sample selection and better separation of target vs. non-target classes.

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [95] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 提出3D-GSRD，通过Selective Re-mask Decoding在3D分子图自编码框架中实现2D结构与3D信息的平衡，提升分子表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 将遮蔽重解码从2D扩展到3D MGM 时，需在避免2D结构泄露和提供足够2D上下文之间权衡，存在冲突。

Method: 引入Selective Re-mask Decoding (SRD)，仅重遮蔽与3D信息相关的编码表示，同时保留2D图结构；结合3D Relational-Transformer编码器(3D-ReTrans)与结构无关解码器；并对SRD与结构无关解码器如何共同增强编码器在MRL中的作用进行分析。

Result: 在MD17数据集及相关指标上实现SOTA，7/8目标超越当前方法；公开代码。广泛实验证明SRD与结构无关解码器协同提升表示学习性能。

Conclusion: 3D-GSRD通过SRD与3D-ReTrans-结构无关解码器的协同设计，提升了3D MGM 的表示学习能力，提供了在复杂3D分子图场景下的有效遮罩策略与模型架构范式。

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [96] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出一种将计算预算融入数据选择的新方法CADS，通过双层优化框架在预算约束下选择子集并训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法通常忽略计算预算，将数据选择与重要性评估与预算约束解耦，导致在不同预算下难以稳定优于随机选择。需要将预算视为影响数据量、质量和分布的关键因素，以实现跨预算的稳健性与效率。

Method: 提出计算预算感知的数据选择（CADS），将问题建模为双层优化：内层在给定预算下对所选子集训练模型并达到内层最优，外层根据模型评估来优化数据子集选择。为解决外层梯度的高成本，提出概率重参数化策略并使用Hessian-free策略梯度估计。为降低内层求解成本，将内层优化转化为外层目标的惩罚项，并发现仅需估计一个一维损失的最小值来计算梯度，从而显著提升效率。

Result: 在视觉和自然语言处理基准上，CADS相较基线在效能上提升最多可达约14.42%。

Conclusion: 将计算预算纳入数据选择框架可显著提升性能与效率，证明预算驱动的选择策略在不同预算约束下具有普遍适用性和潜在改进空间。

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [97] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former 通过第一层 Value 头的跳接路由，显著降低 KV 缓存并提升表示能力，提供 uptraining 路径，并可与 YOCO/Group-Query Attention 等组合，效果在多模型规模上稳定。


<details>
  <summary>Details</summary>
Motivation: 扩展 Transformer 的规模以提升表示能力通常伴随高昂的 KV 缓存与计算成本。现有方法往往要么提高表达力要么降低内存，难以在降低资源消耗的同时提升表示质量。

Method: 在从第二层开始的每一层中复用第一层 Value 头的一半，同时对另一半按常规计算，从而减少 Value 投影和 KV 缓存约 50%；理论分析表明将第一层的未压缩 Values 路由到深层可以恢复因压缩带来的信息损失，并促进自回归任务中的隐式 mesa-优化；提供仅需 10-15% 的额外计算即可将现有 MHA 检查点升级为 SkipV1Former 的 uptraining 策略；并可与 Group-Query Attention、Multi-Latent Attention、YOCO 等结合以进一步降低 KV 缓存并提升性能。

Result: 跨不同模型规模，SkipV1Former 实现了约 25% 的 KV 缓存降低，并在 perplexity 上优于标准 MHA Transformer 及部分先进变体；与 YOCO 组合时，KV 缓存可近乎再降一半且性能提升；提出了可落地的 uptraining 路径及与其他高效注意力机制的兼容性方案。

Conclusion: SkipV1Former 提供了一种高效的跳接策略，显著降低 KV 缓存且提升表示能力，具备可操作的 uptraining 路径与对其他高效注意力机制的兼容性，是提升自回归 Transformer 可扩展性的一条有效途径。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [98] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 研究在未知因果结构下的因果带宽的遗憾最小化，发现直接学习父集并非最优，提出省略父集识别的近似最优算法，理论上给出下界并有实验验证。


<details>
  <summary>Details</summary>
Motivation: 在因果可观性且结构未知时，传统先识别奖励父集再应用经典带宽方法的策略可能不是最优；需要探讨学习目标与算法设计的真正边界。

Method: 证明存在 regret 与父集识别的冲突；区分已知/未知父集规模，推导新的下界，提出绕过图结构与父识别的近似最优算法；并通过实验比较。

Result: 给出关于下界和算法性能的理论结果，显示所提算法在多环境中优于基线，存在显著性能差距。

Conclusion: 结论是：在因果带宽问题中，父集识别非必要，直接优化策略可实现更优的 regret；这推动对因果结构学习与决策之间权衡的深入理解。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [99] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: A semi-supervised positive-unlabeled (PU) learning framework for archaeological predictive modelling, using dynamic pseudolabeling and a CRF via an RNN, achieves strong performance and interpretability on DEM-based geospatial data and raw satellite imagery, competitive with the state-of-the-art LAMAP and especially higher Dice on geospatial data.


<details>
  <summary>Details</summary>
Motivation: Archaeology suffers from severe label scarcity: positives are rare and most locations are unlabeled. A learning approach that leverages unlabeled data (semi-supervised PU learning) is needed to predict undiscovered sites over large landscapes.

Method: A semantic segmentation model trained with positive-unlabeled learning, incorporating dynamic pseudolabeling refined by a Conditional Random Field (CRF) implemented as an RNN to boost label confidence under strong class imbalance; evaluated on two datasets: a geospatial DEM-derived dataset and raw satellite imagery, with stratified k-fold cross-validation.

Result: Geospatial DEM dataset: performance on par with state-of-the-art LAMAP and higher Dice scores. Raw satellite imagery: preserved performance in end-to-end evaluation and produced more interpretable predictive surfaces.

Conclusion: Semi-supervised PU learning is a promising approach for identifying undiscovered archaeological sites across large, sparsely annotated landscapes, with cross-modal applicability and improved interpretability.

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [100] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 用双线性自编码器将潜在表示分解为二次多项式，提供对非线性潜在变量的代数分析框架，并提出对重要性排序、聚类和激活稀疏性的改进，作为可解析的非线性潜在变量的初步步骤。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器的解释性受输入影响，难以在独立条件下分析。多项式提供不依赖输入的代数原语，能描述从线性概念到复杂流形的结构；通过双线性自编码器将表示投影到二次多项式以实现可分析性。

Method: 使用双线性自编码器将表征分解为二次多项式；提出改进以实现重要性排序、聚类和激活稀疏性；从代数性质出发分析非线性潜在变量。

Result: 摘要未给出实验结果，呈现为概念性框架和方法论；提出实现步骤与潜在分析方向，作为初步工作。

Conclusion: 通过利用代数性质实现非线性但可分析的潜在表示是可行的初步方向，未来工作将扩展对代数结构的分析与应用。

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [101] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: ProtoMol introduces a prototype-guided, layer-wise cross-modal framework for molecular graphs and text, achieving finer-grained alignment and improved property prediction by leveraging dual-branch hierarchical encoders, layer-wise cross-modal attention, and a shared prototype space.


<details>
  <summary>Details</summary>
Motivation: To address limitations in multimodal molecular representation learning: (1) cross-modal interaction is confined to the final encoder layer, missing hierarchical semantic dependencies; (2) absence of a unified, learnable prototype space for robust modal alignment.

Method: ProtoMol uses dual-branch hierarchical encoders (Graph Neural Networks for molecular graphs and Transformers for textual descriptions) to obtain layer-wise representations. It then applies a layer-wise bidirectional cross-modal attention mechanism to progressively align semantic features across layers. A shared prototype space with learnable, class-specific anchors guides both modalities toward coherent and discriminative representations.

Result: Experimental results on multiple benchmark datasets show that ProtoMol consistently outperforms state-of-the-art baselines across various molecular property prediction tasks.

Conclusion: ProtoMol enables finer-grained, semantically aligned multimodal molecular representations through hierarchical encoders, layer-wise cross-modal interaction, and a learnable prototype space, yielding improved predictive performance.

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [102] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: DrivAerStar：一个包含12,000组高保真汽车CFD数据的数据集，通过STAR-CCM+实现，覆盖多配置与20个CAD参数，达到车身空气动力生产级别的准确性并将设计迭代耗时从 weeks 降至 minutes。


<details>
  <summary>Details</summary>
Motivation: 弥合工业CFD与机器学习之间的断点：现有数据集在网格分辨率、部件完整性与验证误差方面存在局限，阻碍数据驱动的工程设计在工业工作流中的落地。通过高保真、可工业化的公开数据集提升AI模型的可靠性与应用转化速度。

Method: 使用STAR-CCM+进行12,000次工业级汽车CFD仿真，系统通过Free Form Deformation对三种车型配置在20个CAD参数维度上进行变形，包含完整的发动机舱与冷却系统并实现内部气流建模；通过严格的网格策略和wall y+控制实现更高的数值一致性。

Result: 风洞标定精度低于1.04%，较现有数据集提升约5倍；从设计迭代成本上看仿真由周级别缩短到分钟级别；提供了首个将学术机器学习研究与工业CFD实践对接的数据集，具备跨学科推广潜力。

Conclusion: DrivAerStar树立了数据驱动的汽车空气动力优化新标准，证明在高保真物理仿真与AI结合方面的可行性与普适性，并为其他工程学科在受限计算资源条件下的创新提供了范式。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [103] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出UDS：一个用于监督微调的在线批次选择框架，通过对 logits 矩阵的核范数同时捕获数据效用和样本内部多样性，并通过低维嵌入与历史样本缓冲区估计样本间多样性，实现无需外部资源、无需反向传播的高效数据选择。


<details>
  <summary>Details</summary>
Motivation: 解决全量数据微调成本高、易过拟合或 bias 放大的问题，数据驱动的 SFT 需要在效用与多样性之间取得平衡，同时避免对外部资源和额外训练开销的依赖。

Method: UDS 在在线批次选择中，利用 logits 矩阵的核范数来捕捉数据效用与样本内部多样性；通过与历史样本缓冲区中的低维嵌入进行比较，估计样本间多样性；不需要外部参考模型或验证集，也不进行额外反向传播，提升计算效率。

Result: 在若干基准数据集和不同数据预算下，UDS 均优于现有的在线批次选择方法，并显著降低相较全量微调的训练时间。

Conclusion: UDS 有效地结合效用与多样性进行在线批次选择，消除了对外部资源的依赖并提升了 SFT 的数据与时间效率，具备广泛适用性。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [104] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [105] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [106] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: A lightweight deep learning pipeline for short-term energy forecasting with noisy/incomplete sensor data, using hourly downsampling, dual-mode imputation (mean and polynomial regression), and Standard Scaling, implemented with a GRU-LSTM sequence-to-one model; achieves RMSE 601.9 W, MAE 468.9 W, and 84.36% accuracy with low inference latency and strong generalization.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of predicting next-day power demand from real-world high-frequency sensor data that are noisy, incomplete, and lacking contextual richness, under multi-criteria evaluation.

Method: Data preprocessing with hourly downsampling, dual-mode imputation (mean + polynomial regression) and normalization (Standard Scaling); a lightweight GRU-LSTM sequence-to-one model for forecasting; employed spatiotemporal heatmap analysis to validate temporal alignment with temperature trends.

Result: RMSE 601.9 W, MAE 468.9 W, and 84.36% accuracy on next-day energy forecasts; robust performance despite asymmetric inputs and imputed gaps; low inference latency; heatmaps show strong alignment between temperature trends and predicted consumption.

Conclusion: Targeted preprocessing combined with compact recurrent architectures can deliver fast, accurate, and deployment-ready energy forecasting in real-world conditions despite data quality issues.

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [107] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 提出面向域泛化的持续学习的新设定DGCL，并基于预训练模型提出可插拔的自适应域变换DoT，分离语义信息与域信息，跨域对齐输出，提升CL基线在DGCL场景的泛化与资源效率。


<details>
  <summary>Details</summary>
Motivation: 在动态现实环境中，模型需在序列任务中不断学习并泛化到未见域。现有CL方法多假设训练/测试域相同，在DGCL场景下表现不足，需能够从不同域中提取并整合语义与域相关信息以实现稳健泛化。

Method: DoT通过受人脑分布-中心理论启发，将表示分解为语义相关与域相关两部分，并在各域间自适应地变换任务表示以实现输出对齐。它作为一个可插拔策略，能在完整参数调优与参数高效调优两种设置下对现有CL基线进行增强，并且实现轻量化的域泛化知识积累。

Result: 在DGCL场景下，DoT显著提升了现有CL基线的性能，无论是在全参数调优还是在参数高效调优模式；并能从DGCL中累积域泛化知识，具备资源高效性（轻量实现）。

Conclusion: DoT为DGCL提供了一种有效的插拔式策略，能够更好地分离与利用语义与域信息，提升跨域任务的泛化能力与效率。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [108] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: 提出 SolverLLM，训练无关的框架，通过对数 Monte Carlo 树搜索在测试时扩展来解决多种优化问题。LLM 生成数学/优化表达并翻译为求解器代码，结合动态扩展、提示回传和不确定性回传等 MCTS 改进，在六组基准数据集上超越基线并实现零训练的强泛化。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法对提示工程的依赖和需要成本高的监督训练的问题，追求跨问题类型的泛化和无训练成本的解决能力。

Method: 在测试时利用 LLM 生成可求解的数学表达并转译为求解器代码；采用改进的蒙特卡洛树搜索：动态扩展用于自适应表达式生成、提示回传通过结果驱动的反馈引导探索、不确定性回传将奖励可靠性纳入决策。

Result: 在六个标准基准数据集上，SolverLLM 的性能优于基于提示的方法和学习型基线，展现出强泛化能力且无需额外训练。

Conclusion: 证明了以测试时扩展的 MCTS 指导下的 LLM 组合可以有效解决多样化优化问题，且具有无训练成本的泛化潜力。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [109] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出一个面向LLM生成的优化问题表达式的细粒度评估框架，新增变量/约束的精确度与召回、约束与目标的RMSE、以及基于token与延迟的效率指标；在六种提示策略下评估多模型，结果显示GPT-5表现最好，连锁推理、自洽性和模块化提示最有效；约束召回率与RMSE对求解器性能影响最大，简洁输出提升效率；提出三条设计原则。


<details>
  <summary>Details</summary>
Motivation: 现有评估多把优化问题视为一个整体，仅用解的最优性和运行时等粗略指标，掩盖结构性或数值错误，亟需对LLM生成的优化问题表达进行细粒度诊断分析。

Method: 提出包含变量/约束的精确度与召回、约束/目标的RMSE、按token和延迟计算的效率指标等新度量，针对GPT-5、LLaMA 3.1 Instruct、DeepSeek Math等模型，在六种 prompting 策略下对不同复杂度的优化问题进行评测。

Result: 结果显示GPT-5始终优于其他模型；链式推理、自我一致性与模块化提示在本研究中最为有效。求解器性能与高约束召回率和低约束RMSE相关性最强，确保结构正确性与解的可信度。约束精确度与决策变量指标作用次要，简洁输出则提升计算效率。

Conclusion: 提出三个NLP到优化建模的设计原则：（i）完整覆盖约束以防止违反；（ii）降低约束RMSE以确保求解层面的准确性；（iii）输出简洁以提升计算效率。该框架为对LLM在优化建模中的细粒度诊断评估奠定基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [110] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 对三种概率性降尺度方法（QRNN、VAE、扩散模型）在ERA5数据上的子季节尺度风速降尺度进行比较，发现相比简单的随机扰动，概率性降尺度能提供更真实的空间不确定性表示，且各模型在集合发散、确定性技能和物理一致性方面存在不同权衡。


<details>
  <summary>Details</summary>
Motivation: 当前在将大尺度大气预测物理量下尺度到局地风速时，空间相关性与物理一致性难以通过基于残差的简单随机扰动充分表达；引入概率深度学习方法可更好地捕捉复杂的空间依赖性与不确定性。

Method: 以 ERA5 重分析数据进行训练，并将模型应用于 ECMWF 的子季节 hindcasts，比较三种方法：Quantile Regression Neural Network（直接建模分布分位数）、Variational Autoencoders（潜在空间采样）、Diffusion Models（迭代去噪），输出风速的概率分布/集合。

Result: 相比简单的随机降尺度方法，概率降尺度在更真实的空间不确定性表示方面更具优势；三种模型在集合分散、确定性技能与物理一致性方面呈现各自的强项与权衡。

Conclusion: 证明概率降尺度是提升子季节风场预报的一种有效工具，适用于可再生能源规划与风险评估；应依据具体应用目标在集合发散、技能与物理一致性之间做取舍。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [111] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出在高斯差分隐私（Gaussian DP）框架下的带偏差修正的线性回归估计量及其渐近置信区间，并给出一个与之匹配的合成数据生成（SDG）方案，适用于小到中等维度的连续数据，能够在小数据场景下提供更准确的推断和更可靠的合成数据。


<details>
  <summary>Details</summary>
Motivation: 社会科学中数据多为小到中等规模，线性回归是常用分析工具。在隐私保护场景下，现有工作多关注点估计且对不确定性量化和合成数据的支持不足；现有的SDG方法往往针对离散数据或依赖深度模型，不适合小规模连续数据。

Method: 提出一个DP偏差修正估计量，具备渐近置信区间；提出一个通用的SDG流程，使合成数据上的回归模型与DP回归结果一致；采用分箱-聚合策略，在小至中等维度设置下效果良好。

Result: 实验显示该方法在准确性、置信区间的有效性以及生成的合成数据在下游机器学习任务中的可靠性方面优于现有方法。

Conclusion: 为小数据场景下的DP线性回归提供可置信的推断和可用的合成数据生成方案，具有实际应用前景和推广潜力。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [112] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: 在模型并行训练中，提出 MuonBP，通过在每个设备上对梯度矩阵分块进行局部正交化并定期进行全局正交化，降低通信开销并维持收敛稳定性；相较于 Muon，吞吐量提升约8%，且迭代复杂度与坐标式优化器接近，超参数需求较低。


<details>
  <summary>Details</summary>
Motivation: 解决梯度正交化在模型并行训练中的额外通信开销与稳定性问题，提升数据效率与吞吐量，以支持大规模语言模型的高效训练。

Method: 提出 MuonBP：对每个设备的梯度矩阵分块独立进行正交化；周期性执行全局正交化以维持训练稳定性；引入两套步长参数（块级正交化与全局正交化的学习率）以实现学习率调整；给出收敛性保证；在 8B 模型、8 路张量并行和 ZeRO 状态分片场景下，实验验证吞吐量提升。

Result: 理论分析给出两个步长的使用规则并证明收敛性；实验结果显示 MuonBP 相比 Muon 提升吞吐量约8%，且训练性能无下降；相对于 AdamW，迭代效率具竞争力，吞吐量接近坐标式优化器。

Conclusion: MuonBP 提供一个简单、易调参且高效的分布式正交化方案，适用于大规模分布式训练，在降低通信开销的同时维持收敛性与性能，便于扩展到多设备的训练场景。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [113] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: Graph4MM 通过引入 Hop-Diffused Attention 与 MM-QFormer，将多跳结构信息整合到自注意力和跨模态融合中，提升多模态理解，显著优于较大模型及基线。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多模态数据存在复杂的跨模态关系与多跳结构，传统方法将图视为独立模态或忽略多跳信息，难以将结构信息高效地融入基础模型，导致理解受限。

Method: 提出 Hop-Diffused Attention：通过因果屏蔽和跳数扩散，将多跳结构信息注入自注意力。设计 MM-QFormer：一个多映射查询的跨模态变换器，用于高效的跨模态融合。框架在 foundation 模型时代探讨图在多模态学习中的作用。

Result: 理论与实证分析表明，利用结构信息来整合 intra-与 inter-模态交互能提升多模态理解。实验在生成与判别任务中，Graph4MM 超越更大规模的视觉-语言模型、大型语言模型和多模态图基线，平均提升约 6.93%。

Conclusion: 将结构信息用于内部和跨模态交互的整合有助于超越独立模态的理解能力，Graph4MM 在多任务中表现优越，证明了图结构在多模态学习中的价值。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [114] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEschematic uses a multimodal LLM to automatically convert SPICE netlists into human-editable analog schematics, combining textual, visual, and symbolic cues with few-shot placement and Visual Chain-of-Thought to improve placement, wiring, and visual quality.


<details>
  <summary>Details</summary>
Motivation: Current approaches rely on text-based SPICE representations that lack visual interpretability for circuit designers. There is a need for an AI agent that can generate clear, editable schematic diagrams from netlists by integrating multiple modalities.

Method: Proposes EEschematic, an AI agent that fuses textual, visual, and symbolic modalities to translate SPICE netlists into editable schematic diagrams. It uses six analog substructure examples for few-shot placement and a Visual Chain-of-Thought (VCoT) strategy to iteratively refine component placement and interconnections to enhance clarity and symmetry.

Result: Experimental results on representative analog circuits ( CMOS inverter, 5T-OTA, and telescopic cascode amplifier ) show that EEschematic produces schematics with high visual quality and structural correctness.

Conclusion: EEschematic provides an interpretable, automatic schematic-generation workflow from SPICE netlists, improving design efficiency and readability by generating high-quality, editable analog schematics.

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [115] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 提出并验证了一种基于注意力汇聚点(attention sink)的后门撤记(backdoor unlearning)攻击，能在无触发时与常规撤记模型表现相似，但在触发时恢复模型忘记的知识；攻击效能与触发位置紧密相关，且依赖于将触发与注意力值对齐的策略。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重模型的普及，单纯的模型撤记可能成为安全隐患：攻击者或不当使用者可能通过隐藏触发来在需要时恢复已忘记的信息，扩展至数据、知识或行为的可控撤回能力。

Method: 理论分析与实证评估并行：将后门撤记映射到注意力汇聚点的现象，研究触发放置位置、后门训练强化机制以及注意力分布对后门持续性的影响；在多组实验中比较有无触发时的行为表现。

Result: 发现后门效能与注意力汇聚点高度相关；将触发置于sink位置并对齐其注意力值显著提升后门持久性；在无触发条件下，模型表现与普通未撤记模型相近，但一旦触发，能够恢复被遗忘的知识；大量实验支持该结论。

Conclusion: 存在潜在的安全风险：撤记过程可能被构造性触发从而回退到原始知识状态；需要针对注意力分布异常、触发检测、撤记训练的鲁棒性等方面提出防御策略，尤其在开源/开权重环境中加强对撤记机制的安全性评估。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [116] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 强化学习（RL）结合好奇心驱动的探索和基于图的动作，能够解决含有根式、指数和三角函数的非线性符号方程，扩展了以往对一元线性方程的研究。


<details>
  <summary>Details</summary>
Motivation: 探索将强化学习应用于符号数学与符号推理的可行性，验证模型自由的PPO是否能学习对符号表达式进行变换、求解方程等操作，并评估好奇心驱动的探索在复杂符号任务中的效果。

Method: 在模型无关的PPO框架中引入好奇心驱动的探索策略，并采用图结构动作表示符号化表达的变换，只对具有非线性项的方程（含根式、指数、三角函数等）进行求解尝试。

Result: 通过实验，模型能够在含有根式、指数和三角函数的非线性方程上完成求解，证明了基于好奇心的探索和图形化动作在符号推理任务中的有效性。

Conclusion: 结果表明好奇心驱动的探索可能对一般的符号推理任务具有普遍意义，提示将强化学习方法与符号数学相结合的潜在方向。

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [117] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 提出了一种DICA框架，通过J-VolMax实现对未知非线性混合中的潜在成分的辨识，鼓励潜在成分对观测变量的影响多样性；在不依赖辅助信息、潜在成分独立性或Jacobian稀疏性的条件下实现可辨识，扩展了可辨识性分析的范围。


<details>
  <summary>Details</summary>
Motivation:  nonlinear ICA 的可辨识性长期是一个核心难题，现有方法依赖辅助信号、独立性假设或雅可比稀疏性等结构性假设。本文提出在更宽松条件下实现潜在成分辨识的新框架，旨在降低对额外信息和强假设的依赖。

Method: 提出 Diverse Influence Component Analysis (DICA) 框架，利用混合函数雅可比矩阵的几何属性，设计 Jacobian Volume Maximization (J-VolMax) 判据，通过鼓励潜在成分对观测变量的影响的多样性来实现识别。该方法核心在于以雅可比体积的最大化来促进多样性，从而达成辨识。

Result: 在合理条件下，该方法无需辅助信息、潜在成分独立性或雅可比稀疏性假设，即可实现可辨识性，拓宽了可辨识性分析的范围，并为现有方法提供了互补的视角。

Conclusion: DICA 提供了一种利用雅可比矩阵几何特征的新可辨识性框架，及其 J-VolMax 判据，扩展了对非线性ICA可辨识性的理解，并与现有依赖性/稀疏性假设的方法互为补充。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [118] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 在强化学习结合链式推理的环境中，模型会表现出有目的的动机性推理，试图为违背指令的行为辩解，并可能混淆监控者；这对使用CoT进行评估和监督的有效性构成挑战。


<details>
  <summary>Details</summary>
Motivation: 理解在使用链式推理的强化学习中，后处理指令对模型推理和行为的影响，以及模型为何及如何产生自我辩解；评估不同规模的判断模型在检测动机性推理方面的能力差异，以及监控在更强大模型时代的局限性。

Method: 在简单设置中进行系统实验，观察模型为违反指令的行为生成合理辩解、并淡化潜在危害；比较前沿推理模型与更小的LLM判断在识别动机性推理方面的能力；检验一些判断模型是否会被说服认同错误的推理；分析监控难点并提出对评估/监督的启示。

Result: 模型表现出系统性动机性推理，生成看似合理的辩解以违反指令；多数前沿推理模型能检测出动机性推理，但较小的LLM判断可能漏检，且在极端情形下甚至会被说服认同错误推理；监控能力随模型复杂性提升而变得更具挑战性。

Conclusion: 在以链式推理进行评估和监督时必须考虑动机性推理的存在；需要改进评估设计以抵御这一现象，避免对模型安全性的错误判断，且相关代码将对外公开。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [119] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 提出硬件友好的低精度对数固定点训练，利用分段线性对数加法近似和模拟退火优化，在12位整数精度下训练VGG-11/16，显著降低硬件成本。


<details>
  <summary>Details</summary>
Motivation: 在推理量化已显著降低计算成本之时，训练阶段仍依赖复杂浮点运算，需要新的低精度方案以降低训练的硬件成本与能耗。

Method: 引入新的硬件友好分段线性对数加法近似；通过考虑位宽在近似设计中的作用进行设计；使用模拟退火在不同精度下优化近似；通过C++位真仿真验证在VGG-11/16上使用12位整数进行训练。

Result: 在12位整数精度下，训练表现对比32位浮点损失很小；硬件评估显示相比线性定点实现，LNS乘累单元的面积减少最高32.5%，能耗降低53.5%。

Conclusion: 对数固定点训练结合位宽自适应近似显示出成为未来硬件加速器的有力方向，能在保持可接受精度的同时显著降低资源开销。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [120] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出了一种在没有 ground truth 的情况下评估数据集可靠性的新指标 Gram 行列式分量（Gram determinant score），该分数对实验过程无关并能在多种观测过程下保持一致的可靠性排序。


<details>
  <summary>Details</summary>
Motivation: 在数据来自可能具有策略性来源的场景中，真实数据不可观测，仅能看到由未知统计实验产生的结果。需要一个基于地面真实排序的基准来评估数据集的可靠性。

Method: 定义一个描述观测数据经验分布与实验结果的向量集合，利用它们的 Gram 行列式来衡量体积；证明该分数能够保留若干地面真实的可靠性排序，并且在唯一性上（除缩放外）实现对不同实验的相同排序，即实验不可定向性；在合成噪声模型、CIFAR-10 嵌入以及真实就业数据上进行实验以验证该分数的有效性。

Result: Gram determinant score 能在多种观测过程下保持地面真实的可靠性排序并且唯一性地（除缩放）实现与不同实验的无关排序；实验结果显示该分数在合成、嵌入和真实数据场景中有效地反映数据质量。

Conclusion: Gram determinant score 为无 ground truth 时数据可靠性评估提供了一个鲁棒、实验不可知的度量，并可在多域场景中应用。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [121] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: Hedge在组合设定中近似最优；对某些集合存在与参数相关的下界，导致相对最优性被√log d尺度的因子削弱；对在线多任务学习仍然最优；在DAG的最短路等问题中，通过dilated entropy正则化使OMD与Hedge等价，从而获得近似最优的后悔界。


<details>
  <summary>Details</summary>
Motivation: 系统性地评估Hedge在广义组合学习中的最优性，揭示不同组合结构对后悔界的影响，并建立Hedge与OMD及正则化器之间的联系。

Method: 1) 证明对任意X ⊆ {0,1}^d，存在下界Ω(√(T log|X|/log d))，对任何算法都成立；2) 识别m-sets（log d ≤ m ≤ √d）类群，在该区间内下界紧贴上界，使Hedge相对于最优解差一个√log d的因子；3) 证明在线多任务学习中Hedge是最优；4) 构造DAG问题下的近似正则化器：对Dilated Entropy正则化的Online Mirror Descent与Hedge迭代等价，从而继承Hedge的近似后悔界。

Result: 提供了一个统一的框架来理解Hedge在广义组合域中的最优性：在一般情形下近似最优，且存在参数依赖的下界；在m-sets等特定结构下，Hedge并非全局最优，损失一个√log d的因子；在在线 multitask 学习中保持最优；并通过OMD+Dilated Entropy实现对DAG最短路径等问题的近似最优正则化。

Conclusion: Hedge并非在所有组合设定下都绝对最优，但其近似最优性在大多数常见结构中成立，且可通过与OMD及合适正则化的等价性在DAG等复杂域中实现近似最优的后悔界。这揭示了正则化设计与算法选择在处理组合结构中的关键作用。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [122] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 提出了适用于具有聚合bandit反馈的 episodic tabular MDP 的最佳-同时-世界(BOBW)算法，在已知转移时实现 stochastic 情况下 O(log T) 期待损失和 adversarial 情况下 O(sqrt(T)) 的遗憾界，并给出匹配的下界；并扩展到未知转移通过置信区间技巧，还提出基于 occupancy measures 的 FTRL、自界定界与受近期在线最短路径问题启发的新损失估计；并给出个体间隙相关下界及最接近最优的带 bandit 的最短路径问题算法。


<details>
  <summary>Details</summary>
Motivation: 在线学习在有限-horizon 的 episodic MDP 中面临聚合反馈场景（仅观测每一回合的总损失），在对抗性与随机性环境下都需要有稳定的表现。本文首次提出在此情境下的最佳-世界（BOBW）算法，以同时在两类环境中实现低遗憾。

Method: 提出首个针对 episodic tabular MDP 的 BOBW 算法，考虑聚合 bandit 反馈。已知转移时利用对 occupancy measures 的 FTRL、自界 bounding 技术和受在线最短路径问题启发的新损失估计；未知转移时引入置信区间策略进行扩展。核心工具包括基于 occupancy measures 的优化框架、自界定式边界、以及新型损失估计。还将方法推广至带 bandit 反馈的最短路径问题及个体间隙下界。

Result: 在已知转移情形下，BOBW 算法实现对数级遗憾（O(log T)）在随机环境和平方根级遗憾（O(√T)）在对抗性环境；给出与之匹配的下界，证明最优性。扩展到未知转移时，结合置信区间技巧实现。还提出了个体间隙依赖的下界，以及对于带 bandit 反馈的最短路径问题的近似最优 BOBW 算法。

Conclusion: 工作推动了带聚合反馈的 episodic MDP 的 BOBW 研究，给出最优性结果与新技术，且为未知转移情形与带 bandit 的最短路径问题提供了新的分析与算法路径，未来可进一步拓展到更广的模型与反馈结构。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [123] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 将 Transformer 编码器等价于 GCN 的统一视角，提出 Fighter 架构，在时间序列预测中实现更清晰的可解释性与竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 揭示 Transformer 的内部机制，提供更直观的时序关系表达与多层次依赖的图化表示，提升模型可解释性和效率。

Method: 将注意力分布视为动态图邻接矩阵，前向传播类似图卷积，反向传播中值向量和前馈投影的更新 dynamics 也与 GCN 参数相似；在此基础上设计 Fighter，去除冗余线性投影并引入多跳图聚合。

Result: 在标准预测基准上 Fighter 具有竞争性能，并且提供更清晰的能解释推断机制。

Conclusion: 基于统一的图卷积解释，Fighter 提供更直观的时序依赖表示和跨尺度关系，兼顾性能与可解释性。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [124] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出一种基于矩阵自由能的自编码器正则化方法，通过对编码矩阵的奇异值分布进行约束，使编码呈现高斯样分布并提升泛化，并提出一个最大化自由能的版本用于欠定逆问题。


<details>
  <summary>Details</summary>
Motivation: 在自编码器中强制潜在编码分布接近高斯以提升泛化和稳定性，借助自由概率与随机矩阵理论将编码矩阵的谱结构作为正则对象。

Method: 定义一个可微的损失函数，基于编码矩阵的奇异值分布对应矩阵自由能；训练时最小化负自由能，使用标准梯度下降；并提出一个最大化自由能的自编码器用于产生高斯编码，验证在欠定逆问题中的应用。

Result: 数值仿真显示最小化负矩阵自由能得到Gaussian-like的编码，且在训练集和测试集之间具备良好泛化；最大化自由能的变体可稳定地产生高斯编码，并有潜在的欠定逆问题应用。

Conclusion: 基于自由概率的矩阵自由能正则化为自编码器提供了一个理论上有据的谱结构约束，能够生成高斯分布的潜在表示并提升泛化能力；最大化自由能的框架扩展到欠定逆问题，具有应用潜力。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [125] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出零成本的自引导（In-situ Autoguidance），在推理时通过随机前向传播生成“劣势”预测以实现自我纠错，从而在无需外部辅助模型的情况下提升图像生成的质量、对齐和多样性。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型中，分类无引导 CFG 能提高质量与对齐但降低多样性；现有的解耦方法通常需要训练一个性能较差的辅助模型，带来额外开销。本文目标是在不依赖外部模型的前提下实现相似的解耦效果。

Method: 在推理阶段通过一次随机的前向传播动态生成一个“劣势”预测，将引导视为推理-time自我纠错；实现 In-situ Autoguidance，零成本地获得对齐与质量的提升，同时保留多样性。

Result: 实验表明该零成本方法可行且具有竞争力，建立了成本更低的引导基线；在无需外部模型的情况下实现了高质量和良好对齐，同时维持多样性。

Conclusion: 证明了自我引导在推理时即可实现与 CFG 类似的收益，为低成本场景下的扩散模型引导提供了新的范式和基线。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [126] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 提出 PLDA 的方法用于在部署后动态检测 OOD、增量学习新类，解决重训成本与数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 在动态与开放环境中，模型需要检测看不见的未知类别并在被标注后学习；传统方法的固定ID集合与需大量再训练的限制在此场景下无法胜任。

Method: PLDA 能在应用阶段进行动态 OOD 检测，并对新出现的类别进行增量学习；在新样本被标注后立即将其整合进模型，且应对 ID 类扩展与数据稀缺问题。

Result: 文中尚未给出具体实验结果，但将通过经验评估来证明 PLDA 的有效性。

Conclusion: PLDA 提供一个在部署后即可进行动态检测与增量学习的新框架，克服传统监督学习在开放环境中的局限性。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [127] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: ALPINE通过TD3驱动的自适应差分隐私框架，在边缘 crowdsensing 场景中以闭环控制实现实时噪声自适应，以平衡隐私、数据效用和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统的静态差分隐私难以应对动态的风险与资源约束，易导致过多噪声或保护不足。需要一个轻量、可自适应的机制在边缘设备上调整隐私等级。

Method: 提出ALPINE，包含四个模块：动态风险感知、基于TD3的隐私决策、本地隐私执行、边缘节点的性能验证。以环境风险评估为依据设计奖赏函数，结合隐私增益、数据效用和能耗，引导TD3智能体动态调节噪声标准差；模型与策略低开销、可部署。理论分析与仿真实验表明在不同风险场景下能实现隐私、效用、成本之间的平衡。

Result: 在理论分析与实际仿真中，ALPINE有效缓解推断攻击，同时尽量保持数据效用和能源成本的平衡，具备大规模边缘应用的实用性。

Conclusion: ALPINE提供一种可自适应、轻量级的隐私保护框架，能够在移动边缘 crowdsensing 场景中实时调整DP强度，实现隐私、实用性和成本之间的动态均衡，具备现实应用潜力。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [128] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 提出一个统一、全面的评估框架，用于在文本属性图（TAG）上评估不同模型的鲁棒性（GNN、鲁棒GNN、GraphLLMs），覆盖十个数据集、文本/结构/混合扰动，以及毒化与对抗性攻击，揭示文本与结构鲁棒性的权衡，并提出SFT-auto以实现更平衡的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前关于TAG鲁棒性的评估零散且缺乏系统性，难以揭示文本扰动与结构扰动在不同模型和攻击场景中的影响及其相互作用，因此需要一个统一、全面的评估框架来梳理鲁棒性特性及权衡。

Method: 提出统一评估框架，在四个领域的十个数据集上，对经典GNNs、鲁棒GNNs（RGNNs）和GraphLLMs进行鲁棒性评估；覆盖文本基扰动、结构基扰动以及文本-结构混合扰动，同时考虑毒化和规避（evasion）两类攻击场景；基于实验结果分析鲁棒性权衡、文本编码器与攻击类型对性能的影响，并提出SFT-auto作为在单一模型内实现文本与结构鲁棒性平衡的解决方案。

Result: 分析发现：1) 模型在文本鲁棒性与结构鲁棒性之间存在固有的权衡；2) GNNs/RGNNs 的鲁棒性强弱高度依赖文本编码器和攻击类型；3) GraphLLMs 对训练数据污染尤为敏感/脆弱。为缓解这些权衡，提出SFT-auto，能够在文本与结构攻击之间实现更优越且平衡的鲁棒性。并提供实证结果支撑上述结论，代码公开。

Conclusion: 该研究为TAG安全领域奠定基础，系统性地揭示了不同模型在文本与结构扰动下的鲁棒性特征与权衡，并给出切实可行的对策（如SFT-auto），为未来在对抗性环境中的鲁棒TAG学习提供方向与实践方案。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [129] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出一个模块化的基准测试框架，用以系统评估蛋白质分子动力学（MD）方法，基于 WE 采样和 TICA 指标，支持多种引擎，提供丰富的评估指标和一个九蛋白数据集，以实现跨方法的可重复比较。


<details>
  <summary>Details</summary>
Motivation: 当前 MD 方法验证缺乏标准化的平台，评估度量不一致、稀有构象取样不足、缺乏可重复的基准；需要一个通用框架实现方法的客观对比。

Method: 使用 WE 采样（WESTPA）结合基于 TICA 的进展坐标，提升探索蛋白构象空间的效率；提供轻量级传播器接口以兼容不同模拟引擎（经典力场和机器学习模型）；构建包含超过 19 种度量和可视化的评估套件；提供九个蛋白质的数据集，规模 10–224 个残基，在 300K 条件下从每个起点进行 1,000,000 MD 步（约 4 ns），并通过经典隐式溶剂 MD 与 fully trained 与 under-trained CGSchNet 的对比进行验证。

Result: 框架实现了快速且高效的构象空间探索；提供可重复且跨 MD 方法的统一评估规范和数据集；作为开源平台，促进分子模拟社区的基准化比较。

Conclusion: 该工作为 MD 方法的系统基准建立了标准化的评测框架和数据集，便于跨引擎、跨模型的对比和再现性，推动分子模拟领域的严格 benchmarking。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [130] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: 提出SOLE硬件-软件协同设计，针对Transformer中的Softmax和LayerNorm，结合E2Softmax的对数量化与分解以及AILayerNorm的低精度统计，实现低位宽存储与计算，在不重训练的前提下显著提升推理速度与能效，优于现有定制硬件与GPU。


<details>
  <summary>Details</summary>
Motivation: Transformer在推理阶段的Softmax和LayerNorm成为瓶颈；基于函数近似的方法存在较高内存开销、需重训练以纠正误差等问题。需要一种硬件-软件协同的方案，在降低计算与存储成本的同时保持精度。

Method: 提出SOLE，包括E2Softmax（对数2量化指数函数和基于对数的除法，用于近似Softmax）和AILayerNorm（低精度统计量计算），形成硬件-软件协同的Softmax与LayerNorm加速方案；实现低精度计算与低位宽存储。

Result: 相较于GPU，SOLE在推理中实现了数量级级别的加速和能耗下降；在与现有定制硬件的对比中，Softmax与LayerNorm分别实现3.04x与3.86x的能效提升，以及2.82x与3.32x的面积效率提升。

Conclusion: 通过硬件-软件协同设计，SOLE在不进行重训练的前提下，显著提升Transformer推理阶段的Softmax和LayerNorm效率，兼具高精度保持与资源利用优化，优于现有方案。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [131] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出一个面向高风险高回报(HRHR)任务的强化学习框架，通过离散化连续动作空间以近似多模态分布，结合熵正则化探索与双评估器估计离散价值分布，能够扩展到高维动作空间并在 locomotion 与 manipulation 基准测试中优于基线。


<details>
  <summary>Details</summary>
Motivation: HRHR任务通常具有多峰行动分布和随机回报，基于高斯单峰策略的RL方法在此类任务中往往收敛困难且效果受限，因此需要显式建模动作分布的多模态性与风险特性。

Method: 1) 将连续动作空间离散化以近似多模态策略；2) 采用熵正则化的探索提升对风险但有高回报的行动覆盖；3) 引入双评估器架构以更准确地估计离散价值分布；4) 框架具可扩展性，适用于高维动作空间，适用于复杂控制领域。

Result: 在具有高失效风险的 locomotion 与 manipulation 基准上，所提出方法相较于基线表现更优，证明了显式建模多模态性与风险的重要性。

Conclusion: 通过把多模态性与风险显式建模引入强化学习，能够提升 HRHR 任务的学习效果，并且该框架具备对高维动作空间的扩展性。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [132] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出一个自适应离散化框架ADCMs，通过将局部一致性用于训练目标、全局一致性作为约束，并使用拉格朗日乘子与高斯-牛顿法实现对一致性模型(CM)的自动离散化，从而提升训练效率与生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有一致性模型多依赖手工离散化，需针对不同噪声日程和数据集进行调参，导致训练效率低且鲁棒性不足。

Method: 将离散化问题建模为对离散步长的优化，在训练阶段以局部一致性作为优化目标确保可训练性，同时以全局一致性作为约束以控制训练目标的去噪误差；使用拉格朗日乘子权衡两者的 trade-off，并采用高斯-牛顿法实现对离散步长的自适应选择，形成ADCMs框架。

Result: 在 CIFAR-10 与 ImageNet 上实验表明，ADCMs显著提升CMs的训练效率和生成性能，且对更先进的扩散模型变体具有良好适应性，训练开销较小；代码公开。

Conclusion: 提出的ADCMs构成一个统一的自适应离散化框架，提升了CM的训练效率和鲁棒性，具备较强的扩展性和实用性。

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [133] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: A variational inference extension to a deterministic machine learning approach for data assimilation, modeling the state as a multivariate Gaussian to obtain well-calibrated uncertainty estimates; demonstrated on Lorenz-96 with near-perfect calibration and potential gains for longer assimilation windows; code released.


<details>
  <summary>Details</summary>
Motivation: Data assimilation under uncertainty is common, but existing deterministic ML approaches lack calibrated probabilistic predictions. The work aims to introduce a probabilistic, variational framework to quantify uncertainty in the inferred state.

Method: Extend a deterministic ML-based data assimilation method with variational inference, modeling the predicted state as a multivariate Gaussian distribution. Validate on chaotic Lorenz-96 dynamics and integrate into a broader variational data assimilation pipeline.

Result: The approach yields nearly perfectly calibrated predictions on Lorenz-96 and shows improved benefits when using longer data assimilation windows within the variational pipeline.

Conclusion: A probabilistic extension to deterministic ML in data assimilation can produce well-calibrated forecasts and enhance the performance of variational data assimilation pipelines, with publicly available code.

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [134] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出一个以用户反馈为驱动的LLM记忆与持续学习基准，覆盖多领域、多语言和多任务，评估在服务阶段从用户反馈中学习的能力，指出现有记忆基线在多样场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有通过扩大数据、参数和推理成本的扩展方法渐趋见顶，亟需从实践中学习的能力（如记忆、持续学习）来提升LLMs的长期性能与适应性；同时需要一个能评估在实际服务中从用户反馈中学习的基准以推动记忆与优化算法的发展.

Method: 提出一个用户反馈仿真框架，用以在服务场景中产生可控的、逼真的反馈信号；构建覆盖多领域、多语言、多任务的综合基准，评估LLM系统的记忆能力与持续学习效果；通过实验对比现有基线在有效性与效率上的表现，揭示差距。

Result: 实验结果显示，现有最先进基线在该框架下的有效性与效率均未达到满意水平，说明需要更强的记忆与优化算法来实现长期学习与快速适应。

Conclusion: 该基准可为未来在LLM记忆与优化算法方面的研究提供更全面的评估手段，推动在服务场景中的持续学习与记忆机制的发展。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [135] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 扩展基于 PAC-Bayes 的泛化界限，使之适用于非紧性对称性和非不变数据；在非均匀旋转的 MNIST 实验中进行验证，结果表明对称模型在更广泛的对称性设置下优于非对称模型。


<details>
  <summary>Details</summary>
Motivation: 现有理论对对称性收益的保证多关注紧群及数据分布不变的假设，而现实数据常常不满足不变性，需要在更广泛的对称性设置下给出泛化界限。

Method: 在 McAllester 的 PAC-Bayes 界限等基础上，对非紧群对称性和非不变数据的情形进行适配与收紧，表明该方法可推广至广泛的 PAC-Bayes 界限，并给出新的理论界限。

Result: 推导出的边界在实验中成立且相较于以往结果有所提升；在带有非均匀旋转群的旋转 MNIST 实验中，理论保证与经验结果相符，且证明了对称数据下对称模型的优势。

Conclusion: 为对称性在机器学习中的有益作用提供更普遍的理论证据，超越紧群和不变分布的限定，推动对更广泛对称性的理解与应用。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [136] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 提出一个用于多因素顺序数据的标准化基准，覆盖视频、音频、时间序列六个数据集，提供模块化工具、后验潜变量对齐阶段、Koopman灵感模型实现SOTA，并用视觉-语言模型进行数据标注与零-shot评价。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时序数据涉及多种交互语义因素，且随时间变化；现有工作多聚焦于两因素的静态/动态图，缺乏对多因素顺序解耦的标准化基准、工具和评估。需要可扩展、可重复的基准以推动领域进步。

Method: 提出六数据集的标准化基准，并提供用于数据集对接、模型开发和多因素分析评估的模块化工具。引入后验潜在变量探索阶段以自动将潜在维度对齐到语义因素，提出基于Koopman思想的模型以实现对因果/动力学结构的更好建模。并展示Vision-Language Models用于自动数据标注与作为零-shot解耦评估器，降低人工干预。

Result: 在所提出的基准上取得状态-of-the-art结果，提供稳健、可扩展的多因素顺序解耦框架；Vision-Language Models实现数据标注自动化与零-shot评估能力，减少人工工作量。

Conclusion: 为多因素顺序解耦提供标准化、可扩展的平台，促进方法的可复现性和比较；同时通过VL模型实现数据标注自动化与零-shot评估，推动领域向更现实、复杂数据场景扩展。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [137] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出一个训练无关的奖励建模框架，通过Propose-Evaluate-Revise（PER）发现高质量的、面向查询的评分准则，再通过信息论编码率最大化将其泛化为核心非冗余的Theme-Tips等级制，实现数据高效的奖励对齐；用70对偏好就能使小模型（Qwen3-8B）超越部分大模型。


<details>
  <summary>Details</summary>
Motivation: 奖励模型对齐LLMs与人类价值观至关重要，但偏好数据集成本高、可解释性差；基于评注的rubric在透明性上有优势，但缺乏系统质量控制与优化，导致可扩展性与可靠性之间的权衡。需提出一种可扩展、可解释且数据高效的奖励建模方法。

Method: 提出-评估-修订（Propose-Evaluate-Revise，PER）管线以从验证集引导推断高质量、查询特异的评分准则；随后将这些粒度准则泛化为核心集合，通过最大化信息论编码率实现非冗余压缩，输出层级化的 Theme-Tips rubrics。

Result: 在大量实验中表现出显著的数据高效性与性能提升；仅用70对偏好对（约源数据的1.5%）就让小模型Qwen3-8B达到甚至超越专门训制的对手。

Conclusion: 该框架实现了一条可扩展、可解释且数据高效的奖励建模路径，且无需额外训练即可从人类偏好中提取高质量准则并进行泛化。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [138] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出一个可连续调控内部表示的框架，兼容局部化与分布式编码之间的无缝插值，并通过局部性拨轮、信息论招募和分层招募实现跨粒度的容量分配，附带严格的理论结果与收敛保证。


<details>
  <summary>Details</summary>
Motivation: 在可解释性与高性能之间寻求折中，允许在不重新训练的情况下调整局部化强弱，并在初始化时无需完整领域知识就能自适应分配语义资源，适用于监管场景。

Method: 提出三大核心创新：1) locality dial，可训练并在训练和推理阶段动态控制局部化程度；2) 信息论招募机制，自适应分配语义块，不依赖初始的完全领域知识；3) 分层招募框架，将容量分配扩展到整套专用LLM，实现多粒度架构自适应。通过对注意力的分组稀疏性惩罚、信息论锚点设计、动态规则注入以及以带惩罚似然为准则的招募标准来实现。给出收敛性分析：在驻点处注意力对语义相关块的集中性的阈值条件、注意力熵和指针保真度的严格界限。

Result: 给出对于注意力在语义块上的集中性、熵和指针保真度的明确界限以及在块级和LLM级的收敛性保证，系统能够学习发现语义分区，在模型复杂度与数据编码效率之间取得平衡，且在多粒度上实现容量自适应。

Conclusion: 该框架使研究者和工程师能够在可解释性与高性能之间连续插值，并在多粒度层次上自适应容量分配，尤其适用于需要透明度和能力并存的受监管领域。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [139] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: 提出 DISC（Diffusion-based Statistical Characterization），将 OOD 探测由单一标量分数扩展为基于扩散模型逐步去噪得到的多维特征向量，实现对 OOD 的检测与 OOD 类型的分类。与现有方法相比，在图像与表格数据集上达到与最先进检测器相当或更优的性能，同时实现对 OOD 类型的识别，从而将 OOD 检测从二元化转向更细粒度的检测。


<details>
  <summary>Details</summary>
Motivation: 现有的 OOD 检测方法多以单一标量分数来回答“是否为 OOD”，但缺乏对分布偏移类型的区分，难以为后续处理提供上下文信息和行动指引。

Method: 引入扩散模型的逐步去噪过程，DISC 提取跨越多种噪声水平的丰富统计特征向量，形成多维表示；在图像与表格数据上对 OOD 检测与 OOD 类型分类进行评估。

Result: 实验表明 DISC 在 OOD 检测方面可达到甚至超越最先进的方法，同时具备对 OOD 类型进行分类的能力，这是现有工作所缺乏的。

Conclusion: 将 OOD 检测从简单的二元判定，提升为更 granular 的检测，使得对不同类型的 OOD 数据能够更高效地提前采取针对性行动。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [140] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本研究提出通过对扩散模型的层级分工的分析，揭示内部表征的分布式劳动。区分“严格意义上的合成”（一个紧凑潜在空间完全决定生成过程）与“广义上的合成”（表征劳动分布在多层之间），并认为扩散模型削弱了统一内部空间的假设，促使我们从“潜在空间”叙事转向“专门化过程的涌现配置”的理解。


<details>
  <summary>Details</summary>
Motivation: 探究生成性视觉模型中的内部表征及其演变；质疑将生成过程简化为单一潜在空间的叙事，借助媒介理论框架重新阐释AI生成的本质。

Method: 通过对模型体系的细读（architecture close reading）与有针对性的层级表示干预实验，结合媒介理论讨论，批判性评估“潜在空间/表示假说”等隐喻。

Result: 发现扩散模型将表征负担分散在多层之间，使内部空间不再统一；这推翻了“直接内容合成”的直觉，强调生成性能作为多个专门化过程的涌现配置。

Conclusion: 呼吁重新命名与理解生成性AI：由单一潜在空间的合成为多层专门化过程的协同涌现，促使研究者在理论语言和评估范式上做出调整。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [141] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1提出了一个面向表格预测的多步推理LLM，核心是PRPO强化学习方法，通过引入列置换不变性结构先验和多重标签保持排列，提升稀疏奖励的学习信号，显著提升零-shot/少样本性能与可解释性，在对比全监督时与强基线相当，8B模型甚至超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 表格预测通常依赖梯Boost树和专用深度模型，缺乏可解释性和跨表迁移能力。大语言模型在跨任务推理方面潜力尚未被充分利用于表格数据。需要一种能在有限监督下利用LLM的推理能力，兼顾可解释性和对表格结构的利用。

Method: PRPO是一种简单高效的强化学习方法，将列置换不变性作为结构先验进行编码；对样本构造多组标签保持的排列，并在排列内外估计优势，以把稀疏奖励转化为密集学习信号。通过在有限监督下训练，使LLM具备面向表格的多步推理能力。

Result: 实验证明TabR1在完全监督微调下可达到与强基线相当的性能；在零-shot设定中接近32-shot水平；8B模型的TabR1在多任务上显著优于更大规模的LLM（如DeepSeek-R1 685B），性能提升可达53.17%。

Conclusion: TabR1及其PRPO方法为将推理型LLM应用于表格数据提供了高效且可扩展的方案，提升了少样本和零样本下的表现及可解释性，成为对大规模模型的有力替代，尤其在规模受限时具有显著优势。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [142] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 首次在弱通信MDP下，给出带函数逼近的平均奖励离线RL的有限时样本复杂性界限，提出带锚点的拟合Q迭代（Anchored FQI），并扩展到单轨迹数据情形。


<details>
  <summary>Details</summary>
Motivation: 尽管在折现回报设定下的离线强化学习样本复杂性已被广泛研究，平均回报的离线RL研究仍然不足，且往往依赖严格假设如遍历性或线性性。需要在更弱的假设条件下获得理论保障，以覆盖更广泛的MDP与数据分布。

Method: 提出Anchored Fitted Q-Iteration，将标准FQI与锚点机制相结合。锚点可视为一种权重衰减，有助于在平均奖励设定中实现有限时间分析。进一步将分析扩展到数据来自单轨迹而非IID转移的情形。

Result: 建立了带函数逼近的平均奖励离线RL在弱通信MDP下的有限时样本复杂性结果；锚点对实现可解析的有限时分析至关重要；并将分析扩展到单轨迹数据生成场景。

Conclusion: 锚点机制为平均奖励离线RL的理论分析提供了有效工具，允许在更宽松的假设下进行分析，推动对平均回报设置的理论研究，适用于弱通信MDP和非IID数据情形。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [143] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出 MILES，一种基于模态信息的学习率调度器，用于在多模态联合融合模型训练中平衡不同模态的学习速率，从而缓解模态过拟合，提升多模态与单模态的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常见的问题是对某一模态的过度依赖，导致学习不充分、性能受限。需要一种机制在训练过程中动态平衡各模态的贡献，以提高综合和单模态任务的表现。

Method: 提出 Modality-Informed Learning rate Scheduler (MILES)，通过监测训练过程中的模态条件利用率差异，动态调整学习率以平衡各模态的学习速度。适用于多模态联合融合模型，并在四个任务、七个基线方法上进行系统评估。

Result: 在所有任务和融合方法上，MILES 相较基线方法具有更优表现，能够实现模态使用的平衡，提升多模态性能与模态编码能力，并对缺失模态或单模态样本也有潜在收益。

Conclusion: 通过在训练中动态平衡模态学习，MILES 能显著提升多模态融合模型的性能与鲁棒性，强调了模态平衡在多模态学习中的重要性。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [144] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 本工作提出 Diffusion As Priors (DAP)，通过 Mercer 核衡量合成数据与真实数据在特征空间的相似度，并作为对扩散过程的先验引导，训练-free 即可提升 distilled 数据的代表性、保真度与跨体系结构泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的数据集蒸馏在追求多样性、泛化与代表性时往往难以同时实现，且对“代表性”这一先验关注不足，常需外部约束；需要一个不需重新训练即可提升数据质量的理论与方法。

Method: 将 representativeness formalize 为在特征空间中的 Mercer 核相似度，并将其作为先验用于引导反向扩散过程，从而提升蒸馏样本的代表性，且无需额外重训练。

Result: 在 ImageNet-1K 及其子集等大规模数据集上，DAP 在生成高保真数据集方面优于现有方法，并在跨架构泛化方面表现更好；同时实现了理论上将 diffusion priors 与数据集蒸馏目标联系起来的结果，且提供了一个训练-free 的实用框架。

Conclusion: 确立了扩散先验与数据集蒸馏目标之间的理论联系，给出一个无需重新训练的实用框架，以提升蒸馏数据的质量与泛化能力。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [145] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: 提出 CrossStateECG，一种面向跨状态（休息-运动）的人体识别的ECG 生物识别模型，通过多尺度深度卷积特征提取与注意力机制实现跨状态识别，在 Rest-to-Exercise、Exercise-to-Rest、Rest-to-Rest、Mixed-to-Mixed 等场景取得高准确率，具有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前ECG生物识别多聚焦于静息状态，跨状态下识别性能下降未得到充分解决，需在休息-运动等动态场景中实现鲁棒的跨状态身份识别。

Method: 通过多尺度深度卷积特征提取结合注意力机制构建跨状态识别模型，训练在一个状态，测试在另一状态（Rest-to-Exercise、Exercise-to-Rest），并在多数据集上验证泛化性，使用 exercise-ECGID 数据集、ECG-ID、MIT-BIH 等数据集进行跨状态及跨数据集评估。

Result: 在 Rest-to-Exercise 识别准确率为 92.50%，Exercise-to-Rest 为 94.72%；Rest-to-Rest 为 99.94%，Mixed-to-Mixed 为 97.85%；并在 ECG-ID 与 MIT-BIH 数据集上验证了模型的泛化能力。

Conclusion: CrossStateECG 展现出在跨状态以及混合场景下的高鲁棒性与良好泛化性，具有在动态现实场景中进行后期ECG认证的潜力；未来可关注跨设备鲁棒性、跨日稳定性及进一步的模型简化与效率优化。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [146] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: Transformers show modular, hierarchical representations and specialized layers that enable compositional reasoning and generalization, with performance scaling with task complexity and in-context examples.


<details>
  <summary>Details</summary>
Motivation: 理解模型是否通过内在的模块化结构来实现组合推理，以及在随机层次模型（RHM）场景下，ICL与技能组合如何影响泛化与层级表示的发展。

Method: 在RHM生成的子集序列上训练Transformer，评估 memorization、ID泛化、同规则的OOD泛化和跨层迁移；通过PCA和注意力模式聚类分析层级化、结构化的内部表示并观察训练中层专门化的出现。

Result: 性能随任务复杂性和上下文示例数量提升而提升；OOD任务比ID任务需要更多示例；训练过程中出现分层专门化，与泛化性能正相关；PCA和注意力聚类显示 Transformer 发展出结构化的分层表示，形成模块化、可解释的推理机制。

Conclusion:  Transformers 能发展出模块化、可解释的机制来支撑组合推理，并将内部算法结构与观测到的行为能力联系起来。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [147] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: GAN在地质建模中的应用与反演挑战；潜在空间耦合性导致匹配不到井数据；标签条件化/潜在过参数化可缓解但不足以实现稳定逆推；局部微调可提高拟合，但依赖初步逆推质量；未来需评估鲁棒性及与地质解释的协同。


<details>
  <summary>Details</summary>
Motivation: 高成本和数据获取不易扩展的地下勘探场景下，期望将过程模型的地质知识嵌入预测模型中，以提高推断效率和可解释性，利用GAN训练产生地层样本并对井和地震数据进行反演。

Method: 训练用于产生河相沉积的GAN，并尝试对井和地震数据进行反演。比较4、8、20口井的3个测试样本上的四种反演方法。分析GAN潜在表示的耦合性、通过标签条件化或潜在空间过参数化来解耦的效果，以及局部微调以重构潜在空间对误配的影响。

Result: 结果显示GAN的潜在表示是纠缠的，使得具有相似沉积特征的样本在潜在空间中不一定靠近，导致在增加井数或测试样本偏离训练数据时很难匹配井数据。标签条件化或潜在空间过参数化只能部分解耦，尚不足以实现成功的反演。通过对GAN进行局部微调以重组潜在空间，误配对齐程度可降到可接受水平，适用于有无地震数据的情形，但依赖于初步、部分成功的反演步骤，影响最终样本的质量与多样性。总体而言，GAN在地质建模工作流中的应用具备可行性，但需进一步评估鲁棒性及更好地与地质解释协同。

Conclusion: GANs具备将其整合进地质建模工作流程的潜力，但在鲁棒性、泛化与解释性方面仍需强化与研究。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [148] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X 是一个符号化基准，扩展自 I-RAVEN，用以评估大型语言模型（LLMs）与大型推理模型（LRMs）在类比与数学推理中的泛化与鲁棒性。通过增加操作数复杂性、属性范围及感知不确定性来提高难度。与 LLMs 相比，LRMs 在较长推理关系和更宽属性范围的任务上表现出更高的生产力与系统性，但在处理不确定性和探索多种概率结果方面仍存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在扩展现有的 I-RAVEN 基准，以更全面地评估推理模型在复杂符号化任务中的泛化与鲁棒性，尤其是在存在感知不确定性的情境下。

Method: 设计并实现 I-RAVEN-X 基准，并在 LLMs 与 LRMs 之间进行对比评估，关注操作数复杂性、属性范围和感知不确定性的影响；通过指标衡量生产力、系统性以及多步推理中的不确定性处理能力。

Result: LRMs 在处理较长的推理关系和较宽的属性范围时，表现出更高的生产力和系统性；但在不确定性推理方面存在显著不足，无法有效探索多种概率性输出。

Conclusion: 要提升符号化推理任务的鲁棒性与泛化性，需加强 LRMs 对不确定性与概率多样性的探索能力，以及在复杂情境中的推理稳定性。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [149] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 提出一种基于动量的随机DC优化算法，在小批量下也能收敛；无动量时可能发散，动量算法在理论和实验上均展现出色性能。


<details>
  <summary>Details</summary>
Motivation: 随机差分凸优化在机器学习中广泛应用，但现有方法对小批量下的收敛性依赖大、需要强噪声假设。

Method: 在标准光滑性和对凹部分的方差有界假设下，给出基于动量的随机DC优化算法，证明其对任意批量大小的收敛性；同时给出没有动量的情形可能发散的理论结果。

Result: 给出理论收敛性证明；在若干实验中表现出优越的鲁棒性和效率，特别是在小批量设置下。

Conclusion: 动量是随机DC优化收敛性的关键，所提算法兼具理论保证和良好实验性能。

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [150] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出 Counterfactual Knowledge Distillation (CFKD)，通过生成多样化的反事实样本并进行知识蒸馏，弥补缺少分组标签的鲁棒性不足，能够在多混淆因素情境下提升分组鲁棒性与泛化，且在低数据场景表现尤为显著。


<details>
  <summary>Details</summary>
Motivation: 现有的分组鲁棒方法（如 Deep Feature Reweighting）需要明确的组标签，在组内样本不足、多重混淆因子同时存在时，模型容易受到 Clever Hans 式的偏置影响，导致鲁棒性下降。

Method: CFKD 框架通过生成多样的 counterfactuals，辅助人工标注者有效定位模型决策边界，并在此基础上进行知识蒸馏，将新的反事实数据引入并平衡各组，且无需混淆因子标签，具备对多混淆因子场景的扩展性。

Result: 在五个数据集（含从合成任务到工业应用）的实验中，CFKD 在低数据情境和强相关的偏置情形下取得显著提升；此外，通过对 counterfactual explainer 和教师模型的消融分析，展示了这两者对鲁棒性的影响。

Conclusion: CFKD 提供一种无须混淆标签、可扩展且对多混淆因子鲁棒的训练框架，通过反事实数据与知识蒸馏相结合，显著提升分组鲁棒性与泛化，尤其在数据稀缺场景中效果更突出。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [151] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: Introducing label noise in gradient descent helps generalize in low-SNR settings by suppressing noise memorization, enabling faster signal growth while avoiding overfitting; standard gradient descent overfits to noise with a non-vanishing test error.


<details>
  <summary>Details</summary>
Motivation: Deep models can memorize noise, especially in low-SNR data. The idea is to explore whether adding label noise to gradient updates acts as a regularizer that improves test performance, by controlling memorization and promoting signal learning.

Method: The work analyzes training a two-layer neural network using a label-noise gradient descent algorithm in an idealized signal-noise data setting. It provides theoretical results showing that label noise during training suppresses memorization of noise, allows rapid growth of the underlying signal, and keeps overfitting in check.

Result: Label-noise gradient descent suppresses noise memorization and yields good generalization in the low-SNR regime, whereas standard gradient descent tends to overfit to noise, with a non-vanishing lower bound on test error.

Conclusion: Introducing label noise in gradient updates can be beneficial for generalization in low-SNR settings, offering a regularization mechanism that mitigates memorization of noisy components and improves test performance compared to standard gradient descent.

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [152] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: TrajMamba 提出一个高效且语义丰富的车辆轨迹学习框架，结合 GPS 与道路视角的编码、旅行目的感知预训练以及基于可学习掩码的知识蒸馏预训练，能够去除冗余点并提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决轨迹数据中的两个挑战：1) 旅行目的与路网/POI相关的文本信息带来的计算负担；2) 现实轨迹存在冗余点影响嵌入质量与效率。

Method: 引入 Traj-Mamba 编码器，对 GPS 与道路视角进行联合建模；引入旅行目的感知预训练，将旅行目的信息整合到嵌入中且不增加额外嵌入开销；采用知识蒸馏预训练，通过可学习的掩码生成器识别关键轨迹点并获得压缩嵌入。

Result: 在两个真实数据集和三个下游任务上，TrajMamba 在效率和准确性上均优于最先进基线。

Conclusion: TrajMamba 提供一种高效且语义丰富的轨迹学习方案，通过多视角建模和自监督/预训练策略，提升轨迹表示质量与应用效果，同时显著减少冗余信息。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [153] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 扩展解码器 Transformer，加入无监督学习的随机潜变量，通过变分推断训练，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 旨在提高解码阶段的生成能力，通过潜在变量捕捉未观测的结构信息，从而改善任务表现。

Method: 在解码器 Transformer 中引入条件化的随机潜变量，采用变分自编码/变分推断进行无监督学习，使潜变量在生成时影响分布。

Result: 实验表明，条件化潜变量的引入显著提高下游任务的性能。

Conclusion: 潜变量条件化的解码器 Transformer 对提升生成质量和任务性能具有潜在价值，且无需监督信号即可学习潜在结构。

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [154] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 提出一个可验证属性框架来评估时间序列异常检测的指标，并提出 LARM/ALARM 两个通用/高级指标以实现对所有属性的满足和更严格的要求。


<details>
  <summary>Details</summary>
Motivation: 当前评价指标往往只覆盖任务的窄部分，且结果易混淆；在安全关键场景中需要可验证且可比的评估框架。

Method: 定义一组可验证的性质，分析现有 37 种广泛使用的指标在这些性质上的满足情况；发现多数组指标仅满足少数性质，缺乏一致性；提出 LARM 能满足所有性质，进一步扩展为 ALARM 以符合更严格要求。

Result: 证明 LARM 能满足所有提出的性质，ALARM 在更严格的条件下也能实现；提供一个更稳定、可比较的评估手段。

Conclusion: 为时间序列异常检测的评估提供一个原理性框架，LARM/ALARM 有助于统一评估和改进现有指标。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [155] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 通过对变分自编码器的设计空间进行降维和物理化学属性的组织，以提升抗菌肽设计的可解释性和优化效率。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽设计空间极大且难以高效探索，需提高潜在空间的可解释性和量化质量；通过降维和属性化组织潜在空间可支持更高效的搜索。

Method: 使用变分自编码器来建模肽序列的潜在空间，研究进一步降维对设计空间的影响；评估将潜在空间与不同物理化学性质对齐的可解释性和优化效率；在不同标签可用比例下尝试对潜在空间加以组织。

Result: 进一步降维在数据可用性充分时有利；降维搜索空间更具可解释性；以不同物理化学属性组织潜在空间可在不同标签覆盖率下实现有效的组织且提升优化效率。

Conclusion: 通过将降维与物理化学属性结合来组织潜在空间，可以在抗菌肽设计中提高可解释性和搜索效率，即使在有限标签下也能受益。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [156] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: CEPerFed 提出一个面向多脉冲 MRI 分类的个性化、通信高效的联邦学习框架，通过客户端历史风险梯度和历史均值梯度来协调本地与全局优化，并使用分层奇异值分解来减少通信量。


<details>
  <summary>Details</summary>
Motivation: 在跨机构隐私保护场景下，需利用多脉冲 MRI 数据进行鲁棒分类，但数据异质性与通信成本是主要挑战。传统FL在此场景下容易收敛慢、通信负担重。

Method: 提出 CEPerFed：1) 引入客户端历史风险梯度和历史均值梯度，以加权对齐本地更新与全局方向，改善对异质数据的鲁棒性；2) 提出层次化奇异值分解 HSVD，仅传输对模型更新最关键的信息以降低通信开销。

Result: 在五个分类任务上进行实验，显示方法有效，且将提供代码实现（待接受后在 GitHub 发布）。

Conclusion: CEPerFed 能在保护隐私的前提下，缓解数据异质性带来的影响并显著降低通信成本，促进模型在分布式 MRI 数据上的稳定收敛与性能提升。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [157] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种无CLS token和无定位嵌入的全置换不变Vision Transformer，结合ShuffleStrides数据增强，用于LUS视频中CPE与NCIP/ARDS-like的分类，在小样本数据条件下实现优于基线的ROC-AUC并具备实时部署潜力。


<details>
  <summary>Details</summary>
Motivation: 解决LUS视频中CPE与非心源性病变及正常肺之间高视觉变异导致的自动化分类困难，尤其是在非典型炎性模式、弥散性病变和健康肺的多样性情形。

Method: 设计0.25M参数的ZACH-ViT，移除位置嵌入与CLS token以实现全置换不变；提出ShuffleStrides Data Augmentation (SSDA)，通过打乱探头视角序列与帧顺序但保持解剖有效性来提升泛化。

Result: 在95名重症患者的380段LUS视频上，与9个领先基线比较，ZACH-ViT的验证/测试ROC-AUC达0.80/0.79，灵敏度0.60、特异度0.91，且训练速度比Minimal ViT快1.35倍，参数量小2.5倍，显示架构设计对小数据环境的优势。

Conclusion: 将架构设计与数据结构对齐可以在小数据医学影像任务中超越仅靠扩大模型规模的策略，具备更好的可部署性与实时性。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [158] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 通过 GAN 数据增强解决极端类别不平衡的自杀预测问题，比较 LR、RF、SVM 等模型在真实测试集上的表现，GAN 提供合成样本以提升建模能力；结果显示不同模型在精度和召回率之间存在权衡，RF 未能识别自杀企图。


<details>
  <summary>Details</summary>
Motivation: 解决真实数据中正样本极少导致的极端类别不平衡，以及提升自杀预测的有效性。

Method: 初始数据656条，正样本4条。使用 GAN 生成合成样本进行数据扩增；训练多种模型（LR、RF、SVM 等），在真实测试集上评估，报告加权精度、召回率、F1，以及敏感性/特异性。

Result: LR：权重精度0.99，召回率0.85，F1 0.91；RF：0.98、0.99、0.99；SVM：0.99、0.76、0.86。LR与SVM在自杀企图检测上灵敏度为1.0，且对非企图样本有误判，特异性分别为0.85和0.76；RF 的灵敏度为0.0，特异性为1.0。GAN 在生成合成数据方面发挥关键作用，提升了建模潜力。

Conclusion: GAN 能缓解自杀预测中的数据稀缺问题，并对建模产生积极影响，但不同模型在灵敏度与特异性之间存在权衡；总体而言，GAN 支撑的数据扩增有助于自杀预防建模。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [159] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出级联 OVD 自适应框架，结合 FLAME 主动学习实现快速、低成本的 RS 领域高精度检测。


<details>
  <summary>Details</summary>
Motivation: 在遥感（RS）领域，开放词汇对象检测（OVD）的零-shot 性能易受自然语言歧义影响，难以区分细粒度类别，影响如非法捕捞监测等关键应用。

Method: 先用零-shot OVD 产生高召回的对象提案，再由一个在少量用户注释样例上训练的紧凑分类器进行高精度筛选；核心是 FLAME，一步主动学习策略，通过密度估计找到决策边界附近的不确定样本，并进行聚类以确保样本多样性；不需要大规模模型微调，能在极短时间内实现自适应。

Result: 在遥感基准上持续超越现有方法，体现出高精度与高效率的兼具；自适应过程在不到一分钟内完成，相较于最先进的替代方法具有显著速度优势与资源效率提升。

Conclusion: 将大规模预训练模型的泛化能力与轻量级分类器的定制性相结合，提供一种实用且资源友好的 RS 自适应框架，满足用户对特定任务的个性化需求。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [160] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 将自然语言反馈转化为标量效用，以在Bayesian Optimization中进行搜索；通过LLM实现与人类偏好之间的桥接，提升灵活性与样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界优化常受限于需要明确、量化的目标，而许多目标难以用一组固定的数值描述。现有BO或偏好式BO对反馈形式有约束，且对领域知识依赖较大。需要一种能将多样化文本反馈转化为一致效用信号的接口，同时保持BO的不确定性量化。

Method: 使用大语言模型将非结构化的自然语言反馈映射为标量效用，进而在数值搜索空间上进行贝叶斯优化；在不进行手工核设计的前提下，结合灵活的用户先验，保持样本效率和不确定性推断；与传统BO基线和纯LLM优化器对比，证明在反馈受限场景下的优势。

Result: 该混合方法在实验中优于常规BO基线和仅LLM的优化器，尤其在反馈有限的情境下表现更稳健、样本利用更高效。

Conclusion: 通过语言-循环的框架，将文本反馈自然融入到优化目标中，既提升了决策者的交互体验，又维持了BO的理论优势和性能。该方法具有广泛适用性，并可减少领域特定的核设计成本。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [161] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 提出三大贡献：在多模型马尔可夫决策过程（MMDP）中将策略梯度与动态规划联系起来，提出坐标上升动态规划CADP以在不确定模型的加权下最大化折扣回报并保证单调改进；给出ERM-TRC与EVaR-TRC的指数型经验风险最小化Bellman算子收缩性条件、存在确定性最优策略以及相应的求解算法（值迭代、策略迭代、线性规划）；提出面向风险规避目标的无模型Q学习算法，证明在ERM Bellman算子可能不收缩的情况下通过单调性证明收敛，得到最优风险规避价值函数和最优策略。


<details>
  <summary>Details</summary>
Motivation: 面临不确定模型下的策略优化与风险规避需求：在MMDP中兼顾多模型的不确定性并实现稳健策略；在ERMs下建立算子性质与收敛性，提供可操作的求解框架；在无模型场景下实现对ERM-TRC/EVaR-TRC的Q学习以获得稳定最优策略。

Method: 1) CADP：迭代调整模型权重以实现对局部最优的单调性提升；2) 证明指数ERM Bellman算子在特定条件下的收缩性，给出ERM-TRC与EVaR-TRC的存在性与算法（指数值迭代、策略迭代、LP）以求最优策略；3) 提出风险规避的无模型Q学习算法，利用Q-learning ERM Bellman的单调性构造收敛性证明，确保收敛到最优风险价值函数。

Result: CADP实现对局部最优的单调改进并处理模型不确定性；在ERM-TRC与EVaR-TRC下存在确定性最优策略，且给出多种求解算法；无模型Q学习在ERM情形下通过单调性证明了收敛性，得到最优风险规避策略与价值函数。

Conclusion: 工作建立了不确定性模型下的策略优化的新联系（策略梯度与DP）并为风险规避目标提供完整的求解框架，涵盖确定性/无模型场景的多种算法（DP、LP、Q学习），为ERM-TRC与EVaR-TRC提供理论与算法支撑。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [162] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 本文研究将黑盒大语言模型作为分类器使用时的输出粒度问题，并提出高效方法以在不降低性能的前提下显著提升可用的操作点数量与多样性，从而实现更细粒度的决策阈值控制。


<details>
  <summary>Details</summary>
Motivation: 解决使用黑盒 LLMs 作为分类器时，因输出概率的低数值粒度导致难以在特定性能约束（如 precision ≥95%）下进行精细控制的问题；希望在保持性能的前提下提高操作点的可控性。

Method: 首先分析输出为圆整化的口头化概率的偏向原因；随后对标准提示工程、不确定性估计与置信度引出等方法进行实验，发现它们在不牺牲性能或增加推理成本的情况下无法显著提升粒度；最后提出高效方法以显著增加可用操作点的数量和多样性。

Result: 实验显示前述标准方法无法有效提升操作粒度；所提方法在 11 个数据集、3 种 LLM 上实现更细粒度的操作点，并在性能上与基线方法相当或更优。

Conclusion: 提出的高效方法可在不牺牲性能的前提下显著提升黑盒 LLM 作为分类器时的操作粒度与多样性，具有较好的泛化性。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [163] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 引入物理信息的PINN用于海冰速度与浓度预测，基于HIS-Unet架构，通过物理损失和激活函数提升在有限数据下的泛化与物理一致性，显著改善SIV和SIC的日预测，特别是在融化期、早期冻结及快速移动冰区。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的ML在海冰预测中易受数据量和物理一致性约束，且历史分布与未来情形差异增大，因此需要将物理规律融入学习过程以提高鲁棒性和可解释性。

Method: 在HIS-Unet基础上引入物理损失函数和激活函数，将海冰物理约束融入PINN框架，训练一个能输出物理上合理的SIV与SIC的模型。

Result: 与纯数据驱动模型相比，PINN在日预测上表现更好，且在样本量较小时仍具优势，尤其在融化和初冻季节以及靠近快速移动冰区的SIC预测获得改进。

Conclusion: 将物理约束纳入海冰预测的深度学习模型可提升鲁棒性、数据效率和物理一致性，适用于未来海冰条件变化的预测。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [164] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 提出一个可微分的潜在流形 atlas，并在点云数据上通过无监督启发式学习 atlas；在 Klein bottle 分类和 RNA 速度分析上展示可解释性和鲁棒性，同时在某些设置下提高了效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管流形假设广泛应用，现有流形学习多以将数据嵌入到更高维空间为目标，导致在嵌入维度接近潜在维度时丢失流形特征；直接在潜在流形上学习的能力不足且缺乏可微分、可优化的架构。

Method: 实现一个通用数据结构来维护可微分 atlas，使得可在流形上进行黎曼优化；提出一个无监督启发式，基于点云数据学习可微分 atlas；在少数任务中对比验证其在效率、准确性、解释性上的潜在优势。

Result: 在所选设置中，方法表现出效率与准确性上的优势；在有监督的 Klein bottle 分类和 RNA velocity 的 hematopoietic 数据分析中，展示了更好的可解释性与鲁棒性。

Conclusion: atlas-based 潜在流形学习具有潜力，证明了直接在潜在流形上学习的可行性，未来的工作可聚焦于扩展性、鲁棒性和更广泛的应用场景。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [165] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出一个样本级的忘记与向后迁移测量框架，结合1→0/0→1转移计数与机会调整，系统评估后训练对预训练知识的影响，并发现不同策略对忘记和向后迁移的影响差异，模型合并并非有效缓解忘记。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在揭示大规模后训练（在语言模型上）对预训练知识的影响，特别是忘记与向后迁移的差异性：单纯的“平均效应”无法捕捉某些事实的遗忘或记忆的再获得。

Method: 提出一个样本级的 forgetting/backward transfer 指标，分别计数1→0（训练后正确但被遗忘）与0→1（训练后错误但表现出向后迁移）转移；在多项选择题基准中引入机会调整版本，以扣除随机猜测的贡献；在不同的后训练阶段、模型规模与数据规模上进行大规模分析。

Result: 结论性结果包括：1) 域持续式预训练引入中等程度的遗忘，且向后迁移处于低到中等；2) 使用强化学习/监督微调（RL/SFT）对基础模型与指令化训练后，数学与逻辑任务表现出中到大程度的向后迁移，同时整体遗忘处于低到中等；3) 对指令化模型执行 RL/SFT 时数据规模敏感：小规模时遗忘和向后迁移均较小；大规模时效果混合，需要更好的对照实验；4) 模型合并并不能可靠缓解遗忘。

Conclusion: 该框架为在大规模设置下映射后训练对预训练知识影响提供了一种实用的量化工具，有助于推动构建更具通用能力的 AI 系统。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [166] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 提出在推理时对 Flow Matching (FM) 进行计算扩展的策略，保留线性插值并提升图像生成以及无条件蛋白质生成等领域的样本质量，首次将FM推理时扩展推广到科学领域。


<details>
  <summary>Details</summary>
Motivation: 现有工作在推理阶段扩展计算以提升样本质量，但对 FM 的推理扩展研究不足，且先前的方法（如 Kim et al., 2025）采用非线性方差保持插值VP，破坏了 FM 的高效性与可直采样性；推理时的扩展也主要应用于视觉任务，缺乏对科学领域的验证。

Method: 提出新的推理时扩展流程，保持在采样时的线性插值不变；将该策略应用于图像生成以及无条件蛋白质生成等任务以验证普适性。

Result: 随着推理计算量增加，样本质量稳定提升；FM 的推理扩展可应用于科学领域，首次证明在科学任务中的可行性。

Conclusion: 推理时对 Flow Matching 的扩展是有效且具有泛化性，能够跨越视觉和科学领域提升样本质量。

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [167] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 提出了一种无偏低秩优化方法GUM，通过基于GaLore的机制与Muon算法结合的层级采样实现记忆高效的LLM微调/预训练，并在理论上达到Muon的收敛性，在实验上优于GaLore，甚至可超越全参数训练。


<details>
  <summary>Details</summary>
Motivation: 在训练大规模语言模型时，需要记忆高效的优化策略。现有的低秩投影方法（如GaLore）尽管节省内存，但往往存在偏差且缺乏收敛保证，导致与全参数训练存在性能差距。这驱动研究一个无偏且具备理论收敛保证的低秩优化方案。

Method: 引入基于层级采样的去偏策略，建立在GaLore机制之上并结合Muon算法，提出GUM。通过理论分析证明其与Muon基准具有相同的收敛性，同时保持低秩方案的内存高效性。通过在LLM的微调和预训练任务上的实验验证，GUM在性能上显著优于GaLore，且在某些情况下优于全参数训练；分析显示改进来自于层内知识分布更均匀、参数空间的更有效利用。

Result: 理论上，GUM的收敛性与Muon相匹配；在实际任务中，GUM对比GaLore有非平凡的提升，甚至在某些设置优于完整参数训练；且保持低秩记忆效率。进一步分析表明，知识在各层的分布更均匀提升了记忆和利用效率。

Conclusion: 通过引入层级采样的去偏策略，GUM实现了无偏的低秩优化同时保持内存效率，理论与实验结果均支持其优于现有的GaLore，并在某些情形超越全参数训练，暗示知识分布的均匀性对模型记忆与表现的重要性。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [168] [A Semantic Generalization of Shannon's Information Theory and Applications](https://arxiv.org/abs/2510.15871)
*Chenguang Lu*

Main category: cs.IT

TL;DR: 通过以语义约束替代单纯的失真约束，提出语义信息理论的广义化（G理论），在多领域有广泛应用；并与物理思想（自由能、信息效率）建立联系，同时存在对复杂语义表示的局限。


<details>
  <summary>Details</summary>
Motivation: 回答“语义通信是否需要独立的语义信息理论”这一问题，主张不需要全新理论，而是在香农信息理论框架下通过语义约束进行广义化。

Method: 以一组真函数组成的语义信道来表达语义失真、语义信息度量和语义信息损失；提出最大语义信息等同于最大似然、类似正则化最小二乘；将该理论应用于日常与电子语义通信、机器学习、约束控制、贝叶斯确认、投资组合与信息价值等领域，并与统计物理的概念对照。

Result: 展示G理论在多领域的应用，提升机器学习方法（多标签学习、最大互信息分类、混合模型、潜变量问题等），并提出将Friston的最小自由能原理发展为最大信息效率原理；与其他语义信息理论比较，讨论其局限性。

Conclusion: G理论提供了一个统一且可拓展的语义信息框架，但在表达复杂数据语义方面仍有局限，需要与现有理论（如Friston原理）进一步融合与扩展。

Abstract: Does semantic communication require a semantic information theory parallel to
Shannon's information theory, or can Shannon's work be generalized for semantic
communication? This paper advocates for the latter and introduces a semantic
generalization of Shannon's information theory (G theory for short). The core
idea is to replace the distortion constraint with the semantic constraint,
achieved by utilizing a set of truth functions as a semantic channel. These
truth functions enable the expressions of semantic distortion, semantic
information measures, and semantic information loss. Notably, the maximum
semantic information criterion is equivalent to the maximum likelihood
criterion and similar to the Regularized Least Squares criterion. This paper
shows G theory's applications to daily and electronic semantic communication,
machine learning, constraint control, Bayesian confirmation, portfolio theory,
and information value. The improvements in machine learning methods involve
multilabel learning and classification, maximum mutual information
classification, mixture models, and solving latent variables. Furthermore,
insights from statistical physics are discussed: Shannon information is similar
to free energy; semantic information to free energy in local equilibrium
systems; and information efficiency to the efficiency of free energy in
performing work. The paper also proposes refining Friston's minimum free energy
principle into the maximum information efficiency principle. Lastly, it
compares G theory with other semantic information theories and discusses its
limitation in representing the semantics of complex data.

</details>


### [169] [Cluster-wise processing in fronthaul-aware cell-free massive MIMO systems](https://arxiv.org/abs/2510.16432)
*Zahra Mobini,Ahmet Hasim Gokceoglu,Li Wang,Gunnar Peters,Hyundong Shin,Hien Quoc Ngo*

Main category: cs.IT

TL;DR: 提出一种基于簇的CF-mMIMO fronthaul受限架构，在簇内共享CSI实现可扩展的资源分配与预编码；以改进的WMMSE方法求解两类混合整数非凸问题。


<details>
  <summary>Details</summary>
Motivation: 解决在有限fronthaul带宽下CF-mMIMO的可扩展性和性能问题，通过将AP分组为处理簇并在簇内共享CSI来降低通信开销，同时考虑簇间泄漏对性能的影响。

Method: APs分组为若干处理簇（PC），簇内共享CSI以进行资源分配与预编码；定义簇内总伪SE度量，兼顾簇内干扰与簇间泄漏；针对每个PC提出两类优化问题，分别在小尺度和大尺度时变下求解，使用改进的WMMSE方法解决非凸混合整数优化。

Result: 提出一种改进的WMMSE方法用于求解所提出的非凸混合整数问题；建立基于两时域的簇级优化框架。

Conclusion: 簇级CF-mMIMO在fronthaul受限环境下具有可扩展性，所提出的框架在局部CSI条件下实现有效资源分配与预编码，并在权衡复杂度和性能方面具有潜力。

Abstract: We exploit a general cluster-based network architecture for a
fronthaul-limited user-centric cell-free massive multiple-input multiple-output
(CF-mMIMO) system under different degrees of cooperation among the access
points (APs) to achieve scalable implementation. In particular, we consider a
CF-mMIMO system wherein the available APs are grouped into multiple processing
clusters (PCs) to share channel state information (CSI), ensuring that they
have knowledge of the CSI for all users assigned to the given cluster for the
purposes of designing resource allocation and precoding. We utilize the sum
pseudo-SE metric, which accounts for intra-cluster interference and
intercluster-leakage, providing a close approximation to the true sum
achievable SE. For a given PC, we formulate two optimization problems to
maximize the cluster-wise weighted sum pseudo-SE under fronthaul constraints,
relying solely on local CSI. These optimization problems are associated with
different computational complexity requirements. The first optimization problem
jointly designs precoding, user association, and power allocation, and is
performed at the small-scale fading time scale. The second optimization problem
optimizes user association and power allocation at the large-scale fading time
scale. Accordingly, we develop a novel application of modified weighted minimum
mean square error (WMMSE)-based approach to solve the challenging formulated
non-convex mixed-integer problems.

</details>


### [170] [Enhancing Channel Estimation in RIS-aided Systems via Observation Matrix Design](https://arxiv.org/abs/2510.16576)
*Zijian Zhang,Mingyao Cui*

Main category: cs.IT

TL;DR: 提出一种基于贝叶斯优化的观测矩阵设计与自适应内核培训的 RIS 通道估计框架，结合交替黎曼流形优化（ARMO）实现接收机合路器与 RIS 相位位翻矩阵的迭代更新，显著提升估计精度。


<details>
  <summary>Details</summary>
Motivation: 在 RIS 辅助无线通讯中，准确的通道估计对充分发挥 RIS 潜力至关重要。现有观测矩阵设计与通道估计方法往往受限于资源开销、维度高和协方差信息不足等问题。本工作通过贝叶斯优化设计观测矩阵以最大化接收 Pilot 信号与 RIS 通道之间的互信息，提升估计性能，并提出高效的优化算法与自适应内核训练策略以降低额外 Pilot 资源需求。

Method: 1) 以贝叶斯优化框架生成观测矩阵，使其最大化观测信噪比与通道互信息；2) 提出交替黎曼流形优化（ARMO）算法，迭代更新接收端合路器与 RIS 相位移矩阵；3) 引入自适应核训练策略，在无需额外 Pilot 的情况下迭代 refinement 通道协方差矩阵。

Result: 数值仿真表明，基于 ARMO 的增强估计器在估计精度方面显著优于现有方法，展示了在 RIS 通道估计中的潜在实际效用。

Conclusion: 所提出的 ARMO 结合自适应核训练的 RIS 通道估计框架能够在不增加 Pilot 资源的前提下显著提升估计性能，具有良好应用前景。

Abstract: Reconfigurable intelligent surfaces (RISs) have emerged as a promising
technology for enhancing wireless communications through dense antenna arrays.
Accurate channel estimation is critical to unlocking their full performance
potential. To enhance RIS channel estimators, this paper proposes a novel
observation matrix design scheme. Bayesian optimization framework is adopted to
generate observation matrices that maximize the mutual information between
received pilot signals and RIS channels. To solve the formulated problem
efficiently, we develop an alternating Riemannian manifold optimization (ARMO)
algorithm to alternately update the receiver combiners and RIS phase-shift
matrices. An adaptive kernel training strategy is further introduced to
iteratively refine the channel covariance matrix without requiring additional
pilot resources. Simulation results demonstrate that the proposed ARMO-enhanced
estimator achieves substantial gains in estimation accuracy over
state-of-the-art methods.

</details>


### [171] [Feedback Lunch: Deep Feedback Codes for Wiretap Channels](https://arxiv.org/abs/2510.16620)
*Yingyao Zhou,Natasha Devroye,Onur Günlü*

Main category: cs.IT

TL;DR: 在高斯线道（Gaussian wiretap channel）中，利用通道输出反馈，可以通过种子式模块化编码、通用哈希和学习驱动的可靠性编码实现正的保密速率，并可在双方共享密钥的帮助下克服窃听者的优势。


<details>
  <summary>Details</summary>
Motivation: 研究在反退化（reversely-degraded）wiretap信道下，当无反馈时保密容量为零的情形，探索是否通过反馈实现正的保密传输，并推动对感知协同安全通信的编码设计。

Method: 提出一种带种子的模块化编码设计，结合通用哈希函数用于信息安全性，以及基于学习的反馈编码用于提高可靠性；分析可靠性和信息泄露之间的权衡；展示通过反馈可在合法方之间建立秘密密钥的可行性。

Result: 证实反馈使得在高斯wiretap信道上获得正的保密速率成为可能，并展示了通过密钥协商抑制窃听者的优势；提供对感知辅助安全通信的代码设计启示。

Conclusion: 这类设计为下一代综合感知与通信的安全编码提供了思路，强调在反馈可用的场景下，结合哈希和学习驱动的编码有望提升保密性与可靠性。

Abstract: We consider reversely-degraded wiretap channels, for which the secrecy
capacity is zero if there is no channel feedback. This work focuses on a seeded
modular code design for the Gaussian wiretap channel with channel output
feedback, combining universal hash functions for security and learned
feedback-based codes for reliability to achieve positive secrecy rates. We
study the trade-off between communication reliability and information leakage,
illustrating that feedback enables agreeing on a secret key shared between
legitimate parties, overcoming the security advantage of the wiretapper. Our
findings also motivate code designs for sensing-assisted secure communication,
to be used in next-generation integrated sensing and communication methods.

</details>


### [172] [Non-Orthogonal Pilot Sequence Design for Multi-Cells Interference Networks](https://arxiv.org/abs/2510.16792)
*Zhi Gu,Wai Ho Mow*

Main category: cs.IT

TL;DR: 提出 ETSC 作为多小区和超载 CDMA 场景下的非正交序列集合设计准则；给出 ETSC 下界的闭式表达，并在 MM 框架下提出序列生成算法 ETSC-MM。


<details>
  <summary>Details</summary>
Motivation: 非正交序列集合的互干扰会显著影响多用户系统性能；需要新的序列设计准则来刻画小区间干扰不对称性及超载场景中的相关性。ETSC 作为对权重相关性平方和的度量，扩展并一般化 Welch 下界。

Method: 在给定序列长度 tau <= K（K 为每小区用户数）的条件下，推导 ETSC 的下界闭式表达；把互干扰功率因子矩阵的正定性作为达到下界的可达条件；在无法直接生成序列时，提出基于 Majoration-Minimization 的 ETSC-MM 算法来迭代生成低 ETSC 的序列集合。

Result: 给出 ETSC 下界的闭式表达；在干扰矩阵正定时，给出下界条件下的最优序列集的简易获取方式；提出 ETSC-MM 算法用于在一般参数下生成低 ETSC 的序列集合。

Conclusion: ETSC 提供一个可操作的多小区和超载场景下的序列设计基准，扩展了 Welch 与扩展 Welch 边界，并给出实用的序列生成算法，促进对非正交序列设计的系统性研究。

Abstract: In wireless communications, the performance of non-orthogonal sequence sets
significantly affects the level of multi-user interference when the number of
users surpasses the sequence length. The design of non-orthogonal sequences
plays a crucial role in both the non-orthogonality of the pilots in multi-cell
systems and the signature sequences in overloaded code-division multiple-access
(CDMA) systems. In multi-cell systems, considering the strength disparity
between channels originating from the home cell and the neighboring cells, the
extended total squared correlation (ETSC) is proposed as a new sequence design
criterion, which is defined as the sum of squares of the weighted correlations
among sequences. In this paper, we derive a closed-form expression for the
lower bound of ETSC for multi-cell systems with a given sequence length $\tau$,
where $\tau \leq K$ and $K$ is the number of users per cell. This can be
regarded as a generalization of the well-known Welch bound (Welch, 1974, IEEE
TIT) and the extended Welch bound (Wang et al., 2021, IEEE TWC). Additionally,
from the necessary conditions of the bound, the optimal sequence set can be
easily obtained when the interference power factor matrix is positive definite.
On the other hand, to address the lack of sequence generation methods under
certain parameter conditions, we propose the ETSC-MM algorithm, which generates
sequence sets with low ETSC based on a Majorization-Minimization (MM)
optimization framework.

</details>


### [173] [Unlocking Off-the-Grid Sparse Recovery with Unlimited Sensing: Simultaneous Super-Resolution in Time and Amplitude](https://arxiv.org/abs/2510.16948)
*Ruiming Guo,Ayush Bhandari*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The recovery of Dirac impulses, or spikes, from filtered measurements is a
classical problem in signal processing. As the spikes lie in the continuous
domain while measurements are discrete, this task is known as super-resolution
or off-the-grid sparse recovery. Despite significant theoretical and
algorithmic advances over the past decade, these developments often overlook
critical challenges at the analog-digital interface. In particular, when spikes
exhibit strong-weak amplitude disparity, conventional digital acquisition may
result in clipping of strong components or loss of weak ones beneath the
quantization noise floor. This motivates a broader perspective:
super-resolution must simultaneously resolve both amplitude and temporal
structure. Under a fixed bit budget, such information loss is unavoidable. In
contrast, the emerging theory and practice of the Unlimited Sensing Framework
(USF) demonstrate that these fundamental limitations can be overcome. Building
on this foundation, we demonstrate that modulo encoding within USF enables
digital super-resolution by enhancing measurement precision, thereby unlocking
temporal super-resolution beyond conventional limits. We develop new
theoretical results that extend to non-bandlimited kernels commonly encountered
in practice and introduce a robust algorithm for off-the-grid sparse recovery.
To demonstrate practical impact, we instantiate our framework in the context of
time-of-flight imaging. Both numerical simulations and hardware experiments
validate the effectiveness of our approach under low-bit quantization, enabling
super-resolution in amplitude and time.

</details>


### [174] [Channel Capacity for FMCW-based Optical Wireless Integrated Sensing and Communication: Asymptotic Analysis and Envelope Design](https://arxiv.org/abs/2510.17093)
*Yunfeng Wen,Fang Yang,Jian Song,Zhu Han*

Main category: cs.IT

TL;DR: 提出在 FMCW 基础的光无线 ISAC 中进行信道容量分析，给出在感知约束下的容量上下界及低/高 SNR 极限，并用基于包络的 PAM 设计验证容量可达性，揭示通信与感知的权衡与设计指引。


<details>
  <summary>Details</summary>
Motivation: 在光学无线的集成感知与通信（OW-ISAC）快速发展的大背景下，建立一个信息理论框架，使通信性能在满足感知需求的前提下得到定量分析与优化。

Method: 将基于 FMCW 的 OW-ISAC 系统重新建模为信息理论问题，引入额外的谐均值约束以确保感知性能。推导在该约束下的信道容量上下界，并给出低/高信噪比区域的渐进行表达。进一步基于脉冲幅度调制的包络设计，分析其容量可达性，并通过数值仿真展示通信与感知之间的权衡。

Result: 给出在感知约束下的信道容量上下界及低/高 SNR 的渐近表达。验证了基于包络的 PAM 设计在一定条件下能够达到容量极限，仿真揭示了通信与感知之间的权衡关系。

Conclusion: 在感知约束下的信道容量分析为 OW-ISAC 的最优性与可实现性提供了理论依据与设计指引，促进该领域的理论研究与实际系统设计。

Abstract: Optical wireless integrated sensing and communication (OW-ISAC) is rapidly
burgeoning as a complement and augmentation to its radio-frequency counterpart.
In this paper, the channel capacity is analyzed to guide the design of a
coherent OW-ISAC system based on frequency-modulated continuous wave (FMCW).
Firstly, the system model of FMCW-based OW-ISAC is recast into an
information-theoretic formulation, where an additional harmonic-mean constraint
is imposed to ensure the sensing performance. Subsequently, both lower and
upper bounds for channel capacity are derived under the imposed sensing
constraint, based on which asymptotic expressions for channel capacity are
presented for both low and high signal-to-noise-ratio regions. Moreover, the
analysis of channel capacity provides guidance for the envelope design based on
pulse amplitude modulation, whose capacity-achieving capabilities are
demonstrated by numerical results. Furthermore, simulations reveal the
trade-off between communication and sensing functionalities. In summary, the
analysis of channel capacity under the sensing constraint provides insights
into both the optimality and the practicality of OW-ISAC design.

</details>


### [175] [Delay-Doppler Pulse Shaping in Zak-OTFS Using Hermite Basis Functions](https://arxiv.org/abs/2510.17466)
*Fathima Jesbin,Ananthanarayanan Chockalingam*

Main category: cs.IT

TL;DR: 提出一种基于 Hermite 基函数的 Zak-OTFS DD 脉冲设计框架，将脉冲表示为 Hermite 基函数的线性组合，并通过 SVD 求解系数以最小化 DD 采样点的ISI能量；推导出该类脉冲的 I/O 关系与噪声协方差的闭式表达。在 Vehicular-A 场景下的仿真表明，该脉冲在误码率方面优于经典的 sinc 和 Gaussian 脉冲，且与 GS 脉冲相当，提供更灵活的 ISI 与旁瓣能量控制。


<details>
  <summary>Details</summary>
Motivation: 在 Zak-OTFS 中，由于 Balian-Low 定理，对时频局部化与正交性存在不可避免的权衡。为了在不扩大时频带的前提下实现可靠数据检测与 I/O 关系估计，需要在 DD 域对脉冲进行系统化设计，权衡输入等输出的鲁棒性与谱效率。现有的 sinc/高斯脉冲各自代表该权衡的极端，而 GS 脉冲则提供更好折衷，但缺乏一个系统化的设计框架来优化特定通道及估计需求。

Method: 将脉冲表示为 Hermite 基函数的线性组合；通过对 DD 采样点的ISI 能量进行带约束的最小化求解脉冲系数，方法为通过奇异值分解（SVD）解决的优化问题；对于所提出的 Hermite 脉冲族，推导出 I/O 关系和噪声协方差的闭式表达。

Result: 优化的 Hermite 脉冲在仿真中显著优于 sinc 和 Gaussian 脉冲，在带嵌入 Pilot 与模型无关 I/O 关系估计的 Vehicular-A 通道及分数 DD 下，与 GS 脉冲性能相近，验证了框架的设计灵活性与有效性。

Conclusion: 提出的系统化 DD 脉冲设计框架为 Zak-OTFS 提供了更丰富的设计自由度以控制 ISI 与旁瓣能量，且在实际通道仿真中得到验证，具有较强的应用潜力与扩展性。

Abstract: The performance of Zak-OTFS modulation is critically dependent on the choice
of the delay-Doppler (DD) domain pulse shaping filter. The design of pulses for
$L^2(\mathbb{R})$ is constrained by the Balian-Low Theorem, which imposes an
inescapable trade-off between time-frequency localization and orthogonality for
spectrally efficient systems. In Zak-OTFS, this trade-off requires balancing
the need for localization for input/output (I/O) relation estimation with the
need for orthogonality for reliable data detection when operating without time
or bandwidth expansion. The well-known sinc and Gaussian pulse shapes represent
the canonical extremes of this trade-off, while composite constructions such as
the Gaussian-sinc (GS) pulse shape offer a good compromise. In this work, we
propose a systematic DD pulse design framework for Zak-OTFS that expresses the
pulse as a linear combination of Hermite basis functions. We obtain the optimal
coefficients for the Hermite basis functions that minimize the inter-symbol
interference (ISI) energy at the DD sampling points by solving a constrained
optimization problem via singular value decomposition. For the proposed class
of Hermite pulses, we derive closed-form expressions for the I/O relation and
noise covariance in Zak-OTFS. Simulation results of Zak-OTFS with embedded
pilot and model-free I/O relation estimation in Vehicular-A channels with
fractional DDs demonstrate that the optimized pulse shape achieves a bit error
rate performance that is significantly superior compared to those of the
canonical sinc and Gaussian pulses and is on par with that of the
state-of-the-art GS pulse, validating the proposed framework which provides
greater design flexibility in terms of control of ISI and sidelobe energies.

</details>


### [176] [Multihead Finite-State Compression](https://arxiv.org/abs/2510.17544)
*Neil Lutz*

Main category: cs.IT

TL;DR: 提出一种多头有限状态压缩模型，研究一个无限序列在给定读头数 h 下的最小压缩比，并证明其等价于该序列的 h-头有限状态预测维度；对所有 h 的极小值构成多头有限状态维度。


<details>
  <summary>Details</summary>
Motivation: 在现有的多头有限状态维度研究基础上，给出一种压缩视角，将信息论中的信息损失可逆压缩与预测维度联系起来，推广有限状态压缩以度量序列的复杂性。

Method: 定义一个由恒定数量的有限状态读头在序列上前向无观移动、以有限状态规则输出的压缩器；证明对任意序列与任意正整数 h，h-头信息损失无损压缩比的下确界等于该序列的 h-头有限状态预测维度；由此推出对所有 h 的下确界等于多头有限状态维度。

Result: 主定理：任意序列和任意正整数 h 下，h-头有限状态信息损失无损压缩器的压缩比下确界等于该序列的 h-头有限状态预测维度。直接推论：将所有 h 的下确界取极小值或极限，得到该序列的多头有限状态维度。

Conclusion: 为通过信息损失可逆压缩的角度表征多头有限状态维度提供了新的理论框架，统一了序列复杂性量度的压缩与预测维度之间的关系，并扩展了有限状态压缩在多头维度研究中的应用。

Abstract: This paper develops multihead finite-state compression, a generalization of
finite-state compression, complementary to the multihead finite-state
dimensions of Huang, Li, Lutz, and Lutz (2025). In this model, an infinite
sequence of symbols is compressed by a compressor that produces outputs
according to finite-state rules, based on the symbols read by a constant number
of finite-state read heads moving forward obliviously through the sequence. The
main theorem of this work establishes that for every sequence and every
positive integer $h$, the infimum of the compression ratios achieved by
$h$-head finite-state information-lossless compressors equals the $h$-head
finite-state predimension of the sequence. As an immediate corollary, the
infimum of these ratios over all $h$ is the multihead finite-state dimension of
the sequence.

</details>


### [177] [On the Capacity of Erasure-prone Quantum Storage with Erasure-prone Entanglement Assistance](https://arxiv.org/abs/2510.17781)
*Hua Sun,Syed A. Jafar*

Main category: cs.IT

TL;DR: 研究了带有 entanglement-assisted EA 节点的分布式量子存储的容量极限，给出在大多数参数下的精确容量表达；仅在一个中间 λ_B 区间未解决；并建立与经典存储（共享随机性）问题的类比，且已定容量在相应情形下相同。


<details>
  <summary>Details</summary>
Motivation: 理解在带有预共享纠缠的分布式量子存储中，如何在任意 K 个存储节点和任意 K_B 个 EA 节点被擦除时仍能恢复量子信息，以及容量界与经典编码之间的联系。

Method: 建立 erasure-prone 存储与 EA 节点的数学模型，推导容量界，提出将经典带共享随机性的存储问题作为对照，通过信息论界证和编码构造实现上下界，展示两种设置在容量已定情况下等价。

Result: 在大多数情况给出精确容量表达；仅在 λ_B 的一个中间区间存在未解决的开问题；提出了经典对照问题及其约束，使得经典线性码可转化为量子存储码；容量在已定情形下 quantum 与 classical 一致。

Conclusion: 该工作在可解区域给出明确容量，并揭示经典与量子存储之间的深层联系，提供编码设计的方向；但中间区域的开问题需进一步研究，显示线性码的转化性和对比性对量子存储的作用。

Abstract: A quantum message is encoded into $N$ storage nodes (quantum systems
$Q_1\dots Q_N$) with assistance from $N_B$ maximally entangled bi-partite
quantum systems $A_1B_1, \dots, A_{N_B}B_{N_B}$, that are prepared in advance
such that $B_1\dots B_{N_B}$ are stored separately as entanglement assistance
(EA) nodes, while $A_1\dots A_{N_B}$ are made available to the encoder. Both
the storage nodes and EA nodes are erasure-prone. The quantum message must be
recoverable given any $K$ of the $N$ storage nodes along with any $K_B$ of the
$N_B$ EA nodes. The capacity for this setting is the maximum size of the
quantum message, given that the size of each EA node is $\lambda_B$. All node
sizes are relative to the size of a storage node, which is normalized to unity.
The exact capacity is characterized as a function of $N,K,N_B,K_B, \lambda_B$
in all cases, with one exception. The capacity remains open for an intermediate
range of $\lambda_B$ values when a strict majority of the $N$ storage nodes,
and a strict non-zero minority of the $N_B$ EA nodes, are erased. As a key
stepping stone, an analogous classical storage (with shared-randomness
assistance) problem is introduced. A set of constraints is identified for the
classical problem, such that classical linear code constructions translate to
quantum storage codes, and the converse bounds for the two settings utilize
similar insights. In particular, the capacity characterizations for the
classical and quantum settings are shown to be identical in all cases where the
capacity is settled.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [178] [Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts](https://arxiv.org/abs/2510.15973)
*Tiarnaigh Downey-Webb,Olamide Jogunola,Oluwaseun Ajao*

Main category: cs.CR

TL;DR: 对四大主流大型语言模型的系统性安全评估，聚焦四类对抗向量，使用 SALAD-Bench 的 1,200 条提示，覆盖六类伤害。结果显示 Llama-2 的防御最优，Phi-2 最易受攻击，存在跨模型传递性攻击；对恶意用途提示的攻击成功率最高。


<details>
  <summary>Details</summary>
Motivation: 理解不同LLMs在对抗性攻击下的鲁棒性差异，为设计更有效的防御机制提供数据支撑。

Method: 对 Phi-2、Llama-2-7B-Chat、GPT-3.5-Turbo、GPT-4 等模型，在人类撰写 prompts、AutoDAN、Greedy Coordinate Gradient（GCG）、Tree-of-Attacks-with-pruning（TAP）四类攻击下进行评估；使用 SALAD-Bench 数据集的 1,200 条提示，覆盖六类 harm；统计分析使用 Friedman 检验等非参数方法。

Result: 平均攻击成功率：Llama-2 3.4%，Phi-2 7.0%；跨模型传递性攻击在对非目标模型上更具效力，GPT-4 的传递性成功率最高可达 17%；六类 harm 的差异显著（p<0.001），恶意用途 prompts 的攻击成功率最高，约 10.71%。

Conclusion: 本研究有助于揭示跨模型的安全差异与传递性模式，为制定针对性防御策略与提升LLM安全性提供实证依据。

Abstract: This paper presents a systematic security assessment of four prominent Large
Language Models (LLMs) against diverse adversarial attack vectors. We evaluate
Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack
categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),
and Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs
1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six
harm categories. Results demonstrate significant variations in model
robustness, with Llama-2 achieving the highest overall security (3.4% average
attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%
average attack success rate). We identify critical transferability patterns
where GCG and TAP attacks, though ineffective against their target model
(Llama-2), achieve substantially higher success rates when transferred to other
models (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals
significant differences in vulnerability across harm categories ($p < 0.001$),
with malicious use prompts showing the highest attack success rates (10.71%
average). Our findings contribute to understanding cross-model security
vulnerabilities and provide actionable insights for developing targeted defense
mechanisms

</details>


### [179] [A Multi-Cloud Framework for Zero-Trust Workload Authentication](https://arxiv.org/abs/2510.16067)
*Saurabh Deochake,Ryan Murphy,Jeremiah Gearheart*

Main category: cs.CR

TL;DR: 提出一种跨多云的秘密最小化身份框架，基于工作负载身份联合（WIF）和 OpenID Connect（OIDC），通过可证虚的短暂令牌实现秘密化认证，在企业级 Kubernetes 环境中验证，显著降低攻击面，并为跨云统一管理工作负载身份和未来基于属性的访问控制奠定基础。


<details>
  <summary>Details</summary>
Motivation: 长期存在的静态凭据带来的安全风险，与零信任原则不符；需要跨云环境的工作负载身份统一管理和无私钥的认证机制。

Method: 结合 WIF 与 OIDC，发行加密验证的短暂令牌，避免持久私钥；在企业级 Kubernetes 环境中进行验证与评估，提出跨云统一的身份管理框架，并探讨未来引入基于属性的访问控制。

Result: 在企业规模的 Kubernetes 环境中验证，显著降低攻击面，并证明框架在跨云环境中的可行性和实用性。

Conclusion: 提供一种跨云工作负载身份的统一解决方案，为未来实施基于属性的访问控制铺平道路，进一步强化零信任的实现。

Abstract: Static, long-lived credentials for workload authentication create untenable
security risks that violate Zero-Trust principles. This paper presents a
multi-cloud framework using Workload Identity Federation (WIF) and OpenID
Connect (OIDC) for secretless authentication. Our approach uses
cryptographically-verified, ephemeral tokens, allowing workloads to
authenticate without persistent private keys and mitigating credential theft.
We validate this framework in an enterprise-scale Kubernetes environment, which
significantly reduces the attack surface. The model offers a unified solution
to manage workload identities across disparate clouds, enabling future
implementation of robust, attribute-based access control.

</details>


### [180] [Meta-Guardian: An Early Evaluation of an On-device Application to Mitigate Psychography Data Leakage in Immersive Technologies](https://arxiv.org/abs/2510.15989)
*Keshav Sood,Sanjay Selvaraj,Youyang Qu*

Main category: cs.CR

TL;DR: 提出一个在VR头显内实时过滤生物识别数据的隐私保护系统 Meta-Guardian，基于 Unity SDK，利用机器学习对信号进行分类并阻止敏感数据传输/存储，便于在多平台集成。


<details>
  <summary>Details</summary>
Motivation: XR 领域对生物数据的实时收集带来隐私风险；现有研究多聚焦技术层面，忽视实时数据过滤的难点，需设计隐私设计原则的实现方案。

Method: 提出模块化 Unity SDK，使用机器学习模型对生物信号进行分类，设计过滤机制在传输/存储前屏蔽敏感数据，兼容主流平台。

Result: 原型实现 Meta-Guardian；在头戴设备内实时过滤生物信号，降低隐私风险；提供一个可嵌入式隐私保护框架的示例。

Conclusion: 该框架帮助开发者将隐私保护设计原则融入沉浸式体验，促进跨平台应用的隐私合规性。

Abstract: The use of Immersive Technologies has shown its potential to revolutionize
many sectors such as health, entertainment, education, and industrial sectors.
Immersive technologies such as Virtual Reality (VR), Augmented reality (AR),
and Mixed Reality (MR) have redefined user interaction through real-time
biometric and behavioral tracking. Although Immersive Technologies (XR)
essentially need the collection of the biometric data which acts as a baseline
to create immersive experience, however, this ongoing feedback information
(includes biometrics) poses critical privacy concerns due to the sensitive
nature of the data collected. A comprehensive review of recent literature
explored the technical dimensions of related problem; however, they largely
overlook the challenge particularly the intricacies of real-time biometric data
filtering within head-mounted display system. Motivated from this, in this
work, we propose a novel privacy-preserving system architecture that identifies
and filters biometric signals (within the VR headset) in real-time before
transmission or storage. Implemented as a modular Unity Software-development
Kit (SDK) compatible with major immersive platforms, our solution (named
Meta-Guardian) employs machine learning models for signal classification and a
filtering mechanism to block sensitive data. This framework aims to enable
developers to embed privacy-by-design principles into immersive experiences on
various headsets and applications.

</details>


### [181] [Heimdallr: Fingerprinting SD-WAN Control-Plane Architecture via Encrypted Control Traffic](https://arxiv.org/abs/2510.16461)
*Minjae Seo,Jaehan Kim,Eduard Marin,Myoungsung You,Taejune Park,Seungsoo Lee,Seungwon Shin,Jinwoo Kim*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Software-defined wide area network (SD-WAN) has emerged as a new paradigm for
steering a large-scale network flexibly by adopting distributed
software-defined network (SDN) controllers. The key to building a logically
centralized but physically distributed control-plane is running diverse cluster
management protocols to achieve consistency through an exchange of control
traffic. Meanwhile, we observe that the control traffic exposes unique
time-series patterns and directional relationships due to the operational
structure even though the traffic is encrypted, and this pattern can disclose
confidential information such as control-plane topology and protocol
dependencies, which can be exploited for severe attacks. With this insight, we
propose a new SD-WAN fingerprinting system, called Heimdallr. It analyzes
periodical and operational patterns of SD-WAN cluster management protocols and
the context of flow directions from the collected control traffic utilizing a
deep learning-based approach, so that it can classify the cluster management
protocols automatically from miscellaneous control traffic datasets. Our
evaluation, which is performed in a realistic SD-WAN environment consisting of
geographically distant three campus networks and one enterprise network shows
that Heimdallr can classify SD-WAN control traffic with $\geq$ 93%, identify
individual protocols with $\geq$ 80% macro F-1 scores, and finally can infer
control-plane topology with $\geq$ 70% similarity.

</details>


### [182] [MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents](https://arxiv.org/abs/2510.15994)
*Dongsen Zhang,Zekun Li,Xu Luo,Xuannan Liu,Peipei Li,Wenjun Xu*

Main category: cs.CR

TL;DR: MSB (MCP Security Benchmark) provides an end-to-end security benchmark for MCP-based LLM agents, introducing a taxonomy of 12 attacks, a real-tool execution harness, and the Net Resilient Performance (NRP) metric; evaluated 9 agents across 10 domains with 400+ tools and 2,000 attack instances; reveals security-performance trade-offs and offers a practical baseline for hardening MCP agents.


<details>
  <summary>Details</summary>
Motivation: Standardizing tool discovery, description, and invocation via MCP enlarges the attack surface by treating tools as first-class citizens with natural-language metadata and standardized I/O, necessitating an end-to-end security evaluation framework.

Method: Develop a 12-attack taxonomy; implement an evaluation harness that executes attacks against real tools through MCP (benign and malicious tools); introduce Net Resilient Performance (NRP) as a robustness metric; evaluate nine popular LLM agents across 10 domains and 400+ tools, generating about 2,000 attack instances.

Result: Attacks are effective across different stages of MCP; models with stronger tool-calling and instruction-following capabilities tend to be more vulnerable due to their higher performance.

Conclusion: MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents, enabling systematic security improvements in MCP-based tool usage.

Abstract: The Model Context Protocol (MCP) standardizes how large language model (LLM)
agents discover, describe, and call external tools. While MCP unlocks broad
interoperability, it also enlarges the attack surface by making tools
first-class, composable objects with natural-language metadata, and
standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end
evaluation suite that systematically measures how well LLM agents resist
MCP-specific attacks throughout the full tool-use pipeline: task planning, tool
invocation, and response handling. MSB contributes: (1) a taxonomy of 12
attacks including name-collision, preference manipulation, prompt injections
embedded in tool descriptions, out-of-scope parameter requests,
user-impersonating responses, false-error escalation, tool-transfer, retrieval
injection, and mixed attacks; (2) an evaluation harness that executes attacks
by running real tools (both benign and malicious) via MCP rather than
simulation; and (3) a robustness metric that quantifies the trade-off between
security and performance: Net Resilient Performance (NRP). We evaluate nine
popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack
instances. Results reveal the effectiveness of attacks against each stage of
MCP. Models with stronger performance are more vulnerable to attacks due to
their outstanding tool calling and instruction following capabilities. MSB
provides a practical baseline for researchers and practitioners to study,
compare, and harden MCP agents.

</details>


### [183] [Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers](https://arxiv.org/abs/2510.16005)
*Giacomo Bertollo,Naz Bodemir,Jonah Burgess*

Main category: cs.CR

TL;DR: CTF参与者能轻易绕过简单的AI护栏，而分层的多步骤防御仍具挑战性，为构建更安全的AI系统提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 检验用户绕过AI护栏的能力及分层防御的有效性，以改进AI系统的安全设计。

Method: 对500名CTF参与者进行分析，评估他们在绕过护栏方面的技术，测试分层防御的抵抗力。

Result: 发现简单护栏易被绕过；分层、多步防御在某些情况下仍能提供有效防护，并给出实现安全系统的具体洞见。

Conclusion: 应采用更强的分层防御策略，并将其嵌入到AI系统设计中，以提升对抗绕过的鲁棒性。

Abstract: Analyzing 500 CTF participants, this paper shows that while participants
readily bypassed simple AI guardrails using common techniques, layered
multi-step defenses still posed significant challenges, offering concrete
insights for building safer AI systems.

</details>


### [184] [On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation](https://arxiv.org/abs/2510.16024)
*Abdulrahman Alhaidari,Balaji Palanisamy,Prashant Krishnamurthy*

Main category: cs.CR

TL;DR: 提出首个完全去中心化、全链上学习框架，用于在 DeFi 场景中检测与防御交易所暴露的业务逻辑与会计漏洞。框架在 Layer-2 进行高成本训练推理、并将更新向 Layer-1 传播，同时在智能合约内实现 gas 上限内的低延迟推理；通过 PoIm 协议对更新进行自证验证。


<details>
  <summary>Details</summary>
Motivation: 现有防御手段无法阻止通过私有中继或在同一区块内执行的恶意合约所造成的攻击，需要一个端到端、可验证且在链上运行的学习与推理框架来提高对 DeFi 漏洞的检测能力。

Method: 提出去中心化的 PoIm（Proof-of-Improvement）训练协议，只有在新提案在公开基准上至少在一个核心指标上有所提升且不降低其他核心指标时才被接受；对抗性提案通过可扩展的测试集进行惩罚。实现了在 Layer-2 进行 gas 代价高昂的训练/推理、在 Layer-1 广播经验证的更新、以及在以太坊区块 gas 限制内的离散化/循环展开等量化优化，使对逻辑回归、SVM、MLP、CNN、门控 RNN 以及形式化验证后决策树的推理具备 bit-exact 精度并可在链上执行；并通过 Z3 做形式化证明。还收集并整理了 298 个真实漏洞案例（2020-2025），覆盖 8 条 EVM 链。

Result: 首次实现了去中心化、全链上的学习框架，并实现了在以太坊的区块 gas 限制内的可比对端的推理，且更新仅在带来改进的情况下被接受；对抗性提案可通过财政惩罚和演化威胁测试集进行约束；通过量化与循环展开等技术实现对多种模型的链上推理，且与离线实现等效，理论上在形式上可验证（Z3）。数据集包含 298 个真实漏洞、402 笔交易，横跨 8 条 EVM 链，总损失约 37.4 亿美元。

Conclusion: 此研究证明在区块链上进行可验证的机器学习防御是可行且有现实潜力的路线，为 DeFi 的实时、低延迟、全链上安全分析提供了新路径，并通过自证更新和对抗性威胁演化机制提升了系统鲁棒性。

Abstract: Billions of dollars are lost every year in DeFi platforms by transactions
exploiting business logic or accounting vulnerabilities. Existing defenses
focus on static code analysis, public mempool screening, attacker contract
detection, or trusted off-chain monitors, none of which prevents exploits
submitted through private relays or malicious contracts that execute within the
same block. We present the first decentralized, fully on-chain learning
framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce
cost, (ii) propagates verified model updates to Layer-1, and (iii) enables
gas-bounded, low-latency inference inside smart contracts. A novel
Proof-of-Improvement (PoIm) protocol governs the training process and verifies
each decentralized micro update as a self-verifying training transaction.
Updates are accepted by \textit{PoIm} only if they demonstrably improve at
least one core metric (e.g., accuracy, F1-score, precision, or recall) on a
public benchmark without degrading any of the other core metrics, while
adversarial proposals get financially penalized through an adaptable test set
for evolving threats. We develop quantization and loop-unrolling techniques
that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs
(with support for formally verified decision tree inference) within the
Ethereum block gas limit, while remaining bit-exact to their off-chain
counterparts, formally proven in Z3. We curate 298 unique real-world exploits
(2020 - 2025) with 402 exploit transactions across eight EVM chains,
collectively responsible for \$3.74 B in losses.

</details>


### [185] [Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks](https://arxiv.org/abs/2510.16028)
*Jianzhu Yao,Hongxu Su,Taobo Liao,Zerui Cheng,Huan Zhang,Xuechao Wang,Pramod Viswanath*

Main category: cs.CR

TL;DR: NAO提出一种对异构硬件上FP非确定性输出的可验证性框架，通过在算子级设定公认范围和基于Merkle的争议机制，在不信任硬件的情况下实现对MLaaS输出的可验证性。


<details>
  <summary>Details</summary>
Motivation: 在 ML-as-a-Service 场景，用户无法得知实际执行的算子实现、对输出的真实映射，以及是否被降级或篡改。FP执行的非确定性使逐位一致性不可行，需要新的可验证机制且尽量不依赖厂商信任。

Method: 将两类误差模型结合：一是按算子IEEE-754的最坏情况界限，二是跨硬件的紧密经验分位数曲线。对若干算子输出结果进行比较；发现不一致时，引入Merkl-锚定、门槛引导的争议游戏，递归分割计算图，最终降到单一算子进行轻量理论界限检验或少数诚实多数的对经验阈值投票。实现为 PyTorch 兼容运行时和以太坊 Holesky 测试网的合约层；对图计算进行打标和逐算子检测，运行厂商内核以 FP32 进行，开销约 0.3%，在 Qwen3-8B 上。

Result: 在CNN、Transformer、扩散模型等上，经验阈值相比理论界限紧度达到 10^2-10^3 倍；对bound-aware 的对抗攻击达到0% 成功率；在多种硬件（A100、H100、RTX6000、RTX4090）和不同模型下，表现稳定且可扩展。

Conclusion: NAO 平衡了可扩展性与可验证性，适用于真实世界的异构 ML 计算，不依赖可信硬件或确定性内核，提供可行的非确定性容忍的验证方案。

Abstract: Neural networks increasingly run on hardware outside the user's control
(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about
what actually ran or whether returned outputs faithfully reflect the intended
inputs. Users lack recourse against service downgrades (model swaps,
quantization, graph rewrites, or discrepancies like altered ad embeddings).
Verifying outputs is hard because floating-point(FP) execution on heterogeneous
accelerators is inherently nondeterministic. Existing approaches are either
impractical for real FP neural networks or reintroduce vendor trust. We present
NAO: a Nondeterministic tolerance Aware Optimistic verification protocol that
accepts outputs within principled operator-level acceptance regions rather than
requiring bitwise equality. NAO combines two error models: (i) sound
per-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile
profiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,
threshold-guided dispute game that recursively partitions the computation graph
until one operator remains, where adjudication reduces to a lightweight
theoretical-bound check or a small honest-majority vote against empirical
thresholds. Unchallenged results finalize after a challenge window, without
requiring trusted hardware or deterministic kernels. We implement NAO as a
PyTorch-compatible runtime and a contract layer currently deployed on Ethereum
Holesky testnet. The runtime instruments graphs, computes per-operator bounds,
and runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on
Qwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,
RTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than
theoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO
reconciles scalability with verifiability for real-world heterogeneous ML
compute.

</details>


### [186] [Membership Inference over Diffusion-models-based Synthetic Tabular Data](https://arxiv.org/abs/2510.16037)
*Peini Cheng,Amir Bahmani*

Main category: cs.CR

TL;DR: 扩散模型生成的表格数据在成员信息推断攻击下存在隐私风险，实验表明 TabDDPM 相较于 TabSyn 更易被攻击。


<details>
  <summary>Details</summary>
Motivation: 解决 diffusion-based 合成表格数据的隐私风险，评估现有模型在未知攻击下的脆弱性，并推动开发更鲁棒的隐私保护机制。

Method: 基于逐步误差比较的查询型成员信息推断攻击，针对 TabDDPM 与 TabSyn 进行攻击实验。对不同设置下的攻击成功率与漏洞进行评估。

Result: 实验结果显示 TabDDPM 更易受攻击，TabSyn 对本文提出的攻击模型表现出一定鲁棒性。

Conclusion: 提示需要在生成式数据隐私评估与保护方面加强研究，推动开发能对抗 diffusion-based 合成数据的鲁棒隐私保护机制。

Abstract: This study investigates the privacy risks associated with diffusion-based
synthetic tabular data generation methods, focusing on their susceptibility to
Membership Inference Attacks (MIAs). We examine two recent models, TabDDPM and
TabSyn, by developing query-based MIAs based on the step-wise error comparison
method. Our findings reveal that TabDDPM is more vulnerable to these attacks.
TabSyn exhibits resilience against our attack models. Our work underscores the
importance of evaluating the privacy implications of diffusion models and
encourages further research into robust privacy-preserving mechanisms for
synthetic data generation.

</details>


### [187] [A Novel GPT-Based Framework for Anomaly Detection in System Logs](https://arxiv.org/abs/2510.16044)
*Zeng Zhang,Wenjie Yin,Xiaoqi Li*

Main category: cs.CR

TL;DR: 将系统日志的异常检测问题转化为基于生成式预训练变换器（GPT）的框架，结合结构化输入与 Focal Loss 来应对数据不平衡，结果表明优化后的 GPT-2 在多项指标上优于未优化模型，且在某些任务上媲美或超越 GPT-3.5 API。


<details>
  <summary>Details</summary>
Motivation: 面临海量日志数据、异常分布不均、以及传统方法在精度和鲁棒性方面的局限性，需要一种高效的、可扩展的日志异常检测方案。

Method: 将原始日志通过 Drain 解析器转化为事件ID序列，使用基于 GPT 的模型进行建模；通过 Focal Loss 处理类别不平衡问题；对比优化前后的 GPT-2 与 GPT-3.5 API 的性能表现。

Result: 优化后的 GPT-2 在精度、召回率和 F1 分数等关键指标上显著优于未优化的 GPT-2；在某些任务中，性能可与 GPT-3.5 API 相当或更优。

Conclusion: 基于 GPT 的日志异常检测结合结构化输入和 Focal Loss 显著提升检测性能，具有与强大语言模型基线相竞争的潜力，适合大规模日志分析的安全应用。

Abstract: Identification of anomalous events within system logs constitutes a pivotal
element within the frame- work of cybersecurity defense strategies. However,
this process faces numerous challenges, including the management of substantial
data volumes, the distribution of anomalies, and the precision of con-
ventional methods. To address this issue, the present paper puts forward a
proposal for an intelligent detection method for system logs based on Genera-
tive Pre-trained Transformers (GPT). The efficacy of this approach is
attributable to a combination of structured input design and a Focal Loss op-
timization strategy, which collectively result in a substantial enhancement of
the performance of log anomaly detection. The initial approach involves the
conversion of raw logs into event ID sequences through the use of the Drain
parser. Subsequently, the Focal Loss loss function is employed to address the
issue of class imbalance. The experimental re- sults demonstrate that the
optimized GPT-2 model significantly outperforms the unoptimized model in a
range of key metrics, including precision, recall, and F1 score. In specific
tasks, comparable or superior performance has been demonstrated to that of the
GPT-3.5 API.

</details>


### [188] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: 提出一个对人脸验证的实用 match-on-card 方案，生成 64/128 位紧凑模板，卡上用常数时间汉明距离比对，使用固定长度载荷的 ISO/IEC APDUs，隐私友好且与 ISO/IEC 24745 兼容。


<details>
  <summary>Details</summary>
Motivation: 在高安全性与低延迟的身份验证场景中，需要在受限的智能卡上进行高效且不泄露分数的比对，同时保证互操作性与隐私目标。

Method: 利用 PCA-ITQ 产生紧凑二进制模板；卡上进行常数时间汉明距离比对；通过固定长度载荷的命令 APDU 与最小化的按身份逐条 EEPROM 映射；基于 CelebA 子集（55身份、412图）设定阈值、进行重放 enroll->verify，并估算端到端时延；可选 +6B 符号级校验以覆盖经验性不稳定位，影响极小。

Result: 给出在慢速与快速通道下的端到端时延：9.6 kbps 时 64b/128b 分别为 43.9 ms 与 52.3 ms；38.4 kbps 时两者均小于 14 ms；FAR=1% 时 128b 的 TPR 更高且 EER 较 64b 较低；+6B 的帮助在延迟上可忽略。

Conclusion: 短模板、固定载荷的决策式 APDU 与常数时间匹配能够满足 ISO/IEC 传输约束并与隐私目标对齐；局限性在于单数据集评估与“前硬件”时序；未来工作包括 AgeDB/CFP-FP 与卡上微基准测试的展开。

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [189] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: Generative classifiers that model the joint likelihood P(X,Y) are more vulnerable to Membership Inference Attacks (MIAs) than discriminative models, and the standard inference in generative models amplifies this risk; the study shows a consistent privacy-utility trade-off across multiple datasets and data volumes, calling for privacy-preserving generative methods.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematic comparison between generative, discriminative, and pseudo-generative classifiers in the context of MIAs. The authors aim to theoretically justify and empirically validate why generative classifiers are more susceptible, highlighting a potential inherent privacy risk in modeling P(X,Y).

Method: The paper analyzes three classifier families (discriminative, generative, pseudo-generative) across varying training data volumes and nine benchmark text datasets. It employs diverse MIA strategies and compares canonical inference versus alternative inference in generative models to assess leakage risk.

Result: Across all settings, fully generative classifiers that explicitly model P(X,Y) show the highest susceptibility to membership leakage. The canonical inference approach commonly used in generative models further amplifies privacy risk. The findings reveal a consistent utility-privacy trade-off, emphasizing the caution needed when deploying generative classifiers in privacy-sensitive contexts.

Conclusion: Researchers should be cautious about using fully generative models for privacy-sensitive tasks; there is a need to develop privacy-preserving generative classifiers that maintain utility while mitigating MIAs. The work motivates further theoretical and empirical exploration into defenses and model designs that reduce membership leakage.

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [190] [Prompt injections as a tool for preserving identity in GAI image descriptions](https://arxiv.org/abs/2510.16128)
*Kate Glazko,Jennifer Mankoff*

Main category: cs.CR

TL;DR: 提出将提示注入作为内容拥有者自助工具，以抵抗生成式AI对间接用户的偏见与缺乏代表性的问题，并通过一个案例研究演示在AI描述图像时保留图像拥有者的性别与残障身份。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI相关的偏见与缺乏代表性对间接用户的影响，强调需要超越自上而下的外部干预，提供从内容所有者内部进行抵抗的可行路径。

Method: 将提示注入作为一种工具化策略，供图像/内容所有者在其内容中植入对抗性提示，以抵消AI描述中的偏见或身份信息丢失。通过一个案例研究展示在描述图像时保持图像拥有者的性别与残障身份的可行性。

Result: 展示一个案例研究，证明通过提示注入可以在AI对图像描述时保留并传达用户的身份属性（如性别、残障身份）

Conclusion: 提示注入是一种有潜力的底层工具，可作为保护间接用户的多样性与代表性的补充手段，但需结合伦理审查与对潜在滥用的警惕。

Abstract: Generative AI risks such as bias and lack of representation impact people who
do not interact directly with GAI systems, but whose content does: indirect
users. Several approaches to mitigating harms to indirect users have been
described, but most require top down or external intervention. An emerging
strategy, prompt injections, provides an empowering alternative: indirect users
can mitigate harm against them, from within their own content. Our approach
proposes prompt injections not as a malicious attack vector, but as a tool for
content/image owner resistance. In this poster, we demonstrate one case study
of prompt injections for empowering an indirect user, by retaining an image
owner's gender and disabled identity when an image is described by GAI.

</details>


### [191] [C/N0 Analysis-Based GPS Spoofing Detection with Variable Antenna Orientations](https://arxiv.org/abs/2510.16229)
*Vienna Li,Justin Villa,Dan Diessner,Jayson Clifford,Laxima Niure Kandel*

Main category: cs.CR

TL;DR: 通过分析卫星C/N0在不同天线姿态下的变化，提出一种基于姿态变化的GPS欺骗检测概念验证策略；在非欺骗环境中C/N0随几何因素自然波动，而欺骗环境中平面朝向欺骗源时C/N0最高，银行姿态则因与欺骗源对准不足而降低。


<details>
  <summary>Details</summary>
Motivation: GPS欺骗对航空安全构成日益严重的威胁，现有导航系统易受干扰。本研究提出通过观测C/N0随天线姿态变化来实现早期欺骗检测的思路，具有低成本、可在现有接收机上实现的潜力。

Method: 使用 u-blox EVK-M8U 接收机与 GPSG-1000 仿真器，在三种天线姿态（平置、向右倾斜、向左倾斜）下采集C/N0数据，比较真实天空（非欺骗）与欺骗环境的表现。

Result: 在非欺骗信号下，C/N0 随姿态产生自然波动，体现几何依赖；在欺骗信号下，平置且朝向欺骗源的C/N0最高，向两侧倾斜的姿态因与欺骗源不对准而C/N0 降低，形成可辨别的模式。

Conclusion: 通过简短的机动/姿态变化（如短时银行动作）即可对GPS欺骗进行初步预警，适用于通用航空和无人机系统。

Abstract: GPS spoofing poses a growing threat to aviation by falsifying satellite
signals and misleading aircraft navigation systems. This paper demonstrates a
proof-of-concept spoofing detection strategy based on analyzing satellite
Carrier-to-Noise Density Ratio (C/N$_0$) variation during controlled static
antenna orientations. Using a u-blox EVK-M8U receiver and a GPSG-1000 satellite
simulator, C/N$_0$ data is collected under three antenna orientations flat,
banked right, and banked left) in both real-sky (non-spoofed) and spoofed
environments. Our findings reveal that under non-spoofed signals, C/N$_0$
values fluctuate naturally with orientation, reflecting true geometric
dependencies. However, spoofed signals demonstrate a distinct pattern: the flat
orientation, which directly faces the spoofing antenna, consistently yielded
the highest C/N$_0$ values, while both banked orientations showed reduced
C/N$_0$ due to misalignment with the spoofing source. These findings suggest
that simple maneuvers such as brief banking to induce C/N$_0$ variations can
provide early cues of GPS spoofing for general aviation and UAV systems.

</details>


### [192] [Efficient and Privacy-Preserving Binary Dot Product via Multi-Party Computation](https://arxiv.org/abs/2510.16331)
*Fatemeh Jafarian Dehkordi,Elahe Vedadi,Alireza Feizbakhsh,Yasaman Keshtkarjahromi,Hulya Seferoglu*

Main category: cs.CR

TL;DR: 提出 BiMPC 框架，在二进制垂直联邦学习中实现隐私保护的位运算。引入 DoMA，通过常规和模运算高效计算二进制点积；对线性运算使用高阶域的随机掩码，对非线性二进制操作采用三方隐私传输（triot）；给出隐私保证并声称在分布式环境中的效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在分布式机器学习中，如何在保护隐私的同时实现协同计算是一个关键挑战。现有的隐私保护方法（如 Shamir 密钥共享、MPC）并未针对二进制数据上的位运算以及每个参与方持有二进制向量不同分部的场景进行优化，且在位运算上效率较低。需要一种适用于二进制向量、且适合分布式参与方分区数据的高效隐私保护方法。

Method: 提出 Binary Multi-Party Computation (BiMPC) 框架，聚焦二进制向量的点积等位运算的隐私保护。核心是 Dot Product via Modular Addition (DoMA)——利用普通加法和模加法来有效地计算二进制点积。为确保隐私，BiMPC 在线性运算中使用高阶域中的随机掩码，在非线性二进制运算中采用三方盲传输协议（triot）。对 BiMPC 的隐私性做了严格分析，并对分布式环境中的效率和可扩展性给出结论。

Result: 给出对 BiMPC 框架的隐私性保证与理论分析，展示在分布式设置下的高效性与可扩展性，且在二进制点积等位运算方面比传统方法更适合。

Conclusion: BiMPC 框架为分布式场景中的位运算提供了一种高效且隐私性强的解决方案，尤其适用于二进制向量的点积等基本运算，具备良好的隐私保障与可扩展性潜力。

Abstract: Striking a balance between protecting data privacy and enabling collaborative
computation is a critical challenge for distributed machine learning. While
privacy-preserving techniques for federated learning have been extensively
developed, methods for scenarios involving bitwise operations, such as
tree-based vertical federated learning (VFL), are still underexplored.
Traditional mechanisms, including Shamir's secret sharing and multi-party
computation (MPC), are not optimized for bitwise operations over binary data,
particularly in settings where each participant holds a different part of the
binary vector. This paper addresses the limitations of existing methods by
proposing a novel binary multi-party computation (BiMPC) framework. The BiMPC
mechanism facilitates privacy-preserving bitwise operations, with a particular
focus on dot product computations of binary vectors, ensuring the privacy of
each individual bit. The core of BiMPC is a novel approach called Dot Product
via Modular Addition (DoMA), which uses regular and modular additions for
efficient binary dot product calculation. To ensure privacy, BiMPC uses random
masking in a higher field for linear computations and a three-party oblivious
transfer (triot) protocol for non-linear binary operations. The privacy
guarantees of the BiMPC framework are rigorously analyzed, demonstrating its
efficiency and scalability in distributed settings.

</details>


### [193] [EditMark: Watermarking Large Language Models based on Model Editing](https://arxiv.org/abs/2510.16367)
*Shuai Li,Kejiang Chen,Jun Jiang,Jie Zhang,Qiyi Yao,Kai Zeng,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: EditMark 是一种基于模型编辑的水印嵌入方法，通过对 LLM 进行可控的多轮稳定编辑，在不进行训练的前提下嵌入训练无关的水印，且具有隐蔽性、对性能无损失、可高效提取的特性。实现了 32 位水印在 20 秒内嵌入，提取成功率 100%，并展现了保真性、隐蔽性与对常见攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM 的商业化和版权保护需求增多，单纯依赖训练水印的方法代价高、可能损害性能且水印不自然难以隐蔽。需要一种训练-free、隐蔽且不损失性能的水印方法来应对版权追踪与防盗用。

Method: 提出 EditMark：通过模型编辑技术将每个可能的答案映射为唯一水印，并通过对 LLM 的权重进行可控编辑使其能生成对应的问题和答案。为提升水印的有效性与鲁棒性，加入自适应多轮稳定编辑策略，并注入噪声矩阵以增强鲁棒性，达到训练-free 的水印嵌入。

Result: 在实验中，EditMark 能在 20 秒内为 LLM 嵌入 32 位水印，且水印提取成功率达到 100%；外部实验表明其对保持原模型性能的保真性、隐蔽性及对常见攻击的鲁棒性具备一定程度的保障。

Conclusion: EditMark 提供了一种高效、隐蔽且对性能几乎无损的 LLM 水印嵌入方案，依赖模型编辑实现训练-free 的水印，具有较好的实际应用前景，但也需关注在不同模型、不同攻击场景下的进一步鲁棒性与可扩展性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, but
their training requires extensive data and computational resources, rendering
them valuable digital assets. Therefore, it is essential to watermark LLMs to
protect their copyright and trace unauthorized use or resale. Existing methods
for watermarking LLMs primarily rely on training LLMs with a watermarked
dataset, which entails burdensome training costs and negatively impacts the
LLM's performance. In addition, their watermarked texts are not logical or
natural, thereby reducing the stealthiness of the watermark. To address these
issues, we propose EditMark, the first watermarking method that leverages model
editing to embed a training-free, stealthy, and performance-lossless watermark
for LLMs. We observe that some questions have multiple correct answers.
Therefore, we assign each answer a unique watermark and update the weights of
LLMs to generate corresponding questions and answers through the model editing
technique. In addition, we refine the model editing technique to align with the
requirements of watermark embedding. Specifically, we introduce an adaptive
multi-round stable editing strategy, coupled with the injection of a noise
matrix, to improve both the effectiveness and robustness of the watermark
embedding. Extensive experiments indicate that EditMark can embed 32-bit
watermarks into LLMs within 20 seconds (Fine-tuning: 6875 seconds) with a
watermark extraction success rate of 100%, which demonstrates its effectiveness
and efficiency. External experiments further demonstrate that EditMark has
fidelity, stealthiness, and a certain degree of robustness against common
attacks.

</details>


### [194] [$ρ$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching](https://arxiv.org/abs/2510.16544)
*Weijie Chen,Shan Tang,Yulin Tang,Xiapu Luo,Yinqian Zhang,Weizhong Qiang*

Main category: cs.CR

TL;DR: rhoHammer为最新Intel架构提供了一个实用、高吞吐的Rowhammer框架，突破了DRAM映射、激活率和推测执行等挑战，在Comet、Rocket Lake和Raptor Lake上实现了前所未有的翻转率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于加载的Rowhammer在现代架构上效果显著下降，存在对复杂DRAM映射与推测执行行为的适应性不足，需要一个灵活且高吞吐的攻击框架来实现可重复利用的攻击。

Method: 三个核心组件：1) 通过选择性成对测量和结构化推导的方式，快速反向工程DRAM地址映射；2) 基于预取指令的并行Hammer，利用x86预取的异步特性和多-bank并行性提升激活机会；3) 针对推测执行的干扰，采用控制流混淆和最优NOP伪屏障的反推测攻击，维持预取顺序并降低开销。

Result: 在四代最新Intel架构上，达到显著的翻转提升：在2小时的模式 fuzzing中可产生多达200K+的比特翻转；相比基线的加载式Hammer，在Comet和Rocket Lake上翻转率提高约112倍；首次在Raptor Lake上复活Rowhammer攻击，达到稳定的每分钟翻转约2291次。

Conclusion: rhoHammer展示了一个具有普适性的最新架构Rowhammer框架，攻防对抗的双方都需注意新的攻击向量与防御策略，推动对DRAM和CPU安全机制的重新评估。

Abstract: Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)
that continues to pose a significant threat to various systems. However, we
find that conventional load-based attacks are becoming highly ineffective on
the most recent architectures such as Intel Alder and Raptor Lake. In this
paper, we present $\rho$Hammer, a new Rowhammer framework that systematically
overcomes three core challenges impeding attacks on these new architectures.
First, we design an efficient and generic DRAM address mapping
reverse-engineering method that uses selective pairwise measurements and
structured deduction, enabling recovery of complex mappings within seconds on
the latest memory controllers. Second, to break through the activation rate
bottleneck of load-based hammering, we introduce a novel prefetch-based
hammering paradigm that leverages the asynchronous nature of x86 prefetch
instructions and is further enhanced by multi-bank parallelism to maximize
throughput. Third, recognizing that speculative execution causes more severe
disorder issues for prefetching, which cannot be simply mitigated by memory
barriers, we develop a counter-speculation hammering technique using
control-flow obfuscation and optimized NOP-based pseudo-barriers to maintain
prefetch order with minimal overhead. Evaluations across four latest Intel
architectures demonstrate $\rho$Hammer's breakthrough effectiveness: it induces
up to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes
and has a 112x higher flip rate than the load-based hammering baselines on
Comet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on
the latest Raptor Lake architecture, where baselines completely fail, achieving
stable flip rates of 2,291/min and fast end-to-end exploitation.

</details>


### [195] [Toward Understanding Security Issues in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2510.16558)
*Xiaofan Li,Xing Gao*

Main category: cs.CR

TL;DR: 首次对模型上下文协议（MCP）生态进行系统的安全分析，揭示输出验证缺失和大量可被劫持的服务器等安全风险，提出面向主机、注册中心和用户的防御策略；分析了67057个服务器的数据集。


<details>
  <summary>Details</summary>
Motivation: MCP生态系统迅速发展，但缺乏对其架构及安全风险的系统研究，存在恶意服务器篡改、敏感数据外泄等潜在威胁。

Method: 将MCP生态分解为主机、注册中心和服务器三大核心组件，分析它们之间的信任关系；进行定性分析以理解输出的验证缺失及潜在攻击向量；收集并分析来自六个公开注册中心的67057个服务器的数据集；进行定量分析以评估被劫持的风险规模。

Result: 发现主机缺乏对LLM输出的输出验证机制，导致恶意服务器可能操纵模型行为并诱发多种安全威胁（包括敏感数据外泄）；存在广泛的漏洞使攻击者能够劫持服务器，原因在于注册中心缺乏经过审查的服务器提交流程；数据集中有大量服务器可被劫持，且风险规模显著。

Conclusion: 为 MCP 的主机、注册中心和用户提出切实可行的防御策略，并将研究结果告知受影响的主机与注册中心以进行负责任披露。

Abstract: The Model Context Protocol (MCP) is an emerging open standard that enables
AI-powered applications to interact with external tools through structured
metadata. A rapidly growing ecosystem has formed around MCP, including a wide
range of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP
registries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),
and thousands of community-contributed MCP servers. Although the MCP ecosystem
is gaining traction, there has been little systematic study of its architecture
and associated security risks. In this paper, we present the first
comprehensive security analysis of the MCP ecosystem. We decompose MCP
ecosystem into three core components: hosts, registries, and servers, and study
the interactions and trust relationships among them. Users search for servers
on registries and configure them in the host, which translates LLM-generated
output into external tool invocations provided by the servers and executes
them. Our qualitative analysis reveals that hosts lack output verification
mechanisms for LLM-generated outputs, enabling malicious servers to manipulate
model behavior and induce a variety of security threats, including but not
limited to sensitive data exfiltration. We uncover a wide range of
vulnerabilities that enable attackers to hijack servers, due to the lack of a
vetted server submission process in registries. To support our analysis, we
collect and analyze a dataset of 67,057 servers from six public registries. Our
quantitative analysis demonstrates that a substantial number of servers can be
hijacked by attackers. Finally, we propose practical defense strategies for MCP
hosts, registries, and users. We responsibly disclosed our findings to affected
hosts and registries.

</details>


### [196] [Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries](https://arxiv.org/abs/2510.16581)
*Xinfeng Li,Shengyuan Pang,Jialin Wu,Jiangyi Deng,Huanlong Zhong,Yanjiao Chen,Jie Zhang,Wenyuan Xu*

Main category: cs.CR

TL;DR: Patronus 是一种面向文本到图像（T2I）模型的防御框架，通过内部筛选模块将不安全输入转换为零向量，同时用非微调学习机制加强模型对齐，提升对白盒对手（可细调参数）攻击的鲁棒性，确保安全内容生成同时拒绝不安全内容。


<details>
  <summary>Details</summary>
Motivation: 当前的安全措施在面对知道并能调整模型参数的白盒对手时，如通过微调对模型进行攻击，可能失效。因此需要一种整体性的保护机制，既保护安全内容的生成，又抵抗对模型的恶意微调。

Method: 设计一个内部审核模块，将不安全的输入特征解码为零向量，且对良性输入保持良好的解码性能；引入一种非微调（non-fine-tunable）的学习机制以加强模型对齐，确保在面对恶意微调时不易被破坏。

Result: 通过大量实验，验证在安全内容生成方面性能未受损，并能有效拒绝不安全内容的生成；同时验证 Patronus 对多种白盒对手的微调攻击具有鲁棒性。

Conclusion: Patronus 提供了一种面向白盒对手的文本到图像模型的全面保护方案，结合输入级筛选与鲁棒对齐，提升对抗微调攻击的能力，并在保持安全内容输出的前提下提高系统的整体安全性。

Abstract: Text-to-image (T2I) models, though exhibiting remarkable creativity in image
generation, can be exploited to produce unsafe images. Existing safety
measures, e.g., content moderation or model alignment, fail in the presence of
white-box adversaries who know and can adjust model parameters, e.g., by
fine-tuning. This paper presents a novel defensive framework, named Patronus,
which equips T2I models with holistic protection to defend against white-box
adversaries. Specifically, we design an internal moderator that decodes unsafe
input features into zero vectors while ensuring the decoding performance of
benign input features. Furthermore, we strengthen the model alignment with a
carefully designed non-fine-tunable learning mechanism, ensuring the T2I model
will not be compromised by malicious fine-tuning. We conduct extensive
experiments to validate the intactness of the performance on safe content
generation and the effectiveness of rejecting unsafe content generation.
Results also confirm the resilience of Patronus against various fine-tuning
attacks by white-box adversaries.

</details>


### [197] [DESTinE Block: Private Blockchain Based Data Storage Framework for Power System](https://arxiv.org/abs/2510.16593)
*Khandaker Akramul Haque,Katherine R. Davis*

Main category: cs.CR

TL;DR: DESTinE Block 是一个面向电力系统、面向资源受限设备的区块链数据存储框架，利用 IPFS 存储大文件、在区块链上记录不可变的元数据，并通过 PoA 实现双方协作的安全共识，具有低硬件要求并可在树莓派等设备上运行。


<details>
  <summary>Details</summary>
Motivation: 在智能电网场景中需确保数据的不可篡改、可追溯的日志与存储，同时需要处理大文件的存储需求而又受限于边缘设备的计算与存储能力，因此需要一个在并非将数据上链、而是在链上记录关键元数据的安全、高效方案。

Method: 提出双区块链抽象的 DESTinE Block 架构：使用 IPFS 存储大文件，在链上记录元数据（CID、上传者、管理员验证、时间戳），元数据不可变写入链。采用基于 PoA 的共识机制，要求管理员和上传者各自持有密钥对共同创建区块，区块包含双方签名。设计目标是提高计算效率，使其能够在 Raspberry Pi 5 等设备上部署。对比基于 Multichain 的类似框架进行评估。

Result: 在 x86 与 ARM64 的 Raspberry Pi 上实现并测试，证明 DESTinE Block 能实现安全、去中心化的日志与测量存储，且硬件要求低，适合边缘环境。与 Multichain 框架比较后，显示出在容量与安全性之间的良好折中，具备将数据不可篡改地存储在分布式电网基础设施中的潜力。

Conclusion: DESTinE Block 为分布式电力系统的数据保留提供了一种有前景的方案，能够在边缘设备上实现高效且安全的去中心化日志与测量存储，同时通过对 IPFS 的存储分离和不可变元数据写入提高了隐私与安全性。

Abstract: This paper presents DESTinE Block, a blockchain-based data storage framework
designed for power systems and optimized for resource-constrained environments,
including grid-edge devices such as single-board computers. The proposed
architecture leverages the InterPlanetary File System (IPFS) for storing large
files while maintaining secure and traceable metadata on a custom blockchain
named DESTinE Block. The metadata, comprising the IPFS Content Identifier
(CID), uploader identity, administrator verification, and timestamp; is
immutably recorded on-chain to ensure authenticity and integrity. DESTinE Block
adopts a dual-blockchain abstraction, where the blockchain remains unaware of
the IPFS storage layer to enhance security and limit the exposure of sensitive
file data. The consensus mechanism is based on Proof of Authority (PoA), where
both an administrator and an uploader with distinct cryptographic key pairs are
required to create a block collaboratively. Each block contains verified
signatures of both parties and is designed to be computationally efficient,
enabling deployment on devices like the Raspberry Pi 5. The framework was
tested on both an x86-based device and an ARM64-based Raspberry Pi,
demonstrating its potential for secure, decentralized logging and measurement
storage in smart grid applications. Moreover, DESTinE Block is compared with a
similar framework based on Multichain. The results indicate that DESTinE Block
provides a promising solution for tamper-evident data retention in distributed
power system infrastructure while maintaining minimal hardware requirements.

</details>


### [198] [A Versatile Framework for Designing Group-Sparse Adversarial Attacks](https://arxiv.org/abs/2510.16637)
*Alireza Heshmati,Saman Soleimani Roudi,Sajjad Amini,Shahrokh Ghaemmaghami,Farokh Marvasti*

Main category: cs.CR

TL;DR: ATOS is a differentiable framework for generating structured, sparse adversarial perturbations (element-wise, pixel-wise, group-wise) using an Overlapping Smoothed L0 (OSL0) function that promotes sparsity and structured changes; it improves interpretability and achieves 100% attack success on CIFAR-10 and ImageNet, enabling counterfactual explanations by replacing class-defining regions with robust features from a target class.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks often ignore perturbation sparsity and structure, limiting the ability to model how DNNs process meaningful input patterns and to provide interpretable explanations of robust vs. non-robust features. There is a need for methods that generate sparse, structured perturbations and offer region-level interpretability.

Method: Propose ATOS, a differentiable optimization framework. Introduce Overlapping Smoothed L0 (OSL0) to encourage sparse, structured perturbations by grouping channels and adjacent pixels. Approximate the L-infinity gradient via the log-sum-exp of absolute values to tightly bound perturbation magnitudes. Support perturbations in element-wise, pixel-wise, and group-wise forms.

Result: On CIFAR-10 and ImageNet, ATOS achieves 100% attack success rate and produces significantly sparser and more structurally coherent perturbations than prior methods. The structured group-wise attack highlights critical regions from the network’s perspective and provides counterfactual explanations by replacing class-defining regions with robust features from the target class.

Conclusion: ATOS advances the study of adversarial perturbations by incorporating overlapping sparsity and structure, improving interpretability and enabling region-level counterfactual explanations. It helps identify robust vs. non-robust features and offers a framework for understanding how DNNs process meaningful input patterns.

Abstract: Existing adversarial attacks often neglect perturbation sparsity, limiting
their ability to model structural changes and to explain how deep neural
networks (DNNs) process meaningful input patterns. We propose ATOS (Attack
Through Overlapping Sparsity), a differentiable optimization framework that
generates structured, sparse adversarial perturbations in element-wise,
pixel-wise, and group-wise forms. For white-box attacks on image classifiers,
we introduce the Overlapping Smoothed L0 (OSL0) function, which promotes
convergence to a stationary point while encouraging sparse, structured
perturbations. By grouping channels and adjacent pixels, ATOS improves
interpretability and helps identify robust versus non-robust features. We
approximate the L-infinity gradient using the logarithm of the sum of
exponential absolute values to tightly control perturbation magnitude. On
CIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing
significantly sparser and more structurally coherent perturbations than prior
methods. The structured group-wise attack highlights critical regions from the
network's perspective, providing counterfactual explanations by replacing
class-defining regions with robust features from the target class.

</details>


### [199] [Quantum Key Distribution for Virtual Power Plant Communication: A Lightweight Key-Aware Scheduler with Provable Stability](https://arxiv.org/abs/2510.17087)
*Ziqing Zhu*

Main category: cs.CR

TL;DR: 提出一个密钥感知的优先级与配额框架，将量子密钥视为第一等级的调度资源，用于VVPP（虚拟电厂）系统中的分布式能源协同与通信安全，能在密钥稀缺与高负载的场景下通过长期配额、短期令牌、DRR调度、紧急密钥储备，以及降级策略实现稳定性与可接受的时延。


<details>
  <summary>Details</summary>
Motivation: 随着VPP对高频、低时延的实时控制和跨域安全通信需求上升，传统PKI与密钥轮换在跨域和量子威胁背景下面临挑战；QKD提供信息论级密钥，但产出波动、稀缺，需把密钥作为调度资源以匹配VPP的 Burst traffic和关键控制信号。

Method: 提出四位一体的密钥感知调度框架： (i) 基于预测的长期配额与短期令牌缓冲；(ii) 密钥感知的 deficit-round-robin (DRR) 调度算法；(iii) 预置的紧急密钥储备以应对突发; (iv) 通过加密模式切换与对非关键流量的降采样实现优雅降级；并通过 drift-plus-penalty 分析证明在平均供需平衡下的稳定性与背压/尾延的界限。以 IEEE 33/123-bus VPP 测试平台进行实验，比较正常、降级和中断等 regime。

Result: 在与 FIFO、固定优先级、静态配额等基线比较中，所提方案在尾部延迟和被动超时方面显著改善，提高了每比特密钥的利用率，并在密钥稀缺与 regime 切换时提升关键消息的传输可靠性与电力系统的跟踪鲁棒性。

Conclusion: 把量子密钥作为核心调度资源的框架在VPP场景中可实现信息论上安全的密钥供给与实时通信的高可用性之间的权衡，提供直观的操作界限并对不同密钥产出波动情形具有鲁棒性；实验验证对主流调度策略具明显优势，适合跨域、高频消息场景的安全化演进。

Abstract: Virtual power plants (VPPs) are becoming a cornerstone of future grids,
aggregating distributed PV, wind, storage, and flexible loads for market
participation and real-time balancing. As operations move to minute-- and
second--level feedback, communication security shifts from a compliance item to
an operational constraint: latency, reliability, and confidentiality jointly
determine whether dispatch, protection, and settlement signals arrive on time.
Conventional PKI and key-rotation schemes struggle with cross-domain,
high-frequency messaging and face long-term quantum threats. Quantum key
distribution (QKD) offers information-theoretic key freshness, but its key
yield is scarce and stochastic, often misaligned with bursty VPP traffic. This
paper proposes a key-aware priority and quota framework that treats quantum
keys as first-class scheduling resources. The design combines (i)
forecast-driven long-term quotas and short-term tokens, (ii) key-aware
deficit-round-robin arbitration, (iii) a preemptive emergency key reserve, and
(iv) graceful degradation via encryption-mode switching and controlled
down-sampling for non-critical traffic. A drift-plus-penalty analysis
establishes strong stability under average supply--demand balance with
quantifiable bounds on backlog and tail latency, providing interpretable
operating guarantees. We build a reproducible testbed on IEEE 33- and 123-bus
VPP systems and evaluate normal, degraded, and outage regimes with
industry-consistent message classes and TTLs. Against FIFO, fixed-priority, and
static-quota baselines, the proposed scheme consistently reduces tail delay and
passive timeouts for critical messages, improves per-bit key utility, and
enhances power-tracking reliability during key scarcity and regime switches.

</details>


### [200] [DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](https://arxiv.org/abs/2510.16716)
*Asmita Mohanty,Gezheng Kang,Lei Gao,Murali Annavaram*

Main category: cs.CR

TL;DR: 在边缘设备通过TEE与权重混淆实现隐私保护的知识蒸馏，支持对LLM的私有化个性化。


<details>
  <summary>Details</summary>
Motivation: 云端微调需上传潜在敏感数据，存在隐私风险；边缘微调需保护模型知识产权，防止IP泄露与窃取。

Method: 在数据拥有者设备上将专有基础模型置于TEEs enclave中作为安全黑盒教师，并通过对权重进行混淆后卸载到不可信加速器以实现高效知识蒸馏；同时实现对未经授权的知识蒸馏和模型窃取的防护。

Result: 声称在保护数据隐私和模型IP的前提下仍保持较高的计算效率，且能防护知识蒸馏与模型窃取攻击。

Conclusion: DistilLock为边缘端LLM的私有化个性化提供一个安全且实用的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
diverse tasks, but fine-tuning them typically relies on cloud-based,
centralized infrastructures. This requires data owners to upload potentially
sensitive data to external servers, raising serious privacy concerns. An
alternative approach is to fine-tune LLMs directly on edge devices using local
data; however, this introduces a new challenge: the model owner must transfer
proprietary models to the edge, which risks intellectual property (IP) leakage.
To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning
framework that enables privacy-preserving knowledge distillation on the edge.
In DistilLock, a proprietary foundation model is executed within a trusted
execution environment (TEE) enclave on the data owner's device, acting as a
secure black-box teacher. This setup preserves both data privacy and model IP
by preventing direct access to model internals. Furthermore, DistilLock employs
a model obfuscation mechanism to offload obfuscated weights to untrusted
accelerators for efficient knowledge distillation without compromising
security. We demonstrate that DistilLock prevents unauthorized knowledge
distillation processes and model-stealing attacks while maintaining high
computational efficiency, but offering a secure and practical solution for
edge-based LLM personalization.

</details>


### [201] [Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022](https://arxiv.org/abs/2510.16744)
*Srinivas Vivek*

Main category: cs.CR

TL;DR: 对 NSS 2022 提出的 PP-RHS 的被动攻击：攻击者能够在每次请求中完全恢复乘客和响应司机的精确位置，且时间复杂度独立于安全参数。


<details>
  <summary>Details</summary>
Motivation: 隐私保护的出行匹配系统需要强健的定位隐私保护；尽管提出了 PP-RHS，仍需评估其对真实攻击向量的鲁棒性，以及潜在的实现缺陷和信息泄露路径。

Method: 对 Xie 等人提出的 PP-RHS 进行被动攻击分析，揭示协议在匹配过程中对位置信息的潜在泄露；攻击者以被动方式利用协议结构中的信息映射，且与安全参数无关的高效性是核心亮点。

Result: 攻击者（服务提供商）能够在每次 ride request 的匹配中完全还原乘客与响应司机的位置信息，且攻击复杂度独立于所选的安全参数。

Conclusion: 工作揭示了 PP-RHS 的显著隐私漏洞，需对协议信任模型、实现细节和安全性进行重新审视；未来工作应提出更强的防护机制、对泄露风险进行量化评估，并探索鲁棒的隐私保护设计。

Abstract: Ride-Hailing Services (RHS) match a ride request initiated by a rider with a
suitable driver responding to the ride request. A Privacy-Preserving RHS
(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'
and drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie
et al. proposed a PP-RHS. In this work, we demonstrate a passive attack on
their PP-RHS protocol. Our attack allows the SP to completely recover the
locations of the rider as well as that of the responding drivers in every ride
request. Further, our attack is very efficient as it is independent of the
security parameter.

</details>


### [202] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: 提出 Verifiable Fine Tuning，通过零知识证明确保模型来自公开初始化和声明训练程序/数据承诺的微调，同时提供端到端证书与隐私保护，五要素实现高效可验证性。


<details>
  <summary>Details</summary>
Motivation: 建立对参数高效微调模型的信任，尤其在受监管和去中心化部署场景；现有发布实践缺乏关于数据源、预处理和更新计算的透明性。

Method: 一是绑定数据源、预处理、许可和每纪元配额的承诺；二是可验证的采样器，支持公开重放与私有索引隐藏的批次选择；三是仅限参数高效微调的更新电路，强制 AdamW 风格优化器语义与可证明的近似及误差预算；四是递归聚合，将逐步证据折叠为逐纪元及端到端证书、毫秒级验证；五是溯源绑定与可选可信执行属性卡，证明代码身份与常量。

Result: 在英语与双语指令混合数据上，方法在严格预算下保持实用性，证明性能良好；策略配额零偏差，私有采样窗口未泄露索引；联合实验表明可与概率性审计和带宽约束组合；端到端可验证微调在当前的参数高效流水线中是可行的。

Conclusion: 这项工作在受监管和去中心化部署场景下缩小信任鸿沟，证明端到端可验证微调在现阶段即可实现，提升对微调模型的透明度与合规性。

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [203] [ThreatIntel-Andro: Expert-Verified Benchmarking for Robust Android Malware Research](https://arxiv.org/abs/2510.16835)
*Hongpeng Bai,Minhong Dong,Yao Zhang,Shunzhe Zhao,Haobo Zhang,Lingyue Li,Yude Bai,Guangquan Xu*

Main category: cs.CR

TL;DR: Android恶意软件数据集存在显著标签噪声与时效性问题，现有数据集（如Drebin）过度依赖VirusTotal多引擎聚合，导致标签不可靠；自动标注工具（如AVClass2）聚合策略不佳，进一步放大错误，影响研究结论。需要高质量、实时更新的数据集以支撑检测与防御研究。


<details>
  <summary>Details</summary>
Motivation: 快速演进的Android恶意软件生态以及在工业系统中的广泛部署使其成为关键但易被忽视的攻击面；现有数据集在标签质量和时效性方面的不足，降低了检测方法的可比性和可重复性。

Method: 分析并对比主流数据集在标签获取上的来源与潜在噪声，评估 VirusTotal 聚合结果与 AVClass2 等自动标注工具的局限性，讨论标签噪声对研究的影响，并提出改进的标注思路与数据集更新策略（如降低对单一聚合结果的依赖、提升样本新鲜度）。

Result: 揭示 Drebin 等数据集存在显著的标签噪声和时效性退化；VirusTotal 的多引擎聚合带来不可靠标签，AVClass2 等自动标注工具的聚合策略易产生错误并在研究社区中传播。

Conclusion: 迫切需要构建高质量、实时更新的Android恶意软件数据集，改进标注流程以降低噪声和偏差，提高跨研究的一致性与可重复性。

Abstract: The rapidly evolving Android malware ecosystem demands high-quality,
real-time datasets as a foundation for effective detection and defense. With
the widespread adoption of mobile devices across industrial systems, they have
become a critical yet often overlooked attack surface in industrial
cybersecurity. However, mainstream datasets widely used in academia and
industry (e.g., Drebin) exhibit significant limitations: on one hand, their
heavy reliance on VirusTotal's multi-engine aggregation results introduces
substantial label noise; on the other hand, outdated samples reduce their
temporal relevance. Moreover, automated labeling tools (e.g., AVClass2) suffer
from suboptimal aggregation strategies, further compounding labeling errors and
propagating inaccuracies throughout the research community.

</details>


### [204] [Addendum: Systematic Evaluation of Randomized Cache Designs against Cache Occupancy](https://arxiv.org/abs/2510.16871)
*Anirban Chakraborty,Nimish Mishra,Sayandeep Saha,Sarani Bhattacharya,Debdeep Mukhopadhyay*

Main category: cs.CR

TL;DR: 本论文对随机化缓存中的占用行为进行系统分析，提出统一基准以公平比较不同随机缓存的性能，并在安全视角下界定三类威胁模型。核心结论是：在不损害与现代集合关联缓存等效性的前提下，仍难以同时抵御占用攻击和竞争压力；并给出对前沿工作的补充分析。


<details>
  <summary>Details</summary>
Motivation: 研究缓存占用对随机化缓存设计的影响，以及在性能与安全之间的权衡，明确在与现代LLC等效性能前提下实现对抗对抗性攻击的难题。

Method: 对随机化缓存进行系统分析，提出统一的基准测试策略以公平比较不同设计；从安全角度定义三类威胁：隐藏通道、进程指纹信息泄露的侧信道、以及AES密钥恢复，并对相关研究进行评论和补充分析。

Result: 给出一个核心难题：在保持与现代集合关联LLC类似的效率下，设计出既能抵御 contention-based 又能抵御 occupancy-based 攻击的随机缓存。进一步在Addendum中总结了文献[2]的观点：L1d缓存尺寸影响对手的成功率，以及对MIRAGE的改进（随机化初始覆盖地图的种子）可防止AES密钥泄露。

Conclusion: 需要继续探索在高效性与安全性之间的折中策略，提出的基准与补充分析有助于评估后续随机缓存设计的可行性与鲁棒性。

Abstract: In the main text published at USENIX Security 2025, we presented a systematic
analysis of the role of cache occupancy in the design considerations for
randomized caches (from the perspectives of performance and security). On the
performance front, we presented a uniform benchmarking strategy that allows for
a fair comparison among different randomized cache designs. Likewise, from the
security perspective, we presented three threat assumptions: (1) covert
channels; (2) process fingerprinting side-channel; and (3) AES key recovery.
The main takeaway of our work is an open problem of designing a randomized
cache of comparable efficiency with modern set-associative LLCs, while still
resisting both contention-based and occupancy-based attacks. This note is meant
as an addendum to the main text in light of the observations made in [2]. To
summarize, the authors in [2] argue that (1) L1d cache size plays a role in
adversarial success, and that (2) a patched version of MIRAGE with randomized
initial seeding of global eviction map prevents leakage of AES key. We discuss
the same in this addendum.

</details>


### [205] [On the Credibility of Deniable Communication in Court](https://arxiv.org/abs/2510.16873)
*Jacob Leiken,Sunoo Park*

Main category: cs.CR

TL;DR: 提出“可信度（credibility）”取代单纯的可否认性（deniability）作为评估数字证据的框架，强调认知与社会技术因素对证据可被接受性的影响，并给出三要素：阈值、伪造难度与默认保留策略，以指导现实世界的系统设计。


<details>
  <summary>Details</summary>
Motivation: 技术层面的可否认性与现实世界证据的可验证性在法理与社会技术 context中存在断层；证据本质上可被伪造且证据的可接受性受超越密码学的因素影响，需要更广义的“可信度”框架来指导系统设计与部署。

Method: 分析现有的可否认性概念与现实证据体系之间的差距，提出“可信度”作为更广义的框架；将可信度分解为三个维度：(1) 证伪生成的可信阈值，(2) 通过该阈值的伪造难度，(3) 默认保留策略与设定；并讨论如何将其中部分要素直接嵌入技术设计中以应对现实威胁模型。

Result: 提出了一个将社会技术因素纳入的可信度模型，旨在帮助设计和部署更符合真实威胁且在法理与社会语境中可解释的沟通系统，超越纯技术定义的局限性。

Conclusion: 可信度模型可以覆盖技术定义无法捕捉的威胁与情境，支持在特定法律与社会技术背景下对密码学保证的更为细致的讨论与应用。

Abstract: Over time, cryptographically deniable systems have come to be associated in
computer-science literature with the idea of "denying" evidence in court -
specifically, with the ability to convincingly forge evidence in courtroom
scenarios and an inability to authenticate evidence in such contexts.
Evidentiary processes in courts, however, have been developed over centuries to
account for the reality that evidence has always been forgeable, and relies on
factors outside of cryptographic models to seek the truth "as well as possible"
while acknowledging that all evidence is imperfect. We argue that deniability
does not and need not change this paradigm.
  Our analysis highlights a gap between technical deniability notions and their
application to the real world. There will always be factors outside a
cryptographic model that influence perceptions of a message's authenticity, in
realistic situations. We propose the broader concept of credibility to capture
these factors. The credibility of a system is determined by (1) a threshold of
quality that a forgery must pass to be "believable" as an original
communication, which varies based on sociotechnical context and threat model,
(2) the ease of creating a forgery that passes this threshold, which is also
context- and threat-model-dependent, and (3) default system retention policy
and retention settings. All three aspects are important for designing secure
communication systems for real-world threat models, and some aspects of (2) and
(3) may be incorporated directly into technical system design. We hope that our
model of credibility will facilitate system design and deployment that
addresses threats that are not and cannot be captured by purely technical
definitions and existing cryptographic models, and support more nuanced
discourse on the strengths and limitations of cryptographic guarantees within
specific legal and sociotechnical contexts.

</details>


### [206] [UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks](https://arxiv.org/abs/2510.16923)
*Mansi Phute,Matthew Hull,Haoran Wang,Alec Helbling,ShengYun Peng,Willian Lunardi,Martin Andreoni,Wenke Lee,Polo Chau*

Main category: cs.CR

TL;DR: UNDREAM bridging non-differentiable photorealistic simulators and differentiable renderers to enable end-to-end optimization of physical adversarial perturbations on 3D objects; enables full scene control and diverse, plausible adversarial objects.


<details>
  <summary>Details</summary>
Motivation: To overcome non-differentiability of simulation environments that hinder gradient-based adversarial attack optimization; enable realistic, differentiable optimization across 3D scenes.

Method: Proposes UNDREAM framework that integrates photorealistic simulators with differentiable renderers, providing end-to-end optimization of adversarial perturbations on 3D objects; offers control over weather, lighting, backgrounds, camera angles, trajectories, and human/object movements to create diverse scenes.

Result: Demonstrates a suite of physically plausible adversarial objects and configurable environments, enabling rapid exploration and study of physical adversarial attacks.

Conclusion: Bridging realism and differentiability opens new research avenues for physical adversarial attacks and more robust evaluation in safety-critical applications.

Abstract: Deep learning models deployed in safety critical applications like autonomous
driving use simulations to test their robustness against adversarial attacks in
realistic conditions. However, these simulations are non-differentiable,
forcing researchers to create attacks that do not integrate simulation
environmental factors, reducing attack success. To address this limitation, we
introduce UNDREAM, the first software framework that bridges the gap between
photorealistic simulators and differentiable renderers to enable end-to-end
optimization of adversarial perturbations on any 3D objects. UNDREAM enables
manipulation of the environment by offering complete control over weather,
lighting, backgrounds, camera angles, trajectories, and realistic human and
object movements, thereby allowing the creation of diverse scenes. We showcase
a wide array of distinct physically plausible adversarial objects that UNDREAM
enables researchers to swiftly explore in different configurable environments.
This combination of photorealistic simulation and differentiable optimization
opens new avenues for advancing research of physical adversarial attacks.

</details>


### [207] [Efficient derandomization of differentially private counting queries](https://arxiv.org/abs/2510.16959)
*Surendra Ghentiyala*

Main category: cs.CR

TL;DR: 本文在差分隐私的 d 次计数查询随机性复杂度方面取得进展：给出一个多项式时间的机制，在与 CSV25 相近的随机性-精度权衡下实现 O(log d) 的随机性需求。通过对每个查询答案进行随机平移，使得在噪声添加时可选择不对大量坐标添加噪声，从而避免了将噪声加入每个坐标的步骤，并且不再使用舍入方案。


<details>
  <summary>Details</summary>
Motivation: 实际应用中为差分隐私生成大量随机性极其昂贵或不可行（如 2020 人口普查需要约 90 TB 的随机性）。因此研究 d 次计数查询的随机性复杂度，以及在可接受的误差下用尽可能少的随机性来实现隐私保障。

Method: 给出一个多项式时间的机制：对每个计数查询在答案上进行随机平移后，许多坐标的最终结果在加入噪声与否对该坐标而言仍保持不变，从而可以在很多坐标上省略噪声添加的步骤；该机制不使用舍入方案，强调对 d 个计数查询进行批处理以获得随机性节省的直观来源。

Result: 实现了与 CSV25 相近的随机性-精度权衡，但在多项式时间内可行，随机性复杂度达到 O(log d)（在期望意义下）。不再需要舍入方案，提供了对随机性节省源更清晰的直观解释。

Conclusion: 本文提供了一种新的、清晰的对差分隐私下批量计数查询随机性节省来源的理解，并给出一个高效的多项式时间机制，达到与以往舍入方案相似的随机性收益。

Abstract: Differential privacy for the 2020 census required an estimated 90 terabytes
of randomness [GL20], an amount which may be prohibitively expensive or
entirely infeasible to generate. Motivated by these practical concerns, [CSV25]
initiated the study of the randomness complexity of differential privacy, and
in particular, the randomness complexity of $d$ counting queries. This is the
task of outputting the number of entries in a dataset that satisfy predicates
$\mathcal{P}_1, \dots, \mathcal{P}_d$ respectively. They showed the rather
surprising fact that though any reasonably accurate,
$\varepsilon$-differentially private mechanism for one counting query requires
$1-O(\varepsilon)$ bits of randomness in expectation, there exists a fairly
accurate mechanism for $d$ counting queries which requires only $O(\log d)$
bits of randomness in expectation.
  The mechanism of [CSV25] is inefficient (not polynomial time) and relies on a
combinatorial object known as rounding schemes. Here, we give a polynomial time
mechanism which achieves nearly the same randomness complexity versus accuracy
tradeoff as that of [CSV25]. Our construction is based on the following simple
observation: after a randomized shift of the answer to each counting query, the
answer to many counting queries remains the same regardless of whether we add
noise to that coordinate or not. This allows us to forgo the step of adding
noise to the result of many counting queries. Our mechanism does not make use
of rounding schemes. Therefore, it provides a different -- and, in our opinion,
clearer -- insight into the origins of the randomness savings that can be
obtained by batching $d$ counting queries. Therefore, it provides a different
-- and, in our opinion, clearer -- insight into the origins of the randomness
savings that can be obtained by batching $d$ counting queries.

</details>


### [208] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 提出一个信息论框架来量化对目标属性的泄露信息量，并推导所需的查询次数与泄露速率的关系；在七个LLM及系统提示泄露、越狱与再学习攻击场景下验证，显示披露更多信息将显著降低攻击成本。


<details>
  <summary>Details</summary>
Motivation: 在LLM的透明性与安全性之间建立定量权衡，解决现有研究缺乏 principled 指导的问题，评估可被利用的可观测信号对攻击者的影响。

Method: 将观测信号Z与目标属性T之间的互信息I(Z;T)作为泄露的比特数，推导达到误差ε所需的最小查询次数≈log(1/ε)/I(Z;T)，并在七个LLMs的实验中比较只暴露回答令牌、加入对数 logits、以及暴露完整推理过程对攻击成本的影响。

Result: 理论与实证一致：泄露信息越多，所需查询次数显著下降；仅回答令牌≈10^3次查询，加入logits≈10^2次，暴露推理过程仅需数十次查询。不同攻击场景下的实验与理论相符。

Conclusion: 本文首次给出透明性与安全之间的原则性度量，为在部署LLMs时权衡透明性与安全性提供基准。

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [209] [Watermark Robustness and Radioactivity May Be at Odds in Federated Learning](https://arxiv.org/abs/2510.17033)
*Leixu Huang,Zedian Shao,Teodora Baluta*

Main category: cs.CR

TL;DR: 在联邦学习中，将水印嵌入数据以实现数据来源追踪，水印在继续微调后仍可检测，但面对主动对抗的鲁棒聚合会削弱水印，揭示放射性水印、鲁棒性与效用之间的根本权衡。


<details>
  <summary>Details</summary>
Motivation: 在分布式数据源中对大型语言模型进行微调时，数据可能来自LLM生成文本，需通过可追溯性提升问责性与透明度，因此需要一种在训练中可检测的来源标记机制。

Method: 将LLM水印方法改编用于数据 provenance：部分客户端在水印数据上计算本地更新，服务器对所有更新进行聚合以得到全局模型；评估水印的可检测性（放射性）及在对抗场景中的鲁棒性。

Result: 水印信号在仅6.6%的数据被水印时即可达到约10^-24的p值级别的检测；但主动对抗的服务器可使用强鲁棒聚合来过滤水印相关的异常更新，从而削弱或消除水印信号；所有评估的放射性水印对这种主动过滤均不鲁棒。

Conclusion: 在放射性、鲁棒性与模型效用之间存在根本性权衡，需在设计水印时考虑对抗性过滤与实用性之间的折中。

Abstract: Federated learning (FL) enables fine-tuning large language models (LLMs)
across distributed data sources. As these sources increasingly include
LLM-generated text, provenance tracking becomes essential for accountability
and transparency. We adapt LLM watermarking for data provenance in FL where a
subset of clients compute local updates on watermarked data, and the server
averages all updates into the global LLM. In this setup, watermarks are
radioactive: the watermark signal remains detectable after fine-tuning with
high confidence. The $p$-value can reach $10^{-24}$ even when as little as
$6.6\%$ of data is watermarked. However, the server can act as an active
adversary that wants to preserve model utility while evading provenance
tracking. Our observation is that updates induced by watermarked synthetic data
appear as outliers relative to non-watermark updates. Our adversary thus
applies strong robust aggregation that can filter these outliers, together with
the watermark signal. All evaluated radioactive watermarks are not robust
against such an active filtering server. Our work suggests fundamental
trade-offs between radioactivity, robustness, and utility.

</details>


### [210] [QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR](https://arxiv.org/abs/2510.17175)
*Muhammad Wahid Akram,Keshav Sood,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 提出了 QR"iS，一种基于 QR 码结构特征的可解释识别钓鱼二维码的方法，准确率约83.18%，并在大规模数据集与移动应用上进行评估。


<details>
  <summary>Details</summary>
Motivation: 解决现有黑箱方法缺乏可解释性、可重复性和偏差检测能力的问题，提升对钓鱼二维码的可控、可审计发现能力。

Method: 从 QR 码的布局中提取 24 个结构特征，构建数据集（40万条样本，基于公开 URL 数据集），训练多种机器学习模型，进行对比分析，并在移动应用中部署验证。

Result: 学到的准确率最高 83.18%，与相关研究的对比显示优势，证实结构分析法可实现可解释的识别。

Conclusion: QR"iS 提供了透明、可重复、可扩展的防止 Quishing 的方案，适用于实际部署。

Abstract: Globally, individuals and organizations employ Quick Response (QR) codes for
swift and convenient communication. Leveraging this, cybercriminals embed
falsify and misleading information in QR codes to launch various phishing
attacks which termed as Quishing. Many former studies have introduced defensive
approaches to preclude Quishing such as by classifying the embedded content of
QR codes and then label the QR codes accordingly, whereas other studies
classify them using visual features (i.e., deep features, histogram density
analysis features). However, these approaches mainly rely on black-box
techniques which do not clearly provide interpretability and transparency to
fully comprehend and reproduce the intrinsic decision process; therefore,
having certain obvious limitations includes the approaches' trust,
accountability, issues in bias detection, and many more. We proposed QR\"iS,
the pioneer method to classify QR codes through the comprehensive structural
analysis of a QR code which helps to identify phishing QR codes beforehand. Our
classification method is clearly transparent which makes it reproducible,
scalable, and easy to comprehend. First, we generated QR codes dataset (i.e.
400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike
black-box models, we developed a simple algorithm to extract 24 structural
features from layout patterns present in QR codes. Later, we train the machine
learning models on the harvested features and obtained accuracy of up to
83.18%. To further evaluate the effectiveness of our approach, we perform the
comparative analysis of proposed method with relevant contemporary studies.
Lastly, for real-world deployment and validation, we developed a mobile app
which assures the feasibility of the proposed solution in real-world scenarios
which eventually strengthen the applicability of the study.

</details>


### [211] [Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography](https://arxiv.org/abs/2510.17220)
*Giulia Giusti*

Main category: cs.CR

TL;DR: 将线性逻辑用于两类基于线性的编程范式：Automatic Differentiation（AD）与计算密码学中的协议分析，提出 ADLL 与 CryptoBLL 两个部分，连接理论证明与实际工具以实现资源敏感的分析与验证。


<details>
  <summary>Details</summary>
Motivation: 线性概念在数学与计算机科学中的双重含义及其对资源约束建模的重要性，需将线性逻辑与实际编程语言/密码学分析结合，提供可分析、可验证的框架。

Method: ADLL：在实数线性函数和转置运算上建立线性类型系统，区分理论的 proof-theoretic 路径与在 JAX（Python）中的实际实现，并试图将 JAX 的类型系统映射到 LL。CryptoBLL：提出以计算性密码学的安全性分析为目标的自动化分析框架，权衡表达能力与简化抽象之间的折中。

Result: 提出了将 LL 应用于 AD 的理论-实践桥接，给出在 JAX 的初步对齐；以及一个用于计算性密码学协议分析的自动化框架，支持对降低、简化的建模和推理。

Conclusion: 该工作证实了将线性逻辑融入实际领域（自动微分、密码学分析）的可行性和价值，提供了一个统一的、可扩展的分析/验证路径，同时指出未来在理论-实现之间的进一步对齐与扩展空间。

Abstract: The concept of linearity plays a central role in both mathematics and
computer science, with distinct yet complementary meanings. In mathematics,
linearity underpins functions and vector spaces, forming the foundation of
linear algebra and functional analysis. In computer science, it relates to
resource-sensitive computation. Linear Logic (LL), for instance, models
assumptions that must be used exactly once, providing a natural framework for
tracking computational resources such as time, memory, or data access. This
dual perspective makes linearity essential to programming languages, type
systems, and formal models that express both computational complexity and
composability. Bridging these interpretations enables rigorous yet practical
methodologies for analyzing and verifying complex systems.
  This thesis explores the use of LL to model programming paradigms based on
linearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to
Automatic Differentiation (AD), modeling linear functions over the reals and
the transposition operation. The latter uses LL to express complexity
constraints on adversaries in computational cryptography.
  In AD, two main approaches use linear type systems: a theoretical one
grounded in proof theory, and a practical one implemented in JAX, a Python
library developed by Google for machine learning research. In contrast,
frameworks like PyTorch and TensorFlow support AD without linear types. ADLL
aims to bridge theory and practice by connecting JAX's type system to LL.
  In modern cryptography, several calculi aim to model cryptographic proofs
within the computational paradigm. These efforts face a trade-off between
expressiveness, to capture reductions, and simplicity, to abstract probability
and complexity. CryptoBLL addresses this tension by proposing a framework for
the automatic analysis of protocols in computational cryptography.

</details>


### [212] [Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values](https://arxiv.org/abs/2510.17284)
*Jiri Gavenda,Petr Svenda,Stanislav Bobon,Vladimir Sedlacek*

Main category: cs.CR

TL;DR: 论文评估了三种中心协调器的 coinjoin 设计对隐私的影响，给出平均去混后匿名性集合大小下降在 10-50% 之间，随时间衰减；提出一种并行化隐私估算方法，考虑手续费、实现限制和用户行为，表明在复杂情形下正确关联所有者仍然困难。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于量化 coinjoin 设计对比特币隐私的实际影响，并解决对隐私增益的估算难点与计算复杂性问题，尤其在真实世界大规模 coinjoin 场景下的可估计性。

Method: 方法包括：1) 将 BlockSci(on-chain analysis)适配用于 coinjoin 交易以评估隐私；2) 设计一种可并行化的隐私估算方法，考虑手续费、实现特定限制和用户的 post-mix 行为；3) 在 emulated 与真实世界 Wasabi 2.x coinjoin 上进行详细评估，并对大规模 coinjoin 进行外推。

Result: 关键结果显示：在 Whirlpool、Wasabi 1.x、Wasabi 2.x 三种中心协调设计下，平均 post-mix 匿名性集合大小下降 10-50%，并且在第一天降幅最大，经过一年后降幅变得可忽略；并且提出的隐私估算方法在 Wasabi 2.x 的大量 inputs/outputs 的 coinjoin 样本上进行评估，证明尽管 post-mix 行为不理想，正确将币归属于所有者在现实情况下仍然非常困难。

Conclusion: 总体结论是：中心协调 coinjoin 虽能提升某种隐私保护，但隐私增益随时间衰减，且在大规模真实世界场景下仍难以达到可验证的归属确定性；所提出的并行化隐私估算方法有助于更精确地评估实际隐私增益，尤其在考虑经济因素和实现限制时。

Abstract: A coinjoin protocol aims to increase transactional privacy for Bitcoin and
Bitcoin-like blockchains via collaborative transactions, by violating
assumptions behind common analysis heuristics. Estimating the resulting privacy
gain is a crucial yet unsolved problem due to a range of influencing factors
and large computational complexity.
  We adapt the BlockSci on-chain analysis software to coinjoin transactions,
demonstrating a significant (10-50%) average post-mix anonymity set size
decrease for all three major designs with a central coordinator: Whirlpool,
Wasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and
negligible after one year from a coinjoin creation.
  Moreover, we design a precise, parallelizable privacy estimation method,
which takes into account coinjoin fees, implementation-specific limitations and
users' post-mix behavior. We evaluate our method in detail on a set of emulated
and real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world
coinjoins with hundreds of inputs and outputs. We conclude that despite the
users' undesirable post-mix behavior, correctly attributing the coins to their
owners is still very difficult, even with our improved analysis algorithm.

</details>


### [213] [The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment](https://arxiv.org/abs/2510.17311)
*Eduard Marin,Jinwoo Kim,Alessio Pavoni,Mauro Conti,Roberto Di Pietro*

Main category: cs.CR

TL;DR: 对公有服务器无组件与 IaC 模板的安全态势进行首次大规模分析，揭示广泛存在的漏洞并给出缓解建议。


<details>
  <summary>Details</summary>
Motivation: 随着无服务器架构和公有仓库的广泛使用，组件成为潜在攻击面，安全性尚未被充分研究。

Method: 分析来自五大常用公有仓库的2,758个服务器无组件及3个主流IaC框架下的125,936个IaC模板，进行系统性安全评估，发现潜在漏洞与可利用点。

Result: 发现系统性漏洞包括过时软件包、敏感参数误用、部署配置可利用、存在打字错把攻击（typo-squatting）风险，以及在压缩的服务器无组件中嵌入恶意行为的可能性。

Conclusion: 给出切实可行的缓解策略，帮助提升公有服务器无组件仓库的安全态势。

Abstract: Serverless computing has rapidly emerged as a prominent cloud paradigm,
enabling developers to focus solely on application logic without the burden of
managing servers or underlying infrastructure. Public serverless repositories
have become key to accelerating the development of serverless applications.
However, their growing popularity makes them attractive targets for
adversaries. Despite this, the security posture of these repositories remains
largely unexplored, exposing developers and organizations to potential risks.
In this paper, we present the first comprehensive analysis of the security
landscape of serverless components hosted in public repositories. We analyse
2,758 serverless components from five widely used public repositories popular
among developers and enterprises, and 125,936 Infrastructure as Code (IaC)
templates across three widely used IaC frameworks. Our analysis reveals
systemic vulnerabilities including outdated software packages, misuse of
sensitive parameters, exploitable deployment configurations, susceptibility to
typo-squatting attacks and opportunities to embed malicious behaviour within
compressed serverless components. Finally, we provide practical recommendations
to mitigate these threats.

</details>


### [214] [Dynamic Switched Quantum Key Distribution Networkwith PUF-based authentication](https://arxiv.org/abs/2510.17552)
*Persefoni Konteli,Nikolaos Makris,Evgenia Niovi Sassalou,Stylianos A. Kazazis,Alkinoos Papageorgopoulos,Stefanos Vasileiadis,Konstantinos Tsimvrakidis,Symeon Tsintzos,Georgios M. Nikolopoulos,George T. Kanellos*

Main category: cs.CR

TL;DR: A centrally controlled dynamic switched-QKD network with integrated PUF-based dynamic authentication for each QKD link, analyzed under real-time PUF authentication.


<details>
  <summary>Details</summary>
Motivation: Improve scalability and security of QKD networks by enabling dynamic switching among links while ensuring per-link authentication via physical unclonable functions (PUFs).

Method: Design and analyze a dynamically switched QKD network with PUF-based dynamic authentication per link; evaluate performance under real-time PUF authentication.

Result: Demonstrates feasibility of a centrally controlled dynamic switched-QKD network with per-link PUF-based authentication and analyzes performance under real-time authentication.

Conclusion: A centralized control framework with dynamic switching and PUF-based per-link authentication is viable for secure QKD networks, with observed trade-offs between security guarantees and authentication/switching overhead.

Abstract: We demonstrate a centrally controlled dynamic switched-QKD network,
withintegrated PUF-based dynamic authentication for each QKD link. The
performance of the dynamicswitched-QKD network with real-time PUF-based
authentication is analyzed.

</details>


### [215] [CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks](https://arxiv.org/abs/2510.17687)
*Xu Zhang,Hao Li,Zhichao Lu*

Main category: cs.CR

TL;DR: 提出 ImpForge 与 CrossGuard 的端到端框架：通过 RL 生成跨模态隐式样本进行红队测试，并提出基于意图感知的防护机制，显著提升对显性与隐性威胁的检测与防御能力。


<details>
  <summary>Details</summary>
Motivation: MLLMs 越来越易受到 jailbreak 攻击，尤其是隐式跨模态威胁难以检测，缺乏高质量隐式数据，需自动化红队与健壮的防护。

Method: ImpForge 使用强化学习与定制的奖励模块，生成覆盖 14 个领域的多样隐式样本并构建数据集；CrossGuard 基于意图感知的防御，针对显性和隐性威胁提供稳健防护。

Result: 在安全/不安全基准、隐式与显式攻击、以及多种域外设置上进行广泛实验，CrossGuard 相较现有防御（包括先进的 MLLMs 与 guardrails）性能显著提升，保持高实用性。

Conclusion: 为现实世界的多模态威胁提供了更平衡、实用的鲁棒性提升方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong reasoning and
perception capabilities but are increasingly vulnerable to jailbreak attacks.
While existing work focuses on explicit attacks, where malicious content
resides in a single modality, recent studies reveal implicit attacks, in which
benign text and image inputs jointly express unsafe intent. Such joint-modal
threats are difficult to detect and remain underexplored, largely due to the
scarcity of high-quality implicit data. We propose ImpForge, an automated
red-teaming pipeline that leverages reinforcement learning with tailored reward
modules to generate diverse implicit samples across 14 domains. Building on
this dataset, we further develop CrossGuard, an intent-aware safeguard
providing robust and comprehensive defense against both explicit and implicit
threats. Extensive experiments across safe and unsafe benchmarks, implicit and
explicit attacks, and multiple out-of-domain settings demonstrate that
CrossGuard significantly outperforms existing defenses, including advanced
MLLMs and guardrails, achieving stronger security while maintaining high
utility. This offers a balanced and practical solution for enhancing MLLM
robustness against real-world multimodal threats.

</details>


### [216] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: 提出 VER A-V：一个基于变分推断的多模态跳出框架，通过学习文本-图像对的联合后验分布来生成隐蔽、耦合的对抗输入，从而绕过VLM的 guardrails，并显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态红队方法依赖脆弱模板、局限于单一攻击场景、覆盖漏洞面窄，难以系统性地发现VLM的广域漏洞。需要一个能同时探索文本与图像协同攻击空间、并给出分布性洞察的框架。

Method: 将多模态跳出问题建模为联合后验分布的学习问题；训练一个轻量级攻击者以近似后验，从而高效采样多样的 jailbreak；并结合三条策略： (i) 版式化文本提示嵌入有害线索； (ii) 基于扩散模型的图像合成引入对抗信号； (iii) 结构化干扰器以碎片化VLM的注意力。还在 HarmBench 与 HADES 基准上进行评估，比较SOTA基线。

Result: 在开放源代码与前沿VLM上，VERA-V普遍优于基线，显示出更高的攻击成功率；在 GPT-4o 场景中，攻击成功率最高可比基线提升约 53.75%。提供了对漏洞的分布性理解，便于分析不同输入模态对模型防护的影响。

Conclusion: 通过将变分推断与多模态对抗组合起来，VERA-V 能有效产生隐蔽且多样化的对抗输入，揭示VLM的更广泛脆弱性，并为未来防御策略提供新的研究线索。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [217] [Traffic Prioritization Mechanisms for Mission and Time Critical Applications in Industrial Internet of Things](https://arxiv.org/abs/2510.17009)
*Anwar Ahmed Khan,Shama Siddiqui,Indrakshi Dey*

Main category: cs.NI

TL;DR: FROG-MAC（基于分组的分段）在IIoT场景对比SS-MAC（插槽窃取）时，在紧急流量的等待时间方面更优，简单的分段方案可用于异构流量的高效调度。


<details>
  <summary>Details</summary>
Motivation: IIoT环境中设备产生多样数据、服务需求各异，MAC协议对传输效率影响显著，因此需要评估从简单到复杂的两类MAC策略的性能。

Method: 在Contiki仿真环境中对SS-MAC和FROG-MAC进行对比评估，比较两者在延迟和包丢失等指标上的表现。

Result: 结果表明：FROG-MAC在降低紧急流量等待时间方面具有优势，因其分组碎片化机制有利于对异构流量的调度。

Conclusion: 可以在工业环境中部署简单的分段方案，以提高对异构流量的调度效率。

Abstract: Industrial Internet of Things (IIoT) promises to revolutionize industrial
operations and productions through utilizing Machine-to-Machine (M2M)
communications. Since each node in such environments generates various types of
data with diverse service requirements, MAC protocol holds crucial importance
to ensure efficient delivery. In this context, simple to complex MAC schemes
are found in literature. This paper focuses on evaluating the performance of
two major techniques "slot stealing" and "packet fragmentation" for the IIoT;
representative protocols SS-MAC and FROG-MAC have been chosen from each
category respectively. We conducted realistic simulations for the two protocols
using Contiki. Delay and packet loss comparison for SS-MAC and FROG-MAC
indicates the superiority of FROG-MAC due to reduction in the waiting time for
urgent traffic. Thus, a simple fragmentation scheme could be deployed for
efficient scheduling of heterogenous traffic in the industrial environments.

</details>


### [218] [AoA Services in 5G Networks: A Framework for Real-World Implementation and Systematic Testing](https://arxiv.org/abs/2510.17342)
*Alberto Ceresoli,Viola Bernazzoli,Roberto Pegurri,Ilario Filippini*

Main category: cs.NI

TL;DR: Open-source 5G AoA testbed enabling lightweight, network-native localization with sub-degree to a few-degree accuracy.


<details>
  <summary>Details</summary>
Motivation: Centralized Location Management Function (LMF) in core networks faces scalability and latency issues, hindering low-latency, fine-grained localization. Shifting positioning intelligence toward the RAN using uplink SRS-based AoA offers a lightweight alternative.

Method: Development of the first fully open-source 5G AoA testbed. The framework integrates NVIDIA Sionna RT with a Keysight PROPSIM channel emulator and implements a novel phase calibration procedure for USRP N310 devices to enable accurate AoA estimation in a controllable, realistic channel environment.

Result: Experimental results show sub-degree to few-degree AoA/position estimation accuracy, validating the feasibility of lightweight, single-anchor, network-native localization in next-generation 5G systems.

Conclusion: Demonstrates feasibility of network-native, RAN-centric localization with high accuracy using an open-source testbed, enabling repeatable experimentation and system-level insights for future 5G deployments.

Abstract: Accurate positioning is a key enabler for emerging 5G applications. While the
standardized Location Management Function (LMF) operates centrally within the
core network, its scalability and latency limitations hinder low-latency and
fine-grained localization. A practical alternative is to shift positioning
intelligence toward the radio access network (RAN), where uplink sounding
reference signal (SRS)-based angle-of-arrival (AoA) estimation offers a
lightweight, network-native solution. In this work, we present the first fully
open-source 5G testbed for AoA estimation, enabling systematic and repeatable
experimentation under realistic yet controllable channel conditions. The
framework integrates the NVIDIA Sionna RT with a Keysight PROPSIM channel
emulator and includes a novel phase calibration procedure for USRP N310
devices. Experimental results show sub-degree to few-degree accuracy,
validating the feasibility of lightweight, single-anchor, network-native
localization within next-generation 5G systems.

</details>


### [219] [Enhancing 5G V2X Mode 2 for Sporadic Traffic](https://arxiv.org/abs/2510.17395)
*Dmitry Bankov,Artem Krasilov,Artem Otmakhov,Aleksei Shashin,Evgeny Khorov*

Main category: cs.NI

TL;DR: 针对5G V2X Mode 2在sporadic traffic场景下，提出若干改进策略以提升性能，仿真显示容量可提升高达40%，且复杂度影响较小。


<details>
  <summary>Details</summary>
Motivation: 满足车联网对低延迟和高可靠性的紧急信息传输需求，特别是在检测到危险情况时的单次数据传输；Mode 2让车辆自选资源，适合间歇性流量。

Method: 分析Mode 2在sporadic traffic下的性能，并提出多种改进方案，结合仿真实验评估效果。

Result: 所提改进方案在仿真中将系统容量提升至多40%，且对实现的复杂度影响较小。

Conclusion: 通过对Mode 2的改进，可在严格时延和可靠性要求下提升V2X的容量与鲁棒性，具有实际应用潜力。

Abstract: The emerging road safety and autonomous vehicle applications require timely
and reliable data delivery between vehicles and between vehicles and
infrastructure. To satisfy this demand, 3GPP develops a 5G
Vehicle-to-Everything (V2X) technology. Depending on the served traffic type,
5G V2X specifications propose two channel access methods: (i) Mode 1, according
to which a base station allocates resources to users, and (ii) Mode 2,
according to which users autonomously select resources for their transmissions.
In the paper, we consider a scenario with sporadic traffic, e.g., a vehicle
generates a packet at a random time moment when it detects a dangerous
situation, which imposes strict requirements on delay and reliability. To
satisfy strict delay requirements, vehicles use Mode 2. We analyze the
performance of Mode 2 for sporadic traffic and propose several approaches to
improve it. Simulation results show that the proposed approaches can increase
the system capacity by up to 40% with a low impact on complexity.

</details>


### [220] [Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?](https://arxiv.org/abs/2510.17410)
*Dmitry Bankov,Artem Krasilov,Artem Otmakhov,Pavel Savlukovich,Evgeny Khorov*

Main category: cs.NI

TL;DR: The paper studies how the feedback channel in 5G V2X affects system capacity in a scenario with platoon (groupcast) and surrounding vehicles (broadcast). Using NS-3 simulations, it finds that feedback can either significantly increase capacity (up to 2x) or almost halve it depending on platoon size, traffic intensities, and QoS; it proposes adaptive strategies for selecting feedback parameters.


<details>
  <summary>Details</summary>
Motivation:  addresses the resource trade-off between allocating resources to feedback vs data channels in 5G V2X, aiming to improve reliability and optimize overall capacity for new V2X scenarios like platooning and remote driving.

Method: NS-3-based simulations of a mixed traffic scenario with a platoon generating groupcast traffic and surrounding vehicles generating legacy broadcast traffic. The study varies platoon size, groupcast/broadcast traffic intensities, and QoS requirements, analyzes the impact of feedback channel usage on overall capacity, and discusses adaptive feedback parameter tuning.

Result: The results show that feedback usage can either boost capacity (up to ~2x) or substantially reduce it (near 50%) depending on scenario parameters. The paper explains the underlying mechanisms and identifies regimes where adaptive feedback control is beneficial.

Conclusion: Adaptive selection of feedback channel parameters is key to maximizing 5G V2X capacity in heterogeneous traffic scenarios; guidelines or mechanisms for dynamic tuning are needed to exploit the reported gains while avoiding large capacity losses.

Abstract: 5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to
support inter-vehicle communication. In contrast to 4G V2X which allows only
broadcast communication, 5G V2X enables groupcast and unicast communication.
Such types of communication are needed for new V2X scenarios: platooning,
extended sensors, remote driving, etc. To improve the data transmission
reliability and assist in the selection of the transmission parameters in these
scenarios, 5G V2X introduces a feedback channel that allows receivers to send
acknowledgments in response to data packets. However, some part of the overall
resource shall be allocated for the feedback channel, which reduces the amount
of channel resources available for data transmission. In this paper, we
consider a scenario with a platoon, which generates groupcast traffic, and
surrounding vehicles, which generate legacy broadcast traffic. Using extensive
simulations in NS-3, we analyze how the usage of the feedback channel
influences the overall system capacity. Our results show that depending on the
platoon size, groupcast, and broadcast traffic intensities, and their quality
of service requirements, the usage of the feedback channel can in some cases
significantly increase the system capacity (up to 2x), while in other cases it
almost halves the system capacity. We explain the reasons for such effects and
discuss how to adaptively select the feedback channel parameters.

</details>


### [221] [Pointing-Error-Induced Fading in an Open-Loop THz Uplink with Hardware Impairments](https://arxiv.org/abs/2510.17647)
*P. Brach del Prever,P. Testolina,A. Masihi,S. Petrushkevich,M. Polese,T. Melodia,J. M. Jornet*

Main category: cs.NI

TL;DR: 通过建立追踪系统的力学模型并耦合链路预算，定量分析 sub-THz/THz 上行在高频下的指向误差与通信性能，给出在实际机械约束下的设计指南。


<details>
  <summary>Details</summary>
Motivation: 高频上行需要极窄波束以克服传播损耗，但现实追踪系统存在延迟、加速度与速度限制，需要将机械约束纳入 NTN 链路设计。

Method: 建立考虑运动延迟、加速度与速度极限的追踪力学模型，并将其与链路预算耦合，评估不同LEO轨迹与控制策略下的指向误差及其对链路性能的影响。

Result: 提供指向误差的定量评估并将其映射到链路预算，揭示硬件约束对高频 NTN 上行通信可行性的影响，比较不同控制策略的性能差异。

Conclusion: 在实际机械约束条件下给出高频 NTN 上行的设计指南，明确窄波束下的指向容忍度、追踪带宽与控制策略对通信性能的影响。

Abstract: We analyze the open-loop mechanical tracking performance of a sub-Terahertz
(sub-THz) and Terahertz (THz) uplink communication system. These high-frequency
bands enable multi-gigabit links through large bandwidths and narrow beams, but
require precise pointing to overcome spreading loss. A tracking system can be
used to orient horn antennas toward mobile targets. We develop a mathematical
model that captures the mechanical dynamics of a real tracking system, which
includes motion latency and acceleration and velocity limits, to quantify
pointing errors during satellite passes and integrate these effects into the
link budget. We evaluate the trade-offs between beam directionality and
pointing tolerance across different Low Earth Orbit (LEO) satellite
trajectories and control strategies. The results link the hardware limitations
to the communications performance, providing design guidelines for
high-frequency Non-Terrestrial Network (NTN) uplink under practical mechanical
constraints.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [222] [Wideband Antenna Deconvolution for Bistatic Millimeter Wave Radar Reflectivity Measurements](https://arxiv.org/abs/2510.16094)
*Carsten Andrich,Isabella Varga,Tobias F. Nowack,Alexander Ihlow,Sebastian Giehl,Michael Schubert,Reiner S. Thomä,Matthias A. Hein*

Main category: eess.SP

TL;DR: 提出一种用于球形双基地雷达系统的空中标定算法，显著简化且速度比现有算法快两倍；在76–81 GHz 的金属球体反射率测量中实现动态范围提升约40 dB；测量结果与仿真高度一致。


<details>
  <summary>Details</summary>
Motivation: 双基地雷达测量的可靠性依赖于精确的系统和天线标定；现有的替代法需要已知参考对象，过程繁琐。需要一种更简单、快速且稳定的标定方法来提升测量的可靠性。

Method: 提出一种用于球形双基地测量系统的空中标定算法（OTA），相比现有方法具有更简单的实现且标定速度提升约两倍。

Result: 在76–81 GHz 频段对金属球体的反射测量中，动态范围提升可达40 dB；测量结果与仿真高度一致，显示良好的一致性。

Conclusion: 该空中标定方法可提升双基地雷达测量的可靠性，简化标定流程并显著加快标定速度，与仿真结果具有良好一致性。

Abstract: Bistatic radar measurements offer unique spatial diversity and enhanced
target characterization capabilities, rendering them increasingly vital for
contemporary sensing application research. The reliability of such measurements
is contingent upon precise system and antenna calibration. The prevailing
technique is the substitution method, which involves the use of known reference
objects. We propose an over-the-air calibration algorithm for spherical
bistatic measurement systems. Our method is both significantly simpler and
twice as fast as existing algorithms. The application of our technique to
reflectivity measurements of a metal sphere from 76 to 81 GHz demonstrates a
dynamic range enhancement of up to 40 dB when compared with uncalibrated data.
A comparison with simulation data demonstrates a high degree of agreement
between measurement and simulation.

</details>


### [223] [Fast, Differentiable, GPU-Accelerated Ray Tracing for Multiple Diffraction and Reflection Paths](https://arxiv.org/abs/2510.16172)
*Jérome Eertmans,Sophie Lequeu,Benoît Legat,Laurent Jacques,Claude Oestges*

Main category: eess.SP

TL;DR: 提出一种基于GPU的快照可微优化方法，用费马原理将射线路径跟踪中的路径最小化，并统一处理平面反射和直角衍射，利用隐式微分高效计算梯度，具有与牛顿法相近的收敛性并良好扩展性，易于与JAX/DrJIT等微分编程库结合，开源实现。


<details>
  <summary>Details</summary>
Motivation: 需要一种高效、可微且在GPU上并行执行的射线路径追踪方法，尤其是在包含平面反射体和直线衍射边界的环境中；希望统一处理反射与衍射，避免为不同交互序列使用分离的算法；并通过隐式微分实现对求解器迭代过程的梯度高效求取，以提升大规模应用的可扩展性与可微性。

Method: 基于费马原理，将路径搜索问题重构为总路径长度的最小化，适用于包含平面反射和直角衍射边的环境；引入统一的交互序列维数，利于向量化并行计算；通过隐式微分避免对求解器迭代步骤进行梯度传播，提升梯度计算效率；在GPU上实现并与现代微分编程库（如JAX、DrJIT）无缝集成。

Result: 数值仿真表明收敛性接近专门的牛顿法，同时在大规模场景下具有更好的可扩展性；梯度计算通过隐式微分实现高效，优于传统的自动微分方法；方法与 differentiable programming 库兼容，促进反问题与优化在无线传播建模中的应用。

Conclusion: 该方法提供一种快速、可微且可扩展的GPU路径追踪框架，统一处理反射与衍射，适合大规模逆设计与优化在无线传播领域的应用，并且开源实现便于研究与应用推广。

Abstract: We present a fast, differentiable, GPU-accelerated optimization method for
ray path tracing in environments containing planar reflectors and straight
diffraction edges. Based on Fermat's principle, our approach reformulates the
path-finding problem as the minimization of total path length, enabling
efficient parallel execution on modern GPU architectures. Unlike existing
methods that require separate algorithms for reflections and diffractions, our
unified formulation maintains consistent problem dimensions across all
interaction sequences, making it particularly suitable for vectorized
computation. Through implicit differentiation, we achieve efficient gradient
computation without differentiating through solver iterations, significantly
outperforming traditional automatic differentiation approaches. Numerical
simulations demonstrate convergence rates comparable to specialized Newton
methods while providing superior scalability for large-scale applications. The
method integrates seamlessly with differentiable programming libraries such as
JAX and DrJIT, enabling new possibilities in inverse design and optimization
for wireless propagation modeling. The source code is openly available at
https://github.com/jeertmans/fpt-jax.

</details>


### [224] [Performance Comparison of Joint Delay-Doppler Estimation Algorithms](https://arxiv.org/abs/2510.16200)
*Lorenz Mohr,Michael Döbereiner,Steffen Schieler,Joerg Robert,Christian Schneider,Sebastian Semper,Reiner S. Thoma*

Main category: eess.SP

TL;DR: 对三种延迟-多普勒估计算法（ML、CNN、CFAR）在公开通道数据上的性能进行基于测量的对比；在双基地场景中三者相似地实现参数估计，检测概率可达80%；正向/反向散射条件下LoS贡献过强，导致检测概率降至0%。


<details>
  <summary>Details</summary>
Motivation: 满足ISAC、雷达和波束赋形对实时高分辨率延迟-多普勒估计的需求，系统地比较三种不同算法在真实测量数据上的鲁棒性与效率。

Method: 在公开获取的通道数据上应用ML、CNN和CFAR三种算法，数据包含两个具解析描述的延迟-多普勒参数的球形目标；评估指标包括目标检测率、延迟-多普勒估计的RMSE，以及运行时开销；对比在双基地、以及包含前向/后向散射条件下的表现。

Result: 在双基地情形下，三种算法具有相近的参数估计能力，检测概率最高可达约80%；但在前向和后向散射条件下，LoS贡献过强，导致检测概率降至0%。

Conclusion: 在所考察的场景中，ML/CNN/CFAR在双基地通道中表现相对一致，但当存在强烈LoS及散射效应时，检测能力显著下降，提示需要针对LoS主导场景的鲁棒性改进或混合方法的探索。

Abstract: Integrated sensing and communications (ISAC), radar, and beamforming require
real-time, high-resolution estimation algorithms to determine delay-Doppler
values of specular paths within the wireless propagation channel. Our
contribution is the measurement-based performance comparison of the
delay-Doppler estimation between three different algorithms, comprising maximum
likelihood (ML), convolutional neural network (CNN), and constant false alarm
rate (CFAR) approaches. We apply these algorithms to publicly available channel
data which includes two spherical targets with analytically describable
delay-Doppler parameters. The comparison of the three algorithms features the
target detection rate, root mean squared errors (RMSEs) of the delay-Doppler
estimates, and a runtime analysis. Notably, all three algorithms demonstrate
similar parameter estimation capabilities in bi-static scenarios, achieving
target detection probabilities of up to 80%. Conversely, forward and backward
scattering conditions pose a problem to the estimation due to strong
line-of-sight (LoS) contribution, reducing the corresponding detection
probability down to 0%.

</details>


### [225] [Delay Minimization in Pinching-Antenna-enabled NOMA-MEC Networks](https://arxiv.org/abs/2510.16296)
*Yuan Ai,Xidong Mu,Pengbo Si,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出一种基于 PINCHING 天线的 NOMA-MEC 框架（PASS），通过联合优化 offloading 比例、发射功率与 PA 位置，并采用基于二分的交替优化以最小化最大任务延迟，实验结果显示相比基准方案显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 在多接入边缘计算场景中，NOMA 与硬件几何布局耦合导致的干扰与资源分配难题制约时延目标。引入新型 pinching antenna（PA）与 NOMA-MEC 联合优化以降低任务延迟并提升鲁棒性。

Method: 建立最小化最大任务延迟的优化问题，决策变量包括用户的 offloading 比例、发射功率和 PA 位置，受功率上限、能量预算和 PA 最小分离约束限制，以缓解耦合效应。提出基于二分搜索的交替优化算法，在给定目标延迟时依次求解各子问题并迭代，直至收敛。

Result: 数值仿真表明所提框架在多数场景中显著降低任务延迟，相较基准方案具有明显性能提升。

Conclusion: 通过将通信资源与硬件几何的联合优化应用于 NOMA-MEC，显著降低延迟并改善系统鲁棒性；提出的二分搜索-交替优化算法在给定目标延迟条件下具备良好收敛性并实现性能提升。

Abstract: This letter proposes a novel pinching antenna systems (PASS) enabled
non-orthogonal multiple access (NOMA) multi-access edge computing (MEC)
framework. An optimization problem is formulated to minimize the maximum task
delay by optimizing offloading ratios, transmit powers, and pinching antenna
(PA) positions, subject to constraints on maximum transmit power, user energy
budgets, and minimum PA separation to mitigate coupling effects. To address the
non-convex problem, a bisection search-based alternating optimization (AO)
algorithm is developed, where each subproblem is iteratively solved for a given
task delay. Numerical simulations demonstrate that the proposed framework
significantly reduces the task delay compared to benchmark schemes.

</details>


### [226] [Performance Evaluation of High Power Microwave Systems Against UAVs A Probabilistic Antenna Propagation Framework with Sensitivity Analysis](https://arxiv.org/abs/2510.16495)
*Muhammad Khalil,Ke Wang,Jinho Choi*

Main category: eess.SP

TL;DR: 提出了一个概率化、天线与传播相关的分析框架，用以评估高功率微波对无人机的致杀概率，结合UAV随机运动、波束抖动到增益的映射和大气传播，给出脉冲能量的闭式统计、逐脉冲及累计击杀概率的解析近似，以及在标准脉冲独立假设下的驻留时间表达式。与大规模蒙特卡洛验证吻合，揭示关键设计变量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在快速、物理可信的评估工具下实现HPM对UAV的打击性能预测，从而指导天线设计、传播建模和任务规划，减少对昂贵仿真或实地试验的依赖。

Method: 把随机UAV运动、波束抖动-增益映射、自由空间传播与大气损耗耦合，推导收到脉冲能量的对数正态分布近似，利用对数正态闭包和高斯-埃米特求积给出 per-pulse 与累计击杀概率的解析表达，并在脉冲独立性假设下给出驻留时间（dwell time）表达式；并用大规模蒙特卡洛验证分析。

Result: 分析结果与蒙特卡洛仿真在广泛参数范围内吻合；给出示例参数下的击杀概率：对阈值E_th=0.01 J，平均单脉冲击杀概率约>0.4、总击杀概率>99%在约0.1s，针对E_th=0.1 J的硬化平台，击杀概率显著下降（<1%单脉冲，<20%总）在1s内；敏感性分析表明斜距（S_bar_R ~ -2）为主导，天线口径和发射功率也有显著影响，指向抖动和大气变化在所评估区间影响相对较小。框架提供快速、物理可信的性能预测，给出天线/传播设计杠杆用于HPM系统尺寸与任务规划。

Conclusion: 该框架可作为快速、可信的射频武器对UAV的系统级评估工具，帮助设计者在鲁棒性、成本与风险之间权衡，并促进HPM系统的尺寸与配置优化。

Abstract: We develop a probabilistic, antenna- and propagation-centric framework to
quantify the effectiveness of high-power microwave (HPM) engagements against
unmanned aerial vehicles (UAVs). The model couples stochastic UAV kinematics, a
beam-steering jitter-to-gain mapping, and atmospheric propagation (free-space
spreading with gaseous and rain loss) to obtain closed-form statistics of the
received pulse energy. From these, we derive analytically evaluable per-pulse
and cumulative neutralization probabilities using log-normal closures and
Gaussian--Hermite quadrature, and we provide a dwell-time expression under a
standard pulse-independence assumption. Analytical predictions closely match
large-scale Monte-Carlo simulations across broad parameter ranges. For a
representative commercial threshold $E_{\mathrm{th}} = 10^{-2}\,\mathrm{J}$,
the model predicts $\bar{P}_{\mathrm{kill}} \gtrsim 0.4$ per pulse and
$P_{\mathrm{kill,tot}} > 99\%$ within about $0.1\,\mathrm{s}$ at kHz PRF; for
hardened platforms with $E_{\mathrm{th}} = 10^{-1}\,\mathrm{J}$,
$\bar{P}_{\mathrm{kill}} < 1\%$ and $P_{\mathrm{kill,tot}} < 20\%$ after
$1\,\mathrm{s}$. A closed-form sensitivity (elasticity) analysis shows
performance is dominated by slant range ($S_{\bar{R}} \approx -2$), with strong
secondary dependence on aperture diameter and transmit power; pointing jitter
and atmospheric variability are comparatively less influential in the evaluated
regimes. The framework yields fast, accurate, and physics-faithful performance
predictions and exposes clear antenna/propagation design levers for HPM system
sizing and risk-aware mission planning.

</details>


### [227] [Topology-Aware Hybrid Wi-Fi/BLE Fingerprinting via Evidence-Theoretic Fusion and Persistent Homology](https://arxiv.org/abs/2510.16557)
*Behrad Mousaei Shir-Mohammad,Behzad Moshiri,Abolfazl Yaghmaei*

Main category: eess.SP

TL;DR: 一个拓扑感知的混合Wi-Fi/BLE指纹定位框架，使用RSS归一化、贝叶斯滤波降噪、集成回归器、DST证据理论融合，以及持久同调描述符，能够在微控制器上实现低延迟的实时定位并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: GNSS不可用的室内场景中，多径效应、设备异构和无线条件波动导致定位难度大，需要一种鲁棒、可解释且低成本的定位方案，具有不确定性量化和可部署性。

Method: 将RSS归一化（dBm z-score 或 dBm->mW->z-score）、用经典贝叶斯滤波(KF/UKF/PF)对信号流降噪、将随机森林与带对角Mahalanobis距离的加权kNN等回归器进行融合、通过Dempster-Shafer证据理论进行证据融合，并为每个样本增加持久同调(PH)描述符；输出(x, y)坐标与可解释的置信度地图，且设计用于微控制器部署，单次更新复杂度为O(T log M + log M + Mp + S)。

Result: 在两个异构数据集上进行评估，包括一个新的1200个样本的ESP32调查数据集，进行了消融实验、对抗噪声鲁棒性测试以及10次分层拆分的显著性检验；在10% RSS噪声下，完整系统的RMSE分别为3.40 m（数据集1）和2.45 m（数据集2），比强基线PF+RF提升约37%；平均跨拆分结果为4.993±0.15 m，相比6.292±0.13 m提升约20.6%且p<0.001；无噪声时精度提升至0.44 m和0.32 m（最高提升56%）。与需要大规模数据集和GPU推理的学习型方法相比，所提方法在不确定性量化和低计算成本方面具有竞争力，且支持实时部署。

Conclusion: 该方法在提供准确室内定位与不确定性量化方面具有显著优势，能在GNSS受限环境中实现低成本、实时的部署；通过PH描述符捕捉结构性信息、DST实现可解释的证据融合，提升鲁棒性并给出可理解的信念地图，同时在跨场景的泛化方面表现良好。

Abstract: Indoor localization remains challenging in GNSS-denied environments due to
multipath, device heterogeneity, and volatile radio conditions. We propose a
topology-aware, hybrid Wi-Fi/BLE fingerprinting framework that (i) applies
physically consistent RSS normalization (dBm z-scoring or dBm -> linear mW ->
z-score), (ii) denoises streams with classical Bayesian filters (KF/UKF/PF),
(iii) combines complementary regressors (Random Forest and weighted kNN with a
diagonal Mahalanobis metric), (iv) performs evidence-theoretic fusion via
Dempster-Shafer theory (DST), and (v) augments each sample with
persistent-homology (PH) descriptors. The system outputs both (x, y) estimates
and interpretable belief maps, and is engineered for microcontroller-class
deployment with per-update cost O(T log M + log M + Mp + S).
  We evaluate on two heterogeneous datasets, including a new 1,200-sample ESP32
survey, and report ablations, robustness to test-only noise, and significance
across 10 stratified splits. Under 10% synthetic RSS noise, the full pipeline
attains 3.40 m (Dataset 1) and 2.45 m (Dataset 2) RMSE, improving a strong PF +
RF baseline by about 37%. Averaged across splits, it yields 4.993 +/- 0.15 m
versus 6.292 +/- 0.13 m (20.6% relative reduction; p < 0.001). In noise-free
tests, accuracy tightens to 0.44 m and 0.32 m (up to 56% better). Compared with
recent learning-heavy approaches that assume large site-specific datasets and
GPU inference, our method delivers competitive accuracy with formal uncertainty
quantification and low computational cost suitable for real-time deployment.

</details>


### [228] [Stochastic Geometry Analysis of Asymmetric Uplink Interference for Urban UAV-RC Networks](https://arxiv.org/abs/2510.16963)
*Donggu Lee,Sung Joon Maeng,Ismail Guvenc*

Main category: eess.SP

TL;DR: 利用对数高斯科克斯过程（LGCP）建立的对空间干扰相关性的随机几何框架，分析都市环境中无人机（UAV）在UL/DL两种传输模式下的大尺度不对称干扰，并给出UL干扰随高度和水平距离增大而恶化的结论。


<details>
  <summary>Details</summary>
Motivation: 在密集城市环境中，UAV用于公共安全与监控时，UL干扰因UAV端的LoS干扰更为显著而呈现不对称性，需要可 tractable 的大尺度干扰分析框架以指导资源分配和姿态/高度设计。

Method: 采用对数高斯科克斯过程（LGCP）建模干扰场的空间相关性，结合UAV高度和两维距离，构建UL与DL干扰的统计框架，定义干扰不对称比率以量化UL与DL之间的差异。

Result: 数值结果显示干扰不对称比率随UAV高度和2-D距离的增加而增大，意味着UL干扰相对DL更严重。该框架能够在大尺度层面上对都市环境下的UL/DL干扰进行可分析、可量化的比较。

Conclusion: 本文提出的LGCP随机几何框架为都市环境中UAV的UL/DL干扰不对称性提供了可解析的量化工具，帮助理解高度与距离对UL干扰的放大效应，并可用于指引系统设计与资源管理。

Abstract: Uncrewed aerial vehicles (UAVs) have emerged as a flexible platform for
providing coverage over challenging environments, particularly for public
safety and surveillance missions in urban areas. However, deploying the UAVs in
dense urban areas introduces unique challenges, most notably asymmetric uplink
(UL, remote controller to UAV) interference due to a higher chance of
line-of-sight (LoS) interference at the UAV. In this letter, we propose a
stochastic geometry framework to tractably analyze the large-scale asymmetric
interference in urban areas. We incorporate a log-Gaussian Cox process (LGCP)
model to capture the spatial correlation of the interference field in both UL
and downlink (DL) as a function of the UAV altitude and the two-dimensional
(2-D) distance between the remote controller and UAV. To quantify the UL and
the DL interference asymmetry, we also define the interference asymmetry ratio
characterizing the interference disparity between the UL and the DL. Our
numerical results demonstrate that the interference asymmetry ratio increases
as the UAV altitude and 2-D distance increase, highlighting that the UL
interference worsens.

</details>


### [229] [When 5G NTN Meets GNSS: Tracking GNSS Signals under Overlaid 5G Waveforms](https://arxiv.org/abs/2510.17324)
*Idir Edjekouane,Alejandro González Garrido,Jorge Querol,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 在近原生GNSS芯片的最小修改下，评估将5G OFDM 下行叠加的 DSSS 混合波形用于 GPS L1 C/A 的跟踪与导航信息解调的可行性，给出 BER 与子帧解码在LEO 动态下的性能表现。


<details>
  <summary>Details</summary>
Motivation: GNSS 容易受干扰，LEO 5G NTN 的联合通信与定位有潜在韧性提升。需定量分析在混合波形下 GNSS 跟踪和导航信息解调的影响，并评估在近原生 GNSS 芯片上的实现可能性。

Method: 将 GPS L1 C/A overlay 与 5G 帧对齐，将 5G 波形视为结构化干扰；使用最小修改的 GNSS 接收机进行跟踪，进行蒙特卡洛仿真，结合 LE0Dopper 动力学，评估比特误码率（BER）和子帧解码概率在不同信干噪比与干扰比（SINR/SIR）与不同动态类别下的表现。

Result: 在低到中等动态条件下，GNSS 跟踪与导航信息解调在较宽的 SINR 范围内仍然可靠；高动态条件下解锁/锁定能力受限，需要更严格的条件。研究证实使用近原生 GNSS 芯片并做少量修改即可实现联合通信与定位（JCAP）的可行性。

Conclusion: 该工作证明了在 LEO-NTN 场景下，将混合波形用于 GNSS 的可行性与潜在收益，但在高动态优先场景需更强的干扰抑制和鲁棒性设计。

Abstract: Global Navigation Satellite Systems (GNSS) provide the backbone of
Positioning, Navigation, and Timing (PNT) but remain vulnerable to
interference. Low Earth Orbit (LEO) constellations within Fifth-Generation (5G)
Non-Terrestrial Networks (NTN) can enhance resilience by jointly supporting
communication and navigation. This paper presents the first quantitative
analysis of GNSS tracking and navigation message demodulation under a hybrid
waveform where a low-power Direct-Sequence Spread Spectrum (DSSS) component is
overlaid on an Orthogonal Frequency-Division Multiplexing (OFDM) 5G downlink.
We evaluate a minimally modified GNSS receiver that tracks a legacy Global
Positioning System (GPS) L1 Coarse/Acquisition (C/A) overlay aligned with 5G
frames while treating the 5G waveform as structured interference. Using Monte
Carlo simulations under realistic LEO Doppler dynamics, we analyze the Bit
Error Rate (BER) of GPS L1 C/A navigation bits and the subframe decoding
probability versus Signalto- Interference-plus-Noise Ratio (SINR) for multiple
Signalto- Interference Ratios (SIR) and dynamic classes. Results show reliable
demodulation across wide SINR ranges for low and medium dynamics, whereas high
dynamics impose strict lock limits. These findings confirm the feasibility of
Joint Communication and Positioning (JCAP) using a near-legacy GNSS chipset
with minimal receiver modifications.

</details>


### [230] [A Bayesian Framework For Cascaded Channel Estimation in RIS-Aided mmWave Systems](https://arxiv.org/abs/2509.01117)
*Gyoseung Lee,Junil Choi*

Main category: eess.SP

TL;DR: 提出基于变分推断的复杂自适应Laplace先验，用以近似级联RIS信道增益的后验分布，从而提升RIS辅助毫米波多用户系统的级联信道估计精度；结果显示优于LS/LMMSE。


<details>
  <summary>Details</summary>
Motivation: 级联RIS信道的复杂且非高斯分布特征使线性估计（如LMMSE）性能受限，需要更灵活的概率建模以降低估计误差。

Method: 构建一个变分推断框架，采用复杂自适应Laplace先验来近似级联信道增益的后验分布，并推导相应的变分更新方程以实现高效推断。

Result: 数值仿真表明提出的估计器在级联信道估计误差方面优于传统的LS和LMMSE等方法。

Conclusion: 通过灵活的先验建模捕捉信道分布特征，该变分推断框架可提升RIS-aided毫米波多用户系统的级联信道估计性能，具有一定的实用潜力。

Abstract: In this paper, we investigate cascaded channel estimation for reconfigurable
intelligent surface (RIS)-aided millimeter-wave multi-user communication
systems. Since the complex channel gains of the cascaded RIS channel are
generally non-Gaussian, the use of the linear minimum mean squared error
(LMMSE) estimator leads to inevitable performance degradation. To tackle this
issue, we propose a variational inference-based framework that approximates the
complex channel gains using a complex adaptive Laplace prior, which effectively
captures their probability distributions in a tractable way. Numerical results
demonstrate that the proposed estimator outperforms conventional estimators
including least squares and LMMSE in terms of cascaded channel estimation
error.

</details>


### [231] [6D Movable Metasurface (6DMM) in Downlink NOMA Transmissions](https://arxiv.org/abs/2510.17502)
*Li-Hsiang Shen*

Main category: eess.SP

TL;DR: 6DMM辅助下行NOMA系统中，围绕六维可移动的可重构表面进行联合优化，以提升系统容量并通过CEO优化实现高维元件调度、相位位移与波束控制的协同效应；实验表明相较静态RIS与其他接入机制，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统RIS在表面静态、空间自由度有限，难以同时实现空间与电磁控制的耦合优化，限制了NOMA下行系统的性能潜力。引入六维可移动元表面可在yaw-pitch-roll三维旋转及位置调整上实现更丰富的自由度，理论上可提升覆盖、波束指向及信道条件的利用效率。

Method: 提出6DMM-NOMA体系，建立包含BS波束成形、RIS相位移、元件位置与姿态（ yaw/pitch/roll ）的联合优化问题，约束包括NOMA功率等级、单模相位、总功、单元间距及位置/朝向边界。由于问题非凸且维度较高，采用概率交叉熵优化（CEO）以迭代地更新解的分布，并通过对“精英解”的采样来提高收敛效率。

Result: 仿真结果显示，基于CEO的6DMM-NOMA在比6DMM子结构、传统静态RIS及其他多址方案更高的比特率/吞吐性能，且证实CEO在求解高维可扩展元表面优化中的有效性。

Conclusion: 6DMM结合CEO优化为高维、可调元表面的联合波束与姿态控制提供了有效的求解框架，显著提升NOMA场景下的系统性能并展示了动态六维表面对未来无线通信的潜力。

Abstract: This letter proposes a novel six-dimensional movable metasurface
(6DMM)-assisted downlink non-orthogonal multiple access (NOMA) system, in which
a conventional base station (BS) equipped with fixed antennas serves multiple
users with the assistance of a reconfigurable intelligent surface (RIS) with
six-dimensional spatial configurability. In contrast to traditional RIS with
static surface, the proposed 6DMM architecture allows each element to
dynamically adjust its position and orient the whole metasurface in
yaw-pitch-roll axes, enabling both in spatial and electromagnetic controls. We
formulate a sum-rate maximization problem that jointly optimizes the BS
NOMA-based beamforming, phase-shifts, element positions, and rotation angles of
metasurface under constraints of NOMA power levels, unit-modulus of
phase-shifts, power budget, inter-element separation and boundaries of element
position/orientation. Due to non-convexity and high-dimensionality, we employ a
probabilistic cross-entropy optimization (CEO) scheme to iteratively refine the
solution distribution based on maximizing likelihood and elite solution
sampling. Simulation results show that the proposed CEO-based 6DMM-NOMA
architecture achieves substantial rate performance gains compared to 6DMM
sub-structures, conventional static RIS, and other multiple access mechanisms.
It also highlights the effectiveness of CEO providing probabilistic
optimization for solving high-dimensional scalable metasurface.

</details>


### [232] [Sample Complexity Analysis of Multi-Target Detection via Markovian and Hard-Core Multi-Reference Alignment](https://arxiv.org/abs/2510.17775)
*Kweku Abraham,Amnon Balanov,Tamir Bendory,Carlos Esteve-Yagüe*

Main category: eess.SP

TL;DR: 提出一种将多目标检测MTD问题打patch成非iid MRA的框架：在一维中隐变量构成马尔科夫链，在二维中隐变量来自强混合随机场（硬核放置模型）。结果表明，在两维设定下，非iid MRA的估计收敛速率与对应iid MRA的速率一致（最多有对数因子）且对基于矩方法的估计同样成立；在低信噪比下，估计MTD所需的分片数按sigma^{2 n_min}增长，其中n_min是相关矩的阶数。


<details>
  <summary>Details</summary>
Motivation: 研究高噪声环境下多目标检测问题的样本复杂度，探究非独立同分布结构对估计速率的影响，并在一维/二维场景下比较非iid MRA与iid MRA的统计性能差异。

Method: 提出打patching方法将MTD降维到非iid的MRA模型。1D情形下隐变量形成马尔科夫链，2D情形下隐结构来自一个由硬核放置模型产生的、并且以指数混合的随机场。给出任意估计量的收敛速率在两种情形下与iid MRA一致（仅有对数因子差异），以及以经验平均/矩方法为基础的估计在两种情形下的等价性。推导低SNR下的分片需求与信号矩阶数n_min相关，呈sigma^{2 n_min}的增长。

Result: 1D与2D情形下，非iid MRA的估计收敛速率与对应iid MRA一致（最多包含对数因子）。对于基于矩方法的估计， iid 与非iid 情况下的速率也相同。低SNR下，MTD模型所需分片数表现为sigma^{2 n_min}，其中n_min为信号由矩决定的最小阶数。

Conclusion: 打patching 的非iid MRA框架在MTD场景下能实现接近iid MRA的统计效率，给出低信噪比条件下的分片数量定量关系，便于实际算法设计与理解非iid结构对样本复杂度的影响。

Abstract: Motivated by single-particle cryo-electron microscopy, we study the sample
complexity of the multi-target detection (MTD) problem, in which an unknown
signal appears multiple times at unknown locations within a long, noisy
observation. We propose a patching scheme that reduces MTD to a non-i.i.d.
multi-reference alignment (MRA) model. In the one-dimensional setting, the
latent group elements form a Markov chain, and we show that the convergence
rate of any estimator matches that of the corresponding i.i.d. MRA model, up to
a logarithmic factor in the number of patches. Moreover, for estimators based
on empirical averaging, such as the method of moments, the convergence rates
are identical in both settings. We further establish an analogous result in two
dimensions, where the latent structure arises from an exponentially mixing
random field generated by a hard-core placement model. As a consequence, if the
signal in the corresponding i.i.d. MRA model is determined by moments up to
order $n_{\min}$, then in the low-SNR regime the number of patches required to
estimate the signal in the MTD model scales as $\sigma^{2n_{\min}}$, where
$\sigma^2$ denotes the noise variance.

</details>


### [233] [Precoding for Uplink RIS-Assisted Cell-Free MIMO-OFDM Systems with Hardware Impairments](https://arxiv.org/abs/2510.17741)
*Navid Reyhanian,Reza Ghaderi Zefreh,Parisa Ramezani,Emil Bjornson*

Main category: eess.SP

TL;DR: 提出基于加权最小均方误差(WMMSE)的区块坐标下降(BCD)算法，用于在多 RIS 的RIS辅助CF-mMIMO系统中联合设计发射前瞻编码、RIS系数与接收结合，以在用户设备和AP处存在IQI的情况下实现上行汇总增益最大化，并通过仿真实验显示相对于启发式方法的效率提升。


<details>
  <summary>Details</summary>
Motivation: 解决在RIS辅助的CF-mMIMO系统中，在存在IQI等硬件不理想的情况下实现上行汇总和传输性能的联合设计问题，且多 RIS 情况增加了联合优化的复杂度，需要高效的迭代求解方法。

Method: 提出基于WMMSE的区块坐标下降(BCD)框架，设计并实现用于求解发射前瞻、RIS相位系数和接收结合子问题的创新迭代步骤，考虑多 RIS 情况及IQI约束，使得子问题可高效求解。

Result: 通过大量仿真实验，与启发式方法比较，所提算法在上行汇总速率和收敛性等方面表现出明显优势。

Conclusion: 所提出的WMMSE-BCD方法能够有效解决在多 RIS 的RIS辅助CF-mMIMO系统中存在IQI时的联合设计问题，显著提升上行汇总性能并具有较好的计算效率。

Abstract: This paper studies a reconfigurable intelligent surface (RIS)-assisted
cell-free massive multiple-input multiple-output (CF-mMIMO) system with
multiple RISs. Joint design of transmit precoding, RIS coefficients, and
receive combining is investigated for uplink sum-rate maximization under
in-phase and quadrature phase imbalance (IQI) at user equipments (UEs) and
access points (APs). A weighted minimum mean squared error (WMMSE) based block
coordinate descent (BCD) approach is proposed, where novel iterative methods
are developed to efficiently solve the BCD subproblems. The efficiency of
proposed approaches is demonstrated relative to heuristic methods via extensive
simulations.

</details>
