<div id=toc></div>

# Table of Contents

- [eess.SY](#eess.SY) [Total: 5]
- [cs.IT](#cs.IT) [Total: 26]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 48]
- [eess.SP](#eess.SP) [Total: 11]


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [1] [RIS-Aided E2E Multi-Path Uplink Transmission Optimization for 6G Time-Sensitive Services](https://arxiv.org/abs/2601.09058)
*Zisheng Gong,Ziyue Xiao,Liu Cao,Zhaoyu Liu,Dongyu Wei,Lyutianyang Zhang*

Main category: eess.SY

TL;DR: 利用RIS联合优化多路径上行链路参数，显著降低6G系统时延。


<details>
  <summary>Details</summary>
Motivation: 现有ATSSS多路径操作难以满足6G时延敏感服务的更严格QoS需求，亟需新的技术手段降低E2E时延。

Method: 提出一种交替优化框架，联合调节上行流量分割、发射功率、接收组合与RIS相位，实现在实际约束下的E2E时延最小化。

Result: 相较于基线方案，单用户平均E2E时延降低了约43%，整个系统降低约15%。

Conclusion: 所提出的RIS辅助E2E多路径上行链路框架通过联合优化传输参数显著降低了平均时延，验证了其在6G时延敏感服务中的有效性。

Abstract: The Access Traffic Steering, Switching, and Splitting (ATSSS) defined in the 3GPP latest Release enables traffic flow over the multiple access paths to achieve the lower-latency End-to-end (E2E) delivery for 6G time-sensitive services. However, the existing E2E multi-path operation often falls short of more stringent QoS requirements for 6G time-sensitive services. This proposes a Reconfigurable Intelligent Surfaces (RIS)-aided E2E multi-path uplink(UL) transmission architecture that explicitly accounts for both radio link latency and N3 backhaul latency, via the coupled designs of the UL traffic-splitting ratio, transmit power, receive combining, and RIS phase shift under practical constraints to achieve the minimum average E2E latency. We develop an alternating optimization framework that updates the above target parameters to be optimized. The simulations were conducted to compare the effectiveness of the proposed E2E optimization framework that lowers the average E2E latency up to 43% for a single user and 15% for the whole system compared with baselines.

</details>


### [2] [Dynamic Association of Semantics and Parameter Estimates by Filtering](https://arxiv.org/abs/2601.09158)
*Marcus Greiff,Ray Zhang,Thomas Lew,John Subosits*

Main category: eess.SY

TL;DR: 一种低复杂度多参数概率语义过滤框架，通过贝叶斯矩匹配高效更新，显著提升驾驶场景中的参数-语义关联推断。


<details>
  <summary>Details</summary>
Motivation: 在动态系统参数推断与地图语义类关联的过程中，传统方法无法有效处理多参数、时变关联及计算复杂度高的问题，特别是在驾驶场景下需要快速响应。

Method: 构建一种概率语义过滤框架，定义了将参数与语义类紧耦合的后验分布，并设计了基于地图动态状态的递推过滤器。通过贝叶斯矩匹配技术，将观测更新的计算复杂度降至参数维数线性。

Result: 实验验证表明，该框架在计算效率上优于现有方法，且在驾驶案例中能够更准确地捕捉参数与语义之间的随时间变化的关联关系，揭示传统方法的局限性。

Conclusion: 本文提出的多参数概率语义过滤方法在保持低计算复杂度的同时，有效提升了时变参数-语义关联推断的准确性，适用于需要快速决策的驾驶等动态环境。

Abstract: We propose a probabilistic semantic filtering framework in which parameters of a dynamical system are inferred and associated with a closed set of semantic classes in a map. We extend existing methods to a multi-parameter setting using a posterior that tightly couples semantics with the parameter likelihoods, and propose a filter to compute this posterior sequentially, subject to dynamics in the map's state. Using Bayesian moment matching, we show that the computational complexity of measurement updates scales linearly in the dimension of the parameter space. Finally, we demonstrate limitations of applying existing methods to a problem from the driving domain, and show that the proposed framework better captures time-varying parameter-to-semantic associations.

</details>


### [3] [Boundary adaptive observer design for semilinear hyperbolic rolling contact ODE-PDE systems with uncertain friction](https://arxiv.org/abs/2601.09223)
*Luigi Romano,Ole Morten Aamo,Miroslav Krstić,Jan Åslund,Erik Frisk*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents an adaptive observer design for semilinear hyperbolic rolling contact ODE-PDE systems with uncertain friction characteristics parameterized by a matrix of unknown coefficients appearing in the nonlinear (and possibly non-smooth) PDE source terms. Under appropriate assumptions of forward completeness and boundary sensing, an adaptive observer is synthesized to simultaneously estimate the lumped and distributed states, as well as the uncertain friction parameters, using only boundary measurements. The observer combines a finite-dimensional parameter estimator with an infinite-dimensional description of the state error dynamics, and achieves exponential convergence under persistent excitation. The effectiveness of the proposed design is demonstrated in simulation by considering a relevant example borrowed from road vehicle dynamics.

</details>


### [4] [On Discrete Age of Information of Status Updating System With General Packet Arrival Processes](https://arxiv.org/abs/2601.09302)
*Jixiang Zhang,Han Xu,Daming Cao,Yinfei Xu,Minghao Chen,Chengyu Lin*

Main category: eess.SY

TL;DR: 本文利用PGF方法，完整推导了G/G/1/1（预emption）和G/Geo/1/1（非预emption）离散AoI的分布和均值，弥补了此前研究中的空白


<details>
  <summary>Details</summary>
Motivation: 需要研究具有任意到达与服务过程的G/G/1/1队列中离散AoI的分布；此前仅有连续AoI相关结果

Method: 通过概率生成函数PGF展开，分别推导预emption和非preemption情形下，G/G/1/1以及G/Geo/1/1的离散AoI PGF

Result: 给出完整的PGF表达式，计算平均离散AoI，并对结果进行讨论

Conclusion: 成功填补了G/G/1/1预emption离散AoI分析缺口，拓展并完善了现有理论，可为状态更新系统的设计提供更精准的性能评估

Abstract: Characterizing Age of Information (AoI) in status updating systems with general arrival and service processes has great significance considering that the interarrival and service time of updates can possibly be arbitrary in a real world. While expressions of average continuous AoI under G/G/1/1 queues have been derived in the paper by Soysal and Ulukus, the discrete case remained unsolved. To address it, this paper gives a fully characterization of probability generation functions (PGF) of discrete AoI under G/G/1/1 settings when preemption is allowed. In the non-preemptive case, this paper gives the expressions of PGF of discrete AoI under G/Geo/1/1 settings, which also extends the former results. The average discrete AoI is derived and discussed based on these new theoretical findings.

</details>


### [5] [Residual Power Flow for Neural Solvers](https://arxiv.org/abs/2601.09533)
*Jochen Stiasny,Jochen Cremer*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The energy transition challenges operational tasks based on simulations and optimisation. These computations need to be fast and flexible as the grid is ever-expanding, and renewables' uncertainty requires a flexible operational environment. Learned approximations, proxies or surrogates -- we refer to them as Neural Solvers -- excel in terms of evaluation speed, but are inflexible with respect to adjusting to changing tasks. Hence, neural solvers are usually applicable to highly specific tasks, which limits their usefulness in practice; a widely reusable, foundational neural solver is required. Therefore, this work proposes the Residual Power Flow (RPF) formulation. RPF formulates residual functions based on Kirchhoff's laws to quantify the infeasibility of an operating condition. The minimisation of the residuals determines the voltage solution; an additional slack variable is needed to achieve AC-feasibility. RPF forms a natural, foundational subtask of tasks subject to power flow constraints. We propose to learn RPF with neural solvers to exploit their speed. Furthermore, RPF improves learning performance compared to common power flow formulations. To solve operational tasks, we integrate the neural solver in a Predict-then-Optimise (PO) approach to combine speed and flexibility. The case study investigates the IEEE 9-bus system and three tasks (AC Optimal Power Flow (OPF), power-flow and quasi-steady state power flow) solved by PO. The results demonstrate the accuracy and flexibility of learning with RPF.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [6] [Two-dimensional Entanglement-assisted Quantum Quasi-cyclic Low-density Parity-check Codes](https://arxiv.org/abs/2601.08927)
*Pavan Kumar,Shayan Srinivasa Garani*

Main category: cs.IT

TL;DR: 用张量堆叠设计二维准循环LDPC码，获得girth>4或>6，擦除修正$\ge p^2$；基于此构造的EA-QLDPC码仅需要1个ebit，保持相同擦除性能


<details>
  <summary>Details</summary>
Motivation: 探索二维准循环LDPC码在图形模型中短环结构的条件，以提升编码性能，特别是在量子纠错领域

Method: 推导2g-周期存在的通用条件，利用$p\times p\times p$张量堆叠构造满足girth>4的码；对合成$p$提出两种额外堆叠方案，实现girth>4和girth>6；基于这些经典码，分别从两对或单个经典码构造2‑D EA‑QLDPC码，确保无4‑环并只需共享1个ebit

Result: 获得一族girth>4的二维准循环LDPC码，并在合成$p$时得到girth>4或girth>6族；所有码的擦除纠正能力≥$p\times p$；相应的EA‑QLDPC码同样拥有该擦除纠正能力，且仅需单个ebit

Conclusion: 证明了通过张量堆叠可高效构造高girth、强擦除纠错的二维经典与EA量子LDPC码，为量子错误控制提供了新的实用方案

Abstract: For any positive integer $g \ge 2$, we derive general conditions for the existence of a $2g$-cycle in the Tanner graph of two-dimensional ($2$-D) classical quasi-cyclic (QC) low-density parity-check (LDPC) codes. Based on these conditions, we construct a family of $2$-D classical QC-LDPC codes with girth greater than $4$ by stacking $p \times p \times p$ tensors, where $p$ is an odd prime. Furthermore, for composite values of $p$, we propose two additional families of $2$-D classical LDPC codes obtained via similar tensor stacking. In this case, one family achieves girth greater than $4$, while the other attains girth greater than $6$. All the proposed $2$-D classical QC-LDPC codes exhibit an erasure correction capability of at least $p \times p$. Based on the constructed classical $2$-D QC-LDPC codes, we derive two families of $2$-D entanglement-assisted (EA) quantum low-density parity-check (QLDPC) codes. The first family of $2$-D EA-QLDPC codes is obtained from a pair of binary $2$-D classical LDPC codes and is designed such that the unassisted part of the Tanner graph of the resulting EA-QLDPC code is free of cycles of length four, while requiring only a single ebit to be shared across the quantum transceiver. The second family is constructed from a single $2$-D classical LDPC code whose Tanner graph is free from $4$-cycles. Moreover, the constructed EA-QLDPC codes inherit an erasure correction capability of $p \times p$, as the underlying classical codes possess the same erasure correction property.

</details>


### [7] [On the Information Leakage Envelope of the Gaussian Mechanism](https://arxiv.org/abs/2601.08986)
*Sara Saeidian*

Main category: cs.IT

TL;DR: Paper derives a closed-form pointwise maximal leakage bound for the Gaussian mechanism with Gaussian secret, then shows strongly log-concave priors satisfy a condition ensuring the same bound applies to all unbounded secrets. 


<details>
  <summary>Details</summary>
Motivation: Characterize the smallest information leakage bound under arbitrary post-processing for the Gaussian mechanism, especially for small failure probabilities and non-Gaussian secrets.

Method: Derive closed-form expression for deterministic PML envelope for Gaussian secret, then extend to general unbounded secrets by establishing a sufficient condition; prove strongly log-concave priors satisfy it using Brascamp–Lieb inequality.

Result: Closed-form deterministic PML envelope for Gaussian secret (small failure prob), generalization to unbounded priors with sufficient condition satisfied by strongly log-concave priors via Brascamp–Lieb inequality.

Conclusion: Strongly log-concave priors satisfy the sufficient condition for the deterministic pointwise maximal leakage envelope of the Gaussian mechanism to coincide with the Gaussian case, yielding a closed-form envelope for small failure probabilities.

Abstract: We study the pointwise maximal leakage (PML) envelope of the Gaussian mechanism, which characterizes the smallest information leakage bound that holds with high probability under arbitrary post-processing. For the Gaussian mechanism with a Gaussian secret, we derive a closed-form expression for the deterministic PML envelope for sufficiently small failure probabilities. We then extend this result to general unbounded secrets by identifying a sufficient condition under which the envelope coincides with the Gaussian case. In particular, we show that strongly log-concave priors satisfy this condition via an application of the Brascamp-Lieb inequality.

</details>


### [8] [An Information-Theoretic Perspective on LLM Tokenizers](https://arxiv.org/abs/2601.09039)
*Mete Erdogan,Abhiram Gorle,Shubham Chandak,Mert Pilanci,Tsachy Weissman*

Main category: cs.IT

TL;DR: 研究表明，大语言模型词表可视为结构化压缩器，训练规模提升可产生更短序列但更易预测的标记流；对不同词表进行压缩实验，并设计容量利用指标与压缩感知BPE，揭示压缩、结构与域适应的权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然词表在LLM管道中起关键作用，但其压缩与结构化特性的内在关系尚未清晰，尤其在训练规模扩大和域迁移时的表现差异值得探究。

Method: 通过将预训练的GPT族词表作为黑盒压缩器与不同配置的学习词表（词表大小、训练规模、域）进行对比实验；利用通道镜头和容量利用率指标评估词表行为，并构造压缩感知的BPE变体进行实验验证。

Result: 实验显示，当训练数据规模增大时，整体词序列熵升高但上下文可预测性增强；词表引入了短程规律并提高压缩率；但在训练-测试域不匹配时收益下降；压缩感知BPE实现了更平衡的容量使用。

Conclusion: 本文揭示了分词器在压缩效率、结构影响和域差异鲁棒性方面的折衷，并提出了压缩感知的BPE改进方案，推动构建更高效、可解释且稳健的LLM词表。

Abstract: Large language model (LLM) tokenizers act as structured compressors: by mapping text to discrete token sequences, they determine token count (and thus compute and context usage) and the statistical structure seen by downstream models. Despite their central role in LLM pipelines, the link between tokenization, compression efficiency and induced structure is not well understood. We empirically demonstrate that tokenizer training scale redistributes entropy: as training data grows, the token stream becomes more diverse in aggregate (higher unigram entropy) yet markedly more predictable in-context (lower higher-order conditional entropies), indicating that tokenization absorbs substantial short-range regularity although these gains degrade under train-test domain mismatch. To ground these observations, we first benchmark i) pretrained GPT-family tokenizers as black-box compressors across various domains, and ii) learned tokenizers across configurations spanning vocabulary size, training scale, and domain. Next, we study tokenization as a transform for universal compression and introduce a compression-aware BPE variant. Finally, we adopt a channel lens and introduce capacity-utilization metrics to analyze tokenizer behaviour and outline implications for downstream modeling. Put together, our results expose various trade-offs between compression, induced structure, and robustness under domain shift, and motivate principled, compression-aware tokenizer design.

</details>


### [9] [Hybrid Mono- and Bi-static OFDM-ISAC via BS-UE Cooperation: Closed-Form CRLB and Coverage Analysis](https://arxiv.org/abs/2601.09057)
*Xiaoli Xu,Yong Zeng*

Main category: cs.IT

TL;DR: 基站与UE协同的混合单/双静感知框架，在不增加频谱或协调成本的前提下，可实现更优的目标定位和速度估计，并对覆盖与密度具有可量化的影响。


<details>
  <summary>Details</summary>
Motivation: 在ISAC系统中实现兼顾单、双静感知优势的同时，避免额外频谱消耗和小区间协调成本，探索基站UE协同方式的性能极限。

Method: 构建基于3GPP支持的混合单/双静感知框架，无需额外频谱或多小区协调；推导以目标与UE位置为函数的CRLB；使用有效参数估计算法和加权MSE融合方法进行仿真验证；通过改变UE位置和密度，分析覆盖率与最佳UE选择下的感知准确率。

Result: CRLB与仿真结果表明，混合模式在目标定位与速度估计误差上显著优于纯单静或双静；覆盖率随BS-UE距离先提升后下降；最佳UE选择下的感知准确率随UE密度递增，验证了密度对感知性能的正向影响。

Conclusion: 该论文证明了基站和用户设备协同的混合单静与双静感知模式在目标定位与速度估计方面可实现显著性能提升，尤其当BS-目标-UE构成近似直角三角形时更为突出。通过推导闭式Cramér‑Rao下界、仿真验证以及加权MSE融合，展示了混合模式优于单静或双静模式的原理和实际效果，并进一步分析了不同UE位置及密度对感知覆盖与准确率的影响，为ISAC系统设计提供了量化依据。

Abstract: This paper proposes a hybrid mono- and bi-static sensing framework, by leveraging the base station (BS) and user equipment (UE) cooperation in integrated sensing and communication (ISAC) systems. This scheme is built on 3GPP-supported sensing modes, and it does not incur any extra spectrum cost or inter-cell coordination. To reveal the fundamental performance limit of the proposed hybrid sensing mode, we derive closed-form Cramér-Rao lower bound (CRLB) for sensing target localization and velocity estimation, as functions of target and UE positions. The results reveal that significant performance gains can be achieved over the purely mono- or bi-static sensing, especially when the BS-target-UE form a favorable geometry, which is close to a right triangle. The analytical results are validated by simulations using effective parameter estimation algorithm and weighted mean square error (MSE) fusion method. Based on the derived sensing bound, we further analyze the sensing coverage by varying the UE positions, which shows that sensing coverage first improves then degrades as the BS-UE separation increases. Furthermore, the sensing accuracy for a potential target with best UE selection is derived as a function of the UE density in the network.

</details>


### [10] [Overcoming the Shadow: Bending Airy Beams for Radiative Near-Field Multi-User Access in Half-Space Blockage Scenarios](https://arxiv.org/abs/2601.09098)
*Yifeng Qin,Jing Chen,Zhi Hao Jiang,Zhi Ning Chen,Yongming Huang*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The move to next-generation wireless communications with extremely large-scale antenna arrays (ELAAs) brings the communications into the radiative near-field (RNF) region, where distance-aware focusing is feasible. However, high-frequency RNF links are highly vulnerable to blockage in indoor environments dominated by half-space obstacles (walls, corners) that create knife-edge shadows. Conventional near-field focused beams offer high gain in line-of-sight (LoS) scenarios but suffer from severe energy truncation and effective-rank collapse in shadowed regions, making hardware remedies such as reconfigurable intelligent surfaces (RIS) impractical. We propose a beamforming strategy that exploits the auto-bending property of Airy beams to mitigate half-space blockage without additional hardware. The Airy beam is designed to ``ride'' the diffraction edge, accelerating its main lobe into the shadow to restore connectivity. Our contributions are threefold: (i) a Green's function-based RNF multi-user channel model that analytically reveals singular-value collapse behind knife-edge obstacles; (ii) an Airy analog beamforming scheme that optimizes the bending trajectory to recover the effective channel rank; and (iii) an Airy null-steering method that aligns oscillatory nulls with bright-region users to suppress interference in mixed shadow/bright scenarios. Simulations show that the proposed edge-riding Airy strategy achieves an SNR improvement of over 20 dB and restores full-rank connectivity in shadowed links compared to conventional RNF focusing, virtually eliminating outage in geometric shadows and increasing multi-user spectral efficiency by approximately 35\% under typical indoor ELAA configurations. These results demonstrate robust RNF multi-user access in half-space blockage scenarios without relying on RIS.

</details>


### [11] [The .serva Standard: One Primitive for All AI Cost Reduced, Barriers Removed](https://arxiv.org/abs/2601.09124)
*Rachel St. Clair,John Austin Cook,Peter Sutor,Victor Cavero,Garrett Mindt*

Main category: cs.IT

TL;DR: ServaStack: .serva 格式+Chimera 引擎，压缩+直接计算，显著提升能效、存储、通用性，节省成本。


<details>
  <summary>Details</summary>
Motivation: 传统AI基础设施面临能量与资本成本飙升、数据预处理耗时占比高、生态复杂化等双重危机，需要统一解决机制。

Method: 设计.lossless 的.serva 数据格式，采用激光全息原理实现压缩；开发 Chimera 计算引擎，将算子映射至代表空间，直接对.serva 文件进行无压缩计算；实现无预训练模型时自动预处理，保持 기존模型性能。

Result: 至少 30-374 倍能效提升（96-99% 降能耗），4-34 倍无损存储压缩，68 倍计算负载下降；在 FashionMNIST/MNIST 上保持准确性，假设每十亿日常迭代可节省约 485 万美元/训练周期。

Conclusion: ServaStack将数据格式与计算引擎统一化，显著提升AI基础设施的能效与存储压缩，提高任何模型在任何硬件上的通用性，推动AI研发从基础设施瓶颈转向创意能力的提升。

Abstract: Artificial Intelligence (AI) infrastructure faces two compounding crises. Compute payload - the unsustainable energy and capital costs of training and inference - threatens to outpace grid capacity and concentrate capability among a handful of organizations. Data chaos - the 80% of project effort consumed by preparation, conversion, and preprocessing - strangles development velocity and locks datasets to single model architectures. Current approaches treat these as separate problems, managing each with incremental optimization while increasing ecosystem complexity. This paper presents ServaStack: a universal data format (.serva) paired with a universal AI compute engine (Chimera). The .serva format achieves lossless compression by encoding information using laser holography principles, while Chimera converts compute operations into a representational space where computation occurs directly on .serva files without decompression. The result is automatic data preprocessing. The Chimera engine enables any existing model to operate on .serva data without retraining, preserving infrastructure investments while revamping efficiency. Internal benchmarks demonstrate 30-374x energy efficiency improvements (96-99% reduction), 4x-34x lossless storage compression, and 68x compute payload reduction without accuracy loss when compared to RNN, CNN, and MLP models on FashionMNIST and MNIST datasets. At hyperscale with one billion daily iterations, these gains translate to $4.85M savings per petabyte per training cycle. When any data flows to any model on any hardware, the AI development paradigm shifts. The bottleneck moves from infrastructure to imagination.

</details>


### [12] [Reducing The Sub-packetization Level of Optimal-Access Cooperative MSR Codes](https://arxiv.org/abs/2601.09188)
*Yaqian Zhang,Jingke Xu*

Main category: cs.IT

TL;DR: 提出一种构造，针对两失效节点的最优访问信赖 MSR 码，子分割尺寸从 \(r^{\binom{n}{2}}\) 降至 \(r^{\binom{n}{2}-\lfloor\frac{n}{r}\rfloor(\binom{r}{2}-1)}\)，显著降低复杂度和 I/O。


<details>
  <summary>Details</summary>
Motivation: 在最优访问信赖的 MSR 码中，较大的子分割尺寸会导致实现复杂度升高与 I/O 开销增大；因此需要一种方法在保持最优性和两节点修复的前提下降低子分割尺寸。

Method: 首先设计两种关键的 MDS 数组码以满足特定的两节点修复模式；随后将这两种码多次堆叠，构成最终满足最优访问信用的 MSR 码。

Result: 得到的子分割尺寸为 \(\ell = r^{\binom{n}{2}-\lfloor\frac{n}{r}\rfloor(\binom{r}{2}-1)}\)，相比现有的 \(\ell = r^{\binom{n}{2}}\) 减少了一个比例为 \(1/r^{\lfloor\frac{n}{r}\rfloor(\binom{r}{2}-1)}\) 的量。

Conclusion: 本文提出并实现了一种针对两节点故障的最优访问信用MSR码的新构造，显著降低了子分割尺寸，从而减少了实际实现中的复杂度与 I/O 开销。

Abstract: Cooperative MSR codes are a kind of storage codes which enable optimal-bandwidth repair of any $h\geq2$ node erasures in a cooperative way, while retaining the minimum storage as an $[n,k]$ MDS code. Each code coordinate (node) is assumed to store an array of $\ell$ symbols, where $\ell$ is termed as sub-packetization. Large sub-packetization tends to induce high complexity, large input/output in practice. To address the disk IO capability, a cooperative MSR code is said to have optimal-access property, if during node repair, the amount of data accessed at each helper node meets a theoretical lower bound.
  In this paper, we focus on reducing the sub-packetization of optimal-access cooperative MSR codes with two erasures. At first, we design two crucial MDS array codes for repairing a specific repair pattern of two erasures with optimal access. Then, using the two codes as building blocks and by stacking up of the two codes for several times, we obtain an optimal-access cooperative MSR code with two erasures. The derived code has sub-packetization $\ell=r^{\binom{n}{2}-\lfloor\frac{n}{r}\rfloor(\binom{r}{2}-1)}$ where $r=n-k$, and it reduces $\ell$ by a fraction of $1/r^{\lfloor\frac{n}{r}\rfloor(\binom{r}{2}-1)}$ compared with the state of the art ($\ell=r^{\binom{n}{2}}$).

</details>


### [13] [On Polar Coding with Feedback](https://arxiv.org/abs/2601.09222)
*Ling Liu,Qi Cao,Liping Li,Baoming Bai*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we investigate the performance of polar codes with the assistance of feedback in communication systems. Although it is well known that feedback does not improve the capacity of memoryless channels, we show that the finite length performance of polar codes can be significantly improved as feedback enables genie-aided decoding and allows more flexible thresholds for the polar coding construction. To analyze the performance under the new construction, we then propose an accurate characterization of the distribution of the error event under the genie-aided successive cancellation (SC) decoding. This characterization can be also used to predict the performance of the standard SC decoding of polar codes with rates close to capacity.

</details>


### [14] [A Theoretical Framework for Rate-Distortion Limits in Learned Image Compression](https://arxiv.org/abs/2601.09254)
*Changshuo Wang,Zijian Liang,Kai Niu,Ping Zhang*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a novel systematic theoretical framework to analyze the rate-distortion (R-D) limits of learned image compression. While recent neural codecs have achieved remarkable empirical results, their distance from the information-theoretic limit remains unclear. Our work addresses this gap by decomposing the R-D performance loss into three key components: variance estimation, quantization strategy, and context modeling. First, we derive the optimal latent variance as the second moment under a Gaussian assumption, providing a principled alternative to hyperprior-based estimation. Second, we quantify the gap between uniform quantization and the Gaussian test channel derived from the reverse water-filling theorem. Third, we extend our framework to include context modeling, and demonstrate that accurate mean prediction yields substantial entropy reduction. Unlike prior R-D estimators, our method provides a structurally interpretable perspective that aligns with real compression modules and enables fine-grained analysis. Through joint simulation and end-to-end training, we derive a tight and actionable approximation of the theoretical R-D limits, offering new insights into the design of more efficient learned compression systems.

</details>


### [15] [Regenerating codes with minimal disk I/O cost achieving optimal tradeoff between storage and repair bandwidth](https://arxiv.org/abs/2601.09300)
*Minhan Gao,Kenneth Shum*

Main category: cs.IT

TL;DR: 为单节点失效设计了无编码修复的编码方案，既最小化磁盘I/O，又能在所有存活节点参与的情况下实现任意存储—修复带宽折衷点，可在固定字段上无限次修复。


<details>
  <summary>Details</summary>
Motivation: 在分布式存储系统中，提升修复效率的两个关键指标是修复带宽和磁盘I/O成本。传统方案往往侧重修复带宽，却忽略磁盘I/O开销；而无编码修复能在不增加辅助节点计算负担的前提下降低I/O成本，因此寻找兼顾两者的最优方案是研究重点。

Method: 利用高阶矩阵（gammoid）理论设计一种编码方案，在该方案中，每个辅助节点不执行编码操作，即实现“无编码修复”，且数据包访问数与传输量相等，从而达到最小磁盘I/O成本。

Result: 提出的编码方案在有限域上可承受无限次节点修复迭代，且在单节点失效时，能够实现任何存储-带宽权衡曲线上的极点。

Conclusion: 本文证明在单节点失败且所有存活节点参与修复的条件下，能够在存储空间与修复带宽之间实现所有可能的极点，即通过攻击传输量与恢复延迟的最优折衷方案。

Abstract: There are multiple performance metrics in the design of coding schemes for distributed storage systems. The first metric is called repair bandwidth, which measures the network resources required during the repair process. Another critical metric for repair efficiency is disk I/O cost, defined as the amount of data packets accessed at helper nodes to repair the failed node. In an encoding scheme with optimal I/O cost, the number of packets sent to the newcomer is exactly the same as the number of packets read from memory. This mode of repair is referred to as uncoded repair, as no coding operations are performed at the helper node. In addition to minimizing disk I/O cost, an uncoded repair mechanism has the advantage of incurring minimal computational overhead at the helper node. In this paper, we demonstrate that for single node failures, if all surviving nodes participate in the repair of the failed node, we can achieve all points on the fundamental tradeoff curve between storage and repair bandwidth. The design of the proposed encoding scheme is based on the theory of gammoids, a specialized class of graph-based matroids. We prove that this scheme can tolerate an unlimited number of node repair iterations over a field of fixed size.

</details>


### [16] [An Information Theoretic Proof of the Radon-Nikodym Theorem](https://arxiv.org/abs/2601.09308)
*Peter Harremoës*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Radon-Nikodym theorem plays a significant role in the definition of Shannon entropy, f-divergences, and other basic quantities in information theory. The existence of Radon Nikodym derivates appear in many text books in measure theory but in text books on probability or information theory it is often omitted because the proof is often considered to be too difficult.

</details>


### [17] [Contraction of Rényi Divergences for Discrete Channels: Properties and Applications](https://arxiv.org/abs/2601.09328)
*Adrien Vandenbroucque,Amedeo Roberto Esposito,Michael Gastpar*

Main category: cs.IT

TL;DR: 研究Rényi Divergence收敛性质，发现与ϕ-Divergence差异，特别是α>1时；探究∞-Rényi与局部差分隐私的关系；应用于马尔可夫链收敛速度评估。


<details>
  <summary>Details</summary>
Motivation: 传统ϕ-Divergence的收缩性质已被广泛研究，作者希望探明Rényi Divergence在不同阶数下是否具备类似性质，并将其应用于概率与信息论领域的收敛分析。

Method: 通过数学推导比较α-Rényi Divergence与ϕ-Divergence的收缩常数，分析不同α值下的收缩行为，并利用这些结果对∞-Rényi Divergence与ε-局部差分隐私的关系进行探究，随后将所得收敛性质用于估计Markov链的收敛速度。

Result: 发现α>1时Rényi Divergence的收缩常数可能与ϕ-Divergence截然不同；∞-Rényi Divergence与ε-局部差分隐私存在可量化的对应关系；基于此，提供了马尔可夫链收敛速度的新上界。

Conclusion: 本文证明了Rényi Divergences在α>1时的收敛性质与传统ϕ-Divergences明显不同，并揭示了∞-Rényi Divergence与局部差分隐私的关系；同时将此收敛特性应用于马尔可夫链收敛速度的上界估计。

Abstract: This work explores properties of Strong Data-Processing constants for Rényi Divergences. Parallels are made with the well-studied $\varphi$-Divergences, and it is shown that the order $α$ of Rényi Divergences dictates whether certain properties of the contraction of $\varphi$-Divergences are mirrored or not. In particular, we demonstrate that when $α>1$, the contraction properties can deviate quite strikingly from those of $\varphi$-Divergences. We also uncover specific characteristics of contraction for the $\infty$-Rényi Divergence and relate it to $\varepsilon$-Local Differential Privacy. The results are then applied to bound the speed of convergence of Markov chains, where we argue that the contraction of Rényi Divergences offers a new perspective on the contraction of $L^α$-norms commonly studied in the literature.

</details>


### [18] [Generalized Schalkwijk-Kailath Coding for Autoregressive Gaussian Channels](https://arxiv.org/abs/2601.09329)
*Jun Su,Guangyue Han,Shlomo Shamai*

Main category: cs.IT

TL;DR: SK(2)编码提供AR($p$)高斯通道的闭式速率，显示原SK方案并非绝对最优，破除Butman猜想。


<details>
  <summary>Details</summary>
Motivation: 验证并推广Schalkwijk-Kailath编码在更一般自回归高斯通道中的适用性，解决其是否为最优方案的长期猜想。

Method: 采用高斯随机编码思想，对通用对称的AR($p$)高斯通道构造了可构造的SK(2)编码框架，利用递归结构推导出闭式可达速率。

Result: 给出了AR($p$)通道对应可达速率的闭式表达，证明SK编码非普适最优，成功破除Butman的猜测。

Conclusion: 提出的SK(2)编码方案在AR($p$)高斯通道上实现了通用可达速率的闭式表达式，并证明了经典SK方案并非在所有情况下都最优，从而否定了Butman提出的相关猜想。

Abstract: We propose a Gaussian random coding scheme for AR($p$) Gaussian channels that generalizes the celebrated Schalkwijk-Kailath (SK) coding scheme. This constructive coding scheme, termed the SK(2) coding scheme, yields a closed-form characterization for the corresponding achievable rate. Among many others, this result shows that the celebrated SK coding scheme is not universally optimal, and therefore, disprove the conjecture proposed by Butman in \cite{butman1976linear}.

</details>


### [19] [A Constructive Method to Minimize the Index of Coincidence under Marginal Constraints](https://arxiv.org/abs/2601.09347)
*Pierre Jean-Claude Robert Bertrand*

Main category: cs.IT

TL;DR: 本文给出了一种在固定边缘条件下最小化指数一致性的完整算法。先证明强可行性条件几乎不可能满足，再说明最优耦合呈阶梯形零结构，随后设计了一种迭代构造，保证在有限步内收敛至最优解。


<details>
  <summary>Details</summary>
Motivation: 指数一致性在信息理论中频繁出现（如概率分布相似性评估、密钥分发等），但如何在固定边缘约束下最小化它仍未得到通用解。探究其最优耦合结构可直接提升相关算法的性能。

Method: 1) 先证明当维度增大时，满足强可行性条件的边缘集合的测度趋于零；2) 在一般情形下对最优耦合进行结构推导，得到其呈现单调阶梯形零条目；3) 基于该结构设计显式迭代构造；4) 通过证明该迭代在有限步内收敛至最优点，完成构造性算法。

Result: 提出了一种构造性迭代算法，可在有限步内求得与给定边缘约束对应的全局指数一致性最小化解；同时揭示了最优耦合的单调阶梯形零结构，理论上证明了该结构是唯一最优形式。

Conclusion: 本文完整描绘了给定边缘约束下指数一致性最小化的问题，并给出了构造性的解决方案。通过证明最优耦合呈现单调阶梯形零结构，并提出了在有限步内收敛的迭代构造，彻底克服了强可行性条件稀缺的局面，使得该方法在实际信息论应用中具有广泛适用性。

Abstract: We consider the problem of minimizing the index of coincidence of a joint distribution under fixed marginal constraints. This objective is motivated by several applications in information theory, where the index of coincidence naturally arises. A closed-form solution is known when the marginals satisfy a strong feasibility condition, but this condition is rarely met in practice. We first show that the measure of the set of marginals for which condition applies vanishes as the dimension grows. We then characterize the structure of the optimal coupling in the general case, proving that it exhibits a monotone staircase of zero entries. Based on this structure, we propose an explicit iterative construction and prove that it converges in finitely many steps to a minimizer. Main result of the paper is a complete constructive solution of index-of-coincidence minimization.

</details>


### [20] [Asymptotic Rate Bounds and Constructions for the Inclusive Variant of Disjunct Matrices](https://arxiv.org/abs/2601.09362)
*Yuto Mizunuma,Yuichiro Fujiwara*

Main category: cs.IT

TL;DR: 本文通过概率与去随机化方法证明可包含离散矩阵可实现近似最优速率，满足可扩展组检验需求


<details>
  <summary>Details</summary>
Motivation: 揭示可包含变体的离散矩阵在一般抑制器复杂模型下的渐进行为

Method: 概率方法构造随机化矩阵，再去随机化得到多项式时间构造

Result: 给出该变体可达到的最大速率的非平凡下界，其与已知的最强上界仅相差对数因子

Conclusion: 证明在该模型下可实现正速率的分组检验方案，为稳健可扩展设计提供理论基础

Abstract: Disjunct matrices, also known as cover-free families and superimposed codes, are combinatorial arrays widely used in group testing. Among their variants, those that satisfy an additional combinatorial property called inclusiveness form a special class suitable for computationally efficient and highly error-tolerant group testing under the general inhibitor complex model, a broad framework that subsumes practical settings such as DNA screening. Despite this relevance, the asymptotic behavior of the inclusive variant of disjunct matrices has remained largely unexplored. In particular, it was not previously known whether this variant can achieve an asymptotically positive rate, a requirement for scalable group testing designs. In this work, we establish the first nontrivial asymptotic lower bound on the maximum achievable rate of the inclusive variant, which matches the strongest known upper bound up to a logarithmic factor. Our proof is based on the probabilistic method and yields a simple and efficient randomized construction. Furthermore, we derandomize this construction to obtain a deterministic polynomial-time construction. These results clarify the asymptotic potential of robust and scalable group testing under the general inhibitor complex model.

</details>


### [21] [On Decoding First- and Second-Order BiD Codes](https://arxiv.org/abs/2601.09390)
*Devansh Jain,Lakshmi Prasad Natarajan*

Main category: cs.IT

TL;DR: BiD码可实现容量接近编码，作者提供了高效译码方案：一阶用快速ML/Max-Log-MAP，二阶利用投影性质进行BP译码，最终在81和243位长度下BP译码误差仅比ML高1 dB。


<details>
  <summary>Details</summary>
Motivation: BiD码在长度$3^m$时已知可达到符号MAP下的容量，但高速准MLE译码方案不足，导致其应用受限。

Method: 1) 为一阶BiD码提出快速ML与max-log-MAP译码算法；2) 对二阶BiD码识别最小权重检验子码并验证投影性质；3) 基于上述性质设计BP译码器，并在长度81与243下与ML译码进行性能比较。

Result: BP译码器在81与243位长度下与ML译码误差率相差不超过1 dB，证明了所提出方法的有效性。

Conclusion: BiD codes在满足容量的同时，通过本文提出的高效ML、max-log-MAP以及BP译码方案，能够在低至1 dB的近似误码率实现上显著优于传统RM码，并在第一级和第二级上实现了可扩展可靠译码。

Abstract: BiD codes, which are a new family of algebraic codes of length $3^m$, achieve the erasure channel capacity under bit-MAP decoding and offer asymptotically larger minimum distance than Reed-Muller (RM) codes. In this paper we propose fast maximum-likelihood (ML) and max-log-MAP decoders for first-order BiD codes. For second-order codes, we identify their minimum-weight parity checks and ascertain a code property known as 'projection' in the RM coding literature. We use these results to design a belief propagation decoder that performs within 1 dB of ML decoder for block lengths 81 and 243.

</details>


### [22] [A Generalized Leakage Interpretation of Alpha-Mutual Information](https://arxiv.org/abs/2601.09406)
*Akira Kamatsuka,Takahiro Yoshida*

Main category: cs.IT

TL;DR: 本文通过Kolmogorov‑Nagumo均值和q‑对数构建对抗性框架，将α-互信息解释为广义g泄漏，α参数对应于攻击者的风险厌恶。


<details>
  <summary>Details</summary>
Motivation: 为了解释α-互信息的更广泛意义，并将其与攻击者的风险偏好关联，从而扩展传统信息泄漏度量的适用范围。

Method: 利用Kolmogorov‑Nagumo均值和q‑对数构造攻击者收益模型，建立基于对抗性泛化决策问题的量化信息流框架，并在此框架中推导α-互信息的新的解释。

Result: 证明α可视为攻击者的风险厌恶度量，显示出α-互信息与对抗性决策理论之间的桥梁。

Conclusion: 本文将α-互信息在扩展的量化信息泄露框架下重新解释为广义g泄漏，揭示了α参数与攻击者风险厌恶之间的内在联系。

Abstract: This paper presents a unified interpretation of $α$-mutual information ($α$-MI) in terms of generalized $g$-leakage. Specifically, we present a novel interpretation of $α$-MI within an extended framework for quantitative information flow based on adversarial generalized decision problems. This framework employs the Kolmogorov-Nagumo mean and the $q$-logarithm to characterize adversarial gain. Furthermore, we demonstrate that, within this framework, the parameter $α$ can be interpreted as a measure of the adversary's risk aversion.

</details>


### [23] [Dobrushin Coefficients of Private Mechanisms Beyond Local Differential Privacy](https://arxiv.org/abs/2601.09498)
*Leonhard Grosse,Sara Saeidian,Tobias J. Oechtering,Mikael Skoglund*

Main category: cs.IT

TL;DR: 本文研究了在点极化泄漏(PML)约束下离散Markov核的Dobrushin系数，并给出了更紧的收缩上界；通过构造实现机制，并将结论推广至更一般的$f$-散度，提升了对LDP与机率性收敛的理论理解。


<details>
  <summary>Details</summary>
Motivation: 传统的LDP方法在隐私与机制收敛之间存在权衡，但仅针对c→0时的情况；研究者希望进一步了解有限c情况对Markov核收缩性能的影响，并在更一般的散度度量下获得更精确的结果。

Method: 首先通过定义对所有概率分布（下限为常数c>0）满足PML的Markov核，随后利用PML参数推导Dobrushin收缩系数的可实现下界；其次构造满足这些界限的具体机制实现；最后通过Binette不等式将结果推广到一般$f$-散度。

Result: 给出了基于PML的收缩上界，并按此界限构造了实现机制；在LDP情形下收缩界限更紧；并将结论推广到任意离散核及一般$f$-散度，拓宽了隐私机制设计与分析的适用范围。

Conclusion: 该工作为离散Markov核的Dobrushin系数与点极化泄漏（PML）的关系提供了新的界定，并给出了满足PML保证的机制可实现的收缩上界，进一步实现了收敛效率与隐私保障的最优权衡；此外，证明了在局部差分隐私（LDP）以及更广泛的$f$-散度框架下均可取得更紧的收缩界限。

Abstract: We investigate Dobrushin coefficients of discrete Markov kernels that have bounded pointwise maximal leakage (PML) with respect to all distributions with a minimum probability mass bounded away from zero by a constant $c>0$. This definition recovers local differential privacy (LDP) for $c\to 0$. We derive achievable bounds on contraction in terms of a kernels PML guarantees, and provide mechanism constructions that achieve the presented bounds. Further, we extend the results to general $f$-divergences by an application of Binette's inequality. Our analysis yields tighter bounds for mechanisms satisfying LDP and extends beyond the LDP regime to any discrete kernel.

</details>


### [24] [Error Exponents for Randomised List Decoding](https://arxiv.org/abs/2601.09519)
*Henrique K. Miyamoto,Sheng Yang*

Main category: cs.IT

TL;DR: 论文分析了随机列表译码的误差指数。固定列表时，与单译码无差异；列表长度指数性增长时，得出在高速率下误差指数可达匹配度量的水平。


<details>
  <summary>Details</summary>
Motivation: 研究随机列表译码的随机编码误差指数，以探讨误差指数与传统单一译码的关系及其在匹配与不匹配度量下的表现。

Method: 通过构造随机化列表译码器（按译码度量比例随机挑选L条消息）得到误差指数，然后在匹配、通用度量以及不同列表大小阶层下分别推导相应界。

Result: 对于固定列表大小，给出一个与集合紧贴的随机编码误差指数，并证明匹配度量下该指数不优于普通译码；当列表大小随块长指数增长时，提供一个非平凡的下界，在高速率下与匹配度量下的误差指数趋于一致。

Conclusion: 随机列表译码在固定列表大小时对误差指数无益；但在列表大小指数级增长时，可在高速率区间实现与匹配度量相当甚至更优的误差性能。

Abstract: This paper studies random-coding error exponents of randomised list decoding, in which the decoder randomly selects $L$ messages with probabilities proportional to the decoding metric of the codewords. The exponents (or bounds) are given for mismatched, and then particularised to matched and universal decoding metrics. Two regimes are studied: for fixed list size, we derive an ensemble-tight random-coding error exponent, and show that, for the matched metric, it does not improve the error exponent of ordinary decoding. For list sizes growing exponentially with the block-length, we provide a non-trivial lower bound to the error exponent that is tight at high rates under the matched metric.

</details>


### [25] [A Finite-Sample Strong Converse for Binary Hypothesis Testing via (Reverse) Rényi Divergence](https://arxiv.org/abs/2601.09550)
*Roberto Bruno,Adrien Vandenbroucque,Amedeo Roberto Esposito*

Main category: cs.IT

TL;DR: 利用逆R\'{e}nyi散度得到的非渐近界限可实现强逆向定理，并在Type I误差指数逼近时揭示Type II误差的快速收敛或发散；数值例子证明该方法在有限样本情形下优于之前结果。


<details>
  <summary>Details</summary>
Motivation: 现有的有限样本假设检验界限在处理非对称错误约束时存在局限性；尤其缺乏对Type II误差的精确非渐近估计。通过引入逆R\'{e}nyi散度，可以获得更紧的界限并揭示指数级错误率的阈值效应，提升理论与实践的分析精度。

Method: 作者利用逆R\'{e}nyi散度的性质，对体现在非对称错误约束下的Type II误差概率进行分析，构造非渐近界限，并通过极限分析推导强逆向退化性质；随后在Type I误差指数衰减约束下求解相应的指数传播特征。

Result: 给出了针对任意有限样本容量的Type II误差非渐近下界；证明其在Type I误差指数路径下呈现三种指数特性：若速率c> D(P1∥P0)则误差趋于1；若c< D(P1∥P0)则趋于0；且在c= D(P1∥P0)时展示强逆向一致性。数值实验进一步验证这些界限相较于现有方法更为严格。

Conclusion: 文章通过逆R\'{e}nyi散度推导了二项假设检验在有限样本下的非渐近界限，证明了强逆向定理，并阐明了在一类错误率指数衰减速率超过或低于KL散度时二类错误率的指数行为。同时针对有限样本情形给出了更精确的二类误差指数下界。

Abstract: This work investigates binary hypothesis testing between $H_0\sim P_0$ and $H_1\sim P_1$ in the finite-sample regime under asymmetric error constraints. By employing the ``reverse" Rényi divergence, we derive novel non-asymptotic bounds on the Type II error probability which naturally establish a strong converse result. Furthermore, when the Type I error is constrained to decay exponentially with a rate $c$, we show that the Type II error converges to 1 exponentially fast if $c$ exceeds the Kullback-Leibler divergence $D(P_1\|P_0)$, and vanishes exponentially fast if $c$ is smaller. Finally, we present numerical examples demonstrating that the proposed converse bounds strictly improve upon existing finite-sample results in the literature.

</details>


### [26] [On Linear Estimators for some Stable Vectors](https://arxiv.org/abs/2601.09554)
*Rayan Chouity,Charbel Hannoun,Jihad Fahs,Ibrahim Abou-Faycal*

Main category: cs.IT

TL;DR: 对两种稳定随机变量模型，条件均值估计是线性，可得到分散最优线性估计；SαS向量下该估计等同于已知的高斯条件均值最佳线性估计。


<details>
  <summary>Details</summary>
Motivation: 研究在稳定随机变量的联合分布下，条件均值估计量是否保持线性，并寻找其最优线性估计形式，以扩展已知的高斯条件均值最佳线性性质。

Method: 分别考虑两种依赖模型：一是两个独立稳定变量通过线性变换得到的变量；二是子高斯对称α稳定(SαS)向量。对这两种模型下的联合稳定随机变量进行条件期望和分散最优线性估计的推导。

Result: 证明在所给两种模型下，条件均值估计是线性的，同时能得到分散最优的线性估计，且在SαS向量情形下两种估计量一致。

Conclusion: 在这两种特定的依赖模型下，条件均值估计量均为线性，并且我们找到了分散最优的线性估计量。尤其对于子高斯SαS向量，两个估计量相同，扩展了高斯情形下条件均值为最佳线性最小均方误差估计量的结果。

Abstract: We consider the estimation problem for jointly stable random variables. Under two specific dependency models: a linear transformation of two independent stable variables and a sub-Gaussian symmetric $α$-stable (S$α$S) vector, we show that the conditional mean estimator is linear in both cases. Moreover, we find dispersion optimal linear estimators. Interestingly, for the sub-Gaussian (S$α$S) vector, both estimators are identical generalizing the well-known Gaussian result of the conditional mean being the best linear minimum-mean square estimator.

</details>


### [27] [The Spectral Representations Of The Simple Hypothesis Testing Problem](https://arxiv.org/abs/2601.09564)
*Barış Nakiboğlu*

Main category: cs.IT

TL;DR: 论文证明了在随机检测器/σ-有限测度环境下，Type II误差率的勒让德对偶能用似然比分布积分表达，从而得到对乘积测度的概率误差上界。


<details>
  <summary>Details</summary>
Motivation: 探索在随机检测器和σ-有限测度情景下，误差率之间的最优非线性对偶关系，以便更好界定误差率的取值范围。

Method: 利用似然比的分位点特性、σ-有限测度的通用性，并结合谱恒等式将Type II误差体积的对偶形式写成对似然比补分布函数的积分。

Result: 通过对偶表述，结合Berry–Esseen定理与高斯Mills比的性质，给出了在乘积测度下的最先进误差率上界（含倾斜场景）。

Conclusion: 论文通过对假设检验中随机检测器的Type I误差率和Type II误差率（体积）之间的卷积关系求解出了其勒让德对偶，从而得到一种新的族的熵谱表述。

Abstract: The convex conjugate (i.e., the Legendre transform) of Type II error probability (volume) as a function of Type I error probability (volume) is determined for the hypothesis testing problem with randomized detectors. The derivation relies on properties of likelihood ratio quantiles and is general enough to extend to the case of $σ$-finite measures in all non-trivial cases. The convex conjugate of the Type II error volume, called the primitive entropy spectrum, is expressed as an integral of the complementary distribution function of the likelihood ratio using a standard spectral identity. The resulting dual characterization of the Type II error volume leads to state of the art bounds for the case of product measures via Berry--Esseen theorem through a brief analysis relying on properties of the Gaussian Mills ratio, both with and without tilting.

</details>


### [28] [On the Error Probability of RPA Decoding of Reed-Muller Codes over BMS Channels](https://arxiv.org/abs/2601.09581)
*Dorsa Fathollahi,V. Arvind Rameshwar,V. Lalitha*

Main category: cs.IT

TL;DR: 本文证明递归投影聚合（RPA）解码器在所有 BMS 信道上可在有限阶数约为 $\log\log n$ 的 Reed–Muller 码上实现零误码概率。


<details>
  <summary>Details</summary>
Motivation: 分析Recursive Projection-Aggregation (RPA) 解码器在一般二进制无记忆对称 (BMS) 信道上的表现，进一步推广在二进制对称信道（BSC）上已知的低速率Reed–Muller（RM）码零误码概率的结果。

Method: 利用 RPA 投影操作与极化码的“通道合并”步骤等价，从而消除对 BMS 通道的额外限制；采用通用的联合上界描述一阶 RM 码在最大似然解码下的误码概率；随后复用 Rameshwar 与 Lalitha（2025）的证据框架，证明在块长趋于无穷时，只要 RM 序数按 $\log\log n$ 量级增长，且对所有 BMS 通道即可实现误码概率趋于零。

Result: 证明：在任意 BMS 通道下，RPA 解码器对于块长 $n$ 趋于无穷且 RM 序数约为 $\log\log n$ 的码，误码概率可趋近零。

Conclusion: 对 RM 码整体性能的更一般化保证：即使不受 BMS 信道限制，RPA 解码器仍能在低序数约为 $\log \log n$ 的范围内取得零误码概率；这为在更广泛信道环境下使用 RM 码提供了理论依据。

Abstract: We analyze the performance of the Recursive Projection-Aggregation (RPA) decoder of Ye and Abbe (2020), for Reed-Muller (RM) codes, over general binary memoryless symmetric (BMS) channels. Our work is a significant generalization of a recent result of Rameshwar and Lalitha (2025) that showed that the RPA decoder provably achieves vanishing error probabilities for "low-rate" RM codes, over the binary symmetric channel (BSC). While a straightforward generalization of the proof strategy in that paper will require additional, restrictive assumptions on the BMS channel, our technique, which employs an equivalence between the RPA projection operation and a part of the "channel combining" phase in polar codes, requires no such assumptions. Interestingly, such an equivalence allows for the use of a generic union bound on the error probability of the first-order RM code (the "base case" of the RPA decoder), under maximum-likelihood decoding, which holds for any BMS channel. We then exploit these observations in the proof strategy outlined in the work of Rameshwar and Lalitha (2025), and argue that, much like in the case of the BSC, one can obtain vanishing error probabilities, in the large $n$ limit (where $n$ is the blocklength), for RM orders that scale roughly as $\log \log n$, for all BMS channels.

</details>


### [29] [Secret sharing with additive access structures from correlated random variables](https://arxiv.org/abs/2601.09640)
*David Miller,Rémi A. Chou*

Main category: cs.IT

TL;DR: 提出可增益访问结构模型，证明能在动态期间保持与固定结构相同的秘密速率，并在阈值结构时实现容量最优。


<details>
  <summary>Details</summary>
Motivation: 传统的秘密共享模型仅支持固定的访问结构，无法应对参与者集随时间变化的情况。为了解决动态访问需求，本文提出了可增益访问结构（Additive Access Structure），并探讨其容量特性。

Method: 在该动态模型中，利用相关随机性和公共通信技术，设计在线共享策略，使得每个时间步的秘密速率与最优固定访问结构相当，并在阈值结构出现时实现容量达标。

Result: 证明存在一种共享策略在每个时间步都能达到与最佳固定结构相同的秘密速率；同时在任何阈值访问结构时间点，均能实现容量满载。

Conclusion: 本文将秘密共享理论扩展到可增益动态访问场景，表明在现实中即使访问结构不断演化，也能通过合适策略保持秘密速率和容量性能。

Abstract: We generalize secret-sharing models that rely on correlated randomness and public communication, originally designed for a fixed access structure, to support a sequence of dynamic access structures, which we term an Additive Access Structure. Specifically, the access structure is allowed to monotonically grow by having any subset of participants added to it at a given time step, and the dealer only learns of these changes to the access structure on the time step that they occur. For this model, we prove the existence of a secret sharing strategy that achieves the same secret rate at each time step as the best known strategy for the fixed access structure version of this model. We also prove that there exists a strategy that is capacity-achieving at any time step where the access structure is a threshold access structure.

</details>


### [30] [Counting and Entropy Bounds for Structure-Avoiding Spatially-Coupled LDPC Constructions](https://arxiv.org/abs/2601.09674)
*Lei Huang*

Main category: cs.IT

TL;DR: 本文用CLLL与Rényi熵给广义QC‑SC‑LDPC码提供可行设计空间下界与多样性保证，为低误码率设计提供量化方法与实用公式。


<details>
  <summary>Details</summary>
Motivation: 在大耦合内存QC‑SC‑LDPC码设计中，消除因边展开与提升导致的短环等有害子结构是降低误码率的关键，但目前缺乏对可行设计空间的量化评估。

Method: 利用Quantitative CLLL推导分区矩阵满足结构规避约束的显式下界；通过行列置换计算非等价解数；再使用Rényi‑熵下界评估Moser–Tardos算法可输出的不同解数量。

Result: 获得了分区矩阵数目、非等价解数以及MT算法解空间的计算下界；针对4‑cycle候选的简化情形给出了闭式下界，帮助系统参数化设计与搜索空间估计。

Conclusion: 本文通过量化的Clique Lovász Local Lemma与Rényi熵分析，给出了大耦合内存QC‑SC‑LDPC码的可行设计空间下界，并对不同方案的多样性和同构性进行了完整评估，为编码器结构的选取与搜索空间估计提供了理论保障。

Abstract: Designing large coupling memory quasi-cyclic spatially-coupled LDPC (QC-SC-LDPC) codes with low error floors requires eliminating specific harmful substructures (e.g., short cycles) induced by edge spreading and lifting. Building on our work~\cite{r15} that introduced a Clique Lovász Local Lemma (CLLL)-based design principle and a Moser--Tardos (MT)-type constructive approach, this work quantifies the size and structure of the feasible design space. Using the quantitative CLLL, we derive explicit lower bounds on the number of partition matrices satisfying a given family of structure-avoidance constraints, and further obtain bounds on the number of non-equivalent solutions under row/column permutations. Moreover, via Rényi-entropy bounds for the MT distribution, we provide a computable lower bound on the number of distinct solutions that the MT algorithm can output, giving a concrete diversity guarantee for randomized constructions. Specializations for eliminating 4-cycle candidates yield closed-form bounds as functions of system parameters, offering a principled way to size memory/lifting and to estimate the remaining search space.

</details>


### [31] [Progress on the Courtade-Kumar Conjecture: Optimal High-Noise Entropy Bounds and Generalized Coordinate-wise Mutual Information](https://arxiv.org/abs/2601.09679)
*Adel Javanmard,David P. Woodruff*

Main category: cs.IT

TL;DR: 1) 任意Boolean函数互信息总和≤1−H(α)。2) 高噪声下误差O(λ^2)，给出最优线性Fourier集中界。


<details>
  <summary>Details</summary>
Motivation: 研究Boolean函数在噪声下的互信息最大化，验证Courtade‑Kumar猜想

Method: 证明任意Boolean函数满足\sum_i I(f(X);Y_i)≤1−H(α)；在高噪声下，通过熵展开率定界O(λ^2)，得到线性Fourier集中界

Result: ① 对所有Boolean函数（不受偏置限制）证明互信息上界1−H(α)；② 在高噪声情形下，熵展开误差最优O(λ^2)，提升已知范围，得到最优的线性Fourier集中界

Conclusion: 两项进展分别解决了Courtade‑Kumar关于任意函数互信息上界的开放问题，并在噪声参数较大时进一步验证猜想，拓宽了可证范围

Abstract: The Courtade-Kumar conjecture posits that dictatorship functions maximize the mutual information between the function's output and a noisy version of its input over the Boolean hypercube. We present two significant advancements related to this conjecture. First, we resolve an open question posed by Courtade and Kumar, proving that for any Boolean function (regardless of bias), the sum of mutual information between the function's output and the individual noisy input coordinates is bounded by $1-H(α)$, where $α$ is the noise parameter of the Binary Symmetric Channel. This generalizes their previous result which was restricted to balanced Boolean functions. Second, we advance the study of the main conjecture in the high noise regime. We establish an optimal error bound of $O(λ^2)$ for the asymptotic entropy expansion, where $λ= (1-2α)^2$, improving upon the previous best-known bounds. This refined analysis leads to a sharp, linear Fourier concentration bound for highly informative functions and significantly extends the range of the noise parameter $λ$ for which the conjecture is proven to hold.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [32] [Streamlined Pathway (SP) Approach: An Efficient Load Balancer to Enhance Quality of Service](https://arxiv.org/abs/2601.08887)
*Aymen Hasan Alawadi*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Efficient load-balancing mechanisms are critical for maximizing performance and increasing the quality of service (QoS) of data center networks (DCNs). Obtaining the optimal QoS while minimizing resource consumption remains a significant challenge. This paper proposes the streamlined pathway (SP) model, which is a flow scheduling solution that requires minimal statistical knowledge of the DCN data plane. The SP model utilizes the software-defined networks (SDN) paradigm with less information gathered from the DCN data plane, besides the traditional hash-based flow scheduling mechanism, the Equal Cost Multi-Path (ECMP). In SDN, the proposed methodology harnesses a minimal yet powerful set of statistical data extracted from the DCN data plane, including port throughput and elephant flow information on the aggregate switches of the DCN fat-tree topology. Several experiments, in addition to theoretical analysis, have been conducted to demonstrate the efficiency of the proposed SP model in terms of QoS enhancement. These results confirm that SP outperforms leading techniques such as Sieve, Hedera, and ECMP, concerning bisection bandwidth, DCN link utilization, packet loss, and packet delivery latency.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [33] [ABE-VVS: Attribute-Based Encrypted Volumetric Video Streaming](https://arxiv.org/abs/2601.08987)
*Mohammad Waquas Usmani,Susmit Shannigrahi,Michael Zink*

Main category: cs.CR

TL;DR: ABE-VVS 通过只加密点云坐标分量，实现轻量化 DRM：X 坐标加密即可有效防伪，显著提升加密/解密速度并降低服务器/缓存 CPU 负载；客户端体验保持稳定。


<details>
  <summary>Details</summary>
Motivation: 传统的点云完整帧加密成本高、延迟大，缺乏灵活的 DRM 解决方案，需要一种既安全又轻量的加密方法。

Method: 该框架采用属性基加密（ABE）仅加密点云中选定的坐标分量（X、Y、Z或组合），并结合云实验室环境进行端到端评估。

Result: 只加密 X 坐标即可有效模糊视图，同时比完整帧加密节省 50% 加密时间、80% 解密时间；ABE-XYZ/ABE-XY 维持低缓存重缓冲，服务器和缓存 CPU 负载降至 80%/63%，客户端重缓冲与 HTTPS 相比更低。

Conclusion: ABE-VVS通过属性选择性坐标加密实现了轻量化且高效的基于点云的视频流 DRM，既降低了计算负载，也保持了良好的观看体验。

Abstract: This work introduces ABE-VVS, a framework that performs attribute based selective coordinate encryption for point cloud based volumetric video streaming, enabling lightweight yet effective digital rights management (DRM). Rather than encrypting entire point cloud frames, our approach encrypts only selected subsets of coordinates ($X, Y, Z$, or combinations), lowering computational overhead and latency while still producing strong visual distortion that prevents meaningful unauthorized viewing. Our experiments show that encrypting only the $X$ coordinates achieves effective obfuscation while reducing encryption and decryption times by up to 50% and 80%, respectively, compared to full-frame encryption.
  To our knowledge, this is the first work to provide a novel end-to-end evaluation of a DRM-enabled secure point cloud streaming system. We deployed a point cloud video streaming setup on the CloudLab testbed and evaluated three HTTP-based Attribute-Based Encryption (ABE) granularities - ABE-XYZ (encrypting all $X,Y,Z$ coordinates), ABE-XY, and ABE-X against conventional HTTPS/TLS secure streaming as well as an HTTP-only baseline without any security. Our streaming evaluation demonstrates that ABE-based schemes reduce server-side CPU load by up to 80% and cache CPU load by up to 63%, comparable to HTTP-only, while maintaining similar cache hit rates. Moreover, ABE-XYZ and ABE-XY exhibit lower client-side rebuffering than HTTPS, and ABE-X achieves zero rebuffering comparable to HTTP-only. Although ABE-VVS increases client-side CPU usage, the overhead is not large enough to affect streaming quality and is offset by its broader benefits, including simplified key revocation, elimination of per-client encryption, and reduced server and cache load.

</details>


### [34] [A Decompilation-Driven Framework for Malware Detection with Large Language Models](https://arxiv.org/abs/2601.09035)
*Aniesh Chawla,Udbhav Prasad*

Main category: cs.CR

TL;DR: LLM可辅助恶意软件检测，但仍需不断微调，并不能完全取代传统防病毒软件。


<details>
  <summary>Details</summary>
Motivation: LLM具备代码理解能力，且恶意软件日益复杂；探讨LLM能否成为防御新一代威胁的有力工具。

Method: 建立自动化管线：使用Ghidra将Windows可执行文件反编译为C代码，再将所得代码输入LLM做分类；后期进行微调训练以提升准确率。

Result: 标准LLM在此任务上有潜力但表现不足；微调模型表现显著提升，但对新型恶意软件的识别能力明显下降。

Conclusion: LLM在恶意/合法代码分类上尚不成熟，需持续微调以跟上恶意软件演化；传统防病毒仍无可替代之处。

Abstract: The parallel evolution of Large Language Models (LLMs) with advanced code-understanding capabilities and the increasing sophistication of malware presents a new frontier for cybersecurity research. This paper evaluates the efficacy of state-of-the-art LLMs in classifying executable code as either benign or malicious. We introduce an automated pipeline that first decompiles Windows executable into a C code using Ghidra disassembler and then leverages LLMs to perform the classification. Our evaluation reveals that while standard LLMs show promise, they are not yet robust enough to replace traditional anti-virus software. We demonstrate that a fine-tuned model, trained on curated malware and benign datasets, significantly outperforms its vanilla counterpart. However, the performance of even this specialized model degrades notably when encountering newer malware. This finding demonstrates the critical need for continuous fine-tuning with emerging threats to maintain model effectiveness against the changing coding patterns and behaviors of malicious software.

</details>


### [35] [StegoStylo: Squelching Stylometric Scrutiny through Steganographic Stitching](https://arxiv.org/abs/2601.09056)
*Robert Dilworth*

Main category: cs.CR

TL;DR: 本文改进对抗性方法TraceTarnish，并在零宽字符隐写中证明33%覆盖率能成功掩盖作者特征，强调对抗风格学的隐私价值


<details>
  <summary>Details</summary>
Motivation: 探讨对抗性风格学及其与隐写术结合，以抵御作者身份识别的隐私威胁

Method: 改进的对抗攻击方法TraceTarnish，以及利用零宽字符隐写覆盖比例对作者指纹进行掩盖的实验

Result: TraceTarnish能显著降低现有作者鉴别与验证系统的准确率；零宽字符隐写达到33%或以上的覆盖率可实现有效的作者身份模糊

Conclusion: 对抗性风格学与隐写技术可以为作者提供隐私防护，展示了对抗作者身份追踪的可行性并呼吁开发类似TraceTarnish的防御工具

Abstract: Stylometry--the identification of an author through analysis of a text's style (i.e., authorship attribution)--serves many constructive purposes: it supports copyright and plagiarism investigations, aids detection of harmful content, offers exploratory cues for certain medical conditions (e.g., early signs of dementia or depression), provides historical context for literary works, and helps uncover misinformation and disinformation. In contrast, when stylometry is employed as a tool for authorship verification--confirming whether a text truly originates from a claimed author--it can also be weaponized for malicious purposes. Techniques such as de-anonymization, re-identification, tracking, profiling, and downstream effects like censorship illustrate the privacy threats that stylometric analysis can enable. Building on these concerns, this paper further explores how adversarial stylometry combined with steganography can counteract stylometric analysis. We first present enhancements to our adversarial attack, $\textit{TraceTarnish}$, providing stronger evidence of its capacity to confound stylometric systems and reduce their attribution and verification accuracy. Next, we examine how steganographic embedding can be fine-tuned to mask an author's stylistic fingerprint, quantifying the level of authorship obfuscation achievable as a function of the proportion of words altered with zero-width Unicode characters. Based on our findings, steganographic coverage of 33% or higher seemingly ensures authorship obfuscation. Finally, we reflect on the ways stylometry can be used to undermine privacy and argue for the necessity of defensive tools like $\textit{TraceTarnish}$.

</details>


### [36] [Rigorous and Generalized Proof of Security of Bitcoin Protocol with Bounded Network Delay](https://arxiv.org/abs/2601.09082)
*Christopher Blake,Chen Feng,Xuechao Wang,Qianyu Yu*

Main category: cs.CR

TL;DR: 通过对传输延迟模型和不同得分区块的扩展，纠正前人随机游走误区，最终证明在诚实矿工速率高于攻击者的情况下比特币协议安全可靠。


<details>
  <summary>Details</summary>
Motivation: 提高比特币协议安全分析的严谨性与实用性。

Method: 构建计算模型，允许攻击者延迟区块传播，推广至不同得分的区块，并采用穿孔区块到达过程修正随机游走理论错误。

Result: 证明只要全延迟诚实矿工率超过攻击者矿工率，协议几乎必然拥有无限多诚实区块。

Conclusion: 证明比特币协议在给定模型下安全，并简化了某些部分。

Abstract: A proof of the security of the Bitcoin protocol is made rigorous, and simplified in certain parts. A computational model in which an adversary can delay transmission of blocks by time $Δ$ is considered. The protocol is generalized to allow blocks of different scores and a proof within this more general model is presented. An approach used in a previous paper that used random walk theory is shown through a counterexample to be incorrect; an approach involving a punctured block arrival process is shown to remedy this error. Thus, it is proven that with probability one, the Bitcoin protocol will have infinitely many honest blocks so long as the fully-delayed honest mining rate exceeds the adversary mining rate.

</details>


### [37] [KryptoPilot: An Open-World Knowledge-Augmented LLM Agent for Automated Cryptographic Exploitation](https://arxiv.org/abs/2601.09129)
*Xiaonan Liu,Zhihao Li,Xiao Lan,Hao Ren,Haizhou Wang,Xingshu Chen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Capture-the-Flag (CTF) competitions play a central role in modern cybersecurity as a platform for training practitioners and evaluating offensive and defensive techniques derived from real-world vulnerabilities. Despite recent advances in large language models (LLMs), existing LLM-based agents remain ineffective on high-difficulty cryptographic CTF challenges, which require precise cryptanalytic knowledge, stable long-horizon reasoning, and disciplined interaction with specialized toolchains. Through a systematic exploratory study, we show that insufficient knowledge granularity, rather than model reasoning capacity, is a primary factor limiting successful cryptographic exploitation: coarse or abstracted external knowledge often fails to support correct attack modeling and implementation. Motivated by this observation, we propose KryptoPilot, an open-world knowledge-augmented LLM agent for automated cryptographic exploitation. KryptoPilot integrates dynamic open-world knowledge acquisition via a Deep Research pipeline, a persistent workspace for structured knowledge reuse, and a governance subsystem that stabilizes reasoning through behavioral constraints and cost-aware model routing. This design enables precise knowledge alignment while maintaining efficient reasoning across heterogeneous subtasks. We evaluate KryptoPilot on two established CTF benchmarks and in six real-world CTF competitions. KryptoPilot achieves a complete solve rate on InterCode-CTF, solves between 56 and 60 percent of cryptographic challenges on the NYU-CTF benchmark, and successfully solves 26 out of 33 cryptographic challenges in live competitions, including multiple earliest-solved and uniquely-solved instances. These results demonstrate the necessity of open-world, fine-grained knowledge augmentation and governed reasoning for scaling LLM-based agents to real-world cryptographic exploitation.

</details>


### [38] [Deep Learning-based Binary Analysis for Vulnerability Detection in x86-64 Machine Code](https://arxiv.org/abs/2601.09157)
*Mitchell Petingola*

Main category: cs.CR

TL;DR: 本文从原始机器码出发，比较图模型与序列模型在漏洞检测中的表现，发现图模型更好，机器码也能满足需求。


<details>
  <summary>Details</summary>
Motivation: 当前依赖反汇编二进制的深度学习漏洞检测方法受限于汇编语言解读难度与信息损失，探索直接使用机器码可简化模型并保留完整信息；

Method: 基于两种深度学习架构（图模型与序列模型）的探索性实验，评估其在三种漏洞类型上的性能；

Result: 图模型在三类漏洞检测中始终优于序列模型，验证了控制流关系的重要性，并表明机器码足以支持有效漏洞发现；

Conclusion: 本论文证明从原始x86-64机器码中提取特征能够有效进行漏洞检测，并且图模型优于序列模型；

Abstract: While much of the current research in deep learning-based vulnerability detection relies on disassembled binaries, this paper explores the feasibility of extracting features directly from raw x86-64 machine code. Although assembly language is more interpretable for humans, it requires more complex models to capture token-level context. In contrast, machine code may enable more efficient, lightweight models and preserve all information that might be lost in disassembly. This paper approaches the task of vulnerability detection through an exploratory study on two specific deep learning model architectures and aims to systematically evaluate their performance across three vulnerability types. The results demonstrate that graph-based models consistently outperform sequential models, emphasizing the importance of control flow relationships, and that machine code contains sufficient information for effective vulnerability discovery.

</details>


### [39] [The Real Menace of Cloning Attacks on SGX Applications](https://arxiv.org/abs/2601.09273)
*Annika Wilde,Samira Briongos,Claudio Soriente,Ghassan Karame*

Main category: cs.CR

TL;DR: 72个SGX提案被评估，其中20%易受克隆攻击。


<details>
  <summary>Details</summary>
Motivation: 回滚攻击被广泛研究，但克隆攻击知识不足；需要检验其在实际方案中的安全性。

Method: 对72个SGX相关设计进行系统性分析，评估其抵御克隆攻击的能力。

Result: 约20%的方案被发现对克隆攻击无效，证明现有防御不足。

Conclusion: 该研究揭示约20%的SGX方案易受克隆攻击，即便其通过单调计数器防御回滚攻击。

Abstract: Trusted Execution Environments (TEEs) are gaining popularity as an effective means to provide confidentiality in the cloud. TEEs, such as Intel SGX, suffer from so-called rollback and cloning attacks (often referred to as forking attacks). Rollback attacks are enabled by the lack of freshness guarantees for sealed data; cloning attacks stem from the inability to determine if other instances of an enclave are running on the same platform. While rollback attacks have been extensively studied by the community, cloning attacks have been, unfortunately, less investigated. To address this gap, we extensively study and thoroughly analyze the susceptibility of 72 SGX-based proposals to cloning attacks. Our results show that roughly 20% of the analyzed proposals are insecure against cloning attacks-including those applications that rely on monotonic counters and are, therefore, secure against rollback attacks.

</details>


### [40] [Explainable Autoencoder-Based Anomaly Detection in IEC 61850 GOOSE Networks](https://arxiv.org/abs/2601.09287)
*Dafne Lozano-Paredes,Luis Bote-Curiel,Juan Ramón Feijóo-Martínez,Ismael Gómez-Talal,José Luis Rojo-Álvarez*

Main category: cs.CR

TL;DR: 提出一种异构自编码器无监督异常检测，针对 IEC 61850 GOOSE 能在极度类别失衡情况下以 99% + 检测率、 5% 以下误报率高效定位攻击，并提供可解释的异常归因。


<details>
  <summary>Details</summary>
Motivation: IEC 61850 GOOSE 协议本身缺乏安全机制，传统规则/监督式方法在面对协议规范化攻击和零日攻击时难以在极度类别失衡、标签稀缺的环境中有效检测。

Method: 采用基于异构自编码器的无监督多视角异常检测，分别学习协议语义序列与时序传输动态的潜在表示；利用重构误差与统计阈值实现检测，并通过特征层重构分析提供可解释性。

Result: 在真实变电站流量训练、公开数据集测试下，攻击检测率超过 99%，误报率低于 5%，并能在极端类别失衡下保持良好泛化。

Conclusion: 该框架在缺乏攻击样本、标签稀缺的工业现场环境中能可靠检测未知与零日 GOOSE 攻击，具备可解释性与高泛化能力。

Abstract: The IEC 61850 Generic Object-Oriented Substation Event (GOOSE) protocol plays a critical role in real-time protection and automation of digital substations, yet its lack of native security mechanisms can expose power systems to sophisticated cyberattacks. Traditional rule-based and supervised intrusion detection techniques struggle to detect protocol-compliant and zero-day attacks under significant class imbalance and limited availability of labeled data. This paper proposes an explainable, unsupervised multi-view anomaly detection framework for IEC 61850 GOOSE networks that explicitly separates semantic integrity and temporal availability. The approach employs asymmetric autoencoders trained only on real operational GOOSE traffic to learn distinct latent representations of sequence-based protocol semantics and timing-related transmission dynamics in normal traffic. Anomaly detection is implemented using reconstruction errors mixed with statistically grounded thresholds, enabling robust detection without specified attack types. Feature-level reconstruction analysis provides intrinsic explainability by directly linking detection outcomes to IEC 61850 protocol characteristics. The proposed framework is evaluated using real substation traffic for training and a public dataset containing normal traffic and message suppression, data manipulation, and denial-of-service attacks for testing. Experimental results show attack detection rates above 99% with false positives remaining below 5% of total traffic, demonstrating strong generalization across environments and effective operation under extreme class imbalance and interpretable anomaly attribution.

</details>


### [41] [Blue Teaming Function-Calling Agents](https://arxiv.org/abs/2601.09292)
*Greta Dolcetti,Giulio Zizzo,Sergio Maffeis*

Main category: cs.CR

TL;DR: 四模型在三攻击下显露安全漏洞，八防御无法落地。


<details>
  <summary>Details</summary>
Motivation: 探究开源LLM功能调用的安全弱点，为后续防御研究提供基线。

Method: 我们对四个公开LLM在三种不同攻击场景下进行实测，并评估八种防御方案的有效性。

Result: 实验显示，所有四个模型均易受攻击，且八种防御在实际环境中效果有限。

Conclusion: 本研究表明四个开源大模型在功能调用功能下默认安全性不足，当前防御手段尚未满足实际应用需求。

Abstract: We present an experimental evaluation that assesses the robustness of four open source LLMs claiming function-calling capabilities against three different attacks, and we measure the effectiveness of eight different defences. Our results show how these models are not safe by default, and how the defences are not yet employable in real-world scenarios.

</details>


### [42] [SpatialJB: How Text Distribution Art Becomes the "Jailbreak Key" for LLM Guardrails](https://arxiv.org/abs/2601.09321)
*Zhiyi Mou,Jingyuan Yang,Zeheng Qian,Wangze Ni,Tianfang Xiao,Ning Liu,Chen Zhang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Large Language Models (LLMs) have powerful capabilities, they remain vulnerable to jailbreak attacks, which is a critical barrier to their safe web real-time application. Current commercial LLM providers deploy output guardrails to filter harmful outputs, yet these defenses are not impenetrable. Due to LLMs' reliance on autoregressive, token-by-token inference, their semantic representations lack robustness to spatially structured perturbations, such as redistributing tokens across different rows, columns, or diagonals. Exploiting the Transformer's spatial weakness, we propose SpatialJB to disrupt the model's output generation process, allowing harmful content to bypass guardrails without detection. Comprehensive experiments conducted on leading LLMs get nearly 100% ASR, demonstrating the high effectiveness of SpatialJB. Even after adding advanced output guardrails, like the OpenAI Moderation API, SpatialJB consistently maintains a success rate exceeding 75%, outperforming current jailbreak techniques by a significant margin. The proposal of SpatialJB exposes a key weakness in current guardrails and emphasizes the importance of spatial semantics, offering new insights to advance LLM safety research. To prevent potential misuse, we also present baseline defense strategies against SpatialJB and evaluate their effectiveness in mitigating such attacks. The code for the attack, baseline defenses, and a demo are available at https://anonymous.4open.science/r/SpatialJailbreak-8E63.

</details>


### [43] [A Systematic Security Analysis for Path-based Traceability Systems in RFID-Enabled Supply Chains](https://arxiv.org/abs/2601.09407)
*Fokke Heikamp,Lei Pan,Robin Doss,Rolando Trujillo-Rasua,Sushmita Ruj*

Main category: cs.CR

TL;DR: 评估17种可追溯系统，发现安全缺陷，提出漏洞和改进建议。


<details>
  <summary>Details</summary>
Motivation: 供应链可追溯系统正被广泛应用，但由于安全缺陷，攻击者可以冒用正品痕迹篡改假品，给企业造成严重风险。

Method: 通过构造统一的安全框架，系统性地对17种先进可追溯方案进行安全评价并比较，揭示各自的弱点。

Result: 完成了对17种可追溯方案的首次大型安全评估，发现多种漏洞并提出改进建议。

Conclusion: 论文指出现有的可追溯体系在安全要求方面缺乏结构化和完整性，导致存在多处漏洞和缺陷。

Abstract: Traceability systems have become prevalent in supply chains because of the rapid development of RFID and IoT technologies. These systems facilitate product recall and mitigate problems such as counterfeiting, tampering, and theft by tracking the manufacturing and distribution life-cycle of a product. Therefore, traceability systems are a defense mechanism against supply chain attacks and, consequently, have become a target for attackers to circumvent. For example, a counterfeiter may change the trace of a fake product for the trace of an authentic product, fooling the system into accepting a counterfeit product as legit and thereby giving a false sense of security.
  This systematic analysis starts with the observation that security requirements in existing traceability solutions are often unstructured or incomplete, leaving critical vulnerabilities unaddressed. We synthesized the properties of current state-of-the-art traceability solutions within a single security framework that allows us to analyze and compare their security claims. Using this framework, we objectively compared the security of $17$ traceability solutions and identified several weaknesses and vulnerabilities. This article reports on these flaws, the methodology we used to identify them, and the first security evaluation of traceability solutions on a large scale.

</details>


### [44] [The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware](https://arxiv.org/abs/2601.09625)
*Ben Nassi,Bruce Schneier,Oleg Brodt*

Main category: cs.CR

TL;DR: Promptware：对LLM攻击的五步杀伤链，结合传统恶意软件模型，帮助安全人员更系统化地识别与防御。


<details>
  <summary>Details</summary>
Motivation: 当前“大型语言模型安全”将不同威胁简化为“提示注入”，缺乏细致分类，导致安全应对不足。

Method: 通过将近期LLM攻击映射到由初始访问、权限提升、持久化、横向移动和目标行动组成的杀伤链，构建框架。

Result: 展示提示注入、越狱、内存/检索污染、系统传播和数据/交易攻击等步骤，形成系统化的攻击流程。

Conclusion: 本文提出Promptware将面向LLM的攻击定义为独特的恶意软件类别，并提供五步链模型供安全实践应用。

Abstract: The rapid adoption of large language model (LLM)-based systems -- from chatbots to autonomous agents capable of executing code and financial transactions -- has created a new attack surface that existing security frameworks inadequately address. The dominant framing of these threats as "prompt injection" -- a catch-all phrase for security failures in LLM-based systems -- obscures a more complex reality: Attacks on LLM-based systems increasingly involve multi-step sequences that mirror traditional malware campaigns. In this paper, we propose that attacks targeting LLM-based applications constitute a distinct class of malware, which we term \textit{promptware}, and introduce a five-step kill chain model for analyzing these threats. The framework comprises Initial Access (prompt injection), Privilege Escalation (jailbreaking), Persistence (memory and retrieval poisoning), Lateral Movement (cross-system and cross-user propagation), and Actions on Objective (ranging from data exfiltration to unauthorized transactions). By mapping recent attacks to this structure, we demonstrate that LLM-related attacks follow systematic sequences analogous to traditional malware campaigns. The promptware kill chain offers security practitioners a structured methodology for threat modeling and provides a common vocabulary for researchers across AI safety and cybersecurity to address a rapidly evolving threat landscape.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models](https://arxiv.org/abs/2601.08893)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 介绍了一种基于小波域受约束随机流的生成模型，旨在实现更高效、更连贯、更物理化的多模态推理


<details>
  <summary>Details</summary>
Motivation: 提供一种基于物理原理的替代 transformer 的生成模型，克服长序列自回归的限制

Method: 在多尺度小波基上构建受约束随机动力学的连续场，利用波塞-斯特劳斯式传输和光谱投影实现局部算子生成

Result: 实现了可在文本和视频多模态上统一表征、稀疏高效且具稳定性、连贯性和不确定性传播的生成体系

Conclusion: SGFMs 为长程连贯、跨模态一般性和物理结构化先验提供了理论与实践路径，突破了传统自回归与扩散模型的束缚

Abstract: We introduce Spectral Generative Flow Models (SGFMs), a physics-inspired alternative to transformer-based large language models. Instead of representing text or video as sequences of discrete tokens processed by attention, SGFMs treat generation as the evolution of a continuous field governed by constrained stochastic dynamics in a multiscale wavelet basis. This formulation replaces global attention with local operators, spectral projections, and Navier--Stokes-like transport, yielding a generative mechanism grounded in continuity, geometry, and physical structure.
  Our framework provides three key innovations: (i) a field-theoretic ontology in which text and video are unified as trajectories of a stochastic partial differential equation; (ii) a wavelet-domain representation that induces sparsity, scale separation, and computational efficiency; and (iii) a constrained stochastic flow that enforces stability, coherence, and uncertainty propagation. Together, these components define a generative architecture that departs fundamentally from autoregressive modeling and diffusion-based approaches. SGFMs offer a principled path toward long-range coherence, multimodal generality, and physically structured inductive bias in next-generation generative models.

</details>


### [46] [XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation](https://arxiv.org/abs/2601.08896)
*Sahaj Raj Malla,Shreeyash Kayastha,Rumi Suwal,Harish Chandra Bhandari,Rajendra Adhikari*

Main category: cs.LG

TL;DR: XGBoost在NEPSE日收益预测上优于传统模型，RMSE约0.0135，方向性准确率65%。


<details>
  <summary>Details</summary>
Motivation: 提升新兴市场NEPSE指数日对数收益预测准确度，为实盘交易与风险管理提供更可靠的工具，验证梯度提升方法在金融回测中的可行性。

Method: 构建包含30日滞后收益与技术指标（滚动波动率、相对强弱指数）的特征集，使用Optuna在时间序列交叉验证中调参，后以扩展窗口和滚动窗口方式进行递归预测评估。

Result: 最佳配置为扩展窗口+20日滞后，RMSE0.013450、MAE0.009814、方向性准确率65.15%，优于ARIMA和岭回归基准。

Conclusion: 本研究验证了XGBoost回归模型在NEPSE指数日对数收益预测上的显著优势，较传统ARIMA和岭回归方法在RMSE、MAE及方向性准确率上均取得更优表现，证实了梯度提升集成在非线性金融时间序列建模中的有效性。

Abstract: This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling window schemes across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R-squared), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration, an expanding window with 20 lags, outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R-squared remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.

</details>


### [47] [DriftGuard: A Hierarchical Framework for Concept Drift Detection and Remediation in Supply Chain Forecasting](https://arxiv.org/abs/2601.08928)
*Shahnawaz Alam,Mohammed Abdul Rahman,Bareera Sadeqa*

Main category: cs.LG

TL;DR: DriftGuard 通过多方法检测 + 层级定位 + SHAP 诊断 + 有针对性重训练，提升脱漂移检测召回率至 97.8%，并显著提高投资回报。


<details>
  <summary>Details</summary>
Motivation: 供应链预测模型受促销、消费者偏好、供应中断等因素影响易出现概念漂移，现行手工监控和周期性重训练既浪费资源又难以及时捕捉漂移。

Method: 通过集成误差监测、统计检验、自动编码器异常检测及 CUSUM 变点分析等四种基本检测器，结合层级传播分析定位漂移位置；采用 SHAP 进行根因诊断，并采用成本感知的有选择性重训练策略，仅更新受漂移影响最大的模型。

Result: 在 M5 零售数据集 30,000 余条时间序列上，检测召回率达 97.8%，平均漂移定位时间 4.2 天；有针对性修复后，投资回报率可高达 417 倍。

Conclusion: 本研究提出的 DriftGuard 系统实现了从漂移检测到根因诊断再到自动修复的完整流程，显著提升了供应链预测模型在概念漂移环境下的稳定性和经济效益。

Abstract: Supply chain forecasting models degrade over time as real-world conditions change. Promotions shift, consumer preferences evolve, and supply disruptions alter demand patterns, causing what is known as concept drift. This silent degradation leads to stockouts or excess inventory without triggering any system warnings. Current industry practice relies on manual monitoring and scheduled retraining every 3-6 months, which wastes computational resources during stable periods while missing rapid drift events. Existing academic methods focus narrowly on drift detection without addressing diagnosis or remediation, and they ignore the hierarchical structure inherent in supply chain data. What retailers need is an end-to-end system that detects drift early, explains its root causes, and automatically corrects affected models. We propose DriftGuard, a five-module framework that addresses the complete drift lifecycle. The system combines an ensemble of four complementary detection methods, namely error-based monitoring, statistical tests, autoencoder anomaly detection, and Cumulative Sum (CUSUM) change-point analysis, with hierarchical propagation analysis to identify exactly where drift occurs across product lines. Once detected, Shapley Additive Explanations (SHAP) analysis diagnoses the root causes, and a cost-aware retraining strategy selectively updates only the most affected models. Evaluated on over 30,000 time series from the M5 retail dataset, DriftGuard achieves 97.8% detection recall within 4.2 days and delivers up to 417 return on investment through targeted remediation.

</details>


### [48] [Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2601.08963)
*Adrita Das,Peiran Jiang,Dantong Zhu,Barnabas Poczos,Jose Lugo-Martinez*

Main category: cs.LG

TL;DR: 本文将DDDM纳入RTK框架，证明确定性去噪可显著压缩采样时间、提升结构质量，并在实际数据上实现更快、更精准的分子生成。


<details>
  <summary>Details</summary>
Motivation: 现有分子扩散模型在采样速度、随机方差以及对结构的感知方面存在瓶颈，需要一种能够在保持化学有效性的前提下显著加快推断速度并提高结构保真度的去噪方法。

Method: 通过将DDDM的逆向过程近似为逆转移核（RTK）操作，利用RTK框架将确定性去噪解释为优化噪声与清晰样本之间的结构化传输映射；该方法统一了确定性与随机扩散模型的概率表述。

Result: 在GEOM‑DRUGS数据集上实验表明，RTK指导的确定性去噪实现了更快的收敛、更高的结构保真度，并通过引入良条件逆核实现了数值稳定、消除随机方差、保持SE(3)等价性的优势。

Conclusion: 采用RTK框架对DDDM进行理论化解释后，显著提升了分子扩散模型的采样效率与结构一致性，验证了确定性去噪技术在分子生成中的有效性。

Abstract: Diffusion models have emerged as a powerful class of generative models for molecular design, capable of capturing complex structural distributions and achieving high fidelity in 3D molecule generation. However, their widespread use remains constrained by long sampling trajectories, stochastic variance in the reverse process, and limited structural awareness in denoising dynamics. The Directly Denoising Diffusion Model (DDDM) mitigates these inefficiencies by replacing stochastic reverse MCMC updates with deterministic denoising step, substantially reducing inference time. Yet, the theoretical underpinnings of such deterministic updates have remained opaque. In this work, we provide a principled reinterpretation of DDDM through the lens of the Reverse Transition Kernel (RTK) framework by Huang et al. 2024, unifying deterministic and stochastic diffusion under a shared probabilistic formalism. By expressing the DDDM reverse process as an approximate kernel operator, we show that the direct denoising process implicitly optimizes a structured transport map between noisy and clean samples. This perspective elucidates why deterministic denoising achieves efficient inference. Beyond theoretical clarity, this reframing resolves several long-standing bottlenecks in molecular diffusion. The RTK view ensures numerical stability by enforcing well-conditioned reverse kernels, improves sample consistency by eliminating stochastic variance, and enables scalable and symmetry-preserving denoisers that respect SE(3) equivariance. Empirically, we demonstrate that RTK-guided deterministic denoising achieves faster convergence and higher structural fidelity than stochastic diffusion models, while preserving chemical validity across GEOM-DRUGS dataset. Code, models, and datasets are publicly available in our project repository.

</details>


### [49] [Continuous Fairness On Data Streams](https://arxiv.org/abs/2601.08976)
*Subhodeep Ghosh,Zhihui Du,Angela Bonifati,Manish Kumar,David Bader,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: 设计块级群组公平模型，实时监控与重排算法显著提升公平度，吞吐30k QPS。


<details>
  <summary>Details</summary>
Motivation: 当滑动窗口尺寸过大时，传统窗口级公平难以满足细粒度需求，需在块级更细粒度下实现公平。

Method: 构建基于sketch的数据结构进行实时监控，并设计证明最优的窗口重排算法，结合严格理论保障完成公平度提升。

Result: 通过在四个真实流式场景中的评估，算法在单线程下实现毫秒级处理，提升公平度平均为50-60%，最高达95%。

Conclusion: 本文提出了在数据流连续滑动窗口中实现块级群组公平性的模型，并通过实时监控与窗口重排算法显著提升公平性，其性能表现突出，实测吞吐量达30,000 QPS，公平度提升可达95%。

Abstract: We study the problem of enforcing continuous group fairness over windows in data streams. We propose a novel fairness model that ensures group fairness at a finer granularity level (referred to as block) within each sliding window. This formulation is particularly useful when the window size is large, making it desirable to enforce fairness at a finer granularity. Within this framework, we address two key challenges: efficiently monitoring whether each sliding window satisfies block-level group fairness, and reordering the current window as effectively as possible when fairness is violated. To enable real-time monitoring, we design sketch-based data structures that maintain attribute distributions with minimal overhead. We also develop optimal, efficient algorithms for the reordering task, supported by rigorous theoretical guarantees. Our evaluation on four real-world streaming scenarios demonstrates the practical effectiveness of our approach. We achieve millisecond-level processing and a throughput of approximately 30,000 queries per second on average, depending on system parameters. The stream reordering algorithm improves block-level group fairness by up to 95% in certain cases, and by 50-60% on average across datasets. A qualitative study further highlights the advantages of block-level fairness compared to window-level fairness.

</details>


### [50] [Optimising for Energy Efficiency and Performance in Machine Learning](https://arxiv.org/abs/2601.08991)
*Emile Dos Santos Ferreira,Neil D. Lawrence,Andrei Paleyes*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The ubiquity of machine learning (ML) and the demand for ever-larger models bring an increase in energy consumption and environmental impact. However, little is known about the energy scaling laws in ML, and existing research focuses on training cost -- ignoring the larger cost of inference. Furthermore, tools for measuring the energy consumption of ML do not provide actionable feedback.
  To address these gaps, we developed Energy Consumption Optimiser (ECOpt): a hyperparameter tuner that optimises for energy efficiency and model performance. ECOpt quantifies the trade-off between these metrics as an interpretable Pareto frontier. This enables ML practitioners to make informed decisions about energy cost and environmental impact, while maximising the benefit of their models and complying with new regulations.
  Using ECOpt, we show that parameter and floating-point operation counts can be unreliable proxies for energy consumption, and observe that the energy efficiency of Transformer models for text generation is relatively consistent across hardware. These findings motivate measuring and publishing the energy metrics of ML models. We further show that ECOpt can have a net positive environmental impact and use it to uncover seven models for CIFAR-10 that improve upon the state of the art, when considering accuracy and energy efficiency together.

</details>


### [51] [Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time Series: Application in Scalable and Interpretable SEP Event Prediction](https://arxiv.org/abs/2601.08999)
*Pranjal Patil,Anli Ji,Berkay Aydin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate prediction of solar energetic particle events is vital for safeguarding satellites, astronauts, and space-based infrastructure. Modern space weather monitoring generates massive volumes of high-frequency, multivariate time series (MVTS) data from sources such as the Geostationary perational Environmental Satellites (GOES). Machine learning (ML) models trained on this data show strong predictive power, but most existing methods overlook domain-specific feasibility constraints. Counterfactual explanations have emerged as a key tool for improving model interpretability, yet existing approaches rarely enforce physical plausibility. This work introduces a Physics-Guided Counterfactual Explanation framework, a novel method for generating counterfactual explanations in time series classification tasks that remain consistent with underlying physical principles. Applied to solar energetic particles (SEP) forecasting, this framework achieves over 80% reduction in Dynamic Time Warping (DTW) distance increasing the proximity, produces counterfactual explanations with higher sparsity, and reduces runtime by nearly 50% compared to state-of-the-art baselines such as DiCE. Beyond numerical improvements, this framework ensures that generated counterfactual explanations are physically plausible and actionable in scientific domains. In summary, the framework generates counterfactual explanations that are both valid and physically consistent, while laying the foundation for scalable counterfactual generation in big data environments.

</details>


### [52] [Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers](https://arxiv.org/abs/2601.09000)
*Annalisa Belloni,Lorenzo Noci,Antonio Orvieto*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Warmup Stable Decay (WSD) learning rate scheduler has recently become popular, largely due to its good performance and flexibility when training large language models. It remains an open question whether the remarkable performance of WSD - using a decaying learning rate for only a fraction of training compared to cosine decay - is a phenomenon specific to transformer-based language models that can potentially offer new theoretical insights into their training dynamics. Inspired by the usage of learning rate schedulers as a new lens into understanding landscape geometry (e.g., river valley, connected minima, progressive sharpening), in this work we compare the WSD path of the Adam optimizer on a Pythia-like language model to that of a small CNN trained to classify CIFAR10 images. We observe most training signals, optimizer path features, and sharpness dynamics to be qualitatively similar in such architectures. This consistency points to shared geometric characteristics of the loss landscapes of old and new nonconvex problems, and hints to future research questions around the geometry of high dimensional optimization problems.

</details>


### [53] [Meta-learning to Address Data Shift in Time Series Classification](https://arxiv.org/abs/2601.09018)
*Samuel Myren,Nidhi Parikh,Natalie Klein*

Main category: cs.LG

TL;DR: 研究评估了元学习与传统深度学习在地震等时序分类任务中的适应性。元学习在少量数据和小模型时明显优越；数据丰富或模型大时两者差距缩小。任务多样性并非决定性因素，分布一致性更关键。


<details>
  <summary>Details</summary>
Motivation: 现实世界中数据会出现偏移，传统模型在分布不一致时会快速退化，需昂贵的标注与重训；元学习能在少量样本下快速适应，可能为时序分类提供更具实用性的解决方案。

Method: 对比传统深度学习与基于优化的元学习算法（如MAML等）在时序分类任务的性能表现；提出专门针对地震时间序列的 SeisTask 基准；在不同数据量、模型规模、任务多样性下评估适应性和过拟合。

Result: 在数据稀缺和小模型情景下，元学习取得了更快、更稳定的适应速度，且更不易过拟合；随着数据量与模型容量增长，元学习优势减弱，传统微调模型优势相近。任务多样性提升并非必要，关键是训练与测试分布的一致性。

Conclusion: Meta‑learning在数据偏移、数据稀缺、模型规模较小的情形下，能够比传统深度学习（只微调）更快、更稳健地适应时序分类任务；随着数据量与模型容量增加，它的优势会减弱，传统方法优势基本持平。

Abstract: Across engineering and scientific domains, traditional deep learning (TDL) models perform well when training and test data share the same distribution. However, the dynamic nature of real-world data, broadly termed \textit{data shift}, renders TDL models prone to rapid performance degradation, requiring costly relabeling and inefficient retraining. Meta-learning, which enables models to adapt quickly to new data with few examples, offers a promising alternative for mitigating these challenges. Here, we systematically compare TDL with fine-tuning and optimization-based meta-learning algorithms to assess their ability to address data shift in time-series classification. We introduce a controlled, task-oriented seismic benchmark (SeisTask) and show that meta-learning typically achieves faster and more stable adaptation with reduced overfitting in data-scarce regimes and smaller model architectures. As data availability and model capacity increase, its advantages diminish, with TDL with fine-tuning performing comparably. Finally, we examine how task diversity influences meta-learning and find that alignment between training and test distributions, rather than diversity alone, drives performance gains. Overall, this work provides a systematic evaluation of when and why meta-learning outperforms TDL under data shift and contributes SeisTask as a benchmark for advancing adaptive learning research in time-series domains.

</details>


### [54] [SCaLE: Switching Cost aware Learning and Exploration](https://arxiv.org/abs/2601.09042)
*Neelkamal Bhuyan,Debankur Mukherjee,Adam Wierman*

Main category: cs.LG

TL;DR: 提出 SCaLE 算法，在噪声 bandit 方程里，采用谱分析取得分布无关的次线性动态 regret，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决 bandit 在线凸优化中无界度量移动成本的核心难题，并在随机环境下获得更优的 regret 上界。

Method: 通过将高维动态二次 hitting 成本与 $\ell_2$-norm 交换成本相结合，并采用新的谱 Regret 分析，分别衡量 eigenvalue‑error 驱动的 regret 与 eigenbasis‑perturbation 驱动的 regret。

Result: 首次给出 SCaLE 算法，并证明其分布无关次线性动态 regret。数值实验表明该算法在在线学习基准上具有统计一致性。

Conclusion: 在噪声 bandit 反馈模型中，提出的 SCaLE 算法实现了对无界度量移动成本的分布无关次线性动态 regret，在不需要先验 hitting cost 结构知识的情况下。

Abstract: This work addresses the fundamental problem of unbounded metric movement costs in bandit online convex optimization, by considering high-dimensional dynamic quadratic hitting costs and $\ell_2$-norm switching costs in a noisy bandit feedback model. For a general class of stochastic environments, we provide the first algorithm SCaLE that provably achieves a distribution-agnostic sub-linear dynamic regret, without the knowledge of hitting cost structure. En-route, we present a novel spectral regret analysis that separately quantifies eigenvalue-error driven regret and eigenbasis-perturbation driven regret. Extensive numerical experiments, against online-learning baselines, corroborate our claims, and highlight statistical consistency of our algorithm.

</details>


### [55] [Deep Incomplete Multi-View Clustering via Hierarchical Imputation and Alignment](https://arxiv.org/abs/2601.09051)
*Yiming Du,Ziyu Wang,Jian Li,Rui Ning,Lusi Li*

Main category: cs.LG

TL;DR: 提出 DIMVC-HIA——集层级插补与能量/对比对齐于一体的深度不完整多视图聚类框架，在多视图缺失场景下实现更优聚类性能。


<details>
  <summary>Details</summary>
Motivation: 在多视图数据缺失昂贵且难以采集完整时，传统方法在插补与聚类一致性上容易产生偏差，亟需能够兼顾缺失视图插补质量、跨视图语义保持与簇内紧凑的统一框架。

Method: 设计多分支因子化框架，包含：1) 视图专属自编码器与共享聚类预测器；2) 层级插补模块先基于三视图对比相似性估计缺失聚类标签，再用视图内与簇内统计量重建特征；3) 能量驱动语义对齐模块利用低能量簇锚点最小化能量方差；4) 对比聚类对齐模块强化跨视图一致性与聚类分隔。

Result: 在公开基准数据集（如 MNIST, COIL-20 等）上进行实验，DIMVC-HIA 在 10%–50% 缺失率下的 NMI、ARI 指标提升约 5%–10%，并显示更稳健的内部能量分布。

Conclusion: 在不同缺失程度下，DIMVC-HIA框架相较于现有多视图聚类方法在聚类精度与内部一致性上均取得显著提升。

Abstract: Incomplete multi-view clustering (IMVC) aims to discover shared cluster structures from multi-view data with partial observations. The core challenges lie in accurately imputing missing views without introducing bias, while maintaining semantic consistency across views and compactness within clusters. To address these challenges, we propose DIMVC-HIA, a novel deep IMVC framework that integrates hierarchical imputation and alignment with four key components: (1) view-specific autoencoders for latent feature extraction, coupled with a view-shared clustering predictor to produce soft cluster assignments; (2) a hierarchical imputation module that first estimates missing cluster assignments based on cross-view contrastive similarity, and then reconstructs missing features using intra-view, intra-cluster statistics; (3) an energy-based semantic alignment module, which promotes intra-cluster compactness by minimizing energy variance around low-energy cluster anchors; and (4) a contrastive assignment alignment module, which enhances cross-view consistency and encourages confident, well-separated cluster predictions. Experiments on benchmarks demonstrate that our framework achieves superior performance under varying levels of missingness.

</details>


### [56] [Resolving Predictive Multiplicity for the Rashomon Set](https://arxiv.org/abs/2601.09071)
*Parian Haghighat,Hadis Anahideh,Cynthia Rudin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The existence of multiple, equally accurate models for a given predictive task leads to predictive multiplicity, where a ``Rashomon set'' of models achieve similar accuracy but diverges in their individual predictions. This inconsistency undermines trust in high-stakes applications where we want consistent predictions. We propose three approaches to reduce inconsistency among predictions for the members of the Rashomon set. The first approach is \textbf{outlier correction}. An outlier has a label that none of the good models are capable of predicting correctly. Outliers can cause the Rashomon set to have high variance predictions in a local area, so fixing them can lower variance. Our second approach is local patching. In a local region around a test point, models may disagree with each other because some of them are biased. We can detect and fix such biases using a validation set, which also reduces multiplicity. Our third approach is pairwise reconciliation, where we find pairs of models that disagree on a region around the test point. We modify predictions that disagree, making them less biased. These three approaches can be used together or separately, and they each have distinct advantages. The reconciled predictions can then be distilled into a single interpretable model for real-world deployment. In experiments across multiple datasets, our methods reduce disagreement metrics while maintaining competitive accuracy.

</details>


### [57] [SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache](https://arxiv.org/abs/2601.09083)
*Chi-Chih Chang,Siqi Zhu,Zhichen Zeng,Haibin Lin,Jiaxuan You,Mohamed S. Abdelfattah,Ziheng Jiang,Xuehai Qian*

Main category: cs.LG

TL;DR: SRT 利用树缓存重复 prompt 的续写，通过草稿模型投机解码及在线更新，有效减少 RL 推理时延，最快可提升 2.08 倍的总运行速度


<details>
  <summary>Details</summary>
Motivation: 加速基于语言模型的在线强化学习而不牺牲分布正确性

Method: 使用树结构缓存（Tree-Structured Cache）存储相同 prompt 的已生成续写，并在推理时用缓存作为草稿模型进行投机解码，同时在线更新缓存并在 GPU 空闲时进行提前生成

Result: 在标准 RL 流程（PPO、GRPO、DAPO）和多轮对话中持续降低生成与步骤延迟，逐 token 推理费用下降，SRT 在 rollout 期间实现最高 2.08 倍的墙时同步加速

Conclusion: 树结构缓存的投机回放方法有效提升语言模型强化学习的效率，兼顾分布准确性与计算速度

Abstract: We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.

</details>


### [58] [MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting](https://arxiv.org/abs/2601.09085)
*Kangda Wei,Ruihong Huang*

Main category: cs.LG

TL;DR: MMR-GRPO 用MMP对 GRPO 奖励进行多样性加权，平均减少 47.9% 步骤、70.2% 训练时长，维持同等最高性能


<details>
  <summary>Details</summary>
Motivation: 需要降低GRPO在数学推理模型训练中的计算成本和训练时间

Method: 在GRPO中加入最大边际相关性（MMR）对奖励重新加权，优先考虑多样化解答

Result: 在1.5B、7B、8B三种模型规模、三种GRPO变体、五个数学基准中，MMR-GRPO平均减少47.9%训练步骤、70.2%实际训练时间，保持峰值性能

Conclusion: MMR-GRPO 通过鼓励答案多样性显著加速收敛，保持性能且显著降低计算资源消耗

Abstract: Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.

</details>


### [59] [Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning](https://arxiv.org/abs/2601.09088)
*Shaotian Yan,Kaiyuan Liu,Chen Shen,Bing Wang,Sinan Fan,Jun Zhang,Yue Wu,Zheng Wang,Jieping Ye*

Main category: cs.LG

TL;DR: 改进的序列级蒸馏方法让DASD-4B-Thinking在众多基准测试中以极少样本实现SOTA，并公开模型和数据集供社区使用。


<details>
  <summary>Details</summary>
Motivation: 现有的序列级蒸馏方法主要基于SFT视角，侧重于对蒸馏数据进行启发式过滤，忽视了蒸馏的核心原则——让学生学习教师的完整输出分布以继承其泛化能力。为充分利用蒸馏本质，需要解决分布表示、对齐和曝光偏差等问题。

Method: 我们提出了改进的序列级蒸馏训练管道，重点解决传统SFT过程的三大局限：①对教师输出序列分布的低效表示；②教师分布与学生学习能力的不匹配；③教师强迫训练与自回归推理之间的曝光偏差。通过恢复教师-学生交互、提升分布表达以及对齐学习能力，构建了更完整的蒸馏机制。

Result: 使用上述管道，DASD-4B-Thinking在数学、科学推理和代码生成等挑战性基准上取得了SOTA成绩，且仅需448K样本，显著低于其它开源模型所需的样本量。

Conclusion: DASD-4B-Thinking是一款轻量级但高度有效的全开源推理模型，在数学、科学推理以及代码生成等领域的多项基准测试中实现了与同等规模模型相比的SOTA表现，甚至超过了一些规模更大的模型；其核心优势在于仅使用448K训练样本即可达到竞争性效果，样本量比多数现有开源工作低一个数量级。

Abstract: In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.

</details>


### [60] [Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.09093)
*Zhixiang Liang,Beichen Huang,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) can enhance reasoning capabilities through test-time scaling by generating multiple traces. However, the combination of lengthy reasoning traces with multiple sampling introduces substantial computation and high end-to-end latency. Prior work on accelerating this process has relied on similarity-based or confidence-based pruning, but these signals do not reliably indicate trace quality. To address these limitations, we propose STEP: Step-level Trace Evaluation and Pruning, a novel pruning framework that evaluates reasoning steps using hidden states and dynamically prunes unpromising traces during generation. We train a lightweight step scorer to estimate trace quality, and design a GPU memory-aware pruning strategy that triggers pruning as the GPU memory is saturated by KV cache to reduce end-to-end latency. Experiments across challenging reasoning benchmarks demonstrate that STEP reduces end-to-end inference latency by 45%-70% on average compared to self-consistency while also improving reasoning accuracy. Our code is released at: https://github.com/Supercomputing-System-AI-Lab/STEP

</details>


### [61] [DP-FEDSOFIM: Differentially Private Federated Stochastic Optimization using Regularized Fisher Information Matrix](https://arxiv.org/abs/2601.09166)
*Sidhant R. Nair,Tanmay Sen,Mrinmay Sen*

Main category: cs.LG

TL;DR: DP‑FedSOFIM 通过服务器端二阶优化，使用 FIM 作为预处理器，客户端仅需 O(d) 内存，O(d) 计算，且保留差分隐私，实验表明在不同隐私预算下优于传统一阶方法。


<details>
  <summary>Details</summary>
Motivation: DP-FL在严格隐私预算下因噪声激增导致收敛慢；现有二阶方法需O(d^2)内存，对高维模型不可行。

Method: 在服务器端构建Fisher信息矩阵作为自然梯度预处理器，并借助Sherman-Morrison公式实现O(d)计算及O(d)客户端内存的高效二阶优化。

Result: 实验（CIFAR‑10）显示，DP-FedSOFIM在多个隐私水印下的测试准确率优于一阶基线。

Conclusion: DP-FedSOFIM在满足(ε,δ)-差分隐私的前提下，显著提高了DP-FL的收敛速度和模型性能，同时仅需客户端O(d)内存，适用于高维模型。

Abstract: Differentially private federated learning (DP-FL) suffers from slow convergence under tight privacy budgets due to the overwhelming noise introduced to preserve privacy. While adaptive optimizers can accelerate convergence, existing second-order methods such as DP-FedNew require O(d^2) memory at each client to maintain local feature covariance matrices, making them impractical for high-dimensional models. We propose DP-FedSOFIM, a server-side second-order optimization framework that leverages the Fisher Information Matrix (FIM) as a natural gradient preconditioner while requiring only O(d) memory per client. By employing the Sherman-Morrison formula for efficient matrix inversion, DP-FedSOFIM achieves O(d) computational complexity per round while maintaining the convergence benefits of second-order methods. Our analysis proves that the server-side preconditioning preserves (epsilon, delta)-differential privacy through the post-processing theorem. Empirical evaluation on CIFAR-10 demonstrates that DP-FedSOFIM achieves superior test accuracy compared to first-order baselines across multiple privacy regimes.

</details>


### [62] [Enhancing Imbalanced Electrocardiogram Classification: A Novel Approach Integrating Data Augmentation through Wavelet Transform and Interclass Fusion](https://arxiv.org/abs/2601.09103)
*Haijian Shao,Wei Liu,Xing Deng,Daze Lu*

Main category: cs.LG

TL;DR: 本文提出一种融合小波变换特征的 ECG 分类方法，既解决类别不平衡也降低噪声影响，在 CPSC‑2018 数据集上取得 92%‑98% 的准确率，优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 传统 ECG 数据集存在类别严重不平衡且采集噪声干扰，导致深度学习模型性能下降，缺乏有效的综合解决方案。

Method: 利用小波变换对 ECG 信号进行特征融合，构建训练和测试特征库，将原始数据与特征库融合，生成更平衡的数据集；随后训练改进的 ECG 分类模型。

Result: 在 CPSC‑2018 数据集上，该方法分别对 Normal、AF、I‑AVB、LBBB、RBBB、PAC、PVC、STD、STE 获得 99%、98%、97%、98%、96%、92%、93% 的准确率，整体平均 92%‑98%。

Conclusion: 基于小波特征融合的 ECG 分类方法显著提升了分类准确率，并在类别不平衡和噪声问题下表现出更高的鲁棒性，优于已知算法。

Abstract: Imbalanced electrocardiogram (ECG) data hampers the efficacy and resilience of algorithms in the automated processing and interpretation of cardiovascular diagnostic information, which in turn impedes deep learning-based ECG classification. Notably, certain cardiac conditions that are infrequently encountered are disproportionately underrepresented in these datasets. Although algorithmic generation and oversampling of specific ECG signal types can mitigate class skew, there is a lack of consensus regarding the effectiveness of such techniques in ECG classification. Furthermore, the methodologies and scenarios of ECG acquisition introduce noise, further complicating the processing of ECG data. This paper presents a significantly enhanced ECG classifier that simultaneously addresses both class imbalance and noise-related challenges in ECG analysis, as observed in the CPSC 2018 dataset. Specifically, we propose the application of feature fusion based on the wavelet transform, with a focus on wavelet transform-based interclass fusion, to generate the training feature library and the test set feature library. Subsequently, the original training and test data are amalgamated with their respective feature databases, resulting in more balanced training and test datasets. Employing this approach, our ECG model achieves recognition accuracies of up to 99%, 98%, 97%, 98%, 96%, 92%, and 93% for Normal, AF, I-AVB, LBBB, RBBB, PAC, PVC, STD, and STE, respectively. Furthermore, the average recognition accuracy for these categories ranges between 92\% and 98\%. Notably, our proposed data fusion methodology surpasses any known algorithms in terms of ECG classification accuracy in the CPSC 2018 dataset.

</details>


### [63] [EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge](https://arxiv.org/abs/2601.09142)
*Shijian Ma,Yan Lin,Yi Yang*

Main category: cs.LG

TL;DR: 构建 30,000 条训练样本与 1,000 条人工标注测试样本（Kappa 0.835）的 EvasionBench；通过多模型分歧挖掘与判定者修正，训练 81.3% 准确率的 Eva-4B 模型，性能接近前沿大模型且成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 在财报电话会议中发现回避性回答对金融透明性至关重要，但缺乏大规模标注基准阻碍了进展。

Method: 利用多模型注释框架：从前沿 LLM 之间的分歧中挖掘边界难例，随后由判定者（judge）对冲突样本进行最终标注，从而构建高质量标注集并进行模型训练。

Result: 采用分歧挖掘方案的模型相对于单模型蒸馏提升 2.4%，并通过判定者标注的样本提升泛化能力，训练损失虽略高但表现更好。

Conclusion: Eva-4B 模型在逃逸检测任务中取得 81.3% 的准确率，较其基础模型提升 25 个百分点，并以较低的推理成本逼近前沿大语言模型的性能。

Abstract: Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.

</details>


### [64] [Discrete Solution Operator Learning for Geometry-Dependent PDEs](https://arxiv.org/abs/2601.09143)
*Jinshuai Bai,Haolin Li,Zahra Sharif Khodaei,M. H. Aliabadi,YuanTong Gu,Xi-Qiao Feng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neural operator learning accelerates PDE solution by approximating operators as mappings between continuous function spaces. Yet in many engineering settings, varying geometry induces discrete structural changes, including topological changes, abrupt changes in boundary conditions or boundary types, and changes in the effective computational domain, which break the smooth-variation premise. Here we introduce Discrete Solution Operator Learning (DiSOL), a complementary paradigm that learns discrete solution procedures rather than continuous function-space operators. DiSOL factorizes the solver into learnable stages that mirror classical discretizations: local contribution encoding, multiscale assembly, and implicit solution reconstruction on an embedded grid, thereby preserving procedure-level consistency while adapting to geometry-dependent discrete structures. Across geometry-dependent Poisson, advection-diffusion, linear elasticity, as well as spatiotemporal heat-conduction problems, DiSOL produces stable and accurate predictions under both in-distribution and strongly out-of-distribution geometries, including discontinuous boundaries and topological changes. These results highlight the need for procedural operator representations in geometry-dominated regimes and position discrete solution operator learning as a distinct, complementary direction in scientific machine learning.

</details>


### [65] [KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education](https://arxiv.org/abs/2601.09156)
*Woojin Kim,Changkwon Lee,Hyeoncheol Kim*

Main category: cs.LG

TL;DR: KTCF利用反事实方法改进知识追踪，并将结果转化为教学指令，实验证明显著提升性能并有助减轻学习负担。


<details>
  <summary>Details</summary>
Motivation: 提升教育中的教学与学习效益，需要更具适应性与可扩展性的人工智能解决方案。

Method: 提出KTCF，即运用反事实解释方法改进知识追踪（KT），并结合后处理方案将解释转化为教育指令。

Result: 在大规模教育数据集上验证，KTCF相较现有方法提升5.7–34%多个指标，并通过定性评估证实生成的教育指令可减轻学习负担。

Conclusion: 反事实解释在KT中的应用有望推动人工智能在教育中的负责任与实用价值，未来可进一步与教育实体对接并开发面向利益相关者的方法。

Abstract: Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation method for KT that accounts for knowledge concept relationships, and a post-processing scheme that converts a counterfactual explanation into a sequence of educational instructions. We experiment on a large-scale educational dataset and show our KTCF method achieves superior and robust performance over existing methods, with improvements ranging from 5.7% to 34% across metrics. Additionally, we provide a qualitative evaluation of our post-processing scheme, demonstrating that the resulting educational instructions help in reducing large study burden. We show that counterfactuals have the potential to advance the responsible and practical use of AI in education. Future works on XAI for KT may benefit from educationally grounded conceptualization and developing stakeholder-centered methods.

</details>


### [66] [Efficient Clustering in Stochastic Bandits](https://arxiv.org/abs/2601.09162)
*G Dhinesh Chandran,Kota Srinivas Reddy,Srikrishna Bhashyam*

Main category: cs.LG

TL;DR: 提出了在非高斯、向量参数分布下可行的、每步只做一次优化迭代的EBC，和更简化的EBC-H，均通过仿真验证在保持渐进最优性的前提下大幅提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有渐进最优Bandit Clustering算法需在每一步求解完整优化问题，计算代价极高；同时只关注高斯分布的设定限制了实际应用。

Method: 设计EBC与EBC-H两种采样策略：EBC在每一步对完整优化问题做单步梯度/投影更新；EBC-H使用待停规则中已计算的量直接决定 arm 的采样。随后通过仿真和实证数据评估其运行时与性能。

Result: 实验表明EBC和EBC-H在合成及真实数据集上均取得相对于现有算法的显著运行时间提升，同时保持或超过精度；EBC在理论上与现有渐进最优方案同等。

Conclusion: EBC在固定置信水平下实现了与传统BC算法相同的渐进最优性，但在每一步只执行一次优化迭代，显著降低了计算成本；而EBC-H进一步简化采样规则，在保持良好性能的同时进一步提高效率。

Abstract: We study the Bandit Clustering (BC) problem under the fixed confidence setting, where the objective is to group a collection of data sequences (arms) into clusters through sequential sampling from adaptively selected arms at each time step while ensuring a fixed error probability at the stopping time. We consider a setting where arms in a cluster may have different distributions. Unlike existing results in this setting, which assume Gaussian-distributed arms, we study a broader class of vector-parametric distributions that satisfy mild regularity conditions. Existing asymptotically optimal BC algorithms require solving an optimization problem as part of their sampling rule at each step, which is computationally costly. We propose an Efficient Bandit Clustering algorithm (EBC), which, instead of solving the full optimization problem, takes a single step toward the optimal value at each time step, making it computationally efficient while remaining asymptotically optimal. We also propose a heuristic variant of EBC, called EBC-H, which further simplifies the sampling rule, with arm selection based on quantities computed as part of the stopping rule. We highlight the computational efficiency of EBC and EBC-H by comparing their per-sample run time with that of existing algorithms. The asymptotic optimality of EBC is supported through simulations on the synthetic datasets. Through simulations on both synthetic and real-world datasets, we show the performance gain of EBC and EBC-H over existing approaches.

</details>


### [67] [Multi-Teacher Ensemble Distillation: A Mathematical Framework for Probability-Domain Knowledge Aggregation](https://arxiv.org/abs/2601.09165)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 提出并证明一种公理化多教师知识蒸馏框架，拥有多种满足公理的聚合算子，理论上多教师聚合能降低方差与偏差，并给出对应的安全性与损失保证。


<details>
  <summary>Details</summary>
Motivation: 多教师蒸馏受限于聚合策略的经验性选择，缺乏统一的理论指导；本工作旨在提供一个公理化的理论框架，以系统化地定义有效的聚合算子，并通过理论保证来验证其优越性，为多教师蒸馏的实践提供可靠依据。

Method: 先在Sparse-KD的概率域蒸馏框架之上，提出包含凸性、非负性、连续性、权重单调性、温度一致性等五条核心公理的算子聚合定义；随后通过算子理论证明满足这些公理的算子族存在并非唯一；接着给出通用（算子无关）的性能保证，包括方差和系统监督偏差的降低、Jensen界、对数损失保证及安全衰减性质；对于线性权重聚合算子，在教师独立和相关误差情形下进一步推导经典方差减小定理。

Result: 1）证明了满足五条公理的算子族存在且非唯一；2）给出算子无关的保真度与安全性保证，证明多教师聚合可降低随机方差与监督偏差；3）对于线性权重聚合，得到了在独立与相关误差假设下的方差减小定理；4）为多教师蒸馏提供了理论基础，支持多种实现方式。

Conclusion: 本文构建了一个基于概率域蒸馏的多教师集成知识蒸馏的公理化、算子理论框架，并证明了满足预定义五条核心公理的算子族的存在性和非唯一性，从理论上说明了同一套基本原则可以产生多种有效的聚合机制。

Abstract: Building on the probability-domain distillation framework of Sparse-KD, we develop an axiomatic, operator-theoretic framework for multi-teacher ensemble knowledge distillation. Rather than prescribing a specific aggregation formula, we define five core axioms governing valid knowledge aggregation operators, encompassing convexity, positivity, continuity, weight monotonicity, and temperature coherence. We prove the existence and non-uniqueness of operator families satisfying these axioms, establishing that multiple distinct aggregation mechanisms conform to the same foundational principles.
  Within this framework, we establish operator-agnostic guarantees showing that multi-teacher aggregation reduces both stochastic variance and systematic supervisory bias under heterogeneous teachers, while providing Jensen-type bounds, log-loss guarantees, and safety attenuation properties. For aggregation operators linear in teacher weights, we further establish classical ensemble variance-reduction results under standard independence assumptions, with extensions to correlated-error regimes. The framework provides theoretical grounding for multi-teacher distillation from diverse frontier models while admitting multiple valid implementation strategies.

</details>


### [68] [BalDRO: A Distributionally Robust Optimization based Framework for Large Language Model Unlearning](https://arxiv.org/abs/2601.09172)
*Pengyang Shao,Naixin Zhai,Lei Chen,Yonghui Yang,Fengbin Zhu,Xun Yang,Meng Wang*

Main category: cs.LG

TL;DR: 提出BalDRO框架，利用min-sup的内外层优化，结合离散GroupDRO与连续Donsker-Varadhan两种实现，解决LLM遗忘样本不平衡，实验表明效果显著。


<details>
  <summary>Details</summary>
Motivation: 在LLM遗忘过程中不同样本的遗忘难度差异导致异步遗忘，部分知识保留过久或过度删除，需要一种能平衡遗忘的机制。

Method: 通过min-sup框架，内层寻找极端分布强化对难忘样本的关注，外层在该分布下更新参数；实现两种高效变体：BalDRO-G使用GroupDRO离散近似高损失子集；BalDRO-DV采用Donsker-Varadhan连续双重形式，自适应平滑权重。

Result: 在TOFU和MUSE基准实验中，BalDRO显著提升了遗忘质量和模型效用，超越现有方案，并且实现效率高，代码已公开。

Conclusion: BalDRO在LLM遗忘平衡任务中表现优于现有方法，兼顾遗忘质量与模型效能。

Abstract: As Large Language Models (LLMs) increasingly shape online content, removing targeted information from well-trained LLMs (also known as LLM unlearning) has become critical for web governance. A key challenge lies in sample-wise imbalance within the forget set: different samples exhibit widely varying unlearning difficulty, leading to asynchronous forgetting where some knowledge remains insufficiently erased while others become over-forgotten. To address this, we propose BalDRO, a novel and efficient framework for balanced LLM unlearning. BalDRO formulates unlearning as a min-sup process: an inner step identifies a worst-case data distribution that emphasizes hard-to-unlearn samples, while an outer step updates model parameters under this distribution. We instantiate BalDRO via two efficient variants: BalDRO-G, a discrete GroupDRO-based approximation focusing on high-loss subsets, and BalDRO-DV, a continuous Donsker-Varadhan dual method enabling smooth adaptive weighting within standard training pipelines. Experiments on TOFU and MUSE show that BalDRO significantly improves both forgetting quality and model utility over existing methods, and we release code for reproducibility.

</details>


### [69] [Geometric Stability: The Missing Axis of Representations](https://arxiv.org/abs/2601.09173)
*Prashant C. Raju*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Analysis of learned representations has a blind spot: it focuses on $similarity$, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce $geometric$ $stability$, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present $Shesha$, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated ($ρ\approx 0.01$) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2$\times$ more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability ($ρ= 0.89$-$0.96$); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying $how$ $reliably$ systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.

</details>


### [70] [$D^2Prune$: Sparsifying Large Language Models via Dual Taylor Expansion and Attention Distribution Awareness](https://arxiv.org/abs/2601.09176)
*Lang Xiong,Ning Liu,Ao Ren,Yuheng Bai,Haining Fang,BinYan Zhang,Zhe Jiang,Yujuan Tan,Duo Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) face significant deployment challenges due to their massive computational demands. % While pruning offers a promising compression solution, existing methods suffer from two critical limitations: (1) They neglect activation distribution shifts between calibration data and test data, resulting in inaccurate error estimations; (2) They overlook the long-tail distribution characteristics of activations in the attention module. To address these limitations, this paper proposes a novel pruning method, $D^2Prune$. First, we propose a dual Taylor expansion-based method that jointly models weight and activation perturbations for precise error estimation, leading to precise pruning mask selection and weight updating and facilitating error minimization during pruning. % Second, we propose an attention-aware dynamic update strategy that preserves the long-tail attention pattern by jointly minimizing the KL divergence of attention distributions and the reconstruction error. Extensive experiments show that $D^2Prune$ consistently outperforms SOTA methods across various LLMs (e.g., OPT-125M, LLaMA2/3, and Qwen3). Moreover, the dynamic attention update mechanism also generalizes well to ViT-based vision models like DeiT, achieving superior accuracy on ImageNet-1K.

</details>


### [71] [From Hawkes Processes to Attention: Time-Modulated Mechanisms for Event Sequences](https://arxiv.org/abs/2601.09220)
*Xinzi Tan,Kejian Zhang,Junhan Yu,Doudou Zhou*

Main category: cs.LG

TL;DR: 提出 Hawkes Attention，用可学习的神经核替代传统位置编码，专注多类事件时序与内容交互，实验验证其在多类别时序点过程与时间序列预测上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法仅通过位置编码注入时序信息，且使用共享或参数化衰减结构，无法高效捕捉异构事件与类特定时序效应，限制了模型泛化与解释性。

Method: 基于多变量Hawkes过程理论，设计可学习的每类神经核对 query、key、value 投影进行调制，构成全新的注意力算子，替代传统注意力中的位置编码与衰减结构。

Result: 在多种医疗、社交、商业、金融等实时点事件数据集上实验，Hawkes Attention 在预测准确率、AUC 等指标上均优于基线模型，并能顺利迁移至时间序列预测任务。

Conclusion: 本论文提出一种新型的注意力算子——Hawkes Attention，能够在Transformer框架中同时捕获多变点事件的时序与类型特征，显著提升多类别时序点过程建模的准确性。

Abstract: Marked Temporal Point Processes (MTPPs) arise naturally in medical, social, commercial, and financial domains. However, existing Transformer-based methods mostly inject temporal information only via positional encodings, relying on shared or parametric decay structures, which limits their ability to capture heterogeneous and type-specific temporal effects. Inspired by this observation, we derive a novel attention operator called Hawkes Attention from the multivariate Hawkes process theory for MTPP, using learnable per-type neural kernels to modulate query, key and value projections, thereby replacing the corresponding parts in the traditional attention. Benefited from the design, Hawkes Attention unifies event timing and content interaction, learning both the time-relevant behavior and type-specific excitation patterns from the data. The experimental results show that our method achieves better performance compared to the baselines. In addition to the general MTPP, our attention mechanism can also be easily applied to specific temporal structures, such as time series forecasting.

</details>


### [72] [GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization](https://arxiv.org/abs/2601.09233)
*Zhengyang Zhao,Lu Ma,Yizhen Jiang,Xiaochen Ma,Zimo Meng,Chengyu Shen,Lexiang Tang,Haoze Sun,Peng Pei,Wentao Zhang*

Main category: cs.LG

TL;DR: GIFT 通过有限温度 Gibbs 初始化，将监督嵌入到分布能量中，缓解 SFT 的分布坍塌，使 RL 能更充分探索并实现更高的性能。


<details>
  <summary>Details</summary>
Motivation: 传统 SFT 的严格监督导致表达分布收敛，限制了 RL 的探索空间，无法实现全局最优。

Method: 在后期训练的统一框架下，将监督目标视为有限温度能量势，形成 Gibbs 分布；相较于零温度极限的标准 SFT，GIFT 在初始化阶段保持原始先验，使分布更均匀。

Result: 在若干基准任务上，使用 GIFT 初始化的 RL 在性能上显著优于标准 SFT 及其他对比基线，验证了其有效性。

Conclusion: GIFT 通过在多任务学习中使用有限温度 Gibbs 初始化，解决了传统 SFT 导致的分布坍塌问题，使后续 RL 能更好探索并接近全局最优。

Abstract: The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.

</details>


### [73] [Reward Learning through Ranking Mean Squared Error](https://arxiv.org/abs/2601.09236)
*Chaitanya Kharyal,Calarina Muslimani,Matthew E. Taylor*

Main category: cs.LG

TL;DR: R4是一种基于人类评级的RL方法，采用可微分排序实现rMSE损失；在机器人运动任务上与现有方法竞争力强，且降低了反馈需求。


<details>
  <summary>Details</summary>
Motivation: 奖励设计在实际RL应用中是一个瓶颈；传统的二元偏好难以提供丰富且认知负担较大的监督，使用人类评分可以更充分利用反馈信息。

Method: R4使用评分为序数目标的rMSE损失，训练时采样轨迹集，预测回报并通过可微排序（soft ranks）对轨迹排序，然后将软排序与教师评级进行均方误差优化。

Result: 在OpenAI Gym和DeepMind Control Suite的机器人运动基准上，使用模拟人类反馈时，R4一致匹配或优于现有的评级和偏好RL方法，并且需要显著更少的反馈。

Conclusion: R4提供了一个基于评级的RL方法，并在形式上保证了解集的最小完整性；在机器人运动基准上与现有的评级或偏好RL方法相比，表现相当或更好，并且需要更少的反馈。

Abstract: Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., "bad," "neutral," "good"). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher's ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.

</details>


### [74] [XLinear: A Lightweight and Accurate MLP-Based Model for Long-Term Time Series Forecasting with Exogenous Inputs](https://arxiv.org/abs/2601.09237)
*Xinyang Chen,Huidong Jin,Yu Huang,Zaiwen Feng*

Main category: cs.LG

TL;DR: 提出轻量级MLP基模型XLinear，利用全局token捕捉外生信息并高效预测时序数据，实验表明其在各种基准上均超过主流方法。


<details>
  <summary>Details</summary>
Motivation: 现实中变量重要性并非均匀，且外生变量（如气象）成本低且可单向影响内生变量，传统Transformer在计算开销与排列不变性上存在不足，需更高效且能捕捉局部时序特征的模型。

Method: 利用基于多层感知器的轻量模型XLinear，采用来自内生变量的全局token作为枢纽，与外生变量交互；通过带sigmoid激活的MLP提取时序模式与变量间依赖，并以预测头融合这些信号。

Result: 在七个标准基准和五个含外生输入的实际数据集上，XLinear相较最先进方法显著提升准确性，同时保持低计算成本。

Conclusion: XLinear在多变量和受外因影响的单变量预测中，均显示出优异的准确性和效率。

Abstract: Despite the prevalent assumption of uniform variable importance in long-term time series forecasting models, real world applications often exhibit asymmetric causal relationships and varying data acquisition costs. Specifically, cost-effective exogenous data (e.g., local weather) can unilaterally influence dynamics of endogenous variables, such as lake surface temperature. Exploiting these links enables more effective forecasts when exogenous inputs are readily available. Transformer-based models capture long-range dependencies but incur high computation and suffer from permutation invariance. Patch-based variants improve efficiency yet can miss local temporal patterns. To efficiently exploit informative signals across both the temporal dimension and relevant exogenous variables, this study proposes XLinear, a lightweight time series forecasting model built upon MultiLayer Perceptrons (MLPs). XLinear uses a global token derived from an endogenous variable as a pivotal hub for interacting with exogenous variables, and employs MLPs with sigmoid activation to extract both temporal patterns and variate-wise dependencies. Its prediction head then integrates these signals to forecast the endogenous series. We evaluate XLinear on seven standard benchmarks and five real-world datasets with exogenous inputs. Compared with state-of-the-art models, XLinear delivers superior accuracy and efficiency for both multivariate forecasts and univariate forecasts influenced by exogenous inputs.

</details>


### [75] [HGATSolver: A Heterogeneous Graph Attention Solver for Fluid-Structure Interaction](https://arxiv.org/abs/2601.09251)
*Qin-Yi Zhang,Hong Wang,Siyao Liu,Haichuan Lin,Linying Cao,Xiao-Hu Zhou,Chen Chen,Shuangyi Wang,Zeng-Guang Hou*

Main category: cs.LG

TL;DR: HGATSolver将流固耦合系统编码为异质图，在节点/边层面实现域特定消息传递，并通过物理条件门控与跨域梯度平衡方法实现稳定高效的预测，实验显示其性能领先。


<details>
  <summary>Details</summary>
Motivation: 传统学习式求解器难以统一处理 FSI 中不同物理域的异质动力学与界面耦合导致的不一致响应，以及流体与固体区域学习难度差异导致的预测不稳定。

Method: 将流体、固体及界面区域分别映射为异质图中的不同节点/边类型，采用专用消息传递；使用物理条件门控作为可学习的自适应松弛因子；引入基于预测不确定性的跨域梯度平衡损失，动态平衡各域的优化目标。

Result: 在两个精心设计的 FSI 基准及公开数据集上，HGATSolver 超越现有方法，取得了更高的准确率和更好的数值稳定性。

Conclusion: HGATSolver通过异质图注意力机制、物理条件门控及梯度平衡损失，显著提升了流固耦合系统的预测稳定性与精度，已在两个自构 FSI 基准和公开数据集上获得最先进的性能。

Abstract: Fluid-structure interaction (FSI) systems involve distinct physical domains, fluid and solid, governed by different partial differential equations and coupled at a dynamic interface. While learning-based solvers offer a promising alternative to costly numerical simulations, existing methods struggle to capture the heterogeneous dynamics of FSI within a unified framework. This challenge is further exacerbated by inconsistencies in response across domains due to interface coupling and by disparities in learning difficulty across fluid and solid regions, leading to instability during prediction. To address these challenges, we propose the Heterogeneous Graph Attention Solver (HGATSolver). HGATSolver encodes the system as a heterogeneous graph, embedding physical structure directly into the model via distinct node and edge types for fluid, solid, and interface regions. This enables specialized message-passing mechanisms tailored to each physical domain. To stabilize explicit time stepping, we introduce a novel physics-conditioned gating mechanism that serves as a learnable, adaptive relaxation factor. Furthermore, an Inter-domain Gradient-Balancing Loss dynamically balances the optimization objectives across domains based on predictive uncertainty. Extensive experiments on two constructed FSI benchmarks and a public dataset demonstrate that HGATSolver achieves state-of-the-art performance, establishing an effective framework for surrogate modeling of coupled multi-physics systems.

</details>


### [76] [RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning](https://arxiv.org/abs/2601.09253)
*Zehua Liu,Shuqi Liu,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: RIFT通过奖励重加权自生成数据并采用稳定化损失，既充分利用正负样本，又避免训练崩溃，最终在算术基准上击败传统RFT。


<details>
  <summary>Details</summary>
Motivation: 传统的SFT和RFT要么依赖昂贵的专家数据，要么放弃有价值的负样本，导致数据效率低下。

Method: 利用所有自生成样本，通过标量奖励重新加权损失，学习正负轨迹，并引入稳定化损失公式以避免奖励直接乘法导致的训练崩溃。

Result: 在多种基础模型的数学基准测试中，RIFT持续优于RFT，并展现出更好的数据高效性。

Conclusion: RIFT是一种稳健且数据高效的对齐方法，能够充分利用混合质量的自生成数据。

Abstract: While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.

</details>


### [77] [Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability](https://arxiv.org/abs/2601.09261)
*Zhipeng Zhang,Zhenjie Yao,Kai Li,Lei Yang*

Main category: cs.LG

TL;DR: 为不可观测可靠性环境提供本地可信度推断的元认知调控机制，提升学习系统的自我诊断与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在未观测可靠性下，标准鲁棒学习可能收敛但形成系统错误信念；需解释何时学习、如何学习。

Method: 利用Monitor‑Trust‑Regulator（MTR）框架，将学习者内部动态视为第二层调节循环，构建经验可信度变量并软化学习更新。

Result: 自我诊断能在强化学习中实现有针对性的怀疑与恢复，在监督学习中分辨准确率回升与内部信念锁定二元性。

Conclusion: 自我诊断式元认知调控在EIUR环境下显著提升模型的认知辨识度，既修复了奖励回报问题，也揭示了在有误导数据情况下准确率与内部信念的分离。

Abstract: Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner's own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.
  We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner's internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.
  Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.

</details>


### [78] [Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction](https://arxiv.org/abs/2601.09285)
*Mianzhi Pan,JianFei Li,Peishuo Liu,Botian Wang,Yawen Ouyang,Yiming Rong,Hao Zhou,Jianbing Zhang*

Main category: cs.LG

TL;DR: 作者开发MOF-LLM，利用空间感知CPT、SFT和RL再SAPO优化，显著提升Qwen-3 8B在MOF结构预测上的表现，超越现有去噪和LLM方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测金属有机框架（MOF）的三维结构一直是一个重大挑战，尽管MOF在碳捕集、药物递送等领域具有广泛应用。

Method: 设计MOF-LLM——专为MOF块级结构预测而改造的LLM框架，采用空间感知的持续预训练（CPT）、结构监督微调（SFT）和匹配驱动的强化学习（RL）。在此基础上，引入了显式空间先验和通过Soft Adaptive Policy Optimization (SAPO) 优化结构稳定性，提升Qwen-3 8B在空间推理与MOF构建任务上的性能。

Result: MOF-LLM 在大规模实验中表现优于现有基于去噪和LLaMA等LLM的方案，具有更高的采样效率和更精准的MOF三维结构预测。

Conclusion: 首次提出并验证了面向MOF块级结构预测的LLM框架MOF-LLM，证明了空间感知预训练与强化学习相结合可以显著提升MOF结构生成的准确性与效率。

Abstract: Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.

</details>


### [79] [Single-Round Clustered Federated Learning via Data Collaboration Analysis for Non-IID Data](https://arxiv.org/abs/2601.09304)
*Sota Sugawara,Yuji Kawamata,Akihiro Toyoda,Tomoru Nakayama,Yukihiko Okada*

Main category: cs.LG

TL;DR: DC-CFL：单轮聚类+学习，减少通信，保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统的集群联邦学习需要多轮通信，通信资源受限时效果受限；因此需要单轮完成聚类与学习，以提升实用性。

Method: 利用DC分析中的交互信息，计算标签分布之间的总变差距离进行客户端相似度量，使用层次聚类确定群组，并在同一轮内完成群组模型的协同学习。

Result: 在多个公开数据集、典型非I.I.D.场景下，DC-CFL在单轮通信下的准确率接近多轮基准，沟通成本显著降低。

Conclusion: DC-CFL在单轮通信下实现了与多轮基准相当的准确率，证明了在通信受限环境下的可行性。

Abstract: Federated Learning (FL) enables distributed learning across multiple clients without sharing raw data. When statistical heterogeneity across clients is severe, Clustered Federated Learning (CFL) can improve performance by grouping similar clients and training cluster-wise models. However, most CFL approaches rely on multiple communication rounds for cluster estimation and model updates, which limits their practicality under tight constraints on communication rounds. We propose Data Collaboration-based Clustered Federated Learning (DC-CFL), a single-round framework that completes both client clustering and cluster-wise learning, using only the information shared in DC analysis. DC-CFL quantifies inter-client similarity via total variation distance between label distributions, estimates clusters using hierarchical clustering, and performs cluster-wise learning via DC analysis. Experiments on multiple open datasets under representative non-IID conditions show that DC-CFL achieves accuracy comparable to multi-round baselines while requiring only one communication round. These results indicate that DC-CFL is a practical alternative for collaborative AI model development when multiple communication rounds are impractical.

</details>


### [80] [GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR](https://arxiv.org/abs/2601.09361)
*Jiaying Zhang,Lei Shi,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: GeoRA通过几何感知的低秩适配，克服了RLVR中光谱崩塌和优化不稳问题，显著提升算术基准表现，并在跨域任务中展现出更强的泛化和抗灾难性遗忘能力。


<details>
  <summary>Details</summary>
Motivation: RLVR对可验证奖励系统至关重要，但现有针对SFT设计的参数高效方法无法捕捉RLVR的优化动态和几何结构，直接应用会导致光谱崩塌和优化不稳定。

Method: GeoRA在几何约束子空间内利用SVD提取主方向初始化适配器，并冻结剩余分量，既保持预训练几何结构，也通过稠密算子实现高效GPU计算。

Result: 在Qwen和Llama上实验，GeoRA有效缓解了几何失配带来的优化瓶颈，显著优于传统低秩基线，在关键数理基准上实现了SOTA，并在离域任务中表现出更好的泛化与抗灾难性遗忘能力。

Conclusion: GeoRA为RLVR提供了稳定、高效且能保持预训练几何的低秩适配方案，达成了最新的性能标准。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.

</details>


### [81] [Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps](https://arxiv.org/abs/2601.09428)
*Siyi Li,Joseph G. Lambourne,Longfei Zhang,Pradeep Kumar Jayaraman,Karl. D. D. Willis*

Main category: cs.LG

TL;DR: 提出一种基于几何构造序列的 CAD 曲线生成方法，并通过强化学习进一步优化，显著提升质量并保持参数化编辑能力。


<details>
  <summary>Details</summary>
Motivation: 解决 CAD 形状生成过程中过多自由度导致的质量不稳定问题，借鉴语言模型的链式思考，在参数化约束下提升生成质量并支持灵活编辑。

Method: 利用曲线偏移、旋转、交点等基本几何操作构建一条由设计者输入到最终曲线的构造序列；随后对这些序列使用强化学习进行优化。

Result: 实验表明，加入构造步骤后生成质量提升，且在多种评估指标上优于传统方法；进一步使用强化学习可提升更多未显式优化的指标。

Conclusion: 该方法通过在设计师输入几何与最终曲线之间引入一系列几何构造步骤，显著提升了 CAD 曲线生成质量，并保留了可变参数，支持高精度的参数化编辑；再结合强化学习可进一步改进多项指标。

Abstract: We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer's input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.

</details>


### [82] [On the Hardness of Computing Counterfactual and Semifactual Explanations in XAI](https://arxiv.org/abs/2601.09455)
*André Artelt,Martin Olsen,Kevin Tierney*

Main category: cs.LG

TL;DR: 本文综述并扩展了因果与半因果解释生成的复杂性，指出其不可求解与不可逼近性，警示XAI与政策制定者


<details>
  <summary>Details</summary>
Motivation: 解释机器学习模型的重要性与使用可解释方法的需求

Method: 综述文献中关于因果与半因果解释的计算复杂性，并给出不可逼近性证明

Result: 在多案例中生成说明不可算，且在某些假设下不可用近似近似

Conclusion: 说明的计算难度对XAI发展与监管具有重大影响

Abstract: Providing clear explanations to the choices of machine learning models is essential for these models to be deployed in crucial applications. Counterfactual and semi-factual explanations have emerged as two mechanisms for providing users with insights into the outputs of their models. We provide an overview of the computational complexity results in the literature for generating these explanations, finding that in many cases, generating explanations is computationally hard. We strengthen the argument for this considerably by further contributing our own inapproximability results showing that not only are explanations often hard to generate, but under certain assumptions, they are also hard to approximate. We discuss the implications of these complexity results for the XAI community and for policymakers seeking to regulate explanations in AI.

</details>


### [83] [SimMerge: Learning to Select Merge Operators from Similarity Signals](https://arxiv.org/abs/2601.09473)
*Oliver Bolton,Aakanksha,Arash Ahmadian,Sara Hooker,Marzieh Fadaee,Beyza Ermis*

Main category: cs.LG

TL;DR: 	exttt{simmerge}利用未标记探针特征预测模型合并效果，自动选择合并算子、模型集合与顺序，显著提升二路、三路乃至111B参数合并性能，且无需昂贵实验循环。


<details>
  <summary>Details</summary>
Motivation: 当大型语言模型（LLM）规模不断扩大时，单纯采用多任务训练成本高昂。模型合并提供了兼顾性能与资源的有效替代方案，但规模化合并过程仍需繁复的实验来挑选最佳合并策略。

Method: 提出	exttt{simmerge}——一种基于未标记探针的任务无关相似性信号进行预测的模型合并选择方法。通过计算功能性与结构化特征，预测二路合并的性能，从而自动选取最优合并算子、待合并模型集合以及合并顺序，避免昂贵的合并‑评估循环。

Result: 在7B参数LLM的二路合并实验中，	exttt{simmerge}超越了传统合并算子的表现，并能推广至多路合并及111B参数模型的合并，而无需重新训练。还实现了可在线加入新任务、模型与算子的bandit变体。

Conclusion: 学习合并策略是一条实用路径，能够在模型检查点目录庞大且评估预算有限时，实现可扩展的模型组合。

Abstract: Model merging enables multiple large language models (LLMs) to be combined into a single model while preserving performance. This makes it a valuable tool in LLM development, offering a competitive alternative to multi-task training. However, merging can be difficult at scale, as successful merging requires choosing the right merge operator, selecting the right models, and merging them in the right order. This often leads researchers to run expensive merge-and-evaluate searches to select the best merge. In this work, we provide an alternative by introducing \simmerge{}, \emph{a predictive merge-selection method} that selects the best merge using inexpensive, task-agnostic similarity signals between models. From a small set of unlabeled probes, we compute functional and structural features and use them to predict the performance of a given 2-way merge. Using these predictions, \simmerge{} selects the best merge operator, the subset of models to merge, and the merge order, eliminating the expensive merge-and-evaluate loop. We demonstrate that we surpass standard merge-operator performance on 2-way merges of 7B-parameter LLMs, and that \simmerge{} generalizes to multi-way merges and 111B-parameter LLM merges without retraining. Additionally, we present a bandit variant that supports adding new tasks, models, and operators on the fly. Our results suggest that learning how to merge is a practical route to scalable model composition when checkpoint catalogs are large and evaluation budgets are tight.

</details>


### [84] [Terminally constrained flow-based generative models from an optimal control perspective](https://arxiv.org/abs/2601.09474)
*Weiguo Gao,Ming Li,Qianxiao Li*

Main category: cs.LG

TL;DR: TOCFlow是一种利用终端最优控制、几何敏感阻尼因子为预训练流模型提供高效约束采样的算法，在多科学任务中超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决在预训练流生成模型下对终端受限分布进行采样的问题；传统欧氏梯度或投影方法缺乏对流形几何的尊重，导致约束满足不佳。

Method: 提出基于终端最优控制的TOCFlow方法，利用Hamilton-Jacobi-Bellman方程和Hamiltonian最小化获得反馈控制；在终端协移动框架中求解控制问题，得到Riemann曲率导向的闭式标量阻尼因子；该方法避免矩阵求逆，兼顾Gauss-Newton级几何一致性。

Result: 在三类高维科学任务（Darcy流、受限轨迹规划、满足Kolmogorov谱尺度的湍流快照生成）中，TOCFlow已优于欧氏梯度和投影基线，既提升了约束满足水平，又保留了模型的生成质量。

Conclusion: TOCFlow在终端受限分布的采样中，既能显著提高约束满足度，又能保持预训练流模型的生成质量，展示出优越的几何一致性与计算效率。

Abstract: We address the problem of sampling from terminally constrained distributions with pre-trained flow-based generative models through an optimal control formulation. Theoretically, we characterize the value function by a Hamilton-Jacobi-Bellman equation and derive the optimal feedback control as the minimizer of the associated Hamiltonian. We show that as the control penalty increases, the controlled process recovers the reference distribution, while as the penalty vanishes, the terminal law converges to a generalized Wasserstein projection onto the constraint manifold. Algorithmically, we introduce Terminal Optimal Control with Flow-based models (TOCFlow), a geometry-aware sampling-time guidance method for pre-trained flows. Solving the control problem in a terminal co-moving frame that tracks reference trajectories yields a closed-form scalar damping factor along the Riemannian gradient, capturing second-order curvature effects without matrix inversions. TOCFlow therefore matches the geometric consistency of Gauss-Newton updates at the computational cost of standard gradient guidance. We evaluate TOCFlow on three high-dimensional scientific tasks spanning equality, inequality, and global statistical constraints, namely Darcy flow, constrained trajectory planning, and turbulence snapshot generation with Kolmogorov spectral scaling. Across all settings, TOCFlow improves constraint satisfaction over Euclidean guidance and projection baselines while preserving the reference model's generative quality.

</details>


### [85] [Deep Operator Networks for Surrogate Modeling of Cyclic Adsorption Processes with Varying Initial Conditions](https://arxiv.org/abs/2601.09491)
*Beatrice Ceccanti,Mattia Galanti,Ivo Roghair,Martin van Sint Annaland*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep Operator Networks are emerging as fundamental tools among various neural network types to learn mappings between function spaces, and have recently gained attention due to their ability to approximate nonlinear operators. In particular, DeepONets offer a natural formulation for PDE solving, since the solution of a partial differential equation can be interpreted as an operator mapping an initial condition to its corresponding solution field. In this work, we applied DeepONets in the context of process modeling for adsorption technologies, to assess their feasibility as surrogates for cyclic adsorption process simulation and optimization. The goal is to accelerate convergence of cyclic processes such as Temperature-Vacuum Swing Adsorption (TVSA), which require repeated solution of transient PDEs, which are computationally expensive. Since each step of a cyclic adsorption process starts from the final state of the preceding step, effective surrogate modeling requires generalization across a wide range of initial conditions. The governing equations exhibit steep traveling fronts, providing a demanding benchmark for operator learning. To evaluate functional generalization under these conditions, we construct a mixed training dataset composed of heterogeneous initial conditions and train DeepONets to approximate the corresponding solution operators. The trained models are then tested on initial conditions outside the parameter ranges used during training, as well as on completely unseen functional forms. The results demonstrate accurate predictions both within and beyond the training distribution, highlighting DeepONets as potential efficient surrogates for accelerating cyclic adsorption simulations and optimization workflows.

</details>


### [86] [Class Adaptive Conformal Training](https://arxiv.org/abs/2601.09522)
*Badr-Eddine Marani,Julio Silva-Rodriguez,Ismail Ben Ayed,Maria Vakalopoulou,Stergios Christodoulidis,Jose Dolz*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep neural networks have achieved remarkable success across a variety of tasks, yet they often suffer from unreliable probability estimates. As a result, they can be overconfident in their predictions. Conformal Prediction (CP) offers a principled framework for uncertainty quantification, yielding prediction sets with rigorous coverage guarantees. Existing conformal training methods optimize for overall set size, but shaping the prediction sets in a class-conditional manner is not straightforward and typically requires prior knowledge of the data distribution. In this work, we introduce Class Adaptive Conformal Training (CaCT), which formulates conformal training as an augmented Lagrangian optimization problem that adaptively learns to shape prediction sets class-conditionally without making any distributional assumptions. Experiments on multiple benchmark datasets, including standard and long-tailed image recognition as well as text classification, demonstrate that CaCT consistently outperforms prior conformal training methods, producing significantly smaller and more informative prediction sets while maintaining the desired coverage guarantees.

</details>


### [87] [Constraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels](https://arxiv.org/abs/2601.09579)
*Fiona Murphy,Alessio Benavoli*

Main category: cs.LG

TL;DR: 本文把两种 kernel‑based GC 方法统一到 KPCR，提出带 SIC 限制的 Gaussian Process score 方法，并构建同步因果识别算法，实验显示其优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 对现有两种前沿核方法 Granger 因果性（GC）进行理论统一，并提出更高性能的非线性因果发现与同步因果识别方法。

Method: ① 将两种 kernel‑based GC 方法统一为 Kernel Principal Component Regression (KPCR) 框架；② 基于 GP 的 score–based 模型加入 Smooth Information Criterion (SIC) 对边际似然做惩罚；③ 构建完全基于 GC 的同步因果识别算法（GP_SIC）。

Result: 在与当前最优的非线性时间序列因果发现方法和同步时间序列因果发现算法的比较中，所提出的 GP_SIC 方法表现出更优的性能。

Conclusion: KPCR 统一框架提升了非线性因果识别准确性，GP_SIC 方案在同步与非同步因果发现任务中均具备显著优势。

Abstract: Kernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unification, demonstrating that this approach can improve causal identification. Additionally, we introduce a Gaussian Process score-based model with Smooth Information Criterion penalisation on the marginal likelihood, and demonstrate improved performance over existing state of the art time-series nonlinear causal discovery methods. Furthermore, we propose a contemporaneous causal identification algorithm, fully based on GC, using the proposed score-based $GP_{SIC}$ method, and compare its performance to a state of the art contemporaneous time series causal discovery algorithm.

</details>


### [88] [Energy-Entropy Regularization: The True Power of Minimal Looped Transformers](https://arxiv.org/abs/2601.09588)
*Wai-Lun Lam*

Main category: cs.LG

TL;DR: 使用Tsallis熵+哈密顿动力学框架重塑损失曲线，成功训练小型循环Transformer完成长序列推理任务。


<details>
  <summary>Details</summary>
Motivation: 近年来提出的循环Transformer相比传统深度架构具有更优的推理能力，但在基准任务中训练单头循环模型往往遇到高度非凸与不规则的损失曲面，导致优化停滞在局部最优或鞍点，难以发现全局最小点。

Method: 我们提出一种新的训练框架，结合Tsallis熵和哈密顿动力学，将参数更新视为物理流，从而改变损失曲线的几何结构，使优化过程更易跨越障碍。

Result: 采用该框架，我们成功训练了一个维度d=8的单头循环Transformer，在输入序列长度1000的induction头任务上实现了学习。

Conclusion: 实验结果揭示了循环Transformer优越推理能力背后的内部机制，并证明了通过改造损失曲线几何可有效克服训练瓶颈。

Abstract: Recent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle points of the loss landscape, preventing the model from discovering the global minimum point. The internal mechanisms of these single-head looped transformer models remain poorly understood, and training them from scratch remains a significant challenge. In this paper, we propose a novel training framework that leverages Tsallis entropy and Hamiltonian dynamics to transform the geometry of the loss landscape. By treating the parameter updates as a physical flow, we successfully trained a single-head looped Transformer with model dimension $d = 8$ to solve induction head task with input sequence length of 1000 tokens. This success reveals the internal mechanism behind the superior reasoning capability.

</details>


### [89] [Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric](https://arxiv.org/abs/2601.09624)
*Jiali Cheng,Ziheng Chen,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 提出CUD指标，用电路级信号评估遗忘难度，实验验证其稳健性，并揭示易难遗忘样本的电路差异。


<details>
  <summary>Details</summary>
Motivation: 对构建可信且合规的语言模型而言，机器遗忘已变得至关重要。不同样本在遗忘过程中表现差异显著，部分样本易被有效删除，另一些则依旧存在。

Method: 我们基于模型电路（模型内部交互路径）研究这一现象，提出Circuit‑guided Unlearning Difficulty (CUD)——一种预遗忘阶段的指标，利用电路级信号为每个样本分配连续难度得分。

Result: 实验表明CUD能够稳健区分易难遗忘样本，并在不同遗忘方法下保持稳定。进一步分析揭示：易遗忘样本依赖较短、浅层的交互路径，集中在模型早期至中期；难遗忘样本则循环使用更长、深层路径，接近后期计算。

Conclusion: CUD为理解遗忘难度提供了细粒度、可解释的机械视角，开启了基于模型机制的遗忘方法研究方向。

Abstract: Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information. We study this problem from a mechanistic perspective based on model circuits--structured interaction pathways that govern how predictions are formed. We propose Circuit-guided Unlearning Difficulty (CUD), a {\em pre-unlearning} metric that assigns each sample a continuous difficulty score using circuit-level signals. Extensive experiments demonstrate that CUD reliably separates intrinsically easy and hard samples, and remains stable across unlearning methods. We identify key circuit-level patterns that reveal a mechanistic signature of difficulty: easy-to-unlearn samples are associated with shorter, shallower interactions concentrated in earlier-to-intermediate parts of the original model, whereas hard samples rely on longer and deeper pathways closer to late-stage computation. Compared to existing qualitative studies, CUD takes a first step toward a principled, fine-grained, and interpretable analysis of unlearning difficulty; and motivates the development of unlearning methods grounded in model mechanisms.

</details>


### [90] [Exploring Fine-Tuning for Tabular Foundation Models](https://arxiv.org/abs/2601.09654)
*Aditya Tanna,Pratinav Seth,Mohamed Bouadi,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 零射击已达标，微调效果因模型、数据及方法不同而异；PEFT和元学习在合适条件下可略优，但常规SFT往往退步。


<details>
  <summary>Details</summary>
Motivation: 探究在表格基础模型上微调是否真能提升性能，以及何时“微调”才是有效策略，以提供实际使用指南。

Method: 通过在TALENT、OpenML‑CC18、TabZilla等基准上对零射击、元学习、全监督微调(SFT)和参数高效微调(PEFT)进行系统比较，并分析数据不平衡、规模、维数等因素对结果的影响。

Result: 发现零射击已有强劲表现，元学习和PEFT在特定情形下能略微提升，SFT往往不利于准确度或校准；不同数据因素对效果影响显著。

Conclusion: 在表格基础模型（TFM）中，零射击能力已经非常强，微调效果高度依赖于模型和数据，参数高效微调（PEFT）和元学习在特定条件下可取得中等提升，但完全监督微调常导致准确度或校准下降。

Abstract: Tabular Foundation Models (TFMs) have recently shown strong in-context learning capabilities on structured data, achieving zero-shot performance comparable to traditional machine learning methods. We find that zero-shot TFMs already achieve strong performance, while the benefits of fine-tuning are highly model and data-dependent. Meta-learning and PEFT provide moderate gains under specific conditions, whereas full supervised fine-tuning (SFT) often reduces accuracy or calibration quality. This work presents the first comprehensive study of fine-tuning in TFMs across benchmarks including TALENT, OpenML-CC18, and TabZilla. We compare Zero-Shot, Meta-Learning, Supervised (SFT), and parameter-efficient (PEFT) approaches, analyzing how dataset factors such as imbalance, size, and dimensionality affect outcomes. Our findings cover performance, calibration, and fairness, offering practical guidelines on when fine-tuning is most beneficial and its limitations.

</details>


### [91] [Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection](https://arxiv.org/abs/2601.09684)
*Ziyu Yang,Guibin Chen,Yuxin Yang,Aoxiong Zeng,Xiangquan Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape's capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.

</details>


### [92] [Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design](https://arxiv.org/abs/2601.09693)
*Lisa Schneckenreiter,Sohvi Luukkonen,Lukas Friedrich,Daniel Kuhn,Günter Klambauer*

Main category: cs.LG

TL;DR: ConGLUDe通过对比几何学习整合蛋白与配体信息，在虚拟筛选、靶点猎捕等任务中取得突破，呈现通用药物发现潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的结构基与配体基药物设计分离使用数据与模型假设，限制了规模化联合利用。


Method: 提出ConGLUDe--单一对比几何模型，结合几何蛋白质编码器产生全蛋白表示与隐式绑定位点嵌入，再配合快速配体编码器，透过对比学习把配体与全蛋白及多候选位点对齐，实现配体条件化位点预测、虚拟筛选与靶点猎捕。


Result: 在多项基准实验中，ConGLUDe在无绑定口袋信息条件下实现零样本虚拟筛选的SOTA性能；在挑战性的靶点猎捕任务中显著优于现有方法；并在配体条件化位点选择上表现竞争力。


Conclusion: 统一结构与配体训练显示优势，ConGLUDe迈向通用药物发现基础模型。


Abstract: Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets. By aligning ligands with both global protein representations and multiple candidate binding sites through contrastive learning, ConGLUDe supports ligand-conditioned pocket prediction in addition to virtual screening and target fishing, while being trained jointly on protein-ligand complexes and large-scale bioactivity data. Across diverse benchmarks, ConGLUDe achieves state-of-the-art zero-shot virtual screening performance in settings where no binding pocket information is provided as input, substantially outperforms existing methods on a challenging target fishing task, and demonstrates competitive ligand-conditioned pocket selection. These results highlight the advantages of unified structure-ligand training and position ConGLUDe as a step toward general-purpose foundation models for drug discovery.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [93] [Robust Consensus-Based Distributed Beamforming for Wideband Cell-free Multi-RIS MISO Systems](https://arxiv.org/abs/2601.08946)
*Konstantinos D. Katsanos,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 针对宽带细胞自由MISO系统，提出分散式RIS波束成形，仿真显示优于中心化方法


<details>
  <summary>Details</summary>
Motivation: 提升宽带细胞自由多RIR-增强MISO系统的能效与弹性

Method: 提出分散式协同主动与被动波束成形方案，并采用共识更新优化RIS相位配置，处理误差信道与实际频率选择性

Result: 仿真表明分散式设计优于基于各Lorentzian型宽带模型的中心式方案

Conclusion: 本研究表明分散式RIS配置可有效降低中心处理负载，提供更优的性能

Abstract: The cell-free networking paradigm constitutes a revolutionary architecture for future generations of wireless networks, which has been recently considered in synergy with Reconfigurable Intelligent Surfaces (RISs), a promising physical-layer technology for signal propagation programmability. In this paper, we focus on wideband cell-free multi-RIS-empowered Multiple-Input Single-Output (MISO) systems and present a decentralized cooperative active and passive beamforming scheme, aiming to provide an efficient alternative towards the cooperation overhead of available centralized schemes depending on central processing unit. Considering imperfect channel information availability and realistic frequency selectivity behavior of each RIS's element response, we devise a distributed optimization approach based on consensus updates for the RISs' phase configurations. Our simulation results showcase that the proposed distributed design is superior to centralized schemes that are based on various Lorentzian-type wideband modeling approaches for the RISs.

</details>


### [94] [Joint DOA and Non-circular Phase Estimation of Non-circular Signals for Antenna Arrays: Block Sparse Bayesian Learning Method](https://arxiv.org/abs/2601.09148)
*Zihan Shen,Jiaqi Li,Xudong Dong,Xiaofei Zhang*

Main category: eess.SP

TL;DR: 通过块稀疏 Bayesian 学习与快速边际似然最大化，提出适用于未知 NC 相位的高效 DOA 估计算法。


<details>
  <summary>Details</summary>
Motivation: 传统DOA估计对信号的非圆相位要求较高，难以处理未知 NC 阶段的情况。

Method: 构造块稀疏 NC 信号表示模型，采用置换策略利用块内结构信息；建立稀疏概率模型并在 BSBL 框架下推导成本函数；采用快速边际似然最大化（FMLM）算法实现快速信号恢复。

Result: 仿真结果显示所提方法在未知 NC 阶段下实现更高的恢复精度和更低的误差谱。

Conclusion: 该方法能够有效提升非圆信号的 DOA 估计性能，适用于未知相位场景。

Abstract: This letter proposes a block sparse Bayesian learning (BSBL) algorithm of non-circular (NC) signals for direction-of-arrival (DOA) estimation, which is suitable for arbitrary unknown NC phases. The block sparse NC signal representation model is constructed through a permutation strategy, capturing the available intra-block structure information to enhance recovery performance. After that, we create the sparse probability model and derive the cost function under BSBL framework. Finally, the fast marginal likelihood maximum (FMLM) algorithm is introduced, enabling the rapid implementation of signal recovery by the addition and removal of basis functions. Simulation results demonstrate the effectiveness and the superior performance of our proposed method.

</details>


### [95] [User-Centric Stream Sensing for Grant-Free Access: Deep Learning with Covariance Differencing](https://arxiv.org/abs/2601.09168)
*Sojeong Park,Yeongjun Kim,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 差分协方差+DL分类器提升GF接入的流检测准确率，尤其在流数大于天线数的过载场景中效果显著。


<details>
  <summary>Details</summary>
Motivation: 在大规模连接环境中，传统的用户端感知方法无法有效抑制碰撞，尤其当激活流超过接收天线时。

Method: 通过将问题从求总流数转化为利用协方差差分提取新激活的流；结合基于通道相关性的窗口尺寸理论界限，并使用深度学习分类器校正因有限采样导致的残余干扰。

Result: 在IID平坦瑞利衰落及标准化信道环境下的仿真表明，该方法在过载情况下仍优于非DL基线，并表现出鲁棒性。

Conclusion: 本文提出的差分流检测框架在GF接入的过载场景中能显著降低碰撞风险，并与传统方法相比保持更高的检测准确率。

Abstract: Grant-free (GF) access is essential for massive connectivity but faces collision risks due to uncoordinated transmissions. While user-side sensing can mitigate these collisions by enabling autonomous transmission decisions, conventional methods become ineffective in overloaded scenarios where active streams exceed receive antennas. To address this problem, we propose a differential stream sensing framework that reframes the problem from estimating the total stream count to isolating newly activated streams via covariance differencing. We analyze the covariance deviation induced by channel variations to establish a theoretical bound based on channel correlation for determining the sensing window size. To mitigate residual interference from finite sampling, a deep learning (DL) classifier is integrated. Simulations across both independent and identically distributed flat Rayleigh fading and standardized channel environments demonstrate that the proposed method consistently outperforms non-DL baselines and remains robust in overloaded scenarios.

</details>


### [96] [WiFo-E: A Scalable Wireless Foundation Model for End-to-End FDD Precoding in Communication Networks](https://arxiv.org/abs/2601.09186)
*Weibo Wen,Shijian Gao,Haotian Zhang,Xiang Cheng,Liuqing Yang*

Main category: eess.SP

TL;DR: WiFo‑E：一种稀疏 MoE Transformer 基础模型，用于高效、可扩展的大规模 MIMO 预编码。


<details>
  <summary>Details</summary>
Motivation: 准确的预编码需要高效的信道状态信息（CSI）获取，传统端到端学习框架缺乏可扩展性，难以在不同天线/用户配置之间迁移。

Method: 构建 WiFo‑E 基础模型，通过多任务预训练学习不同系统配置下的通用无线表示；核心采用稀疏 Mixture‑of‑Experts Transformer，按需激活专家子参数，降低任务干扰并提升训练效率。

Result: 在大量仿真中，WiFo‑E 超越单配置训练方案，并能强劲推广到未见过的系统配置，展现了更灵活、更高效的端到端预编码性能。

Conclusion: WiFo‑E 为大规模 MIMO 预编码提供了可扩展、易于迁移的基础模型，具备在不同配置下自适应调优的能力。

Abstract: Accurate precoding in massive multiple-input multiple-output (MIMO) frequency-division duplexing (FDD) systems relies on efficient channel state information (CSI) acquisition. End-to-end learning frameworks improve performance by jointly optimizing this process, but they lack scalability and fail to generalize across different system configurations, such as varying numbers of antennas and users. To overcome this limitation, we introduce WiFo-E, a wireless foundation model designed for scalable end-to-end precoding. WiFo-E employs multi-task pretraining on a diverse set of configurations to learn transferable representations of underlying wireless principles. Central to the model is a sparse Mixture-of-Experts (MoE) Transformer architecture, which mitigates task interference and enhances training efficiency by activating specialized parameter subsets adaptively. Extensive simulations demonstrate that WiFo-E outperforms conventional per-configuration training and shows strong generalization to unseen system configurations, providing a flexible and efficient foundation for adaptive massive MIMO precoding.

</details>


### [97] [Range-Doppler-Acceleration Estimation for Fast-Moving and Accelerating Targets](https://arxiv.org/abs/2601.09317)
*Nadav Neuberger,Simon Kollecker,Martin Kaeske*

Main category: eess.SP

TL;DR: An adaptive, waveform‑independent Range‑Doppler processing method that handles complex target motion and wideband effects, preserving SNR and efficiency, with performance bounded by a unified scene‑system metric.


<details>
  <summary>Details</summary>
Motivation: Traditional Range‑Doppler estimation techniques assume linear motion, narrowband signals or constant velocity—assumptions that fail under quadratic range‑time dynamics, high velocities, accelerations, or wideband operation, causing intra‑pulse Doppler shifts, target migration and degraded performance.

Method: A waveform‑independent Range‑Doppler compression framework that models and compensates for quadratic range‑time behavior, high radial velocity/acceleration, and wideband effects within a coherent processing interval, yielding minimal SNR loss and practical computational complexity.

Result: Experimental/comparative analysis demonstrates that the proposed approach outperforms conventional methods by reducing estimation bias and performance degradation due to adverse conditions, while maintaining acceptable SNR and computation cost.

Conclusion: The proposed generalized, waveform-independent Range‑Doppler compression method effectively mitigates intra‑pulse Doppler shift/stretch and target migration while incurring minimal SNR loss and remaining computationally efficient. Its performance limits, expressed in a unified metric, expose how scene and system parameters affect estimation accuracy.

Abstract: A central aspect of every pulsed radar signal processor is the targets Range-Doppler estimation within a Coherent Processing Interval. Conventional methods typically rely on simplifying assumptions, such as linear target motion, narrowband operation, or constant velocity, to enable fast computation. However, these assumptions break down in scenarios involving quadratic range-time behavior, high radial velocities or accelerations, or wideband signals, leading to undesired effects such as intra-pulse Doppler shift/stretch and target migration across Range-Doppler cells. This paper presents a generalized waveform-independent Range-Doppler compression approach that compensates for these effects while maintaining minimal Signal-to-Noise-Ratio loss and practical computational efficiency. The performance limits of the proposed method are analyzed and expressed through a unified metric that depends on both scene and system parameters. Comparison with other approaches is presented, showing their estimation bias and performance degradation.

</details>


### [98] [Two-Scale Spatial Deployment for Cost-Effective Wireless Networks via Cooperative IRSs and Movable Antennas](https://arxiv.org/abs/2601.09463)
*Ying Gao,Qingqing Wu,Ziyuan Zheng,Yanze Zhu,Wen Chen,Xin Lin,Shanpu Shen*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper proposes a two-scale spatial deployment strategy to ensure reliable coverage for multiple target areas, integrating macroscopic intelligent reflecting surfaces (IRSs) and fine-grained movable antennas (MAs). Specifically, IRSs are selectively deployed from candidate sites to shape the propagation geometry, while MAs are locally repositioned among discretized locations to exploit small-scale channel variations. The objective is to minimize the total deployment cost of MAs and IRSs by jointly optimizing the IRS site selection, MA positions, transmit precoding, and IRS phase shifts, subject to the signal-to-noise ratio (SNR) requirements for all target areas. This leads to a challenging mixed-integer non-convex optimization problem that is intractable to solve directly. To address this, we first formulate an auxiliary problem to verify the feasibility. A penalty-based double-loop algorithm integrating alternating optimization and successive convex approximation (SCA) is developed to solve this feasibility issue, which is subsequently adapted to obtain a suboptimal solution for the original cost minimization problem. Finally, based on the obtained solution, we formulate an element refinement problem to further reduce the deployment cost, which is solved by a penalty-based SCA algorithm. Simulation results demonstrate that the proposed designs consistently outperform benchmarks relying on independent area planning or full IRS deployment in terms of cost-efficiency. Moreover, for cost minimization, MA architectures are preferable in large placement apertures, whereas fully populated FPA architectures excel in compact ones; for worst-case SNR maximization, MA architectures exhibit a lower cost threshold for feasibility, while FPA architectures can attain peak SNR at a lower total cost.

</details>


### [99] [A Hybrid Machine Learning Framework for Improved Short-Term Peak-Flow Forecasting](https://arxiv.org/abs/2601.09336)
*Gabriele Bertoli,Kai Schroeter,Rossella Arcucci,Enrica Caporali*

Main category: eess.SP

TL;DR: 混合XGBoost+RF预测框架显著提升河流水量与峰值预测精度，表现优于EFAS，且计算简便、易推广。


<details>
  <summary>Details</summary>
Motivation: 传统基于过程的水文学模型及纯数据驱动方法在极端事件，尤其峰值流量预测中表现不足，需开发更稳健、准确的预测框架以支持洪水风险管理。

Method: 使用XGBoost对连续流量进行建模，采用RF针对峰值流量进行训练，随后将两模型输出融合构建最终预测；在857个拉马H-CE流域上以6小时雨量与流量观测训练并评估。

Result: 模型在71%的流域中KGE>0.90；峰值检出率87%，误报率13%，在与EFAS比较时实现峰值误差更低、误报更少、整体流量预测更佳。

Conclusion: 本研究提出的混合预测框架利用XGBoost和RF模型提升了河流流量及峰值预测的准确性，表现出高KGE、低峰值误差和低误警率，证明了将过程模型和机器学习结合的有效性，可广泛推广至不同流域并支持洪水预警体系。

Abstract: Reliable river flow forecasting is an essential component of flood risk management and early warning systems. It enables improved emergency response coordination and is critical for protecting infrastructure, communities, and ecosystems from extreme hydrological events. Process-based hydrological models and purely data-driven approaches often underperform during extreme events, particularly in forecasting peak flows. To address this limitation, this study introduces a hybrid forecasting framework that couples Extreme Gradient Boosting (XGBoost) and Random Forest (RF). XGBoost is employed for continuous streamflow forecasting, while RF is specifically trained for peak-flow prediction, and the two outputs are combined into an enhanced forecast. The approach is implemented across 857 catchments of the LamaH-CE dataset, using rainfall and discharge observations at 6-hour resolution. Results demonstrate consistently high skill, with 71% of catchments achieving a Kling-Gupta Efficiency (KGE) greater than 0.90. Peak-flow detection reaches 87%, with a false-alarm rate of 13%. Compared to the European Flood Awareness System (EFAS), the framework achieves lower peak-magnitude errors, fewer false alarms, and improved streamflow and peak-flow forecasting accuracy. The proposed framework is computationally lightweight, scalable, and easily transferable across watersheds, with training times of only seconds on standard CPUs. These findings highlight the potential of integrating hydrological understanding with efficient machine learning to improve the accuracy and reliability of operational flood forecasting, and outline future directions for hybrid hydrological-machine learning model development.

</details>


### [100] [Echo-Side Integrated Sensing and Communication via Space-Time Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2601.09484)
*Marouan Mizmizi,Stefano Tebaldini,Umberto Spagnolini*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents an echo-side modulation framework for integrated sensing and communication (ISAC) systems. A space-time reconfigurable intelligent surface (ST-RIS) impresses a continuous-phase modulation onto the radar echo, enabling uplink data transmission with a phase modulation of the transmitted radar-like waveform. The received signal is a multiplicative composition of the sensing waveform and the phase for communication. Both functionalities share the same physical signal and perceive each other as impairments.
  The achievable communication rate is expressed as a function of a coupling parameter that links sensing accuracy to phase error accumulation. Under a fixed bandwidth constraint, the sensing and communication figures of merit define a convex Pareto frontier. The optimal bandwidth allocation satisfying a minimum sensing requirement is derived in closed form. The modified Cramer-Rao bound (MCRB) for range estimation is derived in closed form; this parameter must be estimated to compensate for the frequency offset before data demodulation. Frame synchronization is formulated as a generalized likelihood ratio test (GLRT), and the detection probability is obtained through characteristic function inversion, accounting for residual frequency errors from imperfect range estimation. Numerical results validate the theoretical bounds and characterize the trade-off across the operating range.

</details>


### [101] [Unique Word Channel Estimation for Oversampled OTFS](https://arxiv.org/abs/2601.09364)
*Radim Zedka,Roman Marsalek,Marek Bobula,Arman Farhang*

Main category: eess.SP

TL;DR: 1. 重采样+脉冲形状导致能量泄漏。2. 理论分析得出能量泄漏与带宽权衡。3. 新的UW-OTFS在时域插入导频，提高36%光谱效率，误码率更低，外波段发射更小。


<details>
  <summary>Details</summary>
Motivation: 解决OTFS中重采样和脉冲成形引起的数据与导频能量泄漏问题。

Method: 对重采样、波形形状的OTFS嵌入式导频方案进行理论分析；随后设计了在时域放置导频的UW-OTFS，并对其性能进行仿真比较。

Result: 分析揭示能量泄漏与多余带宽的权衡；UW-OTFS在光谱效率、误码率与外波段发射方面均优于传统OTFS。

Conclusion: 论文提出并验证了UW-OTFS结构，显著提升了光谱效率（+36%）并降低了误码率与外波段干扰。

Abstract: Practical aspects of orthogonal time frequency space (OTFS), such as channel estimation and its performance in fractional delay-Doppler (DD) channels, are a lively topic in the OTFS community. Oversampling and pulse shaping are also discussed in the existing literature, but not in the context of channel estimation. To the best of our knowledge, this paper is the first to address the problem of data-to-pilot and vice versa energy leakage caused by oversampling and pulse shaping in OTFS. Theoretical analysis is performed on an oversampled, pulse-shaped OTFS implementing the embedded pilot channel estimation technique, revealing a trade-off between the amount of energy leakage and excess bandwidth introduced by the pulse shape. Next, a novel variant of OTFS is introduced, called UW-OTFS, which is designed to overcome the leakage problem by placing the pilot in the oversampled time domain instead of the DD domain. The unique structure of UW-OTFS offers 36 percent higher spectral efficiency than the OTFS with embedded pilot. UW-OTFS also outperforms traditional OTFS in terms of bit error ratio and out-of-band emissions.

</details>


### [102] [Uplink Multi-User MIMO Implementation in OpenAirInterface for a Cell-Free O-RAN Testbed](https://arxiv.org/abs/2601.09384)
*Utku Uçak,Fariba Armandoust,Matthias Mehlhose,Daniel Schäufele,Jochen Fink,Renato L. G. Cavalcante,Sławomir Stańczak*

Main category: eess.SP

TL;DR: OAI实现2x2 MU-MIMO测试，验证SRS估计为非正交复用提供可靠组合方案，支持未来Cell-Free MU-MIMO设计。


<details>
  <summary>Details</summary>
Motivation: 推动Cell-Free MIMO与O-RAN的融合，提供开放源码实现验证新想法的实验平台。

Method: 使用OAI下一代gNB与标准UE搭建2x2 MU-MIMO系统，利用SDR与通用电脑执行，并改造gNB以支持SRS估计和组合器实现。

Result: 成功分离并译码两位用户在非正交时频资源上传输的信号，证明SRS估计可用于构造组合器。

Conclusion: 实现了基于OAI的实时上行MU-MIMO测试平台，验证了利用SRS信道估计计算上行组合器可实现非正交时频资源的多用户复用。

Abstract: Cell-Free Multiple-Input Multiple-Output (MIMO) and Open Radio Access Network (O-RAN) have been active research topics in the wireless communication community in recent years. As an open-source software implementation of the 3rd Generation Partnership Project (3GPP) 5th Generation (5G) protocol stack, OpenAirInterface (OAI) has become a valuable tool for deploying and testing new ideas in wireless communication systems. In this paper, we present our OAI based real-time uplink Multi-User MIMO (MU-MIMO) testbed developed at Fraunhofer HHI. As a part of our Cell-Free MIMO testbed development, we built a 2x2 MU-MIMO system using general purpose computers and commercially available software defined radios (SDRs). Using a modified OAI next-Generation Node-B (gNB) and two unmodified OAI user equipment (UE), we show that it is feasible to use Sounding Reference Signal (SRS) channel estimates to compute uplink combiners. Our results verify that this method can be used to separate and decode signals from two users transmitting in nonorthogonal time-frequency resources. This work serves as an important verification step to build a complete Cell-Free MU-MIMO system that leverages time domain duplexing (TDD) reciprocity to do downlink beamforming over multiple cells.

</details>


### [103] [Evaluating GAN-LSTM for Smart Meter Anomaly Detection in Power Systems](https://arxiv.org/abs/2601.09701)
*Fahimeh Orvati Nia,Shima Salehi,Joshua Peeples*

Main category: eess.SP

TL;DR: 作者评估GAN‑LSTM在大型能源异常检测数据集上的表现，结果显示其异常检测准确率高（F1≈0.89），超过传统方法，可用于配电网的实务监控。


<details>
  <summary>Details</summary>
Motivation: AMI产生的高分辨率用电数据在非线性、非平稳、多尺度特征下异常检测难度大，需要更强的模型来提升监测与决策支持效果。

Method: 采用统一预处理、时窗切分和阈值设置，构建GAN-LSTM框架，对LEAD数据集（406栋建筑的全年小时数据）进行训练，并与统计、核、重建及其他GAN模型进行对比。

Result: GAN-LSTM在实验中取得F1=0.89，明显优于六种基线，验证了其在资产监测、非技术损耗和情境感知中的应用潜力。

Conclusion: 该研究证明了基于GAN-LSTM的异常检测模型在智能电表数据中的高效性能，F1分数可达0.89，显著优于传统六种基线方法，表明对抗式时间序列建模为配电系统监测与损耗检测提供了切实可行的工具。

Abstract: Advanced metering infrastructure (AMI) provides high-resolution electricity consumption data that can enhance monitoring, diagnosis, and decision making in modern power distribution systems. Detecting anomalies in these time-series measurements is challenging due to nonlinear, nonstationary, and multi-scale temporal behavior across diverse building types and operating conditions. This work presents a systematic, power-system-oriented evaluation of a GAN-LSTM framework for smart meter anomaly detection using the Large-scale Energy Anomaly Detection (LEAD) dataset, which contains one year of hourly measurements from 406 buildings. The proposed pipeline applies consistent preprocessing, temporal windowing, and threshold selection across all methods, and compares the GAN-LSTM approach against six widely used baselines, including statistical, kernel-based, reconstruction-based, and GAN-based models. Experimental results demonstrate that the GAN-LSTM significantly improves detection performance, achieving an F1-score of 0.89. These findings highlight the potential of adversarial temporal modeling as a practical tool for supporting asset monitoring, non-technical loss detection, and situational awareness in real-world power distribution networks. The code for this work is publicly available

</details>
